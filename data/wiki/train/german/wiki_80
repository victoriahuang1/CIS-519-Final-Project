<doc id="11878" url="https://de.wikipedia.org/wiki?curid=11878" title="Gleichung">
Gleichung

Unter einer Gleichung versteht man in der Mathematik eine Aussage über die Gleichheit zweier Terme, die mit Hilfe des Gleichheitszeichens („=“) symbolisiert wird. Formal hat eine Gleichung die Gestalt

wobei der Term formula_2 die linke Seite und der Term formula_3 die rechte Seite der Gleichung genannt wird. Gleichungen sind entweder wahr beziehungsweise erfüllt (beispielsweise formula_4) oder falsch (beispielsweise formula_5). Wenn zumindest einer der Terme formula_6 von Variablen abhängig ist, liegt nur eine Aussageform vor; ob die Gleichung wahr oder falsch ist, hängt dann von den konkreten eingesetzten Werten ab. Die Werte der Variablen, für die die Gleichung erfüllt ist, heißen Lösungen der Gleichung. Sind zwei oder mehr Gleichungen angegeben, spricht man auch von einem Gleichungssystem, eine Lösung desselben muss alle Gleichungen simultan erfüllen.

Gleichungen werden in vielen Zusammenhängen verwendet; dementsprechend gibt es verschiedene Möglichkeiten, die Gleichungen nach unterschiedlichen Gesichtspunkten einzuteilen. Die jeweiligen Einteilungen sind zu einem großen Teil unabhängig voneinander, eine Gleichung kann in mehrere dieser Gruppen fallen. So ist es etwa sinnvoll, von einem System linearer partieller Differentialgleichungen zu sprechen.

Gleichungen können allgemeingültig sein, also durch Einsetzen aller Variablenwerte aus einer gegebenen Grundmenge oder zumindest aus einer vorher definierten Teilmenge davon wahr sein. Die Allgemeingültigkeit kann entweder mit anderen Axiomen bewiesen werden oder selber als Axiom vorausgesetzt werden.

Beispiele sind:


In diesem Zusammenhang spricht man auch von einem mathematischen Satz oder Gesetz. Zur Unterscheidung von nicht allgemeingültigen Gleichungen wird bei Identitäten statt des Gleichheitszeichens auch das Kongruenzzeichen („≡“) verwendet.

Häufig besteht eine Aufgabenstellung darin, alle Variablenbelegungen zu bestimmen, für die die Gleichung wahr wird. Diesen Vorgang bezeichnet man als Lösen der Gleichung. Zur Unterscheidung von Identitätsgleichungen werden solche Gleichungen als Bestimmungsgleichungen bezeichnet. Die Menge der Variablenbelegungen, für die die Gleichung wahr ist, bezeichnet man als Lösungsmenge der Gleichung. Wenn es sich bei der Lösungsmenge um die leere Menge handelt, so bezeichnet man die Gleichung als unlösbar oder unerfüllbar.

Ob eine Gleichung lösbar ist oder nicht, kann von der betrachteten Grundmenge abhängen, zum Beispiel gilt:


Bei Bestimmungsgleichungen treten mitunter Variablen auf, die nicht gesucht sind, sondern als bekannt vorausgesetzt werden. Solche Variablen werden als Parameter bezeichnet. Beispielsweise lautet die Lösungsformel für die quadratische Gleichung

bei gesuchter Unbekannte formula_22 und gegebenen Parametern formula_23 und formula_24

Setzt man eine der beiden Lösungen formula_26 in die Gleichung ein, so verwandelt sich die Gleichung in eine Identität, wird also für eine beliebige Wahl von formula_23 und formula_24 zur wahren Aussage. Für formula_29 sind hier die Lösungen reell, ansonsten komplex.

Gleichungen können auch verwendet werden, um ein neues Symbol zu definieren. In diesem Fall wird das zu definierende Symbol links geschrieben, und das Gleichheitszeichen oft durch das Definitionszeichen („:=“) ersetzt oder über das Gleichheitszeichen „def“ geschrieben.

Zum Beispiel wird die Ableitung einer Funktion formula_30 an einer Stelle formula_31 durch

definiert. Im Gegensatz zu Identitäten sind Definitionen keine Aussagen; sie sind also weder wahr noch falsch, sondern nur mehr oder weniger zweckmäßig.

Eine Bestimmungsgleichung der Form

heißt homogene Gleichung. Ist formula_34 eine Funktion, nennt man die Lösung formula_22 auch Nullstelle der Funktion. Homogene Gleichungen spielen bei der Lösungsstruktur linearer Gleichungssysteme und linearer Differentialgleichungen eine wichtige Rolle. Ist die rechte Seite einer Gleichung ungleich Null, heißt die Gleichung inhomogen.

Eine Bestimmungsgleichung der Form

heißt Fixpunktgleichung und deren Lösung formula_22 nennt man Fixpunkt der Gleichung. Genaueres über die Lösungen solcher Gleichungen sagen Fixpunktsätze aus.

Eine Bestimmungsgleichung der Form

heißt Eigenwertproblem, wobei die Konstante formula_39 (der Eigenwert) und die Unbekannte formula_40 (der Eigenvektor) gemeinsam gesucht werden. Eigenwertprobleme besitzen vielfältige Einsatzbereiche in der linearen Algebra, beispielsweise bei der Analyse und Zerlegung von Matrizen, und in Anwendungsgebieten, beispielsweise der Strukturmechanik und der Quantenmechanik.

Eine Gleichung heißt linear, wenn sie in die Form

gebracht werden kann, wobei der Term formula_42 unabhängig von formula_22 ist und der Term formula_44 linear in formula_22 ist, also

für Koeffizienten formula_47 gilt. Sinnvollerweise müssen die passenden Operationen definiert sein, es ist also notwendig, dass formula_44 und formula_42 aus einem Vektorraum formula_50 sind, und die Lösung formula_22 aus dem gleichen oder einem anderen Vektorraum formula_52 gesucht wird.

Lineare Gleichungen sind normalerweise wesentlich einfacher zu lösen als nichtlineare. So gilt für lineare Gleichungen das Superpositionsprinzip: Die allgemeine Lösung einer inhomogenen Gleichung ist die Summe einer Partikulärlösung der inhomogenen Gleichung und der allgemeinen Lösung der zugehörigen homogenen Gleichung.

Wegen der Linearität ist zumindest formula_53 eine Lösung einer homogenen Gleichung. Hat eine homogene Gleichung also eine eindeutige Lösung, so hat auch eine entsprechende inhomogene Gleichung höchstens eine Lösung. Eine verwandte, aber wesentlich tiefer gehende Aussage in der Funktionalanalysis ist die Fredholmsche Alternative.

Nichtlineare Gleichungen werden oft nach der Art der Nichtlinearität unterschieden. Insbesondere in der Schulmathematik werden die nachfolgenden Grundtypen von nichtlinearen Gleichungen behandelt.

Handelt es sich bei dem Gleichungsterm um ein Polynom, spricht man von einer algebraischen Gleichung. Ist dabei das Polynom mindestens vom Grad zwei, so bezeichnet man die Gleichung als nichtlinear. Beispiele sind allgemeine quadratische Gleichungen der Form

oder kubische Gleichungen der Form

Für Polynomgleichungen bis zum Grad vier gibt es allgemeine Lösungsformeln.

Enthält eine Gleichung einen Bruchterm, bei dem die Unbekannte zumindest im Nenner vorkommt, spricht man von einer Bruchgleichung, zum Beispiel

Durch Multiplikation mit dem Hauptnenner, im Beispiel formula_57, lassen sich Bruchgleichungen auf algebraische Gleichungen zurückführen. Eine solche Multiplikation ist im Regelfall keine Äquivalenzumformung und es muss eine Fallunterscheidung vorgenommen werden, im Beispiel ist formula_58 nicht im Definitionsbereich der Bruchgleichung enthalten.

Bei Wurzelgleichungen steht die Unbekannte mindestens einmal unter einer Wurzel, beispielsweise

Wurzelgleichungen sind spezielle Potenzgleichungen mit Exponent formula_60. Wurzelgleichungen lassen sich lösen, indem eine Wurzel isoliert wird und dann die Gleichung mit dem Wurzelexponenten formula_61 (im Beispiel ist formula_62) potenziert wird. Dieses Vorgehen wird wiederholt, bis alle Wurzeln eliminiert sind. Potenzieren mit geradzahligem Exponenten stellt keine Äquivalenzumformung dar und daher ist in diesen Fällen bei der Ermittlung der Lösung eine entsprechende Fallunterscheidung vorzunehmen. Im Beispiel führt Quadrieren zu der quadratischen Gleichung formula_63, deren negative Lösung nicht im Definitionsbereich der Ausgangsgleichung liegt.

Bei Exponentialgleichungen steht die Unbekannte mindestens einmal im Exponenten, zum Beispiel:

Exponentialgleichungen lassen sich durch Logarithmieren lösen. Umgekehrt sind Logarithmusgleichungen - also Gleichungen, bei denen die Unbekannte als Numerus (Argument einer Logarithmusfunktion) auftritt - durch Exponenzieren lösbar.

Treten die Unbekannten als Argument mindestens einer Winkelfunktion auf, so spricht man von einer trigonometrischen Gleichung, beispielsweise

Die Lösungen trigonometrischer Gleichungen wiederholen sich im Allgemeinen periodisch, sofern die Lösungsmenge nicht auf ein bestimmtes Intervall, etwa formula_66, beschränkt wird. Alternativ können die Lösungen durch eine ganzzahlige Variable formula_67 parametrisiert werden. Beispielsweise sind die Lösungen obiger Gleichung gegeben als

Um Gleichungen, bei denen eine reelle Zahl oder ein reeller Vektor gesucht wird, von Gleichungen, bei denen beispielsweise eine Funktion gesucht ist, zu unterscheiden, wird manchmal auch die Bezeichnung algebraische Gleichung verwendet, wobei diese Bezeichnung dann aber nicht auf Polynome eingeschränkt ist. Diese Sprechweise ist jedoch umstritten.

Sucht man ganzzahlige Lösungen einer skalaren Gleichung mit ganzzahligen Koeffizienten, so spricht man von einer Diophantischen Gleichung. Ein Beispiel einer kubischen Diophantischen Gleichung ist

von der ganzzahlige formula_71 gesucht werden, die die Gleichung erfüllen, hier die Zahlen formula_72.

Ist die Unbekannte eine Folge, so spricht man von einer Differenzengleichung. Ein bekanntes Beispiel einer linearen Differenzengleichung zweiter Ordnung ist

deren Lösung für Startwerte formula_74 und formula_75 die Fibonacci-Folge formula_76 ist.

Ist die Unbekannte der Gleichung eine Funktion, die ohne Ableitungen auftritt, so spricht man von einer Funktionalgleichung. Ein Beispiel für eine Funktionalgleichung ist

deren Lösungen gerade die Exponentialfunktionen formula_78 sind.

Wird in der Gleichung eine Funktion gesucht, die mit Ableitungen auftritt, so spricht man von einer Differentialgleichung. Differentialgleichungen treten bei der Modellierung von naturwissenschaftlichen Problemen sehr häufig auf. Die höchste auftretende Ableitung wird dabei Ordnung der Differentialgleichung genannt. Man unterscheidet:





Tritt die gesuchte Funktion in einem Integral auf, so spricht man von einer Integralgleichung. Ein Beispiel einer linearen Integralgleichung 1. Art ist

Befinden sich in einer Zeile mehrere Gleichheitszeichen, so spricht man von einer Gleichungskette. In einer Gleichungskette sollen alle durch Gleichheitszeichen getrennten Ausdrücke vom Wert her gleich sein. Dabei ist jeder dieser Ausdrücke separat zu betrachten. Beispielsweise ist die Gleichungskette

falsch, weil sie in Einzelgleichungen zerlegt zu falschen Aussagen führt. Wahr ist dagegen zum Beispiel

Gleichungsketten sind insbesondere wegen der Transitivität der Gleichheitsrelation sinnvoll interpretierbar. Gleichungsketten treten oft auch gemeinsam mit Ungleichungen in Abschätzungen auf, so gilt beispielsweise für formula_86

Oft werden mehrere Gleichungen, die gleichzeitig erfüllt sein müssen, betrachtet und dabei mehrere Unbekannte gleichzeitig gesucht.

Ein Gleichungssystem - also eine Menge von Gleichungen - heißt lineares Gleichungssystem, wenn alle Gleichungen linear sind. Beispielsweise ist

ein lineares Gleichungssystem bestehend aus zwei Gleichungen und drei Unbekannten formula_89 und formula_90. Fasst man sowohl die Gleichungen, als auch die Unbekannten zu Tupeln zusammen, so lässt sich ein Gleichungssystem auch als eine einzelne Gleichung für einen unbekannten Vektor auffassen. So schreibt man in der Linearen Algebra ein Gleichungssystem als Vektorgleichung

mit einer Matrix formula_92, den unbekannten Vektor formula_93 und der rechten Seite formula_94, wobei formula_95 das Matrix-Vektor-Produkt ist. In obigem Beispiel sind

Gleichungssysteme, deren Gleichungen nicht alle linear sind, werden nichtlineare Gleichungssysteme genannt. Beispielsweise ist

ein nichtlineares Gleichungssystem mit den Unbekannten formula_22 und formula_101. Für solche Gleichungssysteme gibt es keine allgemeingültigen Lösungsstrategien. Oftmals hat man nur die Möglichkeit, näherungsweise Lösungen mit Hilfe numerischer Verfahren zu bestimmen. Ein mächtiges Näherungsverfahren ist beispielsweise das Newton-Verfahren.

Eine Faustregel besagt, dass gleich viele Gleichungen wie Unbekannte benötigt werden, damit ein Gleichungssystem eindeutig lösbar ist. Das ist aber tatsächlich nur eine Faustregel, bis zu einem gewissen Grad gilt sie wegen des Hauptsatzes über implizite Funktionen für reelle Gleichungen mit reellen Unbekannten.

Soweit es möglich ist, versucht man, die Lösungen einer Bestimmungsgleichung exakt zu ermitteln. Wichtigstes Hilfsmittel dabei sind Äquivalenzumformungen, durch die eine Gleichung schrittweise in andere äquivalente Gleichungen (die also dieselbe Lösungsmenge haben) umgeformt wird, bis man eine Gleichung erhält, deren Lösung einfach bestimmt werden kann.

Viele Gleichungen, insbesondere aus naturwissenschaftlichen Anwendungen, können nicht analytisch gelöst werden. In diesem Fall versucht man, am Computer eine näherungsweise numerische Lösung zu berechnen. Solche Verfahren werden in der numerischen Mathematik behandelt. Viele nichtlineare Gleichungen lassen sich approximativ lösen, indem die in der Gleichung auftretenden Nichtlinearitäten linear angenähert werden, und dann die entstehenden linearen Probleme gelöst werden (beispielsweise im Newton-Verfahren). Für andere Problemklassen, etwa bei der Lösung von Gleichungen in unendlich-dimensionalen Räumen, wird die Lösung in geeignet gewählten endlich-dimensionalen Unterräumen gesucht (beispielsweise in der Galerkin-Methode).

Auch wenn eine Gleichung nicht analytisch gelöst werden kann, ist es dennoch oft möglich, mathematische Aussagen über die Lösung zu treffen. Insbesondere interessieren Fragestellungen, ob eine Lösung überhaupt existiert, ob sie eindeutig ist, und ob sie stetig von den Parametern der Gleichung abhängt. Ist dies der Fall spricht man von einem korrekt gestellten Problem. Eine qualitative Analyse ist auch bzw. gerade bei der numerischen Lösung einer Gleichung wichtig, damit sichergestellt ist, dass die numerische Lösung tatsächlich eine Näherungslösung der Gleichung liefert.




</doc>
<doc id="11884" url="https://de.wikipedia.org/wiki?curid=11884" title="Justinian I.">
Justinian I.

Justinian, eigentlich Flavius Petrus Sabbatius Iustinianus, , in einigen Quellen und Teilen der älteren Literatur auch "Justinian der Große" genannt (* um 482 in Tauresium; † 14. November 565 in Konstantinopel), war vom 1. August 527 bis zu seinem Tod römischer Kaiser. In manchen orthodoxen Quellen wird er als Heiliger bezeichnet, im offiziellen kirchlichen Synaxarion wird er jedoch nicht als Heiliger geführt.

Justinian gilt als einer der bedeutendsten Herrscher der Spätantike. Seine Regierungszeit markiert eine wichtige Übergangsphase vom antiken Imperium Romanum zum Byzantinischen Reich des Mittelalters. Für das Ende der Antike stehen die von ihm befohlene Schließung der neuplatonischen Philosophenschule in Athen 529 und die Abschaffung des altrömischen Amts des Konsuls im Jahr 542. Andererseits gelang es ihm in langen Kriegen gegen Ostgoten und Vandalen, weite Teile des 476 untergegangenen Weströmischen Reichs wiederzugewinnen, die im Verlauf der sogenannten Völkerwanderung an die Germanen gefallen waren. Im Osten sah sich das Reich zu seiner Zeit in ähnlich schwere, wechselhafte Kämpfe mit den neupersischen Sassaniden verwickelt. Prägende Bedeutung gewann Justinian für die Rechtsgeschichte, da er die Kompilation des römischen Rechts, das später so genannte Corpus Iuris Civilis, in Auftrag gab.

Das Kaisertum erfuhr während seiner Regierung eine immer stärkere Sakralisierung. Dies zerstörte die letzten Reste der einst vom Prinzipat geschaffenen Fiktion, dass der Kaiser nur ein "primus inter pares" sei. Die wichtigste erzählende Quelle für die Zeit Justinians sind die Werke des Geschichtsschreibers Prokopios von Caesarea, der die Politik des Kaisers heftig kritisierte.

Flavius Petrus Sabbatius Iustinianus war ein ungefähr 482 geborener Bauernsohn der Vigilantia aus dem Dorf Tauresium (heute Taor in der Republik Mazedonien) in der Prätorianerpräfektur Illyrien; der Name Sabbatius deutet vielleicht auf einen thrakischen Ursprung der Familie hin. Justinians Muttersprache war das Lateinische, und Tauresium gehörte zum Zuständigkeitsbereich des Bischofs von Rom; beides waren wohl Gründe für Justinians starke Westorientierung. Prokopios wirft dem Kaiser zwar fehlerhaftes Griechisch vor, tatsächlich aber war er belesen und gebildet. Johannes Malalas schreibt wohl zutreffend: "Beim Sprechen des Griechischen machte er manchen Fehler, doch schreiben konnte er es mit großer Leichtigkeit." Zeitlebens sollte der soziale Aufsteiger Justinian Anfeindungen von Seiten der senatorischen Elite ausgesetzt sein.

Justinian war ein Neffe des späteren Kaisers Justin I., der seit etwa 470 im Heer Kaiser Leos I. und später unter Zenon und Anastasius Karriere machte. Schon zu Lebzeiten seines Onkels, der ihn in die Hauptstadt holte, ihm eine gute Ausbildung ermöglichte und ihn nach seiner Thronbesteigung (518) wohl früh zum Nachfolger aufbaute und wahrscheinlich adoptierte (daher wohl der Name "Iustinianus"), hatte Justinian nach Ansicht der meisten Forscher großen Einfluss auf die Reichspolitik. Diese traditionelle Sichtweise wurde jüngst jedoch bestritten, und wenigstens für die frühen Jahre sollte man die Rolle Justinians wohl nicht überbewerten. Fest steht aber, dass er nach 518 rasch Karriere machte: 519 wurde Justinian zum "comes" ernannt, 521 wurde er "magister equitum et peditum praesentalis" und bekleidete sein erstes Konsulat (drei weitere folgten: 528, 533 und 534); seit 525 trug er vermutlich den Titel "Caesar" und galt damit wohl offiziell als Thronerbe. Anfang April 527 wurde er schließlich zum Mitkaiser ("Augustus") erhoben, am 1. August stieg er dann nach Justins Tod zum Alleinherrscher auf. Er war seit 524 oder 525 mit der ebenfalls aus einfachsten Verhältnissen stammenden Theodora verheiratet, die 527 den Titel "Augusta" erhielt und laut einigen Quellen einen großen Einfluss auf Justinian gehabt haben soll. Die Ehe blieb ohne Nachkommen (mindestens ein Kind starb unmittelbar nach der Geburt). Justinian hatte mehrere Vettern; der bedeutendste war Germanus, der auch ein talentierter Feldherr und wichtiger Vertrauter Justinians war.

Justinian starb am 14. November 565 in Konstantinopel, wo er sich fast seine gesamte Regierungszeit hindurch aufgehalten hatte. Das "Chronicon Paschale" überliefert in diesem Zusammenhang eine Beschreibung seines Aussehens, die als zeitgenössisch gilt: "Untersetzt, aber mit breitem Brustkorb, blass; mit dünnem Haar und einer Stirnglatze, einem runden, gutaussehenden Gesicht mit rötlichen Wangen, stets leicht lächelnd; mit graumeliertem Haar und einem nach römischer Sitte glattrasierten Kinn, einer wohlgeformten Nase und heller Haut." Nach dem Tod des Kaisers, der keinen Mitherrscher ernannt hatte, drohte zunächst ein Machtkampf zwischen zweien seiner Neffen, beide mit Namen Justin: dem General Justin, einem Sohn des oben genannten Germanus, und dem Chef der Hofhaltung, dem späteren Kaiser Justin II. Letzterer konnte sich schließlich durchsetzen; er ließ seinen Rivalen bald darauf ermorden.

Laut dem Augenzeugen Niketas Choniates wurde Justinians Grab in der Apostelkirche im April 1204 von Kreuzrittern geplündert, wobei der Leichnam des Kaisers damals noch ungewöhnlich gut erhalten gewesen sein soll.

Im Verlauf des 5. Jahrhunderts war der Westen des "Imperium Romanum" der direkten kaiserlichen Kontrolle entglitten. 476/80 war das weströmische Kaisertum erloschen, und da West- und Ostrom formal niemals zwei voneinander getrennte Staaten gewesen waren (siehe Reichsteilung von 395), lag die Herrschaft über das Gesamtreich fortan beim einzig verbliebenen "Augustus" in Konstantinopel. Aus Sicht Justinians handelte es sich bei seiner Westpolitik also im Grunde nicht um Außenpolitik, denn die (ost-)römischen Herrscher hatten ihre Ansprüche auf das Westreich seither nie aufgegeben. Sie wurden von fast allen gentilen Reichsbildungen auch tatsächlich formal als Oberherren anerkannt, doch agierten die "reges" des Westens "de facto" weitgehend unabhängig vom Willen Ostroms. Justinian, der als letzter römischer Kaiser Latein als Muttersprache sprach, gab sich mit diesem Zustand nicht zufrieden und strebte die Wiederherstellung der faktischen Herrschaft des Kaisers über die ganze spätantike Oikumene an "(Restauratio imperii)". Während seiner Regentschaft wurden daher große Gebiete im Westen des alten "Imperium Romanum" mit militärischer Gewalt unterworfen.

Ob diese Offensiven bereits von langer Hand geplant waren, ist in der Forschung umstritten. Eine bereits längerfristige Konzeption wird in den letzten Jahren vermehrt bezweifelt; vielmehr habe man erst nach den unerwartet raschen Erfolgen von Justinians Feldherr Belisar über die Vandalen im Jahr 534 begonnen, weiterreichende Ziele zu verfolgen. Die Kriege Justinians wurden besonders von dem Zeitgenossen Prokopios von Caesarea in seinem acht Bücher umfassenden griechischen Geschichtswerk ("Bella" oder "Historien") eingehend geschildert.

Das Hauptaugenmerk der oströmischen Politik lag allerdings nicht auf dem Westen, sondern auf dem Osten, wo die Römer bereits seit drei Jahrhunderten dem mächtigen persischen Sassanidenreich gegenüberstanden. Justinians erster Perserkrieg war dabei ein Erbe aus der Regierungszeit seines Vorgängers Justin I.; seit 526 wurde gekämpft. In Mesopotamien konnte Belisar 530/531 als neuer "magister militum per Orientem" erste Erfolge erzielen (Schlacht bei Dara), musste aber auch Niederlagen erleiden (wie 531 in der Schlacht von Callinicum). Den Sieg bei Dara ließ der Kaiser unter anderem durch die Errichtung eines heute verlorenen Reiterstandbildes feiern; die von seinem damaligen "praefectus praetorio" Flavius Iulianus verfasste Inschrift ist überliefert (Anthol. Palat. 16,63); sie preist den Kaiser dafür, die „Meder [d. h. hier die Perser] niedergemacht“ zu haben. 531 ließ Justinian eine große Siegesfeier abhalten, um sich der Erfolge über Perser und Protobulgaren zu rühmen. In Wahrheit aber endete der Krieg ohne einen eindeutigen Sieger. Mit dem neuen sassanidischen Großkönig Chosrau I. schloss Justinian Ende 532 ein von recht hohen (aber einmaligen) Zahlungen an die Perser begleitetes Abkommen, den „Ewigen Frieden“. Diese Ruhe im Osten machte Justinians anschließende Westpolitik erst möglich, da die Ressourcen Ostroms bereits stark beansprucht waren.

540 brachen die Kämpfe allerdings erneut aus, als der Perserkönig den Frieden brach. Laut Prokopios stand dahinter die Sorge Chosraus, dass ein erneuertes Römerreich stärkere Ressourcen gegen Persien mobilisieren könne; eventuell spielte auch ein ostgotisches Hilfegesuch an Persien eine Rolle. Der Hauptgrund für den persischen Angriff dürfte aber wohl einfach in der günstigen Lage zu sehen sein: Chosrau I. suchte militärischen Ruhm und brauchte Geld, und da das römische Syrien nur schwach verteidigt war, wollte er vermutlich einfach einen Plünderungszug unternehmen und danach wieder Frieden schließen. Überdies brach um diese Zeit die Macht der Hephthaliten, die Persien im Nordosten bedroht hatten, zusammen, so dass Chosrau gegenüber Rom die Hände frei hatte.

Justinian scheint bereits 539 von den Angriffsplänen gewusst zu haben, konnte aber angesichts des Gotenkriegs nicht rechtzeitig Truppen an den Euphrat entsenden – versprochene Verstärkungen trafen nur in sehr geringer Zahl ein. Germanus wurde mit nur 300 Mann nach Antiochia am Orontes geschickt und konnte dort auch nichts ausrichten. Belisars Nachfolger im Orient, der "magister militum" Buzes, musste mit den lokalen römischen Truppen operieren, die der großen persischen Armee zahlenmäßig weit unterlegen waren; er zog sich auf eine Verteidigungsstellung bei Hierapolis zurück und wartete ab. Die bedeutendsten Städte der Region unterwarfen sich Chosrau. Die größte Katastrophe für die Römer war dann zweifellos die Eroberung, Plünderung und anschließende Zerstörung der Weltstadt Antiochia, wobei Chosrau gewaltige Schätze und zahlreiche Gefangene nach Persien überführte, wo sie in einer eigenen Stadt nahe Ktesiphon angesiedelt wurden. Chosrau soll außerdem ein rituelles Bad im Meer genommen und dem Sonnengott geopfert haben. Andere Städte hatten mehr Glück als Antiochia und konnten sich freikaufen oder hielten den persischen Angriffen stand. In Apameia am Orontes, das ihm die Tore öffnete, ließ der König Wagenrennen durchführen und präsentierte sich dabei in kaiserlicher Pose, was eine unerhörte Provokation Justinians darstellte.

Chosrau bot den Römern nun einen erneuten Friedensschluss an, doch Justinian scheint den Glauben an die sassanidische Vertragstreue verloren zu haben und lehnte ab. Der Krieg wurde fortgesetzt; aber es gelang den Römern nur langsam, die Lage zu stabilisieren. Die ohnehin stark beanspruchte oströmische Armee (die Mannschaftsstärke betrug laut Agathias nur etwa 150.000 Mann, aber diese Angabe muss mit großer Vorsicht behandelt werden – viel wahrscheinlicher ist eine Zahl von gut 300.000 Soldaten) musste nun einen Zweifrontenkrieg führen: gegen die Ostgoten in Italien und gegen die Perser im Osten. Überdies war der Balkanraum durch Plünderungszüge der Awaren und Slawen bedroht.

Der östliche Kriegsschauplatz erstreckte sich schließlich vom Kaukasus (vor allem in Armenien, wo Justinians General Sittas bis zu seinem Tod 539 sehr erfolgreich operiert hatte, und um die wichtige Festung Petra am Schwarzen Meer wurde seit 541 erbittert gekämpft) bis nach Mesopotamien. Der wichtigste Streitpunkt und ein Zentrum der Kampfhandlungen zwischen Römern und Persern war vor allem Lazika, ein kleines Königreich am Schwarzen Meer, identisch mit dem früheren Kolchis; dort hatte Ostrom seit dem frühen 6. Jahrhundert seinen Einfluss ausgebaut (siehe Tzath und Gubazes II.). Der Krieg zwischen Ostrom und Persien sollte bis 561/62 andauern (unterbrochen von einem Waffenstillstand, der sich bezeichnenderweise nicht auf Lazika bezog) und die Ressourcen Ostroms stark strapazieren. Anders als oft behauptet, vernachlässigte Justinian dabei keineswegs die Verteidigung der Ostgrenze zugunsten seiner Eroberungen im Westen. Da sich im Orient bald ein militärisches Patt entwickelte und sich die Perser um 560 mit einem neuen Feind, den Türken, konfrontiert sahen, waren sie 562 zum Frieden mit den Römern bereit. In diesem Vertrag, der von Petrus Patricius für Justinian ausgehandelt wurde, überließen die Perser den Römern Lazika – Justinian hatte die Ostgrenze also letztlich doch halten können, wenngleich er nun den Persern jährlich Tribut zahlen musste. Es ist letztlich unklar, wie stark diese Zahlungen die römischen Kassen belasteten – dennoch waren die Römer wohl mehrheitlich nicht glücklich mit der Tributverpflichtung. Justinians Nachfolger Justin II. versuchte dann auch, diesen Vertrag zu revidieren – allerdings mit katastrophalem Ergebnis.

Seit 540 wurde die Mehrheit der römischen Truppen im Orient eingesetzt. Dies trug zur langen Dauer des Krieges in Italien bei. Im Kaukasus und in Mesopotamien operierten zeitgleich stets mindestens zwei große kaiserliche Armeen. Justinian war zwar im Osten eher defensiv tätig und widmete sich stärker der Politik im Westen, er entsandte aber nur dann Truppen nach Italien, wenn er sie im Osten entbehren zu können glaubte. Durch eine Mischung aus diplomatischen und militärischen Mitteln konnte der Kaiser die römische Position gegenüber Persien dabei letztlich halten; die Verpflichtung zu jährlichen Tributen wog dagegen gering, wurde aber als Demütigung empfunden. Sein Nachfolger Justin II. ging daher ab 572 wieder aggressiv gegen die Sassaniden vor – was allerdings nur in einen jahrelangen Krieg mündete, der erst 591 unter Maurikios vorübergehend beendet werden konnte, bevor Chosrau II. dann 602 den letzten und größten römisch-persischen Krieg begann (siehe Herakleios).

Der Krieg gegen das Vandalenreich in Nordafrika (etwa deckungsgleich mit dem modernen Tunesien und dem nordöstlichen Algerien, siehe auch "Africa") begann ursprünglich als Strafexpedition. Der dem Katholizismus nicht feindlich gesinnte arianische König Hilderich, der zudem kaiserliche Vorfahren hatte, war 530 abgesetzt und durch Gelimer ersetzt worden. Justinian bestand nun in seiner Rolle als Oberherr auch des Westens auf der Wiedereinsetzung Hilderichs, was aber abgewiesen wurde. Daraufhin entschied man in Konstantinopel nach längerer Debatte, militärisch im Vandalenreich zu intervenieren und dort einen genehmen Herrscher einzusetzen. Da der letzte Vandalenfeldzug ein halbes Jahrhundert zuvor katastrophal gescheitert war, wollte man es bei einer begrenzten Intervention belassen. Den Charakter einer regelrechten Eroberungskampagne erhielt der Feldzug nach Ansicht vieler Forscher (etwa Mischa Meier oder Hartmut Leppin) daher wohl erst im Nachhinein.

Belisar begann schließlich 533 den Feldzug, ein Jahr nach dem Friedensschluss mit Persien, mit einem etwa 20.000 Mann starken Heer (bestehend aus 15.000 kaiserlichen Soldaten, 1000 "foederati" und den etwa 5000 "buccelarii" Belisars) und 30.000 Matrosen. Die Kämpfe konnten innerhalb kürzester Zeit abgeschlossen werden. Laut Prokopios trugen dabei die berittenen "buccelarii" die Hauptlast. Hilfreich war, dass der Vandalenkönig nicht mit einem Angriff der Oströmer gerechnet und Teile seiner Streitkräfte nach Sardinien gesandt hatte, um eine dortige Revolte niederzuschlagen. Gelimer ließ Hilderich hinrichten, doch Belisar besiegte die Vandalen bei "Ad Decimum". Am 15. September 533 fiel Karthago. Belisar siegte erneut bei "Tricamarum" und nahm 534 schließlich auch Gelimer gefangen, der in einem typisch spätantiken „Triumphzug“ durch Konstantinopel geführt wurde, wo er sich gemeinsam mit Belisar Justinian unterwarf und begnadigt wurde. Statt wie geplant einen neuen "rex" einzusetzen, hatte man unversehens das Vandalenreich zerschlagen. Vermutlich entstand erst jetzt, nach diesem überraschend leichten Sieg, der Plan, auch Italien wieder der direkten kaiserlichen Herrschaft zu unterwerfen.

Nordafrika wurde bereits 534 wieder in die reguläre kaiserliche Verwaltung übernommen; unter einem neuen "praefectus praetorio per Africam" standen sieben Provinzstatthalter, während auf militärischer Seite künftig ein "magister militum per Africam" das Oberkommando über die "duces" von "Tripolitania", "Byzacena", "Numidia", "Mauretania Caesariensis" und "Sardinia" innehatte.

Der unerwartete, rasche Sieg über das gefürchtete Vandalenreich markierte den frühen Höhepunkt der justinianischen Herrschaft; der Kaiser hatte nicht nur einen scheinbar dauerhaften Frieden mit den Persern erzielt, sondern nun auch ein Kernland des "Imperium Romanum" mit scheinbar leichter Hand zurückgewonnen. In der Vorrede der Endfassung des "Codex Iustinianus" (siehe unten) nannte er sich im Dezember 534 daher selbstbewusst IMPERATOR CAESAR FLAVIUS IUSTINIANUS ALAMANNICUS GOTHICUS FRANCICUS GERMANICUS ANTICUS
ALANICUS VANDALICUS AFRICANUS PIUS FELIX INCLITUS VICTOR AC TRIUMPHATOR SEMPER AUGUSTUS („Der "Imperator Caesar" Flavius Justinianus, Sieger über Alamannen, Goten, Franken, Germanen, Anten, Alanen, Vandalen und Afrikaner, der Fromme, Glückliche, Berühmte, der Sieger und Triumphator, allzeit "Augustus"“).

Allerdings kam es in Nordafrika schon bald wieder zu Kämpfen mit den Berbern (Mauren), die einen ständigen Unruheherd darstellten, und auch zu mehreren Meutereien der oströmischen Garnisonstruppen, wie die unter Stotzas. Justinians General Johannes Troglita konnte die Berberrebellion nach langwierigen Kämpfen jedoch niederschlagen und die verbliebenen Eindringlinge friedlich ansiedeln; auch ein vandalischer Restaurationsversuch 546 unter Guntarith scheiterte. Lange nahm die Forschung an, Nordafrika habe nach der Rückeroberung einen massiven Niedergang erlebt, doch ist diese Annahme durch neue Untersuchungen revidiert worden. "Africa" erlebte in den Jahrzehnten um 600 noch einmal eine bescheidene Blüte und blieb immerhin bis 698 römisch und christlich.

Schon kurz nach dem Sieg über die Vandalen begann Justinian einen weiteren Krieg im Westen. Durch die Eroberung Nordafrikas hatte sich die strategische Lage grundlegend verändert, und ein Angriff auf Italien versprach nun erstmals seit Jahrzehnten wieder Erfolg. Den konkreten Anlass für das Eingreifen Ostroms auf der Halbinsel bildeten dabei die Intrigen und Thronkämpfe nach dem Tod des bedeutenden Ostgotenkönigs Theoderich (526). Seine Tochter Amalasuntha suchte eine Anlehnung an Ostrom, während Theoderichs Neffe Theodahad seine eigene Position stärken wollte. Nach dem Tod von Amalasunthas jungem Sohn Athalarich im Jahre 534 gelang es Theodahad, die Erhebung zum "rex" zu erwirken. Das nun folgende diplomatische Intrigenspiel ist kaum zu durchschauen; sicher ist nur, dass Amalasuntha getötet wurde und Justinian dies zum Anlass nahm, Rüstungen einzuleiten. Die Spannungen führten schließlich 535 zum offenen Krieg ("Gotenkrieg"), doch wurden die Kämpfe gegen die wehrhaften Ostgoten langwieriger als erwartet. Ein oströmischer Angriff auf Dalmatien scheiterte, während Belisar Sizilien und bald darauf Neapel einnehmen konnte. Theodahad versagte vollkommen, woraufhin er gestürzt und als "rex" durch Witichis abgelöst wurde. Dieser organisierte den Widerstand recht erfolgreich, verlor aber Ende 536 Rom an Belisar. Versuche, die Stadt, die noch immer etwa 100.000 Einwohner hatte, erneut zu erobern, scheiterten. Es kam zu schweren Kämpfen, die sehr wechselhaft verliefen und für die Bevölkerung Italiens mit großen Lasten verbunden waren. So wurde das von oströmischen Truppen eroberte Mailand 538 von den Ostgoten grausam zurückerobert; zudem kam es zu Hungersnöten im Land. 538 war auch Narses, Belisars Konkurrent, mit nur geringen Verstärkungen nach Italien entsandt worden, doch führten Streitigkeiten zwischen den beiden Kommandeuren dazu, dass die Offensive gegen die Goten im Sande verlief und Narses bald darauf nach Konstantinopel zurückkehrte. Auch der Einfall der merowingischen Franken, die unter Theudebert I. 539 in Norditalien einfielen und diese Region gründlich verwüsteten, forderte zahllose Opfer; dabei kämpften die Franken sowohl gegen die Goten als auch gegen die Oströmer, obwohl sie von beiden Seiten vorher als mögliche Verbündete umworben worden waren.

Im Mai 540 fiel das von Belisar belagerte Ravenna. Ostgotische Adlige hatten ihm die Kaiserwürde im Westen angeboten, und Belisar war darauf eingegangen. Witichis wanderte in die Gefangenschaft, wo er wohl 542 im Range eines "patricius" verstarb. Ob Belisar die Kaiserwürde nur zum Schein annahm, ist unklar. Jedenfalls erweckte dies den Argwohn Justinians, der seinen Generälen ohnehin nie recht traute und einen zweiten "Augustus" neben sich nicht geduldet hätte. Sicher ist, dass Belisar seine Kompetenzen überschritt, als er Witichis gefangen nahm, denn Justinian hatte zuvor mit den Ostgoten vereinbart, dass sich diese in Norditalien als Foederaten ansiedeln sollten. Belisar setzte sich über diese Abmachung eigenmächtig hinweg; vielleicht hätte die Geschichte einen anderen Verlauf genommen, hätte er den kaiserlichen Willen befolgt: Ein nördlich des Po gelegenes Gotenreich hätte als Puffer gegen die Invasionen der Langobarden und Franken fungieren können, und zudem wäre Italien wohl die zweite, blutigere Phase des Ostgotenkrieges erspart geblieben.

Justinian begann sogleich, oströmische Beamte nach Italien zu entsenden, die sich offenbar vielfach eher als Herren denn als Befreier gaben: Aufgrund der sehr hohen Steuern, die sie einforderten, kam es in Italien bald darauf zu Aufständen, wobei sich der 542 in Pavia, wo sich die Reste der Ostgoten gesammelt hatten, zum neuen König erhobene Totila (eigentlich "Baduila") als ein kluger Stratege erwies (Propagandakampagne, Bau einer Flotte). Nur kleine Truppenteile wurden Belisar, der 544 wieder das Kommando des italischen Kriegsschauplatzes übernommen hatte, zur Niederschlagung der „Rebellion“ zur Verfügung gestellt, da Justinian seinem besten General nicht mehr recht vertraute und der Großteil der römischen Truppen im Osten benötigt wurde, wo es seit 540 wieder zu Kämpfen mit den Persern gekommen war (siehe oben). Der so genannte "zweite Gotenkrieg" (541/42 bis 552) erwies sich als noch härter als der vorangegangene. Ende 546 fiel Rom an Totila, der es jedoch bald darauf wieder verlor. Die Kämpfe erstreckten sich über ganz Italien und wurden mit großer Grausamkeit geführt. 549 wurde Belisar, dem Prokopios später zahlreiche Versäumnisse vorwarf, abberufen und zunächst 550 durch Germanus, nach dessen plötzlichem Tod durch Narses ersetzt. Totila hatte derweil Ende 549 Rom ein zweites Mal eingenommen, konnte sich dort aber wieder nicht behaupten. Dieser Krieg ruinierte auch die wohlhabende weströmische Senatsaristokratie, die bis dahin ein Träger der antiken Kultur gewesen war. Zum Ende des Jahrhunderts sollte der Senat in seiner bisherigen antiken Tradition dann aus den Quellen verschwinden.

Erst als mit den Persern ein Waffenstillstand geschlossen worden war, konnte Justinian 551 wieder genügend Männer an der Orientfront entbehren, um den lästigen Gotenkrieg zu entscheiden. Narses gelang es mit dieser neuen Armee Anfang Juni 552, das wieder gotische Ravenna zu erobern und bald darauf die Goten unter Totila bei Busta Gallorum entscheidend zu schlagen; Totila fiel dabei, womit das gotische Heer seinen Strategen verloren hatte. Unter ihrem letzten König Teja stellten sich die Goten im Oktober 552 in der Schlacht am Mons Lactarius in Sichtweite des Vesuv noch einmal zum Kampf, den sie aber ebenfalls verloren. Einzelne gotische Garnisonen konnten sich noch einige Jahre halten, der Krieg war damit jedoch entschieden. Wenig später konnte Narses in der Schlacht am Casilinus auch den fränkischen Einfällen ein Ende setzen.

Italien wurde wie zuvor "Africa" wieder einem römischen "praefectus praetorio" unterstellt; das Land jedoch war verwüstet. Die "Pragmatische Sanktion", mit der es von Justinian 554 administrativ wieder ins "Imperium Romanum" eingegliedert wurde, schaffte fast alle Ämter ab, die zuvor von weströmischen Senatoren besetzt worden waren, und trug damit noch zusätzlich zum Verschwinden dieser Aristokratie bei. Italien verlor seinen Sonderstatus und sollte wie eine gewöhnliche Provinz von Konstantinopel aus regiert werden; das alte Kernland des Imperiums stand nun zwar nach Jahrzehnten wieder unter direkter kaiserlicher Herrschaft, hatte aber sein Eigengewicht weitgehend eingebüßt. In gewisser Hinsicht endete erst jetzt, mit der Beseitigung des weströmischen Hofes in Ravenna, auch das Weströmische Reich. Lediglich der Stadt Rom gestand der Kaiser noch einige Privilegien zu, darunter die Wiederaufnahme der kostenlosen Getreidespenden "(annona civica)" an die nunmehr stark dezimierte Bevölkerung.

Zunächst versuchte Narses in kaiserlichem Auftrag, die zerstörte Infrastruktur der Halbinsel wieder aufzubauen. Doch bald nach Justinians Tod fielen die Langobarden 568 in Italien ein – eventuell im Zusammenhang mit einem gescheiterten Versuch des Narses, sie als Foederaten anzusiedeln – und nahmen es zum größeren Teil in Besitz. Als Teile Ostroms verblieben Genua bis 650, die Region um Ravenna (als Exarchat Ravenna) bis 751, Sizilien bis zum 9. Jahrhundert und Teile Süditaliens bis 1071.

In Spanien konnte der bereits hochbetagte weströmische Senator Liberius im Auftrag Justinians 552 infolge innerer Wirren im Westgotenreich dessen südliche Region um Córdoba und Gibraltar für das Imperium in Besitz nehmen. Dieser Raum, der im Wesentlichen der alten Provinz "Baetica" entsprach, wurde von Justinian als "Spania" reorganisiert, blieb knapp 80 Jahre oströmisch und unterstand einem eigenen "magister militum". Die genauen Grenzen des Gebietes sind unklar.

Der Balkan kam während der ganzen Regierungszeit Justinians nicht zur Ruhe. Immer wieder fielen Awaren, Slawen und Hunnen ein, daher wurde mit erheblichem Aufwand das Festungssystem erweitert und erneuert. Unter anderem wurde das vor Jahrzehnten während der Hunnen- und Gotenzüge zerstörte Legionslager Singidunum an der Donau ab 535 als oströmisches "Kastron", das den mittelalterlichen Kern der Stadt Belgrad begründete, neu aufgebaut. Dennoch erwiesen sich diese Maßnahmen als nicht ausreichend, um die Sicherheit der Provinzen Moesia und Thrakien zu gewährleisten: Das Hinterland war immer wieder Plünderungszügen ausgesetzt, da insbesondere die Donaugrenze vernachlässigt wurde. 545 gewannen die Oströmer die Anten als Verbündete, die einen Teil der Donaugrenze fortan sicherten.

548 und 550 drangen slawische Stämme über die Donau erstmals ins Innere der Balkanhalbinsel vor und erreichten den Golf von Korinth, die Adria und die ägäische Küste. Die Infiltration slawischer Stämme sollte sich in den nachfolgenden Jahrzehnten als folgenschweres Ergebnis justinianischer Politik zeigen, die zu einem völlig neuen demografisch-soziologischen Charakter der Balkanhalbinsel führte und dem Reich über Jahrhunderte kostspielige militärische Operationen sowie eine mit der Kurie konkurrierende Missionstätigkeit abverlangte. 559 drangen „hunnische“ Angreifer (wohl Kutriguren) unter ihrem Häuptling Zabergan bis in die Nähe von Konstantinopel vor und bedrohten die Hauptstadt, sie konnten aber vom noch einmal reaktivierten Belisar abgewehrt werden. Dennoch hat erst kürzlich Alexander Sarantis in einer umfassenden Studie die justinianische Balkanpolitik durchaus wieder positiver bewertet, als dies in der Vergangenheit der Fall war; der folgende Verlust des Balkans erscheint hierbei keineswegs unausweichlich.

Einige Jahre nach Justinians Tod fiel 582 die Schlüsselfestung Sirmium (heute Sremska Mitrovica) dem Ansturm der Awaren und unterworfener slawischer Stämme zum Opfer. Obwohl einer seiner Nachfolger, Kaiser Maurikios, mit zahlreichen Feldzügen versuchte, die Lage zu konsolidieren, konnte er die Landnahme der Slawen auf dem Balkan letztlich nur verzögern, denn seine Nachfolger schenkten der Balkanverteidigung nicht die nötige Aufmerksamkeit. Aus der Konsequenz der Völkerwanderungen der Slawen im 6. Jahrhundert und der späteren byzantinischen Mission hatte der byzantinische Kulturkreis seine natürliche Grenze an Drina und Save.

Es gelang Kaiser Justinian, Kontakte mit dem christlichen Reich von Aksum herzustellen (im heutigen Äthiopien, siehe dazu auch Ella Asbeha), wobei die Aksumiten bereits im Jahr 525 im Jemen gegen die Himyaren interveniert hatten, sehr zum Ärger der Sassaniden, die in dieser Region eigene Interessen verfolgten und die Südküste des persischen Golfs bald nach dem Tod des Kaisers eroberten. An der Südgrenze der Provinz Ägypten kam es zudem immer wieder zu Kämpfen mit den Blemmyern. Aus dem Kaiserreich China konnten unter Justinian Seidenraupen eingeführt werden, was die Abhängigkeit von Importen verringerte und zur Entstehung einer eigenen Seidenproduktion führte. Auch in denjenigen Regionen des Mittelmeerraumes, die nicht der direkten Herrschaft Ostroms unterworfen waren, wurde der Vorrang des Kaisers zu dieser Zeit in der Regel anerkannt. Ebenso wie mit den Hunnen kam es auch mit den Franken immer wieder zu Kämpfen, die aber nicht von entscheidender Bedeutung waren (siehe Gotenkriege in Italien).

Justinian galt als ein „schlafloser Kaiser“, der sich um viele Belange persönlich kümmerte. Justinian verließ die Hauptstadt nur sehr selten und war ein wahrer „Innenpolitiker“, wobei er das Glück hatte, nicht nur über kompetente zivile Mitarbeiter (Tribonian, Johannes der Kappadokier), sondern auch über mehrere sehr fähige Generäle zu verfügen (Belisar, Narses, Germanus, Sittas, Mundus, Johannes Troglita), die seine Kriege für ihn führten.

Unter dem sozialen Aufsteiger Justinian erreichte die spätrömische Herrscherideologie ihre höchste Steigerung; er konnte sich zwar nicht völlig von den älteren Wurzeln des Kaisertums lösen, betonte aber stärker als seine Vorgänger, er habe seine Macht direkt von Gott "(ek theou)" erhalten. Allerdings musste er auch Rückschläge wie den unten besprochenen Nika-Aufstand hinnehmen, in dem er seinen Thron nur durch exzessive Gewalt sichern konnte. Seinen Berater, den einflussreichen "praefectus praetorio" Johannes den Kappadokier, ließ er 541 fallen, da dessen Macht vom Kaiserpaar, vor allem von Theodora, als Gefahrenfaktor eingestuft wurde. Ähnlich erging es wenig später Belisar, der durch seine militärischen Siege zum Rivalen des Kaisers zu werden drohte, aber nicht so tief fiel wie Johannes. Es ist bezeichnend, dass Justinian seine Macht niemals mit einem "Augustus" oder "Caesar" teilte und auch keinen Nachfolger designierte.

Justinian sorgte sich auch um die Städte und die Provinzverwaltung sowie – vor allem in der zweiten Hälfte seiner Regierung – um theologische Fragen. Er versuchte durch zahlreiche Gesetze und Verordnungen, die spätrömische Administration des Reiches zu straffen und den aktuellen Erfordernissen anzupassen – nicht immer mit Erfolg, aber mit bemerkenswerter Energie. Die von ihm veranlasste Rechtskompilation war bahnbrechend und sollte bis in die Neuzeit nachwirken. Eine wichtige Quelle für diese letzte Phase der spätantiken Verwaltungsgeschichte stellt dabei das Archiv des Beamten Dioskoros dar, der unter Justinian und seinen Nachfolgern wichtige Posten in Ägypten bekleidete. Noch wichtiger ist die Schrift "De magistratibus" des ehemaligen kaiserlichen Beamten Johannes Lydos, in der dieser Einblicke auch in die höheren Ränge der spätantiken Verwaltung bietet.

Allerdings belasteten die Kriege – vor allem die im Orient – die Staatsfinanzen erheblich. Dies, die ungebremste Bauwut und insbesondere die Folgen der Pestepidemie sorgten für immer höhere Belastungen, was schließlich möglicherweise zur Verelendung von Teilen der Bevölkerung führte. Andererseits erlebten Kleinasien, Ägypten und die nicht von persischen Invasionen betroffenen Gebiete Syriens und Palästinas unter Justinian eine wirtschaftliche Blüte. Hier wahrten die Städte ihren klassisch-antiken Charakter, den sie vor allem südlich der Donau bereits verloren. Inwiefern Justinian die Kräfte des Reiches wirklich überstrapazierte, ist bislang kaum zu sagen und sehr umstritten. Alles in allem konnte die spätrömische Senatsaristokratie im Osten weiter ihr enormes Sozialprestige, ihre klassische Bildung "(paideia)" und ein teilweise gewaltiges Vermögen bewahren (ein Beispiel hierfür ist die hochadlige Anicia Juliana), es kam aber offenbar zu Spannungen zwischen den politisch weitgehend entmachteten Senatoren und dem Kaiser.
Das innenpolitisch markanteste Ereignis in Justinians Regierungszeit war der so genannte Nika-Aufstand in Konstantinopel im Jahre 532, bei dem die rivalisierenden Zirkusparteien der Blauen und Grünen, verärgert durch Justinians Bestrebungen, ihre Macht einzuschränken, sich gegen ihn zusammenschlossen und einen Gegenkaiser, Flavius Hypatius, den Neffen des 518 verstorbenen Kaisers Anastasius, ausriefen. Wahrscheinlich waren auch andere hochrangige Senatoren an der Revolte beteiligt. Während Justinian die Lage als verloren angesehen haben soll, weigerte sich angeblich (nach Prokopios) Justinians Frau, die "Augusta" Theodora, eine ehemalige Schauspielerin, aus der Hauptstadt zu fliehen. Durch Verhandlungen des "praepositus" Narses mit den Aufständischen und vor allem durch Belisars überraschendes Eindringen (mit kaisertreuen Truppen) in den Circus, wo sich die Aufständischen versammelt hatten, konnte der Aufstand blutig niedergeschlagen werden. Hypatius und sein Bruder Pompeius wurden hingerichtet, und auch zahlreiche Aristokraten fanden den Tod. Insgesamt sollen bei den Unruhen etwa 30.000 Menschen den Tod gefunden haben, und Teile der Hauptstadt brannten nieder.

Nach dem Ende des Aufstands blieb es in Konstantinopel jahrelang ruhig; Justinians Herrschaft wurde im Inneren nicht mehr bedroht. Ein Usurpationsversuch des Johannes Cottistis brach 537 binnen Tagen in sich zusammen. Erst in den letzten Jahren Justinians kam es wieder zu Unruhen unter der Bevölkerung.

Seit 541 tobte die sogenannte Justinianische Pest (wahrscheinlich handelte es sich um Beulenpest) im ganzen Reich, an der wohl auch Justinian selbst erkrankte; sein wichtigster Jurist Tribonian verstarb sogar – und mit ihm zahllose andere. Prokopios hat einen erschütternden Bericht über das Wüten der Seuche in Konstantinopel hinterlassen. Die Folgen waren offenbar weitreichend: Es kam zu Hungersnöten, und es entwickelte sich offenbar eine Endzeitstimmung, die durch andere Faktoren wie Kriege und zahlreiche Erdbeben noch verstärkt wurde. Wie schwerwiegend die Auswirkungen der Seuche wirklich waren, ist aber umstritten.

Vielleicht auch als Folge der Katastrophen (siehe auch die Hypothese zu der Wetteranomalie von 535/536) wandte sich Justinian nun verstärkt theologischen Fragen zu. Es kam zu einer gewissen Zäsur in seiner Regierungszeit; seine Politik war, auch bedingt durch die Rückschläge in den Kriegen, alles in allem weniger dynamisch als zu Beginn. Insgesamt können die enormen Menschenverluste durch die Pest wohl als einer der wichtigsten Einzelfaktoren für den Untergang der antiken Zivilisation gesehen werden.

Eine der größten und langfristig wichtigsten Leistungen Justinians war zweifellos die Kodifikation des römischen Rechts. Bereits 529 wurde der aus früheren privaten und öffentlichen Sammlungen kompilierte "Codex Iustinianus" veröffentlicht, 533 erschienen die "Digesten" (auch "Pandekten" genannt), eine Sammlung von Schriften klassischer römischer Juristen, die die zweite Gruppe geltenden Rechts neben den kaiserlichen Gesetzen darstellten, wobei vor allem Tribonian großen Anteil am Erfolg hatte. Im selben Jahr wurden auch die "Institutionen" veröffentlicht, eine Art juristisches Lehrbuch. Ende 534 wurde dann die zweite, endgültige Version des "Codex Iustinianus" vorgelegt. Den Abschluss dieses "Corpus Iuris Civilis" bildete eine "Novellensammlung," in der die nach Erscheinen des Codex veröffentlichten Verordnungen Aufnahme fanden. Der "Codex" war noch vollständig auf Latein verfasst; die Novellen wurden hingegen auch auf Griechisch publiziert (es existierte aber nach Ansicht der neuesten Forschung stets zumindest auch eine lateinische Version).

Die Wirkung des (erst im Mittelalter so genannten) "Corpus Iuris" war weitreichend: Im 12. Jahrhundert wurde das Corpus an der Rechtsschule von Bologna rezipiert und bildete mit das Grundgerüst für die Programmatik der Staufer, die sich an die spätantike Kaiseridee anlehnten. Am Ende des Mittelalters galt es als allgemein anerkanntes Recht und beeinflusst bis heute auch die Gesetzgebung und die Lehre im Fach Jura.

Justinian entfaltete eine rege Bautätigkeit. So ließ er unter anderem die alte Kirche der "Heiligen Weisheit", die Hagia Sophia, in Konstantinopel nach ihrer Zerstörung im Nika-Aufstand durch einen prachtvollen Neubau ersetzen, dessen gewaltige Kuppel nach einem Erdbeben noch vor Justinians Tod nochmals erneuert werden musste. Dieser weltberühmte Bau, der maßgeblich von den Architekten Anthemios von Tralleis und Isidor von Milet geprägt war, gilt als das letzte Meisterwerk der spätantiken Architektur. Justinians Hagia Sophia war sieben Jahrhunderte lang die größte Kirche überhaupt; die Größe der Kuppel wurde erst nach über einem Jahrtausend vom Petersdom überboten. Seit 641 wurden dort die Kaiser gekrönt, 1453 wurde sie zur Moschee. Bis heute ist sie das Wahrzeichen Istanbuls. Im Zuge der Neugestaltung Konstantinopels nach dem Nika-Aufstand ließ der Kaiser auch eine Siegessäule mit seiner Reiterstatue auf dem "Augusteum" (südwestlich der Hagia Sophia) errichten, die so genannte Justinian-Säule; das heute verlorene Reiterstandbild war bereits 530 nach dem Sieg bei Dara entstanden, wurde nun aber neu platziert.

Auch Antiochia am Orontes wurde nach einem schweren Erdbeben und der Eroberung durch die Sassaniden 540 wieder aufgebaut. In Ephesos ließ Justinian unter anderem eine prächtige Kirche errichten, auch das berühmte Sinaikloster geht auf ihn zurück. Die Zahl der großen und kleinen Städte im Oströmischen Reich wird zu seiner Zeit auf etwa 900 geschätzt, und besonders in den Provinzhauptstädten entfaltete sich noch einmal eine zum Teil rege Bau- und Renovierungstätigkeit. Justinian regelte per Gesetz, welcher Anteil an den Steuern den "poleis" zukommen sollte, um den Unterhalt der öffentlichen Bauten (Theater, Bäder etc.) zu gewährleisten. Die Krisen, die das Reich seit 540 trafen, ließen die kaiserliche Politik zur Förderung der Städte aber letztlich scheitern: Sie verloren in der Folgezeit endgültig ihren antiken Charakter.

Das römische Festungssystem wurde vor allem an der Donau stark erweitert, insbesondere wurde Singidunum, das heutige Belgrad, durch eine neue Burg befestigt, hielt jedoch langfristig dem Ansturm der Slawen bzw. der Awaren nicht stand, die einige Jahre nach Justinians Tod, 582, die Schlüsselstadt Sirmium an der Save einnehmen sollten. Auch an der Grenze zum Sassanidenreich wurden insbesondere nach 540 mehrere neue Befestigungen errichtet. Über die Baumaßnahmen, die Narses nach dem Sieg über die Ostgoten in Italien durchführte, sind wir hingegen nur lückenhaft informiert (z. B. ILS 832).

Unter den profanen Ingenieursbauten ist die monumentale Sangariusbrücke in Bithynien zu nennen, deren Bau vom Kaiser aus strategischen Gesichtspunkten veranlasst wurde. Des Weiteren wurde auf kaiserliche Anweisung hin die Stadt Justiniana Prima (530–615) im heutigen Serbien als neue Bischofsstadt prächtig ausgebaut; entweder handelt es sich dabei um seinen Heimatort oder, wahrscheinlicher, um eine in der Nähe liegende Ortschaft; ihr moderner Name lautet "Caricin Grad" (serb. = Kaiserinnenstadt). Die Anlage ist klein, aber dennoch imposant. Laut einer heute im Pergamonmuseum in Berlin befindlichen Inschrift ließ der Kaiser zudem 538 das berühmte Markttor von Milet erneuern; dass er zugleich eine neue Stadtmauer errichten ließ, die nur noch einen Bruchteil des Stadtgebietes einschloss, gilt hingegen heute als Irrtum der älteren Forschung – die Befestigung entstand erst im 7. Jahrhundert.

Die umfassenden Bautätigkeiten Justinians, die Prokopios in einem eigenen Werk feierte, konnten jedoch nur durch reichlich fließende Steuern finanziert werden. Die hohe fiskalische Belastung war vielleicht ein Auslöser für den Nika-Aufstand 532, doch ist dies fraglich, da der Höhepunkt der Bautätigkeit des Kaisers erst in spätere Jahre fiel.

In der christlichen Kirche seiner Zeit spielte Justinian eine dominierende Rolle. Er verfasste angeblich selbst theologische Traktate und leitete Kirchenversammlungen. Das Zusammenspiel (die "Symphonia") von spätantikem Staat und christlicher Kirche erreichte in dieser Zeit seinen Höhepunkt; der Kaiser beanspruchte, seine Herrschaft direkt von Gott "(ek theou)" erhalten zu haben. Justinian ging auch entschlossen gegen die verbliebenen Nichtchristen im Reich vor, vor allem im südlichen Ägypten. Eifrig um Christianisierung bemüht, ließ der Kaiser 529 zudem die neuplatonische Philosophenschule in Athen, einen Hort religiös geprägter paganer Philosophie, schließen – vermutlich um damit den Einfluss des Heidentums auf Wissenschaft und Bildung zurückzudrängen, vielleicht auch nur, um ein Zeichen zu setzen. Sieben heidnische Philosophen (darunter Damaskios und Simplikios) übersiedelten daraufhin 531 kurzzeitig nach Persien, kehrten aber schon 532 wieder in das Imperium zurück. Zwar sollen noch in den 540er Jahren 80.000 kleinasiatische „Heiden“ bzw. „Hellenen“ getauft und ihre Tempel zerstört worden sein, doch insgesamt dürfte die Zahl der Anhänger der alten Religion inzwischen eher gering gewesen sein. Es gab allerdings noch immer einige bedeutende pagane „Inseln“ im christlichen Reich, zum Beispiel die Stadt Carrhae in der heutigen Türkei oder das syrische Baalbek. Der berühmte Isis-Tempel von Philae in Ägypten, bis dahin das letzte offiziell geduldete pagane Heiligtum im Imperium, wurde um 536 durch kaiserliche Truppen geschlossen. Wie stark die vorchristlichen Kulte unter Justinian noch waren, lässt sich kaum abschließend beurteilen. Der Vorwurf des heimlichen Heidentums entwickelte sich zu einem beliebten Instrument, um unliebsame Angehörige der Oberschicht zu belasten.

Justinian ordnete 545/6 die Verfolgung nichtchristlicher Grammatiker, Rhetoren, Ärzte und Juristen an und ließ im Jahre 562 heidnische Bücher öffentlich verbrennen. Die Kindstaufe wurde zwangseingeführt, die Nichtbeachtung mit dem Verlust von Eigentum und Bürgerrecht bestraft, das Festhalten am „hellenischen“ Glauben bzw. die Apostasie nach der Taufe mit der Todesstrafe. Dies war ein entscheidender Schritt, da nun praktisch jeder Reichsbewohner bereits als Kind getauft wurde und ein Abfall vom Christentum als grundsätzlich todeswürdiges Verbrechen galt. Besonders die Manichäer wurden nicht nur von Justinian, sondern auch in Persien schwer verfolgt und mussten in der Folge nach Indien und China auswandern. Die Rechtslage der Juden verschlechterte sich, doch wurde ihre Religion als einzige neben dem Christentum weiterhin offiziell geduldet.

Einen guten Überblick bezüglich der eschatologischen Erwartungen im „Zeitalter Justinians“ (z. B. im Hinblick auf die Pestepidemie und mehrere Naturkatastrophen) gibt Mischa Meier, "Das andere Zeitalter Justinians". Inwiefern die enttäuschten Parusieerwartungen der Jahre um 500 aber tatsächlich auch für die Zeit um 540 von Bedeutung waren und ob die Quellen, die Meier anführt, wirklich repräsentativ sind, bedarf durchaus noch der weiteren Diskussion.

In der Frage innerkirchlicher Häresien scheiterten Justinians Ausgleichsbemühungen; seine Verurteilung der monophysitischen (miaphysitischen) Lehre, welcher unter anderem selbst Kaiserin Theodora folgte, verschärfte nur die schon existierenden Spannungen zwischen den monophysitischen Kirchen Syriens und Ägyptens und der antimonophysitisch bzw. chalcedonensisch eingestellten römischen und konstantinopolitanischen Kirche.

Justinians harte Religionspolitik führte im Sommer 529 zu einem Aufstand der Samaritaner, einer Splittergruppe des Judentums, in Palästina, der blutig niedergeschlagen wurde (zu den Hintergründen siehe Julian ben Sabar). Überlebende wurden zwangschristianisiert. Die Montanisten, Christen mit abweichenden Endzeiterwartungen, begingen kollektiven Selbstmord, indem sie sich in ihre Kirchen einschlossen und diese anzündeten, ihr Schrifttum ging verloren. Justinian persönlich galt als sehr fromm und als überzeugter Anhänger der orthodoxen Kirche, der auch als Kaiser die strengen Fastenzeiten strikt einhielt. Der angeblich von Justinian selbst verfasste Hymnus, „O einzig-gezeugter Sohn und Wort Gottes“ gehört bis heute zur Liturgie der orthodoxen Kirche. Von Bedeutung war auch, dass Justinian die Verehrung des heiligen Nikolaus von Myra förderte, dem um 550 eine Kirche in Konstantinopel geweiht wurde und dessen Kult in der Folgezeit große Popularität gewann.

Justinian ließ im Januar 543 einen Erlass gegen Origenes (185–254) veröffentlichen, der auch neun doktrinale Anathematismen beinhaltete, welche die Lehre des Origenes zum Inhalt hatten; ein zehnter Anathematismus zielte auf die Person des Origenes, dessen Lehren bereits seit Jahrhunderten umstritten waren. Von der ständigen Synode wurde der Erlass kurz darauf bestätigt.

Fast gleichzeitig brach der erbittert geführte "Dreikapitelstreit" aus; beide Konflikte wurden teils zeitgleich geführt, inhaltlich hatten sie jedoch keine Gemeinsamkeit. Im Dreikapitelstreit ging es um die Schriften dreier christlicher Autoren aus dem 5. Jahrhundert, die im Verdacht standen, dem bereits 431 verworfenen Nestorianismus anzuhängen. Namentlich waren dies Ibas von Edessa, Theodor von Mopsuestia und der Kirchenhistoriker Theodoret. Auch gegen sie ließ Justinian 544/45 eine Schrift verfassen, wogegen sich auch in den Reihen der Patriarchen erheblicher Widerstand formierte; selbst der römische Bischof Vigilius, welcher der Schrift zuerst zögernd zugestimmt hatte, musste seine Zustimmung auf Druck mehrerer westlicher Kirchen (unter anderem der von "Africa") wieder zurückziehen. 546 ließ ihn Justinian in Rom festnehmen und nach Konstantinopel bringen. 548 stimmte Vigilius in der Schrift "Iudicatum" nochmals der kaiserlichen Position zu, nur um angesichts des massiven Widerstands der nordafrikanischen Christen erneut umzuschwenken.

Der Kaiser berief daher 553 das zweite Konzil von Konstantinopel ein, das als das "Fünfte Ökumenische Konzil" (das letzte der Spätantike) in die Geschichte einging. Auch hier kam die Kontroverse um Origenes und um den Dreikapitelstreit noch einmal zur Sprache; Justinian ließ keinen Zweifel daran, dass er eine Verurteilung der drei Autoren wünschte, und setzte zu diesem Zweck auch Vigilius massiv unter Druck. Dieser stimmte den Beschlüssen des Konzils schließlich zu, womit sie ökumenischen Rang erhielten. Ein Ausgleich mit den Monophysiten konnte aber nicht erreicht werden, und trotz der Anerkennung der Konzilsbeschlüsse durch Vigilius, der auf dem Rückweg nach Rom verstarb, stießen sie im Westen noch lange auf Widerstand. Kurz vor seinem Tod entfernte sich der Kaiser dann durch die Propagierung des "Aphthartodoketismus" selbst wieder von der Orthodoxie.

In theologischen Fragen näherte sich Ostrom unter Justinian bereits erkennbar dem byzantinischen Mittelalter an. Teils wird Justinian vorgeworfen, zu einer Verhärtung der Fronten beispielsweise in der Auseinandersetzung mit den Monophysiten beigetragen und somit indirekt die Kraft des Reiches geschwächt zu haben. Der Kaiser selbst, der eine enge Verknüpfung von Kaisertum und Kirche anstrebte, wollte wohl eher das Reich durch eine gemeinsame Religion bzw. Konfession stärken – und wie für die Spätantike typisch war dabei die Frage nach dem „richtigen“ Dogma von entscheidender Bedeutung, da sonst Verdammung statt Erlösung drohte.

In der orthodoxen Kirche werden Justinian und auch seine Frau Theodora I. (obwohl sie sich für den Monophysitismus einsetzte) als Heilige verehrt. Justinians Gedenktag im Kalender der Lutherischen Kirche-Missouri-Synode und der Orthodoxen Kirche ist sein vermutlicher Todestag, der 14. November.

Justinian ist bis in die jüngste Vergangenheit hinein oft als eine leuchtende Herrscherfigur der Spätantike gefeiert worden, und fraglos zählt er neben Diokletian, Konstantin und Theodosius I. zu den bedeutendsten spätrömischen Kaisern. Dennoch ist eine grundsätzliche Bewertung schwierig.

Unter Justinian wurden die letzten Reste der alten römischen Volkssouveränität (die allerdings schon lange nur mehr auf dem Papier existierte) beseitigt und durch ein konsequentes Gottesgnadentum ersetzt. Allerdings blieb die schweigende Zustimmung (das "silentium") der Vertreter von Volk und Heer auch unter Justinian unverzichtbare Legitimation der kaiserlichen Herrschaft. Was die außenpolitischen Erfolge anging, so waren diese von sehr unterschiedlicher Nachhaltigkeit: "Africa" blieb 160, "Spania" immerhin 70 Jahre römisch; in Italien gingen die zurückeroberten Gebiete dagegen bereits ab 568 zu großen Teilen wieder verloren. Im Osten musste das Reich um das nackte Überleben kämpfen und sich den Frieden teuer erkaufen, wobei der oft erhobene Vorwurf, Justinian habe die Perserfront vernachlässigt, um im Westen tätig zu sein, allerdings auf sehr schwachen Argumenten beruht: Im Gegenteil, die Hauptmacht der römischen Truppen widmete sich vor 532 und nach 540 der Abwehr der sassanidischen Angriffe.

Die noch immer zahlreichen Städte des oströmischen Reiches scheinen zumindest bis zur Pest ebenfalls bis zu einem gewissen Grad floriert zu haben, und auch ländliche Regionen in Ägypten, Syrien und Kleinasien prosperierten. Allerdings erwies sich die Pestepidemie ab 541 als verheerend. Große Teile des Reiches waren betroffen; die finanzielle Kraft wurde dadurch ebenfalls in Mitleidenschaft gezogen und das militärische Potenzial des Imperiums verringert. Allerdings trug der Kaiser gerade an dieser Katastrophe, deren Folgen schwer abzuschätzen sind, keine Schuld. Seine Regierungszeit war für die Bevölkerung dennoch zweifellos teils mit schweren Lasten verbunden und von einer zunehmend intoleranten Religionspolitik geprägt (das Ziel, das Reich religiös entsprechend dem Konzil von Chalcedon zu einen, erreichte Justinian damit allerdings so wenig wie seine Vorgänger).

Kritik am Kaiser hatten bereits Zeitgenossen geübt, hier vor allem Prokopios in seiner "Geheimgeschichte." Dabei ist nach wie vor die Frage zu klären, ob Justinians Politik sich tatsächlich wesentlich von der seiner Vorgänger unterscheidet und ob sein Agieren nicht in vielem nur als Pragmatismus zu erklären ist. Das Bild, das Prokopios von Justinian entwirft, ist von tiefem Hass gegen den Kaiser erfüllt:

Im Bereich der Jurisprudenz war Justinian bahnbrechend, während die spätantike Kultur unter ihm noch einmal eine letzte Blüte erlebte: Prokopios von Caesarea, Agathias, Simplikios und Corippus verfassten bedeutende Werke in klassischer Tradition. Zugleich waren andere Entwicklungen zukunftsweisend: die Kirchengeschichte des Euagrios Scholastikos, die Weltchronik des Johannes Malalas und die – nur teilweise überlieferten – Werke des Johannes von Ephesos, die bald nach Justinians Tod entstanden.

Eine gewisse Zäsur ist wohl in den 540er-Jahren zu sehen. War die Zeit vorher von Dynamik gekennzeichnet (Rechtskodifikation, Bautätigkeit, Restaurationspolitik), folgte nun eine Phase nachlassender Aktivität, auch bedingt durch die Katastrophen der Pest und die andauernden Kriege im Westen und Osten, und der Kaiser wandte sich verstärkt der Religionspolitik zu – zumindest ist dies die Kernthese der vielbeachteten Monografie von Mischa Meier "(Das andere Zeitalter Justinians)". Allerdings gelangen drei spektakuläre außenpolitische Erfolge – der Sieg über die Ostgoten, die Eroberung von Teilen Spaniens und der Friedensschluss mit Persien – noch am Anfang der 550er bzw. 560er Jahre, so dass man den Einschnitt der Jahre um 542 vielleicht auch nicht überbewerten sollte. Ostrom war am Ende von Justinians Regierungszeit zweifellos wieder die Vormacht im Mittelmeer, ganz nach dem antiken Reichsideal, allerdings erkauft mit hohen Opfern.

In der Zeit Justinians wurde in vielen Bereichen der Weg für das Byzantinische Reich bereitet, auch wenn dies noch ein langer Prozess sein sollte. Insbesondere zu Beginn seiner Regierung war das Reich noch klar römisch, zum Ende hin ist eine Zunahme der „byzantinischen“ Züge (gerade im religiösen Bereich) zu erkennen. Die Verwaltung des Reiches hielt unter Justinian allerdings zumeist noch an der typisch spätantiken Teilung von militärischer und ziviler Gewalt fest, und auch die zunehmende Verdrängung der lateinischen Sprache aus Verwaltung, Militär und Gesellschaft war bei Justinians Tod noch nicht abgeschlossen. Ein Wechsel deutete sich bereits an, doch der endgültige Bruch mit den antiken Traditionen erfolgte erst im Verlauf des siebten Jahrhunderts.

Die Frage, inwiefern der Kaiser für die Rückschläge und Katastrophen, die das Reich in seinen späteren Jahren und nach seinem Tod trafen, tatsächlich persönlich verantwortlich war, kann durchaus unterschiedlich beantwortet werden. Vielleicht sollte man den faktischen Handlungsspielraum eines spätantiken Herrschers nicht überschätzen und Justinian als einen innerhalb dieses Rahmens ungewöhnlich engagierten und fähigen Monarchen betrachten – als den letzten römischen Kaiser, der diesen Namen wirklich mit Recht trug und der das "Imperium Romanum" noch einmal zur Vormacht der Mittelmeerwelt machte. Zwar darf man auch den sukzessiven Zusammenbruch der justinianischen Ordnung bald nach seinem Tod nicht außer Acht lassen, doch Forscher wie Chris Wickham haben jüngst betont, dass der Kaiser am Ende seines Lebens eine beachtliche Bilanz hinterlassen habe, während die Annahme, seine Politik habe die Kräfte des Imperiums entscheidend überfordert, kaum zu beweisen sei und sich nur aus der Rückschau ergebe. Der Ausgang der Diskussion ist offen.

Die wichtigste Quelle zur Regierungszeit Justinians I. stellen die Werke des Prokopios von Caesarea dar. Dieser schildert in seinen "Historien" ausführlich die Kriege Justinians, in den "Bauwerken" die Baupolitik des Kaisers. Das Bild des Kaisers in der Nachwelt ist stark von Prokop geprägt, dessen Werke sich auf einem hohen Niveau bewegen. Prokop übte durchaus Kritik am Kaiser, dies aber recht subtil; es ist teilweise nicht immer leicht, die genaue Intention Prokops von der eigentlichen Schilderung zu trennen. Noch problematischer ist Prokops "Geheimgeschichte"; sie ist mit äußerster Vorsicht zu lesen, da in dieser stark polemisiert wird. An Prokop schloss Agathias an, ohne jedoch dessen Niveau zu erreichen. Des Weiteren sei unter anderem auf Menander Protektor (dessen Werk nur fragmentarisch erhalten ist, aber wichtige Informationen bietet), Johannes Malalas, den Kirchenhistoriker Euagrios Scholastikos und die diversen Chroniken hingewiesen (z. B. des Victor von Tunnuna). Eine wichtige Quelle ist das so genannte "Corpus Iuris Civilis", die Gesetzessammlung des Kaisers, zumal vor allem in den Vorreden Justinians Herrschaftsauffassung greifbar wird. Hinzu kommen Inschriften, Münzen und archäologische Funde.


Überblickswerke

Biographien

Spezielle Literatur





</doc>
<doc id="11887" url="https://de.wikipedia.org/wiki?curid=11887" title="Josef Kentenich">
Josef Kentenich

Pater Josef (Joseph) Kentenich SAC (* 16. November 1885 in Gymnich bei Köln; † 15. September 1968 in Schönstatt) war Pater in der Gesellschaft der Pallottiner, einer Gesellschaft apostolischen Lebens, und ist Gründer der internationalen Schönstatt-Bewegung.

Kentenich wurde in Gymnich bei Köln als uneheliches Kind von Katharina Kentenich und Matthias Josef Köp geboren und auf den Namen "Peter Josef" getauft.

Vom 23. September 1899 bis 1904 besuchte er das Pallottiner-Gymnasium in Koblenz-Ehrenbreitstein. Nach dem zweijährigen Noviziat (24. September 1904–1906) studierte er von 1906 bis 1911 im Missionshaus der Pallottiner in Limburg a.d. Lahn katholische Theologie. Am 8. Juli 1910 empfing er in Limburg die Priesterweihe. Am 18. September 1911 wurde er Lehrer in Ehrenbreitstein. Von 1912 bis 1919 wirkte er als Spiritual (geistlicher Begleiter) am Studienkolleg der Pallottiner in Vallendar am Rhein.

Mathias Köp, der Vater von Joseph Kentenich, wurde am 9. Dezember 1847 in Eggersheim im Haus Bachstraße 9 geboren. Er starb am 19. März 1931 im Haus Kurfürstenstr. 7. Köp hatte nie geheiratet und starb ledig. Nach dem Lagerbuch zum Friedhof starb er an Altersschwäche und wurde in Grab Nr. 119 beerdigt. Das Grab wurde in den 1970er Jahren eingeebnet.

Köp war Verwalter auf dem Heuserhof im damaligen Alt-Oberbolheim. Dort hatte er auch die Mutter von Josef Kentenich, Katharina Kentenich, kennengelernt. Unbelegt ist, dass Josef Kentenich seinen Vater 1910 in Eggersheim besucht hat, und zwar ohne seine Mutter.

Am 19. April 1914 gründete er dort zusammen mit einigen der ihm anvertrauten Studenten eine Marianische Kongregation, aus der das heutige Schönstattwerk hervorging. Der 18. Oktober 1914 gilt als Gründungstag, an dem er in der Michaelskapelle einen Vortrag vor seinen Studenten hielt. Während des Ersten Weltkriegs erweiterte sich die junge Schönstattbewegung um Mitglieder außerhalb des Vallendarer Kollegs. 1919 wurde der "Apostolische Bund", 1920 die "Apostolische Liga" gegründet. Der "Apostolische Bund" war ein Sammelbecken für Menschen – ob Schüler, Theologiestudenten, Priester, Frauen, Männer, Akademiker, Lehrer oder wer auch immer – die sich von Schönstatt aus zum apostolischen Engagement motivieren und befähigen lassen wollten. Am 8. Dezember 1920 wurden die ersten Frauen offiziell in den "Apostolischen Bund" aufgenommen. Aus diesem Bund heraus bildeten sich verschiedene neue Gemeinschaften. Am 1. Oktober 1926 gründete Kentenich die Schönstätter Marienschwestern als erstes Säkularinstitut.

Bis 1951 gehörte Kentenich der Hausgemeinschaft des 1928 errichteten Bundesheims (heute "Pallotti-Haus") an.

Aufgrund seiner strikt ablehnenden Haltung dem Nationalsozialismus gegenüber („Ich finde keine Stelle an ihm, wo das Taufwasser auftreffen könnte“) wurde Kentenich am 20. September 1941 in Koblenz von der Gestapo verhaftet. Am 13. März 1942 wurde er in das Konzentrationslager Dachau überstellt, wo er bis zum 6. April 1945 inhaftiert war. Am 16. Juli 1942 gründete er dort zwei Schönstattgemeinschaften: das " Schönstatt-Institut Marienbrüder" und – stellvertretend für das Schönstatt-Familienwerk – das "Institut der Schönstattfamilien". 

Am 18. Oktober 1945 gründete er den Verband der Diözesanpriester, am 2. Februar 1946 das Institut der Frauen von Schönstatt.
Am 16. November 1945 begann er seine Auslandsreisen in die Schweiz, nach Italien, Lateinamerika, Südafrika und die USA.

Nach einer bischöflichen Visitation des 1948 kirchenrechtlich anerkannten Säkularinstituts der Schönstätter Marienschwestern durch den Trierer Weihbischof Bernhard Stein am 19. Februar 1949 kritisierte Kentenich den Visitationsbericht auf sehr direkte Weise in einem Brief vom 31. Mai 1949 („Epistola perlonga“) und provozierte damit einen Konflikt mit der Bischofskirche und deren „mechanistischem Denken“. In der Folge erlebte die Bewegung eine lange kirchenamtliche Prüfung, deren Höhepunkt eine päpstliche Visitation durch das Heilige Offizium bildete.

Infolgedessen wurde Kentenich durch eine Verfügung des päpstlichen Visitators am 30. September 1951 aus Europa verbannt und verbrachte die Zeit ab dem 21. Juni 1952 in Milwaukee in den USA als Seelsorger für Deutsche. Da er sich nicht als Ankläger oder Bekämpfer, sondern als Erneuerer und Freund der Kirche sah, befolgte Kentenich alle Auflagen. Im September 1965 erhielt Kentenich ein (anonymes) Telegramm mit einer Einladung in das Generalat der Pallottiner in Rom, wo er am 17. September 1965 eintraf. Im Dezember wurde er in einer Audienz von Papst Paul VI. rehabilitiert. Am Heiligen Abend 1965 kehrte er – über achtzigjährig – nach Schönstatt zurück, wo er sich noch drei Jahre lang der Leitung und Festigung seines weltweiten Werkes widmete. Infolge der Trennung der Schönstattbewegung von den Pallottinern bat Josef Kentenich um Aufnahme in den Diözesanklerus der Diözese Münster. Am 16. November 1965 wurde er von Bischof Josef Höffner in die Diözese Münster inkardiniert. Am 15. September 1968, dem Fest der Sieben Schmerzen Mariens, feierte Pater Kentenich zum ersten und zugleich letzten Mal die Heilige Messe in der neuerbauten Anbetungskirche auf Berg Schönstatt und verstarb unmittelbar nach der Messe völlig überraschend in der Sakristei der Kirche.

Diese ehemalige Sakristei ist heute die Grabkapelle von Pater Kentenich. Sein Sarkophag trägt die lateinische Inschrift („Er liebte die Kirche“).

Josef Kentenich verstand sich als Pädagoge und hat sein pädagogisches Tun auch in hohem Grad reflektiert. Die von ihm gegründete Bewegung wollte er als pädagogische Bewegung verstanden wissen, nicht nur als spirituelle und apostolische. Die Pädagogik Kentenichs ist Frucht seines neuen Menschen-, Gemeinschafts- und Gottesbildes. Es ist eine ausgesprochen menschen- und seelenfreundliche Pädagogik. Sie zielt auf die Freiheit und Vollentfaltung des Menschen (Idealpädagogik). Sie hat die geistige und seelische Bindungsfähigkeit und Bindungsfülle des Menschen im Blick (Bindungspädagogik). Durchgehend vertritt Kentenich ein Gottesbild, wonach Gott der Erzieher des Menschen ist. Auch sein Marienbild ist davon geprägt. Maria ist die Erzieherin schlechthin (Bündnispädagogik). P. Kentenich gilt bis heute auch in nichtkirchlichen Kreisen als großer Pädagoge und Psychologe, er ist auch der Entwickler einer eigenen Pädagogik-Linie, der Kentenich-Pädagogik.





</doc>
<doc id="11890" url="https://de.wikipedia.org/wiki?curid=11890" title="Stadtrecht">
Stadtrecht

Stadtrecht ist ursprünglich das kaiserliche oder landesherrliche Vorrecht ("Stadtregal"), durch das ein Dorf oder eine vorstädtische Siedlung zur Stadt erhoben wurde und Inbegriff der in dem betreffenden Rechtsbezirk gültigen Rechtssätze war. Im Gegensatz dazu wurde das Landrecht zumeist von der Landesherrschaft festgelegt. Das Stadtrecht ist kein einheitliches „Stadtgesetz“, sondern besteht aus mehreren Privilegien (Niederlagsrecht, Zölle) und Einzelrechten, von denen meist das Marktrecht das älteste ist. Als Minderstadt werden Orte mit eingeschränktem Stadtrecht bezeichnet.

Das im mitteleuropäischen Raum übliche Stadtrecht geht vermutlich ursprünglich auf italienische Vorbilder zurück, die ihrerseits an den Traditionen der Selbstverwaltung der römischen Städte ausgerichtet waren.

Im heutigen deutschsprachigen Raum gibt es kein Stadtrecht mehr im eigentlichen Sinne, d. h. die Selbstverwaltung in den Städten regeln staatliche Grundsätze bzw. Gesetze der Bundesländer (vgl. Gemeindeordnung). Die Stadtrechtsverleihung, d. h. die Erhebung einer Gemeinde zur Stadt, wird in Deutschland heute ebenfalls von den Ländern ausgeübt und beschränkt sich auf das Recht, die Bezeichnung „Stadt“ zu führen. Status und Zuständigkeit einer Stadt sind vielmehr an ihre Einwohnerzahl geknüpft, nicht an die Bezeichnung als Gemeinde oder Stadt. So gibt es z. B. in Niedersachsen Städte und Gemeinden, die als sogenannte „selbständige Gemeinden“ mit erweiterten Kompetenzen ausgestattet sind, während andererseits Städte als Mitgliedsgemeinden einer Samtgemeinde ihre Aufgaben praktisch vollständig an den Gemeindeverbund abgegeben haben.

 
Die Bedeutung des deutschen Stadtrechts im Heiligen Römischen Reich Deutscher Nation rechtfertigt eine Hervorhebung der deutschen Stadtrechtstradition. Das deutsche Stadtrecht verschaffte den Städten im internationalen Vergleich besondere städtische Autonomie. Es stand im Mittelalter im Zusammenhang mit der Deutschen Ostsiedlung und war nicht zuletzt für Stadt(neu)gründungen im osteuropäischen Raum vorbildlich.

Stadtrechte entstanden in Deutschland seit dem 10. Jahrhundert. Durch sie wurden nicht nur Privatrechtsverhältnisse, sondern auch Gegenstände des öffentlichen Rechts normiert.

Oft wurde das Recht einer Stadt mehr oder minder vollständig von anderen rezipiert; so die Stadtrechte von Soest (dem ersten im deutschen Raum nachweislich aufgezeichneten Stadtrecht), ganz besonders aber die Stadtrechte von Magdeburg, Lübeck und Köln. Die Gemeinschaft der Städte, die das Recht einer Stadt übernommen bzw. durch den Stadtherrn übertragen erhalten hatten, wird als deren "Stadtrechtsfamilie" bezeichnet.

Das Lübische Stadtrecht wurde 1160 aus dem Soester Recht abgeleitet. Es gewann – bedingt durch die Vormachtstellung von Lübeck in der Hanse – die Küstenstriche von Schleswig bis zu den östlichsten deutschen Ansiedlungen an der Ostsee.

Das Magdeburger Recht verbreitete sich in den Binnenlanden bis nach Böhmen, Schlesien, die heutige Slowakei (u. A. in die Zips) und Polen hinein und als Kulmer Recht über das Deutschordensland Preußen. In Polen war das Magdeburger Stadtrecht das allgemein verbindliche.
Das Stadtrecht spielte eine wichtige Rolle bei der Deutschen Ostsiedlung im Mittelalter: Kolonisten wurden unter der Voraussetzung angeworben (oder siedelten eigenständig), dass sie in den von ihnen gegründeten Orten ihr eigenes Recht behalten konnten. Das Stadtrecht war zunächst im Kern ein Marktrecht, ergänzt durch städtische Gerichtsbarkeit und Befestigungs­recht. Erst später wurden die Stadtrechte auch von Städten übernommen, deren Bevölkerung nicht deutschsprachig (Ostpolen, Litauen, westliches Russland) oder nicht mehr deutschsprachig war (Böhmen, Mähren u. ä.).

Aus dem Magdeburger Recht leitete sich das Brandenburger Stadtrecht in der Mark Brandenburg, in Pommern und im südlichen Mecklenburg ab.

Auch das Bremer Stadtrecht und das Salzwedeler Stadtrecht wurden von anderen Orten übernommen.

Die Übernahme eines Stadtrechts bedeutete in der Regel die Anerkennung der abgebenden Stadt als Rechtsvorort; z. B. war Magdeburg Rechtsvorort für die Städte mit Magdeburger Recht. Der dortige Schöffen­stuhl entschied damit über Rechtsunklarheiten in den mit dem Magdeburger Recht beliehenen Städten. So ist es auch zu erklären, dass bestimmte Stadtrechte unter verschiedenen Namen bekannt sind, obwohl sie ursprünglich aus derselben Quelle stammen: Der Name kennzeichnet dann nicht die ursprüngliche Rechtsherkunft, sondern den anerkannten Rechtsvorort.

Vom heutigen Standpunkt aus ist bemerkenswert, dass eine geschlossene Ansiedlung durchaus in verschiedene Stadtrechtsgebiete aufgeteilt sein konnte. Zahlreiche heutige deutsche Städte sind aus solchen Ansiedlungen entstanden, die im Rechtssinne ursprünglich mehrere Städte umfassten (z. B. Hildesheim, Braunschweig, Kassel).

Infolge der Umgestaltung der Territorialverhältnisse sowie der Rechtsbegriffe wurden Änderungen der Stadtrechte notwendig. So entstanden im Lauf des 15., 16. und 17. Jahrhunderts an vielen Orten "verbesserte Stadtrechte", sogenannte „Reformationen“, wobei aber unter Einwirkung der Rechtsgelehrten mehr und mehr römisches Recht eingemischt wurde, in Hamburg z. B. unter Bürgermeister Hermann Langenbeck. Zuletzt mussten die alten Stadtrechte zugleich mit der eigenen Gerichtsbarkeit und der Autonomie der Städte bis auf dürftige Reste der Autorität der Landesherren weichen.

Mit dem Reichsdeputationshauptschluss 1803 wurden auch fast alle bis dahin 51 reichsfreien Städte mediatisiert, also einer staatlichen Herrschaft unterstellt. Bei den übrigbleibenden freien Städten Frankfurt, Bremen, Hamburg und Lübeck wandelte sich das Stadtrecht mit der durch den Fortfall des Reiches gewonnenen völkerrechtlichen Souveränität in eigenstaatliches Recht um. Nur für das Familien- und Erbrecht blieben einzelne Satzungen der alten Stadtrechte (Statuten) bis zum Inkrafttreten des BGB am 1. Januar 1900 erhalten.

Die heutigen Rechte der Städte Hamburg, Bremen und Berlin sind Landesrechte. Die kreisfreien Städte in Deutschland haben bis heute Sonder-Gemeindeorganisationsrechte.

Im heutigen deutschen Sprachraum gibt es kein Stadtrecht mehr im eigentlichen Sinne, d. h. die Selbstverwaltung in den Städten regeln staatliche Grundsätze und Ländergesetze. Die Bezeichnung Stadt ist nur ein Titel, der aber immer noch vergeben wird (deutsche Gemeinden, denen der Titel Stadt im 21. Jahrhundert vergeben wurde).

Eine Liste aller Städte Deutschlands mit Stadtrecht findet sich unter Liste der Städte in Deutschland.

In Österreich haben insgesamt 201 Gemeinden das Stadtrecht "(Stadtgemeinden)", die oft von den jeweiligen historischen Hauptorten auf die heutige Verwaltungseinheit übergegangen sind – korrekterweise spricht man von "Stadtrang" (bei der heutigen Gemeinde Drosendorf-Zissersdorf hat nur Drosendorf Stadtrecht). Dieses Stadtrecht spielt heute in der Verwaltung nur mehr eine untergeordnete Rolle.
15 davon sind "Statutarstadt" (auch "Städte mit eigenem Statut" genannt), nämlich Eisenstadt, Graz, Innsbruck, Klagenfurt, Krems, Linz, Rust, Salzburg, St. Pölten, Steyr, Villach, Waidhofen/Ybbs, Wels, Wien, Wiener Neustadt, wobei alle Landeshauptstädte außer Bregenz Statutarstädte sind. Statutarstädte stehen in der Verwaltungsgliederung sowohl auf einer Ebene mit der Gemeinde ( LAU-2) als auch auf der Ebene des Bezirks (keine NUTS-Ebene, zwischen NUTS-3 und LAU-1). Seit 1962 ist im Bundes-Verfassungsgesetz festgesetzt, dass die Erhebung zur Statutarstadt nur von Gemeinden mit zumindest 20 000 Einwohnern beantragt werden kann.
Eine Stadt, Scheibbs, beruft sich auf altes Stadtrecht und nennt sich "Titularstadt" (sonst nur in Deutschland zu finden, altes Recht des Heiligen Römischen Reichs), die beiden Statutarstädte Eisenstadt und Rust berufen sich als "Freistadt" auf Königlich Ungarisches Stadtrecht.

Acht historische Orte mit Stadtrecht wurden eingemeindet und führen nicht mehr den Titel "Stadt", oder nur mehr formal.

Abweichend vom stadtrechtlichen Begriff bezeichnet man in Österreich üblicherweise Siedlungen bzw. Ballungsräume über 5000 Einwohner als "städtisch".
Vertreten werden die Städte in Verhandlungen mit Land und Bund oft durch den Österreichischen Städtebund im Gegensatz zu anderen Gemeinden, die vom Österreichischen Gemeindebund vertreten werden.

Die Schweiz kennt kein Stadtrecht im rechtlichen Sinne, das verliehen werden könnte. Der Begriff „Stadt“ ist primär eine statistische Größe: Weist eine Politische Gemeinde mehr als 10.000 Einwohner auf, gilt sie statistisch als Stadt. Ob sich eine Gemeinde selbst als Stadt bezeichnet, ist meist historisch bedingt. So nennen sich längst nicht alle Gemeinden mit über 10.000 Einwohnern Stadt, umgekehrt gibt es zahlreiche historische Kleinstädte mit weniger als 10.000 Einwohnern, die sich als Stadt definieren. Diese Bezeichnung rührt von früher verliehenen Stadtrechten her.

In Tschechien gibt es 23 Statutarstädte.






</doc>
<doc id="11891" url="https://de.wikipedia.org/wiki?curid=11891" title="The Beatles">
The Beatles

The Beatles war eine britische Beat- und Rockband in den 1960er Jahren. Mit mehr als 600 Millionen – nach Schätzungen ihrer Plattenfirma EMI sogar mehr als einer Milliarde – verkauften Tonträgern ist sie die kommerziell erfolgreichste Band der Musikgeschichte. Die Bandmitglieder in der kommerziell erfolgreichen Zeit der Band waren John Lennon, Paul McCartney, Ringo Starr und George Harrison.

Die musikalischen Ursprünge der Band liegen im Rock ’n’ Roll der ausgehenden 1950er Jahre, in den dann sehr bald Stilelemente der Liverpooler Beatmusik einflossen. Ihre erste Single "Love Me Do" erschien 1962. Den weltweiten Durchbruch schaffte die Gruppe 1963 mit der Single "I Want to Hold Your Hand". Aufgrund ihres damals neuartigen Musikstils und ihres öffentlichen Auftretens entwickelten sie sich schnell zu einer der populärsten Bands. Den Höhepunkt ihrer Karriere erreichten die Beatles zwischen 1964 und 1968, als sie zeitweise in fast allen Ländern die Hitparaden anführten. 1970 trennten sich die Wege der vier Bandmitglieder aufgrund interner Spannungen. Die Musiker verfolgten danach erfolgreich eigene Musikprojekte.

Die späteren Beatles gingen Ende der 1950er Jahre aus einer Reihe von Vorläuferbands um den Liverpooler Schüler John Lennon hervor. Der musikbegeisterte 16-Jährige gründete gegen Ende des Jahres 1956 in Liverpool die Skiffle-Gruppe The Quarrymen, benannt nach der "Quarry Bank High School", die er besuchte. Erste Mitglieder dieser Schülerband waren neben Lennon (Gesang/Gitarre) der Gitarrist Eric Griffiths, der Schlagzeuger Colin Hanton, der Banjospieler Rod Davis, der Bassist Len Garry und Pete Shotton am Waschbrett. Die Besetzung der Quarrymen, die bei privaten Feiern und Schulfesten ihre ersten Auftritte absolvierte, wechselte allerdings ständig, was nicht unwesentlich am exzentrisch-launischen Bandleader Lennon lag. Zum Repertoire der Gruppe gehörten Stücke damaliger Musikgrößen wie Buddy Holly, Eddie Cochran und Lonnie Donegan.

Der wohl wichtigste Tag der noch jungen Bandgeschichte war der 6. Juli 1957, als die Quarrymen bei einem Gartenfest der Pfarrgemeinde von Liverpool-Woolton auftraten. Nach dem ersten der zwei Konzerte wurde John Lennon von seinem Schulfreund Ivan Vaughan dem 15-jährigen Paul McCartney vorgestellt. Lennon war beeindruckt, als McCartney textsicher eine Version des Cochran-Stücks "Twenty Flight Rock" zum Besten gab. Einige Tage später wurde er in die Gruppe aufgenommen. Der erste gemeinsame Auftritt fand am 18. Oktober 1957 in der Liverpooler "New Clubmoor Hall" statt. Die Aufnahme von Paul McCartney bedeutete für John Lennon einerseits den Verlust seiner unumschränkten Führungsposition, andererseits aber eine künstlerische Konkurrenz auf Augenhöhe und eine gewinnbringende Quelle der Inspiration. Im Sommer 1957 begannen Lennon und McCartney mit dem gemeinsamen Komponieren eigener Stücke, zu den ersten gehörten "Too Bad About Sorrows" und "Just Fun". Viele davon entstanden im Wohnzimmer des Reihenhauses der McCartney-Familie in der 20 Forthlin Road.

Am 6. Februar 1958 besuchte George Harrison, ein 14-jähriger Freund von Paul McCartney, ein Konzert der Quarrymen in der "Wilson Hall" von Liverpool-Garston. Nachdem er während einer Busfahrt John Lennon das Instrumentalstück "Raunchy" vorgespielt hatte, wurde auch er in die Gruppe aufgenommen. Aufgrund seines eher zurückhaltenden Charakters und des leichten Altersunterschiedes blieb er aber bis auf weiteres im Schatten von Lennon und McCartney, deren Freundschaft noch enger wurde, nachdem Lennons Mutter Julia am 15. Juli 1958 bei einem Autounfall ums Leben gekommen war. McCartneys Mutter war bereits 1956 an Brustkrebs gestorben.

Ungefähr im Frühling oder Sommer 1958 nahmen The Quarrymen, die inzwischen aus Lennon, McCartney, Harrison, Colin Hanton und dem Klavierspieler John Lowe bestanden, zu privaten Zwecken die Lieder "That’ll Be the Day" (eine Buddy-Holly-Komposition) und "In Spite of All the Danger" (eine McCartney/Harrison-Komposition) im Homerecording-Studio von Percy Phillips in Liverpool auf. Die Lieder wurden direkt auf eine Schellackplatte aufgezeichnet. Die Schallplatte mit diesen ersten Aufnahmen der späteren Beatles wurde im Jahr 1981 von Lowe an Paul McCartney verkauft und 1995 im Rahmen der "Anthology" erstmals veröffentlicht.

Um sich für einen Auftritt im britischen Fernsehen zu qualifizieren, nahmen die Bandmitglieder im Oktober 1959 unter ihrem neuen Namen "Johnny and the Moondogs" an einem Talentwettbewerb des Unternehmers Carroll Levis teil. Nachdem sich die Gruppe für die Endrunde am 15. November 1959 in Manchester hatte qualifizieren können, schien ein Sieg möglich, doch fand das Finale ohne die Moondogs statt, da die Gruppe aus finanziellen Gründen den letzten Zug nach Liverpool nehmen musste.

Im Januar 1960 stieß John Lennons Freund von der Kunstschule, der angehende Maler Stuart Sutcliffe, als Bassist zu den Moondogs. Der musikalisch unbedarfte Sutcliffe hatte sich auf Lennons Anraten hin vom Erlös eines seiner Gemälde eine Bassgitarre gekauft. Kurze Zeit später wurde die Gruppe in Anlehnung an die Buddy-Holly-Band The Crickets "(Die Grillen)" in "The Silver Beetles" und im Juli 1960 in "The Silver Beatles" umbenannt, wobei "Beatles" eine gedankliche Verbindung des Musikstils Beat und des englischen Wortes für „Käfer“, "beetles", herstellt. In einem Artikel, den Lennon für das Liverpooler Musikmagazin "Mersey Beat" verfasste, erläuterte er in seiner eigenen Weise, die Idee sei von einem Mann auf einem brennenden Kuchen gekommen, der ihnen in einer Vision erschienen sei.

Im Mai 1960 begleiteten die Silver Beetles den Sänger Johnny Gentle auf einer Schottland-Tournee, die durch die Vermittlung des Managers Larry Parnes zustande gekommen war. Doch der erhoffte Durchbruch entwickelte sich zum Fiasko. Während der einwöchigen Reise musste sich die noch unbekannte Gruppe mit spärlichem Publikum, Hunger und Erschöpfung auseinandersetzen. Eines der zum Teil verwahrlosten Hotels musste wegen Geldmangels Hals über Kopf verlassen werden. Zudem wurde der damalige Schlagzeuger Tommy Moore bei einem Autounfall verletzt.

Im August 1960 verzichtete die Gruppe auf den Zusatz "Silver" im Namen und nannte sich fortan "The Beatles".

Auf der Suche nach englischen Musikgruppen für seine Hamburger Nachtlokale wandte sich der deutsche Veranstalter Bruno Koschmider im Sommer 1960 an seinen Liverpooler Geschäftspartner Allan Williams. Dieser hatte zeitweise das Management der Beatles übernommen und bot ihnen ein entsprechendes Engagement an. Am 16. August 1960 kamen John Lennon, Paul McCartney, George Harrison und Stuart Sutcliffe sowie der wenige Tage zuvor rekrutierte Schlagzeuger Pete Best in Hamburg an.

Am 17. August 1960 gab die Gruppe im Hamburger Rotlichtviertel St. Pauli ihr erstes Konzert unter dem Namen „The Beatles“. Von nun an spielten sie täglich im „Indra“, einem Stripclub an der berüchtigten Großen Freiheit. Nach und nach passten sich die Gruppenmitglieder dem spannungsgeladenen Umfeld aus Gewalt, Alkohol, Drogen und Sex an. Durch die bis zu neun Stunden dauernden Arbeitstage wurden Repertoire, Spontaneität und Selbstbewusstsein vergrößert. Die Aufforderung des Clubbesitzers, eine „Schau“ zu machen, mündete in wüsten Bühnenauftritten, die das Publikum anlockten und die Band zum angesagtesten Geheimtipp der Großen Freiheit machten.

In Hamburg lernten die Beatles die Fotografen Astrid Kirchherr und Jürgen Vollmer sowie den angehenden Künstler Klaus Voormann kennen, die später den Stil und das Auftreten der Gruppe in Bezug auf Kleidung und Frisuren („Pilzkopf“, engl. "moptop") entscheidend prägten. Zudem war Kirchherr, die sich besonders mit dem zurückhaltenden Bassisten Sutcliffe anfreundete, die erste, die die Band im Rahmen professioneller Fotoaufnahmen ablichtete. Der Beatles-Haarschnitt, die sogenannte „Pilzkopf“-Frisur, wurde von dem deutschen Fotografen Jürgen Vollmer kreiert. Er selbst hatte sich diese Frisur geschnitten, lange bevor ihn Astrid Kirchherr und Klaus Voormann kennenlernten. In der Paul-McCartney-Biografie "Many years from now" von Barry Miles wird McCartney wie folgt zitiert:

Auch in einem von John Lennon 1967 gegebenem Interview – zitiert in "The Beatles Anthology" – heißt es:

Vollmer erinnerte sich in einem Interview an das Treffen mit Lennon und McCartney im September 1961:

Weil der Indra Club wegen Ruhestörung geschlossen werden musste, zogen die Beatles Anfang Oktober in den Kaiserkeller um. Hier trat auch die ebenfalls aus Liverpool stammende Gruppe Rory Storm & the Hurricanes mit ihrem Schlagzeuger Ringo Starr auf. Am 15. Oktober 1960 vereinigten sich die beiden Gruppen zu einer privaten Aufnahme des Titels "Summertime" in einem kleinen Studio an der Kirchenallee 57. Im Laufe der Jahre gingen allerdings alle neun gepressten Platten dieser ersten gemeinsamen Produktion von Lennon, McCartney, Harrison und Starr verloren.

Ende November mussten die Beatles wegen der Minderjährigkeit von George Harrison (der zu diesem Zeitpunkt erst 17 Jahre alt war) und einer angeblichen Brandstiftung durch Paul McCartney und Pete Best Deutschland verlassen. Nur Stuart Sutcliffe blieb bei seiner Freundin Astrid Kirchherr in Hamburg, wo er am 10. April 1962 im Alter von nur 21 Jahren an einer Hirnblutung starb. In den folgenden zwei Jahren absolvierten die Beatles noch vier weitere Spielzeiten in der Hansestadt, davon eine im Top Ten Club, die restlichen im renommierten Star-Club.

Am 27. Dezember 1960 spielten die Beatles ein Konzert in der Liverpooler Litherland Town Hall, bei dem die Gruppe ihren Status als „beste Band der Stadt“ festigte und erste Anzeichen der aufkommenden Popularität zu spüren bekam. Bei diesem Auftritt wurde Sutcliffe durch den Studenten Chas Newby vertreten. Wenig später übernahm Paul McCartney endgültig die Rolle des Bassisten, und die Beatles wurden zu einer Vier-Mann-Band.

Die nachhaltige Wirkung ihres Konzertes in der Litherland Town Hall ermöglichte den Beatles Auftritte in zahlreichen Clubs von Liverpool und die damit verbundene Steigerung ihres Bekanntheitsgrades. Am 9. Februar 1961 feierten Lennon, McCartney, Harrison und Best ihr Debüt im Cavern Club, einem rauchig-düsteren Kellerlokal in der Mathew Street, das schnell zum Stammlokal der Gruppe wurde und heute durch 292 nachgewiesene Auftritte einen Kultstatus innehat.

Während ihres zweiten Deutschlandaufenthaltes begleiteten die Beatles zwischen dem 22. und 23. Juni 1961 den britischen Sänger Tony Sheridan bei einigen Aufnahmen für die Plattenfirma Polydor in der Friedrich-Ebert-Halle in Hamburg-Heimfeld, darunter befanden sich die Stücke "My Bonnie", "The Saints (When The Saints Go Marching In)", "Why (Can’t You Love Again)", die Harrison/Lennon-Instrumentalkomposition "Cry for a Shadow" und "Nobody’s Child". Diese ersten professionellen Aufnahmen der Gruppe (später auf den deutschen Veröffentlichungen als "The Beat Brothers" bezeichnet) wurden von dem deutschen Komponisten und Bandleader Bert Kaempfert produziert. Das galt auch für die folgende Session am 24. Juni 1961, die im Studio Rahlstedt (Rahlau 128, Hamburg-Tonndorf) stattfand. Hier entstanden "Ain’t She Sweet" und "Take Out Some Insurance on Me".

Am 28. Juni 1961 wurde ein Vertrag zwischen John Lennon und dem Musikverleger Alfred Schacht geschlossen, der am 1. Juli 1961 in Kraft trat und zunächst für ein Jahr galt. Auf Bitten von Brian Epstein wurden die Beatles am 25. Mai 1962 aus diesem Vertrag vorzeitig entlassen; am Tag zuvor hatten sie noch mit Tony Sheridan "Sweet Georgia Brown" und "Swanee" eingespielt. "My Bonnie" wurde am 23. Oktober 1961 in Deutschland veröffentlicht; es wurden hier 100.000 Exemplare verkauft, auch wegen intensiven Airplays bei Radio Luxemburg.

Am 28. Oktober 1961 beauftragte ein Kunde Brian Epstein, den Liverpooler Geschäftsführer des Schallplattenladens NEMS, ein Exemplar der in Deutschland erschienenen Tony Sheridan/Beatles-Single "My Bonnie" zu beschaffen. Nach weiteren Anfragen recherchierte Epstein den Aufenthaltsort der Gruppe. Am 9. November 1961 besuchte er mit seinem Assistenten Alistair Taylor ein Beatles-Konzert im Cavern Club. Er war beeindruckt von der Ausstrahlung der Bandmitglieder und bot ihnen einige Wochen später an, sie zu managen. Die Übernahme des Managements durch Brian Epstein bedeutete für John Lennon, Paul McCartney, George Harrison und Pete Best, die bisher in Lederkluft und ohne festes Programm aufgetreten waren, weitreichende Stilveränderungen. Epstein bestand auf ordentlichen Anzügen, einem festen Bühnenprogramm und angemessenem Verhalten auf der Bühne. Am 9. Dezember 1961 gaben die Beatles im Palais Ballroom von Aldershot vor gerade mal 18 Zuschauern ihr Südengland-Debüt.

Brian Epstein ermöglichte den Beatles einen Vorspieltermin bei der Plattenfirma Decca in London, wo die Gruppe am 1. Januar 1962 bei der sogenannten Decca Audition 15 Stücke spielte (darunter: "Money, Till There Was You" und "Besame Mucho"). Schließlich entschieden sich die Decca-Geschäftsführer Mike Smith und Dick Rowe aber doch für Brian Poole & The Tremeloes, da nach ihrer Meinung „Gitarrengruppen aus der Mode kommen“ würden.

Am 7. März 1962 produzierten die Beatles in Manchester zum ersten Mal Stücke für eine Radiosendung der BBC. Drei Monate später, am 6. Juni 1962, stand ein weiterer Vorspieltermin in den Londoner Abbey Road Studios an, bei dem die Gruppe vor den Parlophone-Produzenten George Martin und Ron Richards auftrat. Lennon, McCartney, Harrison und Best spielten "Besame Mucho, Love Me Do, P.S. I Love You" und "Ask Me Why". Nach dem Vorspielen bot Martin, der eigentlich nach einem Solokünstler mit Begleitgruppe im Stil von Cliff Richard und The Shadows für sein Label gesucht hatte, den Beatles einen Plattenvertrag an.

Kurz vor den ersten Plattenaufnahmen entließen die Beatles überraschend den beim Publikum beliebten Schlagzeuger Pete Best und ersetzten ihn durch Ringo Starr, der mit seiner Gruppe Rory Storm & the Hurricanes gerade in Skegness aufgetreten war. Die Gründe für die Trennung von Best wurden nie vollständig geklärt. Der Schlagzeuger war wegen mangelnder Kontaktfreudigkeit wohl eher ein Außenseiter innerhalb der Band, zudem hatte sich George Martin nach den ersten Probeaufnahmen ebenfalls für einen Wechsel ausgesprochen. Das erste gemeinsame Konzert von Lennon, McCartney, Harrison und Starr fand am 18. August 1962 in der Hulme Hall von Birkenhead statt.

Am 22. August 1962 wurden die Beatles bei einem Auftritt im Cavern Club von einem Granada-TV-Team gefilmt – der erste Fernsehauftritt der Gruppe. Zu Silvester spielte die Gruppe ihr letztes Konzert im Star-Club in Hamburg.

Am 5. Oktober 1962 erschien mit "Love Me Do" die erste offizielle Single der Beatles in Großbritannien. Ringo Starr war zunächst bei der Aufnahme von Produzent George Martin durch den erfahrenen Studiomusiker Andy White ersetzt worden, um das Schlagzeug einzuspielen. Letztlich erklärte Martin sich dann aber doch mit Ringo Starr einverstanden.

"Love Me Do", eine Eigenkomposition, die John Lennon und Paul McCartney gegen das von Martin favorisierte "How Do You Do It" durchgesetzt hatten, stieg bis auf Rang 17 der britischen UK Top 40. Die zweite Single "Please Please Me" wurde am 11. Januar 1963 veröffentlicht und schaffte es bis auf Rang 2 der UK-Hitlisten.

Die Erfolge ihrer ersten Platten und die daraus resultierenden Radio- und Fernsehauftritte ermöglichten den Beatles zunehmend Gastspiele auch außerhalb des Einzugsgebietes Liverpools. Die neuen Roadmanager Neil Aspinall – ein alter Freund von Paul McCartney – und Malcolm „Mal“ Evans, der ehemalige Türsteher des Cavern Club, fuhren die Gruppe mit ihren Instrumenten in einem gebraucht erstandenen Kleinbus zu ihren Auftrittsorten.

Am 2. Februar 1963 starteten die Beatles im Gaumont von Bradford als Vorgruppe der 16-jährigen Sängerin Helen Shapiro ihre erste professionelle Tournee durch Großbritannien. Mit dabei waren auch Dave Allen, Danny Williams und Kenny Lynch. Die zweite Tour zeigte die Band vom 9. März an neben Tommy Roe und Chris Montez. Nach ersten Zuschauerreaktionen wurden die Beatles bereits einen Tag später als Hauptgruppe angekündigt. Es folgten hochkarätige Auftritte bei der Show "Swinging Sound" in der Londoner Royal Albert Hall (18. April 1963) und vor 10.000 Zuschauern im Rahmen des "NME-Poll-Winners"-Konzerts im Empire Pool von Wembley (21. April 1963).

Das erste offizielle Beatles-Album "Please Please Me" erschien am 22. März 1963. Lennon, McCartney, Harrison und Starr hatten die LP am 11. Februar innerhalb von nur zwölf Stunden in den Abbey-Road-Studios aufgenommen und dabei vorwiegend Stücke aus ihrem Live-Repertoire verwendet, darunter "I Saw Her Standing There", "Do You Want to Know a Secret" und "Twist and Shout". "Please Please Me" stieg, wie alle folgenden Beatles-LPs bis 1970 (mit Ausnahme von "Yellow Submarine" 1969), an die Spitze der britischen Hitlisten.

Die dritte Single "From Me to You" erschien am 11. April 1963. Die A-Seite war von John Lennon und Paul McCartney im Tourbus komponiert worden. Die Platte erreichte wenig später mit 650.000 verkauften Exemplaren die Spitze der britischen Hitparade und stellte den Beginn einer eindrucksvollen Erfolgsserie dar: Bis zum Februar 1967 eroberten die Beatles mit allen ihren Singles den ersten Platz der britischen Hitlisten.

Das erste Treffen der Beatles mit den noch eher unbekannten Rolling Stones, die kurz vor der Veröffentlichung ihrer ersten Single standen, fand am 14. April 1963 im Crawdaddy Club des Londoner Station-Hotels statt. Später überließen John Lennon und Paul McCartney, die sich mit Mick Jagger und Keith Richards angefreundet hatten, der Gruppe ihre Komposition "I Wanna Be Your Man". Der Titel erschien als zweite Single der Rolling Stones und erreichte Platz 12 der britischen Hitparade.

Vom 18. Mai bis zum 9. Juni 1963 absolvierten die Beatles ihre dritte England-Tournee, diesmal an der Seite von Roy Orbison und Gerry & the Pacemakers. Auch Orbison trat nach der ersten Show zugunsten der Beatles als Headliner zurück. Während der Tour erhielt die Gruppe mit "Pop Go to the Beatles" eine eigene, wöchentlich ausgestrahlte Radiosendung bei der BBC. Im Juli und August folgten fünf je einwöchige Gastspiele in Ferienorten (u. a. in Margate und Bournemouth).

Wegen der Aufnahmen in den Abbey-Road-Studios und den zahlreichen Auftritten in Radiosendungen hatten die Beatles bereits zu Beginn des Jahres ihren Lebensmittelpunkt nach London verlegt. Besuche in Liverpool wurden wegen des engen Terminplans immer seltener. Am 3. August 1963 spielte die Gruppe vor 274 Zuschauern ihr letztes Konzert im Cavern Club.

Mit ihrer stetig steigenden Bekanntheit machte der Gruppe die ekstatische und zunehmend außer Kontrolle geratende Verehrung durch weibliche Fans zu schaffen, die nach der Veröffentlichung der nächsten Single "She Loves You" am 23. August 1963 ungeahnte Ausmaße erreichte. Öffentliche Auftritte gerieten zu Massenveranstaltungen, viele der Konzertsäle konnten sie nur noch mit starkem Polizeischutz erreichen und wieder verlassen. "She Loves You" avancierte mit 1,6 Millionen verkauften Exemplaren zu einer der meistverkauften britischen Singles.

Der endgültige Durchbruch gelang den Beatles am 13. Oktober 1963 mit einem Auftritt in der populären ATV-Sendung "Sunday Night at the London Palladium", den 15 Millionen Menschen sahen. Hunderte von kreischenden Fans verursachten ein Verkehrschaos vor dem Theater. Nach ausgiebigen Fernseh-, Radio- und Presseberichten wurde die Hysterie, genannt „Beatlemania“, zum Selbstläufer. In Anspielung auf ihre Berühmtheit wurden sie nun auch oft „Fab Four“ (berühmte/famose/fabelhafte Vier) genannt.

Ende Oktober 1963 absolvierten die Beatles eine kurze Schweden-Tournee, die Auftritte in Stockholm, Borås und Eskilstuna einschloss, dazu Rundfunk- und Fernsehaufnahmen für die örtliche Presse. Als sie am 31. Oktober wieder in Heathrow landeten, wurde der amerikanische Showmaster Ed Sullivan zufällig Zeuge eines phänomenalen Empfangs. Trotz strömenden Regens warteten tausende kreischende Fans auf die Heimkehrer. Sullivan hatte noch nie von den Beatles gehört, buchte sie aber sofort für seine Show. Am Tag darauf ging die Gruppe erneut auf Großbritannien-Tournee, diesmal von Anfang an als Hauptgruppe. Die teilweise chaotische Tour wurde von starken Polizeikräften begleitet, die den hysterischen Sympathiebekundungen weiblicher Anhänger entgegenwirken sollten. Um den Fans zu entkommen, verkleideten sich die Beatles in Birmingham als Polizisten. In Plymouth gelang die Flucht durch das Kanalsystem der Stadt, während die Polizei auf den Straßen Wasserwerfer einsetzte.

Am 4. November 1963 spielten die Beatles im Rahmen der alljährlichen "Royal Variety Performance" vor der britischen Königinmutter Elizabeth, Lord Snowdon und Prinzessin Margaret im Londoner Prince of Wales Theatre. Zum Programm gehörten auch Auftritte von Marlene Dietrich und Harry Secombe. Das letzte Stück, "Twist and Shout", kündigte John Lennon mit den Worten an:

Die Londoner "Times" kritisierte den Auftritt der Beatles als zu laut für das dezente Umfeld des Abends:

Das zweite Beatles-Album "With the Beatles" erschien am 22. November 1963. Die LP enthielt neben sieben Coverversionen (u. a. "Roll Over Beethoven" von Chuck Berry) die Lennon/McCartney-Stücke "All My Loving" und "I Wanna Be Your Man" sowie die erste George-Harrison-Komposition "Don’t Bother Me". Am 29. November folgte die Single "I Want to Hold Your Hand", für die es allein in England eine Million Vorbestellungen gab.

Vom 24. Dezember 1963 bis zum 11. Januar 1964 traten die Beatles im Rahmen einer eigens einstudierten "Christmas Show" mit Musik und Sketchen im Londoner Finsbury Park Astoria auf. Mit dabei waren unter anderem Cilla Black und Billy J. Kramer & the Dakotas. Anschließend folgte ein dreiwöchiges Gastspiel im Olympia Theater von Paris, dort allerdings nur als Vorgruppe der damals in Frankreich sehr populären Künstler Trini Lopez und Sylvie Vartan. In den örtlichen Pathé-Marconi-Studios nahmen die Beatles die Stücke "Can’t Buy Me Love" sowie "Komm Gib Mir Deine Hand" und "Sie Liebt Dich" auf, die für eine Veröffentlichung auf dem deutschen Markt gedacht waren.

Bereits kurz nach den ersten Erfolgen der Beatles hatte ihr Manager Brian Epstein versucht, die Gruppe auch auf dem US-amerikanischen Markt zu etablieren. Britische Musiker hatten es diesbezüglich in den USA traditionell schwer. Das US-Plattenlabel Capitol Records, das eigentlich die US-Vertriebsrechte für EMI wahrnahm, lehnte jedoch zunächst die Veröffentlichung von Beatles-Platten ab. Im Februar 1963 sprang das kleine Label Vee-Jay Records ein, das die Single "Please Please Me" / "Ask Me Why" in den USA veröffentlichte. Ende Mai folgte die Single "From Me to You" / "Thank You Girl". Beide Veröffentlichungen konnten sich nicht in den US-amerikanischen Hitparaden platzieren. Die nächste Single – "She Loves You" / "I’ll Get You" erschien in den USA im September 1963 auf dem Label Swan Records. Erst im Dezember 1963 war Capitol Records bereit, einen Tonträger der Beatles – die Single "I Want to Hold Your Hand" / "I Saw Her Standing There" – in den USA auf den Markt zu bringen. Capitol machte Promotion, Zeitungen schrieben über die seltsame Gruppe aus England, und die Radio-DJs fingen an, die Single zu spielen. Am 16. Januar 1964 erreichte die Gruppenmitglieder in ihrem Pariser Hotelzimmer die Nachricht, dass mit "I Want to Hold Your Hand" die Eroberung der Spitzenposition in den US-Charts gelungen war. Der Durchbruch der Gruppe auf dem US-amerikanischen Markt war geschafft. Als Resultat dieses Erfolgs brachte Vee-Jay Records die früheren Tonträger ein weiteres Mal heraus und veröffentlichte im Januar 1964 das Album "Introducing… the Beatles", um von der Popularität der Beatles zu profitieren. Es kam zu Rechtsstreitigkeiten, die einen systematischen Vertrieb der Beatles-Singles in den USA behinderten. Folge war, dass gleich vier Plattenfirmen Beatles-Platten konkurrierend herausbrachten. Im April 1964 einigten sich Vee-Jay Records und Capitol Records darauf, dass Vee-Jay die 16 Beatles-Stücke, die vom Label bereits veröffentlicht worden waren, bis zum 15. Oktober 1964 weiterhin vertreiben durfte. Anschließend fielen die Veröffentlichungsrechte an Capitol.

Mit einem Nummer-eins-Hit reisten die Beatles zu ihren ersten Auftritten in die USA, unter anderem in der Show von Ed Sullivan, der sie bereits Monate zuvor gebucht hatte. Am 7. Februar 1964 landeten die Beatles in einem PanAm-Flugzeug auf dem New Yorker Flughafen, wo 5000 Fans und 200 Journalisten sie auf dem Rollfeld empfingen. Im Flughafengebäude fand eine große Pressekonferenz statt, die die Gruppe auf humorvolle Art und Weise meisterte:
Frage: "“Will you sing something?”" („Singen Sie etwas für uns?“)
Lennon: "“No, we need money first.”" („Nein, nicht ohne vorherige Bezahlung.“)
Frage: "“What is the secret of your success?”" („Was ist das Geheimnis Ihres Erfolgs?“)
Starr: "“We have a press agent.”" („Wir haben einen Pressesprecher.“)
Frage: "“What do you think of the campaign in Detroit to stamp out the Beatles?”" („Was meinen Sie zur Kampagne in Detroit, die Beatles auszulöschen?“)
McCartney: "“We have a campaign to stamp out Detroit.”" („Wir haben eine Kampagne, um Detroit auszulöschen.“)
Frage: "“What do you believe is the reason you are the most popular singing group today?”" („Was ist Ihrer Meinung der Grund dafür, dass Sie momentan die populärste Musikgruppe sind?“)
Lennon: "“We’ve no idea. If we did we’d get four long-haired boys, put them together, and become their managers.”" („Wir haben keine Ahnung. Wenn wir es wüssten, würden wir vier langhaarige Jungs suchen und sie managen.“)
Frage: "“What do you do when you’re cooped up in a hotel room between shows?”" („Was machen Sie, während Sie im Hotelzimmer auf ihre Auftritte warten?“)
Harrison: "“We ice-skate.”" („Wir laufen Schlittschuh.“)
Frage: "“How did you find America?”" („Wie fanden Sie Amerika?“ [diese Formulierung ist im Englischen ebenso doppeldeutig wie im Deutschen])
Lennon: "“We went to Greenland and made a left turn.”" („Wir sind bei Grönland links abgebogen.“)
Am 9. Februar 1964 traten die Beatles mit fünf Stücken "(All My Loving", "Till There Was You", "She Loves You", "I Saw Her Standing There" und "I Want to Hold Your Hand)" in der populären "Ed Sullivan Show" auf. 73,7 Millionen Zuschauer an den Fernsehschirmen verfolgten die Live-Sendung.

Während der Show verlas der Moderator Ed Sullivan ein Glückwunschtelegramm von Elvis Presley und dessen Manager Tom Parker. Nach Konzerten in Washington und in der New Yorker Carnegie Hall flog die Gruppe nach Miami Beach (Florida), wo während eines Kurzurlaubs ein Treffen mit dem Boxer Cassius Clay stattfand. Nach einem weiteren Auftritt in der "Ed Sullivan Show" am 16. Februar kehrten die Beatles am 22. Februar 1964 nach London zurück.

Bis heute unerreicht ist die musikalische Dominanz, mit der die Beatles im April 1964 die Hitlisten beherrschten: In den USA besetzten sie am 4. April die ersten fünf Plätze der Single-Hitparade (an der Spitze die am 16. März 1964 mit 1,7 Millionen Vorbestellungen veröffentlichte Single "Can’t Buy Me Love"), in Australien waren es sogar die ersten sechs Ränge. Zudem war die Gruppe mit sieben weiteren Stücken in den US-Top 100.

Wegen des anhaltenden Erfolgs wurde den Beatles geraten, sich auch im Filmgeschäft zu versuchen. Am 2. März 1964 begannen auf dem Londoner Marylebone-Bahnhof unter der Regie von Richard Lester die Dreharbeiten für die Musikkomödie "Yeah Yeah Yeah" (Originaltitel: "A Hard Day’s Night"). Das Titelstück "A Hard Day’s Night" komponierte John Lennon nach einer von Ringo Starr erfundenen Redewendung: Jener ging nach einem langen Arbeitstag aus dem Aufnahmestudio und wollte sagen “It’s been a hard day”, doch noch während er es sagte, bemerkte er, dass es bereits dunkel war und fügte in seiner typischen Art ein “’s night” an. Der Schwarzweißfilm feierte am 6. Juli 1964 in London Weltpremiere und wurde zu einem Kassenschlager. Am 10. Juli erschienen das dazugehörige Soundtrack-Album, das erstmals nur Kompositionen von Lennon/McCartney enthielt (unter anderem "Things We Said Today, I Should Have Known Better, And I Love Her" und "If I Fell)" sowie die Single "A Hard Day’s Night".

Nach einem knapp einmonatigen Urlaub gingen die Beatles ab dem 4. Juni 1964 erstmals auf Welttournee. Da Ringo Starr an einer schweren Mandelentzündung litt, wurde er bei den Auftritten in Europa und Hongkong durch den Session-Schlagzeuger Jimmie Nicol ersetzt. Erst in Sydney traf Starr wieder mit der Band zusammen. In Adelaide wurde die Gruppe von 300.000 Fans in der Innenstadt empfangen – der größten Menschenmenge, die sich bis dato in Australien versammelt hatte.

Am 10. Juli 1964 kehrten die Beatles für die Nordengland-Premiere von "Yeah Yeah Yeah" nach Liverpool zurück, wo sie von 203.000 Menschen an den Straßen begrüßt wurden. Zum Ende des Monats war die Gruppe erneut in Schweden zu Gast.

Vom 19. August bis 20. September 1964 absolvierten die Beatles ihre erste große US-Tournee, die einem Triumphzug glich. Neben Auftritten in San Francisco, Kansas City, Dallas und New Orleans spielte die Gruppe am 23. August in der Hollywood Bowl von Los Angeles. Am 28. August besuchte Bob Dylan die Band in ihrer Hotel-Suite in New York und führte sie in den Genuss von Marihuana ein.

Nur drei Wochen nach ihrer Rückkehr starteten die Beatles am 9. Oktober 1964 eine weitere, einmonatige Großbritannien-Tournee. Am 23. November wurde die Single "I Feel Fine" veröffentlicht. Das Album "Beatles for Sale" erschien am 4. Dezember 1964 und offenbarte nach Meinung von Kritikern erste Ermüdungserscheinungen. Nur 8 von 14 Stücken waren Eigenkompositionen, darunter "Eight Days a Week, Baby’s in Black" und "I’m a Loser". Tatsächlich hatten die vielen Konzerte und anderen Verpflichtungen den vieren kaum Zeit gelassen, neue Songs zu schreiben, was der Grund für die sechs Coverversionen auf dem Album war. Das Jahr endete mit der zweiten Auflage der "Beatles Christmas Show" im Londoner Hammersmith Odeon, die 39 Auftritte in drei Wochen beinhaltete.

Am 23. Februar 1965 begannen auf den Bahamas die Dreharbeiten für den zweiten Beatles-Film "Hi-Hi-Hilfe!", diesmal in Farbe. Regisseur war erneut Richard Lester, zu den Nebendarstellern gehören Victor Spinetti und Eleanor Bron. Weitere Aufnahmen entstanden in Obertauern, London und auf dem Salisbury Plain in der Nähe von Stonehenge. Die eher seichte Slapstick-Komödie feierte am 29. Juli 1965 im Beisein von Prinzessin Margaret in London Weltpremiere. Die Single "Help!" erschien am 19. Juli, das gleichnamige Soundtrack-Album (u. a. mit "You’ve Got to Hide Your Love Away, Another Girl" und "You’re Going to Lose that Girl") am 6. August 1965.

Die neunte Beatles-Single "Ticket to Ride" war bereits am 6. April 1965 veröffentlicht worden. Zwei Monate später, am 14. Juni, produzierte die Gruppe in den Abbey Road Studios mit "Yesterday" eines ihrer bekanntesten Stücke. Paul McCartney hatte die Ballade mit dem Arbeitstitel "Scrambled Eggs" (‚Rühreier‘) nach einer Melodie geschrieben, die ihm eines Tages beim Aufwachen im Haus seiner Freundin Jane Asher eingefallen war. Das als McCartney-Solo aufgenommene und mit Streichern unterlegte Lied wurde nicht als Single, sondern im Rahmen der "Help!"-LP und auf einer EP veröffentlicht. Es gehört heute zu den berühmtesten Pop-Stücken der Musikgeschichte.

Vom 20. Juni bis zum 3. Juli 1965 absolvierten die Beatles eine zweiwöchige Europatournee. Nur einen Monat später ging es erneut in die Vereinigten Staaten. Am 15. August 1965 spielte die Gruppe im ausverkauften New Yorker Shea Stadium vor 55.600 Zuschauern (darunter Mick Jagger, Keith Richards, Barbara Bach und Linda Eastman) den größten Live-Auftritt ihrer Karriere. Es war das erste Mal, dass eine Popgruppe in einem offenen Stadion auftrat und die bis dahin größte zahlende Zuschauermenge bei einem Konzert. Der Lärmpegel der weiblichen Fans war während der 30-minütigen Show so hoch, dass weder die Zuschauer noch die Beatles die Musik hören konnten.

Nachdem sich die Beatles während der Tournee bereits mit Stars wie Bob Dylan, Del Shannon, Joan Baez, Peter Fonda und Peter, Paul and Mary getroffen hatten, fand am 27. August 1965 das langerwartete Treffen mit ihrem Idol Elvis Presley in dessen Haus in Bel Air statt. Man unterhielt sich und ging schließlich zu einer Jam-Session über. Angeblich wurde dabei eine Coverversion von "You’re My World" auf Band mitgeschnitten.

Am 12. Juni 1965 gab das britische Königshaus bekannt, dass die Beatles wegen ihrer Verdienste um die britische Exportindustrie von Königin Elisabeth II. mit dem MBE-Orden ausgezeichnet werden würden. Die Ehrung fand am 26. Oktober 1965 im Londoner Buckingham Palace statt. 4000 Fans und Journalisten harrten vor dem Gebäude aus. Die Hysterie um die Ordensverleihung blieb allerdings nicht ohne Folgen. Vornehmlich Militärs waren es, die ihre jahrzehntelange Pflichterfüllung im Dienste der Krone durch die Verleihung desselben Ordens an eine Popgruppe herabgesetzt sahen und daraufhin ihre MBE-Orden zurückgaben.

Am 3. Dezember 1965 veröffentlichten die Beatles die Doppel-A-Seiten-Single "We Can Work It Out" / "Day Tripper" und das Album "Rubber Soul". Die Stücke der LP (unter anderem "Drive My Car, Nowhere Man, Girl" und "In My Life") lassen erste Anzeichen eines musikalisch-geistigen Reifungsprozesses der Gruppenmitglieder erkennen. Bei "Norwegian Wood (This Bird Has Flown)" spielte George Harrison auf einer Sitar, einem indischen Instrument, das er während der Dreharbeiten zu "Hi-Hi-Hilfe!" kennengelernt hatte. Paul McCartney steuerte mit "Michelle" eine englisch-französische Ballade bei. Zu einigen der Lieder drehten die Beatles erstmals sogenannte „Promotionfilme“, Vorläufer von Musikvideos.

Zur selben Zeit starteten die Beatles ihre sechste und letzte Großbritannien-Tournee (unter anderem mit The Moody Blues als Vorgruppe). Während dieser Tour bestritten sie auch den letzten Auftritt in ihrer Heimatstadt Liverpool am 5. Dezember 1965. Viele ihrer Freunde und Verwandten saßen im Publikum.

Am 1. Mai 1966 standen die Beatles im Rahmen des "NME-Poll-Winners"-Konzerts zum letzten Mal auf einer britischen Konzertbühne. Einige Aufregung verursachte im Juni 1966 die Veröffentlichung des US-Albums "Yesterday and Today". Das Coverfoto zeigte die Beatles inmitten von rohem Fleisch und zerbrochenen Puppen "(Butcher Cover)". Nach scharfen Protesten aus der Öffentlichkeit wurde die LP zurückgezogen und erschien mit einem neuen Cover. Am 10. Juni 1966 kam die Single "Paperback Writer" in die Läden.

Vom 24. Juni bis zum 4. Juli 1966 absolvierten die Beatles eine weitere Welttournee, die im Rahmen der BRAVO-Beatles-Blitztournee auch sechs Konzerte an drei Tagen in Deutschland umfasste (München, Essen und – zum letzten Mal am 26. Juni – Hamburg). Für Unfrieden sorgten Proteste von Traditionalisten in Japan, die einen Auftritt der Gruppe im Budōkan, einer vorwiegend der Kampfkunst vorbehaltenen Halle in Tokio, ablehnten. In Manila spielte die Gruppe zwei Konzerte vor insgesamt 80.000 Zuschauern. Die Tour endete mit einem Eklat, als Brian Epstein im Namen der Band eine Einladung der philippinischen Diktatoren-Gattin Imelda Marcos zum Abendessen ablehnte. Während der darauffolgenden hastigen Abreise wurden die Beatles und ihre Mitarbeiter auf dem Flughafen der Hauptstadt von „Sicherheitskräften“ attackiert. Ringo Starr ging nach einem Kinnhaken zu Boden, ein Chauffeur erlitt einen Rippenbruch.

Die Veröffentlichung der LP "Pet Sounds" durch die Beach Boys spornte Lennon und McCartney zu neuen Höchstleistungen an. Am 5. August 1966 erschien das Album "Revolver", mit dem die Beatles in Bezug auf Klang und instrumentale Bandbreite musikalisches Neuland betraten, was einzelne zeitgenössische Kritiker bewog, das Album vorschnell als „Selbstmordwaffe“ des Quartetts abzuwerten. Das Cover des Albums wurde von Klaus Voormann, dem Freund aus Hamburger Tagen, gestaltet. Die langwierig ausgefeilten, teilweise auch unter Drogeneinfluss entstandenen Stücke, darunter "Eleanor Rigby", "Taxman", "Good Day Sunshine", "I’m Only Sleeping", "For No One" und das psychedelisch angehauchte "Tomorrow Never Knows", waren aufwendig mit klassischen Elementen und Tonbandschleifen aus dem Studio angereichert worden. Gleichzeitig wurde die populäre Single "Yellow Submarine" veröffentlicht, ein Kinderlied, das Paul McCartney für den stimmlich etwas eingeschränkten Ringo Starr komponiert hatte. Die Single erreichte in den USA „nur“ Rang 2 der Hitlisten.

Am 12. August 1966 starteten die Beatles in Chicago ihre vierte US-Tournee. Die Vorkommnisse in Asien und die Tatsache, dass durch den enormen Lärmpegel während der Auftritte kaum noch Musik zu hören war, sorgten bei den Gruppenmitgliedern zunehmend für Verdrossenheit. Neue Aufregung entstand durch die Veröffentlichung einer Bemerkung John Lennons aus einem früheren Interview, wonach die Beatles „schon jetzt populärer als Jesus“ seien. Nach nervenaufreibenden Pressekonferenzen, öffentlichen Plattenverbrennungen und Demonstrationen des Ku-Klux-Klans entschloss sich die Gruppe, nicht mehr auf Tournee zu gehen und ihre Arbeit ausschließlich ins Studio zu verlegen. Die Tournee endete am 29. August 1966 mit einem Konzert vor 25.000 Zuhörenden (Eintrittspreis damals fünf US-Dollar) im "Candlestick Park" von San Francisco.

Im Herbst 1966 gönnten sich die Beatles einen fast dreimonatigen Urlaub, der einen radikalen Imagewandel nach sich zog. John Lennon wurde in Celle, wo er für den Richard-Lester-Film "Wie ich den Krieg gewann" vor der Kamera stand, der Pilzkopf gestutzt. Die Anzüge der Liveshows gehörten der Vergangenheit an und wurden durch farbenfrohe Kleidung ersetzt. Zudem ließen sich die vier Bandmitglieder einen Schnurrbart stehen.

Allen Trennungsgerüchten zum Trotz begann die Gruppe am 24. November 1966 mit den Aufnahmen für ein neues Album. Am 13. Februar 1967 erschien die Doppel-A-Seiten-Single "Strawberry Fields Forever" / "Penny Lane", auf der sich die Beatles musikalisch-nostalgisch ihrer Heimatstadt Liverpool widmen. Erstmals seit 1963 erreichte die Platte nicht die Spitze der britischen Hitparade, sondern blieb auf Rang 2 stehen, was darauf zurückzuführen ist, dass die Verkaufszahlen der Single durch zwei geteilt wurden, um somit als zwei einzelne Singles in den britischen Charts geführt werden zu können.

Fünf Monate lang arbeiteten Lennon, McCartney, Harrison und Starr in den Abbey Road Studios an ihrer LP "Sgt. Pepper’s Lonely Hearts Club Band", die als eines der ersten Konzeptalben in die Musikgeschichte einging.

Die unbegrenzt verfügbare Aufnahmezeit ermöglichte es den Beatles, alle Studioressourcen einzusetzen und wochenlang an einzelnen Musik- oder Textpassagen zu feilen. Es entstanden Stücke wie "With a Little Help from My Friends", "Lucy in the Sky with Diamonds" und "When I’m Sixty-Four". An den Aufnahmen zu "A Day in the Life" wirkte ein 41-köpfiges Orchester mit. Für das Coverfoto ließen sich die Beatles in Kostümen vor einer Bilderwand berühmter Persönlichkeiten (unter anderem Stan Laurel, Mae West, Bob Dylan, Shirley Temple, Aleister Crowley und Marlon Brando) ablichten. Begleitet von großem Medieninteresse wurde "Sgt. Pepper" am 1. Juni 1967 veröffentlicht.

Nachdem Bob Dylan die Beatles 1964 mit der Droge Marihuana bekannt gemacht hatte, griffen die Gruppenmitglieder im Laufe der Zeit auch zu stärkeren Drogen. John Lennon und George Harrison waren bereits 1965 mit LSD in Kontakt gekommen. Paul McCartney gab im Juni 1967 während eines Fernsehinterviews öffentlichkeitswirksam bekannt, viermal LSD konsumiert zu haben.

Am 25. Juni 1967 traten die Beatles mit der von Lennon komponierten Friedenshymne "All You Need Is Love" in der per Satellit weltweit übertragenen BBC-Fernsehsendung "Our World Live" auf, die von 400 Millionen Zuschauern gesehen wurde. Zu den Gästen in den Abbey Road Studios, wo die Aufnahmen stattfanden, gehörten Mick Jagger, Keith Richards, Eric Clapton, Keith Moon und Marianne Faithfull. Die Single "All You Need Is Love" wurde am 7. Juli 1967 veröffentlicht. Sie brachte die Gruppe beiderseits des Atlantiks an die Spitze der Hitparaden zurück.

Ende Juli 1967 verbrachten die Mitglieder der Beatles mit ihren Frauen einen gemeinsamen Urlaub in Griechenland. Die Idee zum Kauf einer privaten Insel und die damit verbundene Einrichtung eines Aufnahmestudios wurde verworfen, nachdem die griechische Regierung den Besuch zur Touristenwerbung genutzt hatte. Am 19. August 1967 wurde Ringo Starrs zweiter Sohn Jason in London geboren.

Auf Pattie Harrisons Anregung hin besuchten die Beatles am 24. August 1967 einen Vortrag des indischen Gurus Maharishi Mahesh Yogi, der in London für die Techniken der Transzendentalen Meditation warb. Die Gruppe wurde zu einem Einführungs-Wochenende nach Bangor eingeladen. Mit dabei waren auch Mick Jagger und Marianne Faithfull. Während des Kurses erhielten die Beatles die Nachricht, dass ihr Manager Brian Epstein am 27. August 1967 tot in seiner Wohnung aufgefunden worden war.

Schockiert vom Tod ihres Freundes begannen die Beatles am 11. September 1967 mit den Dreharbeiten für ihren dritten Film "Magical Mystery Tour". Die psychedelische Komödie nach einer Idee von Paul McCartney entstand völlig ohne Drehbuch. McCartney übernahm weitgehend die Rolle des Regisseurs, was bei Lennon und Harrison für Verstimmungen sorgte. Das Soundtrack-Album "Magical Mystery Tour" (unter anderem mit "The Fool on the Hill" und "I Am the Walrus") wurde am 27. November in den USA veröffentlicht. Die Single "Hello, Goodbye" erschien bereits am 24. November. Am 26. Dezember 1967 feierte "Magical Mystery Tour" im BBC-Fernsehen Weltpremiere, um kurz darauf – als erster Misserfolg – von Presse und Kritikern verrissen zu werden.

Mitte Februar 1968 reisten die Beatles mit ihren Frauen nach Rishikesh (Indien), wo ein mehrwöchiger Meditationskurs des Maharishi stattfand. Weitere Teilnehmer waren Mia Farrow, Mike Love und Donovan. Ringo Starr kehrte bereits Anfang März nach England zurück, Paul McCartney folgte drei Wochen später. John Lennon und George Harrison verließen Indien erst Mitte April, nachdem Lennons Freund Yanni „Magic Alex“ Alexis Mardas das Gerücht verbreitet hatte, der Maharishi habe Mia Farrow und andere weibliche Kursteilnehmer sexuell belästigt – ein Gerücht, das von Farrow selbst nie bestätigt wurde und für das sich George Harrison später beim Maharishi entschuldigt haben soll. Während der Indienreise erschien am 15. März 1968 die an den Stil von Fats Domino angelehnte Single "Lady Madonna". Der Aufenthalt in Rishikesh war nach Ansicht von Paul Saltzman, Autor eines Bildbandes über jene Tage, die kreativste Zeit der Beatles: 48 Songs seien in diesen sieben Wochen komponiert worden.

Am 11. Mai 1968 flogen John Lennon und Paul McCartney nach New York, wo sie bei Pressekonferenzen und Fernsehauftritten das neue Beatles-Unternehmen "Apple" vorstellten. Der Konzern mit Abteilungen für Musik, Film und Kunst sollte sich vorrangig der Entdeckung und Förderung von Talenten aller Art widmen. Als Leiter ihrer Pressestelle engagierten sie den Journalisten Derek Taylor, der 1964 bereits einmal als Pressesprecher für die Gruppe gearbeitet hatte. In der Folgezeit entwickelte sich Apple jedoch durch den mangelnden Geschäftssinn der Beatles und ihrer Mitarbeiter zum Millionengrab. Bereits im November 1967 war in London eine Apple-Boutique eröffnet worden, die jedoch mangels Einnahmen nach wenigen Monaten geschlossen werden musste. Zu den wenigen Lichtblicken gehörten die Entdeckung der Sängerin Mary Hopkin und der Gruppe Badfinger durch Paul McCartney.

Die fortschreitende Liebesbeziehung John Lennons zu Yoko Ono – er hatte die japanische Avantgarde-Künstlerin am 9. November 1966 bei einer ihrer Londoner Ausstellungen kennengelernt – führte im Mai 1968 zur Trennung von Ehefrau Cynthia. Die äußerst enge Verbindung des jungen Paares wirkte sich schließlich auch negativ auf die Zusammenarbeit Lennons mit den anderen Beatles aus, da Ono darauf bestand, bei sämtlichen Studioaufnahmen der Gruppe anwesend zu sein.

Am 30. Mai 1968 begannen die Beatles mit den Aufnahmen zu einem neuen Album; in Anbetracht der Fülle der in Indien komponierten Stücke entschloss man sich zur Produktion eines Doppelalbums. Die Arbeiten, die wegen des Termindrucks teilweise getrennt stattfanden, wurden von einer zunehmend schlechten Stimmung zwischen den Gruppenmitgliedern geprägt. Neben der offenen Abneigung, die McCartney, Harrison und Starr gegenüber Yoko Ono hegten, war man sich auch über die Single-Auskopplung des Lennon-Songs "Revolution" uneinig. Nach einem heftigen Streit verließ Ringo Starr die Gruppe, konnte aber zur Rückkehr überredet werden. Trotz aller Schwierigkeiten bewerkstelligten die Beatles die Aufnahme von Liedern wie "Back in the USSR", "Ob-La-Di, Ob-La-Da", "Dear Prudence", "Helter Skelter" und "Happiness Is a Warm Gun". Bei George Harrisons Komposition "While My Guitar Gently Weeps" wurde die Gruppe von Eric Clapton unterstützt, der das E-Gitarren-Solo spielte.

"Yellow Submarine", ein Zeichentrickfilm, der auf dem gleichnamigen Lied basiert, feierte am 17. Juli 1968 in London Weltpremiere. Das dazugehörige Soundtrack-Album erschien erst am 13. Januar 1969 und ist die einzige Beatles-LP, die nicht die Spitze der Charts erreichte – möglicherweise, weil die zweite Seite des Albums nur Instrumentalmusik im „Klassikstil“ von George Martin enthält.

Mit der Veröffentlichung von "Hey Jude" am 26. August 1968 (erstmals auf dem Apple-Label) gelang der Gruppe ein weiterer Welterfolg. Das von Paul McCartney komponierte Stück über die Situation von John Lennons Sohn Julian nach der Trennung seiner Eltern wurde zur erfolgreichsten Beatles-Single überhaupt. Am 4. September spielte die Gruppe das Lied in der britischen Fernsehsendung "Frost on Sunday" – der letzte gemeinsame Fernsehauftritt der Beatles. Das Album "The Beatles", wegen seines vollständig weißen Covers auch als "White Album" (: ‚Weißes Album‘) bekannt, erschien am 22. November 1968. Die ersten Auflagen waren mit einer fortlaufenden Nummerierung versehen, sodass jedes Cover ein Unikat war.

Am 1. November 1968 veröffentlichte George Harrison sein erstes Soloalbum "Wonderwall Music", das den Soundtrack zum Film "Wonderwall" darstellt. Am 12. November folgten John Lennon und Yoko Ono mit ihrer Avantgarde-LP "", die vor allem wegen des Coverbildes, das Lennon und Ono nackt zeigt, für Aufregung sorgte.

Am 2. Januar 1969 begannen die Beatles in den Filmstudios von London-Twickenham mit den Dreharbeiten für die Musikdokumentation "Get Back" (später: "Let It Be"). Nach der Idee von Paul McCartney sollte die Gruppe bei der Produktion eines neuen Albums und der Ausarbeitung einer Liveshow von Kameras beobachtet werden. Die Stücke sollten ohne Orchesterbegleitung und Doppelspur-Aufnahmen eingespielt werden. Regisseur war Michael Lindsay-Hogg. Schon nach wenigen Tagen bildeten sich erneut Spannungen zwischen den Bandmitgliedern. Während sich John Lennon zunehmend unbeteiligt zeigte und bei Entscheidungen Yoko Ono für sich sprechen ließ, übernahm Paul McCartney wiederholt die Rolle des Produktionsleiters. George Harrison und Ringo Starr debattierten über das Ausmaß der geplanten Liveshow. Am 10. Januar 1969 verließ Harrison das Aufnahmestudio, nachdem er von McCartney vor laufenden Kameras für sein Gitarrenspiel kritisiert worden war. Nach einem klärenden Gespräch setzten Lennon, McCartney, Harrison und Starr die Dreharbeiten zwölf Tage später in den neu eingerichteten Londoner Apple-Studios fort. Mit dabei war der Keyboarder Billy Preston, der zur Beruhigung der Situation beitrug.

Der geplante Live-Auftritt fand schließlich am 30. Januar 1969 statt: Auf dem Dach der Apple-Studios in der Londoner Savile Row spielten die Beatles (mit Billy Preston) unter freiem Himmel das sogenannte Rooftop Concert. Wegen der zahllosen Schaulustigen drohte ein Verkehrschaos, und einige Anwohner fühlten sich wegen Ruhestörung belästigt. Daher erschien die Polizei vor Ort, und die Beatles brachen die Show nach 42 Minuten ab. Das Konzert endete mit John Lennons Worten:

Nach dem Ende der Dreharbeiten wurde das "Get-Back"-Projekt im Februar vorläufig auf Eis gelegt. Am 12. März 1969 heiratete Paul McCartney die Fotografin Linda Eastman in London. Gleichzeitig wurden George und Pattie Harrison von dem Rauschgiftfahnder Norman Pilcher, der im Herbst 1968 bereits John Lennon und Yoko Ono verhaftet hatte, wegen Drogenbesitzes festgenommen und später zu Geldstrafen verurteilt.

John Lennon und Yoko Ono heirateten am 20. März im britischen Konsulat von Gibraltar. Anstatt einer Hochzeitsreise entschloss sich das Paar, eine Woche lang in einem Hotelbett in Amsterdam für den Frieden zu demonstrieren "(Bed-in)". Später folgten weitere Friedensproteste, darunter ein weiteres Bed-in in Montreal, mehrere Auftritte in Säcken "(Bagism)" und die Rückgabe von Lennons MBE-Orden.

Im Frühling 1969 nahmen die geschäftlichen Probleme der Beatles überhand, als Dick James und dessen Partner Charles Silver ohne Wissen der Gruppenmitglieder ihre Anteile an "Northern Songs Ltd.", die die Lennon/McCartney-Kompositionsrechte beinhalten, an die "Associated Television Corporation" (ATV) verkauften. Im Laufe der Bemühungen zu einem Rückkauf stellte sich heraus, dass Paul McCartney seine Anteile ohne das Wissen von John Lennon erhöht hatte. Für zusätzliche Verwicklungen sorgten Lennon, Harrison und Starr am 8. Mai, als sie den Geschäftsmann Allen Klein gegen den Willen von McCartney als neuen Beatles-Manager einsetzten.

Am 11. April 1969 veröffentlichten die Beatles ihre Single "Get Back". Bereits am 29. Mai folgte das ausschließlich von John Lennon und Paul McCartney eingespielte Stück "The Ballad of John and Yoko".

Trotz der persönlichen und geschäftlichen Differenzen begannen die Beatles am 16. April 1969 mit den Aufnahmen für ein neues Album. Durch die von den Gruppenmitgliedern angestrebte Konzentration auf ihre Musik und die von Produzent George Martin unterstützte Rückbesinnung auf die Machart früherer Arbeiten verlief die Produktion weitgehend harmonisch. Unter einer spürbaren sentimentalen Abschiedsstimmung entstanden Klassiker wie "Come Together", "Golden Slumbers", "Oh! Darling", "Octopus’s Garden" (eine Komposition von Ringo Starr) und "Here Comes the Sun" (Harrison). Am 20. August 1969 waren die Beatles zum letzten Mal zu viert im Studio, und zwei Tage später fanden im Tittenhurst Park die letzten gemeinsamen Fotoaufnahmen statt.

Paul und Linda McCartneys erste Tochter Mary wurde am 28. August 1969 in London geboren.

Am 20. September 1969 unterzeichneten John Lennon, Paul McCartney und Ringo Starr in London einen neuen Tantiemen-Vertrag mit der EMI und Capitol Records. Als McCartney während des Treffens eine Reihe weiterer Beatles-Konzerte vorschlug, erklärte John Lennon seinen Ausstieg aus der Gruppe. Sechs Tage später wurde das Album "Abbey Road", auf dessen berühmtem Cover die Beatles den Zebrastreifen der Londoner Abbey Road überqueren, veröffentlicht. Die LP endet nach einem mehrteiligen Medley mit dem Stück "The End", das mit den Worten schließt:

Die Dreharbeiten für den Promofilm zur Single "Something", die am 6. Oktober erschien, und die Aufnahme einer Weihnachtsplatte für die Fanclubs im November gingen bereits getrennt vonstatten. Nach einer Fernseh- und Presse-Umfrage wurde John Lennon am 2. Dezember 1969 neben John F. Kennedy und Mao Zedong zum "Mann des Jahrzehnts" gewählt.

Im Oktober 1969 und Anfang der 1970er Jahre geriet das "White Album" noch einmal in den Fokus der Weltöffentlichkeit, da das Doppelalbum eine Rolle bei einer Mordserie in Los Angeles spielte, die aufgrund der Prominenz der Opfer ein großes Medieninteresse hervorrief und die Aufklärung der Morde in diesem Zeitraum erfolgte. Der Sektenführer Charles Manson und seine Anhänger meinten aus einzelnen darin veröffentlichten Liedern "(Helter Skelter, Piggies, Blackbird, Honey Pie, Revolution 9)" Botschaften herauszuhören und initiierten nachfolgend eine Mordserie, deren prominentestes Opfer die hochschwangere Schauspielerin Sharon Tate war (siehe: Mansons „Helter Skelter“).

Die Beatles distanzierten sich von Mansons Interpretation ihrer Lieder. Die Verkaufszahlen des "Weißen Albums" stiegen in den USA aufgrund des Medienhypes um den Prozess stark an. Das "White Album" ist bis heute das Beatles-Album mit den meisten Auszeichnungen der RIAA in den USA.

Während sich John Lennon mit seiner Frau Yoko Ono in Dänemark aufhielt, arbeiteten Paul McCartney, George Harrison und Ringo Starr am 3. Januar 1970 in den Abbey Road Studios an der Harrison-Komposition "I Me Mine". Einen Tag später vollendete McCartney sein Stück "Let It Be", das am 6. März 1970 als Single veröffentlicht wurde. Am 27. März 1970 erschien Ringo Starrs erstes Soloalbum "Sentimental Journey".

Ohne das Wissen von Paul McCartney hatte John Lennon den Produzenten Phil Spector mit der Abmischung der Filmbänder vom Januar 1969 beauftragt. Dieser veränderte den ursprünglichen "Get-Back"-Gedanken einer einfachen Aufnahmetechnik und versetzte die Stücke mit klassischer Begleitung und Gesangsdoppelungen. Im Rahmen dieser Arbeiten fand am 1. April 1970 die letzte Beatles-Aufnahmesitzung statt, als Ringo Starr in den Abbey Road Studios für drei der Stücke Schlagzeug-Overdubs einspielte. Das Soundtrack-Album wurde schließlich als "Let It Be" am 8. Mai 1970 veröffentlicht. Der gleichnamige Film feierte am 20. Mai in London Weltpremiere und wurde ein Jahr später mit einem Oscar für die beste Filmmusik ausgezeichnet.

Am 10. April 1970 schickte Paul McCartney der britischen Presse ein in den Vorabexemplaren seiner ersten Solo-LP "McCartney" enthaltenes Interview, in dem er die Trennung der Beatles bekanntgab:

Am gleichen Abend erschien die letzte offizielle Presseerklärung der Beatles:
Nach den Querelen um "Let It Be" und Paul McCartneys Alleingang bei der Bekanntgabe der Trennung war das Verhältnis zwischen ihm und den restlichen Beatles zu Beginn der 1970er Jahre zerrüttet. Keines der Gruppenmitglieder war bei der Weltpremiere des Films anwesend. John Lennon, der das Ende der Beatles in seiner Komposition "God" verarbeitete, zerstreute in einem Interview mit Jann Wenner vom Magazin "Rolling Stone" jegliche Hoffnungen auf eine baldige Wiedervereinigung. Am 31. Dezember 1970 verklagte McCartney die restlichen Ex-Beatles und "Apple" vor dem Londoner High Court auf Beendigung der Geschäftsbeziehungen untereinander. Der Prozess endete am 12. März 1971 zugunsten McCartneys.

Trotz der Beatles-Trennung blieben John Lennon, Paul McCartney, George Harrison und Ringo Starr in den 1970er Jahren in mehr oder weniger engem Kontakt. Zu einem gemeinsamen Treffen aller vier kam es jedoch nicht mehr.

John Lennon schrieb und produzierte für Ringo Starr dessen Stücke "I’m the Greatest" (1973), "Goodnight Vienna" (1974) und "Cookin’" (1976). Paul McCartney unterstützte Starr musikalisch bei den Aufnahmen zu "You’re Sixteen" und "Six O’Clock" (1973) und "Pure Gold" (1976). George Harrison arbeitete 1971 mit Lennon an der LP "Imagine" und mit Starr an "Ringo" (1973) und "Ringo’s Rotogravure" (1976). Ringo Starr selbst war auf mehreren Soloalben seiner ehemaligen Kollegen vertreten, darunter "John Lennon/Plastic Ono Band" (Lennon 1970), "All Things Must Pass" (Harrison 1970) und "Living in the Material World" (Harrison 1973).

Am schwierigsten gestaltete sich die Beziehung zwischen John Lennon und Paul McCartney. McCartney griff seinen ehemaligen Partner 1971 in einigen Stücken seines Albums "Ram" persönlich an. Lennon konterte seinerseits mit der bösartigen Komposition "How Do You Sleep?". Erst 1972 kam es zu einer Versöhnung. Die enge Freundschaft aus Beatles-Tagen sollte sich jedoch nicht wiederbeleben lassen.

Am 12. Mai 1971 besuchten Paul McCartney und Ringo Starr die Hochzeit von Mick Jagger und Bianca Morena de Macías in Saint-Tropez. Starr spielte am 1. August auf George Harrisons "Konzert für Bangladesch" im New Yorker Madison Square Garden. 1972 nahmen Ringo Starr, George Harrison und John Lennon gemeinsam mit Klaus Voormann (Bass) und Billy Preston (Orgel) den Titel "I’m the Greatest" auf, der 1973 auf Starrs Soloalbum "Ringo" veröffentlicht wurde.

Im März des folgenden Jahres nahmen die drei an einer Geschäftsbesprechung von Capitol Records in New York teil. Am 2. April 1973 wurden die äußerst erfolgreichen Greatest-Hits-Alben "1962–1966 und 1967–1970", auch bekannt als „Rotes“ und „Blaues Album“, veröffentlicht. Im Herbst kaufte Ringo Starr das Anwesen "Tittenhurst Park" von John Lennon, der mit Ono nach New York gezogen war. Am 2. November verklagten Lennon, Harrison und Starr ihren Ex-Manager Allen Klein wegen der Unterschlagung von Tantiemenzahlungen. Der Prozess wurde ein Jahr später zu ihren Gunsten entschieden.

Am 28. März 1974 besuchte Paul McCartney in Los Angeles die Burbank Studios, wo John Lennon an einem Album arbeitete. Es kam zu einer Jam-Session, die auch aufgenommen wurde. Diese Aufnahmen erschienen bislang nur auf Bootlegs. Es war das letzte Mal, dass Lennon und McCartney gemeinsam Musik machten. Am 19. Dezember versäumte John Lennon einen gemeinsamen Termin im New Yorker Plaza Hotel, bei dem die Auflösung der Beatles-Geschäftspartnerschaft offiziell besiegelt werden sollte. Er unterzeichnete den Vertrag erst eine Woche später in Disney’s Polynesian Village Resort in Florida.

Der ehemalige Roadie und enge Beatles-Freund Mal Evans wurde am 4. Januar 1976 in Los Angeles von der Polizei erschossen. Am 26. Januar lief der Beatles-Plattenvertrag mit der EMI endgültig aus. Nach einer Wiederveröffentlichung aller Beatles-Singles in Großbritannien war die Gruppe im April mit 23 Songs in den Top-100-Charts vertreten. Am 24. April, einige Wochen nachdem den Beatles eine siebenstellige Summe für eine Wiedervereinigung angeboten worden war, war Paul McCartney bei John Lennon in dessen Wohnung im New Yorker Dakota-Haus zu Gast. Abends sahen sie sich gemeinsam die NBC-Fernsehsendung "Saturday Night Live" an, als Moderator Lorne Michaels den Beatles „den tariflichen Mindestlohn“ (rund 1000 US-Dollar) anbot, wenn sie noch während der Show vorbeikämen. Die beiden hielten die Idee für „cool“, aber entschieden sich dann doch gegen einen spontanen Auftritt.
Einen Tag später kam McCartney erneut zu Besuch, wurde aber von Lennon an der Tür abgewiesen. Es war das letzte Treffen der beiden Musiker.

Am 2. Mai 1977 wurde der Mitschnitt des letzten Beatles-Konzerts im Hamburger Star-Club von 1962 auf einem Album veröffentlicht. Vier Tage später folgte die LP "The Beatles at the Hollywood Bowl", die Live-Aufnahmen aus den Jahren 1964/1965 enthält.

Die Beatles-Parodie "The Rutles – All You Need Is Cash" (unter anderem mit George Harrison, Eric Idle, Bill Murray, Dan Aykroyd und John Belushi) feierte am 22. März 1978 auf NBC Premiere. Im selben Jahr verklagte Apple Corps die damals aufstrebende Computerfirma Apple wegen Verletzung von Markenrechten. Eine Einigung wurde erzielt, in der sich die Computerfirma verpflichtete, sich auf den Bereich der Computer zu beschränken und nicht im Musikgeschäft tätig zu werden.

Am 19. Mai 1979 besuchten Paul McCartney, George Harrison und Ringo Starr die Hochzeit von Eric Clapton und Harrisons Ex-Frau Pattie Boyd in Ewhurst (Surrey). Während der Feier spielten die drei Ex-Beatles ein Überraschungskonzert mit mehreren Rock-’n’-Roll-Klassikern. Am 24. Oktober 1979 wurde Paul McCartney, der mit seiner neuen Band Wings auch in den 1970er Jahren zahlreiche Hits landete, vom Guinness-Buch der Rekorde als „erfolgreichster Komponist aller Zeiten“ mit einer Rhodium-Schallplatte ausgezeichnet.

John Lennon und George Harrison besuchten am 28. September 1980 eine Aufführung der Comedy-Truppe Monty Python in der Hollywood Bowl. Am 28. November stimmte Lennon mit seiner Unterschrift einer eidesstattlichen Erklärung zu, die ein einmaliges Beatles-Konzert als Werbekampagne für den geplanten Dokumentarfilm "The Long and Winding Road" vorsah.

Am 8. Dezember 1980 wurde der 40-jährige John Lennon, der gerade mit dem Album "Double Fantasy" sein musikalisches Comeback gefeiert hatte, vor seinem Wohnhaus in New York von dem psychisch gestörten Fan Mark David Chapman erschossen. Paul McCartney (gerade in London), George Harrison (in Henley-on-Thames) und Ringo Starr (auf den Bahamas) wurden telefonisch über den Tod ihres Ex-Partners informiert. Starr flog am 9. Dezember mit seiner Freundin Barbara Bach nach New York und besuchte Yoko Ono und Sean Lennon im Dakota-Haus. Eine Woche später trafen sich die restlichen Ex-Beatles mit Ono im Plaza Hotel, um die geschäftlichen Konsequenzen von Lennons Ermordung zu diskutieren.

Als Ringo Starr am 27. April 1981 die Schauspielerin Barbara Bach in London heiratete, gehörten auch Paul und Linda McCartney sowie George und Olivia Harrison zu den 70 Gästen. Während der Feier traten McCartney, Harrison und Starr mit Harry Nilsson und Ray Cooper als "All-Star-Band" auf. Am 11. Mai veröffentlichte George Harrison die John-Lennon-Tribut-Single "All Those Years Ago", die musikalische Beiträge von Paul McCartney und Ringo Starr enthält. Starrs Album "Stop and Smell the Roses" vom 27. Oktober beinhaltete je zwei McCartney- und Harrison-Kompositionen.

Die am 26. April 1982 veröffentlichte Paul-McCartney-LP "Tug of War" enthält das John-Lennon-Tribut-Stück "Here Today". Ringo Starr spielte bei "Take It Away" Schlagzeug. Am 24. Mai besuchten George Harrison und Ringo Starr die Aufführung des Films "The Cooler" (mit Starr und McCartney) auf dem Filmfestival von Cannes. Am 23. Juni spielten Paul McCartney und Ringo Starr ein gemeinsames Konzert vor Fanclub-Mitgliedern in den Elstree Film Studios in Hertfordshire.

Den möglichen Rückkauf der Firma "Northern Songs" erörterten Paul McCartney, George Harrison und Ringo Starr bei einem Treffen am 25. Juli 1983 im Londoner Gore-Hotel. Im September klagten die drei Ex-Beatles gegen eine Veröffentlichung der Beatles-Fan-Club-Weihnachtssingles aus den Jahren 1963 bis 1969. Bei einem weiteren Geschäftstreffen mit Yoko Ono am 1. Dezember wurde über die Zukunft von Apple diskutiert.

Am 25. Oktober 1984 feierte der Paul-McCartney-Kinofilm "Broad Street" in New York Weltpremiere. Der Film und der Soundtrack "Give My Regards to Broad Street" entstanden in enger Zusammenarbeit mit Ringo Starr. Im Dezember verhinderten die drei Ex-Beatles die Veröffentlichung des Albums "The Beatles Sessions", das bisher unveröffentlichte Aufnahmen enthalten hätte.

Im Jahr 1985 flammten die geschäftlichen Auseinandersetzungen zwischen den Ex-Beatles erneut auf. Am 25. Februar wurde Paul McCartney von George Harrison, Ringo Starr und Yoko Ono verklagt, da er angeblich seine Tantiemen-Quoten an den Beatles-Platten heimlich erhöht hatte. Am 4. April feierte der von George Harrison produzierte Film "Wasser – Der Film" Premiere, in dem Harrison unter anderem mit Ringo Starr und Eric Clapton das Lied "Freedom" singt. Am 10. August kaufte Michael Jackson für 47,5 Millionen Dollar die Rechte an allen 251 Lennon/McCartney-Kompositionen. Der unterlegene Bieter Paul McCartney war entsetzt, da Yoko Ono ihm versichert hatte, ein Angebot von fünf Millionen Dollar würde ausreichen.

Ab dem 26. Februar 1987 wurden sämtliche Beatles-Alben auf CD neu veröffentlicht. Dabei stieg "Sgt. Pepper" zum 20-jährigen Jubiläum bis auf Rang 3 der britischen Hitparade. Am 5. Juni traten George Harrison und Ringo Starr gemeinsam bei der "Prince’s Trust Rock Gala" in der Londoner Wembley Arena auf. Auf dem am 2. November veröffentlichten Harrison-Album "Cloud Nine", das auch den Beatles-Tribut-Song "When We Was Fab" enthält, spielte Starr Schlagzeug.

George Harrison und Ringo Starr nahmen am 20. Januar 1988 an der Einführung der Beatles in die "Rock and Roll Hall of Fame" im New Yorker Waldorf Astoria-Hotel teil. Paul McCartney blieb wegen der andauernden geschäftlichen Differenzen fern. Harrison und Starr gaben mit Bob Dylan, Mick Jagger, Bruce Springsteen und Mike Love ein All-Star-Konzert. Am 8. März wurden die Alben "Past Masters Vol. 1 & 2" veröffentlicht, die eine Zusammenfassung aller Beatles-Singles und EPs enthalten.

Am 8. November 1989 wurden die Plattenfirmen EMI und Capitol dazu verurteilt, 100 Millionen US-Dollar unbeglichene Tantiemen an die Ex-Beatles nachzuzahlen. Ende des Monats enthielt eine Presseerklärung von George Harrison das Zitat, die Beatles würden sich nicht wiedervereinigen „solange John Lennon tot bleibt“.

Im April 1990 wurden vier in den Jahren 1983/1984 entdeckte Asteroiden von der Internationalen Astronomischen Union in "Lennon", "McCartney", "Harrison" und "Starr" umbenannt. Im Oktober 1991 endete der zweite Rechtsstreit zwischen Apple Corps und der Computerfirma Apple in einer außergerichtlichen Einigung.

Am 6. April 1992 trat Ringo Starr auf einem von George Harrison organisierten Wohltätigkeitskonzert zugunsten der Meditiations-Organisation Natural Law Party in der Londoner Royal Albert Hall auf. Am 8. Oktober trafen sich McCartney, Harrison und Starr mit Yoko Ono zu einer Geschäftsbesprechung in der Londoner MPL-Zentrale.

Bereits 1970 und 1980 hatte es Pläne für eine groß angelegte Fernsehdokumentation unter dem Arbeitstitel "The Long and Winding Road" gegeben, bei der Neil Aspinall die Geschichte der Beatles aufarbeiten sollte. Kurz vor der Ermordung John Lennons waren die Verhandlungen mit den Ex-Mitgliedern, die als Teil der Werbekampagne sogar einem gemeinsamen Live-Auftritt zugestimmt hatten, weit fortgeschritten. Im Oktober 1987 einigten sich McCartney, Harrison und Starr bei einem Treffen in London schließlich auf eine Verwirklichung des Projekts "The Beatles Anthology", das neben der Fernsehproduktion und einem Buch auch drei Alben und drei neue Singles vorsah. Unter der Regie von Geoff Wonfor begannen im Mai 1992 die Dreharbeiten für die aufwendige Dokumentation, in der die Ex-Beatles in stundenlangen Interviews, die mit Originalton- und -filmaufnahmen untersetzt sind, ihre eigene Geschichte erzählen. Neben Kommentaren der engen Vertrauten George Martin, Neil Aspinall und Derek Taylor wurden auch Archivzitate von John Lennon eingefügt.

Am 19. Januar 1994 wurde John Lennon postum in Anwesenheit von Paul McCartney und Yoko Ono im New Yorker Waldorf-Astoria-Hotel als Solointerpret in die "Rock and Roll Hall of Fame" aufgenommen. Auf der anschließenden Pressekonferenz bestätigte McCartney die Gerüchte um eine bevorstehende Beatles-Reunion. Ono übergab McCartney vier von Lennon in den 1970er Jahren angefertigte Demobänder, die bis dato unveröffentlichte Stücke enthielten.

Paul McCartney, George Harrison und Ringo Starr begannen am 11. Februar 1994 in McCartneys Heimstudio in Peasmarsh (Sussex) mit den Aufnahmen für die erste neue Beatles-Single seit 25 Jahren. Unter der Produktion von Jeff Lynne wurden dabei die neu eingespielten Gesangs- und Instrumentalspuren mit Hilfe modernster Technik dem digital gesäuberten "Free as a Bird"-Demoband von John Lennon hinzugefügt.

Ein Querschnitt der Beatles-Rundfunkaufnahmen aus den Jahren 1963 bis 1965 wurde am 30. November 1994 auf dem Doppel-Album "Live at the BBC" veröffentlicht, das die Spitze der britischen Hitparade erreichte. Ringo Starrs ehemalige Frau Maureen Tigrett starb am 30. Dezember im Alter von 48 Jahren in Los Angeles an den Folgen einer Leukämie-Knochenmarktransplantation.

Am 6. Februar 1995 nahm die Gruppe in Peasmarsh die Arbeit zu ihrer zweiten neuen Single "Real Love" auf. Die Produktion gestaltete sich diesmal schwieriger, da die Tonqualität des Demobandes katastrophal war. Die Vollendung der weiteren neuen Stücke, "Now and Then" und "Grow Old with Me", wurde einen Monat später wegen Meinungsverschiedenheiten abgebrochen.

In Anwesenheit von George Martin begannen die Beatles am 22. Mai 1995 in den Londoner Abbey Road Studios mit der Auswahl und Abmischung der Stücke für die geplanten drei "Anthology"-Alben, die Outtakes und unveröffentlichte Stücke der Gruppe enthalten sollten. Ihr langjähriger Freund Klaus Voormann wurde mit der Covergestaltung beauftragt. Im Oktober und November fanden Dreharbeiten für die Videoclips zu "Free as a Bird" und "Real Love" statt.

Begleitet von einer monströsen Werbekampagne feierte am 19. November 1995 der erste Teil der fünfstündigen Dokumentation "The Beatles Anthology" im US-Fernsehen Weltpremiere. Zwei Tage später wurde das Doppelalbum "Anthology 1" mit Titeln aus den Jahren 1958 bis 1964 veröffentlicht. Es erreichte die Spitze der US-Hitparade. Am 4. Dezember kam die Single "Free as a Bird" in die Läden, die entgegen allen Erwartungen hinter Michael Jacksons "Earth Song" auf Rang 2 der britischen Hitlisten stehen blieb.

Am 4. März 1996 erschien die zweite Comeback-Single "Real Love", die vom britischen BBC-Rundfunk wegen „tontechnischer Gründe“ aus dem Programm verbannt wurde. "Anthology 2" (mit Titeln von 1965 bis 1968) erschien am 18. März und stieg beiderseits des Atlantiks an die Spitze der Hitlisten. Der Beatles-Produzent George Martin wurde am 15. Juni zum Ritter geschlagen. Am 7. Oktober erschien die "Anthology" in einer achtteiligen Videobox.

In einer Presseerklärung gaben McCartney, Harrison und Starr am 17. Oktober 1996 bekannt, dass es keine weiteren gemeinsamen Aufnahmen geben würde. Das am 28. Oktober veröffentlichte Doppelalbum "Anthology 3" (mit Titeln von 1968 bis 1970) erreichte wiederum Rang 1 der US-Hitlisten. Die Beatles sind damit die einzige Gruppe, von denen zweimal innerhalb von zwölf Monaten drei Alben die Spitze der US-LP-Hitparade erreichten (1964 und 1996). Insgesamt wurde die Gruppe für die dreiteilige "Anthology" mit 15 Multi-Platin-Schallplatten und drei Grammy Awards ausgezeichnet.

Am 11. März 1997 wurde Paul McCartney im Londoner Buckingham Palace von Königin Elisabeth II. zum Ritter geschlagen. Zwei Monate später veröffentlichte er das Album "Flaming Pie" (benannt nach Lennons Äußerung zur Entstehung des Namens „Beatles“), das musikalische Beiträge von Ringo Starr enthält. McCartney revanchierte sich ein Jahr später bei dessen CD "Vertical Man". Am 14. Oktober besuchten die beiden Ex-Beatles gemeinsam die Weltpremiere von McCartneys Orchesterwerk "Standing Stone" in der Royal Albert Hall.

Im Mai 1998 klagten Paul McCartney, George Harrison und Ringo Starr gegen eine Plattenfirma, die den Mitschnitt eines Star-Club-Konzerts der Beatles auf CD veröffentlicht hatte. Am 8. Juni nahmen McCartney, Harrison und Starr in London an der Trauerfeier für Linda McCartney teil, die am 17. April in Tucson (Arizona) ihrem Brustkrebsleiden erlegen war. Am 28. Dezember wurden die Beatles mit einem Stern auf dem "Hollywood Walk of Fame" verewigt.

Am 14. September 1999 wurde der digital restaurierte Zeichentrickfilm "Yellow Submarine" auf DVD veröffentlicht. Gleichzeitig erschien ein neu zusammengestelltes und abgemischtes Soundtrack-Album. Zum Ende des Jahres wurden die Beatles in einer landesweiten Umfrage unter 600.000 britischen Bürgern zu den "Bedeutendsten Künstlern" und zur "Besten Musikgruppe" des 20. Jahrhunderts gewählt. "Sgt. Pepper’s" gewann die Wahl zum "Besten Album".

Am 30. Dezember wurde George Harrison bei einem Messerattentat in seiner Villa in Henley-on-Thames im Brustbereich verletzt.

George Harrison und Ringo Starr waren am 17. September 2000 bei einer Party des Formel-1-Piloten Damon Hill in Hambledon (Surrey) zu Gast, wo sie zusammen mit Leo Sayer ein kurzes Konzert gaben. Am 5. Oktober erschien das Buch "The Beatles Anthology" mit den Abschriften der Interviews für die gleichnamige Fernsehdokumentation. Mit dem am 17. November veröffentlichten Greatest-Hits-Album "1" kehrten die Beatles noch einmal in 34 Ländern (unter anderem in Deutschland, Großbritannien und den USA) an die Spitze der Charts zurück. Die bestverkaufte CD des Jahres qualifizierte sich allein in der ersten Woche weltweit für 35 Platin-Schallplatten und verkaufte sich bis zum Jahr 2006 über 24 Millionen Mal.

Bereits im August/September 1997 hatte sich George Harrison nach der Entfernung eines geschwollenen Lymphknotens in London einer Radium-Strahlentherapie unterzogen. Im Januar 1998 folgte eine Behandlung in der renommierten Mayo Clinic von Rochester (Minnesota). Am 21. März 2001 wurde Harrison mit Verdacht auf Lungenkrebs erneut ins Krankenhaus eingeliefert. Bei der anschließenden Operation wurde ein bösartiges Krebsgeschwür von der Lunge entfernt. Paul McCartney besuchte seinen Ex-Kollegen am 8. Mai während dessen Erholungsurlaubs in der Toskana. Nur eine Woche später diagnostizierten Ärzte der Krebsklinik von Bellinzona einen schweren Hirntumor. Das letzte Treffen von Paul McCartney, George Harrison und Ringo Starr fand am 12. November im University Hospital von Staten Island (New York) statt, wo Harrison mit einer starken Chemotherapie behandelt wurde.

Am 29. November 2001 starb George Harrison im Alter von 58 Jahren in Los Angeles (Kalifornien).

Paul McCartney und Gerry & the Pacemakers traten am 24. Februar 2002 bei einem Gedenkkonzert für George Harrison im "Empire Theatre" von Liverpool auf. Im Juli desselben Jahres nahmen die beiden Ex-Beatles McCartney und Starr mit George Martin an einer Gedenkveranstaltung für Harrison in dessen Haus in Henley-on-Thames teil. Am 29. November wurde in der Londoner Royal Albert Hall das "Concert for George" veranstaltet. Mit dabei waren neben Paul McCartney und Ringo Starr u. a. Eric Clapton, Tom Petty, Jeff Lynne, Billy Preston und die Comedy-Truppe Monty Python.

Das am 25. März 2003 veröffentlichte Ringo-Starr-Album "Ringo Rama" enthält das George-Harrison-Tribut "Never Without You". Die "Beatles-Anthology"-Dokumentation erschien am 31. März in einem 5er-DVD-Set. Am 24. September besuchten Paul McCartney, Ringo Starr und Yoko Ono eine Vorab-Premiere des Films "Concert for George" in den Warner-Brothers-Studios von Burbank (Los Angeles County). Am 17. November erschien unter dem Titel "Let It Be… Naked" eine „Rohversion“ des Beatles-Albums von 1970.

Am 9. September 2004 wurden die Beatles in die "UK Music Hall of Fame" aufgenommen. Der ehemalige Quarrymen-Gitarrist Eric Griffiths starb am 29. Januar 2005 im Alter von 64 Jahren in Edinburgh an Magenkrebs. Am 15. Dezember desselben Jahres verklagte die Firma Apple Records im Namen der Ex-Beatles den EMI-Konzern wegen Hinterziehung von Beatles-Tantiemen in Höhe von 30 Millionen Pfund.

Nach dem erneuten Aufflammen des Konflikts im Jahr 2003 begann am 29. März 2006 vor dem Londoner High Court der Prozess zwischen der Beatles-Firma Apple Corps und dem gleichnamigen Computer-Konzern. Das Gericht erkannte jedoch nicht an, dass die Elektronikfirma das von den Beatles entworfene und geschützte Apfel-Logo für Musik verwendet. Das Apple-Logo würde lediglich für Hardware-Programme um einen Online-Musikdienst, jedoch nicht für den Musikverkauf an sich genutzt.

Der ehemalige Beatles-Keyboarder Billy Preston starb am 6. Juni 2006 im Alter von 59 Jahren in Scottsdale (Arizona). Paul McCartney und Ringo Starr besuchten am 30. Juni 2006 die Weltpremiere der in Zusammenarbeit mit dem Cirque du Soleil entstandenen offiziellen Beatles-Musical-Show "Love" im "The Mirage" von Las Vegas.

Im August 2006 wurde das Album "Sgt. Pepper’s Lonely Hearts Club Band" nach einer BBC-Umfrage zum „besten Album aller Zeiten“ gewählt.

In den Top Ten dieser Umfrage finden sich drei weitere Beatles-Alben, "Revolver" (Rang 6), "Abbey Road" (8) und "The Beatles" "(„Weißes Album“)" (10). Die Hörer durften anlässlich der seit 50 Jahren bestehenden Album-Hitparade aus allen Alben wählen, die sich an der Spitzenposition platzieren konnten.

Am 17. November 2006 erschien "Love". Es handelt sich bei diesem Album um neue Abmischungen bekannter Beatles-Stücke durch George Martin und dessen Sohn Giles Martin. Die Grundlage für dieses Album bildet die gleichnamige Show des "Cirque du Soleil" in Las Vegas.

Am 5. Februar 2007 verkündete die Aktiengesellschaft Apple Inc. unter der Führung ihres CEO Steve Jobs, dass sie mit der Beatles-Firma Apple Corps Ltd. einen neuen Vergleich erzielt habe. Die Apple Inc. ist dem Vergleich zufolge Eigentümerin aller Markenrechte, die mit dem Namen "Apple" zu tun haben und wird bestimmte Rechte an die Beatles-Firma lizenzieren. Finanzielle Details wurden nicht genannt. Bereits im Zuge der Präsentation von Apples markenrechtlich ebenfalls umstrittenen iPhone auf der Macworld Conference & Expo 2007 zeichnete sich ab, dass mit Apple Records zumindest Verhandlungen geführt wurden, da unter anderem in Verbindung mit dem Mobiltelefon das Schallplattencover "Sgt. Pepper’s Lonely Hearts Club Band" der Beatles dargestellt wurde, deren Musik bis zum 16. November 2010 nicht über den wettbewerbsrechtlich umstrittenen iTunes Store zu beziehen war.

Neil Aspinall zufolge sollen alle 13 Alben der Beatles, die 1987 auf CD veröffentlicht wurden, zum gleichen Zeitpunkt bei allen Internetdiensten heruntergeladen werden können. Damit ist die Kompilation "Hey Jude" aus dem Jahr 1969 weiterhin nur als Vinylversion verfügbar.

Im November 2007 erschien eine Neuveröffentlichung des Films "Help!" aus dem Jahr 1965. Der Film wurde in einer Doppel-DVD-Special-Edition veröffentlicht. Ein Making-of und eine Dokumentation über die Restauration des Films sind unter anderem als Extramaterial enthalten.

Aus Anlass des 40. Jahrestages der Aufnahme des Beatles-Songs "Across the Universe" strahlte die NASA am 4. Februar 2008 um 19 Uhr EST (5. Februar 2008, 1 Uhr MEZ) den Song in Richtung Polarstern, Sternbild Ursa Minor (Kleiner Bär), über ihre Madrider – zum "Deep-Space"-Antennennetz gehörende – Anlage aus. Das Datum markiert drei weitere Ereignisse: 50 Jahre zuvor war die NASA gegründet worden, 45 Jahre zuvor gründete sie das "Deep-Space"-Projekt, und 40 Jahre zuvor hatten die USA mit "Explorer 1" ihren ersten NASA-Satelliten ausgesetzt. Das als MP3 kodierte, etwa vier Minuten lange Lied benötigt 431 Jahre, bis es im Jahr 2439 die 2,5 Billiarden Kilometer entfernte Polarsternregion erreicht haben wird.

Geschrieben hatte das Stück John Lennon, der es später als eine seiner besten Kompositionen bezeichnen sollte. Paul McCartney bat in einer Grußbotschaft an die NASA, seine „Grüße an die Aliens“ zu schicken. Yoko Ono sprach vom „Beginn eines neuen Zeitalters, in dem wir mit Milliarden von Planeten kommunizieren werden“. Beatles-Fans riefen den 4. Februar 2008 zum "Across the Universe Day" aus und spielten den Song zum Zeitpunkt der Übertragung weltweit mit. Das Ereignis wurde von der NASA via Internet live übertragen, untermalt mit einem Video, das den Song illustriert.

Bei einem Benefizkonzert der David Lynch Foundation for Consciousness-Based Education and World Peace am 4. April 2009 in der Radio City Music Hall, New York, betraten Paul McCartney und Ringo Starr zum ersten Mal seit 2002 wieder gemeinsam die Bühne. Das Konzert stand unter dem Motto „Change Begins Within“; mit den Einnahmen will die Stiftung sozial benachteiligten Kindern die Möglichkeit geben, Transzendentale Meditation zu erlernen: jene Meditationstechnik, die die Beatles 1968 im indischen Rishikesh bei Maharishi Mahesh Yogi erlernt hatten. Mit von der Partie: Donovan und Paul Horn (Weggefährten auf der damaligen Indienreise), Ben Harper, Sheryl Crow, Pearl Jams Frontmann Eddie Vedder, Moby, Bettye LaVette und Jim James.

Kurz nach dem Benefizkonzert in New York wurde berichtet, dass Paul McCartney und Ringo Starr weiter musikalisch zusammenarbeiten wollen. Noch in diesem Jahr solle ein gemeinsames Album fertiggestellt werden.

Nach vierjähriger Vorbereitung erschienen am 9. September 2009 alle 13 Studioalben der Beatles in einer neu gemasterten Fassung. Erstmals sind damit alle Beatles-Alben in Stereo auf CD erhältlich. Die CDs gibt es einzeln oder als Box mit 16 CDs. Auch eine Mono-Version der Alben, die ursprünglich als Monoversion veröffentlicht wurden, ist im neuen Mastering erhältlich.

Im November 2013 wurde ein zweiter Teil des 1994 erschienenen Albums "Live at the BBC" unter dem Namen "On Air – Live at the BBC Volume 2" veröffentlicht. Gleichzeitig wurde "Live at the BBC" einem Remastering unterzogen und in neuer Aufmachung wiederveröffentlicht. Im Dezember 2013 folgte die digitale Kompilation "The Beatles Bootleg Recordings 1963", die ausschließlich bei iTunes erhältlich ist. Sie enthält 59 bisher unveröffentlichte Aufnahmen aus dem Jahr 1963, davon 15 Studio-Outtakes, 42 BBC-Aufnahmen sowie die zwei Demos "Bad to Me" und "I’m in Love".

Am 20. März 2018 wurde auch Ringo Starr zum Ritter geschlagen.

Die Beatles haben während ihrer musikalischen Karriere eine beachtliche Anzahl von Rekorden aufgestellt, von denen einige bis heute Bestand haben.

Obwohl sie nur über einen Zeitraum von acht Jahren Plattenaufnahmen machten, sind die Beatles die Gruppe mit den meistverkauften Tonträgern der Welt. Laut Angaben ihrer Plattenfirma EMI beläuft sich die Anzahl der verkauften Einheiten auf über eine Milliarde.

Die Beatles haben mehr Multi-Platin-Auszeichnungen erhalten als jeder andere Künstler (13 in den USA).

Die Beatles hatten mehr Nr.-1-Singles als alle anderen Gruppen oder Sänger (22 in den USA, 23 in Australien, 23 in den Niederlanden, 22 in Kanada, 21 in Norwegen, 18 in Schweden, 12 in Deutschland).

Die Beatles hatten mehr Nr.-1-Alben als andere Einzelkünstler und Gruppen (19 in den USA und 15 in Großbritannien).

Die Beatles verbrachten die meisten gezählten Wochen auf Platz 1 in den Album-Charts (132 in den USA und 174 in Großbritannien).

Die erfolgreichste erste Woche von Verkäufen für ein Doppelalbum erreichten die Beatles mit "The Beatles Anthology Volume 1". Im Zeitraum vom 21. November bis zum 27. November 1995 wurde das Album 855.797 Mal in den USA verkauft.

Lennon und McCartney sind die bisher erfolgreichsten Komponisten der Popgeschichte mit 28 Nr.-1-Singles in den USA für McCartney und 21 für Lennon (19 Titel wurden gemeinsam geschrieben). Lennon war verantwortlich für 21 Nummer-eins-Singles in Großbritannien, und McCartney hatte dort 23 Erfolge (18 Titel wurden gemeinsam geschrieben).

Während der ersten Aprilwoche 1964 hielten die Beatles die ersten fünf Plätze der "Billboard Hot 100", was niemandem zuvor gelungen war. Die fünf Titel waren "Can’t Buy Me Love", "Twist and Shout", "She Loves You", "I Want to Hold Your Hand" und "Please Please Me".

In der Woche vom 11. bis zum 17. April 1964 hielten die Beatles 14 Plätze in den "Billboard Hot 100". Vor den Beatles war neun die höchste Anzahl von gleichzeitigen Titeln eines Interpreten in den "Hot 100" (Elvis Presley, 19. Dezember 1956).

Die Beatles waren die einzigen, die sich selbst zweimal auf dem ersten Platz der "Billboard Hot 100" ablösten. Dieses Kunststück gelang ihnen Anfang des Jahres 1964. Dem Titel "I Want to Hold Your Hand" folgte "She Loves You", und nach dieser Single kam "Can’t Buy Me Love" auf Platz 1. Boyz II Men, OutKast und Elvis Presley gelang es jeweils einmal, sich selbst direkt mit einem eigenen Titel von Platz 1 abzulösen.

Die Beatles waren die ersten, deren Live-Auftritt weltweit im Fernsehen übertragen wurde. Diese Übertragung fand unter Beteiligung von 26 Ländern am 25. Juni 1967 statt und hatte in aller Welt 600 Millionen Zuschauer. Jedes Land lieferte einen Beitrag. So konnte man zum Beispiel eine Geburt in Mexiko miterleben. Und man war dabei, als in England die Beatles den Song "All You Need Is Love" live spielten, nur ein Teil der Instrumentierung war Playback. Viele Freunde der Beatles sangen am Ende des Songs mit, als dieser langsam ausgeblendet wurde, z. B. Mick Jagger, Keith Richards, Marianne Faithfull, Eric Clapton und Keith Moon. Der Auftritt fand im Studio 1 der Abbey Road Studios statt, wo sie von 1962 bis 1970 die meisten ihrer Titel aufnahmen.

In den USA wurden sie am 14. Juni 2005 zum sechsten Mal mit einem Diamond Award ausgezeichnet, den die "Recording Industry Association of America" (RIAA) für mehr als zehn Millionen in den USA verkaufte Alben verleiht. Die im Jahr 2000 erschienene Greatest-Hits-Compilation "1" hatte diese imposante Zahl überschritten. Das in den USA bestverkaufte Beatles-Album ist noch immer das 1968 erschienene "The Beatles" "(„White Album“)". Insgesamt belaufen sich die Verkäufe von Tonträgern der Beatles in den USA auf 170 Millionen Alben (Stand: 2008).

Die Beatles werden vielfach als wichtigste Popband der 1960er Jahre bezeichnet
und haben nahezu alle Genres der Pop-/Rockmusik nachhaltig beeinflusst. Bis zum Jahr 2012 haben sie mehr als 600 Millionen Tonträger verkauft. Anfänglich standen sie namensgebend für die Musikrichtung des "Beats". Dies war ein gitarrenlastiger Musikstil, der sich durch schnellen -Takt und eingängige Melodien auszeichnete. Seine Bedeutung liegt darin, dass er zum ersten Mal einen eigenständigen Rockstil in Großbritannien etablierte und damit eine musikalische Unabhängigkeit von den US-amerikanischen Vorbildern ermöglichte.

Ab 1964 begannen die Beatles, sich musikalisch weiterzuentwickeln und mit anderen Musikstilen zu experimentieren. So zeigt das Album "Beatles for Sale" deutliche Einflüsse aus der Folk- und Country-Musik. Das folgende Album "Help!" enthält erstmals Gastmusiker, und zwar – wie nachfolgend regelmäßig – aus dem klassischen Bereich: Unter anderem ist mit der Ballade "Yesterday" ein Stück zu hören, in dem der Sänger (McCartney) neben seiner Akustikgitarre nur von einem Streichquartett begleitet wird. Auf dem 1965 veröffentlichten Album "Rubber Soul" wurden erstmals indische Einflüsse hörbar. Das 1966er Album "Revolver" beeindruckte durch eine breite Palette unterschiedlichster Pop-Derivate. Darin zeigte sich eine wesentliche Stärke der Beatles: Sie vermochten verschiedenste Geschmäcker zu bedienen, ohne dabei ihren eigenen Charakter zu verlieren.

Mit dem Album "Sgt. Pepper’s Lonely Hearts Club Band" von 1967 legten sie eines der ersten Konzeptalben der Popgeschichte vor, das auch von "Pet Sounds" der Beach Boys inspiriert ist.
"Sgt. Pepper" war auch die erste Platte in der Popgeschichte, auf deren Cover die Texte der Lieder abgedruckt waren. Mit der Doppel-LP "The Beatles" – wegen des Covers gemeinhin „White Album“, das „Weiße Album“, genannt – demonstrierten die Beatles noch einmal ihr gesamtes musikalisches Spektrum, unter anderem mit "Helter Skelter", einem der ersten Hard-Rock-Stücke der Rockgeschichte.

Somit bediente die Musik der Beatles Stilrichtungen von Rock ’n’ Roll, Beat und Balladen über Country, Folk, Music Hall und Blues bis zu Psychedelic Rock, klassischer Musik und auch Hardrock.

Die Musikzeitschrift "Rolling Stone" wählte die Beatles im Jahr 2004 auf den ersten Platz der 100 größten Musiker aller Zeiten.

Bis zu den Beatles hatten lediglich Frank Sinatra in den 1940er sowie in den 1950er Jahren Elvis Presley eine ähnliche Welle der Begeisterung und fanatischen Verehrung ausgelöst. Dabei nahmen die Beatles gegenüber den Rolling Stones die Rolle der „braven Jungs“ ein und blieben auch textlich noch recht lange dem harmlosen Schlager treu. Doch im Rahmen der geistigen und gesellschaftlichen Veränderungen der späten 1960er Jahre änderten die Beatles ihren Stil. Ab 1965 wurden die Texte, inspiriert durch Bob Dylan, allmählich tiefgründiger und persönlicher. Die Kompositionen und Arrangements wurden zunehmend komplexer.

In Liedern wie "She Said, She Said" verarbeiteten sie die ab 1966 gemachten Erfahrungen mit LSD. Sie beschäftigten sich mit indischer Spiritualität und bekannten sich 1967 zu dem Guru Maharishi Mahesh Yogi und seiner Meditationstechnik. Letztlich verstanden es die Beatles meisterhaft, sich den jeweiligen gesellschaftlichen Strömungen anzupassen, ohne sich zu sehr zu exponieren. So experimentierten sie zwar 1967 mit psychedelischen Elementen in der Musik, blieben dabei aber wesentlich gemäßigter als etwa Pink Floyd zur selben Zeit. Nicht zuletzt durch ihre Filme – insbesondere durch "Yellow Submarine" (1968) – übten sie einen nachhaltigen Einfluss auf die Popkultur aus und trugen zur Entwicklung des Musikvideos bei, da sie zeitweise kurze Filme speziell für ihre neuen Single-Erscheinungen drehten und diese an die Fernsehsender schickten. Ihre Clips für "Paperback Writer", "Rain", "Strawberry Fields Forever" und "Penny Lane" waren somit die ersten Musikvideos der Popgeschichte, in denen die Band nicht einfach ihren Song spielte, sondern in denen auch andere Dinge passierten. Allerdings waren sie damals noch auf Film aufgenommen und nicht auf Video. Starr und Harrison sagten später dazu, sie hätten diese Filme gedreht, um ihre Titel nicht mehr live in Fernsehstudios präsentieren zu müssen.

Die Diskografie der Beatles variierte bis Ende der 1980er Jahre weltweit, was dem Umstand einer international unterschiedlichen Veröffentlichungspolitik geschuldet ist. Seit dem Jahr 1987, in dem alle Beatles-Alben erstmals auf CD veröffentlicht wurden, ist die Diskografie einem sogenannten „Beatles-Kanon“ unterworfen, das heißt, international werden die Studioalben der Beatles einheitlich so veröffentlicht, wie sie ursprünglich in Großbritannien erschienen.





United Artists war für den Kinovertrieb dieser Filme zuständig. Der Vertrieb des Soundtracks des Filmes "A Hard Day’s Night" erfolgte in den USA und Kanada exklusiv durch dessen Tochterunternehmen United Artists Records und unterschied sich in der Titelliste deutlich von der britischen Version.





</doc>
<doc id="11892" url="https://de.wikipedia.org/wiki?curid=11892" title="Erbsünde">
Erbsünde

Erbsünde (lateinisch "peccatum originale" oder "peccatum hereditarium") ist ein Begriff der christlichen Theologie für einen Unheilszustand, der durch den (seit der Aufklärung häufig auch nur symbolisch verstandenen) Sündenfall Adams und Evas herbeigeführt worden sei und an dem seither jeder Mensch als Nachfahre dieser Ureltern teilhabe.

Die deutschsprachige Bezeichnung "Erbsünde" ist zuerst in mittelhochdeutscher Zeit seit etwa 1225 belegt. Der zugrundeliegende lateinische Ausdruck "peccatum originale" (wörtlich ‚ursprüngliche Sünde‘, ‚Ursünde‘) umfasste in seiner Bedeutung sowohl die Sünde Adams und Evas infolge ihres Sündenfalls ("lapsus Adami", "peccatum primorum parentum", "primum peccatum") als auch die dadurch entstandene Erbsünde der Menschheit allgemein. Zur begrifflichen Unterscheidung zwischen dem aktiv begangenen "peccatum originale" der ersten Eltern und dem nur passiv durch Abstammung aus leiblicher bzw. libidinöser Zeugung erworbenen "peccatum originale" ihrer Kinder und Nachfahren unterschied die Scholastik seit Alain de Lille zwischen dem "(peccatum) originale active" (Erbsünde ‚im aktiven Verständnis‘) und "(peccatum) originale passive" (‚im passiven Verständnis‘), seit Petrus von Tarantasia auch zwischen "(peccatum) originale originans" (‚erzeugend‘) und "(peccatum) originale originatum" (‚erzeugt‘). Beide Unterscheidungen wurden seither zum Gemeingut der theologischen Literatur und werden zu Begriffsklärungen auch in neuerer Zeit noch herangezogen. Im Zentrum der theologischen Betrachtung steht in jüngerer Zeit zumeist nicht der Sündenstand der Ureltern, sondern der der Menschheit allgemein, so dass sich die Bedeutung des Ausdrucks Erbsünde zumindest in der Tendenz weitgehend auf das (passive) "peccatum originale originatum" fokussiert hat.

Der Urstand als Bezeichnung des Zustands von Adam und Eva vor dem Sündenfall ist der Gegenbegriff zur Erbsünde; das Konzept des Urstands entstand bereits in der Antike.

Der Begriff wird in der orthodoxen, römisch-katholischen und den verschiedenen evangelischen Traditionen unterschiedlich aufgefasst. Gemeinsam ist in allen christlichen Traditionen die Lehre der Trennung des Menschen von Gott, bedingt durch die Erbsünde. Mit Hilfe Jesu Christi kann die Gemeinschaft mit Gott wiederhergestellt werden. Der Mensch allein besitzt nicht die Kraft dafür. Unterschiede bestehen innerhalb der christlichen Konfessionen hinsichtlich der Art des Weges, welcher zur Erlösung gegangen werden muss (Rechtfertigungslehre).

In den Evangelien sprechen weder Jesus Christus noch die Autoren der Evangelien vom Sündenfall Adams, dessen Fehler Jesus rückgängig zu machen habe. Es sind jedoch deutliche Aussagen über die Verderbtheit der Welt enthalten, die mit der späteren Erbsündenlehre inhaltlich in Einklang gebracht werden können (vgl. ; ).

Der Apostel Paulus entwickelt eine Theologie der Sünde und eine damit zusammenhängende Anthropologie, die als Grundlage der späteren Erbsündenlehre gelten kann, . 
Paulus parallelisiert darin den für die ganze Menschheit stehenden ersten Menschen, Adam (das hebräische Wort "Adam" bedeutet einfach „Mensch“), mit dem für die neue Menschheit stehenden zweiten Adam, Christus. So wie aufgrund der Sünde des Ersten die Menschheit dem Tod ausgeliefert war, wird sie aufgrund der Erlösungstat des Zweiten aus diesem Tod errettet: „Durch einen einzigen Menschen kam die Sünde in die Welt und durch die Sünde der Tod, und auf diese Weise gelangte der Tod zu allen Menschen, weil ("eph' hô") alle sündigten. […] sind durch die Übertretung des einen die vielen dem Tod anheim gefallen, so ist erst recht die Gnade Gottes und die Gabe, die durch die Gnadentat des einen Menschen Jesus Christus bewirkt worden ist, den vielen reichlich zuteil geworden.“ 
Der zentrale Punkt wird im ersten Brief an die Korinther nochmals betont:

Die Erbsünde stellt somit ein spezifisch christliches, aus dem Erlösungsbegriff hergeleitetes Dogma dar, das im Judentum kein direktes lehrmäßiges Vorbild hat.

Der erstmals vom Kirchenschriftsteller Tertullian gelehrte Generatianismus besagt, dass nicht nur der Körper, sondern auch die Seele im Zeugungsvorgang vom Vater über den Samen an das Kind vermittelt wird. Hingegen besagt die vom Kirchenvater Laktanz formulierte Lehre des Kreatianismus, dass die Seele zum Zeitpunkt der Zeugung von Gott neu erschaffen wird. Der Generatianismus erklärt gut die später postulierte Erbsündenlehre, denn so vererbt sich die Sünde Adams auf alle nachfolgenden Geschlechter. Hingegen ist beim Kreatianismus zunächst nicht einsichtig, wieso die neu geschaffene Seele die Sünde ihrer leiblichen Vorväter erben soll.

Der Kirchenvater Augustinus von Hippo formulierte die Erbsündenlehre. Er konnte sich nicht zwischen Generatianismus und Kreatianismus entscheiden, da er zwar mit dem Kreatianismus sympathisierte, jedoch erkannte, dass dieser seine Erbsündenlehre nicht unterstützt. Augustinus kam mutmaßlich auf die Erbsündenlehre, weil der griechische Begriff "eph' hô" aus in der lateinischen Bibelübersetzung, der Vulgata, als "in quo" wiedergegeben wurde, also: „In ihm (Adam) haben alle gesündigt“.

Augustinus stand wahrscheinlich die Bibel Vetus Latina zur Hand: 

Augustinus lehrte: der Mensch komme beladen mit der Erbsünde auf die Welt. Er benötige deshalb zur Erlösung die Gnade Gottes. Dies wurde durch die Menschwerdung, Kreuzigung und Auferstehung Jesu Christi ermöglicht. Aus diesem Grund habe der Apostel Paulus von Christus als dem „neuen Adam“ gesprochen. Die Erlösung finde der Mensch durch das Sakrament der Taufe, da der Getaufte nicht mehr der Erbsünde unterliegt. Daher war für Augustinus die Säuglingstaufe besonders empfehlenswert, um das unmündige Kind der Verdammnis zu entreißen, die ihm drohe, falls es ungetauft sterbe. Gleichwohl verbleibe der Mensch in der sterblichen Welt mit den Folgen der Erbsünde behaftet und die Erbsünde rechtfertige auch eine ewige Bestrafung der Sünder in der Hölle.

Im Gegensatz zu Augustinus' Erbsündenlehre steht der von seinem britischen Zeitgenossen Pelagius vertretene Pelagianismus. Nach ihm trage der Mensch die volle Verantwortung für sein eigenes Seelenheil. Adams Sünde habe sich nicht auf alle späteren Menschen vererbt. Pelagius sah seine These der möglichen Sündlosigkeit auch durch die große Zahl der Heiligen in der Schrift belegt. Im Pelagianismus ist die Rolle der Gnade Gottes zweitrangig und auch Christus dient dem Menschen nur als "gutes Beispiel", nicht aber als Erlöser. Somit bestehe auch keine Notwendigkeit einer Säuglingstaufe. Augustinus hingegen hielt die Lehre des Pelagius, der Mensch könne, ja müsse sich sein Seelenheil selbst „verdienen“, für eine Überforderung. Dagegen setzte er, in seelsorglicher Absicht, die Erinnerung an die biblischen Schriften, denen zufolge der Mensch eben nicht fehlerlos und sündenfrei ist. Für Augustinus war das Wissen um die Erbsünde ein Schutz vor einem unerfüllbaren Selbstanspruch an die eigene Vollkommenheit – und insofern entlastend und keineswegs bedrückend. Der Pelagianismus wurde auf dem Konzil von Ephesos im Jahre 431 als Häresie verurteilt.

Die Erbsündenlehre ist bis heute zentral für das westliche Christentum, obwohl der sie unterstützende Generatianismus von der katholischen Kirche mehrfach verurteilt wurde und ihre verbindliche Lehrmeinung heute der Kreatianismus ist, der mit der Erbsündenlehre eher im Konflikt steht.

Nach Ansicht der Orthodoxen Kirche wurde nicht Adams Sünde als solche, wohl aber die Folge der Sünde Adams, der Tod, auf seine Nachkommen vererbt und versklavte damit die gesamte Schöpfung, die dabei von ihrer eigentlich guten Natur in einen widernatürlichen schlechten Zustand überging. Die Angst vor dem Tod wird in einem „Teufelskreis“ zur Hauptursache weiterer Sünden. Menschen haben aber auch nach dem Sündenfall noch ihren freien Willen und sind innerlich immer noch fähig und gewillt zu den guten Taten, die ihrer eigentlichen, gottgewollten Natur entsprechen; in der versklavten Schöpfung sind gute Taten jedoch nur sehr schwer auszuüben. Da der Mensch nach dem Sündenfall nicht mehr zu Gott kommen konnte, kam Gott in Christus zu den Menschen und versöhnte die Menschen und die ganze Schöpfung so wieder mit sich; der versöhnte Mensch verlässt allmählich den widernatürlichen Zustand und wird frei, seine Fesselung an den Tod und die von diesem unterjochte Welt wird gelockert, wodurch auch die Auferstehung und damit die völlige Überwindung der Fessel möglich wird. Gute Taten werden für ihn mehr und mehr selbstverständlich. Es wird dabei betont, dass der Mensch mit Gott wieder versöhnt wurde und nicht Gott mit dem Menschen. Der Ausdruck "eph' hô" aus Röm 5,12 kann auch als „deshalb (also wegen des Todes) haben alle gesündigt“ verstanden werden.

Im Verständnis Martin Luthers und der meisten Reformatoren ist der Mensch immer schon im Zustand der Sünde, der das eigene Handeln von Anfang an negativ beeinflusst. Selbst das neugeborene Kind ist nach diesem Verständnis sündig und bedarf daher der Erlösung. Durch die Taufe kommt es zu keiner Aufhebung der Erbsünde; der Christ wird von Gott gerecht gesprochen (Rechtfertigungslehre), nicht gerecht gemacht. Besonders im Calvinismus wird betont, dass die menschliche Natur an und für sich bereits sündig sei, noch vor jeder konkreten Tat.

Das Konzil von Trient befasste sich, ausgelöst durch die Reformation, abschließend mit diesem Thema und stellte im "Decretum de Peccato Originali" fest, dass alle Menschen in Nachfolge des Adam, mit Ausnahme von Maria (Mutter Jesu), von der Erbsünde betroffen sind. Dabei wird die Erbsünde durch die Taufe allerdings „vollkommen“ getilgt. Die Erbsünde ist mithin definitionsgemäß derjenige Mangel im Menschen, der bereits durch die Taufe (oder eine ihr entsprechende Zuwendung zu Gott, siehe Begierdetaufe) restlos überwunden wird. 

Aus "katholischer" Sicht zieht der Mensch durch den Sündenfall Adams das Missfallen Gottes auf sich, da der Mensch die übernatürliche Ausstattung der Gnade verloren hat. Der Mensch kann ohne Gnade durch seine guten Handlungen keine „übernatürliche Vollkommenheit“ verdienen. So ist er, von seiner Empfängnis an, schon im Mutterleib im Zustand der Erbsünde, was dazu führt, dass der Mensch zum Bösen neigt und der Verstand nicht mehr das Gute erkennt. Auch die Sinne verhalten sich nicht mehr, wie die Übernatur dies verlangt. Der Ausweg aus der Erbsünde wird im Kreuzestod Jesu Christi und der damit verbundenen Erlösung gesehen. 

Im Katechismus der Katholischen Kirche (KKK) heißt es u. a.:

Die neuere Theologie versucht einen adäquateren Begriff für den mit „Erbsünde“ bezeichneten Sachverhalt zu finden. Hier setzt sich in den letzten Jahren der Terminus „universale Sündenverfallenheit“ durch, der überholte Vorstellungen, wie den Monogenismus oder die personale Implikation des Begriffes Sünde (als ginge es um eine persönlich begangene Tat) aufhebt, um zugleich die unfreie Situiertheit und bleibende Verführbarkeit des Menschen zum Bösen, aber auch seine Erlösungsbedürftigkeit als Inhalte zu erhalten. Andere terminologische Vorschläge sind: "Erbverwundung", "Erbunheil", "Erbschwäche".

Peter Knauer sieht in der fortwährenden Angst um sich selbst das Wesen der Erbsünde, das jedem weiteren Fehlverhalten zugrunde liegt und aus dem heraus es erwächst.

Joseph Ratzinger versteht die Erbsünde nicht im Sinne einer "biologischen" Vererbung, sondern betont die kollektiven menschlichen Verstrickungen der Vergangenheit, in die jeder Mensch durch seine Geburt eintritt. Diese schränken die Selbstbestimmung ein und geben den Rahmen der eigenen Freiheit vor: „Niemand hat die Möglichkeit, an einem perfekten ‚Punkt Null‘ anzufangen und sein Gutes in völliger Freiheit zu entwickeln.“ Am 20. April 2007 erklärte er als Papst Benedikt XVI., der "Limbus puerorum" gehöre nicht zur Lehre der Kirche, sondern sei eine ältere theologische Theorie. Roland Minnerath, der Erzbischof von Dijon, erläuterte die Entscheidung: Die Theologen im Vatikan seien zu der Auffassung gelangt, dass kleine Kinder, die nicht getauft sind und sterben, direkt ins Paradies kämen. Das Dokument der Internationalen Theologenkommission besagt jedoch auch (in Absatz 41), dass der Limbus eine „mögliche theologische Meinung bliebe“.

Im frühen Quäkertum glaubte man an eine Befreiung von der Erbsünde durch die Hinwendung zu Gott und einem verdienstvollen Lebenswandel. So schreibt George Fox in seinem Tagebuch:

Im heutigen liberalen Quäkertum hat die Erbsünde keine theologische Relevanz mehr.

Der Islam kennt keine Erbsündenlehre. Zwar erinnert der Koran (7,19–25; 2,35–39; 20,117–124) an den Sündenfall und die Vertreibung aus dem Paradies (), doch übernimmt er nicht die paulinische Lehre von der Erbsünde. Im Koran, Sure 2, Vers 36, wird sogar ausdrücklich erwähnt, dass Allah Adam bereits verziehen habe, weswegen das christliche Dogma von der Erbsünde dem islamischen Dogma vom allverzeihenden Gott gegenüberstehe. Jeder einzelne Mensch wird nach islamischer Lehre nur für seine eigenen Taten zur Verantwortung gezogen; beim Gericht kann niemand einem anderen Menschen helfen oder schaden. Wenn ein Mensch schlechte Taten aufrichtig vor Gott bereut und um Vergebung bittet, so wird ihm diese zuteil.

Das Judentum kennt den Begriff der Erbsünde nicht. Im Judentum wird die Vertreibung von Adam und Eva aus dem Garten Eden daher nicht als Beginn einer zwangsläufigen erblichen Sünde gesehen. 
Die verhängte Ausweisung aus dem "Garten Eden" und die weiteren Konsequenzen zeigen das Bild der Welt, wie sie ist, und werden im Judentum als Maßnahmen verstanden, die das materielle, nicht aber das spirituelle Leben der Menschen betreffen. Allerdings ist durch den Verzehr der verbotenen Frucht der „böse Trieb“ in den Menschen geraten, der seitdem in jedem Menschen vorhanden ist und ihn in seinem Handeln beeinflusst.

Die Ankündigung, dass die Nachkommen Evas den Nachkommen der Schlange den Kopf zertreten werden , wird als Aussage zur Gefahr von Giftschlangen und menschlicher Angst vor ihnen gewertet (im Christentum wird dies hingegen als Ankündigung des Sieges Jesu über den Satan gedeutet). Der Tanach bezieht sich auch in keiner Erzählung, in der das Volk Israel fehlgeht, auf die Vertreibung Adams und Evas aus dem Paradies, weil nicht die Lokalität die Rolle spielt, sondern die Fähigkeit des Menschen, seinen „bösen Trieb“ zu überwinden. 

Die wichtigste jüdische Aussage zum Status der Seele des Menschen lautet, sie sei ein Funke Gottes und somit rein. Wenn der Mensch aber sündigt, verunreinigt er seine Seele, hat aber durch aufrichtige Reue und den konsequenten Entschluss, diese Sünden nie wieder zu begehen "(Teschuva)," die Möglichkeit, seine Seele wieder rein zu machen, denn Gott ist barmherzig und vergibt Sünden. Hätten Adam und Eva ihre Sünde bereut, dann hätte Gott auch ihnen vergeben.
Die Sünden der Vorfahren haben keinen Einfluss auf die Seele des Menschen, denn er war nicht an ihnen beteiligt und es wäre ungerecht, ihn dafür verantwortlich zu machen. Wenn er jedoch die Sünden seiner Vorfahren fortsetzt, und zwar mit einer noch stärkeren Intensität, als sie sie getan haben, werden diese Sünden auch ihm zugerechnet. 
Dies alles hat nichts mit der Lokalität zu tun, darum gibt es in dieser Hinsicht keinen direkten Bezug zum „Garten Eden“. 
Eine Erlösung im christlichen Sinne ist darum nicht nötig, weil es eben keine Erbsünde gibt. Das Warten im Judentum auf den Messias hat nichts mit Erlösung zu tun, sondern ist das Zeichen für den Beginn der „Kommenden Welt“, in der alle Juden (von den „vier Enden der Erde“) gesammelt werden.

Die Lehre von der Erbsünde soll laut Sigmund Freud orphischer Herkunft sein; sie sei in den Mysterien erhalten geblieben und habe von dort aus Eingang in die Philosophenschulen des griechischen Altertums gefunden. Sie finde sich in Schopenhauers Philosophie wieder, der in "Die Welt als Wille und Vorstellung" den Weltwillen als ewig schuldigen begreift. 

René Girard betrachtet in seiner "mimetischen Theorie" die Erbsünde kulturanthropologisch. Die ewige Schuld der Menschen besteht nach Girard darin, dass sie immer versuchen, die eigene Gewalt durch Ritualisierung der Gewalt einzudämmen. Indem sie unschuldige Opfer töten und anschließend heiligen, halten sie den Opferzyklus in Gang. Diesen Zyklus erkannt und verurteilt zu haben, stellt sich Girard zufolge als Hauptverdienst der neutestamentlichen Offenbarung dar.

Hoimar von Ditfurth sieht in der Erbsünde „jene unserer kardinalen Schwächen, auf die auch die evolutionäre Betrachtung des heutigen Menschen uns hat stoßen lassen: unsere prinzipielle, aus unserer ‚Natur‘ entspringende Unfähigkeit, das, was wir als richtig erkannt haben, auch zu tun“.





</doc>
<doc id="11893" url="https://de.wikipedia.org/wiki?curid=11893" title="Byzanz">
Byzanz

Byzanz steht für:

byzantinisch bezieht sich auf:

Siehe auch:


</doc>
<doc id="11895" url="https://de.wikipedia.org/wiki?curid=11895" title="565">
565














</doc>
<doc id="11896" url="https://de.wikipedia.org/wiki?curid=11896" title="Porto (Begriffsklärung)">
Porto (Begriffsklärung)

Porto steht für:

Porto steht für folgende Orte:

Porto ist ein Namensteil von:

Porto ist der Familienname folgender Personen:

Siehe auch:


</doc>
<doc id="11902" url="https://de.wikipedia.org/wiki?curid=11902" title="Mythologie">
Mythologie

Als Mythologie (von "mythos" u. a. „Erzählung, Rede“ und λέγειν "legein" „erzählen“; deutsch auch "Sagenwelt") wird die Gesamtheit der Mythen eines Kulturareales oder eines Volkes, einer Region oder einer sozialen Gruppe sowie ihre systematische Darlegung in literarischer, wissenschaftlicher oder religiöser Form bezeichnet. Daneben bestehen diverse kulturübergreifende Unterteilungen wie die Astralmythologie, die Zahlenmythologie und die Eschatologie. Das deutsche Wort „Mythologie“ erscheint erstmals 1712 in dem Buch "Die teutsche Mythologie oder Beschreibung heidnischer Götter".

Neben einem Verständnis als Sammlung von Geschichten mythischen Charakters wurde der Begriff "Mythologie" – vor allem in moderner Zeit – auch parallel zu ähnlichen Begriffsbildungen (wie "Theologie, Biologie") als „Wissenschaft von den Mythen“ gedeutet. Die in diesem Sinn verstandene wissenschaftliche Mythologie beschäftigt sich mit der Frage nach der Herkunft der Mythen und ihrem Verhältnis zu anderen Erzählformen wie Legende, Sage oder Epos. Moderne Märchen enthalten oft Elemente, die sie Mythen vergleichbar machen. Die Entwicklung der Mythen als erzählerischer Gattung und ihre Transformation zu Märchen bildet einen Gegenstand der Erzählforschung (Narratologie). Die Entstehung von Mythen ist daneben auch Gegenstand der Psychologie, besonders der im Laufe des 19. Jahrhunderts aufkommenden Völkerpsychologie, die in Carl Gustav Jungs Theorien des kollektiven Unbewussten und der Archetypen einen Nachfolger fand.

Soweit der Gegenstand der Mythen religiös gesehen wird, ist ihre Erforschung eng mit der Geschichte der Religion verbunden. Informationen aus Mythen sind wichtig zur Rekonstruktion religiöser Vorstellungen, die manchmal Inhalte unterschiedlicher mythologischer Ursprünge zu einem System verbinden. Der britische Schriftsteller Robert Graves definierte Mythologie als „Erforschung jener religiösen oder heldenhaften Legenden, die in der Erfahrung des Studierenden so fremdartig sind, dass er sie nicht für wahr halten kann.“ Joseph Campbell wies darauf hin, dass aus religiöser Sicht "Mythos" als „die Religion anderer Leute“ definiert werden kann. Insofern sei Religion „missverstandene Mythologie“. Das Missverständnis bestehe darin, dass „mythische Metaphern als Hinweise auf unumstößliche Tatsachen interpretiert werden“.

Zentrale Themen von kosmogonischen Mythen sind die Erschaffung der Welt (Schöpfungsgeschichten) aus dem Urmeer, die Ordnung der Welt um einen Weltenberg sowie Prozesse der Zerstörung und anschließender Erneuerung dieser Ordnung, häufig in Verbindung mit dem Kampf unterschiedlicher Mächte (sowohl konkreter Göttergestalten, Himmel und Erde, als auch abstrakter Eigenschaften wie "gut" und "böse", "hell" und "dunkel"). Sie sind nicht als historische Wirklichkeit gedacht, sondern als Darstellung einer archetypischen, metaphysischen Struktur ,hinter der Wirklichkeit‘ in sprachlichen Bildern (vgl. die biblische Genesis).

Allerdings fehlen gerade dem Bericht der Genesis trotz seiner Teilhabe an mythischem Material bestimmte typische Elemente der altorientalischen oder auch der griechischen Mythologie. Darauf weist Alberto Soggin hin, der davon ausgeht, dass sich in der erhaltenen Fassung bereits ein Prozess der Entmythologisierung ausdrückt. Da die Autoren monotheistisch denken, ist keine Rede von Konflikten innerhalb eines polytheistischen Pantheons; die Konflikte zwischen Gott und der Menschheit entstehen allein aus deren sündigem Charakter. Es fehlen auch der göttliche Zeugungsakt, die Verbindung zu den im Kult zu wiederholenden Zyklen der Natur und die Erschaffung der Welt als Sieg eines Demiurgen über dämonische Kräfte oder das Chaos personifizierende Ungeheuer.

Einen weiteren Schwerpunkt von Mythologien bilden die Erschaffung des Menschen und, von den ersten Menschen ausgehend, die genealogische Ableitung von Herrschergeschlechtern, seltener von Göttern oder von anderen Völkern hohen Ansehens. Sie dienen der religiösen Herrschaftslegitimation und dem Zusammengehörigkeitsbewusstsein von Stämmen durch das „Wir-Gefühl“ gleicher Abstammung. Als Beispiel kann die altnordische Ynglingatal dienen (siehe auch Herkunftssage, Origo gentis).





</doc>
<doc id="11904" url="https://de.wikipedia.org/wiki?curid=11904" title="Triumphbogen">
Triumphbogen

Ein Triumphbogen ("fornix" oder "arcus", ab den Severern "arcus triumphalis") ist ein frei stehendes, torförmiges Bauwerk, das ursprünglich in Rom zu Ehren triumphierender Feldherren oder Kaiser errichtet wurde. Nicht vom Senat beschlossene Bogenbauten gleicher Formgebung, etwa Stiftungen von Städten, Provinzialverwaltungen oder Privatpersonen, werden zumeist Ehrenbogen genannt. Oft werden beide Begriffe synonym gebraucht, da eine allgemein akzeptierte, wissenschaftlich exakte Definition des Begriffs "Triumphbogen" aussteht.

In der Römischen Republik wurden diese Monument stets "fornix", ab der Kaiserzeit "arcus", seltener "ianus" genannt. Bisweilen wurden Monumente durch zusätzliche Angaben wie "cum insignibus triumphorum", "triumphis insignis" oder als Triumphbogen spezifiziert. Ab dem Ende des 2. Jahrhunderts kommt der zunächst nur in Nordafrika anzutreffende Ausdruck "arcus triumphalis" auf, wird ab der Spätantike allgemein für derartige Bogenmonumente gebräuchlich und war Vorbild für die moderne Bezeichnung "Triumphbogen". Der griechische Ausdruck lautete „Tropaia tragender Bogen“ (). 

Der Bau eines Triumphbogens sollte an Ereignisse von großer Tragweite erinnern, die von ranghohen Mitgliedern der römischen Führungsschicht zumeist in militärischen Aktionen bewirkt wurden. Bereits zu Beginn des 2. Jahrhunderts v. Chr. wurden erste Triumphbögen durch Feldherren in Rom erbaut. Wurden in republikanischer Zeit Götter, deren Statuen man in einer Quadriga auf der Attika des Bogens aufstellte, durch den Bau des Bogens geehrt, hob man ab dem Prinzipat den Kaiser in entsprechender Aufstellung über die Welt hinaus. Im Falle des Triumphbogens verkündete die Bogeninschrift auf der Attika den Anlass der Errichtung und das Verdienst des Geehrten.

Die Bezeichnung "Triumphbogen" wurde in der Neuzeit auf die meisten Bogen- und sonstigen Monumente übertragen, die anlässlich eines militärischen Sieges, eines Triumphes oder zur Erinnerung an einen Friedensschluss errichtet wurden. Ein bekanntes Beispiel ist der Arc de Triomphe in Paris. Triumphtore wie das Brandenburger Tor bilden hingegen einen eigenen, wenn auch verwandten Bautypus. Das Motiv des Triumphbogens wird seit nachantiker Zeit zur Gestaltung weiterer Gebäudeteile genutzt und wird dort Triumphbogenschema genannt.

Zu den Anlässen zählten die Wiederherstellung des Staates und die Ausdehnung des Reichsgebietes (Septimius-Severus-Bogen), Befreiung des Staates von Hochverrat (Konstantinsbogen) oder die Abwendung von äußeren Gefahren (Fornix Fabianus). Für die Triumphbögen im engeren Sinn, das heißt die stadtrömischen Bögen, trat immer der Senat als Auftraggeber auf. Doch gab es vergleichbare, Ehrenbögen genannte Bauten ab der Kaiserzeit auch in den Provinzen des römischen Reiches und auf der restlichen Apenninhalbinsel selbst. Auftraggeber waren in diesen Fällen Beamte der Provinzialverwaltung oder Priesterschaften. Zugleich änderte sich der Anlass der Errichtung, der in diesen Fällen zumeist ziviler Natur war, aber auch eine Stadtgründung selbst als Folge militärischer Auseinandersetzung feiern konnte, wie etwa der Augustusbogen in Aosta oder ein Bogen in Béja. Zivile Anlässe widmeten sich meist Infrastrukturmaßnahmen: Errichtung von öffentlichen Bauten, Fernstraßen, Wasserleitungen, Brücken oder Häfen.

Der Anlass bestimmte zumeist den Bauschmuck des Bogens: Tropaia, Viktorien, Genien und Eroten gehörten ebenso zur Standardausstattung wie Fruchtgirlanden oder Füllhörner. Hinzu traten Reliefstatuen, Reliefs und Figurenfriese, die einen konkreten Bezug zum Anlass herstellen konnten.

All diese Bögen zeugten vom römischen Macht- und Sendungsbewusstsein, das sie monumental herausstellten. Entsprechend dominant war ihre Position innerhalb des Stadtbildes. In Rom reihten sie sich entlang des Wegs, den der Triumphzug, ausgehend von der "porta triumphalis" am Marsfeld, zum Kapitol nahm. In anderen Städten standen sie anfangs über Ausfallstraßen und Straßenkreuzungen des Umlandes, markierten Grenzen der Stadt oder eines Territoriums. Später waren sie Ausgangspunkt oder Ziel von innerstädtischen Straßenzügen, waren Eingang zu Foren und Heiligtümern oder überspannten zentrale Straßenkreuzungen.

Mehr als 360 römische Triumph- und Ehrenbögen sind durch Bauten, Inschriften und Münzen überliefert. Allein in Rom befanden sich über 50 von ihnen. Während im Westen der eintorige Bogen dominierte, bevorzugte man im Osten des Reiches den dreitorigen Bogen. Zweitorige Bögen des Westens stehen den klassischen Stadttoren nahe. Ursprünglich besaßen die Bögen weder Säulen- noch Gebälkgliederungen und die Attika war noch kein verpflichtendes Bauelement. Mit ihrer zunehmend propagandistischen Nutzung wurde auch der durch sie ausgedrückte Repräsentationswille gesteigert, was zur Entwicklung reicher Prunkfassaden führte. Halbsäulen, Vollsäulen mit und ohne Postamente, die Fassaden bis ins Gebälk durchziehende Verkröpfungen wurden ins Repertoire der Bogengestaltung aufgenommen. Bei dreitorigen Bögen oder gar dem vor allem in Nordafrika beliebten vierseitigen Quadrifrons wurden die Fassaden zusätzlich mit Ädikulen bereichert. Der hinter diesen Fassaden liegende Baukörper, der Bogenkern, folgte hingegen mehr oder minder immer dem gleichen Aufbau: zwei kubische Pylone, die mittels eines Tonnengewölbes verbunden wurden.

Von frühen Triumphbögen in Rom sind nur wenige Reste erhalten. Zu nennen sind hier der "fornix Fabianus", den Quintus Fabius Maximus 121 v. Chr. errichten ließ, außerdem die nur in Fundamentresten und Fragmenten der Inschriften überlieferten Actiumbogen und Partherbogen für Augustus. Nur vermutet werden kann der Standort des Bogens für Gaius und Lucius Caesar, der nördlich des Tempels für den vergöttlichten Caesar gestanden haben muss. Noch erhaltene Triumphbögen in Rom sind der Titus- und der Septimius-Severus-Bogen auf dem Forum Romanum sowie der Bogen für Konstantin I. neben dem Kolosseum.

Da die Triumphzüge römischer Feldherren oder Kaiser ausschließlich in Rom und Konstantinopel begangen wurden, werden Bögen, die außerhalb der beiden Hauptstädte errichtet worden sind, nicht als Triumph-, sondern als Ehrenbögen bezeichnet. Außerhalb Roms sind beispielsweise erhalten: die Ehrenbögen für Augustus in Rimini, in Susa, Aosta, Glanum und Fano; für Trajan in Ancona und Benevent; außerdem ein Bogen in Carpentras in Südfrankreich, der Bogen von Orange und der Galeriusbogen in Thessaloniki.
Auch gab es Ehrenbögen für Privatpersonen oder Familien wie den Sergierbogen in Pula oder den Gavierbogen in Verona. Die Hadrianstore in Athen und Antalya tragen ebenfalls Züge eines Ehrenbogens. Zum Ehrenbogen für Gallienus umgeweiht wurde die "porta Esquilina", eigentlich ein Stadttor in Rom.

Ein vierseitiger Ehrenbogen (Tetrapylon), ist ebenfalls im antiken Cáparra in Südwestspanien erhalten. Im Jahre 1986 wurden die Reste des bislang nördlichsten römischen Ehrenbogens in Mainz-Kastel entdeckt.

Die ursprüngliche Bedeutung des sogenannten Heidentors bei Carnuntum gibt der Forschung ebenso Rätsel auf wie die Funktion des vierseitigen stadtrömischen Janusbogens.




</doc>
<doc id="11905" url="https://de.wikipedia.org/wiki?curid=11905" title="Straßenbahn">
Straßenbahn

Eine Straßenbahn, auch Trambahn oder Tramway beziehungsweise kurz "die" (Deutschland und Österreich) respektive "das" (Schweiz) Tram, ist ein schienengebundenes, fast immer mit elektrischer Energie betriebenes öffentliches Personennahverkehrsmittel im Stadtverkehr, das den speziellen Bedingungen des Straßenverkehrs angepasst ist.

Straßenbahnen fuhren ursprünglich nur auf Gleisen, die in oder direkt neben der Straße verlegt waren. Als innerstädtisches Verkehrsmittel oder als Überlandstraßenbahn wurden sie aber in verschiedener Weise weiterentwickelt. Die Verlegung von Strecken auf eigene Bahnkörper oder in Tunnel erhöhte die Reisegeschwindigkeit. Derart modifizierte Straßenbahnen werden häufig als Stadtbahnen bezeichnet, besonders wenn sie als U-Straßenbahn auch Tunnelstrecken befahren, die Unterscheidung ist fließend. Tram-Trains benutzen außerhalb geschlossener Ortschaften teilweise Eisenbahnstrecken, sie fahren deshalb meist auf Normalspur und sind für verschiedene Spannungen sowie Netzfrequenzen ausgerüstet. Sie verbinden Vororte mit Stadtzentren beziehungsweise erschließen das Umland der Städte.

Mit dem Wort "Straßenbahn" wird umgangssprachlich sowohl ein gesamtes System beziehungsweise Netz als auch ein einzelnes Fahrzeug gemeint. Es handelt sich dabei um eine Kurzform der veralteten Bezeichnung "Straßen-Eisenbahn", analog zum englischen "street railway".

In Deutschland und der Schweiz ist alternativ "Trambahn" beziehungsweise kurz "Tram" gebräuchlich, in Wien "Tramway" oder "Bim". Es heißt dabei "die" Tram in Deutschland, aber "das" Tram in der Schweiz. Das Wort „Tram“ ist deutschen Ursprungs und ist gleichbedeutend mit „Balken“. Ein im Bergbau eingesetztes und auf geschlitzten Balken basierendes spurgeführtes Bahnsystem kam im 15. Jahrhundert von Deutschland nach England, wo sich der Begriff „Tramway“ allgemein für spurgeführte Bahnen entwickelte. Vom Englischen fand die Bezeichnung ihren Weg in weitere Sprachen. "Tramway" war früher im deutschen Sprachraum bekannt, so wurde mitunter von einer "Dampftramway" oder einer "Pferdetramway" gesprochen. Eine weitere Bezeichnung ist "Elektrische". Oft dient die jeweilige Liniennummer als Benennung, mit "der 5er" oder "die 5" kann also sowohl ein Zug der Linie 5 als auch die Linie an sich gemeint sein. Eine ganz spezielle Bezeichnung hat sich in Wien für einige wenige Linien eingebürgert, so werden die Züge der Linien D und O als D-Wagen und O-Wagen bezeichnet.

Daneben existieren eine ganze Reihe regionale, umgangssprachliche beziehungsweise mundartliche Bezeichnungen. So ist "Bim" (lautmalend, österreichisch) ein in Wien, Graz und Linz gebräuchlicher Name für die Straßenbahn, es ist die Abkürzung der süddeutschen bzw. österreichischen Bezeichnung "Bimmelbahn". Bimmeln meint das Betätigen der lauten Fußklingel durch den Fahrer des Straßenbahnwagens. Die ehemalige Güterstraßenbahn in Wien wurde offiziell "Güterbim" genannt. Ein ähnlicher Begriff ist Bimmel, in Bonn gibt es beispielsweise eine "Partybahn Bönnsche Bimmel". Die Kurzbezeichnung in Dresden war häufig "Glocke". Die Straßenbahn Hongkong wiederum heißt umgangssprachlich auch "Ding Ding".

In Darmstadt heißt die Straßenbahn vereinzelt "Ellebembel", in Stuttgart mundartlich "Strambe" respektive "Strampe", die Straßenbahn Heilbronn hieß im Volksmund "Spatzenschaukel" während die Straßenbahn Offenbach mitunter "Knochemiehl" (Knochenmühle) und die Straßenbahn Freiburg "Hoobl" (Hobel) heißt. In München wurde in den 1920er Jahren vom "Stangerlwagen" gesprochen, bezogen auf die markanten Stangenstromabnehmer der damaligen Fahrzeuggenerationen. In Basel heißt es baseldeutsch "Drämmli". Alternative deutsche Kurzbezeichnungen sind "Straba" oder "Strab". Weitere umgangssprachliche Bezeichnungen lauten "Strapazenbahn", "Funkenkutsche" bzw. "Funken-Chaise".

In Innsbruck und Wien wird die letzte Straßenbahnfahrt vor der nächtlichen Betriebseinstellung als "die Blaue" bezeichnet. Diese Bezeichnung rührt daher, dass das Liniensignal der jeweils letzten Garnitur blau unterlegt war. In zahlreichen deutschen und Schweizer Städten wird die letzte Fahrt in der Nacht als "Lumpensammler" bezeichnet.

Straßenbahnen werden in Deutschland in Abs. 1 Personenbeförderungsgesetz definiert und nach der Verordnung über den Bau und Betrieb der Straßenbahn (BOStrab) betrieben, sie unterscheiden sich damit von den Eisenbahnen, die der Eisenbahn-Bau- und Betriebsordnung (EBO) unterliegen. Diese Trennung orientiert sich an den unterschiedlichen Betriebsanforderungen an Straßenbahnen, deren Züge üblicherweise am öffentlichen Straßenverkehr teilnehmen, auf Sicht fahren und zum Beispiel Fahrtrichtungsanzeiger (Blinker) besitzen müssen, und an Eisenbahnen, wo die Züge mit höheren Geschwindigkeiten und größeren Massen vor allem auf besonderem Bahnkörper, durch Signale gesteuert, verkehren. Mischbetrieb mit Straßenbahnfahrzeugen auf Eisenbahnstrecken und umgekehrt erfordert die Einhaltung beider Verordnungen, wenn nicht von den Aufsichtsbehörden Sondergenehmigungen erteilt werden. Derartige Systeme werden meist als Stadtbahn bezeichnet.
Die Führer und Fahrer von Straßenbahntriebwagen werden nach der BOStrab als "Fahrbedienstete" bezeichnet. Deren Ausbildung erfolgt durch "Lehrbedienstete" und die Prüfung durch den Betriebsleiter oder einen von ihm beauftragten Bediensteten des jeweiligen Straßenbahnbetriebes. Die Fahrbediensteten sind danach in regelmäßigen Abständen nachzuschulen.

In Österreich sind Straßenbahnen Eisenbahnen im Sinne des "Eisenbahngesetzes 1957". Die näheren Bestimmungen über Betrieb, Anlagen, Fahrzeuge, Signale und dergleichen sind in der vom Bundesminister für Wissenschaft und Verkehr (jetzt: Bundesminister für Verkehr, Innovation und Technologie) auf Grund von § 19 Abs. 4 des Eisenbahngesetzes erlassenen "Straßenbahnverordnung 1999", Bundesgesetzblatt II Nr. 76/2000, geregelt. In Österreich gibt es in folgenden Städten Straßenbahnen: Baden (hier als Teil der Wiener Lokalbahn), Gmunden, Graz, Innsbruck, Linz und Wien. In Klagenfurt verkehrt eine Museumsstraßenbahn, die Lendcanaltramway.

Straßenbahnen werden jedoch nach der Straßenverkehrsordnung (StVO) betrieben, das heißt unter anderem Fahren auf Sicht, keine Andreaskreuze, keine Eisenbahnkreuzungen. Es gibt aber Ausnahmen: So wurden bei Haltestellen oder neuralgischen Punkten (erhöhtes Unfallrisiko) eigene rot-gelbe Ampeln angebracht.

In Wien waren bei den älteren Triebwagen keine Rückspiegel angebracht, diese wurden inzwischen komplett nachgerüstet.

In der Schweiz werden Straßenbahnen in Art. 2 des Eisenbahngesetzes (EBG) als Nebenbahnen definiert. Gesetzliche Regelungen, die Straßenbahnen betreffen, finden sich auf Bundesebene unter anderem in der "Verordnung über Bau und Betrieb der Eisenbahnen" ("Eisenbahnverordnung", "EBV"), in der "Signalisationsverordnung (SSV)" sowie in Art. 48 "Regeln für Strassenbahnen" des "Strassenverkehrsgesetzes (SVG)", in dem festgehalten ist: 

Dazu kommen kantonale Regelungen, etwa in Basel-Stadt der "Grossratsbeschluss betreffend Vereinbarung mit dem Kanton Basel-Landschaft vom 26. Januar 1982 über die Basler Verkehrs-Betriebe (BVB) und die Baselland Transport AG (BLT)".

Generell gilt: Das Tram ist gegenüber allen anderen Verkehrsteilnehmern vortrittsberechtigt (Art. 38 SVG). Dies gilt auch gegenüber Fussgängern auf Fussgängerstreifen (Art. 47 VRV). Das Halten auf Tramschienen ist verboten (Art. 25 VRV).

Ausnahmen: Fährt das Tram auf einer Nebenstrasse, so sind die Verkehrsteilnehmer einer kreuzenden Hauptstrasse vortrittsberechtigt (Art. 45 VRV). Fahrzeuge der Blaulichtorganisationen sind gegenüber dem Tram vortrittsberechtigt (Art. 45 VRV).

Der für Stadtbahnen 2005 verbindlich gewordene israelische Standard SI 5350 entspricht der englischen Übersetzung der deutschen BOStrab. Dadurch wird auf Sicht gefahren (§ 49 SI 5350, übersetzt aus § 49 BOStrab) und die technische Signalisierung ist insgesamt übersetzt aus Anlage 4 zur BOStrab (SI 5350). Angewandt wird dieser Standard bei der Stadtbahn Jerusalem.

In aller Regel werden Straßenbahnnetze von kommunalen oder privaten Verkehrsunternehmen betrieben, nicht selten handelt es sich dabei um gemischte Ver- und Entsorgungsbetriebe wie Stadtwerke. Um Synergiepotenziale zu nutzen bildeten in den Anfangszeiten der elektrischen Straßenbahn oft auch Elektrizitätswerk und Straßenbahngesellschaft eine unternehmerische Einheit. Viele Städte führten gleichzeitig mit der elektrischen Straßenbahn auch die elektrische Straßenbeleuchtung ein. Die Straßenbahngesellschaften sind meist auch für den örtlichen Omnibus- und – soweit vorhanden – Oberleitungsbusverkehr zuständig. Insbesondere in den Staaten der ehemaligen Sowjetunion werden hingegen häufig nur Straßenbahn und Oberleitungsbus zusammengefasst – meist enden diese Unternehmensnamen auf "-elektrotrans" für "elektrischen Transport" – während der Omnibusverkehr von einer anderen Gesellschaft verantwortet wird. In bestimmten Ausnahmefällen treten auch Staatsbahnen als Betreiber von Straßenbahnen auf. Beispiele hierfür sind:

Eine weitere diesbezügliche Besonderheit waren die sogenannten "Staatsstraßenbahnen" in der Region Dresden. Dort war der Freistaat Sachsen in Form der Königlich Sächsischen Staatseisenbahnen zwar Eigentümer, vergab den Betrieb aber an die Dresdner Straßenbahn AG als Subunternehmer. Dies betraf die Lößnitzbahn, die Plauensche Grundbahn, die Vorortbahn Loschwitz–Pillnitz, die Bühlauer Außenbahn und die Vorortbahn Cotta–Cossebaude. 

Mitunter taucht der Begriff „Straßenbahn“ noch als Unternehmensname auf, obwohl diese schon seit Jahrzehnten keinen Schienenverkehr mehr durchführen. Beispiele hierfür sind die Hanauer Straßenbahn GmbH (HSB, seit 1944 nur Bus), die Celler Straßenbahn GmbH (seit 1956 nur Bus), die Aachener Straßenbahn und Energieversorgungs-AG (ASEAG, seit 1974 nur Bus), die Hagener Straßenbahn AG (seit 1976 nur Bus) und die Vestische Straßenbahnen GmbH (VESTISCHE, seit 1982 nur Bus).

In den Anfängen der elektrischen Straßenbahn wurden noch später unübliche Techniken für die Stromzuführung eingesetzt. Bei der Lokalbahn Mödling–Hinterbrühl in der Nähe von Wien und der ersten kommerziell eingesetzten elektrischen Straßenbahn in Deutschland, der Frankfurt-Offenbacher Trambahn-Gesellschaft (FOTG), wurden noch keine Stromabnehmer und Oberleitungen später üblicher Bauarten verwendet. Verwendung fand bei diesen beiden Bahnen stattdessen ein System aus einer zweipoligen Fahrleitung in Kupferrohren mit kleinem Durchmesser und einem Schlitz an der Unterseite (Schlitzrohrfahrleitung). In dieser liefen zwei Kontaktwägelchen, die von dem Triebwagen nachgezogen wurden. Als Fahrstrom wurde Gleichstrom mit 300 Volt verwendet.

Die frühen Straßenbahnen setzten zu den elektrischen, zunächst meist zweiachsigen Triebwagen ein bis zwei angehängte Beiwagen ein, auf Überlandstrecken auch mehr. Die Beiwagen waren in den ersten Jahren des elektrischen Betriebes häufig umgebaute Wagen der vormaligen Pferdebahn. Üblich waren zwei Varianten: offene Sommerwagen sowie geschlossene Wagen für den Ganzjahresbetrieb.

Die Entwicklung der Fahrzeuge war geprägt durch die Wünsche der jeweiligen Straßenbahnbetriebe, denen die Hersteller mit der Fertigung von individuellen Kleinserien folgten. Die oft engen Raumverhältnisse in den Städten erzwangen zudem auch besondere Anpassungen, aus denen heraus beispielsweise in Dresden die vorn und hinten verjüngten "Hechtwagen" entstanden. Die frühen Straßenbahnwagen hatten häufig Längsbänke mit einem Mittelgang, auf dem der Schaffner mit dem mancherorts in Deutschland typischen Galoppwechsler die Fahrgelder von den Fahrgästen einsammelte. Umstellungen des Betriebes auf Fahrgastfluss, verbunden mit einem fest eingebauten Schaffnersitz, führten zu – z. B. in Hamburg "Sambawagen" genannten – Fahrzeugen mit großer Aufnahmefläche im Einstiegsbereich. In Deutschland wurden nach dem Zweiten Weltkrieg erstmals bei der Hamburger Straßenbahn vierachsige Großraumwagen mit Fahrgastfluss eingeführt. Mit der Entwicklung der längeren Gelenkwagen wurden die Beiwagen zunehmend verdrängt.

Bis in die 1950er Jahre hinein wurde ein großer Teil der Straßenbahnwagen individuell nach Wunsch des jeweiligen Straßenbahnbetriebs angefertigt, nicht selten in eigener Werkstatt der Verkehrsbetriebe, besonders die Beiwagen. Schon seit den 1930er Jahren entstand ein Trend zu überörtlicher Standardisierung, um Herstellungs- und Wartungskosten zu senken. Das gilt für die amerikanischen PCC-Wagen der 1930er Jahre, die auch außerhalb der USA kopiert wurden. In Deutschland wurde während des Zweiten Weltkriegs zum Ersatz von Bombenschäden der Kriegsstraßenbahnwagen entwickelt, der einfach, aber leistungsfähig war. Seit den frühen 1950er Jahren fertigte die Düsseldorfer Waggonfabrik (Duewag) in Anlehnung an den PCC-Wagen den Duewag-Einheitswagen, der – vor allem als sechsachsiger Gelenkwagen – in der Bundesrepublik Deutschland und in Österreich weit verbreitet war. Diesem folgte in den 1970er Jahren der Stadtbahnwagen Typ M/N aus gleichem Hause. In der DDR wurde von der LOWA ein Einheitswagen entwickelt und vom VEB Waggonbau Gotha für die Straßenbahnbetriebe im ostdeutschen Staat produziert. Später kam es zu einer internationalen Vereinheitlichung der Straßenbahnproduktion in den Ländern des RGW. Der tschechoslowakische Hersteller ČKD Tatra lieferte nach einer Lizenz auf den PCC-Wagen basierende Fahrzeuge in die meisten Länder Osteuropas einschließlich der Sowjetunion. Weiterentwicklungen von ČKD Tatra wurden teils exportiert, teils wurde die Konstruktion von Herstellerbetrieben in Partnerländern nachgebaut, so von Konstal in Polen.

Ein 1923 erbauter Niederflur-Straßenbahnwagen wurde 1924 von der Waggonfabrik Uerdingen an die Frankfurter Lokalbahn geliefert, die diesen im Vorortverkehr auf Überlandstraßenbahnstrecken zwischen Frankfurt und Oberursel sowie Bad Homburg einsetzte. Dieses Fahrzeug sollte aus Sicht des Herstellers neue Aufträge generieren, blieb jedoch ein Einzelstück. Es war bis zum 31. Dezember 1954 in Betrieb. Außer Dienst gestellt wurde es, da die BOStrab die damals neuen Schienenbremsen vorschrieb, die dafür erforderlichen neuen Drehgestelle jedoch zu teuer gewesen wären. Das Fahrzeug befindet sich im Frankfurter Verkehrsmuseum. 1926 beschaffte die Große Leipziger Straßenbahn von Christoph & Unmack in Niesky und WUMAG in Görlitz je einen Dreiwagen-Versuchszug aus zweiachsigen Mitteleinstiegwagen und einem Niederfluranteil von 40 Prozent. Auf deren Grundlage wurden 1929 eine Serie von Mitteleinstiegbeiwagen mit nahezu identischen Maßen und einer Einstiegshöhe von 400 Millimetern geliefert. Die zugehörigen, erst 1930 und 1931 gebauten Triebwagen waren Vierachser, jedoch ohne Niederflureinstieg. Die Niederflurbeiwagen wurden erst 1973 ausgemustert, weil sie keinen Einbauraum für Magnetschienenbremsen boten. 

In den 1980er Jahren begannen die systematische Entwicklung der Niederflurtechnik sowie die Modularisierung der Fahrzeuge. Aufgrund der seinerzeit noch nicht so weit entwickelten Antriebs- und Fahrwerkstechnik verfügten die ersten Fahrzeuge nur über einen 60- bis 70-prozentigen Niederfluranteil im Fahrgastraum, wie bei den ersten Niederflur-Gelenktriebwagen (NGT6C) der Straßenbahn Kassel. Die Wagenteile auf den Antriebsdrehgestellen lagen hochflurig, was analog bei Omnibussen als Low-Entry-Konzept aus Gründen der Kostenersparnis wieder eingeführt wird. Antriebslose Wagenteile konnten auf Losradsätzen (Einzelradfahrwerken) oder als Sänften freischwebend durch benachbarte Wagenteile aufgehängt sein, wodurch ein durchgehend ebener Wagenboden entstand. Die elektrischen Einrichtungen wurden weitgehend auf dem Fahrzeugdach untergebracht, was fortan eine erhöhte Steifigkeit des Wagenkastens erforderte.

Da bis in die 1990er Jahre hinein konventionelle Hochflurfahrzeuge bestellt wurden und Straßenbahn-Triebwagen durchschnittlich eine Einsatzdauer von 40 Jahren aufweisen, stehen die Straßenbahnbetriebe bei der Umstellung auf Niederflurwagen vor einem Problem: ein 1990 angeschaffter Triebwagen wäre damit erst 2030 ausmusterungsreif, sodass der Fuhrpark erst dann vollständig auf Niederflur umgestellt wäre. Da aber eine frühere Außerdienststellung der zuletzt beschafften Hochflurwagen finanziell scheitert, gingen die Verkehrsbetriebe in Kooperation mit den Herstellern dazu über, in ihren vorhandenen Hochflur-Gelenktriebwagen nachträglich ein Niederflur-Mittelteil einzusetzen oder dieses umzubauen, sodass in einer dreiteiligen Zuggarnitur zumindest ein Niederflureinstieg zur Verfügung steht. Für zweiteilige Gelenktriebwagen wie die Duewag-Einheitswagen wurden neue Niederflur-Beiwagen (Anhänger) beschafft.

Moderne Fahrzeuge werden von Herstellern wie Bombardier Transportation oder Siemens in Modulbauweise hergestellt und sind an die örtlichen Gegebenheiten anpassbar (Normalspur/Meterspur, Fahrzeugbreite, Türen rechts/links/beidseitig, Fahrt in eine oder beide Richtungen, Achsanzahl, Niederfluranteil). Beispiele für derartige Fahrzeugserien sind der Combino, der ULF und die Variobahn.

Die maximale Laufleistung eines Radreifens liegt bei der Straßenbahn Budapest zwischen 250.000 km (Ganz-Gelenkwagen), 300.000 km (Tatra T5C5) und 500.000 km (Combino Plus).

Bei den ersten Straßenbahnen wurden die Wagen mit den Fahrgästen von Pferden gezogen. Dampflokomotiven im Straßenraum waren vor allem in großen Städten unbeliebt. In mehreren Städten der USA ließ man Straßenbahnen mittels in Schlitztunneln verlegter Kabel ziehen, die von stationären Dampfmaschinen angetrieben wurden. Das letzte System dieser Art ist das Cable Car in San Francisco. Mit der Elektrische Straßenbahn Lichterfelde in Groß-Lichterfelde (seit 1920 ein Ortsteil von Berlin) hielt 1881 der elektrische Antrieb Einzug in den Straßenbahnverkehr. Die in Lichterfelde angewandte zweipolige Energiezufuhr am Boden war aber für einen klassischen Straßenbahnbetrieb ohne separaten Gleiskörper ungeeignet. Werner von Siemens führte noch im selben Jahr auf der Internationalen Elektrizitätsausstellung 1881 in Paris eine erste Straßenbahn mit Oberleitung vor. Seine Schlitzrohroberleitung war noch anfällig, aber mit der versuchsweise 1884 und im Regelbetrieb von Frank Julian Sprague 1888 eingeführten Fahrdrahtoberleitung war und ist eine gefahrlose und zuverlässige Stromzufuhr am leichtesten zu bewerkstelligen.

Allerdings gab es in mehreren großen Städten ästhetische Vorbehalte gegen Oberleitungen. Eine 1885–1888 in Denver (Colorado), danach vor allem im New Yorker Stadtteil Manhattan praktizierte Ausweichlösung waren unterirdische Stromschienen, die man dort großenteils in die schon vorhandenen Kabelbahnrinnen legen konnte. Unter anderem im Stadtzentrum von Berlin gab es Linien mit Akkumulatoren als Energiequelle, ebenso in Paris. In Paris setzte man im Stadtzentrum nicht zuletzt auf den 1876 von Louis Mékarski entwickelten Druckluftantrieb. Nach einer elfjährigen Experimentierphase liefen Druckluftstraßenbahnen im Regelbetrieb von 1887 bis 1906. Es gab auch Gasmotorantrieb, doch der Einsatz von Verbrennungsmotoren fiel in den USA in die Phase des Niedergangs der Straßenbahnen, als die großen Erdölgesellschaften daran gingen, die elektrische Konkurrenz auszuschalten.

Insgesamt setzte sich der Oberleitungsbetrieb wegen seiner Zuverlässigkeit durch. Erst mit Beginn des 21. Jahrhunderts gab es entscheidende Verbesserungen bei Stromspeichern und Stromschienen, so dass nun vermehrt oberleitungsfreie Streckenstücke zu finden sind. Die ersten Straßenbahnfahrzeuge mit Brennstoffzellen existieren bereits, auch wenn sie noch nicht planmäßig eingesetzt werden (Stand 2015).

Pferdebahnen waren anfangs weit verbreitet. Sie wurden meist auf elektrischen Betrieb umgestellt oder aus wirtschaftlichen Überlegungen eingestellt. Zu dieser Antriebsart gehörten auch jene Bahnen, die von Maultieren gezogen wurden. Eine Sonderform der Pferdebahn stellte der sogenannte Perambulatorbetrieb dar, hierbei konnten die Wagen sich auch abseits der Schienen fortbewegen.

Ein Beispiel für die gegenwärtige Nutzung als Attraktion ist die im Juni 2007 wieder in Betrieb genommene Döbelner Straßenbahn, die in den Sommermonaten an jedem ersten Samstag sowie zu besonderen Anlässen verkehrt.
Der Gasmotorantrieb wurde erstmals von "Hasse & Co." in Berlin erprobt, deren Konstruktion einer Gaslokomotive am 6. Dezember 1877 als deutsches Patent veröffentlicht wurde. Viele Konstrukteure im In- und Ausland begannen die Verbesserung der Vorrichtung, so Gottlieb Daimler in Cannstatt, die Gebrüder Körting in Hannover und Holt in Manchester. Der erste praktische Versuch einer Gasmotorenstraßenbahn wurde 1885 in Melbourne durchgeführt. Ihre Bedeutung erlangte diese Konstruktion durch Karl Lührig in Dresden, dessen Entwicklung 1892 veröffentlicht wurde und der einen Probebetrieb mit fünf Motorwagen in Dresden einrichtete. Die Patente Lührigs wurden von der im März 1895 gegründeten Deutschen Gasbahngesellschaft in Dessau übernommen, die im Anschluss an die 1894 erfolgte Eröffnung der Dessauer Gasbahn gegründet wurde. Die erste längere Gasbahn ging 1897 zwischen Hirschberg und Warmbrunn mit einer Länge von 15,2 Kilometern in Betrieb, die Hirschberger Talbahn. Hohe Betriebskosten und häufige Reparaturen der Gasmotoren führten zum wirtschaftlichen Misserfolg, sodass 1899 die Spurweite von Normalspur auf Meterspur umgestellt und 1900 der elektrischen Betrieb eingeführt wurde. Die einzige nennenswerte Gasmotorenbahn in Deutschland befand sich in Dessau. Die Wagen benötigten durchschnittlich einen halben Kubikmeter Gas pro Wagenkilometer. In der Komprimierstation wurde das Gas auf acht bis zwölf Atmosphären komprimiert. Mit einer Gasladung wurden zwölf bis sechzehn Kilometer und mehr bewältigt. Eine Ladung dauerte etwa zwei bis drei Minuten.

Gottlieb Daimler betrieb zur Demonstration und Weiterentwicklung des von ihm für den Fahrzeugeinsatz weiterentwickelten Viertakt-Ottomotors von 1887 bis 1899 eine Benzinmotor-Straßenbahn mit 450 Millimetern Spurweite, die von seiner Werkstatt unweit des Kursaals in Cannstatt bei Stuttgart durch die Königstraße (→ König-Karl-Straße) zum Wilhelmsplatz (Strecke der Stadtbahnlinie U2) führte. Gleise und Betriebsanlagen ließ Daimler auf eigene Kosten errichten. Zum Einsatz kamen kleine, zweiachsige, offene Verbrennungstriebwagen mit "Dos-à-dos"-Längssitzbänken. Für die circa 700 Meter lange Strecke benötigten die Wagen etwa zwei Minuten. Die Bahn galt als Attraktion und erfreute sich in der Bevölkerung großer Beliebtheit, auch die Antriebstechnik bewährte sich, sodass von den damaligen Pferdebahngesellschaften "Stuttgarter Pferde-Eisenbahnen" (SPE) und "Neue Stuttgarter Straßenbahnen Lipken & Co." (NSS) bald Anfragen für einen Einsatz der Verbrennungsmotortechnik in ihren Fahrzeugen vorlagen.

Es wurden drei Versuchsfahrzeuge gebaut und auf den Stuttgarter Pferdebahnnetzen zu teilweise öffentlichen Probefahrten eingesetzt. Die Versuche blieben aus verschiedenen Gründen erfolglos. Ab 1895 setzte sich endgültig die elektrische Traktion in Stuttgart durch. Dennoch konnte Daimler aufgrund der guten Erfahrungen mit der Cannstatter Versuchsstrecke mehrere Aufträge zur Einrichtung von Touristenbahnen erhalten. Als 1899 die meterspurige elektrische Cannstatter Straßenbahn durch die Königstraße gebaut wurde, verlor die Daimler-Bahn an Bedeutung und wurde eingestellt. Erhalten blieben Teile der Fahrzeugremise als Unterstehhütte im unteren Kurpark und ein 1894 erbauter Motorwagen, der im Mercedes-Benz-Museum in Bad Cannstatt ausgestellt ist.

Die Nordhäuser Straßenbahn setzt seit 2004 Fahrzeuge mit Hybridantrieb auf der Duolinie 10 ein, die innerstädtisch mit Elektromotor und Oberleitung auf den Gleisen der Straßenbahn verkehren und außerhalb von Nordhausen auf den Gleisen der Harzer Schmalspurbahnen von einem Dieselmotor angetrieben werden. Die Strecke führt quer durch die Nordhäuser Innenstadt und weiter zum Nachbarort Ilfeld. Ähnliche, allerdings normalspurige Dieselhybridfahrzeuge fahren für die RegioTram Kassel. Die meterspurige Straßenbahn von Rotterdam nach Hellevoetsluis wurde ebenfalls mit Diesellokomotiven betrieben.

Zur Energieversorgung wird fast immer Gleichstrom mit einer Spannung zwischen 500 und 750Volt verwendet. Der Wert der gewählten Spannung variiert zwischen den Unternehmen. Es gab vormals an manchen Orten Straßenbahnen, die mit Drehstrom versorgt wurden. Wegen der aufwändigen zweipoligen Oberleitung wurde es nur selten angewendet und blieb nicht dauerhaft im Einsatz. Die Antriebsmotoren sind allerdings bei vielen modernen Wagensystemen Drehstrommaschinen, von der Bauart zumeist Drehstrom-Asynchronmotoren. Dafür wird mit Wechselrichtern aus dem Gleichstrom ein Dreiphasen-Wechselstrom erzeugt, mit dem die Motoren betrieben werden. Über Änderung der Frequenz wird die Drehzahl der Motoren und damit die Fahrgeschwindigkeit geregelt. Daneben verfügen zahlreiche Triebwagen über Gleichstromreihenschlussmotoren, die mit einem Gleichstromsteller auf Thyristorbasis in der Drehzahl gesteuert werden. Bei Fahrzeugen, die etwa bis 1965 entwickelt wurden, werden die Fahrmotoren über Vorwiderstände gesteuert, wobei die zu Anfang verwendeten Schleifring- erst durch Nockenfahrschalter und zuletzt durch Schützensteuerungen ersetzt wurden. Oft sind bei diesen Fahrzeugen zwei Widerstandssätze vorhanden, einer auf dem Dach und einer unter den Sitzen, der im Winter die Funktion der Fahrzeugheizung übernimmt. Der Wunsch nach höheren Fahrgeschwindigkeiten führte nach Versuchen mit Verbundschaltungen, durch die gekuppelte Triebwagen wie ein einzelner geschaltet wurden, zur Anwendung von Vielfachsteuerungen auch im Straßenbahnbetrieb.

Nach dem Ersten Weltkrieg wurden die Fahrmotoren auch zum Bremsen genutzt, indem sie als Generatoren geschaltet und der dadurch induzierte Strom in Widerständen in Wärme umgewandelt wurde. Dafür wurden in der Regel die Anfahrwiderstände einschließlich der Wagenheizkörper mitbenutzt. Die Beiwagen erhielten Solenoide, die die mechanische Bremse betätigten, dadurch wurde auf einfache Weise eine allerdings nicht selbsttätig wirkende Zugbremse realisiert. Mit der Einführung von rückspeisefähigen Gleichrichterunterwerken ab den 1980er Jahren konnte auch in Straßenbahnnetzen zur Nutzbremse übergegangen werden.

Vor der Einführung der Widerstandsbremse wurde die Gegenstrombremsung genutzt, jedoch war das in der Regel nur als Notbremsmöglichkeit im Gefahrenfall zulässig.

Elektrizität wird meistens mittels Stromabnehmer durch Oberleitungen zugeführt.

Seitliche Stromschienen, die über dem Erdboden liegen, werden bevorzugt in Tunneln eingesetzt.

Aus ästhetischen Gründen (Verzicht auf die Oberleitung) wurde manchmal auch eine Stromabnahme aus unterirdischen Stromschienen eingebaut, so auf der Wiener Ringstraße zwischen 1898 und 1915. Auch in Paris wurden ab 1896 verschiedene Linien und Teilstrecken mit unterirdischen Stromschienen ausgerüstet.

Seit der Jahrtausendwende werden wieder Teilstrecken mit einer mittig zwischen den Fahrschienen liegende Stromschiene gebaut. Die Stromschiene ist dabei in einzelne Segmente unterteilt, die aus Sicherheitsgründen für andere Verkehrsteilnehmer nur dann unter Spannung stehen, wenn sie sich unter einem darüberfahrenden Straßenbahnzug befinden. Es gibt dabei zwei verschiedene Systeme:

In der Anfangszeit wurden auch Akkumulatortriebwagen eingesetzt. In Paris etwa waren sie ab 1892 im Linienbetrieb unterwegs. Problematisch waren neben der Masse der seinerzeit verfügbaren Bleiakkumulatoren, die sich negativ auf die Fahrleistungen und den Oberbau auswirkten, ihre mechanische Empfindlichkeit und die bei der Ladung auftretenden Säuredämpfe. 

Im 21. Jahrhundert werden nun erneut Batterien mit verbesserter Technik eingebaut, mit deren Hilfe fahrleitungsfreie Streckenabschnitte befahren werden können.

Ein in Erprobung befindliches Energieversorgungssystem stellt das von Bombardier Transportation vorgestellte Primove dar. Dieses basiert auf induktiver Energieübertragung anstelle von Fahrleitungen, zusätzlich sind die Fahrzeuge mit Akkumulatoren für den Antrieb ausgestattet. Von September bis Dezember 2010 fand ein Testbetrieb mit einem umgerüsteten Straßenbahnwagen der Rhein-Neckar-Verkehrsbetriebe auf einer 800Meter langen Teststrecke am Augsburger Messegelände statt. Vorteile der drahtlosen Energieübertragung sind die geringere visuelle Beeinträchtigung, da es keine Fahrleitungsanlagen mehr gibt. Nachteile sind in den hohen Kosten und der im Vergleich zu Oberleitung schlechtere Wirkungsgrad der drahtlosen Energieübertragung zum Fahrzeug.

Nach ernst zu nehmenden Quellen hat die Sifang Company aus der ostchinesischen Küstenstadt Qingdao ein Straßenbahnfahrzeug entwickelt, das durch Brennstoffzellen angetrieben wird. Die Brennstoffzelle wird mit Wasserstoff betrieben, der in Drucktanks gespeichert ist. Das Betanken dauert nur drei Minuten. Mit einer Füllung kann bei einer Spitzengeschwindigkeit von 70km/h eine Entfernung von bis zu 100Kilometern zurückgelegt werden.

Der französische Hersteller Alstom will bis 2018 einen Brennstoffzellen-Triebwagen entwickeln.

Im Jahr 1870 konstruierte Louis Mékarski einen Druckluftantrieb für Straßenbahnen. 1876 erhielt Mékarski die Konzession zum Betrieb einer Straßenbahn in Nantes, die 1879 ihren Betrieb aufnahm.

Das Hauptbetätigungsfeld Mékarskis wurden die Pariser Straßenbahnen: 1894 wurden dort die ersten Mékarski-Wagen eingesetzt. Bis 1903 wurden insgesamt zehn Linien auf Druckluftbetrieb umgestellt. Die letzten Fahrzeuge des Typs Mékarski wurden im August 1914 durch elektrische Triebwagen ersetzt. In einigen anderen kleineren Städten Frankreichs kamen derartige Fahrzeuge jedoch zum Einsatz.

An einigen Orten wurden gegen Ende des 19. Jahrhunderts Dampfstraßenbahnen errichtet, etwa wenn sich der Bau einer "vollwertigen" Eisenbahn nicht lohnte. Diese wurden meist mit speziellen Kleinlokomotiven und Leichtbau-Personenwagen betrieben, und die Schienen waren überwiegend im Straßenplanum verlegt. Bekannte Beispiele sind die Dampfstraßenbahn Neuötting–Altötting und die Filderbahnstrecke von Degerloch nach Hohenheim. Die letztere wurde von 1888 bis zu ihrer Elektrifizierung 1902 mit Dampf betrieben. Daneben ist die Straßenbahn der Siegener Kreisbahnen zu nennen, die inzwischen nicht mehr existiert. Am ehesten vermittelt noch die (allerdings als Eisenbahn konzessionierte) Chiemsee-Bahn von Prien nach Stock einen Eindruck von den damaligen Dampfstraßenbahnen.

In Bern ist ein betriebsfähiges Dampftram erhalten und wird von der "Berner Tramway-Gesellschaft AG" als Attraktion betrieben. Diese Tochtergesellschaft von Bernmobil baut derzeit (Stand Frühling 2007) nach alten Plänen ein Luftdrucktram (Lufttram) nach. Von 1890 bis 1902 verkehrten zehn druckluftbetriebene Straßenbahntriebwagen auf einer Linie durch die historische Altstadt von Bern.

Eine ungewöhnliche Bauform war die Natronlokomotive, die Mitte der 1880er Jahre versuchsweise bei der Straßenbahn Aachen sowie in Charlottenburg eingesetzt wurde und den Dampf mittels erhitzter Natronlauge erzeugte. Diese Bauform entlastete durch das Fehlen einer Feuerung den Fahrer und war zudem im Betrieb völlig rauchlos. Sie konnte sich jedoch nicht durchsetzen, da die Korrosion infolge der verwendeten Natronlauge eine regelmäßige und teure Erneuerung der Kesselanlage erforderte.

Straßenbahnen mit Standseilbahn-Technik werden als Kabelstraßenbahnen bezeichnet. Weltweit sind nur noch wenige in Betrieb. Die bekanntesten sind die Cable Cars in San Francisco, welche seit 1873 unterwegs sind. Auch Paris hatte eine Kabelstraßenbahn: Sie befuhr im Stadtteil Belleville von 1891 bis 1924 eine Steilstrecke. Betreiber war die Compagnie du Tramway Funiculaire de Belleville.

Innerstädtische Standseilbahnen (mit Pendelkabinen) sind keine Straßenbahnen, da sie meist auf einer ihnen allein vorbehaltenen stark geneigten Trasse verkehren, zum Teil sogar gänzlich in einem Tunnel. Als Beispiele sind die Tünel und die Füniküler Kabataş–Taksim in Istanbul, die Fun'ambule in Neuchâtel oder Polybahn und Seilbahn Rigiblick in Zürich zu nennen.

Wie andere Landfahrzeuge auch brauchen Straßenbahnfahrzeuge mindestens zwei voneinander unabhängige Bremssysteme, von denen mindestens eines auch nach Ausfall der Stromzufuhr funktionieren muss. Als Betriebsbremse ist die elektrodynamische Bremse üblich und in Deutschland von der BOStrab vorgeschrieben. Hier bewirkt der Elektromotor selbst die Bremsverzögerung, indem er zum Generator wird. Bei der älteren Form, der Kurzschlussbremse, wird die kinetische Energie in einem Widerstand in Wärme umgewandelt, bei modernen elektrodynamischen Bremsen wird in das Versorgungsnetz eingespeist. Daneben gibt es mechanische Bremsen, bei denen die Bremskraft entweder in Form von Überdruck als Druckluftbremse oder von Federkraft als Federspeicherbremse vorgehalten wird. Die mechanischen Bremsen wirken meist auf die Räder, sei es als Trommel-, Scheiben- oder Klotzbremse. Historisch wurden auch Bremsen verwendet, deren Klötze sich elektromagnetisch an die Schienen zogen. Jeder Wagen eines Zuges hat mindestens eine Bremse, die vom Fahrer ausgelöst werden kann. Außer bei musealen Fahrzeugen sind auch Notbremsen vorgeschrieben. Außerdem ist nach BOStrab eine Magnetschienenbremse vorgeschrieben. Bei dieser wird die Bremskraft erzeugt, indem ein Schleifschuh durch einen Elektromagneten auf die Schiene gezogen wird.

In deutschsprachigen Ländern ist die Straßenbahnklingel so bekannt, dass sie in Wien (die Bim), in Leipzig (de Bimmel) und in Dresden (de Glocke) gar zum Spitznamen der Straßenbahn wurde. Über lange Zeit gab es eigentlich zwei Klingeln: Die "Warnglocke" entspricht der Hupe schienenunabhängiger Motorfahrzeuge. Darüber hinaus hatte lange Zeit jeder Straßenbahnwagen, also auch der Beiwagen, an beiden Enden je eine "Signalglocke", die vom Schaffner mittels einer über Kopfhöhe längs durch den Wagen geführten Klingelschnur betätigt werden konnte. Dies ermöglichte es dem Zugpersonal, im laufenden Betrieb untereinander zu kommunizieren. Mit der Verbesserung elektrischer Kommunikationsmittel wurden die Signalglocken ersetzt. In den in Osteuropa hergestellten Straßenbahnwagen kam eine dritte Art von Klingel auf: Als die zentrale Steuerung der Türen ohne Schutzvorrichtungen wie Trittbretttasten oder Lichtschranken eingeführt wurde, warnte ein durchdringender Klingelton die Fahrgäste, bevor die Türen sich elektrisch schlossen.

Die Entwicklung der Straßenbahnen begann mit der Einführung der Pferdebahnen. Die erste wurde am 26. November 1832 in New York eröffnet, kurz nachdem die Pferdeeisenbahn Budweis–Linz–Gmunden am 1. August 1832 als zweite europäische Festlandeisenbahn eröffnet worden war. Später sind die Pferdebahnen auf einzelnen Linien durch Dampfstraßenbahn-Fahrzeuge ersetzt worden. Die erste Straßenbahn in Europa war die Überlandstraßenbahn Montbrison–Montrond, sie nahm ihren Betrieb trotz Konzession für Dampftraktion 1839 mit Pferdekraft auf; 1848 musste sie ihn wieder einstellen. Wesentlich kürzer und noch kurzlebiger war die erste Wiener Pferde-Tramway, die 1840 bis 1842 in der Nähe des Augartens betrieben wurde. Die erste pferdebetriebene Tramstrecke der Schweiz wurde 1862 in Genf in Betrieb genommen. Die erste deutsche Pferdebahn wurde 1865 zwischen Berlin und Charlottenburg durch die Berliner Pferde-Eisenbahn eröffnet, die erste deutsche Dampfstraßenbahn fuhr ab 1877 in Cassel. Es wurde auch mit weiteren Antriebsarten (beispielsweise Druckluft oder Natron) experimentiert; schon bald überzeugte der elektrische Antrieb durch seine Leistungsfähigkeit.

Ab dem 3. September 1880 führte der russische Artillerieoffizier Fjodor Pirozki auf einer kurzen Vorortstrecke des Petersburger Straßenbahnnetzes einen mehrwöchigen Versuchsbetrieb mit einem umgebauten Pferdebahnwagen durch. Es wurde auch international darüber berichtet, aber er betrieb das nur als Hobby und verfolgte es später nicht weiter.

Am 16. Mai 1881 nahm in Lichterfelde bei Berlin die Elektrische Straßenbahn Lichterfelde–Kadettenanstalt den Probebetrieb auf. Die von Werner von Siemens gebauten Wagen hatten eine Länge von 4,3 m und fuhren auf einer 2,5 km langen Strecke mit einer Geschwindigkeit von maximal 20 km/h. Die Stromaufnahme des 3,7 kW (5 PS) starken Motors erfolgte über beide Schienen. Ab 1883 verkehrte sie im regulären Betrieb zwischen der Preußischen Hauptkadettenanstalt in Lichterfelde West und dem Bahnhof Lichterfelde. Im strengen Sinne war es keine Straßenbahn, denn das Streckengleis befand sich auf einer eigenen Trasse neben den Fahrwegen oder im freien Gelände. Das Betreten des Bahnkörpers war wegen der Lage beider Pole der Energiezufuhr am Boden von Anfang an untersagt.

Gleiches gilt für Volk’s Electric Railway in Brighton. Am 4. August 1883 in Betrieb genommen, gilt sie als erste elektrische Straßenbahn in Großbritannien.

Im Jahr 1881 fand in Paris die "Exposition d’électricité" statt. Aus diesem Anlass baute "Siemens & Halske" eine etwa 500 Meter lange, von der Place de la Concorde zum Palais de l’Industrie führende Strecke. Letzteres befand sich an Stelle der Grand Palais. Das Doppelstockfahrzeug mit offenem Deck konnte 50 Personen befördern. Die Stromzufuhr erfolgte über Schlitzrohrfahrleitungen. In Paris handelte es sich um zwei Messingröhren von 22 Zentimeter Durchmesser, die unten einen sechs Millimeter breiten Schlitz hatten. Im Innern der Röhren befand sich je ein metallener beweglicher Körper – genannt Schiffchen – der über Kabel mit dem Fahrzeug verbunden war und vom fahrenden Fahrzeug mitgezogen wurde. Die Bahn war nur während der Dauer der Ausstellung in Betrieb. Es handelte sich um die weltweit erste Strecke mit Oberleitung.

In Österreich eröffnete die erste elektrische Straßenbahn am 22. Oktober 1883 zwischen Mödling und Hinterbrühl den Betrieb, sie hatte eine symmetrisch angebrachte zweipolige Schlitzrohroberleitung und wurde in Teilbereichen im Straßenplanum geführt.

Am 18. Februar 1884 wurde die erste regelmäßig in Deutschland betriebene elektrische Straßenbahn mit Oberleitung von der Frankfurt-Offenbacher Trambahn-Gesellschaft (FOTG) zwischen der Alten Brücke in Frankfurt-Sachsenhausen und dem Mathildenplatz in Offenbach eröffnet. Die Oberleitung war noch eine zweipolige Schlitzrohrfahrleitung, und als Stromabnehmer dienten daran hängende kleine Kontaktwagen. Ein Teil der historischen Strecke wird noch von der Straßenbahnlinie 16 befahren, jedoch wurde das Frankfurter Ende von der Alten Brücke in Sachsenhausen nach und nach über den Frankfurter Hauptbahnhof, die Hamburger Allee und die Schloßstraße bis nach Ginnheim verlängert, während das Offenbacher Ende um 2000 stillgelegt und durch eine Stadtbuslinie ersetzt wurde.

Der elektrische Straßenbahnbetrieb in der Schweiz begann am 6. Juni 1888 mit dem ersten Teilstück der Tramway Vevey–Montreux–Chillon.

In den USA entwickelte Frank Julian Sprague 1888 einen Straßenbahnwagen, der seine Energie aus einer Oberleitung bezog, und baute damit in Richmond das erste erfolgreiche größere elektrische Straßenbahnsystem auf. Dabei wurden auf den Hügeln von Richmond Steigungen bis 10 % bewältigt, was sich als überzeugender Prüfstein für die Anwendung in anderen Städten erwies. Um 1889 befanden sich 110 elektrische Bahnen mit Sprague-Ausrüstung in Bau oder in Planung. Edison, der Sprague-Ausrüstungen anfertigte, kaufte 1890 die "Sprague Electric Railway & Motor Company" auf.

Im April 1891 wurde in Halle an der Saale die erste große innerstädtische elektrische Straßenbahnlinie eröffnet, mit Straßenbahnwagen erbaut von der Allgemeinen Elektricitäts-Gesellschaft (AEG) nach den Patenten des Amerikaners Frank J. Sprague, mit Stangenstromabnehmer. Eine Demonstrationsstrecke nach diesem System war im Jahr zuvor vorübergehend anlässlich der Nordwestdeutschen Gewerbe- und Industrieausstellung betrieben worden. Bis Juni 1891 kamen drei weitere Linien hinzu. Halle gibt an, dass dies das erste elektrische Straßenbahnnetz Europas war.

Am 22. Februar 1892 wurde in Gera die zweitälteste noch existierende elektrische Straßenbahn eröffnet. Im gleichen Jahr begann der dauerhafte elektrische Betrieb in Bremen. 1893 kamen Chemnitz, Dresden und Hannover hinzu, 1894 Hamburg, Dortmund, Erfurt, Gotha und Wuppertal, auch die Straßenbahn Plauen nahm ihren Betrieb auf.

In den Jahren 1895 und 1896 verlegte die "Finnische Gesellschaft für Schifffahrt" auf der zugefrorenen Newa Gleise für drei elektrisch betriebene Straßenbahnlinien. Sie umging damit das Monopol der "Zweiten Aktiengesellschaft der Pferdebahnen in Sankt Petersburg". Mit dem Neubau einer Brücke endete 1910 dieser Betrieb.

In Deutschland wurden erste Fahreignungsuntersuchungen an Straßenbahnfahrern am „psychotechnischen Labor“ von Hugo Münsterberg durchgeführt. Diese Untersuchungen gehörten zu den ersten Berufseignungstests in der Geschichte der Psychologie überhaupt und stellen einen Meilenstein in der Entwicklung der verkehrspsychologischen Diagnostik dar.

Der Begriff „Straßenbahn“ für schienengebundene Transportmittel in Städten ist bereits aus den 1860er Jahren belegt, bis mindestens Ende der 1880er Jahre mangelte es jedoch an einer genauen Abgrenzung des Begriffs.

Genehmigungen, Konzessionen und Betriebsvorschriften von Straßenbahnen wurden in Deutschland zunächst von lokalen und regionalen Behörden, Eisenbahnverwaltungen und Aufsichtsbehörden allein oder in gemeinsamer Abstimmung vorgegeben, wobei selbst innerhalb eines Landes lokal uneinheitlich vorgegangen wurde. Die ersten Pferdebahnen wurden offenbar durchgängig bereitwillig genehmigt und die Vorschriften meist von den Polizeidirektionen erlassen. Sie wurden dabei mehr den Pferdeomnibussen gleichgestellt, wobei zunächst offen war, ob sie als Eisenbahnen betrachtet werden sollten oder nicht.

Richtungsweisend war schließlich das "Preußische Gesetz über Kleinbahnen und Privatanschlussbahnen" vom 28. Juli 1892, in dem dreierlei unterschieden wurde: Bahnen ohne Maschinenbetrieb (Pferdebahnen), die regionalen Polizeibehörden unterstanden; Kleinbahnen mit Maschinenbetrieb (zu denen auch die Straßenbahnen zählten), die dem Kleinbahngesetz unterlagen; und Eisenbahnen, die der Eisenbahngesetzgebung unterstanden. In den Ausführungsbestimmungen von 1898 wurden die Kleinbahnen differenziert in Straßenbahnen und nebenbahnähnliche Kleinbahnen. In der Folge setzte in Preußen ein Boom im Kleinbahn- und Straßenbahnbau ein, da die Gesetzeslage jetzt einheitliche und gegenüber Eisenbahnen erheblich vereinfachte Betriebsvorschriften und Genehmigungen vorsah. Angeregt durch diesen Erfolg übernahmen andere deutsche Länder den Grundgedanken dieser Gesetzgebung.

Auch in der Weimarer Republik bestanden diese Regelungen fort. Die Eisenbahngesetzgebung wurde bereits in den 1920er Jahren vereinheitlicht, aber erst 1937 die ab 1. April 1938 deutschlandweit geltende "Verordnung über den Bau und Betrieb der Straßenbahnen", kurz "BOStrab", erlassen und frühere Gesetze aufgehoben. Fortan wurde nurmehr zwischen Straßenbahnen einerseits und Eisenbahnen andererseits unterschieden.

Die Verkehrsunternehmen, die bisher unter die Kleinbahngesetzgebung der Länder fielen, mussten entscheiden, ob sie ihren Betrieb zukünftig als Straßenbahnen oder als Eisenbahnen durchführen wollten. Bedingt durch die vorangegangene unterschiedliche gesetzliche Entwicklung in den deutschen Ländern wurden ab 1938 zum Beispiel die großen Straßenbahn- und Kleinbahnnetze an Rhein und Ruhr (Preußen) bis auf Ausnahmen als Straßenbahnen betrieben, im Land Baden wurden vergleichbare elektrische Überlandbahnen zu Eisenbahnen.

Seit 1938 wurden die Bestimmungen immer wieder aktuellen Erfordernissen und Bedingungen angepasst; die Grundprinzipien – die Unterscheidung zwischen Straßenbahn und Eisenbahn mit vereinfachten Betriebsvorschriften für die Straßenbahn – blieben noch erhalten.

In rascher Folge wurden weltweit viele Straßenbahnbetriebe – in Deutschland gab es diese um 1900 in ca. 150 Städten – gegründet und verdrängten die anderen Traktionsarten. Die Straßenbahn entwickelte sich zum Massentransportmittel. Sie beeinflusste in maßgeblicher Weise Entwicklung und Planung der Städte, indem sie das Umland mit der Stadt verband und durch kostengünstige Reisemöglichkeiten und ihre große Beförderungskapazität die Bedingungen für die Ansiedlung großer Industrien an der Peripherie der Städte schuf.
Anfang des 20. Jahrhunderts entstanden überwiegend in Europa und den USA sehr viele Bahnen, die auch außerhalb der Städte aus Kostengründen nicht auf eigener Trasse, sondern auf bereits vorhandenen Straßen verkehrten, ohne als Straßenbahn konzipiert zu sein. Die Konflikte mit dem Straßenverkehr waren durchaus programmiert und auch gewünscht. So haben sich manche Konzessionsbetriebe lange Zeit geweigert, ihre Schienen so zu verlegen, dass diese von anderen Fahrzeugen passiert oder überquert werden konnten. Diese Epoche wird als die Zeit der "Eisenbahnschlachten" in der Verkehrsgeschichte gehandelt.

Um 1920 war der Höhepunkt der Straßenbahnentwicklung erreicht, als in Europa, Nord- und Südamerika sowie Australien fast alle Groß- und Mittelstädte, aber auch viele Kleinstädte über Straßenbahnen verfügten. Zu jener Zeit waren ausgedehnte Überlandstraßenbahnnetze sehr verbreitet. Weltweit existierten damals über 3000 Straßenbahnbetriebe, nur wenige davon in Afrika und Asien. Es bestanden neben sehr großen Netzen auch diverse Klein- und Kleinstbetriebe, und in den Großstädten existierten oft mehrere, bis zu zwei Dutzend, einzelne, private Betriebe nebeneinander.
Zunächst in Nordamerika, später auch in Europa und den anderen Kontinenten, reduzierte sich mit dem Aufkommen des motorisierten Individualverkehrs die Zahl der Straßenbahnen. Die Betriebe mussten häufig geschlossen werden, da vielerorts jahrzehntelang kaum noch in sie investiert wurde und sie fast vollständig veraltet waren. Für die stillgelegten Straßenbahnen wurden häufig Oberleitungsbussysteme eingerichtet, in der Mehrzahl überwogen jedoch Omnibusse mit Verbrennungsmotoren.

Die Stilllegungswelle verlangsamte sich zunächst durch den Zweiten Weltkrieg, da viele Busse für den Kriegsdienst verwendet wurden. Obwohl in der Sowjetunion nach Beendigung des Krieges ab 1945 neue Betriebe eröffnet wurden, nahm weltweit die Zahl der Straßenbahnbetriebe ab. Zudem war damals die Meinung vorherrschend, die Straßenbahn zerschneide und behindere den Verkehrsfluss, deshalb wurden auch unzerstörte Linien stillgelegt. Im einstigen Ostblock wurde dagegen konsequent auf Straßenbahnen gesetzt. Hier verschwanden andererseits Straßenbahnen – auch aus touristisch wichtigen und gut erschlossenen Bereichen wie der (Ost-)Berliner Mitte rund um den Alexanderplatz.

Um 1970 schien sicher zu sein, dass in wenigen Jahrzehnten der letzte Straßenbahnbetrieb im Westen aufgegeben werden würde. So wurden in jener Zeit überwiegend der Einsatz von Omnibussen und der Bau von U-Bahnen gefördert. Doch der extrem angestiegene motorisierte Individualverkehr in den Städten hatte die Lebensqualität, die Ölkrise das Bewusstsein der Menschen so verändert, dass eine neue Richtung der Verkehrspolitik erwogen werden musste.

Vor allem in Mittelstädten, in denen sich der sehr teure U-Bahn-Bau nicht rechnet, kehrte die Straßenbahn zurück. In Deutschland, aber auch zahlreichen anderen Staaten war eine Modernisierung der Bahnen angestrebt, wobei sie häufig durch eigene Gleiskörper und den Bau einzelner Tunnelabschnitte aus dem allgemeinen Verkehrsfluss genommen wurden. Aus der Straßenbahn entstand so mancherorts eine Stadtbahn. Die steigenden Fahrgastzahlen bewiesen die Akzeptanz der Veränderung durch die Bevölkerung. Die Einführung der Niederflurtechnik zeigte die Möglichkeiten und die Bequemlichkeit des bewährten Verkehrsmittels.

Eine weltweite Trendwende begann ab 1980 zunächst in Nordamerika, etwas später auch in Europa, als neue Straßen- und Stadtbahnbetriebe eröffnet wurden. Als erste Stadt hat Edmonton in Kanada Anfang der 1980er Jahre begonnen, sein Straßenbahnnetz wieder aufzubauen. Ab etwa 1985 erreichte die Trendwende auch Europa. Obwohl seitdem auch einige kleine und unrentable Betriebe stillgelegt wurden, stieg durch Neueröffnungen die Anzahl der Straßenbahnbetriebe insgesamt an. Dort, wo neue Betriebe eröffnet und bestehende Netze erweitert wurden, überschritten die Fahrgastzahlen in der Regel sämtliche Prognosen.

Die Wiedereinführung der Straßenbahn ging vielfach mit einem Umbau der durch sie genutzten Straßen und der Stadtzentren einher, indem die Aufteilung des Straßenraumes verändert wurde. Die Flächen für den motorisierten Individualverkehr wurden reduziert, und Straßenbahn, Radfahrer sowie Fußgänger bekamen mehr Platz. Besonders in Frankreich, aber auch in den USA finden sich Beispiele derartiger Verkehrspolitik.

Die teilweise Benutzung vorhandener oder brachliegender Eisenbahnstrecken – Karlsruhe war und ist hier beispielgebend – schuf weitere Möglichkeiten der Erschließung neuer Verkehrsräume. Die Menschen waren nun in der Lage, mit öffentlichen Verkehrsmitteln ohne Umsteigen zügig die Stadtzentren zu erreichen.

Sogar von Güterstraßenbahnen wird wieder gesprochen. In Dresden wird durch die Straßenbahn der Güterverkehr zur Versorgung der Gläsernen Manufaktur betrieben. Der Transport der Fahrzeugteile geschieht mit eigens dafür gefertigten Güterstraßenbahnwagen, der CarGoTram. Auf Grund der Neuausrichtung der Produktionsstätte wurde der Betrieb Anfang 2016 vorübergehend eingestellt, eine Wiederaufnahme ist für 2017 geplant.

Auch in Wien war eine Studie mit dem Projekttitel Güterbim in Auftrag, die die Straßenbahn als Gütertransportmittel innerhalb der Stadt untersuchte. Mangels Interesse potenzieller Kunden endete das Projekt ergebnislos. In Zürich wird unter dem Namen Cargotram-Zürich versucht die Entsorgung von Sperrmüll per Straßenbahn zu organisieren.

Gab es um 1980 nur noch etwa 300 Straßenbahnbetriebe weltweit, so sind es schon wieder rund 390. Laut UITP, des internationalen Verbands für öffentliches Verkehrswesen, sind gegenwärtig zudem etwa 100 Straßen- und Stadtbahnsysteme in Bau bzw. in Planung.
Nach Einschätzung der UITP wird sich die seit Anfang der 1980er Jahre veränderte Verkehrspolitik fortsetzen. Diese macht einerseits durch den stärkeren Umweltschutzgedanken das Verkehrsmittel Straßenbahn wieder populär und verhindert andererseits geplante U-Bahn-Bauvorhaben aufgrund nicht ausreichender finanzieller Mittel. Obwohl es in Zukunft sicher weiterhin zu einzelnen Stilllegungen kleiner oder unrentabler Betriebe kommen wird, ist also insgesamt mit einer weiteren Zunahme von Straßenbahnbetrieben zu rechnen.
Wachstumsmarkt für Straßenbahnen dürfte China werden: Bis zum Jahr 2020 sollen in 36 Städten insgesamt mehr als 5.000 Kilometer Straßenbahnstrecken gebaut werden. Umgerechnet 500 Milliarden Euro sind dafür notwendig.

Im Stadtverkehr ist der Einsatz von Straßenbahnen dort sinnvoll, wo die vorhandenen Fahrgastzahlen einen Betrieb mit Buslinien unwirtschaftlich werden lassen, wo für den Bau einer U-Bahn jedoch die Fahrgastzahlen auch perspektivisch zu niedrig sind. Für deutsche Verhältnisse gilt als Faustregel, dass eine Straßenbahnlinie ab etwa 4000 Fahrgästen pro Werktag wirtschaftlicher als eine Buslinie ist. Bei circa 30.000 bis 40.000 Fahrgästen je Tag ist die Grenze erreicht, bei der ein Schnellbahnbetrieb betriebswirtschaftlich sinnvoller ist. Hierbei ist zu beachten, dass eine reine Linienbetrachtung in den wenigsten Fällen ausreicht, da beim Systemwechsel in der Regel umfangreiche Linienanpassungen im betroffenen Netz vorgenommen werden.

Neben diesem quantitativen Einsatzkriterium gibt es weitere qualitative Einsatzmerkmale, die auch in Fällen, in denen diese genannten Fahrgastzahlen nicht erreicht werden, einen Straßenbahnbetrieb gegenüber einem Busbetrieb vorteilhaft werden lassen.

Eine Straßenbahn hat die für schienengebundenen Verkehrsmittel typische Laufruhe und damit einen hohen Fahrkomfort. Dies wird insbesondere beim Einsatz moderner und leiser Fahrzeuge erreicht. Bei Bussen kann vergleichsweise die Laufruhe stärker von unterschiedlichen Fahrbahnzuständen beeinflusst werden. Verkehrt die Straßenbahn an der Oberfläche, so müssen zu den Haltestellen im Allgemeinen keine Treppen überwunden werden. Hochbahnsteige oder Niederflurfahrzeuge erleichtern den Einstieg für Rollstuhlbenutzer, ältere Leute und Fahrgäste mit Kinderwagen, Fahrrädern oder Gepäck.

Mittlere Fahrgeschwindigkeiten städtischer Verkehrsmittel hängen immer vom Haltestellenabstand und den Haltezeiten und Interaktionen mit anderen Verkehrsteilnehmern ab. So erreicht die moderne Straßenbahn von Bordeaux als schnellste Straßenbahn Frankreichs eine Durchschnittsgeschwindigkeit (inklusive der Stationsaufenthalte) von 21 km/h.

Bei Straßenbahnen können im Stadtverkehr dann hohe Reise- und Fahrgeschwindigkeiten erreicht werden, wenn ein großer Teil der Strecke auf unabhängigem oder besonderem Bahnkörper geführt wird oder die Straßenbahn an Ampeln eine Vorrangschaltung besitzt und vor Störungen geschützt wird. Auf kurzen und mittellangen Strecken kann eine Straßenbahn sogar mit Schnellbahnen konkurrieren, da die Gesamtreisezeit des Fahrgastes durch kurze Zu- und Abgangswege an den meist ebenerdigen Haltestellen (keine Treppen) und dichtere Haltestellenabstände verkürzt wird.

Die Straßenbahn ist das einzige Schienenverkehrsmittel, das eine hohe Flächendeckung erreichen kann. Dadurch können viele Direktverbindungen angeboten werden, wodurch die Zahl der Umsteigevorgänge reduziert und damit die Reisezeit verkürzt wird.

Ein Vorteil gegenüber Bussen ist die Möglichkeit der Zugbildung. Die variable Beigabe von Bei- oder weiteren Triebwagen vergrößert die Kapazität, ohne dass der Personalbedarf erhöht wird, wie es etwa bei Einsatzbussen unvermeidlich ist. Die Straßenbahn kann somit Verkehrsspitzen im Berufsverkehr und Schwachlastzeiten mit jeweils angepasstem Fahrzeugeinsatz abdecken. Durch die in der Bau- und Betriebsordnung Straßenbahn festgesetzte maximale Zuglänge von 75 Metern können Straßenbahnzüge vergleichsweise viele Fahrgäste auf einmal befördern.

Besonders gegenüber dem Bus besitzt sie den Schienenbonus.

Da bei der Straßenbahn im Gegensatz zu Omnibussen die Energie nicht lokal produziert wird, sondern aus Oberleitungen bezogen wird, entstehen am Betriebsort keine Emissionen für die Energieherstellung. Davon profitieren besonders Anwohner von Straßenbahnstrecken und Städte, in denen aufgrund des dichten Kfz-Verkehrs die Umweltverschmutzung ohnehin sehr groß ist. Die lokalen Feinstaubemissionen sind bei der Straßenbahn hingegen nur unwesentlich tiefer als beim Omnibus, was auf den Abrieb von Rädern, Schienen, Stromabnehmern und Oberleitung zurückzuführen ist.

Die konkrete Umweltfreundlichkeit der Straßenbahn hängt maßgeblich vom verwendeten Strommix sowie den verwendeten Fahrzeugen, dem Beschaffungsintervall und der Auslastung der Fahrzeuge ab.

In Annahme der durchschnittlichen Flottenzusammensetzung sowie Auslastung der einzelnen Verkehrsträger ergeben sich in der Schweiz folgende Umweltbilanz pro Personenkilometer, wobei auch die Energieproduktion und die Herstellung der Fahrzeuge und Infrastruktur miteinbezogen wurden:
Laut einer neueren Studie der Bergischen Universität Wuppertal hat eine Straßen- oder auch Stadtbahnlinie einen positiven Einfluss auf die Entwicklung eines Stadtteiles. Dies bezieht sich sowohl auf die Immobilienpreise als auch den Einzelhandel. Umgekehrt lassen sich nach Verlagerung oder Stilllegung einer Linie gegenteilige Entwicklungen nachweisen.

Die folgenden Faktoren sind eher subjektiv und regional unterschiedlich. Durch die Führung der Straßenbahn an der Oberfläche hat sie einige Vorteile gegenüber U-Bahnen und Bussen. Die hohe Präsenz der Straßenbahn im öffentlichen Raum macht die Linienführung ohne weiteres erkenn- und nachvollziehbar, führt aber auch zu möglichen Beeinträchtigungen von Parkraum und Kfz-Verkehr und erhöht die Ansprüche an die architektonische Einbindung. Die Fahrgäste erleben den Stadtraum und das Stadtleben unmittelbar mit. Nicht nur Sonderfahrten (in historischen Wagen, in „Christkindl“- oder Partytrams), bereits die alltägliche Benutzung vermittelt Urbanität. Die Präsenz wie der ebenerdige Zugang an beleuchteten Haltestellen verbessert auch abends und nachts das subjektive Sicherheitsgefühl. Der Neubau von Straßenbahnen hat oft erhebliche Konflikte um die neue Infrastruktur zur Folge – nach der Durchsetzung weist die Tram gegenüber dem Bus den Schienenbonus wie auch eine höhere Identifikation klassischer Tramstädte und Tramanlieger mit diesem Verkehrsmittel auf.

Einer der größten Vorteile der Straßenbahn ist auch gleichzeitig ihr größter Nachteil. Da eine Straßenbahn an Schienen gebunden ist, wird sie im Straßenverkehr leicht behindert, z. B. durch im Gleisbereich parkende Fahrzeuge. Bei Baustellen kann eine Straßenbahn nur umgeleitet werden, wenn eine Parallelstrecke vorhanden ist. Ist dies nicht der Fall, bleibt nur die zeitweilige, ggf. teilweise Einstellung der die betroffene Strecke befahrenden Linien oder die mit zusätzlichen Kosten verbundene Verlegung von besonderen Gleisen für die Zeit der Bauarbeiten. Auch die Anpassung an veränderte Verkehrsströme ist meist nur mit vergleichsweise großem Aufwand möglich.

Die Preise für neues Rollmaterial sind in den letzten Jahren stark gestiegen, dies ist mitunter auf neue Technologien z. B. in der Fahrgastinformation zurückzuführen, die auch in anderen öffentlichen Verkehrsmitteln Einzug gehalten haben. Moderne Straßenbahnfahrzeuge kosten je nach Ausführung zwischen 1,5 Millionen und 4 Millionen Euro. Omnibusse sind zwar in der Anschaffung billiger, erreichen jedoch in der Regel lediglich eine Lebensdauer von acht bis zwölf Jahren, während Straßenbahnwagen mit entsprechenden Retrofit-Programmen 30 bis 50 Jahre eingesetzt werden können. Weiterhin bieten Straßenbahnen üblicherweise mehr Fahrgästen Platz als ein Omnibus oder Oberleitungsbus, wodurch weniger Fahrzeuge benötigt werden, um dieselbe Menge Passagiere zu transportieren. Bezogen auf die Rollmaterialkosten pro Personenkilometer schneidet die Straßenbahn daher bei gleicher Auslastung der Fahrzeuge nach wie vor günstiger ab.

Durch ihre Oberleitung und die Schienen können Straßenbahnen an (historisch) interessanten Orten als störend empfunden werden. Dieses Phänomen wird durch Kettenfahrleitungen und überdimensionierte Haltestellenmasten verstärkt. Es gibt allerdings auch die Möglichkeit, dank einer zusätzlichen Stromschiene zwischen den Fahrschienen auf die Oberleitung zu verzichten, wie es 2004 im Zentrum von Bordeaux realisiert wurde. Oder die Fahrzeuge werden mit Energiespeichern, etwa auf Kondensatorbasis, ausgerüstet, mit denen sie kurze Strecken ohne Fahrleitung befahren können. Ein Einsatz einer derartigen Technologie ist auf einer Strecke in Heidelberg geplant, um Störungen empfindlicher Gerätschaften in Universitätsgebäuden entlang der Strecke durch elektromagnetische Felder zu vermeiden. Die entsprechenden Variobahnen sind bereits ausgeliefert und befinden sich auf anderen Strecken im Einsatz.

Vor allem in Städten, die noch über alte Straßenbahntrassen verfügen, fehlt eine Schwingungsdämpfung beispielsweise durch eine Neopren-Lagerung der Gleise. Durch sich im Untergrund ausbreitenden Körperschall können Erschütterungsschäden an Gebäuden entstehen. Bei einer Gleisbettung nach dem Stand der Technik wird eine solche Gefährdung ausgeschlossen. Ein zusätzliches Problem kann Schienenverkehrslärm darstellen.

Im Vergleich zu S- und U-Bahnen sind Straßenbahnen in der reinen Fahrzeit deutlich langsamer, was zum einen an der dichteren Haltestellenfolge liegt, zusätzlich kommen oft Wartezeiten an Ampelkreuzungen dazu, die bei Schnellbahnen ohne niveaugleiche Kreuzungen entfallen. Muss sich die Straßenbahn die Strecke mit dem Individualverkehr teilen, so kann es hier zu weiteren Verzögerungen durch ein- und ausparkende Fahrzeuge, Stauungen usw. kommen. Diese Tatsache steht scheinbar im Widerspruch zu ihren Vorteilen hinsichtlich der Reisegeschwindigkeit, tatsächlich kann der objektive Nachteil längerer Fahrzeit (teilweise oder vollständig) kompensiert werden, und dadurch ist die Reisezeit mit der Straßenbahn durch den Fortfall von Zu- und Abgangswegen auf kurzen und mittleren Strecken geringer (z. T. erheblich geringer) als die von in gleichen Distanzen verkehrenden S- und U-Bahnen.

Straßenbahnenfahrzeuge stellen aufgrund ihrer vergleichsweise hohen Fahrzeugmasse eine höhere Gefahr für andere Verkehrsteilnehmer als Busse dar. Reibwertunabhängige Bremsen wie Magnetschienenbremsen kompensieren diese zwar weitgehend, doch ist die mögliche Bremsverzögerung mit Rücksicht auf stehende Fahrgäste begrenzt. Zudem ist es an Straßenbahnhaltestellen im Gegensatz zu Schnellbahnstationen in der Regel erlaubt, die Gleise zu überschreiten. Hier besteht insbesondere an großen Umsteigehaltestellen die Gefahr für die Gleise querende Fahrgäste, die Annäherung der leisen Wagen nicht rechtzeitig wahrzunehmen und von diesen ggf. erfasst zu werden. Diese Art der Gefährdung besteht bei Schnellbahnen nicht, da hier ein niveaugleiches Queren der Gleise durch Fahrgäste in der Regel nicht erlaubt und baulich meist auch nicht möglich ist.
Bei winterlichen Extremsituationen (Regenfälle nach vorheriger Frostperiode) kann es in seltenen Fällen durch Eisbildung an den Oberleitungen zu deren Bruch kommen. Unter Spannung stehende Fahrleitungsenden, die dann herunterhängen, können eine Gefahr für Passanten und andere Verkehrsteilnehmer darstellen.

Um das Be- und Überfahren des Gleiskörpers durch nicht schienengebundene Fahrzeuge zu unterbinden, kann dieser durch eine Schwelle von der Fahrbahn getrennt werden.

Im Straßenplanum verlegte Rillenschienen können für einspurige Fahrzeuge wie Motorräder und insbesondere Fahrräder eine Sturzgefahr darstellen, wenn diese längs bzw. in spitzem Winkel schräg überquert werden. Bei mehrspurigen Fahrzeugen besteht die Gefahr des plötzlichen Verlusts bzw. der starken Minderung der Bodenhaftung beim Wechsel von Asphalt zu Stahl. Dies kann besonders bei Regen zu Unfällen führen.

Als U-Straßenbahn, kurz "U-Strab", in Belgien auch "Prémetro", nur in Wien auch U-Straba, werden Straßenbahnen klassifiziert, die teilweise im Tunnel und dadurch unbehindert vom sonstigen Verkehr verkehren. In einigen Städten waren U-Straßenbahnen der Ursprung von Stadtbahnsystemen, die dort teilweise auch „U-Bahn“ genannt werden. Dazu zählen insbesondere Köln und Stuttgart. In Wien führt die U-Bahn-Linie U2 zwischen den Stationen Karlsplatz und Schottentor über die U-Straba-Strecke der ehemaligen Straßenbahnlinien E, G und H mit ihren Stationen Museumsquartier, Volkstheater und Rathaus. Auch die offiziell als Stadtbahn eingestuften Dortmunder Linien U43 und U44 sind de facto noch eher U-Straßenbahn-Linien. Die Trasse zwischen Nürnberg Eberhardshof und Fürth Jakobinenstraße wurde teilweise als Hochbahnstrecke für den dort nur zehnjährigen Straßenbahnbetrieb ausgeführt, wobei bereits beim Bau im Vorgriff auf die geplante U-Bahn-Verlängerung die Erfordernisse als reine U-Bahn-Strecke berücksichtigt wurden.

Im Bereich mancher Ballungszentren wie der Rhein-Neckar-Region und dem Rhein-Ruhrgebiet sind die Streckennetze mehrerer Straßenbahnbetriebe untereinander verbunden. Solche Verbindungsstrecken werden als Überlandstraßenbahnen bezeichnet. Beispiele sind die U79 der Rheinbahn zwischen Düsseldorf und Duisburg oder die U70/76 zwischen Düsseldorf und Krefeld und die Oberrheinische Eisenbahn im Dreieck Mannheim–Heidelberg–Weinheim, die allerdings außerhalb der Innenstädte von Heidelberg und Mannheim als Eisenbahn fährt. Die Rheinufer- und Vorgebirgsbahn zwischen Köln und Bonn wurden 1979 und 1986 zu (Regional-)Stadtbahnstrecken umgebaut, die jedoch weiterhin als Eisenbahnstrecken konzessioniert sind. Reine Umlandstrecken betreiben die Thüringerwaldbahn von Gotha bis Bad Tabarz (Linie 4 mit 21 km) und die Kirnitzschtalbahn ab Bad Schandau.

Viele Überlandstraßenbahnen werden nun als (Regional-)Stadtbahn betrieben. Der Übergang zur Eisenbahn ist oft fließend und meist nur von der Art der Konzession abhängig. Häufig findet auf den Überlandstrecken dieser Bahnen auch Güterverkehr statt, wobei die Güterwagen mitunter von Lokomotiven gezogen werden, während der Personenverkehr mit Straßenbahntriebfahrzeugen stattfindet (beispielsweise die Lossetalbahn von Kassel über Kaufungen und Helsa nach Hessisch Lichtenau). Wegen trotz gleicher Spurweite unterschiedlicher Maße für Radsätze und Gleis insbesondere bei der Rückflächenführung in Weichen und Kreuzungen mussten Kompromissradprofile entwickelt werden. 

Das amerikanische Gegenstück zu Überlandstraßenbahnen und Regionalstadtbahnen sind Interurbans.

In der Schweiz gibt es viele Schmalspurüberlandbahnen, so in Basel, Zürich, Bern, Aarau, die durchwegs mit den Überlandstraßenbahnen vergleichbar sind. Nur noch wenige verkehren wie Straßenbahnen auf derselben Fahrbahn wie der Straßenverkehr, die meisten wurden in den letzten Jahren aus Sicherheitsgründen auf den Betrieb mit einer eigenen Trasse umgestellt.

Eine Weiterentwicklung ist die Regionalstraßenbahn, kurz Regiotram, deren Vorreiter in Deutschland die Stadtbahn Karlsruhe ist. Dort führen dieselben Linien abschnittsweise als Straßenbahn auf den Fahrbahnen von Stadtstraßen und sogar durch die Fußgängerzone, als Stadtbahn auf separater Trasse durch Vororte, als Nebenbahn in Täler von Schwarzwald und Kraichgau und auf DB-Hauptstrecken in andere Städte der Region.

In Kassel wurde mit der RegioTram Kassel eine systemübergreifende Lösung zwischen Straßenbahn und S-Bahn geschaffen. Hier gibt es drei RT-Linien von verschiedenen Endpunkten im Kasseler Straßenbahnnetz über den Innenstadtring und den Tunnel am Haupt- und Kulturbahnhof in das Netz der DB. Ab dem Kasseler Haupt- und Kulturbahnhof verkehren zwei Linien in nordöstliche und eine Linie in südwestliche Richtung. Die RegioTram Kassel verbindet somit Umland und Region direkt mit der Kasseler Innenstadt, da die Mehrzahl der Straßenbahn- und RT-Linien durch die Haupteinkaufsstraße führen. Es werden Zweisystemfahrzeuge eingesetzt, damit ein umsteigefreier Betrieb zwischen beiden Netzen (600 V Gleichspannung bei der KVG und 15 kV Wechselspannung bei der DB) ermöglicht werden kann. Zehn Einheiten sind Zweikrafttriebwagen für Gleichstrom- und Dieselbetrieb. Ein vergleichbarer Betrieb entstand seit 2002 auch in Chemnitz.

"Schnellstraßenbahn" ist eine veraltete Bezeichnung, die früher insbesondere für Straßenbahnen auf eigenem Gleiskörper verwendet wurde. Schnellstraßenbahnen entstanden ab den 1920er Jahren insbesondere dort, wo Gemeinden im Umland großer Städte an deren Straßenbahnnetz anzubinden waren. Ein Beispiel für eine Schnellstraßenbahnstrecke ist die Linie zwischen Radebeul und Weinböhla, die 1930/1931 von der DRÜVEG als Teil einer geplanten Trasse von Dresden nach Meißen gebaut wurde. Ein weiterer Anwendungsfall war die Schnellstraßenbahn zwischen Nürnberg und Fürth, sie bestand von 1927 bis 1939 und war mit einer roten Liniennummer signalisiert. Ebenfalls gehört die 1976 in Dortmund eröffnete Neubaustrecke zwischen Kirchderne, Franz-Zimmer-Siedlung und Grevel dazu. Sie wird von der Linie U42 der Stadtbahn Dortmund bedient und wurde ursprünglich ebenfalls als Schnellstraßenbahn bezeichnet.

Die Düsseldorfer Stadtbahnlinien U76 (Düsseldorf–Krefeld) und U79 (Düsseldorf–Duisburg) wurden vor Inbetriebnahme des Düsseldorfer Innenstadttunnels als Schnellstraßenbahnlinien 76 und 79 bezeichnet. Sie hielten im Gegensatz zu den Straßenbahnlinien 710 und 711 nicht an allen Stationen. Abgesehen vom Innenstadttunnel und dem Duisburger Tunnel sind die meisten Streckenabschnitte schnellstraßenbahnmäßig ausgebaut.

Noch immer als Schnellstraßenbahn werden zwei Außenstrecken der Straßenbahn Wien bezeichnet. Hierbei handelt es sich um die Abschnitte Mauer–Rodaun der Linie 60 und Rothneusiedl–Therme Oberlaa der Linie 67. Die Strecke Alaudagasse – Oberlaa-Therme Wien wurde allerdings im März 2014 stillgelegt, da hier die Trasse der in Bau befindlichen U1-Verlängerung entlangführen soll. Auch die ehemalige Schnellstraßenbahn-Linie 64 nach Siebenhirten wurde 1995 auf U-Bahn-Betrieb umgestellt.

Mit der Schnellstraßenbahn vergleichbare Konzepte sind die MetroTram oder die auch als "Sneltram" bezeichnete Premetro Antwerpen.

Grundsätzlich halten Straßenbahnen, ähnlich den Personenzügen bei der Eisenbahn, an allen Stationen. Jedoch gibt es vereinzelt Bestrebungen, den Fahrgästen auf bestimmten Relationen und zu bestimmten Zeiten beschleunigte Verbindungen anzubieten, auch "Expresslinie", Eilzug oder Eilkurs genannt. Diese bedienen nicht alle Zwischenhaltestellen und ermöglichen dadurch kürzere Reisezeiten:




Teilweise werden spurgeführte Oberleitungsbusse aus Marketinggründen als Straßenbahn auf Gummireifen bezeichnet. Die französische Originalbezeichnung hierfür lautet "tramway sur pneumatiques", in der Schweiz wird von einem Pneu-Tram gesprochen. Wichtigste Vertreter sind die Systeme "Transport sur Voie Réservée" ("TVR") von Bombardier und Translohr von Lohr Industrie. Die Definition dieser Verkehrsmittel ist jedoch umstritten, die Übergänge zwischen einem klassischen Straßenfahrzeug und einer Bahn sind fließend. Teilweise variiert die juristische Einordnung, zum Beispiel bezüglich der Pflicht, ein Kraftfahrzeugkennzeichen anzubringen, von Staat zu Staat.

In der Praxis stößt der dauerhafte Betrieb solcher alternativer Systeme oft auf erhebliche technische Schwierigkeiten. Daher neigen bestehende oder potenzielle Betreiber inzwischen oft wieder zur Entscheidung zur herkömmlichen Straßen-/Stadtbahn. Gegenüber dem weltweiten Straßenbahnsektor spielt der Marktanteil spurgeführter omnibusähnlicher Systeme keine Rolle.



Eine Besonderheit ergibt sich für die Stadtbahn Jerusalem: Sie liegt vollständig auf dem Stadtgebiet von Jerusalem und ihr derzeitiger Betrieb ist, anders als bei den anderen grenzüberschreitenden Linien, als "grenzüberschreitend" in dem Sinne nicht erkennbar. Gleichwohl führt die Strecke auf der Demarkationslinie zwischen Israel und Palästina von 1967 entlang vom Damaskustor bis "Shim'on Ha-Tsadik" und weiter nördlich schließlich in den Teil Jerusalems, der von der palästinensischen Autonomiebehörde als Staatsgebiet beansprucht wird. Aus der Sicht des israelischen Jerusalemgesetzes wurden die von ihr erschlossenen Stadtgebiete wie French Hill und Pisgat Ze’ev zum untrennbaren Teil eines israelischen Jerusalem erklärt, was wiederum völkerrechtlich als nichtig betrachtet wird (siehe Resolution 478 des UN-Sicherheitsrates).










</doc>
<doc id="11906" url="https://de.wikipedia.org/wiki?curid=11906" title="Daemon">
Daemon

Als Daemon [] oder Dämon (auch häufig in der Schreibweise Demon) bezeichnet man unter Unix oder unixartigen Systemen ein Programm, das im Hintergrund abläuft und bestimmte Dienste zur Verfügung stellt. Benutzerinteraktionen finden hierbei nur auf indirektem Weg statt, zum Beispiel über Signale, Pipes und vor allem (Netzwerk-)Sockets. Der Begriff Daemon wird auch als Abkürzung von "disk and execution monitor" interpretiert, was jedoch ein Backronym ist.

Bei Microsoft Windows heißen die entsprechenden Programme "services" oder "Systemdienste".

Normalerweise werden Daemons nicht durch Benutzerinteraktion gestartet, sondern automatisch beim Wechsel in ein anderes Runlevel bzw. beim Systemstart. Dadurch stellen Daemons einen wesentlichen Anteil des Bootprozesses dar, da bei den meisten Unix-Derivaten die wesentliche Anwendungslogik des Betriebssystems im Userspace und damit in den Daemons abläuft. Typische Daemon-Programme sind bei Unix-Betriebssystemen daher nicht nur Server-Prozesse wie Netzwerkdienste, E-Mail-Server, Datenbankserver und Druckerserver, sondern auch Prozesse, die die Hardwarekonfiguration bzw. -Überwachung vornehmen, wie Sounddaemons oder Wechselmedienverwaltungsdaemons. Auch periodische Aufgaben oder zu festgelegten Zeiten anfallende Aufgaben werden mithilfe von Daemons realisiert.

Daemons können jedoch auch wie normale Prozesse in einer Shell durch einen Benutzer gestartet werden. Anschließend "forken" diese Prozesse und erstellen auf diese Weise einen Prozess, der mit der aufrufenden Shell nicht mehr verbunden ist und damit ein direkter Kindprozess des Hauptprozesses "init" wird. In der Praxis werden viele Daemons so konstruiert, dass sie per Kommandozeilenparameter wahlweise sowohl im Hintergrund ("detached") laufen können als auch im Vordergrund, d. h. im Kontext der Shell, verbleiben können. Oft werden Logausgaben dann auf der Standardausgabe ausgegeben.

Um ihren Daemon-Charakter zu bezeichnen, hat der Name vieler solcher Programme ein angehängtes „d“, zum Beispiel "syslogd oder "cupsd.

In Anspielung auf einen Dämonen haben sich die BSD-Unix-Derivate einen solchen zum Logo gemacht. Die Figur enthält einige Metaphorik, die Daemons charakterisiert. Sie wurde als Logo für diese Unix-Derivate gewählt, weil Daemons eine zentrale Betriebssystemkomponente darstellen.

Auch die BSD-Derivate FreeBSD und NetBSD hatten den Daemon ursprünglich übernommen, mittlerweile sind alle Derivate jedoch auf ein alternatives oder abgewandeltes Logo umgestiegen.




</doc>
<doc id="11907" url="https://de.wikipedia.org/wiki?curid=11907" title="Dämon">
Dämon

Als Dämon (Plural "Dämonen", von "daimon") wird in verschiedenen Mythologien, Religionen und mystischen Lehren zunächst ein „Geist“ oder eine Schicksalsmacht ( "daimónion") als „warnende oder mahnende Stimme (des Gewissens)“ und „Verhängnis“ verstanden. Unter christlichem Einfluss wandelte sich die Bedeutung dann bis zu „Teufel“, „Satan“, „Luzifer“. Heute wird mit „Dämon“ entgegen dem neutralen bis eher positiven Sinn des Ursprungswortes für die gemeinten Geisteserscheinungen oder Geisteswesen ausschließlich ein solches „Wesen“ bezeichnet, das nach allgemeiner Vorstellung Menschen erschreckt, bedroht oder ihnen Schaden zufügt, also in jeder Hinsicht als böser Geist erscheint. Die systematische Erfassung der Dämonen bezeichnet man als Dämonologie.

Im archäologischen Sprachgebrauch bedeutet "Dämon" ein tierköpfiges Mischwesen (Chimäre) mit mindestens menschengestaltigen Beinen. Den Gegensatz dazu bilden „Monster“, wie Mischwesen mit Tierkörpern und Tierköpfen phantastischer Art bezeichnet werden (z. B. Greif oder Drachen) oder Tierkörper mit menschlichen Köpfen wie Sphinx (Frauenkopf und Löwinnenkörper), Mantikor (Männerkopf mit Löwenkörper und Skorpionschwanz) und Zentauren (Menschenoberkörper und Pferdeleib).

Das Wort stammt vom griechischen Wort (Daimon) ab. Daimon stand in der griechischen Mythologie ursprünglich für den "Geist der Abgeschiedenen" oder umgekehrt "den abgeschiedenen Geist der Verstorbenen" (griech. oder "skiá" für ‚Schatten‘), ‚die körperlosen Gestalten der Abgeschiedenen‘. Es scheint insoweit eine positive Bedeutung im Vordergrund gestanden zu haben, solange die Sage von Bedeutung war, in der die Seelen der Menschen des goldenen Zeitalters ("daimones") genannt wurden, die dabei „eine Mittelstufe zwischen Göttern und Menschen, eine zweite Klasse niederer Götter“ darstellten. In ganz ähnlicher Bedeutung existiert im Deutschen das nur selten gebrauchte Wort "Schemen" (vgl. auch „schemenhaft“) für Geist(er), Gespenst(er) und „Spukgestalten“. Zugrunde liegt diesen und anderen Wortbildungen (wie scheinen, schimmern, schier und Schimmel) nach dem "Herkunftwörterbuch" des "Großen Duden" das idg. Wurzelwort *"skai-" in der Bedeutung von (stumpf) glänzen, Glanz, Abglanz.

Das Herkunftwörterbuch des "Großen Dudens" gibt die Bedeutung von "Dämon" „böser Geist“, als "ein Mittelwesen zwischen Gott und Mensch" an und führt es auf griech. für „(ver)teilen“, „zer-“ und „zuteilen“ sowie „geteilt werden“ zurück. Von daher wird dort als Grundbedeutung von "Dämon" die Angabe ‚Ver- und Zuteiler (des Schicksals)‘ hergeleitet. Interessant sind die weiteren Beziehungen von : einerseits zu dem griech. Wort für Volk "dämos" – wie in "Demokratie" –, andererseits und noch weiter ausholend zu „Zeit“ (time, tide/Tide[(n)hub]/Gezeiten, siehe auch Zeile, Ziel, Zeitung) i. S. v. Abschnitt, Abgeteiltes: bei alledem handelt es sich sprachhistorisch oder etymologisch um Ableitungen aus dem idg. Wurzelwort *"da[i]"- für teilen, zerreißen, zerschneiden.

Das Wort wiederum steht in Verbindung mit dem griechischen Wort ("daimonion") in der Bedeutung des Schicksals oder Gewissens, das den Menschen jederzeit unsichtbar begleitet. Es gibt die Einschätzung, dass erst im Lauf des Mittelalters der Begriff "Dämon" mit unangenehmen Vorstellungen verbunden worden sei und damit eine Verschiebung ins Negative erhalten habe. "Pandämonium" bezeichnet die Gesamtheit aller Dämonen oder ihren Aufenthaltsort, der im übertragenen Sinn ein Ort des Grauens ist.

In nahezu allen ethnischen Religionen kommen Dämonen vor (in der Fachliteratur auch als Polydämonismus bezeichnet). Hier stehen sie für den Glauben an eine Vielzahl unberechenbarer, launischer und unheimlicher Mächte im Sinne personifizierter Naturgewalten oder Krankheiten. Im Zusammenhang mit dem Glauben der sogenannten „Naturvölker“ werden die Bezeichnungen „Dämonen“ und „Geister“ häufig synonym benutzt. So handelt es sich ebenso um schutzgewährende Totem- oder Hausgeister, verbündete Tiergeister oder harmlose Naturgeister. Bei den Dämonen steht der Aspekt der konkreten Gestalt gegenüber dem Spirituellen im Vordergrund. Die negative Konnotation ist vor allem eine Folge der christlichen Mission, die die Dämonen mit dem Teufel gleichsetzte. Vor der Einwirkung des Christentums waren es verschiedene unkörperliche, selbstbewusste Wesen mit teilweise übermenschlichen Kräften und Fähigkeiten, die die gesamte Natur bevölkerten. Sie galten entweder als immateriell, jedoch zumeist als gebunden an Objekte oder Lebewesen und standen in der religiösen Hierarchie unter den Göttern. Abgesehen von den Geistern verstorbener Menschen werden Geister und Dämonen als unverwandt mit den Menschen, jedoch mit menschlichen Eigenschaften beschrieben. Die Vielfalt der mythologischen Vorstellungen macht es häufig schwierig, Dämonen und Geister klar von der Beseeltheit der Naturerscheinungen (Animismus) zu trennen.

Die ägyptische Mythologie postulierte eine ungeheure Zahl Dämonen auf der Erde, in der Luft und im Wasser. In der sumerischen und später auch der babylonischen Mythologie wurden neben den himmlischen Geistern im Sterndienst auch solche verehrt, deren Aufenthalt und Wirksamkeit an bestimmte Gegenden gebunden war, und noch tiefer unten die verfinsterten Geister, die auf und in der Erde und in ihrer Atmosphäre wohnten, wie die Feuer-, Licht-, Feldgeister etc.

Systematisiert wurde die Dämonenlehre in der persischen Mythologie, in welcher dem Ahura Mazda außer den sieben Amschaspands viele gute Genien und dem Ahriman außer den sieben Daevas noch zahllose böse Geister untergeordnet sind.

Die griechische Mythologie ging zunächst bei sämtlichen Naturphänomenen von Dämonen aus (übernatürliche Wesen), z. B. Blätterrauschen im Wind, Zikadenzirpen usw., später nur noch bei ungeklärten Vorgängen oder Prozessen, z. B. Gärung von Milch oder Alkohol, Verdunstung, Verwesung, Verwitterung, Alterung, etc. Im alten Orient galten Dämonen als Teil der Weltordnung, indem ihnen der Ursprung von Krankheit und auch Tod zugesprochen wurde. Der griechische Epenschreiber Hesiod (etwa 700 v. Chr.) beschreibt in seinem Hauptwerk Theogonie den Glauben an ganze Scharen und verschiedene Klassen von Dämonen als Zwischenwesen zwischen den Göttern und den Menschen. Sie umschweben den Menschen als quasi unsichtbare Wächter über Recht und Unrecht und spenden auch Reichtum. Außerdem wirken sie in der irdischen Sphäre als Natur- und Elementargeister, entweder als Wohltäter oder als Verderber. Eine größere Rolle spielt die Dämonologie auch in der neuplatonischen Philosophie (ab etwa 300 n. Chr.), welche den ganzen Polytheismus der Griechen in Gestalt des Glaubens an Dämonen, die als Untergötter der Natur und allen Lebensbeziehungen vorstehen und als „weltschöpferische Mittelwesen“ zwischen den hilfsbedürftigen Menschen und der Gottheit vermitteln sollten, in ihr System mit aufnahm.

Die Philosophen haben diesen Glauben mit vielen einzelnen Beziehungen auf das Natur- und das menschliche Seelenleben (auch mit Übertragung auf die geheimnisvolle Geisterwelt der Verstorbenen) immer weiter ausgebildet. Besonders in zwei Richtungen trat derselbe hervor:

Etwas anders entwickelte sich der Dämonenglaube in der römischen Mythologie, in der griechische Ideen sowie orientalische Vorstellungen (durch Vermittlung über die etruskische Mythologie) aufgenommen und weitergebildet wurden. Hier erscheinen die Dämonen als so genannte Genien. Diese Vorstellungen wurden dann vom frühchristlichen Dämonenglauben übernommen.

In der indischen Mythologie haben die Dämonen (Asuras) einen festen Platz. Im Hinduismus sind sie die Gegenspieler der Götter (Devas). Nach alter indischer Vorstellung waren die Dämonen einstmals Götter. Als die Asuras geschaffen wurden, gab man ihnen als Gabe die Wahrheit und die Lüge mit, wobei sie später die Wahrheit ablegten und dämonisiert wurden. Es gibt verschiedene Dämonengruppen, die Daityas, die Danavas oder die Rakshasas, die oft als Tiere, z. B. Geier, Hunde oder Tiger, oder als besonders hässliche Menschen dargestellt werden.

Die iranische Dämonologie, welche insbesondere die persische Dämonenlehre einschließt, macht für jedes Unglück, Krankheit, Laster oder Übel einen bestimmten Dämon verantwortlich. Hierbei werden Dämonen als Daevas, Divs bzw. Druj (abgeleitet von Avestisch "druj" bzw. "drug" entsprechend Altpersisch "drauga-" ‚Lüge‘, ‚Trug‘; Pahlavi: "druz") bezeichnet. Sie betrügen den Menschen, indem sie das Böse als gut deklarieren. Ahriman lenkt dieses Dämonenheer und ist mit dem Satan der biblisch-christlichen Dämonologie vergleichbar. Im Volksglauben werden die bösen Feen (Paris bzw. Pairikas) und besonders die Yatus gefürchtet, die sich durch Zauberei verwandeln können, um die Menschen zu täuschen und zu hintergehen.

Der Zoroastrismus weist einen stark differenzierten Dämonenglauben auf, u. a. in Form der Dämonin "Drug" (Lüge), der Leichenhexe "Nasu" oder der Zornesgestalt "Aesma Daeva" Asmodeus. Von bedeutendem Einfluss auf die jüdische Dämonologie war deren Berührung mit den zoroastrisch-iranischen Dämonenlehren. Während noch Plato Dämonen als Mittlergestalten zwischen Göttern und Menschen begreift, verstärkt sich mit dem antiken Judentum und Christentum die rein negative Einschätzung der Dämonen.

Shedim ist der hebräische Begriff für Dämonen. Er wird besser mit ‚Halbgötter‘ oder ‚Geister‘ übersetzt. Der Begriff ist ein Lehnwort aus dem Akkadischen ("schedu") und bezeichnet dort eine freundliche, gute, beschützende Macht (Geist). Er erscheint nur zweimal im Tanach (stets im Plural), jedes Mal für falsche Götter, Götzen, „Nicht-Götter“ im Kontext verbotener Kinds- und Tieropfer. Aus geht hervor, dass ihnen im heidnischen, nichtjüdischen Kult Menschenopfer dargebracht wurden. Im zweiten Lied des Mose segnet Mose sein Volk Israel und eröffnet ihm eine Zukunftsvision. Dann wird ihm geboten, auf den Berg Nebo zu steigen und sich auf seinen Tod vorzubereiten. In beklagt Mose Avoda sara (עבודה זרה hrb. ‚fremder Dienst‘), das heißt Götzendienst, unter den Kindern Israels. Das Lied führt die Klage, dass die Bundestreue des Volkes Israel zu Gott im Kontakt mit den heidnischen Völkern Kanaans verblasst war und dass heidnischer Kult Geister anbetet, denen die notwendige Macht fehlt und die deshalb des Götzendienstes durch Israel unwürdig sind. Im Tanach spielen Dämonen sonst eine ganz untergeordnete Rolle; sie werden selten als Anklänge alter Zeiten und heidnischer Bräuche erwähnt; neben den Schedim noch Asasel und Nephilim.

Als Dibbuk können „irrende Seelen“, die keine Ruhe finden, nach dem Tode in Lebende fahren. Es handelt sich hierbei um jüdische Mystik und Aggada, jüdische Erzählung, Sage, die Maimonides in diesem Zusammenhang vermied, wie auch auf die Rede von Himmel, Hölle und die ins Unendliche reichende körperliche Auferstehung. Er betont die Unmöglichkeit menschlichen Denkvermögens, über sich hinauszugehen und Unsägliches zu sagen, und das abstrakte Weiterleben der Persönlichkeit.

Die Qliphoth der kabbalistischen Kosmologie sind metaphorisch als verhüllende Schalen um „Funken göttlicher Lichtemanation“ aufgefasst. Göttlichkeit im Judentum bedeutet die Offenbarung der heiligen ein-einzigen Wirklichkeit Gottes. Qliphoth verhüllen diese jedoch, wie Schalen die enthaltene Frucht umhüllen. Qliphoth sind daher synonym mit Götzendienst (Idolatrie), Unreinheit, bösen geistigen Kräften, Quellen von spiritueller, religiöser Unreinheit.
Zunächst lebten zu Beginn des Christentums bei einigen Menschen auch die alten heidnischen Götter als Dämonen noch lange fort. In der Frühgeschichte des Christentums wurden im Gnostizismus, Marcionismus, Manichäismus und anderen häretischen Bewegungen dualistische Vorstellungen vertreten, die den Menschen in der Knechtschaft eines bösen, geringeren Gottes, dem Erschaffer der Materie, sahen und man glaubte, Erlösung werde nur durch die Intervention eines gnädigen Gottes zuteil. Alles Leid, die Unordnung und das Böse wurden dabei als Teufel oder Antichrist personifiziert, als deren Stellvertreter man gemeinhin oft die Juden ansah, woraus im mittelalterlichen Christentum eine machtvolle Dämonologie geschaffen wurde.

Die Grundlagen der christlichen Dämonologie wurden von Augustinus entwickelt, der, beeinflusst vom dualistischen Manichäismus, die Lehre von den zwei Reichen begründete, nämlich die civitas Dei (Gottesreich) und die civitas Diaboli (Dämonenreich). Nach der Lehre des Augustinus seien die Dämonen gefallene Engel. Er zweifelte nicht an deren Realität und ihrem wirksamen Eingreifen in den Lauf der Dinge, allerdings nur, soweit Gott es zuließ. Allerdings gab es schon sehr früh einzelne Bischöfe und Synodalbeschlüsse, die den mit Hilfe der Dämonen ausgeführten wahrsagerischen Handlungen jeglichen Wirklichkeitsgehalt absprachen. Im Jahr 820 veröffentlichte der Erzbischof von Lyon Agobard (um 769-840) seinen "Liber contra insulsam vulgi opinionem de grandine et tonitruis", wo er den Glauben an die Möglichkeiten der Wettermacher ("tempestarii" oder "immissores tempestatum") als illusorisch verwarf. Das Konzil von Tours (813) lehrte, dass magische Künste und Beschwörungen weder kranken Menschen noch Tieren helfen könnten. Es handele sich vielmehr um Täuschungen der Dämonen. Dieser Text wurde dann auch in das "Decretum Gratiani" übernommen, wobei allerdings Augustinus fälschlich als Verfasser genannt wurde. Die das "Decretum Gratiani" kommentierenden Dekretisten nahmen teilweise an, dass mit Hilfe von Dämonen die Erzeugung einer Impotenz, "impotentia ex maleficio", möglich sei. Gratian ging davon aus, dass die Dämonen hauptsächlich bei der Wahrsagerei aktiv seien. In diesen Zusammenhang übernahm er einen langen Text zu diesem Thema aus dem Buch "De divinatione daemonum" des Augustinus.

Dieser Text des Augustinus im "Decretum Gratiani" befasst sich auch mit der Natur der Dämonen. Sie hätten einen feinstofflichen, luftigen Körper, der ihnen eine schärfere Sinneswahrnehmung als die des Menschen ermögliche. Auf Grund ihrer dauerhaften Existenz hätten sie auch eine bei weitem größere Lebenserfahrung. Auf Grund dieser beiden Eigenschaften könnten sie Zukünftiges voraussagen. Dazu deuteten sie wie ein Arzt natürliche Zeichen (später wurde auch die Wettervorhersage des Landwirts herangezogen), insbesondere den Menschen unbekannte Luftschichten, die sie im Hinblick auf die Zukunft interpretierten. Auch könnten sie für den Menschen nicht erkennbare Zeichen der inneren Gemütsverfassung wahrnehmen und nutzen. Sie könnten auch Krankheiten in ihren luftigen Körper aufnehmen und die Menschen mit dieser krankmachenden Luft infizieren. Sie hätten mit imaginären Erscheinungen auch Zugang zur Gedankenwelt der Menschen. Die Dekretisten folgten im Wesentlichen diesen Vorstellungen. In der kommentierenden anonymen "Summa Tractaturus Magister Gratianus de iure canonico" wird noch hinzugefügt, dass die Dämonen nach ihrem Sündenfall – anders als die Engel mit ihren feinstofflichen Körpern – auch eines der niederen Elemente in sich aufgenommen hätten, so dass sie im Gegensatz zu Engeln auch leiden könnten. Die französischen Glossenapparate "Ecce vicit leo" und "Animal est substantia" vertreten, dass Dämonen nur dann einen Körper annehmen könnten, wenn ihnen Gott das erlaube. Auch sie als körperlose Wesen könnten leiden, allerdings anders als körperliche Wesen.

In ihren Kommentierungen setzten sie sich insbesondere mit der Konkurrenz der Vorhersage der Dämonen mit der Allwissenheit Gottes einerseits und der Willensfreiheit des Menschen andererseits auseinander. Danach sind die Dämonen auf Interpretationen ihrer Beobachtungen angewiesen. Aber die Möglichkeit, die Gedanken der Menschen unmittelbar lesen zu können, wurde abgelehnt. Dies sei allein Gott vorbehalten.

Die Glaubensgemeinschaft der Christadelphians lehnt sowohl die Vorstellung eines übernatürlichen Satans als auch den Glauben an die Existenz von Dämonen als böse Wesen und Geister als unbiblisch ab und lehrt stattdessen, mit dem Begriff „Dämonen“ würden in der Bibel körperliche, häufig aber auch geistige und seelische Krankheiten bezeichnet.

Die islamische Dämonenwelt ist hierarchisch aufgebaut und teilt die Geisterwesen in drei Kategorien ein: Neben den Engeln werden Teufel (Iblis bzw. Schaitan) und die Dschinn genannt, die zu den guten Dämonen zählen, solange sie sich für Allah entscheiden, andernfalls hält man sie für böse. Die Dschinn, Schaitan und Iblis stammen noch aus der vorislamischen Zeit. Der Islam sieht die Dschinnen als, aus dem Feuer geschaffene Wesen an, die neben Engeln, Teufeln und Menschen eine eigene Bedeutung haben. Sie werden der Sphäre der Dschinn zugeschrieben, die sich hierarchisch zwischen der irdenen Sphäre (Sphäre der Menschen) und der himmlischen Sphäre (Sphäre der Engel) befindet. Die Dschinn können mit Menschen verkehren; böse Dschinnen gelten ebenfalls als Teufel ("shayatin"). Salomo machte sich die Dschinn lt. Koran 34, 12–14 als Erster dienstbar. Häufig erscheinen sie als kriechendes Getier, schwarze Hunde und Windhosen können aber auch jede andere Form annehmen. Dschinnen werden auch positiv bewertet.

Die altchinesische Dämonologie des Daoismus kennt zwei Hauptgruppen von Dämonen die ihrem Wesen nach den beiden kosmischen Kräften Yin (das Weibliche, Böse, Dunkle und die Erde) und Yang (das Männliche, Gute, Helle und der Himmel) entsprechen. Mit „Shen“ (gute Dämonen) wurden ursprünglich als himmlisch kategorisierte Geister bezeichnet. Die Kuei (böse Dämonen) umfaßten hauptsächlich die Seelen der Toten, die, wie beispielsweise Ertrunkene, kein offizielles Begräbnis erhalten hatten oder denen von ihnen Nahestehenden keine Ahnenopfer dargebracht wurden.






</doc>
<doc id="11909" url="https://de.wikipedia.org/wiki?curid=11909" title="Saxophon">
Saxophon

Das Saxophon (nach reformierter Schreibung auch Saxofon) ist ein Musikinstrument aus Messing. Das Rohr ist konisch, es weitet sich also zum Ende hin. Damit unterscheiden sich alle Bauweisen des Saxophons etwa von der zylindrischen Klarinette. Das Saxophon gehört der Definition nach, anders als sein metallischer Korpus vermuten lässt, zur Familie der Holzblasinstrumente, da sein Ton mit Hilfe eines Rohrblatts erzeugt wird.

Das Instrument wurde als „Saxophon“ von dem Belgier Adolphe Sax (eigentlich Antoine Joseph Sax) im Jahr 1840 erfunden und am 21. März 1846 unter der Nummer 3226 in Frankreich patentiert. Im Patentantrag begründet Sax seine Erfindung mit dem Fehlen gut klingender Holzblasinstrumente der tiefen Lage und wollte mit der Erfindung des Saxophons ein Holzblasinstrument kreieren, das klanglich zwischen dem „wärmend-biegsamen“ Klang der Klarinette und dem eher durchdringenden, näselnden Sound der Oboe liegt.

Das erste von Adolphe Sax gebaute Saxophon war ein Bassinstrument in C. Er konzipierte das neue Instrument jedoch von vornherein für die Stimmlagen von Sopran bis Subkontrabass, abwechselnd im Quart-/Quintabstand. Dabei sah er die C/F-Stimmung für den Gebrauch im Sinfonieorchester vor, während die B/Es-Stimmung für die Militärmusik gedacht war. Die C- und F-Instrumente werden heute kaum noch hergestellt und hatten nie die Bedeutung, die sie nach ihrem Erfinder hätten haben sollen. Einzig das C-Melody-Saxophon wurde bis in die 1950er-Jahre gebaut und im Jazz, vor allem im Swing, viel verwendet.

1929 übernahm Henri Selmer die Pariser Werkstatt von Adolphe Sax und wurde zum offiziellen Inhaber von dessen Patentrechten. 2010 feierte Henri Selmer Paris das 125-jährige Firmenjubiläum. Saxophone von Henri Selmer Paris, besonders das Mark VI, haben mittlerweile Kultstatus und gehören zu den handwerklich herausragenden Produkten.

Erst längere Zeit nach seiner Erfindung begann schließlich mit dem Aufkommen des Jazz in New Orleans der eigentliche Siegeszug dieses Instruments, mit seinem sehr variablen Klang und großen dynamischen Umfang. Alfred Baresel nannte es 1929 „das wichtigste Melodie-Instrument des Jazz“.

Seiner Historie folgend ist das Saxophon also nicht mit dem Jazz entstanden bzw. wurde gar nicht hierfür entworfen, sondern war eigentlich für die klassische „ernsthafte“ Konzertmusik vorgesehen, in der es sich allerdings bis heute nie (zumindest nicht ansatzweise wie im Jazz) behaupten konnte.

Entgegen der landläufigen Meinung ist das Saxophon dennoch bei weitem kein Instrument, das nur im Jazz zu Gebrauch kommt. Tatsächlich werden Saxophone seit dem 20. Jahrhundert in unzähligen Musikgenres verwendet, einschließlich Pop, Rock ’n’ Roll, elektronischer Musik, aber auch in serieller Musik sowie zahlreichen weiteren Genres. Auch bei Konzert- und Tanzmusik ist es eines der beliebtesten Soloinstrumente, und viele bekannte Saxophon-Solisten haben ihre eigenen Bands oder Combos gegründet.

Im Deutschen Reich wurde das Saxophon zur Zeit des Nationalsozialismus und teils bereits zuvor als Instrument der "Entarteten Musik" bzw. "Negermusik" bekämpft. So forderte die Deutsche Tonkünstler-Zeitung bereits 1929 ein Verbot des Instruments, das mit dem sukzessiven Verbot der Jazzmusik, dem Haupteinsatzgebiet des Saxophons, ab 1933 großteils entstand. Jedoch wandten sich dadurch von der Existenz bedrohte Hersteller an das Reichswirtschaftsministerium, das auf Anfrage vom Reichsministerium für Volksaufklärung und Propaganda die Auskunft erhielt, dass „das Saxofon an der Negermusik völlig unschuldig sei“, da die „Erfindung des Adolf Sax […] hauptsächlich in der Militärmusik gebraucht [werde]. Wie mit allen anderen Instrumenten könne man auch mit dem Saxofon gute Musik machen.“ Die "Deutsche Kultur-Wacht" schrieb 1933: „Wenn es richtig gespielt wird, ohne die bisher üblichen Mätzchen (Glissando usw.), erweist es sich als ein wertvolles Hilfsmittel der Tanzmusik“. So kam das Saxophon sowohl in der Tanz-, als auch in der Militärmusik weiter zum Einsatz. Die Musikkorps der Deutschen Luftwaffe enthielten ab 1940 einen fünf Instrumente umfassenden Saxophon-Satz. Hans Hinkel erneuerte die Meinung des Propagandaministeriums 1942 erneut, indem er feststellte, dass das Saxophon nur „fälschlich als Negerinstrument“ bezeichnet wird.

Viele NS-Ideologen (z. B. Herbert Gerigk, Alfred Rosenberg) sahen das Saxophon wegen seines Klangs und seiner Verwendung in der "Negermusik" dennoch weiterhin nicht als zur „guten Musik“ fähig.

Der Ton entsteht beim Saxophon durch ein einzelnes schwingendes Rohrblatt (wie z. B. auch bei der Klarinette). Deswegen zählt das Saxophon zu den Holzblasinstrumenten und nicht, wie sich aufgrund des Korpusmaterials vermuten ließe, zu den Blechblasinstrumenten.

Das Saxophon ist in drei Einzelteile zerlegbar:

Das Sopranino- und Soprillo- sowie manche geraden Sopransaxophone bestehen nur noch aus Korpus und Mundstück.

Beim Spielen wird das Instrument meist an einem Tragriemen befestigt, den sich der Spieler um den Hals hängt. Für größere Instrumente (ab Baritonsaxophon und größer) sowie für kleinere Saxophonisten gibt es auch Rücken- oder Schultergurte. Diese setzen sich wegen des höheren Tragekomforts in jüngerer Zeit auch vermehrt bei Tenorsaxophonisten durch, schränken aber deren Beweglichkeit und Spieldynamik stark ein. Sopransaxophon und Sopraninosaxophon spielt man oft ohne Trageriemen. Basssaxophone und größere werden meistens im Ständer und sitzend gespielt, da sie für einen bloßen Gurt zu schwer sind.

Anders als die Klarinette überbläst das Saxophon nicht in die Duodezime, sondern (wie die Querflöte und Oboe) in die Oktave. Dies wird durch den konischen Verlauf der Schallröhre (eng am Mundstück und sehr weit am Schallbecher) verursacht. Grifftechnisch ist das Instrument weitgehend mit der B-Klarinette und teilweise auch mit der modernen Querflöte verwandt.

Am häufigsten in Gebrauch sind Alt- und Tenorsaxophon, gefolgt von Sopran- und Baritonsaxophonen und anschließend dem Basssaxophon, während die Varianten in den extremen Lagen seltener – solistisch oder zur Bereicherung der Klangfarben in größeren Ensembles – eingesetzt werden. Das Altsaxophon ist nicht zuletzt deshalb am beliebtesten, weil es von Größe, Preis und Gewicht her auch als Anfängerinstrument für Kinder und Jugendliche geeignet ist. Hinzu kommt, dass die hohen Töne keinen allzu starken Ansatz und die tiefen nicht zu viel Luft erfordern.

Seit Anfang dieses Jahrtausends gibt es ein noch kleineres Saxophon als das Sopranino: Das Soprillo in B-Stimmung in gerader Bauform mit Applikatur bis zum hohen e, bei normalem Fingersatz. Eine Besonderheit ist die obere Oktavklappe, die im Mundstück eingebaut ist. Die Schallröhre des Soprillos hat eine Länge von 30 cm.

Etwa seit der gleichen Zeit gibt es auch sehr gut spielbare Kontrabass- und Subkontrabass-Saxophone. Ihre Bauform weicht etwas von der klassischen Form ab: Das Hauptrohr ist nochmals gebogen. Dadurch werden diese Instrumente kompakter. In modernisierter Bauform, "Tubax" genannt, werden Kontrabass- und Subkontrabasssaxophon in Deutschland von Benedikt Eppelsheim hergestellt. Beide haben kleinere Mundstücke (Bariton- bzw. Basssaxophonmundstück), was die Ansprache verbessert, und eine engere Mensur, wodurch weniger Luft verbraucht wird und längere Töne gespielt werden können.

Die Beweglichkeit und Vielseitigkeit des Saxophons führte zu zahlreichen Experimenten. Das "Conn-O-Sax" (um 1928) wurde in einer geraden Form mit einem bauchigen Schallstück gebaut. Die gerade Form des Saxophons verleiht dem Instrument einen oboenartigen Klang. Beim "Conn-O-Sax" wird durch das kugelartige Schallstück der nasale Klang eines Doppelrohrblattinstruments (Oboe, Englischhorn oder Fagott) erreicht. Die F-Stimmung dieses Instruments gleicht der des Englischhorns, liegt also einen Ganzton höher als die des Altsaxophons in Es und entspricht damit der ursprünglichen Intention von Sax, ein sinfonieorchestertaugliches Instrument zu bauen. Obwohl seine Vorteile gegenüber dem Englischhorn überwiegen, hat sich das Instrument nicht durchgesetzt.

Das "Saxello" ist ein Bb-Sopransaxophon mit zurückgebogenem Kopfende und nach vorne gebogener Spitze. Es wurde ab etwa 1924 von der King H. N. White Company gefertigt ("King Saxello"), seine Produktion wurde aber in den 1930er Jahren während der Großen Depression eingestellt. Heute jedoch werden Saxellos wieder produziert. Bekannt wurden sie u. a. durch den Jazzmusiker Rahsaan Roland Kirk, der auch auf dem ähnlichen "Manzello" spielte. Er spielte auch ein "Stritch" genanntes umgebautes Altsaxophon von Buescher.

Eine andere Entwicklung ging dahin, Saxophone aus Holz zu bauen, um einen weicheren Klang zu erzielen. Diese Formen sind allerdings instrumentenbaulich schwer herzustellen und weisen starke Intonationsprobleme auf. Das Holzsaxophon ist der Klarinette und dem ungarischen "Tárogató" sehr ähnlich.
In den 1950er Jahren baute die Firma "Grafton" eine kleine Serie von Saxophonen aus Kunststoff, die durch Ornette Coleman und Charlie Parker bekannt wurden. Das Konzept des Kunststoff-Saxophons wurde mit Erscheinen des ersten Vibratosax der thailändischen Firma "Vibrato" seit 2010 wiederbelebt.

Auch in Bezug auf die Technik zur Abdeckung der Tonlöcher wurden verschiedentlich neue Ansätze verfolgt. Der Regelfall ist auch heute noch die Verwendung hohler Metallklappen, die innen mit einem mit Leder überzogenen Filzpolster ausgefüllt sind. Das Material dieser Polster wurde im Zuge der technischen Entwicklung auch durch moderne Kunststoffmaterialien (Codera, TopTone oder Jim Schmidt (USA)) ersetzt, die gepolsterte Klappe ist jedoch bis heute Standard. Zur Verbesserung des Resonanzverhaltens werden bei den größeren Klappen verschiedene Auflagen aus Metall oder Kunststoff (sogenannte Resonatoren) verwendet, die auf das Polster aufgenietet, im Fall der Buescher-Snap-In-Polster auch eingeschraubt sein können. Heute wie damals werden in 99 % aller Saxophone lederbezogene Polster verbaut.

Eine Ausnahme stellte ein in der Zeit von 1938 bis 1941 vorübergehend von Selmer USA produziertes Saxophon dar: In Zusammenarbeit mit dem damals renommierten US-amerikanischen Hersteller Buescher wurde ein Saxophon ohne Klappenpolster („a padless saxophone“) entwickelt, bei dem die Dichtung der Tonlöcher durch einen Lederring am Tonloch selbst und die Verwendung absolut plangeschliffener Messingklappen erzielt wurde. Obwohl Resonanzverhalten und Ansprache des Modells als sehr gut beschrieben wurden, waren die dünnen Dichtungsringe auf die Dauer zu anfällig, sodass das System bald wieder vom Markt verschwand.

Eine besondere Form der Klappenpolster findet sich beim zuvor erwähnten Vibratosax Kunststoff-Saxophon: die Polster bestehen gänzlich aus flexiblem Silikon und sind direkt am Hebel, an einem, verhältnismäßig kleinen, im Durchmesser nur wenige Millimeter messenden Punkt, rundum beweglich aufgehängt. Die Klappen sind hierdurch vollständig selbstnivellierend, was dafür sorgt, dass die Tonlöcher stets bestmöglich geschlossen werden, und den Wartungsaufwand, verglichen mit traditionellen Tonlochabdeckungen, entsprechend merklich reduziert.

Eher wegen des Namens und seines Klangs, als von den charakteristischen Instrumenten-Eigenschaften her, kann noch das Taschensaxophon als Form des Saxophons angesehen werden, das aber eher ein Mittelding zwischen Klarinette und Flöte darstellt.

Das Mundstück wird so in den Mund genommen, dass die oberen Schneidezähne vorn auf der schrägen Fläche liegen. Beim "klassischen (geschlossenen) Ansatz" wird, wie bei der Klarinette, die Unterlippe leicht über die unteren Zähne nach innen gezogen und gegen das Blatt gedrückt. Im Gegensatz dazu wird beim "modernen (offenen) Ansatz" die Unterlippe nach außen gewölbt. Dabei dürfen die Zähne das Blatt nicht berühren. Mit diesem Ansatz wird eine härtere Klangfarbe erzielt. Damit das Blatt leichter "anspricht", wird es vor dem Spielen von beiden Seiten befeuchtet.

Die linke Hand bedient die oberen Klappen des Saxophons. Der Daumen dieser Hand ruht auf der dafür vorgesehenen Daumenplatte und hält so den oberen Teil des Korpus. Bei Bedarf drückt er, um das Überblasen zu erleichtern, die direkt darüber befindliche Oktavklappe. Die rechte Hand bedient die unteren Klappen. Mit dem rechten Daumen hält der Spieler das Instrument in der Mitte, indem er es am Daumengriff leicht von sich drückt. Für Zeige-, Mittel- und Ringfinger der rechten und linken Hand hat er je eine Klappe. Der Zeigefinger der oberen Hand bedient außerdem die kleine B-Klappe und die Flageolet-Klappe. Die kleinen Finger der rechten und linken Hand bedienen je zwei Klappen, die zur Erleichterung des Klappenwechsels mit Rollen verbunden sind.

Fingersatztabellen existieren sowohl für den normalen Ton- als auch für den Altissimo-Bereich.

Die ersten Saxophonsätze der frühen Swingorchester Ende der 1920er Jahre bestanden aus zwei Altsaxophonen und einem Tenorsaxophon (z. B. Fletcher Henderson). Mit der Größe der Orchester nahmen auch die Saxophonsätze zu, zunächst auf vier Musiker (zwei Altsaxophone, zwei Tenorsaxophone) und dann auf fünf als Standardbesetzung der Bigbands der 1940er Jahre mit zwei Altsaxophonen, zwei Tenorsaxophonen und einem Baritonsaxophon. Diese Formation gilt seitdem als übliche Bigband-Besetzung, obwohl es immer auch abweichende Zusammensetzungen mit einem besonderen Sound gegeben hat. Beispiele dafür sind die „Four-Brothers-Besetzung“ von Woody Herman mit drei Tenor- und einem Baritonsaxophon oder der von einer Klarinette angeführte Saxophonsatz von Glenn Miller. In den 1950er-Jahren wurde der Saxophonsound von Billy Vaughn populär, bei dem die (zweistimmige) Melodieführung durch zwei Altsaxophone in der hohen Lage erfolgte. Üblicherweise sind die Saxophone in einem Saxophonsatz parallel mehrstimmig gesetzt, wobei dem ersten Altsaxophon die Melodieführung obliegt, während das Baritonsaxophon die Basslinien ausfüllt. Selten taucht auch einmal ein Bass-Saxophon in einer Orchesterbesetzung auf, allerdings in der Regel nicht als Mitglied des Saxophonsatzes, sondern als Ersatz für die Tuba oder den Kontrabass.

Sofern Saxophone in Musik-Combos eingesetzt werden, handelt es sich häufig um ein Tenorsaxophon (typisch für die Besetzung einer Rock-’n’-Roll-Combo) oder auch um ein Altsaxophon (hin und wieder in der Rockmusik). Sofern ein dreistimmiger Bläsersatz in einer Pop- oder Rockmusik-Combo Verwendung findet, besteht dieser meistens aus einem Saxophon (Alt oder Tenor) zusammen mit einer Trompete und einer Posaune. In größeren (Blas-)Musikkapellen der eher volkstümlichen Art kommen Saxophone (Alt und Tenor) oft in den Nebenstimmen (zusammen mit Tenorhörnern) vor.

Die längste Tradition hat das Saxophon in den Blasorchestern. Dort hat das Saxophon eine wichtige klangliche Bedeutung. Es verbindet das Holz-Register Holzblasinstrumente mit dem Blech-Register Blechblasinstrumente. Einerseits hat das Saxophon die Beweglichkeit eines Holzblasinstrumentes, andererseits steht es der Lautstärke eines Blechblasinstrumentes kaum nach. 1844 prophezeite Berlioz dem Saxophon durch seine Eigenschaften eine große Zukunft.

In der klassischen Musik wird das Saxophon vor allem als Soloinstrument, in Saxophonformationen (vor allem dem Saxophonquartett) und in Kammermusikbesetzungen verwendet. Im Sinfonieorchester findet man es seltener. Obwohl erste Bauarten des Saxophons bereits im 19. Jahrhundert entstanden, wurde es in der klassischen Kunstmusik erst ab Beginn des 20. Jahrhunderts vermehrt eingesetzt. Als eines der wohl bekanntesten, seltenen Beispiele aus dem (späteren) 19. Jahrhundert gilt Georges Bizets "L'Arlésienne". Bekannte Beispiele aus dem 20. Jahrhundert sind u. a. George Gershwins "Rhapsody in Blue" und "ein Amerikaner in Paris", Maurice Ravels "Boléro", Alban Bergs "Violinkonzert" und "Lulu" oder die "Sinfonia domestica" von Richard Strauss. Das Instrument wird hierbei aber in der Regel von einem der Klarinettisten als Nebeninstrument verlangt. Eines der frühesten Saxophonkonzerte ist Alexander Konstantinowitsch Glasunows "Altsaxophonkonzert in Es-Dur Opus 109". Des Weiteren hatte u. a. der DDR-Sinfoniker Max Butting eine Vorliebe für den Klang des Instrumentes und verwendete es in den meisten seiner Orchesterwerke. In jüngerer Zeit setzten u. a. Luciano Berio, Pierre Boulez oder Péter Eötvös das Saxophon im Orchester ein; mittlerweile ist es in der zeitgenössischen Orchesterliteratur durchaus kein seltener Gast mehr.

Besonders in der Neuen Musik wird das Saxophon als Soloinstrument verwendet. Die Komponisten betonen in ihren Kompositionen dabei besonders die avantgardistischen Klangmöglichkeiten und die technischen Aspekte des Saxophons: Multiphonics, Flageoletttechniken, Klappengeräusche, Glissando, Tremolo und viele weitere.

Einige der Komponisten, die Werke für Saxophon solo geschrieben haben, sind: Eckart Beinke, Paul Bonneau, Pierre-Max Dubois, Anders Eliasson, Jean Françaix, Alexander Konstantinowitsch Glasunow (der allerdings ins 19. Jahrhundert gehört), Jacques Ibert, Paul Hindemith, Nicolaus A.Huber, Christian Lauba, Fabien Lévy, Jean-Marie Londeix, Ryō Noda, Guido Rennert, Bertold Hummel, Fuminori Tanada, Pierre-Max Dubois, Jean Rivier, Sigfrid Karg-Elert oder Patrice Sciortino.

Dem US-amerikanischen Altsaxophonisten John-Edward Kelly ist eine Reihe von Werken für sein Instrument und Orchester gewidmet. So schrieben Anders Eliasson seine fünfsätzige "Sinfonia concertante: Symphonie Nr. 3 für Alt-Saxophon und Orchester" (1989; 2010 entstand eine Fassung für Sopransaxophon), Pehr Henrik Nordgren "Phantasme" (1992) und Jan Sandström "My Assam Dragon" (1996) für Kelly. Ebenfalls für Kelly entstanden das "Konzert für Streicher und Altsaxophon" (2003, Fassung für Sopransaxophon 2009) von Anders Eliasson und die "Kammersinfonie Nr. 3 für 20 Streicher und Altsaxophon" (1996/97) von Kalevi Aho, der für das Raschèr Saxophone Quartet ein "Konzert für Saxophon-Quartett, Glocken und Streicher" (mit dem Titel "Kellot" / "Glocken") schrieb.

Auf Wunsch des schwedischen Saxophonisten Anders Paulsson schrieb Anders Eliasson 2009 und 2010 Fassungen seiner "Sinfonia concertante: 3. Symphonie für Alt-Saxophon und Orchester" und des "Konzerts für Streicher und Altsaxophon" für das Sopransaxophon. Ebenfalls für das Sopransaxophon schrieb Friedrich Cerha. Sein "Konzert für Sopransaxophon und Orchester" wurde 2004 uraufgeführt.

Seit neuestem gibt es sogar ein Konzert für Baritonsaxophon und Orchester von Georg Friedrich Haas, das am 3. Mai 2008 vom WDR-Sinfonieorchester Köln unter Leitung von Emilio Pomarico uraufgeführt wurde (Livesendung im Radio). Solist war Marcus Weiss.

Für den Saxophonisten Dieter Kraus schrieb Timo Jouko Herrmann 2009 ein auf das Gedicht "Morphine" von Heinrich Heine anspielendes Konzertstück mit großem Orchester, in dem der Solist beständig zwischen Sopran- und Altsaxophon wechselt.

In der Kammermusikbesetzung wird das Saxophon in Kombination mit anderen Soloinstrumenten verwendet. Einige der Kombinationen, die in Kompositionen Verwendung finden, kombinieren das Saxophon unter anderem mit Gesang, Geige, Flöte, Klarinette, Oboe, Fagott, Trompete, Posaune oder auch Schlagzeug. Besonders häufig wird das Saxophon als Hauptinstrument mit Begleitung verwendet. Verbreitet sind hier insbesondere die Kombination von Saxophon und Klavier, aber es existieren auch Stücke in Begleitung von Orgel, Akkordeon, Harfe, Kontrabass und Gitarre.

Die Formationen, in denen das Saxophon in der klassischen Musik verwendet wird, sind an die Formationen der Streichinstrumente angelehnt. Insbesondere wird das Saxophon im Saxophonquartett und in größeren Saxophonensembles gespielt. Die Standardbesetzung des Saxophonquartetts ist Sopran, Alt, Tenor und Bariton. Wichtige klassische Saxophonquartette sind u. a. das Raschèr Saxophone Quartet, das Aurelia Saxophone Quartet, das Sonic.art Saxophonquartett oder das Pindakaas Saxophon Quartett. Beim Saxophonquintett findet man am häufigsten die Besetzung Sopran, Alt 1, Alt 2, Tenor und Bariton. Die Besetzung der größeren Saxophonensembles variiert je nach Ensembleleiter und Komposition. Die Standardzusammensetzung des Saxophonensembles, die Jean-Marie Londeix eingeführt hat, besteht aus einem Sopranino-, zwei Sopran-, drei Alt-, drei Tenor-, zwei Bariton- und einem Bass-Saxophon. Neben der kammermusikalischen Londeix-Besetzung mit zwölf selbständig geführten Stimmen trifft man auf Saxophonorchester-Formationen, welche (ähnlich wie beim Streichorchester oder Posaunenchor) die Stimmen mehrfach besetzen (saxophone choir), dafür aber von weniger Stimmen ausgehen, z. B. einem fünfstimmigen Satz S A A T B. Andere, insbesondere kleinere Saxophonensembles, wie zum Beispiel das Rova Saxophone Quartet, verändern je nach Komposition die Zusammenstellung der Instrumente. Auch hier werden die sechs gängigen Saxophonarten verwendet.

Seit der Swing-Ära ist der Jazz „saxophonisiert“. Es gab und gibt kaum ein Jazzensemble, in dem das Saxophon nicht zumindest eine wichtige Solistenrolle ausfüllen würde. Am häufigsten findet man Quartettbesetzungen, also Rhythmusgruppe plus Saxophon. Davon abgeleitet muss man die Triobesetzungen sehen, bei denen das Harmonieinstrument, also Gitarre oder Piano, weggelassen wird.

Reine Saxophonbesetzungen sind seltener, nehmen aber nicht nur in Grenzgebieten des Jazz aufgrund der fast unbegrenzten Klangmöglichkeiten eine interessante Rolle ein, wie etwa das World Saxophone Quartet oder die Kölner Saxophon Mafia als zwei sehr langlebige Ensembles belegen. Wichtig sind Saxophone auch in einer Big Band, hier sind sie meist wie folgt besetzt: erstes und zweites Altsaxophon, erstes und zweites Tenorsaxophon sowie ein Baritonsaxophon.

Um im Jazz eine individuelle Klangfarbe erzielen zu können, ist neben dem eigentlichen Instrument die Wahl des Equipments, also der Zubehörausstattung, entscheidend. Nicht nur das Material des Mundstückes (meist Metall oder Ebonit) und dessen Form, sondern auch die Bauart der Ligatur beeinflussen die Klangfarbe. Hierbei ist es nicht selten, dass Mundstücke nachträglich in der Form angepasst („refaced“) werden.

Ein „röhrendes“ Saxophon spielte eine tragende Rolle im klassischen Rock ’n’ Roll, namentlich im Twist. Besonderer Beliebtheit erfreute es sich in der schwarzen amerikanischen Musik, dem Soul und dem Blues. Auch im jamaikanischen Ska, ähnlich wie im Soul und der Dancehall-Musik der späten fünfziger und frühen sechziger Jahre, durfte es in den Bläsersektionen nicht fehlen. Seltener wurde es hier auch als Soloinstrument verwendet. In der Rockmusik, insbesondere im New Wave und im wiederbelebten Two-Tone-Ska der 1980er Jahre, war das Saxophon als Soloinstrument sowie als Sektions-Instrument ebenfalls besonders populär und ein regelrechtes Modeinstrument.





</doc>
<doc id="11910" url="https://de.wikipedia.org/wiki?curid=11910" title="Welle (Begriffsklärung)">
Welle (Begriffsklärung)

Welle (von althochdeutsch ‚wellan‘, ‚wälzen‘) steht für:

geographische Objekte:
Welle ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="11911" url="https://de.wikipedia.org/wiki?curid=11911" title="Reformationstag">
Reformationstag

Der Reformationstag, das Reformationsfest oder der "Gedenktag der Reformation" wird von evangelischen Christen in Deutschland und Österreich am 31. Oktober im Gedenken an die Reformation der Kirche durch Martin Luther gefeiert. In der Schweiz gilt der erste Sonntag im November als Reformationssonntag, steht also, trotz des reformierten Bekenntnisses, ebenfalls in Beziehung zur Reformation in Wittenberg.

Laut der Überlieferung soll der Mönch und Theologieprofessor Martin Luther am Abend vor Allerheiligen 1517 an die Tür der Schlosskirche zu Wittenberg 95 Thesen in lateinischer Sprache zu Ablass und Buße angeschlagen haben, um eine akademische Disputation herbeizuführen. Damit leitete er die Reformation der Kirche ein. Im Kern bestritt er die herrschende Ansicht, dass eine Erlösung von der Sünde durch einen Ablass in Form einer Geldzahlung möglich sei. Dies sei schon durch das Opfer Jesu Christi am Kreuz geschehen.

Luther hatte seine Thesen in Briefform mehreren geistlichen Würdenträgern und Bischöfen des Reiches zugesandt. Als die Bischöfe nicht reagierten, soll er die 95 Thesen an die Schlosskirche Wittenbergs angeschlagen haben. Ob der Thesenanschlag tatsächlich stattgefunden hat, ist jedoch nicht zweifelsfrei erwiesen. Der Kirchenhistoriker Thomas Kaufmann hält Luthers Schrift "An den christlichen Adel deutscher Nation" (1520) ohnehin für wesentlich wichtiger als die Thesen.

Bereits im Reformationsjahrhundert finden sich vereinzelte Jahresfeiern. Zunächst wurden auch der 10. November und der 18. Februar (Luthers Geburts- und Todestag) als Gedenktage gefeiert. Zudem galt der 25. Juni als Tag der Augsburger Konfession als Festtag.

Zur Säkularfeier 1617 wurde in den meisten lutherischen und reformierten Gebieten des Thesenanschlags gedacht. Kurfürst Johann Georg II. von Sachsen setzte ab 1667 den 31. Oktober als Gedächtnistermin für alle Protestanten einheitlich fest und stellte damit die Verbindung zum legendären Thesenanschlag Luthers an der Wittenberger Schlosskirche her.

Nach den Jubiläen 1717 und 1817 setzte sich das Reformationsfest weiter durch, „meist wurde es allerdings auf den Sonntag nach dem 31. Oktober gelegt“, z. B. in Preußen.

In Deutschland kamen zu den regionalen, historisch gewachsenen Unterschieden noch die großen Umbrüche – einige Schlaglichter:

Bis zum 21. Oktober 1921 war in Thüringen der Reformationstag bereits ein staatlich anerkannter allgemeiner Feiertag, wurde jedoch durch ein Notgesetz des thüringischen Staatsministeriums außer Kraft gesetzt, ohne den Landtag zu befragen.

Bis 1966 wurde der Feiertag in den meisten Bezirken der DDR begangen.

Der Gottesdienst zum Reformationstag findet dort, wo der Tag nicht gesetzlicher Feiertag ist, gewöhnlich am Abend des 31. Oktober statt. Sein Thema ist weniger das Gedächtnis des Thesenanschlags als die Lehre von der Rechtfertigung des Sünders allein durch den Glauben, die für Luther Auslöser und Kern der Reformationsbewegung war. Die Epistellesung des Tages ist daher , Evangelium .

Das Lied des Tages ist entweder "Nun freut euch, lieben Christen g’mein", Luthers „Erzähllied“ von seiner reformatorischen Entdeckung (EG 341), oder "Ist Gott für mich, so trete gleich alles wider mich" von Paul Gerhardt (EG 351). Kaum ein Reformationsgottesdienst endet außerdem ohne Luthers "Ein feste Burg ist unser Gott" (EG 362).

Die liturgische Farbe ist Rot, die Farbe des Heiligen Geistes und der Kirche.

Der Reformationstag am 31. Oktober ist seit der deutschen Wiedervereinigung gesetzlicher Feiertag in den deutschen neuen Ländern Brandenburg, Mecklenburg-Vorpommern, Sachsen, Sachsen-Anhalt und Thüringen, sowie (seit 2018) in Schleswig-Holstein und Hamburg. In Baden-Württemberg ist der Reformationstag schulfrei, liegt allerdings häufig ohnehin in den Herbstferien. In Niedersachsen haben evangelische Schüler auf Antrag die Möglichkeit, für die Dauer eines Gottesdienstes vom Unterricht freigestellt zu werden. In anderen Ländern mit einem größeren evangelischen Bevölkerungsanteil hat der Tag oft eine besondere gesetzliche Stellung, die mit der der Stillen Tage vergleichbar ist. Der Tag fällt gewöhnlich in die Kalenderwoche 44 der üblichen Zählung nach DIN ISO 8601; nur in Nichtschaltjahren, die mit einem Freitag beginnen (Sonntagsbuchstabe C), fällt er auf den Sonntag der 43. Woche.

Im Jahr 2017, dem 500. Jahr des Beginns der Reformation, war der 31. Oktober einmalig ein gesamtdeutscher gesetzlicher Feiertag. Dazu haben alle Bundesländer, in denen der Reformationstag normalerweise kein Feiertag ist, Gesetze bzw. Verordnungen erlassen, die den 31. Oktober 2017 zum Feiertag erklären:
Baden-Württemberg,
Bayern,
Berlin,
Bremen,
Hamburg,
Hessen,
Niedersachsen,
Nordrhein-Westfalen,
Rheinland-Pfalz,
Saarland
und Schleswig-Holstein.

Am 2. Februar 2018 empfahlen die Ministerpräsidenten von Schleswig-Holstein, Bremen, Hamburg und Niedersachsen auf einer Sondersitzung der Konferenz Norddeutschland (KND) in Berlin den Reformationstag in den von ihnen regierten Bundesländern als gesetzlichen Feiertag einzuführen. Dies hängt von der Zustimmung der jeweiligen Landesparlamente ab.

Am 22. Februar 2018 beschloss der schleswig-holsteinische Landtag, den Reformationstag als gesetzlichen Feiertag einzuführen. Am 28. Februar 2018 folgte auch die Hamburger Bürgerschaft und stimmte für den Reformationstag als neuen Feiertag in Hamburg.

In der Schweiz feiern die reformierten Kirchen am ersten Sonntag im November (also am ersten Sonntag nach dem 31. Oktober) den "Reformationssonntag". Der Gottesdienst ist gewöhnlich mit der Feier des Abendmahls verbunden (an gewöhnlichen Sonntagen findet üblicherweise ein Predigtgottesdienst statt).

Im überwiegend katholischen Österreich ist der Tag kein gesetzlicher Feiertag. Allerdings haben evangelische Schüler am 31. Oktober die Möglichkeit, sich vom Schulunterricht freistellen zu lassen, und evangelischen Arbeitnehmern ist ein Besuch des Gottesdienstes möglich, sofern dies mit den Erfordernissen des Betriebes vereinbar ist.

Weiterhin ist der Reformationstag in Slowenien und Chile ein gesetzlicher Feiertag.





</doc>
<doc id="11914" url="https://de.wikipedia.org/wiki?curid=11914" title="Grundrechenart">
Grundrechenart

Die Grundrechenarten (auch Grundrechnungsarten) sind die vier mathematischen Operationen Addition, Subtraktion, Multiplikation und Division.
Die Beherrschung der Grundrechenarten gehört zu den Grundfertigkeiten Lesen, Schreiben und Rechnen, die von Schülern während der Schulzeit zu erwerben sind.

Von den vier Grundrechenarten werden in der Arithmetik die Addition und die Multiplikation als Grundoperationen und die Subtraktion und die Division als abgeleitete Operationen angesehen. Für die beiden Grundoperationen gelten eine Reihe von Rechenregeln, wie die Kommutativgesetze, die Assoziativgesetze und die Distributivgesetze. In der Algebra werden diese Konzepte dann abstrahiert, um sie auf andere mathematische Objekte übertragen zu können.

Die Addition ist der Vorgang des Zusammenzählens zweier (oder mehrerer) Zahlen. Der Operator für die Addition ist das Pluszeichen +, die Operanden werden Summanden genannt und das Ergebnis heißt Summe:

Das Ergebnis der Addition natürlicher Zahlen ist wieder eine natürliche Zahl. Durch Auswendiglernen und elementare Rechentechniken können kleine Zahlen im Kopf addiert werden. Die Addition großer Zahlen kann per Hand mit Hilfe der schriftlichen Addition durchgeführt werden.

Die Subtraktion ist der Vorgang des Abziehens einer Zahl von einer anderen Zahl. Der Operator für die Subtraktion ist das Minuszeichen −, die beiden Operanden werden Minuend und Subtrahend genannt und das Ergebnis heißt Differenz:

Das Ergebnis der Subtraktion zweier natürlicher Zahlen ist jedoch nur dann wieder eine natürliche Zahl, wenn der Minuend größer als der Subtrahend ist. Sind Minuend und Subtrahend gleich, erhält man als Ergebnis die Zahl Null, die oft auch zu den natürlichen Zahlen gezählt wird. Ist der Subtrahend größer als der Minuend, erhält man als Ergebnis eine negative Zahl. Um die Subtraktion uneingeschränkt durchführen zu können, wird daher der Zahlbereich auf die ganzen Zahlen erweitert. Die Subtraktion großer Zahlen kann per Hand mit Hilfe der schriftlichen Subtraktion durchgeführt werden.

Die Multiplikation ist der Vorgang des Malnehmens zweier (oder mehrerer) Zahlen. Der Operator für die Multiplikation ist das Malzeichen · (oder ×), die Operanden werden Faktoren genannt und das Ergebnis heißt Produkt:

Sind die Faktoren natürliche oder ganze Zahlen, so ist das Ergebnis der Multiplikation ebenfalls wieder eine natürliche oder ganze Zahl. Durch Auswendiglernen des Einmaleins können kleine Zahlen im Kopf multipliziert werden. Die Multiplikation großer Zahlen kann per Hand mit Hilfe der schriftlichen Multiplikation durchgeführt werden.

Die Division ist der Vorgang des Teilens einer Zahl durch eine andere Zahl. Der Operator für die Division ist das Geteiltzeichen : (oder /), die beiden Operanden werden Dividend und Divisor genannt und das Ergebnis heißt Quotient:

Das Ergebnis einer Division zweier natürlicher oder ganzer Zahlen ist jedoch nur dann wieder eine natürliche oder ganze Zahl, wenn der Dividend ein Vielfaches des Divisors ist. Andernfalls erhält man eine Bruchzahl. Um die Division uneingeschränkt durchführen zu können, wird daher der Zahlbereich auf die rationalen Zahlen erweitert. Die Division durch null kann jedoch nicht sinnvoll definiert werden. Die Division großer Zahlen kann per Hand mit Hilfe der schriftlichen Division durchgeführt werden.

Die Grundrechenarten werden während der ersten Schuljahre im Mathematikunterricht behandelt. In der Grundschule (Primarstufe) wird zunächst das Rechnen mit kleinen natürlichen Zahlen gelehrt und später auf größere Zahlen erweitert. Unterrichtsinhalte sind auch das kleine Einmaleins, die Division mit Rest, das Lösen einfacher Gleichungen und der Dreisatz. Es werden Kopfrechnen, schriftliches Rechnen, Überschlagsrechnen und Anwendungen in Form von Textaufgaben eingeübt. Für vorteilhaftes Rechnen werden einfache Rechengesetze angewendet. In den ersten Jahren einer weiterführenden Schule (Sekundarstufe I) werden dann auch negative Zahlen betrachtet, die Bruchrechnung und damit die rationalen Zahlen eingeführt, sowie die Gesetze bei der Verbindung der vier Grundrechenarten behandelt.

Im Folgenden sind formula_1, formula_2 und formula_3 Zahlen aus dem zugrundeliegenden Zahlbereich. Für die Addition und die Multiplikation gelten die Kommutativgesetze

das heißt das Ergebnis einer Summe oder eines Produkts ist unabhängig von der Reihenfolge der Summanden bzw. Faktoren. Weiter gelten die Assoziativgesetze

Bei der Addition oder der Multiplikation mehrerer Zahlen ist es also unerheblich, in welcher Reihenfolge die Teilsummen oder Teilprodukte gebildet werden. Daher können bei Summen und Produkten die Klammern auch weggelassen werden. Zudem gelten die Distributivgesetze

mit denen durch Ausmultiplizieren ein Produkt in eine Summe umgewandelt werden kann und umgekehrt durch Ausklammern eine Summe in ein Produkt. Weiterhin verhält sich die Zahl formula_10 neutral bezüglich der Addition und die Zahl formula_11 neutral bezüglich der Multiplikation, das heißt

Für die Subtraktion und die Division gelten diese Gesetze nicht oder nur eingeschränkt. Weitere Rechenregeln, wie Punkt vor Strich, die Klammerregeln und die Gesetze der Bruchrechnung, finden sich in der Formelsammlung Arithmetik.

In der Arithmetik betrachtet man Addition und Multiplikation als Grundoperationen. Dabei wird die Addition natürlicher Zahlen als wiederholte Ermittlung des Nachfolgers eines Summanden und die Multiplikation natürlicher Zahlen als wiederholte Addition eines Faktors mit sich selbst angesehen. Diese Sichtweise wird dann auf andere Zahlbereiche, wie ganze oder rationale Zahlen, übertragen.

Subtraktion und Division führt man als abgeleitete mathematische Operationen der Grundoperationen ein. Zur Subtraktion und Division gelangt man über die Frage nach der Lösung elementarer Gleichungen der Form

wobei formula_1 und formula_2 gegebene Zahlen aus dem zugrundeliegenden Zahlbereich sind und die Zahl formula_18 gesucht ist. Um diese Gleichungen zu lösen, wird eine Umkehroperation zur Addition benötigt, nämlich die Subtraktion, und ebenso eine Umkehroperation der Multiplikation, nämlich die Division:

Die Subtraktion einer Zahl formula_1 wird nun als Addition mit der Gegenzahl formula_22 definiert und die Division durch eine Zahl formula_1 als Multiplikation mit dem Kehrwert formula_24:

Die Gegenzahl und der Kehrwert einer Zahl werden als die inversen Zahlen bezüglich der Addition und der Multiplikation bezeichnet. Auf diese Weise lassen sich die Rechenregeln für die Addition und Multiplikation auch auf die Subtraktion und Division übertragen.

In der Algebra werden diese zunächst für die Arithmetik geschaffenen Konzepte abstrahiert, um sie auf andere mathematische Objekte übertragen zu können. Eine algebraische Struktur besteht dann aus einer Trägermenge (hier einer Zahlenmenge), sowie ein oder mehreren Verknüpfungen auf dieser Menge (hier die arithmetischen Operationen), die nicht aus ihr herausführen. Die verschiedenen algebraischen Strukturen unterscheiden sich dann nur über die Eigenschaften der Verknüpfungen (die Rechenregeln), die als Axiome festgelegt werden, nicht jedoch bezüglich der konkreten Elemente der Trägermenge. Für die Grundoperationen erhält man die folgenden algebraischen Strukturen:


Nach dem Permanenzprinzip gelten dabei alle Rechenregeln einer grundlegenden Struktur (hier eines einfachen Zahlbereichs mit den Grundoperationen) auch in einer entsprechend spezielleren Struktur (hier einem erweiterten Zahlbereich mit den gleichen Operationen). Diese Strukturierung und Axiomatisierung erlaubt es nun, gewonnene Erkenntnisse von Zahlen auf andere mathematische Objekte zu übertragen. Beispielsweise sind entsprechende Operationen bei Vektoren die Vektoraddition und bei Matrizen die Matrizenaddition. Spezielle Strukturen entstehen bei der Betrachtung endlicher Mengen, zum Beispiel Restklassenringe als mathematische Abstraktion einer Division mit Rest.

Alle vier Grundrechenarten waren bereits in der altägyptischen Mathematik und in der babylonischen Mathematik bekannt. Die Multiplikation und die Division waren jedoch keine eigenständigen arithmetischen Operationen. Die Multiplikation natürlicher Zahlen wurde auf das fortgesetzte Verdoppeln ("Duplatio") eines Faktors und anschließende Addition der Teilergebnisse zurückgeführt. Die Division wurde bei nicht ganzzahligen Quotienten näherungsweise mittels fortgesetzter Halbierung ("Mediatio") durchgeführt. Multiplikation und Division finden sich als eigenständige Operationen erst in der altgriechischen Mathematik, etwa bei Euklid und bei Pappos.

Welche arithmetischen Operationen zu den Grundrechenarten gezählt werden, hat sich im Lauf der Zeit stark gewandelt. Bei Heron und Diophantos kamen zu den bekannten vier Rechenoperationen das Quadrieren und das Quadratwurzelziehen als weitere Grundrechenarten hinzu. In der indischen Mathematik wurden diese Operationen durch das allgemeinere Potenzieren und Wurzelziehen ersetzt und in neuerer Zeit um das Logarithmieren als siebte Grundrechenart ergänzt. In der arabischen Mathematik wurden beginnend mit Al-Chwarizmi auch die "Duplatio" und die "Mediatio" als eigene Rechenoperationen angesehen.

In den Rechenbüchern des Mittelalters gab es weitere Ergänzungen der Grundrechenarten, die dort als „Spezies“ bezeichnet wurden. So finden sich um 1225 bei Johannes de Sacrobosco insgesamt neun dieser Spezies: "Numeratio", "Additio", "Subtractio", "Duplatio", "Multiplicatio", "Mediatio", "Divisio", "Progressio" und "Radicum extractio". Die "Numeratio" behandelte das Zählen, Lesen und Schreiben der Zahlen, als "Progressio" wurde die Summation aufeinanderfolgender natürlicher Zahlen bezeichnet und die "Extractio" umfasste lediglich das Ziehen von Quadratwurzeln. Erst 1494 verwarf Luca Pacioli die "Duplatio" und die "Mediatio" als Spezialfälle der Multiplikation und der Division wieder. Daraufhin erfolgten weitere Reduktionen bis Gemma Frisius 1540 als einer der ersten Autoren die Grundrechenarten auf die bekannten vier beschränkte.



</doc>
<doc id="11917" url="https://de.wikipedia.org/wiki?curid=11917" title="NTFS">
NTFS

Microsoft NTFS ist ein proprietäres Dateisystem von Microsoft für alle Betriebssysteme der Windows-NT-Reihe. Die Abkürzung steht für New Technology File System.

Im Vergleich zum Dateisystem FAT bietet NTFS unter anderem einen gezielten Zugriffsschutz auf Dateiebene sowie größere Datensicherheit durch Journaling. Allerdings ist keine so breite Kompatibilität gegeben wie bei FAT. Ein weiterer Vorteil von NTFS ist, dass die Dateigröße nicht wie bei FAT auf 4 GiB beschränkt ist. Größere Dateien werden beispielsweise beim Erstellen von DVD-Images benötigt.

Als die Entwicklung von Windows NT, dem späteren Microsoft Windows NT 3.1 begann, war noch nicht klar, welches Dateisystem das zukünftige Betriebssystem benutzen würde. Zu diesem Zeitpunkt existierten das Dateisystem FAT16, welches von MS-DOS verwendet wurde, und HPFS, das Dateisystem von OS/2. Das Dateisystem FAT war zu diesem Zeitpunkt bereits weit verbreitet, aber nach Ansicht von David Cutler erfüllten sowohl FAT als auch das fortschrittlichere HPFS nicht die Voraussetzungen an Zuverlässigkeit, die er an ein Dateisystem stellte. Das neue Dateisystem musste nach seiner Ansicht in der Lage sein, beschädigte Dateien automatisch wiederherzustellen. Zudem hatten beide Dateisysteme Beschränkungen in der maximalen Dateigröße und -anzahl, und es war zu erwarten, dass das neue Betriebssystem in Zukunft größere Datenmengen verwalten würde. Die Entwicklung eines dritten Dateisystems drohte allerdings den Zeitplan des Betriebssystems zu gefährden.

So begann zunächst die Spezifikationsphase des neuen Dateisystems. Unter FAT16 waren Dateinamen auf das 8.3-Format beschränkt. Diese Namen waren durch die erzwungene Kürze oft kryptisch und darüber hinaus schwer zu merken. HPFS unterstützte zwar Dateinamen, die bis zu 255 Zeichen lang sind, aber ältere DOS- oder Windows-Programme konnten solche Dateien nicht sehen. NTFS sollte dieses Problem lösen, indem jeder lange Dateiname automatisch eine Kurzform erhielt, durch die die Datei auch von älteren Anwendungen bearbeitet werden konnte.

Die Entwicklung des neuen Dateisystems stand jedoch auf wackeligen Beinen. Der April 1991 hätte beinahe das Aus für NTFS bedeutet, als sich mehrere Entwickler dafür aussprachen, die Entwicklung dieses Dateisystems aus Zeitgründen zu beenden. Erst als Cutler, der sich zu dieser Zeit im Urlaub befand, zurückkehrte und die Wiederaufnahme der Entwicklung anordnete, gingen die Arbeiten weiter. Im Februar 1992 begann die Testphase des neuen Dateisystems, erst im Oktober 1992 war das Dateisystem stabil genug für eine Implementierung.

NTFS erbte viele Konzepte des Dateisystems HPFS von IBM, das in dem anfangs zusammen mit Microsoft entwickelten Betriebssystem OS/2 verwendet wurde, geht aber in einigen Aspekten weit darüber hinaus.

Im Gegensatz zu Inode-basierten Dateisystemen, welche bei Unix zum Einsatz kommen (Konzept: alles ist eine Datei), werden bei NTFS alle Informationen zu Dateien in einer Datei (Konzept: alles ist in einer Datei), der Master File Table, kurz MFT gespeichert.
In dieser Datei befinden sich die Einträge, welche Blöcke zu welcher Datei gehören, die Zugriffsberechtigungen und die Attribute. Zu den Eigenschaften (Attributen) einer Datei gehören unter NTFS Dateigröße, Datum der Dateierstellung, Datum der letzten Änderung, Freigabe, Dateityp und auch der eigentliche Dateiinhalt.

Sehr kleine Dateien und Verzeichnisse werden in der MFT direkt abgespeichert. Größere Dateien werden dann als Attribut in einem Datenlauf gespeichert. Es existieren 4 Stadien des Dateiwachstums.

Beim Formatieren der Festplatte wird für die MFT ein fester Platz reserviert, der nicht von anderen Dateien belegt werden kann. Wenn dieser Bereich mit Informationen komplett gefüllt ist, beginnt das Dateisystem freien Speicher vom Datenträger zu benutzen, wodurch es zu einer Fragmentierung der MFT kommen kann. Standardmäßig wird ein Bereich von 12,5 % der Partitionsgröße für die MFT reserviert.

Beim Speichern von Metadaten wird ein Journal geführt, was bedeutet, dass eine geplante Aktion zuerst in das Journal geschrieben wird. Erst dann wird der eigentliche Schreibzugriff auf die Daten ausgeführt, und abschließend wird das Journal aktualisiert. Wenn ein Schreibzugriff nicht vollständig beendet wird, zum Beispiel wegen eines Absturzes, braucht das Dateisystem nur die Änderungen im Journal zurückzunehmen und befindet sich anschließend wieder in einem konsistenten Zustand.

Die folgende Liste spiegelt die Zuordnung zwischen NTFS- und Windows-Version wider:



Die Unterschiede gegenüber FAT sind:



Analysepunkte (englisch auch "" genannt) stellen eine flexible Erweiterung für das Dateisystem dar, indem es Dateisystemeinträge mit Funktionen verknüpft. Diese können auf vielfältige Art verwendet – so etwa über den Befehl codice_1 verwaltet – und auch in zukünftigen Versionen erweitert werden. Ein Dateisystemtreiber, der eine bestimmte Art Analysepunkt nicht kennt, führt diesen nicht aus. Beim Zugriff auf einen Analysepunkt werden die funktionsspezifischen Analysedaten dynamisch durch die entsprechende Funktion ausgewertet (daher „Analyse“). Dies impliziert, dass eine solche Analyse auch fehlschlagen kann und ein Zugriff auf die durch den Analysepunkt bereitgestellten Daten (möglicherweise durch aktuelle, vorübergehende Umstände) nicht möglich ist.

Folgende Funktionen werden derzeit von NTFS unterstützt:

Mit der Einführung von Windows Vista wurde das NTFS-Dateisystem um das Konzept atomarer Operationen (Transaktionen) erweitert. Dieses transaktionsbasierte NTFS (engl. "Transactional NTFS"; kurz: TxF) ermöglicht es Anwendungen, Dateioperationen atomar auszuführen. Veränderungen am Dateisystem werden also nur dann ausgeführt, wenn die gesamte Transaktion erfolgreich durchgeführt werden konnte. Zu einer Transaktion kann dabei eine Einzeloperation oder eine Abfolge von Dateioperationen gehören (beispielsweise das Erzeugen, Löschen oder Umbenennen einer oder mehrerer Dateien bzw. Verzeichnisse).

Transactional NTFS wurde auf Basis des ebenfalls mit Windows Vista eingeführten "Kernel Transaction Manager" (KTM) implementiert, der Transaktionen auf der Ebene des Kernels ermöglicht. Es erweitert die bereits in vorigen NTFS-Versionen enthaltene Journal-Funktionalität, die sich auf die Integrität der Strukturen des Dateisystems beschränkt, um folgende Möglichkeiten:


Windows unterstützt Transaktionen ab Windows Vista bzw. Windows Server 2008. Mittlerweile empfiehlt Microsoft allerdings den Einsatz von Alternativen, die API muss damit als "deprecated" betrachtet und von einem Einsatz abgeraten werden.

Je nach Größe des Laufwerks werden folgende Standard-Clustergrößen vergeben:

„nicht unterstützt (MBR)“ = Der Master Boot Record unterstützt nur Laufwerke bis 2 Tebibyte, darüber hinaus wird die GUID Partition Table verwendet, welche erst ab Windows 2000 und von Computern mit Extensible Firmware Interface unterstützt wird.






Da es sich bei NTFS um ein proprietäres Dateisystem handelt, ist ein Zugriff durch andere Betriebssysteme als die der Windows-NT-Reihe unter Umständen nur in begrenztem Umfang möglich.

Für DOS-basierte Betriebssysteme, zu denen auch die Betriebssysteme Windows-9x-Reihe zählen, existieren Treiber wie "NTFS4DOS", die einen vollständigen Zugriff auf NTFS-Laufwerke ermöglichen.

Linux unterstützt über NTFS-3G vollständigen Lese- und Schreibzugriff, Lesezugriff auf verschlüsselte Dateien und kann Datenträger in NTFS formatieren. Weiterhin gibt es einen Kerneltreiber. macOS kann ab Version 10.3 NTFS-Dateisysteme lesen, aber nicht schreiben. In Version 10.6 (Snow Leopard) wurde eine versteckte Schreibfunktionalität gefunden, die aber nicht offiziell freigegeben ist.

Unter Linux, macOS und FreeBSD kann außerdem über den User-Mode-Treiber NTFS-3G auch schreibend auf NTFS-Dateisysteme zugegriffen werden. Darüber hinaus stellt Paragon einen kommerziellen Treiber zur Verfügung.





</doc>
<doc id="11920" url="https://de.wikipedia.org/wiki?curid=11920" title="Dschihad">
Dschihad

Der Begriff Dschihad [] (; auch "Djihad" oder gelegentlich in der englischen Schreibweise "Jihad") bezeichnet im religiösen Sinne ein wichtiges Konzept der islamischen Religion, die Anstrengung/den Kampf auf dem Wege Gottes ().

Etymologisch steht er für eine auf ein bestimmtes Ziel gerichtete Anstrengung. Im Koran und der Sunna bezeichnet dieser Begriff primär militärischen Kampf. Aus dem Koran geht nicht eindeutig hervor, ob es sich dabei um einen universellen Kampf gegen Andersgläubige handelt oder dieser Kampf nur defensive Ziele verfolgt: „"Das vorhandene Schriftmaterial bedarf der Interpretation, wobei die Haltung der Interpreten von entscheidender Bedeutung ist, die maßgeblich von den unterschiedlichen politischen und gesellschaftlichen Rahmenbedingungen geprägt wurde und wird."“

Nach klassischer islamischer Rechtslehre (Fiqh), deren Entwicklung in die ersten Jahrhunderte nach dem Tode Mohammeds zu datieren ist, dient dieser Kampf der Erweiterung und Verteidigung islamischen Territoriums, bis der Islam die beherrschende Religion ist. In seiner späteren Entwicklung sowie insbesondere im Zuge der Moderne haben muslimische Gelehrte begonnen, nichtmilitärische Aspekte dieses Kampfes zu betonen (Siehe Absatz unten: Nichtmilitärische Auslegungen des Dschihadbegriffs). Muslimische Autoren der Moderne sehen ausschließlich solche Kriege als legitim an, die der Verteidigung islamischer Staaten, der Freiheit der Muslime, den Islam außerhalb dieser zu verkünden, und dem Schutz der Muslime unter nichtislamischer Herrschaft dienen. Dem entspricht ihre Auslegung entsprechender Koranverse.

Der Dschihad stellt als eines der Grundgebote des islamischen Glaubens und eine allen Muslimen auferlegte Pflicht ein wichtiges Glaubensprinzip des Islam dar. Manche sunnitische Gelehrte rechnen den Dschihad als sechste zu den „fünf Säulen des Islams“.

Die Charidschiten zählen den Dschihad zu den fünf Grundpfeilern des Islam.

Im imamitischen Dschihadverständnis gibt es bis zum Erscheinen des sogenannten "verborgenen Imams", Muhammad ibn Hasan, keinen Dschihad zur Erweiterung des islamischen Herrschaftsbereichs, da erst dieser berechtigt ist, diesen zu führen. Die Verteidigung des eigenen Territoriums ist dennoch auch im imamitischen Dschihadverständnis nach wie vor verpflichtend, welche allerdings nicht als "Dschihad" verstanden wird.
Die imamitische Rechtslehre hat es verstanden, in Angesicht militärischer Notwendigkeit Wege um diese Regelung zu finden, so dass auch Kriege in Abwesenheit des verborgenen Imam rechtliche Legitimierung erhalten konnten.

Im europäischen Sprachraum wird der Begriff öfter mit dem Ausdruck "Heiliger Krieg" übersetzt. Dem entgegnen muslimische Autoren, dass "Dschihad" semantisch nicht nur Kriegsführung bezeichne, es nichtmilitärische Bedeutungen des Dschihadbegriffs gebe und sehen deshalb eine derartige Übersetzung als falsch an und lehnen sie ab.

In der Islamwissenschaft ist eine Umschreibung des "Dschihad" als "Heiliger Krieg" im Sinne eines von Gott vorgeschriebenen, seinetwegen geführten und von ihm belohnten Krieges gängig. Eine Gleichsetzung beider Begriffe als solcher stößt in der Forschung allerdings oft auf Ablehnung.
Dschihad ist zudem ein arabischer Vorname, dessen Gebung auch in anderen Schreibweisen im deutschen Sprachraum gesetzlich erlaubt ist.

Seinen Ursprung hat die Dschihadlehre im Koran und der Sunna Mohammeds. In diesen Quellen wird der Begriff im militärischen Sinne, als Kampf gegen einen Feind verstanden. Die entsprechenden Koranverse wurden vor dem Hintergrund der Auseinandersetzung des Propheten und seiner Anhängerschaft mit ihren polytheistisch-arabischen sowie jüdischen und christlichen Gegnern offenbart. Diese Verse sowie die Mohammed zugeschriebenen Aussprüche und Taten, seine "Sunna", bildeten die primäre Grundlage für die spätere Entwicklung der Dschihadlehre im islamischen Recht.

Im Koran kommt das Nomen "Dschihad" viermal vor. Zusammen mit seinen unterschiedlichen Verbformen ist es im Koran fünfunddreißig Mal vorzufinden. Dem folgt meist der Zusatz „auf dem Wege Gottes“, „mit Gut und Blut“ oder eine Kombination beider: Bei der Benutzung eines dieser bzw. beider Zusätze ist stets ein bewaffneter Kampf gemeint.

Zu Beginn seiner prophetischen Karriere schloss sich Mohammeds Anhängerschaft aus wenigen, meist einflussarmen Personen zusammen. Als Mohammed den Götzenkult der Quraisch zu kritisieren begann, kam es zu Verspottungen der Heilsbotschaft des Propheten bis hin zu offenen Ausschreitungen gegenüber den Anhängern der neuen Glaubensgemeinschaft von Seiten der heidnischen Mekkaner. Ein Gebot zum Kampf gegen diese existierte vor der Auswanderung Mohammeds nach Medina, der Hidschra, noch nicht. Aufgrund ihrer völligen Unterlegenheit blieb der damaligen Gemeinschaft Mohammeds nichts anderes übrig, als die Unterdrückung von Seiten der Quraisch ohne Gegenwehr zu ertragen und sie möglichst zu ignorieren: Auch in der Zeit unmittelbar nach der Ankunft der "Muhādschirūn", der aus Mekka stammenden muslimischen „Auswanderer“, in Yathrib wurde die Anhängerschaft Mohammeds vor einer militärischen Konfrontation mit den Mekkanern zurückgehalten. Bestätigt wird dies unter anderem durch folgenden Koranvers, in dem es retrospektiv darauf Bezug nehmend heißt: Erst in den Folgemonaten wurde der Vers offenbart, den die islamische Koranexegese mehrheitlich als den ersten Aufruf zum Kampf ansieht: Der Krieg der Muslime auf der Arabischen Halbinsel durchlief mehrere Phasen und mündete schließlich in einem allgemeinen Kampf gegen die arabischen Götzendiener zum einen …:

… und die Schriftbesitzer, die Juden und Christen, auf der Halbinsel zum anderen:

Diese Koranverse, auch bekannt als die Schwertverse, wurden in der mehrheitlich als Aufruf zu einem allgemeinen Kampf gegen die nichtmuslimische Welt verstanden. Einzelne Koranexegeten dieser Periode haben indes den letzteren Koranvers lediglich auf die Schriftbesitzer der arabischen Halbinsel bezogen.

Die Auswanderer aus Mekka, denen in Yathrib mehrheitlich jegliche finanzielle Grundlage fehlte, folgten dem altarabischen Brauch der "Razzia" und begannen Karawanen der Quraisch zu überfallen und zu plündern. Die Übernahme dieses schon zu vorislamischen Zeiten gängigen Konzepts der Karawanenüberfälle, die nun als "Dschihad" bezeichnet wurden, beschränkte sich nicht auf einen Namenswechsel: Während solche Raubzüge zuvor einen Angriff eines Stammes gegen einen anderen ungeachtet ihres damaligen Verhältnisses bedeuteten, war "Dschihad" der Kampf einer religiösen Gemeinschaft gegen Andersgläubige. Damit einhergehend expandierte diese Gemeinschaft, da ein Stamm von diesen Feldzügen nicht mehr betroffen war, sobald er den Islam annahm. „"Es war dieses 'religiöse' "[sic]" Wesen des "Dschihad", das die Energien der Araber auf so eine Weise lenkte, dass sie in weniger als einem Jahrhundert ein Imperium begründeten, das sich vom Atlantik und den Pyrenäen im Westen bis zum Oxus und dem Punjab im Osten erstreckte. Es scheint so gut wie sicher zu sein, dass diese Expansion sich ohne das Dschihadkonzept nicht ereignet hätte."“

Die Tatsache, dass in einzelnen Koranversen das Verb "dschahada" (dt.: „sich anstrengen“, „kämpfen“) ohne einen der weiter oben erwähnten Zusätze benutzt wird, legt dar, dass diese Karawanenüberfälle zunächst keinen religiösen Charakter hatten. So heißt es zum Beispiel in Sure 16, Vers 110:

Dieser religiöse Charakter, gewöhnlicherweise durch einen solchen Zusatz gekennzeichnet, sei – so Watt – den Überfällen erst später hinzugefügt worden, als Mohammed eine Beteiligung der medinensischen Muslime, der sogenannten Helfer, an den Raubzügen zu verlangen begann und entsprechende Koranverse offenbart wurden:

Die Helfer hatten sich bis dahin nur dazu verpflichtet, den Muslimen aus Mekka bei einem Angriff der Quraisch militärisch beizustehen und die jeweiligen Quellen, darunter mehrere Koranverse, machen klar, dass bis zur Schlacht von Badr im Jahre 624 n. Chr. sich hauptsächlich bis ausschließlich Emigranten an den jeweiligen Karawanenüberfällen beteiligt hatten.

Im Zuge dieser Raubzüge kam es zu militärischen Konfrontationen größeren Ausmaßes zwischen den Quraisch und den Anhängern Mohammeds, die erst 628 n. Chr. temporär durch einen Friedensvertrag, den sogenannten „"Vertrag von al-Hudaibiya"“, beendet wurden. Auf den Bruch dieses Vertrags von Seiten der Mekkaner folgte die Eroberung Mekkas 630 n. Chr. Als Mohammed am achten Juni 632 n. Chr. verstarb, erstreckte sich der islamische Herrschaftsbereich über die gesamte Arabische Halbinsel.

Siehe auch: Mohammed#Die medinensische Periode der Prophetie (622–630)

Der Koran nimmt mehrmals Bezug auf den Kampf gegen Ungläubige. Viele Verse fordern die Muslime zum Kampf auf und versprechen den Gefallenen unter ihnen Belohnungen im Jenseits. Die muslimischen Kommentatoren sind sich uneinig darüber, ob die Märtyrer des Kampfes erst am Ende der Zeit das Leben wiedererlangen (so die Mu'taziliten) oder bereits jetzt als lebendig gelten sollen. Für die Vertreter der zweiten Meinung gilt, dass die Seelen der Märtyrer weiterleben und Gott loben und verehren.

Der Koran droht den “Heuchlern”, die sich nicht am Kampf beteiligen, mit Höllenstrafen: Weitere Verse behandeln kriegsrechtliche Fragen, wie beispielsweise die Behandlung von Kriegsgefangenen, den Ausschluss vom Kriegsdienst oder Waffenstillstände.

Zwei Textstellen im Koran sprechen vom „Abmühen um Gottes Willen“: Sure 29, Vers 69: … sowie Sure 22, Vers 77–78: 

Diese Verse können als Aufforderung zur Bemühung, um sich „"den bösen Gelüsten und Verführungen entgegenzustemmen"“ interpretiert werden. Die klassische Koranexegese hat sie allerdings auf Kriegsführung bezogen.

Ob der Koran Krieg nur zum Zweck der Verteidigung sanktioniert oder einen allgemeinen Kampf gegen Andersgläubige vorsieht ist unklar und obliegt der Exegese, da die Absichten und Ziele des Dschihad aus dem Koran nicht eindeutig hervorgehen. Die darin enthaltenen Kriegsverordnungen haben eher den Charakter der Werbung von Kämpfern und behandeln keine kriegsethischen Fragen.

Neben dem Koran behandeln auch die maßgeblichen Hadithsammlungen den Dschihad, die jeweils ein ganzes Kapitel zu diesem Thema enthalten. Darin enthaltene, auf Mohammed zurückgeführte Überlieferungen behandeln unter anderem die Vorzüge des Kampfes auf dem Wege Gottes, die jenseitige Belohnung derjenigen, die sich an diesem Kampf beteiligen und vor allem derjenigen, die bei diesem Kampf ums Leben kommen.

So wird in der kanonischen Hadithsammlung Bucharis folgender dem Propheten zugeschriebener Ausspruch im Kapitel über den "Einsatz für die Sache Gottes" verzeichnet:

In der Aussage diesem Hadith ähnlich wird folgender Ausspruch auf den Propheten zurückgeführt:

Hier verbindet sich die Lehre vom Dschihad, dem bewaffneten Kampf, mit dem Gedanken des Martyriums. Das islamische Schrifttum ist in und außerhalb der kanonischen Hadithsammlungen des 9. Jahrhunderts reich an Werken über den Dschihad und über die Vorzüge desselben als religiöse Pflicht.

Zusätzlich zu diesem Thema behandeln entsprechende Traditionen auch kriegsrechtliche Fragen, wie die Behandlung von Gefangenen oder das Verbot, Frauen und Kinder zu töten.

Im Zuge der Entwicklung des islamischen Rechts in den ersten Jahrhunderten nach dem Tode des Propheten haben muslimische Rechtsgelehrte die Doktrin des Dschihad ausgearbeitet. Fast alle klassischen islamischen Rechtsbücher enthalten deshalb auch ein eigenes Kapitel zum Dschihad. Zu den mittelalterlichen juristischen Autoren, deren Dschihad-Kapitel bereits in europäischen Übersetzungen vorliegen, gehören Ibn Abī Zaid al-Qairawānī (gest. 996), Ibn Tūmart (gest. 1130), Averroës (gest. 1198) und Ibn Taimīya (gest. 1328). Das Kapitel zum Dschihad in dem Rechtshandbuch "al-Muqaddimāt al-mumahhadāt" des andalusischen Gelehrten Ibn Ruschd al-Dschadd (gest. 1126) ist von Janina Safran ausgewertet worden.

Die gängige sprachtechnische Definition des Dschihadbegriffs in den jeweiligen Rechtswerken ist „"sich so sehr anzustrengen, wie es einem möglich ist"“, während man die Dschihadpflicht im rechtlichen Sinne als Kampf gegen die Ungläubigen verstanden hat. Von der großen Mehrheit der klassischen muslimischen Theologen, Juristen und Traditionarier wurde der Dschihadbegriff im militärischen Sinne verstanden. Ausnahmen bildeten einzelne Theologen schiitischer Zugehörigkeit, die zwischen einem größeren Dschihad als innerseelischem Kampf und einem kleineren Dschihad im eben beschriebenen Sinne unterschieden. (Siehe dazu den Absatz Nichtmilitärische Auslegungen des Dschihadbegriffs)

In der islamischen Rechtsprechung stellt der Dschihad die einzig zulässige Form eines Krieges gegen Nichtmuslime dar. Neben dem Kampf gegen die Ungläubigen ist ein Krieg gegen vom Islam Abgefallene, Aufrührer und Fahnenflüchtige sowie Straßenräuber legitim. Als Dschihad im Sinne einer religiösen Pflicht gilt nur der Krieg gegen Nichtmuslime und Apostaten. Mit den juristischen Fragen der Kriegsführung beschäftigt sich ein eigenständiger Zweig der islamischen Rechtsprechung, die Siyar-Literatur, das islamische Völkerrecht.

Als unmittelbares Ziel des Dschihad galt die Stärkung der islamischen Religion, der Schutz der Muslime und die Beseitigung des Unglaubens auf der Welt mit dem Ziel einer islamischen Vormachtstellung auf dem gesamten Globus. Als Grundlage dafür dienten Koranverse wie der Folgende: Eine Zwangsbekehrung oder Vernichtung der Nichtmuslime war hingegen nicht vorgesehen.

Unter den Nichtmuslimen sind die Polytheisten zu bekämpfen, bis sie den Islam annehmen; die Schriftbesitzer haben neben der Möglichkeit zur Konversion auch das Recht, mit dem muslimischen Herrscher einen Dhimma-Vertrag zu schließen. Letzteres war ursprünglich nur Juden, Christen und Sabäern vorbestimmt. Im Laufe der islamischen Expansion hat man indes das Angebot der "Dhimma" auch auf andere Religionsgemeinschaften, wie beispielsweise die Zoroastrier oder die Hindus, ausgeweitet, so dass letzten Endes alle Nichtmuslime schlechthin dazu befähigt waren, einen Dhimma-Vertrag mit den muslimischen Eroberern zu schließen.

Die schiitische Dschihadlehre unterschied sich von der gängigen sunnitischen hauptsächlich dadurch, dass nach schiitischem Verständnis nur der "verborgene Imam" dazu befähigt ist, einen Dschihad zur Ausweitung des islamischen Machtbereichs zu führen; eine Verteidigung vor feindlichen Angriffen war zwar erlaubt, ist aber kein "Dschihad" in solchem Sinne. Anderweitig bestanden dahingehend keine größeren Diskrepanzen.

Aufgrund der Meinungsvielfalt unter den Gelehrten ist es nicht möglich, von einer einheitlichen klassischen Dschihadlehre zu sprechen. Die entsprechenden Angaben in diesem Artikel stellen lediglich Grundlagen der Kriegsführung dar, die unter den Rechtsschulen allgemein als solche anerkannt waren.

Von grundlegender Bedeutung für das klassisch-islamische Völkerrechtsverständnis ist die Einteilung der Welt in ein "Haus des Islam" ("Dār al-Islām") und ein "Haus des Krieges" ("Dār al-Harb"). Während ersteres alle Gebiete unter islamischer Herrschaft bezeichnet, gilt jedes Land außerhalb des islamischen Herrschaftsbereichs als zum "Haus des Krieges" zugehörig. Es gilt als Pflicht der islamischen Gemeinschaft, möglichst große Teile des "Dar al-Harb" auf militärischem Wege dem "Dar al-Islam" einzuverleiben.

Die schafiitische Rechtsschule nennt noch eine weitere Kategorie: Das "Haus des Vertrags" ("Dar al-Ahd"). Als solches gelten Gebiete, deren nichtmuslimische Bewohner ein Waffenstillstandsabkommen mit den Muslimen unter der Bedingung geschlossen haben, dass sie ihre Gebiete behalten und stattdessen jährlich einen bestimmten Geldbetrag oder eine bestimmte Anzahl an Gütern zahlen würden.

Als einzige der vier sunnitischen Rechtsschulen legt die hanafitische fest, unter welchen Umständen ein zum "Haus des Krieges" zugehöriges Gebiet zum "Haus des Islam" zugehörig wird und umgekehrt. Zum "Haus des Islam" wird nach allgemein anerkannten Regelungen ein Gebiet, wenn es sich unter islamischer Herrschaft befindet und das islamische Recht, die "Scharia", dort angewandt wird. In Bezug darauf, wann ein zuvor dem "Haus des Islam" zugehöriges Gebiet als Teil des "Hauses des Krieges" zu gelten hat, hat der muslimische Rechtsgelehrte Abu Hanifa, auf den die Schule der Hanafiten zurückgeht und dessen diesbezügliche Meinung in der hanafitischen Rechtsschule dominiert, folgende Bedingungen festgesetzt:


Diese Bedingungen können erfüllt werden, wenn ein Teil des "Hauses des Islam" erobert wird oder eine Gruppe von Dhimmis ihren Vertrag mit den Muslimen aufkündigt.

Die militärische Expansion des "Dar al-Islam" ist eine kollektive Pflicht der islamischen Gemeinschaft, d. h., dass, sofern eine ausreichende Anzahl an Truppen bereitsteht, der Rest aller Muslime von dieser Pflicht befreit ist. Sofern sich niemand am Dschihad beteiligt, sündigt die gesamte islamische Gemeinschaft. Der jeweilige muslimische Herrscher hat die Pflicht, mindestens einmal im Jahr den Dar al-Harb anzugreifen. Sofern dies aus irgendeinem Grund vorerst nicht möglich sein sollte, ist es ihm erlaubt, dieses jährliche Unternehmen zu verschieben. (Vgl. Absatz unten: Hudna)

Zu einer individuellen Pflicht wird der Dschihad im Verteidigungsfall, wobei jede wehrfähige Person im angegriffenen Gebiet zu kämpfen hat. Sofern ihre militärische Stärke nicht ausreichen sollte, gilt diese Pflicht auch den jeweiligen benachbarten Gebieten. Des Weiteren wird der Kampf zur individuellen Pflicht der jeweiligen Personen, wenn der Kalif sie zum Kriegsdienst bestimmt oder sie einen Schwur leisten, am Dschihad teilzunehmen.

Ausgeschlossen vom Kriegsdienst sind unter anderem Frauen, Kinder, Sklaven, körperlich oder geistig Behinderte sowie Personen, die aus materiellen Gründen nicht teilnehmen können. Als Begründung für diese Ausnahmebedingungen zitieren die jeweiligen Rechtsgelehrten entsprechende Koranverse beziehungsweise Überlieferungen von Aussprüchen, die dem Propheten zugeschrieben werden.

Das islamische Völkerrecht sah vor, dass dem Kampf gegen die nichtmuslimischen Feinde die Aufforderung an diese, den Islam anzunehmen oder – im Falle von Schriftbesitzern – im Gegenzug zur Zahlung der Dschizya in ihrer Religion zu verbleiben (siehe Dhimma), vorausging. Eine Grundlage hierfür bildete Sure 17, Vers 15, wo es heißt:

Diese Aufforderung vor der Kampfhandlung war zudem Sunna des Propheten als auch seiner unmittelbaren Nachfolger.

Die klassische islamische Völkerrechtslehre verbot zudem – auch auf Basis von Koranversen beziehungsweise Prophetensprüchen – bestimmte Taten während der Kampfhandlungen, darunter die Tötung von Nichtkombattanten wie Frauen, Kindern oder Mönchen (sofern sie sich nicht am Kampf beteiligen), die Verstümmelung sowohl menschlicher als auch tierischer Leichen, Vertragsbruch, die unnötige Zerstörung fremden Guts sowie die Tötung von Geiseln.

In den entsprechenden Rechtswerken werden neben diesen auch andere kriegstechnische Fragen behandelt, wie zum Beispiel die Behandlung von Kriegsgefangenen oder die Verteilung von Beute.

Die historischen Verträge zwischen den muslimischen Eroberern und den Bevölkerungen der jeweiligen Gebiete sind in den Geschichtswerken, bei at-Tabari und al-Baladhuri – um hier nur die frühesten Kompilationen zu nennen –, überliefert und in der Forschung mehrfach erörtert worden. Im Allgemeinen verzeichnen diese Verträge die Sicherheitsgarantie für Leben und Besitz, die Gewährung freien Abzugs für diejenigen, die nicht unter islamischer Herrschaft leben wollen, aber auch die Verpflichtung, Kirchen und Befestigungsanlagen nicht zu zerstören. (Siehe auch: Dhimma)

Das klassisch-islamische Recht sah den Kriegszustand als den gewöhnlichen Zustand der Beziehungen zwischen dem "Dār al-Islām" und dem "Dār al-Harb" an. Ein zeitlich unbegrenztes Friedensabkommen mit Letzterem sah es nicht vor. Für einen bestimmten Zeitraum konnte der Kriegszustand durch einen Waffenstillstand, eine sogenannte "hudna", eingestellt werden. Die Dauer solcher Verträge ist in den Rechtsschulen nicht einstimmig festgelegt. Von den Hanafiten abgesehen darf nach jeder Rechtsschule ein solcher Vertrag nur temporäre Geltung besitzen.

Ausschlaggebend für das Konzept der "hudna" ist unter anderem Sure 9, Vers 1, in der „eine bindende Abmachung“ mit den Heiden erwähnt wird …: … sowie Sure 8, Vers 61: Ferner war der 628 geschlossene Vertrag Mohammeds mit den Mekkanern bei al-Hudaibiya, bei dem ein zwei-, nach anderen Quellen ein zehnjähriges Waffenstillstandsabkommen abgeschlossen wurde, von entsprechender Bedeutung.

Einem außerhalb des islamischen Herrschaftsbereichs lebenden Nichtmuslim ist es möglich, durch einen sogenannten "aman", eine Schutzerklärung eines Muslims, als Musta'min auf islamischem Gebiet ohne jegliche Steuerverpflichtungen zu verweilen, solange er dort keine permanente Residenz begründet. Als rechtliche Grundlage dient hierfür Sure 9, Vers 6:

Dem Angriff gegen den nichtmuslimischen Feind ging das Angebot voraus, zum Islam überzutreten oder einen Dhimma-Vertrag zu schließen. Eine Zwangsbekehrung zum Islam sieht die Dschihadlehre nicht als Zweck des Kampfes an. Die in Sure 2, Vers 256 formulierte Norm „"In der Religion gibt es keinen Zwang"“, die einigen klassischen Korankommentaren zufolge durch spätere Koranverse wie den Schwertvers abrogiert worden ist, und die Dschihad-Theorie vom bewaffneten Kampf gegen Ungläubige schließen sich gegenseitig nicht zwangsläufig aus, da den Nichtmuslimen Religionsfreiheit nach ihrer Niederlage gewährt werden konnte. Klassische Korankommentatoren, die den Vers nicht als abrogiert ansahen, tendierten dazu, zu argumentieren, dass sich der Vers nur auf die Schriftbesitzer beziehe, denen die Möglichkeit offenstand, als Dhimmis unter muslimischer Autorität zu leben, ohne zum Islam zu konvertieren.

Der Dschihad zwecks Konversion beschränkte sich lediglich auf die frühislamische Zeit, auf die Unterwerfung der arabischen Stämme zur Zeit Mohammeds und kurz nach seinem Tode. Diese Auffassung im klassischen islamischen Recht war zwar nicht unumstritten, jedoch wird sie in der Moderne im Allgemeinen als geltende Norm akzeptiert.

Der tunesische Gelehrte und Koranexeget Tahir ibn Āschūr (1879–1970) harmonisiert den Inhalt der Sure 2:256 und die religiösen Pflicht des Dschihad wie folgt: Der Vers sei zur Zeit nach der Eroberung Mekkas 630 n. Chr. offenbart worden und abrogiere alle Verse und Prophetensprüche, denen zufolge das Kriegsziel die Konversion der Bekämpften sei. Seit der Offenbarung dieses Verses habe sich das Kriegsziel dahingehend geändert, dass es nicht mehr die Konversion, sondern die Unterwerfung der Bekämpften und ihre Akzeptanz islamischer Dominanz ist. Einen ähnlichen Standpunkt vertrat der syrische Gelehrte al-Qāsimī (1866–1914).

Während sowohl Koran als auch Sunna sowie die Mehrheit der klassischen Gelehrten unter "Dschihad" primär bzw. ausschließlich eine militärische Betätigung verstanden, entstanden im Verlauf ihrer Entwicklung auch nichtmilitärische Auslegungen der Dschihadlehre. Dies geschah insbesondere im Zuge tiefgreifender politischer Veränderungen wie der Kolonialisierung großer Teile der islamischen Welt, der Aufhebung des Kalifats und der Entwicklungen der Moderne.

Einzelne schiitische Theologen der klassischen Zeit unterschieden zwischen dem sogenannten größeren Dschihad im Sinne eines spirituellen Kampfes gegen innere Gelüste und dem kleineren Dschihad im Sinne einer militärischen Konfrontation gegen einen äußeren Feind. Dem entspricht die Betonung nichtmilitärischer Aspekte der Dschihadpflicht vieler gegenwärtiger muslimischer Autoren, als auch muslimischer Asketen und Mystiker.

Postklassische Juristen haben den Begriff in vier Arten unterteilt:

Mit diesem Verständnis richtet sich der Dschihad gegen das eigene Ich, gegen die „Triebseele“ ("an-nafs al-ammāra bi 'l-sūʾ"). Die Durchsetzung der Anweisung, das Rechte zu gebieten und das Verwerfliche zu verbieten, geschieht „mit der Zunge, mit der Hand und mit dem Schwert, je nachdem, wozu man imstande ist“. Die Asketen sehen im Kampf gegen sich selbst ("mudschahadat an-nafs") das höchste Ideal.

Ein bekanntes Beispiel für derartige Auslegungen aus der Moderne war Präsident Habib Bourguibas Verkündung, dass der Kampf gegen die ökonomische Dekadenz Tunesiens als Dschihad anzusehen war. Da ein Mudschahed, ein sich am Dschihad Beteiligender, von der Pflicht im Ramadan zu fasten befreit war, argumentierte Bourguiba, dass das Fasten im Ramadan deshalb auch für Arbeitende – die dadurch ebenfalls als Glaubenskämpfer anzusehen waren – keine Pflicht sei. Auf diese Weise hat er versucht die alljährliche wirtschaftliche Stagnation in diesem Monat zu beseitigen. Seine diesbezügliche Ansicht wurde im Nachhinein auch von Teilen der islamischen Gelehrsamkeit übernommen.

Bei vielen Terroranschlägen der jüngsten Vergangenheit rechtfertigten die Attentäter ihr Handeln mit dem Verweis auf das Dschihadkonzept. Mehrere islamistische Organisationen führen das Wort "Dschihad" in ihrem Namen, wie zum Beispiel Islamischer Dschihad und Al-Dschihad. Diese Gruppierungen rechtfertigen ihre militärische Auslegung des Begriffs "Dschihad" auch aus dem islamischen Glauben heraus.

Selbstmordattentäter werden dabei als "Schahīd"-Märtyrer bezeichnet, denen ein Platz im Paradies sicher ist.
An sich gilt Suizid im Islam als Sünde, die im Jenseits mit der endlosen Wiederholung des Moments des Todes bestraft wird.

Der Internationale Islamische Gelehrtenrat, der nach den Terroranschlägen am 11. September 2001 Stellung zu islamistisch-extremistisch motivierter Gewalt bezog, verurteilte in dem Manifest von Mekka „"Extremismus, Gewalt und Terrorismus"“, beteuerte, dass diese „"nicht im Geringsten zum Islam gehören"“ und hielt fest: „"Dschihad ist kein Terrorismus."“

Siehe auch: Islamistischer Terrorismus







</doc>
<doc id="11928" url="https://de.wikipedia.org/wiki?curid=11928" title="Polarlicht">
Polarlicht

Das Polarlicht (als Nordlicht auf der Nordhalbkugel wissenschaftlich Aurora borealis, als Südlicht auf der Südhalbkugel Aurora australis) ist eine Leuchterscheinung durch angeregte Stickstoff- und Sauerstoffatome der Hochatmosphäre (Elektrometeor), die in Polargebieten beim Auftreffen beschleunigter geladener Teilchen aus der Erdmagnetosphäre auf die Atmosphäre hervorgerufen wird. Polarlichter sind meistens in zwei etwa 3 bis 6 Breitengrade umfassenden Bändern in der Nähe der Magnetpole zu sehen.

Polarlichter entstehen, wenn elektrisch geladene Teilchen des Sonnenwinds aus der Magnetosphäre (hauptsächlich Elektronen, aber auch Protonen) auf Sauerstoff- und Stickstoffatome in den oberen Schichten der Erdatmosphäre treffen und diese ionisieren. Bei der nach kurzer Zeit wieder erfolgenden Rekombination wird Licht ausgesandt. Durch die Energieübertragung rutschen die Elektronen also eine Schale nach außen, danach aber wieder auf die ursprüngliche Schale zurück. Dabei wird elektromagnetische Strahlung emittiert, es wird Licht ausgesandt.

Die Energie stammt ursprünglich aus Emissionen der Sonne. Sie sendet ein elektrisch geladenes Plasma mit einer durchschnittlichen Geschwindigkeit von 500 bis 800 km/s und einer Dichte von ca. 5 × 10 Teilchen pro Kubikmeter aus. Die größten Sonnenwindausbrüche geschehen durch magnetische Rekonnexionen im Bereich von Sonnenflecken während der turbulenten, fleckenreichen Phase des Sonnenzyklus. Sonnenwindteilchen treffen auf die irdische Magnetosphäre und treten mit ihr in Wechselwirkung.
Aufgrund des Abstandes von der Sonne zur Erde, rund 150 Millionen Kilometer, benötigt das Sonnenwindplasma bis zum Auftreffen auf die Erdmagnetosphäre zwei bis vier Tage. Die auftreffenden Sonnenwindpartikel stauchen die Erdmagnetosphäre auf der sonnenzugewandten Seite und ziehen sie auf der abgewandten Seite zu einem langen Schweif aus. Aufgrund ihrer Ladung werden die Sonnenwindpartikel hauptsächlich längs der Richtung des Erdmagnetfeldes abgelenkt, und umströmen die irdische Magnetosphäre, welche die darunterliegende Biosphäre dadurch vor dem Sonnenwind schützt. Dabei wird die Magnetosphäre durch den unsteten Sonnenwind fortlaufend bewegt. Durch die Bewegung des Magnetfeldes gegenüber den geladenen Teilchen darin werden Ströme induziert. Die größten Energiefreisetzungen geschehen durch magnetische Rekonnexionen im Schweifbereich der irdischen Magnetosphäre. Innerhalb der irdischen Magnetosphäre findet sich daher ein komplexes System bewegter elektrischer Ladungen, die sich in teils großen, weltumspannenden Strömen wie dem Ringstrom, den Birkelandströmen, den Pedersenströmen und dem polaren Elektrojet um die Erde bewegen. Wenn die Plasmateilchen bis in die Atmosphäre herunterströmen, regen sie bei Kollisionen die verdünnten Gase in hohen Schichten der Atmosphäre an. Diese emittieren beim Abfallen der Erregung ein Fluoreszenzlicht.

Polarlichter treten hauptsächlich in den Polarregionen auf, wo die Feldlinien die Atmosphäre durchdringen.
Sie kommen sowohl in nördlichen Breiten ("Nordlichter", auch "Aurora borealis") als auch auf der Südhalbkugel vor ("Südlichter", auch "Aurora australis").

Auch auf anderen Planeten des Sonnensystems werden diese Erscheinungen beobachtet. Voraussetzung hierfür ist, dass der Planet ein eigenes Magnetfeld und eine Atmosphäre besitzt. 2015 konnten Astronomen erstmals Polarlichter außerhalb des Sonnensystems beobachten. Die an dem 18 Lichtjahre entfernten Stern LSR J1835+3259 mit geringer Masse beobachteten Aktivitäten waren etwa 10.000 mal stärker als Polarlichter auf dem Jupiter. 

Auch Kernwaffentests in hohen Atmosphären-Schichten (400 km) rufen solche Phänomene hervor, wie beispielsweise der Starfish-Prime-Test der USA am 9. Juli 1962.

Die Häufigkeit der Polarlichterscheinungen in den mittleren Breiten (Mitteleuropa) hängt von der Sonnenaktivität ab. Die Sonne durchläuft einen Aktivitätszyklus (Sonnenfleckenzyklus), der vom Anfang (solares Minimum) über die Mitte (solares Maximum) bis zum Ende (erneutes Minimum) im Durchschnitt elf Jahre dauert. Mit diesem Zyklus schwankt auch die Häufigkeit von Polarlichtern. Insbesondere während des Aktivitätsmaximums (auch Solarmax genannt; zuletzt aufgetreten 2013/2014) finden starke Eruptionen auf der Sonne besonders häufig statt. Die großen koronalen Massenauswürfe sind für Polarlichter in Mitteleuropa essentiell. In frühen und späten Phasen des Sonnenzyklus, nahe am solaren Minimum, treten viel weniger dieser Eruptionen auf und somit gibt es auch eine deutlich geringere Wahrscheinlichkeit für Polarlichter. Dennoch können auch im abfallenden und ansteigenden Sonnenzyklus starke Ereignisse beobachtet werden. 

So wurden unter anderem im Herbst 2003 Polarlichter in Griechenland und auch auf den Kanarischen Inseln gesichtet. Im Mittel können während der Phase des solaren Maximums im deutschsprachigen Raum etwa 10 bis 20 dieser Leuchterscheinungen pro Jahr beobachtet werden. Im Allgemeinen sind sie am Nordhimmel zu sehen, nur bei besonders starkem Sonnenwind können sie auch in südlicher Richtung auftreten. Durch erdgebundene, visuelle Sonnenbeobachtung können Polarlichter kurzfristig vorhergesagt werden. Besser gelingt dies aber durch das Hinzuziehen von frei verfügbaren Daten der diversen Weltraummissionen von ESA und NASA zur Erforschung der Sonne und des Sonnenwindes.
Da der Sonnenwind zwei bis vier Tage von der Sonne bis zur Erde unterwegs ist, kann in diesem Zeitabstand nach einer starken, erdgerichteten Sonneneruption mit Polarlichtern gerechnet werden.

Die statistische Ableitung, dass Polarlichter hauptsächlich im Herbst/frühen Winter, von Ende Oktober bis Mitte Dezember, sowie im späten Winter/Frühjahr, von Ende Februar bis Anfang April, auftreten, ist nicht absolut gesichert. Hier stehen zwar die Magnetfelder von Erde und Sonne besonders günstig zueinander, aber dieser Effekt ist aufgrund der geringen Neigung der Erdachse gegen die Ekliptik eher zu vernachlässigen. Wahrscheinlicher ist es, dass besonders in den kältesten Winternächten die Beobachtungen aufgrund des Wetters sehr rar sind. Ähnliches gilt für die fehlenden Sommerbeobachtungen, denn zu dieser Jahreszeit herrscht im Norden die Mitternachtssonne und macht Polarlichtsichtungen praktisch unmöglich. Im Allgemeinen kann man sagen, dass Polarlichter mit zunehmender Distanz zum jeweiligen Pol, etwa von Deutschland, Österreich, der Schweiz und Italien aus, meist nur während des Aktivitätsmaximums der Sonne beobachtet werden können, was relativ selten ist. Mittels moderner Digitalkameras kann man jedoch auch während der weniger aktiven Phasen des Sonnenzyklus noch einzelne Ereignisse – von Mitteleuropa aus – dokumentieren.

Die Intensität der Polarlichter stieg seit 2007 wieder an und hatte 2013/2014 ihren letzten Höhepunkt. Laut der amerikanischen Luft- und Raumfahrtbehörde NASA sollten es die stärksten Polarlichter seit 50 Jahren werden. Tatsächlich fiel der aktuelle Sonnenfleckenzyklus aber schwächer als seine Vorgänger aus, wodurch die Polarlichtaktivität eher verhalten ist.

Polarlichter können verschiedene Farben haben. Grünes Licht (557,7 Nanometer Wellenlänge) entsteht durch Sauerstoffatome, die in gut 100 km Höhe angeregt werden und während ihrer angeregten Zeit auf andere Teilchen treffen. Ohne Zusammenstoß emittieren Sauerstoffatome rotes Licht (630 Nanometer Wellenlänge), was hauptsächlich in der dünneren Atmosphäre in höheren Schichten in etwa 200 km Höhe auftritt. Angeregte Stickstoffatome senden auch violettes bis blaues Licht (428 Nanometer) aus. Zur Anregung von Stickstoffatomen sind jedoch sehr hohe Energien notwendig, deshalb lassen sich diese Farben nur bei starken magnetosphärischen Störungen beobachten. Wegen der hohen Empfindlichkeit des Auges für grünes Licht und der relativ hohen Konzentration von Sauerstoff werden grüne Polarlichter am häufigsten beobachtet.
Außerdem existieren jeweils unsichtbare Polarlichter im Langwellen-, Ultraviolett- und Röntgen-Bereich sowie das schwarze Polarlicht, das sozusagen ein Anti-Polarlicht darstellt.

Da der Sonnenwind außerhalb der Polarregionen nur selten tief in die Atmosphäre eindringen kann, sind Polarlichter in der gemäßigten Zone, also auch in Europa, meistens rot.

Das menschliche Auge nimmt Farben in der Dunkelheit nur begrenzt wahr, die Farbwahrnehmung von Polarlichtern ist oft individuell unterschiedlich.

Es treten vier verschiedene Arten von Polarlichtern auf, welche abhängig von den Sonnenwinden sind. Diese sind: Corona, Vorhänge, ruhige Bögen und Bänder.
Wissenschaftlich werden sie gemäß der Vallance-Jones Classification unterteilt:

Der möglicherweise früheste datierbare Bericht über Polarlichter findet sich in einer über 2500 Jahre alten babylonischen Keilschrift. Sie berichtet von einem ungewöhnlichen roten Leuchten am Nachthimmel, das präzise auf die Nacht vom 12. auf den 13. März 567 v. Chr. datiert ist.

Vor oder außerhalb der wissenschaftlichen Erklärungen bestanden zahlreiche Legenden und Sagen. Die Völker in Lappland, Sibirien und Alaska glaubten hier Zeichen ihrer Götter zu sehen, die auf diese Weise mit ihnen in Verbindung treten wollten. Die Zeichen galten oft als Vorboten für schlimme Zeiten wie Krieg, Pest, Hungersnot. Ebenfalls ein nahendes Unglück vermuteten die Samen, Skandinaviens Ureinwohner, besonders beim Aufleuchten roter Farbschleier. In ihrem Fall wurden allerdings die Toten als Auslöser vermutet, man verbot den Kindern auf die Straße zu gehen und wartete schweigend darauf, dass sich die Toten wieder zur Ruhe legten. Ebenfalls an Aktivitäten der Geister ihrer Verstorbenen glaubten die Eskimos. Diese gingen jedoch davon aus, dass das Leuchten durch einen hin und her geworfenen Walross-Schädel beim Ballspielen entstand. Ebenfalls bei den Eskimos fand sich die Auffassung, die Lichter seien eine Brücke ins Jenseits, die von Fackeln der Toten beleuchtet wird, um frisch Gestorbenen Orientierung zu bieten. Die Indianer in Kanada meinten, dass sich ihr leuchtend in Erscheinung tretender Gott über das Wohlbefinden seiner Stämme vergewissern wollte. Eine kleine Geschichte diente den Wikingern als Erklärung. Sie sahen in den Polarlichtern das Zeichen, dass irgendwo auf der Welt eine große Schlacht geschlagen worden war. Nach ihrer Vorstellung ritten die Walküren nach jedem Gefecht über den Himmel und wählten die Helden aus, die fortan an Odins Tafel speisen sollten. Dabei spiegelte sich das Licht des Mondes auf ihren schimmernden Rüstungen und die bunten Nordlichter entstanden. Besonders im Mittelalter galten in Europa Polarlichter, ähnlich wie Kometen, als Vorboten kommenden Unheils (zum Beispiel Kriege, Seuchen, Hungersnöte). Mitteleuropäische Christen sollen im Mittelalter darin Vorzeichen der Apokalypse gesehen haben, was mit der in ihren Breiten häufigsten feuerroten Erscheinung zusammenhängen dürfte. Für die Maori auf der Südhalbkugel galt das in Neuseeland äußerst seltene Südlicht als Feuer, das die Ahnen auf ihrem Weg in Richtung Antarktis entzündet hatten, um sich an die warmen Tage in Neuseeland zu erinnern.

Im 18. Jahrhundert wurden dann die ersten Versuche unternommen, die Entstehung von Polarlichtern wissenschaftlich zu erklären. Die Forscher gingen zunächst davon aus, dass es sich bei den Polarlichtern um Reflexionen von Sonnenlicht an Wolken oder Eiskristallen handle. Erst einige Zeit später erkannte der englische Astronom und Mathematiker Edmond Halley – wahrscheinlich als erster – den Zusammenhang zwischen dem Erdmagnetfeld und Polarlichtern. Das Leuchten konnte er aber nicht erklären. Dieses gelang erst 1867 dem schwedischen Astronom und Physiker Anders Jonas Ångström, der zeigen konnte, dass es sich bei den Polarlichtern um selbstleuchtende Gase handelt. Eine Theorie für die Ursache des Leuchtens stellte der norwegische Physiker Kristian Birkeland im Jahre 1896 auf: Er ging davon aus, dass Elektronen der Sonne das Gasgemisch der oberen Atmosphäre zum Leuchten anregen. Da die Existenz des Sonnenwindes zu dieser Zeit aber noch nicht bekannt war – dies wurde erst 1959 durch die sowjetische Sonde Lunik 1 nachgewiesen –, wurde seine Theorie jedoch häufig bezweifelt. Obwohl die Entstehung des Lichtes heute nicht mehr umstritten ist, ist noch nicht vollständig geklärt, warum an bestimmten Orten Polarlicht zu beobachten ist.

Die ersten Fotografien des Nordlichts gelangen Martin Brendel und Otto Baschin am 1. Februar 1892.

Eine viele Jahrzehnte lang aufrechterhaltene und immer noch verbreitete moderne Theorie besagt, Polarlichter entstünden durch das direkte Auftreffen geladener Teilchen des Sonnenwindes auf die Erdatmosphäre. Sonnenwindpartikel würden dabei längs der Erdmagnetfeldlinien trichterartig auf die polnahe Atmosphäre geleitet, wo die Feldlinien annähernd senkrecht die Atmosphäre durchdringen. Diese Theorie muss zumindest als stark vereinfacht angesehen werden, da sie die komplexen Prozesse zwischen der Aufnahme der Energie des Sonnenwindes bis zur Entstehung der Leuchterscheinungen unterschlägt.

In der Wissenschaft war spätestens seit Burritt (1845) der Zusammenhang zwischen den Leuchterscheinungen und Aktivitäten im Erdmagnetfeld bekannt.

Im 19. Jahrhundert wurde die Verbindung mit den Sonnenflecken bekannt.

Mitte des 19. Jahrhunderts wurde in den Anfangsjahren der Telegraphie induzierte Impulse auf den Telegraphenleitungen beobachtet.

Anfang des 20. Jahrhunderts demonstrierte Birkeland die Plausibilität einer Erklärung über Elektrizität, Plasma und die Sonnenaktivität.

Ende der 1950er Jahre wurden mit der ersten Forschungsrakete von Churchill Elektronenströme in der Atmosphäre nachgewiesen.

Die energiereichen, elektrisch geladenen Teilchen des Sonnenwindes, die für die Entstehung von Polarlichtern verantwortlich sind, erzeugen elektromagnetische Felder, die schädigende Auswirkungen auf elektronische Einrichtungen ausüben können. Gefährdet sind insbesondere Satelliten sowie Flugzeuge. Zur Sicherheit wird daher zu Zeiten erhöhter Polarlichtaktivität im Flugverkehr in geringerer Höhe geflogen oder es werden Flugrouten gewählt, die abseits der Polarregionen liegen.

Zudem kann es in Stromnetzen durch Induktionen zu Spannungsschwankungen kommen. So wurde beispielsweise der Stromausfall in Kanada im Jahre 1989 auf einen starken Sonnenwind zurückgeführt.

Während des Auftretens von Polarlichtern werden durch Teilreflexion auch Funkwellen oberhalb des Kurzwellenbereiches an den ionisierten Bereichen der Atmosphäre (Ionosphäre) reflektiert. Funkamateure nutzen diesen Effekt im Amateurfunkdienst, um die Reichweite ihrer Signale zu erhöhen. Da aber die reflektierten Signale in den Sender- und Empfängergeräten den Funkverkehr stören, werden die Verbindungen oft in der Betriebsart Morsetelegrafie (CW, A1A) aufgebaut.

In Science-Fiction-Romanen und -Filmen wird das Phänomen der Polarlichter hin und wieder als Begleiterscheinung übernatürlicher Ereignisse als besonderer Effekt eingesetzt; so etwa in der Novelle Langoliers von Stephen King und im Film Frequency. Sie sind ein zentrales Thema des ersten Teils der His-Dark-Materials-Trilogie von Philip Pullman.




</doc>
<doc id="11929" url="https://de.wikipedia.org/wiki?curid=11929" title="Widerstand">
Widerstand

Widerstand steht für:

Elektrotechnik, Physik, Physiologie und Maschinenbau:

Soziologie und Politologie:

Jura:

Sonstiges:

Siehe auch:


</doc>
<doc id="11930" url="https://de.wikipedia.org/wiki?curid=11930" title="Benvenuto Cellini">
Benvenuto Cellini

Benvenuto Cellini (* 3. November 1500 in Florenz; † 13. Februar 1571 ebenda) war als italienischer Goldschmied und Bildhauer ein berühmter Vertreter des Manierismus.

Benvenuto Cellini gilt als einer der großen Bildhauer der Nachantike und als ein typischer „uomo universale“ der italienischen Renaissance. Nachdem sein Werk mehrere hundert Jahre nahezu vergessen war, wurde es zu Beginn des 19. Jahrhunderts neu entdeckt. Er wirkte an der Schwelle der Hochrenaissance zum Manierismus als Bildhauer, Goldschmied, Medailleur, aber auch als Schriftsteller und Musiker.

Benvenuto Cellini wurde am 3. November 1500 in Florenz als Sohn des Baumeisters und Musikers Giovanni Cellini und seiner Frau Maria Elisabetta di Stefano Granacci im 21. Jahr ihrer Ehe geboren.
Sein Vater war Architekt von Verteidigungsanlagen im Dienste der Medici. Darüber hinaus arbeitete er in kunsthandwerklichen Bereichen und fertigte auch Musikinstrumente an. Seinem Willen gemäß sollte Benvenuto Musiker werden; dieser entschied sich aber mit 14 Jahren für das Goldschmiedehandwerk. 

Nur mit Mühe erreichte Benvenuto, dass ihn sein Vater in die Lehre zu Michelangelo da Viviano, dem Vater seines späteren Erzrivalen Baccio Bandinelli gab. Als ihn der Vater jedoch bald darauf wieder zurücknahm, riss er aus und ging in die Werkstatt von Antonio di Sandro, wo er rasch Fortschritte machte. 1516 wurde er wegen einer Schlägerei für sechs Monate aus Florenz verbannt, die er bei dem Meister Francesco Castoro in Siena verbrachte. Auf Geheiß von Giulio de' Medici, dem späteren Papst Clemens VII., durfte er jedoch zurückkehren. Es trieb ihn aber bald wieder in die Fremde. In Bologna arbeitete er für kurze Zeit bei Meister Ercole, dann für den Miniaturmaler Scipio Cavaletti. Anschließend war er wieder in Florenz, das er jedoch wegen eines Streits mit seinem Bruder bald wieder verließ, um planlos über Lucca nach Pisa zu wandern, wo er ein Jahr lang unter dem Meister Ullivieri della Chiostra "in Gold und Silber schöne und bedeutende Sachen" arbeitete, bevor er nach Florenz zurückkehrte.

Das Musizieren, das er bis dahin seinem Vater zuliebe weiter betrieben hatte, war ihm nun restlos zuwider; es kam zu einem Streit mit dem Vater, in dessen Folge er Florenz erneut verließ und nach Rom in die Werkstatt von Firenzuola di Lombardia ging. Er kehrte aber auf Bitten seines Vaters nach zwei Jahren zurück und nahm auch das Musizieren wieder auf. Streitigkeiten mit Kollegen arteten unterdessen immer häufiger in Schlägereien aus, in deren Folge Cellini – als Mönch verkleidet – aus Florenz fliehen musste. 

Er ging wieder (etwa November 1523) nach Rom, wo gerade sein Gönner Giulio de' Medici als Papst Clemens VII. inthronisiert wurde.

In der Werkstatt des aus Mailand stammenden Giovanpiero della Tacca begann Cellini einen „großen Wasserkessel“, den der Maler Gioanfrancesco Penni für den Bischof von Salamanca entworfen hatte. Die zügige Ausführung des Auftrags wurde aber behindert, nachdem Cellini, durch seinen ihm flehend und drohend im Traum erschienenen Vater bewogen, als Musiker in päpstliche Dienste getreten war. Durch die Terminverzögerung der Ablieferung verärgert, weigerte sich der Bischof, nach dem Erhalt der Arbeit vertragsgemäß zu zahlen. Nachdem es Cellini gelungen war, sich wieder in den Besitz des Werkes zu bringen, verteidigte er die erneute Herausgabe mit Waffengewalt gegen die Dienerschaft des Bischofs. Dieser Coup machte Cellini in weiten Kreisen der römischen Gesellschaft bekannt. Die nun eintreffenden Aufträge ermöglichten es ihm, seine erste eigene Werkstatt zu eröffnen. Er begann, neben den Goldschmiedearbeiten auch als Siegelstecher zu arbeiten. 

In diese Zeit fällt auch das Zusammentreffen Cellinis mit der von ihm hochgeschätzten Porzia Chigi, der er eine Reihe von Aufträgen zu verdanken hatte, und die ihn darin bestärkte, seine eigene Werkstatt zu eröffnen. Im Hause der Chigi (heute Villa Farnesina) studierte er die dortigen Werke des Raffael, was nicht ohne Einfluss auf sein weiteres Schaffen geblieben sein dürfte. 

Der im Jahr 1525 in Rom ausgebrochenen Pest wich Cellini, wie viele Römer, durch Aufenthalt aufs Land aus. Die von den Bauern bei der Feldarbeit relativ häufig gefundenen antiken Medaillen, Gemmen und Edelsteine kaufte er für ein Weniges auf, um sie, zurück in Rom, gewinnbringend an kunstsinnige Kardinäle zu verkaufen.

Bei der Belagerung Roms im Jahr 1527 griff er als treuer Anhänger der Medici ebenfalls zu den Waffen und übernahm die Aufsicht einiger Geschütze auf der Engelsburg. Im Auftrag des Papstes zerstörte er ohne jeden Skrupel eine Reihe wertvoller Kunstgegenstände der Goldschmiedekunst im Werte von insgesamt 200 Pfund Gold, um sie nicht den Belagerern in die Hände fallen zu lassen. Einen Teil des Schatzes veruntreute er dabei.

Nach dem Ende der Belagerung kehrte er als Capitano nach Florenz zurück, um bald darauf weiter nach Mantua zu reisen, um dort für kurze Zeit in die Dienste des Mailänders Niccolo, Goldschmied des Herzogs zu treten. Vier Monate später war er wieder in Florenz, wo gerade die Pest gewütet hatte. Er fand nur noch einen Bruder und eine jüngere Schwester am Leben. Diesen zuliebe blieb er einige Zeit daheim und erwarb sein Geld hauptsächlich durch das Fassen von Juwelen. 

Im Auftrag des Girolamo Mazzeti arbeitete er eine „goldene Medaille am Hut zu tragen, worauf in ganz erhabenem Relief ein Herkules, der dem Löwen den Rachen aufreißt“. Diese Arbeit gewinnt den Beifall Michelangelo Buonarrotis. Durch dieses Lob des damals schon „vergöttlichten“ Künstlers stieg sein Geltungsbedürfnis enorm an und sein Verlangen nach größer dimensionierten Arbeiten wuchs. Eine weitere Medaille aus dieser Zeit für Federico Ginori gelangte später in den Besitz Franz’ I. von Frankreich und führte in der Folge zu seiner Berufung an den Französischen Hof.

Von Florenz ging Cellini wieder nach Rom und arbeitete dort vorübergehend bei dem Goldschmied Raffaelo del Moro. Durch günstige Auftragslage konnte er schon bald wieder eine eigene Werkstatt eröffnen, in der fünf Gesellen für ihn arbeiteten. Sein Bruder Francesco, der sich als Soldat in Rom aufhielt, wurde dort in dieser Zeit in einem Handgemenge auf offener Straße erschossen. Cellinis anschließender Mord am Mörder seines Bruders wird vom Papst „mit grimmigem Seitenblick“ bestraft. 

Nach einem nächtlichen Einbruch in seine Werkstatt, bei dem eine Reihe von Schmuckstücken und Prägestempeln gestohlen wurden, kamen mit seinem Stempel geschlagene gefälschte Münzen in den Umlauf. Cellini geriet in den Verdacht der Falschmünzerei, gegen den der Papst ihn in Schutz nahm. Nach der Entdeckung der Täter sah sich der Papst in seinem Vertrauen zu ihm bestärkt und schenkte ihm die einträgliche Stelle eines päpstlichen Leibtrabanten. Dieser hierarchische Aufstieg führte bei Cellini zu einer Reihe von Überheblichkeiten und Streitereien, bis er sich zuletzt einer Übermacht an Feinden gegenübersah. 

Als Cellini auf offener Straße einen seiner Gegner durch einen Steinwurf schwer verletzte, musste er nach Neapel fliehen und arbeitete dort für kurze Zeit in der Werkstatt des Goldschmieds Domenico Fontana, kehrte jedoch bald heimlich nach Rom zurück, wo er die Genesung seines Opfers abwartete.

Papst Clemens VII. starb 1534. Ihm folgte Paul III. aus dem Hause Farnese. Bei passender Gelegenheit tötete Cellini Pompeo de´ Capianeis, den Anstifter seiner Verfolgung. Statt dafür vom Papst bestraft zu werden, erhielt er von diesem einen Freibrief. Durch diesen gesichert, arbeitete er zunächst die Stempel für die Ausgabe neuer Scudi anlässlich der Wahl Pauls III. Die Familie des ermordeten Pompeo suchte unterdessen, nachdem sie kein Recht fand, Gleiches mit Gleichem zu vergelten. Den gedungenen Mördern entkam Cellini nach Florenz. 

Ein erneuter Freibrief des Papstes ermöglichte ihm die Rückkehr nach Rom. Dort wurde er eines Nachts überfallen, rettete sich nicht zuletzt durch den Freibrief. Darauf erkrankte er so lebensbedrohend, dass mit seinem Ableben gerechnet wurde. Obwohl bereits Sonette auf seinen Tod verfasst waren, erholte er sich wie durch ein Wunder. Darauf suchte er völlige Heilung im Hause seiner Schwester in Florenz.

Cellini beging laut eigener Aussage drei Morde. Der dritte geschah an einem Postbeamten in Siena. Mehrmals in seinem Leben stand er vor Gericht. Einmal wurde er zum Tode verurteilt. Außer Delikten wegen Körperverletzung stand Cellini mehrfach auch wegen Diebstahls und als abartig angesehener sexueller Praktiken vor Gericht.

Im Jahr 1535 reiste er über Padua, Venedig, den Bernina- und Albulapass in die Schweiz und von dort aus nach Lyon und weiter nach Paris. Die Reise verlief jedoch ohne den erhofften Erfolg. Ein Treffen mit König Franz I. kam nicht zustande und so reiste er über den Simplonpass zurück. Nachdem er unterwegs dem Herzog von Ferrara seine Aufwartung gemacht und in Loreto für seine Genesung gedankt hatte, ging er weiter nach Rom und eröffnete wieder eine Werkstatt mit zwölf Gesellen.

Dort holte ihn seine Vergangenheit ein. Er wurde auf Grund eines von seinen Feinden lancierten Gerüchts verhaftet, demzufolge er beim Einschmelzen des päpstlichen Schatzes wertvolle Edelsteine entwendet hätte. Zwei Jahre blieb er ohne Anklage in der Engelsburg inhaftiert. Ein Fluchtversuch führte zu noch strengerer Haft. Es wurden Versuche unternommen, ihn zu vergiften. Erst nach Intervention des Kardinals von Ferrara, Ippolito d’Este, kam Cellini wieder frei; dieser hatte den Auftrag, ihm die Einladung an den französischen Hof Franz’ I. zu überbringen. Am 22. März des folgenden Jahres brach Cellini zum zweiten Mal nach Frankreich auf.

In Fontainebleau traf er zum zweiten Mal König Franz I., der ihm große Aufträge in Aussicht stellte. Aber schon bald machte sich bei Cellini Unmut breit: Die Aufträge blieben aus, die in Aussicht gestellte Bezahlung empfand er als unangemessen. So plante er, wie schon während der Gefangenschaft erwogen, eine Wallfahrt nach Jerusalem zu unternehmen. Eines Morgens reiste er unvermittelt ab, wurde jedoch ergriffen und wieder zurückgebracht. Der Ausreißversuch hatte jedoch insofern Erfolg, als dass der König ihn umgehend mit dem Auftrag für zwölf lebensgroße silberne Statuen bedachte. Er erhielt dazu als Atelier ein kleines Schloss in unmittelbarer Nähe des Louvre, das er erst nach wiederholten Streitigkeiten mit den dortigen Bewohnern beziehen konnte.

Die Arbeiten an den Figuren stießen auf das lebhafte Interesse des Königs, der ihn auch überraschend in seiner Werkstatt besuchte. Bei einem dieser Besuche erging der Auftrag des Königs zur Herstellung des Salzfasses, dessen Modell Cellini bereits in Rom für Hippolyt d’Este erstellt hatte. Die Protektion des Königs führte auch zu einer Flut von Aufträgen für die Werkstatt von außerhalb. Es entstanden zahlreiche Werke in Edelmetallen und in Bronze. Auch hier schaffte sich Cellini in kürzester Zeit einflussreiche Feinde, an erster Stelle die Geliebte des Königs, Madame d’Étampes.

Im Sommer 1545 erbat Cellini Urlaub, der ihm nach anfänglichen Ablehnungen endlich gewährt wurde. Er reiste nach Italien. Ende Juli erreichte er Florenz und wurde bei Herzog Cosimo vorstellig, der ihn ersuchte, in seine Dienste einzutreten. Die von ihm in Aussicht gestellte Skulptur des Perseus, mit der Cellini hoffte, in Konkurrenz zu seinen großen Vorbildern Michelangelo und Donatello treten zu können, reizte ihn so sehr, dass er das Angebot annahm. Franz I. beschuldigte ihn darauf des Undanks, verlangte eine genaue Abrechnung und verzichtete auf dessen Rückkehr. 

In Florenz wurde ihm vom Herzog im Jahre 1545 ein Haus geschenkt, das er bis zu seinem Lebensende 1571 bewohnte und in dessen Garten unter abenteuerlichen Bedingungen der Perseus gegossen wurde. Die Vollendung des Perseus nahm wegen immer wieder auftretender Schwierigkeiten acht Jahre in Anspruch. Daneben entstanden weiterhin vereinzelt Goldschmiedearbeiten, die er mit seinen Gehilfen vor allem für den herzoglichen Hof ausführte. Er selber sah sich von nun an eher als Bildhauer denn als Goldschmied.

Nach einigen erfolgreichen Güssen an einer Büste des Herzogs und vier kleineren Statuen für den Perseus-Sockel goss er die dem Perseus zu Füßen liegende enthauptete Medusa. Nachdem ihm dieser Guss zur absoluten Befriedigung gelungen war, schritt Cellini endlich zur Tat und bereitete den Guss des Perseus vor, der wie die antiken Bronzen in einem Stück vollführt werden musste. Die Schilderung des Unternehmens fand breiten Platz in der Vita – wie nach dem Anstoß das Gebälk des Daches Feuer fing, Regen und Sturm hereindrangen, endlich der glühende Ofen platzte. Nach zwei Tagen zeigte sich jedoch, dass der Guss bis auf eine kleine Stelle am rechten Fuß vorzüglich gelungen war. Dem Herzog überbrachte er diese Nachricht zugleich mit der Bitte um Urlaub. Der wurde ihm gewährt und Cellini reiste nach Rom.

Seine Bemühungen dort galten einer Anstellung unter Papst Julius III., die ihm aber verwehrt blieb. Zurück in Florenz arbeitete er an der Errichtung einiger Festungstore, bevor er wieder die Arbeit an der Basis des Perseus aufnahm. 1554 wurde der Perseus in der "Loggia dei Lanzi" aufgestellt, wo er sich noch heute befindet. Nachdem Cellini die vier Figuren für den Perseus-Sockel nicht der Herzogin überlassen wollte, die diese für ihre Privatsammlung begehrte, hatte er allerdings auch seinen Kredit in der Familie des Herzogs weitgehend verspielt, so dass keine größeren Aufträge mehr von dort zu erwarten waren. Zu allem Überfluss konnten sich die Parteien auch nicht auf den Preis der Monumentalskulptur einigen. Cellini verlangte 10.000 Scudi, der Herzog bezahlte dagegen nur 3.500 Scudi. 

Der Perseus war Cellinis größtes Werk. Noch im Jahr der Enthüllung wurde Cellini feierlich in den florentinischen Adel aufgenommen, ungeachtet der Tatsache, dass er zu dieser Zeit im Ansehen des Herzogs tief gesunken war, ohne allerdings gänzlich in Ungnade zu fallen.

Die ewigen Streitereien und die letztlich unerfüllte Geltungssucht zeigten immer deutlichere Spuren bei Cellini. So entschloss er sich zum Wechsel in den geistlichen Stand und nahm auch 1558 die Tonsur. An seiner Streitsucht änderte das aber nicht viel. Die letzten Jahre gestalteten sich für ihn trübe. Er hatte Unglück mit denen, die sein Geld verwalteten, und wurde in Prozesse verwickelt; da er nicht oder nur wenig arbeitete, blieben Einnahmen aus. 

Schon 1560 wurde er wieder von seinen Gelübden entbunden und heiratete im Jahre 1563 seine Haushälterin Piera di Salvadore Parigi, mit der er bereits einen unehelichen Sohn hatte, der aber bereits 1559 gestorben war. Der Ehe des Dreiundsechzigjährigen entsprossen drei Kinder, zwei Töchter und ein Sohn, die ihn überlebten, während seine zahlreichen außerehelichen Kinder zumeist in früher Jugend starben. 

Ungeachtet dessen sah sich Cellini mehrfach mit dem Vorwurf der Homosexualität, insbesondere von Seiten Baccio Bandinellis, konfrontiert. Auch die Kunstgeschichte bringt die Formensprache seiner Skulpturen häufig mit Homoerotik in Verbindung. Er wurde viermal der Sodomie angeklagt. 

Nach den letzten Jahren großer finanzieller Schwierigkeiten, aus denen zahlreiche Bittbriefe erhalten sind, starb Cellini am 13. Februar 1571 in Florenz an einer Brustfellentzündung, an der er seit längerer Zeit litt.


Zu seinen bekanntesten Werken gehört die Saliera, welche er von 1540 bis 1543 in Paris für den französischen König Franz I. anfertigte. Das Salzfass wurde 2003 aus dem Wiener Kunsthistorischen Museum gestohlen, jedoch von der österreichischen Polizei im Januar 2006 wieder sichergestellt.

Cellinis bekanntestes bildhauerisches Werk ist das Bronzestandbild "Perseus mit dem Haupt der Medusa" von 1554. Das überlebensgroße Standbild wurde in der Florentiner Loggia dei Lanzi aufgestellt. Cellini gestaltete dafür zusätzlich den Sockel aus Marmor mit vier Bronzestatuetten von Jupiter, Merkur, Minerva und Perseus’ Mutter Danaë. Später wurde in den Sockelfuß zusätzlich ein Bronzerelief Cellinis eingelassen, das die Befreiung der Andromeda darstellt, welches sich heute im Bargello befindet und durch einen Nachguss am Sockel ersetzt wurde. Von Dezember 1996 bis Juni 2000 wurde die Statue in den Werkstätten der Uffizien aufwändig restauriert.

Ein drittes erhaltenes Werk ist eine überlebensgroße Bronzebüste, die Cosimo I. darstellt. Diese schuf Cellini im Wettbewerb mit dem Florentiner Bildhauer Baccio Bandinelli. Sie wurde 1548 fertig, stand bis 1557 im Palazzo Vecchio (bzw. Palazzo della Signoria), wurde dann nach Cosmopoli auf Elba „verbannt“ und gelangte erst im 18. Jahrhundert in den Bestand der Florentiner Skulpturengalerie, den Bargello. 

Das Salzfass, der Perseus, die Büste Cosimos sowie das Kruzifix sind wichtige Beispiele für den Florentiner Manierismus in der Skulptur. Dies manifestiert sich in ihrer üppigen Ausstattung, der komplexen Ikonographie und der Aufnahme neuester Strömungen in der Skulptur, wie der Forderung nach der Vielansichtigkeit und der „figura serpentinata“.

Um das Jahr 1557 begann Cellini mit der Niederschrift seiner Autobiografie und setzte diese Arbeit ungefähr ein Jahrzehnt hindurch fort. Dann vernichtete er den Teil seiner Niederschrift, der die Zeit im Dienste des Herzogs Cosimo betraf, aus Furcht vor dem möglichen Zorn des Herzogs, des späteren ersten Großherzogs von Toskana, und führte sein Vorhaben von da ab nicht mehr weiter. Die Autobiografie endet so, ziemlich abrupt, mit dem November 1566. Erst im Jahr 1728 wurde nach der Vorlage einer handschriftlichen Kopie durch den Florentiner Antonio Cocchi eine Druckausgabe erstellt. Johann Wolfgang von Goethe publizierte das Buch 1798 auf Deutsch unter dem Titel "Leben des Benvenuto Cellini". Seine Ausgabe ist eine sehr freie, ungenaue Übersetzung mit Auslassungen. Das Gewicht liegt bei ihm auf dem literarischen Interesse an der Figur des Autors. Das Buch fand dann auch folgerichtig Aufnahme in seine gesammelten Werke. 1830 folgte in Florenz eine weitere Ausgabe, der diesmal die wiederaufgefundene Originalhandschrift zugrunde lag. Eine der aktuellen Übersetzungen der Originalhandschrift stammt von Jacques Laager. 

Cellini verfasste außerdem zwei Traktate über die Goldschmiedekunst und die Skulptur "Trattati dell’ Oreficeria e della Scultura di Benvenuto Cellini", die bereits zu seinen Lebzeiten – im Jahr 1568 – veröffentlicht wurden. In seinen letzten Lebensjahren verfasste er zudem einige kurze Abhandlungen über die Baukunst.





</doc>
<doc id="11931" url="https://de.wikipedia.org/wiki?curid=11931" title="Arthur Schnitzler">
Arthur Schnitzler

Arthur Schnitzler (* 15. Mai 1862 in Wien, Kaisertum Österreich; † 21. Oktober 1931 ebenda, Republik Österreich) war ein österreichischer Arzt, Erzähler und Dramatiker. Er gilt als Schriftsteller als einer der bedeutendsten Vertreter der Wiener Moderne.

Arthur Schnitzler kam als erster Sohn der insgesamt vier Kinder des jüdischen Medizinprofessors und Laryngologen Johann Schnitzler (1835–1893) und dessen Gattin Luise geb. Markbreiter (1840–1911) in der Praterstraße 16 (2. Bezirk, Leopoldstadt) sehr nahe am historischen Stadtzentrum Wiens zur Welt.

Von 1871 bis 1879 besuchte er das Akademische Gymnasium im 1. Bezirk und legte am 8. Juli 1879 die Matura mit Auszeichnung ab. Danach studierte er an der Universität Wien Medizin. Am 30. Mai 1885 wurde er zum Dr. med. promoviert. Sein jüngerer Bruder Julius (1865–1939) wurde ebenfalls Arzt.

Arthur Schnitzler begann früh, als Verfasser literarischer Texte und Gedichte aufzutreten. Sein literarisches Debüt gab er mit "Liebeslied der Ballerine" 1880 in der Zeitschrift "Der freie Landbote" und veröffentlichte in der Folge Gedichte und Erzählungen u. a. auch in "Blaue Donau", "Moderne Dichtung", "Frankfurter Zeitung" und "Freie Bühne".

1885 bis 1888 arbeitete er als Assistenz- und Sekundararzt am Allgemeinen Krankenhaus der Stadt Wien in der Inneren Medizin als auch auf dem Gebiet der Psychiatrie und Dermatologie. Danach war er bis 1893 Assistent seines Vaters an der laryngologischen Abteilung der Poliklinik in Wien. Von 1886 bis 1893 publizierte Schnitzler zu medizinischen Themen und verfasste mehr als 70 Beiträge, meist Rezensionen von Fachbüchern, unter anderem als Redakteur der von seinem Vater gegründeten "Internationalen Klinischen Rundschau". Er verfasste eine (einzige) wissenschaftliche Veröffentlichung: "Über funktionelle Aphonie und deren Behandlung durch Hypnose und Suggestion" (1889).

Ab 1890 war Schnitzler mit seinen Freunden Hugo von Hofmannsthal, Hermann Bahr und Richard Beer-Hofmann einer der Hauptvertreter des Jungen Wien, der literarischen Wiener Moderne, deren bevorzugter Treffpunkt das Café Griensteidl im Stadtzentrum war. Schnitzler besuchte aber auch gerne das Restaurant Leidinger in der Kärntner Straße 61 und war auch mit Sigmund Freud bekannt.
Nach dem Tod seines Vaters, 1893, verließ er die Klinik und eröffnete seine eigene Praxis, zuerst am Burgring 1 (1. Bezirk, Innere Stadt), dann in der Frankgasse 1 (9. Bezirk, Alsergrund, unmittelbar bei der großen Votivkirche). Am 1895 posthum erschienenen "Klinischen Atlas der Laryngologie" seines Vaters hatte er noch mitgewirkt.

Um die Jahrhundertwende war er einer der bedeutendsten Kritiker der österreichisch-ungarischen k.u.k. Gesellschaft und ihrer Entwicklung. Nach der Veröffentlichung von "Leutnant Gustl", worin er den Ehrenkodex des Militärs angriff, wurde ihm am 14. Juni 1901 der Offiziersrang als Oberarzt der Reserve aberkannt.

Danach war er nicht mehr ärztlich, sondern nurmehr als freier Schriftsteller in Wien tätig.

„Frauen spielten im Leben Arthur Schnitzlers immer eine zentrale Rolle. In der Zeit vor seiner Ehe … waren es einerseits Beziehungen zu Schauspielerinnen …, andererseits auch Liebschaften mit Mädchen aus der Vorstadt …, die ihn auch in seinem künstlerischen Schaffen beeinflussten.“ Von großer Bedeutung für sein Werk sind die mehrjährigen Beziehungen zu Marie Glümer und Maria Reinhard. Schnitzlers und Maria Reinhards gemeinsames Kind wurde 1897 geboren und soll bei der Geburt gestorben sein.

Am 9. August 1902 brachte die Schauspielerin Olga Gussmann (1882–1970) den gemeinsamen Sohn Heinrich Schnitzler zur Welt. Am 26. August 1903 heiratete das Paar. Am 13. September 1909 wurde Tochter Lili geboren.
1910 kaufte Schnitzler von Hedwig Bleibtreu die Villa in Wien 18., Sternwartestraße 71, im Währinger Cottageviertel unweit der Universitätssternwarte Wien. In der Nähe wohnten Bekannte wie Richard Beer-Hofmann und Felix Salten. Zuvor hatte er in der nächsten Parallelstraße, damals Spöttelgasse 7 (seit 1918 Edmund-Weiß-Gasse), gewohnt.

Seit Anfang des 20. Jahrhunderts gehörte der Literat zu den meistgespielten Dramatikern auf deutschen Bühnen. Mit Beginn des Ersten Weltkrieges ging das Interesse an seinen Werken zurück. Dies hing auch damit zusammen, dass er sich als einer von wenigen österreichischen Intellektuellen nicht für die Kriegstreiberei begeistern konnte.

1919 begegnete er Hedy Kempny. Daraus entwickelte sich eine Freundschaft, die bis zu Schnitzlers Tod andauerte. 1921 wurde er von Olga Schnitzler geschieden, die Kinder Heinrich und Lili blieben bei ihm.
Von 1923 an war die verwitwete Schriftstellerin Clara Katharina Pollaczek geb. Loeb (1875–1951) Schnitzler in einer „romantischen Beziehung“ verbunden. Die beiden führten einen umfangreichen Briefwechsel; dokumentiert sind auch zahlreiche gemeinsame Kinobesuche.

1921 wurde ihm anlässlich der Uraufführung des Bühnenstücks "Reigen", die 1920 / 1921 in Berlin und dann in Wien zu einem inszenierten Theaterskandal führte, ein Prozess wegen Erregung öffentlichen Ärgernisses gemacht, der allerdings letztlich beim Wiener Verfassungsgerichtshof zu Gunsten des Autors ausging. Nach weiteren Aufführungen in Wien bat Schnitzler aber 1922 seinen Theaterverlag, weitere Aufführungen nicht mehr zuzulassen. (Das Verbot ließ sein Sohn erst 1982 aufheben.)

In der Folgezeit isolierte sich der Schriftsteller zunehmend. 1927 verhängte die österreichische Radio Verkehrs AG (RAVAG) einen „großen Radio-Bann“ über Schnitzler, der urheberrechtlich Ansprüche auf Tantiemen stellte.
Olga und Arthur Schnitzler ließen zu, dass ihre siebzehnjährige, von Ehestreitigkeiten traumatisierte Tochter Lili am 30. Juni 1927 den italienischen Offizier Arnoldo Cappellini heiratete. 13 Monate später, am 26. Juli 1928, beging sie in Venedig, wo sie mit ihrem Mann wohnte, Selbstmord, wofür u. a. Clara Pollaczek Arthur Schnitzler verantwortlich macht. „Mit jenem Julitag war mein Leben doch zu Ende“, notierte Schnitzler in seinem Tagebuch.

In seinen letzten Lebensjahren schrieb er vor allem Erzählungen, in denen er Einzelschicksale um die Jahrhundertwende aus psychologischer Sicht darstellt.

Arthur Schnitzler starb am 21. Oktober 1931 im Alter von 69 Jahren an einer Hirnblutung. Er wurde am 23. Oktober 1931 auf dem Wiener Zentralfriedhof in der Alten Israelitischen Abteilung, Tor 1, bestattet. Benachbart liegen die jüngeren Gräber von Oskar Strnad, Friedrich Torberg, Gerhard Bronner und Harry Weber. Das Grab wird von der Wiener Stadtverwaltung als Ehrengrab geführt.

Schnitzler schrieb Dramen und Prosa (hauptsächlich Erzählungen), in denen er das Augenmerk vor allem auf die psychischen Vorgänge seiner Figuren lenkt. Gleichzeitig mit dem Einblick in das Innenleben der Schnitzlerschen Figuren bekommt der Leser auch ein Bild von der Gesellschaft, die diese Gestalten und ihr Seelenleben prägt.

Die Handlung der Werke Schnitzlers spielt meist im Wien der Jahrhundertwende. Viele seiner Erzählungen und Dramen leben nicht zuletzt vom Lokalkolorit. Ihre handelnden Personen sind typische Gestalten der damaligen Wiener Gesellschaft: Offiziere und Ärzte, Künstler und Journalisten, Schauspieler und leichtlebige Dandys, und nicht zuletzt das "süße Mädel" aus der Vorstadt, das zu so etwas wie einem Erkennungszeichen für Schnitzler wurde sowie simultan für seine Gegner zu einem Stempel, mit dem sie Schnitzler als einseitig abqualifizieren wollten.

Es geht Schnitzler meist nicht um die Darstellung krankhafter seelischer Zustände, sondern um die Vorgänge im Inneren gewöhnlicher, durchschnittlicher Menschen mit ihren gewöhnlichen Lebenslügen, zu denen eine Gesellschaft voll von ungeschriebenen Verboten und Vorschriften, sexuellen Tabus und Ehrenkodices besonders die schwächeren unter ihren Bürgern herausfordert.

Wie Sigmund Freud in der Psychoanalyse bringt Arthur Schnitzler etwa zur gleichen Zeit jene Tabus (Sexualität, Tod) zur Sprache, die von der bürgerlichen Gesellschaft unterschlagen wurden. Im Gegensatz zu Freud offenbart sich das Wesen dieser Gesellschaft und ihrer Teilnehmer bei Schnitzler nicht als (vorher) Unbewusstes, sondern als "Halb-Bewusstes" etwa im inneren Monolog eines Protagonisten. Freud selbst schrieb in einem Brief an Schnitzler: 

Schnitzlers Werke beschäftigen sich häufig mit Themen wie Ehebruch (z. B. im Drama "Reigen"), heimlichen Affären und Frauenhelden ("Anatol", Dramenzyklus).

Nicht zufällig war es Schnitzler, der mit seiner Novelle "Leutnant Gustl" (1900) den inneren Monolog in die deutschsprachige Literatur einführte. Mithilfe dieser besonderen Perspektive gelang es ihm, dem Leser einen tieferen, direkteren Einblick in die inneren Konflikte seiner Figuren zu geben. Er führte diese Erzählform auch in "Fräulein Else" fort.

In dem Roman "Der Weg ins Freie" und im Stück "Professor Bernhardi" befasste sich Schnitzler mit dem in Wien stark ausgeprägten Antisemitismus.

Zugleich ist er einer der großen Diaristen der deutschsprachigen Literatur. Von seinem siebzehnten Lebensjahr bis zwei Tage vor seinem Tod führte er pedantisch Tagebuch. Es wurde postum 1981 bis 2000 in zehn Bänden veröffentlicht.

Der Großteil des aus geschätzten 40.000 Blättern bestehenden Nachlasses wurde durch die Aktivität eines in Wien lebenden Briten, der für die diplomatische Unterschutzstellung des Materials durch die britische Vertretung in Wien sorgte, vor den Nationalsozialisten gerettet. Das NS-Regime respektierte bei mehreren Hausdurchsuchungen, dass es (im Unterschied zu den Wohnräumen Schnitzlers) auf die in separat zugänglichen Souterrainräumen von Schnitzlers Villa gelagerten Materialien rechtmäßig keinen Zugang hatte: Auf Grund einer „Schenkung“ wurden die Materialien in die Cambridge University Library gebracht.

Problematisch an der „Schenkung“ war, dass sie von der dazu nicht berechtigten geschiedenen Frau Arthur Schnitzlers, Olga, vorgenommen wurde. Der rechtmäßige Eigentümer, der Sohn Heinrich, hielt sich nicht in Wien auf.

Während des Zweiten Weltkriegs und danach hat sich Heinrich Schnitzler daher bemüht, diesen geistigen Nachlass seines Vaters aus Großbritannien zurückzuerhalten; er blieb dabei allerdings erfolglos. In einem Artikel in der Wiener Tageszeitung "Kurier" stufte Thomas Trenkler am 11. Jänner 2015 das Verhalten der britischen Instanzen und der Cambridge University als Enteignung unter Ausnutzung einer Notlage ein; der Nachlass sollte der Familie restituiert werden. Die Familie, die Enkel Michael Schnitzler und Peter Schnitzler, ließen daraufhin verlauten, den Nachlass neuerlich zurückzufordern.

Zu den heutigen Verwahrorten und einer umfänglicheren Darstellung siehe den Artikel Arthur Schnitzlers Nachlass.




Zu laufenden Editionsprojekten siehe Schnitzler-Editionsprojekte.





Während Schnitzler als jüdischer Autor in der Zeit des Nationalsozialismus verpönt war, setzte in der Nachkriegszeit eine langsame Institutionalisierung zum Klassiker ein.

1959/60 wurde der Arthur-Schnitzler-Hof nach ihm benannt. 1971 wurde eine Schnitzler-Büste von Sandor Jaray im Burgtheater enthüllt und 1981 eine Büste von Paul Peschke im Wiener Türkenschanzpark. Seit April 2012 heißt der kleine Park gegenüber vom Bahnhof Baden "Arthur Schnitzler-Park". Einem Gemeinderatsbeschluss vom September 2016 folgend wurde am 6. Mai 2017 im 7. Wiener Gemeindebezirk der Vorplatz des Volkstheaters zwischen Burggasse, Museumstraße und Neustiftgasse „Arthur-Schnitzler-Platz“ benannt. 

Von der „Arthur Schnitzler Gesellschaft“ wird alle vier Jahre der Arthur-Schnitzler-Preis vergeben. Dieser wird vom österreichischen Unterrichtsministerium und der Kulturabteilung der Stadt Wien mit 10.000 Euro dotiert.

Schnitzlers Werk diente als Vorlage für zahlreiche Kino- und Fernsehfilme, darunter:

Für den Film "360" (2011) diente die Struktur des "Reigen" als Inspiration.










</doc>
<doc id="11932" url="https://de.wikipedia.org/wiki?curid=11932" title="Peter Altenberg">
Peter Altenberg

Peter Altenberg (* 9. März 1859 in Wien; † 8. Jänner 1919 ebenda; eigentlich "Richard Engländer") war ein österreichischer Schriftsteller. Sein Pseudonym wählte er nach dem Rufnamen "„Peter“" seiner Jugendliebe Berta Lecher, die in "Altenberg an der Donau" (heute Ortsteil der Gemeinde St. Andrä-Wördern) wohnte.

Richard Engländer wurde als Sohn von Moritz Engländer, einem jüdischen Kaufmann, und seiner Gattin Pauline, geb. Schweinburg, geboren. Er studierte erst Jus, dann Medizin, brach die Studien aber ab und nahm eine Buchhändlerlehre bei der Hofbuchhandlung Julius Weise in Stuttgart auf. Diese brach er ebenso ab wie einen erneuten Versuch des Jus-Studiums. 1895 verfasste er erste literarische Arbeiten, durch den Kontakt mit Karl Kraus kam es ab 1896 zu Veröffentlichungen (Skizzenband "Wie ich es sehe"). Im März bzw. April 1900 trat er „aus der israelitischen Religionsgemeinschaft“ aus, blieb dann zehn Jahre konfessionslos und ließ sich schließlich im Jahr 1910 in der Karlskirche taufen. Sein Taufpate war der Architekt Adolf Loos. Altenberg, der nach einer kurzen Zeit in München wieder nach Wien zurückgekehrt war, war dort schon zu Lebzeiten eine stadtbekannte Figur, um die sich die Legenden rankten. 1904 riet er dem achtzehnjährigen Sohn Heinz von Marie Lang, der sich an ihn gewandt hatte, weil Lina Loos die Affäre beendet hatte, er solle sich umbringen, was dieser dann tat. Schnitzler verarbeitete das in dem unvollendeten „P.A.-Stück“ Das Wort.

Nach einigen fehlgeschlagenen Versuchen, ein normales Berufsleben zu beginnen, attestierte ihm ein Arzt wegen einer „Überempfindlichkeit des Nervensystems“ die Unfähigkeit, einen Beruf auszuüben. Seither führte er das Leben eines Bohemiens und verbrachte die meiste Zeit in Kaffeehäusern. 

Trotz Erfolges blieb Altenberg von Spenden abhängig, zu denen seine Freunde – darunter Karl Kraus und Adolf Loos – aufriefen. Seine letzten sechs Lebensjahre wohnte er in einem Zimmer im Hotel Graben in der Dorotheergasse. Nachdem er in den letzten zehn Lebensjahren häufig in Alkoholentzugs- und Nervenheilanstalten gewesen war, starb er am Vormittag des 8. Jänner 1919 an der III. Medizinischen Klinik (siehe: Franz Chvostek junior) des Wiener Allgemeinen Krankenhauses. Er wurde am 11. Jänner 1919 auf dem Wiener Zentralfriedhof in einem Ehrengrab bestattet (Gruppe 0, Reihe 1, Nummer 84). Karl Kraus schloss seine Grabrede mit den Worten: „Wehe der Nachkommenschaft, die Dich verkennt!“

Von kurzen Eindrücken, flüchtigen Begegnungen und zufällig mitgehörten Gesprächen inspiriert, schrieb Altenberg als Gelegenheitskünstler sogenannte Kaffeehausliteratur. Diese stellte bei ihm eine impressionistische Studie der Gesellschaft und des Lebens der Wiener Moderne dar. Er selbst beschrieb den Prozess der Entstehung dieser Texte in einem Brief an Arthur Schnitzler folgendermaßen:

Stilistisch zeichnen sich seine Texte durch teilnahmslose Schilderung aus, der Autor scheint ausschließlich zu beobachten. Das Beobachten führt dazu, dass häufig keine Hauptfigur oder eine Handlung existiert, ebenso wenig wie auf Botschaft und Verknüpfung Wert gelegt wird.

Das Werk Peter Altenbergs besteht ausschließlich aus diesen kurzen Prosatexten, die sich nur schwer einer der kanonisierten literarischen Formen zuordnen lassen. Sie werden meistens als Prosaskizzen oder Prosagedichte bezeichnet. Es sind Momentaufnahmen, die in konzentrierter Form das Leben, die Gesellschaft Wiens um die Jahrhundertwende zeigen. Die Kunst Peter Altenbergs besteht darin, mit wenigen „literarischen Pinselstrichen“ ein umfassendes Bild zu schaffen; mit Hilfe von kurzen Andeutungen vor dem Leser, der bereit ist, auch zwischen den Zeilen zu lesen, ein ganzes Panorama der Gesellschaft, ein ganzes Netz von Beziehungen auferstehen zu lassen.

Altenberg versucht nicht, das Leben auf einen ideologischen Nenner zu bringen, sondern zeigt es in seiner ganzen Buntheit, seiner oft widersprüchlichen Vielfalt. Eine wichtige Rolle in seinen Skizzen spielen sinnliche Eindrücke – Farben, Gerüche, Stimmungen. Er gilt als einer der wichtigsten Vertreter des Impressionismus.

Auf der anderen Seite sind seine kurzen Texte teilweise auch für die Bühne geeignet – so trug etwa einer seiner Freunde, der Schriftsteller Egon Friedell, der auch als Kabarettist und Conférencier tätig war, immer wieder auch Texte Altenbergs öffentlich vor. Teile aus Friedells Gesprächen mit Altenberg erschienen späterhin als "Anekdoten", für die allerdings Friedell als Verfasser verantwortlich zeichnete. Einige seiner Texte wurden von Alban Berg vertont. Das kurze Stück „Masken“ widmete er Josef Hoffmann. Es wurde 1907 im "Kabarett Fledermaus" aufgeführt und zwar mit Szenen- und Kostümentwürfen von Carl Otto Czeschka.

Im Jahr 1929 wurde in Wien-Döbling (19. Gemeindebezirk) die "Peter-Altenberg-Gasse" nach ihm benannt.

Im Wiener Café Central wurde er als lebensgroße, Zeitung lesende Figur aufgestellt. Im Büro des Wiener Bürgermeisters befindet sich eine ähnliche Figur.






</doc>
<doc id="11933" url="https://de.wikipedia.org/wiki?curid=11933" title="Richard Engländer">
Richard Engländer

Richard Engländer ist der Name folgender Personen:

<onlyinclude>* Richard Engländer, bekannt unter dem Künstlernamen Peter Altenberg (1859–1919), österreichischer Schriftsteller


</doc>
<doc id="11934" url="https://de.wikipedia.org/wiki?curid=11934" title="Sonnenwind">
Sonnenwind

Der Sonnenwind ist ein Strom geladener Teilchen, der ständig von der Sonne in alle Richtungen abströmt. Im Vergleich zum Sternwind anderer Fixsterne ist er relativ schwach, muss aber bei der Ursonne stärker gewesen sein. 

Der Sonnenwind ist Bestandteil der kosmischen Strahlung. Er ist anders als die Sonnenstrahlung keine elektromagnetische Strahlung, sondern ein Teilchenstrom. Gelegentlich wird auch der falsche Begriff "Sonnenstaub" (analog zu Sternenstaub) verwendet, was insbesondere bei der Berichterstattung der Presse zur Genesis-Sonde der Fall war. Der Sonnenwind ist sehr variabel und setzt sich aus sehr verschiedenen Arten von Teilchenströmen zusammen. Seine extremste Form sind koronale Massenauswürfe (CME), die auch auf der Erde massive Folgen hervorrufen können.

Der Sonnenwind besteht hauptsächlich aus ionisiertem Wasserstoff (Protonen und Elektronen) sowie aus 8 % Helium-4-Atomkernen (Alphateilchen). Daneben enthält er Spuren von ionisierten Atomkernen der Elemente Kohlenstoff, Stickstoff, Sauerstoff, Neon, Magnesium, Silizium, Schwefel und Eisen. Nichtionisierte (elektrisch neutrale) Atome sind kaum enthalten, weshalb der Sonnenwind ein sogenanntes Plasma darstellt, das elektrisch hoch leitfähig ist. Dennoch ist der interplanetare Raum wegen der geringen Teilchendichte nur sehr gering leitfähig.

Man unterscheidet den langsamen und den schnellen Sonnenwind. Diese beiden unterscheiden sich nicht nur durch ihre Geschwindigkeit, sondern auch durch ihre chemische Zusammensetzung, ihre Temperatur und ihr Strömungsverhalten. Obwohl er aus den äußeren Schichten der Sonne stammt, spiegelt der Sonnenwind die Elementhäufigkeit dieser Schichten nicht exakt wider. Denn durch Fraktionierungsprozesse (FIP-Effekt) werden manche Elemente im Sonnenwind angereichert beziehungsweise verdünnt. Im Inneren der Sonne wurden seit ihrer Entstehung die Elementhäufigkeiten durch die dort ablaufende Kernfusion geändert; da aber die äußeren Sonnenschichten nicht mit den inneren gemischt sind, entspricht deren Zusammensetzung noch jener des Urnebels, aus dem sich das Sonnensystem gebildet hat. Die Erforschung des Sonnenwindes ist deshalb auch interessant, um sowohl auf die chemische Zusammensetzung als auch auf die Isotopenhäufigkeiten des Urnebels schließen zu können.

Die Sonne verliert durch den Sonnenwind pro Sekunde etwa eine Million Tonnen ihrer Masse. Mit zunehmendem Abstand von der Sonne nimmt die Dichte des Sonnenwindes mit dem Quadrat der Entfernung ab. In Erdnähe hat der Sonnenwind eine Dichte von ungefähr 5 · 10 Teilchen pro Kubikmeter.

Das Plasma der unteren Sonnenkorona wird mit der Rotation der Sonne mitgedreht. Ab einem gewissen Abstand, etwa 2,5 Sonnenradien (~2,5 · R) wächst der thermische Druck über den magnetischen hinaus und das Plasma strömt ab diesem Punkt radial von der Sonne fort. Es werden zwei Arten des Sonnenwinds unterschieden, der "langsame" und der "schnelle".

Der "langsame Sonnenwind" hat eine Zusammensetzung ähnlich der Sonnenkorona. Während er von der Sonne abströmt, verdoppelt er seine Geschwindigkeit von 150 km/s im Abstand von 5 · R auf 300 km/s im Abstand 25 · R. Sein Ursprung ist noch nicht abschließend geklärt. Man nimmt an, dass beobachtete tropfenartige Plasma-Ablösungen von "Helmet Streamern" zum langsamen Sonnenwind beitragen. Der Hauptanteil des langsamen Sonnenwinds dürfte jedoch aus Regionen außerhalb der "Helmet Streamer" stammen, wahrscheinlich aus den inneren Begrenzungsrändern von koronalen Löchern. Er beschleunigt während seines Fortströmens von der Sonne weiter und strömt nach Messungen von Sonden wie Ulysses in einem bestimmten Abstand zur Sonne vor allem nahe deren Äquatorebene, zwischen etwa 20° Nord und 20° Süd. Er benötigt 5 oder mehr Tage, nach anderen Angaben etwa 20 Tage, um die Region der Erde zu erreichen. In Erdbahnnähe hat er eine Geschwindigkeit von etwa 300 bis 500 km/s und eine Temperatur im Bereich von etwa 1,4 · 10 K bis 1,6 · 10 K. Die Plasmaschallgeschwindigkeit beträgt in Erdbahnnähe etwa 50 km/s, der Sonnenwind ist also deutlich überschallschnell.

Der "schnelle Sonnenwind" hat eine Zusammensetzung ähnlich der Photosphäre der Sonne. Er tritt aus dem Inneren von koronalen Löchern (also vorwiegend, insbesondere zu Zeiten des Sonnenfleckenminimums, in der Nähe der Sonnenpole) aus, wird zwischen 1,5 · R und 2,5 · R auffallend stark beschleunigt und besitzt in der Bereichsmitte, also bei 2 · R, eine Geschwindigkeit von 300 km/s. Dabei sind die Sauerstoffionen erheblich schneller als die leichteren Protonen. Die Messungen durch das "Ultraviolet Coronal Spectrometer" (UVCS) des Forschungssatelliten Solar and Heliospheric Observatory (SOHO) ergaben, dass der schnelle Sonnenwind über den Polen der Sonne erheblich schneller beschleunigt wird, als durch die Thermodynamik erklärt werden kann. Diese Theorie sagt voraus, dass die Schallgeschwindigkeit etwa vier Sonnenradien über der Photosphäre überschritten werden sollte. Tatsächlich findet man diese Grenze bereits in etwa 25 % dieser Distanz. Als Ursache dieser Beschleunigung werden Alfvén-Wellen angesehen. Der schnelle Sonnenwind beschleunigt weiter bis etwa 10 bis 20 Sonnenradien Distanz, ab dann strömt er mit ungefähr konstanter Überschallgeschwindigkeit fort. Der schnelle Sonnenwind benötigt etwa 2 bis 4 Tage, um die Region der Erde zu erreichen. In Erdbahnnähe hat er eine Geschwindigkeit von etwa 750 km/s und eine Temperatur von etwa 8 · 10 K.

Der Sonnenwind strömt radial von der Sonne fort. Aufgrund der Sonnenrotation – eine Umdrehung in etwa 27 Tagen, bezogen auf die Erde – bildet er jedoch dabei spiralig gekrümmte Kurven, ähnlich dem Wasserstrahl eines Sprinklers. Der schnelle Sonnenwind formt dabei steilere Spirallinien als der langsame Sonnenwind (siehe nebenstehende Abbildung). Hierdurch entstehen an den Kreuzungspunkten Druckwellen, bestehend aus einem vorwärts und einem rückwärts gerichteten Wellenpaar. Diese werden "co-rotating interaction regions" (CIRs) genannt. Mit den Voyager-Sonden wurde entdeckt, dass Gruppen dieser CIRs ihrerseits miteinander verschmelzen können, wodurch "merged interaction regions" (MIRs) entstehen. Diese Interaktionen geschehen typischerweise bis etwa 10 AE. Jenseits davon bestehen komplexe Strukturen, so dass der Sonnenwind auch in großer Entfernung kein homogener Fluss ist.

Der Sonnenwind strömt so lange mit Überschallgeschwindigkeit von der Sonne fort und dünnt sich dabei mit dem Quadrat der Entfernung aus, bis sein fortwährend geringer werdender Druck den Partikeln und Feldern des lokalen interstellaren Mediums nicht mehr standhalten kann. An dieser Stelle, die Terminationsschock genannt wird, wird der Sonnenwind abrupt von ca. 350 km/s auf ca. 130 km/s, und damit auf Unterschallgeschwindigkeit, abgebremst. Dabei verdichtet er sich und heizt sich auf. Die genaue Form und Größe des Terminationsschocks ist variabel, da sie von Dichteschwankungen des Sonnenwinds ebenso wie von Stärkeschwankungen des interstellaren Mediums abhängt. Die Raumsonden Voyager 1 und Voyager 2 erreichten den Terminationsschock bei 94 AE bzw. 84 AE Entfernung.

Außerhalb des Terminationsschocks befindet sich die Zone der Heliohülle "(heliosheath)". In dieser vermischen sich die Teilchen des abgebremsten Sonnenwinds mit denen des lokalen interstellaren Mediums. An der Heliopause schließlich sind die Sonnenwindteilchen mit dem interstellaren Medium im Gleichgewicht.

Ein deutlich sichtbares Anzeichen für die Existenz des Sonnenwinds liefern die Kometen: Durch die Wirkung des Sonnenwindes wird Material aus der Koma eines Kometen gerissen. Der bläulich leuchtende Gasschweif eines Kometen zeigt immer in gerader Linie von der Sonne weg, unabhängig von der Bewegungsrichtung des Kometen. Auch der Staubschweif eines Kometen zeigt von der Sonne weg, aber da die Staubpartikel deutlich langsamer als die Gas-Ionen sind, ist der Staubschweif wegen der Eigenbewegung des Kometen gekrümmt und sein Winkel zur Sonne ist kleiner als 180 Grad.

Koronale Massenauswürfe und Sonneneruptionen führen zu enormen Stoßwellen im sonst relativ kontinuierlichen Sonnenwind. Deren Auswirkungen im erdnahen Bereich werden als Weltraumwetter bezeichnet. 

Da der Sonnenwind ein elektrisch leitendes Plasma darstellt, verformt er sowohl das Magnetfeld der Sonne als auch das der Erde. Das irdische Magnetfeld hält den Teilchenschauer zum größten Teil von der Erde ab. Bei einem starken Sonnenwind kann das Plasma das Erdmagnetfeld so stark verformen, dass durch magnetische Rekonnexion geladene Teilchen zur Erde beschleunigt werden und in den hohen Schichten der Erdatmosphäre Polarlichter hervorrufen. Hierbei handelt es sich um sogenannte sekundäre Teilchen, da diese nicht von der Sonne stammen, sondern aus der Magnetosphäre der Erde.

Starke Sonnenwinde haben auch Einfluss auf die Ausbreitung von elektromagnetischen Wellen und können unter anderem den Kurzwellenfunk und die Kommunikation mit Satelliten stören. Sonnenwinde und ihre Auswirkungen auf die Technik sind seit z. B. 1847, 1859, 1921 und 1940 bekannt, weil es zu Störungen in der Telegraphie, an Signalanlagen der Bahn, bei der Radiokommunikation und vereinzelt sogar zum explosionsartigen Durchschmoren von Transformatoren gekommen ist (zu einem Transformatorenausfall ist es z. B. am 13. März 1989 in Quebec gekommen). Es wird für möglich gehalten, dass besonders starke Sonnenwinde zu einem globalen Totalausfall von Stromversorgung und Computerfunktionen führen könnten.

Innerhalb der Heliosphäre gibt es eine Schicht, in der das Magnetfeld der Sonne seine Polarität ändert. Dadurch entstehen elektrische Ströme im Sonnenwind, die von Raumsonden gemessen werden konnten. Diese Schicht ist unregelmäßig geformt und heißt Heliosphärische Stromschicht.

Bereits beim Carrington-Event von 1859 beobachtete der Forscher Richard Carrington einen Zusammenhang zwischen Sonnenflares und zeitlich versetzten irdischen Magnetfeldstürmen, was – obwohl damals unerklärlich – ein frühes Indiz für die Existenz des Sonnenwindes war. Anfang des 20. Jahrhunderts vertrat der norwegische Physiker Kristian Birkeland die Auffassung, die Polarlichter würden durch Teilchenströme von der Sonne ausgelöst. Seine Idee wurde jedoch ebenso wenig ernst genommen wie die des deutschen Physikers Ludwig Biermann, der eine „Solare Teilchenstrahlung“ annahm, um die Richtung der Kometenschweife erklären zu können. Astronomen war aufgefallen, dass die Kometenschweife nicht exakt von der Sonne weg gerichtet waren, sondern einen kleinen Winkel dazu aufwiesen. Biermann erklärte diese Eigenschaft 1951 durch die Bewegung des Kometen in einem sich ebenfalls bewegenden Teilchenstrom, gewissermaßen ein seitliches Abdriften durch die Strömung. E. N. Parker hat 1959 die englische Bezeichnung "solar wind" eingeführt und eine magnetohydrodynamische Theorie zur Beschreibung des Sonnenwindes vorgeschlagen.

Die Existenz des Sonnenwinds konnte erst 1959 durch die sowjetische Lunik 1 und 1962 durch die amerikanische Raumsonde Mariner 2 auf ihrem Weg zur Venus experimentell bestätigt werden. Ein weiterer Meilenstein in der Erforschung des Sonnenwindes waren die Sonnenwindsegel, die mit Ausnahme von Apollo 17 bei allen Mondlandungen aufgestellt wurden und Daten über die Isotopenhäufigkeiten der Edelgase Helium, Neon und Argon im Sonnenwind lieferten. Viele weitere Missionen haben zum Verständnis des Sonnenwindes beigetragen. Die Raumsonden Pioneer 10/11, Voyager 1/2 und die Ulysses-Mission lieferten Daten des Sonnenwindes außerhalb der Erdumlaufbahn, während Helios 1/2 und die Mariner- und Pioneer-Missionen zur Venus sowie russische Vega-Sonden Daten von innerhalb der Erdumlaufbahn lieferten. IMP 1–8, AIMP 1/2, ACE, ISEE 1–3 Sonden sowie das Sonnenobservatorium SOHO und die Raumsonde Wind lieferten Sonnenwinddaten in Erdnähe. Die Ulysses-Mission lieferte auch Daten über den Sonnenwind außerhalb der Ekliptik. Im Jahr 2001 wurde die Genesis-Mission gestartet, bei der hochreine Kristalle in einem der Lagrange-Punkte (L1) des Erde-Sonne-Systems dem Sonnenwind ausgesetzt wurden und danach zur Untersuchung zur Erde zurückgebracht werden sollten. Die Mission schlug bei ihrem Abschluss im Jahr 2004 fehl, weil die Kapsel mit den Sonnenwindteilchen nicht abgebremst wurde, sondern auf dem Erdboden zerschellte. Die Raumsonde Voyager 1 hat im Dezember 2004 den Terminationsschock erreicht, und im August 2007 erreichte Voyager 2 diese Grenze und übermittelte Messdaten.

Es gibt Bemühungen, den Sonnenwind mit Hilfe von Sonnensegeln zum Antrieb von Raumfahrzeugen zu nutzen.





</doc>
<doc id="11936" url="https://de.wikipedia.org/wiki?curid=11936" title="Wirbel">
Wirbel

Wirbel (althochdeutsch "", aus "(h)werban, (h)werfan" „sich drehen“) bezeichnet:

Siehe auch:



</doc>
<doc id="11937" url="https://de.wikipedia.org/wiki?curid=11937" title="Glorious Revolution">
Glorious Revolution

In der Glorious Revolution – der Glorreichen Revolution – von 1688/1689 entschieden die Gegner des königlichen Absolutismus in England den seit Beginn des 17. Jahrhunderts geführten Machtkampf mit dem Stuartkönigtum endgültig zu ihren Gunsten. Sie schufen mit der Durchsetzung der Bill of Rights die Grundlage für das heutige parlamentarische Regierungssystem im Vereinigten Königreich. Seit der Revolution ist dort der König nicht mehr allein, sondern nur in Verbindung mit dem Parlament ("King-in-parliament") Träger der Staatssouveränität. 

Mit der Revolution endete die Politik religiöser Toleranz, die König Jakob II. von England, der selbst zum Katholizismus konvertiert war, gegenüber Katholiken betrieben hatte. Während die protestantischen Dissenters mit der Toleranzakte Glaubensfreiheit zugesichert bekamen, galten für die Katholiken nach der Revolution für über ein Jahrhundert diskriminierende Regelungen. Nach dem Sturz des katholischen Monarchen bestieg Wilhelm III. von Oranien, Jakobs Neffe und Schwiegersohn, gemeinsam mit Jakobs protestantischer Tochter Maria II. den Thron. 

Bereits die Zeitgenossen verwendeten die Bezeichnung "Glorreiche Revolution" in bewusstem Gegensatz zu den Wirren des Englischen Bürgerkriegs, der mit der Hinrichtung König Karls I. und der Errichtung einer Republik unter Oliver Cromwell geendet hatte. Man war der Auffassung, dass der vergleichsweise unblutige Umsturz von 1688/1689 auch deshalb von Erfolg gekrönt war, weil ihn die „Gloriole“ eines neuen Königtums umstrahlt habe.

Bereits Jakobs Bruder Karl II., der England von 1660 bis 1685 regierte, stand in seinen persönlichen Glaubensüberzeugungen dem Katholizismus nahe. Um seine erst 1660 restaurierte Herrschaft nicht zu gefährden, schützte er jedoch die verfassungsmäßigen Rechte der anglikanischen Staatskirche und erließ Gesetze gegen Katholiken und Nonkonformisten. Diese Politik wurde von großen Teilen der Bevölkerung, der Gentry und des Hochadels sowie von Parlament und Kirche unterstützt. Erst auf dem Sterbebett konvertierte Karl II. zum Katholizismus. 
Jakob hatte diesen Schritt bereits Ende der 1660er Jahre getan. So entstand 1685 die Situation, dass ein Katholik König von England und Oberhaupt der Anglikanischen Kirche wurde. Zudem war er Befürworter einer absolutistischen Monarchie nach französischem und spanischem Vorbild. Bald nach seiner Thronbesteigung am 23. April 1685 ging Jakob auf Konfrontationskurs mit den Whigs. Jakob betraute auch Katholiken mit Ämtern am Hof, in der Verwaltung und in der Armee, womit er sich über Gesetze wie die Testakte hinwegsetzte. Dazu glaubte er sich aufgrund seines absolutistischen Herrschaftsverständnisses berechtigt. Gleichzeitig übte er aber auch Toleranz gegenüber protestantischen Nonkonformisten wie den Quäkern.

Da Jakob keinen männlichen Erben hatte und seine beiden aus erster Ehe stammenden erbberechtigten Töchter Maria und Anne protestantisch erzogen waren, nahmen große Teile der englischen Bevölkerung und Teile der anglikanischen Kirche seine Politik zunächst hin. Es bestand die begründete Aussicht, dass die Politik des bereits über 50-jährigen Königs nach seinem Tod rasch revidiert werden könnte. Insbesondere die anglikanische Hochkirche, die seit jeher das gottgegebene Herrschaftsrecht der Könige ("Divine Right") verteidigt hatte, argumentierte, man dürfe auch einer katholischen Obrigkeit nicht den Gehorsam verweigern oder ihr gar Widerstand entgegensetzen. Dies änderte sich erst, als Jakob die anglikanischen Bischöfe zwingen wollte, eine Toleranzerklärung gegenüber dem Katholizismus von den Kanzeln verlesen zu lassen. Mit der Weigerung der Bischöfe gaben sie das Prinzip der "non resistance" auf, das sie von jeher zur stärksten Stütze des Königtums gemacht hatte.

Der letzte Anstoß zur Einigung aller politischen und religiösen Gegner des Königs aus Parlament, Adel und Kirche erfolgte jedoch aus dynastischen Gründen: Als dem König und seiner zweiten Frau, der streng katholischen Maria Beatrix von Modena, im Sommer 1688 ein Sohn geboren wurde, drohte die dauerhafte Etablierung einer katholischen Dynastie. Auch die anglikanischen Bischöfe stimmten nun mehrheitlich zu, die protestantische Tochter des Königs, Maria, und ihren calvinistischen Ehemann, Wilhelm von Oranien, den Generalstatthalter der Niederlande, ins Land zu rufen und ihnen gemeinsam die Krone anzutragen. Dieser folgte dem Ruf vor allem aus bündnispolitischen Erwägungen. Wilhelm sah die Chance, England für das Lager der Gegner König Ludwigs XIV. von Frankreich zu gewinnen.

Nach der Einladung durch eine Gruppe von Adligen, die "Immortal Seven", setzte Wilhelm von Oranien im Herbst 1688 mit einer starken Söldnerarmee nach England über, um dem „bedrängten englischen Volk“ zu Hilfe zu eilen. Francisco Lopes Suasso, einer der eifrigsten Unterstützer des Hauses von Oranien und einer der mächtigsten Bankiers in der Republik der Sieben Vereinigten Provinzen, finanzierte die Expedition nach England mit 2 Millionen Gulden.

Wilhelms Landung war die einzige erfolgreiche Invasion Englands seit 1066. Jakob II. sah sich aufgrund des Überlaufens zahlreicher Adliger zu Wilhelm außerstande, diesem militärisch zu begegnen und floh nach Frankreich. Das Staatssiegel warf er auf seiner Flucht in die Themse. Dies wurde von seinen Gegnern als Abdankung interpretiert, so dass seine Tochter und sein Schwiegersohn als Maria II. und Wilhelm III. den vakanten Thron besteigen konnten. 

Trotz mehrerer Versuche einer gewaltsamen Rückeroberung gelang es Jakob II. und seinen Erben nicht, ihre Ansprüche auf die Krone erneut geltend zu machen. Jakob starb am 16. September 1701 im französischen Exil. Bis zur Mitte des 18. Jahrhunderts kam es zu einigen Aufständen ihrer Anhänger, der Jakobiten. Doch Wilhelm gelang es, seine Herrschaft in England zu stabilisieren. Er stärkte durch seine Gesetzgebung maßgeblich den englischen Parlamentarismus, übte religiöse Toleranz nur gegenüber den protestantischen Dissenters und konnte sicherstellen, dass auch nach seinem Tod im Jahr 1702 und der Thronbesteigung Annes, einer weiteren Tochter Jakobs II., keine Annäherung Englands an Frankreich mehr stattfand.

Die Glorreiche Revolution war ein vor allem auf religiösen Motiven basierender Vorgang, der durch politische Erwägungen aller Beteiligten zusätzlich verschärft wurde. Eine wichtige Rolle spielte hierbei die Church of England, die durch Heinrich VIII. von Rom abgespalten worden war. In den seiner Entstehung folgenden Jahren stabilisierte sich der zwar „katholisch“ (episkopal) verfasste, aber in der Lehre vorherrschend calvinistische Anglikanismus, und auch die Versuche Maria Tudors, England wieder zu katholisieren, blieben erfolglos. 

Durch das Elizabethan Settlement von 1559 wurde der König von England zugleich auch zum „Supreme Governor of the Church“ ernannt und somit die Trennung von Rom untermauert. Die Einführung des Book of Common Prayer wiederum spaltete die Reformationsbewegung und führte zur Gründung der puritanischen Bewegung, die in den folgenden Jahren immer stärkeren Zulauf erhielt und neben einer strengen Sittenlehre die Unabhängigkeit der Kirche vom Königtum forderte. Gleichzeitig radikalisierten sich die Bevölkerung und die Geistlichkeit zu einem strikten Anti-Katholizismus, verstärkt durch die Exkommunikation Königin Elisabeths 1570, das Vorgehen der spanischen Armada gegen England 1588, den Gunpowder Plot von 1605 (eine katholische Verschwörung gegen Jakob I. und das Parlament), die Furcht vor einer katholischen Invasion in den 1620er Jahren, und den Ausbruch der Irischen Rebellion von 1641. Antipapismus "(Anti-Popery)" wurde so zu einem unabdingbaren Grundfaktor der englischen Gesellschaft des 17. Jahrhunderts, und zwar nicht nur auf nationaler, sondern auch auf lokaler Ebene. 

Im starken Gegensatz dazu stand die Politik der Krone, die mit einer Annäherung erst an das katholische Spanien und später an das ebenfalls katholische Frankreich im diametralen Gegensatz zu den Erwartungen des Volkes und eines großen Teils des Adels handelte. Auch die von Karl I. maßgeblich geförderte arminianische Kirchenpolitik, die eine stärkere Formalisierung der Kirchenliturgie vorsah und zentrale Elemente der calvinistischen Kirchenreformen rückgängig zu machen suchte (z. B. in der Kleiderordnung, der Rolle des Presbyteriums und der Abendmahlsliturgie), schürte das Misstrauen gegen die Krone. Immer stärker begann sich die Überzeugung durchzusetzen, dass ein „in katholische Verschwörungen verwickelter König das Anrecht auf Gehorsam verwirkt“ hatte. 1637 kam es über diesen Konflikt zum organisierten Widerstand in Schottland, den Karl im Jahr 1640 mit einem Einmarsch seiner Truppen (Bischofskriege) beantwortete. Ein Parlament, welches er hierzu hatte einberufen lassen, das seine finanziellen Forderungen jedoch nicht unterstützte, wurde von ihm bereits nach drei Wochen wieder entlassen (Kurzes Parlament). Nachdem die englische Armee Ende August jedoch von schottischen Truppen geschlagen worden war und sich diese in Nordengland festsetzten, sah sich der König gezwungen, ein neues Parlament (Langes Parlament) einzuberufen und einen Waffenstillstandsvertrag zu unterschreiben. 

Das Lange Parlament konnte von Beginn an einen Teil der königlichen Souveränität an sich reißen, indem es die Friedensverhandlungen mit Schottland und die Bezahlung der Armee zu handhaben verlangte. Gleichzeitig richtete sich sein Handeln gegen die unmittelbare Umgebung des Königs, vor allem gegen den königlichen Berater und Katholiken Thomas Wentworth, 1. Earl of Strafford, gegen den ein Amtsenthebungsverfahren eingeleitet wurde. Ebenfalls angeklagt wurde Erzbischof Laud, was die faktische Entmachtung der kirchlichen Gerichte und damit den Kollaps der staatlich-kirchlichen Zensur zur Folge hatte. 

Verschärft wurde der Konflikt zwischen Parlament und König im Mai 1641, als ruchbar wurde, dass Karl ernsthaft erwog, die Reste seiner Armee für eine Befreiung Straffords aus dem Tower einzusetzen (Army Plot). Das Unterhaus sprach daraufhin ein Todesurteil gegen Strafford aus. Öffentliche Proteste und Demonstrationen vor dem Oberhaus und dem königlichen Palast sorgten schließlich dafür, dass der König seinen früheren engen Berater fallen ließ. Das Parlament verschärfte in der Folge seinen Kurs gegen den König, beschnitt dessen Recht zur Auflösung des Parlaments, schaffte wesentliche durch den König eingeführte Steuern und Abgaben ab und löste besondere Regierungsorgane (Gerichtshöfe) auf, die dem König direkt unterstanden. Auch die Grand Remonstrance war als Angriff auf den König zu verstehen, listete sie doch alle Fehlleistungen und Rechtsverstöße der königlichen Politik seit den 1620er Jahren auf und forderte, die Verantwortlichen zu bestrafen und aus dem Umfeld des Königs zu entfernen.

Durch den Aufstand der Iren, den die Gegner Karls als erneute Verschwörung des Königs gegen das Parlament verurteilten, eskalierte die Lage vollends: Nachdem das Parlament angesichts der prekären Lage eine Kontrolle über die zur Niederschlagung des Aufstandes benötigte Armee forderte (Militia Ordinance), reagierte der König mit einer Hochverratsklage gegen mehrere Mitglieder des Unterhauses und versuchte einen Tag später, die Angeklagten unter Verletzung ihrer Immunität verhaften zu lassen. Erschreckt von den daraufhin einsetzenden Protesten der Londoner Bevölkerung verließ König Karl die Hauptstadt. 

Der Englische Bürgerkrieg begann am 23. Oktober 1642 und dauerte, mit einigen Unterbrechungen, bis 1649. Anfangs gelang es den königstreuen Truppen, die Armee des Parlaments zurückzudrängen; von 1645 an konnte diese sich jedoch reorganisieren und mit Unterstützung der schottischen Armee, die durch religiöse Konzessionen erneut den Norden Englands besetzte, einen Umschwung im Konflikt erreichen. 1646 ergab sich Karl I. der schottischen Armee, die ihn jedoch binnen Jahresfrist an die Parlamentsarmee auslieferte. Im Juni 1646 wurden die Kampfhandlungen daraufhin eingestellt, und eine starke Gruppierung im Parlament (Friedenspartei) versuchte, getrieben von der Angst vor einem zu starken Einfluss radikaler Kräfte in Parlament und Armee, die stehende Truppe schnellstmöglich aufzulösen. Diese wiederum sahen darin einen Versuch, die Interessen des Volkes einem billigen Kompromiss mit dem König zu opfern, und Ende Mai und Anfang Juni 1647 kam es zu Meutereien gegen das Parlament. Am 3. Juni übernahm die so in Opposition zur Friedenspartei getretene und religiös radikalisierte Armee endgültig die Bewachung des gefangenen Königs und wurde somit zur politisch dominierenden Macht im Staate. 

Die Friedensverhandlungen fanden mit der Flucht Karls I. an Weihnachten 1647 und dessen militärischer Allianz mit den schottischen Streitkräften (im Gegenzug für die Einführung einer presbyterialen Kirchenverfassung in England für die Dauer von vorerst drei Jahren) ein abruptes Ende; im April brach der Zweite Bürgerkrieg aus. Er war, abgesehen von der Invasion einer schottisch-royalistischen Armee im Nordwesten, vor allem ein Aufstand einzelner Grafschaften und Regionen (insbesondere Wales, Kent, Essex) gegen die verhasste Revolutionsregierung. Militärisch dominierte die Parlamentsarmee jedoch das Geschehen und setzte den König schon im Herbst 1648 mit der Eroberung Schottlands wieder gefangen. Erneut wurden Verhandlungen mit Karl aufgenommen, und eine Einigung und die Wiedereinsetzung des Königs schien in den folgenden Monaten denkbar, denn ein nicht unerheblicher Teil der Abgeordneten schien eine Restauration des Monarchen als geringeres Übel im Vergleich zu einer weiteren Militärherrschaft zu betrachten. Da verweigerte die Armee, die einen Ausverkauf ihrer Interessen fürchtete, den gemäßigten Abgeordneten den Zutritt zum Unterhaus und erzwang das sogenannte Rumpfparlament. Weitere Verhandlungen mit dem König wurden somit unterbunden, und am 30. Januar 1649 wurde Karl gemäß dem Willen der führenden Offiziere nach einem kurzen Prozess hingerichtet: England wurde zur Republik.

Die folgende Dekade brachte England eine republikanische Grundordnung: Bereits im Februar beschloss das verbliebene Rumpfparlament die Auflösung des Oberhauses und nur kurze Zeit später die Abschaffung der Monarchie. Auf Frieden wartete das englische Volk jedoch vorerst vergebens. Nach dem Tod Karls I. folgte ihm sein Sohn Karl II. auf den schottischen Thron. Mit der Anerkennung der schottischen Kirchenverfassung sicherte er sich die Loyalität der Bevölkerung und stieß mit einer Armee nach England vor. Oliver Cromwell gelang es jedoch, die Truppen des Königs zu besiegen, so dass dieser ins französische Exil fliehen musste (Schottland wurde als Folge in eine Union mit England gezwungen). Auch in Irland setzte Cromwell die englischen Interessen mit Gewalt durch und unterwarf das Land mit großer Brutalität. Unmittelbar daran schloss sich der Englisch-Niederländische Seekrieg (1652–1654) an, mit dem die Niederlande auf die Versuche des Rumpfparlaments reagierten, sie aus dem englischen Seehandel zu vertreiben. 

1653 wurde das Rumpfparlament selbst aufgelöst und Cromwell das quasi-monarchische Amt eines Lordprotektors angetragen, das dieser zögernd annahm. Die unter ihm gewährte Religionsfreiheit wiederum sollte sich als erneuter Stein des Anstoßes in der englischen Geschichte erweisen. Mit dem Aufkommen und dem Wachstum neuer religiöser Richtungen wuchs die Angst in der englischen Gesellschaft vor Sekten und vor einer kirchlichen und gesellschaftlichen Desintegration. Der Tod Cromwells am 3. September 1658 führte binnen kurzer Zeit zu einer Rückkehr des Königs am 25. Mai 1660.

Karls Regentschaft erwies sich als durchaus wechselhaft. Zum einen versuchte er sich in einem Bündnis mit der französischen Krone, gipfelnd im Jahr 1670 im geheimen Vertrag von Dover (in dem der König im Gegenzug für französische Zahlungen sogar zusagte, zum katholischen Glauben zu konvertieren, wenn ihm dies innenpolitisch möglich werde), andererseits musste Karl der antifranzösischen Stimmung im eigenen Land Rechnung tragen und ließ die Verheiratung Maria Stuarts, der ältesten Tochter seines Bruders Jakob, Herzog von York, mit Wilhelm III. von Oranien und somit einem Feind Frankreichs zu. Eine erneute Spaltung der Bevölkerung und der politischen Akteure verstand es Karl indes zu vermeiden, indem er 1660 mit dem Act of Indemnity and Oblivion Republikanern und Anhängern Cromwells Straffreiheit und königliche Gnade zusicherte. Auch eine versprochene (jedoch nie eingelöste) Religionsfreiheit für „tender consciences“ tat ihren Teil dazu, den Übergang zum restaurierten System zu erleichtern. Trotzdem traten Konflikte mit früheren Republikanern und religiösen Minderheiten (vor allem die Quäker (Quaker Act) und die Presbyterianer (Uniformitätsakte)) in der Praxis durchaus auf und schließlich führte das großzügige Vorgehen des Königs in religiösen Fragen zu einer erneuten Furcht vor Sekten und Papisten, die 1673 auch zum Erlass der Testakte führte, die Katholiken von sämtlichen zivilen und militärischen Staatsämtern ausschloss. 

Genährt wurde die antikatholische Stimmung 1678 durch die Aufdeckung einer angeblichen „Papisten-Verschwörung“, die eine Einführung des Katholizismus in England plane, um anschließend den König zu ermorden und London niederzubrennen. Obwohl frei erfunden, sorgte diese Geschichte von Titus Oates für eine Hysterie unter der Bevölkerung und trug zum Ausbruch der sogenannten Exklusionskrise bei. In ihr versuchte die politische Opposition der Whigs, den sich offen zum Katholizismus bekennenden Jakob per Gesetz von der Thronfolge auszuschließen (Exclusion Bill). Er war auch bereits vorher das wohl prominenteste Opfer der Testakte geworden, bei deren Verabschiedung er das Amt des Lord High Admiral hatte abgeben müssen. 

Die Reaktion des Königs erfolgte in Form der Auflösung des Parlaments und einer scharfen Verfolgung der Nonkonformisten, die durch die Aufdeckung der Rye House Plots, einer nach dem Leben des Königs und des Herzog von York trachtenden Verschwörung, zusätzlich verschärft wurde. Das Verhältnis zwischen Hof/Regierung und den Dissenters blieb auch über den Tod Karls II. hinaus gespannt, und als der Herzog von Monmouth, ein illegitimer, im Volk aber sehr populärer Sohn Karls II., Jakob II. mit einer Invasion und einem militärischen Aufstand im Jahr 1685 den Thron streitig machen wollte, befanden sich unter den Rebellen auch viele Whigs. Karl II. starb am 6. Februar 1685, nachdem er noch auf dem Sterbebett zum Katholizismus übergetreten war.

Anders als sein Bruder hielt Jakob II. nichts von Zurückhaltung im Konfessionsstreit. Der überzeugte Katholik setzte sich von Beginn seiner Amtszeit für eine weitgehende Toleranz gegenüber dem Katholizismus in England ein. Dabei dominierte ihn die Vorstellung, dass sich die katholische Kirche bei einer solchen Tolerierung ganz natürlich gegen die anglikanische Kirche und die zahlreichen sektiererischen Abspaltungen durchsetzen werde und auf diesem Wege eine vollständige Restauration im Land erreicht werden könnte. Jakobs Toleranzpolitik erstreckte sich aber auch auf radikale protestantische Denominationen wie die Quäker.

Zuerst einmal galt es jedoch, die Monmouth-Rebellion niederzuschlagen. Dies fiel dem durch Adel und Parlament gestützten Monarchen zu diesem Zeitpunkt indes nicht schwer und obwohl Monmouth beliebt war und seine Invasionstruppen Verstärkung durch zahlreiche Dissenters aus dem Volk erhielten, konnte ihn Jakob am 5. Juli 1685 bei Sedgemoor vernichtend schlagen. Monmouth und etliche seiner Anhänger wurden hingerichtet (Bloody Assizes).
Anschließend wandte sich Jakob erneut dem Versuch zu, die ihm verhassten Whigs zu schwächen und gleichzeitig die Rechte der katholischen Minderheit zu stärken. Eine Reform der Wahlbezirke (Chartered Boroughs) brachte den Tories in den Parlamentswahlen des gleichen Jahres (1685) eine starke Mehrheit und Jakob damit ein Parlament nach seinen Wünschen. Nachdem bereits eine im Januar 1685 erlassene, allgemeine königliche Gefangenenamnestie die Katholiken besonders begünstigt hatte, wandte sich Jakob vom Sommer 1686 an der Aufgabe zu, katholischen Untertanen den Staatsdienst unter Umgehung der 1673 eingeführten Testakte (Suprematseid und Testeid) zu öffnen. Sowohl im Kronrat als auch in der Armee und der ländlichen Verwaltung (d. h. in den Reihen der Friedensrichter) positionierte der König Glaubensbrüder; als der Bischof von London, Henry Compton, hiergegen aufbegehrte, wurde im Sommer 1686 der Court of Ecclesiastical Commission eingerichtet, ein Prärogativgericht zur Kontrolle der Kirche. Dieses rechtlich sehr umstrittene Konstrukt (König und Parlament hatten eine entsprechende Einrichtung 1641 und 1660 noch abgelehnt) nutzte Jakob nicht zuletzt dafür, die beiden Universitäten des Landes (Oxford und Cambridge) zu einer Aufnahme von Katholiken in den Lehrkörper zu zwingen.
Auch unter den Tories sorgte der königliche Kurs für Misstrauen, das durch die Versuche Jakobs, auch über die Niederschlagung des gegen ihn gerichteten Aufstands hinaus ein stehendes Heer zu unterhalten, noch verstärkt wurde. Wenig kompromissbereit zeigten sich die Abgeordneten deshalb auch bei Jakobs Ansinnen, die Testakte aufheben zu lassen und die Öffnung des Staatsdienstes und kirchlicher Institutionen für Katholiken somit zu legitimieren. Als der König damit scheiterte, entschied er sich zwischen November 1686 und März 1687 für eine radikale Wende in seiner Politik, in der er eine Reihe einflussreicher Tories aus ihren Ämtern entließ und sich seinen bisherigen Gegnern, den Whigs und mit ihnen den Dissenters, zuzuwenden begann. Jakobs neues Ziel war es nun, die Whigs für seine Toleranzpolitik zu gewinnen, um mit diesen dann in einem neu gewählten Parlament entsprechende Gesetzesänderungen zu erreichen.
Um sich die politische Unterstützung sowohl der Whigs als auch der Dissenters zu sichern, erließ der König am 4. April 1687 eine Toleranzerklärung (Declaration of Indulgence), die den bisher unterdrückten Sekten und Katholiken eine Reihe von Freiheiten (z. B. Recht auf Versammlung, Ende des 3-Meilen-Banns, etc.) zugestand, und entließ drei Monate später das Parlament. Die mit der Toleranzerklärung verknüpfte Hoffnung Jakobs auf eine weitere Zersplitterung der reformierten Kirche erfüllte sich indes nicht: Stattdessen entfachte der Schritt des Königs eine lebhafte Diskussion darüber, ob die Krone einseitig ein mit dem Parlament gemeinsam verabschiedetes Gesetz außer Kraft setzen dürfe.
Nachdem Jakob bereits zahlreiche Friedensrichterpositionen mit katholischen Anhängern hatte besetzen lassen, folgte im Oktober 1687 eine weitere Entlassungswelle all der „Justices of Peace“, die auf Anfrage eine Aufhebung des Clarendon Codes wie der Testakte pauschal abgelehnt hatten. Trotz dieses Vorgehens blieben die Ziele Jakob II. zu dieser Zeit noch darauf beschränkt, die Stellung der katholischen Bevölkerung zu verbessern und seiner Religion eine langfristige Tolerierung zu ermöglichen. Begründet lag dies nicht zuletzt in dem Wissen, dass wegen seiner Kinderlosigkeit die Krone nach seinem Tode an seine mit Wilhelm von Oranien vermählte Tochter Maria fallen würde und zu weit gehende Rechte der Katholiken nach dem Thronwechsel voraussichtlich rückgängig gemacht werden würden. Dies änderte sich schlagartig, als im November 1687 deutlich wurde, dass Jakob erneut Vater werden würde und somit die Möglichkeit eines katholischen Thronfolgers bestand. Von da an verfolgte der König seine Toleranzpolitik gegenüber den Dissenters und seine Versuche, durch ein von den Whigs dominiertes Parlament Unterstützung für seinen religionspolitischen Kurs zu erhalten, mit sehr viel mehr Nachdruck.

Je stärker der König diesen Kurs jedoch verfolgte, desto stärker rückten die früheren Feinde, Dissenters und anglikanische Kirche, zusammen, verband sie doch gleichermaßen das Interesse an einem protestantischen Thronfolger. Die Geburt des Kronprinzen am 10. Juni 1688, die von großen Teilen der Bevölkerung als Betrug abgetan wurde, schweißte diese Gruppen noch enger zusammen.

Die Revolution von 1688/1689 war keineswegs eine zwangsläufige Konsequenz des Handelns Jakobs II., sondern bedurfte verschiedener Impulse auch auf europäischer Ebene. Trotz des starken Widerstandes gegen seine Toleranzpolitik war 1688 nämlich noch keineswegs sicher, dass es in der Bevölkerung, in der Kirche und beim Adel zu einer starken und gegen das Königshaus gerichteten Reaktion kommen würde. Zu frisch waren noch die Erinnerungen an die Herrschaft Cromwells und die Schrecken des Bürgerkriegs. Erst Jakobs katastrophales Agieren nach der Geburt seines Sohnes ließ die Situation endgültig eskalieren.

Im Mai 1688 erließ der König eine zweite Toleranzerklärung, die zwar wenig Neuerungen gegenüber der ein Jahr zuvor verkündeten Erklärung aufwies, dieses Mal jedoch in zwei Lesungen in den Gottesdiensten des ganzen Landes verkündet werden sollte. Die anglikanische Geistlichkeit wurde damit einem enormen Gewissensdruck ausgesetzt, da eine Weigerung eine Amtsenthebung durch den Court of Ecclesiastical Commission zur Folge haben würde, sich die Erklärung andererseits aber direkt gegen die Rechte der Anglikanischen Kirche richtete. In dieser Situation fassten sich im Mai 1688, kurz vor dem ersten angesetzten Termin der Verkündung, sieben Bischöfe ein Herz und verweigerten in einer Petition an den König die Kanzelverlesung der Deklaration. Jakob entschied sich daraufhin, den Widerständlern den Prozess vor dem Kirchengerichtshof zu machen und ließ sie in den Tower sperren. Dieser Prozess der sieben Bischöfe erwies sich jedoch als fatal. In der Bevölkerung und der Kirche wurden die Sieben wie Märtyrer gefeiert, große Menschenmengen pilgerten zu ihrer Unterstützung zum Tower, Wachleute ließen sich von ihnen segnen und die Verlesung des königlichen Edikts scheiterte desaströs. Auch der Prozess selbst ging für die Krone verloren, denn das Gericht wagte es, die Bischöfe von der Anklage der „aufrührerischen Verleumdung“ freizusprechen.
Das Verfahren sorgte zudem für einen außerordentlich bedeutsamen Stimmungsumschwung unter den Tories und den Anhängern der Staatskirche: Die 1662 zum Gesetz erhobene und von den Anhängern der Krone bislang widerspruchslos akzeptierte Non Resistance Idee (die Ablehnung jeglichen Widerstands gegen den König) erschien nun nicht mehr unumstritten, da selbst in der hohen Geistlichkeit nicht sakrosankt. Der Prozess sorgte somit für ein endgültiges Zusammenrücken der oppositionellen Kräfte sowohl der Whigs und Dissenters als auch der Tories und Anglikaner.
Von da an mehrten sich die Zeichen, dass die Regierung an verschiedenen Orten die politische Kontrolle über das lokale Geschehen zu verlieren begann. Katholische Lord Lieutenants und Friedensrichter beklagten sich in wachsendem Maße darüber, dass die protestantischen Untertanen ihnen den Gehorsam verweigerten. An anderen Orten wurden Gerichtsverfahren gegen Katholiken vorgenommen, die im Widerspruch zu bestehenden Gesetzen staatliche Ämter bekleideten. Gleichzeitig nahm die Zahl gegen das Königshaus gerichteter politischer Flugschriften stark zu, während immer häufiger der Ruf nach einer Unterstützung durch den Schwiegersohn des Königs, Wilhelm von Oranien, laut wurde.

Wilhelm selbst plante bereits seit längerem ein Eingreifen in England, da er durch die pro-französische Politik Jakobs ein weiteres Erstarken der katholischen Kräfte in Europa befürchtete und im vom englischen König forcierten Aufbau eines stehenden Heeres eine unmittelbare Gefahr für seine Interessen sah. Dabei war ihm daran gelegen, den Eindruck einer feindlichen Invasion nicht entstehen zu lassen. Im April 1688 bat er eine Gruppe einflussreicher Angehöriger des Ober- und Unterhauses, unter ihnen auch Compton, ihm eine förmliche Einladung zukommen zu lassen. Gleichzeitig versicherte er sich der Unterstützung des Kaisers Leopold I. und der deutschen Fürsten für sein Unternehmen. Nachdem im Juni der gesunde Thronfolger zur Welt gekommen war, fielen letzte Bedenken und die sogenannten "Immortal Seven" kamen Wilhelms Aufforderung schließlich nach. Wilhelm konnte nun die Invasion vorbereiten, indem er die niederländischen Generalstände überzeugte, ihm Geld und Truppen zur Verfügung zu stellen. Hierbei spielte ihm der im September ausgebrochene Pfälzische Erbfolgekrieg in die Hände, der das geplante Unternehmen als Präventivmaßnahme gegen eine Erneuerung des anglo-französischen Bündnisses, das 1672 beinahe zum Untergang der niederländischen Republik geführt hätte, erscheinen ließ. Viele englische Adlige und Militärs, darunter der spätere Herzog von Marlborough, sagten Wilhelm im Voraus ihre Unterstützung zu.

Im November 1688 landeten Wilhelms Truppen bei Torbay in Südwestengland und wurden dort von der Bevölkerung begeistert aufgenommen. Die ihm entgegen gesandten Truppen Jakobs II., die durch irische Kämpfer verstärkt worden waren, leisteten den Invasoren wenig Widerstand und Wilhelms kluges Vorgehen sowie eine freundliche Haltung gegenüber der Zivilbevölkerung sorgten für ein Überlaufen zahlreicher englischer Offiziere und Soldaten. Zusätzlich schwächte sich Jakob selbst, indem er die Königin und seinen Sohn nach Frankreich bringen ließ und selbst zu fliehen versuchte. Erst nach dem zweiten Versuch gelang ihm dies im späten Dezember 1688, nicht ohne das Große Staatssiegel in der Themse zu versenken und den Aufständischen Vergeltung anzudrohen. Der Weg für eine Thronbesteigung Wilhelms war geebnet.

Im Januar 1689 trat das in aller Eile neu gewählte Convention Parliament zusammen und setzte Wilhelm III. von Oranien und Maria II. mit dem Argument in die Thronfolge ein, der Thron sei vakant geworden. Damit wurde ein weiterer Streit um das Widerstandsrecht umgangen, dessen Befürworter sich nicht auf einen Konflikt mit den wieder gestärkten Tories einlassen wollten, und die Schuld am Bruch in der Thronfolge dem letzten Stuartkönig zugeschrieben. Die verfassungspolitische Regelung der Thronfolge, nach der Marias jüngere Schwester Anne Wilhelm und Maria auf den Thron folgen sollte, wurde im Frühjahr 1689 in der Declaration of Rights vorgenommen. Außerdem reservierte sich das Parlament für alle Zukunft das Recht, gemeinsam mit den königlichen Gerichtshöfen den Inhalt und insbesondere die Grenzen der königlichen Prärogative zu bestimmen. Der Krone wurde ferner das Recht bestritten, missliebige Richter in Zukunft einfach des Amtes zu entheben, gemeinsame Beschlüsse des Parlaments und der Krone einseitig außer Kraft zu setzen und schließlich eine Stellung außerhalb des Gesetzes einzunehmen. Durch die Zustimmung der Krone wurde die "Declaration of Rights" im Dezember 1689 in entschärfter Form zur Bill of Rights und vervollständigte damit auf der Verfassungsebene das Prinzip der Rechtsstaatlichkeit. Allerdings wäre es falsch, hierin die Begründung einer konstitutionellen Monarchie in England zu sehen. Diese basierte vielmehr auf Konzessionen, die nicht 1689, sondern erst im Zuge der nächsten zwölf Jahre erkämpft wurden.

Ende 1689 nahm das Parlament auch noch ein zweites Anliegen auf, das durch die Vorkommnisse in den vorhergegangenen Jahren unausweichlich geworden war: Mit der Toleranzakte wurde eine Regelung der Religionsfrage vorgenommen, die den Dissenters nun endlich gewisse Rechte und Freiheiten der Ausübung ihrer Religion zugestand, diese jedoch sowohl an den Huldigungs- als auch den Suprematseid knüpfte. Katholiken, Juden und Anti-Trinitaristen blieben von der Regelung jedoch weiterhin ausgeschlossen.
Während Wilhelms Thronbesteigung in England überwiegend positiv aufgenommen wurde, traf er in den schottischen Highlands auf massiven Widerstand der Anhänger Jakobs (Jakobiten). Eine erste Erhebung unter Führung von „Bonnie Dundee“ erfolgte bereits während der Tagung des Convention Parliaments und konnte erst im Mai 1690 niedergeschlagen werden. Das harte und kompromisslose Vorgehen seiner schottischen Vertreter (z. B. beim Massaker von Glencoe (1692)) und seine antifranzösische Politik bescherten dem König zusätzlich viel Misstrauen bei seinen nördlichen Untertanen.
Noch problematischer erwies sich die Situation in Irland, in dem Jakob mit einem französischen Heer am 12. März 1689 bei Kinsale gelandet war. Unterstützt von der katholischen Bevölkerung zog er erst nach Dublin und dann nach City of Londonderry, einer protestantischen Hochburg, die er erfolglos belagerte. Am 12. Juli des Folgejahres konnte Wilhelm Jakob in der Schlacht am Boyne entscheidend besiegen und in der Folge bis 1691 die gesamte Insel zurückerobern. Jakob zog sich erneut in sein französisches Exil zurück, aus dem er nie wiederkehrte. Die Bewegung der Jakobiten, die erst Jakob, dann seinen Sohn James ("the Old Pretender") und schließlich dessen Sohn Charles ("Bonnie Prince Charlie") als Thronfolger einsetzen wollten, blieb bis 1746 eine Bedrohung für die Nachfolger Wilhelms.

Wilhelm von Oranien und Maria wurden am 11. April 1689 in London in der ersten und bis heute einzigen Doppelkrönung gemeinsam gekrönt. Durch diese Doppelkrönung erhoffte man sich eine Legitimitätsteigerung von Wilhelm von Oranien, da im strengen Sinne seine Frau Thronfolgerin war. Bei der Krönungszeremonie wurden ein paar wesentliche Neuerungen eingeführt. Das House of Commons nahm an der Zeremonie teil. Sie konnten von ihrer erhöhten Tribüne das Geschehen gleichsam überwachen. Der Krönungseid wurde so verändert, dass er die Monarchen an das Parlament band. Mit der neuen Eidformel versprachen sie "… to govern the people of this kingdom of England … according to the statutes in parliament agreed on, and the laws and customs of the same". Schließlich wurde die Krönung nicht vom Erzbischof von Canterbury vollzogen – dieser weigerte sich, da er immer noch Jakob II. als legitimen König betrachtete –, sondern vom Bischof von London.

Wilhelm III. zog England in das Kriegsgeschehen Kontinentaleuropas hinein und legte mit seiner Politik den Grundstein für die Entwicklung des Staates zur europäischen Großmacht und später zur Weltmacht. Mit seinem Coup stieg Wilhelm außerdem zu einem der Führer des protestantischen Widerstands gegen die Hegemonialbestrebungen des französischen Königs auf und fügte diesem im Frieden von Rijswijk eine empfindliche Niederlage zu. Auch im Spanischen Erbfolgekrieg erwies sich England als eine treibende Kraft, die es verstand, die französische Koalition zurückzudrängen.
Die militärischen Unternehmungen Wilhelms III. hatten zudem einen erheblichen Nebeneffekt: Die hohen Kosten der Kriegsführung machten eine gute Zusammenarbeit mit dem Parlament, dem die Bewilligung der Steuern und Einnahmen oblag, zwingend notwendig. Das Parlament ließ sich im Gegenzug von der Krone zahlreiche Konzessionen bewilligen. Den Auftakt machte 1694 die Erneuerung des Triennial Act, die eine Einberufung des Parlaments im Drei-Jahres-Rhythmus und eine maximale Dauer des jeweiligen Parlaments von drei Jahren vorschrieb. Am Vorabend des Spanischen Erbfolgekriegs folgte mit dem Act of Settlement die Festlegung einer protestantischen Erbfolge, womit sich das Parlament erstmals das Recht erstritt, die Erbfolge zu regeln (dieses Recht wurde 1707 mit dem Regency Act bestätigt). Außerdem verfügte der Act of Settlement erneut die Unabhängigkeit der Gerichte des Landes von der Regierung und trug so zur Entstehung der konstitutionellen Monarchie bei. Parallel förderte das Parlament die Entstehung einer breiten politischen Öffentlichkeit dadurch, dass es das bestehende System der Pressezensur 1695 nicht erneuerte. Die Zahl der Wahlberechtigten stieg – trotz der Erhöhung der Qualifikationen für eine Wahlberechtigung durch die Tories - an.

Langfristig erwies sich der Umsturz als bedeutend nicht nur für die Geschichte Englands, sondern für die Entwicklung des Parlamentarismus weltweit. Da sich seit dem 18. Jahrhundert immer mehr Staaten das englische Regierungssystem zum Vorbild nahmen, prägten die in der Glorious Revolution wie die zuvor schon im Englischen Bürgerkrieg aufgeworfenen Verfassungsfragen die politische Theorie weit über Großbritannien hinaus. Ähnlich langfristig waren die Auswirkungen der Glorious Revolution auf das britische Finanzsystem. Die britische Krone hatte im Vergleich zu anderen europäischen Monarchien deutlich weniger Landbesitz. Mit der Glorious Revolution verstärkte sich die unmittelbare Kontrolle des Parlaments über die Ausgaben des britischen Königshauses. Sie führte letztlich 1717 zur Einführung des Goldstandards, was dem Lord Treasurer die Möglichkeit nahm, durch einen verringerten Edelmetallgehalt der Münzen eine versteckte Abwertung der britischen Währung vorzunehmen. Eine verstärkte Professionalisierung erfolgte auch bei der Besetzung öffentlicher Ämter. Ämter wie etwa die des Steuereintreibers wurden auf Gehaltsbasis vergeben und nicht wie in Frankreich versteigert, was zu einer größeren Steuergerechtigkeit führte. In der Mitte des 18. Jahrhunderts verfügte London aufgrund dieser soliden Form der Staatswirtschaft über eine funktionierende Börse, auf der vor allem Anleihen der britischen Regierung gehandelt wurden. Der Wertpapierhandel zog auch ausländische Investoren an. So investierten vor allem Niederländer in britische Staatspapiere. Großbritannien unterschied sich damit auch in seinem Wirtschaftssystem von europäischen Monarchien und legte damit die Basis für eine wirtschaftliche und politische Entwicklung, die sehr viel ruhiger als auf dem europäischen Festland verlief. 

Einige der diskriminierenden Regelungen gegen britische Katholiken endeten mit der Katholikenemanzipation. Bis heute machen sich die Folgen der „Glorious Revolution“ aber auch im Nordirlandkonflikt zwischen Katholiken und Protestanten bemerkbar.



</doc>
<doc id="11938" url="https://de.wikipedia.org/wiki?curid=11938" title="Yggdrasil">
Yggdrasil

Yggdrasil, altnordisch "Yggdrasill", auch: Weltesche, ist in der nordischen Mythologie der Name einer Esche, die als Weltenbaum den gesamten Kosmos verkörpert. Andere Namen dieses Baums waren wohl Mimameid oder Lärad.

Nachdem die Götter den Ur-Riesen Ymir getötet haben, erschaffen sie dem Mythos nach aus seinem Leichnam alle existierenden Dinge. Die Weltenesche Yggdrasil ist der erste Baum, der wächst. Er ist der größte und prächtigste Baum der Welt. Seine Äste breiten sich über alle neun Welten aus und erstrecken sich über den Himmel. Ein Adler ohne Namen sitzt im Geäst, zwischen seinen Augen ist ein Habicht, der Vedrfölnir genannt wird.

Yggdrasil hat drei große Wurzeln, von denen eine nach Jötunheim, dem Land der Riesen, wächst, wo sich auch Mimirs Brunnen befindet. Die andere Wurzel führt nach Niflheim zur Quelle Hvergelmir, wo der Drache Nidhöggr an ihr nagt. Die dritte Wurzel findet sich in der Nähe von Asgard. Das Eichhörnchen Ratatöskr klettert immer zwischen der Baumkrone und Wurzeln hin und her und verbreitet dabei üble Nachrede vom Adler bis zum Drachen. Vier Hirsche namens Dain, Dwalin, Dunneir und Durathror fressen die Triebe der Weltenesche ab. Die zwei Schlangen Goin und Moin, die von Grafwitnir (Grabeswolf) abstammen, nagen an den Wurzeln von Yggdrasil.

Unter den Zweigen des Baums halten die Götter Gericht. Am Fuße Yggdrasils liegt auch der Urdbrunnen, an dem die drei Nornen Urd (das Gewordene), Werdandi (das Werdende) und Skuld (was da kommen soll) ihren Sitz haben, die das Schicksal der Menschen bestimmen. Wenn Yggdrasil zu beben (oder zu welken) beginnt, naht das Weltenende Ragnarök.

In den eddischen Literaturen werden bestimmte mythische Orte einer der Welten zugeordnet. An keiner Stelle wird jedoch aufgezählt, wie die neun Welten, die Yggdrasil verkörpert, heißen. Insgesamt werden mehr Weltennamen als neun genannt. Die unten stehende Aufteilung in drei Ebenen und die Benennung der neun Welten ist deswegen eine rein neuzeitliche Rezeption, die in der Forschung so nicht mehr vertreten wird, aber außerhalb davon noch viel Anklang findet.

1. Oberwelt


2. Erde


3. Unterwelt


Yggdrasil, die Weltenesche, ist die Verkörperung der Schöpfung als Gesamtes: räumlich, zeitlich und inhaltlich. Er ist der "Weltenbaum", weil er im Zentrum der Welt steht und alle Welten miteinander verbindet. Als "Weltachse" (axis mundi) verbindet er die drei Ebenen Himmel, Mittelwelt und Unterwelt. Als "Himmelsstütze" stützt er das Himmelsgewölbe. Die Edda nennt ihn auch den "Maßbaum". Die Welt reicht nur so weit, wie seine Zweige und Wurzeln reichen, und die Schöpfung besteht nur so lange, wie er besteht: ein Sinnbild der "Raumzeit". Yggdrasil ist auch ein "Sinnbild des Lebens" an sich und von Vergehen und Werden, der "Wiedererneuerung des Lebens". Die Tiere am Baum nehmen von seiner Lebenskraft, die drei Nornen besprengen ihn mit dem heiligen Wasser des Urdbrunnens und schenken ihm immer wieder neue Lebenskraft. Da Yggdrasils Leben sich immer wieder erneuert oder weil Yggdrasil immergrün ist, ist die Weltenesche auch ein Sinnbild der "Unsterblichkeit". Durch Odins Selbstopfer wird Yggdrasil zum "Opferbaum". Da Odin sich selbst am Baum aufhängt, um das geheime Wissen bei den Wurzeln Yggdrasils zu erlangen, kann man in Yggdrasil auch einen "Wissensbaum" sehen, über den man zum geheimen Wissen gelangt.

Über die Rolle der Weltenesche Yggdrasil im Kult ist nichts bekannt. Angesichts der zahlreichen germanischen Baumkulte ist es wahrscheinlich, dass bei den Germanen bestimmte physische Bäume den mythischen Weltenbaum darstellten. Sie müssen aber nicht zwingend eine Esche gewesen sein. Die Donareiche, die Irminsul oder der heilige Baum im schwedischen Uppsala, von dem Adam von Bremen im 11. Jahrhundert berichtet, könnten kultische Entsprechungen von Yggdrasil gewesen sein. Sie sind zusammenhängend mit Yggdrasil zu betrachten. Von den baltischen Prußen ist eine solche Kultstätte in Romove überliefert.

Nach der Edda ist Yggdrasil der Thingplatz der Götter. Hier versammeln sie sich, beraten sich und halten Gericht. Da die Verhältnisse in der Götterwelt oft die irdischen Verhältnisse spiegeln, kann man annehmen, dass das germanische Thing an oder in der Nähe einer Verkörperung des Weltenbaums stattfand. Vermutlich wurde es von Ritualen begleitet. In germanischer Zeit waren das Religiöse und das Rechtliche noch nicht voneinander getrennt. Die Gerichtsbäume des Mittelalters (in Deutschland Eichen und Linden) könnten ein Nachhall der alten Zeit sein.

Vom heiligen Baum in Uppsala werden Tier- und Menschenopfer berichtet. Die Irminsul wird als Himmelsstütze und Weltachse gedeutet.

Der Name Yggdrasil, altnordisch "Yggdrasill", setzt sich wohl zusammen aus altnordisch "yggr" „Furcht“, „Schrecken“, „Schrecklicher“ (siehe Liste der Beinamen Odins)‘ und altnordisch "drasill" „Pferd“.

Einige Interpreten vermuten, dass Yggdrasil „Pferd des Schrecklichen“ heißt, und dass damit die Weltesche selbst als Odins Pferd bezeichnet wird. Nach der Hávamál hing ein Wesen, das man meist als Odin identifiziert, neun Nächte in einem Selbstopfer an einem Baum, der oft mit dem Weltenbaum gleichgesetzt wird. Noch in späterer Zeit sagten die Deutschen, Engländer und die nordgermanischen Völker zum Galgenbaum "Ross" und zum Gehängten "Reiter".

Der isländische Gelehrte Eirikr Magnússon meinte jedoch, Yggdrasil sei das Reittier Odins und nicht der Baum selbst gewesen. Der eigentliche Weltenbaum habe "askr Yggdrasil" geheißen, also der Baum, an den Odin sein Pferd bindet.

Eine weitere Ansicht führt altnordisch "yggr" auf seine eigentliche Bedeutung ‚Schrecken‘ zurück und übersetzt den Baumnamen mit „Schreckensbaum“, „Galgen“ Damit würde wiederum Odins Selbstopfer am Galgen des Weltenbaums zum Ausdruck gebracht werden.

Grundsätzlich weiter zurückreichende Deutungen beruhen auf Vergleichen zu anderen indogermanischen, religiös-kultischen Vorstellungen. Danach bedeutet Yggdrasil „Eibensäule“. Altnordisch "yggia" wäre demnach von germanisch "*igwja" „Eibe“ und altnordisch "drasill" von indogermanisch "*dher-" „stützen“ abzuleiten.

Die geschichtlichen Wurzeln des nordischen Weltenbaums reichen mindestens zurück bis in indogermanische Zeit, da der Weltenbaum zum mythologischen Fundus vieler indogermanischer Völker gehört: Balten (die Eiche Austras koks), Inder (der Feigenbaum Asvattha), Perser (Simurgh-Baum) und Slawen – gegebenenfalls auch der Baum der Hesperiden der Griechen. In diesen Mythologien findet sich oft ein Greifvogel an der Spitze und/oder eine Schlange an den Wurzeln des Baums.

Heutzutage wird kaum noch die Meinung vertreten, dass Yggdrasil eine spätheidnische Entlehnung des mittelalterlichen, christlichen Kreuzbaums ist. Eher glaubt man, dass die Vorstellung des christlichen Kreuzbaums durch heidnische Vorstellungen beeinflusst wurde.

In den schamanischen Kulturen des eurasischen Nordens finden sich ähnliche Vorstellungen des Weltenbaums, wie sie von Yggdrasil berichtet werden. Das Selbstopfer Odins an Yggdrasil, sein enger Bezug zur Ekstase und sein achtbeiniges Pferd Sleipnir sind Merkmale, die dem klassisch sibirischen Schamanismus sehr nahestehen.

Man kann deswegen der Ansicht sein, dass der nordische Weltenbaum aus einer Zeit stammt, in der er von Schamanen in ihrer praktischen Arbeit genutzt wurde. Kennzeichnend für Weltenbäume des sibirischen Schamanismus ist – bzw. war – die Vorstellung, dass er die Welt in ihrer Gesamtheit darstellt. Somit war er zugleich auch der erste aller Bäume. Er stand im Zentrum der Schöpfung und verband die drei Ebenen Himmel, Erde und Unterwelt (und alle sonstigen Welten, die es gibt) miteinander. Meist war er mit einer Muttergottheit und dem Martyrium verbunden. Sieht man in der Norne Urd eine alte Muttergottheit, so vereinigt Yggdrasil all diese grundlegenden Merkmale in sich. Unterschiedlich war im Übrigen in den verschiedenen eurasischen Kulturen die Baumart des Weltenbaums.

Schamanen nutzten den Weltenbaum in ihrer Vorstellung für ihre Arbeit zum „Reisen“ in die jenseitige Geisterwelt, damit sie dort Angelegenheiten der Menschen erledigen konnten. An einer Entsprechung des Weltenbaums wurden Schamanen zum Teil auch initiiert. Odins Selbstopfer (Martyrium) an Yggdrasil, um an das geheime Wissen (der Runen) in der Tiefe zu gelangen, kann durchaus als schamanischer Initiationsritus aufgefasst werden.

In der Forschung findet man häufig die Auffassung, dass der westnordische Weltenbaum in früherer Zeit ursprünglich keine Esche, sondern eine Eibe ("Taxus sp.") war. Diese Meinung stützt sich vor allem auf die Schilderung Adams von Bremen aus dem 11. Jahrhundert über den heiligen Baum, der im Tempelbezirk von Uppsala in Schweden stand:

Die fast deckungsgleiche Beschreibung des mythischen Weltenbaums in der Lieder-Edda legt nahe, dass der heilige Baum zu Uppsala den mythischen Weltenbaum verkörperte. So heißt es über Yggdrasil in der Lieder-Edda:

Über den Baum Mimameid, den man mit Yggdrasil gleichsetzt, wird gesagt:

Alle Merkmale beider Beschreibungen stimmen überein bis auf eins. Die in Europa heimische Gemeine Esche ("Fraxinus excelsior") ist nicht immergrün. (Wenngleich es durchaus auch immergrüne Exemplare der Eschen-Gattung gibt.) Die einzige heimische immergrüne Baumart, die in diesem Landstrich Schwedens noch wachsen konnte, war die Eibe – ein Nadelbaum. Dass der mythische Weltenbaum als Nadelbaum gedacht war, dafür spricht unter anderem auch die zweimalige Verwendung des Wörtchens "barr" in der Edda:

Das liegt daran, dass man altnordisch "barr" zwar mit "Baum" oder "Blatt" übersetzen kann, aber genauso gut auch mit "Nadelbaum" oder "Nadel".

Die Verwandlung von Eibe zu Esche erklärt man sich zum Beispiel dadurch, dass beide Bäume auf Island, wo die Eddatexte abgefasst wurden, nicht wuchsen und die Baumarten in Unkenntnis verwechselt wurden. Die Nordgrenze des Verbreitungsgebiets beider Bäume in Europa ist Südskandinavien.

Es gibt aber auch Argumente, die gegen eine "Eibe" Yggdrasil sprechen. Die Schilderung des heiligen Baums in Uppsala, die Adam von Bremen aus zweiter oder dritter Hand hatte, kann mythologisch beeinflusst gewesen sein. Darauf deutet die fast deckungsgleiche Beschreibung zwischen Kultbaum und mythischem Baum hin, insbesondere die Bemerkung Adams von Bremen, dass niemand wisse, welcher Art der Baum sei. Auch wenn der heilige Baum in Uppsala ein Nadelbaum gewesen wäre, so wäre damit noch nicht gesagt, dass man sich auch Yggdrasil als Nadelbaum vorstellte. Denn schließlich kann jede Baumart den mythischen Weltenbaum vertreten. Genauso gut kann Yggdrasil jeden irdischen Baum repräsentieren, sofern man Yggdrasil als frühzeitlichen Schöpfungsmythos versteht.




</doc>
<doc id="11940" url="https://de.wikipedia.org/wiki?curid=11940" title="Wasserkefir">
Wasserkefir

Wasserkefir ist ein kohlensäurehaltiges Gärgetränk, das mit Hilfe einer Kefirkultur ähnlich der Kefirknollen angesetzt wird. Diese besteht aus Hefen ("Saccharomyces", "Candida") und Milchsäurebakterien. Das entstehende Getränk hat geschmacklich (süß-säuerlich) und vom Aussehen her (gelblich, trüb) eine gewisse Ähnlichkeit mit Kombucha. Wasserkefir ist nicht im Handel erhältlich, kann aber selbst hergestellt werden.

Dem mittels dieser Kefirkultur hergestellten Getränk werden verschiedenste Heilwirkungen nachgesagt, die jedoch wissenschaftlich nicht belegt sind. Bei einem Ansatz im Haushalt werden zusätzlich Zucker und als Stickstoffquelle Trockenfrüchte, zum Beispiel Feigen oder Rosinen, benötigt. Ansäuern mit einer Zitrone verhindert das Wachstum von Schimmelpilzen. Nach der Gärung bleibt noch Restzucker übrig. Der Alkoholgehalt beträgt je nach Ansetzdauer 0,2–2 %.

Die Kefirkultur vermehrt sich pro Ansatz um ca. 25–75 %, allerdings nur, wenn hartes Wasser verwendet wird. Das darin enthaltene Calcium wird für die Vermehrung benötigt. Verwendet man sehr weiches Wasser, nimmt die Menge der Kefirkultur in den ersten Ansätzen noch ein wenig zu, stagniert dann aber. Durch Zugabe von Calcium in löslicher Form, etwa als Calciumgluconat oder -citrat, kann dieser Mangel ausgeglichen und eine starke Vermehrung der Kefirkultur bewirkt werden.

Die an Bitter Lemon erinnernde Bitterkeit des Kefirgetränkes kann durch Beigabe von Zitronenschale erreicht werden. Die im weißen Teil der Schale enthaltenen Bitterstoffe werden dann während der Gärung herausgelöst.

Wegen der weißlich durchscheinenden Kefirkultur, deren Aussehen Kristallen ähnelt, sind gebräuchliche und teilweise irreführende Benennungen der Kefirkultur auch: "Japankristall", "Japanische Kristallalge", "Japanische Meeresalge", "Himalaya-Kristallalge", "Tibi", "Tibicos" oder "lebende Körnchen (graines vivantes)".

Die einzelnen Kefirkristalle erreichen selten Größen über einem Zentimeter. Wenn in ihrem Innern Kohlendioxid entsteht, werden sie „gesprengt“, wobei die Bruchstücke weiterwachsen.

Wasserkefirferment ist ein weißes Pulver, das in Reformhäusern angeboten wird. Im Gegensatz zur „echten“ Kefirkultur handelt es sich hierbei um weitaus kurzlebigere Mikroorganismen, die sich bereits nach dem 10. Ansatz verbraucht haben. Bei der Zubereitung des Getränks gibt es keine Unterschiede.

Mindestens zwei wissenschaftliche Veröffentlichungen beschreiben ein natürliches Vorkommen der Tibi. Auf der Oberfläche von Opuntien in Mexiko bilden sie harte Körner, die sich in einer wässerigen Zuckerlösung vermehren und dabei „Kristalle“ bilden, die mit den bekannten Wasserkefirkristallen identisch sind. 

Wasserkefir reift sehr gut in einem verschlossenen Gefäß, beispielsweise in einer sauberen 1-Liter-Milchflasche, das die entstehende Kohlensäure nicht entweichen lässt. Allerdings ist darauf zu achten, dass der Druck in dem Gefäß nicht zu groß wird, da dieses sonst platzen könnte.


</doc>
<doc id="11943" url="https://de.wikipedia.org/wiki?curid=11943" title="Endosymbiontentheorie">
Endosymbiontentheorie

Die Endosymbiontentheorie ( endo ‚innen‘ und συμβίωσις "symbiōsis" ‚Zusammenleben‘) besagt, dass Eukaryoten dadurch entstanden sind, dass prokaryotische Vorläuferorganismen eine Symbiose eingegangen sind. Demnach sind chemotrophe und phototrophe Bakterien von anderen prokaryotischen Zellen (möglicherweise Archaeen) durch Phagozytose aufgenommen worden, sie wurden jedoch nicht verdaut, sondern lebten im Innern weiter und wurden dadurch zu Endosymbionten. Später haben sich die Endosymbionten zu Zellorganellen in ihren Wirtszellen entwickelt. Die Komplexe aus den Wirtszellen und den darin befindlichen Organellen sind Eukaryoten. Die Zellorganellen, die auch heute noch viele Merkmale von Prokaryoten tragen, sind Mitochondrien und Plastiden. Komplexe pflanzliche, tierische und somit auch menschliche Zellen haben damit ihren Ursprung in der Verschmelzung von Prokaryoten (vgl. Abb.). Es gibt jedoch auch Eukaryoten ohne derartige Organellen, wobei diskutiert wird, ob diese Zellbestandteile stammesgeschichtlich sekundär verloren gingen. Eukaryoten ohne solche Organellen können weder Zellatmung noch Photosynthese betreiben.

Der Gedanke der Endosymbiontentheorie ist erstmals von dem Botaniker Andreas Franz Wilhelm Schimper im Jahr 1883 veröffentlicht worden, der damit die Entstehung der Chloroplasten zu erklären versuchte. Die Hypothese wurde erneut um 1905 von dem russischen Evolutionsbiologen Konstantin Sergejewitsch Mereschkowski aufgegriffen. Doch erst 1967 mit der Veröffentlichung von Lynn Margulis wurde sie bekannter.

Vereinfacht gesprochen besagt die Theorie, dass im Laufe der Entwicklung des Lebens einzellige Lebewesen durch ein anderes einzelliges Lebewesen aufgenommen und zu Bestandteilen der Zelle eines so entstandenen höheren Lebewesens wurden. Im Laufe der Evolution entstanden so immer komplexere Lebewesen. Auch Bestandteile menschlicher Zellen gehen ursprünglich auf einzellige Lebewesen zurück, die zu einem Bestandteil der Zellen wurden.

Die Endosymbiontentheorie geht davon aus, dass Mitochondrien und Plastiden sich aus eigenständigen prokaryotischen Lebewesen entwickelt haben. Im Zuge des Evolutionsprozesses sind diese Einzeller eine Endosymbiose mit einer anderen Zelle eingegangen, das heißt, sie leben in ihrer Wirtszelle zum gegenseitigen Vorteil. Auch heute noch kann man beobachten, dass amöboide Einzeller (also solche mit einer „weichen“ Membran) Cyanobakterien aufnehmen, ohne sie zu verdauen.

Das Zusammenspiel der beiden zellulären Organismen hat sich dann im Verlauf der Evolution zu einer gegenseitigen Abhängigkeit entwickelt, in der keiner der beiden Partner mehr ohne den anderen überleben konnte, das heißt, es entstand eine Symbiose. Diese wird Endosymbiose genannt. Die Abhängigkeit geht so weit, dass die Organellen Teile ihres (nicht mehr benötigten) genetischen Materials verloren haben oder die entsprechenden Gene teilweise in das Kern-Genom integriert wurden. Einzelne Protein-Komplexe in den Organellen, wie z. B. die ATP-Synthase, werden so zum Teil aus kerncodierten, zum Teil aus mitochondrial codierten Untereinheiten zusammengesetzt.

Analysen der Genome deuten darauf hin, dass Plastiden von Cyanobakterien abstammen, während Mitochondrien von aeroben Proteobakterien abstammen. Diese Form der Endosymbiose zwischen einem Eukaryoten und einem Prokaryoten wird als primäre Endosymbiose bezeichnet. Entstand das Zellorganell durch die Aufnahme eines Eukaryoten, der bereits ein primäres Endosymbioseereignis erfahren hat, wird dies als sekundäre Endosymbiose bezeichnet.

Primäre Plastiden sind von zwei Hüllmembranen umgeben, die den beiden Membranen des aufgenommenen Cyanobakteriums entsprechen, während die bei der Phagocytose entstehende darum herumliegende, ursprüngliche dritte Membran nicht mehr vorhanden ist. So gibt es drei Typen von primären Plastiden und somit drei Linien von autotrophen Organismen:

Sekundäre Plastiden verfügen über drei oder sogar vier Hüllmembranen. Es ist kein Fall bekannt, in dem eine Aufnahme eines Glaucophyten zu einer sekundären Endosymbiose geführt hätte. Dagegen existiert eine Fülle von Organismengruppen, die eine Rotalge aufgenommen haben und sie in unterschiedlichem Maße reduziert haben. Einige Autoren gehen davon aus, dass dieses Ereignis nur einmal in der Evolution stattgefunden hat, und definieren so das Monophylum der Chromalveolata. In diese Gruppe gehören die Braunalgen, Gelbgrünalgen, Goldalgen, Cryptophyceen, Haptophyceen (Kalkalgen), und die Apicomplexa (z. B. Malaria-Erreger "Plasmodium").

Auch die sekundäre Endosymbiose zwischen Grünalgen und Eukaryoten ist bekannt. So wird angenommen, dass die Euglenozoa und die Chlorarachniophyta unabhängig voneinander primäre Endosymbionten in sich aufgenommen haben.

Es gibt einige Protozoen, die keine Mitochondrien (und keine Plastiden) besitzen („Archezoa“). Zunächst wurde angenommen, sie seien primitiv und unmittelbar aus der urtümlichen Wirtszelle der Endosymbionten hervorgegangen. Dies ist vermutlich falsch, da ihre DNA Sequenzen enthält, die eindeutig mitochondrialen Ursprungs sind. Wahrscheinlich haben alle amitochondriaten Eukaryoten ihre Mitochondrien sekundär verloren.






</doc>
<doc id="11944" url="https://de.wikipedia.org/wiki?curid=11944" title="Posaune">
Posaune

Die Posaune ist ein tiefes Blechblasinstrument, das wegen seiner weitgehend zylindrischen Bohrung (enge Mensur) zu den Trompeteninstrumenten zählt. Der Ton wird mittels Anregung der natürlichen Resonanzen des Instruments durch Lippenschwingungen des Bläsers erzeugt.

Die Posaune besteht aus einem S-förmig gebogenen zylindrischen Rohr, in das an einem Ende ein Mundstück eingesetzt wird und das sich am anderen Ende zum "Schalltrichter" öffnet (auch: "Stürze" oder "Schallbecher").

Alle Bauformen der Posaune werden mit einem Kesselmundstück gespielt.

Dank ihrer Mensur klingen die Töne der Posaune härter als bei den Instrumenten der Bügelhornfamilie, die ein sanfteres Klangvolumen zeigen. Die Mensur wirkt sich auch auf die Schallenergie aus. So wird der Schall bei einer Posaune stärker gebündelt als beispielsweise bei einem Tenorhorn.

Der Ton entsteht wie bei allen Blechblasinstrumenten durch die Vibration der Lippen des Spielers am Mundstück zur Anregung stehender Wellen in der Luftsäule des Instruments.

Die dynamische Spannbreite liegt bei etwa 37 dB. Die Schallpegel nehmen in allen Dynamikstufen zur Höhe hin zu. In 16 Metern Entfernung erreichen die Werte beim pp 40 dB in der Tiefe und 70 dB in der Höhe. Die Werte im ff liegen bei 85 dB und 100 dB. Am Ohr des Spielers erreicht das Instrument Spitzenwerte von über 115 dB.

Über die Entstehung der Posaune gibt es nur wenige Daten. Zu den ältesten Existenz-Belegen des Instruments zählt ein englisches Dokument von 1495 und ein Gemälde von Matteo di Giovanni, der 1495 verstorben ist. Die Posaune ist neben der Violine eines der ältesten voll chromatisch spielbaren Orchesterinstrumente.

Weil eine Naturtrompete (Tromba) mit dem Grundton b eine unhandliche Länge von etwa 2,80 Meter (9 Fuß) aufweist, wurden die Instrumente in S-Form gebogen, gerollt oder in „Brezelform“ hergestellt. Der englische, französische und italienische Name des Instruments "trombone" bedeutet wörtlich nichts anderes als „große Trompete“. Der deutsche Name entwickelte sich aus der altfranzösischen Bezeichnung "„buisine“".

Bis etwa 1700 wurde die Posaune als genau intonierbares Blasinstrument häufig im Ensemble mit Sängern und Streichern, aber auch eigenständig eingesetzt. In Bläserensembles dieser Zeit ("„Alta capella“", Stadtpfeifer) wurden neben anderen damals gebräuchlichen Instrumenten wie Zinken, Schalmeien und Zugtrompeten eben auch (Renaissance-)Posaunen gespielt. Sie traten in der Regel bei gesellschaftlichen, mitunter auch kirchlichen, jedoch weniger bei höfischen Anlässen auf und griffen mangels spezieller Kompositionen häufig auf Vokalmusik zurück, auch zur Tanzbegleitung.

Der europäische Adel bevorzugte traditionell Saiten- und Streichinstrumente zur Unterhaltungsmusik, eine Präferenz, die mit steigendem Wohlstand auch von bürgerlichen Schichten zunehmend imitiert wurde. Dadurch kam es im 17. Jahrhundert zu einem gewissen Rückgang der Bläsermusik, im Zuge dessen die Posaune in weiten Teilen Europas aus der Mode geriet. So wurde sie beispielsweise in italienischen Kanzonen und Sonaten ab 1630 kaum mehr besetzt. Nicht zuletzt infolge des Dreißigjährigen Krieges ist über die Musik und die Komponisten dieser Zeit heute jedoch weniger bekannt als über andere Epochen. Johann Sebastian Bach sah nur in 15 seiner überaus zahlreichen Kantaten Posaunen vor und setzte sie lediglich zur Verdoppelung der Chorstimmen ein.

Eine bemerkenswerte Ausnahme dieses Trends bildet das Oratorium "La Resurrezione" des jungen Georg Friedrich Händel aus dessen Zeit in Italien. Händel griff auch in seinen späteren Oratorien "Saul" und "Israel in Egypt" von 1739 wieder auf Posaunen zurück und inspirierte damit möglicherweise Christoph Willibald Gluck dazu, das Instrument in seinem Ballett "Don Juan" (1761) sowie "Orfeo ed Euridice" (1762) und seinen späteren Opern einzusetzen. Gluck wiederum bildete eine Inspiration für Mozart. 

Dieser komponierte noch für die "Renaissanceposaune" (auch "„Sackbutt“" genannt), die er nicht nur in einigen Opern (unter anderem der Zauberflöte) verwendete, sondern etwa auch in seinem Requiem. Es enthält mit dem "Tuba mirum" eine der bekanntesten solistisch geprägten Orchesterpassagen für die Tenorposaune.

Die ersten „modernen“ Posaunen, die auch das metallische Forcieren des Klanges ermöglichten, wurden erst nach Mozarts Tod gebaut. Erst seit dieser Zeit wird die Posaune auch im Satz mit Trompeten eingesetzt.

Das klassische Sinfonieorchester ging aus dem Opernorchester hervor, doch obgleich die Posaune bereits im 18. Jahrhundert von einigen weniger bedeutenden Komponisten einbezogen worden war, blieb es Beethoven vorbehalten, sie dort zu etablieren. Er besetzte sie erstmals im vierten Satz der 5. Sinfonie.

Seit der romantischen Epoche bis in die heutige Zeit beinhaltet die übliche Besetzung eines Sinfonieorchesters zwei bis drei Tenorposaunen und eine Bassposaune. Im modernen Blasorchester sind für gewöhnlich vier Posaunenstimmen üblich.

Zu den wichtigsten Solokonzerten für Posaune zählen die in der Mitte des 19. Jahrhunderts entstandenen Werke von Ferdinand David, Ernst Sachse und Friedrich August Belcke. Der überwiegende Teil der Sololiteratur für die Posaune entstammt jedoch dem 20. Jahrhundert, wie etwa die "Sonate für Posaune und Klavier" von Paul Hindemith.

In der klassischen Musik des 20. Jahrhunderts behielt die Posaune ihre bedeutende Stellung im Sinfonieorchester und erhielt markante Passagen in Werken praktisch aller bedeutenden Komponisten dieser Epoche.

Nach dem Zweiten Weltkrieg entdeckten Komponisten das Instrument nach und nach auch wieder neu als Solo-Instrument und für die Kammermusik. Stücke wie Edgar Varèses "Octandre", Paul Hindemiths "Sonate" und Luciano Berios "Sequenza V" bereiteten weniger bekannten Komponisten den Weg bei der Schaffung eines breiteren Repertoires. Der bekannteste Posaunist der Neuen Musik der zweiten Hälfte des zwanzigsten Jahrhunderts ist Vinko Globokar, der auch als Komponist hervorgetreten ist.

Zu den beliebtesten Vortragsstücken zählen heute Stjepan Suleks "Vox Gabrieli", die "Sonatinen" von Jacques Castérède und Bertold Hummel, sowie die "Deux Danses" von Jean-Michel Defaye. Zu den bekanntesten Posaunenkonzerten der Zeit nach dem Zweiten Weltkrieg zählen etwa Werke von Launy Grøndahl, Lars-Erik Larsson, Gordon Jacob und Derek Bourgeois. 

In den letzten 20 Jahren haben u. a. Kalevi Aho ("Sinfonie Nr. 9 für Posaune und Orchester – Sinfonia concertante Nr. 2" / 1994), Anders Eliasson und Jan Sandström Konzerte für Christian Lindberg geschrieben, der auch als Komponist – darunter einer Reihe von Werken für Posaune in verschiedenen Besetzungen – in Erscheinung tritt. Ein neues "Konzert für Posaune und Orchester" von Kalevi Aho wird am 2. März 2012 in Den Haag uraufgeführt.

Zugleich kam es im 20. Jahrhundert zu zahlreichen konstruktiven Weiterentwicklungen, etwa bei den verwendeten Werkstoffen. Die Durchmesser von Mundstücken, Bohrungen und Schalltrichtern stiegen, innovative Ventilkonstruktionen und verschiedene Typen von Dämpfern wurden entwickelt.

Heute wird die Posaune in allen Arten der Blasmusik, Sinfonie-Orchestern, Marsch- und Militärkapellen, Brass Bands, Posaunenchören und anderen Genres gespielt. Sie kann auch in kleineren Gruppen mitspielen wie etwa Blechbläserquintetten, -quartetten und -trios und reinen Posaunentrios oder -quartetten.

Die Posaune ist darüber hinaus aus dem Swing, Jazz, Salsa und Ska nicht mehr wegzudenken. Gerade Jazz und Swing sind für die wohl größten spieltechnischen Entwicklungen seit dem Anfang des 20. Jahrhunderts verantwortlich und brachten eine Vielzahl herausragender Jazz-Posaunisten hervor, in Deutschland vor allem Albert Mangelsdorff und Conny Bauer, in den USA Musiker wie Kid Ory, Jack Teagarden, Trummy Young, Tommy Dorsey, Glenn Miller, Ted Heath, Kai Winding, J. J. Johnson, Curtis Fuller, Ed Neumeister, Bill Watrous, Urbie Green, Frank Rosolino, Carl Fontana, Wycliffe Gordon und Don Lusher.

Unter einer Posaune versteht man im Allgemeinen eine "Zug"posaune. Die "Ventilposaune" (siehe unten) bildet eine Sonderform.

Der Zug dient der Tonhöhenänderung, da durch Hinausschieben die Luftsäule verlängert wird. Er besteht aus den beiden über einen Quersteg verbundenen Rohren des "Innenzugs" und dem U-förmigen "Außenzug", in der Regel mit Wasserklappe. Die Innenrohre sind am Ende zum "Schuh" etwas größer im Außendurchmesser und berühren hier fast den Außenzug. Der Zug muss regelmäßig „geschmiert“ werden: Durch auf den Innenzug aufgetragenes "Posaunenfett" bildet dazukommendes Wasser kleine Perlen, ähnlich den Kugeln eines Kugellagers, auf denen dann die beiden Teleskoprohre reibungsarm laufen. Ein direkter Materialkontakt (Messing/Neusilber/Chrom) oder sogar das trockene Laufen auf dem Fett hätte eine extrem hohe Haftreibung zur Folge, die eine leichtgängige Funktion des Zuges nicht gewährleistet. Aus diesem Grund befeuchtet der Posaunist den Zug auch während des Musizierens regelmäßig mit vorhandenem Kondenswasser aus dem Zug oder mit einer kleinen Pump-Wasserflasche. Alternativ zur Fett/Wasser-Kombination kann auch ein Siliconölgemisch aus verschiedenen Konsistenzen verwendet werden.

Die Posaune besitzt in der Regel sieben "Zugpositionen" (auch: "Lagen"), die sich jeweils um einen Halbton unterscheiden. Der Zug kann stufenlos verschoben werden, so dass die Posaune als einziges Blechblasinstrument von einer Tonhöhe zu einer anderen "gleiten" kann "(echtes Glissando)".

Wie bei allen gängigen Blechblasinstrumenten kann die Stimmung der Posaune über einen Stimmzug kalibriert werden. Er befindet sich in der Regel im hinteren Bogen vor dem Schallstück. Diese Anordnung ist kostengünstig und mit geringem Aufwand zu realisieren. Sie ist aber für die Ansprache und die Intonation nicht ganz unproblematisch, da ein Bereich des Instruments, der sich bereits deutlich konisch erweitert, hier nochmals durch "zylindrische" Passagen unterbrochen wird. Eine wenig gängige Alternative besteht darin, die Stimmvorrichtung in den Zug zu integrieren.

Die heute am weitesten verbreiteten "Stimmlagen" sind die Tenor- und Bassposaune. Vergleichbar anderen Instrumenten der Renaissance wie etwa der Blockflöte wurde die Posaune historisch jedoch in allen Stimmlagen von Piccolo bis Kontrabass gebaut.

Die bei weitem gängigste Bauform ist die der Tenorposaune, heute in der Regel in B und mit einer Bohrung von etwa 12,2 bis 13,89 mm. Dieses Instrument ist ab dem großen E aufwärts voll chromatisch spielbar (siehe jedoch auch unter Falsett).

Ebenso wie das Erreichen der selten geforderten Pedaltöne vom Kontra-B bis zum Kontra-E richtet sich der Tonumfang in der Höhe nach dem Können des Spielers. Anfänger erreichen in der Regel bald das d' und nach einiger Zeit das f'; von Fortgeschrittenen wird das b' erwartet. Im professionellen Bereich ist ein Tonraum bis zum f<nowiki>"</nowiki> erforderlich.

Der deutsche Instrumentenbauer Christian Friedrich Sattler stattete im 19. Jahrhundert erstmals eine Tenorposaune mit einem zusätzlichen Quartventil aus. Diese heute noch verbreitete Bauform "(Quartposaune)" bezeichnet man mitunter auch als Tenorbassposaune, da das Instrument bei betätigtem Ventil in F gestimmt ist wie die vor Sattlers Innovation gängige Bauform der Bassposaune. 

Die Länge des Zuges ist so bemessen, dass die Quarte zwischen dem ersten und dem zweiten Naturton chromatisch überbrückt werden kann. In höheren Lagen muss der Zug meist nur noch wenig ausgezogen werden, weil die Naturtöne dort enger beieinander liegen. Zugleich lassen sich höhere Töne auf mehreren Stellungen des Zuges erreichen. Eine vollständige Tabelle der Zugpositionen mit und ohne Verwendung des Quartventils ist in der nebenstehenden Tabelle angegeben. 

Posaunen mit Quartventil sind nicht nur ab dem großen C voll chromatisch spielbar: Ein mindestens ebenso wesentlicher "spieltechnischer" Vorzug besteht darin, dass mit Ventil alternative Zugpositionen möglich werden, eine bedeutende Erleichterung besonders im unteren Register. So braucht der Zug für das kleine c nicht mehr bis auf die sechste Position ausgezogen zu werden, sondern dieser Ton ist mit Ventil bereits auf der ersten Position erreichbar; das große H liegt mit Ventil zwischen der zweiten und der dritten Lage statt ganz am Ende des Zuges.

Da sich bei Betätigen des Quartventils wegen der verlängerten Luftsäule die Abstände zwischen den einzelnen Zugpositionen gegenüber dem Spielen ohne Quartventil vergrößern, ist die siebte Zugposition nicht mehr erreichbar. Daher ist das Kontra-H nur durch Ausziehen des Ventilstimmzugs zu erreichen, was die letzte Lücke zu den Pedaltönen schließt. Manche Modelle besitzen eigens dazu einen zweiten Ventilstimmzug. Bei Instrumenten mit weiterer Bohrung sprechen die Ventil- und Pedaltöne in der Regel besser an und klingen auch meist besser.

Um zu erreichen, dass der Klang und die Ansprache einer Quartposaune sich bei betätigtem Ventil möglichst wenig ändert, gingen manche Hersteller in den 1980er Jahren dazu über, die Ventilschleife in möglichst wenigen, großen Radien zu winden, so dass sie nach hinten über den Bogen des Schalltrichters hinaus ragt "(open wrap)". Vor diesem Hintergrund entstanden auch das Thayer-Ventil und die dadurch angeregten, anderen innovativen Ventilkonstruktionen. Eine heute verbreitete Kompromisslösung mit wenigen weiten Radien, die jedoch nicht über den hinteren Bogen hinausragt, bezeichnen einige Hersteller als "semi-open". Manche Posaunisten bevorzugen jedoch nach wie vor eine eng innerhalb des Korpus verlegte Ventilschleife "(traditional wrap)".

Wieder andere wählen nach wie vor ventillose Posaunen, etwa weil das untere Register für ihre Musikrichtung weniger bedeutend ist und weil diese Instrumente, besonders mit enger Bohrung, meist etwas besser ansprechen. Besonders Jazz-Solisten verwenden bevorzugt eng gebaute Tenorposaunen mit kleinem Schalltrichter und ohne Ventil.

Die Bassposaune ist eigentlich nur eine etwas größere Tenorbassposaune. Wie diese ist sie in B gestimmt, hat also die gleiche Rohrlänge und unterscheidet sich von ihr nur durch eine weitere Bohrung (etwa 14,30 bis 14,89 mm), einen größeren Schalltrichter und ein meist etwas größeres Mundstück. Dadurch spricht sie in der tiefen Lage besser an, klingt voller und kann lauter gespielt werden. Das hohe Register klingt dafür matter und erfordert eine größere Anstrengung.

Die Bassposaune wird in der Regel nichttransponierend im Bassschlüssel notiert. Der Tenorschlüssel bildet die Ausnahme, der Altschlüssel kommt, wenngleich selten, ebenfalls vor.

Die moderne Bassposaune besitzt mindestens ein Ventil, heute in der Regel wie bei der Tenorposaune auf eine Quarte gestimmt. Meistens ist zusätzlich ein "zweites Ventil" vorhanden, mit dem die Grundstimmung zugunsten voll chromatischer Spielbarkeit ab dem tiefsten Pedalton weiter herabgesetzt werden kann. Die gängigsten Grundstimmungen in der Ventilkombination sind Es und D, doch es gab auch Versuche mit allen Stimmungen zwischen G und E. 

Das zweite Ventil ist entweder in die Rohrschleife des Quartventils integriert und kann nur mit diesem zusammen benutzt werden ("versetzte" Bauweise, "offset", "abhängige" Konfiguration) oder es befindet sich vor oder hinter dem Quartventil und kann auch einzeln benutzt werden ("in-line", "unabhängige" Konfiguration). Die abhängige Bauweise wurde erstmals von den amerikanischen Bassposaunisten Kauko Kahila (BSO) und Edward Kleinhammer (CSO) unabhängig voneinander in den 1950er Jahren propagiert. Ihre Landsleute Burt Herrick und Larry Minnick leiteten einige Jahre später aus diesem Konzept die unabhängige Bauweise ab. Beide Bauformen besitzen ihre Eigentümlichkeiten; die Entwicklung des Instruments kann noch nicht als abgeschlossen betrachtet werden.

Der Tonumfang der Bassposaune reicht in der Tiefe je nach Stimmung des zweiten Ventils - theoretisch - bis zum Subkontra-B und in der Höhe bis zum c und höher. In der Praxis sind nur talentierte und geübte Spieler in der Lage, tiefere Töne als das Kontra-E im normalen Vortrag zu nutzen. Die Literatur geht kaum über den Bereich zwischen Kontra-B und b hinaus. Bekannte Ausnahmen bilden die Symphonie en ré mineur von César Franck sowie das Konzert für Orchester von Béla Bartók. Diese Komponisten gingen von einer „echten“ Bassposaune in F aus, die neben Instrumenten in G, E, Es, D und C von der Renaissance bis zur Einführung der Tenorbassposaune üblich war.

 Die Altposaune ist heute deutlich seltener als die Tenor- und Bassposaune. Sie ist zumeist in Es, selten in F gestimmt und wird in der Regel nichttransponierend im Altschlüssel notiert.
Bei einigen Posaunenlehrern wird die Altposaune für Einsteigerkinder gebraucht.

Konstruktiv unterscheidet sich die Altposaune von der Tenorposaune durch eine kürzere Rohrlänge und einen kleineren Schalltrichter mit einem Durchmesser zwischen 165 und 177 Millimetern (6,5 bis 7 Zoll). Ihre Bohrung ist jedoch nur selten kleiner als die einer engen Tenorposaune. Manche Instrumente besitzen ein Quart- oder Sekundventil. 

Die Altposaune klingt brillanter als die größeren Instrumente. Ihr Tonumfang reicht (ohne Pedal- und Ventiltöne) in etwa vom großen A bis zum zweigestrichenen b, doch höhere Töne als das zweigestrichene f werden nur selten gefordert.

Infolge des kürzeren Zuges unterscheiden sich ihre Lagen deutlich von denen auf der Tenor- und Bassposaune, was den Wechsel zwischen diesen Instrumenten über die für Tenorposaunisten ungewohnte Notation hinaus weiter erschwert. Von professionellen „hohen“ Posaunisten wird heute jedoch erwartet, dass sie auch die Altposaune beherrschen. 

Vom 16. bis zum 18. Jahrhundert bildete die Altposaune die reguläre Oberstimme des dreistimmigen Posaunensatzes. Anfang des 19. Jahrhunderts ging ihre Verbreitung infolge der Entwicklung der Ventiltrompete jedoch deutlich zurück. Als sich der Posaunensatz im klassischen und romantischen Sinfonieorchester etablierte, wurde die Oberstimme in der Regel mit einer Tenorposaune besetzt, deren in dieser Zeit nach oben erweiterter Tonumfang dazu meist ausreichte. Zwar griffen Tenorposaunisten noch bis ins 20. Jahrhundert vereinzelt auf die Altposaune zurück, doch erst in jüngster Vergangenheit nimmt die Beliebtheit des Instruments wieder merklich zu.

Die Altposaune wird in erster Linie in der Chormusik sowie im symphonischen und Opernbereich besetzt, doch es existiert auch ein bescheidenes solistisches Repertoire, insbesondere aus der Wiener Klassik. Zeitgenössische Komponisten haben das Instrument erst in jüngster Zeit wieder neu entdeckt.

Die frühesten "Kontrabassposaunen" ("Octavposaune" bei Praetorius, gelegentlich auch "Doppelposaune" (trombone doppio)) wurden in der Renaissance gebaut und waren in Kontra-B (16 Fuß) gestimmt. Sie hatten zunächst einen überlangen Zug mit Schwengel, um die äußersten Lagen zu erreichen. Später kamen Bauformen mit doppelt gewundenem verkürztem Zug auf.

Im späten 19. Jahrhundert kam es zur Neuentdeckung der Kontrabassposaune, als Richard Wagner für den Ring des Nibelungen erstmals ein Opernorchester mit einem vierstimmigen Posaunensatz vorsah. Ihm folgten beispielsweise Richard Strauss mit seiner Oper Elektra von 1908 und Arnold Schönberg 1913 mit den Gurre-Liedern. Dennoch, und trotz einiger konstruktiver Innovationen, konnte sich die Kontrabassposaune nicht dauerhaft im modernen symphonischen Orchester etablieren und wurde im 20. Jahrhundert nur vereinzelt von Komponisten besetzt. 

Bei modernen Kontrabassposaunen handelt es sich um in F oder Kontra-B gestimmte Sonderanfertigungen in gleicher Stimmlage (und entsprechend mit gleicher Rohrlänge) wie die Bass- beziehungsweise Kontrabasstuba. Ihre Bohrung ist meist etwas weiter als die der modernen Bassposaune, und sie besitzen ein bis zwei Ventile.

Seit dem Zweiten Weltkrieg ist im Orchester eigentlich nur noch die Kontrabassposaune in F mit zwei Ventilen in Gebrauch. Ursprünglich aus Platzgründen war das Schallstück mit einer Windung versehen, aber ab den 1970er Jahren setzte sich die lange, gerade Bauform durch. Durch die Kombination der beiden Ventile erübrigte sich auch der Schwengel, und man kommt mit fünf Positionen am Zug aus. Übliche Ventilkombinationen sind Sekund- und Quintventil, Terz- und Quintventil oder seltener Terz und Quartventil.

Die Sopranposaune gehört spätestens seit Ende des 17. Jahrhunderts zur Familie der Posaunen. Sie wurde seit Beginn an relativ selten verwendet, auch wenn Komponisten wie Heinrich Schütz, J. S. Bach, Ch. W. Gluck, und W. A. Mozart sie in dem bis Ende des 18. Jahrhunderts üblichen vierstimmigen Posaunensatz einsetzten. 

Posaunen höherer Stimmlage als die Altposaune besitzen heute nur noch Exotenstatus und werden meist von Trompetern als Gag eingesetzt, wie etwa von Matthias Höfs bei Auftritten von German Brass. Allerdings sollte dabei nicht vergessen werden, dass ursprünglich für einige wenige Kantaten Johann Sebastian Bachs der Chor in einigen Stücken (insbesondere den Chorälen) durch einen vierstimmigen Posaunensatz unterstützt wird. Da die höchste Stimme dabei in der Regel an die Grenzen der Möglichkeiten einer Altposaune stößt, empfiehlt es sich nach wie vor, für diese Stimme statt einer Trompete eine Diskantposaune zu verwenden.

Der Name Zugtrompete ist umgangssprachlich gebräuchlich, da sie den gleichen Tonumfang und Mundstückgröße wie eine Trompete besitzt. Die Grundstimmung heutiger Instrumente ist in B oder hoch B („Piccoloposaune“). Die Notation der Sopranposaune ist mangels Spieltradition nicht geklärt. Nach dem Instrumentenhandbuch in Tabellenform von Winfried Pape wird die Sopranposaune nichttransponierend in klingend C notiert. Da aber sowieso kein konventioneller Posaunist wegen des Trompetenmundstücks und der extrem kurzen Lagenwege auf die Sopranposaune zurückgreifen kann und nicht vorausgesetzt werden darf, dass ein solcher auch den Violinschlüssel lesen kann, sondern nur Trompeter, empfiehlt sich eine transponierende Notation nach B einen Ganzton höher, wie sie für die B-Trompete traditionell üblich ist.

In Deutschland wurden Posaunen historisch mit den verschiedensten Bohrungen und Schallstückdurchmessern gefertigt. Die "Deutsche Konzertposaune" unterscheidet sich konstruktiv in mehrfacher Hinsicht deutlich von der heute üblichen, amerikanischen Bauweise. Sie wird meist mit einem eher kleinen Mundstück gespielt, hat jedoch ein sehr langes Mundrohr von mindestens 30 bis 60 Zentimetern. 

Das gesamte Instrument besteht in der Regel aus Goldmessing, stellenweise auch aus Neusilber, was sich auf den Klang auswirkt. Das Metall ist hart, aber biegsam. Seine Wandstärke ist insgesamt geringer als bei der amerikanischen Bauweise und nimmt vor allem zum Schallstück hin kontinuierlich ab. Getreu der Philosophie von Eduard Kruspe gilt dabei das Prinzip der Fragilität: Je leichter und dünnwandiger, desto hochwertiger das Instrument.

Das Schallstück ist einteilig mit gerader Lötnaht und besitzt einen Kranz aus Neusilber, oft mit Gravur, der jedoch nicht flächig aufgelötet ist. Dieser "Schmetterkranz" bewirkt das sogenannte "unendliche Fortissimo": Die Lautstärke kann kontinuierlich gesteigert werden, ohne dass der Ton „aufbricht“, wobei die absolut erreichbare Lautstärke hinter der amerikanischen Bauweise zurückbleibt.

Der Zug ist grundsätzlich „konisch“: das obere Rohr hat eine kleinere Bohrung als das untere. Man gibt dabei keine konkreten Durchmesser an, sondern teilt die Instrumente in fünf "„deutsche Weiten“"" ein (1 = eng, Soloposaune bis 5 = weiteste Bassposaune). Die Strebe des Außenzugs ist nicht fest verlötet.

Die in Deutschland üblichen Bohrungen galten im 19. Jahrhundert als vergleichsweise groß, blieben jedoch seit 150 Jahren in etwa auf dem gleichen Stand, sodass die Deutsche Konzertposaune heute insgesamt etwas enger ausfällt als ihr amerikanisches Gegenstück. Die Schallstückdurchmesser dagegen sind mit 254 Millimetern und mehr im internationalen Vergleich immer noch am größten "(hyperbolische Bauweise)". 

Schwerere und leichtere Baugruppen wechseln sich „rhythmisch“ ab. Das Ventil befindet sich deutlich weiter hinten im Korpus und wird traditionell per Lederriemen betätigt; weniger radikale Konstruktionen setzen inzwischen jedoch auf den üblichen metallenen Hebel.

Wie bei der "Deutschen Konzerttrompete" kommen ausschließlich Drehventile zum Einsatz, selbst bei den Ventilposaunen deutscher Bauweise.

Zu den weiteren häufigen Merkmalen der Deutschen Konzertposaune zählen die lange Wasserklappe (mit Betätigungseinrichtung am Quersteg des Außenzuges) sowie Schlangenverzierungen an den Bögen von Zug und Schallstück. Insgesamt zeichnen sich Instrumente deutscher Bauweise bei der Fertigung durch einen hohen Anteil an Handarbeit aus, sind also als kunsthandwerkliche, individuell auf den Auftraggeber zugeschnittene Produkte zu verstehen.

In der Summe unterscheidet sich die Deutsche Konzertposaune daher stark im Klang, im Obertonspektrum sowie in der Abstrahlcharakteristik und in der Ansprache, auch je nach Dynamikbereich, worauf sich der Spieler bei der Klangformung einstellen muss.

Die Ventilposaune besitzt statt eines Zuges zur Tonhöhenveränderung drei bis vier Ventile wie viele andere Blechblasinstrumente. Ihr Tonumfang ist gleich der entsprechenden Zugposaunenstimmlage, doch sie unterscheidet sich leicht in Ansprache und Klang, da sie von der Bau- und Spielweise her eher einer großen Trompete ähnelt. Mit einer solchen hat sie unausweichlich auch die Spielgeschwindigkeit in allen Lagen und die generelle Ventiltechnik gemeinsam (z. B. chromatische und diatonische Triller auf fast allen Tönen). Es gibt eine kurze und eine lange Bauform. 

Manche Posaunisten vertreten die Auffassung, dass dieses Instrument eine problematische Intonation aufweist, zumal es in der Regel nicht kompensiert und nur bei höherwertigen Instrumenten Trigger oder andere Intonationshilfen besitzt. Eine Minderheit zieht sie der gängigeren Zugposaune jedoch vor. 

Historisch gab es Ventilposaunen in allen Stimmlagen von Alt bis Kontrabass, doch die Tenorventilposaune erreichte stets die größte Verbreitung. Eine Variante der Ventilposaune, die 1867 von Václav František Červený entwickelt wurde und die in Tubaform mit gebogenem Schallstück beziehungsweise in Helikonform konstruiert war, war die sogenannte Armeeposaune.

Den Zenit ihrer Beliebtheit erreichte die Ventilposaune im 19. Jahrhundert, als die Ventiltechnik die größten Fortschritte machte. Als gegen Ende dieses Jahrhunderts Zugposaunen besserer Qualität mit industrieller Präzision preisgünstig in Serie gefertigt wurden, gewann diese Bauform wieder die marktbeherrschende Stellung zurück. Regional überwiegt jedoch bis heute die Ventilposaune, etwa in Österreich, Italien, Tschechien, der Slowakei, Spanien, Portugal, Südamerika und Indien.

In der Blasmusik in Österreich und in der alpenländischen Volksmusik wird die Ventilposaune im Violinschlüssel transponierend in B♭ notiert wie das Tenorhorn, was den Musikern bei Bedarf den Wechsel des Instruments, etwa auch zur Trompete, erleichtert.

Manche Passagen sind auf einer Ventilposaune einfacher zu spielen, andere auf einer Zugposaune. Auf der Ventilposaune sind schnelle technische Passagen in der Regel mit größerer Geläufigkeit zu spielen. Viele (Zug-)Posaunisten bemängeln jedoch ihren matteren, weniger offenen Klang. Im sinfonischen Kontext wird die Ventilposaune heute nicht mehr verwendet, wenngleich insbesondere Verdi eindeutig auf ihre technischen Möglichkeiten setzte. So ist die Ouverture zur Oper "Die Macht des Schicksals" auf der Zugposaune nur von Virtuosen zu bewältigen. Auch Posaunisten von Weltformat wie etwa die Mitglieder von German Brass griffen vor diesem Hintergrund auf bestimmten Aufnahmen zu einer Ventilposaune, die deshalb beispielsweise auch zu den auf dem Cover ihres Albums Bach 300 abgebildeten Instrumenten zählt.

Auch im Jazz wird die Ventilposaune eher selten eingesetzt. Zu den bekanntesten Ventilposaunisten zählen Bob Brookmeyer, Raul de Souza, Juan Tizol vom Duke Ellington Orchestra sowie Bob Enevoldsen. Da die Fingersätze der Ventilposaune denen der B-Trompete entsprechen, wird sie zudem als Zweitinstrument von einigen Jazz-Trompetern gespielt. 

Eine exotische Bauform besitzt sowohl einen Zug als auch eine Ventilmaschine. Solche Instrumente wurden erstmals im frühen 20. Jahrhundert produziert und sind heute dank dem Einfluss von Jazzmusiker Maynard Ferguson als "Superbone" bekannt.

Beim Einstiegsalter gelten ähnliche Grundsätze wie beim Waldhorn und den anderen Blechblasinstrumenten. Insbesondere für Kinder, die infolge ihrer Armlänge die äußersten Zugpositionen noch nicht erreichen können, bringen verschiedene Hersteller in jüngster Zeit die wiederentdeckte Bauform der B/C-Posaune mit Sekund-Verkürzungsventil (und meist mit nur sechs Zugpositionen) auf den Markt, beispielsweise Günter Frost, Thein und Yamaha "("Kompaktposaune")".

Zunehmend werden als Erstinstrument für junge Posaunen-Anfänger auch Altposaunen eingesetzt. Da die in Es stehenden Instrumente aufgrund ihres Tonumfanges deutlich kleiner als die üblichen Tenorposaunen sind, können schon Kinder ab etwa sechs Jahren auf einem solchen Instrument lernen. Als mögliche Schwierigkeit ergibt sich daraus, dass beim späteren Umstieg auf die Tenorposaune die zu den einzelnen Zügen gehörenden Töne umgelernt werden müssen. Zum Teil wird aus diesem Grund transponierend gelehrt: Jeder Ton klingt dann eine Quarte höher, als er benannt wird; auf der Tenorposaune ergibt sich dadurch später ohne Umlernen die Übereinstimmung von Notenname und klingendem Ton.

Die Posaune wird von Komponisten, etwa in der Filmmusik, getreu der biblischen Konnotation immer wieder gern eingesetzt, wenn es an Endzeitszenarien geht. Beispiel Thirteen Days: Man beachte das Motiv im Anschluss an die Szene, in der Dean Acheson in einer Konferenz mit Präsident John F. Kennedy erstmals andeutet, dass die Kubakrise zum nuklearen Holocaust führen könnte.



</doc>
<doc id="11953" url="https://de.wikipedia.org/wiki?curid=11953" title="Gesundheitssystem">
Gesundheitssystem

Das Gesundheitssystem oder Gesundheitswesen eines Landes umfasst als Öffentliches Gesundheitswesen alle Personen, Organisationen, Einrichtungen, Regelungen und Prozesse, deren Aufgabe die Förderung und Erhaltung der Gesundheit sowie deren Sicherung durch Vorbeugung und Behandlung von Krankheiten und Verletzungen ist. In Österreich wird hierfür auch der Begriff Sanitätswesen verwendet. Der Begriff Gesundheitswesen dient der Beschreibung des äußerst komplexen Gesundheitssystems zur Krankenversorgung und Gesunderhaltung, wogegen die Gesundheitswirtschaft als ganze neben der stationären und ambulanten Versorgung Kranker und der Vorbeugung gegen Krankheiten bei Gesunden unter anderem auch die Herstellung von Arzneimitteln und Medizinprodukten, den Gesundheitstourismus, die Wellness- sowie die Fitnessbranche umfasst.

In der wissenschaftlichen Literatur findet man folgende Ziele für ein Gesundheitswesen:

Im Jahr 2000 legte die Weltgesundheitsorganisation WHO Ziele fest, an denen sie nationale Gesundheitssysteme misst:

2001 definierte auch die EU-Kommission Ziele für Gesundheitswesen und Altenpflege:

Die Frage der Finanzierung ist verbunden mit der Frage, welche Kranke wie schnell welche Behandlung bekommen.
In den Arztpraxen hat das Sparen - speziell bei niedergelassenen Ärzten am Ende eines Quartals, wenn die 'Punkte' der Praxis aufgebraucht sind - längst zu einer heimlichen Rationierung geführt.
Schweden praktiziert ein System, das Kosten-Nutzen-Überlegungen in die ärztlichen Entscheidungen einfließen lässt.

Die Beteiligten an einem Gesundheitssystem sind

Ein charakteristisches Merkmal eines Gesundheitssystems ist die Art seiner Finanzierung. Es werden grundsätzlich drei Klassen unterschieden:


Hinzu kommt das Holländische Modell, bestehend aus Gesundheitsprämie und einkommensentsprechendem Beitrag.

Die rechte Tabelle zeigt die Länder mit den höchsten relativen Ausgaben im Gesundheitswesen als Anteil des Bruttoinlandsprodukts bzw. die Länder mit den höchsten absoluten Ausgaben als kaufkraftbereinigte US-Dollar pro Kopf im Jahr 2008.

Im Durchschnitt der OECD-Mitgliedsländer wachsen die Gesundheitsausgaben stärker als die Wirtschaftskraft. Die Pro-Kopf-Ausgaben stiegen von 1990 bis 2005 um über 80 %, während die Bruttoinlandsprodukte (BIP) pro Kopf nur um 37 % wuchsen. Lagen die Gesundheitsausgaben 1970 durchschnittlich noch bei 5 % des BIP, war der Anteil 1990 auf 7 % angewachsen und stieg bis zum Jahr 2005 weiter auf 9 %. In Deutschland lagen die Gesundheitsausgaben im Jahr 2010 bei 287,3 Milliarden Euro, dies entspricht 11,6 % des BIP.


Die Qualität von Gesundheitssystemen zu beurteilen ist schwierig. So muss beispielsweise ein hoher Anteil an Kranken in der Bevölkerung nicht unbedingt auf eine schlechte medizinische Versorgung hindeuten. Im Gegenteil wird ein Diabetiker in einem Land mit schlechter medizinischer Versorgung bald sterben und damit aus der Krankenstatistik herausfallen. In einem Land mit guter medizinischer Versorgung hingegen kann er noch lange weiterleben, wird in der Statistik aber als Kranker geführt. Ein hoher Anteil kranker Menschen an der Bevölkerung ist allerdings ein Indiz dafür, dass die medizinische Versorgung sich mehr um die Behandlung von Symptomen kümmert als um die Beseitigung der Ursachen. Der Diabetiker erhält beispielsweise Insulin, um mit seiner Erkrankung weiterleben zu können, die konkreten Ursachen der Funktionsstörung werden aber nicht fest- und gegebenenfalls abgestellt. Ähnlich sieht es bei einer Reihe weiterer, im Wesentlichen durch Ernährung und Lebensstil bedingter Erkrankungen aus, die für einen großen Teil der Pflegefälle verantwortlich sind.

Die Weltgesundheitsorganisation hat eine Rangordnung der Gesundheitssysteme ihrer 191 Mitgliedsländer aufgestellt. Verglichen wurde anhand der oben genannten Ziele – Gesundheitsniveau, Bedürfnisorientierung und Finanzierungsgerechtigkeit – auf der Grundlage von Daten aus dem Jahr 1997. Die Studie ergab folgende Platzierungen in der oben stehenden Tabelle.

Das Deutsche Ärzteblatt wirft der Studie allerdings eine zu geringe und unseriös gewählte Datenbasis und eine politische Färbung vor. Außerdem wird kritisiert, dass sich die Rankings in den einzelnen Kategorien der Studie sehr stark unterscheiden.

Nach einer internationalen Studie unter Leitung der University of Washington hat sich die Gesundheit von 1990 auf 2015 in 167 von 195 Ländern verbessert. Die beste Bewertung von Zugang und Qualität der Gesundheitsversorgung erhielt 2015 Andorra, gefolgt von Island und der Schweiz; Österreich erreichte den 13. und Deutschland den 20. Rang.

Die internationalen Patientensicherheitsziele dienen der weltweiten Qualitätssicherung in der Medizin.





</doc>
<doc id="11955" url="https://de.wikipedia.org/wiki?curid=11955" title="Kohle (Begriffsklärung)">
Kohle (Begriffsklärung)

Kohle (althochdeutsch "kol") steht für:



Kohle ist Familienname folgender Personen:

Kohle ist Bestandteil der Titel folgender Werke:

Siehe auch:


</doc>
<doc id="11958" url="https://de.wikipedia.org/wiki?curid=11958" title="Militär">
Militär

Als Militär (von lateinisch ' ‚den Kriegsdienst betreffend‘ bzw. ' ‚Soldat‘), Streitmacht oder Streitkräfte bezeichnet man die mit Kriegswaffen ausgestatteten Träger der Staatsgewalt, die hoheitlich mit der Gewährleistung der äußeren Sicherheit betraut sind und zusätzlich in den meisten Staaten auch als Verstärkung der Polizei zum Schutz der inneren Sicherheit bei besonderem Bedarf eingesetzt werden können.

Die Bezeichnung "Militär" (von französisch "", Einzahl: ‚der Militär‘; Mehrzahl: ‚die Militärs‘) wird auch als Synonym für eine Zahl von Soldaten verwendet. In der Einzahl steht sie ähnlich wie die Bezeichnung "Dienstgrad" für eine einzelne Militärperson der Offiziere und Generäle.

Vorrangiger Zweck von Militär ist typischerweise die Gewährleistung der "äußeren Sicherheit", also der Verteidigung des Landes oder seiner Bündnispartner gegen Gefahren von außen. Aber auch die Gewährleistung der "inneren Sicherheit", d. h. die Abwehr innerstaatlicher Bedrohungen, kann Aufgabe der Streitkräfte sein.

Als Aufgaben sind beispielsweise für die deutsche Bundeswehr festgelegt:

Das Instrument, das den Streitkräften zu diesem Zweck zur Verfügung steht und nach dem (nach den Genfer Konventionen) grundsätzlich Kombattanten von Zivilisten unterschieden werden, ist traditionell die Waffengewalt. Spannungen zwischen Staaten können kriegerische Auseinandersetzungen hervorrufen, allerdings werden zunehmend auch andere Gefahren, wie Terrorismus für Kampfeinsätze ursächlich, wodurch die Streitkräfte heutzutage mehr und mehr mit der sogenannten unkonventionellen Kriegführung konfrontiert werden. Außerdem beschränkt sich die Aufgabe des Militärs nicht mehr ausschließlich auf bewaffnete Konflikte, sondern es rücken Techniken der Informationsgewinnung, internationale Friedensmissionen oder andere Arten der militärischen Auseinandersetzung, zum Beispiel der sogenannte „Cyberwar“, in den Vordergrund.

In der Öffentlichkeit war das Militär vor allem in der Vergangenheit eine wichtige Komponente des Nationalverständnisses. In vielen Staaten ist es auch heute ein Wesensbestandteil des Patriotismus. Vor allem in militärisch geprägten Ländern wie China oder den USA spielen die Streitkräfte eine entscheidende Rolle, um Stärke zu demonstrieren. Die Streitkräfte sind von Natur aus ein wichtiges Mittel der Politik, insbesondere der Außen- und Sicherheitspolitik. Die Ausuferung einer innerstaatlichen Macht des Militärs kann von der Unterdrückung eines Volkes bis zu einem Putsch und einer Militärregierung führen.

Neben der Verteidigung hat das Militär auch weitreichendere Aufgabengebiete wie die Friedenssicherung (z. B. bei Internationalen Blauhelmeinsätzen), Katastrophenhilfe (so auch Hurrican Katrina), Durchsetzung der wirtschaftlichen und politischen Interessen eines Landes/Bündnisses (wie die oft kritisierten „Öl-Kriege“) und zeremonielle Zwecke (u. a. Staatsempfänge).

Die Unterhaltung von Streitkräften ist kostenintensiv und macht meist einen großen Teil der Staatsausgaben aus. Daher gibt es Staaten ohne Militär und Militärbündnisse wie die der NATO, die militärische Aufgaben auf mehrere Mitglieder verteilen und ein gegenseitiges Schutz-Verhältnis gewährleisten.

Die aktuell quantitativ größte Streitkraft der Welt hat China mit circa 2,3 Mio Soldaten, gefolgt von den Vereinigten Staaten, die mit 574,9 Mrd US$ den höchsten Militär-Etat der Welt haben (siehe: Liste der Streitkräfte).

Die Militärtechnik hat traditionell eine wichtige Vorreiter- und Schrittmacherrolle bei der allgemeinen technischen Entwicklung inne. So wurden beispielsweise das Fernsehen, das Internet oder GPS ursprünglich im militärischen Auftrag entwickelt und anfangs nur vom Militär genutzt (siehe Wehrtechnik und Rüstung).

"Siehe auch: Militärgeschichte, Militärwissenschaft, Militärsoziologie, Kriegssoziologie und Konfliktforschung"

Zwar ist jede Streitkraft unterschiedlich organisiert, allerdings kann in der heutigen Zeit allgemein nach "Teilstreitkräften" unterschieden werden. Dies sind zumeist Landstreitkräfte (Heer), Seestreitkräfte (Marine) und Luftstreitkräfte (Luftwaffe). Manche Staaten ergänzen ihre Streitkräfte durch weitere Teilstreitkräfte, wie einer Küstenwache, der Marineinfanterie oder paramilitärischer Gendarmerie.

In demokratisch organisierten Gesellschaften wird die Rolle des Militärs von der der innerstaatlichen Sicherheitskräfte (Polizei) abgegrenzt. Außerdem bestehen Schutzmechanismen, um das Militär zu kontrollieren. Dies zeigt sich in der Eigenschaft einer sogenannten Parlamentsarmee. Dagegen sind in vielen repressiven Staaten diese beiden Funktionen vermischt und das Militär übernimmt innenpolitische Aufgaben. Ausdruck für diesen Dualismus ist die Gendarmerie. Gendarmen sind ebenfalls häufig Teil der Streitkräfte wie in Frankreich und Italien oder unterstanden historisch einmal dem Verteidigungsressort wie die frühere Bundesgendarmerie in Österreich. Verfassungs- und verwaltungsrechtliche Normen stellen dabei sicher, dass solche Einheiten im Frieden dem Innen- bzw. dem Justizressort unterstehen.

Sogenannte paramilitärische Organisationen, oder Milizen (meist offiziell nicht-militärische Einheiten, die aber dennoch ähnliche Aufgaben haben), die in vielen nicht als Krieg bezeichneten bewaffneten Konflikten (etwa Bürgerkriegen) teilnehmen, gelten offiziell nicht als Streitkraft und werden nach internationalen Konventionen auch anders behandelt.

Seit dem 17. Jahrhundert hat sich die Form der stehenden Streitkräfte durchgesetzt, bei denen eine ständig verfügbare Anzahl von Soldaten dauernd unter Waffen stehen und dadurch jederzeit einsatzbereit sind. Dies hat vor allem Gründe der besseren und einheitlichen Ausbildung, sowie der schnelleren Verfügbarkeit und Zuverlässigkeit und ist in der heutigen Zeit insbesondere durch das System der Berufsarmee zum Standard geworden. Das gegensätzliche System war eine „Bedarfsarmee“, die (bspw. im Rahmen einer Milizorganisation) erst bei Bedarf aufgestellt wurde und somit meist keine ausreichende Erfahrung und Ausbildung hatte, dafür aber wesentlich billiger im Unterhalt war.

Gemäß den Genfer Konventionen wird bei den Beteiligten an einem militärischen Konflikt grundsätzlich zwischen Kombattanten und der Zivilbevölkerung unterschieden. Kombattanten sind alle unter Waffen stehenden und auch als solche erkennbaren Mitglieder einer Konfliktpartei. Nur sie sind zu Kriegshandlungen berechtigt und im Gegensatz zu Zivilisten auch nicht direkt vor gegnerischer Gewalteinwirkung geschützt. Ihnen stehen im Falle einer Kriegsgefangenschaft bestimmte Rechte, wie ein faires Gerichtsverfahren, zur Verfügung. Demgegenüber haben die USA den völkerrechtlich stark umstrittenen Begriff des „ungesetzlichen Kombattanten“ eingeführt.

Streitkräfte zeichnen sich in der Regel (und nach internationalen Standards) durch folgende Eigenschaften aus:

Militärische Organisationen zeichnen sich durch eine hierarchische Dienstgradstruktur aus, der eine Befehlsstruktur folgt. Alle Soldaten unterliegen dabei dem Prinzip von Befehl und Gehorsam gegenüber ihren Vorgesetzten und müssen gewisse Einschränkungen ihrer Freiheiten und Grundrechte hinnehmen.
Das Militär hat in den meisten Staaten und auch nach dem Kriegsvölkerrecht (z. B. durch die Genfer Konventionen und die Haager Landkriegsordnung) einen gesonderten Status. Soldaten (sogenannte Kombattanten) nehmen einen Sonderstatus ein, indem sie in vielen Staaten einer speziell eingerichteten Militärgerichtsbarkeit, sowie einem internationalen Völkerstrafrecht unterliegen (z. B. Internationaler Militärgerichtshof). Die sogenannte Innere Führung wird geprägt von Rangordnung und Gehorsam (siehe Militärischer Befehl), allerdings haben Soldaten in Deutschland die Möglichkeit sich außerhalb des normalen Dienstweges an den Wehrbeauftragten zu wenden, der die Behandlung der „Staatsbürger in Uniform“ nach gültigem Recht überwacht.

In vielen Staaten der Welt, insbesondere in jenen mit langer Militärtradition, oder bei ständiger Bedrohung (z. B. Israel, wo auch Frauen zum Militärdienst verpflichtet werden), müssen die Staatsbürger für einen gewissen Zeitraum in der Armee oder einer anderen Wehrformation (zum Beispiel im Bereich des Katastrophenschutzes) ihrem Land dienen. Dieses System der Wehrpflicht soll die (zumeist männliche) Bevölkerung für den Fall eines Krieges vorbereiten, in dem sie dann gegebenenfalls zum Dienst an der Waffe verpflichtet werden können, um ihr Land als Reservisten zu verteidigen. Diese Personen können, wenn sie als wehrfähig erachtet wurden, im Notfall zu den Streitkräften eingezogen werden und stehen somit als sogenanntes Reservesystem zur Verfügung.

Das Militär genießt in den meisten demokratischen Staaten wie Deutschland tendenziell hohes öffentliches Ansehen, wird aber auch durch Friedensbewegungen kritisiert, die das Töten von Menschen, die Kriegführung und die daraus folgende Zerstörung und Vernichtung als barbarisch, moralisch verwerflich und für eine nachhaltige Konfliktbearbeitung als ungeeignet erachten. Antimilitarismus und Pazifismus problematisieren das Militärische und erstreben eine gewaltfreie Bearbeitung von Konflikten durch Hinwendung zur Diplomatie und Zivilen Konfliktbearbeitung. Sie versuchen aktiv, bewaffnete Konflikte und die militärische Rüstung zu verhindern, den Krieg als Mittel der Politik langfristig auszuschließen und Bedingungen für dauerhaften Frieden zu schaffen. Kriegs-Befürworter werden von ihnen als Bellizisten bezeichnet.

Durch die zunehmende Aufrüstung (insbesondere beim Wettrüsten in der Zeit des Kalten Krieges) wurde durch die militärtechnische Weiterentwicklung eine Grenze überschritten, wodurch bei einem Kriegsausbruch zwischen zwei Supermächten nicht mehr nur die beteiligten Staaten, sondern die gesamte Weltbevölkerung betroffen wäre, da die Vernichtungskraft des Militärs einiger Staaten durch Massenvernichtungswaffen unkalkulierbar sein kann (siehe auch Overkill). Angesichts des potenziellen Vernichtungs-Ausmaßes dieser Waffen haben viele Staaten inzwischen multilateralen Verträge wie dem Atomwaffensperrvertrag oder Abrüstungsabkommen wie „START“, „INF“ und „KSE“ unterzeichnet, die diese Gefahr zumindest teilweise reduzieren sollen (siehe auch Rüstungskontrolle und Conference on Disarmament). Besonders nennenswerte Abkommen, die von der Staatengemeinschaft als überaus unmenschlich angesehene Vorgehensweisen verhindern wollen, sind bspw. die Ottawa-Konvention, die Chemiewaffenkonvention, oder der ABM-Vertrag.

International unterliegen die Streitkräfte heute völkerrechtlichen Abkommen, die militärische Auseinandersetzungen regeln und verhindern sollen. Wichtige Schriften sind dabei die Haager Landkriegsordnung und die Genfer Konventionen. Nach Artikel 2 (4) der Charta der Vereinten Nationen besteht ein generelles Gewaltverbot, das nur in Ausnahmefällen (zur Verteidigung und im Rahmen einer friedenserzwingenden Maßnahme durch eine Resolution des UN-Sicherheitsrates) umgangen werden darf.



</doc>
<doc id="11962" url="https://de.wikipedia.org/wiki?curid=11962" title="Großkreis">
Großkreis

Ein Großkreis ist ein größtmöglicher Kreis auf einer Kugeloberfläche. Sein Mittelpunkt fällt immer mit dem Mittelpunkt der Kugel zusammen und ein Schnitt auf dem Großkreis teilt die Kugel in jedem Fall in zwei („gleich große“) Hälften. Da es unendlich viele Möglichkeiten gibt, eine Kugel so zu zerschneiden, dass die Schnittebene den Kugelmittelpunkt trifft, gibt es auch unendlich viele Großkreise.

Großkreise spielen z. B. in der Geographie sowie der Schiff- und Luftfahrt eine bedeutende Rolle. Anhand von ihnen werden auch die Zeitzonen festgelegt. Die Sphärische Geometrie beinhaltet Großkreise als elementaren Bestandteil. Das Verständnis der Orthodrome als kürzeste Verbindung zweier Punkte auf einer Kugeloberfläche ist unerlässlich für das Verständnis der „geradlinigen“, unbeschleunigten (Abkehr vom Konzept der Gravitation) Bewegung im gekrümmten Raum (allgemeine Relativitätstheorie, Raumkrümmung).

Im geografischen Koordinatensystem der Erde gibt es Sonderfälle von Großkreisen. Sie sind besonders gelagerte Großkreise. Diese Sonderfälle sind der Äquator (hier durchgezogene blaue Linie) sowie die Längenkreise (hier gelbe Linie). Der Äquator ist der Großkreis, der die Erdkugel in der Mitte zwischen Süd- und Nordpol trennt. Die Längenkreise gehen durch den Süd- und durch den Nordpol. Auf ihnen liegen die Meridiane, die sich jeweils vom Nord- zum Südpol erstrecken. Wie z. B. der Nullmeridian (0°) und der 180°-Meridian. Die Meridiane werden auch Längengrade genannt. Hingegen sind die Breitenkreise (hier gestrichelte Linien), mit Ausnahme des Äquators, keine Großkreise, sondern kleiner als der maximale Kugelumfang. Man nennt sie deshalb Neben- oder Kleinkreise.

Auf Großkreisen der Erde entspricht eine Bogenminute einer Seemeile, abgekürzt "sm" (engl. "nautical mile", "nm" oder "NM"). Sie wird (also als „Längenminute“ bzw. als „Breitenminute am Äquator“) mit 1852 Metern errechenbar bei einem angenommenen Erdumfang von 40.000 km. Der mittlere Erdradius beträgt 6371 km.

Die kürzeste Verbindung zwischen zwei Punkten auf einer Kugeloberfläche – die sogenannte Orthodrome – ist immer Teil eines Großkreises (der sogenannte Hauptbogen). Deshalb führen Schifffahrts- und vor allem Flugrouten meist entlang von Großkreisen. Das Befahren der Erdkugel auf Orthodromen wird Großkreissegeln genannt; bei Start- und Zielpunkt auf ähnlicher geographischer Breite verlaufen die „Großkreiskurse“ dabei über etwas größere Breiten (z. B. München–Peking über Sibirien).

Auf dem Erdellipsoid und anderen Flächen wird die Orthodrome geodätische Linie genannt. Sie ist eine "Kurve höherer Ordnung" (Abweichung vom Großkreis einer Kugel einige Promille) und entspricht dem Verlauf eines straff gespannten, reibungsfreien Fadens. Auf dem Erdellipsoid, z. B. nach WGS84, berechnet man Anfangskurs und Distanz nach der Formel von Thaddeus Vincenty.

Da viele Landkarten (z. B. bei der Mercatorkarte) so dargestellt werden, dass die Breitengrade als gerade, waagrechte Linien erscheinen, wirken die Flugrouten trotz ihrer Kürze gekrümmt und verlaufen weiter polwärts (siehe auch Loxodrome). Um das Zeichnen zu vereinfachen, gibt es spezielle Großkreiskarten (siehe gnomonische Projektion), auf denen alle Großkreise als Gerade erscheinen, die Umgebung allerdings etwas verzerrt ist. Kürzeste Flugrouten und lange kürzeste Schiffsrouten (z. B. bei einer Atlantiküberquerung) können auf einer Gnomonischen Karte als Gerade dargestellt werden. Der dabei zu fahrende Kompasskurs ändert sich dabei stetig und kann auf der Karte abgelesen werden als Winkel zwischen Meridian und Kurslinie.

Auf Seekarten ist am rechten und linken Rand die geografische Breite aufgetragen, d. h. der jeweilige Ausschnitt des betreffenden Längen-Großkreises. Hier kann der Nautiker mit dem Stechzirkel eine Distanz abgreifen (1 Bogenminute = 1 Seemeile = 1,852 km) und zum Einzeichnen von Positionen und Kursen in die Karte übertragen.

Der Winkel zwischen den Punkten A und B mit den Breitenkoordinaten formula_1 und den Längenkoordinaten formula_2 auf dem Großkreis berechnet sich wie folgt:

Wird formula_4 im Bogenmaß angegeben, kann die "Großkreisentfernung d" zwischen den beiden Punkten aus dem Erdradius r berechnet werden:

Die Großkreisentfernung beträgt maximal den halben Erdumfang.

Den Schnittwinkel des Großkreises von A und B mit dem Meridian im Punkt A nennt man Kurswinkel formula_6. Er berechnet sich mit:

Für östliche Kurse (λ > λ) liegt der Kurswinkel zwischen 0° und 180°, für westliche Kurse (λ < λ) liegt der Kurswinkel zwischen 180° und 360°. Im Gegensatz zur ebenen Geometrie unterscheiden sich die Kurswinkel von A nach B und von B nach A nicht um 180°. Im Extremfall, wenn der Großkreis über die Pole führt, können die beiden Kurswinkel sogar gleich sein.



</doc>
<doc id="11965" url="https://de.wikipedia.org/wiki?curid=11965" title="Liste von Religionen und Weltanschauungen">
Liste von Religionen und Weltanschauungen

Eine Reihe der Religionen und Weltanschauungen der Welt lässt sich schwer systematisieren, da vielfältige Elemente ineinanderspielen und es unterschiedliche Auffassungen dazu gibt, was eine Religion oder eine Weltsicht ausmacht (mit diesem Thema beschäftigt sich unter anderem die Religionswissenschaft). Die Systematisierung von Religion ist abendländisch geprägt, und auch wenn die Klassifizierung auf Strömungen in anderen Kulturen angewendet wird, gibt es dort teilweise keinen vergleichbaren Begriff.











Lokale, schriftlose Religionen indigener Völker, (kleinere) Volksreligionen und Religionen ethnisch-religiöser Gruppen (Aufgrund der Vielzahl hier überwiegend Hauptartikel)






















</doc>
<doc id="11966" url="https://de.wikipedia.org/wiki?curid=11966" title="UN-Kinderrechtskonvention">
UN-Kinderrechtskonvention

Das Übereinkommen über die Rechte des Kindes, kurz UN-Kinderrechtskonvention (, "CRC"), wurde am 20. November 1989 von der UN-Generalversammlung angenommen und trat am 2. September 1990, dreißig Tage nach der 20. Ratifizierung durch ein Mitgliedsland, in Kraft. Beim Weltkindergipfel vom 29. bis 30. September 1990 in New York verpflichteten sich Regierungsvertreter aus der ganzen Welt zur Anerkennung der Konvention.

Der Kinderrechtskonvention sind mehr Staaten beigetreten als allen anderen UN-Konventionen, nämlich alle Mitgliedsstaaten mit Ausnahme der USA. Zuletzt haben Somalia und Südsudan im Oktober 2015 die Kinderrechtskonvention ratifiziert. Einige der 196 Staaten (auch die Nichtmitgliedsstaaten Cookinseln, Niue, Palästina und der Heilige Stuhl) haben die Konvention ratifiziert, erklärten allerdings Vorbehalte (darunter zunächst auch Deutschland, Österreich und Schweiz).

Die Konvention (Übereinkunft) definiert Kinder als Menschen, die das 18. Lebensjahr noch nicht abgeschlossen haben, soweit die Volljährigkeit nach dem auf das Kind anzuwendenden Recht (wie z. B. in manchen islamischen Ländern) nicht früher eintritt. Dabei geht die Kinderrechtskonvention nicht genauer darauf ein, ab wann sie für das einzelne Individuum Geltung bekommt: Sei dies ab der Geburt, erst später oder schon vorher.

Sie legt wesentliche Standards zum Schutz der Kinder weltweit fest und stellt die Wichtigkeit von deren Wert und Wohlbefinden heraus. Die vier elementaren Grundsätze, auf denen die Konvention beruht, beinhalten das Überleben und die Entwicklung, die Nichtdiskriminierung, die Wahrung der Interessen der Kinder sowie deren Beteiligung.

Der Text umfasst 54 Artikel in der für völkerrechtlich verbindliche Texte üblichen Sprache; eine offizielle Fassung in „kindgerechter“ oder gar Leichter Sprache existiert nicht. Die UNICEF, die Kinderrechtsorganisation der UNO, fasst den 20 Seiten langen Text in zehn Grundrechten zusammen (Die Nummerierung entspricht nicht jener der Artikel!):

In der Praxis umfassen die Kinderrechte das Recht, in einer sicheren Umgebung ohne Diskriminierung zu leben, Zugang zu sauberem Wasser, Nahrung, medizinischer Versorgung und Ausbildung zu erhalten und bei Entscheidungen, die ihr Wohlergehen betreffen, das Recht auf Mitsprache. 

Die Grundlage für die obige Unicef-Zusammenfassung zu 10 Grundrechten bilden die 54 Artikel der Vereinten Nationen. Darin werden konkret folgende Rechte geregelt:
Die Artikel 45 bis 54 betreffen nur die Vereinten Nationen:

Zur UN-Kinderrechtskonvention gibt es 3 Fakultativprotokolle.

Das 1. Fakultativprotokoll ist über Beteiligung von Kindern an bewaffneten Konflikten, (Kindersoldaten). Es trat auf den 12. Februar 2002 in Kraft.
166 Staaten haben das 1. Fakultativprotokoll unterzeichnet und 130 Staaten haben es ratifiziert (Stand 09.06.17). Deutschland (13.12.04), Liechtenstein (04.02.05), Österreich (01.02.02) und die Schweiz (26.01.02) haben es ratifiziert.

Das 2. Fakultativprotokoll ist über Kinderhandel, Kinderprostitution und Kinderpornografie. Es trat auf den 18 Januar 2002 in Kraft.
173 Staaten haben das 2. Fakultativprotokoll unterzeichnet und 121 Staaten haben es ratifiziert (Stand 09.06.17). Deutschland (15.07.09), Liechtenstein (30.01.13), Österreich (06.05.04) und die Schweiz (19.07.06) haben es ratifiziert.

Das 3. Fakultativprotokoll betrifft das Recht auf Individualbeschwerde beim UN-Ausschuss für die Rechte des Kindes. Es trat auf den 14. April 2014 in Kraft.
50 Staaten haben das 3. Fakultativprotokoll unterzeichnet und 34 Staaten haben es ratifiziert (Stand 09.06.17). Österreich hat es lediglich unterzeichnet, Deutschland (28.02.13 ), Liechtenstein (25.01.17 ) und die Schweiz (24.04.17) haben es ratifiziert.

Die Einhaltung der Bestimmungen der Konvention überwacht das zuständige UN-Vertragsorgan, der UN-Ausschuss für die Rechte des Kindes, der periodisch die Berichte der Unterzeichnerstaaten entgegennimmt und auswertet.

National Coalition:
In der National Coalition für die Umsetzung der UN-Kinderrechtskonvention (UN-KRK) haben sich derzeit in der Bundesrepublik Deutschland rund 100 bundesweit tätige Organisationen und Initiativen aus verschiedenen gesellschaftlichen Bereichen zusammengeschlossen mit dem Ziel, die UN-Kinderrechtskonvention in Deutschland bekannt zu machen, ihre Umsetzung zu kontrollieren und voranzubringen. Sie erstellt zu den periodischen Staatenberichten jeweils ergänzende Berichte, auch Schattenberichte genannt, in denen sie sich kritisch mit den Staatenberichten auseinandersetzt.

Bereits im 19. Jahrhundert gab es Bestrebungen, das Elend von Kindern der unteren Gesellschaftsschichten in den sich industrialisierende Staaten durch Schutzgesetze abzumildern. Von besonderer Bedeutung sind hierbei Gesetze zum Verbot der Kinderarbeit.

Die UN-Kinderrechtskonvention steht in der Tradition der internationalen Kinderschutzbewegung, die sich im „Jahrhundert des Kindes“ (so der Titel des 1902 erschienenen Buches der schwedischen Pädagogin Ellen Key) dazu aufgerufen fühlte, die Probleme der Jugendhilfe einem Erfahrungsaustausch und Lösungsansätzen über die nationalen 
Grenzen hinwegzuführen. Schon der Erste Internationale Kinderschutz-Kongress, der 1913 in Brüssel durchgeführt 
wurde, diskutierte über internationale Verträge zum Schutz des Kindes, so über die Ausarbeitung einer Konvention zur 
Durchsetzung von Unterhaltstiteln im Ausland. Diese Aufgaben wurden nach dem Ersten Weltkrieg vom Völkerbund 
übernommen.

Die britische Sozialreformerin Eglantyne Jebb schuf als Reaktion auf die Verelendung von Kindern im Ersten Weltkrieg am 15. April 1919 den "Save the Children Fund" und sammelte dafür mittels Fundraising Spenden. Im Jahr 1920 entstand auf ihre Initiative hin die „International Save the Children Union“. 1921 konzentrierte sich die Hilfsorganisation auf Unterstützungen für Kinder in Griechenland und in Saratow. Jebb arbeitete 1923 ein Papier über Kinderrechte aus und mobilisierte für ihre "Children’s Charter" den Völkerbund. Ihre Idee wurde aufgegriffen, und am 24. September 1924 wurde von der Generalversammlung des Völkerbundes in Genf eine Charta verabschiedet. Ein Recht auf Bildung sieht die „Genfer Erklärung“ vom 26. September 1924, die der Charta folgte, nicht vor. Statt dessen heißt es in der Erklärung: „Das Kind soll in die Lage versetzt werden, seinen Lebensunterhalt zu verdienen […].“

Die Generalversammlung der 1945 gegründeten Vereinten Nationen, der Nachfolge-Organisation des Völkerbundes, fügte 1948 in ihre "Allgemeine Erklärung der Menschenrechte" Aussagen zugunsten der Kinder ein, die auf einen Schutz abzielten. Am 20. November 1959 verabschiedete die Generalversammlung die „Deklaration über die Rechte der Kinder“, griff dabei auf Eckpunkte der früheren "Genfer Deklaration" zurück und ergänzte sie. In dem Jahr 1979, dem Jahr des Kindes, legte Polen Entwürfe für eine Kinderrechtskonvention vor, die zur Ausgangsbasis für das Übereinkommen vom 20. November 1989 wurden.

Der Deutsche Bundestag hat der Kinderrechtskonvention mit Gesetz vom 17. Februar 1992 () zugestimmt. Nach Ratifikation am 6. März 1992 ist die Konvention am 5. April 1992 für die Bundesrepublik Deutschland in Kraft getreten (). Die dabei zunächst erklärten Vorbehalte sind 2010 zurückgenommen worden ().

Ein Nationaler Aktionsplan für ein kindergerechtes Deutschland 2005–2010 dient der Bundesrepublik zur Umsetzung der Kinderrechtskonvention. Er ist eine Initiative der Bundesregierung, die aus dem Abschlussdokument „Eine kindgerechte Welt“ der Vereinten Nationen, 2002 in New York, hervorgegangen ist. Basis dieses Aktionsplans ist dementsprechend die UN- Konvention über die Rechte des Kindes. Das Grundanliegen des deutschen NAP ist die Verbesserung der Lebensbedingungen von Kindern sowie ihrer Rechte. Hierzu wurde er in sechs Themenfelder unterteilt:

Obwohl deutsche Delegierte noch 1988 kundtaten dieser Konvention keine Zustimmung zu erteilen, unterschrieb 1992 die Bundesregierung trotz weiterer Proteste die UN-Kinderrechtskonvention, jedoch nur unter ausländerrechtlichen Vorbehalten, nach denen das deutsche Ausländerrecht Vorrang vor Verpflichtungen der Konvention hat. Deutschland verhängte neben Österreich als einziges weiteres Land in Europa Abschiebehaft gegen Kinder und Jugendliche. Allein in Hamburg befanden sich 2003 etwa 125 Minderjährige länger als drei Monate in Abschiebehaft.

Nach Zustimmung des Bundesrates hat die Bundesregierung am 3. Mai 2010 beschlossen, die bei der Ratifizierung der UN-Kinderrechtskonvention abgegebene Vorbehaltserklärung zurückzunehmen. Die rechtsverbindliche Rücknahme-Erklärung wurde am 15. Juli 2010 bei der UN in New York hinterlegt. Damit gilt Art. 3 Abs. 1 UN-KRK unbeschränkt, das heißt „bei allen Maßnahmen, die Kinder betreffen, gleichviel ob sie von öffentlichen oder privaten Einrichtungen der sozialen Fürsorge, Gerichten, Verwaltungsbehörden oder Gesetzgebungsorgan getroffen werden, [ist …] das Wohl des Kindes ein Gesichtspunkt, der vorrangig zu berücksichtigen ist“. In diesem Art. 3 UN-KRK wird teilweise ein bislang noch weitgehend unberücksichtigtes Potential für die innerstaatliche Rechtsanwendung, sowohl in materiell- wie prozessrechtlicher Hinsicht vermutet.

Es ist Pflicht und Aufgabe aller deutschen Behörden und Gerichte, dem Vorrang des Kindeswohls Geltung zu verschaffen, indem sie ihre Entscheidungspraxis an Abwägungs- und Begründungserfordernissen der Konvention ausrichten.

Die deutsche Bundesregierung hob allerdings 2014 hervor, dass es keinen Bedarf mehr gebe, das innerstaatliche Recht zu ändern: „Das Übereinkommen setzt Standards, die in der Bundesrepublik Deutschland verwirklicht sind, und bietet keinen Anlass, grundlegende Änderungen oder Reformen des innerstaatlichen Rechts zu betreiben.“

Österreich unterzeichnete die UN-Konvention bereits mit den Erstunterzeichnerstaaten 1990.

Die Schweiz hatte bei der Ratifizierung der Kinderrechtskonvention zu fünf Artikeln insgesamt sieben Vorbehalte geltend gemacht. Die folgenden drei Vorbehalte gelten weiterhin:

Zurückgezogen wurden die folgenden vier Vorbehalte:

In Liechtenstein ist die UN-Konvention seit dem 21. Januar 1996 in Kraft. Per 1. Oktober 2009 waren die beiden Vorbehalte zu Artikel 7 und 10 sowie die Erklärung zu Artikel 1 zurückgenommen worden.
Seit 1989 gilt der 20. November als "Internationaler Tag der Kinderrechte" oder "Weltkindertag". Der Tag wird vielfach zum Anlass genommen, Rechte von Kindern in aller Welt zu thematisieren. Deutschland entschied sich hingegen für den 20. September als (deutschen) Kindertag.

Von den Kindern bzw. ihren Eltern sind die Rechte, die die Konvention garantiert, nur dann vor Gericht einklagbar, wenn das in der Rechtsordnung des zuständigen Staates vorgesehen ist. Im Übrigen besagt Art. 4 Abs. 1 der Konvention, dass aus der Konvention direkt keine individuellen Rechtsansprüche abgeleitet werden können. Um Normen der Konvention praxisrelevant zu machen, müssen sie in nationales Recht überführt werden. 

An mehreren Stellen wird in der Charta auf die (finanziellen) Möglichkeiten des Staates Bezug genommen, der die Kinder auf seinem Gebiet schützen soll. Kein Staat muss also mehr leisten, als er leisten kann. Deutlich wird die Rücksichtnahme auf finanziell schlecht ausgestattete Staaten auch durch die Sachverhalte, die die Konvention "nicht" regelt, wie z. B. ein Recht von Schulkindern auf Unterricht im Anschluss an die Grundschule. Ein derartiges Recht würde die Staatshaushalte armer Länder stark belasten sowie ihre Fähigkeit, entsprechende Infrastrukturen bereit zu halten, überfordern.





</doc>
<doc id="11967" url="https://de.wikipedia.org/wiki?curid=11967" title="Katholizismus">
Katholizismus

Katholizismus bezeichnet die Repräsentation des römisch-katholischen Christentums in der Gesellschaft, basierend auf der durch den katholischen Glauben geprägten Weltanschauung und Wertvorstellung. Dies schließt insbesondere die sich daraus ergebenden politischen, staatlichen und sozialen Aktivitäten nicht nur der institutionellen Strukturen, sondern auch der katholischen Gläubigen, deren gesellschaftliche Organisationsformen und Brauchtum ein.

Im Zuge der Entwicklung der katholischen Soziallehre kam der Begriff des Katholizismus im 19. Jahrhundert mit einer politisch-programmatischen Bedeutung als Kontrast zu Marxismus und Liberalismus auf. Daraus ging der politische Katholizismus hervor, der im Kulturkampf eine besondere Rolle spielte und die Grundlage der Deutschen Zentrumspartei bildete. Als Antwort auf die sozialen Entwicklungen entstanden nach den Grundideen von Wilhelm Emmanuel von Ketteler und Adolph Kolping Gesellen- und Arbeitervereinen sowie der Volksverein für das katholische Deutschland. Zur Förderung der Wissenschaft wurden Görres-Gesellschaft und Österreichische Leo-Gesellschaft gegründet. In Deutschland stehen die seit 1848 stattfindenden Katholikentage und international die Katholische Aktion in besonderer öffentlicher Wahrnehmung.

Die Gesellschaften vieler Staaten Europas und Lateinamerikas sind katholisch geprägt, in manchen Staaten wie Irland und Polen bildet der Katholizismus einen Bestandteil der nationalen Identität. Im deutschsprachigen Raum ist er vor allem im Westen und Süden Deutschlands und Österreich verbreitet.

Der Begriff "katholisch" stammt aus dem Griechischen und bedeutet dort ‚allumfassend, total, universell‘. Das griechische καθολικός leitet sich (auch heute) von κατά "katá" ‚von […] herab, über […] hin‘; ὅλος "hólos" „ganz, umfassend“ ab. In diesem Sinne wird er schon von Aristoteles oder Polybios, aber auch von frühen christlichen Schriftstellern verwendet; so spricht etwa Justin der Märtyrer von der „katholischen Auferstehung“. Diese Bedeutung hat sich in der Bezeichnung Katholische Briefe für einige neutestamentliche Bücher erhalten. Die Wortverbindung „katholische Kirche“ wurde erstmals von Ignatius von Antiochien um das Jahr 110 verwendet, um diese von Kleingruppen abzugrenzen.

Seit der Reichsgründung 1871 waren die Katholiken mit etwa einem Drittel Bevölkerungsanteil eine Minderheit in Deutschland. Das änderte sich nach 1945, als die vorwiegend protestantischen Gebiete des deutschen Nordostens verloren gingen bzw. Kerngebiete der Reformation in Mitteldeutschland zur sowjetischen Besatzungszone wurden. In der aus den verbliebenen westdeutschen Ländern gebildeten Bundesrepublik bildeten evangelische und katholische Christen jeweils knapp die Hälfte der Bevölkerung mit einem knappen evangelischen Überhang bis in die 1990er Jahre, wobei das politisch aktive und besonders im Rahmen der nun bewusst überkonfessionellen CDU stark engagierte katholische Milieu erhebliche Wirkung auch über seine engeren Grenzen hinaus entfalten konnte.

In der Bundesrepublik von 1949 bis 1989 hatte das römisch-katholische Milieu bis in die 1970er Jahre einen großen gesellschaftspolitischen Einfluss. Nicht nur Bundeskanzler wie Konrad Adenauer und Helmut Kohl, sondern auch ein Kritiker wie Heinrich Böll waren besonders im rheinischen Katholizismus verwurzelt. Führungspersönlichkeiten wie die Kardinäle Joseph Frings, Julius Döpfner und Joseph Höffner wurden stark beachtet, in etwa auch noch Karl Lehmann. Die Deutsche Bischofskonferenz, jetzt unter ihrem Vorsitzenden Reinhard Marx, steht für eine große öffentliche Wahrnehmung der römisch-katholischen Kirche ein. Trotz der Folgen der staatlichen Wiedervereinigung (relativ starker Anstieg des statistischen Anteils konfessionsloser Deutscher) bemüht man sich, diesen Einfluss auch weiterhin zu sichern. Dennoch kann man nicht von einer an sich klerikalen Republik sprechen.

Nach statistischen Daten der Deutschen Bischofskonferenz ist die Zahl der Katholiken in Deutschland (jeweils 1990 zu 2007) von 28.252.000 auf 25.461.000, die Zahl der Taufen von 299.796 auf 185.586, die Zahl der Trauungen von 116.332 auf 49.393 und der Anteil der Gottesdienstteilnehmer von 21,9 auf 13,7 Prozent zurückgegangen.

Die von Papst Benedikt XVI. anlässlich des Weltjugendtages in Köln 2005 geforderte Hinwendung der Kirche in Deutschland dazu, „missionarisch“ zu werden, könnte eine neue, zum kulturellen Umfeld eher kontrastierende Tendenz begünstigen. Die starke ökumenische Orientierung der römisch-katholischen Kirche in Deutschland wird derzeit jedoch noch fortgesetzt. Diese ökumenische Ausrichtung wird von manchen Theologen kritisiert, da die römisch-katholische Kirche durch die Hinwendung zum Protestantismus ihre eigene Identität aufzugeben scheine.

Infolge des Josephinismus wies der österreichische Katholizismus während der Habsburgermonarchie eine ausgeprägte Nähe zum Staat auf. Während des Austrofaschismus von 1933 bis 1938 galt Österreich für manche konservativen Katholiken dann geradezu als „Musterstaat“. Österreichische Bischöfe haben den Anschluss 1938 an das Großdeutsche Reich überdies viel wohlwollender begrüßt als deutsche Bischöfe. Aus dieser historischen Situation heraus belastet, hat der Episkopat das II. Vatikanum zum Anlass genommen, sich weitgehend liberal zu profilieren, doch verstärken sich seit etwa 1990 konservative Tendenzen. 

Die Situation der katholischen Kirche in der Schweiz ist noch schwieriger zusammenzufassen, da die konfessionelle und staatskirchenrechtliche Eigenart jedes einzelnen Kantons berücksichtigt werden muss. In manchen Kantonen üben staatliche Stellen über eigene Organe eine mittelbare Kontrolle kirchlicher Angelegenheiten aus.

Papst Pius IX. setzte 1870 die Definition der päpstlichen Unfehlbarkeit in Lehrfragen durch. Diese „Geburtsurkunde“ des römischen Anspruchs in moderner Zeit bewirkte, außerhalb der eigentlich theologischen Probleme, eine eindeutige Zuordnung des Papsttums zum „geistlichen Bereich“. Die zunächst unfreiwillige Abkehr von päpstlich-kirchenstaatlicher Politik beseitigte zwangsläufig auch eine wesentliche Ursache der Intransigenz der Päpste Leo XII. bis Gregor XVI. Der so zugleich begründete päpstliche Internationalismus brachte Papst Leo XIII., dem Begründer der neueren katholischen Soziallehre († 1903), großes Ansehen ein. In der Konfrontation mit dem optimistischen Humanismus der Moderne kämpfte Papst Pius X. (1903–1914) um eine größere religiöse Wirksamkeit der Kirche in der Gesellschaft. Im Ersten Weltkrieg gelang es Papst Benedikt XV. überdies, den Katholizismus als überparteilich und supranational zu festigen. Seither hat die römisch-katholische Kirche die wesentlichen Forderungen der internationalen Friedensbewegung (Schiedsgerichtsbarkeit, Abrüstung) in ihr Programm integriert.

Der Sturz vieler europäischer Monarchien 1918 und die Befreiung der nichtkatholischen Staatskirchen von politischer Bevormundung eröffnete auch neue Möglichkeiten des interkonfessionellen Dialogs, der aber erst nach den Erfahrungen des Zweiten Weltkrieges breite Akzeptanz auf römisch-katholischer Seite fand.

Die programmatische Selbstkorrektur durch das Zweite Vatikanische Konzil (1962–65), vorbereitet durch die Päpste Pius XI. und Pius XII., durchgeführt von den Päpsten Johannes XXIII. und Paul VI., hat aber gerade in den typisch römisch-katholischen Milieus interne Krisen heraufbeschworen. In jedem der katholisch geprägten Länder war die Situation der Kirche noch nicht frei von Belastungen aus den politischen Konflikten. In Spanien fand die Kirche erst allmählich eine Distanz zum Franco-Regime. In Italien besteht auch heute noch Dissens, ob die "civiltà cattolica" eine eher christlich demokratische oder eine eher traditionell autoritätsbezogene politische Haltung begünstigt. Besonders gravierend ist die Situation in Frankreich, wo der nationale Katholizismus in Opposition zur Republik stand, so dass es (vor allem vor 1914 und nach 1945) auch zu Übertreibungen in der Gegenrichtung kam. Jüngere kirchliche Bewegungen in diesen Ländern werden seitens liberaler Theologie oft mit politischen Kategorien bewertet, ohne dass dies dem spirituellen Anliegen des "renouveau catholique" entspräche.

Der Schwerpunkt des weltweiten Katholizismus hat sich seit den 1980er Jahren nach Lateinamerika, Afrika und allmählich auch Asien verlagert. Afrika südlich der Sahara wendet sich verstärkt dem Christentum zu. Die römisch-katholische Kirche in Lateinamerika steht aber, wegen ihrer langen Bindung an die europäisch-katholische Tradition, vor besonders gravierenden Herausforderungen. Hier wurde seit den 1960er Jahren der Versuch einer Befreiungstheologie unternommen, die von der Kirche jedoch als Rückfall in Konzepte, die eine politische Theologie begünstigen, jetzt aber unter marxistischer Perspektive zu sehen sei, bekämpft wurde.

Im anglo-amerikanischen Kulturraum hat die römisch-katholische Kirche seit dem 19. Jahrhundert nach und nach an Akzeptanz gewinnen können, ist aber noch immer als konfessionelle Minderheit zu sehen.

Insgesamt steht der Katholizismus zu Beginn des 21. Jahrhunderts erst am Anfang der ihm vom Zweiten Vatikanischen Konzil gestellten Aufgabe, gleichermaßen die religiöse Tradition fortzuführen und zugleich inmitten des jeweiligen kulturellen Umfeldes „auf der Höhe der Zeit“ mitzuwirken. Papst Johannes Paul II. versuchte in seinem Pontifikat, den Selbstvollzug der Kirche als Weltkirche in einem universalen und interreligiösen Horizont persönlich zu verorten; mit seinen Reisen und Lehrschreiben, den Weltjugendtagen und Heiligsprechungen erlangte er dabei große Aufmerksamkeit.

Unmittelbare Sonderrechte innerhalb der Staatsordnungen strebt der Katholizismus ausdrücklich nicht mehr an. So vereinbarten Italien und der Heilige Stuhl im Jahre 1984 den Verzicht auf die römisch-katholische Staatsreligion.

Der Begriff Katholizismus wird wissenschaftlich von der Konfessionskunde, der Phänomenologie und Soziologie benutzt, um die Praxis des katholischen Glaubens durch den einzelnen Gläubigen, aber auch die gesellschaftliche Relevanz des katholischen Glaubens zu beschreiben. Entsprechendes gilt für das Wort Protestantismus, das dies bei den protestantischen Christen beschreibt.

Der Katholizismus gilt traditionell sowohl in Bezug auf Moral, als auch politisch als konservativ, aber auch volkstümlich (Volksfrömmigkeit). Eine große Rolle spielen weiterhin das Gemeinschaftsbewusstsein sowie die religiös-kulturelle Tradition. Innerhalb des Katholizismus bestehen auch diverse Strömungen, die häufig mit der Politik entlehnten Begriffen zusammengefasst werden (insbesondere Linkskatholizismus) bzw. als „Progressisten“ und „Traditionalisten“ bezeichnet werden.

Vor dem Hintergrund globaler gesellschaftlicher Veränderungen befindet sich der Katholizismus, jedenfalls sofern er selbst einen traditionellen Kulturkreis prägte, zwar in einem grundlegenden Umgestaltungsprozess. Unter der Jugend in Europa ist der überlieferte Katholizismus auch weiterhin ein Minderheitenphänomen. Die Auflösung der typisch katholischen Milieus bewirkt bei progressiven wie konservativen Kräften aber entgegengesetzte Reaktionen. Die Gegenwehr des Integralismus vermag nur eine sehr geringe Minderheit der Katholiken anzuziehen (z. B. Lefebvre-Bewegung, mit der höchstens 0,015 % der 1,1 Mrd. Katholiken sympathisieren), da die traditionelle Volksfrömmigkeit stark nachlässt. Aus sozialwissenschaftlicher Sicht wirken die konfessionellen Milieus jedoch auch dann noch auf die persönlichen Verhaltensmuster, wenn die eigentlich kirchliche Bindung bereits nicht mehr bewusst empfunden wird.

Zentrales Element im Verständnis der römisch-katholischen Kirche ist die kirchliche Hierarchie mit dem Papst als oberstem Bischof. Die als „petrinisches Prinzip“ bezeichnete Funktion des Bischofs von Rom unterscheidet diese Kirche sichtbar von anderen Konfessionen. Als Nachfolger des Apostels Petrus gilt der Papst in der römisch-katholischen Kirche als Fels der Gesamtkirche und Stellvertreter Christi auf Erden.

Dem Vorwurf, dass sich das Papsttum in die Politik einmische, wird von der römisch-katholischen Kirche entgegengehalten, dass der christliche Glaube über die politischen und gesellschaftlichen Sphären hinaus reiche. Das petrinische Prinzip etabliert einen religiösen Internationalismus, einen weltweiten öffentlichen Anspruch. Das wiederum findet sich in der ursprünglich Wortbedeutung von „katholisch“ als „allgemein“ wieder.

In der Folge dieses Anspruchs, die Religion der Politik und der Gesellschaft überzuordnen, setzte sich der Heilige Stuhl mit dem jeweiligen Gegenüber im staatlichen Bereich auseinander. Mit ihrer jahrhundertelangen Diplomatie weisen die päpstlichen Institutionen einen großen Erfahrungsschatz auf. Heute wird die Institution des Papsttums (nicht nur der Vatikanstaat) von fast allen Staaten der Welt auch völkerrechtlich anerkannt.

Während sich die Orthodoxie in ein orientalisches Staatskirchentum einfügte, traten die Bischöfe von Rom in einen vielhundertjährigen Konflikt mit den Staatsgewalten ein. Im Bereich der lateinischen Kirche trat der Anspruch des päpstlich formulierten Primats mit besonderer Deutlichkeit im 11. Jahrhundert in Erscheinung (Gregor VII., "Dictatus Papae", 1075).

In der Zeit seit dem Spätmittelalter nahm die Machtfülle der Territorialstaaten zu. Diese nutzten die Reformation zum Zweck weiterer Steigerung ihrer Autorität aus, übrigens auch in den katholischen Monarchien. Seit dem Westfälischen Frieden von 1648 deshalb fast vom „diplomatischen Parkett“ verschwunden, schien das Papsttum vor 1789 den absoluten Monarchien insgesamt unterlegen zu sein. In nachnapoleonischer Zeit gelang jedoch ein schrittweiser Wiederaufstieg.

Heute urteilen auch manche außerkirchliche Beobachter, dass der Katholizismus im 20. Jahrhundert angesichts der „Krise des Humanismus“ einen wesentlichen Beitrag für den Fortbestand der Zivilisation geleistet habe.



</doc>
<doc id="11968" url="https://de.wikipedia.org/wiki?curid=11968" title="Taler">
Taler

Der Taler, dt. Schreibweise bis 1901 auch "Thaler," ist ein Typus einer großen, von der Frühneuzeit bis ins 19. Jahrhundert bedeutenden europäischen Silbermünze. Silbermünzen, die vom zeitgenössischen Wert her einer goldenen Guldenmünze entsprachen, wurden erstmals 1486 in Hall in Tirol geprägt. Solche Münzen breiteten sich ab 1500 allmählich in ganz Europa und darüber hinaus aus. Die im böhmischen Joachimsthal (heute Jáchymov) geschlagenen Guldengroschen wurden zunächst „Joachimsthaler“ und später, entsprechend dem im Volksmund als „s Tal“ bezeichneten Joachimstal, verkürzt „Thaler“ genannt, was zum Gattungsbegriff für Münzen dieser Größe wurde. Der gesetzmäßige Silbergehalt der verschiedenen Taler nahm von den Reichsguldinern (1524) bis zu den Vereinstalern von etwa 27,4 g auf 16,7 g Feinsilber ab.

In Deutschland blieb der Taler in Form des Vereinstalers (1857–1871) bis zur Einführung der Mark die wichtigste große Silbermünze. Am 19. Mai 1908 wurde parallel zur Außerkurssetzung der Vereinstalers die Ausgabe von wertgleichen 3-Mark-Stücken beschlossen. Sie wurden im Volksmund weiterhin als „Taler“ bezeichnet. Auch außerhalb Deutschlands waren Silbermünzen in Talergröße beliebt; so leitet sich auch die Bezeichnung "Dollar" von der niederdeutsch/niederländischen Aussprache von "Taler" ab.

Der Tiroler Guldiner von 1486 wurde auch Großer Groschen oder Großer Pfennig genannt und war der erste seiner Art. Er entstand, als man im mit neu entdeckten Silbervorkommen gesegneten Tirol auf die Idee kam, eine Silbermünze im Wert eines Guldens zu prägen. Aus Gold geprägte Gulden (siehe auch Florentiner) hatten sich seit dem Spätmittelalter von Norditalien aus als beliebte Geldsorte für hohe Zahlungen verbreitet. Bei einem Wertverhältnis zwischen Gold und Silber von 11,58:1 musste der Guldiner ein Feingewicht von 29,9 g Silber haben. Bei einer Silberlegierung von 15 Lot = 937,5 ‰ entspricht das einer theoretischen Münzmasse (Raugewicht) von fast 32 Gramm. Tatsächlich ergaben Messungen, dass das Raugewicht zwischen 27,17 g und 32,02 g schwankte. So große Silbermünzen waren für die Zeit neu und konnten erst geprägt werden, nachdem einige technische Probleme gelöst worden waren. Die großen Unterschiede im Raugewicht scheinen darauf hinzudeuten, dass es sich hier weniger um eine Umlaufmünze als vielmehr um Repräsentationsprägungen handelte. Dies gilt auch für die vielen frühen Nachahmungen.

Da die Münze dem Wert eines Guldens entsprechen sollte, wurde sie als „Guldiner“ bezeichnet. In der Folge blieb der Name „Guldiner“ oder „Gulden“, an der Silbermünze haften. Der Gulden aus Gold wurde nun als „Goldgulden“ bezeichnet – ein Pleonasmus. Der Guldiner wurde in 60 Kreuzer unterteilt, und im Laufe der Zeit setzte sich dieser Wert als Rechnungsmünze durch: 1 Gulden war die Maßeinheit für 60 Kreuzer, auch wenn die geprägte Münze höher bewertet wurde, weil der Silbergehalt der ausgeprägten Kreuzer stetig sank.

Bereits 1492 und 1493 hatten die Münzstätten Zwickau und Schneeberg im silberreichen Kurfürstentum Sachsen Bartgroschen sowie ab 1496 Zinsgroschen geprägt. Die eigentliche Geschichte des Talers als Umlaufmünze beginnt dann 1500, als Kursachsen anfing, einen „groschen so einen gulden tut“ auszugeben.

Der Kurfürst Friedrich der Weise erließ im Einvernehmen mit Herzog Albrecht, der durch seinen Sohn Georg vertreten war, und seinem Bruder Johann die sächsische Münzordnung von 1500, die als Muster für andere Münzstände und auch als Grundlage für die Reichsmünzordnungen des 16. Jahrhunderts diente. Aus einer rauen Kölner Mark (= 233,86 g) sollten acht Münzen geschlagen werden. Diese Münze hatte also eine Masse von 29,23 g. Bei einem Feingehalt von 15 Lot = 937,5 ‰ betrug das Feingewicht formal 27,41 g Silber, d. h. es wurden 8 Münzen aus einer Kölner Mark Feinsilber geprägt. Seit ca. 1505 wurde der Feingehalt um 2 Grän auf 14 Lot = 930,6 ‰ vermindert, so dass das neue Feingewicht etwa 27,2 g betrug (zu Schwankungen der Masseangaben siehe z. B. Rittmann, Geldgeschichte, S. 725).

Da das Münzbild den Kurfürsten und die beiden Herzöge mit Klappmützen zeigte, wurde der bis 1525 geprägte sächsische Guldengroschen später, nachdem sich die Bezeichnung Taler durchgesetzt hatte, als „Klappmützentaler“ bezeichnet. Er wurde in den Münzstätten Annaberg, Buchholz, Leipzig und eventuell auch in Wittenberg ausgemünzt.

Auch im böhmischen Erzgebirge wurde ab 1516 Silber in großen Mengen abgebaut, und seit 1519 ließen die Grafen Schlick nach dem sächsischen Münzfuß – also 29,232 g rau und 27,202 g fein – riesige Mengen Guldengroschen schlagen. Nach ihrem Herkunftsort Joachimsthal wurden sie bald „Joachimsthaler“, später verkürzt „Thaler/Taler“, genannt. Dieser Name setzte sich ab Mitte des 16. Jahrhunderts für alle Silbermünzen dieser Größe durch. Der Joachimsthaler trug auf der einen Seite das Bild des Ortsheiligen, des hl. Joachim, und auf der Rückseite das Löwenwappen Böhmens, weswegen die Münze alternativ auch als „Löwengroschen“ bezeichnet wurde. Ab 1536 wurde der Feingehalt des Joachimsthalers auf 14 Lot 8 Grän = 902,77 ‰ und damit auf 26,39 g Silber reduziert.

Die Versuche, eine reichseinheitliche, von allen Reichsständen akzeptierte Münzordnung zu schaffen, scheiterten auf den Reichstagen von 1524, 1551 und 1559. Stets weigerten sich einige Münzstände, die Bestimmungen einzuhalten. Allerdings hatten zwei der getroffenen Verfügungen Bestand: 1) Auf dem Reichstag in Esslingen 1524 war die Kölner Mark (= 233,856 g) als Münzgrundgewicht für das ganze Reich festgelegt worden. Sie wurde erst 1857 durch das Zollpfund zu 500 g ersetzt. 2) Auf dem Reichstag von Augsburg 1559 hatte sich endgültig die Erkenntnis durchgesetzt, dass sich die ursprüngliche Einheit von Goldgulden = Silbergulden = 60 Kreuzer nicht mehr aufrechterhalten ließ. Der Goldgulden sollte 72–75 Kreuzer gelten. Der Reichsguldiner, der schon 1524 mit 63 Kreuzern und 1551 mit 72 Kreuzern bewertet worden war, wurde nun auf 60 Kreuzer, den traditionellen Wert des Guldens, heruntergesetzt. So sollten Rechnungsmünze und ausgeprägte Münze wieder zusammenfallen. Dies bedeutete aber auch, dass ab sofort Taler und Reichsguldiner zwei verschiedene Münzen waren: die großen, jetzt allgemein „Taler“ genannten Münzen mit einem Feingehalt von ca. 27 g und die um einiges geringerhaltigen Reichsguldiner mit einem Feingehalt von 22,907 g. Letztere sind als die ersten (Silber-)Gulden im eigentlichen Sinne anzusehen. Sie wurden mit nennenswerten Prägezahlen unter dem Namen „Reichsguldiner“ oder „Guldentaler“ nur in Österreich und Nürnberg geprägt.

Da die Taler bereits in großen Mengen im Reich umliefen und vor allem in Norddeutschland immer noch in großen Mengen geprägt wurden, weigerten sich viele Reichsstände, die Prägung des Reichsguldiners von 1559 auch nur in Erwägung zu ziehen. Auf dem Reichstag in Augsburg 1566 trug man dann der Realität Rechnung und erließ zur Reichsmünzordnung von 1559 eine Ergänzung, die auch den Taler in den Rang einer Reichsmünze erhob. Von diesem Reichstaler sollten – wie schon traditionell – 8 aus einer rauen Kölner Mark geprägt werden. Der Silbergehalt der Münzlegierung wurde allerdings etwas reduziert. Der Feingehalt sank auf 888,89 ‰ (damals: 14 Lot 4 Grän). Somit konnten genau neun Reichstaler aus einer Kölner Mark Feinsilber geprägt werden. Der Reichstaler hatte damit ein rechnerisches Feingewicht von 25,984 g bei einer Masse von 29,232 g. Der Reichstaler wurde in der Reichsmünzordnung von 1566 mit 68 damaligen Kreuzern bewertet. Er setzte sich nach den üblichen Einwänden relativ schnell überall im Reich und auch darüber hinaus durch.

Ein großes Problem, das sich durch die ganze deutsche Münzgeschichte bis weit ins 19. Jahrhundert hinzog, war die ständige Münzverschlechterung. Betrügerische Münzherren reduzierten den Edelmetallgehalt ihrer Prägungen immer wieder, vor allem bei den kleineren Nominalen. Zu einer Zeit, da der Edelmetallgehalt einer Münze für deren Wert entscheidend war, bedeutete dies, dass Kleinmünzen wie Kreuzer, Groschen und Schillinge kontinuierlich an Wert verloren. Wurde der Reichstaler anfänglich mit 68 Kreuzern bewertet, so stieg er schnell auf 72 Kreuzer. Die Zeitgenossen beschwerten sich ständig über das „Steigen des Talers“; dies war Folge der Verschlechterung der Kleinmünzen.

Nach der Hyperinflation der Kipper- und Wipperzeit wurden die Kleinmünzen ab 1623 ca. 40 Jahre lang stabil ausgebracht. Für einen Reichstaler mussten damals 90 Kreuzer bezahlt werden. Man gewöhnte sich an dieses lange Zeit stabile Verhältnis und sah den Reichstaler bald als Rechnungsgröße für 90 Kreuzer. In Teilen Norddeutschlands wurde der Reichstaler als Wert von 24 Guten Groschen, 36 Mariengroschen oder 48 lübischen Schillingen gesehen.

Bald setzte aber wieder die Verschlechterung des Kleingelds ein, und der Reichstaler stieg erneut. Ganze Reichstaler nach den Reichsmünzfuß wurden zusehends weniger ausgeprägt.

Als im Laufe des 17. Jahrhunderts die Silberausbeute der Bergwerke im Heiligen Römischen Reich deutlich zurückging, gingen viele Münzherren dazu über, nur noch kleinere Teilstücke des Reichsspeziestalers zu prägen. Die Stelle der großen ('groben') Silbermünzen wurde zunehmend von ausländischen Prägungen übernommen. Vor allem die seit 1641 geprägten französischen Taler, die Écus blancs, bildeten in der ersten Hälfte des 18. Jahrhunderts in weiten Teilen Deutschlands die Hauptumlaufmünze, und sie wurden dem Reichsspeziestaler gleichgestellt – um 1700 waren das 1 ⅓ (Rechnungs-)Reichstaler oder 2 Gulden –, auch wenn nicht alle dessen vollen Wert erreichten, da sonst nicht genügend grobe Sorten zur Verfügung gestanden hätten. Diese Situation führte zu verschiedenen Versuchen, durch die Herausgabe neuer – d. h. jeweils im Silbergehalt verringerter – Typen von Talermünzen die Situation zu stabilisieren (oder von ihr zu profitieren). In den Habsburger Erblanden wurde anstelle des Reichstalers ein Österreichischer Taler mit eigenem Münzfuß geprägt.

Für die Finanzierung des Siebenjährigen Krieges brachten verschiedene Münzherren – allen voran Friedrich II. von Preußen – ihre eigenen Münzen mit immer schlechterem Feingehalt aus (siehe Ephraimiten). Es wird auch von einer Dritten Kipper- und Wipperzeit gesprochen. Zudem wurden eigene und fremde Münzen in großem Umfang gefälscht "(Heckenmünze)". Auch Taler und Taler-Teilstücke waren in großem Umfang betroffen (siehe Münzstätte Leipzig – unter preußischer Besatzung). Erst nach Ende des Krieges normalisierte sich die Situation.

In der ersten Hälfte des 17. Jahrhunderts sank der Silbergehalt der Kleinmünzen für längere Zeit auf 90 Kreuzer (24 Gute Groschen) pro Reichstaler; der Reichstaler wurde zur Rechnungsmünze, an der sich langfristige Verträge orientierten (siehe auch Bancotaler). Nach dem Ende des Dreißigjährigen Krieges breiteten sich aber langsam wieder unterwertiges Geld aus. Zudem flossen wegen der zurückgehenden eigenen Talerprägungen verstärkt ausländische Talermünzen ins Land, die meist nach einem etwas leichteren Münzfuß geprägt waren. Beispiele sind die spanisch-niederländischen Philippstaler und französische Silbertaler ("ecu d'argent").

Ein erster nennenswerter Versuch zur Ordnung der Verhältnisse war der Münzrezess von Zinna, der 1667 zwischen Kursachsen und Kurbrandenburg geschlossen wurde. Der ausgeprägte, vollwertige Reichstaler und sein Rechenwert waren damals in Norddeutschland auf 105 (nochmals verschlechterte) Kreuzer im Wert gestiegen. Es wurde nun vereinbart, den Reichsmünzfuß nur für die ganzen Reichstaler beizubehalten. Für Teilstücke vom ⅔-Taler abwärts sollte statt des 9-Taler-Fußes ein 10,5-Taler-Fuß gelten. Dies entspricht genau der Rückführung der Rechengröße Reichstaler von 105 Kreuzern wieder auf 90 Kreuzer.

Die nach dem Zinnaer Fuß ausgeprägten Stücke waren eigentlich unterwertige Scheidemünzen: Statt 25,98 g Feinsilber enthielten ein ⅔- plus ein ⅓-Talerstück nur noch 22,272 g Silber. Die „alten“, nach dem Reichsmünzfuß von 1566 ausgeprägten – also „in specie“ vorhandenen – Reichstaler wurden zur Unterscheidung nun "Speciestaler" oder "Reichsspeziestaler" genannt. 1668 schloss sich das Herzogtum Braunschweig-Lüneburg dem Zinnaer Vereinbarungen an. Da traditionell ein Gulden 60 Kreuzer repräsentiert, wurden die oft ausgeprägten ⅔-Taler auch Gulden genannt. Diese Zweidritteltaler wurden für längere Zeit ein vorherrschender Münztypus in Norddeutschland.

Den zweiten wichtigen Reformversuch stellt der Leipziger Rezess von 1690 dar; die Vertragspartner waren die gleichen, wie die des Zinnaischen Rezesses. Der Reichsspeziestaler war inzwischen auf einen Wert von 120 Kreuzern gestiegen. Erneut würde der Münzfuß zur Ausprägung der Scheidemünzen angepasst und zu einem 12-Taler-Fuß übergegangen (12 Taler aus einer Gewichtsmark Feinsilber). Der rechnerische Silbergehalt eines Talers nach dem Leipziger Fuß sank auf nur noch 19,488 g. Der ganze Taler war wieder nur eine Rechengröße, die nun als "Reichstaler" (Rtlr., Rthlr.) oder "Taler Courant" (Kurantaler) bezeichnet wurde. Ausgeprägt wurden ebenfalls zunächst höchstens ⅔-Stücke.

Dieser Münzfuß wurde 1738 zum Reichsfuß erhoben. Durch das glatte Verhältnis zwischen 9-Taler- und dem 12-Taler-Fuß ergab sich ein vergleichsweise sehr übersichtliches Münzsystem, das bis ins 19. Jahrhundert Bestand hatte: So ergaben 2 ⅔-Stücke wieder genau einen Reichstaler nach dem Fuß von 1566.

Wenn man von den nach dem Zinnaer und Leipziger Münzfuß geschlagenen sächsischen Klippen und Gedenktalern absieht, die zwar gelegentlich die Aufschrift „1 Thal.“ oder „1 Thal. C:“, also ein „Taler Courant“, aufwiesen, aber tatsächlich Gedenkmünzen wie Sterbetaler und Schießtalerklippen waren, wurden Kuranttaler vor Mitte des 18. Jahrhunderts nicht geprägt.

Zu den wichtigsten aus dem Ausland einströmenden Münzen gehörte der seit 1726 geprägte Écu aux lauriers, der hierzulande wegen der darauf abgebildeten Lorbeerzweige als Laub-, Lorbeer- oder Federtaler bezeichnet wurde. Zum Münzfuß der Laubtaler gibt es die unterschiedlichsten Angaben: Gesetzmäßig sollten 8 aus der Pariser Mark zu 244,753 g geschlagen werden; dies bedeutete ein Raugewicht von 29,488 g, was bei einem Feingehalt von 14 Lot 12 Grän = 916,66 ‰ eine Aufzahl von 9 und ein Feingewicht von 27,031 g ergab. Allerdings zeigten Untersuchungen Ende des 18. Jahrhunderts, dass diese Werte häufiger unterschritten wurden. Erst nach dem Aufkommen der Konventionstaler und der preußischen Reichstaler verlor der Laubtaler nach 1765 seine „Alleinherrschaft unter den groben Münzen“ in Deutschland.

Das Bestreben, sich von ausländischen Sorten unabhängig zu machen und das zerrüttete eigene Münzsystem mit den immer schlechter werdenden Kleinmünzen zu reformieren, führte 1748 in den Habsburgischen Erblanden zur Einführung eines neuen Münzfußes: Der Reichsspeziestaler, der um 1700 120 Kreuzer gegolten hatte, war bis auf 133⅓ Kreuzer gestiegen. Wie schon im norddeutschen Bereich unter preußischer Führung ging nun auch Habsburg daran, den Silbergehalt der ausgeprägten Großsilbermünzen herabzusetzen.

Der neue „Taler nach dem Konventionsfuß“ wurde durch eine Verminderung seines Silbergehalts wieder auf 120 Kreuzer herabgesetzt. Bei dem theoretischen Silberwert eines Kreuzers von 25,984 g (Reichsmünzfuß) : 133,333 (aktueller Kurs des Reichsspeziestalers) = 0,195 g ergab das für die neue Münze eine Masse von 0,195 g • 120 = 23,386 g. Aus einer feinen Kölner Mark konnten so genau 10  Konventionstaler geprägt werden. Die Münzen nach dem Konventionsfuß waren aus Silber von 833,33 ‰ Feinheit. Die Konventionstaler hatten damit ein Raugewicht von 28,063 g.

Um den neuen Münzfuß auf eine breitere Grundlage zu stellen, schloss Österreich 1753 eine Konvention mit Bayern zur Übernahme seines Münzfußes ab; seither wurden die neuen Taler als „Konventionstaler“ bezeichnet. In der Folge führten viele Staaten in Süd- und Westdeutschland das Konventionsgeld ein, allerdings ab 1754 mit einer Neubewertung des Talers: Er wurde nicht wie in Österreich mit 120 Kreuzern, sondern mit 144 Kreuzern bewertet; auf die Ausprägung der Münzen hatte dies jedoch in der Regel keine Auswirkung.

Nach Friedensschluss 1763 übernahmen eine Reihe von Staaten in Norddeutschland den Konventionsfuß. Auch hier prägte man die Konventionsmünzen, rechnete aber – wie in Süd- und Westdeutschland – anders, nämlich weiterhin nach dem Reichstaler zu 24 Guten Groschen, dem ein Wert von ¾ Konventionstalern zukam; entsprechend tragen norddeutsche Konventionstaler des Öfteren die Aufschrift „SPECIESTHALER“.

Traditionell wiesen Taler bisher keine Wertangaben auf, Teilstücke derselben konnten bei gleichem Münzbild nur an der Größe unterschieden werden, der Münzfuß war dem Normalverbraucher in der Regel unbekannt. Mit dem Aufkommen der Konventionstaler bürgerte sich der Brauch ein, die Aufzahl anzugeben, z. B. „X EINE FEINE MARK“ für den ganzen Taler, „XX EINE FEINE MARK“ für die halben Taler usw., womit der Silbergehalt eindeutig bestimmt war. Im Gebiet des heutigen Deutschland wurden Konventionstaler letztmals 1838 in Sachsen geprägt, aber dann auf Grundlage des Münchner (1837) und des Dresdner Münzvertrags (1838) durch die neue „VEREINSMÜNZE“ (s. u.) ersetzt.

Der berühmteste Konventionstaler, der Maria-Theresien-Taler, war in Österreich bis 1858 gesetzliches Zahlungsmittel, wanderte aber bereits im 18. Jahrhundert in Massen nach Vorderasien und Nordafrika ab, wo er z. B. in Äthiopien bis 1945 die Landeswährung darstellte (Levantetaler). Er wurde vielfach im Ausland und wird auch heute noch mit der Jahreszahl 1780 offiziell von der Münze Österreich weitergeprägt.

Ab 1755 ließen die Habsburger in den Österreichischen Niederlanden, dem heutigen Belgien, das zwar immer noch zum Heiligen Römischen Reich gehörte, sich innerlich aber schon längst daraus verabschiedet hatte, neue Taler prägen, die sogenannten Kronentaler. Sie lösten die Albertustaler ab, die gelegentlich auch in Deutschland nachgeprägt worden waren und eine wichtige Handelsmünze im Ostseeraum darstellten. Von diesen Brabanter Kronentalern oder Écus de Flandre gingen 9½ auf die raue Wiener Mark von 280,668 g, er wog also 29,54 g, was bei einem Feingehalt von 13 Lot 17 Grän = 871,53 ‰ ein Feingewicht von 25,75 g ergab. Ab etwa 1790, besonders aber nachdem Frankreich 1792 Belgien erobert hatte, breitete sich die heimatlos gewordene Münze rasch in Süddeutschland aus und verdrängte dort die Konventionstaler und die noch umlaufenden Laubtaler. Da der Taler auf Drängen Österreichs überbewertet wurde – 162 statt 158½ Kreuzer –, lohnte es sich nach dem Greshamschen Gesetz, die besseren älteren Taler einzuschmelzen und in minderwertigere Kronentaler umzuprägen.

Als Österreich 1800 die Ausgabe einstellte, wurden die Kronentaler langsam knapp, so dass die süddeutschen Staaten ab 1809 mit eigenen Prägungen begannen. Die Münzdaten der brabantischen Kronentaler waren nicht bekannt, deshalb musste man sich nach den umlaufenden Münzen richten, die unterschiedlich abgegriffen waren. Das führte dazu, dass jeder der sieben ausgebenden Staaten oder Münzstände seine Kronentaler mit einem etwas anderen Gewicht und Feingehalt prägte, als Extremwerte beim Feingewicht werden in AKS 25,47 g und 25,74 g genannt, bei Rittmann 24,89 g und 25,90 g. Dies macht die Kronentaler zu den unzuverlässigsten deutschen Kurantmünzen; trotz der Unterschiede wurden jedoch alle im Umlauf als gleichwertig behandelt. 1837 entschlossen sich die süddeutschen Staaten dann, zum Zweck der Vereinheitlichung ihr Münzsystem auf den Kronentalerfuß, der ungefähr einem 24½-Gulden-Fuß entsprach, umzustellen. Allerdings wurden die unzuverlässigen Kronentaler selbst nicht mehr geprägt, und sie verschwanden ab der Jahrhundertmitte langsam aus dem Umlauf, wurden aber noch 1858 in einer Zusatzvereinbarung der süddeutschen Staaten zum Wiener Münzvertrag von 1857 im Kurs von 162 Kreuzern bestätigt.

Als 1837/38 die Prägung der Kronen- und der Konventionstaler eingestellt wurde, endete die Ausgabe von Speziestalern im heutigen Deutschland.

Johann Philipp Graumann (ca. 1706–1762) schuf für Friedrich den Großen von Brandenburg-Preußen einen neuen, heute als „Graumannschen“ bezeichneten Münzfuß. Während der Zinnaer, Leipziger und Konventions-Münzfuß immer noch einen Bezug zum Reichsspeziestaler gewahrt hatten, löste Graumann sich völlig von diesem Bezug. Stattdessen untersuchte er den Wert der umlaufenden Groschen und errechnete deren durchschnittlichen Silbergehalt. Weil traditionell 24 Groschen einen Taler bilden, verglich er den Silbergehalt von 24 Groschen mit einer feinen Kölner Mark. Es ergab sich eine Aufzahl von etwa 14 Talern pro Kölner Mark. Ein so geprägter Taler hat ein Feingewicht von 16,704 g, was bei einem Feingehalt von 12 Lot = 750 ‰ ein Raugewicht von 22,272 g ergibt. Dieser Taler bekam die Aufschrift „EIN REICHSTHALER“. Seit 1790 trug er auch häufig nur noch die Bezeichnung „EIN THALER“. 1809 übernahm man auch in Preußen den Brauch des Konventionsgeldes, die Aufzahl anzugeben: „XIV EINE FEINE MARK“. Mit der Ausprägung des Kuranttalers verschwand endlich auch die Diskrepanz zwischen Rechnungsmünze und Speziesmünze.

Der Graumannsche Taler war äußerst erfolgreich: Der Doppeltaler, dem 3½ süddeutsche Gulden gleichgestellt waren, wurde 1838 zur „VEREINSMÜNZE“ der am Dresdner Münzvertrag beteiligten 18 deutschen Zollvereinsstaaten, wobei sich zehn von ihnen für die Übernahme des preußischen Taler-Systems entschieden; dieses wurde bis 1858 von elf weiteren deutschen Staaten übernommen. Im Wiener Münzvertrag von 1857 wurde der preußische Taler leicht modifiziert zum VEREINSTALER (s. u.).

Obwohl bereits 1871 die Mark im (zweiten) Deutschen Reich eingeführt wurde, blieb der Graumannsche Reichstaler formal bis 1907 preußische Währungsmünze.

Nach Einführung der leichteren „Reich-“ und Konventionstaler von den führenden Münzständen prägten auch einige weitere deutsche Staaten Kuranttaler:
Zwei besondere Kuranttaler waren:

Dieser ist im Grunde genommen nichts anderes als der preußische Taler, allerdings wurde er auf das neue, im Wiener Vertrag von 1857 festgelegte Grundgewicht von 500 g = 1 Zollpfund bezogen. Statt „XIV EINE FEINE MARK“, also 233,856 : 14 = 16,704 g, hieß es jetzt „XXX EIN PFUND FEIN“, was ein Feingewicht von 500 : 30 = 16,667 g ergab. Eine solche Differenz hätte früher eine Herabsetzung im Wert bedeutet, da sich aber im 19. Jahrhundert allmählich die Vorstellung durchsetzte, dass der Wert einer Münze nicht mehr vom Materialwert (= innerer Wert), sondern vom staatlich garantierten Wert (= äußerer Wert) abhing, wurde die neue Münze, die offiziell die Bezeichnung „VEREINSTHALER“ trug, mit dem alten Taler gleichgesetzt. Der Vereinstaler war bei einem Feingehalt von 900 ‰ deutlich leichter als der Graumannsche Taler: Er wog 18,519 g gegenüber 22,272 g.

Der Vereinstaler wurde von 26 deutschen Zollvereinsstaaten sowie Österreich und Liechtenstein eingeführt. In Norddeutschland, den "Taler-Ländern", wurde er in 30 Silbergroschen (in Sachsen 30 Neugroschen) eingeteilt, in den süddeutschen "Gulden-Ländern" galt er 105 Kreuzer (1 Gulden und 45 Kreuzer), in Österreich und Liechtenstein 150 Neukreuzer. In Deutschland wurde der Vereinstaler bis 1871 geprägt – als Doppeltaler in Sachsen sogar noch 1872, also nach Einführung der Reichswährung –, und er blieb im Umlauf im Wert von 3 Mark bis 1907. In Österreich wurde er bis 1867 geprägt und 1893 außer Kurs gesetzt. Allerdings lief zu der Zeit der weitaus größte Teil der österreichischen Vereinstaler im Deutschen Reich um; dort wurden sie erst 1901 eingezogen.

Mitte des 18. Jahrhunderts führte die Hansestadt Bremen die Goldwährung ein, die auf dem französischen Louis d’or beruhte. Ein Louisdor wurde mit rund 5 Talern bewertet. Der Taler Gold wurde als eigenständige Münze nie geprägt. Erst 1863, 1865 und 1871 gab die Stadt drei Gedenkmünzen in Silber mit der Aufschrift „EIN THALER GOLD“ aus. Er wurde in 15-löthigem Silber geprägt (986,11/1000) und wog 17,539 g bei einem Feingehalt von 17,269 g. Da die Bremer Sorten sich in keiner Weise in das System der neuen Reichswährung einpassen ließen – der Taler Gold entsprach 3,3214 Mark –, waren sie die ersten, die bereits 1872 aus dem Verkehr gezogen wurden, während die Vereinstaler deutschen Gepräges bis 1907 umliefen und dann ab 1908 durch das neue 3-Mark-Stück ersetzt wurden – den, wenn man so will, letzten Kuranttaler.

Der Taler, der 1493 vom schweizerischen Stadtstaat Bern übernommen worden war und schon bald in fast dem gesamten Gebiet der Alten Eidgenossenschaft unentbehrliche Handelsmünze wurde, gab ab 1795 dem von Bern, anschließend von der helvetischen Regierung und zuletzt zwischen 1812 und 1835 von den Kantonen Aargau, Appenzell Außerrhoden, Bern, Freiburg, Luzern, Solothurn, Tessin, Waadt und Zürich herausgegebenen 4-Franken-Stück (zu 40 Batzen) den Namen. Inoffiziell wurden auch die nach 1850 herausgegebenen 5-Franken-Stücke des 1848 gegründeten schweizerischen Bundesstaates da und dort "Taler" genannt, etwa in Appenzell, im Berner Oberland oder im St. Galler Rheintal.

Bereits sehr früh wurden Taler auch außerhalb des heutigen Deutschland geprägt:


Weiterhin wird unterschieden zwischen Speciestalern und Kuranttalern. Speciestaler bedeutet zum einen tatsächlich zum gültigen Münzfuß ausgemünzer Taler im Gegensatz zur reinen Rechnungsmünze oder zu auf Taler lautendem Papiergeld. Zudem kam es nach 1566 zunehmend zur Ausprägung von Talern, mit einem gegenüber den offiziellen Reichstaler herabgesetzten Silbergehalt (s. o.). Diese schlechteren, tatsächlich umlaufenden Taler wurden im Gegensatz zu ursprünglichen Reichstaler (= Speciestaler) als Kuranttaler bezeichnet. Genauso, wie Friedrich II. von Preußen sich nicht scheute, seinen frisch auf 16,4 g Feinsilber abgewerteten Taler mit der Aufschrift „Reichsthaler“ zu schmücken, prägten andere Münzstände später ihren ebenso vom Münzfuß von 1566 abweichenden Talern die Bezeichnung „Speciesthaler“ auf. Diese Art „Etikettenschwindel“ verwirrte nicht nur die Zeitgenossen, sondern erschwert auch heute das Verständnis der realen Wirtschaftsgeschichte.

Wichtig zu wissen ist, dass nicht immer ein Gepräge, das einen Talernamen trägt, auch ein Taler, also eine Münze ist. Bekannte Beispiele dafür sind Hustaler, Kleetaler, die Locumtenenstaler mit hohem Relief und die Luftpumpentaler, die mit gleichem Münzbild als Medaille und Reichstaler geprägt wurden unter etlichen anderen talerförmigen Medaillen mit einem Talernamen.

Es gibt auch Goldabschäge von Talerstempeln im mehrfachen Dukatengewicht, die umlauffähig waren z. B. das 10-Dukaten-Stück, ein Goldabschlag von den Stempeln des Weidenbaumdoppeltalers.






</doc>
<doc id="11969" url="https://de.wikipedia.org/wiki?curid=11969" title="Feld">
Feld

Feld (althochdeutsch für ‚Ebenes, Breites‘) steht für:





Feld heißen folgende Gemeinden, Ortschaften, Ortsteile und Weiler: 


Feld ist der Familienname folgender Personen:

Siehe auch:


</doc>
<doc id="11970" url="https://de.wikipedia.org/wiki?curid=11970" title="Laizismus">
Laizismus

Laizismus (genauer: Laizität) geht auf das altgriechische Wort "laïkós" „der Ungeweihte, Laie (im Gegensatz zum Priester)“ zurück. Der Begriff "laïcité" wurde 1871 vom französischen Pädagogen und späteren Friedensnobelpreisträger Ferdinand Buisson geprägt, der sich für einen religionsfreien Schulunterricht einsetzte. Im heutigen Sprachgebrauch beschreibt der Begriff „Laizität“ die religionsverfassungsrechtlichen Modelle, denen das Prinzip strenger Trennung zwischen Religion und Staat zugrunde liegt. In einigen Staaten ist der Begriff „Laizismus“ inzwischen in der Verfassung verankert. Etliche weitere, meist westliche Staaten sind laut ihrer Verfassung zwar nicht explizit laizistisch, sie praktizieren die Trennung von Staat und Religion(en) jedoch in unterschiedlichem Umfang.

1894 begann in Frankreich die Dreyfus-Affäre. Sie trug zu einer jahrelangen gesellschaftlichen Polarisierung in Frankreich bei. 
Im Frühjahr 1902 erhielt bei der Wahl zur Abgeordnetenkammer der Bloc des gauches, angeführt von Emile Combes, eine knappe absolute Mehrheit; die Droite parlementaire unter Jules Méline erhielt 48,82 Prozent der Stimmen. 
1905 wurde in Frankreich das Gesetz zur Trennung von Kirche und Staat verabschiedet, für das sich insbesondere der damalige Abgeordnete und spätere Ministerpräsident Aristide Briand eingesetzt hatte. 
Damit wurde das von Buisson geschaffene Prinzip erstmals konsequent angewendet. Der Begriff "laïcité" wurde erstmals in der Verfassung von 1946 verwendet. Deren Artikel 1 lautet: "La France est une République indivisible, laïque, démocratique et sociale."

Der in Deutschland häufig verwendete Begriff des Laizismus ist mit Laizität nicht gleichzusetzen, da er etymologisch eine andere Nebenbedeutung hat, auch wenn diese im allgemeinen Sprachgebrauch oft nicht berücksichtigt wird. Während Laizismus („laïcisme“) zumindest ursprünglich als Kampfbegriff gegenüber einer antireligiösen Ideologie entstanden ist, umfasst Laizität neben der Trennung von Religion und Staat auch das Gebot der Gleichheit und des Respekts gegenüber allen Religionen und die weltanschauliche Neutralität des Staates. Im Gegensatz zur französischen Ausprägung der Laizität, nach der primär der Staat vor dem als schädlich angesehenen Einfluss der katholischen Kirche geschützt werden sollte, kann die Trennung von Kirche und Staat – wie in den USA – auch primär dem Schutz der Kirchen vor staatlicher Einflussnahme dienen und mit einem starken gesellschaftlichen Einfluss der Kirchen einhergehen.

Bei folgenden Staaten ist der Begriff „Laizismus“ in der Verfassung verankert:

Allerdings weisen diese Staaten markante Unterschiede in der Ausprägung und Umsetzung des Laizismus auf.

Tschechien, Frankreich und Portugal sind die einzigen ihrem verfassungsrechtlichen Anspruch nach laizistischen Staaten der Europäischen Union. Am 9. Dezember 1905 wurde in Frankreich das Gesetz zur Trennung von Kirche und Staat verabschiedet. Es realisierte in Frankreich das heute noch geltende Prinzip der vollständigen Trennung von Religion und Staat. Das Gesetz galt vor allem der katholischen Kirche; aus Gründen der Neutralität wurden die anderen Konfessionen in diese Regelung einbezogen. 
Allerdings ist in Portugal sowie in beiden elsässischen Départements und dem Département Moselle in Frankreich die Umsetzung des Laizismus durch in Konkordaten vereinbarte Rechte der römisch-katholischen Kirche unvollständig.

In der Türkei wird der Laizismus als „Unterordnung der Religionsausübung unter den Staat“ interpretiert. Der Staat bildet die islamischen Imame aus und macht durch das Amt für Religiöse Angelegenheiten enge inhaltliche Vorgaben für deren Arbeit.

Viele westliche Staaten sind zwar nicht nach ihrer Verfassung explizit laizistisch, praktizieren jedoch in unterschiedlichem Ausmaß die Trennung von Staat und Religion(en) und so die Neutralität des Staates in religiösen und weltanschaulichen Belangen. In diesem Sinne gelten die meisten Staaten in Afrika, Amerika, Ozeanien und Europa als säkular.

Der Katholizismus akzeptiert seit dem Zweiten Vatikanischen Konzil eine relative Laizität des Staates und der weltlichen Sachbereiche (vgl. Gaudium et Spes, 1965), hält aber an seinem geistlichen Absolutheitsanspruch fest. Den evangelischen oder orthodoxen Staatskirchen ist eine Anerkennung des Laizismus eigentlich nicht möglich; sie gelingt nur auf dem theologischen Umweg über den traditionellen „Gehorsam des Christen“ gegenüber jedweder Obrigkeit , also auch der säkular-demokratischen. Die evangelischen Freikirchen haben das Staatskirchentum immer abgelehnt, also aus religiöser Perspektive die Religionsfreiheit gutgeheißen, und begrüßen daher auch die jüngere Selbstkorrektur der katholischen Position.

Die Auswirkungen des französischen politischen Kampfes von 1905 sind bis heute in der Interpretation des Begriffs im Alltagsleben spürbar. Dabei sind zwei Interpretationen zu unterscheiden: eine liberale, die unter Laizismus die institutionelle Trennung von Staat und Kirche versteht, und eine radikale ("laïcard"), für die Laizismus das Verbot jeglicher religiöser Betätigung außerhalb eines engen, privaten Bereiches bedeutet. Während das liberale Verständnis des Laizismus heute auch bei den christlichen Kirchen akzeptiert wird, gibt es zahlreiche Vertreter der harten Interpretation in den Reihen der politischen Elite, traditionell insbesondere der politischen Linken bis hin zum Parti communiste français. Der Katholizismus hat den ideologischen Laizismus bis heute nicht anerkannt, da das Papsttum an einem Vorrang seiner geistlichen Autorität gegenüber der Staats- und Gesellschaftsordnung festhält. Allerdings hat die katholische Kirche seit dem Zweiten Vatikanischen Konzil bewusst auf politische Sonderrechte und Privilegien im Staat verzichtet und vertritt heute nicht mehr das Konzept einer Staatsreligion, die zuletzt auch in Italien abgeschafft wurde (1984).

Im heutigen französischen Verständnis ist Laizismus zu einem politischen Ideal geworden, das die Grundsätze der "Neutralität" des Staates gegenüber den Religionen, deren "Gleichbehandlung" sowie die "Glaubensfreiheit" zum Ziel hat. Laizismus ist ein Verfassungsprinzip. Religion ist ausschließlich Privatangelegenheit, woraus folgt, dass Religion nicht nur keine staatliche, sondern auch keine öffentliche Funktion hat. Bereits während der Französischen Revolution war das gesamte Kirchenvermögen verstaatlicht worden, allerdings wurde mit dem Konkordat von 1801 den Religionsgemeinschaften ein Ausgleich durch staatliche Besoldung der Geistlichen gewährt. Mit der "Loi sur la séparation des Églises et de l'État" von 1905 wurde jegliche staatliche Förderung von Religionsgemeinschaften ersatzlos eingestellt. Vor 1905 errichtete Kirchen oder Synagogen sind noch immer in staatlichem Besitz, wobei jene Teile, die „dem Kult dienen“, den einzelnen Glaubensgemeinschaften zur Nutzung überlassen werden können. Frankreich erkennt „kirchliche Organisationen“ zwar in ihrer Existenz an, sie erhalten jedoch keine staatlichen Zuschüsse; allerdings existieren steuerliche Begünstigungen. Davon ausgenommen sind das Elsass und das Département Moselle, die zum Zeitpunkt des Gesetzes von 1905 nicht zu Frankreich gehörten und deren Bewohner sich nach der Rückkehr 1919 gegen die Übernahme der französischen Regelung wehrten, so dass hier bis heute die Regelungen des Konkordats von 1801 gelten. Auch im Übersee-Département Französisch-Guyana wird das Personal der katholischen Kirche vom Staat bezahlt. Die Anstaltsseelsorge ("aumôneries") ist ebenfalls vom Verbot der staatlichen Förderung von Religion ausgeschlossen, wobei auf die Verwirklichung der Religionsfreiheit hingewiesen wird (Art. 1 Abs. 2 des Trennungsgesetzes von 1905). Dazu gehört auch die Militärseelsorge, die zunächst auf katholische, protestantische und jüdische Militärgeistliche beschränkt war. 2005 wurde sie durch eine islamische Militärseelsorge ergänzt.

Der Laizismus wird in Frankreich strikt praktiziert. Der Staat sieht es als Aufgabe an, seine Bürger gegen religiöse Praktiken, die der öffentlichen Ordnung oder den Rechten des Einzelnen zuwiderlaufen, zu schützen. In französischen öffentlichen Schulen ist es verboten, Lehrer oder Schüler nach ihrer Religion zu fragen. Jedoch existiert daneben ein fest verankertes, breit gefächertes privates Schulsystem, insbesondere das "enseignement catholique". Pfarrer können nicht zugleich für öffentliche Unternehmen arbeiten. Frankreich betrachtet die religiösen Auffassungen der Bürger als reine Privatsache; es gibt keine amtlichen Statistiken zur Religionszugehörigkeit der Bevölkerung. Dies hat unter anderem zur Folge, dass es in der aktuellen politischen Debatte in Frankreich zu Segregation oder Diskriminierung auf dem Arbeitsmarkt keine verlässlichen Zahlen gibt. Seit 2004 ist es auch untersagt, in Schulen auffällige religiöse Zeichen zu tragen, wie Schleier, Kippa, Kreuze, Turbane (bei Sikhs) oder Ordenstracht. Andererseits übertragen die staatlichen Sender "France 2" (TV) sowie "France Culture" (Radio) sonntägliche Gottesdienste und Andachten.

Der vormalige französische Präsident Nicolas Sarkozy hat in verschiedenen Reden und in seinem Buch "Der Staat und die Religionen" eine Neubestimmung der französischen Laizität vorgeschlagen, die er "offene" oder "positive" Laizität nennt. Damit sollen die Religionen mehr in die öffentliche Verantwortung genommen werden, auch um Fundamentalismus zu verhindern. In der noch offenen Diskussion der auch durch persönliches Verhalten unterstrichenen Neupositionierung wird Sarkozy von laizistischen Verbänden stark kritisiert.

Mehrere andere Staaten nahmen sich das französische Modell zum Vorbild, insbesondere die Türkei unter Staatsgründer Mustafa Kemal Atatürk. Mit den weitreichenden Veränderungen in der Zeit Atatürks wurde der Laizismus zu einer Staatszielbestimmung. Der Laizismus als Teil des Kemalismus ist bis heute (2018) in der türkischen Verfassung verankert. Zunächst hatte der neue Staat noch massiv antireligiös gewirkt – so wurden Wallfahrten nach Mekka verboten und ein Religionsstudium war von 1933 bis 1948 nicht möglich. Faktisch hat sich der Staat der (sunnitisch-islamischen) Religion bemächtigt und versucht, diese durch die staatliche Religionsbehörde "Diyanet" zu domestizieren. Mit der Zeit hat sich der Begriff des Laizismus in der Türkei verhärtet. In der Zeit von Atatürk war es noch üblich, ein Kopftuch zu tragen. Später wurde der Laizismus so interpretiert, dass auf staatlicher Ebene solches Sichtbarmachen der Religion nicht gerne gesehen wird. So war es verpönt, sich als Politiker in der Öffentlichkeit zu seiner Religion zu bekennen.

Im Februar 2008 hatte das Parlament auf Initiative der regierenden AKP per Verfassungsänderung eine Freigabe des Kopftuches für Studentinnen durchgesetzt. Die Änderung wurde vier Monate später vom Verfassungsgericht rückgängig gemacht. Damit waren Frauen, die Kopftücher tragen, zunächst weiterhin von einem Hochschulstudium ausgeschlossen. Im Herbst 2010 hob der oberste Hochschulrat der Türkei das Kopftuchverbot an Universitäten endgültig auf. Religiöse Minderheiten der Türkei müssen heute weiterhin mit Repressionen rechnen. Am 14. März 2008 wurde vom Generalstaatsanwalt Abdurrahman Yalçınkaya ein Verbotsverfahren gegen die AKP beantragt. Zur Begründung hieß es, die AKP sei ein „Zentrum anti-laizistischer Aktivitäten“. Das Verfahren wurde vom Verfassungsgericht der Türkei behandelt. Der Generalstaatsanwalt forderte für 71 Personen ein Politikverbot, darunter befanden sich der damalige türkische Präsident Abdullah Gül, der heutige türkische Präsident und Vorsitzende der AKP Recep Tayyip Erdoğan und der ehemalige Parlamentspräsident Bülent Arınç. Im Gericht stimmten zwar sechs der elf Richter für eine Schließung der AKP, die notwendige Anzahl von sieben Richtern wurde aber knapp nicht erreicht. Die staatliche Finanzförderung für die AKP wurde gestrichen. Die oppositionelle CHP setzt sich in ihrem Programm für einen konsequenten Laizismus ein.



</doc>
<doc id="11971" url="https://de.wikipedia.org/wiki?curid=11971" title="Privacy">
Privacy

Privacy steht für:

Privacy wird besonders im Internetbereich oft im Sinne von Datenschutz verwendet:


</doc>
<doc id="11973" url="https://de.wikipedia.org/wiki?curid=11973" title="Belegarzt">
Belegarzt

Ein Belegarzt ist ein niedergelassener Arzt, der einige Betten („Belegbetten“) in einem Krankenhaus (meistens der Grund- und Regelversorgung) mit seinen Patienten belegen darf.

Belegärzte gibt es vor allem in den „kleinen“ Fächern wie HNO, Mund-, Kiefer- und Gesichtschirurgie, Urologie, Augenheilkunde, Geburtshilfe und Gynäkologie.

Analog gibt es unter Hebammen die Stellung der Beleghebamme.

Im österreichischen Recht findet sich (Stand 2009) keine Definition zum Belegarzt.




</doc>
<doc id="11974" url="https://de.wikipedia.org/wiki?curid=11974" title="Sauerteig">
Sauerteig

Sauerteig ist ein Teig zur Herstellung von Backwaren, der meist dauerhaft durch Milchsäurebakterien und Hefen in Gärung gehalten wird. Das dabei entstehende Kohlenstoffdioxid lockert den Teig auf. Die typischen Arten von Milchsäurebakterien sind "Lactobacillus plantarum" (homofermentativ) und "Lactobacillus brevis" (heterofermentativ). Ein typischer Hefestamm im Sauerteig ist "Saccharomyces cerevisiae".
Sauerteig wird als Triebmittel zur Lockerung von Backwerk zugefügt und macht Roggenteige überhaupt erst backfähig. Sauerteige verbessern Verdaulichkeit, Aroma, Geschmack, Haltbarkeit und Schnitt der Backwaren. Ebenso werden ernährungsphysiologische Eigenschaften verbessert.

Sauerteige enthalten eine Lebensgemeinschaft von Milchsäurebakterien und Hefepilzen, die der Mensch seit mehreren tausend Jahren für die Herstellung von Getreidefladen, Brot und brotähnlichen Nahrungsmitteln nutzt. Die Stoffwechselprodukte dieser Mikroorganismen lockern den Teig und verbessern die Verdaulichkeit, das Aroma, den Geschmack und die Haltbarkeit der Backwaren. Eine besondere Bedeutung hat Sauerteig bei der Verwendung von Roggenmehl. Während für Weizenmehl auch reine Hefe als Triebmittel verwendet werden kann, ist bei der Verwendung von Roggenmehl die Zuführung von Säure erforderlich, damit das Brot aufgeht und nicht flach bleibt. Die Milchsäurebakterien des Sauerteigs produzieren diese in Form von Milchsäure und Essigsäure.

Während in der Vergangenheit Verfahren und Methoden zur preiswerten und effizienten Brotbereitung gesucht wurden, stehen heute die geschmacklichen und ernährungsphysiologischen Eigenschaften von Backwaren aus Sauerteig im Vordergrund. Gerade Vollkornprodukte erhalten durch Sauerteig ein verbessertes „Mundgefühl“ und einen besseren Geschmack, wobei nährwertbestimmende Substanzen erhalten bleiben.

Plinius der Ältere beschrieb um 79 n. Chr. die Gewinnung von Sauerteig durch die Vermischung von Weizenkleie mit drei Tage altem Traubenmost. Auch Verfahren der Spontansäuerung und die Weiterführung von Sauerteigen (Sauerteigführung) waren ihm bekannt. Im Mittelalter geriet das vielfältige Wissen um Ackerbau und Brotbereitung allerdings wieder in Vergessenheit. Lediglich an den Höfen und in Klöstern wurde dieses Wissen noch gepflegt.

In Deutschland lieferten im 15. und 16. Jahrhundert die Brauer und Schnapsbrenner den Bäckern die erste Hefe. In Frankreich war diese Technik lange umstritten, da die Bevölkerung eine Gesundheitsgefährdung durch dieses neue Lebensmittel fürchtete. Erst 1670 wurde die Verwendung von Bierhefe erlaubt, wobei sie vorher mit Sauerteig vermischt werden musste. In Europa wurden die ersten Hefezüchtungen um 1700 bekannt. Ende des 18. Jahrhunderts hatte sich die fabrikmäßige Produktion von Hefe etabliert. Auftrieb brachten der Entwicklung der Bäckerhefe die Kühlmaschinen des Carl von Linde. Die Brauer stiegen nach 1877 von obergärigem zu untergärigem Bier um. Dies war nur durch Kühlung möglich, wobei aber keine brauchbare Hefe für die Bäckereien mehr anfiel. Zwangsweise wurde damit die Entwicklung spezieller Backhefen gefördert. Die Hefen der Brauer und Brenner waren von schlechter Qualität. Der Stamm Saccharomyces cerevisiae wurde gezielt selektiert und produziert, da mit dieser Hefe hervorragende Backergebnisse erzielt wurden.

Roggensauer wurde um 1900 erstmals durch Zitronensäure ersetzt, die Backergebnisse waren allerdings nicht optimal. Um 1920 empfahl man beim Roggenteig zur Vereinfachung der Brotbereitung den Zusatz von Säuren, was nach heutigem Erkenntnisstand auch sinnvoll war, denn damalige Mehle verfügten über einen hohen Enzymgehalt. Parallel dazu entwickelte man Sauerteigstarterkulturen und bot sie als „Reinzuchtsauer“ an. 1930 gelangten die ersten Fertigsauer auf den Markt. Sie bestanden aus Quellmehl und Gärungsmilchsäure. Ebenfalls um 1930 wurde getrockneter Sauerteig angeboten, der allerdings verfahrensbedingt nur einen geringen Säureanteil hatte. 1970 wurde der Trockensauer als „Sauerteig-Extrakt-Roggen“ auf den Markt gebracht.

Danach blieb die Entwicklung nicht stehen. Unzählige Verfahren und Backmittel wurden entwickelt und laufend verbessert, um die bestehenden Sauerteigverfahren zu optimieren und zu vereinfachen.

In der Regel wird Sauerteig aus Brotgetreidemehl (aus Weizen oder Roggen) hergestellt, es kann jedoch entweder teilweise oder ganz durch andere Getreidemahlerzeugnisse oder mehlähnliche Erzeugnisse ersetzt werden. Sehr weit verbreitet sind in Afrika Teff-Brot in Äthiopien und z. B. Laxoox in Somalia.

Weizensauer ist ein Teig aus Weizenmehl, Wasser, Hefen (überwiegend "Saccharomyces cerevisiae") und oft einem geringen Anteil von Milchsäurebakterien ("Lactobacillus plantarum", "Lactobacillus brevis ssp. lindneri"). Einige traditionelle Gebäcke mit Weizensauerteig sind Ciabatta, Panettone oder der Hermann-Teig.

Roggensauer ist ein Gemisch aus Roggenmehl, Wasser und Milchsäurebakterien ("Lactobacillus plantarum", "Lactobacillus fructivorans" und "Lactobacillus brevis") und meistens Hefen ("Saccharomyces cerevisiae"). Nicht alle Roggensauerteige enthalten ausreichend Hefen für die Lockerung. Deswegen werden oft Sauerteighefen (säurefeste und triebkräftige Hefen) zugesetzt.

Mit viel Mehl zu trockenen Krümeln verarbeiteter Sauerteig wird als "Krümelsauer" (auch "Gerstl") bezeichnet und dient der Aufbewahrung.


Sauerteigbrote enthalten viele Geruchs- und Geschmacksstoffe. Es sind über 300 Aromastoffe bekannt, wobei aber nur wenige aroma- und geschmacksbestimmend sind. Viele der beteiligten Substanzen sind bereits im Mehl vorhanden, ergeben aber einen schwachen Aromaeindruck. Durch Gärung und Bildung von Estern (aus Ethanol und Säuren) im Sauerteig werden diese Aromastoffe kräftig ausgebildet. Tragende Aromaverbindungen wie Methylbutanol und Diacetyl nehmen in der Konzentration zu, aber es werden auch unerwünschte Aromen reduziert; so wird z. B. der grasige Aromastoff Hexanal im Sauerteig abgebaut.

Durch Sauerteig wird das spezifische Volumen erhöht. Die Porung der Krume ist feiner. Daneben zeichnet eine höhere Feuchte und Elastizität eine Sauerteigführung aus, wodurch das Gebäck schnittfester wird. Beim Verzehr ist das Mundgefühl angenehmer, da alle Fraktionen des Mehles besser aufgeschlossen werden: ein Effekt, der besonders bei Vollkornprodukten wirkt. Roggenbrote können durch die Absenkung des pH-Wertes besser gekaut werden. Beim Kauen haftet die ungesäuerte Krume an den Zähnen, bewirkt ein unangenehmes Mundgefühl; mit fallendem pH-Wert nimmt die Elastizität der Krume zu.


Das Altbackenwerden ist komplex und noch nicht vollständig erforscht. Das Brot verliert Aroma und Mundgefühl, es wird härter und spröder; die Stärke gibt Feuchtigkeit ab und geht in einen kristallinen Zustand über. Die Feuchtigkeit wandert von der Krume zur Kruste, wo sie austritt. Bisher ist nur sicher, dass die Bildung von Exopolysacchariden, die bei der Sauerteigfermentation durch Milchsäurebakterien entstehen, darauf wesentlichen Einfluss hat.

Sauerteiggeführte Brote schimmeln langsamer als nicht gesäuerte oder chemisch gesäuerte Brote. Ursachen und Wirkung sind vielfältig. Gesäuerte Brote sind weniger anfällig gegen:

Dieses Verfahren ist recht alt und sicher. Es wird auch heute noch angewendet, um das sogenannte Anstellgut als Grundlage für den Sauerteig zu ziehen.

Milchsäurebakterien kommen überall vor. Sie sind im Mehl und im Wasser zu finden. Daher kann ein Sauerteig dadurch hergestellt werden, dass man je die gleiche Menge (Roggen-)Mehl und Wasser miteinander vermengt und zugedeckt bei Zimmertemperatur ca. zwei Tage stehen lässt. Riecht der Teig dann angenehm säuerlich, oftmals mit einer Fruchtnote, kann man davon ausgehen, dass sich die Milchsäurebakterien durchgesetzt haben. Riecht der Ansatz jedoch nach faulen Eiern, so muss man davon ausgehen, dass sich Fäulnisbakterien durchgesetzt haben.

Das Ergebnis ist jedoch stark vom Zustand der Rohstoffe abhängig. Wesentlich beeinflussen die Lagerung der Rohstoffe (Temperatur, Feuchtigkeit) sowie der Befall von Schädlingen die Anzahl und Arten der Mikroorganismen. Damit sich eine bestimmte Art von Bakterien oder Hefen sicher entwickeln kann, müssen genügend Keime dieser Art vorhanden sein. Erfolg und Qualität hängen stark vom Zufall ab. Es ist ein Verdrängungswettbewerb, der darüber entscheidet, welche Kulturen sich entwickeln.

Sauerteig ist eine stabile symbiotische Kultur von Milchsäurebakterien und Hefen in einem Teig von Mehl und Wasser. Milchsäurebakterien verwandeln Zucker, die die Hefen nicht abbauen können, und auf der anderen Seite verstoffwechseln Hefen Produkte der Milchsäurefermentation. Generell kann man sagen, dass die Hefen das Gas produzieren, das den Teig aufgehen lässt, und die Milchsäurebakterien die Milchsäure produzieren, die für den säuerlichen Geschmack und die Enzymhemmung verantwortlich ist.
Milchsäurebakterien arbeiten anaerob, d. h., sie können sich bei Abwesenheit von Sauerstoff vermehren. Hammes und Vogel haben 1995 drei unterschiedliche Stoffwechselgruppen von Milchsäurebakterien unterschieden:


Die Milchsäurebakterien zersetzen einen Teil der Zuckerstoffe im Mehl zu Milchsäure, zu Essigsäure und einen geringen Anteil Kohlendioxid. Die Hefen bilden Kohlendioxid sowie kleine Mengen Alkohol (Ethanol). Der Alkohol wird von den Essigsäurebakterien in Essigsäure umgewandelt. Durch verschiedene Bedingungen (Gärungszeit, Mischungsverhältnis, Temperatur) kann Einfluss auf die Anteile der einzelnen Gärungsprodukte und somit auf Eigenschaften (Wirkung) und Geschmack des Sauerteigs genommen werden. Durch Temperatursteuerung kann erreicht werden, dass im Teig mehr Essigsäure (24–28 °C) gebildet wird, also das Brot saurer schmeckt. Milchsäure ist geschmacklich milder. Soll also mehr Milchsäure gebildet werden, so muss der Teig bei ca. 30 °C geführt werden. Im fertigen Sauerteig herrschen pH-Werte zwischen 3,8 und 4,3.
Der bei der Gärung gebildete Alkohol verbindet sich zum Teil mit Säuren im Teig zu Estern, die zu Geruch und Geschmack des Gebäcks beitragen. Ein anderer Teil wird zu Essigsäure umgewandelt. Das Verhältnis von Milchsäure zu Essigsäure im Teig sollte zwischen 3:1 bis 4:1 liegen. Wird das Verhältnis in Richtung Essigsäure verschoben, so wird der Geschmack meist als zu sauer empfunden.

Starterkulturen sind die Grundlage eines standardisierten Sauerteigs. In der Produktion wird ein Mehl-Wasser-Gemisch gezielt mit bestimmten Kulturen geimpft, woraus sich ein Sauerteig entwickelt. Auch hier wird ein Rest Anstellgut für den nächsten Sauerteig aufgehoben. In bestimmten Zeitabständen wird der Sauer oft vollkommen neu mit Reinzuchtsauer angesetzt, wobei andere Produzenten wiederum auf eigene, bewährte, jahrelang stabile Kulturen setzen.



Starterkulturen werden für folgende Getreidearten angeboten:

Ohne Bedeutung ist die Art des Getreides für die Entwicklung der gewünschten Mikroorganismen. Wichtiger ist die Versorgung mit den notwendigen Nährstoffen, wobei dunkle Mehle deutlich besser abschneiden.

Die Randschichten der Roggen- und Weizenkörner enthalten Phytinsäure, welche die Entwicklung des jungen Getreidekorns fördert und vor Fressfeinden schützt. Dunkle Mehle enthalten viel Phytinsäure, während der Anteil zu hellen Mehlen abnimmt und bei Auszugsmehl unbedeutend ist. Phytinsäure gilt als antinutritiv: Sie bindet Mineralstoffe (Ca, Mn, Mg, Fe) durch Eiweißkomplexe (Phytate), wodurch die Resorption im Darm erschwert oder vermindert wird. Grundsätzlich ist Phytin im Brot für den menschlichen Organismus unbedeutend, da es thermisch nicht stabil ist; jedoch bleiben darin Mineralien gebunden. Im Sauerteig – durch den niedrigen pH-Wert – bauen getreideeigene Enzyme Phytat ab.

"Hauptartikel:" Sauerteigführung
Als Führung bezeichnet man die Herstellung eines Sauerteiges über ein oder mehrere Stufen unterschiedlicher Temperatur, wobei versucht wird, ein dem gewünschten Produkt entsprechendes Verhältnis der Mikroorganismen (Hefen und Milchsäurebakterien) zu erreichen.

Wird ein reifer Sauerteig nicht vollständig verbraucht, kann der Rest als Anstellgut erneuert werden. Dazu werden Mehl und Wasser zugegeben. Da unter Umständen die Säurebildung nicht mehr zufriedenstellend ist, können sich unerwünschte Mikroorganismen im Teig vermehren.
Mögliche Folgen:
Um dies zu vermeiden, sollte regelmäßig ein Reinzuchtsauer als Ansatz verwendet werden.

Durch Fehler bei der Sauerteigmenge und -reife kann es zu typischen Brotfehlern kommen:
Mit der Bezeichnung Sauerteig verbindet der Verbraucher ein Produkt mit hoher Qualität und natürlichem Ursprung. Verschiedene Länder haben daher Richtlinien und Produktbeschreibungen zum Schutz des Verbrauchers erlassen:

Im Deutschen Lebensmittelbuch werden Leitsätze (§ 15 LFGB) über die geltende Verkehrsauffassung für viele Lebensmittel beschrieben. Diese Leitsätze wurden von Verbrauchern, Wissenschaftlern, Wirtschaft, Lebensmittelüberwachung und Behörden erarbeitet und fortgeführt. Leitsätze sind aber keine Rechtssätze. Man könnte sie als vorweggenommenes Sachverständigengutachten bezeichnen. Produkte können von diesen Leitsätzen abweichen und bei entsprechender Kennzeichnung in Verkehr gebracht werden.

Definition von 2006 (Auszug):

In den Leitsätzen wird auch Sauerteigbrot definiert:

Den Leitsätzen des Deutschen Lebensmittelbuches, entspricht in Österreich der „Codex Alimentarius Austriacus“. Auszüge:

B9: Backhefe, Sauerteig, Backpulver, Triebmittel für besondere Zwecke:

In Österreich wird also Natursauer recht eng definiert. Seine Verwendung wird bei der Herstellung von Land- und Bauernbrot (Codexkapitel B18) gefordert.

Im Kap. 16 des Schweizerischen Lebensmittelbuches wird Sauerteig definiert (Auszug):


Am 13. September 1993 trat das Dekret No. 93 – 1074 zum Schutz der Brotsorten: Pain Maison, Pain de Tradition Française und Pain au levain in Kraft. Auszüge:


In der religiösen Praxis des Christentums spielt Sauerteig keine bedeutende Rolle.
Die Hostien für die Eucharistie werden in der westlichen Kirche zwar seit dem 11. Jahrhundert aus ungesäuertem Weizenteig hergestellt, aber diese Tradition hat keine religiöse Verbindlichkeit. In den Ostkirchen, die Armenier seit alters ausgenommen, wird stets gesäuertes Brot verwendet.

Sauerteig hat in den Schriften des Neuen Testaments eine andere Bedeutung als im Judentum: Der Sauerteig ist Sinnbild einer Dynamik, deren Qualität jeweils von der des Impulses bestimmt wird.

Das bedeutendste positive Bild ist das Gleichnis vom Sauerteig ( par. ), in dem das Reich Gottes als ein Geschehen beschrieben wird, das wie ein Sauerteig stetig und unaufhaltsam Veränderung schafft, auch wenn der Anfang klein erscheint.

Als negativ gewertetes Beispiel gilt die Dynamik, die von den Pharisäern, Sadduzäern und von Herodes ausgeht.

In schreibt Paulus, dass die Gläubigen sich von den abgelegten Lehren und Lebensweisen reinigen sollen, wie man ein Haus für Pessach vom alten Sauerteig reinigt, damit sie als ungesäuerter Teig die neue Lebensweise annehmen können:



</doc>
<doc id="11976" url="https://de.wikipedia.org/wiki?curid=11976" title="Ilse Aichinger">
Ilse Aichinger

Ilse Aichinger (* 1. November 1921 in Wien; † 11. November 2016 ebenda) war eine österreichische Schriftstellerin. Sie gilt als bedeutende Repräsentantin der deutschsprachigen Nachkriegsliteratur.

Ilse Aichinger und ihre Zwillingsschwester Helga Michie wurden als Töchter eines Lehrers und einer jüdischen Ärztin in Wien geboren. Bis zur frühen Scheidung der Eltern (1927) verbrachte sie ihre Kindheit in Linz. Die Mutter zog mit den Kindern zurück nach Wien, wo Ilse Aichinger meist bei ihrer jüdischen Großmutter bzw. in Klosterschulen lebte.

Der Anschluss Österreichs bedeutete für die Familie Verfolgung und Lebensgefahr. Helga konnte am 4. Juli 1939 mit einem Kindertransport nach Großbritannien fliehen, der Rest der Familie aber nicht mehr nachkommen, da der Krieg ausbrach. Ilse Aichinger blieb bei ihrer Mutter, um sie als Betreuerin einer noch unmündigen „Halbarierin“ vor der Deportation zu bewahren. Die Mutter verlor ihre Stellung, wurde aber tatsächlich bis 1942 nicht behelligt.

Ilse Aichinger lebte völlig isoliert von der Öffentlichkeit, ein Studienplatz wurde ihr verweigert. Sie und ihre Mutter wurden in den Kriegsjahren dienstverpflichtet; Ilse Aichinger ging die Gefahr ein, selbst deportiert und getötet zu werden, weil sie ihre Mutter nach Erreichen der eigenen Volljährigkeit versteckte – in einem der Tochter zugewiesenen Zimmer direkt gegenüber dem Gestapo-Hauptquartier im ehemaligen Hotel Metropol am Morzinplatz. Die Großmutter und die jüngeren Geschwister der Mutter wurden 1942 verschleppt und kamen im Vernichtungslager Maly Trostinez in der Nähe von Minsk um.

1945 begann Ilse Aichinger Medizin zu studieren, brach aber nach fünf Semestern ab, um ihren teils autobiografischen Roman "Die größere Hoffnung" zu schreiben. Er entstand „in der Küche einer armseligen Wohnung in einem äußeren Bezirk“ von Wien sowie im Dienstzimmer „einer Anstalt für Unheilbare, Alte, Abgeschobene“, in der ihre Mutter als Ärztin arbeitete. Der Kritiker Hans Weigel empfahl ihr, sich und ihre Texte beim Bermann-Fischer Verlag vorzustellen, der schließlich ihre Werke veröffentlichte. Aber schon davor erregten Aichingers frühere Texte – publiziert in Zeitungen und Zeitschriften wie Wiener Kurier, Plan, Der Turm – Aufsehen, so dass, nach Hans Weigel, die österreichische Literatur nach 1945 überhaupt erst mit Ilse Aichinger begann. 1949/50 arbeitete Ilse Aichinger als Verlagslektorin für S. Fischer, 1950/51 als Assistentin von Inge Aicher-Scholl an der Hochschule für Gestaltung in Ulm.

1951 wurde sie erstmals zur Gruppe 47 eingeladen, wo sie ihren späteren Mann Günter Eich kennenlernte. 1952 gewann sie mit ihrer "Spiegelgeschichte" den Preis der Gruppe. Im selben Jahr erschien die vielbeachtete "Rede unter dem Galgen". Von 1956 bis 1993 war sie Mitglied der Akademie der Künste (Berlin West). 1953 heiratete sie den Schriftsteller Günter Eich. Das Ehepaar lebte mit den Kindern Clemens, der ebenfalls Schriftsteller wurde, und Mirjam zuerst in Lenggries, dann in Breitbrunn am Chiemsee und seit 1963 in Großgmain im Land Salzburg. Im Jahr der Heirat mit Eich wurde Aichingers Hörspiel-Debüt, "Knöpfe", erstmals gesendet.

1957 wurde sie Mitglied der Schriftstellervereinigung PEN.

1972 starb Günter Eich. Neun Jahre später, nach dem Tod der Mutter, zog Ilse Aichinger nach Frankfurt am Main und 1988 nach Wien, wo sie nach einer längeren Schaffenspause Ende der 1990er Jahre wieder zu schreiben begann. Sie gewann regelmäßig renommierte Literaturpreise, obwohl ihre Veröffentlichungen immer weniger und kürzer wurden. 1996 unterzeichnete sie die "Frankfurter Erklärung zur Rechtschreibreform" und untersagte 1997, ihre Texte in Schulbüchern den neuen Regeln anzupassen.

Seit 1977 war sie Mitglied der Deutschen Akademie für Sprache und Dichtung in Darmstadt. Außerdem war sie ordentliches Mitglied der Bayerischen Akademie der Schönen Künste und gehörte nach der Fusion der Westberliner Akademie der Künste mit der Akademie der Künste der DDR seit 1993 der Akademie der Künste Berlin an.

Nach dem Unfalltod ihres Sohnes Clemens im Februar 1998 zog sich die Autorin aus der literarischen Öffentlichkeit fast völlig zurück. Zwei Jahre später erschienen nach 14-jähriger Schreibpause die teils autobiografischen Essays "Film und Verhängnis". Es folgten noch zwei schmale Bände, die Texte für die Tageszeitung "Die Presse" versammelten. 

In Wien, wo sie bis zu ihrem Tod lebte, hielt sich Ilse Aichinger fast täglich in ihrem Stammcafé Café Demel am Kohlmarkt auf und ging auch häufig ins Kino.

Ilse Aichinger starb am 11. November 2016 im Alter von 95 Jahren.

Von Anfang an rief Aichinger in ihren Werken zur Kritik an politischen und gesellschaftlichen Zuständen auf und sprach sich gegen falsche Harmonie und Geschichtsvergessenheit aus. Bereits 1945 schrieb sie einen Text über die Welt der Konzentrationslager "(Das vierte Tor)", der erste in der österreichischen Literatur. Ein Jahr später schrieb sie in dem Essay "Aufruf zum Misstrauen": „Wir müssen uns selbst misstrauen. Der Klarheit unserer Absichten, der Tiefe unserer Gedanken, der Güte unserer Taten! Unserer eigenen Wahrhaftigkeit müssen wir misstrauen!“ Mit diesem Aufruf gegen die Verdrängung der Geschichte und für eine schonungslose Eigenanalyse wandte sich Ilse Aichinger gegen die deutsche Kahlschlagliteratur, deren Anhänger nach dem Ende des Zweiten Weltkrieges einen radikalen Neubeginn propagierten.

1948 schrieb sie ihren einzigen Roman "Die größere Hoffnung", in dem sie unter Bezug auf ihre eigene Biografie das Schicksal einer jungen „Halbjüdin“ im Nationalsozialismus schildert. Der Roman bietet keine konkret-realistische Darstellung von Demütigungen, Angst und verzweifelter Hoffnung, sondern eine allegorische Schilderung in zehn chronologisch angeordneten Bildern aus der subjektiven Perspektive eines fünfzehnjährigen Mädchens. Es wird nicht chronologisch nachvollziehbar erzählt, der Text ist eher ein Geflecht aus Traum, Märchen, Mythos und Historie. Monologe wechseln ab mit Dialogen, auktoriales Erzählen mit personalem. „Durch die symbolische Überhöhung wird das Grauen keineswegs verharmlost, sondern nur auf eine andere Ebene gehoben und mit zeitlosen Themen verknüpft.“

In ihren frühen Erzählungen, die den Einfluss Franz Kafkas zeigen, beschreibt Aichinger das „existentielle Gefesseltsein des Menschen durch Ängste, Zwänge, Träume, Wahnvorstellungen und Fieberphantasien“. Das Thema der schwierigen Beziehung zwischen Traum und Realität und zwischen Freiheit und Zwang kehrt immer wieder, etwa im Prosaband "Wo ich wohne" (1963). In der gleichnamigen Titelerzählung geht es aber auch um das Thema der Entfremdung und um die Frage von Autonomie und Verantwortung.

Von Anfang an zeigte Aichingers Werk eine ausgeprägte Tendenz zur Verknappung, feststellbar zum Beispiel an der Bearbeitung ihres ersten und einzigen Romans "Die größere Hoffnung" (1948 und 1960). Der Sammelband "Schlechte Wörter" (1976) zeigte dazu eine Themenveränderung bei Ilse Aichinger: „Dominierte einst die Wahrheitssuche, gelangt sie jetzt zur subversiven Sprachkritik.“ Sprache erschien der Autorin immer mehr als unbrauchbares Ausdrucksmittel. Zu dieser Auffassung passte das zunehmend seltener werdende Schreiben, zudem wurden die Texte immer kürzer, bis hin zum Aphorismus.

Ilse Aichinger selber erklärte das als Reaktion auf die fehlenden Zusammenhänge in der Welt der Gegenwart: "Man kann nicht einfach drauflosschreiben und künstlich Zusammenhänge herstellen." Ihre "Poetik des Schweigens" ist ihre Konsequenz aus der Ablehnung jeder Form von Konformismus: „Gegen die sehr häufige Meinung des ‚So ist es eben‘, die, was sie vorfindet, fraglos akzeptiert. Die Welt verlangt danach, gekontert zu werden.“


Die folgenden Ehrenzeichen und Orden hat Ilse Aichinger nicht angenommenen.












</doc>
<doc id="11977" url="https://de.wikipedia.org/wiki?curid=11977" title="Kongruenzsatz">
Kongruenzsatz

Als Kongruenzsatz bezeichnet man in der ebenen Geometrie Aussagen, anhand deren sich einfach die Kongruenz von Dreiecken nachweisen lässt. Dreiecke sind kongruent, wenn sie in Form und Flächeninhalt gleich sind. Die "Dreieckskongruenz" (also die Kongruenz von Dreiecken) bildet eine Äquivalenzrelation, das heißt, kongruente Dreiecke können als gleich angesehen werden.

In den üblichen Bezeichnungen der vier Kongruenzsätze steht jeweils „S“ für die Übereinstimmung einer Seitenlänge und „W“ für die Übereinstimmung eines Winkels:

Stimmen zwei Dreiecke in zwei oder allen drei Innenwinkeln überein, so sind sie "nicht" notwendigerweise kongruent. Sie sind jedoch ähnlich.

Die nachfolgende Abbildung zeigt für jeden der vier Kongruenzsätze die Größen, in denen zwei Dreiecke übereinstimmen müssen.

Von links nach rechts: SSS, WSW, SWS, SSW.

Klassisch beweist man die Kongruenzsätze, indem man Konstruktionen mit Zirkel und Lineal angibt, die aus den entsprechenden gegebenen Größen eines Dreiecks ein zweites konstruieren. Geht dies nur auf genau eine Weise, so sind die beiden Dreiecke kongruent. Mit Bezeichnungen wie in obiger Abbildung geht dies wie folgt:


Das nebenstehende Bild zeigt, dass der Winkel beim SSW-Satz der längeren Seite gegenüberliegen "muss". Andernfalls hätte man Dreiecke, die zwar in drei Teilen (SSW) übereinstimmen, aber nicht kongruent sind: Die beiden Dreiecke formula_12 und formula_13 stimmen in den Seitenlängen formula_1 und formula_3 sowie im Winkel formula_50 überein. Die Seitenlängen formula_51 und formula_52 unterscheiden sich aber.




Die vier Kongruenzsätze bilden die Grundlage eines Beweisverfahrens, das in der Elementargeometrie häufig verwendet wird: In einem "Kongruenzbeweis" begründet man die Gleichheit zweier Streckenlängen oder zweier Winkelgrößen dadurch, dass man zunächst die Kongruenz zweier geeigneter Dreiecke zeigt und anschließend die Gleichheit entsprechender Seitenlängen bzw. Winkel folgert.



</doc>
<doc id="11979" url="https://de.wikipedia.org/wiki?curid=11979" title="Atari ST">
Atari ST

Atari ST ist eine Serie von Heim- bzw. Personal Computern der Atari Corporation, die von 1985 bis 1994 produziert wurde. Die ST-Serie eignete sich durch die grafische Oberfläche "GEM" unter anderem für professionelle Büroanwendungen und wurde wegen der serienmäßig vorhandenen MIDI-Schnittstelle als "der" Standardcomputer schlechthin in kleinen und großen Tonstudios eingesetzt. Die Abkürzung „ST“ steht dabei für "Sixteen/Thirty-Two" (16/32), da der verwendete Hauptprozessor, der Motorola 68000, einen 16 Bit breiten Datenbus hat und intern mit 32 Bit arbeitet.

Die ST-Serie war eine Alternative zu den wesentlich teureren Apple-Macintosh-Modellen und stand in direkter Konkurrenz zu IBM-PC-kompatiblen Computern.

Der Atari ST war eines der ersten verbreiteten Modelle mit einer grafischen Benutzeroberfläche, dem GEM von Digital Research. Die Hauptspeichergröße lag zwischen 512 KiB (520ST) und 4 MiB (Mega ST4), diese Zahl wurde, nach Aufrunden, Teil der Modellbezeichnung (520ST – 512 KiB; 1040ST – 1024 KiB = 1 MiB).

Einzige Ausnahmen bildeten der 260ST (wurde mit 512 KiB ausgeliefert) und der 520ST+ (1 MiB). Der 260ST sollte – getreu seiner Bezeichnung – nur mit 256 KiB ausgeliefert werden, in der Endphase der Entwicklung stellte sich jedoch heraus, dass 256 KiB definitiv nicht ausreichen würden, um den Rechner mit TOS sinnvoll zu betreiben. Da jedoch die Werbung bereits angelaufen war, wurde er kurzerhand mit 512 KiB ausgeliefert. Bei den ersten in Europa erhältlichen 260ST wurde wie beim ersten amerikanischen 520ST das TOS von Diskette nachgeladen, was die Größe des verfügbaren Speichers stark reduzierte. Der 260ST unterschied sich kaum vom 520ST. Eigentlich bestand der einzige Unterschied darin, dass der 520ST das TOS in Festwertspeichern (ROM) mitbrachte, wobei die ersten 520ST und ST+ noch ohne solche Speicherbausteine geliefert wurden, weil diese zum Produktstart noch nicht verfügbar waren. Die Nachrüstung war aber auch beim 260ST einfach, da die Sockel bereits vorhanden waren. Der Ur-520ST verschwand bereits ein halbes Jahr nach Erscheinen vom Markt, ihm folgte ein Jahr später der 260ST. 1989 wurden sämtliche STs ohne Diskettenlaufwerk sowie der 1040STF eingestellt, Ende 1990 der MegaST und im Januar 1994 alle übrigen Geräte (zu dem Zeitpunkt bestand die Modellpalette aus 1040STFM, 1040STE, MegaSTE und den High-end-Rechnern Falcon 030 und TT 030).

Die Konzeptänderungen nach Abschluss der Entwicklungsphase zeigen sich auch beim Modell 520ST+. Hier fanden die zusätzlichen Speicherbausteine auf der Hauptplatine keinen Platz, so dass sie von Hand huckepack auf die Speicherschaltkreise der regulären Bestückung gelötet werden mussten. Einzelne Pins (RAS, CAS) dieser zusätzlichen Schaltkreise wurden nach oben gebogen und frei verdrahtet.

Zusätzliche Buchstaben gaben weitere Ausstattungsmerkmale an: „F“ im Namen gab ein internes Diskettenlaufwerk an, „M“ einen HF-Modulator. „+“ bedeutete eine Speichererweiterung (520ST+) oder andere Erweiterungen wie beim nicht veröffentlichten 1040 STE+. „E“ kommt von "enhanced", diese Rechner waren um einige Funktionen, beispielsweise die Fähigkeit, Töne in Stereo wiederzugeben, und eine auf 4096 Töne erweiterte Farbpalette erweitert worden. Die Modelle „LST“ (Stacy) und „NST“ (ST Book) bezeichnen tragbare Geräte.

Bis November 1985 wurde der Atari ST mit dem Betriebssystem auf Diskette ausgeliefert (TOS 1.0), spätere Modelle hatten das Betriebssystem im Festwertspeicher eingebaut.

Der Software-Emulator CPMZ80 für den Zilog-Z80-Hauptprozessor und das Betriebssystem CP/M 2.2 wurden kostenlos von Atari mitgeliefert. Dadurch konnten, vor allem zu Beginn, die damals weitverbreiteten 8-Bit-Programme für CP/M, wie z. B. WordStar, dBASE, Microsoft Multiplan, Turbo Pascal und andere, auf dem Atari ST weiterbenutzt werden, und die mit diesen Programmen erzeugten Dokumente, Datenbanken, Quellen und Daten blieben weiter verwendbar.

Die MegaST-Serie besaß eine abgesetzte Tastatur und einen Hauptspeicher von bis zu 4 MiB. Festplattenlaufwerke waren ebenfalls verfügbar (anfangs mit MFM-Verfahren und 20 MB) und direkt an den Atari ST anschließbar (DMA-Port, auch ACSI-Port (SCSI-Variante, steht für „Atari Computer System Interface“) genannt).

Der Atari ST besaß die Möglichkeit, entweder einen hochauflösenden Schwarzweiß- oder einen geringer auflösender Farbbildschirm anzuschließen. Die Farbauflösung betrug 320×200 Bildpunkte bei 16 Farben und 640×200 Bildpunkte bei vier Farben, jeweils aus einer Palette von 512 Farben (bzw. 4096 beim STE).

Der weit verbreitete monochrome Monitor SM124 hatte eine Auflösung von 640×400 Bildpunkten bei 70 Hz Bildwiederholfrequenz. Dies waren für die damalige Zeit hervorragende Werte, im Bereich der IBM-PC-kompatiblen Computer gab es gerade CGA, HGC und für besonders teure Rechner EGA, das nur 640×350 Bildpunkte und 60 Hz Bildwiederholfrequenz bot. Die Nachfolgemodelle SM125 und SM14x stellten auch 640×400 in 70 Hz dar.

Der Rechner wurde besonders im CAD- oder Desktop-Publishing-Bereich populär, insbesondere als Atari und andere Hersteller die hochauflösenden 19″-Monochrom-Monitore auf den Markt brachten. Im deutschsprachigen Raum überwogen auch ansonsten eher Büroanwendungen wie Textverarbeitung oder Tabellenkalkulation. Insbesondere für die Naturwissenschaften stand mit dem Textverarbeitungsprogramm Signum! eine Software zur Verfügung, die in dieser Zeit nahezu Alleinstellungsmerkmale aufwies.

In den Vereinigten Staaten wurde der ST vorwiegend mit Farbmonitor eingesetzt und galt eher als Spiele- und Demomaschine (siehe: Atari Demos). Weltweit brachte dem Atari ST eine fest eingebaute MIDI-Schnittstelle eine weite Verbreitung bei Musikern und in Tonstudios ein. Dazu gehörte auch Mike Oldfield, der den Atari ST überwiegend für die Arrangements seiner Songs nutzte. Auf dem Album "Earth Moving" wurde die Verwendung des Atari ST auf der Rückseite des Covers angegeben. Auch Depeche Mode und Fleetwood Mac nutzten in dieser Zeit den ST. Außerdem war der Atari ST sozusagen ein Mittler zwischen den Welten. Das Dateisystem der Disketten war mit dem von MS-DOS weitgehend kompatibel, so dass man beispielsweise Zugriff auf Textdateien hatte, die auf einem PC erstellt wurden. Es gab auch einen Apple-Emulator sowie einen CP/M- und Z80-Emulator, und er wurde – mit entsprechender Software versehen – als intelligentes Terminal und Entwicklerstation an verschiedensten Großrechnern und Mini-Computern von Hewlett-Packard sowie Workstations von Texas Instruments und Hewlett-Packard eingesetzt. Bemerkenswert am Betriebssystem der ST-Familie war die Vielfalt der bereitgestellten Schnittstellen. So war von Anfang an an den Anschluss von Grafiktabletts und Kameras gedacht, was zur damaligen Zeit einzigartig war, zumal entsprechende Hardware auf dem Markt überhaupt nicht verfügbar war.

Der Atari ST war zum im Jahr 1984 dreimal so teuren Apple Macintosh eine für breite Massen erschwingliche Alternative und stand in Konkurrenz zum etwas später auf den Markt gekommenen Amiga von Commodore.

Mehrere Fachzeitschriften wie "ST-Computer", "ST-Format", "ST Magazin", "TOS", "XEST", "ATOS", "68000er" oder "Atari Inside" versorgten die Nutzer mit Informationen zu diesem Rechner.


Standardmäßig bei allen ST-Varianten vorhanden sind:

Einige Modellvarianten haben darüber hinaus noch zusätzliche Anschlüsse:








Im Internet kursiert eine Fülle von Emulatoren für Atari ST. Die meisten dieser Emulatoren benötigen für ihre Funktion jedoch ein Abbild originaler Atari-ROMs, daher dürfen sie aus lizenzrechtlichen Gründen nur dann benutzt werden, wenn der Eigentümer des Rechners selbst auch Eigner eines Atari ST (bzw. seiner ROMs) ist. Als legaler Ausweg bleibt die Verwendung nachprogrammierter, frei erhältlicher TOS-Versionen wie EmuTOS.

Atari-Disketten können meist vom PC (oder einem älteren Apple Macintosh) gelesen werden. Schwierigkeiten bereiten jedoch Umlaute in Pfad- und Dateinamen aufgrund der unterschiedlichen Zeichensätze sowie höher formatierte Disketten (d. h. mit mehr als 80 Spuren und/oder mehr als 9 Sektoren pro Spur); diese funktionieren zuverlässig nur mit einem Original-Atari-Rechner und müssen daher umkopiert werden. Falls kein entsprechender Atari-Rechner zur Verfügung steht, kann dies z. B. mit der Live-CD von ARAnyM erfolgen. Dieser Weg, der lizenzrechtlich unproblematisch ist und der keinerlei Installation auf dem „Wirts-PC“ voraussetzt, macht es möglich, auf einer PC-Hardware auch ältere Atari-Diskettenformate zu lesen und die Dateien auf DOS-formatierte Disketten zu kopieren. Eine weitere Möglichkeit: 2DD-Disketten, die unter MS-DOS mit dem Befehl "format a: /u /f:720" formatiert werden, können sowohl vom Atari als auch von Windows gelesen und geschrieben werden und daher als Transportmedium zwischen beiden Computerwelten dienen. Windows XP unterstützt den Parameter "/f:720" nicht mehr, deswegen muss man auf "format /t:80 /n:9 a:" ausweichen.

Da aktuellen PCs das Diskettenlaufwerk fehlt, bleiben hier die Lösungen, die Daten entweder auf CD zu brennen, ein Diskettenlaufwerk nachzurüsten bzw. ein USB-Laufwerk anzuschließen, Wechselmedien wie ZIP-, JAZ- oder DVD-RAM-Laufwerke, Speicherkarten wie CF und SD (der Atari kann mit geeigneter Software wie dem HDDRiver oder BIGDOS PC-kompatible Fest- und Wechselplatten sowie Speicherkarten lesen und schreiben) oder den Atari und den aktuellen Computer mittels Netzwerkkarte am Atari (EtherNEC bzw. EtherNEA) und meist vorhandenem Netzwerkanschluss zu verbinden.

Ein Open-Source-Emulator ist Hatari.

Ein professioneller Atari-Emulator für Apple Macintosh ist MagiCMac(X). Dieser Emulator stellt Calamus u. a. eine Drucker-Schnittstelle zur Verfügung, so dass Calamus jeden Apple-seitig installierten Drucker erkennen und ansprechen kann. Auf der Intel-MagicMac-Version funktioniert dieser Druckertreiber jedoch nicht mehr. Calamus-SL-Anwender behelfen sich mit dem PDF-Export und drucken die erzeugten PDF-Dateien dann unter OS X aus.

Für Windows-Betriebssysteme gibt es das Pendant MagiC PC. Auf Windows 7-32 Bit läuft der STemulator.

Für die Apple-Computer "Performa 450", "475" und "630" existierte zudem (1995) mit "Mac STout" eine MagicMac-kompatible Schnittstellenkarte, die die beim Macintosh nicht vorhandene parallele (Centronics-/Drucker-)Schnittstelle sowie eine serielle Schnittstelle mit originalen Atari-Chips nachrüstete. In Verbindung mit MagiC Mac konnten Programme wie Wordplus, Signum!, Script oder Calamus so mit PC-Druckern drucken, und Atari-Programme liefen auf dem Macintosh mit etwa vierfacher Geschwindigkeit eines Atari TT. McSTout existierte zudem in einer Variante, die zudem auch noch MIDI-Ports bereitstellte.



Siehe auch .




</doc>
<doc id="11980" url="https://de.wikipedia.org/wiki?curid=11980" title="Fernsehen">
Fernsehen

Als Fernsehen (auch kurz "TV", vom griechisch-lateinischen Kunstwort "Television") bezeichnet man ein Massenmedium, das Fernsehsendungen konzipiert und produziert oder sie zukauft und sie live oder zeitversetzt unidirektional an ein disperses Massenpublikum vermittelt.

Der Begriff Fernsehen benennt auch den soziotechnischen Komplex von Organisationen (öffentlich-rechtlichen oder privatwirtschaftlichen Sendern) und Techniken (Aufnahme-, Aufzeichnungs-, Produktions-, Sende- und Empfangstechniken).

Das Verb "fernsehen" bezeichnet die Tätigkeit, mittels eines Empfangsgerätes eine Fernsehsendung anzuschauen.

Fernsehen wird üblicherweise mit modulierter Hochfrequenzübertragung über Antennen oder kabelgebunden (Kabelfernsehen) genutzt. Die Übertragung über Antennen erfolgt dabei entweder nicht-terrestrisch mit Hilfe von Rundfunksatelliten (Satellitenfernsehen) oder über terrestrische Frequenzen (Antennenfernsehen, auch terrestrisches Fernsehen genannt).

Auf welche Weise die Bild- und Toninformationen bei der Übertragung kodiert werden, hängt von der verwendeten Fernsehnorm ab. Es gibt weltweit eine Vielzahl von Fernsehnormen, die jedoch auf wenige Grundparameter reduziert werden können. Zum einen wird zwischen analogem Fernsehen und digitalem Fernsehen unterschieden. Als analoges Fernsehen bezeichnet man Fernsehen, bei dem zumindest die Bilddaten, meist aber auch die Tondaten analog übertragen werden.
Es gibt mehrere analoge Fernsehnormen. In vielen Regionen ist das analoge Fernsehen von der digitalen Bildübertragung verdrängt worden (siehe digitales Fernsehen, Analogabschaltung).

Beim Medium Fernsehen stehen wenige Programmanbieter einem Massenpublikum von mehreren Millionen Zuschauern (Konsumenten) gegenüber.

Dieser Umstand ist in den meisten Ländern historisch bedingt, da aufgrund der bestehenden Gesetze das Fernsehen zunächst ausschließlich von öffentlichen Anstalten betrieben wurde. Dies änderte sich mit sinkenden Sendekosten und der Liberalisierung des Marktes. Die Kosten zur Produktion der Inhalte werden seitens der Programmanbieter auf verschiedene Weisen gedeckt (siehe hierzu Duales Rundfunksystem).

Die öffentlich-rechtlichen Rundfunkanstalten, die einen staatlich festgelegten Programmauftrag haben, an dem sich ihre Programme orientieren müssen, finanzieren sich in Deutschland zu einem großen Teil aus staatlich festgelegten Beiträgen (siehe hierzu auch Beitragsservice) und Werbung. Einige weitere Einnahmequellen bestehen im Weiterverkauf oder in der Lizenzierung eigener Fernsehproduktionen.

Die privaten Fernsehsender finanzieren sich dagegen fast ausschließlich durch Werbung. Fernsehproduktionen werden in der Regel in Auftrag gegeben, ein Großteil der Programminhalte wird gekauft oder lizenziert. Bei den gekauften oder lizenzierten Produktionen der deutschen privaten Fernsehsender handelt es sich meist um umsatzträchtige US-amerikanische Serien oder Filme, teilweise ganze Fernsehformate.

Neben den öffentlich-rechtlichen und den privaten Fernsehsendern gibt es noch sogenannte Bezahlfernsehsender (engl. "Pay-TV"; auch "subscription television"), die vom Zuschauer direkt bezahlt werden (z. B. Sky Deutschland, früher Premiere). Diese verschlüsseln ihre Sendungen, die so nur mit speziellen Decodern gesehen werden können. Der Zuschauer bezahlt dann je nach System entweder pro Programm beziehungsweise Programmpaket oder pro Sendung (Pay-per-View). Da Werbung vom Zuschauer meist als störend empfunden wird, verzichten Bezahlfernsehsender in der Regel auf Werbung. Dies empfinden manche Pay-TV-Abonnenten als großen Vorteil. Die Eigenwerbung der Pay-TV-Sender (On Air Promotion) kann als störend empfunden werden.

In Großbritannien existiert auch eine Zwischenform. Das dortige BSkyB ist zwar Bezahlfernsehen, es sendet gleichwohl etwa soviel Werbung wie werbefinanziertes Privatfernsehen.

Das Fernsehen zählt zu den Massenmedien. Es hat Anteil an der Meinungsbildung der Bevölkerung.

Genutzt wird das Fernsehen seitens der Zuschauer hauptsächlich zur Unterhaltung, Freizeitgestaltung sowie zur Informationsbeschaffung oder Bildung. Entsprechend unterteilt man die Fernsehsendungen auch in Unterhaltungssendungen, Nachrichtensendungen und Bildungsfernsehen. Letzteres wird in Deutschland, mit Ausnahme von Dokumentationen, gemäß ihrem Bildungsauftrag fast nur von öffentlich-rechtlichen Fernsehanstalten angeboten. Zunehmend werden Bildung und Nachrichten mit Unterhaltung zum sogenannten Infotainment vermischt. Diese Art von Sendungen wird auch zunehmend von privaten und Bezahlfernsehsendern angeboten.

Das Spektrum der Unterhaltungssendungen ist äußerst vielfältig und umfasst unter anderem Filme, Serien und Unterhaltungssendungen, die sich in weitere Sparten unterteilen lassen. Zur Rubrik Bildungsfernsehen gehören Dokumentationen, Politik-, Ratgeber- und Wissenschaftssendungen.

Seitens der Wirtschaft wird das Fernsehen zur Werbung benutzt. In kurzen Werbespots werden einzelne Produkte und/oder Marken präsentiert und deren Kauf empfohlen. Neben den Werbespots gibt es auch sogenannte Verkaufsshows oder Dauerwerbesendungen. Einige spezialisierte Sender, sogenannte Home-Shopping-Sender zeigen den ganzen Tag nichts anderes als solche Verkaufsshows.

Für die Werbeindustrie ist das Fernsehen eines der wichtigsten Medien, da auf diesem Weg viele Menschen erreicht werden können. Außerdem eignet sich das Fernsehen für die Werbeindustrie, weil man mit diesem Medium besonders gut Gefühle und Emotionen beim Konsumenten wecken kann.

In Deutschland regeln staatsvertragliche Auflagen Dauer und Häufigkeit der Werbung pro Sendung beziehungsweise Sendezeit. Die Rechte zur Ausstrahlung von Werbung in den öffentlich-rechtlichen Fernsehanstalten sind stärker eingeschränkt. Sie dürfen beispielsweise nach 20 Uhr sowie an Sonntagen und im ganzen Bundesgebiet anerkannten Feiertagen keine Werbung ausstrahlen. Bei nicht öffentlich-rechtlichen Fernsehanstalten darf die Werbung 20 Prozent des Programms, somit 12 Minuten pro Stunde, bisher nicht überschreiten.

Staat und Politik benutzen das Fernsehen hauptsächlich dazu, die Bevölkerung zu informieren und die Meinungsbildung zu fördern. Das Spektrum der Inhalte reicht dabei von Hinweisen auf wichtige Ereignisse bis zu Katastrophenwarnungen. Politiker versuchen oft, über das Fernsehen die Bevölkerung von ihren eigenen Ansichten zu überzeugen oder ihre Arbeit zu rechtfertigen. In vielen Staaten wird Fernsehen auch häufig seitens der Regierung zur Propaganda eingesetzt.

Ein weiteres Anwendungsgebiet der Fernsehtechnik besteht in Videoüberwachung, die zunehmend günstiger wird, da die Technik einerseits qualitativ besser und gleichzeitig billiger wird, andererseits die besonders kostenintensive Herstellung von Inhalten entfällt.

In der Weltraumforschung, die sowieso hohe Kosten für Missionen veranschlagt, wird die Fernsehtechnik ebenfalls eingesetzt. Allerdings werden hier selten bewegte Bilder aufgenommen. Stattdessen kommen Spezialkameras zum Einsatz, die oft besonders hohe Auflösungen besitzen und/oder Licht anderer Spektralbereiche, als die vom menschlichen Auge wahrgenommen, aufzeichnen und teilweise zeitversetzt zur Erde senden.

Technische Koordinationsstelle jedes Rundfunksenders ist der Schaltraum. Hier kreuzen sich alle ein- und ausgehenden Signale. Dabei gilt der Grundsatz, dass für jeden Bildkanal automatisch ein Audiokanal freigeschaltet wird ("Audio follows video"). Im Schaltraum treffen die Signalströme von Satellitenübertragungen, Berichte aus den Außenstudios und Live-Schaltungen zu den Korrespondenten und Ü-Wagen zusammen. Eine Kommandoverbindung führt zu den anderen technischen Einrichtungen des Senders wie Aufnahmestudios, Kontrollraum, Grafik, Schnitt, Regie etc. Bei vielen Sendern sind die technischen Abteilungen bereits vollständig miteinander vernetzt. So wird etwa ein Film am Computer geschnitten und über das Netzwerk verschickt. Über den Schaltraum laufen auch Direktverbindungen zu einem etwaigen Senderverbund (z. B. ARD-Stern)

In der Frühzeit des Fernsehens wurden viele Sendungen live ausgestrahlt oder als audiovisuelle Sequenz vorproduziert. Aufzeichnungen von Live-Sendungen erfolgten damals teilweise noch durch Abfilmen vom Monitor. Seit Mitte der 1960er-Jahre erfolgte die Studioaufzeichnung und Sendeabwicklung vorgefertigter Inhalte mit MAZ-Geräten, aus denen Ende der 1970er-Jahre die Videorekorder für den Heimbedarf entwickelt wurden und die analoge oder digitale Signale auf Magnetbänder oder Festplatten speichern und mit denen auch die Bearbeitung der einzelnen Beiträge erfolgt. An ihre Stelle sind heute vielfach Computer getreten, die eine Vielzahl an zusätzlichen Bearbeitungsmöglichkeiten bieten. Um die alte Außentechnik weiter benutzen zu können, sind MAZ-Geräte aber immer noch im Einsatz.

Mittlerweile sind die meisten Fernsehsender zu exakt programmierbaren Videoservern übergegangen, auf die sich die Beiträge nach einer festen Zeitabfolge speichern und abrufen lassen. Videoserver können große Sendestrecken vollautomatisch fahren und reduzieren damit den technischen und personellen Aufwand auf ein Minimum. Wegen der digitalen Form der Signalaufbereitung erfolgt heute auch die langfristige Speicherung (Archivierung) der Beiträge in digitaler Form. Die Fernsehsender stecken gerade in einer Migrationsphase, weil ältere Bestände der Fernseharchive oft mit großem Aufwand digitalisiert werden, um eine weitere Schädigung der auf alten Magnetbänder gespeicherten Inhalte zu verhindern und um einen schnellen Zugriff zu ermöglichen.

Für jedes Programm steht eine sogenannte „Sendestraße“ zur Verfügung. Die Videoserver oder auch sogenannte LMS (Library Management System) werden von der Ausspielsoftware (Harris, Pebble Beach u. a.) angesteuert. D. h. sowohl das Starten des Audio-/Video-Sigals als auch das Schalten bzw. Mischen auf dem Sendemischer wird von der Ausspielsoftware ferngesteuert. So kann ein Aufsichtsingenieur die Ausstrahlung mehrerer Programme technisch überwachen. Er muss eng mit der Programmredaktion zusammenarbeiten und auf kurzfristige Änderungen eingreifen können. Dies erfordert gute Kenntnisse der Ausspielsoftware und der Editierung der Programmevents.

Die digitale Archivierung erfolgt heute überwiegend auf Bandmedien oder Festplatten. Wegen der niedrigen Kosten werden heute überwiegend LTO-Bänder verwendet. Bei den Dateitypen dominieren das IMX-Format von Sony und das DVCPro50-Format von Panasonic. Die Auflösung beträgt 720×576 Pixel. Die Datenrate beträgt 50 Mbit/s, pro Stunde fallen bei der Aufzeichnung etwa 27 Gigabyte an Daten an. Bei der alltäglichen Archivierung wählen die Sender heute Kombinationslösungen zwischen Band und Festplatte, weil bei der kurzfristigen Speicherung auf Festplatten ein schnellerer Zugriff möglich ist. Die LTO-Bänder, die über eine Speicherkapazität von 200 bis 500 Gigabyte verfügen, werden von computergesteuerten (redundanten) Robotern verwaltet, die einen Bestand von bis zu 6000 Medien organisieren können. Eine derartige Bibliothek fasst an die 1200 Terabyte pro Roboter.

Das moderne Computernetzwerk größerer Fernsehsender ermöglicht eine vollautomatische Datenübertragung vom Datenband zum Videoserver und umgekehrt, auch Online-Abfragen externer Archivkunden (andere Sender, Agenturen etc.) können jederzeit abgewickelt werden. Moderne Steuerungssoftware, wie "DIVArchive", organisiert alle Anfragen und ist mittlerweile so leistungsfähig, dass am PC Szenen aus einzelnen Archivbeständen herausgeschnitten werden können, ohne dass der gesamte Beitrag aus dem Archiv geladen werden muss ("Partial Restore"). Sendeabwicklung und Archiv (das Herzstück jedes Senders) sind damit so eng verzahnt, dass ein jederzeitiger Zugriff möglich ist. Bei analogen Archivbeständen muss der jeweilige Beitrag immer noch von einem Mitarbeiter aus dem Regal geholt und mühsam von Hand weiterbearbeitet werden, was Zeit und Ressourcen kostet.

IMX und DV sind in Verbindung mit "Wrapper-Formaten" wie MXF geeignet, Metainformationen zu speichern. Diese haben rein beschreibende Funktion und enthalten neben Titel und Personenangaben auch Stichwörter zum Sendungsinhalt. Anhand dieser Begriffe können die jeweiligen Beiträge (ähnlich wie bei digitalen MP3-Musiksammlungen) direkt über das Netzwerk aufgerufen werden. Damit lässt sich jeder Archivinhalt anhand direkt eingegebener Stichwörter finden und öffnen.

Ältere Archive wurden zum Teil zerstört, teils aus politischen Gründen, zum Teil einfach, um Platz zu schaffen. Viele Fernsehsender begriffen gar nicht, welche Werte sie vernichteten. So sucht heute zum Beispiel der britische Fernsehsender BBC weltweit nach eventuell noch vorhandenen Kopien aus den Anfängen der Doctor-Who-Serie, bei der über 100 Teile aus dem BBC-eigenen Archiv absichtlich vernichtet wurden.

Viele in der DDR gezeigte Filme verschwanden ebenfalls aus den Archiven oder erreichten nach der Wiedervereinigung der beiden deutschen Staaten einen Zustand, in dem sie nicht mehr gezeigt werden können. Zu diesen Filmen gehört zum Beispiel die Serie: „Heißer Draht ins Jenseits“.

Besonders Filme aus den Anfängen des Fernsehens wurden lange Zeit nicht als genügend wertvoll zum Archivieren betrachtet. Videobänder waren teuer und knapp und wurden nach dem Senden neu bespielt. Auch urheberrechtliche Gründe verhinderten das Interesse an einer Aufbewahrung. So war ein erneutes Abspielen bereits gezeigter Filme teilweise mit sehr hohen Gebühren verbunden.

In Deutschland besitzen (Stand 2004) 95 Prozent der Haushalte mindestens ein Fernsehgerät, 28 Prozent der Haushalte zwei Fernseher (synonym: Fernsehgerät), elf Prozent mehr als zwei. Satellitenempfang haben 37 Prozent der Haushalte, Kabelfernsehen haben 54 Prozent der Haushalte. 70 Prozent der Haushalte verfügen über einen Videorecorder.

In der Bundesrepublik hat sich der Fernsehkonsum seit Einführung des Fernsehens bis einschließlich 2011 – bezogen auf alle über drei Jahre alten Personen – stetig erhöht. Im Schnitt sah jeder Bundesbürger im Jahr 1992 158 Minuten täglich fern, 1997 183 Minuten. Im Jahr 2004 waren es bereits 210 Minuten, 2011 225 Minuten, also 3,75 Stunden. Hochgerechnet auf ein Jahr entsprach dies schon 2005 einer Non-Stopp-Fernsehdauer von fast zwei Monaten pro Jahr. Überdurchschnittlich viel sehen Arbeitslose und ältere Menschen fern. Frauen und Mädchen nutzen das Fernsehen öfter als Männer, und aus Entspannungsgründen; sie lassen es aber auch oftmals im Hintergrund laufen. 90 % der Befragten beider Geschlechter geben jedoch an, dass sie das Fernsehen primär zur Informationsgewinnung nutzen.

Fernsehen ist ein Massenmedium und hat sich seit den 1950er Jahren in den Industriestaaten zum Leitmedium entwickelt. Inzwischen haben rund 95 Prozent der deutschen Haushalte mindestens ein Fernsehgerät, 40 Prozent davon sogar zwei oder mehr. Für viele Menschen ist es Teil des Alltags geworden und strukturiert oft sogar den Tagesablauf. Es erfährt eine Zuwendung durch alle Schichten und Altersgruppen und tritt mit einer zuvor nicht gekannten Wirksamkeit an die Stelle aller Institutionen mit publizistischem Anspruch, ohne diese aber vollständig zu ersetzen. In Deutschland sahen im Jahr 2008 die Zuschauer pro Tag etwa 3,5 Stunden Fernsehen, in den USA betrug die Dauer im vierten Quartal 2008 5 Stunden.

Fernsehen wirkt orientierend und nivellierend (ausgleichend). Wichtigstes Instrument dafür ist die häufige Wiederholung. Dadurch wird es zur Grundlage der allgemeinen Geschmacks- und Stilbildung und beeinflusst die gesellschaftliche Kommunikation. So konstatiert Alexander Kluge eine Nivellierung der Stimmen auf einen Durchschnittsfernsehton.

Durch das verstärkte Aufkommen von Spartenkanälen, verbunden mit der wachsenden Rolle des Internets bzw. digitalen Fernsehens und der neuen sozialen Medien im gesellschaftlichen Leben geht die stilbildende Funktion des Fernsehens allerdings wieder zurück. Jedermann kann in gewissen Grenzen sein eigener Programmdirektor werden. Dies bewirkt eine zunehmende Zersplitterung und Fragmentierung der gesellschaftlichen Wahrnehmung und Kommunikation. Damit sachgemäß umgehen zu erlernen, ist Aufgabe der Medienpädagogik. Das Fernsehen hat wie alle Medien oft einen Einfluss auf die Meinungsbildung der Konsumenten. Da die Inhalte des Fernsehens jedoch öfter als andere Medien transportiert werden, hat es hierbei eine vorrangige Bedeutung.

Fernsehsender erheben oft den Anspruch, dem Zuschauer einen Blick auf die komplexe Gesellschaft zu präsentieren. Immer mehr und unterschiedlichere Bereiche des gesellschaftlichen Lebens erfahren eine breiterwerdende mediale Beachtung. Aber der darzustellende Lebensbereich muss interessant, verständlich und optisch umsetzbar sein. Damit geht von vornherein eine Selektion einher, verbunden mit einer Reduktion der vielfältigen kommunikativen Codes und Zeichensysteme, sowie einer Spezialisierung und Perfektionierung akustisch-visueller Signale. Tatsächlich ist es in vielen Fällen sogar so, dass Fernsehen die Themen, über die Verständigung lohnend scheint, erst schafft. Auf der Suche nach neuen Themen, die die Zuschauer binden, durchbrechen die Fernsehmacher oft Grenzen, sodass es in den Augen vieler oft zu Tabubrüchen kommt.

Medienkritiker wie Neil Postman befürchten durch den wachsenden Medienkonsum den Verfall von moralischen Verhaltensregeln. Speziell die Darstellung von Gewalt im Fernsehen wird kritisiert. Studien zeigen einen Zusammenhang zwischen Fernsehsendungen mit Darstellungen von Gewalt und gewalttätigem Verhalten. In welchem Maß es sich dabei um einen kausalen (ursächlichen) Zusammenhang handelt, ist umstritten und Gegenstand wissenschaftlicher Untersuchungen.

Mehrere Studien deuten darauf hin, dass die Höhe des Fernsehkonsums in der Kindheit mit dem später erreichten Bildungsabschluss in Beziehung steht (d. h., je höher der Fernsehkonsum, desto schlechter der Abschluss).

Außerdem wird hoher Fernsehkonsum in der Kindheit und Jugend mit Übergewicht bzw. einem überdurchschnittlich hohen Body-Mass-Index, Bewegungsmangel, schlechter körperlicher Fitness, Tabakrauchen und den entsprechenden Folgeerkrankungen im Erwachsenenalter in Verbindung gebracht.

Auch das Gegenteil ist möglich: diskutiert werden Zusammenhänge zwischen bestimmten Fernsehformaten (z. B. Germany’s Next Topmodel) und Magersucht.

Darüber hinaus ist es fraglich, ob selbst speziell für Kleinkinder konzipierte Fernsehsendungen und Videos deren Spracherwerb unterstützen. Gemäß einer Studie aus dem Jahre 2009 scheinen Kinder unter drei Jahren, auch von speziell auf Kleinkinder zugeschnittene Sendungen zur Förderung der Sprachbildung, kaum zu profitieren: Kleinkinder waren nur dann in der Lage neue Verben zu erlernen, wenn ein Erwachsener sie dabei aktiv unterstützte.

Die Verbreitung des Fernsehens im ländlichen Indien hat laut einer Studie von Robert Jensen und Emily Oster zu höherem weiblichen Schulbesuch und einer geringeren Fertilitätsrate geführt.

In Brasilien hat die Verbreitung von Rede Globo die Zuschauerzahlen für seine Seifenopern stark erhöht. In diesen Seifenopern haben über 70 Prozent der dargestellten weiblichen Charaktere keine Kinder. Insbesondere arme Frauen hatten seit der Verbreitung von Rede Globo in den 1970er und 1980er Jahren weniger Kinder. Der fertilitätsreduzierende Effekt entspricht etwa dem von zwei zusätzlichen Jahren weiblichen Schulbesuchs.

In den Anfangsjahren der Entwicklung des Fernsehens wurden Begriffe wie "elektrisches Sehen", "telegraphisches Sehen", "elektrisches Fernsehen" und "Telephanie" benutzt.

Im Jahr 1883 erfand Paul Nipkow das „Elektrische Teleskop“, das mit Hilfe einer rotierenden Scheibe („Nipkow-Scheibe“), die mit spiralförmig angeordneten Löchern versehen war, Bilder in Hell-Dunkel-Signale zerlegte beziehungsweise wieder zusammensetzte. Nipkow gab damit erstmals eine realisierbare Form für eine funktionierende Fernsehbildübertragung an, die jedoch erst viele Jahre später umgesetzt werden konnte. Durch die grundlegende Idee zur Realisierung wird Paul Nipkow manchmal als Erfinder des Fernsehens bezeichnet. Die Technik zur Bildzerlegung und -wiedergabe nach Nipkow war opto-mechanisch, jedoch gab es zum Zeitpunkt seiner Entwicklung noch keine Fernsehverfahren, die diese Erfindung weiterverwendeten.

Dass eine Wiedergabe eines Fernsehbildes auf elektronischem Weg möglich ist, demonstrierte 1906 in Deutschland Max Dieckmann (ein Schüler von Ferdinand Braun), der eine Braunsche Röhre umgebaut hatte. Als Aufnahmeinstrument setzte Dieckmann jedoch eine Nipkow-Scheibe ein. Mangels elektronischer Verstärkung im Jahr 1906 waren anstelle der Löcher in der Nipkow-Scheibe Drahtbürsten angebracht, die eine Metallschablone abtasteten. Diese Bilder konnten mit der Hand bewegt werden, der Fernseher von Dieckmann gab so bewegte Bilder (in einer Auflösung von 20 Zeilen bei 10 Bildern pro Sekunde) wieder. Da es keine „richtige“ Fernsehkamera gab, blieb der Fernseher von Dieckmann zunächst ohne praktische Bedeutung, die Bildröhre wurde damals als zu teuer und zu kompliziert angesehen.

1907 gelang dem Russen Boris Rosing die erste Übertragung und Empfang eines schemenhaften Fernsehbildes, wofür er in vielen Ländern, darunter auch in Deutschland, ein Patent erhielt.

In den 1920er Jahren ersetzte Leon Theremin die Löcher in der Nipkow-Scheibe durch Spiegelanordnungen und erreichte 1927 bereits Übertragungen bei Tageslicht und Bilder mit 100 Zeilen auf großflächigen Projektionen. Allerdings wurden seine Ergebnisse nicht publiziert, sondern vom sowjetischen Geheimdienst zur Personenüberwachung genutzt. (Der bekannte Weltrekord war damals 48 Zeilen.)

Das erste voll-elektronische Fernsehen entwickelte 1926 der ungarische Physiker, Elektroingenieur und Erfinder Kálmán Tihanyi – das „Radioskop“ (das zugehörige Patent ist mittlerweile in das Weltdokumentenerbe aufgenommen worden).

Die Kathodenstrahlröhre war viele Jahre die am weitesten verbreitete Methode, Bilder für das Fernsehen darzustellen. Die ersten Anwendungen fand sie aber in Messapparaturen. Die für das Fernsehen entscheidenden Weiterentwicklungen der Kathodenstrahlröhre steuerte Vladimir Zworykin bei, der 1923 den ersten brauchbaren elektronischen Bildabtaster, die Ikonoskop-Röhre erfand, welche ab 1934 in Serie hergestellt wurde, heute aber keine Verwendung mehr findet. 1929 erfand Vladimir Zworykin die Kineskop-Röhre zur Bildwiedergabe. Einigen Angaben nach hatte Philo Farnsworth schon 1927 unabhängig von Zworykin eine funktionierende Kathodenstrahlröhre und Kameraröhre entwickelt und im Labor mit beiden ein Bild übertragen, noch bevor Zworykin sein Bildröhrenpatent realisierte. Somit ebneten die beiden den Weg zum vollständig elektronischen Fernsehen.

1926 gelang Kenjiro Takayanagi erstmals nachweisbar die Übertragung und Darstellung eines Bildes in Form eines aus zwei Strichen bestehenden Buchstabens auf elektronischem Weg. In einem Versuchsaufbau, der auf die Idee Max Dieckmanns aufbaute, wurde ein japanisches Katakana-Schriftzeichen, das zuvor mit einer Nipkow-Scheibe als Bild zerlegt wurde, auf dem Schirm einer Kathodenstrahlröhre abgebildet.

John Logie Baird griff wie Kenjiro Takayanagi zur Bildzerlegung auf die ursprüngliche Nipkow-Scheibe zurück und entwickelte ein vollständiges funktionsfähiges System vom Studio, Übertragung bis zum Empfänger. Bereits 1928 gelang ihm mit seinem opto-mechanischen bzw. opto-elektronischen Fernsehverfahren die transatlantische Übertragung eines Fernsehbildes von London nach New York. Neben seinen zahlreichen Erfindungen und Reichweitenerfolgen dürfte Bairds größter Verdienst sein, dass er publikumswirksam die Möglichkeiten des Fernsehens vorführte, es dadurch populär machte und somit auch die technische Entwicklung insgesamt sehr beschleunigte. Außerdem war er ein Beteiligter bei der ersten Etablierung des Fernsehens als Medium.

Dieses erste Fernsehen der 1920er bis 1940er Jahren mit regelmäßigen Sendungen von etwa 40 Stationen war anfänglich Fernsehen in 30-Zeilennorm. Es strahlte über Radiowellen auf der Lang-, Mittel- und Kurzwelle aus und war in Nordamerika, Europa und Australien empfangbar. ("siehe auch →" Sendertabelle)

Die weltweit erste Fernsehübertragung mittels rein elektronischer Bildzerlegung und -wiedergabe und zeilenweiser Abtastung über eine Photozelle und Wiedergabe auf einer Kathodenstrahlröhre gelang Manfred von Ardenne am 14. Dezember 1930 in seinem Lichterfelder Laboratorium. Zur Funkausstellung in Berlin führte er ab dem 21. August 1931 das erste voll elektronische Fernsehen vor, das für lange Zeit die Zukunft des Fernsehens bestimmen sollte.

Ab dem 22. März 1935 wurde in Deutschland das erste regelmäßige Fernsehprogramm der Welt in hochauflösender Qualität ausgestrahlt (s. Fernsehsender Paul Nipkow, Berlin). Dieser Titel wird jedoch von der BBC streitig gemacht, da der Fernsehsender Paul Nipkow zunächst nur in 180 Zeilen sendete, die BBC hingegen seit 1936 mit 405 Zeilen. (1937 führte Deutschland 441 Zeilen ein.) Schon seit 1929 gab es verschiedene regelmäßige Fernsehdienste, so auch von der BBC, allerdings nur in niedrigen Auflösungen zwischen 30 und 60 Zeilen. Ihren Höhepunkt erlebten die Sendungen im Deutschen Reich 1933 bis 1945, die nur wenige tausend Zuschauer in sogenannten Fernsehstuben und Großbildstellen in Berlin und später Hamburg erreichten, mit den umfangreichen Übertragungen von den Olympischen Sommerspielen 1936. Hierfür wurde auch erstmals ein aus 15 Fahrzeugen bestehender mobiler Fernsehsender in Dienst gestellt. Für die Darstellung der Fernsehbilder in Leinwandgröße in den Großbildstellen wurde das Zwischenfilmverfahren verwendet. Weitere funktionsfähige Fernsehgroßbildverfahren in dieser Zeit waren: 1) die Zellenrastermethode (eine Zellenrastertafel mit 10.000 bis 40.000 Glühlampen als Bildpunkte; entwickelt von A. Karolus) sowie 2) Projektionsgeräte mit Schmidtschem Spiegelobjektiv (mit Braunscher Röhre). Das später sehr bekannte Eidophor-Großbildprojektionsgerät mit einem neuartigen Funktionsprinzip wurde 1939 erst theoretisch konzipiert. Nach langwierigen Prototypenphasen wurde es ab 1959 seriell produziert.

1939 wurde dem damals schon weitverbreiteten Volksempfänger, einem sehr einfach konstruierten Radio, der „Volksfernseher“, offizielle Bezeichnung „Einheitsempfänger“, zur Seite gestellt (Deutscher Einheits-Fernseh-Empfänger E 1). Es war geplant, dass fünf Firmen diesen Fernseher in den folgenden Jahren in großer Stückzahl herstellen sollten. Dieses Gerät war hauptsächlich mit Stahlröhren der 11er- und 14er-Serie bestückt und hatte einen fest eingestellten Empfangskanal. Das Gerät verfügte über eine sehr hochwertige Rechteckbildröhre für 441 Zeilen und Zeilensprungverfahren. Technisch gesehen war der E1 seiner Zeit weit voraus, da die Bildröhre sehr flach war. Es trat kaum eine Kissenverzerrung auf, somit ist diese Röhre mit Röhren aus den 1970er-Jahren durchaus vergleichbar. In Großbritannien wurde bereits 1936 mit „hochauflösendem Fernsehen“, allerdings mit nur 405 Zeilen, jedoch ebenfalls im Zeilensprungverfahren, begonnen. Der Ausbruch des Zweiten Weltkriegs verhinderte jedoch eine Produktion des E 1 in großen Stückzahlen. Die Angaben über die Anzahl der produzierten Geräte liegen bei ca. 50. Der geplante Verkaufspreis lag bei 650 Reichsmark. Obwohl die Nationalsozialisten auch das Fernsehen für ihre Zwecke zu nutzen versuchten, blieb das Radio (siehe Hörfunk) wegen der Reichweite und der erprobten Technik das wichtigste Medium für die nationalsozialistische Propaganda. Es waren bis 1939 wahrscheinlich nicht mehr als 500 Fernsehgeräte in privater Hand. Zu Kriegsbeginn waren die Entwicklungen aber schon weit gediehen. Breitbandkabel durchzogen bereits große Teile des Deutschen Reiches, es war an eine kombinierte Verbreitung des Fernsehprogramms über Antenne und Kabel gedacht. Es liefen Versuche mit Fernsehtelefonie. In Berlin und Leipzig wurde jeweils eine sogenannte Fernsehsprechzelle installiert. Sie verfügten neben dem Telefon noch über einen Bildabtaster sowie einen Bildschirm, so, dass sich die Gesprächspartner nicht nur hören, sondern auch sehen konnten. Im Winter 1944 wurden die zuletzt im Kuppelsaal des Olympiapark Berlin aufgenommenen Fernsehsendungen, die zu dieser Zeit hauptsächlich der Truppenbetreuung in hauptstadtnah liegenden Lazaretten dienten, im Deutschen Reich eingestellt. Entwicklungen fanden seit Kriegsbeginn nur noch für militärische Zwecke statt. Bekannt wurde z. B. die Gleitbombe Henschel 293, die vom Flugzeug aus auf Seeziele abgeworfen werden sollte. Sie hatte im Bug eine Fernsehkamera und einen nur ca. 5 kg schweren Fernsehsender. Dieser und die Kamera waren batteriegespeist. Der Sender sendete die aufgenommenen Bilder über eine im Heck eingebaute Yagi-Antenne ans Mutterflugzeug, wo sie über einen Monitor und Fernsteuerung ins Ziel gelenkt werden sollte. Das Kriegsende bedeutete für Deutschland den Verlust sämtlicher Patente sowie ein striktes Verbot von Entwicklungen auch im Bereich der Radio- und Fernsehtechnik.

Als europäische Norm wurden 625 Zeilen festgelegt. Diese Norm wurde von beiden deutschen Staaten ab 1952 übernommen. In den USA hatte man sich 1942 in einem Kompromiss auf 525 Zeilen bei 60 Halbbildern im Zeilensprungverfahren geeinigt, nachdem bereits verschiedene Sender mit Auflösungen von 441 bis 608 Zeilen in Betrieb waren. Frankreich entschied sich für eine eigene Norm mit 819 Zeilen, die zunächst auch von Belgien übernommen wurde. Schnell wechselte man jedoch auf 625 Zeilen, für die Wallonie gab es noch einen kurzzeitigen Parallelbetrieb mit 819 Zeilen. Großbritannien blieb bei der 405-Zeilen-Norm mit amplitudenmoduliertem Tonträger.

In der Deutschen Demokratischen Republik begann der offizielle Fernsehbetrieb am 21. Dezember 1952 (Stalins Geburtstag). Der erste offiziell verkaufte Fernseher war der vom Sachsenwerk hergestellte Rembrandt 852B (mit runder Bildröhre). Der Vorgänger, Leningrad T2, war hauptsächlich für Reparationsleistungen an die Sowjetunion gedacht.

In der Bundesrepublik Deutschland sollte erst am 25. Dezember 1952 der Fernsehbetrieb aus einem Hochbunker in Hamburg wieder aufgenommen werden. Bereits 1955 stellte Philips einen Fernseher vor, der alle europäischen Normen empfangen konnte. Der erste in der Bundesrepublik nach dem Krieg in Serie hergestellte Fernseher war der Telefunken FE8. Er kostete 1000 DM. Die Preise für Fernsehgeräte fielen im Laufe der Jahre, und 1957 wurden die ersten Geräte unter 1000 DM angeboten. Ein Statussymbol der 1950er Jahre, aber nur für Wohlhabende erschwinglich, waren sogenannte Fernsehtruhen, die Fernseher, Radio, Plattenspieler und manchmal auch noch ein Tonbandgerät in einem meist truhenartigen Gehäuse vereinigten. Solche „Röhrenfriedhöfe“ wurden bis in die späten 1960er produziert.

An der Schweizerischen Landesausstellung 1939 in Zürich wurde eine Schauspielgruppe mit einer von der ETH entwickelten Technik gefilmt und dem Publikum auf einem Bildschirm-Prototypen gezeigt. Als erste konzessionierte Gesellschaft startete die Schweizerische Radio- und Fernsehgesellschaft (SRG SSR) am 1. Januar 1958 einen regelmässigen Sendebetrieb.

Nachdem 1948 in den Bell Laboratories/USA der Transistor entwickelt worden war und wenige Jahre später die ersten Transistorradios auf den Markt kamen, lag es nahe, diese Technik auch für den Bau von Fernsehgeräten anzuwenden. Der Transistor hat gegenüber der Elektronenröhre, wie sie bis dahin in allen Geräten der Unterhaltungselektronik verwendet wurde, den Vorteil höherer Lebensdauer bei kompakterer Bauform. Da Transistoren auch mit niedriger Spannung und kleinerem Stromverbrauch arbeiten, sind sie vor allem für batteriebetriebene Geräte interessant.

Als Pioniere beim Bau von Transistorfernsehern gelten die japanische Firma Sony mit dem Modell 5-303 (1960) sowie die amerikanische Firma Philco mit dem Modell Safari 2010 (1959). Die Entwickler der transistorisierten Fernseher waren vor die Aufgabe gestellt, mit der damals noch relativ neuen Technik ein Gerät zu entwickeln, das qualitativ mit den in konventioneller Technik gebauten Fernsehern mithalten konnte bzw. diese noch übertreffen musste. Da die Transistorfernseher transportabel sein sollten, musste ein besonderes Augenmerk auf eine kompakte sowie mechanisch und elektrisch robuste Bauweise gerichtet werden. Da die einzig noch verbliebene Röhre, nämlich die Bildröhre z. T. mit sehr hohen Spannungen arbeitet, waren zumindest bei Batteriebetrieb Schaltungen notwendig, um diese Spannungen aus der Batteriespannung, meist 12 V, zu generieren. Als 1960 noch der UHF-Bereich mit Frequenzen über 300 MHz eingeführt wurde, waren für das Empfangsteil Transistoren notwendig, die außer dem breitbandigen Bildsignal auch diese hohen Frequenzen verarbeiten konnten.

Diese Anforderungen machten die frühen Transistorfernseher sehr teuer. Sie lagen preislich oft oberhalb hochwertiger Heimgeräte und spielten anfangs auf dem Markt der Unterhaltungselektronik kaum eine Rolle. Der erste deutsche Transistorfernseher war der Imperial Astronaut 1514. Er hatte allerdings außer der Bildröhre noch eine weitere Röhre, die Diode DY80 zur Gleichrichtung der Anodenspannung der Bildröhre. Der erste sowjetische Transistorfernseher war der Elektronika 50, ein recht kompaktes Gerät mit 50 mm sichtbarem Bild.

Die hohen Preise begannen zu fallen, als Ende der 1960er Jahre Geräte aus Fernost auf dem Markt kamen, die z. T. nur noch halb so viel wie deutsche Geräte kosteten. Neben den damals schon bekannten japanischen Markennamen kamen in den frühen 1970er Jahren Marken auf den Markt, von denen selbst Experten noch nie etwas gehört hatten. Diese „No Name“-Produkte waren oft von fragwürdiger Qualität, der Preisverfall machte aber tragbare Fernseher für viele Interessenten erstmals bezahlbar.

Durch die immer kompaktere Bauweise bei Geräten der Unterhaltungselektronik war man schon früh auf die Idee gekommen, verschiedene Geräte zu einer leicht zu transportierenden Einheit zu kombinieren. Bekanntestes Beispiel hierfür ist der Radiorecorder. Bereits 1974 hatte eine kleine japanische Firma unter dem Namen Standard einen Radiorecorder mit integriertem 5"-SW-Fernseher auf dem Markt gebracht. In den 80ern „wuchsen“ solche Kombinationen zu oft schwergewichtigen tragbaren Stereoanlagen, der Klang und auch der Preis stellte manches Heimgerät in den Schatten. Diese Geräte kamen fast durchweg aus Fernost, auch deutsche Marken ließen ihre Geräte dort produzieren.

Die Möglichkeiten immer kompakterer Bauweise, nicht zuletzt durch Einsatz integrierter Schaltungen, bekannt als IS oder IC spornte die Entwickler zur Entwicklung immer kleinerer Fernseher an. Solche „Handhelds“ waren mehr ein Statussymbol, der Nutzen war eher gering. Die bekanntesten Beispiele sind der Panasonic TR-001 (1970) sowie der MTV-1 des englischen Computerherstellers Sinclair. Die Größe dieser Geräte lag in etwa bei der eines Lexikons, sie waren mit einer Kathodenstrahlröhre mit drei bis vier Durchmesser ausgestattet. Im Jahre 1984 brachte Casio mit dem TV-10 den ersten echten Taschenfernseher auf den Markt und läutete durch Verwendung eines SW-LCDs zur Bildwiedergabe das Ende der Kathodenstrahlröhre ein. Den Vogel in Sachen Miniaturisierung schoss Seiko 1982 mit seiner TV-Watch ab. Hier hatte man einen winzigen Flüssigkristallbildschirm in eine Armbanduhr integriert, der Empfang geschah allerdings über ein externes Kästchen in der Größe eines Taschenradios, das man an die Uhr anschloss und in die Hemdtasche steckte. Ein Exot dieser Miniaturisierung war ein ebenfalls aus Fernost kommendes, für den Einbau in den DIN-Schacht im Armaturenbrett geeignetes Auto-Cassettenradio mit integriertem 35-mm-Fernseher – in Kathodenstrahltechnik.


Einer der Pioniere des Farbfernsehens war John Logie Baird, dem es 1928 gelang, erste farbige Fernsehbilder zu übertragen. Dies geschah aber noch mit mechanischer Technik auf der Sende- wie Empfängerseite. Erst mit der Erfindung der Schattenmaskenröhre bereits 1938 durch Werner Flechsig in Deutschland, die in modifizierter Form bis heute Bestandteil jedes Fernsehers oder Monitors mit Bildröhre ist, hatte man die Möglichkeit, zumindest auf der Empfängerseite auf mechanische Bauteile zu verzichten. 1939 wurden auf der Internationalen Funkausstellung Berlin die ersten farbigen Fernsehbilder vorgeführt.

Ab den späten 1940er-Jahren wurden in den USA Versuchsendungen mit verschiedenen Verfahren, z. B. mit rotierenden Farbfiltern für die drei Grundfarben ausgestrahlt, aber keines der Verfahren fand die Akzeptanz der Fachleute. Neben vielerlei technischer Probleme war das Hauptproblem, Farbfernsehsendungen so zu übertragen, dass sie auch kompatibel zu den Schwarzweiß-Fernsehern waren. Erst 1953 wurde in Nordamerika die NTSC-Norm für Farbfernsehen eingeführt. Man war bei der Durchsicht der in Deutschland nach dem Krieg erbeuteten Patentschriften auf das Patent von Werner Flechsig gestoßen. Der Ingenieur und Vizepräsident der RCA Norton Goldsmith entwickelte dann eine Farbbildröhre. 1954 begann der offizielle Sendebetrieb in Farbe. Das Problem der Kompatibilität zu Schwarz-Weiß-Fernsehern hatte man gelöst, indem die Farbsignale (Farbton und Farbsättigung, „Chroma“) und die Helligkeitssignale („Luma“) getrennt übertragen werden. Der Schwarz-Weiß-Fernseher empfängt nur die Helligkeitssignale, die dann das schwarz-weiße Bild ergeben, während im Farbfernseher zusätzlich die auf einem Seitenband gesendeten Chromasignale verarbeitet werden.

Im März 1954 brachte RCA mit dem CT100 den ersten in Großserie produzierten Farbfernseher auf den Markt. Er hatte 36 Röhren, eine 15-Zoll-Bildröhre und kostete 1000 Dollar, was nach heutiger Kaufkraft einem Preis von ca. 12.000 Dollar entspricht. Die Bildqualität war eher bescheiden und das Gerät war auch recht anfällig. Trotzdem waren damit die Amerikaner den Europäern weit voraus, die erst 12 bis 13 Jahre später mit der PAL-Norm von Walter Bruch das Farbfernsehen einführten. Der Vorsprung der Nordamerikaner hatte aber auch seinen Preis. Die NTSC-Norm besitzt einige Schwächen, sodass es häufig zu Farbtonfehlern bei der Darstellung kommt. Um diese Fehler auszugleichen, gibt es an NTSC-Farbfernsehern eine zusätzliche Einstellmöglichkeit, „Farbton“, mit dem eine händische Korrektur des Fehlers möglich ist. Das PAL-Verfahren sowie das französische SECAM (SÉquentiel Couleur À Mémoire) hatten zum Ziel, diese Farbfehler, die hauptsächlich auf dem Übertragungsweg durch Phasenfehler entstehen, auszugleichen. Das bedeutete erheblich höheren technischen Aufwand auf Sende- wie Empfängerseite. Deshalb bekam das PAL-System von den Amerikanern den Spitznamen „Pay Additional Luxury“. Im Gegenzug bezeichnen heute noch PAL-Techniker das amerikanische NTSC scherzhaft als „Never Twice Same Color“ bzw. „Never The Same Colour“.

Die Entscheidungen der Länder der Welt für ihr jeweiliges Farbfernsehsystem hatten wohl hauptsächlich politischen Hintergrund. Während viele den USA nahestehende Länder in Südamerika und Asien wie z. B. Japan sich für NTSC entschieden, führten z. B. ehemalige französische Kolonien oft das SECAM-System ein. Die Mehrheit der Länder, die in den 1960er Jahren noch kein Farbfernsehen hatten, entschieden sich aber für PAL, darunter nach heftigen internen Diskussionen auch England, allerdings mit der schon vorher eingeführten anderen Ton-Norm. Die UdSSR entschied sich nach vergeblichen Versuchen, ein eigenes Farbfernsehsystem einzuführen, neben zahlreichen anderen Ländern des ehemaligen Warschauer Paktes für SECAM. Im Gegensatz dazu entschied sich die Volksrepublik China für das PAL-System.

Am 25. August 1967 wurde durch einen symbolischen Druck auf einen roten Knopf durch Außenminister Willy Brandt das Farbfernsehen in Deutschland eröffnet. Der erste in Deutschland für das PAL-System produzierte Fernseher war der Telefunken FE807 PAL Color. In der DDR begann das Farbfernsehen 1969, dem 20. Jahrestag der DDR. Als Verfahren wurde auch hier das französische SECAM gewählt. Der erste in der DDR produzierte Farbfernseher war der RFT Color 20/1. Im Gegensatz zu den allermeisten damaligen Farbfernsehern war er (mit Ausnahme der Bildröhre) bereits volltransistoriert.

Das Zeitalter des Satellitenfernsehens begann am 12. August 1960. An diesem Tag wurde mit Echo 1 der erste passive Nachrichtensatellit in eine Umlaufbahn um die Erde gebracht. Die Qualität der übertragenen Fernsehbilder war allerdings sehr schlecht. Der erste aktive Nachrichtensatellit Telstar wurde am 10. Juli 1962 in seine Umlaufbahn gebracht. Mit seiner Hilfe wurde am 23. Juli die erste Live-Sendung für die Eurovision aus den USA gesendet. Am 26. Juli 1963 wurde mit Syncom 2 erstmals ein Kommunikationssatellit in eine fast vollständige geostationäre Umlaufbahn gebracht. Dies ermöglichte es, von einem festen Punkt auf der Erde ununterbrochen Signale an einen Satelliten zu senden oder von diesem zu empfangen. Neuartige rauscharme HEMT-Transistoren ermöglichten 1985 einen kostengünstigen Direktempfang von Fernsehsignalen der ECS-Telekommunikationssatelliten auf der geostationären Satellitenposition 13 Grad Ost. 1987 begann mit dem Astra 1A die Erfolgsgeschichte des europäischen Direktempfangsfernsehens per Satellit.

Am 30. April 2012 wurde die analoge Satelliten-Übertragung der deutschsprachigen Fernsehprogramme abgeschaltet. Eigens dafür wurde bereits bei den öffentlich-rechtlichen Sendern Das Erste, ZDF und Bayerisches Fernsehen sowie bei den privaten Sendern ProSieben, RTL und Sat.1 die Videotext-Seite 198 und im Internet die Webseite „klar digital“ eingerichtet.

Herkömmliches Fernsehen war bisher immer linear; aus diesem Grund war eine Ressource „TV-Sendekanal“ historisch immer begrenzt und so auch umkämpft; wer solche Ressourcen „Sendekanäle“ kontrollieren konnte, war auch in der Lage, unliebsame Mitbewerber vom Markt fernzuhalten. Wie nun Helmut Thoma im Heise-Forum "c’t" ausführte, ist mit einer Verbreitung von Fernsehen über das Internet nun diese historische Beschränkung gefallen; per Internet stehen rein theoretisch unendlich viele Sendekanäle zur Verfügung.

Das ZDF realisierte im Jahr 2007 ein neuartiges "Mediathek" genanntes Sendekonzept, welches das Potential von nicht linearem Fernsehen per Internet verdeutlichte und heftige Kritik von privaten TV-Sendeanstalten auslöste. Diese forderten eine Begrenzung von öffentlich-rechtlich finanzierten Online-Produkten.

Viele Bestrebungen, die technische Qualität zu verbessern, sind mangels Kooperation verschiedenster Interessengruppen gescheitert. Sicher ist die Ablösung des analogen durch das digitale Fernsehen, die beim Satellitenfernsehen im April 2012 bereits abgeschlossen wurde. Deutschland hatte sich das Ziel gesetzt, bis 2010 auf die digitale Ausstrahlung der Fernsehprogramme umzustellen. 2003 wurde in Berlin bereits die terrestrische analoge Fernsehausstrahlung auf digitale auf Basis von DVB-T umgestellt, im Ende 2008 war die Umstellung abgeschlossen. Auch beim Kabelfernsehen sollte in Deutschland der Analog-Digital-Übergang bis 2010 abgeschlossen sein. Kabel Deutschland hat im Jahr 2009 jedoch angekündigt, die analoge Verbreitung noch für mehrere Jahre parallel zur digitalen betreiben zu wollen. Daher wird sich zumindest in diesem Bereich die Analogabschaltung noch verzögern.

Erst in der Folge wird es wohl auch zu einer Verbesserung der Bild- und Tonqualität, insbesondere bei der Auflösung der Bilder (HDTV) kommen, da mit der fortschreitenden Weiterentwicklung der Computertechnik Anpassungen an zukünftige Fernsehnormen auch softwareseitig möglich werden und dies den Zwang zum Kauf neuer Geräte vermeiden helfen könnte. Mit HDTV soll zudem das Verschlüsselungssystem HDCP etabliert werden, mit dem sich das Aufzeichnen von Filmen und Sendungen theoretisch regeln und verhindern lässt.

Die früher überwiegend auf Kathodenstrahlröhren basierenden Fernsehgeräte werden bei sinkenden Preisen langsam von den flachen und damit platzsparenden Plasmabildschirmen oder Flüssigkristallbildschirmen abgelöst. Im Jahr 2006 wurden in Deutschland erstmals mehr Flachbildfernseher verkauft als konventionelle Röhrenfernseher. Eine weitere Alternative, aber noch nicht ganz ausgereifte Technik für Flachbildschirme sind OLED-Bildschirme, die gegenüber den beiden anderen genannten Flachbildschirmtechniken mehrere Vorteile (Blickwinkelunabhängigkeit, weiter reduzierter Stromverbrauch, schnellere Schaltzeiten, biegsam) mit sich bringen könnten. Des Weiteren sind auch flache Röhrenfernseher in der Entwicklung, die mit der sogenannten SED-Technik (Surface Conduction Electron Emitter Display) arbeiten. Jeder Bildpunkt bekommt dabei eine eigene kleine Röhre.

Für großformatige, kinoähnliche Abbildungen werden Videoprojektoren ("Beamer") verwendet. Ähnliche Geräte waren bereits 1936 bei den Olympischen Spielen im Einsatz.

Mit dem Internet entsteht eine völlig neue Verbreitungsmöglichkeit von bewegten Bildern und Ton ("siehe auch" IPTV), die in Konkurrenz zur klassischen Nutzungs- und Verbreitungsform des Fernsehens (siehe oben) treten könnte, aber deutlich weitergehende Möglichkeiten und Vorteile bietet. Dies könnte der klassischen Verbreitungsform von Fernsehen zunehmend das Publikum entziehen. Allerdings bleibt abzuwarten, ob und wie die Content-Produzenten dabei ihre Produktionskosten einfahren können und wie das klassische Fernsehen darauf reagieren wird. Bill Gates erwartet, dass in fünf Jahren oder eher, das herkömmliche Fernsehen zu großen Teilen vom Zuschauer per Einschaltquote durch Internetfernsehen ersetzt sein wird. Erste große Verdrängungsprozesse finden heute (1/07) bereits statt. Auch eine Analyse des Meinungsforschungsinstitutes Allensbach belegt, dass insbesondere bei jungen Menschen die Bedeutung des herkömmlichen Fernsehens zugunsten des Internets abnimmt.

In einer Studie, die von der EIAA (European Interactive Advertising Association) in Auftrag gegeben wurde, wurde festgestellt, dass bei deutschen Jugendlichen im Alter von 16 bis 24 Jahren das Internet häufiger als das Fernsehen genutzt wird. Eine 2014 veröffentlichte Studie der Stiftung für Zukunftsfragen – eine Initiative von British American Tobacco kommt zu dem gleichen Ergebnis. Demnach landet das Internet bei den beliebtesten Freizeitbeschäftigungen von Jugendlichen und jungen Erwachsenen auf Platz eins.

US-Amerikaner nutzen als Nachrichtenquellen vorwiegend (48 Prozent) das Internet, wogegen das Fernsehen nur noch von 29 Prozent als primäre Nachrichtenquelle genutzt wird. Die Management-Beratung Accenture stellte in einer Studie fest, dass junge TV-Zuschauer (16–24 Jahre) auf Videoplattformen abwandern. Heute (6/08) schauen bereits 40 Prozent dieser Altersgruppe regelmäßig Video im Internet. Als Hauptursache werden die starren Sendezeiten beim Fernsehen genannt. Mark Thompson, Chef der BBC, meinte auf der IFA 2008, dass die Zukunft des Fernsehens eindeutig on Demand sei und dass das Web vermutlich in Zukunft der wichtigste Vertriebsweg für Fernsehen sein wird. Das statistische Bundesamt hat festgestellt, dass im ersten Quartal 2008 38 Prozent mehr Menschen in Deutschland (14 Mio.) TV- und Hörfunk via Internet konsumiert haben als im Jahr zuvor.
Im Jahr 2011 sank in den USA erstmals die Zahl der in den Haushalten vorhandenen Fernsehgeräte gegenüber dem Vorjahr.

Der Chef von Netflix Reed Hastings hält im Mai 2015 klassisches Fernsehen für ein Auslaufmodell. Das traditionelle Fernsehen werde durch Streamingdienste im Internet ersetzt. Netflix hat weltweit starke Zuwachszahlen bei den Nutzern. Hastings führt das Wachstum der Nutzerzahlen auf die Popularität von Eigenproduktionen zurück.











</doc>
<doc id="11981" url="https://de.wikipedia.org/wiki?curid=11981" title="Antikörper">
Antikörper

Antikörper (Immunglobuline, im internationalen Sprachgebrauch auch Immunoglobulin, veraltet Gammaglobulin) sind Proteine (Eiweiße) aus der Klasse der Globuline, die in Wirbeltieren als Reaktion auf bestimmte Stoffe, so genannte Antigene, gebildet werden. Antikörper stehen im Dienste des Immunsystems. Antikörper werden von einer Klasse weißer Blutzellen, den B-Lymphozyten, produziert.

Als Antigene wirken fast ausschließlich Makromoleküle oder an Partikel gebundene Moleküle, zum Beispiel Lipopolysaccharide an der Oberfläche von Bakterien. Ein bestimmtes Antigen induziert in der Regel die Bildung nur weniger, ganz bestimmter, dazu passender Antikörper, die über spezifische, nicht-kovalente Bindung zumeist nur diesen Fremdstoff erkennen (dass auch verwandte Ziele erkannt werden können, macht man sich z. B. bei der Pockenschutzimpfung zunutze: Die vom Körper gegen die harmlosen Kuhpocken gebildeten Antikörper erkennen auch für Menschen pathogene Pockenviren). Die spezifische Bindung von Antikörpern an die Antigene bildet einen wesentlichen Teil der Abwehr gegen die eingedrungenen Fremdstoffe. Bei Krankheitserregern (Pathogenen) als Fremdstoffen kann die Bildung und Bindung von Antikörpern zur Immunität führen. Antikörper sind zentrale Bestandteile des Immunsystems höherer Wirbeltiere. 

Antikörper werden, wie 1948 von der schwedischen Immunologin Astrid Fagraeus erstmals beschrieben wurde, von einer Klasse weißer Blutzellen (Leukozyten) sezerniert, die als Effektorzellen beziehungsweise Plasmazellen bezeichnet werden und differenzierte B-Lymphozyten darstellen. Sie kommen im Blut und in der extrazellulären Flüssigkeit der Gewebe vor und „erkennen“ meist nicht die gesamte Struktur des Antigens, sondern nur einen Teil desselben, die sogenannte antigene Determinante (das Epitop). Die spezifische Antigenbindungsstelle des Antikörpers bezeichnet man als Paratop. Die Antikörper erzeugen beim Kontakt mit dem Antigen die sogenannte humorale Immunantwort.

Da einige Aminosäurereste Zuckerketten tragen, zählen Antikörper zu den Glykoproteinen.
Jeder Antikörper besteht aus zwei identischen schweren Ketten (engl. , H) und zwei identischen leichten Ketten (engl. , L), die durch kovalente Disulfidbrücken zu einer Ypsilon-förmigen Struktur miteinander verknüpft sind. Die leichten Ketten bestehen aus jeweils einer variablen und einer konstanten Domäne. Bezeichnet werden diese als V und C.
Die schweren Ketten hingegen haben jeweils eine variable und drei (IgG, IgA) bzw. vier (IgM, IgE) konstante Domänen. Bezeichnet werden diese analog als V und C1, C2, C3.

Die variablen Domänen einer leichten und einer schweren Kette bilden die Antigenbindungsstelle. Die Domäne C2 besteht u. a. auch aus einer Kohlenhydratkette, die eine Bindungsstelle für das Komplementsystem bildet. Die Domäne C3 ist die "Fc"-Rezeptor-Bindungsstelle zur Opsonierung. Die variablen Domänen bilden ihrerseits verschiedene charakteristische Paratope aus, die zusammen einen Idiotyp bilden.

Die beiden Leichtketten sind je nach Organismus und Immunglobulin-Subklasse entweder vom Typ κ oder λ und bilden zusammen mit dem oberhalb der Gelenkregion (engl. ) liegenden Anteil der schweren Ketten das antigenbindende Fragment "Fab" (engl. ), welches enzymatisch mit Hilfe von Papain von dem darunterliegenden kristallisierbaren Fragment "Fc" (engl. ) abgespaltet werden kann. Die ausgesprochene Variabilität der Antigenbindungsstellen (abgekürzt CDR, engl. ) erreicht der Organismus über die V(D)J-Rekombination.

Papain spaltet oberhalb der Disulfidbrücken beider schwerer Ketten zueinander. Man erhält zwei "Fab"-Fragmente und ein vollständiges Fragment "Fc". Pepsin hingegen spaltet unterhalb der Disulfidbrücken. Die Gelenkregion bleibt zwischen beiden Fab-Fragmenten erhalten. Man nennt dieses Fragment dann F(ab). Pepsin und Plasmin spalten auch das Fc-Fragment zwischen der zweiten und dritten Domäne des konstanten Teils der schweren Kette.

Antikörper binden mit ihrer A(ntigen)B(indungs)-Region „ihr“ Paratop "relativ" spezifisch, analog dem Schlüssel-Schloss-Prinzip. Es passiert jedoch nicht selten, dass, metaphorisch gesprochen, ein zweiter oder dritter Schlüssel existiert, der in das Antikörper-„Schloss“ passt, aufgrund der (zufällig) ähnlichen oder identischen Konfiguration des Epitops. Mit sehr geringer Wahrscheinlichkeit kann das auch eine körpereigene Struktur sein. Auf diesem Phänomen beruht ein Ansatz zur Erklärung von Autoimmunerkrankungen.

Die Bindung zwischen Epitop und Immunglobulin ist nicht-kovalent und unterliegt dem Massenwirkungsgesetz. Eine effektive Agglutination, das heißt eine Verklumpung durch Ausbildung großer Komplexe, ist daher nur bei etwa gleicher Anzahl von Epitopen und Bindungsstellen möglich. Bei großen Abweichungen nach oben oder unten bleiben die Komplexe in Lösung; trotzdem tritt meist eine Neutralisation der Wirkung der Antigene ein. Gegen mehrere Virusstämme wirksame neutralisierende Antikörper werden als "breitneutralisierende Antikörper" bezeichnet, z. B. Breitneutralisierende Anti-HIV-Antikörper.

Membranständige Antikörper (als B-Zell-Rezeptoren (BCR) bezeichnet) können B-Zellen aktivieren, wenn sie durch Antigene quervernetzt werden. Die B-Zelle nimmt daraufhin den Immunkomplex durch Endocytose auf, verdaut das Antigen proteolytisch und präsentiert über MHC-Klasse-II-Moleküle (Peptide mit 13–18 Aminosäuren) Fragmente davon auf ihrer Zelloberfläche. Wenn die präsentierten Fragmente auch (parallel auf anderen professionellen Antigen-präsentierenden Zellen oder auf ebendieser B-Zelle) von einer CD4-T-Zelle (T-Helferzellen) erkannt werden, stimuliert diese T-Zelle die B-Zelle, was weitere Reifungsprozesse (somatische Hypermutation, Klassenwechsel) und Proliferation zu Antikörper-sezernierenden Plasmazellen oder/und zu B-Gedächtniszellen auslöst. Diese Reifungsprozesse finden innerhalb von Keimzentren ("germinal center") in den sekundären lymphatischen Organen (Milz, Lymphknoten) statt und werden unter dem Begriff der Keimzentrumsreaktion ("germinal center reaction") zusammengefasst.

Der B-Zell-Rezeptor ist, mit Ausnahme eines kleinen Teils am Carboxylende der schweren Kette, mit dem Antikörper der jeweiligen B-Zelle identisch. Der B-Zell-Rezeptor besitzt dort eine hydrophobe, in der Zellmembran verankerte Sequenz, der Antikörper dagegen eine hydrophile Sequenz, die seine Sekretion bewirkt. Die beiden Formen entstehen durch alternative RNA-Prozessierung.

Sezernierte Antikörper wirken durch verschiedene Mechanismen:


Bei den meisten Wirbeltieren gibt es fünf verschiedene Klassen (Isotypen) von Immunglobulinen, die anhand ihrer unterschiedlichen Gen-Abschnitte für die konstanten Teile der schweren Kette eingeteilt werden. Darüber hinaus gibt es einige Klassen, die nur in einzelnen Tiergruppen zu finden sind. Die verschiedenen Isotypen kommen in verschiedenen Kompartimenten des Körpers vor und haben unterschiedliche Aufgaben.

Immunglobulin A (IgA) wird auf allen Schleimhäuten der Atemwege, der Augen, des Magen-Darm-Trakts, des Urogenitaltrakts sowie über spezielle Drüsen rund um die Brustwarze von Müttern sezerniert und schützt dort vor Pathogenen (auch das Neugeborene). Sezerniertes IgA kommt in Form von Homodimeren vor; die beiden Anteile sind durch das "Joining-Peptide" verbunden.

Immunglobulin D (IgD) wird durch alternatives Spleißen der IgM/IgD-Prä-mRNA zusammen mit IgM als B-Zell Rezeptor (BCR) auf reifen, naiven (antigenunerfahrenen) B-Zellen membranständig präsentiert. IgD ist nur in geringen Mengen in sezernierter Form in Blut und Lymphe vorhanden. Es wirkt als Antigenrezeptor bei der von Antigen stimulierten Vermehrung und bei der Differenzierung der B-Zellen.

Immunglobulin E (IgE) vermittelt den Schutz vor Parasiten, wie z. B. Würmern, und ist an Allergien beteiligt. Es wird durch Fc-Rezeptoren auf Mastzellen gebunden. Aus diesem Grund ist nahezu alles IgE membrangebunden, im Blut ist es praktisch nicht vorhanden. Bei Antigenkontakt wird es quervernetzt, was zur Ausschüttung von Histamin, Granzymen etc. durch die Mastzellen (Mastzelldegranulation – hier greifen Allergiemedikamente, die die Mastzellen „stabilisieren“) und Granulozyten führt (allergische Sofortreaktion). Die ausgeschütteten Mediatoren wirken stark gefäßerweiternd, was das Herankommen anderer Immunzellen erleichtert. 

Immunglobulin G (IgG) wird erst in einer verzögerten Abwehrphase (3 Wochen) gebildet und bleibt lange erhalten. Der Nachweis zeigt eine durchgemachte Infektion oder eine Impfung an. Ein Beispiel ist anti-Masern-IgG, gegen das Masernvirus gerichtete Antikörper der IgG-Klasse als Zeichen einer gegenwärtigen oder früheren Infektion oder Impfung. Die Rhesusfaktor-D-Antikörper sind ebenfalls von diesem Typ, was zu Komplikationen bei einer Schwangerschaft führen kann, da Immunglobulin G plazentagängig ist.

Krankheiten mit einem angeborenen oder erworbenen Mangel an Antikörpern betreffen oft IgG, siehe Antikörpermangel. Bildet der Körper gegen eigene Körperbestandteile Antikörper, so genannte Autoantikörper, spricht man von einer Autoimmunkrankheit. IgG wird außerdem aktiv über das Blut und die Plazenta(barriere) in den Fötus transportiert und sorgt dort auch nachgeburtlich für einen ersten Schutz vor Infektionen. Über zwei antigengebundene IgG wird das Komplementsystem aktiviert. Der Fc-Rezeptor vermittelt Phagozytose.

Immunglobuline sind nach intravenöser Verabreichung in der Blutbahn des Empfängers unmittelbar und vollständig bioverfügbar. Sie verteilen sich relativ rasch zwischen Plasma und extravaskulärer Flüssigkeit; nach etwa drei bis fünf Tagen wird ein Gleichgewicht zwischen intra- und extravaskulärem Kompartiment erreicht. Die In-vivo-Halbwertszeit von IgG bei Patienten mit primärem Antikörpermangelsyndrom beträgt 35 Tage. Die Halbwertszeit von IgG kann jedoch von Patient zu Patient variieren, vor allem bei Patienten mit primären Immunmangelsyndromen. Immunglobuline und IgG-Komplexe werden in den Zellen des mononukleären phagozytischen Systems abgebaut.

Immunglobulin G besitzt ein breites Antikörperspektrum gegen verschiedene infektiöse Erreger. Opsonisierung und Neutralisierung von Mikroben und Toxinen durch spezifische Antikörper wurden nachgewiesen. IgG-Antikörper werden aus Plasma von mindestens 1000 Spendern hergestellt; die Subklassenverteilung entspricht der des humanen Plasmas. Durch entsprechende Dosierungen können erniedrigte IgG-Serumspiegel auf Normalwerte angehoben werden. Der Wirkmechanismus bei anderen Anwendungsgebieten als der Substitutionstherapie ist noch nicht vollständig erforscht, schließt jedoch immunmodulatorische Wirkungen ein. Die Fertigprodukte sind auf einen schwach sauren pH-Wert eingestellt. Nach Verabreichung hoher Dosen von IgG wurde keine Veränderung des Blut-pH-Wertes gemessen. Die Osmolalität von Fertigarzneimitteln liegt nahe an den physiologischen Werten (285–295 mOsmol/kg).

Immunglobuline sind normale Bestandteile des menschlichen Körpers. Die akute Toxizität beim Tier ist nicht festzulegen, da das zu verabreichende Volumen oberhalb der tolerierbaren Grenze läge. Tierstudien über chronische Toxizität und Embryotoxizität sind nicht möglich, da diese durch die Bildung von Antikörpern gegen Humanproteine gestört werden. Klinische Erfahrungen haben keine Hinweise auf kanzerogene oder mutagene Effekte geliefert. Deswegen wurden experimentelle Untersuchungen am Tier nicht für notwendig erachtet.

Immunglobulin M (IgM) ist die Klasse von Antikörpern, die bei Erst-Kontakt mit Antigenen gebildet wird und zeigt die akute Infektionsphase einer Krankheit an, beispielsweise anti-Masern-IgM, gegen das Masernvirus gerichtete Antikörper der IgM-Klasse als Zeichen einer frischen Infektion.

IgM ist ein Pentamer (Multimer) aus fünf Protein-Untereinheiten. Auch diese Untereinheiten sind durch das cysteinreiche "Joining Peptide" verbunden. Da IgM 10 Bindungsstellen für Antigene hat, führen diese Antikörper zu einer starken Agglutination. Der Antigen-Antikörperkomplex von IgM-Pentameren aktiviert den klassischen Weg des Komplementsystems, weiterhin werden die AB0-Blutgruppen von IgM-Antikörpern erkannt.

Immunglobulin W (IgW) wurde erst 1996 in einer Haiart entdeckt. Aufgrund dessen wurde ursprünglich angenommen, dass es nur in Knorpelfischen vorkommt. 2003 wurde IgW jedoch auch in Lungenfischen, einer Klasse der Knochenfische, nachgewiesen. IgW besitzt wahrscheinlich einige Eigenschaften eines hypothetischen Ur-Immunglobulins und ist deshalb vor allem für die Forschung zur Evolution des Immunsystems von Interesse.

Immunglobulin Y (IgY) auch "Chicken IgG", "Egg Yolk IgG" oder "7S-IgG" genannt, ist in Hühnern das funktionelle Äquivalent zu IgG und ähnelt diesem in seiner Struktur. Es ist in hohen Konzentrationen in Hühnereiern zu finden. Für die Verwendung für bioanalytische Zwecke in Immunassays bietet IgY verschiedene Vorteile gegenüber IgG.

Aus Tieren gewonnene Antikörper (Antiseren) werden als Therapeutikum für verschiedenste Zwecke eingesetzt. Ein wichtiges Beispiel ist die Verwendung als passiver Impfstoff.

Intravenöse Immunglobuline (IVIG) sind zugelassen zur Substitutionsbehandlung bei verschiedenen angeborenen oder erworbenen Störungen der Antikörperbildung (z. B. bei chronisch lymphatischer Leukämie, Multiplem Myelom oder nach allogener hämatopoetischer Stammzellentransplantation) sowie zur Immunmodulation bei einigen Autoimmunerkrankungen (z. B. Immunthrombozytopenie, Guillain-Barré-Syndrom) und Erkrankungen unbekannter Ätiologie (z. B. Kawasaki-Syndrom). Die Querschnitts-Leitlinien der Bundesärztekammer zur Therapie mit Blutkomponenten und Plasmaderivaten erwähnen darüber hinaus die Off-Label-Anwendung in verschiedenen Indikationen. Zu den lange bekannten möglichen seltenen Nebenwirkungen von IVIG-Präparaten zählen reversible hämolytische Reaktionen. Diese werden vermutlich ausgelöst durch Antikörper gegen Blutgruppenantigene (Isoagglutinine), die in den IVIG-Präparaten enthalten sein können. Im März 2013 thematisierte die Arzneimittelkommission der deutschen Ärzteschaft (AkdÄ) in ihrer Drug Safety Mail Meldungen von schweren hämolytischen Reaktionen nach intravenöser Gabe von Immunglobulinen.

Außerdem werden spezifische monoklonale Antikörper seit neuestem in der Medizin therapeutisch eingesetzt. Hauptanwendungsgebiet ist die Hämatologie und Onkologie, daneben werden sie auch in der Behandlung von Autoimmunerkrankungen wie Multipler Sklerose, Rheumatoider Arthritis (RA) oder CIDP (chronische inflammatorische demyelisierende Polyneuropathie), AIDP (akute inflammatorische demyelisierende Polyneuropathie), MMN (Multifokale motorische Neuropathie) und verwandten neuromuskulären Erkrankungen eingesetzt. Hierbei erkennen diese Antikörper pro-inflammatorische Zytokine wie Interleukin-1 oder Lymphotoxin-α. Antikörper gegen den B-Zell-Oberflächenmarker CD20 erkennen zwar nur naive (antigenunerfahrene) und Gedächtnis-B-Zellen, jedoch keine Plasmazellen (CD20neg), dennoch ist auch diese Therapie relativ erfolgreich. Damit stellen Antikörper eine Medikamentenklasse dar, die erstmals in der Lage ist, spezifisch in die entzündlichen Vorgänge einzugreifen ("Biologicals").

In der Immunszintigrafie werden Antikörper dazu verwendet bestimmte Zielstrukturen, beispielsweise Tumorzellen, im Körper ausfindig zu machen. Dazu wird an den Antikörper ein meist sehr kurzlebiges Radionuklid gekoppelt. Mittels Szintigrafie oder Positronen-Emissions-Tomographie kann man dann feststellen, wo sich der Antikörper bzw. dessen Zielstruktur ("Target") genau befindet.

Früher war der konstante Teil der Antikörper noch murinen (aus der Maus) Ursprungs, was zu Abstoßungsreaktionen durch das Immunsystem führen konnte. Um dieses Problem zu umgehen, werden neuerdings sogenannte "humanisierte Antikörper" verwendet. Herkömmliche monoklonale Antikörper enthalten neben der die Spezifität gegen humane Antigene vermittelnden variablen Region immer noch Proteinbestandteile der Maus, die das menschliche Immunsystem möglicherweise als fremdartig abstößt. Mit Hilfe molekularbiologischer Verfahren werden deshalb die murinen Teile der konstanten Abschnitte entfernt und durch baugleiche konstante Teile menschlicher Antikörper ersetzt. Die konstanten Abschnitte der Antikörper spielen für die spezifische Bindung des monoklonalen Antikörpers keine Rolle. Der so entstandene monoklonale Antikörper wird als „humanisierter monoklonaler Antikörper“ bezeichnet und wird vom Immunsystem des Menschen nicht mehr abgestoßen. Humanisierte Antikörper werden in einer Kultur aus Hamster-Ovarialzellen hergestellt, weshalb ihre Produktion sehr viel aufwändiger und deshalb auch teurer als die Produktion in Mikroorganismen ist.

Bei der Radioimmuntherapie ist eine ionisierende Strahlungsquelle mit möglichst kurzer Reichweite an einen Antikörper gekoppelt. Zum Einsatz kommen hier vor allem kurzlebige Beta-, seltener Alphastrahler, mit kurzer Reichweite im Gewebe.

Bei der Impfstoffentwicklung gegen variable Pathogene werden "breitneutralisierende Antikörper" (bnAb) untersucht, die gegen mehrere Stämme eines Virus wirksam sind, beispielsweise breitneutralisierende Anti-IAV-Antikörper und breitneutralisierende Anti-HIV-Antikörper.

Die hohe Spezifität, mit der Antikörper ihr Antigen erkennen, macht man sich in der Biologie zu Nutze, um das Antigen, in den allermeisten Fällen ein Protein, sichtbar zu machen. Die Antikörper sind entweder direkt mit einem Enzym (setzt ein Substrat in Farbe oder Chemolumineszenz um), mit Fluoreszenzfarbstoffen oder mit radioaktiven Isotopen gekoppelt (gelabelt) oder werden mit einem Sekundärantikörper, der an den ersten (Primärantikörper) bindet und entsprechend gelabelt ist, nachgewiesen.


Ein monoklonaler Antikörper ist gegen "genau ein" spezifisches Epitop eines Antigens gerichtet. Zunächst müssen, wie bei der polyklonalen Antikörperherstellung beschrieben, Tiere immunisiert und dann deren Plasmazellen (aus Milz oder Lymphknoten) gewonnen werden. Da die Plasmazellen die Fähigkeit zur Zellteilung verloren haben, muss zuerst eine Verschmelzung mit Tumorzellen erfolgen. Die so entstandenen Zellhybriden (Hybridom-Technik) erhalten von den Plasmazellen die Eigenschaft, einen bestimmten Antikörper zu produzieren und zu sezernieren und von der Tumorzelle die Fähigkeit, in Kultur sich theoretisch unendlich oft teilen zu können und somit theoretisch unendlich lange zu leben. Durch mehrfaches Vereinzeln (Klonieren) wird ein Stamm von Zellen gewonnen, der auf eine einzelne Hybridoma-Zelle und somit auf eine einzelne Plasma-Zelle zurückgeht.

Die so erhaltenen Zelllinien können nun in Kultur unendlich stark expandiert werden und damit auch theoretisch unendlich große Mengen Antikörper produzieren. Da alle Zellen auf eine einzige Zelle zurückzuführen sind, handelt es sich bei allen Zellen einer Kultur um identische Kopien ein und derselben Zelle. Aufgrund dessen produzieren auch alle Zellen einen bestimmten, identischen Antikörper, der sich hinsichtlich seiner Eigenschaften (z. B. Bindungsstelle am Antigen, Stärke der Bindung etc.) genau definieren lässt und in theoretisch unbegrenzter Menge herstellbar ist.

Rekombinante Antikörper werden in vitro hergestellt, das heißt ohne Versuchstier. Rekombinante Antikörper werden typischerweise aus Genbibliotheken hergestellt, die für die Herstellung der Antikörper in Mikroorganismen geeignet sind. Die Auswahl des richtigen (=spezifisch bindenden Antikörpers) erfolgt dabei nicht durch das Immunsystem eines Tieres/Menschen, sondern durch einen Bindungsschritt im Reagenzglas. Rekombinante Antikörper können auf vielfältige Weise angewendet werden, da sie einfach verändert werden können, denn ihre Erbsubstanz ist bekannt. So kann ihre Bindungsstärke oder Stabilität verbessert werden, oder es können Eiweiße mit anderen Funktionen angehängt werden, z. B. zur Erzeugung von bispezifischen Antikörpern oder Immuntoxinen.

Polyklonale Antiseren sind eine Mischung aus verschiedenen gegen diverse Epitope gerichteten Antikörpern. Zunächst muss das Antigen, gegen das der Antikörper gerichtet sein soll, ausgewählt und produziert werden. Dies kann auf verschiedene Weisen erreicht werden, zum Beispiel, indem ein Protein isoliert, ein Peptid "in vitro" synthetisiert oder das Protein als ganzes rekombinant in Bakterien hergestellt wird. Anschließend wird das Protein einem Tier eingespritzt, dessen Immunsystem dann Antikörper gegen das Protein bildet. Dieser Vorgang heißt „Immunisierung“. Als Antikörper-Produzenten werden besonders Mäuse, Ratten und Kaninchen, aber auch Ziegen, Schafe und Pferde verwendet. Die Immunisierung wird mehrfach wiederholt. Nach ein paar Wochen kann das polyklonale Antiserum entnommen werden. Darin sind verschiedene durch die Immunisierung gebildete, gegen das Antigen gerichtete Antikörper enthalten, die sich im erkannten Epitop unterscheiden können.

Als "Hypogammaglobulinämie" wird der Mangel an Antikörpern und als "Agammaglobulinämie" ihr völliges Fehlen bezeichnet. Ein Zuviel an Antikörpern bezeichnet man als "Hypergammaglobulinämie".
Die Diagnose einer Störung der Antikörper wird in der Regel durch die Eiweißelektrophorese des Blutserums gestellt. Eventuell muss diese noch durch eine Immunelektrophorese ergänzt werden.

Ursachen einer ausgeprägteren "Hypo"gammaglobulinämie können u. a. sein:


Ursachen einer ausgeprägteren "Hyper"gammaglobulinämie können u. a. sein:


Durch Proteindesign wurden verschiedene Derivate von Antikörpern mit teilweise veränderten Eigenschaften erzeugt, z. B. F(ab)2-Fragmente, Fab-Fragmente, scFv-Fragmente, Einzeldomänenantikörper oder Mikroantikörper.



</doc>
<doc id="11982" url="https://de.wikipedia.org/wiki?curid=11982" title="Antennenfernsehen">
Antennenfernsehen

Als Antennenfernsehen, auch terrestrisches Fernsehen, bezeichnet man den Fernsehempfang eines auf der Erde befindlichen Fernsehsenders über eine Hausantenne oder eine Zimmerantenne. Dabei werden die Inhalte vom Sender als modulierte elektromagnetische Wellen ausgestrahlt und mehr oder weniger geradlinig auf Haus- oder Zimmerantennen übertragen.

Die Übertragungstechnik kann von analoger oder digitaler Art sein.

Die verwendeten Sendefrequenzen liegen im UKW- oder Dezimeterwellenbereich; somit breiten sie sich – ähnlich wie Licht – nahezu geradlinig aus und können der Erdkrümmung kaum folgen, was die Senderreichweiten stark beschränkt; hinzu kommen Abschattungen durch landschaftliche Gegebenheiten und Gebäude sowie Wettereinflüsse. An flächigen Hindernissen in der Nähe der Empfangsantenne können auch Reflexionen entstehen, die die direkte Einstrahlung überlagern; bei analoger Übertragung wirken sie störend („Geisterbilder“), bei digitaler Übertragung dagegen kann die Übertragungstechnik sich diese sogar zunutze machen, um die Empfangsqualität zu steigern.

Im Allgemeinen ist für einen wetterfesten, flächendeckenden Empfang neben den starken Grundnetzsendern eine relativ große Anzahl von Füllsendern notwendig (unter anderem zur „Ausleuchtung“ von Tälern); dementsprechend teuer ist diese Technik. Ferner sind bei analoger Übertragung nur 60 Sendefrequenzen verfügbar, von denen jeder Grundnetz- und Füllsender eines größeren Gebietes meist eine eigene benötigt, damit keine gegenseitigen Störungen auftreten. Daher können an einem gegebenen Ort nur maximal knapp ein Dutzend Programme mit analoger Technik empfangbar sein. Günstiger verhält es sich beim digitalen Antennenfernsehsystem DVB-T.

Um die Reichweite zu vergrößern, werden die Sender meist auf Bergen oder/und Fernsehtürmen beziehungsweise Sendemasten angebracht. Das Fernsehsignal eines Programms wird dabei überregional über eine Richtfunkverbindung oder über Satellit dorthin übertragen. In Ballungsräumen oder auf hohen Bergen wird dann das Signal mit sehr hoher Leistung ausgesendet. Um entfernte Gebiete oder Täler versorgen zu können, sind sogenannte Fernsehumsetzer aufgestellt, die das Signal eines Fernsehturmes, eines Satelliten oder eines anderen Umsetzers empfangen und es auf einem anderen Kanal verstärkt wieder aussenden.

Das Bildhelligkeitssignal des analogen Antennenfernsehens wird mit einer Variante der Amplitudenmodulation, der Restseitenbandmodulation, moduliert; für den Farbhilfsträger wird im PAL- und NTSC-System Quadraturamplitudenmodulation und im SECAM-System Frequenzmodulation verwendet, das Tonsignal ist meist frequenzmoduliert, im CCIR-System L amplitudenmoduliert.

Digitales Antennenfernsehen wird im Jahr 2014 in Europa und weiten Teilen von Asien im Standard DVB-T und dem dazu nicht kompatiblen Nachfolgestandard DVB-T2 ausgestrahlt. In anderen Regionen kommen weitere, zueinander inkompatible digitale Verfahren zu Anwendung. Beispielsweise ATSC in Nordamerika, ISDB in Japan und Südamerika. Das Verfahren DTMB findet in China Anwendung.

In Deutschland, Österreich und der Schweiz sowie in vielen anderen Ländern wurde oder wird auf digitale Übertragung der Fernsehprogramme umgestellt. Dabei ist zu einem bestimmten Stichtag die Ausstrahlung des analogen Signals regional abgeschaltet und die Ausstrahlung des DVB-T-Signals eingerichtet worden. In Deutschland ist die analoge Verbreitung des terrestrischen öffentlich-rechtlichen Fernsehens seit 2009 vollständig abgeschaltet. Ursprünglich sollte bis 2012 das analoge Antennen-Fernsehen europaweit der Vergangenheit angehören (siehe dazu "Umstellung auf DVB-T" und "Analogabschaltung"). Dieses Ziel konnte jedoch nicht erreicht werden – so wurde in Moldawien die digitale Übertragung erst im November 2016 (parallel zur analogen) eingeführt.

Andererseits gibt es Überlegungen, reguläres terrestrisches Fernsehen (auch das DVB-T) durch Web-TV zu ersetzen, beispielsweise ab 2020. Seit der kommerziellen Einführung von DVB-T am 23. Februar 2003 im Raum Berlin sind noch 20 %, etwa die Hälfte nur darüber, von terrestrisch ausgestrahlten digitalem Fernsehen abhängig. Bei einer Kalkulation von 242 Minuten TV, 191 Minuten Radio, 23 Minuten Lesen von Tageszeitungen und sechs Minuten in Zeitschriften, bei 22 Minuten für Bücher und 83 Minuten Internet jedes Erwachsenen am Tag würde das Webstreaming 96 GByte erfordern. Ohne DVB-T kämen für die 1,8 Millionen Berliner Fernsehhaushalte, wovon 408.000 DVB-T benutzen, davon 264.000 ausschließlich, erhebliche Änderungen im Zugang. 182.000 Haushalte besitzen bereits einen Breitband-Internet-Anschluss, jedoch 82.000 nutzen diesen nicht und grundsätzlich fallen etwa 49.000 als Nutzer wegen Alters- oder Geldgründen aus. Selbst wenn insgesamt 280.000 Endgeräte vom Breitbandnetz versorgt werden, wären für das web-TV bei einem 2-Mbit/s-Streaming 560 Gbit/s nötig. Zudem sind 55 % der deutschen TV-Programme zu Beginn der 2010er Jahre schon nicht mehr kostenfrei. Der Einsatz von DVB-T2, das zeitweise einen Parallelbetrieb erfordert, entstehen für die Netzbetreiber deutschlandweit Kosten von jährlich 20 Millionen Euro, die letztlich vom Kunden zu tragen wären. Druck entsteht auch durch die Bundesnetzagentur, die die TV-Frequenzen im 700-MHz-Bereich (694 MHz bis 790 MHz) für Mobilfunk umnutzen will.





</doc>
<doc id="11983" url="https://de.wikipedia.org/wiki?curid=11983" title="Kabelfernsehen">
Kabelfernsehen

Beim Kabelfernsehen werden Bild- und Tonsignale, insbesondere Rundfunkprogramme, über ein Koaxialkabel (Breitbandkabel) zu den Endkunden übertragen.

Zum Empfang benötigt man ein Empfangsgerät, das die im Kabel unterstützte Übertragungsnorm unterstützt. Früher war das PAL B/G, heutzutage ist das meist DVB-C.
Das Empfangsgerät kann im Fernsehgerät eingebaut sein, ein Beistellgerät sein oder eine Einsteckkarte/USB-Stick für den PC sein. Das Empfangsgerät wird mit einem geeigneten Kabel mit der Antennendose verbunden (meist Koaxialkabel RG-59).

Der prinzipielle Aufbau sieht folgendermaßen aus:

Diese Antennensteckdose wird gewöhnlich durch einen von dem Kabelfernsehnetzbetreiber beauftragten Techniker in der Wohnung installiert oder ist bereits in der Wohnung als ein immer fester Bestandteil des Mietobjektes vorhanden.

Im Unterschied zum Antennen- und Satellitenfernsehen muss man für das Kabelfernsehen in der Regel Anschlussgebühren und für Spartenprogramme meist zusätzliche monatliche Abonnements-Gebühren bezahlen. Unabhängig davon ist die Rundfunkgebühr zu entrichten (Deutschland: ARD ZDF Deutschlandradio Beitragsservice (bis 2012: GEZ); Österreich: GIS; Schweiz: Billag).

Neben dem analogen Fernsehsignal wird auch ein analoges UKW-Hörfunkangebot im Band II über das Kabel übertragen. In den bis mindestens 450 MHz ausgebauten Netzgebieten wird im sogenannten Hyperband (Sonderkanäle beginnend mit S21) auch ein digitales Angebot ("siehe auch" Angebot von DVB-C) in Form digitaler Senderpakete beziehungsweise Bouquets bereitgestellt. Das digitale Fernsehen ist dabei, die alte analoge Übertragungsart abzulösen (siehe Analogabschaltung). Zum Empfang der in den digitalen Angeboten übertragenen Fernseh- und Hörfunkprogramme wird ein Digitalreceiver benötigt – oft eine Set-Top-Box, bei heutigen Fernsehgeräten ist der Empfänger meist bereits eingebaut (siehe IDTV) –, der die im DVB-C-Standard codierten Signale für die Wiedergabe umsetzt. Zum Empfang von Bezahlfernsehen wie Sky, den Privatsendern allgemein bei einigen Anbietern, den HDTV-Varianten vieler Sender, Kabel Digital Home oder KabelKiosk wird zusätzlich eine Dekoder-Karte benötigt.

Jedoch ist das DVB-T und DVB-T2 durch seine Technik und in Bezug auf die Anzahl der Sender in einigen Städten eine kostengünstige Alternative zum Kabelfernsehen geworden und viele Empfänger unterstützen aufgrund der technischen Ähnlichkeit ohnehin beide DVB-Standards. Daher sind die Kabelfernsehanbieter verstärkt dazu gezwungen, ihre Netze für den Kunden durch die Digitalisierung mit dem gleichzeitigen Ausbau ihres Angebotes attraktiver zu machen. Beim Kabelfernsehen werden die digitalen DVB-T-Signale zentral in der Kopfstation in analoge Fernsehsignale – meist in PAL – oder in DVB-C-Signale umgesetzt. 

Technisch ist die Übertragung von DVB-T und DAB-Radio über ein Kabelfernsehnetz problemlos möglich. Jedoch wird diese Option in der Regel nur bei Gemeinschaftsantennenanlagen aus Kostengründen genutzt.

Bei Fernsehgeräten, die nur beim DVB-T-Empfang eine Versorgungsspannung von 5 V an der Antennendose für einen Antennenverstärker zur Verfügung stellen können, ist es mittels eines zusätzlichen automatischen Umschalters (Relais) möglich, wahlweise Kabelfernsehen oder DVB-T über eine zusätzlich vorhandene DVB-T-Antenne zu empfangen.

In Deutschland wurden häufig von großen Kabelfernsehanbietern UKW-Radio, ARD, ZDF und die lokalen Fernsehsender über analoges Kabelfernsehen als kleine Grundversorgung sehr preisgünstig auf Nachfrage angeboten. Hier wird mit Hilfe von (Sperr-)Filtern, die sich im Verteilsystem oder in der Kabeldose befinden, die Bandbreite des Kabelanschlusses technisch begrenzt und damit die zusätzlichen Angebote ausgesperrt.

Das Kabelfernsehen kann auch Rundfunksender verbreiten. 

Im Frequenzbereich von 87,5 MHz bis 108 MHz (VHF-Band II, in einigen Ländern abweichend, siehe OIRT-Band) wird auch ein analoges UKW-Radioangebot im Kabel übertragen. Zum Empfang dieser Radiosender kann jeder handelsübliche UKW-Radioempfänger mit einem 75 Ohm-Antennenanschluss (gemäß IEC 60169-2) für externe UKW-Antenne genutzt werden. Es ist nur eine Verbindung des Antennenanschlusses am UKW-Empfänger mit der entsprechenden Anschlussbuchse an der Kabelanschlussdose mit einem Antennenkabel erforderlich. Die Anschlüsse des Antennenkabels sind die gleichen wie bei dem Fernseh-Antennenkabel. Durch die Umkehrung von Stecker und Kupplung bei den Anschlüssen ist die Verlegerichtung zum Fernsehkabel umgekehrt. Die gleichen Kabeltypen wie beim Fernsehempfang können auch für den Rundfunkempfang verwendet werden. Einige ältere UKW-Empfänger, die statt eines koaxialen Antennenanschlusses einen Anschluss für das symmetrische 240 Ohm-Bandkabel haben, benötigen zusätzlich noch ein sogenanntes Empfänger-Symmetrierglied oder eine Radioantennenweiche mit einem integrierten Symmetrierglied für den symmetrischen UKW-Antennenanschluss.

Neben dem analogen Programmangebot über UKW ist auch die digitale Übertragung von Radioprogrammen über DVB-C in hoher Qualität möglich. Da die Übertragung mittels DVB-C auch verschlüsselt möglich ist, kann DVB-C für die Übertragung von zusätzlichen kostenpflichtigen Radioprogrammen (Pay-Radio) verwendet werden, was von "Sky" (eingeschränkt) genutzt wurde und von vielen Kabelnetzbetreibern genutzt wird. Die unverschlüsselte Einspeisung aller Radioprogramme der ARD über DVB-C erfolgte, obwohl mit geringem technischen Aufwand möglich, lange Zeit nur durch die wenigsten Kabelnetzbetreiber. Seit Sommer 2008 werden alle öffentlich-rechtlichen Radiosender digital und unverschlüsselt im digitalen Kabelnetz von Kabel Deutschland übertragen. Inzwischen sind digitale Radioprogramme bei allen größeren Netzbetreibern, wie Kabel Deutschland oder Unitymedia, verfügbar.

Das Kabelfernsehen wurde in letzter Zeit schrittweise durch die Einführung von digitalen Zusatzdiensten (Internet, Telefonie usw.) und die Umstellung von der analogen auf die digitale Bildübertragungsverfahren digitalisiert. Dabei werden analoge und digitale Signale parallel über das Kabel übertragen. Irgendwann, wenn möglichst alle Kunden umgestiegen sind, wird das analoge Signal abgeschaltet. Über Satellit und Antenne wird in Deutschland, Österreich und der Schweiz sowie den meisten Ländern weltweit nur noch digital ausgestrahlt (siehe Analogabschaltung). Durch den Einsatz von digitalen Datenübertragungsverfahren wie DVB-C und DVB-C2 bzw. HD lässt sich die aus technischen Gründen begrenzte Bandbreite im Kabelfernsehen wirtschaftlicher nutzen, weil durch diese digitale Übertragungsverfahren die Bildqualität gesteigert werden kann oder mehr Sender im vorhandenen Frequenzspektrum gleichzeitig übertragen werden können. In der Regel sind die verschiedenen analogen und digitalen Angebote je nach Vertragsverhältnis und Anbieter verfügbar. Heute werden viele Programme von digitalen Übertragungswegen, als digitale Signalzuführung in den Kopfstationen des Kabelfernsehens noch in analoge Rundfunksignale für das Kabelnetz umgesetzt.

Der Empfang des digitalen Kabelfernsehens setzt einen DVB-C-Tuner für ein Fernsehgerät voraus. Heutige Fernsehgeräte haben digitale Tuner für DVB-C, DVB-S und DVB-T eingebaut, sowie sog. CI-Interfaces zum Einstecken von Karten zum Entschlüsseln von Bezahlsendern. Ältere Fernsehgeräte benötigen eine sog. Set-Top-Box (STB), auch als Digitalreceiver oder Kabelbox bekannt. Mit einem DVB-C-Tuner können meist auch digitale Radioprogramme, die meist ebenfalls mit DVB-C übertragen werden, empfangen werden.

Das in Deutschland bekannteste Gerät für das digitale Fernsehen war die d-box, die es auch für Kabelfernsehen gab. Da dieses Gerät nicht mehr weiterentwickelt wurde (Produktion wurde 2001 eingestellt), haben die regionalen Kabelfernsehbetreiber weiter entwickelte Set-Top-Boxen ihren Kunden (je nach Vertragsart auch subventioniert) angeboten. Diese Geräte müssen aber nicht für Sky geeignet sein. Jeder Verbraucher kann auch eigenständig einen für DVB-C geeigneten Digitalreceiver im Fachhandel erwerben und anschließen.

Analog wird in Deutschland kein HDTV-Signal verbreitet. Ältere Fernsehgeräte sind zwar oft HDTV-fähig, verfügen aber oft nur über einen Analogempfänger. Erst mit einer für digitalen, für HDTV ausgelegten Set-Top-Box kann mit diesen alten Geräten ein HDTV-Empfang erfolgen.

Viele digitale Programme (Bezahlfernsehen) werden verschlüsselt im Kabelfernsehen übertragen und sind nur nach einer zusätzlichen Freischaltung durch sogenannte Smartcards mittels kostenpflichtiger Zusatzverträge empfangbar. Um verschlüsselte Programme freizuschalten, muss in den Kartenleser die passende Smartcard des Anbieters eingesteckt werden.

Sogenannte digitale Fernsehgeräte können die DVB-Signale heute direkt empfangen und benötigen keine separate Set-Top-Box mehr. Diese Fernsehgeräte sind heute Standard im Handel.

Eine Set-Top-Box kann gemeinsam mit einer Smartcard in der Regel nur ein Fernsehprogramm dekodieren, so dass beim Einsatz von digitalen Videorekordern häufig eine weitere Smartcard und eventuell CI-Interface für den Decoder erforderlich ist, um verschlüsselte Programme aufzunehmen.

Eine automatische Verbesserung der Bildqualität ist bei der Umstellung von analogem zu digitalem Kabelfernsehen nicht gegeben. Jedoch wird sie in der Regel von den Sendern und den Kabelnetzbetreibern angestrebt. Die Anzahl der digital verfügbaren DVB-C-Sender kann sich bei der Umstellung auf das Fünf- bis Zehnfache erhöhen.

Bei einigen deutschen Kabelnetzbetreibern erfolgte die Aussendung der privaten Fernsehprogramme bis April bzw. Mai 2013 mit einer Grundverschlüsselung. Die Grundverschlüsselung wurde jedoch Anfang des Jahres 2013 vom Bundeskartellamt verboten und die RTL- und ProSiebenSAT.1-Gruppe mit einem Bußgeld in Höhe von 55 Millionen Euro wegen illegaler Absprachen belegt. Seit 2. Mai 2013 kann man jetzt bis zu 51 Programme über den einfachen digitalen Kabelanschluss von Kabel Deutschland empfangen. Darunter die Programme der RTL- und der ProSiebenSAT.1-Gruppe sowie sixx, Tele 5, ServusTV, Sport1, Eurosport und noch weitere Programme. Die Grundverschlüsselung wurde zunächst für die nächsten zehn Jahre aufgehoben.
Früher benötigte man eine spezielle Smartcard des Kabelnetzbetreibers. Die öffentlich-rechtlichen Sender haben diese Grundverschlüsselung untersagt. Kabel BW hat von Anfang an und seit 2. Januar 2013 auch Unitymedia sowie viele kleinere Kabelnetzbetreiber auf eine Grundverschlüsselung verzichtet.

Beim Kabelfernsehen findet die Umstellung mit DVB-C statt. Die Digitalisierung beim Kabelfernsehen soll vorerst marktgetrieben stattfinden. 

Seit Juli 2015 wird im Kabelnetz von Unitymedia nach und nach die analoge Verbreitung eingestellt: Am 1. und 6. Juni 2017 wurde das analoge Signal in Baden-Württemberg komplett abgeschaltet, am 13. Juni folgte Hessen sowie am 20. und 26. Juni Nordrhein-Westfalen. 

Der ANGA Verband Deutscher Kabelnetzbetreiber teilte mit, dass spätestens Ende 2018 die analoge Fernsehverbreitung in den deutschen Kabelnetzen eingestellt wird. Der Betrieb und die Kanalbelegung beim Kabelfernsehen wird in Deutschland durch die Landesmedienanstalt des jeweiligen Bundeslandes genehmigt.

Im Jahr 2016 empfingen etwa 82,1 Prozent aller Haushalte mit Kabelempfang das Fernsehsignal digital. Die Steigerung beträgt jährlich 7 Prozent (Austausch von alten Fernsehgeräten gegen neue). Fast alle neuen Fernsehgeräte verfügen heute über DVB-C-Tuner, um das digitale Signal im Kabel zu empfangen. So kommt es, dass etwa die Hälfte der Haushalte, die heute noch analoges Fernsehsignal über Kabel empfängt, tatsächlich schon ein Fernsehgerät mit digitalem Empfänger besitzt.

Insbesondere viele ältere Menschen und viele einkommensschwache Haushalte empfangen immer noch analoges Kabelfernsehen über ältere Geräte, durch veraltete Kanallisten in ihren Geräten oder durch veraltete Hausverteilanlagen. Für Haushalte mit rein analogen Fernsehgeräten könnte eine von den Kabelgesellschaften kostenlos bereitgestellte digitale Set-Top-Box einen Umstieg bewirken.

Auch der UKW-Rundfunk ist von der Analogabschaltung in den Kabelfernsehnetzen betroffen.

Moderne Kabelnetze sind in der Regel bidirektionale Netze, mit deren Hilfe eine Rückkanalfähigkeit gegeben ist. Dadurch lassen sich auch Mehrwertdienste und interaktive Dienste nutzen, wie zum Beispiel Breitband-Internetzugang mittels Kabelmodem über das Fernsehkabel, sofern sie vom Kabelnetzbetreiber angeboten werden. Darüber hinaus ist das Telefonieren über das Kabelnetz in Ausbaugebieten möglich.

In vielen Fällen wurde die Rückkanalfähigkeit erst in den letzten Jahren nachgerüstet, nachdem das Kabelnetz ursprünglich als reines Verteilmedium für Radio- und Fernsehsignale konzipiert worden war. Für die Umstellung auf Rückkanalfähigkeit war es insbesondere erforderlich, die alten Verstärker in den oft am Straßenrand befindlichen Verteilerkästen auszutauschen. Die neuen Verstärker unterstützen zudem auch ein erweitertes Frequenzspektrum von bis zu 862 MHz, wodurch im Kabelnetz zusätzliche Kapazitäten entstehen. Wird die Rückkanalfähigkeit nachgerüstet, werden oft nur diejenigen Wohnungen mit einer geeigneten Antennenanschlussdose (dritter Anschluss für den Breitband-Internetzugang) ausgestattet, die den Internetzugang von Anfang an nutzen. Für herkömmliche Dosen gibt es deshalb Aufsteckadapter. Sie enthalten einen Filter, der das Datensignal aus dem vorhandenen Signal extrahiert und es über einen dritten Anschluss zur Verfügung stellt.

Ein häufig verwendetes Protokoll für die Anbindung des Internet über das Fernsehkabel ist DOCSIS, das in der neueren Spezifikation 3.1 Datenraten bis zu 10 Gbit/s im Downstream und 1 Gbit/s im Upstream und Datenverkehr in Echtzeit in Zukunft ermöglicht. Für private Endkunden stehen solche Datenraten noch nicht zur Verfügung. Üblich sind derzeit Geschwindigkeiten mit DOCSIS 3.0 bis maximal 400 Mbit/s. Der nächste Entwicklungsschritt findet mit diesen Datenraten statt und soll nach Angabe des Portals Teltarif die Verschmelzung von Fernsehen, Internet und Multimediadiensten in ausgewählten deutschen Großstädten ermöglichen. Für die gleichzeitige Nutzung von Fernsehen/Radio, Internet und Telefon, das sogenannte Triple Play, genügen bereits Datenraten ab ca. 2000 kbit/s, allerdings kann IPTV mit fernsehbildähnlicher Auflösung höhere Datenraten erfordern.

Die angebotenen Telefoniedienste sind ausschließlich Voice over Cable also VoIP-Anschlüsse (Internettelefonie). Die verwendeten Kabelmodems werden von den meisten Anbietern als Mietgeräte zur Verfügung gestellt und beinhalten einen integrierten IP-Adapter sowie für den SIP-Standard bei den Fritzboxen von AVM. Sie ermöglichen den Anschluss eines herkömmlichen Analogtelefons am Kabelmodem. Die Modemmodelle werden bereits fest vom Anbieter vorkonfiguriert. Telefonie über alternative VoIP-Anbieter erfordert in diesem Fall den Anschluss eines externen SIP-Adapters des Kunden oder die Nutzung alternativer Dienste wie Skype.

Gateways des Internetdienstanbieters verbinden den Telefonkunden über das Internet mit dem Festnetz der anderen Telefongesellschaften. Die Identifikation des Teilnehmers erfolgt über die MAC-Adresse des IP-Adapters. Bei internetbasierenden Telefoniediensten und Skype kann es generell durch Paketverluste zu Störungen kommen. Auch bei der Nutzung von analogen Datendiensten mittels Modem und Fax über diese Anschlüsse kann es je nach verwendetem Übertragungsprotokoll zu Störungen kommen. (Siehe dazu VoIP, Fax über IP (Fax over IP, FoIP)).

HD-Telefonie mit erweitertem Frequenzspektrum wird bisher von keinem namhaften Kabelnetzbetreiber angeboten, ist aber über die reguläre Internetnutzung mit alternativen Telefonieanbietern möglich. Beschränkt wird diese Möglichkeit nicht durch das Kabelfernsehen, sondern durch die Voraussetzung, dass die gesamte Übertragungskette zwischen den Teilnehmern durchgängig (G.722-kompatibel) ausgebaut sein muss.

Cable TV Fiber Optic (CATV FO) ist eine Glasfaserlösung für die Übertragung von digitalen Fernsehsignalen über Ethernet/LAN (Local Area Network). Dieses Verfahren zählt zu den internetbasierten (IP) Übertragungsverfahren, die unabhängig vom klassischen Kabelfernsehen existieren können. Mit diesem System können die Signale im Glasfasernetz ohne zusätzliche Verstärker über längere Strecken übertragen werden und so weitläufige Regionen digital erschlossen werden, um sie nach einer Umsetzung mit Kabelfernsehen und anderen Medien zu versorgen.

Einsatzgebiete sind Gebäudekomplexe und größere Unternehmensstandorte mit strukturierter Verkabelung; beispielsweise Firmen- und Gewerbeparks, Freizeitanlagen, Hotels, Logistikzentren, Raffinerien, Häfen, Bahnhöfe oder moderne Wohnanlagen. Mit einzelnen Produkten können Fernsehsignale über Distanzen bis 2000 Meter und an bis zu 64 Empfänger verteilt werden (FTTH). Die Multimedialösung ist in der strukturierten Gebäudeverkabelung (strukturierte Verkabelung) als Campus-, Gebäude- sowie Etagenverteiler einsetzbar und kann in bestehende Netzwerke integriert werden.

Diese Technik ist für alle digitalen Übertragungsverfahren, besonders jedoch für IPTV und Webradio geeignet. Zusätzlich lässt sie sich auch für DVB-C und nach einer Digitalisierung auch für analoge Radio- und Fernsehsignale einsetzen. Damit jedoch auch klassische Rundfunkempfänger, die ihre Signale mittels einer Antennendose erhalten und nur über DVB-C oder analog Kabelfernsehen empfangen werden können, versorgt werden können, sind in der Kopfstelle Umsetzer für die Signale von "Cable TV Fiber Optic" auf die konventionellen Systeme erforderlich. Aufgrund der möglichen hohen digitalen Übertragungsqualität kann diese Übertragungstechnik zur Zuführung von Signalen in die Kopfstelle des Kabelnetzanbieters verwendet werden.

Eine weitere neue Möglichkeit ist die Sat-over-IP-Technik zur Übertragung und Verteilung von DVB-S und auch von DVB-C-Fernsehsignalen als IP-Datenstrom. Auch mit Hilfe von Smart-TVs und HDMI-Sticks stehen heute weitere internetbasierende Empfangsmöglichkeiten zur Verfügung.

Rundfunkprogramme und Multimediadienste können mit unterschiedlichen Protokollen über das Internet übertragen werden. Diese auf dem Internet basierenden Dienste sind technisch unabhängig vom gewählten Übertragungsmedium (Telefon-/ Fernsehkabel oder Mobilfunk).
Während Webradio auf Grund seiner relativ geringen Datenrate heute kaum noch Probleme verursacht, sind hochauflösende unterbrechungsfreie Video-Streams (IPTV) in der Regel nur mit entsprechend leistungsfähigen Anschlüssen möglich.
Die Durchleitung durch ein anbieterkontrolliertes geschütztes Netzwerk unterscheidet jedoch die IPTV-Dienste, die vor allem Kabel- und DSL-Anbieter vermarkten, vom frei verfügbaren Internetfernsehen.

Hochauflösendes Fernsehen, auch HDTV genannt, wird i. d. R. digital übertragen, da die zur Fernsehübertragung erforderliche Bandbreite bei HDTV wesentlich größer ist als beim konventionellen Fernsehen.

HD-fähige Fernsehgeräte mit nur einem analogen Anschluss benötigen einen HD-fähigen Digitalreceiver (Set-Top-Box), um HDTV zu empfangen.

Die Übertragung von HDTV in Deutschland erfolgt heute digital mit DVB-C.

Die Übertragung des HDTV kann wahlweise auch verschlüsselt, beispielsweise für Pay-TV, erfolgen. Die Einspeisung aller HD-Programme der ARD und des ZDF erfolgt, obwohl mit geringem technischen Aufwand möglich, nur durch wenige Kabelnetzbetreiber. Der Grund dafür liegt darin, dass Kabelbetreiber eine Einspeisegebühr von den öffentlich-rechtlichen Anstalten verlangen. Auch der Empfang von privaten Fernsehsendern wird zurzeit von den großen Kabelnetzbetreibern recht unterschiedlich ermöglicht. Hier geht es vor allem um die Grundverschlüsselung der HD-Programme. Auch der Empfang des reinen Pay-TV, wie Sky, ist ebenfalls abhängig von der Einspeisung des Kabelnetzbetreibers möglich. Es werden jedoch derzeit von Sky aus Kostengründen nicht alle HD-Sender in den Kabelfernsehnetzen eingespeist. Nur über die Astra Satellitenposition 19,2° Ost wird in DVB-S das komplette Paket angeboten.

Das Kabelfernsehen kann prinzipiell mit einer Großgemeinschaftsantennenanlage (GGA, siehe Hausantenne) oder einer Gemeinschaftsantennenanlage (etwa einer Hausantenne auch mit einem zusätzlichen Einkabelsystem) sowie mit den Satellitendirektempfang mittels Multischalter kombiniert oder damit verwechselt werden.

Bei Bedarf können zusätzliche Signale in das lokale Kabelnetz des Hauses eingespeist werden. Es ist auch ein Satellitendirektempfang mittels eines SAT-Receivers in der eigenen Wohnung beim Kabelfernsehen prinzipiell möglich. So können auch zusätzliche lokale Radio- oder Fernsehsender, ebenso wie DVB-T-Signale oder Signale einer Satellitenrundfunk-Empfangsanlage in die Kabelanlage direkt eingespeist oder für das Kabelfernsehen umgesetzt werden.

Die Auskoppelung kann dann an der jeweiligen Antennenanschlussdose des Nutzers beispielsweise mittels eines zusätzlichen DVB-T oder SAT-Receivers erfolgen.

Um Verwechslungen mit Gemeinschaftsantennenanlagen zu vermeiden, wurden Netzebenen für das Kabelfernsehen definiert, um Abgrenzungen vornehmen zu können. Das Kabelnetz in Deutschland kann organisatorisch in mehrere Netzebenen aufgeteilt werden:


In anderen Ländern, wie zum Beispiel in der Schweiz, kann diese Aufteilung anders sein.
In Deutschland ist man in der Regel Kunde eines Anbieters auf der Netzebene 4.

Während man bei einer "Gemeinschaftsantennenanlage" (GA) bei Hotels oder Wohnblöcken von bis zu einigen hundert Teilnehmern ausgeht, sind "Großgemeinschaftsantennenanlagen" (GGA) für mehrere hundert Teilnehmeranschlüsse und weiter auseinander liegende Objekte gedacht. Großgemeinschaftsantennenanlage war auch die übliche Bezeichnung bis in die 1970er/1980er Jahre, als wie bei einer Hausantenne das übliche analoge terrestrische Fernsehsignal durch Kleinbetriebe, Vereine oder Gemeinden verteilt wurde. Dies geschah teilweise um einfach einen örtlich schlechten Empfang auszugleichen, teilweise um mit etwas größerem technischen Aufwand auch Programme des nahen Auslands zu empfangen. Die Übergänge in der verwendeten Technik zwischen Großgemeinschaftsanlage und Kabelnetz sind dabei fließend. Die Bezeichnung "GGA" hat sich vor allem in der schon früh ins Kabelfernsehen eingestiegenen Schweiz in zahlreichen Organisationsbezeichnungen erhalten, vereinzelt auch in Österreich und Deutschland.

Unter der Bandbreite des Kabelfernsehens versteht man die Anzahl der gleichzeitig übertragbaren Kanäle und den dabei genutzten Frequenzbereich. Dieser nutzbare Frequenzbereich wird kontinuierlich von den Betreibern erweitert und ausgebaut. 
Durch eine Erhöhung des nutzbaren Frequenzbereiches des Kabelfernsehnetzes wurde auch die Einführung der sogenannten Sonderkanäle für das Kabelfernsehen möglich. Die nutzbare Bandbreite ist immer von den verwendeten Kabeln, Verstärkern, Verteilern, Anschlussdosen und Größe der Anlage abhängig.
Da beim digitalen Kabelfernsehen mehr Fernsehsender als beim analogen Kabelfernsehen in dem gleichen Frequenzbereich übertragen werden können, werden neben der Erweiterung des nutzbaren Frequenzbereiches neue Fernsehkanäle vorzugsweise digital übertragen. Das Kabelfernsehen bietet Platz für mehr Kanäle als bei terrestrischer Ausstrahlung, aber viel weniger als über Satellit.

Unter der Ausbaustufe wird häufig die nutzbare Bandbreite (Frequenzbereich) beim Kabelfernsehen verstanden. In der Geschichte des Kabelfernsehens wurde die Anzahl der gleichzeitig übertragbaren Kanäle und der dabei genutzte Frequenzbereich mehrfach von den Betreibern erweitert. Ursprünglich war eine maximale Übertragungsfrequenz für das Kabelnetz von 300 MHz und 450 MHz geplant. Später wurde auf 600 MHz und 750 MHz erweitert. In neuester Zeit wurde der Bereich bis auf 862 MHz erweitert. Somit stehen z. Zt. in ausgebauten Gebieten in Empfangsrichtung der Frequenzbereich von maximal 80 MHz bis 862 MHz und in Senderichtung (Rückkanal) von maximal 5 MHz bis 65 MHz zur Verfügung.

Durch den Einsatz von Rückkanälen für Internet und Telefonie über Kabelfernsehen sinkt jedoch für den Kunden die nutzbare Bandbreite für das Fernsehangebot. Hier wird für in Senderichtung (engl. "upstream") der Bereich von 30 MHz bis 65 MHz genutzt. In Zukunft wird durch die Einführung von Docsis 3.1 auch der Frequenzbereich von 15 MHz bis 30 MHz genutzt werden. 

Es ist jedoch auch technisch möglich, unerwünschte Fernsehkanäle oder Dienste aus dem Kabelnetz mittels Sperrfilter zu unterdrücken und in den vorhandenen Lücken weitere Radiosender und Fernsehkanäle (z. B. zur Videoüberwachung) hinzuzufügen.

Bei der sogenannten kleinen analogen Grundversorgung, die früher sehr preisgünstig bei Kabel Deutschland angeboten wurde, wurde die Bandbreite mittels eines Sperrfilters bei dem betroffenen Teilnehmer eingeschränkt, um das frei verfügbare Fernsehangebot zu begrenzen. Bei diesen Anschlüssen sind in der Regel nur ARD, ZDF, das lokale Dritte Programm, mögliche lokale Fernsehsender sowie das UKW-Angebot nutzbar.

In Wohngebieten mit vorhandenen älteren Hausverkabelungen wurden häufig alte, oft unzureichend geschirmte Antennenleitungen weiterverwendet, wodurch zum Beispiel im Autoradio Radiosender aus dem Kabelnetz empfangen werden können und andere Funkdienste gestört werden. Bereits in der Anfangsphase musste die Deutsche Bundespost die analoge Nutzung von einigen Sonderkanälen (S02, S03 und S04) wieder einstellen, um eine Störung des Flugfunks zu vermeiden. Zudem kam es seit der Einführung des DAB-Radio (Digital Audio Broadcasting) immer wieder zu Störungen von Fernsehsendern im Kabelnetz. Dieses Problem wurde durch Erhöhung der Sendeleistung von DAB auf den Fernsehkanälen 5 bis 12 erneut deutlich sichtbar. Dabei wirken Teile des Kabelfernsehnetzes als Antenne, oder es kommt zu einer direkten Einstrahlung in schlecht abgeschirmten Verstärkern und Verteilern, weil Errichtungsvorschriften des Betreibers oder beim Kunden in der Wohnung nicht eingehalten wurden.

Diese alten schlecht abgeschirmten Koaxialkabel und Verteilelemente, sowie Weiterverwendung veralteter Steckdosen in Wohngebäuden bei der Umstellung von terrestrischem Antennenempfang auf einen Kabelanschluss, ist als Ursache vieler Störungen in alten Kabelnetzen zu nennen. Die damalige Deutsche Bundespost gab zwar eine Empfehlung zur Verwendung von doppelt abgeschirmten 75-Ω-Verteilerelementen an die Installationsbetriebe heraus, kontrolliert wurde die Einhaltung der Verordnung häufig jedoch nicht. Somit war diese Empfehlung in der Praxis häufig unwirksam, da sogar die damals noch üblichen 60-Ω-Installationen aus Kostengründen weiter verwendet wurden.

Die Störungen sind jedoch durch den Verursacher selbst zu beseitigen. Auch Nachfolger des Entstörungsdienstes der Deutschen Bundespost ermittelt diese Störursachen für den Betroffenen auf Antrag kostenlos und kann dann den Verursacher verbindlich zu deren Beseitigung verpflichten.

Vor der Einführung des Kabelfernsehens sind bereits viele der jetzt dem Kabelfernsehen zur Verfügung stehenden Frequenzen auch anderen Funkdiensten wie dem Flugfunk und Amateurfunkdienst oder BOS (Funkdienste der Behörden und Organisationen mit Sicherheitsaufgaben) zugeteilt worden. Diese Funkdienste können gestört werden, falls die Antennenkabel und Steckverbindungen nicht korrekt oder nicht ausreichend abgeschirmt sind oder bewusst manipuliert wurden. Bei diesen Frequenzen handelt es sich in der Regel um die sogenannten Sonderkanäle der Kabelfernsehens. Sie werden in Deutschland mit einem zusätzlichen "S" gekennzeichnet. Ein direkter Empfang dieser Kanäle ist nur mit Fernsehgeräten möglich, die mit einem sogenannten Kabeltuner ausgerüstet sind. Ab Mitte der 1980er Jahre konnten die meisten Fernsehgeräte diese Sonderkanäle empfangen oder ein Kabeltuner bei Bedarf nachgerüstet werden.

Auch eine unzureichende Entzerrung des Pegels im genutzten Frequenzband oder eine zu hohe Verstärkung kann den Kabelfernsehempfang empfindlich stören.

Typisch waren folgende Störungen:
Besonders beim Internetzugang über die Leitungen des Kabelfernsehens haben die oben beschriebenen Effekte noch größere Auswirkungen, da durch elektromagnetische Emission im Kurzwellenbereich Störungen verursacht werden können. Gestört werden dann der Kurzwellenrundfunk, der Seefunkdienst, Wetterfunk, Flugfunk, militärische Funkdienste, Botschaftsfunk, Amateurfunkdienst/Notfunk und Sendungen von Presseagenturen. Gegenüber den bisher bekannten Störungen im UHF-/VHF-Bereich, die einige 100 m weit reichen konnten, können diese ungewünschten Aussendungen zu Störungen im Kurzwellenbereich in mehreren Kilometern Entfernung führen.

Störungen durch die Aussendung des Kabelfernsehsignals werden in Deutschland durch die Bundesnetzagentur, den Nachfolger des früheren Funkentstördienstes, in der Regel auf Anfrage des Betroffenen ermittelt. Der Verursacher wird kostenpflichtig auf die Einhaltung der gesetzlichen Grenzwerte verpflichtet oder die Störquelle wird beschlagnahmt. Es können dann zusätzlich noch weitere juristische Maßnahmen ähnlich wie bei der Verfolgung von Schwarzsendern erfolgen.
Da nun auch für DVB-C die Sonderkanäle S02, S03 und S04 benutzt werden, ist zum Empfang häufig eine neue breitbandige Antennendose erforderlich, die in der Regel der Betreiber bei Bedarf beim Kunden vor Ort ersetzt. Die „alte“ Antennendose hat einen Frequenzfilter, der oberhalb von 108 MHz den digitalen Kanal S02/113 MHz dämpft und damit dessen störungsfreien Empfang beeinträchtigt. Ältere breitbandige Antennendosen können allerdings durch hohe UKW-Pegel Bildstörungen durch Übersteuerungen am Fernseher hervorrufen. Das kann durch die Verwendung moderner Antennendosen mit Frequenzfilter behoben werden, da die modernen Antennendosen mit Frequenzfilter für den Empfang der Sonderkanäle geeignet sind. Es gibt alternativ auch Hochpassfilter, die man in Verbindung mit der breitbandigen Antennendose verwenden kann. 

Mit der Einführung von DVB-T und DVB-T2 sowie den Mobilfunkstandard LTE im 800-MHz-Frequenzband sorgen schlecht abgeschirmte Kabelfernsehnetze für wechselseitige Störungen, da hier der Frequenzbereich bis 862 MHz genutzt wird. Hierbei mussten bereits Fernsehkanäle durch den Netzbetreiber abgefiltert oder gar gesperrt werden. Siehe auch Digitale Dividende.

Darüber hinausgehende Installationen und Wartungen in Kabelnetzen sollten auf Grund der umfangreichen Fehlermöglichkeiten immer von einem qualifizierten Fachbetrieb mit entsprechender Technik nach den Vorgaben des zuständigen Kabelnetzbetreibers erfolgen.

An der Antennensteckdose hört der Verantwortungsbereich des Kabelnetzbetreibers auf. Hier kann der Endbenutzer durch Verwendung von hochwertigen, mehrfach geschirmten Kabeln und Verteilern im eigenen Bereich viele dieser Störungen selbst beheben. Ein zusätzlicher kapazitiver Mantelstromfilter am Antenneneingang sorgt für eine Netztrennung am Antennenanschluss zwischen dem Kabelfernsehnetz und den eigenen Geräten und verhindert somit die sogenannten Netzeinstreuungen und eine „Brummschleife“.

Der Anschluss von Geräten an das Kabelfernsehnetz, die nicht den CE-Normen entsprechen, sowie von Minisendern, CB-Funkgeräten und das Abgreifen von Antennensignalen mittels Krokodilklemme (beispielsweise für eine Teleskopantenne bei mobilen Geräten) ist nicht statthaft.

Die vom Kabelfernsehbetreiber zur Verfügung zu stellende Signalstärke und die Signalqualität sind genormt. Diese Parameter sind für den Kunden am zentralen Hausübergabepunkt (Ende Netzebene 3) oder an der Antennendose (Ende Netzebene 4) zur Verfügung zu stellen.

Für den analogen Fernsehbereich gilt:
Sie reicht in etwa von 60 dBµV bis 75 dBµV (umgerechnet: 1 mV bis 5,6 mV). In den meisten Häusern befinden sich nahe dem eigentlichen Anschluss im Keller ein Verstärker und ein zentraler Verteiler, die idealerweise an jeder Anschlussdose 67 dBµV (umgerechnet: 2,2 mV) zur Verfügung stellen sollten.

Für den digitalen Fernsehbereich DVB-C mit einer Modulation von 64 QAM gilt:
Sie reicht in etwa von 50 dBµV bis 65 dBµV (umgerechnet: 0,3 mV bis 1,8 mV). Am Hausverstärker sollte das Signal so eingepegelt werden, dass unter Berücksichtigung der Verteilstruktur nahezu 57 dBµV (0,7 mV) an jeder Anschlussdose zur Verfügung stehen.

Für den digitalen Fernsehbereich DVB-C mit einer Modulation von 256 QAM gilt:
Sie reicht in etwa von 57 dBµV bis 69 dBµV (umgerechnet: 0,7 mV bis 2,8 mV). Am Hausverstärker sollte das Signal so eingepegelt werden, dass unter Berücksichtigung der Verteilstruktur nahezu 63 dBµV (1,4 mV) an jeder Anschlussdose zur Verfügung stehen.

Der Pegel (Signalstärke) ist so hoch, dass in der Regel zwei Fernsehgeräte mit einem passiven Verteiler direkt und ohne Störungen an eine Kabelantennensteckdose angeschlossen werden können. Durch die Übertragung von Signalen mit einer definierten Stärke (Pegel) auf einer festgelegten Frequenz im Kabel (Pilotton) können Kabelverstärker automatisch schwankende Kabeldämpfungen, die durch Temperaturschwankungen entstehen, selbständig ausgleichen. Diese Verstärker werden in der Regel nur zur Überbrückung großer Entfernungen, nicht jedoch als sogenannte Hausanschlussverstärker eingesetzt.

Kabelfernsehen wird von verschiedenen Netzbetreibern gegen Gebühr bereitgestellt. Im Jahr 2015 nutzten etwa 41 Prozent der Deutschen Kabelfernsehen. In dieser Statistik werden auch Satellitenzuschauer ausländischer Herkunft oder mit verschiedenen Sparteninteressen erfasst, die fremdsprachiges Fernsehen oder im Kabel nicht vorhandene Spartenprogramme über Satellit empfangen, obwohl ein Kabelanschluss zur Verfügung steht und über die Mietnebenkosten bezahlt werden muss. Außerdem werden Kunden erfasst, die andere Empfangswege bevorzugen, aber über ihren Mietvertrag an den Kabelanschluss gebunden sind. Mindestens 46 Prozent nutzen eine Satellitenantenne, und 5 Prozent empfangen terrestrisch (DVB-T) über die Hausantenne oder verzichten auf Fernsehen.

In Deutschland gibt es zurzeit folgende große Anbieter: Unitymedia (Baden-Württemberg, Hessen und Nordrhein-Westfalen), Vodafone Kabel Deutschland (übrige 13 Bundesländer, der bei weitem größte Anbieter) und Pÿur (siehe auch Kabelnetzbetreiber #Deutschland). Die Landesmedienanstalten überwachen die Kabelnetzbetreiber und legen die Programme fest, die in das Kabelnetz eingespeist werden müssen. In der Regel müssen alle lokalen (deutschen) Sender, die ohne besonderen Aufwand vor Ort frei empfangbar sind, in das Kabelnetz analog eingespeist werden. Das digitale Angebot der öffentlich-rechtlichen Rundfunkanstalten (ARD und ZDF) braucht via DVB-C jedoch nur in SD-Qualität in das Kabelnetz eingespeist zu werden. Die Einspeisung aller Fernseh- und Radioprogramme der ARD über DVB-C erfolgt, obwohl mit geringem technischen Aufwand möglich, zurzeit nicht von allen Kabelnetzbetreibern. Zurzeit sind von einigen Anbietern keine Termine zur Einspeisung der fehlenden ARD-Sender und des sogenannten Hörfunktransponders über DVB-C bekannt. Es gibt für die Kabelnetzbetreiber keine gesetzliche Verpflichtung für die Einspeisung der öffentlich-rechtlichen Fernsehprogramme in HD-Qualität. 

Unitymedia und Kabel Deutschland bieten heute für Neukunden kein analoges Kabelfernsehen mehr an (siehe DVB-C in Deutschland). Trotzdem sind bei beiden Kabelanbietern analoges und digitales Kabelfernsehen für alle Kunden noch verfügbar, wenn auch für Neukunden vertraglich kein Anspruch mehr auf analoges Kabelfernsehen besteht. Anders ist die Lage in Mietobjekten von Großvermietern (ehem. kommunale Wohnungsgesellschaft). Da dort der Kabelnetzbetreiber oft das Monopol in einem Objekt hat, kann der Mieter nicht zwischen den Anbietern wählen. Daher wird oft in solchen Mietobjekten ein Vertrag mit ermäßigten Konditionen zwischen Vermieter und Kabelnetzbetreiber ausgehandelt, so dass in diesen Fällen auch für Neukunden eine (im Vergleich zur Digitalversorgung) günstige Analogversorgung möglich ist.

Die ersten konsequent entwickelten Anwendungen des Kabelfernsehens gab es bereits Mitte der 1930er Jahre in Berlin und später auch in Hamburg unter dem Begriff „Fernseh-Drahtfunk“. Über Koaxialkabel wurden Fernsehsignale auf einen Träger von 4,2 MHz aufmoduliert und in die Fernsehstuben und Gemeinschaftsempfangsanlagen Berlins übertragen. Der Fernseh-Drahtfunk hatte gegenüber dem Antennen-Fernsehempfang den Vorteil, dass Bildstörungen durch Störsignale aus dem S-Bahn-Netz vermieden werden konnten. Viele der damaligen Fernsehempfänger hatten daher sowohl einen Fernsehantennenanschluss als auch einen Kabeleingang. Der Ton wurde teils über dieselbe Leitung, teils auch über das Telefonnetz übertragen.

In der Bundesrepublik Deutschland kam es erst Anfang der 1960er Jahre zur Einführung einer Form von Kabelfernsehen, die Gemeinschafts- und Großantennenanlagen zur Versorgung von Mietshäusern. Diese Anlagen lieferten per Kabel meist nur ortsübliche lokale Radio- und Fernsehprogramme in die einzelnen Wohnungen. Dieses war allerdings allenfalls eine Vorstufe zum Kabelfernsehen, wie man es heute versteht.

Erst 1972 begann die Deutsche Bundespost schrittweise mit dem Bau von Kabelversuchsnetzen in abgeschatteten Gebieten mit einer Kapazität von je zwölf Fernseh- und Hörfunkprogrammen. Ende 1974 beziehungsweise 1978 wurden diese Kabelnetze, die mit analogen Kabelkopfstellen mit einer Bandbreite von 47 MHz bis 350 MHz ausgestattet waren, in Betrieb genommen. 

In der DDR gründeten örtliche "Antennengemeinschaften" Ende der 1970er Jahre die ersten Kabelfernsehnetze, die neben einer Verbesserung des lokalen Rundfunkempfangs auch den Empfang des Westfernsehens in hoher Qualität ermöglichten.

Kabelfernsehen wurde in Westdeutschland erstmals am 1. Januar 1984 im Rahmen des Kabelpilotprojekts Ludwigshafen am Rhein bis 1986 ausgestrahlt. Weitere Kabelpilotprojekte folgten in München (1984–1985), Dortmund (1985–1988) und West-Berlin (1985–1990). Hier wurden auch erstmals Spartenkanäle (für Bildung oder Sport) sowie offene Kanäle für Bürgerprogramme erprobt. Die Geburtsstunde des Kabelfernsehens 1984 war auch gleichzeitig die Geburtsstunde des kommerziellen Privatfernsehens.

Das Kabelfernsehen war für die meisten Bürger zeitiger und kostengünstiger als der heutige Satellitendirektempfang verfügbar.

"Hier gibt es offenbar widersprüchliche Informationen. Aus Recherchen über Kabel Deutschland ging Folgendes hervor:" Anlässlich der Eurobau soll 1978 in Altenholz (bei Kiel) die erste Kopfstelle in Betrieb gegangen sein. Die erste Satellitenrundfunk-Empfangsanlage (SEE) soll 1983 auf dem Kieler Fernsehturm aufgebaut worden und in Betrieb gegangen sein.

Noch 1982 zählte die Bundesrepublik mit einem Versorgungsgrad von rund zwei Prozent zu den „Kabel-Entwicklungsländern“. 1984 bezifferte die Deutsche Bundespost die Kosten der Verkabelung auf 13,5 Milliarden DM (6,9 Milliarden Euro) und 1988 auf 15 Milliarden DM (7,7 Milliarden Euro). Die Verkabelung verlieh laut einem im Oktober 1987 veröffentlichten Gutachten des Deutschen Instituts für Wirtschaftsforschung, im Gegensatz zu Prognosen des Bundesministeriums für das Post- und Fernmeldewesen, dem Arbeitsmarkt nur in geringem Maß positive Impulse. Bis Ende 1990 erhielten in Westdeutschland 8,1 Millionen Haushalte einen Kabelanschluss, das entsprach nach offiziellen Angaben eine Versorgungsdichte mit Kabelanschlüssen von 31,5 Prozent aller Haushalte in der Bundesrepublik.


Am 31. Dezember 1995 meldete die Deutsche Telekom 15,8 Millionen Nutzer von Kabelanschlüssen; die Kabeldichte lag damit bundesweit bei 65,3 Prozent, der Versorgungsgrad (Anschlussdichte) bei 64,7 Prozent. Die höchste Anschlussdichte aller Bundesländer verzeichneten Mecklenburg-Vorpommern (75,1 Prozent) und Brandenburg (74,4 Prozent), der höchste Versorgungsgrad konnte in Hamburg (99,4 Prozent) und Bremen (98,0 Prozent) erreicht werden.

Die niedrigste Anschlussdichte haben im deutschen Kabelnetz demnach Sachsen-Anhalt (56,9 Prozent) und Schleswig-Holstein (61,4 Prozent), den niedrigsten Versorgungsgrad haben Thüringen (32,4 Prozent) und Sachsen-Anhalt (27,6 Prozent), gefolgt von Brandenburg (39,5 Prozent) und Mecklenburg-Vorpommern (39,6 Prozent). Die meisten an das Fernsehkabelnetz angeschlossenen Wohnungen gibt es in Nordrhein-Westfalen (3,7 Mio.), Baden-Württemberg (2,0 Mio.) und Bayern (2,3 Mio.).

Anfang 1997 forderte die Monopolkommission, die Deutsche Telekom müsse ihr Kabelnetz verkaufen. Die Bundesregierung lehnte diese Forderung noch im März 1997 mit der Begründung ab, ein solcher Zwangsverkauf verstoße gegen Grundgesetz und Aktienrecht.

Nach der Untersagung der bundesweiten gemeinsamen digitalen Kabelfernseh-Medienplattform von Deutscher Telekom, Bertelsmann und der Kirch-Gruppe aus kartellrechtlichen Gründen durch die Europäische Kommission 1998 und aufgrund weiteren Drucks der EU- und nationalen Wettbewerbsbehörden verkaufte die Telekom ihr Kabelnetz ab 2000 schrittweise bis 2003 in Form von regionalen Teilnetzen wie zum Beispiel ish, iesy, Kabel Deutschland, Kabel BW, Primacom sowie Tele Columbus, was den Weg für den rückkanalfähigen Ausbau und das Angebot von Kabeltelefonie und Kabelmodem (Triple Play) über das deutsche Kabelnetz freimachte. Kabel Deutschland erhielt bei Abkauf der Kabelnetze in Schleswig-Holstein alle Unterlagen zum Kabelfernsehen. Die heutige Deutsche Telekom AG hat bestätigt, dass Akten und Vermerke zur Errichtung der Kabelnetze komplett an Kabel Deutschland übergeben wurden.

Anfang 2004 stand Kabel Deutschland vor der Übernahme anderer großer Mitbewerber, was erneut zu einer Monopolstellung geführt hätte; das Bundeskartellamt hat die Übernahme daher zunächst untersagt. Anfang 2005 strebten in einem neuen Anlauf zwei regionale Anbieter – ish (Nordrhein-Westfalen) und iesy (Hessen) – einen Zusammenschluss an. Dieser Zusammenschluss ist im Mai 2007 unter dem gemeinsamen Namen Unitymedia erfolgt. Der neue Name steht für die Vereinigung von Fernsehen, Internet und Telefon aus einer Hand. Durch Zukauf von Tele Columbus West wurden noch mehr Kunden integriert.

Mit der Fußball-Weltmeisterschaft 2006 hielt der neue HDTV-Standard auch im Kabelnetz Einzug, jedoch nur im Bezahlfernsehen. Durch die Umrüstung auf zentrale Verteilung mit Glasfaserkabel und Fernwartung wurden mittlerweile viele alte Standorte von Kopfstationen (örtlichen Empfangsstellen) aufgegeben oder zurückgebaut, sowie Kabelnetzsegmente zusammengefasst. Durch diese Zentralisierung sind lokal mittels Antenne frei empfangbare oder ortsübliche ausländische Sender, wie beispielsweise der ORF im grenznahen bayerischen Gebiet, nicht mehr im lokalen Kabelnetz verfügbar.

In der Bundesrepublik Deutschland kam es mit der Einführung des Kabelfernsehens häufig zu langfristigen Verträgen zwischen Kabelnetzbetreibern und Wohnungsverwaltungen sowie gewerblichen Vermietern zur Versorgung von Mietshäusern. Diese Verträge sicherten für viele Mieter erstmals einen hochwertigen störungsfreien Rundfunk- und Fernsehempfang und sorgten gleichzeitig dafür, dass in vielen Wohnungen ein Kabelanschluss verfügbar ist. Gleichzeitig wurden die häufig unzureichenden Einzelantennenanlagen der Mieter überflüssig, und häufig wurden die bereits vorhandenen Gemeinschaftsantennenanlagen für Wohnblocks und Hochhäuser in die neuerrichteten Kabelnetze einbezogen.

In einigen Städten gründeten Wohnungsverwaltungen auch eigene Kabelnetzbetreiber oder arbeiteten mit örtlichen Elektrobetrieben zusammen, um ihre Wohneinheiten auf der Netzebene 4 zu versorgen. Da die Programmzuführung meist von den großen Kabelbetreibern stammt, kommt es hier oft zu Konflikten bei der Versorgung mit Internet und Telefon über diese Anlagen. Teilweise werden hier Installationsverbote durch den NE4-Betreiber ausgesprochen, was zu Unmut bei Mietern führen kann.

Nach dem Regierungswechsel in der Bundesrepublik im Oktober 1982 wurde der Errichtung flächendeckender Breitbandkabel-Verteilnetze (BK-Netze) in Kupferkoaxialtechnik eine hohe Priorität eingeräumt. Mit 24 bis 35 theoretisch nutzbaren Fernseh- und etwa 40 Hörfunkkanälen bot das Kupferkoaxialkabel im Breitbandnetz eine Übertragungskapazität, die ein vielfältiges Programmangebot ermöglichte. Der damalige Bundespostminister Christian Schwarz-Schilling äußerte 1983 sehr deutlich: – gemeint waren in erster Linie private Anbieter. Die Deutsche Bundespost verdoppelte die Finanzmittel für den Ausbau der Breitbandkommunikationsnetze mit Kupferkoaxialkabeln im Jahre 1983 auf insgesamt eine Milliarde DM. Auch für die Folgejahre wurde von einer ähnlich hohen Investitionssumme ausgegangen, 1990 waren es schließlich 1,5 Milliarden DM im Jahr. Vor 1982 beschränkte sich die Deutsche Bundespost darauf, nur solche Gebiete zu verkabeln, die – meist auf Grund von geografischen Gegebenheiten – keinen oder nur einen schlechten Empfang von terrestrischen Sendern zuließen. Bis 1982 wurden etwa 300.000 Haushalte (1,3 Prozent der Fernsehhaushalte) an die Kabelnetze der Deutschen Bundespost angeschlossen. Die Verkabelungsstrategie der Post wurde nun unter maßgeblichem Einfluss des neuen Postministers verändert: Der bisher bedarfsorientierte Ausbau der Kabelverteilnetze wurde durch eine angebotsorientierte Absatzstrategie ersetzt. Die für die medienpolitisch erwünschte Programmvermehrung, die auch für privaten Fernsehanbieter benötigte Kabelinfrastruktur schien dabei nur durch den massiven Einsatz staatlicher Gelder realisierbar – und über die Verwendung der vergleichsweise teuren Kupferkoaxialkabel. Das Kabelfernsehen war damals für viele Bürger früher und preiswerter als der Satellitendirektempfang verfügbar.

Diese Netze verbreiteten neben dem öffentlich-rechtlichen Fernsehen von ARD, später allen Dritten Programmen sowie das ZDF auch das staatliche Fernsehen der DDR, ab Mitte der 1980er Jahre auch erstmals flächendeckend kommerzielle Privatsender (zunächst Sat.1 und RTL plus), sowie in Grenznähe den ORF und SRG SSR, sogar unverschlüsselt den schweizerischen Pay-TV-Sender Teleclub. Die ersten Digitalprogramme strahlten die Fernsehsender (Eins-Plus, 3sat, RTL plus, Sat.1) in D2-MAC 4:3 (16:9 optional) aus. In ausgebauten Kabelfernsehanlagen wurde dafür 12 MHz breite Hyperband-Sonderkanäle genutzt. Das digitale Satellitenradio DSR wurde über das Kabelfernsehen zeitweilig angeboten.

Die ursprünglich für das Kabelfernsehen geplanten Breitbandverteilnetze der Deutschen Bundespost hatten die Verwendung eines Rückkanals vorgesehen. Beim Kabelfernsehen ist eine derartige Funktion primär nicht erforderlich. Jedoch gab es damals auch schon Versuche, den Rückkanal für interaktives Fernsehen, ähnlich dem Hybrid-Fernsehen und Home-Shopping, zu realisieren. Dazu wurde der störanfällige Bereich von 5 MHz bis 30 MHz meist mit passivem Rückweg genutzt. Nach Protesten von Datenschützern wurde diese Funktionalität nie großflächig eingesetzt. Erst mit dem Einzug von Internet über Fernsehkabel zur Jahrtausendwende verbreitete sich die Rückkanalfähigkeit im Bereich von 30 MHz bis 65 MHz wieder, indem Verteiler und Verstärker ausgetauscht wurden. Zu diesem Zeitpunkt gehörten die deutschen Kabelfernsehnetze nicht mehr der Telekom.

In Österreich gibt es mehrere große Kabelanbieter sowie einige regionale Kabelnetzbetreiber. Alle großen österreichischen Kabelnetzbetreiber bieten über ihre Leitungen auch Internet und Telefonie an.

Allerdings ist in Österreich das digitale Satellitenfernsehen stärker als in jedem anderen europäischen Staat verbreitet – 55 Prozent der österreichischen Haushalte sind für den digitalen Satellitenempfang ausgerüstet. Daher spielt das Kabelfernsehen in Österreich mit 36,5 Prozent eine geringere Rolle. Vor allem die einfache Verfügbarkeit aller österreichischen und deutschen Programme und die mittlerweile kostengünstig und ohne weitere laufende Kosten zur Verfügung stehende Satellitenempfangstechnik tragen zu diesem Umstand bei.

Jedoch gibt es als einzelne Ausnahmefälle durchaus Gemeinden mit hoher Kabelanschlussrate – entweder, weil das Kabelfernsehen bereits sehr früh (meist durch kleinere Unternehmen oder die Gemeinden selbst) ausgebaut und entsprechend kundennah beworben wurde, oder weil punktuell besonders strenge Vorschriften zum Ortsbildschutz in Kraft sind, die Fernsehantennen und Satellitenschüsseln auf Gebäuden untersagen.

Im November 1974 beschloss Vöcklabruck als erste Stadt Österreichs eine Gemeinschaftsanlage für das Kabelfernsehen zu errichten. Im Mai 1975 wurden die ersten Haushalte mit den beiden ORF-Programmen und drei deutschen Sendern versorgt. Letztere waren durch die Grenznähe mit guter Antennentechnik empfangbar. Ab 1985 wurden über Satellit SAT 1 und 3sat in das Kabelnetz eingespeist.

Ein anderer Pionier ist die vereinsmäßig organisierte Großgemeinschaftsantennenanlage (GGA) in der Gemeinde Dietach, welche seit Mai 1976 neben zwei ORF-Programmen auch drei deutsche Programme an anfangs 90 Haushalte übertrug. Ab Oktober 1986 wurden auch Satellitensignale eingespeist.

Die Wiener Telekabel (heute UPC Austria) wurde 1978 von der Stadt Wien und Philips gegründet und 1979 startete der Betrieb in Gemeindebauten in Floridsdorf und Donaustadt. Neben den zwei österreichischen Programmen wurde über Richtfunkstrecken der Post- und Telegraphenverwaltung die deutschen Sender ARD, ZDF, BR3 und WDR und das schweizerische SRG SSR zugeführt. Die Richtfunkstrecke nach Wien versorgte später auch andere Städte auf ihrer Strecke bzw. gab das Signal vom Funkturm Wien-Arsenal weiter unter anderem an die im Burgenland 1980 fertiggestellten Verteilerpunkte der Burgenländischen Kabelfernseh-Gesellschaft (BKG). Schon über Satellit wurde zu Beginn der paneuropäische Super Channel eingespeist. Ab 1984 kamen über Satellit 3sat, SAT1 und RTL Plus hinzu. In Wien begann man 1995 auch die Rückkanalfähigkeit zu realisieren. Dabei wurde Internet über Telekabel mit gesharten 10 Mbit/s für einen Verteilungsbereich über den Kabelanschluss verfügbar.

1978 wurde auch die Liwest von den Magistraten Linz, Wels, Steyr und der Siemens AG Österreich gegründet. Sie speiste ab 1985 die Satellitenprogramme SAT1 und 3sat ein.

Wegen örtlich schlechter Empfangsbedingungen verlegte die Gemeinde Weng im Gesäuse 1978 parallel zum Bau der öffentlichen Schmutzwasserkanalanlage ein Koaxialkabel. 1979 wurde der Kabelbetreiber KTV-Eisenerz gegründet.

Etwa 93 Prozent der Schweizer Fernsehhaushalte empfangen ihre Programme über einen Kabelnetz-Anbieter, die restlichen Haushalte mit einer Dachantenne als DVB-T oder Satellit.

Wichtigster Anbieter ist die UPC Cablecom mit einem Marktanteil von über 50 Prozent. Weitere wichtige Anbieter sind Quickline und die Digital Cable Group. Gesamtschweizerisch gibt es 422 Kabelnetzbetreiber. Diese gehören Unternehmen, Genossenschaften oder Gemeinden. Nur elf Kabelnetzbetreiber haben mehr als 22.000 Kunden. Dieses zeigt, wie viele kleinere Anbieter es gibt. Die Netze sind in der Regel für 862 MHz ausgelegt. Dadurch werden oftmals über 50 verschiedene Fernsehprogramme verbreitet. In den meisten Kabelnetzen wird auch (meist kostenpflichtiges) digitales Fernsehen angeboten, mit insgesamt 600.000 Kunden (Stand: Dezember 2009).

Erfolgreicher ist der Breitband-Internetzugang über das Kabelnetz mittels Kabelmodem mit rund 750.000 Anschlüssen (Stand: Dezember 2009). Die Konkurrenztechnologie ADSL zählt 1.368.000 Anschlüsse (Stand: Dezember 2007).

Seit dem Jahre 2004 ist in verschiedenen Netzen auch das Telefonieren über das Kabelnetz möglich, hier gibt es gemäß dem Branchenverband swisscable 365'000 Anschlüsse (Stand Dezember 2009). Seit die SRG SSR seit 1. Dezember 2007 mit HD suisse einen eigenen HD-Kanal betreibt, haben die meisten Schweizer Kabelnetze HDTV-Angebote aufgeschaltet.

Bereits Mitte der 1930er Jahre entstanden die ersten Kabelnetze für Hörfunk. Die PTT (heute Swisscom) verzichtete auf ihr Monopol, da sie noch zahlreiche Telefonanschlüsse in Betrieb nehmen musste und sich deshalb nicht auch noch mit dem Aufbau eines Kabelnetzes beschäftigen wollte. Zudem verfügte sie mit dem Telefonrundspruch über eine eigene drahtgebundene Technik für Hörfunk. Bereits in den 1950er Jahren verbreiteten die ersten Kabelnetze auch Fernsehsignale. In den folgenden Jahren entstanden an zahlreichen Orten Kabelnetze. Anfangs der 1990er Jahre wurden die Kabelnetze oftmals auf 600 MHz oder gar 862 MHz ausgebaut. In einigen Netzen wurden bis zu 65 analoge Fernsehprogramme verbreitet. 1994 fusionierten mehrere größere Kabelnetze zur Cablecom. 1995 boten die ersten Kabelnetze den Internetzugang. Seit 1. November 2006 bietet die Swisscom auch Fernsehprogramme über VDSL-Anschlüsse (IPTV) an.

Schon Mitte der 1940er Jahre entstand in den USA vereinzelt eine Form von Kabelfernsehen, allerdings als Notlösung: Um auch Menschen in abgelegenen Gebieten Fernsehempfang zu ermöglichen, wurden in manchen Regionen große Antennen aufgestellt; anschließend wurden von dort Kabel zu den einzelnen Häusern verlegt. Der Wunsch nach mehr Programmen führte in den USA und in Kanada dann in den 1950er Jahren zu der Einführung eines „echten Kabelfernsehens“ neben der gewöhnlichen Fernsehversorgung per Funk.

Historisch bedingt sind vor der Einführung des Kabelfernsehens die technisch zur Verfügung stehenden Frequenzen dem Fernsehempfang, dem UKW-Radio und auch anderen Funkdiensten zugeteilt worden. Um eine mögliche Doppelvergabe oder Verwechslung der bestehenden Fernsehkanäle zu verhindern, wurden die im Kabel noch freien Frequenzlücken, die sonst durch andere Funkdienste belegt waren, mit den sogenannten Sonderkanälen für das Kabelfernsehen belegt.

Deshalb können Kanäle, die nur eine Nummer tragen, auch für das herkömmliche Fernsehen über Antenne verwendet werden. Die mit "S" bezeichneten Kanäle, sogenannte Sonderkanäle, sind dagegen außerhalb des Kabelnetzes (im Funkbetrieb) für andere Zwecke reserviert. Fernsehgeräte von vor etwa 1985 können daher gewöhnlich auch am Kabel nur Sender empfangen, die nicht auf mit "S" bezeichneten Kanälen liegen, da ihr Empfänger nicht für die Sonderkanäle gebaut ist. Meistens sind die Kanäle S02 bis S19 jedoch auch mit älteren Geräten als Kanal 82 bis 99 empfangbar.

Um den Empfang der Sonderkanäle des Kabelfernsehens auf alten Geräten zu ermöglichen, konnten die Empfangseinheiten (engl. "tuner") durch sogenannte Kabeltuner ersetzt werden. Eine weitere Möglichkeit war die Umsetzung der Fernsehkanäle mittels eines Kabelkonverters in den UHF-Bereich (Fernsehkanäle 21–69), der damals durch das Kabelfernsehen nicht oder nur teilweise genutzt wurde.

Die folgende Tabelle gibt nur die Zuordnungen für den deutschsprachigen Raum an – in anderen Ländern können die gleichen Kanalnummern teilweise andere Frequenzen bezeichnen (siehe dazu auch Frequenzen der Fernsehkanäle). Angegeben ist jeweils die Frequenz des Bildträgers – zu jedem Kanal gehören aber alle Frequenzen von 1,25 MHz unterhalb der angegebenen Bildträgerfrequenz bis 1,25 MHz unterhalb der Bildträgerfrequenz des nächsten Kanals. Der Kanal 2 erstreckt sich von 47 MHz bis 54 MHz.

Die angegebenen Empfangsfrequenzen sind

Neben den standardisierten Empfangsfrequenzen kommen vereinzelt auch vom Standard abweichende Empfangsfrequenzen zum Einsatz, um z. B. EMV-Probleme zu reduzieren.


Der Frequenzbereich VHF II (87,50 MHz–108,00 MHz) ist für UKW-Rundfunk reserviert.

Hierbei handelt es sich um sogenannte Sonderkanäle.

Der Sonderkanal S01 kann wegen der Überschneidung mit dem UKW-Radio-Frequenzband nicht genutzt werden.

Die Sonderkanäle S02 und S03 wurden nur selten analog genutzt. Seit dem 31. März 2009 verbietet die "Sicherheitsfunk-Schutzverordnung" (SchuTSEV) der Bundesnetzagentur die Nutzung der Kanäle im Bereich 112 bis 125 MHz für die analoge Signalverbreitung.

Für die digitale Belegung der Kanäle S02 und S03 wird von den QPSK-QAM-Umsetzern eine Bandbreite von 8 MHz benötigt. In den vorhandenen Kabelnetzen wird vor diesem Hintergrund eine Neuordnung des Kanalrasters mit einer Kanalbreite von 8 MHz angestrebt. Die Neuanordnung beginnt hier ausgehend vom bestehenden 8 MHz-Digital-Kanalraster im Hyperband, von wo aus das Raster hin zu niedrigeren Frequenzen fortgesetzt wird (D114, D122, ..., D170). Einzelne Kabelnetzbetreiber nutzen bis zu einer Neuordnung übergangsweise das bestehende Frequenzraster für die Verbreitung digitaler Kanäle. Hierdurch ergibt sich ein verschobenes Raster für digitale Kanäle, beginnend mit D113, D121, ..., D169.

Seit dem 1. Januar 2011 dürfen aufgrund der "Sicherheitsfunk-Schutzverordnung" (SchuTSEV) auch die Sonderkanäle S04 und S05 nicht mehr für die analoge Signalverbreitung genutzt werden. Im Bereich 125 MHz bis 137 MHz wird seitdem ausschließlich digital gesendet.

Durch diese Umstellung entfallen im analogen Kabelfernsehprogramm bei Kabel Deutschland (Vodafone) in jedem Bundesland ein Drittes Fernsehprogramm der ARD (bspw. in Niedersachsen das Bayerische Fernsehen). Bei Unitymedia erfolgte die Umstellung erst zum 25. Januar 2011.

Sofern S06 noch analog auf 140,25 MHz genutzt wird, entfällt eine digitale Nutzung von S05 aufgrund von Überlappung der Kanäle bis zu einer weiteren Abschaltung analoger Kanäle.

In der Schweiz werden in den Kabelnetzen alle Sonderkanäle S02 bis S10 analog genutzt, teilweise mit einzelnen Kanälen um bis zu 1,5 MHz Versatz, um Störungen zu Lokal- oder Behördenfunk zu vermeiden.

Alternative Digital-Belegung im Rahmen einer Übergangsphase zwischen Analog- und Digital-Ausstrahlung:

Die Kanäle 5 bis 12 werden bei den Kabelnetzbetreibern im Jahr 2017 noch für die analoge Programmverteilung genutzt.

Die Sonderkanäle S11 bis S20 werden bei den Kabelnetzbetreibern derzeit nur analog genutzt.

Für den Internetzugang in ausgebauten Kabelnetzen stehen nach Euro-Docsis die Frequenzbereiche von 5 MHz bis 65 MHz in Senderichtung (engl. "upstream" oder "upload") und 450 MHz bis 862 MHz in Empfangsrichtung (engl. "downstream" oder "download") zur Verfügung (Obergrenze abhängig vom Ausbau, nicht durch Docsis spezifiziert), in der Praxis sind jedoch diese Frequenzbereiche nicht vollständig verfügbar bzw. werden nur eingeschränkt vom Netzbetreiber (Euro-Docsis 2.0 bzw. 3.0) von 30 MHz bis 65 MHz und (Euro-Docsis 3.1) 15 MHz bis 30 MHz in Sende- sowie von 450 MHz bis 640 MHz in Empfangsrichtung unterstützt.

Siehe auch: Breitband-Internetzugang: Frequenzbereiche für Internetzugang nach Euro-Docsis




</doc>
<doc id="11984" url="https://de.wikipedia.org/wiki?curid=11984" title="Satellitenrundfunk">
Satellitenrundfunk

Mit Satellitenrundfunk bzw. Satellitenfernsehen werden Rundfunkprogramme, insbesondere Fernsehprogramme, über Satelliten zu den Empfängern übertragen.

In Mitteleuropa empfangbare und per Satellit übertragene Rundfunkprogramme werden beispielsweise von den Astra- oder Eutelsat-Satelliten ausgestrahlt. Zum Empfang von Satellitenrundfunk wird üblicherweise eine Parabolantenne mit LNB (Rauscharmer Signalumsetzer) sowie ein Satellitenrundfunkempfänger („Receiver“) benötigt.

Von den 34,83 Millionen deutschen Haushalten haben 17,5 Millionen (also ca. 50 %) direkt über eine Individualantenne oder eine Gemeinschafts-Empfangsanlage mit eigenem Receiver Satellitenempfang („Satellitendirektempfang“).

Bereits 1928 publizierte der Österreich-Ungarische Raumfahrttheoretiker Herman Potočnik die geostationäre Position in einem schmalen Streifen über dem Äquator, auf welcher Satelliten in 35.800 km Höhe scheinbar stillstehen. In den Bestrebungen, dies für die Rundfunkübertragung zu nutzen, wurde bei der Weltfunkkonferenz (WARC) in Genf 1977 ein weltweiter Rundfunk-Satellitenplan beschlossen. Ab 1. Januar 1979 galt eine Vereinbarung mit einer Laufzeit von 15 Jahren, welche jedem Land fünf Kanäle für Fernsehen/Radio auf einer Satellitenposition zuteilte. Die jeweilige Position sollte sich dann jedes Land mit bis zu acht anderen Ländern (und damit Satelliten) teilen. Diese Satelliten sollten dann in der Orbitalposition mit einem Abstand von 6° über dem Äquator positioniert werden. Eine gemeinsame Satellitenposition (19° West TV-SAT) wurde Belgien, der Bundesrepublik Deutschland, Frankreich, den Niederlanden, Italien, Luxemburg, Österreich und der Schweiz zugewiesen.

Die Bundesrepublik Deutschland und Frankreich beschlossen am 2. Oktober 1979 in Bonn ein Rahmenabkommen über den Bau zweier staatlicher Fernsehdirektsatelliten (TV-SAT), Luxemburg entschied sich hingegen für ein nicht staatliches, privates Projekt (der SES Astra).

1982 wurde die „European Telecommunications Satellite Organization“ (Eutelsat) in Frankreich als eine zwischenstaatliche Organisation mit dem Ziel gegründet, eine satellitenbasierte Telekommunikationsinfrastruktur für Europa aufzubauen. Eutelsat-Satelliten waren (damals nur mit vergleichsweise geringer Sendeleistung ausgestattet) nicht zum TV-Direktempfang geplant, sondern sollten als Zubringer zu terrestrischen Rundfunksendern und Kabelfernsehanlagen dienen.

Gemeinsam beschlossen 1984 das deutsche ZDF, der österreichische ORF und das Schweizer SRF die Ausstrahlung von 3sat über Satellit. Am 1. Dezember 1984 startete die Ausstrahlung von 3sat über den Satelliten Eutelsat I-F1 (ECS-1).

1985 kamen jedoch neuartige kostengünstige, rauscharme HEMT-LNBs auf den Markt, welche zunächst ambitionierten Bastlern den Direktempfang des Eutelsat ECS 1 (Orbitalposition 13° Ost) mit vertretbarem Aufwand auch in Privathaushalten ermöglichten. Dies schien wenige Jahre zuvor zumindest in Europa noch unrealistisch zu sein, wie folgende im Jahre 1975 veröffentlichte Einschätzung zeigt:

Der Direktempfang war anfangs nicht unumstritten. So mussten die individuellen Satellitenempfangsanlagen bei der Behörde angemeldet werden, welche dann eine laufende Gebühr erhob.

SES Astra verhalf schließlich mit ihren Medium-Power-Direktempfangssatelliten dem Satellitenfernsehen in Europa auf breiter Basis zum Durchbruch. Anfang der 1990er Jahre forderte die Deutsche Bundespost, Astra-LNBs noch mit einem zusätzlichen Filter am Eingang auszustatten, „da sonst möglicherweise eigene terrestrische Richtfunkstrecken gestört werden könnten“. Diese Befürchtung stellte sich später als unbegründet heraus; möglicherweise war diese Forderung auch lediglich eine protektionistische Maßnahme.

Ab 1994 wurde parallel zum analogen Satellitenfernsehen vorerst für Bezahlfernsehangebote ein digitaler Übertragungsweg (DVB-S) eingeführt. Triebfeder hierfür waren Bezahlfernsehsender wie DF1. Leo Kirch ging hier durch Propagieren seiner digitalen d-box hohe finanzielle Risiken ein, endgültig populär wurde DVB-S schließlich durch die Qualitäts- und Kapazitätsvorzüge, was schließlich am 30. April 2012 endgültig zur sogenannten Analogabschaltung führte.

Am 19. August 2005 um 14 Uhr 44 nahm die ARD auf dem Satelliten Astra 19,2° Ost den sogenannten Hörfunktransponder in Betrieb. Seitdem sind praktisch alle deutschen öffentlich-rechtlichen Hörfunkprogramme der ARD in sehr hoher Qualität zu empfangen. Gesendet wird in der DVB-S-Norm. 

Da der Träger-Rauschabstand (CNR) bei Satellitenübertragungen sehr niedrig ist – typisch sind 10 bis 18 dB – (das zu übertragende Nutzsignal liegt nur wenig über dem Grundrauschen zwischen Sender und Empfänger), wird in der Regel bei analoger Übertragung das Signal mittels Frequenzmodulation störresistenter gemacht; bei digitaler Übertragung kommen nur Modulationsverfahren wie 4-PSK, 8-PSK, 16-APSK und 32-APSK in Frage. Derzeitige Satelliten benutzen 4-PSK bzw. 8-PSK und damit eine Symbolrate von 2 bzw. 3 Bit pro Symbol.

Die spektrale Verteilung eines frequenzmodulierten FBAS-Signales bei analoger Übertragung ist allerdings etwas ungünstig, so dass bei schwachen Empfangsbedingungen zwar Bilder mit geringer Farbsättigung noch gut übertragen werden können, Bilder mit intensiven Rottönen jedoch zu sogenannten „Fischchen“ (schwarze und weiße Schmierer) neigen.

Das in den 1980er Jahren entwickelte analoge D2-MAC-Verfahren, das ohne Farbunterträger funktioniert, ist in dieser Beziehung deutlich robuster, auch wenn das nicht das primäre Entwicklungsziel war. Besonders in nördlichen Ländern fand es großen Zulauf.

Die Empfangsprobleme konnten mit den Fehlerkorrektur-Verfahren der "volldigitalen" Fernsehtechnik DVB-S vollständig überwunden werden.

Die einzelnen Sender sind mit einem Kanalraster von 29,5 MHz, wie bei Astra 1F, aufgereiht. Die Bänder der horizontalen und vertikalen Polarisierung sind um jeweils das halbe Kanalraster verschoben. Im Low-Band (10,7–11,9 GHz) waren vorwiegend analoge Sender untergebracht, deren Signal mit FM moduliert war. Trotzdem finden sich einzelne digitale Signale (DVB-S), die z. B. mit einer 8-PSK moduliert werden. Das nebenstehende Bild zeigt eine Aufnahme des Spektrums (bereits nach dem LNB, horizontale Polarisation) von 1453 bis 1653 MHz. Von den 7 abgedeckten Kanälen sind 5 analoge (blau bezeichnet) und 2 digitale (rot) zu sehen.

Da die Signale von Fernsehsender zum Satelliten und wieder zurück zum Zuschauer gesendet werden müssen, entsteht eine Verzögerung zwischen 239 ms (Äquator) und 270 ms (70. Breitengrad) gegenüber dem ausgesendeten Signal.

Diese Verzögerung, die es auch beim analogen Empfang gibt, erhöht sich beim digitalen Empfang durch folgende Maßnahmen weiter:

Übliche Verzögerungszeiten bei digitalem Satellitenempfang sind daher eine bis fünf Sekunden.

Die Rundfunk- und Fernsehsignale werden von einer Uplinkstation auf Uplinkfrequenzen 12,75 – 13,25 GHz, 13,75 – 14,5 GHz und 17,3 – 18,1 GHz zum Satelliten gesendet. Für jeden Transponder wird dazu eine eigene Sendeeinheit mit Parabolantenne verwendet. Diese befinden sich in Uplinkstationen, die entweder dem Satellitenbetreiber gehören, der dort für seine Kunden den Uplink durchführt, oder die Kunden (Sendeanstalten) führen mit eigenen Anlagen den Uplink durch. Das lohnt sich besonders für große Sendeanstalten, die mehrere Transponder belegen, weil sie so die Übertragungsstrecke zur Uplinkstation des Satellitenbetreibers sparen.

Der Satellit sendet auf den Downlinkfrequenzen (10,7 – 11,7 GHz Lowband u. 11,7 - 12,75 GHz Highband ) zu den Empfangsantennen.

Grundsätzlich erfolgt die Umsetzung der Signale beim Satellitenempfang im steuerbaren Aktivteil der Parabolantenne (LNB). Da ein Koaxialkabel bei den hohen Satellitenfrequenzen im SHF-Bereich eine sehr hohe Dämpfung aufweist, konvertiert der LNB die Signale auf die tieferen SAT-ZF-Frequenzen (950–2150 MHz).

Über verschiedene Verkabelungsverfahren (z. B. Satblock-Verteilung oder Einkabelsysteme) werden die Signale zum Verbraucher (Satellitenreceiver) weitergeleitet.

Satellitenradio, den Hörfunkempfang über einen Satelliten, gibt es eigenständig als Digitalradio oder als „Untermieter“ beim Satellitenfernsehen.

Satellitenradio ist in Europa (derzeit) kein Ersatz für den terrestrischen UKW-Hörfunk, da ein mobiler Empfang z.B. per Autoradio praktisch ausgeschlossen ist und weil die Verteilung eines Satellitensignal innerhalb einer Wohnung mit weit höherem Aufwand verbunden ist als einer Wurfantenne. Es gibt auch keinen Radiowecker mit DVB-S-Empfangsteil. 

Deswegen sind hier nur sehr wenige private Hörfunkveranstalter zu finden. Lokale oder regionale Privatsender, die in ihren Sendegebieten oft Marktführer sind, betrachten die sehr teure europaweite Ausstrahlung als unwirtschaftlich. Oft übertragen Programmanbieter ihre Programme über Satellit nur für die Signalzuführung der UKW-Sender. Anders ist die Situation in dünn besiedelten Gebieten. In den USA bieten die Unternehmen XM Satellite Radio und Sirius Radio Radioempfang über Satellit an. Die Gebühren liegen unter 15 $/Monat. In Städten, wo Hochhäuser die Signale abschatten, übernehmen teilweise Pseudolite die Signalausstrahlung.

Im Ku-Band braucht man zum Empfang eine Satellitenschüssel, einen LNB und einen Satellitenreceiver. Im L-Band reicht eine im Empfänger integrierte Antenne und Sichtverbindung zum Satelliten. Über Satellit sind hunderte deutsche und europäische Sender frei und kostenlos empfangbar. Es gibt jedoch auch einige Pay-Radio-Anbieter, die verschiedene Sender mit speziellen Musikrichtungen zum Abonnieren anbieten.





</doc>
<doc id="11985" url="https://de.wikipedia.org/wiki?curid=11985" title="Allerseelen">
Allerseelen

An Allerseelen () begeht die römisch-katholische Kirche das Gedächtnis ihrer Verstorbenen. Das Gedächtnis aller Seelen wird im Kirchenjahr am 2. November begangen, einen Tag nach dem Hochfest Allerheiligen. Durch Gebet, Fürbitte, Almosen und Friedhofsgänge gedenken die Menschen aller Armen Seelen im Fegefeuer und wenden ihnen Ablässe zu. In der römisch-katholischen Kirche hat der Allerseelenablass daher eine besondere Bedeutung.

Wo die Gräbersegnung nicht bereits am Nachmittag von Allerheiligen stattgefunden hat, findet sie an Allerseelen statt, wohin sie eigentlich gehört. Da Allerseelen in der liturgischen Rangordnung den Hochfesten des Herrn gleichgestellt ist, verdrängt das Gedächtnis aller Seelen in Jahren, in denen es auf einen Sonntag fällt, den Sonntag im Jahreskreis. Die liturgische Farbe ist schwarz oder violett, es wird kein Gloria gesungen; an die Stelle des Halleluja-Rufs tritt ein Tractus.

In der evangelischen Kirche wird der Verstorbenen am Ewigkeitssonntag gedacht. In der Selbständigen Evangelisch-Lutherischen Kirche kann der "Gedenktag der Entschlafenen" ebenfalls am 2. November gottesdienstlich begangen werden.

In der Neuapostolischen Kirche wird zum Gedenken der Verstorbenen dreimal im Jahr ein Gottesdienst für Entschlafene gefeiert. Die Gottesdienste finden jeweils am ersten Sonntag im Juli, im März und im November statt. Diese Regelung gilt seit Juli 1954 weltweit.

In Österreich ist Allerseelen zwar kein gesetzlicher Feiertag, es findet jedoch an den öffentlichen Schulen kein Unterricht statt und an einigen Universitäten ist der Tag vorlesungsfrei.

Der Allerseelentag am 2. November geht auf Abt Odilo von Cluny zurück; er hat diesen Gedenktag in allen von Cluny abhängigen Klöstern eingeführt. Das Dekret Odilos aus dem Jahr 998 ist noch erhalten. Bald wurde der Allerseelentag auch außerhalb der Klöster gefeiert. Für Rom ist er seit Anfang des 14. Jahrhunderts bezeugt. Von Cluny aus verbreitete sich der Allerseelentag in der ganzen lateinischen Kirche. Er steht theologisch in enger Verbindung mit der Lehre vom Fegefeuer (Reinigungsort, Purgatorium) als Ort der Läuterung der Verstorbenen, die Hilfe von den Lebenden durch Gebet, Fasten und Almosen erhalten. Allerseelen ist vor allem in den Alpenländern mit zahlreichen Volksbräuchen verbunden.

Nach altem Volksglauben stiegen die Seelen der Verstorbenen an Allerseelen vom Fegefeuer auf und ruhten für kurze Zeit aus. Daher gibt es in vielen Regionen vor allem in Süddeutschland und Österreich Gebildebrote, wie Allerseelenbrot, -zöpfe und ähnliches. Diese wurden den Seelen an besonderen Plätzen, meist den Gräbern, hingelegt.

Vielerorts, unter anderem auf den britischen Inseln, gab es einen Brauch, bei dem arme Kinder Allerseelenkuchen erbetteln konnten. Die Vermutung liegt nahe, dass dieser Brauch über irische Auswanderer nach Amerika gelangte und hier die Wurzeln des Heischebrauchs „Trick or treat“ liegen.

Das mexikanische Totenfest Día de los muertos ging aus einer Vermischung indigener Traditionen mit dem christlichen Allerseelentag hervor.


Jürgen Bärsch: "Allerseelen. Studien zu Liturgie und Brauchtum eines Totengedenktages in der abendländischen Kirche." Aschendorff, Münster 2004, ISBN 3-402-04069-7.



</doc>
<doc id="11986" url="https://de.wikipedia.org/wiki?curid=11986" title="Analoges Fernsehen">
Analoges Fernsehen

Analoges Fernsehen ist ein retronymer Begriff und bezeichnet die Ausstrahlung von Fernsehsignalen, bei denen zumindest die Bilddaten, meist aber auch die Tondaten analog übertragen werden. Aufnahme und Bearbeitung (Schnitt, Effekte usw.) können dabei aber durchaus mittels digitaler Technik erfolgen. Dies ist heutzutage sogar üblich. Im Zuge des Umstiegs auf digitales Fernsehen wird die Übertragung analoger Fernsehprogramme in den kommenden Jahren sukzessive eingestellt. Siehe: Analogabschaltung.

In der Schweiz wurde analoges Fernsehen noch bis 2015 in den Kabelnetzen verteilt.



</doc>
<doc id="11987" url="https://de.wikipedia.org/wiki?curid=11987" title="Digitales Fernsehen">
Digitales Fernsehen

Als Digitalfernsehen bezeichnet man die Ausstrahlung von Fernsehprogrammen in digitalisierter Form. Dazu werden die herkömmlichen (analogen) Bild- und Tonsignale mit Hilfe sogenannter A/D-Wandler in digitale Datenströme (Binärcodes) verwandelt.

Die Herstellung und Verarbeitung der Bild- und Tonsignale geschieht seit Jahren (Stand April 2006) in digitaler Form. Die Verbreitung der Bild- und Tondaten geschieht heute (Stand 05/06) meist in digitaler Form (DVB-T, DVB-C, DVB-IPI, DVB-S, DVB-H, DMB, IPTV). Neue Fernsehgeräte können die digitalen Signale (DVB) direkt verarbeiten ("siehe" IDTV). Für alte, analoge Fernsehgeräte muss der digitale Datenstrom mit entsprechenden Geräten (Digital-Receiver, Set-Top-Boxen) in analoge Signale verwandelt werden. Teilweise geschieht die Verbreitung der Bild- und Tonsignale heute noch auf analoge Weise (Stand 2015/12). Jedoch geht das analoge Fernsehen dem Ende entgegen ("siehe dazu" Analogabschaltung).

Für technische Details bezüglich der analogen und digitalen Signalübertragung siehe die entsprechenden Artikel "Analogsignal" und "Digitalsignal."

Die meisten digitalen Übertragungsnormen arbeiten mit Datenreduktion, d. h. dass die Datenmengen der digitalen Ursprungssignale unter Verlust der Bildqualität vor der Verbreitung reduziert werden müssen, um Datenrate zu sparen (durch Verringerung der Bitrate). Das heute allgemein verwendete Datenreduktionssystem heißt MPEG und existiert sowohl für Audio- als auch Videosignale. Die verschiedenen MPEG-basierten Codes lassen sich zur Datenspeicherung sowie zur Datenübertragung nutzen. Bei der Entwicklung der Reduktionsverfahren steht der größtmögliche Erhalt der Bildqualität bei vorgegebenen Bitraten im Vordergrund. Auch das beliebte MP3-Format oder die gängigen DVD-Normen beruhen auf MPEG. Je nach Übertragungsplattform unterscheidet man beim digitalen Fernsehen (DVB) folgende technischen Varianten: DVB-S (Satellit), DVB-T (terrestrischer Antennenempfang), DVB-C (Kabel), DVB-H (Mobiles Fernsehen), DVB-IPTV (IPTV); alle beruhen (derzeit) auf der MPEG-2 Norm.

Durch die digitale Sendetechnik lassen sich zahlreiche Zusatzdienste mit den bestehenden Fernsehprogrammen kombinieren: z. B. Datendienste mit Zusatzinformationen und Illustrationen im Design von Webseiten; erweiterte Teletextangebote; Senderkennungen, die das rasche Auffinden z. B. im automatischen Suchlauf digitaler Receiver ermöglichen; und nicht zuletzt digitale Programmzeitschriften und Informationen zum Programmablauf (EPG). Durch die Einrichtung eines Rückkanals (im Kabel oder via Satellit) können diese Dienste interaktiv gemacht werden – das heißt, man kann mit dem Anbieter in Verbindung treten, E-Mails versenden oder als Nutzer registriert werden. Als "Triple-Play" bezeichnet man die parallele Verbreitung von Rundfunk, Telefon und Internet über denselben Übertragungsweg (z. B. Kabel).

Digitale Übertragungsverfahren sind wegen ihrer hohen Kompatibilität zu Computersystemen (Empfang, Speicherung, Bearbeitung, Wandlung oder Weiterverbreitung) vorteilhaft und zeitgemäß; durch die geringen Verbreitungskosten über Satellit oder Kabel (fünf bis zehn digitale Fernsehprogramme benötigen etwa die gleichen Übertragungskapazität wie ein analoges Programm) können auch kleinere Anbieter ihre Programme überregional verbreiten. Außerdem können große Fernsehkonzerne ihr bestehendes Angebot kostengünstig um zahlreiche Zusatzkanäle erweitern. Ein großer Vorteil besteht auch in der Kompatibilität von Hörfunk- und Fernsehnormen bei DVB-T, sodass im Datenstrom eines Senderbouquets ein Fernsehsender gegen eine entsprechende Anzahl von Hörfunksendern ausgetauscht werden kann oder umgekehrt und die Betreiber der Sender so besser auf den Bedarf der Programmanbieter reagieren können.

In Berlin ist schon seit 2003 terrestrisches Digitalfernsehen (DVB-T) obligatorisch. Aus Gründen der Kanalknappheit wird fast nur noch ein harter Umstieg (Abschaltung der analogen Signale bei Aufschaltung der digitalen Signale) durchgeführt. Bundesweit war der Umstieg Ende 2008 vollzogen.

Digitale Datenströme lassen sich auch in computerbasierten Netzen, z. B. im Internet verbreiten und mit entsprechender Software auf dem Bildschirm darstellen oder auf Datenträger aufzeichnen (z. B. IPTV). Digitale Signale (englisch "Streams") lassen sich in verschiedenen Normen übertragen, die zwar verwandt aber untereinander nur begrenzt kompatibel sind. Die meisten Empfangsgeräte lassen sich jedoch mittels einer Update-Funktion über Satellit, Internet oder Kabel aktualisieren oder um andere Sendecodes erweitern. Da heutzutage viele Programme verschlüsselt senden, benötigt man zum Empfang des Angebots ein sogenanntes "Empfangsmodul", meist in Form einer Steckkarte, die in den Receiver eingeschoben wird. Wegen der großen Vielfalt an Verschlüsselungssystemen verfügen viele moderne Geräte über mehrere Kartenschächte ("Steckplätze"), um die Dienste verschiedener Anbieter empfangen zu können.

Man nennt diese plattformübergreifenden Geräte CICAM-Receiver = "Common Interface Conditional Access Module". Im Gegensatz zu der Ende der 1990er Jahre vom Kirch-Konzern für dessen damaliges Pay-TV-Angebot Premiere promoteten d-box, die nur zur Entschlüsselung von Premiere geeignet war und andere Pay-TV-Anbieter diskriminierte, enthalten "CICAM-Receiver" Schnittstellen für alle gängigen Verschlüsselungstechnologien (Betacrypt, Nagravision, Videoguard, Viaccess, Cryptoworks u. a.).

Die Registrierung, Aktualisierung und Kundenverwaltung erfolgt dann meist online (bzw. via Satellit). So lassen sich zum Beispiel von Hackern geknackte Verschlüsselungscodes durch andere ersetzen oder säumige Kunden vom Empfang ausschließen. Die angebotenen Geräte verfügen je nach Plattform über verschiedene Technologien, weshalb sich Digitalreceiver für den Satellitenempfang nicht für digitalen Kabelempfang nutzen lassen oder umgekehrt.

Das Digitalfernsehen wurde kommerziell erstmals im Frühjahr 1994 per Satellit unter dem Markennamen DirecTV in den USA angeboten.

Ende 1993 einigten sich zwölf europäische Länder darauf, möglichst rasch eine Spezifikation für die Übertragung digitaler Fernsehsignale über Satellit und Kabel zu erstellen; die Einführung war bereits für 1995 geplant. Das daraus erwachsene DVB wurde in Deutschland dann erstmals am 28. Juli 1996 vom dafür neu geschaffenen Pay-TV-Veranstalter DF1 ("Digitales Fernsehen 1") ausgestrahlt. Wegen Erfolglosigkeit trotz großangelegter Werbekampagne und populärer Sportereignisse wurde es später mit dem Bezahlsender Premiere aus demselben Haus, der Kirch-Gruppe, zusammengelegt. Der mittlerweile börsennotierte Sender konnte seine Kapazität dank der digitalen Sendetechnik stark ausbauen und viele Zusatzprogramme bzw. Themenkanäle (auch von Fremdanbietern) in sein Angebot aufnehmen.

Währenddessen begannen beinahe alle überregionalen deutschen Fernsehsender vorwiegend über die Astra-Satelliten des Betreibers SES parallel zur analogen auch mit der digitalen Ausstrahlung ihrer Programme. Neben zusätzlichen nur für Digital-Nutzer zugänglichen Sendern nutzen vor allem viele neue Spartenprogramme sowie einige Lokalsender die kostengünstige Digitaltechnik. Einige kleinere Fernsehanbieter schicken das Fernsehsignal via DSL oder Glasfasernetz durchs Internet, wo es von den Betreibern einer "Uplink-Station" (meist größere Fernsehsender) aufgenommen und auf den Satelliten überspielt wird. So werden hohe Übertragungskosten, wie sie z. B. bei der teuren Richtfunktechnik entstünden, vermieden. Auf diesem Wege wird die räumliche Distanz zwischen Programmanbieter und Sender problemlos überwunden.

Durch den flächendeckenden Ausbau des Internets und der Einführung von ADSL2 als neuem Übertragungsstandard kann digitales Fernsehen künftig auch direkt aus dem Internet empfangen werden. Durch die hohen Downloadraten bei ADSL2 oder Lichtleiter ist die Empfangsqualität und Störungsresistenz der Programme genauso hoch wie z. B. über das klassische Kabel oder via Satellit. Nach und nach würde die Vision vom „weltweiten Fernsehen“ über, vorerst entlang breitbandiger Datennetze, Realität.

Anfang 2003 begann, etwas später als in vergleichbaren Ländern (z. B. Großbritannien 1996), die Einführung von DVB-T, der terrestrischen DVB-Variante. Im August desselben Jahres nahm Deutschland mit der vollständigen Abschaltung des analogen terrestrischen Fernsehens im Großraum Berlin weltweit eine Vorreiterrolle ein. Nach Berlin folgten weitere Ballungsgebiete mit der Umstellung auf DVB-T. Seit 2004 wurden in Europa einzelne Sendungen oder Versuchskanäle in HDTV-Norm ausgestrahlt. Ende April 2012 wurde das analoge Satellitenfernsehen abgeschaltet.

Eine entgegengesetzte Entwicklung deutet sich in anderen Ländern an: So geben in Großbritannien immer mehr Programme ihre Verschlüsselung auf. Während der 1990er Jahre sendeten die meisten britischen Fernsehprogramme auf der Plattform von British Sky Broadcasting (BSkyB), das neben eigenen Programmen auch fremde Angebote vermarktete und mit einer Grundverschlüsselung versah. Viele dieser Anbieter, darunter auch BBC und ITV, lösten sich von Sky und senden mittlerweile unverschlüsselt.

Derzeit sind mehr als 120 für den britischen Markt bestimmte Fernsehprogramme unverschlüsselt über Satellit zu empfangen, darunter viele kommerzielle Voll- und Spartenprogramme. Sie werden unter den Labels Free-to-air (FTA, Bezeichnung für alle unverschlüsselten Angebote), Freesat (markengeschützter Name für Programmpakete von BBC und ITV) bzw. Freeview (terrestrischer DVB-T-Empfang mit ca. 30 Programmen) vermarktet. Um der Konkurrenz standzuhalten, nahm sogar der Pay-TV-Konzern BSkyB ein eigenes unverschlüsseltes, werbefinanziertes Programm in Betrieb.

Das verschlüsselte Angebot von BSkyB ("Sky Digital") ist das umfangreichste Europas und bietet eine Vielzahl von Film-, Sport- und Unterhaltungskanälen sowie zahlreiche Spartenkanäle auch von Fremdanbietern wie Disney, Discovery, NBC Universal oder Viacom. BSkyB gehört zum Imperium des Medientycoons Rupert Murdoch ("News Corporation").

In vielen europäischen Ländern haben sich verschlüsselte Kabelbouquets etabliert, die meist vollständig ins Kabel eingespeist werden. Viele Sender verschlüsseln aus lizenzrechtlichen Gründen, weil sie lediglich Ausstrahlungsrechte für ein begrenztes Sendegebiet erworben haben. Der Empfang dieser Programme ist jedoch kostenlos, Entschlüsselungskarten ("Smartcards") werden üblicherweise unentgeltlich an die Zuschauer versandt.

In den Niederlanden verschlüsseln alle Vollprogramme ihr Signal über Satellit, weil alle internationalen Produktionen, mit Ausnahme einiger Kindersendungen, in der Originalsprache (mit Untertiteln) ausgestrahlt werden. Film- und Sportangebote werden im "Canal Digitaal"-Bouquet (Kabel, Satellit) vermarktet. Die großen Kabelgesellschaften bieten Digitalpakete, die neben "Canal Digitaal" auch die internationalen Fernsehmarken wie "Discovery" und "MTV" beinhalten. Auch sie werden mit niederländischen Untertiteln versehen.

Ähnlich ist die Situation in Skandinavien und den meisten Ländern Osteuropas, wo der mit der Synchronisation verbundene Aufwand wirtschaftlich unrentabel wäre (Ausnahmen: Russland, teilweise Polen). Um lizenzrechtlichen Problemen aus dem Weg zu gehen, werden meist alle Programme verschlüsselt. Die starke Stellung des Abonnementfernsehens in diesen Ländern erklärt sich auch durch den kleinen Werbemarkt.

In Belgien, insbesondere in Flandern, konzentriert sich das digitale Programmangebot fast völlig aufs Kabel bzw. ADSL. Es gibt kaum Satellitendirektempfang. Auch in der Schweiz werden kostenpflichtige Digitalpakete (v. a. Teleclub) im Kabel angeboten.

In Österreich hat der Satellitenempfang eine starke Stellung; ähnlich wie in Deutschland entfällt der Abonnementbereich fast ausschließlich auf Sky, das gilt auch für den Kabelempfang.

In Frankreich dominieren drei große Pay-TV-Netzwerke, "Canal Satellite numerique" (besser bekannt als "Canal+"), "AB Sat" und "TPS" ("Télévision par satellite"). TPS und Canal+ haben kürzlich fusioniert und werden bald ihre Angebote zusammenlegen. Alle drei kooperieren wiederum mit zahlreichen Fremdanbietern, unter anderem mit den großen nationalen Fernsehsendern. In Frankreich sind nur wenige Programme frei empfangbar.

Über Eutelsat sendet "Sky Italia", der dominierende Digitalfernsehanbieter Italiens. Wie "BSkyB" wird er von Rupert Murdoch beherrscht. "Sky Italia" ging aus der Fusion der beiden einstigen Konkurrenten STREAM und TELE+ hervor.

In Spanien fusionierten die beiden Digitalplattformen "Vía Digital" von Telefónica und "Canal Satélite Digital" von Sogecable zum Monopolisten "Digital +". Der Zusammenschluss war einer einheitlichen Programm- und Strukturpolitik und besseren Verdienstmöglichkeiten geschuldet. Im Kabel dominieren die Marktführer AUNA und ONO. Obwohl in Spanien nahezu alle internationalen Produktionen mit spanischer Synchronisation ausgestrahlt werden, verschlüsseln die meisten nationalen Fernsehkanäle ihr Programm über Satellit. In letzter Zeit ist jedoch ein verstärkter Trend zur unverschlüsselten Ausstrahlung zu verzeichnen; vor allem regionale Netzwerke und viele Spartensender gehen diesen Weg. Ehemalige Pay-TV-Kanäle gehen zunehmend zur werbefinanzierten Ausstrahlung über.

Eine Grundgebühr für werbefinanzierte Fernsehangebote gibt es meist nur im Kabel. Einige kleinere Programmanbieter bevorzugen jedoch nach wie vor eine Mischfinanzierung, die auf Werbung und einer niedrigen Abonnementgebühr beruht. Dieses Grundabonnement wird jedoch auf eine große Anzahl von Spartensendern aufgeteilt, die in sogenannten Basispaketen gemeinsam vertrieben werden. Diese Mischfinanzierung ist üblich, wenn ein Programm durch Werbeeinnahmen allein nicht wirtschaftlich betrieben werden kann, etwa weil das Sendegebiet oder die Zielgruppe zu klein sind.

In der Volksrepublik China und in Kuba wird das dem DVB-T ähnliche "Digital Terrestrial Multimedia Broadcast" (nach der chinesischen Norm GB20600-2006) für die digitale Ausstrahlung von Fernsehprogrammen verwendet.

Mit dem IPTV-Verfahren ("Internet-Protokoll-TV") werden bereits heute zahlreiche Fernsehprogramme unter Verwendung des Internet-Protokolls weltweit verbreitet. Derzeit werden über 200 Themenkanäle in deutscher Sprache angeboten, die über jedes webfähige Endgerät mit ausreichender Rechenleistung empfangen werden können. Dazu gehören unter anderem Informationskanäle größerer Unternehmen wie Markenhersteller oder Reisebüros; mittlerweile werden jedoch auch bereits viele journalistisch hochwertige Spartenkanäle zu unterschiedlichsten Themenbereichen angeboten.

Noch sind die Programme ausschließlich für den Internetempfang konzipiert, sie können jedoch in naher Zukunft auch problemlos in Kabelbouquets eingebunden oder auf anderem Wege weiterverbreitet werden. So kann das Internet künftig auch als reiner Übertragungsweg genutzt werden, Aufbereitung und Empfang der Dienste lägen dann bei den Kabelgesellschaften. "IPTV" ist noch nicht für den Massenempfang konzipiert, kann jedoch durch Aufrüstung mit Multicast-fähigen Servern bereits in wenigen Jahren massentauglich gemacht werden. In letzter Zeit hat sich ein Quasi-Monopol rund um den Münchner Medienunternehmer "Ingo Wolf" etabliert, der zahlreiche Spartenprogramme gestützt auf die IPTV-Technologie meist unverschlüsselt anbietet oder weiterverbreitet.

Künftig dürfte das Internet für alle Fernsehanbieter zum hart umkämpften Terrain werden, RTL und SAT1/Pro7 werden ihre Programme schon in Kürze auch im Internet verbreiten (siehe auch IPTV). Einige Netzbetreiber wie der Kabelanbieter HanseNet mit seinem Fernsehbouquet "Alice" oder die Deutsche Telekom mit ihrem breitbandigen Übertragungsstandard VDSL bieten bereits heute IPTV-Fernsehen (einschließlich des hochauflösenden HDTV) auf Basis von "IP Multicast", wegen der derzeit noch ungenügenden Internetkapazitäten allerdings nur innerhalb der eigenen Netze.

"DVB-T" ist die Abkürzung für den englischen Begriff Digital Video Broadcasting Terrestrial (zu deutsch etwa: Digitales erdgebundenes Fernsehen) und bezeichnet die terrestrische (erdgebundene) Verbreitung digitaler Radio-, Fernseh- und Datensignale in der Erdatmosphäre. DVB-T ist eine Variante des Digital Video Broadcasting (DVB), die vor allem in verschiedenen europäischen, asiatischen und afrikanischen Staaten sowie in Australien als Standard für die Übertragung von digitalem Fernsehen und Hörfunk per Antenne verwendet wird.

Im schweizerischen Engadin fand am 1. Februar 2003 erstmals die Aufschaltung digitaler terrestrischer Fernsehsignale im deutschsprachigen Europa statt. Berlin folgte am 4. August 2003, Österreich bundesweit einheitlich (Multiplex A) erst am 26. Oktober 2006. Während in allen drei Ländern die Analogabschaltung inzwischen abgeschlossen ist, werden in Österreich derzeit noch die regionalen Multiplexe (Mux C) aufgebaut. Private Programme gibt es in Deutschland hauptsächlich in den urbanen Gebieten der sogenannten „Startinseln“ (also dort, wo mit der DVB-T-Umstellung begonnen wurde), in Österreich wird auch landesweites Privatfernsehen verbreitet. In der Schweiz gibt es wegen der hohen Verkabelungsquote derzeit keinen Bedarf an privaten Programmen, weshalb dort lediglich ein Multiplex mit vier bis fünf öffentlich-rechtlichen Programmen empfangbar ist. In Südtirol gibt es ein reichhaltiges Angebot italienisch- und deutschsprachiger Programme aus allen vier Ländern, die von der Rundfunk-Anstalt Südtirol verbreitet werden, darunter, wie in ganz Italien, einige Programme auch in hochauflösender Qualität.

In Deutschland werden meist vier TV-Programme pro Multiplex verbreitet, in anderen Ländern auch mehr (in der deutschsprachigen Schweiz fünf Programme) oder weniger, zusätzlich Radioprogramme (in Österreich und teilweise in Deutschland) oder HDTV-Inhalte (z. B. in Italien). Darüber hinaus setzt man in Deutschland von Anfang an gebietsweise auf den Aufbau von Gleichwellennetzen (SFNs) und sowohl UHF als auch VHF, während manche andere Länder zumindest anfangs Mehrfrequenznetze (MFNs) benutzen oder sich auf UHF beschränken.

Die vollständige Umstellung auf DVB-T, also die Abschaffung der analogen Ausstrahlung von Fernsehprogrammen, war in Deutschland laut Beschluss der Bundesregierung bis spätestens zum Jahr 2010 vorgesehen. Allgemein soll die Ausstrahlung des analogen Rundfunks (Fernsehen und Hörfunk) in der EU im April 2012 enden (siehe dazu Analogabschaltung).

Die Übertragung erfolgt in Europa wie in den meisten Staaten der Welt im DVB-Standard, der neben Fernseh- und Radioprogrammen auch Zusatzinformationen (z. B. Untertitel, EPG) und neue Datendienste (z. B. Texte, PC-Daten, MHP) bietet.
Es gibt verschiedene Übertragungsmöglichkeiten für DVB:


Japan nutzt ein verwandtes Verfahren (ISDB), während die USA zumindest für den Antennenempfang ihr ATSC-Format propagieren.

Die Bandbreite eines typischen Transponders (Satellitenkanal) auf dem bekannten Rundfunksatelliten Astra beträgt meist 33 MHz, das entspricht einer Nettodatenrate von 36 Mbit/sec bzw. einer Symbolrate von 26 Msymb/s. Ein Fernsehsender benötigt üblicherweise etwa 3-4 Mbit/sec, je nach seiner Symbolrate (Kodierung von je 2 Bit zu einem Symbol). Sie bewegt sich zwischen 2,2 und 2,75 Msymb/s. Gemeinsam mit der Fehlerkorrektur (üblicherweise 3/4 Nutz- zu Korrekturdaten) ergibt sich die erforderliche Datenrate. Je schneller sich das Bild ändert (Veränderungen im Bildaufbau), umso höher ist die erforderliche Datenrate. Sport- und Actionszenen erfordern also mehr Bandbreite als bewegungsarme Sequenzen (Talkshows etc.). Ist die Bandbreite zu schmal berechnet, kommt es bei schnellem Szenenwechsel bei höherer Auflösung zu Artefakten (Bauklötzchen-Effekten).

Üblicherweise werden über einen Transponder auch noch Hörfunkprogramme mit einer Datenrate zwischen 64 und 320 kbit/sec im MPEG1-L2-Standard sowie diverse Datendienste übertragen. Über einen reinen Fernsehtransponder lassen sich bequem acht Fernsehprogramme in guter Qualität übertragen, jedoch nur ein analoges. Durch die Einführung des effizienteren MPEG-4-Codecs lässt sich künftig noch effektiver komprimieren, bei gleich bleibender Bild- und Tonqualität lassen sich künftig mehr als 10 Fernsehprogramme je Transponder übertragen. Durch das zeitaufwendige Kodieren und Dekodieren des Fernsehsignals kommt es gegenüber der analogen Ausstrahlung zu mehrsekündigen Verzögerungen. Je effizienter komprimiert wird, desto komplizierter ist der Kodierungsalgorithmus.

Bei DVB wird erst das Fernsehprogramm in MPEG-2 codiert, anschließend wird das Programm mit eventuellen Zusatzdiensten im Multiplexer verschachtelt. Mehrere Programme (Fernsehen, Radio etc.) innerhalb eines Paketes lassen sich so zu einem Datenstrom kombinieren, der auch Transportstrom genannt wird. Anschließend wird der Datenstrom mit dem Trägersignal moduliert, verstärkt und zum Sender bzw. Satelliten bzw. zur Kabelkopfstation gelenkt.

Die folgende Tabelle erläutert die Unterschiede zwischen Analog- und Digitalfernsehen anhand einzelner Aspekte.
Die Marktanteile für Digitalfernsehen in Deutschland jeweils zum 1. Januar eines Jahres auf Basis der Daten der Arbeitsgemeinschaft Fernsehforschung (AGF), TV Scope, Basis Fernsehpanel D+EU:




</doc>
<doc id="11989" url="https://de.wikipedia.org/wiki?curid=11989" title="Marianne Bachmeier">
Marianne Bachmeier

Marianne Bachmeier (* 3. Juni 1950 in Sarstedt; † 17. September 1996 in Lübeck) beging 1981 in einem Saal des Landgerichtes Lübeck Selbstjustiz und erschoss den mutmaßlichen Mörder ihrer Tochter Anna Bachmeier. Sie wurde 1983 wegen Totschlags und unerlaubten Waffenbesitzes zu einer Freiheitsstrafe von sechs Jahren verurteilt.

Marianne Bachmeier wuchs in Sarstedt auf, wohin ihre Eltern aus Ostpreußen geflüchtet waren. Das Paar trennte sich, und die Mutter heiratete erneut.

Im Alter von 16 Jahren wurde Marianne Bachmeier Mutter und mit 18 Jahren wieder von ihrem damaligen Lebensgefährten schwanger. Kurz vor der Entbindung ihrer zweiten Tochter wurde sie vergewaltigt. Ihre ersten beiden Kinder gab sie jeweils kurz nach der Geburt zur Adoption frei. Im Jahr 1973 kam ihre dritte Tochter Anna zur Welt, die bei ihr aufwuchs. Marianne Bachmeier betrieb in Lübeck ein Restaurant.

Am 5. Mai 1980 ging die damals siebenjährige Anna Bachmeier aus Trotz gegen ihre Mutter nicht zur Schule. Sie wollte eine gleichaltrige Freundin besuchen und fiel dabei dem 35-jährigen Fleischer Klaus Grabowski in die Hände. Er soll sie bei sich zu Hause mehrere Stunden festgehalten und anschließend mit einer Strumpfhose erdrosselt haben. Laut Staatsanwaltschaft habe er das Mädchen gefesselt, in einen Karton gepackt und diesen dann am Ufer eines Kanals in einer Mulde abgelegt. Die Leiche soll er anschließend in ein Loch gelegt und mit Erde bedeckt haben. Am Abend wurde er in der Gaststätte "Im alten Zolln" festgenommen.

Klaus Grabowski war ein vorbestrafter Sexualstraftäter und zuvor wegen sexuellen Missbrauchs zweier Mädchen verurteilt worden. Während seiner Haft ließ er sich 1976 kastrieren und unterzog sich zwei Jahre später einer Hormonbehandlung. Marianne Bachmeier und Christian Berthold (Annas leiblicher Vater) erstatteten später erfolglos Strafanzeige gegen den Urologen, der die Hormonbehandlung bei Grabowski angewandt und somit ihrer Meinung nach dessen Gefährlichkeit wiederhergestellt hatte.

Grabowski gab an, das Mädchen nicht sexuell belästigt zu haben. Bei der polizeilichen Vernehmung sagte er, das Mädchen habe der Mutter erzählen wollen, er habe es unsittlich berührt mit dem Ziel, von ihm eine D-Mark zu erpressen.

Marianne Bachmeier schmuggelte am 6. März 1981 eine Pistole des Typs Beretta M1934 in den Gerichtssaal des Lübecker Landgerichts und erschoss damit am dritten Verhandlungstag im Strafprozess den wegen Mordes an ihrer Tochter Anna angeklagten Klaus Grabowski. Sie zielte mit der Waffe auf Grabowskis Rücken und drückte insgesamt achtmal ab. Sechs der Schüsse trafen; der 35-jährige Angeklagte war sofort tot.

Der bislang bekannteste Fall von Selbstjustiz in der Bundesrepublik löste ein großes Medienecho aus und wurde in der Öffentlichkeit kontrovers diskutiert. Fernsehteams aus aller Welt reisten nach Lübeck, um über diesen Fall zu berichten. Reporter befragten auf der Straße zahlreiche Passanten über die Tat der 31-jährigen alleinerziehenden Frau. Ein Großteil der Bevölkerung zeigte Verständnis für die Tat, andere verurteilten sie als mit der Rechtsstaatlichkeit unvereinbar. Marianne Bachmeier verkaufte ihre Lebensgeschichte für rund 250.000 D-Mark exklusiv an das Nachrichtenmagazin "Stern". Sie vertraute sich dem "Stern"-Reporter Heiko Gebhardt an, der sie während ihrer Untersuchungshaft besuchen durfte.

Am 2. November 1982 wurde Marianne Bachmeier vor Gericht zunächst wegen Mordes angeklagt. Später ließ die Anklage den Mordvorwurf fallen. Nach 28 Verhandlungstagen erging am 2. März 1983 durch die Schwurgerichtskammer des Landgerichts Lübeck die Verurteilung wegen Totschlags und unerlaubten Waffenbesitzes zu sechs Jahren Haft. Damit ging einer der aufsehenerregendsten Prozesse der Nachkriegszeit zu Ende. Nach drei Jahren Haft wurde sie vorzeitig aus dem Gefängnis entlassen.

Marianne Bachmeier heiratete 1985 und zog 1988 mit ihrem Ehemann, einem Lehrer, nach Lagos in Nigeria. Dort lebten sie in einem deutschen Camp, in dem ihr Lebensgefährte an der deutschen Schule unterrichtete. Sie ließ sich 1990 von ihm scheiden und ging nach Sizilien, wo sie in einem Hospiz in der Hauptstadt Palermo Sterbende pflegte. Nachdem sie erfahren hatte, dass sie an Krebs erkrankt war, kehrte sie nach Deutschland zurück.

Im Jahr 1994, 13 Jahre nach ihrer Tat, gab sie ein Interview im Deutschlandfunk: „Ich finde es einen sehr großen Unterschied, ob ich ein kleines Mädchen umbringe, weil ich Angst habe, ich muss danach ins Gefängnis für mein Leben lang. Und dann auch das ‚Wie‘, also dass ich mich hinter das Mädchen stelle und zuziehe, das ist jetzt wörtlich aus seiner Aussage: ‚Ich höre, dass etwas aus ihrer Nase tritt, dann ziehe ich fester zu, dann kann ich den Anblick der Leiche nicht mehr ab‘.“

Am 21. September 1995 trat sie in der Talkshow "Fliege" in der ARD auf. Sie gab zu, dass sie den mutmaßlichen Mörder ihrer Tochter nach reiflicher Überlegung erschossen habe, um Recht über ihn zu sprechen und ihn daran zu hindern, weiter Unwahrheiten über Anna zu verbreiten.

Am 17. September 1996 starb Marianne Bachmeier im Alter von 46 Jahren an Bauchspeicheldrüsenkrebs in einem Lübecker Krankenhaus. Es war eigentlich ihr Wunsch gewesen, in ihrer Wahlheimat Palermo zu sterben. Vor ihrem Tod bat sie den NDR-Reporter Lukas Maria Böhmer, sie mit der Filmkamera in ihrem letzten Lebensabschnitt zu begleiten. Auf dem Burgtorfriedhof in Lübeck wurde sie im Grab ihrer Tochter Anna beigesetzt.




</doc>
<doc id="11990" url="https://de.wikipedia.org/wiki?curid=11990" title="Außenminister">
Außenminister

Der Außenminister ist Mitglied der Regierung eines souveränen Staates. Er ist der Hauptverantwortliche bei der Umsetzung der Außenpolitik eines Landes.

Zudem ist der Außenminister im Ausland neben dem Regierungschef der höchste Vertreter eines Staates und wird außerhalb seines Landes mit "Seine Exzellenz" (S.E.) angesprochen. Er gilt im Bereich der internationalen Beziehungen in der Regel als vollumfänglich bevollmächtigter Vertreter seines Landes.

Mit 40 Jahren Dienstzeit (1975–2015) war Saud ibn Faisal als Außenminister des Königreichs Saudi-Arabien am längsten im Amt (Stand 2017).

In Deutschland lautet die offizielle Bezeichnung "Bundesminister des Auswärtigen", er leitet das Auswärtige Amt. Seit dem 14. März 2018 ist Heiko Maas (SPD) Außenminister im Kabinett Merkel IV.

Die offizielle Bezeichnung in Österreich lautet gegenwärtig "Bundesminister für Europa, Integration und Äußeres", er leitet das Bundesministerium für Europa, Integration und Äußeres (BMEIA). Derzeit amtierende Ministerin ist Karin Kneissl (von der FPÖ nominiert, parteilos).

In der Schweiz lautet die Bezeichnung des Regierungsmitgliedes, das dem Bereich Außenpolitik vorsteht, dem Departement für auswärtige Angelegenheiten, "Vorsteher des Eidgenössischen Departements für auswärtige Angelegenheiten". Derzeit amtierender Bundesrat ist Ignazio Cassis (FDP).

Im Fürstentum Liechtenstein ist Regierungsrätin Aurelia Frick (FBP) die Außenministerin und ist verantwortlich für die Ressorts Äußeres, Justiz und Kultur.

In Italien ist die offizielle Bezeichnung des für die Außenpolitik Zuständigen Minister der Auswärtigen Angelegenheiten ("Ministro degli Affari Esteri"). Derzeitiger Amtsinhaber ist Angelino Alfano.

In der Türkei lautet die offizielle Bezeichnung "Dışişleri Bakanı" (deutsch: Minister für Auswärtige Angelegenheiten), er leitet das "Dışişleri Bakanlığı" (deutsch: Ministerium für Auswärtige Angelegenheiten). Derzeitiger Amtsträger ist seit dem 24. November 2015 Mevlüt Çavuşoğlu im Kabinett Yildirim der AKP.

In den Vereinigten Staaten von Amerika ist der Vorsteher des "State Departments", der "Secretary of State" für die Außenpolitik zuständig. Er ist das ranghöchste Mitglied des Kabinetts. Seit dem 13. März 2018 ist John J. Sullivan (Republikaner) geschäftsführender Außenminister.

In der Europäischen Union lautet die offizielle Bezeichnung Hoher Vertreter der EU für Außen- und Sicherheitspolitik, er leitet den Europäischen Auswärtigen Dienst. Derzeitige Amtsinhaberin ist Federica Mogherini.



</doc>
<doc id="11992" url="https://de.wikipedia.org/wiki?curid=11992" title="Johann Albrecht Bengel">
Johann Albrecht Bengel

Johann Albrecht Bengel (* 24. Juni 1687 in Winnenden; † 2. November 1752 in Stuttgart) war ein schwäbischer lutherischer Theologe und ein Hauptvertreter des deutschen Pietismus.

Bengel studierte von 1703 bis 1706 an der Universität Tübingen (als Stipendiat des Evangelischen Stifts) zunächst die Freien Künste, dann Theologie. Danach wurde er Stiftsrepetent in einer Zeit, als das Stift stark vom Radikalen Pietismus beeinflusst war. Obwohl Bengel sich nie von der Kirche trennte, sondern dort Karriere machte, hat sich diese Prägung stark ausgewirkt, vor allem in seiner intensiven Beschäftigung mit der Offenbarung des Johannes. Der junge Mann wurde Vikar – unter anderem in Metzingen – und trat nach einer Studienreise nach Halle (Saale) 1713 als Lehrer in das evangelische Kloster Denkendorf (Württemberg) ein.

In Denkendorf beeinflusste er zwei Generationen von Schülern, die als pietistische Pfarrer in Erscheinung traten und eine erhebliche Wirksamkeit innerhalb der Landeskirche entfalteten. 1741 wurde er Prälat von Herbrechtingen, 1749 Abt von Alpirsbach.

Mit Zinzendorf hatte er 18 Jahre lang Streit, wodurch es zu einem Bruch zwischen der Herrnhuter Brüdergemeine und der von Bengel vertretenen Richtung des württembergischen Pietismus kam. In diesem Streit stellte sich Bengel mit seiner klaren systematisierenden Einsicht in den göttlichen Heilsplan gleichsam dogmatisch gegen das dynamische, allem Systematischen abholde ökumenisch-missionarische Streben Zinzendorfs. So schreckte Bengel nicht vor chronologischen Manipulationen historischer Kalendarien zurück, was Zinzendorf gleichsam als abergläubische „zeichen-deuterey“ abtat.

Bengel kam 1747 in den Landtag und wurde 1751 Dr. theol. h. c.

Von den 12 Kindern, die er mit seiner Frau, Johanna Regina Bengel, geb. Seeger, hatte, erreichten sechs das Erwachsenenalter. Ernst Bengel (1735–1793), ein Sohn Johann Albrechts, schlug später ebenso die theologische Laufbahn wie sein Vater ein. Johann Albrecht Bengels Schüler Philipp David Burk (1714–1770) wurde sein Hauslehrer, später sein Schwiegersohn und veröffentlichte ein umfangreiches Buch über die Rechtfertigung.

Bengel gilt als der wichtigste württembergische Pietist des 18. Jahrhunderts und tat sich besonders in der Exegese des Neuen Testaments und seinen chiliastischen Endzeittheorien hervor. Er ist einer der Begründer der Textkritik des Neuen Testaments, da er einen beträchtlichen Teil seiner Forschungsarbeit darauf verwendete, die Lesevarianten zu untersuchen, die durch die verschiedenen Manuskripte überliefert waren. Er kam dabei zu dem in der Textkritik noch immer angewendeten Grundsatz, dass die „"schwierigere Lesart der leichteren vorzuziehen sei"“. Dieser Grundsatz basiert auf der Erkenntnis, dass wenn Kopisten ihre Texte bewusst veränderten, dies in der Regel passierte, weil sie versuchten, den Text zu verbessern oder zu harmonisieren. Um den älteren und damit möglicherweise originaleren Text erkennen zu können, ist die „"schwierigere"“ Lesart in der Regel vorzuziehen. Auf Bengel geht außerdem die Methode der Textkritik zurück, die Dokumente in eng miteinander verbundene Gruppen einzuteilen. Damit wird eine Stammlinie von Dokumenten entwickelt.

1734 erschien eine textkritische Ausgabe des Neuen Testaments, wo er an der Verbalinspiration festhielt, welche er mit dem auf Luther („scriptura sui ipsius interpres“) zurückgehenden Satz begründete: „Die Heilige Schrift wird durch nichts sicherer als durch sich selbst ausgelegt“.

1740 erschien die "Erklärte Offenbarung Johannis", worin er aus in chiliastischer und postmillenaristischer Manier den Beginn des ersten eschatologischen Millenniums (Zeitraums von tausend Jahren) für den 18. Juni 1836 berechnete. Bengel vertrat in der Eschatologie einen Dischiliasmus, d. h. die Auffassung, dass der persönlichen Wiederkunft Christi und dem Jüngsten Gericht ein Zeitraum von zweimal (griech. „dís“) tausend Jahren (vgl. griech. „dischília éte“ = 2000 Jahre) vorausgehen sollte. 1742 veröffentlichte er den lateinischen "Gnomon Novi Testamenti", einen um Genauigkeit bemühten Kommentar zum Neuen Testament, der den wahren Sinn des Textes aufschließen, aufzeigen sollte. „Gnomon“ bedeutet „Zeiger“, ursprünglich den Schattenzeiger an der Sonnenuhr; mit dieser emblematisch (sinnbildlich) ausgerichteten Begriffswahl weist Bengel auf sein Interesse an der seiner Ansicht nach chronologisch fassbaren, berechenbaren Heilsgeschichte hin.

Noch lange nach seinem Tod wirkten die Gedanken Bengels in Württemberg nach. Beispielsweise wurde auch Johann Tobias Beck von ihm geprägt. Beck beeinflusste seinerseits einen Sohn seines Schülers Fritz Barth, den weltberühmten Theologen Karl Barth, in dessen Frühzeit. Freilich wurden Bengels Ideen von einem Teil seiner Anhänger stark umgedeutet. Der Akzent wurde bei diesen im Anschluss an auf die dem Beginn des ersten Jahrtausends vorausgehenden Gerichte gelegt, und sie gingen im Gegensatz zu Bengel von einer persönlichen Wiederkunft Christi im Jahre 1836 aus. Als im frühen 19. Jahrhundert Missernten und Fehlherbste auftraten und Württemberg in eine schwere Krise stürzte, schienen sich solche Erwartungen zu erfüllen. Deshalb kam es 1816/17 zu einer starken Auswanderungsbewegung in den Kaukasus, das damalige "Südrussland" (siehe auch Kaukasiendeutsche). Da Palästina wegen der osmanischen Herrschaft nicht zugänglich war, wollte man dem wiederkommenden Christus wenigstens ein Stück weit entgegen ziehen. Aber 1836 blieb die erwartete Wiederkunft aus.

Das Albrecht-Bengel-Haus in Tübingen ist ein Studienhaus für pietistisch geprägte Theologiestudentinnen und -studenten, die ihr Studium an der Universität Tübingen in diesem Geist absolvieren wollen. In Bengels Geburtsstadt Winnenden existiert ein Bengelplatz sowie das dort gelegene "Café Bengel." Im Kloster Denkendorf wurde in Bengels früherer Amtsstube eine kleine Gedenkstätte, das „Bengelstüble“, eingerichtet. Seit Aufgabe der Landeskirchlichen Fortbildungsstätte stehen die Vitrinen leer. Die Evangelische Kirche in Deutschland erinnert mit einem Gedenktag im Evangelischen Namenkalender am 2. November an Bengel.





</doc>
<doc id="11994" url="https://de.wikipedia.org/wiki?curid=11994" title="Allerheiligen">
Allerheiligen

Allerheiligen () ist ein christliches Fest, zu dem aller Heiligen gedacht wird, der „verherrlichten Glieder der Kirche, die schon zur Vollendung gelangt sind“, der bekannten wie der unbekannten. Das Fest wird in der Westkirche am 1. November begangen, in den orthodoxen Kirchen am ersten Sonntag nach Pfingsten. Allerheiligen ist ein Hochfest und in nahezu allen Diözesen ein gebotener Feiertag der katholischen Kirche und ein "Principal Feast" der anglikanischen Kirche. Die liturgische Farbe ist Weiß. Die lutherischen Kirchen feiern es als Gedenktag der Heiligen (hier ist die liturgische Farbe Rot), ähnlich auch weitere protestantische Kirchen.

Im Lauf der ersten Jahrhunderte wurde es wegen der steigenden Zahl von Heiligen zunehmend schwierig, jedes Heiligen an einem eigenen Fest zu gedenken. Jährliche Gedenktage für Verstorbene gab es bereits im antiken Christentum. In der Ostkirche finden sich seit Anfang des 4. Jahrhunderts dann ausdrücklich Allerheiligenfeste, die als "Herrentag aller Heiligen" am 1. Sonntag nach Pfingsten gefeiert wurden. In der Westkirche weihte Papst Bonifatius IV. am 13. Mai 609 oder 610 das zuvor allen Göttern Roms geweihte Pantheon der Jungfrau Maria und allen Märtyrern "(Sancta Maria ad Martyres)" und ordnete eine jährliche Feier an, zunächst am Freitag nach Ostern, da das Fest inhaltlich stark von Ostern her geprägt ist. Papst Gregor III. weihte über hundert Jahre später eine Kapelle in der Basilika St. Peter allen Heiligen und legte daher für die Stadt Rom den Feiertag auf den 1. November. Ende des 8. Jahrhunderts begann man an diesem Tag das Fest vor allem auch in Frankreich zu feiern, und der Termin verbreitete sich allmählich in der gesamten Westkirche, bis Papst Gregor IV. 835 Allerheiligen für die gesamte Westkirche auf den 1. November festlegte. Seit Ende des 10. Jahrhunderts wird, ausgehend von der Benediktinerabtei Cluny, am 2. November mit Allerseelen zusätzlich ein Gedenktag aller Verstorbenen gehalten, die sich nach katholischem Verständnis im Purgatorium befinden und die volle Gemeinschaft mit Gott noch nicht erreicht haben.
In Österreich, Belgien, Liechtenstein, den katholisch geprägten Kantonen der Schweiz (siehe Feiertage in der Schweiz), den katholisch geprägten deutschen Bundesländern Baden-Württemberg, Bayern, Nordrhein-Westfalen, Rheinland-Pfalz und Saarland sowie in Luxemburg, Italien, Frankreich, Spanien, Portugal, Polen, Ungarn, Kroatien, Slowenien, Slowakei, Litauen und den südostasiatischen Philippinen ist Allerheiligen ein gesetzlicher Feiertag und wird am 1. November begangen. In Schweden und Finnland fällt der Feiertag auf den Samstag zwischen dem 31. Oktober und 6. November. Beide Daten liegen normalerweise in Kalenderwoche 44, nur in Schaltjahren, die mit einem Donnerstag beginnen (Sonntagsbuchstabe DC, 13mal in 400 Jahren), liegen beide in KW 45 nach DIN ISO 8601. 
In den Niederlanden wurde Allerheiligen als gesetzlicher Feiertag 1960 abgeschafft.

In den genannten deutschen Bundesländern ist Allerheiligen ein sogenannter stiller Feiertag. Das heißt, dass an diesem Tag keine öffentlichen Tanzveranstaltungen durchgeführt werden dürfen und laute Musik verboten ist.

Als Allerheiligengebäck kennt man im süddeutschen Sprachraum den Allerheiligenstriezel, den die Tauf- oder Firmpaten an ihre Patenkinder verschenken. Den Brauch gibt es vom Burgenland über das oberösterreichische Inn- und Hausruckviertel bis zum südostbayerischen Chiem- und Rupertigau. Der Striezel wird aus Germteig (Hefeteig) in Form geflochtener, mit Hagelzucker oder Streuseln bestreuter Zöpfe hergestellt. In der nördlichen Oberpfalz heißt dieses Allerheiligengebäck „Strietzl“. In der Region um das Altmühltal gibt es an Allerheiligen auf Spitzlmärkten „Spitzl“ (Spitzel), ein rautenförmiges Lebkuchengebäck. Zu Allerheiligen finden auch Allerheiligenmärkte statt.

Am Tag nach Allerheiligen begeht die römisch-katholische Kirche den Allerseelentag, an dem der Armen Seelen im Fegefeuer gedacht wird. Vielerorts wird die damit verbundene Gräbersegnung bereits am Nachmittag von Allerheiligen, dem arbeitsfreien staatlichen Feiertag, vorgenommen. Damit verbunden ist der Brauch, die Gräber vor allem mit Lichtern besonders zu schmücken. Auf den Mainzer Friedhöfen wird die traditionelle Mainzer Kerze, der Newweling, entzündet.

Im englischen Sprachraum wird Allerheiligen (engl. „All Saints“, „All Saints’ Day“) häufig mit dem Prozessionslied "For All the Saints" begangen.

Am Vorabend, dem 31. Oktober, wird in den Vereinigten Staaten und vielen Ländern Europas Halloween gefeiert. Das Wort "Halloween" leitet sich aus der englischen Bezeichnung "All Hallows Eve", dem liturgischen Vorabend von Allerheiligen, ab. In der heutigen, aus Nordamerika zurückgekommenen Form hat es eine stark kommerzialisierte und säkularisierte Form angenommen.


</doc>
<doc id="11998" url="https://de.wikipedia.org/wiki?curid=11998" title="Fidschi">
Fidschi

Fidschi, offiziell Republik Fidschi (auf Fidschi: "Viti" bzw. "Matanitu ko Viti"; bzw. "Republic of Fiji") ist ein Inselstaat im Südpazifik nördlich von Neuseeland und östlich von Australien. Fidschi ist seit 1970 unabhängig und seit 1987 eine Republik mit der Hauptstadt Suva auf der Insel Viti Levu. Seit der demokratischen Wahl am 17. September 2014 ist Fidschi eine parlamentarische Demokratie. Der vorherige Premierminister Frank Bainimarama gewann 59,20 Prozent der Stimmen und wurde damit wiedergewählt.

Fidschis jüngere Geschichte wurde dominiert durch den Konflikt zwischen der indigenen fidschianischen Mehrheit und der indischstämmigen Minderheit des Landes.

Fidschi ist eine isolierte Inselgruppe im Südwestpazifik und liegt etwa 2100 Kilometer nördlich von Auckland, Neuseeland. Die Küstenlänge beträgt insgesamt 1129 Kilometer. Der Archipel besteht aus 332 Inseln mit einer Gesamtfläche von 18.333 Quadratkilometern, von denen 110 bewohnt sind. Außerhalb des Fidschi-Archipels gehört noch die Insel Rotuma im äußersten Norden sowie das Atoll Ceva-i-Ra im äußersten Südwesten zum Staatsgebiet Fidschis.

Die Hauptinseln Viti Levu (10.429 km²) und Vanua Levu (5.556 km²) umfassen fast neun Zehntel der Landesfläche und sind wie die meisten der Fidschiinseln vulkanischen Ursprungs, die kleineren entstanden aus Korallenriffen. Auf den beiden großen Inseln liegen zerklüftete Gebirgsketten, die sich über schmalen Küstenebenen und vorgelagerten Korallenriffen erheben. Der höchste Berg ist mit der "Tomanivi" (vormals: "Mount Victoria") auf Viti Levu. Neben Viti Levu und Vanua Levu sind noch Taveuni (470 km²), Kadavu (411 km²) sowie Gau und Koro (je 140 km²) von gewisser Größe.
Es herrscht tropisch-heißfeuchtes Klima mit Temperaturen von 16 bis 32 Grad Celsius. In der niederschlagsreicheren Sommerzeit von November bis April suchen tropische Wirbelstürme (Zyklone) den Südpazifik heim. Statistisch wird die Inselgruppe in 10 Jahren von 10 bis 12 Wirbelstürmen betroffen, wovon zwei bis drei schwere Schäden verursachen. In den Wintermonaten von Mai bis Oktober bringen beständige Südostwinde kühlere Luft. Diese trifft auf warme äquatoriale Luft (South Pacific Convergence Zone) und verursacht Wolkenbildung und Regenfälle. Diese durch Passatwinde verursachten Wetterlagen bringen speziell der bergigen, dicht bewaldeten Südostküste der Hauptinsel (Viti Levu) starke Regenfälle. Die westlichen Hänge mit lichtem Baumbestand und Grasland sind wesentlich trockener. Über zwei Drittel der Landesfläche von Fidschi sind bewaldet und nur ein geringer Anteil ist Weideland.

Vor 2007 fanden in der Central und der Western Division Gebietsreformen statt, von denen insbesondere Suva betroffen war, die Gebiete an die neugegründete Stadt Nasinu und an Nausori abtreten musste. Zusammen mit Nausori und Nasinu bildet Suva die Agglomeration "Suva-Nausori-Korridor". Die größten Städte sind (Stand 16. September 2007): Nasinu mit 87.446 Einwohnern, Suva mit 85.691 Einwohnern, Lautoka mit 52.220 Einwohnern, Nausori mit 47.604 Einwohnern und Nadi mit 42.284 Einwohnern. In Fidschi werden zwei Kategorien von Städten unterschieden, die 12 "incorporated cities/towns", die sich aus Stadtgebiet und Umland zusammensetzen und über eine eigene Stadtverwaltung verfügen, sowie die sieben "unincorporated towns" ohne eigene Stadtverwaltung.

An den Küsten finden sich Mangroven, während im Inneren der größeren Inseln natürlicherweise entweder tropischer Regenwald (an den Luvseiten) oder aber Savannenformationen vorherrschen. Aufgrund seiner isolierten Lage beherbergt Fidschi eine eigenständige Pflanzen- und Tierwelt, die sich aus Arten zusammensetzt, die selbst oder deren Vorfahren die Weiten des Pazifischen Ozeans vor allem von Westen her überqueren konnten. Dies geschah entweder aus eigener Kraft (etwa Seevögel), passiv z. B. durch Drift mittels Treibgut oder möglicherweise auch im Zuge katastrophaler Ereignisse wie Tsunamis oder tropischer Wirbelstürme.
Die Süßgewässer von Fidschi werden von insgesamt 53 Arten von Süß- und Brackwasserfischen bevölkert, dabei sind vor allem die Familien der Grundeln und der Schläfergrundeln, mit neun bzw. mit sechs Arten vertreten. Der Milchfisch, drei Arten der Flaggenschwänze, vier Arten von Meeräschen und drei Arten der Seenadelgattung "Microphis" leben küstennah im Brackwasser. Raubfische sind die drei Arten der Flussaale, die ins Süßwasser wandernde Muräne "Gymnothorax polyuranodon" und der Barramundi. Vom Menschen eingeführt wurden einige Karpfenfische, darunter der Karpfen, der Silberkarpfen und der Blaubandbärbling, Lebendgebärende Zahnkarpfen, wie Guppy, Schwertträger und Moskitofisch, sowie drei Buntbarscharten der Gattung "Oreochromis".

Bemerkenswert ist auch das Vorkommen von zwei hier endemischen Froscharten – "Platymantis vitianus" (engl. Name: „Fiji Ground Frog“) und "Platymantis vitiensis" (engl. Name: „Fiji Tree Frog“). Abgesehen von der vom Menschen eingeführten Aga-Kröte sind dies die einzigen Amphibien Fidschis. Beide weisen eine direkte Entwicklung auf – aus ihren an Land abgelegten Eiern schlüpfen also fertig entwickelte Jungfrösche, keine Kaulquappen. "Platymantis vitiensis" gehört außerdem zu den wenigen Froschlurcharten weltweit, bei denen nicht nur die Männchen, sondern auch die Weibchen zu Lautäußerungen fähig sind. Insbesondere der bodenbewohnende Frosch "Platymantis vitianus", der ehemals auf vielen Inseln Fidschis heimisch war, ist inzwischen stark gefährdet. Hauptgrund ist die Einführung von Mangusten – hier wohl des Kleinen Mungos – zum Zweck der Rattenbekämpfung. Heute kommt diese Froschart daher fast nur noch auf Inseln ohne Mungos vor (Ovalau, Taveuni, Gau, Viwa sowie eine Einzelpopulation auf Vanua Levu).

Erst im Jahr 1979 wurde eines der größten Reptilien der Inselgruppe entdeckt und zwei Jahre später wissenschaftlich beschrieben, der Fidschi-Kammleguan ("Brachylophus vitiensis"). Diese grüne Echse mit weißen „Zebrastreifen“, gelben Nasenlöchern und einem Rückenkamm aus Hornspitzen wird bis zu einem Meter lang. Sie ist allerdings akut vom Aussterben bedroht und kommt nur noch in wenigen Tausend Exemplaren auf einzelnen westlichen Nebeninseln wie Yadua Taba, Monuriki und Macuata vor, nachdem ihre Lebensräume andernorts durch Feuer, Stürme, Habitatumwandlung für Landwirtschaft, verwilderte Ziegen sowie Fraßdruck durch ausgesetzte Raubtiere (Mungos, Katzen) zerstört wurden. Insgesamt besteht die Reptilienfauna der Inselgruppe aus 36 Arten.
Zur Vogelwelt Fidschis werden rund 150 Arten gezählt, darunter befinden sich 23 endemische. Zu diesen Endemiten gehören unter anderem der Fidschihabicht ("Accipiter rufitorques"), Orange-, Gold- und Smaragdtaube ("Ptilinopus victor, P. luteovirens, P. layardi") sowie die Braunschwanz-Fruchttaube ("Ducula latrans"), fünf oder sechs Papageienarten der Familie Psittacidae, darunter das Rothöschen ("Charmosyna amabilis"), drei Honigfresser (Meliphagidae), der Kandavufächerschwanz ("Rhipidura personata"), vier Arten von Monarchen (Monarchidae), der Langbein-Buschsänger ("Megalurulus rufus") und der Laubhuscher ("Cettia ruficapilla") sowie der Layardbrillenvogel ("Zosterops explorator") und die Schwarzstirn-Papageiamadine ("Erythrura kleinschmidti"). Als kurz vor dem Aussterben stehende endemische Vogelart der Fidschis ist außerdem der Macgillivraysturmvogel zu erwähnen.

Säugetiere sind ohne menschliche Hilfe nur wenige auf den abgelegenen Archipel gelangt. Es handelt sich bezeichnenderweise namentlich um die flugfähigen Fledertiere, von denen mehrere Arten in Fidschi heimisch sind, darunter auch der endemische Affengesicht-Flughund ("Pteralopex acrodonta") auf Taveuni.

Zum Schutz der marinen Flora und Fauna wurde das Vuata Ono Marine Protected Area eingerichtet.

Rund 32 % der Bevölkerung des Landes ist unter 14 Jahre alt, 65 % zwischen 15 und 64 Jahren und 4,5 % der Bevölkerung ist über 64 Jahre alt. Die Wachstumsrate betrug 2006 knapp 1,4 %, bei einer Geburtenrate von 22,55 Geborenen pro 1000 Einwohner und Jahr einer Sterberate von 5,65 pro 1000 Einwohner. Die Lebenserwartung liegt bei der Geburt bei etwa 69,9 Jahren (67,3 für Männer, 72,5 für Frauen).

Laut dem Zensus von 2017 hat Fidschi 884.887 Einwohner.

Quelle: Fiji Bureau of Statistics - Key Statistics: 1.2A Census population of Fiji by ethnicity.

Die Bevölkerung des Landes besteht zu 57,3 % aus Angehörigen der ursprünglichen fidschianischen Bevölkerung ("iTaukei" – vorherrschend Melanesier mit polynesischem Einschlag) und zu etwa 37,6 % (schwankend) aus Indern. 1,2 % der Bevölkerung sind Rotumanen. Daneben gibt es Minderheiten von anderen Inselbewohnern des Pazifiks, Europäern und Chinesen. Seit dem Jahre 1987 haben viele Inder Fidschi verlassen.

Die Amtssprachen sind Fidschianisch ("Bauan", der Dialekt der Insel Bau), das zu den austronesischen malayopolynesischen Sprachen zählt, Fidschi-Hindi und Englisch. Fiji-Englisch ist ein Basilekt der Unterprivilegierten. Die indischstämmige Bevölkerung spricht Fidschi-Hindi (auch „Fiji Talk“), neben Hindi und Urdu eine Variante des Hindustani, das ebenfalls die Amtssprache ist. Daneben werden auch Lauanisch und Rotumanisch gesprochen.

Im Jahr 2007 bezeichneten sich 55,4 % der Einwohner als evangelische Christen, davon sind 34,6 % Methodisten, 5,7 % Anhänger der Assembly of God, 3,9 % Sieben-Tags-Adventisten und 0,8 % Anglikaner. Kleinere evangelische Kirchen machten 10,4 % der Bevölkerung aus. 9,1 % sind römisch-katholisch. 27,9 % der Gesamtbevölkerung sind Hindus, 6,3 % sind Muslime und 0,3 % sind Sikhs. Den traditionellen melanesischen und polynesischen Religionen gehören infolge der christlichen Missionierung weniger als 0,3 % der Bevölkerung an, 0,7 % der Bevölkerung gaben keine Religion an.

Ursprünglich war das Schulsystem am britischen System orientiert. Jetzt erfolgt die Schulbildung meist durch lokale Komitees. Zunehmend wird nach Volksgruppen getrennt unterrichtet. Erst seit 2002 sind Schläge mit der Gerte verboten. Schulpflicht besteht für die Dauer von acht Jahren.

In der Hauptstadt Suva befindet sich die 1968 gegründete University of the South Pacific, in welche außer Fidschi die Pazifikstaaten der Cookinseln, Salomonen, Kiribati, Marshallinseln, Nauru, Niue, Samoa, Tokelau, Tonga, Tuvalu und Vanuatu eine gewisse Anzahl an Studenten entsenden dürfen. Die Anzahl der Studenten beträgt 8600. 

Die erst im Dezember 2004 gegründete private University of Fiji befindet sich noch in der Aufbauphase. 2008 wurden die ersten Absolventen entlassen. Ihr Sitz ist in Lautoka.

Tropenkrankheiten kommen nicht vor; etwa 0,11 % sind HIV-positiv (UNAIDS.org 2005). In Fidschi praktizieren 290 Ärzte. 

Der Archipel ist seit mehr als 3000 Jahren von Melanesiern bewohnt, die vermutlich von Südostasien aus einwanderten. Die Ureinwohner lebten von Ackerbau (Knollenanbau), Schweinehaltung und Fischfang unter urgesellschaftlichen Bedingungen. Sie nahmen durch Kontakte mit anderen Inselbevölkerungen viele Elemente polynesischer Kultur an. Im Zuge der Auflösung der frühgesellschaftlichen Strukturen bildeten sich gentilaristokratische Häuptlingsdynastien heraus.

Am 6. Februar 1643 wurde Fidschi von Abel Tasman gesichtet, 1774 von James Cook besucht und 1827 von Jules Dumont d’Urville erforscht. Nach gelegentlichen Besuchen von Sandelholzfällern zwischen 1801 und 1813 siedelten sich europäische Pflanzer an, die ab etwa 1860 Baumwolle anbauten. 1835 nahmen Vertreter britischer Missionsgesellschaften ihre Tätigkeit auf. 

1871 wurde eine Scheinverfassung nach britischem Muster erlassen mit Cakobau als König (Titel: "Tui Viti") und einem Häuptlingsrat ("Council of Chiefs"), bis am 10. Oktober 1874 Fidschi zur Kronkolonie erklärt wurde. Australische und britische Firmen konzentrierten sich in der Folge auf den Zuckerrohranbau. Sie holten am 14. Mai 1879 die ersten indischen rekrutierten Arbeitskräfte – bis 1916 insgesamt 61.000 – ins Land, deren Nachkommen zwischen 1946 und 1997 die Bevölkerungsmehrheit bildeten. 1881 wurde die Insel Rotuma Island an die Kronkolonie angeschlossen.

Im Zweiten Weltkrieg war Fidschi eine bedeutende Versorgungsstation der Alliierten; es wurden zahlreiche Luftwaffen- und Marinestützpunkte errichtet.

1960 kam es zu einem landesweiten Streik der Zuckerrohrpflanzer.

1966 errang Fidschi innere Autonomie, am 10. Oktober 1970 seine vollständige Unabhängigkeit im Rahmen des Commonwealth of Nations und wurde Mitglied der Vereinten Nationen. Staatsoberhaupt war die britische Königin, Elisabeth II., erster Premierminister des Landes wurde Ratu Sir Kamisese Mara.

1987 gewannen indische Parteien die Wahlen. Durch zwei aufeinanderfolgende Militärputsche unter der Führung von Sitiveni Rabuka am 14. Mai und 25. September des gleichen Jahres wurde jedoch die Vorherrschaft der Melanesier wiederhergestellt und zugleich die bisherige konstitutionelle Monarchie in eine Republik umgewandelt, worauf das Commonwealth im Oktober das Land aus seinen Reihen ausschloss. Die republikanische Verfassung von 1990 verfestigte den undemokratischen Zustand, indem sie die Hegemonie der Melanesier in Verfassungsrang erhob. Die politischen Unruhen (Streiks, politische Wirren), Missernten und sinkenden Weltmarktpreise für Zucker führten zu einem Niedergang der Wirtschaft und einem bis heute anhaltenden Massenexodus der von den Melanesiern politisch unterdrückten, aber ökonomisch dominierenden indischstämmigen Bevölkerung des Landes. 1994 wurde Ratu Sir Kamisese Mara Staatspräsident und berief im Folgejahr eine Kommission zur Überarbeitung der Verfassung ein. Als Ergebnis wurde eine neue Verfassung entworfen, die die Gleichberechtigung aller Bevölkerungsgruppen wiederherstellte und im Juli 1997 in Kraft trat. Noch im selben Jahr wurde Fidschi wieder in das Commonwealth aufgenommen.

Im Mai 1999 wurde Mahendra Chaudhry, der Vorsitzende der Fiji Labour Party (FLP), nach dem Wahlsieg seiner Partei neuer Premierminister des Landes. Als wichtigstes innenpolitisches Ziel formulierte der erste indischstämmige Regierungschef die Beendigung der Spannungen zwischen den einzelnen ethnischen Gruppen. Chaudry und das gesamte Kabinett wurden am 19. Mai 2000 von Putschisten unter Führung des Geschäftsmannes George Speight und mit der Unterstützung konservativer Kräfte als Geiseln genommen. Frank Bainimarama, Chef der Streitkräfte, erklärte nach dem Staatsstreich Präsident Ratu Sir Kamisese Mara für abgesetzt und sich selbst zum neuen Herrscher des Inselstaates. Das Einschreiten des Militärs beendete zwar den Putsch, bedeutete aber zugleich auch das vorläufige Ende der kurzen Phase politischer Entfaltung und Gleichberechtigung der Indo-Fidschianer.

Nach der Freilassung der Geiseln wählte der Große Rat der Stammeshäuptlinge am 13. Juli 2000 Ratu Josefa Iloilo zum neuen Oberhaupt des Staates. Dem Kabinett des neuen Regierungschefs Laisenia Qarase gehörte den Forderungen der Putschisten zufolge nur ein indischstämmiger Minister an. Am 10. September 2001 wurde Qarase nach dem Wahlsieg seiner wenige Monate zuvor gegründeten Fijian People’s Party (SDL) – sie vertritt die Interessen der indigenen Bevölkerung - von Staatspräsident Iloilo zum Premierminister ernannt. Qarase weigerte sich, gemäß der verfassungsmäßigen Sitte die größte Oppositionspartei – Chaudhrys multiethnische "Labor Party" – an der Kabinettszusammensetzung zu beteiligen. Die Regierungsbildung gestaltete sich problematisch und vertiefte die ethnischen Spannungen im Land. Für neue Aufregung sorgte im Frühsommer 2005 die Ankündigung des Regierungschefs, eine Amnestieregelung auf den Weg zu bringen, die den Putschisten des Jahres 2000, allen voran Anführer George Speight, die vorfristige Freiheit ermöglicht hätte. Ebenso sollte das Land in den Küstengebieten neu verteilt werden und im Zuge dessen Ur-Fidschianer die indische Minderheit in diesen Gebieten verdrängen. Nach starken Protesten gegen das Vorhaben sagte Qarase eine Überarbeitung des Gesetzentwurfes zu.

Am 5. Dezember 2006 (Fidschi-Zeit = GMT + 12) begann das Militär mit dem vierten Putsch innerhalb von nur 20 Jahren. Um die Hauptstadt Suva herum wurden Straßensperren errichtet und Premierminister Laisenia Qarase sowie einige Minister unter Hausarrest gestellt. Militärchef Frank Bainimarama erklärte die Regierung Qarase für abgesetzt. Als Gründe gab er fortdauernde Korruption innerhalb der Regierung Qarases trotz mehrfacher Warnungen sowie ein umstrittenes Amnestievorhaben für die Beteiligten des Putsches von 2000 an, die damals versucht hatten, ihn umzubringen. Zudem warf Bainimarama dem Premierminister vor, ethnische Spannungen zwischen den einheimischen Fidschianern (Melanesier) und den indischstämmigen Fidschianern, die immerhin mehr als 40 % der Bevölkerung ausmachen, zu schüren. Hierin seien »viel größere illegale Aktivitäten« zu sehen als in dem durchaus illegalen Staatsstreich. Er sagte auch, Demokratie sei gut für das Volk, dürfe aber nicht dazu benutzt werden, Korruption zu verstecken. Die Bewertung dieses Coups als diktatorischer Putsch im klassischen Sinn ist auf der internationalen politischen Bühne umstritten. 

Am 4. Januar 2007 gab Bainimarama überraschend die Macht an Präsident Iloilo zurück. Auch der Übergangspremier Jona Senilagakali gab sein Amt auf. Iloilo billigte in einer Ansprache das Vorgehen des Militärs und kündigte die Einsetzung einer Übergangsregierung und Neuwahlen an. Als neuer Regierungschef wurde Bainimarama am 5. Januar 2007 vereidigt.

Im April 2009 erklärte ein Berufungsgericht die Militärregierung für illegal und forderte den Präsidenten auf, eine neue Regierung zu bilden. Daraufhin setzte Iloilo die Verfassung außer Kraft und entließ alle Richter. Er kündigte an, eine Übergangsregierung einzusetzen, die bis 2014 demokratische Wahlen vorbereiten soll. Entgegen den Vorgaben des Berufungsgerichts, weder Frank Bainimarama noch dessen Vorgänger Laisenia Qarase einzusetzen, wurde wiederum Bainimarama Regierungschef.

Am 2. Mai 2009 wurde Fidschi von der Mitarbeit im Pacific Islands Forum suspendiert, nachdem Bainimarama eine Frist zur Wiederherstellung demokratischer Verhältnisse in Fidschi unbeachtet verstreichen ließ. Fidschi bleibt jedoch ein Mitglied des Forums.

Am 1. September 2009 folgte die Suspendierung der Mitgliedschaft im Commonwealth of Nations, nachdem auch dessen Forderung, spätestens für Oktober 2010 demokratische Wahlen in Fidschi anzuberaumen, ungehört blieb. Bainimarama beharrt auf seinem "Fahrplan", der erst für 2014 demokratische Wahlen vorsah. So viel Zeit sei notwendig, um die Korruption im Land erfolgreich zu bekämpfen und die Basis für eine gewählte demokratische Regierung zu bilden. Durch den Ausschluss verlor Fidschi die Berechtigung zur Teilnahme an Commonwealth-Treffen und -Sportveranstaltungen (so z. B. an den Commonwealth Games im Jahr 2010) und die Berechtigung zum Erhalt technischer Unterstützungen seitens des Commonwealth. Zusätzlich war Fidschi am Sitz des Commonwealth und bei Commonwealth-Veranstaltungen nicht länger mit seiner Nationalflagge oder sonstige Symbole vertreten, blieb aber dennoch formal ein Mitglied der Organisation.
Anfang 2011 änderte die Regierung den offiziellen Staatsnamen von "Republic of the Fiji Islands" zu "Republic of Fiji". Bereits von der Abschaffung der Monarchie im Jahr 1987 bis zum Inkrafttreten der Verfassung von 1997 trug Fidschi diesen Namen.

Im Jahr 2013 trat eine neue Verfassung in Kraft, nach der alle Staatsbürger unabhängig von ihrer Herkunft gleichberechtigte Fidschianer sind. Die bis dahin bestehenden politischen Vorrechte der Urbevölkerung wurden abgeschafft.

2014 fanden Wahlen statt. Daraufhin wurde die Suspendierungen in den internationalen Organisationen aufgehoben. Während der Jahre der Militärherrschaft hatten Australien und Neuseeland auch ihre Entwicklungshilfe eingestellt. China sprang mit Darlehen in die Bresche aber Anfangs 2018 schätzte der ehemalige Premierminister Sitiveni Rabuka, dass es schwierig werden könnte, diese aufgelaufenen Schulden zurückzuzahlen.

Die Republik Fidschi wurde Mitte November 2016 auf der 22. UN-Klimakonferenz in Marrakesch für den Vorsitz der 23. UN-Klimakonferenz in Bonn 2017 ausgewählt. Da sich auf ihrem Territorium die erwartete Zahl von 15.000 bis 20.000 Teilnehmenden nicht zusammen an einem Ort versammeln könnte, wurde als Austragungsort der Sitz des UN-Klimasekretariats in Bonn festgelegt.

Die Republik Fidschi ist eine parlamentarische Republik. Staatsoberhaupt und zugleich Oberbefehlshaber der Streitkräfte ist der Präsident der Republik, derzeit Jioji Konousi Konrote. 

Der Regierungschef ist der Premierminister, der die Richtlinien der Politik vorgibt. Der Premierminister wird vom Einkammerparlament gewählt. Aus den Parlamentswahlen im September 2014 ging die Partei FijiFirst von Frank Bainimarama mit einer absoluten Mehrheit als Wahlsieger hervor. Dieser wurde schließlich vom Parlament zum Regierungschef gewählt und löste somit die unter ihm geführte Militärregierung ab.

Das Parlament hat 50 Sitze. Es wurde im Jahr 2014 nach der Militärregierung auf Basis der neuen Verfassung wieder eingesetzt, jedoch von einem Zweikammerparlament in ein Einkammerparlament umgewandelt. Zurzeit sind drei Parteien im Parlament vertreten:


Im Demokratieindex 2016 belegt das Land Platz 89 von 167 Ländern und gilt damit als eine „Hybridregime“ mit sowohl demokratischen als auch autoritären Elementen.

Die häufigsten Vergehen in Fidschi sind Diebstahl und alkoholbedingte Gewalt. Nach dem Staatsstreich 2000 übten viele Fidschianer Selbstjustiz aus, vor allem gegen Indo-Fidschianer. Zurzeit sind circa 960 Häftlinge in Gefängnissen. Es gibt keine Todesstrafe.

Bis zum Jahre 2006 waren homosexuelle Handlungen verboten, jedoch war eine Diskriminierung aufgrund der Verfassung nicht erlaubt. Eine derartig widersprüchliche Regel gab es weltweit nur auf Fidschi. Nachdem ein Verurteilter Revision eingelegt hatte und dieser stattgegeben wurde, überarbeitete der Senat das Gesetz. Seitdem sind homosexuelle Handlungen nicht mehr strafbar.

Durch die Staatsstreiche von 1987, 2000 und 2006 sowie die Diskriminierung der Indo-Fidschianer durch die Melanesier hat das Ansehen Fidschis international sehr gelitten. Hauptansprech- und Handelspartner sind Australien und Neuseeland. Außerdem bemüht man sich um gute Beziehungen zu den anderen Pazifik-Staaten, besonders zu den direkten Nachbarn Samoa und Tonga. Der kleine Pazifikstaat Tuvalu steht mit den Fidschi-Inseln in Verhandlungen zum Zwecke eines Landerwerbes; diese Verhandlungen wurden allerdings nach dem Staatsstreich im Jahre 2000 auf unbestimmte Zeit verschoben.

Fidschi ist Mitglied u. a. der Vereinten Nationen, des Pacific Islands Forum, der Pazifischen Gemeinschaft sowie der Alliance of Small Island States.

Die Streitkräfte Fidschis ("Republic of Fiji Military Forces") bestehen aus den Komponenten Heer und Marine und umfassen 3500 Soldaten, davon ca. 300 Marinesoldaten, bei einer Einwohnerzahl von 853.445 (Stand: Ende 2006 geschätzt). Die meisten Soldaten sind Fidschianer. Etwa 20 Prozent stehen in UN-Diensten. Fidschi gibt circa 32 Millionen US-Dollar für das Militär im Jahr aus, es ist mit einem Hubschrauber und gepanzerten Fahrzeugen ausgestattet. Die Armee führte innerhalb von 20 Jahren bereits vier Putsche gegen die Regierung durch.

Fidschi gliedert sich in vier Divisionen ("divisions") und das Schutzgebiet ("dependency") Rotuma mit der Hauptstadt Ahau.

Die Divisionen unterteilen sich in 14 Provinzen.

Die vier Divisionen sind:

Nach dem Putsch im Jahre 2000 sank die Wirtschaftsleistung um 12,5 Prozent. Viele indo-fidschianische Fachleute wandern wegen der gesellschaftlichen Probleme im Land aus. Die Arbeitslosigkeit liegt gegenwärtig bei etwa fünf Prozent. 82 Prozent des Energiebedarfs von Fidschi werden zurzeit aus Wasserkraft gewonnen.

Zu den bedeutendsten Anbauprodukten gehören unter anderem Zuckerrohr, Ingwer, Kokosnüsse, Reis, Kakao, Kaffee, Taro, Ananas und Tabak. Die Grundlage der Industrie des Landes ist die Weiterverarbeitung von Erzeugnissen aus der Landwirtschaft, der Forstwirtschaft und der Fischerei.

Zuckerrohr ist seit der Kolonialzeit bedeutendster agrarischer Rohstoff Fidschis und in veredelter Form neben Fisch, Textilien, Holz und Kokosnussöl wichtiges Exportprodukt. Die politischen Unruhen mit dem Staatsstreich 1987 und die nachfolgende Wirtschaftskrise führten zu einer stärkeren Diversifizierung des Exports. Heute sind die Zuckerwirtschaft, die Textil- und Bekleidungsindustrie sowie der Tourismus die Haupteinnahmequellen des Landes. Mehr als 100.000 Beschäftigte sind in den drei Sektoren beschäftigt, die Exporterlöse von 1,1 Milliarden Fidschi-Dollar erbrachten (2004).

Fidschi verfügt über große Goldvorkommen. Im Jahre 2000 belief sich die Jahresproduktion auf 3675 Kilogramm.

Neben landwirtschaftlichen Produkten und Bodenschätzen exportiert Fidschi auch Wasser. Das in einem artesischen Brunnen auf Viti Levu gewonnene Quellwasser wird als "Fiji Water" fast vollständig für den Export abgefüllt.

Die wirtschaftliche Bedeutung des Fremdenverkehrs wächst zunehmend. Fidschi liegt im Bereich des Südpazifik bei den Touristenankünften mit Abstand an der Spitze. Das an zweiter Stelle stehende Französisch-Polynesien hatte 2003 nur etwa halb so viele Touristen (212.767) und Samoa weniger als ein Viertel (92.313). Die Zahl der Touristen steigt und fällt mit der politischen Lage im Land. So sank die Zahl der Touristen nach dem Putsch im Jahr 2000 auf 294.070 von 409.955 im Jahr zuvor. Im Jahr 2003 hatten sich die Ankunftszahlen wieder auf 430.800 erholt. Die meisten Touristen kamen aus Australien (32,9 %), Neuseeland (17,4 %), dem Vereinigten Königreich (11,6 %) und aus Japan (5,4 %).

Die attraktivsten Ziele für Touristen sind kleine Resortinseln auf den Mamanucas, den Yasawas oder Strände der Coral Coast auf der Hauptinsel Viti Levu. Auf Taveuni und Vanua Levu spielt außerdem der Tauchtourismus eine Rolle. Touristen erhalten bei Einreise und unter Vorlage eines Rückflugtickets sowie ausreichender finanzieller Mittel eine Einreisegenehmigung für vier Monate.

Das Bruttoinlandsprodukt (BIP) beträgt 4,70 Milliarden US-Dollar (2016). Der Dienstleistungssektor hat daran einen Anteil von 70,7 Prozent, die Industrie von 18,1 Prozent und die Landwirtschaft von 11,3 Prozent. Daraus ergibt sich ein BIP pro Einwohner von 5.233 US-Dollar womit Fidschi zu den Ländern mit mittlerem Einkommensniveau gehört.

Die Währung in Fidschi ist der Fidschi-Dollar.

Der Staatshaushalt umfasste 2017 Ausgaben von umgerechnet 1,687 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 1,446 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 4,8 % des BIP.
Die Staatsverschuldung betrug 2016 47,0 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Die Städte Nadi und Suva (Nausori) besitzen internationale Flughäfen. Die Fidschi-Inseln haben eine eigene Fluggesellschaft, die Fiji Airways, mit der sie vor allem die umliegenden Inseln anfliegen. Der Staat Fidschi (51 %) und Qantas (46,05 %) kontrollieren den Großteil der Anteile. Neben Fiji Airways gibt es die Inlandsfluggesellschaft Sun Air.
Die Länge des Straßennetzes in Fidschi beträgt 3440 Kilometer, das Eisenbahnnetz ist 595 Kilometer lang, beschränkt sich aber auf Bahnen zum Transport von Zuckerrohr zur Raffinerie in Lautoka. Personenzüge gibt es nicht; der öffentliche Nahverkehr wird mit dem Omnibus abgewickelt.

Die "Coral Coast Railway" bietet auf dem Netz der staatlichen "Fidschi-Zucker-Bahn" täglich ein Zugpaar von Yanuca zum Strand von Natadola und von Yanuca nach Sigatoka.

Fidschi ist einer der wichtigsten Verkehrsknotenpunkte im südlichen Teil des Pazifischen Ozeans. Regelmäßige Schiffsverbindungen bestehen mit den USA, Kanada, Australien, Neuseeland sowie einigen Pazifikinseln. Haupthafen ist die Hauptstadt Suva. Bedeutend ist auch der Hafen von Lautoka, einem Zentrum der Zuckerindustrie.

Das größte Fest in Fidschi ist das Hibiskusfest, das im August eine Woche lang in der Hauptstadt Suva gefeiert wird. Die gesetzlichen Feiertage des Landes sind Neujahr (1. Januar), Ostern (Karfreitag bis Ostermontag), der National Youth Day (im März), Ratu Sir Lala Sukuna Day (im Mai oder Juni), der Geburtstag der britischen Königin (gefeiert an einem Montag um den 14. Juni), Constitution Day (im Juli), der Unabhängigkeitstag (an einem Montag um den 10. Oktober), der Geburtstag des Propheten Mohammed (im Juli), das hinduistische Lichterfest Divali (im Oktober oder November), Weihnachten (25. Dezember) und der Boxing Day (26. Dezember). Letzterer Feiertag geht auf die britische Tradition zurück.

Nationalsport auf den Fidschi-Inseln ist Rugby Union (15er-Rugby). Fidschi gehört zu den zwölf besten Rugby-Nationen der Welt. Das Land nimmt regelmäßig an Rugby-Weltmeisterschaften sowie am Pacific Nations Cup teil. Noch erfolgreicher ist Fidschi jedoch im 7er-Rugby, dessen Regeln auf jenen des 15er-Rugby basieren. So gewann die 7er-Nationalmannschaft zwei der bisher fünf Austragungen der 7er-Rugby-Weltmeisterschaft, dreimal die renommierte IRB Sevens World Series und alle drei Rugby-Turniere bei den World Games.

Der größte sportliche Erfolg des Landes ist der Gewinn der Goldmedaille im 7er-Rugby der Herren bei den Olympischen Spielen 2016 in Rio de Janeiro. Fidschi ist damit der erste pazifische Inselstaat Ozeaniens, der eine Goldmedaille bei Olympischen Sommerspielen gewinnen konnte.

Fidschi hat allerdings, ebenso wie die anderen Inselstaaten, mit einigen Problemen zu kämpfen: Da die Republik recht klein ist, gibt es kaum Sponsoren, die eine professionelle Ausübung des Sportes gewährleisten würden. Die größeren Nationen in Ozeanien, Australien und Neuseeland, setzen Scouts auf den Inseln ein und ködern die größten Talente mit Sportstipendien. So gehen Fidschi und den anderen Inselstaaten zahlreiche Talente verloren, da diese vom jeweiligen Zielland eingebürgert werden. Fidschi bemüht sich seit Jahren, ein Team in der Super Rugby-Liga stellen zu dürfen. Die Aufnahme wurde allerdings bisher abgelehnt. Die meisten Spieler Fidschis versuchen ihr Glück im Ausland, da sie dort mit dem Sport Geld verdienen können. Hauptziele sind Australien, Neuseeland und England. Vor allem englische Vereine gewähren den Fidschianern selten eine Freigabe für Spiele der Nationalmannschaft.

Auch Fußball erfreut sich auf den Fidschi-Inseln zunehmender Beliebtheit. Hier nimmt Fidschi am Ozeanienmeisterschaft teil (entspricht einer Europameisterschaft und der Qualifikation für die Weltmeisterschaft in einem). Allerdings ist auch hier die Situation etwas verfahren, da sich Neuseeland und Australien auch hier weigern, die Inseln zu besuchen. Entsprechend werden alle Spiele im Turniermodus in Australien ausgetragen.

Auf Fidschi findet auch als Fiji Pro eine der Veranstaltungen der Weltmeisterschaften im Wellenreiten der ehemaligen ASP World Tour statt., 2015 umbenannt in WSL World Tour.

In Fidschi erscheinen drei Tageszeitungen, die "Fiji Times", die "Fiji Sun" und die "Fiji Post". Außerdem gibt es noch die "Nai Lalakai" und die "Shanti Dut", sie erscheinen wöchentlich. Die tägliche Zeitungsauflage beträgt 51 auf 1000 Einwohner. Erst 1991 wurde das Fernsehen eingeführt. Im Land befindet sich nur ein staatlicher TV-Sender. Dazu kommen noch eine staatliche und drei unabhängige Radiostationen. Rundfunksendungen sind über Internet Live-Streams unter www.radiofiji.com.fj z. B. Fiji Bula Fm zugänglich.

Die derzeitige Regierung hat am 10. April 2009 die Pressefreiheit drastisch eingeschränkt. Ausländische Berichterstatter wurden des Landes verwiesen. In der Redaktion der Presseorgane und des Fernsehsenders ist stets ein Zensor mit Polizeibegleitung anwesend, der jede Berichterstattung unterbindet, die die militärischen Machthaber nicht in einem positiven Licht erscheinen lässt. Oppositionelle wie Virisila Buadromo von der Fiji Women's Rights Movement (FWRM), die 2008 von Condoleezza Rice mit dem „International Woman of Courage Award“ ausgezeichnet wurde, haben keinerlei öffentliches Sprachrohr mehr. Aus Protest gegen die Zensurmaßnahmen erschien die „Fiji Times“ am Tag nach der Einschränkung der Pressefreiheit mit einer unbedruckten Titelseite.

Fidschi besitzt 102.000 Telefonanschlüsse, 109.000 Mobiltelefone, 541.500 Radios, 91.000 Fernseher und 55.000 Internetanschlüsse.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Fidschi Platz 67 von 180 Ländern.

In Fidschi sind mehrere über die Insel hinaus bekannte bildende Künstler und Grafiker beheimatet, unter anderem William Bakalevu, der als sozialkritischer Mauermaler begann, Mason Lee, der vor allem Bilder aus Fidschis Mythen- und Legendenwelt malt, und der Designer Josaia McNamara.





</doc>
<doc id="11999" url="https://de.wikipedia.org/wiki?curid=11999" title="Ravensburg">
Ravensburg

Ravensburg ([] oder []) ist eine Mittelstadt sowie Kreisstadt und größte Stadt des gleichnamigen Landkreises im südlichen Oberschwaben. Die ehemalige Freie Reichsstadt liegt im Schussental unweit des Bodensees und wurde aufgrund ihrer zahlreichen, gut erhaltenen mittelalterlichen Türme früher auch als „das schwäbische Nürnberg“ bezeichnet. Seit 1. April 1956 ist Ravensburg Große Kreisstadt.

Ravensburg liegt etwa 17 km (Luftlinie) nord-nordöstlich des am Bodensee gelegenen Friedrichshafen zwischen dem Linzgau im Westen und dem Altdorfer Wald im Nordosten. Durchflossen wird die Stadt von der Schussen (Bodensee- bzw. Rhein-Zufluss).

Die Burg Veitsburg (oberhalb der Altstadt von Ravensburg) liegt auf einem Höhenrücken, der das Schussental-Becken im Osten begrenzt. Dieses Becken – das nicht durch die vergleichsweise kleine Schussen, sondern durch eine eiszeitliche Gletscherzunge geformt wurde, die sich von den Alpen über den Bodensee nach Norden vorschob – verengt sich bei Ravensburg, um sich nördlich von Weingarten wieder etwas zu weiten, bis es in den engen Schussentobel übergeht.
Nordöstlich des Burgbergs durchbricht die tief eingeschnittene Schlucht des Flappachtals den Höhenrücken. Das Flappachtal bietet eine eher sanfte Aufstiegsmöglichkeit in Richtung Allgäu, schon früh verlief dort daher eine für den Fernhandel wichtige Route über Wangen im Allgäu Richtung Südosten.

Die Stadt Ravensburg entstand auf halber Höhe unterhalb der Veitsburg auf dem Abhang zur Schussen hin; der Flappach wurde als Stadtbach teilweise durch die Stadt, teilweise durch den nördlichen Stadtgraben geleitet. Außerhalb der Stadtmauern befand sich im Flappachtal die Mühlenvorstadt und unterhalb der Stadt am Auslauf des Flappachs zur Schussen das Gerber- und Färberviertel "Pfannenstiel".
Auf dem der Stadt gegenüberliegenden Westhang des Schussentals befand sich im Mittelalter die "Galgenhalde" genannte Richtstätte sowie nördlich davon das "Sennerbad". Oberhalb davon entstand Mitte des 20. Jahrhunderts das Neubaugebiet „Weststadt“.

Gleichzeitig wuchs die Stadt nach Norden und Süden hin, so dass sich auf der Ostseite des Schussentals heute ein durchgehendes Siedlungsband von Baindt im Norden über Baienfurt, Weingarten und Ravensburg bis Eschach im Süden gebildet hat.

Folgende Städte und Gemeinden grenzen an die Stadt Ravensburg. Sie werden im Uhrzeigersinn von West über Nord nach Süd aufgezählt:

Horgenzell, Berg, Weingarten, Schlier, Grünkraut und Bodnegg (alle Landkreis Ravensburg) sowie Tettnang, Meckenbeuren, Friedrichshafen und Oberteuringen (alle Bodenseekreis).

Das Stadtgebiet besteht aus der Kernstadt (mit der historischen Altstadt und Stadterweiterungen im Norden, Süden und Osten sowie dem Neubaugebiet Weststadt) und den im Rahmen der Gemeindereform der 1970er Jahre eingegliederten ehemaligen Gemeinden Adelsreute im Südwesten, Eschach im Süden, Schmalegg im Westen und Taldorf im Südwesten der Kernstadt.

Die eingegliederten Gemeinden sind (mit Ausnahme von Adelsreute, das zu Taldorf gehört) heute zugleich Ortschaften im Sinne der baden-württembergischen Gemeindeordnung, das heißt, sie haben jeweils einen von den Wahlberechtigten bei jeder Kommunalwahl neu zu wählenden Ortschaftsrat mit einem Ortsvorsteher als Vorsitzenden. In jeder der Ortschaften gibt es eine Ortsverwaltung, quasi ein „Rathaus vor Ort“, dessen Leiter der Ortsvorsteher ist.

Zu fast allen Stadtteilen und zur Kernstadt gehören noch viele räumlich getrennte Wohnplätze mit eigenen Namen, die oft nur wenige Einwohner haben, oder Wohngebiete mit eigenen Namen, deren Bezeichnung sich im Laufe der Bebauung ergeben haben und deren Grenzen dann meist nicht genau festgelegt sind. Im Einzelnen sind zu nennen:

Ravensburg bildet zusammen mit Weingarten und Friedrichshafen das Oberzentrum der Region Bodensee-Oberschwaben. Ravensburg und Weingarten übernehmen für ihren Einzugsbereich auch die Funktion des Mittelzentrums. Der Mittelbereich umfasst die südwestlichen Gemeinden des Landkreises Ravensburg mit 129.507 Einwohnern (Stand: 31. Dezember 2011), im Einzelnen die folgenden Städte und Gemeinden:
Durch die Lage in einem sich zum Bodensee hin weitenden Talbecken wird das lokale Klima sehr durch den Bodensee bestimmt. Der rund 500 km² große See hat in den meisten Jahren eine stark ausgleichende Wirkung auf das regionale Mesoklima, da er als jahreszeitlicher Temperaturspeicher wirkt. Daher fallen Winterfröste hier deutlich schwächer aus als im Umland; andererseits hält der Winter auch länger an, wenn der See in besonders kalten Wintern (wie zuletzt 1963) zufriert. Eine Nebenwirkung der Wärmetemperierung sind die häufig auftretenden Bodenseenebel in der kalten Jahreszeit, wenn die vom See heraufziehenden wärmeren und daher feuchteren Luftschichten auf die kältere Umgebungsluft treffen.

Wie das gesamte Voralpenland kennt auch Ravensburg das Phänomen des Föhnwindes, der warme Luft aus dem Mittelmeergebiet über die Alpen bringt, die dann als Fallwind ins Rheintal abfällt und über den Bodensee auch Ravensburg erreicht. Mitunter erreicht der Föhn orkanartige Geschwindigkeit.

Die besonderen klimatischen Bedingungen haben jahrhundertelang den Weinbau trotz der Höhenlage (zwischen 450 und ) begünstigt; vorübergehende Klimaverschlechterungen Ende des 18. und in der ersten Hälfte des 19. Jahrhunderts (u. a. Jahr ohne Sommer 1816 durch den Ausbruch des Tambora) läuteten aber den Niedergang ein. Der letzte privat bewirtschaftete Weinberg wurde um 1960 aufgegeben; seit Ende der 1970er Jahre betreibt die Stadt wieder einen Weinberg am Rauenegg-Hang.

Die etymologische Herkunft des Ortsnamens ist nicht gesichert, nicht angezweifelt wird jedoch, dass die Stadt ihren Namen von der heute Veitsburg genannten Burg oberhalb der Stadt erhielt. Eine verbreitete Theorie zur Herkunft des Burgnamens besagt, dass sich der erste Namensteil von "Rabe" oder einem "Raban", der die Burg ursprünglich gegründet habe, ableitet. Zudem kann die frühere, an das Lateinische angelehnte, Schreibweise "Ravenspvrg" als "Rauenburg" interpretiert werden, wodurch auch eine Zurückführung auf einen "rauen" Hang nahe der Burg möglich ist.

Der Ortsname veränderte sich im Lauf der Geschichte von "Ravenspurch" im Jahr 1088 über "Ravensberc" (1231) und "Ravensburg" (1323) zur heutigen Form.

Erste Ansiedlungen gab es schon in der Jungsteinzeit um ca. 2000 v. Chr., dann unter römischer Herrschaft und nach dem Einfall der Alamannen, wie archäologische Funde im Schussental und auf dem Veitsburghügel belegt haben. Urkundlich wurde Ravensburg erstmals 1088 erwähnt und war bis 1803 freie Reichsstadt. Der Kern der heutigen Stadt entstand nach aktuellem Wissensstand als Burgsassen-Siedlung unterhalb der mächtigen Stammburg der Welfen auf dem Höhenrücken zwischen Flappach- und Schussental.

Die erste Burg der Welfen stand in Altdorf-Weingarten, bevor sie um 1050 eine neue, größere Burg, die „Ravensburg“, errichteten (heute "Veitsburg" nach der Sankt Veit geweihten Burgkapelle). Grabungsbefunde legen nahe, dass der Standort der Burg schon in der Keltenzeit als Fluchtburg planiert und vom Rest des Höhenrückens durch einen Graben abgetrennt worden war, jedoch in römischer Zeit und danach brach lag. Die welfische, später staufische Burg nahm den hinteren, nordwestlichen Teil des Plateaus ein, während der vordere wohl unter anderem als Turnier- und Festplatz diente, etwa während der Hochzeitsfeierlichkeiten für Friedrich den Schönen und seine Gemahlin Elisabeth von Aragón 1315.

Der letzte Welfe auf der Ravensburg war Welf VI., Herzog von Spoleto. Nach dem Tod seines Sohnes vermachte er den Schussengau mit Ravensburg und Altdorf per Erbvertrag seinem Neffen, dem Staufer Friedrich I. Barbarossa, und enttäuschte damit seinen anderen Neffen, Heinrich den Löwen, Herzog von Bayern und Sachsen (der 1129/1130 oder 1133/1135 eventuell auf der Ravensburg geboren sein soll). Fortan gehörte Ravensburg zum Hausbesitz der Staufer. Angeblich brach Konradin, der letzte Staufer, von hier zu seinem verhängnisvollen Italienzug auf.

Mit dem Untergang der Staufer endete auch das Herzogtum Schwaben als politische Körperschaft. Wie viele andere Städte Schwabens regierte sich Ravensburg fortan selbst, die Burg wurde Reichsbesitz. 1276 bestätigte der Habsburger König Rudolf I. die reichsstädtischen Privilegien Ravensburgs. Auf der Burg saß fürderhin der kaiserliche Landvogt der Reichslandvogtei Schwaben.

Zur Überwachung der Vorgänge in der Reichsburg errichtete die Reichsstadt an der höchsten Stelle des Stadtgebiets den über 50 Meter hohen Sankt-Michaels-Turm, benannt nach der damals noch dort stehenden Michaelskapelle, der ältesten Kirche der Stadt. Im Volksmund hieß dieser Turm bald "Mehlsack", als Verballhornung des offiziellen Namens wie auch unter Bezugnahme auf die mehlweiße Farbe des Turms. Dass der Turm zeitweise auch als Mehlspeicher genutzt wurde, gehört vermutlich ins Reich der Legenden. Heute ist „Mehlsack“ der offizielle Name dieses einzigen Rundturms der Stadtbefestigung.

Mit strategischer Weitsicht wurde verhindert, dass in unmittelbarer Nähe Konkurrenz aufkäme: So wurde gemeinsam mit dem Kloster Weingarten erfolgreich und zu beiderseitigem Nutzen interveniert, um den Aufstieg der Nachbargemeinde Altdorf zur Reichsstadt zu unterbinden.

Im Spätmittelalter war Ravensburg Sitz der "Großen Ravensburger Handelsgesellschaft", der damals führenden deutschen Handelsgesellschaft, die in ganz Europa Niederlassungen hatte (noch vor den Fuggern).

Die rasante wirtschaftliche Entwicklung hatte vor allem im 14. Jahrhundert mehrfach zu Erweiterungen des Stadtgebiets geführt. Am markantesten ist dabei die Einbeziehung der so genannten "Unterstadt" in die Stadtbefestigung und der Abriss der alten Stadtmauer zwischen den beiden Stadtteilen (1330–1370). Dabei entstand auf der Fläche des alten, zugeschütteten Stadtgrabens der heute noch das Stadtbild prägende Platz, heute "Marienplatz" genannt, zwischen "Frauentor" im Norden und dem im 19. Jahrhundert abgerissenen "Kästlinstor" im Süden.

Um 1530 ging die Große Ravensburger Handelsgesellschaft unter. Die führenden Familien hatten sich zu diesem Zeitpunkt schon Landsitze und Rittergüter zugelegt. Es gab Zwist zwischen verschiedenen Familienzweigen, insbesondere Augsburger Händler drängten in die von den Ravensburgern bedienten Routen – und die Erschließung Amerikas durch Kolumbus hatte zu tiefgreifenden Veränderungen im europäischen Fernhandel geführt. Schließlich fanden sich nicht mehr genug Gesellschafter zur turnusmäßigen Verlängerung der Verträge bereit.

Ende des 14. Jahrhunderts entstand in Ravensburg eine bedeutende Papierproduktion, die ihre Blütezeit im 16. Jahrhundert hatte. Im Mittelalter galt Ravensburg als größter Papierlieferant nördlich der Alpen und war neben Nürnberg die zweite Stadt, die in Deutschland überhaupt Papier herstellte. Bis ins 19. Jahrhundert verlieh dies der Stadt eine markante wirtschaftliche Prägung. Bereits 1393 ist die erste Ravensburger Papierproduktion über Wasserzeichen nachweisbar. Zur Blütezeit der Produktion wurden jährlich etwa 9000 Ries Papier hergestellt. Der Niedergang begann mit dem Ende der Reichsstadtzeit und der Zugehörigkeit zu Württemberg: Alte Absatzmärkte fielen wegen der neuen Grenzen weg. 1833 schloss die erste von sechs Papiermühlen, 1876 die letzte.

1544 wurde auf Betreiben der Zünfte die Reformation eingeführt. Dabei gab es zunächst große Differenzen zwischen den Anhängern Luthers und Zwinglis, doch konnten sich die Lutheraner schließlich durchsetzen. Ferner gab es auch Anhänger von Kaspar Schwenckfeld. Nach dem Schmalkaldischen Krieg ab 1547 regte sich jedoch eine Gegenbewegung und es setzte eine Rekatholisierung großer Teile der Bevölkerung ein. Zahlenmäßig hatten daraufhin wohl wieder die Katholiken die Mehrheit.

Das Zusammenleben beider Konfessionen wurde als Parität bezeichnet, die aber erst nach dem Dreißigjährigen Krieg 1649 formell festgelegt wurde. Die protestantische Gemeinde erhielt als Gotteshaus das Langhaus der Karmelitenkirche, die fortan als Stadtkirche bezeichnet wurde.
Das paritätisches Regierungssystem beinhaltete die Gleichberechtigung und exakte Ämterverteilung zu gleichen Teilen zwischen Katholiken und Protestanten und wurde durch den Westfälischen Frieden 1648 für die vier so genannten Paritätischen Reichsstädte Ravensburg, Augsburg, Biberach und Dinkelsbühl betätigt. Es bestand ins 19. Jahrhundert.

Die ohnehin schon – unter anderem durch die Verlagerung von Handelsrouten nach Augsburg – wirtschaftlich geschwächte Stadt musste (bedingt durch Hunger und Seuchen) im Dreißigjährigen Krieg einen starken Bevölkerungsrückgang hinnehmen. Davon erholte sie sich in den folgenden Jahrhunderten nur schwer. So kam es bis in die Neuzeit zu keinen weiteren Stadterweiterungen, das Stadtbild blieb bis ins 19. Jahrhundert weitgehend unverändert.

Gegen Ende des Dreißigjährigen Krieges wurde auch die Burg über der Stadt – nun auch schon zur Unterscheidung von der Stadt Ravensburg "Veitsburg" genannt – von schwedischen Truppen geschleift. Nur einige Wirtschaftsgebäude blieben stehen. Der Landvogt der Reichslandvogtei Schwaben residierte daher ab 1647 in Altdorf-Weingarten.

1803 wurde durch den Reichsdeputationshauptschluss die Reichsunmittelbarkeit der meisten Reichsstädte aufgehoben (Mediatisierung) und kirchlicher, bisher ebenfalls reichsunmittelbarer Grundbesitz, verstaatlicht (Säkularisation). Beide Maßnahmen dienten unter anderem der territorialen Entschädigung von Reichsfürsten, die durch die napoleonischen Eroberungen linksrheinische Gebiete verloren hatten.

Die Stadt wurde zunächst Bayern (siehe Landgericht Ravensburg (Bayerische Verwaltungseinheit)) zugeschlagen. Da die umliegenden habsburgischen Lande an Württemberg fielen, die Reichsklöster Weingarten und Weißenau wiederum anderen Reichsfürsten zugeordnet wurden, entstand eine Enklavensituation, die wirtschaftlich sehr abträglich war. Erst 1810 kam Ravensburg nach einem Gebietstausch im Allgäu – bestätigt durch den Wiener Kongress 1815 – zum Königreich Württemberg, das zuvor auch schon die ausgedehnten Territorien von Weingarten (einschließlich des heutigen Schlosses in Friedrichshafen) erworben hatte. Die Stadt wurde Sitz des Oberamts Ravensburg.

Mit der Zugehörigkeit zum Königreich Württemberg begann eine allmähliche wirtschaftliche Erholung. Die industrielle Entwicklung stützte sich vor allem auf die lange Tradition in der Nutzung der Wasserkraft. Bereits 1402 war in Ravensburg eine der ersten Papiermühlen nördlich der Alpen entstanden; auch für andere industrielle Zwecke wurden seit langem Wassermühlen eingesetzt.

Darauf aufbauend entwickelte sich nun schnell eine vielfältige Maschinenbau-Branche; ein weiteres Standbein war die Textilherstellung. Mit dem Bau der Bahnstrecke Ulm–Friedrichshafen trug ab 1847 dieses neue Verkehrsmittel erheblich zum Aufschwung bei. Im 19. Jahrhundert fand in Ravensburg die Vermittlung der Schwabenkinder, die aus armen bäuerlichen Verhältnissen in Tirol und der Schweiz kamen, auf dem „Hütekindermarkt“ statt.

Um die Mitte des 19. Jahrhunderts war Ravensburg die größte und industriell am weitesten entwickelte Stadt Oberschwabens und auch politisch ein Zentrum der Region.

In der Zeit des Nationalsozialismus wurden 691 Patienten der psychiatrischen Heilanstalt Weißenau als Opfer der „Euthanasie“ in der Tötungsanstalt Grafeneck ermordet. Die in der Stadt ansässigen Sinti wurden zunächst im Zigeunerzwangslager interniert. 36 Sinti wurden 1943 deportiert, 29 von ihnen in Auschwitz ermordet. Die wenigen damals in Ravensburg ansässigen Juden wurden zur Flucht gezwungen; einige wurden als Opfer des Holocaust ermordet.

Im Zweiten Weltkrieg war Ravensburg kaum von Luftangriffen der Alliierten betroffen, die historische Bausubstanz blieb daher vollständig erhalten.

Insbesondere ab den 1950er Jahren wurde die Stadt in alle Richtungen durch Wohngebiete erweitert; deren größtes ist die "Weststadt", inzwischen der einwohnerreichste Stadtteil. Nachdem die Einwohnerzahl bereits in den 1930er Jahren die Grenze von 20.000 überschritten hatte, wurde die Stadt mit Inkrafttreten der baden-württembergischen Gemeindeordnung am 1. April 1956 kraft Gesetzes zur Großen Kreisstadt erklärt.

In den 1970er Jahren wuchs die Stadt durch Eingemeindung einiger Dörfer des Umlands weiter, besonders im Süden und im Westen (die heutigen Ortsteile Eschach, Schmalegg und Taldorf).

1982 fanden in Ravensburg die Heimattage Baden-Württemberg statt.

In die Stadt Ravensburg wurden folgende Gemeinden eingegliedert. Sie gehörten vor der Kreisreform überwiegend zum Landkreis Ravensburg, Adelsreute gehörte bis 1. Januar 1969 noch zum Landkreis Überlingen und wurde dann dem Landkreis Ravensburg eingegliedert.

Der ursprünglich zur Gemeinde Adelsreute gehörende Ort Tepfenhart wurde am 1. Dezember 1974 nach Horgenzell umgegliedert.

Einwohnerzahlen nach dem jeweiligen Gebietsstand (jedoch ohne Weingarten 1939). Die Zahlen sind Volkszählungsergebnisse (¹) oder amtliche Fortschreibungen der jeweiligen Statistischen Ämter (nur Hauptwohnsitze).

¹ Volkszählungsergebnis

Am 30. September 2011 hatte Ravensburg erstmals mehr als 50.000 Einwohner.

Die Stadt Ravensburg ist Sitz des Gemeindeverwaltungsverbands Mittleres Schussental.

Die Kommunalwahl vom 25. Mai 2014 führte zu folgendem Ergebnis:
An der Spitze der Stadt Ravensburg stand ab dem 13. Jahrhundert ein Ammann, der bis 1348 das alleinige Stadtoberhaupt war. Danach gab es den Bürgermeister und der Ammann war nur noch Vorsitzender des Gerichts. Um 1220 ist auch ein Rat genannt. 1531 erhielt die Stadt eine neue Ratsordnung mit drei Bürgermeistern, die mit zwei geheimen Räten den "Geheimen Rat" bildeten. Nach dem Dreißigjährigen Krieg gab es zwei Bürgermeister und vier geheime Räte, die konfessionell paritätisch (evangelisch und katholisch) besetzt waren. 1803 wurde die bayerische und ab 1810 die württembergische Verwaltung eingesetzt. Danach gab es einen Stadtschultheiß und den Rat. Mit der neuen Gemeindeordnung von 1906 wurde der Titel des Stadtschultheiß ersetzt durch die Amtsbezeichnung Oberbürgermeister. Die neue Gemeindeordnung beseitigte die lebenslange Dauer des Bürgermeisteramtes, ließ Wiederwahl jedoch zu. Heute wird der Oberbürgermeister für eine Amtszeit von acht Jahren gewählt. Er ist Vorsitzender des Gemeinderats und Leiter der Stadtverwaltung. Der Oberbürgermeister hat einen Beigeordneten als hauptamtlichen Stellvertreter. Er trägt die Amtsbezeichnung „Erster Bürgermeister“.

Das Motiv des Stadtwappens von Ravensburg ist 1267/1268 erstmals auf Stadtsiegeln belegt. Es zeigt ein offenes Stadttor zwischen zwei Wehrtürmen und symbolisiert damit zugleich Wehrhaftigkeit und Offenheit der Freien Reichsstadt.

Wappenbeschreibung: "In Silber auf gemauerter Konsole eine doppeltürmige blaue Burg mit hochgezogenem Fallgatter; zwischen den Zinnentürmen ein blauer Schild, darin ein silbernes Kreuz mit Tatzenenden".

Vom 19. Dezember 1940 bis zum Frühjahr 1946 führte die Stadt Ravensburg ein leicht abgeändertes Wappen, auf dem statt des blauen Schilds mit silbernem Kreuz ein roter Welfenlöwe mit einem roten Schildchen (mit silbernem Querbalken) dargestellt war. Diese Farben und Symbole entstammen dem Wappen der damals eingemeindeten Nachbarstadt Weingarten.

Die Ravensburger Ortschaften führen kein amtliches eigenes Wappen, verwenden aber inoffiziell die Wappen der ehemals selbständigen Gemeinden Eschach, Schmalegg und Taldorf (dort zusammen mit dem Wappen der ehemaligen Gemeinde Adelsreute).

Die Stadtflagge ist blau-weiß und wird meist mit dem Stadtwappen verwendet.

Als inoffizielle Hymne gilt das Heimatlied "Mein Ravensburg im Schwabenland" des Oberlehrers Wilhelm Mayer aus dem Jahr 1924. Ursprünglich wurde das Lied nach einer Melodie von Karl Friedrich Zelter gesungen; seit 1952 wird eine eigens komponierte Melodie von G. Heim verwendet. Die ersten drei Strophen des Heimatlieds werden insbesondere bei den Veranstaltungen des Rutenfests häufig gesungen; viele Ravensburger kennen sie daher auswendig.

Ravensburg unterhält Städtepartnerschaften mit folgenden Städten:

Partnerschaftliche Beziehungen von Ortsteilen:

Das historische Stadtbild konnte trotz eines Teilabbruchs der alten Stadtmauer, deren Steine im 19. Jahrhundert zum Bau des Bahnhofs und zum Bau von Fabriken verwendet wurden, erhalten werden. Im Zweiten Weltkrieg blieb Ravensburg wegen seiner strategischen und rüstungsindustriellen Bedeutungslosigkeit und auch dank eines großen, von Schweizern geführten Versorgungszentrums des Roten Kreuzes von größeren Angriffen der alliierten Luftwaffe verschont. Sanierungs-Bausünden, vor allem der 1970er-Jahre, hielten sich in Grenzen. In den 1980er Jahren wurde die Altstadt aufwendig saniert und für den Durchgangsverkehr geschlossen.

Durch die ganze Altstadt vom "Grünen Turm" im Norden bis zur Straße "Hirschgraben" im Süden zieht sich der langgestreckte Marienplatz und gliedert sie in zwei unterschiedliche Teile: 

im Süden auf der Westseite des Platzes:
nördlich anschließend auf der Ostseite:
gegenüber dem Waaghaus zwischen den beiden Abgängen der Bachstraße:
Nördlich des Waaghauses verlassen zwei Straßen den Platz. Die Marktstraße führt südostwärts zum "Obertor", die Kirchstraße parallel zum Platz nordwärts zum "Frauentor". Auf ihrer Ostseite stehen: 



Zur letzten Mauerring gehören:

Kirchengebäude sind – teilweise ausführlich – unter Kultur → Religionen behandelt.

Den "Kreuzbrunnen" am Frauentorplatz ziert eine Kreuzigungsgruppe des 17. Jahrhunderts. Die Originale der Figuren wurden im 20. Jahrhundert aus konservatorischen Gründen durch Kopien ersetzt.

Ein Kriegerdenkmal von 1878 mit Germaniastatue von Josef Dressel (im Alten Friedhof an der Georgstraße) und eine Kaiser-Wilhelm-Büste an der Karlstraße sind Beispiele für Denkmäler des 19. Jahrhunderts. Bei der Aufstellung des Kaiser-Wilhelm-Denkmals kam es 1888 zum Eklat, da die katholische Bevölkerung aufgrund des Kulturkampfs nicht an der Einweihung teilnahm. In großer Zahl sind Grabdenkmäler des 19. und beginnenden 20. Jahrhunderts auf dem Hauptfriedhof Ravensburg erhalten.

Auch in den 1960er Jahren erregte die geplante Aufstellung einer Skulptur heftige Diskussionen zwischen den Konfessionen: eine nach einem Kriegsgelübde gestiftete "Mariensäule" als Friedenssäule wurde von der evangelischen Bevölkerung am Standort Marienplatz vehement abgelehnt. Die von der Künstlerin Maria Elisabeth Stapp gestaltete Säule wurde dann an der Wilhelmstraße bei der katholischen Liebfrauenkirche aufgestellt (und später etwas versetzt an den heutigen Standort an der Herrenstraße).

Die Brunnenskulptur "Caide" des in Ravensburg geborenen Bildhauers Robert Schad (in der Öffentlichkeit ausschließlich als "Schad-Brunnen" bekannt) steht auf dem nördlichen Marienplatz. Schad selbst hat die mehrteilige Skulptur als Darstellung von „Kopf- und Bauchmenschen“ interpretiert. Die Aufstellung Anfang der 1990er Jahre war von heftigen Kontroversen begleitet. Der ursprünglich geplante Aufstellungsort zwischen Rat- und Lederhaus wurde verworfen, auch wurde Schads ursprünglicher Wettbewerbsentwurf als ungeeignet kritisiert. Nach Änderungen durch Robert Schad konnte der Brunnen am jetzigen Standort aufgestellt werden. Danach kritisierten Anwohner die Lärmentwicklung, so dass zusätzlich lärmdämpfende Metallgitter in das Becken eingebaut werden mussten.

Den westlichen Stadteingang (an der Einmündung in die Weststadt aus Richtung Meersburg) markiert die Skulptur "Das blaue Haus" von Ottmar Hörl von 1997/1998.

Die Skulptur "Ravensburger Kindermarkt" von Peter Lenk wurde 2002 von privater Seite an der Ecke eines Hauses in der Bachstraße angebracht und zeigt ein Schwabenkind, das auf seinen Schultern einen Knecht trägt, der auf seinen Schultern wiederum einen Geistlichen zu tragen hat. Hintergrund: bis März 1914 wurden alljährlich arme Bergbauernkinder aus Tirol, Vorarlberg und Graubünden als Saisonarbeitskräfte auf Bauernhöfen in Oberschwaben und im Allgäu eingesetzt, die am Ravensburger „Hütekindermarkt“ in der Bachstraße an die Bauern vermittelt wurden.

In der oberen Marktstraße wurde 2011 in privater Initiative die monumentale Holzskulptur "Ibykus II" von Klaus Prior aufgestellt.

Von 2009 bis 2013 entstand in der Ravensburger Innenstadt ein „Museumsviertel“ mit vier neuen Museen:





Zahlreiche moderne Gemälde aus der Sammlung des Landkreises Ravensburg sind in den Gängen des Landratsamts (Friedenstraße) ausgestellt. Außerdem gibt es in Ravensburg mehrere private Galerien, die Wechselausstellungen zeigen.

Die Freiwillige Feuerwehr betreibt in der Feuerwache im Salzstadel ein Feuerwehrmuseum mit 15 Großgeräten und weiteren Ausstellungsstücken. Die Ortschaft Eschach besitzt ein eigenes Heimatmuseum in Weißenau.

Ravensburg verfügt über die größte Sammlung historischer Theaterkulissen in Deutschland. Sie besteht aus 135 auf Leinwand gemalten großformatigen Prospekten (ca. 10 × 5 Meter) und 208 Stellwänden in verschiedenen Formaten und Techniken. Die Kulissen entstanden zwischen 1902 und 1910 für das Ravensburger Theater. Sie haben einen einheitlichen künstlerischen Duktus, eine dreidimensional wirkende Illusionsmalerei. Sie stellen u. a. mittelalterliche Stadtbilder, Ratssäle oder Venedig-Ansichten dar und konnten in mehreren Stücken verwendet werden.

Die Entstehung der Sammlung wurde durch den Brand des Königlich-Württembergisches Hoftheaters in Stuttgart im Januar 1902 begünstigt. Der württembergische König Wilhelm II., der im nahegelegenen Friedrichshafen eine Sommerresidenz unterhielt, entsandte das Hoftheaterensemble nach dem Brand an das damalige Konzerthaus in Ravensburg, das bis 1910 Ersatzbühne blieb. Die Kulissen wurden im Stuttgarter Marsstall in zwei Malsälen unter Leitung des Hofrats Wilhelm Plappert angefertigt und anschließend nach Ravensburg transportiert. Nach dem 1. Weltkrieg änderte sich die Aufführungspraxis, so dass derartige Kulissen nicht mehr benötigt wurden.

Die Sammlung lagert im Kulissenhaus des Theaters. Heute dürfen diese Kulissen aus Gründen des Brandschutzes nicht mehr bei Theateraufführungen verwendet werden. Digitale Reproduktionen wurden jedoch in Aufführungen von "Phantom der Oper" und "Giulio Cesare" im Jahr 2013 verwendet.

Das Gebiet der heutigen Stadt Ravensburg gehörte anfangs zum Bistum Konstanz und war dem Archidiakonat Allgäu Landkapitel Ravensburg unterstellt. 
Im ausgehenden 15. Jahrhundert kam es hier zu einer ersten großen Welle von Hexenverfolgungen in Europa. Mindestens 48 Frauen wurden als Hexen in der Stadt Ravensburg bei lebendigem Leib verbrannt. Es kam auch zu zwölf Freisprüchen, die möglicherweise mit Einsprüchen und Bürgschaften der jeweiligen Familien zusammenhingen. Die Prozesse führte zum Teil der päpstliche Inquisitor Heinrich Institoris, der Autor des bald weit verbreiteten Hexenhammers. Weitere Prozesse nach Anklagen der weltlichen Obrigkeit folgten in der weiteren Region und bis ins 17. Jahrhundert. Als päpstlichen Dank gab es die Verkaufsvollmacht für Ablässe für die Kirche in der Stadt.
Eine Marienkirche wird bereits um 1250 als Filiale von Altdorf erwähnt. 1275 wurde sie Pfarrkirche und ab 1279 der Abtei Weingarten inkorporiert. 1340 bis 1380 wurde die Pfarrkirche Liebfrauen an der Stelle der alten Marienkirche neu erbaut, später gab es mehrere Umbauten. In der Liebfrauenkirche war auch der ursprüngliche Standort der Ravensburger Schutzmantelmadonna, ein Werk des Ulmer Schülers Michael Erhart. Die Kirche blieb bis 1802 beim Kloster Weingarten inkorporiert. Gegenüber der Kirche entstand das Stadthaus des Klosters Weingarten, das den Reichtum des Klosters widerspiegelte (heute als Technisches Rathaus mit Kultur- und Einwohnermeldeamt genutzt).

Im 14. Jahrhundert wurde Sankt Jodok als zweite Pfarrkirche Ravensburgs durch das Kloster Weißenau und dem städtischen Rat erbaut. Sie war für die Unterstadt zuständig und war bis 1802 dem Kloster Weißenau inkorporiert. Im Weiler Sankt Christina wurde im 13. Jahrhundert eine weitere Pfarrei errichtet, nachdem dort bereits im 12. Jahrhundert eine gleichnamige Kirche erwähnt ist, die 1253 neu erbaut wurde. Diese Pfarrei war für den Süden und Westen Ravensburgs zuständig und war ebenfalls Weißenau unterstellt.

Nachdem ein beträchtlicher Teil der Bevölkerung zum Protestantismus übergetreten war, erhielt die protestantische Gemeinde das Langhaus der 1349 geweihten Karmeliterkirche für ihre Gottestdeinste. Der Chor des Kirchengebäudes blieb jedoch bis 1806 noch im Besitz des Karmelitenordens; die Kirche wurde also zwischen beiden Konfessionen geteilt. Eine Mauer trennte die beiden Bereiche. Um die Benutzung der Kirche wurden jahrhundertelang Streitigkeiten ausgetragen; im 17. Jahrhundert stritten sich der evangelische Mesner und die Karmeliten zum Beispiel darum, wer das Gras auf dem Kirchhof mähen dürfe.

Die evangelische Kirchengemeinde Ravensburg blieb bis 1802 selbständig und wurde dann in die Evangelische Landeskirche in Württemberg eingegliedert. Ravensburg wurde daraufhin Sitz eines Dekanats (siehe Kirchenbezirk Ravensburg). Die evangelische Stadtkirche Ravensburg erhielt 1842/1845 einen Turm. Eine zweite evangelische Kirche gab es bereits ab 1628. Dabei handelte es sich um ein ehemaliges Kornhaus, das zur Dreifaltigkeitskirche umgebaut worden war. Diese Kirche wurde aber 1852 abgebrochen.

Weitere Klöster im Stadtgebiet waren das Franziskanerinnenkloster Sankt Michael (13. Jahrhundert) und das vor der nördlichen Stadtmauer gelegene Kapuzinerkloster (gegründet 1629). 1806 wurden die drei Klöster säkularisiert. Weitere kirchliche Einrichtungen waren das Spital zum Heiligen Geist, das im 15. Jahrhundert erbaut wurde (Spitalkapelle von 1498), die Kapelle Sankt Leonhard (bereits im 15. Jahrhundert profaniert) und die Mühlbrugg-Kapelle aus dem 15. Jahrhundert, die 1812 abgebrochen und 1929 neu erbaut wurde.

Die katholischen Gemeinden gehörten bis 1802 noch zum Bistum Konstanz. 1808 wurden die Gemeinden dem Ordinariat Ellwangen unterstellt, aus dem 1821/1827 das neu gegründete Bistum Rottenburg (heute Rottenburg-Stuttgart) hervorging. Nach dem Zweiten Weltkrieg entstanden zu den drei bisherigen katholischen Gemeinden (Liebfrauen, Sankt Jodok und Sankt Christina) noch zwei weitere Gemeinden, „Christus König“ (Christkönigkirche in der Südstadt von 1952) und „Zur heiligsten Dreifaltigkeit“ (Dreifaltigkeitskirche in der Weststadt von 1965).

Weitere katholische Kirchengemeinden gibt es in den Stadtteilen (siehe Artikel Schmalegg, Taldorf und Eschach). Alle katholischen Kirchengemeinden gehören zum Dekanat Ravensburg des Bistums Rottenburg-Stuttgart.
Dies betrifft auch ehemalige badische Exklaven(z. B. Adelsreute[Taldorf], Tepfenhart[Horgenzell] u. a.), die kirchenrechtlich formell weiterhin zum Erzbistum Freiburg gehören, das die ehemals badischen und hohenzollerschen Gebiete umfasst.

Vor der Eingemeindung der Umlandgemeinden in den 1970er Jahren gehörten etwa 70,7 Prozent der Ravensburger der römisch-katholischen Kirche an, 25 % waren evangelisch (Stand: 1970).
Nach dem Zweiten Weltkrieg entstanden dann zwei weitere Kirchengemeinden, die Johannesgemeinde mit Kirche von 1963 für die Weststadt und die Kirchengemeinde Eschach für die südlichen Stadtteile Ravensburg. Diese drei Kirchengemeinden bilden heute die „Evangelische Gesamtkirchengemeinde Ravensburg“, zu der auch die Protestanten nahezu aller Stadtteile gehören. Lediglich Bavendorf hat noch eine eigene Kirchengemeinde; zu ihr zählen die evangelischen Christen der Ortschaft Taldorf. Die Ortschaft Schmalegg gehört zur Kirchengemeinde Wälde-Winterbach (Horgenzell). Alle genannten evangelischen Kirchengemeinden gehören zum Dekanat Ravensburg. Ferner gibt es in Ravensburg den Hensoltshöher Gemeinschaftsverband e. V. (Landeskirchliche Gemeinschaft Ravensburg).

Neben den beiden großen Kirchen gibt es in Ravensburg auch freikirchliche Gemeinden, darunter eine Evangelisch-Freikirchliche Gemeinde (Baptistengemeinde), die BFP Freie Christengemeinde sowie die „Adventgemeinde Ravensburg“ der Siebenten-Tags-Adventisten. Auch die Neuapostolische Kirche und die Zeugen Jehovas sind in Ravensburg vertreten.

Ravensburg ist eine Station des Oberschwäbischen Jakobswegs von Ulm nach Konstanz.

Von 1330 bis 1429 waren erstmals jüdische Familien als im Ort ansässig erwähnt. Sie lebten als kleine Jüdische Gemeinde gettoartig in der heutigen "Grüner-Turm-Straße", die bis 1934 „Judengasse“ hieß, und erbauten 1345 eine Synagoge. Nach ihrer Vertreibung 1429 vergingen Jahrhunderte, ehe sich erst im 19. Jahrhundert wieder eine kleinere Zahl jüdischer Familien niederließ, so dass es nicht mehr zur Wiedererrichtung eines Gotteshauses kam. Unter der nationalsozialistischen Gewaltherrschaft wurden sie erneut vertrieben oder im Holocaust ermordet. Seit 1983 erinnert eine Gedenktafel an dieses Geschehen.

In Ravensburg gibt es drei islamische Gemeinden. Die zentral in der Nordstadt (Schützenstraße) gelegene Mevlana-Moschee gehört zum DITIB-Dachverband. Sie verfügt über ein Minarett.
Daneben gibt es die Fatih Camii-Gemeinde und die bosnische Islamska Zajednica Bodensee Ravensburg e. V. (Islamische Gemeinschaft Bodensee-Ravensburg), deren Gebetsräume sich im selben, etwas abseits gelegenen Gebäude (Höll 19) befinden.

Außerdem gibt es eine aktive alevitische Gemeinschaft.

Das Buddhistische Zentrum Ravensburg wurde unter diesem Namen 1991 in Baienfurt gegründet. Im Jahr 1995 erfolgte der Umzug nach Weingarten. Seit dem Frühjahr 2000 wird der Buddhismus in der Ravensburger Weststadt gemäß der Tradition des Diamantweges der Karma-Kagyü-Linie praktiziert. Das Zentrum ist eines von über 600 Zentren weltweit und steht unter der spirituellen Leitung von Trinley Thaye Dorje und Ole Nydahl. 2008 wurde in der Ortschaft Untereschach ein weiteres buddhistisches Angebot eröffnet: das Kloster Viên Đức, eine Einrichtung der Congregation der Vereinigten Vietnamesischen Buddhistischen Kirche (CVBK).

Das "Konzerthaus Ravensburg", erbaut 1896–1897 vom Wiener Büro Fellner & Helmer, ist mit 574 Plätzen das größte Theater in Ravensburg. Es wird regelmäßig für Gastspiele, Konzerte und Aufführungen lokaler Kulturträger genutzt. 

Die älteste Theatertradition in Ravensburg hat das Rutentheater, bei dem Ravensburger Schüler seit 1821 jedes Jahr Märchenaufführungen präsentieren. Die Aufführungen finden ebenfalls im Konzerthaus statt.

Kleiner besetzte klassische Konzerte (Solokonzerte, Kammermusik) finden in Ravensburg normalerweise im mittelalterlichen "Schwörsaal" im Waaghaus in der Stadtmitte oder im barocken Festsaal des Klosters Weißenau im Ortsteil Weißenau statt.

Das 1987 begründete "Theater Ravensburg" spielt seit den 1990er Jahren mit eigenem Ensemble in fester Spielstätte (mit etwa 150 Sitzplätzen) in der Nordstadt. Auf dem Programm stehen hauptsächlich selbst produzierte Schauspiele und Kabarettprogramme.

Die "Kleinkunstbühne Zehntscheuer" präsentiert seit 1983 in einer renovierten Fachwerkscheune in der Innenstadt Theater, Musik, Kabarett und Comedy.

Das "Figurentheater Ravensburg" im Keller des Alten Theaters (rund 70 Sitzplätze) in der Innenstadt spielt Puppentheater für Kinder und Erwachsene.

Für größere Kulturveranstaltungen stehen im gemeinsamen Oberzentrum Ravensburg/Weingarten/Friedrichshafen das "Kultur- und Kongresszentrum" in Weingarten (bis zu 900 Plätze) und das "Graf-Zeppelin-Haus" (bis zu 1300 Plätze) in Friedrichshafen zur Verfügung; für noch größere Veranstaltungen wie etwa Rock- und Popkonzerte werden die "Oberschwabenhalle" in Ravensburg (erbaut 1959) und die "ZF-Arena Friedrichshafen" verwendet.

Der größte Sportverein in Ravensburg ist die Sektion Ravensburg des Deutschen Alpenvereins mit fast 8500 Mitgliedern. Der TSB Ravensburg besitzt fast 4000 Mitglieder, die in zahlreichen Sportarten aktiv sind.

Den größten Zuschauerzuspruch der Ravensburger Sportvereine hat die Profimannschaft des EV Ravensburg (seit 2010 "Ravensburg Towerstars"), die seit der Saison 2007/08 in der zweiten Eishockey-Liga spielt und 2011 Meister dieser Liga wurde.


→ Hauptartikel: "Rutenfest Ravensburg"

Das fünftägige, zum Schuljahresende im Sommer stattfindende, "Rutenfest" ist ein seit dem 17. Jahrhundert belegtes traditionelles Volksfest, in das die Einwohner (auch ehemalige) sehr viel Engagement stecken. Trommelgruppen und Fanfarenzüge trommeln Stadtbewohner, Freunde und Förderer an, so dass tagelang allgegenwärtiger Trommelklang zu hören ist. Die Stadt ist mit Fahnen geschmückt, viele private Gartenfeste ergänzen das offizielle Programm.

Die "Rutenfestkommission Ravensburg" veranstaltet in Zusammenarbeit mit den Schulen und der Stadtverwaltung Programmpunkte, die auch viele Besucher aus der Region anziehen. Höhepunkte des Rutenfests sind

In der Reichsstadt Ravensburg wurden Fastnachtsbräuche mit Einführung der Paritätsverfassung abgeschafft.

1908 wurde die Fasnetsgesellschaft "Milka e. V." gegründet, die sich auf karnevalistische Saalveranstaltungen konzentriert. Der Name ist auf eine unpopuläre Milchpreiserhöhung zurückzuführen und steht für den Spottnamen „Milch-Kommandit-Aktien-Gesellschaft“. Unter diesem Motto fand damals ein Umzug statt, auf dem der ganze Produktionszweig von Milch, Butter und Käse in origineller Aufmachung dargestellt wurde.

Seit den 1960er Jahren wird in Ravensburg auch die schwäbisch-alemannische Fasnet gefeiert. Treibende Kraft ist die "Schwarze Veri Zunft", die nach dem Schwarzen Veri, einem Räuber des 19. Jahrhunderts, benannt wurde, mit ihren Narrenfiguren "Räuber", "Hexenliesel" und "Papierkrattler" (eine Figur, die auf die frühe Papiermacherei in Ravensburg ab 1402 Bezug nimmt und die angebliche Hochnäsigkeit der Papiermacher karikiert). Hauptanziehungspunkt ist der große Narrensprung am Fasnetsmontag, an dem auch viele Zünfte der Umgebung teilnehmen. Die Schwarze-Veri-Zunft ist Mitglied des Alemannischen Narrenrings. In allen Ravensburger Teilorten gibt es ebenfalls Narrenzünfte.

Die "Oberschwabenschau" ist eine jährlich im Oktober stattfindende, traditionelle regionale Produktmesse mit landwirtschaftlichem Schwerpunkt. Sie fand erstmals im September 1961 statt. 2009 hatte die neun Tage dauernde Ausstellung 92.000 Besucher.

Neuere jährliche Veranstaltungen sind das Spielefest "Ravensburg spielt", der Stadtlauf "Ravensburg läuft", das Jazzfestival "Jazz in town" und die Inlineveranstaltung "Ravensburg rollt".

Seit 1998 findet jährlich Ende September in mehr als 20 Galerien die "Ravensburger Kunstnacht" statt.

Das Karate-Sommercamp des KJC Ravensburg mit über 1000 Gästen aus dem In- und Ausland findet seit 1982 jährlich im Karate-Bundesleistungszentrum Ravensburg statt.

Gemeinsam mit der Nachbarstadt Weingarten ehrt die Stadt Ravensburg seit 1977 Künstler und Wissenschaftler mit dem „Kulturpreis der Städte Ravensburg und Weingarten“. Seit 1989 vergibt die Stadt den Kleinkunstpreis „Ravensburger Kupferle“. Der „Ravensburger Medienpreis“ ist eine Auszeichnung der Stiftung Ravensburger Verlag.

Ravensburg liegt an einem Kreuzungspunkt der Bundesstraßen 30, 32 und 33 etwa 15 km nördlich von Friedrichshafen.

Die nächsten Autobahnzugänge liegen bei Ulm (A 7, A 8) und bei Wangen (A 96).

Ravensburg hat schon seit 1847 einen Bahnhof an der Südbahn Ulm–Friedrichshafen, die Teil der ersten durchgehenden Strecke des württembergischen Eisenbahnnetzes von Heilbronn bis nach Friedrichshafen ist. Neben den Zügen der DB verkehrt seit 1993 auch die Bodensee-Oberschwaben-Bahn auf der Bahnstrecke.

Ravensburg gehört dem Bodensee-Oberschwaben Verkehrsverbund ("bodo") an, hat aber einen eigenen Tarif für den Stadtbus Ravensburg Weingarten.

Der nächstgelegene Verkehrsflughafen ist in Friedrichshafen.

→ "Hauptartikel: Straßenbahn Ravensburg–Weingarten–Baienfurt"

Im Jahre 1888 wurde eine 4,2 km lange dampfgetriebene Straßenbahnstrecke (Spurweite 1000 mm) zwischen Ravensburg und Weingarten eröffnet. Sie wurde 1910 elektrifiziert und 1911 durch eine 2,4 km lange Strecke bis Baienfurt erweitert. Am 23. Februar 1959 wurde die Strecke Ravensburg–Weingarten stillgelegt, im Juni 1959 folgte die Reststrecke Weingarten–Baienfurt.
Der ehemalige Betriebshof der Straßenbahn wird heute für die Busse der RAB genutzt.

Ravensburg hat seit 1990 auch einen eigenen 84 Meter hohen Fernmeldeturm (Geographische Koordinaten: ). Er ist im Unterschied zu zahlreichen Fernmeldetürmen ähnlicher Höhe kein Typenturm, sondern ein Sonderturm.

Der Name der Stadt ist nicht zuletzt durch die hier ansässige Firmengruppe "Ravensburger AG" weltbekannt geworden. International verbreitet sind besonders die Produkte der Ravensburger Spieleverlag GmbH und ihrer zahlreichen Schwesterunternehmen, die u. a. Gesellschaftsspiele und Puzzles herstellen, der vor allem für Kinder- und Jugendbücher bekannte Ravensburger Verlag sowie das „Ravensburger Spieleland“ (in der Nachbargemeinde Meckenbeuren).

"Tekrum" ist ein Hersteller von „Premium-Gebäck-Spezialitäten“. Das 1897 von Theodor Krumm gegründete Unternehmen war bis 1997 ein Familienbetrieb. Seit Januar 2017 gehört Tekrum vollständig zum Kambly SA-Konzern.

"OMIRA" (Oberland Milchverwertung Ravensburg GmbH) ist Hersteller von Milchprodukten aller Art, die zum Teil in ganz Deutschland und Teilen Europas erhältlich sind. Letzteres trifft insbesondere auf die unter der Marke "MinusL" vertriebene laktosefreie Milch zu.

Die "Mönchmühle", eine Handwerksmühle am Rande der Altstadt, spezialisiert auf die Herstellung hochwertiger Mehle für Bäckereien und Haushalte aus Weizen, Dinkel und Roggen.

Ansonsten ist die Region stark vom Maschinenbau geprägt, der sich aus der ausgeprägten Mühlentradition (Korn-, Papier-, Säge- und andere Mühlen) und dem Bedarf der frühindustriellen Papier- und Textilindustrie entwickelt hat. Wichtigste Vertreter dieser Branche in Ravensburg sind die heute zu "Voith" bzw. "Andritz AG" gehörenden Unternehmen, die aus dem 1856 eröffneten Betrieb des Schweizer Escher-Wyss-Konzerns (seit 1969 Sulzer) hervorgegangen sind. Weitere Maschinenbauunternehmen sind u. a. die "Maschinenfabrik Arnold GmbH & Co", die "Bezner Maschinen GmbH", die "LCM-Schokoladenmaschinen GmbH", die "Rugel Maschinenfabrik GmbH & Co. KG" und die "Schuler Konstruktionen GmbH & Co. KG".

Erwähnenswert sind außerdem die Automobilzulieferer-Firmengruppe "EBZ (Engineering Bausch & Ziege GmbH)", die 2008 die "ThyssenKrupp Drauz Nothelfer GmbH" von der ThyssenKrupp Technologies übernommen hat (firmiert neu als EBZ SysTec), die Verpackungshersteller "Paccor Packaging Solution" (früher Zach Verpackungen), die in den 80ern von der Autobar Gruppe übernommen und weiter an die Veriplast Gruppe veräußert wurde, und "Moosmann & Co." die Werkzeugfabrik "Hawera Probst" und das aus einer traditionsreichen Ravensburger Apotheke hervorgegangene Pharmaunternehmen "Vetter Pharma".

Auch Unternehmen aus dem Bereich Erneuerbare Energien sind in Ravensburg ansässig, wie etwa die Mage Solar AG, ein internationaler Anbieter von Systemkomponenten für Photovoltaikanlagen auf Wohn-, Gewerbe- und Nutzgebäuden sowie auf Freiflächen.
Die Firmen der "Solpower-Gruppe", insbesondere die "Solpower AG" und die "Pro Solar Solarstrom GmbH" sind bedeutende Anbieter von photovoltaischen Anlagen unter eigenem Markennamen. Auch thermische Solaranlagen werden von verschiedenen kleineren Unternehmen projektiert; die in diesem Bereich zeitweise bedeutende "Pro Solar Energiesysteme GmbH" gehört mittlerweile zur "General Solar Systems Deutschland GmbH" in Regensburg und hat einen Großteil der Arbeitsplätze dorthin verlagert.

Das hier ansässige "Munzinger-Archiv" bietet recherchierte, verifizierte und grundlegende Daten für die alltägliche Arbeit in journalistischen Redaktionen, Verlagen, Sendern oder für sonstige Interessenten auf der ganzen Welt. Die kostenpflichtige Datenbank umfasst Personen, Länder, Sport, Chronik, Pop, Gedenktage, Film, KLG (ein kritisches Lexikon zur deutschsprachigen Gegenwartsliteratur), KDG (Komponisten der Gegenwart). Der Zugriff ist über das Online-Portal, über CD-ROM oder die klassische Loseblattsammlung möglich.

Die "dwp eG" wurde 1988 von mehreren Weltläden in der Region Oberschwaben gegründete und ist mittlerweile Deutschlands zweitgrößter Importeur von Fairhandels-Produkten. dwp vertreibt die Waren über Weltläden, Regionalverteiler, Großverbraucher und Naturkostläden in Deutschland und im europäischen Ausland.

"Für Informationen zum Rundfunksender „Ravensburg“ siehe:" Sender Ravensburg (Wilhelmskirch)

In den 1920er Jahren erlangte die Firma "Hermann Spohn" mit exklusiven Karosserien, die meist auf Maybach-Fahrgestelle montiert wurden, einiges Ansehen. Verschiedene Mitglieder der Industriellenfamilie Spohn – tätig in der Textil-, Maschinenbau- und Baubranche – traten in Ravensburg seit Ende des 19. Jahrhunderts als großzügige Spender auf. Julius Spohn stiftete u. a. das Konzerthaus und das Gebäude für das humanistische Gymnasium, heute "Spohn-Gymnasium". Auf dem Hauptfriedhof hat die Familie eines der aufwendigsten Familiengräber.

Bis 2013 war das Unternehmen Carthago, ein bekannter Hersteller von Wohnmobilen, im Teilort Schmalegg ansässig.

Im Mittelalter wurde an den Hängen des Schussentals Wein angebaut. Ein Großteil der Rebflächen südlich der Stadt unterhalb Sankt Christina bis nach Weingartshof waren im Besitz des Klosters Weißenau, teilweise auch an Ravensburger Bürger verpachtet, während die Flächen im Rauenegg östlich der Stadt im Eigentum Ravensburger Bürger waren.
Nach einigen klimatisch schlechten Jahren Ende des 18. Jahrhunderts richtete das Kloster Weißenau im Kirchsprengel Sankt Christina oberhalb der Weinberge eine Bierbrauerei ein. Im Laufe des 19. Jahrhunderts folgten weitere schlechte Jahre. Außerdem verdrängten wohl Importweine den einheimischen Wein, der wegen ungenügender Ausbaumethoden qualitativ nicht mithalten konnte. So wurden im Laufe des 19. Jahrhunderts die meisten Rebgärten aufgegeben. Inwieweit die sich damals ausbreitende Reblaus oder der zunehmende Bierkonsum auch eine Rolle spielten, ist nicht geklärt. Große Teile der Flächen am Rauenegg wurden Anfang des 19. Jahrhunderts wertvolles Bauland; im Bereich Sankt Christina existierte noch bis ca. 1960 ein privat betriebener Weinbaubetrieb.

Auf den weniger steilen Flächen wurden teils Obstgärten und Streuobstwiesen angelegt, in Stadtnähe wurden einige Flächen auch in Sommerfrischen und Schrebergärten umgewandelt. Seit rund 30 Jahren wird vereinzelt wieder Wein angebaut. Die Stadt Ravensburg betreibt einen Weinberg mit den Sorten Müller-Thurgau und Spätburgunder am Rauenegg; außerdem gibt es einen gemeindeeigenen Weinberg in der Ortschaft Taldorf. Die Flächen zählen zum Bereich Württembergischer Bodensee des Anbaugebiets Württemberg.

Daneben war Ravensburg in früheren Jahrhunderten für seine Leinenproduktion bekannt; zum Rösten des Flachses wurde am Flappach (Stadtbach) oberhalb der Stadt in Ittenbeuren eine große Anzahl kleiner Teiche angelegt, die heute als Fischteiche genutzt werden.

Heute herrschen vor allem im Süden und Westen der Stadt Obstbau und Hopfengärten vor. Das Kompetenzzentrum Obstbau-Bodensee im Ortsteil Bavendorf betreibt Grundlagenforschung sowie anwendungsorientierte Untersuchungen und Beratung von Landwirten.

Daneben spielt in der Region Milchwirtschaft noch eine gewisse Rolle, besonders in den tieferen und stadtnahen Lagen geht deren Bedeutung aber immer weiter zurück. Fast ganz verschwunden aus der Landschaft ist der früher für Oberschwaben so typische Getreideanbau (vor allem Dinkel, aber auch Gerste und Weizen); auf dem Rahlenhof jedoch wird heute sogar Bio-Braugerste angebaut.
Die Region zeichnet sich überhaupt durch einen hohen Anteil von Demeter- und Biolandbetrieben aus.

Die "Schwäbische Zeitung" hat in Ravensburg ihren Sitz und betreibt dort auch eine Lokalredaktion. Auch die regionalen Anzeigenblätter "Wochenblatt" und der "Südfinder" sind in Ravensburg mit Lokalredaktionen vertreten.

Hinzu kommen die Anzeigenblätter "Stadtkurier" und "Südfinder" vom Medienhaus Schwäbischer Verlag GmbH & Co. KG (Schwäbische Zeitung). Diese erscheinen längst nicht mehr nur im Stadtgebiet Ravensburg. Zusätzlich ist das vom Südkurier Medienhaus herausgegebene "Ravensburger Stadtmagazin" in der Stadt ansässig. 

In Ravensburg befinden sich außerdem Sendestudios der Radiosender Radio 7 und SWR4 Baden-Württemberg sowie ein Studio des Regionalfernsehsenders Regio TV Bodensee.

Das "Munzinger-Archiv" (siehe „ansässige Unternehmen“) wird als Personen-Datenbank von Verlagen und Journalisten bundesweit täglich genutzt.

Ravensburg ist Sitz des Amtsgerichts Ravensburg, des Landgerichts Ravensburg, verschiedener Kammern des Arbeitsgerichts Ulm, der Polizeidirektion, der Staatsanwaltschaft Ravensburg sowie der Justizvollzugsanstalt Ravensburg im Stadtteil Hinzistobel.

In Ravensburg ist einer der Standorte der Dualen Hochschule Baden-Württemberg Ravensburg, der ehemaligen "Berufsakademie Ravensburg". Ferner gibt es eine Schule für Gestaltung ("Freie Kunstschule"), das "Institut für Soziale Berufe Ravensburg" in katholischer Trägerschaft sowie eine Zweigstelle der Württembergischen Verwaltungs- und Wirtschaftsakademie.

Die Hochschule Ravensburg-Weingarten hat ihren Sitz in Weingarten.

Die Außenstelle Weißenau der Universität Tübingen befasste sich von 1959 bis 1992 mit Astronomie (ein inzwischen demontiertes Radioteleskop mit 26 Meter Durchmesser ermöglichte dort radioastronomische Beobachtungen). Von 1983 bis zur Schließung 2001 befasste sich die Außenstelle auch mit Neuropsychologie.

Die Stadt Ravensburg betreibt drei allgemeinbildende Gymnasien (Albert-Einstein-Gymnasium, Spohn-Gymnasium, Welfen-Gymnasium), eine Realschule, eine Grund- und Hauptschule (Stefan-Rahl-Schule Obereschach), zwei selbständige Werkrealschulen (Werkrealschulen Kuppelnau und Neuwiesen), sieben selbständige Grundschulen (Grundschulen Klösterle, Kuppelnau, Neuwiesen, Oberzell, Schmalegg, Weißenau und Weststadt) und eine Förderschule (Sankt Christina).

Der Landkreis Ravensburg ist Träger von drei beruflichen Schulen: "Edith-Stein-Schule" (Haus- und landwirtschaftliche Schule, unter anderem mit ernährungswissenschaftlichem, agrarwissenschaftlichem und biotechnologischem Gymnasium), "Gewerbliche Schulen" (unter anderem mit technischem Gymnasium) und "Humpis-Schule" (kaufmännische Schule, unter anderem mit Wirtschaftsgymnasium). Auch die "Martinusschule" für Geistigbehinderte mit Schulkindergarten und die "Fachschule für Landwirtschaft" stehen unter Trägerschaft des Landkreises.

Ferner gibt es die "Staatliche Schule für Kranke Weißenau" und eine "Krankenpflegeschule" am Zentrum für Psychiatrie Weißenau.

Ravensburg beherbergt neben den staatlichen Schulen eine Reihe von Privatschulen, darunter das Bildungszentrum St. Konrad mit Grund- und Werkrealschule, Realschule und Gymnasium, die Bernd-Blindow-Schule, an der Ausbildung und Weiterbildung, die Fachhochschulreife, Abitur und sogar ein Studium möglich ist, die Grundschule Klösterle und die Theresia-Gerhardinger-Realschule (ehemals von den Armen Schulschwestern betrieben), eine Freie Waldorfschule, das Institut für sozialpädagogische Berufe, die Josef-Wilhelm-Schule (Private Berufsschule des Berufsbildungswerks Adolf Aich), das Kolping-Bildungswerk (mit Abendrealschule, Abendgymnasium, Kolping-Berufskolleg und sozialwissenschaftlichem Gymnasium), die Krankenpflegeschule und Kinderkrankenpflegeschule am Krankenhaus Sankt Elisabeth, die Krankenhausschule Sankt Nikolaus, die Private Kaufmännische Schule Schindele, ein Zentrum für Naturheilkunde und Homöopathie sowie die Hör-Sprachzentrum gGmbH mit Heimsonderschule für Gehörlose, Schwerhörige und Sprachbehinderte.

Ravensburg bietet insgesamt 1700 Kindergartenplätze in 32 Kindergärten in öffentlicher, kirchlicher oder privat-gemeinnütziger Trägerschaft (darunter unter anderem ein Waldorf-Kindergarten sowie ein Waldkindergarten) und erfüllt damit die gesetzlichen Vorgaben. Elf Einrichtungen in Ravensburg und Weingarten bieten darüber hinaus Betreuungsplätze für Kinder unter drei Jahren an.

Ihren Ursprung in der Sozialarbeit der katholischen Kirche haben das Berufsbildungswerk Adolf Aich und die Einrichtungen der Behinderten- und Altenhilfe der Stiftung Liebenau.

Darüber hinaus unterhält die aus der evangelischen Sozialarbeit entstandene BruderhausDiakonie Einrichtungen der Altenhilfe und der Sozialpsychiatrie. Die Zieglerschen Anstalten, seit 2009 mit dem neuen Namen „Die Zieglerschen“, betreiben ein Sprachheilzentrum, bieten betreutes Wohnen im Rahmen der Behindertenhilfe und verschiedene Betreuungsangebote im Rahmen der Jugendhilfe an.

Das Krankenhaus Sankt Elisabeth mit einer Abteilung für Kinder- und Jugendmedizin Sankt Nikolaus und das Heilig-Geist-Spital (geriatrischer Schwerpunkt) stehen unter Trägerschaft der größtenteils landkreiseigenen Oberschwabenklinik.

Im ehemaligen Kloster Weißenau und den umliegenden Neubauten liegt der Standort Weißenau des ZfP Südwürttemberg. Dort befindet sich eine Klinik für Psychiatrie und Psychotherapie sowie ein psychiatrisches Fachpflegeheim mit Außenwohngruppen und eine Klinik für forensische Psychiatrie und Psychotherapie. Die Weißenauer Werkstätten dienen der beruflichen Reintegration psychisch behinderter Menschen. In Ravensburg betreibt das ZfP Südwürttemberg außerdem eine Tagesklinik sowie die SINOVA Klinik für psychosomatische Medizin.

Die Stadt Ravensburg hat folgenden Personen das Ehrenbürgerrecht verliehen (chronologisch sortiert nach Verleihung):

Die Ehrenbürger der früheren Gemeinden Eschach und Taldorf werden ebenfalls in der Ehrenbürgerliste der Stadt Ravensburg geführt:

Träger der "Ehrenmedaille der Stadt Ravensburg" sind:

Zu den Trägern der Verdienstmedaille der Ortschaft Taldorf siehe Artikel Taldorf.


Die Deutsche Bahn hat einen ICE 3 mit dem Namen "Ravensburg" in Betrieb. Da die Bahnstrecke der Südbahn, an der Ravensburg liegt, jedoch nicht elektrifiziert ist, kann der am 15. April 2004 getaufte Triebzug 325 seine Patenstadt nicht anfahren.

Am 28. September 2008 legten die Ravensburger AG und die Stadt Ravensburg zusammen mit über 10.000 Puzzlefans auf dem Marienplatz in weniger als fünf Stunden das weltgrößte Puzzle mit 1.141.800 Teilen und einer Fläche von 600 Quadratmetern. Ravensburg übertraf damit den alten Puzzlerekord von rund 212.000 Teilen und bekam dafür einen Eintrag ins "Guinness-Buch der Rekorde". 20 Teile des Puzzles legten zuvor mit Hilfe von Geocachern bis zu 20.546 Kilometer zurück.





</doc>
<doc id="12003" url="https://de.wikipedia.org/wiki?curid=12003" title="ZDF">
ZDF

Das Zweite Deutsche Fernsehen (ZDF) ist eine der größten öffentlich-rechtlichen Sendeanstalten Europas mit Sitz in der rheinland-pfälzischen Landeshauptstadt Mainz. Gemeinsam mit den in der ARD zusammengeschlossenen Landesrundfunkanstalten und dem Deutschlandradio bildet das ZDF den öffentlich-rechtlichen Rundfunk in Deutschland. Das ZDF beschäftigt aktuell rund 3600 feste Mitarbeiter.

Noch vor der Gründung der Bundesrepublik im Jahr 1949 bestanden bereits einige Rundfunkanstalten. Konrad Adenauer betrachtete Rundfunk, Radio und Fernsehen als „politisches Führungsmittel“ und versuchte, auf die bestehende Rundfunkordnung Einfluss zu nehmen. Die Ziele des Bundes waren ein gesamtdeutscher und ein internationaler Radiosender sowie ein zweites Fernsehprogramm. Am 30. September 1959 verabschiedete das Bundeskabinett den „Entwurf eines Gesetzes über den Rundfunk“. Am 25. Juli 1960 wurde sodann die Deutschland-Fernsehen GmbH zum Betrieb des zweiten Fernsehprogramms gegründet. Einige Bundesländer riefen daraufhin das Bundesverfassungsgericht auf, um die Kompetenz von Bund und Ländern zur Gründung neuer Rundfunkanstalten prüfen zu lassen.

Nachdem das Bundesverfassungsgericht im 1. Rundfunk-Urteil vom 28. Februar 1961 das sogenannte „Adenauer-Fernsehen“ (die Deutschland-Fernsehen GmbH) untersagt hatte und die volle Rundfunkkompetenz den Ländern zusprach, beschlossen die Länder im März 1961 unabhängig von den bisherigen Anstalten, eine zentrale gemeinnützige Fernsehanstalt des öffentlichen Rechts zu gründen. Am 6. Juni 1961 unterzeichneten die Ministerpräsidenten auf der Ministerpräsidentenkonferenz in Stuttgart den Staatsvertrag über die „Errichtung der Anstalt des öffentlichen Rechts "Zweites Deutsches Fernsehen"“. Nachdem bis zum 1. Dezember 1961 nicht alle Länder den Vertrag ratifiziert hatten, trat der Staatsvertrag an diesem Tag zwar in Kraft, aber nur in den Ländern, die bis dahin die Ratifizierungsurkunden hinterlegt hatten (Baden-Württemberg, Nordrhein-Westfalen, Rheinland-Pfalz). Als letztes Land hinterlegte Bayern am 9. Juli 1962 die Ratifizierungsurkunde. Bei der Aufsicht über das ZDF wechseln sich die Länder in zweijährigen Perioden ab.

Für das zweite öffentlich-rechtliche Fernsehprogramm begann die Deutsche Bundespost um 1960 mit dem Aufbau einer zweiten Senderkette. Diese sendete im UHF-Bereich, was eine zweite Antenne und einen Fernsehapparat mit erweitertem Frequenzbereich erforderte. Für ältere Empfänger hielt der Handel spezielle, ca. 80 DM teure UHF-Konverter bereit. Bei der Senderplanung spielte – genau wie beim ersten Programm – ein optimaler Empfang in möglichst vielen Teilen der DDR eine wichtige Rolle. Um die Senderkette schon einmal nutzen zu können und die Zuschauer zum UHF-Empfang zu animieren, gestattete man der ARD mit dem Programm ARD 2 ein befristetes zweites Programm, das täglich von 20 bis 22 Uhr sendete. Sendestart war am 1. Mai 1961 im Sendegebiet des hr und einen Monat später bundesweit.

Bei dem Standort für den Sender gab es Meinungsverschiedenheiten zwischen den Ministerpräsidenten. Franz Meyers hatte sich zunächst für Essen eingesetzt, nachdem er aber mit dem Vorschlag alleine dastand, war er aber zusammen mit den übrigen Unions-Ministerpräsidenten für Mainz. Die sozialdemokratischen Ministerpräsidenten befanden, Mainz sei zu provinziell und schlugen Frankfurt am Main vor. Das Argument gegen Frankfurt bestand darin, dass es keine Stadt sein sollte, in der schon eine Landesrundfunkanstalt existierte. Auf der entscheidenden Konferenz brachte Franz Meyers überraschend Düsseldorf ins Spiel. Die erste Probeabstimmung ging über Frankfurt und ergab vier Stimmen dafür (Berlin, Bremen, Hamburg, Hessen), eine Enthaltung (Niedersachsen) und sechs Stimmen dagegen, die alle von den unionsgeführten Ländern kamen. Die zweite Probeabstimmung ging über Mainz oder Düsseldorf. Sie ergab fünf Stimmen für Düsseldorf (Berlin, Bremen, Hamburg, Hessen, Nordrhein-Westfalen), eine Enthaltung (Niedersachsen) und fünf Stimmen für Mainz. Georg-August Zinn argumentierte daraufhin, er sei in der Sache gegen Mainz, vom Persönlichen aber dafür, da man Peter Altmeier für seine jahrelangen Verhandlungen über ein zweites Fernsehprogramm danken müsse. Die nach getrennten Beratungen durchgeführte geheime Abstimmung ergab fünf Stimmen für Düsseldorf und sechs Stimmen für Mainz.

Bei der Wahl des Intendanten standen parteipolitische Überlegungen im Vordergrund. Eine Mehrheit der Unionsparteien gestattete es dem Fernsehrat, einen Vorschlag aus den eigenen Reihen zu machen, für die Wahl waren aber auch Stimmen aus den Kreisen der SPD erforderlich. So machte ein elfköpfiger Wahlausschuss einen ausgewogenen Besetzungsvorschlag: Der Legationsrat im Auswärtigen Amt Gerhard Brand sollte Intendant werden, die SPD sollte den Programmdirektor bestimmen dürfen, der gleichzeitig stellvertretender Intendant war, die CDU den Chefredakteur und die FDP den Verwaltungsdirektor. Brand lehnte aber ab, woraufhin sich der Ausschuss auf keinen Intendanten einigen konnte, es gab lediglich eine Mehrheit für Bruno Heck, der aber bei der geheimen Wahl des Fernsehrats am 27. Februar 1962 durchfiel. Ein neuer, nun sechsköpfiger Ausschuss machte daraufhin vier Vorschläge: Hans Bausch, Berthold Martin, Karl Holzamer und Wilhelm Vaillant, Mitinhaber der RIVA-Fernsehstudios. Die SPD-Mitglieder des Fernsehrats hielten alle vier Kandidaten für akzeptabel, den CDU-Mitgliedern behagten Bausch und Vaillant nicht und man entschied sich mit 16 zu 13 Stimmen zwischen Holzamer und Martin. Holzamer wurde dann mit 44 von 58 Stimmen vom Fernsehrat gewählt, bei neun Gegenstimmen und vier Enthaltungen.

Um am internationalen Programmaustausch teilnehmen zu können, musste das ZDF Mitglied bei der Europäischen Rundfunkunion werden. Die Landesrundfunkanstalten gingen zunächst davon aus, das ZDF würde Mitglied bei der ARD werden. Dies lehnte man aber beim ZDF strikt ab, da man seine Unabhängigkeit in Gefahr sah, und schlug bei einem ersten Treffen mit der ARD am 12. September 1962 in Stuttgart einen Dachverband vor, dem ARD und ZDF angehören sollten. Dies behagte den ARD-Vertretern aber nicht. So beantragte der ZDF-Intendant am 2. Mai 1962 eine eigene Mitgliedschaft für das ZDF, die aber abgelehnt werden musste, da nur Stationen aufgenommen werden durften, die schon ihren Sendebetrieb begonnen hatten. Weil die Positionen zwischen ARD und ZDF unverrückbar blieben, verzichtete das ZDF auf einen Programmaustausch vor Sendebeginn und beantragte dann eine erneute Mitgliedschaft. Der Verwaltungsrat der Europäischen Rundfunkunion stimmte auf seiner Sitzung vom 17. bis 20. Mai 1963 dem Antrag zu, woraufhin ARD und ZDF gleichberechtigte Mitglieder wurden.

Den Sendernamen "Zweites Deutsches Fernsehen" hatte man ohne größere Diskussionen im Staatsvertrag aufgenommen. Da manche Mitarbeiter in der Bezeichnung "Zweites" etwas Minderwertiges sahen, kam es zu Alternativvorschlägen:

Eine repräsentative Umfrage im Herbst 1962 wies für "Zweites Deutsches Fernsehen" die höchste Zustimmung aus, gefolgt von "Neues Deutsches Fernsehen" und "Deutsches Länderfernsehen". Die Rechtslage erlaubte aber keinen neuen Namen ohne Änderung des Staatsvertrags, so dass man die Abkürzung "ZDF" plus einen der Vorschläge erwog. Der Verwaltungsrat hielt aber die Zustimmung der Ministerpräsidenten für erforderlich, woraufhin die Diskussion vertagt und schließlich vom Intendanten eingestellt wurde.

Für das ZDF-Erkennungszeichen hat man einen Wettbewerb unter „zehn anerkannten Grafikern“ ausgeschrieben und sich im Januar 1963 für den Entwurf von G. Woldemar Hörnig entschieden. Er zeigte zwei Antennenmasten und zwei stilisierte Augen, wobei man im Sender gerne von Matschaugen sprach.

Am 21. März 1963 wählte man als akustische Senderkennung das 2. Thema aus dem 1. Satz des Violinkonzerts von Ludwig van Beethoven.

Mit der großen Programmreform vom Oktober 1973 führte man die ZDF-Hausschrift ein. Hierzu nahm Otl Aicher die Schriftart Univers und wandelte sie etwas ab. Da bei der damaligen Technik, mit der Schriften ins Bild eingeblendet wurden, die Buchstaben leicht abgerundete Ecken bekommen konnten, umging Aicher das Problem mit von vornherein – stark – abgerundeten Buchstaben.

Die Einführung der Hausschrift ging einher mit einem Corporate Identity, das ebenfalls von Otl Aicher stammte. Dazu gehörte die Gestaltung der Bildschirm-Zeituhr wie auch ein einheitliches Design der Studios und Übertragungswagen mit viel Blau, aber ohne Rot und Schwarz. Für politische Sendungen beschaffte man ein variables Rohrgestell-System von einem Schweizer Hersteller, an das man Schrifttafeln hängen konnte.

Als Sendebeginn war der 1. Juli 1962 vorgesehen, der sich jedoch verzögerte. Die erste Versuchssendung strahlte das ZDF in der Nacht vom 19. auf den 20. März 1963 ohne Ankündigung über den Sender Feldberg aus. Um 23.51 Uhr erschien der Schriftzug "Zweites Deutsches Fernsehen" und um 0.10 Uhr wurden zufällige Zuschauer gebeten, eine Postkarte mit der Beschreibung der Empfangsqualität an die Anstalt zu senden. Es folgte eine Dokumentation über Hong Kong, wobei es sich um eine FFG-Produktion handelte, zwei Folgen aus der US-Serie "Drei gute Freunde" und der österreichischen Produktion "Wolken über Kaprun". Um 1.28 Uhr endete die Ausstrahlung. Die zweite Versuchssendung lief in der Nacht vom 26. auf den 27. März über alle Sender der Kette. Sie zeigte auch Live-Bilder: eine Nachrichtensendung mit Schaltungen in die Inlandsstudios Hamburg und München.

Offizieller Sendebeginn des ZDF war der 1. April 1963. Zu diesem Zeitpunkt konnten 61 Prozent der Fernsehteilnehmer erreicht werden, die aber noch nicht alle ein Empfangsgerät für den UHF-Frequenzbereich besaßen. Bei der ARD hoffte man, nach dem Start des ZDF ein drittes Programm ausstrahlen zu dürfen. Dies ließ sich jedoch mangels freier Frequenzen nicht sofort realisieren.

Die erste Farbversuchssendung des ZDF lief wie bei der ARD am 3. Juli 1967, der Regelbetrieb begann am 25. August 1967. Inzwischen war die Reichweite so erhöht, dass ca. 80 Prozent der Teilnehmer versorgt wurden.

Der Sendebetrieb des ZDF begann in jenen provisorischen Studios in Eschborn, die man von der Freies Fernsehen Gesellschaft übernommen hatte. Es handelte sich um einen Bauernhof und einige ehemalige Arbeitslager-Gebäude, eine gern als "Telesibirsk" bezeichnete Umgebung. Darin waren zwei Studios mit 230 bzw. 160 Quadratmetern und je drei Kameras untergebracht, ferner ein Synchronstudio mit zwei zusätzlichen Kameras, damit es auch für Ansagen dienen konnte. Außerdem standen zur Verfügung: eine Magnetbildanlage, drei Filmabtaster für 16 und 35 Millimeter, eine 16-mm-Aufzeichnungsanlage und eine 16-mm-Entwicklung.

Der ZDF-Verwaltungsrat befand einen Ausbau in Eschborn als zu teuer, woraufhin ein Umzug in den provisorischen Studiokomplex der Taunusfilm in Wiesbaden, Unter den Eichen, erfolgte. Dort nahm man am 1. April 1964 den Sendebetrieb auf. Auf dem Gelände der Taunus-Film GmbH waren die Sendeleitung, Aktueller Dienst und Technik untergebracht, auf einem angrenzenden Grundstück Schneideräume, Kopierwerk, Fotolabor und Chefredaktion. Darüber hinaus hatte das ZDF bis zu 30 Räume im Großraum Mainz/Wiesbaden angemietet, in denen sich beispielsweise das Zentralarchiv oder die Synchronisation befanden.

Die Planungen sahen von vornherein einen zentralen Standort für alle Abteilungen vor. Hierzu kaufte das ZDF am 25. Juni 1964 ein 1,04 km² großes, im Mainzer Stadtteil Lerchenberg gelegenes Grundstück. Im ersten Bauabschnitt konnte ein Gebäude für die Übertragungswagen Anfang 1967 bezogen werden; der zweite Abschnitt, das 14-geschossige Redaktions- und Verwaltungsgebäude im Frühjahr 1974.

Am 15. September 1977 begannen die Erdarbeiten für den 3. Bauabschnitt des Sendebetriebsgebäudes, einen von der Planungsgruppe Stieldorf entworfenen Rundbau mit dem größten Durchmesser von 166 Meter. Nach sechsjähriger Bauzeit nahm dann schließlich am 6. Dezember 1984 das neue Sendezentrum (das damals größte in Europa) seinen Betrieb auf.

Die Verbundenheit des Stadtteils Lerchenberg zum ZDF spiegelt sich auch im Wappen wider.

Am 17. Juli 2009 ging ein neues Fernsehstudio in Betrieb. Es wird für die aktuellen Nachrichtensendungen verwendet und ist ein virtuelles Studio. Im senderinternen Jargon wird es die „Grüne Hölle“ genannt, weil die Wände komplett grün sind, um dort die Hintergründe und 3D-Grafiken digital hineinprojizieren zu können. Rund 30 Millionen Euro hat das Studio laut Medienberichten gekostet.

Bis zum 2. August 2015 produzierte man noch in SD, obwohl die meiste Studiotechnik bereits für HD vorhanden war. Es gab Probleme, das virtuelle Set unter Echtzeit-Bedingungen in HD zu rendern. Im Januar 2016 folgte auch die Umstellung der Technik für die Einspieler, seitdem ist das Studio komplett HD-fähig.

Heute verfügt der Sender über 16 Inlandsstudios, eines in jeder Landeshauptstadt. In Berlin wird seit Februar 2000 zusätzlich das Hauptstadtstudio im Zollernhof, Unter den Linden, betrieben. Für die Berichterstattung aus dem Ausland sind 18 Auslandsstudios zuständig, davon zwei in den Vereinigten Staaten (New York und Washington, D.C.).

Die Finanzierung des ZDF gestaltete sich in den ersten Jahren sehr schwierig. Von den 5 DM Radio- und Fernsehgebühren gingen 1,35 DM oder 27 % an die Post für das Sendernetz und den Gebühreneinzug, den Rest bekamen zu 70 % die ARD und zu 30 % das ZDF, was 1,095 DM ausmachte – erst 1972 kam es zu einer Erhöhung der Rundfunkgebühren. Während der ARD-2-Ära musste die Hälfte der Einnahmen an die ARD weitergegeben werden, danach bereitete der Bayerische Rundfunk Schwierigkeiten mit seinem Standpunkt, der Staatsvertrag verletze das Grundgesetz und die bayerische Landesverfassung, woraufhin die Zahlungen am 1. September 1963 endeten und erst nach einem Urteil des Bundesverwaltungsgerichts vom 5. November 1965 wieder flossen. Auch blieben die Werbeeinnahmen in den ersten Jahren hinter den Erwartungen zurück.

Aktuell zieht der Beitragsservice von ARD, ZDF und Deutschlandradio (bis 2012: GEZ) 17,50 Euro monatlich pro Wohnung ein, von denen rund 4,32 EUR an das ZDF gelangen. Der Etat lag 2005 bei ca. 1,6 Milliarden Euro (kaufkraftbereinigt heute: rund  Milliarden Euro). Vier Jahre später überschreitet das ZDF mit einem Ansatz von 2,048 Milliarden Euro erstmals knapp die Zwei-Milliarden-Grenze. Im Jahr 2012 verzeichnet das ZDF einen Umsatz von 2,028 Mrd. Euro. Für Sportrechte wurden in den Jahren 2013 bis 2016 durchschnittlich 243 Millionen Euro ausgegeben.

Aufgaben, Organisation und Finanzierung des ZDF sind im ZDF-Staatsvertrag geregelt. Vertragsparteien sind die 16 deutschen Bundesländer.

Der Fernsehrat überwacht das Programm, genehmigt den vom Verwaltungsrat beschlossenen Haushalt und wählt den Intendanten, der das ZDF nach außen vertritt und für die Geschäfte sowie die Programmgestaltung der Fernsehanstalt verantwortlich ist.

Der ZDF-Verwaltungsrat beschließt den Haushaltsplan und überwacht die Tätigkeit des Intendanten insbesondere in Haushaltsfragen. Er besteht aus 14 Mitgliedern, darunter fünf Vertreter der Bundesländer und ein Vertreter des Bundes. Die weiteren acht Mitglieder werden vom Fernsehrat gewählt und dürfen keiner Regierung oder gesetzgebenden Körperschaft angehören.

Vorsitzender ist Kurt Beck. Weitere Vertreter der Länder sind die Ministerpräsidenten Stanislaw Tillich, Matthias Platzeck, Horst Seehofer und der Erste Bürgermeister der Freien und Hansestadt Hamburg Olaf Scholz. Vertreter des Bundes ist der Staatsminister für Kultur und Medien Bernd Neumann. Aufgrund der „Causa Brender“ im Jahre 2009 ist die Einflussnahme der Parteien im Verwaltungsrat stark umstritten.

Der Erste Senat des Bundesverfassungsgericht (BVerfG) urteilte am 25. März 2014, dass mehrere Regelungen des ZDF-Staatsvertrages nicht mit dem Grundgesetz vereinbar sind. Die Zusammensetzung des „Fernsehrats“ und des „Verwaltungsrats“ verstößt in der heutigen Form gegen die Rundfunkfreiheit. Der Anteil von Politikern und „staatsnahen Personen“ muss von derzeit mehr als 40 Prozent auf ein Drittel reduziert werden. Zudem dürfen Politiker bei der Auswahl der aus gesellschaftlichen Gruppen entsandten Mitglieder des Fernsehrates „keinen bestimmenden Einfluss“ mehr ausüben. Das ZDF „darf nicht zum Staatsfunk werden“, meinte das Gericht in seiner Begründung durch Vizepräsident Ferdinand Kirchhof. Gesellschaftliche Meinungen sind „facettenreich wider(zu)spiegeln“.

Der von der FDP 2010 als Verfassungsrichter vorgeschlagene Jurist Andreas Paulus formulierte im Urteil eine abweichende Meinung: Das Urteil werde seinen eigenen Maßstab nicht gerecht, die Meinungen der Gesellschaft im Rundfunkrat abzubilden. Seiner Meinung nach ist die Beteiligung von Mitgliedern der Exekutive grundsätzlich schädlich, eine Drittelquote zu hoch. „Wenn die Aufsichtsgremien von Rundfunk und Fernsehen von denen beherrscht werden, deren Kontrolle sie unter anderem ermöglichen sollen, ist damit eine Beeinträchtigung ihrer Funktion verbunden. Durch die Möglichkeit der Entsendung von Exekutivvertretern definiert das Urteil die Staatsgewalt von einer Bedrohung der Vielfalt zu einem Element ebendieser Vielfaltsgewährleistung um.“ Paulus kritisierte im Urteilstext ebendies Urteil als „einen utopischen, kaum überprüfbaren Maßstab für die Ausübung des erteilten Mandats“ für die ZDF-Gremien.

Die Länder haben bis 30. Juni 2015 Zeit, den ZDF-Staatsvertrag neu zu fassen. Kläger vor dem Bundesverfassungsgericht waren die Länder Rheinland-Pfalz und Hamburg. Themen der Verhandlung waren unter anderem die inoffiziellen, politischen „Freundeskreise“, in denen die Fernsehratsitzungen vorbereitet werden und deren Einfluss unbestritten ist, die Besetzung der Ausschüsse und R-Gruppen. Die Ministerpräsidentenkonferenz beschloss am 12. Juni 2014 erste Schritte zur Umsetzung des Urteils.

Der Intendant steht in der Hierarchie des Senders an der Spitze, vertritt das ZDF nach außen und führt die Geschäfte. Weiter ist er für das Programm verantwortlich und schlägt den Chefredakteur vor, der durch den Verwaltungsrat gewählt wird. Gewählt wird der Intendant durch den Fernsehrat.

Die bisherigen Amtsinhaber waren:

Dem Intendanten sind fünf Direktoren unterstellt, die die Direktionen der Fernsehanstalt leiten. Dies sind aktuell:

Media Control errechnete die höchste Zuschauerzahl der letzten 20 Jahre in Rheinland-Pfalz und Hessen, wohingegen sie in Mecklenburg-Vorpommern am geringsten ausfallen würde. Insgesamt läge das ZDF aber im Jahr 2013 „in der Gunst des Publikums vorne.“ Rekorde mit mehr als 20 Millionen Zuschauern wurden vor allem in den 1980er Jahren mit Sendungen wie "Das Traumschiff" oder der "Schwarzwaldklinik" erreicht, 1992 erreichte eine "Wetten, dass..?"-Ausgabe 20,47 Millionen Zuschauer. Der Marktanteil lag laut "media control" bei 65,8 Prozent. Aufgrund der Aufsplitterung des Fernsehmarktes findet man solche Quoten nur noch selten, zuletzt wurden solche Reichweiten nur bei Sportübertragungen wie der bisher höchsten Reichweite des ZDF 2006 im WM-Halbfinale Italien gegen Deutschland mit 29,66 Millionen Zuschauern ermittelt. Das Durchschnittsalter der ZDF-Zuschauer lag Mitte 2012 bei 61 Jahren. Durch die Olympischen Spiele 2012 und die Fußball-Europameisterschaft 2012 sank es auf 60 Jahre.

Weil die Zuschauer des Hauptprogrammes einen recht hohen Altersdurchschnitt aufweisen, wird das ZDF auch "Kukident-Sender" genannt. Diese Bezeichnung soll auf den früheren RTL-Geschäftsführer Helmut Thoma zurückgehen. Das ZDF reagierte auf die in den Medien „längst zum Allgemeinplatz“ gewordenen Vorwürfe, das „ZDF mit Häme als ‚Kukident-‘ oder ‚Heizdecken-Sender‘“ zu bezeichnen, was die älteren Zuschauer verletze, „die zu Recht danach fragen, was eigentlich so schlimm daran ist, wenn ein Sender für sie attraktiv ist.“ Das ZDF sähe solche Anwürfe mittlerweile souverän – vor allem deshalb, da die Verjüngung des Programms längst eingeleitet sei. Thomas Bellut erklärte bei seinem Amtsantritt: „[…] Die Zielgruppe des ZDF ist die ganze Gesellschaft. Deshalb müssen wir mit unserem Programm verstärkt jüngere Zuschauer erreichen. Die Erfolge der Digitalkanäle und Online-Angebote reichen dafür allein nicht aus.“ Allerdings würde es „keine krampfhaften Verjüngungsversuche geben, sondern eine konsequente Modernisierung, die alle Altersgruppen“ ansprechen würden.


Am 1. April 1963 begann der Sendebetrieb mit dem Slogan „Am Ersten das Zweite“, wobei das Programm im Wesentlichen aus der vorab aufgezeichneten 90-minütigen Unterhaltungssendung "Berlin-Melodie" mit Musik aus dem alten Berlin bestand. Im ersten Jahr spielten die für das FFG produzierten Sendungen noch eine erhebliche Rolle, außerdem konnte man mit Sportsendungen, speziell dem Aktuellen Sportstudio – überaus erfolgreich – glänzen. Mit dem Umzug nach Wiesbaden kam Die Drehscheibe, eine bekannte Vorabendsendung, die viele bisherige Ratgebersendungen zusammenfasste. Nun konnte auch die Sendezeit schrittweise ausgedehnt werden. Seit 1966 beteiligt sich das ZDF am gemeinsamen Vormittagsprogramm mit der ARD. Das Kinder- und Jugendprogramm ZDFtivi wird vormittags am Wochenende und an bundesweiten Feiertagen ausgestrahlt.

Wie auch Das Erste darf das ZDF an Werktagen entsprechend dem Rundfunkstaatsvertrag nur bis 20 Uhr Werbung zeigen, und zwar maximal 20 Minuten täglich und ausschließlich im Hauptprogramm. An Sonn- und bundesweiten Feiertagen sowie in den drei digitalen Spartenkanälen des ZDF darf keine Werbung gesendet werden. Seit dem Beginn der Werbung im ZDF werden die einzelnen Werbespots durch die Mainzelmännchen unterbrochen. Mit der Novellierung des Rundfunkstaatsvertrages wurde die bisherige Unterscheidung zwischen Werbung und Sponsoring weitgehend aufgehoben, weswegen es seit Januar 2013 im öffentlich rechtlichen Fernsehen nach 20 Uhr und an bundeseinheitlichen Feiertagen auch kein Sponsoring mehr gibt.

In den ersten Jahrzehnten seines Bestehens strahlte das ZDF sein Programm ausschließlich über die zweite terrestrische Senderkette in der Bundesrepublik Deutschland aus. Diese wird, anders als die Sender des Ersten Fernsehprogramms der ARD, von Media Broadcast, früher von der Deutschen Bundespost, betrieben. Wie bei der ARD waren auch hier vor 1990 viele Sender so platziert, dass sie auch in weiten Gebieten der DDR empfangbar waren, wo das ZDF viele Zuschauer hatte. Seit Dezember 1990 wird das Programm auch über diverse Sender in den neuen Bundesländern abgestrahlt. Genutzt werden hierbei Frequenzen, die einmal für ein eventuelles drittes Fernsehprogramm der DDR vorgesehen waren.

Seit den ersten Kabelpilotprojekten ist das ZDF auch im Kabelfernsehen vertreten. Die europaweite Satellitenausstrahlung über Astra 1C begann am 27. August 1993 zur Internationalen Funkausstellung in Berlin. Im selben Jahrzehnt begann die Ausstrahlung von Digital Video Broadcasting-Signalen über Kabel und Satelliten.

Seit 2002 ist das ZDF über DVB-T auch als Digitales Fernsehen in den ausgebauten Regionen zu empfangen. In der Anfangszeit wurde über DVB-T das Tonsignal im Format Dolby Digital ausgestrahlt. Aus Kapazitätsgründen ist dieser Dienst zwischenzeitlich eingestellt worden.

Im Internet wird über die seit 2001 bestehende ZDFmediathek via Live-Stream oder Video-on-Demand eine Auswahl an Sendungen angeboten. Neben der regulären Version existiert eine spezielle Ausgabe der Mediathek für mobile Endgeräte, seit Mitte 2011 wird auch eine App für diverse Betriebssysteme angeboten. Ein Live-Stream des kompletten Programms wird seit April 2008 über die Software Zattoo angeboten. Seit 12. Februar 2013 sendet das ZDF – sofern lizenzrechtlich möglich – seine Kanäle endgültig als rund um die Uhr Internet-Livestream.

Als erster Sender in Deutschland begann das ZDF, seine Magazin- und Nachrichtensendungen ausschließlich im zu senden. Bereits im Jahr 2006 hatte das ZDF die Fußball-Weltmeisterschaft in 16:9 gesendet und im Laufe des Jahres Zug um Zug Magazinsendungen wie Frontal21 oder das Morgenmagazin auf Breitbild umgestellt. Die letzte Umstellung vom bisherigen TV-Format 4:3 auf 16:9 erfolgte am 25. Juni 2007 mit den heute-Nachrichten.

Am 12. Februar 2010 ging der HD-Ableger "ZDF HD" in den Regelbetrieb über.

Seit 15. November 2012 wird das Programm auch landesweit in Südtirol (seit Juni 2013 auch in Trentino) verbreitet.

Das ZDF wird ab 1. Mai 2016 über die DVB-T2 HD-Plattform in Deutschland im Rahmen des Pilotprojektes von Media Broadcast sein Hauptprogramm in Full-HD-Auflösung senden, das Bild wird jedoch nur von einem 720p-Signal hochskaliert.

Das ZDF trägt, gemeinsam mit der ARD und allen 16 deutschen Ländern, die öffentlich-rechtliche Körperschaft Deutschlandradio (DLR) mit den drei nationalen Hörfunkprogrammen Deutschlandradio Kultur, Deutschlandfunk und Deutschlandfunk Nova.

Das ZDF beteiligt sich zudem, in Zusammenarbeit mit anderen Rundfunk- bzw. Fernsehanstalten, an folgenden Fernsehprogrammen:

In seinem Digitalpaket (DVB) ZDFvision sind zusätzlich frei zu empfangen:

Ehemalige Programme von ZDFvision:

Ehemalige Programme des ZDF:

Vorwurf des „embedded journalism“

Ulrich Tilgner ließ 2010 seinen Vertrag mit dem ZDF auslaufen. Grund war seine Unzufriedenheit mit den Arbeitsbedingungen (Eingriffe in die Pressefreiheit und Bündnisrücksichten) in Deutschland. Tilgner kritisierte unter anderem, dass es seiner Ansicht nach im ZDF mangelnde Unabhängigkeit und seit Gerhard Schröder einen Hang zum „eingebetteten Journalismus“ gebe. Tilgner findet viele Sendungen zu boulevardesk und zu regierungsfromm. Es sei ein geschlossener Kreislauf entstanden, „in dem Journalisten die Adressaten symbolischer Politik sind und die Wahrheit auf der Strecke bleibt“.

Manipulationen beim ZDF 2014 bei der Fernsehsendung "Deutschlands Beste!"

Wie sich erst im Nachhinein beim ZDF 2014 nach der Fernsehsendung "Deutschlands Beste!" herausstellte, beginnend mit Anfragen des Zapp-Magazins und des Medienjournalisten Stefan Niggemeier, wurde lediglich eine zweite Forsa-Umfrage zum Ranking der jeweils „besten“ 50 herangezogen; das ZDF-Online- und das HörZu-Voting wurden nicht berücksichtigt. Begründet wurde dies unter anderem mit der angeblichen Einflussnahme von Fangruppen auf das Onlinevoting. Darüber hinaus kam es zu willkürlichen Manipulationen durch die Redaktion, bei denen eingeladene Gäste wie Claus Kleber und Franz Beckenbauer auf vordere Positionen verschoben wurden, dafür hingegen der Moderator von "RTL aktuell", Peter Kloeppel, auf eine hintere Position gesetzt wurde.

Im Zusammenhang mit dem Voting-Skandal um die Sendung "Deutschlands Beste!" wurde auch bekannt, dass bereits 2007 ein Votingergebnis der Band Böhse Onkelz von Platz 1 auf Platz 25 heimlich und mit Wissen des damaligen Programmdirektors Thomas Bellut manipuliert wurde.

Daraufhin bot ZDF-Unterhaltungschef Oliver Fuchs seinen Rücktritt an. Außerdem wurde die für die beiden Shows 2014 zuständige Teamleiterin ihrer Führungsfunktion enthoben und eine weitere Redakteurin abgemahnt. In diesem Zusammenhang wurde zudem die Einstellung der Reihe "Deutschlands Beste!" verkündet. Das Rücktrittsgesuch von Fuchs wurde von ZDF-Intendant Thomas Bellut und Programmdirektor Norbert Himmler angenommen.

Vorwurf der Quotenhörigkeit und des Qualitätsverlusts

Wolfgang Herles kritisierte nach seiner Pensionierung in seiner Publikation „Die Gefallsüchtigen“ den „Quotenfetischismus“ des ZDF, dem nichts ferner sei als Kritik, Provokation und Aufklärung. Medien und Politiker folgten der „Macht des Marktes“, was zu einem platten homogenen Unterhaltungsprogramm und zur Niveausenkung führe. Die Aufgabe, vierte Gewalt zu sein, würden die Gebührensender „dramatisch verfehlen“. Herles plädiert daher für eine radikale Programmreform, die Abschaffung des Gebührenfernsehens und eine Finanzierung aus Steuermitteln.

Parteiendominanz und Spitzelsystem

Chefredakteur Nikolaus Brender kritisierte 2010 in einem Interview des Spiegel die parteipolitische Dominanz im öffentlich-rechtlichen Rundfunk und das „Proporzdenken“ der Parteien. Er sprach weiterhin von einem „Spitzelsystem, das davon lebt, dass Redakteure den Parteien Senderinterna zutragen“, und bezeichnete diese als „Inoffizielle Mitarbeiter“ der Parteien, die „wirklich vergleichbar mit den IM der DDR“ seien. Da sei ein „feingesponnenes Netz von Abhängigkeiten“. Er selbst habe versucht, „solche Spione wenigstens von Posten mit echter Verantwortung fernzuhalten“. Insbesondere gäbe es in der Union ein „dunkles Schattenreich, das sich im Verwaltungsrat eingenistet hat und ihn mittlerweile zu dominieren versucht“. Das Bundesverfassungsgericht sei „die einzige Institution, die dem öffentlich-rechtlichen Rundfunk Staatsferne, Form und damit Zukunft sichern“ könne. 

Die Parteidominanz wurde auch in den Jahren 2001 und 2002 bei der Suche eines Nachfolgers für Intendant Dieter Stolte deutlich. Erst nach fünf Wahlgängen war im März 2002 Markus Schächter zum neuen Intendanten gewählt worden.

Vorwurf der einseitigen Berichterstattung über die Staatsschuldenkrise in Griechenland

Sowohl dem ZDF als auch der ARD legt eine wissenschaftliche Studie der Otto-Brenner-Stiftung zur Last
im Rahmen ihrer Programme zur griechischen Staatsschuldenkrise einseitig und unausgewogen berichtet zu haben.
Persönliche Meinungen von Journalisten und objektive Tatsachen waren gegenüber den Zuschauern nicht eindeutig voneinander zu trennen, allgemeine Themen und die Reformbemühungen der griechischen Regierung wurden oberflächlich widergespiegelt, die griechische Regierung konnte ferner seltener zu Wort kommen als die deutsche, die Titel waren oft plakativ. Die Studie kam in einer Dauer von mehreren Monaten zu ihren Schlüssen. Wenige Stunden nach ihrer Veröffentlichung kritisierte die ARD die Studie vollständig.


Das ZDF betreibt mit unterschiedlichen Redaktionen mehrere Websites:




</doc>
<doc id="12005" url="https://de.wikipedia.org/wiki?curid=12005" title="El Greco">
El Greco

El Greco ( ‚der‘, ‚Grieche‘; * 1541 in Candia auf Kreta; † 7. April 1614 in Toledo); eigentlich Domínikos Theotokópoulos, () war ein Maler griechischer Herkunft und Hauptmeister des spanischen Manierismus und der ausklingenden Renaissance. Er war auch als Bildhauer und Architekt tätig. Seine künstlerische Arbeit begann auf Kreta mit der Ausbildung zum Ikonenmaler in der byzantinischen Tradition. Er siedelte nach Venedig über und kam mit der Kunst Tizians in Berührung, bevor er sich in Rom niederließ. Anschließend gelangte El Greco auf ungeklärte Weise nach Spanien und zog nach Toledo. Trotz einiger Konflikte konnte er sich dort durchsetzen und blieb bis zu seinem Lebensende.

El Greco malte hauptsächlich Bilder mit religiösen Themen und Porträts. Hinzu kommen einige wenige Landschaften und Genrebilder. In Venedig und Rom adaptierte er westliche Bildthemen und künstlerische Techniken. So wandte er sich der Ölmalerei und Leinwänden als Malgrund zu. Gegen Ende seines Italienaufenthaltes fand El Greco zu einer starken Körperlichkeit seiner Figuren, was sich in Spanien fortsetzte. Dort arbeitete er an großen Altarprojekten und fertigte Porträts einflussreicher Personen an. Für seine Altarbilder entwarf El Greco oft auch das architektonische Rahmenwerk. Seine Malerei entwickelte sich weg vom Naturalismus hin zu einem Individualstil, indem er versuchte, einen neuen Ausdruck für spirituelle Phänomene zu suchen, und sich in seinem Spätwerk zunehmend auch wieder auf seine Herkunft als Ikonenmaler bezog. El Greco bereicherte die katholische Bilderwelt um neue Themen und um eine Neuinterpretation bekannter Ikonographien. Seine Kunst wurde weniger vom Adel gefördert, sondern von Intellektuellen, Geistlichen und Humanisten unterstützt.

Die Rezeption El Grecos fiel über die Zeit sehr unterschiedlich aus. Mit seinem Individualstil ging er einen sehr eigenen Weg, der von der Entwicklung der Malerei in Spanien weitestgehend unabhängig war. Nach seinem Tod wurde seiner Kunst wenig Wertschätzung zuteil und sie wurde zum Teil gar nicht beachtet. Eine langsame Wiederentdeckung El Grecos setzte im 19. Jahrhundert ein, um 1900 hatte er dann seinen Durchbruch. Dieser war weniger von der Kunstwissenschaft getragen, sondern von Schriftstellern, der Kunstkritik und der künstlerischen Avantgarde. Er wurde von Künstlern der Moderne, besonders des Expressionismus, als ein wichtiger Bezugspunkt gesehen und in Werken rezipiert. Zudem wurde er von spanischen Künstlern und Intellektuellen zur Stärkung der nationalen Identität herangezogen.

Domenikos Theotokopoulos, genannt ‚El Greco‘, wurde 1541 in Candia geboren, der damaligen Hauptstadt der Insel Kreta, heute Heraklion. Sein Geburtsjahr ergibt sich aus einem Dokument aus dem Jahr 1606, in dem sich El Greco als 65 Jahre alt bezeichnete. Kreta gehörte zur Zeit seiner Geburt zur Republik Venedig, für die sein Vater Georgios Theotokopoulos als staatlicher Steuereintreiber tätig war. Der Vater war zudem Händler, wie auch der ältere Bruder des Künstlers, Manoussos, der außerdem Seefahrer war. Die Familie stammte ursprünglich nicht aus Candia, wahrscheinlich wanderte der Vater in den späten 1520er-Jahren aus der Region um Chania ein. Weder über die Mutter El Grecos noch über seine erste, griechische Ehefrau sind irgendwelche Informationen überliefert. Auch über die Kindheit El Grecos ist nichts bekannt. Die Familie gehörte zur Mittelklasse der Insel.

Da in Candia die orthodoxe und die lateinische Kirche nebeneinander bestanden, ist es bis heute nicht klar, welcher der beiden Konfessionen die Familie El Grecos angehörte. Auf Kreta gab es eine Schule für Ikonenmalerei, die orthodoxe Tradition mit westlichen, über Druckgraphik aus Venedig auf die Insel gelangten Einflüssen verband. Die kretischen Werkstätten waren sowohl im östlichen Mittelmeerraum als auch in Venedig beliebt. In einer dieser Werkstätten erhielt El Greco seine künstlerische Ausbildung in der Tradition der kretischen Schule. Im Jahre 1563 wurde er in einem Dokument als Meister der Ikonenmalerei bezeichnet. Er muss ein anerkannter Künstler gewesen sein, denn 1565 schätzte der kretische Ikonenmaler Georgios Klontzas eine "Passion Christi" von ihm auf den hohen Preis von 70 Dukaten. Der zweite Gutachter, ein Priester, schlug sogar 80 Dukaten als Preis vor. Der Preis von 70 Dukaten lag in dem Bereich, der zu dieser Zeit für Bilder venezianischer Meister wie Tintoretto bezahlt wurde. Mit dieser preislichen Bewertung geht die Annahme einher, dass El Greco bereits vor seiner Abreise nach Venedig der am stärksten geschätzte Maler Kretas war, denn durchschnittlich erzielten die Künstler in Candia deutlich geringere Preise als Venezianer.

Das früheste heute noch bekannte und von El Greco mit seinem bürgerlichen Namen signierte Werk ist ein Motiv der Entschlafung Mariens. Das 1567 gemalte Bild hängt seit etwa 1850 in der gleichnamigen Kirche von Ermoupoli auf der Insel Syros.

1568 war El Greco in Venedig anwesend, was durch einen Brief vom 18. August 1568 belegt ist. Darin teilt er mit, dass er Zeichnungen an den griechischen Kartographen Giorgio Sideris, genannt Calapodas, geschickt habe. Sideris gehörte zu jenen Intellektuellen, die den langsamen Aufstieg El Grecos unterstützt hatten. Es ist möglich, dass der Kartograph sogar den Anstoß für die Übersiedlung nach Venedig gegeben hat. In der Forschung wird angenommen, dass El Greco bereits im Frühjahr oder Sommer des Jahres 1567 nach Venedig aufgebrochen war.

Er hielt sich in Venedig drei Jahre lang auf und malte dort zahlreiche Bilder. Sie verbindet vor allem, dass El Greco sich den einheimischen Künstlern wie Jacopo Bassano, Jacopo Tintoretto und Tizian annäherte. An die Stelle des Goldgrundes setzte El Greco nun einen perspektivischen Raum, wobei er etwa auf Architekturtraktate wie das von Sebastiano Serlio zurückgriff. Zudem gab er die Temperamalerei auf, wandte sich der im Westen seit Jan van Eyck verbreiteten Ölmalerei zu und begann, Leinwände als Bildträger zu verwenden. Dennoch legte er bis zu seinem Lebensende viele seiner Gemälde noch mit Temperafarben an, vollendete sie dann jedoch mit Ölfarben. Für die Lichtgestaltung und Farbwahl El Grecos war der Aufenthalt in Venedig prägend.

Im Jahre 1570 wies der Miniaturmaler Giulio Clovio seinen Mäzen Alessandro Farnese in Rom auf ein heute verlorenes Selbstporträt El Grecos hin, das die römischen Künstler erstaunt hätte, und empfahl, den Künstler in der Villa Farnese aufzunehmen. Er legte El Greco seinem Mäzen als Schüler Tizians ans Herz. Dieser malte daraufhin ein Porträt Clovios, das vielleicht als Gegenleistung für die Empfehlung gedacht war. Im Palazzo Farnese lernte er etwa den bedeutenden Humanisten und Bibliothekar Fulvio Orsini kennen, in dessen Sammlung sich später sieben Werke El Grecos befanden. Möglicherweise lernte er über Orsinis Freund Pedro Chacón zudem den kirchlichen Würdenträger Luis de Castilla aus Spanien kennen, mit dem El Greco in der Folge eine enge Freundschaft verband.

Im Haus der Farnese war El Greco wenig beansprucht, da dort vor allem Freskomaler gebraucht wurden. Zwar wurde die Mitarbeit eines griechischen Malers an den Fresken überliefert, es lässt sich ihm aber kein Werk zuordnen. El Greco suchte sich mit seinem verlorenen Selbstporträt, dem Porträt Clovios und weiteren Werken seine eigene Marktnische als Bildnismaler. Mit innovativen Bildnissen und anderen Bildexperimenten wie dem Genrebild eines eine Kerze entzündenden Jungen machte er sich einen Namen in den Kreisen römischer Gelehrter und Intellektueller. Auch suchte er in anderen Gattungen nach Anerkennung, musste sich in Rom jedoch der Konkurrenz vieler hochrangiger Maler stellen, die in der Tradition Michelangelos wirkten. Um sich abzusetzen und seine Fremdheit als Stärke zur Geltung zu bringen, berief sich El Greco auf Tizian. In diesem Kontext steht auch die von Mancini überlieferte Anekdote, nach der El Greco dem Papst angeboten habe, das kritisierte "Jüngste Gericht" von Michelangelo in der Sixtinischen Kapelle zu übermalen. Daraufhin habe er aufgrund der Kritik der römischen Maler die Stadt verlassen müssen.

El Greco wurde aus dem Haus Farnese entlassen und beschloss, eigene Wege in Rom zu gehen. Am 18. September 1572 entrichtete er die zwei Scudi Aufnahmegebühr und trat somit der römischen Lukasgilde unter dem Namen Dominico Greco bei. Er eröffnete in der Folge eine eigene Werkstatt in Rom, wobei er zuerst von dem Sieneser Maler Lattanzio Bonastri da Lucignano unterstützt wurde. Etwas später trat Francesco Prevoste, der El Greco später nach Spanien begleitete, der Werkstatt bei. Über die Zeit von September 1572 bis zum Oktober 1576 liegen keine Dokumente vor, die Hinweise geben könnten, was El Greco in dieser Zeitspanne tat. Auch weshalb er Italien verließ, ist nicht bekannt.

Für den Oktober 1576 ist die Anwesenheit El Grecos in Spanien nachgewiesen – wie er dorthin gelangte, ist nicht bekannt. Zwischen Rom und Spanien bestanden damals enge Kontakte. In Rom hielten sich viele Spanier auf, und zahlreiche italienische Künstler zog es auf die Iberische Halbinsel. Während seines Aufenthaltes bei den Farneses konnte El Greco Kontakte zu Spaniern wie zum Beispiel Luis de Castilla knüpfen. Über de Castilla erhielt El Greco mehrere Aufträge in Toledo, vor allem in der Anfangszeit seines Spanienaufenthaltes. Bevor er nach Toledo kam, hielt er sich wahrscheinlich kurzzeitig in Madrid auf, wo er sich eine Anstellung am Hof erhoffte. Genauere Informationen zu dieser Station existieren aber nicht. Auf Vermittlung von Diego de Castilla, dem Vater seines Freundes und Dekan der Kathedrale, schuf er einen Altar für die Zisterzienserinnenabtei Santo Domingo de Silos in Toledo. Er gestaltete nicht nur das Bildprogramm, das mit der leiblichen Aufnahme Mariens in den Himmel als zentralem Bild zur Begräbniskapelle passte, sondern entwarf auch die Architektur des Retabels, seines plastischen Schmucks und des Tabernakels.

Ebenfalls auf Vermittlung Diego de Castillas hin, der aber in diesem Fall nicht allein verantwortlich war, malte El Greco "Christus wird seiner Kleider beraubt" für die Kathedrale von Toledo. Dabei kam es zum Konflikt um den Preis und die Gestaltung des Gemäldes, wie es ihn in der Folge auch bei weiteren Gemälden gab. Der Gemäldepreis wurde in Spanien zu dieser Zeit nach Vollendung des Gemäldes durch vom Künstler und vom Auftraggeber beauftragte Gutachter festgesetzt. El Grecos Vertreter schlug den hohen Preis von 900 Dukaten vor, während die Vertreter der Kathedrale nur 227 Dukaten zahlen wollten. Die große Abweichung wurde damit erklärt, dass es Kritik am Bild gegeben habe. Üblicherweise hätte El Greco die Kritik nacharbeiten müssen, er weigerte sich jedoch, weil er sich als Schöpfer seiner Werke und nicht als bloßes ausführendes Organ seiner Auftraggeber sah. Der Konflikt ergab sich somit aus der unterschiedlichen sozialen Stellung des Malers in Italien und Spanien. Im September 1579 gab es in diesem Streitfall eine erste Einigung auf 317 Dukaten, die jedoch nicht lange hielt. 1585 gab es einen weiteren Kompromiss, nach dem El Greco auch den Rahmen gestalten sollte. Zwei Jahre später wurde nun der neue Preis von 535 Dukaten für Bild und Rahmen ausgehandelt. Dass der Rahmen in diesem Zusammenhang höher bewertet wurde als das Gemälde, lag in dem gegenüber der Malerei höheren Status der Skulptur im Spanien dieser Zeit. An der Ikonographie des Bildes veränderte El Greco im Laufe dieser Zeit nichts, obwohl das Bild weiterhin in der Hauptkirche des Bistums hing.

Zwischen 1577 und 1579 malte El Greco die "Anbetung des Namen Jesu", mit der er sich bei König Philipp II. empfehlen wollte. In diesem Bild brachte er den König direkt als Figur ein. Im Jahr 1578 bekam er mit seiner Lebensgefährtin Jerónima de las Cuevas, über die kaum Informationen überliefert sind, einen Sohn, der nach seinem Vater und Bruder Jorge Manuel genannt wurde. Die neuere Forschung zu El Greco geht davon aus, dass Jerónima de las Cuevas wahrscheinlich eher aus einer Handwerker-Familie und nicht, wie oft angenommen, aus dem Adel stammte. Die Beziehung dauerte nicht lange, da Jerónima de la Cuevas jung starb.

In den Jahren 1580 bis 1582 malte El Greco "Das Martyrium des heiligen Mauritius" als Probebild für die Kirche des Escorial, um nach seinen Erfolgen in Toledo auch in Madrid bei Hofe Fuß zu fassen. In dieser Situation vollzog El Greco einen Stilwechsel vom Naturalismus hin zu einer Malerei, in der er nach gestalterischen Ausdrucksmöglichkeiten für Spiritualismus suchte. Der König verfolgte mit dem Bau des Escorial die Absicht, die Ideen des von ihm mitgeprägten Konzils von Trient umzusetzen. Zu diesem Zweck wollte er eigentlich Juan Fernández de Navarrete mit der Gestaltung sämtlicher Altäre betrauen. Navarrete starb jedoch, so dass neue Maler gesucht werden mussten. Vielleicht aufgrund des ersten Bildes, mit dem sich El Greco am Hof empfehlen wollte, der "Anbetung des Namen Jesu", fasste der König den Griechen als möglichen Ersatz ins Auge. El Greco lieferte zwar ein kunstvolles Bild, doch es widersprach in seiner Wendung gegen den Naturalismus den Idealen des Konzils. Dieses Werk wurde zwar gut bezahlt und es kam nicht zu Korrekturen, dennoch erhielt El Greco keine weiteren königlichen Aufträge, da Philipp II. das Bild als für den Bestimmungsort ungeeignet empfand. Statt an dem geplanten Ausstellungsort auf dem Altar der Escorialkirche wurde das Bild an einem weniger prominenten Ort in der Kirche aufgehängt. Philipp II. erteilte Romulo Cincinato den Auftrag, ein Bild zum gleichen Thema anzufertigen. Dieser orientierte sich an der Komposition El Grecos, veränderte jedoch deren Schwerpunktsetzung. Für sein Werk erhielt Cincinato 500 Dukaten, während El Greco 800 erhalten hatte. Das Verhalten des Königs zeigte die aufkommende Unterscheidung zwischen Altar- und Sammlerbild. Insgesamt steht dieses Vorgehen den Anekdoten und Berichten über den starken Einfluss der Inquisition auf die Kunstproduktion in Spanien entgegen. Gerade mit Unterstützung aufgeschlossener Kirchenkreise konnte der Grieche El Greco in Spanien barocke Bildideen entwickeln, die sich andernorts erst im 17. Jahrhundert durchsetzen konnten.

El Greco hatte zweimal Kontakt mit der Inquisition. Im ersten Fall arbeitete er an neun Terminen zwischen Mai und Dezember 1582 als Übersetzer bei einem Verfahren gegen einen griechischen Diener, der wegen Häresie angeklagt, jedoch freigesprochen wurde. Der zweite Kontakt hatte direkt mit El Greco und seiner Kunst zu tun. Nach dem Fehlschlag am Hof suchte der Maler unter der Geistlichkeit von Toledo neue Mäzene. Sein Probebild war das "Porträt eines Kardinales", das Fernando Niño de Guevara zeigte, der um 1600 Großinquisitor in Toledo war.

Die Zurückweisung in Madrid verstärkte El Grecos Bindung an Toledo. Am 10. September 1585 mietete er sich im Palast des Marques de Villena im ehemaligen jüdischen Viertel ein, wo er drei Wohneinheiten belegte und nach dem frühen Tod seiner Lebensgefährtin allein mit seinem Sohn wohnte. Dort lebte er bis 1590 und dann wieder ab 1604. Im Jahre 1589 wurde El Greco in einem Dokument als Bürger der Stadt bezeichnet. Am 18. März 1586 erteilte der Priester seiner eigenen Pfarrei den Auftrag für das Gemälde "Das Begräbnis des Grafen von Orgaz". Zwischen 1596 und 1600 malte El Greco das Retabel für das Augustinerkolleg der Doña María de Aragón in Madrid. Für dieses Werk erhielt er mit 6000 Dukaten den höchsten Preis, den er je für ein Gemälde erzielen konnte.

Am 9. November 1597 erhielt El Greco den Großauftrag, die Capilla de San José in Toledo auszugestalten, seinen bedeutendsten Auftrag in Toledo nach Santo Domingo el Antiguo. Der Vertrag umfasste die beiden Altargemälde sowie die Gestaltung und Vergoldung des Rahmens. Sein Sohn, der für ihn in diesem Jahr zu arbeiten begann, tauchte als Name in einem Dokument auf, in dem er sich verpflichtete, im Falle des Todes seines Vaters ein Werk zu vollenden. Ab 1603 findet sich der Sohn häufiger in Dokumenten zum Werkstattbetrieb.

Trotz zahlreicher gut dotierter Aufträge befand sich El Greco oft in ökonomischen Schwierigkeiten, da er einen sehr gehobenen Lebensstil pflegte. So beschäftigte er zeitweise Musikanten, die ihn während der Mahlzeiten unterhielten. Zwischen 1603 und 1607 gab es Konflikte um das Bildprogramm für das Hospital de la Caridad in Illescas. Der Vertrag enthielt für El Greco ungünstige Konditionen, so dass ihm kaum seine Kosten erstattet wurden und ein Prozess notwendig wurde. Kritik rief unter anderem hervor, dass unter dem Schutzmantel der Madonna reiche Bürger anstatt arme gezeigt wurden. Nach dem Tod El Grecos wurden aus diesem Grund die Halskrausen übermalt. In dieser Zeit bildete er Luis Tristán aus, der nach El Grecos Tod zum bedeutendsten Maler von Toledo wurde und zwischen 1603 und 1606 als Schüler in El Grecos Atelier nachweisbar ist. 1607 übernahm El Grecos Sohn an Stelle des verstorbenen Prevoste eine leitende Position im Atelier. Vater und Sohn erhielten von der Erzdiözese Toledo Aufträge, die Ausstattung von Kirchen auf die Orthodoxie ihrer Bildprogramme zu untersuchen. Im Anschluss konnten sie sich mehrmals lukrative Aufträge sichern.

Im folgenden Jahr übernahm El Greco von Pedro Salazar de Mendoza den Auftrag für drei Altarbilder für das Hospital de Tavera. Dieses Werk blieb jedoch unvollendet. 1611 besuchte Francisco Pacheco El Greco in Toledo. Er fertigte sowohl ein Porträt des Malers als auch eine Biographie an, die in seinem Buch über berühmte Maler erschien. Beide Zeugnisse sind heute verschollen. In seinem 1649 erschienenen Buch "El arte de la pintura" veröffentlichte Pacheco Informationen über die Arbeitsweise und künstlerischen Ideen El Grecos. Von ihm wurde überliefert, dass El Greco auch als Theoretiker arbeitete. Am 7. April 1614 starb El Greco. Zwei Griechen waren als Zeugen am Totenbett anwesend. Luis de Castilla regelte in der Folge seinen Nachlass. Zum Zeitpunkt seines Todes war El Greco hoch verschuldet. Er hinterließ kein Testament, was zur damaligen Zeit ungewöhnlich war.

El Greco wurde zunächst im Kloster des hl. Dominikus von Silos bestattet, wo er 1612 die Einrichtung einer Begräbniskapelle mit Altar und dem Altarbild der Anbetung der Hirten vereinbart hatte. 1618 starb Luis de Castilla, der Patron des Klosters, und in der Folge kam es mit den Nonnen zum Streit über den Preis. Deshalb ließ der Sohn El Grecos dessen Leichnam 1619 nach San Torcuato umbetten. Diese Kirche wurde später abgerissen, wobei die sterblichen Überreste El Grecos verlorengingen.

Jorge Manuel Greco erstellte ein Inventar des Besitzes seines Vaters, worunter sich 143 meist fertige Gemälde, unter anderem drei Laokoon-Versionen, 15 Gipsmodelle, 30 Tonmodelle, 150 Zeichnungen, 30 Pläne, 200 Druckgrafiken und über 100 Bücher befanden. Die Familie des Sohnes blieb in seinem Haus wohnen. 1621 wurde anlässlich der zweiten Hochzeit des Sohnes noch einmal ein Inventar des Besitzes von El Greco angefertigt.

El Greco malte viele religiöse Bilder und Porträts. Hinzu kamen einige wenige Genrebilder und Landschaften. Von seinen Zeichnungen haben sich nur wenige Exemplare erhalten. Sein Werk lässt sich in drei geographisch definierte Phasen unterteilen. Seine Anfänge auf Kreta waren in der Forschung lange umstritten. Heute ist es kunsthistorischer Konsens, dass El Greco dort seine künstlerische Laufbahn als Ikonenmaler begann. Die zweite Phase ist seine Zeit in Italien, wo er westliche Techniken und Kompositionen adaptierte. Er arbeitete in Venedig und Rom, bevor er nach Spanien übersiedelte. Dort fand er zu seinem eigenständigen Stil und schuf seine Hauptwerke.

El Greco war ein technisch versierter Künstler, der hochwertige Materialien verwendete. Deshalb befinden sich seine Werke in der Regel in einem guten Erhaltungszustand. Er behielt von jedem Bild eine kleinformatige Ölreproduktion in seiner Werkstatt und griff Motive zu verschiedenen Zeiten erneut auf. Sein Beitrag zur künstlerischen Reform der katholischen Bilderwelt lag vor allem in der Formulierung neuer Bildthemen und Ikonographie und in der Abwandlung bereits bekannter Motive. Zudem experimentierte er mit einer neuen Bildsprache. Für sie besann er sich im hohen Alter erneut auf seine Wurzeln in der östlichen Ikonenmalerei und verknüpfte diese mit seinen westlichen Erfahrungen zu einem erfolgreichen Individualstil. El Greco maß der Zeichnung im Arbeitsprozess wohl eine wichtige Bedeutung zu. So ist es nicht verwunderlich, dass sich 1614 im Inventar seines Nachlasses 150 Zeichnungen befanden. Jedoch haben sich nur sehr wenige Zeichnungen El Grecos erhalten, da diesem Medium auf der Iberischen Halbinsel keine Bedeutung zugemessen und somit keine große Aufmerksamkeit geschenkt worden war.

Das früheste bekannte Gemälde El Grecos ist ein "Marientod", den er um 1567 gemalt hat und der heute in der Kirche der Entschlafung Mariens in Ermoupoli auf der Insel Syros zu sehen ist. Er signierte das Gemälde mit "Domenikos Theotokopoulos". Der "Marientod" lässt in seiner Konzeption erkennen, dass El Greco als Ikonenmaler ausgebildet worden war, jedoch löste er sich bereits von den typisierten Vorbildern, den zweidimensionalen und gleichen Formen folgenden Figuren samt Kleidung und dem aus dem Inneren der Form kommenden Licht. Die vom Heiligen Geist ausstrahlende Lichtaureole, in deren Zentrum sich eine Taube befindet, verbindet die schlafende mit der thronenden Madonna. Zudem neigt sich Christus in einer zärtlichen Geste. Ein weiteres auffälliges Detail sind die drei Kandelaber, die sich im Vordergrund befinden. Der mittlere weist an seiner Basis Karyatiden auf, die auf eine druckgraphische Vorlage verweist. Diese Bildelemente waren eigene künstlerische Beiträge des Malers, die über den bestehenden Bildtypus hinausgingen. Dass er das Bild signierte, war zudem ungewöhnlich, da Ikonen in der Regel nicht signiert wurden. Damit unterstrich er seinen humanistischen Anspruch und seine weiter gehenden künstlerischen Ambitionen. Ein weiteres Bild aus seiner kretischen Phase ist "Der Heilige Lukas malt eine Ikone der Jungfrau mit dem Kind", das zwar stark beschädigt ist, aber immer noch Teile seiner Signatur trägt. Das zentrale Motiv des Evangelisten Lukas und der Maria in der Form einer Hodegetria malte El Greco in traditioneller byzantinischer Weise, während er in den Randmotiven neue Motive einführte wie etwa Malerwerkzeug, Renaissancestuhl und malerisch umgesetzte Engel. Die bekannten Werke, die El Greco auf Kreta schuf, weisen alle eine hohe künstlerische Qualität auf mit ihrer Lichtführung und dem starken Ausdruck. Zudem haben sie alle freihändige Vorzeichnungen.

Die erste Werkphase war lange Zeit umstritten, da El Greco als Ikonenmaler nicht in den westlichen Kunstkanon passte. Harold E. Wethey ging etwa davon aus, dass sich El Greco erst in Venedig zum Künstler entwickelt hat. Zudem gab es auf Kreta zwei weitere Maler namens Domenikos. Erst als der "Marientod" gefunden wurde, der auch den Nachnamen Theotokopoulos trug, gab es ein eindeutiges Referenzwerk, das stilistische Vergleiche zweifelsfrei zuließ. In der aktuellen Forschung ist der Beginn der künstlerischen Laufbahn auf Kreta allgemein anerkannt.

In Venedig wandte El Greco sich der Ölmalerei zu und verwendete Leinwände als Bildträger. Wie dort üblich nutzte er grobe Leinwände, die mit ihrer plastischen Textur expressive Wirkungen unterstützten. Zuerst trug er eine dünne weiße Grundierung auf, über die er nochmals eine zweite Grundierung auftrug, die rosa bis dunkelrot gefärbt war. Dann trug er mit einem Pinsel und schwarzer Farbe die Konturen der Figuren als Vorzeichnung auf und setzte zudem mit Weiß Lichtpunkte und mit Schwarz und Karmin die dunkelsten Stellen über die ganze Bildfläche. Erst in einem weiteren Schritt wurde in einem komplexen Verfahren der eigentliche Farbauftrag vorgenommen. Die Formate blieben aber weiterhin eher klein, was auch der Auftragslage El Grecos geschuldet gewesen sein kann. Technisch blieb El Greco venezianisch geprägt. Am Übergang zwischen seiner byzantinischen und venezianischen Malweise steht der als "Modena-Triptychon" bekannte Tragealtar, dessen Auftraggeber wahrscheinlich aus einer kreto-venezianischen Familie stammte. Der Objekttypus mit den vergoldeten Rahmenteilen war im 16. Jahrhundert auf Kreta üblich, die Ikonographie ist jedoch deutlich westlich geprägt. Der Altar trägt die Signatur El Grecos und ist somit ein wichtiges Referenzwerk für die Beurteilung von Werken aus dieser Zeit.

El Greco malte im Laufe seines Lebens mehrmals dasselbe Thema zu verschiedenen Zeiten. An diesen Bildern ist seine künstlerische Entwicklung nachvollziehbar. So malte er die erste Version der "Blindenheilung" in Venedig noch auf Holz. In ihr bezog er sich auf Bilder Tintorettos, aus denen er die Aufteilung in zwei Figurengruppen, den Fernblick und den in venezianischen Bildern beliebten Hund im Vordergrund entlehnte. Die Posen der Figuren beziehen sich auf verschiedene Druckgraphiken, die El Greco als Vorlagen nutzte. Die zweite Version entstand wahrscheinlich bereits in Rom und wurde auf Leinwand gemalt. Im Hintergrund ergänzte El Greco Ruinen, die Figuren ähnelten mehr antiken Skulpturen und Michelangelos Akten. Der nur leicht mit einem Tuch bekleidete Mann ähnelt dem Herkules Farnese. Zwar blieb El Greco in seinem Schaffen zeit seines Lebens venezianischen Einflüssen treu; er nahm jedoch zum Ende seines Romaufenthaltes und zu Beginn seines Aufenthaltes in Spanien Bezüge zu Michelangelo auf. So malte er in den frühen 1570er-Jahren eine "Pietà" auf Holz, die sich auf Michelangelos um 1550 entstandene Skulpturengruppe "Pietà di Palestrina" in Florenz bezog. Im Gegensatz zum Vorbild stellte El Greco an die Spitze der Komposition Maria. Er verlieh dem Bild eine Dramatik, die sich bis dahin nicht in seinen Werken fand und schon stärker in Richtung Barock wies. Die Christusfigur hatte für El Grecos Werke eine ungewöhnliche Körperlichkeit. Eine weitere Version der "Pietà" malte er auf Leinwand. Sie wirkt noch monumentaler und die Gewänder stärker ausgearbeitet, auch wenn sie am rechten Arm noch Probleme mit den Proportionen erkennen lässt. Formal hat das Gemälde bereits Parallelen zu den frühen in Spanien entstandenen Werken. Dass es aber dort entstanden sein soll, wird jedoch in der Forschung abgelehnt.

Diese Entwicklung zur Körperlichkeit setzte El Greco in seinen ersten Aufträgen in Toledo fort. Dies ist am Altar für das Monasterio Santo Domingo el Antiguo in Toledo nachvollziehbar. Passend zur Aufstellung in der Begräbniskapelle ist das zentrale Bild eine "Himmelfahrt Marias", das von den ganzfigurigen Bildern "Johannes der Täufer" und "Johannes der Evangelist" eingefasst wird, sowie den Brustbildern "Heiliger Bernhardt" und "Heiliger Benedikt". Im Giebelfeld befindet sich ein Bild des Schweißtuchs der Veronika und im folgenden Stockwerk mit der "Heiligen Dreifaltigkeit" ein weiteres großformatiges Bild. Der Hauptaltar wird von einer "Anbetung der Hirten" und der "Auferstehung Christi" als kleineren seitlichen Retabeln gerahmt. Dieser Altar war ein deutlich größerer Auftrag als seine von Kreta oder aus Italien bekannten Gemälde. El Greco bereitete die Arbeiten gründlich mit Vorzeichnungen vor. Eine Vorzeichnung von Johannes dem Täufer und zwei von Johannes dem Evangelisten haben sich erhalten. In den ersten Entwürfen positionierte er die beiden in Nischen und der Evangelist war im Profil dargestellt und blickte auf die Himmelfahrt. In der zweiten Zeichnung positionierte der Künstler ihn bereits so, wie er auch gemalt wurde. Bei der endgültigen Ausführung verzichtete El Greco jedoch auf den in der Zeichnung als Symboltier beigefügten Adler. Unter dem Einfluss Michelangelos fand El Greco zu einem sehr naturalistischen Stil mit monumentalen Figuren. Zudem folgte seine Farbwahl der römischen Schule und verlieh etwa der "Himmelfahrt Marias" eine große Leuchtkraft, während er bei der "Heiligen Dreifaltigkeit" die kontraststarken kalten Farbtöne Grün, Gelb und Blau verwendete und zudem Weiß in einer dominierenden Rolle im Bildzentrum einsetzte. Das architektonische Rahmenwerk, das El Greco entwarf, weist klare klassizistische Formen auf.

Mit dem "Martyrium des Heiligen Mauritius" aus den Jahren 1580 bis 1582 vollzog El Greco den Wechsel vom Naturalismus hin zu einer Malerei, in der er nach einem gestalterischen Ausdruck für spirituelle Phänomene suchte. In den 1580er-Jahren wandte er sich immer mehr von den Regeln der Renaissance für Proportion und Perspektive ab. Statt lebende Modelle zu studieren, begann El Greco wie Tintoretto mit Tonmodellen zu arbeiten. Er ließ dem Licht eine deutlich stärkere symbolische Funktion zukommen, statt es bloß in natürlicher Weise zu verwenden. So entstanden starke Hell-Dunkel-Kontraste. Die verwendeten Farben wurden deutlich expressiver. Statt wie üblich den Fokus der Darstellung auf das Martyrium zu legen, zeigte El Greco vor allem das von rhetorischen Gesten begleitete Gespräch in Anlehnung an eine Sacra Conversazione. Der Stilwechsel wurde von El Greco auch in anderen Werken dieser Zeit vollzogen.

Eines seiner bekanntesten Gemälde schuf El Greco mit dem "Begräbnis des Grafen von Orgaz", das er 1586 bis 1588 malte und das später zu einem Hauptwerk zum Studium des Malers wurde. Das Bild ist in zwei Zonen aufgeteilt. Im unteren Teil stellte El Greco die Begräbnisfeier dar, die einem Begräbnis wie zu dieser Zeit in Toledo üblich nachempfunden worden war. Der Adelige wird von den Heiligen Stephanus und Augustinus in das Grab gelegt, womit sich der Künstler auf die Legende zum Begräbnis bezog. Rechts liest wahrscheinlich der Auftraggeber das Requiem. Die obere Zone zeigt den Himmel, in den die Seele des Verstorbenen als Kind von einem Engel eingeführt wird, die dem Weltenrichter sowie Johannes und Maria als seine Fürsprecher und weiteren Heiligen gegenübertritt. In diesem Bild verwendete El Greco Licht nur noch als symbolisches Element. Im Himmel malte er ein unruhig erscheinendes Streiflicht. Die untere Hälfte ist dagegen gut ausgeleuchtet wie eine Bühne, die dortigen Fackeln haben keine reale Lichtwirkung. Das Gemälde nimmt zum einen auf eine historische Begebenheit, die religiös verklärt wurde, Bezug, ist zum anderen aber auch ein Gruppenporträt.

Ein Beispiel für ein von El Greco entwickeltes neues ikonographisches Thema ist die reuige Heilige in Halbfigur, das bereits in den folgenden Barock verweist. Eine einzelne Heiligenfigur wurde isoliert und monumental dargestellt und bot dem Betrachter die Möglichkeit, die Figur als gefühlsmäßigen Ansprechpartner zu sehen. Diese Bilderfindung kann als revolutionär eingeschätzt werden. Beispiele sind etwa "Die büßende Magdalena", die im Gegensatz zu dem gleichnamigen Bild Tizians jedoch ohne erotische Bezüge auskommt, oder "Der reuige Heilige Petrus". Ebenso populär waren Bilder des Heiligen Franziskus. El Greco malte Franziskus nicht wie bis dahin üblich beim Empfang der Wundmale Christi, sondern bei der Reflexion mit einem Totenkopf. Von dieser Bildidee gibt es noch etwa 40 erhaltene Versionen. Pacheco lobte, dass El Greco die in den Chroniken überlieferte Gestalt des Ordensgründers besonders gut dargestellt habe. Die Vielzahl der Bilder dieses Themas lag in der Popularität des Franziskus in Spanien begründet. Zudem setzte El Greco, wie er es aus Italien kannte, auf die druckgraphische Verbreitung, um seine Komposition zu popularisieren. Er ließ das Franziskus-Gemälde von seinem Schüler Diego de Astor nachstechen.

Neben neuen Bildideen erneuerte El Greco die katholische Bilderwelt mit stilistischen Innovationen. Zum einen bezog er sich auf seine Wurzeln als kretischer Ikonenmaler wie beim "Das Begräbnis des Grafen von Orgaz", das sich etwa in der Komposition auf den frühen "Marientod" bezog. Auf der anderen Seite zeigt sich bei einer späten Version der "Tempelreinigung" von 1610 bis 1614 eine hohe Abstrahierung von der Naturbeobachtung. Die Bewegung und das Licht sind in solchem Maße gesteigert, dass sie teils als „expressionistisch“ charakterisiert wurden. Das Visionäre von El Grecos Kunst lässt sich auch in dem Gemälde "Die Öffnung des fünften Siegels", das die Vision des Evangelisten Johannes zum Thema hat und ein Fragment eines späten Altarprojekts war, entdecken. Im Gegensatz zu anderen Bildern, die diese Vision thematisieren, integrierte El Greco den Heiligen in das Bild und verschob somit die Bedeutung von der Darstellung des erschienenen Ereignisses hin zum Moment der Erscheinung selbst. In diesem Bild erreichte die Entmaterialisierung der Form bei El Greco ihren Höhepunkt. Erst 1908 wurde die Thematik des Bildes erkannt und hat sich in seiner Bestimmung durchgesetzt. Zuvor existierten zahlreiche Interpretationen. Der obere Teil des Gemäldes ist verloren und die Stellung im geplanten Gesamtensemble ist nicht zu rekonstruieren. Visionen, wie hier eine von El Greco gemalt wurde, sind ein häufiges Thema in der spanischen Barockmalerei. Deshalb ist dieses Gemälde kein isoliertes Werk, sondern steht im Kontext der spanischen Malereientwicklung und verwies auf sie voraus.

Zwischen 1610 und 1614 malte El Greco drei Versionen des "Laokoon", die sein Atelier nicht verließen und nach seinem Tod im Inventar verzeichnet wurden. Nur eine Version ist erhalten geblieben. Es handelt sich um das einzige mythologische Werk El Grecos und steht in einer reichen Bildtradition, die auf Vergils "Aeneis" und auf der 1506 in Rom entdeckten Plastik des Laokoon basierte. Das Bild konnte der Künstler vor seinem Tod nicht mehr fertigstellen, weshalb die Figuren am rechten Bildrand nicht vollständig ausgeführt wurden. Bei einer Restaurierung wurden 1955 die Pentimenti freigelegt, so dass nun ein dritter Kopf und ein fünftes Bein in der rechten Figurengruppe zu sehen sind. Diese Figuren wurden unterschiedlich interpretiert, unter anderem als Adam und Eva, womit El Greco eine Synthese von Mythos und Religion geschaffen hätte. An Stelle Trojas setzte der Maler seine Heimatstadt Toledo ins Bild.

El Greco war ein anerkannter Porträtmaler. Seit seinem Aufenthalt in Italien bis in seine letzten Lebensjahre fertigte er Bildnisse an, die ihm ein regelmäßiges Einkommen sicherten. Kurz nach seiner Übersiedlung nach Rom um 1570 malte er das Porträt von Giulio Clovio, das den anerkannten Miniaturmaler als Halbfigur mit dem Stundenbuch der Farnese in seiner Hand zeigt. Das Fenster am rechten Bildrand zeigt einen Ausblick auf eine Landschaft mit stürmischem Himmel. Das Querformat dieses Porträts ist ungewöhnlich für ein Porträt. Eines der herausragendsten Beispiele für El Grecos Bildnismalerei ist das "Ganzfigurenporträt des Malteserritters Vincenzo Anastagi", das 1571–1576 entstanden ist. Der Ritter ist mit samtener Pluderhose und Brustpanzer vor einem dunklen Vorhang dargestellt. Der Raum, in dem ein Helm auf dem Boden liegt, ist sehr kahl und durch das Licht modelliert. Ein weiteres Porträt aus dieser Zeit, das El Greco zugeschrieben wird, ist das "Bildnis von Charles de Guise, Kardinal von Lothringen" aus dem Jahr 1572. Der sitzende Kardinal hält mit seiner rechten Hand ein Buch offen, in dem das Entstehungsjahr und das Alter des Dargestellten angegeben sind. Der Papagei im Fenster soll die Ambition des Kardinals auf das Amt des Papstes aufzeigen. In Toledo malte El Greco um 1600 mit "Ein Kardinal (der Großinquisitor Fernando Niño de Guevara)" ein sehr ähnliches Gemälde. Der Porträtierte trägt eine Bügelbrille, die zu der Zeit sehr modern und noch umstritten war. Dieses Attribut weist den Kardinal als dem Neuen aufgeschlossen aus, ebenso wie seine Wahl, El Greco als Künstler zu engagieren.

In Toledo war El Greco ein bedeutender Porträtmaler, der künstlerisch herausragend arbeitete. Das "Bildnis eines Edelmannes mit der Hand auf der Brust" aus den Jahren 1583 bis 1585 hat eine in der venezianischen Tradition stehende sehr reiche Farbigkeit des Hintergrundes und der Kleidung. El Greco nutzte für das Bild im Gegensatz zu den Madrider Hofmalern eine offene Malweise in der Tradition von Tizian, bei der im vollendeten Bild der Pinselstrich noch immer erkennbar ist. Die Haltung des Dargestellten mit seiner Schwurgeste ist streng. El Greco verzichtete abgesehen vom goldenen Knauf des Degens gänzlich auf Symbolik. Er porträtierte wichtige Persönlichkeiten Toledos wie den Mönch Hortensio Félix Paravicino y Arteaga, Antonio de Covarrubias und Jerónimo de Cevallos. In seinem Spätwerk findet sich zudem das "Porträt des Kardinal Tavera", der unter Karl V. Großinquisitor und Regierungschef von Kastilien und zum Zeitpunkt des Malens bereits über ein halbes Jahrhundert tot war.

Ein kleiner Teil des Werks El Grecos lässt sich der Genremalerei zuordnen. Mit dem Bild "Knabe, der eine Kerze entzündet", verwirklichte er eine originelle Bildidee, die er in verschiedenen Versionen und Kopien ausführte. Das in den frühen 1570er Jahren entstandene Gemälde, von dem sich eine Version in der Sammlung Farnese erhalten hat, war wahrscheinliche eine Ekphrasis nach einem Vorbild aus der Antike. Es gab zudem einige Vorläufer in der venezianischen Malerei, wo ein solches Motiv aber in einen größeren erzählerischen Kontext eingebunden worden war. El Greco isolierte eine einzelne Figur, die durch die Lichtregie und die Perspektive von unten in eine besondere Nähe zum Betrachter gesetzt wurde. Dieses Bild sicherte ihm in Rom eine eigene kleine Nische.

In seinem Spätwerk fertigte El Greco einige wenige Landschaftsgemälde an und ließ Elemente aus ihnen in andere Werke einfließen. So malte er in den Jahren 1597 bis 1599 die "Ansicht von Toledo", in der er zum einen auf die bedeutende Geschichte der Stadt und die zu dieser Zeit erfolgten städtebaulichen Neuerungen Bezug nahm. Er schuf eine eigenwillige Sicht auf die Stadt, die sich stark von anderen Darstellungen unterschied und sich nicht um historische Treue bemühte. El Greco malte eine Sicht auf den östlichen Teil der Stadt mit dem Palast, der Alcántara-Brücke, der Burg von San Servando und dem nach rechts versetzten Glockenturm der Kathedrale. Damit steigerte er den Anstieg des Stadtberges in dramatischer Weise. Er ließ zudem die Stadtmauer weg und veränderte im Bild den Flusslauf im Vordergrund. Das höchstgelegene Gebäude auf der rechten Seite ist der Alcázar, das Gebäude unter ihm mit dem Arkadengeschoss als Abschluss entspricht keinem realen Gebäude in Toledo. Es wurde als symbolischer Verweis auf die vielen Stadtpaläste reicher Bürger gedeutet. In einem weiteren Gemälde "Ansicht und Plan von Toledo", das zwischen 1610 und 1614 entstand, verlieh der Maler der Stadt eine innere Leuchtkraft, die sie von ihrer realen Existenz in die Richtung des Himmlischen Jerusalem entrückte. Auch in dem Altargemälde "Der Heilige Joseph mit dem Christuskind", das zwischen 1597 und 1599 gemalt wurde, und in weiteren Heiligenbildern nahm El Greco in der Landschaft Bezug auf Toledo. Auch im Hintergrund seines "Laokoon" ist eine Ansicht von Toledo zu sehen.

El Greco entwarf für viele seiner Altargemälde zudem das architektonische Rahmenwerk und den Skulpturenschmuck. Damit verschaffte er sich zusätzliche Einnahmen, vor allem den Umstand nutzend, dass damals die Skulptur in Spanien höher geschätzt und besser entlohnt wurde als die Malerei. Jedoch führte er diese Skulpturen meist nicht persönlich aus, sondern beauftragte andere Bildhauer. Dennoch stellte er wohl vor allem kleinere Skulpturen in verschiedenen Techniken her, die er meist als Modelle nutzte, wie er es bei Jacopo Tintoretto kennengelernt hatte. Diese Figuren aus Gips, Wachs oder Ton waren jedoch nicht sehr haltbar und gingen im Laufe der Zeit verloren. Daneben gab es eine Holzfigur, die sich im Besitz des Sohnes befand und zu Andachtszwecken genutzt wurde.

Es sind nur wenige Skulpturen El Grecos erhalten geblieben, zu denen zudem nur wenige Erkenntnisse vorliegen. Sie bezeugen vielfältige Einflüsse und sind damit für das Werk des Künstlers charakteristisch. Zu den heute noch erhaltenen Skulpturen zählen "Epimetheus und Pandora", die zwischen 1600 und 1610 geschaffen wurden. Als Aktfiguren sind sie für die spanische Kunst der Renaissance ungewöhnlich. Zudem war das mythologische Thema nicht üblich, entsprach jedoch der humanistischen Bildung des Künstlers und seines Umfeldes in Toledo. Zudem wurde der Mythos von Epimetheus und Pandora zu dieser Zeit als heidnische Version von Adam und Eva interpretiert. Technisch führte El Greco sie in spanischer Tradition in polychromatischem Holz aus. Von der Gestaltung her ähneln die Figuren dem Manierismus von Alonso Berruguete, jedoch ist die Darstellung der Körper zugleich eine individuelle Gestaltungsweise El Grecos, die sich auch in seiner Malerei findet. Eine weitere erhalten gebliebene Skulptur ist ein "Auferstandener Christus", den El Greco um 1595/1598 schuf. Sie war Teil des Tabernakels des Hauptaltars im Hospital de San Juan Bautista in Toledo. Die Haltung ähnelt gemalten Christusfiguren El Grecos dieser Zeit. Der männliche Akt war für das Spanien des 16. Jahrhunderts ein ungewöhnliches Sujet, wie etwa Harold E. Wethey betonte. Somit ging El Greco auch in der Skulptur seinen eigenen künstlerischen Weg.

Neben seiner künstlerischen Tätigkeit setzte sich El Greco auch mit der Kunst- und Architekturtheorie auseinander. Seine Überlegungen sind aber nur in Fragmenten als Annotationen in Büchern aus seiner Bibliothek überliefert. Sie zählen zu den wertvollsten handschriftlichen Dokumenten El Grecos. Im 17. Jahrhundert kursierte in Spanien ein Traktat El Grecos mit seinen theoretischen Überlegungen, welches der Künstler dem König präsentiert hatte. Diese Schrift ist jedoch verlorengegangen.

Die heute bekannten Überlegungen finden sich als Anmerkungen in einer Ausgabe von Giorgio Vasaris "Viten" und Vitruvs "De architettura" aus der Bibliothek des Künstlers. Insgesamt umfassen die Äußerungen El Grecos 18.000 Wörter, 7.000 zu Vasari, 11.000 zu Vitruv. Im Hinblick auf seine Position in Spanien ist bemerkenswert, dass er die religiöse Funktion der Kunst in den bekannten Äußerungen nicht behandelte. Hingegen stellte El Greco die Autonomie des Künstlers in Bezug auf die Gestaltung des Bildes heraus. Er betonte die Erkenntnisabsicht der Malerei in Hinblick auf Philosophie und Naturalismus. In seinen Anmerkungen setzte er sich von der mathematisch-theoretischen Richtung ab, die stark auf ein Studium der Proportionen abzielte. Außerdem wandte sich El Greco gegen den Klassizismus, der in der Tradition Michelangelos in Spanien populär geworden war. Die "Viten" forderten den Maler zur Stellungnahme heraus. Er lobte Tizian, während er Michelangelos Farbbehandlung und Raffaels starke Antikenrezeption kritisierte. Zudem lehnte El Greco Vasaris Modell des Verlaufs der Kunstgeschichte ab, das die byzantinische Kunst, aus deren Tradition El Greco selbst stammte, als plump und der italienischen Kunst unterlegen betrachtete.

Die Rezeption El Grecos fiel im Laufe der Zeit sehr unterschiedlich aus. Er wurde nicht vom Adel gefördert, sondern stützte sich vor allem auf Intellektuelle, Geistliche, Humanisten und andere Künstler. Nach seinem Tod wurde seiner Kunst wenig Wertschätzung zuteil und sie wurde zum Teil gar nicht beachtet. Seine langsame Wiederentdeckung setzte im 19. Jahrhundert ein, um 1900 hatte El Greco dann seinen Durchbruch. Dieser war weniger von der Kunstwissenschaft getragen, sondern von Schriftstellern, der Kunstkritik und der künstlerischen Avantgarde.

El Greco war die herausragende Künstlerpersönlichkeit im Spanien von Philipp II. und Philipp III. Sowohl künstlerisch als auch mit seinem Auftreten, mit dem er sich als Künstler ins Zentrum seines Schaffens rückte, wirkte er revolutionär, was bei seinen Zeitgenossen zum einen Bewunderung, zum anderen aber auch Ablehnung hervorrief. Er suchte nach neuen Ausdrucksformen und reformierte die Ikonographie und die Bildthemen der religiösen Malerei. Zu seinem Lebensende hin wandte er sich in seiner Kunst wieder seinen Anfängen als kretischer Ikonenmaler zu und ließ sich damit insgesamt nur schwer in die spanische Kunst des beginnenden 17. Jahrhunderts einordnen. Selbst sein Sohn setzte seinen Individualstil nicht fort. Dennoch wirkte sein Werk als Vorbereitung des Barock.

Aufgrund seines Werkes und Auftretens war El Greco zu Lebzeiten bereits eine Berühmtheit, ihm wurde jedoch abseits seiner Porträts und seiner koloristischen und naturalistischen Werke wenig Wertschätzung zuteil. Zeitgenössische Zeugnisse stammten etwa von Alonso de Villegas, Francisco de Pisa und dem Italiener Giulio Mancini, dessen um 1615 entstandene Aufzeichnungen jedoch erst 1956 publiziert wurden. Auch Pacheco hatte El Greco in Toledo besucht. Er malte ein Porträt von ihm und schrieb eine Biographie, die jedoch beide verloren gingen. Der Mönch Hortensio Félix Paravicino y Arteaga lobte El Greco in seinem 1641 erschienenen Werk "Obras postumas, divinas y humanas". In ihm befand sich ein Sonett, in dem er das von ihm geschaffene Porträt seiner selbst pries. In vier weiteren Sonetten lobte er zudem allgemein die Kunst El Grecos.

Auf Basis der Zeugnisse direkter Zeitgenossen schrieb der Maler und Kunstschriftsteller Palomino El Greco zu, dass dieser mit der Weigerung, die Handwerkssteuer zu zahlen, dazu beigetragen habe, dass die Malerei in Spanien zu den „freien Künsten“ aufstieg. Zwar ging es dem Künstler dabei vor allem um seinen eigenen finanziellen Vorteil, dennoch trug sein Auftreten dazu bei, dass die Bildende Kunst in Spanien sich aus dem Handwerk zu lösen begann. Zugleich kritisierte Palomino das Spätwerk von El Greco, der sich im Versuch, sich von Tizian abzusetzen, weniger kräftigen Farben und verrenkten Zeichnungen zugewandt habe. Trotz der zeitlichen Nähe zu El Greco, der noch keine hundert Jahre tot war, überlieferte Palomino jedoch auch größere Fehlinformationen. 1776 schrieb Antonio Ponz in seinem Werk "Reise durch Spanien", in dem er die Kunstschätze Spaniens verzeichnete, über El Grecos Altar für die Kathedrale von Toledo, dass „schon allein diese Gemälde ausreichen, um El Greco unter den Malern den höchsten Ruhm zu sichern.“ Auf Palominos Beschreibung baute im Jahre 1800 Juan Agustin Ceán Bermúdez auf, der einige weitere Informationen aus dem Monasterio de Santo Domingo de Silos (el Antiguo) hinzufügte. Das Schaffen El Grecos ordnete er zwischen Verrücktheit und Vernunft ein. In den 1840er Jahren veröffentlichte William Stirling-Maxwell die erste Publikation mit einer Abbildung eines Gemäldes von El Greco.

Infolge der Wiederentdeckung El Grecos durch Maler und Künstler des 19. und frühen 20. Jahrhunderts gab es zahlreiche Veröffentlichungen, die sich oftmals weiter von der historischen Realität entfernten. Autoren und Philosophen wie Maurice Barrès, Benito Pérez Galdós, Gregorio Marañón, Max Raphael, Carl Einstein, Julius Meier-Graefe und Rainer Maria Rilke näherten sich El Greco auf einer eher subjektiven Ebene und versuchten, neue Interpretationen abseits der bis dahin allgemein anerkannten Positionen zu finden. Hinzu kam, dass die Kunstgeschichte als Fach in Spanien noch nicht so gut entwickelt war. Nach historischem Erkenntnisgewinn strebten andere Autoren wie etwa Elías Tormo, Fracisco Navarro Ledesma, Max Dvořák und Carl Justi. Justi, der ein Pionier der Spanienforschung in der Kunstgeschichte war, stand El Grecos Werk eher kritisch gegenüber, schätzte jedoch dessen Porträts. Einige Kunstkritiker erklärten den Stil des Spätwerks mit einer Augenkrankheit. Jedoch zeigen die detailliert ausgeführten Gemälde eindeutig El Grecos Kunstwollen. 1910 veröffentlichte der Kunstkritiker Julius Meier-Graefe sein Buch "Spanische Reise", in dem er El Greco besonders lobte. Damit begleitete er einen wichtigen kunstkritischen Orientierungswechsel weg vom Impressionismus und hin zum Expressionismus. Für Meier-Graefe hat El Greco besondere Kraft aus seinem Mangel an Heimat gezogen. Seine scheinbaren Defizite im westlichen Kunstzusammenhang als byzantinisch ausgebildeter Ikonenmaler habe er zu seiner Stärke entwickelt und somit einen starken Individualstil gefunden. Zwischen 1911 und 1931 arbeitete August Liebmann Mayer an seinem Werk zu El Greco, in dem er historische Fakten mit seiner Kennerschaft verband. Er stand in engem Kontakt mit dem Kunsthändler Tomás Harris.

Die Zahl der Werke El Grecos war umstritten und die Zuschreibungen veränderten sich im Laufe der Zeit. Immer mehr Werke wurden ihm zugeschrieben. Manuel Bartolomé Cossío erweiterte das Gesamtwerk des Malers von 1908 bis 1928 von 174 Gemälden erst auf 235 und dann auf 383. Mayer erweiterte die Zahl von 191 im Jahr 1911 auf 357 im Jahr 1931, zugleich stieg für ihn die Anzahl der Varianten von 145 auf 408. In den beiden Katalogen von 1950 und 1970 von José Camón Aznar wurden 803 Werke El Greco zugeordnet, während Harold E. Wethey in den 1960er Jahren deutlich strengere Händescheidung vornahm und so auf nur 285 gesicherte Werke kam. Die Zuschreibung zu El Greco mit der Händescheidung zwischen eigenhändigen Werken, Werken des Sohnes und Atelierarbeiten erwies sich als schwierig und kompliziert. In den 1930er Jahren kam zudem das künstlerische Werk auf Kreta hinzu, das anfangs kaum zur Kenntnis genommen wurde und umstritten war. Erst infolge der Entdeckung des "Marientods" 1983 auf Syros, der mit El Grecos vollständigem griechischen Namen signiert ist, setzte sich langsam die Anerkennung des kretischen Werkteils in der Kunstgeschichte durch. Sie wird von der Mehrheitsmeinung getragen, es blieben aber einzelne ablehnende Positionen etwa von Wethey und Jonathan Brown bezüglich des "Modena-Triptychons". Problematisch bezüglich der Zuschreibung ist auch, dass El Grecos Werke ein beliebtes Handelsgut sind. Infolge der Desamortisation des Kirchenbesitzes 1935 und der Ausstellungen und Publikationen um 1900 wurden Werke des Malers beliebte Sammelobjekte. Damit liegt ein Interesse an der Zuschreibung zu El Greco auch bei unklaren und umstrittenen Werken vor. Hinzu kamen Restaurierungen, die mit der Maßgabe ausgeführt wurden, den Bildern eine expressionistische Ausdrucksweise zu verleihen, um damit den Geschmack des Publikums zu treffen.

Die Quellenlage zu El Greco hat sich mit der Zeit stark verbessert. Zu seiner Zeit in Spanien gibt es über 500 Dokumente. Mittlerweile sind auch vier Dokumente zu seiner Zeit auf Kreta, eines zu seinem Venedig-Aufenthalt und fünf zu seinem Aufenthalt in Rom bekannt. Zudem wurden einige theoretische Überlegungen El Grecos bekannt, die er als Anmerkungen in Büchern aus seinem Besitz hinterließ.

El Greco wurde bereits im 17. Jahrhundert in geringem Umfang künstlerisch in Spanien rezipiert, auch wenn sich keine Nachfolge in seinem Individualstil ausbildete. Diego Velázquez besaß drei Porträts von El Greco und lehnte sich in seiner Modellierung durch Licht an dessen Malweise an. Die römische Barockkunst nahm die Existenz der Kunst El Grecos jedoch kaum zur Kenntnis. In Spanien wurden besonders während des Klassizismus und der Aufklärung um 1800 die Werke El Grecos abgelehnt und der Künstler etwa aus dem Umfeld Goyas heraus kritisiert.

Der erste Schritt zur Aufwertung des Schaffens El Grecos war 1838 die Eröffnung der Spanischen Galerie im Louvre durch König Louis-Philippe I. In ihr wurden neun Gemälde des Künstlers präsentiert. Aber erst um 1900 wurde El Greco durch spanische Intellektuelle und Künstler, die auf der Suche nach einer nationalen Identität waren, als für Spanien typischer Maler rezipiert. Dabei nahm Ignacio Zuloaga eine führende Rolle ein. Er kopierte 1887 erste Werke El Grecos im Museo del Prado und adaptierte dann einige seiner Motive in eigenen Werken. 1905 erwarb er "Die Öffnung des fünften Siegels", das er als „Vorbote der Moderne“ bezeichnete, und verwendete es als Hintergrund in seinem Bild "Mis amigos", in dem er einige der wichtigsten Schriftsteller seiner Zeit porträtierte. Beeinflusst von Zuloaga kaufte Santiago Rusiñol 1893 in Paris zwei Werke El Grecos und inszenierte den Transport in sein Haus in Spanien als symbolische Überführung der Werke nach Spanien. Pablo Picasso hatte diese Wiederentdeckung El Grecos in Barcelona und Madrid selber erfahren. In seiner ersten wichtigen Arbeitsphase, der Blauen Periode, verwies sein Bild "Das Begräbnis von Casagemas" auf "Das Begräbnis des Grafen Orgaz". Eine Zeichnung Picassos trug sogar den Titel "Yo El Greco" („Ich El Greco“). In der Rosa Periode griff er in seinem als Skandal aufgenommenen Gemälde "Les Demoiselles d’Avignon" Motive aus "Die Öffnung des fünften Siegels" auf. Auch in den 1950er Jahren setzte sich Picasso in seiner Kunst noch mit El Greco auseinander. In Frankreich kopierte zudem etwa Paul Cézanne "Die Dame mit dem Hermelin" nach einer Reproduktion, als diese noch eindeutig El Greco zugeschrieben wurde. Julius Meier-Graefe schrieb außerdem von einer inneren Verwandtschaft zwischen Cézanne und El Greco, was in der Folge etwa von Rilke oder von Franz Marc in "Der Blaue Reiter" aufgegriffen wurde. Das Interesse der französischen Künstler an El Greco war insgesamt groß. Édouard Manet reiste zusammen mit Théodore Duret 1865 nach Toledo. Jean-François Millet und dann Edgar Degas besaß das Porträt "Kniender Domingo". Bei Eugène Delacroix findet sich unter anderem eine "Pietà", die an El Grecos Komposition angelehnt ist und die wiederum von Vincent van Gogh aufgegriffen wurde. Auch Marcel Duchamp setzte sich zum Ende seines malerischen Werkes etwa mit den Bildern "Portrait (Dulcinée)" und "Le Printemps (Jeune homme et jeune fille dans le printemps)" aus dem Jahr 1911 mit El Greco auseinander.

In Deutschland hatte Julius Meier-Graefes Buch "Spanische Reise" bedeutenden Einfluss auf die El-Greco-Begeisterung und beeinflusste die Künstler der Moderne. Karl Hofer ging auf sein Anraten nach Paris, wo er sich gegen Ende seines Frühwerks auch intensiv mit der Malerei des Spaniers auseinandersetzte. Ein weiterer wichtiger Impuls war die Ausstellung der Sammlung des Ungarn Marcell Nemes 1911 in der Alten Pinakothek in München, in der acht Werke El Grecos gezeigt wurden, ergänzt um das eine im Besitz des Museums und die Leihgabe des "Laokoon" aus Privatbesitz. Die Präsentation wurde etwa von Paul Klee sehr positiv rezipiert. Sie löste auch direkte künstlerische Beschäftigung mit El Greco aus wie etwa bei Antonín Procházka, Emil Filla und Franz Marc. Im unter anderem von Marc verantworteten Almanach "Der Blaue Reiter" wurde dann auch El Grecos "Heiliger Johannes" auf einer Doppelseite mit dem "Tour Eiffel" von Robert Delaunay, die sich beide in der Sammlung von Bernhard Koehler befanden, gezeigt. Marc betont zudem den Zusammenhang zwischen der Wertschätzung für El Greco und dem Aufstieg der zeitgenössischen Kunst. Dieser ideelle Einfluss ist bei Marc deutlich stärker als ein künstlerischer Niederschlag, was auch für August Macke gilt. Die Gegenüberstellung von Delaunay und El Greco ist einschlägig, da dieser den Spanier als einen Einfluss benennt. Sein Gemälde "La Ville de Paris" greift die rechte Figurengruppe des Bildes "Die Öffnung des fünften Siegels" auf.

Im Jahre 1912 setzte sich die Befruchtung der Moderne durch El Greco im Rheinland fort. In Köln fand die Sonderbundausstellung statt, die eine besondere Bedeutung für die Avantgarde hatte, und war mit einer Hommage an El Greco verbunden. Wie viele und welche Bilder dort gezeigt wurden, lässt sich nicht mehr rekonstruieren. Wahrscheinlich waren es zwei im Kontext einer retrospektiven Schau mit Werken von für die Moderne wichtigen Künstlern. Zugleich wurde in der Kunsthalle in Düsseldorf die Sammlung Nemes gezeigt, in der auch die zehn Grecos ausgestellt wurden. Bei Walter Ophey, Wilhelm Lehmbruck und Heinrich Nauen, die an der Sonderbundausstellung teilnahmen, ist die Auseinandersetzung mit El Greco nachgewiesen. Bei anderen ist eine solche erkennbar.

Die Rezeption El Grecos setzte sich auch abseits der klassischen Moderne fort. Das Frühwerk von Jackson Pollock war von seiner Beschäftigung mit der Kunst der Renaissance beeinflusst. In den beiden umfangreichsten Zeichenbüchern aus dieser Zeit finden sich einige Skizzen nach Kompositionen von El Greco. Hinzu kommen mehr als 60 lose Blätter mit Zeichnungen nach Werken des spanischen Künstlers. Dabei verwendete er zwei verschiedene Techniken. Zum einen überführte er Kompositionen El Grecos in reduzierte Darstellungen, die die Körper in wenigen dominanten Strichen übertrugen, zum anderen fertigte er detaillierte Studien nach einzelnen Figuren an, wobei Pollock in beiden Fällen individuelle Details wie Hände und Füße wegließ. Neben den Zeichnungen fertigte Jackson Pollock zudem auch Gemälde an, die sich etwa in der Gestaltung von Licht und Schatten an El Greco anlehnten. Zugleich lehnte er aber die Bedeutung der symbolischen Komponenten der Werke El Grecos ab. Bis in die Gegenwart hinein wurde El Greco künstlerisch bearbeitet. So malte Michael Mathias Prechtl in den 1980ern Bilder, die sich mit dem Spanier auseinandersetzen. Dabei schuf er etwa das ironische Werk "Das Leben des Lazarillo von Tormes", das sich auf das Porträt "Ein Kardinal (der Großinquisitor Fernando Niño de Guevara)" bezog, dem er eine vollbusige Frau auf den Schoß setzte. Mit "Der Traum Toledo – El Grecos Begräbnis" schuf Prechtl ein Bild, das er der "Ansicht Toledos" des Spaniers nachempfunden hatte.

Beeinflusst von Zuloaga reiste Rainer Maria Rilke 1912 nach Toledo, um dort Werke El Grecos zu sehen. Er schrieb in seinen Briefen dieser Zeit viel über El Greco, etwa an Auguste Rodin oder an die Fürstin von Taxis. Dabei beschrieb er die Begegnung mit dessen Werk als eines der größten Ereignisse dieser Jahre; seine Beweggründe für die Reise sei die Absicht gewesen, den Künstler in El Greco ohne großen Publikumsrummel in dessen Heimatstadt erfahren zu können. Der Schriftsteller Stefan Andres schildert in seiner 1936 erschienenen Novelle "El Greco malt den Großinquisitor" die Bedingungen von Kunst in einer Diktatur anhand des Porträts des Großinquisitors Fernando Niño de Guevara, das El Greco um 1600 malte. Dabei positionierte er den Maler als Gegner der Inquisition, was aber historisch nicht haltbar ist.

El Grecos Leben wurde mehrmals verfilmt. Aus dem Jahr 1935 stammt die zehnminütige Dokumentation "Toledo y El Greco". In den 1940er und 1950er Jahren entstanden weitere Dokumentationen mit dem Namen "El Greco en Toledo". Die Dokumentation "El Greco en su obra maestra: El entierro del Conde Orgaz" aus dem Jahr 1953 beschäftigte sich insbesondere mit dem Bild "Das Begräbnis des Grafen von Orgaz", das zu den Hauptwerken des Malers zählt.

Im Jahr 1944 entstand der Kurzfilm "Evocación de El Greco". Der erste Langfilm fürs Kino wurde 1966 mit "El Greco" veröffentlicht. Die Regie führte Luciano Salce, die Hauptfigur spielte Mel Ferrer. In diesem Film wurde das Leben des Malers vor dem geschichtlichen Hintergrund der Vorgänge am spanischen Hof und der Inquisition erzählt. 1976 wurde der spanische Fernsehfilm "El caballero de la mano en el pecho" von Juan Guerrero Zamora gedreht, in dem José María Rodero den Künstler verkörperte. In dem Film "El Greco" aus dem Jahr 2007 stellte Nick Ashdon den Maler dar. Der Film zeigte El Greco als einen Vorkämpfer für die Freiheit und erhielt einen Goya für das beste Kostümdesign.




</doc>
<doc id="12010" url="https://de.wikipedia.org/wiki?curid=12010" title="Wind (Begriffsklärung)">
Wind (Begriffsklärung)

Wind (althochdeutsch "wint" ‚der Wehende‘) steht für:

Wind ist der Name folgender Orte:
Wind ist der Familienname folgender Personen: 
Der Wind steht für:
WinD steht als Abkürzung für:
WIND steht als Abkürzung für:
Siehe auch:


</doc>
<doc id="12013" url="https://de.wikipedia.org/wiki?curid=12013" title="Beaufortskala">
Beaufortskala

Die Beaufortskala (Bft) ist eine Skala zur Klassifikation der Windgeschwindigkeit in 13 Windstärkenbereiche von 0 (Windstille) bis 12 (Orkan). Sie ist benannt nach Sir Francis Beaufort und das weitestverbreitete System zur Beschreibung von Windgeschwindigkeiten.

Die Beaufortskala stammt nicht von Francis Beaufort (1774–1857). Bereits Tycho Brahe hatte versucht, die Windstärken zu kategorisieren. Der Ingenieur John Smeaton charakterisierte sie erstmals durch ihren Effekt auf Windmühlenflügel. Seine 1759 veröffentlichte Tabelle enthielt elf Windstärken (Windstille nicht mitgezählt), jede Windstärke beschrieb er durch ihre Wirkung auf die Umgebung, etwa, dass ein Orkan Bäume ausreiße und Gebäude zerstöre. Dazu gab er jeweils die Windgeschwindigkeit und den Winddruck an. In einem Manuskript von 1790 wird Smeatons Skala zu neun Windstärken zusammengefasst, die durch ihre Wirkung auf eine Windmühle charakterisiert wurden. So war eine „Brise“ etwa dadurch definiert, dass sie die Äste eines Baumes bewegen konnte und ein Mühlenflügel pro Minute sechs bis neun Umdrehungen zurücklegte.

Von großen Handelsunternehmen wie der East India Company sowie der Kriegsmarine wurden zu Beauforts Zeit die Logbücher der zurückkehrenden Kapitäne systematisch ausgewertet, unter anderem um Hinweise auf günstige Windverhältnisse zu erhalten. Die damals üblichen Bemerkungen wie „mäßiger Wind“ waren jedoch nicht standardisiert und deswegen nicht vergleichbar. Das Problem bestand darin, dass Wind auf See – an Bord eines Segelschiffs, das sich mit dem Wind fortbewegte – nicht direkt gemessen werden konnte und an sich unsichtbar war. Verschiedene Forscher versuchten deshalb, den Wind durch seine Auswirkungen zu charakterisieren.

Der damalige Hydrograf der „East India Company“ und ab 1795 Erste Hydrograf der Admiralität, Alexander Dalrymple, machte Smeatons Skala in der Seefahrt bekannt. Dalrymples Version enthielt zwölf Windstärken (wieder ohne Windstille), die von 1 „schwacher Luftzug“ bis 12 „Sturm“ reichten. Dalrymple publizierte seine eigene Skala, die ausdrücklich für die Verwendung in Logbüchern vorgesehen war, 1779 in einer Broschüre und 1790 in dem Buch "Practical Navigation".

Beaufort lernte Dalrymples Skala vermutlich bereits durch dessen Broschüre von 1779 kennen. Zeit seines Lebens führte er ein umfangreiches Tagebuch, in dem er unter anderem das Wetter festhielt. Am 12. oder 13. Januar 1806 trug er hier die Worte ein: „Fortan werde ich die Stärke des Windes gemäß folgender Skala schätzen, denn nichts vermittelt eine unklarere Vorstellung von Wind und Wetter als die alten Ausdrücke mäßig und bewölkt, etc. etc.“ Es folgt eine Skala von 1 bis 13, die von „Windstille“ bis „Sturm“ reicht. Am 14. September 1807 legte er ein neues Tagebuch an, in dem er – wie üblich – auf dem Vorsatzblatt die Windskala notierte, die er zu verwenden gedachte. Kurz darauf verbesserte er die Skala, indem er hinzufügte, wie sich die Windstärke an den Segeln einer vollgetakelten Fregatte erkennen ließ. So herrscht etwa Windstärke 5, wenn Marssegel, Bramsegel, Royal, Flieger und Stagsegel gehisst bleiben können, bei Windstärke 7 müssen die drei obersten Segel gerefft werden.

Nachdem Beaufort 1829 Hydrograf der Admiralität geworden war, gab er seine verbesserte Skala an alle weiter, die sich dafür interessierten. Sie wurde 1832 im "Nautical Magazine" der Admiralität veröffentlicht. 1837 wurde die Skala bereits im gesamten Vermessungsdienst eingesetzt, und Ende 1838 gab die Admiralität eine Anweisung heraus, mit der sie verbindlich eingeführt wurde. Diese Skala trug jedoch keinen Namen und wurde lediglich als das „beigefügte Schema“ bezeichnet. Auch in den ausführlichen Nachrufen aus Anlass von Beauforts Tod wird an keiner Stelle eine Windskala erwähnt.

Erst 1906 schuf der britische Wetterdienst die Version der Skala mit 13 Stufen (inkl. 0), die als Beaufortskala bekannt geworden ist (in der folgenden Tabelle die Skala mitsamt der Spalte „Wirkung an Land“). Hier wird etwa Windstärke 0, also Windstille, dadurch charakterisiert, dass Rauch vertikal aufsteigt. Bei Windstärke 2, einer „leichten Brise“, wird der Wind auf dem Gesicht spürbar und Blätter rascheln. Dazu wird die jeweilige Windgeschwindigkeit angegeben. In dieser Version ist die Beaufort-Skala in zahlreiche Wörterbücher und Enzyklopädien aufgenommen worden. 1927 formulierte der deutsche Kapitän Peter Petersen die ursprüngliche Skala von Beaufort für Zwecke der Seefahrt um, weil Segel in der Schifffahrt keine große Rolle mehr spielten. In der Petersen-Seegang-Skala wird die Wirkung des Windes auf die See beschrieben (in der Tabelle die Skala mitsamt der Spalte „Wirkung auf dem Meer“, siehe auch die folgenden Bilder). So bilden sich bei Windstärke 6, „starker Wind“, große Wellen von 2,5 bis 4 Meter Höhe, überall sind ausgedehnte, weiße Schaumkämme zu sehen; es gibt etwas Gischt.

1935 wurde die Beaufortskala auf der "Ersten Internationalen Meteorologischen Konferenz" in Brüssel als allgemein gültig angenommen und 1946 auf Beschluss der "Internationalen Meteorologischen Organisation" noch einmal um 5 weitere auf insgesamt 18 Stufen (inkl. 0) erweitert :

Nachdem die Nachfolgeorganisation World Meteorological Organization die erweiterte Beaufortskala im Jahre 1970 wieder auf die traditionelle 12-teilige Skala reduzierte, wird die um eine weitere Windstärke erweiterte 18-teilige Skala inzwischen nur noch in Taiwan sowie auf dem ebenfalls wiederholt von solchen Windstärken heimgesuchten chinesischen Festland genutzt. Dort ist die Windstärke 17 als Super-Hurrikan/Super-Taifun definiert mit 109–119 kn bzw. 202–220 km/h und darüber hinaus gehende Windgeschwindigkeiten ≧120 kn bzw. ≧220 km/h werden als Hyper-Hurricane/Hyper-Taifune bezeichnet. 

1971 veröffentlichte Theodore Fujita von der Universität Chicago speziell für Windgeschwindigkeiten von "Tornados", die über die Beaufort-Skala hinausgehen, die Fujita-Tornado-Skala mit den Abstufungen F0 bis F12, bei der schon ein Tornado „mittlerer Stärke“ F5 asphaltierte Straßen vom Boden „saugen“ kann. Seit 1972 ist außerdem die siebenstufige Saffir-Simpson-Hurrikan-Skala im Gebrauch, die die eigentlichen "Hurrikane" noch einmal in fünf Kategorien unterteilt. Die Japan Meteorological Agency und die beiden Regional Specialized Meteorological Centres im nördlichen und südwestlichen Indik wenden davon abweichende Wirbelsturmskalen an. Für den südlichen Pazifik und die australische Region schließlich nutzen die dortigen Tropical Cyclone Warning Centers die Skala des australischen Bureau of Meteorology, die ebenfalls auf einer Erweiterung der Beaufortskala basiert.

Alles in allem jedoch gibt es von dieser Beaufortskala – das heißt, mitsamt der Beschreibung nach phänomenologischen Kriterien (s. u.) – keine wirklich verbindliche Version, so dass sie auch weiter in vielen verschiedenen Varianten verwendet wird. Professionelle Meteorologen messen die Windgeschwindigkeit daher stets direkt mit einem Anemometer und greifen nur im Notfall auf die Beaufortskala zurück.

Zwischen Windgeschwindigkeit formula_1 und Beaufort-Stärke "Bft" (kaufmännisch gerundet) besteht dabei nach der Revision der Beaufortskala von 1946 folgender Zusammenhang:

oder aufgelöst nach B:
wobei "v" die Windgeschwindigkeit 10 Meter über der Oberfläche ist. Die erste der Formeln wurde 1913 vom Britischen Wetterdienst als nationaler Standard festgelegt und als internationaler Standard 1914 vorgeschlagen (der Vorschlag wurde abgelehnt). Sie basiert auf einer Untersuchung des späteren Wetterdienstdirektors Sir George Clarke Simpson aus dem Jahre 1905/6, die jedoch auf der Basis von Meilen pro Stunde veröffentlicht wurde.
Für andere Einheiten gilt angenähert:
bzw.

Ein vereinfachter Zusammenhang lautet:

Da es unüblich ist, mit Bruchteilen oder Dezimalbrüchen von Windstärken zu arbeiten, gilt es, die unteren und oberen Grenzen der ganzzahligen Windstärken nach einer der genannten Formeln zu berechnen. Dabei werden die Grenzwerte in den Einheiten Knoten, km/h und Mph auf ganze Zahlen und die Werte in m/s auf genau eine Dezimalstelle gerundet. Obergrenzen werden ab-, Untergrenzen werden aufgerundet. Windstärke 9 Bft, das heißt "B" zwischen 8,5 und 9,4, entspricht also einer Windgeschwindigkeit zwischen 20,7 und 24,4 m/s, 10 Bft entsprechend 24,5 bis 28,4 m/s usw.

Tatsächlich stellt die Berechnungsformel nur den „kleinsten gemeinsamen Nenner“ dar, bei den Versuchen auf zahlreichen internationalen meteorologischen und geophysikalischen Konferenzen zwischen 1914 (in Rom) und 1970 eine gemeinsame Basis für eine einheitliche Wettervorhersage zu schaffen, da seit dem Untergang der Titanic jede seefahrende Nation zur Verbreitung von aktuellen Wetterberichten vor ihren Küsten verpflichtet wurde. Gegenstand der Verhandlungen war neben den Grenzwerttabellen selbst das jeweilige Messverfahren, wechselnde Messhöhen oder Messzeiten und andere Formalitäten. Einigkeit erzielte man erstmals 1926 mit der "Wiener Skala", die sich aus den arithmetischen Mittelwerten der britischen Simpsonskala und der international verbreiteten, älteren Skala der Deutschen Seewarte Hamburg errechnete. 1946 übernahm man die Simpsonskala von 1906 bzw. 1913 als sogenannte "Pariser Skala", um diese dann 1947 nach der Berechnungsformel bis Windstärke 17 für die Beschreibung von Hurrikans zu erweitern. Diese wurde dann 1949 mit der Bestimmung, Windgeschwindigkeiten haben in Knoten gemessen zu werden, als "Washington Code" bestätigt. 1960 wurde die Erweiterung auf 17 (zeitweilig sogar inoffiziell auf 23 Windstärken) zurückgenommen, da die Luftfahrt komplett aus der Beaufortskala ausgestiegen war, die Meteorologie eigene Hurrikanskalen entwickelte und die Seefahrt mit 12 Windstärken auskam.

In extremen Fällen oder für Stürme auf anderen Planeten, etwa dem Mars, wird die Windgeschwindigkeit direkt in einer geeigneten Einheit angegeben.

Der Winddruck steigt mit dem Quadrat der Windgeschwindigkeit und damit mit dem Kubus der Beaufort-Windstärke. Da die Beaufortskala jedoch nur die Geschwindigkeit und nicht die von der Höhe und der Temperatur abhängige Luftdichte berücksichtigt, ist die Zuordnung von Beaufort-Windstärke und Winddruck nicht eindeutig. Ein Sturm mit z. B. Beaufort 11 auf einem 6000 Meter hohen Berg entwickelt nur etwa den halben Winddruck wie ein Sturm der gleichen Beaufort-Stärke auf Meereshöhe.

Auf der Suche, was neben dem Quadrat für den Staudruck die dritte Potenz in der Formel für eine praktische Bedeutung haben könnte, stößt man auf das Widerstandsmoment im Querschnitt biegebelasteter Stäbe. Es ändert sich mit der dritten Potenz des Durchmessers bei runden bzw. des Trägheitsradius bei beliebigen Querschnitten. Das führt im Sinne von Beaufort zu der praktisch-anschaulichen und auch auf Landobjekte übertragbaren, freilich um weitere Bedingungen verkürzten, Aussage: Bei gleicher Besegelung bricht doppelte Windstärke doppelte Mastdurchmesser.





</doc>

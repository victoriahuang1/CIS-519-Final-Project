<doc id="9493" url="https://de.wikipedia.org/wiki?curid=9493" title="Albrecht Dürer">
Albrecht Dürer

Albrecht Dürer der Jüngere (auch "Duerer"; * 21. Mai 1471 in Nürnberg; † 6. April 1528 ebenda) war ein deutscher Maler, Grafiker, Mathematiker und Kunsttheoretiker. Mit seinen Gemälden, Zeichnungen, Kupferstichen und Holzschnitten zählt er zu den herausragenden Vertretern der Renaissance.

Der Name "Dürer" leitet sich vom ungarischen "Ajtósi" ab. Albrecht Dürer der Ältere, der aus dem Dorf Ajtós in der Nähe der Stadt Gyula in Ungarn stammte, ist in Ungarn unter diesem Namen ("Ajtósi Dürer Albrecht") bekannt. In Deutschland nannte er sich anfangs "Thürer" (= Türmacher), was auf Ungarisch "ajtós" heißt ("ajtó" = Tür).

Albrecht Dürer glich die von seinem Vater gebrauchte Schreibweise "Türer" an die in Nürnberg übliche fränkische Aussprache der harten Konsonanten an und schuf mit der Umwandlung in "Dürer" die Voraussetzung für sein Monogramm, das große A mit dem untergestellten D.

Dürer war der erste bedeutende Künstler nach Martin Schongauer, der seine Grafiken systematisch mit einem Monogramm kennzeichnete. Diese Urheberangabe wurde bald zu einem Gütesiegel, das auch nachgeahmt wurde.

Albrecht Dürers Vater, Albrecht Dürer der Ältere, kam 1455 aus Ungarn nach Nürnberg und übte hier erfolgreich den Handwerksberuf eines Goldschmieds aus. 1467 heiratete er Barbara Holper (* 1452; † 16. Mai 1514), die Tochter des Hieronymus Holper. Binnen 25 Jahren gebar sie 18 Kinder, von denen nur drei die Kindheit überlebten. 

Als drittes Kind dieser Ehe wurde Albrecht am 21. Mai 1471 geboren: „Ich Albrecht Dürer bin am Prudentientage, der war am Freitag, da man gezählt hat 1471 Jahr, in der freien Reichsstadt Nürnberg geboren.“ Seit 1475 lebte die Familie Dürer in einem eigenen Haus unterhalb der Burg (Burgstr. 27: Eckhaus der Gasse unter der Vesten/ heute: Obere Schmiedgasse). Albrecht Dürer jun. beschrieb seine Mutter als eine emsige Kirchgängerin, die ihre Kinder „fleißig“ und oft bestrafte. „Wohl geschwächt durch die vielen Schwangerschaften war sie häufig krank.“

Bis zu seinem 13. Lebensjahr besuchte Albrecht Dürer die Schule. In früher Jugend nahm ihn der Vater in seine Werkstatt, um ihn gleichfalls zum Goldschmied auszubilden. Aus diesen Lehrjahren stammen sein Brustbild, das er 1484 nach dem Spiegel auf Pergament zeichnete (jetzt in der Albertina in Wien) und eine Madonna mit zwei Engeln von 1485 (Kupferstichkabinett Berlin).

Ende 1486 bis 1490 lernte und arbeitete er bei dem Nürnberger Maler Michael Wolgemut; Indizien sprechen dafür, dass Dürer an den Entwurfsarbeiten zur 1493 erschienenen "Schedelschen Weltchronik" beteiligt war. Daneben bildete sich Dürer auch anhand von zeitgenössischen Kupferstichen, zum Beispiel denen von Martin Schongauer.

Von Ostern 1490 bis Pfingsten 1494 begab sich Dürer auf Wanderschaft an den Oberrhein; der genaue Weg dieser ersten von drei größeren Reisen während seines Lebens ist unbekannt. Möglicherweise war er zunächst in den Niederlanden oder am Mittelrhein, bevor er sich 1492 im Elsass aufhielt. Den in Colmar lebenden Maler Martin Schongauer, dessen Werk ihn sehr beeinflusste, lernte er nicht mehr kennen, da dieser bereits am 2. Februar 1491 gestorben war. Später wirkte Dürer in Basel. Hier entstanden die berühmten Holzschnitte zu Sebastian Brants Narrenschiff (Erstdruck 1494).

1494 heiratete er Agnes Frey (1475–1539), die Tochter eines Freundes seines Vaters aus einer alteingesessenen, angesehenen Nürnberger Familie, die allerdings nur 200 Florin Mitgift in die Ehe einbrachte. Die Ehe blieb kinderlos.

In der Folgezeit bis 1500 schuf er eine Serie von kleinen Landschaftsaquarellen mit Nürnberger Motiven bzw. mit Motiven von Stationen seiner ersten Italienreise, die er in der ersten Hälfte des Oktobers 1494, bereits drei Monate nach seiner Hochzeit, antrat. Diese Reise verstärkte sein Interesse an der Kunst des Quattrocento. Im Mai 1495 kehrte er zurück nach Nürnberg.

Von der jüngeren Forschung wird angezweifelt, dass Dürer im Rahmen dieser Reise jemals die Grenzen des deutschen Sprachgebiets überschritt, und die Indizien, die gegen einen Aufenthalt in Venedig sprechen, häufen sich: Dürer selbst erwähnte in seiner Familienchronik 1494/95 keine Reise nach Venedig. Die italienischen Züge in seinen Werken ab 1497 interpretieren manche als direkten Einfluss des paduanischen Malers Andrea Mantegna, der 1494/95 zwar nicht in Padua war, dessen Werke Dürer aber dort gesehen haben könnte. Beweisbar ist nur, dass Dürer in Innsbruck, Trient und Arco beim Gardasee war. Von Orten südlich von Arco gibt es bei Dürers Aquarellen keine Spur, also auch nicht von Venedig. Auch die Route spricht gegen die Venedig-Theorie: Für Dürer hätte es näher gelegen, den für Nürnberger (Kaufleute) üblichen Weg nach Venedig zu nehmen, der über Cortina und Treviso verlief und „Via Norimbergi“ genannt wurde. Die Bilder aus seiner späteren, nachweisbar venezianischen Zeit ab 1505 haben deutlich stärker venezianische Charakteristika.

Dürer machte sich 1497 selbstständig, und wohl ab 1503 konnte er in der Nürnberger Altstadt eine Werkstatt mit Hans Schäufelein, Hans von Kulmbach und Hans Baldung Grien als Mitarbeiter betreiben. Er arbeitete sehr intensiv an seinen Werken. In diese erste Periode seines Künstlerlebens fallen vorwiegend Porträts und einige Selbstporträts: das Bildnis seines Vaters (1497) in London (National Gallery), sein Selbstporträt (1498) im Prado in Madrid, das des Lindauer Kaufmanns Oswald Krell (beschriftet „Oswolt Krel. 1499“) in München (Bayerische Staatsgemäldesammlung), sein Selbstporträt (1500) ebenfalls in München, Bildnis Friedrichs des Weisen (1494/97) in Berlin (Staatliche Museen Preußischer Kulturbesitz) u. a. Von 1500 stammt auch der kleine Christus am Kreuz in der Dresdner Galerie, ein Bildchen von unvergleichlicher Feinheit der Ausführung, und aus derselben Zeit ein Altarwerk ebenfalls in Dresden („Die sieben Schmerzen Mariä“ und Maria das Kind anbetend, Mitteltafel in München), der „Dresdner Altar“.

Hauptsächlich widmete er sich jedoch dem Kupferstich und dem Vorlagenzeichnen für den Holzschnitt. Besonders den Kupferstich erprobte er schon sehr früh; das erste datierte Blatt ist von 1497, dem aber gewiss schon verschiedene andere vorangegangen waren. Aus dieser Zeit stammen ferner: "Die Offenbarung des Johannes" (1498), eine Folge von 16 Holzschnitten, "Adam und Eva" (1504), ein Kupferstich und "Der verlorene Sohn bei den Schweinen" (um 1496), dessen Tierdarstellung maßgeblich für die Rückzüchtung des so genannten Albrecht-Dürer-Schweins wurde.

Dürers Verbindung zum Humanismus kommt u. a. in den Illustrationen zu Conrad Celtis’ Schrift "Quatuor libri Amorum" (1502) zum Ausdruck, der seinerseits Dürer zuvor bereits als zweiten Apelles gepriesen hatte.

Im Jahr 1505 unternahm er eine belegbare Reise nach Venedig, wo damals die größten Renaissancemaler der venezianischen Schule, Tizian, Giorgione, Palma il Vecchio, tätig waren. Vor allen aber beeindruckte ihn Giovanni Bellini, den er in einem Brief als den „pest in gemell“ (Bester in der Malerei) pries. Wenn ihn sein ernstes Studium, sein Fleiß und seine Einsicht schon früher in der Heimat den Wert der Korrektheit der Zeichnung und eine wahre Naturauffassung schätzen lehrten, so sah er hier eine ungeahnte Kraft und Tiefe des Kolorits, die nachhaltig auf ihn einwirkten.

Die deutschen Kaufleute in Venedig, deren Oberältester Jakob Fugger aus Augsburg war, bestellten für die Bartholomäuskirche ein großes Bild, das "Rosenkranzfest", welches Kaiser Rudolf II. später für eine große Summe erwarb und von vier Männern nach Prag tragen ließ, wo es sich jetzt in der Národní Galerie (Nationalgalerie) befindet (zuvor im dortigen Kloster Strahov). Es stellt eine Krönung der Madonna durch zwei Engel dar. Die Jungfrau reicht dem Kaiser, das Christuskind dem Papst Rosenkränze, ebenso der heilige Dominik und mehrere Engel den Umstehenden. In dem durch Übermalung sehr verdorbenen Bild ist der venezianische Einfluss in der Komposition und Farbgebung deutlich zu erkennen. In Venedig malte Dürer auch ein paar Portraits, z. B. 1506 Burkhard von Speyer. Obgleich Dürer in Venedig hohe Anerkennung fand und der Rat von Venedig ihm ein Jahresgehalt von 200 Dukaten anbot, wenn er sich in der Stadt dauerhaft niederlassen würde, trat er die Rückreise in seine Vaterstadt an. Ein 1505 in Venedig erschienenes Exemplar von "Euklids Elementen der Mathematik" trägt ein Monogramm Dürers nebst den Worten: "Dz puch hab ich zw Venedich vm ein Dugatn kawft im 1507 jor. Albrecht Dürer" („Dieses Buch habe ich zu Venedig um einen Dukaten gekauft im 1507ten Jahr. Albrecht Dürer“).

Ab 1509 war Dürer Gesandter des Größeren Rats in Nürnberg, und so kann man davon ausgehen, dass er maßgeblich an der Planung künstlerischer Projekte der Stadt beteiligt war.

Während dieser Jahre veröffentlichte Dürer außer vielen kleineren Arbeiten in Kupferstich und Holzschnitt drei beeindruckende Holzschnittfolgen; in diesen Werkkomplexen zeigt sich Dürers Meisterschaft auf dem Gebiet der Grafik ganz besonders. Im Einzelnen handelt es sich um:

Ferner sind aus dieser Periode noch zu nennen:

Damals machte Dürer auch Versuche, mit der kalten Nadel auf Kupfer zu ritzen; so entstanden "Die heilige Veronika von 1510", "Der Leidensheiland und der büßende Hieronymus", beide von 1512. Von dieser Zeit an überwiegen die Arbeiten Dürers in Holzschnitt und Kupferstich, und man begegnet seltener Gemälden von seiner Hand.

Von den Gemälden kennt man aus dem Jahr 1512 das Tafelbild Maria mit der Birnenschnitte. In dasselbe Jahr fällt zum großen Teil eine Reihe kleiner Kupferstiche, die eine dritte Darstellung der Passion umfassen. Auch erhielt Dürer einen Freibrief von seinem Gönner Kaiser Maximilian zum Schutz vor der Nachbildung seiner Holzschnitte und Kupferstiche. Als hervorragende Werke aus dem Jahr 1512 sind noch zu erwähnen die Stiche: "Maria auf der Rasenbank", "Christus der Dulder", beides Nadelarbeiten, der heilige Hieronymus in der Felsenschlucht vor dem Betpult, sowie die "Auferstehung", weiterhin 1513 "Das Schweißtuch der Veronika, von zwei Engeln gehalten" (ein sehr ähnliches Motiv entstand 1516 als Eisenradierung) und 1514 der "Dudelsackpfeifer".

Dürer hat mehrfach im Auftrag des Kaisers Maximilian I. gearbeitet. Seit spätestens 1510/11 gab es Verbindungen, die eventuell Willibald Pirckheimer vermittelt hatte. Alle Werke dienten zumindest mittelbar der Ehre und dem Ruhm des Kaisers – neben Dürer waren in diesem Sinne z. B. die Künstler Hans Burgkmair, Hans Schäufelin und Beck oder auch Albrecht Altdorfer, Lucas Cranach und Jörg Breu tätig.

In der Albertina in Wien wird ein Manuskript eines Fechtbuchs (Cod. HS 26-232) aus dem Jahr 1512 aufbewahrt. Der Deckel trägt die Inschrift OPUS ALBERTI DURERI ("Werk Albrecht Dürers"). 200 großformatige Pergamentblätter enthalten kolorierte Federzeichnungen mit Ringer- und Fechterszenen. Es ist nicht klar, ob die Zeichnungen als eigenständiges Werk gedacht waren oder als Vorlage für ein nie ausgeführtes gedrucktes Fechtbuch mit Holzschnitten. Eine Auftraggeberschaft Kaiser Maximilians lässt sich nicht nachweisen, liegt jedoch nahe.

Weitere Werke: Illustrationen zu den "Hieroglyphen" des Horapollon in der Übersetzung von Willibald Pirckheimer; "Der Triumph" (Ehrenpforte Maximilians I. und Großer Triumphwagen), für den Dürer und dessen Werkstatt-Mitarbeiter Hans Springinklee und Wolf Traut den größten und bedeutendsten Teil zu liefern hatten (die Beschriftungen sind Johann Neudörffer zu verdanken); das möglicherweise für den St. Georgs-Orden bestimmte Gebetbuch Maximilians I.

Zu dieser Zeit entstanden parallel seine drei als Meisterstiche bekannten Werke: Ritter, Tod und Teufel (1513), Der heilige Hieronymus im Gehäus (1514), Melencolia I (1514) sowie vielleicht das ursprünglich für die Nürnberger Katharinen-Kirche bestimmte, jetzt in der Münchener Pinakothek befindliche Altarblatt der Geburt Christi mit den beiden Stifterbrüdern Paumgartner, bekannt als Paumgartner-Altar. Im gleichen Jahr hat er auch ein einzelnes tanzendes Bauernpaar gestochen und die vierschrötigen Tänzer recht lebendig geschildert. Zwei Monate vor deren Tod († 1514) fertigt er eine Kohlezeichnung seiner Mutter an; das erste Porträt eines sterbenskranken Menschen. Seit 1515 sind auch Eisenradierungen von Dürer überliefert.

Die Jahre unmittelbar vor seiner niederländischen Reise waren von einer intensiven Zuwendung zu seinen theoretischen Arbeiten geprägt. Sein Lehrbuch der Malerei konnte er wegen seines Ablebens nicht vollenden, jedoch erschien 1525 in Nürnberg sein Lehrbuch zur Geometrie und Mathematik, gefolgt von der Befestigungslehre im Jahre 1527. Postum wurde schließlich 1528 sein theoretisches Hauptwerk zur Proportionslehre, die "Vier Bücher von menschlicher Proportion", dank seiner Frau Agnes veröffentlicht.

Im Jahr 1515 entstand der Holzschnitt "Rhinocerus", eines der bekanntesten Werke Dürers.

Im Sommer 1518 war er als Vertreter der Stadt Nürnberg auf dem Reichstag in Augsburg, wo er Jakob Fugger und andere bedeutende Persönlichkeiten im Werk verewigte. Die Bekanntschaft mit Schriften Luthers, „der mir aus großen engsten geholfen hat“, fällt wohl in diese Zeit.

Vom 12. Juli 1520 ab begab sich Dürer mit seiner Frau und der Magd Susanna über Bamberg (dem Bischof Georg III. übergab er eine gemalte Madonna, ein Marienleben, eine Apokalypse und für einen Gulden Kupferstiche), Frankfurt, Mainz, Köln nach Antwerpen. Letztere Stadt sollte während seines Aufenthalts seine zentrale Residenz werden, von wo aus er zahlreiche Ausflüge in andere Städte unternahm. Ein Jahr später, am 2. Juli 1521 trat er die Rückreise an. 

Der Grund für die Reise war vor allem ökonomischer Natur. Im Januar 1519 war Dürers wichtigster Gönner Kaiser Maximilian I verstorben. Dieser hatte dem Künstler 1515 eine jährliche Leibrente von 100 Gulden zugesprochen, die die Stadt Nürnberg von der Reichsteuern abziehen sollte. Mit dem Ableben des Kaisers verweigerte der Nürnberger Rat die Fortzahlung dieses Privilegs und forderte eine neuerliche Bestätigung durch den Nachfolger Maximilians, dem späteren Karl V.

Die Krönung sollte am 20. Oktober in Aachen stattfinden und die Monate vorher nutzte Dürer dafür, um sich ein breitgefächertes Netzwerk aus Personen aus dem näheren und weiteren Umfeld des Thronanwärters zu spannen, die er als Fürsprecher für seine Angelegenheit gewinnen wollte. Vor allem die Gunst von Karls Tante Margarete von Österreich (1480–1530) sollte sich als maßgebend herausstellen.

Die Bestätigung seiner Rente erreichte ihn bereits am 12. November in Köln und doch verweilte Dürer noch viele weitere Monate in den Niederlanden. Dies hängt sicherlich auch mit dem Erfolg, der ihm während der Reise zuteil kam, zusammen. Die Reise in die Niederlande war ein Triumph ohnegleichen und überall wurde der Meister mit Respekt und Bewunderung überschüttet, die er wohlwollend entgegen nahm; Fürsten, fremde Botschafter, Händler, Gelehrte, wie Erasmus von Rotterdam, und Künstler nahmen ihn bereitwillig in ihrer Mitte auf. Der Antwerpener Magistrat bot ihm sogar vergeblich ein Jahresgehalt von 300 Philippsgulden, Steuerfreiheit, ein schönes Haus zum Geschenk, freien Unterhalt und außerdem Bezahlung aller seiner öffentlichen Arbeiten an, um ihn zum ständigen Verbleiben in seiner Stadt zu bewegen.

Von hoher Bedeutung für ihn waren der Anblick der niederländischen Kunstschätze und die Bekanntschaft mit den hervorragenden dortigen Künstlern. Sein während dieser Reise geführtes Tagebuch ist im von Rupprich herausgegebenen "Schriftlichen Nachlaß" enthalten. Auch eine große Anzahl Bildnisse von Geistlichen, fürstlichen Personen, Künstlern usw. sind ein Ergebnis seiner niederländischen Reise.

Nach seiner Heimkehr in die Vaterstadt widmete sich Dürer wieder der künstlerischen Tätigkeit. In den Jahren 1520/21 leitete er die heute verlorene Ausschmückung des Nürnberger Rathauses, die in Nachzeichnungen von 1530 in Wien, Albertina, überliefert ist. Das Programm für die Fassadenmalereien hatte Pirckheimer entworfen.

Aus dem Jahr 1526 besitzt die Alte Pinakothek in München zwei monumentale Tafeln, die zu den bedeutendsten Werken des Künstlers gehören: die lebensgroßen Figuren der vier Apostel Paulus und Petrus und der Evangelisten Markus und Johannes (Seitenstücke), zugleich die vier Temperamente verbildlichend (siehe Temperamentenlehre). Diese Tafeln hatte Dürer ursprünglich der Stadt Nürnberg geschenkt, sie waren im dortigen Rathaus ausgestellt. Aus dem Jahr 1526 stammt auch das Ölbild des Hieronymus Holzschuher in Berlin (Staatliche Museen Preußischer Kulturbesitz), das beste aller Bildnisse von der Hand Dürers, und ferner das Bildnis "Jakob Muffels" (ebenfalls in Berlin). Besonders erwähnenswert – nicht zuletzt auch wegen des ungewöhnlichen Darstellungstypus – ist das "Bildnis Johann Kleeberger", welches sich im Kunsthistorischen Museum in Wien befindet. Es stammt aus dem Jahr 1526 und soll das letzte Gemälde sein, das Albrecht Dürer gemalt hat.

In den letzten Jahren widmete sich Dürer vermehrt der Kunsttheorie; dabei kommt er zu Einsichten, die durchaus denen der Italiener widersprechen.

Dürer starb von Krankheit ausgezehrt („ausgedörrt“) am 6. April 1528, kurz vor seinem siebenundfünfzigsten Geburtstag. Vielfach wurde gemutmaßt, Dürer habe seit dem Aufenthalt in den Niederlanden (insbesondere Schouwen in der Provinz Zeeland) Ende 1520 an Malaria gelitten, die sich im April 1521 in Antwerpen erstmals mit einer ausgeprägten und mit starkem Fieber verbundenen Symptomatik bei ihm bemerkbar gemacht habe. Auf einer undatierten Skizze im Brief an seinen Arzt zeigt er auf seine Milz-Region und schreibt: „Do der gelb fleck ist und mit dem finger drawff dewt do ist mir we.“ („Da, wo der gelbe Fleck ist und worauf ich mit dem Finger deute, da tut es mir weh.“). Das könnte auf eine Milzvergrößerung (Splenomegalie) hinweisen, ein typisches Symptom der Malaria. Allerdings ist die Zeichnung vermutlich bereits vor dem Aufenthalt in den Niederlanden entstanden. Sowohl die klimatischen Bedingungen während seiner winterlichen Reise als auch seine Krankheitsgeschichte (Dürer hatte bereits seit 1507 immer wieder Fiebererkrankungen) und die Entwicklung nach 1520 passen insgesamt nicht zu einem typischen Malaria-Verlauf.

Bis zu seinem Tod war Dürer produktiv tätig, wobei er wohl zuletzt an der Vorbereitung zum Druck einer theoretischen Hauptschrift zur Proportionslehre arbeitete.

Nicht weit entfernt von dem Grab seines Freundes Willibald Pirckheimer (St. Johannis I / 1414) ruhten die irdischen Reste Dürers auf dem St. Johannisfriedhof zu Nürnberg lange unter einer einfachen Metallplatte, die sein Schwiegervater Frey für sich und seine Familie errichten ließ, bis 1681 Joachim von Sandrart das verfallene Grab neu errichtete (St. Johannis I / 0649).

Dürer hat für die Entwicklung des Holzschnittes und des Kupferstiches Bedeutendes geleistet. Den Holzschnitt hat er aus dem „Dienst der Buchillustration“ befreit und ihm den Rang eines eigenständigen Kunstwerks verliehen, das dem gemalten Bild an die Seite gestellt werden konnte. Dürer schuf durch Verfeinerung der Linien und eine Erweiterung des künstlerischen Vokabulars eine reichere Tonigkeit bzw. feinere Farbabstufungen und führte den Holzschnitt so formal in die Nähe des Kupferstichs.

Wie den Holzschnitt, so perfektionierte und revolutionierte Dürer auch die Techniken des Kupferstichs. Durch Blätter wie "Ritter, Tod und Teufel" und "Melencolia I" wurde er in ganz Europa bekannt. Dürer hat genau wie Tizian, Michelangelo und Raffael die Bedeutung der Druckgrafik darin gesehen, den eigenen künstlerischen Ruf zu verbreiten und durch den Vertrieb zu Einnahmen zu kommen. Benutzten die Italiener die Grafik zur Verbreitung ihrer Gemälde, so erhebt Dürer den Holzschnitt selbst zum Kunstwerk. In diesem Zusammenhang spricht man von Reproduktionsgrafik und Originalgrafik. Dürer hat seine druckgrafischen Zyklen im eigenen Verlag verlegt und über den Buchhandel vertrieben. Der Vertrieb druckgrafischer Blätter hatte zur Folge, dass neue künstlerische Entwicklungen schnell und gleichmäßig in ganz Europa Verbreitung fanden.

Das gesteigerte Selbstbewusstsein und die vielschichtige Selbstreflexion deutet sich in Dürers zahlreichen Selbstporträts an. In ihnen thematisiert der Künstler seinen eigenen gesellschaftlichen Stand und darüber hinaus die hohe Wertigkeit der bildenden Kunst als intellektuelle Disziplin in einer Zeit, als diese noch zum gemeinen Handwerk gezählt wurde.

Nach Fedja Anzelewsky, "Albrecht Dürer: Werk und Wirkung", elektron. Ausg. 1999 ("Vier Bücher von menschlicher Proportion"):
»Dann warhafftig steckt die kunst inn der natur / wer sie herauß kann reyssen der hat sie / vberkumbstu sie / so wirdet sie dir vil fels nemen in deinem werk vnd durch die Geometria magstu deins wercks vil beweyssen.« Nach Anzelewsky ist das Wort „kunst“ in diesem Zusammenhang als Gesetzmäßigkeit zu verstehen, und mithin spricht sich Dürer hier nicht für ein Schaffen nach Prinzipien des späteren Naturalismus aus.

In der Geschichte der Mathematik zeichnet sich die Renaissance als ein Zeitraum aus, in der wesentliche mathematische Fortschritte gehäuft von Praktikern kamen, so von dem Ingenieur Simon Stevin, dem Uhrmacher Jost Bürgi, dem Juristen François Viète, dem Kartografen Gerhard Mercator oder dem Künstler Piero della Francesca.

Der „mathematischste Kopf“ unter den Künstlern seiner Zeit war jedoch Albrecht Dürer. So erwarb er 1507 ein Exemplar der ersten Ausgabe der von Zamberti in das Lateinische übersetzten "Elemente des Euklid" von 1505, des ersten Buchdrucks dieses Werks überhaupt, und wirkte 1515 im Auftrag von Kaiser Maximilian I. an einer von dem Hofastronomen Johannes Stabius entworfenen Karte der Erdhalbkugel mit ("Stabius-Dürer-Karte"). Sein Kupferstich "Melencolia I" enthält einige mathematische Andeutungen: Zum einen ist ein magisches Quadrat abgebildet, dessen Zeilen, Spalten, Diagonale, die Zahlen in den 4 Quadranten, die 4 Zahlen im Zentrum und die 4 Zahlen in der Ecke stets dieselbe Summe 34 ergeben und das in seinen beiden mittleren unteren Feldern das Entstehungsjahr 1514 angibt – in den Feldern links und rechts daneben zeigen zudem die Ziffern 4 und 1 die Initialen Dürers im Alphabet an (4 entspricht dem vierten Buchstaben des Alphabets, also dem D wie Dürer, die 1 dem ersten Buchstaben, also dem A wie Albrecht); zum anderen wird ein Polyeder ("siehe Hauptartikel" Rhomboederstumpf) gezeigt, der durch Streckung zweier diametral gegenüberliegender Ecken eines Würfels zu einem Rhomboeder und durch anschließendes Abschneiden der beiden Spitzen senkrecht zu dieser Achse entsteht, so dass er wieder eine Umkugel wie der ursprüngliche Würfel besitzt.

Wissenschaftshistorisch bemerkenswert jedoch ist seine "Underweysung der messung mit dem zirckel und richtscheyt in Linien ebnen unnd gantzen corporen", das erste Mathematikbuch deutscher Sprache mit bedeutenden neuen Erkenntnissen. Im Titel ist das Wort „Messung“ im Zusammenhang mit der damals vorherrschenden Übersetzung „Messkunst“ für das griechische Wort Geometrie zu verstehen und bedeutet im heutigen Wortsinn eher „Konstruktion“. In der "Underweysung" definiert Dürer spezielle Kurven, insbesondere erstmals die "Muschellinie" und die "Pascalsche Schnecke" (die er selber wegen ihrer Konstruktionsvorschrift „Spinnenlinie“ nannte), gibt eine neue Konstruktion einer Ellipse an, erkennt Ellipse, Parabel und Hyperbel als Kegelschnitte (und ist damit Vorläufer von Gaspard Monge), zeigt ein neuartiges und sehr genaues Verfahren zur Winkeldreiteilung und stellt die Tangens-Funktion grafisch dar (motiviert durch das ganz praktische Problem, die Schrifthöhe in Abhängigkeit von der Höhe ihrer Anbringung so zu staffeln, dass alle Zeilen gleich hoch erscheinen).

Dürer geht dabei deduktiv und systematisch vor und ist sich des grundlegenden Unterschieds zwischen exakten Lösungen (er nennt sie „demonstrative“) und näherungsweisen („mechanice“) Lösungen stets bewusst, was ihn sogar von den meisten Mathematikern seiner Zeit abhebt.

Der Oxforder Kunsthistoriker Martin Kemp wies in einem Beitrag der englischen Wissenschaftszeitschrift Nature darauf hin, dass Dürer Parkettierungen gezeichnet hat, die Ähnlichkeit mit einem Fußbodenbelag in der Eingangshalle des "Molecular and Chemical Sciences Building" der University of Western Australia in Perth aufweisen, der auf einer Penrose-Parkettierung beruht.

Heute ist man fast sicher, dass Dürer eigentlich keine Schüler angenommen und ausgebildet hat; vielmehr war es offensichtlich so, dass er relativ eigenständige Maler bzw. Zeichner in seine Werkstatt als Gesellen aufnahm und diese sich weiterentwickeln ließ.

Als Mitarbeiter Dürers gelten
Hans Baldung genannt „Grien“ (ab 1503 Geselle in der Werkstatt, bis längstens 1508),
Barthel Beham,
Sebald Beham,
Georg Pencz,
Hans Schäufelin (ab 1503 Geselle),
Hans Springinklee und
Hans Suess von Kulmbach.

Es gibt Indizien, dass Matthias Grünewald von Dürer abgewiesen wurde. Grünewalds Mitarbeit am Heller-Altar, einer Gemeinschaftsarbeit mit Albrecht Dürer, ist jedoch belegt (Kopie von Jobst Harrich, Frankfurt a. M., Historisches Museum; Original verbrannt). Hans Dürer war sehr wahrscheinlich in der Werkstatt seines Bruders Albrecht tätig.


Dürer werden etwa 20 Exlibris zugeschrieben. Das bekannteste ist wohl davon das für seinen Freund W. Pirckheimer.



Maßgebliche Ausgabe der Schriften, Tagebücher usw.:

Das Manuskript der "Proportionslehre" befindet sich in Dresden, im "British Museum" liegen ein Band Zeichnungen und vier Bände Manuskripte. Die Bayerische Staatsbibliothek in München beherbergt das Manuskript der eigenhändigen Überarbeitung der "Vnderweysung" (4° L. impr. c. n. mss. 119). Ein entnommenes Blatt liegt in der Herzog August Bibliothek in Wolfenbüttel (Bibel-S. 4° 197, Rückdeckel)

Das Fragment seines Gedenkbuchs befindet sich im Kupferstichkabinett Berlin.

Zu Dürers schriftlichem Nachlass vergleiche den Vorbericht von Hans Rupprich, "Dürers schriftlicher Nachlaß und seine Veröffentlichung", in: Anzeiger des Germanischen Nationalmuseums 1940–1953 (1954), S. 7–17. Jetzt auch Thomas Schauerte: "Dürer und Spranger: Ein Autographenfund im Spiegel der europäischen Sammlungsgeschichte", in: Mitteilungen des Vereins für Geschichte der Stadt Nürnberg 93 (2006), S. 25–69.

Dürer zu Ehren wurden vor allem im 19. Jahrhundert Denkmäler aufgestellt und seine Büste 1842 in die Walhalla aufgenommen.

Darüber hinaus gibt es verschiedene Denkmäler, die sich auf Werke von Dürer beziehen.


Einige Gemälde Albrecht Dürers sind auf Banknoten der Deutschen Mark abgebildet. Anlässlich des 500. Geburtstages 1971 gab die Deutsche Bundesbank eine Gedenkmünze zu Ehren Albrecht Dürers heraus.

In der Novelle "Künstlerehe" von Leopold Schefer (1828) sind Albrecht und Agnes Dürer die Hauptpersonen.


Bereits im 16. Jahrhundert und besonders um 1600 gab es eine große Zahl von Nachahmungen von Werken Albrecht Dürers. Einer der bekanntesten ist wohl Hans Hoffmann (auch Hofmann, * um 1530 in Nürnberg; † 1591/2 in Prag). Viele seiner Werke sind in Details abgewandelte Dürer-Zitate, von denen einige bis in die Neuzeit für echte Dürer-Werke gehalten wurden. Auch Paul Juvenell der Ältere (1579–1643) schuf viele Dürer-Kopien.

Nach Albrecht Dürer sind unter anderem Gymnasien in seiner Heimatstadt Nürnberg, in Berlin und in Hagen, eine Grundschule in Aue (Sachsen), eine Mittelschule in Haßfurt, eine Realschule in Dortmund und Wiesbaden, ein Berufskolleg der Stadt Düsseldorf, eine Gesamtschule in Weiterstadt, eine Grundschule sowie Schule für Sehbehinderte in Mannheim, einer Gesamtschule in Heilbronn-Neckargartach und eine Förderschule in Hannover benannt.

Die Stadt Nürnberg hat zeitweise einen Albrecht-Dürer-Preis an Maler und Grafiker vergeben, so etwa an Arthur Erdle (1929), Josef Steib (1932), den Fürther Maler Karl Hemmerlein (1932), Peter Foerster (1935), Joseph Mader (1936), Anton Richter (1938), Karl Schricker (1939), Hans Böhme (1943) und HAP Grieshaber (1971).

Für Albrecht Dürer wurden folgende kirchliche Gedenktage eingerichtet:

Der Gedenktag am 7. April wurde vor der Einführung des offiziellen EKD-Namenkalenders bereits unter anderem geführt in:

Unter einem anderen Datum fand sich ein Gedenktag für Albrecht Dürer in:

Im Jahr 2011 produzierte Playmobil im Auftrag des Germanischen Nationalmuseums zur damaligen Ausstellung "Der frühe Dürer" den Künstler als Playmobil-Spielfigur.

Die kleine Dürerquelle nahe Kalchreuth, die der Überlieferung nach ihn zu seiner Federzeichnung "Quelle im Wald mit Antonius uns Paulus" inspirierte.










</doc>
<doc id="9495" url="https://de.wikipedia.org/wiki?curid=9495" title="Morbus">
Morbus

Morbus ist das lateinische Wort für Krankheit. In der Sprache der Medizin gibt "Morbus" in Verbindung mit dem Namen des Erstbeschreibers einer Erkrankung einen Namen (siehe: Eponym), wobei oft verschiedene Namen für die gleiche Erkrankung üblich sind. 

Im englischen Sprachraum ist zwar die Verwendung von "Morbus" unüblich, die Krankheiten werden trotzdem häufig mit dem Namen des Erstbeschreibers benannt. Analog zur Konstruktion im Deutschen "Basedow'sche Krankheit" heißt beispielsweise der "Morbus Basedow" im Englischen "Graves' disease".






















</doc>
<doc id="9497" url="https://de.wikipedia.org/wiki?curid=9497" title="Kindertag">
Kindertag

Der Kindertag, auch Weltkindertag, Internationaler Kindertag oder Internationaler Tag des Kindes, ist ein in über 145 Staaten der Welt begangener Tag, um auf die besonderen Bedürfnisse der Kinder und speziell auf die Kinderrechte aufmerksam zu machen. Die Art seiner Ausrichtung reicht von einem Gedenk- bzw. Ehrentag für Kinder über einen Quasi-Feiertag mit Festen und Geschenken bis zu politischen Aktionen und Demonstrationen in der Tradition eines Kampftages. Ziel des Tages ist, Themen wie Kinderschutz, Kinderpolitik und vor allem die Kinderrechte in das öffentliche Bewusstsein zu rücken.

Es gibt kein international einheitliches Datum, das historisch begründet ist (siehe Geschichte). In über 40 Staaten, wie in China, teilweise in den USA, in vielen mittel- und osteuropäischen Ländern sowie Nachfolgestaaten der Sowjetunion wird am 1. Juni der "Internationale Kindertag" begangen. In anderen Ländern sind es andere verschiedene Daten. In Deutschland und Österreich wird am 20. September der "Weltkindertag" ausgerichtet, aber auch der 1. Juni als "Internationaler Kindertag" gefeiert.

Die Vereinten Nationen selbst begehen den "Weltkindertag" am 20. November als Internationalen Tag der Kinderrechte, dem Jahrestag, an dem die UN-Vollversammlung die Kinderrechtskonvention von 1989 verabschiedete; auch diesem Datum haben sich viele Staaten angeschlossen.

Erste Ideen reichen bis zum Anfang des 20. Jahrhunderts zurück und sind eng mit der Entwicklung der Kinderrechte verbunden. 1902 veröffentlichte die schwedische Reformpädagogin Ellen Key ihr Buch "Jahrhundert des Kindes", womit sie den Schutz, die Bedürfnisse und Rechte der Kinder in das Blickfeld einer zunehmend aufgeklärten Öffentlichkeit rückte.

„Dabei war es nicht etwa ein Staat Westeuropas, sondern die Türkei, die 1920 als erstes Land einen Kindertag einführte“, zeitgleich mit der Staatsgründung. Seit 1921 begeht die Türkei den "Tag des Kindes" (Ulusal Egemenlik ve Çocuk Bayramı) am 23. April.

Aufgerüttelt durch das massenhafte Elend der Flüchtlingskinder vor allem in Osteuropa nach dem Ersten Weltkrieg gründete die englische Grundschullehrerin Eglantyne Jebb das britische Komitee „Save the Children“. Überzeugt von der Notwendigkeit, für die Interessen des Kindes einzutreten, entwarf sie ein Fünf-Punkte-Programm, welches sie 1923 an den Völkerbund in Genf schickte. Diese Charta – bekannt als Genfer Erklärung – wurde am 24. September 1924 von der Generalversammlung des Völkerbundes verabschiedet. Auch wenn es bei dieser Erklärung vor allem um den Schutz und das Wohl der Kinder ging, enthielt sie mit dem Artikel 5 ein wichtiges Element des sozialistisch geprägten internationalen Kindertages: „Das Kind soll in dem Gedanken erzogen werden, seine besten Kräfte in den Dienst seiner Mitmenschen zu stellen.“ Im Zuge dieser und weiterer Entwicklungen führten einige Staaten einen entsprechenden Tag ein. In Deutschland propagierte vor allem die Arbeiterbewegung einen Kindertag. Als 1931 in Wien die 2. Internationale Arbeiterolympiade stattfand, begannen die Feierlichkeiten am 19. Juli mit einem „Fest des Kindes“ und es wurde ein Internationaler Kindertag ausgerufen. Wegen der politischen Verhältnisse wurde dieser Aktionstag als sozialistische Propaganda angesehen.

Mit der Gründung der UNO am 26. Juni 1945 und der darauffolgenden Auflösung des Völkerbundes 1946 wurde die Genfer Erklärung nicht übernommen und verlor somit ihre völkerrechtliche Grundlage. Zunächst beschäftigte man sich mit den Menschenrechten im Allgemeinen, welche 1948 verabschiedet wurden.

Die Einrichtung eines internationalen, jährlich stattfindenden Kindertages wurde im Dezember 1948 in einer Resolution des in Budapest tagenden 2. Weltkongresses der Internationalen Demokratischen Frauenföderation (IDFF) vorgeschlagen. Auf der Ratstagung der IDFF im November 1949 in Moskau wurde dann der 1. Juni als Datum des Kindertags festgelegt. Der ebenfalls sozialistisch ausgerichtete Weltbund der Demokratischen Jugend schloss sich dieser Idee im Januar 1950 an. Der erste internationale Tag des Kindes wurde noch im selben Jahr am 1. Juni begangen und etablierte sich gleichzeitig oder kurz darauf in vielen sozialistischen Ländern.

Die Gründe für die Wahl des 1. Juni sind nicht mehr klar greifbar. Möglicherweise geht es auf zwei Ereignisse zurück. Zum einen fingen wohl die Konferenzen von 1924 bereits im Juni an und man bezog sich auf ein entsprechendes Datum. Zum anderen besagt eine Theorie, dass dies auf einen chinesischen Konsul in San Francisco zurückgeht, der anlässlich des chinesischen Drachenbootfestes Anfang Juni ein Fest für Waisenkinder in der amerikanischen Westküstenstadt veranstaltete. Anfänglich und in den offiziellen Schriften der sozialistischen Länder war der Kindertag – analog zum Internationalen Frauentag ein „Kampftag für die glückliche und friedliche Zukunft aller Kinder“ weltweit. Entsprechend lag der Schwerpunkt auf dem Recht eines jeden Kindes, in Frieden und Wohlstand aufwachsen zu dürfen.

Als die Geburtsstunde des UN-Weltkindertages gilt der 21. September 1954. An diesem Tag empfahl die 9. Vollversammlung der UNO ihren Mitgliedsstaaten die Einrichtung eines weltweiten Kindertages. So sollte


Die Staatengemeinschaft beauftragte UNICEF mit der Ausrichtung dieses weltweiten Tages. Damit griffen die Vereinten Nationen sowohl den Vorschlag auf, den die amerikanische Organisation „International Union for Child Welfare“ unterbreitete, die bereits 1952 für einen weltweiten Kindertag plädierte, als auch trugen sie dem Umstand Rechnung, dass sich in den sozialistischen Staaten schon seit 1950 ein internationaler Kindertag (1. Juni) zu etablieren begann. Die Wahl eines geeigneten Datums wurde jedem UN-Mitglied freigestellt, ebenso der Schwerpunkt und die Art und Weise.

Am 29. November 1959 wurde von der Generalversammlung der UN die „Erklärung der Rechte des Kindes“ – 35 Jahre nach der Genfer Erklärung – einstimmig verabschiedet, aber noch immer nicht völkerrechtlich bindend. 1979 wurde von der UN das gesamte Jahr zum Internationalen Jahr des Kindes erklärt. Im Zuge dessen wurde auf polnische Initiative hin eine Kommission eingesetzt, die eine Kinderrechtskonvention erarbeiten sollte. Weitere 10 Jahre später, 1989, wurde das „Übereinkommen über die Rechte des Kindes“ völkerrechtsverbindlich von der UN-Vollversammlung verabschiedet. Das Datum, der 20. November, wird seitdem von der UN als Internationaler Tag der Kinderrechte gefeiert.

Auch in der DDR wurde der Kindertag im Jahre 1950 eingeführt. Anlässlich des 1. Kindertages 1950 wurde z. B. in Dresden eine Kindereisenbahn, die spätere Pioniereisenbahn und heutige Parkeisenbahn, eröffnet. Fortan gehörte der 1. Juni – wie in den meisten anderen Ostblockstaaten auch – zu den herausragenden jährlichen Ereignissen im Leben vieler Kinder. Für die Kinder war der Tag oft ganz pragmatisch ein unterrichtsfreier Feiertag. In Schulen und Kindergärten wurden Kinderfeste veranstaltet. Es gab Kinder- bzw. Pionierfeste, Veranstaltungen mit Gratulationen und kleinen Geschenken von den Eltern, Lehrern und Erziehern. In Schulen und Kindereinrichtungen wurde der Tag mit Umzügen, Spielveranstaltungen und festlichen Programmen gestaltet, wobei politische Kinderlieder wie "Kleine weiße Friedenstaube", "Immer lebe die Sonne" oder "Über allen strahlt die Sonne" auch an den Charakter des Festes für Frieden und Völkerverständigung erinnerten.
Gefeiert wurden die Kinder an sich – die Träger der zukünftigen Gesellschaft. Aber immer auch das sozialistische System mit seinen tatsächlichen (z. B. konsequentere Abschaffung der Prügelstrafe) und vermeintlichen Errungenschaften gegenüber den kapitalistischen Ländern. Entsprechend wurde vor allem propagandistisch die Kinderarmut, ein rückständigeres Familienrecht und der Analphabetismus in der westlichen bzw. dritten Welt gegeißelt. Eigene Probleme wurden komplett ausgeblendet bzw. erst angesprochen, wenn diese erfolgreich angegangen werden konnten, wie z. B. die prekäre Ernährungssituation vieler Kinder Anfang der 1950er Jahre.

Mit der Verabschiedung der Resolution von 1954 wählte die Bundesrepublik Deutschland den 20. September als Weltkindertag. Abgesehen von vereinzelten Aktionen und Festen wurde dieser Tag von der Bevölkerung kaum wahrgenommen. Dies zeigte sich zum Beispiel auch in der Tatsache, dass der Kindertag in keinem bedeutenden westdeutschen enzyklopädischen Werk als Stichwort zu finden ist; ganz im Gegensatz zum 1. Juni in vergleichbaren Schriften der DDR und auch im Gegensatz zum "Internationalen Frauentag". Dies hat verschiedene Gründe. Zum einen wurde dieser Tag nicht propagandistisch genutzt. Im Zentrum standen vor allem nach innen gerichtete familienpolitische Themen, Missstände in der Jugendhilfe und Forderungen nach einer kindergerechteren Gesellschaft z. B. bei der Städteplanung. Zum anderen gab es nicht zu unterschätzende gesellschaftspolitische Vorbehalte. Eine konsequente Entwicklung und Umsetzung einiger Kinderrechte stand im Gegensatz zum damaligen deutschen Familienrecht (letztlich unterschrieb Deutschland die Kinderrechtskonvention von 1989 deshalb nur unter Vorbehalt und nahm diesen erst 2010 zurück). Erwähnenswerte, regelmäßige Aktionen mit Kinderfesten und politischen Forderungen wurden z. B. von der Deutschen Sektion von UNICEF und vom Deutschen Kinderschutzbundes durchgeführt.

Erst als das Deutsche Kinderhilfswerk (DKHW) 1989 – im Zuge der Unterzeichnung der Kinderrechtskonvention – den Weltkindertag mit einem Kinderfest in Bonn feierte, wuchs die Bedeutung dieses Datums wieder an. Fortan gab es von der Bundesregierung bzw. -Ministerien jährliche Pressemitteilungen.

In fast jeder größeren deutschen Stadt werden zum Kindertag Kinderfeste gefeiert, oft von freien Trägern der Jugendhilfe, Kinderschutzorganisationen und Kommunen gemeinsam. Das größte Fest mit über 100.000 Besuchern findet in der Hauptstadt Berlin auf dem Potsdamer Platz um den 20. September statt. Als bundeszentrale Festveranstaltung zum Weltkindertag übernimmt jährlich ein bedeutender Politiker die Schirmherrschaft; in den Jahren 2011 bis 2013 war es der jeweilige Bundespräsident, seit 2014 ist es die Familienministerin.
Beim Datum ist die Republik noch immer klar zweigeteilt: In den alten Bundesländern feiert man am 20. September den Weltkindertag, in den neuen Bundesländern traditionell den 1. Juni als "Internationalen Kindertag". Von der Bundesregierung und in den alten Bundesländern wurde die ersten 15 Jahre nach der Wende der 1. Juni politisch ignoriert und offiziell ausschließlich der Weltkindertag am 20. September begangen. Nach und nach akzeptierte man auch das andere Datum mit seiner Tradition, welches – vielleicht auch wegen des durchschnittlich besseren Wetters – nicht aufgegeben wurde, und begeht auch diesen Tag ganz bewusst. In Berlin findet das größte Kindertagsfest um den 1. Juni auf dem Alexanderplatz statt. Die Veranstaltung zum 1. Juni im FEZ wird auch von den beiden Hauptträgern des Weltkindertagfestes Unicef und Deutsches Kinderhilfswerk unterstützt.

In einem Appell zum Weltkindertag 2017 hat die Stiftung Deutsche Krebshilfe die Gesundheitspolitiker der Bundesregierung und der Parteien aufgefordert, die Gesundheit der Kinder und Jugendlichen ganzjährig durch Aufklärung über richtige Ernährung und die Wichtigkeit sportlicher Betätigung zu fördern. Die Hilfsorganisation selbst hat im aktuellen Förderungsjahr im Kampf gegen Krebs bei Kindern und Jugendlichen einen Schwerpunkt auf die Erforschung des Ewing-Sarkom gesetzt. Für die Entstehungsforschung und die Entwicklung neuer Behandlungsmethoden dieses „aggressiven Knochentumors“ im Pathologischen Institut am Klinikum der Universität München hat die Krebshilfe 671.000 Euro Spendengelder bereitgestellt. , Abruf am 21. September 2017

In Österreich wird der Kindertag (meist Weltkindertag genannt, um Verwechslungen mit dem Unschuldigen Kindertag zu vermeiden) seit 1953 (damals im Oktober) am 20. September gefeiert. In manchen Schulen und ähnlichen Einrichtungen werden jedoch, wie in Deutschland, Feste und Umzüge veranstaltet.

Laut UNICEF ist der Weltkindertag „Anlass für Informationen, Diskussionen und Gedanken über die Situation aller Kinder dieser Welt“.
Nach Umfragen will man hier nicht nur der Kinder in schlechten Situationen gedenken, sondern auch Kinder ehren und feiern.

Wie in Deutschland wird in Österreich auch am 1. Juni der „Internationale Kindertag“ gefeiert. Viele Spielwaren-Fachgeschäfte laden (seit 2013) an diesem Tag Kinder und ihre Eltern zu Aktivitäten ein.

In der Schweiz wird am 1. Juni der „Internationale Tag des Kindes“ gefeiert.

In Albanien wird der Kindertag am 1. Juni gefeiert.

In Argentinien ist am zweiten Sonntag im August Kindertag.

Der Bolivianische Kindertag (") wird am 12. April gefeiert.

In Brasilien wird der Kindertag am 12. Oktober zusammen mit dem Tag der Marienerscheinung " in Aparecida gefeiert.

In Bulgarien wird der "" () am 1. Juni gefeiert.

In der Volksrepublik China wird der Kindertag () am 1. Juni gefeiert.

In Chile wird der Kindertag am 1. Sonntag im August gefeiert.

In Finnland wird der Kindertag am 20. November gefeiert.

In Indien feiert man am 14. November den Geburtstag Jawaharlal Nehrus als "Kindertag".

In Japan wird der Kindertag bzw. das Knabenfest ( "Kodomo no hi") am 5. Mai gefeiert. Ursprünglich hieß der Tag "Tango no sekku" () und war ein Festtag ausschließlich für Jungen. 1948 änderte sich die Bedeutung hin zu einem Tag, an dem allen Kindern Glück und Gesundheit gewünscht wird.

In Kanada ist am 1. Juni Kindertag.

In Kosovo wird der Kindertag am 1. Juni gefeiert.

In Kuba feiert man seit 1974 den Kindertag ("Día del Niño") am dritten Sonntag im Juli.

In Mexiko wird der "Día del Niño" am 30. April gefeiert.

In Paraguay gilt seit 1948 der 16. August als Kindertag. Dieses Datum wurde in Erinnerung an die Schlacht von Campo Grande, auch "Acosta Ñu", vom 16. August 1869 gewählt, in der tausende Kinder als Soldaten eingesetzt und getötet wurden.

In Polen wird der " am 1. Juni gefeiert.

Rumänien feiert " am 1. Juni.

In Russland wird am 1. Juni der „Internationale Tag des Kindes“ () gefeiert.

Der Südkoreanische Kindertag (; "Eorininal"), seit dem 1. Mai 1923 gefeiert, wurde 1975 als offizieller Feiertag eingeführt. Er ist am 5. Mai und üblicherweise besuchen Familien an diesem Tag kinderfreundliche Vergnügungseinrichtungen wie Zoos oder Freizeitparks.

In Spanien wird der Kindertag am zweiten Sonntag im Mai gefeiert. Er heißt "Día del Niño".

In Taiwan wird der Kindertag am 4. April als gesetzlicher Feiertag gefeiert. Falls das Qingming-Fest auf den 4. April fällt, wird der Kindertag am 3. April begangen. Falls der 4. April auf einen Donnerstag fällt, wird er am darauffolgenden Tag gefeiert.

In Thailand ist am zweiten Samstag im Januar Kindertag.

In der Türkei wurde 1920 der 23. April als Feiertag namens „Tag des Kindes“ eingeführt. An dem Tag versammelte sich die Große Nationalversammlung in Ankara das erste Mal. Diese machte am nächsten Tag Mustafa Kemal, der 1934 den Nachnamen Atatürk (Vater der Türken) erhielt, zu ihrem Vorsitzenden. Mustafa Kemal widmete den Kindern den 23. April unter dem Motto „Kinder von heute sind Erwachsene von morgen“.

In Tunesien ist am 11. Januar Kindertag.

Kindertag ist der letzte Sonntag im Mai.

1. Juni

In der Slowakei und in Tschechien wird der Kindertag am 1. Juni gefeiert.




</doc>
<doc id="9498" url="https://de.wikipedia.org/wiki?curid=9498" title="Viskosität">
Viskosität

Die Viskosität bezeichnet die Zähflüssigkeit oder Zähigkeit von Flüssigkeiten und Gasen (Fluiden). Je größer die Viskosität ist, desto dickflüssiger (weniger fließfähig) ist das Fluid; je niedriger die Viskosität, desto dünnflüssiger (fließfähiger) ist es. 

Ohne weitere Angaben ist der Widerstand des Fluids gegenüber Scherung gemeint. Sie wird daher als Scherviskosität bezeichnet, zur Abgrenzung gegenüber der Dehnviskosität bei Dehnung sowie der Volumenviskosität bei gleichmäßigem Druck. Des Weiteren wird zwischen der dynamischen Viskosität und der kinematischen Viskosität unterschieden. Die "dynamische" Viskosität ist das Verhältnis von Schubspannung und Geschwindigkeitsgradient. Der Kehrwert der dynamischen Viskosität ist die Fluidität. Die "kinematische" Viskosität ergibt sich aus der dynamischen Viskosität durch Division mit der Dichte des Fluids. 

Teilchen zäher Flüssigkeiten sind stärker aneinander gebunden und somit weniger beweglich; man spricht von der "inneren Reibung". Sie resultiert "nicht nur" aus den Anziehungskräften zwischen den Teilchen des Fluids (Kohäsion). Bei dünnflüssigeren Fluiden resultiert die Viskosität aus einem Impulsfluss im Fluid. Die Viskosität von Feststoffen ist vergleichsweise hoch und damit schwer bestimmbar. Statt Viskosität werden Begriffe wie Verlustfaktor, Speicher- und Verlustmodul verwendet.

Das Wort Viskosität geht auf den typisch zähflüssigen Saft der Beeren in der Pflanzengattung Misteln ("Viscum") zurück. Aus diesen Misteln wurde der Vogelleim gewonnen. „Viskos“ bedeutet „zäh wie Vogelleim“.

Man unterscheidet zwischen der "dynamischen" und der "kinematischen" Viskosität. Die dynamische Viskosität formula_1 und die kinematische Viskosität formula_2 stehen über die Dichte formula_3 in direktem Zusammenhang:

Im SI-Einheitensystem gilt: Ein Stoff, der sich zwischen zwei Platten befindet, hat die Viskosität 1 Ns/m², wenn bei einer Größe der Platten von 1 m² und einem Plattenabstand von 1 m eine Kraft von 1 N benötigt wird, um die Platten mit einer Geschwindigkeit von 1 m/s gegeneinander zu verschieben. Für die physikalische Einheit der dynamischen Viskosität gilt also:

Für die SI-Einheit der kinematischen Viskosität gilt:

In der Praxis wird für die dynamische Viskosität neben der Pa·s (Pascalsekunde) außerdem der tausendste Teil der SI-Einheit mPa·s (Millipascalsekunde) für Medien niedriger Viskosität verwendet.

Im CGS-System wird die dynamische Viskosität in "Poise" (P) gemessen, wobei 1 Ns/m²=1 Pa·s=10 Poise=1000 Centipoise=1000 cP=1 kg/ms, und die kinematische Viskosität in Stokes (St), 1 St=10 m/s.

Das Engler-Grad ist eine veraltete Einheit für die Viskosität. Diese Einheit gibt die Viskosität im Vergleich zu Wasser an.

Den Effekt "innerer Reibung" kann man sich vereinfacht durch die Bewegung zweier übereinander liegender, "verzahnter" Molekülschichten vorstellen (siehe Abb. 1, Punkt 1). Beim Fließen gleiten die Moleküle aneinander vorbei, und um die "Verzahnung" zu überwinden, benötigt man eine gewisse Kraft. Den Zusammenhang zwischen dieser Kraft und den Eigenschaften des vorliegenden Fluids definiert die Viskosität. Erkennbar wird dieser Zusammenhang besonders gut an der homologen Reihe der Alkane (kettenförmige Kohlenwasserstoffe), hier steigt die Viskosität mit der Kettenlänge und damit den zunehmenden intermolekular wirkenden Van-der-Waals-Kräften kontinuierlich an. Bei den mittleren Alkanen (ab Nonan, neun C-Atome) hat sie bereits einen Wert ähnlich dem von Wasser.

Sehr gut veranschaulichen kann man sich die Viskosität auch an folgendem Beispiel: gleitet Wind über das Wasser eines Ozeans, erzeugt dies eine Bewegung der Wasserschicht an der Oberfläche. Je tiefer man nun taucht, desto ruhiger wird das Wasser, bis man einen Punkt erreicht, wo keine Strömung herrscht. Die einzelnen Flüssigkeitsschichten bewegen sich mit unterschiedlicher Geschwindigkeit formula_7, es entsteht ein Geschwindigkeitsgradient (siehe Abb. 1, Punkt 2).

Man stelle sich zwei im Abstand formula_8 parallel angeordnete Platten der Fläche formula_9 vor. Zwischen diesen Platten befindet sich eine Flüssigkeit, die an beiden Platten haftet. In unserer Vorstellung soll der Raum mit der Flüssigkeit in Schichten unterteilt sein. Wird nun die obere Platte mit der Geschwindigkeit formula_7 bewegt, so bewegt sich die Schicht in unmittelbarer Nachbarschaft auf Grund der Haftung ebenfalls mit der Geschwindigkeit formula_7. Da die untere Platte ruht, ruht auch ihre Nachbarschicht. Die innenliegenden Flüssigkeitsschichten gleiten mit unterschiedlichen Geschwindigkeiten aneinander vorbei. Die Geschwindigkeit nimmt von der ruhenden Platte zur bewegten zu.

Von der obersten, an der Platte haftenden Schicht geht eine Tangentialkraft auf die darunterliegende Schicht aus. Diese bewegt sich folglich mit der Geschwindigkeit formula_12. Diese Schicht wirkt wiederum auf die darunterliegende Schicht und bewegt sie mit der Geschwindigkeit formula_13.

Im Experiment lässt sich zeigen, dass im Idealfall die Kraft formula_14, die nötig ist, um die obere Platte zu bewegen, proportional zur Fläche formula_9, dem Geschwindigkeitsunterschied formula_16 und antiproportional zum Abstand der Platten formula_17 ist:

Hieraus ergibt sich die Gleichung

Die Proportionalitätskonstante formula_1 ist die dynamische Viskosität. Die Änderung der Geschwindigkeit senkrecht zur Bewegungsrichtung, also der Geschwindigkeitsgradient

auch mit formula_24 oder formula_25 bezeichnet, wird Verformungsgeschwindigkeit, Schergeschwindigkeit oder Scherrate genannt. Mit der Schubspannung

ergibt sich der Zusammenhang

Handelt es sich um sehr dünne Fluidschichten, so ist der Geschwindigkeitsverlauf linear, wie in obiger Herleitung. Dieser Zusammenhang wurde bereits 1687 von Isaac Newton unterstellt:

Flüssigkeiten, die diesem linearen Zusammenhang folgen, werden deswegen als newtonsche Fluide bezeichnet. Ist formula_1 von formula_7 abhängig, so bezeichnet man die Flüssigkeit als "nicht-newtonsch". Beim newtonschen Viskositätsgesetz wird stets laminare Strömung sowie Temperatur- und Druckunabhängigkeit der Flüssigkeitseigenschaften angenommen. Für diese Stoffe stellt sich das im Schubspannungs-Schergeschwindigkeits-Diagramm gezeigte, lineare Geschwindigkeitsprofil ein (Abb. 3, Kurve 2: Newtonsches Fluid).

In den rheologischen Modellen wird das newtonsche Verhalten durch das "Newton"-Element, einem Dämpfungszylinder ähnlich einem Stoßdämpfer, dargestellt.

Viele Substanzen folgen diesem Gesetz jedoch nicht, sondern zeigen ein zeit- oder schergeschwindigkeitsabhängiges Verhalten. Dabei unterscheidet man verschiedene Arten der Abweichung:


Im allgemeinen Fall muss das Schergefälle formula_31 aus dem Scherwinkel in der Flüssigkeit berechnet werden und nicht über den Geschwindigkeitsgradienten. Das Verhältnis formula_33 wird in diesem Fall auch "scheinbare Viskosität" genannt.

Viskoelastische Materialien können mit der komplexen Viskosität, bei der von einer sinusförmigen Scherung ausgegangen wird, beschrieben werden.

Typische nicht-newtonsche Fluide sind zusammengesetzte Substanzen wie Blut und Ketchup. Da Blut sowohl aus den festen Substanzen des Hämatokrits als auch des Blutplasmas besteht, das Mischungsverhältnis jedoch stark variiert, verändert sich auch die Viskosität. Bei hohem Anteil der festen Substanzen ist z. B. die Leistungsfähigkeit in Ausdauer-Sportarten deutlich erhöht, bei zu hohen durch Doping kann dies zum Tod führen.

Die "dynamische Viskosität" der meisten Flüssigkeiten nimmt mit steigender Temperatur ab und kann oft mit der Arrhenius-Andrade-Beziehung beschrieben werden:

mit
Bei Flüssigkeiten in der Nähe (d. h. bis ca. 100 K über) der Glasübergangstemperatur gilt meist die WLF-Beziehung. Hier dominiert nämlich das sehr geringe freie Volumen, das in der Nähe der Glasübergangstemperatur viel stärker von der Temperatur abhängt als die Kettenbeweglichkeit, die hinter der Arrhenius-Andrade-Beziehung steht.

Die Abhängigkeit der "kinematischen Viskosität" von der Temperatur wird bei Ölen durch den Viskositätsindex beschrieben.

Die Viskosität von Flüssigkeiten kann mit einem Viskosimeter z. B. gemäß EN ISO 3219 gemessen werden. Ein Rheometer ermöglicht es, darüber hinaus noch weitere rheologische Eigenschaften, auch von Festkörpern, zu bestimmen. Bei beiden Gerätetypen wird entsprechend der Viskositätsdefinition die zu messende Probe im Spalt zwischen zwei Körpern (z. B. zwei koaxialen Zylindern oder zwei parallelen Platten) eingebracht. Ein Teil der Anordnung rotiert oder oszilliert mit definierter Geschwindigkeit, während der andere ruht. Aus der Geometrie der Messanordnung und der Geschwindigkeit des bewegten Teiles ergibt sich die Schergeschwindigkeit. Das zur Aufrechterhaltung der Bewegung notwendige Drehmoment wird gemessen, woraus sich dann die Schubspannung und damit die Viskosität ermitteln lässt.

Eine schnelle und einfache, aber auch sehr ungenaue Methode der Viskositätsbestimmung ist der Auslaufbecher.


Bei Fluiden mit niedriger Viskosität entspricht das Bild der inneren Reibung „nicht den physikalisch korrekten Vorstellungen über molekülbedingte Transportvorgänge in Fluiden“. Stattdessen resultiert hier die Viskosität im Fluid aus einem Impulsfluss, der mit dem folgenden Bild veranschaulicht werden kann: Die Stromfäden in der Strömung werden durch Züge versinnbildlicht, die mit unterschiedlicher Geschwindigkeit parallel nebeneinander her fahren und mit Sandsäcken beladen sind. Die Sandsäcke entsprechen den Fluidelementen im Stromfaden. Personen auf den Zügen werfen die Sandsäcke auf den jeweils anderen Zug, was der zufälligen thermischen Bewegung der Fluidelemente zwischen den Stromfäden gleichkommt. Landet ein Sack des langsameren Zuges auf dem schnelleren, dann nimmt der Sandsack Impuls auf, den der schnellere Zug an ihn abgibt und so selbst langsamer wird. Wenn umgekehrt ein Sack des schnelleren Zuges auf dem langsameren landet, nimmt der Zug den Impuls des Sandsacks auf und wird so selbst schneller. Durch diesen Impulsaustausch wird der schnellere Zug abgebremst und der langsamere beschleunigt.

Viskosität stellt demnach einen Impulsfluss von einem schneller fließenden Stromfaden auf einen langsamer fließenden dar. In einer Kontinuumsströmung tauschen die Fluidelemente Impuls über die zwischen ihnen wirkenden Spannungen aus. Zwischen den unterschiedlich schnell nebeneinander her fließenden Stromfäden kommt es zu Schubspannungen, die sich makroskopisch als Viskosität bemerkbar machen.

Für Gase lässt sich die Viskosität anhand einer mikroskopischen Betrachtung des Impulsflusses abschätzen:

mit der freien Weglänge formula_40 für die Gasteilchen, der Masse der Gasteilchen formula_41, der mittleren Teilchengeschwindigkeit formula_7 und der Teilchenzahldichte formula_43.

Die Viskosität von Gasen ist bei niedrigen Drücken (≈0,1 bis 10 bar) unabhängig vom Druck. Dies gilt solange, wie die freie Weglänge klein gegenüber den Gefäßabmessungen und groß gegenüber den Molekülabmessungen ist. Mit anderen Worten: Für ein sehr dünnes oder ein sehr dichtes Gas wird die Viskosität doch wieder vom Druck beziehungsweise der Dichte des Gases abhängig.

Grundsätzlich abhängig ist die Viskosität aber von der Temperatur. Mit zunehmender Temperatur steigt die Viskosität, da die mittlere Teilchengeschwindigkeit formula_7 proportional zu formula_45 wächst (siehe unten). Dieses Verhalten ist bei den meisten Flüssigkeiten genau entgegengesetzt. Die folgende Tabelle listet zu einigen Gasen die Viskositäten und freien Weglängen auf.

Nach Hirschfelder kann die Viskosität reiner Gase mit Hilfe der kinetischen Gastheorie in einem großen Temperaturbereich (etwa von 200 bis 3000 Kelvin) berechnet werden.

Hierbei ist
formula_41 die Molekülmasse,
formula_48 die Boltzmann-Konstante,
formula_38 die Temperatur,
formula_50 der Lennard-Jones-Stoßdurchmesser und
formula_51 das reduzierte Stoßintegral, das
von der reduzierten Temperatur formula_52 abhängt. formula_53 ist die Energie des Lennard-Jones-Potentials. Werte für die Lennard-Jones-Parameter und das reduzierte Stoßintegral sind in Lienhards Lehrbuch zur Wärmeübertragung in Kapitel 11 aufgeführt. Das reduzierte Stoßintegral formula_51 ist so definiert, dass für ein ideales Gas, bei dem Teilchenwechselwirkungen wie Stöße harter Kugeln betrachtet werden, formula_55 gilt.

Die Viskosität begründet sich aus dem Experiment, nach dem zur Aufrechterhaltung einer Scherströmung eine Kraft erforderlich ist.
Diese Kraft bewirkt einen Impulsaustausch innerhalb der Strömung bzw. mit dem Rand, weshalb sie zur Kategorie der Oberflächenkräfte zählt. Kontinuumsströmungen tauschen Impuls über mechanische Spannungen aus, wobei ein Spannungsanstieg eine Beschleunigung bewirkt. Im Kontinuum lässt sich die beschleunigende Kraft in der allgemeinsten Form als Divergenz eines Tensors formulieren:

wobei formula_57 die Komponente des Spannungstensors auf Grund der Viskosität ist und "zäher Spannungstensor" oder "Reibungstensor" heißt. Der Nabla-Operator formula_58 bildet hier die Divergenz div des Reibungstensors.

Aus dem Experiment folgt unmittelbar, dass der Reibungstensor eine Funktion der räumlichen Änderung der Strömungsgeschwindigkeit ist:

Der Operator grad bildet aus der Geschwindigkeit formula_60 den Geschwindigkeitsgradient. Da kein Impulsfluss bei homogener Strömung formula_61 oder formula_62 enthält der Reibungstensor keine Komponenten, die unabhängig vom Geschwindigkeitsgefälle sind. In newtonschen Fluiden sind die Spannungen linear in den Geschwindigkeitsgradienten, eine Annahme, die gerechtfertigt ist, wenn der Geschwindigkeitsgradient im Sinne der Hydrodynamik klein ist.

Weiterhin tritt keine Viskosität auf, wenn sich die Strömung in starrer Rotation (formula_63 mit dem Abstandsvektor formula_64 von der Drehachse) befindet, wobei formula_65 die Winkelgeschwindigkeit ist, die aus dem schiefsymmetrischen Anteil des Geschwindigkeitsgradienten resultiert, siehe Kinematik in der Strömungsmechanik. Dieser schiefsymmetrische Anteil hat mithin keinen Einfluss auf die Spannungen, weswegen allein der symmetrische Anteil D des Geschwindigkeitsgradienten

Spannungen verursacht. Das hochgestellte T bildet die Transposition. Mit der weiteren Annahme einer isotropen Flüssigkeit werden die unmittelbaren Stoffeigenschaften durch skalare Größen beschrieben. Damit wird der Reibungstensor:

Darin ist δ das Kronecker-Delta, formula_68 die Volumenviskosität, λ die erste Lamé-Konstante, Sp der Spur-Operator und 1 ist der Einheitstensor. Der erste Term beschreibt die Viskosität durch volumentreue Deformation (der Tensor in den eckigen Klammern ist spurfrei oder deviatorisch). Der zweite Term stellt die Viskosität durch Volumenänderung dar. Dieser Term wird bei Inkompressibilität verschwinden, denn dann ist formula_69.




</doc>
<doc id="9499" url="https://de.wikipedia.org/wiki?curid=9499" title="Sterntag">
Sterntag

Ein Sterntag ist der Zeitraum zwischen zwei oberen Kulminationen des Frühlingspunkts, verursacht durch die Eigendrehung der Erde. Der Sterntag bildet die Basis der sogenannten Sternzeit, einer in Astronomie und Geodäsie verwendeten Zeitskala.

In SI-Einheiten gemessen dauert ein "mittlerer Sterntag" 86.164,091 Sekunden bzw. 23,9344697 Stunden oder 23 Stunden, 56 Minuten, 4,091 Sekunden und ist damit um rund 1/365 kürzer als ein "bürgerlicher Tag" (der als Jahresmittelwert der Sonnentage dem üblichen Kalendertag zugrunde gelegt wird).

In genauen astronomischen Jahrbüchern werden die Sternörter meist für den Zeitpunkt der Kulmination in Greenwich tabuliert. Somit ist das Tafelintervall eine ganze Zahl von Sterntagen. Um Fehlern bei der Interpolation vorzubeugen, wird das Argument oft nicht in Stunden, sondern in Tagesbruchteilen angegeben.

Trotz seines Namens bezieht sich der Sterntag "nicht" auf die Sterne, sondern auf den Frühlingspunkt, der sich durch die Präzession der Erde langsam gegenüber den Fixsternen verschiebt.

Allgemein wird in der Astronomie für „auf den Frühlingspunkt bezogen“ das Wort „tropisch“ verwendet; daher heißt die dem Sterntag entsprechende jährliche Periode, verursacht durch den Umlauf der Erde um die Sonne, tropisches Jahr. Während man allerdings von tropischem Jahr und tropischem Monat spricht, ist die Bezeichnung "tropischer Tag" anstelle von Sterntag nicht gebräuchlich, obwohl sie logisch und konsequent wäre.

Im Unterschied zum Sterntag bezieht sich ein Siderischer Tag tatsächlich auf den Fixsternhimmel als Hintergrund (siehe auch Siderische Periode). Er beschreibt die Zeitspanne zwischen zwei oberen Kulminationen eines (von seiner Eigenbewegung befreiten) fernen Sterns. Ein Siderischer Tag dauert etwa 0,008 Sekunden länger als ein Sterntag.

Die Benennung in englischsprachiger Literatur ist gerade andersherum als im Deutschen: Hier heißt der Sterntag "sidereal day" und der Siderische Tag "stellar day"(!).

In der astronomischen Praxis wird häufig – in vereinfachter Berechnung der Koordinaten von beobachteten Himmelskörpern – die Dauer des mittleren Sterntags auch als (angenäherte) "Rotationsdauer der Erdumdrehung" bezeichnet, obgleich die Dauer des mittleren Siderischen Tages näher am Fundamentalsystem der Astronomie gemessen wäre. Da jedoch die Geschwindigkeit der Erdrotation nicht konstant ist, sind die aktuellen Zeitspannen sowohl eines siderischen Tages wie eines Sterntages leichten Schwankungen unterworfen, die sich im Bereich einiger Millisekunden bewegen.


</doc>
<doc id="9501" url="https://de.wikipedia.org/wiki?curid=9501" title="Snelliussches Brechungsgesetz">
Snelliussches Brechungsgesetz

Das Brechungsgesetz, auch Snelliussches Brechungsgesetz, Snelliussches Gesetz oder Snellius-Gesetz beschreibt die Richtungsänderung der Ausbreitungsrichtung einer ebenen Welle beim Übergang in ein anderes Medium. Ursache der Brechung genannten Richtungsänderung ist die Änderung der materialabhängigen Phasengeschwindigkeit, die als Brechungsindex in das Brechungsgesetz eingeht. Das bekannteste Phänomen, welches durch das Brechungsgesetz beschrieben wird, ist die Richtungsablenkung eines Lichtstrahls beim Durchgang einer Mediengrenze. Das Gesetz ist aber nicht auf optische Phänomene begrenzt, sondern gültig für beliebige Wellen, insbesondere Ultraschallwellen.

Das Brechungsgesetz ist nach dem niederländischen Astronomen und Mathematiker Willebrord van Roijen Snell benannt, in einigen Sprachen nach der latinisierten Form „Snellius“, der es 1621 zwar nicht als Erster fand, aber als Erster veröffentlichte.

Die Richtung des einfallenden Strahls und das Lot auf die Grenzfläche bestimmen die Einfallsebene. In dieser Ebene liegen auch der gebrochene und der reflektierte Strahl. Die Winkel werden zum Lot hin gemessen. Das Brechungsgesetz ist folgende Beziehung zwischen dem Einfallswinkel formula_1 und dem Winkel formula_2 des gebrochenen Strahls:
Darin sind formula_4 und formula_5 die Brechungsindizes der jeweiligen Medien. Luft hat einen Brechungsindex, der sehr nahe an formula_6 liegt. Beim Übergang von Luft zu Glas kann daher das Brechungsgesetz genähert werden als:

Der Brechungsindex eines optischen Mediums ist im Allgemeinen abhängig von der Wellenlänge. Diese Dispersion geht in das Brechungsgesetz ein. Unterschiedliche Wellenlängen werden unterschiedlich stark gebrochen. Dies wird bei Dispersionsprismen zur Auftrennung des Lichts nach Farben ausgenutzt.

Das Brechungsgesetz gilt nur für schwach absorbierende Medien.

Brechung wurde von Ptolemäus in seinem Werk „Optik“ beschrieben. Sein lineares Gesetz gilt aber nur für kleine Winkel. Korrekt angegeben wurde das Brechungsgesetz zum ersten Mal im 10. Jahrhundert von Ibn Sahl. Das Gesetz wurde 1601 von Thomas Harriot 
und um 1621 wurde es von Willebrord van Roijen Snell wiederentdeckt, aber nicht veröffentlicht. Während Harriots Entdeckung erst 350 Jahre später bekannt wurde, wurde Snellius' Beitrag 1632 von Jacob Golius bekannt gemacht. Fast zur gleichen Zeit und vermutlich unabhängig von Snellius veröffentlichte René Descartes 1637 in seiner "Dioptrique" einen ähnlichen Zusammenhang. Seine Ableitung war allerdings falsch, da er von einer höheren Lichtgeschwindigkeit im optisch dichteren Medium ausging (korrekt leitete es erst Pierre de Fermat ab).

Der Brechungsindex formula_8 eines Mediums gibt an, um wie viel dort die Phasengeschwindigkeit formula_9 und die Wellenlänge formula_10 geringer bzw. kürzer sind als im Vakuum:
Von einem Medium in ein anderes ändert sich die Wellenlänge um den Faktor formula_12, beim rechts dargestellten Übergang in ein optisch dichteres Medium (formula_13) wird die Welle also gestaucht. Diese Stauchung führt zur Ablenkung.
Im 2. Bild ist der gleiche Vorgang schematisch dargestellt. Zwischen zwei parallel verlaufenden Strahlen ist an zwei besonderen Stellen eine Wellenfront eingezeichnet: Die Wellenfront hat auf dem einen Strahl die Grenzfläche gerade erreicht (A) und muss auf dem anderen Strahl noch die Strecke L (= |BB'|) im Medium 1 zurücklegen bis sie die Grenzfläche (bei B') berührt. Dazu benötigt der zweite Strahl im Medium 1 die Zeit formula_14:
Analog dazu durchläuft in dieser Zeit der erste Strahl die Strecke L (= |AA'|) im Medium 2. Durch Umstellung und Gleichsetzung nach "c" ergibt sich, dass die Strecke formula_16 um obigen Stauchungsfaktor formula_12 kürzer ist als formula_18.

Zwischen der Grenzfläche und den beiden Wellenfronten treten die gleichen Winkel formula_1 und formula_2 auf, wie zwischen dem Lot und den einfallenden bzw. gebrochenen Strahlen. Die Gegenkatheten dieser Winkel sind L bzw. L, die in der Grenzfläche liegende Hypotenuse der Länge |AB'| haben sie gemeinsam. Folglich gilt

und

Durch Umstellung und Gleichsetzung nach |AB'| ergibt sich daraus

bzw. mit der oben genannten Beziehung zwischen Brechungsindex und den Strecken formula_18 und formula_16

was zum Brechungsgesetz äquivalent ist.

Das Brechungsgesetz kann auch aus dem fermatschen Prinzip gefolgert werden, das besagt, dass kleine Änderungen des Weges, den das Licht zwischen zwei Punkten P und Q nimmt, die optische Weglänge nicht ändern. Im Fall der Brechung wäre eine systematische Variation die Verschiebung des Knickpunktes innerhalb der Grenzfläche, etwa von A nach B' im vorstehenden Bild. Bei der Verschiebung, die so klein ist im Vergleich zur Entfernung zu den Punkten P und Q, dass sich dabei die Winkel nicht ändern, vergrößert sich der geometrische Weg im Medium 1 um L, während im Medium 2 L hinzukommt. Wegen der verschiedenen Phasengeschwindigkeit ändert sich insgesamt die Phase nicht.

Für formula_27 und genügend große formula_1 ist

und damit durch kein (reelles) formula_2 erfüllbar. In diesen Fällen tritt Totalreflexion auf, bei der das Licht vollständig reflektiert wird.

Für den "Grenzwinkel der Totalreflexion" formula_31 gilt Gleichheit, also

Totalreflexion wird zum Beispiel in Umkehrprismen von Ferngläsern genutzt.

Betrachtet man von außerhalb des Wassers Gegenstände, die sich unter Wasser befinden, so erscheinen sie in senkrechter Richtung gestaucht. Der Boden des Gefäßes erscheint höher als bei einem Bild derselben Szene ohne Wasser. Diese Erscheinung wird daher auch optische Hebung genannt. An einem geraden Stab, der schräg ins Wasser eintaucht, sieht man einen Knick an der Wasseroberfläche. Aufgrund unterschiedlicher Brechungsindizes von Wasser und Luft entsteht ein anderer Brechungswinkel der vom Stab ins Auge kommenden Lichtstrahlen über und unter der Wasseroberfläche an der Grenzfläche zum Glas. Das menschliche Gehirn berücksichtigt diese unterschiedlichen Brechungswinkel nicht und verlängert die Strahlen geradlinig nach hinten, so dass der Stab unter Wasser flacher erscheint als der Stab über Wasser.
Auch für mechanische Wellen, das heißt Druck- oder Scherwellen, gilt das Brechungsgesetz. Im Rahmen der Akustik bzw. Ultraschalltechnik wird das Snelliussche Brechungsgesetz aber ohne Brechungsindizes formuliert, sondern mit Hilfe der Wellenzahl formula_33. Es gilt (siehe nebenstehendes Bild für die Winkelbezeichnungen):

Mit der Definition formula_35 mit formula_36 erhält man das Brechungsgesetz in der Formulierung mit den Phasengeschwindigkeiten der betreffenden Wellentypen im betreffenden Medium und somit die gleiche Formulierung wie in der Optik (falls man dort die Vakuumlichtgeschwindigkeit rauskürzen würde). Die Herleitung des Gesetzes in der Akustik geschieht über die Forderung nach der Erfüllung der Kontinuitätsgleichung für mechanische Spannungen und Verschiebungen an der Mediengrenze. Das nebenstehende Bild zeigt eine einfallende Longitudinalwelle in einem Festkörper, die an einer Grenzfläche zu einem zweiten Festkörper teilweise reflektiert und transmittiert wird. Im Allgemeinen entstehen an der Grenzfläche aus der einfallenden Longitudinalwelle (P-Welle) neue Wellentypen, so dass zwei verschiedene Wellentypen reflektiert und transmittiert werden: P-Wellen und S-Wellen (Scherwelle). Beide Wellentypen breiten sich mit unterschiedlichen Phasengeschwindigkeiten in den beiden Medien aus, daher werden sie auch unter unterschiedlichen Winkeln gebrochen. Diese Winkel können mit obigem Gesetz berechnet werden, falls die einzelnen Phasengeschwindigkeiten formula_37 sowie der Einfallswinkel der Primärwelle formula_38 bekannt sind. Im Falle von schubspannungsfreien Medien (Flüssigkeiten und Gase) treten keine Scherwellen auf, so dass die einfallende P-Welle nur eine reflektierte und eine transmittierte P-Welle erzeugen würde.



</doc>
<doc id="9504" url="https://de.wikipedia.org/wiki?curid=9504" title="Metalldampflampe">
Metalldampflampe

Metalldampflampen sind Gasentladungslampen, in denen Metallatome durch Stoßvorgänge mit Elektronen angeregt werden und Energie in Form von Licht abgeben.
Um den Strom zu begrenzen, werden meist Drosselspulen benutzt.
Eine Metalldampflampe besteht aus einem Glaskolben, in den eine kleine Menge Metall sowie ein Edelgas gefüllt werden. Das Edelgas dient dem Aufbau eines Funkens zwischen den beiden in den Kolben hineinreichenden Elektroden. Hierbei verdampft das namensgebende Metall, wodurch die Lampe einsatzbereit wird.

Beim Betreiben einer Metalldampflampe muss ein Vorschaltgerät bestehend aus Zündgerät und Drossel verwendet werden.
Das Zündgerät sorgt für eine Hohe Spannung, um den ersten Funken zu erzeugen, danach kann die die Lampe mit Wechselstrom im Bereich um 80 V bis 100 V betrieben werden (abhängig vom Lampentyp und der Füllung).
Die Drossel begrenzt den Strom durch die Lampe, um diese vor der Selbstzerstörung zu schützen.

Beim Einschalten der Lampe generiert das Zündgerät eine Hochspannung (Zündspannung), die einen Funken zwischen den Elektroden überspringen lässt. Die Ablagerungen des Füllmetalls werden hierbei durch die entstehende Hitze verdampft und helfen – zusammen mit dem enthaltenen Edelgas – den Stromfluss zwischen den Elektroden bei der geringeren Betriebsspannung aufrecht zu erhalten.

Durch den Stromfluss in der Röhre bewegen sich Elektronen von der Kathode zur Anode. Da die Elektronen zwischen den Elektroden beschleunigt werden, können sie genug Energie aufnehmen, um unelastische Stöße mit den in der Lampe präsenten Metallatomen auszuführen: Die beschleunigten Elektronen können ein Elektron in der Schale eines Metallatoms auf eine höhere Schale bewegen (das Atom wird angeregt).
Energetisch betrachtet bewegt sich das Elektron vom Grundzustand "E" in einen angeregten Zustand "E". Der damit frei gewordene Platz wird wieder besetzt; ein Elektron im Atom bewegt sich also von einem Zustand "E" in den freien Zustand "E", wobei die Energiedifferenz in Form eines Photons mit der Energie formula_1freigesetzt wird.
Aus dieser Energie ergeben sich somit auch die charakteristischen Emissionsspektren (beispielsweise das typisch gelbe Licht von Natriumdampflampen) mit den Wellenlängen formula_2 und daher den Frequenzen formula_3 mit "h" gleich dem Planck'schen Wirkungsquantum und "c" der Lichtgeschwindigkeit.Dies gilt jedoch nur für Niederdruck-Metalldampflampen, da die Elektronen- und Gastemperatur kaum gekoppelt ist.
Bei Lampen mit höheren Drücken kommt es aufgrund der Kopplung der Elektronen- an die Gastemperatur zu einer Verbreiterung der Spektrallinien.

Sogenannte Spektrallampen enthalten ein charakteristisches Element und dienen der Kalibrierung von Spektrometern sowie als Quellen für monochromatisches Licht. Alkalimetalle eignen sich gut für den Einsatz in Spektrallampen. Durch deren Elektronenkonfiguration haben sie nur ein freies Elektron und besitzen damit nur wenige, scharfe Spektrallinien. Des Weiteren werden Wasserstoff, Helium, Quecksilber und Cadmium usw. als Füllstoffe eingesetzt. 
Die Spektrallinien werden, ausgehend von der niedrigsten Energie (also der größten Wellenlänge) mit den Buchstaben des lateinischen Alphabets bezeichnet. So hat z. B. die i-Linie des Quecksilbers eine Wellenlänge von 365 nm bzw. eine Frequenz von 821,35 THz.

Diese sind auch oft in Terrarien für Wüstentiere zu finden.

Natriumdampflampen und Quecksilberdampf-Hochdrucklampen werden z. B. zur Straßenbeleuchtung eingesetzt. Natriumdampflampen sind an ihrer charakteristischen gelben Lichtfarbe zu erkennen.

In der Veranstaltungstechnik wird für Beleuchtungsanlagen mit sehr hohem Bedarf an Helligkeit auf Metalldampflampen zurückgegriffen. Das sind insbesondere Verfolgerscheinwerfer für Arenen und Effektscheinwerfer, deren Lichtausbeute des Weiteren durch Effekträder gemindert wird.

Außerdem werden Metalldampflampen als nächtliche Außenbeleuchtung für Gebäude benutzt, da die extrem hohe Lichtausbeute bei geringerem Stromverbrauch als bei gewöhnlichen Glühlampen in Kombination mit dem beinahe bläulichen Licht bestens für solche Szenarien geeignet ist.

In Leuchtstofflampen wird ebenfalls Quecksilberdampf als primäre Lichtquelle verwendet. Allerdings ist der Glaskolben mit einem Leuchtstoff beschichtet, der die UV-Linien des Quecksilbers in sichtbares Licht umwandelt. Auch die so genannten Energiesparlampen gehören zu den Leuchtstofflampen. Die Wahl des Leuchtstoffs beeinflusst den Farbton der Leuchtstofflampen. Lampen mit der Bezeichnung WW (warmweiß) setzen einen rötlich-warmen Akzent, TW (tageslichtweiß) einen bläulich-kalten.

Halogenmetalldampflampen enthalten neben Metallen zusätzlich Halogene in der Füllung und weisen gegenüber Quecksilberdampf-Hochdrucklampen ein besonders farbtreues (weißes) Lichtspektrum auf. Sie werden u. a. zur Beleuchtung von Geschäftsauslagen und in Tageslichtfilmscheinwerfern (im Englischen als "hydrargyrum medium-arc iodide" bezeichnet, HMI) eingesetzt.

Metalldampflampen können meist nicht direkt mit Netzspannung betrieben werden, sondern benötigen ein an die Leistung der Lampe angepasstes Vorschaltgerät.



</doc>
<doc id="9505" url="https://de.wikipedia.org/wiki?curid=9505" title="Kondensor">
Kondensor

Ein Kondensor besteht aus ein oder zwei Sammellinsen und wird in der Beleuchtungseinrichtung eines abbildenden optischen Gerätes verwendet. Solche mit Lampenlicht arbeitende Geräte sind Projektoren (Diaprojektor oder Vergrößerungsgerät) oder Mikroskope. Der Kondensor hat die Aufgabe, einen möglichst großen Teil des Lichts der Projektionslampe in den abbildenden Strahlengang einzubringen. Dabei ist auch darauf zu achten, dass das Bild gleichmäßig ausgeleuchtet wird.

Oft ist mit Kondensor schon das gesamte Beleuchtungssystem gemeint.

Der Kondensor bildet die Wendel der Lampe in die Öffnung des abbildenden Objektivs ab, sodass anstatt der Wendel deren vergrößertes Bild zur Eintrittspupille des Objektivs wird und eine größere Bildhelligkeit entsteht.

Er wird dicht vor dem abzubildenden Objekt angebracht. Damit bleibt er eine Feldlinse (speziell, da Feldlinsen in der Regel am Ort eines Zwischenbildes angeordnet werden), die die Abbildung des Objektes nicht beeinflusst, aber die Bildhelligkeit verbessert.

Das Beleuchtungssystem besteht aus Kondensorlinse/n und Lampe und in der Regel noch aus einem Kugelspiegel, wodurch die an der Projektion beteiligte Lichtmenge etwa verdoppelt wird. Dazu gehört oft noch ein Wärmeschutzfilter, das sich bei Zweiteiligkeit des Kondensors zwischen den beiden Linsen befindet.

Um Baulänge zu sparen und mehr Licht der Lampe zu nutzen, wird der Kondensor häufig mit großem Öffnungswinkel (große numerische Apertur) gebaut. Das bedingt sogenannte Öffnungsfehler (sphärische Aberration), die zum Beispiel zu ungleichmäßiger Ausleuchtung des Bildes führen können. Man verbessert die Situation, indem man die abbildende Funktion des Kondensors auf mehrere entsprechend korrigierte Linsen verteilt. Der Doppel-Kondensor enthält dann eine asphärische und eine sphärische Sammellinse.


</doc>
<doc id="9506" url="https://de.wikipedia.org/wiki?curid=9506" title="Interferenzfilter">
Interferenzfilter

Interferenzfilter und Interferenzspiegel sind optische Bauelemente, die den Effekt der Interferenz nutzen, um Licht frequenzabhängig, d. h. farbabhängig für sichtbares Licht, zu filtern oder zu spiegeln. Die Bezeichnung des Bauelements als Filter oder Spiegel hängt davon ab, ob man das transmittierte oder reflektierte Licht nutzt. Zumeist sind diese Bauelemente als dielektrische, dünne Schichten auf einem Träger aufgebaut (z. B. Bragg-Spiegel). Daneben gibt es auch Bauelemente in Form von Fabry-Perot-Interferometern.

Ein solcher Filter bzw. Spiegel hat für Licht unterschiedlicher Wellenlängen, unterschiedlichen Einfallswinkel und teilweise unterschiedlicher Polarisation einen verschiedenen Reflexions- und Transmissionsgrad.

Die wesentlichen Eigenschaften sind:
jeweils mit der Wellenlänge formula_6, dem Einfallswinkel AOI (engl. ) und dem Polarisationszustand des einfallenden Lichts formula_7.
Bei Transmissions- und Reflexionsgrad unterscheidet man zwischen dem gerichteten und dem diffusen Anteil. Für die spektrale Beschreibung wird der gerichtete Anteil zugrundegelegt, während die Summe der diffusen Anteile die spektrale Streuung ergeben.
In besonderen Fällen spielt die Änderung der Phasenbeziehung zwischen s- und p-polarisiertem Anteil der einfallenden Strahlung durch das Bauteil eine Rolle.

Die Unterteilung von Interferenzfiltern- und Spiegeln kann hinsichtlich der genutzten Materialien als auch hinsichtlich der spektralen Eigenschaften erfolgen.
Hinsichtlich der Materialwahl gibt es im Wesentlichen zwei unterschiedliche Formen. Filter und Spiegel der ersten Gruppe nutzen teildurchlässige, das heißt sehr dünne, metallische Schichten (meist zwei Schichten, die durch eine Abstandsschicht getrennt werden, ähnlich einem Fabry-Pérot-Interferometer). Die zweite Gruppe basiert auf der Interferenz in einem Stapel aus meist mehreren dielektrischen Schichten unterschiedlicher Materialien.

Hinsichtlich ihrer spektralen Eigenschaften unterscheidet man zwischen folgenden Filtern:

Filter, die einen unterschiedlichen Transmissions- oder Reflexionsgrad für zwei Wellenlängenbereiche aufweisen werden auch als "dichroitische" Interferenzfilter bzw. -Spiegel bezeichnet. Bauelemente für drei Wellenlängenbereiche werden "trichroitische" bezeichnet.

Interferenzfilter können sowohl schmalbandige Filter, sogenannte Linienfilter, als auch breitbandige Bandfilter sein.

Im klassischen Sinne sind Interferenzfilter- und Spiegel "nicht" durchstimmbare Fabry-Pérot-Interferometer und bestehen beispielsweise aus einer dicken Trägerschicht (Glas), auf die eine teildurchlässige metallische Spiegelschicht (z. B.: Silber, Aluminium) aufgedampft wird, gefolgt von einer dünnen dielektrischen, transparenten Schicht und einer zweiten Spiegelschicht (Mehrfachinterferenzfilter). Durch die Schichtdicke formula_8 der dielektrischen Schicht legt man fest, welche Wellenlängen gefiltert werden. Der Transmissionsgrad der Spiegelschichten beeinflusst die Güte des Bauteils (bei dünnen Spiegelschichten ist das Maximum des transmittierten Frequenzbandes breit und dessen Intensität hoch; daraus folgt eine geringe Güte des Filters).

Daneben gibt es zunehmend aufwändigere Interferenzfilter, die allein aus dielektrischen (nichtmetallischen) Schichten auf einem transparenten Substrat aufgebaut sind, sogenannte dielektrische Filter. Dabei wechseln sich in der Regel auf einen Trägersubstrat Schichten aus zwei transparenten Materialien mit unterschiedlichem Brechungsindex ab, wobei eine unterschiedliche Dicke von Schicht zu Schicht erforderlich sein kann. Es gibt auch Fälle, in denen mehr als zwei Materialien zum Einsatz kommen. Die Dicken der einzelnen Schichten liegen dabei zwischen etwa zehn und etwa tausend Nanometern. Die Untergrenze der Schichtdicke ist in der Regel durch die Steuerbarkeit des Herstellungsprozesses bestimmt. Die Obergrenze hängt vom Wellenlängenbereich ab, in dem der Filter eingesetzt werden soll (je größer die Wellenlänge, desto dickere Schichten können erforderlich sein). Die Anzahl der Schichten kann je nach Anforderungen an den Filter zwischen einigen wenigen und mehreren hundert liegen. Der Entwurf solcher Schichtfolgen geschieht heute mit aufwändigen Simulationsprogrammen, die als Eingabedaten die optischen Eigenschaften (Brechungsindex und Absorption in Abhängigkeit von der Wellenlänge, Dispersion) der zu verwendenden Materialien sowie das gewünschte Transmissions- bzw. Reflexionsspektrum, ggf. in Abhängigkeit vom Einfallswinkel, benötigen. Ausgegeben wird das simulierte Transmissions- bzw. Reflexionsspektrum, ggf. winkelabhängig, sowie die Schichtenfolge. Es gibt Schnittstellen, mit denen eine entsprechende Beschichtungsanlage direkt angesteuert werden kann. Die Berechnung erfolgt in einem Iterationsverfahren und kann je nach Komplexität Sekunden bis mehrere Stunden erfordern (Stand 2015). Auf diese Weise können selbst Filter mit komplizierten Anforderungen, z. B. Mehrband-Filter, entworfen und hergestellt werden.

Schichtsysteme für Interferenzfilter werden heute in der Regel durch Kathodenzerstäubung (Sputtern) hergestellt, besonders, wenn es sich um aufwändige Filter mit hohen Genauigkeitsanforderungen handelt. Einfachere Filter, z. B. für Entspiegelungen, werden auch durch Aufdampfen von Schichten hergestellt. Dank besserer Steuerungsmöglichkeiten, sowohl von der prozessanalytischen Seite als auch von der Anlagenseite her, werden aber auch zunehmend komplexere Filter durch Bedampfung herstellbar.

Um die Funktionsweise eines Interferenzfilters bzw. -Spiegels zu erklären, wird im Folgenden ein einfaches System einer dünnen, dielektrischen Schicht auf einem Substrat beschrieben.

Tritt ein „Lichtstrahl“ in das Bauteil, so wird der Lichtstrahl gemäß den an jeder (optischen) Grenzfläche teilweise transmittiert (T, T, …) und reflektiert (R, R, …). Es findet eine Aufspaltung der auf die Oberfläche treffenden Strahlen statt. Die transmittierten, gebrochenen Strahlen werden wiederum teilweise an der Unterseite der Schicht reflektiert und treffen wiederum auf die Oberfläche. Bei der dort stattfindenden Reflexion verlässt nach erneuter Brechung ein Teil der Strahlen (R) die dünne Schicht, der andere Teil wird reflektiert und erfährt im weiteren Verlauf in der Schicht Mehrfachreflexionen. Dies führt zu vielen parallel austretenden Strahlen gleicher Frequenz auf beiden Seiten des Bauteils.

Der Interferenz an dünnen Schichten geht eine Strahlteilung voraus. Daher wird sie auch als Amplitudenteilung bezeichnet; im Gegensatz zur Interferenz durch Beugung wie beim Doppelspaltversuch, bei der von Wellenfrontteilung gesprochen wird.

Um die Funktionsweise einfacher zu verdeutlichen, wird zunächst schwache Reflexion vorausgesetzt, d. h., die Mehrfachreflexionen werden vernachlässigt. Es genügt die Interferenz von zwei Teilwellen zu betrachten, beispielsweise R und R. Die beiden Parallelstrahlen werden nun durch eine Sammellinse (beispielsweise das Auge) zur Interferenz gebracht. Durch die unterschiedlichen Weglängen der Wellen in der dünnen Schicht, weisen sie nach der Reflexion einen Gangunterschied formula_9 auf.
wobei formula_8 die Schichtdicke, formula_12 den Brechungsindex der dünnen Schicht und formula_13 der durch die Reflexionen eventuell zusätzlich erzeugte Gangunterschied ist.

Durch den Gangunterschied kommt es zur Auslöschung "(destruktive Interferenz)" oder Verstärkung "(konstruktive Interferenz)" von Strahlen bestimmter Wellenlängen. Auslöschung und Verstärkung bestimmter Wellenlängen sind abhängig von der gewählten Schichtdicke formula_8 des Filters und vom Einfallswinkel formula_15 der Strahlen.

Damit es zur vollständigen konstruktiven und/oder destruktiven Interferenz kommen kann, müssen folgende Bedingungen erfüllt werden:

Im Folgenden wird eine Reihe von Filtern aufgelistet, deren Wirkung auf Interferenzeffekten beruht:

Neben den beschriebenen Interferenzfiltern gibt es noch weitere optische Bauelemente, bei denen Interferenzen genutzt bzw. beobachtet werden. Dazu zählt unter anderem die Lummer-Gehrcke-Platte bei der Licht mehrfach in einer planparallelen Platte reflektiert (nahe dem Grenzwinkel der Totalreflexion) wird, dabei streifend austritt und interferiert.

Dichroitische Spiegel werden zum Beispiel in größeren Videokameras (Drei-CCD-Kameras) eingesetzt, um das einfallende Licht in den RGB-Farbraum aufzuspalten, wozu zwei solcher Spiegel mit Reflexion in verschiedenen Wellenlängen-Bereichen gebraucht werden (siehe auch CCD-Sensor).

Die früher übliche Einteilung in Filter und dichroitische Spiegel ist heute oft nicht mehr sinnvoll, weil Interferenzfilter häufig in beiden Funktionen gleichzeitig eingesetzt werden. Weil das Licht - auch im verallgemeinerten Sinne, was auch UV- und Infrarotlicht einschließt - derjenigen Wellenlängen, die nicht transmittiert werden (durch den Filter hindurchgehen), reflektiert wird, ist mit Interferenzfiltern eine Aufteilung eines Lichtstrahls in zwei Strahlen mit komplementären Wellenlängenbereichen möglich. Für den einen (hindurchtretenden) Strahl wirkt der Interferenzfilter als Filter, für den anderen, reflektierten Strahl hingegen als Spiegel.

Ein heute sehr bedeutendes Einsatzgebiet von Interferenzfiltern ist die digitale Projektionstechnik. Hier werden diese Filter sowohl zur Auftrennung des Lichts in verschiedene Farben als auch zur Vereinigung von Bildern in den drei Primärfarben zu einem vollfarbigen Bild eingesetzt.

In einfachen Projektoren erfolgt die Darstellung verschiedener Farben in schneller Folge nacheinander (sequenziell). Dazu befindet sich im Strahlengang von der Lichtquelle zum Bildgeber ein schnell rotierendes Filterrad, das Segmente aus verschiedenen Interferenzfiltern trägt. Mindestens erforderlich sind die drei Primärfarben für die additive Farbmischung, Rot, Grün und Blau. Häufig enthalten solche Filterräder aber zusätzliche Segmente mit Transmissionsbereichen für Cyan, Gelb und Weiß, um die Helligkeit auf Kosten der Farbsättigung zu erhöhen. Synchron mit dem Farbwechsel durch das Filterrad erfolgt die Umschaltung der Teilbilder für die betreffenden Farben im Bildgeber (Imager). Durch die sequentielle Farbdarstellung kann es bei schnellen Bewegungen zum sog. Regenbogeneffekt kommen, bei dem die Kanten zwischen hellen und dunklen Objekten im Bild farbige Ränder zu bekommen scheinen. Ein weiterer Nachteil dieser Technik ist die schlechte Ausnutzung des Lichts, weil diejenigen Wellenlängen, die das Filterrad nicht passieren können, verworfen werden müssen. Dies wird versucht dadurch auszugleichen, dass die Helligkeit heller Bildteile durch zusätzliches Licht, das die Sekundärfarben- oder Weißsegmente des Filterrades passiert, aufgehellt wird. Dies führt zu einer insgesamt höheren Bildhelligkeit, verfälscht aber die Farbsättigung.

Qualitativ bessere, aber auch aufwändigere Projektoren nutzen das Licht der Lichtquelle besser aus, indem es vor den eigentlichen Bildgebern in drei Strahlen der Primärfarben Rot, Grün und Blau aufgeteilt wird, die gleichzeitig (parallel) genutzt werden. Dies geschieht mit Interferenzfiltern, die gleichzeitig als dichroitische Spiegel eingesetzt werden. So wird häufig zunächst einer dieser Spiegel verwendet, um das (kurzwellige) blaue Licht hindurch zu lassen, während das übrige Licht, das nun gelb erscheint, in einem Winkel von 90° reflektiert wird. Dieses wird nochmals auf einen dichroitischen Spiegel geleitet, der selektiv das grüne Licht (mittlere Wellenlänge) im Winkel von 90° reflektiert und nur noch den roten Anteil (große Wellenlänge) hindurchlässt. Die drei Strahlen werden, ggf. über weitere, normale Spiegel, zu drei separaten Bildgebern gelenkt, die drei Teilbilder in den drei Primärfarben erzeugen. Diese werden abschließend abermals durch Interferenzfilter wieder zu einem gemeinsamen, vollfarbigen Bild vereinigt. Letztere Interferenzfilter befinden sich dabei häufig auf den Diagonalflächen eines Glaswürfels, der aus vier Prismen mit dreieckiger Grundfläche zusammengesetzt ist. Gelegentlich befinden sich weitere Interferenzfilter im Strahlengang (sog. Cinema- oder Yellow-Notch-Filter) oder direkt vor den Bildgebern (sog. Trimming-Filter), die das Lichtspektrum nochmals beschneiden, um die erzielbare Farbsättigung zu erhöhen. Durch die Verwendung von drei Bildgebern und einem Prismenwürfel ist diese Technik aufwändiger und teurer, liefert aber auch eine verbesserte Bildqualität bei besserer Lichtausbeute.

Auch in einem Verfahren zur stereoskopischen 3D-Projektion werden Interferenzfilter eingesetzt, weswegen dieses Verfahren auch als Interferenzfiltertechnik bezeichnet wird.

Dichroitische Spiegel werden in der Fluoreszenzmikroskopie eingesetzt, um das aus einem Epifluoreszenzkondensor einfallende anregende Licht in den Strahlengang des Objektivs einkoppeln zu können, ohne den Durchtritt der Fluoreszenzemission zu behindern. Zur Beobachtung von mehrfach gefärbten Proben können auch polychroitische Spiegel zum Einsatz kommen, die mehrere reflektierende bzw. transmittierende Spektralbereiche haben.

Neben der Anwendung als spektral selektierende Strahlteiler können dichroitische Spiegel auch als Strahlvereiniger genutzt werden, um z. B. mehrere Laser mit verschiedenen Wellenlängen in einen gemeinsamen Strahlengang einzukoppeln (siehe Diodenlaser).

Dichroitische Spiegel spiegeln im Gegensatz zur Reflexion an Metalloberflächen das Licht einer Wellenlänge sehr verlustarm und werden daher gerne in der Lasertechnik verwendet. Wegen der verlustarmen Reflexion wird bei intensiven Laserstrahlen weniger Leistung im Spiegel deponiert; dichroitische Spiegel sind daher auch bei sehr hohen Laserleistungen, bei denen Metallspiegel beschädigt würden, verwendbar.

Bei einem dichroitischen dielektrischen Spiegel für Laseranwendungen lässt sich der Reflexionsgrad in Abhängigkeit von der Wellenlänge durch geeignete Wahl von Schichtzahl, Dicke und Brechungsindex der verwendeten Dielektrika nahezu beliebig und sehr exakt einstellen, was zur wellenlängenabhängigen Kopplung von Laserstrahlen ein unerlässliches Hilfsmittel darstellt.

Für Reflexvisiere werden dichroitische Spiegel verwendet, um das rote Laserlicht des Zielpunktes in das Auge des Schützen zu projizieren.

Kaltlicht- und Wärmespiegel sind spezielle, in ihrer Wirkung entgegengesetzte, dichroitische Spiegel. Ein "Wärmespiegel" (engl. ) zeichnet sich durch einen hohen Transmissionsgrad im sichtbaren und einen hohen Reflexionsgrad (geringer Transmissionsgrad) im infraroten Bereich aus. Ein "Kaltlichtspiegel" (engl. ) wirkt hingegen genau entgegengesetzt, er reflektiert sichtbares Licht gut und lässt infrarotes Licht (Wärmestrahlung) passieren, z. B. für die Verwendung in Kaltlichtspiegellampen. Die Infrarotstrahlung, d. h. die Wärmestrahlung der Lampe tritt durch den Reflektor durch und es kommt zu einer geringeren Erwärmung des beleuchteten Objekts als bei metallischen Reflektoren. Diese Art von Lichtquellen nennt man auch Kaltlichtquelle.


Zur Spezifikation von optischen Interferenzfiltern gibt es die ISO-Norm ISO 9211 (Optik und Photonik – Optische Schichten). Dieser besteht aus den Teilen
Die Beschreibung der Filtereigenschaften von Brillengläsern ist in der gesonderten Norm EN ISO 13666 Augenoptik – Brillengläser – Vokabular (ISO 13666:1998) genormt. Die Norm ist in Deutschland als DIN-Norm DIN EN ISO 13666 gültig.



</doc>
<doc id="9508" url="https://de.wikipedia.org/wiki?curid=9508" title="Oktoberfest">
Oktoberfest

Das Oktoberfest in München (mundartlich "d’Wiesn") ist das größte Volksfest der Welt.

Es findet seit 1810 auf der Theresienwiese in der bayerischen Landeshauptstadt München statt. Veranstalter ist die Stadtverwaltung München (Referat für Arbeit und Wirtschaft). Für das Oktoberfest brauen einige Münchner Brauereien ein spezielles Bier, das eine Stammwürze von mindestens 13,5 % aufweisen muss und ca. 5,8 bis 6,4 Volumenprozent Alkohol enthält. Das Oktoberfest generiert in den zwei Wochen jährlich durchschnittlich eine Milliarde Euro Umsatz.

Vom 16. September 2017 bis 3. Oktober 2017 fand das 184. Oktoberfest statt, die Veranstaltung hatte etwa 6,2 Millionen Besucher.

Anlässlich der Hochzeit zwischen Kronprinz Ludwig und Prinzessin Therese am 12. Oktober 1810 fanden in München zahlreiche private und öffentliche Feiern statt. Auf deren letzte, das Pferderennen am 17. Oktober, geht das Oktoberfest zurück. Vermutlich im Gedanken an das 1786 letztmals ausgetragene Scharlachrennen, das im 15. Jahrhundert erstmals vor dem Karlstor stattfand und Teil der Jakobidult wurde, schlug Andreas Michael Dall’Armi in seiner Funktion als Major der Nationalgarde ein Pferderennen mit Ausstellung und Schau zur öffentlichen Huldigung des Brautpaares vor. Überliefert ist, dass die initiale Idee, die zu diesem Vorhaben führte, von dem Lohnkutscher und Unteroffizier der Nationalgarde Franz Baumgartner ausging. Dieser Ursprung des Festes gilt jedoch als umstritten.

Der Festplatz außerhalb der Stadt wurde aufgrund seiner natürlichen Eignung ausgesucht. Der Sendlinger Berg (heute Theresienhöhe) diente als Tribüne für die 40 000 Zuschauer des Rennens. Die Festwiese blieb bis auf das Königszelt unbebaut. Die Verköstigung der Besucher erfolgte oberhalb der Tribüne auf der Anhöhe, wo „Traiteurs“ u. a. Wein und Bier anboten. Bevor das Rennen begann, erfolgte eine Huldigung der Hochzeiter und des Königshauses in Form eines Zuges aus 16 Kinderpaaren, die mit Trachten der Wittelsbacher, der neun bayerischen Kreise sowie weiterer Regionen bekleidet waren. Anschließend sang ein Chor aus Feiertagsschülern, bevor schließlich das Festrennen mit 30 Pferden auf einer 11 200 Schuh (3270 Meter) langen Rennbahn folgte. Als Sieger ging das Pferd des möglichen Initiators Franz Baumgartner über die Ziellinie, der seine Goldmedaille von Rennmeister und Staatsminister Maximilian Graf von Montgelas überreicht bekam.

1813 fiel das Fest aus, da Bayern in die napoleonischen Kriege verwickelt war. Danach wuchs das Fest von Jahr zu Jahr. Zur Pferderennbahn kamen Kletterbäume, Kegelbahnen und Schaukeln hinzu. 1818 wurde das erste Karussell aufgestellt. Mehrere Losstände zogen vor allem die ärmeren Stadtbewohner an, da es Porzellan, Silber und Schmuck zu gewinnen gab. 1819 übernahmen die Münchner Stadtväter die Festleitung. Von nun an sollte das Oktoberfest planmäßig jedes Jahr gefeiert werden.

Seit 1850 ragt die knapp 20 Meter hohe Statue der Bavaria über die Festwiese. 1853 wurde die Ruhmeshalle zu Füßen der Bavaria fertiggestellt. In den folgenden Jahren fielen einige Feste aus. Grund dafür waren zwei Cholera-Epidemien in den Jahren 1854 und 1873, der Preußisch-Österreichische Krieg 1866 und der Deutsch-Französische Krieg 1870.

Gegen Ende des 19. Jahrhunderts entwickelte sich das Oktoberfest immer mehr zu dem heute in aller Welt bekannten Volksfest. Es wurde zeitlich verlängert und in die wegen des Altweibersommers zumeist schönen und warmen letzten Septembertage vorverlegt. Seitdem fällt nur das letzte Wiesnwochenende in den Oktober. Von 1880 an genehmigte die Stadtverwaltung den Bierverkauf und 1881 eröffnete die erste "Hendlbraterei." Um mehr Sitzplätze für Besucher und Raum für Musikkapellen zu schaffen, errichteten die Brauereien an Stelle der Bierbuden schließlich große Bierhallen. Gleichzeitig zog das Fest immer mehr Schausteller und Karussellbesitzer an, die für zusätzliche Unterhaltung sorgten. 1885 wurden die Außenbereiche des Festgeländes erstmals elektrisch beleuchtet. Die dabei verwendeten 16 Bogenlampen wurden von der Elektrotechnischen Fabrik J. Einstein & Cie installiert und galten damals als eine Sensation. 1892 folgte das erste elektrisch betriebene Fahrgeschäft und 1901 wurden die ersten Festzelte elektrisch beleuchtet.

1904 stellten Post und Telegraphenamt (vermutlich erstmals) öffentliche Telefone auf:

1910 feierte die Wiesn ihren 100. Geburtstag und es wurden 12.000 Hektoliter Bier ausgeschenkt. In der Bräurosl, dem damals größten Bierzelt, fanden bereits 12 000 Gäste Platz. Heute ist die Hofbräu-Festhalle mit 10 000 Plätzen das größte Bierzelt auf der Wiesn.

Von 1914 bis 1918 fiel das Fest wegen des Ersten Weltkriegs aus.
1919/1920 feierte man nur ein kleines „Herbstfest“; 1923 gab es wegen der galoppierenden Inflation kein Oktoberfest. Im November 1923 wurde die Rentenmark eingeführt; 1924 gab es kein Oktoberfest.

Während der Zeit des Nationalsozialismus nutzte die NS-Propaganda das Oktoberfest. 1933 wurde der Preis für die Maß Bier auf 90 Pfennig festgelegt. Es wurde Juden verboten, auf dem Oktoberfest zu arbeiten. 1935 wurde das 125. Jubiläum pompös inszeniert; unter anderem mit einem großen Jubiläumsumzug (Motto: „Stolze Stadt – Fröhlich Land“ sollte für die angebliche Überwindung der Schichten und Klassen stehen; er demonstrierte die Gleichschaltung und die gefestigte Macht des NS-Regimes). Die künstlerische Leitung für den Jubiläumsumzug übertrug die Gauleitung dem NS-Maler Albert Reich. 1938 – im März hatte Hitler Österreich annektiert und auf der Münchner Konferenz in der Sudetenfrage auf ganzer Linie gewonnen – wurde das Oktoberfest in „Großdeutsches Volksfest“ umbenannt. Das NS-Regime transportierte eine große Zahl von Sudetendeutschen auf die Festwiese.

Während des Zweiten Weltkriegs von 1939 bis 1945 fand kein Fest statt. In der Nachkriegszeit 1946 bis 1948 gab es ein „Herbstfest“. Seit seinem Bestehen war das Oktoberfest damit 24-mal ausgefallen. Im September 1949 fand das erste Oktoberfest nach dem Krieg statt.

1950 wurde die Veranstaltung von Thomas Wimmer (Oberbürgermeister von 1948 bis 1960) zum ersten Mal mit dem inzwischen traditionellen Fassanstich im Festzelt Schottenhamel eröffnet. Im Lauf der folgenden Jahrzehnte entwickelte sich das Oktoberfest zum größten Volksfest der Welt. Das Pferderennen wurde nach dem Krieg mit Ausnahme des 150. Jubiläums 1960 und des 200. Jubiläums 2010 nicht mehr veranstaltet.

Am Abend des 26. September 1980 explodierte am Haupteingang des Festgeländes (Öffentliche Bedürfnisanstalt am Bavariaring) eine Bombe. 13 Menschen starben, über 200 wurden verletzt, 68 davon schwer. Das Attentat gilt als einer der schwersten Terroranschläge der deutschen Geschichte. Die Tat ist bis heute nicht vollständig aufgeklärt. Der Generalbundesanwalt beim Bundesgerichtshof hat das Verfahren im Dezember 2014 wieder aufgenommen.

Das Oktoberfest zieht in den letzten Jahrzehnten alljährlich Millionen Besucher an. Die Gäste kommen immer zahlreicher aus dem Ausland, vorwiegend aus Italien, aus den USA, Japan und Australien. In den letzten Jahren gibt es einen Trend zur Tracht: viele Wiesnbesucher kommen in Lederhosen bzw. Dirndl.

Der übermäßige Alkoholkonsum eines Teils der Wiesnbesucher wird seit langem thematisiert. Um zu vermeiden, dass die Stimmung auf der Wiesn immer mehr der Stimmung auf dem Ballermann (Mallorca) gleicht, entwickelten 2005 die verantwortlichen Organisatoren das Konzept der "Ruhigen Wiesn." Die Zeltbetreiber sind dazu angehalten, bis 18 Uhr nur traditionelle Blasmusik zu spielen und die Musiklautstärke auf 85 dB(A) SPL zu begrenzen. Dies soll das Oktoberfest für Familien und ältere Besucher attraktiv halten. Nach 18 Uhr werden auch Schlager und Popmusik gespielt.

Von einem Teil der Münchner und der Gäste sowie teilweise in der Presse wird das Fest inzwischen allerdings als nerviges Massenbesäufnis bezeichnet, das mit einem Volksfest nichts zu tun hat.

Anlässlich des 200-jährigen Jubiläums wurde 2010 erstmals am Südende der Theresienwiese ein historisches Oktoberfest („Oide Wiesn“) auf dem Areal des zentralen Landwirtschaftsfestes gestaltet. Es eröffnete einen Tag vor Beginn des eigentlichen Oktoberfestes gleichfalls mit dem Fassanstich durch den Oberbürgermeister. Auf dem fünf Hektar umfassenden abgezäunten Gelände präsentierten sich historische Fahrgeschäfte, Festzelte und andere historische Attraktionen wie beispielsweise eine Steckerlfischbraterei, ein Kettenkarussell oder ein Zuckerwattestand. Gegen Eintritt können neben dem Museums- ein Tierzelt sowie die Pferderennbahn besichtigt werden. Das Tierzelt beinhaltet unter anderem einen Streichelzoo und wird vom Tierpark Hellabrunn sowie dem Bayerischen Bauernverband betreut. Das Münchner Stadtmuseum übernahm die Gestaltung des Museumszeltes. Begleitet wurde die Jubiläumswiesn von einem künstlerischen und kulturellen Rahmenprogramm, in dem beispielsweise die Biermösl Blosn auftrat. Die Musikkapellen im – mit 850 Sitzplätzen vergleichsweise kleinen – "Herzkasperl Festzelt" kamen ohne Lautsprecheranlage aus. Namensgebend für das Festzelt war eine bekannte Bühnenfigur des 2009 verstorbenen Schauspielers Jörg Hube. Die sechs Münchner Brauereien Augustiner, Hacker-Pschorr, Hofbräu, Löwenbräu, Paulaner und Spaten präsentierten ausschließlich hier ein gemeinsam gebrautes, dunkles Spezialbier, das nach einer historischen Rezeptur vom Beginn des 19. Jahrhunderts hergestellt wurde. Die Maßkrüge in den Festzelten trugen demgemäß die Aufschrift Münchner Bier und nicht das Unternehmenslogo einer einzelnen Brauerei. Im Gegensatz zum restlichen Oktoberfest schloss die „Oide Wiesn“ bereits um 20 Uhr. Statt der von der Stadtverwaltung erwarteten 300 000 Gäste kamen weit mehr als eine halbe Million Besucher. Das Gelände musste sogar mehrfach, wie sonst nur Bierzelte, wegen Überfüllung vorübergehend geschlossen werden.

In den darauffolgenden Jahren bestand das Traditionsoktoberfest nach dem Konzept des damaligen Wirtschaftsreferenten und heutigen Oberbürgermeisters der Stadt München Dieter Reiter unter der Bezeichnung „Oide Wiesn“ weiter, allerdings in reduzierter Form. Es wird seitdem beispielsweise auf die Pferderennbahn verzichtet. Der Eintrittspreis wurde auf drei Euro vermindert. Das historische Festzelt wird um 2000 Plätze vergrößert und die altertümlichen Fahrgeschäfte bleiben erhalten. Die "Oide Wiesn" wird in Zukunft zur Dauereinrichtung werden. Alle vier Jahre, während des "Bayerischen Zentral-Landwirtschaftsfestes", zuletzt 2016, konnte sie aus Platzgründen jedoch nicht aufgebaut werden.

Laut Stadtratsbeschluss vom 16. Oktober 2012 betrug der Eintritt für die „Oide Wiesn“ 2013 wieder drei Euro. Erstmals war mit diesem Ticket ein Wiedereinlass möglich. Die historischen Fahrgeschäfte verlangten 2013 einen Euro Eintritt. Im Musikantenzelt erhöhte sich im Innenbereich die Anzahl der Sitzplätze von 1000 auf 1500; im Außenbereich stieg sie von 800 auf 1000. Die Stadt München unterstützte die Schaustellerstiftung mit 200 000 Euro, damit diese das Museumszelt, das Velodrom sowie ein Kinderprogramm betrieb.

2013 vergrößerte sich das Gelände und schöne Eingänge kamen hinzu. Außerdem war das Museumszelt des Stadtmuseums München/Schaustellerstiftung wie im Jubiläumsjahr 2010 wieder vertreten (n.n. 2011). Das Velodrom wurde vergrößert und den Zuschlag für das Musikantenzelt hatte Fraunhofer-Wirt Beppi Bachmaier erhalten und das Zelt unter der Bezeichnung „Herzkasperlzelt“ wie 2010 betrieben. Bei der Oiden Wiesn 2011 betrieb noch die Familie Reichert unter dem Namen „Zur Schönheitskönigin“ das Musikantenzelt.

Das Oktoberfest wird im Bairischen meist "Wiesn" (von „Theresienwiese“) genannt. Das Genus von "Wiesn" ist weiblich "(die Wiesn", im Bairischen "d’Wiesn)," der Numerus ist Singular. Da es sich bei "Wiesn" um die bairische Form von "Wiese" (Singular) und nicht von "Wiesen" (Plural) handelt, ist ein Apostroph bei der Schreibung falsch ("Wies’n" suggeriert das Pluralwort "Wiesen" mit ausgelassenem e). Das „d'“ hingegen ist die Verkürzung von „de“, bairisch für „die“, und somit der Apostroph richtig.

Der Begriff wird außerdem zum Teil für die Theresienwiese selbst verwendet; man kann also außerhalb der Oktoberfestzeit auf die Wiesn gehen. Während des Oktoberfestes wird die Theresienwiese im offiziellen Sprachgebrauch als "Festwiese" bezeichnet, so beispielsweise auf den eigens für das Fest montierten Hinweisschildern der öffentlichen Verkehrsmittel.

Die Geschichte des Einzugs der Wiesnwirte und Brauereien als feierlicher Auftakt des Oktoberfestes geht zurück auf das Jahr 1887, als der damalige Wirt Hans Steyrer erstmals von seiner Wirtschaft in der Tegernseer Landstraße samt Personal, Blaskapelle und einer Bierladung auf die Theresienwiese zog. In seiner heutigen Form findet der Einzug im Wesentlichen seit 1935 statt, als erstmals alle Brauereien gemeinsam am Umzug teilnahmen. Seitdem wird der Zug von einer Frau in gelb-schwarzer Mönchskutte, die das Münchner Kindl darstellt, angeführt. Seit 1950 folgt ihm der amtierende Münchner Oberbürgermeister in der Kutsche der Familie Schottenhamel. Ihnen folgen wiederum die prachtvoll geschmückten Pferdegespanne und Festwagen der Brauereien sowie die Kutschen der anderen Wirte und Schausteller. Begleitet wird der Zug von den Musikkapellen der Festzelte.

Nach dem Einzug der Wiesnwirte auf Kutschen von der Innenstadt zur Festwiese sticht um 12 Uhr der Oberbürgermeister im Schottenhamel-Festzelt das erste Bierfass an. Mit dem Anstich und dem traditionellen Ruf: O’zapft is! Auf eine friedliche Wiesn („Es ist angezapft! Auf ein friedliches Oktoberfest“), gilt das Oktoberfest als eröffnet. Anschließend werden auf der Treppe der Bavaria zwölf Böllerschüsse abgegeben. Dies ist das Zeichen für die anderen Wirte mit dem Ausschank beginnen zu dürfen. Traditionell erhält der bayerische Ministerpräsident die erste Maß. Danach werden in den anderen Festzelten die ersten Fässer angezapft und Bier an die Wiesnbesucher ausgeschenkt.

Jedes Jahr warten viele mit Spannung darauf, wie viele Schläge der Bürgermeister tätigt, bis das erste Bier aus dem ersten Fass fließt. Die beste Leistung liegt bei zwei Schlägen (Ude, 2005, 2008, 2009, 2010, 2011, 2012 und 2013; Reiter, 2015, 2016 und 2017), es waren aber auch schon 17 oder 19 Schläge erforderlich (Wimmer, 1950), eine eindeutige Angabe gibt es nicht.

Zu Ehren der Silberhochzeit von König Ludwig I. von Bayern und Prinzessin Therese fand 1835 erstmals ein Trachtenumzug statt. 1895 organisierte der Heimatschriftsteller Maximilian Schmidt einen weiteren Umzug mit 1400 Teilnehmern in 150 Trachtengruppen. Seit 1950 wird dieser jährlich veranstaltet und ist mittlerweile einer der Höhepunkte des Oktoberfests und einer der weltgrößten Umzüge dieser Art. Am ersten Wiesn-Sonntag ziehen 8000 Teilnehmer in ihren historischen Festtagstrachten vom Maximilianeum aus auf einer sieben Kilometer langen Strecke bis zur Festwiese.

Auch dieser Umzug wird vom Münchner Kindl (heute eine Frau) angeführt; ihm folgen die Honoratioren des Stadtrates und der Stadtverwaltung und des Freistaates Bayern, meist der Ministerpräsident und dessen Gattin, Trachten- und Schützenvereine, Musikkapellen, Spielmannszüge, farbenprächtige Fahnenschwinger und etwa 40 Kutschen mit festlich geschmückten Pferdegespannen. Die Vereine und Gruppen kommen größtenteils aus Bayern, aber auch aus anderen deutschen Bundesländern, aus Österreich, aus der Schweiz, aus Norditalien und aus anderen europäischen Ländern. Der Einzug der Wiesnwirte und der Trachten- und Schützenzug wird vom Festring München ausgerichtet.

Dem Besucher stehen 14 große und 15 kleinere Festzelte zur Auswahl. Laut Betriebsvorschriften des Festes darf auf dem Oktoberfest nur Bier der Münchner Traditionsbrauereien ausgeschenkt werden. Derzeit sind die Münchner Brauereien Spatenbräu, Augustiner, Paulaner, Hacker-Pschorr, Hofbräu und Löwenbräu mit Festzelten vertreten. Da die Zelte in den letzten Jahren manchmal wegen Überfüllung geschlossen werden mussten, hat die Stadt München auf ihrer Website ein "Wiesnbarometer" eingerichtet, das die jeweilige Auslastung der Zelte vorhersagt.

Es gibt 144 Gastronomiebetriebe, davon bieten 37 Zelte in der Summe 119.000 Sitzplätze an.
Zusätzlich befindet sich auf der Oidn Wiesn zwei Festzelte („Tradition“ und „Herz-Kasperl Festzelt“) mit insgesamt 10.500 Plätzen.

Die in den Festzelten verwendeten Maßkrüge sind heute nicht mehr aus Ton, sondern aus Glas, um Schankbetrug zumindest zu erschweren. Sie sind Eigentum der jeweiligen Brauereien. Besonders in den 1980er- und 1990er-Jahren nahmen die Maßkrugdiebstähle stark überhand und das Sicherheitspersonal der Festzelte wurde angewiesen, nach Dieben Ausschau zu halten. Die Festwirtevereinigung stellt bereits seit Jahren generell Strafantrag gegen Maßkrugdiebe. Maßkrüge, die offiziell als Souvenir gekauft werden, sind zur einfacheren Unterscheidung mit einer farbigen Plakette markiert.

Seit einigen Jahren wird das Lied, das in den Bierzelten am häufigsten gespielt und mitgesungen wird, von der Boulevardpresse zum sogenannten "Wiesn-Hit" erklärt. Zahlreiche Printmedien und Radiosender fordern ihre Zielgruppen regelmäßig im zeitlichen Umfeld der Veranstaltung ebenfalls dazu auf, entsprechende Musikstücke zu wählen. Da vielen Wiesnbesuchern die Texte nicht immer geläufig sind, gibt es eine "Wiesn-Singfibel" mit den beliebtesten Liedern. Einige Webseiten und Internetportale bieten ebenfalls Listen der aktuellen Wiesn-Hits einschließlich der Liedtexte an, teilweise mit weiterführenden Hintergrundinformationen.

Wenngleich der Begriff als solches erst Mitte der 1980er Jahre aufkam, wird dieser Tage vielfach das Lied "Schützenliesel", komponiert von Gerhard Winkler mit Text von Fred Rauch und Fini Busch, mit seinen charakteristischen drei Paukenschlägen aus dem Jahr 1952 als der erste Wiesn-Hit, zumindest der Nachkriegszeit, angesehen.

Auf dem Oktoberfest gibt es an die 200 Schaustellerbetriebe, davon sind knapp 80 Fahrgeschäfte. Viele Schaustellerfamilien sind seit Beginn des 20. Jahrhunderts auf der Wiesn vertreten. Zu den beliebten nostalgischen Attraktionen kommen jedes Jahr neue Fahrbetriebe, die auf dem Stand modernster Technik für Nervenkitzel sorgen.

Das Riesenrad wurde erstmals 1880 aufgestellt und hatte zu diesem Zeitpunkt eine Höhe von 12 Metern. Seit 1979 zählt Willenborgs Riesenrad mit einer Höhe von 48 Metern zu den bekanntesten Attraktionen und bietet Fahrgästen einen Blick über die Festwiese. Der Schausteller ist auf der Wiesn seit den 1930er Jahren regelmäßig mit unterschiedlichsten Fahrgeschäften vertreten, seit 1960 erstmals mit einem stählernen Riesenrad.

Die Krinoline ist ein traditionelles Rundkarussell, das seit den 20er-Jahren auf dem Oktoberfest steht. Die runde Form und die schwankende Bewegung erinnern an eine Krinoline. Noch bis 1938 wurde das Karussell mit Muskelkraft bewegt. In jenem Jahr wurde für die "Original-Krinoline-Blaskapelle" an der Außenwand des Karussells ein kleiner Balkon angebaut. Das Fahrgeschäft wird in der Regel musikalisch durch diese Blaskapelle, aber auch durch ausgesuchte Gastkünstler begleitet.

Die Hexen- oder Illusionsschaukel ist ein historisches Fahrgeschäft, das in Amerika Ende des 19. Jahrhunderts erstmals von Harris Wheels präsentiert und auf dem Oktoberfest zum ersten Mal 1894 aufgestellt wurde: Zwei einander gegenüberliegende Sitzbänke für jeweils ca. acht Personen sind auf einer großen Schaukel angeordnet, wobei die Fahrgäste den Eindruck haben, dass sich diese Schaukel während der Fahrt mehrfach überschlägt. Als ausgewiesene Antiquität, die besonderer Achtsamkeit und Pflege bedarf, reist sie nicht mehr von Volksfest zu Volksfest, sondern ist nur noch auf dem Oktoberfest sowie auf dem Straubinger Gäubodenvolksfest vertreten.

Der Toboggan ist eine Turmrutschbahn, die erstmals 1906 aufgestellt wurde. Den jetzigen Toboggan gibt es seit 1933 auf dem Oktoberfest. Der Begriff entstammt der Sprache der kanadischen Algonkin-Indianer und bezeichnet einen leichten Schneeschlitten. Mittels eines schnell laufenden Förderbandes werden die Fahrgäste auf etwa acht Meter Höhe transportiert. Von der Turmspitze rutscht man mit hoher Geschwindigkeit in einer sich um den Turm windenden Holzrinne wieder nach unten. Der besondere Reiz für die Zuschauer sind die Versuche der Fahrgäste, das Förderband zu betreten. Bei diesem bewegt sich, anders als bei einer Rolltreppe, der Handlauf nicht mit und wer sich festhält, dem zieht es unweigerlich die Füße weg.

Das Teufelsrad wurde auf dem Oktoberfest erstmals 1910 aufgebaut. Es handelt sich um eine liegende, drehbare Holzscheibe mit etwa fünf Metern Durchmesser. Die Besucher werden aufgefordert, sich darauf zu setzen oder zu legen und sich bei ständig steigender Drehzahl so lange wie möglich auf dieser Scheibe zu halten. Mitarbeiter des Fahrgeschäfts versuchen mit Hilfe eines Strohsacks, die Teilnehmer „herunterzukegeln“ oder sie mit einem Lasso herunterzuziehen. Ein Rekommandeur, der die Vorgänge mit derbem bayerischen Humor kommentiert, macht das Teufelsrad zu einer Attraktion für die Zuschauer. Im Laufe des Tages werden auf dem Rad wiederholt Boxkämpfe zwischen freiwilligen Zuschauern durchgeführt.

„Der Schichtl“, benannt nach seinem Gründer "Michael August Schichtl" (1851–1911), ist seit 1869 fester Bestandteil des Oktoberfestes. In kurzen Vorstellungen werden Zaubereien und Kuriositäten präsentiert. Berühmt wurde der Schichtl durch die "Enthauptung einer lebendigen Person mittels Guillotine", die (mit einem der Zuschauer als Enthauptungsopfer) bis heute aufgeführt wird. Bisher wurden so mehr als 9000 Zuschauer „enthauptet“. Der Spruch "„Auf geht’s beim Schichtl“" ist zumindest im Münchner Raum noch immer allgemein bekannt. Charakteristisch ist die den Vorstellungen vorausgehende humorvolle, öffentliche Beschimpfung des an dem Zelt vorbeigehenden Publikums auf einer Bühne vor dem Eingang. Ein Ansager in kurzer Lederhose und ordensgeschmückter Offiziersjacke mit Leopardenfell neckt das Publikum in bayerischer Mundart und wirbt neue Besucher für die jeweils nächste Vorstellung.

Pitts Todeswand besteht aus einer großen, hölzernen zylindrischen Steilwand von etwa zwölf Metern Durchmesser und acht Metern Höhe. An ihrer Innenwand rasen Motorradfahrer, nur durch die Fliehkraft gehalten, bis dicht an die Oberkante, an der die Zuschauer stehen. Dabei vollführen sie allerlei akrobatische Kunststücke. Das Unternehmen ist seit 1932 auf dem Oktoberfest und aus dieser Zeit stammen auch die Motorräder.

Zu den moderneren Fahrgeschäften zählen der Olympia Looping, die größte mobile Achterbahn der Welt mit fünf Loopings, und weitere Rund-, Hochfahr- und Laufgeschäfte.

Aufgrund des oft kühlen Wetters im Oktober beginnt das Oktoberfest seit 1872 schon im September. Eröffnet wird seither am Samstag "nach" dem 15. September, Ende des Festes ist traditionell der erste Sonntag im Oktober. Seit 2000 gilt folgende Regel: Ist der 1. oder 2. Oktober ein Sonntag, wird das Fest bis zum Tag der Deutschen Einheit am 3. Oktober verlängert. Damit dauert das Fest mindestens 16 und höchstens 18 Tage. 2010 wurde aufgrund des zweihundertjährigen Jubiläums ein zusätzlicher Oktoberfestmontag angehängt.

Die Festzelte öffnen am ersten Wiesn-Samstag bereits um 09:00 Uhr, Bier wird allerdings erst ab 12:00 Uhr nach dem ersten Anstich verkauft. Unter der Woche sind die Zelte von 10:00 Uhr bis 23:30 Uhr, an Samstag, Sonn- und Feiertagen von 09:00 Uhr bis 23:30 Uhr geöffnet. Der Bierausschank endet um 22:30 Uhr (jedoch Käfer Wies'n Schänke und Kufflers Weinzelt 1:00 Uhr, letzter Ausschank ist dort um 0:30 Uhr.).
Die Verkaufsstände und Fahrgeschäfte öffnen in der Regel um 10:00 Uhr und schließen um 23:30 Uhr, an den Samstagen um 24:00 Uhr.
Die Oide Wiesn öffnet täglich von 10 bis 23 Uhr, letzter Einlass ist um 21 Uhr; die Ausschankzeiten sind täglich 10 bis 21:30 Uhr.

Die Theresienwiese ist 42 Hektar groß. Die Fläche des Festgeländes beträgt 31 Hektar (26 Hektar bei der "Kleinen Wiesn"), jene der "Oiden Wiesn" 3 Hektar.

Die durchschnittliche Besucherzahl beträgt jährlich um sechs Millionen (2016: ca. 5,6 Millionen). Den bisherigen Besucherrekord stellte die Jubiläumswiesn 1985 mit 7,1 Millionen Besucher auf. Für 100.000 Personen stehen Sitzplätze zur Verfügung. Jährlich arbeiten etwa 12.000 Personen auf der Wiesn, davon sind 1600 Kellner. Durchschnittlich werden jedes Jahr rund 60.000 Hektoliter Bier und knapp 500.000 Brathendl verkauft. Der Gesamtumsatz beträgt etwa 450 Mio. € (Stand 2009).

Die Stadt München erzielte als Veranstalterin 2009 direkte Einnahmen von den Wirten und Schaustellern in Höhe von 3,85 Millionen Euro, diesen standen Kosten in Höhe von 4,2 Millionen Euro gegenüber. Während der zwei Wochen der Veranstaltung gaben 2011 etwa 6 Millionen Besucher 1,1 Mrd. Euro aus. 2009 waren es ebenfalls sechs Millionen Besucher; sie gaben rund 830 Millionen Euro in der Stadt aus. Gegenüber dem Zeitraum vor der Rezession entsprach dies einem Rückgang um 120 Millionen €. Nach Angaben des Münchner Wirtschaftsreferats gab der durchschnittliche Besucher 2009 bei jedem Besuch 54 Euro aus. Insgesamt verzehrten die Besucher eine halbe Million Hendl und 25 Tonnen Fisch. Der Bierverbrauch lag bei 6,6 Millionen Maß. Im Jahr 2014 wurden insgesamt 65.000 Hektoliter Bier ausgeschenkt.
In den Jahren 2009 und 2010 veröffentlichte die Bank Unicredit/HVB eine Studie zur Preisentwicklung auf dem Oktoberfest im Zeitraum seit 1985. Ausgehend von der angenommenen Berechnungsgrundlage, dem „Wiesnbesucherpreisindex (WBPI)“ (Kosten für zwei Maß Bier, ein halbes "Hendl" und Hin- und Rückfahrt mit öffentlichen Nahverkehrsmitteln) kommt die Studie zu einem Anstieg des Preisindexes von 140 Prozent bis zum Jahr 2009 und 152 Prozent bis zum Jahr 2010.

Die Steigerung der Kosten für den zugrunde gelegten Wiesnbesuch ist hierbei fast doppelt so hoch wie die allgemeine Inflation. Am stärksten stieg hierbei der Bierpreis mit ca. vier Prozent jährlich mehr als doppelt so schnell wie außerhalb des Volksfestes. Die Autoren der Studie kamen aufgrund der ihnen vorliegenden Fakten zu dem Schluss, das Oktoberfest sei eine Sonderwirtschaftszone.

Nach einer Untersuchung der Landeshauptstadt München lagen die Bierpreise anderer gastronomischer Großbetriebe im Raum München 2010 zwischen 6,60 und 8,40 Euro. In vergleichbaren Zelten auf dem nahegelegenen Rosenheimer Herbstfest kostete die Maß 2010 7,40 Euro, nachdem ihr Preis in den Jahren 2007–2009 durchschnittlich 6,80 Euro betragen hatte. Von 2009 bis 2010 stieg der WBPI um 3,4 Prozent und lag damit erneut deutlich über der Inflationsentwicklung in Deutschland.

Während sich der Benzinpreis für einen Liter Super zwischen 1950 und 2010 um 347 Prozent steigerte, betrug im gleichen Zeitraum die Steigerung des Bierpreises auf dem Oktoberfest 942 Prozent.

Die jährlichen Ankündigungen von Preiserhöhungen sorgen in der örtlichen Presse jedes Jahr für intensive Diskussionen. Traditionell erwidern die Wirte stets, mit den Preissteigerungen lediglich die steigenden Kosten an die Bierkonsumenten weiterzugeben. Die Preiserhöhungen wirkten sich nicht auf den Bierkonsum aus. Dieser stieg von 0,68 Maß/Kopf 1981 auf 1,17 Maß/Kopf 2009 an. Ein weiteres Dauerthema ist das schlechte Einschenken. Einem „besonders geschickten“ Schankkellner gelang es, aus einem 200-Liter-Fass 289 Maß Bier einzuschenken. Gegen derartige Rekorde wendet sich der "Verein gegen betrügerisches Einschenken". Er fordert regelmäßig, dass die offiziell zulässige Differenz zwischen einer Maß und einem Liter von höchstens 0,1 Liter, die sogenannte Toleranzgrenze des Kreisverwaltungsreferats München, beseitigt wird. Im September 2008 erklärte Vereinspräsident Jan-Ulrich Bittlinger, dass ein Maßkrug auf der Wiesn maximal 0,9 Liter Bier enthält. Die 1-Liter-Maß sei auf dem Oktoberfest nach wie vor so selten wie ein Sechser im Lotto. Die Schuld daran habe die Stadt München, da sie es den Wiesnwirten offiziell erlaube, die Gäste pro Maß um 0,1 Liter oder mehr als 0,80 Euro zu betrügen. Laut Bittlinger machen die Wirte dankbar Gebrauch von dieser Regelung. Bei einem Durchschnittspreis von 8 Euro pro Maß, 6 Mio. verkaufter Maß Bier in 16 Tagen und einer durchschnittlichen Füllmenge pro Maßkrug von nur 0,9 Liter entstehe so den Oktoberfestbesuchern ein Schaden von hochgerechnet 4,8 Millionen Euro. Der Verein testete am ersten Wiesn-Samstag 2008 250 Maßkrüge in unterschiedlichen Zelten. Den Negativrekord erreichte dabei die Augustiner-Festhalle. Zwölf Maßkrüge enthielten weniger als 0,8 Liter Bier.

2009 erklärte der Verein, die Preise für eine Maß Weißbier zwischen 9,00 Euro und 13,60 Euro seien „Gier pur“ und schadeten dem Volksfestcharakter des Oktoberfestes. Wie die für die Angemessenheitsprüfung zuständige Stadt München 13,60 Euro als ‚angemessen‘ für eine Maß Bier bezeichnen kann, sei schleierhaft. Auf dem Oktoberfest 2015 kostet die Maß Bier durchschnittlich 10,22 Euro und ist damit zum ersten Mal nicht mehr unter 10 Euro zu haben.

Über den Umsatz eines Festzeltes und den Gewinn des Wiesn-Wirtes gab es lange nur Spekulationen. 2014 kam im Rahmen eines Prozesses wegen Steuerbetruges gegen den bekannten Wirt Sepp Krätz heraus, dass dessen Zelt Hippodrom im Jahr 2012 insgesamt 3,3 Millionen Euro Umsatz machte. Nach Abzug aller Ausgaben blieb ein Nettogewinn von 1,5 Millionen Euro übrig. Da das Hippodrom ein relativ kleines Zelt war, wird vermutet, dass die größeren Festzeltbetreiber einen noch höheren Gewinn machen.

Um die Zahl der Diebstähle, Schlägereien und sexuellen Übergriffe rund um die Wiesn zu senken, wurden in den letzten Jahren die Maßnahmen zum Schutz der Besucher erweitert. So startete zum Beispiel 2003 die Aktion "Sichere Wiesn für Mädchen und Frauen."

2004 wurde im "Behördenhof" des Oktoberfestes ein neues "Servicezentrum" eingerichtet, in dem die Polizei, die Berufsfeuerwehr München, der Sanitätsdienst unter der Leitung des Bayerischen Roten Kreuz München und das Kreisverwaltungsreferat stationiert sind. Für das Oktoberfest wird eine eigene Polizeiinspektion aufgestellt, die unter der Notrufnummer "5003220" zu erreichen ist. Auf Grund der zahlreichen italienischen Wiesnbesucher sind seit 2005 Polizeibeamte der italienischen Polizei aus Bozen vor Ort. Das Bayerische Rote Kreuz ist seit Jahrzehnten für den Sanitätsdienst auf der Wiesn zuständig. Zusätzliche Sanitätsdienste gibt es im Zelt der Fischer-Vroni (Aicher Ambulanz) sowie im Auftrag der Münchner Verkehrsgesellschaft eine zusätzliche Absicherung der U-Bahn-Station Theresienwiese durch die Johanniter-Unfall-Hilfe. Im Behördenhof stehen ein Notarzteinsatzfahrzeug und eine vollausgestattete Krankenstation mit einem kleinen OP-Raum zur Verfügung. Um in dem Gedränge einigermaßen zügig voranzukommen, hat sich der Einsatz von Schiebetragen mit Sichtschutzplanen bewährt (Spitzname: "Banane" oder "gelber Sarg"). Die Vorhaltung im öffentlichen Rettungsdienst wird während der Zeit des Oktoberfests erhöht, zusätzliche Rettungsfahrzeuge stehen der Leitstelle zur Verfügung.

Seit 2010 gilt, als eine „Maßnahme zur allgemeinen Sicherheit“, ein generelles Hunde- und Tierverbot. Seitdem dürfen die typischen Münchner Zamperl die Festwiese nicht mehr betreten. 2012 wurde aufgrund vermehrt auftretender Schnittverletzungen ein Glasflaschenverbot erlassen.

Das Sicherheitskonzept der Veranstaltung wurde im Laufe der vergangenen Jahrzehnte immer wieder angepasst. Nach dem Bombenanschlag 1980 wurde der Haupteingang der Wiesn 1981 umgestaltet. 2008 wurde die Theresienwiese erstmals während des Aufbaus des Oktoberfestes für die Öffentlichkeit gesperrt. 2009 kam es zu weiträumigen Absperrungen und Zufahrtskontrollen während des Festes. Hintergrund dafür war die Androhung von Anschlägen durch Islamisten. 2010 wurde ein erweitertes Sicherheitskonzept umgesetzt. Es sieht unter anderem drei Sperr-Ringe um die Theresienwiese, Zugangskontrollen und ein Flugverbot über der Festwiese vor. Zudem wurden im Jubiläumsjahr 2010 im Rahmen des erweiterten Sicherheitskonzepts erstmals 52 je zwei Meter hohe Beton-Litfaßsäulen an den Zufahrten und Zugängen zur Festwiese aufgestellt, um ein Szenario zu verhindern, nach dem Attentäter versuchen könnten, mit Sprengstoff beladene Laster auf die Theresienwiese zu fahren. 2011 wurden an allen Zufahrten und Zugängen zur Theresienwiese mit großem Aufwand 170 teils fest verankerte, teils versenkbare Sicherheitspoller eingebaut, die künftig den gewaltsamen Zugang zum Festgelände mit Fahrzeugen verhindern sollen. Der Bavariaring ist gesperrt, um im Ernstfall Platz zu gewinnen, und damit die Sicherheitskräfte besser agieren können. Die Polizei kann bei Massenandrang kurzfristig durch Rundfunkmeldungen die Besucherströme umleiten oder U- oder S-Bahn-Stationen schließen. Im Jahr 2014 verzeichnete die Polizei 2.205 Einsätze.

Seit 2016 gibt es erstmals aufgrund der erhöhten Terrorgefahr ein grundsätzliches Rucksack- und Taschenverbot sowie einen 2-Meter hohen Sicherheitszaun um das Festgelände. Im Jahr 2016 waren 450 Ordner im Einsatz, knapp doppelt so viele wie im Vorjahr.

Technische Unfälle sind in der Geschichte des Oktoberfestes selten. Die Fahrgeschäfte werden im Vorfeld intensiv geprüft. Die Aufgabe der technischen Prüfung übernimmt traditionell die Abteilung "Seilbahnen und fliegende Bauten" des heutigen TÜV Süd. Dennoch ereignete sich am 30. September 1996 auf der "Euro-Star"-Achterbahn ein Auffahrunfall, bei dem es 30 Verletzte gab. Ursache war der unbemerkt gebliebene Verschleiß der Sicherheitsbremse des auffahrenden Zuges. Die Münchner Staatsanwaltschaft leitete damals ein Verfahren wegen fahrlässiger Körperverletzung gegen einen Ingenieur des TÜV München ein, das aber eingestellt wurde.

43 Kilometer Kabel versorgen die Wiesn über 18 zum Teil unterirdische Trafostationen mit Energie. Der Stromverbrauch des Oktoberfests beträgt insgesamt rund 2,7 Millionen Kilowattstunden (ohne Auf- und Abbau). Das entspricht in etwa 13 % des täglichen Strombedarfs in München. Ein großes Festzelt benötigt durchschnittlich eine Leistung von 400 Kilowatt, größere Fahrgeschäfte 300 Kilowatt. Zur Versorgung der Zelte mit Erdgas wurde ein vier Kilometer langes Netz aus Gasleitungen verlegt. Der Gasverbrauch beläuft sich auf 180.000 Kubikmeter für den Küchenbetrieb und 20.000 Kubikmeter zum Beheizen der Biergärten. Die meisten Festzelte und Fahrgeschäfte (2007: 61 %) beziehen den von den Stadtwerken München angebotenen Öko-Strom.

Da selbst ein kurzzeitiger Stromausfall zu einer Massenpanik führen könnte, wurde die gesamte Stromversorgung zweifach angelegt und wird separat gespeist. Sogar die Lampen der einzelnen Festzelte werden so von zwei verschiedenen Trafostationen versorgt. Trotz dieser Vorsichtsmaßnahmen kam es am 25. September 2007 zu einem mehrstündigen Stromausfall, nachdem bei starken Regenfällen ein Kabelkanal überflutet worden war. Da der Strom in den Vormittagsstunden ausfiel, kam es zwar zu Versorgungsengpässen in der Gastronomie, aber nicht zu einer Massenpanik.

Um für genügend Kapazität der Mobilfunknetze zu sorgen, werden jedes Jahr etliche mobile Sendemasten auf dem Wiesngelände und seit 2005 in einem Festzelt aufgestellt.

Die Münchner Verkehrsgesellschaft (MVG) befördert nach eigenen Angaben knapp vier Millionen Besucher von und zur Festwiese. Vor allem in den Abendstunden sind die U- und S-Bahnen voll besetzt. Der U-Bahnhof "Theresienwiese" wird in Stoßzeiten im Drei-Minuten-Takt angefahren und muss nach der Sperrstunde der Bierzelte gelegentlich wegen Überfüllung geschlossen werden. Um den reibungslosen Betrieb und die Sicherheit der Fahrgäste zu gewährleisten, setzen die MVG und die Deutsche Bahn verstärkt Sicherheitspersonal ein.

Im Straßenverkehr kommt es zu größeren Beeinträchtigungen. Da zahlreiche Wiesnbesucher trotz des Alkoholkonsums ihren Heimweg mit dem Auto antreten, werden von der bayerischen Polizei groß angelegte Alkoholkontrollen durchgeführt. Ringstraßen und Autobahnen im Münchner Umland werden dazu bis auf eine Spur gesperrt, wodurch es zu Rückstaus kommt.

Besonders zum mittleren Wiesn-Wochenende reisen viele Italiener mit Wohnwagen an (dieses Wochenende wird von den Münchnern daher als „Italiener-Wochenende“ bezeichnet). Das veranlasste die Stadtverwaltung dazu, in weiten Teilen der Stadt ein Campingverbot zu verhängen und gesonderte Parkplätze außerhalb der Innenstadt, jedoch im Bereich des öffentlichen Personennahverkehrs einzurichten. Große Flächen stehen zum Beispiel nahe der Allianz Arena zur Verfügung. Trotzdem ist die Parkplatzsituation rund um die Theresienwiese angespannt. Als Folge ist der Aufwand für Kontrollen und Abschleppdienste hoch.

2010 wurden im Rahmen des neuen Sicherheitskonzepts die Taxistände verlegt und befinden sich nun außerhalb der Sperr-Ringe in etwas größerer Entfernung zur Festwiese.

Jährlich fallen auf dem Oktoberfest knapp 1000 Tonnen Restmüll an. Jeden Morgen schafft Reinigungspersonal den Müll weg und spritzt die Wege sauber. Die Reinigung bezahlen zu einem Teil die Stadt München und zum anderen Teil die Sponsoren.

Nachdem 2004 die Warteschlangen vor den Toilettenanlagen so lang wurden, dass die Polizei den Zugang regeln musste, wurde im Folgejahr die Zahl der Toiletten um 20 % erhöht. Jetzt stehen etwa 1800 Klosetts und Urinale zur Verfügung.

Einige Wiesngäste suchen die Toiletten auf, um ungestört zu telefonieren. Aus diesem Grund plante man 2005, Faradaysche Käfige oder Störsender auf den Toiletten zu installieren und dadurch das Telefonieren zu unterbinden. Allerdings sind Störsender in Deutschland nicht zugelassen und die Faradayschen Käfige (in Form von Kupfernetzen) zu teuer. Daher wurden stattdessen lediglich Verbotsschilder aufgestellt. Der Grund hierfür ist, dass die Toilettenplätze unnötig „in Beschlag genommen“ werden.

Seit 1952 gibt es auf Initiative des Kommunalbeamten Heinz Strobl jedes Jahr ein neues Plakat zum Münchner Oktoberfest. Das Referat für Arbeit und Wirtschaft der Stadt selbst ruft diesen Wettbewerb aus. Seit 2000 ist er, vom Stadtrat beschlossen, nur noch ein geschlossener Wettbewerb. Das offizielle Wiesnplakat ist mit dem Wiesnlogo versehen, wird weltweit zum Werbeeinsatz gebracht und ziert seit 1978 den limitierten Sammlerkrug. Auf dem Oktoberfest selbst zeigt die Hühnerbraterei Heinz nahezu alle über die Jahrzehnte entstandenen Plakate in ihrem Zelt.

Anlässlich des 200-jährigen Jubiläums gab das Finanzministerium am 9. September 2010 eine Sonderbriefmarke à 55 Eurocent (siehe Abbildung) heraus. Der Entwurf stammt von Michael Kunter aus Berlin.
Eine Jugendmarke der Deutschen Bundespost aus dem Jahr 1978 zeigt einen Heißluftballon über dem Oktoberfest im Jahr 1820.


Nach dem Vorbild des Münchner Oktoberfestes entstanden ähnliche Volksfeste. Zu den größten zählen das Oktoberfest in Qingdao (China) mit rund 3 Mio. Besuchern jährlich, das Oktoberfest in Kitchener in Kanada mit rund 700.000 Besuchern jährlich und das Oktoberfest Blumenau in Blumenau in Brasilien mit rund 600.000 Besuchern jährlich. Auch in den USA, Australien, Russland, Namibia und Japan wird nach deutschem Vorbild gefeiert. Beim Oktoberfest Zinzinnati gedenken die Einwohner von Cincinnati seit 1976 alljährlich ihrer deutschen Vorfahren und nannten das Fest in Anlehnung an die deutsche Aussprache so. Es ist mit über 500.000 Besuchern jährlich das größte Oktoberfest in den USA.

In Deutschland gibt es diverse regionale Varianten des Oktoberfestes. Das größte ist das Oktoberfest Hannover, das mit jährlich rund 900.000 Besuchern das zweitgrößte Oktoberfest in Deutschland darstellt.

Vom 23. September bis 2. Oktober 2011 wurde die erste „Wiener Wiesn“ auf der Kaiserwiese zwischen Praterstern und Riesenrad vor dem Wiener Prater mit drei Festzelten von 150.000 Menschen besucht.

Das häufig als das zweitgrößte Volksfest der Welt bezeichnete Cannstatter Volksfest („Wasen“) ist keine Kopie des Oktoberfestes, sondern hat eine davon unabhängige Geschichte.





</doc>
<doc id="9509" url="https://de.wikipedia.org/wiki?curid=9509" title="Reibung">
Reibung

Reibung, auch Friktion oder Reibungswiderstand genannt, ist eine Kraft, die zwischen Körpern oder Teilchen wirkt, die einander berühren. Die Reibungskraft erschwert dann die Bewegung der Körper gegeneinander. Um eine Bewegung zu erzeugen oder aufrecht zu erhalten, ist Arbeit notwendig. Wenn bei einer Bewegung Reibung auftritt, wird ein Teil der Arbeit oder der Bewegungsenergie durch Dissipation in Reibungswärme umgewandelt und/oder für Verschleiß verbraucht.

Bei der Betrachtung von Reibungsvorgängen unterscheidet man zwischen "äußerer Reibung" und "innerer Reibung". Die "äußere Reibung" tritt auf bei Reibung zwischen sich berührenden Außenflächen von Festkörpern. Die "innere Reibung" tritt auf zwischen benachbarten Teilchen bei Verformungsvorgängen innerhalb von Festkörpern, Flüssigkeiten und Gasen. In physikalischen Modellen werden Reibungskräfte oft vernachlässigt, wenn sie relativ klein und/oder quantitativ schwer erfassbar sind. Mit der wissenschaftlichen Untersuchung von Reibungsvorgängen beschäftigt sich die Tribologie (Reibungslehre).

Äußere Reibung wird auch als "Festkörperreibung" bezeichnet, weil sie zwischen den Kontaktflächen von sich berührenden Festkörpern auftritt. Sie wird unterteilt in "Haftreibung" und "Gleitreibung", die beide zu Ehren des Physikers Charles Augustin de Coulomb auch als Coulombsche Reibung bezeichnet werden. Sie treten nicht immer strikt voneinander getrennt auf. Sie können zugleich oder abwechselnd auftreten; zum Beispiel ist der Stick-Slip-Effekt ein periodischer Übergang zwischen Haft- und Gleitreibung. Auf die Anwendung bezogene Begriffe sind "Rollreibung", "Bohrreibung" und "Seilreibung".

Die Reibungskraft formula_1 nimmt mit der Normalkraft formula_2 zu, oft annähernd linear und unabhängig von der Größe der Kontaktfläche (siehe Amontonssche Gesetze): 
Dabei sind die Reibungskoeffizienten µ abhängig von der Beschaffenheit der Oberflächen. Der Koeffizient für Haften ist grundsätzlich größer als der für Gleiten. Ihr Wert wird experimentell bestimmt. Die Ungleichheit kommt daher, dass eine Reibungskraft niemals stoßen kann.

In vielen Fällen ist Haften zwischen sich berührenden Körpern erwünscht. Ohne Haftreibung würde der Alltag gar nicht funktionieren. Möbel würden nicht an ihrem Platz bleiben, auf der Straße abgestellte Fahrzeuge (die Räder blockiert) könnten allein vom Wind fortbewegt werden. Man könnte keinen Fuß „fest“ auf den Boden setzen, alle angetriebenen Fahrzeugräder würden „durchdrehen“, also keine Traktion ermöglichen. In technischen Anwendungen wird außer der meistens wirkenden Gewichtskraft ein technisch erzeugter Druck zwischen den Kontaktflächen benutzt, zum Beispiel mittels gespannter Federn in einer Reibungs-Kupplung.

Haften ist ein Zustand der Ruhe, so dass weder Verschleiß noch Energieverluste auftreten. Es ist eine Kombination von Formschluss im Kleinen, durch Rauheit als Gestaltabweichung 3. bis 5. Ordnung, der bei Bewegung zerstört würde, und molekularem Kraftschluss im Kleinen durch molekulare Anziehungskräfte, also Adhäsion.

Gleitreibung tritt an den Kontaktflächen zwischen Körpern auf, die sich relativ zueinander bewegen. Bei einigen Werkstoffkombinationen tritt ein Kriechen auf, so dass die Reibungskraft entgegen dem Amontonsschen Gesetz geschwindigkeitsabhängig wird. Die Gleitreibungskraft ist meist geringer als die Haftreibungskraft bei gleicher Normalkraft.

Rollreibung entsteht beim Rollen eines Körpers auf einer Unterlage. Im Modell lässt sich die Rollreibung durch die Deformation eines nicht ideal starren Körpers erklären. Die Rollreibung wird durch die dimensionslose Rollreibungszahl beschrieben. Diese ist definiert als Verhältnis der "Rollreibungslänge" formula_4 und dem Radius formula_5 des Rollkörpers:

Überlagern sich Gleit- und Rollreibung, so wird dies als Wälzreibung bezeichnet.

Bohrreibung entsteht am Auflagepunkt eines sich um die vertikale Achse drehenden Körpers auf einer Ebene. Da die Bohrreibung bei einer rotierenden Bewegung wirkt, wird die Bohrreibung als Drehmoment angegeben:

Der Koeffizient der Bohrreibung formula_8 hat die Dimension einer Länge und ist als Radius der scheinbaren Auflagescheibe deutbar, also als der resultierende Hebelarm der Flächenmomente. Er lässt sich jedoch im Allgemeinen "nicht" als Produkt aus einem konstanten mittleren Radius der Auflagefläche und einer Materialkonstanten berechnen.

Die Euler-Eytelwein-Formel beschreibt die Reibung eines um einen runden Körper gelegten Seils, auf das beidseitig Kräfte wirken, und gibt an, unter welchen Bedingungen das Seil haftet.

Innere Reibung bewirkt die Zähigkeit von Materialien und Fluiden und hat Einfluss auf Verformungen und Strömungen. Neben der Bewegung der Teilchen in einem Stoff beschreibt die innere Reibung auch den Reibungswiderstand von Körpern, die sich in Fluiden bewegen, sowie die Dämpfung von Schallwellen. Typischerweise nimmt in Gasen die innere Reibung (Viskosität) mit der Temperatur zu, und in Flüssigkeiten ab. In einfachen Fällen ist mit den Mitteln der statistischen Physik eine quantitative Beschreibung möglich.

Bei Temperaturen nahe dem Temperaturnullpunkt verlieren einige Flüssigkeiten ihre innere Reibung vollkommen (siehe Suprafluidität).

Anders als in der Mechanik, in der Reibung so lange wie möglich vernachlässigt wird, ist innere Reibung in der Standardtheorie der Hydrodynamik, den Navier-Stokes-Gleichungen, fest enthalten (daher auch "Stokes’sche Reibung"). Diese nichtlinearen Gleichungen sind im Allgemeinen nur numerisch lösbar. Für den Fall kleiner Reynolds-Zahl Re, wenn also die Advektion von Impuls gegenüber dem Impulstransport durch Viskosität vernachlässigt werden kann, existieren für einfache Geometrien und Newtonsche Fluide geschlossene Lösungen:

Das gilt beispielsweise für eine dünne Schicht von Schmiermittel zwischen sich gegeneinander bewegenden Flächen. Die Reibung ist dann proportional zur Scherrate, also zur Geschwindigkeit formula_9. Dieselben Verhältnisse liegen für den Fall einer kleinen Kugel in einem zähen Fluid vor, siehe das Gesetz von Stokes. Bei dominierender Impulsadvektion ist dagegen die Dissipation proportional zum Quadrat der Geschwindigkeit, siehe Strömungswiderstand.

Die plastische Verformung von Festkörpern ist in der Regel stark nichtlinear und damit nicht gut durch die Viskosität zu beschreiben. Auch bei kleineren Kräften oder Spannungen, gibt es Abweichungen von der Idealen Elastizität als eine andere Art der inneren Reibung im Festkörper, die sich aber auch nicht einfach als Viskosität verstehen lässt. Entsprechend ist die Gleichsetzung von innerer Reibung und Viskosität auf Fluide beschränkt.

Entsprechend dem Energieerhaltungssatz geht durch Reibung keine Energie verloren. Dies gilt auch dann, wenn Energie aus einem betrachteten System verschwindet, weil sie unter Entropiezunahme in Wärmeenergie umgewandelt wurde. Ein gleitender Hockey-Puck kommt zum Stillstand, weil Reibung seine kinetische Energie in Wärme umwandelt, die die Thermoenergie des Pucks und der Eisoberfläche erhöht. Da diese Wärme schnell dissipiert, unterlagen frühe Philosophen, einschließlich Aristoteles, dem Trugschluss, dass bewegte Objekte ohne Einfluss einer treibenden Kraft Energie verlieren.

Wenn ein Objekt entlang eines Pfades formula_10 auf einer Oberfläche verschoben wird, berechnet sich die verrichtete Reibungsarbeit formula_11 aus dem Produkt des Weges und der entlang des Weges wirkenden Kraft, entsprechend der Definition der Arbeit. Sind Kraft oder Reibungskoeffizient über den Weg nicht konstant, ist ein Kurvenintegral anzusetzen.

Geht man von einer vollständigen Umwandlung in Wärmeenergie aus, gilt

wobei

Die aus einem System durch Reibung verlorene Energie ist ein klassisches Beispiel der thermodynamischen Irreversibilität.

Die Optimierung von Reibungsvorgängen ist Gegenstand der Tribologie.

Bei der "Festkörperreibung" berühren sich die aufeinander gleitenden Flächen. Dabei werden Oberflächenerhöhungen eingeebnet (Abrieb oder Verschleiß). Bei ungünstiger Werkstoffpaarung und großer Flächenpressung verschweißen die Oberflächen miteinander (Adhäsion). Festkörperreibung tritt beispielsweise bei Verwendung von Trockenschmierstoffen (Graphit, PTFE) auf, wenn kein Schmierstoff verwendet wird oder die Schmierung versagt. Dieser Reibungszustand wird daher auch als "Trockenreibung" bezeichnet und kann durch Linearkugellager deutlich verringert werden.
Die "Mischreibung" kann bei unzureichender Schmierung oder zu Beginn der Bewegung zweier Reibpartner mit Schmierung auftreten. Dabei berühren sich die Gleitflächen punktuell. Die Reibungskraft im Mischreibungsbereich ist Geschwindigkeitsabhängig und lässt sich an Gleitlagern beobachten. Dabei nimmt die Reibkraft / das Reibmoment mit steigender Gleitgeschwindigkeit ab, bis reine Fluidreibung auftritt und die Reibflächen trennt. Bei weiter steigender Gleitgeschwindigkeit nimmt dann die Reibkraft / das Reibmoment wieder zu.
Im Mischreibungsgebiet nimmt der Verschleiß ähnlich mit dem Reibmoment ab, bis die Gleitgeschwindigkeit die nahezu verschleißfreie Fluidreibung erreicht hat. Die "Mischreibung" ist daher im Dauerbetrieb stets unerwünscht, ist aber manchmal unvermeidlich oder ihre Vermeidung ist so aufwändig, dass die Kosten für Verschleißreparaturen in Kauf genommen werden.

Die "Fluidreibung" tritt dann auf, wenn sich zwischen den Gleitflächen ein permanenter "Schmierfilm" bildet. Typische Schmierstoffe sind Öle, Wasser aber auch Gase (siehe Luftlager). Die Gleitflächen sind vollständig voneinander getrennt. Die entstehende Reibung beruht darauf, dass die Schmierstoffmoleküle aufeinander gleiten. Damit diese Scherkräfte nur zu einer tragbaren Temperaturerhöhung des Schmierstoffes führen, muss die entstehende Wärme auf geeignete Weise abgeführt werden. Fluidreibung ist der gewünschte Reibungszustand in Lagern und Führungen, wenn Dauerhaltbarkeit, hohe Gleitgeschwindigkeit und hohe Belastbarkeit benötigt werden. Ein wichtiges Beispiel ist die Drucköl-Schmierung der Lagerschalen zwischen Kurbelwelle und Pleuelstange im Automotor (Hydrodynamisches Gleitlager).

Der Übergang von der Mischreibung zur Fluidreibung wird durch die Stribeck-Kurve dargestellt, das Minimum von Reibkraft/-moment der Kurve markiert den Übergang zur reinen Fluidreibung.






</doc>
<doc id="9514" url="https://de.wikipedia.org/wiki?curid=9514" title="Barometrische Höhenformel">
Barometrische Höhenformel

Die barometrische Höhenformel beschreibt die vertikale Verteilung der (Gas-)Teilchen in der Atmosphäre der Erde, also die Abhängigkeit des Luftdruckes von der Höhe. Man spricht daher auch von einem vertikalen Druck-Gradienten, der jedoch aufgrund der hohen Wetterdynamik innerhalb der unteren Atmosphäre nur mit Näherungen auf mathematischem Wege beschrieben werden kann.

In der einfachsten Form kann grob angenommen werden, dass der Luftdruck in der Nähe des Meeresspiegels um ein Hektopascal (entsprechend 1 ‰ des mittleren Luftdrucks) je acht Meter Höhenzunahme abnimmt.

Etwas besser ist die Näherung, dass der Druck mit zunehmender Höhe exponentiell abnimmt. Dieser Zusammenhang war 1686 erstmals von Edmond Halley erkannt worden.

Die Änderung von Druck und Dichte der Atmosphäre mit der Höhe wird durch die hydrostatische Grundgleichung beschrieben. Zur Herleitung betrachte man ein quaderförmiges Volumenelement mit der Grundfläche formula_1 und der infinitesimal kleinen Höhe formula_2, welches Luft der Dichte formula_3 enthält. Von unten wirkt auf die Grundfläche nur die vom Atmosphärendruck formula_4 ausgeübte Kraft formula_5. Die von oben auf die Grundfläche wirkende Kraft setzt sich zusammen aus der Gewichtskraft formula_6 der im Volumen formula_7 enthaltenen Luftmasse formula_8 und der vom Atmosphärendruck auf die Oberseite ausgeübten Kraft. Der Atmosphärendruck ist in dieser Höhe um den Betrag formula_9 verschieden von dem auf die Unterseite wirkenden Druck; die durch ihn ausgeübte Kraft ist daher formula_10.

Im hydrostatischen Gleichgewicht sind alle Luftströmungen zur Ruhe gekommen. Damit das Gleichgewicht erhalten und das betrachtete Volumenelement auch weiterhin in Ruhe bleibt, muss die Summe aller darauf wirkenden Kräfte null sein:

Kürzen und Umstellen liefert

Nach dem idealen Gasgesetz lässt sich die Dichte formula_13 ausdrücken als formula_14,

so dass sich schließlich ergibt:
formula_15

Dabei ist

Die hydrostatische Grundgleichung gibt an, um welchen Betrag formula_20 sich der Atmosphärendruck ändert, wenn sich die Höhe um einen kleinen Betrag formula_21 ändert. Wie das negative Vorzeichen zeigt, ist formula_20 negativ, wenn formula_21 positiv ist; der Druck wird mit zunehmender Höhe also geringer. So nimmt beispielsweise bei mittlerem Luftdruck auf Meereshöhe (formula_24 = 1013 hPa) bei einer Temperatur von 288 K (= 15 °C) der Druck auf einem Meter Höhenunterschied um 0,12 hPa beziehungsweise auf 8,3 Metern Höhenunterschied um 1 hPa ab. Der Höhenunterschied, der einem Druckunterschied von 1 hPa entspricht, ist die "barometrische Höhenstufe". In größeren Höhen (kleineres formula_24) und bei höheren Temperaturen formula_26 verändert sich der Luftdruck langsamer, die barometrische Höhenstufe nimmt zu.

Benötigt werden in der Regel explizite Werte für Druck und Dichte auf vorgegebenen Höhen. Daraus lassen sich bei Bedarf auch die Druckunterschiede für "größere" Höhenunterschiede ablesen. Die gesuchte Lösung der Grundgleichung erhält man durch Trennung der Variablen

und anschließende Integration zwischen den gesuchten Höhen beziehungsweise den zugehörigen Drücken:

Integration der linken Seite ergibt formula_29. Zur Integration der rechten Seite muss die Höhenabhängigkeit von formula_30 und formula_26 bekannt sein. Die Schwerebeschleunigung formula_30 kann für nicht zu große Höhen als konstant angesehen werden. Die Temperatur formula_26 variiert in komplizierter und kaum vorhersagbarer Weise mit der Höhe. Es müssen daher vereinfachende Annahmen über den Temperaturverlauf formula_34 getroffen werden.

Die in einführender Literatur und im Schulunterricht meist zitierte klassische barometrische Höhenformel gilt für den Spezialfall, dass die Temperatur formula_26 in jeder Höhe gleich, die Atmosphäre also "isotherm" ist.

Die Integration der hydrostatischen Grundgleichung liefert bei konstantem formula_26:

Durch Einführung der Skalenhöhe formula_37 vereinfacht sich die Höhenformel zu
formula_38

Mit jeder Höhenzunahme um formula_39 nimmt der Luftdruck um den Faktor formula_40 ab. Die Skalenhöhe ist daher ein natürliches Maß für die Höhe der Atmosphäre und den Druckverlauf in ihr. Sie beträgt in der hier angenommenen Modellatmosphäre bei einer Temperatur von 15 °C etwa 8,4 km.

Für die Dichte gilt entsprechend:
formula_41

Für einen bergab wandernden Beobachter nimmt der Luftdruck ständig zu, da eine immer schwerere Luftsäule auf ihm lastet. Die Zunahme verläuft exponentiell, da die Luft kompressibel ist: für jeden Meter Höhenunterschied nimmt die Gewichtskraft der auf einer Messfläche lastenden Luftsäule um das Gewicht des auf dieser Strecke hinzukommenden Säulenvolumens zu. Dieses Gewicht hängt aber von der Dichte der Luft und diese wiederum vom Luftdruck ab. Der Luftdruck wächst also umso schneller, je höher er bereits ist. Ändert sich eine Größe stets um einen Betrag, der der Größe selbst proportional ist, so geschieht die Änderung exponentiell.

In einem Teilchensystem, das sich bei der Temperatur formula_26 im thermischen Gleichgewicht befindet (das also insbesondere überall dieselbe Temperatur aufweist) und dessen Teilchen die kontinuierlich oder diskret verteilten Energieniveaus formula_43 einnehmen können, ist die Wahrscheinlichkeit, dass sich ein Teilchen gerade auf dem Energieniveau formula_43 befindet, gegeben durch die Boltzmann-Verteilung

Dabei ist formula_46 die Boltzmann-Konstante und formula_47 ein Normierungsfaktor (die so genannte Zustandssumme), der sicherstellt, dass die Summe über alle Wahrscheinlichkeiten gleich 1 ist. Besteht das System aus formula_48 Teilchen, so ist die Anzahl der Teilchen auf dem Energieniveau formula_43 im Mittel formula_50.

Ein Gasteilchen der Masse formula_51 hat im Schwerefeld der Erde die potentielle Energie formula_52 und wegen seiner Temperatur im Mittel die thermische Energie formula_53, insgesamt also die Energie formula_54. Betrachtet man zwei gleich große Volumenelemente auf den Höhen formula_55 beziehungsweise formula_56, so haben die Teilchen auf der Höhe formula_56 eine um den Betrag formula_58 höhere Energie. Die Wahrscheinlichkeit, ein Teilchen im höheren Volumenelement anzutreffen, verhält sich daher zur Wahrscheinlichkeit, es im tieferen Volumenelement anzutreffen wie

Für eine hinreichend große Anzahl formula_48 von Teilchen verhalten sich die Teilchendichten formula_61 wie die Aufenthaltswahrscheinlichkeiten

und wegen des idealen Gasgesetzes folgt für den Druck formula_63 dasselbe Verhältnis

wobei man die molare Masse formula_65 und die Gaskonstante formula_66 erhält, indem man die Teilchenmasse formula_51 beziehungsweise die Boltzmann-Konstante formula_46 mit der Avogadro-Konstante multipliziert.

Allerdings wird hier bei der Energie-Betrachtung der Gleichverteilungssatz vorausgesetzt. Diese Voraussetzung ist aber allgemein nur in dichter Atmosphäre erfüllt, weil nur dort durch Stöße zwischen den Gasmolekülen die Energien zwischen den verschiedenen Freiheitsgraden ausgetauscht werden. Der Grund dafür, dass der Gleichverteilungssatz generell nicht für die Höhenenergie gilt, ist dass der Gleichverteilungssatz direkt nur auf Potentiale angewendet werden kann, die quadratisch in die Hamilton-Funktion eingehen. Weil die Höhenenergie nur linear in die Hamilton-Funktion eingeht, kann man den Gleichverteilungssatz in sehr dünnen Gasen nicht einfach voraussetzen.

Der streng lineare Temperaturverlauf besteht nur in der idealisierten Vorstellung einer ruhenden Atmosphäre ohne Konvektion ohne Ausgleich des Temperaturgefälles durch Wärmeleitung. Um das besser verwendbar zu machen, wurde die potentielle Temperatur eingeführt. Obwohl der adiabatische Gradient ein Temperaturgefälle ist, ist die potentielle Temperatur konstant, d. h. ein Gleichgewicht. Mit der potentiellen Energie eines Teilchens im Gravitationsfeld ("E" = "m" · "g" · "h") hat das nichts zu tun. Besonders deutlich wird das mit der Zahl der Freiheitsgrade. Teilchen gleicher Masse, aber unterschiedlicher Zahl an Freiheitsgraden haben unterschiedliche Temperaturgradienten.

Da für die Aufrechterhaltung des linearen Temperaturverlaufs die Wärmeleitung keine Rolle spielen darf, darf in der Realität der permanente „Wärmetransport“ (Wärmeleitung) durch schnelle Zirkulation nur einen geringen Einfluss haben. Weil Konvektionslosigkeit und Zirkulation nicht gleichzeitig vorkommen können, wird der lineare Verlauf immer leicht modifiziert durch Wärmetransport aller Art, der bekannteste ist die Kondensation von Wasserdampf, die zu einem geringeren Temperaturabfall führt („feucht-adiabatisch“, eine etwas irreführende Bezeichnung).

Aus der Gleichung für die Druckänderung

und der mit Hilfe logarithmischer Ableitungen geschriebenen
Gleichung für die adiabatische Zustandsänderung

folgt sofort die lineare Temperaturabnahme gemäß

Mit der mittleren molaren Masse des Atmosphärengases
"M" = 0,02896 kg mol,
der Schwerebeschleunigung
"g" = 9,807 m s,
der universellen Gaskonstante
"R" = 8,314 J K mol
und dem Adiabatenexponenten von (trockener) Luft formula_72 = 1,402 erhält
man den Temperaturgradienten

Dies ist näherungsweise der unten angegebene Temperaturgradient. Jener wird allerdings im Wesentlichen durch die feuchtadiabatische Expansion bestimmt: der feuchtadiabatische Adiabatenexponent ist kleiner als der trockenadiabatische Adiabatenexponent. Bei einer reinen Wasserdampfatmosphäre wäre der Temperaturgradient

Weitere Grenzen des adiabatischen Ansatzes: Wird die Luft sehr kalt, ändert sich auch bei trockener Luft der Adiabatenexponent. Bei sehr großen Höhen (geringe Dichte) wird auch die mittlere freie Weglänge sehr groß, so dass die Gasgleichungen kaum noch gelten. Dazu kommt noch, dass durch den Treibhauseffekt auch der adiabatische Ansatz (kein Energieaustausch mit der Umgebung) verletzt wird.

Im Allgemeinen ist die Temperatur nicht konstant, sondern variiert mit der Höhe. Der einfachste Ansatz zur Berücksichtigung einer solchen Veränderlichkeit geht von einer linearen Abnahme mit der Höhe aus. Aus der adiabatischen Beziehung folgt wie oben beschrieben ein konstanter Temperaturgradient

formula_75

sodass für die Temperatur formula_34 gilt:
wobei formula_78 der (positiv zu nehmende) Betrag des vertikalen atmosphärischen Temperaturgradienten ist, der angibt, um wie viele Kelvin die Lufttemperatur pro Meter Höhenunterschied abnimmt. Das Integral über die rechte Seite der Grundgleichung lautet damit

Wegen

ist die Lösung des Integrals

so dass insgesamt aus dem Integral über die Grundgleichung

die barometrische Höhenformel für linearen Temperaturverlauf folgt:

oder wegen formula_84
formula_85

Für die Dichte gilt entsprechend
formula_86

Der Exponent ist hier durch κ geteilt, da der Dichte/Druck-Zusammenhang aus der adiabatischen Beziehung der beiden Größen resultiert.

Diese erweiterte barometrische Höhenformel bildet die Grundlage für die barometrische Höhenfunktion der Standardatmosphäre in der Luftfahrt. Dabei wird zunächst die Atmosphäre in Teilschichten mit jeweils linear interpoliertem Temperaturverlauf unterteilt. Dann werden, mit der untersten Schicht beginnend, Temperatur und Druck an der Obergrenze der jeweiligen Teilschicht berechnet und für die Untergrenze der darüber liegenden Schicht eingesetzt. Auf diese Weise entsteht induktiv das Modell für die gesamte Atmosphäre.

Wie Messungen der Temperaturprofile in der Atmosphäre zeigen, ist die Annahme einer linearen Temperaturabnahme im Mittel eine gute Näherung, wenn auch im Einzelfall deutliche Abweichungen auftreten können, zum Beispiel bei Inversionswetterlagen. Die Hauptursache für die Temperaturabnahme mit der Höhe ist die Erwärmung der unteren Luftschichten durch die von der Sonne aufgeheizte Erdoberfläche, während die oberen Luftschichten Wärme in den Weltraum abstrahlen. Dazu kommen trockenadiabatische oder feuchtadiabatische Temperaturänderungen einzelner aufsteigender oder absinkender Luftpakete und zusätzliche Modifikationen durch Vermischungsvorgänge zwischen Luftmassen unterschiedlicher Herkunft.

In Warmluftmassen und bei Aufgleitvorgängen nimmt der Temperaturgradient Werte um 0,3 bis 0,5 K pro 100 m an, in einbrechender Kaltluft meist um 0,6 bis 0,8 K pro 100 m, im Mittel über alle Wetterlagen 0,65 K pro 100 m. In Tallagen können häufige Bodeninversionen den mittleren Temperaturgradienten auf 0,5 K pro 100 m senken, in den Wintermonaten sogar auf 0,4 K pro 100 m.

Die beschriebenen Verhältnisse sind auf die Troposphäre beschränkt. In der Stratosphäre nimmt die Temperatur deutlich langsamer ab, meist nimmt sie sogar wieder zu, vor allem wegen der Absorption von UV-Strahlung in der Ozonschicht.

Für einen Temperaturgradienten von 0,65 K pro 100 m nimmt der Exponent formula_87 den Wert 5,255 an:

Wird der Exponent durch den Isentropenkoeffizienten formula_89 ausgedrückt, so wird:
Das bedeutet 8,5 Freiheitsgrade.

Aus dem Temperaturgradienten ergibt sich auch die mittlere Wärmekapazität der Luft über alle Wetterlagen:

Dieser Wert liegt zwischen dem Wert von trockener Luft (1005 Ws/(kg K)) und Wasserdampf (2034 Ws/(kg K)).

Die nachfolgende Tabelle zeigt den Zusammenhang zwischen Höhe und Druck (im Mittel):

In dieser Form bietet sich die Höhenformel für den häufigen Fall an, dass Temperatur und Luftdruck auf einer der beiden Höhen bekannt sind, nicht aber der zurzeit bestehende Temperaturgradient.

Die barometrische Höhenstufe ist die vertikale Strecke, die zurückgelegt werden muss, um 1 hPa Luftdruckänderung zu erzielen. In Bodennähe beträgt die barometrische Höhenstufe etwa 8 Meter, in 5 Kilometer Höhe 16 Meter und in 10 Kilometer Höhe 32 Meter.

Mit der Höhenformel ergibt sich folgende Tabelle für die Höhen- und Temperaturabhängigkeit der barometrischen Höhenstufe:

Als Faustformel für mittlere Höhen und Temperaturen gilt „1 hPa/30 ft“. Diesen Rundungswert nutzen Luftfahrer häufig für überschlägige Berechnungen.

Setzt man die Referenzhöhe formula_55 auf Meereshöhe und nimmt für die dortige Atmosphäre einen mittleren Zustand an, wie er durch die Internationale Standardatmosphäre beschrieben wird (Temperatur 15 °C = 288,15 K, Luftdruck 1013,25 hPa, Temperaturgradient 0,65 K pro 100 m), so erhält man die "Internationale Höhenformel" für die Troposphäre (gültig bis 11 km Höhe):

Diese Formel erlaubt die Berechnung des Luftdrucks (in Gestalt des sog. "Normaldrucks") auf einer gegebenen Höhe, ohne dass Temperatur und Temperaturgradient bekannt sind. Die Genauigkeit im konkreten Anwendungsfall ist allerdings begrenzt, da der Berechnung statt des aktuellen Atmosphärenzustands lediglich eine "mittlere" Atmosphäre zugrunde gelegt wird.

Internationale Höhenformel im Rahmen der Internationalen Standardatmosphäre "nach der Höhe" aufgelöst, zur Umrechnung des Luftdrucks p(h) (Normaldruck) in die damit korrespondierende Höhe in Metern (m):

Die Lösung der hydrostatischen Grundgleichung lautet allgemein
beziehungsweise
mit noch zu lösendem Integral.

Die Gaskonstante formula_66 ist eine Naturkonstante und kann vor das Integral gezogen werden. Die mittlere molare Masse der Atmosphärengase formula_65 ist, sofern vom stark variablen Wasserdampfgehalt abgesehen wird, innerhalb der Troposphäre ebenfalls praktisch konstant und kann auch vor das Integral gezogen werden. Die unterschiedlichen Skalenhöhen der verschieden schweren Atmosphärengase würden in einer ruhenden Atmosphäre zwar zu einer teilweisen Entmischung führen, so dass sich schwerere Komponenten in den unteren Schichten und leichtere Komponenten in den höheren Schichten anreichern würden; die durch das Wettergeschehen bedingte intensive Durchmischung der Troposphäre verhindert dies jedoch. Der veränderliche Wasserdampfgehalt sowie verallgemeinert auch sonstige geringfügige Änderungen von "M" (vor allem in den höheren Atmosphärenschichten) kann durch Verwendung der entsprechenden virtuellen Temperatur formula_100 anstelle der tatsächlichen Temperatur formula_26 berücksichtigt werden. Für "M" kann daher der Wert für trockene Luft in Meereshöhe eingesetzt werden.

Die Schwerebeschleunigung formula_30 nimmt mit der Höhe ab, was bei großen Höhendifferenzen oder hohen Genauigkeitsanforderungen berücksichtigt werden muss. Eine variable Schwerebeschleunigung im Integranden würde die Integration allerdings erheblich erschweren. Dies lässt sich umgehen durch Verwendung "geopotentieller" statt geometrischer Höhen. Man denke sich dazu eine Testmasse formula_51 bei variablem formula_30 von Meereshöhe auf die Höhe formula_105 gehoben. Weil formula_30 mit der Höhe abnimmt, ist die dabei gewonnene potentielle Energie formula_107 kleiner als wenn formula_30 stets den Meereshöhenwert formula_109 hätte. Die geopotentielle Höhe formula_110 ist die Höhe, gemessen in geopotentiellen Metern, die rechnerisch zu überwinden ist, um der Masse bei stets konstanter Schwerebeschleunigung formula_109 dieselbe potentielle Energie formula_107 zuzuführen (mit anderen Worten: formula_110 ist das durch formula_109 dividierte Schwerepotential. Flächen gleicher geopotentieller Höhe sind Äquipotentialflächen im Schwerefeld).

Für die zu einer geometrischen Höhe formula_105 gehörige geopotentielle Höhe formula_110 soll also gelten:

woraus folgt:

Für das Verhältnis der Schwerebeschleunigung formula_30 in der Höhe formula_105 zur Schwerebeschleunigung formula_109 auf Meereshöhe gilt, da das Gravitationsfeld quadratisch mit dem Abstand zum Erdmittelpunkt abnimmt:
mit dem Erdradius formula_123.
Integration von
liefert

formula_123 ist dabei auf den Wert 6356 km zu setzen. Gegebenenfalls muss außerdem noch berücksichtigt werden, dass die Schwerebeschleunigung auf Meereshöhe formula_109 von der geographischen Breite abhängt.

Auf diese Weise müssen nur einmal vor der Rechnung die gewünschten geometrischen Höhen in geopotentielle Höhen umgerechnet werden; in der Höhenformel kann dann statt der veränderlichen Schwerebeschleunigung einfach der konstante Meereshöhenwert verwendet werden. Für nicht zu große Höhen ist der Unterschied zwischen geometrischen und geopotentiellen Höhen gering und oft vernachlässigbar:

Mit der Schwerebeschleunigung auf Meereshöhe formula_109, den geopotentiellen Höhen formula_130 und formula_131 und der virtuellen Temperatur formula_100 vereinfacht sich die allgemeine Höhenformel zu

Es bleibt das Integral über formula_134 zu lösen, wozu nur noch das Temperaturprofil formula_135 bekannt sein muss. Es kann in der realen Atmosphäre zum Beispiel durch Radiosonden-Aufstiege bestimmt werden. Für vereinfachte Modellatmosphären mit konstanter oder linear veränderlicher Temperatur ergeben sich wieder Höhenformeln des eingangs behandelten Typs.

Der von einem Barometer gemessene Luftdruck (QFE) hängt sowohl vom meteorologischen Zustand der Atmosphäre als auch von der Standorthöhe ab. Sollen die Angaben verschiedener Barometer in einem größeren Gebiet für meteorologische Zwecke untereinander verglichen werden (zum Beispiel um die Lage eines Tiefdruckgebiets oder einer Front zu bestimmen), so muss der Einfluss der Standorthöhen aus den Messdaten entfernt werden. Zu diesem Zweck werden die gemessenen Druckwerte auf eine gemeinsame Bezugshöhe, üblicherweise Meereshöhe, umgerechnet. Diese Umrechnung geschieht mittels einer Höhenformel. Das Umrechnen wird auch als Reduktion bezeichnet (auch wenn der Zahlenwert größer wird), da der Messwert dabei von unerwünschten Störeffekten befreit wird. Das Ergebnis ist der "auf Meereshöhe reduzierte Luftdruck" (QNH).

Je nach Genauigkeitsanforderungen muss eine geeignete Höhenformel benutzt werden. Bei geringeren Ansprüchen kann zum Beispiel aus der Höhenformel für konstante Temperatur ein fester Umrechnungsfaktor abgeleitet werden, wozu eine repräsentative Temperatur zu wählen ist:

Für eine Standorthöhe von 500 m und bei Verwendung einer Jahresmitteltemperatur von 6 °C ergibt sich z. B. ein Reduktionsfaktor von 1,063, mit dem die gemessenen Werte zu multiplizieren sind.

Bei etwas höheren Ansprüchen sollte zumindest die aktuelle Lufttemperatur berücksichtigt werden. Deren Einfluss zeigt folgendes Beispiel, in dem ein auf 500 m Höhe gemessener Luftdruck von 954,3 hPa mit der Höhenformel für linearen Temperaturverlauf ("a" = 0,0065 K/m) unter Annahme verschiedener Stationstemperaturen formula_34 auf Meereshöhe reduziert wird:

Verwendung einer falschen Temperatur kann also durchaus zu Abweichungen von einigen hPa führen. Falls eine höhere Genauigkeit gewünscht ist, aktuelle Lufttemperaturen zur Verfügung stehen und Genauigkeit sowie Kalibrierung des verwendeten Barometers den Aufwand rechtfertigen, sollte die Reduktion stets unter Verwendung der aktuellen Lufttemperatur erfolgen. Als Höhenformel bietet sich die Variante für linearen Temperaturverlauf an. Es kann aber ebenso gut die Variante für konstanten Temperaturverlauf verwendet werden, sofern die auf halber Stationshöhe herrschende aktuelle Temperatur eingesetzt wird:

Diese Variante ist zwar theoretisch etwas weniger genau, da sie die Veränderlichkeit der Temperatur mit der Höhe ignoriert, während die lineare Variante diese zumindest ansatzweise berücksichtigt. Bei den für Wetterstationen üblicherweise vorkommenden Höhen und Temperaturen sind die Unterschiede jedoch völlig unbedeutend.

Die vom Deutschen Wetterdienst empfohlene Reduktionsformel entspricht der Variante mit konstantem Temperaturverlauf. Aus der auf Standorthöhe gemessenen Temperatur wird mit Hilfe des Standard-Temperaturgradienten die Temperatur auf halber Standorthöhe geschätzt. Die Luftfeuchte findet Berücksichtigung durch Übergang zur entsprechenden virtuellen Temperatur.

formula_141
mit
Falls keine gemessene Luftfeuchte zur Verfügung steht, kann formula_142 auch gemäß folgender Approximation geschätzt werden, welche auf langjährigen Mittelwerten von Temperatur und Feuchte beruht:

Die hier erhobenen Genauigkeitsanforderungen an gemessenen Luftdruck und Barometerhöhe werden für einen Amateurmeteorologen in der Regel nicht zu erfüllen sein. Bei den Barometern von Hobby-Wetterstationen wird durchaus mit systematischen Fehlern von mindestens 1 bis 2 hPa zu rechnen sein. Einer solchen Unsicherheit entspricht über die barometrische Höhenstufe eine Unsicherheit der Standorthöhe von zehn bis zwanzig Metern. Der Ehrgeiz, die Standorthöhe genauer bestimmen zu wollen, führt höchstwahrscheinlich nicht zu einem genaueren Ergebnis. In diesem Lichte wäre auch die Notwendigkeit oder Überflüssigkeit einer Berücksichtigung der Luftfeuchte zu betrachten.

Gegebenenfalls empfiehlt es sich, nicht die reale Standorthöhe zu verwenden, sondern eine fiktive Höhe, welche die beste Übereinstimmung des reduzierten Luftdrucks mit den Angaben eines nahe gelegenen Referenzbarometers (offizielle Wetterstation, Flughafen usw.) erzielt. Durch eine solche Kalibrierung lässt sich ein eventueller systematischer Fehler des Barometers größtenteils kompensieren. Hierzu ist es zweckmäßig, zunächst eine genäherte Höhe zur Reduktion zu verwenden und die eigenen Ergebnisse über einen längeren Zeitraum (vor allem auch bei verschiedenen Temperaturen) mit den Referenzangaben zu vergleichen. Wird ein systematischer Unterschied festgestellt, kann mit Hilfe einer geeigneten Höhenformel die Höhendifferenz berechnet werden, welche ausgehend von der genäherten Standorthöhe die reduzierten Höhen um den gewünschten Betrag verschiebt. Die auf diese Weise korrigierte Höhe wird dann für künftige Reduktionen verwendet. Wird die Temperatur bei der Reduktion nicht berücksichtigt, sollte beim Kalibrieren die Situation bei einer repräsentativen Temperatur zugrunde gelegt werden.
Einfache Wohnzimmerbarometer werden in der Regel so eingestellt, dass sie unmittelbar den reduzierten Luftdruck anzeigen. Meist geschieht dies durch eine Schraube auf der Gehäuserückseite, mit der sich die Vorspannung der Druckdosenfeder ändern lässt. Die Kalibrierung entspricht also einer Verschiebung der Anzeigeskala. Das ist streng genommen nicht korrekt. Wie die Höhenformeln zeigen, muss die Reduktion durch "Multiplikation" mit einem Kalibrierfaktor erfolgen und nicht durch bloße Addition einer Konstanten: Der reduzierte Luftdruck ändert sich um etwas mehr als ein hPa, wenn sich der Luftdruck auf Standorthöhe um ein hPa ändert. Die Skala müsste also zusätzlich zur Verschiebung auch leicht gestreckt werden. Der dadurch verursachte Fehler ist jedoch geringer als der Fehler, der dadurch entsteht, dass diese Geräte den Temperatureinfluss auf die Reduktion ignorieren. Da sie keine Eingabemöglichkeit für die Standorthöhe haben, kann eine Kalibrierung nur durch Vergleich mit einem Referenzbarometer erfolgen, wodurch wiederum gleichzeitig systematische Nullpunktfehler des Instruments vermindert werden. Die Kalibrierung muss für den Standort des Barometers (oder einen Ort vergleichbarer Höhe) erfolgen. Es hat keinen Zweck, das Gerät beim Händler „richtig einstellen“ zu lassen, wenn es dann an einem völlig anderen Ort aufgehängt wird. Wenn das Barometer dazu dient, aus Luftdruck"änderungen" eine kurzfristige Wettervorhersage abzuleiten, ist eine genaue Kalibrierung weniger wichtig.

Generell ist bei der Reduktion von Luftdruckmessungen zu bedenken, dass die dabei rechnerisch addierte Luftsäule für die meisten Standorte in Wirklichkeit nicht existieren "kann" und es daher auch keinen „wahren“ Wert für den „Meereshöhendruck am Standort“ geben kann, der durch hinreichend aufwendiges Rechnen präzise angenähert werden könnte. Die Reduktionsformeln beruhen zum Teil lediglich auf Konventionen und dienen, abgesehen von speziellen wissenschaftlichen Anwendungen, hauptsächlich dazu, die Messwerte der Wetterstationen so weit wie möglich untereinander vergleichbar zu machen. Ein Beispiel zur Fiktivität der addierten Luftsäule: Über ebenem Gelände, auf dem kalte Luft nicht abfließt, kann sich in klaren Nächten wegen der Wärmeabstrahlung des Erdbodens die bodennahe Luft merklich abkühlen (Bodeninversion). Eine dort befindliche Wetterstation wird diese verringerte Temperatur registrieren und mit dem üblichen Temperaturgradienten rechnerisch nach unten fortsetzen. Befände sich das Gelände aber auf Meereshöhe, so wäre jene Luft wegen des nun fehlenden Erdbodens gar nicht abgekühlt und die nun tatsächlich existierende Luftsäule hätte eine deutlich höhere Temperatur als die rechnerische. Die Rechnung hat also eine zu hohe Dichte der addierten Luftsäule angenommen und ergibt einen höheren reduzierten Luftdruck, als er bei derselben Wetterlage herrschen würde, falls das gesamte Gelände auf Meereshöhe läge.

Die Höhenabhängigkeit des Luftdrucks kann auch zur Höhenmessung verwendet werden. Barometrische Höhenmessungen sind schnell und relativ einfach durchzuführen, in ihrer Genauigkeit jedoch begrenzt. Ein für die Höhenbestimmung ausgelegtes Barometer bezeichnet man als Höhenmesser oder Altimeter. Die Vorgehensweise richtet sich nach Verwendungszweck und Genauigkeitsansprüchen. Anwendung findet das Verfahren unter anderem beim Wandern und mit etwas höheren Genauigkeitsansprüchen in der Landvermessung.





</doc>
<doc id="9515" url="https://de.wikipedia.org/wiki?curid=9515" title="Lawine">
Lawine

Als Lawinen werden Massen von Schnee oder Eis bezeichnet, die sich von Berghängen ablösen und zum Tal gleiten oder stürzen. Lawinen, die große Sach-, Personen- oder Umweltschäden verursachen, werden zu den Naturkatastrophen gezählt.

Das Wort «Lawine» geht auf alpinromanisch "lavīna" beziehungsweise lateinisch "labīna" ‚Erdrutsch‘ zurück; diesem zugrunde liegt das lateinische Verb "lābi" ‚gleiten‘. Im Altoberdeutschen des 8./9. Jahrhunderts diente "lewina, lewin" oder "louwin" als Übersetzung für lateinisch "torrēns" ‚Wildbach‘; in den hoch- und höchstalemannischen Mundarten, wo aus alpinromanisch "lavīna" Laut- und Formvarianten wie "Lauwene, Laubene, Lauene" und ähnlich wurden (mit Betonung je auf der ersten Silbe), erlangte das Wort im Laufe des Mittelalters in erster Linie die Bedeutung «Schneerutsch». Die Schweizer Humanisten versuchten das für sie nicht mehr durchsichtige Wort hingegen an „Löwin“ anzuschließen.

Im 18. Jahrhundert wurde das schweizerdeutsche "Lauwene, Laubene, Lauene" durch die Reiseliteratur im ganzen deutschen Sprachraum bekannt. Dass sich dabei die relatinisierte Form "Lawine" (mit Betonung auf der mittleren Silbe) durchsetzte, ist im Wesentlichen auf Friedrich Schillers 1804 uraufgeführtes Drama "Wilhelm Tell" zurückzuführen.

Das im deutschen Sprachraum regional gebräuchliche Wort "Lahn, Lähn" und ähnlich (vgl. den Bergnamen Lahnerkopf) und davon abgeleitet "Lahngang" ‚Lawinenhang‘ ist mit Lawine zwar urverwandt, ist aber nicht wie dieses ein Lehnwort aus dem Romanischen, sondern ein deutsches Erbwort.

Spätestens seit der Mensch den alpinen Lebensraum erschlossen hat, ist er von Lawinenabgängen bedroht. Aus der Literatur sind vor allem Heerzüge, die die Alpen überquerten, als betroffen bekannt. So verlor Hannibal auf seiner Alpenüberquerung im Jahre 218 v. Chr. angeblich rund die Hälfte seiner Soldaten (etwa 20.000 Mann) und eine unbekannte Anzahl von Elefanten durch Lawinen.

Abhängig von der Art ihres Abgangs unterscheidet man zwei grundsätzliche Arten von Schneelawinen, und zwar nach der Art ihres Anrisses "Schneebretter" und "Lockerschneelawinen", daneben teilt man sie auch nach ihrem Umfang und Ausmaß ein.
Eine Dachlawine ist eine Schneelawine im Kleinen, die von Gebäuden abgeht.

Zu unterscheiden sind Lawinen vom "Eissturz".

Kennzeichen für Schneebrettlawinen ist ein linienförmiger Anriss quer zum Hang. Ausgedehnte Schichten der Schneedecke – oft aus Triebschnee – rutschen auf einer Gleitschicht zunächst zusammenhängend ab. Im Verlauf des Abgangs kann sich eine Schneebrettlawine zu einer Staublawine entwickeln. Eine solche Gleitschicht kann beispielsweise durch den Nigg-Effekt entstehen.

Sie stellen die klassische Gefahrenlawine für Schneesportler und Bergsteiger dar. Gefahren für Opfer einer solchen Lawine sind Ersticken, Verletzungen durch Aufprall an Felsen, Absturz oder der Druck der oft tonnenschweren Schneemassen.

Schneebrettlawinen treten regelmäßig bei Hangneigungen zwischen 30° und 50° auf. Sie sind aber auch bei einer Hangneigung ab etwa 25° möglich. Bei Hangneigungen über 50° sind Schneebrettlawinen selten, bei diesen Hangneigungen treten in der Regel vorher Lockerschneelawinen auf.

Der Begriff Schneebrett ist etwas irreführend, suggeriert er doch eine harte Beschaffenheit. In der Realität kommen jedoch auch in sehr weichem, schwer erkennbarem Triebschnee flächige Lawinenauslösungen vor: Der Begriff beschreibt, dass eine ganze Schneemasse auf einmal losfährt. In wissenschaftlichen Studien wird untersucht, wie sich in Schwachschichten Brüche ausbreiten, sodass ein ganzer Hang auf einmal abbricht. Mit der Bewegung zerbricht das Brett dann in kleinere Teile, die sich im Auslauf übereinander schieben und als ein verfestigter Lawinenkegel (oder eine verfestigte Ablagerung) liegen bleiben.

Es wird zwischen trockenen und nassen Schneebrettlawinen unterschieden.

Eine Lockerschneelawine ist durch einen punktförmigen Anriss gekennzeichnet. Durch eine Kettenreaktion wächst die Lawine. Solche Lawinen kommen vor allem in unverfestigtem Schnee vor. Es wird weiter in "trockene Lockerschneelawinen" und in "nasse Lockerschneelawinen" (oberflächliche Nässung) unterteilt. Lockerschneelawinen verlangen, wegen der zur Fortpflanzung der Bewegung notwendigen Energie, eine etwas höhere Hangneigungen als Schneebrettlawinen. Ein häufiges Auftreten wird bei etwa 40–60° Hangneigung beobachtet.

Staublawinen entstehen, wenn eine große Schneemasse einen steilen Hang hinabstürzt und dabei weiteren Schnee aufnimmt. Der Schnee wird aufgewirbelt, sodass ein Schnee-Luft-Gemisch (Aerosol) entsteht. Eine Staublawine kann eine Geschwindigkeit von über 300 km/h erreichen.

Einher mit der Staublawine gehen gewaltige Luftdruckschwankungen (Druck vor der Front, dahinter Sog), die sehr gefährlich sind. Durch diese Druckschwankungen, die den Bedingungen in einem Wirbelsturm gleichen können, kommt es zu den großen Zerstörungen. Bäume werden abgeknickt, Hausdächer weggerissen und Fenster eingedrückt, wodurch Schnee in das Haus eindringt. Gelangt das Schnee-Luft-Gemisch in die Lunge von Menschen oder Tieren, so kann dies nach kurzer Zeit zum Tode durch Ersticken führen. Zudem ist der Fließanteil von Staublawinen gefährlich, da er zu Verschüttungen führen kann.

Eislawinen sind eine Folge der langsamen Gletscherbewegungen. Das Eis bewegt sich bis zum Rand eines Abbruchs und stürzt in einzelnen Brocken darüber hinaus. Dies gleicht zunächst mehr einer Steinlawine als den bekannten Schneelawinen, doch dann werden die herabstürzenden Eisbrocken beim Aufprall in feine Schneepartikel zerschlagen und sind kaum mehr von einer Fließlawine zu unterscheiden.

Sind berechenbare Grundlawinen, die vor allem im Frühjahr bei Tauwetter losbrechen. Der weiche Schnee verliert schneller an Haftung und rutscht den Berg herunter.

Die oben genannte Einteilung kann noch verfeinert werden:

"Hanglawinen" erreichen im Gegensatz zu "Tallawinen" nicht den Fuß des Hangs (bzw. das Tal), sondern kommen im Hang zum Stillstand.

An der Entstehung einer Lawine sind viele Faktoren beteiligt, die sich gegenseitig verstärken oder abschwächen können. Man kann die Entstehung einer Lawine nicht unabhängig von der Art der Lawine betrachten, da es sich z. B. bei Schneebrettern und Nassschneelawinen um ziemlich unterschiedliche Prozesse handelt. Auch die Gefahrenbeurteilung erfolgt darum je nach Lawinenart unterschiedlich.

Eine große Menge Neuschnee innerhalb einer kurzen Zeit erhöht die Lawinengefahr. Während bei sehr günstigen Verhältnissen (d. h. wenig Wind, recht hohe Temperaturen, stabile bestehende Schneedecke) bis zu 50 cm Schnee fallen können, bevor die Lawinengefahr ansteigt, kann dies bei ungünstigen Verhältnissen (starker Wind → zusätzlicher Treibschnee, sehr tiefe Temperaturen, unstabile bestehende Schneedecke) schon bei 10 cm der Fall sein.

Die Gefahr eines Lawinenabgangs besteht vor allem bei Hangneigungen zwischen 30° und 50°, wobei stärkere Hangneigung einen Lawinenabgang im Allgemeinen begünstigt – vergleiche hierzu die Kräfte an der Schiefen Ebene. Maßgeblich dabei ist die steilste Stelle (ca. 10 m × 10 m) im Hang. Unter 25° Grad Hangneigung entstehen Lawinen nur sehr selten, beziehungsweise nur unter besonderen Umständen. Bei 60° Hangneigung oder mehr sind Lawinen fast unmöglich, da der Schnee schon früh und spontan abrutscht. Bedeutende Schneemengen können sich gar nicht ansammeln.

Die Neigung des Geländes ist auch bezüglich der Sonneneinstrahlung relevant: Fällt das Licht mehr oder weniger rechtwinklig auf den Boden, nimmt der Schnee mehr Wärme auf, als wenn die Sonne im flachen Winkel auf den Schnee scheint. Dies spielt zum Beispiel bei Nassschneelawinen eine Rolle.

Eine wesentliche Rolle spielt die Hanglage. Nord-Hänge sind (in nördlichen Breiten) der Sonneneinstrahlung am wenigsten ausgesetzt, wodurch sich die Stabilisierung der Schneedecke verlangsamt und Gefahrenstellen länger konserviert werden. Umgekehrt sind Südhänge im späten Winter heikler, da größere Wärme Nassschnee-Lawinen begünstigt.

Je nach Windsituation sammelt sich Triebschnee auch an spezifischen Hanglagen.

Die Bodenbedeckung ist ein weiterer Faktor, der die Entstehung von Lawinen beeinflusst. Dichter Wald kann den Abgang von Schneebrettern erschweren, umgekehrt begünstigt Altgras u. ä. den Abgang von Grundlawinen, eingeschneiter Reif oder Eisschichten begünstigen Oberlawinen. Der Wald kann das Anreißen von Lawinen verhindern, aber große Staublawinen nicht stoppen.

Schnee kann durch den Wind verfrachtet werden. Dieser so genannte Triebschnee lagert sich auf der windabgewandten Seite von Graten, in Rinnen und Mulden oder am windzugewandten Fuß von Hängen ab. An Graten bildet er Schneewehen und Schneewechten. Dieser verfrachtete Schnee ist instabil, und bereits kleinste Störeinflüsse können zu einer Schneebrettlawine führen. Triebschnee kann sowohl hart als auch weich sein und ist dadurch nicht einfach zu erkennen. Auch nach dem Einschneien durch nachfolgenden Neuschnee behält er sein Gefahrenpotenzial bei. Triebschnee ist gefährlich, weil die Eiskristalle abgeschliffen sind, und sich nicht ineinander verzahnen. Der Zusammenhalt von Triebschnee ist somit viel geringer als jener von Neuschnee. Sogenannte Sastrugi können auf Triebschnee-Ansammlungen hinweisen.

Wenn viel Schnee in kurzer Zeit auf einem Hang zu liegen kommt, wächst die Belastung der Schneedecke durch das zusätzliche Gewicht schneller als die Setzung und Verfestigung voranschreiten kann. Der Druck auf die unteren Schichten wird so groß, dass diese der Belastung nicht mehr standhalten. Bereits geringe Zusatzbelastung, z. B. das Gewicht eines Skifahrers, kann dazu führen, dass die Schneeschichten ins Rutschen geraten und es zu einem Lawinenabgang kommt. Besonders instabil sind Schneedecken mit großen Festigkeitsunterschieden zwischen den Schichten oder eine schwache Schneedecke, die das erste Mal durchfeuchtet wird. In die Schneedecke eingelagerte Zwischenschichten (z. B. Raureif, Schwimmschnee oder Eislamellen) tragen zur Verschärfung der Situation bei und bilden die Gleithorizonte, auf denen die darüber liegende Schneedecke abrutscht.

Je tiefer die Temperatur ist, desto länger dauert es, bis sich Schichten aus Neuschnee verfestigen. Jedoch destabilisieren rasch steigende Temperaturen die Schneedecke, und erhöhen so die Lawinengefahr. Diese Instabilität entsteht durch Umwandlung der Schneekristalle oder Durchfeuchtung bis auf den Grund; so entstehen insbesondere Nassschneelawinen. Grundsätzlich hat aber jeder Temperaturwechsel eine Veränderung der Lawinensituation zur Folge. Die Lawinengefahr ist am geringsten, wenn milde Temperaturwechsel die Verfestigung des Schnees beschleunigen.

Früher glaubte man, Lawinen würden von Hexen oder Geistern ausgelöst oder wären eine Strafe Gottes. Im Spätmittelalter kam natürlichen Ereignissen als Auslöser von Lawinen größere Aufmerksamkeit zu, wie z. B. laute Geräusche oder das Werfen von Objekten (Schneebällen) auf einen lawinengefährdeten Hang.

Heute werden Lawinen wissenschaftlich erforscht, und zwar durch Modellversuche im Labor und Gelände, Computersimulationen oder durch künstlich ausgelöste Lawinen (z. B. am WSL-Institut für Schnee- und Lawinenforschung SLF in Davos).

Um die Lawinengefahr möglichst korrekt einschätzen zu können, müssen Feldversuche unternommen werden. Dazu gehört z. B. das Erstellen von Schneeprofilen, um die verschiedenen Schichten und Formen der Schneekristalle zu analysieren, oder das Anlegen von Rutschblöcken. Lawinenforscher stützen sich auch auf meteorologische Daten, um so eine Aussage über die Art des Schnees machen zu können, was wiederum Einfluss auf die Lawinenbildung hat.

Ungefähr seit dem Jahr 2000 versucht man, Satellitenbilder in die Lawinenforschung zu integrieren. Aus dem Vergleich von Bildern, die in verschiedenen Wellenlängen des elektromagnetischen Spektrums aufgenommen wurden, kann man auf die Art der Schneekristalle schließen, weil jede Schneeart das Licht unterschiedlich stark reflektiert. Somit kann man die Schneedichte sowie Temperatur, Wasser- und Luftgehalt bestimmen. Der Nachteil der Satellitenbilder ist, dass sie nur die oberste Schneeschicht zeigen, was eine eingehendere Analyse der Lage erschwert.

In der Forschung werden noch viele weitere Methoden eingesetzt, um die Schneedecke, ihre Wechselwirkung mit der Atmosphäre sowie die Entstehung und Dynamik von Lawinen zu untersuchen und Maßnahmen zum Lawinenschutz bzw. Risikomanagement zu entwickeln. Dazu gehören z. B. Messinstrumente wie Radar, SnowMicroPen oder Nahe-Infrarot-Kameras, mit denen die Schichtung der Schneedecke analysiert wird, sowie seismische, akustische und optische Sensoren, mit denen Lawinenabgänge detektiert werden. Ebenso werden Computermodelle verwendet, die die Schneedecke simulieren (Snowpack, bzw. Alpine 3D) oder Lawinenabgänge berechnen (RAMMS) und wichtige Informationen für die Lawinenwarnung oder die Berechnung von Lawinengefahrenzonen liefern.

In den Alpenländern, den USA, Kanada und Japan wird ein großer Aufwand betrieben, um die Bevölkerung vor Lawinenabgängen zu schützen.

Die aktuelle Lawinengefahr für ein bestimmtes Gebiet wird in den Gefahrenstufen 1 bis 5 in der europäischen Lawinengefahrenskala angegeben. Diese aktuelle Lawinenwarnstufe wird in den Alpenländern von den Lawinenwarndiensten jeden Tag bekanntgegeben. Örtliche Lawinenkommissionen beraten die Behörden hinsichtlich der Erforderlichkeit von Schutzmaßnahmen für Siedlungen, Skigebiete und Verkehrswege.

Die Lawinengefahr kann jeweils nur anhand der lokalen Gegebenheiten an einem potentiellen Lawinenhang beurteilt werden. Der Beurteilung liegen
zu Grunde.

Lawinenschutz kann aufgrund der Eingriffsart in aktive und passive Maßnahmen eingeteilt werden:

"Passive Schutzmaßnahmen" dienen größtenteils der Prävention. So können in lawinengefährdeten Gebieten Baugenehmigungen entzogen werden oder Evakuierungen angeordnet werden. Zu den passiven Maßnahmen gehören auch Lawinengalerien sowie Umlenk- und Bremsverbauten zum Schutz von Straßen, Brücken und Bauwerken.

"Aktive Schutzmaßnahmen" sollen dem Entstehen von Lawinen vorbeugen. Den kostengünstigsten Schutz bieten Wälder. Deshalb gibt es besondere Aufforstungsprogramme (siehe dazu Schutzwald). Sind keine Bäume vorhanden, werden künstliche Schutzbauten (Lawinenverbauungen) erstellt. Dazu werden in Hängen, aus denen Lawinen abgehen können, Netze, Gitter oder windbrechende Barrieren aus Holz, Beton oder Stahl montiert. Dadurch wird die Schneedecke entweder unterteilt, so dass sich keine großen Schneebretter ablösen können, oder Schneeanhäufungen an kritischen Punkten werden verhindert. Auch "Lawinensprengungen" gehören zu dieser Maßnahmengruppe. Mit Hilfe von Hubschraubern, Kanonen oder Seilbahnsystemen wird Sprengstoff an kritische Stellen befördert, von fest installierten Masten abgeworfen oder die Schneedecke wird durch Zündung eines explosiven Gasgemisches destabilisiert, um kleine kontrollierte Lawinen auszulösen. Dadurch wird die Schneedecke entlastet und man kommt unkontrollierten Lawinenabgängen zuvor.

Lawinenschutzmaßnahmen können im Hinblick auf die Wirkungsweise auch in permanente und temporäre Schutzmaßnahmen eingeteilt werden.

Temporäre Lawinenschutzmaßnahmen werden kurzfristig eingesetzt und auf Zeitpunkt, Ort und Ausmaß der Lawinengefahr abgestimmt. Dabei entscheiden auf Basis von Lawinenwarnung, Lagebeobachtung, -prognose und -berichten örtliche Lawinenkommissionen oder andere Gremien über

Unter permanentem Lawinenschutz versteht man technische, forstlich-biologische und raumplanerische Maßnahmen sowie die Aufklärung von betroffenen und interessierten Personenkreisen über Schnee- und Lawinenvorgänge.


Warnsysteme können Lawinen erkennen welche sich langsam entwickeln, z. B. Eislawinen bei Eisabbrüchen von Gletschern. Mittels interferometrischen Radaren, hochauflösende Kamerasystemen oder Bewegungssensoren kann ein instabiles Gebiet über einen langen Zeitraum von einigen Tagen hin zu Jahren beobachtet werden. Durch die Interpretation der Daten können Experten bevorstehende Abbrüche erkennen und Maßnahmen veranlassen. Mittels solchen Systemen (z. B. die Gletscherüberwachung am Weissmies in der Schweiz) können Ereignisse einige Tage im Voraus erkannt werden. 

Moderne Radartechnologie erlaubt es, große Gebiete zu überwachen und Lawinen bei allen Witterungsbedingungen bei Tag oder Nacht zu lokalisieren. Komplexe Alarmsysteme können in kürzester Zeit die Lawine detektieren und so ein gefährdetes Gebiet unmittelbar und automatisch sperren (z. B. Straßen und Bahnen) oder evakuieren (z. B. Baustellen). Ein solches Projekt befindet sich beispielsweise in der Schweiz auf der einzigen Zufahrt nach Zermatt. Zwei Radare überwachen eine Bergflanke unter welcher die Zufahrtsstraße durchführt. Im Fall einer Lawine wird die Straße mit mehreren Barrieren und Ampeln automatisch und innert Sekunden gesperrt, so dass keine Personen zu Schaden kommen können.

Lawinen bedrohen nicht nur Siedlungen, sondern auch den Menschen, der sich in der Natur bewegt. Vor allem durch Schneebrettlawinen werden regelmäßig Skitourengeher, Snowboarder, Schneeschuhgeher und andere Wintersportler erfasst. Allein in der Schweiz sterben jeden Winter durchschnittlich 25 Personen in Lawinen, der große Teil davon hat die Lawine selber ausgelöst. Die meisten Opfer waren allerdings zu beklagen, wenn große Lawinen Dörfer trafen und wie im Lawinenwinter 1950/51 die Leute in ihren Häusern überraschten.

Im verschneiten alpinen Gelände ist eine potentielle Lawinengefahr gegeben. Das gesicherte Skigebiet zu verlassen bedeutet ein gewisses Risiko in Kauf zu nehmen. Viele alpine Wintersportarten nutzen aber gerade den Naturraum als Handlungsfeld. Das erfordert eine präventive Auseinandersetzung mit dem Risikofaktor durch strategische Entscheidungssysteme, auch bezeichnet als strategische Lawinenkunde. Strategische Lawinenkunde ist der systematische Umgang mit dem Lawinenrisiko innerhalb eines Risikomanagements. Als wegweisend zu ihrer Entwicklung war die Anfang der 1990er Jahre entwickelte Formel 3×3 und elementare Reduktionsmethode nach Munter.

Die Komplexität der Faktoren, die zur Lawinenbildung führen (speziell in der Schneedecke), überfordern die kognitiven Fähigkeiten des Menschen. Trotzdem muss eine „JA-oder-NEIN“ Entscheidung für die Begehung eines Hanges getroffen werden. Wichtig ist dabei, dass nicht nur Experten, sondern auch laienhafte Winterbergsteiger solche Entscheidungen treffen müssen. Je komplexer eine Entscheidung, desto wichtiger ist es, einfache Entscheidungs- und Handlungskonzepte parat zu haben. Dies geschieht durch die Anwendung von Risikomanagement-Systemen und Entscheidungsstrategien, die wahrscheinlichkeitsorientiert arbeiten. Um das Risiko entsprechend einschätzen zu können, sind ausreichendes Wissen, Kompetenz und Erfahrung nötig. Eine gute körperliche Kondition ermöglicht es, entsprechende Entscheidungen auch umsetzen zu können.

Weiterhin zählt zur Notfallprävention eine ausreichende, zweckmäßige und erprobte Sicherheitsausrüstung. Dabei haben sich folgende Geräte als Mindeststandard für jeden Winterbergsteiger etabliert:
Ergänzend dazu existieren der Avalanche-Ball, Lawinenairbag und die Avalung. Durch Einhalten von Sicherheitsabständen, gute Spuranlage und vorsichtige Fahrweise bei der Abfahrt in einem Hang kann das Risiko weiter minimiert werden. Halteriemen von Stöcken und Ski sollten vor einer Abfahrt gelöst werden, da sie im Verschüttungsfall den Sportler nach unten ziehen können.

Von behördlicher Seite können Präventionsmaßnahmen wie zunächst die Sperrung einzelner Gebiete, später auch kontrolliertes Auslösen von Lawinen durch Sprengung (Lawinen-Sicherungstrupps) in Betracht kommen.

Wenn man von einer Lawine erfasst zu werden droht, kann man auf mehrere Handlungsoptionen zurückgreifen, die allerdings keine Erfolgsgarantie beinhalten. Es erhöht jedenfalls die Überlebenschancen, wenn der Wintersportler möglichst wenig tief verschüttet wird und eine "Atemmöglichkeit" hat. Eine früher häufig empfohlene „Schussflucht“ (also das schnelle Fahren in der Falllinie, um der Lawine zu enteilen) scheint nur selten erfolgreich gewesen zu sein, da Lawinen generell sehr schnell sind und oft der komplette Hang aufbricht. Falls man sich am Rand eines Lawinenhangs befindet, kann man versuchen, durch schnelle Fahrt weg von den Schneemassen das Verschüttungsrisiko zu mindern. Auch ein geschicktes „Reiten“ mit Ski auf der Lawine dürfte nur wenigen Personen geglückt sein. Ebenfalls empfohlene „Schwimmbewegungen“ in den Schneemassen sind nach Aussagen von Verschütteten sinnlos. Erfolgversprechender ist, mitgeführte Rettungsmittel sofort zu aktivieren. Dies sind zum Beispiel ein "„Lawinen-Airbag“" (durch Ziehen am Auslösegriff wird eine Gaspatrone gezündet, welche einen oder mehrere Luftkissen am Rucksack aufbläst), der eine tiefe Verschüttung verhindern kann oder die "„Avalung“" (man nimmt eine Art Schnorchel in den Mund und kann so auch unter dem Schnee in der Regel atmen – die Ausatemluft wird am Rücken abgeleitet), welche die Erstickungsgefahr verringert. Ski, Snowboard und Stöcke wirken wie ein Anker innerhalb einer Lawine und können eine Person tiefer in die Schneemassen hinein ziehen. Deswegen sollte der Sportler versuchen sein/e Ski/Snowboard zu lösen und die eventuell vorhandenen Stöcke wegzuwerfen. Das Verwenden von Fangriemen ist in diesem Kontext zu vermeiden, da sie wie eine Ankerkette wirken können.

Oft sind weitere Personen vor Ort, die nicht vom Lawinenabgang betroffen sind. Da die Überlebensrate von Lawinenverschütteten schnell abnimmt, kann die „Kameradenhilfe“ durch Anwesende lebensrettend sein. Die organisierte Bergrettung benötigt schon aufgrund der Alarmierungs- und Ausrückezeiten meist länger als eine Viertelstunde bis zur Ankunft. Die Hilfe vor Ort beginnt mit einer möglichst genauen Beobachtung der Verschüttung. Die Registrierung von Erfassungspunkt und Verschwindepunkt ermöglicht Rückschlüsse auf den primär abzusuchenden Bereich. Parallel sollte ein korrekter Notruf abgesetzt werden.

Unter Beachtung des Eigenschutzes (Nachlawinen) muss dann zügig die Rettung eingeleitet werden. Man sucht die Lawinenoberfläche nach dem Stillstand zuerst nach Kleidungsstücken oder Ausrüstungsteilen ab. Mancher Teilverschüttete kann so gefunden werden. Gleichzeitig sucht man mit elektronischen LVS-Geräten. Es ist sicherzustellen, dass alle Teilnehmer vor der Suche ihre LVS von Senden auf Empfangen umschalten, um sich nicht gegenseitig zu orten. Nach der Ortung des Verschütteten setzt man Lawinensonden ein, um den Standort noch genauer zu lokalisieren. Da man mit der Lawinensonde auch die Verschüttungstiefe feststellt, kann man unterhalb der Sonde zu graben beginnen und sich waagrecht zum Verschütteten vorarbeiten. Man achtet darauf, ob eine Atemhöhle vorhanden war und beginnt mit Maßnahmen der Ersten Hilfe. Falls der Patient unterkühlt ist, muss er vorsichtig geborgen werden. Wird er zu stark bewegt und dadurch der Kreislauf angeregt, fließt unterkühltes und äußerst sauerstoffarmes Blut in Richtung der inneren Organe. Es droht der sog. Bergungstod. Der Bergrettungsdienst kann neben den oben genannten Hilfsmitteln auch – falls vorhanden – das RECCO-System und Lawinensuchhunde einsetzen. Der Einsatz von Lawinenhunden wäre am sinnvollsten gleich zu Anfang, bevor noch ein Mensch den Lawinenkegel betreten hat, was aber in den seltensten Fällen zu verwirklichen ist.

In den letzten 100 Jahren gab es in den Alpen im Schnitt jährlich 100 Tote durch Lawinenabgänge. Einige besonders schwere Unglücke weltweit sind hier verzeichnet.

„Die Wahrscheinlichkeit, eine Verschüttung länger als zwei Stunden zu überleben, liegt bei drei bis zehn Prozent.“ (erhoben im Alpenraum). Versorgung mit Sauerstoff, der etwa durch lockeren Schnee von unten nachsickert, ist eine gute Voraussetzung, dem Verschütteten zu helfen, seine Körpertemperatur möglichst lange möglichst wenig absinken zu lassen. Rückatmung von ausgeatmetem Kohlenstoffdioxid in einer abgeschlossenen Atemhöhle führt zu Bewusstlosigkeit. Unter 32 °C Körpertemperatur kommt es zu Herzrhythmusstörungen, unter 24 °C erlöschen Lebensfunktionen meist dauerhaft. Wiedererwärmen einer tief (unter 30 °C) unterkühlten Person ist ein intensivmedizinischer Prozess, der stunden- bis tagelang dauern kann. Am längsten überlebte eine Frau, die 1974 in der Lombardei 48 Stunden verschüttet war.

Auch bei anderen Phänomenen spricht man von lawinenartigen Vorgängen, wenn die Vorgänge selbstverstärkend sind. Diese Vorgänge haben wie Schneelawinen, Eislawinen oder Schlammlawinen gemeinsame Verhaltenstypen („Universalität“, „Selbstorganisation“). Zur Auslösung solcher Vorgänge reichen schwer zu kontrollierende kleine Ursachen.

Quantitative physikalische Theorien dazu hat der dänische Physiker Per Bak aufgestellt.



In Österreich besteht seit Mitte Dezember 2011 ein eigenes technisches Regelwerk, in dem der "Stand der Technik im Lawinenschutzbau zusammengefasst wurde.


Glossar Lawinen
Lawinenforschung:
Lawinengefahr:
Lawinenschutz:
Lawinen und Recht:


</doc>
<doc id="9519" url="https://de.wikipedia.org/wiki?curid=9519" title="Endlicher Automat">
Endlicher Automat

Ein endlicher Automat (EA, auch Zustandsmaschine, Zustandsautomat; , "FSM") ist ein Modell eines Verhaltens, bestehend aus Zuständen, Zustandsübergängen und Aktionen.

Ein Automat heißt endlich, wenn die Menge der Zustände, die er annehmen kann (später S genannt), endlich ist. Ein endlicher Automat ist ein Spezialfall aus der Menge der Automaten. Ein Zustand speichert die Information über die Vergangenheit, d. h. er reflektiert in gewissem Umfang die Änderungen der Eingabe seit dem Systemstart bis zum aktuellen Zeitpunkt. Ein Zustandsübergang zeigt eine Änderung des Zustandes des EA und wird durch logische Bedingungen beschrieben, die erfüllt sein müssen, um den Übergang zu ermöglichen. Eine Aktion ist die Ausgabe des EA, die in einer bestimmten Situation erfolgt. Es gibt vier Typen von Aktionen:


Ein EA kann als Zustandsübergangsdiagramm wie in Abbildung 1 dargestellt werden. Zusätzlich werden mehrere Typen von Übergangstabellen (bzw. Zustandsübergangstabellen) benutzt. Die folgende Tabelle zeigt eine sehr verbreitete Form von Übergangstabellen: Die Kombination aus dem aktuellen Zustand (B) und Eingabe (Y) führt zum nächsten Zustand (C). Die komplette Information über die möglichen Aktionen wird mit Hilfe von Fußnoten angegeben. Eine Definition des EA, die auch die volle Ausgabeinformation beinhaltet, ist mit Zustandstabellen möglich, die für jeden Zustand einzeln definiert werden (siehe auch virtueller EA).

Die Definition des EA wurde ursprünglich in der Automatentheorie eingeführt und später in der Computertechnik übernommen.

Zustandsmaschinen werden hauptsächlich in der Entwicklung digitaler Schaltungen, Modellierung des Applikationsverhaltens (Steuerungen), generell in der Softwaretechnik sowie Wort- und Spracherkennung benutzt.

Generell werden zwei Gruppen von EA unterschieden: Akzeptoren und Transduktoren.

Sie akzeptieren und erkennen die Eingabe und signalisieren durch ihren Zustand das Ergebnis nach außen. In der Regel werden Symbole (Buchstaben) als Eingabe benutzt. Das Beispiel in der Abbildung 2 zeigt einen EA, der das Wort „gut“ akzeptiert. Akzeptoren werden vorwiegend in der Wort- und Spracherkennung eingesetzt.

Transduktoren generieren Ausgaben in Abhängigkeit von Zustand und Eingabe mit Hilfe von Aktionen. Sie werden vorwiegend für Steuerungsaufgaben eingesetzt, wobei grundsätzlich zwei Typen unterschieden werden:



Sofern das zeitliche Verhalten unberücksichtigt bleiben kann, sind Moore- und Mealy-Automaten gleichwertig. Unter dieser Voraussetzung kann der eine in den jeweils anderen überführt werden; oft werden in der Praxis Mischmodelle benutzt. Im Bereich des synchronen Systemdesigns (Digitalelektronik) dagegen gibt es wichtige Unterschiede, die nicht außer acht gelassen werden dürfen. Diese betreffen sowohl die unterschiedliche Zahl von Zuständen als auch die zeitliche Charakteristik der generierten Kontrollsignale.

Eine weitere Klassifizierung der EA wird durch die Unterscheidung zwischen deterministischen (DEA) und nicht-deterministischen (NEA) Automaten gemacht. In den deterministischen Automaten existiert für jeden Zustand genau ein Übergang für jede mögliche Eingabe. Bei den nicht-deterministischen Automaten kann es keinen oder auch mehr als einen Übergang für die mögliche Eingabe geben.

Ein EA, der nur aus einem Zustand besteht, wird als kombinatorischer EA bezeichnet. Er benutzt nur Eingabeaktionen.

Der nächste Zustand und die Ausgabe des EA ist eine Funktion der Eingabe und des aktuellen Zustandes. Abbildung 5 zeigt den Ablauf der Logik.

Es gibt unterschiedliche Definitionen, je nach Typ des DEA. Ein Akzeptor ist ein 5-Tupel (Q, s, Σ, F, δ,), wobei:

Ein Transduktor ist ein 6-Tupel (Σ, Γ, S, s0, δ, ω), wobei:

Falls die Ausgabefunktion eine Funktion von Zustand und Eingabealphabet ist (ω: S x Σ → Γ), dann handelt es sich um ein Mealy-Modell. Falls die Ausgabefunktion nur vom Zustand abhängt (ω: S → Γ), dann ist es ein Moore-Modell.

Ein EA wird optimiert, indem die Zustandsmaschine mit der geringsten Anzahl von Zuständen gefunden wird, die die gleiche Funktion erfüllt. Dieses Problem kann zum Beispiel mit Hilfe von Färbungsalgorithmen gelöst werden.

Eine Homing-Folge (auch Homing-Sequenz) ist eine Folge von Eingaben, sodass sich anhand der Ausgaben bestimmen lässt, in welchem Zustand sich die Maschine danach befindet. Dadurch kann bei stark zusammenhängenden Zustandsmaschinen sehr leicht eine Folge gefunden werden, um wieder zum Initialzustand zurückzukehren, also nach Hause (englisch "home"). Jede minimale Zustandsmaschine besitzt eine Homing-Folge.

Eine UIO-Folge (Unique-Input-Output-Folge) ist eine Folge von Eingaben, um anhand der Ausgaben zu bestimmen, aus welchem Zustand man gestartet ist. Eine solche Folge existiert nicht immer, das Problem, eine zu finden, ist PSPACE-vollständig.

Beispiel UIO-Folgen zu Abb. 6:

In digitalen Schaltungen werden EA mit Hilfe von speicherprogrammierbaren Steuerungen, logischen Gattern, Flip-Flops oder Relais gebaut. Eine Hardwareimplementation benötigt normalerweise ein Register, um die Zustandsvariable zu speichern, eine Logikeinheit, die die Zustandsübergänge bestimmt, eine zweite Logikeinheit, die für die Ausgabe verantwortlich ist, sowie einen Taktgeber oder ein Verzögerungsglied, um zwischen vorherigem, aktuellem und nachfolgendem Zustand weiterschalten/unterscheiden zu können.

In der Softwareentwicklung werden meist folgende Konzepte verwendet, um Applikationen mit Hilfe von Zustandsmaschinen zu modellieren bzw. implementieren:

Alle in der wirklichen Welt existenten digitalen Computer haben eine endliche Speichergröße und können somit nur eine endliche (wenn auch sehr hohe) Zahl von digitalen Schaltzuständen annehmen. Sie lassen sich daher als Teilmenge der endlichen Automaten betrachten. Jedoch ist es für theoretische Betrachtungen oft nützlicher, sie stattdessen als Teilmenge leistungsfähigerer Automatenmodelle, wie etwa der Turingmaschine, zu betrachten.

Die allgemeinen Regeln für das Zeichnen eines Zustandsübergangsdiagramms sind wie folgt:






</doc>
<doc id="9524" url="https://de.wikipedia.org/wiki?curid=9524" title="Origami">
Origami

Das Origami ([], ) (jap. [] ; von "oru" für „falten“ und "kami" für „Papier“) ist die Kunst des Papierfaltens. Ausgehend von einem zumeist quadratischen Blatt Papier entstehen durch Falten zwei- oder dreidimensionale Objekte wie z. B. Tiere, Papierflieger, Gegenstände und geometrische Körper. 

Bereits vor der Erfindung des Papiers (rund 100 v. Chr. in China) wurden Stoffe und andere Materialien gefaltet.

Im Jahr 610 wurde das Papier durch buddhistische Mönche von China nach Japan gebracht, wo das Papierfalten in der Muromachi-Zeit (1333–1568) eine erste und in der Edo-Zeit (1603–1868) eine zweite Blüte erlebte. Da Papier zu jener Zeit recht teuer war, war es vermutlich zeremoniellen Faltungen vorbehalten, wie etwa für Noshi in der Muromachi-Zeit.

Unabhängig davon entwickelte sich die europäische Papierfaltkunst, die sich von Ägypten und Mesopotamien aus im 16. Jahrhundert nach Spanien und später weiter in Westeuropa ausbreitete. Lange Zeit kannte man im Origami nur eine kleine Anzahl traditioneller Modelle wie z. B. den Kranich (Japan) oder die Pajarita (Spanien). Erst der Japaner Akira Yoshizawa (1911–2005) brach mit traditionellen Vorlagen und schuf neue Modelle. Er entwickelte ein System aus einfachen systematischen Zeichnungen (Diagramme genannt), um Faltanleitungen zu erstellen, die weitergegeben und allgemein verstanden werden konnten. Dieses System ist die Basis für das Yoshizawa-Randlett-System, die heute übliche Notation für Faltanleitungen. 

Innerhalb kurzer Zeit kam es zu einer Revolution des Origami, die Modelle mit einer Komplexität hervorbrachte, die man zuvor nicht für möglich gehalten hätte. Eine große Rolle spielten dabei die sogenannten „Bug-Wars“ in den 1960er Jahren, ein freundschaftlicher Wettstreit zwischen mehreren Faltern (z. B. Robert J. Lang), der zum Ziel hatte, möglichst lebensechte Käfer und Insekten zu falten.

Moderne Origamimodelle sind mitunter sehr komplex und man benötigt oft mehrere Stunden, um sie zu falten. Andererseits bevorzugen viele Falter auch heute noch einfache Strukturen und Formen. Manche Falter spezialisieren sich auch auf ganz bestimmte Modelle (z. B. Schachteln).

Nach einer japanischen Legende wird demjenigen, der tausend Origami-Kraniche (, "Sembazuru") faltet, von den Göttern ein Wunsch erfüllt. Seit dem Tode des Atombombenopfers Sadako Sasaki, die mit dem Falten von Kranichen vergeblich gegen ihre durch die Strahlung verursachte Leukämie-Erkrankung ankämpfte, sind Origami-Kraniche auch ein Symbol der internationalen Friedensbewegung und des Widerstandes gegen den Atomkrieg.

Damit ein Modell im gefalteten Zustand flach bzw. eben ist, muss der zugehörige Faltplan folgende Bedingungen erfüllen:


Heute unterscheidet man im Origami mehrere verschiedene Richtungen, die auch untereinander gemischt werden können.

Zum Falten von Origamimodellen wird traditionell quadratisches Papier in den Standardgrößen von 7,5 cm, 15 cm, 18 cm, 20 cm und 25 cm Seitenlänge verwendet. Andere Größen von 2,5 bis 70 cm sind erhältlich, wobei jedoch 15 × 15 cm die am weitesten verbreitete Seitenlänge sind. Bei den Sorten ist zwischen handgeschöpften Papier "(Washi)" und industriell hergestellten Papier "(Kami)" zu unterscheiden. Klassisches Origamipapier ist einseitig mit einer Farbe bedruckt; die Rückseite ist weiß. Im deutschen Einzelhandel sind vollfarbige Papiere (Vorder- und Rückseite haben dieselbe Farbe) am häufigsten, es gibt jedoch auch Papiere, die auf beiden Seiten verschieden gefärbt oder gemustert sind. In gut sortierten Origami-Shops findet man auch besondere Papiersorten, wie Tissue Foil, Elefantenhaut, Tant, Lokta, Unriyu und viele andere. Besonders für komplexere Modelle bieten sich besonders dünne, widerstandsfähige Papiere wie etwa Tissue Foil an.

Grundlegende Falttechniken, die sich im Origami häufig wiederholen, sind:

Die klassischen Grundformen sind:

Insgesamt gibt es heute wesentlich mehr Grundformen; diese sechs Grundformen werden „klassisch“ genannt, weil sie seit Jahrhunderten benutzt werden. Da mit diesen Grundformen die Komplexität der Modelle jedoch eingeschränkt ist, wurden in der letzten Zeit mehr und mehr Grundformen entwickelt. Diese werden oft nur für ein bestimmtes Modell entwickelt und sind zum Teil recht komplex.
Einfache Origami-Modelle kommen gewöhnlich mit 10 bis 30 Faltschritten aus und sind daher meist nicht so naturgetreu. Die neueren superkomplexen Modelle aus den speziellen Basen haben dagegen nicht selten bis zu 300 Faltschritte und sind dem Original meist sehr ähnlich.

Origami nicht nur mit einem künstlerischen, sondern auch mit einem pädagogischen Zugang zu betrachten, hat sich seit Friedrich Fröbel, der die Papierfaltkunst aus Japan in der Konzeption seiner Kindergarten übernahm, erfolgreich etabliert. Die Arbeit mit Kindern am Papier hat vor allem in therapeutischen Settings, Kindergarten und Schule Erfolge vorzuweisen. So gibt es in Japan und Israel Schulen mit Origami als Unterrichtsfach inkl. einer theoretisch-konzeptionellen Fundierung. Hierbei haben sich insbesondere die Pädagoginnen Dina Vardi und Miri Golan verdient gemacht.

Neben dem Verbessern feinmotorischer Fähigkeiten, Erlernen akkuraten Arbeitens, Folgen von Anweisungen und Erkennen geometrischer Zusammenhänge, ist die Konzentration die zentrale Komponente, die mit Hilfe von Origami gefördert werden kann. Ein zweiter pädagogisch-therapeutischer Mehrwert bildet die Stärkung sozialer Kompetenz in den Bereichen Miteinander, Kommunikation, Fremdwahrnehmung und Selbstkontrolle. Da es nicht um Geschwindigkeit geht, sondern der kreativ-ästhetische Aspekt und natürlich der Spaß am Erschaffen im Mittelpunkt steht, gibt es untereinander auch keinen Wettbewerb, bzw. wird dieser dort schnell abgewöhnt. Von den Fertigkeiten her heterogene Kleingruppen eignen sich dabei besonders gut - hier sind durch die Möglichkeit gegenseitiger Motivation und Unterstützung für den individuellen Selbstwertaufbau am größten.

Während es beim Papierfalten um das Erstellen eines 3D-Objektes aus einer Fläche geht, lassen sich die Grundtechniken auch umkehren, um ein 3D-Objekt zu Transportzwecken möglichst kompakt zusammenzufalten. Anwendungsbeispiele reichen von Solarsegeln (s. Miura-Faltung) über Airbags bis hin zu Stents.

Mathematische Forschungen zum Origami wurden unter anderem von Erik Demaine am Massachusetts Institute of Technology (MIT) durchgeführt. Ebenso hat Toshikazu Kawasaki mit dem Satz von Kawasaki einen bedeutenden Beitrag im Bereich der mathematischen Origami-Forschung beigetragen.

Der US-Amerikaner Robert J. Lang hat mit Hilfe mathematischer Methoden zwei Computerprogramme entwickelt, die fortgeschrittenen Faltern das Entwerfen eigener Modelle vereinfachen sollen:





Organisationen:


</doc>
<doc id="9526" url="https://de.wikipedia.org/wiki?curid=9526" title="Arithmetik">
Arithmetik

Die Arithmetik ( "arithmetiké [téchne]", wörtlich „die Zahlenmäßige [Kunst]“) ist ein Teilgebiet der Mathematik. Sie umfasst das Rechnen mit den Zahlen, vor allem den natürlichen Zahlen. Sie beschäftigt sich mit den Grundrechenarten, also mit der Addition (Zusammenzählen), Subtraktion (Abziehen), Multiplikation (Vervielfachen), Division (Teilen) sowie den zugehörigen Rechengesetzen (mathematische Operatoren bzw. Kalküle). Zur Arithmetik gehört auch die Teilbarkeitslehre mit den Gesetzen der Teilbarkeit ganzer Zahlen sowie der Division mit Rest. Die Arithmetik kann als Teil der Algebra verstanden werden, etwa als „Lehre von den algebraischen Eigenschaften der Zahlen“. Die Arithmetik leitet zur Zahlentheorie über, die sich im weitesten Sinn mit den Eigenschaften der Zahlen beschäftigt. Die Arithmetik ist ein Kalkül.

Als Wissenschaft wurde die Arithmetik von den Griechen begründet. Aus der vorgriechischen Zeit sind uns z. B. von den Ägyptern und den Babyloniern lediglich empirische Regeln zur Lösung von Aufgaben aus dem praktischen Leben überliefert. Für die Pythagoreer machen die natürlichen Zahlen das Wesen der Dinge aus. In den Büchern VII-X von Euklids Elementen werden die damals bekannten arithmetischen/algebraischen/zahlentheoretischen Ergebnisse erstmals zusammenfassend dargestellt. Vor allem nach dem Fall von Toledo (1085) gelangt die von den Arabern gesammelte griechische Mathematik, bereichert um die von den Indern eingeführte Zahl 0 und das mit dieser Ergänzung voll entwickelte Dezimalsystem, zurück ins Abendland. In der Renaissance findet eine Wiederbelebung der griechischen Mathematik statt. 

Auf dieser Basis wird die Arithmetik im 16. und 17. Jahrhundert vor allem durch die Einführung einer zweckmäßigen Zeichensprache für Zahlen und Operationen weiter entwickelt. Damit wird es möglich, Zusammenhänge, die bei verbaler Wiedergabe sehr undurchsichtig wirken, mit einem Blick zu überschauen. François Viète (Vieta, 1540–1603) unterteilt die damals "Logistik" genannte Rechenkunst in eine "logistica numerosa", in unserem Sinne die Arithmetik, und eine "logistica speciosa", aus der sich die Algebra entwickelt. Er benutzt für Zahlengrößen Buchstaben und als Operationszeichen + für die Addition, - für die Subtraktion und den Bruchstrich für die Division. William Oughtred (1574–1660) benutzt "x" als Zeichen der Multiplikation, das er aber auch mal weg lässt. Der heute übliche Multiplikationspunkt geht auf Gottfried Wilhelm Leibniz (1646–1716) zurück. John Johnson benutzt seit 1663 den heute üblichen Doppelpunkt (:) für die Division. Thomas Harriot (1560–1621) verwendet die heute üblichen Zeichen für "größer als" (>) und "kleiner als" (<) sowie kleine Buchstaben als Variablen für Zahlen. Robert Recorde (1510–1558) führt das Gleichheitszeichen (=) ein. Von René Descartes (1596–1650) stammt die Schreibweise für Quadrate. Leibniz nimmt mit dem Versuch einer axiomatischen Begründung des Rechnens mit natürlichen Zahlen Gedanken der modernen mathematischen Grundlagenforschung vorweg. 

Carl Friedrich Gauß (1777–1855) wird gerne zitiert mit der Aussage: „Die Mathematik ist die Königin der Wissenschaften, und die Arithmetik ist die Königin der Mathematik.“ – Diese Wortschöpfung lässt die Liebe zur Zahlentheorie bei C. F. Gauß erkennen und zeigt, wie sehr Mathematiker sich dieser Teildisziplin verschreiben können. Wie Gauß selber in der Vorrede seiner berühmten „Untersuchungen über höhere Arithmetik“ (siehe Literatur) bemerkt, gehören die Theorie der Kreisteilung oder der regulären Polygone, welche im siebenten Abschnitt behandelt wird, zwar an und für sich nicht in die Arithmetik; doch müssen ihre Prinzipien einzig und allein aus der höheren Arithmetik geschöpft werden. Da sich die heutige Zahlentheorie weit darüber hinaus entwickelt hat, wird lediglich die elementare Zahlentheorie auch als arithmetische Zahlentheorie (= höhere Arithmetik nach Gauß) bezeichnet. Die Bezeichnung „Arithmetik“ (elementare Arithmetik nach Gauß) im eigentlichen Sinne ist zur Hauptsache dem Rechnen vorbehalten.

Leopold Kronecker (1823–1891) wird der Ausspruch zugeschrieben : "„Die ganzen Zahlen hat der liebe Gott gemacht, alles andere ist Menschenwerk.“"


Stichworte: Kardinalzahl, Ordinalzahl, 0 oder 1 als kleinste natürliche Zahl, natürliche Zahl, Peano-Axiome, Dezimalsystem, Stellenwertsystem, Zahlschriften, Zahlzeichen. Die Frage nach der Grundlegung der natürlichen Zahlen führt in die Grundlagen der Mathematik, insbesondere die Mengenlehre.


Stichworte: Abgeschlossenheit bezüglich der jeweiligen Grundrechenart, Kommutativgesetz, Assoziativgesetz, neutrales Element, inverses Element, Umkehroperation, Distributivgesetz, Vergleich. Verallgemeinerung und Abstraktion führen in die Algebra.


Stichworte: Die Zahl Null (0) (falls nicht schon als kleinste natürliche Zahl eingeführt), ganze Zahlen, Gegenzahl, Betrag einer Zahl, Vorzeichen einer Zahl, Bruchzahl, Kehrwert, rationale Zahl, Mächtigkeit der Zahlenmengen. Verallgemeinerung und Abstraktion führen in die Algebra. Zahlenmengen wie zum Beispiel die reellen Zahlen, die komplexen Zahlen oder die Quaternionen gehören nicht mehr zur Arithmetik. 


Stichworte: Teiler, Teilbarkeit, Teilbarkeitssätze, größter gemeinsamer Teiler (ggT), kleinstes gemeinsames Vielfaches (kgV), Euklidischer Algorithmus, Primzahl, Sieb des Eratosthenes, Primzahlsieb von Sundaram, Primfaktorzerlegung, Fundamentalsatz der Arithmetik, Mächtigkeit der Menge der Primzahlen. Verallgemeinerung und Abstraktion führen in die Zahlentheorie.




</doc>
<doc id="9528" url="https://de.wikipedia.org/wiki?curid=9528" title="Wirtschaft Berlins">
Wirtschaft Berlins

Die Wirtschaft Berlins hat nach der deutschen Wiedervereinigung im Jahr 1990 einen tiefgreifenden Strukturwandel durchlaufen. Das ökonomische Schwergewicht Berlins verlagerte sich vom produzierenden Sektor hin zu mehr Dienstleistungswirtschaft mit höherer Wertschöpfung. Die Industrie selbst durchlief einen Erneuerungsprozess von arbeits- hin zu wissensintensiver Produktion unter Einsatz von Hochtechnologien. Infolgedessen stieg die Produktivität an.

Mit einem städtischen Bruttoinlandsprodukt von 129,5 Milliarden Euro im Jahr 2016 ist Berlin die größte Stadtökonomie im deutschsprachigen Raum und die fünftgrößte innerhalb der Europäischen Union.

Zu den bedeutendsten Wirtschaftszweigen in Berlin gehören die Kreativ- und Kulturwirtschaft, der Tourismus, die Biotechnologie und Gesundheitswirtschaft mit Medizintechnik und pharmazeutischer Industrie, die Informations- und Kommunikationstechnologien, die Bau- und Immobilienwirtschaft, der Handel, die Optoelektronik, die Energietechnik sowie die Messe- und Kongresswirtschaft. 

Berlin zählt zu den Weltstädten mit sehr hoher Lebensqualität. Seit etwa 2010 entwickelt sich die Metropole zu den wesentlichen internationalen Zentren für innovative Unternehmensgründer und verzeichnet jährlich hohe Zuwachsraten bei der Anzahl der Beschäftigten. Innerhalb Europas ist Berlin seit 2015 eines der wichtigsten Zielgebiete für Wagniskapital-getriebene Investitionen.

Bereits zu Beginn des 20. Jahrhunderts war Berlin eine europäische Industriemetropole von internationalem Rang. Lokomotiven der Maschinenbaufabrik Borsig (gegründet: 1837) wurden nach ganz Europa exportiert, in der Elektroindustriewaren die Firmen Siemens & Halske (gegründet: 1847) und AEG (gegründet: 1883) weltbekannt. Die 1864 gegründete Schering AG hatte sich auf dem Gebiet der chemischen Industrie einen Namen gemacht. Sechs Jahre später wurde die heute größte deutsche Bank, die Deutsche Bank, gegründet. Alle diese Unternehmen spielen auch nach über einhundert Jahre langem Bestehen eine bedeutende wirtschaftliche Rolle. 

1902 wurde die Industrie- und Handelskammer zu Berlin gegründet. 1903 wurde die Telefunken Gesellschaft für drahtlose Telegraphie m.b.H., ein Unternehmen der Funk- bzw. Nachrichtentechnik, gegründet. Durch den Ausbruch des Ersten Weltkriegs 1914 brach die Produktion in vielen mittelständischen und handwerklichen Betrieben der Stadt zusammen. Ein Teil der Großunternehmen hingegen konnte, aufgrund der enormen Kriegsproduktion, wirtschaftlich profitieren.

Das Bruttosozialprodukt erreichte erst wieder 1927 das Vorkriegsniveau. Die galoppierende Inflation Anfang der 1920er Jahre sorgte in der Bevölkerung für große Verunsicherung und führte zu einem florierenden Schwarzmarkt. Am 4. November 1923 kostete ein einziges Brot 420 Milliarden Mark. Das vorläufige Ende der Krise wurde dadurch eingeleitet, dass das wertlos gewordene Papiergeld ab dem 15. November 1923 von der Rentenmark ersetzt wurde.

1926 wurde die Deutsche Luft Hansa Aktiengesellschaft in Berlin gegründet. Der Flugzeugbestand der Fluggesellschaft umfasste am Gründungstag 162 Flugzeuge.

Im Jahr 1925 arbeiteten allein in der Berliner Metallindustrie in 15.000 Betrieben rund 400.000 Menschen. Der Börsenkrach an der New Yorker Börse am 25. Oktober 1929 löste eine Weltwirtschaftskrise aus, die auch für Europa schlimme Folgen hatte. In Berlin stieg die Zahl der Arbeitslosen auf bis zu 600.000 im Jahr 1932 an, die Arbeitslosenquote betrug damit 30,8 %. Dabei ist zu beachten, dass es zu dieser Zeit bei Arbeitslosigkeit nur eine geringe und während der Krise noch sinkende soziale Absicherung gab. 1932 war die industrielle Produktion in Berlin auf die Hälfte von 1928 zurückgefallen. 

Nach der „Machtergreifung“ Hitlers 1933 wurden viele der jüdischen Unternehmer dazu gezwungen ihre Betriebe an dem NS-System nahestehende Personen zu verkaufen. Andere wurden enteignet, verfolgt und, soweit sie Deutschland nicht rechtzeitig verlassen konnten, in die Vernichtungslager deportiert. Die Nachfolger in den „arisierten“ Unternehmen bereicherten sich zum größten Teil an dem ihnen nicht zustehenden Vermögen der Verfolgten. Die Verbände wurden gleichgeschaltet.

Im Zweiten Weltkrieg wurde Berlin mit seiner Elektro- und Maschinenindustrie zur Waffenschmiede des „Dritten Reichs“. In den betreffenden Betrieben mussten auch viele Zwangsarbeiter arbeiten. Wegen der starken Bombardements verlegten viele Konzerne ihre Forschungs- und Entwicklungsabteilungen und später häufig auch die Führungsabteilungen aus der Hauptstadt in weniger gefährdete Gebiete Deutschlands. Der Bombenkrieg und der Kampf um Berlin ließen von den Produktionskapazitäten in der Stadt nicht viel übrig. Nach 1945 wurde durch Demontage ganzer Fabrikanlagen der Bestand weiter dezimiert.

Aufgrund der großflächigen Zerstörungen des Zweiten Weltkriegs musste beim Wiederaufbau der Wohngebiete, der Infrastruktur und der Arbeitsstätten vielerorts bei Null begonnen werden. Die politische Teilung der Stadt und die Blockade West-Berlins erschwerten den Neuanfang in den ersten fünf Jahren nach Kriegsende. Für die Westsektoren Berlins stellten der Marshall-Plan der USA sowie die Hilfe der Bundesrepublik Deutschland in dieser Zeit eine sehr große Hilfe dar.

Viele Jahre war die Berliner Wirtschaft auf Hilfe und Subventionen angewiesen. Vor allem die Subventionen brachten aber auch Mitnahmeeffekte mit sich. Der Begriff der „verlängerten Werkbank“ westdeutscher Unternehmen, die in Berlin nur geringste Teile ihrer Fertigung vornehmen ließen, dafür aber die volle Berlin-Subvention einstreichen konnten, machte die Runde.

Der Bau der Berliner Mauer am 13. August 1961 brachte für die Stadt die nächste tiefgreifende Zäsur mit sich. Die Bewohner Ost-Berlins und der Deutschen Demokratischen Republik konnten nun nicht mehr ausreisen, West-Berlin war zur politischen und wirtschaftlichen Insel geworden. 50.000 Pendler aus Ost-Berlin konnten nicht mehr in West-Berlin zur Arbeit gehen. In den Folgejahren wurde West-Berlin als Industriestandort durch die Umsiedlung ganzer Betriebe, sowie die vielerorts drastische Reduzierung der Arbeitsplätze (nicht nur durch Rationalisierung) geschwächt. Der West-Berliner Arbeitsmarkt wurde immer stärker von der öffentlichen Beschäftigung, der Wissenschaft und dem Dienstleistungssektor geprägt. Ost-Berlin blieb für die DDR-Wirtschaft zugleich weiterhin der Schwerpunkt der industriellen Produktion.

Die zentralistische und ebenfalls hoch subventionierte Planwirtschaft der DDR konnte jedoch keinen wirtschaftlichen Aufschwung erreichen. Veraltete Anlagen sowie Rohstoff- und Devisenmangel verhinderten, dass die DDR-Wirtschaft mit den technologischen Entwicklungen im internationalen Maßstab Schritt halten konnte. Nach der politischen Wende 1989 brachen die Absatzmärkte vieler Ost-Berliner-Betriebe gleichsam mit den politischen Systemen zusammen. Hunderte von Betrieben und weit über Hunderttausend von Arbeitsplätzen wurden in der Folgezeit wegen geringer Wettbewerbsfähigkeit im Ostteil der Stadt abgebaut.

Die Isolation West-Berlins wurde durch die deutsche Wiedervereinigung im Oktober 1990 beendet. Somit war die Voraussetzung für die erneute wirtschaftliche Einheit von Ost- und Westteil der Stadt gegeben, die Verflechtungen mit dem Umland nahmen stark zu. So wurden auch zahlreiche Kooperationen insbesondere mit dem Land Brandenburg aufgebaut. 

Die Industrie verlor in den 1990er Jahren zunächst dramatisch an Bedeutung. Im Zuge des wirtschaftlichen Strukturwandels entwickelten sich die Schwerpunkte in Berlin nach dem Jahr 2000 auf den Dienstleistungssektor und die industrielle Produktion wurde zugunsten innovativer Branchen verlagert. 

Die arbeitende Bevölkerung im Ballungsgebiet Berlin wächst seit etwa 2005, innovative Wirtschaftscluster ziehen seit dem Unternehmer an und machten Berlin zur Gründungshauptstadt Deutschlands. Mit einem BIP von 90,4 Milliarden Euro löste Berlins Stadtökonomie 2006 die von Hamburg als führende in Deutschland ab. Zwischen 2006 und 2010 lag die Gründerquote in Berlin auf dem deutschlandweit höchsten Wert von 2,67 %. Die Wirtschaftsleistung wuchs in dem Zeitraum um knapp 13 % und damit doppelt so stark wie die des Bundes. 

Im Dienstleistungssektor wurde im Jahr 2010 von etwa 80 % der Berliner Erwerbstätigen ein Anteil an der städtischen Bruttowertschöpfung von über 67 % erwirtschaftet. Das produzierende Gewerbe bot für etwa 8 % der Erwerbstätigen einen Arbeitsplatz, erwirtschaftete jedoch einen Anteil von fast 15 % der Berliner Wertschöpfung im Jahr 2010. Innerhalb der Berliner Industrie lag die Exportquote 2009 bei rund 45 %. Die bedeutendsten Warengruppen der Berliner Exporte waren derzeit pharmazeutische Erzeugnisse, elektrische Ausrüstungen und Kraftmaschinen.

Im Jahr 2014 betrug das nominale Bruttoinlandsprodukt (BIP) des Landes Berlin 117,3 Milliarden Euro, was einer Veränderung von +2,2 % gegenüber 2013 bedeutet (Deutschland: +1,6 %). Damit lag Berlin auf Platz 2 aller Bundesländer beim Anstieg des Bruttoinlandsprodukts. Berlins städtische Wirtschaft ist gemessen am nominalen BIP nach London, Paris, Madrid und Rom die fünftgrößte innerhalb der EU.

Im Vergleich mit dem regionalen BIP (in Kaufkraftstandards) je Einwohner erreichte Berlin in der EU (EU-27: Indexzahl 100) im Jahr 2008 einen Index von 99, Brandenburg von 82 und Deutschland von 116. Im Jahr 2016 erreichte Berlin einen Index von 118, Brandenburg von 89 und Deutschland von 124.

Nach der Wiedervereinigung im Jahr 1990 hat sich der Arbeitsmarkt in Berlin stark verändert. Von 1991 bis 2006 wurden etwa 260.000 Industrie-Arbeitsplätze abgebaut. Das Land Berlin hat aufgrund vieler Doppelstrukturen in der Verwaltung und im Öffentlichen Dienst zwischen den Jahren 1995 und 2015 etwa 60.000 Stellen abgebaut. 

Die Zahl der Erwerbstätigen in Berlin lag im Jahr 2015 bei 1,85 Millionen Personen. Das sind 35.800 oder 2,0 % mehr als im Vorjahr. Berlin verzeichnete damit zum vierten Mal in Folge die stärkste Zuwachsrate bei der Zahl der Erwerbstätigen unter allen deutschen Ländern. 

Die Arbeitslosenquote lag im April 2017 bei 9,2 % (1,0 % weniger als im Vorjahr) und erreichte den niedrigsten Stand seit der Wiedervereinigung im Jahr 1990. 16,5 % der Einwohner bezogen im Jahr 2015 Leistungen nach dem SGB II.

Dennoch verspüren Berliner Unternehmen Fachkräfteengpässe – und zwar nicht nur im Bereich der Hochqualifizierten, sondern auch bei Absolventen mit dualer Ausbildung und Weiterbildungsabschlüssen.

Im Berliner Landesdienst, der aus Haupt- und Bezirksverwaltung besteht, waren im Jahr 2015 insgesamt 127.035 Beschäftigte tätig. Im öffentlichen Dienst in Berlin waren 193.110 Personen beschäftigt. Die beiden größten vom Land Berlin beschäftigten Berufsgruppen sind Lehrer und Polizisten.

Die bereinigten Einnahmen des Landes Berlin betrugen 2017 rund 27,7 Milliarden Euro (2014: 23,8 Milliarden Euro). Die bereinigten Ausgaben im Jahr 2017 betrugen rund 26,6 Milliarden Euro (2014: 23,0 Milliarden Euro). Die Einnahmen des Landes Berlin an Steuern, Länderfinanzausgleich und Allgemeine Bundesergänzungszuweisungen im Jahr 2017 beliefen sich auf 15,4 Milliarden Euro.

Das Land Berlin war Ende 2011 mit etwa 62 Milliarden Euro verschuldet, jährlich fallen mehr als zwei Milliarden Euro Zinsen an. Die Schulden stiegen insbesondere in den 1990er Jahren an. Aufgrund der guten Konjunktur seit 2010 konnte die Verschuldung auf etwa 59 Milliarden Euro im Jahr 2016 zurückgefahren werden.

Das mittlere monatliche Haushaltsnettoeinkommen lag im Jahr 2014 bei etwa 1750 Euro.

Im Jahr 2011 war in Berlin ein Anteil von 12,3 % der erwachsenen Bevölkerung überschuldet (Deutschland: 9,4 %). 

37.368 Mädchen und Jungen kamen in Berlin im Jahr 2014 auf die Welt. Das waren 2330 Neugeborene (6,6 Prozent) mehr als ein Jahr zuvor.

Berlin ist die Bundeshauptstadt der Bundesrepublik Deutschland, der weltweit viertgrößten Volkswirtschaft. Die Metropole ist Teil der Eurozone und der Europäischen Union, einem der größten Binnenmärkte der Welt. Wirtschaftsgeografisch bildet Berlin den Kern der Metropolregion Berlin-Brandenburg, die etwa sechs Millionen Einwohner umfasst. 

Berlin zeichnet sich als Standort durch die Internationalität, die relativ niedrigen Mieten, das vielfältige kulturelle Angebot, die zahlreichen Grünflächen in der Stadt und die Nähe zu verschiedenen Naherholungsgebieten sowie einen gut ausgebauten ÖPNV aus. Durch zahlreiche im Stadtgebiet angesiedelte Technologieparks und die insgesamt 79 wissenschaftlichen Einrichtungen ist Berlin eine Wissenschaftsregion. Die große Anzahl an Hochschulabsolventen bietet vor allem innovativen Unternehmen ein großes Fachkräftepotential. Der neue Flughafen Berlin Brandenburg soll die Flüge von und nach Berlin am Standort Schönefeld bündeln und somit die benötigten Kapazitäten im Luftverkehr bieten. 

In den unterschiedlichsten international angelegten Studien wird die Lebensqualität der Stadt Berlin als überdurchschnittlich bewertet. Der "Global Power City Index" aus Japan listete im Jahr 2017 Berlin im Vergleich der Lebensqualität auf Platz 1. von 44 untersuchten Weltstädten. Laut "European Cities Monitor", einer Umfrage unter Europas führenden Unternehmern, wird Berlin im Jahr 2011 auf dem fünften Platz als bester Standort für internationale Konzerne in Europa gelistet.

Zu den Schwächen des Standorts zählt das gering ausgeprägte bürgerliche Engagement der Bevölkerung. Die schwache Wirtschaftsorientierung der Stadtgesellschaft spiegelt sich in der Parteienlandschaft des Abgeordnetenhaus wieder von dem sehr wenige Initiativen zur strategischen Standort- oder Wohlstandsentwicklung ausgehen. Die städtische Verwaltung gilt als ineffizient (Stand: 2018). Der seit 2016 im Amt befindliche Senat ist für seine wirtschaftskritische Haltung bekannt. Sowohl der Berliner Senat als auch die Oppositionsparteien genießen nur einen sehr unterdurchschnittlichen Rückhalt bei der Wahlbevölkerung (Stand: 2017).

In Berlin gibt es 11 staatliche Hochschulen und 22 staatlich anerkannte private Hochschulen an denen rund 180.000 Studenten (Wintersemester 2016/2017) eingeschrieben sind. Im Jahr 2010 schlossen an den Berliner Hochschulen knapp 25.000 Studenten ihr Studium ab. Außerdem sind in der Stadt jeweils sechs Einrichtungen der Max-Planck-Gesellschaft und der Fraunhofer-Gesellschaft, 16 Einrichtungen der Wissenschaftsgemeinschaft Gottfried Wilhelm Leibniz und circa 18 weitere Forschungseinrichtungen und -verbünde angesiedelt.

So hat Berlin pro Kopf deutschlandweit die höchste Forscher- und Akademikerdichte, insgesamt sind an 79 Einrichtungen etwa 200.000 Menschen in Wissenschaft und Forschung aktiv. In Berlin bestehen Kooperationen von Wirtschaft und Wissenschaft. Ein Beispiel ist das Technologiezentrum Berlin Adlershof in dem sechs Institute der Humboldt-Universität und etwa 1000 Unternehmen in direkter Nähe zueinander angesiedelt sind. 

Zu den Schwächen des Hochschulstandorts zählt die starke Trennung von Forschung und Lehre und der daraus resultierenden verminderten Reputation der wissenschaftlichen Leistungsfähigkeit. Die nach 1945 eingeleitete Abkehr vom Humboldtschen Bildungsideals in Deutschland führte zu einem zu hoch ausdifferenzierten Forschungs- und Bildungssystem, dem es im globalen Kontext an Sichtbarkeit fehlt. Spitzenleistungen an Berliner Forschungsinstituten werden deshalb nicht mit städtischen Universitäten in Zusammenhang gebracht.

In Berlin treffen sich 10 Eisenbahn-Hauptstrecken, die die Stadt zu einem europäischen Bahnknoten machen. Mit täglich rund 300.000 Reisenden ist der Berliner Hauptbahnhof Europas größter Kreuzungsbahnhof.

Mit S-Bahn, U-Bahn, Regionalbahn, Straßenbahn, Bussen und Fähren hat Berlin einen sehr umfangreichen ÖPNV. Es ist tariflich zusammengefasst durch den Verkehrsverbund Berlin-Brandenburg (VBB), dem flächengrößten Verkehrsverbund in Deutschland.

Derzeit verfügt Berlin über zwei Flughäfen, Berlin-Tegel (TXL) und Berlin-Schönefeld (SXF/BER). Sie fertigten im Jahr 2014 ca. 28 Millionen Passagiere ab. Gegenüber dem Vorjahr bedeutet dies eine Steigerung von 6,3 %. Das Passagieraufkommen nimmt in Berlin stark zu, so wurden im Jahr 2000 noch 13,3 Millionen Passagiere abgefertigt. Das noch aus Zeiten der Stadtteilung resultierende wenig international wettbewerbsfähige und zerteilte Flughafensystem gilt als großes Entwicklungshemmnis in der Region Berlin-Brandenburg. Nur wenig Interkontintalverbindungen werden vom Flugverkehrsstandort Berlin aus angeboten.

Gegenwärtig in Bau befindet sich der neue Flughafen Berlin Brandenburg (BER). Der ausgebaute Flughafenstandort Schönefeld soll mit einer Startkapazität von wenigstens 36 Millionen Passagieren den bestehenden Flugplatz Tegel ersetzen und die Flüge an einem Standort bündeln. Je nach Passagierentwicklung soll der Flughafen für bis zu 55 Millionen Passagiere ausgebaut werden.

Nach einer Mietpreisstudie von ECA International belegt Berlin im Europavergleich der Städte mit den höchsten Mieten nur Platz 34. So betrug beispielsweise 2011 die Miete einer Dreizimmerwohnung in Berlin etwa 775 Euro, in Europa durchschnittlich 1090 Euro und in London, der teuersten Stadt Europas, musste man durchschnittlich sogar 2500 Euro zahlen.

Berlin verfügte 2006 mit einer Strecke von 200.000 Kilometern über das umfangreichste städtische Glasfasernetz in ganz Europa. Im gesamten Stadtgebiet sind mobile Breitbandanwendungen über UMTS- und LTE-Netze garantiert (Stand: 2017). Außerdem gibt es in Berlin über 700 öffentliche W-Lan Hotspots.

Die Gründer- und Start-up-Szene Berlins sorgt insbesondere seit den späten 2000er Jahren für einen steten Zuwachs an Arbeitsplätzen, Innovation, Kaufkraft und Internationalität des Berliner Wirtschaftsumfeldes.

Einer stadtökonomischen Untersuchung Berlins von 2014 zufolge sei im urbanen Umfeld eine hohe Einwohnerdichte und ein lebendiges Ausgeh- und Kulturleben für die Gründerszene wichtig, die Arbeit und Leben nah beieinander zusammenbringt.

Entscheidend für den Erfolg von Startups und Unternehmensgründern insgesamt sei laut einer RKW-Studie von 2015 ein „fruchtbares regionales Gründer-Ökosystem“ bzw. gründerfreundliches Klima, das durch das Zusammenspiel von Talenten, erfolgreichen Unternehmern, Finanzierungsmöglichkeiten, Bildungseinrichtungen, bürokratiearmer Politik und Verwaltung, potenziellen Kunden, leistungsfähiger Infrastruktur, die Offenheit für Innovationen, Kreativität und eine hohe Lebensqualität entstehe. In Berlin sei diese Dynamik gegeben.

Die beiden Länder Berlin und Brandenburg haben 2011 auf politischer Ebene eine gemeinsame Innovationsstrategie formuliert. Dazu wurden neben den tragenden marktwirtschaftlich getriebenen Branchen sogenannte Zukunftsfelder identifiziert, die gemeinsam entwickelt werden sollen. Die Felder sind Biotechnologie/Medizintechnik/Pharma, Medien/Informations- und Kommunikationstechnologie, Verkehrssystemtechnik und Logistik, Optik und Photonik, sowie Energietechnik. 

Berlin hat sich seit dem Jahr 2000 zu einer wichtigen europäischen Metropole für die Kreativ- und Kulturwirtschaft entwickelt. 2006 wurde Berlin in das Creative Cities Network der Unesco aufgenommen. 

Im Jahr 2009 erwirtschafteten in der Kreativ- und Kulturwirtschaft knapp 160.000 Erwerbstätige in 25.500 Unternehmen einen Umsatz von rund 12,8 Milliarden Euro. Gemeinsam mit dem IKT-Sektor erwirtschaftet der Medien-, Kreativ- und Kulturbereich somit rund 16 % des Berliner Gesamtumsatzes (2009). 2014 zählte die Berliner Medien- und Unterhaltungsbranche zu zehn umsatzstärksten Medienstandorten der Welt.

Innerhalb der Kulturwirtschaft spielt die Musikbranche in Berlin eine besondere Rolle. Über 9,4 % aller Musikunternehmen Deutschlands haben ihren Sitz in Berlin. Die Angebotsvielfalt an Musikveranstaltungen und Veranstaltungsorten, die 155 ansässigen Musikverlage, die über 100 Musikschulen mit 40.000 Schülern und über 3.000 Studenten im Fach Musik bieten gute Voraussetzungen dafür, dass sich die Branche auch in Zukunft gut entwickelt.

Die Deutsche Kreditbank (DKB) ist ein Kreditinstitut und 100%ige Tochtergesellschaft der Bayerischen Landesbank. Mit einer Bilanzsumme von 76,5 Milliarden Euro stand die DKB 2015 an 16. Stelle in der Liste der größten Banken in Deutschland.

Die Börse Berlin ist eine Regionalbörse in Berlin, deren Eigentümer der "Verein Berliner Wertpapierbörse e. V." ist. 2016 verzeichnete die Börse einen Gesamtumsatz von 63,7 Milliarden Euro. Handelsteilnehmer waren 61 Kreditinstitute, 29 Finanzdienstleister, 5 Skontroführer und 6 Market-Maker.

Die Investitionsbank Berlin (IBB) ist das zentrale Förderinstitut des Landes Berlin. Aufgabenschwerpunkt der IBB liegt in der Wirtschaftsförderung kleiner und mittlerer Unternehmen sowie dem Wohnungsbau.

Die Berliner Sparkasse ist eine öffentlich-rechtliches Kreditinstitut und hat ihren Sitz in Berlin. Mit etwa zwei Millionen Kunden und mehr als 1,3 Millionen Girokonten ist die 1818 gegründete Sparkasse Marktführer in der Stadt. Die Sparkassen-Finanzgruppe, die mehr als 600 dezentral arbeitende Unternehmen im Bereich der Finanzdienstleistungen umfasst hat ihren Sitz ebenfalls in Berlin.

Seit 2015 ist Berlin die Stadt mit der größten Anzahl an Wagniskapitalgesellschaften in Deutschland. Etwa 24 % aller Investmentunternehmen im Land haben dort ihren Sitz. Zu den bekanntesten Kapitalgebern gehört Rocket Internet.

Berlin ist eines der meistbesuchten Zentren des nationalen und internationalen Städtetourismus. Im Jahr 2017 wurden etwa 31,2 Millionen Übernachtungen in Berliner Beherbergungsbetrieben von 13,0 Millionen Gästen gezählt.

Dies stellt im Vergleich zum Jahr 2001 (11,3 Millionen Übernachtungen von 4,9 Millionen Gästen) nahezu eine Verdreifachung der Zahlen dar. Hinzu kommen jährlich geschätzt mehr als 100 Millionen Tagesbesucher. Die Stadt ist damit nach London und Paris ein bevorzugtes Reiseziel innerhalb Europas. 

Die Berliner Hotellandschaft bestand 2017 aus knapp 800 Unterkünften mit einer Bettenkapazität von rund 143.000. Die Verweildauer der Hotelgäste beträgt im Durchschnitt 2,2 Tage. Internationale Gäste machen etwa 45 Prozent der Besucherzahlen aus. Hierbei liegen Besucher aus Großbritannien, Italien, den Niederlanden, Spanien und den Vereinigten Staaten von Amerika in der Spitzengruppe. Hauptanziehungspunkte sind Architektur, historische Stätten, Museen, Festivals, Einkaufsmöglichkeiten, Nachtleben sowie Großveranstaltungen, die jährlich mehrere hunderttausend Besucher zählen.

Durch die sehr positive Entwicklung seit dem Jahr 2000 ist die Tourismusbranche in Berlin zu einer wichtigen Säule der regionalen Wirtschaft geworden. Die größten Profiteure des Berlin-Tourismus sind das Gastgewerbe (42,8 % des Umsatzes), der Einzelhandel (40,4 %) und die Dienstleistungsunternehmen (16,8 %). 

Berlin ist einer der besucher- und umsatzstärksten Messe- und Kongressstandorte der Welt. Im Jahr 2011 wurden etwa 115.700 Veranstaltungen mit rund 9,7 Millionen Teilnehmern durchgeführt. Laut ICCA Statistik ist Berlin, gemessen an der Anzahl von Kongressen mit internationaler Beteiligung, auf Rang 4 aller Kongressstädte gelistet.

Das Messegelände in Charlottenburg-Wilmersdorf rund um den Funkturm wurde im Jahr 1822 erstmals als Gewerbeausstelungsgelände genutzt und weist heute eine Hallenausstellungsfläche von 160.000 m² und eine Freifläche von etwa 100.000 m² aus. Betreiber ist der zu 99 % stadteigene Veranstaltungsträger Messe Berlin. Das Berlin Convention Office der Berliner Tourismuswerber visitberlin vermarktet die deutsche Hauptstadt als Kongressmetropole.

Eine Vielzahl international relevanter Leitmessen sind auf dem Berliner Messegelände und auf dem Berlin ExpoCenter Airport in Brandenburg etabliert. Hierzu gehören u. a. die Konsumelektronikmesse IFA, die Tourismusbörse ITB, die Luft- und Raumfahrtschau ILA Berlin Air Show, die Schienenverkehrsmesse InnoTrans, die Landwirtschaftsschau Grüne Woche, die Ernährungsmesse Fruit Logistica sowie die Fachmesse für Erotik & Erwachsenenunterhaltung Venus Berlin. Die privatwirtschaftlich organisierte Modemesse Bread & Butter wird vom Modehändler Zalando veranstaltet, der European Film Market während der Berlinale im Martin-Gropius-Bau.

Die Kongresswirtschaft, zu der international, national sowie lokal ausgerichtete Tagungen, Informationsveranstaltungen und geschäftliche Zusammenkünfte aller Art gehören, ist ein weiterer Zweig der Berliner Wirtschaft. Eine Vielzahl von Hotels sind auf Tagungen eingestellt und erzielen hiermit einen wichtigen Teil ihres Jahresumsatzes. Das ICC war das größte Konferenzzentrum Europas. Medizinkongresse und Fachtagungen von mehreren Tausend Teilnehmern konnten in dem 1979 eröffneten Bau abgehalten werden.

Insgesamt waren in Berlin 2009 etwa 226.000 Personen, das sind 13,5 % der Berliner Erwerbstätigen, im Gesundheitssektor mit seinen Teilbereichen Industrie, Handel, Handwerk sowie privaten und öffentlichen Dienstleistungen beschäftigt gewesen. In der Region Berlin-Brandenburg waren 2016 etwa 360.000 Personen in der Gesundheitsbranche tätig. Der Gesamtumsatz des Sektors betrug im selben Jahr rund 23 Milliarden Euro. Das entsprach ca. 12 % der gesamten Wirtschaftsleistung in der Region.

Die Biotechnologie ist eine innovative Querschnittstechnologie, die durch überdurchschnittlichen Forschungs- und Entwicklungsaktivitäten gekennzeichnet ist. Die unmittelbare Nähe zu Hochschulen und Universitäten sowie die Einrichtung zahlreicher Technologieparks wie beispielsweise dem berlinbiotechpark Charlottenburg begünstigt Unternehmensansiedlungen. In der Hauptstadtregion sind rund 200 kleine und mittlere Biotechnologie-Unternehmen mit ca. 3.700 Mitarbeitern in Produktion, Forschung und Entwicklung tätig (2010).

In Berlin existierten 2014 insgesamt 65 Einkaufszentren. Zu den bekanntesten Adressen zählt das KaDeWe.

Berlin ist Deutschlands umsatzstärkster Standort für den Immobilienhandel. 2016 wechselten Immobilien für 5,4 Milliarden Euro in Berlin den Besitzer. 2015 lag der Umsatz bei 8 Milliarden Euro.

Seit etwa 2005 findet in Berlin ein zunehmend starkes Wachstum der IT- und Internetwirtschaft statt. Ausgehend von der Anzahl und der Popularität der in Berlin ansässigen Internetunternehmen, gilt die Stadt derzeit als der wichtigste Standort der Webwirtschaft im deutschsprachigen Raum. 

Das in den letzten Jahren in diesem Bereich entstandene Cluster zieht derzeit die Aufmerksamkeit von Investoren auf sich und macht Berlin zu einem idealen Standort für Internetfirmen. Die Internetwirtschaft ist durch den starken Gründungswillen der Berliner geprägt. Die Industrie und Handelskammer geht von etwa 1300 neu entstandenen Software-, Web-, und IT-Unternehmen in den Jahren von 2008 bis 2011 aus.

In über 400 verkehrstechnischen Betrieben der Region Berlin-Brandenburg sind 2010 etwa 54.000 Mitarbeiter beschäftigt. Von ihnen arbeiten etwa 2.000 als wissenschaftliche Mitarbeiter in Forschungszentren. Im Bereich der Verkehrssystemtechnik in den letzten Jahren ein Beschäftigungszuwachs zu verzeichnen. Handlungsfelder der Verkehrssystemtechnik sind die Bahntechnik, Kraftfahrzeugtechnik, Verkehrstelematik/Verkehrslogistik, Luft- und Raumfahrttechnik und der Bereich Binnenschifffahrt.

Berlin/ Brandenburg ist eine Modellregion für Elektromobilität. Energieeffizienz wird für die großen Automobilhersteller immer mehr zum beherrschenden Thema. In Berlin bildet dabei die Integration der verschiedenen Arten von Elektrofahrzeugen in das traditionelle Angebot des öffentlichen Nahverkehrs und des Tourismus, Mobilitäts- und Wohnungsdienstleistungen und innerstädtische Verteilerverkehre einen Schwerpunkt. In der Hauptstadt gibt es seit einigen Jahren mehrere Pilotprojekte zu diesem Thema. Beispiele hierfür sind die Kooperationen von BMW und Vattenfall („Mini E Berlin“), die Zusammenarbeit von Eon und Volkswagen („Flottenversuch Elektromobilität“) sowie der Flottenversuch von Daimler („EMKEP“)

Optik wird als eine der Schlüsseltechnologien des 21. Jahrhunderts betrachtet. Im Technologiepark Berlin-Adlershof sind viele Unternehmen der Photonik, Optik, Optoelektronik und Lasertechnologie vertreten. Eine Initiative von Unternehmen und wissenschaftlichen Einrichtungen in der Region ist Optec-Berlin-Brandenburg (OpTecBB) e. V., der die Erschließung und Nutzung optischer Technologien fördern soll. 

Die Unternehmen der Sparte optische Technologien und Mikrosystemtechnik konnten in den Jahren 2001 bis 2010 durchschnittlich ein jährliches Umsatzwachstum von etwa 8 % verbuchen, im Jahr 2011 lag das Wachstum sogar bei 12 %. Auch die Beschäftigungszahlen sind in den letzten Jahren jährlich um durchschnittlich 3,5 % gestiegen.

Die Energietechnik beschäftigt sich u. a. mit der Effizienz von Energieumwandlung, Energietransport und Energienutzung. Die bundesländerübergreifende wirtschaftliche Kooperation zwischen Berlin und Brandenburg wurde in den letzten Jahren intensiviert. Insgesamt sind in der Region 2016 etwa 600 Unternehmen mit rund 56.000 Beschäftigten in diesem Bereich tätig.

Außerdem beschäftigen sich insgesamt 35 wissenschaftliche Einrichtungen mit energiebezogener Forschung und Lehre. Die Region nimmt damit in Deutschland eine führende Rolle auf dem Gebiet der Energietechnik ein.

Im Jahr 2016 gab es in Berlin insgesamt 529 öffentlich zugängliche Stromtankstellen für elektrisch betriebene Fahrzeuge. Die Stadt war damit Spitzenreiter bei der Anzahl von urbanen Ladepunkten in Deutschland. Eine der leistungsfähigsten deutschen Stationen mit Schnellladetechnik befindet sich seit 2017 in Berlin-Adlershof.
Die Ernährungswirtschaft in Berlin beschäftigte im Jahr 2015 etwa 10.100 Arbeitnehmer in 91 Betrieben. Ein Viertel des Umsatzes in der Nahrungs- und Futtermittelindustrie erwirtschaftet die Süßwarenindustrie.

Die August Storck KG ist mit einem Jahresumsatz von etwa zwei Milliarden Euro das größte Nahrungsmittelunternehmen in der Stadt. Zu den national und international erfolgreichsten Produkten von Storck gehören u. a. Knoppers, Lachgummi, Mamba, Merci, nimm2, Super Dickmanns, Toffifee und Werther’s Original.

In Berlin gibt es nach Aussage des Landesverbandes Landwirtschaft & Pferdehaltung Berlin rund 1900 Hektar landwirtschaftliche Nutzfläche, die von 40 Landwirten bewirtschaftet werden. Der Anteil der Landwirtschaftsfläche beträgt demnach 2,2 % des Stadtgebietes. Die Betriebe sind hauptsächlich in den Randbezirken Berlins zu finden. Die Fläche wird in der Hauptsache als Grünland für die Futtergewinnung, meist Heu, und beim Ackerland für den Roggenanbau genutzt. Ein kleiner Teil eignet sich auch für den Gemüse- und Rapsanbau. Berlin verzeichnet, Stand 2015, 3832 Hektar Landwirtschaftsfläche, was einem Anteil von 4,3 % entspricht. Der Anteil landwirtschaftlicher Flächen im Bezirk Pankow bei 17,8 %, und im Bezirk Spandau, bei 8,4 %. Im Jahr 2010 waren 300 Erwerbstätige in Berlin, die in der Landwirtschaft tätig waren, mehr als zehn Prozent davon im ökologischen Landbau. In keinem Bundesland arbeiten so wenig Menschen wie in Berlin in der Landwirtschaft.

Die beiden größten und bekanntesten Technologieparks in Berlin sind der Wissenschafts- und Wirtschaftsstandort Berlin-Adlershof (Wista) in dem rund 80 hochspezialisierte Weltmarktführer ansässig sind und der BiotechPark Berlin-Buch. Sie nehmen eine ausgesprochen positive Entwicklung und sind in internationale Netzwerke eingebunden.

Einer der größten europäischen Technologiestandorte ist das unter der Kurzform Wista bekannte Gelände im Ortsteil Adlershof im Bezirk Treptow-Köpenick. Hier werden ein Medienzentrum, ein Technologiepark und ein Universitätsstandort räumlich verbunden, sodass Forschung, Entwicklung und Produktion ineinander greifen können. Die Schwerpunkte liegen in der Umwelttechnologie, Informations- und Kommunikationstechnologie und Optoelektronik. 

2017 waren auf dem Wista-Gelände 1088 Unternehmen und wissenschaftliche Einrichtungen (2016: 1041 Unternehmen) angesiedelt, in denen knapp 18.000 Menschen tätig waren (2016: rund 17.000). Hinzu kamen rund 6.700 Studenten und ca. 780 (2016: 870) Auszubildende. Die Umsätze und Haushaltsmittel lagen mit 2,1 Milliarden Euro um 6,6 % über denen des Vorjahres. In der Medienstadt Adlershof arbeiteten 2017 etwa 2.360 Beschäftigte in 147 Unternehmen. Insgesamt befinden sich 2018 sechs Institute der Humboldt-Universität zu Berlin (HU) und zehn außeruniversitäre Institute auf dem auf dem Wista-Campus.

Der Standort Berlin-Buch hat sich in den letzten Jahren deutschlandweit zu einem der größten Biotechnologieparks und Gesundheitsforschungszentren entwickelt. Auf dem 32 Hektar großen Gelände sind Einrichtungen wie das Max-Delbrück-Centrum für Molekulare Medizin (MDC), das Forschungsinstitut für Molekulare Pharmakologie (FMP) und ein Biotechnologiepark ansässig. Außerdem bietet ein modernes Kongresszentrum mit integrierten Laboren vielfältige Möglichkeiten für wissenschaftlichen Austausch. 

Rund 2500 Wissenschaftler, Ärzte und Angestellte sind auf dem Gelände tätig. Bisher haben sich 54 Unternehmen angesiedelt. Die BBB Management GmbH ist seit 1995 als Entwicklungs- und Betreibergesellschaft tätig. 

Neben den beiden großen Technologieparks gibt es auch noch weitere Standorte, an denen gezielt Unternehmen in räumlicher Nähe zueinander gefördert werden. In Charlottenburg gibt es die Bestrebung, am Standort der Technischen Universität sowie der Universität der Künste einen Uni-Campus City-West zu gründen, um dort hochschulnahe Firmen in der Gegend rund um den Ernst-Reuter-Platz anzusiedeln.

In Berlin-Siemensstadt betreibt Siemens seit 2007 den Siemens Technopark Berlin. Auf dem 480.000  m² großen ehemaligen Werksgelände sind 1950 Menschen beschäftigt. Der Innovationspark Wuhlheide ist auf technologieorientierte Unternehmen aus verschiedenen Disziplinen und Branchen ausgerichtet. Auf dem 53.000 m² großen Gelände haben sich bisher 155 meist kleine und mittlere Firmen angesiedelt, in denen insgesamt rund 1.400 Personen beschäftigt sind.

Zahlreiche namhafte Unternehmen wie Siemens, Deutsche Bank, Lufthansa, Allianz, AEG, Telefunken, Osram, Knorr-Bremse, Edeka wurden in Berlin gegründet. Auch die großen Banken Deutschlands hatten alle ihren Hauptsitz im Berliner Bankenviertel, damit verbunden auch das Finanzzentrum Deutschlands. Nach dem Zweiten Weltkrieg und der darauffolgenden Teilung Berlins und Deutschlands gingen sowohl die Unternehmen als auch die Banken in den Westen Deutschlands. 

Nach der Wiedervereinigung der Stadt im Jahr 1990 und insbesondere in der Zeit nach 2000 hat sich Berlin zu einem Anziehungspunkt für Gründer aus aller Welt entwickelt, die in der Metropole ihre Geschäftsideen verwirklicht haben. Eine Vielzahl von im DAX notierten Unternehmen haben Innovationslabore in Berlin angesiedelt.

Angaben der Unternehmen (Stand: 2016):




</doc>
<doc id="9530" url="https://de.wikipedia.org/wiki?curid=9530" title="Moräne">
Moräne

Moränen (französisch: "moraine" „Geröll“; schwed. "Rullstensås") sind die Gesamtheit des von einem Gletscher transportierten Materials, im Speziellen die Schuttablagerungen, die von Gletschern bei ihrer Bewegung mitbewegt oder aufgehäuft werden, sowie die im Gelände erkennbaren Formationen.

Ursprünglich wurde dieser Begriff von der Bevölkerung aus dem Gebiet von Chamonix verwendet, die damit die Felswälle bezeichneten, die die Gletscher angehäuft hatten. In der wissenschaftlichen Literatur wurde dieser Begriff 1799 von dem Gletscherforscher Horace Bénédict de Saussure eingeführt.

"Wandermoränen" sind all jene Moränen, die noch vom Gletscher bewegt werden. Das vom Eis transportierte Gesteinsmaterial wird Geschiebe genannt. Nach dem Ort der Bildung unterscheidet man genauer:

"Abgelagerte Moränen" sind Moränen, die nach der Ablagerung nicht mehr bewegt wurden:

In der Fachliteratur ist es in den letzten Jahren üblich geworden, den Begriff "Moräne" nur noch auf das aktuell bewegte Material im Gletschereis und die Reliefformen, die unter dem und um das Gletschereis entstehen, zu beziehen. Die Ablagerungen (Sediment) hingegen werden als "Geschiebemergel" (oder auch "Till") bezeichnet.

Nach der Entstehungszeit werden Jungmoränen (Weichseleiszeit, Würmeiszeit) von Altmoränen (aus den vorherigen Eiszeiten) unterschieden.

Der Begriff Moräne umfasst alle Schuttmassen, die durch das Eis mitgeführt und abgelagert wurden. Moränen bestehen dabei aus Material verschiedener Körnungsgröße, vom Ton über Sand bis zu größeren Gesteinsblöcken. Das Material ist jedoch durchmischt und weist in der Regel keine Sortierung oder Schichtung nach der Größe auf.

Auch finden sich End- und Grundmoräne als Sohl-, Zwischen- oder Deckschicht gemischt sowie Altgestein. Ufermoränen, das sind bereits abgelagerte Seitenmoränen, können ebenfalls durchaus eine unsaubere Schichtung aufweisen.

Da Moränen aus losem "zusammengebackenen" Sand und Gestein bestehen, sind die steilen Flanken oft steinschlaggefährdet. Sie werden zweckmäßigerweise meist oben am Moränenkamm begangen. Bei Übergängen oder beim Absteigen von einer Moräne auf den Gletscher sollte man sorgfältig die ungefährlichste Stelle aussuchen und sich an Steigspuren halten, falls solche zu erkennen sind.

Moränen können aber auch als wertvolle Orientierungshilfe in einem sonst wenig Anhaltspunkte bietenden strukturlosen Gelände dienen.




</doc>
<doc id="9532" url="https://de.wikipedia.org/wiki?curid=9532" title="Wintersonnenwende (Begriffsklärung)">
Wintersonnenwende (Begriffsklärung)

Wintersonnenwende bezeichnet:
Siehe auch:


</doc>
<doc id="9535" url="https://de.wikipedia.org/wiki?curid=9535" title="Ergebnisse der Bundestagswahlen">
Ergebnisse der Bundestagswahlen

Die Sortierung folgender Parteien/Fraktionen erfolgte in drei Kategorien. Zunächst Parteien/Fraktionen welche aktuell im Bundestag vertreten sind, Parteien/Fraktionen die jemals im Bundestag vertreten waren und Parteien/Fraktionen die niemals im Bundestag vertreten waren. Anschließend nach der letzten Vertretung im Bundestag, dann nach der letzten Teilnahme an einer Bundestagswahl und zuletzt nach dem prozentualen Mittelwert aller Endergebnisse (Prozent der gültigen Zweitstimmen).

<nowiki>*</nowiki> Von 1949 bis 1987 Aufteilung der Bundestagsabgeordneten aus Westdeutschland und West-Berlin.<br>
<nowiki>**</nowiki> 1949 nur Erststimmen.<br>
<nowiki>***</nowiki> Summe der ungültigen und gültigen Zweitstimmen liegt wegen nicht aufklärbaren Unstimmigkeiten bei eingesetzten Stimmenzählgeräten im Wahlkreis 247 (Sankt Wendel) um 80 niedriger als die Zahl der Wähler insgesamt.<br>
<nowiki>****</nowiki> Infolge technischen Versagens eines Stimmenzählgerätes im Wahlkreis 247 (Sankt Wendel) ist die Zahl der abgegebenen Zweitstimmen 1983 um 1 geringer als die Zahl der Wähler.<br>
<nowiki>*****</nowiki> Wegen Sonderregelung des Einigungsvertrages durch Überschreiten der Fünf-Prozent-Hürde im Wahlgebiet Neue Bundesländer im Bundestag.<br>




</doc>
<doc id="9536" url="https://de.wikipedia.org/wiki?curid=9536" title="PISA-Studien">
PISA-Studien

Die PISA-Studien der OECD sind internationale Schulleistungsuntersuchungen, die seit dem Jahr 2000 in dreijährlichem Turnus in den meisten Mitgliedstaaten der OECD und einer zunehmenden Anzahl von Partnerstaaten durchgeführt werden und die zum Ziel haben, alltags- und berufsrelevante Kenntnisse und Fähigkeiten Fünfzehnjähriger zu messen. Das Akronym PISA wird in den beiden Amtssprachen der OECD unterschiedlich aufgelöst: englisch als Programme for International Student Assessment ("Programm zur internationalen Schülerbewertung") und französisch als Programme international pour le suivi des acquis des élèves ("Internationales Programm zur Mitverfolgung des von Schülern Erreichten").

Dieser Artikel behandelt die "internationale" Studie der OECD. Das Konzept der OECD sieht ausdrücklich die Möglichkeit vor, dass Teilnehmerstaaten den internationalen Test um nationale Komponenten erweitern. Diese Möglichkeit wurde in "Deutschland" 2000 bis 2009 in starkem Ausmaß genutzt:


Außerdem bietet die OECD Erweiterungsmodule an, die nur von einem Teil der Staaten genutzt werden, z. B.:


Die OECD ließ 2011/12 eine weitere Studie umsetzen, die die Kompetenzen von 16- bis 65-Jährigen prüft: die PIAAC, von den Medien als „PISA für Erwachsene“ bezeichnet.

Die folgenden Merkmale unterscheiden PISA zum Teil deutlich von früheren Schulleistungsuntersuchungen:

Vertragsmäßige Aufgabe der OECD ist Politikberatung. PISA soll nicht nur eine Beschreibung des Ist-Zustandes liefern, sondern Verbesserungen auslösen. Insoweit PISA ein eigenes Bildungskonzept zugrunde liegt, wird zumindest implizit der Anspruch erhoben, auf die nationalen Lehrpläne zurückzuwirken.

Jede PISA-Studie umfasst die drei Bereiche Lesekompetenz, Mathematik und Naturwissenschaften. Bei jedem Durchgang wird ein Bereich vertieft untersucht: 2000 die Lesekompetenz, 2003 Mathematik, 2006 Naturwissenschaften. Dieser Zyklus wird derzeit (2009, 2012, 2015) ein zweites Mal durchlaufen. Die Ergebnisse werden jeweils im Dezember des Folgejahres veröffentlicht, die technischen Berichte einige Monate später.

Zusätzlich wird in jeder Studie ein Querschnittsthema untersucht: 2000 Lernstrategien und Selbstreguliertes Lernen, 2003 Problemlösung, 2006 Informationstechnologische Grundbildung. Diese Zusatzuntersuchung wird nicht in allen Staaten durchgeführt.

Nach jeder Testrunde wird ein Teil der Testaufgaben „freigegeben“ und veröffentlicht. Sämtliche freigegebenen Aufgaben finden sich auch online auf den Webseiten der OECD und der nationalen Projektleitungen. Die Testhefte umfassen typischerweise ungefähr zwanzig Aufgabeneinheiten. Jede Aufgabeneinheit besteht aus Einleitungsmaterial und eine bis sieben daran anschließenden Aufgaben.

Die Aufgabeneinheit „Tschadsee“ aus dem Lesetest von PISA 2000 zeigt beispielhaft, wie weit das "literacy"-Konzept von PISA gefasst ist. Die Einleitung dieser Aufgabe enthält keinen Lesetext im herkömmlichen Sinn, sondern besteht in der Hauptsache aus zwei Diagrammen („nicht-textuelles Lesematerial“), die die Schwankungen des Wasserstands in den letzten 12.000 Jahren und das Verschwinden und Auftauchen von Großtierarten darstellen. Daran schließen sich fünf Aufgaben an. Beispielhaft:


Die Fragen 4 und 5 sind dann wieder im Multiple-Choice-Format.

PISA ist eines von mehreren Projekten, mit denen sich die OECD seit den 1990er Jahren verstärkt im Bereich Bildungsmonitoring engagiert. Die Koordination und die Endredaktion der internationalen Berichte obliegen einer kleinen Arbeitsgruppe am Hauptsitz der OECD in Paris unter Leitung des Deutschen Andreas Schleicher. Politisch wird das Projekt von einem Rat aus Regierungsvertretern gesteuert; wissenschaftlich wird es von einem Expertengremium nebst Unterausschüssen begleitet. Die Erstellung und Auswertung der Testaufgaben wurde ausgeschrieben und an ein Konsortium aus mehreren Unternehmen der Testindustrie unter Leitung des Australian Council for Educational Research (ACER) vergeben.

In den Teilnehmerstaaten wurden nationale Projektzentren eingerichtet. In jedem Staat werden Stichproben von mindestens 5000 Schülern gezogen; in manchen Staaten, vor allem um Regionalvergleiche zu ermöglichen, ein Vielfaches davon.

Der Test umfasst eine zweistündige „kognitive“ Testsitzung, gefolgt von einer knapp einstündigen Fragebogensitzung („Questionnaire“). Im kognitiven Test bearbeiten nicht alle Schüler dieselben Aufgaben; 2003 wurden dreizehn verschiedene Testhefte (sowie in manchen Ländern in Sonderschulen ein Kurzheft) eingesetzt; von insgesamt 165 verschiedenen Aufgaben hatte jeder einzelne Schüler nur etwa 50 zu bearbeiten.

Die Schülerlösungen werden von angelernten Hilfskräften kodiert, digital erfasst und ans internationale Projektzentrum nach Australien zur weiteren Auswertung übermittelt. Die meisten Aufgaben werden letztlich nur als entweder „falsch“ oder „richtig“ bewertet. Je nachdem, wie viele Schüler eine Aufgabe richtig gelöst haben, wird der Aufgabe ein bestimmter „Schwierigkeitswert“ zugeordnet. Je nachdem, wie viele Aufgaben ein Schüler gelöst hat, wird dem Schüler eine bestimmte Spanne „plausibler“ „Kompetenzwerte“ zugeordnet. Schwierigkeits- und Kompetenzwerteskala werden nachträglich so skaliert, dass die Kompetenzwerte im OECD-Staatenmittel den Mittelwert 500 und die Standardabweichung 100 haben. Um auszugleichen, dass die Testhefte unterschiedlich schwierig waren und dass einzelne Aufgaben in einzelnen Staaten, zum Beispiel wegen Druckfehlern, nicht gewertet werden konnten, wird die gesamte „Skalierung“ der Schwierigkeits- und Kompetenzwerte unter Zuhilfenahme eines komplexen mathematischen Modells des Schülerantwortverhaltens, der sogenannten Item-Response-Theorie berechnet.

Die Aufgabenschwierigkeitswerte erlauben ansatzweise eine „didaktische“ Interpretation der Testergebnisse: Wenn ein Schüler beispielsweise 530 Kompetenzpunkte erzielt hat, dann kann er mit 62-prozentiger Wahrscheinlichkeit (die Zahl 62 Prozent ist willkürlich festgelegt worden) eine Aufgabe der Schwierigkeit 530 lösen. Wenn man sich nun veröffentlichte Aufgabenbeispiele anschaut, deren Schwierigkeitswert in der Nähe von 530 liegt, dann bekommt man einen Eindruck, was ein Kompetenzwert von 530 bedeutet. Allerdings muss man dabei beachten, dass der Test unter erheblichem Zeitdruck stattfindet (knapp über zwei Minuten pro Aufgabe).

Fast alle weiterführenden Auswertungen beruhen darauf, dass die statistische Verteilung der Schülerkompetenzwerte in den Teilnehmerstaaten oder feiner aufgeschlüsselten Populationen untersucht wird.

PISA misst Schülerleistung in Punkten auf einer willkürlichen Skala. Interpretierbar werden die Punktwerte erst, wenn sie in einen Kontext gesetzt werden. Das geschieht regelmäßig durch den Vergleich zwischen verschiedenen Ländern. Die Berichte der OECD und ihrer Projektpartner bestehen dementsprechend zu einem erheblichen Teil aus Länder-Ranglisten.

Die elementarste und meistbeachtete Statistik fasst die Schülerleistungen zu Mittelwerten zusammen. In der folgenden Tabelle sind die bisherigen Ergebnisse der mehrheitlich deutschsprachigen Staaten, einiger weiterer OECD-Staaten sowie einiger Nicht-OECD-Staaten (kursiv) zusammengefasst; in Klammern der OECD-Rangplatz.

Shanghai (China) belegt den ersten Platz in allen Fächern der Jahre 2009 und 2012. Im Jahr 2015 belegte Singapur den ersten Platz. Neben Finnland, Japan und Kanada befinden sich auch Südkorea, Neuseeland, Australien und das nicht zur OECD gehörige Territorium Hongkong regelmäßig in der Spitzengruppe. Vor der Türkei und Mexiko befinden sich am Tabellenende neben Italien regelmäßig Portugal, Griechenland und Luxemburg.

Bei einer Aufschlüsselung nach Sprachgruppen fällt auf:


Die Ergebnisse aus Liechtenstein haben erhöhte Fehlerbalken, weil dort kaum mehr als 350 Fünfzehnjährige wohnen. Immerhin entfallen die diversen Probleme der Stichprobenziehung, denn es wurde, wie auch in Luxemburg, ein Kompletttest aller Schüler durchgeführt. Außerdem ist Liechtenstein das einzige Land, welches nicht von nationalen Organisationen getestet wird, sondern von der Pädagogischen Hochschule St. Gallen aus der benachbarten Schweiz.

Zu den starken Unterschieden zwischen den deutschen Bundesländern → PISA-E.

Die Korrelation mit den TIMSS-Studien, die in einigen Staaten parallel zu PISA fortgeführt werden, ist mäßig, was offiziell mit unterschiedlichen Inhalten und mit Normierungseffekten aufgrund unterschiedlicher Teilnehmerschaft erklärt wird.

Um den zahlenmäßigen Ergebnissen eine anschauliche Bedeutung zu geben, teilt das Konsortium die Punkteskala willkürlich in sechs »Kompetenzstufen« und eine darunter liegende Stufe absoluter Inkompetenz. Anhand der Aufgaben, die auf einer Stufe zu lösen sind, wird dann eine verbale Beschreibung dessen, was Schüler auf einer bestimmten Stufe typischerweise können, erarbeitet. Zu beachten ist dabei, dass der Anteil der Schüler auf einer bestimmten Stufe im OECD-Mittel konstant, weil durch die Konstruktion der Schwierigkeits- und Leistungsskalen festgelegt ist. Interpretierbar sind lediglich die zumeist geringen Unterschiede zwischen Staaten.

Schüler „unterhalb“ der Stufe 1 werden international als „at risk“ bezeichnet. Die deutsche Projektleitung hat den Begriff „Risikogruppe“ jedoch ausgedehnt und die Stufe 1 darin einbezogen. Das wurde in Teilen der Öffentlichkeit verkürzt und im Gegensatz zu Aussagen der internationalen Berichte so rezipiert, als sei ein knappes Viertel aller Fünfzehnjährigen nicht in der Lage, zu rechnen und sinnerfassend zu lesen.

Im Anschluss an die zweistündige Testsitzung zur Messung kognitiver Leistungen bearbeiten die Schüler einen Fragebogen mit Fragen zum familiären Hintergrund, zum schulischen Umfeld, zu Lerngewohnheiten und ähnlichem. In den offiziellen Ergebnisberichten und in zahlreichen Sekundärstudien wird dargestellt, wie sich diese Kontextvariablen auf die kognitive Testleistung auswirken.

In PISA 2000 wurde festgestellt, dass der Zusammenhang zwischen Testergebnis und elterlichem Beruf in Deutschland so stark ist wie nirgendwo sonst. In den Folgerunden wurde dieses Ergebnis jedoch nicht repliziert; der stärkste Zusammenhang wurde 2003 in Ungarn, 2006 in der Tschechischen Republik gefunden. Die deutschen Kennwerte (Quantildifferenzen, Gradienten und Korrelationskoeffizienten der Testleistung als Funktion einer Berufsklassifikation oder eines sozial-ökonomisch-kulturellen Indexes) lagen überwiegend im oberen Teil eines breiten Mittelfeldes; die Abweichungen vom OECD-Durchschnitt waren teilweise statistisch insignifikant.

Diesen Auswertungen liegen unterschiedliche Sozialindizes zugrunde, die zum Teil nur den Beruf der Eltern, zum Teil auch deren Bildungsabschlüsse und die Ausstattung des Haushalts mit kulturellen Besitztümern berücksichtigen. Zwischen dem deutschen Konsortium und der internationalen Projektleitung besteht Dissens über die sachgerechte Quantifizierung von sozialem Hintergrund; im deutschen Bericht zu PISA 2006 wird durchgehend ein anderer Index verwendet als im internationalen Bericht.

Ein weiteres Beispiel bei der Dateninterpretation besteht darin, dass sozialer Status und Einwanderungshintergrund stark miteinander korreliert sind. Die Daten alleine sagen nicht, inwieweit schwache Leistungen von Migrantenkindern ihrer unterdurchschnittlichen sozialen Lage oder zum Beispiel ihrer ungenügenden sprachlichen Integration zuzuschreiben sind.

In Deutschland ist überraschend, dass Einwanderer der ersten Generation (454 Punkte, Mathematikleistung 2003) im Schnitt besser abschneiden als im Land geborene Kinder zugewanderter Eltern (2. Generation, 432 Punkte); Schüler ohne Migrationshintergrund: 525 (OECD-weite Vergleichszahlen in derselben Reihenfolge: 475, 483, 523). Daraus wurde teilweise geschlossen, Kinder der zweiten Generation zeigten in Deutschland generell schlechtere Leistungen als Kinder der ersten Generation. Die weitere Aufschlüsselung hat dieses paradoxe Ergebnis jedoch damit erklärt, dass die Anteile der wichtigsten Herkunftsländer innerhalb der Kinder der ersten und der zweiten Generation deutlich unterschiedlich sind (z. B. größerer Anteil Jugendlicher aus der Türkei innerhalb der 2. Generation; vgl. die Erläuterungen zu ). Beim selben Herkunftsland sind die Ergebnisse der zweiten Generation durchweg besser als die der ersten.

Ein quantitativ bedeutsames Problem stellen die schwachen Leistungen türkischer Jugendlicher dar (Mathematikleistung 2003: erste Generation 382, zweite Generation 411). Erstaunlicherweise schnitten Schüler mit Migrationshintergrund bei sprachlastigen Aufgaben etwas besser ab als bei relativ sprachfreien; die Gründe dafür sind ungeklärt.

Die Ergebnisse aus PISA 2003 zeigen einen erheblichen Leistungsvorsprung der Mädchen im Lesen (34 Punkte international, 42 Punkte in Deutschland). Vierfach geringer ist der Vorsprung der Jungen in Mathematik (11 Punkte international, 9 Punkte in Deutschland). Im Problemlösen liegen Mädchen vorne (2 Punkte international, 6 Punkte in Deutschland), wobei diese Differenz nicht signifikant ist. In den Naturwissenschaften wurde ebenfalls kein signifikanter Geschlechterunterschied gefunden (international liegen Jungen vorne mit 6 Punkten, 6 Punkte in Deutschland). Wuttke (2007) versucht nachzuweisen, dass dieses Ergebnis allein an der Mischung von Aufgaben aus verschiedensten Gebieten liege; in Übereinstimmung mit nationalen, lehrplannäheren Tests findet er die Alltagsbeobachtung bestätigt, dass im Mittel Jungen in Physik, Mädchen in Biologie höhere Leistungen erzielten.

PISA 2009 hat gezeigt, dass die Kompetenzunterschiede zwischen Mädchen und Jungen seit PISA 2000 in Deutschland praktisch unverändert geblieben sind. Der OECD-Bericht "Equally prepared for life? How 15 year-old boys and girls perform in school" untersucht geschlechtsspezifische Leistungsunterschiede und stützt sich dabei primär auf die Ergebnisse aus PISA 2009 (sowie auf die IGLU- und TIMSS-Studien). Die Forscher kommen zu dem Schluss, dass geschlechtsbezogene Vorurteile („Rollenmodelle“ und „Stereotype“ in der Sprache von PISA) die Bildungsergebnisse von Mädchen beeinflussen. Mädchen hätten eine geringere Selbsteinschätzung in Bezug auf mathematische Kompetenzen, weil Stereotype und falsche Rollenmodelle dieses forcierten, deshalb erbrächten sie schlechtere Leistungen – und nicht etwa umgekehrt. Auch die Entscheidung über den weiteren Bildungsweg und Beruf werde im Falle der Mädchen v. a. durch gesellschaftliche Entwicklungen und weniger durch die Mädchen und jungen Frauen selbst beeinflusst. Die stärker vorhandenen diagnostizierten Sprachschwächen unter Jungen werden da, wo sie überhaupt thematisiert werden, v. a. durch deren pädagogikinkompatibles Verhalten erklärt, Jungen passten weniger in eine feminisierte Lernumwelt wie Schule und lehnten als „unmännlich“ bewertete Tätigkeiten wie Lesen eher ab. 

<nowiki>*</nowiki>PISA-Test wurden nur in den Provinzen Guangdong und Jiangsu sowie den Städten Shanghai und Peking durchgeführt in denen zusammengenommen ca. 200 Millionen Einwohner leben.

Der Schwerpunkt der PISA Studie von 2009 lag wie bei der ersten Studie aus dem Jahr 2000 auf der Lesekompetenz. Der Vergleich mit der ersten Studie dokumentiert für Deutschland eine positive Entwicklung auf breiter Basis:


In einigen Feldern sind die positiven Entwicklungen schwächer ausgeprägt:


PISA 2000, PISA 2003 und PISA 2006 haben in einigen Teilnehmerstaaten ein heftiges Medienecho ausgelöst; in Deutschland ist das Wort „PISA“ zum Inbegriff aller Probleme des Bildungswesens geworden.

Deutschland hatte in den 1970er und 80er Jahren an keinem internationalen Schulvergleich teilgenommen. Der Richtungswechsel begann mit der Teilnahme an der Mathematikstudie TIMSS 1995. Die mittelmäßigen Ergebnisse wurden von Bildungspolitikern und Fachlehrern ausgiebig diskutiert, drangen aber nur kurz an die breite Öffentlichkeit.

Die Veröffentlichung der ersten PISA-Ergebnisse Ende 2001 wurde durch Vorabberichte mehrere Wochen lang vorbereitet und erzielte ein so überwältigendes Medien-Echo, dass bald von einem „PISA-Schock“ gesprochen wurde, was an den „Sputnikschock“ und die Debatte der 1960er Jahre um die von Georg Picht beschworene „Bildungskatastrophe“ erinnerte.

Ende 2002 beherrschte PISA erneut die Schlagzeilen, weil zum ersten Mal ein Leistungsvergleich zwischen den Bundesländern veröffentlicht wurde. Das Nord-Süd-Gefälle überraschte relativ wenig; in vielen Kommentaren wurde es durch den Verweis relativiert, dass in Bayern die Abiturquote zu niedrig und ein bestimmter statistischer Kennwert für soziale Selektivität besonders hoch sei (siehe Hauptartikel PISA-E).

Wenige Tage vor der Veröffentlichung von PISA 2006 (Teilergebnisse waren auch diesmal an die Presse durchgesickert) eskalierte der Konflikt zwischen der OECD und der deutschen Projektgruppe. Andreas Schleicher bezog sich auf den OECD-Bericht, dem zufolge weder in den drei Bereichen Naturwissenschaft, Mathematik, Leseleistung noch bei der Koppelung der Ergebnisse mit sozialer Herkunft Verbesserungen erreicht wurden, die über den statistischen Fehlerbereich hinausgingen. Die naturwissenschaftliche Aufgabenstellung sei im Wesentlichen neu konzipiert, in den wenigen Testaufgaben, die sowohl 2003 als auch 2006 verwendet wurden, seien die Leistungen unverändert. Umweltthemen hätten Deutschland 2006 begünstigt. Der deutsche Projektleiter Manfred Prenzel behauptete demgegenüber, die Ergebnisse seien sehr wohl zu vergleichen. Diese unterschiedlichen Einschätzungen stehen auch in den offiziellen Berichten.

CDU-Kultusminister, insbesondere die hessische Ministerin Karin Wolff, nahmen das zum Anlass, Schleichers Entlassung zu fordern. Akut warf man ihm vor, mit der Kommentierung einer Vorveröffentlichung eine selbstgesetzte Sperrfrist verletzt zu haben. Die grundlegenden Vorwürfe an ihn waren:

Die OECD wies diese Vorwürfe zurück. Die Interpretation entspreche den Ergebnissen der Studie. Eine Vergleichbarkeit über die Jahre sei sehr wohl gegeben, allerdings nicht in den jeweils neu getesteten Einzelbereichen. Es sei so geplant gewesen, dass aufbauend alle drei Jahre ein neuer Bereich getestet werde. Nach der Leseleistung 2000 waren dies 2003 die mathematischen und 2006 erst die naturwissenschaftlichen Kenntnisse.

Einige CDU-Kultusminister erwogen laut, aus der PISA-Studie auszusteigen. Als Alternativen wurden der Bildungsmonitor der wirtschaftsnahen Initiative Neue Soziale Marktwirtschaft oder das Institut zur Qualitätsentwicklung im Bildungswesen genannt. Gleichzeitig drohte die OECD dem deutschen Projektleiter damit, dem ländervergleichenden PISA-E-Test das Label „PISA“ zu entziehen, wenn er die Daten nach seiner eigenen Methode skaliere, da dies dann nichts mehr mit PISA zu tun habe.

Der PISA-Konflikt reiht sich ein in eine Auseinandersetzung zwischen konservativen Bildungspolitikern in Deutschland und internationalen Organisationen. Streitpunkt war jedes Mal das dreigliedrige Schulsystem und die Frage, ob dieses Migrantenkinder und Kinder aus unteren Schichten benachteilige. Im Dezember 2006 wurde ein entsprechendes Papier der EU-Kommission zurückgewiesen, und im Frühjahr 2007 wurde der UN-Beobachter für das Menschenrecht auf Bildung, Vernor Muñoz, für seinen kritischen Bericht scharf angegriffen. Bereits im Sommer 2007 wurde die internationale jährlich erscheinende OECD-Studie Bildung auf einen Blick von konservativen Lehrerverbänden als „ideologisch“ bezeichnet. Ein großes Problem der Pisastudie liegt darin, dass die OECD lediglich zwischen Schülern, die in Deutschland geboren sind, und solchen, die im Ausland geboren sind, unterscheidet. Auf diese Weise lassen sich die schulischen Erfolge von Migrantenkindern und Kindern mit rein deutschen Wurzeln nur bedingt vergleichen.

In Österreich trat der „PISA-Schock“ verspätet ein: Nachdem man sich 2000 noch daran delektiert hatte, deutlich besser als Deutschland abgeschnitten zu haben, wurde das Ergebnis aus 2003 als „Absturz“ wahrgenommen. Daraufhin veranlasste Bildungsministerin Elisabeth Gehrer eine Überprüfung durch die Statistiker Erich Neuwirth, Ivo Ponocny und Wilfried Grossmann, die in ihrem 2006 erschienenen Untersuchungsbericht zahlreiche Ungereimtheiten bei der Stichprobenziehung und Datenauswertung zutage förderten. Insbesondere war 2000 die Stichprobe in den Berufsschulen nicht korrekt gezogen worden. In einem Vorwort zu diesem Untersuchungsbericht spielt der PISA-Koordinator der OECD, Andreas Schleicher, die Korrekturen herunter und behauptet, die OECD habe bislang darauf hingewiesen, dass eine Interpretation der österreichischen Ergebnisse nur „eingeschränkt zulässig“ gewesen sei. Nach weiterer Verzögerung hat die OECD Anfang 2007 nahezu unbeachtet die Ergebnisse von Neuwirth et al. in den internationalen Datensatz übernommen und damit die österreichischen Ergebnisse aus 2000 offiziell nach unten korrigiert. Diesen korrigierten Daten zufolge hat es nie einen Absturz gegeben; 2000 und 2003 lagen die österreichischen Schülerleistungen gleichermaßen im Mittelfeld; es gab keine statistisch signifikanten Veränderungen.

Aufschlussreich ist die offensichtliche Falschberechnung des PISA-Ergebnisses für 2000 deshalb, weil durch den angeblichen PISA-Absturz von 2000 auf 2003 zunächst der neu angetretenen konservativen Regierung bescheinigt werden sollte, sie habe durch ihre Bildungspolitik den Absturz verschuldet. Die nach den Wahlen im Oktober 2006 zurückgetretene konservative Unterrichtsministerin Elisabeth Gehrer (ÖVP) wollte hingegen durch die von ihr beauftragte Schlechter-Bewertung des unter einer sozialdemokratischen Regierung zustande gekommenen Ergebnisses für 2000 den Absturz widerlegen. Beide Male wurde das österreichische PISA-Ergebnis parteipolitisch instrumentalisiert bzw. nach oben und dann nach unten gerechnet, ohne dass das PISA-Reglement hinreichend klar festgelegt hätte, wie Ergebnisse richtig zu berechnen und Stichproben weltweit einheitlich festzulegen sind.

Auch das Ergebnis der PISA-Studie 2009 wirft grundsätzliche Fragen bezüglich Validität und Aussagekraft auf. Das PISA-Konsortium räumte offiziell ein, dass zahlreiche Fragebögen deutliche Spuren eines von den Lehrergewerkschaften ausgerufenen Boykotts aufwiesen. Dennoch wurde die österreichische Stichprobe ausgewertet, nachdem jene Fragebögen ausgeschieden worden waren. Das Ergebnis fiel in allen drei Bereichen deutlich schlechter aus als in den Testdurchgängen davor (2000–2006) und danach (2012), was für eine Verfälschung des Ergebnisses durch den Boykott spricht. In jahresübergreifenden Trendvergleichen schließt die OECD die österreichischen Daten für 2009 jetzt aus und bezeichnet sie als fehlend, ohne eine Erklärung dafür zu liefern. 

Ähnlich wie in Deutschland sind soziale Unterschiede auffällig. Die Regierungsparteien (ÖVP und FPÖ) verwiesen bevorzugt auf schlechte Deutschkenntnisse von Ausländerkindern. Die damalige Ministerin Gehrer konstatierte weiterhin ein Fehlverhalten von Eltern, die sich zu wenig um ihre Kinder kümmern würden. Die Opposition (SPÖ und Grüne) äußerte den Vorschlag, statt des stark diversifizierten Schulensystems eine Gesamtschule einzuführen. Diese Idee ist stark beeinflusst vom finnischen Vorbild. Im dortigen Schulsystem gibt es zwar extreme Leistungsunterschiede innerhalb, aber kaum zwischen den Schulen. In Österreich jedoch war das Gegenteil zu spüren.

Ein am Fall Österreich virulent gewordenes technisch-statistisches Problem scheint indes weiterhin ungelöst zu sein, denn PISA wird in den einzelnen Ländern nach wie vor nicht nach einheitlichen Kriterien durchgeführt. In Österreich werden auch Lehrlinge, Migranten ohne Deutschkenntnisse und Sonderschüler getestet. Für PISA 2009 wurden in Tirol etwa an der Hauptschule Neustift ausschließlich drei Schüler mit Migrationshintergrund ausgelost, die keine reguläre Schullaufbahn hinter sich hatten, erst seit wenigen Jahren in Österreich lebten und mit 16 Jahren auch gar nicht der "target population" anzugehören scheinen.

Das italienische PISA-Zentrum INVALSI ermöglicht seit 2003 den Regionen und autonomen Provinzen an PISA mit einer so großen Stichprobe teilzunehmen, dass eine separate Auswertung zulässig ist. Für die Autonome Provinz Bozen – Südtirol werden die deutschsprachigen Testhefte aus Österreich übernommen. Die Provinz erzielt regelmäßig weit über dem nationalen Durchschnitt liegende Ergebnisse.

Luxemburg liegt in den drei geprüften Fächern – Lesen, Mathematik und Naturwissenschaften – unter dem OECD-Durchschnitt und rangiert hinter den meisten anderen europäischen Ländern.
Bei der Interpretation der Resultate können u. a. folgende Hintergründe wichtig sein:


Finnland wurde in der öffentlichen Rezeption in Deutschland und Österreich allgemein als „Testsieger“ angesehen. Zahlreiche Erklärungen für das exzellente Abschneiden Finnlands wurden vorgeschlagen (siehe auch: Bildungssystem Finnland):


Die Begeisterung für Finnland rief auch kritische Stimmen auf den Plan, die darauf hinwiesen, dass Alkoholismus unter finnischen Schülern weitverbreitet und die Selbstmordrate alarmierend hoch sei. Aus statistischer Sicht relativiert sich das gute Abschneiden Finnlands, sobald man demographische, insbesondere soziale, Hintergrundvariablen kontrolliert.

Befürworter der Gesamtschule nutzten die PISA-Ergebnisse für eine Neuauflage der deutschen und österreichischen Schulstrukturdebatte. Sie verwiesen insbesondere auf:


Gegner wenden ein, dass die PISA-Ergebnisse keineswegs eindeutig sind:


Als unmittelbare Reaktion auf den PISA-Schock beschlossen die deutschen Kultusminister die Entwicklung bundesweiter „Bildungsstandards“ und die Gründung des Instituts zur Qualitätsentwicklung im Bildungswesen, das diese Standards in Form von Testaufgaben operationalisiert.

Es war politisch von Anfang an beabsichtigt, dass PISA auf die Schulwirklichkeit zurückwirken solle. Beteiligte Mathematikdidaktiker hofften beispielsweise, ihre Vorstellung von sinnvollem Unterricht durchzusetzen (Meyerhöfer in Jahnke/Meyerhöfer 2007). Konkret spürbar ist der Einfluss der PISA-Beispielaufgaben zum Beispiel, wenn in neuen Mathematiklehrplänen verstärktes Gewicht auf das Arbeiten mit Graphiken und Tabellen gelegt wird.

Die PISA-Studien haben nicht nur ein außergewöhnliches Medienecho, sondern auch heftige wissenschaftliche Debatten ausgelöst. Aufgrund der Komplexität des Gegenstands ist die Kritik ein interdisziplinäres Unterfangen, an dem sich sowohl Pädagogen als auch Psychologen und andere Wissenschaftler mit statistischer Fachkunde (Mathematiker, Physiker, Ökonomen) beteiligen. Je nach Herkunft haben sie ihre Anmerkungen an weit gestreuten, zum Teil entlegenen Stellen veröffentlicht. Erst mit einiger Verzögerung erschienen erste Sammelbände, die die zuvor verstreute Kritik bündelten (Jahnke/Meyerhöfer 2006, erweitert 2007; Hopmann/Brinek/Retzl 2007).

Im Mai 2014 veröffentlichten der Pädagogikprofessor Heinz-Dieter Meyer (State University of New York) und die Schulleiterin Katie Zahedi einen offenen Brief, der auf negative Folgen von PISA hinwies und die Forderung erhob, den dreijährigen Testzyklus zugunsten einer Denkpause zu unterbrechen; diesem Aufruf schlossen sich Hunderte Pädagogikprofessoren, Vertreter von Lehrerverbänden und prominente Intellektuelle (u. a. Noam Chomsky, Konrad Liessmann) an.

Das utilitaristische Bildungsziel von PISA wird insbesondere von frankophonen Autoren kritisiert: Es bewirke zunächst einmal eine Verzerrung der Testergebnisse zugunsten angelsächsischer Staaten und sodann einen Druck, Lehrpläne so anzupassen, dass unmittelbar alltagsrelevante Fertigkeiten ein größeres Gewicht bekämen. Das bedrohe zum Beispiel die Spezifität des französischen Mathematikunterrichts, der großen Wert auf strenge Beweise legt. In diesem Zusammenhang wird auf die ökonomische Zielsetzung der OECD und auf die Intransparenz und mangelnde demokratische Legitimität der Entscheidungsprozesse bei PISA hingewiesen. Ein ähnlicher Einwand lautet, dass PISA mit seinen Schwerpunkten Mathematik, Muttersprache, Naturwissenschaften die Marginalisierung gesellschaftswissenschaftlicher und musischer Fächer forciere.

Der Mathematikdidaktiker Thomas Jahnke kritisiert den Grundgedanken, Bildung „standardisieren“ zu wollen (vgl. Bildungsstandards), und deutet PISA auch als Markterschließung der Testindustrie. Der Philosoph Konrad Paul Liessmann kritisiert PISA als ökonomischen Versuch, die (humanistische) Bildung im Grunde abschaffen zu wollen und durch simples Wissen (im Gegensatz zu Bildung) zu ersetzen. Er beklagt die Transformation der Bildungseinrichtung Schule in eine Berufsschule für Kinder und damit das Ende des bewussten und geistigen Menschen und seine Reduktion auf einen Arbeitnehmer und Konsumenten.

Zentral an der PISA-Studie beteiligte Wissenschaftler, wie der Bildungsforscher Eckhard Klieme, bestätigen in diesem Zusammenhang durchaus, dass die Studie auch ein Instrument der Bildungspolitik sei und die OECD eine Agenda mit PISA verfolge. Sie halten dem aber entgegen, dass sie als verantwortliche Wissenschaftler eigene Beiträge in die öffentliche Debatte einbringen, um PISA nicht instrumentalisieren zu lassen, und alle Ergebnisse offen vorlägen. Denn letztlich seien die Studien dennoch ein „Instrument der Aufklärung, das uns viel über die Probleme unseres Bildungssystems verrät und zu mehr Ehrlichkeit und Transparenz beiträgt“.

Im Anschluss an die Testungen 2000 und 2003 wurde jeweils nur ein kleiner Teil der eingesetzten Aufgaben (der „Instrumente“ in der Sprache der Psychologie) veröffentlicht. Eine Vielzahl von Autoren hat diese Aufgabenbeispiele kritisiert, besonders gründlich der Mathematikdidaktiker Meyerhöfer. In einer didaktischen Analyse mit Methoden der objektiven Hermeneutik zeigt er, dass PISA dem Anspruch, eine spezielle „Mathematische Literalität“ zu testen, nicht gerecht wird.

Das seit den allerersten vergleichenden Schulstudien ungelöste Übersetzungsproblem bewirkt auf verschiedenen Wegen eine Verzerrung der internationalen Vergleiche:


Ein weiteres Problem ist die unterschiedliche Vertrautheit mit dem Aufgabenformat. Meyerhöfer spricht hier von „Testfähigkeit“; in den USA wird schon lange über die Bedeutung von „testwiseness“ diskutiert. Wuttke (2007) hat entdeckt, dass bis zu 10 Prozent der deutschsprachigen Schüler das Multiple-Choice-Format nicht verstehen und mehr als "eine" Antwortalternative ankreuzen.

Zusätzlich ist zu berücksichtigen, dass dieselben Aufgaben in verschiedenen Sprach- und Kulturkontexten nicht als gleichwertig bezüglich ihrer Schwierigkeit betrachtet werden können. Damit ist ein Unterschied der Ergebnisse zu erwarten, der nicht in der Leistungsfähigkeit der Probanden, sondern im Bereich der sprachlichen Übersetzbarkeit und Interpretierbarkeit der Aufgabenstellung liegt.

Bei der Auswertung von PISA und ähnlichen Studien stellt sich das Grundproblem, dass Leistungsunterschiede "innerhalb" eines jeden Staats wesentlich größer sind als typische Unterschiede "zwischen" Staaten. Es ist deshalb eine Messgenauigkeit im unteren Prozentbereich erforderlich, um statistisch signifikante Aussagen über solche Unterschiede treffen zu können. In PISA wird das formal durch die Verwendung sehr großer Stichproben (etwa 5000 Schüler/Staat) erreicht. Die offiziellen Standardfehler berücksichtigen jedoch nicht mögliche systematische Verzerrungen (Wuttke 2007). Solche Verzerrungen werden unter anderem bewirkt durch:


Aus systemtheoretischer Sicht wird kritisiert, dass die Systemgrenzen in PISA nicht sachgerecht seien (verglichen werden Staaten, auch wenn diese kein einheitliches Bildungssystem haben); dass die Gleichsetzung von Schülerleistung mit Schulsystemleistung nicht gerechtfertigt ist (weil es außer dem Schulsystem eine ganze Reihe anderer Eingangsgrößen gibt); dass ein Systemvergleich nur Denkanstöße, aber keine politischen Handlungsempfehlungen liefern kann (Bank 2008). Konkret kann man zum Beispiel die Vorbildfunktion des finnischen Schulsystems in Zweifel ziehen, wenn man die Eingangsgröße „Migrantenanteil“ herausrechnet und nicht mit einem fiktiven gesamtdeutschen Schulsystem, sondern konkret mit dem sächsischen oder bayerischen vergleicht (Bender 2006). Wenn man die PISA-Erfolge in Relation zu den Bildungsausgaben setzt, erscheint das finnische System sogar als vergleichsweise ineffizient (Bank 2008).

Das von PISA postulierte Bildungsziel „literacy“ führt zu einer Verwischung der Grenze zwischen den einzelnen Testgebieten. Die Ergebnisse sind hochkorreliert. Deshalb argumentiert Heiner Rindermann (2006), dass man PISA in guter Näherung als einen Intelligenztest deuten kann.

Das kriminologische Forschungsinstitut Niedersachsen e. V. stellte fest, dass jene Gruppen bei PISA am schlechtesten abschnitten, die sich durch den höchsten Medienkonsum auszeichnen.













</doc>
<doc id="9540" url="https://de.wikipedia.org/wiki?curid=9540" title="Nestorpapageien">
Nestorpapageien

Die Nestorpapageien ("Nestor") sind die einzige Gattung der Unterfamilie der Nestorpapageien (Nestorinae) innerhalb der Strigopidae.

Die Gattung besteht heute nur noch aus zwei rezenten Arten, die in Neuseeland vorkommen. Eine dritte Art, der Dünnschnabelnestor, lebte bis Mitte des 19. Jahrhunderts auf den Norfolkinseln nördlich davon, eine vierte auf den östlich der Südinsel gelegenen Chatham-Inseln. Nestorpapageien sind große und kräftig gebaute Papageien. Der Geschlechtsdimorphismus ist bei allen Arten nur schwach ausgeprägt. Charakteristisch für sie ist ein kurzer quadratischer Schwanz. Der Schaft der Steuerfedern überragt die Fahne und bildet eine dornartige Spitze. Die Wachshaut ist teilweise von borstigen Federn bedeckt. Die Unterseite des Oberschnabels weist längslaufende sägeartige Einkerbungen auf. An der Spitze der Zunge befindet sich ein haarartiger Saum.





</doc>
<doc id="9541" url="https://de.wikipedia.org/wiki?curid=9541" title="Weihnachtsmann">
Weihnachtsmann

Der Weihnachtsmann ist eine Symbolfigur weihnachtlichen Schenkens, die in Deutschland vor allem in Nord-, Mittel- und Ostdeutschland sowie in der übrigen Welt besonders in evangelisch geprägten Regionen, wie in der französischsprachigen Westschweiz "(Père Noël)," den Niederlanden, Skandinavien, Estland, Lettland, Großbritannien und den Vereinigten Staaten, populär ist.

Dargestellt wird er als rundlicher, freundlicher alter Mann mit langem weißem Rauschebart, rotem und mit weißem Pelz verbrämten Gewand; Attribute sind sein Geschenkesack und (häufig) eine Rute. Dass es diese Darstellung bereits im 19. Jahrhundert gab, beweisen zeitgenössische Postkarten.
Die Coca-Cola Company nutzte ab 1931 alljährlich zur Weihnachtszeit diese Darstellung für eigene Werbekampagnen.

Angeblich bringt der Weihnachtsmann braven Kindern am Heiligen Abend Geschenke, den unartigen hingegen bloß eine Rute. Er vereinigt somit Eigenschaften des heiligen Bischofs Nikolaus von Myra und seines Begleiters, des Knecht Ruprechts.

Die Gestalt des Weihnachtsmannes geht vor allem auf die europäischen Volkslegenden um den heiligen Nikolaus zurück; er ist aber keinesfalls mit diesem gleichzusetzen. Nikolaus von Myra war ein Bischof im 4. Jahrhundert, um den sich zahlreiche Legenden ranken. Schon im Mittelalter wurden im Hinblick darauf Kinder am Gedenktag des hl. Nikolaus, dem 6. Dezember, oft auch schon am Vorabend beschenkt. Dieses Datum war früher auch der Bescherungstag, der erst im Laufe der Reformation und infolge deren Ablehnung der Heiligenverehrung in vielen Ländern auf das Weihnachtsfest gelegt wurde (siehe auch Christkind). Großen Anteil an der Verbreitung der Geschichte vom Weihnachtsmann hatte der Schriftsteller August Heinrich Hoffmann von Fallersleben, der 1835 das Lied "Morgen kommt der Weihnachtsmann" schrieb.

Seit dem 13. Jahrhundert verkörperte in Kloster- und Stiftsschulen oftmals ein Schüler am Gedenktag des hl. Nikolaus den „Kinderbischof“ mit entsprechenden Weisungsrechten, ein Brauch, der später an einzelnen Orten wieder aufgenommen wurde. Nach dem Konzil von Trient, das Festlegungen zur Residenz- und Visitationspflicht der Bischöfe in ihren Diözesen traf, trat der heilige Nikolaus auch als Visitator auf, der, in Begleitung eines gezähmten Teufels, das Volk zu Hause besuchte und nachfragte, ob die Kinder im vergangenen Jahr brav oder unartig gewesen waren.

Die Tradition des Nikolaus lebt heute noch in Teilen Europas weiter. In den Niederlanden (Sinterklaas), Belgien, Luxemburg, Westdeutschland, Sachsen, Tschechien und der Schweiz kommt der Nikolaus am Vorabend des 6. Dezember ins Haus, um vorweihnachtliche Leckereien zu schenken oder diese in der Nacht in einen vor dem Fenster aufgestellten Nikolaus-Stiefel zu legen.

Ähnlich agiert er in Bayern und Österreich am 6. Dezember, wo vielfach am Tag davor die wohl von den winteraustreibenden Perchten (keltischen Ursprungs) inspiriertem Krampusse die Gegenspieler sind – in vollständig traditioneller Version mit einem Pferdefuß und zwei Hörnern, einer Reisigrute (die vielfach [spielerisch] benutzt wird) und über dem Teufelsschwanz einer auf den Rücken geschnallten Holzbutte, in der angeblich die „schlimmen“ Kinder abtransportiert werden. Krampustag ist eigentlich der 5. Dezember, dennoch kommt der Krampus aus praktischen Gründen oft erst am 6. Dezember gemeinsam mit dem Nikolaus, der ihn dann bei jedem Auftritt erfolgreich in die Schranken weist. Der Begriff der Perchten findet sich auch in der Bezeichnung „Knecht Ruprecht“ für den Begleiter des Nikolaus wieder.

Im fränkischen Raum ist der „Pelzmärtel“ (vom westmitteldeutschen "pelzen" = ‚prügeln‘ und "Märtel" als Verkleinerungsform von Martin) seit der Reformation als Gabenbringer verbreitet, im Schwäbischen tritt "Belzmärte" als dunkler Begleiter des Nikolaus auf. Hier dürften Elemente des Brauchtums am Martinstag (11. November) mit dem Nikolausbrauchtum zusammengeflossen sein. Auch „Pelznickel“ ("Nickel" als Verkleinerungsform von "Nikolaus") kommt im Unterfränkischen vor.

Neben dem Nikolaus hatte sich in zahlreichen Regionen des deutschsprachigen Raumes die Symbolgestalt des Christkinds herausgebildet, auch in Abgrenzung zum „Heiligen Christ“, den Martin Luther an die Stelle des heiligen Nikolaus gesetzt hatte. Der Brauch des Beschenkens der Kinder an Weihnachten wurde von Luther seit etwa 1535 als Alternative zur bisherigen Geschenksitte am Nikolaustag propagiert, um so das Interesse der Kinder auf Christus anstelle der Heiligenverehrung zu lenken. Die Beschenkung war seit der Reformation auf den Heiligen Abend verlegt worden. Im Volk blieb aber der Nikolaus als Geschenkeüberbringer ebenfalls populär.

Im bekannten Weihnachtslied "Morgen kommt der Weihnachtsmann", dessen Text 1835 von Hoffmann von Fallersleben verfasst wurde, lauten die ersten beiden Zeilen: „Morgen kommt der Weihnachtsmann, kommt mit seinen Gaben.“ Dies belegt, dass spätestens im Biedermeier die Rolle des Weihnachtsmanns als Gabenbringer allgemein bekannt war.

Die nordische Sagengestalt des Nisse (von dänisch "Niels" für "Nikolaus"), deutsch adaptiert als Wichtel, erinnert mit ihrer roten Mütze an den Weihnachtsmann. Davon abgeleitet ist der Brauch des Wichtelns in der Vorweihnachtszeit, in dem man sich gegenseitig und anonym in zufälliger Zuordnung von Schenkendem und Beschenktem beschenkt.

Im nördlichen Europa existiert seit alters her eine Gestalt, die mit Rute und Nüssen die Menschen auf die lange Winterszeit vorbereitet. Die Rute galt dabei als Fruchtbarkeitssymbol, die Nüsse als gehaltvolle und haltbare Nahrung. Dieser bärtige alte Mann war in einen langen braunen Winterpelz mit Kapuze gekleidet und fuhr auf einem Rentierschlitten, sein Wohnort ist Lappland. Man geht davon aus, dass hier noch Elemente des nordischen Gottes Odin sowie des Gottes Balder enthalten waren. <br>
In Finnland kennt man den Joulupukki. 
Die russische Variante des Weihnachtsmanns ist "Ded Moros" (, auch "Deduschka Moros"), „Väterchen Frost“, wörtlich übersetzt „Großvater Frost“. Er trägt ein blau-weißes Kostüm, was für Frost und Kälte steht. An seiner Seite ist immer ein kleines Mädchen, die Enkelin des Ded Moros, die Snegurotschka (, „Schneeflöckchen“) genannt wird.

Europäische Auswanderer brachten den Sankt-Nikolaus-Brauch mit in die Vereinigten Staaten von Amerika. Insbesondere in den niederländischen Kolonien wurde das "Sinterklaasfeest" gefeiert, zumal Sankt Nikolaus auch der Schutzpatron von Nieuw Amsterdam, dem späteren New York, war. Aus dem holländischen "Sint Nicolaas" oder "Sinterklaas" wurde der englische "Saint Nicholas" oder "Santa Claus".

Der heutige populäre Mythos des Weihnachtsmanns, der mit einem von Rentieren gezogenen fliegenden Schlitten reist, nachts durch den Kamin in die Häuser steigt und dort die Geschenke verteilt, geht zurück auf das 1823 anonym veröffentlichte Gedicht "The Night before Christmas"; früher wurde es meist Clement Clarke Moore zugeschrieben, heute gelegentlich Major Henry Livingston Jr., aber auch diese Zuschreibung ist nicht völlig gesichert. Der Autor nennt in dem Gedicht auch die Namen seiner Rentiere: Dasher, Dancer, Prancer, Vixen, Comet, Cupid, Dunder und Blixem. Aus Dunder und Blixem wurde später Donner und Blitzen. Rudolph kam erst 1939 durch ein Gedicht von Robert L. May dazu, das die Vorlage für das Weihnachtslied "Rudolph, the Red-Nosed Reindeer" von Johnny Marks lieferte.

Die Gestalt des heiligen Bischofs Nikolaus wurde Mitte des 19. Jahrhunderts säkularisiert und verlor ihren Ornat (Albe, Stola und Chormantel oder Messgewand), den Bischofsstab und die Mitra. An die Stelle des liturgischen Gewands traten Mantel und Zipfelmütze, welche an die kleinasiatische Phrygische Mütze erinnert. Möglicherweise flossen Elemente von Knecht Ruprecht und Wintergestalten wie dem rauhen Percht mit ein.

Eine der ersten Beschreibungen, die der heutigen Form des Weihnachtsmannes ähnelt, stammt aus einem Gedicht des New Yorkers William Gilley. Dieser beschrieb im Jahr 1821 "Santeclaus" als ganz in Fell gekleidet und auf einem von Rentieren gezogenen Schlitten fahrend.

Eine weitere Darstellung findet sich in dem Gedicht „‘Twas the night before Christmas“ (A Visit from St. Nicholas) von Clement Clarke Moore aus New York aus dem Jahr 1822, das erheblich größeren Einfluss hatte. Er beschrieb den Nikolaus als rundlichen, lustigen Elf mit rundem kleinen Bauch, ganz in Fell gekleidet, mit glitzernden Augen, rosigen Bäckchen, einer Nase wie eine Kirsche, einem langen schneeweißen Bart und einer Pfeife.

Die Darstellung des Nikolaus im weltweit verbreiteten Kinderbuch "Struwwelpeter" des Frankfurter Arztes Heinrich Hoffmann, das 1844 entstand, ist der heutigen Darstellung des Weihnachtsmannes sehr ähnlich. Aus derselben Zeit stammt eine Zeichnung von Moritz von Schwind im Münchner Bilderbogen Nr. 5 von 1848 unter dem Titel "Herr Winter", der eine gestrenge, ungeliebte Figur darstellt und zu dem die Menschen Distanz halten.

Der Deutsch-Amerikaner Thomas Nast, der im Jahr 1846 nach New York auswanderte und in den Vereinigten Staaten als Karikaturist bekannt wurde, zeichnete Weihnachten 1863 während des Amerikanischen Bürgerkrieges für das Magazin "Harper’s Weekly" einen alten, bärtigen Mann, der vom Schlitten herab die Soldaten der Unionstruppen beschenkt. Nasts Vorstellung vom Weihnachtsmann ging auf den pfälzischen „Belzenickel“ zurück, eine regionale, Pelz tragende Weihnachtsmannfigur aus dem 19. Jahrhundert, die er noch aus Kindheitstagen kannte. Als er später dazu aufgefordert wurde, seine Zeichnung zu kolorieren, wählte er die Farben rot und weiß. Später wurde daraus der Pfeife rauchende gemütliche und fidele Alte. Diese Geschichten malte Nast bis an sein Lebensende. 1923 wurde nach dieser Vorlage im Zuge einer Werbekampagne des New Yorker Getränkehersteller White Rock Beverages für sein beliebtes Dry Ginger Ale der heute bekannte Weihnachtsmann gezeichnet.

Schon zu Beginn des 20. Jahrhunderts erschien Santa Claus immer wieder in blauen, braunen, goldenen aber auch schon in roten Mänteln. Bereits 1822 beschrieb Clement Clark Moore in seinem Gedicht „A Visit From St. Nicholas“ einen gutmütigen, fröhlichen Mann in einem roten Mantel.
Die New York Times schrieb 1927: „Ein standardisierter Santa Claus erscheint den New Yorker Kindern. Größe, Gewicht, Statur sind ebenso vereinheitlicht wie das rote Gewand, die Mütze und der weiße Bart“. 

Die Coca-Cola-Company schreibt dazu auf ihrer deutschen Webseite: „Die heute bekannte Figur des Coca-Cola Santa Claus gibt es seit 1931. Verantwortlich für sein Aussehen ist der Cartoonist und Grafiker Haddon Sundblom. Er entwarf den freundlichen Gesichtsausdruck und den weißen Bart und kleidete ihn in den Coca-Cola Farben Rot und Weiß“.
Als Haddon Sundblom das Gesicht des Weihnachtsmann gestaltete, hatte er ein reales Modell vor Augen: Lou Prentiss, ein langjähriger Freund des Designers und ehemaliger Verkäufer der Company. Später nutzte Sundblom hilfsweise sein eigenes Spiegelbild.
Möglicherweise flossen in Sundbloms Gestalten auch skandinavische Motive ein. Bis 1964 zeichnete er jedes Jahr mindestens einen Weihnachtsmann für die Coca-Cola-Werbung und prägte, zusammen mit den immensen globalen Marketingkampagnen der Coca-Cola-Company, nachhaltig die Vorstellung des modernen Weihnachtsmannes.

Der amerikanische Santa Claus wohnt am Nordpol. Auch der europäische Weihnachtsmann wird im hohen Norden verortet. So lebt er etwa nach Auffassung der Finnen im lappländischen Korvatunturi; es wird allerdings auch behauptet, er lebe in Rovaniemi, vor allem seitdem dort 1998 ein SantaPark gestaltet wurde. Nach Überzeugung der Schweden lebt er in Dalarna, oder er hat in der alten Tradition als "tomte" keinen festen Wohnsitz. Nach dänischer Auffassung ist er in Grönland zu finden. In manchen Kinderbüchern hingegen wird der Weihnachtsmann als am Südpol wohnhaft beschrieben. Der Schweizer Nikolaus ist im Schwarzwald zuhause und geht am 6. Dezember mit seinem Esel auf Wanderschaft und wird vom Schmutzli begleitet. Der niederländische Sinterklaas hingegen kommt mit einem Dampfschiff aus Spanien, begleitet wird er von seinem Kompagnon, dem "Zwarten Piet". Andere behaupten, dass der Weihnachtsmann bzw. dessen Ursprung aus der Region von Antalya stammen soll, was sich wohl auf den historischen Nikolaus von Myra bezieht.

Der amerikanische Weihnachtsmann fährt seine Geschenke mit einem Schlitten aus, der von bis zu zwölf Rentieren gezogen wird. Bekanntestes Rentier ist Rudolph mit der roten Nase (Rudolph, the Red-Nosed Reindeer). In den letzten Jahren wurden in Deutschland zur Weihnachtszeit dem Weihnachtsmann ähnliche Puppen als „Fassadenkletterer“ an Fassaden angebracht. In Mietshäusern ist dazu die Erlaubnis des Vermieters erforderlich, und die Hausfassade darf nicht beschädigt werden.

Viele Kinder schicken in der Adventszeit Briefe mit Wünschen an den Weihnachtsmann. Diese werden in vielen Staaten im deutschsprachigen, angloamerikanischen und skandinavischen Kulturraum in eigens eingerichteten Weihnachtspostämtern gesammelt und zumeist auch beantwortet.

In Deutschland beantworten Briefmarkensammler im Dorf Himmelreich, einem Ortsteil von Neustadt am Rübenberge bei Hannover, zu Weihnachten regelmäßig die Post, die von Kindern an den „Weihnachtsmann im Himmelreich“ geschickt wird. Das geschieht auch in dem kleinen Dorf Himmelreich (beim Höllental) im Schwarzwald. Das einzige bayrische Weihnachtspostamt gibt es in Himmelstadt, in der Nähe von Würzburg.

Weitaus bekannter ist jedoch das Weihnachtspostamt im brandenburgischen Himmelpfort in der Nähe von Fürstenberg nördlich von Berlin, wo jährlich mittlerweile bis zu 200.000 Briefe aus aller Welt von ehrenamtlich tätigen Menschen beantwortet werden. Auch im niedersächsischen Himmelpforten in der Nähe von Stade werden seit 1961 Briefe an den Weihnachtsmann beantwortet. 2001 waren es etwa 23.000 Briefe, die das dortige Postamt erreichten.

Ein weiteres Weihnachtspostamt gibt oder gab es in Deutschland im Hildesheimer Stadtteil Himmelsthür sowie in Engelskirchen in Nordrhein-Westfalen und in St. Nikolaus südwestlich von Saarbrücken.

In Österreich erhalten seit 1950 zwischen dem ersten Adventsonntag und dem 6. Jänner Briefsendungen im Postamt Christkindl des praktischerweise seit drei Jahrhunderten Christkindl heißenden Wallfahrtsorts, der heute ein Ortsteil der Stadt Steyr ist, einen Sonderstempel (rund zwei Millionen jährlich).

"Die Briefe vom Weihnachtsmann" verfasste unter anderem der Autor J. R. R. Tolkien. Die von Aquarellen und Zeichnungen begleiteten Briefe an seine Kinder beginnen in den 1920er Jahren und erzählen Geschichten von dem am Nordpol lebenden Weihnachtsmann, dem tollpatschigen Nordpolarbär und gefährlichen Kobolden. Sie wurden in den 1970er Jahren posthum veröffentlicht.

Im Jahr 1897 schrieb die achtjährige Virginia O’Hanlon aus New York an die Zeitung "New York Sun" einen Brief, worin sie fragte: „Gibt es einen Weihnachtsmann?“ Die diffizile Sache erhielt der Redakteur Francis Pharcellus Church zur Beantwortung. Ihm glückte eine so überzeugende Darlegung, dass das Editorial über ein halbes Jahrhundert – bis zur Einstellung der "New York Sun" Anfang 1950 – alle Jahre wieder zur Weihnachtszeit auf der Titelseite der Zeitung gedruckt wurde. Seit 1977 – nachdem der "Welt-am-Sonntag"-Autor Rolf R. Bigler (1930–1978) die Idee dazu hatte – wird dieser Briefwechsel zur Weihnachtszeit in dieser Zeitung abgedruckt.

Besonders am Nikolaustag wie auch am 24. Dezember sind Schokoladenfiguren des hl. Nikolaus als auch des Weihnachtsmannes sehr beliebt. Um 1820 entstanden die ersten aus massiver Schokolade gefertigten Nikolausfiguren im Bischofsornat mit Mitra und Stab. Etwa 20 Jahre später wurden sie erstmals in der heute gewohnten Form als Hohlkörper hergestellt. Die Darstellung wechselte mit dem Aufkommen der Weihnachtsmannfigur überwiegend zur heute vorherrschenden Form.

Zur Herstellung wird flüssige Schokolade zwischen zwei Formen gebracht und im sogenannten Schleudergussverfahren solange gedreht, bis sich die Schokolade gleichmäßig verteilt hat und erkaltet ist. Über 9000 Tonnen Schokolade werden in Deutschland jährlich zu etwa 100 Millionen Schokoladenweihnachtsmännern verarbeitet. Damit liegt der Schokoladenweihnachtsmann an zweiter Stelle hinter dem Schokoladenosterhasen, auf den 56 % der Schokoladenhohlkörperproduktion entfallen (Zahlen aus dem Jahr 2002). Schokoladennikoläuse nehmen in Deutschland einen geringeren Marktanteil ein, gehören jedoch für einige Süßwarenhersteller ebenso zum festen Sortiment.

In Österreich sind nach wie vor Schokoladennikolaus und -krampus üblich, Schokoladenweihnachtsmänner sind hingegen eher seltener zu finden. Mittlerweile sind manche Produzenten dazu übergegangen, aus ein und denselben Gussformen sowohl Schokoladenweihnachtsmänner als auch -nikoläuse herzustellen, die sich lediglich durch ihre Verpackung unterscheiden.

Das bekannteste Lied "Morgen kommt der Weihnachtsmann" stammt von Hoffmann von Fallersleben (1798–1874). Es entstand etwa um 1840.

Der Weihnachtsmann erfreut sich in neueren Kinderbuchserien wachsender Beliebtheit. Dabei werden etwa in "Lauras Stern", "Conni" oder "Felix" eigene Bücher rund um Weihnachten gestaltet, in denen die Frage nach der Existenz des Weihnachtsmanns und seinen geheimnisvollen Fähigkeiten, Wünsche zu erraten und zu erfüllen, thematisiert werden.

In dem bekannten Märchen "Peterchens Mondfahrt" von Gerdt von Bassewitz (1912 uraufgeführt, 1915 als Buch erschienen) spielt der Weihnachtsmann mit seiner „Weihnachtwiese“, auf der alle Geschenke wachsen, eine herausgehobene Nebenrolle. Das Märchen wurde 1959 und 1990 verfilmt und häufig im Weihnachtsprogramm der ARD gesendet.

Siegfried Lenz verarbeitet das Motiv in seiner Kurzgeschichte "Risiko für Weihnachtsmänner" (1957).

In Ray Bradburys erstmals 1949 erschienenen Kurzgeschichte "Die Verbannten" ist der Weihnachtsmann zusammen mit anderen phantastischen Figuren auf den Mars geflüchtet, da Phantasie auf der von Wissenschaftlern dominierten Erde verboten ist. Eine "armselige, heruntergekommene" Figur, löst er sich wie die anderen Phantasiegeschöpfe in Luft auf, als die Raumschiffexpedition von der Erde auf dem Mars die letzten phantastischen Bücher verbrennt, die noch in einem Museum aufbewahrt worden waren.

In vielen US-amerikanischen Filmen ist das Thema „Glaube an den Weihnachtsmann“ als Erfüller von Kinderwünschen ein zentraler Punkt. "Das Wunder von Manhattan" wurde von 1947 bis 1994 mehrfach verfilmt. Der Schauspieler Edmund Gwenn erhielt für die Rolle sogar einen Oscar. Ebenfalls dieses Thematik haben die Weihnachtsfilme der "Santa-Clause"-Trilogie mit Tim Allen. Der Film "The Nightmare Before Christmas" (1993) ist dagegen ein satirisches Gegenstück dazu. Eine sehr klassische Abhandlung der Weihnachtsmann-Thematik bietet der Film "Der Polarexpress" aus dem Jahr 2004.

Als Gegengewicht zu den zahlreichen US-amerikanischen Santa-Claus-Filmen entwickelte der Kinderfernsehsender KiKA die Figur des Beutolomäus, unter anderem in dem Film "Beutolomäus und der geheime Weihnachtswunsch" (2006).




</doc>
<doc id="9544" url="https://de.wikipedia.org/wiki?curid=9544" title="Sukkulente">
Sukkulente

Sukkulenten (von lateinisch "sucus" für ‚Saft‘ bzw. "suculentus" für ‚saftreich‘) sind saftreiche Pflanzen, die an besondere Klima- und Bodenverhältnisse angepasst sind. Je nach dem Pflanzenorgan, das zur Wasserspeicherung umgebildet ist, wird zwischen Blatt-, Stamm- und Wurzelsukkulenten unterschieden, wobei alle Kombinationen möglich sind. Im Bereich der Anatomie wird flüssigkeitsreiches Gewebe als sukkulent bezeichnet.

Obwohl Kakteen nur einen sehr kleinen Teil aller existierenden Sukkulenten ausmachen, sind diese die bekanntesten Vertreter der sukkulenten Pflanzen. Im Sprachgebrauch wird deshalb zwischen Kakteen und „anderen“ Sukkulenten unterschieden.

Wegen der bei den einzelnen Arten sehr unterschiedlich ausgeprägten Sukkulenz ist eine genaue Trennung zwischen sukkulenten und nicht sukkulenten Arten manchmal sehr schwierig. Selbst innerhalb einer Art können sukkulente und nicht sukkulente Exemplare vorkommen. An der Grenzlinie liegen sowohl eher krautige oder holzige Pflanzen als auch solche, die in ihrer Rübenwurzel eher Zucker und Stärke als Wasser speichern. Sukkulenten werden gelegentlich auch als „Fettpflanzen“ bezeichnet.

Pflanzenfamilien und -gattungen, in denen sich Sukkulenten befinden, sind in der folgenden Tabelle aufgeführt. Um die Tabelle übersichtlich und lesbar zu halten, sind alle Familien und Gattungen alphabetisch nach ihrem botanischen Namen aufgeführt und mit den deutschen Trivialnamen der Familien, falls vorhanden, ergänzt. An dieser Stelle unberücksichtigt bleiben die deutschen Gattungsnamen, die Autoren der Taxa sowie Angaben über Synonyme und weitere hierarchische Stufe der biologischen Systematik (wie Unterfamilien), die den einzelnen Artikeln zu entnehmen sind.





</doc>
<doc id="9545" url="https://de.wikipedia.org/wiki?curid=9545" title="Kea">
Kea

Kea steht für:

kea steht für:

Kea ist der Familienname folgender Personen:

Kea ist der Vorname folgender Personen:

KEA steht für:


</doc>
<doc id="9546" url="https://de.wikipedia.org/wiki?curid=9546" title="Kakteengewächse">
Kakteengewächse

Die Kakteengewächse (Cactaceae) oder kurz Kakteen sind eine Familie in der Ordnung Nelkenartige (Caryophyllales) innerhalb der Bedecktsamigen Pflanzen (Magnoliopsida).

Kakteen sind ausdauernde Sträucher, seltener Bäume oder Geophyten. Fast alle Arten sind Stammsukkulenten, deren Sprossachsen stark angeschwollen sind. Die Wurzeln sind meist faserig oder bilden bei Pflanzen mit nur geringer Stammsukkulenz manchmal sukkulente Knollen oder Rüben. Die Hauptsprosse stehen, häufig charakteristisch für bestimmte Gattungen, einzeln oder verzweigen von den Basen oder weiter oben. Hauptsprosse und Zweige wachsen meist aufrecht oder aufstrebend, manchmal auch kriechend oder hängend. Die Sprosse sind zylindrisch oder zu Platykladien abgeflacht und tragen häufig gut ausgebildete Rippen oder spiralig arrangierte Warzen. Areolen, die stark reduzierte Kurztriebe darstellen, stehen auf zylindrischen oder abgeflachten Sprossen meist gefeldert verteilt oder sonst auf den Erhöhungen der Rippen oder Warzen. Sie sind filzig und tragen Dornen, die umgewandelte Blätter darstellen, sowie häufig auch Wolle oder Borsten. Filz und Dornen sind bei jungen Sämlingen immer vorhanden, werden aber manchmal später abgeworfen oder von erwachsenen Pflanzen nicht mehr gebildet. Die den Areolen entspringenden Laubblätter sind manchmal vollständig ausgebildet (Unterfamilie Pereskioideae), häufig pfriemförmig, sukkulent und kurzlebig (Unterfamilien Opuntioideae und Maihuenioideae), fehlen aber meist völlig (Unterfamilie Cactoideae). Nebenblätter sind nicht vorhanden.

Kakteen können sehr unterschiedliche Größen annehmen. "Carnegiea gigantea" wird bis zu 15 Meter hoch. Der kleinste Kaktus, "Blossfeldia liliputana", bildet dagegen flachkugelige Körper von kaum einem Zentimeter Durchmesser. Auch die Wuchsgeschwindigkeiten sind sehr unterschiedlich. Einige Cereen erreichen je Spross Zuwächse von mehr als 1 Meter pro Jahr. Bei "Aztekium ritteri" ist dagegen auch im Verlauf mehrerer Jahre kaum ein Zuwachs erkennbar.

Die Lebensdauer der Kakteen variiert ebenfalls stark. Langsam wachsende, groß werdende und erst im hohen Alter blühfähige Pflanzen wie "Carnegiea" und Arten von "Ferocactus" können bis zu 200 Jahre alt werden. Die Lebensspanne sich schnell entwickelnder und früh blühender Pflanzen ist dagegen kürzer. So wird die schon im zweiten Lebensjahr blühende, selbstfertile und reichlich Samen produzierende "Echinopsis mirabilis" selten älter als etwa 13 bis 15 Jahre.

Im Inneren der Pflanzen sind die Leitbündel entlang der Zentralachsen ringförmig, bei abgeflachten Sprossen oval angeordnet. Verzweigungen der Leitbündel führen jeweils zu einer Areole. Der enthaltene Saft ist fast immer klar, nur wenige Arten von "Mammillaria" enthalten Milchsaft.

Die Blüten entspringen meist einzeln, manchmal in kleinen Gruppen den Areolen, seltener (in und um "Mammillaria") den Axillen oder Furchen zwischen Areolen und Axillen. Manchmal werden sie nur in besonderen, stark bewollten oder beborsteten Bereichen (Cephalien), entweder entlang den Sprossachsen und in diese eingesenkt ("Espostoa", "Espostoopsis") oder endständig und den Wuchs begrenzend ("Melocactus", "Discocactus") gebildet. Die Blüten sind zwittrig und meist radiärsymmetrisch, seltener zygomorph. Die Durchmesser der Blüten variieren von 5 mm bis 30 cm, meist sind die Blüten jedoch relativ groß und bei kleinwüchsigen Arten oft größer als die Pflanzenkörper. Die vielen (fünf bis 50 oder mehr) Blütenhüllblätter wechseln meist in Form und Struktur von außen nach innen von hochblatt- zu kronblattartig. Staubblätter sind in großer Zahl (50 bis 1500, selten weniger) vorhanden. Je nach Anpassung an die Bestäuber (Tagfalter, Nachtfalter, Fledermäuse, Kolibris oder Bienen) sind die Blüten nachts (oft nur für wenige Stunden) oder tagsüber (dann meist mehrere Tage lang) geöffnet und röhren-, glocken- oder radförmig. Sie öffnen sich meist weit, bei röhrenförmiger Gestalt aber manchmal nur wenig. Selten (bei "Frailea") sind die Blüten kleistogam und öffnen sich nur ausnahmsweise. Die Fruchtknoten sind meist unterständig (in Unterfamilie Pereskioideae halb oberständig). Die die Fruchtknoten enthaltenden Bereiche der Blüte (Ovarien) sind von außen meist mit Schuppen, Dornen oder Wolle bewehrt und von innen mit Haaren abgetrennt.

Die beerenähnlichen, oft fleischigen und bei Reife auffällig gefärbten Früchte enthalten wenige bis meist viele (bis etwa 3000) 0,4 bis 12 mm große Samen. Ziegen, Vögel, Ameisen, Mäuse und Fledermäuse tragen wesentlich zur Verbreitung der Samen bei. Die Samen der meisten Kakteenarten sind Lichtkeimer.

Die Chromosomengrundzahl beträgt x = 11.

Das natürliche Vorkommen der Kakteen ist, mit Ausnahme von "Rhipsalis baccifera", auf den amerikanischen Kontinent beschränkt. Dort erstreckt sich ihr Verbreitungsgebiet vom südlichen Kanada bis nach Patagonien in Argentinien und Chile. Die größte Dichte an Kakteenvorkommen findet man in den Gebieten um den nördlichen (Mexiko) und südlichen Wendekreis (Argentinien/Bolivien).

Kakteen besiedeln die verschiedensten Lebensräume, von Tiefebenen bis zu Hochgebirgen, von tropischen Regenwäldern über Steppen und Halbwüste bis zu Trockenwüsten. Allen Lebensräumen ist gemein, dass das zum Überleben notwendige Wasser nicht ganzjährig, sondern nur saisonal zur Verfügung steht.

Als ihr Schwestertaxon gelten die (früher zu den Portulacaceae gezählten) Anacampserotaceae.

Die Pflanzenfamilie der Kakteen mit etwa 100 bis 130 Gattungen und 1500 bis 1800 Arten wird in vier Unterfamilien gegliedert:

Bereits bei den Azteken findet man in bildlichen Darstellungen, Skulpturen und Bezeichnungen immer wieder Kakteen, vor allem "Echinocactus grusonii". Dieser auch als „Schwiegermuttersessel“ bekannte Kaktus hatte große rituelle Bedeutung – auf ihm wurden Menschenopfer dargebracht. Tenochtitlán, das heutige Mexiko-Stadt, bedeutet "Ort des heiligen Kaktus". Das Staatswappen Mexikos trägt bis heute Adler, Schlange und Kaktus. Auch die wirtschaftliche Nutzung der Kakteen geht auf die Azteken zurück. Den Gehalt an Alkaloiden in manchen Kakteen nutzten die Indianer Nordamerikas für ihre rituellen Handlungen. Aus den gebogenen Dornen mancher Kakteen fertigten sie Angelhaken. 

Heutzutage dienen Kakteen neben der Verwendung als Nahrungsmittel (Marmelade, Obst, Gemüse) vor allem als Wirtspflanzen für die Cochenille-Laus, aus der roter Farbstoff für Campari oder qualitativ hochwertige Lippenstifte gewonnen wird. Besonders in Südamerika liefern abgestorbene Säulenkakteen wertvolles Bauholz. Auch für die Pharmazie haben einige Kakteen Bedeutung. Kakteen werden auch als Zimmerpflanzen kultiviert.

Kakteen erfreuten sich im Laufe der Zeit wachsender Beliebtheit, manchmal waren sie nur der Wissenschaft vorbehalten, oft erlebten sie als Modepflanzen einen regelrechten Boom. Seit Beginn des 20. Jahrhunderts stieg das Interesse an Kakteen stetig, unterbrochen nur durch die beiden Weltkriege. Damit verbunden war auch das steigende kommerzielle Interesse, dessen negative Auswüchse in regelrechten Raubzügen zu den Kakteenstandorten gipfelten und eine Ausrottung vieler Arten zur Folge hatten. Durch die große Anzahl an Kakteenliebhabern, sei es als Hobby oder aus wissenschaftlichem Interesse, werden heute noch jedes Jahr neue Arten und Varietäten gefunden.

Mit Ausnahme der Gattungen "Pereskia", "Pereskiopsis" und "Quiabentia" sind alle Kakteen im Washingtoner Artenschutzabkommen erfasst, viele Arten sind durch Aufnahme in den Anhang I vollkommen geschützt. Einige Länder nehmen eine etwas widersprüchliche Haltung zum Artenschutz ein. In Mexiko beispielsweise kann man eine Gefängnisstrafe bekommen, wenn man beim Ausgraben von Kakteen erwischt wird, andererseits werden Kakteenstandorte zugunsten neuer Straßen und Stromleitungen vernichtet. Bedenklich dabei ist vor allem, dass einige Kakteen-Standorte eine Ausdehnung von höchstens 1000 Quadratmeter besitzen. Wird dieser Standort vernichtet (Bauarbeiten, Plünderung), so ist die dort wachsende Art für die Nachwelt verloren, falls sie endemischen Charakter hat, also nur dort und sonst nirgends vorkommt.

Die folgenden deutschsprachigen Gesellschaften fördern die Kenntnis und Pflege der Kakteen und anderer Sukkulenten u. a. durch Erfahrungsaustausch, Ausstellungen, Vorträge und die Herausgabe der gemeinsamen Zeitschrift "Kakteen und andere Sukkulenten" (KuaS).




</doc>
<doc id="9547" url="https://de.wikipedia.org/wiki?curid=9547" title="Agavengewächse">
Agavengewächse

Die Agavengewächse (Agavoideae) sind eine Unterfamilie in der Familie der Spargelgewächse (Asparagaceae) innerhalb der Ordnung der Spargelartigen (Asparagales). Einige Arten werden als Zierpflanzen in Parks, Gärten und Räumen gezüchtet. Von wenigen Arten ist eine weitere Nutzung durch den Menschen bekannt: aus dem Saft einiger "Agave"-Arten wird Pulque hergestellt und die Fasern einiger "Agave"- und "Yucca"-Arten werden verarbeitet.

Es sind mehrjährige oder ausdauernde Pflanzen, es gibt krautige bis strauchförmig oder sogar baumförmig wachsende Arten; sie bilden Rhizome. Wenige Arten sind Epiphyten. Viele Arten sind immergrün. Viele Arten sind Sukkulenten, sie können also Wasser speichern. "Agave" und "Yucca" sind CAM-Pflanzen. Die Sprossachsen reichen oft nicht über die Bodenoberfläche, aber bei manchen Arten bilden sich in vielen Jahren einfache oder verzweigte Stämme aus. Die Laubblätter sind wechselständig und spiral und meist in Rosette angeordnet. Die Laubblätter sind immer einfach und parallelnervig (nie netznervig). Bei vielen Arten sind die Blätter am Rand bewehrt. Die Stomata sind anomocytisch.

Sie blühen oft erst im hohen Alter, wobei einige Arten nach der Bildung der Samen absterben, also monokarp sind (Hapaxanthe Pflanzen), aber die meisten Arten sind polykarp. Oft sind die einfachen oder verzweigten, traubigen Blütenstände reichblütig. Es sind zwei oder mehr Hochblätter vorhanden, die aber den Blütenstand nie vollständig umhüllen (Abgrenzung der Asparagaceae zu anderen Familien der Ordnung). Die dreizähligen Blüten sind meist zwittrig, oder seltener funktional eingeschlechtig. Es sind zwei Kreise mit mehr oder weniger gleichgestaltigen Blütenhüllblättern vorhanden; sie sind oft frei, können aber auch zu einer Röhre verwachsen sein. Es sind zwei Kreise mit je drei fertilen Staubblättern vorhanden, die die Blütenhülle überragen oder nicht. Die drei Fruchtblätter sind verwachsen. Die Nektarsekretion erfolgt in Septalnektarien.

Sie bilden meist Kapselfrüchte, einige "Yucca"-Arten bilden Beeren. Sie Samen sind meist flach und enthalten keine Stärke. Das Endosperm ist helobial und ölhaltig.

Die Chromosomenzahlen betragen oft 2n = 24, 30. Beispielsweise "Leucocrinum montanum" mit Basischromosomenzahlen von x = 11, 13, 14 oder "Echeandia" x = 8 weichen davon ab.

Bei vielen Taxa sind Saponine nachgewiesen.

Die Unterfamilie der Agavoideae besitzt eine fast weltweite Verbreitung. Der Schwerpunkt der Artenvielfalt liegt in der Neotropis. Die natürlichen Areale vieler reichen vom Süden der USA, über Mittelamerika, bis ins nördliche Südamerika und Asien. Jeweils einige Arten kommen beispielsweise in Malesien, nördlichen Australien und Neuseeland und auf Karibischen Inseln vor. Daneben gibt es Taxa auch in tropischen bis subtropischen Bereichen anderer Erdteile und einige Taxa gibt es auch in der Holarktis. Durch den Menschen gelangten einige amerikanische Arten zum Beispiel auch ins südliche Europa und in andere Gebiete der Erde. Die meisten Arten vertragen keinen Frost.

Die Unterfamilie wurde 1837 von William Herbert als „Agaveae“ aufgestellt. Typusgattung ist "Agave" 

2006 durch David John Bogler, Joseph Christopher Pires und Javier Francisco-Ortega ausgeführte molekulargenetische Untersuchungen ergaben folgende Verwandtschaftsverhältnisse:

Der Umfang einiger Gattungen wurde durch Neubearbeitungen in den letzten etwa zehn Jahren stark verändert. Es gibt heute 18 Gattungen mit über 600 Arten in der Unterfamilie der Agavengewächse (Agavoideae):


Molekulargenetische Untersuchungen haben in den letzten gut zehn Jahren dazu geführt, dass die Familiengrenzen innerhalb der Ordnung der Spargelartigen (Asparagales) sich stark verschoben haben. So sind die Agavengewächse heute nur mehr eine Unterfamilie (Agavoideae) in der Familie der Spargelgewächse (Asparagaceae). Zuvor war es eine eigenständige Familie Agavaceae. Die Gattungen waren früher auch in die Liliaceae eingegliedert. Die Familie Agavaceae wurde als Agavineae 1829 von Barthélemy Charles Joseph Dumortier in "Analyse des Familles de Plantes" 57, 58 erstbeschrieben.

Synonyme für Agavoideae sind: Agavaceae , Anthericaceae , Anemarrhenaceae , Behniaceae , Chlorogalaceae , Funkiaceae , Herreriaceae , Hesperocallidaceae , Hostaceae und Yuccaceae 



</doc>
<doc id="9549" url="https://de.wikipedia.org/wiki?curid=9549" title="Insekten">
Insekten

Insekten (lat. '), auch Kerbtiere oder Kerfe"' genannt, sind die artenreichste Klasse der Gliederfüßer (Arthropoda) und zugleich die mit absoluter Mehrheit auch artenreichste Klasse der Tiere überhaupt. Beinahe eine Million Insektenarten sind bisher wissenschaftlich beschrieben worden (925.000 nach Grimaldi/Engel 2005, 865.000 nach Nielsen/Mound 1997). Damit sind mehr als 60 Prozent aller beschriebenen Tierarten Insekten. Nach verschiedenen Hochrechnungen rechnet man allerdings mit einem Vielfachen, wobei vor allem in den tropischen Regenwäldern noch Millionen unentdeckter Arten vermutet werden. Fossil lassen sich Insekten zum ersten Mal vor rund 400 Millionen Jahren im Devon nachweisen.

Das Wort „Insekt“ (aus „eingeschnitten“) wurde im 18. Jahrhundert eingedeutscht und bedeutet demnach „eingeschnittenes (Tier)“, was sich auf die stark voneinander abgesetzten Körperteile bezieht. Es ist eine Lehnübersetzung von „Insekt“ (zu ), das in "Entomologie" (Insektenkunde) enthalten ist. Der Begriff „Kerbtier“ geht auf den deutschen Schriftsteller Philipp von Zesen zurück. Früher wurde auch der wissenschaftliche Name "Hexapoda" (griechisch für „Sechsfüßer“) verwendet, der heute für eine übergeordnete Gruppe reserviert ist (siehe Abschnitt zur Systematik).

Die Größe der Insekten variiert sehr stark und liegt bei den meisten Arten zwischen einem und 20 Millimetern. Die kleinsten Arten sind in Eiern anderer Insekten parasitierende Hautflügler (Zwergwespen), das Männchen der kleinsten bekannten Art ist 0,15 (bis 0,24) mm lang. Die kleinsten frei lebenden Insekten sind Zwergkäfer mit einer Körperlänge von 0,4 mm (noch kleinere Angaben beruhen auf Messfehlern). Die größten bekannten Insekten sind Stabheuschrecken mit etwa 33 Zentimetern Körperlänge sowie der Riesenbockkäfer "Titanus giganteus" mit einer Körperlänge von 15 Zentimetern. Entsprechend ihrer Lebensweise kann der Körper der Insekten langgestreckt, abgeflacht oder mehr oder weniger kugelig sein.

Allen Insekten gemeinsam ist die meist deutlich sichtbare Gliederung des Leibes in die drei Abschnitte (Tagmata) Kopf (Caput), Brust (Thorax) und Hinterleib (Abdomen) sowie der feste Chitinpanzer und das Vorhandensein von genau drei Beinpaaren. Alle Körperteile bestehen aus einzelnen Segmenten, die eine Rückenplatte (Tergit), eine Bauchplatte (Sternit) sowie Seitenplatten (Pleurite) aufweisen. Seitlich der einzelnen Segmente sind im Grundbauplan Öffnungen des Tracheensystems, so genannte Stigmen, angelegt, deren tatsächliche Anzahl jedoch bei den einzelnen Taxa der Insekten stark schwanken kann. Auch Extremitäten und deren Teile (Mundwerkzeuge, Styli, Gonopoden) können an jedem einzelnen Segment jeweils in Paaren vorkommen. Der Thorax trägt sechs Beine (drei Paare). Jedes Segment besitzt außerdem ein Ganglienpaar im Bauchmark, wobei die Ganglien des Kopfes zu einem Oberschlund- und einem Unterschlundganglion verschmolzen sind. Zwischen den einzelnen Segmenten befinden sich dehnbare Häutchen, die Intersegmentalhäute, die eine Beweglichkeit der Segmente gegeneinander sowie eine Volumenveränderung des Körpers bei der Eiproduktion, Nahrungsaufnahme oder Atmung ermöglichen. Bei starr verschmolzenen Segmenten, etwa im Kopf, sind diese Häutchen nicht vorhanden.

Der Chitinpanzer, der neben Chitin auch mehrere Proteine enthält, bildet die äußere Schicht des Insektenkörpers. Er wird von einer darunter liegenden Epidermis gebildet. In ihn eingebettet befinden sich die Sinnesorgane sowie verschiedene Drüsen des Insekts. Mit mehr oder weniger starker Beteiligung der Epidermis bildet der Chitinpanzer verschiedene Oberflächenstrukturen aus, zu denen Warzen, Dornen, Haare, Borsten, Schuppen und Höcker gehören. Durch die Einlagerung von Farbstoffen (Pigmentfarben) oder aufgrund spezieller lichtbrechender Oberflächen (Interferenzfarben) können die Außenschicht des Insekts oder einzelne Körperteile gefärbt sein.

Als Sinnesorgane dienen vor allem Haarsensillen, die über den Körper verteilt sind. Diese reagieren auf Erschütterungen und Schwingungen, können jedoch auch Gerüche, Feuchtigkeit oder Temperaturen wahrnehmen. Einige dieser Sinneszellen sind zu Sinnesorganen gruppiert, so etwa das Johnstonsche Organ am Pedicellus der Antenne oder die Tympanalorgane zur Geräuschwahrnehmung, die man beispielsweise bei den Langfühlerschrecken findet. Dadurch ist die Wahrnehmung von Schwingungen im Bereich von 1 Hz bis 100.000 Hz möglich. Als optische Sinnesorgane dienen die Facettenaugen sowie die Ocellen, bei vielen Larven auch die larvalen Punktaugen.

Der Insektenkopf besteht aus einem vorderen Abschnitt, dessen Segmentnatur umstritten ist (nach der früher vorherrschenden Articulata-Hypothese entspricht er dem Acron der Anneliden) und fünf weiteren miteinander verschmolzenen Segmenten und trägt die Augen und die Gliederantenne sowie einen typischen Apparat von Mundwerkzeugen aus paarigen Mandibeln und Maxillen sowie einem unpaaren Labium. Entsprechend befindet sich am Kopf auch die Mundöffnung, durch die Nahrung aufgenommen wird. Die von außen sichtbaren Nähte des Insektenkopfes haben mit den Segmentgrenzen bis auf eine Ausnahme, der Grenze zwischen dem fünften und sechsten Segment (Postoccipitalnaht), nichts zu tun.

Die Facettenaugen sitzen auf dem vorderen Abschnitt, daneben besitzen Insekten im Grundbauplan drei Punktaugen (Ocellen) zur Hell-Dunkelwahrnehmung. Das zweite Segment ist das Antennalsegment mit der für die Insekten typischen Geißelantenne. Diese unterscheidet sich von der Gliederantenne, die man bei den ursprünglichen Hexapoden findet, dadurch, dass sie nur ein Basal- oder auch Schaftglied mit Muskulatur besitzt, den Scapus. Das darauf folgende Wendeglied, der Pedicellus, ist gemeinsam mit der Geißel gegenüber dem Scapus beweglich. Im Pedicellus befindet sich das Johnstonsche Organ, ein Sinnesorgan, welches auf Erschütterungen und Schall reagieren kann. Die Geißel selbst ist unterschiedlich lang ausgebildet und kann auch sehr unterschiedlich geformt sein. Sie besitzt Sinneszellen, die vor allem der Geruchswahrnehmung dienen.

Als drittes Segment folgt das Intercalarsegment, welches wiederum keine eigenen Strukturen trägt (es entspricht dem zweiten Antennensegment der Krebstiere). Am vierten bis sechsten Segment befinden sich die Mundwerkzeuge, beginnend mit den Mandibeln und den Maxillen, die jeweils paarig vorliegen, und abschließend mit dem Labium, das durch eine Verwachsung des Basalglieds unpaar ist und den Mundraum nach hinten abschließt. Den vorderen Abschluss bildet der Clypeus mit dem Labrum. Die Mandibel besteht nur aus einem Glied. Sie stellt als Beiß- und Kauwerkzeug bei den meisten Insekten das kräftigste Mundwerkzeug dar. Die Maxille lässt den Extremitätencharakter der Mundwerkzeuge sehr deutlich erkennen und erinnert an das Spaltbein der Krebstiere. Sie besteht aus einem Basalglied, dem Coxopoditen, der durch einen Cardo und einen Stipes geteilt ist. Darauf folgen zwei Kauladen, die als Galea und Lacinia bezeichnet werden, sowie ein mehrgliedriger Taster, der Palpus maxillaris. Das Labium entspricht diesem Aufbau, mit der Ausnahme, dass hier die beiden Coxopoditen verschmolzen sind und eine gemeinsame Basalplatte bilden, die in Mentum und Submentum unterteilt ist. Auch hier gibt es auf jeder Seite zwei Kauladen, die Glossae und die Paraglossae, sowie einen Palpus, den Palpus labialis. Vor allem die Palpen besitzen Sinneszellen zur Geruchs- und Geschmackswahrnehmung. Als weitere Strukturen des Mundraumes gibt es den Hypopharynx als zungenförmige Bildung des Mundvorraums und den Epipharynx an der Innenseite des Labrums.

Der hier beschriebene Bauplan entspricht dem Grundmuster der Insekten und wird als kauend-beißender Typ bezeichnet. Durch Abwandlungen der einzelnen Strukturen in Anpassung an unterschiedliche Ernährungsweisen können sich aus diesem Typus verschiedene andere Mundwerkzeugstypen entwickeln, darunter die leckend-saugenden Mundwerkzeuge vieler Fliegen oder die stechend-saugenden Mundwerkzeuge der Mücken oder der Wanzen.

Der Thorax besteht immer aus drei Segmenten und trägt bei allen Insekten die sechs Laufbeine sowie bei den geflügelten Insekten zudem die beiden Flügelpaare. Die Einzelsegmente werden aufgrund ihrer Lage zueinander als Prothorax, Mesothorax und Metathorax bezeichnet. Jedes dieser Segmente trägt ein Paar der Thorakalbeine, die gemeinhin als Laufbeine ausgeprägt sind. Diese Beine können, abhängig von ihrer Funktion, sehr unterschiedlich gestaltet sein, besitzen jedoch immer den gleichen Grundaufbau. Sie bestehen aus einer Hüfte (Coxa), einem darauf folgenden Schenkelring (Trochanter), dem Oberschenkel (Femur), der Schiene (Tibia) sowie einem gegliederten Fuß (Tarsus), der aus einem bis fünf Tarsalgliedern sowie einem Praetarsus besteht. Der Praetarsus trägt im Normalfall zwei Krallen sowie bei vielen Insekten mehrere weitere Strukturen, die vor allem dem Festhalten an verschiedenen Oberflächen dienen.

Bei den geflügelten Insekten tragen der Meso- und der Metathorax außerdem jeweils ein Paar Flügel, die bei den verschiedenen Insektengruppen unterschiedlich ausgeprägt sein können. Da Meso- und Metathorax hier eine Einheit bilden, werden sie gemeinsam auch als Pterothorax (von griech. "pteron" = „Flügel, Feder, Flosse“) bezeichnet.

Die ursprünglichen Insektengruppen wie die Felsenspringer und die Fischchen sind allerdings ungeflügelt, entsprechend sind Flügel nicht im Grundbauplan der Insekten vorhanden und erst später innerhalb der Insekten, mit der Entwicklung der Pterygota, entstanden. Stattdessen besitzen die Insekten im Grundbauplan seitlich an den Thoraxsegmenten Duplikaturen der Epidermis, die als Paranota bezeichnet und als Vorläuferstrukturen der Flügel angesehen werden.

Das Abdomen besteht ursprünglich aus elf Segmenten, von denen jedoch bei den verschiedenen Insektentaxa einzelne verschmolzen oder abhandengekommen sind. Die Abdominalsegmente tragen nur abgewandelte Extremitäten und keine echten Beine. So finden sich etwa Styli, vor allem bei Larvenformen. Außerdem besitzt eine Reihe von Insektengruppen Cerci am letzten Segment, die für unterschiedliche Funktionen ausgebildet sind. Auch die Gonopoden, die die Begattungsorgane der Männchen und den Legeapparat (Ovipositor) der Weibchen bilden, sind umgewandelte Extremitäten. Das Abdomen enthält im Gegensatz zum Thorax nur sehr wenig Muskulatur, stattdessen sind hier die meisten Organsysteme lokalisiert.

Das Nervensystem der Insekten entspricht im Wesentlichen dem gemeinsamen Grundbauplan der Mandibulata, zu denen neben ihnen auch die Krebstiere und Tausendfüßer gehören. Es besteht aus einem paarigen Bauchmarkstrang, der in jedem Segment ein Ganglienpaar aufweist. Im Kopf sind die Ganglien zu einem Oberschlundganglion, dem Gehirn, sowie zu einem Unterschlundganglion verschmolzen, beide sind miteinander über die Schlundkonnektive verbunden. Das Gehirn selbst weist einen sehr großen vorderen Bereich auf, das Protocerebrum, von dem beidseitig Bereiche in die Zentren der Komplexaugen ausstrahlen, die als Lobi optici bezeichnet werden. Auch die Ocellen werden von diesem Hirnteil innerviert. Vom Deuterocerebrum, dem zweiten Hirnabschnitt, ziehen Nerven in die Antennen und das Tritocerebrum versorgt den Vorderdarm durch eine Tritocerebralkommissur mit Nerven. Die Mundwerkzeuge werden von den im Unterschlundganglion konzentrierten Ganglien des vierten bis sechsten Segments versorgt. Durch den Thorax zieht sich das Bauchmark in Form eines typischen Strickleiternervensystems, wobei drei besonders große Ganglienpaare im Thorax die Beine und (wenn vorhanden) die Flügel versorgen. Das Abdomen enthält im Grundbauplan 7 normale Ganglienpaare und eine Ganglienmasse im achten Abdominalsegment, die aus den Ganglien aller folgenden Segmente besteht.

Das vegetative Nervensystem besteht aus drei Abschnitten. Dabei wird der vordere Abschnitt durch die Nerven des Mund- und Vorderdarmbereiches mit dem Frontal-, dem Hypocerebral- und dem Ventrikularganglion sowie den Corpora cardiaca und den Corpora allata gebildet. Den zweiten Abschnitt bildet die paarige Bauchganglienkette, der die Innervierung der Stigmen zukommt. Der hintere Darmabschnitt und die Genitalien werden vom caudalen Nerv versorgt. Besonders im Gehirn besitzen Insekten zudem neurosekretorische Drüsen, zu denen auch die bereits benannten Corpora cardiaca und Corpora allata gehören. Letztere schütten das Juvenilhormon aus, welches bei der Häutung das Entwicklungsstadium beeinflusst. Die Häutung selbst wird durch die Häutungshormone induziert, vor allem das Ecdyson.

Als Atmungsorgane dienen bei den Insekten weitestgehend starre Röhren, die den gesamten Körper durchziehen und in immer feineren Röhren bis an die Organe und Einzelzellen reichen. Dieses System wird als Tracheensystem bezeichnet und entwickelt sich aus Einstülpungen der Epidermis nach innen. Entsprechend sind die Tracheen wie die Außenseite der Insekten mit einer Epidermis und einer dünnen Chitincuticula überzogen. Die Cuticula wird durch Taenidien versteift, spiralförmige Strukturen, die sich um die Röhre winden, um ein Kollabieren zu verhindern. Nach außen öffnen sich die Tracheen in Atmungsöffnungen, die als Stigmen bezeichnet werden.

Der Atemvorgang ist primär passiv. Durch diese Form der Atmung ist auch bedingt, dass Insekten in wärmeren Gebieten größer werden können. In eher kalten Gebieten reicht die Molekularbewegung der Luft nicht aus, um die inneren Tracheen ausreichend mit Sauerstoff zu versorgen. Der Luftaustausch wird jedoch durch eine muskulöse Kontrolle der Stigmen reguliert und kann durch Ventilation erhöht werden. Bei vielen Insekten wurde die Anzahl der Stigmen von ursprünglich einem Paar an jedem Thorax- und Abdomensegment durch Querverbindungen (Anastomosen) reduziert, außerdem haben sich bei vielen Taxa Luftsäcke zur Unterstützung der Atmung und zur Sauerstoffspeicherung entwickelt. Bei Wasserinsekten haben sich zudem weitere Anpassungen entwickelt, die eine Atmung unter Wasser ermöglichen. So findet man bei vielen Wasserkäfern und anderen Wasserinsekten einen Bereich, der es durch eine besondere Oberfläche ermöglicht, eine Luftblase zu halten (Physikalische Kieme). Andere Arten besitzen ein volumenkonstantes Plastron oder Atemrohre zur Atmung. Vor allem bei wasserlebenden Insektenlarven wird auf die Tracheenatmung vollständig verzichtet und der Sauerstoff wird stattdessen über Tracheenkiemen oder die Haut aufgenommen.

Das Blutgefäßsystem von Insekten besteht im Wesentlichen aus einem Hohlraum, dem "Haemocoel", in dem alle inneren Organe in Blut (Hämolymphe) schwimmen (offener Blutkreislauf).

Als verbliebenen Teil des Blutgefäßsystems besitzen Insekten ein unverzweigtes, einfaches Rückengefäß. Dieses besitzt einen kontraktilen Teil im Abdomen, der die Hämolymphe in den Körper pumpt und entsprechend Herz (nach caudal geschlossen) genannt wird, und einen Ast, der bis in den Kopf, das Gehirn und den Kopf mit Blut versorgt und Aorta (nach cranial offen) genannt wird. Der Blutstrom wird über die Peristaltik des Herzens in Bewegung gehalten, dabei wird die Hämolymphe von einem bis zwölf Paar seitlicher Öffnungen des Herzens, den Ostien, in das Herz gesaugt und nach vorn durch die Aorta in den Kopf gepumpt. Von dort verteilt es sich über den Thorax in den Hinterleib und außerdem in die Beine und Flügel des Tieres. An den Beinen, Antennen und Flügeln sorgen kleinere Pumpsysteme für eine Versorgung bis in die Enden der Strukturen. So befindet sich im Kopf ein muskulöses sogenanntes Antennenherz, es pumpt die Hämolymphe in die Antennenampullen und in die dazugehörigen Antennen. Unterhalb des Herzens liegt ein dorsales Häutchen aus Muskulatur und Bindegewebe, welches als Diaphragma den Rückenteil des Körpers (Pericardialsinus) vom Hauptraum des Körpers (Perivisceralsinus) mit dem Darm und den Geschlechtsorganen trennt. Ein weiteres Diaphragma liegt oberhalb des Bauchteils (Perineuralsinus) mit dem Bauchmark.

Die Hämolymphe selbst macht 20 bis 40 Prozent des Körpergewichts aus und besteht aus Zellen, den Hämocyten, sowie dem flüssigen Plasma. Es transportiert das bei der Atmung entstehende Kohlendioxid, Proteine, Nährstoffe, Hormone und Exkrete und dient zudem der Osmoregulation und dem Aufrechterhalten des Körperinnendrucks. Das Blut ist meist farblos, aber es kann auch grünlich-gelb sein; es transportiert nicht nur Nährstoffe zu allen Körperteilen, sondern bringt auch die Hormone zu ihrem Bestimmungsort. Das Blut enthält spezielle Zellen, die Krankheitserreger bekämpfen, und andere, die Schäden oder Wunden reparieren. Anders als das Blut von Wirbeltieren enthält das Insektenblut keine Zellen, die mit der Atmung zu tun haben.

Die traditionelle Ansicht, das Blut der Insekten hätte aufgrund des effektiven Tracheensystems seine Funktion der Sauerstoffverteilung verloren, wurde in den letzten Jahren aufgrund neuer Befunde relativiert. Vorkommen des roten Blutfarbstoffs Hämoglobin sind bereits seit langem in aquatischen Insektenlarven, die unter Sauerstoffmangelbedingungen leben, wie z. B. Zuckmückenlarven, bekannt. Genomanalysen haben nun gezeigt, dass die Hämoglobinproduktion zum Grundvermögen beinahe aller Insekten gehört. Hämoglobin kommt vorwiegend in Kiemen und anderen respiratorischen Organen vor, so dass seine Funktion für die Atmung gesichert ist. Bei zahlreichen Insektenordnungen wurde außerdem der blaue Blutfarbstoff Hämocyanin nachgewiesen, zumindest für eine Steinfliege wurde auch hier eine Funktion für die Sauerstoffversorgung nachgewiesen.

Die Verdauung erfolgt im Darmsystem der Tiere. Dieses ist in drei funktionelle Abschnitte gegliedert, die entsprechend ihrer Lage als Vorder-, Mittel- und Enddarm bezeichnet werden. Der vordere und der hintere Anteil sind ektodermal gebildet und besitzen eine Auskleidung mit einer Chitincuticula, die entsprechend auch gehäutet werden muss. Der Vorderdarm beginnt mit der Mundöffnung und besteht vor allem aus dem muskulösen Pharynx. Über die Speiseröhre ist dieser Abschnitt mit dem Mitteldarm verbunden, wobei diese bei vielen Insekten auch Bereiche zur Nahrungsspeicherung (Ingluvies) oder einen Vormagen (Proventriculus) mit Kaustrukturen wie Leisten aus Chitin enthalten kann. Der Mitteldarm ist mit einem Drüsenepithel ausgekleidet und produziert die Enzyme, die zur Verdauung notwendig sind. Hier erfolgt auch die Resorption der Nährstoffe, die bei vielen Insekten in Blindschläuchen (Caeca) oder Krypten stattfindet. In den Caeca und Krypten können bei vielen Insekten auch endosymbiotische Mikroorganismen (Bakterien, Pilze oder Flagellaten) leben, die bei der Aufspaltung von bestimmten Nahrungsbestandteilen benötigt werden, z. B. für den Abbau von Cellulose. Daneben wurde bei wenigen Insekten eine endogene Cellulase nachgewiesen (bei den Termitenarten "Reticulitermes speratus" und "Coptotermes formosanus"). Herkunft des tierischen Cellulasegens wird beim letzten gemeinsamen Vorfahren der Bilateria gesehen.

Die unverdaulichen Reste (Exkremente) werden über den Enddarm ausgeschieden. Die Exkretion der Insekten erfolgt über kleine Blindschläuche, die am Übergang des Mitteldarms zum Enddarm in den Darm münden. Diese werden als Malpighische Gefäße bezeichnet und sind wie der Enddarm ektodermalen Ursprungs. In den Zellen dieser Schläuche werden aktiv stickstoffhaltige Exkrete der Hämolymphe entzogen und mit den Exkrementen ausgeschieden. In den Rektalpapillen wird den Ausscheidungsprodukten vor der Ausscheidung noch Wasser entzogen.

Wichtige Strukturen der Nährstoff- und Exkretspeicherung sind die Fettkörper, die als große Lappen im Abdomen der Insekten liegen. Neben der Speicherung dienen sie der Synthese von Fetten und Glykogen sowie dem Abbau von Aminosäuren.

Bis auf wenige Arten sind alle Insekten getrenntgeschlechtlich. Sehr wenige Arten sind Zwitter, eine Reihe von Arten pflanzt sich durch Parthenogenese fort.

Die Männchen besitzen paarige Hoden zur Spermienproduktion im Hinterleib, die über Samenleiter (Vasa deferentia) mit paarigen Samenbläschen (Vasa seminales) verbunden sind. Diese münden in einen unpaaren oder paarigen Ejakulationsgang, den Ductus ejaculatorius, und danach über mehr oder weniger komplex aufgebaute Begattungsorgane, den Aedeagus, meist im neunten Abdominalsegment nach außen. Zusätzlich können noch Zusatzdrüsen vorhanden sein, die Samenflüssigkeiten oder Stoffe zur Bildung von Spermatophoren bilden und den Spermien zugeben.

Auch die Ovarien der Weibchen sind im Regelfall paarig angelegt. Sie bestehen meistens aus einem Büschel einzelner Ovarienstränge, die als Ovariolen bezeichnet werden. Jede dieser Ovariolen besteht aus einem Germarium, in dem die Eizellen produziert werden, und einem Vittelarium zur Produktion der Dotterzellen. Abhängig von der Art, wie die Eier mit Dotter versorgt werden, unterscheidet man dabei drei verschiedene Formen von Ovariolen, die bei unterschiedlichen Insektentaxa vorkommen können. Bei der ersten Form, die als panoistische Ovariole bezeichnet wird, werden einzelne Eier im Vitellarium mit Dotter versorgt. Beim meroistisch-polytrophen Typ besitzt jede einzelne Eizelle mehrere Nährzellen und erhält über diese den Dotter. Bei der meroistisch-telotrophen Ovariole bleibt die einzelne Eizelle über einen Nährstrang mit dem Germarium verbunden und erhält den Dotter über diesen Weg. Um die heranwachsenden Eizellen legen sich bei allen Typen Follikelzellen.

Die Ovariolen vereinigen sich und enden in einer unpaaren Vagina, die zwischen dem siebten und neunten Abdominalsegment entweder direkt nach außen oder in eine Begattungstasche, der Bursa copulatrix, endet (Ausnahme: Eintagsfliegen). Im Bereich der Vagina ist bei fast allen Insekten eine Spermiensammeltasche, das Receptaculum seminis vorhanden, außerdem können verschiedene Anhangsdrüsen zur Produktion von Kittsubstanzen oder ähnlichem vorhanden sein.

Insekten sind mit Ausnahme der Ozeane in fast allen Lebensräumen und Gebieten der Erde zu finden. Dabei existiert die größte Artenvielfalt in den tropischen Gebieten, während in Extremlebensräumen wie den Polargebieten, den Hochgebirgen und den küstennahen Meeresgebieten nur sehr wenige, aber hochangepasste Insektenarten leben. So findet man etwa in der Antarktis die Zuckmückenart "Belgica antarctica", auf der Meeresoberfläche die zu den Wanzen zählenden Meerwasserläufer und im Gezeitenwatt die Larven von Zuckmücken der Gattung "Clunio".

Einige Arten sind sehr stark spezialisiert und kommen entsprechend nur in besonders geeigneten Lebensräumen vor (stenöke Arten), andere dagegen können in fast allen Lebensräumen mit Ausnahme der Extremlebensräume leben (euryöke Arten) und wurden teilweise durch den Menschen weltweit verbreitet, so dass sie heute Kosmopoliten sind. Zu letzteren gehören vor allem verschiedene Arten der Schaben, Ameisen und Termiten sowie die als Nutztiere gehaltenen Honigbienen.

Die meisten Insekten leben in Böden oder auf bodennahen Strukturen sowie auf und in Pflanzen. Man geht davon aus, dass mit jeder Baumart der tropischen Regenwälder etwa 600 Insektenarten assoziiert sind, wobei man bei 50.000 Baumarten auf eine Artenzahl der Insekten von etwa 30 Millionen kommt. Auch auf Tierarten leben eine Reihe von Insekten, meistens als Ektoparasiten wie die verschiedenen Arten der Tierläuse und Flöhe oder als Kommensalen und Jäger im Fell der Tiere. Der Mensch stellt hierbei keine Ausnahme dar, auf ihm findet man etwa die verschiedenen Arten der Menschenläuse. Seltener sind Insekten Endoparasiten in Tieren. Zu nennen sind hier vor allem die zu den Zweiflüglern gehörenden Dasselfliegen, bei denen sich die Larven im Rachen (Rachendasseln), der Nasenhöhle (Nasendasseln) oder sogar im Magen (Magendasseln) von Pflanzenfressern entwickeln.

Aufgrund ihrer Vielfalt haben Insekten heute beinah jede ihrer Größe angemessene ökologische Nische realisiert. Dabei spielt eine große Anzahl der Arten eine bedeutende Rolle bei der Remineralisierung organischer Stoffe im Boden, in der Bodenstreu, im Totholz und in anderen organischen Strukturen. Zu dieser Gruppe gehören auch die Leichenzersetzer, die in Tierleichen zu finden sind. Viele weitere Arten leben als Pflanzenfresser von lebenden Pflanzenteilen, das Spektrum reicht dabei von Wurzelhaaren über Holz bis hin zu Blättern und Blüten. Eine Reihe von Arten lebt als Nektar- und Pollensammler und spielt dabei eine wichtige Rolle bei der Pflanzenbestäubung. Wieder andere Insekten leben in und an Pilzen und ernähren sich von diesen. Eine große Gruppe von Insekten ernährt sich räuberisch von anderen Insekten oder kleineren Beutetieren. Eine letzte Gruppe stellen diejenigen Insekten dar, die sich von Teilen größerer Tiere wie Haaren, Schuppen und ähnlichem ernähren. In diese Gruppe gehören auch die zahlreichen Parasiten unter den Insekten, die beispielsweise Blut saugen oder sich in lebenden Geweben entwickeln.

Eine Besonderheit innerhalb der Insekten stellen verschiedene Arten von staatenbildenden Insekten dar. Diese Form des Zusammenlebens hat sich mehrfach unabhängig voneinander bei den Termiten und verschiedenen Hautflüglern (Ameisen, Bienen, Wespen) entwickelt. Bei diesen Tieren kommt es zum Aufbau eines Insektenstaates, in dem die Einzeltiere bestimmte Rollen innerhalb der Gesellschaft übernehmen. Häufig kommt es dabei zur Bildung von Kasten, deren Mitglieder sich morphologisch und in ihrem Verhalten gleichen. Bei vielen Ameisen findet man beispielsweise Arbeiter, Soldaten und Nestpfleger. Die Fortpflanzung übernehmen in diesen Fällen nur sehr wenige Geschlechtstiere innerhalb des Insektenstaates, manchmal nur eine einzige Königin, die befruchtete und unbefruchtete Eier legt.

Die Spermienübertragung erfolgt bei den Insekten ursprünglich über Spermatophoren, also Spermienpakete. Diese werden bei den Felsenspringern und den Fischchen von den Männchen auf dem Boden abgelegt und hier von den Weibchen aufgenommen. Bei allen folgenden Gruppen der Insekten gibt es eine direkte Spermienübertragung durch eine Kopulation, bei der die Spermien direkt in die Vagina oder die Bursa copulatrix eingebracht werden und hier entweder die Eizellen befruchten oder in das Receptaculum seminis zur Lagerung weitergeleitet werden. Die meisten Insekten legen nach der Begattung Eier ab (Ovoparie), andere sind ovovivipar, brüten die Eier also noch im Körper zur Schlupfreife aus. Noch seltener kommt es vor, dass bereits fertig entwickelte Larven (Larviparie) oder sogar Puppen (Pupiparie) zur Welt gebracht werden.

Die Furchung erfolgt bei den meisten Insekten superfiziell. Das bedeutet, dass sich auf dem sehr dotterreichen Ei mit zentralem Dotter (centrolecithales Ei) ein Furchungszentrum ausbildet, von dem die Furchung ausgeht. In seinem Bereich bilden sich mehrere Tochterkerne mit umgebendem Plasma (Furchungsenergiden), die durch Teilungen zu einem einschichtigen Blastoderm als Hüllepithel oder Serosa um den Dotter herum. Im ventralen Bereich bildet sich dann eine Keimanlage, die als Keimstreif in den Dotter hineinwächst und eine Höhle bildet (Amnionhöhle). In dieser Höhle findet die Hauptkeimbildung statt, nach deren Abschluss sich der Keim wieder nach außen entrollt und über dem Dotter der Rücken des Tieres geschlossen werden kann. Aus den Eiern schlüpfen dann Larven (Juvenilstadien mit eigenen Larvalmerkmalen) oder Nymphen (Juvenilstadien ohne eigene Merkmale).

Außerhalb des Eies folgt die postembryonale Entwicklung, nach der Insekten klassisch in hemimetabole und holometabole Insekten unterteilt werden. Dabei handelt es sich allerdings nur bei letzteren auch um eine taxonomische Gruppe, da die hemimetabole Entwicklung dem ursprünglichen Zustand entspricht. Die Entwicklung ist bei den verschiedenen Gruppen sehr unterschiedlich und hängt sehr stark von der Lebensweise der Juvenilstadien und der Imagines ab. Bei allen hemimetabolen Insekten kommt es über eine unterschiedliche Anzahl von Larven- oder Nymphenstadien ohne Puppenstadium zur Ausbildung des ausgewachsenen Tieres, der Imago. Zwischen den einzelnen Stadien findet dabei immer eine Häutung statt, bei der die alte Kutikula abgeworfen und eine neue angelegt wird. Abhängig von der Konzentration des Juvenilhormons im Blut kommt es dabei entweder zu einer Häutung von einer Larvenform in die nächste (bei viel Juvenilhormon) oder von einer Larvenform zur Imago (bei wenig Juvenilhormon). Den Zeitpunkt der Häutung bestimmt ein weiteres Hormon, das Ecdyson. Bei der Häutung kommt es vor allem zum Wachstum der Tiere, außerdem werden einzelne Merkmale neu angelegt. Dies geschieht durch eine Histolyse einzelner Strukturen und die Ausbildung von Imaginalanlagen oder das Ausstülpen spezieller Imaginalscheiben.

Bei den meisten hemimetabolen Insekten wie den verschiedenen Heuschreckenformen oder den Wanzen ähnelt die Nymphe in Grundzügen dem erwachsenen Tier und weist außer den fehlenden Flügeln keine besonderen Larvalanpassungen auf. Demgegenüber gibt es allerdings auch hemimetabole Insekten mit echten Larven, beispielsweise die Libellen oder die Eintagsfliegen. Eine feinere Unterteilung der Hemimetabolie ist möglich. So spricht man etwa von einer Palaeometabolie (Fischchen, Felsenspringer), wenn die Larven kaum eigene Merkmale aufweisen und sich nur durch Größenänderung zur Imago entwickeln. Die Heterometabolie ist vor allem durch eine schrittweise Entwicklung der Flügel gekennzeichnet und kommt bei den Libellen, Steinfliegen und den meisten Schnabelkerfen vor. Schließlich gibt es noch die Neometabolie, bei der die Flügelanlagen erst bei den beiden letzten Larvenstadien angelegt werden; dies ist etwa bei einigen Zikaden und den Fransenflüglern der Fall.

Holometabole Insekten durchlaufen eine Metamorphose, ausgehend vom Ei über die Larve zur Puppe und dann zum erwachsenen Tier (Imago). Die Larve hat oft nicht die geringste physische Ähnlichkeit mit der Imago und besitzt eine Reihe von Eigenmerkmalen einschließlich anderer Lebensräume und Futterquellen im Vergleich zur Imago – ein nicht unerheblicher ökologischer Vorteil der Holometabola.

Die Populationsdynamik vieler Insektenarten hat näherungsweise diskreten Charakter: In der zeitlichen Entwicklung einer Population können gegeneinander abgegrenzte, nicht-überlappende Generationen ausgemacht werden.

Schreibt man formula_1 für die Größe, oder nach Normierung Abundanz, einer Population zum Zeitpunkt t und r für die Anzahl der pro Individuum generierten Nachkommen, so ergibt sich für die einfachst mögliche Beschreibung einer Populationsdynamik:

formula_2

Hängt der Anteil der selbst zur Reproduktion kommenden Nachkommen zum Zeitpunkt t+1 von der Größe der Population zum Zeitpunkt t ab, erhält man einen funktionalen Zusammenhang der Form:

formula_3 mit einer Funktion f. Für f sind dabei verschiedene konkrete Ansätze möglich.Verwendung findet zum Beispiel die Hassel-Gleichung.

Auf einen ähnlichen Formalismus führt auch eine unmittelbare Abhängigkeit der Fertilität von der Populationsgröße.

Das früheste einem Insekt sicher zuzuordnende Fossil ist "Rhyniognatha hirsti" aus dem Pragium, einer Stufe des Unterdevons vor etwa 407 Millionen Jahren. Aus derselben Formation stammt auch der älteste Springschwanz "Rhyniella precursor", der als das älteste Insekt galt, solange diese zu den Insekten gerechnet wurden. Diese relativ fortgeschrittenen Fossilien lassen auf eine bereits länger dauernde Evolutionsgeschichte schließen, die vermutlich bis ins Silur zurückreicht. Insekten sind damit wohl nur unwesentlich jünger als die ältesten Funde von terrestrischen Tieren überhaupt. Da von Rhyniognatha bis auf die Kiefer nichts erhalten blieb, sind Aussagen darüber, wie dieses Insekt aussah und in welche heutige Verwandtschaft es einzuordnen wäre, spekulativ. Die Erstbeschreiber halten sogar einen Zusammenhang mit geflügelten Formen für denkbar, was allerdings sehr spekulativ bleibt. Tatsächliche fossile Funde geflügelter Insekten liegen etwa von Gesteinen der Grenze Unterkarbon-Oberkarbon vor, das heißt, sie sind etwa 95 Millionen Jahre jünger. (Ältere devonische Funde, als "Eopterum devonicum" beschrieben, erwiesen sich später als Reste des Carapax einer Krebsart.) Geflügelte Insekten liegen hier aus einer Vielzahl von Ordnungen, darunter sowohl lebende als auch ausgestorbene, vor. Die Flügelmorphologie erlaubte bereits unterschiedliche Flugstile. Zur karbonischen Fauna gehörten z. B. die Palaeodictyoptera, die manchmal auch am ersten Thoraxsegment kleine flügelartige Fortsätze aufwiesen. Während des Karbons machten diese altertümlichen Ordnungen etwa ein Drittel der Insektenfauna aus; Schaben waren individuenreich vertreten, aber nicht sonderlich artenreich. Von den 27 im Karbon nachgewiesenen Insektenordnungen sind 8 am Übergang zum Perm oder wenig später ausgestorben. Drei weitere Ordnungen starben am Übergang zur Trias aus. Seit diesem evolutionären Flaschenhals haben alle überlebenden Linien bis heute mehr oder weniger stetig an Diversität zugenommen, auch das dritte Massenaussterben an der Kreide/Tertiär-Grenze scheint die Insekten nur wenig betroffen zu haben. Wichtiger für diese erhöhte Diversität scheinen überraschenderweise nicht schnellere Artbildungsraten, sondern verminderte Aussterberaten zu sein.

Sowohl Radiationen wie längere Perioden mit niedrigen Artenzahlen und wenig evolutionären Neuerungen können mit ausgeprägten Schwankungen des Sauerstoffspiegels der Erde im Paläozoikum in Verbindung gebracht werden, ausgedehnte Perioden mit sehr geringen fossilen Funden sind daher wohl nicht ausschließlich auf Zufälle der fossilen Überlieferung zurückzuführen, sondern können durchaus real sein. Während der Perioden mit ungewöhnlich hohem Sauerstoffgehalt lebten spektakuläre Rieseninsekten mit Körpergrößen, die alle rezenten Formen bis um das Zehnfache übersteigen. Die Urlibelle "Meganeura monyi" (gefunden in Commentry, Frankreich) erreichte eine Flügelspannweite von 63 cm, "Meganeuropsis permiana" soll sogar 71 cm Spannweite besessen haben, wahrscheinlich nahe an der Obergrenze des Arthropoden-Bauplans für Flugfähigkeit.

Im Perm tauchten die meisten modernen Insektenordnungen auf, doch spielten gerade die heute dominierenden holometabolen Insekten lange Zeit eine untergeordnete Rolle, und erst ab der Trias kam es bei diesen zu einer wahren Explosion der Arten- und Formenvielfalt. Gründe für diesen Erfolg werden im Massenaussterben am Ende des Perms gesehen (bisher dominierende Insekten starben aus), in besserer Widerstandsfähigkeit gegen das aride Klima des Perms und in der Ausbreitung der Samenpflanzen.

Alle gegenwärtig anerkannten Insektenordnungen und knapp zwei Drittel der Familien sind auch fossil belegt, wobei ihre tertiären Artenzahlen gut mit ihren heutigen korrelieren (Ausnahme: Schmetterlinge, die fossil weniger artenreich belegt sind).

Drei Gruppen, die traditionell als Urinsekten zu den Insekten gezählt wurden, die Springschwänze (Collembola), Doppelschwänze (Diplura) und Beintastler (Protura) gelten heute nicht mehr als eigentliche Insekten. Sie werden zusammen mit diesen hier innerhalb der übergeordneten Gruppe der Sechsfüßer (Hexapoda) geführt. Sowohl die Hexapoda als auch die Insekten als solche gelten aufgrund ihrer typischen Merkmale (Apomorphien) als gesicherte Taxa. Die tatsächliche Schwestergruppe der Insekten innerhalb der Hexapoda ist allerdings umstritten.

Die Insekten werden häufig auch als Freikiefler (Ectognatha) den Sackkieflern (Entognatha) gegenübergestellt, die aus eben den benannten Taxa der Urinsekten bestehen. Sie besitzen als wichtigste Eigenmerkmale eine Geißelantenne mit einem Grundglied (Scapus) und einem Pedicellus mit Johnstonschem Organ. Die eigentliche Geißel ging auf das ursprünglich dritte Glied der Antenne zurück, welches sich in eine unterschiedliche Anzahl von Geißelsegmenten aufgeteilt hat. Weitere Apomorphien sind der Besitz rückwärtiger Äste des Tentoriums, einer Skelettstruktur im Kopf, paarige Krallen am Praetarsus mit einer gelenkigen Verbindung zu selbigem. Auch der primär in fünf Tarsenglieder gegliederte Tarsus und die Umbildung der hinteren Abdominalextremitäten zu Gonopoden und die Beteiligung derselben am Legeapparat (Ovipositor) der Weibchen wird als neu erworbenes Merkmal angesehen. Das ursprünglich namensgebende Merkmal, die freiliegenden Kiefer, ist dagegen ein ursprüngliches Merkmal (Plesiomorphie), die bereits bei den gemeinsamen Vorfahren der Krebstiere, Hundertfüßer, Tausendfüßer und Hexapoda vorhanden war (Mandibulata).

Innerhalb der Insekten werden in der klassischen Systematik heute je nach Quelle unterschiedliche Anzahlen von Unterklassen, Überordnungen und Ordnungen unterschieden. Dies hängt mit der in Teilen noch unverstandenen tatsächlichen Verwandtschaft der Insektengruppen untereinander sowie der unterschiedlichen Ansicht der Beschreiber über die Anordnung innerhalb einer klassischen Hierarchiebeschreibung zusammen. In diesem Werk soll folgender Systematik gefolgt werden; Unterschiede zu anderen Werken ergeben sich vor allem im Rang der Libellen und Eintagsfliegen, die häufig als Ordnung eingeordnet werden, sowie bei den Schnabelkerfen, deren Unterordnungen in einigen Werken als eigenständige Ordnungen geführt werden, ebenso bei den Heuschrecken, bei denen häufig noch die Kurzfühlerschrecken und die Langfühlerschrecken als Ordnungen (anstelle wie hier als Unterordnungen) eingestuft werden.


Die phylogenetische Systematik versucht, das System anhand der tatsächlichen Verwandtschaft aufzubauen. Unterschiede zur klassischen Systematik ergeben sich vor allem in der Behandlung der sog. paraphyletischen Gruppen. Paraphyletische Taxa werden nur durch altertümliche Merkmale Symplesiomorphien abgegrenzt. Sie umfassen nicht alle Nachfahren eines tatsächlich gemeinsamen Vorfahren. Wichtigste Methode ist nach wie vor eine sorgfältige Untersuchung der Anatomie und Morphologie, auch unter Einschluss aussagekräftiger Fossilien.

Als zusätzliche Methode sind heute molekulare Stammbäume von sehr großer Bedeutung. Molekulare Stammbäume beruhen auf dem direkten Vergleich homologer DNA-Sequenzen. Dazu werden dieselben Genabschnitte der verschiedenen zum Vergleich stehenden Organismen mit Hilfe der Polymerase-Kettenreaktion vervielfältigt, sequenziert und anschließend verglichen. Mittels verschiedener statistischer Techniken werden die Sequenzen zu einem molekularen Stammbaum angeordnet, der im Idealfall genau die abgestufte Verwandtschaft der untersuchten Individuen widerspiegelt. Häufig verwendete Gene für den molekularen Stammbaum der Insekten sind Gene für Ribosomale RNA, Histon H3, Elongationsfaktor 1 alpha 1 und verschiedene mitochondriale Gene (z. B. mt-CO1), zunehmend aber auch die gesamte Mitochondriale DNA. Ein Vergleich auf Basis des gesamten sequenzierten Genoms wäre naturgemäß am aussagekräftigsten, er wurde aufgrund der wenigen vollständig sequenzierten Insektengenome auch tatsächlich bereits unternommen. Wie jede andere Methode hat auch die Erstellung molekularer Stammbäume ihre Schwierigkeiten und Fallstricke. Wie häufig bei neuen Methoden, ist die anfängliche Euphorie einer gewissen Ernüchterung gewichen, weil zu viele an verschiedenen Genen oder mit verschiedenen Methoden erhobene Stammbäume einander krass widersprechende Ergebnisse erbrachten. Ein Hauptproblem besteht darin, dass viele Insektenordnungen offensichtlich in einer schnell ablaufenden Radiation in kurzer zeitliche Folge entstanden sind und sich anschließend über mehr als dreihundert Millionen Jahre auseinanderentwickelten, wodurch die Informationen der ursprünglichen Radiation kaum noch aufzuschlüsseln sein werden.

Aktuelle Vorschläge für phylogenetische Systematiken beruhen in der Regel auf einer Kombination von morphologischen und molekularen Stammbäumen. Zusätzlich werden sogenannte "supertrees" aufgestellt, die die Ergebnisse zahlreicher Studien zusammenführen, ohne selbst neue Daten beizutragen. Auch die zurzeit intensiv diskutierten Stammbäume (u. a.:) kommen an zahlreichen Punkten nicht zur Übereinstimmung. Dennoch haben sich gegenüber früheren auf rein morphologischer Basis erstellten Systematiken (klassische Grundlage:) zahlreiche Verbesserungen ergeben.

Das phylogenetische System kommt ohne die Rangstufen der klassischen Systematik oberhalb der Art aus. In der Regel werden die Insektenordnungen aber als Verwandtschaftsgruppen beibehalten, auch wenn klar ist, dass sie weder gleiches Alter noch gleichen tatsächlichen Rang besitzen können. Folgende klassische Ordnungen haben sich unzweifelhaft als paraphyletisch erwiesen:

Die jeweils zweitgenannte Gruppe ist also in Wirklichkeit keine Schwestergruppe der erstgenannten, sondern ein Teil davon (d. h., sie ist mit einigen Familien davon näher verwandt, als diese es untereinander sind).
Die Monophylie der übrigen Ordnungen wurde, manchmal nach zwischenzeitlich aufgekommenen Zweifeln, bestätigt (v. a. Schnabelfliegen in Bezug auf Zweiflügler und Großflügler in Bezug auf Kamelhalsfliegen: noch nicht sicher).

Das im Folgenden dargestellte System (nach den o. g. Quellen kombiniert) ist unsicher, an verschiedenen Stellen umstritten und wird sich möglicherweise in den nächsten Jahren noch verändern. Es gibt in etwa den heutigen Stand des Wissens wieder. Die Verwandtschaftsverhältnisse innerhalb der Polyneoptera (deren Monophylie heute fast unstrittig ist) weiter aufzulösen, ist zurzeit mit hinreichender Sicherheit unmöglich. Einige traditionell sehr umstrittene Fragen, wie die Stellung der Fächerflügler oder der Bodenläuse, scheinen aber einer Lösung nähergerückt.

Als Nutztiere hält der Mensch vor allem die Honigbiene, die Schlupfwespen zur biologischen Schädlingsbekämpfung, die Raupen des Seidenspinners sowie in vielen Ländern Heuschrecken und Grillen als Nahrungsmittel (Speiseinsekten). Der Verzehr von Insekten ist über große Teile Afrikas, Südostasiens und Mittel- bzw. Südamerikas verbreitet und wird als Entomophagie bezeichnet. Dabei stellen Insekten eine proteinreiche Kost dar; etwa 500 Arten werden weltweit für die menschliche Ernährung genutzt. Allerdings spielen auch sehr viele Insekten sekundär als Pflanzenbestäuber eine große Rolle in der Nahrungsproduktion des Menschen, da ohne sie keine Fruchtbildung stattfinden könnte.

Einige Insekten werden auch als Haustiere in Terrarien gehalten. Besonders zu nennen sind dabei die Ameisen, die in Formicarien gehalten werden. Sie gehören zu den Hautflüglern und sind somit enge Verwandte der Bienen, Wespen und Hornissen. Weitere beliebte Insekten der Terrarianer sind Stabheuschrecken, Fangschrecken und verschiedene Käfer- und Schabenarten, außerdem werden Mehlkäferlarven (Mehlwürmer), Grillen und andere Insekten als Futtertiere für Reptilien und als Angelköder gezüchtet. Eine Entwicklung der letzten Jahrzehnte ist die Haltung von Schmetterlingen in großen Gewächshäusern, die als Schmetterlingshäuser für Besucher geöffnet werden und an manchen Orten zu den Sehenswürdigkeiten der Region werden.

Auch in der pharmazeutischen Industrie werden verschiedene Insekten eingesetzt, das bekannteste Beispiel stellen hierbei die Spanische Fliege ("Lytta vesicatoria") sowie einige weitere Ölkäfer dar, die den Stoff Cantharidin produzieren. Als wissenschaftliche Versuchstiere haben sich "Drosophila melanogaster" sowie verschiedene Heuschrecken- und Käferarten etabliert. Die Leichenzersetzer unter den Insekten, vor allem die Larven verschiedener Fliegen und Käfer, spielen heute zudem eine bedeutende Rolle in der Kriminalistik. Der Forschungszweig der Entomologischen Forensik zur Aufklärung von Kriminalfällen basiert auf der Erforschung dieser Tiere. Schmeißfliegenlarven werden in der Wundheilung eingesetzt. Außerdem werden verschiedene Arten der Schildläuse zur Produktion von Farbstoffen, Lacken oder Wachsen genutzt wie z. B. Karmin (Cochenille) und Schellack.

Das Verteidigungsministerium der USA suchte im Jahr 2006 nach Möglichkeiten, Insekten schon während der Metamorphose mit Mikrotechnik auszustatten und sie als militärische „Insekten-Cyborgs“ einzusetzen.

Die Rolle von Insekten für die Funktion aller terrestrischen Ökosysteme und speziell den Stoffumsatz ist wegen ihrer Allgegenwart normalerweise kaum abzuschätzen. Deutlich wird sie zum Beispiel an der Entsorgung weggeworfener organischer Abfälle. Forscher von der North Carolina State University in Raleigh (USA) stellten beispielsweise in einer im Herbst 2014 veröffentlichten Untersuchung fest, dass Insekten auf 24 begrünten Straßen-Mittelstreifen im New Yorker Stadtteil Manhattan jährlich, pro Mittelstreifen, etwa 4 bis 6,5 Kilogramm Essensabfälle „entsorgen“ können. Wichtigster „Entsorger“ war dabei eine eingeschleppte neozoische Ameisen-Art.

Eine Reihe von Insekten ist als Schädling (Ungeziefer) an Nutz- und Zierpflanzen, Holzkonstruktionen und -produkten (Holzschutz) und Nahrungsvorräten bekannt, andere leben als Parasiten am Menschen und seinen Haustieren. Ferner sind einige Insektenarten, vor allem in großen Individuenzahlen vorkommende blutsaugende Insekten, Überträger (Vektoren) von Krankheiten wie der Rattenfloh für die Pest, Mücken der Gattung "Anopheles" für die Malaria oder die Tse-Tse-Fliegen für die Schlafkrankheit.

Der Befall mit Pflanzenschädlingen kann bei der heute üblichen Monokultur von Nutzpflanzen zu großen Ausfällen der Ernte führen. Neben der direkten Schädigung durch Fraß übertragen viele Insekten auch Pflanzenkrankheiten, meist Virosen und Pilzerkrankungen. Viele weitere Schädlinge leben als Vorratsschädlinge auch in geernteten oder auch bereits verarbeiteten Materialien und führen hier zu weiteren Schäden. Dem stehen andere Insektenarten gegenüber, die im Sinne der biologischen Schädlingsbekämpfung eingesetzt werden und Schädlinge direkt jagen oder in ihnen ihre Eier ablegen. Vor allem verschiedene Schlupfwespenarten werden entsprechend gezielt gegen spezifische Schädlinge eingesetzt.

Zur Erfassung der Artenvielfalt und der Variationsbreite der Insekten werden Insektensammlungen von Privatsammlern und wissenschaftlichen Einrichtungen angelegt. Vor allem Museensammlungen erreichen dabei beachtliche Größen und Individuenzahlen. Die wissenschaftliche Insektensammlung im Naturhistorischen Museum des Stiftes Admont (Steiermark) von Pater Gabriel Strobl (1846–1925) beinhaltet beispielsweise etwa 252.000 Exemplare aus 57.000 verschiedenen Arten, die Dipteren-Sammlung zählt mit ihren etwa 80.000 aufbewahrten Exemplaren und rund 7500 verschiedenen Artnamen zu den drei bedeutendsten Fliegen-Kollektionen in Europa.

Auch private Sammler legen sich häufig Insektensammlungen an; vor allem auffällige Schmetterlinge und Käfer werden dabei gerne aufgrund ästhetischer Vorlieben oder seltene Arten aufgrund ihres Seltenheitswertes gesammelt. Von einer Sammlung aus rein ästhetischen Gründen ist abzuraten, da vor allem die Populationen seltener Schmetterlinge wie des europäischen Apollofalters durch die Sammelleidenschaft (neben ihrem Lebensraumverlust und anderer Gründe) an den Rand der Ausrottung gebracht werden können. Auf der anderen Seite gibt es vor allem in tropischen Ländern wirtschaftliche Interessen an der vor allem in Europa und Nordamerika beliebten Sammelleidenschaft. Dort werden Schmetterlinge und andere beliebte Insekten in speziellen Farmen gezüchtet und an Sammler verschickt.

In die Kultur des Menschen haben vor allem die ihm nützlichen Insektenarten wie die Honigbiene und andere in seinem direkten Umfeld lebende Arten Einzug gefunden. Die bekannteste Verarbeitung des Insektenmotivs in der Literatur ist dabei "Die Biene Maja und ihre Abenteuer" von Waldemar Bonsels. In der Musik sind das mexikanische Volkslied "La Cucaracha" und "Der Hummelflug" besonders populär. Letzterer fand über die Band Manowar auch Einzug in den Heavy Metal. In Deutschland ist das Kinderlied "Maikäfer flieg!" lange Zeit kulturelles Allgemeingut gewesen.





</doc>
<doc id="9551" url="https://de.wikipedia.org/wiki?curid=9551" title="Bassas da India">
Bassas da India

Bassas da India ist ein unbewohntes Atoll im Indischen Ozean, westlich von Madagaskar in der südlichen Straße von Mosambik gelegen. Politisch gehört es zu den Îles Éparses, einem Distrikt des französischen Überseegebiets "Terres australes et antarctiques françaises".

Bassas da India liegt im südwestlichen Indischen Ozean, ungefähr 430 km östlich der zu Mosambik gehörenden Insel Bazaruto, 380 km westlich des madagassischen Küstenorts Morombe und 110 km nordwestlich der ebenfalls zu den Îles Éparses zählenden Insel Europa. Das über einem erloschenen submarinen Vulkan befindliche Atoll mit einem Durchmesser von 10,5 km besteht aus einem aus Steinkorallen gebildeten Riff, das eine flache, bis maximal 14 m tiefe Lagune umschließt. Das gesamte Atoll nimmt eine Fläche von 86,5 km² ein; die Landfläche der mehr als zehn winzigen Inselchen beträgt jedoch insgesamt nur etwa 0,2 km².

Das Atoll erhebt sich bei Niedrigwasser lediglich etwa 1,20 m über die Meeresoberfläche und wird bei Hochwasser bis auf einige Korallenfelsen im Norden beinahe vollständig überflutet. Es bietet deshalb weder für Landpflanzen noch für Landtiere Lebensraum. Aus demselben Grund gibt es keinerlei menschengemachte Infrastruktur. Die reichhaltige Unterwasserfauna ist noch wenig erforscht; bisher wurden 301 Arten von Knochenfischen und 10 Arten von Haien erfasst, darunter der gefährdete Weißspitzen-Hochseehai und der ebenfalls bedrohte Bogenstirn-Hammerhai. Die umgebenden Gewässer erreichen eine Tiefe von etwa 3000 m.

Bassas da India wurde am Beginn des 16. Jahrhunderts von dem portugiesischen Seefahrer "Gaspero Gonsuales" entdeckt und von ihm "Baixo da India" genannt.

Im Jahr 1585 sank die Karacke "Santiago" mit 400.000 Silbertalern an Bord nahe Bassas da India. Nachdem 1987 eine erste Suchexpedition scheiterte, machte sich 2011 erneut ein Forschungsteam mit dem größten französischen Meeresforschungsschiff "Marion Dufresne" auf, das Wrack zu untersuchen, musste aber feststellen, dass dieses nahezu vollständig geplündert worden war.

Am 31. Oktober 1897 wurde die Inbesitznahme durch Frankreich „in Ausführung des Gesetzes vom 6. August 1896“ offiziell verkündet. Wie die übrigen Îles Éparses wird das Atoll seit 2005 vom Präfekten und obersten Verwalter der französischen Süd- und Antarktisgebiete verwaltet. Mit Gesetz vom 21. Februar 2007 wurde Bassas da India gemeinsam mit den anderen Îles Éparses als fünfter Distrikt in die französischen Süd- und Antarktisgebiete eingegliedert. Davor war es seit 1960 vom Präfekten des Übersee-Départements Réunion verwaltet worden, ohne selbst zu Réunion zu gehören.

Die ausschließliche Wirtschaftszone (AWZ), die unmittelbar an diejenige der Insel Europa angrenzt, erstreckt sich über eine Fläche von 123.700 km².

Seit seiner Unabhängigkeit im Jahr 1960 erhebt Madagaskar Anspruch auf das Atoll.


</doc>
<doc id="9552" url="https://de.wikipedia.org/wiki?curid=9552" title="Senf">
Senf

Senf () ist ein scharfes Gewürz, das aus den Samenkörnern des Weißen, Braunen und des Schwarzen Senfs hergestellt wird. Gebräuchlich sind sowohl reine Senfkörner (ganze Samen), Senfpulver (gemahlene Samen) und vor allem die aus den mehr oder weniger fein gemahlenen Samen und weiteren Zutaten bereitete Würzpaste, genannt „Tafelsenf“ oder „Mostrich(t)“.

Das Wort "Senf" ist ein Lehnwort aus dem lateinischen "sināpi", das dem gleichlautenden altgriechischen "σίναπι" entstammt, dessen weitere Wortherkunft allerdings unbekannt ist.

Senfsamen enthalten etwa 20 bis 36 % nussig-mildes Senföl, 28 % Eiweiß und die Glykoside Sinalbin (Weißer Senf – bildet daraus Bisphenol F) bzw. Sinigrin (Brauner und Schwarzer Senf), die für den scharfen Geschmack und damit auch für die appetitanregende und verdauungsfördernde Wirkung verantwortlich sind. Diese Senfölglycoside sind allerdings selbst nicht scharf. Erst durch das Schroten oder Mahlen der Samen und Kontakt mit Flüssigkeit wird das ebenfalls im Senfsamen enthaltene Enzym Myrosinase aktiv und verwandelt die Glykoside in Traubenzucker, Hydrogensulfat (HSO) und scharfe, tränenreizende Isothiocyanate (R-N=C=S), die häufig ebenfalls wie das „fette Senföl“ auch „ätherisches Senföl“ genannt werden. Deshalb haben Senfkörner erst einen milden, nussartigen Geschmack, der nur nach längerem Kauen etwas an Schärfe gewinnt, und auch trockenes Senfpulver hat zunächst einen faden, leicht bitteren, aber nicht scharfen Geschmack. Bei Tafelsenf dagegen hat sich durch Mahlen und die Zugabe von Wasser und anderen Zutaten bereits Isothiocyanat bilden können, so dass sich das scharfe Aroma von vornherein entfalten kann.

Neben den unterschiedlichen Farben der Samen zeichnen sich weißer, brauner und schwarzer Senf auch durch unterschiedliche Schärfe aus: Weißer Senf mit dem Glykosid Sinalbin ist milder, brauner und schwarzer Senf mit dem Glykosid Sinigrin sind deutlich schärfer. Durch das Mischungsverhältnis aus weißem und braunem bzw. schwarzem Senf kann daher die Schärfe einer Senfpaste nach Wunsch bestimmt werden. Da Schwarzer Senf ("Brassica nigra") maschinell schwerer zu ernten ist, werden heute im Wesentlichen nur noch Weißer Senf ("Sinapis alba") und Brauner Senf ("Brassica juncea") verwendet.

Senf wurde schon vor 3000 Jahren in China als Gewürz geschätzt. Über Kleinasien gelangte er nach Griechenland, wo er im 4. Jahrhundert v. Chr. als Heilmittel bekannt war. Das erste überlieferte Rezept zur Senfzubereitung verfasste der Römer Columella im 1. Jahrhundert n. Chr. Im 8. Jahrhundert wird Senf in einer Schrift Karls des Großen erstmals in Mitteleuropa erwähnt und wurde – von arabischen Pflanzungen in Spanien aus verbreitet – an den europäischen Tafeln bald zu einem wichtigen Gewürz. So ernannte Papst Johannes XXII., der ausschließlich in Avignon und nicht in Rom residierte, einen seiner Neffen zum "Grand moutardier du Pape" – zum „Großen päpstlichen Senfbewahrer“. 

Senf und Meerrettich waren über Jahrhunderte die einzigen scharfen Gewürze, die europäischen Köchen zur Verfügung standen und entsprechend weit verbreitet waren. Später wurden sie durch Pfeffer (13. Jahrhundert) und Chili (15. Jahrhundert) ergänzt.

In Deutschland wird bereits seit 1726 der Düsseldorfer ABB-Senf hergestellt, damit ist ABB-Senf die älteste deutsche noch existierende Senfmarke. Diese wird heute vom Düsseldorfer Unternehmen Löwensenf industriell hergestellt. Eine traditionelle Herstellung lässt sich noch in Einrichtungen wie der Senfmühle Kleinhettstedt besichtigen.

Ganze Senfsamen des Weißen und des Schwarzen Senfs werden zum Kochen, Braten und Marinieren verwendet. Die mild-nussig schmeckenden Samen entfalten dabei erst während des Garens bzw. Einlegens nach und nach ihr scharfes Aroma.

Senfkörner des Weißen Senfs werden beispielsweise zum Einlegen von Senfgurken und Mixed Pickles sowie zur Wurstherstellung verwendet.

In Indien werden die Blätter und Stängel der Senfpflanze mit Zwiebeln und Knoblauch gekocht und mit Naan-Brot gegessen.

Senfpulver besteht zu mindestens 80 % aus gemahlenen, meist weißen Senfkörnern mit Schalen (Senfmehl) sowie aus Gewürzen, Kräutern und Getreidemehl. Besonders verbreitet ist es in der englischen Küche. Falls es nicht mitgekocht wird, wird es mit Wasser zu einer Paste verrührt, die ähnlich wie Tafelsenf verwendet wird.

Senfpulver kann auch Bestandteil von Currypulver sein.

Die größte Verbreitung findet Senf von jeher zu einer Paste verarbeitet, da sich seine Schärfe so am besten entfalten kann. Traditionell wird der Senf dazu mit Traubenmost vermischt, was von lateinisch "mustum ardens" (brennender Most) über das altfranzösische "mostarde" zum deutschen, heute nur noch landschaftlich gebrauchten Namen "Mostrich" oder "Mostert" und zum englischen "mustard" führte. Heute übliche Grundzutaten sind neben Senf noch Wasser, Essig und Salz. Je nach Sorte können auch verschiedene Gewürze und andere Zutaten hinzukommen.

Zur Herstellung werden die gereinigten Senfkörner zuerst zwischen Walzen geschrotet und meist anschließend entölt. Dann wird der Schrot mit den übrigen Zutaten vermischt und der entstandenen Maische Zeit zum Fermentieren gegeben – dadurch entsteht das typische Aroma. Anschließend wird der Brei, je nach Sorte, zu einer mehr oder weniger glatten Paste vermahlen. Dabei darf die Temperatur 50 °C nicht übersteigen, um die ätherischen Öle zu erhalten. Bei einem zweiten, historischen Verfahren wird die Maische zwischen schweren Steinen, meist Granit oder Basalt vermahlen. Bei diesem sogenannten Kaltvermahlungsverfahren steigt die Temperatur der Masse nicht über 27 °C. In der industriellen Herstellung wird die Masse durch geschlossene, wassergekühlte Mahlwerke vermahlen, wodurch keine ätherischen Öle entweichen können und die Masse nicht über 20 °C erhitzt. Danach muss der Senf noch einige Stunden gelagert werden, da er noch zu dünnflüssig für die industrielle Abfüllung ist. Die Endreife erreicht er dann im Glas oder in der Tube. Frisch abgefüllter mittelscharfer Senf ist noch zu scharf und muss deshalb ein paar Wochen lagern, bis er diese Schärfe abgebaut hat und verkauft werden kann.

Tafelsenf wird zu Wurst, gepökeltem und kaltem Fleisch gereicht oder dient als würzende Zutat für Saucen, Salate, Fisch- und Gemüsegerichte. Wegen seiner neben der würzenden auch emulgierenden Wirkung ist er auch Bestandteil von Mayonnaisen und Vinaigrettes.

Schon durch die Auswahl und das Verhältnis von weißem zu schwarzem Senf, den Mahlgrad der Körner und den verwendeten Most oder Essig entstehen ganz unterschiedliche Geschmacksrichtungen und Konsistenzen. Durch weitere Zutaten wie Zucker, Karamell oder Honig, Meerrettich oder Cayennepfeffer, Kräuter und Gewürze wie Estragon oder Zimt, Zitronensaft, Wein oder Bier, Knoblauch, Tomaten oder Paprika entstehen weitere Geschmacksnuancen zwischen süßlich mild, würzig und sehr scharf. 

In den USA ist Senf flächendeckend in einer einheitlichen, eher lieblichen und milden Geschmackssorte erhältlich. Bei der Herstellung wird Essig verwendet. Amerikanischer Senf hat eine typisch grell-gelbe Farbe, die durch die Zugabe einer höheren Menge an Kurkuma als in Europa üblich erreicht wird. Andere Senfsorten, vor allem die in Europa traditionellen, finden sich jedoch als Importprodukte in den meisten großen Supermärkten.

Beliebteste Form des Senfs in Deutschland ist "mittelscharfer Senf", auch "Delikatess-Senf" genannt. Er wird überwiegend aus weißer und brauner Senfsaat hergestellt. In Österreich und in Teilen der östlichen Bundesländer ist als Variante auch "Meerrettichsenf/Krensenf" beliebt, der zusätzlich Meerrettich/Kren oder dessen Aroma enthält.

Er entspricht dem mittelscharfen, enthält aber einen höheren Anteil brauner Senfsaat. In Österreich versteht man unter „scharfem Senf“ den dunkleren "Estragonsenf".

"Süßer Senf", "bayerischer" oder "Weißwurstsenf" besteht aus grob gemahlenen, teilweise gerösteten Senfkörnern und wird mit Zucker, Süßstoff oder Apfelmus gesüßt. Besonders hochwertiger süßer Senf wird stattdessen mit Honig gesüßt ("Honigsenf"). In Deutschland wird er hauptsächlich zu Weißwurst und Leberkäse gegessen. Erstmals hergestellt wurde bayrischer Senf 1854 von Johann Conrad Develey. Heute gilt Händlmaier's als Marktführer für süßen Senf. Auch in Skandinavien, Frankreich, Österreich und Tschechien kennt man verschiedene Sorten süßen Senfs.

In Krems an der Donau wurde 1819 von den Brüdern "Hietzgern" ein Handelsunternehmen gegründet und im Jahr 1851 begann man in der dortigen Senfsiederei mit der Senfherstellung. Kremser Senf wird aus grob gemahlenen gelben und braunen Senfsamen, etwas Zucker, unvergorenem Weinmost und Weinessig, ursprünglich aus der Region Krems, hergestellt.
Seit 1913 wurde auch im Unternehmen Mautner Markhof die Produktion von Presshefe für Brauereien auch auf andere Lebensmittel wie Senf und Essig erweitert und das Unternehmen gilt heute als Marktführer für Kremser Senf.

Er wird in Österreich umgangssprachlich auch als „Süßer Senf“ bezeichnet, ist jedoch würziger als der in Deutschland ebenso bezeichnete „Bayerische Senf“. Er gehört neben Estragonsenf (umgangssprachlich „Scharfer Senf“) zur Standardauswahl an Senf in der österreichischen Gastronomie.

"Rotisseur-Senf" oder "körniger Senf" ist eine allgemeine Bezeichnung für nur grob geschroteten Senf, der weniger hitzeempfindlich und deshalb zum Würzen von Grillfleisch und Ähnlichem geeignet ist.

Die französische Stadt Dijon erhielt im 13. Jahrhundert ein Monopol auf die Senfherstellung. Dijon-Senf muss nach dem europäischen "Code of Practice" aus braunen oder schwarzen Senfkörnern hergestellt werden. Die Senfkörner werden durch eine Siebschleuder mit feinen Löchern gekratzt, wobei die Schalen im Inneren zurückbleiben und nur der Senfkern weiterverarbeitet wird. Traditionell werden die Senfkörner bei Dijon-Senf nicht entölt, was ihm ein besonders volles Aroma verleiht.

Klassischer Dijon-Senf ist fein gemahlen und scharf. Eine der bekanntesten Varianten ist "Estragonsenf". Nach Dijon-Verfahren hergestellt ist auch der Düsseldorfer Löwensenf.

Traditioneller "Englischer Senf" ist als Zubereitung ein besonders scharfer Senf aus dem Mehl weißer und schwarzer Senfkörner. Varianten enthalten unterschiedliche zusätzliche Gewürze, aber beim Original kommt die besondere Schärfe allein aus dem Senfmehl und nicht etwa aus den Zusätzen.

Auch wenn Mostrich in einigen Regionen als Synonym für alle Senfsorten verwendet wird, ist er doch ursprünglich Senf, bei dem statt Essig unvergorener Traubensaft („Most“) verwendet wird.

Dieser scharfe, malzig-braune Senf wird in Düsseldorf aus brauner und gelber Senfsaat und Branntweinessig nach einer Rezeptur aus dem 18. Jahrhundert hergestellt.

Eine französische Variante des süßen Senfs ist der violette Senf, der hauptsächlich wegen seiner Farbe verwendet wird.

Als Auslöser der Senfallergie unterliegt Senf als Zutat in Nahrungsmitteln der Kennzeichnungspflicht nach der europäischen Lebensmittel-Informationsverordnung.




</doc>
<doc id="9553" url="https://de.wikipedia.org/wiki?curid=9553" title="Honduras">
Honduras

Honduras ([], spanische Aussprache [], vollständige Bezeichnung Republik Honduras, spanisch ) ist ein Staat in Zentralamerika. Honduras grenzt an Nicaragua, El Salvador und Guatemala. Der Name kommt von dem spanischen Wort "hondura", das "Tiefe" bedeutet.

Honduras liegt an der breitesten Stelle der mittelamerikanischen Landbrücke und grenzt im Südosten an Nicaragua, im Nordwesten an Guatemala und im Südwesten an El Salvador. Die Südküste liegt am Nordpazifik, im Norden liegt das Karibische Meer. Hier reicht das von Flüssen, Sümpfen und Lagunen durchsetzte Küstentiefland 70 Kilometer ins Landesinnere hinein. Durch das vorwiegend gebirgige Land verläuft von Ost nach West ein Bergmassiv, das von mehreren Flüssen durchzogen wird. Die höchste Erhebung des Landes ist der Cerro Las Minas mit 2870 Meter Höhe. Dem Golf von Fonseca im Südwesten sind zahlreiche vulkanische Inseln vorgelagert. In den Tälern an der Karibikküste befinden sich zahlreiche Bananenplantagen. Der Großteil der Bevölkerung lebt im Westteil des Landes und im Departamento Cortés nördlich des Yojoa-Sees.

Honduras hat ein tropisches Klima, das in den höheren Lagen im Landesinneren gemäßigt ist. Im immerfeuchten karibischen Tiefland liegt die durchschnittliche Jahrestemperatur bei rund 26 °C. Im wintertrockenen pazifischen Raum beträgt das Jahresmittel etwa 31 °C, in den gemäßigten höheren Lagen circa 20 °C.
Die Niederschlagsmenge nimmt von Nord nach Süd ab. In den Gebirgstälern liegt das Jahresmittel bei 1016 Millimetern, entlang der Nordküste bei 2540 Millimetern. Die Trockenzeit dauert von November bis Mai. Die Regenzeit beginnt ungefähr im Mai und endet im Oktober. An der karibischen Küste jedoch regnet es das ganze Jahr über. Wirbelsturm Mitch verursachte im November 1998 große Schäden.

Die meisten Flüsse des Landes fließen in den Atlantischen Ozean. Der 320 Kilometer lange Río Patuca und der 240 Kilometer lange Río Ulúa sind die beiden längsten Flüsse des Landes. Der größte See und gleichzeitig größtes Trinkwasserreservoir Honduras' ist der etwa 80 km² große Lago de Yojoa.

Wichtigste Flüsse:

Honduras weist einen großen Naturreichtum auf. Dazu gehören insbesondere der noch nahezu unberührte größte zusammenhängende Regenwald in Zentralamerika im Nordosten des Landes (Mosquitia) sowie das nach dem australischen Barrier-Riff weltweit größte Korallenriff-Ökosystem um die dem honduranischen Festland vorgelagerten Karibikinseln (Islas de la Bahía). In beiden Fällen besteht allerdings auch die Gefahr von zunehmenden Umweltgefährdungen durch illegales Abholzen und Brandrodungen aufgrund des demografischen Drucks einerseits und durch Überfischung, Meeresverschmutzung sowie zunehmenden Tauchertourismus andererseits.

Zunehmend setzt sich jedoch auch bei der honduranischen Regierung der Gedanke einer Förderung des internationalen Ökotourismus durch. In den großen Städten gibt es viele Umweltprobleme wie Wassermangel, Erosionserscheinungen, fehlende Abwasserentsorgung, illegale Mülldeponien und unkontrollierte Bautätigkeit.

Etwa 48,1 Prozent des Landes sind von Wald bedeckt (2000). Das rund 5000 km² große Biosphärenreservat Río Plátano ist das größte Naturschutzgebiet von Honduras und gehört zu den letzten noch intakten Regenwaldgebieten Zentralamerikas. Der Park wurde 1982 von der UNESCO zum Weltnaturerbe erklärt. Insgesamt stehen etwa zehn Prozent der Landesfläche unter Naturschutz.
Eichen- und Kiefernwälder bedecken die kühleren Hochländer, im Flachland herrschen Gräser vor, Mangroven und Palmen finden sich in den Küstengebieten. Jährlich werden ca. 3000 km² Wald zerstört.

Wo das Holz abtransportiert werden kann, hat der Raubbau die Bestände an Edelhölzern wie Mahagoni, Zeder und Gelbholz schon stark verringert. Inzwischen haben US-Gesellschaften damit begonnen, die Kiefernsavannen im Nordosten ebenfalls holzwirtschaftlich zu nutzen. Sollte diese Entwicklung anhalten, werden in etwa 20 Jahren Wüsten die üppigen Wälder ersetzt haben.
In Honduras sind zahlreiche Tierarten beheimatet, u. a. Insekten, Krokodile, Schlangen, Echsen, Schildkröten, Hirsche, Affen (z. B. der Weißschulterkapuziner, Honduras' kleinster Affe) und Kojoten. Ferner leben hier Raubkatzen wie Jaguar, Puma und Ozelot sowie verschiedene Reptilien, Vögel und Meerestiere.

Die Bevölkerung konzentriert sich auf das Hochland im Nordwesten, das Gebiet um Tegucigalpa sowie den pazifischen Süden. Anders als etwa in Guatemala gehört der überwiegende Teil der honduranischen Bevölkerung zu den Mestizen, Nachfahren von europäischen Einwanderern und den Ureinwohnern des Landes. Sie stellen etwa neunzig Prozent der Bevölkerung. Der Rest der Bevölkerung sind Indigene (sieben Prozent), Schwarze (zwei Prozent) und Europäer (ein Prozent).

Die Garifuna, die indianische und afrikanische Vorfahren haben, leben vor allem in Küstendörfern an der gesamten Karibikküste von Honduras, Belize und z. T. Nicaragua und Guatemala. Sie haben sich eine vollständig eigenständige Kultur bewahrt und sprechen noch die Garífuna-Sprache, die zu den Arawak-Sprachen gehört. Entgegen der häufigen Meinung leben die Garifuna noch nicht sehr lange auf den Cayos Cochinos und den Islas de la Bahía (Bay Islands). Die dortigen Garifuna-Siedlungen entstanden erst in den 1950er Jahren.

Im Landesinneren und in Mosquitia gibt es noch kleine Gruppen indianischer Ureinwohner. Sie haben sich in schwer zugängliche Bergtäler zurückgezogen, wo sie vom Bildungs- und Gesundheitswesen und vom politischen Geschehen abgeschnitten sind. Einige Stämme von Flussindianern durchstreifen die Wälder an der Karibikküste noch als Jäger und Sammler. Das Vorrücken der Holzfällerkolonnen könnte auch ihren Lebensraum zerstören.

Das hohe Bevölkerungswachstum verstärkt die Landflucht und lässt die Elendsviertel der Städte wachsen. Mehr als die Hälfte der Einwohner lebt unterhalb der Armutsgrenze, ein Fünftel sind Analphabeten. Unter- und Fehlernährung sind weit verbreitet. Die medizinische Versorgung auf dem Land ist miserabel. Der Anteil der Analphabeten wurde zwar in den Städten auf unter zwanzig Prozent gedrückt, liegt aber auf dem Land noch immer bei über fünfzig Prozent. Der schlechte Ausbildungsstand ist wiederum ein Grund für den Mangel an Fachkräften, der die Entwicklung in allen Bereichen behindert.

Quelle: UN

Altersstruktur

Die Altersstruktur der honduranischen Bevölkerung ist durch das relativ hohe Bevölkerungswachstum von derzeit jährlich 2,8 Prozent geprägt. Kinder und Jugendliche unter 15 Jahren machen über ein Drittel (41 Prozent) der Gesamtbevölkerung aus. Honduras liegt bei diesen Indikatoren erheblich über dem lateinamerikanischen Durchschnitt (jährliches Bevölkerungswachstum: 1,6 Prozent; Bevölkerung unter 15 Jahren: dreißig Prozent).

41 Prozent der Bevölkerung sind unter 15 Jahre alt, 56 Prozent sind zwischen 15 und 64 Jahre alt, 4 Prozent sind älter als 65 Jahre. Das Durchschnittsalter liegt bei 19 Jahren.

Amtssprache ist Spanisch, was auch von den Mestizen gesprochen wird. Daneben sind indigene Sprachen verbreitet, wie zum Beispiel Miskito und Tawahka an der Miskitoküste, oder wenige Tausend Kekchí- und Chortí-Sprecher im Westen. An der Atlantikküste und auf den vorgelagerten Inseln wird kreolisch eingefärbtes Englisch gesprochen.

Die Bevölkerung gehört zu ähnlichen Teilen der römisch-katholischen (47 %) und anderen christlichen (protestantischen und evangelikalen) Glaubensrichtungen (41 %) an. Durch Unterstützung aus den USA haben verschiedene protestantische Kirchen erfolgreiche Missionsarbeit geleistet, besonders auf den Islas de la Bahía.
Der Katholizismus ist jedoch die mit Abstand größte einheitliche Religionsgemeinschaft. International anerkannt ist der Anfang des Jahres 2001 in den Kardinalsstand erhobene Erzbischof von Tegucigalpa und ehemalige Vorsitzende der lateinamerikanischen Bischofskonferenz, Óscar Rodríguez Maradiaga. Als Vorsitzender der vom Staatspräsidenten gebildeten Anti-Korruptionskommission setzt er sich für mehr Transparenz und soziale Verantwortung im politischen Leben seines Landes ein.

Die Indigenen – insbesondere in den abgelegenen Regionen – pflegen noch heute ihre traditionellen mesoamerikanischen Religionen, häufig synkretistisch vermischt mit christlichen oder auch afrikanisch-religiösen Elementen.

Honduras ist nach Haiti eines der ärmsten Länder Mittelamerikas.
71,6 % der Bevölkerung lebt unterhalb der Armutsgrenze. Andere Schätzungen gehen von bis zu 80 % aus.

Jedes Jahr wandern viele Honduraner ins Ausland aus, vornehmlich in die USA. Die Arbeitslosigkeit in Honduras ist hoch, jedoch sind aufgrund der Erwerbsstruktur die Statistiken kaum verlässlich. Gerade die tatsächliche Armut der Landbevölkerung ist über eine Arbeitslosenstatistik nicht erfassbar.

Problematisch sind auch die aus Armut und Hoffnungslosigkeit entstandenen Jugendbanden wie „Barrio 18“ (auch als Mara 18 bekannt) oder „Mara salvatrucha“, die teilweise ganze Viertel und Städte terrorisieren. Viele Mitglieder kommen aus zerrütteten Familien.
Schätzungen gehen davon aus, dass beide rivalisierende Jugendbanden jeweils bis zu 40.000 Mitglieder haben. Ähnliche Banden, deren Mitglieder ihre Zugehörigkeit u. a. durch Tätowierungen zum Ausdruck bringen, sind auch in anderen benachbarten Staaten ein Problem.
Die Regierung geht hart gegen diese Jugendbanden vor. Seit 2003 gibt es ein Gesetz, das allein die Mitgliedschaft in einer Bande mit mindestens drei Jahren Gefängnis bestraft. Die katholische Kirche und andere christliche Organisationen haben ein Resozialisierungsprogramm für ehemalige „Mareros“ eingerichtet.

Die Fruchtbarkeitsrate beträgt 3,7 Kinder pro Frau. Die Sterblichkeit bei unter 5-jährigen beträgt 40 pro 1000 Lebendgeburten. Honduras hat 57 Ärzte pro 100 000 Einwohner
Die Lebenserwartung liegt bei 71,1 Jahren (Stand 2016).

Die größte Einzelposition im honduranischen Staatshaushalt ist der Bildungs- und Erziehungssektor. Nahezu 20 % des Gesamtbudgets entfallen auf diesen Bereich. Das Land verfügt über ca. 60.000 Lehrer, die rund eine Million Grundschüler und 700.000 Schüler weiterführender Schulen betreuen. Die Regierung bemüht sich verstärkt um eine Reduzierung der Analphabetismusrate von 11,5 %. In Honduras besteht Schulpflicht für Kinder zwischen sieben und zwölf Jahren, der Schulbesuch ist kostenlos.
Trotzdem haben in Honduras rund 50 % aller Kinder keinen Grundschulabschluss. Ursachen sind fehlende Mittel für Schulbücher und Lehrmittel, Lehrermangel in abgelegenen Gebieten und teilweise Schulunterricht in großen Klassen (1. bis 6. Klasse) in einem einzigen Schulzimmer.
Nach der Pflichtschule kann der "plan basico" absolviert werden (normalerweise mit 15 Jahren). Danach kann am "colegio" in weiteren 6 Jahren (jeweils 3 Jahre Grundkurs und 3 Jahre Spezialisierung) ein Fachabitur erlangt werden. Wer später studieren will, macht einen Abiturabschluss, den "bachillerato".

Es gibt zwei staatliche Universitäten, die „Nationale Autonome Universität von Honduras“ (UNAH) in Tegucigalpa mit weiteren Studienzentren in San Pedro Sula, La Ceiba, Santa Rosa de Copán und Comayagua sowie die „Nationale Pädagogische Universität Francisco Morazán“ (UNPFM) in Tegucigalpa. Die wichtigsten Privatuniversitäten sind die Technische Universität (Universidad Tecnologíca) und die von der katholischen Kirche getragene Universität (Universidad Católica). Zudem gibt es im Land zwei Fachhochschulen im Agrar- und Forstbereich (Escuela Agricola Panamericana Zamorano, Escuela Nacional de Ciencias Forestales).

Hinweise auf Siedlungen der Maya in Honduras gibt es seit mindestens 1000 v. Chr. in Copán im Westen des Landes, doch wie viele Stadtstaaten der Maya, wurde auch diese unter geheimnisvollen Umständen um etwa 900 n. Chr. verlassen. Kolumbus setzte im Jahre 1502 bei Trujillo im Norden von Honduras erstmals seinen Fuß auf das amerikanische Festland und nannte das Land in Anspielung auf die große Tiefe des Wassers vor der Karibikküste „Honduras“ (Tiefe).

Cristóbal de Olid ging am 3. Mai 1523 im Auftrag von Hernán Cortés an Land und nahm es für die spanische Krone in Besitz.
Als die spanischen Konquistadoren bei ihrem Vormarsch 1524 in dieses ehemals von den Maya besiedelte Gebiet eindrangen, stießen sie auf zahlreiche untereinander verfeindete Indiostämme wie die Lenca, Pipil, Chorotega, Jicaques, Paya und andere, die zunächst erbitterten Widerstand leisteten. 1525 unternahm Hernán Cortés einen Kriegszug, 1536 Pedro de Alvarado einen weiteren, um den vom Häuptling Lempira geführten Aufstand niederzuschlagen. 1540 gründeten die Spanier schließlich die Hauptstadt Comayagua und gliederten die Provinz dem Generalkapitanat von Guatemala ein. Die heutige Hauptstadt wurde 1579 als Siedlung in der Nähe von Gold- und Silberminen gegründet. Um diese Edelmetalle ging es den Eroberern in erster Linie. Die durch Kriege, Zwangsarbeit und eingeschleppte Krankheiten von über 1,2 Millionen auf im Jahre 1778 ca. 88.000 dezimierten Indios behielten zwar ihr Gemeindeland zur Selbstversorgung, mussten aber Arbeitskräfte für Bergwerke und Ländereien der Kolonisten stellen.

1821 schloss sich die Provinz-Oligarchie der Erhebung der anderen zentralamerikanischen Provinzen gegen die spanische Krone an. Nach nur zweijähriger Anbindung an das Kaiserreich von Mexiko verselbständigten sie sich im Staatenbund der „Vereinigten Provinzen von Zentralamerika“ (Zentralamerikanische Konföderation). Eine seiner Führungsfiguren, der als „mittelamerikanischer Bolívar“ verehrte Francisco Morazán (1792–1842), stammte aus Honduras. Aber auch er konnte das Ausbrechen seiner Heimatprovinz und den Zusammenbruch des Staatenbundes 1839 nicht verhindern. Von Beginn der „Freiheit“ an kämpften mehrere Fraktionen der Oligarchie um die Macht.

Von 1821 bis 1876 wechselten sich 85 Regierungen ab. Erst 1876 stabilisierte sich die Regierung und Marco Aurelio Soto leitete eine liberale Wende ein: er säkularisierte den Kirchenbesitz, führte die Zivilehe und ein staatliches Bildungswesen ein. Seine Gegner (Kirche und Großgrundbesitzer) und Befürworter (das städtische Bürgertum) organisierten sich später in der Nationalen Partei und in der Liberalen Partei, die bis heute die wichtigsten Parteien geblieben sind. Gleichzeitig trieb Soto die Erschließung und Weltmarktöffnung des isolierten Landes voran. Großzügige Konzessionen lockten US-Konzerne an und führten zur Entwicklung der „Bananenrepublik“. Die Geschichte einer kolonieähnlichen Fremdbestimmung begann. Die Diktatoren Tiburcio Carías Andino und Juan Manuel Gálvez, die von 1933 bis 1948 bzw. von 1949 bis 1954 an der Macht waren, betätigten sich als Handlanger der United Fruit Company. Der Streik von rund 25.000 Arbeitern auf den US-amerikanischen Bananenplantagen im Jahre 1954 leitete die Bändigung der Macht ein, die von der „Bananen-Enklave“ auf den „Rest des Landes“ ausgeübt wurde. Ebenfalls 1954 stellte Honduras sein Territorium für eine von der CIA organisierte Invasionstruppe im Rahmen der Operation PBSUCCESS zum Sturz des guatemaltekischen Präsidenten Jacobo Arbenz Guzmán zur Verfügung.
1969 kam es zu einer militärischen Auseinandersetzung mit El Salvador, die als „Fußballkrieg“ in die Geschichte einging. Der Grund waren Spannungen um Wirtschaftsflüchtlinge aus El Salvador, die seit längerem von der Bevölkerung Honduras’ für die wirtschaftlichen Probleme verantwortlich gemacht und angefeindet wurden. Der Krieg kostete 3000 Menschen das Leben, 6000 wurden verletzt. Der Konflikt selbst dauerte jedoch bis 1980 an und wurde unter Vermittlung der Organisation Amerikanischer Staaten durch ein Friedensabkommen beigelegt.

Die Reformansätze des Präsidenten José Ramón Villeda Morales, der von 1957 bis 1963 regierte, wurden zwar zunächst durch Militärputsche aufgehalten, aber die Militärregierung unter Oswaldo López Arellano, der von 1972 bis 1975 an der Macht war, griff sie wieder auf und trieb sie unter dem Druck von Gewerkschaften und Bauernorganisationen voran. Sie packte vor allem eine vergleichsweise großflächige Agrarreform an, obwohl ihre Durchführung von zwei nachfolgenden Militärregierungen gebremst wurde. Die Ausmaße der Unterdrückung und Folter unter den Militärregimen in Honduras waren jedoch nicht ganz so stark ausgeprägt wie in den Nachbarstaaten. Sie verboten zwar allzu linke Parteien und Organisationen, beließen aber den beiden großen Parteien samt ihren Abspaltungen und vor allem den starken Bauernorganisationen politische Freiräume. Diese autoritäre Toleranz bildet einen wesentlichen Grund, warum bislang in Honduras Guerillagruppen über Ankündigungen des bewaffneten Widerstandes nicht hinauskamen. Dem sogenannten Bataillon 316 werden im Zusammenhang mit der Diktatur die Ermordung, das Verschwindenlassen und die Folterung hunderter Honduraner vorgeworfen. Das Bataillon wurde vom amerikanischen CIA und dem argentinischen Militär trainiert.

Bei den Präsidentschaftswahlen von 1981, die die Rückkehr zur Demokratie signalisierten, erhielt der Kandidat der Liberalen Partei, Roberto Suazo Córdova, eine eindeutige Mehrheit. Viele Beobachter hegten Zweifel an seiner politischen Überlebensfähigkeit in einem Land, in dem es in 150 Jahren immerhin 125 Militärputsche gab. Er überstand eine schwere Wirtschaftskrise und viele Putschgerüchte. Er und sein Nachfolger José Azcona Hoyo mussten es allerdings wohl oder übel hinnehmen, dass Honduras zu einem Brückenkopf der USA im unerklärten Krieg gegen Nicaragua wurde. Erst 1989 erzielte eine internationale Konferenz Einigung über den Abzug der nicaraguanischen „Contras“.

Das von der Schuldenkrise gebeutelte Land ist von ausländischer Überlebenshilfe (insbesondere aus den USA) abhängig, die es jedoch nicht ohne Gegenleistungen gibt. Die Tradition der Fremdbestimmung erhielt eine neue Variante, das Wort von der „Bananenrepublik“ eine neue Begründung. Zu den Folgewirkungen der massiven Präsenz der USA gehörten nationalistische Aufwallungen. 1992 schlossen El Salvador, Guatemala und Honduras ein Freihandelsabkommen. Der 1993 gewählte Präsident Carlos Roberto Reina versuchte in seiner Amtszeit, den Einfluss des Militärs einzudämmen. Sein Nachfolger Carlos Roberto Flores Facussé initiierte eine reformorientierte Wirtschaftspolitik. Bei den Präsidentschaftswahlen 2002 ging Ricardo Maduro Joest von der Nationalen Partei als Sieger hervor.

1998 zog der Wirbelsturm "Mitch" gleich mehrere Male über Honduras hinweg und verwüstete dabei große Teile des Landes. Selbst das von der Karibikküste beinahe 300 km entfernte Tegucigalpa blieb nicht verschont und noch heute sind die Folgen der Naturkatastrophe sichtbar. „Mitch“ warf die honduranische Wirtschaft um Jahre zurück, da insbesondere die Bananenplantagen unter der Katastrophe gelitten haben.

Die Präsidentschaftswahlen im November 2005 gewann mit Manuel Zelaya Rosales wieder ein Kandidat der Liberalen. Er versprach vor allem nachhaltige Maßnahmen gegen die Kriminalität. Manuel Zelaya wollte jedoch nach Ablauf seiner 4-jährigen Amtszeit ein Verfassungsreferendum durchführen lassen, welches seine Wiederwahl ermöglicht hätte. Da das der honduranischen Verfassung nicht entsprach, wurde gegen ihn ein Amtsenthebungsverfahren durch die beiden Kongresskammern eingeleitet, welches durch das Oberste Gericht (Corte Suprema de Justicia) bestätigt wurde. Da Zelaya dem nicht Folge leisten wollte, veranlasste der Oberste Richter seine Verhaftung.

Am Morgen des 28. Juni 2009 wurde der amtierende Präsident Manuel Zelaya Rosales vom Militär gestürzt, zunächst gefangengenommen und dann nach Costa Rica ausgeflogen. In weiten Teilen der Hauptstadt wurden der Strom abgestellt und die Telefonverbindungen gekappt. Der staatliche Fernsehsender Canal 8 und viele Radiosender, sowie ausländische Kanäle wie teleSUR und Cubavisión Internacional, wurden abgeschaltet. Die nicht abgeschalteten Fernsehkanäle sendeten keine politischen Informationen. Als Nachfolger wurde vom Parlament Roberto Micheletti zum Interimspräsidenten ernannt, der als Erstes eine 48-stündige Ausgangssperre verhängte. Trotzdem kam es zu Protesten. Der neue Präsident war international nicht anerkannt. Sämtliche EU-Staaten und sämtliche Staaten Amerikas, mit Ausnahme der USA, zogen ihre diplomatischen Vertretungen auf Botschafter-Ebene aus Honduras ab. Nicaragua, El Salvador und Guatemala schlossen ihre Grenzen zu Honduras. Am 21. September 2009 kehrte Zelaya überraschend nach Honduras zurück und fand vorübergehend in der brasilianischen Botschaft Asyl, ab Mai 2011 schloss Zelaya mit dem Präsidenten Lobo einen Vertrag über seinen Verbleib im Land.

Am 29. November 2009 fand wie geplant die Präsidentenwahl statt. Als neuer Präsident von Honduras wurde Porfirio Lobo Sosa gewählt. 

Im Jahr 2013 wurde Juan Orlando Hernández aufgrund eines Wiederwahlverbots der Nachfolger von Lobos. 

Im Nachgang zu den Wahlen im November 2017, welche Hernández knapp gewonnen hatte, wurden Manipulationsvorwürfe laut und die Opposition mit dem Kandidaten Salvador Nasralla wollte das Resultat nicht anerkennen. Bei Unruhen kamen bis Mitte Januar 2018 mindestens 20 Menschen ums Leben, über 1500 Personen waren festgenommen worden. Die Opposition wollte die Proteste bis mindestens zur Amtseinführung am 27. Januar 2018 fortsetzen.

Im Demokratieindex 2016 der britischen Zeitschrift "The Economist" belegt Honduras Platz 79 von 167 Ländern und gilt damit als eine „Hybridregime“ aus demokratischen und autoritären Elementen. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „partiell frei“ bewertet. Das Land verfügt zwar über ein Mehr-Parteien System, jedoch wird die politische Stabilität durch Korruption, institutionelle Schwäche und die allgegenwärtige Gewalt im Land bedroht.

Honduras ist laut Verfassung von 1982 ein demokratischer Rechtsstaat. Gewaltenteilung und Unabhängigkeit der Justiz sind garantiert. Traditionell spielt der mit umfassenden Befugnissen ausgestattete Staatspräsident eine vorherrschende Rolle. Der Präsident wird vom Volk direkt gewählt. Die Amtszeit des Präsidenten beträgt vier Jahre, ohne Möglichkeit der Wiederwahl. Der Einkammer-Nationalkongress setzt sich aus insgesamt 128 Abgeordneten der beiden großen traditionellen Parteien (Partido Liberal und Partido Nacional) sowie der Splitterparteien PINU, PDCH und PUD zusammen. Dem Obersten Gerichtshof gehören neun von Regierung und Parlament gemeinsam benannte Richter an, ihre Amtszeit beträgt sieben Jahre. Jüngste institutionelle Reformen haben einen nationalen Rechnungshof und ein Oberstes Wahlgericht geschaffen.

Seit Ende der 1980er Jahre sind die jeweiligen Staatspräsidenten aus freien und demokratischen Wahlen hervorgegangen. Die beiden großen Parteien haben sich in dieser Zeit in der Regierungsverantwortung abgelöst. Am 28. Juni 2009 wurde jedoch der letzte gewählte Staatspräsident Manuel Zelaya vom Militär gestürzt. Dem Staatsstreich vorausgegangen war ein Streit über ein Referendum zur Einberufung einer Verfassungsgebenden Versammlung. Am Tag des Putsches war der Volksentscheid geplant. Der Staatspräsident setzte die Bemühungen der Vorgängerregierung um Armutsbekämpfung und Verbesserung der wirtschaftlichen und sozialen Verhältnisse fort. Diese Bemühungen waren durch den verheerenden Wirbelsturm „Mitch“ Ende des Jahres 1998 erheblich erschwert worden. Das von Honduras im September 2001 vorgelegte Armutsbekämpfungsprogramm wurde von der Weltbank und Internationalem Währungsfonds als Voraussetzung für die Teilnahme des Landes an der HIPC-Entschuldungsinitiative (Highly-Indebted Poor Countries) angenommen. Ein erster Schritt dazu war im Februar 2004 die Einigung mit dem IWF über ein Dreijahres-Hilfsprogramm 2004/2006.

Die Lage der Menschenrechte in Honduras hat sich nach der Konsolidierung des Friedensprozesses in Zentralamerika und der Etablierung demokratischer Regierungen erheblich verbessert. Allerdings kommt es nach Angaben von nichtstaatlichen Menschenrechtsorganisationen immer wieder zu institutionellen Unzulänglichkeiten und Übergriffen von Polizei und Sicherheitsorganen. Besonders davon betroffen sind Angehörige ethnischer Minderheiten (indigene Gruppen, Nachfahren afrikanischer Einwanderer) und sozial benachteiligte Gruppen (Straßenkinder). Ein weiter nicht gelöstes Problem aus der Vergangenheit ist die Straflosigkeit der für Menschenrechtsverletzungen während früherer Militärregimes verantwortlichen Personen. Wiederholte Bemühungen des vom Nationalkongress eingesetzten Menschenrechtskommissars und nationaler Nichtregierungsorganisationen (Komitee der Familienangehörigen von Verhafteten/Verschwundenen) haben bisher nicht die erhofften Ergebnisse erbracht. Die Lage in den honduranischen Haftanstalten ist besorgniserregend. Im Mai 2004 kam es im Zentralgefängnis in der zweitgrößten honduranischen Stadt San Pedro Sula zu einer Brandkatastrophe, bei der über einhundert Todesopfer zu beklagen waren. Der vom damaligen Staatspräsidenten Ricardo Maduro Joest verfügte Einsatz des Militärs zur Verbrechensbekämpfung hat nach Ansicht von Amnesty International und lokaler Menschenrechtsgruppen wie der Kinderrechtsorganisation Casa Alianza nicht zu einer Verbesserung geführt. Ein Problem stellt die Bekämpfung der vor allem in den Großstädten ausufernden Bandenkriminalität dar.

Seit dem Staatsstreich in Honduras 2009 beklagen Menschenrechtsorganisationen eine andauernde Welle von Menschenrechtsverletzungen gegenüber Gegnern des Staatsstreichs, darunter zahlreiche Morde, Folter, Vergewaltigungen und Entführungen. Die Vorsitzende des "Komitees der Familienangehörigen von Festgenommenen und Verschwundenen in Honduras (COFADEH)", Bertha Oliva, bezeichnete die Menschenrechtsverletzungen unter der Regierung Lobo als „systematisch und selektiv“ und konstatierte, dass diese Menschenrechtsverletzungen nun von dafür bezahlten Personen begangen werden. Darüber hinaus seien Beamte der staatlichen Institutionen aktiv daran beteiligt, Ermittlungen zu manipulieren und Anzeigen wegen Menschenrechtsverletzungen zu vertuschen.

Im Gefängnis von Comayagua brach am 14. Februar 2012 ein Großbrand aus, der 358 Insassen das Leben kostete. Das Gefängnis war für 400 Personen ausgelegt, doch mit 820 Personen überbelegt.

Nach der Beilegung der Bürgerkriege in Nicaragua (1990), El Salvador (1992) und Guatemala (1996) ist Honduras insbesondere an der wirtschaftlichen Entwicklung in der Region und an einer weitergehenden Integration Zentralamerikas interessiert. Das Land ist aktives Mitglied in dem durch das Protokoll von Tegucigalpa 1991 gegründeten Zentralamerikanischen Integrationssystem. Mit Nicaragua und El Salvador gibt es seit geraumer Zeit Grenzstreitigkeiten, die jeweils dem Internationalen Gerichtshof in Den Haag zur rechtlichen Klärung vorgelegt und zum Teil auch gelöst wurden. Auch die Organisation Amerikanischer Staaten (OAS) ist mit den Grenzfragen befasst und tritt als Vermittler auf.

Die Beziehungen zu den Nachbarstaaten im Norden (Guatemala, Belize und Mexiko) und im Süden (Costa Rica, Panama) sind freundschaftlich. Am 16. April 2001 trat der im Jahr zuvor unterzeichnete Freihandelsvertrag („Tratado de Libre Comercio“) zwischen Mexiko und den drei Ländern des sog. „nördlichen Dreiecks“ El Salvador, Honduras und Guatemala in Kraft.

Honduras unterhält diplomatische Beziehungen zu Taiwan. Die Beziehungen zu diesem Land, aber auch zu Japan und Korea, sind aufgrund der erheblichen entwicklungspolitischen Leistungen und des handelspolitischen Gewichts der betroffenen Staaten intensiv. Im Januar 2002 wurden die diplomatischen Beziehungen zu Kuba wiederhergestellt.

Honduras hatte während des Dritten Golfkrieges 370 Soldaten im Irak stationiert und zählte zu den Staaten der „Koalition der Willigen“, die die USA mit Truppen unterstützten. Kurz nach der Entscheidung Spaniens, seine Soldaten zurückzuziehen, folgte Honduras Mitte April 2004 diesem Beispiel.

Honduras ist Gründungsmitglied der Vereinten Nationen und der Organisation Amerikanischer Staaten, von der es seit dem 5. Juli 2009 allerdings vorübergehend suspendiert wurde. Auch Honduras' Mitgliedschaft in der CELAC ist derzeit suspendiert. Außenpolitisch wichtig für das Land ist die Zugehörigkeit zur WTO, den Bretton-Woods-Institutionen (Internationaler Währungsfonds und Weltbank) sowie zu den Institutionen des zentralamerikanischen Integrationssystems. 
UNDP, UNIDO, UNICEF, FAO, WHO, IMF, Weltbank, Interamerikanische Entwicklungsbank, OAS und die Internationale Organisation für Migration (OIM) haben eigene Vertretungen in Honduras. Die Zentralamerikanische Bank für Wirtschaftsintegration (BCIE) hat ihren Sitz in Tegucigalpa. Honduras ist Mitglied von Petrocaribe.

Honduras ist, zumindest was die Städte betrifft, eines der unsichersten Länder der Welt. Die Zeitungen sind tagtäglich voll von Meldungen über Morde, Überfälle, Vergewaltigungen, Einbrüche, Entführungen usw. Ein Teil dieser Taten, wenn auch wahrscheinlich nicht in dem von den Medien dargestellten Ausmaß, geht auf das Konto der schon erwähnten Jugendbanden (maras). In den Städten sind vor allem die Armenviertel von der Gewalt betroffen. In den reicheren Vierteln gibt es viele staatliche und private Sicherheitsleute. Viele Gewalttaten haben jedoch auch einen privaten Hintergrund.

Nach Zahlen der UNODC hatte Honduras 2015 die weltweit zweithöchste Zahl an Tötungsdelikten pro Einwohner, nämlich 63,7 je 100.000 (auf dem ersten Platz liegt El Salvador mit 108,6, in Deutschland sind es 0,8). Im weltweiten Städtevergleich (größer 300.000 Einwohner) lag San Pedro Sula 2014 mit 171,2 Tötungsdelikten/100.000 Einwohner zum vierten Mal hintereinander auf dem ersten Platz.

Im April 1995 beschloss das Parlament die Abschaffung der allgemeinen Wehrpflicht und den Aufbau einer Armee aus freiwilligen Soldaten mit einer Stärke von ca. 18.800 Mann. Durch eine Verfassungsänderung vom Januar 1999 wurden die Streitkräfte ("Fuerzas Armadas de Honduras") dem Präsidenten unterstellt.


Honduras ist in 18 Verwaltungsbezirke (Departamentos) aufgeteilt.

Die Hauptstadt und größte Stadt mit 1,05 Millionen Einwohnern (Stand 2017) ist Tegucigalpa, sie liegt im südlichen Teil des Landes im Gebirge, im Hinterland der Pazifikküste. Die größte Stadt im Norden ist San Pedro Sula (640.000 Einwohner), sie liegt im Nordwesten des Landes, in einer Entfernung von etwa 40 km zur Karibikküste und ist ein wichtiges Handelszentrum. Die drei wichtigsten internationalen Flughäfen befinden sich in diesen zwei Städten, sowie in La Ceiba (189.000 Einwohner), die zusammen mit Puerto Cortés (65.000 Einwohner) die größten Hafenstädte an der Karibikküste sind.

2016 lebten 55,3 % der Bevölkerung in Städten oder städtischen Räumen. Die größten Städte sind (Stand 2017):

Für viele ist Honduras noch immer die sprichwörtliche „Bananenrepublik“. Diese Bezeichnung ist heute ebenso irreführend wie die Zweiteilung in die „Bananenenklave“ und den „Rest des Landes“. Drei US-amerikanische Konzerne, die United Fruit Company, die Standard Fruit Company und die Cuyamel Fruit Company, hatten sich um die Wende vom 19. zum 20. Jahrhundert mit Hilfe großzügiger Konzessionen riesige Flächen im karibischen Tiefland angeeignet. Sie bauten Straßen, Eisenbahnen und Siedlungen für ihre Arbeiter und wurden die größten Arbeitgeber im Land; sie zahlten nur geringe Steuern, obwohl ihre Gewinne viele Jahre den Staatshaushalt überstiegen; sie machten die „Enklave“ zum weltgrößten Exporteur von Bananen; sie korrumpierten die Politiker und versorgten willfährige Diktatoren mit Geld und Waffen. Als diese in den Jahren 1911, 1913 und 1924/1925 dennoch mit Streiks und Unruhen nicht fertigwurden, schickte Washington Interventionstruppen. Während der Weltwirtschaftskrise in den 1930er-Jahren kaufte die United Fruit Company den Cuyamel-Konkurrenten auf und wurde zum „Staat im Staate“. Der von 1933 bis 1948 regierende Diktator Tiburcio Carías Andino leistete ihr durch die Unterdrückung von Gewerkschaften und Streiks Schützenhilfe. Nach dem großen Streik von 1954 verkleinerten die beiden Konzerne schrittweise ihre Plantagen und halbierten die Zahl der Arbeiter. Allmählich nahm der Staat stärkeren Einfluss auf die Produktion und Vermarktung der Bananen. Gleichzeitig holte der „Rest des Landes“ auf. Der Anteil der Bananen am Gesamtexport fiel von rund 50 % in den 1960er-Jahren auf ein Drittel in den 1990er-Jahren. Vor allem die Anteile von Kaffee und Fleisch erhöhten sich.

Honduras gehört zu den ärmsten Ländern Mittelamerikas. Der größte Teil der Bevölkerung (80 %) lebt an oder unter der absoluten Armutsgrenze. Eine hohe Arbeitslosenrate und eine extrem hohe Auslandsverschuldung kennzeichnen die wirtschaftliche Situation. Honduras profitiert von dem 2005 beschlossenen internationalen Schuldenerlass. Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Honduras Platz 96 von 137 Ländern (Stand 2017–2018). Im Index der Wirtschaftlichen Freiheit belegt das Land 2017 Platz 100 von 180 Ländern.

Den Empfehlungen der internationalen Gebergemeinschaft entsprechend investiert die honduranische Regierung im Rahmen ihrer nationalen Armutsbekämpfungsstrategie verstärkt in den sozialen Bereich. Nahezu 50 % der Ausgaben des Haushalts 2005 entfallen auf Erziehung, Gesundheit, Sozialversicherung, sozialen Wohnungsbau und ländliche Entwicklungsprogramme (d. h. auf die Umsetzung der nationalen Armutsbekämpfungsstrategie). Die Wirtschaftspolitik der Regierung steht im Zeichen einer engen Zusammenarbeit mit den bilateralen und multilateralen Gebern und den internationalen Finanzinstitutionen. Honduras ist eine liberale, auf Freihandel und Investitionserleichterungen ausgerichtete Marktwirtschaft (Lohnveredelungsregime, Freihafenregelungen, Zoll- und Steuervergünstigungen für Investoren, freier Kapitaltransfer). Insbesondere die Maquilagesetzgebung nach mexikanischem Vorbild hat in den vergangenen Jahren zahlreiche Investoren insbesondere aus den USA, Taiwan und Südkorea ins Land gebracht. Die Textil- und Holzverarbeitung konzentriert sich auf San Pedro Sula in der Nähe der Karibikküste, die zweitgrößte Stadt und Wirtschaftszentrum des Landes. Der Sektor beschäftigt insgesamt rund 130.000 Personen und hat 2004 830,7 Mio. US-Dollar erwirtschaftet.

Die makroökonomischen Daten zeigen, dass sich die honduranische Wirtschaft von den Folgen des Wirbelsturms weitgehend erholt hat. Die bis zu 70 % zerstörten Kulturen der beiden großen Exporterzeugnisse (Kaffee und Bananen) produzieren inzwischen wieder auf dem Niveau vor der Katastrophe. Das Bruttoinlandsprodukt (BIP) wies für 2004 eine Wachstumsrate von 5,0 % auf und lag damit deutlich über dem Bevölkerungswachstum (2,5 %). Die Inflationsrate betrug 9,2 %. Problematisch sind für Honduras die niedrigen Weltmarktpreise für Kaffee, die Marktzugangsbeschränkungen für Bananen und insbesondere die Ölpreiserhöhungen des Jahres 2004.

Die honduranische Währung heißt Lempira, benannt nach dem indianischen Freiheitskämpfer und Nationalhelden, der sich bis zu seinem Tod 1537 gegen die spanischen Invasoren aufgelehnt hatte.
100 Centavos sind ein Lempira. Ein erheblicher Teil der honduranischen Wirtschaft ist dollarisiert, d. h., dass Geschäfte ab einer bestimmten Höhe (etwa Mietverträge und ein großer Teil der Sparkonten) in US-Dollar abgewickelt werden. Anders als im offiziell dollarisierten Nachbarland El Salvador oder auch in Nicaragua kann man aber bei Kleingeschäften, etwa an der Supermarktkasse oder bei einer Taxifahrt, nicht mit Dollar bezahlen - mit Ausnahme der Touristenzentren der Islas de la Bahía, wo der Dollar im Wesentlichen als Zweitwährung fungiert.

Ein Drittel der Landesfläche könnte landwirtschaftlich genutzt werden; tatsächlich sind es nur etwa 12 %. Während viele Großgrundbesitzer riesige Flächen brach liegen lassen, haben die meisten Kleinbauern zu wenig Land, um genügend Nahrungsmittel für ihre eigenen Familien und darüber hinaus für die rasch wachsende Stadtbevölkerung anbauen zu können. Eine in den Jahren 1974 bis 1978 eingeleitete Landreform sollte 120.000 Familien mit Land versorgen. Der Widerstand der Großgrundbesitzer und Verwaltungsprobleme verzögerten aber die Durchführung. Am Ende wurde ihre Zielvorgabe um die Hälfte verfehlt; ein Drittel der Bevölkerung muss sich weiterhin als Wanderarbeiter durchschlagen. Erfolgreicher war das Bemühen der Entwicklungsplaner, die außenwirtschaftlich gefährliche Monokultur der Banane zu überwinden.

Der Viehbestand umfasst vorwiegend Rinder und Schweine. Geflügel wird hauptsächlich für den Eigenverbrauch gezüchtet.

Die Volkswirtschaft ist auf den Agrarsektor ausgerichtet, in dem 15 % des BIP erzeugt werden. Dabei sind jedoch nicht mehr Bananen die Hauptanbauprodukte, sondern Kaffee und Krustentiere. Die meisten Familienbetriebe produzieren Mais und Bohnen für den Eigenbedarf. Auf den wenigen Großplantagen, die überwiegend in US-amerikanischem Besitz sind, werden Bananen und Kaffee für den Export angebaut.

Die Forstwirtschaft ist ein bedeutender Wirtschaftszweig des Landes. Allerdings schreitet dadurch auch die Zerstörung der üppigen tropischen Regenwälder in Honduras zusehends voran. Allein 1998 rodete die holzverarbeitende Industrie 6,92 Millionen Kubikmeter Holz. Programme zur Wiederaufforstung werden durch schonungslose Rodungsmethoden und eine mangelhafte Verkehrsinfrastruktur behindert. Wertvolle Holzarten sind Kiefer, Mahagoni, Ebenholz, Walnuss und Rosenholz. Der Fischfang umfasst hauptsächlich Schalentiere.

Weitere 50 Prozent des BIP entfallen auf den Handel und andere Dienstleistungen. In den letzten Jahren gab es ein leichtes Realwachstum, doch der mittelständischen Wirtschaft geht es weiterhin schlecht. Einen Gewinn können fast ausschließlich ausländische Unternehmen verbuchen, die aufgrund der geringen Mindestlöhne und vielen Arbeitslosen Fabriken in Honduras bauen. Eine wachsende Einnahmequelle ist der stetig wachsende Tourismus. Besonders beliebt ist die große Ausgrabungsstätte von jahrtausende alten Maya-Ruinen in Copán (Ruínas de Copán) und den "Islas de la Bahía" (Bay Islands), einer Inselkette im Norden des Landes. Sie besteht hauptsächlich aus den drei vorgelagerten Karibikinseln Utila (die kleinste), Guanaja und Roatán, die größte und gleichzeitig beliebteste unter ihnen. Die Touristenzahlen steigen jährlich, weiteres Wachstum in diesem Bereich wird erwartet. Im Jahr 2004 empfing Honduras über eine Million Besucher und erzielte damit einen Umsatz von 400 Millionen US-Dollar.

Honduras hat erste Schritte zur Industrialisierung unternommen – allerdings nicht aus eigener Kraft, sondern mit Hilfe von Auslandsunternehmen und Auslandskapital, in erster Linie aus den USA. US-Konzerne beherrschen alle gewinnversprechenden Industrie- und Dienstleistungsbranchen. Die einheimischen Klein- und Mittelbetriebe, die vorwiegend Erzeugnisse der Landwirtschaft verarbeiten, erwirtschaften mit ihrer niedrigen Produktivität nur etwa 40 % der Industrieproduktion, beschäftigen aber fast zwei Drittel der Industriearbeiter. Honduras war dem Konkurrenzdruck innerhalb des Zentralamerikanischen Gemeinsamen Marktes nicht gewachsen und nutzte den Fußballkrieg mit El Salvador (1969) als Vorwand zum Ausscheiden. Die Industrialisierung hat bislang wenig zum Abbau der hohen Arbeitslosigkeit beigetragen.

Der Schwerpunkt der schwach entwickelten Industrie liegt in der Verarbeitung von Agrarprodukten. Dringend benötigte Devisen bringen die Überweisungen der etwa 1 Mio. im Ausland lebenden Honduraner und der Tourismus.

Honduras besitzt reiche Vorkommen an Silber, Zink und Blei. Weitere, größtenteils nicht geförderte Bodenschätze sind Eisenerz, Kohle, Kupfer und Antimon.

Die reelle Arbeitslosigkeit liegt bei 40 Prozent und der gesetzliche Mindestlohn beträgt je nach Beschäftigung 4055 L bis 5500 L. Damit liegt der Monatsmindestlohn bei ungefähr 160 bis 220 Euro.

Aufgrund der fehlenden binnenwirtschaftlichen Impulse ist keine Besserung der Wirtschaft oder Verminderung der Armut in Sicht.

Bedeutende Gewerkschaften sind die Confederación de Trabajadores de Honduras,
Confederación General de Trabajadores und die Confederación Unitaria de Trabajadores de Honduras. Die Gewerkschaften stellen keine geschlossene einheitliche Kraft dar. Bedeutendster gewerkschaftlicher Dachverband ist die „Confederación de Trabajadores de Honduras“, die sich um modernes Management und internationale Zusammenarbeit bemüht. Einzelne Gewerkschaften haben merkbares Gewicht, so die Gewerkschaft der Bananenarbeiter und die Vereinigung der Lehrkräfte und Angestellten im Gesundheitswesen.

Trotz der Exportorientierung der honduranischen Außenwirtschaft weist die Handelsbilanz einen negativen Saldo aus. Honduras exportierte 2004 Waren im Wert von 1.580,5 Mio. US-Dollar und importierte Waren im Wert von 3.678,5 Mio. US-Dollar. Die Dienstleistungsbilanz ist dank des expandierenden Tourismussektors dagegen positiv. Ein wichtiger Wirtschaftsfaktor sind schließlich die unentgeltlichen Transfers, d. h. Überweisungen der rund 1 Mio. im Ausland lebenden Honduraner (2004: 1.300 Mio. US-Dollar, d. h. ca. 10 % des BIP). Die Quote der internationalen Währungsreserven im Vergleich zur Außenverschuldung betrug 2004 rund 30 %. Die 2005 zu erwartende Mittelfreisetzung im Rahmen der Schuldenerlassinitiative zugunsten der am höchsten verschuldeten armen Entwicklungsländer (HIPC) wird diese Situation verbessern.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 4,38 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 3,98 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 1,9 % des BIP.
Die Staatsverschuldung betrug 2016 9,7 Mrd. US-Dollar oder 45,4 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Das Straßennetz ist rund 13.603 Kilometer lang, davon sind 20 Prozent (ungefähr 2775 km) befestigt. Die Panamericana (160 Kilometer in Honduras) verbindet das Land mit Nicaragua, Guatemala und El Salvador. Wetterfeste Straßen führen von Tegucigalpa nach San Pedro Sula, Puerto Cortés, La Ceiba und den wichtigsten Städten an der Karibikküste und am Golfo de Fonseca im Süden. Von Fahrten nach Einbruch der Dunkelheit wird abgeraten. Abseits der Überlandstraßen ist die Benutzung von geländegängigen Fahrzeugen ratsam. Fernbusse, die Hauptverkehrsmittel, verkehren regelmäßig zwischen den größeren Städten. Rechtzeitige Buchung wird empfohlen, die Fahrpreise sind sehr günstig. Taxis haben keine Taxameter, innerhalb der Städte gelten Einheitstarife. Sie sind nicht immer in sicherem Zustand. Auf längeren Strecken sollte man den Fahrpreis im Voraus vereinbaren. Es gibt auch Sammeltaxen, sogenannte Colectivos.

Das Eisenbahnnetz in Honduras, das hauptsächlich für den Bananentransport aufgebaut wurde, erstreckt sich über fast 700 Kilometer entlang der Nordküste. Es gibt nur drei Bahnstrecken im Norden des Landes. Besucher konnten auf einem Bananenzug von San Pedro Sula reisen und in La Ceiba in einen Touristenzug umsteigen. Auch diese letzte Strecke ist schon seit 2007 nicht mehr in Betrieb. Jedoch bemüht man sich seit 2010 um einen Wiederaufbau. Eine erste Teilstrecke innerhalb San Pedro Sula wurde wieder in Betrieb genommen.

Der gebirgige Charakter des Landes macht das Flugzeug zu einem wichtigen Transportmittel. Drei internationale Flughäfen und über 100 kleine Flugplätze sind in Betrieb. Die Inlandsfluggesellschaften Isleña Airlines, Aero Honduras, Atlantic Honduras und Sosa Airlines verbinden Tegucigalpa täglich mit den Provinzstädten des Landes. Die anderen größeren Flughäfen sind San Pedro Sula (SAP), La Ceiba (LCE) und Roatan Island Airport (RTB). Isleña Airlines und Sosa Airlines bieten Flüge auf die Insel Utila vor der Karibikküste an. Es gibt über 30 Flugplätze für Geschäfts- und Charterverkehr. Auch abgelegene Regionen werden regelmäßig von Leichtflugzeugen angeflogen.

Der Yojoasee und eine Reihe von Flüssen sind schiffbar. Die Länge der schiffbaren Wasserstraßen beträgt 465 km. Fähren verkehren zwischen den Häfen an der Karibik- und Pazifikküste. Einzelheiten kann man von den Hafenbehörden erfahren. Von La Ceiba und Puerto Cortés gibt es mehrmals wöchentlich Verbindungen zu den Islas de la Bahía. Vereinbarungen müssen mit den Bootseigentümern getroffen werden.

Die Landesvorwahl von Honduras ist 00504. Weitere Vorwahlen (für Städte oder Regionen) gibt es nicht. Die Festnetznummern einer Region beginnen jedoch immer mit den gleichen Ziffern (z. B. Tegucigalpa mit 23). Mobiltelefon-Nummern beginnen mit einer 9 (tigo), einer 8 (digicell) oder einer 3 (claro).
In Honduras gibt es ungefähr dreihunderttausend Telefonanschlüsse und etwas mehr Mobiltelefone. Internetzugänge gibt es landesweit etwa 170.000.

Die staatliche monopolistische Telefongesellschaft Empresa Hondureña de Telecomunicaciones (Hondutel) ist der einzige Anbieter von Telefonleitungen. In den touristisch erschlossenen Gebieten wie den Islas de la Bahía und der Region um Tela und La Ceiba gibt es mehrere lokale Anbieter von satelliten- und funkgestützten Internetanbindungen. Öffentliche Telefone sind oft Kartentelefone. In manchen Orten gibt es auch öffentliche Telefonzentralen von Hondutel, in denen man Faxe empfangen und senden, sowie Anrufe absetzen und annehmen kann. Ein Internetanbieter ist NetSys. In größeren Städten und touristischen Regionen gibt es zahlreiche Internetcafés.

Die Bemühungen um eine Privatisierung des Festnetzsektors sind bislang gescheitert, da die potenziellen Käufer vor den enormen Investitionssummen zurückschrecken, die nötig wären, um die bislang staatliche Telefongesellschaft Hondutel rentabel zu machen. Das Staatsunternehmen betreibt derzeit ca. 300.000 Festnetzanschlüsse; ca. 400.000 potentielle Kunden stehen auf der Warteliste. Die Wartezeiten bis zur Verlegung und Freischaltung eines Anschlusses betragen durchschnittlich drei Jahre, in vielen Fällen bis zu sieben Jahren. (Durch die Zahlung eines entsprechenden Betrages an den jeweiligen Hondutel-Angestellten lässt sich die Zeit jedoch auf zwei Wochen verkürzen). Kunden, die es sich leisten können, weichen auf die Dienste der beiden privaten Mobiltelefonanbieter Airtel Africa und Megatel aus. Von öffentlichen Telefonzellen aus, die von Hondutel betrieben werden, kann man keine Handys anrufen. Für Telefonate ins Ausland, insbesondere in die USA und nach Europa, greifen viele Honduraner auf die Möglichkeit der sehr preiswerten Internettelefonie zurück, die fast in jedem Internetcafé angeboten wird.

2016 nutzten 21,5 % der Bevölkerung das Internet.

Luftpost nach Europa benötigt vier bis sieben Tage. Öffnungszeiten der Postämter sind Montag bis Samstag 8.00 bis 12.00 Uhr und 14.00 bis 18.00 Uhr. Die Situation der staatlichen Post Honducor ist problematisch. Aufgrund ihres schlechten Services (lange Transportzeiten und hohes Risiko, dass Sendungen verloren gehen) ist sie in Finanznöten, die 2002 beinahe zur Schließung geführt hätten. Wer es sich leisten kann, schickt private Kuriere.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Honduras Platz 140 von 180 Ländern. Laut dem Bericht der Nichtregierungsorganisation ist Situation der Pressefreiheit im Land „schwierig“.

Im Jahr 2017 ist ein Journalist in Honduras getötet worden. Laut dem Bericht von Reporter ohne Grenzen steht der Tod des Opfers in direktem Zusammenhang mit der journalistischer Tätigkeit.

Die Presse- und Informationsfreiheit ist in Honduras zwar nicht durch staatliche Eingriffe, wohl aber durch die oligopolartige Besitzstruktur der Medienorgane erheblich eingeschränkt. Es gibt in Honduras etwa 100 private Radio- und 9 private Fernsehsender.

Die Besitzer der landesweiten Tageszeitungen, Radio- und Fernsehsender sind entweder selbst Politiker (Ex-Präsident Carlos Flores besitzt u. a. die Tageszeitung El Tiempo) oder in ihren Interessen mit diesen eng verwoben (weitere Tageszeitungen mit landesweiter Verbreitung sind El Heraldo, La Prensa und La Tribuna). Journalisten werden durch Korruption oder Repressalien (z. B. Entlassung) gefügig gemacht. Eine Gruppe kritischer und unabhängiger Journalisten setzt sich unter dem Namen C-Libre auf politischer und gesellschaftlicher Ebene für mehr Presse- und Informationsfreiheit ein und betreibt die Internet-Zeitung ConexiHon.

Außerdem gibt es die Wochenzeitungen "Tiempos del Mundo" und die englischsprachige "Honduras this week".

Mit Ausnahme einiger abgelegener indigener Siedlungen ist die Kultur von Honduras hauptsächlich spanisch geprägt. Sowohl in Tegucigalpa als auch in der früheren Hauptstadt Comayagua überwiegen koloniale Bauten. Das Herzstück der kolonialen Architektur ist die barocke Kathedrale von Comayagua aus dem 18. Jahrhundert. Das bedeutungsvollste indianische Erbe ist der Ruinenkomplex von Copán, eine Tempelanlage aus der spätklassischen Epoche.

Musikalisch ist Honduras vom Neben- und Miteinander der verschiedenen ethnisch-kulturellen Identitäten geprägt. Der im Radio und Diskotheken zu hörende „Mainstream“ enthält die in Lateinamerika übliche Mischung aus spanisch- und englischsprachigem Pop und Rock und die jeweils dominierenden lateinamerikanischen Musikrichtungen von Merengue über Salsa bis zu Reggaeton und Bachata. Bei privaten und öffentlichen Festlichkeiten sind die traditionelleren und spezifisch honduranischen Musikstile beliebt, allen voran die trommelbetonte und äußerst temporeiche Puntamusik der Garífuna. Der aus dem Küstenstädtchen La Ceiba stammende Liedermacher Guillermo Anderson verbindet in seinen Songs moderne Rock-, Pop- und Reggae-Elemente mit Einflüssen der Punta und sozialkritischen Liedtexten. Auf Dorffesten und anderen Feierlichkeiten im Hochland ist, besonders bei älteren Menschen, die Musik von Marimba-Orchestern sehr beliebt. Diese kleinen Combos bestehen oft aus Musikern im Seniorenalter, die, was Virtuosität und persönliche Ausstrahlung betrifft, durchaus mit ihren kubanischen Kollegen des Buena Vista Social Club vergleichbar sind.

Die Marimba ist das populärste Musikinstrument; sie wird vorwiegend im nördlichen Küstengebiet gespielt und kommt bei afrokaribischer Folklore zum Einsatz.

Literarisch kann Honduras nicht mit einer breiten Palette erfolgreicher Werke oder Autoren aufwarten. Das mag am allgemein niedrigen Bildungsniveau liegen und daran, dass die gebildete Mittelschicht, die in vielen Gesellschaften die meisten Leistungsträger der „Hochkultur“ (Malerei, Literatur usw.) stellt, in Honduras verschwindend klein ist. Außerdem wird die Lesekultur durch einen für die allermeisten Honduraner aus ökonomischen Gründen nicht zugänglichen Buchmarkt stark behindert. Ins Deutsche wurden nur wenige Werke übersetzt.

Juan Ramón Molina (1875–1908) gilt als einer der bedeutendsten lateinamerikanischen Dichter des 19. Jahrhunderts. Sein berühmtestes Werk ist „Tierras, Mares y Cielos“. Bis in die 1930er Jahre dominierten romantische und modernistische Lyrik und Prosa. Der Begründer des sozialen Realismus und zugleich der bekannteste honduranische Schriftsteller war Ramón Amaya Amador (1916–1966). Sein erster und bekanntester Roman „Prisión Verde“ (zuerst erschienen 1945; 1958 auf Deutsch als „Das grüne Gefängnis“ in der DDR erschienen) schildert das Elend der Arbeiter auf den Bananenplantagen der United Fruit Company im Norden des Landes und beruht auf eigenen Erfahrungen des Autors. Ebenfalls ins Deutsche übersetzt und in der DDR veröffentlicht wurden seine Romane „Aufstand in Tegucigalpa“ („Constructores“), der vom Kampf der Bauarbeiter für eine gewerkschaftliche Organisation handelt, und „Morgendämmerung“ („Amanecer“), der vom Aufstand des Jahres 1944 in Guatemala, der zeitweiligen Wahlheimat des Autors handelt.

Roberto Sosa (1930–2011) hat mehrere Bände mit sozialkritischer Lyrik und Prosa verfasst: „Los pobres“ (wofür er 1969 den spanischen "Premio Adonaís des Poesía" erhielt), „Un mundo para todos dividido“ und „Prosa armada“.

Weitere bedeutende honduranische Schriftsteller sind die Lyriker Óscar Acosta (1933–2014) und José Adán Castelar (* 1941), der Schriftsteller und Herausgeber Rigoberto Paredes (* 1948), der Erzähler, Essayist und Jugendbuchautor Eduardo Bär (* 1940). Juan Pablo Suazo Euceda ist Agraringenieur und behandelt in mehreren seiner Bücher das Leben in der Provinz La Mosquitia.

Das erfolgreichste Werk der honduranischen Gegenwartsliteratur, der Roman „Big Banana“ stammt von Roberto Quesada (* 1962) und behandelt das Leben der Exilanten in New York. Sein Werk „El humano y la diosa“ (1996) erhielt den "Premio des instituto latinoamericano de escritores" in den USA.

Die Grundnahrungsmittel der Honduraner sind Mais, Reis und Bohnen. Mais gibt es vor allem in Form von flachen Tortillas (die ausschließlich aus Maismehl und Wasser hergestellt werden) und ist Bestandteil so gut wie jeder Mahlzeit. Ein typisches honduranisches Frühstück besteht aus Tortillas, Bohnenmus (frijoles fritos) und Rühr- oder Spiegelei, evtl. kommen noch einige Scheiben gekochter oder frittierter Kochbanane (plátano) dazu. Eine besondere Form der Tortilla ist die "Pupusa", eine dickere Maistortilla, die mit Wurststückchen oder Käse gefüllt ist.

In einigen Restaurants bekommt man als Vorspeise ein so genanntes Anafre: Auf dem Tisch des Gastes wird ein spezielles Tongefäß platziert, in dem glühende Kohlestückchen Käsestreifen in Bohnenmus zerschmelzen lassen. Die entstehende Masse wird dann mithilfe von knusprig frittierten Tortilla-Stückchen („tostadas“) zum Mund geführt. Für europäische Gaumen ungewohnter ist das Chili, das in Form von Saucen oder in Essig eingelegten Chilischoten Bestandteil vieler Gerichte ist. Die traditionelle "sopa de mondongo" wird mit Rindsinnereien zubereitet.

Eine weitere Suppenspezialität ist die "sopa de caracol", eine Suppe mit in Streifen geschnittenen Riesenmeeresschnecken (deren Gehäuse von den Garífuna als Musikinstrument in der Puntamusik verwendet wird, da man durch Hineinblasen einen lauten, sehr tiefen Ton erzeugen kann). An der Nordküste, insbesondere in den afrohonduranischen Gemeinschaften, wird viel mit Kokos (-milch, -raspeln usw.) gekocht. Eine besondere Spezialität der garífunas ist casabe, eine Art Fladen aus Maniok.

Fußball ist Nationalsport; dreimal gelang der Nationalmannschaft bislang die Qualifikation für die Endrunde einer Fußball-WM (1982, 2010 und 2014). Baseball, Basketball, Boxen und Bowling sind ebenfalls sehr beliebt.

Folgende Feiertage werden in Honduras begangen:

Während der Karwoche (Semana Santa) schließen die Geschäfte von Mittwoch bis Sonntag. Alle Dörfer und Städte haben eigene Feiertage zu Ehren ihrer jeweiligen Schutzheiligen. Außerdem werden regionale Feste veranstaltet, wie der Karneval von La Ceiba im Mai. Im Alter von 15 Jahren feiern die Mädchen "La Fiesta Rosa", bei der sie als vollwertige Mitglieder in die soziale Gemeinschaft aufgenommen werden. Im Rahmen aufwendiger Feiern wird ihre Reife zur Frau gewürdigt.




</doc>
<doc id="9554" url="https://de.wikipedia.org/wiki?curid=9554" title="Kathedrale Notre-Dame de Paris">
Kathedrale Notre-Dame de Paris

Die katholische Kirche Notre-Dame de Paris („Unsere Liebe Frau von Paris“) ist die Kathedrale des Erzbistums Paris. Die Unserer Lieben Frau, also der Gottesmutter Maria geweihte Kirche wurde in den Jahren von 1163 bis 1345 errichtet und ist somit eines der frühesten gotischen Kirchengebäude Frankreichs. Ihr Name lautet auf Französisch "Cathédrale Notre-Dame de Paris", oft einfach nur Notre-Dame. Ihre charakteristische Silhouette erhebt sich im historischen Zentrum von Paris auf der Ostspitze der Seine-Insel Île de la Cité im 4. Pariser Arrondissement.

Die beiden Türme sind 69 Meter hoch, der Dachreiter 96 Meter. Das Kirchenschiff ist im Inneren 130 Meter lang, 48 Meter breit und 35 Meter hoch; es bietet bis zu 10.000 Personen Platz.

Der Bau der heutigen Kathedrale begann zu der Zeit des Übergangs von der Romanik zur Gotik und erstreckte sich über annähernd 200 Jahre. Er ist dadurch gekennzeichnet, dass in weitgehend noch romanischem Stil mit dem Chor begonnen wurde, mit dem Fortschreiten des Baus nach Westen zunehmend technische Möglichkeiten und Stilmittel der Gotik eingesetzt wurden, nach achteinhalb Jahrzehnten das Bauwerk so gut wie fertig war und das nächste Jahrhundert mit gotischer Umgestaltung, Erweiterung und Ausstattung älterer Bauteile verging.

Die Kathedrale ersetzt einen Vorgängerbau, der unter der Herrschaft des fränkischen Königs Childebert I. (König von 511 bis 558) in den Jahren um 540/550 entstanden und als "Cathédrale St. Etienne" (Stefansdom) bekannt war. Damit ist ihr Standort – nach denen des Panthéons und einer von Gregor von Tours erwähnten Begräbniskapelle bei der damaligen Nekropole Saint-Marcel – einer der ältesten unter den bekannten christlichen Gebetsstätten innerhalb der heutigen Pariser Stadtgrenzen.

Der Bau des Chores und seiner zwei Umgänge wurde 1163 unter Bischof Maurice de Sully und Ludwig VII. begonnen. Nach Fertigstellung wurde der Chor 1182 geweiht.

In der zweiten Bauphase wurde bis 1190 das mittlere Drittel des Kirchenschiffs gesetzt, bestehend aus der Vierung mit Querschiff, den drei davor liegenden Jochen des Hauptschiffs und jeweils zwei Jochen der beiden nördlichen und der beiden südlichen Seitenschiffe. Das Schiff hatte zunächst keinen Abschluss nach Westen.

In der dritten Bauphase von 1190 bis 1225 wurden die unteren Geschosse der Westfassade und das vordere Drittel des Kirchenschiffs errichtet, bestehend aus dem vordersten Joch des Hauptschiffs mit den unteren Geschossen der Türme vor den Seitenschiffen und dem zweiten Joch des Hauptschiffs mit den ersten beiden Jochen aller vier Seitenschiffe. Die ersten 18 Jahre davon vergingen mit dem Setzen der Fundamente. Ab 1208 wurde das Erdgeschoss der Westfassade mit den drei großen Portalen hochgemauert und ausgeschmückt. Ab 1218 wurden die ersten beiden Joche des Kirchenschiffs hochgezogen, die zur Standsicherheit der Fassade gebraucht wurden. 1220 bis 1225 entstand dann das Rosengeschoss der Turmfront mit der westlichen Fensterrose. Damit war das Kirchenschiff in ganzer Länge gebaut.

Während der vierten Bauphase von 1225 bis 1250 wurden die Turmgeschosse errichtet. Aus dieser Zeit sind Änderungen des Bauplans und erste Umbauten dokumentiert. Ab etwa 1230 wurden die Schrägdächer der Seitenschiffe durch flache Terrassen ersetzt, was größere Fenster der Obergaden des Hauptschiffs ermöglichte. An die Seitenschiffe wurden zwischen den Strebepfeilern Kapellen angebaut. Nach der Fertigstellung des Südturms 1240 wurde noch im selben Jahr beschlossen, den Türmen keine Spitzen aufzusetzen. Mit dem Abschluss des Nordturms 1250 war die Kathedrale faktisch fertiggestellt und funktionstüchtig.

Inzwischen nahm man Anstoß daran, dass früh gebaute Teile wie die Fassaden des Querschiffs noch in romanischem Stil errichtet worden waren und zu der modernen gotischen Westfassade kontrastierten. Darum wurde das Querschiff teilweise wieder abgebrochen und durch Jean de Chelles ab 1250 nach Norden und anschließend nach Süden verlängert. Er schuf noch die neue, gotische Nordfassade des Querhauses.

Dessen neue Südfassade schuf sein Nachfolger, der auch an der Errichtung der Sainte-Chapelle beteiligte Pierre de Montreuil. Dann begann er, die romanischen Strebepfeiler des Chors durch gotische zu ersetzen.

Der nächste Baumeister, Pierre de Chelles, errichtete den Lettner und begann 1296, den doppelten Chorumgang mit einem Kranz von Kapellen zu versehen.

Jean Ravy war Baumeister von 1318 bis 1344. Er vollendete die letzten Seitenkapellen des Chorumgangs und baute die elegantesten Strebepfeiler des Chores. Im Innenraum begann er mit der Gestaltung der Chorschranken.

Sein Neffe Jean le Bouteiller leitete die Arbeiten von 1344 bis 1363.

Dessen Nachfolger Raymond du Temple brachte die Bauarbeiten zum Abschluss. Er vollendete vor allem die Chorschranken.

Im Zeitalter der Aufklärung wurden im Jahr 1728 die Buntglasfenster durch weiße Glasfenster ersetzt und die Wände weiß übertüncht. In den folgenden Jahrzehnten wurde ein Großteil der Figuren an den Türmen entfernt.

1793 stürmten die Verfechter der Revolution das Gotteshaus und zerstörten die Inneneinrichtung, deren metallene Gegenstände im Hôtel des Monnaies eingeschmolzen wurden. Im Gegensatz zu zahlreichen französischen Klöstern wurde die Kirche nicht abgerissen, aber entweiht und zum Tempel des "höchsten Wesens", der Vernunft, erklärt. Später diente sie als Weindepot.

Nach der Unterzeichnung des Konkordates von 1801 gestattete Napoléon I. im Jahr 1802 die erneute liturgische Nutzung der Kathedrale, bevor er sich zwei Jahre später hier zum Kaiser krönte. Am 27. Februar 1805 wurde die Kirche durch Papst Pius VII. zur ersten französischen Basilica minor erhoben. Doch auch das konnte den durch die Revolution begonnenen Verfall nicht aufhalten. Zudem verwüsteten während der Julirevolution von 1830 Aufständische den an die Kirche angrenzenden erzbischöflichen Palast und die Schatzkammer.

Erst Victor Hugos 1831 erschienener Roman "Der Glöckner von Notre-Dame" rückte die Schönheit des Gebäudes wieder ins Blickfeld und trug zu der 1844 getroffenen Entscheidung für eine umfassende Restaurierungskampagne unter der Leitung von Eugène Viollet-le-Duc bei, die erst zwanzig Jahre später zum Abschluss kam. Unter anderem wurden die beschädigten oder fehlenden Skulpturen ersetzt und ein neuer Dachreiter errichtet, der nun 90 m in die Höhe ragt. Im Jahr 1858 wurden im Zuge der Restaurierung der erzbischöflichen Grabkammer weitere Gräber freigelegt.

Vor einigen Jahren wurden Westfassade und Türme erneut restauriert, so dass sie heute wieder hell erstrahlen.

Die Errichtung der Westfassade begann 45 Jahre nach der Weihe des Chores und in völlig anderem Stil als die Fassaden des Querschiffes, das infolgedessen weitere 40 Jahre später grundlegend modernisiert wurde. Vollendet wurde sie um 1250, also ungefähr zu der Zeit, als die Gotik in Deutschland einsetzte.

Die Vollendung der Fassade der Basilika von Saint-Denis 1137 liegt zu diesem Zeitpunkt bereits 60 Jahre zurück und die der Kathedrale von Laon 1190 immerhin schon zehn Jahre. Aber Paris greift die Strömungen nicht auf, sondern bietet wieder das traditionelle kastenförmige Aussehen mit den betonten waagerechten Elementen, allerdings in einem sehr ausgewogenen und genau berechneten Verhältnis.

Die Proportionen der Fassade beruhen auf mehreren Ineinanderverschränkungen von Quadraten. Damit wird ein Rechteck ungefähr im Seitenverhältnis 2:3 aufgebaut. Hier wurde das Ideal des heiligen Augustinus verwirklicht: eine Architektur, deren Proportionen auf musikalischen Konsonanzen beruhten, die ihrerseits die harmonische Ordnung des Universums widerspiegeln.

Das Mittelportal ist im Vergleich zu den Seitenportalen nur geringfügig hervorgehoben. So ist Notre-Dame rückschrittlich und erinnert fast an die normannische Fassade der Klosterkirche Saint-Étienne in Caen von 1060, also 140 Jahre zuvor.
Andererseits wurde in Paris eine entscheidende Neuerung der Fassadengestaltung eingeführt, nämlich die Königsgalerie über der Portalzone als Sinnbild der Vereinigung von Kirche und Monarchie. Die 28 Figuren stellen die Könige von Juda dar. Schon im 13. Jahrhundert hielt das Volk sie für die Könige Frankreichs. Diese Reihung von überlebensgroßen Königsstatuen wurde in einigen der bedeutendsten Kathedralen nach Paris übernommen, so in Reims und Amiens. Die Königsfiguren der Pariser Kathedrale sind jedoch allesamt neuzeitliche Interpretationen. Ihre Originale – die ja für das Volk gemeinhin den Herrschaftsanspruch der französischen Könige darstellten – wurden wie viele Kunstwerke in Notre-Dame während der Französischen Revolution zerstört. Zwei der Königsfiguren weisen die Gesichtszüge von Eugène Viollet-le-Duc und Jean-Baptiste Lassus auf, welche die Kathedrale im 19. Jahrhundert restauriert haben.

Notre-Dame verfügt über bedeutende Figurenportale sowohl an der Westfassade als auch an den Querhäusern. Die drei Portale der Westfassade sind im 19. Jahrhundert stark restauriert worden und bestehen nur noch zum geringen Teil aus originaler Substanz. Als man im Zuge der Französischen Revolution Notre-Dame in einen Tempel der Vernunft umweihte, wurden die meisten Darstellungen zerstört oder schwer beschädigt. Da das ursprüngliche Programm und Aussehen jedoch bekannt waren, haben sich die Restauratoren weitgehend an den mittelalterlichen Zustand gehalten. Das südliche der drei Westportale, das Portail de la Vierge, ist das älteste. Es entstand etwa um 1200 und ist der Jungfrau Maria geweiht. Im Tympanon, dem Giebelfeld über dem Portal, ist die sogenannte Marienkrönung dargestellt. Der Türsturz darunter zeigt die von Christus erweckte Maria, die, in Anwesenheit der 12 Apostel, von zwei Engeln aus ihrem Sarg gehoben wird. Der unterste Teil ist mit Prophetendarstellungen verziert. Das zentrale Westportal, das Portal des Jüngsten Gerichts, ist etwas später als das Portal de la Vierge entstanden. Portalgestaltungen zum Jüngsten Gericht waren in der mittelalterlichen Gotik weit verbreitet und sind auch bei anderen bedeutenden Kathedralen zu finden. Zuoberst im Tympanon ist Christus als Weltenrichter dargestellt. Direkt unter Christus ist ein Engel mit einer Waagschale zu sehen, der die Seelen der Verstorbenen aufwiegt. Unmittelbar neben ihm steht ein Teufel, der mit dem Engel darum streitet, welche Toten in die Hölle (rechts vom Teufel) und welche in den Himmel (links vom Engel) eintreten. Das Portail de Sainte Anne ist das jüngste der drei Westportale (etwa um 1230) besitzt jedoch die ältesten Elemente, da der Architekt ein Tympanon sowie einen Türsturz des 12. Jahrhunderts verwendete. Wie das Portail de la Vierge steht es im Zeichen der Jungfrau Maria, die in der Mitte des Tympanons thront und das segnende Jesuskind auf ihrem Schoß hält.

Eine für die Geschichte der gotischen Architektur ganz entscheidende Erfindung hat man lange den Baumeistern von Notre-Dame, Pierre de Montreuil und Jean de Chelles, zugeschrieben: das Strebewerk von 1180/1200. Es ist bis jetzt nicht gesichert, an welchem Bauwerk das offene Strebewerk zum ersten Mal angewendet worden ist. Nachträgliche Vergrößerungen der Fensterzonen, Bauschäden oder Restaurierungen haben den ursprünglichen Zustand vieler früherer Strebewerke verunklärt. Hier in Paris waren anfangs die Streben noch unter den Emporendächern eingebaut.

Zwischen 1160 und 1180 entstanden die ersten über den Seitenschiffdächern hinaufsteigende Strebebögen und das möglicherweise nicht hier bei der Notre-Dame, sondern einige hundert Meter weiter bei St. Germain-des-Prés. Das Strebesystem hier der Notre-Dame ist demnach später vergrößert und erhöht worden. Genaue Daten liegen hier allerdings wie gesagt nicht vor.

Möglicherweise sind die Pariser Strebebögen erst "nach" denen von Bourges und Chartres – nach 1200 – entstanden, denn Notre-Dame de Paris hatte anfangs keine Gewölbe, sondern eine Holzdecke, weshalb es die Probleme mit dem Seitenschub kaum gab. Der erste Architekt der Pariser Kathedrale wusste noch nicht, wie er ein so hohes Gewölbe abstützen konnte und ließ das Problem für spätere Generationen offen. Erst der zweite Architekt konnte auf den Erfahrungen an anderer Stelle aufbauen, zog das Gewölbe ein und stützte es außen mit dem offenen Strebewerk ab. Wegen des vergleichsweise früheren Baubeginns der gesamten Kathedrale 1163 hat man lange geglaubt, das Strebewerk sei überhaupt hier erfunden worden.

Bis dahin hatte man in der Architektur noch versucht, den Gewölbeschub über dicke Mauern oder über Kapellen, Seitenschiffe und Emporen abzuleiten. Es gab zwar einige Vorformen des gotischen Strebewerkes, z. B. an der Hagia Sophia in Konstantinopel im 6. Jahrhundert oder bei Umgangschören ab etwa 1160 in der Normandie und Île-de-France. Aber hier in Paris entstand jetzt um 1160/80 eine ganz neue Idee, nämlich für den Gewölbeschub neben der eigentlichen Kirche eine gesonderte Konstruktion zu errichten, und nur diese wird gotisches Strebewerk genannt.

Mit der Erfindung des Strebewerkes als äußerer Abstützung des Gewölbeschubes ist eine ganz neue Dimension in den gotischen Kathedralbau gekommen. "Jetzt erst war es möglich, mit der Kombination von Kreuzrippengewölbe, Spitzbogen und Strebewerk die lastenden Kräfte der Kirche, also vor allem das Gewicht des Gewölbes und den Seitenschub zu lenken, zu konzentrieren und nach außen zu verlagern". Das äußere Stützsystem war von innen kaum zu ahnen. Jetzt konnte man das gotische Prinzip der Wandauflösung, die Verwandlung der Mauer in eine lichtdurchschienene, entkörperlichte Glasschicht erst richtig durchführen, da die Mauer von einem Großteil ihrer stützenden Funktion entlastet wurde. Im Innern der Kathedrale herrschte jetzt jene vielzitierte „aufwärtsstrebende Körperlosigkeit“.

Damit konnte man jetzt auch in ganz andere "Höhen" hinein bauen, weil das Problem des Gewölbeschubes von der Konstruktion des Innenraumes weitgehend unabhängig wurde. Paris erreichte auch mit einer Gewölbehöhe im Mittelschiff von 32,5 Meter eine bis dahin für unmöglich gehaltene Höhe. Sens, Noyon und Laon bewegten sich zwischen 22 und 24 Metern. Notre-Dame war also zehn Meter höher. Die Gewölbehöhe in den gotischen Kathedralen Frankreichs wird sich noch bis zum absoluten Höchstwert von 48 Metern in Beauvais steigern.

Auf ein Motiv am oberen Rand dieser Fassade soll besonders eingegangen werden, auf die berühmten Grotesken der "„Galerie des Chimères“", die von der oberen Balustrade auf die Stadt hinabblicken (siehe auch Drolerie).

Victor Hugo (1802–1885) schrieb 1831 den historischen Roman "Notre-Dame de Paris", dessen Kernhandlung unter dem deutschen Titel "Der Glöckner von Notre-Dame" und den zahlreichen davon abgeleiteten Filmen bekannt geworden ist. Der unglückliche, verkrüppelte Held Quasimodo beobachtet das Treiben auf der Straße von oben.

Die originalen Wasserspeier wurden im 18. Jahrhundert entfernt, als einige durch die Witterungseinflüsse zu zerbröckeln begannen und 60 Meter tief auf das Pflaster stürzten. Die Figuren sind heute Kopien, bzw. Neuschöpfungen aus dem 19. Jahrhundert und durch den Roman von Victor Hugo beeinflusst. Man merkt das aus der Nähe deutlich an dem Betoncharakter des Materials.

Solche grotesken Phantasiefiguren an den Außenseiten der Kirchen hatten seit alters her apotropäische Bedeutung, sie sollten also bösen Zauber abwehren. Die Monstren aller Art sind eine Besonderheit der romanischen Kunst. Im 13. Jahrhundert geht ihre Darstellung an bevorzugten Plätzen wie den Portalen merklich zurück, vermutlich durch den starken Einfluss der Zisterziensermönche. So wurden die seltsamen Fabelwesen in gotischer Zeit nur noch an den Regenwasserspeiern angebracht.

Notre-Dame ist die letzte große frühgotische Kathedrale Frankreichs und zugleich die letzte und größte Emporenkirche. Der fünfschiffige Innenraum misst in der Länge knapp 130 Meter und bietet Platz für etwa 9000 Menschen. Das Mittelschiff erreicht 32,5 Meter Höhe. Der Blick nach Osten in den Chor zeigt aber nicht das Originalbild des 12. Jahrhunderts, denn als der Original-Chor 1182 vollendet wurde, gab es noch kein Maßwerk. Auch der Innenraum hat zwischen erster und endgültiger Fertigstellung durchgreifende Änderungen erfahren. Er hatte anfangs nicht einmal ein Gewölbe.

Das Hauptschiff hatte ursprünglich einen "viergeschossigen" Wandaufriss mit Triforium wie bei den Kathedralen von Noyon und Laon. Da der Innenraum mit erst in großer Höhe beginnenden Lichtgaden aber zu dunkel war, änderte man dies ab 1220 zu einem dreigeschossigen Wandaufbau mit einem Maßwerkgeschoss im Obergaden nach dem Vorbild von Reims.

An einer Stelle – rund um die Vierung herum – machte Viollet-le-Duc im 19. Jahrhundert die Veränderung wieder rückgängig, um wenigstens hier den ursprünglichen Zustand zu dokumentieren. Was an der heutigen Gebäudegestalt wie ein Stilbruch erscheint, erklärt sich also aus den zu jener Zeit einsetzenden denkmalpflegerischen Absichten, die es in den Jahrhunderten zuvor nicht gab.

In Beibehaltung romanischer Formen haben die das Hauptschiff begrenzenden Säulen einen runden Querschnitt und enden unter den Bögen der Seitenschiffe in Kapitellen. In gotischer Formensprache führen jedoch darauf aufsitzenden Diensten bis in die Gewölbe des Hauptschiffs.

Mittelschiff und Querschiff sind mit 32 Metern wesentlich höher als die Seitenschiffe, und mit über 12 Metern etwa doppelt so breit. Um einerseits jedem Joch einen annähernd quadratisches Seitenverhältnis zu geben, andererseits alle Dienste bis zum Scheitelpunkt zu führen, entspricht in Längsrichtung jeweils ein Hauptschiffsjoch zwei Seitenschiffsjochen, stützt sich auf sechs Säulen und hat sechs Felder. Ebenso sind die inneren, die Seitenschiffe von den Chorumgängen trennenden Joche der Querschiffe gestaltet und die vorderen beiden Hauptschiffsjoche des Chors. Das letzte Hauptschiffsjoch des Chores mit der polygonalen Apsis hat acht Felder. Das Gewölbe der Vierung hat natürlich nur vier Felder, ebenso die äußeren Joche der Querschiffe, die stark vom quadratischen Grundriss abweichen und erst in der Erweiterungsphase gebaut wurden.

In den Säulenreihen, die innere und äußere Seitenschiffe voneinander trennen, ist jede zweite Säule stärker gotisch gestaltet, hier ist fast der gesamt Schaft von Diensten umstanden.
Der Grundriss zeigt die ungewöhnliche Form des Chores von Notre-Dame. Chorumgang und Kapellenkranz setzen eigentlich die Seitenschiffe des Langhauses lediglich fort und umkreisen den Chor mit mathematischer Genauigkeit. 1330 kamen die Chor-Kapellen hinzu, so dass die Kathedrale wie siebenschiffig wirkte und das in der Mitte liegende Querhaus kaum noch hervortrat.

Um das Querhaus über die Flucht der Kapellenwände hinausragen zu lassen, war bereits 1267 die alte Querhaus-Fassade abgebrochen und dieser Bauteil an beiden Seiten um ein Joch verlängert und mit einer neuen Fassade versehen worden, die jetzt so kunstvoll und aufwändig gestaltet war, dass sie nicht mehr drohte, in dem übrigen Bau unterzugehen. Die neuen, riesigen Fenster sind feinstes Maßwerk. Sie gehören zum Besten und Schönsten, was es auf dem Gebiet gibt.

Die Stilstufe der Maßwerkfenster wird in der Kunstgeschichte als „rayonnant“ bezeichnet, also strahlenförmig. Von 1270 bis 1380 herrschte der Maßwerk-Stil in Frankreich vor (Hochgotik). Die Querhaus-Fassade von Notre-Dame in Paris ist zugleich eine der ersten und bedeutendsten dieser Stilstufe.

Den Bau von Kirchen mit dem Altarraum bzw. Chor zu beginnen und den zu weihen, also seiner Funktion zu übergeben, lange bevor die übrigen Teile fertiggestellt waren, das war das übliche Vorgehen zu einer Zeit, als Gottesdienst vorrangig als Dienst der Priester an Gott aufgefasst wurde. Mehr als eine Kirche aus dem Mittelalter besteht bis heute nur aus dem, was einst der Chor sein sollte. Dass aber mit der Modernisierung älterer Teile begonnen wurde, kaum dass der ursprüngliche Plan einigermaßen vollständig ausgeführt war, ist wohl selten so deutlich wie bei der Kathedrale Notre-Dame von Paris.

Der gotische Baustil entwickelte sich im Umfeld des französischen Königshofes. Daraus ist jedoch keine Verwunderung abzuleiten, dass nicht die Kathedrale der Hauptstadt die erste große gotische Kirche war, sondern die Abteikirche Saint-Denis. Paris war noch nicht die überragende Metropole des Landes. Die Abteikirche hatte als Grablege der Könige eine Spitzenstellung unter den Gotteshäusern des Königreiches. Frankreich wurde noch nicht absolutistisch regiert wie unter Ludwig XIV. Bauherren der großen Kirchen waren abgesehen von königlichen Stiftern hohe Amtsträger der Kirche. Von deren Ambitionen hing nicht unwesentlich ab, wann und wo architektonische Neuerungen eingeführt wurden. Zu diesen mächtigen Kirchenmännern gehörten Suger von Saint-Denis, Abt 1122–1151, und Maurice de Sully, Bischof von Paris 1160–1196.

Von dem mehrmals – zuletzt von Viollet-le-Duc – erneuerten ehemaligen Hochaltar im Chor ist einzig die Mensa erhalten, über der sich eine marmorne Pietà von Nicolas Coustou erhebt. Sie ist von zwei Marmorskulpturen flankiert: links Ludwig XIV. von Coysevox, rechts Ludwig XIII. von Guillaume Coustou, beide in kniender Haltung.

Den heutigen Volksaltar aus Bronze, der im Bereich der Vierung steht, gestaltete Jean Touret (1916–2004) im Jahr 1989 im Auftrag des Erzbischofes von Paris, Kardinal Jean-Marie Lustiger. An der Stirnseite sind die vier Evangelisten des Neuen Testaments (Matthäus, Markus, Lukas und Johannes) zu sehen, an den beiden kurzen Seiten die vier großen Propheten des Alten Testaments (Ezechiel, Jeremia, Jesaja und Daniel).

Die Chorschranke trennt den Chorbereich von den benachbarten Seitenschiffen. Die Skulpturen wurden zwischen 1300 und 1350 von Pierre de Chelles, Jean Ravy und Jean Le Bouteiller geschaffen und zeigen im südlichen Seitenschiff neun Szenen mit Erscheinungen des Auferstandenen, während im nördlichen Seitenschiff das Leben Jesu von der Kindheit bis zum Tod dargestellt ist.

In einem Anbau kann ein Teil des Kirchenschatzes besichtigt werden. Neben historischen Kelchen und Gewändern finden sich hier alte Kruzifixe in prächtigen Schränken. Bedeutend sind die beiden in der napoleonischen Zeit entworfenen Behältnisse für die Dornenkrone und einen Kreuznagel. Die Reliquien waren ursprünglich in der eigens errichteten Sainte-Chapelle untergebracht und befinden sich heute unter Verschluss in der Kathedrale.

Die Geschichte der (Orgel-)Musik in Notre-Dame reicht wohl in das ausgehende 11. bzw. beginnende 12. Jahrhundert zurück. Früheste konkrete Nachweise für die Existenz einer Orgel datieren auf das Jahr 1357. Es handelte sich dabei um ein Blockwerk, welches als Schwalbennest-Orgel im Hauptschiff hing und mutmaßlich Mitte des 13. Jahrhunderts erbaut worden war. Heute verfügt Notre-Dame über zwei Orgeln: die Große Orgel, die im Kern auf eine Cavaillé-Coll-Orgel von 1868 (unter Verwendung von älterem Pfeifenwerk aus dem 17. bis 19. Jahrhundert) zurückgeht und zwischen 1904 und 2014 mehrfach umfassend umgebaut worden ist und derzeit 115 Register auf fünf Manualen und Pedal umfasst. Die zweimanualige Chororgel wurde 1969 gebaut, zwischen 1970 und 2005 wiederholt umgebaut und erweitert und hat 30 Register. 

In Notre-Dame gibt es derzeit drei Titularorganisten: Vincent Dubois, Olivier Latry und Philippe Lefebvre. Die Organisten der Kathedrale waren bis 1985:

Seit 1985:


Die Geschichte der Kirchenglocken von Notre-Dame reicht zurück in das 12. Jahrhundert. Unmittelbar vor Beginn der Französischen Revolution (Stand 1769) hatte die Kathedrale insgesamt 20 Glocken: 8 Glocken im Nordturm, 2 große Glocken (Bourdons) im Südturm, 7 Glocken im Dachreiter, und 3 weitere Glocken am Querschiff, die als Uhrenglocken dienten. 1791 und 1792 fiel ein Großteil der Glocken der beiden Türme der Revolution zum Opfer, insbesondere der zweite Bourdon „Marie“. 1856 wurde der Nordturm mit vier neuen Glocken bestückt.

Im Südturm hängt die 13 Tonnen schwer geschätzte Glocke "Emmanuel", die als die wohlklingendste Glocke Frankreichs gerühmt wird. Der sog. „Grand Bourdon“ wurde im Jahre 1685 von den drei lothringischen Wandergießern Chapelle, Gillot et Moreau neu gegossen, und hat als einzige Glocke die Wirren der Zeit, insbesondere der Französischen Revolution, unbeschadet überstanden. Diese tontiefste Glocke des Geläuts von Notre Dame erklingt nur zu den höchsten Festtagen, wie Ostern, Weihnachten und Pfingsten sowie zum Tod des Papstes und zu besonderen Anlässen.

Bis zum Jahr 2012 bestand das Geläut von Notre Dame aus 5 Glocken. Ergänzt wurde der Grand Bourdon durch vier Glocken aus dem Jahre 1856, die von der Fonderie Guillaume-Besson gegossen worden waren. Diese 4 Glocken waren für das Läuten zu den Messen an Sonn- und Werktagen, zum Angelusläuten um 8, 12 und 19 Uhr und für den Uhrschlag bestimmt; auf "Denise David" schlugen die vollen Stunden.

Nachdem im Oktober 2011 bekannt wurde, dass das Geläut von Notre-Dame im Zuge der Restaurierung bis zum Jahre 2013 auf insgesamt 10 Glocken erweitert werden sollte, um das Gesamtgeläut der Bedeutung der Kathedrale und den liturgischen Notwendigkeiten anzupassen, wurden die vier historischen Nordturm-Glocken am 20. Februar 2012 aus dem Nordturm ausgebaut. Als Grund dafür wurde die schlechtere Qualität des verwendeten Metalls der Glocken von 1856, deren schlechtere akustische Qualität und die mangelnde klangliche Abstimmung der historischen Glocken aufeinander angeführt.

Ursprünglich war geplant, die vier historischen Glocken einzuschmelzen. Die zuständige (obere) Denkmalbehörde hatte dem zugestimmt. Aufgrund von landesweitem Protest in der Bevölkerung wurden die Glocken am 8. November 2012 durch gerichtlichen Beschluss vorerst beschlagnahmt. Seit dem 18. Februar 2014 sind sie innerhalb der Umzäunung im Außenbereich des Chors zur Rue de Cloître hin aufgestellt.

Seit 2013 hängt in den beiden Westtürmen von Notre-Dame ein 10-stimmiges Geläut, das in Anlehnung an den Zustand vor Beginn der Revolution disponiert wurde. Klangliches Fundament ist der "Grand Bourdon Emmanuel" im Südturm.

Um den "Grand Bourdon" zu entlasten, wurde im Südturm ein weiterer Bourdon aufgehängt. Dieser sogenannte "Petit bourdon" wurde von der Glockengießerei Eijsbouts (Asten, Niederlande) gegossen. Er trägt den Namen "Marie", zu Ehren der Jungfrau und Gottesmutter Maria, die zugleich Patronin der Kathedrale Notre-Dame und des Erzbistums Paris ist. Der Name erinnert zugleich an den Namen des ersten "petit bourdon" aus dem Jahre 1378, der im Jahre 1792 unterging.

Im Nordturm wurden insgesamt acht neue Glocken aufgehängt, die allesamt von der Glockengießerei Cornille-Havard (Villedieu-les-Poêles, Département Manche) gegossen wurden. Die Namen der neuen Glocken sind mit Blick auf bedeutende Heilige beziehungsweise Personen der Bistumsgeschichte ausgewählt worden. Im August und September 2012 wurden die Glocken "Marcel" und "Étienne" sowie "Denis, Maurice" und "Jean-Marie" gegossen.

Die Glocken wurden am 2. Februar 2013 vom Pariser Erzbischof André Vingt-Trois geweiht. Sie läuteten zum ersten Mal am Vorabend des Palmsonntags, dem 23. März 2013 um 17:00 Uhr.

Musikgeschichtlich bedeutsam ist die Notre-Dame-Schule, die von etwa 1160 bis 1250 betrieben wurde. Ihr Name leitet sich von der Pariser Kathedralkirche her, an der die beiden Hauptvertreter der Kompositionsschule, Léonin und Pérotin, als Magister tätig waren.

Die Geschichte Notre-Dames spiegelt die Geschichte Frankreichs wider.

In der Kathedrale wurden beigesetzt:




Am 3. September 2006 wurde der Domvorplatz "Parvis de Notre-Dame" aufgrund des großen historischen Beitrages von Papst Johannes Paul II. feierlich in "Parvis de Notre-Dame – place Jean Paul II" umbenannt. Das hatte der Pariser Oberbürgermeister Bertrand Delanoë im April 2006 trotz heftiger Proteste oppositioneller politischer Gruppen, z. B. Les Verts, die auf die Wahrung des Prinzips der Laizität des Staates hinwiesen, im Stadtparlament durchgesetzt.

Frankreichs "kilomètre zéro" ("Kilometer Null"), der Referenzpunkt für die Entfernungsangaben z. B. der nach Paris führenden Autobahnen, liegt auf dem Platz vor der Kathedrale.


Zum Vergleich zur Kathedrale wird in Paris oft die Pfarrkirche Saint-Eustache (1532–1640) herangezogen, die am Ende der Gotik, im Übergang zur Renaissance ebenfalls auf einem Vorgängerbau entstand. Dies gilt neben ihrer besonderen Gestalt und Größe insbesondere für ihre Hauptorgel. 




</doc>
<doc id="9555" url="https://de.wikipedia.org/wiki?curid=9555" title="Antigua und Barbuda">
Antigua und Barbuda

Antigua und Barbuda () ist ein unabhängiger Staat innerhalb des Commonwealth, der aus den Inseln Antigua und Barbuda gebildet wurde. Der zur Ostkaribik gehörende Inselstaat, dem auch die unbewohnte abgelegene Insel Redonda angehört, liegt zwischen dem Nordatlantik und der Karibik, südöstlich von Puerto Rico.

Der Antillenstaat Antigua und Barbuda gehört zu den sogenannten Inseln über dem Winde. Er befindet sich zwischen der Karibik und dem Atlantischen Ozean, ca. 650 km südöstlich von Puerto Rico. Das insgesamt 442,6 km² große – am Land liegende – Staatsgebiet
umfasst zwei Haupt- und einige kleinere Inseln. Die größte ist die 280 km² große Insel Antigua, auf der sich auch die Hauptstadt Saint John’s befindet. Barbuda umfasst 161 km² und liegt 48 km nördlich von Antigua. 56 km südwestlich von Antigua befindet sich die nur 1,6 km² große unbewohnte Felseninsel Redonda. Andere Inseln über 1 km Länge sind Long Island, Guinea Island und Green Island vor der Küste von Antigua, sowie die Nehrung 11 Mile Beach auf Barbuda.

Auch wenn die Hauptinsel Antigua vor allen aus Kalkformationen aufgebaut ist, geht sie auf einen vor etwa 34 Millionen Jahren im Südwesten der Insel ausgebrochenen unterseeischen Vulkan zurück. Durch das Wachstum von Steinkorallen breiteten sich Kalkablagerungen in Richtung Nordosten aus. Der höchste Punkt der Insel und zugleich des Inselstaates ist der 402 m hohe Mount Obama, ehemals "Boggy Peak". Er ist der Rest des Vulkankraters im Südwesten Antiguas. Barbuda ist die Oberfläche eines großen Korallenriffs. Der höchste Punkt der noch weitgehend unberührten und naturbelassenen Koralleninsel ragt allerdings nur 44,5 m aus dem Meer empor. Die Küstenlinien der beiden Hauptinseln sind überaus reich gegliedert. Sie verfügen über eine Reihe von Stränden, Lagunen und zahlreichen natürlichen Häfen (Buchten). Umgeben sind die Inseln von einer Reihe von Korallenriffen und Untiefen. Die kleine Felseninsel Redonda ist der Rest eines 296 m hohen erloschenen Vulkans.

Das Klima von Antigua und Barbuda ist tropisch. Die Temperaturen erreichen im Monatsmittel Werte von 22 (Dezember bis Februar) bis 30 °C (Juni bis September). Mit einem mittleren Jahresniederschlag von 900 bis 1000 mm ist es für karibische Verhältnisse relativ trocken. Die meisten Niederschläge fallen zwischen September und November. Im Sommer (Juli bis Oktober) besteht die Gefahr des Auftretens von tropischen Wirbelstürmen.

Im Jahr 2017 lebten in Antigua und Barbuda 94.731 Menschen. Ein Großteil der Einwohner lebt auf Antigua, Barbuda hat nur etwa 1600 Einwohner (Stand 2011), Redonda ist unbewohnt. Die Bevölkerungsentwicklung verlief in den letzten 50 Jahren fast durchweg positiv. Zwischen 1961 und 1969 wuchs die Einwohnerzahl von 55.000 auf 65.000. Danach stagnierte sie bis Anfang der 1990er Jahre bei ca. 63.000. Ab 1993 war ein erneutes Anwachsen auf das heutige Niveau zu verzeichnen. Die Fertilitätsrate (TFR) liegt mit 2,0 (Deutschland 1,46) knapp über dem Bestandserhaltungsniveau und bewirkt ein geringes natürliches Wachstum. Dagegen ist aber die Säuglingssterblichkeit viermal so hoch wie in Deutschland. Auch die Wanderungsbilanz ist negativ; allein 2005 verließen 420 Menschen den Inselstaat.

Laut Census 2011 setzte sich die Bevölkerung aus 44.581 Frauen und 40.986 Männern zusammen. Die Alterspyramide ist sehr regelmäßig aufgebaut. Ein knappes Viertel der Bevölkerung ist 14 Jahre und jünger, während etwa 6,9 Prozent 65 Jahre und älter sind. Mit einem Durchschnittsalter von 32,5 Jahren ist die Bevölkerung sehr jung.

Die folgenden Zahlen geben den Stand von 2017 wieder (ohne Touristen).

Verbreitete Sprachen sind Englisch (Amtssprache) und eine auf diesem basierende Kreolsprache. In ethnischer Hinsicht besteht die Bevölkerung zu 87,3 % aus Schwarzafrikanern und zu 4,7 % aus Mischlingen. Daneben gibt es Europäer und Asiaten.

Nominelle Mitglieder von Religionsgemeinschaften:

Etwa ab 10.000 v. Chr. besiedelten Indianer vom Stamm der Siboney („Steinmenschen“) die Inseln. Bis 1200 verdrängten die Arawak-Indianer aus dem Orinoco-Gebiet die Siboney. Daraufhin fielen Kariben aus Südamerika ein und verbreiteten sich in dem Gebiet.

Im Jahr 1493 landete Christoph Kolumbus in Antigua. In den folgenden Jahren wurden die auf den Inseln lebenden Indianer von Spaniern zur Sklavenarbeit nach Hispaniola und in andere spanische Kolonien verschleppt. Ein Großteil der Indianer starb innerhalb kurzer Zeit an den Folgen der Arbeit oder an Krankheiten. Nach 1500 versuchten Spanier, Franzosen und Briten abwechselnd, jeder jedoch erfolglos, die Inseln dauerhaft zu besiedeln und in ihren Besitz zu bringen. Piraten nutzten zu dieser Zeit die Inseln als Rückzugsorte.

Über hundert Jahre später gelangte Barbuda (1628) in britischen Besitz. Bemühungen seitens der Regierung zur Besiedlung scheiterten allerdings. Vier Jahre danach wurde Antigua von St. Kitts und Nevis aus durch britische Siedler kolonisiert. Die Siedler bauten dort zunächst Tabak an. Saint John’s wurde als erste dauerhafte europäische Siedlung gegründet. 1663 trafen dort die ersten Siedler aus England ein. Auch auf Barbuda wurde 1666 die erste dauerhafte Siedlung errichtet.

Im Jahr 1680 gingen weite Teile von Barbuda in den Besitz der Familie Codrington über, die hier den nach ihnen benannten Ort als Verwaltungszentrum anlegte. Fünf Jahre später wurden auf den Inseln Zuckerrohrplantagen angelegt, welche zum größten Teil von afrikanischen Sklaven bewirtschaftet wurden.
Admiral Horatio Nelson baute 1784 auf Antigua einen britischen Flottenstützpunkt auf. Aufgrund des sturmsicheren Hafens wurde English Harbour zum Hauptquartier der auf den Antillen stationierten Flotte ausgebaut. Die Flottenpräsenz vertrieb die letzten Piraten. Die Sklaverei wurde 1834 abgeschafft, wodurch den Plantagen die wirtschaftliche Basis verloren ging, was eine schwierige Phase des ökonomischen Abschwungs und der Umstrukturierung einleitete. 1860 kam es zur Vereinigung von Antigua und Barbuda; der Name der Kolonie lautete nur Antigua.

Unter der Führung des späteren Premierministers Vere Cornwall Bird formierte sich ab dem Jahr 1940 eine Unabhängigkeitsbewegung. Schon sechs Jahre später erhielten die Inseln ein eigenes Parlament. Antigua und Barbuda erhielten 1956 den Status einer eigenständigen Kolonie. Von 1958 bis 1962 gehörten die Inseln zur Provinz der Westindischen Föderation.

Die Inseln traten 1967 der Gruppe der West Indies Associated States bei. Sie erlangten hierdurch die vollständige innenpolitische Autonomie; außenpolitische Belange wurden von Großbritannien wahrgenommen. Im Mai 1968 wurde der Staat Mitglied der "Caribbean Free Trade Association (CARIFTA)." Die seit Jahrhunderten dominierende Zuckerrohrwirtschaft wurde 1972 eingestellt. Am 1. November 1981 erlangten die Inseln die Unabhängigkeit von Großbritannien unter dem neuen Namen Antigua und Barbuda.

Antigua und Barbuda ist seit 1981 eine zum Commonwealth of Nations gehörende Konstitutionelle Monarchie. Das politische System orientiert sich in seinem Aufbau am britischen Vorbild. Antigua und Barbuda ist Mitglied der CARICOM, der OECS und der OAS und gehört zu den Trägern der Universität der Westindischen Inseln.

"Exekutive:" Staatsoberhaupt ist der Monarch des Vereinigten Königreiches, derzeit Königin Elisabeth II. Auf Antigua und Barbuda selbst wird sie von einem Generalgouverneur vertreten. Dieses Amt wird seit dem 14. August 2014 von Rodney Williams ausgeübt. Die eigentliche Regierungsmacht liegt aber beim Premierminister, der sich auf die Mehrheit der Abgeordneten im Repräsentantenhaus stützt. Das Amt des Premiers hatte zwischen 1967 und 1971 sowie 1976 und 1994 Vere Cornwall Bird und zwischen 1994 und 2004 dessen Sohn Lester Bird inne. Ihm folgte Baldwin Spencer, der von 2004 bis 2014 amtierte. Seit 2014 ist Gaston Browne amtierender Premierminister. Aus den Reihen des Parlamentes ernennt der Premierminister die Mitglieder seiner Regierung, die derzeit 15 Minister umfasst. Im Gegensatz zum Vereinigten Königreich besitzt Antigua und Barbuda eine geschriebene Verfassung.

"Legislative:" Das seit 1946 bestehende Parlament besteht aus zwei Kammern: einem Repräsentantenhaus und einem Senat mit je 17 Sitzen. Während die Mitglieder des Senates vom Generalgouverneur ernannt werden, erfolgt die Bestimmung der Mitglieder des Repräsentantenhauses in Wahlen, die alle fünf Jahre stattfinden. Die letzten Parlamentswahlen am 12. Juni 2014 gewann die "Arbeitspartei" (Antigua Labour Party, ALP). Sie erhielt 14 Sitze und stellt den Premierminister. Weitere 3 Sitze entfielen auf die bisher regierende "Vereinigte Fortschrittspartei" (United Progressive Party, UPP).

Der Staat ist seit 1981 Mitglied der Vereinten Nationen, seit 1995 Mitglied der WTO, Mitglied in der AOSIS, Gruppe der 77, CELAC und ist Vertragsstaat des ICC. Antigua und Barbuda ist zwar Mitglied in der Bolivarianischen Allianz für Amerika. Es unterhält aber auch gute Beziehungen zu den Vereinigten Staaten.

Der Staat gliedert sich in sechs Verwaltungsbezirke ("Parishes") und die zwei Nebengebiete ("Dependencies") Barbuda und Redonda. Die Einwohnerzahlen in der folgenden Tabelle beziehen sich auf die Volkszählung vom 28. Mai 2001.

Quelle: Census Office of Antigua and Barbuda

Der Staatshaushalt umfasste 2000 Ausgaben von umgerechnet 145,9 Millionen US-Dollar, dem standen Einnahmen von umgerechnet 123,7 Millionen US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 3,6 % des BIP. Die Staatsverschuldung betrug 2009 etwa eine Milliarde US-Dollar oder ca. 90 % des BIP.

Der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche betrug:

Auf Antigua und Barbuda existiert kein reguläres Eisenbahnnetz. Allerdings gibt es ein 77 km langes Schmalspurnetz, das dem Transport von Zuckerrohr dient. Es umfasst Linien in der Spurweite von 760 mm (64 km) und 610 mm (13 km).

Die Hauptlast des Transportwesens liegt auf dem Straßenverkehr. Das Straßennetz hatte 2011 eine Länge von 1.170 km. Davon waren aber nur 386 km befestigt. Autobahnen fehlen ganz. Der Überlandverkehr wird von einigen Buslinien abgewickelt. 2005 waren im Inselstaat über 15.000 Kraftfahrzeuge zugelassen.

Der internationale "Flughafen VC Bird International" (IATA-Flughafencode: ANU) befindet sich im Nordosten der Insel Antigua, etwa acht Kilometer nordöstlich der Hauptstadt Saint John’s. Er wurde im Zweiten Weltkrieg als amerikanischer Militärstützpunkt angelegt und verfügt heute über eine 2744 m lange Start- und Landebahn. Von ihm aus werden Flüge nach Nordamerika und Europa angeboten. Angeflogen werden unter anderem Puerto Rico (eine Stunde), Miami (drei Stunden), New York (vier Stunden), Baltimore (vier Stunden), Toronto (vier Stunden), London (acht Stunden), Paris (acht Stunden) und Frankfurt am Main (neun Stunden). Der Flughafen ist Heimatbasis der Caribbean Airlines, die von Antigua aus Ziele auf den Kleinen Antillen und in Südamerika anfliegt. Des Weiteren fliegen folgende Fluggesellschaften täglich bis wöchentlich nach St. John’s auf Antigua: Air Canada, American Airlines, Continental Airlines, Delta Airlines und US Airways. Nach Europa bieten weitere Airlines ihre Flugdienste an: British Airways, Virgin Atlantic, Sunsail Airways und Condor Flugdienst. LIAT, Air St.Kitts/Nevis, Carib Aviation, Caribbean Airlines, Montserratt Airways und Caribbean Star Airlines Ltd verbinden St. John’s mit den meisten Inseln der Karibik.

In Saint John’s befindet sich auch der Überseehafen, welcher unter anderem von Kreuzfahrtschiffen angelaufen wird. Unter der Flagge des Inselstaates waren 2005 insgesamt 981 Schiffe mit einer Größe von über sieben Millionen Bruttoregistertonnen registriert. Darunter nutzten aber 953 Schiffe die Flagge von Antigua und Barbuda als Billigflagge. Von den ausgeflaggten Schiffen stammten 1094 aus Deutschland.









</doc>
<doc id="9559" url="https://de.wikipedia.org/wiki?curid=9559" title="Klasse">
Klasse

Klasse (von lateinisch "classis" „Herbeirufung, herbeigerufene Volksmasse, Flotte“) steht für:

Siehe auch:



</doc>
<doc id="9561" url="https://de.wikipedia.org/wiki?curid=9561" title="Hanfgewächse">
Hanfgewächse

Die Hanfgewächse (Cannabaceae) sind eine Pflanzenfamilie in der Ordnung der Rosenartigen (Rosales). Die etwa elf Gattungen mit etwa 170 Arten sind fast weltweit verbreitet.

Die wichtigsten Nutzpflanzen der Familie der Hanfgewächse sind in den beiden Gattungen Hanf ("Cannabis") und Hopfen ("Humulus").

Es sind ausdauernde krautige Pflanzen oder verholzende Pflanzen: Sträucher und Bäume; einige Arten sind Kletterpflanzen. Manche Arten enthalten Harz. 

Die Anordnung der Blätter an den Stängeln oder Zweigen ist gegenständig oder wechselständig und spiralig oder zweizeilig. Die gestielten Laubblätter besitzen eine sehr vielgestaltige Blattspreite von einfach über handförmig gelappt bis geteilt. Sie haben immer einen gesägten Blattrand. Nebenblätter sind immer vorhanden, sie können untereinander verwachsen sein oder nicht. 

Sie sind meist zweihäusig (diözisch), selten einhäusig (monözisch) getrenntgeschlechtig. Die verzweigten, dichten Blütenstände mit Deckblättern enden in einem zymösen Teilblütenstand. 

Die eingeschlechtigen Blüten sind radiärsymmetrisch und fünfzählig. Oft ist ein Teil der Blütenhüllblätter reduziert. Es sind fünf Kelchblätter vorhanden: in männlichen Blüten sind sie frei, in weiblichen Blüten sind sie teilweise verwachsen. Kronblätter fehlen oft. In den männlichen Blüten ist nur ein Kreis mit fünf fertilen Staubblättern vorhanden. In den weiblichen Blüten sind zwei Fruchtblätter zu einem synkarpen, oberständigen Fruchtknoten verwachsen. Der sehr kurze Griffel endet in zwei langen Narben. Die Bestäubung erfolgt meist durch den Wind (Anemophilie). 

Sie bilden oft Steinfrüchte oder seltener bei "Cannabis" und "Humulus" Nussfrüchte. 

An Inhaltsstoffen sind Alkaloide und Flavonole vorhanden.
Die Chromosomengrundzahl beträgt meist x = 10.

Die gültige Erstveröffentlichung des Familiennamens Cannabaceae erfolgte 1820 durch Ivan Ivanovič Martinov in "Tekhno-Botanicheskīĭ Slovar …", S. 99. Die Veröffentlichung von Cannabidaceae durch Stephan Ladislaus Endlicher erfolgte erst 1837 in "Genera plantarum secundum ordines naturales disposita". Typusgattung ist "Cannabis" L.

Die Familie Cannabaceae bestand lange Zeit nur aus den beiden Gattungen "Cannabis" und "Humulus" innerhalb einer Ordnung Urticales. Molekulargenetische Untersuchungen zeigten, dass die sechs oder sieben Familien und 2600 Arten der früheren Ordnung Urticales mit in die Ordnung Rosales gehören. Es stellte sich heraus, dass die Unterfamilie Celtidoideae mit den Gattungen "Aphananthe", "Celtis", "Gironniera", "Pteroceltis" und "Trema" nicht näher mit der Unterfamilie Ulmoideae verwandt sind. Statt in die Ulmaceae gehören die Gattungen der Celtidoideae zu den Cannabaceae.

Obwohl die Celtidoideae mehr Gattungen und Arten enthalten, heißt die Familie nicht Celtidaceae, sondern aus Prioritätsgründen Cannabaceae. So ergeben sich folgende Synonyme für Cannabaceae Martinov: Cannabidaceae Endl., Celtidaceae Engl., Lupulaceae Schultz Sch., nom. illeg.

Verwandte Familien innerhalb der Ordnung Rosales:
Die Familie der Hanfgewächse (Cannabaceae) enthält heute etwa (neun bis) elf Gattungen mit etwa 170 Arten:






</doc>
<doc id="9566" url="https://de.wikipedia.org/wiki?curid=9566" title="Franz Marc">
Franz Marc

Franz Moritz Wilhelm Marc (* 8. Februar 1880 in München; † 4. März 1916 in Braquis bei Verdun, Frankreich) war ein deutscher Maler, Zeichner und Grafiker. Er gilt als einer der bedeutendsten Maler des Expressionismus in Deutschland. Neben Wassily Kandinsky war er Mitbegründer der Redaktionsgemeinschaft Der Blaue Reiter, die am 18. Dezember 1911 ihre erste Ausstellung in München eröffnete. Der Blaue Reiter ging aus der Neuen Künstlervereinigung München hervor, in der Marc kurzzeitig Mitglied war. Für den Almanach "Der Blaue Reiter" und andere Veröffentlichungen verfasste er kunsttheoretische Schriften.

Waren Marcs frühe Werke noch dem naturalistischen Stil des Akademismus verhaftet, widmete er sich nach einem Parisbesuch im Jahr 1907 dem Postimpressionismus unter dem Einfluss von Gauguin und van Gogh. Zwischen 1910 und 1914 verwendete er Stilelemente des Fauvismus, Kubismus, Futurismus und Orphismus, trennte sich in seinem Werk jedoch nicht vollständig vom Gegenstand. In dieser Zeit entstanden seine bekannten Gemälde, die hauptsächlich Tiermotive zum Inhalt haben wie "Der Tiger", "Blaues Pferd I", "Die gelbe Kuh", "Der Turm der blauen Pferde" oder "Tierschicksale". Marcs erste abstrakte Gemälde wie "Kleine Komposition I" und "Kämpfende Formen" entstanden 1913 und 1914. Zu Beginn des Ersten Weltkriegs wurde er eingezogen und fiel zwei Jahre später im Alter von 36 Jahren vor Verdun.

Franz Marc wurde am 8. Februar 1880 als zweiter und letzter Sohn der Familie Marc in der Schillerstraße Nr. 35 in München geboren. Sein Vater Wilhelm Marc, der zunächst ein rechtswissenschaftliches Studium abgeschlossen hatte, bevor er an der Akademie der Bildenden Künste München Malerei belegte, war ein Landschafts- und Genremaler. Er entstammte einer bayerischen Beamtenfamilie. Seine Mutter Sophie, geborene Maurice, kam aus dem Elsass und hatte ihre Kindheit in der französischen Schweiz verlebt, wo sie ein streng calvinistisches Internat besuchte. Sie hatte als Erzieherin in der Familie ihres späteren Ehemanns gearbeitet. Wilhelm und Sophie Marc hatten spät geheiratet.
Franz und sein drei Jahre älterer Bruder Paul Marc wurden katholisch getauft, jedoch protestantisch erzogen. Sie wuchsen zweisprachig auf. Den Sommer 1884 verbrachte Marc erstmals in Kochel am See, wo sich die Familie während der nächsten Jahre fast jeden Sommer aufhielt. Beide Brüder besuchten das Luitpold-Gymnasium in München, wo zeitweise Albert Einstein ein Mitschüler war.

Der Vater Wilhelm Marc verließ die katholische Kirche und konvertierte 1895 zum protestantischen Glauben. Franz Marc trug sich mit dem Gedanken, wie sein älterer Bruder Paul Altphilologie oder Theologie zu studieren – wie er Pastor Otto Schlier, dessen Konfirmandenunterricht ihn nachhaltig beeindruckt hatte, in einem Brief 1897 mitteilte. Als 18-jähriger Schüler befasste er sich mit Literatur und Philosophie, insbesondere mit dem Werk von Thomas Carlyle und Friedrich Nietzsche. Im Jahr 1899 legte Franz Marc das Abitur am Luitpold-Gymnasium ab.

1899 verwarf Marc den Gedanken an einen geistlichen Beruf und schrieb sich für ein Philologiestudium an der Ludwig-Maximilians-Universität München ein. Vor Antritt seines Studiums trat er im Oktober desselben Jahres seinen einjährigen Militärdienst in Lagerlechfeld bei Augsburg an und lernte reiten. Während dieser Zeit entschied er sich, den Beruf seines Vaters zu ergreifen. Im Oktober 1900 immatrikulierte er sich an der Münchner Kunstakademie. Er erhielt zunächst Unterricht in Anatomie bei Gabriel von Hackl und ab 1901 Malerei bei Wilhelm von Diez, die beide in der Tradition der Münchner Malerschule des 19. Jahrhunderts unterrichteten. Während der Semesterferien der Jahre 1901 und 1902 hielt er sich auf der Staffelalm in der Jachenau auf, die in der Nähe des Ferienortes der Familie in Kochel am See lag. 1902 betrieb er zeitweise Studien im nördlich von München gelegenen Dachauer Moos.

Mit seinem Studienfreund Friedrich Lauer, der über ausreichend Geldmittel verfügte, reiste er im Mai 1903 durch Frankreich. Aus dieser Zeit ist ein französischsprachiges Tagebuch erhalten. Zunächst machten sie für einige Monate in Paris Station, Ende Juli fuhren sie in die Bretagne, danach in die Normandie. In Paris besuchte Marc die Pariser Museen, insbesondere die antiken Sammlungen, kopierte Gemälde im Louvre und zeichnete in den Straßen. Er studierte die Sehenswürdigkeiten und kaufte im Kunsthandel bei Flammarion japanische Holzschnitte, deren Technik und Komposition ihn stark beeindruckt haben sollen. In der Kathedrale Notre-Dame-de-Chartres faszinierten ihn die gotischen Glasfenster. Nach der Rückkehr nach München Anfang September desselben Jahres verließ Marc, vom akademischen Unterricht enttäuscht, die Kunstakademie.

1904 zog Marc aus dem Elternhaus in Pasing aus und richtete sich ein Atelier in der Kaulbachstraße 68 in Schwabing ein. Er unterhielt in dieser Zeit eine Affäre mit der um neun Jahre älteren Kunst- und Antiquitätenkennerin Annette Simon, geborene von Eckardt (1871–1934), Frau des Münchner Professors für Indologie, Richard Simon (1865–1934). Als Malerin, Schriftstellerin und Kopistin verfügte sie über gute Beziehungen zum Kunsthandel und zu Antiquaren. Sie vermittelte dem unter Geldsorgen leidenden Marc Aufträge für Grafiken und die Möglichkeit, durch den Verkauf von Büchern, japanischen Farbholzschnitten und anderer Antiquitäten aus seiner Sammlung etwas zu verdienen.

Im Februar des Jahres 1905 lernte Franz Marc auf dem Bauernkirchweihball, einem Schwabinger Kostümfest, die Kunststudentin Maria Franck kennen. Da sie kurz darauf nach Berlin zurückkehrte, verloren sie sich bis Dezember 1905 aus den Augen. Gegen Ende des Jahres oder im März 1906 trennte sich Annette Simon-von Eckardt von Franz Marc, sie blieben allerdings lebenslang freundschaftlich verbunden.

Um sich von der emotionalen Belastung abzulenken, reiste er im April 1906 mit seinem Bruder, der Byzantinist geworden war und eine wissenschaftliche Aufgabe in Griechenland zu erfüllen hatte, nach Saloniki und zu dem Berg Athos. Nach dieser Studienreise zog sich Franz Marc zum Arbeiten nach Kochel zurück, wo er bis in den Herbst blieb. Sowohl Maria Franck als auch eine weitere Freundin, die Malerin Marie Schnür, folgten ihm nach. Die drei ließen sich auf ein Dreiecksverhältnis ein, in dem sich Marc mehr und mehr der elf Jahre älteren Schnür zuwandte. Marie Schnür wollte ihren im Februar 1906 in Paris unehelich geborenen Sohn aus der Beziehung mit Angelo Jank (eine andere Quelle nennt August Gallinger als Vater) zu sich nehmen, und Franz Marc gab ihr ein Eheversprechen, was er Maria Franck im November 1906 mitteilte.

Franz Marc und Marie Schnür schlossen am 27. März 1907 in München die Ehe. Er reiste am selben Tag allein nach Paris, wo ihn besonders die Werke von Vincent van Gogh und Paul Gauguin beeindruckten. Seine Begeisterung über die dortige Kunst- und Ausstellungsszene fasste er in Berichte, die er Maria Franck zuschickte. Im darauf folgenden Jahr, am 8. Juli, wurde die Ehe mit Marie Schnür geschieden. Da diese jedoch Marc entgegen den Vereinbarungen des Ehebruchs mit Maria Franck beschuldigte, konnte Marc nach geltendem Recht zunächst keine zweite Ehe eingehen.

Im Jahr 1909 beauftragte die Münchner Lehrmittelhandlung Wilhelm Plessmann Franz Marc damit, für den "Plessmannschen Handwebstuhl" Web-Muster zu entwerfen. Die Texte dazu schrieb seine frühere Geliebte Annette Simon-von Eckardt.

August Macke, den er Anfang 1910 kennengelernt hatte, besuchte ihn zusammen mit seinem Vetter, dem Maler Helmuth Macke und Bernhard Koehler jun. (1882–1964), dem Sohn seines späteren Mäzens Bernhard Koehler sen., in seinem Atelier in der Münchner Schellingstraße 33. Der Anlass des Besuchs waren zwei Lithographien Marcs in der Münchner Kunsthandlung von Franz Josef Brakl, die Macke begeistert hatten. Für Marc war es der erste Kontakt mit einem gleichgesinnten Künstler. 1912 entstand in Mackes Atelier in Bonn ein gemeinsam geschaffenes Wandbild mit dem Titel "Paradies". Mit Macke verband ihn eine lebenslange Freundschaft, und Marc unterhielt mit ihm einen lebhaften Briefwechsel über Fragen der Kunsttheorie.

Koehler jun. hatte durch Brakl einige Bilder Marcs seinem Vater schicken lassen. Anschließend besuchte Koehler sen. Marc Ende Januar in seinem Atelier und kaufte das 1905 entstandene Gemälde "Der tote Spatz", das auf Marcs Schreibtisch stand, und von dem sich der Künstler äußerst ungern trennte. Das Bild bildete den Grundstein zu Koehlers umfangreicher Marc-Sammlung. In der Folge unterstützte er den am Existenzminimum lebenden Künstler mit monatlich 200 Mark und erhielt als Gegenwert Bilder seiner Wahl, zunächst begrenzt auf ein Jahr.

Im Februar des Jahres 1910 hatte Franz Marc seine erste Einzelausstellung in der Kunsthandlung Brakl, die 31 Gemälde sowie Gouachen und Lithografien umfasste. Zwei Monate später zogen Marc und Maria Franck nach Sindelsdorf in das Haus des Schreinermeisters Josef Niggl, wo sie bis 1914 wohnten. Ihre Ateliers in München gaben beide auf. Dieses Haus hat heute die Adresse „Franz-Marc-Straße 1“.

1911 wurde ein von Marc erneut beantragter Dispens für eine Eheschließung mit Maria Franck verweigert, weshalb beide Anfang Juni nach London reisten, um dort eine Ehe nach englischem Recht einzugehen, was ihnen jedoch nach Aussagen Maria Marcs nicht gelang. Dennoch bezeichneten sie sich fortan öffentlich als Ehepaar.

Im Dezember 1909 sah Marc mehrmals die erste Ausstellung der Neuen Künstlervereinigung München (N.K.V.M.) in der Modernen Galerie Heinrich Thannhauser im Arco-Palais in der Theatinerstraße 7. Vom 1. bis zum 14. September 1910 fand die zweite Ausstellung der N.K.V.M. statt, an der insgesamt 29 Künstler beteiligt waren. Ausgestellt wurden beispielsweise Werke von Georges Braque, Pablo Picasso und Georges Rouault. Die Ausstellung wurde – wie die von 1909 – in der Presse und in der Öffentlichkeit angegriffen, woraufhin Marc, der diese Ausstellung besucht hatte, eine positive Besprechung verfasste, die über Reinhard Piper dem Galeristen Thannhauser zukam.

Franz Marc lernte am 1. Januar 1911 in Marianne von Werefkins Atelierwohnung in der Giselastraße 23 Wassily Kandinsky und Gabriele Münter kennen und besuchte am folgenden Tag gemeinsam mit ihnen, begleitet von Alexej von Jawlensky und Helmuth Macke, ein Konzert von Arnold Schönberg in München. Unter dem Eindruck der neuartigen Musik Schönbergs malte Kandinsky kurz darauf das Bild "Impression III (Konzert)" und schrieb dem ihm unbekannten Komponisten einen Brief, womit er eine inhaltliche Diskussion auslöste, in der Kandinskys „Thesen von der Verwandtschaft der "Dissonanzen in der Kunst" in der aktuellen Malerei wie in der musikalischen Komposition von Schönberg […] aufgegriffen und fortgesetzt wurden.“ Am 4. Februar 1911 wurde Franz Marc zum 3. Vorsitzenden der N.K.V.M. ernannt. Marc stellte seine Arbeiten und die seiner Freundin Maria Franck vor.

Im Herbst 1911 verschärften sich die Spannungen zwischen den konservativen Mitgliedern und der Gruppe um Kandinsky, in deren Folge es am 2. Dezember hinsichtlich des von der Jury wegen Übergröße abgewiesenen Gemäldes "Komposition V/Das Jüngste Gericht" von Kandinsky zum Eklat kam. Noch am selben Tag traten Wassily Kandinsky, Gabriele Münter, Franz Marc und Alfred Kubin aus.

Nachdem sich als Abspaltung (Secession) von der N.K.V.M. die von Kandinsky und Marc begründete Redaktionsgemeinschaft des Blauen Reiters formiert hatte, wurde am 18. Dezember 1911 die „Erste Ausstellung der Redaktion ‚Der Blaue Reiter‘“ in der Galerie Thannhauser eröffnet. Zeitgleich lief die dritte Ausstellung der restlichen acht Mitglieder der N.K.V.M. im darüber gelegenen Stockwerk. 14 Künstler waren auf der ersten Ausstellung vertreten, neben Marc und Kandinsky Künstler wie die Gebrüder Burljuk, Heinrich Campendonk, Robert Delaunay, Jean-Bloé Niestlé, Elisabeth Epstein, August Macke, Gabriele Münter, Henri Rousseau und Arnold Schönberg. Franz Marc war unter anderem mit seinen Gemälden "Reh im Walde I" und "Die gelbe Kuh" vertreten; beide sind auf einem Foto von Gabriele Münter, die die Ausstellung fotografisch dokumentierte, zu sehen.

Die Ausstellung ging anschließend auf Tournee in weitere Städte, so nach Köln in den Gereonsklub und nach Berlin in Herwarth Waldens Galerie Der Sturm. Weitere Stationen bis zum Jahr 1914 waren unter anderem Bremen, Hagen, Frankfurt, Hamburg, Budapest, Oslo, Helsinki, Trondheim und Göteborg. In der Wanderausstellung waren zusätzlich Werke von Jawlensky und Werefkin zu sehen, die inzwischen ebenfalls aus der N.K.V.M. ausgetreten waren und sich dem Blauen Reiter angeschlossen hatten.

Die zweite Ausstellung des Blauen Reiters folgte vom 12. Februar bis zum 18. März 1912 unter dem programmatischen Titel „Schwarz-Weiß“ in der Münchner Buch- und Kunsthandlung Hans Goltz in der Brienner Straße 8. Sie zeigte ausschließlich druckgrafische Blätter und Zeichnungen, unter anderem auch Werke von Paul Klee und den Brücke-Künstlern. Hier traf Franz Marc zum ersten Mal auf Paul Klee, ein Treffen, in dessen Folge eine enge Freundschaft zwischen den beiden Künstlern entstand. Im Mai 1912 gaben Marc und Kandinsky, finanziell unterstützt von Bernhard Koehler, den Almanach "Der Blaue Reiter" mit einem Titelholzschnitt Kandinskys bei Piper in München heraus.

Im Oktober 1912 besuchten Franz und Maria Marc das Ehepaar Macke in Bonn und sahen sich die Sonderbundausstellung in Köln an. Kurz vor der Eröffnung im Sommer hatte es zwischen Marc und dem Mitorganisator Macke wegen der Ausjurierung einiger Bilder eine Auseinandersetzung gegeben. Doch war Marc unterdessen sehr angetan von der Ausstellung. Sein Gemälde "Der Tiger" war in der Ausstellung als Bild Nr. 450 präsent. Die Freunde entschlossen sich zu einer Parisreise, wo sie Robert Delaunay, der beim Blauen Reiter ausgestellt hatte, persönlich kennenlernten. Dessen Werk, dem Guillaume Apollinaire den Begriff Orphischer Kubismus gegeben hatte und das, von der Farbe beherrscht, zur „reinen Malerei“, zur Trennung vom Gegenständlichen führte, beeindruckte und prägte beide Maler. Für Macke war er eine „Offenbarung“, Marc übernahm lediglich gewisse Stilmittel von Delaunay.

Im Dezember 1912 lernte Marc im Berliner Heim seiner Schwiegereltern die Dichterin Else Lasker-Schüler kennen, die geschiedene Frau von Herwarth Walden. Es verband sie bald eine enge Freundschaft, aus der sich ein reger Briefwechsel zwischen dem Prinzen Jussuf von Theben (Else Lasker-Schüler) und Franz Marc bis zum Sommer 1914 entspann. Marc schickte ihr in den folgenden Jahren insgesamt 28 eigenhändig bemalte Postkarten. Das Aquarell "Der Turm der blauen Pferde" war ein Neujahrsgruß auf das Jahr 1913 und ist der einzig erhaltene farbige Entwurf für das gleichnamige, seit 1945 verschollene Ölgemälde.

Im Frühjahr 1913 plante Marc zusammen mit Kandinsky die Herausgabe einer illustrierten Bibel, an der mit deren Zustimmung Alfred Kubin, Paul Klee, Erich Heckel und Oskar Kokoschka beteiligt werden sollten. Marc hatte sich für das Kapitel "Genesis" aus dem 1. Buch Mose entschieden. Es sollte im Piper Verlag als Blaue-Reiter-Ausgabe erscheinen. Die Verhandlungen über die Veröffentlichung scheiterten am Ausbruch des Ersten Weltkriegs.

Am 3. Juni 1913 fand die standesamtliche Heirat mit Maria Franck statt. Marc meldete das Ereignis am Tag danach an Kandinsky: „Ich bedaure, daß ich Ihnen und Klee nicht den Spaß gemacht habe, gestern unsere Trauzeugen zu machen, – die spielen auf dem Münchner Standesamt eine Komödie, die schon die Grenzen des Erlaubten und Vorstellbaren überschreitet.“

Im selben Jahr war Marc wesentlich beteiligt an der Ausrichtung der Ausstellung von Herwarth Waldens Erstem Deutschen Herbstsalon, die ab September 1913 in Berlin stattfand. Dort zeigten 90 Künstler aus Frankreich, Deutschland, Russland, den Niederlanden, Italien, Österreich, der Schweiz und den Vereinigten Staaten ihre Werke. Stark waren das Ehepaar Delaunay, die Mitorganisatoren Marc, Macke und Kandinsky sowie weitere Künstler des Blauen Reiters und die Futuristen vertreten. Marc hatte sieben Gemälde, darunter "Der Turm der blauen Pferde", "Tirol" und "Tierschicksale", dessen Titelgebung von Klee stammt, in die Ausstellung gegeben.

Anfang des Jahres 1914 erhielt Marc das Angebot einer Inszenierung von William Shakespeares "Der Sturm" von Hugo Ball, zu dieser Zeit Dramaturg der Münchner Kammerspiele. Doch bereits am 18. April resignierte er nach kritischen Zeitungsberichten und schrieb an Hugo Ball: „Es müßte doch unbedingt ausgesprochen werden, […] daß wir die Scene selbst […] neu organisieren und nach unserem künstlerischen Vorstellungsleben gestalten wollen.“

Ende April 1914 erwarb Marc eine Villa in Ried bei Benediktbeuern – seit 1918 zu Kochel am See gehörig – im Tausch gegen sein Elternhaus in Pasing. Mit der finanziellen Unterstützung seiner Schwiegermutter kaufte er ein Stück Land hinzu, um den ebenfalls erworbenen Rehen ein Gehege zu bieten. Zum Ausbau eines Ateliers kam es nicht mehr; dennoch entstanden in Ried seine letzten großen Gemälde, teils abstrakt, teils gegenständlich.
Im August wurden Marc und Macke zum Kriegsdienst im Ersten Weltkrieg einberufen. In früheren Biografien wurden sie hierbei als Kriegsfreiwillige bezeichnet, neuere Veröffentlichungen widersprechen jedoch dieser Behauptung. Marcs Truppe wurde Ende des Monats an die französische Front verlegt. Wie viele Künstler und Intellektuelle jener Zeit neigten beide dazu, den Ausbruch des Krieges als „positive Instanz“ zu überhöhen. Macke fiel bereits zwei Monate später. Sein Tod traf Marc tief, änderte aber zunächst nichts an seiner Einstellung. In seinem Nachruf, der erst nach dem Krieg veröffentlicht wurde, drückte er die Trauer um den Freund aus, hielt aber an dieser Opferbereitschaft fest. In seinen „Briefen aus dem Feld“ wird deutlich, dass er ein krankes Europa sah, das durch den Krieg geläutert werden müsse. Ein Sinneswandel setzte erst später ein, wie auch bei vielen anderen Menschen, etwa Max Beckmann. Im Oktober 1915 schrieb Marc einen Brief an Lisbeth Macke, die Witwe seines Freundes. Darin bezeichnete er den Krieg als den „gemeinsten Menschenfang, dem wir uns ergeben haben“.

Ein Antrag Anfang 1916 auf „Freistellung“ vom Kriegsdienst, der später abgelehnt wurde, blieb bedeutungslos: Am 4. März 1916 fiel Franz Marc als Leutnant der Landwehr während eines Erkundungsritts bei Braquis, knapp 20 km östlich von Verdun. Er war von einem Granatsplitter (Granatdoppelschuss) getroffen worden. Am nächsten Morgen wurde Franz Marc im Park des Schlosses Gussainville bei Braquis unter einem schlichten Gedenkstein beigesetzt. 1917 ließ Maria Marc seinen Leichnam nach Kochel am See überführen.

Franz Marc verwendete Techniken wie Ölfarben, Gouachen, Bleistift, Aquarell und schuf Holzschnitte. Seine bevorzugten Motive waren die Tiere als Sinnbild von Ursprünglichkeit und Reinheit, da sie die Idee der Schöpfung verkörpern und im Einklang mit der Natur leben. Er drückte mit diesen Bildern seine Utopie einer paradiesischen Welt aus. Der Farbeinsatz in seinen Werken ist nicht nur expressiv, sondern auch symbolisch, da Marc eigene Farbgesetze aufstellte.

Insgesamt sind 244 Ölgemälde im "Werkverzeichnis I", erschienen bei Beck, aufgeführt. Das fortschreibende "Werkverzeichnis II" führt 261 Zeichnungen und Aquarelle auf, 94 Postkarten, 8 Glasbilder, 17 kunstgewerbliche Entwürfe auf Papier und 11 kunsthandwerkliche Arbeiten, 9 Stickereien und 15 plastische Arbeiten. Einige Arbeiten konnten von den Kunstexperten nicht Franz Marc zugeschrieben werden.

In der Malklasse des aus der Münchner Schule stammenden Künstlers Wilhelm von Diez, der eine virtuose, dunkeltonige Historienmalerei entwickelt hatte, arbeitete Marc 1901 intensiv, wenngleich zurückgezogen. Während aus den Anfängen von Marcs Werk Papierarbeiten ab 1897 bekannt sind, datieren Ölgemälde erst ab 1902. Die Landschaften, die im Sommer 1902 auf der Staffelalm oberhalb von Kochel am See und im Dachauer Moos entstanden, sind vom Naturalismus geprägt. Ein Beispiel seiner traditionellen Malerei ist das Gemälde " Moorhütten im Dachauer Moos" aus dem Jahr 1902, das – akribisch gemalt – von dunklen Braun- und Grüntönen beherrscht wird.

Zwischen 1904 und 1907 suchte Marc nach seinem eigenen Stil. In einem Illustrationszyklus zu einem Band mit Texten von Dichtern wie Richard Dehmel, Carmen Sylva und Hans Bethge setzte er sich mit dem Jugendstil auseinander. Das Buch erschien in einer Auflage von 110 Exemplaren unter dem Titel "Stella Peregrina" postum 1917 beim Verlag Franz Hanfstaengl in München. Annette Simon-von Eckardt hatte 18 Faksimile-Illustrationen Marcs aus dieser Zeit handkoloriert, die Einleitung stammte von Hermann Bahr.

1905 schloss Marc mit dem jungen Schweizer Tiermaler Jean-Bloé Niestlé Freundschaft. Dieser regte ihn dazu an, die Vorliebe für Tiere derart umzusetzen, dass sie nicht als zoologische Darstellungen abzubilden seien; vielmehr solle sich der Künstler in das Tier hineinversetzen und sein Wesen in der Malerei einfangen. Die Begegnung mit Niestlé gab Marc den Anstoß dazu, die Tiermalerei als ein Mittel künstlerischen Ausdrucks fortzuentwickeln. Im selben Jahr entstand als erstes Beispiel "Der tote Spatz".

Zurückgezogen verbrachte er den Sommer 1905 wieder auf der Staffelalm, wo er Bilder in einem wenig farbigen Stil in herkömmlicher Weise mit Licht und Schatten malte. In diesem Jahr kam er über Marie Schnür in Kontakt mit den Künstlern der Scholle, deren Maler in der Nachfolge des Impressionismus eine Variante der Jugendstilmalerei praktizierten. Im Herbst traf er sich in Dachau mit Adolf Hölzel, dem Mitbegründer der Künstlerkolonie Dachau, die damals die Freilichtmalerei pflegte, von der Marc bis Dezember 1910 nicht loskommen sollte.

Franz Marc und Maria Franck verbrachten den Sommer 1908 mit intensiver Malerei in Lenggries. Der Eindruck, den die Kunst van Goghs auf ihn machte, wurde im Dezember 1909 verstärkt durch eine Ausstellung im Kunsthaus Brakl in München, wo er beim Hängen der sieben gezeigten Gemälde aushalf. Er setzte sich mit van Goghs Formensprache auseinander, und das Ergebnis wird mit dem zwischen Dezember 1909 und Anfang Januar 1910 entstandenen Gemälde "Katzen auf rotem Tuch" dokumentiert. Während eines Besuchs in Berlin im Mai 1910 sah er Werke der fauvistischen Maler Henri Matisse und Kees van Dongen und setzte sich anschließend in dem Gemälde "Akt mit Katze" mit dem fauvistischen Stil auseinander. Für das Gemälde diente seine Geliebte Maria Franck als Modell.

Ab 1910, nach seinem Umzug nach Sindelsdorf, konzentrierte sich Marc, für den das Tier zunehmend eine Metapher für kreatürliche Reinheit und Unschuld wurde, in der ländlichen Abgeschiedenheit auf das Tierbild. Nach naturalistischen Anfängen und Erfahrungen mit dem Impressionismus kam er in seinen Bildern und Plastiken um 1909 seinem Ziel der „Animalisierung der Kunst“ um einige Schritte näher. In einem Aufsatz für das 1910 bei Reinhard Piper herausgegebene Buch "Das Tier in der Kunst" schildert Marc:

In dieser Phase suchte Marc, die „Animalisierung“, die Verlebendigung seiner Bilder, durch Schwingungen und Parallelisierungen der Linien zu erreichen, wodurch das Innere, das organische Leben der Tiere in ihrem harmonischen Zusammenhang mit der Umgebung sichtbar wurde. Motive hierfür sind für mehrere Jahre die rhythmisch angeordneten Pferdegruppen auf der Weide, wie es seine in noch naturalistischer Farbigkeit gehaltenen "Weidenden Pferde I" zeigen. Schon Jakob Johann von Uexküll hatte 1909 in seiner Publikation "Umwelt und Innenwelt der Tiere", das der belesene Franz Marc womöglich damals bereits zur Kenntnis genommen hatte, ausführlich das „Innenleben“ der Tiere behandelt. Bei Marc führte dies zu der Frage, wie ein Pferd, ein Adler, ein Reh oder ein Hund die Welt sieht, was zu einer selbstkritischen Einordnung seiner eigenen Konventionen führte – „die Tiere in eine Landschaft zu setzen, die unsren Augen zugehört, statt uns in die Seele des Tieres zu versenken, um dessen Bildkreis zu erraten“.

Das Gemälde "Liegender Hund im Schnee", eine Darstellung von Marcs sibirischem Schäferhund Russi, strahlt beispielsweise völlige Harmonie im Miteinander von Tier und Natur aus; es spiegelt das Einssein zwischen der ihn umgebenden Natur, dem Ruhen des Schnees und dem Ruhen des Hundes auf ihm wider – „eine gemeinsame Stille von belebter und unbelebter Natur.“

1910 rang Marc noch darum, „aus der Beliebigkeit der Farbe herauszukommen“, und gestand am 6. Dezember des Jahres: „[…] aber dazu muß man über die Farbe noch viel mehr wissen und nicht so planlos Beleuchtung fummeln“. Zwei Tage später entsann er sich eines Gesprächs, in dem Marianne von Werefkin Helmuth Macke darüber aufklärte, dass „fast alle Deutschen den Fehler begehen, das Licht für Farbe zu nehmen, während die Farbe etwas ganz anderes ist und mit Licht, d. h. Beleuchtung, überhaupt nichts zu tun hat.“ Durch diese Bemerkung angeregt, begann Marc sich mit der Farbenlehre Johann Wolfgang von Goethes und Wilhelm von Bezolds ebenso auseinanderzusetzen wie mit der Farbensymbolik Philipp Otto Runges, wobei Adolf Erbslöh ihm mit einer „Kleinausgabe des Chevreul“ aushalf.

In regem Briefverkehr mit August Macke schilderte er ausführlich seine Erkenntnisse und die Absicht, sich daraus seine eigene Farblehre zu schaffen. Er formulierte sie in einem Brief an Macke vom 12. Dezember 1910:

1911 entstanden die Gemälde "Blaues Pferd I" (Städtische Galerie im Lenbachhaus, München) und "Blaues Pferd II" (Kunstmuseum Bern). In ihnen macht Marc das Blau von einer „Erscheinensfarbe“ zu einer „Wesensfarbe“. Er fand mit dem Tierbild ein Sinnbild für eine „Spiritualisierung der Welt“. "Das Blaue Pferd" drückt wie die Blaue Blume der Romantik die Suche nach Erlösung von irdischer Schwere und materieller Gebundenheit aus. Im Gegensatz zum Blau verwirklichte Marc seine Vorstellung von Gelb als „das weibliche Prinzip, sanft, heiter, sinnlich“ in dem Gemälde der "Gelben Kuh", ebenfalls aus dem Jahr 1911, das Lebensfreude ausdrückt. Der "Tiger" aus dem Jahr 1912 weist eine kubistische Formensprache auf, die Marc auf der zweiten Ausstellung der N.K.V.M. in den Bildern von Pablo Picasso und Georges Braque kennengelernt hatte. Er wandelte sie expressiv um.

Die bis 1914 entstandenen Bilder nähern sich einer „prismatischen“ und „kristallinen“ Abstraktion an, die aus einer Verschmelzung der Formen des italienischen Futurismus und des Orphismus von Robert Delaunay entstehen. Beispiele hierfür sind die ungegenständlichen "Kleinen Kompositionen", die in vier Motiven zwischen Ende 1913 und Anfang 1914 entstanden.

Die Pferdekörper auf dem seit 1945 verschollenen Werk "Der Turm der blauen Pferde" aus dem Jahr 1913 wirken bereits abstrahiert, setzen sich aus geometrischen Formen zusammen, und der landschaftliche Hintergrund besteht nur noch aus abstrakten Formationen. Stärker noch zeigt sich sein abstrakter Malstil in dem Gemälde "Tierschicksale" aus demselben Jahr, auf dem zwischen spitzen, bedrohlich wirkenden Formen undeutlich Pferde, Schweine und Wölfe hinzustoßen und im Mittelpunkt ein blauweißes Reh erkennbar ist, das seinen Kopf extrem nach oben reckt. Auf der Rückseite des Bildes vermerkte Marc: „Und alles Sein ist flammend Leid“; diesen Text interpretierte er in einem Brief 1915 an seine Frau als Vorahnung des Krieges.

Aus dem Jahr 1914 stammen vier in Ried entstandene ungegenständliche Werke, die Gemälde "Heitere Formen", "Spielende Formen", "Kämpfende Formen" und "Zerbrochene Formen". In den Titeln zeigt sich die Zwiespältigkeit seiner Gefühle. Eine Interpretation der "Kämpfenden Formen" vergleicht die rote Farbfläche links mit einem Adler, der sich auf ein nicht näher definiertes dunkles Wesen stürzt. Gegenständlicher als die "Formen" ist "Tirol", auf dem eine Madonna erkennbar ist. Das letzte Gemälde ist "Rehe im Walde II", das in stark abstrahierter Form drei Rehe auf einer Waldlichtung zeigt. Das Tier hatte für ihn an Bedeutung verloren; in einem Brief aus dem Feld an seine Frau vom 12. April 1915 schrieb Marc:

Reinhard Piper veröffentlichte 1910 in seinem Verlag Marcs Essay "Über das Tier in der Kunst". Marc schrieb an den Verleger:

Im Sommer 1911 erschien die Schrift "Im Kampf um die Kunst" beim Piper Verlag, in der Kandinsky und Marc neben anderen Künstlern, Galerieleitern und Schriftstellern als Antwort Beiträge gegen die polemische Schrift "Ein Protest deutscher Künstler" von Carl Vinnen verfassten. Vinnen hatte sich anlässlich des Ankaufs eines van-Gogh-Gemäldes durch den Direktor der Kunsthalle Bremen, Gustav Pauli, im April 1911 gegen die „Überfremdung deutscher Kunst“ gewandt, und er gewann in seinem Aufruf unter anderem die Unterschriften von Thomas Theodor Heine, Franz von Stuck und Käthe Kollwitz. Die Kontroverse wurde bekannt unter dem Begriff Bremer Künstlerstreit.

Im März 1912 erschien in der Kunstzeitschrift "Pan" Marcs Artikel über „Die Neue Malerei“, in dem er nachweisen wollte, dass seine als „neu“ bezeichnete Malerei nicht auf den Impressionismus zurückzuführen sei, sondern höchstens, und dann auch nur bedingt, auf Paul Cézanne. Jede Zeit habe ihre Qualität und über den künstlerischen Wert oder Unwert der neuen malerischen Ideen müsse diskutiert werden. Max Beckmann, der Marc bei "Im Kampf um die Kunst" noch beigestanden hatte, kritisierte im folgenden "Pan" die Ausführungen von Marc, indem er die Werke von Gauguin, Matisse und Picasso in die Nähe des Kunstgewerbes rückte und mit dem Satz schloss: „Die Gesetze der Kunst sind ewig und unvergänglich, wie das moralische Gesetz in uns“. Marc antwortete im "Pan", doch es prallten zwei Standpunkte aufeinander, die auch in der Gegenwart noch ausgefochten werden.

Im Mai 1912 erschien der Almanach " Der Blaue Reiter" mit einer Widmung für Hugo von Tschudi in einer Auflage von 1200 Exemplaren, den Kandinsky mit drei langen Beiträgen abschloss. Das Wort „Almanach“ musste auf Wunsch des Verlegers Piper aus dem Titelholzschnitt von Kandinsky vor Drucklegung entfernt werden. Das Werk wurde nicht, wie ursprünglich geplant, ein jährlich erscheinendes Organ, sondern erlebte 1914 nur einen Nachdruck. 141 Bildreproduktionen, 19 Textbeiträge und drei Musikbeilagen waren im Buch gelistet. Marc war mit Abbildungen seiner Gemälde und mit drei kurzen einleitenden Kapiteln vertreten.

In "Geistige Güter" klagte er darüber, dass geistiges Gut weniger geschätzt werde als materielles. Im zweiten Artikel, "Die „Wilden“ Deutschlands", erklärte er, dass die modernen Künstler – die „Wilden“, angelehnt an die Fauves – der Brücke, der Neuen Secession in Berlin und der Neuen Künstlervereinigung München konsequent den Weg der geistigen Erneuerung der Kunst gingen:

Im letzten Kapitel "Zwei Bilder" stellte Marc eine Illustration aus Grimms Märchen von 1832 dem Gemälde "Lyrisches" von Kandinsky aus dem Jahr 1911 gegenüber. Beide Bilder seien „von ganz gleich tiefer Innerlichkeit des künstlerischen Ausdrucks“. Die Autoren neben den beiden Herausgebern Marc und Kandinsky wie Delaunay, Macke und Schönberg lieferten Texte und Bildbeispiele aus unterschiedlichen Bereichen der Bildenden Kunst, Volkskunst, Musik und des Theaters. Der Almanach von Künstlern für Künstler wurde eine der wichtigsten deutschsprachigen programmatischen Schriften für die Kunst des 20. Jahrhunderts; er wurde in allen Weltsprachen veröffentlicht.

Für den Ersten Deutschen Herbstsalon im September 1913 schrieb Marc stellvertretend für seine ausstellenden Künstlerkollegen, neben Herwarth Waldens Vorrede, ein Vorwort für den Katalog.

Unter dem Titel "Im Fegefeuer des Krieges" erschien am 15. Dezember 1914 in der "Vossischen Zeitung" Franz Marcs erste Kriegsschrift; er hatte sie im Oktober desselben Jahres während eines wegen Dysenterie erforderlichen Aufenthaltes im Lazarett verfasst. Im darauffolgenden Jahr erschien unter dem Titel "Das geheime Europa" seine zweite Kriegsschrift. Darin: „Der Krieg geht umher. Europa ist krank am alten Erbübel und will gesund werden, darum will es den furchtbaren Blutgang […] Um Reinigung wird der Krieg geführt und das kranke Blut vergossen.“

Marcs Briefe aus dem Felde und sein Skizzenbuch, die einzige bildliche Äußerung aus der Kriegszeit, wurden unter dem Titel "Franz Marc, Briefe, Aufzeichnungen und Aphorismen" im Jahr 1920 bei Paul Cassirer in Berlin herausgegeben.

In seinem Beitrag "Über die Formfrage" hob Kandinsky im "Almanach" die Bedeutung Marcs für die abstrakte Kunst anhand des dort abgebildeten Gemäldes "Der Stier" hervor und betonte das „starke abstrakte Klingen der körperlichen Form“, das nicht nach Zerstörung des Gegenständlichen verlange, sondern vielmehr „seine einzelnen Teile“ zu einem „gesamten abstrakten Hauptklang“ vereine.

Paul Klee schrieb 1916 in Gedenken an Franz Marc in sein Tagebuch: „Wenn ich sage, wer Franz Marc ist, muss ich zugleich bekennen, wer ich bin, denn vieles, woran ich teilnehme, gehört auch ihm. Menschlicher ist er, er liebt wärmer, ausgesprochener. Zu den Tieren neigt er sich menschlich. Er überhöht sie zu sich.“

Die Lyrikerin Else Lasker-Schüler veröffentlichte im "Berliner Tageblatt" vom 9. März 1916 einen Nekrolog, beginnend mit den Zeilen: „Der blaue Reiter ist gefallen, ein Großbiblischer, an dem der Duft Edens hing. Über die Landschaft warf er einen blauen Schatten. Er war der, welcher die Tiere noch reden hörte; und er verklärte ihre unverstandenen Seelen“. Einen weiteren Nachruf publizierte sie im folgenden Jahr: "Als der blaue Reiter war gefallen …", ein Gedicht, das seine Entstehung dem vermeintlichen Verlust des Bildes "Tierschicksale", das 1917 bei einem Brand schwer beschädigt worden war, verdankte. 1919 erschien ihr Franz Marc gewidmeter Roman "Der Malik. Eine Kaisergeschichte mit Bildern und Zeichnungen" im Verlag Paul Cassirer in Berlin.

In einem Brief an Marie-Anne von Goldschmidt-Rothschild vermerkte Rainer Maria Rilke im Herbst 1916 nach einem Besuch der Münchner Gedächtnisausstellung, „endlich wieder einmal ein oeuvre“ gesehen zu haben, „eine im Werk erreichte und errungene Lebens-Einheit.“

Vom 14. September bis zum 15. Oktober 1916, ein halbes Jahr nach Marcs Tod, wurde in der Münchener Neuen Secession – dem Ausstellungsgebäude der 1913 entstandenen Künstlergruppe – die „Franz Marc-Gedächtnis-Ausstellung“ gezeigt. Es folgte im November die Gedächtnisausstellung in Herwarth Waldens Sturm-Galerie in Berlin, die fast 200 Werke des Künstlers, einschließlich der "Tierschicksale", enthielt. Bei einer Zwischenlagerung wurde das Gemälde 1917 durch einen Brand teilweise zerstört und 1919 von Paul Klee restauriert. Auf der 16. Biennale in Venedig im Jahr 1928 waren Werke von ihm ausgestellt.

In der Zeit des Nationalsozialismus bestand bis zum Ende der Olympischen Spiele im Sommer 1936 für die Kunstwelt eine Schonfrist. Zum 20. Todestag Franz Marcs veranstaltete die Kestnergesellschaft in Hannover vom 4. März bis zum 14. April 1936 eine Gedächtnisausstellung, in der 165 Werke des Künstlers gezeigt wurden. Sie waren Bestandteil des ersten Werkverzeichnisses von Marc, das der Kunsthistoriker Alois Schardt zusammen mit seiner Frau und Marcs Witwe, Maria Marc, verfasste und noch 1936 in Berlin herausbrachte. Es enthält insgesamt 996 Arbeiten. Nach Hannover war sie ab dem 4. Mai in den Galerien Nierendorf und von der Heyde in Berlin zu sehen. Der Einführungsvortrag von Alois Schardt am Vorabend der Eröffnung wurde von der Gestapo verboten, Schardt verhaftet und sein gerade erschienenes Buch über Marc konfisziert. Das Gemälde "Die kleinen blauen Pferde" von 1911 war in beiden Gedächtnisausstellungen zu sehen und hängt heute in der Staatsgalerie Stuttgart. Das Bild gehörte dem Kunstsammler und Mäzen Alfred Hess, und die Eigentumsverhältnisse sind, wie bei einigen weiteren Bildern Marcs, noch nicht abschließend geklärt.

1936/37 wurden 130 Werke von Franz Marc aus deutschen Sammlungen konfisziert. Insgesamt 650 Werke verschiedener Künstler aus 32 deutschen Museen wurden ab dem 19. Juli 1937 zeitgleich mit der Großen Deutschen Kunstausstellung im neu eröffneten Haus der Deutschen Kunst im nahegelegenen Galeriegebäude am Hofgarten in der Ausstellung „Entartete Kunst“ ausgestellt. Von Marc wurden sechs Gemälde gezeigt. Noch vor der Veräußerung der Werke ins Ausland wählte Hermann Göring 13 Gemälde für seine Sammlung aus, darunter "Hirsche im Walde" und "Der Turm der blauen Pferde". Beide Bilder sind seit 1945 verschollen.

Nach dem Zweiten Weltkrieg begann die Malerei Franz Marcs populär zu werden, und die Tierbilder aus den Jahren 1911/12 hingen als Kunstkopien bald in vielen Wohnungen. Einige seiner Werke wurden auf der documenta 1 (1955), der documenta II (1959) und der documenta III im Jahr 1964 in Kassel gezeigt. Die Bedeutung des Künstlers als einer der Vorreiter der abstrakten Kunst in seinen späteren Werken erschloss sich jedoch erst ab 1976 durch die Monografie von Klaus Lankheit und die Gedächtnisausstellung 1980 im Münchner Lenbachhaus.

Eine Retrospektive des malerischen und grafischen Werks von Franz Marc, die größte nach der Gesamtschau im Jahr 1916, wurde am 17. September 2005 im Lenbachhaus und im zugehörigen Kunstbau eröffnet. Sie erreichte bis zum 8. Januar 2006 mit rund 300.000 Besuchern einen Besucherrekord.
"Das Blaue Jahr – 100 Jahre Blauer Reiter", unter diesem zusammenfassenden Titel bot das Franz Marc Museum in Kochel im Jahr 2011 neben Sammlungspräsentationen unter anderem Sonderausstellungen von Ernst Ludwig Kirchner und Paul Klee sowie ab dem 18. September 2011 "Franz Marc und Joseph Beuys. Im Einklang mit der Natur". Die Ausstellung, die anschließend ab dem 8. Dezember 2011 bis zum 12. Februar 2012 im Sinclair-Haus der Altana-Kulturstiftung in Bad Homburg zu sehen war, machte deutlich, dass Joseph Beuys und Franz Marc in ihrem Denken und Werk durch Naturverbundenheit geprägt sind und sich in ihren Werken gemeinsame Ausgangspunkte eines in der Tradition der deutschen Romantik verwurzelten Naturbegriffs widerspiegeln. Wie das Pferd oder das Reh bei Marc zum Symbol des Spirituellen wird, so sind bei Beuys Hirsch, Schwan, Biene und Hase Symbolträger einer eigenen Mythologie, erwachsen aus christlichen, literarischen und naturwissenschaftlichen Zusammenhängen und mit einer gesellschaftlichen Relevanz aufgeladen.

Ebenfalls im "Blauen Jahr" wies das Schloßmuseum Murnau in der Ausstellung "Die Maler des „Blauen Reiter“ und Japan", die vom 21. Juli bis zum 6. November 2011 lief, erstmals auf den Einfluss japanischer Kunst auf die Künstler des Blauen Reiters hin. Sammlungsstücke der Maler, darunter die japanische Kunstsammlung Franz Marcs, sowie Werkbeispiele bildeten das Spektrum der Ausstellung, die den Zusammenhang zum „klassischen Japonismus“ herstellte.

Unter dem Gemälde "Die blauen Fohlen" aus dem Jahr 1913 wurde im Sommer 2013 eine Studie von zwei Katzen entdeckt, vermutlich ebenfalls 1913 von Marc gemalt. Es wird in der Kunsthalle in Emden ab dem 3. Oktober des Jahres gezeigt.

Am 5. November 2013 wurde auf einer im Fernsehen übertragenen Pressekonferenz zum spektulären Schwabinger Kunstfund eine Studie zu den "Großen blauen Pferden" aus dem Jahr 1911 mit dem Titel "Pferde in Landschaft" gezeigt, die aus dem Besitz des Kunsthändlers Hildebrand Gurlitt stammt. Früherer Eigentümer war das Kunst- und Gewerbemuseum Moritzburg in Halle (Saale).

1949 bat Maria Marc den Galeristen Otto Stangl, den künstlerischen Nachlass ihres Mannes zu verwalten. Nach dem Tod von Maria Marc am 25. Januar 1955 wurde Stangl „Hüter des Franz-Marc-Nachlasses“; gemäß dem Vermächtnis der Witwe schenkte er einigen bedeutenden Museen eine von ihr festgelegte Anzahl von Gemälden.

Der schriftliche Nachlass Franz Marcs wurde 1973 vom Deutschen Kunstarchiv im Germanischen Nationalmuseum Nürnberg von der Galerie Stangl, München, käuflich erworben. Eine Komplettierung durch 200 Schriftstücke erfolgte als Schenkung eines Erben Stangls im Jahr 2005.

In Kochel am See befindet sich das 1986 gegründete und 2008 erweiterte Franz Marc Museum. Der Nachlassverwalter Otto Stangl hatte bereits bei der Gründung des Franz Marc Museums die Vision, es später zu erweitern, um die Fortsetzung der dem Blauen Reiter wichtigen Idee des „Geistigen in der Kunst“ durch die Abstraktion der Nachkriegszeit verständlich zu machen. Die Stiftung Etta und Otto Stangl vermachte dem Museum viele Werke, unter anderem Gemälde seiner Künstlerfreunde aus dem Umfeld des Blauen Reiters. Die unterschiedlichen Einflüsse auf die Kunst Franz Marcs sowie die Anregungen, die von ihr ausgingen, werden mit Werkbeispielen im Franz Marc Museum präsentiert.

Die von Franz Marc 1903 in Paris erworbene Kollektion japanischer Farbholzschnitte ist nicht vollständig erhalten. Aus seinem Nachlass gelangten 2009 21 Tuschzeichnungen und Holzschnitte sowie 17 illustrierte Bücher an das Schloßmuseum Murnau. Seit 1908 ließ Marc sich zumindest in drei chinesische und japanische Specksteine nach ostasiatischer Art seinen Namen bzw. sein Monogramm schneiden, um sie als Stempel auf Postkarten und Briefen zu verwenden.

Franz Marc erfuhr vor allem im letzten Drittel des 20. Jahrhunderts verstärkte öffentliche Aufmerksamkeit.

In mehreren Städten Deutschlands sind Straßen nach Marc benannt, so in Fulda, Fürth, Hamburg, Landshut, Wolfsburg, Oldenburg, Puchheim, Vechta, Elmshorn, Heidelberg, Kochel am See, Köln, Kösching, Leverkusen, Mühlheim, Saarbrücken, Schifferstadt, Schweinfurt, Sindelsdorf, Töging und München. 

Am 13. Oktober 2000 wurde ein 1991 entdeckter Asteroid nach ihm benannt: (15282) Franzmarc. 

Die Deutsche Bundespost brachte am 15. Februar 1974 im Rahmen einer Doppelausgabe zum Deutschen Expressionismus eine Briefmarke mit den "Roten Rehen" im Wert von 30 Pfennig heraus, wobei die zweite Marke zu 40 Pfennigen Alexej von Jawlenskys "Kopf in Blau" zeigt. Anlässlich des 100. Jahrestags des "Blauen Reiters" gab die Deutsche Post AG eine Sondermarke im Wert von 145 Eurocent heraus. Ausgabetag war der 9. Februar 2012, der Entwurf stammt von der Kommunikationsdesignerin Nina Clausing aus Wuppertal und orientiert sich am Werk "Blaues Pferd I" aus dem Jahr 1911 von Franz Marc.

Das 1986 eröffnete Franz Marc Museum wurde dem Künstler gewidmet. Drei Jahre später, 1989, wurde das frühere Gymnasium Markt Schwaben zu Ehren des Künstlers in Franz-Marc-Gymnasium umbenannt. In der Ruhmeshalle München ist seine Büste aufgestellt. 

Der Freistaat Bayern feierte im Jahr 2011 zwei Jubiläen, den 125. Todestag des „Märchenkönigs“ Ludwig II. und zugleich den 100. Geburtstag des "Blauen Reiters". Viele Ausstellungen in Museen zeigten in Sonderschauen die Werke der beteiligten Künstler, beispielsweise das Schloßmuseum Murnau, das Franz Marc Museum, das Buchheim Museum in Bernried und das Stadtmuseum Penzberg. Im Jahr 2014, anlässlich des 100. Todesjahrs von Franz Marcs Künstlerfreund August Macke, eröffnete das Kunstmuseum Bonn die Ausstellung „August Macke und Franz Marc. Eine Künstlerfreundschaft“. Erstmals präsentierte sie etwa 200 Werke, die sich ausschließlich auf die Freundschaft der beiden Künstler und ihre Kunst beziehen. Von Januar bis Mai 2015 wurde sie im Münchner Lenbachhaus gezeigt. Zum 100. Todestag des Künstlers am 4. März 2016 widmete ihm das Franz Marc Museum eine Ausstellungstrilogie unter dem Sammeltitel „Franz Marc – Zwischen Utopie und Apokalypse“, deren dritter Teil im Januar 2017 endete.

Im Februar 2008 erreichte die Versteigerung von "Weidende Pferde III" aus dem Jahr 1910 bei Sotheby’s in London einen Rekordpreis von umgerechnet 16,5 Millionen Euro. Das war der doppelte Schätzpreis. Der Bieter blieb unbekannt.

Im Juni 2009 erzielte eines der letzten impressionistischen Gemälde Marcs, "Springende Pferde", ebenfalls aus dem Jahr 1910, auf einer Versteigerung bei Christie’s in London umgerechnet 4,4 Millionen Euro. Es blieb damit knapp unter dem Schätzpreis.

Gemälde

Skulptur

Druckgrafik


Der Blaue Reiter

Briefwechsel, Schriften und Dokumente

Werkverzeichnisse





</doc>
<doc id="9568" url="https://de.wikipedia.org/wiki?curid=9568" title="Hopfen">
Hopfen

Hopfen ("Humulus") ist eine Pflanzengattung aus der Familie der Hanfgewächse (Cannabaceae). Alle Hopfen-Arten kommen auf der Nordhalbkugel vor. Der bekannteste Vertreter der Gattung ist der Echte Hopfen, der zum Bierbrauen verwendet wird.

Die "Humulus"-Arten sind schnellwachsende einjährige bis ausdauernde krautige Kletterpflanzen, die – von oben betrachtet – im Uhrzeigersinn winden. Sie gehören also, wie z. B. auch die Geißblatt-Arten, zu den rechtswindenden Schlinggewächsen. Die Stängel und Blattstiele besitzen steife, zweiarmige gestielte Haare ("Klimmhäkchen"). Die Stängel sind grob, sechsrippig bis geflügelt. Die gegenständigen, gestielten Laubblätter sind mehr oder weniger herzförmig und meist drei- bis sieben-, selten bis neunlappig. Es sind Nebenblätter vorhanden.

"Humulus"-Arten sind zweihäusig getrenntgeschlechtig (diözisch). 
Die männlichen Blüten stehen in lockeren, rispigen Blütenständen zusammen. Die weiblichen Blüten stehen in zapfenförmigen, ährigen zymösen Blütenständen zusammen. Ihre Hochblätter vergrößern sich nach der Blütezeit. Die Nussfrüchte sind breit eiförmig, wobei der Kelch noch vorhanden ist.

Die Gattung enthält nur drei Arten:




</doc>
<doc id="9569" url="https://de.wikipedia.org/wiki?curid=9569" title="Amedeo Modigliani">
Amedeo Modigliani

Amedeo Clemente Modigliani (* 12. Juli 1884 in Livorno; † 24. Januar 1920 in Paris) war ein italienischer Zeichner, Maler und Bildhauer. Die heutige Bekanntheit beruht vor allem auf seinen Aktgemälden, die zu seiner Zeit als skandalös empfunden wurden und erst später Akzeptanz fanden. Seine Jugend verbrachte Modigliani in Italien, wo er die Kunst der Antike und Renaissance studierte, bis er 1906 nach Paris zog. Dort kam er in Kontakt mit bedeutenden Künstlern wie Pablo Picasso und Constantin Brâncuși. Sein Leben war von Lungenkrankheiten geprägt. In einem Fiebertraum soll er seine Berufung zur Kunst erkannt haben, mit 35 Jahren starb er an Tuberkulose. Die Informationen über Modiglianis Leben beruhen auf nur wenigen verbürgten Dokumenten, so dass es vor allem nach seinem Tod zur Legendenbildung um ihn kam.

Modiglianis Gesamtwerk umfasst vor allem Gemälde und Zeichnungen. Von 1909 bis 1914 widmete er sich jedoch hauptsächlich der Bildhauerei. Das Hauptmotiv ist der Mensch, sowohl in den Bildern als auch bei den Skulpturen. Daneben gibt es wenige Bilder mit Landschaftsmotiven. Interieurszenen und Stillleben von Modigliani sind nicht bekannt. Modigliani bezog sich in seinen Werken oft auf die Renaissance, griff aber auch andere Elemente wie die zu seiner Zeit populäre afrikanische Kunst auf. Hingegen lässt er sich keiner der zeitgenössischen Stilrichtungen zuordnen, wie etwa dem Kubismus oder dem Fauvismus. Während seines Lebens hatte Amedeo Modigliani nur wenig Erfolg mit seiner Kunst, erst nach seinem Tod erreichte er größere Popularität und seine Kunstwerke erzielten hohe Preise.

Amedeo Modigliani wurde als viertes und jüngstes Kind von Flaminio und Eugenia Modigliani geboren. Einer seiner Brüder war Giuseppe Emanuele Modigliani, der später Politiker des "Partito Socialista Italiano" und Abgeordneter des italienischen Parlaments wurde. Die Familie Modigliani gehörte dem aufgeklärten jüdischen Bürgertum der Stadt an. Als sephardische Juden lebten die Mitglieder der Familie nach einer liberalen Auslegung ihres Glaubens. Als Amedeo Modigliani geboren wurde, war der mit Holz und Kohle handelnde Familienbetrieb infolge der schlechten Konjunktur bereits bankrottgegangen. Deshalb trug Modiglianis Mutter als Privatlehrerin und Übersetzerin – unter anderem von Gedichten Gabriele D’Annunzios – zum Familienunterhalt bei. Daneben verfasste sie unter einem Pseudonym Literaturkritiken. Amedeo Modigliani nahm wahrscheinlich an den traditionellen Fünf-Uhr-Tees im Haus seines Großvaters Isaac Garsin teil, bei denen beispielsweise über Werke von Oscar Wilde diskutiert wurde. Da seine Mutter aus Marseille stammte, lernte Amedeo Modigliani bereits früh die französische Sprache, was ihm später seine Integration in Paris erleichterte.

Im Alter von elf Jahren litt Amedeo Modigliani an einer schweren Rippenfellentzündung. 1898, im Alter von 14 Jahren, erkrankte er an Typhus, der zu dieser Zeit noch als tödliche Krankheit galt. Während der Krankheit hatte er laut der Darstellung seiner Mutter einen Fiebertraum, in dem er über die künstlerischen Meisterwerke in Italien phantasiert und der ihm damit seine künstlerische Bestimmung aufgezeigt habe. Nachdem Amedeo Modigliani wieder gesund war, erhielt er von seinen Eltern die Erlaubnis, die Schule abzubrechen und ein Kunststudium zu beginnen.

Amedeo Modigliani schrieb sich 1898 an der privaten Zeichen- und Malschule des Malers Guglielmo Micheli in Livorno ein. Dort war er mit seinen 14 Jahren der jüngste Student in seiner Klasse. Neben der künstlerischen Ausbildung an der Schule, die sich noch stark am Impressionismus orientierte, lernte er im Atelier von Gino Romiti das Aktmalen. Im Juli 1900 erkrankte er an Tuberkulose. Weil die Luftveränderung seine Genesung begünstigen sollte, verbrachte er den Winter 1900/1901 zusammen mit seiner Mutter auf einer Reise nach Neapel, Capri und Rom. Von dort aus schrieb Amedeo Modigliani fünf Briefe an den neun Jahre älteren Künstler Oscar Ghiglia, mit dem er befreundet war. Diese Briefe gehören zu den wenigen schriftlichen Dokumenten Modiglianis, die erhalten geblieben sind. In ihnen schilderte er unter anderem seinen Eindruck von Rom: „Rom ist nicht um mich, während ich Dir erzähle, sondern in mir, gleich einem von seinen sieben Hügeln wie von sieben gebieterischen Ideen eingefassten schrecklichen Juwel.“

Im Frühjahr 1901 folgte Amedeo Modigliani seinem Freund Ghiglia nach Florenz. Nachdem er den Winter 1901/1902 in Rom verbracht hatte, kehrte er nach Florenz zurück und schrieb sich am 7. Mai 1902 an der "Scuola libera di Nudo" (deutsch: freie Aktzeichenschule) ein. Dort studierte er bei Giovanni Fattori und beschäftigte sich daneben hauptsächlich mit der Kunst der Renaissance. 1903 ging er, wiederum Ghiglia folgend, nach Venedig, wo er bis zu seiner Übersiedlung nach Paris lebte. Er schrieb sich am 19. März dieses Jahres am "Istituto di Belle Arti di Venezia" ein. Dort belegte er unter anderem Kurse der Freien Aktzeichenklasse. Sein Schwerpunkt lag auf dem Studium der italienischen Kunstgeschichte, die Malerei betrieb er weniger intensiv. 1903 und 1905 kam er auf den Biennalen in Kontakt mit den Werken der französischen Impressionisten, mit Skulpturen Rodins und Werken des Symbolismus. Während seiner Studienzeit in Venedig begann Modigliani Haschisch zu konsumieren, und nahm an spiritistischen Sitzungen teil.

Zu Beginn des Jahres 1906 zog Amedeo Modigliani nach Paris, da die dortige Künstlerszene als besonders fortschrittlich galt und neue Kunststile entwickelte und etablierte. Im vorangegangenen Jahr hatten noch relativ unbekannte Künstler, wie zum Beispiel André Derain und Henri Matisse, die von der Kritik mit dem Spottnamen "Les Fauves" (deutsch: "Die Wilden") bezeichnet worden waren, im Herbstsalon ausgestellt. Die Pariser Kunstwelt wurde mit fortschrittlichen Kunsthändlern, die junge und innovative Künstler wie Pablo Picasso unterstützten, zum Zentrum der avantgardistischen Malerei. Nach seiner Ankunft lebte Modigliani anfangs in einem komfortablen Hotel am rechten Seineufer, was ihm wegen seiner Herkunft aus einer bürgerlichen Familie angemessen erschien. Nach kurzer Zeit zog er in das Viertel Montmartre, wo er unter anderem im Bateau-Lavoir lebte und ein einfaches Atelier nutzte, und nahm Aktzeichenunterricht an der Académie Colarossi. Seine Mutter schickte ihm zwar so viel Geld, wie es ihr möglich war, aber es reichte für Modigliani nicht zum Überleben. Deshalb wechselte er oft seine Unterkunft und ließ manchmal seine Kunstwerke zurück, wenn er aus einer Wohnung floh, weil er die Miete nicht mehr bezahlen konnte. Eine der ersten Freundschaften, die Modigliani in Paris schloss, verband ihn mit dem deutschen Maler Ludwig Meidner. Dieser beschrieb später Modiglianis Stellung und Auftreten in der Pariser Gesellschaft: „Unser Modigliani […] war ein charakteristischer und gleichzeitig hoch begabter Vertreter der Bohème vom Montmartre; wahrscheinlich sogar der letzte echte Bohémien.“ Modigliani nahm trotz seiner gesundheitlichen Probleme am ausschweifenden Leben der Künstler am Montmartre teil.

Im Frühjahr 1907 wurde Modigliani von dem Maler Henri Doucet in ein Haus mitgenommen, das Paul Alexandre für junge Künstler angemietet hatte. Der junge Arzt Alexandre war von Modiglianis Bildern fasziniert und begann deshalb, ihn zu unterstützen. Er kaufte ihm Bilder und Zeichnungen ab und vermittelte ihm Porträtaufträge. Modigliani stellte 1907 im avantgardistischen Herbstsalon aus, der von den Fauvisten geprägt war. Im Folgejahr zeigte er sechs Gemälde im Salon des Indépendants, darunter "Die Jüdin". Seine Bilder fanden jedoch kaum Beachtung. So erwähnte der einflussreiche Publizist Guillaume Apollinaire Amedeo Modigliani nur am Rande seiner Kritik des Salons. Paul Alexandre gelang es, Modigliani Zugang zu vermögenden Kreisen zu verschaffen, so dass er im Frühjahr 1909 mit dem Porträt "Die Amazone" der Baronin Marguerite de Hasse de Villers den ersten bezahlten Auftrag erhielt.

1909 lernte Modigliani über Paul Alexandre den rumänischen Bildhauer Constantin Brâncuși kennen und bezog auf dessen Anraten im April 1909 sein Atelier in der Cité Falguière am Montparnasse. Modigliani begann in Folge dieser Bekanntschaft im selben Jahr mit der Steinbildhauerei, die für einige Zeit in den Vordergrund seines Schaffens trat. In Livorno und in Carrara hatte er sich, beeindruckt von dem knappen Stil Brâncuşis, zur Bildhauerei hingezogen berufen. Ebenfalls könnte Modigliani schon vorher den Wunsch gehabt haben, als Bildhauer tätig zu werden, hatte jedoch nicht die technischen Möglichkeiten, die erst mit dem neuen Atelier gegeben waren. Auch könnte das antike Erbe Italiens, das er aus eigener Erfahrung kannte, eine Inspiration zur Anfertigung von Skulpturen gewesen sein. Eine weitere Möglichkeit wäre, dass Modigliani sich wegen des stagnierenden Erfolgs seiner Malerei in einer anderen künstlerischen Gattung versuchen wollte. 

1910 lernte Modigliani die aus Russland stammende Dichterin Anna Achmatova kennen, mit der er in der folgenden Zeit ein Verhältnis hatte. 1911 stellte er seine archaisch wirkenden Steinskulpturen im Atelier des portugiesischen Künstlers Amadeo de Souza-Cardoso aus. Eine Phase der intensiven Beschäftigung mit dem Motiv der Karyatiden in seinen Werken, sowohl in der Skulptur, als auch in Gemälden, begann. Im folgenden Jahr wurden die Skulpturen Modiglianis im Herbstsalon ausgestellt. Modigliani lernte die Bildhauer Jacob Epstein und Jacques Lipchitz, die ebenfalls in Paris lebten, kennen, von denen letzterer die Kunst Modiglianis als „Ausdruck seines persönlichen Empfindens“ beschrieb. Im Frühling 1913 hielt sich Amedeo Modigliani in Livorno auf, wo er in der Nähe eines Steinbruchs Quartier bezog. In diesem betätigte er sich als Marmorbildhauer, nachdem er zuvor nur mit Kalksandstein gearbeitet hatte. Die fertig gestellten Skulpturen schickte er nach Paris; sie wurden jedoch nicht überliefert. 

Für die Beendigung seiner Bildhauertätigkeit nach 1913 sind die genauen Gründe nicht bekannt. Ein Anlass könnte seine angeschlagene Gesundheit gewesen sein, die durch die staubige Umgebung weiter geschädigt wurde. Auch könnte er keine Zukunft für seine Arbeit als Bildhauer gesehen haben. Er entwickelte sich künstlerisch nicht weiter und die wenigen Ausstellungen brachten kaum Aufmerksamkeit und finanzielle Verbesserungen. So könnte er sich aus diesen Überlegungen heraus wieder der lukrativeren Malerei zugewandt haben.

Im Frühjahr 1914 lernte Amedeo Modigliani den Kunsthändler Paul Guillaume kennen, der einige junge und noch unbekannte Künstler vertrat. Guillaume übernahm auch die Vertretung Modiglianis, nachdem dieser mit Beginn des Ersten Weltkrieges Paul Alexandre aus den Augen verloren hatte, und beteiligte ihn an mehreren Gruppenausstellungen in seiner Galerie. Mit Kriegsbeginn meldete sich Modigliani freiwillig zum Kriegsdienst, wurde jedoch aufgrund seines schlechten Gesundheitszustandes nicht eingezogen. Deshalb gehörte er zu dem kleiner gewordenen Kreis von Künstlern, die sich in Paris aufhielten. 

Im Juni 1914 lernte Modigliani die englische Literatin Beatrice Hastings kennen, mit der ihn über zwei Jahre eine Liebesbeziehung verband. Sie hielt sich in Paris als Kolumnistin der englischen Zeitung "The New Age" auf und schrieb über das Gesellschaftsleben der Stadt. Sie beschrieb unter anderem Modiglianis Konsum von Haschisch und Alkohol, unter dem er „niemals etwas Gutes“ vollbrachte. Während der turbulenten Beziehung mit Beatrice Hastings verstärkte sich Modiglianis exzessives Leben noch weiter. Sein Konsum von Alkohol und Opium, den er mit seinen Freunden Maurice Utrillo und Chaim Soutine teilte, wurde in der Presse aufgegriffen.

1915 zog Modigliani mit Beatrice Hastings in die Rue Norvaine an der Butte Montmartre und porträtierte Pablo Picasso. Ein Jahr später folgten weitere Porträts berühmter Persönlichkeiten, darunter sein Freund Jacques Lipchitz sowie Chaim Soutine, für den Modigliani ebenfalls ein enger Freund und Unterstützer war. Mit diesen Porträts der Avantgarde von Paris war Modigliani selbst mit ihr verbunden. Sie sicherte ihm einen singulären Platz unter den Pariser Künstlern, da er mit seinen Porträts ein Bild dieser Szene festhielt, und ermöglichte die spätere Legende von Modigliani als Hauptfigur der Pariser Künstlerschaft. Daneben lernte Amedeo Modigliani auf Vermittlung des befreundeten Künstlers Moïse Kisling den polnischen Kunsthändler und Dichter Leopold Zborowski kennen. Dieser verfügte als Händler zwar nicht über die Kontakte Guillaumes und dessen Gespür für die avantgardistische Malerei, dennoch unterstützte er Modigliani in dessen letzten Lebensjahren. So nahmen er und seine Frau Anna den Künstler in ihre Wohnung auf, nachdem er sich von Beatrice Hastings getrennt hatte. Zborowski bezahlte Modigliani ein Tagegeld und das Malmaterial und ließ ihn in seiner Wohnung arbeiten. Später bezahlte er auch die Modelle für die Aktgemälde Modiglianis.

Amedeo Modigliani fertigte 1916 und 1917 eine Serie von etwa 30 Aktgemälden an. Auf Vermittlung von Leopold Zborowski wurden diese Bilder in einer Einzelausstellung in der Galerie der Kunsthändlerin Berthe Weill gezeigt. Am 3. Dezember 1917 wurde die Ausstellung mit einer Vernissage mit geladenen Gästen eröffnet. Die Galerie lag gegenüber einer Polizeistation und ein Kommissar wurde auf den Menschenauflauf aufmerksam, der sich infolge eines im Schaufenster präsentierten Aktes bildete. Er rief Berthe Weill zu sich und forderte sie auf, die Ausstellung zu beenden und die Bilder abzuhängen, weil diese zu freizügig seien. Um eine Beschlagnahmung der Bilder zu verhindern, kam Weill der Aufforderung nach.

Im April 1917 lernte Modigliani die 19-jährige Jeanne Hébuterne kennen, die an der "Académie Colarossi" studierte. Die beiden bezogen kurze Zeit später eine gemeinsame Wohnung. 1918 verließen sie zusammen mit dem Ehepaar Zborowski und Modiglianis Freund Soutine Paris, als eine Invasion deutscher Truppen drohte. Daneben könnte auch Zborowskis Interesse als Kunsthändler ein Motiv für diesen Schritt gewesen sein, da eine Luftveränderung den beiden kranken Künstlern Modigliani und Soutine möglicherweise gut tun und ihre Produktivität steigern würde. Sie begaben sich an die französische Mittelmeerküste, wo Modigliani zahlreiche Porträts malte, denen er sich nach den Akten wieder zugewandt hatte. Die fertigen Bilder schickte er zum Verkauf nach Paris.

Über das Jahr Modiglianis in Südfrankreich ist nur wenig bekannt, da es kaum schriftliche Dokumente gibt und die Pariser Zeitgenossen in seiner Abwesenheit wenig über ihn zu berichten hatten. Anfangs wohnten Modigliani, Jeanne Hébuterne und seine Freunde in Cagnes-sur-Mer, später zogen sie nach Nizza. Dort brachte Jeanne Hébuterne am 29. November 1918 eine Tochter zur Welt. Amedeo Modigliani erkannte die Vaterschaft des Kindes an, das den Vornamen der Mutter erhielt. Während seines Aufenthaltes in Nizza und der näheren Umgebung besuchte Modigliani Pierre-Auguste Renoir, der ein Anwesen über der Küste bewohnte. Ein in der Nachbarschaft wohnender Maler berichtete später, dass es zwischen dem Altmeister des Impressionismus und dem jungen Maler zu einem Streit über Ratschläge Renoirs gekommen sei.

Nach Vermittlung durch Zborowski wurden 1919 mehrere Werke Modiglianis auf Ausstellungen in England gezeigt, so unter anderem im Rahmen der Ausstellung "Modern French Painting" in Heale. Weiterhin zeigte im September dieses Jahres die Londoner "Hill Gallery" zehn Werke Modiglianis. Ende Mai 1919 kehrte Modigliani nach Paris zurück, wo er am Herbstsalon teilnahm. In dieser Zeit unterstützte ihn auch der finnische Maler Léopold Survage, der ihm sein Atelier zur Verfügung stellte. Als Jeanne Hébuterne erneut schwanger wurde, verlobte sich Amedeo Modigliani mit ihr. Es existiert eine Heiratsverpflichtung vom 7. Juli 1919, in dem er sie als seine zukünftige Ehefrau und die gemeinsame Tochter offiziell als sein Kind anerkennt. Diese Heiratsabsicht konnte er jedoch nicht mehr umsetzen, da er gegen Jahresende schwer an Tuberkulose erkrankte.

Am 24. Januar 1920 verstarb Modigliani in der Charité in Paris. Am folgenden Tag starb seine Verlobte durch Suizid. Modigliani wurde unter großer Anteilnahme auf dem Friedhof Père Lachaise beigesetzt. Hébuterne wurde später, nachdem ihre Familie den Widerstand dagegen aufgegeben hatte, neben ihm begraben. Ihre Tochter Jeanne wurde von Modiglianis Schwester in Florenz adoptiert.

Das Gesamtwerk Amedeo Modiglianis besteht aus Gemälden, Zeichnungen und Skulpturen. Das Œuvre umfasst etwa 420 Gemälde, von denen nur 14 datiert sind, und etwa 25 Skulpturen. Mit der Ausnahme von wenigen Landschaftsgemälden liegt der Schwerpunkt der Kunst Modiglianis auf der Darstellung des Menschen. Diese kommt in den Porträts, Akten und Skulpturen menschlicher Köpfe beziehungsweise Figuren zum Ausdruck und zeigt ein intaktes Bild des Menschen. Modigliani lässt sich keiner modernen Kunstströmung zuordnen. Seine Werke vereinen expressionistische, kubistische und symbolistische Elemente, zeigen jedoch ebenso einen Rückbezug zur Antike, zur Renaissance und zum Manierismus, die er aus seiner Studienzeit in Italien kannte. Somit entwarf er seinen ganz individuellen Stil.

Für den Stil Amedeo Modiglianis sind lineare und lang gestreckte Formen charakteristisch. Viele seiner Bilder zeigen für Modigliani typische Elemente, wie lang gezogene Gesichter und blinde Augen. Die Darstellung ist stark reduziert, in den Porträts und Akten ist sie so auf die Person fokussiert, dass der Raum in den Hintergrund tritt und kaum Attribute neben der zentralen Figur zu finden sind. Nur in den späten Werken Modiglianis werden durch einige wenige Objekte im Bild Hinweise auf das soziale Umfeld der dargestellten Person gegeben. Vor 1914 entstanden außerdem wenige Zirkus- und Varietézeichnungen, welche die Personen in einen größeren erzählerischen Zusammenhang stellen.

Da Modigliani nur einen Bruchteil seiner Werke datiert hat, lässt sich die genaue Folge der Bilder nur über stilistische Analysen und Überlieferungen aus zeitgenössischen Berichten rekonstruieren. Dabei lässt sich innerhalb des Œuvres eine stilistische und kompositorische Entwicklung nachweisen, die zu immer weiter der Vollendung entgegenstrebenden Bildern führte.

Den Großteil der Bilder Amedeo Modiglianis machen Porträts aus. Zu seinen frühen Werken gehört das Bild "Die Jüdin", das um 1908 entstanden ist. Das 55 × 46 Zentimeter große Ölgemälde ist statuarisch aufgebaut. Es zeigt eine streng blickende Frau, deren Gesicht klar herausgearbeitet ist und sich deutlich von der eher verschwommenen Umgebung abhebt. Das Hauptaugenmerk des Bildes liegt auf der Darstellung der Psychologie der Person, die in der Mitteilung der Stimmung, der Verhaltenheit im Bild und im Blick der Frau auf den Betrachter zum Ausdruck kommt. Deshalb ist die Darstellung auch sehr maßvoll und trotz der leichten Pinselführung nicht auf die voneinander unabhängige Verwendung von Farben und Flächen ausgerichtet. Es gibt jedoch auch Teile des Bildes, die stark von der rein malerischen Behandlung der Fläche geprägt sind. Ein Beispiel dafür ist das Farbfeld in der rechten unteren Bildecke, das nicht einem konkreten Gegenstand zugeordnet werden kann. Darin liegt ein Bezug zu der Vorstellung von Maurice Denis, dass ein Bild, bevor es seinen Gegenstand zeigt, nur eine Oberfläche sei, die nach einer bestimmten Ordnung mit Farbe bedeckt werde. Diese Definition war von großer Bedeutung für die Zeit, in der auch Modigliani arbeitete. "Die Jüdin" zeigt aber auch die Suche Modiglianis nach seinem Platz in der Kunst inmitten der verschiedenen avantgardistischen Strömungen. Das Werk beinhaltet Einflüsse von Henri de Toulouse-Lautrec, Edvard Munch und Paul Cézanne und steht im Kontrast zur vorherrschenden fauvistischen Malerei mit ihrer dominierenden Farbvielfalt. Als das Bild im 1908 im Salon des Indépendants ausgestellt wurde, stand es mit seinen dunklen Farben im Kontrast zu den meisten anderen ausgestellten Werken. In der Bilddarstellung ist "Die Jüdin" ebenfalls zurückhaltend, in Anbetracht dessen, dass im Vorjahr der Kubismus, mit dem der Raum und die Perspektive zersprengt wurden, entstanden war. Der Farbauftrag Modiglianis in diesem Bild orientiert sich dabei stark an dem des Expressionismus.

Während des Ersten Weltkrieges, nach seiner Phase als Bildhauer, porträtierte Modigliani viele Freunde und Künstler, die in Paris verblieben waren. Eines dieser Porträts ist das "Bildnis Diego Rivera" aus dem Jahr 1914. Das 100 × 79 Zentimeter große, mit Öl auf Karton gemalte Bild zeigt den mexikanischen Maler Diego Rivera, der drei Jahre vorher nach Paris gekommen war und schnell zum Freundeskreis um Picasso gehörte. 1929 heiratete Rivera Frida Kahlo und war in den 1930er-Jahren der Hauptvertreter der mexikanischen Wandmalerei. Er wurde von Modigliani mehrmals gemalt. Dieses Bild spiegelt durch die freie Farbfleckenmalerei die revolutionären Ansichten und das Temperament Riveras wider. Der Farbauftrag auf den Malgrund erfolgte tupfend. Dabei orientierte Modigliani sich nicht am Kolorismus des von ihm bewunderten Cézanne, sondern brachte eher einen Nachklang der impressionistischen Malerei hervor. Die Strichtechnik dieses Bildes erinnert auch an die Arbeit Modiglianis beim Erschaffen der Skulpturen. Sie erzeugt den Eindruck eines in den Malgrund geritzten Reliefs. Das Bild zeigt die obere Körperhälfte Riveras. Das runde Gesicht wird von den Haaren und der Brust gerahmt. Beide Elemente sind wie der Oberkörper nicht konkret vom Bildhintergrund abgegrenzt. Daher entsteht der Eindruck, dass der Körper das gesamte Bild ausfüllt. Die Augen Diego Riveras sind fast ganz geschlossen, seinen Mund umspielt ein Lächeln. Deshalb wirkt er versonnen und zufrieden. Aufgrund der Malweise ist das Bild eines der expressiveren von Amedeo Modigliani, im Gegensatz dazu stehen die folgenden Porträts, deren Malstil einfacher und glatter ist und in denen er mehr Wert auf die Darstellung der äußeren Erscheinung als auf die des Charakters der abgebildeten Person legte.

Das Porträt "Bildnis Jacques Lipchitz und seine Frau Berthe Lipchitz" aus dem Jahr 1916 oder 1917 gehört zu den Bildern, in denen Amedeo Modigliani die Darstellung der Psychologie eines Charakters aufgab und stattdessen repräsentative Porträts schuf, die sich an Vorbilder des Barock anlehnten. Dieses 80,2 × 53,5 Zentimeter große Ölgemälde malte Modigliani nach dem Hochzeitsphoto der Lipchitz in mehreren Porträtsitzungen. Das war nicht ungewöhnlich, da er zu dieser Zeit mehrmals Fotografien als Vorlagen für Bilder nutzte. Dieses Bild nimmt in Modiglianis Werk eine besondere Stellung ein, da es zu seinen wenigen Doppelporträts gehört. Die Bedeutung dieser Bilder wird daran deutlich, dass Gruppenbildnisse in Modiglianis Œuvre überhaupt nicht vorhanden sind. Dem Porträt des jungen Ehepaars gingen mehrere Skizzen voraus, die jedoch noch auf ein Einzelporträt ausgerichtet waren. In ihnen tastete sich Modigliani immer weiter an die endgültige Bildkomposition heran. Das Bild zeigt den Bräutigam Jacques Lipchitz stehend hinter seiner sitzenden Frau Berthe. Den linken Arm hat er seiner Frau um die Schulter gelegt. Sie sind beide dunkel gekleidet und heben sich so vom helleren Bildhintergrund ab. Ihre Gesichter sind rundlich und die Augen leer. Die beiden Dargestellten, zu denen Modigliani freundschaftlichen Kontakt pflegte, wirken in dem Porträt zwar sympathisch, werden aber aus einer emotionslosen Distanz dargestellt. Diese ist ein zentrales Merkmal der Porträts der folgenden Jahre bis zum Tod Modiglianis. Auf dem Weg zur endgültigen Fassung des Porträts vollzog sich auch eine stilistische Entwicklung. So treten die Vertikale und die Horizontale in den Hintergrund, so dass geschwungene Linien und fließende Formen im Bild dominieren. Das Porträt des Ehepaars Lipchitz weist mit dem in Großbuchstaben geschriebenen, unregelmäßigen Schriftzug "LIPCHITZ" ein stilistisches Merkmal Modiglianis auf, das viele seiner Porträts von Freunden gemeinsam haben. Diese Beschriftung, die sich in ihrer ungelenken Pinselführung deutlich von der Signatur unterscheidet, orientierte sich formal an der Tradition von Bildern der Renaissance. Künstler wie Giorgone und Tizian gaben in den Inschriften nicht nur über den Namen des Dargestellten, sondern auch beispielsweise über die Zugehörigkeit zu Geheimbünden Auskunft. Mit dem formalen Zitat kann Modigliani auf eine ähnliche innere Verwandtschaft mit den porträtierten Freunden angespielt haben. Außerdem nutzte er sie zur Auflockerung der Bildkomposition. In den Barock- und Renaissance-Bezügen wird weiterhin deutlich, dass Modigliani seine Kenntnisse im Bereich früherer Kunstepochen, die er in den Studien seiner frühen Lebensjahre erworben hatte, auch in seinen Bildern anwandte.

Viele Porträts Amedeo Modiglianis zeigen seine beiden Geliebten Beatrice Hastings und Jeanne Hébuterne. Die Gemälde von Hastings weisen oft eine pointierte Wiedergabe von Augen, Mund und Nase auf. Sie zeigen eine starke Fokussierung auf den Charakter seiner als exzentrisch geltenden Geliebten. Die Bilder haben eine spielerische und inoffizielle Wirkung, die ein Resultat der engen und nach Hastings eigenen Angaben dramatischen Beziehung sind. Die Beziehung zu Jeanne Hébuterne war aufgrund des großen Altersunterschiedes nicht so turbulent. Sie sah zu Modigliani auf und war nicht die Diskussionspartnerin wie Hastings. Die Porträts Hébuternes zeigen keine großen Variationen in der Perspektive und des Stils. In einigen Bildern wird sie als Kindfrau dargestellt, was einen Bezug zur jugendlichen Naivität aufzeigt, obwohl sie in Modiglianis Leben eine Konstante darstellte. Jeanne Hébuterne ließ Modigliani neu über sein Leben nachdenken, das von Drogen und Alkohol bestimmt war. Mit der Schwangerschaft Jeannes war eine gravierende Änderung in Modiglianis Leben verbunden und stand in Kontrast zu seinem bisherigen Lebenswandel. In dieser Situation entstand das Bild "Porträt der Jeanne Hébuterne", das sie als hochschwangere Frau zeigt. Das Porträt stellt Jeanne Hébuterne in einer sitzenden Position mit im Schoß liegenden Händen und zur Seite geneigten Kopf dar. Der Umstand der Schwangerschaft wird von Modigliani nicht verdeckt, sondern betont. So zeigt zum Beispiel der gekrümmte Finger der rechten Hand auf den Bauch. Ein weiteres betonendes Element sind die die Aufmerksamkeit auf den Bauchbereich lenkenden Streifen in der Hüftregion. Trotz des lang gestreckten Halses ist ein leicht angedeutetes Doppelkinn, das auf die Schwere der letzten Phase der Schwangerschaft hindeutet. Die Farbgebung ist sehr ausgewogen. Die dunkle Fläche der Kleidung und des Haares werden durch die Streifen an der Hüfte und den Armen, sowie dem grünen und orangen Bildhintergrund ausbalanciert. Das Bild erzeugt eine sehr ruhige Stimmung, es enthält keine Bewegung und die Person wirkt in sich ruhend, was besonders durch die Kopfhaltung deutlich wird. Eine besondere Intensität wird durch den direkten Blick Jeanne Hébuternes auf den Betrachter erzeugt, der ebenfalls Ruhe ausstrahlt.

Amedeo Modigliani malte während seines ganzen Lebens Aktgemälde, welche die zweitgrößte Werkgruppe nach den Porträts darstellen. Die ersten stammen aus dem Jahr 1908, wie zum Beispiel das Bild "Leidender Akt – Nudo Dolente". Dieses 81 × 54 Zentimeter große Ölgemälde zeigt eine halbfigurige Frauendarstellung. Die hagere Frau wird vollkommen nackt gezeigt. Der Kopf ist nach hinten geworfen, der Mund geöffnet. Dies ist ein Zeichen von Ekstase, Leid, Schmerz und Sinnlichkeit. Der wahre Ausdruck der Person ist hinter dem maskenartigen Gesicht verborgen. Die Schultern sind nach vorn gezogen. An ihnen hängen die unnatürlich langen Arme lose herab, die Hände ruhen auf den Oberschenkeln. Die Frau ist so dünn, dass sie Ähnlichkeit mit einem Skelett aufweist. Der Akt Modiglianis widersprach damit den klassischen Inhalten dieser Bildgattung, die einen sinnlichen und stark sexuellen Bezug hatten. Vor dem dunklen Bildhintergrund tritt der helle, fast weiße Körper deutlich hervor und wird durch den Hell-Dunkel-Kontrast betont. Der Farbauftrag ist rau und lässt das Bild teilweise unvollendet wirken. In diesem Gemälde wird die ähnliche Körperauffassung von Modigliani in Bezug auf andere Künstler dieser Zeit deutlich. So gibt es Ähnlichkeiten mit Bildern wie "Madonna" von Edvard Munch aus dem Jahr 1894 oder Werken von George Minne.

In den Jahren 1916 und 1917 malte Modigliani seine bekannte Serie von Akten, die 30 Gemälde umfasst. Sie zeigen sitzende, stehende oder liegende Modelle, die idealisiert in ihrer Nacktheit dargestellt sind. Die Frauenkörper bilden das zentrale Bildelement, der Raum und andere Gegenstände treten in den Hintergrund und sind nur im geringen Umfang im Bild dargestellt. Ihre Darstellung hat keinen mythologischen oder historischen Bezug, sondern dient zur alleinigen Darstellung der Nacktheit. Trotzdem stehen sie in der Tradition der Darstellung der nackten Venus, die von der Renaissance bis ins 19. Jahrhundert das vorherrschende Aktmotiv war. Jedoch orientierte Modigliani sich an den italienischen Meistern der Renaissance wie Tizian, Sandro Botticelli und Giorgione, die jedoch vor der akademischen Ära der Malerei arbeiteten. Ihre Darstellungen folgten keinen bestimmten Aktposen, sondern weisen individuelle Ausprägungen jedes Künstlers auf. Mit den Kunstakademien hatte sich ein prägendes Verständnis des Aktes etabliert. Es gab einen bestimmten und eingeschränkten Kanon von Posen der Aktmodelle, strenge und formale Regeln. Amedeo Modigliani bricht in seinen Akten mit dieser akademischen Tradition, indem er Proportionen, Anatomie und Bewegung in den Bildern unbeachtet lässt. Daneben sind die Posen der Modelle nicht der akademischen Lehre angepasst. Modiglianis Akte sind auch durch sein Studium beeinflusst worden, das er an der Académie Colarossi betrieb. Dort wurden den Studenten Modelle zur Verfügung gestellt, deren Haltung sie frei bestimmen konnten. Daneben wurden so genannte Viertelstundenakte gemalt, die eine skizzenhafte, schnelle Erfassung des Motivs erforderten. So haben sich aus den Pariser Jahren viele Aktzeichnungen Modiglianis erhalten.

Das 60,6 × 92,7 Zentimeter große Gemälde "Liegender Akt" gehört zu den berühmtesten Werken Modiglianis und stammt ebenfalls aus der Bilderserie der Jahre 1916 und 1917. Es zeigt ein liegendes Modell, das sich zentral in der Bildmitte befindet. Die Frau wird in einer leichten Aufsicht aus nächster Nähe gezeigt, so dass ihre Extremitäten nicht vollständig abgebildet werden. So fehlen die Unterarme mit den Händen und die Beine unterhalb der Hüfte. Das Gesicht ist dem Betrachter zugewandt, die Augen sind geöffnet und blicken den Betrachter direkt an. Die Hüften sind leicht nach hinten gedreht, wodurch die Scham nicht zu erkennen ist. Der Körper befindet sich auf einem roten Bettlaken, was einen leichten Hell-Dunkel-Kontrast erzeugt. Unter dem Kopf befindet sich ein weißes Kissen, das mit dem Ansatz der weißen Bettdecke die hellsten Flächen des Bildes bildet. Im Hintergrund ist die Wand zu erkennen. Insgesamt gibt es neben dem Körper kaum Bildelemente, die von ihm ablenken. Das Bild orientiert sich in der Komposition an die Aktfotografie der Zeit, dabei ist die Erotik nicht überbetont, sondern melancholisch abgestuft. Wobei von der dargestellten Frau Würde und Kühle ausgeht, was einen Anschluss an die Skulpturen Modiglianis darstellt.

Nach der Serie der Jahre 1916 und 1917 malte Amedeo Modigliani nur noch gelegentlich Akte wie "Stehender Akt – Elvira" aus dem Jahr 1918. Dieses 92 × 60 Zentimeter große Ölgemälde entstand während Modiglianis Aufenthalt in Südfrankreich. Für seine Gemälde dieser Zeit typisch, wurde es mit deutlich helleren Farben gemalt. An die Stelle der dominierenden dunklen Farbtöne, besonders Rot, tritt in diesem Bild Türkis. Das stehende Modell, das nur von den Oberschenkeln an gezeigt wird, nimmt im Bild die zentrale Position ein. Neben einem weißen Laken, das die Scham verdeckt, gibt es keine weiteren kompositorischen Bildelemente. Die Umrisslinien des Körpers sind stark hervorgehoben und die Farben großflächig aufgetragen, womit der Präsenz der Person verstärkt wird.

Unter Amedeo Modiglianis Werken befinden sich nur wenige Landschaftsbilder. Diese entstanden in seinen frühen Jahren in Italien, auf seinen Reisen in seine Heimat und während seines Aufenthaltes in Südfrankreich. Während das 1898 entstandene Bild "Landschaft in der Toskana" sich noch am Impressionismus orientierte und keine klaren Konturen aufweist, sondern unscharf wirkt, steht im Gegensatz dazu beispielsweise das 1919 gemalte 60 × 45 Zentimeter große Bild "Landschaft". Dieses in Südfrankreich entstandene Gemälde wird durch klare Umrisse gegliedert. In den Hügeln im Bildhintergrund bilden die Gebäude klare geometrische Strukturen, die im Kontrast zu den Formen der sie umgebenden Wolken stehen, die jedoch ebenfalls klare Umrisse aufweisen. Im Vordergrund ist eine diagonal durch das Bild verlaufende rote Fläche zu sehen, die entweder einen Weg oder ein Brückengeländer darstellt. Mit dem Rot dieses Bildelementes wird die Farbe der Hausdächer erneut aufgegriffen. Es ist klar von der Umgebung abgegrenzt, was ein Gefühl der Enge und Begrenztheit hervorruft. Die Hügel in der Bildmitte führen terrassenförmig in den Hintergrund und erzeugen einen Eindruck von räumlicher Tiefe. Im Kontrast dazu stehen die Bäume im Vordergrund, die mit ihren langen linearen Strukturen das Bild zusätzlich gliedern.

Auch in den Landschaftsbildern, die Modigliani in Südfrankreich malte, kommt der Fokus von Modigliani auf das Porträt über das Format zum Ausdruck. Anstelle des für Landschaften üblichen Querformats nutzte er auch das Hochformat. Die Bilder Amedeo Modiglianis weisen Ähnlichkeiten zu Landschaften Paul Cézannes auf, der zu Modiglianis künstlerischen Vorbildern zählte, sowie zu Bildern anderer Künstler dieser Zeit. So sind beispielsweise kompositorische Parallelen zu Bildern von Gustav Klimt vorhanden. Daneben gibt es weitere Ähnlichkeiten wie das stilisierte Aussehen der Bäume, sowie die räumliche Anordnung der Bildelemente.

Zwischen 1909 und 1914 widmete sich Amedeo Modigliani fast ausschließlich der Bildhauerei. Neben ihm wandten sich zu dieser Zeit auch andere Maler dieser Kunstgattung zu, wie beispielsweise Picasso, Matisse und André Derain. Dies geschah infolge der vor dem Ersten Weltkrieg großen Zuspruch findenden afrikanischen Kunst, deren Skulpturen unter dem Begriff "Negerplastik" hohe Popularität erreichten. Daneben lernte Amedeo Modigliani den Bildhauer Brancusi kennen, dessen Skulpturen im Herbstsalon neben seinen Bildern ausgestellt waren. Erst nach dem Kontakt mit Brancusi wandte sich Modigliani der Skulptur zu und zog in ein für die Bildhauerei eingerichtetes Atelier am Montparnasse.

Die meisten Skulpturen Amedeo Modiglianis stellen Köpfe dar, die er als "Säulen der Zärtlichkeit" bezeichnete. Laut dem Kunsthistoriker Gerhard Kolberg schwanken diese Skulpturen „zwischen hohem ideellen und bildhauerischem Anspruch und primitiver bis archaischer skulpturaler Ausführung“. Dabei ist besonders auffällig, dass Modigliani trotz seiner Unerfahrenheit als Bildhauer fähig war, seinen Kopf-Skulpturen ein einheitliches stilistisches Aussehen zu verleihen. Sie haben alle ein einheitliches Grundmaß und sind aus hochrechteckigen Steinblöcken gearbeitet. Die Köpfe sind idolhaft und ikonenartig ausgearbeitet und strahlen aufgrund ihrer Schlichtheit eine majestätische Würde aus. In den gemeinsamen Ausstellungen dieser Skulpturen im Jahre 1911 wurde deutlich, dass sie nur in der Gesamtheit, jedoch nicht im Einzelwerk ihre Bedeutung zeigen. Um die Wirkung der Präsentation noch zu steigern, entwickelte Modigliani ein eigenes Beleuchtungskonzept der Objekte. Mit dieser Art der Ausstellung inszenierte er seine Skulpturen so, dass ein mysteriöser und religiös anmutender Eindruck entstand. Ein Exemplar dieser Serie ist der 70,5 × 23, 5 × 7,6 Zentimeter große "Kopf einer Frau", der sich im Besitz des Philadelphia Museum of Art befindet. Er weist die typischen Merkmale der Kopfdarstellungen Modiglianis auf. Das Gesicht ist in die Länge gestreckt, so dass Nase und Ohren unnatürlich lang sind. Das Kinn ist spitz zulaufend, der Abstand zwischen den Augen gering. Der Gesichtsausdruck vermittelt keine Emotion, sondern strahlt allein Ruhe aus.

Neben den Kopfskulpturen schuf Amedeo Modigliani nur zwei weitere, die heute bekannt sind: eine stehende Figur und eine Karyatide. Diese weist einen deutlichen Rückbezug zur griechisch-römischen Antike auf. Karyatiden sind Gewandfiguren in menschlicher Gestalt, die als Stützen ganze Gesimse oder Geschosse tragen und seit der Antike ein fester Bestandteil der Architektur waren. Die Skulptur "Karyatide" aus dem Jahr 1914 weist nur noch durch ihre Haltung einen Bezug zu dieser Funktion auf. Die Figur kniet auf einem Bein, das andere ist angewinkelt an den Körper gezogen. Die kräftige weibliche Gestalt hält beide Arme über den Kopf erhoben. Die Last, die sie tragen musste, deutet Modigliani nur durch eine Platte an. Das Gewicht der Skulptur konzentriert sich allein auf die Zentralachse der Figur, was ihr Standfestigkeit verleiht. Der verwendete Kalksandstein wurde von Modigliani nur grob bearbeitet, was eine raue Oberfläche erzeugte im Gegensatz zu den glatten Oberflächen der Kopfskulpturen. Es ist kein Gesicht herausgearbeitet, so dass die Figur eine besondere Anonymität aufweist.

Während seiner bildhauerischen Schaffensphase malte Modigliani nur wenige Bilder. Diese hatten meist ebenfalls einen Bezug zur Skulptur, waren von Statuen inspiriert oder griffen das Motiv der Karyatide auf. Ein Beispiel für diese Bildergruppe ist das 72,5 × 50 Zentimeter große Ölgemälde "Karyatide", das etwa 1911/1912 entstand. Von der Haltung her weist die Figur eine große Ähnlichkeit mit der 1914 entstandenen Skulptur auf. Der Körper ist auf eine geometrische Weise aus einzelnen Elementen zusammengesetzt worden. Der gesamte Körper ist in die Länge gestreckt und weist verlängerte, kraftvolle Arme auf. Das Bild zeigt somit keine sinnliche Darstellung einer Frau, sondern die von Kraft und Ruhe. Das Gesicht der Frau ähnelt denen altägyptischer und mykenischer Statuen, was ebenfalls eine Rezeption der von ihm studierten Werke darstellt. Die Beugung der Figur ist bildhauerisch nicht realisierbar, da die Verteilung des Gewichts an der Basis die Figur umstürzen lassen würde. So stellt das Bild für Modigliani eine Möglichkeit dar, in der Behandlung des Themas nicht an die Beschränkungen der Erschaffung einer Skulptur gebunden zu sein.

Von Amedeo Modigliani sind viele Zeichnungen erhalten geblieben. Ihre hohe Zahl kann in der Studienzeit des Künstlers begründet liegen. In der Académie Colarossi lernte er im 15-Minuten-Akt das schnelle skizzenhafte Erfassen einer Figur, da das Modell nach 15 Minuten jeweils eine andere Position einnahm. Auch später entstanden seine Zeichnungen in sehr kurzer Zeit und ohne viele Korrekturen. Er legte die Zeichnungen in großen Zügen an, wobei offensichtliche Ungenauigkeiten, die in einigen Ausnahmefällen existieren, einen vom Künstler gewollten Effekt darstellen. Während seiner gesamten künstlerischen Laufbahn brachte er die Zeichnungen erst auf das Papier, nachdem sie vorher in seinem Kopf bereits Gestalt angenommen hatten. Dabei gab es keine technische Entwicklung in seinem Schaffen. Er nutzte meist Blätter, die er an der Perforierung aus seinen Skizzenblöcken heraustrennte und auf denen er mit Tusche und schwarzer Ölkreide skizzierte. Seltener nutzte er Aquarell, Graphit, Rötel, Kohlestift und blaue bis violette Kreide.

Die meisten Zeichnungen Modiglianis stellen Studien dar, in denen er Motive und Kompositionen testete, und aus dem Moment entstandene Wiedergaben einer Situation beziehungsweise einer Figur. Dabei ragen die Zeichnungen, die vom Theater und Zirkus inspiriert wurden und 1908 entstanden, aus dem übrigen Werk Modiglianis heraus, da sie ein komplett anderes Sujet darstellen. Sie sind nicht allein auf die Figur konzentriert, sondern stellen diese in einem weitergehenden erzählerischen Zusammenhang dar. Zwei der Skizzen zeigen einen Saal des Gaîté-Rochechouart am Montmartre, in dem Modigliani eine Aufführung gesehen hat. Die eine zeigt eine Gruppe von Schauspielern auf der Bühne, die andere eine leere Bühne. In beiden Bildern sind dieselben Zuschauer zu sehen, so dass nachgewiesen ist, dass beide Zeichnungen in derselben Vorstellung vor Ort ausgeführt wurden. Mit den Zeichnungen von Tänzerinnen, Artisten und einer Marionette machen die aus dem Theater die einzigen Werke Modiglianis aus, in denen sein Vergnügen am und seine Neigung zu Theater und Zirkus deutlich werden.

Bei der großen Anzahl von Zeichnungen in den Jahren als Bildhauer wird vermutet, dass es sich größtenteils nicht um Vorlagen für konkrete Einzelstücke handelt, sondern dass Modigliani darin Ideen für nicht realisierte Skulpturen festhielt. Dabei ist keine Zeichnung die Kopie einer anderen, aber aufgrund ihrer stilistischen Ähnlichkeit weisen sie laut Claude Roy eine „wunderbare Monotonie der Besessenheit“ auf. Weiterhin existieren viele Porträt- und Aktzeichnungen.

Den Großteil seines Lebens hatte Amedeo Modigliani mit seiner Kunst nur wenig Erfolg. Seine Werke waren wenig gefragt, da sie nicht den großen neuen Strömungen in der Kunst angehörten, sondern einen persönlichen Stil zeigten. Ab 1914, nach dem Ende der Phase als Bildhauer, entwickelte Modigliani einen neuen dekorativeren Stil, der mit seinen langen Hälsen und mandelförmigen Augen allmählich steigenden Erfolg brachte. Zu dieser Zeit kam die Bewegung der Neuen Sachlichkeit auf, in der die Darstellung eines intakten Menschenbildes gefragt war. Daneben ging Modigliani stärker Kompromisse ein infolge der Beziehung zu Jeanne Hébuterne und der Geburt seiner Tochter. So wurden seine Bilder gefälliger und damit verkaufsträchtiger. Die Preise für seine Bilder stiegen in der Folge rasch an. Einer der bedeutendsten Sammler von Werken Modiglianis war dabei Roger Dutilleul, ein französischer Industrieller und Kunstsammler, der von 1918 bis 1925 etwa ein Zehntel des Gesamtwerkes Modiglianis erwarb.

Modigliani selbst hat sich nur selten zu seiner Kunst geäußert. Laut Berichten seiner Zeitgenossen pflegte er das Bild eines jüdischen Außenseiters und Ausländers. Dabei war er nach seinem Selbstverständnis als Künstler ein „superuomo“ (deutsch: „Übermensch“) und Auserwählter der Gesellschaft. Auffällig ist die strikte Trennung des Privatlebens von der Kunst. Er malte nur ein Selbstbildnis und griff in seinen Kunstwerken nicht den unsteten Lebensstil auf, sondern entwickelte einen klaren und einfachen Stil. Seine Bilder verbinden die Tradition mit der Moderne. Dabei war Modigliani kein Wegbereiter oder Vorreiter einer künstlerischen Entwicklung. Vielmehr blieb er ein stilistischer Außenseiter und Einzelgänger. Mit dem Brückenschlag zwischen moderner Kunst und vergangenen Kunstepochen leistete Modigliani einen herausragenden individuellen Beitrag in der Kunst des 20. Jahrhunderts.

In der Rezeptionsgeschichte stellen die Aktgemälde Modiglianis den am meisten besprochenen und gerühmten Teil seines Gesamtwerkes dar, obwohl sie nur etwa ein Zehntel an diesem ausmachen. Die Stellung dieser Bilder wurde durch die Zensur, die noch nach Modiglianis Tod bestehen blieb, und die Wirkung auf den Betrachter begründet. Noch nach dem Zweiten Weltkrieg lösten die Akte Diskussionen über ihren pornografischen Gehalt aus. So musste sich das Solomon R. Guggenheim Museum in New York rechtfertigen, als es Reproduktionen des in seinem Besitz befindlichen Gemäldes "Akt mit Halskette" drucken lassen wollte. Die Akte Modiglianis wurden mit verschiedenen, widersprüchlichen Urteilen belegt. Auf der einen Seite wurden sie positiv als „faszinierend-sinnlich“ bezeichnet, während andere Kritiker sie als „kalt-abstoßend“ empfanden. Amedeo Modigliani entwickelte in den Akten einen eigenen Stil. Während andere Maler dieser Zeit wie Picasso, Matisse, Tsuguharu Foujita oder Kees van Dongen den nackten weiblichen Körper in einem weitergehenden Sujet darstellten, konzentrierte sich Modigliani allein auf die Darstellung des weiblichen Körpers, gab im Bild keine Handlung und auch keine weitergehenden Informationen über das Modell wieder.

Im Jahr 1955 wurden Arbeiten von ihm auf der documenta I in Kassel und 1964 auch auf der documenta III in der berühmten Abteilung "Handzeichnungen" gezeigt. Eine ähnliche Kopfskulptur aus dem Besitz der Erben des französischen Kaufhausgründers Gaston Lévy erzielte bei Christie’s in Paris im Juni 2010 einschließlich des Aufgeldes einen Preis von 43,2 Millionen Euro. Der Höchstpreis für ein Gemälde des Künstlers wurde am 9. November 2015 bei einer von Christie’s durchgeführten Auktion in New York erzielt, als das Gemälde "Nu couché" für 170,4 Mio. US-Dollar (ca. 158,5 Mio. Euro) den Besitzer wechselte. Fünf Jahre zuvor war Modiglianis "Nu assis sur un divan (La Belle Romaine)" für 68.962.500 US-Dollar versteigert worden. Den bislang höchsten Preis für eine Skulptur von Modigliani zahlte ein anonymer Käufer am 4. November 2014 bei einer Auktion in der New Yorker Filiale von Sotheby’s, als die Skulptur "Tête" für 70,7 Millionen US-Dollar den Besitzer wechselte.

Als nach dem Tod Modiglianis seine Kunstwerke im Preis stiegen, wurden zu Beginn authentische Bilder Modiglianis nachträglich mit dessen Signatur versehen. Damit konnten Geschäftsleute den Wert der Bilder noch erheblich steigern. Daneben wurden komplett neue Bilder produziert, die als Werke Modiglianis ausgegeben wurden und ebenfalls gute Preise erzielen konnten. Die Fälschungen wurden von der schweren Feststellbarkeit der Echtheit begünstigt. Eine Aussage darüber ließ sich oft nur über vergleichendes Sehen treffen. Dabei werden durch Studium möglichst vieler Originale die Maltechnik eines Künstlers, Signatur, Farbwahl, Komposition und weitere Merkmale analysiert, so dass andere Gemälde dahingehend verglichen werden können. Aufschluss über Malweise und verwendetes Material konnten auch Röntgenuntersuchungen geben. Auf dem Gebiet der Feststellung der Authentizität von Werken Modiglianis leistete die Pariser Retrospektive aus dem Jahr 1981 Maßgebliches. Während der Vorbereitungen wurden Kriterien zur Beurteilung der Vorgehensweise Modiglianis und der Signatur festgelegt.

Daneben wurde die Echtheit von Werken Modiglianis über das Provenienzprinzip nachgewiesen. So beispielsweise für das Porträt "Bildnis Jacques Lipchitz und seine Frau". Es wurde von den privaten Sammlern Frederic Clay und Helen Birch Bartlett aus einer zuverlässigen Quelle erworben und die Entstehung von Lipchitz persönlich bestätigt. Danach wurde es direkt dem Art Institute of Chicago übergeben.

1984 sorgte die Entdeckung von drei angeblichen Steinskulpturen Modiglianis in einem Kanal in Livorno für Aufregung. Diese Arbeiten hätte er während seines letzten Aufenthaltes in seiner Heimatstadt in einem Wutausbruch in das Gewässer geworfen. Die Echtheit der Skulpturen wurde unter Kunsthistorikern kontrovers diskutiert. Nach einiger Zeit gab jedoch eine Gruppe von Studenten sowie ein Hobbykünstler bekannt, diese Skulpturen anlässlich des 100. Geburtstags Amedeo Modiglianis selbst geschaffen und in den Kanal geworfen zu haben.

Modiglianis Leben in Armut, sein Konsum von Drogen und seine Aktgemälde waren beliebte Themen der literarischen Rezeption. Ein besonders häufig verarbeitetes Motiv stellt die Fantasie dar, Modigliani habe seine Modelle nicht nur gemalt, sondern auch sexuelle Kontakte zu ihnen unterhalten. Ein Beispiel dafür ist ein Roman André Salmons, der Amedeo Modigliani zum Thema hat. In einer Szene entkleidet sich gerade ein Modell, und mit jeder neuen Pose erinnert sie Modigliani an ein kunsthistorisches Vorbild. Dies beeindruckt ihn ebenso wie der Anblick des nackten Mädchens, so dass er nicht gleich mit dem Malen beginnt, sondern erst mit ihr Geschlechtsverkehr hat. In der Darstellung dieser Begebenheit setzt Salmon den Blick des Menschen mit dem Blick des Künstlers Modigliani auf eine raffinierte Art gleich.

In Ken Folletts Roman Der Modigliani Skandal stößt die Kunststudentin Dee bei Recherchen zu ihrer Dissertation auf die Spur eines unbekannten Gemäldes von Modigliani. Während sie mit ihrem Freund die Spur verfolgt, bekommen zwei skrupellose Schatzjäger Wind von der Sache und heften sich an ihre Fersen, um womöglich - jeder für sich - selbst als erste an das Bild zu kommen. Dee und ihr Freund gewinnen das Rennen knapp. In einem zweiten Handlungsstrang kämpfen zwei junge, wenig erfolgreiche Maler mit einem spektakulären Coup dafür, dass der florierende Kunsthandel sich sozial für junge Künstler engagiert. Sie verkaufen den wichtigsten Galerien erfundene, selbstgemalte, Werke von Van Gogh, Munch, Picasso etc., erstatten das so ergaunerte Geld allerdings später zurück. Am Ende jedoch werden die beiden von Dees Freund gezwungen, den neu entdeckten Modigliani zweimal zu kopieren. Mit diesen Fälschungen werden die Schatzjäger betrogen und damit in den Augen des Lesers ,gerechterweise' bestraft.

Neben der literarischen Rezeption wurde das Leben Amedeo Modiglianis auch in drei Spielfilmen behandelt. 1958 entstand unter der Regie Jacques Beckers der Film "Les Amants de Montparnasse". Er behandelt die letzten Lebensjahre Modiglianis, der von Gérard Philipe verkörpert wurde, im Pariser Stadtviertel Montparnasse. Dabei legt der Film einen besonderen Fokus auf dessen Armut und Drogensucht. Aus dem Jahr 1990 stammt der Film "Modi", in dem Richard Berry den Künstler darstellte. Der Film behandelt das Leben Modiglianis ab seinem Umzug von Livorno nach Paris und folgt ihm weiter durch die Zeit des Ersten Weltkrieges und der Nachkriegszeit.

2004 führte Mick Davis Regie in dem Film "Modigliani". Der Darsteller Andy García spielte hier Modigliani; thematisiert werden dessen Beziehung zu Jeanne Hébuterne und die angebliche Rivalität zu Picasso. Diese kommt 1919 in der Konkurrenz während eines Wettbewerbes zum Ausdruck, der dem Sieger Erfolg und ein hohes Preisgeld verspricht. Modigliani möchte ein Meisterwerk abliefern, um das finanzielle Auskommen seiner jungen Familie zu sichern. Der Film wurde in der "New York Times" verrissen: 

Im Film Balduin, das Nachtgespenst mit Louis de Funès und Jean Gabin aus dem Jahre 1968 findet Modigliani ebenfalls eine filmtragende Erwähnung.

Diese Auswahl umfasst eine der Skulpturen, sowie 25 der rund 400 Gemälde Modiglianis. Sie orientiert sich an den ganzseitigen Farbtafeln des unter Literatur aufgeführten Werkes "Amedeo Modigliani" von Jane Rogoyska und Frances Alexander.





</doc>
<doc id="9570" url="https://de.wikipedia.org/wiki?curid=9570" title="Josef Albers">
Josef Albers

Josef Albers (* 19. März 1888 in Bottrop; † 25. März 1976 in New Haven, Connecticut) war ein deutscher Maler, Kunsttheoretiker und -pädagoge.

Albers erhielt von 1905 bis 1908 seine Ausbildung am Lehrerseminar in Büren und unterrichtete anschließend als Volksschullehrer. 1908 sah er zum ersten Mal Werke von Paul Cézanne und Henri Matisse im Museum Folkwang in Hagen. Durch Piet Mondrian inspiriert, malte er 1913 sein erstes abstraktes Bild. Nach dem Studium an der Königlichen Kunstschule in Berlin von 1913 bis 1915 sowie der Kunstgewerbeschule in Essen von 1916 bis 1919 studierte er an der Akademie der Bildenden Künste in Berlin und 1919–1920 bei Franz von Stuck an der Kunstakademie in München. 

1920 ging Albers - zunächst als Student - an das Bauhaus Weimar. Dort nahm er am so genannten Vorkurs von Johannes Itten teil, einer Klasse zur Vermittlung gestalterischer Grundlagen, in der die kontrastierenden Effekte von Formen und Farben im Zentrum des Studiums standen und die die Studierenden auf das weitere Studium in den verschiedenen Werkstätten des Bauhauses vorbereiten sollte.

1923 erhielt Albers selbst einen Lehrauftrag zur Durchführung des Vorkurses, nachdem Johannes Itten das Bauhaus verlassen hatte. In dieser Zeit lag ein Schwerpunkt seiner Arbeiten in der Glasmalerei und der Gestaltung von Glasfenstern. Diese Arbeiten bildeten den Hintergrund für seine Bestellung zum Werkmeister in der Glasmalereiwerkstatt. Eines seiner Hauptwerke bildete das Ensemble von 18 Glasfenstern im Leipziger Grassi Museum, 1927 gefertigt und eingebaut, 1943 zerstört und 2011 rekonstruiert. 1925 wurde Albers von Walter Gropius als Jungmeister an das Bauhaus, das zwischenzeitlich nach Dessau umgezogen war, berufen und leitete dort (bis 1928 zusammen mit László Moholy-Nagy, danach alleine) den Vorkurs. Der Vorkurs von Josef Albers vermittelte den Studierenden die Fähigkeit, mit einfachen Werkzeugen die Materialeigenschaften verschiedener Stoffe zu analysieren. Darüber hinaus sollte die Herstellung räumlicher Strukturen durch das Zusammenspiel von Material und Funktion als Ressourcen-effizienter Konstruktionsprozess erarbeitet werden.

1930 wurde Albers stellvertretender Direktor des Bauhauses und lehrte vor allem Glastechnik und Holzverarbeitung. 1932 hatte er seine erste Einzelausstellung. In der letzten kurzen Phase des Bauhauses nach Umzug nach Berlin (Oktober 1932 bis April 1933) war Albers nicht nur Leiter des Vorkuses, sondern auch Lehrer für Zeichnen und Schrift. Als im Jahre 1933 das Bauhaus durch die Nationalsozialisten geschlossen wurde und alle Lehrkräfte entlassen wurden, verließ Albers mit seiner Frau Anni Albers, die am Bauhaus gelernt hatte und kommissarisch nach Gunta Stölzl als Werkmeisterin an der Leitung der Weberei beteiligt war, Deutschland und emigrierte in die USA.

In Amerika erhielt er einen Ruf an das Black Mountain College in North Carolina, an dem er von November 1933 bis 1949 tätig war. Die amerikanische Staatsbürgerschaft nahm er 1939 an. Zu seinen bedeutendsten Schülern zählen Robert Rauschenberg, Donald Judd und Kenneth Noland. Während seiner Lehrtätigkeit am Black Mountain College kam er unter anderem auch mit Künstlern wie John Cage und Merce Cunningham in Kontakt. Von 1934 bis 1936 gehörte er der Pariser Künstlergruppe Abstraction-Création an. 1935 reiste das Künstlerehepaar erstmals nach Kuba und Mexiko und wurde durch die dortige Architektur beeindruckt und beeinflusst. Von 1950 bis 1959 stand er dem Art Department der Yale-Universität vor, wo er unter anderem Eva Hesse und Richard Serra, Richard Anuszkiewicz und Julian Stanczak unterrichtete. Daneben hatte er zahlreiche Gastdozenturen (so in Harvard, Hartford, Havanna und Santiago de Chile). In den Jahren 1954 und 1955 lehrte er als Gastprofessor an der neu gegründeten Hochschule für Gestaltung in Ulm, die sich in der Tradition des Bauhauses sah.

Albers experimentierte mit der Wirkung von Farben, Formen, Linien und Flächen aufeinander, mit der Subjektivität der optischen Wahrnehmung: . Mit seinen Zeichnungen auf der Grundlage von optischen Täuschungen gehört er neben Victor Vasarely zu den Begründern der Op-Art. In diesen Zusammenhang gehört seine berühmteste Serie "Hommage to the Square", deren Bilder immer gleich aus drei oder vier ineinander geschachtelten Quadraten verschiedener Farben bestehen. Die Farben sind nie gemischt, sondern direkt aus industriell hergestellten Farben aufgetragen, deren Artikel-Nummern der Maler auf der Rückseite der Bilder vermerkte. Dadurch wird ersichtlich, dass ein und dieselbe Farbe je nach Umgebung völlig unterschiedlich auf den Betrachter wirkt. Daher wird Albers auch zu den Vertretern von Hard Edge gezählt.

Josef Albers war Teilnehmer der documenta 1 im Jahr 1955 und der 4. documenta 1968 in Kassel. 1958 zeichnete ihn der Bundespräsident mit dem Verdienstkreuz Erster Klasse der Bundesrepublik Deutschland aus. Albers wurde 1968 in die American Academy of Arts and Letters und 1973 in die American Academy of Arts and Sciences gewählt, und wurde 1970 Ehrenbürger seiner Heimatstadt Bottrop, die später einen großen Teil seines Nachlasses als Schenkung von seiner Frau Anni Albers erhielt. Seit 1983 gibt es in Bottrop das didaktisch gestaltete Josef-Albers-Museum im Quadrat Bottrop.

Seine bekanntesten Werke sind:






</doc>
<doc id="9572" url="https://de.wikipedia.org/wiki?curid=9572" title="Echter Hopfen">
Echter Hopfen

Der Echte Hopfen ("Humulus lupulus") ist eine Pflanzenart in der Gattung Hopfen und durch seine Verwendung beim Bierbrauen bekannt. Er gehört zur Familie der Hanfgewächse (Cannabaceae).

Echter Hopfen wurde zur Arzneipflanze des Jahres 2007 gekürt.

Die Wildform des Echten Hopfens wächst bevorzugt an stickstoffreichen Standorten mit höherer Bodenfeuchte, zum Beispiel in Auwäldern, aber auch an Waldrändern und in Gebüschen auf trockeneren Flächen. Selten bildet er größere Bestände, kommt aber meist in kleinen Gruppen vor. Er ist in Mitteleuropa eine Charakterart der Ordnung Prunetalia, kommt aber auch in Gesellschaften der Verbände Alno-Ulmion oder Alnion vor. In den Allgäuer Alpen steigt er im Tiroler Teil bei Elbigenalp bis zu einer Höhenlage von 1036 Metern auf.

Aus einem dicken Wurzelstock (Rhizom) treibt der Hopfen meist sehr zahlreich aus. Es sind dünne, raue Stängel mit ankerartigen Kletterhaaren, die eine erstaunliche Haftfähigkeit haben. Diese Triebe werden auch als "Reben" bezeichnet und wachsen im Durchschnitt 10 cm pro Tag. Wie bei allen Stauden kommt es nicht zu einer durchgehenden Verholzung der Pflanze. Hopfen ist ein "Rechtswinder", die oberirdischen Triebe sind einjährig und sterben nach der Samenreife ab. Mit zwei bis sechs Metern Höhe ist die Wildform kleiner als die Zuchtsorten 4–8 m; ebenso sind die Blütenstände deutlich kleiner. In Mitteleuropa ist der Wilde Hopfen nahezu überall anzutreffen, kleinere Lücken gibt es im Alpenvorland. Der Hopfen ist eine zweihäusige Pflanze. Der "männliche" Blütenstand ist eine Rispe, der "weibliche" eine zapfenartige Ähre.

In Parks und Gärten würgt die mehrjährige Schlingpflanze andere teilweise recht große Pflanzen ab. Wegen seines weitverzweigten Wurzelwerkes, über das sich der "Wilde Hopfen" auch fortpflanzt, ist es schwierig, ihn zu beseitigen.

Die Chromosomenzahl beträgt 2n = 20.

Die jungen Hopfensprosse eignen sich als Delikatesse mit feinem harzigen Geschmack sehr gut, wenn sie kurz gekocht werden, entweder im Dampf (bei den noch sehr zarten) oder in Salzwasser (2–4 Minuten). Bei der Ernte kann man die richtige Länge (ca. 10 bis 25 cm) herausfinden, indem man mit den Fingern den Stängel hinauffährt und ihn dabei leicht biegt. Er bricht dann an einem bestimmten Punkt ab und das ist die richtige Stelle, weil der Spross ab da aufwärts noch genügend zart ist.

Die Kultursorten des Echten Hopfens werden landwirtschaftlich angebaut. Die wichtigsten deutschen Anbaugebiete sind die Hallertau in Bayern, das Elbe-Saale-Anbaugebiet in den Bundesländern Sachsen, Thüringen und Sachsen-Anhalt, das Schussental zwischen Tettnang und Ravensburg in Baden-Württemberg und die Region um Spalt in Mittelfranken. Die Ähren heißen in der Hopfenwirtschaft Dolden und finden beim Bierbrauen Verwendung. Allerdings sind auch die jungen Triebe im Frühling und die Samen im Herbst essbar.

Eine Befruchtung durch den Pollen "männlicher" Pflanzen verringert den Ertrag an Bierwürze, verkürzt das Erntezeitfenster (denn überreife Hopfendolden schmecken scheußlich) und erschwert die Verarbeitung in der Brauerei. Darum sind die Felder komplett "pistillat" (botanisch weiblich). Die Dolden besitzen an der verdeckten Oberfläche der Kelchblätter ("Calyxen") und Tragblätter ("Brakteen") Harzkügelchen, aus denen man das gelbe "Lupulin" gewinnen kann. Es wirkt als Geschmacksstoff und Konservierungsmittel. Man unterscheidet grundsätzlich zwischen den "Bitterhopfensorten" und den "Aromahopfensorten". Letztere sind dadurch charakterisiert, dass ihr Bitterungspotential in der Konzentration deutlich geringer ist als jenes des Bitterhopfens. Für den Brauprozess sind hauptsächlich die sogenannten „α-Säuren“, d. h. α-Lupulinsäure oder Humulon und deren Derivate, von Bedeutung; die „β-Säuren“ β-Lupulinsäure (Lupulon) und ihre Abkömmlinge sind für die Geschmacksaromen wichtig. Der α-Säuren-Anteil von Aromasorten beträgt etwa 3–9 % im Vergleich zu 12–20 % bei Bittersorten, jedoch haben Aromasorten dafür deutlich höhere Konzentrationen an aromatisch hoch wirksamen Inhaltsstoffen wie ätherischen Ölen oder Polyphenolen.

Früh im Brauprozess zugesetzter und lange mitgekochter Hopfen erhöht die Hopfenausbeute, die eine chemische Umwandlung der α-Säuren in Iso-α-Säuren darstellt; die Würze wird dadurch bitterer. Später hinzugefügt entsteht ein eher mildes Bier. Faktoren wie beispielsweise die Art des Hopfenprodukts (Pellets, Extrakte usw.) oder auch die Stärke des Kochens und der Extraktgehalt der Würze beeinflussen die Hopfenausbeute mit.

Seine Bedeutung erreichte der Hopfen ursprünglich durch die Tatsache, dass seine Bitterstoffe beim Brauen von Bier aufgrund ihrer bakteriziden Wirksamkeit wesentlich zur Haltbarkeit des Gebräus beitrugen. Die antiseptische Kraft des Hopfens wurde bereits 1153 n. Chr. von Hildegard von Bingen mit den Worten „putredines prohibet in amaritudine sua“ (seine Bitterkeit verhindert die Fäulnis) beschrieben.

Die ältesten schriftlich Quellen zum Hopfenanbau stammen aus dem frühen Mittelalter. Angeblich wurde der Hopfenanbau erstmals im Jahre 736 n. Chr. bei Geisenfeld in der Hallertau erwähnt; konkrete Quellen existieren für die Jahre 768 (Kloster St. Denis nördlich von Paris), 822 (Kloster Corvey) und 859 bis 875 (Hochstift Freising). Eine erste Erwähnung des Hopfens als Brauzusatz findet sich im Jahre 1079. Im Hochmittelalter kommen Wollin, Breslau, Troppau, Brüx, Wismar, Braunschweig und Lübeck als Hauptanbaugebiete hinzu.

Hopfen wird alljährlich im Frühjahr ab Ende März in den Gerüstanlagen von sogenannten Hopfengärten kultiviert.

Vermehrt wird die Pflanze vegetativ über Stecklinge, die auch "Fechser" genannt werden.

Zwei oder drei Triebe werden um einen Draht als Kletterhilfe gelegt und wachsen bis Ende Juli auf die in Deutschland übliche Gerüsthöhe von sieben Metern. Neuere und wiederentdeckte Sorten erfordern andere, meist geringere Gerüsthöhen und damit alternative, teils vorteilhaftere Gerätschaften, was jedoch eine Umstellung erfordert und letztlich ihr Durchsetzungsvermögen merklich hemmt. Sind die Ähren der weiblichen Pflanze reif, werden die Hopfenreben während der etwa dreiwöchigen Erntezeit (letzte August- und erste September-Dekade) knapp über dem Boden abgeschnitten und von den Gerüstanlagen gerissen. War die Ernte früher reine Handarbeit werden heute hierfür Pflückmaschinen eingesetzt. Anschließend werden sie zum Hof gefahren. Dort werden von Pflückmaschinen die Dolden vom Hopfenstock getrennt. Die weichen und feuchten Dolden werden in der Darre getrocknet, bis sie nur noch etwa 11 Prozent Feuchtigkeit enthalten, dann gepresst und gekühlt. Oft wird Hopfen zu Pellets (kleine, gepresste Zylinderstücke) weiterverarbeitet. So erreicht der Hopfen, luftdicht verpackt, längere Haltbarkeit. Wird er zu warm oder nicht luftdicht abgepackt, verliert er schnell die flüchtigen Aromen und in einem Jahr bis zu 35 Prozent seines Brauwertes.

95 Prozent des Hopfens wird für die Bierherstellung verwendet. Er verleiht dem Bier sein ausgeprägtes Aroma und seine typische Bitterkeit. Die Hopfeninhaltsstoffe wirken zusätzlich "beruhigend", "konservierend" und "schaumstabilisierend". Zum Brauen werden ausschließlich die Dolden der "weiblichen" Hopfenpflanzen verwendet. Für 10.000 Liter Bier benötigt man etwa 170 Kilogramm Hopfen. Beim Grünhopfen-Pils wird der Hopfen ohne Trocknung direkt von der Ernte verarbeitet. Ein geringer Anteil des geernteten Hopfens wird zu medizinischen Zwecken, hauptsächlich als Sedierung­smittel, verwendet. Vereinzelt wird Hopfen als Geschmacksbereicherung für einige Liköre und Schnäpse verwendet, inzwischen wird auch Hopfen-Limonade hergestellt.

Hopfen dient auch in vielen alten Bibliotheken als Schutz vor Feuchtigkeit und Ungeziefer. Man legt Hopfendolden hinter den Büchern aus. Sie regulieren die Luftfeuchtigkeit, und ihre ätherischen Öle halten Insekten fern. Die Dolden müssen alle paar Jahre ausgewechselt werden.

Seit einigen Jahren gewinnt auch die Ernte von Hopfenspargel wieder an Bedeutung. Hierbei werden in einem zwei- bis dreiwöchigen Zeitraum in den Monaten März und April (je nach Witterung) die weißen, frisch ausgetriebenen Sprösslinge des Hopfens aus der Erde gegraben und regional als Spezialität angeboten. Die sehr kurze Saison und die zeitaufwändige, weil in Handarbeit erfolgende Ernte machen den Hopfenspargel zu einer der teuersten in Deutschland angebauten Gemüsesorten.

Wertbestimmende Bestandteile der Hopfendolde sind eine Harzfraktion (Hopfenbitterstoffe) und ein ätherisches Öl, das Hopfenöl. Daneben sind Rohfasern (15 %), Eiweiße (20 %) und mineralische Bestandteile (8 %), Polyphenole (Tannine) (2–5 %) enthalten.

Die "weiblichen" Blütenstände sind die sogenannten Hopfenzapfen "Lupuli strobulus" (2,5 – 5 cm), welche die trockenhäutigen Deckblätter tragen. Diese wiederum sind von sandkorngroßen Drüsenhaaren besetzt, die das gelbe bis rötliche Harz beinhalten. Das Harz befindet sich in den Hopfendrüsen "Lupuli glandula" (Hopfenmehl, Hopfenstaub, Lupulin), die durch Abklopfen oder Schütteln der Hopfenzapfen gewonnen werden. Das Hopfenharz gliedert sich in zwei Harzfraktionen, die Hopfendrüsen enthalten etwa 50–80 % Hexan lösliches "Weichharz" und zum anderen das hexanunlösliche "Hartharz". Im frühen 19. Jahrhundert erfolgte die Extraktion des Harzes mit Wasser, Ethanol, Dampf oder auch Schwefelkohlenstoff. Durch die zunehmende Erforschung der Bestandteile des Harzes und seiner lipophilen Komponenten wurden ab dann effektivere Lösungsmittel wie Aceton, Chloroform, Alkohol oder Hexan verwendet. Aufgrund der Angst vor schädlichen Lösungsmittelrückständen wurde dann überkritisches Kohlendioxid verwendet.

Die Extraktion des Harzes liefert die kristallisierbaren, oxidationsempfindlichen Hopfenbittersäuren:
Die Hopfensäuren machen anteilig etwa die Hälfte am Harz aus. Sie sind sehr instabil und nur im frischen Hopfen, nicht jedoch in gelagerter Ware enthalten. Beim Würzkochen (Bierherstellung), aber womöglich auch bei der pharmazeutischen Drogenextraktion entstehen durch Ringverengung stark bittere Isoverbindungen, die Isohumulone. Während der Lagerung der Droge werden durch oxidativen Abbau aus den Hopfenbitterstoffen verschiedene Verbindungen gebildet, darunter das 2-Methyl-3-buten-2-ol, für das im Tierexperiment sedierende Wirkungen festgestellt wurden. Weiterhin enthält das Harz Chalkone (Xanthohumol), 0,5 – 1,5 % Flavonoide und 2–4 % Gerbstoffe. Das gelb gefärbte Xanthohumol – für das eine chemopräventive Wirkung nachgewiesen wurde – kommt spezifisch im Hopfen vor und ist eine analytische Leitsubstanz; beim Brauprozess wird dieses jedoch weitestgehend in Isoxanthohumol umgewandelt.

Im Hopfenöl sind über 150 Einzelstoffe für ihr Vorkommen bekannt, unter denen Mono- und Sesquiterpene (z. B. Myrcen, Humulen und β-Caryophyllen, Farnesene) und verschiedene Fettsäureester hervorzuheben sind. Je nach Mengenverhältnis der Hauptterpene unterscheidet man myrcen- und humulenreiche Hopfensorten. Sie bestimmen maßgeblich das Bieraroma.

Hopfen enthält geringe Mengen an östrogenwirksamen Stoffen, unter denen das potenteste als das zu den Flavonoiden gehörende Hopein (8-Prenylnaringenin) identifiziert wurde. Auch eine antimikrobielle und tuberkulostatische Wirkung sollen Hopfenzapfen haben.

Pharmazeutisch verwendete Drogen sind die Hopfenzapfen ("Lupuli flos", "Strobuli Lupuli", "Strobulus Lupuli"), bei denen es sich um die getrockneten, vollständigen "weiblichen" Blütenstände handelt, und die Hopfendrüsen ("Lupuli glandula", Hopfenmehl, "Lupulin"), die von den Fruchtständen abgesiebten Drüsenhaare. Hopfendrüsen stellen ein grüngelbes klebriges Pulver dar, das aromatisch riecht und würzig bitter schmeckt. Sie werden durch das Ausklopfen der Hopfenzapfen gewonnen.
In gepresster Form werden die Hopfendrüsen als "Hopfenhasch" (Lupu-Hash) als Räucherwerk oder zum Rauchen verwendet.

Zubereitungen aus Hopfenzapfen werden als leichtes Einschlaf- und Beruhigungsmittel genutzt. Im Handel sind Hopfenextrakte als Fertigarzneimittel erhältlich, oft im Gemisch mit anderen pflanzlichen Sedativa wie Baldrian. Deren Wirksamkeit wurde von der Kommission E des BfArM bestätigt.

Welche Inhaltsstoffe für die Wirkung verantwortlich sind, ist noch nicht vollständig geklärt. An erster Stelle sind die Bitterstoffe des Hopfens zu nennen. Aus diesen entsteht durch die Verbindung der Stoffe Humulon und Lupulon bei der Lagerung, Verarbeitung und/oder im menschlichen Körper 2-Methylbut-3-en-2-ol, das wahrscheinlich für die beruhigende Wirkung verantwortlich ist. Die Verbindung wirkt außerdem antibakteriell. Die entsprechend konservierende Wirkung spielt beim Bierbrauen eine wichtige Rolle. Die Bitterstoffe stimulieren auch die Magensaftsekretion, deshalb wird der Hopfen in der Volksheilkunde bei Appetitlosigkeit und bei Verdauungsbeschwerden eingesetzt.

In der Aromatherapie werden Hopfenblüten als „Aromakissen“ oder Hopfenauszüge als Badezusatz verwendet. Eine östrogene Wirkung des Hopfens geht hauptsächlich auf den Gehalt an Hopein (8-Prenylnaringenin) zurück. Der Stoff wirkt als Agonist am Estrogenrezeptor.

Frische Hopfenzapfen können bei Hautkontakt allergische Reaktionen auslösen (Hopfenpflückerkrankheit).

Weltweit existieren mehrere hundert Hopfensorten, von denen allerdings nicht alle aktuell eine wirtschaftliche Bedeutung haben.

70 % der auf deutschen Hopfenflächen angebauten Sorten sind Züchtungen aus dem Hopfenforschungszentrum Hüll.
2016 waren in Deutschland 32 Hopfensorten in Verwendung – Tendenz steigend.



Aromahopfen gehören zu den qualitativ hochwertigsten und teuersten Hopfensorten, denn sie enthalten ein deutlich ausgeprägteres Spektrum an Aromen, als dies bei den üblicherweise eingesetzten Bitterhopfen der Fall ist. Beim Brauprozess entfalten Aromahopfen eine außergewöhnliche Aromenintensität, die dem Bier noch mehr „Körper“ verleiht. Er macht es würziger, aromatischer und charaktervoller. Aromahopfen besitzen jedoch einen geringeren Ertrag an Bitterstoffen, die beim Brauprozess ebenfalls eine wichtige Rolle spielen. Man braucht daher eine deutlich größere Menge. Daher ist dieser Hopfentyp – auch wirtschaftlich gesehen – der hochwertigste, den man zum Bierbrauen einsetzen kann. Die wiederum höhere Hopfengabe bringt mehr Xanthohumol in die Würze bzw. das Bier ein, obwohl Bittersorten einen höheren Gehalt an Xanthohumol aufweisen. Viele Brauereien verwenden dennoch ausschließlich Bitterhopfen. Die edelsten Aromahopfensorten sind die alten Landsorten „Hallertauer Mittelfrüher“, „Spalter“, „Tettnanger“, „Hersbrucker Spät“ und der böhmische „Saazer“, die allerdings sehr empfindlich gegenüber Pilzkrankheiten und Schädlingen sind.

In Tettnang und der Hallertau werden seit 2013 sogenannte "Special Flavor Hops" angebaut, das sind seit 2006 gezüchtete Aroma-Hopfensorten mit natürlichen fruchtigen Geschmacksrichtungen wie beispielsweise



Die drei führenden Hopfenbau-Länder sind heute der Fläche nach die USA, Deutschland und Tschechien. 2016 betrug die geschätzte Hopfenernte in der Hallertau 36.500 Tonnen. 

Im historischen Vergleich betrug 1928 die Welternte 60.300 Tonnen; davon entfielen 14.900 Tonnen auf die USA, 12.300 Tonnen auf Großbritannien, 9.430 Tonnen auf die Tschechoslowakei und 8.370 Tonnen auf das Deutsche Reich.

In Deutschland gibt es sechs größere Hopfenanbaugebiete, die insgesamt auf 18.598 ha produzieren (Stand 2016).
Die folgenden Zahlen geben den Anteil an der deutschen Gesamtfläche (Stand 2010) an:



In Österreich mit insgesamt 248 ha Anbaufläche (Stand 2016) gibt es drei Anbaugebiete: 
Während bis 1939 wesentlich mehr Hopfen angebaut wurde, können heute nur noch 23 % des Landesbedarfes im Inland gedeckt werden.

In der Schweiz befinden sich Anbauflächen von insgesamt 18 ha, nur etwa 10 % des Landesbedarfs kann so aus der einheimischen Produktion in 20 landwirtschaftlichen Betrieben gedeckt werden.

Mit 21.433 ha (Stand 2016) liegt etwa ein Viertel der weltweiten Anbauflächen in Nordamerika in den US-Bundesstaaten Idaho, Oregon und Washington; aus der Hopfenernte 2013 wurden 3.680 Tonnen Alphasäure gewonnen.

Weltweit an dritter Position liegt Tschechien mit 4.783 ha. Das bedeutendste Hopfenanbaugebiet liegt bei Žatec ("Saaz"), weitere bei Úštěk ("Auscha") und Dubá ("Dauba") sowie Tršice ("Tirschitz").

Hopfenanbau mit internationaler Bedeutung findet auch noch in der Volksrepublik China (2.508 ha), in Polen (1.524 ha), in Slowenien (1.528 ha), in der Ukraine (369 ha) und in England (928 ha) statt. Weitere europäische Länder mit Hopfenbau von geringer, manchmal aber regionaler Bedeutung sind Frankreich (453 ha, die in erster Linie im Elsass angebaut werden), Spanien (534 ha vorwiegend um die Stadt León), Rumänien (282 ha), Belgien (155 ha, im Besonderen in der Gegend von Poperinge, Provinz Westflandern) und Slowakei (137 ha).
Außerdem wird in Neuseeland auf ca. 412 ha Hopfen angebaut, der größtenteils exportiert wird. Als Ergebnis der neuseeländischen Züchtungsbemühungen erzielen diese hochwertigen Sorten vergleichsweise hohe Preise. In Australien sind es 488 ha und in Süd-Afrika 402 ha. (Alle Angaben Stand 2016.)

Zu den durch Pilzen verursachten Krankheiten zählen der Echte und der Falsche Mehltau sowie der Hopfenrußtau. Erdflohkäfer und Spinnmilben gehören zu den tierischen Schädlingen. Zu den weiteren Schadtieren gehört bei Massenbefall die Hopfenblattlaus oder auch Hopfenlaus bzw. Phorodon humuli.

Hopfen ist langjährig betrachtet eines der am weitesten preisschwankenden Güter (Größenordnung ca. 1 zu 10), ein Umstand, der sowohl den Hopfenanbau als auch den Hopfeneinkauf zu wirtschaftlich risikoreichen Unternehmungen macht.

Weltmarktführer als Hopfenlieferant und Hersteller von Hopfenprodukten ist die Barth-Haas-Group.


Mit dem Hopfenanbau sind auch kulturelle Traditionen verknüpft wie z. B. die Wahl einer Hopfenkönigin.




</doc>
<doc id="9573" url="https://de.wikipedia.org/wiki?curid=9573" title="Kreis">
Kreis

Ein Kreis ist eine ebene geometrische Figur. Er wird definiert als die Menge aller Punkte einer Ebene, die einen konstanten Abstand zu einem vorgegebenen Punkt dieser Ebene (dem "Mittelpunkt") haben. Der Abstand der Kreispunkte zum Mittelpunkt ist der Radius oder "Halbmesser" des Kreises, er ist eine positive reelle Zahl. Der Kreis gehört zu den klassischen und grundlegenden Objekten der euklidischen Geometrie.

Schon die alten Ägypter und Babylonier versuchten, den Flächeninhalt des Kreises näherungsweise zu bestimmen. Besonders in der griechischen Antike war der Kreis wegen seiner Vollkommenheit von großem Interesse. Beispielsweise versuchte Archimedes erfolglos, mit den Werkzeugen Zirkel und Lineal den Kreis in ein Quadrat mit gleichem Flächeninhalt zu überführen, um so den Flächeninhalt des Kreises bestimmen zu können. Ein solches Verfahren zur Berechnung des Flächeninhalts nennt man die Quadratur des Kreises. Erst 1882 konnte Ferdinand von Lindemann durch Nachweis einer besonderen Eigenschaft der Kreiszahl zeigen, dass diese Aufgabe unlösbar ist.

Nach der eingangs genannten Definition ist ein Kreis eine Kurve, also ein eindimensionales Gebilde, und keine zweidimensionale Fläche. Da das Wort „Kreis“ aber oft ungenau auch für die eingeschlossene Fläche benutzt wird, verwendet man zur Verdeutlichung häufig die Begriffe "Kreislinie, Kreisrand" oder "Kreisperipherie" anstatt Kreis – im Gegensatz zur "Kreisfläche" oder "Kreisscheibe." Mathematiker unterscheiden dann noch zwischen der "abgeschlossenen" Kreisfläche oder -scheibe und der "offenen" (oder dem "Kreisinneren"), je nachdem ob die Kreislinie dazugehört oder nicht.

Eine zusammenhängende Teilmenge des Kreises (also der Kreislinie) ist ein "Kreisbogen." Eine Verbindungsstrecke von zwei Punkten auf der Kreislinie bezeichnet man als "Kreissehne." Zu jeder Sehne gehören zwei Kreisbögen. Die längsten Kreissehnen sind diejenigen, die durch den Mittelpunkt verlaufen, also die Durchmesser. Die zugehörigen Kreisbögen heißen Halbkreise. Ist die Kreissehne kein Durchmesser, so sind die Kreisbögen unterschiedlich lang.

Ein "Kreissektor (Kreisausschnitt)" ist eine Fläche, die von zwei Radien und einem dazwischen liegenden Kreisbogen begrenzt wird. Bilden die zwei Radien einen Durchmesser, wird der Sektor auch als Halbkreis bezeichnet.

"Kreissegmente (Kreisabschnitte)" werden von einem Kreisbogen und einer Kreissehne eingeschlossen.

Ein "Kreisring" entsteht, wenn man aus einem Kreis einen kleineren Kreis mit demselben Mittelpunkt herausschneidet.

Für die Lage einer Geraden in Bezug auf einen gegebenen Kreis gibt es drei Möglichkeiten:

In einer Ebene formula_1 ist ein Kreis formula_2 mit Mittelpunkt formula_3 und Radius formula_4 die Punktmenge

Dabei ist der Radius formula_6 eine positive reelle Zahl, und formula_7 bezeichnet die Länge der Strecke formula_8.

Der doppelte Radius heißt Durchmesser und wird oft mit formula_9 bezeichnet. Radius formula_6 und Durchmesser formula_9 sind durch die Beziehungen formula_12 oder formula_13 miteinander verknüpft.

Manchmal wird auch jede "Strecke," die den Mittelpunkt mit einem Punkt auf der Kreislinie verbindet, als "Radius" bezeichnet, und jede Strecke, die durch den Mittelpunkt geht, und deren beide Endpunkte auf der Kreislinie liegen, als "Durchmesser." Bei dieser Sprechweise ist die "Zahl" formula_6 die "Länge" jedes Radius und die Zahl formula_9 die Länge jedes Durchmessers.

Die offene Kreisfläche ist formal definiert als die Punktmenge

die abgeschlossene Kreisscheibe als

Der Kreis gehört neben dem Punkt und der geraden Linie zu den ältesten Elementen der vorgriechischen Geometrie. Schon vor viertausend Jahren beschäftigten sich die Ägypter mit ihm in ihren Studien zur Geometrie. Sie konnten den Flächeninhalt formula_18 eines Kreises näherungsweise bestimmen, indem sie vom Durchmesser d ein Neuntel seiner Länge abzogen und das Ergebnis mit sich selbst multiplizierten. Sie rechneten also

und bestimmten so näherungsweise (mit einer Abweichung von nur etwa +0,6 %) den Flächeninhalt einer Kreisfläche. Diese Näherung wurde in der altägyptischen Abhandlung Papyrus Rhind gefunden, sie lässt sich erhalten, wenn man den Kreis durch ein unregelmäßiges Achteck annähert.

Die Babylonier (1900 bis 1600 vor Christus) benutzten eine ganz andere Methode, um den Flächeninhalt der Kreisscheibe zu berechnen. Im Gegensatz zu den Ägyptern gingen sie vom Kreisumfang formula_20 aus, den sie als dreimal den Kreisdurchmesser formula_9 schätzten. Der Flächeninhalt wurde dann auf ein Zwölftel des Quadrates des Umfanges geschätzt, also
mit einer Abweichung von −4,5 % ein deutlich schlechteres Ergebnis.

Die Babylonier beschäftigten sich aber auch schon mit Kreissegmenten. Sie konnten die Länge der Sehne oder die Höhe des Kreissegments (die senkrecht auf der Sehnenmitte stehende Strecke zwischen Sehne und Umfang) berechnen. Damit begründeten sie die Sehnengeometrie, die später von Hipparch weiterentwickelt wurde und die Claudius Ptolemaios an den Anfang seines astronomischen Lehrbuches "Almagest" stellte.

Die Griechen werden meist als die Begründer der Wissenschaft von der Natur angesehen. Als der erste bedeutende Philosoph dieser Zeit, der sich mit Mathematik beschäftigte, gilt Thales von Milet (624–546 v. Chr.). Er brachte Wissen über die Geometrie aus Ägypten mit nach Griechenland, wie zum Beispiel die Aussage, dass der Durchmesser den Kreis halbiert. Andere Aussagen zur Geometrie wurden von Thales selbst aufgestellt. Der heute nach Thales benannte Satz besagt, dass Peripheriewinkel im Halbkreis rechte Winkel sind. Insbesondere war Thales der erste, bei dem der Begriff des Winkels auftrat.

Die erste bekannte Definition des Kreises geht auf den griechischen Philosophen Platon (428/427–348/347 v. Chr.) zurück, die er in seinem Dialog "Parmenides" formulierte: 

Zirka 300 Jahre vor Christus lebte der griechische Mathematiker Euklid von Alexandria. Über ihn selbst ist wenig bekannt, aber sein Werk im Bereich der Geometrie war beachtlich. Sein Name ist heute noch in Zusammenhängen wie euklidischer Raum, euklidische Geometrie oder euklidische Metrik in Gebrauch. Sein wichtigstes Werk waren "Die Elemente," eine dreizehnbändige Abhandlung, in der er die Arithmetik und Geometrie seiner Zeit zusammenfasste und systematisierte. Er folgerte die mathematischen Aussagen aus Postulaten und begründete damit die euklidische Geometrie. Der dritte Band der Elemente beschäftigte sich mit der Lehre über den Kreis.

Von Archimedes, der vermutlich zwischen 287 v. Chr. und 212 v. Chr. auf Sizilien lebte, ist eine ausführliche Abhandlung mit dem Titel "Kreismessung" überliefert. Er bewies in dieser Arbeit, dass der Flächeninhalt eines Kreises gleich dem Flächeninhalt eines rechtwinkligen Dreiecks mit dem Kreisradius als der einen und dem Kreisumfang als der anderen Kathete ist. Der Flächeninhalt des Kreises lässt sich also als angeben. Mit dieser Erkenntnis führte er das Problem der Quadratur des Kreises auf die Frage der Konstruierbarkeit des Umfangs aus dem vorgegebenen Radius zurück.

In seiner Abhandlung "Kreismessung" konnte Archimedes ebenfalls zeigen, dass der Umfang eines Kreises größer als 3/ und kleiner als 3/ des Durchmessers ist. Für praktische Zwecke wird diese Näherung / (~ 3,143) heute noch verwendet.

Aus diesen beiden Aussagen folgert man, dass sich der Flächeninhalt eines Kreises zum Quadrat seines Durchmessers nahezu wie / verhält. Euklid war bereits bekannt, dass sich der Flächeninhalt eines Kreises proportional zum Quadrat seines Durchmessers verhält. Archimedes gibt hier eine gute Näherung der Proportionalitätskonstante an.

In einer weiteren Arbeit "Über Spiralen" beschreibt Archimedes die Konstruktion der später nach ihm benannten archimedischen Spirale. Mit dieser Konstruktion war es Archimedes möglich, den Umfang eines Kreises auf einer Geraden abzutragen. Auf diese Weise konnte nun der Flächeninhalt eines Kreises exakt bestimmt werden. Jedoch kann diese Spirale nicht mit Zirkel und Lineal konstruiert werden.

Apollonios von Perge lebte zirka 200 Jahre vor Christus. In seiner Kegelschnittlehre "Konika" fasste er unter anderem die Ellipse und den Kreis als Schnitte eines geraden Kreiskegels auf – genauso wie es heute noch in der algebraischen Geometrie definiert wird. Seine Erkenntnisse gehen auf seine Vorgänger Euklid und Aristaios (um 330 v. Chr.) zurück, deren verfasste Abhandlungen über Kegelschnitte jedoch nicht mehr überliefert sind.

Nach Apollonios ist weiterhin das apollonische Problem benannt, zu drei gegebenen Kreisen mit den euklidischen Werkzeugen Lineal und Zirkel die Kreise zu konstruieren, die die gegebenen berühren. Jedoch im Vergleich zu Euklids Elementen, die auch im Mittelalter die Grundlage der Geometrie bildeten, fanden die Werke von Apollonios zunächst nur im islamischen Bereich Beachtung. In Westeuropa erlangten seine Bücher erst im 17. Jahrhundert größere Bedeutung, als Johannes Kepler die Ellipse als die wahre Bahn eines Planeten um die Sonne erkannte.
In der Wissenschaftsgeschichte nennt man den Zeitraum zwischen 1400 n. Chr. und 1630 n. Chr. üblicherweise Renaissance, auch wenn der zeitliche Abschnitt nicht mit der Periodisierung etwa der Kunstgeschichte übereinstimmt. In dieser Zeit fanden Euklids "Elemente" wieder mehr Beachtung. Sie gehörten zu den ersten gedruckten Büchern und wurden in den darauffolgenden Jahrhunderten in vielen verschiedenen Ausgaben verlegt. Erhard Ratdolt stellte 1482 in Venedig die erste gedruckte Ausgabe der "Elemente" her. Eine der bedeutendsten Ausgaben von Euklids "Elementen" wurde von dem Jesuiten Christoph Clavius herausgegeben. Er fügte den eigentlichen Texten Euklids neben den spätantiken Büchern XIV und XV noch ein sechzehntes Buch und weitere umfangreiche Ergänzungen hinzu. Beispielsweise ergänzte er eine Konstruktion der gemeinsamen Tangenten zweier Kreise.

Nach Vorleistungen von Leonhard Euler, der die eulersche Identität aufstellte, Johann Heinrich Lambert und Charles Hermite konnte Ferdinand von Lindemann 1882 beweisen, dass die Zahl π transzendent ist. Das heißt, es gibt keine Polynomfunktion mit rationalen Koeffizienten, für die π eine Nullstelle ist. Da jedoch schon im 17. Jahrhundert gezeigt wurde, dass die Kreiszahl π eine Nullstelle einer solchen Polynomfunktion sein müsse, damit die Quadratur des Kreises mit Zirkel und Lineal funktioniere, wurde somit zugleich bewiesen, dass es kein solches Verfahren geben kann.

In der analytischen Geometrie werden geometrische Objekte mit Hilfe von Gleichungen beschrieben. Punkte in der Ebene werden dazu meist durch ihre kartesischen Koordinaten formula_23 dargestellt und ein Kreis ist dann die Menge aller Punkte, deren Koordinaten die jeweilige Gleichung erfüllen.

Der euklidische Abstand eines Punktes formula_24 vom Punkt formula_25 berechnet sich als
Durch Quadrieren der definierenden Gleichung formula_27 ergibt sich die Koordinatengleichung
für die Punkte formula_23 auf dem Kreis mit Mittelpunkt formula_25 und Radius formula_6.
Ausmultipliziert ergibt sich daraus:
mit

Ein wichtiger Spezialfall ist die Koordinatengleichung des Einheitskreises

Da der Kreis kein Funktionsgraph ist, lässt er sich auch nicht durch eine Funktionsgleichung darstellen. Behelfsweise kann ein "Paar" von Funktionsgleichungen
verwendet werden. Für den Einheitskreis vereinfacht sich dieses zu

Eine andere Möglichkeit, einen Kreis durch Koordinaten zu beschreiben, bietet die Parameterdarstellung (siehe auch Polarkoordinaten):

Hier werden die Koordinaten formula_40 und formula_41 durch den Parameter formula_42 ausgedrückt, der alle Werte mit formula_43 annehmen kann.

Wendet man auch diese Gleichungen speziell auf den Einheitskreis an, so erhält man:

Es ist auch eine Parameterdarstellung ohne den Rückgriff auf trigonometrische Funktion möglich "(rationale Parametrisierung)," allerdings wird dabei die gesamte Menge der reellen Zahlen als Parameterbereich benötigt und der Punkt formula_45 wird nur als Grenzwert für formula_46 erreicht.

Für den Einheitskreis ergibt sich dann:

In der komplexen Zahlenebene lässt sich der Kreis um formula_49 mit Radius formula_4 durch die Gleichung
darstellen. Mit Hilfe der komplexen Exponentialfunktion erhält man die Parameterdarstellung

Die Koordinatengleichung des Kreises durch 3 vorgegebene Punkte formula_53, die nicht auf einer Gerade liegen, ergibt sich durch Umformung der "3-Punkteform" (Beseitigung der Nenner und quadratische Ergänzung):

Da alle Kreise ähnlich sind, ist das Verhältnis von Kreisumfang und Kreisdurchmesser für alle Kreise konstant. Der Zahlenwert dieses Verhältnisses wird in der Elementargeometrie als Definition für die Kreiszahl formula_55 verwendet. Es handelt sich hierbei um eine transzendente Zahl, bei der sich außerdem gezeigt hat, dass sie in vielen Bereichen der höheren Mathematik eine herausragende Bedeutung besitzt.

Im Rahmen der Elementargeometrie ist formula_56 das Verhältnis von Kreisumfang formula_20 zu dessen Durchmesser formula_9, und zwar für beliebige Kreise. Somit gilt

Mit formula_60 ist der Radius des Kreises gemeint.

Der Flächeninhalt der Kreisfläche formula_18 (lat. "area:" Fläche) ist proportional zum Quadrat des Radius formula_6 bzw. des Durchmessers formula_9 des Kreises. Man bezeichnet ihn auch als Kreisinhalt.

Um die Formel für den Kreisinhalt zu erhalten, sind Grenzwert-Betrachtungen unerlässlich. Recht anschaulich ergibt sich eine solche aus der nebenstehenden Zeichnung:

Die Kreisfläche ist zerlegungsgleich mit der Fläche der rechten Figur. Diese nähert sich bei feiner werdender Sektoreinteilung einem Rechteck an mit der Länge formula_64 und der Breite Die Flächenformel ist somit

Die Flächenformel kann zum Beispiel durch Integrieren der Kreisgleichung oder mit Hilfe der unten beschriebenen Annäherung durch regelmäßige Vielecke bewiesen werden.
Der Durchmesser formula_9 eines Kreises mit Flächeninhalt formula_18 und mit Radius formula_6 lässt sich durch

berechnen.

Eine im Vergleich zu den bis jetzt beschriebenen Größen weniger elementare Eigenschaft des Kreises ist die Krümmung. Zur präzisen Definition der Krümmung werden Begriffe aus der Analysis benötigt, sie lässt sich jedoch aufgrund der Symmetrieeigenschaften des Kreises einfach berechnen.
Anschaulich gibt die Krümmung in jedem Punkt formula_70 an, wie stark der Kreis in der unmittelbaren Umgebung des Punktes formula_70 von einer Geraden abweicht. Die Krümmung formula_72 des Kreises im Punkt formula_70 lässt sich durch
berechnen, wobei formula_6 wieder der Radius des Kreises ist. Im Gegensatz zu anderen mathematischen Kurven hat der Kreis in jedem Punkt die gleiche Krümmung. Außer dem Kreis hat nur noch die Gerade eine konstante Krümmung, mit Bei allen anderen Kurven ist die Krümmung vom Punkt formula_70 abhängig.

Da die Kreiszahl formula_56 eine transzendente Zahl ist, gibt es kein Konstruktionsverfahren mit Zirkel und Lineal, mit dem man den Flächeninhalt exakt bestimmen kann. Außerdem sind transzendente Zahlen auch irrational, und daher hat formula_56 auch keine endliche Dezimalbruchentwicklung, weshalb der Kreisflächeninhalt bei rationalem Radius auch keine endliche Dezimalbruchentwicklung besitzt. Aus diesen Gründen wurden bis heute unterschiedliche Näherungsverfahren für den Flächeninhalt und somit auch den Umfang eines Kreises entwickelt. Manche der Näherungsverfahren, wie beispielsweise das im Abschnitt "Annäherung durch Vielecke" erläuterte Verfahren, können durch mehrfache Wiederholung ein beliebig genaues Ergebnis liefern.

Ein Kreis mit Radius formula_6 wird mit einem Quadrat der Seitenlänge formula_82 umschrieben. Ihm wird weiter ein Quadrat mit der Diagonalen formula_82 einbeschrieben. Der Flächeninhalt des äußeren Quadrates ist der des inneren nach der Dreiecksflächen­formel formula_84 und der Mittelwert ist somit Mit dieser Näherung formula_85 wird die Kreisfläche mit einem relativen Fehler von weniger als 5 % bestimmt.

Die Kreisfläche lässt sich annähernd bestimmen, indem man ihr viele kleine Quadrate unterlegt (z. B. mit Millimeterpapier). Zählt man alle Quadrate, die vollständig innerhalb des Kreises liegen, so erhält man einen etwas zu niedrigen Wert für die Fläche, zählt man auch alle Quadrate mit, die den Kreis lediglich schneiden, so ist der Wert zu groß. Der Mittelwert beider Ergebnisse ergibt eine Näherung für den Flächeninhalt des Kreises, deren Güte mit der Feinheit des Quadratrasters steigt.
Man kann die Fläche des Kreises aus im Verhältnis zum Radius sehr schmalen Streifen zusammensetzen. Dazu verwendet man die Gleichungen

Bei einer anderen Möglichkeit zur Kreisflächenbestimmung ist in den Kreis ein regelmäßiges Sechseck einzuzeichnen, dessen Ecken auf dem Kreis liegen. Werden nun die Seitenmitten vom Mittelpunkt aus auf den Kreis projiziert und diese neuen Punkte mit den alten Ecken verbunden, so entsteht ein regelmäßiges Zwölfeck. Wird dieser Vorgang wiederholt, entstehen nacheinander ein 24-Eck, ein 48-Eck und so fort.

In jedem Sechseck sind die Seiten gleich lang wie der Umkreisradius. Die Seiten der folgenden Vielecke ergeben sich mit Hilfe des Satzes von Pythagoras jeweils aus den Seiten der vorhergehenden. Aus den Seiten lassen sich die Flächen der Vielecke durch Dreiecksflächen­berechnung exakt bestimmen. Sie sind alle etwas kleiner als die Kreisfläche, der sie sich bei steigender Eckenzahl jedoch annähern.

Entsprechend kann man mit einem Sechseck verfahren, das von außen an den Kreis gezeichnet ist, dessen Seitenmitten also auf ihm liegen. Man erhält eine fallende Folge von Flächenmaßen, deren Grenzwert wiederum die Kreisfläche ist.

Der Kreis ist eine geometrische Figur von sehr hoher Symmetrie. Jede Gerade durch seinen Mittelpunkt ist eine Symmetrieachse. Zudem ist der Kreis rotationssymmetrisch, d. h., jede Drehung um den Mittelpunkt bildet den Kreis auf sich selbst ab. In der Gruppentheorie werden die genannten Symmetrieeigenschaften des Kreises durch seine Symmetriegruppe charakterisiert. Formal ergibt sich dafür die orthogonale Gruppe formula_88, das ist die Gruppe der orthogonalen 

Alle Kreise mit dem gleichen Radius sind zueinander kongruent, lassen sich also durch Parallelverschiebungen aufeinander abbilden. Zwei beliebige Kreise sind zueinander ähnlich. Sie lassen sich stets durch eine zentrische Streckung und eine Parallelverschiebung aufeinander abbilden.

Eine Kreissehne mit Endpunkten A und B teilt einen gegebenen Kreis in zwei Kreisbögen. Ein Winkel formula_89 mit Scheitel C auf einem der Kreisbögen wird "Umfangswinkel" oder "Peripheriewinkel" genannt. Der Winkel formula_90 mit Scheitel im Mittelpunkt M heißt "Mittelpunktswinkel" oder "Zentriwinkel."

Im Spezialfall, dass die Sehne den Mittelpunkt enthält, also ein Durchmesser des Kreises ist, ist der Mittelpunktswinkel ein gestreckter Winkel mit 180°. In dieser Situation gilt eine grundlegende Aussage der Kreisgeometrie, der Satz von Thales: Er besagt, dass Umfangswinkel über einem Durchmesser stets rechte Winkel sind, also 90° betragen. Der Kreis um das rechtwinklige Dreieck wird in dieser Situation auch "Thaleskreis" genannt.

Auch im Fall einer beliebigen Kreissehne sind alle Umfangswinkel, die auf dem gleichen Kreisbogen liegen, gleich groß. Diese Aussage wird auch "Umfangswinkelsatz" genannt. Der Kreisbogen, auf dem die Scheitel der Umfangswinkel liegen, heißt "Fasskreisbogen." Liegen Umfangswinkel und Zentriwinkel auf der gleichen Seite der Sehne, dann ist der Zentriwinkel doppelt so groß wie der Umfangswinkel "(Kreiswinkelsatz)." Zwei Umfangswinkel, die auf gegenüberliegenden Seiten der Sehne liegen, ergänzen einander zu 180°.

Der Umfangswinkel ist genauso groß wie der spitze "Sehnentangentenwinkel" zwischen der Sehne und der durch einen ihrer Endpunkte verlaufenden Tangente "(Sehnentangentenwinkelsatz)."

Für Kreise gilt der Sehnensatz, der besagt: Schneiden zwei Sehnen [AC] und [BD] einander in einem Punkt S, so gilt
d. h., die Produkte der jeweiligen Sehnenabschnitte sind gleich.

Zwei Sehnen eines Kreises, die einander nicht schneiden, können verlängert werden zu Sekanten, die entweder parallel sind oder einander in einem Punkt S außerhalb des Kreises schneiden. Ist Letzteres der Fall, so gilt analog zum Sehnensatz der Sekantensatz

Im Fall einer Sekante, die den Kreis in den Punkte A und C schneidet, und einer Tangente, die den Kreis im Punkt B berührt, gilt der Sekanten-Tangenten-Satz: Ist S der Schnittpunkt von Sekante und Tangente, so folgt

Sind A, B, C drei Punkte, die nicht auf einer Geraden liegen, also ein nicht ausgeartetes Dreieck bilden, dann existiert ein eindeutig bestimmter Kreis durch diese Punkte, nämlich der Umkreis des Dreiecks ABC. Der Mittelpunkt des Umkreises ist der Schnittpunkt der drei Mittelsenkrechten des Dreiecks. Ebenso kann jedem Dreieck ein eindeutig bestimmter Kreis "einbeschrieben" werden, der die drei Seiten berührt, d. h., die Dreiecksseiten bilden Tangenten des Kreises. Dieser Kreis wird Inkreis des Dreiecks genannt. Sein Mittelpunkt ist der Schnittpunkt der drei Winkelhalbierenden.

In der Elementargeometrie werden noch weitere Kreise am Dreieck betrachtet: Die Ankreise liegen außerhalb des Dreiecks und berühren eine Seite und die Verlängerungen der beiden anderen Seiten. Ein weiterer interessanter Kreis am Dreieck ist der Feuerbachkreis, benannt nach Karl Wilhelm Feuerbach. Auf ihm liegen die drei Seitenmittelpunkte und die drei Fußpunkte der Höhen. Da auf ihm außerdem die drei Mittelpunkte der Strecken zwischen dem Höhenschnittpunkt und den Ecken des Dreiecks liegen, wird der Feuerbachkreis auch "Neunpunktekreis" genannt. Sein Mittelpunkt liegt wie der Schwerpunkt, der Umkreismittelpunkt und der Höhenschnittpunkt auf der eulerschen Geraden.

Im Gegensatz zu Dreiecken besitzen Polygone mit mehr als drei Ecken im Allgemeinen keinen Umkreis oder Inkreis. Für regelmäßige Vielecke existieren beide allerdings stets. Ein Viereck, das einen Umkreis besitzt, wird Sehnenviereck genannt. Ein konvexes Viereck ist genau dann ein Sehnenviereck, wenn sich gegenüberliegende Winkel zu 180° ergänzen. Ein Viereck, das einen Inkreis besitzt, wird Tangentenviereck genannt. Ein konvexes Viereck ist genau dann ein Tangentenviereck, wenn die Summe der Seitenlängen zweier gegenüberliegender Seiten gleich der Summe der beiden anderen Seitenlängen ist.

Die Kreisspiegelung, auch Inversion genannt, ist eine spezielle Abbildung der ebenen Geometrie, die eine „Spiegelung“ der euklidischen Ebene an einem gegebenen Kreis formula_2 mit Mittelpunkt formula_95 und Radius formula_6 beschreibt. Ist formula_97 ein gegebener Punkt, dann ist sein Bildpunkt formula_98 dadurch bestimmt, dass er auf der Halbgeraden formula_99 liegt und sein Abstand von formula_95 die Gleichung
erfüllt. Die Kreisspiegelung bildet das Innere des gegebenen Kreises formula_2 auf sein Äußeres ab und umgekehrt. Alle Kreispunkte von formula_2 werden auf sich selbst abgebildet. Kreisspiegelungen sind winkeltreu, orientierungsumkehrend und kreistreu. Letzteres bedeutet, dass verallgemeinerte Kreise – das sind Kreise und Geraden – wieder auf verallgemeinerte Kreise abgebildet werden.

Die Hintereinanderausführung zweier Kreisspiegelungen ergibt eine Möbiustransformation. Möbiustransformationen – eine weitere wichtige Klasse von Abbildungen der Ebene – sind daher ebenfalls winkeltreu und kreistreu, allerdings orientierungserhaltend.

Kreisspiegelungen und Möbiustransformationen lassen sich besonders übersichtlich mit Hilfe komplexer Zahlen darstellen:
Bei einer Kreisspiegelung eines Punktes formula_104 an dem Kreis formula_105 lautet die Formel für den Bildpunkt formula_106
Für die Spiegelung am Einheitskreis gilt einfach formula_108.

Möbiustransformationen der komplexen Ebene werden durch "gebrochen lineare Funktionen" der Gestalt
mit formula_110 und formula_111 dargestellt.

Ein klassisches Problem der Geometrie ist die Konstruktion geometrischer Objekte mit Zirkel und Lineal in endlich vielen Konstruktionsschritten aus einer gegebenen Punktemenge. In jedem Schritt dürfen dabei Geraden durch gegebene oder bereits konstruierte Punkte gezogen werden sowie Kreise um solche Punkte mit gegebenem oder bereits konstruiertem Radius gezogen werden. Die dadurch konstruierten Punkte ergeben sich als Schnittpunkte zweier Geraden, zweier Kreise oder einer Geraden mit einem Kreis. Naturgemäß spielen daher bei allen Konstruktionen mit Zirkel und Lineal Kreise eine wichtige Rolle.

Im Folgenden sollen exemplarisch einige Konstruktionen angesprochen werden, die im Zusammenhang mit der Geometrie von Kreisen von Bedeutung sind.

Für die Konstruktion des Thaleskreises über einer gegebenen Strecke formula_112 wird zunächst der Mittelpunkt formula_113 dieser Strecke konstruiert, der auch der Mittelpunkt des Thaleskreises ist. Dazu werden um formula_114 und formula_115 jeweils zwei kurze Kreisbögen mit dem gleichen Radius formula_6 geschlagen, wobei formula_6 so groß gewählt werden muss, dass die vier Kreisbögen sich in zwei Punkten formula_118 und formula_119 schneiden. Das ist z. B. für formula_120 der Fall. Die Strecke formula_121 schneidet dann formula_112 im Mittelpunkt Der gesuchte Thaleskreis ist nun der Kreis mit Mittelpunkt formula_113 und Radius 

Gegeben sei ein Punkt formula_124 außerhalb eines Kreises formula_2 mit Mittelpunkt formula_113 und es sollen die beiden Tangenten an den Kreis konstruiert werden, die durch den Punkt formula_124 laufen. Diese elementare Konstruktionsaufgabe lässt sich einfach mit Hilfe des Satzes von Thales lösen: Man konstruiert den Thaleskreis mit der Strecke formula_128 als Durchmesser. Die Schnittpunkte dieses Kreises mit formula_2 sind dann die Berührpunkte der gesuchten Tangenten.

Die Fläche eines Kreises lässt sich geometrisch verdoppeln, indem ein Quadrat gezeichnet wird, dessen eine Ecke im Kreismittelpunkt liegt, wobei zwei weitere Ecken auf dem Kreisbogen liegen. Durch die vierte Ecke wird ein Kreis um den alten Mittelpunkt gezogen. Dieses Verfahren wurde im 13. Jahrhundert im Bauhüttenbuch des Villard de Honnecourt dargestellt.
Dieses Verfahren funktioniert, da (nach dem Satz des Pythagoras)
und damit der Flächeninhalt des großen Kreises
genau doppelt so groß ist, wie der des kleinen Kreises.
Ein weiteres bereits in der Antike untersuchtes Konstruktionsproblem ist die Kreisteilung. Hierbei soll zu einer gegebenen natürlichen Zahl formula_132 einem gegebenen Kreis ein regelmäßiges einbeschrieben werden. Die auf dem Kreis gelegenen Eckpunkte teilen diesen dann in formula_132 gleich lange Kreisbögen. Diese Konstruktion ist nicht für alle formula_132 möglich: Mit Hilfe der algebraischen Theorie der Körpererweiterungen lässt sich zeigen, dass sie genau dann durchführbar ist, wenn formula_132 eine Primfaktorzerlegung der Form
hat mit formula_137 und paarweise verschiedenen fermatschen Primzahlen also Primzahlen der Form 
Damit ist die Konstruktion also beispielsweise für formula_138 möglich, jedoch nicht für z. B. Carl Friedrich Gauß wies im Jahre 1796 nach, dass die Konstruktion des regelmäßigen Siebzehnecks unter alleiniger Verwendung von Zirkel und Lineal möglich ist.

In der modernen Analysis werden die trigonometrischen Funktionen und die Kreiszahl formula_56 üblicherweise zunächst ohne Rückgriff auf die elementargeometrische Anschauung und auf spezielle Eigenschaften des Kreises definiert. So lassen sich etwa Sinus und Kosinus über ihre Darstellung als Potenzreihe definieren. Eine gängige Definition für den Wert von formula_56 ist dann das Doppelte der kleinsten positiven Nullstelle des Kosinus.

In der Differentialgeometrie, einem Teilgebiet der Analysis, das geometrische Formen mit Hilfe der Differential- und Integralrechnung untersucht, werden Kreise als spezielle Kurven angesehen. Diese Kurven lassen sich mit Hilfe der oben genannten Parameterdarstellung als Weg beschreiben. Legt man den Koordinatenursprung in den Mittelpunkt eines Kreises mit Radius dann ist durch die Funktion formula_141 mit
eine solche Parametrisierung gegeben. Mit Hilfe der trigonometrischen Formel formula_143 folgt für die euklidische Norm der parametrisierten Punkte das heißt, sie liegen tatsächlich auf einem Kreis mit Radius 
Da Sinus und Kosinus formula_144-periodische Funktionen sind, entspricht das Definitionsintervall formula_145 von formula_146 genau einem Kreisumlauf.

Der Umfang des Kreises ergibt sich als Länge des Weges formula_146 durch Integration zu

Analog gilt für die Länge formula_149 des durch formula_150 gegebenen Teilkreisbogens Dadurch erhält man als Parametrisierung des Kreises nach der Bogenlänge
mit formula_152.

Der Flächeninhalt formula_18 der Kreisscheibe formula_154, also das Maß der Menge kann als (zweidimensionales) Integral
dargestellt werden. Um die etwas mühsame Berechnung dieses Integrals in kartesischen Koordinaten zu umgehen, ist es günstig, eine Transformation formula_156, formula_157 auf Polarkoordinaten durchzuführen. Damit ergibt sich

Eine andere Möglichkeit zur Berechnung der Kreisfläche besteht darin, die Sektorformel von Leibniz auf die Parameterdarstellung des Kreisrandes anzuwenden. Mit formula_159, formula_160 erhält man damit ebenfalls

Für die oben hergeleitete Parametrisierung formula_162 des Kreises nach seiner Bogenlänge ergibt sich
Für die Krümmung des Kreises erhält man daher
Die Krümmung des Kreises ist also konstant und der Krümmungsradius formula_165 ist gerade sein Radius.

In der Differentialgeometrie wird gezeigt, dass eine ebene Kurve bis auf Kongruenz durch ihre Krümmung eindeutig bestimmt ist. Die einzigen ebenen Kurven mit konstanter positiver Krümmung sind daher Kreisbögen. Im Grenzfall, dass die Krümmung konstant gleich 0 ist, ergeben sich Geradenstücke.

Unter allen Flächen der euklidischen Ebene mit gegebenem Umfang besitzt die Kreisfläche den größten Flächeninhalt. Umgekehrt hat die Kreisfläche bei gegebenem Flächeninhalt den kleinsten Umfang. In der Ebene ist der Kreis daher die eindeutig bestimmte Lösung des sog. isoperimetrischen Problems. Obwohl diese anschaulich einleuchtende Tatsache schon den Mathematikern im antiken Griechenland bekannt war, wurden formale Beweise erst im 19. Jahrhundert erbracht. Da eine Kurve gesucht ist, die ein Funktional maximiert, nämlich den umschlossenen Flächeninhalt, handelt es sich dabei aus moderner Sicht um ein Problem der Variationsrechnung. Ein gängiger Beweis für stückweise stetige Kurven verwendet die Theorie der Fourierreihen.

Es ist möglich, den Kreis als Objekt der Ebene in den dreidimensionalen Raum zu verallgemeinern. Dann erhält man die Hülle einer Kugel. Dieses Objekt wird in der Mathematik Sphäre oder genauer 2-Sphäre genannt. Analog lässt sich die 2-Sphäre auf formula_132 Dimensionen zur verallgemeinern. In diesem Kontext nennt man den Kreis auch 1-Sphäre.

In der ebenen Geometrie kann der Kreis als spezielle Ellipse aufgefasst werden, bei der die beiden Brennpunkte mit dem Kreismittelpunkt zusammenfallen. Beide Halbachsen sind dabei gleich dem Kreisradius. Der Kreis ist daher ein spezieller Kegelschnitt: Er entsteht als Schnitt eines geraden Kreiskegels mit einer Ebene senkrecht zu Kegelachse. Er ist damit ein Spezialfall einer zweidimensionalen Quadrik.
Hierbei ergibt sich eine weitere, äquivalente Definition für Kreise (Kreis des Apollonios): Ein Kreis ist die Menge aller Punkte in der Ebene, für die der Quotient formula_167 ihrer Abstände von zwei gegebenen Punkten konstant ist. Die beiden Punkte liegen auf einem von formula_168 ausgehenden Strahl im Abstand formula_169 bzw. formula_170 und wechselseitig auf der Polaren des jeweils anderen Punktes als Pol. Ähnliche Definitionen gibt es auch für die Ellipse (konstante Summe), Hyperbel (konstante Differenz) und die Cassinische Kurve (konstantes Produkt der Abstände).

In der synthetischen Geometrie können Kreise in bestimmten affinen Ebenen (zum Beispiel präeuklidischen Ebenen) ohne einen Abstandsbegriff allein durch eine "Orthogonalitätsrelation" definiert werden, indem der Satz vom Umkreis (Mittellotensatz) zur Definition des Kreises verwendet wird. Dadurch kann dann ein schwächerer Begriff der „Abstands-“ oder „Längengleichheit“ von Punktepaaren formula_171 in solchen Ebenen eingeführt werden. → Siehe dazu Präeuklidische Ebene.

Für das Zeichnen von angenäherten Kreisen in einem Punktraster wurden mehrere Algorithmen entwickelt, siehe dazu Rasterung von Kreisen. Diese Verfahren sind insbesondere für die Computergrafik von Belang. Für die zweifarbige Rasterung von Kreisen reichen die Grundrechenarten aus.





</doc>
<doc id="9574" url="https://de.wikipedia.org/wiki?curid=9574" title="Protestantismus">
Protestantismus

Mit dem (ursprünglich politischen) Begriff Protestanten werden im engeren Sinne die Angehörigen der christlichen Konfessionen bezeichnet, die, ausgehend von Deutschland und der Schweiz, vor allem in Mittel- und Nordeuropa durch die Reformation des 16. Jahrhunderts entstanden sind und sich seitdem in verschiedene Gruppen weltweit weiterentwickelt haben.

Allgemeiner werden auch nachreformatorisch entstandene Konfessionsrichtungen als evangelisch bezeichnet, die gleiche oder ähnliche Grundsätze wie die reformatorischen Kirchen vertreten und sich deshalb von der römisch-katholischen Kirche distanzierten. In diesem Sinne wird beispielsweise die anglikanische Kirche zum Protestantismus gezählt. Nach der Unabhängigkeitserklärung nannten sich die amerikanischen Anglikaner "Protestant Episcopal Church in the United States of America". Auch die evangelischen Freikirchen gehören zum evangelischen Spektrum. Hierzu zählen im deutschsprachigen Raum unter anderem die bereits in der Reformationszeit entstandenen Mennoniten sowie die Baptisten, die Methodisten, die Siebenten-Tags-Adventisten und die Pfingstler. Die bereits im 12. Jahrhundert entstandenen Waldenser schlossen sich im 16. Jahrhundert dem schweizerischen beziehungsweise französischen Protestantismus an. Die im englischsprachigen Raum verbreiteten Presbyterianer, Kongregationalisten und eine Anzahl anderer Kirchen gehören zur reformierten Kirchengemeinschaft. Auch die Unitarier entstanden als reformatorische Kirche, ein Teil von ihnen hat sich jedoch ab dem späten 19. Jahrhundert vom Christentum gelöst. Die Quäker entstanden ebenfalls im Umkreis der englischen Reformation.

Die einflussreichsten Reformatoren waren Martin Luther und Philipp Melanchthon (Evangelisch-lutherische Kirchen), Ulrich Zwingli, Johannes Calvin und John Knox (Reformierte Kirchen) sowie Thomas Cranmer und Martin Bucer (Anglikanische Kirche). Die führenden Theologen in der Frühzeit der mitgliederstärksten Freikirchen waren Konrad Grebel, Felix Manz und Menno Simons (Täufer/Mennoniten), Robert Browne und John Cotton (Kongregationalisten), Thomas Helwys und John Smyth (Baptisten), George Fox (Quäker) sowie John Wesley, Charles Wesley und George Whitefield (Methodisten).

Die deutschen evangelischen Landeskirchen haben sich in der Evangelischen Kirche in Deutschland (EKD) organisiert. Die evangelischen Freikirchen sind in der Vereinigung Evangelischer Freikirchen zusammengefasst. Durch Auswanderung und Mission entstanden in vielen Ländern der Erde größere oder kleinere evangelische Kirchen (Weltprotestantismus). Besonders stark wachsen sie in China und Lateinamerika. Weltweit gibt es rund 800 Millionen Evangelische. Die meisten lutherischen Kirchen sind im Lutherischen Weltbund zusammengeschlossen, die reformierten Kirchen in der Weltgemeinschaft Reformierter Kirchen. Auch die evangelischen Freikirchen kennen entsprechende internationale Zusammenschlüsse wie beispielsweise den Weltrat methodistischer Kirchen, den Baptistischen Weltbund und die Mennonitische Weltkonferenz. Die große Mehrzahl der evangelischen Kirchen sind Mitglieder des Ökumenischen Rates der Kirchen. Evangelische bilden die Bevölkerungsmehrheit in Skandinavien, dem Vereinigten Königreich, den Vereinigten Staaten, Australien, Island und Neuseeland. In Deutschland, den Niederlanden und der Schweiz leben etwa gleich viele evangelische wie katholische Christen.

Der Begriff "Protestanten" geht zurück auf die Speyerer Protestation der evangelischen Stände auf dem Reichstag zu Speyer 1529: Sie protestierten gegen die Aufhebung des Abschieds von Speyer 1526, mit dem den Ländern und Reichsstädten, die Reformationen durchgeführt hatten, Rechtssicherheit zugesagt worden war, und beriefen sich dabei auf die Glaubensfreiheit des Einzelnen.

Weltliche Herrscher, angeführt von Kaiser Karl V., fürchteten um die Reichseinheit ihres katholisch durchdrungenen Machtbereichs, wobei der päpstliche Machtbereich als eigener gelten konnte. In einer Anzahl von Kriegen war der Protestantismus der mehr oder weniger schwerwiegende Gegenstand; dazu gehören die Hugenottenkriege in Frankreich und der Dreißigjährige Krieg, der fast ganz Europa und insbesondere Deutschland erfasste. Erst mit dem Augsburger Religionsfrieden 1555 wurde die lutherische Konfession und mit dem Westfälischen Frieden von 1648 auch die reformierte Konfession anerkannt. Den Täufern und anderen Bewegungen der sogenannten radikalen Reformation blieb in Deutschland und anderen europäischen Ländern bis ins 18. Jahrhundert jede Form von Anerkennung versagt.

Von Ernst Troeltsch wurde die Unterscheidung von „Altprotestantismus“ und „Neuprotestantismus“ zum Thema der kirchlichen Diskussion gemacht. Altprotestantismus bezieht sich danach auf die protestantische Bewegung unmittelbar zur Reformationszeit bis ins 17. Jahrhundert, die in dieser Zeit noch wesentlichen mittelalterlichen und katholischen Grundsätzen hinsichtlich der Rolle der Kirche in Staat und Gesellschaft folgte. Hierzu zählt auch die lutherische Orthodoxie.

Erst mit dem Druck der Aufklärung im 18. Jahrhundert setzten sich in Deutschland und den anderen protestantisch geprägten oder beeinflussten Ländern die evangelischen Grundideen in der Lebenspraxis verstärkt um. Besonderen Rang bekamen nun die Prinzipien der Freiwilligkeit und der persönlichen Überzeugung. Diese waren im angloamerikanischen Raum schon im 17. Jahrhundert betont worden, vor allem von den Dissenters. Diese hatten auf die völlige Trennung von Kirche und Staat gedrängt. Die Existenz eines säkularen Staates wurde nun auch in Kontinentaleuropa, besonders den protestantisch geprägten oder stark beeinflussten Ländern, mehr und mehr als selbstverständlich betrachtet, so dass die Identität von göttlichem Recht "(Lex Dei)" und dem zumeist noch als Naturrecht verstandenen weltlichen Recht "(ius naturae)" in den Hintergrund trat. So ermöglichte die kritische Weltsicht der Aufklärung auch den Beginn der Bibelkritik. Allerdings hatte bereits Luther entscheidende Weichenstellungen in der (theologischen) Hermeneutik vorgenommen. Er verwarf die mittelalterliche Lehre vom vierfachen Schriftsinn und ließ nur den Literalsinn gelten. Zudem benutzte er den hermeneutischen Zirkel. Aus dem Gesamtzeugnis der Bibel gewann er deren zentrale Aussage: „was Christum treibet“ („Ein Text, der Christus bezeugt“). Diesen Grundsatz wandte er dann auf das Verstehen der einzelnen biblischen Bücher an; so äußerte er sich zum Beispiel kritisch über den Jakobusbrief. Diesen Ansatz führten der arminianische Reformierte Hugo Grotius durch seine grammatische Bibelauslegung und andere protestantische Theologen wie Johann Salomo Semler, Friedrich Schleiermacher, Ferdinand Christian Baur, Albrecht Ritschl, Adolf von Harnack, Ernst Troeltsch und Paul Tillich weiter. In der Gegenwart wandelte sich die theologische Dogmatik von einer Begründung des Protestantismus zu einem innerkirchlichen Diskurs. Die reformatorischen Kirchen und ihre Theologen haben eine große theologische Bandbreite, die von strikt konservativen Positionen (z. B. Lutheran Church – Missouri Synod) bis zu sehr liberalen Anschauungen reicht. Im 20. Jahrhundert wurde der liberale Pol besonders von Friedrich Gogarten, Rudolf Bultmann, Gerhard Ebeling, Ernst Fuchs, Ernst Käsemann und Günther Bornkamm eingenommen; Eduard Thurneysen, Dietrich Bonhoeffer, Helmut Thielicke, Karl Barth und dessen Schüler (Otto Weber u. a.) waren gemäßigt konservativ; der konservative Flügel war von evangelikal-pietistischen Theologen besetzt. Die amerikanischen Kirchen hatten neben Tillich in Richard Niebuhr und Reinhold Niebuhr herausragende Gelehrte. Andere Theologen engagierten sich in der ökumenischen Bewegung, zum Beispiel Nathan Söderblom, Willem Adolf Visser ’t Hooft. Im angehenden 21. Jahrhundert hat sich an dieser theologischen Lage nichts Wesentliches verändert.

Die „Protestation“ war im 16. Jahrhundert ein herkömmliches Rechtsinstrument des Reichsrechts, mit der eine Minderheit von Ständen und Reichsstädten ihre Anliegen auf einem Reichstag vorbringen konnte. Das lateinische Verb "protestari" hat eine positive Grundbedeutung: „für etwas zum Zeugen aufgerufen werden“, „für etwas Zeugnis ablegen“. Heute werden die Begriffe „protestantisch“ (geprägt aus der Fremdwahrnehmung durch die römische Kurie) und „evangelisch“ (geprägt aus der Selbstwahrnehmung der Gemeindemitglieder und ihrer Landeskirchen) in der deutschen Umgangssprache austauschbar verwendet. Jedoch bezeichnen sich die deutschen in der Tradition der Reformation stehenden Kirchen selbst als „evangelisch“ und nicht als „protestantisch“. Eine Ausnahme bildet die Evangelische Kirche der Pfalz (Protestantische Landeskirche). Auch die Begriffe „Kirche(n) der Reformation“ und „reformatorische Kirche(n)“ werden verwendet.

Daneben existieren die Bezeichnungen „lutherisch“ bzw. „A.B. (Augsburgischen Bekenntnisses)“, die die Kirchen in der Tradition der Wittenberger Reformation bezeichnen, und „reformiert“ bzw. „H.B. (Helvetischen Bekenntnisses)“, die die Kirchen in der Tradition der Schweizer Reformatoren Ulrich Zwingli und Johannes Calvin bezeichnen. Je nach Organisationsform gibt es Zusammenschlüsse, die eine Differenzierung aufgeben, wie die unierten Kirchen der Bremischen Evangelischen Kirche oder die United Church of Christ in den Vereinigten Staaten. Im Bereich der evangelischen Freikirchen finden sich Bezeichnungen wie evangelisch-freikirchlich, evangelisch-methodistisch, Altevangelisch Taufgesinnte oder Evangelische Freie Gemeinde.

„Evangelisch“ muss von dem im 20. Jahrhundert entstandenen Begriff „evangelikal“ unterschieden werden, besonders bei Übersetzungen in andere bzw. aus anderen Sprachen. Im englischsprachigen Raum sind die Begriffe „Protestantism“ und „Protestant“ unentbehrlich, da „evangelical“ außer „evangelisch“ vor allem in Nordamerika auch „evangelikal“ bedeutet. Auch gibt es keine englische Entsprechung für das Adjektiv „reformatorisch“. Das Adjektiv „reformed“ bedeutet „reformiert“ und wird zum Beispiel in Bezeichnungen für bestimmte Kirchen verwendet, etwa Dutch Reformed Church (Niederländisch-Reformierte Kirche). Der historische Begriff „evangelical“ entstand im Umfeld des Puritanismus in England.

Prägend sind die Konzentration auf die Bibel, die Anerkennung Jesu Christi als alleiniger Autorität für die Kirche und den einzelnen Glaubenden sowie die Lehre, dass der Mensch „allein aus Gnade“ – und nicht aufgrund seines Handelns – errettet wird. Rechtfertigung erfährt der Mensch „allein durch den Glauben“. Entsprechend wird der Ablass (Nachlass zeitlicher Sündenstrafen gegen Geld, Bußübungen oder andere gute Werke) abgelehnt.

Die evangelische Lehre wird oft in den „vier Soli“ – solus Christus (allein Christus), sola scriptura (allein durch die Schrift), sola gratia (allein durch Gnade) und sola fide (allein durch den Glauben) zusammengefasst.

Einige aus der katholischen Kirche bekannte Sakramente (z. B. Firmung, Ehe, Priesterweihe und Krankensalbung) werden von den evangelischen Kirchen nicht anerkannt, da sie als nicht von Christus eingesetzt betrachtet werden. Martin Luther sprach vom allgemeinen „Priestertum aller Gläubigen“. Als eindeutig von Christus eingesetzte Sakramente gelten die Taufe und das Abendmahl. Luther hält an der Beichte fest, sie gilt in der Regel jedoch offiziell nicht mehr als Sakrament, lediglich einige lutherische Kirchen erkennen der Beichte einen sakramentalen Charakter zu. In den evangelisch-reformierten Kirchen und in vielen evangelischen Freikirchen besitzen die Sakramente lediglich Symbolcharakter. Auch wenn Luther zunächst noch ein lateinisches Messbuch entwickelte, wurden spätestens seit seiner Veröffentlichung einer Messe in deutscher Sprache "(Deutsche Messe B)" Gottesdienste und Messen in allen reformatorischen Kirchen in der jeweiligen Landessprache abgehalten, während die katholische Kirche bis heute offiziell an der lateinischen Sprache festhält, in der Praxis jedoch seit dem Zweiten Vatikanum auch zur Volkssprache übergegangen ist. Jedoch hat mit einer jüngeren Entscheidung von Papst Benedikt XVI., die tridentinische Messe wieder voll zugänglich zu machen, die lateinische Sprache im katholischen Gottesdienst erneut an Bedeutung zugenommen.

Die Reformation war zwar im Kern eine religiöse Bewegung, aber sie hatte stärkste Auswirkungen auf alle Lebensbereiche. Die Reformatoren und der (frühe) Protestantismus entwickelten ein umfassendes Modell für Staat und Gesellschaft, dessen weitere Entfaltung sich vor allem in der von Calvins Denken geprägten angloamerikanischen Welt vollzog. Die Grundzüge dieses Modells wurden nach und nach in vielen Teilen der Welt auch von Nichtprotestanten übernommen.

In den Kirchen der Reformation sind die Geistlichen nicht zur Ehelosigkeit verpflichtet. Das evangelische Pfarrhaus hatte einen enormen Einfluss auf das Entstehen einer geistigen Elite. Zum Beispiel verzeichnete um 1955 die Allgemeine Deutsche Biographie von den etwa 1600 großen Deutschen 861 Pfarrerssöhne. Ähnliches gilt für die anderen protestantisch geprägten Länder. Spätestens seit Mitte des 20. Jahrhunderts sind in nahezu allen protestantischen Kirchen Frauen zum Pfarramt zugelassen, auch in Führungspositionen, z. B. die frühere lutherische Bischöfin und Ratsvorsitzende der Evangelischen Kirche in Deutschland, Margot Käßmann, und seit Sommer 2014 leitet Antje Jackelén als Erzbischöfin die lutherische Kirche Schwedens. Das war ein wesentlicher Beitrag zur rechtlichen und faktischen Gleichstellung von Frauen und Männern.

Insbesondere in Schottland sowie in England und seinen Kolonien in Nordamerika schufen Protestanten demokratische Strukturen im weltlichen Bereich (John Milton, Oliver Cromwell, John Locke, William Bradford, John Winthrop, Roger Williams, Thomas Hooker, William Penn, George Washington, Thomas Jefferson, John Adams u. a.). Vor allem die amerikanische Demokratie wurde für viele Staaten, darunter Deutschland, zum Vorbild bei der Schaffung ihrer eigenen demokratischen Gesellschafts- und Staatsform. Die prinzipielle Trennung von Geistlichem und Weltlichem war bereits durch Luthers Zwei-Reiche-Lehre vollzogen worden. Indem Luther allen Getauften das allgemeine Priestertum zusprach, wertete er die Laien in der Kirche außerordentlich stark auf; dies führte er unter anderem in seiner Schrift "Dass eine christliche Versammlung oder Gemeine Recht und Macht habe, alle Lehre zu beurteilen und Lehrer zu berufen, ein- und abzusetzen" von 1523 aus. Dieses demokratische Element wurde im reformierten Bereich durch die Schaffung von Presbyterien (Kirchengemeinderat) und Synoden, deren Mitglieder von den Gemeindegliedern gewählt wurden und in denen Geistliche und Laien gleichberechtigt in der Leitung der Kirche zusammenwirkten, wesentlich verstärkt (kirchliche Selbstregierung).

Das Bildungswesen nahm in den von der Reformation erfassten Gebieten einen starken Aufschwung, da jedes Gemeindeglied in die Lage versetzt werden sollte, die Bibel selbst zu lesen. Philipp Melanchthon erhielt für sein Engagement auf diesem Gebiet den Ehrennamen "Praeceptor Germaniae" (Lehrer Deutschlands). Schulpflicht sowohl für Jungen als auch für Mädchen sorgte für einen hohen Grad der Alphabetisierung. Das reformatorische Gottes- und Menschenbild hat zur Folge, dass der in Christus erwählte und erlöste Mensch in Bewegung gesetzt wird. Er darf und soll alle ihm vom Schöpfer verliehenen Kräfte, einschließlich Verstand und Vernunft, frei entfalten – im geselligen Umgang, in der Wirtschaft, in den Wissenschaften und in der Kunst. Er darf und soll Gottes gute Schöpfung erforschen und im Sinne von nachhaltig nutzen. Dies schuf ein günstiges kulturelles Klima für das Erblühen der Geisteswissenschaften, der Naturwissenschaften und der Technik. Der von dem amerikanischen Soziologen Robert King Merton 1938 entwickelten Merton-These zufolge wurde die naturwissenschaftlich-technologische Revolution des 17. und 18. Jahrhunderts im Wesentlichen von Protestanten, vor allem englischen Puritanern und deutschen Pietisten, getragen. Die Naturrechtsjuristen Hugo Grotius und Samuel Pufendorf sowie die Philosophen der englischen und deutschen Aufklärung John Locke, John Toland, Matthew Tindal, Gottfried Wilhelm Leibniz, Christian Wolff, Christian Thomasius, Immanuel Kant sowie der Genfer Jean-Jacques Rousseau entstammten dem Protestantismus und waren von diesem geprägt. Sie griffen Entscheidungen auf, die in der Reformation und in Teilen des frühen Protestantismus getroffen worden waren, und entwickelten sie weiter, z. B. Religionsfreiheit, Gleichheit der Menschen und Demokratie. Beispielsweise leitete Locke, der tief im protestantischen Denken verwurzelt war, die Gleichheit der Menschen, einschließlich der Gleichheit von Mann und Frau, nicht aus philosophischen Prämissen, sondern aus 1. Mose (Imago Dei) ab. Der Gleichheitsgrundsatz ist unerlässliche Grundlage der rechtsstaatlichen Demokratie. Der bedeutendste Philosoph des Deutschen Idealismus Georg Wilhelm Friedrich Hegel studierte am Evangelischen Stift in Tübingen. Die Ideen der Aufklärung wurden in erster Linie vom protestantischen Bürgertum umgesetzt. Die katholische Kirche lehnte die Aufklärung ab.

Die Reformatoren befürworteten einen Lebensstil, der Fleiß, (Selbst-)Disziplin, Pflichtbewusstsein, Ehrlichkeit, Genügsamkeit, Sparsamkeit und – vor allem bei Calvin – Verzicht auf Luxus einschloss. Max Weber schloss in seiner "Protestantischen Ethik", dass dadurch Geld für Investitionen frei wurde, was das Wirtschaftsleben in den protestantisch geprägten Gebieten nachhaltig beflügelte. Webers populäre Thesen halten – zumindest im deutschen Sprachraum – einer empirischen Überprüfung jedoch nicht stand. Dagegen fand der US-amerikanische Soziologe Gerhard Lenski in einer 1958 durchgeführten empirischen Untersuchung im Großraum Detroit (US-Bundesstaat Michigan) wesentliche Teile von Webers Thesen bestätigt, insbesondere im Hinblick auf die unterschiedlichen Einstellungen gegenüber dem Wirtschaftsleben und den Naturwissenschaften bei Katholiken einerseits und (weißen) Protestanten sowie Juden andererseits. Der Soziologe Eduard Heimann schloss, dass Wirtschaft einerseits und Naturwissenschaften und Technik andererseits sich gegenseitig verstärkten, da in der Wirtschaft die jeweils neuesten und effektivsten Erkenntnisse, Methoden und Maschinen zum Einsatz kamen, um die Produktivität zu steigern. Dies habe zu einem ständig steigenden Lebensstandard geführt, auch für die unteren sozialen Schichten. Diese Entwicklung halte nach wie vor an.

Die prinzipielle Trennung von Geistlichem und Weltlichem durch Luther sollte in protestantischen Gebieten und Ländern ein kirchliches Inquisitionsverfahren unmöglich machen. Den Glauben, so Luther, kann man nicht erzwingen. Er ist ein Werk des Heiligen Geistes. Allerdings sah Luther in der Ablehnung des Eides, des Kriegsdienstes und teilweise des Eigentums durch die Täufer eine politische Gefahr für das Gemeinwesen. Deshalb kam es nicht nur in katholischen, sondern auch in lutherischen und reformierten Gebieten zur Verfolgung von Täufern. Diese forderten unermüdlich religiöse Toleranz und traten durch ihr geduldiges Leiden dafür ein. Sie und die ebenfalls verfolgte Minderheitskirche der reformierten Hugenotten praktizierten schon seit ihren Anfängen im 16. Jahrhundert die völlige Trennung von Kirche und Staat. Anfang des 17. Jahrhunderts gingen aus dem englischen Puritanismus unter Einfluss des niederländischen Mennonitismus die baptistischen Kirchen hervor. Baptisten wie John Smyth, Thomas Helwys und Roger Williams traten in Streitschriften vehement für Religionsfreiheit ein. Sie hatten starken Einfluss auf John Milton und Locke. In „Milton verkörpern sich alle Toleranzmotive der Zeit in großartiger Einheit. Gewissensfreiheit war ihm christliches und protestantisches Urprinzip und Grundlage aller bürgerlichen Freiheiten. Darum forderte er über Cromwell hinaus völlige Trennung von Staat und Kirche.“ Zudem sprach er sich für das Recht auf Ehescheidung und die Pressefreiheit aus. Letztere wurde 1694 in England eingeführt, eine Frucht der Glorreichen Revolution. In einigen englischen Kolonien in Nordamerika wurde die Religionsfreiheit mit demokratischer Selbstverwaltung verknüpft (Roger Williams in Rhode Island (1636); Thomas Hooker in Connecticut (1639); William Penn in Pennsylvania (1682)). Die amerikanische Unabhängigkeitserklärung und die Verfassung der Vereinigten Staaten führten diese Tradition fort. Die Abschaffung der Folter (Christian Thomasius, Friedrich der Große) und der Sklaverei (William Wilberforce, Abraham Lincoln, Harriet Beecher Stowe u. a.) ging hauptsächlich auf die Initiative von Protestanten zurück.

Protestanten fühlten sich von Anfang an verantwortlich für Kranke und sozial Benachteiligte. Es entstanden weltweit Krankenhäuser, Heime und andere Hilfseinrichtungen für behinderte, alte und arme Menschen, in Deutschland beispielsweise Diakonisches Werk und Brot für die Welt. Pioniere waren vor allem Johann Hinrich Wichern und Friedrich von Bodelschwingh sen. Als Reaktion auf die Verelendung großer Teile der Stadt- und Landbevölkerung gründeten in Großbritannien um 1845 Anglikaner und Mitglieder von Freikirchen Genossenschaften als Selbsthilfeorganisationen. In Deutschland schufen der überzeugte Reformierte Friedrich Wilhelm Raiffeisen und der Preuße Hermann Schulze-Delitzsch ab 1862 ein rasch wachsendes Netz von Genossenschaften. Unter dem Druck der „sozialen Frage“ des 19. Jahrhunderts beschloss Preußen während der Kanzlerschaft Otto von Bismarcks von 1881 bis 1889 Sozialversicherungsgesetze. Das humanitäre Völkerrecht erfuhr durch den reformierten Pietisten Henry Dunant eine große Bereicherung. Er war die treibende Kraft hinter der Entstehung der Genfer Konvention, und auf sein Engagement geht das Rote Kreuz zurück.

Der Protestantismus wirkte befruchtend auf die Kunst. Im deutschen Sprachraum konnten jahrhundertelang mehr Menschen die Texte der Kirchenlieder Martin Luthers und Paul Gerhardts auswendig als Gedichte von Johann Wolfgang Goethe und Friedrich Schiller, die ihrerseits ebenfalls einen protestantischen Hintergrund hatten. Komponisten wie Heinrich Schütz, Johann Hermann Schein, Samuel Scheidt, Georg Philipp Telemann, Georg Friedrich Händel, Johann Sebastian Bach, Johannes Brahms und Felix Mendelssohn Bartholdy schufen Höhepunkte der weltlichen und geistlichen Musik. Die Maler Albrecht Dürer, Lucas Cranach d. Ä. und Lucas Cranach d. J. schlossen sich der Reformation an. Rembrandt, Frans Hals, Vincent van Gogh u. a. entstammten dem niederländischen Protestantismus. Protestantisches Denken und Glauben inspirierte bis in die Gegenwart große Schriftsteller wie William Shakespeare, John Milton, John Bunyan, Friedrich Gottlieb Klopstock, Johann Gottfried Herder, Nathaniel Hawthorne, Jeremias Gotthelf, Conrad Ferdinand Meyer, Samuel Taylor Coleridge, William Wordsworth, Hans-Christian Andersen, Jane Austen, Emily Brontë, Charles Dickens, Wilhelm Raabe, Theodor Fontane, Selma Lagerlöf, Agatha Christie, William Faulkner, Thomas Mann, Friedrich Dürrenmatt, John Updike, Sibylle Lewitscharoff u. v. a. m.




</doc>
<doc id="9579" url="https://de.wikipedia.org/wiki?curid=9579" title="Bahamas">
Bahamas

Die Bahamas () sind ein Inselstaat im Atlantik und Teil der Westindischen Inseln. Sie liegen südöstlich der Vereinigten Staaten sowie nordöstlich von Kuba und werden geographisch zu Mittelamerika gezählt. Von den mehr als 700 Bahamainseln sind nur 30 bewohnt.

Die Inselgruppe erhielt ihren Namen durch die spanischen Konquistadoren. Sie nannten die Gewässer um die Inseln "Baja Mar" ( für ‚flaches Meer‘), woraus später Bahamas wurde. Seit 1973 sind die Bahamas vom Vereinigten Königreich unabhängig, aber die ehemalige Kronkolonie blieb eine konstitutionelle Monarchie mit Königin Elisabeth II. als Staatsoberhaupt.

Die Bahamas erstrecken sich von der Südostküste der USA entlang der Nordostküste Kubas bis zur Nordwestküste der Turks- und Caicosinseln mit einer Nord-Süd-Ausdehnung von etwa 650 km und einer West-Ost-Breite von bis zu 750 km. Obwohl die Bahamas im Atlantik liegen, werden sie der Karibik zugezählt.

Die Bahamas setzen sich aus den Inseln und mehr als 2400 Korallenriffen, den sogenannten Cays, zusammen. In vielen Fällen trennt nur ein wenige Zentimeter tiefer Meeresarm zwei Inseln beziehungsweise Cays voneinander. Daher schwanken die Zahlen abhängig davon, ob in diesem Fall zwei Inseln oder eine Insel mit einer leicht unter Wasser gesetzten Landbrücke gezählt werden.

Die Inseln werden unterteilt in die beiden am stärksten bewohnten Inseln New Providence sowie Grand Bahama und in die sogenannten Out Islands beziehungsweise Family Islands. Die flächengrößte Insel ist Andros mit 5957 km². Der höchste Punkt der Bahamas ist der Mount Alvernia mit 63 m auf Cat Island.

Die beiden mit Abstand wichtigsten Städte der Bahamas sind die Hauptstadt Nassau und Freeport. In beiden zusammen leben mehr als drei Viertel der Bevölkerung der Bahamas. Nassau ist mit seinen mehr als 200.000 Einwohnern die mit Abstand größte Stadt des Inselstaates.

Die Bahamas weisen ein für einige Regionen der Subtropen typisches Ostseitenklima auf, da sie durch die Luftsysteme des Nordamerikanischen Kontinents abgekühlt werden. Im Sommer liegt die Durchschnittstemperatur bei ca. 28 °C, im Winter lässt der warme Golfstrom (Wassertemperaturen zwischen 24 und 29 °C) die Temperatur selten unter 20 °C sinken. Im Jahresmittel liegt die Temperatur bei 26 °C.

Die Bevölkerungszahl der Bahamas lag nach der letzten Volkszählung im Jahr 2010 bei 353.658 Menschen. Die Bevölkerung ist relativ jung, so sind rund 26 % unter 15 Jahren alt und nur 6 % der Bahamaer 65 Jahre oder älter. Im Jahr 2010 lag das natürliche Bevölkerungswachstum bei 0,77 %. Prognosen gehen davon aus, dass sich dieser Wert in etwa 30 Jahren auf 0 % abgesenkt haben wird. Die durchschnittliche Lebenserwartung liegt bei 72 Jahren für Männer und bei 79 Jahren für Frauen.

Menschen afrikanischer Herkunft bilden mit 85 % den größten Anteil der Inselbevölkerung. Die restlichen 15 % teilen sich in 12 % europäischer Herkunft, sowie 3 % asiatischer und lateinamerikanischer Herkunft auf.

Neben der Amtssprache Englisch wird noch haitianisches Kreol gesprochen, allerdings hauptsächlich von den zahlreichen Einwanderern aus Haiti. Nach dem Erdbeben in Haiti 2010 stieg die Immigration aus Haiti sprunghaft an.

AIDS hat sich auf den Bahamas zu einem bedeutenden Thema entwickelt. So lebten 2009 etwa 3,1 % der 15–49-jährigen Bevölkerung mit dem HI-Virus. Damit liegen sie auf Platz 23 der weltweit prozentual am stärksten betroffenen Länder.

Die Bahamas sind christlich geprägt: Die wichtigsten Glaubensrichtungen sind die Protestanten mit 67,6 % (Baptisten 35,4 %, Anglikaner 15,1 %, Pfingstbewegung (Pentecostalism) 8,1 %, Gemeinde Gottes (Church of God) 4,8 %, Methodisten 4,2 %) und die römisch-katholische Kirche mit 13,5 % und andere Christen mit 15,2 % Anteil an der Bevölkerung.

→ "Hauptartikel: Geschichte der Bahamas"

Die frühesten Spuren einer Besiedlung gehen in das 4. Jahrhundert zurück, dauerhaft besiedelt wurden einige der Inseln aber erst im 9. und 10. Jahrhundert durch die Lucayan, ein zum Volk der Arawak zählender Stamm.

Nachdem Christoph Kolumbus am 12. Oktober 1492 die Bahamainseln entdeckte, wurden die etwa 40.000 Inselbewohner bis 1520 nach Hispaniola verschleppt und versklavt, wo sie in den Minen durch Krankheit und Auszehrung umkamen.

Mitte des 17. Jahrhunderts errichteten englische Siedler die ersten Kolonien, nachdem König Karl I. von England die Inseln 1629 beansprucht hatte. Freibeuter, wie der berühmte Blackbeard, nutzten die Inseln Ende des 17. und Anfang des 18. Jahrhunderts als Unterschlupf, da sie für die Kolonialmächte, auf Grund ihres Rohstoffmangels und der ungeeigneten Böden, nur eine geringe Bedeutung hatten.

Im Jahr 1717 wurden die Bahamas zur britischen Kronkolonie und Woodes Rogers zum ersten Krongouverneur der Inseln ernannt. Er löste das Piratenproblem, das sich auf der Inselgruppe entwickelt hatte, und gab den Bahamas 1729 ein eigenes Parlament, das House of Assembly. Im Zuge des Amerikanischen Unabhängigkeitskriegs flohen eine große Zahl britischer Loyalisten auf die Bahamas, speziell nach Abaco. Während des Amerikanischen Unabhängigkeitskriegs (1775–1783), des Amerikanischen Bürgerkriegs (1861–1865) und der Prohibition in den USA (1919–1932) waren die Inseln auf Grund ihrer günstigen Nähe zu den USA Ausgangspunkt von ausgeprägtem Handel mit Schmuggelware. Während des Zweiten Weltkriegs war der Gouverneur der Bahamas der Duke of Windsor. Großbritannien gewährte 1964 den Bahamas die innere Selbstverwaltung, was schließlich dazu führte, dass sie 1973 in die Unabhängigkeit entlassen wurden.

Seit Mitte des 20. Jahrhunderts gilt der zum Commonwealth of Nations zählende Inselstaat als Touristenziel und Steueroase. Auf den Bahamas leben heute etwa 300.000 Menschen, wovon mehr als 70 % in der Hauptstadt Nassau leben. Da das Land über keine nennenswerten Rohstoffe verfügt, ist die Wirtschaft stark auf den Tourismus und das Bankengewerbe angewiesen. Auch wenn die meisten Touristeninformationen die Bahamas als ein Paradies erscheinen lassen, so sind für die Regierung unter Premierminister Perry Christie die hohe Arbeitslosigkeit, der Drogenhandel und die Korruption in politischen Kreisen ernsthafte, noch ungelöste Probleme.

Am 13. November 1965 ging 60 Meilen vor Nassau der amerikanische Passagierdampfer "Yarmouth Castle" in Flammen auf, brannte aus und sank, wobei 90 Passagiere und Besatzungsmitglieder ums Leben kamen. Das Unglück führt zu neuen Regelungen zur Sicherheit auf See im Rahmen der International Convention for the Safety of Life at Sea.

Die Bahamas sind seit 1973 ein souveräner Staat. Das Staatsoberhaupt ist der König oder die Königin des Vereinigten Königreichs Großbritannien und Nordirland, seit 1952 Königin Elisabeth II. Sie wird auf den Bahamas durch den von ihr persönlich ernannten Generalgouverneur vertreten. Dies ist seit dem 8. Juli 2014 Marguerite Pindling. 

Regierungschef ist der Premierminister, der von der Regierungspartei gestellt wird. Seit 8. Mai 2012 ist dies Perry Christie von der Progressive Liberal Party als Nachfolger des seit 2007 regierenden Hubert Ingraham vom Free National Movement. Am 7. Mai 2012 gewann die PLP die Wahlen zum House of Assembly und erzielte 29 Sitze, während das bisher regierende FNM nur noch 9 Sitze erreichte. Die Bahamas wenden das Westminster-System an. Die Regierungspartei wird alle fünf Jahre durch die Parlamentswahlen bestimmt. Das Parlament besteht nach dem britischen Vorbild aus zwei Kammern, dem "Senat" mit 16 Mitgliedern und dem House of Assembly mit 39 Mitgliedern. Die 16 Mitglieder des Senats werden vom Generalgouverneur ernannt, neun in Absprache mit dem Premierminister, vier in Absprache mit dem Führer der Opposition und drei durch den Generalgouverneur selbst. Die Mitglieder des House of Assembly werden alle fünf Jahre nach dem Mehrheitswahlrecht vom Volk selbst gewählt. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „frei“ bewertet.

Die Bahamas sind Mitglied in der Alliance of Small Island States (AOSIS).

"Siehe auch: Liste der Premierminister der Bahamas"

"Siehe auch: Liste der Gouverneure der Bahamas"

Einzige Hochschule ist das "College of The Bahamas" mit etwa 6000 Studierenden.

Es gibt drei Tageszeitungen, "The Nassau Guardian", "The Tribune" und "The Freeport News". Die einzige Fernsehstation ist "ZNS TV".

Die Landwirtschaft auf den Bahamas ist unterentwickelt, da es keine geeigneten Bewässerungsmöglichkeiten gibt. Der Transport zwischen den zahlreichen und weit auseinanderliegenden Inseln ist schwierig und es sind auch keine geeigneten Böden vorhanden. Hauptsächlich wird für den Eigenbedarf produziert, der aber nicht abgedeckt wird. In wenigen moderneren Farmen werden vor allem Gurken, Tomaten, Zwiebeln, Zitrusfrüchte und Ananas für den Export angebaut.

Kiefern, Hart- und Farbhölzer auf Andros, Great Abaco und Grand Bahama kommen als Bauholz in den Handel oder werden für den Schiffbau und die US-amerikanische Papierindustrie geschlagen.

Für die Fischerei bestehen durchweg günstige Bedingungen; sie erfolgt für den Eigenbedarf.

Die Industrie besteht hauptsächlich aus Kleinbetrieben, aber auch Schiffbau, Gewinnung von Meersalz sowie die Erzeugung von Konsumgütern wird ausgeweitet. 1954 wurde auf Grand Bahama eine zoll- und steuerfreie Zone eingerichtet, die zur Ansiedlung ausländischer Unternehmen führen sollte. Seit 1968 ist die "Bahama Monetary Authority" für die Ausgabe von Banknoten verantwortlich. Durch die günstige Steuergesetzgebung erreichten die Bahamas den Status eines internationalen Finanzzentrums mit Sitz vieler Banken, Investment- und Treuhandgesellschaften.

Am 28. Januar 2016, legte die EU-Kommission ein Maßnahmenpaket "zur Bekämpfung von Steuerflucht" vor, bei dem unter anderem die Bahamas auf der "schwarzen Liste" der Steueroasen auftauchen.

Die Bahamas sind im Vergleich zur restlichen Karibik-Region relativ wohlhabend. Das BIP pro Kopf lag 2016 mit ca. 24.000 US-Dollar ungefähr auf dem Niveau von Malta.

Im Index der Wirtschaftlichen Freiheit belegt das Land 2017 Platz 90 von 180 Ländern.

Der wichtigste Flughafen ist der bei Nassau gelegene Flughafen Nassau Lynden Pindling. Staatsfluggesellschaft ist die Bahamasair mit weniger als zehn Flugzeugen. Die Infrastruktur für den Luft- und Seeverkehr ist gut ausgebaut, allerdings gibt es wegen der topographischen Bedingungen keine Eisenbahn. Brauchbare Landverkehrswege befinden sich auf New Providence, Grand Bahama und einigen anderen Inseln.
Auf den Bahamas gilt Linksverkehr.

Haupterwerbszweig der Bahamas ist der Tourismus, in dem etwa 60 % der erwerbstätigen Bevölkerung beschäftigt sind. Wichtig sind insbesondere Kreuzfahrten US-amerikanischer Passagiere ab Miami.

Auf den Tourismus hat sich auch der Dreh mehrerer James-Bond-Filme auf den Bahamas günstig ausgewirkt. Die Bahamas beziehungsweise Nassau dienten unter anderem für , Feuerball und dessen 1983 erschienene Neuverfilmung Sag niemals nie als Drehorte.

Lockere Gesetze haben dazu geführt, dass es auf den Bahamas bis zu 400 Finanzunternehmen gab. Ein Großteil dieser Banken, so wird vermutet, beschäftigt sich hauptsächlich mit Geldwäsche. 2001 wurden auf Druck der USA die Gesetze verschärft, seitdem wurden fast 15 % der Geldinstitute geschlossen.

Die "Commonwealth Bank of the Bahamas" ist eine Bank, die seit 1984 voll im Besitz der Bahamas ist.

Auf den Bahamas existieren insgesamt 22 Nationalparks, die vom Bahamas National Trust verwaltet werden. Damit sind über 260.000 Hektar Land- und Meeresfläche geschützt. Der Central Andros Nationalpark ist mit 117.000 Hektar der flächenmäßig größte Nationalpark. Er umfasst ebenso Mangroven- und Feuchtgebiete wie auch Teile des Andros Barrier Reef, das drittgrößte Riff der Welt. Das nördlichste Schutzgebiet der Bahamas ist der Walker’s Cay Marine Park mit seinen Korallen und einer äußerst vielseitigen Meeresfauna. Der südlichste Nationalpark ist die Insel Little Inagua. Die Insel ist unbewohnt und allein der Natur und ihren Besuchern vorbehalten. Am Strand von Little Inagua legen die gefährdeten Wasserschildkröten ihre Eier. Über 60.000 westindische Flamingos leben auf der Insel Great Inagua. Sie wurden vor dem Aussterben gerettet.

Auf Bimini hingegen planen amerikanische Investoren eine riesige Touristenanlage mit über 1000 Appartements, 450 Villen, einem Luxushotel, Restaurants, Casino, Wellness Center und einem Golfplatz sowie einem Yachthafen mit rund 400 Anlegeplätzen. Auf Druck der Bevölkerung konnten diese für die Natur verheerenden Pläne gestutzt werden. Ganz verhindern konnte man das Projekt bisher aber nicht. Mit der Unterstützung von Reiseveranstaltern und Tauchgästen aus aller Welt wehren sich die Bewohner von Bimini gegen das Mega-Projekt auf ihrer winzigen Insel, das zur Vernichtung ihrer Naturressourcen führen würde.

Der Staatshaushalt umfasste im Fiskaljahr 2016 Ausgaben von umgerechnet 2,3 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 1,9 Mrd. US-Dollar gegenüber. Dies entspricht einem Haushaltsdefizit von 4,4 % des BIP.
Die Staatsverschuldung betrug 2016 64,4 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Die 1980 gegründeten Streitkräfte der Bahamas werden als "" (RBDF) bezeichnet. Die Bahamas verfügen über keine Armee oder Luftwaffe, die vorhandenen Einheiten sind der Marine zugeordnet und haben Polizeicharakter.

→ "Hauptartikel: Distrikte der Bahamas"

Seit 1999 sind die Bahamas durch den Local Government Act von 1996 in 31 Kommunalverwaltungsdistrikte und New Providence, das von der nationalen Regierung verwaltet wird, aufgeteilt.




</doc>
<doc id="9580" url="https://de.wikipedia.org/wiki?curid=9580" title="Objekt">
Objekt

Objekt steht für:


</doc>
<doc id="9581" url="https://de.wikipedia.org/wiki?curid=9581" title="Ereignis">
Ereignis

Ein Ereignis (aus ahd. "", nhd. "eräugen" = vor Augen stellen, zeigen) ist das Auftreten eines beobachtbaren Geschehens; beobachtbar, weil es sich um ein Geschehen handelt, das im ursprünglichen Sinne des Wortes "vor Augen tritt", "eräugt" werden kann.

Ein astronomisches Ereignis ist ein am Himmel zu einem bestimmten Zeitpunkt stattfindendes Phänomen. Manche astronomischen Ereignisse treten mit hoher Regelmäßigkeit ein, wie die Mondphasen. Andere astronomische Ereignisse unterliegen keinen direkten Periodizitäten, wie Okkultationen durch Planeten. Nicht alle astronomischen Ereignisse können vorausgesagt werden.

Die Entdeckung einer chemischen Reaktion im Rahmen von labortechnischen Versuchsserien, bei denen chemische Bindungen ein bisher nicht existentes, neues Produkt hervorrufen.

Unter einem Ereignis (englisch "") versteht man in der ereignisorientierten Programmierung eine Begebenheit, die – über ein Event-Handler-Programm – eine "Aktion" und (in deren Folge) ggf. eine Zustandsveränderung auslöst. Diese Ereignisse können Benutzereingaben (Mausklick, Taste, Spracheingabe, Geräteanschluss, …) oder Systemereignisse (Zeitpunkt, Fehler, Datenveränderung, Sensor, …) sein.

In der Wahrscheinlichkeitstheorie wird ein Zufallsereignis auch "Ereignis" genannt und in Bezug auf die möglichen Ausgänge oder Ergebnisse eines Zufallsexperiments definiert. Als Ereignis wird eine Zusammenfassung von Ergebnissen bezeichnet, die eine Teilmenge der Ergebnismenge ist und der eine bestimmte Wahrscheinlichkeit zwischen 0 und 1 zugeordnet werden kann.

In der Medizin wird der Begriff Ereignis vorwiegend im Zusammenhang mit einem unerwünschten oder nachteiligen Geschehen im Rahmen einer Behandlung verwendet: man spricht dann häufig von einem "unerwünschten Ereignis" (UE). Vielfach findet man auch in der deutschsprachigen Literatur die englische Entsprechung »adverse event« oder »AE«. Unerwünschte Ereignisse werden eingeteilt nach den Common Toxicity Criteria (CTC).

Der Begriff des unerwünschten Ereignisses wird in zwei Bereichen der Medizin, der Qualitätssicherung und der Arzneimittelforschung, unterschiedlich definiert:



In der modernen Philosophie wird zumeist in zwei verschiedenen Kontexten von „Ereignis“ gesprochen:

Von einigen Theoretikern wird dabei vertreten, dass im Grunde die gesamte Ontologie nicht auf Gegenständen, sondern Ereignissen fußen sollte, beispielsweise, indem argumentiert wird, dass eine wechselseitige Reduzierbarkeit besteht, man aber Ereignisse ohnehin für eine funktionierende Ontologie benötigt und also ohne Gegenstände auskommt oder, indem argumentiert wird, dass damit ontologische Probleme des qualitativen Wandels bei Objektpersistenz besser zu behandeln sind. Ein klassischer Vertreter einer solchen Ereignisontologie ist beispielsweise Alfred North Whitehead, ein jüngerer Klassiker Donald Davidson.

Die philosophische Konzeption von Ereignissen als eigener Sorte von Entitäten in der Welt, insbesondere nach Donald Davidson hat einen starken Einfluss auf linguistische Darstellungen der Semantik von Verben genommen, und führte zum Entstehen der sog. Ereignissemantik.

Das Ereignis in der Psychologie ist die Erfahrung; das ist das im Gedächtnis registrierte und fortan verfügbare Geschehen einer Situation, in der ein Individuum lebt. Die Speicherung des Ereignisses ist subjektiv und damit im Gedächtnis die Grundlage für Lernprozesse, zum andern für die menschliche Entwicklung grundsätzlich. Diese Entwicklung (siehe Entwicklungspsychologie) ohne Erfahrung(en) (bzw. Ereignisse) ist nicht denkbar bzw. möglich. Ein menschlicher Organismus ist davon abhängig, Erfahrungen zu machen, insbesondere in der frühen Kindheit; andernfalls erleidet er (Existenz gefährdende) Schädigungen (siehe dazu René A. Spitz).

In der Relativitätstheorie wird ein durch Ort und Zeit festgelegter Punkt der Raumzeit als Ereignis bezeichnet. Die gesamte Beschreibung der Realität fußt auf diesen Ereignissen – was für einige Interpreten eine Ereignisontologie begünstigt.

In der soziologischen Systemtheorie bezeichnet Ereignis die zeitpunktbezogene, nicht bestandsfähige Einheit der Differenz von Vorher/Nachher in autopoietischen Systemen. Nach dem Ereignis ist etwas anderes möglich als vorher. Genau dieser Unterschied verleiht den Systemelementen trotz fehlender Dauerhaftigkeit ihre operative Anschlussfähigkeit im Zeitablauf. Beispielsweise bestehen Worte nur zum Zeitpunkt des Sprechens und sind danach sofort wieder vergangen. Haben nacheinander gesprochene Worte eine Anschlussfähigkeit, dann ergeben sie einen zusammengehörenden Satz. Der Satz kann nur dann entstehen, wenn die einzelnen Ereignisse (hier: Worte) keine dauerhafte Existenz haben.

Ereignis ist versicherungsrechtlich der Auslöser, der die Leistungspflicht des Versicherers begründet. Bestandteil des Begriffs Schaden ist das ihn auslösende Ereignis; ein schädigendes Ereignis löst den Versicherungsfall aus. Beim Ereignis handelt es sich um zufällig eintretende, also nicht vorhersehbare Vorkommnisse, deren negative Folgen zu einem Schaden führen. Ereignis ist ein Geschehensablauf, als dessen Folge die Schädigung unmittelbar entstanden ist. Ereignis ist nach § 1 Allgemeine Versicherungsbedingungen für die Haftpflichtversicherung (AHB) ein plötzlicher und heftiger Geschehensablauf, der in seiner Gesamtheit nicht objektiv voraussehbar ist. Als "Schadensereignis" im Sinne des § 5 Nr. 1 AHB ist das vor dem Schadenseintritt liegende äußere oder innere Geschehen zu verstehen, von dessen Beginn an der Schadenseintritt in hohem Maße wahrscheinlich ist. Nach Abs. 2 StVG haftet der Fahrzeughalter nicht bei einem "unabwendbaren Ereignis" (höhere Gewalt). Dies ist ein Vorfall, der selbst dann nicht zu vermeiden ist, wenn die größtmögliche Sorgfalt angewendet wird. Damit geht der verkehrsrechtliche Sorgfaltsbegriff weit über den des BGB hinaus. Unabwendbar ist ein Ereignis, das „weder auf einem Fehler in der Beschaffenheit des Fahrzeugs noch auf einem Versagen seiner Verrichtungen beruht“ (§ 7 Abs. 2 Satz 1 StVG). Unabwendbar kann auch auf das Verhalten des Verletzten oder eines Tieres zurückzuführen sein. Unfall ist ein von außen auf den Menschen einwirkendes, schädigendes, plötzliches Ereignis. Das Ereignis ist versicherungsrechtlich immer ein negativer Vorgang, positive Vorkommnisse wie das „freudige Ereignis“ (als Umschreibung einer bevorstehenden Geburt) werden hiervon nicht erfasst.

Das Ereignis wird seit dem Jahrtausendwechsel in der englischen Version „Event“ (Veranstaltung) immer häufiger verwendet. Als Beispiele gelten Eventmanager, Eventtechniker, Eventsafety, Eventversicherung usw.



Psychologie





</doc>
<doc id="9582" url="https://de.wikipedia.org/wiki?curid=9582" title="Francis Bacon (Maler)">
Francis Bacon (Maler)

Francis Bacon (* 28. Oktober 1909 in Dublin; † 28. April 1992 in Madrid) war ein in Irland geborener britischer Maler. Francis Bacon gehört zu den bedeutendsten gegenständlichen Malern des 20. Jahrhunderts. In seinen Werken setzte er sich vornehmlich mit der Darstellung des deformierten menschlichen Körpers in eng konstruierten Räumen auseinander. Besonders bekannt ist Bacon für seine Papstbilder, Kreuzigungsdarstellungen und Porträts seiner engsten Freunde. Seine ungeschönten Figuren und gewagten Darstellungen sind oftmals grotesk und emotionsgeladen. Bacon sprach oft davon, dass er „in Serien“ dachte, wobei er sich normalerweise über einen längeren Zeitraum hinweg auf ein einziges Subjekt konzentrierte und dieses häufig in Form eines Triptychons oder Diptychons ausführte. Seine Arbeiten können gemeinhin als eine Sequenz oder Variation eines einzelnen Motivs beschrieben werden.

Als Autodidakt begann er Ende der 1920er Jahre mit der Malerei, nachdem er zuvor als Innenausstatter gearbeitet hatte. Berüchtigt für sein Leben als "bon vivant" und seine Spielsucht, gelang ihm 1944 jedoch der Durchbruch als Künstler mit dem Triptychon "Drei Studien zu Figuren am Fuße einer Kreuzigung" (Tate Britain, London)"." Seit der Mitte der 1960er Jahre beschäftigte er sich insbesondere mit Porträts von seinen Freunden und Trinkgefährten. Nachdem sich 1971 sein Liebhaber George Dyer das Leben nahm, wurde seine Kunst zunehmend düster, nach innen gerichtet und setzte sich mit dem Lauf der Zeit und dem Tod auseinander. Der Höhepunkt seines Spätwerks wird von einer Vielzahl von Meisterwerken bezeichnet, wie u. a. "Studie für ein Selbstporträt – Triptychon, 1985–86" (Marlborough Fine Art, London).

Ungeachtet von Bacons existentialistischer Einstellung war Bacon höchst charismatisch, einnehmend, belesen und wortgewandt, wie aus den Interviews und Gesprächen mit David Sylvester hervorgeht. Kunstkritiker Robert Hughes hat ihn als den unfehlbarsten und lyrischen englischen Künstler des 20. Jahrhunderts beschrieben und ihn nebst dem Künstler Willem de Kooning als einen der einflussreichsten Maler der beunruhigenden menschlichen Figur in den 1950ern bezeichnet.

Francis Bacons Leben stand unter dem Einfluss von Alkohol und Glücksspiel. Sein Biograph Daniel Farson berichtet von einer Vita zwischen Halb- und Unterwelt: von Verführungen des 15-jährigen Bacon durch Stallburschen in Dublin, von einem geheimen Spielclub in seiner Wohnung, von zwielichtigen Etablissements in Berlin und Paris bis hin zu Bacons Kriegserlebnissen, wo er nach Bombenangriffen Tote wegkarrte. Dem Abgründigen gegenüber steht der großmütige Gentleman Bacon; der belesene Intellektuelle, der die antiken Tragödien, Nietzsche, Sigmund Freud und Marcel Proust las, von James Joyce und T. S. Eliot schwärmte und eine ungewöhnliche Arbeits- und Selbstdisziplin aufbrachte.

Francis Bacon wurde am 28. Oktober 1909 als Sohn britischer Eltern in Dublin geboren. Er war das zweite von fünf Kindern. Seine Brüder starben früh, seine Schwestern wanderten später aus. Francis’ Vater, Edward Anthony Mortimer Bacon, leitete seine Herkunft von dem elisabethanischen Staatsmann und Philosophen Sir Francis Bacon ab. Ehemals in militärischen Diensten, arbeitete das als autoritär und gewalttätig geltende Familienoberhaupt als Zureiter und Trainer von Rennpferden. Bacons Mutter Christina Winifred Firth hingegen wurde als aufgeschlossen, gesellig und gebildet bezeichnet.

Mit Ausbruch des Ersten Weltkrieges wurde der Vater ins Kriegsministerium nach London berufen. Die folgenden zehn Jahre waren von häufigen Wohnortswechseln geprägt. In Bacons Jugend wohnte die Familie abwechselnd in Dublin und in London. Aufgrund der Kriegswirren und der zahlreichen Umzüge wuchs Bacon ohne regelmäßige Schulbildung und teilweise sich selbst überlassen auf. Aus einem Internat flüchtete er nach wenigen Wochen. Früh machte Francis Bacon die Erfahrung von Gewalt. Während des Osteraufstands der Sinn Féin-Bewegung im Jahre 1916 lebte er wieder in Irland. Später erinnerte er sich, „in einem mit Sandsäcken verbarrikadierten Haus“ gewohnt zu haben.

Mit 16 Jahren wurde sich Bacon seiner Homosexualität bewusst. Sein Vater überraschte ihn dabei, als er Dessous seiner Mutter anprobierte, und warf ihn aus dem Haus. Francis Bacon ging nach London, wo er Gelegenheitsarbeiten annahm und unter anderem bei einem Notar arbeitete. 1927 schickte ihn sein Vater nach Berlin. Dort sollte er unter der Obhut von Harcourt-Smith, einem ehemaligen Kameraden des Vaters und ebenfalls Pferdezüchter, leben. Dieser verkehrte jedoch selbst in fragwürdigen Kreisen und die Erziehungsversuche scheiterten. Bacon stürzte sich ins Berliner Leben, wohnte mit Harcourt-Smith im Hotel Adlon und besuchte die Kinos. Im Sommer begab er sich aber nach Paris, wo er zu zeichnen und zu aquarellieren begann. Den Wunsch, eine Kunstschule zu besuchen, äußerte er nicht. Gelegentlich arbeitete er als Innendekorateur und Designer. Im Juli begegnete er in der Galerie Paul Rosenberg Werken Picassos, die ihn sehr beeindruckten. Er besuchte Ausstellungen mit Arbeiten von Fernand Léger, Joan Miró, Max Ernst und Giorgio de Chirico.

Ein Jahr später bezog er eine zum Atelier umgewandelte Garage in Queensberry Mews West, South Kensington. Dort veranstaltete er Ausstellungen mit Wand- und Bodenteppichen sowie mit modernen Möbeln aus Stahl und Glas nach eigenen Entwürfen. Francis Bacon begann, als Autodidakt Ölbilder zu malen. Der vom Kubismus beeinflusste australische Künstler Roy de Maistre unterwies ihn zwar im Umgang mit der Ölfarbe, blieb jedoch der einzige Lehrer, den Bacon je hatte. Weil seine Ausstellungen ohne Resonanz blieben, gab Bacon seine Tätigkeit als Möbeldesigner wieder auf. 1931 zog er in ein Atelier in der Fulham Road und nahm kunstferne Gelegenheitsarbeiten an, um seinen Lebensunterhalt zu verdienen.

Auf der Ausstellung „Art Now“ der Londoner Mayor Gallery wurde Bacons Werk "Kreuzigung" (1933) ausgestellt und in einem Buch reproduziert. Der Sammler Michael Sadler erwarb erste Bilder Bacons. Da er keinen Galeristen fand, um sein Werk zu betreuen, organisierte Francis Bacon 1934 seine erste Einzelausstellung mit Ölbildern und Gouachen im Untergeschoss des Hauses eines Freundes. Die Ausstellung wurde ein Misserfolg und fand weder bei Presse noch Publikum ein Echo. Entmutigt malte er weniger und ergab sich der Spielleidenschaft, reiste durch Europa und arbeitete eine Zeit lang als Croupier. 1936 wurden seine Bilder zu einer internationalen Surrealisten-Ausstellung für zu wenig surrealistisch befunden und ausgeschlossen. Im Folgejahr nahm Bacon aber an einer Gruppenausstellung junger britischer Künstler teil. Aufgrund seines Asthmas wurde er 1941 vom Kriegsdienst befreit und zum Zivilschutz einberufen, von dem er nach kurzer Zeit jedoch krankheitsbedingt ebenfalls freigestellt wurde. Eine Zeit lang lebte Bacon nun auf dem Land in Petersfield in Hampshire.

Ende 1942 kehrte Francis Bacon nach London zurück und bezog sein altes Atelier in Kensington. Zwischen 1942 und 1943 zerstörte er seine künstlerische Produktion der vergangenen Jahre fast vollständig. Aus dem Zeitraum 1929–1944 blieben nur 15 Bilder erhalten. Ab 1944 intensivierte Bacon seine Malerei. Das Triptychon "Drei Studien zu Figuren am Fuße einer Kreuzigung" (1944) entstand und löste bei einer Ausstellung im Folgejahr heftige Diskussionen aus. Es folgten diverse Gruppenausstellungen, in denen seine Bilder das Publikum polarisierten. 1946 ließ sich Bacon in Monte Carlo nieder, wo er sich seiner Spielleidenschaft widmete. Bis 1950 pendelte er zwischen Monte Carlo und London hin und her, bevor er auf Dauer in die britische Hauptstadt zurückkehrte.

1946 besuchte die Galeristin Erica Brausen auf Anraten von Graham Sutherland Bacon in seinem Atelier. Brausen erwarb für £ 350 sein "Painting 1946," das sie 1949 ans Museum of Modern Art in New York verkaufte, wo es auch heute noch zu sehen ist. Erica Brausen kuratierte 1949 in ihrer frisch eröffneten Hannover Gallery in London Bacons erste Einzelausstellung. Dies verhalf dem Künstler zum Durchbruch. Bacon und Brausen blieben viele Jahre enge Freunde.

Für einige Monate unterrichte Franics Bacon am Londoner Royal College of Art. In dieser Zeit begann er seine Papstserien nach einem Motiv des Papstes Innozenz X. von Velazquez zu malen. Er schloss Freundschaften zu Lucian Freud, den er 1951 erstmals porträtierte und der ihn ebenfalls 1951/52 porträtierte sowie mit David Sylvester, mit dem er zwischen 1962 und 1986 zahlreiche Interviews führte. Francis Bacon reiste nach Südafrika, um seine Mutter zu besuchen, die seit dem Tode des Vaters (1940) dort lebte. Auch in Kairo machte er Station. 1952 lernte er Peter Lacy kennen, einen Barpianisten, der Bacons Geliebter wurde. Bacon besuchte ihn in Tanger.

In den nächsten Jahren folgten mehrfache Wohnungs- und Atelierwechsel. Bacon hatte verschiedene Einzelausstellungen in London und New York und gestaltete 1954 gemeinsam mit Lucian Freud und Ben Nicholson den Britischen Pavillon auf der XXVII. Biennale in Venedig. Ebenso entstanden Landschaftsbilder, in denen Bacons Bewunderung gegenüber Vincent van Gogh zum Ausdruck kam. In den nächsten Jahren fanden zahlreiche Einzelausstellungen in Italien, England, Frankreich und den USA statt. Das Institute of Contemporary Arts in London organisierte 1955 seine erste Retrospektive. Weitere Retrospektiven durch die Tate Gallery in London (1962), das Guggenheim-Museum in New York (1963) und das Art Institute of Chicago (1963) zeugten ebenso von Bacons wachsender internationalen Bedeutung, wie seine Beteiligung an der Documenta (documenta II 1959) in Kassel. Am Abend der Londoner Retrospektive 1962 starb Bacons Freund Peter Lacy infolge schwerer Krankheit durch exzessiven Alkohol- und Drogenmissbrauch.

Francis Bacon lernte George Dyer kennen, einen gleichermaßen depressiven wie gewaltbereiten Gauner aus einfachen Verhältnissen, mit dem er eine Beziehung einging. Bacons Ruhm nahm in der Folgezeit zu, er erhielt mehrere Preise, 1967 den Rubenspreis der Stadt Siegen. Im Jahr 1964 war Francis Bacon Teilnehmer der documenta III in Kassel. 1971 stand er an der Spitze einer Rangliste der zehn bedeutendsten lebenden Künstler.

Am Vorabend der großen Retrospektive im Pariser Grand Palais wurde George Dyer tot in seinem Hotelzimmer aufgefunden, zusammengekauert auf dem Toilettensitz. Den durch Tabletten und Alkohol wahrscheinlich herbeigeführten Suizid verarbeitete Bacon in verschiedenen Bildern "(Triptychon August 1972; Triptychon Mai–Juni 1973)". 1975 traf Bacon Andy Warhol in New York, 1978 Balthus in Rom. Während dessen wurden weitere Retrospektiven in der Kunsthalle Düsseldorf (1972), im Metropolitan Museum of Art in New York (1975), in der Tate Gallery in London (1985), in der Staatsgalerie Stuttgart (1985) sowie in der Nationalgalerie in Berlin (1986) organisiert; dazwischen größere Ausstellungen in Spanien, Japan und Amerika und Beteiligungen an der documenta 6 (1977) und der DOCUMENTA IX (1992) in Kassel. Als einer der wenigen westlichen Künstler erhielt Bacon eine Ausstellung in der Tretjakow-Galerie in Moskau. Im Alter wurde ihm eine Niere entfernt, was ihn nicht von weiterem Alkoholkonsum abhielt. 1991 reiste der Maler zum Besuch der Velazquez-Ausstellung im Prado nach Madrid. Dort starb Francis Bacon nach einem Herzinfarkt am 28. April 1992. Sein Leichnam wurde eingeäschert, die Asche in England verstreut.

„Es gab so viel Krieg in meinem Leben“, bekannte Francis Bacon in seinem letzten veröffentlichten Interview drei Monate vor seinem Tod. Der Maler spannte damit eine Hintergrundfolie, vor der man sein Werk deuten kann, rückte Lebenslage und Schaffen in einen spezifischen Zusammenhang. Tatsächlich spielt der Gewaltaspekt eine zentrale Rolle in Bacons Bildern. Immer wieder beschäftigte er sich mit den Themen Gewalt, Zerstörung und Verfall, in deren Zentren die menschliche Figur steht. Torsohafte, verkrüppelte Körper, bluttriefende Fleischmassen und verstümmelte Kadaver sind erklärte Ausdrucksträger exzessiver Gewalttätigkeit. Seine Entwürfe sind Spiegel der Schicksalhaftigkeit menschlicher Existenz, die für ihn ein Dasein zum Tode ist.

Bacons Leinwände sind gerahmt, fast immer in den Maßen 198 cm × 147,5 cm (größere Maße ließ sein Atelier nicht zu), nicht gefirnisst, sondern hinter Glas ausgestellt. Seine Bilder, meist mit Öl gemalt, waren zunächst durch einfache symmetrische Beziehungen und übersichtliche Farbzonen strukturiert. Dies gilt sowohl für Einzelbilder als auch für Bildergruppen, seine Triptychen. Simple geometrische Formen bilden den Hintergrund. Ein oftmals kreisrunder Untergrund oder eine elliptische Horizontlinie, rechteckige Wandflächen, gerüstartige Linien im Raum bei geordneter, pastelliger Farbpalette geben Bacons Figuren Struktur ("Studie zu einem Portrait von Lucian Freud," 1973; "Studie zu einem Selbstporträt," 1985). Die Figuren selbst sind mit reicherer Palette aufgetragen, mit groben Pinselstrichen verschmiert, die Farben mit Bürsten oder Lappen auf die Leinwand gestrichen. So verhalten sich die farbigen Inkarnate kontrapunktisch zu den entleerten, fast sterilen Hintergründen und geraten auf diese Weise in Isolation. Sie sind unscharf, verwackelt und teilweise bis zur Unkenntlichkeit deformiert. Mehrere Farbschichten überlagern einander, entstellen die Züge der Figur, verwackeln ihre Konturen. Die Beziehung zwischen Figur und Hintergrund wird brüchig. Erscheint die Kulisse statisch und neutral, werden die Figuren oftmals dynamisch und energiegeladen dargestellt. Auf eine fast klinische Folie legt sich der Gegensatz einer zerdehnten Körpermasse.

Francis Bacon entlehnte seine figuralen Stellungen teilweise der Fotografie, aber auch der Skulptur. So lieh er Motive von Eadweard Muybridges Bewegungsstudien aus ("Man with Dog," "Zwei Figuren," 1953; "Zwei Figuren im Gras," 1954; "Studien des menschlichen Körpers," 1970), nutzte historische Pressefotos sowie Reproduktionen schematischer Darstellungen aus einem Handbuch für röntgenologische Lagerungstechniken und übernahm Elemente diverser Skulpturen Michelangelos neben Merkmalen antiker Marmorstatuen. Er griff auch die Effekte der Fotografie auf, Bildüberblendungen, Unschärfen, Verwacklungen und Negativumkehrungen sowie Bewegungsphasen (Three Studies of Isabel Rawsthorne 1967).

Francis Bacon schuf erst 1956 sein erstes Selbstporträt und 1958 schließlich das zweite. In den folgenden Jahrzehnten befasst er sich mehr und mehr mit dem menschlichen Körper und porträtiert seine engen Freunde und sich selbst. Bacon malt die Porträts oft als Triptychen, da er jedes Bild in ständiger Veränderung sah. Zitat: „Ich sehe Bilder in Serien. Ich vermute ich könnte über das Triptychon weit hinausgehen und fünf oder sechs nebeneinander stellen, halte aber das Triptychon für eine ausbalancierte Einheit.“ Diese Tafeln setzt er jedoch in einzelne Rahmen, da er die Verbindung innerhalb der Bilder, welche der Betrachter automatisch aufbaut, dadurch zu hinterfragen und zu unterbinden versucht. Auffällig bei den Selbstporträts, aber auch den Porträts allgemein, sind die Kreise, Schraffuren und Ovale welche das Gesicht des Dargestellten dominieren. Es ist ein weiterer Versuch Bacons, den Betrachter von traditionellen Porträts zu distanzieren und das Illustrative zu vermeiden. Auch fällt es dem Gegenüber dadurch schwer, einen Ruhepol in dem Gesicht ausfindig zu machen. Stets wandert der Blick umher und folgt den geometrischen Formen. Gleichzeitig gliedern diese Formen aber auch das Gesicht, was durch die jeweilige Farbigkeit verstärkt wird. „Weil ich immer hoffe, die Leute so umzuformen, dass ihre Ausstrahlung deutlich wird; ich kann sie nicht wörtlich abmalen“. Dieses Zitat lässt erkennen, dass es ihm auch nicht um die Wiedergabe der äußeren Erscheinung einer Person ging, sondern um dessen Ausstrahlung. Bacon gibt uns in diesen Gesichtern Hinweise, wo Nase, Augen, Mund oder Haare zu finden sind, lässt sie uns aber nicht definitiv erkennen. Das Porträt soll die Gefühle wiedergeben, welche der jeweils abgebildete Mensch charakterisiert und welcher er oft empfindet. So vereint er unterschiedliche Gefühle und Emotionen, mit der jeweiligen körperlichen Umsetzung, in einem Bild.

Kreise, Pfeile und käfigartige Linienkonstruktionen ("Figur in Bewegung," 1976) umlagern häufig das Bildzentrum und deuten auf das separierte Moment der Figur, auf ihre bloße Kreatürlichkeit. Durch das Liniengerüst entsteht ein Raum im Raum, die Unterbrechung eines Kontinuums, die sich in der Präsentation des Exponats wiederholt: umfangreiche Rahmung und die schützende Glasscheibe versiegeln die Bildoberfläche, schalten zwischen Betrachter und Bild einen Wahrnehmungsfilter.

Der Raum in Bacons Werken ist polyperspektivisch definiert. Parallellinien, die sich glaskastenförmig um die Figur legen, unterteilen das Bild in perspektivische Felder. Ein eindeutiger Blickwinkel, ein exakter Fluchtpunkt wird aufgegeben. Es entstehen multiple Raumdefinitionen, die nebeneinander stehen und gegeneinander wirken. Raum ist keine Eigenschaft der Geometrie mehr, sondern eine Eigenschaft der Figur. Bacons Raum ist ein verschränktes Raumsystem, das miteinander in Konflikt steht und durch die Präsenz der Figur zusammengehalten wird. So wird es möglich, dass die dargestellte Raumstruktur unterschiedliche Interieurs suggeriert: innen und außen, Kammern und Säle, private Zimmer und öffentliche Bühnen, Weitläufigkeit, Enge und Begrenztheit. Bacons Räume bieten ganze Spektren: Offenheit und Abgrenzung, Aseptik und blutige Befleckung.

Die Bilder Francis Bacons erzählen keine Geschichten. Sie sind vielmehr durch das Fehlen sinnvoller Verbindungsstücke charakterisiert. Zwar stehen seine Bildensembles in einer Beziehung zueinander, seine Triptychen in einem formalen Ordnungszusammenhang. Jedoch gibt es keine verbindliche narrative Logik zwischen den einzelnen Bildtafeln. Das Dargestellte soll sich nicht zu einer Erzählung verbinden lassen, soll kein dramaturgisches Geschehen abbilden. Einen Sinn gibt es nicht, weil die erzählerischen Elemente fehlen.

Die Kontinuität der Handlungsabfolge scheint unterbrochen. Bacon wiederholt diesen Aspekt seiner Kunst in der Figurendarstellung selbst. Die Figur bewegt sich, bleibt aber auf der Stelle. Ihre Bewegung ruht in sich selbst, erzeugt eine plastische Metamorphose, zieht eine Bewegungsspur. „Ich möchte, dass meine Bilder so aussehen, als sei ein menschliches Wesen durch sie hindurchgegangen, wie eine Schnecke, eine Spur von menschlicher Anwesenheit und die Erinnerung an vergangene Ereignisse zurücklassend, so wie die Schnecke ihren Schleim zurücklässt“, lautet eine Formulierung des Malers.

Kein anderes Motiv hat Bacons Arbeiten stärker inspiriert als die Bewegungsstudien von Edward Muybridge. Dessen Fotoreihen dokumentieren sequentielle Abläufe menschlicher und tierischer Körper. Ihre Bewegung vollzieht sich in einem zeitlichen Nacheinander und einem räumlichen Hintereinander. Ganz ähnlich hatten Marcel Duchamp ("Akt, eine Treppe hinabsteigend," 1912) und Umberto Boccioni ihre futuristischen Bilder gestaltet, als raumzeitliche Abfolgen, als ein Nebeneinander zusammenhängender Körperstellungen. Bacon hingegen verzichtet auf die Darstellung der Bewegung im Raum. Bei ihm sind die Bewegungsphasen übereinander geblendet, so als schnitt man die Figuren Muybridges aus und projizierte sie übereinander. Die Schnitte im Bewegungsablauf erfährt der Betrachter als Schnitte des Körpers. So entsteht eine Identität von Bewegung und Verletzung. Das, was der Betrachter als körperliche Verformung wahrnimmt, sind fehlende Verbindungsglieder der Bewegungssequenz.

Das "Gemälde 1946" zeigt eine abgründige, dunkle Figur, eingepasst zwischen Stahlrohren und einem aufgespannten Regenschirm. Ein aufgehängter, geschlachteter Ochse, der an Rembrandt van Rijns Ochsen erinnert ("Der geschlachtete Ochse," 1655), kündet von der Kreuzigungsszene. Religiöse Motive finden sich häufig in Bacons Werk. So gestaltet er zwischen 1950 (Study after Velazquez I, amer. Privatbesitz) und 1965 zahlreiche Variationen zu Papst Innozenz X. nach Diego Velázquez. Insgesamt variiert er das Thema 45-mal. Dem Sakralen ordnet Bacon das Irdische zu, der Papst ist ein schreiender Papst, Ausdruck alltäglichen Leidens an der Welt.

Francis Bacon entlehnt diese elementare menschliche Äußerung, den aufgerissenen Mund, aus ihm vertrauten Abbildern. Der Maler ist vom Gesichtsausdruck der Mutter in Nicolas Poussins Gemälde "Der Bethlehemitische Kindermord" (1628) beeindruckt; gleichermaßen vom Zerrbild der Kindermädchenmimik auf der Potemkinischen Treppe von Odessa in Sergej Eisensteins Filmklassiker "Panzerkreuzer Potemkin". Übertragen auf die Papstbilder, durchbricht der Schrei die liturgische Ordnung und unterminiert das kirchliche Weltbild. Der Schrei als Metapher des Schmerzes zeugt von Verlangen nach Erlösung und der Verzweiflung ihres Ausbleibens.

In den 1960er Jahren wählt Bacon vermehrt das Triptychon als Darstellungsform. Bacon selbst behauptet, das Breitwandkino mit seinem Cinemascope-Format habe ihn zu dieser Idee verführt. Tatsächlich jedoch ist die Dreiteiligkeit der Bildtafeln von tieferer Bedeutung. So erinnert sie in ihrer Symmetrie an das Format des Altarbildes mit seinen aufklappbaren Seitenflügeln. Assoziationen an die Dreifaltigkeit werden ebenso geweckt wie an die drei Kreuze der Kreuzigungsdarstellungen Christus’. Das Triptychon bietet Bacon die Möglichkeit, seinen Bildern eine religiöse Thematik einzuschreiben, ohne diese darlegen zu müssen.

Mit den Werken "Drei Studien zu Figuren am Fuße einer Kreuzigung" (1944), "Drei Studien für eine Kreuzigung" (1962) oder "Kreuzigung" (1965) thematisiert er den Passionsgedanken jedoch unmittelbarer. Die Allegorie des Todes findet in der Marter des Körpers ihren Ausgang. Das Schlachthaus ist für Bacon das moderne Passionsgelände, das geschlachtete Fleisch Sinnbild existentieller Erfahrung. In einem Interview mit David Sylvester äußert sich Bacon wörtlich: „Wenn man in eines dieser großen Lagerhäuser geht, und diese riesigen Hallen des Todes durchschreitet, kann man das Fleisch und die Fische und die Vögel und vieles andere sehen, das da tot daliegt. Und selbstverständlich wird man als Maler ständig daran erinnert, dass die Farbe von Fleisch tatsächlich sehr, sehr schön ist. […] Nun, wir sind ja schließlich selbst Fleisch, potentielle Kadaver.“ Der Tierkadaver ist für Bacon das Bild eigener Sterblichkeit, über ihn findet er den direkten Weg zur Kreuzigung. „Ich war schon immer sehr berührt von Bildern, die mit Schlachthäusern und Fleisch zu tun haben. Für mich gehören sie sehr stark zu dem ganzen Thema der Kreuzigung“, sagt der Maler 1962. Bacon selbst hat die Kreuzigung als eine Art Gerüst beschrieben, „an dem man alle denkbaren Gefühle und Eindrücke aufhängen kann“.

Francis Bacons Gemälde rezitieren den körperlichen Schmerz, führen Mensch und Vieh auf die Elementarebene des Seins zurück und machen sie zur Kreatur, zum anonymen Geschöpf. Der offene Körper, das blutende Opfer und die Gewalt des Fleisches werfen alle mystischen Implikationen ab und stehen für ein Leiden ohne Sinn.

So verwundert es kaum, dass in den knapp 130 Bildern, die das Werkverzeichnis von Bacons Anfängen bis zu "Study for the Nurse from the Battleship Potemkin" von 1957 verzeichnet, etwa ein Viertel der Gestalten mit geöffnetem Mund dargestellt sind. Das allein zeigt, welche Bedeutung Bacon dem Schrei beigemessen hat. Eine vermehrte und intensive Auseinandersetzung mit diesem Thema zeigt 1944 sein erstes Triptychon "Three Studies at the Base of a Crucifixion" und schließt mit dem Werk "Study for the Nurse from the Battleship Potemkin". Im Hinblick auf sein gesamtes Werk kann man feststellen, dass der Schrei in mehr oder weniger starker Form immer wieder präsent ist, so taucht das Motiv des geöffneten Mundes erstmals in seinem Werk "Figures in a Garden" von 1936 auf. Die auffällig obsessive Beschäftigung mit diesem Thema lässt sich jedoch in den 1950er Jahren verorten, so ist der offene Mund das auffälligste Merkmal der Serien von Köpfen und Kopfpartien, die Bacon zwischen 1948 und 1952 gemalt hat.

Wenn man seine Werke betrachtet, in denen der Schrei beziehungsweise der weit aufgerissene Mund eine zentrale Rolle spielt, dann nehmen die Papstdarstellungen, auch wegen ihrer großen Anzahl einen besonderen Rang ein, so zählt beispielsweise "Study after Velázquez’s Portrait of Pope Innocent X" zu einem seiner bedeutendsten Werke, uns begegnet das Motiv des geöffneten Mundes aber auch in anderem Kontext. Bacon schöpfte für das Motiv des Schreis aus zahlreichen und unterschiedlichen Quellen (Text und Bildfragmente), dies ist vor allem auch deshalb sehr wichtig, da dieses „ästhetische Verfahren“ charakteristisch für das Arbeiten Bacons ist. Neben Nicolas Poussins Gemälde "Der Bethlehemitische Kindermord" (1628) und dem Standfoto der ins Auge geschossenen, tödlich verletzten Kinderfrau aus Sergej Eisensteins Film "Panzerkreuzer Potemkin" gibt es eine dritte wichtige Bildquelle, die ihn tief prägte und ihm immer wieder als Inspirationsquelle und Bildauslöser diente. Sie ist aus eher malerisch-formalen Gründen für ihn wichtig. Es handelt sich dabei um eine Reihe von medizinischen Fotos, die er Büchern und Magazinen entnahm und die in Großaufnahmen verzerrte Münder, Zähne oder auch allerhand Mundkrankheiten zeigen. Zuletzt sei eine vierte Quelle genannt, Georges Batailles Artikel "Bouche" in der Zeitschrift "Documents" (1930). Bataille diskutiert in ihm die Tatsache, dass durch den Mund die bedeutendsten Erfahrungen von Vergnügen und Schmerzen des Menschen ausgedrückt werden und dadurch die Ähnlichkeit von Mensch und Tier enthüllt wird. Bacon besaß die Zeitschrift, was darauf hinweist, dass ihm die Theorien Batailles nicht nur bekannt waren, sondern von ihm auch adaptiert wurden. Diese Verbindung lässt sich beispielsweise in Bacons "Werken Head I" (1948) und "Head II" (1949) feststellen, in denen ein Geschöpf, teils Mensch, teils Tier, seinen kreatürlichen Schrei abgibt.
Der Schrei wurde zu Bacons Urbild, zu einem Beispiel für Mehrdeutigkeit, der ohne die geringste Differenzierung Wut, Schmerz, Angst und Lust vermittelt.

1988 wurde Bacons Atelier in der Reece Mews Nr. 7, South Kensington, London, abgebaut, nach Dublin transportiert und in der Dublin City Gallery The Hugh Lane wieder aufgebaut. Ein Team von Restauratoren, Archäologen und Kuratoren wurde mit der Katalogisierung des Inventars und der originalgetreuen Rekonstruktion beauftragt. Vor dem Transport wurden in einer speziell entwickelten Datenbank mehr als 7000 Einzelstücke katalogisiert: Bacons Arbeitsmaterial, Bücher, Fotografien, Korrespondenz u. a.

Bei einer Auktion in New York wechselte am 14. Mai 2008 Bacons Werk "Triptych, 1976" für 86,3 Millionen US-Dollar (55,7 Millionen Euro) den Besitzer. Zu dem Zeitpunkt war noch nie so viel für ein Bild der Nachkriegszeit gezahlt worden. Es liegt aktuell auf Rang vierzehn der teuersten Gemälde der Welt.

Im Juli 2011 wurde "Studies for a portrait" für 20 Millionen Euro und "Crouching Nude" (1961) für 9,3 Millionen Euro verkauft.

Am 12. November 2013 wurde "Three Studies of Lucian Freud (Triptychon)" aus dem Jahr 1969 bei Christie’s in New York für 142,4 Millionen US-Dollar versteigert (Höchstgebot 127.000.000 US-Dollar zuzüglich Kommission). Das Werk war damit das zu diesem Zeitpunkt am teuersten verkaufte Gemälde.






</doc>
<doc id="9584" url="https://de.wikipedia.org/wiki?curid=9584" title="Schule">
Schule

Eine Schule (lateinisch "schola" von [], Ursprungsbedeutung: „Müßiggang“, „Muße“, später „Studium, Vorlesung“), auch Bildungsanstalt oder Lehranstalt genannt, ist eine Institution, deren Bildungsauftrag im Lehren und Lernen, also in der Vermittlung von Wissen und Können durch Lehrer an Schüler, aber auch in der Wertevermittlung und in der Erziehung und Bildung zu mündigen, sich verantwortlich in die Gesellschaft einbringenden Persönlichkeiten, besteht.

Wortlisten aus Sumer lassen vermuten, dass es Schulen schon seit dem 4. Jahrtausend v. Chr. gibt. Schultexte aus dem 3. Jahrtausend v. Chr. als direkte Belege wurden im sumerischen Schuruppak (im heutigen Irak) gefunden. Die sumerischen Schulen, in denen die Schulbänke aus Lehmziegeln bestanden, wurden als Tafelhäuser bezeichnet. Als Fächer lassen sich Rechnen, Zeichnen und Sumerisch, d. h. Lesen und Schreiben, bestimmen. Im Unterricht wurden Aufsätze, Fabeln, Weisheitslehren, Hymnen und Epen verfasst. Die „Väter“ genannten Lehrer zeigten zum Teil Humor, wie die Lehrgeschichte Fabel vom klugen Wolf und den neun dummen Wölfen zeigt, den die Schüler abschreiben mussten.

Im Alten Ägypten war der Schulbesuch nur den Wohlhabenden möglich, da die Kinder der gesellschaftlichen Unterschicht, größtenteils Bauern und einfache Handwerker, ihren Eltern meist bei der Arbeit helfen mussten. Wer schreiben konnte, genoss ein hohes Ansehen und hatte so die Möglichkeit, Priester oder Beamter zu werden. Mädchen besuchten im Alten Ägypten ebenfalls die Schule, auch wenn dies seltener vorkam als bei Jungen. Unterrichtet wurde für gewöhnlich in Tempelschulen und Verwaltungsgebäuden. Die Erziehung in diesen Einrichtungen war sehr streng, sodass auch körperliche Züchtigung deren fester Bestandteil war. Geschrieben wurde auf Ostrakon, da Papyrus zu kostbar für einfache Schreibübungen war. Unterrichtsgegenstände waren Lesen und Schreiben, Mathematik, Geographie, Geschichte, Astronomie, Bildhauerei, Malerei und auch Sport.

Im antiken Griechenland herrschte keine einheitliche Staatsform, da sich das Land aus zahlreichen Stadtstaaten, den sogenannten Poleis (Einzahl: „Polis“) zusammensetzte. Daher war das Leben dort von Region zu Region unterschiedlich. Während Sparta als kriegsorientierte Nation das Hauptaugenmerk der Ausbildung der Jungen auf militärische Ziele legte, konnten die Kinder wohlhabender Familien Athens allgemein bildende Schulen besuchen. Dennoch gab es auch in Athen weder eine Schulpflicht noch öffentliche Schulgebäude. Stattdessen wurden die Kinder beim Lehrer zuhause unterrichtet. Anders als im Alten Ägypten hatten die Lehrer im antiken Griechenland aber kein großes Ansehen und wurden dementsprechend auch schlecht bezahlt. Dies änderte sich erst langsam um 500 v. Chr. Als Schreibstoff benutzten die Schüler Wachstafeln oder Papyrus. Viel bedeutender als Lesen und Schreiben war für die Griechen eventuell die Musik. Weil es noch keine Noten gab, mussten die Schüler auf ihren Instrumenten dem Lehrer nachspielen. Auch der Gesang wurde gelehrt, da Sänger im antiken Griechenland sehr geachtet waren, sowie außerdem Sport, sodass die Schüler an großen Wettkämpfen teilnehmen konnten.

In der Römischen Republik übernahmen die Eltern den Unterricht selbst. Es gab somit weiterhin keine Schulpflicht und auch keine öffentliche Schulform. Erst in der Römischen Kaiserzeit wurden öffentliche Schulen gegründet. Die Schüler schrieben wie im antiken Griechenland auf Wachstafeln oder Papyrus. Der Mathematik wurde zu dieser Zeit eher geringe Bedeutung beigemessen und die meisten Lehrer genossen nach wie vor kein hohes Ansehen. Der Unterricht fand teilweise auf dem Forum statt, wo es jedoch sehr laut war.

Im mittelalterlichen Europa gab es zunächst nur kirchliche Schulen in Klöstern, in denen der Unterricht im Lesen und Schreiben ausschließlich Priestern und Mönchen vorbehalten war. Cassiodor verfasste im 6. Jahrhundert eine später „Lehrplan“ genannte Studienordnung. Dieses Werk stellte eine Kanonisierung des Wesentlichen für die Schule dar. Er sah seine Klosterakademie als Bildungsgemeinschaft. Seine Schulpraxis stützte sich formal auf die ciceronische Überzeugungsrhetorik als erzieherischem Lehransatz. Dieser Lehrplan fand erst nach dem Tod des Verfassers in dem politisch geschaffenen abendländischen Kulturraum eine konsekutive Überlieferung in verschiedene Erscheinungsformen der Schule. Erst ab dem 13. Jahrhundert wurden langsam öffentliche Schulen eingerichtet. Holztafeln oder Wachsplatten dienten als Schreibutensilien. Die körperliche Züchtigung war keine Seltenheit.

Obwohl das Herzogtum Pfalz-Zweibrücken bereits 1592 als kleinere staatliche Einheit die allgemeine Schulpflicht für Mädchen und Knaben eingeführt hatte, und die Stadt Straßburg dem auch schon im Jahre 1598 mit einem entsprechenden Gesetz gefolgt war, setzte sich erst 1919 mit der Weimarer Verfassung die allgemeine Schulpflicht für ganz Deutschland durch.

Schule ist jedoch bis heute als kulturelle Einrichtung Sache der Einzelstaaten bzw. Bundesländer geblieben. Die Ausdifferenzierung hatte in der Weimarer Zeit einen Höhepunkt erreicht und wurde in der Zeit des Nationalsozialismus durch ein Reichserziehungsministerium einer gewissen Vereinheitlichung zugeführt. Dies bedeutete jedoch nicht, dass es gerade in den "neuen" Schulfächern wie Leibesübungen/Turnunterricht im Hinblick auf Ausbildung, Einsatz, Besoldung, Stundenzahl usw. so große Unterschiede gab, dass solche Fachlehrer scharenweise dem Nationalsozialismus zuliefen.

Siehe auch: Chengdu Shishi Zhongxue, Liste der ältesten Schulen im deutschen Sprachraum, Paideia, Römische Erziehung, Sieben Freie Künste, Humanismus, Deutsches Bildungssystem, Bildungsreform, Residential Schools (Kanada)

Schulentwicklung bezeichnet die Weiterentwicklung von Schulen in personeller (Personalentwicklung in Schulen) und organisatorischer Sicht. Ziel ist die inhaltliche Veränderung von Schule als Reaktion auf gesellschaftliche Rahmenbedingungen zu Beginn des 21. Jahrhunderts. Dieses Konzept ergibt sich aus der internen Diskussion der an Schule beteiligten Institutionen.

Schulen werden je nach Schulträger in Schulen in öffentlicher Trägerschaft oder Privatschulen unterschieden. Im Schul- und Bildungssystem gibt es den Primär-, Sekundär-, Postsekundär- und Tertiär­bereich, wobei die International Standard Classification of Education (ISCED) diese Stufen noch weiter unterteilt und jede Stufe mehrere Jahrgangsstufen umfassen kann.

Das Schulwesen in Deutschland ist in Grund- und Sekundarschulen mit ihren Schülern sowie Hochschulen mit ihren Hochschülern bzw. Studierenden (Studenten) aufgeteilt.

In Deutschland gibt es eine gesetzliche Schulpflicht, die sowohl den Schulträger wie die Eltern bindet.

2010 gab Deutschland durchschnittlich etwa 5800 Euro pro Schüler an einer öffentlichen Schule aus, davon sind rund 4600 Euro Personalkosten, 700 Euro Lehrmittel und 500 Euro Bau- und Sachinvestitionen. Dabei variieren die Ausgaben stark nach Schultyp und Land.

Für Schüler an allgemeinbildenden Schulen wurden durchschnittlich 6400 Euro ausgegeben, gegenüber 4000 Euro an beruflichen Schulen. Für Grundschulen betrugen die Durchschnittskosten pro Schüler 5200 Euro, bei integrierten Gesamtschulen und Gymnasien jeweils gleichermaßen 6600 Euro, bei Berufsschulen im dualen System 2500 Euro.

Das Bundesland mit den höchsten Ausgaben war Thüringen mit 7700 Euro, gefolgt von Sachsen-Anhalt mit 7100 Euro, während Nordrhein-Westfalen mit 5000 Euro die geringsten Kosten hatte. Bei den allgemeinbildenden Schulen erstrecken sich die Ausgaben pro Schüler von 8600 Euro (Thüringen) bis 5500 Euro (Nordrhein-Westfalen). Auch bei den beruflichen Schulen liegt Thüringen (5300 Euro) auf dem ersten Platz, danach folgen Baden-Württemberg und Hamburg mit je 4700 Euro. Auf dem letzten Platz liegt hier Mecklenburg-Vorpommern mit 3100 Euro pro Schüler.

Nicht einberechnet sind bei diesen Zahlen die Kosten des Schüler-BAföG.

Zu beachten ist, dass die Ausgaben für Schulen nicht unmittelbar mit dem Lernerfolg oder der Qualität des Schulsystems zusammenhängen, da insbesondere die Effizienz beim Mitteleinsatz von entscheidender Bedeutung ist.

Der gesellschaftliche Auftrag der Schule, der in Deutschland meist im Schulgesetz eines Bundeslandes festgehalten wird, liegt in der Entwicklung der Schüler zu mündigen und verantwortungsvollen Persönlichkeiten. Sie soll Bildung, also Wissen, Fähigkeiten und Werte im Unterricht gezielt vermitteln. Die gesellschaftlichen Grundwerte sind durch das Grundgesetz vorgegeben. Als weitere Aufgaben werden verschiedentlich Erziehung zur Ehrfurcht vor dem Leben, zur Bewahrung der Umwelt und Verantwortung für künftige Generationen genannt.

Die schulische Persönlichkeitsbildung entbindet die Eltern nicht von ihrem Erziehungsauftrag, sondern ergänzt diesen. Die Eltern sollen bei innerschulischen Konflikten mäßigend auf ihre Kinder einwirken. Bei Wertkonflikten zwischen Eltern und Gesellschaft (z. B. in Fragen der Sexualität, Schwimmunterricht für muslimische Mädchen, Hausunterricht) sucht die Schule eine Lösung im Sinne des Kindes, muss aber wenn nötig auch gegen den Willen der Eltern die schulische Bildung durchführen. In Streitfällen entscheiden die zuständigen Verwaltungsgerichte.

Die klassischen Schulfunktionen (vgl. Helmut Fend, "Theorie der Schule", 1980) sind:

Qualifikation und Sozialisation vermitteln den Heranwachsenden zusätzlich zum elterlichen Beitrag das kulturelle Kapital einer Gesellschaft von den Grundfähigkeiten wie Schreiben und Lesen bis zum erfolgversprechenden Auftreten. Manche Gesellschaftskritiker sprechen von einem zusätzlichen „heimlichen Lehrplan“, der all das umfasst, was neben dem offiziellen Lernprogramm angeeignet wird, um Erfolg zu haben, beispielsweise die Bildung von Netzwerken mit den Mitschülern oder Schummelstrategien.

Die Schule erfüllt neben der Förderung auch die Funktion der Selektion, das heißt, die Heranwachsenden nach ihrer Leistungsfähigkeit einzuschätzen und ihnen am Ende der Schulzeit durch Vergabe von Schulabschlüssen für weitere Ausbildungsgänge eine vorläufige soziale Position zuzuweisen. Die schulische Funktion einer Berechtigungsvergabe ist im deutschen Schulwesen traditionell stärker ausgeprägt als etwa in den USA, wo andere Selektionsmechanismen greifen. Auf dem Wege dahin sind Schullaufbahnentscheidungen zu treffen. In der demokratischen Gesellschaft soll jedem Schüler eine gerechte Chance gegeben werden. Die Realisierung von Chancengleichheit gehört zu den zentralen Streitpunkten der Bildungspolitik. Auch der angemessene Zeitpunkt der Selektion ist umstritten. Die Persönlichkeit der Schüler wird vorwiegend geprägt, sich gegenüber gestellten Leistungsanforderungen und ihrer Bewältigung positiv einzustellen. Kritiker der Selektionsfunktion wenden ein, dass die Schule faktisch weitgehend die soziale Schichtlage, in die jemand hineingeboren wird, reproduziert und insofern eine demokratische Chancengleichheit nur auf dem Papier existiert. Die Berechtigungen, die etwa ein Hauptschulabschluss verleiht, sind zudem recht gering geworden.

Loyalität zu gesellschaftlichen und politischen Normen stellt sicher, dass die bestehenden Institutionen und Verfahren überhaupt von der nächsten Generation akzeptiert werden und weiter funktionieren. Loyalitätssicherung ist in allen politischen Systemen eine zentrale Funktion des Bildungssystems. So war die Schule der DDR in höchstem Maße darauf ausgerichtet, die Existenz der DDR zu rechtfertigen (am Ende weitgehend erfolglos). In demokratischen Systemen ist die Ausbildung eines Demokratiebewusstseins eine Hauptaufgabe der gesellschaftswissenschaftlichen Fächer. Das Auftreten von jugendlichem politischem Extremismus führt in der Regel zu einer Verstärkung der entgegensteuernden Schulaktivitäten im gefährdeten Bereich. In dieser Hinsicht erweist sich die Schule als ein die Gesellschaft stabilisierendes System.

Der staatliche Auftrag, Schulen zu unterhalten, kann vom Staat selbst (öffentliche Schulen) oder von privaten Trägern (nach Grundgesetz Art. 7 (4) (Privatschulen)) erfüllt werden. In engen Grenzen kann die Schulpflicht auch an Nicht-Schulen erfüllt werden. So können zum Beispiel Kinder, Jugendliche und junge Erwachsene mit einer geistigen Behinderung in Niedersachsen anstatt einer Regelschule oder einer Förderschule auch eine Tagesbildungsstätte besuchen. Die Aufsicht über alle Einrichtungen, in denen Schüler ihrer Schulpflicht nachkommen, liegt nach Art. 7 (1) GG beim Staat.

Die rechtlichen Beziehungen zwischen den Angehörigen der Schule sind im Schulrecht geregelt. Über das Schulgesetz, Erlasse und Verordnungen sowie Lehrpläne werden die Schulangelegenheiten von den zuständigen Landesparlamenten und Kultusministerien geregelt. Der Schulleiter sorgt für die rechtsstaatliche Einhaltung aller Bestimmungen und ist der Empfänger von Beschwerden gegen die Lehrer. Bei der Leitung einer Schule konkurrieren das hierarchische Schulverwaltungsrecht (Schulleiter leitet die Schule) und das demokratische Schulmitbestimmungsrecht (Entscheidungsinstanzen der Schule sind die Gremien, zum Beispiel die Schulkonferenz) miteinander. Die Notengebung ist der Mitbestimmung durch Konferenzen weitgehend entzogen, während Entscheidungen über die Nichtversetzung von Schulgremien getroffen werden.

Über die einzelnen Schulen wachen auf verschiedenen Ebenen (je nach Schulform) die Schulaufsicht sowie die für Schule zuständigen Ministerien (ebenfalls mit verschiedenen Namen in den Bundesländern). Der direkte Vorgesetzte des Schulleiters ist meist ein Schulrat, ein Schulamtsdirektor oder ein Regierungsschuldirektor.

Die Organisation einer Schule beruht auf einer Schulgemeinschaft.

Sie besteht je nach Schulart und Ausstattung aus:


Je nach Schule wird diese Organisation ergänzt durch:

Schulen als demokratische Einrichtungen unterliegen der Beteiligung der Beschäftigten (Lehrer) sowie der Eltern, Schüler und weiterer gesellschaftlicher Gruppen:

Auf der Seite des Schulträgers bestimmt der Rat nach vorbereitender Arbeit im Schulausschuss (der je nach Zuschnitt des jeweiligen Schulträgers auch anders heißen kann). In diesem sind auch weitere gesellschaftliche Gruppen vertreten (Kirchen, Ortslehrerschaft, Sportverband…).

Organisatorisch ergänzend gibt es

Die Zusammenarbeit in der Schule ist in Konferenzen organisiert. Die Beratungen sind nicht öffentlich. Die Konferenzen tragen verschiedene Bezeichnungen in den diversen Staaten und Ländern:


Der Besuch einer Schule ist in Deutschland durch die Schulpflicht vorgeschrieben. Üblicherweise gilt die Vollzeit-Schulpflicht bis zum neunten oder zehnten Pflichtschuljahr, das heißt vom sechsten bis zum fünfzehnten oder sechzehnten Lebensjahr. Rückstellungen sind unter Umständen möglich, neue Tendenzen legen den möglichen Schulbeginn bereits in das fünfte Lebensjahr. An die Vollzeit-Schulpflicht schließt sich in Deutschland die Berufsschulpflicht an, die in der Regel bis zum Abschluss einer Berufsausbildung oder bis zum Ende des zwölften Schulbesuchsjahres gilt.

Durch die Schulpflicht schreibt in Deutschland der Staat im Gegensatz zu einer Bildungspflicht sogar vor, wie und in welcher Form Bildung zu erfolgen hat. Hausunterricht, bei dem Schüler von ihren Eltern oder Privatlehrern unterrichtet werden, ist – von wenigen Ausnahmen abgesehen – in Deutschland unzulässig. Aus der Schulpflicht folgt eine Aufsichtspflicht der Schule über die Kinder und Jugendlichen.

Es ist daher auch falsch, die Schule als reine staatliche Dienstleistung zu betrachten, sondern sie stellt auch eine Ausübung von legitimer Macht dar und schränkt einige Grundrechte der Eltern und Kinder nach gesetzlichen Normen (Freizügigkeit, freie Berufswahl) ein. Deutlich wird das auch an dem Verbot der Kinderarbeit, das in einem engen Zusammenhang mit der Vollzeit-Schulpflicht steht. Insofern hat die Schule eine hoheitliche Funktion, die letztlich nur vom Staat geregelt werden darf.

Die Einschulung erfolgt seit dem 18. Jahrhundert nur einmal im Jahr. Es gibt aktuelle Überlegungen, dies zu ändern.

In der Bundesrepublik Deutschland gibt es etwa 47.000 allgemeinbildende und berufsbildende Schulen. Das Bildungswesen steht unter der Kulturhoheit der Bundesländer. Die Bezeichnungen und Unterrichtsinhalte der einzelnen Schultypen
können sich daher von Bundesland zu Bundesland unterscheiden. Die Kultusministerkonferenz vereinbart Regeln zur Vergleichbarkeit von Abschlüssen und andere Eckpunkte.
Schultypen (unvollständig):


In Österreich ist das Schulwesen bundeseinheitlich geregelt.

In der Schweiz sind die Kantone für das Schulwesen zuständig.

Im Fürstentum Liechtenstein ist das Schulwesen einheitlich geregelt.

Laut PISA-Studien hat Finnland das weltweit erfolgreichste Schulsystem. Eines der Vorbilder war das Schulsystem der DDR. In heutigen Tagen suchen finnische Bildungspolitiker nach neuen Vorbildern, um Kosten zu sparen und eine effektivere Selektion in Finnland einzuführen.

So besuchen 90 % der finnischen Schüler im Alter von sechs Jahren eine qualifizierte Vorschule. Die Schulpflicht beginnt mit sieben Jahren. Die Klassenstärken liegen bei ca. 20 Schülern.

Da diese Länder zumeist auf Grund politischer Instabilitäten und Fehlentwicklungen weder über das erforderliche Budget noch über ein wahres Interesse am „lesenden Volk“ verfügen, sind qualifizierte Schulsysteme in diesen Ländern selten. Somit ist das Bildungsniveau in diesen Ländern über Jahrzehnte hinweg katastrophal und einer der Hauptgründe für fehlende Demokratisierungsprozesse und fehlende wirtschaftliche Erfolge.

In den ehemaligen englischen Kolonien wie zum Beispiel Tansania, Kenia und Gambia orientiert sich das Schulwesen an dem der früheren Kolonialmacht. Es gibt auch noch die klassische Schuluniform. Eine Schulpflicht wird hier nicht durchgesetzt. Neben einem monatlichen Schulgeld müssen die Schüler auch die Schulmaterialien bezahlen sowie die Lehrkräfte finanziell unterstützen. Kinder aus wohlhabenden Familien gehen zumeist in England zur Schule.

Im subsaharischen Afrika sind seit den 1940er Jahren zahlreiche muslimische Schulen des Madrasa-Typs entstanden. In Nigeria hat das islamische Schulwesen besonders seit den 1970er Jahren rasant an Bedeutung zugenommen.


Die Art der Wissensvermittlung und der Erziehungsauftrag der Schulen standen und stehen immer wieder in der öffentlichen Kritik. Insbesondere im 20. Jahrhundert wurden die traditionellen Schulformen (Volksschule - Realschule - Gymnasium/Berufsschule), aber auch das bestehende Hochschulsystem kritisiert und Änderungen wurden eingefordert. Der Jugendbewegung folgte in der Bundesrepublik Deutschland die Reformpädagogik. In den 1970er Jahren wurden im Zusammenhang mit den gesellschaftlichen Diskussionen über die antiautoritäre Erziehung Alternativschulen wie Haupt- und Gesamtschulen gegründet, z. B. die Glockseeschule in Hannover.

Die allgemeinbildenden Schulen wurden beschuldigt, das Lernen bei Kindern eher zu verhindern als zu fördern, vor allem, indem die Unterrichtssituation Angst erzeuge. Pädagogen wie Francisco Ferrer oder Alexander Sutherland Neill gründeten freiere Schulen (Summerhill), während andere wie John Caldwell Holt im Unschooling eine Alternative sahen (vergleiche auch: Deschooling und Hausunterricht).

Der amerikanische Psychologe David Keirsey zeigt in seinen kritischen Essays, dass vom Schulsystem nur bestimmte Temperamenttypen gefördert, andere hingegen gehemmt werden. Er spricht sich auch deutlich gegen verhaltensändernde Pharmazeutika aus, die besonders im amerikanischen Raum häufig eingesetzt werden.

Der in Lateinamerika tätige Priester Ivan Illich verwies besonders für die Dritte Welt auf das Versagen der Schule ("Schulen helfen nicht", 1972).

Der französische Soziologe Pierre Bourdieu zeigte in seinem Buch "Die feinen Unterschiede" (frz., 1979), wie Schulen zum Erhalt von Klassenstrukturen beitragen.

In Deutschland verbieten zwar die meisten Bundesländer Werbung an Schulen, Sponsoring hingegen ist aber erlaubt. Kritiker bemängeln, dass den Schulleitern und Lehrern die Schulgesetze derzeit nur grobe Vorgaben machen, wie Werbung und Sponsoring in Schulen zu handhaben sei.






</doc>
<doc id="9585" url="https://de.wikipedia.org/wiki?curid=9585" title="Walther Schreiber">
Walther Schreiber

Walther Carl Rudolf Schreiber (* 10. Juni 1884 in Pustleben im Kreis Nordhausen; † 30. Juni 1958 in West-Berlin) war ein deutscher Politiker. Zunächst war er Mitglied der liberalen Deutschen Demokratischen Partei (DDP) und als solches von 1925 bis 1932 preußischer Handelsminister. In den 1950er Jahren war Schreiber als Mitglied der CDU für knapp 15 Monate Regierender Bürgermeister von Berlin.

Als Sohn eines Rittergutsbesitzers besuchte er das Gymnasium in Weimar. In München, Halle, Berlin und Grenoble studierte er Rechts- und Staatswissenschaften und wurde 1910 in Halle zum Dr. jur. promoviert. Von 1911 bis 1925 arbeitete er als Rechtsanwalt und Notar in Halle (Saale). Im Ersten Weltkrieg kriegsfreiwilliger Offizier, wurde er im November 1918 in den Soldatenrat bei der Obersten Heeresleitung gewählt. Als Mitglied der Deutschen Demokratischen Partei war er ab 1919 Mitglied des Preußischen Landtages.

Nach dem Zweiten Weltkrieg wurde Schreiber Christdemokrat und Mitbegründer der CDU am 26. Juni 1945 in Berlin und in der SBZ. Bei der Wahl zum Berliner Abgeordnetenhaus 1950 war er Spitzenkandidat seiner Partei. Während die SPD mit 44,7 % die absolute Mandatsmehrheit nur um zwei Sitze verfehlte, erreichte die CDU 24,6 % der Stimmen und die FDP 23,0 %.

Ernst Reuter, bis dato amtierender Regierender Bürgermeister, war bemüht, die bisherige Drei-Parteien-Koalition aus SPD, CDU und FDP fortzuführen, was zwar auf Zustimmung der CDU stieß, jedoch Schreiber nicht daran hinderte, sich dem Abgeordnetenhaus neben Reuter zur Wahl zu stellen. Nachdem beide je 62 Stimmen erhalten hatten, verzichtete Schreiber zugunsten Reuters auf eine Aufrechterhaltung seiner Kandidatur. Nach Reuters Tod am 29. September 1953 zerbrach die Koalition, die SPD ging in die Opposition und Schreiber wurde Regierender Bürgermeister. Bei der Wahl 1954 steigerte sich die CDU zwar auf 30,4 % der Stimmen, jedoch sackte die FDP auf 12,8 % der Stimmen ab, so dass die 44,6 %, welche die SPD erhielt, zum Gewinn der absoluten Mandatsmehrheit ausreichten. Die SPD ging eine Koalition mit der CDU ein und neuer Regierender Bürgermeister wurde Otto Suhr (SPD).

Schreiber ist auf dem Waldfriedhof Dahlem bestattet. Sein Grab ist als Ehrengrab der Stadt Berlin gewidmet.

Der Walther-Schreiber-Platz an der Grenze zwischen Steglitz und Friedenau wurde 1958 nach ihm benannt. Dort wurde 1971 der U-Bahnhof Walther-Schreiber-Platz eröffnet.




</doc>
<doc id="9587" url="https://de.wikipedia.org/wiki?curid=9587" title="Wiesn">
Wiesn

Mit Wiesn („Wiese“) werden umgangssprachlich und dialektal (meist bairisch) folgende Volksfeste gemeint:


"Wiesn" ist bairisch und bedeutet Wiese. Auf den häufig gesetzten Apostroph "Wies’n" wird dabei nach der gültigen Rechtschreibung verzichtet.



</doc>
<doc id="9588" url="https://de.wikipedia.org/wiki?curid=9588" title="Zeitmaschine">
Zeitmaschine

Eine Zeitmaschine ist eine fiktive Maschine zur Durchführung von Zeitreisen. Derartige Maschinen, die ein beliebiges Vor- und Zurückbewegen in der Zeit erlauben sollen, sind Fiktionen: Zwar sind unter Ausnutzung der Zeitdilatation Zeitreisen in die Zukunft nach heutigem Wissen machbar, doch die Konstruktion einer Maschine, die eine Reise in die "Vergangenheit" vollbringt, ist nach heutigem Kenntnisstand nicht möglich.

Zeitmaschinen sind ein beliebtes Thema der Science-Fiction. Ernsthaft beschäftigt sich die Wissenschaft u. a. auf Grundlage von Albert Einsteins Annahmen einer änderbaren Raumzeit und neuerer Erkenntnisse zu reduzierten spontanen Materiebildungen neben großen Massen lediglich mit der Übermittlung von ‚Informationen‘ in die Vergangenheit. Aber auch diese Experimente erscheinen aufgrund der Gegebenheiten (hoher Energiebedarf, große Dimension) in Zukunft nicht realisierbar.

Zeitmaschinen sind häufig auch Ausgangspunkt für Alternativweltgeschichten. Dadurch, dass ein Zeitreisender die Vergangenheit ändert, entwickelt sich die Geschichte anders als überliefert, und eine parallele Geschichte entsteht.

Eine Liste der Zeitreisen in der Literatur ergibt: 


In "Die Physik des Unmöglichen" zeigt Michio Kaku nach neuesten wissenschaftlichen Kenntnissen realistische Möglichkeiten auf, eine Zeitmaschine praktisch zu realisieren. Demnach wird der Bau einer Zeitmaschine aufgrund des technologischen und finanziellen Aufwandes erst in ferner Zukunft möglich sein.

Im an Laien gerichtetes Buch "Die kürzeste Geschichte der Zeit" von Stephen Hawking und Leonard Mlodinow wird das Thema Zeitmaschine bzw. Zeitreisen und Wurmlöcher in Kapitel zehn thematisiert. Die physikalischen Gesetze scheinen demnach zwar Zeitreisen zuzulassen, diese seien aber unlogisch: Weshalb habe die Menschheit Probleme in der Gegenwart, wenn in der Zukunft irgendwann einmal Zeitreisen möglich wären? Wo seien die Besucher aus der Zukunft?












Nicht mit der Zeitmaschine zu verwechseln ist die Zeitkapsel, ein für eine bestimmte Zeit verschlossener Gegenstand, der erst in ferner Zukunft geöffnet werden darf.



</doc>
<doc id="9589" url="https://de.wikipedia.org/wiki?curid=9589" title="Havel">
Havel

Die Havel [] ist ein Fließgewässer im Nordosten Deutschlands und mit 334 Kilometern Länge der längste rechtsseitige Nebenfluss der Elbe. Die direkte Entfernung zwischen Quelle und Mündung beträgt allerdings nur 94 Kilometer. Die Havel entspringt in Mecklenburg-Vorpommern, durchfließt Brandenburg, Berlin und Sachsen-Anhalt und mündet an der Grenze zwischen Brandenburg und Sachsen-Anhalt in die Elbe. 285 Flusskilometer verlaufen in Brandenburg. Sie fließt zunächst durch zahlreiche kleine Seen in südöstlicher, dann in südlicher, westlicher und schließlich in nordwestlicher Richtung. Dabei beträgt der Höhenunterschied lediglich 40,6 Meter. Mit durchschnittlich 103 Kubikmetern pro Sekunde hat die Havel nach Moldau (150 m³/s) und Saale (117 m³/s) die drittgrößte Wassermenge unter den Nebenflüssen der Elbe. Der größte Teil des Flusslaufs ist schiffbar. Fast im gesamten Verlauf regulieren Wehre und Schleusen die Tiefe und Führung des Wassers. Trotz des Ausbaus zur Wasserstraße hat die Havel dank der zahlreichen natürlichen Seen, durch die sie fließt, ein beachtliches Speichervermögen und hält den Wasserstand auch bei längeren Trockenperioden. Gefährlich hohe Wasserstände sind selten und werden meist im Havelunterlauf vom Elbhochwasser ausgelöst. Etliche Seitenkanäle verkürzen den Wasserweg für die Binnenschifffahrt.

Größter Nebenfluss der Havel ist die Spree, die an ihrer Mündung mehr als doppelt so viel Wasser wie die Havel führt (38 m³/s gegenüber 15 m³/s) und diese zudem auch in der Länge übertrifft (380 km).

2004 wurde die Havel von den Naturfreunden Deutschlands und dem Deutschen Anglerverband zur "Flusslandschaft des Jahres" gekürt. Der Fluss wird seit 2009 durch den Havelradweg begleitet.

Der Name der Havel (sorbisch: "Habola") soll noch aus der vorslawischen, germanischen Besiedlungsphase stammen und somit der ältesten Schicht von Territorialbezeichnungen auf dem Gebiet des Landes Brandenburg und der Stadt Berlin zugehören, abgeleitet vom germanischen "Habula". Er ist mit "Hafen" und "Haff" etymologisch verwandt. Der Wortstamm "Haf" bezeichnet eine Bucht oder Ausbuchtung. Auch irdene, bauchige Keramik wurde im deutschen Sprachraum zu früheren Zeiten oft als „Hafen“ bezeichnet. Dieser Interpretation zufolge deckt sich die Namensgebung mit der Gestalt der Havel als buchtenreichem Fluss.

Die Havel entspringt in der Mecklenburgischen Seenplatte. Als Quellgebiet gilt heute das Diekenbruch bei Ankershagen im Nordosten des Müritz-Nationalparks. Der historische Quellsee der Havel ist allerdings der Bornsee, der in den Trinnensee und weiter in den Mühlensee entwässert. Ab dem Mühlensee ist der historische Verlauf der Havel nach Süden durch das Diekenbruch. Im 14. oder 15. Jahrhundert schütteten vermutlich Mönche zwischen Mühlensee und Diekenbruch einen Damm auf, und leiteten das Havelwasser durch einen künstlichen Durchstich in den Mühlenbach, um eine Mühle zuverlässiger mit Wasser versorgen zu können. Dadurch wurde die Wasserscheide zwischen Nord- und Ostsee künstlich leicht nach Süden verschoben und wird im Bereich der Havelquelle nicht mehr durch die Pommersche Hauptendmoräne definiert, sondern offiziell durch den Damm am Diekenbruch. Der auf moorigem Untergrund aufgeschüttete Damm dichtet jedoch den Mühlensee nicht vollständig gegen das Diekenbruch ab, so dass die historische Havelquelle nach wie vor geringfügig in das Flusssystem der Elbe entwässert. Seit 2007 wird ein Teil des Sickerwassers in einem kleinen, künstlich gefassten Quelltopf an die Oberfläche geführt und als Havelquelle definiert.

Die ersten von der Havel durchflossenen Seen sind der Dambecker- und der Röthsee. Hier ist die Havel noch ein grabenähnliches Fließ. Ab dem Käbelicksee bei Kratzeburg hat sie bereits eine Breite von drei bis vier Metern und ist ein beliebtes Paddelgewässer. Zwischen dem folgenden Granziner- und dem Pagelsee ist die Havel ein in eine schmale Rinne gezwängter Wildwasserbach. Das Befahren des Bachlaufes ist in diesem Abschnitt nicht gestattet. Für den Landtransport der Boote steht eine Lorenverbindung zur Verfügung. Dem Pagelsee folgen der Zotzensee, der Jäthensee und der Useriner See. Der Flusslauf der Havel zwischen diesen Seen ist bereits teilweise kanalartig ausgebaut. Ab dem Useriner See, 12 km von ihrer Quelle entfernt, ist die Havel ein schiffbarer Fluss von bis zu 10 m Breite. Am Großen Labussee befindet sich die erste Schleuse der Havel, die Schleuse Zwenzow. Ab hier darf die Havel mit motorbetriebenen Booten befahren werden. Es folgt die Quassower Havel und der Woblitzsee bei Wesenberg. Über den in den Woblitzsee von Norden einmündenden Kammerkanal und den Zierker See ist die Havel mit Neustrelitz, der ehemaligen Landeshauptstadt von Mecklenburg-Strelitz, verbunden. Vom Woblitzsee an ist die Havel, einschließlich des Kammerkanals, mit mehreren Staustufen als Obere Havel-Wasserstraße ausgebaut. In Mecklenburg streift sie noch den Drewensee und den Wangnitzsee und fließt durch den Kleinen und Großen Priepertsee sowie den Ellbogensee und den Ziernsee. Bereits im Land Brandenburg streift sie den Menowsee und durchfließt den Röblinsee und Baalensee.

Am Ostrand der Stadt Fürstenberg streift die Havel den Schwedtsee und erreicht hinter Fürstenberg den Stolpsee bei der Klosterruine Himmelpfort. 15 km südlich davon, nördlich von Zehdenick, wurde in ihrer Niederung früher viel Ton abgebaut (jetzt touristischer Ziegeleipark). Von dort bis nach Berlin-Spandau durchfließt die Havel den Naturraum der Zehdenick-Spandauer Havelniederung. Zum Transport von Brennmaterial und Ziegeln wurde ab Zehdenick südwärts nach Liebenwalde 1880–1882 der Voßkanal gebaut, neben dem die alte Havel unter dem Namen Schnelle Havel als stark mäandrierendes und nicht schiffbares Flüsschen erhalten geblieben ist. So fließt der Hauptanteil des Havelwassers ab Zehdenick durch den Voßkanal und Malzer Kanal und speist seit 1914 die Scheitelhaltung des Oder-Havel-Kanals; über die Kanalstufe Lehnitz fließt das Havelwasser südlich Oranienburg wieder dem Havelbett zu. Unmittelbar südlich von Liebenwalde zweigt der Kanal „Langer Trödel“, ein Abschnitt des Finowkanals, eines Vorgängerbaues des Oder-Havel-Kanals, vom Voßkanal nach Osten in Richtung Oder ab. Der Malzer Kanal, die Verlängerung des Voßkanals nach Süden, trifft südlich von Liebenwalde auf den Oder-Havel-Kanal. Der Malzer Kanal ist hier das nördliche Reststück eines Kanals von Malz her, damals als Verbindung zum „Langen Trödel“; der Malzer Kanal ist zum größeren Teil 1914 in den Oder-Havel-Kanal aufgegangen.

Die Schnelle Havel fließt nach Verlassen des Eberswalder Urstromtales parallel zum Oder-Havel-Kanal bis zur Einmündung des südlichen Reststücks des Malzer Kanals am Nordrand von Friedrichsthal. Von hier bis zur ehemaligen Schleuse Sachsenhausen wird sie als Friedrichsthaler Havel bezeichnet, anschließend als Oranienburger Havel bis Hohen Neuendorf, dann als Spandauer Havel bis zur Spreemündung. Hinter Hennigsdorf, am Nieder Neuendorfer See, zweigt von der Havel der Havelkanal nach Westen ab, 1951/52 von der DDR zur Umschiffung West-Berlins angelegt; er mündet zusammen mit dem Sacrow-Paretzer Kanal bei Paretz wieder in die Havel ein.
Von der Nordwestecke Berlins durch Potsdam und bis unterhalb der Stadt Brandenburg fließt die Havel durch eine lange Kette teilweise recht großer Seen. Weitere Seen haben hier durch kleinere oder künstliche Gewässer Verbindung zum Fluss. Bei Potsdam sind die Seen an vielen Stellen von bewaldeten Moränenhügeln umgeben. Bei Brandenburg liegen sie überwiegend in einer flachen Niederung, in der es nur inselhaft Moränenhügel gibt. Zwischen Hennigsdorf und Spandau sind das der Niederneuendorfer, Tegeler und Spandauer See. Gegenüber der Insel Großer Wall zweigt der Berlin-Spandauer Schifffahrtskanal in südöstlicher Richtung ab. Unterhalb der Schleuse Spandau gegenüber der Spandauer Altstadt mündet die Spree in die Havel. Von hier an bildet die Havel bis zu ihrer Mündung in die Elbe zusammen mit einigen Kanälen die Untere Havel-Wasserstraße. Kurz nach der Mündung der Spree passiert die Havel die Tiefwerder Wiesen, das letzte natürliche Berliner Überschwemmungs- und Hechtlaichgebiet. Dann kommen der Große Wannsee, die Pfaueninsel und der Jungfernsee, aus dem nach Westen der Sacrow-Paretzer Kanal abzweigt. Zwischen Wannsee und Tiefem See gibt es noch eine kleine Seenkette östlich des Glienicker Forstes. Durch deren bekanntestes Glied, den Griebnitzsee, geht der Teltowkanal, der im Südosten Berlins Verbindung zur Dahme und über den Britzer Verbindungskanal zur Spree hat.

Ab dem Jungfernsee bis zum Göttinsee wird ein etwa 29 Kilometer langer Flussabschnitt Potsdamer Havel genannt. Sie wird vom Sacrow-Paretzer Kanal kurzgeschlossen und daher nur noch von einem Drittel bis der Hälfte des Havelwassers durchflossen.

Am Göttinsee münden in die Havel ein: der Havelkanal und der Sacrow-Paretzer Kanal, durch den der größte Teil der Frachtschifffahrt den großen Potsdamer Havelbogen, die Potsdamer Havel abkürzt. Dann fließt die Havel in Mäandern westwärts. Dabei verästelt sie sich sowohl zwischen Ketzin/Havel und dem Trebelsee als auch in Brandenburg, dessen Dom auf einer Insel steht. Gut ein Kilometer vor der Vorstadtschleuse Brandenburg zweigt der Brandenburger Stadtkanal als Zufahrt zur Stadtschleuse in südwestlicher Richtung ab und mündet nach 4,1 Kilometern in die Brandenburger Niederhavel ein. Der Brandenburger Stadtkanal ist auch der Zulauf zu den innerstädtischen Regulierungsbauwerken der Staustufe Brandenburg. Nach rechts zweigen von ihm ab: die Stimmingsarche mit Wehr zum Kleinen Beetzsee und mit dem Nebenarm Krakauer Havel, es folgen die drei ehemaligen Mühlenarme Altstädtischer Streng, Domstreng und Neustädtischer Streng (auch Näthewinde) mit Durchlässen zur Brandenburger Niederhavel. Kurz unterhalb der Vorstadtschleuse mündet von Norden her der Beetzsee ein; an gleicher Stelle zweigt nach Westen der Silokanal ab und fließt nach Südwesten die hier als Brandenburger Niederhavel bezeichnete Havel weiter und mündet nach etwa sieben Kilometern in den Breitlingsee. Zwischen Brandenburg und seinem Ortsteil Plaue an der Havel durchfließt die Havel den Plauer See (Brandenburg), der zusammen mit dem Quenz-, dem Breitling- und dem Möserschen See einen einzigen großen buchten- und inselreichen See ergibt. Vom Plauer See zweigt der Elbe-Havel-Kanal ab, der durch den Großen Wendsee in Richtung Magdeburg verläuft.

Vom Plauer See fließt die Havel in mal mehr nördlicher, mal mehr westlicher Richtung am Pritzerber See und an den Städten Premnitz und Rathenow entlang in Richtung Havelberg. Bis zur Grenze Sachsen-Anhalts gehört hier der Flusslauf zum Naturpark Westhavelland, dem größten Naturpark im Land Brandenburg, der sich vom Beetzsee über den Rhin erstreckt und zum Naturschutzgebiet Untere Havel Süd, im Süden mehr von sandigen Hügeln („Ländchen“), im Norden mehr von feuchten Niederungen („Luchen“) geprägt.

Nördlich von Rathenow liegen knapp östlich des Flusses Hohennauener See und Gülper See, beide mit Verbindung zur Havel. Der Verlauf der Landesgrenze deutet hier an, wie gewunden der Flusslauf einst war. Nach der Einmündung von Rhin (in die Gülper Havel) und Dosse erreicht die Havel in Sachsen-Anhalt die Stadt Havelberg, die am Rand der Elbaue liegt.

Da die Untere Havel ein geringeres Gefälle hat als die Elbe, wurde Anfang des 20. Jahrhunderts der Gnevsdorfer Vorfluter angelegt. Dieser Kanal ermöglicht es, das Wasser der Havel erst zehn Kilometer flussabwärts der natürlichen Mündung in die Elbe zu leiten. Dadurch liegt jetzt bei Havelberg der mittlere Wasserspiegel der Havel unter demjenigen der Elbe. Ein Schleusenkanal ermöglicht der Schifffahrt, diesen Höhenunterschied zu überwinden. Bei bedrohlichen Elbhochwassern kann die Havelniederung als Polder zum Hochwasserschutz für die Elbe geflutet werden.

Die Havelpolder sind sechs räumlich getrennte Überflutungsflächen vor der Mündung der Havel in die Elbe. Im Falle eines Havelhochwassers können diese geflutet werden, um so Ortschaften am nicht eingedeichten Havelunterlauf zu schützen, insbesondere auch wenn die Wehre zur Elbe geschlossen sind.

Am Sonntag, 9. Juni 2013 ab 13 Uhr wurde das bereits überspülte Wehr zur Elbe bei Neuwerben stückweise geöffnet, so dass wie bereits im Jahr 2002 Elbwasser die Havel hinauf in die Havelpolder flutete.
Hierdurch soll der Elbpegel flussabwärts um 30 cm sinken, was der Pegel in Wittenberge auch zeigt.
Die Polder unterliegen der Wasserbehörde von Stendal/Sachsen-Anhalt.

Die geschützte Untere Havelniederung bildet zusammen mit den angrenzenden Luchlandschaften Rhinluch, Havelländisches Luch, Dossebruch und Jäglitzniederung das größte zusammenhängende Binnen-Feuchtgebiet des westlichen Mitteleuropas.

Basierend auf dieser Naturnähe wurde 2005 damit begonnen, die Renaturierung der Havel in die Wege zu leiten. Akteure sind das Bundesamt für Naturschutz (BfN), die Länder Brandenburg und Sachsen-Anhalt sowie der Naturschutzbund (NABU). Innerhalb der nächsten Jahrzehnte soll das Gewässer die ursprüngliche Gestalt (weitläufige Auen, kurvigerer Flussverlauf, weitgehender Verzicht auf Uferbefestigungen) wiedererhalten. Profitieren werden seltene Vogelarten, aber auch Fischotter, Biber und Flussneunaugen.

Geschichte und wirtschaftliche Entwicklung Brandenburgs sind eng mit der Havel verknüpft. Zur Zeit der slawischen Besiedlung bildeten Havel und Nuthe die Grenze zwischen den Hevellern, die in der Zauche und im Havelland siedelten, und den Sprewanen auf östlicher Seite. Wo Fernhandelswege die Havel kreuzten, hatten die Slawen Burgen und solide Holzbrücken, in Plaue (heute Ortsteil von Brandenburg), Brandenburg (Dominsel), Potsdam (die Lange Brücke) und Spandau (Funde am Burgwall).

Der deutschen Besiedlung des Havellandes gingen mehr als zwei Jahrhunderte kriegerischer Auseinandersetzungen voraus: 928/929 eroberte der deutsche König Heinrich I. Brennabor und besiegte die Heveller. Sein Sohn Otto I. der Große (röm. Kaiser ab 962) gründete 937 die Nordmark. Mit dem Großen Slawenaufstand von 983 wurde das Land östlich der Elbe zunächst wieder unabhängig. Der Askanier Albrecht der Bär begann 1134 mit der erneuten Eroberung, ab 1157 als Markgraf. Nach seinem Sieg über die Heveller in Spandau 1157 begann die Einwanderung von Deutschen in die Mark Brandenburg. An der Havelbucht Jürgenlanke erinnert das Schildhorn-Denkmal an die Schildhornsage und den Sieg Albrechts.

Mit der mittelalterlichen Kolonisation nahm die Bedeutung der Havel als Transportweg zu, da die Region zunehmend Massengüter wie Getreide und Holz exportierte. In dieser Blütezeit von Handel und Gewerbe entstand aber ein Nutzungskonflikt zwischen Transport und Energiegewinnung. Für den Betrieb von Mühlen wurden in Spandau, Brandenburg und Rathenow Staudämme angelegt, welche die Schifffahrt mittels Umflutkanälen und einfachen Schleusen passieren musste. Der Mühlenstau in Brandenburg hob den Wasserspiegel flussaufwärts bis nach Spandau. Möglicherweise verwandelten diese Mühlenstaus älteres, slawisches Kulturland wieder in Feuchtbiotope. Ein bedeutender Wirtschaftsfaktor der Region war lange Zeit die Fischerei.

Der Dreißigjährige Krieg reduzierte die Einwohnerzahl von Städten wie Potsdam und Rathenow auf einen Bruchteil ihrer ursprünglichen Bevölkerung.

Vom 18. Jahrhundert bis in die DDR-Zeit wurden systematisch Feuchtgebiete trockengelegt. Mit dem Aufstieg Brandenburg-Preußens zur führenden Macht in Deutschland und der preußischen Könige zu deutschen Kaisern wurde das Havelland zur Hauptstadtregion. Die Sommerresidenz Potsdam wurde zum „preußischen Versailles“, umgeben von Schlossparks, Mustergütern und herrschaftlichen Jagden. Sogar in kleinen Dörfern traten neben die einfachen Bauernkaten stuckverzierte Wohnhäuser und die eine oder andere „byzantinische“ Dorfkirche (beispielsweise die Heilandskirche am Port von Sacrow). Der Bedarf der Hauptstadt ließ die Gegend um Werder und Ketzin/Havel zum Obstbaugebiet werden. Und nördlich von Zehdenick an der oberen und bei Deetz, Ketzin und Glindow der mittleren Havel entstanden große Tongruben und Ziegeleien.

Als Bundeswasserstraße ist die Havel einbezogen in die Obere Havel-Wasserstraße vom Useriner See (Schleuse Zwenzow) bis Zehdenick. Von Zehdenick bis Oranienburg fließt Havelwasser durch die künstlichen Gewässerbetten der Oberen Havel-Wasserstraße, Voßkanal und Malzer Kanal, und weiter durch den Oder-Havel-Kanal bis wieder zum natürlichen Gewässerbett der Havel südlich Oranienburg. Von dort bis zur Spreemündung bilden der untere Teil der ausgebauten Oranienburger Havel und ab Hohen Neuendorf die Spandauer Havel bis zur Spreemündung Teilstrecken der Havel-Oder-Wasserstraße. Zur nicht mehr durchgehend befahrbaren Alten Havel-Oder-Wasserstraße gehören die Friedrichsthaler Havel und der obere Teil der Oranienburger Havel. Von der Spreemündung bis zur Mündung der Havel in die Elbe bildet die Havel die Untere Havel-Wasserstraße, einschließlich Sacrow-Paretzer Kanal und Silokanal.

Das „Verkehrsprojekte Deutsche Einheit Nr. 17“ sah auf der Strecke Wolfsburg-Magdeburg-Berlin den Ausbau von 280 km Wasserweg vor. Die gesamte betroffene Strecke sollte auf 4 m Tiefe und je nach Uferprofil auf 42–55 m (in Kurven bis zu 72 m) Wasserspiegelbreite ausgebaggert werden. Damit sollte ermöglicht werden, dass hier Schubverbände von 180 m Länge, 11,40 m Breite und 2,80 m Abladetiefe, entsprechend Binnenwasserstraßenklassifizierung Vb, fahren können. Die Havel würde von diesen Ausbaumaßnahmen in ihrem mittleren Bereich von der Einmündung des Havelkanals und des Sacrow-Paretzer Kanals bei Paretz bis zum Silokanal in Brandenburg an der Havel betroffen sein. Gegenwärtig sind entsprechend der Binnenschifffahrtsstraßen-Ordnung, Kapitel 22 "Untere Havel-Wasserstraße und Havelkanal", für den Bereich von km 20,00 bis km 66,70 (ab der Spreemündung) Größen der Fahrzeuge und Verbände (Schubverbände und gekuppelte Fahrzeuge) von 125 m Länge und 9 m Breite bzw. 156 m Länge und 8,25 m Breite zugelassen.

Die Havel wurde für 2004/05 als dritter Fluss zu Deutschlands Flusslandschaft des Jahres gewählt. Die Preisverleihung sollte unter anderem auch als Protest gegen den geplanten Havelausbau verstanden werden, der die Flusslandschaft erheblich beschädigen würde. Seit 1992 setzt sich ein "Aktionsbündnis gegen den Havelausbau" aus über 30 Organisationen für den Erhalt der natürlichen Vielfalt und Schönheit des Flusses ein.

Von der Quelle flussabwärts:

Von der Quelle flussabwärts:
Im Verlauf der Havel und ihrer Seitenarme wurden im Laufe der Jahrhunderte verschiedene Staustufen installiert. Im Mittelalter geschah dies meist in Städten und mit der Absicht, die Wasserkraft für das Antreiben von Mühlen nutzbar zu machen. Beispiele hierfür sind die Staustufen in Oranienburg, Brandenburg und Rathenow. Diese Staustufen führten zu Konflikten mit der Schifffahrt, deren Fahrwege somit verlegt wurden. Die Schifffahrt stellte zu jener Zeit jedoch die einzige Möglichkeit dar, große Mengen Massengüter über weitere Entfernungen zu transportieren. Die Havel war bereits eine wichtige Wasserstraße. Der Interessenkonflikt wurde gelöst, indem man die ersten Schleusen im Verlauf der Havel errichtete.

In späterer Zeit wurden weitere Staustufen angelegt. Anders als die mittelalterlichen Mühlenstaue sollten hierbei Wehre einzelne Abschnitte der Havel durch Aufstauen überhaupt erst schiffbar machen. Dies betraf vor allem den Bereich der Oberen Havel, die dort im natürlichen Verlauf ein zu schmales und flaches Bett aufwies. Die Mehrzahl der Havelwehre unterhalb der Stadt Brandenburg wurde im Zuge von Maßnahmen zur Verbesserung des Hochwasserschutzes errichtet.

Die angelegten Staustufen wurden und werden mit unterschiedlichen Schleusenbauwerken überwunden. Teilweise wurden Großbauwerke mit über hundert Meter langen Schleppzuschleusen und unweit dieser wenige Meter lange und durch Hand zu betreibende Kahnschleusen errichtet. In den letzten Jahren ist man dazu übergegangen, einzelne Schleusen per aus der Ferne mittels Funksignalen zu bedienen.

Bereits seit dem Mittelalter sind Fährverbindungen über die Havel bekannt. Der Großteil wurde später durch Brückenbauwerke abgelöst. Aufgrund der vielerorts seeartigen Breite der Havel, der sumpfigen Uferbereiche, aus volkswirtschaftlichen Überlegungen, aus Naturschutz- oder Tourismusgründen blieben jedoch einige Fährverbindungen bis in die heutige Zeit bestehen. So existieren noch heute im Bereich der Unteren Havel innerhalb Berlins die Verbindungen F 10 über den Großen Wannsee und eine Fähre zur Pfaueninsel.

Erstere wird durch die Berliner Verkehrsbetriebe betrieben und wird mit der fahrgastschiffähnlichen "Wannsee" befahren. Die Verbindung zur Pfaueninsel wird mit einer Wagenfähre betrieben, die mehrere Autos und 150 Personen transportieren kann. Die Wagenfähre zur Pfaueninsel ist eine frei fahrende Fähre.
Außerdem quert eine Autofähre die Havel zwischen Spandau-Hakenfelde und dem Reinickendorfer Ortsteil Tegelort. Im Land Brandenburg gibt es noch weitere Fährverbindungen wie in Potsdam zwischen dem Ufer "Auf dem Kiewitt" und Hermannswerder als reine Personenfähre mit Fahrradmitnahme. Diese wird durch den Verkehrsbetrieb Potsdam betrieben. Wagenfähren verkehren noch in Caputh (Tussy II), in Ketzin (Charlotte), in der Stadt Brandenburg (Fähre Neuendorf) und in Havelsee (Fähre Pritzerbe).

Die Fischerei hat auf der Havel eine jahrtausendelange Tradition. Bei Pritzerbe wurden beispielsweise jungpaläolithische bis mesolithische knöcherne Angelhaken gefunden. Auch im 21. Jahrhundert wird die Havel über weite Strecken intensiv kommerziell befischt. So gibt es beispielsweise in den Orten Brandenburg an der Havel, Plaue, Briest und Pritzerbe noch mehrere Berufsfischer. Der gefangene Havelfisch, wie beispielsweise Zander und Europäischer Aal, wird zubereitet oder unzubereitet frei verkauft und gehört zur traditionellen Brandenburger Küche.

Im Januar 2012 erhielt ein bis dahin unbenanntes Tal auf dem Mars den Namen „Havel Vallis“.
Anlässlich der BUGA 2015 komponierte Marian Lux eine Sinfonie für die Havel. Sie wurde am 13. Juni 2015 im Landgut Borsig in Groß Behnitz im Rahmen der Havelländischen Musikfestspiele uraufgeführt.





</doc>
<doc id="9590" url="https://de.wikipedia.org/wiki?curid=9590" title="Zeitreise">
Zeitreise

Als Zeitreise bezeichnet man in der Physik und der Science-Fiction eine Bewegung in der Zeit, die vom gewöhnlichen gerichteten Zeitablauf abweicht, bzw. auch eine Bewegung durch die Zeit. Mittels der Relativitätstheorie sind Szenarien beschreibbar, in denen durch den Effekt der Zeitdilatation „Reisen“ in die Zukunft stattfinden. Dass hingegen auch Reisen in die Vergangenheit, wie sie in vielen Werken der Science-Fiction beschrieben werden, überhaupt physikalisch, logisch oder metaphysisch möglich seien, wird vielfach bezweifelt und es gibt dafür keine empirische Evidenz.

Die Relativitätstheorie Albert Einsteins bietet verschiedene Möglichkeiten für Zeitreisen:

Verlässt man mit einem fast lichtschnellen Raumschiff (es reichen u. U. auch relativistische Geschwindigkeiten um 10 % der Lichtgeschwindigkeit) die Erde und kehrt nach Ablauf einer bestimmten Reisedauer wieder zurück, ist auf der Erde ein längerer Zeitraum verstrichen als an Bord des Raumschiffes. Die Ursache dafür ist die Zeitdilatation, die nach der speziellen Relativitätstheorie von Albert Einstein bei derartig hohen Geschwindigkeiten auftritt. Der genaue Ablauf einer solchen Zeitreise ist unter Zwillingsparadoxon beschrieben.

Bei hinreichend großer Reisegeschwindigkeit und Beschleunigung wäre dabei im Prinzip in beliebig kurzer Reisedauer für den Reisenden eine beliebig ferne Zukunft auf der Erde erreichbar. Bei einer dem Menschen zumutbaren Beschleunigung erfordert jedoch eine Zeitverschiebung von Jahren auch eine Reisezeit aus der Sicht der Raumschiffbesatzung von über einem Jahr (jeweils rund 347 Tage für Beschleunigung und Abbremsen mit 9,81 m/s²).

Für ein Objekt, das sich mit Vakuumlichtgeschwindigkeit bewegt, würde die Zeit stillstehen. Ein Photon, das sich im Vakuum bewegt, kann also theoretisch die Zeitspanne vom Beginn bis zum Ende des Universums „in einem Augenblick“ durchmessen.

Nach der allgemeinen Relativitätstheorie ist der Lauf der Zeit auch von den Gravitations- und Beschleunigungsbedingungen abhängig, denen ein System unterworfen ist. So vergeht die Zeit etwa auf einem hohen Berg geringfügig schneller als auf Meereshöhe. Dieses Phänomen ließe sich als Zeitreise in die Zukunft interpretieren, wobei nicht nur eine raschere, sondern auch eine gebremste Reise möglich ist.

Auf einem Neutronenstern kann die gravitative Zeitdilatation erheblich sein. So könnte ein hypothetischer Bewohner eines Neutronensterns eine zeitaufwändige Aufgabe in einer Umlaufbahn um den Stern erledigen, um einen Termin auf der Sternoberfläche leichter einhalten zu können. Im noch extremeren Gravitationspotential nahe dem Schwarzschildradius Schwarzer Löcher kann sich die Zeit gemäß der Allgemeinen Relativitätstheorie beliebig dehnen.

Nach derzeitigem Stand der Wissenschaft sind Zeitreisen in die Vergangenheit prinzipiell nicht möglich. Bestehende Theorien, nach denen eine solche Reise möglich sei, sind spekulativ und umstritten. Unbestritten ist jedenfalls, dass die "praktische" Umsetzung derartiger Theorien in absehbarer Zeit unmöglich ist.

1949 entdeckte Kurt Gödel, dass eine Lösung der Allgemeinen Relativitätstheorie, bei der das Universum rotiert, das Zurückkehren eines Objekts in seine eigene Vergangenheit ermöglicht. Ein solches Universum wird als Gödeluniversum (R-Universum) bezeichnet.

Auch wenn bewiesen werden kann, dass unser Universum nicht rotiert, zeigt das R-Universum, dass die Einsteinschen Feldgleichungen ein Universum mit geschlossenen zeitartigen Kurven zulassen. Folglich gehört das gleichförmige Vergehen der Zeit nicht zu den von vorneherein notwendigen Eigenschaften eines Universums, das den Gleichungen genügt.

Nach der allgemeinen Relativitätstheorie ist es denkbar, dass zwei verschiedene Bereiche der Raumzeit über sogenannte Wurmlöcher miteinander verbunden sein könnten. Wenn die beiden Ausgänge eines solchen Wurmloches zwei Bereiche unterschiedlicher Zeit verbinden würden, wäre eine Zeitreise auch in die Vergangenheit möglich. Allerdings zeigen Rechnungen, dass Wurmlöcher normalerweise nicht stabil sind und so schnell zusammenbrechen, dass eine Passage nicht möglich ist. Hätte man eine hypothetische Materie mit negativer Energiedichte zur Verfügung, die sogenannte exotische Materie, so könnte man damit ein Wurmloch stabilisieren. Die dazu erforderliche Menge an exotischer Materie steht aber nach derzeitigem Wissensstand im gesamten derzeit bekannten Universum nicht zur Verfügung.

Eventuell wäre auf einer speziellen Flugbahn in der Umgebung eines hinreichend schnell rotierenden Schwarzen Loches eine Reise in die eigene Vergangenheit möglich. Man nimmt jedoch an, dass es keine derart schnell rotierenden Schwarzen Löcher gibt.

Eine Zeitreise in die Vergangenheit wäre auch in der Umgebung zweier kosmischer Strings möglich, die hinreichend schnell aneinander vorbeifliegen. Es existieren jedoch keine Belege für die Existenz kosmischer Strings.

Nach einer sehr umstrittenen Interpretation wurde durch Superluminares Tunneln für Teilchen beziehungsweise Photonen eine Zeitumkehr erreicht, das heißt, aus der experimentellen Anordnung – dem „Tunnel“ – schienen die Teilchen herauszukommen, bevor sie hineingestrahlt wurden. Jedoch wurde bei diesen Experimenten eine sehr umstrittene Definition des Signalzeitpunktes verwendet.

Sollten Reisen in die Vergangenheit möglich sein, würde sich die Frage stellen, wie die Paradoxa vermieden werden, die sich in diesem Zusammenhang aus der Verletzung der Kausalität ergeben können, wie beispielsweise das Großvaterparadoxon. Als mögliche Antwort käme vor allem die Everettsche Vielwelten-Theorie in Frage. Danach wäre die Vergangenheit, in die man reist, in einer Parallelwelt angesiedelt. Der ursprüngliche Ablauf der Dinge und ein durch einen Eingriff in die Vergangenheit modifizierter Ablauf würden sich beide abspielen. Insbesondere wäre es für den Reisenden unmöglich, wieder in seine ursprüngliche Version der Gegenwart zurückzukehren, wohl aber in eine Parallelwelt, die dieser nahezu identisch wäre.

Gelegentlich werden, wenn von Reisen in die Vergangenheit die Rede ist, hypothetische überlichtschnelle Teilchen, sogenannte Tachyonen, ins Spiel gebracht. Könnte sich ein Teilchen mit Überlichtgeschwindigkeit von A nach B bewegen, so ließe sich immer ein Beobachter finden, für den die Bewegung von B nach A stattfände. Da die Beobachter die zeitliche Reihenfolge der Ereignisse A und B unterschiedlich beurteilen, bewegt sich für alle Beteiligten das Tachyon von der Vergangenheit in die Zukunft. Aus einer hypothetisch überlichtschnellen Bewegung die Möglichkeit einer Reise in die Vergangenheit abzuleiten, ist nicht möglich.

Zeitreisen sind ein alter Menschheitstraum und werden daher oft in Science-Fiction-Literatur und im Science-Fiction-Film thematisiert. Meist erfolgt dort eine Zeitreise mittels einer Zeitmaschine (so etwa in "Zurück in die Zukunft"), seltener mittels anderer Methoden, beispielsweise in der "Highland-Saga" von Diana Gabaldon per Gang durch einen Steinkreis oder aber durch Zauberei, wie in der Serie "Charmed – Zauberhafte Hexen". Anders aber bei der Serie "Zurück in die Vergangenheit", wo Dr. Samuel „Sam“ Beckett (gespielt von Scott Bakula), ein mit dem Nobelpreis ausgezeichneter Quantenphysiker und Erfinder des „Quantenbeschleunigers“, infolge eines gescheiterten Selbstversuchs unkontrolliert innerhalb seiner eigenen Lebenszeit durch die Zeit reist und dabei temporär die Identität verschiedener Menschen dieser Zeit annimmt und mit ihnen den Körper tauscht. In der Science-Fiction-Fernsehserie "Star Trek" ist die Zeitreise ein wiederkehrendes, beliebtes und sowohl methodologisch als auch narrativ variiertes Thema.

In der Mangareihe Dragonball wird die Thematik in der Cell-Saga ebenfalls thematisiert, da ein Besucher aus der Zukunft die Gegenwart ändert. Hier wird jedoch absichtlich ein neuer Zeitstrang geschaffen.

Insbesondere sind die Probleme im Zusammenhang mit der Kausalität bei Zeitreisen in die Vergangenheit ein beliebtes Thema der Science-Fiction-Literatur, etwa in den Romanen "Der Schutzengel" von Dean Koontz oder "Geschichte machen" von Stephen Fry.

Dabei wird gerne der Schmetterlingseffekt aus der Chaostheorie thematisiert. Das bedeutet, dass selbst kleinste Veränderungen in der Vergangenheit extreme Auswirkungen auf die weitere Entwicklung haben können (siehe z. B. "Butterfly Effect" oder "Zurück in die Zukunft"), die unter Umständen mit der Nicht-Existenz des Zeitreisenden enden. Gelöst wird dieses Problem nicht selten mit der überraschenden Pointe, dass die Zeitreise von Beginn an Teil der bekannten Vergangenheit war und durch die Zeitreise ein Kreis geschlossen wird. Dadurch wird wiederum die Annahme in Abrede gestellt, durch eine Zeitreise könne man die „Zeitlinie“ verändern oder gar zerstören.

Eine andere wiederkehrende Idee ist die der Zeitschleife, in welcher der Protagonist festsitzt. Das ist zum Beispiel in den Filmen "", "Retroactive" oder "Und täglich grüßt das Murmeltier", in "Supernatural" Staffel 3 Folge 11 („Und täglich grüßt …“) und der Animationsserie "Steins;Gate" der Fall.

In der britischen BBC-Serie Doctor Who geht es um einen mysteriösen Zeitreisenden, der nur als ‚der Doktor‘ bekannt ist. Er reist mit seinen Begleitern in der TARDIS, die von außen wie eine alte Polizei-Notrufzelle aussieht, durch Zeit und Raum und wird dabei in verschiedene Abenteuer verwickelt.

Erdoğan Ercivan behauptet in seinen Büchern, dass das Ägyptische Unterweltsbuch "Amduat" eine exakte Reisebeschreibung des verstorbenen Pharao zum Doppelstern Sirius enthält, der sich 8,6 Lichtjahre von der Erde entfernt befindet. Unter anderem sollen die Götter der Ägypter ein Wurmloch südwestlich vom Orion verwendet haben, das sie "Wamemti" nannten.

Michael Crichton verwendet in seinem Buch "Timeline" die These, dass unterschiedliche Zeiten nur unterschiedliche Welten in der Viele-Welten-Interpretation sind. Eine Zeitreise erfolgt daher durch die Reise in eine Parallelwelt.

Eine ähnliche Interpretation findet sich in den Romanen "Der letzte Tag der Schöpfung" und "Das Cusanus-Spiel" von Wolfgang Jeschke, in denen Menschen in die Vergangenheit reisen und dadurch parallele, voneinander abweichende Zukünfte schaffen, aus denen verschiedene weitere Personen in die Vergangenheit reisen.

Das Problem der grammatikalischen Tempusbildung für Zeitreisende wird von Douglas Adams in seinem Roman "Das Restaurant am Ende des Universums" thematisiert.

Die Beschreibung einer Zeitreise in die Zukunft bietet dem Autor die Möglichkeit, Fehlentwicklungen und Gesellschaftskritik zugespitzt darzustellen. Davon hat unter anderem H. G. Wells 1895 in seinem Buch "Die Zeitmaschine" Gebrauch gemacht.

Eng damit verwandt ist das Thema der Parallelwelten, wie etwa im Film "Ist das Leben nicht schön?" oder in der Serie "Sliders".

In dem Roman von Audrey Niffenegger "Die Frau des Zeitreisenden" muss die Hauptfigur (Henry) wegen eines Gendefekts durch die Zeit reisen.

Im Roman "Das Jesus Video" von Andreas Eschbach reist ein unfreiwillig Zeitreisender zum Schauplatz der letzten Lebenswochen Jesu Christi und sorgt ebenfalls unfreiwillig durch ein in die Gegenwart überliefertes Video für Dramen ob der tatsächlichen Begebenheiten und Berichterstattungen in der Bibel über das Leben Jesu. Mit dem 2014 erschienenen Buch Der Jesus-Deal wird die Geschichte fortgesetzt und im evangelikalen Milieu der USA angesiedelt.

In der Serie "Unterwegs nach Atlantis", nach Johanna von Koczian und Ota Hofman, wird eine Reise durch die Zeit mit Vorgängen beim Träumen verglichen.

Im Stargate-Franchise (SG-1, SGA & SGU) wird häufig das Stargate in Verbindung mit Wurmlöchern, die durch genau zu diesem Zeitpunkt stattfindende Sonneneruptionen führen, als Mittel für Zeitreisen angegeben. Dabei sind Reisen in beide Richtungen, sowohl in die Zukunft als auch in die Vergangenheit, möglich.

In der Edelstein-Trilogie "Liebe geht durch alle Zeiten" von Kerstin Gier geht es um ein Mädchen, das unkontrolliert durch die Zeiten springt.

In Fabian Lenks Kinderbuchreihe "Die Zeitdetektive" reisen drei Teenager und eine magische Katze mithilfe eines Tempus durch die Zeit und klären Fälle in der Geschichte auf.

Im Film "Star Trek (2009)" reisen der Romulaner Nero und der alte Mr. Spock mithilfe eines Wurmloches 154 Jahre in die Vergangenheit.

In der ersten Staffel der Jugendserie "Hotel 13" reisen drei beste Freunde in einer Kugel aus Metall ins Jahr 1927. Die kleinste Veränderung in der Vergangenheit kann große Folgen in der Gegenwart haben.

Auch im Film zur Serie "" treffen die drei Freunde auf ein Mädchen aus der Vergangenheit. Sie müssen das Mädchen wieder zurück in ihre Gegenwart, die 1950er Jahre, bringen, sonst würden die drei Freunde sich niemals kennenlernen.
In der zweiten Staffel erleben die drei Freunde ein Abenteuer in der Zukunft.

In dem Buch "Der Anschlag" reist der Lehrer Jacob Epping aus der Jetzt-Zeit in die fünfziger Jahre mit Hilfe eines Zeitportals in einem Diner. Dort will er nach einigen Versuchen und deren Überprüfung in der Jetzt-Zeit (das Portal ist beiderseitig begehbar), das Attentat auf Kennedy verhindern. Die Besonderheit bei dieser Geschichte ist, dass die Zeitlinie sich bei jeder erneuten Reise in die Vergangenheit auf die ursprüngliche zurücksetzt und vom Protagonisten vorgenommene Änderungen wieder verschwinden. Auch führt das Portal zu immer genau demselben Ort und Zeitpunkt (zum 9. September 1958). Außerdem werden die Schwierigkeiten, denen Epping gegenübersteht, umso größer, je stärker er die Zeitlinie zu beeinflussen sucht. Die Zeit scheint sich zu wehren.

Im illustrierten Roman "Das andere Ufer der Zeit" (1970) greift Jack Finney die Idee einer Zeitreise mit Hilfe von Autosuggestion auf. Die Idee wurde vom Autor im Jahr 1995 in dem Roman "Im Strom der Zeit" fortgesetzt.

In den letzten beiden Episoden ("Eine tierische Zeitreise zu den Dodos" und "Eine tierische Zeitreise zu den Beutelwölfen") der Kinderserie "Go Wild! Mission Wildnis" reist das Wild-Team mithilfe eines Zeitreise-Trampolins in die Vergangenheit und beobachtet auf diese Weise die ausgestorbenen Dodos und Tasmanischen Beutelwölfe.

Die Fernsehserie Dark handelt von der fiktiven Stadt "Winden", in der Kinder in einem Jahr verschwinden und (teilweise tot) in einem anderen Jahr auftauchen.


Theoretische Philosophie und Physik

Populäre Darstellungen



</doc>
<doc id="9592" url="https://de.wikipedia.org/wiki?curid=9592" title="Die Zeitmaschine">
Die Zeitmaschine

Die Zeitmaschine (engl. Originaltitel "The Time Machine") ist ein Science-Fiction-Roman von H. G. Wells.

Dieser 1895 erschienene Klassiker der Science-Fiction-Literatur ist die erste literarische Beschreibung einer Zeitreise in die Zukunft, die mittels einer Zeitmaschine bewerkstelligt wird. Der Roman wurde mehrmals verfilmt (siehe: Die Zeitmaschine und The Time Machine) und gilt außerdem als ein Schlüsselwerk zur Entstehung des Steampunk.

Der Zeitreisende, der im Roman nicht mit Namen genannt wird, hat sich gegen Ende des 19. Jahrhunderts eine Maschine gebaut, mit deren Hilfe er sich in der vierten Dimension, der Zeit, bewegen kann. Er erklärt einem Kreis von skeptischen Freunden das Prinzip dieses Geräts sehr anschaulich und auch für den heutigen Leser durchaus nachvollziehbar.

Bei seiner ersten Reise in die Zukunft erreicht er das Jahr 802.701. Dort findet er die Welt zweier gegensätzlicher Arten von Lebewesen menschlicher Abstammung bewohnt, die sich im Verlauf von Jahrtausenden aus den beiden extremen Gesellschaftsklassen des viktorianischen England zu zwei Menschenrassen weiterentwickelt haben: den oberirdisch lebenden "Eloi" und den unterirdischen "Morlocks" (in der deutschen Übersetzung "Morlocken").

Die kindlichen Eloi leben scheinbar sorgenfrei und glücklich, aber völlig unreflektiert und verweichlicht in einer paradiesischen Umgebung, sehen ähnlich aus wie heutige Menschen und scheinen alle relativ jung zu sein. Es ist dem Zeitreisenden anfangs unverständlich, wer sie ernährt und kleidet, da sie offensichtlich nie zu arbeiten brauchen. Andererseits scheint eine namenlose Furcht vor der Dunkelheit, besonders den mondlosen Nächten, ihrer Idylle entgegenzustehen.

Die Morlocks, nach Empfinden des Zeitreisenden hässliche, affenartige, lichtscheue Wesen, hausen in unterirdischen Höhlen. Bei seinen Nachforschungen stellt er fest, dass sie dort riesige Maschinen betreiben und auf diese Weise das Leben der oberirdischen Eloi ermöglichen und erhalten. Anfangs scheint es ihm, als seien die Morlocks die Sklaven der Eloi, so wie in der Vergangenheit die Arbeiterklasse ausgebeutet wurde, um den Wohlstand der oberen Klassen zu sichern. Allmählich aber erkennt er, dass sich das Verhältnis inzwischen umgekehrt hat: Die Morlocks halten sich die Eloi wie Bauern das Vieh, sie sorgen für ihr leibliches Wohl, weil die Menschenfresser sie als Nahrung brauchen. In den dunklen Nächten holen sie sich oben ihre Mahlzeiten.

Vom Jahr 802.701 aus reist der Zeitreisende noch viel weiter in die Zukunft. Dort erblickt er im ewigen Zwielicht der stillstehenden Erde vor einem riesigen roten Feuerball, der einstmals die Sonne war, krabbenartige sowie ballförmige, hüpfende Lebewesen und erkennt, dass die Menschheit mittlerweile ausgestorben ist.

Nach seiner Rückkehr in die Gegenwart glauben seine Freunde ihm die Geschichte nicht, und er beschließt, ein weiteres Mal in die Zukunft zu reisen, diesmal besser ausgerüstet, u. a. mit einer Kamera, um seine Entdeckungen zu dokumentieren. Von dieser Reise kehrt er nicht mehr zurück.

Der Roman stellt eine Anklage gegen die Klassenunterschiede und gegen die Unterdrückung des Menschen durch den Menschen im 19. Jahrhundert dar. In beiden populären Kino-Filmen von 1960 und 2002 wurde dieser wesentliche Aspekt jedoch ausgeblendet. Wells benutzt diese Literaturgattung, in britischer Tradition beispielsweise von Jonathan Swift mit seinen "Gullivers Reisen", um das England seiner Epoche satirisch bloßzustellen und gesellschaftskritisch zu hinterfragen. Der Roman ist somit auch einer der ersten der Gattung Dystopie (siehe Utopische Literatur).

Mehrere Verfilmungen, sowohl für das Kino als auch für das Fernsehen, haben den Roman zur Vorlage genommen, ihn aber um zusätzliche Szenen und Motive ergänzt bzw. wesentliche Motive weggelassen.

1908 schrieb der populäre deutsche Schriftsteller Carl Grunert eine Fortsetzung zu Wells' Geschichte, in der ein junger Mann namens Maurignac Wells' Zeitmaschine wiederfindet und damit in die Vergangenheit reist. In dieser Novelle, betitelt "Pierre Maurignacs Abenteuer" (in der Sammlung „Der Marsspion“ erschienen), taucht auch H. G. Wells selber kurz auf. Diese Geschichte wurde in der DDR unter dem Titel „Das Zeitfahrrad“ in der gleichnamigen Anthologie im Jahr 1974 veröffentlicht.

1914 schrieb Wilhelm Bastiné eine humorige Fortsetzung zu Wells' Geschichte mit dem Titel "Die wiedergefundene Zeitmaschine" (Illustrierte Weltall-Bibliothek Band 5).

Egon Friedell schrieb eine Satire über den Roman unter dem Titel "Die Reise mit der Zeitmaschine" (posthum 1946 erschienen). Späterer Titel: "Die Rückkehr der Zeitmaschine".

Christopher Priest verknüpfte in seinem Roman "Sir Williams Maschine (The Space Machine)" von 1976 die Geschehnisse der Wells-Romane "Krieg der Welten" und "Die Zeitmaschine".

1979 erschien der Roman "Die Nacht der Morlocks: Die Zeitmaschine kehrt zurück (Morlock Nights)" von K. W. Jeter, eine Fortsetzung von Wells' Werk. In dieser fällt die Zeitmaschine in der Zukunft den Morlocks in die Hände. Mit ihrer Hilfe starten diese eine Invasion in das viktorianische London. Das Buch gilt als eines der Schlüsselwerke des Steampunk.

Karl Alexander veröffentlichte ebenfalls 1979 den Zeitreise-Kriminalroman "Flucht ins Heute (Time After Time)" mit H. G. Wells und Jack the Ripper als Protagonisten. (Siehe auch Flucht in die Zukunft)

In "Der Sohn des Hexers" von Wolfgang Hohlbein von 1992 treten die Morlocks und Eloi ebenfalls auf. In diesem Buch werden sie von H. G. Wells und einigen Romanfiguren besucht.

1995 erschien mit "Zeitschiffe (Time Ships)" eine Fortsetzung von Stephen Baxter. Diese greift das Ursprungsthema auf und führt es weiter. Dabei wird es mit aktuellen wissenschaftlichen Erkenntnissen untermauert.

Ronald Wright knüpfte mit seinem 1998 erschienenen Roman "Die Schönheit jener fernen Stadt (A Scientific Romance)" ebenfalls an Wells’ Zeitmaschine an. Hier findet der Protagonist die Zeitmaschine leer zurückgekehrt vor und reist in eine zeitlich nähergelegene Zukunft.

Die ersten sechs Einzelhefte der Comicreihe The League of Extraordinary Gentlemen (1999–2000), zusammengefasst 2000 als "The League of Extraordinary Gentlemen Vol. 1" von Alan Moore und Kevin O’Neill enthalten Moores Kurzgeschichte "Allan und der geteilte Schleier (Allan and the Sundered Veil)", in welcher Allan Quatermain dem Zeitreisenden aus "Die Zeitmaschine" begegnet.

Im Roman Der Funke des Chronos von Thomas Finn (erschienen 2006) taucht eine Zeitmaschine auf, die mit jener aus Wells Roman identisch ist und später von dem als Nebenfigur auftretenden Wells erworben wird.

2010 erschien der Roman "Die Landkarte der Zeit (Mapa del tiempo)" von Félix J. Palma. Auch dieses Werk bedient sich der Zeitmaschine und der Person ihres Schöpfers.



</doc>
<doc id="9594" url="https://de.wikipedia.org/wiki?curid=9594" title="Ausgleichsmandat">
Ausgleichsmandat

Ausgleichsmandate dienen dazu, die bei bestimmten Wahlsystemen zustandekommenden Überhangmandate so auszugleichen, dass andere Parteien, die keine Überhangmandate bekommen haben, nicht benachteiligt werden. So wird erreicht, dass die Parteien im Parlament entsprechend ihrem Zweitstimmenanteil (d. h. gemäß der Verhältniswahl) vertreten sind.

Überhangmandate kann es geben, wenn die Abgeordnetensitze sowohl in einer Mehrheitswahl (über Direktmandate) als auch in einer Verhältniswahl (über Parteilisten) vergeben werden. Dieses Verfahren der sogenannten personalisierten Verhältniswahl wird beispielsweise bei der Wahl zum Deutschen Bundestag, in einigen Bundesländern bei Landtagswahlen und zum Teil bei Kommunalwahlen angewendet.

Das Verfahren der Zuteilung der Ausgleichsmandate ist sehr unterschiedlich und wird auch nur in einigen deutschen Bundesländern durchgeführt. In Niedersachsen zum Beispiel wird die doppelte Anzahl der Überhangmandate zu der ursprünglichen Anzahl der zu vergebenden Sitze hinzuaddiert. Dann werden sämtliche Berechnungen so wiederholt, als wäre die Summe die ursprüngliche Anzahl der Sitze. Dadurch soll das Kräfteverhältnis annähernd wiederhergestellt werden.

Bei der Anwendung der Unechten Teilortswahl, z. B. in Baden-Württemberg, entstehen Ausgleichssitze durch den Abgleich zwischen dem Gesamtergebnis einer Liste in einem Gesamtgemeindegebiet und dem damit erzielten Anspruch ihrer Liste auf Sitze des zu wählenden Gremiums und den Ergebnissen ihrer Kandidaten für den von ihnen vertretenen Wahlbezirk (Teilort bzw. Wohnbezirk).

Bei den Bundestagswahlen von 1949 bis 2009 gab es keine Ausgleichsmandate, daher (und aufgrund der Sperrklausel) entsprach die Sitzverteilung im Bundestag nicht zwangsläufig der prozentualen Zweitstimmenverteilung.

Seit Dezember 2011 galt ein modifiziertes Verfahren der Sitzverteilung für den Bundestag. Im Juli 2012 erklärte das Bundesverfassungsgericht dieses Verfahren für unwirksam. Der Zweite Senat gab mit seiner Entscheidung Verfassungsklagen der Bundestagsfraktionen von SPD und Grünen sowie von mehr als 3000 Bürgern statt.

Im Oktober 2012 einigten sich schließlich die Regierung (CDU, CSU und FDP) und Teile der Opposition (SPD und Grüne) im Bundestag darauf, ab der Bundestagswahl 2013 Ausgleichsmandate für den Bundestag einzuführen. Die Wahlrechtsreform wurde am 21. Februar 2013 im Bundestag beschlossen und durch Anpassung des des Bundeswahlgesetzes umgesetzt.

Die Wahl 2013 ergab schließlich vier Überhangmandate für die CDU. Durch die umfangreiche Regelung des Ausgleichs bestimmten aber nicht die CDU-Überhangmandate die Gesamtgröße, sondern die Sitzzahl der CSU, sodass die anderen Parteien zusätzlich insgesamt 29 Ausgleichsmandate erhielten.

Die Regelung des Ausgleichs – nicht der Ausgleich an sich – und die damit verbundene Gefahr einer starken Aufblähung der Sitzzahl wurde bereits im Rahmen des Gesetzgebungsverfahrens kritisiert. Der Bund der Steuerzahler Deutschland forderte nach der Bundestagswahl 2013 und im Vorfeld der Wahl 2017 aufgrund der zu erwartenden Kostensteigerungen und Bürokratisierungen die Abschaffung der Ausgleichsmandate sowie die Deckelung der Zahl der Abgeordneten von 630 für die Bundestagswahl 2017.



</doc>
<doc id="9598" url="https://de.wikipedia.org/wiki?curid=9598" title="Kreis (Begriffsklärung)">
Kreis (Begriffsklärung)

Kreis steht für:




Kreis ist der Familienname folgender Personen:

Kreis, Weiteres:
Siehe auch:


</doc>
<doc id="9599" url="https://de.wikipedia.org/wiki?curid=9599" title="Disassembler">
Disassembler

Ein Disassembler ist ein Computerprogramm, das die binär kodierte Maschinensprache eines ausführbaren Programmes in eine für Menschen lesbarere Assemblersprache umwandelt. Seine Funktionalität ist der eines Assemblers entgegengesetzt.

Zu jedem Prozessorbefehl existiert ein entsprechender Befehl in Assemblersprache, ein Mnemonic. Auch die einzelnen Argumente der Prozessorbefehle, wie Registernummern oder Speicheradressen, lassen sich eins zu eins in Assemblercode übertragen. Ein Disassembler übersetzt also die binären Repräsentationen der Prozessorbefehle zurück in für Menschen lesbare Assemblersprache.

Bei einem Programm, das in Assembler geschrieben wurde, lässt sich der ursprüngliche Code im Allgemeinen nachträglich nicht im Originaltext wiederherstellen, sondern lediglich funktional gleich. Pseudobefehle, Labels und Kommentare sind vom Programmierer gewählt und haben daher zumindest keine direkte, d. h. eindeutig umkehrbare Entsprechung im Maschinencode, was die exakte textliche Rekonstruktion unmöglich macht. Bei einem Programm, das in einer höheren Programmiersprache geschrieben wurde, hat der Compiler meist verschiedene Optimierungen vorgenommen, welche die Programmausführung beschleunigen, aber den Maschinencode komplexer machen.

Ein Disassemblat ist damit in der Regel deutlich schlechter lesbar als der ursprüngliche Assemblercode. Auch aus diesem Grund ist es bei komplexeren Programmabschnitten sinnvoll und üblich, die Disassemblierung nicht in einem einzigen Durchgang vollautomatisch vorzunehmen, sondern in Wiederholungen mit manuellen Zwischenschritten. Hierbei können aus schrittweiser Erfahrung und Erkenntnis zunehmend Bezeichner für Sprungmarken, Unterprogramme, Variablen, Konstanten sowie Kommentare manuell verfeinert und zur Optimierung in nachfolgenden Durchläufen vorgegeben werden.

Der Zweck eines Disassemblers liegt meistens darin, die Diagnose und das Auffinden von potenziellen Fehlern in einem Programm zu erleichtern oder die Ausgabe eines Compilers zu überprüfen. Oft wird er auch zum Reverse Engineering benutzt.

Proprietäre Software steht oft unter einer EULA, die das Disassemblieren sowie Dekompilieren untersagt. Dies ist jedoch in Deutschland nur in seltenen Fällen rechtlich bindend. Auch ist Reverse Engineering zur Herstellung von Kompatibilität mit der Software Dritter ausdrücklich gesetzlich gesichert erlaubt – und damit nicht durch einen Lizenzvertrag zu unterbinden.

Die Tätigkeit des Disassemblierens ist bei urheberrechtlich geschützten Programmen allerdings in einigen Ländern durch gesetzliche Bestimmungen reglementiert, da das Disassemblieren die Möglichkeit eröffnet, den Code zu ändern und danach wieder in ein Programm zu assemblieren. So könnte z. B. von erfahrenen Assembler-Programmierern und Crackern eine Passwortabfrage entfernt, eine aus lizenzrechtlichen Gründen abgeschaltete Funktion aktiviert, oder ein Virus unbemerkt in das Programm eingeschleust werden. Ausführliche Erläuterungen zu diesem Aspekt von Disassemblern und vergleichbaren Werkzeugen finden sich im Artikel Reverse Engineering.
Nahezu alle Debugger, einige Programme zur Inspektion von Binärdateien sowie manche Assembler beinhalten einen Disassembler. Ein bekannter und häufig eingesetzter Disassembler ist IDA von Ilfak Guilfanov, der Assemblercode strukturiert anzeigen kann.

Der Vorgang des Disassemblierens wird in der deutschsprachigen Fachliteratur synonym mit den Begriffen „entassemblieren“ oder nicht ganz korrekt mit „entkompilieren“ bezeichnet. Letzteres würde korrekterweise das Dekompilieren meinen.



</doc>
<doc id="9600" url="https://de.wikipedia.org/wiki?curid=9600" title="IIOP">
IIOP

Internet Inter-ORB Protocol (IIOP) ist eine Spezifikation der Object Management Group (OMG). Es handelt sich dabei um ein in CORBA definiertes Protokoll auf der Basis von GIOP, mit dem Object Request Broker (ORB) über das Internet kommunizieren können, um Methodenaufrufe von Objekten auf anderen Rechnern durchzuführen.

IIOP ist eine Spezialisierung des abstrakten "General Inter-ORB Protocol" (GIOP)
für die Kommunikation über TCP/IP.
GIOP und IIOP wurden in der CORBA-Version 2.0 definiert.
Mit Hilfe des IIOP können die ORBs verschiedener Hersteller miteinander kommunizieren.

IIOP verwendet dabei den TCP-Port 683, für die mit SSL verschlüsselte Variante den TCP-Port 684. In der Liste der verwendeten Ports der IANA werden die Ports als "CORBA IIOP" und "CORBA IIOP SSL" bezeichnet. Dort sind auch die entsprechenden UDP-Ports reserviert; diese werden in der Praxis aber kaum benutzt.

Das Protokoll IIOP dient außerhalb von CORBA-Architekturen auch als alternatives Kommunikationsprotokoll für Remote Method Invocation in Java-Anwendungen. Das Gesamtprotokoll heißt dann "RMI over IIOP".



</doc>
<doc id="9605" url="https://de.wikipedia.org/wiki?curid=9605" title="X86-Prozessor">
X86-Prozessor

x86 ist die Abkürzung einer Mikroprozessor-Architektur und der damit verbundenen Befehlssätze, welche unter anderem von den Chip-Herstellern Intel und AMD entwickelt werden.

Die x86-Befehlssatzarchitektur (, kurz „ISA“) ist nach den Prozessoren der 8086/​8088-Reihe benannt, mit der sie 1978 eingeführt wurde. Die ersten Nachfolgeprozessoren wurden später mit 80"1"86, 80"2"86 usw. benannt. In den 1980er-Jahren war daher von der 80x86-Architektur die Rede – später wurde die „80“ am Anfang weggelassen. Die x86-Architektur erweiterte sich seither mit jeder Prozessorgeneration und war mit dem 80386 1985 bereits eine 32-Bit-Architektur, die explizit auch als i386 bezeichnet wurde.

Während der Entwicklung des Itanium benannte Intel die x86-Architektur, die damals eine 32-Bit-Architektur war, retronym in „“ um, abgekürzt IA-32. Auch die retronyme Bezeichnung IA-16 für die 16-Bit-Architektur des 8086/​80286 ist bekannt, fand aber keine breite Verwendung. Hingegen wurden die alten Bezeichnungen „x86“ und „i386“ (für 32-Bit-x86) weiterhin genutzt.

Die Architektur des unabhängig entwickelten und nicht kompatiblen Itanium bezeichnete Intel "IA-64", was auch deshalb zu Verwechslungen führen kann, weil AMD mit der 2003 erstmals verfügbaren 64-Bit-Befehlssatzerweiterung AMD64 die Befehlssatzarchitektur IA-32 ebenfalls zur 64-Bit-Architektur gemacht hat. Intel selbst ist mit Intel 64 2005 nachgezogen; dabei ist Intel 64 zu AMD64 kompatibel. Moderne 64-Bit-x86-Prozessoren sind somit weiterhin als zur IA-32-Architektur zugehörig zu bezeichnen, was seither jedoch uneindeutig ist. Um 32- und 64-Bit voneinander unterscheiden zu können, wurde in Anlehnung an „x86“ für den 64-Bit-Modus die Bezeichnungen „x64“ (für x86 mit 64 Bits) eingeführt. Die retronyme Bezeichnung „x32“ (für x86 mit 32 Bits) ist eher selten anzutreffen und obendrein uneindeutig, da es sich entweder um einen 32-Bit-x86-Prozessor(-Modus) oder um 32-Bit-Adressierung auf einem im 64-Bit-Modus laufenden 64-Bit-Prozessor handeln kann.

Da sich Ziffernkombinationen nicht markenrechtlich schützen lassen, gingen Intel und die meisten Mitbewerber nach Einführung des 80486 dazu über, Wortmarken wie Pentium oder Celeron zu verwenden, aber das alte Nummernschema blieb als Name der ganzen Familie erhalten.

Die x86-Architektur wurde 1978 mit Intels erster 16-Bit-CPU, dem 8086, eingeführt, der die älteren 8-Bit-Prozessoren 8080 und 8085 ablösen sollte. Obwohl der 8086 anfangs nicht sonderlich erfolgreich war, stellte IBM 1981 den ersten PC vor, der eine abgespeckte Variante des 8086, den 8088, als CPU verwendete. Durch den enormen Erfolg des IBM-PC und seiner zahlreichen Nachbauten, der sogenannten IBM-PC-kompatiblen PCs, wurde die x86-Architektur innerhalb weniger Jahre zu einer der erfolgreichsten CPU-Architekturen der Welt und ist es bis heute geblieben.

Außer Intel haben auch andere Hersteller über die Jahre x86-kompatible CPUs in Lizenz produziert, darunter Cyrix (heute VIA Technologies), NEC, UMC, Harris, TI, IBM, IDT und Transmeta. Der nach Intel größte Hersteller x86-kompatibler Prozessoren war und ist aber das Unternehmen AMD, das neben Intel heute zu einer treibenden Kraft bei der Weiterentwicklung des x86-Standards geworden ist.

Intel entwickelte den 8086 1978 in der Zeit der zu Ende gehenden 8-Bit-Ära. Mit dem 80386 führte Intel dann bereits 1985 die erste x86-CPU mit einer 32-Bit-Architektur ein. Heute ist diese Architektur unter dem Namen IA-32, oft auch unter der Bezeichnung i386-Architektur bekannt; sie ist sozusagen die Erweiterung der Befehlssätze von 8086 und 80286 auf 32 Bit, schließt deren Befehlssätze aber vollständig mit ein. Die 32-Bit-Ära war der bisher längste und lukrativste Abschnitt der x86-Geschichte, wobei sich IA-32 – maßgeblich unter Intels Federführung – permanent weiterentwickelte. Erst 2003 brach für x86 die 64-Bit-Ära an, diesmal aber auf Initiative von AMD. Der 64-bittige x86-Standard heißt AMD64 und wurde unter dem Namen Intel 64 auch von Intel übernommen. Die dafür gebräuchliche Bezeichnung ist x64 oder x86-64.

Die von Intel in der Itanium-Produktlinie verwendete IA-64-Architektur hat mit IA-32 – inklusive x64 – nichts zu tun. Sie ist eine Neuentwicklung, die außer einer x86-Emulation (nur in der ältesten Itanium-Baureihe) keine Spuren der x86-Technik enthält. Dagegen ist die AMD64-Reihe vollständig abwärtskompatibel zu 32- und 16-bit-Anwendungen.

Da sich der Befehlssatz ständig erweiterte, kann man nur von einem minimal erforderlichen Befehlssatz ausgehen, wenn man von einer x86-Befehlssatzarchitektur spricht – oder vom jeweils aktuellen Stand, mit allen möglichen Erweiterungen. In diesem Punkt ist die Bezeichnung „x86“ sehr uneindeutig. Bei der Benennung hat sich daher eine gewisse Konvention herausgebildet, die durch die geschichtliche Entwicklung begründet ist.
Während die Befehlssatzarchitektur x86 die ungenaueste Bezeichnung darstellt, sagen die gelisteten genaueren Benennungen jedoch immer noch nicht präzise die vorhandenen (von einer Software benötigten) Maschinenbefehle bzw. den genauen integrierten Befehlssatz im Prozessor aus. Unter Linux hat sich beispielsweise die Angabe „i686-pae“ für den Pentium‑II-Befehlssatz mit PAE durchgesetzt. So gibt es beispielsweise von GParted ein ISO-Abbild für „i486“ und für „i686-pae“ – hat eine Prozessor kein PAE-Flag (wie z. B. der erste Pentium M), muss man auf die i486-Variante zurückgreifen. Auch unter Windows ist nicht klar, ob die 64-Bit-Variante auch tatsächlich auf einem älteren 64-Bit-x86-Prozessor (mit AMD64- oder Intel-64-Erweiterung) läuft, da z. B. ab Windows 8.1 zusätzlich zur x64-Befehlssatzerweiterung die Funktionen codice_1, codice_2 und codice_3 vorhanden sein müssen.

Die x86-Architektur verwendet einen CISC-Befehlssatz mit variabler Instruktionslänge. Speicherzugriffe in Wortgröße sind auch auf nicht Wort-ausgerichtete Speicheradressen erlaubt. Wörter werden in Little-Endian-Richtung gespeichert. Leichte Portierbarkeit von Intel-8085-Assemblercode war eine treibende Kraft der Architekturentwicklung. Dies bedingte einige nicht optimale und im Nachhinein problematische Designentscheidungen.

Heutige x86-Prozessoren sind hybride CISC/RISC-Prozessoren, denn sie übersetzen den x86-Befehlssatz zunächst in RISC-Mikro-Instruktionen konstanter Länge, auf die moderne mikro-architektonische Optimierungen angewendet werden können. Die Übergabe erfolgt zunächst an sogenannte Reservierungsstationen, das heißt an kleine Puffer, die den verschiedenen Rechenwerken vorgeschaltet sind. Der erste hybride x86-Prozessor war der Pentium Pro.

Die Intel 8086 und 8088 hatten 14 16-Bit-Register. Vier von ihnen (codice_4, codice_5, codice_6, codice_7) waren Mehrzweck-Register. Zusätzlich hatte jedes noch eine Sonderfunktion:

Auf jedes Register konnte mittels zwei separater Bytes zugegriffen werden (das hohe Byte in codice_5 unter dem Namen codice_13, das niederwertige Byte als codice_14). Von den zwei Zeigerregistern zeigt codice_15 („StackPointer“) auf das oberste Element des Stacks und codice_16 („BasePointer“) kann auf einen anderen Platz im Stack oder Speicher zeigen (häufig wird codice_16 als Zeiger auf einen Funktionsrahmen verwendet). Die beiden Index-Register codice_18 („SourceIndex“) und codice_19 („DestinationIndex“) können für Blockoperationen oder zusammen mit codice_15 oder codice_16 als Index in einem Array benutzt werden. Zusätzlich gibt es die vier Segmentregister codice_22 („Codesegment“), codice_23 („DataSegment“), codice_24 („StackSegment“) und codice_25 („ExtraSegment“), mit denen jeweils die Basisadresse für ein 64 kB großes Speichersegment festgelegt wird. Außerdem gibt es das Flag-Register, das Flags wie ', ', "" usw. enthalten kann, und den (codice_26), der auf die gegenwärtige Instruktion zeigt.

Im "Real Mode" ist der Speicherzugriff „segmentiert“. Dies geschieht, indem die Segmentadresse um 4 Bit nach links geschoben wird und ein Offset addiert wird, so dass eine 20-Bit-Adresse entsteht. Der gesamte Adressraum im "Real Mode" ist also 2 Byte (1 Megabyte), was 1978 sehr viel war. Es gibt zwei Adressierungs-Modi: ' und ' ("engl." für nah und fern). Im "Far Mode" werden sowohl das Segment als auch der Offset angegeben. Im "Near Mode" wird nur der Offset angegeben, und das Segment wird einem Register entnommen. Für Daten ist dies codice_23, für Code codice_22 und für den Stack codice_24. Wenn codice_23 zum Beispiel "A000h" und codice_18 "5677h" ist, zeigt codice_32 auf die absolute Adresse codice_23 × 16 + codice_18 = "A5677h".

In diesem Schema können unterschiedliche Segment/Offset-Paare auf dieselbe absolute Adresse zeigen. Wenn codice_23 "A111h" und codice_18 "4567h" ist, zeigt codice_32 ebenfalls auf die obige Adresse "A5677h". Das Schema sollte die Portierbarkeit von Intel-8085-Code erleichtern, doch erschwerte es letztlich die Arbeit der Programmierer.

Zusätzlich hatte der i8086 64 kB von 8-Bit-I/O-Adressraum (alternativ auch 32 kB mit 16 Bit) sowie einen hardwareunterstützten Stack von ebenfalls 64 kB. Nur Wörter (2 Byte) können auf dem Stack abgelegt werden. Der Stack wächst zu niedrigeren Adressen hin und codice_38 zeigt auf das zuletzt auf den Stack gelegte Wort (die niedrigste Adresse). Es gibt 256 Interrupts, die sowohl von Hardware als auch Software ausgelöst werden können. Die Interrupts können kaskadieren und benutzen den Stack, um die Rücksprungadresse zu speichern.

Der Intel-80286-Prozessor kannte einen weiteren Arbeitsmodus, den „Protected Mode“. Durch Integration einer MMU ("engl." für Speicherverwaltungseinheit) auf dem Chip konnten im "Protected Mode" bis zu 16 MB Speicher adressiert werden. Ein spezielles MMU-Register zeigt dabei auf eine Segmenttabelle im Hauptspeicher, in der die 24-Bit-Basisadressen der Segmente festgelegt wurden. Die Segmentregister dienten dann lediglich als Index in diese Segment-Tabelle. Zusätzlich konnte jedem Segment einer von vier Privilegien-Levels zugeordnet werden („Ringe“ genannt). Insgesamt bedeuteten diese Neuerungen eine Verbesserung. Allerdings war Software für den "Protected Mode" inkompatibel mit dem "Real Mode" des i8086-Prozessors.

Die Intel-80386 brachte den wahrscheinlich größten Sprung für die x86-Architektur. Mit Ausnahme des Chips „Intel 80386SX“, der nur 24-Bit-Adressierung unterstützte und einen 16-Bit-Datenbus hatte, waren alle 386er vollständig 32-Bit – Register, Instruktionen, E/A-Raum und Speicher. Bis zu 4 GB Speicher konnten angesprochen werden. Dazu wurde der "Protected Mode" zum „32-Bit-Enhanced-Mode“ erweitert. Wie auf dem 80286 wurden auch im "Enhanced Mode" die Segmentregister als Index in einer Segmenttabelle verwendet, die die Aufteilung des Speichers beschrieb. Allerdings konnten in jedem Segment 32-Bit-Offsets verwendet werden. Dies führte zum sogenannten „Flat Memory Model“, bei dem jedem Prozess nur noch ein 4-GB-Datensegment und ein 4-GB-Codesegment zur Verfügung gestellt wird. Beide Segmente beginnen ab der Adresse codice_39 und sind 4 GB groß. Die eigentliche Speicherverwaltung wird dann nur noch durch das ebenfalls mit dem 80386er eingeführte Paging durchgeführt, einem Mechanismus, der den gesamten Speicher in gleich große Teile ("engl." , also Speicher"seiten") einteilt und pro Prozess eine beliebige Abbildung zwischen logischen und physischen Adressen ermöglicht, was die Realisierung von virtuellem Speicher stark vereinfacht hat. Es wurden keine neuen Mehrzweck-Register hinzugefügt. Allerdings wurden bis auf die Segmentregister alle Register auf 32 Bit verbreitert. Das erweiterte Register codice_4 hieß fortan codice_41, aus codice_18 wurde codice_43 usw. Zwei neue Segmentregister namens codice_44 und codice_45 kamen noch hinzu.

Die grundlegende Architektur des 386er-Prozessors (auch IA-32 genannt) wurde zur Basis aller weiteren Entwicklungen in der x86-Architektur. Bis heute funktionieren alle x86-kompatiblen CPUs nach dem Prinzip des i80386.

Der bis dato separate mathematische Coprozessor 80387 wurde ab der nächsten CPU, dem „Intel 80486“, direkt in den Prozessor integriert (mit Ausnahme des 486SX, welcher keinen Coprozessor besitzt). Mit diesem Coprozessor konnten Gleitkommaberechnungen in Hardware durchgeführt werden. Ohne ihn mussten diese auf Berechnungen mit ganzen Zahlen abgebildet werden (Emulation). Nicht nur werden so recht viele Befehle pro Gleitkommaoperation benötigt, auch treten dabei häufig Schleifen und Verzweigungen auf, sodass Gleitkommaoperationen ohne den Coprozessor vergleichsweise sehr langsam ausgeführt wurden.


1996 führte Intel die MMX-Technologie ein (englisch ', besonders vom Marketing aber auch häufig ' tituliert). MMX definierte 8 neue SIMD-Register von 64 Bit Breite, die allerdings denselben Speicherplatz wie die Register der Floating Point Unit (FPU) benutzten. Dies verbesserte zwar die Kompatibilität zu bestehenden Betriebssystemen, die beim Umschalten zwischen verschiedenen Anwendungen weiterhin nur die altbekannten FPU-Register sichern mussten. Aber zwischen MMX und FPU musste aufwendig umgeschaltet werden. Dazu kam, dass MMX auf Integer-Operationen beschränkt war und lange Zeit von den Compilern nicht richtig unterstützt wurde. Insbesondere Microsoft tat sich schwer, den hauseigenen Compiler wenigstens mit Unterstützung für MMX-Intrinsics auszustatten. MMX wurde daher nur relativ selten verwendet, am ehesten noch für 2D-Videobearbeitung, Bildbearbeitung, Videowiedergabe usw.

1997 erweiterte AMD den MMX-Befehlssatz um Gleitkomma-Operationen für Gleitkommazahlen einfacher Genauigkeit und nannte die so entstandene Technik 3DNow. Dies löste zwar nicht die Compiler-Probleme, aber 3DNow! ließ sich im Unterschied zu MMX für 3D-Spiele verwenden, die auf schnelle Gleitkomma-Operationen angewiesen sind. Spieleentwickler und Hersteller von 3D-Grafikprogrammen verwendeten 3DNow!, um die Anwendungsperformance auf AMDs K6- und Athlon-Prozessoren zu verbessern.

1999 brachte Intel mit dem Pentium-III-Prozessor den SSE-Befehlssatz. Wie AMD fügte Intel hauptsächlich Gleitkomma-SIMD-Befehle hinzu.
Des Weiteren schuf man für SSE eine separate Funktionseinheit auf dem Prozessor mit 8 neuen 128-Bit-Registern (XMM0 bis XMM7), die sich nicht mehr mit den Gleitkommaregistern überlagerten. Da diese neuen Register aber auch bei einem Kontextwechsel vom Betriebssystem gesichert werden müssen, wurde eine Sperre in der CPU implementiert, die erst von SSE-fähigen Betriebssystemen freigeschaltet werden muss, um die SSE-Register in Anwendungsprogrammen verfügbar zu machen. 

AMD-Prozessoren unterstützten anfänglich nur die 64-Bit-Befehle der Erweiterung, welche in der MMX-Funktionseinheit arbeiten, da die separate Funktionseinheit komplett fehlte. Ein Großteil dieser Befehle arbeitet nur mit Daten vom Typ Integer, deshalb existiert auch die Bezeichnung ISSE, wobei I für Integer steht. Ab dem Athlon-XP-Prozessor wird SSE komplett unterstützt.

SSE2, von Intel 2001 mit dem Pentium 4 eingeführt, fügte erstens weitere Ganzzahlbefehle für die SSE-Register hinzu und zweitens 64-Bit-SIMD-Gleitkomma-Befehle. Erstere machten MMX fast obsolet, und letztere erlaubten auch konventionellen Compilern, SIMD-Instruktionen zu verwenden. Daher wählte AMD mit der Einführung der 64-Bit-Erweiterung SSE2 als integralen Bestandteil der AMD64-Architektur aus, sodass alle 64-Bit-x86-Prozessoren diese Erweiterung unterstützen (AMD-Prozessoren ab Athlon64).

Mit der "Prescott"-Revision des Pentium 4 lieferte Intel ab 2004 SSE3 aus, das hauptsächlich Speicher- und Threadmanagement-Instruktionen liefert, um die Leistung von Intels Hyper-Threading-Technologie zu steigern.

AMD beherrscht seit den Athlon-64-Prozessoren mit den Kernen "Venice" und "San-Diego" ebenfalls den Befehlssatz SSE3.

"Siehe auch:" SSSE3, SSE4, SSE4a und SSE5

Um das Jahr 2002 erreichte der Speicherausbau moderner x86-Rechner die durch die 32-Bit-Adressengröße bedingte Adressierungsgrenze der x86-Befehlssatzarchitektur von 4 GB. Zwar hatte Intel mit PAE bereits mit dem Pentium Pro eine Möglichkeit eingeführt, mehr als 4 GB Arbeitsspeicher zu adressieren, allerdings war dessen Nutzung programmtechnisch aufwendig und der pro Prozess nutzbare Speicher blieb auch so nach wie vor auf maximal 4 GB beschränkt.

Intel wollte ursprünglich den Sprung auf 64 Bit mit einer neuen Prozessorarchitektur namens IA-64 vollführen, allerdings konnte sich diese nur als Nischenprodukt im Marktsegment der Server und Workstations etablieren. AMD hingegen erweiterte die 32-bit Prozessorarchitektur auf 64 Bit und nannte diese Erweiterung AMD64. Später übernahm Intel große Teile dieser Erweiterung unter der Bezeichnung "EM64T", schließlich dann unter dem heute bei Intel-Produkten gebräuchlichen Namen "Intel 64".

Für 64-Bit-Prozessoren, die auf der x86-Architektur beruhen, hat sich die Bezeichnung "x86-64" oder kurz "x64" etabliert. Dadurch, dass AMD bereits 2003 als erster Hersteller die 64-Bit-Befehlssatzerweiterung veröffentlicht hatte, hat sich jedoch auch die Bezeichnung "amd64" durchgesetzt – im Kontext von Software bezeichnet "amd64" jedoch jeden zu AMD64 kompatiblen Befehlssatz, also auch x64 bei Prozessoren von Intel und VIA.

Obwohl die Virtualisierung eines x86-Prozessors aufgrund der umfassenden Architektur aufwändig ist, gibt es mehrere Produkte, die einen virtuellen x86-Prozessor zur Verfügung stellen, darunter VMware, Hyper-V und Virtual PC oder auch Open-Source-Software wie Xen oder VirtualBox. Hardwareseitige Virtualisierung gibt es auch als Erweiterung, sie wird bei Intel „Intel VT“ (für Virtualization Technology), bei AMD „AMD Virtualization“ genannt.

Im Jahr 2008 sollen die SIMD-Erweiterungen nach MMX, SSE 1-4 wieder erweitert werden und Intel schlägt „AVX“ vor. AVX wird erstmals 2011 in der SandyBridge Mikroarchitektur realisiert. Gegenüber SSE wird die Wortbreite für Daten und Register auf 256 Bit verdoppelt. Es kommen viele neue Befehle hinzu, die als 256-Bit- Erweiterungen der SSE-Befehle verwendet werden können. Mit der nächsten Überarbeitung der Mikroarchitektur, der Haswell-Mikroarchitektur wird AVX wieder um neue Befehle erweitert, fortan AVX-2 genannt und kann nun fast alle SSE-Befehle in einer 256-Bit-Erweiterung bieten.

Da im High-Performance-Computing mittlerweile die Energieeffizienz immer wichtiger wird und das SIMD-Konzept Fortschritte ermöglicht, wird für die Intel Xeon Phi genannten Rechenbeschleunigerkarten (ebenfalls 2013) AVX nochmals komplett überarbeitet, die Daten- und Registerbreite auf 512 Bit verdoppelt und die Anzahl der Register auf 32 verdoppelt. Diese Erweiterung nennt Intel AVX-512. Sie besteht aus mehreren spezifizierten Gruppen von neuen Befehlen, die nicht alle gleich realisiert werden. Die zweite Xeon Phi-Generation („Knights Corner“) erhält die „Foundation“-, die dritte Generation („Knights Landing“) 2016 zusätzlich „CD“-, „ER“- und „PF“-Erweiterungen.

Für die für 2017 angekündigte Skylake-Xeon-Server-Generation EP/EX ist AVX-512 ebenfalls angekündigt.

x86-kompatible Prozessoren wurden von vielen Firmen entwickelt und hergestellt, darunter:





</doc>
<doc id="9611" url="https://de.wikipedia.org/wiki?curid=9611" title="Accelerated Graphics Port">
Accelerated Graphics Port

Der Accelerated Graphics Port (AGP) ist eine Anschlussnorm auf PC-Hauptplatinen („Mainboard“) zur direkten Verbindung der Grafikkarte mit dem Chipsatz/Northbridge. Er basiert technisch auf dem PCI-Bus.

Sobald der Grafikkartenspeicher nicht mehr ausreicht, benutzt AGP den normalen Arbeitsspeicher (RAM) des PCs, um dort zum Beispiel Monitorbild, Texturen, Z-Buffer-, Alpha-Blending- und andere Grafikdaten zu hinterlegen. Sogenannte „Bursts“ ermöglichen in diesem zusammenhängenden Speichermanagement ein schnelles Lesen der Daten. Es erlaubt auch die effizientere Nutzung des Framebuffer-Speichers und beschleunigt so ebenfalls 2D-Grafik. Von einigen RAID-Controllern wird der AGP-Slot wegen seiner höheren Transferrate als Steckplatz anstelle von PCI benutzt, obwohl er eigentlich nicht dafür entwickelt worden ist. AGP ist kein Bus, sondern eine Punkt-zu-Punkt-Verbindung. Es gibt einige wenige Hauptplatinen mit mehreren unabhängigen AGP-Steckplätzen (denen dann jeweils ein eigener Controller auf der Hauptplatine zugeordnet sein muss), im Normalfall ist aber nur ein AGP-Steckplatz vorhanden.

AGP wurde von Intel entwickelt und kam erstmals 1997 im i440LX-Chipsatz für den Pentium-II-Prozessor zum Einsatz. Ab 1998 tauchte AGP in großem Stil in handelsüblichen PCs auf, die auch mit Chipsätzen anderer Hersteller bestückt waren. Auf der Hauptplatine ist im Wesentlichen der AGP-Steckplatz und die AGP-Bridge erforderlich, die meistens in der Northbridge des Chipsatzes untergebracht ist. Die Grafikkarte ist auf diese Weise auf dem schnellsten Weg mit der CPU und dem Arbeitsspeicher verbunden, die beide für die Grafikleistung von essentieller Bedeutung sind.

Die erste AGP-Version, AGP 1.0, erschien 1997 mit den Modi AGP 1x und AGP 2x. AGP 1x überträgt pro Takt 32 Bit an Daten mit einer gegenüber PCI verdoppelten Taktfrequenz von 66 MHz und erreicht eine theoretische Transferrate von 266 Megabyte pro Sekunde. AGP 2x überträgt die Daten im DDR-Verfahren und erreicht bei gleicher Taktfrequenz 533 MB/s. Die Signalspannung bei AGP 1.0 beträgt 3,3 Volt. Die schnellsten kompatiblen Karten basieren auf den ATI-Radeon-9700- bzw. Nvidia-GeForce-FX-GPUs.

AGP 2.0 (1999) bot zusätzlich den Modus AGP 4x mit einer maximalen Transferrate von 1066 MB/s bei einer reduzierten Signalspannung von 1,5 Volt. Die Geschwindigkeitssteigerung wurde erreicht, indem nun vier Datenpakete pro Takt übertragen wurden. Die meisten Grafikkarten dieser Generation waren voll abwärtskompatibel zum älteren Standard und ließen sich auf Hauptplatinen mit 3,3 V bei AGP 2x betreiben. Diese Modelle werden nach der AGP-Spezifikation Universal AGP-Karten genannt und haben zwei Kerben in ihrer Anschlussleiste.

AGP 3.0 (2002) erreichte im neuen Modus AGP 8x bei nur noch 0,8 Volt Signalspannung die mit 2133 MB/s höchste AGP-Transferrate, statt vier wurden nun acht Datenpakete pro Takt übertragen. Die Modi 1x und 2x existieren in diesem Standard nicht mehr. Praktisch alle Grafikkarten dieser Generation unterstützen jedoch für den Betrieb in älteren Mainboards auch AGP 2.0 mit 1,5 V und dessen langsamere Modi („Universal 1.5V AGP 3.0“), einige sogar AGP 1.0 mit 3,3 V („Universal AGP 3.0“) – das ist an den zwei Kerben in der Anschlussleiste erkennbar. Da die mechanische Kodierung von AGP-3.0-Modellen mit der von 2.0 identisch ist, tolerieren alle 3.0-Karten den Einbau in 2.0-Systeme, Nicht-Universalkarten starten darin jedoch nicht. Ältere Karten verursachen in Hauptplatinen mit AGP-3.0-Standard dank der im Standard bereits vorgesehenen Schutzschaltung ebenfalls keinen Schaden, starten jedoch nur in den gleichsam benannten Universalmodellen.

AGP 3.5 (2004) führte keine neuen Transfermodi ein, sondern widmete sich der Stabilisierung der Umsetzung. Einheitliche Standards für GART und Aperture (siehe AGP-Begriffe) sollten einen einheitlichen AGP-Treiber für alle Mainboards bringen, welcher in Form der Datei uagp35.sys auch in Windows XP SP2 einzog. Die vorher verwendeten chipsatzeigenen Treiber führten immer wieder zu Problemen bei AGP-Optionen wie FastWrites und schnelleren Transfermodi. Auch Dual-AGP-Mainboards zur Zusammenschaltung mehrerer Grafikkarten sollten möglich werden, das kam aber durch die Entwicklung von PCI Express und den darauf aufbauenden Multi-Grafikkarten-Verfahren (z. B. Crossfire oder SLI) kaum zum Tragen.

Zusätzlich existieren noch Karten, die auf dem sogenannten AGP-Pro-Steckplatz basieren. Deren Kontaktleisten sind etwas länger als die normaler AGP-Karten. AGP Pro versorgt Karten mit bis zu 50 W anstatt der 25 Watt des normalen Steckplatzes. Als Alternative dazu können stromhungrige Grafikkarten extern über eine eigens angebrachte Spannungsversorgung gespeist werden (5 V/12 V, Molex-Steckverbinder), was wegen der geringen Verbreitung der Pro-Steckplätze die häufiger verwendete Lösung war. Ein AGP Pro-Steckplatz nimmt auch normale AGP-Karten auf und war zumeist im semiprofessionellen Workstationbereich zu finden.

AGP-Steckkarten besitzen an unterschiedlichen Stellen eine Einkerbung im Stecker, in den der entsprechende Steg im Steckplatz auf der Hauptplatine passen muss. Bei AGP 1.0 (AGP 1x/2x) sitzt der Steg weiter in Richtung Slotblech, ab AGP 2.0 dagegen in Richtung der Gehäusevorderwand. Das soll verhindern, dass Karten mit 3,3 V Signalspannung in Hauptplatinen mit nur noch 1,5 bzw. 0,8 V Signalspannung gesteckt werden, da sonst bei Inbetriebnahme eine Beschädigung droht (siehe nächster Absatz).

Durch die Zuordnung der Stege zu den Signalspannungen sollte ein versehentliches Einbauen inkompatibler Grafikkarten physisch verhindert werden. Dennoch gab es sowohl Hersteller von Grafikkarten als auch Hauptplatinen, die einen AGP-Universalanschluss verbauten, obwohl ihr Produkt zwingend an eine AGP-Version und damit Spannung gebunden war. Ein somit mögliches Einbauen inkompatibler Karten führt mit großer Wahrscheinlichkeit zur Beschädigung von Grafikkarte und Mainboard-Komponenten.

So wurden einige TNT2- und SiS305-basierende Grafikkarten gefertigt, welche mit AGP 1.0 arbeiten, mit ihrem irreführenden Universalanschlusse mit zwei Kerben aber auch in Hauptplatinen mit reinem AGP 2.0-Anschluss passen. Diese reinen AGP-2.0-Hauptplatinen sind jedoch inkompatibel mit der für Version 1.0 vorgesehenen Spannung. Einige dieser Hauptplatinentypen haben daher eine Schutzschaltung, die bei solchen 3,3-V-Karten einen Start mit einhergehendem Schaden verhindert. Diesen Fall zeigen sie meist akustisch oder via LED an.

Es gab Hauptplatinen-Modelle mit Intel 845, Intel 850 und Nvidia nForce. Diese besaßen versehentlich einen AGP-Universalanschluss, obwohl sie AGP 1.0 nicht unterstützen. Auch hier bewirkte das so mögliche Einsetzen einer 3,3-V-Karte meist eine spannungsverursachte Beschädigung beider Komponenten. Die Verwendung dieser Universalanschlüsse könnte an den tatsächlich universalkompatiblen Vorgängermodellen der Chipsätze liegen. So unterstützte der Intel 815-Chipsatz AGP 1.0 und 2.0-Karten. Er erschien zwei Jahre vor dem i845, welcher die Unterstützung für Version 1.0 und damit allen 3,3 V-Karten kappte.

AGP 3.0-Karten haben standardgemäß den gleichen Anschluss wie 2.0-Karten, verwenden daher jedoch alle die vorgesehene Schutzschaltung, um beim Betrieb in 2.0-Mainboards mit deren höherer Spannung keinen Schaden zu nehmen. Zur Kompatibilität siehe obigen Absatz zu AGP 3.0.

Aperture Size ist eine Menge an Arbeitsspeicher, die der Grafikchip über den AGP-Port zur Speicherung von Texturen verwenden kann. Er wird auch als AGP Memory bezeichnet und sollte günstige Grafikkarten durch geringere Mengen lokalen, auf der Grafikkarte verbauten Speichers ermöglichen. Der Aperture-Bereich wird vom Betriebssystem nicht ausgelagert, um hohe Geschwindigkeit zu sichern, ist im Vergleich zum lokalen Grafik-Speicher jedoch langsamer (in einem konkreten Beispiel sank die Rendering-Geschwindigkeit hierdurch um rund 30 % ab). Es gibt zwei Modi der Verwendung, deren Bezeichnung oft vermischt wird. Im "DiME"-Modus ("Di"rect "M"emory "E"xecute) kann der Grafikchip "nur" Texturen aus dem Aperture-Bereich zur Fertigstellung eines Bildes verwenden, sein lokaler Speicher wird ausschließlich für Rendering und Framebuffer verwendet. Der Intel-i740-Chip nutzte diesen Modus zur Demonstration der Aperture, obwohl er auch zum "DiMEL"-Modus ("Di"rect "M"emory "E"xecute and "L"ocal Memory) fähig war. Dieser Modus erlaubt die sinnvolle Kombination aus lokalem und AGP-Speicher. Hierzu werden die meistverwendeten Texturen lokal gehalten, was einem Textur-Cache des Grafikchips entspricht. Dieses Vorgehen beherrschten zuvor auch manche PCI-Grafikkarten; AGP bewirkte mit DiMEL jedoch den Vorteil der schnelleren AGP-Bandbreite zum Arbeitsspeicher sowie die Sicherheit, dass Texturen nicht auf die Festplatte ausgelagert werden.

System Memory bezeichnet normalen Arbeitsspeicher außerhalb der „aperture size“, welcher auch AGP-Grafikkarten wie ihren Vorgängern zur Verfügung steht. Der wohl erste Chip, der diesen Speicher nutzte, war der in vielerlei Hinsicht exotische nVidia NV1.

GART – Die zum Auslagern von Texturen vorgesehene Aperture Size wird im Arbeitsspeicher erst bei Bedarf reserviert. Zuvor steht sie dem System für andere Zwecke zur Verfügung. Kommt sie dann zum Einsatz, findet der im Mainboard-Chipsatz befindliche Speicher-Controller kaum einen zusammenhängenden Speicherabschnitt dieser Größe. Daher werden die Daten verteilt (fragmentiert) abgelegt. Dem Grafikchip wird jedoch ein zusammenhängender Speicher vermittelt. Die Übersetzung übernimmt eine GART ("Graphics Address Remapping Table"). Diese ähnelt der Memory Management Unit eines Hauptprozessors.

Sideband Addressing ist die zeitversetzte Übermittlung von Adressen auf dem AGP. Auf diesen acht „Sideband lines“ kann der Grafikchip neue Daten anfordern, während die aktuellen noch auf den 32 Adress- und Datenleitungen transferiert werden. Damit stehen die neuen Daten dann direkt nach Versendung der aktuellen zur Verfügung.

Fast-Writes wurde erst mit AGP 2.0 eingeführt und zuerst von den Nvidia-Geforce-256-Grafikkarten umgesetzt. Dabei können Grafikkarte und CPU Daten direkt austauschen, anstatt sie erst in den Arbeitsspeicher zu schreiben und von dem Empfänger nochmals von dort lesen zu lassen. Diese Transfer-Dopplung ohne Fast-Writes halbiert folglich effektiv die RAM-Bandbreite. Zum Tragen kommt das, sobald die Grafikkarte entsprechend viele Daten anfordert. Fast-Writes behebt dies und umfasst allen Datenaustausch, 2D wie 3D. Oft unterstützt die Grafikkarte jedoch AGP-Modi, die sie mit ihrer Rechenleistung gar nicht auslasten "kann". Speziell im Einsteigersegment wurden hohe AGP-Modi mehr wegen ihrer Vermarktungswirkung implementiert. Fehlt hier das Fast-Writes-Feature, bewirkt das demnach genauso wenig Nachteil wie bei allen AGP-Mainboards, bei denen die RAM-Bandbreite etwa dem Doppelten der AGP-Datenrate entspricht (wie bei Modellen mit Dual-Channel-DDR). Zudem achteten Spielehersteller meist auf eine möglichst bandbreitenschonende Programmierung, so dass die Maximalauslastung des AGP und damit der Bedarf an Fast-Writes seltener zustande kam. Herausforderung bei der Fast-Writes-Umsetzung ist ein sauberes Daten-Signal. Daher deaktivierte Intel im 815-Chipsatz diese Funktion ebenso wie der ATI-Catalyst-Treiber in als instabil diagnostizierten Systemen. Einen entsprechenden Test vollzieht er nach seiner Installation. Fast-Writes ist durch Heruntersetzen des AGP-Modus oft möglich, wo es zuvor instabil lief, und kann so dennoch zu einer Geschwindigkeitssteigerung verhelfen. Dies erfordert jedoch individuelle Messungen mittels Benchmarks. Fast-Writes ist für den AGP-2x-Modus aufwärts verfügbar.

Mit der Version AGP 8x war diese Technik zu Ende entwickelt, da sich aufgrund von Timing-Problemen bei hohen Taktungen, die durch die parallele Datenübertragung entstehen, und dem dadurch zunehmend komplizierter werdenden Platinendesign die Geschwindigkeit nicht weiter steigern lässt. Auch das besondere Merkmal von AGP, eine schnelle Punkt-zu-Punkt-Verbindung zum Arbeitsspeicher anzubieten, hat inzwischen an Wichtigkeit verloren, da bei rasant gestiegen Speicherchip-Kapazitäten die meisten Grafikkarten damit gut ausgestattet sind. Dazu kommt, dass AGP nur als Schnittstelle für Grafikkarten konzipiert ist, aber auch immer mehr andere Komponenten, die bis dahin immer noch über PCI oder PCI-X angebunden wurden, höhere Anforderungen bezüglich der Datenübertragungsrate aufweisen. Unter anderem deswegen hat die PCI-SIG den Standard PCI Express als Nachfolger von PCI und AGP entworfen, der eine noch größere Datenübertragungsrate als AGP bot.
Für die (nach z. B. Sockel 939) Chipsätze mit DDR2-800 Speicher Unterstützung wurden keine Windows-9x-Treiber mehr geliefert.
Seit Mitte 2006 wurden kaum noch neue Hauptplatinen für AGP-Grafikkarten vorgestellt. Grafikkarten für einen älteren 1-Kern-Prozessor AGP-Slot sind selten geworden.



</doc>
<doc id="9613" url="https://de.wikipedia.org/wiki?curid=9613" title="Speicherdirektzugriff">
Speicherdirektzugriff

Der Begriff Speicherdirektzugriff oder englisch Direct Memory Access (DMA) bezeichnet in der Computertechnik eine Zugriffsart, die über ein Bussystem direkt auf den Speicher zugreift.

Diese Technik erlaubt angeschlossenen Peripheriegeräten, wie z. B. Netzwerkkarte oder Soundkarte, ohne Umweg über die CPU direkt mit dem Arbeitsspeicher zu kommunizieren. Der Vorteil des Speicherdirektzugriffs ist die schnellere Datenübertragung bei gleichzeitiger Entlastung des Prozessors.

Speicherdirektzugriff ist mittels Remote Direct Memory Access auch über Netzwerkverbindungen möglich, sofern Hardware und Betriebssystem dies unterstützen.

Die konventionelle Methode, Daten von einer Eingabe-Ausgabe-Einheit in den Arbeitsspeicher zu transferieren, nutzt Prozessorregister zur Zwischenspeicherung. Erst werden die Daten vom Prozessor in sein internes Register eingelesen, um dann in einem weiteren Schritt in den Arbeitsspeicher verschoben zu werden. Dafür sind viele Taktschritte notwendig, in denen der Prozessor nicht für die Ausführung anderer Befehle zur Verfügung steht, und somit wird die Ausführungsgeschwindigkeit laufender Programme verringert.

Der direkte Speicherzugriff ist eine Schaltungs- und Steuermaßnahme, die über spezielle Datenleitungen auf dem Motherboard eine Verbindung zwischen Steckkarten (oder auch I/O auf dem Motherboard selbst) und dem Arbeitsspeicher herstellt. Dadurch können die Daten ohne Umweg über den Prozessor direkt in den Speicher geschrieben werden, und die Ausführungsgeschwindigkeit laufender Programme wird nicht beeinflusst.

Beim PC gibt es nur eine reale DMA-Leitung. Der Zugriff der unterschiedlichen Komponenten (Steckkarten) wird anhand eines Index unterschieden. Demnach darf ein DMA-Index nur einem Gerät zugeordnet werden. Der Index gibt die Nummer des DMA-Kanals an. Insgesamt gibt es 8 DMA-Kanäle. Die Kanäle 0, 2 und 4 sind für interne Zwecke bereits fest vergeben. Die Kanäle 1, 3, 5, 6 und 7 können frei belegt werden. Die Kanäle 0–3 arbeiten mit 8, die Kanäle 4–7 mit 16 Bit.

In modernen Chipsätzen ist der DMA-Controller (DMA-C) integriert. Ältere Motherboards haben einen separaten Baustein wie zum Beispiel den 8237 oder den 8257 von Intel.

Will die I/O-Hardware Daten senden oder empfangen, trennt der DMA-Controller den Prozessor vom Bussystem. Der DMA-Controller führt dann die Anforderung mit hoher Geschwindigkeit aus. Danach wird die Verbindung zwischen Prozessor und Bussystem wieder hergestellt. Für den Speichertransfer benötigt der Prozessor bis zu 40 Takte je Byte. Der DMA-Controller führt den Zugriff innerhalb von vier Takten aus.

Der DMA-Controller dient zum Datentransport zwischen Arbeitsspeicher und Peripherie. Dies führt zu einer Entlastung des Prozessors. Neben einem Geschwindigkeitszuwachs bei speicherintensiven Anwendungen ermöglicht die Verwendung von DMA-Controllern außerdem sehr hohe Datenraten z. B. beim Brennen von DVD-Medien. Selbst mit aktuellen PC-Systemen sind Brenngeschwindigkeiten von 16× ohne DMA-Unterstützung nicht möglich.

Der DMA-Controller muss die Daten zwangsläufig über dieselben Daten-, Adress- und Steuerleitungen des jeweiligen Bussystems übertragen (lassen) wie sonst die CPU. Es muss also dafür gesorgt werden, dass CPU und DMA-Controller nicht kollidieren. Dazu wird im Voraus eine „Arbitrierung“ durchgeführt, ein Verfahren, bei dem der DMA-Controller die Kontrolle über die Busse bei der CPU anfordert, letztere dies bei nächster Gelegenheit gewährt und dann die Busse freigibt. Nach Abschluss des DMA-Transfers wird die Busanforderung wieder zurückgenommen, und die CPU kann wieder übernehmen.

Elektronisch gibt es dazu verschiedene Implementierungen, mit verschiedenen Anzahlen von Steuerleitungen. Die einfachste Variante verfügt über eine "Bus-Request"-Leitung (Anforderung) vom DMA-Controller zur CPU sowie eine "Bus-Grant"-Leitung (Gewährung) in umgekehrter Richtung.

Man unterscheidet zwei verschiedene Adressierungsverfahren. Beim so genannten "Explicit Addressing" (auch "Two Cycle Transfer") holt der DMA-Controller zunächst ein Datenwort (oder auch -byte) ab und speichert dieses in einem internen Register (wie eine CPU). Danach adressiert er die Zielkomponente und überträgt ihr die Daten. Für dieses Vorgehen werden also zwei Buszyklen benötigt. − Beim "Implicit Addressing" (oder "Single Bus Transfer") entfällt die Zwischenspeicherung in einem Register: Der DMA-Controller adressiert das zu holende Datenwort und stellt dieses sofort zum Zielbaustein durch. Es wird nur ein einziger Buszyklus benötigt. Dieses Verfahren ist nicht für Speicher-zu-Speicher-Übertragungen geeignet, da jeweils nur eine Adresse am Speicher anliegen kann.

Effizient wird das DMA-Verfahren allerdings erst, wenn nicht nur ein einzelnes Datenwort zu übertragen ist, sondern größere zusammenhängende Speicherbereiche, z. B. ganze Datensektoren oder -spuren von einer Festplatte. Dann lohnt sich auch der gewisse Overhead, der dadurch entsteht, dass zuallererst der DMA-Controller durch Setzen diverser Registerinhalte für die bevorstehende Aufgabe aufgesetzt werden muss.

Die klassische DMA-Technik des ISA-Busses basiert dabei auf der Signalisierung der Peripherie für Transferbedarf durch einzelne Datenleitungen, die so genannten DMA-Kanäle. Wird nun solch ein Kanal angesteuert, so liefern Schaltkreise, die der Bus-Steuerung zugehörig sind, Adressen auf den Bus, die je Zyklus inkrementiert oder konstant gehalten sein können, während der initiierende periphere Schaltkreis entweder die aufgeschalteten Daten entgegennimmt oder aber selbst Daten auf den Bus aufschaltet. Da ein Speicherdirektzugriff die normale Bus-Tätigkeit unterbricht, ist eine Operation mit der CPU in diesem Zeitraum nicht möglich. Da es mehrere DMA-Initiatoren gibt, muss mit entsprechenden Prioritätsschemata gearbeitet werden. Weiterhin muss ein DMA-Kanal zunächst initialisiert werden und die Beschaltung des Busses mit Adress- und Steuersignalen muss ebenso erst per Software vorgegeben werden. Nach erfolgreichem Transfer wird meist ein Interrupt ausgelöst, der der System-Software mitteilt, dass der Vorgang abgeschlossen ist. Daraufhin werden die DMA-Schaltkreise meist so umprogrammiert, dass sie auf den nächsten zu bearbeitenden Datenblock verweisen.

Als DMA-Controller im weiteren Sinne muss jeder Baustein verstanden werden, der einen Speicherzugriff (entweder auf den Hauptspeicher selbst oder auf eine Peripheriekomponente) durchführen kann, ohne dabei die CPU in Anspruch zu nehmen. Dieses Grundprinzip kann jedoch vielfach variiert werden. So gibt es etwa Systeme mit einem zentralen DMA-Coprozessor, aber auch Systeme, in denen jede Komponente über einen eigenen, dezentralen DMA-Controller verfügt. Das letztere Verfahren unterscheidet sich von der Verwendung eines zentralen Controllers dadurch, dass keine DMA-Kanäle existieren. Vielmehr reserviert der anfordernde Schaltkreis den Bus für sich und adressiert diesen auch selbst. Jede Komponente, wie Prozessor oder periphere Bauteile können dabei der Initiator sein, der die Befehlsmacht über den Hauptspeicher (Adressierung, Datentransfer und Bussteuersignale) übernimmt. Der Busmaster überträgt nun seinem Ziel, dem "Target", die für ihn vorgesehenen Daten. Zu einem bestimmten Zeitpunkt kann prinzipiell immer nur eine Komponente Busmaster sein. So können etwa PCI-Controller zugleich auch DMA-Busmaster sein. 




</doc>
<doc id="9647" url="https://de.wikipedia.org/wiki?curid=9647" title="Zeugung">
Zeugung

Unter Zeugung (von althochdt. "giziogon" „beschaffen, fertigen“), Fertilisation oder Befruchtung werden die Vorgänge verstanden, die zur Bildung einer Zygote aus einer Eizelle (Oozyte) führen. Unter anderem beim Menschen verschmelzen dazu zwei Keimzellen: das männliche Spermium und die weibliche Eizelle.

Entstehen die Nachkommen aus unbefruchteten Eizellen, wird von einer Jungfernzeugung oder Parthenogenese gesprochen.

Bei einem Samenerguss während eines Vaginalverkehrs gelangen ca. drei bis fünf ml Sperma (Samenflüssigkeit) des Mannes in die weibliche Scheide. Das entspricht etwa 200-600 Millionen Spermien. Während der meisten Zeit des Zyklus sterben sie im sauren Milieu der Scheide innerhalb von ½ -3h ab. Sobald während der Follikelreifungsphase - östrogenstimuliert – Zervixschleim gebildet wird, können die Spermien in diesem Zervixschleim in den Zervixdrüsen des Gebärmutterhalses bis zu 5 Tage überleben und auf einen Eisprung warten. Auf ihrem Weg zum Eileiter werden die Spermien erst zeugungsfähig, indem bestimmte Proteine im Sperma und an den Spermien durch weibliche Enzyme entfernt werden. Der Vorgang der Kapazitation dauert einige Stunden, in denen die Spermien kein Ei befruchten können. Nur wenige Hundert Spermien erreichen den Eileiter. Diese sich bei der Frau abspielenden Vorgänge und alle nachfolgenden hier behandelten Abläufe werden insgesamt als Empfängnis bezeichnet.

Die weitere Entwicklung hängt davon ab, ob sich in einem der Eileiter eine befruchtungsfähige Eizelle befindet oder ob ein Eisprung bevorsteht. Die Spermien können sich mehrere Tage lebend im Eileiter aufhalten. Wenn ein Eisprung erfolgt ist, bewegen sich die Spermien auf die Eizelle zu. Für das Auffinden der Eizelle werden hormongesteuerte Abläufe vermutet oder auch der Einfluss eines Stoffes, der von der Eizelle abgegeben wird, für den die Spermien einen Duftrezeptor (OR1D2 und hOR17-4) besitzen . Die Spermien bewegen sich vermutlich dem Konzentrationsgefälle eines Duftstoffes entgegen. Neuere Untersuchungen zeigten, dass der Duftstoff Bourgeonal, welcher auch im Duft von Maiglöckchen vorhanden ist, den o. g. Duftrezeptor in den Spermien aktiviert und so zu einer positiven Chemotaxis führt. Allerdings spielen wahrscheinlich auch noch andere Faktoren eine Rolle. Nach neusten Erkenntnissen wird ein Steuerungssystem der Spermien durch das weibliche Hormon Progesteron aktiviert, welches den Eizellen als anlockender Botenstoff dient, welcher in den Spermien die Kalzium-Konzentration ansteigen lässt. Das Schlagmuster ihres Schwanzes wird dadurch verändert, so dass die Spermien die Eizelle ansteuern können.

Eizellen sind von einer lockeren Schicht von Follikelzellen umgeben. Über der Plasmamembran der Eizelle befindet sich außerdem die Zona pellucida, eine Schicht von verschiedenen engmaschigen Glykoproteinen. Durch beide muss das Spermium, um mit der Eizelle verschmelzen zu können. Spezielle Glykoproteine der "Zona pellucida" binden an auftreffenden Spermienköpfen und lösen die Fusion des Akrosoms mit der "Zona pellucida" aus. Die im Akrosom gespeicherten Enzyme lösen die "Zona pellucida" auf und ermöglichen dem Spermium, die darunter liegende Plasmamembran zu erreichen. Die Akrosomreaktion bewirkt weiter eine Aktivierung und Exponierung von Proteinen auf dem Spermium, die spezifisch an passenden Rezeptoren auf der Plasmamembran des Eis binden.

Bei Kontakt verschmelzen nun Spermium und Eizelle und das gesamte Spermium wird in die Eizelle gezogen. Gleichzeitig wird die Membran der Eizelle depolarisiert, wodurch für kurze Zeit eine weitere Befruchtung mit anderen Spermien verhindert wird (Polyspermie). Die Depolarisation wird durch den „langsamen Block gegen Polyspermie“ abgelöst, bei dem die Zona pellucida durch Enzyme aus den Corticalgranula ihre Struktur verändert und sich gegen weitere Penetration durch Spermien schützt.

Ist das Spermium in die Eizelle aufgenommen worden, beendet diese ihre zweite Reifeteilung oder Meiose II. Sie schnürt hierbei ein letztes Polkörperchen ab. Wie vorher hat sie jetzt nur noch den haploiden Ein-Chromatid-Chromosomensatz vorliegen. Die männlichen und weiblichen haploiden Ein-Chromatid-Chromosomen verwandeln sich zu haploiden Zwei-Chromatid-Chromosomen. So bilden sich der männliche und der weibliche Vorkern. Diese vereinigen sich jetzt zu einem vollständigen, 2-fachen oder diploiden Zwei-Chromatid-Chromosomensatz. Damit ist der Vorgang der eigentlichen Zeugung abgeschlossen, es hat sich eine befruchtete Zelle gebildet, die Erbgut beider Elternteile in sich trägt.

Die befruchtete Zelle, die als Zygote bezeichnet wird, beginnt einen Tag nach der Zeugung mit der Zellteilung. Sie erreicht nach etwa drei Tagen im 12- bis 16-Zellen-Stadium die Gebärmutter, wo sie sich etwa fünf bis sechs Tage nach dem Eisprung in der Gebärmutterschleimhaut einnistet, womit nach vorherrschender Meinung die Schwangerschaft beginnt. War die Einnistung erfolgreich, wird folglich das humane Choriongonadotropin (kurz hCG - Hormon")" steigend produziert, wodurch ab diesem Tag ein vorperiodlicher Schwangerschaftstest möglich ist.

Bis ca. zu den 60er Jahren des 20. Jahrhunderts wurden im Frühling in den meisten Gesellschaften der nördlichen Hemisphäre nachweislich mehr Kinder gezeugt – bis zu 10 % über dem Jahresmittel – als in den anderen Jahreszeiten. In den 60er und 70er Jahren änderte sich dies vergleichsweise abrupt: das Maximum verlor an Ausprägung und erstreckte sich auch in den Sommer und teilweise sogar den Herbst hinein. Eine naheliegende Erklärung für diesen Vorgang ist die zunehmende Unabhängigkeit des Menschen von jahreszeitlichen Schwankungen der Temperatur und des Sonnenlichts durch fortschreitende Technik und immer größere Anteile der Bevölkerung, die von meteorologischen Bedingungen unabhängige Berufe ausüben.




</doc>
<doc id="9650" url="https://de.wikipedia.org/wiki?curid=9650" title="Dunkelziffer">
Dunkelziffer

Als Dunkelziffer bzw. auch „Dunkelzahl“ wird in der Regel das "Verhältnis" zwischen der Zahl der statistisch ausgewiesenen und der wirklich begangenen Straftaten verstanden. Die Bezeichnung stammt aus der Kriminalstatistik, heute wird sie auch erweitert verwendet.

Ursprünglich beruht sie auf einer falschen Übersetzung des englischen Ausdrucks „dark number“ (Dunkelzahl) in einer deutschen Dissertation, die im Jahr 1908 von einem japanischen Staatsanwalt (Shigema Oba) angefertigt wurde. In der Folgezeit hat sie sich in der deutschsprachigen Forschung eingebürgert, auch wenn die Dunkelziffer keine Ziffer, sondern eine Zahl bezeichnet.

Im Bereich der Kriminologie ist die Dunkelziffer weitgehend identisch mit dem Dunkelfeld. Basierend auf der Erhebung empirischer Daten durch repräsentative Befragungen sieht man sich heute in der Lage, in gewissem Rahmen die Dunkelziffer zu schätzen und das Dunkelfeld aufzuhellen. Eine exakte Aufklärung der Dunkelziffer ist jedoch nicht möglich.

Dunkelziffer wird auch bei Straßenverkehrsunfällen verwendet. Hier bezeichnet er Unfälle, die der Polizei nicht zur Kenntnis gelangen und die damit nicht in die amtliche Unfallstatistik eingehen. Dies betrifft besonders Alleinunfälle, aber insbesondere auch bei Fahrradunfällen und Fußgängerunfällen mit Kfz werden auch Unfälle mit Personenschaden häufig nicht gemeldet. Durch Befragungen oder Vergleiche mit ärztlichen oder Krankenhausunterlagen ist die Dunkelziffer abgeschätzt worden. Demnach gelangen beispielsweise fast 99 Prozent der Fahrrad-Alleinunfälle der Polizei nicht zur Kenntnis, bei Unfällen zwischen Rad- und Pkw-Fahrern gehen 82 Prozent der Unfälle nicht in die Verkehrsunfallstatistik ein.

Im übertragenen Sinn wird Dunkelziffer auch im Gesundheitswesen verwendet und bezeichnet ein Missverhältnis von diagnostizierten (oder auch statistisch erfassten bzw. gemeldeten) Krankheitsfällen zur tatsächlichen Krankheitshäufigkeit (Prävalenz).




</doc>
<doc id="9653" url="https://de.wikipedia.org/wiki?curid=9653" title="Bilanz">
Bilanz

Bilanz (; aus „doppelt“ und „Schale“) ist ein in vielen Fachgebieten vorkommender Begriff, worunter allgemein eine nach bestimmten Kriterien gegliederte, summarische und sich ausgleichende Gegenüberstellung von Wertkategorien verstanden wird.

Oftmals wird der Ausdruck Bilanz lediglich mit dem herkömmlichen betriebswirtschaftlichen Rechnungswesen eines Unternehmens assoziiert. In diesem Sinne wird der Begriff Bilanz dann synonym zu den Begriffen Abschlussbilanz, "balance sheet" (englisch), Bilanzrechnung, Handelsbilanz, Konzernabschluss, HGB-Bilanz, Jahresbilanz, oder Unternehmensbilanz gebraucht.

Je nach den geltenden Vorschriften gibt es verschiedene Bilanzen wie etwa die Steuerbilanz und die Vermögensbilanz. Neben den vorgeschriebenen Bilanzen werden mikro- und makroökonomische Bilanzen mit anderem Inhalt und Verwendungszweck wie Zahlungsbilanz, volkswirtschaftliche Handelsbilanz, Devisenbilanz, Kapitalbilanz erstellt; diese dienen entweder zur internen Information innerhalb eines Unternehmens (siehe Controlling) oder in der Öffentlichkeit zur Darstellung im Zusammenhang mit wirtschaftspolitischen Erörterungen.

Darüber hinaus gibt es Bilanzen, die wirtschaftliche Aspekte im Zusammenhang mit anderen sehen wie Sozialbilanz, Gemeinwohl-Bilanz, Umweltbilanz und Energiebilanz, wobei zum Teil versucht wird, entsprechende Umrechnungen von Faktoren vorzunehmen, um ihren Geldwert zu bemessen oder einen einheitlichen Maßstab festzustellen.

Des Weiteren wird der Begriff verwendet, um das wirtschaftliche Rechnungs- und Vergleichs-Denken auf andere Bereiche zu übertragen wie etwa die Wissensbilanz.

Die hier behandelte "Unternehmensbilanz" ist aus rechtlicher Sicht eine systematische Aufstellung von geldwerten Rechten (Vermögen), Pflichten (Schulden) und dem Nettovermögen, das sich aus der Summe der geldwerten Rechte (Bruttovermögen) abzüglich der Schulden ergibt. Die Rechte umfassen dabei Eigentumsrechte (Sach- und geistiges Eigentum wie Urheber- und Markenrechte, Patente etc.) und Forderungen. Vermögensrechte werden auf der Aktivseite, Pflichten (Schulden) und Nettovermögen auf der Passivseite verbucht (nur Insolvenz anzeigendes negatives Nettovermögen wird aus Gründen der Bilanzidentität auf der Aktivseite verbucht). Das Netto-Geldvermögen ergibt sich dabei aus den Zahlungsmitteln plus den sonstigen Forderungen abzüglich der Verbindlichkeiten. Nettogeldvermögen und Eigentumsrechte ergeben aufaddiert das Nettovermögen oder Eigenkapital. Ist dieses positiv, wird es (als Überschuss des Bruttovermögens über die Schulden) auf der Passivseite, ist es negativ, wird es als Überschuss der Schulden über das Bruttovermögen auf der Aktivseite verbucht, um auf beiden Seiten dieselbe Bilanzsumme zu erhalten.

Aus kaufmännischer Sicht stellt eine Bilanz eine summarische Gegenüberstellung von Verwendung (Aktiva, gegliedert nach Anlage- und Umlaufvermögen) und Herkunft der Mittel eines Unternehmens (Passiva) dar. Das Vermögen eines Unternehmens (Aktiva) kann dabei aus Eigen- oder Fremdkapital (Passiva) herstammen (betriebswirtschaftlicher Kapitalbegriff). Die Bilanz ist ein Bestandteil des Jahresabschlusses eines Unternehmens und dient Gläubigern, Ratingagenturen, Anteilseignern, Arbeitnehmern, dem bilanzierenden Unternehmen und dem Staat zur Orientierung über die Vermögens-, Finanz- und Ertragslage des entsprechenden Unternehmens.

Eine Bilanz wird auf einen Bilanzstichtag aufgestellt, während die (mit der Bilanz verzahnte) Gewinn- und Verlustrechnung (GuV) für einen Zeitraum erstellt wird. Durch Vergleich der Endbestände der verschiedenen Aktiv- und Passivkonten zu unterschiedlichen Zeitpunkten kann die wirtschaftliche Entwicklung eines Unternehmens zeitübergreifend dargestellt und durch Einsichtnahme in die Buchführung nachvollzogen werden. Die Bilanz ist somit auch die Grundlage zur betriebswirtschaftlichen Gewinnermittlung (siehe Betriebsvermögensvergleich).

Eine systematisch aus der Buchhaltung abgeleitete Bilanz nach heutiger Vorstellung wurde erstmals 1494 durch den Franziskaner und Mathematiker Luca Pacioli in seinem Buch "Summa de arithmetica, geometria, proportioni et proportionalità" beschrieben, jedoch nachweislich wesentlich früher bereits in Genua und anderen italienischen Städten angewendet. Es handelte sich um die erste geschlossene Darstellung der „venezianischen Methode“ (doppelte Buchführung), wie sie vermutlich in den Fernhandel treibenden italienischen Stadtstaaten ausgeübt wurde. Pacioli verstand unter der Bilanz „ein der Länge nach zusammengefaltetes Blatt, auf dem man rechts die Gläubiger und links die Schuldner aufschreibt. Wenn Du siehst, dass die Sollsumme so viel beträgt wie die des Habens, so ist das Hauptbuch in Ordnung“. Seine „venezianische Methode“ mit dem Prinzip der Doppik hat in ihren Grundzügen noch heute international Geltung.

Im Jahre 1511 stellte anlässlich einer Erbteilung (Tod der Brüder Georg und Ulrich Fugger) der Hauptbuchhalter der Fugger, Matthäus Schwarz, die erste Unternehmensbilanz in Deutschland auf. Das Preußische Allgemeine Landrecht vom Juni 1794 führte die Bilanzierungspflicht ein, denn „ein Kaufmann, welcher entweder gar keine ordentliche Bücher führt, oder die Balance seines Vermögens, wenigstens alljährlich einmal zu ziehen unterlässt, und sich dadurch in Ungewissheit über die Lage seiner Umstände erhält, wird bei ausbrechendem Zahlungsunvermögen als fahrlässiger Bankerutirer (Bankrotteur) bestraft.“ Im Mai 1861 gab es erste einheitliche gesetzliche Regelungen im Allgemeinen Deutschen Handelsgesetzbuch, das in Art. 31 ADHGB vom Kaufmann die Aufstellung einer Inventur und Bilanz forderte. Das HGB vom Mai 1897 übernahm diese Vorschrift fast wörtlich in § 39 HGB. Im Oktober 1937 brachte das Aktiengesetz Verschärfungen der Bilanzierungspflicht für Aktiengesellschaften. Das Publizitätsgesetz sorgte ab August 1969 für die Offenlegung von bisher nicht publizitätspflichtigen Unternehmensbilanzen von Personengesellschaften und Einzelunternehmen ab einer bestimmten Betriebsgröße. Im Dezember 1985 gab es mit dem Bilanzrichtlinien-Gesetz eine erste EU-einheitliche Regelung, im Mai 2009 brachte das Bilanzrechtsmodernisierungsgesetz insbesondere eine Deregulierung und Aufwandssenkung zu Gunsten kleiner und mittlerer Unternehmen. Das Bilanzrichtlinie-Umsetzungsgesetz vom Juli 2015 ist das Transformationsgesetz der EU-Bilanz-Richtlinie. Außer den Änderungen der Größenklassen hat es keine wesentlichen Auswirkungen auf die Bilanz.

Die betriebswirtschaftliche Forschung hat sich mit der Bilanz erstmals umfassend auseinandergesetzt, nachdem ein Urteil des Reichsoberhandelsgerichts vom Dezember 1873 eine breite wissenschaftliche Diskussion über Bilanztheorien ausgelöst hatte. Es forderte unter anderem, dass Bilanzen „der objektiven Wahrheit möglichst nahe kommen“ sollten. Der Bilanzjurist Hermann Veit Simon beschrieb deshalb erstmals 1899 die Bilanzierungsgrundsätze Bilanzwahrheit und Bilanzklarheit. Eugen Schmalenbach betonte bereits 1919 in seiner dynamischen Bilanztheorie die Bilanz als „formal ausgeglichene Aufstellung der Vermögens- und Kapitalteile einer Unternehmung, bei der der erfolgsrechnerische Gesichtspunkt im Vordergrund steht“. Im Jahre 1921 erschien vom Schweizer Johann Friedrich Schär das grundlegende Werk „Buchhaltung und Bilanz auf wirtschaftlicher, rechtlicher und mathematischer Grundlage“, in dem er die Gewinn- und Verlustrechnung als Teil der Bilanz interpretierte. Erich Kosiol sah 1940 die Bilanz als systematischen Abschluss der Buchhaltung, Erich Gutenberg bezeichnete die Bilanz als Kontrollinstrument mit spezifischen Kontrollzwecken. Inzwischen ist die Bilanztheorie zu einem wichtigen Bestandteil der Principal-Agent-Theorie und damit der Organisationstheorie geworden.

Bei Unternehmensbilanzen gibt es folgende Arten:

Die Bilanz erfüllt folgende Funktionen:

Umgangssprachlich bezeichnet Bilanz den gesamten Jahresabschluss eines Unternehmens. Die Beurteilung eines Unternehmens auf Grund seines Jahresabschlusses heißt in diesem Sinne Bilanzanalyse. Sie ist ein wesentlicher Bestandteil der Fundamentalanalyse.

Die Informationsfunktion steht bei Abschlüssen nach den IFRS (International financial reporting standards) im Vordergrund. Die Aktivitäten internationalisierter Kapitalmärkte bedürfen einheitlicher Regelungen, nach denen der Erfolg des Unternehmens bemessen wird. Konzernabschlüsse nach HGB oder IFRS weisen aber nicht den ausschüttungsfähigen Gewinn aus. Dieser wird bei Unternehmen mit Sitz in Deutschland weiterhin ausschließlich nach dem HGB ermittelt.

Die Bilanz wird in zwei Bereiche aufgeteilt dargestellt.


In den meisten Staaten gibt es ein gesetzlich vorgegebenes Gliederungsschema, das Unternehmensbilanzen vereinheitlichen soll und im Rahmen der Bilanzklarheit dem Bilanzleser einzelne Gliederungsposten mit gleichem Inhalt vermitteln soll. Die "Bilanzposition" ist ein Posten in diesem Gliederungsschema der Bilanz, der einen bestimmten Vermögensgegenstand, Eigenkapitalbestandteil oder Schuldenposten beinhaltet.

Die Gliederung erfolgt als gesonderter Ausweis von Anlage- und Umlaufvermögen, Eigenkapital, Schulden sowie Rechnungsabgrenzungsposten (). Gemäß HGB ist eine nach dem deutschen Handelsgesetzbuch i. d. F. v. 20. Dezember 2012 (BGBl I S. 2751) aufgestellte Bilanz einer Kapitalgesellschaft aufgebaut.

Erleidet die Unternehmung einen so hohen Verlust, dass das Eigenkapital rechnerisch negativ wird, so wird bei Kapitalgesellschaften der „Negativbetrag“ (mathematisch korrekt als Absolutwert des negativen Eigenkapitals) am Schluss der Bilanz auf der Aktivseite gesondert als „nicht durch Eigenkapital gedeckter Fehlbetrag“ ausgewiesen, Abs. 3 HGB. Bei Personenhandelsgesellschaften entspricht dies dem „nicht durch Vermögenseinlagen gedeckten Verlustanteil“ der persönlich haftenden Gesellschafterin oder der Kommanditisten nach HGB.

Abweichungen von diesem Bilanzierungsschema ergeben sich aufgrund der Rechtsform (nach HGB), Betriebsgröße (nach HGB) oder Branche (wie Kreditinstitute und Versicherungen). Es können einzelne wesentliche Geschäftsvorfälle oder Posten nach HGB Anpassungen erforderlich machen. Für Kredit- und Finanzdienstleistungsinstitute, Versicherungen und Pensionsfonds bestehen eigenständige Bilanzgliederungsschemata nach gesonderten Vorschriften (Verordnung über die Rechnungslegung der Kreditinstitute und Finanzdienstleistungsinstitute oder Verordnung über die Rechnungslegung von Versicherungsunternehmen).

Im schweizerischen Recht sind die kaufmännischen Buchführungsvorschriften im Obligationenrecht (OR) geregelt. Für die Bilanz verlangt Art. 959a OR folgende "Mindestgliederung".

In der Praxis werden die Aktiven nach dem Liquiditätsprinzip, die Passiven nach dem Fälligkeitsprinzip geordnet. Das heißt, je schneller ein Aktivposten flüssig gemacht werden kann oder ein Passivposten fällig wird, desto weiter oben wird er platziert. Der Vergleich derart geordneter Aktiven und Passiven gibt einen Hinweis auf die "Liquidität" der Unternehmung.

Die Bankbilanz gibt Aufschluss über die Liquiditätslage und die Risikosituation eines Kreditinstituts. Die Gliederung erfolgt nach HGB in Verbindung mit Formblatt 1 der RechKredV auf der Aktivseite nach abnehmender Liquidität, sie beginnt mit dem Kassenbestand. Man unterscheidet statische und dynamische Liquidität. Auf der Passivseite steht das Fremdkapital vor dem Eigenkapital; sie gliedert sich nach zunehmender Fristigkeit.

Die Aktivseite differenziert zwischen Forderungen und Wertpapieren; dagegen wird nicht zwischen Anlage- und Umlaufvermögen unterschieden. Die Sachanlagen finden sich unter Sonstige Aktiva wieder. Die Passivseite unterscheidet Verbindlichkeiten und verbriefte Verbindlichkeiten.

Die Aussagefähigkeit der Bankbilanz hinsichtlich der Liquiditätslage ist eingeschränkt. Es handelt sich um eine stichtagsbezogene Rechnung. Außerdem erfordert die Objektivierung, beispielsweise hinsichtlich der Laufzeit oder der Ansatz- und Bewertungsvorschriften, eine Differenzierung nach unterschiedlicher Liquidität.

Die Bankbilanz berücksichtigt Bonitätsrisiken, aber keine Zinsänderungs- und Wechselkursrisiken. Nachrangige Forderungen werden aus der Bilanz ausgegliedert oder im Anhang aufgeführt.

Eine nach International Financial Reporting Standards (IFRS) aufgestellte Bilanz unterscheidet sich in ihrem Aufbau von einer Bilanz nach deutschem HGB oder anderen nationalen Regeln. Der Aufbau einer IFRS-Bilanz ist in IAS 1, Paragrafen 51-77 („Balance Sheet“) dargestellt, wobei IAS 1.51 den Unternehmen zwei grundsätzliche Möglichkeiten zur Bilanzgliederung einräumt.


Eine eindeutige Vorgabe, ob diese Gliederung in aufsteigender oder in absteigender Ordnung zu erfolgen hat, existiert dabei nicht.

Allerdings präferiert das Rechnungslegungs Interpretations Committee (RIC) des Deutschen Rechnungslegungs Standards Committee e. V. (DRSC) in RIC 1 (Rechnungslegungs Interpretation Nr. 1 „Bilanzgliederung nach Fristigkeit gemäß IAS 1 "Darstellung des Abschlusses"“) als Leitlinie für die Bilanzierung deutscher Unternehmen nach IFRS offenbar die Reihenfolge langfristig – kurzfristig (vgl. Anhang zu RIC 1: Beispiel für ein Bilanzgliederungsschema).

Grundlage bei der Erstellung einer Bilanz ist die ordnungsmäßige Buchführung. Die Bilanz soll ein gerechtes, zutreffendes und nachvollziehbares Bild des Unternehmens zum Stichtag zeichnen. Dies wird als Prinzip der Bilanzwahrheit und Prinzip der Bilanzklarheit bezeichnet. Zusätzlich gilt das Vorsichtsprinzip, ungenau bezifferbare Bestände sollten eher pessimistisch eingeschätzt und mögliche Risiken gegebenenfalls berücksichtigt werden. Geregelt sind diese Normen in den Grundsätzen ordnungsmäßiger Buchhaltung (GoB).

Wer eine Bilanz aufzustellen hat ist im Handelsgesetzbuch geregelt. Dort ist der gesetzlich vorgeschriebene Aufbau zu finden.

In die Bilanz müssen alle Fakten einbezogen werden, die zum Zeitpunkt der Bilanz bekannt und für den Zeitraum zwischen zwei Bilanz-Stichtagen relevant sind. Daher genügt es nicht, einen zum jeweiligen Stichtag vorliegenden Kontostand in die Bilanz aufzunehmen. Zusätzlich müssen im Zeitraum vor dem Bilanzstichtag bezogene, noch nicht bezahlte Leistungen bewertet werden. Es muss ebenfalls festgestellt werden, welche Zahlungen bereits für Leistungen geleistet wurden, die erst im folgenden Jahr bezogen werden − beispielsweise eine Vorauszahlung für Rohstofflieferungen.

Eine Schwierigkeit bei der Erstellung von Bilanzen ist daher, dass zu einem Zeitpunkt selten alle zu berücksichtigenden Fakten bereits bezifferbar sind. So ist zum Beispiel bekannt, dass ein Unternehmen für den Monat Dezember eine Telefonrechnung erhalten wird. Da auch die Nutzung dieser Leistung bereits im Dezember erfolgte, muss die berechtigte Forderung des Anbieters in die Bilanz einfließen. Die entsprechende Rechnung liegt jedoch möglicherweise erst Ende Januar des Folgejahres vor. Somit ist es praktisch unmöglich, eine Bilanz sowohl präzise als auch zeitnah zu erstellen. Entsprechend vergehen bei großen Unternehmen zumeist zwei bis vier Monate bis zur Bekanntgabe der ordentlichen Bilanz. Andererseits wird gerade bei börsennotierten Unternehmen eine zeitnahe Bilanz erwartet, so dass – und diese Tendenz verschärft sich zunehmend – oftmals zu Lasten der Genauigkeit eine möglichst schnelle Bilanz erstellt wird, in der viele Werte nur geschätzt werden konnten.

Des Weiteren fordert die umfassende Darstellung des finanziellen Bildes eine tatsächliche Bestandsaufnahme zum Zeitpunkt der Bilanzerstellung. Für vorhandene Waren erfolgt dies zumeist in Form einer Inventur, in der mögliche Differenzen zwischen den erfassten Lagerveränderungen und den tatsächlich vorhandenen Beständen festgehalten werden können.

Schließlich erfolgt die Bewertung der Anlagegüter eines Unternehmens. Hier müssen für die in der Produktion genutzten Mittel (z. B. Maschinen) sowie für die langfristigen Finanzanlagen (z. B. Immobilien und Unternehmensbeteiligungen) realistische Werte ermittelt werden. Dies kann über Abschreibungen erfolgen, so dass der Wert eines Firmenfahrzeuges über die geplante Nutzung von (beispielsweise) acht Jahren gleichmäßig in jedem Jahr um ein Achtel des Kaufpreises herabgesetzt wird (lineare Abschreibung). Ein anderer Ansatz ist die Ermittlung des theoretischen Verkaufspreises, der insbesondere für Investitionen in öffentlich gehandelte Werte (Aktien anderer Unternehmen) angebracht ist. Hier können beispielsweise die im Besitz des Unternehmens befindlichen Aktien zum Kurs des letzten Handelstages vor dem Bilanzstichtag bewertet werden.

Die Bilanzpolitik bezeichnet die Grundausrichtung des Bilanzierenden bei der Nutzung von Ausweis-, Gliederungs- und Erläuterungswahlrechten (formelle Bilanzpolitik) sowie bei der Nutzung von Bilanzierungs- und Bewertungswahlrechten, Ermessensspielräumen und Sachverhaltsgestaltungen (materielle Bilanzpolitik).

Auch die Ausrichtung der Bilanzpolitik eines Unternehmens ist nicht zu vernachlässigen. So kann diese die Bilanz durch beispielsweise verschiedene Bewertungsansätze und diverse Darstellungsmöglichkeiten von Sachverhalten verändern.

Zum Schutz der Gläubiger eines Unternehmens gilt bei der Bewertung von Vermögenswerten das so genannte Vorsichtsprinzip, welches seine konkrete Anwendung in folgenden Bewertungsprinzipien findet:



Spätestens seit dem Beginn des Informationszeitalters zeigt sich, dass der Wert von immateriellen Aktiva für die Bewertung eines Unternehmens eine wachsende Bedeutung erlangen kann. So können außerordentliche Kenntnisse ("Unternehmenswissen" aus Wissensmanagement oder Wissensbilanz) einen erst in Zukunft in finanziellen Erfolg umsetzbaren Marktvorteil erbringen. Ebenso gelten erfolgreich eingeführte Marken als wertvolles Eigentum, sie helfen bei der Schaffung von Kundenvertrauen und Kundenbindung.

Die Schwierigkeit bei der Präsentation einer realistischen wirtschaftlichen Darstellung liegt jedoch darin, diesen durchaus relevanten immateriellen Gütern einen angemessenen Wert zuzusprechen, also den Markenwert zu ermitteln. Da der zukünftig aus diesen Gütern erwachsende Ertrag nicht realistisch vorhersehbar ist, dürfte eine unternehmensübergreifende Struktur für diese Darstellung noch einige Jahre zu erforschen sein.

Besonders bei Dienstleistungsunternehmen kann der Geschäftserfolg – und damit der Wert des Unternehmens – möglicherweise von Aktiva abhängen, die sich bilanziell bislang gar nicht erfassen lassen, insbesondere den Mitarbeitern mit ihren Fähigkeiten und Kenntnissen. Die Mitarbeiter gehen auf Basis der mit ihnen geschlossenen Arbeitsverträge ihrer Tätigkeit zum Wohle des Unternehmens nach. Sie – und nicht Maschinen – erzeugen Dienstleistungen und damit Umsatz mit Kunden. Sie entscheiden mittels der Qualität ihrer Leistung über den Erfolg oder Misserfolg eines Unternehmens. Ein Beispiel sind die „Investmentbanken“, deren geschäftlicher Erfolg zu wesentlichen Teilen von den Kundenkontakten und Kenntnissen der Mitarbeiter abhängt. Aufgrund einer bestehenden Vertrauensbasis zwischen Kunden und einem einzelnen Mitarbeiter, erhält das Unternehmen Aufträge – oder eben nicht.

Selbsterstelltes immaterielles Anlagevermögen (vgl. § 268 Abs. 8 HGB), beispielsweise Software oder Patente, kann nach deutschem und österreichischem Handelsgesetzbuch nur unter engen Voraussetzungen in der Bilanz angesetzt werden (ähnlich den in IAS 38 angeführten Kriterien). In entsprechender Höhe ist im Eigenkapital eine gesonderte Rücklage zu bilanzieren, so dass eine Ausschüttungssperre in Höhe der immateriellen Werte abzgl. darauf lastender latenter Steuern greift. Für den Verkauf erstellte immaterielle Werte werden als Umlaufvermögen angesetzt und sind mit den Herstellungskosten zu bewerten.

Beim Erwerb von Unternehmen (alle Aktiva und Passiva sowie schwebende Verträge) ohne Übernahme des Rechtsmantels (ansonsten handelt es sich schlicht um einen Beteiligungserwerb, der zu Anschaffungskosten bilanziert wird) entsteht regelmäßig ein Unterschiedsbetrag (Goodwill), der als zeitlich begrenzt nutzbarer Vermögensgegenstand gilt. Hieraus folgen planmäßige Abschreibungen. In den IFRS wird eine Abwertung nur bei Wertverlusten am Stichtag vorgenommen.

Zu Zeiten der Übertreibungen der New Economy führten Akquisitionen von Unternehmen zum Teil zu unberechtigt hohen Kaufpreisen, die sich in hohem "Goodwill" ausdrückten. Bei Unternehmenserwerben werden die künftigen Erfolge der Kaufpreisermittlung zu Grunde gelegt, nicht die Substanz, also etwa in der Bilanz ausgedrückte Werte. Gegenüber den Erwartungen im Erwerbszeitpunkt zurückbleibende Erträge führen dazu, dass der Goodwill abgeschrieben werden muss. Es kam zu den größten Verlusten, die Unternehmen jemals ausgewiesen haben. Am stärksten war dies bei AOL zu erkennen, das den Goodwill abschreiben musste, der bei der Akquisition von Time Warner entstanden war. Im internationalen Vergleich sind in europäischen Konzernen gegenüber amerikanischen Konzernen deutlich höhere immaterielle Werte zu verzeichnen. Zum Teil lässt sich dies auf die strenge SEC-Aufsicht zurückführen.

Jahresabschlüsse (bestehend aus Bilanz, Gewinn- und Verlustrechnung sowie Anhang) von Kapitalgesellschaften müssen beim Registergericht veröffentlicht oder hinterlegt und – bei Überschreiten gewisser Wertgrenzen – um einen Lagebericht () von einem Wirtschaftsprüfer geprüft werden. Hinsichtlich der Funktionsfähigkeit der Kapitalmärkte wird und wurde das Berufsrecht der Wirtschaftsprüfer mehrfach modifiziert. Mitgliedsstaaten der Europäischen Union und das Europäische Parlament haben sich vorläufig auf den Text der geänderten Richtlinie und der neuen Verordnung über Abschlussprüfungen geeinigt. AstV (Ausschuss der Ständigen Vertreter) billigte die Kompromissfassungen am 18. Dezember 2013.

Das Europäische System der Volkswirtschaftlichen Gesamtrechnung (ESVG) sieht in der VGR Vermögensbilanzen vor, bei denen die Aktiva (Vermögensgüter und Forderungen) mit den Passiva (Verbindlichkeiten) zum Reinvermögen saldiert werden.

Finanzielle Vermögensbilanzen saldieren das Bruttogeldvermögen, die Forderungen, mit den Verbindlichkeiten zum Nettogeldvermögen.




</doc>
<doc id="9654" url="https://de.wikipedia.org/wiki?curid=9654" title="Überschuldung">
Überschuldung

Überschuldung (, ) liegt vor, wenn das Vermögen eines Schuldners dessen bestehende Verbindlichkeiten nicht mehr deckt. 

Überschuldung ist ein Zustand exzessiver Schulden, den der Schuldner nach menschlichem Ermessen nicht mehr aus vorhandenen Einnahmen oder Vermögen beseitigen kann. Der Begriff Überschuldung hat sich umgangssprachlich sowie durch die Verwendung in Spezialgesetzen gebildet. In Deutschland wird der Begriff als eine mögliche Ursache für eine Insolvenz in der Insolvenzordnung (InsO) definiert, deren Vorgänger die aus Oktober 1879 stammende Konkursordnung war. Der Begriff wird auch im Rahmen der Nachlassinsolvenz definiert.
Als Schuldner kommen alle Wirtschaftssubjekte in Betracht (Privathaushalte, Unternehmen sowie der Staat mit seinen Untergliederungen Länder, Gemeinden, Gemeindeverbände, Staatsunternehmen). Bei juristischen Personen und bestimmten Personengesellschaften stellt die Überschuldung nach deutschem Insolvenzrecht gemäß der Legaldefinition in Abs. 2 InsO einen Insolvenzgrund dar. Bei Privatpersonen und bei Personengesellschaften stellt allein die Überschuldung keinen Insolvenzgrund dar, sondern lediglich die Zahlungsunfähigkeit.

Überschuldung liegt bei allen Wirtschaftssubjekten vor, wenn deren Schulden ihr Vermögen übersteigen:

Das kann einerseits durch Wertminderungen im Vermögen, andererseits durch Anwachsen der Schulden (etwa Zinseszins-Effekt, exponentielles Wachstum der Schulden durch zunehmende Neuverschuldung) geschehen.

Ursachen für Überschuldung können einerseits Vermögensverfall (Wertminderungen im Vermögen etwa durch Kursverluste bei Wertpapieren, Wertverluste bei Immobilien) oder Einnahmerückgänge (Umsatzeinbrüche oder überhöhte Investitionsrisiken bei Unternehmen; Arbeitslosigkeit, Kurzarbeit, Einkommenseinbußen bei Arbeitsplatzwechsel, Scheidung und Krankheit bei Privathaushalten), andererseits auch ein kontinuierliches oder abruptes Anwachsen der Schulden etwa durch kumulierende Schuldzinsen (Zinseszinsen) oder die so genannte Schuldenfalle sein. Als Schuldenfalle werden Kreditaufnahmen bezeichnet, insbesondere Konsumkredite („Konsumschulden“), bei denen unkontrolliert Schulden entstehen und anwachsen können (etwa bei Dauerschuldverhältnissen wie Handyvertrag oder Ratenkauf). Der Zinseszinseffekt trägt aufgrund mathematischer Gesetzmäßigkeiten zu einem exponentiellen Schuldenwachstum und damit zur Überschuldungsgefahr bei.

Im Rahmen der Studie "Schulden-Kompass" der Schufa Holding AG stellen sich die Hauptursachen der Überschuldung aus Sicht der Schuldnerberatung 2007 wie folgt dar:


Diese Daten basieren letztlich auf den Angaben der Schuldner. Die Studie merkt an, dass diese nicht immer objektiven Maßstäben entsprechen. Beispielsweise lag in den Fällen, bei denen ein Scheidungsdatum angegeben war, das Scheidungsdatum nur zu einem Viertel innerhalb der vergangenen zwei Jahre seit Aufsuchen der Beratungsstelle. Subjektive Ursachen wie Konsumverhalten oder mangelnde wirtschaftliche Bildung werden bei Erhebungen in Schuldnerberatungen systematisch unterschätzt. Auch ist die Klientel von Schuldnerberatungen nicht zwingend repräsentativ für die Gesamtheit der Überschuldeten.

Bei einem geringen Anteil der Beratungsfälle (etwa 10 %) sind Schuldenprobleme auf die unzureichend ausgebildete Fähigkeit zurückzuführen, mit Geld umzugehen. Das Angebot des Handels, auf Darlehensbasis zu kaufen, oder der Banken, das Konto zu überziehen, verführt oftmals dazu, mehr Geld auszugeben, als eingenommen wird. Es ist natürlich schwieriger, die eigenen Lebenshaltungskosten zu erfassen, als Kredite in Anspruch zu nehmen. Auch der bargeldlose Zahlungsverkehr ist eine Schuldenfalle. Da heutzutage das Konsumieren von Waren und Dienstleistungen mit dem Bezahlen meist nicht mehr zeitlich zusammenfällt (Gebührenrechnungen kommen erst einen Monat später, automatische Abbuchungen vom Konto) entsteht die Vorstellung, unbegrenzt über Geldreserven verfügen zu können. In diesen Fällen sind die betroffenen Menschen bei Lektüre der entsprechenden Abrechnungen (wie bei Handyrechnungen) sehr überrascht und reagieren dann zum Teil mit Verdrängung.

Die im Rahmen des Schulden-Kompasses zitierte Studie betrachtet auch die Verteilung der Volumina der Schulden auf einzelne Gläubigergruppen.

Durchschnittliche Schulden 2007 nach Gläubigern ohne Selbständige und Personen mit Immobilienschulden:

Insgesamt: 22.555 Euro durchschnittliche Schulden

Hat eine Person Schulden bei einer anderen Privatperson, so beliefen sich diese Privatschulden auf 13.400 Euro. Für nicht geleistete Unterhaltsverpflichtungen hatten die unterhaltspflichtigen Personen einen durchschnittlichen Rückstand von 6.700 Euro.

Bei den Volumina sind Bankschulden deutlich überrepräsentiert, weil hier im Einzelfall sehr hohe Beträge auflaufen können, insbesondere bei gescheiterten Immobilienfinanzierungen, Existenzgründungen oder in der Vergangenheit erfolgten Umschuldungen. Wer nicht durch gescheiterte Einzelprojekte, sondern durch regelmäßigen überzogenen Konsum (Handy, Energie/Versorger, Versandhaus, Miete etc.) in die Schuldenfalle tappt, hat wertmäßig meist deutlich geringere Außenstände. Insbesondere die häufig in den Medien verbreitete Aussage, dass Handyrechnungen zur Überschuldung Jugendlicher beitragen, ist durch die Zahlen nicht belegt.

Auch aus Sicht der Gläubiger sind Einkommensveränderungen häufigste Auslöser von Überschuldung.
Aus Gläubigersicht sind zusätzlich jedoch noch die folgenden Punkte Ursachen für Überschuldung (die aus Gläubigersicht als Kreditausfall wahrgenommen wird):


Generell verfügen nach einer Untersuchung von Infratest (s.unten) nur 5 % der Bundesbürger über eine gute finanzielle Allgemeinbildung. Dies rächt sich insbesondere in kritischen Situationen.


Dies betrifft auch gescheiterte Immobilienfinanzierungen. Vielfach erfolgt nicht sofort nach Trennung oder Arbeitslosigkeit ein freihändiger Verkauf der Immobilie. Stattdessen wird auf Besserung gehofft und zunehmende Verschuldung in Kauf genommen.


Bei etwa zwei Drittel der Insolvenzen deutscher mittelständischer Unternehmen liegt Überschuldung vor. Ursache der Überschuldung sind in über 90 % dieser Fälle operative Verluste; aus der direkten Geschäftstätigkeit sind über einen längeren Zeitraum hinweg die Erträge geringer als die Aufwendungen, sodass keine Gewinne erwirtschaftet werden können. Die entstehenden Verluste schmälern das Eigenkapital, sodass entstehende Liquiditätsengpässe meist nur durch Erhöhung der Schulden beseitigt werden können. Bei Großunternehmen und Konzernen, Banken und Selbständigen können auch andere Gründe vorherrschend sein. Weitere Gründe für eine Überschuldung von Unternehmen können Wertberichtigungen auf Beteiligungen, notwendige hohe Rückstellungen, sonstige Risikovorsorge, Altlasten und bedeutende Zahlungsausfälle sein.

Ursachen einer exzessiven Staatsverschuldung sind zumeist strukturelle und permanente Haushaltsdefizite eines Staates, also ein krisenhaft strukturell zu großer Unterschied zwischen Ausgaben und Einnahmen des Staates. Weitere Gründe sind auch Ausgaben für Aufrüstung, für Kriege, für Reparationszahlungen, für nicht nachhaltig finanzierbare Sozialleistungen und Personalausgaben beim Staat sowie allgemein in Korruption und Ineffizienz in Zusammenhang mit Staatsausgaben.

Rezessionen führen regelmäßig zu einer Belastung des Staatshaushalts durch Rückgang der einkommensbezogenen Steuern (progressive Einkommensteuer, Körperschaftsteuer) bei gleichzeitigem Anstieg der Sozialausgaben (Automatische Stabilisatoren). Zudem kann eine Wirtschafts- oder Finanzkrise hohe Ausgaben zur Stabilisierung der Wirtschaft, Sicherung des sozialen Friedens und zur Unternehmens- bzw. Bankenrettung verursachen und zu höheren Staatsschulden führen.

Die Hauptursachen für krisenhaft geringe Staatseinnahmen sind oft in zu niedrigen Steuern, der Duldung von Ineffizienz und Korruption bei der Steuererhebung und beim Staatsapparat insgesamt zu sehen.

Wenn ein Staat seine strukturellen und permanenten Defizite zunächst kaschiert und/oder nicht wieder senkt und stattdessen mehrfach mit erheblichen Schuldenaufnahmen finanziert, werden die Zins- und Tilgungslasten aus diesen Schuldenaufnahmen erfahrungsgemäß immer mehr selbst die Ursache für strukturelle Haushaltsdefizite eines Staates.

Bei der Untersuchung der Überschuldungsthematik ist danach zu unterscheiden, ob der Schuldner Privatperson, Unternehmen oder ein Staat ist.

Innerhalb der EU gibt es keine einheitliche Definition für die Überschuldung von Privatpersonen. In Deutschland spricht das Bundesfamilienministerium von Überschuldung bei Privathaushalten, wenn deren „Einkommen über einen längeren Zeitraum nach Abzug der Lebenshaltungskosten trotz Reduzierung des Lebensstandards nicht zur fristgerechten Schuldentilgung ausreicht.“ Dem Ministerium kommt es also darauf an, dass vermögenslose Haushalte ausschließlich ihr Einkommen zur Schuldentilgung einsetzen und dabei ihre Ausgabenstruktur so reduzieren, dass hiermit eine Verringerung des Lebensstandards einhergeht. Ist jedoch noch verfügbares – und nicht bereits zur Kreditsicherung verwendetes – Vermögen vorhanden, so muss das nicht für eine bescheidene Lebensführung notwendige Vermögen zwecks Schuldenabbau veräußert werden.

Minderjährige können nach geltendem Recht ohne Einwilligung der Eltern keine eigenen rechtsverbindlichen Verpflichtungen eingehen, die zu Schulden führen ( Nr. 8 BGB). Eine eigenmächtige Verschuldung von Minderjährigen in Form von Kontoüberziehung ist daher ebenso ausgeschlossen wie die Begründung von Zahlungsverpflichtungen infolge eines Darlehensvertrages. Eine Ver- und Überschuldungsgefahr stellen allerdings Dauerschuldverhältnisse dar, bei denen die Höhe der monatlichen Beträge nicht feststeht oder nach oben begrenzt ist (Handyverträge).

Bei Privathaushalten stellt sich die Frage, wann Überschuldung eintritt. Maßgröße ist hier das Einkommen eines Haushalts, an dem die Schulden zu orientieren sind. Die englische "Task Force on Tackling Overindebtedness" geht bei einer Konsumentenkredit-Quote von 25 % des Bruttoeinkommens von einer massiven Risikogefährdung aus, in finanzielle Schwierigkeiten zu geraten. In Deutschland geben die Haushaltswissenschaften einen Wert von 20 % des Haushaltsbruttoeinkommens an, den die Summe aller Schuldverpflichtungen nicht überschreiten sollte. Der „Kreditratgeber“ des Beratungsdienstes der Sparkassen empfiehlt bei kleinen und mittleren Einkommen sogar, bereits bei einem Übersteigen der Kreditrate von mehr als 10 % des Haushaltseinkommens Vorsicht walten zu lassen. Eine nicht quantifizierte, sondern am Haushaltsniveau orientierte Definition liefert Groth. „Überschuldung liegt dann vor, wenn nach Abzug der fixen Lebenshaltungskosten (Miete, Energie, Versicherung, etc. zzgl. Ernährung) der verbleibende Rest des monatlichen Einkommens für zu zahlende Raten nicht ausreicht“. In Frankreich wird Überschuldung in Art. 1 Abs. 1 des „Loi Neiertz“ als offensichtliches Unvermögen des Schuldners beschrieben, seinen Verbindlichkeiten nichtberuflicher Art nachzukommen.

Der Wirtschaftsauskunftei Creditreform zufolge waren im Jahr 2010 etwa 6,5 Millionen erwachsene Deutsche nicht in der Lage, ihre Kreditverpflichtungen (Raten) zu begleichen; 300.000 Personen mehr als 2009. Der Verlust des Arbeitsplatzes sei der wichtigste Auslöser für finanzielle Engpässe. Auch die steigenden finanziellen Belastungen für Gesundheit und Altersvorsorge oder die Miete ließen weniger Spielraum, um bestehende Kredite zurückzahlen zu können. Gerade bei jüngeren Erwachsenen sitze zudem im Aufschwung das Geld wieder lockerer in der Tasche; auch die Bereitschaft steige, neue Kredite aufzunehmen.

Liegen die Tatbestandsvoraussetzungen vor, kann eine vorsätzlich oder fahrlässig herbeigeführte Überschuldung (Bankrott) strafrechtlich verfolgt werden, wenn dadurch Gläubiger geschädigt werden ( StGB). Dies kann sowohl bei überschuldeten Privatpersonen als auch bei Unternehmen der Fall sein, nicht aber bei überschuldeten Staaten.

Formalrechtlich hat der Gesetzgeber in der Insolvenzordnung (§ 19 Abs. 2 InsO) versucht, den Überschuldungsbegriff für Zwecke der Unternehmenskrise zu definieren. Danach ist Voraussetzung, dass das Vermögen des Schuldners die bestehenden Verbindlichkeiten nicht mehr deckt, es sei denn, die Fortführung des Unternehmens ist nach den Umständen überwiegend wahrscheinlich. Das Gesetz ist damit zum zweistufigen Überschuldungsbegriff zurückgekehrt, wie er unter Geltung der früheren Konkursordnung noch vertreten wurde. Regelfall ist mithin die Gegenüberstellung des Vermögens mit den Schulden. Übersteigen die Schulden das Vermögen (Aktiva) und ist somit das Eigenkapital rechnerisch negativ, liegt nur dann keine Überschuldung vor, wenn eine positive Fortführungsprognose besteht. Die Eigenkapitalquote ist ein wesentlicher Faktor bei der Beurteilung der Bonität eines Schuldners, die wiederum ausschlaggebend dafür ist, ob überhaupt und zu welchen Konditionen Kredite gewährt werden.

In der Überschuldung kann deshalb ein Zustand von Schuldnern verstanden werden, der auf der Grundlage eines Schuldenüberschusses über das Vermögen auch für die Zukunft keine positive Entwicklung verspricht. Für Unternehmen ist eine Überschuldungsbilanz aufzustellen, die die realisierbaren Vermögensgegenstände nach Liquidationswerten enthält und diese den tatsächlichen Schulden gegenüberstellt. Ergibt sich hierbei ein Schuldenüberschuss, und eine Fortführungsprognose fällt aufgrund der Kosten- und Umsatzplanung negativ aus, sind die Voraussetzungen der insolvenzrechtlichen Überschuldung nach § 19 Abs. 2 InsO gegeben. Diese Überschuldungsbilanz beruht nicht auf den Rechnungslegungsvorschriften (Handelsgesetzbuch, IFRS), sondern berücksichtigt die realisierbaren Vermögenswerte. Vermögensgegenstände, die aufgrund einer gesetzlichen Aktivierungspflicht in der Handelsbilanz ausgewiesen werden müssen, aber zum Zeitpunkt der Aufstellung der Überschuldungsbilanz wertlos sind, werden nicht berücksichtigt.

Spezialregelungen sehen bei Kapitalgesellschaften bestimmte Konsequenzen vor, wenn Vorstufen der Überschuldung erreicht werden. So ist bei GmbHs eine Gesellschafterversammlung einzuberufen, wenn die Hälfte des Stammkapitals verloren ist Abs. 3 GmbHG. Eine vergleichbare Regelung für Aktiengesellschaften enthält Abs. 1 AktG. Ein Verstoß hiergegen kann über Abs. 2 BGB zur zivilrechtlichen Haftung sowie zur Strafbarkeit ( GmbHG, StGB – Bankrott) führen.

Bei „begründete[r] Besorgnis einer Überschuldung“ ist eine Zwischenbilanz zu erstellen; diese wird von der Revisionsstelle geprüft (Art. 725 Abs. 2 Satz 1 OR). Auf eine Überschuldungsanzeige hin kann sodann – Ausnahmen vorbehalten – ohne vorgängige Betreibung der Konkurs eröffnet werden (Art. 192 SchKG in Verbindung mit Art. 725a Abs. 1 Satz 1 OR).

In Österreich gibt es keine Legaldefinition des Begriffs „Überschuldung“, die Vorgangsweise ist in Insolvenzordnung geregelt: Überschuldung ist ein Insolvenzgrund bei Personengesellschaften deren unbeschränkt haftender Gesellschafter keine natürliche Person ist(beispielsweise eine GmbH & Co. KG), juristischen Personen und Verlassenschaften, also nicht – wie die Zahlungsunfähigkeit – ein allgemeiner Insolvenzgrund. Nach einer OGH-Grundsatzentscheidung liegt eine insolvenzrechtlich bedeutende Überschuldung nur in dem Fall vor, dass Zentral im österreichischen Insolvenzrecht ist daher die zukunftsbezogene Perspektive: Ist davon auszugehen, dass das Unternehmen in absehbarer Zeit seine prekäre Vermögenssituation verbessern kann und weiterhin liquide bleibt, liegt der Insolvenzgrund „Überschuldung“ nicht vor.

Erblasser können neben Erbschaftsvermögen den Erben im Wege der Gesamtrechtsnachfolge auch alle Schulden hinterlassen. Dies ist solange unproblematisch, wie das Erbschaftsvermögen die zu erbenden Schulden übersteigt. Auch hier stellt das Gesetz die Schulden dem Vermögen gegenüber. Für diese Schulden muss der Erbe mit dem eigenen Vermögen haften, wenn er die Erbschaft antritt und zum Gesamtrechtsnachfolger des Erblassers nach Abs. 1 BGB wird. Um die Erben vor den Schulden und den daraus resultierenden Haftungsfolgen zu schützen, gibt es die Nachlassinsolvenz. Hierbei handelt es sich um eine Sonderform der Insolvenz, durch die die Erben von der Haftung mit ihrem eigenen Vermögen befreit werden. Die Erben müssen dann bei Schulden, die durch den Tod des Erblassers an sie übergangen sind, nicht mit dem eigenen Vermögen haften. Die Nachlassinsolvenz wird neben der Zahlungsunfähigkeit auch durch Überschuldung ausgelöst ( InsO). Der Antrag auf ein solches Verfahren kann von den Erben erst dann gestellt werden, wenn die Überschuldung des Nachlasses feststeht.

Die Gefahr der Überschuldung eines Staates wird verstärkt, wenn neben einem hohen Länderrisiko auch ein hohes Währungsrisiko besteht. Ein hoch verschuldeter Staat kann gezwungen sein, seine künftige Staatsverschuldung in Fremdwährung aufzunehmen (so genanntes Original Sin). Eine Abwertung der Inlandswährung (und die damit verbundene Zunahme der Staatsschulden in inländischen Währungseinheiten) oder eine Aufwertung der Fremdwährung (bei Fremdwährungskrediten) können den Prozess der Überschuldung erheblich beschleunigen. Umgekehrt kann ein Land, dessen Währung international als Reservewährung akzeptiert wird, eine bestehende Staatsverschuldung monetisieren.

Beim Staat als größtem Aggregat ist es schwer, den Zustand der Überschuldung zu ermitteln. Einen Anhaltspunkt bieten die Stabilitätskriterien (Konvergenzkriterien) der Maastrichter Verträge zur Europäischen Wirtschafts- und Währungsunion sowie Folgevereinbarungen. Diese sehen vor, dass die Mitgliedstaaten als Voraussetzung zum Beitritt in die Europäische Währungsunion zur Begrenzung ihrer öffentlichen Verschuldung zwei Ziele erfüllen müssen. Einerseits darf das jährliche öffentliche Haushaltsdefizit 3 % des Bruttoinlandsprodukts (BIP) und andererseits der öffentliche Schuldenstand (kumuliertes Haushaltsdefizit) 60 % des BIP nicht übersteigen. Werden diese Zielgrößen geringfügig und nur kurzfristig überschritten, kann noch nicht von einer Überschuldung eines Staates gesprochen werden. Bei exzessiver und längerfristiger Überschreitung wie im Falle Griechenlands liegt der Zustand der Staatsüberschuldung jedoch nahe. Bei Griechenland kann mit einem Defizit von 15,4 % (2009) bzw. 10,4 % (2010) sowie einem Schuldenstand von 127,8 % (2009) und 142,8 % des BIP (2010) mit nur geringen Wachstumschancen des BIP bereits von Überschuldung gesprochen werden. Maßgröße im Staatssektor ist also der Wert aller in einem Jahr in einem Staat produzierten Güter und erbrachten Dienstleistungen, ausgedrückt im BIP als Symbol für die Wirtschaftskraft eines Staates.

Eine weitere Maßgröße für die Überschuldung von Staaten sind die jährlich erzielten Exporterlöse. Die Auslandsverschuldung darf 150 % der Exporterlöse nicht übersteigen, wenn nachhaltig gesicherte Staatsfinanzen die finanzielle Flexibilität eines Staates erhalten sollen. Überschreitet mithin die Staatsverschuldung längerfristig die Grenze von 150 % deutlich, kann von Überschuldung gesprochen werden, insbesondere bei Staaten mit geringem Exportniveau oder wenig diversifizierten Exportstrukturen.

Den staatlichen Untergliederungen – wie etwa Gemeinden – droht ebenfalls die Gefahr einer Überschuldung. Die Überschuldung kann nämlich nicht nur in der doppischen Bilanz ermittelt werden, sondern auch bei kameralistisch aufgestellten öffentlichen Haushalten. Bei diesen kommt es darauf an, dass eine nach § 41 Abs. 4 GemHVO zu bildende Allgemeine Rücklage besteht, die als Residualgröße die Aufgabe des kommunalen Eigenkapitals erfüllt. Sofern diese Allgemeine Rücklage vollständig zum Zwecke des Haushaltsausgleichs aufgezehrt worden ist, liegt nach § 75 Abs. 7 GemO NRW Überschuldung vor. Überschuldung ist jedoch bei Bund, Bundesländern, Gemeindeverbänden und Gemeinden kein Insolvenzgrund, weil diese nach Nr. 1 und 2 InsO insolvenzunfähig sind.

Während säumigen Schuldner historisch die Schuldknechtschaft oder das Schuldgefängnis drohte, besteht in modernen Gesellschaften lediglich die Rechtsfolge, dass Gläubiger Vermögensgegenstände oder Einkommen des Schuldners pfänden können. Die Einführung des Verbraucherinsolvenzverfahrens im Jahr 1999 stellt ein wichtiges Element zur Überwindung der Überschuldung natürlicher Personen dar. Es ermöglicht überschuldeten Personen durch einen Schuldenbereinigungsplan, dem die Gläubiger zugestimmt haben, am Ende einer Wohlverhaltensperiode von 6 Jahren vom Rest ihrer Schulden befreit zu werden. Danach ist ein wirtschaftlicher Neuanfang möglich. Bei Einführung 1999 wurden 1.634 Fälle registriert, 2001 waren es bereits 9.070 Fälle, 2011 ist die Zahl auf 103.289 (2010: 108.798) Fälle angestiegen. Damit steigen gleichzeitig jedoch auch die Chancen, dass die betroffenen Haushalte von ihren belastenden Schulden befreit werden.

Bildung der Privathaushalte ist die wesentliche Quelle für eine eigenverantwortliche Lebensführung und für eine verantwortungsvolle Teilhabe an der Gesellschaft. Finanzielle Allgemeinbildung – also das Wissen und die Kompetenzen im Umgang mit Finanzdienstleistungen und Konsumwünschen – ist eine wichtige Grundlage, um Überschuldungsrisiken vorzubeugen. Entsprechend kommt der Verankerung der finanziellen Allgemeinbildung in der schulischen wie der außerschulischen Bildung eine zentrale Bedeutung zu.

Menschen, die von Überschuldung betroffen sind, wird dringend empfohlen, frühzeitig eine der Schuldnerberatungsstellen bei den Wohlfahrtsverbänden, Verbraucherzentralen oder den Kommunen aufzusuchen. Schulden sind keine Schande. Falsche Scham sollte überwunden werden. Überschuldete Personen haben dann die Möglichkeit, in ein Verbraucherinsolvenzverfahren zu kommen. Entweder wird dann ein außergerichtlicher Vergleich durch die Beratungsstelle erreicht (dies gelingt in etwa 10 % der Fälle) oder es gelingt ein gerichtlicher Vergleich. Kommt es bei letzterem zu keiner Einigung, so folgt ein Insolvenzverfahren. Dies ermöglicht nach sechs Jahren die Streichung sämtlicher Schulden. Auf diese Weise gelingt dem vormals Überschuldeten ein frischer Start.

Alternativ zu den Hilfsangeboten der öffentlichen Schuldnerberatungsstellen kann auch die Hilfe eines Rechtsanwalts in Anspruch genommen werden. Während bei den Schuldnerberatungsstellen häufig Wartezeiten von ein bis zwei Jahren vor Durchführung eines Verbraucherinsolvenzverfahrens in Kauf zu nehmen sind, kann mit anwaltlicher Hilfe das Verfahren sofort betrieben werden. Die Inanspruchnahme eines Rechtsanwalts ist allerdings mit Kosten verbunden, die nur zum Teil von der öffentlichen Hand übernommen werden.

Vorsicht ist bei Anzeigen in Zeitungen oder im Internet geboten, die angeblich Hilfe für Überschuldete versprechen. Dubiose Anbieter nutzen oftmals Scham und Not Überschuldeter aus, etwa auch durch „schnelle Kredite“.

Überschuldeten Unternehmen bietet die Fortführungsinsolvenz die Chance, aufgrund einer günstigen Fortführungsprognose die Unternehmenskrise zu überwinden. Verschiedene Sanierungsmaßnahmen wie Verkauf unrentabler Betriebsteile, Konzentration auf nachhaltig gewinnbringende Geschäftsmodelle, Verstärkung des Kerngeschäfts, eine weniger risikobehaftete Investitionspolitik, Schuldenerlass oder eigenkapitalstärkende kapitalkräftige Gesellschafter ermöglichen eine Weiterexistenz und einen wirtschaftlichen Neuanfang.

Nach dem Beschluss der G8-Finanzminister vom Juni 2003 soll die Schuldentragfähigkeit nach der Definition von IWF und Weltbank künftig auch als Ziel für Schuldensenkungen des Pariser Clubs gelten. Auch bei einem Insolvenzverfahren für Staaten wäre die Definition von Schuldentragfähigkeit entscheidend für die Berechnung des Umfangs von Schuldenerlassen. Schuldentragfähigkeit liegt vor, wenn „ein Land seinen momentanen und zukünftigen Schuldendienst vollständig leisten (kann), ohne auf Schuldenrestrukturierungen und das Aussetzen von Zahlungen zurückgreifen zu müssen und ohne dabei sein Wachstum zu gefährden.“ Schuldentragfähigkeit ist demnach gegeben, wenn ein Staat
nachweisen kann.

Staaten müssen deshalb zur Vermeidung von Überschuldungsgefahren ihre Wirtschaftspolitik so ändern, dass die vorstehenden Zielgrößen erfüllt werden können.

Auch bei Immobilien spricht man von Überschuldung, wenn die nominelle Belastung mit Grundpfandrechten den Beleihungswert übersteigt. Diese Situation kann nicht bei einer ersten Immobilienfinanzierung eintreten, sondern lediglich im Falle sinkender Immobilienpreise, an welche die Beleihungswerte angepasst werden. Dies war insbesondere bei der Immobilienblase in den USA während der Finanzkrise ab 2007 der Fall.

Übersteigen die Schulden des Erblassers den Wert des Nachlasses, so ist der Nachlass überschuldet. Aus finanzieller Sicht ist es für den Erben in diesem Fall sinnvoll, das Erbe auszuschlagen.





</doc>
<doc id="9658" url="https://de.wikipedia.org/wiki?curid=9658" title="Gewinnrücklage">
Gewinnrücklage

Eine Gewinnrücklage ist im Rechnungswesen die Folge nicht ausgeschütteter Jahresüberschüsse einer Kapitalgesellschaft und gehört zum Eigenkapital. Sie wird aus einbehaltenen (thesaurierten) Gewinnen gebildet. 

Neben dem Grundkapital (oder „gezeichnetem Kapital“) als weitgehend konstant bleibende Größe hat der Gesetzgeber noch zum Eigenkapital gehörende Reservepositionen geschaffen, die ebenfalls dem Gläubigerschutz dienen, aber durchaus variablen Charakter besitzen. Dazu gehören die gesamten Rücklagen. Die Gewinnrücklage ist Bestandteil der offenen Rücklagen. Diese sind nach der Gliederungsvorschrift des Abs. 3 HGB voneinander getrennt auszuweisen. Nach dem Grundkapital folgt zunächst die Kapitalrücklage und danach die Gewinnrücklage mit ihren verschiedenen Unterarten.

Die Unterarten der Gewinnrücklage sind in Abs. 3 HGB abschließend aufgezählt: 

Die Kapitalrücklage wird in die Berechnung der gesetzlichen Rücklage einbezogen, während die satzungsmäßigen Rücklagen und andere Gewinnrücklagen regelmäßig nicht Bestandteil der gesetzlichen Rücklage sein können.

Seit Mai 2009 sind „Rücklagen für Anteile an einem herrschenden oder mehrheitlich beteiligten Unternehmen“ als so genannte Rückbeteiligung zu bilden und wirtschaftlich im Zusammenhang mit eigenen Anteilen zu sehen. Eigene Anteile, die eine Kapitalgesellschaft hält, sind hingegen nunmehr vom Eigenkapital abzusetzen ( Abs. 1a HGB). Die nach den , AktG gehaltenen Anteile an Konzernunternehmen sind als Gewinnrücklage auszuweisen. Aus Sicht des herrschenden oder mehrheitlich beteiligten Unternehmens ist es gleichgültig, ob es die eigenen Anteile selbst hält oder sie von einem abhängigen oder in Mehrheitsbesitz stehenden Unternehmen gehalten werden. Diese Vorschrift füllt eine bisherige Gesetzeslücke und ist eine Ausschüttungssperre, da etwaige Verluste beim herrschenden oder mehrheitlich beteiligten Unternehmen sich auf den Wertansatz der aktivierten Anteile beim beherrschten Unternehmen auswirken. Sinkt der Wert dieser Anteile, ist die Gewinnrücklage entsprechend zu reduzieren, sodass der verteilungsfähige Bilanzgewinn sinkt.

Gewinnrücklagen sind aus dem entstandenen Jahresüberschuss des abgeschlossenen Geschäftsjahrs oder früheren Geschäftsjahren zu dotieren; Gewinne werden insoweit nicht ausgeschüttet, sondern einbehalten ( Abs. 2 und 3 HGB). Die Bildung der Gewinnrücklagen erfolgt also aus versteuerten Gewinnen. Sofern Jahresüberschüsse erwirtschaftet werden, besteht ein Zwang zur Dotierung der gesetzlichen Rücklage bei Aktiengesellschaften und Kommanditgesellschaften auf Aktien. Für satzungsmäßige und andere Rücklagen besteht indes ein Wahlrecht. Hat die gesetzliche Rücklage 10 % des gezeichneten Kapitals erreicht, endet der gesetzliche Dotierungszwang. Nach Abs. 4 HGB dürfen die Bildung oder Veränderung von Gewinnrücklagen erst nach dem Posten „Jahresüberschuss/Jahresfehlbetrag“ ( Abs. 3 Nr. 19 HGB) ausgewiesen werden. Damit wird verdeutlicht, dass die Bildung der Gewinnrücklagen eine Gewinnverwendung darstellt. 

Die Gesellschafterversammlung einer GmbH kann auch den gesamten Jahresüberschuss in die Gewinnrücklagen einstellen ( Abs. 2 GmbHG), sodass kein Bilanzgewinn ausgewiesen wird. Der Bilanzgewinn zeigt also nicht den tatsächlich erwirtschafteten Unternehmenserfolg an, sondern den nach Ergebnisverwendung verbliebenen Teil, der für eine Ausschüttung zur Verfügung steht.

Gewinnrücklagen entstehen demnach ausschließlich aus Innenfinanzierung, während Kapitalrücklagen aus der Außenfinanzierung herrühren.

Die Auflösung von Gewinnrücklagen unterliegt strengen Bedingungen ( Abs. 3 und 4 AktG). Haben die Gewinnrücklagen zusammen mit den Kapitalrücklagen die Höhe der gesetzlich erforderlichen Rücklagen noch nicht erreicht, dürfen sie nur zur Verlustabdeckung verwandt werden, wenn sowohl Gewinnvortrag als auch andere Gewinnrücklagen hierfür nicht ausreichen ( Abs. 3 AktG). Übersteigen die Rücklagen die gesetzlich erforderliche Mindestgrenze, so dürfen sie auch zur Kapitalerhöhung verwandt werden ( Abs. 4 Nr. 3 AktG). Wenn keine Verluste auszugleichen sind, können die anderen Gewinnrücklagen frei verwendet werden, etwa zur Kapitalerhöhung oder Ausschüttung von Dividenden, wenn der Jahresüberschuss hierzu nicht ausreicht (Dividendenkontinuität).

In Abs. 4 HGB wird ausdrücklich angeordnet, dass Rücklagenauflösungen erst nach der Position „Jahresüberschuss/Jahresfehlbetrag“ ( Abs. 2 Nr. 20 und Abs. 3 Nr. 19 HGB als Ergebnis der Gewinn- und Verlustrechnung) auszuweisen sind. Damit unterliegen Rücklagenauflösungen einem Abführungsverbot. Eine Ausnahme hiervon ist in Satz 2 AktG für den Fall vorgesehen, dass Beträge während der Laufzeit eines Gewinnabführungsvertrags in andere Gewinnrücklagen eingestellt worden sind. Das Gesetz will hierdurch einen Anreiz geben, nicht alle in der Vertragszeit angefallenen Gewinne abzuführen, sondern diese vielmehr auch in Gewinnrücklagen zu speichern. Diese Ausnahme besteht jedoch ausschließlich für Gewinnrücklagen und nicht für Zuführungen zur Kapitalrücklage.

 Abs. 3 UGB: Als Gewinnrücklagen dürfen nur Beträge ausgewiesen werden, die im Geschäftsjahr oder in einem früheren Geschäftsjahr aus dem Jahresüberschuss nach Berücksichtigung der Veränderung unversteuerter Rücklagen gebildet worden sind. Die Gewinnrücklage stellt eine Innenfinanzierung dar. Als Basis zur Berechnung der Gewinnrücklage muss vom Jahresüberschuss/Jahresfehlbetrag (Gewinn nach Steuern bzw. Verlust) die Auflösung unversteuerter Rücklagen addiert und die Zuweisung zu unversteuerten Rücklagen subtrahiert werden. Dieser Betrag kann dann der Gewinnrücklage zugeführt werden (Zuweisung zu Gewinnrücklagen an Gewinnrücklagen). 

Bei der Gewinnrücklage ist zwischen einer gesetzlichen, einer satzungsmäßigen und einer freien zu unterscheiden:

Gesetzliche Rücklage § 229 Abs. 6 UGB: 
In die gesetzliche Rücklage ist ein Betrag einzustellen, der mindestens 5 % des um einen Verlustvortrag geminderten Jahresüberschusses nach Berücksichtigung der Veränderung unversteuerter Rücklagen entspricht, bis der Betrag der gebundenen Rücklagen insgesamt 10 % oder den in der Satzung bestimmten höheren Teil des Nennkapitals erreicht hat. 

Satzungsmäßige Rücklage: 
Kann in der Satzung der Kapitalgesellschaft geregelt werden.
Freie Rücklage: 
Kann vom Jahresabschluss beschließenden Organ (im Regelfall Vorstand mit Billigung durch den Aufsichtsrat) bestimmt werden. Dadurch wird das Ausschüttungspotenzial gesteuert. Zuweisungen zu und Auflösungen von Gewinnrücklagen stellen Maßnahmen zur Gewinnverwendung dar und berühren daher die Ertragssteuer nicht! Damit verringert sich aber der ausgewiesene und folglich ausschüttbare Gewinn. Lediglich der verbleibende Gewinn steht den Aktionären zur Ausschüttung zur Verfügung.



</doc>
<doc id="9665" url="https://de.wikipedia.org/wiki?curid=9665" title="Kanton Tessin">
Kanton Tessin

Das Tessin [] ( TI; [], [], , ), amtlich ital. (Republik und Kanton Tessin), ist ein Kanton in der Schweiz. Der Hauptort ist Bellinzona, die bevölkerungsreichste Stadt Lugano. In Chiasso befindet sich der südlichste Punkt der Schweiz. Der südliche Teil des Kantons zählt zur grenzüberschreitenden schweizerisch-italienischen Metropolregion Tessin.

Im Tessin wird Italienisch und Lombardisch (Ticinese im Sopraceneri und Comasco-Lecchese im Sottoceneri) gesprochen; in der Gemeinde Bosco/Gurin ist Deutsch (Walserdeutsch) die altherkommene Sprache. Neben Graubünden ist das Tessin der einzige Schweizer Kanton mit Italienisch als Amtssprache.

Höchste Erhebung ist mit das Rheinwaldhorn (ital. "Adula"); der tiefste Punkt liegt mit am Lago Maggiore und ist zugleich der tiefste Punkt der Schweiz.

Zusammen mit den vier Bündner Südtälern Bergell "(Val Bregaglia)," Calancatal "(Val Calanca)," Misox "(Val Mesolcina)" und Puschlav "(Val Poschiavo)" bildet es die italienisch(sprachig)e Schweiz "(Svizzera italiana)".

Das Tessin macht den grössten Teil der italienischen Schweiz aus. Es liegt auf der Alpensüdseite und ist weitgehend von Italien umgeben; im Norden und Nordosten grenzt es an die Schweizer Kantone Graubünden, Wallis und Uri. Campione d’Italia bildet eine Enklave. Die Fläche beträgt 2812 Quadratkilometer, was 7 Prozent der Gesamtfläche der Schweiz entspricht. Etwa ein Viertel des Gebiets gilt als unproduktiv und ein Drittel davon ist bewaldet. Wichtige Akzente setzen die beiden grossen Seen Langensee ("Lago Maggiore" oder "Verbano") und Luganersee ("Lago di Lugano" oder "Ceresio").

Seinen Namen hat der Kanton vom Fluss Tessin "(Ticino)," der auf der Südseite des Gotthardmassivs am Nufenenpass entspringt, das Bedrettotal "(Val Bedretto)" und die Leventina "(Valle Leventina)" in Richtung Südost durchfliesst, um dann, ab der Kantonshauptstadt Bellinzona nach Westen durch die Magadinoebene fliessend, in den Langensee zu münden.

Hauptzuflüsse auf diesem Weg sind der Brenno aus dem Bleniotal "(Valle di Blenio)" und die Moësa aus dem bündnerischen Misox. Auch der übrige Teil des nördlich des Monte Ceneri gelegenen Kantonsteils "(Sopraceneri)" wird in den Lago Maggiore entwässert, nämlich durch die Flüsse Maggia und Verzasca. Der südliche Kantonsteil "(Sottoceneri)" entwässert grösstenteils in den Luganersee und von dort über die Tresa ebenfalls in den Langensee, dessen Abfluss – wiederum «Ticino» genannt – wenig unterhalb der Stadt Pavia in den Po mündet.

Teile des südlichsten «Zipfels» der Schweiz, des Mendrisiotto, entwässern zwar ebenfalls letztlich in den Po, aber nicht über den Tessin. Aus dem Muggiotal kommend durchfliesst die Breggia den Talkessel von Chiasso und vereinigt sich im Comer See mit dem Wasser der Adda. Das westliche Mendrisiotto wird vom Flüsschen Gaggiolo durchflossen, welches unter dem Namen «Rio Ranza» (auch «Rio Ranzo» oder «Rio Lanza») bei Malnate in der italienischen Provinz Varese in die Olona mündet.
Unter der reichhaltigen Flora besonders erwähnenswert sind die ausgedehnten Wälder von Edelkastanien. Sie gibt es sonst nur noch in wenigen Gegenden der Welt in dieser Reinheit und Fläche. Zudem gedeihen im Tessin unzählige Palmen, Zypressen und andere Mittelmeerpflanzen. Daher wird der Kanton als «Sonnenstube der Schweiz» bezeichnet.

Die Tessiner Bevölkerung fühlt sich mit ihrem Nachbarn Italien kulturell stark verbunden. Per betrug die Einwohnerzahl des Kantons Tessin . Die Bevölkerungsdichte liegt mit /2812 round 0 Einwohnern pro Quadratkilometer unter dem Schweizer Durchschnitt (/41285 round 0 Einwohner pro Quadratkilometer). Der Ausländeranteil (gemeldete Einwohner ohne Schweizer Bürgerrecht) bezifferte sich am auf  Prozent, während landesweit  Prozent Ausländer registriert waren. Per betrug die Arbeitslosenquote  Prozent gegenüber  Prozent auf eidgenössischer Ebene.

Im Tessin ist Italienisch die Amtssprache. 87,7 Prozent haben Italienisch als Hauptsprache, 10,8 Prozent Deutsch, 5,1 Prozent Französisch (Mehrfachangabe war möglich). Im Weiteren war Englisch mit 3,1 Prozent vertreten.

Ein grosser, allerdings im Abnehmen begriffener Teil der Bevölkerung spricht lokale Dialekte, die zum Lombardischen gehören («Ticinés»). Nach der Volkszählung von 2000 sprachen im Tessiner Alltag 14 Prozent ausschliesslich Lombardisch und 40 Prozent sowohl Lombardisch als auch Standarditalienisch. Da die norditalienischen Dialekte der Lombardei (inklusive italienischsprachiger Schweiz), des Piemonts, Liguriens und der Emilia-Romagna einen galloromanischen Hintergrund besitzen, ähneln sehr viele Ausdrücke dem Französischen, und auch nasale Laute sowie palatale «ö» und «ü» sind häufig. Im Tessiner Dialekt heisst es zum Beispiel:

«un om al gheva dü fiöö» , auf Standard-Italienisch würde man sagen: «un uomo aveva due figli» (ein Mann hatte zwei Söhne). «Herz» heisst im Dialekt «cör» , ähnlich wie das französische «cœur» und nicht wie das italienische «cuore» . Und «500 Schweine» (cinquecento maiali) heisst auf Tessinerisch «cinc-cent ciügn» .

Der dialektale Wortschatz und die Volkskultur des Tessins und Südbündens werden vom Vocabolario dei dialetti della Svizzera italiana dokumentiert.

Eine Besonderheit stellt die Walsergemeinde Bosco/Gurin dar, in der Deutsch die herkömmliche Sprache ist (nicht zu verwechseln mit Amtssprache).

Im Kanton Tessin wird die "Tessiner Gebärdensprache" verwendet, die Lingua dei segni della Svizzera italiana (LIS-SI). Sie ist ein Dialekt der Lingua dei Segni Italiana (LIS). LIS-SI gehört wie die Deutschschweizer Gebärdensprache und die Westschweizer Gebärdensprache zur Familie der französischen Gebärdensprachen. Die Tessiner Gebärdensprache kennt zwei Varietäten (Bellinzona und Lugano).

Das Tessin ist stark katholisch geprägt. 2012 waren 69,6 Prozent der Bevölkerung römisch-katholisch, 4,4 Prozent evangelisch-reformiert und 16,2 Prozent konfessionslos. Weitere 2,0 Prozent bekennen sich zu einer islamischen Gemeinschaft, 0,1 Prozent zum Judentum und 0,5 zu anderen Religionsgemeinschaften.

Bis Anfang des 20. Jahrhunderts war das Tessin ein ländlich geprägter Kanton. Mitte des 19. Jahrhunderts lebte ein Drittel der Bevölkerung in Dörfern über 600 m, heute sind es 7 Prozent.

Die aktuelle Kantonsverfassung datiert vom 14. Dezember 1997 (mit seitherigen Änderungen).

Gesetzgebendes Organ (Legislative) ist der Grosse Rat "()," das Kantonsparlament. Der Grosse Rat umfasst 90 Mitglieder, die alle vier Jahre in einem einzigen Wahlkreis nach dem Proporzwahlrecht (Verhältniswahl) gewählt werden.

Verfassungsänderungen unterliegen dem obligatorischen Referendum und sind damit zwingend der Volksabstimmung zu unterbreiten. Dem fakultativen Referendum unterliegen Gesetzeserlasse bzw. -änderungen, rechtsetzende Dekrete mit allgemeinverbindlichem Charakter, Ausgabenbeschlüsse über einmalige Ausgaben von mehr als einer Million Franken bzw. jährlich wiederkehrende Ausgaben von mehr als 250'000 Schweizer Franken sowie rechtsetzende Staatsverträge, wenn dies innert 45 Tagen nach der Veröffentlichung im Amtsblatt von mindestens 7000 Stimmberechtigten oder von einem Fünftel der Gemeinden verlangt wird.

Das Volk kann in Form einer Volksinitiative selbst Gesetze und Gesetzesänderungen vorschlagen, wenn 7000 Stimmberechtigte innert 60 Tagen nach Veröffentlichung des Begehrens im Amtsblatt einen dahingehenden Vorschlag unterstützen. Initiativen auf Verfassungsänderungen bedürfen der Unterstützung von 10'000 Stimmberechtigten.

Seit der Wahl vom 19. April 2015 verteilen sich die Sitze wie folgt:

Oberstes vollziehendes bzw. ausführendes Organ (Exekutive) ist der Staatsrat "()" und somit die Kantonsregierung. Er besteht aus fünf Mitgliedern, die alle vier Jahre in einem einzigen Wahlkreis vom Volk direkt nach dem Proporzwahlrecht gewählt werden. Aus ihren Reihen wählen die Mitglieder des Staatsrates jeweils für ein Jahr den Präsidenten.

15'000 Stimmberechtigte können verlangen, dass eine Volksabstimmung über die Absetzung des Staatsrats stattfindet. Der Grosse Rat kann selbst ein Mitglied des Staatsrates mit der absoluten Mehrheit seiner Stimmen abberufen.
Das Amt des Staatsschreibers "(Cancelliere dello Stato)" hat Arnoldo Coduri inne.

Die Zivilgerichtsbarkeit wird durch die Friedensrichter (";" Schlichtungsbehörde), die ' (in erster Instanz) und das Appellationsgericht (";" in zweiter Instanz) ausgeübt, die Strafgerichtsbarkeit durch das Strafgericht "()" und die ' (beide in erster Instanz), das Appellationsgericht (";" in zweiter Instanz) und den Jugendrichter. Das Geschworenengericht wurde 2011/2012 nach Inkrafttreten der schweizerischen Strafprozessordnung, die keine Prozesse nach dem Unmittelbarkeitsprinzip mehr vorsieht, in eine aus Strafrichtern und Laien zusammengesetzte und über Akteneinsicht verfügende Kammer des Strafgerichts umgewandelt.

Die Verwaltungsgerichtsbarkeit wird durch drei besondere Kammern des Appellationsgerichts, die als Verwaltungsgericht "()," Versicherungsgericht "()" und als Steuergericht "()" fungieren, sowie durch das Enteignungsgericht "()" ausgeübt.

Die Richter werden mit Ausnahme der vom Volk gewählten Friedensrichter vom Grossen Rat gewählt; ihre Amtszeit beträgt jeweils zehn Jahre. Die Aufsicht über die Richter wird durch den Richterrat "()" ausgeübt, der sich aus vier vom Grossen Rat und drei von den vollamtlichen Richtern gewählten Personen zusammensetzt.

In Bellinzona hat ferner das Bundesstrafgericht "()" seinen Sitz.

Die politischen Gemeinden "()" sind im Rahmen des kantonalen Staats- und Verwaltungsrechts autonom. Sie erfüllen die allgemeinen öffentlichen Aufgaben auf lokaler Ebene. Die Bürgergemeinden "()" und die römisch-katholischen sowie evangelisch-reformierten Kirchgemeinden sind ebenfalls öffentlich-rechtlich anerkannt und je für Angelegenheiten der Ortsbürger bzw. der Kirchen zuständig.

Die Kreise "()" sind heute lediglich noch Friedensrichtersprengel. Im Rahmen der Verwaltung spielen sie keine Rolle mehr und werden dementsprechend in der Kantonsverfassung nicht mehr erwähnt.

Die Bezirke "()," die weitgehend mit den verschiedenen Land- und Talschaften des Tessins identisch sind, dienen laut Verfassung zwar der Verwaltungsdezentralisation, spielen aber nur eine ganz marginale Rolle und haben deshalb in erster Linie identitätsstiftende Funktion.

2011 betrug das Bruttoinlandsprodukt (BIP) pro Einwohner 80'400 Schweizer Franken. 2012 wurden 212'679 Beschäftigte im Kanton Tessin gezählt, wovon 3'397 auf den primären (Urproduktion), 51'394 auf den sekundären (Industrie) und 157'888 auf den tertiären Sektor (Dienstleistung) entfielen. 33'311 Arbeitsstätten wurden 2012 im Kanton gezählt (davon 1'263 im primären, 5'009 im sekundären und 27'039 im tertiären Sektor). Die Arbeitslosenquote bezifferte sich per auf  Prozent gegenüber  Prozent auf eidgenössischer Ebene.

Der Kanton Tessin ist stark vom Tourismus abhängig, 12 Prozent der Beschäftigten (in 183'500 Vollzeitstellen) arbeiten in diesem Sektor und erwirtschafteten im Jahr 2012 2'725 Milliarden Schweizer Franken, was 9,6 Prozent des kantonalen Bruttoinlandsproduktes entspricht (Schweiz: 2,6 Prozent). Viele Besucher, vor allem aus der Deutschschweiz und aus Deutschland, kommen in den Tessin, um das mediterrane Klima und das italienische Flair, kombiniert mit schweizerischen Sekundärtugenden, zu geniessen. 2012 kamen 21,3 Millionen Gäste, davon waren 58 Prozent Übernachtungsgäste, 20 Prozent Tagesgäste und 41 Prozent andere Personen (Shopping, Casino, Transit). In Ferienwohnungen, Ferienhäusern und Rustici (Zweitwohnungen) werden 36 Prozent der Übernachtungen generiert, was für die lokale Wirtschaft von Bedeutung ist.

Der Lago Maggiore und der Luganersee sowie die Städte Bellinzona, Locarno, Ascona und Lugano zählen zu den wichtigsten touristischen Zentren. Hinzu kommen viele Berge, Täler, Dörfer und Weiler, die meist noch nicht von Touristen überrannt sind und viel von ihrem ursprünglichen Charme mit einer einfachen Bauweise und einer ländlichen, ruhigeren Lebensart erhalten haben.

Am Luganersee liegt die italienische Exklave Campione d’Italia, die für ihr Spielkasino bekannt ist.

Der Kanton Tessin führte seit seiner Gründung einen «Kampf» gegen die geographische Isolierung vom Rest des Landes und brachte enorme finanzielle Opfer für den Ausbau der Passstrassen; dabei blieb er lange machtlos gegen die witterungsbedingten Schliessungen der Alpenübergänge während der langen Winterzeiten.

Mit der Eröffnung des Gotthard-Bahntunnels im Jahr 1882 wurde erstmals eine ganzjährige Verbindung des Kantons mit der restlichen Schweiz geschaffen. In der zweiten Hälfte des 20. Jahrhunderts folgten zwei schneefreie Strassenverbindungen, nämlich der am 1. Dezember 1967 eröffnete San-Bernardino-Strassentunnel im Nachbarkanton Graubünden und der am 5. September 1980 dem Verkehr übergebene Gotthard-Strassentunnel.

Seit 2016 befindet sich der 57 km lange Gotthard-Eisenbahn-Basistunnel im Betrieb, der die Fahrtzeit von Zürich nach Mailand um ca. eine Stunde auf 2 h 40 min verkürzt und damit eine konkurrenzfähige Alternative zum Flugzeug und zum Auto geworden ist. Seit seiner Fertigstellung im Jahr 2016 ist er der längste Eisenbahntunnel der Welt und soll die mit dem wachsenden Güter- und Personenverkehr überforderten Gotthard-Strassentunnel und Gotthard-Bahntunnel entlasten.

Die Böden und klimatischen Bedingungen sind im Tessin in zwei Teile zu trennen, zwischen denen der Monte Ceneri steht. Nördlich und südlich des über 500 Meter hohen Passes gibt es Unterschiede in den Luftströmungen und den Geländebeschaffenheiten. Im Norden sind die Böden leicht, sandig, und wasserdurchlässig. Im Süden dagegen ist der Boden schwerer und fruchtbarer. Beide enthalten jedoch die für die Mineralienversorgung der Trauben wichtigen kalkhaltigen Sedimente. Das Tessin ist zu Recht eines der bekanntesten und trotz seiner geringen Grösse wichtigen Weinbaugebiete Europas.

In den 1980er-Jahren erlebte der Weinbau im Tessin einen grossen Aufschwung. Zu verdanken ist das vor allem einigen fortschrittlichen Winzern, die für Qualität mit beschränkter Anbaumenge eintraten, und innovative Weinbaumethoden publizierten. Ein kleiner Teil der Anbaufläche wird noch für Amerikaner-Reben genutzt. Aus ihnen werden Traubensaft und Grappa hergestellt.

Im Kanton Tessin sind neun Schuljahre Pflicht. Diese teilen sich in fünf Jahre "scuola elementare" (Primarschule) und vier Jahre "scuola media" (Mittelschule) auf. Hat ein Schüler diese neun Schuljahre absolviert, kann dieser freiwillig das "liceo," das dem Gymnasium entspricht, oder ähnliche Schularten besuchen. Ab dem 15. Lebensalter darf eine Person eine Arbeit suchen. Das durchschnittliche Einschulungsalter liegt bei sechs Jahren.

Das Tessin besitzt eine Universität, die allerdings nicht als Volluniversität ausgestaltet ist, mit drei Fakultäten der Kommunikations-, Wirtschaftswissenschaften und der Informatik in Lugano sowie einer Architekturfakultät in Mendrisio. Die Universität der italienischen Schweiz (USI/SUP) hat etwa 3000 Studierende. Ferner werden Fachhochschulstudien an der "Scuola universitaria professionale della Svizzera italiana" (SUPSI) angeboten.

Jeweils im August findet das Internationale Filmfestival von Locarno statt, auf dem der «Goldene Leopard» verliehen wird.

Im Mittelalter wurde das Gebiet des heutigen Kantons Tessin von den Herzögen von Mailand beherrscht (→ Geschichte Mailands). Die Eidgenossen errangen ihre Vorherrschaft in den südlichen Alpentälern während der sog. Ennetbirgischen Feldzüge zwischen 1403 und 1515, die Gebiete ausser Livinen ("Leventina"), das Uri untertan war, wurden Gemeine Herrschaften der gesamten Alten Eidgenossenschaft (ausser Appenzell). 

Mit dem Einrücken französischer Revolutionstruppen 1798 ins Gebiet der Eidgenossenschaft endete der Untertanen-Status des Tessins. Von Napoleon Bonaparte vor die Wahl gestellt, zur Lombardei oder zur «Helvetischen Republik» zu gehören, schufen die Tessiner die Parole «liberi e svizzeri». Das Tessin wurde zur Mediationszeit 1803 zum vollwertigen Schweizer Kanton aufgewertet, es löste die 1798 gegründeten helvetischen Kantone Bellinzona und Lugano ab. 

Die innere Geschichte des damals jungen Kantons blieb jedoch infolge des Gegensatzes zwischen den Klerikalen, welche im Sopraceneri, und den Liberalen, die im Sottoceneri die entschiedene Mehrheit besassen, bewegt. Durch innere Auseinandersetzungen und Vermittlung oder Einschreiten der Bundesbehörden mussten im Verlaufe des 19. Jahrhunderts diverse Konflikte zwischen den beiden Gruppen gelöst werden ("vgl:" Kulturkampf in der Schweiz).

Der Kanton Tessin umfasst 115 politische Gemeinden (2. April 2017). Der Tessiner Staatsrat beabsichtigt, die Anzahl der Gemeinden bis zum Jahr 2020 von 135 auf 23 zu reduzieren (vergleichbar mit der Glarner Gemeindereform).

Nachfolgend aufgelistet sind bevölkerungsreichsten politischen Gemeinden des Tessins mit über 5'000 Einwohnern per :

Der Kanton Tessin ist in acht Bezirke "()" eingeteilt, diese wiederum in 38 Kreise "():"


Zur Bundestagswahl 1972 produzierte der politische Grafiker Klaus Staeck sein ironisches politisches Plakat "Deutsche Arbeiter! Die SPD will euch eure Villen im Tessin wegnehmen". Das Plakat erreichte eine Druckauflage von 75.000 Exemplaren und ist das bekannteste von Staecks Motiven.



</doc>
<doc id="9666" url="https://de.wikipedia.org/wiki?curid=9666" title="Niederschlag">
Niederschlag

Unter Niederschlag versteht man in der Meteorologie Wasser inklusive seiner Verunreinigungen, das aus Wolken, Nebel oder Dunst (beides Wolken in Kontakt mit dem Boden) oder wasserdampf­haltiger Luft (Luftfeuchtigkeit) stammt und das

Durch Verdunstung und Sublimation gelangt Wasserdampf in die Atmosphäre. Wolken entstehen von Kondensationskeimen ausgehend durch Kondensation der Feuchtigkeit in der Luft. Um wieder als Niederschlag auf die Erdoberfläche fallen zu können, muss die Größe (bzw. Masse) der kondensierten Teilchen einen bestimmten Wert überschreiten. Durch den Niederschlag wird der Wasserkreislauf geschlossen.

Die Häufigkeit und die durchschnittliche Menge des Niederschlages sind charakteristisch für die entsprechenden geographischen Gebiete. Der Niederschlag ist dabei ein Faktor, der das lokale Klima mitbestimmt. Besonders für die Landwirtschaft ist er relevant, da erst ab einer bestimmten Niederschlagsmenge erfolgreicher Regenfeldbau möglich ist. Aus einer angetroffenen Ökozone kann daher meist grob auf eine mittlere Niederschlagsmenge gefolgert werden.

Bei der Kondensation aus feuchter Luft geht Kondensationswärme, beim Resublimieren geht Resublimationswärme aus dem Wasserdampf, beim Gefrieren geht Gefrierwärme aus dem Wasser in die Umgebung (Luft, Wasser, Bewuchs, sonstige Oberflächen) über. Gefriert unterkühlter Nebel oder unterkühlter Regen, so ist der Wärmeübergang gering. Beim Verdunsten und Sublimieren von Niederschlag wird Wärme der Umgebung entzogen, dies wirkt abkühlend auch auf die Erdoberfläche und reguliert teilweise das (Klein)Klima.

Niederschlag kann unter bestimmten meteorologischen Konstellationen künstlich erzeugt werden, indem eine große Menge an künstlichen Eiskeimen, also Kondensationskernen (z. B. Silberiodid) in unterkühlte Wolken ausgebracht wird; siehe Hagelflieger. Aus großtechnischen Wasserdampfemissionen stammender Industrieschnee ist ebenso künstlicher Niederschlag, der zivilisatorisch bedingt entstehen kann.

Kunstschnee von Beschneiungsanlagen, Kunsteis und Schwarzeis (gefrorenes See- und Meerwasser) werden nicht zu den Niederschlägen gezählt, weil das Wasser nicht direkt und hauptsächlich aus Wolken, Nebel oder Luftfeuchtigkeit stammt. Ortsverlagerter Niederschlag (beispielsweise vom Schneepflug versetzter Schnee, Sprühfahnen, Dachlawinen, Regenwasser in Fließgewässern) bleibt trotzdem Niederschlag.

„Regen“ aus Beregnungsanlagen wird i. A. nicht dem Niederschlag zugerechnet, kann jedoch durch die dadurch erhöhte Verdunstung zu vermehrter Wolkenbildung und erhöhtem „allgemeinem Niederschlag“ führen.


Gemessen wird mit zwei verschiedenen Arten von Messgeräten:

Die meisten Niederschlagsmesser sammeln den Niederschlag als punktuelle Niederschlagsmessung in einem Messgefäß. Ein Millimeter (Maßeinheit) entspricht der Wasserhöhe (Niederschlagshöhe) von 1 mm, die sich ergäbe, wenn kein Wasser abflösse oder verdunstete. Alternativ wird oft auch die Wassermenge (Niederschlagsmenge) in formula_1 (ebene Fläche) angegeben. Ein Millimeter ist gleich einem Liter pro Quadratmeter. Jene Anteile, die nicht in Form flüssigen Wassers vorkommen, werden entweder in die entsprechende Menge desselben umgerechnet (sofern die Dichte bekannt ist), oder bei Schnee und Hagel durch leichte Erwärmung, um die Verdampfung und den Messfehler zu verringern, in Wasser umgewandelt.

Neben der direkten Berechnung vor Ort können Niederschlagsintensitäten auch durch Radarmessungen bestimmt werden. Dazu zieht man die von der Stärke des Regens abhängige Radarreflektivität heran.
Über Niederschlagsradare können die gefallenen Mengen inzwischen auch flächendeckend geschätzt werden. Vor allem im Bereich des Hochwassermanagements ist dies von Bedeutung (punktuelle Messwerte verifizieren bzw. kalibrieren). Neben der reinen Niederschlags-Höhe bzw. Menge sind dabei vor allem auch die Niederschlagsintensität und die Niederschlagsdauer wichtig.

Langfristige (klimatologische) Niederschlagsmessungen lassen statistische Berechnungen zu, um die mittlere Häufigkeit von unterschiedlichen Niederschlagsereignissen (v. a. Starkregenereignisse) anzugeben, die Intensität und Dauer zueinander in Bezug setzen.

Die "Niederschlagshöhe" wird in der Meteorologie für Regen üblicherweise in Millimeter (Wasserhöhe) angegeben und für gefrorene Niederschläge in Zentimeter. Sie gibt wiederum Aufschluss über die Niederschlagsmenge.

Wenn die Niederschlagshöhe, bzw. die daraus resultierende Niederschlagsmenge nicht messbar ist, wird sie mit „kleiner 0,1 mm“ angegeben. Bei Schneefall, Hagel oder Graupel wird sie in formula_2 (Zentimeter) angegeben. Eine Umrechnung in die Niederschlagsmenge in Liter bzw. in die wasseräquivalente Niederschlagshöhe pro Quadratmeter kann nur nach Bestimmung der Dichte erfolgen, da bei gefrorenen Niederschlägen große Unterschiede bestehen können.

Man betrachtet das flüssige Wasser (Niederschlagswasser), das sich bei Niederschlag (Regen, Schnee, Hagel, Nebel usw.) in einer definierten Zeitspanne (siehe auch Niederschlagsintensität) in einem nur nach oben offenen Gefäß mit definierter horizontaler Öffnung sammelt. Als "Niederschlagsmenge" bezeichnet man das Volumen der Flüssigkeit bezogen auf die Fläche der Öffnung und gibt sie an in Litern pro Quadratmeter (1 Liter ist 1 Kubikdezimeter). Mit der Umrechnung
ist sie nur zahlenmäßig gleich der Niederschlagshöhe in Millimetern. Wenn man aus der Millimeter-Angabe also die Niederschlagsmenge in Litern berechnen will, muss die Millimeter-Zahl mit der waagrecht projizierten Sammelfläche in Quadratmetern multipliziert werden: Millimeter x Quadratmeter = Liter.

Das übliche Messintervall (es ist immer mit anzugeben) sind 24 Stunden (1 Tag), aber auch 48 oder 72 h und so fort für länger dauernde Starkregen-Ereignisse, für Schlagregen auch 1 Stunde und entsprechend mehr, aber auch bis hin zu 5 Minuten (etwa als Bemessung für Abflusseinrichtungen an Gebäuden) sowie ein Monat, eine Jahreszeit und das ganze Jahr für klimatologische Betrachtungen. In den Fällen, in denen man mehrere Standardintervalle addiert, spricht man auch von "Niederschlagssumme".

Faktoren wie Verdunstung, Bodenversickerung oder Abfluss werden bei der Messung nicht berücksichtigt.

Der Begriff "Niederschlagsdauer" steht für die Zeitdauer eines einzelnen Niederschlagsereignisses. Auf Basis der Niederschlagsdauer unterscheidet man zwischen Dauerniederschlägen und Schauern. Zudem ist sie für die Festlegung von Wiederkehrsintervallen von Starkregenereignissen und Überschwemmungsszenarien notwendig.

Als "Niederschlagsintensität" bezeichnet man den Quotienten aus Niederschlagshöhe bzw. -menge und Zeit. Sie wird für Regen in der Regel in Millimeter pro Stunde beziehungsweise Liter je Quadratmeter (und Stunde, was oft unerwähnt bleibt), bei Schnee in Zentimeter pro Stunde angegeben.


Andere Angaben für statistische Zwecke können noch Millimeter (bei Schnee Zentimeter) pro Tag, Woche, Monat oder Jahr sein.

Ein mittelstarker Regenschauer in Mitteleuropa hat eine Intensität um 5 mm/h, ein Starkregen um 30 mm/h oder als Platzregen 5 mm/5 min. Bei einem heftigen Unwetter kann die Regenmenge auf 50 mm/h und mehr anwachsen. Niederschlagsmengen von wenigen 100 mm in einigen Tagen (etwa 300 mm/4 d) führen schon, wenn sie großflächig sind, zu schweren Hochwasserereignissen an den großen Flüssen. Tropenstürme erreichen Werte von 130 mm/h und weit darüber.

Für die durchschnittliche Höhe des Niederschlags im Laufe einer bestimmten Periode an einem definierten Ort oder in einer bestimmten Region existieren folgende meteorologische-klimatologischen Ausdrücke.

Für die Charakteristika eines speziellen Jahres werden die gemessenen Niederschläge aufsummiert (kumuliert), und dann mit den mittleren Niederschlägen desselben Bemessungszeitraumes verglichen: So kann eine Aussage gemacht werden, ob ein Monat oder Jahr „zu nass“ oder „zu trocken“, ein Winter „schneereich“ ist, oder dass bei einem Starkregenereignis „der Normalniederschlag eines Monats in drei Tagen gefallen“ ist. Ebenso können Klimata und Jahreszeitcharakteristika verglichen werden, also etwa „wintertrocken“, „Niederschlagsmaximum im Spätsommer“.

"Regen, Positivrekorde"
Regional begrenzte Extremniederschläge können auch deutlich höher liegen. So wurde für das Regenereignis am 2. Juni 2008 im baden-württembergischen Killer- und Starzeltal ein Niederschlag von rund 240 Millimeter in einer Stunde ermittelt.

"Regen, Negativrekorde"



</doc>
<doc id="9667" url="https://de.wikipedia.org/wiki?curid=9667" title="Lugano">
Lugano

Lugano ( , deutsch veraltet "Lauis", ) ist eine Stadt und politische Gemeinde im Bezirk Lugano des Schweizer Kantons Tessin. Sie liegt im Sottoceneri und ist die grösste des Kantons. Sie ist in die Kreise Lugano West, Lugano Ost und seit 2013 auch Lugano Nord gegliedert.

Die Stadt ist nach Zürich und Genf der drittgrösste Finanzplatz der Schweiz. Seit den Eingemeindungen von Pregassona (2004), dann Barbengo, Carabbia und Villa Luganese im Jahr 2008 und von Bogno, Cadro, Carona, Certara, Cimadera, Sonvico und Val Colla im Jahr 2013 ist Lugano flächenmässig die siebtgrösste Schweizer Stadt, belegt den neunten Platz hinsichtlich der Einwohnerzahl und den zehnten Platz bezüglich der vorhandenen Arbeitsplätze.

Die Buchstaben LVGA im Wappen stehen für die Anfangsbuchstaben des Ortsnamens. Die Stadt ist die grösste italienischsprachige politische Gemeinde ausserhalb Italiens.
Der Ort liegt im Süden des Bezirks Lugano und des Kantons an der Mündung des Flusses Cassarate in den Luganersee. Lugano zieht als Universitäts-, Kongress- und Kulturstadt (vor allem zwischen Frühjahr und Herbst) zahlreiche Besucher aus Italien und von jenseits der Alpen an.

In der vom Schweizer Bundesamt für Statistik definierten statistischen Raumkategorie wurde Lugano der Metropolregion Tessin zugerechnet, die neu zum "multipolaren Agglomerationssystem" herabgestuft wurde. Dieses umfasst mehrere Agglomerationen des Tessins und der Lombardei mit insgesamt über 500'000 Einwohnern. Es ist mit Como-Chiasso-Mendrisio Nachbaragglomeration der Metropolregion Mailand (Grande Milano) mit rund 7,5 Millionen Einwohnern. Das Zentrum Mailands ist mit dem Auto und der Bahn in zirka einer Stunde erreichbar.

Lugano liegt am Luganersee (italienisch Lago di Lugano, in Italien Lago Ceresio) und ist umgeben von den drei Aussichtsbergen Monte Brè () im Osten, Monte San Salvatore () im Westen und dem Sighignola () (am gegenüberliegenden Seeufer), dessen Gipfel "Balcone d’Italia" bereits auf italienischem Boden liegt.

Mit einem Jahresmittel von 12,4 °C für die Periode 1981–2010 gehört Lugano zusammen mit Locarno und Grono zu den wärmsten Orten der Schweiz.

Die Quartiere 1–9 sind die alten Stadtquartiere, die schon vor der grossen Eingemeindung von 2004 zur Stadt gehörten. Bei den Quartieren 11–18 handelt es sich um die Gebiete der 2004 mit Lugano fusionierten Gemeinden. Die Quartiere 19–21 kamen durch die Eingemeindung des Jahres 2008, die Quartiere 22–25 durch die Eingemeindung des Jahres 2013 hinzu.

Aufgrund einiger Bodenfunde und im Raum Lugano aufgefundener Grabinschriften ist anzunehmen, dass das Gebiet um Lugano von Lepontiern besiedelt war. 804, 844 (Kopie um 1300), 854 (Kopie um 1300) und 875 wird Lugano erstmals urkundlich erwähnt; die Namensformen lauteten erst "Luanasco," dann "Luano". Die Bedeutung des Namens ist unsicher, womöglich geht er auf lateinisch "lūcus" «Hain, Wald» zurück.

Im Mittelalter war Lugano jahrhundertelang von Konflikten zwischen Como und Mailand betroffen, da diese oft auf Schlachtfeldern ausgetragen wurden, die auf dem Gebiet des heutigen Kantons Tessin liegen. In der zweiten Hälfte des vierzehnten Jahrhunderts gelangte die Stadt unter die Herrschaft der Mailänder Visconti. Später wurde sie von französischen Söldnern besetzt, die 1513 ihrerseits von den Eidgenossen vertrieben wurden; seither stand Lugano unter eidgenössischer Herrschaft. Mit dem Einrücken französischer Revolutionstruppen 1798 in das Gebiet der Eidgenossenschaft endete der Untertanenstatus des Tessins, und Lugano wurde für einige Jahre zum Hauptort des Kantons Lugano der Helvetischen Republik. 1803 kam Lugano zum Kanton Tessin, dessen Hauptort bis 1878 alle sechs Jahre zwischen Bellinzona, Locarno und Lugano wechselte.

1972 wurden die früheren Gemeinden Brè-Aldesago und Castagnola in die Stadt Lugano eingegliedert.

2004 fusionierten acht weitere Gemeinden mit der Stadt Lugano: Breganzona, Cureggia, Davesco-Soragno, Gandria, Pambio-Noranco, Pazzallo, Pregassona und Viganello. Dadurch vergrösserte sich sowohl die Fläche als auch die Bevölkerung von Lugano erheblich.

Am 30. September 2007 stimmten die Stimmbürger von Barbengo, Carabbia und Villa Luganese sowie von Lugano der Eingemeindung dieser drei Gemeinden zu. Die Stimmberechtigten der Gemeinde Cadro hingegen lehnten die Fusion ab, weshalb Villa Luganese zu einer Exklave der Stadt Lugano wurde. Die Eingemeindung wurde am 20. April 2008 vollzogen.

Per 14. April 2013 wurden die Gemeinden Bogno, Cadro, Carona, Certara, Cimadera, Sonvico und Val Colla mit Lugano fusioniert, wodurch die Stadt rund 3400 zusätzliche Einwohner erhielt.

Aufgrund der durch See und Berge beengten Verhältnisse im Stadtgebiet spielt sich die wirtschaftliche Entwicklung heute hauptsächlich ausserhalb der Gemeindegrenzen in der Vedeggio-Talebene ab.

Seit den Eingemeindungen in den Jahren 2004, 2008 und 2015 zählt die Stadt Lugano 63'668 Einwohner. Im ganzen Ballungsgebiet sind es rund 145'000 Einwohner.
Amtssprache ist Italienisch. Der Tessiner Dialekt "()," eine Varietät des zum Arealtypus Norditalienisch oder gehörenden Lombardischen, ist auf dem Rückzug; Urbanisierung, Binnenmigration und Tourismus werden als Ursachen angesehen.

Im Jahr 2014 gaben 87,6 % Italienisch, 9,7 % Deutsch, 6,1 % Englisch und 5,8 % Französisch als Hauptsprache an (Nennung von mehr als einer Hauptsprache möglich).

In den Tessiner Schulen wird als erste Fremdsprache Französisch und als zweite Deutsch unterrichtet.

Stand 2014

Die Legislative wird durch den Gemeinderat repräsentiert. Er besteht aus 60 Mitgliedern aus 6 Parteien und wird alle vier Jahre durch das Volk neu gewählt Sitzverteilung 2016–2020 (in Klammern Resultate von 2013): FDP 18 (19), Lega 18 (18), CVP 9 (9), SP 9 (9), SVP 3 (2), GPS 3 (3).

Die Exekutive ist der Stadtrat. Er besteht aus sieben Mitgliedern und wird wie der Gemeinderat alle vier Jahre durch das Volk neu gewählt.

Stadträte (seit 2016): Stadtpräsident Marco Borradori (Lega), Lorenzo Quadri (Lega), Michele Foletti (Lega), Michele Bertini (FDP), Giovanna Masoni Brenni (FDP), Cristina Zanini Barzaghi (SP), Angelo Jelmini (CVP).
Bei den Schweizer Parlamentswahlen 2015 betrugen die Wähleranteile in Lugano: Lega 26.3 %, FDP 22.3 %, SP 15.2 %, CVP 15.1 %, SVP 13.3 %, Grüne 4.1 %, "MontagnaViva" 1.1 %, PCSI "1.0 %".

Aktive Ortsbürgerfamilien von Brè-Aldesago
Ehemalige Ortsbürgerfamilien von Brè-Aldesago

Das Stadtbild ist im Inventar der schützenswerten Ortsbilder der Schweiz (ISOS) als schützenswertes Ortsbild der Schweiz von nationaler Bedeutung eingestuft.
Sehenswert ist der "Parco civico" mit seiner üppigen südlichen Vegetation und der "Villa Ciani". Westlich des Parks schliesst sich die etappenweise zwischen 1864 und 1920 erstellte Seepromenade an, die bis nach Paradiso führt.

Weitere Attraktionen von Lugano sind die beiden Hausberge Monte San Salvatore und Monte Brè, von denen aus man ein schönes Panorama über die Stadt, den Luganersee und die Tessiner Berge geniesst. Beide Berge sind sowohl per Bahn als auch zu Fuss erreichbar. Am Fusse des Monte Brè liegen das Dorf Gandria und die Villa Favorita.

Die mittelalterlichen und frühneuzeitlichen Altstadt von Lugano wurde auf der Grundlage des Richtplanes von 1902 zwischen 1910 und 1942 zum grössten Teil abgebrochen oder ausgekernt und durch ein neues Stadtzentrum ersetzt. Von der einstigen Baustruktur existieren deshalb heute nur noch einige Kirchen und vereinzelte Profanbauten. Neben diesen sind vor allem die Flaniermeile "Via Nassa" und die "Piazza Riforma" einen Besuch wert.

Kirchen

Palazzi und weitere Gebäude
Von der ehemaligen Altstadt sind nur noch wenige zivile Gebäude erhalten. Deren drei tragen den Namen des Adelsgeschlechts der Riva.

Von Gebäuden, die im Rahmen der Stadterneuerung errichtet wurden, sind etwa die "Palazzi Gargantini" (1912–1930, Neobarock bis Art déco), die "Palazzina Alhambra" (1926, Neorenaissance), der "Palazzo degli Studi" (1903–1904, Historismus) und die "Kantonsbibliothek" (1937–1940, Meilenstein der modernen Architektur im Tessin) zu nennen.


Tourismus, Finanzen und Handel sind die Hauptpfeiler der Wirtschaft Luganos und sichern insgesamt 27'000 Arbeitsplätze. Weiter verfügt Lugano als eine von sieben Schweizer Städten über ein Casino mit A-Konzession.

Lugano ist das drittgrösste Finanzzentrum der Schweiz. Neben der 1873 gegründeten Banca della Svizzera Italiana BSI SA, der ältesten Bank des Kantons Tessin, haben die Banca Arner, Banca Commerciale Lugano, Banca del Ceresio, Banca del Sempione und die Cornèr Bank ihren Sitz in Lugano. Ihren Sitz in Lugano hatte auch die einst grösste Bank des Kantons Tessin, die Banca del Gottardo. Dazu sind in Lugano neben den beiden Schweizer Grossbanken UBS und Credit Suisse auch zahlreiche weitere Schweizer und internationale Private-Banking-Institute mit Niederlassungen vertreten wie Banque Cramer & Cie SA.

Daneben sind in Lugano auch die Rohstoffhandelsunternehmen Duferco, Filofibra und Interbulk Trading ansässig.

Die Gemeinde beherbergt das Studio des Schweizer Radios der italienischen Sprache RSI und das Orchestra della Svizzera italiana (OSI), das aus dem 1935 gegründeten RSI Orchestra hervorgegangen ist. Lugano ist Sitz verschiedener freier Theatergruppen, die sowohl in Lugano und im Tessin, als auch schweiz- und weltweit aktiv sind, u. a. Teatro Pan, Markus Zohner Theater Compagnie, Teatro Sunil, Teatro delle Radici.

International bekannt ist das seit 1979 veranstaltete Estival Jazz, die grösste Jazz-Veranstaltung Europas.

1956 fand in Lugano unter dem Titel Gran Premio Eurovisione della Canzone Europea die erste Ausgabe des Eurovision Song Contest statt.
Berühmt ist auch das "Progetto Martha Argerich" unter der Leitung der weltbekannten Pianistin.

Die Schweizerische Nationalphonothek (Fonoteca Nazionale Svizzera), gegründet 1987, befindet sich ebenfalls in Lugano.

Der Bahnhof Lugano liegt an der Eisenbahnlinie Zürich/Basel – Mailand (Gotthardbahn). Es halten hier EuroCity-Züge, Intercity-Express, InterRegio-Züge, RegioExpress und die S-Bahn. Nach zweijähriger Bauzeit wurde der renovierte Bahnhof am 11. Dezember 2016 eingeweiht.

Die Stadt liegt an der Autobahn A2 (Basel – Gotthard – Lugano – Chiasso) mit zwei Ausfahrten (Nr. 49 Lugano Nord und Nr. 50 Lugano Süd).

In und um Lugano wurden bis in die zweite Hälfte des 20. Jahrhunderts vier meterspurige Strassenbahnen betrieben, nämlich die Strassenbahn Lugano, die Lugano-Cadro-Dino-Bahn (LCD), die Lugano-Tesserete-Bahn (LT) sowie die Lugano-Ponte-Tresa-Bahn (FLP). Die ersten drei wurden später auf Busbetrieb umgestellt, die FLP hingegen schrittweise zur leistungsfähigen Vorortbahn ausgebaut. Für die Zukunft steht eine unterirdische Einführung in die Innenstadt (unter Aufgabe des jetzigen Endpunkts beim SBB-Bahnhof) zur Debatte.

Innerhalb der Stadt wurden die alten, meist einspurigen Tramstrecken zwischen 1954 und 1959 durch den Trolleybus Lugano ersetzt. In den 1990er-Jahren wurde der Unterhalt der Trolleybusse sowie dessen Infrastruktur zunehmend zurückgefahren. Nachdem das Bundesamt für Verkehr für Teile des Fahrzeugparks nur noch befristete Betriebsbewilligungen gewährt und diverse weitere Auflagen an den Weiterbetrieb des Netzes geknüpft hatte, wurde der Trolleybusbetrieb am 30. Juni 2001 auf Dieselbusse umgestellt und die elektrischen Anlagen demontiert. Im Zusammenhang mit den Umbauplänen der FLP wird auch die Wiedereinführung eines Trambetriebs diskutiert.

Für den innerstädtischen Verkehr wichtig und daher stark frequentiert ist die 1886 eröffnete Standseilbahn "(Funicolare)" genannt "Sassellina," welche die Altstadt ab der Piazza Cioccaro mit dem SBB-Bahnhof verbindet. Nach einer zweijährigen Renovationsphase hat die neue Standseilbahn am 11. Dezember 2016 ihren Betrieb wieder aufgenommen.
Die Bahn überwindet eine Höhendifferenz von 50 Metern und sie ist die höchst frequentierte Standseilbahn der Schweiz.

Westlich der Stadt liegt der Flughafen Lugano-Agno, von dem aus Linienflüge nach Genf und Zürich durchgeführt werden. Mit dem Zug ist er durch die Lugano-Ponte-Tresa-Bahn (Linie S60 der S-Bahn Tessin) mit dem Stadtzentrum verbunden, mit dem Auto erreicht man ihn in Richtung Ponte Tresa und über die Anhöhe von Sorengo (Abzweigung zur Collina d’Oro).

Die ""Società Navigazione del Lago di Lugano"" , die ""Schifffahrtsgesellschaft des Luganersees"" also, verbindet mehrere innerhalb der Stadt liegende Schiffslandestege und die am Luganersee gelegenen Orte.

In Lugano befindet sich die 1996 gegründete Università della Svizzera italiana mit den Fakultäten Kommunikationswissenschaft, Wirtschaftswissenschaft und Informatik. Zur gleichen Universität gehört auch die Architekturakademie in Mendrisio.

Im Zentrum von Lugano bei der Università della Svizzera italiana steht die Scuola Europea di Studi Avanzati in Oftalmologia (ESASO), eine Weiterbildungseinrichtung für Augenärzte.

Das Franklin College, eine von der Schweiz und den USA anerkannte private Universität, hat seinen Sitz in Lugano. Seine Schwerpunkte liegen in den Fächern Internationale Beziehungen, Wirtschaft sowie Geschichte.

Im Lugano-Cornaredo unterhält die ETH das nationale Centro svizzero di calcolo scientifico CSCS.

Im Manno befindet sich die Università professionale della Svizzera italiana (SUPSI) und die Scuola superiore di teatro di movimento.

Mit der 1992 gegründeten Theologische Fakultät Lugano befindet sich auch eine Philosophisch-theologische Hochschule in Lugano; sie dient vorwiegend der Priesterausbildung.

Das "Regionalspital von Lugano" (Ospedale Regionale di Lugano, ORLugano) ist aus den beiden Spitälern "Ospedale Civico" und "Ospedale Italiano" hervorgegangen.

Ferner befinden sich folgende Kliniken in der Stadt Lugano:

Das Gesundheitswesen im Grossraum Lugano wird zudem durch folgenden Institutionen ergänzt:

Der Eishockeyverein HC Lugano spielt in der höchsten Eishockeyliga der Schweiz, der NLA, im Stadion La Resega. Er ist siebenfacher Schweizer Meister (1986, ’87, ’88, ’90, ’99, 2003, ’06) und damit national erfolgreichste Eishockey-Mannschaft seit Einführung der Play-Offs 1985.

Der Fussballverein FC Lugano spielt seit 2015 wieder in der höchsten Fussballliga der Schweiz im Stadio di Cornaredo. Er wurde drei Mal Schweizer Meister (1938, ’41 und ’49) und drei Mal Schweizer Cupsieger (1931, ’68 und ’93). Ottmar Hitzfeld spielte von 1978 bis 1980 beim FC Lugano.

Hier gibt es auch Football Club Rapid Lugano Football Club Femminile Rapid Lugano, Football Club Trevano und F. C. Os Lusiadas.

Lugano war einer von sechs Austragungsorten der Fussball-Weltmeisterschaft 1954, der UCI-Strassen-Weltmeisterschaften 1953 und der 1996.

Zwischen 1999 und 2010 fand im Tennis-Club Lido Lugano das ATP Challenger Turnier statt, das der Schweizer Stan Wawrinka zweimal gewann.

Persönlichkeiten, die in Lugano gewirkt haben

In Lugano geborene Persönlichkeiten

Geschichte

Kunstgeschichte



</doc>
<doc id="9668" url="https://de.wikipedia.org/wiki?curid=9668" title="Ulrich Leman">
Ulrich Leman

Ulrich Leman (* 15. Oktober 1885 in Düsseldorf; † 22. April 1988 in Deià, Mallorca) war ein Maler des Rheinischen Expressionismus.

In Düsseldorf geboren und in Danzig aufgewachsen, kam er 1903 zum Studium nach Düsseldorf. Nach der Kriegsteilnahme im Ersten Weltkrieg kam er 1919 nach Düsseldorf zurück und fand Aufnahme in der Gruppe Das Junge Rheinland, wurde Meisterschüler von Heinrich Nauen, ging 1922 bis 1925 mit einem Staatsstipendium nach Griechenland. 1927 unternahm er mit einigen Künstlern des Jungen Rheinlandes und der „Künstlermutter“ Ey seine erste Reise nach Mallorca. 1928 erhielt er den „Großen Preis der Rheinprovinz“ auf der Großen Deutschen Kunstausstellung in Düsseldorf. 

1929 bis 1939 lebte und arbeitete er in Deià/Mallorca, Paris, Düsseldorf und Zoppot bei Danzig. Nach dem Zweiten Weltkrieg flüchtete er vor der Russischen Armee und kam nach Hessen. Hier lebte und arbeitete er in der Nähe von Gießen und Wetzlar, war Mitglied des Oberhessischen Künstlerbundes und beschickte dessen Ausstellungen. Später lebte und arbeitete er abwechselnd wieder in seinem 1930 erworbenen Haus in Deià (Mallorca) und Aßlar, in der Nähe von Wetzlar in Deutschland. Leman starb 1988, er ist auf dem Friedhof in Deià begraben.

Erst Ende des 20. Jahrhunderts fand sein überwiegend am Anfang und bis zum Beginn des Zweiten Weltkrieges entstandenes Werk in der Kunstgeschichte wieder größere Beachtung.




</doc>
<doc id="9669" url="https://de.wikipedia.org/wiki?curid=9669" title="Graupel">
Graupel

Graupel ist eine Form von Niederschlag, bei dem Schneekristalle durch angefrorene Wassertröpfchen zu kleinen, bis zu 5 mm großen Kügelchen verklumpt werden. Bei Korngrößen von unter einem Millimeter Durchmesser spricht man auch von Griesel.

Graupelkörner sind im Vergleich zu Hagel deutlich kleiner und weisen lediglich einen maximalen Durchmesser von 2 bis 5 Millimeter auf. Ihre Dichte ist geringer als die von Hagelkörnern und sie haben eine rauere Struktur. Dadurch fallen sie langsamer und können kaum Schaden anrichten. Im Gegensatz zum Hagel fällt Graupel hauptsächlich im Winter bei Temperaturen um 0 °C.

Graupel entsteht nur, wenn die Wolken nicht zu viel Feuchtigkeit enthalten, was oftmals im Frühjahr der Fall ist. Häufig tritt Graupel bei trockener Polarluft auf. Bei zu feuchten Wolken entsteht eher Hagel. 

Es gibt verschiedene Arten von Graupel:


</doc>
<doc id="9671" url="https://de.wikipedia.org/wiki?curid=9671" title="Kinyarwanda">
Kinyarwanda

Kinyarwanda, auch Kinjaruanda oder Ruandisch, ist eine in Ruanda sowie in angrenzenden Gebieten der Demokratischen Republik Kongo und Ugandas verbreitete Bantusprache mit über sieben Millionen Sprechern. Es ist Muttersprache von Hutu, Tutsi, Banyamulenge und Twa. 

Kinyarwanda ist neben Französisch und Englisch Amtssprache in Ruanda und mit dem Kirundi, der Sprache des benachbarten Staates Burundi, praktisch identisch. Anfang des 20. Jahrhunderts, unter der deutschen Kolonialzeit in Ruanda-Urundi, erfolgte eine lateinisch basierte Verschriftung. 




</doc>
<doc id="9672" url="https://de.wikipedia.org/wiki?curid=9672" title="Max Beckmann">
Max Beckmann

Max Carl Friedrich Beckmann (* 12. Februar 1884 in Leipzig; † 27. Dezember 1950 in New York City) war ein deutscher Maler, Grafiker, Bildhauer, Autor und Hochschullehrer. Beckmann griff die Malerei des ausgehenden 19. Jahrhunderts ebenso auf wie die kunsthistorische Tradition und formte einen figurenstarken Stil, den er ab 1911 der aufkommenden Gegenstandslosigkeit entgegensetzte.

Beckmann war in seiner frühen Zeit Mitglied der Berliner Secession, stilisierte sich dann aber lieber als Einzelgänger. Der Moderne, insbesondere Pablo Picasso und dem Kubismus, setzte er eine eigenwillige Räumlichkeit entgegen. Zudem entwickelte er eine erzählende und mythenschaffende Malerei, insbesondere in zehn Triptychen. Besondere Bedeutung kommt Beckmann als prägnantem Zeichner, Porträtisten (auch zahlreicher Selbstporträts) und als subtilem Illustrator zu.

Max Beckmann wurde als drittes Kind von Antonie und Carl Beckmann geboren. Die beiden Geschwister Margarethe und Richard waren wesentlich älter. Die Eltern stammten aus der Gegend von Braunschweig, wo der Vater Müller gewesen war. In Leipzig betrieb er eine Mühlenagentur. In Falkenburg in Pommern, dem heutigen Złocieniec, besuchte Max Beckmann die Volksschule. Von Ostern 1894 bis November 1894 war er Schüler der Sexta des Königlichen Gymnasiums in Leipzig. Als Elfjähriger zog er mit der Familie nach Braunschweig um. Hier starb kurz darauf der Vater. Max Beckmann setzte den Schulbesuch in Braunschweig und Königslutter fort. Sein erstes überliefertes Aquarell, eine Märchenillustration, wird auf 1896 datiert, das erste Selbstporträt auf 1897. Seit dieser Zeit begeisterte Beckmann sich für fremde Kulturen. Er war ein schlechter Schüler, zeigte aber früh umfassendes Interesse an der Kunstgeschichte. 1899 besuchte er ein privates Internat in einem Pfarrhaus in Ahlshausen bei Gandersheim. Aus dieser Zeit stammen die ersten überlieferten Briefe und Zeichnungen. Im darauffolgenden Winter lief er von dort weg. Im Jahr 1900 bestand er die Aufnahmeprüfung der Großherzoglich-Sächsischen Kunstschule in Weimar. Anekdotische Zeichenkunst offenbart sich in Beckmanns frühen Blättern ebenso wie ein sicheres Formgefühl und die Neigung zum Grotesken.

Auf der modern und liberal ausgerichteten Weimarer Kunstschule trat Beckmann 1901 in die Klasse des norwegischen Porträt- und Genremalers Carl Frithjof Smith ein, den er zeit seines Lebens als seinen einzigen Lehrer betrachtete. Von ihm übernahm er die kräftige Vorzeichnung und behielt sie ein Leben lang bei. Zudem lernte er hier 1902 den Frankfurter Maler Ugi Battenberg sowie die Malerin Minna Tube kennen und begründete mit beiden lebenslange Freundschaften. Ein Selbstbildnis mit aufgerissenem Mund aus dieser Zeit gilt als erste überlieferte Radierung. Das Blatt ist expressiv und verrät den Einfluss von Rembrandt van Rijn und Edvard Munch. Mit Belobigungsdiplomen für Zeichnen und für Malerei in der Tasche verließ Beckmann 1903 die Akademie und ging für ein paar Monate nach Paris. Hier beeindruckten ihn vor allem die Werke von Paul Cézanne. Der junge Künstler las und schrieb viel. In Paris entstanden nach einem kurzen Ausflug in den Pointillismus die Vorstudien für sein erstes chef d’œuvre, das Ölgemälde "Junge Männer am Meer". Er reiste nach Amsterdam, Den Haag und nach Scheveningen, sah vor allem Werke von Rembrandt, Gerard ter Borch, Frans Hals und Jan Vermeer und malte bevorzugt Landschaften. 1904 brach er zu einer Italienreise auf, die jedoch in Genf endete. Er besuchte Ferdinand Hodler im Atelier und sah unterwegs in Colmar den damals noch wenig bekannten Isenheimer Altar. In den Landschaften und Meeresbildern des Sommers setzte sich der Künstler mit der Überwindung des Jugendstils und des europäischen Japonismus auseinander. Einige dieser Arbeiten zeigen eine eigenständige, ausschnitthafte Komposition. Nach Abbruch des Parisaufenthaltes und der Italienreise richtete sich Beckmann in Berlin-Schöneberg (damals Schöneberg bei Berlin) ein Atelier ein.

Im Sommer 1905 arbeitete Beckmann an der dänischen Nordsee an seinem Gemälde "Junge Männer am Meer" (Öl auf Leinwand, 148 × 235 cm). Das Bild ist stilistisch von Luca Signorelli und Hans von Marées beeinflusst mit Anlehnungen an den Neoklassizismus. 

1906 erhielt Beckmann vom drei Jahre zuvor gegründeten Deutschen Künstlerbund für dieses Gemälde den Villa-Romana-Preis. Er beteiligte sich im selben Jahr auch mit zwei Arbeiten an der 11. Ausstellung der Berliner Secession.

Den Tod der Mutter 1906 verarbeitete er in zwei Sterbeszenen in der Tradition von Edvard Munch. Er heiratete seine Studienfreundin Minna Tube und reiste mit ihr nach Paris und anschließend für sechs Monate nach Florenz, als Stipendiat der Villa Romana. Dort malte er das "Bildnis meiner Frau mit rosaviolettem Grund", ein flirrendes Porträt Minna Tubes, das heute in der Hamburger Kunsthalle hängt. 1907 zog Beckmann mit seiner Frau in ein von ihr entworfenes Atelierhaus in Berlin-Hermsdorf. Ebenfalls in diesem Jahr wurde Beckmann in die Berliner Secession als Mitglied aufgenommen.

Die Einladung, in die Dresdner Künstlergruppe Brücke einzutreten, lehnte er ab, trat aber der Berliner Secession bei. Der Wille zum Ruhm des jungen Künstlers drückte sich vor allem in forcierten Katastrophenszenerien aus; Impressionismus und Neoklassizismus vereinten sich hier zu einer brachialen Aktionsmalerei. Den Expressionismus lehnte er ab. Im Kontrast zu seinen großformatigen Gemälden pflegte Beckmann Interieur und Porträt, vor allem Selbstporträt; diese Arbeiten sind zum Teil duftig und atmosphärisch subtil. Schon in jenen Jahren entstanden zudem Handzeichnungen von altmeisterlicher Vollendung. Die Zeichnung sollte stets das Rückgrat von Beckmanns Kunst bleiben.

1908 reiste der Künstler neuerlich nach Paris und wurde im Herbst Vater eines Sohnes, Peter Beckmann, der als Kardiologe und Altersforscher bekannt wurde. Im Jahr darauf stellte er zum ersten Mal im Ausland aus und machte die folgenreiche Bekanntschaft des Kunstschriftstellers Julius Meier-Graefe, der sich bis zu seinem Tod publizistisch für Beckmann einsetzte. Seit 1909 befestigte der Künstler seinen altmeisterlichen Anspruch zunehmend auch in einem grafischen Œuvre. Im selben Jahr setzt er im "Doppelbildnis Max Beckmann und Minna Beckmann-Tube" seiner Beziehung zu der Kollegin und Ehefrau ein Denkmal in der Tradition der repräsentativen Paarporträts à la Gainsborough. Mit veristischen Massenszenarien in kolportagehafter Komposition wie bei der "Szene aus dem Untergang von Messina" stellte er sich in die Rubensnachfolge, wenn auch die Anlage und Ausführung solcher Bilder beim jungen Beckmann einigermaßen unausgegoren blieben.

Max Beckmann wollte sich als neokonservatives Gegenmodell zu der um 1910 aufkommenden radikalen Abstraktion von Malern wie Henri Matisse und Pablo Picasso sowie der Gegenstandslosigkeit eines Wassily Kandinsky profilieren. Ebenso wie Max Liebermann oder Lovis Corinth war er auf der Suche nach einer modernen Form der figurativen Malerei.

1910 wurde Beckmann in den Vorstand der Berliner Secession gewählt, war mit seinen 26 Jahren dort das jüngste Mitglied, trat jedoch bald wieder aus. Zwei Jahre zuvor war er daran gescheitert, eine vom Händler Paul Cassirer unabhängige Ausstellungsorganisation zu gründen. Er distanzierte sich fortan von Künstlervereinigungen, beteiligte sich jedoch weiterhin an den großen DKB-Jahresausstellungen in Mannheim (1913), Dresden (1927; dort war er Mitglied der Aufnahme-Jury), Köln (1929), Stuttgart (1930), Essen (1931), Königsberg/Danzig (1932) und Hamburg (1936).

Der Kunsthändler Israel Ber Neumann und der Verleger Reinhard Piper trugen zu Beckmanns Vorkriegsruhm bei, der um 1913 seinen Höhepunkt erreichte. Nun trat der 29-jährige Maler ganz aus der Secession aus und begründete 1914 die Freie Secession mit. Er hielt sich weiterhin vom Expressionismus fern, zeigte sich aber ebenso wie dieser in Grafik und Malerei von der Großstadt fasziniert. Sein Programm war nun festgelegt: Max Beckmann würde nie gegenstandslos arbeiten. Vielmehr setzte er sich zum Ziel, das Erbe der klassischen Kunst (Raum, Farbe, hergebrachte Gattungen, Mythologie, Symbolik) auszuweiten. Im März 1912 stipulierte er: „Die Gesetze der Kunst sind ewig und unveränderlich.“

„Meine Kunst kriegt hier zu fressen“, bemerkte Beckmann im Ersten Weltkrieg, den er für ein „nationales Unglück“ hielt. Der Künstler gab im Krieg keinen einzigen Schuss ab. „Auf die Franzosen schieße ich nicht, von denen habe ich so viel gelernt. Auf die Russen auch nicht, Dostojewskij ist mein Freund.“ 1914 diente er als freiwilliger Sanitätshelfer an der Ostfront, im Jahr darauf in Flandern. Seine Zeichnungen aus dieser Zeit spiegeln die ganze Härte des Krieges wider. Sie begründen Beckmanns neuen, hartkonturierten Stil. Der künstlerische Umschwung wurde flankiert durch die Kriegsprosa der "Briefe im Kriege", die noch während des Krieges erschienen. 1915 erlitt der Künstler einen Nervenzusammenbruch, diente vorübergehend am Kaiserlichen Hygieneinstitut in Straßburg und ließ sich kurz darauf in Frankfurt-Sachsenhausen nieder. Hier lebte er im heutigen Max-Beckmann-Haus auf der Schweizer Straße 3 in direkter Nähe zum Städel Museum, seinem späteren Arbeitsplatz. Es zeigte sich nun, dass sein persönlicher Zusammenbruch zugleich ein Neuanfang sein sollte. Der schonungslose Zeichenstil des Krieges wird in Grafik (besonders Kaltnadelradierung) und Malerei überführt. Im "Selbstbildnis als Krankenpfleger" betreibt der Künstler nun ebenso eine schonungslose, um äußerste Wahrhaftigkeit ringende Spiegelung seiner selbst, wie er in den Grafikmappen wie dem Lithografienzyklus "Die Hölle" die Kriegs- und Nachkriegswirklichkeit hartkantig und virtuos zusammenschachtelt und ihre Substanz offenlegt. Die christliche Ikonografie erhält nun die Aufgabe der Darstellung der conditio humana; ein Gemälde wie "Christus und die Sünderin" von 1917 zeigt den gefallenen Menschen und den Jesus der praktischen Ethik.

In der Weimarer Republik wuchsen Beckmanns politische Interessen, zugleich studierte er Geheimlehren wie die Theosophie, die seit der Jahrhundertwende viele Künstler beschäftigte. Er fasste die Physiognomien seiner Zeit scharf ins Auge, suchte hier aber keinen Realismus, sondern was er "transzendente Sachlichkeit" nannte. Das berühmte Bild der Frankfurter Synagoge und des Eisernen Steges entstanden in dieser Zeit. Beckmann war eng in das intellektuelle Leben seiner Zeit eingebunden durch seine Freundschaften mit dem Schriftsteller Benno Reifenberg, mit Heinrich Simon, dem Chefredakteur der Frankfurter Zeitung, durch seine Verbindungen zu dem Kunsthändler Günther Franke, dem Schauspieler Heinrich George und Künstlerkollegen wie Alfred Kubin. Er schrieb Dramen und Gedichte, die sich nach seinem Tod als aufführbar und lesenswert erwiesen. Neben der umfangreichen grafischen Arbeit entstanden wieder Selbstporträts, die den Dargestellten zum Chronisten nicht allein seiner selbst, sondern seiner Epoche machten.

Ab 1922 wurde Beckmann von Lilly von Mallinckrodt-Schnitzler gefördert, die seine Bilder sammelte und ihn gesellschaftlich bekannter machte. 1924 lernte Beckmann in Wien die junge Mathilde Kaulbach, Tochter von Friedrich August von Kaulbach, kennen. Er trennte sich von Minna Tube und machte fortan seine neue Frau unter ihrem Wiener Spitznamen "Quappi" zu einer der meistgemalten und -gezeichneten Frauen der Kunstgeschichte. Reisen nach Italien, Nizza und Paris, vertiefte Studien gnostischer, altindischer und theosophischer Lehren lockerten und erweiterten seinen künstlerischen Stil. Zugleich nahm die Farbigkeit seiner Gemälde zu. Seit 1925 leitete er ein Meisteratelier an der Kunstschule des Städel-Museums in Frankfurt. Zu seinen Schülern zählten Theo Garve, Léo Maillet und Marie-Louise von Motesiczky. Gemälde wie "Doppelbildnis Karneval" oder "Italienische Phantasie" reflektieren die Beruhigung der politischen Verhältnisse ebenso wie die bösen Vorahnungen auf ein baldiges Ende der Goldenen Zeit. In dem spektakulären Bild Galleria Umberto ahnt der Künstler bereits 1925 den Tod Mussolinis. Beckmanns Biograf Stephan Reimertz spricht vom Vorgesicht des Künstlers. Auf dem Höhepunkt der Weimarer Republik gab Beckmann sich indes noch einmal staatstragend als Stresemann-Deutscher. 1927 malte er das Selbstbildnis im Smoking und schrieb einen Aufsatz mit dem Titel "Der Künstler im Staat". Beckmanns ausgeprägtes Selbstbewusstsein war allgemein bekannt.

1928 erreichte sein Ruhm in Deutschland den Höhepunkt mit dem "Reichsehrenpreis Deutscher Kunst" und einer umfassenden Beckmann-Retrospektive in Mannheim. Seine Kunst zeigt nun grandiose Formvollendung; sie verrät zudem den mondänen Erotiker, der Beckmann immer gern sein wollte. Diese Rolle ist eine der vielen Masken, hinter der der ängstliche und sensible Künstler sich versteckte. Auf der DKB-Jubiläumsausstellung ("25 Jahre Deutscher Künstlerbund") 1929 im Kölner Staatenhaus am Rheinpark waren von Max Beckmann fünf Ölgemälde zu sehen. 1930 zeigte die Biennale in Venedig sechs Bilder von Beckmann, der auch in diesem Jahr auf der Jahresausstellung der Prager Secession vertreten war. Zugleich wurde der Künstler von der nationalsozialistischen Presse heftig angegriffen. In Paris fand er kurzfristig eine gewisse Beachtung unter Intellektuellen, die sich sowohl vom Surrealismus, als auch von der Dominanz von Henri Matisse und Pablo Picasso zu lösen suchten. 1932 richtete die Berliner Nationalgalerie einen Beckmann-Saal ein, die sog. Neue Abteilung der Nationalgalerie Berlin im Kronprinzenpalais. Der Künstler begann in diesem Jahr mit dem ersten von zehn Triptychen. Unter dem Namen "Abfahrt" begonnen, stellte er es Jahre später als "Departure" fertig.

Im April 1933 wurde Beckmann fristlos aus seiner Professur an der Frankfurter Städelschule entlassen. Seine Schüler, aber auch andere junge Künstler, die von Beckmann beeinflusst wurden, wie z. B. der Maler Joseph Mader, hatten keine Möglichkeiten mehr, sich künstlerisch zu betätigen; später sprach man von einer verschollenen Generation. Einige ihrer Werke wurden von den Nazis auf dem Römerberg verbrannt. Der Beckmann-Saal im Kronprinzenpalais wurde anders genutzt. Max Beckmann war für die Nazis einer der meistgehassten Künstler. Er war in den Ausstellungen zur Entarteten Kunst, die durch ganz Deutschland tourten, prominent vertreten.

Beckmann verließ Frankfurt und lebte bis zu seiner Emigration in Berlin. Er lernte den Schriftsteller Stephan Lackner kennen, der ihm ein treuer Freund, Sammler und Interpret blieb. In dieser Zeit malte Beckmann auch viele anekdotenhafte Bilder wie "Ochsenstall" und "Der kleine Fisch", Selbstbildnisse wie jenes mit schwarzer Kappe oder mit der Glaskugel, die die Unsicherheit seiner Situation reflektierten und zu überspielen versuchten. Auch begann er nun eine plastische Arbeit und schuf die Bronze "Mann im Dunkeln", in der sich seine Position als unerwünschter Künstler manifestiert. Bis zur Schließung der letzten DKB-Jahresausstellung 1936 im Hamburger Kunstverein – sein Ausstellungsbeitrag "Landungskai im Sturm" (1936, Öl auf Leinwand, 41 × 80,5 cm) befindet sich heute im Besitz des Frankfurter Städel Museums – war Beckmann Mitglied im Deutschen Künstlerbund, in den er bereits 1906 eingetreten war. Weitere Werke Beckmanns wurden in der 1937 in den Münchener Hofarkaden veranstalteten Ausstellung „Entartete Kunst“ gezeigt.

Nach der Rundfunkübertragung von Hitlers Rede zur Eröffnung der gleichzeitigen "Großen Deutschen Kunstausstellung" in München hat Max Beckmann Deutschland für immer verlassen. Im selbstgewählten Exil Amsterdam malte er Selbstporträts wie "Der Befreite", auf dem er Ketten sprengt. Tief verrätselte Bilder und weitere Triptychen mit teils mythologischen Themen prägen sein Exilwerk.

1938 schrieb er in sein Tagebuch:

Er hielt Kontakte zu deutschen Widerstandskreisen, auch um Gisèle van Waterschoot van der Gracht und Wolfgang Frommel in Amsterdam. Beckmann musste sich Musterungen der deutschen Wehrmacht unterziehen und bewarb sich seit 1939 um ein Visum für die Vereinigten Staaten. Seine Bemühungen um eine Ausreise scheiterten jedoch, so dass er während der gesamten Kriegszeit in Amsterdam bleiben musste.

Erst im Sommer 1947 erhielten Max und Mathilde Beckmann Visa für die USA. Ab Ende September lehrte der Künstler an der Art School der Washington University in St. Louis. Zu seinen amerikanischen Schülern gehörten Walter Barker und Jack Bice. Im Mai 1948 zeigte das Saint Louis Art Museum eine große Beckmann-Retrospektive. Der Sammler Morton D. May begann mit dem Aufbau seiner Beckmann-Sammlung, heute die umfangreichste der Welt.

Neben Reisen quer durch die USA und Lehrtätigkeiten in Boulder (Colorado) und Carmel (Kalifornien) nahm Max Beckmann Ende 1949 eine Professur für Malen und Zeichnen an der Art School des Brooklyn Museums in New York an. Es fiel ihm zunehmend schwer, seine Kunst gegen die inzwischen populäre gegenstandslose Malerei durchzusetzen. Am 27. Dezember 1950 starb Max Beckmann an einem Herzinfarkt mitten auf der Straße in Manhattan (Central Park West, 61st St.). Sein zehntes Triptychon "Amazonen" blieb unvollendet.

Max Beckmanns Werke erzielen sehr hohe Preise. Im Jahr 2001 wurde sein "Selbstbildnis mit Horn" in New York für 22,6 Millionen Dollar versteigert. Sein Gemälde "Blick auf Vorstädte am Meer bei Marseille" aus dem Jahr 1937 wurde im November 2009 für 2,6 Millionen Euro versteigert; es war damit das teuerste deutsche Gemälde des wirtschaftlich schwierigen Auktionsjahres 2009. 2017 wurde sein Gemälde "Hölle der Vögel" für 40,8 Millionen Euro versteigert. Nie zuvor wurde für ein Kunstwerk des deutschen Expressionismus so viel bezahlt.















Nach Beckmann ist eine Grundschule in Nürnberg benannt.



</doc>
<doc id="9673" url="https://de.wikipedia.org/wiki?curid=9673" title="69">
69

Das Jahr 69 nach unserer Zeitrechnung ("DCCCXXII" nach dem römischen Kalender "ab urbe condita") geht als das erste Vierkaiserjahr in die Geschichte des Römischen Reichs ein. In kurzen Abständen folgen einander Galba, Otho und schließlich Vitellius auf den Kaiserthron. Erst relativ spät im Jahr greift ein weiterer Thronaspirant in den Bürgerkrieg ein: Vespasian, der von den Legionen der östlichen Provinzen Judäa und Ägypten auf den Schild gehoben wird, besiegt Vitellius und seine Rheinlegion in der Zweiten Schlacht von Bedriacum entscheidend und besteigt somit als erster Kaiser aus der Dynastie der Flavier den Thron.

Der neue Kaiser sieht sich sofort einer schweren Aufgabe gegenüber. Denn in der Provinz "Germania inferior" haben sich – angestachelt durch Vorhersagen der Seherin Veleda – die Bataver unter Julius Civilis erhoben, um die römische Fremdherrschaft abzuschütteln.










</doc>
<doc id="9674" url="https://de.wikipedia.org/wiki?curid=9674" title="Eiskorn">
Eiskorn

Eiskörner sind eine Form von Niederschlag, bei der Regentropfen oder geschmolzene Schneeflocken durch eine kalte Luftschicht fallen und dabei (wieder) gefrieren. 

Im Gegensatz zu Graupel sind Eiskörner weitgehend durchsichtig (dem entspricht die Unterscheidung von Klareis und Rauheis), im Gegensatz zu Hagelkörnern haben sie keinen Kern und sind nicht aus Eisschichten aufgebaut. Ihre Größe beträgt nicht mehr als 6 Millimeter.

Wenn Eiskörner vom Himmel fallen, spricht man speziell von "Eisregen"; in anderem Zusammenhang spricht man von Eisregen aber auch, wenn Regen am Boden ausfriert, entweder, weil er auf sehr kalten Untergrund trifft (Gefrierender Regen) oder die Tropfen selbst unterkühlt sind (Unterkühlter Regen).
Wenn Eiskörner auf dem Boden auftreffen, bilden sie oft ebenfalls einen festen Eisüberzug. Anders als beim gefrierenden Regen ist die Eisfläche jedoch nicht völlig glatt, sondern leicht geriffelt, weil jedes Eiskorn einen kleinen Hügel bildet.



</doc>
<doc id="9677" url="https://de.wikipedia.org/wiki?curid=9677" title="Hagel">
Hagel

Hagel ist eine Form von Niederschlag, der aus Eisklumpen besteht und überwiegend in warmen Jahreszeiten und den Mittleren Breiten auftritt. Zur Abgrenzung spricht man erst bei einem Durchmesser von über 0,5 Zentimetern von Hagel bzw. Eishagel, darunter von Graupel. Bei Aggregaten von Schneeflocken mit einem Durchmesser unter einem Millimeter spricht man von Griesel.

Das Wort "Hagel" (altdeutsch "hagal") ist vermutlich verwandt mit dem griechischen Wort κάχληξ "káchlēx" „Kieselstein“, das auf der indogermanischen Wurzel *"kaghlo-s" „kleiner Stein“ beruht. 

Die ebenfalls gebräuchliche Bezeichnung "Schloße" für ein Hagelkorn ist namensgebend für den Farbnamen "schlohweiß" (dissimiliert aus "schloßweiß"), was also „weiß wie ein Hagelkorn“ bedeutet und einen hellen, fahlen Weißton bezeichnet.

Hagelkörner entstehen in den niedrigeren Schichten von Gewitterwolken bzw. innerhalb einer Gewitterzelle durch unterkühltes Wasser, das an Kristallisationskernen zu Eis gefriert. Diese Kerne müssen dabei in vergleichsweise geringer Zahl vorkommen, so dass die je Kern zur Verfügung stehende Wassermenge ausreichend groß ist, um ein schnelles Wachstum zu ermöglichen. Da es sich also um sehr wasserreiche Wolken handelt, haben die über Phasenumwandlungen umgesetzten latenten Wärmemengen eine starke Labilität der Temperaturschichtung innerhalb der Wolke zur Folge. Die hierdurch erzeugten starken Aufwinde von durchaus 20 bis 30 m/s sind ein weiterer wichtiger Faktor für die Hagelbildung, denn die Gefrierungsprozesse haben eine stetige Massenzunahme der Partikel zur Folge. Ohne einen Aufwind würden die Partikel durch die Schwerkraft absinken, sich aus der Wolke entfernen und dadurch nicht weiter anwachsen können.

Es zeigt sich dabei, dass der Aufwind innerhalb einer Wolke unterschiedlich stark ist und Partikel dadurch einen Kreislauf durchfahren können. Zunächst werden sie durch den Aufwind angehoben, danach fallen sie wieder in tiefere Luftschichten, nehmen weiteres Wasser auf, werden abermals nach oben gerissen, und zusätzliches Wasser gefriert an. Dieser Vorgang wiederholt sich solange, bis ein Hagelkorn zu schwer ist, um von den Aufwinden getragen zu werden. Aus der Größe der Hagelkörner kann daher auf die Windstärke im Inneren der Gewitterwolke geschlossen werden, was in der Umkehrung auch zur Prognose von Hageldurchmessern dient.

Die stufenweise Entstehung der Hagelkörner kann an den einzelnen Anlagerungsschichten, aus denen ein Hagelkorn besteht, abgelesen werden. Dabei deuten die klaren Schichten auf eine sehr wasserreiche Umgebung mit dementsprechend schnellem Gefrieren hin, während die trüben Bereiche auf niedrigere Wassergehalte zurückgehen. Die Trübung selbst wird dabei durch unzählige kleine Lufteinschlüsse hervorgerufen. Ist das Hagelkorn letztendlich zu schwer und sinkt aus dem Aufwindbereich ab, so kommt es aufgrund der Größe des Hagelkorns und einer Temperatur von meist unter 0 °C nicht zu einem Aufschmelzen.

Weltweit sind am stärksten die Mittleren Breiten (Gemäßigte Zone) von Hagelereignissen betroffen, insbesondere Zentral- und Südeuropa, die USA, Mexiko, Ostchina, Argentinien, Südafrika und Südost-Australien. Jedoch sind vereinzelt bei entsprechenden Bodengegebenheiten auch äquatornahe Gebiete betroffen, darunter Kenia (insbesondere die Region Kericho) oder Äthiopien. Auch an Gebirgsrändern tritt häufiger Hagel auf, zum Beispiel an den Anden in Peru, Ecuador und Kolumbien oder am Himalaya im Norden Indiens oder in Nepal.

Die Größe von Hagelkörnern variiert stark: Die Hagelschaden-Skala der Schweizerischen "Vereinigung Kantonaler Feuerversicherungen" unterscheidet elf Intensitätsklassen, die von kleinem Hagel unter 0,5 cm bis zu außergewöhnlich großem Hagel von über 10 cm Durchmesser reicht. Das Gewicht der Körner variiert entsprechend zwischen 0,1 g und mehr als 0,5 kg. Zu Schäden an Autos, Glasscheiben und Zelten kommt es ab einem Durchmesser von etwa 2 cm; Hagelkörner dieser Größe erreichen Fallgeschwindigkeiten von etwa 70 km/h; kleinerer Hagel fällt langsamer (ca. 35 km/h), außergewöhnlich großer kann dagegen Geschwindigkeiten von über 150 km/h erreichen.
Laut Nachrichtensender CNN wurde das größte Hagelkorn in der US-Geschichte mit 20 cm Durchmesser und einem Gewicht von 875 g am 9. August 2010 im amerikanischen Bundesstaat South Dakota gefunden.

Hagelschauer können beträchtliche Schäden verursachen, an Pflanzen und Tieren, aber auch an Gebäuden und Autos. Der Großteil der landwirtschaftlichen Anbauflächen ist ohne Schutz dem Hagel ausgesetzt. Zudem verursachen Hagelschläge hohe Schäden an Bäumen und senken damit den Waldbestand. Aufgrund der geringen Ausdehnung eines Hagelschlags sind meistens nur begrenzte Schäden zu verzeichnen. Der Hagelschlag mit der höchsten Schadensumme fand 1984 in München statt. Der Schaden belief sich auf über 1,5 Milliarden €. 2013 führte Hagelschlag, insbesondere die Ereignisse in Baden-Württemberg und Niedersachsen am 27./28. Juli, zu Schäden von insgesamt 2,7 Milliarden €. Auch was Hagelschäden an Autos angeht, war 2013 ein bemerkenswertes Jahr: 635.000 Autos in Deutschland wurden durch Hagelschauer beschädigt – so viele wie noch nie. Dadurch entstand ein versicherter Schaden von 1,5 Milliarden Euro. Hagelschäden an der Karosserie von Fahrzeugen können ab etwa 2 cm Durchmesser entstehen.
Das unter Umständen existenzielle finanzielle Risiko von Ernteausfällen durch Hagel kann nur durch Spezialversicherungen (Hagelversicherung) abgesichert werden. Die Hagelversicherung ist eine Form der Schadenversicherung, bei der die versicherten Bodenerzeugnisse, insbesondere alle wirtschaftlich nutzbaren Pflanzen, gegen Schäden, die durch Einwirkung des Hagelschlags entstehen, versichert sind. Auch Gewächshäuser können eingeschlossen werden. Die Wohngebäudeversicherung übernimmt die Schäden durch Hagel an Häusern. An Fahrzeugen greift bei Hagelschäden die Teilkaskoversicherung. 

Wegen der teilweise beträchtlichen Hagelschäden wurde schon recht früh versucht, Mittel und Wege zu entwickeln, um Hagel zu vermeiden.

Der Klimawandel scheint einen Einfluss auf die Häufigkeit des Auftretens von Hagelunwettern zu haben. Wie Forscher der Universität Karlsruhe herausfanden, hat die Zahl der Hagelgewitter in den letzten Jahrzehnten deutlich zugenommen. Habe die Zahl der Tage mit Hagelschäden 1986 noch bei fünf gelegen, so sei sie 2004 auf 34 gewachsen.

Zur Vermeidung von Hagelschäden wurden seit der Antike verschiedenste Methoden eingesetzt, von Bittopfern, rituellen Prozessionen, dem Wetterläuten und Schutzzeichen bis zu technischen Methoden wie dem Beschuss der Gewitterwolken mit Böllern, Kanonen und Raketen. Ab der Mitte des 20. Jahrhunderts wurde das Einbringen von Silberiodid (AgI) in die Wolken als Methode entdeckt. Silberiodid ist eiskeimbildend und soll durch zusätzliche Kristallisationkeime die Bildung größerer Hagelkörner verhindern. Die Ausbringung des AgI erfolgt vorwiegend durch Raketen oder speziell ausgerüstete Kleinflugzeuge, sog. Hagelflieger. Die Effektivität der Hagelbekämpfung mit Silberiodid ist wissenschaftlich bislang nicht belegt.

Ein weiterer Versuch wird mit Hagelkanonen unternommen. Hierbei werden im Abstand einiger Sekunden durch Gasexplosionen Schallwellen erzeugt und durch einen Trichter ausgestoßen. Damit soll die Hagelbildung gestört und der Hagel in Nassschnee umgewandelt werden. Die Methode wird von Ingenieuren und Meteorologen für wirkungslos gehalten.

Zudem werden in der Landwirtschaft Hagelschutznetze aus Polyethylen (PE) verwendet, die über komplette Pflanzen gespannt werden und die Hagelkörner im Traufebereich herabfallen lassen.



</doc>
<doc id="9679" url="https://de.wikipedia.org/wiki?curid=9679" title="DAX">
DAX

Der DAX (Abkürzung für Deutscher Aktienindex) ist der bedeutendste deutsche Aktienindex. Er misst die Wertentwicklung der 30 größten und (bezogen auf die Streubesitz-Marktkapitalisierung) liquidesten Unternehmen des deutschen Aktienmarktes und repräsentiert rund 80 Prozent der Marktkapitalisierung börsennotierter Aktiengesellschaften in Deutschland. Der Index ist ein Produkt und eingetragene Wortmarke und Bildmarke der Deutschen Börse AG, deren Aktien selbst im DAX enthalten sind. Er ist das wichtigste Mitglied der DAX-Indexfamilie.

Die im DAX enthaltenen Aktiengesellschaften sind im Prime Standard an der Frankfurter Wertpapierbörse gelistet. Der Index basiert auf Handelsdaten im elektronischen Handelssystem Xetra und wird börsentäglich von ca. 9 Uhr bis 17:30 Uhr berechnet.

Entwickelt wurde der DAX gemeinsam von der Arbeitsgemeinschaft der Deutschen Wertpapierbörsen, der Frankfurter Wertpapierbörse und der Börsen-Zeitung. Er wurde am 1. Juli 1988 eingeführt und setzt den Index der Börsen-Zeitung fort, dessen Geschichte bis ins Jahr 1959 zurückgeht. Für den 31. Dezember 1987 wurde er auf 1.000 Indexpunkte normiert. Zunächst war der DAX nicht als Konkurrenz, sondern als Ergänzung zu bereits etablierten deutschen Aktienindizes gedacht. Inzwischen hat er die anderen Indizes an Bekanntheit hinter sich gelassen und ist als Leitindex für den deutschen Aktienmarkt national und international anerkannt.

Der DAX wird sowohl als Performance- als auch als Kursindex veröffentlicht. Beim Performanceindex werden die Dividenden der im DAX enthaltenen Unternehmen rechnerisch reinvestiert; beim Kursindex bleiben die Dividenden unberücksichtigt. Umgangssprachlich wird unter der Bezeichnung DAX der Performanceindex verstanden. Bei anderen Indizes (z. B. beim Euro Stoxx 50) ist es oft umgekehrt.

Die gesamte Marktkapitalisierung des DAX betrug 1.033,3 Milliarden Euro (Stand: September 2017). Laut der World Federation of Exchanges liegt der DAX auf Platz 10 der weltweit größten Börsen nach Marktkapitalisierung (Stand: August 2017).

Im Frühjahr 1994 wurde der sogenannte DAX 100 eingeführt. Dieser Index sollte die Wertentwicklung der 100 liquidesten Werte des Aktienmarkts dokumentieren. Mit Einführung des MDAX 1996 umfasste er die 30 Werte des DAX und die 70 des MDAX. Durch die Änderung des MDAX auf 50 Werte trat an die Stelle des DAX 100 der HDAX. Der CDAX umfasst dagegen alle in Frankfurt gehandelten Aktien. Der SDAX umfasst 50 kleinere Werte hinter dem MDAX.

Als sogenannten Strategie-Index gibt es u. a. den ShortDAX. Er ist umgekehrt proportional zur täglichen Entwicklung des DAX und ermöglicht so Anlegern, die nur Long-Positionen eingehen können, die Partizipation an negativen Kursentwicklungen. Verliert beispielsweise der DAX an einem Tag 5 %, dann legt der ShortDAX um eben soviel Prozent zu. Dadurch haben z. B. Indexanbieter die Möglichkeit, ein Produkt zu offerieren, das bei negativer Entwicklung des DAX steigt und dem „Käufer“ Möglichkeiten eröffnet, auch bei negativen Kursentwicklungen Gewinne zu erzielen.

Daneben gibt es auch Branchenindizes wie seit 2007 den ÖkoDAX für Unternehmen aus dem Bereich der erneuerbaren Energien oder Strategieindizes wie den DivDAX für dividendenstarke Unternehmen. Zu den wichtigsten deutschen Indizes gehört auch der TecDAX, der 30 der größten Technologiewerte beinhaltet.

Die implizite Volatilität, also die erwartete Schwankungsbreite des DAX für den Zeitraum der nächsten 30 Tage misst der VDAX-NEW, ein ebenfalls von der Deutschen Börse AG veröffentlichter Volatilitätsindex. Sein Vorgänger, der VDAX, gibt die implizite Volatilität des DAX über 45 Tage auf Grundlage eines anderen methodischen Ansatzes an. Beide Volaindizes werden in annualisierter Form notiert.

Die Eurex bietet Optionen (ODAX) und Futures (FDAX) auf den DAX an.

Der Index basiert seit dem 21. Juni 1999 auf den Kursen des elektronischen Handelssystems Xetra. Seine Berechnung beginnt seit Januar 2016 börsentäglich ab 9:06 Uhr MEZ/MESZ. Wenn zu diesem Zeitpunkt für einzelne Aktien noch keine Eröffnungskurse vorliegen, werden die jeweiligen Schlusskurse des vorangegangenen Handelstags herangezogen. Die Berechnung endet mit den Kursen der Xetra-Schlussauktion, die um 17:30 Uhr beginnt und 5 bis 15 Minuten dauern kann. Bis zum 31. Dezember 2005 wurde der DAX so im 15-Sekunden-Takt aktualisiert, seit 1. Januar 2006 jede Sekunde.

Zur Berechnung, die auf der Indexformel von Étienne Laspeyres basiert, werden die Aktienkurse der ausgewählten börsennotierten Gesellschaften nach ihrer Marktkapitalisierung gewichtet. Dabei werden nur die Aktien in Streubesitz berücksichtigt. Ist das Kapital einer Gesellschaft in mehrere Aktienarten unterteilt (z. B. in Stammaktien und Vorzugsaktien), wird die Aktienart mit der höheren Liquidität herangezogen. Dabei werden auch Sonderfälle berücksichtigt, z. B. die 2011/2012 zum Umtausch gehandelten Aktien der Deutschen Börse AG.

Für Zeiten außerhalb des Xetra-Handels (vor Handelsbeginn und nach der Schlussauktion) berechnet die Deutsche Börse AG von 8:00 Uhr bis 9:00 Uhr und von 17:30 Uhr bis 20:00 Uhr den LDAX (auch L-DAX, L/E-DAX) auf Basis der Aktienkurse an der Börse Frankfurt, die entsprechend längere Handelszeiten hat. Außerdem wird von 8:00 Uhr bis 9:00 Uhr und von 17:30 Uhr bis 22:15 Uhr der XDAX (auch X-DAX) auf Basis an der Terminbörse Eurex gehandelter DAX-Futures berechnet.

Damit ein Unternehmen in den DAX aufgenommen wird, muss es im Prime Standard gelistet sein, fortlaufend in Xetra gehandelt werden und mindestens einen Streubesitz von 10 % aufweisen (bis Oktober 2008 lag diese Grenze bei 5 %). Außerdem muss das Unternehmen einen Sitz in Deutschland haben oder den Schwerpunkt seines Handelsumsatzes an Aktien in Frankfurt und einen Sitz in der EU haben. Als Sitz des Unternehmens kann sowohl der juristische Sitz als auch das operative Hauptquartier, also der Sitz der Geschäfts- beziehungsweise Verwaltungsführung gelten.

Unter den Unternehmen, die diese Grundvoraussetzungen erfüllen, erfolgt die weitere Auswahl anhand folgender zwei Merkmale:

Eine Anpassung des DAX findet nach folgenden vier Regeln statt:

Nur jeweils zum ordentlichen Anpassungstermin im September werden alle vier Regeln angewendet. Die Fast-Exit- und Fast-Entry-Regel finden dagegen auch an den außerordentlichen Anpassungsterminen im März, Juni und Dezember Anwendung. Außerordentliche Aktualisierungen werden außerdem im Fall von Insolvenzen eines Unternehmens vorgenommen oder falls ein Unternehmen die anfangs genannten Grundvoraussetzungen nicht mehr erfüllt.

Zudem kann in Ausnahmefällen, wie zum Beispiel kurzfristig angekündigte Übernahmen oder signifikante Veränderung des Freefloat, der Vorstand der Deutsche Börse AG in Abstimmung mit dem Arbeitskreis Aktienindizes von diesen Regeln abweichen.

Die Werte im DAX werden entsprechend ihrer Streubesitz-Marktkapitalisierung gewichtet. Maßgeblich ist also nicht der gesamte Börsenwert oder das gesamte Kapital eines Unternehmens, sondern nur der Wert der frei handelbaren Aktien (engl. ) der im DAX vertretenen Aktiengattung. Festbesitz, das ist der Besitz von Großaktionären, die 5 % oder mehr dieser Aktien halten, bleibt für die Gewichtung unberücksichtigt. Zum Festbesitz zählen auch eigene Aktien, die das herausgebende Unternehmen selbst hält, unabhängig von der Höhe des Anteils. Seit 2006 gibt es eine Kappungsgrenze: Einzelne Titel dürfen ein Maximalgewicht von 10 % haben.

Aufgrund von Änderungen beim Aktienbesitz, aber vor allem aufgrund von Kursveränderungen ändert sich die tatsächliche Streubesitz-Marktkapitalisierung fortwährend. Die Gewichtung wird dabei bei jeder sekündlichen Neuberechnung des DAX auf Basis der aktuellen Kurse ebenfalls neu berechnet. Änderungen des Streubesitz-Anteils werden dagegen von der Deutschen Börse nur zu den vierteljährlichen Anpassungsterminen erfasst und fließen erst dann in die Gewichtung ein. Dies führte zu Situationen, in denen sich ein Aktienkurs in Echtzeit einem veränderten Freefloat anpasst, dem aber in der Gewichtung erst zeitverzögert entgegenwirkend Rechnung getragen wird. Auf die Kapriolen der Volkswagen Stammaktie hin führte die Deutsche Börse im November 2008 zusätzliche Regeln ein, um derartigen extremen Verzerrungen entgegenzuwirken. Ein Wert kann demnach zwischen Anpassungsterminen aus dem Index genommen werden, falls seine Gewichtung 10 % überschreitet und die historische 30-Tages-Volatilität des Aktienkurses 250 % übersteigt.

Die Tabelle zeigt alle Unternehmen im DAX (Stand: 19. März 2018)

1) Daimler, E.on und ThyssenKrupp waren durch ihre Vorgängerunternehmen seit dem 1. Juli 1988 im DAX vertreten.

Seit der Einführung des DAX am 1. Juli 1988 wurden zahlreiche Anpassungen an seiner Zusammensetzung vorgenommen. Ursprünglich waren folgende Werte Bestandteil des Index. Die mit * gekennzeichneten Unternehmen sind bis heute ununterbrochen im DAX vertreten; die mit (*) gekennzeichneten Aktiengesellschaften sind durch Nachfolgeunternehmen bis heute ununterbrochen im DAX vertreten:
Ausgehend von dieser anfänglichen Zusammensetzung wurden folgende Veränderungen vorgenommen:

Der DAX wird von der Deutschen Börse seit dem 1. Juli 1988 berechnet und startete bei 1163,52 Punkten. Die Indexbasis liegt bei 1000 Punkten per 31. Dezember 1987. Da Investoren nicht nur an der aktuellen Performance des Deutschen Aktienindexes Interesse haben, sondern auch an der historischen, wurde dieser 1988 von Frank Mella, damals Redakteur bei der Börsen-Zeitung und Erfinder des DAX, auf täglicher Basis exemplarisch bis 1959 zurückgerechnet.

Mella verkettete den DAX am 30. Dezember 1987 mit den 30 Aktienwerten im Index der Börsen-Zeitung (BZ-Index), der wiederum aus einer Verknüpfung mit den 24 Werten im Hardy-Index des Bankhauses Hardy & Co. zum 1. April 1981 hervorgegangen ist und bis zum 28. September 1959 zurückreicht. Anders als im DAX und im BZ-Index berücksichtigte der Hardy-Index keine Dividenden. Hardy- und BZ-Index wurden viermal am Tag (um 12:00 Uhr, 12:30 Uhr, 13:00 Uhr und 13:30 Uhr) ermittelt. Die von Mella erzeugte Zeitreihe wird von der Deutschen Börse als offizielle Rückrechnung des DAX verwendet.

Längerfristige Rückrechnungen des deutschen Aktienindex stützen sich vor allem auf den Index des Statistischen Bundesamtes, von dem bis 1948 zurückreichende monatliche Kurse zur Verfügung stehen, den Aktienindex des Statistischen Reichsamtes, der von 1922 bis 1943 ermittelt wurde und den Index des Institutes für Konjunkturforschung, deren Zeitreihen bis in das Jahr 1840 zurückgehen.

Am 20. Juni 1985 schloss der DAX rechnerisch mit 1007,18 Punkten erstmals über der Marke von 1000 Punkten. Am 19. Oktober 1987 verbuchte der Index einen Tagesverlust von 9,39 Prozent. Grund war der Schwarze Montag an der New York Stock Exchange, als der Wert des Dow Jones Industrial Average um 22,6 Prozent einbrach. Am Montag, den 16. Oktober 1989, erlitt der DAX den größten Tagesabsturz seiner Geschichte. Er brach um 12,81 Prozent ein. Am Freitag zuvor war in den USA die Finanzierung der Fluggesellschaft UAL gescheitert. Am 19. August 1991 verbuchte der DAX einen Tagesverlust von 9,40 Prozent. Es war eine Reaktion auf den später gescheiterten Putsch gegen den sowjetischen Präsidenten Michail Gorbatschow.

Am 20. März 1998 schloss der Index mit 5001,55 Punkten zum ersten Mal über der Marke von 5000 Punkten. Am 7. März 2000 erreichte der DAX mit 8136,16 Punkten im Handelsverlauf und mit 8064,97 Punkten auf Schlusskursbasis seine bis dahin höchsten Werte. Diese wurden erst mehr als sieben Jahre später, im Sommer 2007, übertroffen. Nach dem Platzen der Spekulationsblase im Technologiesektor (Dotcom-Blase) sank der DAX bis zum 12. März 2003 auf einen Schlussstand von 2202,96 Punkten. Tiefer schloss der Index letztmals am 24. November 1995. Das war ein Rückgang um 72,7 Prozent gegenüber seinem Höchststand vom 7. März 2000.

Der 12. März 2003 markiert den Wendepunkt der Talfahrt. Ab dem Frühjahr 2003 war der DAX wieder auf dem Weg nach oben. Im Juli 2007 wurden neue Höchststände erreicht; bester Wert waren 8151,57 Punkte am 13. Juli 2007.

Im Dezember 2007 waren erstmals die 30 größten deutschen börsennotierten Konzerne mehrheitlich im Besitz ausländischer Investoren. Gegenüber 2005 war ihr Anteil um 20 auf nunmehr 53 Prozent gestiegen.

Im Verlauf der internationalen Finanzkrise, die im Sommer 2007 in der US-Immobilienkrise ihren Ursprung hatte, begann der DAX wieder zu sinken. Ab Herbst 2008 wirkte sich die Krise zunehmend auf die Realwirtschaft aus. Infolgedessen brachen die Aktienkurse weltweit ein. Am 9. Oktober 2008 schloss der DAX mit 4887,00 Punkten erstmals seit dem 4. November 2005 unter der Grenze von 5000 Punkten. Am 13. Oktober 2008 erlebte der DAX mit 11,40 Prozent den größten Tagesgewinn seiner Geschichte. Zwei Wochen später, am 28. Oktober 2008, beendete der Index den Handel 11,28 Prozent höher als zum Vortagsschluss. Es war eine Reaktion auf die Rettungspläne der US-Regierung für die US-amerikanische Finanzbranche. Einen neuen Tiefststand erzielte der Index am 6. März 2009, als er den Handel mit 3666,41 Punkten beendete. Seit dem Allzeithoch vom 13. Juli 2007 entspricht das einem Rückgang um 54,8 Prozent. Der 6. März 2009 bedeutete das Ende der Talfahrt. Ab dem Frühjahr 2009 war der DAX wieder auf dem Weg nach oben. Bis zum 2. Mai 2011 stieg er um 105,3 Prozent auf einen Schlussstand von 7527,64 Punkten.

Die Abschwächung der globalen Konjunktur und die Verschärfung der Eurokrise ließen den deutschen Leitindex erneut einbrechen. Am 12. September 2011 beendete der DAX den Handel bei 5072,33 Punkten. Der Verlust seit dem Höchststand am 2. Mai 2011 betrug 32,6 Prozent. Die Ankündigungen neuer Anleihekaufprogramme der Europäischen Zentralbank und der US-Notenbank in grundsätzlich unbegrenztem Umfang führten anschließend wieder zu einer Erholung der Kurse am Aktienmarkt. Die monetären Impulse spielten eine größere Rolle bei der Kursbildung als die weltweite Wirtschaftsabkühlung und die Lage der Unternehmen. Im Mai 2013 konnten die bis dahin höchsten Stände vom Juli 2007 erstmals übertroffen werden: Am 22. Mai 2013 schloss der Index erstmals in seiner Geschichte über 8500 bei 8530,89 Punkten und damit um 68,18 Prozent höher als beim letzten Tiefpunkt am 12. September 2011. Nach einem Rücksetzer in den Sommermonaten erreichte er ab September 2013 immer neue Allzeithochs auf Schlusskursbasis. Am 29. Oktober 2013 schloss der DAX erstmals in seiner Geschichte über 9000 bei 9022,04 Punkten. Die Marke von 9500 Punkten wurde zwei Monate später, am 27. Dezember 2013 mit einem Schlusskurs von 9589,39 Punkten durchbrochen.

Am 5. Juni 2014 um 14:32 Uhr sprang der Index im Handelsverlauf erstmals über 10.000 Punkte. Zwei Handelstage später, am Pfingstmontag, dem 9. Juni 2014, erzielte der DAX erstmals einen fünfstelligen Stand (nämlich 10 008,63 Punkte) auch auf Schlusskursbasis. In Folge der Bekanntgabe der EZB, wegen einer sehr niedrigen Inflation bzw. drohenden Deflation, Anleihenkäufe von monatlich 60 Mrd. Euro bis September 2016 durchzuführen, erreichte der Index am 22. Januar 2015 ein neues Allzeithoch von 10.454,05 Punkten auf Schlusskursbasis. Bereits am Folgetag sprang der DAX im Handelsverlauf auf einen weiteren neuen Höchstwert von 10.702,84 Punkten. Rund zwei Drittel der DAX-Aktien befinden sich im ausländischen Besitz (Stand:2014). Am 19. Februar 2015 schloss der DAX erstmals in seiner Geschichte über 11.000 Punkten, bei 11.001,94 Punkten auf Schlusskursbasis und markierte somit ein neues Allzeithoch. In den folgenden drei Wochen erzielte der DAX immer neue Allzeithochs und schloss am 16. März 2015 erstmals über 12.000 Punkten, bei 12.167,72 Punkten zum Handelsschluss. 

Gleichzeitig löste der DAX (gerechnet ohne Dividenden) im Handelsverlauf dieses Tages den 15 Jahre alten Höchststand des Kursindex (6266,15 Punkte am 7. März 2000) ab, indem er zwischenzeitlich auf 6266,76 Punkte stieg. Auf Schlusskursbasis übertraf der Kursindex den Stand aus dem Jahr 2000 am 10. April 2015 mit einem Wert von 6331,39 Punkten.

Zu Schwankungen besonderer Art kam es Ende Oktober 2008, als die Porsche Automobil Holding versuchte, den wesentlich größeren Volkswagen-Konzern zu übernehmen. Porsche war im Oktober 2008 durch Zukäufe und Derivatgeschäfte im Besitz von etwa 42,6 % der Stammaktien und hatte über Kaufoptionen Anspruch auf weitere 31,5 %.

Als Porsche dies vier Tage vor der Veröffentlichung des Quartalergebnisses von VW bekannt gab, kam es in den Folgetagen zu einem heftigen Short Squeeze. Einige Marktteilnehmer (darunter auch Hedge-Fonds) hatten die VW-Aktien leerverkauft und mussten sich nun wieder eindecken. Nachdem Porsche etwa 75 % und das Land Niedersachsen weitere 20 % der Aktien hielten (VW-Gesetz), verblieben nur noch knapp 6 % an Streubesitz. Durch dieses knappe Angebot und die zusätzlich erhöhte Nachfrage der Leerverkäufer, schnellte die VW-Aktie bis zum 28. Oktober 2008 zeitweise auf über 1.000 €. Damit übertraf sie kurzzeitig die gesamte Marktkapitalisierung des zu dieser Zeit weltgrößten Unternehmens ExxonMobil.

Durch diesen außerordentlichen Preisanstieg erhöhte sich die Gewichtung der Aktie im DAX von ursprünglich 5,88 % im September auf 27,22 % zu Handelsschluss am 28. Oktober 2008. An diesem Tag kündigte Dow Jones & Company bereits an, den Free-Float-Faktor der VW-Stammaktie in seinen Indizes zum freitäglichen Handelsbeginn hin von 49,63 auf 37,32 Prozent herabzusenken. Die Deutsche Börse handelte zum Stichtag 3. November 2008, indem sie eine außerordentliche DAX-Indexneugewichtung vornahm und dabei die Gewichtung der Volkswagenaktie im DAX auf zehn Prozent kappte.

Die Übersicht zeigt die Allzeithöchststände des DAX als Performanceindex (mit Dividenden) und als Kursindex (ohne Dividenden).

Die Tabelle zeigt das erstmalige Überschreiten der jeweils nächsten 1000-Punkte-Marke auf Schlusskursbasis des bis 1959 zurückgerechneten DAX.

Die Tabelle zeigt die prozentual höchsten Tagesgewinne des von 2007 bis 1959 zurückgerechneten DAX.

Die Tabelle zeigt die prozentual höchsten Tagesverluste des von 2007 bis 1959 zurückgerechneten DAX.

Der DAX wurde bis 1981 als Performanceindex (mit Dividenden) und bis 1948 als Kursindex (ohne Dividenden) zurückgerechnet. Die Zeitreihe basiert auf einer Verkettung des DAX mit dem BZ-Index (1981–1986), dem Hardy-Index (1959–1980) und dem Aktienindex des Statistischen Bundesamtes (1948–1958). Frühere Daten stammen von Professor Richard Stehle, Institut für Bank-, Börsen- und Versicherungswesen, Humboldt-Universität zu Berlin. Stehle hat die Aktienrendite derjenigen Unternehmen berechnet, die im DAX-Performanceindex enthalten gewesen wären, wenn es das Börsenbarometer bereits ab 1937 gegeben hätte.

Das beste Jahr in der Geschichte des DAX war rechnerisch 1949 mit einem Gewinn von 152,13 Prozent, gefolgt von 1951 mit einem Plus von 115,41 Prozent und 1954 mit einem Gewinn von 82,59 Prozent. Das schlechteste Jahr war 1948 mit einem Verlust von 87,35 Prozent, gefolgt von 2002 mit einem Minus von 43,94 Prozent und 2008 mit einem Verlust von 40,37 Prozent. Der Zeitraum von 1948 bis 1954 ist geprägt von der Währungsreform und dem Wirtschaftswunder, der Zeitraum seit 2000 vom Platzen der Dotcom-Blase und den Auswirkungen der Finanzkrise ab 2007.

Die Tabelle zeigt die jährliche Entwicklung des bis 1937 zurückgerechneten deutschen Aktienindex DAX.




</doc>
<doc id="9681" url="https://de.wikipedia.org/wiki?curid=9681" title="Sanssouci">
Sanssouci

Schloss Sanssouci ( deutsch: "ohne Sorge") liegt im östlichen Teil des Parks Sanssouci und ist eines der bekanntesten Hohenzollernschlösser der brandenburgischen Landeshauptstadt Potsdam. Nach eigenen Skizzen ließ der preußische König Friedrich II. in den Jahren 1745 bis 1747 ein kleines Sommerschloss im Stil des Rokoko errichten. Mit der Planung beauftragte er den Architekten Georg Wenzeslaus von Knobelsdorff. Unter Friedrich Wilhelm IV. wurde das Schloss 1841/42 durch Umbau und Verlängerung der zwei Seitenflügel erweitert. Nach Skizzen des Königs erstellte Ludwig Persius die Entwurfszeichnungen.

Die Schlösser und Gartenarchitekturen in der weitläufigen Parkanlage Sanssouci werden von der Stiftung Preußische Schlösser und Gärten Berlin-Brandenburg verwaltet und stehen seit 1990 als Welterbe unter dem Schutz der UNESCO. Die Deutsche UNESCO-Kommission begründet die Aufnahme in die Welterbeliste wie folgt: „Schloss und Park von Sanssouci, oft als "preußisches Versailles" bezeichnet, sind eine Synthese der Kunstrichtungen des 18. Jahrhunderts in den Städten und Höfen Europas. Das Ensemble ist ein herausragendes Beispiel von Architekturschöpfungen und Landschaftsgestaltungen vor dem geistigen Hintergrund der monarchistischen Staatsidee.“

Die berühmte Gartenansicht von Sanssouci entstand nach der Entscheidung Friedrichs des Großen, am Südhang des Bornstedter Höhenzugs einen terrassierten Weinberg anzulegen. Vormals standen auf der Anhöhe Eichen. Zu Zeiten des Soldatenkönigs Friedrich Wilhelm I. wurden die Bäume gefällt und beim Ausbau der Stadt Potsdam für die Befestigung des sumpfigen Bodens verwendet. Nachdem Friedrich Wilhelm I. 1714 den bisherigen Lustgarten am Potsdamer Stadtschloss zu einem Exerzierplatz hatte umbauen lassen, ließ er als Ersatz 1715 nordwestlich des Brandenburger Tors, auf einem Gelände, das bisher von Potsdamer Bürgern als Gartenfläche genutzt worden war, den Marlygarten als Lust- und Küchengarten anlegen und mit einem Lusthaus aus Fachwerk versehen. In diesem Zusammenhang wurden am Abhang des ansonsten kahlen Bornstedter Mühlenbergs bereits erste Weinpflanzungen gesetzt. In diesem Zustand kannte Friedrich II. aus seiner Kronprinzenzeit das Areal.

Am 10. August 1744 gab Friedrich II. Order, den „Wüsten Berg“ durch die Anlage von Weinterrassen zu kultivieren. Unter Leitung des Architekten Friedrich Wilhelm Diterichs wurde der Südhang in sechs breite Terrassen gegliedert mit zur Mitte hin bogenförmig nach innen schwingenden Mauern, um eine größtmögliche Ausnutzung der Sonnenstrahlung zu erreichen. An den Wänden der Stützmauern wechseln gerade Flächen, an denen Spaliere mit heimischen Obst- und Weinsorten empor rankten, mit 168 verglasten Nischen, in denen ausländische Sorten wuchsen. Die einzelnen Terrassenpartien waren oberhalb der Mauern durch Rasenstreifen begrenzt und mit Spalierobst bepflanzt. Zwischen 96 Taxuspyramiden standen im Sommerhalbjahr 84 Orangenbäume in Kübeln. Mit den gärtnerischen Arbeiten war Philipp Friedrich Krutisch betraut. In der Mittelachse führten 120 (heute 132) Stufen den Hang hinauf, entsprechend den Terrassen sechsmal unterteilt und zu beiden Seiten des Hanges je eine Auffahrtrampe. Die Arbeiten an den Weinbergterrassen waren 1746 weitgehend fertig gestellt.

Unterhalb der Terrassen, im Parterre, entstand ab 1745 ein Ziergarten im barocken Stil mit Rasenflächen, Blumenbroderien und flankierenden Bosketten. Die Mitte des Parterres zierte 1748 ein vierpassförmiges Brunnenbecken, die „Große Fontäne“. Die Mitte des Vierpassbeckens schmückten vergoldete Bleiplastiken mit Darstellungen aus der griechischen Mythologie, die heute nicht mehr erhalten sind. Seit 1750 umsäumen zwölf marmorne Statuen, acht Götterfiguren und allegorische Darstellungen der vier Elemente das Wasserbassin: Merkur, das Wasser „La pêche dans la mer“, Apollon mit dem getöteten Python, Diana beim Bade, das Feuer „Venus betrachtet den von Vulkan für Aeneas geschmiedeten Schild“, Juno mit dem Pfau, Jupiter mit Jo, die Erde „Ceres lehrt Triptolemos das Pflügen“, Mars, Minerva, die Luft „Le retour de la chasse“ sowie Venus. Venus und Merkur, Arbeiten des Bildhauers Jean-Baptiste Pigalle, und zwei Jagdgruppen, Allegorien der Elemente Luft und Wasser von Lambert-Sigisbert Adam, waren Geschenke des französischen Königs Ludwig XV. Die übrigen Figuren stammen aus der Werkstatt von François Gaspard Adam, dem Leiter des von Friedrich II. in Berlin gegründeten französischen Bildhauerateliers. Die Vervollständigung des sogenannten „Französischen Rondells“ dauerte bis 1764. Das Parterre begrenzte im Süden ein Wassergraben. Ein südöstlich liegender Nutzgarten, der Marlygarten, blieb bestehen. Den 1715 unter Friedrich Wilhelm I. angelegten Küchengarten nannte der Soldatenkönig spöttisch „mein Marly“, in Anlehnung an die aufwändige Gartenanlage Marly-le-Roi des französischen Königs Ludwig XIV. Auf die Verbindung von Zier- und Nutzgarten, Kunst und Natur, legte Friedrich II. auch bei der späteren Parkerweiterung großen Wert.

Die Harmonie zwischen Kunst und Natur spiegelt sich ebenfalls in der Lage und Gestaltung des Schlosses Sanssouci auf der Höhe des Weinbergs wider. Der seit dem 13. Jahrhundert in der Mark Brandenburg durchaus übliche Weinanbau nahm in dieser Gegend nie eine zentrale Stellung in der künstlerischen Gestaltung der fürstlichen Lustgärten ein. In Sanssouci sollte er durch die Anlage der Weinbergterrassen mit dem bekrönenden Schloss und dem Parterre zum Mittelpunkt des Parks werden. Mit einem weiten Blick in die Landschaft, inmitten der Natur, wollte der preußische König in den Sommermonaten leben und seinen persönlichen Neigungen und künstlerischen Interessen, aber auch den Staatsgeschäften nachgehen. Eine Bockwindmühle, die bereits seit 1739 auf der Anhöhe stand, unterstrich die ländliche Idylle des Ortes. Friedrich II. war der Meinung, dass "die Mühle dem Schloss eine Zierde sey".

In der Kabinettsorder vom 13. Januar 1745 verfügte Friedrich II. den Bau eines "Lust-Hauses zu Potsdam". Nach Skizzen des Königs hatte Georg Wenzeslaus von Knobelsdorff Entwurfszeichnungen angefertigt. Den Vorschlägen Knobelsdorffs, das Gebäude durch ein Sockelgeschoss zu erhöhen, zu unterkellern und bis nahe an den Rand der obersten Terrasse zu stellen, um dem Gebäude vom Parterre aus gesehen eine bessere Wirkung zu verleihen, widersprach Friedrich. Er wünschte kein repräsentatives Gebäude, sondern ein intimes Wohnschloss im Stil des Rokoko, das nur seinen privaten Bedürfnissen entsprach. Einen ebenerdigen Bau, dessen Sockel der Berg war, ein „maison de plaisance“, ohne eine Vielzahl von Stufen, um vom Innenraum direkt auf eine breite Terrasse und von dort in den Garten zu gelangen. Eine enge Verbindung zwischen Wohnkultur und freier Natur.

Bei allen im Auftrag Friedrichs II. geschaffenen Bauwerken in Potsdam und Berlin griff dieser administrativ und künstlerisch in das Baugeschehen ein. Nach seinen Vorgaben wurden Entwürfe angefertigt und vor jedem Baubeginn Kostenvoranschläge gemacht. Erst nach der Genehmigung durch den König durften die Arbeiten beginnen. Er mischte sich in alles ein und wollte in allen Einzelheiten unterrichtet werden, was oft zu Missstimmigkeiten zwischen den Architekten und dem König führte und auch Rückbauten auslöste. Die autokratische Wesensart Friedrichs II. schränkte somit auch die baukünstlerischen Vorstellungen Knobelsdorffs ein, der die eigenwilligen Wünsche seines Auftraggebers architektonisch umzusetzen hatte. Diterichs übertrug die Risse Knobelsdorffs ins Detail, wählte die Materialien aus, schloss die Verträge mit Bildhauern und Steinmetzen und beauftragte Johann Gottfried Büring und Carl Ludwig Hildebrandt, mit denen er bereits den Weinberg terrassiert hatte, als „Conducteure“ mit der Ausführung. Am 14. April 1745 wurde der Grundstein gelegt. Am 2. Mai wurde Diterichs durch Kabinettsbefehl als Bauleiter durch Jan Bouman ersetzt und kehrte mit Büring nach Berlin zurück.
Nach nur zwei Jahren Bauzeit fand am 1. Mai 1747 die Einweihung des Weinbergschlosses statt, obwohl noch nicht alle Räume fertiggestellt waren. Außer in Kriegszeiten lebte Friedrich II. dort von Ende April bis Anfang Oktober. Das Gebäude war nur für den König und von ihm ausgewählte Gäste konzipiert. Von seiner Gemahlin Elisabeth Christine von Braunschweig-Bevern, mit der er seit 1733 verheiratet war, trennte er sich räumlich nach seiner Thronbesteigung 1740. Ihr wies er das Schloss Schönhausen bei Berlin zu.

Im Rokoko kam es zu einer Trennung von privatem und öffentlichem Bereich. Für die repräsentativen Verpflichtungen war das Potsdamer Stadtschloss vorgesehen, dessen Umbau zur selben Zeit stattfand und das von Friedrich II. in den Wintermonaten bewohnt wurde. Potsdam entwickelte sich zur eigentlichen Residenz, während das Berliner Schloss, in dem die Königin Repräsentationsaufgaben wahrnahm und Schloss Charlottenburg, wo Friedrich II. zu Beginn seiner Regierungszeit den „Neuen Flügel“ an der östlichen Seite anbauen ließ, an die zweite Stelle traten und das Königsberger Schloss sowie das Breslauer Stadtschloss nur gelegentlich besucht wurden.

In Sanssouci komponierte, musizierte und philosophierte der preußische Monarch. Er regierte diszipliniert sein Land und lebte bescheiden ohne Prunk. Seine Bescheidenheit steigerte sich im Alter bis zum Geiz. Zu seinen Lebzeiten ließ Friedrich II. an der Außenfassade keine und in den Innenräumen nur mit Widerwillen Reparaturen vornehmen, da es, wie er bei anderer Gelegenheit sagte, "nur bey meinem Leben dauern" soll. Die Gleichgültigkeit des an Rheuma und Gicht leidenden Königs gegenüber nötigen Renovierungen kritisierte Oberhofbaurat Heinrich Ludwig Manger später in seiner „Baugeschichte von Potsdam“: "Leider hat der große Mann an vielen seiner Baue Schadhaftigkeiten erlebt, deren Reparaturkosten ihm außerordentlich empfindlich waren". So stellte sich auch die fehlende Unterkellerung, auf die der König gegen Knobelsdorffs Rat bestanden hatte, als Baufehler heraus, da sie zur Beschädigung der Parketts durch aufsteigende Feuchtigkeit und zu ständiger Fußkälte führte.

Der „Alte Fritz“, wie er im Volksmund genannt wurde, starb am 17. August 1786 im Sessel seines Arbeitszimmers im Schloss Sanssouci. Er wollte laut eigener Verfügung in einer Gruft neben seinen Lieblingshunden beigesetzt werden. Die unterirdische, gemauerte und mit Marmorplatten bedeckte Grabkammer hatte er bereits 1744, noch vor Beginn des eigentlichen Schlossbaues, seitlich auf der obersten Terrasse des gerade angelegten Weinbergs errichten lassen. In seiner 46-jährigen Regierungszeit beschäftigte sich Friedrich immer wieder mit dem Tod. Neben seinem Politischen Testament von 1752 verfasste er vor fast jeder Schlacht, vor jedem Krieg neue Verfügungen, in denen er bis ins kleinste Detail alles Familiäre und Finanzielle regelte. Ebenso oft wiederholte er die Anweisungen für sein Begräbnis:

Sein Neffe und Nachfolger Friedrich Wilhelm II. befolgte diese Anweisungen nicht. Er ließ Friedrichs II. Sarg stattdessen in der Gruft der Potsdamer Garnisonkirche unmittelbar neben dem Sarg seines Vaters, des Soldatenkönigs Friedrich Wilhelm I., aufstellen. Besuchern zeigte er jedoch die Grabstelle auf der Terrasse mit den Worten: „Hier wollte mein Vorgänger begraben sein, er wollte lieber neben seinen Hunden als zwischen seinen Vorfahren liegen.“ Die Zeitgenossen hielten eine solche Grabstätte für eines Königs unwürdig und einen Ausdruck von Friedrichs Menschenverachtung, obwohl die Gruft vor den Hundegräbern entstanden war. Auch Friedrichs Vorbild Moritz von Nassau war 1680 aus seinem Waldgrab in eine Fürstengruft überführt worden. Der Brauch der Garten- und Parkbegräbnisse setzte erst mit der von der Romantik geprägten nachfolgenden Generation ein, so wurde bereits Friedrichs Bruder Heinrich 1802 wunschgemäß im Park von Schloss Rheinsberg bestattet.

Im Zweiten Weltkrieg, brachten Soldaten der Wehrmacht die Särge aus der Garnisonkirche in Sicherheit. Im März 1943 kamen sie in das „Objekt Kurfürst“, einen unterirdischen Bunker auf dem Gelände des heutigen Einsatzführungskommandos der Bundeswehr im Ortsteil Geltow (Wildpark-West) der Gemeinde Schwielowsee und im März 1945 in das Salzbergwerk Bernterode im Eichsfeld. Nach Kriegsende verbrachten Soldaten der amerikanischen Armee die Särge im Mai 1945 in das Marburger Landgrafenschloss, im Februar 1946 in das Hessische Staatsarchiv Marburg und im August 1946 in die Elisabethkirche. Dort blieben sie bis zur Überführung auf die Burg Hohenzollern bei Hechingen im August 1952.
Nach der Wiedervereinigung Deutschlands wurde die testamentarische Verfügung Friedrichs II. erfüllt. Die Initiative ging von Louis Ferdinand Prinz von Preußen aus, dem seinerzeitigen Chef des Hauses Hohenzollern und Hausherrn der Burg Hohenzollern, der die Särge 1953 in die Christuskapelle der Burg hatte überführen lassen. Die unterirdische Grabkammer erwies sich auch nach fast 250 Jahren als weitgehend intakt, das Mauerwerk wurde saniert und eine wasserdichte Bedachung eingesetzt. Am 17. August 1991, dem 205. Todestag Friedrichs II., wurde der Sarkophag mit den sterblichen Überresten des Königs im Ehrenhof des Schlosses Sanssouci aufgebahrt, eskortiert von einer Ehrenwache der Bundeswehr. Da der König angeordnet hatte: "Im übrigen will ich, was meine Person betrifft, in Sanssouci beigesetzt werden, ohne Prunk, ohne Pomp und bei Nacht" fand die Beisetzung um Mitternacht statt, in Anwesenheit des Bundeskanzlers Helmut Kohl, von Mitgliedern des Hauses Preußen und übertragen vom Fernsehen.

Laut Nicolai soll Friedrich II. bei einem Spaziergang über die Schlossbaustelle einst zum Marquis d’Argens gesagt haben: . Die Grabstelle zieren die 1749 von François Gaspard Adam geschaffene Marmorgruppe „Flora mit Zephyr“ und sechs im Halbrund aufgestellte Porträtbüsten römischer Kaiser.

Franz Theodor Kugler fasst die Bedeutung der Grabstätte im Zusammenhang mit der Gesamtanlage 1840 wie folgt zusammen:

Besucher legen auf dem schlichten Grabstein mit der Aufschrift "Friedrich der Große" Blumen und Kartoffeln nieder, in Erinnerung an den Kartoffelbefehl.

Friedrichs Vater, der Soldatenkönig, fand seine vorerst letzte Ruhe im Kaiser-Friedrich-Mausoleum an der Friedenskirche im Park Sanssouci, weil die von ihm erbaute und zu seiner Grablege bestimmte Potsdamer Garnisonkirche 1945 ausgebrannt und 1968 abgerissen worden war.

Nach dem Tod Friedrichs II. begann in Preußen eine neue Epoche, die auch durch den Formenwandel in der Architektur sichtbar wurde. Mit dem Regierungsantritt des Nachfolgers Friedrich Wilhelm II. hielt der in Europa schon längst favorisierte klassizistische Baustil auch in Potsdam und Berlin Einzug. Der neue König ließ nach seinem Regierungsantritt das Marmorpalais im Neuen Garten bauen und wohnte nur bis zu dessen Fertigstellung in Sanssouci. Noch im Todesjahr Friedrichs II. wurden die Möbel ausgewechselt und das stark verwohnte Arbeits- und Schlafzimmer durch den Architekten Friedrich Wilhelm von Erdmannsdorff renoviert und verändert. Zu der Zeit, als Friedrich II. von 1763 bis 1769 das Neue Palais in den Formen des Barock errichten ließ, schuf der Dessauer Architekt mit Schloss Wörlitz im Wörlitzer Park den frühesten klassizistischen Bau in Deutschland. Nach seinen Plänen entstand nun in Sanssouci der erste konsequent im Stil des Klassizismus gestaltete Innenraum der Potsdamer und Berliner Schlösser.

Der ab 1797 regierende Friedrich Wilhelm III. nutzte Sanssouci lediglich für gelegentliche Aufenthalte, ohne etwas am Inventar zu verändern. Nur seine Gemahlin Luise wohnte 1794 mit ihrer Schwester Friederike für einige Monate im Schloss, während Friedrich Wilhelm in Polen war. Die Familie verbrachte die Sommermonate vorzugsweise im Schloss Paretz oder auf der Pfaueninsel. Schloss und Inventar überstanden auch die französische Besetzung Potsdams 1806 unbeschadet, da es Napoléon unter seinen persönlichen Schutz stellte und so vor Plünderungen bewahrte.

Fast einhundert Jahre nach dem Bau des Schlosses Sanssouci kam ein König auf den preußischen Thron, der ein Bewunderer Friedrichs des Großen und seiner Welt war. Friedrich Wilhelm IV., der „Romantiker auf dem Thron“, empfand eine Gemeinsamkeit der vielschichtigen Interessen, besonders auf dem Gebiet der Architektur und der künstlerischen Mitgestaltung. Schon in der Kronprinzenzeit bezog er im Jahr 1815 die ehemaligen Räume Friedrichs II. im Berliner Stadtschloss. 1835 erhielt er die Erlaubnis, auch im Schloss Sanssouci wohnen zu dürfen, obwohl ihm und seiner Gemahlin Elisabeth Ludovika von Bayern das erst wenige Jahre zuvor erbaute, südwestlich gelegene Sommerschloss Charlottenhof zur Verfügung stand. Das Kronprinzenpaar bezog die ehemaligen Gästezimmer auf der Westseite. Die Räume Friedrichs II. auf der Ostseite dienten zunächst als Staats- und Gesellschaftsräume und wurden erst Jahre später in die private Nutzung einbezogen.

Nach der Thronbesteigung 1840 machte die größere Hofhaltung einen Um- und Ausbau der Seitenflügel nötig. Nach Skizzen Friedrich Wilhelms IV. fertigte Ludwig Persius die Entwürfe. Die alten Seitenflügel wurden abgerissen und 1841/42 unter Leitung des Architekten Ferdinand von Arnim verlängert und aufgestockt. Das vorhandene Mobiliar blieb erhalten, fehlende Stücke wurden nach Möglichkeit durch Möbel aus friderizianischer Zeit ersetzt. Das unter Friedrich Wilhelm II. umgestaltete Sterbezimmer Friedrichs II. sollte wieder in seinen ursprünglichen Zustand versetzt werden. Eine Realisierung dieses Plans erfolgte jedoch nicht, weil Friedrich Wilhelm IV. die Unterlagen und Entwürfe nicht authentisch genug erschienen.

Die in friderizianischer Zeit fast kahle, nur mit Laubengängen, Gitterpavillons und Bildwerken ausgestattete oberste Weinbergterrasse wurde 1845 mit Vasen und von Persius und Ludwig Ferdinand Hesse entworfenen Wasserspielen geschmückt, durch eine Marmorbalustrade begrenzt und auf den fünf unteren Ebenen Schöpfbrunnen angelegt. Hofgärtner Hermann Sello bepflanzte die Terrassen mit Gehölzen. Im Parterre erweiterte Persius 1840/41 das Fontänenbecken zu einem Kreis, wodurch sich auch der Skulpturenkreis des „Französischen Rondells“ um circa drei Meter vergrößerte. 1848 kamen zehn (heute acht) halbrunde, von Hesse entworfene Marmorbänke zwischen die Figuren. Aus demselben Jahr stammen auch vier in den Außenkompartimenten westlich und östlich der „Großen Fontäne“ aufgestellte Marmorsäulen mit Figurenkopien nach antiken Vorbildern sowie je zwei marmorne Brunnenwände mit Bagnerolen (Marmorwannen) und Statuen der Musen Klio, Polyhymnia, Euterpe und Urania. Am südlichen Ende des Parterres, in der Mittelachse, wurde 1866 eine verkleinerte Nachbildung des Reiterstandbilds Friedrichs des Großen aufgestellt, das heute im „Neuen Stück“ unterhalb des Orangerieschlosses steht.

Friedrich Wilhelm IV. starb am 2. Januar 1861 im Schloss Sanssouci und wurde in der Gruft der nahe liegenden Friedenskirche beigesetzt. Die letzte Bewohnerin des Schlosses war seine Witwe Elisabeth Ludovika. Sie lebte noch dreizehn Jahre in Sanssouci, bis sie am 14. Dezember 1873 starb und neben Friedrich Wilhelm IV in einer Zeremonie bestattet wurde.

Nach 1873 stellte Wilhelm I. das Schloss mit Inventar musealen Zwecken zur Verfügung, wodurch es mit zu den ältesten Schlossmuseen in Deutschland gehört. Nach dem Ersten Weltkrieg und dem Ende der Monarchie verblieb es zunächst im Besitz der Hohenzollern und kam 1927 in die Obhut der am 1. April desselben Jahres gegründeten preußischen „Verwaltung der Staatlichen Schlösser und Gärten“. Unter Leitung des Direktors Ernst Gall versuchte die Schlösserverwaltung mit Unterstützung der Staatlichen Museen zu Berlin die Innenraumgestaltung zur Zeit Friedrichs II. wiederherzustellen. Unter anderem kam der Schreibtisch Friedrichs des Großen in das Arbeits- und Schlafzimmer zurück. Das denkmalpflegerische Konzept betraf auch den gesamten friderizianischen Parkteil, mit dessen Rekonstruktion Gartenoberinspektor Georg Potente, seit Juni 1927 Gartendirektor des Parkreviers Sanssouci, betraut wurde. Im Zuge dieser Wiederherstellungsarbeiten ließ er ab 1927 die stark bewachsenen Weinbergterrassen freilegen und neu bepflanzen, zwei Halbrundbänke im „Französischen Rondell“ aus der Mittelachse nehmen sowie die Wasserspiele und Bildwerke aus der Zeit Friedrich Wilhelms IV. von der obersten Terrasse entfernen.

Als im Zweiten Weltkrieg die Luftangriffe auf Berlin begannen, wurden 1942 zahlreiche Kunstgegenstände nach Rheinsberg und Bernterode ausgelagert. Aus Schloss Sanssouci kamen Gemälde französischer Maler des 18. Jahrhunderts, Konsolvasen aus Meißener Porzellan, fast alle Möbel aus der „Kleinen Galerie“ und die Bibliothek Friedrichs II. Die restlichen Möbel, fast alle Skulpturen und Bilderrahmen blieben im Schloss. Die Kämpfe um Potsdam im April 1945 überstand das Gebäude unbeschadet, obwohl auf der Nordseite, zwischen der Auffahrt zum Schloss und der Historischen Mühle, Kampfhandlungen stattfanden, in deren Verlauf die Galeriewindmühle abbrannte. Nach dem Einmarsch der Roten Armee in Potsdam am 27. April 1945 wurde der Park Sanssouci unter die Kontrolle des Oberstleutnants der Garde Jewgeni Fjodorowitsch Lutschuweit gestellt und bis zum 4. Juni 1946 für die Öffentlichkeit geschlossen. Die meisten der nach Rheinsberg ausgelagerten und der in Sanssouci gebliebenen Kunstgegenstände gelangten als Beutegut in die damalige Sowjetunion und kamen 1958 nur zu einem geringen Teil zurück. Die von amerikanischen Soldaten gefundenen Kunstgegenstände aus Bernterode wurden zunächst zum Central Art Collecting Point im Museum Wiesbaden gebracht und 1957 in das Schloss Charlottenburg in West-Berlin. Nach der Wiedervereinigung Deutschlands kehrte die Büchersammlung Friedrichs II. 1992 von Charlottenburg nach Sanssouci zurück. Zwischen 1993 und 1995 folgten sechsunddreißig Ölgemälde und zwei Marmorbüsten der Amphitrite und des Neptun von Lambert-Sigisbert Adam. Mit Hilfe der Kulturstiftung der Länder und der Stiftung Deutsche Klassenlotterie konnten bereits 1990 die nach Rheinsberg ausgelagerten Gemälde „Sultan im Garten“ und „Wahrsagerin“ von Jean-Baptiste Pater aus dem Kunsthandel zurückerworben werden. 1966 begann eine umfassende Gebäuderestaurierung. Seit 1981 ist der westliche Seitenflügel, der sogenannte „Damenflügel“, und seit 1993 die Küche im östlichen Seitenflügel für die Öffentlichkeit zugänglich.

Das für einen Regenten in seinen Ausmaßen eher bescheidene Schloss mit zwölf Räumen, von denen Friedrich II. nur fünf selbst bewohnte, entsprach der Veränderung in der höfischen Baukunst um die Mitte des 18. Jahrhunderts. Die barocken Residenzschlösser, die nach dem Vorbild von Versailles ab der Mitte des 17. Jahrhunderts errichtet wurden, dienten den fürstlichen Bauherren vor allem zur Repräsentation ihrer politischen und wirtschaftlichen Macht. Sie gingen in ihrer Größe oft weit über den eigentlichen Nutzen als Wohnsitz und die Notwendigkeit einer standesgemäßen Hofhaltung hinaus.

Dieses Übermaß an Pracht und Größe erweckte die Sehnsucht nach Intimität und Bequemlichkeit. Der Wandel wurde jedoch nicht radikal vollzogen, sondern erfolgte allmählich. Friedrich II., der zeit seines Lebens die Formen des Barock und Rokoko bevorzugte, ließ noch zwei Jahrzehnte nach dem Bau des Schlosses Sanssouci das Neue Palais im westlichen Teil des Parks errichten. Nach dem Siebenjährigen Krieg wollte er mit dem Gästeschloss die Macht und Stärke Preußens demonstrieren. So bezeichnete er es auch als seine „Fanfaronnade“ (Prahlerei, Angeberei).

Der eingeschossige Hauptbau nimmt mit seinen angrenzenden Seitenflügeln fast die gesamte Breite der obersten Terrasse ein. Die Länge des Hauptbaus "mit den beiden runden Kabinetten an den Seiten, beträgt 292 Fuß [91,6 m], und die Tiefe 49 Fuß [15,4 m]. […] die ganze Höhe von außen 39 Fuß 2 Zoll [rund 12,3 m]". Die 15-achsige Südseite betont ein vorspringender, halbovaler Mittelbau mit einer bekrönenden Kuppel. Über dem mittleren Rundbogenfenster ist der Name des Schlosses in vergoldeten Bronzelettern angebracht. Zwischen den fast bodentiefen Rundbogenfenstern stützen sechsunddreißig paarweise angeordnete Atlanten das Gebälk. Die Sandsteinfiguren des Bildhauers Friedrich Christian Glume stellen Bacchanten und Bacchantinnen dar und wurden 1746 aus roh versetzten Steinblöcken vor Ort ausgearbeitet. An der Gestaltung des Skulpturenschmucks auf der umlaufenden Dachbalustrade und der Puttengruppen auf den Kuppelfenstern war er ebenso beteiligt wie auch sein Vater Johann Georg Glume und die Werkstätten der Zierratenbildhauer Johann Melchior Kambly und Matthias Müller.

Die in friderizianischer Zeit schmucklosen Seitenflügel, "jeder von 98 Fuß [31 m] Länge und 35 Fuß [11 m] Tiefe", in denen die Küche, Stallungen und Räume für die kleine Dienerschaft untergebracht waren, verdeckte Knobelsdorff mit symmetrisch angeordneten Laubengängen, die in je einem freistehenden, mit vergoldeten Ornamenten verzierten Gitterpavillon ihren Abschluss fanden. Vor den Laubengängen stehen Porträtbüsten römischer Persönlichkeiten und Vasenkopien. Im östlichen Pavillon ließ Friedrich II. die Figur des „Betenden Knaben“ aufstellen, die er 1747 aus dem Besitz des Fürsten Wenzel von Liechtenstein erworben hatte. Seit 1900 steht dort ein Nachguss aus der Berliner „Bronce-Waaren-Fabrik L. C. Busch“.

Die schlichter gehaltene Nordseite des Schlosses steht im auffälligen Gegensatz zur bildhauerisch verspielten Südseite. Anstelle der Atlanten gliedern hier korinthische Pilaster die Front. Das Pendant zum halbovalen Mittelbau auf der Gartenseite bildet ein rechteckiger Risalit mit Blendsäulen und flachem Pultdach. Die Front schließt an beiden Enden mit kurzen, im rechten Winkel angesetzten Flügelbauten. Weiterführende Kolonnaden umschließen im Halbrund den schmucklosen Ehrenhof und öffnen sich zu der nördlich gelegenen steilen Zufahrtsrampe. Die in zwei Reihen angeordneten vierundvierzig Säulenpaare lassen Platz für Wandelgänge. Wie auf der Südseite schmückt auch hier eine Balustrade mit Sandsteinvasen den Dachansatz des Schlossbaus und die Viertelbögen der Kolonnade. Wein- und Blumenranken aus Sandstein zieren die Rundbögen der fast bodentiefen Fenster und Fenstertüren.

Nach dem Abriss der eingeschossigen Anbauten aus friderizianischer Zeit entstanden um zwei Achsen verlängerte Seitenflügel, mit je zehn Fensterachsen und dreibogigen Vorhallen an den Stirnseiten. Unter Einhaltung der Traufhöhe des Schlossbaus wurden die Anbauten um ein Geschoss aufgestockt und das flache Satteldach hinter einer Balusterattika verborgen. Die Fenster bekamen einen geraden Abschluss. Für die Fassaden übernahm Persius die Gestaltungselemente der Nordseite. Pilaster, Baluster und Verzierungen wurden aus Zink gegossen und gesandelt, sodass sie den Vorbildern aus Sandstein täuschend ähnlich sehen.

Das Schloss entspricht den Grundsätzen eines „Maison de plaisance“, dessen Räume in Sanssouci auf einer Ebene liegen, um von ihnen mühelos in den Garten zu gelangen. Auch bei der Raumaufteilung wurde Wert auf Bequemlichkeit gelegt. Nach Ansicht der zeitgenössischen französischen Architekturtheorie entsprach das Appartement double dem höfischen Komfort. Bei dieser Aufteilung liegen zwei Reihen von Zimmern hintereinander: die Haupträume auf der dem Garten zugewandten Seite, in der Regel nach Süden, und die Dienerkammern dahinter auf der Nordseite des Gebäudes. Ein „Appartement double“ besteht somit aus einem Hauptraum und einer anschließenden Dienerkammer. Türen verbinden die Appartements miteinander. Sie sind in einer Achse angeordnet, einer Enfilade, so dass die Ausdehnung des Schlosses im Innern mit einem Blick erfasst werden kann. Ein repräsentativer Eingangsbereich beherrscht den Mittelbau, der den intimen Charakter des Gebäudes nicht sogleich erkennen lässt.

Friedrich der Große fertigte nach diesen Regeln höfischer Baukunst Grundrissskizzen an, die aber unter Berücksichtigung seiner persönlichen Wünsche und Vorstellungen von Wohnkomfort in einigen Bereichen von der französischen Bautheorie abwichen. Auch bei der Ausstattung der Innenräume bestimmte er bis ins Detail, wie die Räume auszusehen hatten. Nach oft von ihm vorgefertigten Skizzen schufen Künstler wie Johann August Nahl, die Brüder Johann Michael und Johann Christian Hoppenhaupt, die Brüder Johann Friedrich und Heinrich Wilhelm Spindler und Johann Melchior Kambly Kunstwerke im Stil des Rokoko. Friedrich dem Großen war jede „Luxussucht“, was seine Person anbetraf, fremd. Er kümmerte sich wenig um Etikette und Mode, was ihn mit zunehmendem Alter mit verschmutzter und verschlissener Kleidung herumlaufen ließ, aber es war ihm ein inneres Bedürfnis, sich mit edlen Dingen zu umgeben. Er hatte ein feines Gespür für alles Schöne und gestaltete seine Privatgemächer nach eigenem Geschmack und eigenen Bedürfnissen, wobei er das Gängige oft ignorierte. Diese „Eigenkompositionen“ in der Rokokokunst führten zu dem Begriff "friderizianisches Rokoko".

Im Mittelteil des Schlosses liegen in der Nord-Süd-Achse das Vestibül und der dem Garten zugewandte Marmorsaal. Nach Osten schließt die Königswohnung an, mit Audienzzimmer, Konzertzimmer, Arbeits- und Schlafzimmer, Bibliothek und einer langgestreckten Galerie auf der Nordseite. Westlich der beiden Mittelsäle liegen fünf Gästezimmer.

Im Vestibül, das vom Ehrenhof betreten wird, wiederholt sich die gekuppelte Säulenstellung der Kolonnade. Die Wände des rechteckigen Vorsaals sind durch zehn korinthische Säulenpaare aus weißem Stuckmarmor mit vergoldeten Basen und Kapitellen gegliedert. Sie stehen vor korinthischen Pilastern, die nur leicht aus der Wand heraustreten. Das Deckenbild über der gewölbten Voute zeigt die römische Göttin Flora mit Genien, die Blumen und Früchte vom Himmel streuen. Das Gemälde schuf 1746 der schwedische Maler Johann Harper. Den drei Fenstertüren an der Ehrenhofseite entsprechen auf der gegenüberliegenden Seite drei flache, rundbogige Blendnischen mit Türen. Über der mittleren Flügeltür, dem Eingang zum Marmorsaal, und über zwei Türen in der West- und Ostwand sind vergoldete Supraportenreliefs von Georg Franz Ebenhech angebracht. Sie stellen mit Themen aus dem Bacchusmythos ebenso einen Bezug zum Weinberg her wie die Ornamente an den Türfüllungen mit vergoldeten Weinranken, Hermen und Musikemblemen von Johann Christian Hoppenhaupt. Die 1730 von Lambert-Sigisbert Adam geschaffene Marmorkopie des Ares Ludovisi kam als Geschenk Ludwigs XV. zusammen mit den Figuren aus dem „Französischen Rondell“ 1752 nach Potsdam. Den Ares ließ Friedrich II. als Gegenstück einer Statue des Merkur aufstellen, die aus der Sammlung seiner Schwester Wilhelmine von Bayreuth stammte. Friedrich Wilhelm II. ließ den Merkur im Marmorpalais aufstellen und durch eine Trajan-Statue ersetzen. Beide Figuren gelangten 1830 in die Berliner Antikensammlung. Als Ersatz für den Trajan kam ein Merkur von Jean-Baptiste Pigalle in das Vestibül. An dessen Stelle trat 1846 die von Heinrich Berges geschaffene Sitzstatue der jüngeren Agrippina.

Der auf der Gartenseite liegende Marmorsaal diente als Festsaal. Für den ovalen Grundriss und die durch eine Lichtöffnung im Scheitelpunkt geöffnete Kuppel nahm Knobelsdorff das Pantheon in Rom zum Vorbild. Der namengebende Marmor aus Carrara und Schlesien findet sich an Säulen, Wänden, Fensterlaibungen sowie in den ornamentalen Einlegearbeiten des Fußbodens. Die vergoldeten Stuckarbeiten in der Kuppel führten Carl Joseph Sartori (1709–1770) und Johann Peter Benkert aus. Sie gestalteten die Wölbung mit kassettierten Feldern, militärischen Emblemen und in Medaillons dargestellten Attributen der Künste und Wissenschaften. Vier weibliche Figuren und Puttengruppen von Georg Franz Ebenhech auf dem Gesims symbolisieren die Zivil- und Militärarchitektur, die Astronomie und Geographie, die Malerei und Bildhauerkunst sowie die Musik und Poesie. Die Anordnung der acht korinthischen Säulenpaare wiederholt sich wie im Vestibül. In den dazwischen liegenden Nischen neben der Tür sind die 1748 von François Gaspard Adam geschaffenen Skulpturen der Venus Urania und des Apollon platziert. Der zur Venus gewandte Apollon hält ein geöffnetes Buch in der Hand, das als das Werk „De rerum natura“ des epikureischen Dichters Lukrez zu deuten ist. Ihr gelten die in vergoldeten Lettern eingefügten Worte "Te sociam studeo scribendis versibus esse / Quos ego de rerum natura pangere conor" (zu deutsch: „Nach dir [Venus] verlange ich als meiner Gefährtin beim Dichten der Verse, die ich mich über der Dinge Wesen zu schreiben erkühne“). Die Bronzebüste des schwedischen Königs Karl XII., von Jacques Philippe Bouchardon (1711–1753), ist seit 1775 im Marmorsaal nachweisbar. Friedrich II. erhielt die Büste 1755 von seiner Schwester, der schwedischen Königin Luise Ulrike, zum Geschenk.
Das östlich anschließende Audienzzimmer wurde in friderizianischer Zeit auch als Speisezimmer genutzt. In diesem Raum, der an kühlen Sommertagen beheizt werden konnte, fanden vermutlich die geselligen „Tafelrunden“ Friedrichs II. statt und nicht, wie von Adolf Menzel auf dem Gemälde „Tafelrunde von Sanssouci“ dargestellt, im Marmorsaal, der nur bei besonderen Anlässen als Esssalon diente. Zahlreiche Gemälde französischer Maler des 18. Jahrhunderts dominieren das Erscheinungsbild des Raums. Die mit violettrosafarbenem Seidendamast bespannten Wände schmücken in loser Hängung Werke von Jean-Baptiste Pater, Jean François de Troy, Pierre Jacques Cazes (1676–1754), Louis de Silvestre, Antoine Watteau und anderen. Die Supraportenreliefs mit Putten, die mit Blumen und Büchern spielen, sind Arbeiten von Friedrich Christian Glume. Das Deckengemälde über der mit Blattmotiven geschmückten Voute, „Zephir bekränzt Flora“ von Antoine Pesne, zeigt den Windgott mit der Blumengöttin.

Im Konzertzimmer wird die überschwängliche Ornamentform des Rokoko, die Rocaille, an den in Weiß und Gold gehaltenen Wänden und der Decke im Überfluss sichtbar. Die Wandgemälde von Antoine Pesne und Wandspiegel sind in die Dekoration eingepasst und werden durch die Rocaillen mit ihren typischen S-Kurven und C-Schwüngen umrahmt. Die Holzeinfassungen stammen aus der Werkstatt des Bildhauers Johann Michael Hoppenhaupt (d. Ä.). Zwei Supraportenbilder mit Landschaften, antiken Monumenten und Ruinen malte Charles Sylva Dubois, Antoine Pesne ein Landschaftsbild und die Ansicht des Schlosses Sanssouci. Das Hammerklavier von Gottfried Silbermann aus dem Jahr 1746 und das Notenpult Friedrichs II., eine Arbeit des Zierratenbildhauers Johann Melchior Kambly von 1767, weisen auf die Nutzung des Raumes hin. Adolf Menzels Gemälde „Das Flötenkonzert von Sanssouci“ gibt die festliche Atmosphäre bei königlichen Konzerten eindrucksvoll wieder.
Das Arbeits- und Schlafzimmer zeigte zur Zeit Friedrichs II. ebenso reiche, vergoldete Stuck- und Holzschnitzarbeiten wie das Konzertzimmer. Nach der Umgestaltung im klassizistischen Stil durch Friedrich Wilhelm von Erdmannsdorff blieb nur noch der Kamin an seinem Platz. Die seladongrüne Seidenbespannung der Wände mit aufliegenden vergoldeten Holzschnitzarbeiten wich einer hellgrünen Bespannung. Die ehemals stuckierte Decke bemalte der Dekorationsmaler Johann Fischer mit einer Art Velarium, um das sich Tierkreiszeichen, Opferszenen und Götterdarstellungen gruppieren und in die Zwickel Allegorien des Geschichtsruhms, des Friedens, der Kriegs- und Dichtkunst. Die ursprünglich reich ornamentierte Putten-Brüstung, die den Arbeits- vom Schlafbereich abgrenzte, wurde durch zwei, auf Postamenten ruhende ionische Säulen und zwei mit Blumen-Fruchtgehängen bemalte Pilaster ersetzt. Unter |Friedrich Wilhelm IV. kam Mitte des 19. Jahrhunderts ein Teil der friderizianischen Möbel in den Raum zurück, unter anderem 1843 der Sterbesessel Friedrichs II. Zudem ließ er die Wände mit Gemälden schmücken, die vor allem Friedrich den Großen zeigen. Die Werke schufen Antoine Pesne, Johann Georg Ziesenis, Joachim Martin Falbe, Charles-Antoine Coypel, Edward Francis Cunningham, Christian Bernhard Rode, Johann Christoph Frisch und Anton Graff.

Die Bibliothek weicht von der Raumordnung französischer Schlossbaukunst ab. Das kreisrunde Zimmer liegt fast versteckt außerhalb der Enfilade am Ende der Königswohnung und ist durch einen schmalen Gang vom Arbeits- und Schlafzimmer zu erreichen. Die Lage unterstreicht den privaten Charakter des Raums, in den sich der „Philosoph von Sanssouci“ ungestört zurückziehen konnte. Mit Zedernholz getäfelte Wände und in Wandnischen eingelassene Bücherschränke aus gleichem Holz, in die auch die Eingangstür eingebunden ist, zeigen ein geschlossenes Bild in der Wanddekoration. Die harmonische Farbgestaltung in Braun mit der goldfarbenen Ornamentik der Rocaille vermittelt eine ruhige Stimmung. Vier vergoldete Bronzereliefs über den Schränken, mit Allegorien der Künste, schuf Benjamin Giese. Nischen nehmen den Kamin und die Sitzgarnitur auf. Die Bücherschränke sind gefüllt mit circa 2100 Bänden der griechischen und römischen Dichtung und Geschichtsschreibung in französischer Übersetzung sowie französischer Literatur des 17. und 18. Jahrhunderts, deren Mittelpunkt die Werke von Voltaire bilden. Die deutsche Literatur fand bei Friedrich II. kaum Beachtung. Die Bücher sind in braunes oder rotes Ziegenleder gebunden und reich vergoldet. Der König besaß in seinen Schlossbibliotheken die jeweils gleiche Ausstattung an Werken und ließ sie ab 1771 mit goldenen Buchstaben auf dem Buchdeckel kennzeichnen.

Auch bei der im Norden liegenden Galerie wich Friedrich II. von der französischen Raumordnung des „Appartement double“ ab, nach der in diesem Bereich Kammern für die Dienerschaft vorgesehen waren. Die Wand des schmalen, langgestreckten Raums wird durch Nischen gegliedert, in denen Marmorskulpturen griechisch-römischer Gottheiten aus der Sammlung des französischen Kardinals Melchior de Polignac platziert sind. Über fünf Sofas hängen Gemälde von Nicolas Lancret, Jean-Baptiste Pater und Antoine Watteau. An der durch Fenster und Spiegel unterbrochenen Außenwand stehen zehn Marmorbüsten auf Postamenten und auf den Kaminen an den Enden der Galerie die zwei Büsten der Amphitrite und des Neptun von Lambert Sigisbert Adam. Das fünfteilige Deckengemälde über der mit Weinlaubranken dekorierten Voute stammt von Johann Gottlieb Glume und zeigt blumenstreuende Putten. Die Tempelruine auf dem ostseitigen Supraportenbild fertigte Charles Sylva Dubois und die Figurenstaffage auf der westseitigen Supraporte Antoin Pesne.

Die nach Westen an den Marmorsaal anschließenden fünf Gästezimmer haben die Fenster zur Gartenseite und die vier ersten Zimmer einen Alkoven an der gegenüberliegenden Wand. Neben dieser Bettnische führt eine Tür durch einen schmalen Gang in das im Norden angrenzende Dienerzimmer und eine weitere Tür in eine kleine Kammer, die zur Aufbewahrung der Kleidung vorgesehen war.

Die Wände des ersten Gästezimmers sind mit weiß gestrichenem Holz getäfelt, in dessen schmale Felder Friedrich Wilhelm Hoeder zartrosafarbene Ornamente und figürliche Darstellungen im chinoisen Stil malte. Der Raum erfuhr bereits 1747 eine Veränderung, als über die Vertäfelung eine blaue Satinade (halbseidener Atlas) gespannt wurde. Vermutlich führte die Verwendung von zu feuchtem Holz zur Rissbildung, die auf diese Weise verdeckt werden sollte. Nach der Entfernung 1953 hätten die bis dahin vierzehn Gemälde die Bemalung von Hoeder verdeckt, sodass nur noch je zwei Werke von Antoine Pesne und Jean-Baptiste Pater an der Alkovenwand Platz fanden.

Die Wände des zweiten und dritten Gästezimmers bekamen schon bei der Einrichtung eine textile Wandbespannung. Neben Supraportenbildern mit Stillleben von Augustin Dubuisson (1700–1771), einem Sohn von Jean Baptiste Gayot Dubuisson, hängen auf der blau-weiß gestreiften Bespannung des zweiten Zimmers Werke von Malern des 18. Jahrhunderts und auf der rot-weiß gestreiften Wandfläche des dritten Zimmers Landschaftsdarstellungen und Veduten von Giovanni Paolo Pannini, Luca Carlevaris, Michele Marieschi und anderen.

Es ist nicht genau bekannt, wer im Lauf der Jahrzehnte das Privileg erhielt, in Sanssouci leben zu dürfen. Durch die Namensgebung des vierten Raums, des „Voltairezimmers“, und des fünften, des „Rothenburgzimmers“, werden jedoch zwei Gäste mit Sanssouci in Verbindung gebracht. Es ist nicht sicher, ob Voltaire während seines Aufenthalts in Potsdam von 1750 bis 1753 im Sommerschloss gelebt hat, da er Räume im Potsdamer Stadtschloss bewohnte; auf jeden Fall war er in den drei Jahren häufiger Gast des Königs. Das „Voltairezimmer“ wird in einer Inventarliste von 1782 als „Blumenkammer“ bezeichnet und war wie das erste Gästezimmer vermutlich durch feuchtes Holz so reparaturbedürftig, dass Johann Christian Hoppenhaupt 1752/53 eine neue Holzvertäfelung fertigte. Die ursprüngliche Bemalung von Hoeder, mit graulila Ornamenten, ist heute nur noch in der Bettnische sichtbar. Hoppenhaupt schuf eine gelblackierte Eichenholzvertäfelung mit bunten, plastischen Holzschnitzereien, die Blumen, Früchte, Sträucher und Tiere darstellen. Die farbenfrohe Blumendekoration aus Stuck und Eisenblech setzt sich an der Decke fort. Eine Büste Voltaires ließ Wilhelm II. nach dem 1774 geschaffenen Modell des Porzellanmodelleurs Friedrich Elias Meyer d. Ä. 1889 kopieren und vor 1905 in den Raum stellen.

Das Pendant zur Bibliothek bildet das ebenfalls außerhalb der Enfilade gelegene kreisrunde „Rothenburgzimmer“. Es erhielt seine heute noch gültige Bezeichnung nach einem engen Vertrauten des Königs, dem Grafen Friedrich Rudolf von Rothenburg, der den Raum regelmäßig bis zu seinem Tod 1751 bewohnte. Die zartgrün gestrichene Holzvertäfelung bemalte Hoeder mit chinesischen Motiven, die der Gestaltung im ersten Gästezimmer ähneln. Die Bilder eines unbekannten Künstlers in der Bettnische zeigen Grotesken, die auf Ornamentstiche nach Antoine Watteau zurückgehen. Alle Zimmer wurden mit Kaminen ausgestattet und sind heute, bis auf das „Rothenburgzimmer“, mit Möbeln und Kunstgegenständen aus dem 18. Jahrhundert museal eingerichtet.

In friderizianischer Zeit waren im eingeschossigen Seitenflügel auf der Ostseite die Zimmer für Bedienstete und auf der Westseite die Schlossküche sowie Stallboxen für die Pferde. Durch den Neubau unter Friedrich Wilhelm IV. kam die Küche in den östlichen Flügel und die Zimmer für Bedienstete in das aufgestockte Obergeschoss. Der Westflügel nahm die Wohnräume für Hofdamen auf.

Im neu unterkellerten Küchenflügel wurden das Weinlager, ein Raum zur Eisbereitung, größere Vorratsräume, die Lampenkammer, Arbeitsräume für Kellerknechte und die Konditorei untergebracht. Die Arbeitsräume zur direkten Versorgung der Schlossbewohner lagen im Erdgeschoss. Neben der 115 m² großen Küche, die die gesamte Breite des Seitenflügels einnimmt, gab es eine Kaffeeküche für die Zubereitung des Frühstücks und kalter Speisen, eine Kaffetier-Stube, eine Backkammer, die Schreibstube des Küchenmeisters (Kaffetier), eine kleine Speisekammer und zwei Räume zur Reinigung des Tafelsilbers. Im aufgestockten Obergeschoss wohnten der Küchenmeister, der Haushofmeister und weitere Bedienstete. Da die Küche nur von 1842 bis 1873 benutzt wurde und danach keine baulichen Veränderungen stattfanden, ist das feststehende Inventar noch bis heute vorhanden. Dazu gehört eine gusseiserne „Kochmaschine“ mit Messingbeschlägen und einer umlaufenden Messingstange. Der zu seiner Zeit hochmoderne Herd ist neben Kochplatten in verschiedenen Größen mit Fächern zum Braten und Backen, einer Wasserblase und einem Wärmeschrank ausgerüstet.

Der Westflügel, auch "Damenflügel" genannt, diente der Unterbringung von Hofdamen und Gästen. Neben kleineren Kaffeeküchen und einer Stube für die Ordonnanzen sind im Erdgeschoss drei Wohnungen für Hofdamen und im Obergeschoss zwei Kavalierswohnungen und eine Damenwohnung eingerichtet worden. Jedes Appartement hat zwei Zimmer. Die Raumfolge entspricht in etwa dem „Appartement double“. Neben der Bettnische führt eine Tür über einen kurzen Gang in das angrenzende Dienerzimmer oder ins Treppenhaus und eine weitere Tür in einen kleinen Toilettenraum. Die bevorzugten Räume im Parterre, mit ihrem direkten Zugang zum Garten, ließ Friedrich Wilhelm IV. mit holzvertäfelten Wänden aufwändiger gestalten als die in der Regel tapezierten Räume im Obergeschoss. Die Kamine stammen fast alle aus friderizianischer Zeit und waren vermutlich in der um 1800 umgestalteten Westwohnung Friedrichs II. im Potsdamer Stadtschloss eingebaut gewesen. Die Zimmer wurden mit Rokokomöbeln aus friderizianischer Zeit und neu angefertigten Stücken im Stil des „zweiten Rokoko“ eingerichtet. In späteren Jahren kamen aber auch zeitgenössische Möbel hinzu.

Das „zweite Rokoko“ war ab Mitte der 1820er-Jahre und besonders in den 1840er-Jahren eine Stilrichtung der vielschichtigen Kunst des 19. Jahrhunderts. Für Friedrich Wilhelm IV. in Verbindung mit Sanssouci jedoch nicht nur eine Modeerscheinung, sondern auch eine Rückbesinnung auf die künstlerischen Werte Friedrichs II. und in dieser Konsequenz nur in Sanssouci zu finden. Bei den zahlreichen anderen Bauten, die während seiner Regierungszeit in Potsdam entstanden, bevorzugte er Stilformen der Antike, der Renaissance und des Klassizismus.

Nach der Terrassierung des Weinbergs und der Fertigstellung des Schlosses Sanssouci wurde die Umgebung in die Gestaltung einbezogen. Es entstand ein barocker Ziergarten mit Rasenstücken, Blumenrabatten, Hecken und Bäumen. In den Heckenquartieren pflanzte man 3000 Obstbäume. Orangen, Melonen, Pfirsiche und Bananen gab es in den Treibhäusern der zahlreichen Parkgärtnereien. Auf die Verbindung von Zier- und Nutzgarten weisen die Göttinnen Flora und Pomona hin, die das Obeliskportal am östlichen Parkausgang schmücken.

Durch die Ausweitung der Anlage nach dem Bau weiterer Gebäude bildete sich eine schnurgerade, rund zwei Kilometer lange Hauptallee. Diese begann im Osten an dem 1748 errichteten Obelisken und verlängerte sich im Laufe der Jahre bis zum Neuen Palais, das den Abschluss im Westen bildet. In Höhe der 1764 errichteten Bildergalerie und der 1774 errichteten Neuen Kammern, die das Schloss flankieren, öffnet sich die Allee zu Rondellen mit Fontänenbecken, die von Marmorplastiken umsäumt werden. Von diesen Punkten zweigen zwischen hochgewachsenen Hecken Wege sternförmig in weitere Gartenbereiche ab.

Bei der Gestaltung der Parkanlage führte Friedrich der Große fort, was er schon in Neuruppin und Rheinsberg begonnen hatte. Schon während seines Aufenthalts in Neuruppin, wo er in seiner Kronprinzenzeit von 1732 bis 1735 Befehlshaber eines Regiments war, ließ er an seinem Wohnsitz einen Zier- und Nutzgarten anlegen. Bereits hier wich er von der klassischen Gestaltung der rein auf Repräsentation bedachten barocken Gartenanlagen nach dem Vorbild von Versailles ab, indem er das Schöne mit dem Nützlichen verband. Diesem Prinzip folgte er auch in Rheinsberg. Bei der Umgestaltung des Schlosses, das Friedrich II. 1734 von seinem Vater, dem Soldatenkönig Friedrich Wilhelm I., geschenkt bekam, ließ er von Hecken eingefasste Obst- und Gemüsequartiere anlegen. Die Hauptachse ist hier nicht mehr auf das Schloss gerichtet, wie es in Parkanlagen französischer Prägung üblich war, sondern verläuft in west-östlicher parallel zum Gebäude.

Friedrich der Große investierte viel Geld in das Fontänensystem des Parks, da Wasserspiele ein fester Bestandteil barocker Gärten waren. In Sanssouci scheiterte das Projekt jedoch an der mangelnden Fachkenntnis der Baumeister, sodass es nicht gelang, Wasser aus einem Hochbecken auf dem Ruinenberg hinunter in den Park zu leiten. Die 1757 fertiggestellte Neptungrotte im östlichen Parkteil kam deshalb ebenso wenig zu ihrer vorgesehenen Funktion wie die Fontänenanlagen oder die 1751 bis 1762 nach Plänen von Georg Wenzeslaus von Knobelsdorff errichtete Marmorkolonnade, die sich im westlichen Abschnitt der Hauptallee, innerhalb des Rehgartens, befand. Das durch seinen ehemaligen Standort auch „Rehgartenkolonnade“ genannte Wasserspiel ist heute nicht mehr erhalten, da es schon 1797 wegen Baufälligkeit abgebrochen werden musste.

Erst einhundert Jahre später gelang das Vorhaben mit Hilfe der Dampfkraft, und der Zweck des Wasserreservoirs wurde erfüllt. Im Oktober 1842 ging eine von August Borsig erbaute 81,4 PS starke Dampfmaschine in Betrieb und ließ den Wasserstrahl der „Großen Fontäne“ unterhalb der Weinbergterrassen auf 38 Meter steigen. Eigens für diese Maschine wurde an der Havelbucht eine Pumpstation gebaut, die, wie Persius in sein Tagebuch schrieb, "nach Art der türkischen Moscheen mit einem Minarett als Schornst.[ein]" von Friedrich Wilhelm IV. in Auftrag gegeben und zwischen 1841 und 1843 von Persius errichtet wurde.

Bereits Jahre zuvor erwarb Friedrich Wilhelm III. ein Areal, das südlich an den Park Sanssouci grenzte, und schenkte es Weihnachten 1825 seinem Sohn Kronprinz Friedrich Wilhelm (IV.). Auf der Stelle eines ehemaligen Gutshauses errichteten Karl Friedrich Schinkel und Ludwig Persius das Schloss Charlottenhof. Mit der Gestaltung des umgebenden Geländes wurde Peter Joseph Lenné beauftragt. Unter Berücksichtigung des barocken Zier- und Nutzgartens aus friderizianischer Zeit verwandelte der Gartenarchitekt das flache, stellenweise sumpfige Gelände in einen offenen Landschaftspark. Durch weite Wiesenflächen entstanden Sichtachsen zwischen Schloss Charlottenhof, den Römischen Bädern und dem Neuen Palais mit dem Freundschaftstempel aus der Zeit Friedrichs des Großen. Locker gesetzte Strauch- und Baumgruppen beleben die große Parkfläche, an deren südöstlichem Ende ein Wassergraben zu einem Teich erweitert wurde. Den Erdaushub nutzte Lenné für die Gestaltung eines sanft hügeligen Geländes, auf dessen obersten Punkten die Spazierwege sternförmig zusammentreffen. Dieser südliche Teil ist auch als "Park Charlottenhof" bekannt.

Friedrich II. und Friedrich Wilhelm IV. prägten im 18. und 19. Jahrhundert die Anlage im jeweils zeitgenössischen Stil und schufen unter eigener künstlerischer Mitwirkung durch ihre Architekten, Bildhauer, Maler, Dekorateure und Gartengestalter ein Gesamtkunstwerk von Architektur und Gartengestaltung, dessen Herzstück die Weinbergterrassen mit dem bekrönenden Schloss sind. Die historische Parkanlage Sanssouci mit einer Fläche von etwa 290 ha und fast 70 Kilometer Wegelänge ist die größte in der Mark Brandenburg.

Im Park und auf dem angrenzenden Klausberg entstanden neben dem Schloss Sanssouci noch weitere Gebäude und Gartenarchitekturen unter Friedrich II., die auch heute noch erhalten sind:

Friedrich Wilhelm IV. ließ den Park Sanssouci mit weiteren Gebäuden ergänzen:


Der Park stand allen Besuchern offen, ebenso die Bildergalerie, die man unter Führung des Aufsehers besichtigen konnte. Aber auch das Neue Palais und sogar Schloss Sanssouci selbst wurden Besuchern zugänglich gemacht, wenn der König gerade nicht anwesend war.

Der französische General Graf Guibert schrieb über Sanssouci zur Zeit Friedrichs:

Marschall Francisco de Miranda schrieb 1785:

Und Goethe schrieb nach einem Besuch 1778:




</doc>

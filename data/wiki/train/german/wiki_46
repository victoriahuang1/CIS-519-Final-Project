<doc id="5210" url="https://de.wikipedia.org/wiki?curid=5210" title="Technik">
Technik

Das Wort Technik stammt von und leitet sich ab von "téchne", zu Deutsch etwa Kunst, Handwerk, Kunstfertigkeit. „Technik“ kann bedeuten:

Es gibt Versuche, diese verschiedenen Bedeutungen auf einen gemeinsamen Grundbegriff zurückzuführen. Doch scheinen die Technikbegriffe zu unterschiedlich, als dass man sie ohne Weiteres vereinheitlichen könnte. Im Folgenden werden weitläufige Wortverwendungen vor allem nach (2), (3) und (4) beiseitegelassen. „Technik“ wird als wohlbestimmter Ausdruck der Technikforschung und -lehre betrachtet, der die Bedeutung (1) als notwendiges, wenn auch nicht hinreichendes Bestimmungsmerkmal enthält.

Im Sinne der VDI-Richtlinie 3780 umfasst Technik:

Diese Begriffsbestimmung enthält keine Spekulationen über das „Wesen der Technik“, sondern beschreibt lediglich, welche Erscheinungen unter dem Namen „Technik“ zusammenzufassen sind. „Technik“ bezeichnet also zunächst die von Menschen gemachten Gegenstände, aber auch die Entstehung und Verwendung der technischen Sachen und das dafür erforderliche Können und Wissen. Technik ist in diesem Verständnis kein isolierter, selbständiger Bereich, sondern auf das Engste mit Wirtschaft, Gesellschaft, Politik und Kultur verflochten.

Fließende Übergänge gibt es erstens zu Gegenständen der bildenden Kunst (Architektur, Kunsthandwerk, Industriedesign) und zweitens zu natürlichen Erscheinungen und Lebewesen, soweit diese durch menschlichen Eingriff mehr oder weniger stark verändert werden (Kulturlandschaften, Gärten, Zuchtpflanzen und Zuchttiere, heute zunehmend auch gentechnische Hervorbringungen, die teils schon den Charakter von Artefakten annehmen). Die gelegentlich als neuartige, „abstrakte“ oder „transklassische“ Technik aufgefasste Programmierung elektronischer Datenverarbeitungsgeräte lässt sich der Teilmenge (3) der modernen Technikdefinition zuordnen, da sie eine besondere Fertigkeit für die Verwendung der Computer darstellt. Oft wird heute gleichbedeutend mit „Technik“ der Ausdruck „Technologie“ verwendet (z. B. Raumfahrttechnologie). Aus fachgeschichtlichen und sprachlogischen Gründen meinen manche Technikforscher, dieser Ausdruck sollte der Wissenschaft von der Technik vorbehalten bleiben (Allgemeine Technologie).

In den Technikwissenschaften – dieser Ausdruck löst allmählich den Namen „Ingenieurwissenschaften“ ab – ist neuerdings mit der Modellvorstellung des technischen (Sach-)Systems ein allgemeiner Begriff für beliebige technische Hervorbringungen entstanden, der an die Stelle der uneinheitlich gebrauchten und schlecht abgrenzbaren Ausdrücke „Maschine“, „Gerät“, „Apparat“ u. ä. tritt. Ein technisches System „ist durch die Funktion gekennzeichnet, Stoff (Masse), Energie und/oder Information zu wandeln, zu transportieren und/oder zu speichern. Es ist stofflich-konkret und besteht aus Werkstoffen mit definierten Eigenschaften, die aus Systemen der (physikalischen, chemischen, biologischen) Verfahrenstechnik hervorgehen. Es ist ein räumliches Gebilde mit geometrisch definierter Gestalt und setzt sich aus Bauteilen“ mit geometrisch definierter Gestalt „zusammen. Die Gestaltgebung erfolgt in Systemen der Fertigungstechnik“. Gegenständlich verwirklicht wird alle Sachtechnik also in den technischen Systemen der Stoffwandlung. Immer häufiger gründen neue Technologien auf kreativen Kombinationen von bereits bekannten („combinatorial evolution“), wobei die je verfolgten Zwecke in unterschiedlichen Sets von Komponenten ausgedrückt werden können.

Die Funktionen der technischen Systeme und die Teilfunktionen ihrer Subsysteme werden durch naturale Wirkungszusammenhänge realisiert, die (bekannten oder noch nicht bekannten) Naturgesetzen unterliegen. W. Brian Arthur definiert als Quintessenz von Technik die Fähigkeit des „capturing phenomena“, d. h. die Kapselung von zuverlässig beherrschten kausalen Wirkungsmechanismen. Noch pointierter drückt es Luhmann aus: Technik sei „funktionierende Simplifikation im Medium der Kausalität“

Dieser Umstand hat zu der Vorstellung verleitet, Technik wäre gleichbedeutend mit angewandter Naturwissenschaft. Damit aber wird die Bedeutung naturwissenschaftlichen Wissens für die Technik, vor allem hinsichtlich früherer Entwicklungsstadien, stark überschätzt. Auch bei zunehmender Verwissenschaftlichung der modernen Technik unterscheiden sich die Wissensformen der Technikwissenschaften und der technischen Praxis von den Naturwissenschaften derart, dass nicht ohne Weiteres von einer einfachen Anwendung die Rede sein kann. Umgekehrt ist naturwissenschaftliche Forschung häufig auch angewandte Technik, insoweit sie ihre Gegenstände nur mit erheblichem apparativem Aufwand darstellen und untersuchen kann. Naturwissenschaft und Technik sind verschiedenartige, relativ selbständige Bereiche, die einander nur teilweise überschneiden.

Technische Systeme gehen bei aller Künstlichkeit moderner Werkstoffe letztlich auf Naturstoffe zurück, sie setzen bei ihrer Verwendung Stoff und Energie um, und am Ende ihrer Lebensdauer werden sie selbst zu Abfall. So bringen sie grundsätzlich Eingriffe in das natürliche Ökosystem mit sich, die allerdings in der Vergangenheit häufig zu wenig beachtet wurden. Erst mit dem gewaltigen Anstieg der Umweltbelastungen verbreitet sich in den Ingenieurwissenschaften und in der technischen Praxis die Einsicht, dass auch die naturwissenschaftlichen Erkenntnisse der Ökologie bei technischen Systemen zu berücksichtigen sind, damit der Verbrauch an natürlichen Ressourcen und die schädlichen Emissionen und Deponate zugunsten des Umweltschutzes begrenzt werden.

Herkömmlicherweise wird die Technik nach ingenieurwissenschaftlichen Fachgebieten oder nach Industriebranchen eingeteilt (Bergbau- und Hüttentechnik, Bautechnik, Maschinenbau, Fahrzeugtechnik, Feinwerktechnik, Chemietechnik, Elektrotechnik usw.). Die jeweilige Eigenart der eingesetzten und hervorgebrachten Technik lässt sich damit aber nur sehr unzureichend kennzeichnen; z. B. werden im Maschinenbau energietechnische, produktionstechnische sowie förder- und verkehrstechnische Systeme hergestellt.

Die Beschreibungsmerkmale des technischen Systems erlauben nun eine stimmige Klassifikation nach der Art der Funktion (Wandlung, Transport, Speicherung etc.) und nach der Kategorie der Objekte (Stoff bzw. Material, Energie, Information etc.). Verbindet man diese beiden Einteilungen, ergeben sich neun Technikfelder:

Das Schema dieser Einteilung ist weithin anerkannt. Die Terminologie in den Technikwissenschaften ist aber nach wie vor sehr uneinheitlich, und so variieren die Bezeichnungen. Manchmal verwendet man die herkömmlichen Ausdrücke, die teilweise zur Erläuterung in Klammern angegeben wurden. Oft werden die Teilbereiche (4) bis (6) ungegliedert als „Energietechnik“, die Teilbereiche (7) bis (9) zusammenfassend als „Informationstechnik“ bezeichnet. Die neun Technikfelder sind ihrerseits noch weiter zu untergliedern. So kann man die Energiewandlungstechnik nach der Art der Energie-Inputs und -Outputs einteilen. Oder man klassifiziert die Informationsspeichertechnik nach dem physikalischen Prinzip des Speichermediums (Buch, Schallplatte, Film, Magnetband, Magnettonplatte, Speicherchip u. a.).

Dass Technik nicht in angewandter Naturwissenschaft aufgeht, wird vollends klar, wenn man ihre Verwendungszusammenhänge in den Blick nimmt. Technische Systeme verwirklichen ihre Funktionen grundsätzlich nur im Rahmen gesellschaftlich geprägten menschlichen Handelns, technische Systeme sind immer Teile soziotechnischer Systeme, und sie verkörpern menschliche Zwecksetzungen, Handlungsmuster und Arbeitsprozesse. Entweder ersetzen sie menschliche Handlungs- und Arbeitsfunktionen (Substitution), z. B. der Buchdruck, der die manuelle Vervielfältigung von Schriften erübrigt, oder sie fügen den menschlichen Handlungssystemen neue, nur technisch darstellbare Teilfunktionen hinzu, die Menschen mit ihrer organischen Ausstattung gar nicht leisten könnten (Komplementation), z. B. das Flugzeug, das dem flügellosen Menschen das Fliegen ermöglicht.

Neben die gesellschaftliche Arbeitsteilung (sozioökonomische Produktionsteilung, Berufsdifferenzierung, betriebliche Arbeitszerlegung) tritt im Zuge der Technisierung die soziotechnische Arbeitsteilung, die Aufteilung von Handlungs- und Arbeitsfunktionen zwischen Menschen und technischen Systemen. Im Verlauf der Technikgeschichte wurden immer mehr Handlungs- und Arbeitsfunktionen mit technischen Systemen realisiert. Ein deutliches Muster zeigt sich besonders bei der Substitution: Erst ersetzen Werkzeuge die reine Handarbeit, dann ersetzen Antriebssysteme die Muskelkraft, später ersetzen Steuerungssysteme die menschliche Koordination von Sinneswahrnehmung und Arbeitsbewegung, und inzwischen ersetzen Computer auch einfache geistige Leistungen. In manchen Produktions- und Verwaltungsabläufen hat die soziotechnische Arbeitsteilung das Stadium der Automatisierung erreicht, wobei Menschen weder ständig noch in einem erzwungenen Rhythmus für die Arbeitsabläufe tätig werden müssen. Ob freilich die „menschenleere Fabrik“ möglich und sinnvoll sein wird, ist keineswegs unumstritten, und auch in der Informationsverarbeitungstechnik ist die Frage offen, bis zu welchem Grade die „künstliche Intelligenz“ der Computer die Menschen wirklich ersetzen kann.

Wie jede Arbeitsteilung ist auch die soziotechnische Arbeitsteilung auf ergänzende Arbeitsverbindung angewiesen. Menschliche und technische Komponenten im soziotechnischen System werden aufeinander abgestimmt und beeinflussen einander wechselseitig. Die Verwendung von technischen Systemen ist an bestimmte Bedingungen geknüpft (z. B. Bedienbarkeit und Bedienungskompetenz, Beherrschbarkeit und Zuverlässigkeit, Ver- und Entsorgungssysteme usw.) und hat bestimmte Folgen (z. B. Veränderung der Bedürfnisse und der psycho-physischen Funktionen des Menschen, Prägung von Handlungsmustern und Sozialbeziehungen usw.).

Ursprünglich haben Arbeitswissenschaft und Industriesoziologie solche Wechselbeziehungen zwischen Mensch und Technik lediglich für die Industriearbeit untersucht. In den letzten Jahrzehnten ist aber die Technisierung verstärkt auch in die alltägliche Lebensführung und in die privaten Haushalte eingedrungen (Pkw, Haustechnik, Telefon, Rundfunk und Fernsehen, Foto-, Video- und Computertechnik), sodass die psychosozialen Folgen der Technikverwendung beträchtliche Dimensionen annehmen, die bislang nur unzulänglich erforscht wurden.

Bekannt sind einige allgemeine Entwicklungstendenzen der Gesellschaft, die mit dem Technikeinsatz zusammenhängen. Dazu gehören die anfängliche Zentralisierung und Bevölkerungskonzentration in den Stadt- und Industrierevieren, aber auch die inzwischen durch Verkehrs- und Kommunikationstechnik möglich gewordene neuerliche Dezentralisierung. In der Beschäftigungsstruktur hatte sich der Schwerpunkt zunächst von der Landwirtschaft auf den industriellen Sektor verlagert und verschiebt sich heute zunehmend auf den Dienstleistungssektor. Traditionelle Berufe haben an Bedeutung verloren, und zahlreiche neue Berufe sind entstanden. Darauf reagiert zunächst die Berufsausbildung, allmählich aber auch das allgemeine Bildungssystem. Der nicht zuletzt aufgrund der Technisierung gewachsene Anteil an freier Zeit wird häufig der Technikverwendung, vor allem technischen Hobbyaktivitäten, dem Auto und dem Fernsehkonsum gewidmet.

Bei der Entstehung neuer technischer Systeme unterscheidet man verschiedene Phasen:

Von der naturwissenschaftlichen Erkenntnis unterscheidet sich die Erfindung vor allem dadurch, dass sie zugleich mit der technischen Lösungsidee eine Nutzungsmöglichkeit, also eine technisierbare Handlungs- oder Arbeitsfunktion angibt, der die Lösung dienen soll. Da die Erfindung immer einen möglichen Zweck vorwegnimmt, ist Technik grundsätzlich nicht zweckneutral. Auf welche Weise in der Erfindung die Vorstellung von neuartiger Wirklichkeit hervorgebracht wird, kann man bislang nur unzulänglich beschreiben. Erfahrung und Wissen gehören in aller Regel dazu, doch die eigentliche Kreativität, die Fähigkeit, etwas Neues zu konzipieren, das zuvor völlig unbekannt war, bleibt auch dann schwer nachvollziehbar, wenn man sie teils mit intuitiv-unbewussten Assoziationsvorgängen und teils mit systematisch-rationaler Kombinationsarbeit erklärt. Ist eine Erfindung tatsächlich neuartig, brauchbar und dem bekannten Stand der Technik deutlich überlegen, kann darauf ein Patent erteilt werden, das dem Erfinder die Verwertungsrechte sichert.

Ob allerdings eine Erfindung zur Innovation wird, darüber entscheiden, wenn nicht militärische oder andere staatliche Interessen im Spiel sind, vor allem wirtschaftliche Gesichtspunkte. Die anfängliche Lösungsidee muss durch Konstruktionsarbeit in allen Einzelheiten festgelegt werden, in einem Prototyp erprobt und gegebenenfalls verbessert werden. Schließlich sind die Fertigungsanlagen bereitzustellen oder überhaupt erst zu schaffen, und der Markt muss für das neue Produkt erschlossen werden. Diese technischen und unternehmerischen Aktivitäten erfordern beträchtliche finanzielle Vorleistungen, die nur dann aufgebracht werden, wenn die Innovation eine entsprechende Nachfrage auf dem Markt und damit hinreichenden Gewinn verspricht. So wird technische Entwicklung, abgesehen von politischen Impulsen und rechtlichen Regelungen, vor allem wirtschaftlich gesteuert.

Die einzelnen Innovationen verknüpfen sich in ihrer Gesamtheit zu einem Prozess, den man bis vor Kurzem als technischen Fortschritt bezeichnete. Da inzwischen zweifelhaft geworden ist, ob alle technischen Neuerungen immer auch einen wirklichen Fortschritt für die Menschen bedeuten, spricht man heute eher von technischer Entwicklung, technischem Wandel oder von Technikgenese. Dieser Prozess wird von interdisziplinärer Technikforschung zunehmend untersucht, ist aber bisher nur unzureichend erklärt worden. Bis zum letzten Drittel des 20. Jahrhunderts überwog eine Vorstellung, die heute als „technologischer Determinismus“ kritisiert wird, die Annahme nämlich, der technische Wandel folge einer selbständigen Eigengesetzlichkeit. Inzwischen wird die technische Entwicklung als gesellschaftlicher Prozess verstanden, in dem naturale und technische Gegebenheiten, wissenschaftliche Erkenntnisse, technische Erfindungen, menschliche Bedürfnisse, konkurrierende wirtschaftliche Interessen, politische Interventionen und soziokulturelle Orientierungsmuster auf eine bislang kontrovers diskutierte Weise zusammenwirken.

Im Zusammenhang mit dem „technologischen Determinismus“ steht auch die Zweiteilung in „primitive“ und „fortschrittliche“ Technologie. Dies suggeriert eine Übertragung auf die angebliche "„Kulturhöhe“" einer Gesellschaft. Dabei wird jedoch übersehen, dass jegliche Technik primär den Zweck erfüllt, das Überleben zu sichern. Insofern kann auch der Einsatz einer sogenannten „primitiven“ Technologie bei Kulturen, die sehr naturangepasst leben, diesen Zweck ausreichend erfüllen. Der Unterschied liegt in erster Linie beim Grad des benötigten Energieeinsatzes.

Das Attribut „technisch“ bezeichnet eine Qualität von Substanzen (Gase, Lösungsmittel), die sich zur industriellen Anwendung eignen. Für medizinische oder lebensmitteltechnische Anwendungen sind die Substanzen nicht geeignet, weil sie die Qualitätsnormen nicht einhalten.

Ausgelöst durch die Erfahrung zunehmender Umweltschäden, steigender Risiken und wachsender Belastungen der psychosozialen Lebensqualität aufgrund forcierter Innovationsdynamik ist im letzten Drittel des 20. Jahrhunderts eine normative Wende im Technikverständnis eingetreten. Neue Technik wird nicht pauschal als Fluch verdammt, aber auch nicht mehr vorbehaltlos als reiner Segen gefeiert. An technische Neuerungen wird der Anspruch gestellt, dass sie über Funktionsfähigkeit und Wirtschaftlichkeit hinaus übergreifenden Werten und der Lebensqualität Rechnung tragen. Anstelle besonders risikoträchtiger Innovationen werden zunehmend alternative Lösungswege gefordert, die technisch fast immer möglich wären. Mit Programmen einer Ethik der Technik und einer gesellschaftlichen Technikbewertung versucht man auf die technische Entwicklung derart einzuwirken, dass technische Neuerungen von vornherein in Bezug auf Umwelt- und Gesellschaftsqualität optimal gestaltet werden, siehe beispielsweise.

Die Einsicht in die Gestaltungsoffenheit der technischen Entwicklung, die eher durch sozioökonomische als durch technische Faktoren begrenzt wird, relativiert auch manche Deutungen der Technikphilosophie. Wenn man Technik als Fortsetzung des göttlichen Schöpfungsplanes (Friedrich Dessauer), als übermächtiges Seinsgeschick (Martin Heidegger) oder als Fortsetzung der natürlichen Evolution (Hans Sachsse) begreift, verkennt man, dass die konkrete Phantasie der Menschen die in der Natur angelegten Potentiale gemäß den herrschenden Zweckvorstellungen sehr verschiedenartig ausschöpfen kann (Ernst Bloch). Ganz gleich, ob man die Technik als biologisch notwendige Überlebensstrategie des menschlichen „Mängelwesens“ (Arnold Gehlen) oder als den objektiv überflüssigen Luxus des menschlichen Kulturwesens (José Ortega y Gasset) versteht, wird man doch jeweils im Einzelfall prüfen müssen, welche konkreten Arten von Technik unverzichtbar sind und welche man entbehren könnte.

In ihrer Grundtendenz, menschliche Lebenserhaltung und Lebensentfaltung zu erleichtern, folgt die Technik dem Prinzip der Zweckrationalität (Friedrich von Gottl-Ottlilienfeld), die sich freilich manchmal als ökonomische Rationalität verselbständigt und die Komplexität der Folgen vernachlässigt. Dass Technik auch als Ausfluss eines elementaren menschlichen Gestaltungswillens gedeutet werden kann, als Vergegenständlichung des Subjekts in den Produkten der eigenen Arbeit (Karl Marx), als Vehikel innerweltlich-heilsgeschichtlicher Selbsterlösung (Donald Brinkmann) oder als Medium des „Willens zur Macht“ (Friedrich Nietzsche, Oswald Spengler), verweist auf irrationale Tiefenstrukturen, die von technologischer Aufklärung berücksichtigt und bewältigt werden müssen.






</doc>
<doc id="5211" url="https://de.wikipedia.org/wiki?curid=5211" title="Tetris">
Tetris

Tetris () ist ein puzzleartiges Computerspiel des russischen Programmierers Alexei Paschitnow, der die erste spielbare Version am 6. Juni 1984 auf einem Elektronika-60-Rechner fertigstellte. Tetris gilt heute als Computerspiel-Klassiker, hat sich inzwischen über 100 Millionen Mal verkauft, wurde vielfach ausgezeichnet und ist für mehr als 65 Computerplattformen erschienen.

Das Tetris-Spielprinzip lehnt sich an das Puzzlespiel Pentomino an; im Unterschied zu diesem besitzt Tetris jedoch nur sieben statt zwölf Formen. Diese stets aus vier Quadraten zusammengesetzte Formen werden häufig mit den lateinischen Buchstaben bezeichnet, denen sie ähneln. Während I, O und T symmetrisch sind, gibt es bei den Formen Z und L zwei spiegelbildliche Varianten (J/L und S/Z), woraus sich die abgebildete Gesamtzahl von sieben Formen in Tetris ergibt. Abgeleitet von dem mathematischen Begriff Tetromino (aus vier Quadraten bestehende Form) existiert die Bezeichnung "Tetrimino" speziell für die Tetris-Spielsteine.

Vom Spieler müssen die einzeln vom oberen Rand des rechteckigen Spielfelds herunterfallenden Tetrominos in 90-Grad-Schritten gedreht und verschoben werden, so dass sie am unteren Rand horizontale, möglichst lückenlose Reihen bilden. Sobald eine Reihe komplett ist, verschwindet diese. Alle darüberliegenden Reihen rücken nach unten und geben damit einen Teil des Spielfeldes wieder frei. Für das gleichzeitige Tilgen mehrerer Reihen erhält der Spieler eine höhere Punktzahl pro Reihe als für eine einzelne. Der Name des Spiels rührt von dem altgriechischen Wort "tetra" (dt. „vier“) und bezeichnet die Zahl der Quadrate pro Tetromino. So wird das gleichzeitige Tilgen von vier Reihen auch umgangssprachlich als "Tetris" bezeichnet.

In der ursprünglichen Form des Spiels verhalten sich die Spielsteine physikalisch nicht korrekt. Die Tetrominos bleiben in der Position liegen, in der sie landen, statt gemäß Gravitation zu kippen. Die nachrückenden Reihen füllen in vielen Versionen keine vorher vorhandene Lücken auf. Auf diese Weise können Tetrominos das Vervollständigen darunterliegender Reihen erschweren. Wenn eine bestimmte Anzahl entfernter Reihen erreicht worden ist, wird die Fallgeschwindigkeit der Tetrominos erhöht. Das Spiel endet, sobald sich die nicht abgebauten Reihen, also jene mit Lücken, bis zum oberen Spielfeldrand aufgetürmt haben.

Der Russe Alexei Paschitnow, damals beim "Dorodnitsyn Computer Zentrum" der Sowjetischen Akademie der Wissenschaften in Moskau angestellt, kam im Frühling 1984 auf die Idee, das Puzzlespiel Pentomino, das er aus seiner Kindheit kannte und sehr mochte, in ein Computerspiel umzusetzen. Die erste Version, von Paschitnow auf seiner Elektronika 60 entwickelt und noch ohne Sound und Farbe, war bald fertig und zog nach und nach die ganze Belegschaft in ihren Bann. Paschitnow gab dem Spiel den Namen "Tetris", als Kombination aus "Tetromino" und "Tennis". Vadim Gerasimov übertrug das Spiel kurze Zeit später auf einen IBM-PC. Im Sommer 1985 entstand die erste Farbversion, die Paschitnow dann auch nach außen weitergab. Das Spiel wurde immer weiter kopiert und breitete sich schnell in der Sowjetunion und danach im ganzen Ostblock aus.

Ungarn war zu der Zeit recht erfolgreich mit dem Export von Puzzlespielen und Computertechnologie, so dass dort der Geschäftsmann Robert Stein von Andromeda Software erstmals auf Tetris aufmerksam wurde. Seine Anfrage beim Computerzentrum wurde recht emotionslos aufgenommen, da man dort Tetris wenig ernst nahm. Steins Angebot beantwortete man aber mit der Bekundung von Interesse. Da formlose Vereinbarungen in der Softwareindustrie üblich waren, sah dieser die Antwort als Zusage an. Er nahm Kontakt mit Mirrorsoft auf, dem Softwareflügel der Maxwell Corporation, die 1986 zu den großen britischen Medienimperien gehörte. Auch bei Mirrorsoft fand das Spiel großen Anklang. Stein gelang zudem der Verkauf an Spectrum HoloByte in Kalifornien.

Als aber die erste Version auf dem Markt erschien, erhielt Stein ein Telex von Electronorgtechnica ("Elorg"), einer in Moskau ansässigen Staatsfirma, die für die zentrale Vermarktung von sowjetischen Softwareprodukten zuständig war. Diese erklärte ihm, dass die Rechte nie vergeben worden waren. Stein gelang es aber bei einem Besuch in Moskau, die Rechte für Personal- und Heimcomputer zu erhalten.

Der große Spielemarkt lag damals aber bei den Spielkonsolen, sodass Mirrorsoft mit Atari Games Kontakt aufnahm, eine Version für diese zu produzieren. Allerdings war man sich nicht bewusst, dass man die Rechte für diese Gerätegattung gar nicht besaß.

Henk Rogers vom japanischen Publisher Bullet Proof Software entdeckte das Spiel 1987 auf der Unterhaltungselektronik-Messe CES am Stand von Spectrum Holobyte und bemühte sich um die Verwertungsrechte für den japanischen Markt. Die Lizenz für Computer-Umsetzungen erhielt er direkt von Spectrum Holobyte. Die Rechte für TV-Spielkonsolen erhielt Rogers von Atari Games und veröffentlichte auf dieser Grundlage 1988 in Japan eine Version für die Famicom-Konsole.

Bei Nintendo wollte man Tetris bei der Markteinführung dem neu entwickelten Game Boy beilegen. Henk Rogers sollte daher Nintendo schnell die Rechte sichern. Robert Stein hatte diese schon mit Mirrorsoft weitgehend abgeklärt, wollte Rogers aber als potenziellen Abnehmer für andere Software bei der Stange halten.

Obwohl sich das Spiel erfolgreich verkaufte, blieben die Lizenzzahlungen in der Verwertungskette hängen, so dass Elorg nie die vereinbarten Zahlungen von Andromeda Software erhielt. Nikolai Belikow untersuchte bei Elorg im Auftrag der sowjetischen Regierung den Fall. Die Angelegenheit wurde zunehmend ein Politikum, da der Chef der Maxwell Corporation, Robert Maxwell, einer der reichsten Männer der Welt war und gute Kontakte zu Gorbatschow hatte.

Da nicht gezahlt wurde, blieben die weiteren Verhandlungen stecken. Rogers fühlte sich von Stein hingehalten und flog selbst nach Moskau. Auch Stein war in Moskau, und Mirrorsoft hatte heimlich Kevin Maxwell, den Sohn von Robert Maxwell, geschickt, um sich die Rechte zu sichern. Während die beiden Termine bei Elorg hatten, kam Rogers unvorbereitet und unerwartet nach Moskau. So ergab es sich, dass alle drei am selben Tag bei Elorg erschienen, wobei man allerdings vermied, dass sie sich begegneten. Rogers war der erste und zeigte Belikow die japanische Tetris-Version, die bei Atari Games und damit indirekt bei Robert Stein lizenziert war. Es stellte sich heraus, dass die Rechte dafür gar nicht an Stein vergeben worden waren. Rogers war schockiert, offerierte aber seine guten Kontakte mit Nintendo. Stein war der zweite Besucher des Tages und hatte gehofft, er könnte sich weitere Rechte an Tetris sichern, sah sich dann aber wegen der Lizenzüberschreitungen und ausbleibenden Zahlungen nur mit Vorwürfen konfrontiert. Maxwell erschien als dritter und brach die Verhandlungen schnell ab, da ihm die Lizenzverstöße bis dahin unbekannt waren und er zuerst Rücksprache halten musste.

Belikow gelang es durch eine geschickte Verhandlungstaktik, die Rechte für Stein präzise auf bestimmte Geräte einzugrenzen, ohne dass sich dieser dessen bewusst war. Letztendlich legte der neue Vertrag weitgehend die sowieso schon abgeschlossenen Rechte fest. Rogers hatte sich derweil mit Paschitnow angefreundet. Da Maxwell zudem arrogant auftrat, entschied Elorg sich, die Handheld-Rechte an Rogers zu vergeben. Bei Nintendo wollte man sich auch die Konsolenrechte sichern und flog heimlich nach Moskau und schloss dort einen Vertrag mit Elorg ab, der einen Betrag von 500.000 US-Dollar zuzüglich 0,50 US-Dollar pro Kopie festlegte.

Als man sich bei Maxwell bewusst wurde, dass man die Rechte nicht erhalten hatte, war man wütend und drohte am 23. März 1989 in einem Telex an Belikow, bei Gorbatschow persönlich bei dessen Staatsbesuch in Großbritannien Druck auszuüben. Belikow wurde auch von sowjetischer Seite unter Druck gesetzt, aber da die Situation sich durch die Perestroika verändert hatte, waren die Auswirkungen für ihn nicht so fatal, wie sie noch wenige Jahre zuvor hätten ausfallen können.

Für Atari Games entwickelte sich die Angelegenheit zu einem Desaster und wurde so zu einem der Gründe für den Niedergang der Firma. Man hatte schon Millionen in Entwicklung und Vermarktung investiert, als Nintendo die Firma davon in Kenntnis setzte, dass sie die Videospielrechte nicht hätten. Atari Games' Konsolenspieltochter Tengen brachte das Produkt im Mai 1989 trotzdem auf den Markt und Atari Games strengte einen Prozess gegen Nintendo an, bei dem Belikow als Zeuge aussagen sollte. Dieser wurde vor seinem Abflug vor einen staatlichen Ausschuss zitiert, wo man ihm drohte, ihn für alle Verluste verantwortlich zu machen, die der Sowjetunion entstehen würden, wenn Atari Games den Prozess gewinnen würde. Im November 1989 fiel die Entscheidung zugunsten Nintendos. Atari Games musste Hunderttausende von Spielemodulen in ihrem Lager vernichten.

Für Nintendo wurde Tetris ein großer Erfolg. Sie verkauften acht Millionen Kopien für das Nintendo Entertainment System. Der Game Boy wurde auch durch das im Lieferumfang enthaltene Tetris zu einem Erfolg und verkaufte sich insgesamt 70 Millionen mal.

Paschitnow erhielt allerdings nichts aus den gezahlten Geldern und entschied sich 1991, in die USA auszuwandern und in Seattle eine Firma aufzubauen. Zwischen 1996 und 2005 arbeitete er für Microsoft. Die ursprünglich vergebenen Rechte liefen ebenfalls 1996 aus, so dass er von da an auch Geld für das von ihm entwickelte Spiel erhielt. Allerdings machten die Gewinne zu diesem Zeitpunkt nur noch einen Bruchteil der in den vorangegangenen Jahren gezahlten Summen aus.

Tetris ist wie kaum ein anderes Spiel kopiert und nachgeahmt (Software-Clone) worden und in vielen Versionen und Variationen für nahezu jedes System erschienen. Schon bald erschienen Ende der 1980er Jahre Tetris-Spiele von Herstellern wie Atari, THQ und Nintendo für verschiedene Heimcomputer und stationäre Arcade-Systeme.

Die monochrome Version von 1989 für die tragbare Spielekonsole Game Boy von Nintendo wurde sehr populär und erfolgreich, da die ersten Game Boys zusammen mit einem Tetris-Modul ausgeliefert wurden. Die Game-Boy-Variante von Tetris kann mit zwei Game Boys über ein Link-Kabel zu zweit gespielt werden. Durch gleichzeitiges Abbauen von zwei oder mehr Zeilen kann man dem Gegner eine unvollständige Zeile von unten in sein Spielfeld schieben. Zwei komplette Zeilen ergeben eine Zeile beim Gegner, drei Zeilen entsprechend zwei. Nur bei einem „Tetris“ – also vier Zeilen auf einmal – werden auch vier Zeilen zum Gegner geschickt. Die Zeilen enthalten eine freie Spalte, wodurch sie, falls diese Spalte nicht durch andere Steine verdeckt ist, mit einer passenden Form eliminiert und so zurückgeschickt werden können.

1989 war Tetris inklusive Verweis auf Alexei Paschitnow und Wadim Gerasimow im ersten Microsoft Windows Entertainment Pack für 16-bit Windows enthalten. Die Windowsversion selbst ist von Dave Edson.

Ab 1998 erschien mit eine Arcadeserie, sowie eine Version für Xbox 360.

Für den Nintendo DS sind zwei Tetris-Spiele bekannt. "Tetris DS" erschien am 20. April 2006 und bot einige klassische Spielmodi, die auf den Touchscreen des Nintendo DS angepasst waren. "Tetris Party Deluxe" ist ein weiteres Tetris-Spiel, das am 3. September 2010 erschienen ist. Spieler haben nun auch die Möglichkeit, selbst Rätsel zu erstellen. Das Spiel ist auch für Nintendo Wii erschienen.

Für den Nintendo 3DS erschien am 21. Oktober 2011 das Spiel "Tetris". Mittels der AR-Funktionen des Handhelds ist es möglich, das Spielfeld in die Umgebung zu projizieren, außerdem können alle 20 Spielmodi in 3D gespielt werden. Ebenfalls neu ist ein 8-Spieler-Online-Modus.

Tetris zählt zu den von Fremdherstellern und Hobbyprogrammierern am meisten nachgeahmten und geklonten Computerspielen überhaupt. Beispielsweise gibt es selbst für den Commodore 64 über 100 Varianten des Spiels, da viele Hobbyprogrammierer selbst neue Versionen erstellen. Für Unix Desktop Environments wie Gnome oder KDE wurden später Tetris-Klone erstellt. Auch für Editoren wie Emacs oder Vim gibt es entsprechende Nachahmungen. Von Electronic Arts erschienen Versionen für diverse Mobiltelefone.

Tetris gilt inzwischen als Computerspiel-Klassiker und wurde später retrospektiv als „Mutter aller Casual Games“ bezeichnet, als früher Ursprung oder Vorläufer dieses später sehr erfolgreichen Computerspiel-Genres. Tetris war auch abseits der Computerspielbranche einflussreich; so inspirierte es beispielsweise auch Geschirrhersteller oder wurde auf vielen Gebäuden gespielt, z. B. über die Fensterbeleuchtung.

Es ist auch wie kaum ein anderes Spiel in vielen Versionen und Variationen für nahezu jedes System erschienen: Spielkonsolen, Computer, Mobiltelefone, Portable Media Player, PDAs, Network music players, graphische Taschenrechner und sogar als Easter Egg in Nicht-Media-Produkten wie Oszilloskopen.
"Guinness World Records" hat Tetris als das Spiel gelistet, welches auf die meisten Systeme portiert wurde; 2011 gibt das Buch 65 verschiedene Plattformen an.

Es wurden zwar viele Versionen von Tetris in den 1980er Jahren für viele Heimcomputer und Arcadesysteme verkauft, jedoch war es die enorm erfolgreiche Game-Boy-Version von 1989, welche Tetris’ Ruf als eines der populärsten Spiele überhaupt begründete. Von Tetris wurden mehr als 70 Millionen Kopien verkauft. Im Januar 2010 wurde bekannt gegeben, dass Tetris seit 2005 100 Millionen Mal allein auf Mobiltelefonen verkauft wurde. In der Ausgabe 100 von "Electronic Gaming Monthly" erschien Tetris auf dem ersten Platz als "Greatest Game of All Time". 2007 war Tetris auf dem zweiten Platz in IGNs "100 Greatest Video Games of All Time" gelistet. Im Jahr 2009 erreichte Tetris den dritten Platz der "Top 200 Games of All Time" von "Game Informer".

Tetris wurde im März 2007 in die "Game Kanon" gewählt, eine Liste von Computerspielen, die als Meilensteine gelten.

Das Tetris-Tetromino L konnte 2007 auch den sechsten jährlichen Videospielcharakter-Popularitätskontest der Videospielwebseite GameFAQs für sich entscheiden, obwohl dieses ursprünglich als nicht ernst gemeinte Einreichung gehandelt wurde.

Am 6. Juni 2009 ehrte Google Tetris zum 25. Jahrestag durch eine Anpassung des Logos (Google Doodle) zu einer Version bestehend aus Tetrominos.

Am 29. November 2012 gab das Museum of Modern Art den Erwerb von 14 Computerspielen, darunter Tetris, für eine neue Design-Dauerausstellung in den Philip Johnson Galleries ab März 2013 bekannt. In der Bekanntmachung wurden die Titel als herausragende Vertreter im Bereich des Interaktionsdesigns bezeichnet. Kriterien waren demnach neben der visuellen Qualität und der ästhetischen Erfahrung sämtliche Aspekte, die zur Gestaltung der Interaktion beitragen, etwa die Eleganz des Programmcodes oder das Design des Spielerverhaltens.

Bekannt geworden durch ihre eingängige Melodie ist die Musik der Game-Boy-Ausgabe von Tetris (Music A). Es basiert auf dem Lied "Korobeiniki", das auf dem 1861 geschriebenen, gleichnamigen Gedicht von Nikolai Alexejewitsch Nekrassow basiert. Dieses Lied hat den Charakter eines russischen Tanzes und wird normalerweise mit einer stetigen Tempobeschleunigung (Accelerando) gespielt.
Music C in der Game-Boy-Version ist eine modifizierte Adaption des Menuetts der dritten Französischen Suite von Johann Sebastian Bach, einer Tanzsammlung für Tasteninstrumente, in h-Moll, BWV 814. Auf dem Game Boy erklingt das zweistimmige Stück in a-Moll.

Die berühmte Musik ist auch in dem Spiel Super Smash Bros. Brawl zu finden: Eine orchestrale und volksliedähnliche Variante des Typs A und eine eher arabische Version des Typs B; beide Titel werden in der Stage "Luigi’s Mansion" gespielt.

Die Commodore-64-Version des Spiels hat einen vierstimmigen Soundtrack mit einer Länge von fast 26 Minuten.


Das PC-Spiel von Spectrum HoloByte (1987) hatte als Zusatz eine arbeitsspeicherresidente Version des Spieles inkludiert, die mit ihrer Größe von 56 Kilobyte in den damals üblicherweise 640 Kilobyte großen Arbeitsspeicher geladen und mit der Tastenkombination + jederzeit ein- und auch wieder ausgeschaltet werden konnte. Das ermöglichte es, eine beliebige Software auszuführen und trotzdem in Sekundenbruchteilen zwischen Tetris und selbiger wechseln zu können. Zudem war diese Version ohne Geräusche oder Musik gehalten. Theoretisch war es dadurch möglich das Spiel am EDV-Arbeitsplatz unauffällig während der Arbeitszeit zu verwenden: Wäre eine Aufsichtsperson in Sichtweite gekommen, konnte mit der Tastenkombination + sofort wieder in die aktuelle Arbeitsumgebung gewechselt werden, in der nahtlos weitergearbeitet werden konnte, als hätte es keine Unterbrechung gegeben. Es gab keinerlei sichtbaren Hinweis, dass im Hintergrund Tetris im Pausenmodus wartete und mit erneutem Betätigen von + der aktuelle Spielstand fortgesetzt werden konnte.

1989 veröffentlichte Rainbow Arts eine 3D-Variante von Tetris namens BlockOut. Bei der Schwierigkeitsstufe "Flat Fun" sind einfach die herkömmlichen Tetris-Spielsteine aus Würfeln zusammengesetzt.

Im 2015 erschienenen Videogame Rise of the Tomb Raider findet sich ein Tetris-Easter Egg: In der Kupferwerk-Zone findet Lara Croft als Relikt ein Spiel, das ihr irgendwie bekannt vorkommt - Tetris. Den Namen des Spiels nennt sie zwar nicht, aber bei der Betrachtung des Relikts ist die starke Ähnlichkeit unverkennbar.



</doc>
<doc id="5212" url="https://de.wikipedia.org/wiki?curid=5212" title="Tenzin Gyatso">
Tenzin Gyatso

Tenzin Gyatso (gebürtig "Lhamo Döndrub" ( / "lha mo don 'grub"); * 6. Juli 1935 in Taktser, Provinz Amdo, Osttibet) ist der 14. Dalai Lama.

Er ist buddhistischer Mönch und Linienhalter der Gelug-Schule des tibetischen Buddhismus. Er befürwortet die Rime-Bewegung. Ab seiner Inthronisierung war er, wie alle "Dalai Lamas" zuvor, Oberhaupt der tibetischen Regierung und gilt als geistliches Oberhaupt der Tibeter. 2011 trat er von seinen politischen Ämtern zurück.

1989 wurde er mit dem Friedensnobelpreis ausgezeichnet.

Formelle Bezeichnung ist, in Anlehnung an die Anrede vergleichbarer religiöser Würdenträger, auf Deutsch "Seine Heiligkeit" bzw. bei direkter Anrede "Eure Heiligkeit", Tibeter nennen ihn "Kundün (tibet. Gegenwart), Yishi Norbu" (tibet. „Wunscherfüllendes Juwel“) oder "Gyelwa Rinpoche".

Tenzin Gyatso ist der Mönchsname des jetzigen Dalai Lama. Er wurde am 6. Juli 1935 mit dem Namen Lhamo Döndrub in Taktser, einem Dorf in der tibetischen Provinz Amdo im Nordosten Tibets, als zweiter Sohn der Bauernfamilie Dekyi Tshering und Chökyong Tshering geboren. Seine Mutter brachte insgesamt 16 Kinder zur Welt, von denen sieben die Kindheit überlebten. Tenzin Gyatso hat vier Brüder – Thubten Jigme Norbu (Reinkarnation des Taktser Rinpoche), Gyalo Thöndrup, Lobsang Samten und Tenzin Chögyel – sowie zwei Schwestern: Tshering Dölma und Jetsün Pema. Seine Schwester Tshering Dölma und seine Brüder Lobsang Samten und Thubten Jigme Norbu sind inzwischen verstorben.

Im Alter von knapp zwei Jahren wurde er von vier Mönchen anhand einer Vision des Regenten, von Orakelsprüchen und anderen Vorzeichen als Wiedergeburt des 1933 verstorbenen XIII. Dalai Lama aufgefunden. Berichtet wird, dass das Kleinkind sowohl einen als Diener verkleideten hohen Lama spontan als „Lama aus dem Kloster Sera“ erkannte als auch etliche Gegenstände aus dem Besitz des 13. Dalai Lama, die neben ähnlichen aufgelegt waren, als sein Eigentum an sich nahm.

Lhamo Döndrub musste zunächst vom Provinzgouverneur durch erhebliche Bestechungssummen freigekauft werden, was nahezu zweijähriger Verhandlungen bedurfte. Er kam im Alter von etwa vier Jahren in Lhasa an, wo er im Rahmen des Neujahrsfestes am 22. Februar 1940 im Potala als 14. Dalai Lama durch die "Sitringasol"-Zeremonie inthronisiert wurde. Sein neuer Name lautete "Jetsün Jampel Ngawang Lobsang Yeshe Tenzin Gyatso" – „Heiliger Herr, gütiger Herr, mitfühlender Verteidiger des Glaubens, Ozean der Weisheit“. Tibeter sprechen vom Dalai Lama normalerweise als "Yishin Norbu" („alle Wünsche erfüllender Edelstein“) oder einfach als "Kundün" (Gegenwart).

Von 1946 bis 1950 hielt sich der österreichische Bergsteiger und Abenteurer Heinrich Harrer in Lhasa auf und freundete sich ab 1948 mit dem jungen Dalai Lama an, für den er filmte, fotografierte und den er in der westlichen Form des Rechnens sowie in Englisch und Geografie unterwies.

Am 17. November 1950 wurde dem damals 15-jährigen Dalai Lama die weltliche Herrschaft über Tibet übertragen, er musste sich aber angesichts der offenbar aussichtslosen Lage unter der Bedrohung Tibets durch die Volksrepublik China zu dieser Zeit mitsamt seinem Regierungsstab unverzüglich nach Dromo, unmittelbar an der indischen Grenze, in Sicherheit bringen. Harrer begleitete ihn noch bis März 1951 und brach dann nach Europa auf. Der Dalai Lama kehrte im Sommer nach Lhasa zurück, um gemäß einem Abkommen mit den Chinesen seine religiösen Funktionen wieder aufzunehmen.

Am 23. Mai 1951 unterzeichneten Vertreter der tibetischen Regierung in Peking das „17-Punkte-Abkommen zur friedlichen Befreiung Tibets“, mit dem Tibet innenpolitische Autonomie und Religionsfreiheit zugesichert werden sollte, die Vertretung in der Außenpolitik, im Außenhandel sowie in militärischen Angelegenheiten aber von der Regierung der Volksrepublik China beansprucht wurde. Am 24. Oktober 1951 wurde nach einem entsprechenden Beschluss der tibetischen Nationalversammlung in Lhasa die Zustimmung Tenzin Gyatsos an Mao Zedong und die Regierung der Volksrepublik China telegrafiert. Er selbst erklärte dazu später, er habe dem Abkommen nur zugestimmt, um sein Volk und das Land "„vor der völligen Zerstörung zu bewahren“". Dem Abkommen vorausgegangen war der Einmarsch der chinesischen Volksbefreiungsarmee in der osttibetischen Provinz Chamdo.

Im Jahr 1954 reiste Tenzin Gyatso auf Einladung der Regierung der Volksrepublik China und gegen den Widerstand der Äbte von Sera, Drepung und Ganden mit einem Gefolge von fast 500 Personen nach Peking. Konservative Kräfte des tibetischen Adels und Klerus befürchteten, dass der Dalai Lama in Peking gegen ihre Interessen beeinflusst werden könnte. Im September 1954 wurde er zum Abgeordneten und danach zum stellvertretenden Vorsitzenden des Ständigen Ausschusses des Nationalen Volkskongresses gewählt. Der Dalai Lama überreichte Mao Zedong Geschenke und schrieb eine Hymne an ihn, in der er Mao Zedong u. a. mit Brahma verglich. Die chinesische Regierung finanzierte den Bau des Palastes Tagten Migyur Phodrang für den 14. Dalai Lama auf dem Gelände des Norbulingka. Der Palast wurde 1956 fertiggestellt.

Im Winter besichtigte der 14. Dalai Lama weitere chinesische Städte, 1955 feierte er das tibetische Neujahr in Peking und gab aus diesem Anlass ein Bankett für Mao Zedong, Zhou Enlai, Liu Shaoqi und Zhu De.

Im April 1958 wurde das Vorbereitungskomitee des Autonomen Gebiets Tibet gegründet und der 14. Dalai Lama wurde zum Vorsitzenden des Komitees gewählt.

Während des Tibetaufstands verließ Tenzin Gyatso am 17. März 1959 seinen Sommerpalast Norbulingka und floh nach Indien. Dort hatte er kurz zuvor die Geshe, die Doktorwürde der buddhistischen Theologie (vgl. indische Philosophie) erlangt, nachdem er sich in philosophischen Debatten mit Lehrern der Klöster Drepung, Ganden und Sera bewährt hatte.

Am 3. April 1959 informierte Jawaharlal Nehru das indische Parlament darüber, dass er dem Dalai Lama, seiner Familie und den Begleitern Asyl gewährte. Als erster Aufenthaltsort wurde Gyatso Tezpur zugewiesen, wohin er am 18. April aufbrach. Bald folgte eine weitere Verlegung nach Masuri. Schließlich wurde er nach Dharamsala (Himachal Pradesh) beordert, wo er bis heute residiert.

Vor der Flucht war das Nechung-Orakel befragt worden mit dem Ergebnis, „der Dalai Lama solle Tibet verlassen“.

Neben früheren politischen Aktivitäten setzt sich der 14. Dalai Lama stets intensiv für den friedfertigen, konstruktiven und mitfühlenden Dialog der Menschen ein. Dazu führte er Vortragsreisen rund um den Globus und gab Schriften heraus, in denen die differenzierten Vorstellungen der tibeto-buddhistischen Religion zu Fragen der Lebenspraxis, zur Natur des menschlichen Bewusstseins und weiteren existenziellen Fragen erläutert werden.

Der Dalai Lama gilt als Freund des Christentums und war oftmals Gast im Vatikan; freundschaftlich verbunden war er mit Papst Johannes Paul II.
Wegen seines regen Interesses an wissenschaftlichen Themen und der Zusammenarbeit mit dem nordamerikanischen Rechtsanwalt Adam Engle und dem chilenischen Neurobiologen und Philosophen Francisco Varela entstand 1990 das in den USA ansässige Mind and Life Institute. Der Dalai Lama ist nach eigener Aussage davon überzeugt, dass Wissenschaft und die buddhistischen Thesen absolut vereinbar sind.

Im Oktober 2002 nahm Tenzin Gyatso in München am Wissenschaftskongress „Unity in Duality – Einheit in der Dualität“ – gemeinsam mit neun Wissenschaftlern verschiedener Länder aus verschiedenen wissenschaftlichen Bereichen teil: z. B. für den Bereich Pharmazie Candace B. Pert, Pharmakologin, USA; im Bereich Astrophysik Trịnh Xuân Thuận, Astrophysiker geb. Vietnam; für die biochemische Fachrichtung Rupert Sheldrake, England; im Bereich Psychologie Jean Shinoda Bolen, Psychiaterin und Jung’sche Analytikerin, USA; der philosophische Bereich vertreten durch Marit Rullmann, Projektmanagerin der lokalen Agenda 21, Deutschland; für die neurobiologische Fachrichtung Humberto Maturana Biologe, im Kontext von Neuroanatomie, Neurobiologie, Chile. Formuliert wurde letztlich, dass das dualistische Weltbild der Wissenschaft nicht im Widerspruch zu dem Gedanken einer aller Erfahrungen zu Grunde liegenden Einheit stehen müsse. Dieses Treffen gilt als wichtige Station im Dialog von buddhistischer Lehre und westlicher Wissenschaft.

Der gegenwärtige Dalai Lama besucht im Westen buddhistische Schulen und gibt dort in regelmäßigen Abständen Einweisungen in die buddhistische Lehre und Rituale.

Neben der moralischen Autorität hat der 14. Dalai Lama im Westen vor allem den Status eines „Botschafters des Friedens“. Für seine Bemühungen, mit friedlichen Mitteln auf die Lage in seinem Heimatland Tibet aufmerksam zu machen, wurde ihm 1989 der Friedensnobelpreis verliehen.

Der bewaffnete Widerstand von Tibetern gegen die Volksrepublik China wurde ab den 1960er-Jahren im Zuge des Kalten Krieges durch Zahlungen der Central Intelligence Agency (CIA) an die tibetische Exilregierung mitfinanziert. Rund 250 Tibeter wurden, zum Teil wiederholt, in Camp Hale, einem auf Kriegsführung im Gebirge spezialisierten Ausbildungslager in Colorado, in Guerilla-Taktiken geschult. Im Zuge dieses Programmes kam es laut dem Fernsehmagazin Panorama auch zu persönlichen Kontakten zwischen Tenzin Gyatso und Mitarbeitern des Nachrichtendienstes. Die Kontakte wurden durch Vertreter Tenzin Gyatsos, darunter seine Brüder, über das US-Konsulat in Kalkutta und die US-Botschaft in Neu-Delhi initiiert. Thema der Kontakte war laut einem Bericht der Financial Times Deutschland explizit auch militärische Hilfe. In der Fachliteratur wird hingegen betont, dass Gyatso jeglichen Kontakt zu CIA oder den in Mustang stationierten Kham-Rebellen mied, und diese Aufgabe durch seinen Bruder Gyalo Thondup wahrgenommen wurde.

Im Oktober 1998 bestätigte die Exilregierung, in den 1960ern dafür über mehrere Jahre jeweils rund 1.700.000 US$ von der CIA erhalten zu haben. Dass Tenzin Gyatso persönlich von einer weiteren Fixsumme in Höhe von 180.000 US$ profitiert habe, die laut Recherchen der Süddeutschen Zeitung in CIA-Unterlagen als „Geldhilfe für den Dalai Lama“ deklariert wurde, wurde hingegen verneint. Diese Beträge wurden nach Angabe seiner Verwaltung für den Aufbau von Büros in Genf und New York City und Lobbytätigkeiten genutzt.

Die CIA stellte die Unterstützung des Kampfes ein, als sich die Niederlage der Vereinigten Staaten im Vietnamkrieg abzeichnete und Richard Nixon gegenüber der Volksrepublik China eine Politik der Entspannung einleitete. Die CIA begann 1967, die Operation zu reduzieren, im Jahr 1974 forderte der Dalai Lama die letzten Guerilleros dazu auf, die Waffen niederzulegen.

Tenzin Gyatso ist durch Heinrich Harrer, den er mehrmals in Hüttenberg besuchte, mit Kärnten besonders verbunden. 1992 weihte der Dalai Lama den buddhistischen Gebetsraum des Heinrich-Harrer-Museums ein, und beim 90. Geburtstag Harrers 2002 segnete er den neugebauten tibetischen Pilgerpfad Lingkor. Im Oktober 1998 hielt er vor rund 8.000 Menschen täglich eine einwöchige Unterweisung auf der großen Freifläche des ehemaligen britischen Militärlagers Camp Reinsehlen in der Lüneburger Heide. Am 14. Mai 2006 besuchte Tenzin Gyatso Hüttenberg, um den Grundstein für ein Tibet-Zentrum zu legen. An der Grundsteinlegung nahmen ca. 5.000 Personen teil.

Ende Juli 2005 war Tenzin Gyatso drei Tage lang zu Gast in Wiesbaden. Dabei wurde ihm der Hessische Friedenspreis verliehen und sein 70. Geburtstag nachgefeiert. Zum Abschluss seines Besuchs hielt er vor ungefähr 20.000 Menschen eine Rede im Wiesbadener Kurpark.

Vom 19. bis 29. Juli 2007 war der 14. Dalai Lama in Hamburg, wo er unter anderem im Rothenbaum-Stadion Vorträge hielt. Zum Schluss einer seiner längsten Auslandsreisen war er in Freiburg im Breisgau, um im dortigen Tibet-Kailash-Haus, einem tibetischen Gesundheitszentrum, einen Stupa einzuweihen und in der Rothaus-Arena einen Vortrag zu halten.

Vom 17. bis zum 19. September 2007 nahm der Dalai Lama am Waldzell Meeting im Stift Melk in Niederösterreich teil. Am 20. September wurde er von Bundeskanzler Alfred Gusenbauer zu einem Gespräch ins Bundeskanzleramt eingeladen, wofür dieser von chinesischer Seite heftig kritisiert wurde.
Vom 20. bis 21. September 2007 besuchte Tenzin Gyatso zum zweiten Mal nach 1998 Münster. Dort erhielt er von der Westfälischen Wilhelms-Universität die Ehrendoktorwürde des Fachbereichs Chemie und Pharmazie für seine Verdienste um die Vermittlung zwischen Religion und Wissenschaft. Dies ist für Gyatso sowohl der erste Ehrendoktor einer deutschen Hochschule als auch der erste aus dem Bereich der Naturwissenschaften. Am zweiten Tag diskutierte er im Schloss Münster mit Vertretern der "Graduate Schools" und hielt einen Vortrag zum Thema „Globale Verantwortung in Wissenschaft und Gesellschaft“ vor rund 4500 Gästen.

Nachdem er am 22. September 2007 dem Hessenpark einen Besuch abstattete, besuchte Tenzin Gyatso einen Tag später Bundeskanzlerin Angela Merkel in ihrem Amtssitz. Dies war das erste Treffen mit einem deutschen Regierungschef. Die chinesische Regierung kritisierte Merkel für ihre Entscheidung und sagte diverse Treffen mit deutschen Regierungsvertretern ab.

Vom 14. bis 20. Mai 2008 besuchte der Dalai Lama nach einer Einladung der Tibet Initiative Deutschland unter dem Motto „Kein Frieden ohne Menschenrechte“ mehrere Städte, darunter Bochum, Mönchengladbach, Nürnberg, Bamberg und Berlin.

Anfang 1998 trat Tenzin Gyatso im Rahmen einer Werbeanzeige des IT-Konzerns Apple in Erscheinung. Die Anzeige mit einem großformatigen Bild des Dalai Lama war Bestandteil einer Marketingkampagne die unter dem Claim "Think Different" speziell auf die asiatischen Märkte abzielte. Die anderen verwendeten Persönlichkeiten waren Alfred Hitchcock, Albert Einstein, Pablo Picasso, Mahatma Gandhi und Amelia Earhart. Im April 1998 erklärte Apple, zukünftig auf das Motiv im Rahmen seiner Anzeige verzichten zu wollen und sich stattdessen auf bekanntere Persönlichkeiten zu konzentrieren. Dies führte zu Spekulationen darüber, dass die chinesische Regierung Druck auf das Unternehmen ausgeübt hätte. Apple verneinte dies ausdrücklich. Ein bekannter Marktforscher der Hong Kong University kommentierte die Entscheidung des Unternehmens mit den Worten, es sei unmöglich, dass Amelia Earhart in der Region bekannter sei als der Dalai Lama. Ein Sprecher des Dalai Lama erklärte, es sei offensichtlich, dass der Vorgang in Zusammenhang mit China stehe, einem bedeutenden Markt für das Unternehmen. Der Dalai Lama habe dem Unternehmen eine Erlaubnis gegeben, das Bildmotiv weltweit zu nutzen, er sei nicht darauf hingewiesen worden, dass es nicht in der Kampagne in China verwendet werden würde.

Im März 2011 bat der Dalai Lama das tibetische Exil-Parlament, ihn von seinen politischen Aufgaben zu entbinden. Demokratisch gewählte Volksvertreter sollten ihn ersetzen. Tsewang Rigzin, der Präsident des weltweit organisierten tibetanischen Jugendkongresses, stellt den gewaltlosen Kurs des religiösen Oberhauptes schon länger in Frage. Seine politische Rolle soll der Jurist Lobsang Sangay ausfüllen, der Ende April 2011 zum neuen Ministerpräsidenten der tibetischen Exilregierung gewählt wurde.

Eine Minderheit buddhistischer Mönche und Laien führt die Verehrung der Schutzgottheit Dorje Shugden weiterhin fort, welche der 14. Dalai Lama abgebrochen hat. Er sieht in Dorje Shugden mittlerweile einen ihm persönlich und der mit seiner Person verbundenen politischen Position feindlich gesinnten Geist. 1996 untersagte er Mitarbeitern der exiltibetischen Regierung und den ihm unterstehenden Mönchen und Laien die Shugden-Verehrung. Die überwiegende Anzahl exiltibetischer Klöster schlossen Mönche aus, die die Praxis aufzugeben verweigerten. Die Illegalisierung der Praxis innerhalb der tibetischen Exilgemeinde und der mit dieser verbundenen Klöster gipfelte 1997 in der Ermordung dreier hochrangiger Mönche, welche dem 14. Dalai Lama nahestanden, und die immer noch nicht hinreichend aufgeklärt werden konnten. Beide Seiten erhoben in diesem Zusammenhang schwerwiegendste gegenseitige Vorwürfe.

Daneben wurde auch seine Rolle im Karmapa-Konflikt kontrovers öffentlich diskutiert.

Kritiker werfen Tenzin Gyatso vor, dass er die Zustände im damaligen Tibet heute noch idealisiert darstelle. Den Kritikern zufolge habe den lamaistischen Mönchen zusammen mit einer kleinen Adelsschicht aller Grund und Boden gehört; es habe Leibeigene und Sklaven gegeben, die von einer Mönchspolizei überwacht worden seien. Auch verstümmelnde Körperstrafen seien häufig gewesen.

Dieser auch von den chinesischen Behörden verbreiteten Darstellung widerspricht die tibetische Exilregierung. Es sei „nicht korrekt, die alte tibetische Gesellschaft als "Feudalgesellschaft" oder "Sklavensystem" zu bezeichnen“. Tatsächlich sei Tibet „vor der Invasion wesentlich egalitärer als die meisten anderen asiatischen Länder in dieser Zeit“ gewesen.

Tenzin Gyatso setzt sich für die Autonomie Tibets innerhalb der Volksrepublik China ein, was von den Machthabern als Separatismus bezeichnet wird. Am 10. März 1963 verkündete der 14. Dalai Lama eine an demokratischen Prinzipien orientierte Verfassung für die tibetische Exilregierung, die ihn, bis zum Rücktritt, 2011, als Staatsoberhaupt betrachtete. Vielen Tibetern geht die Forderung Tenzin Gyatsos nach Autonomie nicht weit genug, sie fordern die volle Unabhängigkeit Tibets.

In der Frage der nuklearen Bewaffnung Indiens brachte er wiederholt sein Bedauern über diese Entwicklung zum Ausdruck. 1998, nach unterirdischen Kernwaffentests in Indien und im Jahr der ersten Tests in Pakistan, rief er zu einer weltweiten Eliminierung aller Kernwaffen auf und verwies dabei auch auf entsprechende Aussagen der Führung der Volksrepublik China. Zugleich vertrat er den Standpunkt, es sei undemokratisch, wenn andere große Länder für sich das Recht in Anspruch nehmen, Kernwaffen zu besitzen, aber von Ländern wie Indien fordern, auf diese Waffen zu verzichten.

2005 erklärte er in einer Rede, Krieg sei „veraltet“ („out of date“), und das Ziel eine demilitarisierte Welt. Er ist Unterstützer der Internationalen Kampagne zur Abschaffung von Atomwaffen (ICAN). Während des Wahlkampfs zur Präsidentschaftswahl in den Vereinigten Staaten 2008 unterstützte er einen Appell an die Kandidaten, sich im Fall ihrer Wahl für nukleare Abrüstung einzusetzen. 2009 war er Mitunterzeichner einer von 17 Nobelpreisträgern an US-Präsident Barack Obama gerichteten Petition zum Verzicht auf Kernwaffen.

Der 14. Dalai Lama engagiert sich bereits seit den 1960er Jahren für die Arbeiten der SOS-Kinderdörfer. Ausgangspunkt waren die tibetischen Flüchtlingskinder, die in Indien zu diesem Zeitpunkt großen Bedarf an Zufluchtsorten hatten.

Auch im Kampf gegen den Walfang bezieht der 14. Dalai Lama Stellung, er ist Unterstützer der Umweltschutzorganisation Sea Shepherd.

Tenzin Gyatso erhielt eine Reihe von Auszeichnungen, darunter den Friedensnobelpreis, verliehen am 10. Dezember 1989 in Oslo.

Im deutschsprachigen Raum wurde er mit folgenden Ehren ausgezeichnet:









</doc>
<doc id="5213" url="https://de.wikipedia.org/wiki?curid=5213" title="Transaktionsnummer">
Transaktionsnummer

Eine Transaktionsnummer (TAN) ist ein Einmalkennwort, das üblicherweise aus sechs Dezimalziffern besteht und vorwiegend im Online-Banking verwendet wird.

Es gibt verschiedene Ansätze, um TANs zu erzeugen, zu prüfen und an den Nutzer zu übertragen. Einige davon werden im Folgenden beschrieben.

Beim klassischen TAN-Verfahren erhält der Teilnehmer beim Electronic Banking, meist per Post, eine Liste von Transaktionsnummern. Bei jedem Buchungsvorgang – der Transaktion – muss eine beliebige TAN der aktiven Liste eingegeben werden. Sie ist eine Ergänzung zur Persönlichen Identifikationsnummer (PIN). Falls die Bank nach Eingabe der korrekten PIN einen Buchungsauftrag mit korrekter TAN erhält, geht sie davon aus, dass der Auftrag vom Kunden abgesendet wurde. Die TAN wird von der Bank als Quasi-Unterschrift interpretiert. Sie verfällt nach einmaligem Gebrauch. Wenn die TAN-Liste zur Neige geht, erhält der Kunde von der Bank nach Anforderung oder automatisch eine neue.

Auf Grund stark anwachsender Phishing-Angriffe wurde diese Art der TAN-Liste von vielen Banken ab etwa 2005 durch die indizierten TAN-Liste ersetzt.

Einen Schritt weiter als das klassische TAN-Verfahren gehen die "indizierten Transaktionsnummern", kurz "iTAN": Der Kunde kann hier seinen Auftrag nicht mehr mit einer beliebigen TAN aus seiner Liste legitimieren, sondern wird von der Bank aufgefordert, eine bestimmte, durch eine Positionsnummer (Index) gekennzeichnete TAN aus seiner zu diesem Zweck nun durchnummerierten Liste einzugeben. Der TAN-Aufforderung muss der Kunde innerhalb weniger Minuten folgen. Außerdem wird die angeforderte TAN auch im Falle einer Nichtverwendung im Bankrechner als verbraucht gekennzeichnet.

Dieses Verfahren kann von Hackern auf zwei verschiedene Weisen angegriffen werden:


In beiden Fällen ist das Ziel des Angreifers, sich unbemerkt in die Kommunikation der Bank mit dem Kunden einzuklinken und dem Bankkunden zu suggerieren, dass er sich auf einer Bank-Website befinde, obwohl er sich tatsächlich auf einer gefälschten Bank-Website befindet. Durch Einblenden eines Formulars innerhalb eines gefälschten Online-Banking-Systems kann dann der Online-Banking-Kunde z. B. zur Eingabe mehrerer TANs inklusive Indexnummer aufgefordert werden.

Die Schadsoftware tauscht im Hintergrund die Überweisungsdaten aus; mit der angeforderten TAN bestätigt der Online-Banking-Kunde also in Wirklichkeit die betrügerische Überweisung des Hackers. Auf seinem PC-Bildschirm sieht er immer noch seine Originalüberweisung. Selbst die Umsatzanzeige und der Kontosaldo können somit manipuliert werden, um keinen Verdacht aufkommen zu lassen.

Immer mehr Banken sehen die iTAN-Liste nur noch als Mindestschutz und empfehlen ihren Kunden andere Verfahren (siehe unten).

Ein Nachteil der iTAN ist, dass für Überweisungen von unterwegs (z. B. aus dem Urlaub) immer die komplette iTAN-Liste mitgeführt werden muss. Bei dem einfachen TAN-Verfahren ist es möglich, wenige TANs so mitzuführen, dass sie von Fremden nicht als solche erkennbar oder nicht korrekt lesbar sind, zum Beispiel in einer Telefonliste. Eine iTAN-Liste ist schwerer zu tarnen, und einem Dieb fällt auch immer die komplette Liste in die Hände. Diese ist jedoch wertlos, solange der Dieb nicht auch im Besitz der PIN ist. Erst dann kann er durch Überweisungen Geld erbeuten.

Dagegen besteht ein praktischer Vorteil der iTAN gegenüber der einfachen TAN darin, dass man verbrauchte iTANs nicht von der Liste streichen muss. Dadurch kann man beispielsweise unabhängig ohne Synchronisierungsprobleme mit mehreren verschlüsselten elektronischen Kopien einer iTAN-Liste arbeiten.

Das iTAN-Verfahren gilt seit Einführung nicht als 100 % sicher. Phishing-Angriffe sind durch iTAN zwar schwieriger geworden, aber nicht unmöglich. Das BKA hat im Jahr 2008 rund 1800 erfolgreiche Phishing-Angriffe registriert, die in aller Regel durch die Einschleusung von Trojanern erfolgten.

Anfang 2007 tauchten demnach erste Phishing-Kits auf, die in der Lage gewesen seien, über die bereits genannte Man-in-the-middle-Attacke abgefischte iTANs in Echtzeit für eigene Transaktionen zu benutzen. 2010 wurden mit einem entsprechenden iTAN-Trojaner 1,65 Millionen Euro erbeutet. Der Trojaner manipulierte dabei im Hintergrund sowohl die Überweisungsdaten als auch die Kontoübersichtsseite, sodass die tatsächlich getätigten Überweisungen verborgen blieben.

2012 empfahl die Europäische Agentur für Netz- und Informationssicherheit (ENISA) daher allen Banken, die PCs ihrer Kunden grundsätzlich als infiziert zu betrachten und deshalb Sicherheitsverfahren zu verwenden, bei denen der Kunde noch einmal unabhängig vom PC die tatsächlichen Überweisungsdaten kontrollieren kann. Beispiele sind laut ENISA – unter dem Vorbehalt, dass die Sicherheit des Mobiltelefons gewährleistet werden kann – mTAN oder Smartcard-basierte Lösungen mit eigenem Kontrolldisplay wie chipTAN.

Der beim iTAN-Verfahren mögliche Man-in-the-middle-Angriff wird durch das sogenannte iTANplus-Verfahren erschwert, aber nicht verhindert. Bei diesem Verfahren wird vor Eingabe der iTAN ein Kontrollbild angezeigt, in welchem sämtliche Transaktionsdaten noch einmal aufgeführt werden. Außerdem wird als digitales Wasserzeichen das Geburtsdatum des Kontoinhabers angezeigt, welches einem Man-in-the-middle-Angreifer in der Regel nicht bekannt sein sollte. Dadurch soll ein automatisches Generieren eines manipulierten Kontrollbildes durch einen Angreifer erschwert werden. Nachteil dieses Verfahrens ist die Verschlechterung der Ergonomie, da das Kontrollbild schwieriger zu lesen ist als die Aufforderung zur iTAN-Eingabe in normaler Textform.

Das Verfahren kann um eine Bestätigungsnummer (BEN) erweitert werden, mit der die Bank die Auftragsannahme im Gegenzug quittiert. Das bedeutet, dass ein Angriff als Man-in-the-Middle und nahezu in Echtzeit stattfinden muss, um nicht aufzufallen. Phishing und Pharming fielen auf, da keine (korrekten) BENs zurückgegeben würden.

Die Variante SMS-TAN ("smsTAN") wird oft auch "Mobile TAN" ("mTAN") genannt und besteht aus der Einbindung des Übertragungskanals SMS. Dabei wird dem Onlinebanking-Kunden nach Übersendung der ausgefüllten Überweisung im Internet seitens der Bank per SMS eine nur für diesen Vorgang verwendbare TAN auf sein Festnetz- oder Mobiltelefon gesendet. Der Auftrag muss anschließend mit dieser TAN bestätigt werden.

Wird für die mTAN auf ein Festnetz-Anschluss benutzt, so ist dieses Verfahren weit sicherer als beim Mobilnetz. Als Mobiltelefone werden heute hauptsächlich Smartphones mit Betriebssystem Android oder iOS benutzt, welche durch Viren, Bots und unsichere Apps angreifbar sind. Diese Möglichkeit entfällt bei Festnetztelefonen, da dort meist keine Software installiert werden kann.

Durch den SMS-Versand der TAN gilt mTAN als sicherer als iTAN oder das klassische TAN-Verfahren. Dadurch, dass die Gültigkeitsdauer der TAN begrenzt ist, zusätzlich die Zielkontonummer (oder Teile davon) sowie der Überweisungsbetrag in der SMS angegeben werden und die TAN nur für genau diese Transaktion gültig ist, soll eine Umleitung auf ein anderes Konto durch einen Man-in-the-middle-Angriff auf die Homebanking-Webseite verhindert werden. Auch werden Phishing-Angriffe auf TANs im mTAN-Verfahren erschwert. Der TÜV Rheinland bescheinigte im Jahr 2006 dem mTAN-System der Postbank AG eine „wirksame Absicherung der von außen zugänglichen technischen Systeme gegen unbefugte Nutzung“. Mitte 2007 erhielt auch das mTAN-System der Fiducia IT AG (IT-Dienstleister der Volks- und Raiffeisenbanken) diese TÜV-Zertifizierung.

Als Vorteil wird angesehen, dass man für Transaktionen unterwegs keine TAN-Liste dabei haben muss. Erfahrungsgemäß wird auch der Verlust des Mobiltelefons eher vom Nutzer bemerkt als der Verlust der Bankkarte oder des TAN-Bogens. Zusätzlich erfährt der Nutzer womöglich per SMS von unautorisierten Überweisungsversuchen, sodass eine Sperrung des Kontos veranlasst werden kann.

Da Mobiltelefone zunehmend mit Internetzugang ausgestattet sind, hat das mTAN-Verfahren grundsätzliche Schwächen, etwa wenn die mTAN auf dasselbe Gerät gesendet wird, das auch für das Online-Banking genutzt wird. Bei einem Verlust des Mobiltelefons besteht zudem der einzige Schutz vor missbräuchlichen Transaktionen in den Zugangsdaten zum Banking. TAN-Generatoren bieten in dieser Hinsicht wesentlich mehr Sicherheit, da die Geräte nicht vernetzt sind.

Entsprechend kann auf einem Smartphone ein Trojaner, der die Kontrolle über das System übernimmt, zuerst Benutzernamen und Passwort des Online-Bankings abhören und anschließend vollautomatisch Überweisungen durchführen, indem er sich erst ins Online-Banking einloggt, dann eine Überweisung ausfüllt, anschließend die SMS der mTAN abfängt und damit die Überweisung legitimiert. Es reicht also für einen Trojaner ein einmaliges Einloggen beim Online-Banking über das Smartphone, damit diese Sicherheitslücke ausgenutzt werden kann. Das Ganze funktioniert auch, wenn der Angreifer Benutzernamen und Passwort des Online-Bankings auf einem anderen Wege in Erfahrung gebracht hat, z. B. durch einen weiteren Trojaner auf dem PC.

Im September 2010 wurde erstmals über neue Varianten des Banking-Trojaners ZeuS berichtet, der mit einer mobilen Variante entsprechende Parallel-Infektionen durchführt. Im Dezember 2012 wurde berichtet, dass durch diesen Trojaner bereits über 36 Millionen Euro erbeutet worden sind.

Eine weitere Schwachstelle ist die mit der Rufnummer verknüpfte SIM-Karte des Mobiltelefons. Diesbezüglich wurden Angriffe bekannt, bei denen mithilfe etwa per Trojaner erbeuteter Nutzerdaten die Rufnummer des Opfers auf eine neue SIM-Karte portiert oder von den Angreifern eine Zweit-SIM angefordert wurde, mit der sie anschließend das mTAN-Verfahren aushebeln konnten.

Der Versand der TAN-Nachrichten per SMS wird dem Kunden von einigen Banken in Rechnung gestellt oder dieser erhält unter Umständen nur begrenzte Frei-Kontingente. Banken dürfen nach einer Entscheidung des Bundesgerichtshofes von Juli 2017 jedoch nur für den SMS-Versand von solchen TANs Gebühren abrechnen, die tatsächlich verwendet werden.

Denkbare Angriffe gegen das mTAN-Verfahren basieren darauf, dass die PIN für das Onlinebanking im ersten Schritt über herkömmliche Methoden wie Phishing oder Trojaner ausgespäht wird. Sobald der Phisher die PIN hat, kann er sich im Onlinebanking des Betrugsopfers einloggen und dessen persönliche Daten auslesen, unter anderem Kontonummer, Bankleitzahl, Adresse sowie die hinterlegte Mobiltelefonnummer für das mTAN-Verfahren. Das genutzte Mobilfunknetz kann über eine Netzabfrage herausgefunden werden. Daher wird die hinterlegte Mobilfunknummer im Onlinebankingportal nicht komplett angezeigt.

Im zweiten Schritt müsste es dem Phisher gelingen, auch die mTAN zu manipulieren.

Sobald man die PIN hat, kann man sich selbst (d. h. das eigene Mobiltelefon) für das mTAN-Verfahren registrieren lassen. Die dabei von den Banken vorgenommenen Authentisierungsverfahren sind noch nicht ausgereift, es wird aber ständig an einer Optimierung der Prozesse gearbeitet.
Einfache Festnetztelefone haben keine Software, hier wird die SMS vorgelesen (siehe dazu auch Festnetz-SMS). Bei modernere Komfort-Festnetztelefonen mit (Text-)SMS-Funktion ist die Firmware in einem integriertem Schaltkreis oder in einem EPROM und somit eine Veränderung von außen technisch nicht möglich.

Eine Möglichkeit bestünde darin, auch das Mobiltelefon des Betrugsopfers zu kompromittieren, insbesondere Smartphones sind hier gefährdet. Ein bereits infizierter Computer kann auch das Mobiltelefon des Betrugsopfers infizieren, wenn das Betrugsopfer sein Mobiltelefon mit dem Computer verbindet, zum Beispiel für den eigentlichen Zweck der Synchronisation. Auch denkbar wäre SMiShing oder exploiten des Mobiltelefons.

Auf Grund der anfangs noch geringen zu erwartenden Gewinne durch Betrug mit mobilen Geräten und durch die damals vielen verschiedenen miteinander inkompatiblen Plattformen (Betriebssystemen) der Mobiltelefone, waren Handyviren früher nicht zahlreich verbreitet.

Inzwischen werden fast ausschließlich Smartphones mit den Betriebssystemen Android, iOS und in geringem Maße Windows benutzt, damit hat sich das grundlegend geändert. Deshalb verbieten aktuelle Bedingungen der Geldinstitute die Verwendung ein und desselben Mobiltelefons zum Onlinebanking und zum Empfang der mTAN.

Sobald das Mobiltelefon des Betrugsopfers kompromittiert ist, kann der Betrüger beliebige Überweisungen ausführen, während die Anzeige und Signalisierung der mTAN nur für den Betrüger sichtbar sind, nicht jedoch für das Betrugsopfer.

Ein solcher Angriffsversuch mittels Spyware auf dem Mobiltelefon wurde im April 2011 tatsächlich entdeckt.

Einen weiteren Angriffspunkt, unabhängig vom Handy-Modell, stellen darüber hinaus ältere SIM-Karten dar, die noch mit der inzwischen geknackten DES-Verschlüsselung arbeiten. Über eine präparierte Service-SMS ist es dadurch möglich, unbemerkt die SIM-Karte des Opfers zu übernehmen. Aufgrund unzureichend gesicherter Java-VMs auf der SIM-Karte kann man darüber hinaus auch deren Master-Key auslesen und damit die SIM-Karte klonen.

Weitere Möglichkeiten wären der Einsatz eines IMSI-Catchers oder das Brechen der inzwischen nicht mehr sicheren GSM-Verschlüsselung. Diese Methoden erfordern allerdings örtliche Nähe zum Betrugsopfer, wofür erheblich mehr kriminelle Energie notwendig ist als beim Phishing, das ohne räumlichen Bezug ausgeführt wird.

Durch Ausnutzen einer Sicherheitslücke in dem in Mobilfunknetzen eingesetzten Signalling System 7 ist zudem auch das Abfangen von SMS aus der Ferne möglich, wie Tobias Engel 2014 auf dem 31. Chaos Communication Congress demonstrierte. Anfang 2017 wurde diese Schwachstelle von Kriminellen erfolgreich genutzt, um mTANs umzuleiten und so Geld von Bankkunden zu stehlen.

Eine weitere Möglichkeit wäre ein kompromittierter Netzbetreiber, entweder durch einen menschlichen Komplizen oder durch Sicherheitslücken.

Ein Angreifer kann im Namen des Opfers bei dessen Mobilfunkanbieter eine neue SIM-Karte beantragen und sich zusenden lassen oder teilweise sofort im Geschäft austauschen lassen. Bei den meisten Mobilfunkanbietern ist dies relativ einfach möglich. Allerdings wird üblicherweise die alte Karte 1–2 Tage vor Eingang der neuen Karte gesperrt, so dass das Opfer den Angriff leicht bemerken kann.

Des Weiteren wurde 2013 in Deutschland ein Angriff bekannt, bei dem die Angreifer, nachdem sie – vermutlich per Trojaner – die Online-Banking-Zugangsdaten sowie die Handynummer des Opfers ausgespäht hatten, eine zweite SIM-Karte als Multi-SIM unter dem Namen des Opfers beim Mobilfunk-Provider bestellten. Bei diesem Angebot der Netzbetreiber wird unter derselben Rufnummer eine weitere SIM-Karte ins Mobilfunknetz eingebucht, die sich per Kurzwahl so konfigurieren lässt, dass alle SMS nur von dieser zweiten SIM-Karte empfangen werden. Das Opfer muss dies nicht bemerken, da mit dessen Handy und der ursprünglichen SIM-Karte weiterhin ein- und ausgehende Anrufe möglich sind.

Die Täter haben dann die uneingeschränkte Kontrolle über das Konto des Opfers: Da die zur Legitimation erforderliche mTAN an ihre SIM-Karte gesendet wird, können sie das Überweisungslimit ändern und beliebig viele Überweisungen auf Fremdkonten tätigen. Entsprechend hoch sind die Schadenssummen bei diesen Angriffen.

2014 wurden mehrere Fälle bekannt, bei denen die Täter in Handyshops Zweitkarten erhielten, weil sie sich dort nicht, wie eigentlich vorgeschrieben, mit amtlichem Ausweis als Erstkarteninhaber identifizieren mussten.

Der Angreifer kann den Laufzeit- oder Prepaidvertrag des Opfers kündigen und danach eine Rufnummernmitnahme zu einem neuen Mobilfunkanbieter auslösen. Für die Kündigung beim alten Anbieter sowie für den neuen Laufzeit- oder Prepaidvertrag und die Rufnummernmitnahme beim neuen Anbieter wird nur die Unterschrift benötigt. Diese lässt sich mit wenig Aufwand fälschen und wird teilweise auch nur ungenügend oder überhaupt nicht geprüft. Ein entsprechender Angriff wurde 2009 in Australien festgestellt.

Es ist möglich, eine betrügerische Adressänderung sowie Kündigung in einem Schreiben an den alten Anbieter in einem Brief gleichzeitig zu formulieren. Die Kündigungsbestätigung sowie die neue SIM-Karte könnte also an eine vom Angreifer frei wählbare Adresse gesandt werden.

Im für den Angreifer günstigsten Fall würde die Rufnummer des Opfers auf seiner alten SIM-Karte um 24 Uhr abgeschaltet werden und für ihn um 6 Uhr angeschaltet werden, was ein erfolgversprechendes Zeitfenster darstellt.

Bei diesem Angriff wird versucht, den Benutzer mittels „Social Engineering“ dazu zu bewegen, von sich aus freiwillig eine Überweisung zu Gunsten der Betrüger zu tätigen. Dazu wird der PC des Opfers mit einem Trojaner infiziert und danach im Onlinebanking-Auftritt der Bank etwa die Meldung angezeigt, der Kunde solle im Auftrag der Bank zu „Testzwecken“ eine angebliche „Test-Überweisung“ zu Gunsten der Betrüger ausführen oder es wird angezeigt, eine größere Summe sei fälschlicherweise auf das Konto des Kunden überwiesen worden und dieser solle nun Geld „zurücküberweisen“. In anderen Fällen soll der Nutzer per Überweisung an einem Gewinnspiel teilnehmen oder Daten synchronisieren. Die Banken warnen vor diesen Betrugsversuchen. Eine Bank würde einen Kunden niemals auffordern, eine Überweisung zu tätigen, die dieser nicht selbst veranlasst hat.

pushTAN ist ein App-basiertes TAN-Verfahren der Sparkassen, das einige Nachteile des mTAN-Verfahrens beseitigt. So fallen für die Versendung der pushTAN-Nachrichten bei manchen Geldinstituten keine Kosten an und es ist kein SIM-Karten-Betrug möglich, da die Nachrichten mithilfe einer speziellen Textmessaging-App über eine verschlüsselte Internetverbindung an das Smartphone des Benutzers gesendet werden. Wie bei mTAN werden zur Kontrolle gegen verdeckte Manipulationen der Überweisungsdaten durch Trojaner auf dem PC die bei der Bank tatsächlich eingegangenen Überweisungsdaten noch einmal in der pushTAN-Nachricht mitgeschickt. Allerdings besteht analog zur Nutzung von mTAN mit einem Smartphone das Risiko, dass sowohl PC als auch Smartphone von Schadsoftware infiziert werden. Zur Verringerung dieser Gefahr verweigert die pushTAN-App auf gerooteten Geräten ihre Funktion.

Mit einem TAN-Generator können TANs elektronisch erzeugt werden. Hierfür gibt es mehrere unterschiedliche Verfahren.

Bei diesem Verfahren erhält der Nutzer von seinem Kreditinstitut einen TAN-Generator ohne Zifferntasten. Sobald die Kundenkarte (z. B. eine Maestro-Card oder eine V-Pay-Karte) in den Generator eingesteckt wird, können auf Knopfdruck TANs erzeugt werden. Diese TANs können nur der Reihe nach im Online-Banking eingegeben werden. Werden beispielsweise 5 TANs generiert, jedoch nur die zuletzt erzeugte TAN für eine Transaktion verwendet, sind die vorherigen vier TANs ungültig. Das Kreditinstitut kann als Herausgeber der Kundenkarte die TANs überprüfen.

Die Generierung der TANs erfolgt über den Chip auf der Kundenkarte des Kunden. Der TAN-Generator selbst ist nicht auf den Kunden individualisiert. Bei einem Verlust der Karte können mit einem beliebigen TAN-Generator gültige TANs erzeugt werden. Da für Transaktionen auch die PIN notwendig ist, stellt dies eine überschaubare Gefahr dar.

Dieses Verfahren ist anfällig für Phishing- bzw. Man-in-the-middle-Angriffe, da die generierten TANs für beliebige Transaktionen verwendet werden können. Eine Auftragsbindung findet nicht statt. Die Verbreitung dieses Verfahrens ist gering.

Im Dezember 2006 hat die BW-Bank dieses Verfahren eingeführt. Kunden erhalten einen individualisierten TAN-Generator, der unter Einbeziehung eines geheimen Schlüssels, der aktuellen Uhrzeit und der Kontonummer des Empfängers eine temporär gültige TAN erzeugt. Die Empfängerkontonummer muss über das Ziffernfeld des TAN-Generators eingegeben werden. Weitere Personendaten werden nicht verwendet.

Dieses Verfahren schützt vor Phishing- bzw. Man-in-the-middle-Angriffen, sofern die korrekte Empfängerkontonummer eingegeben wird. Die manuelle Eingabe der Kontonummer ist wenig komfortabel. Bei einem Verlust des TAN-Generators können weiterhin TANs generiert werden. Da für Transaktionen auch die PIN notwendig ist, stellt dies eine überschaubare Gefahr dar.

Bei weiteren Banken (z. B. der Santander Consumer Bank) muss statt der Empfängerkontonummer eine für die jeweilige Überweisung generierte Kontrollnummer (Startcode) eingegeben werden. Im Gegensatz zur Eingabe des Empfängerkontos ist dieses Verfahren für Man-in-the-middle-Angriffe anfällig, da die Empfängerkontonummer nicht kontrolliert wird.

Einige TAN-Generatoren anderer Banken erstellen zeitlich begrenzt gültige TANs nur anhand eines individuellen geheimen Schlüssels und der Uhrzeit, ohne Eingabe einer Kontrollnummer oder eines Empfängerkontos. Hier ist weder Kartenlesegerät noch Tastatur am Generator erforderlich. Da keinerlei Auftragsbezug besteht, ist das Verfahren ähnlich Phishing-anfällig wie das vorher beschriebene sm@rt-TAN-Verfahren. Allerdings müsste die unerwünschte Transaktion innerhalb des kurzen Gültigkeitszeitraums der TAN veranlasst werden.

Zahlreiche Volks- und Raiffeisenbanken sowie viele Sparkassen bieten dieses Verfahren an.

Es wurde 2006 auf der CEBIT vom Deutschen Genossenschafts-Verlag in der ersten Version (HHD 1.2) vorgestellt. Mit den folgenden Versionen HHD 1.3 und HHD 1.4 (ab 2010) wurde danach unter der Bezeichnung „SmartTAN optic“ bzw. „chipTAN comfort“ die Möglichkeit geschaffen die Auftragsdaten nicht mehr nur manuell, sondern auch per optischer Schnittstelle zu übertragen (siehe chipTAN comfort/SmartTAN optic (Flickering)) sowie jeweils die Menge der Auftragsdaten, die an das Gerät übertragen werden und in die TAN-Berechnung mit einfließen, erhöht.

Nachdem eine Überweisung im Online-Banking erfasst wurde, wird ein (Start-)Code am Bildschirm angezeigt. Nun muss die persönliche Bankkarte in den TAN-Generator eingesteckt werden und der (Start-)Code über das Ziffernfeld des TAN-Generators eingetippt und bestätigt werden. Bei den meisten Instituten müssen danach noch die Empfängerkontonummer (bzw. Teile davon) sowie in manchen Versionen der Betrag der Überweisung eingetippt werden. Ist dies nicht der Fall (wie etwa bei der Wüstenrot Bank), ist der Schritt am TAN-Generator ohne Eingabe zu bestätigen.

Nach Eingabe der TAN im Online-Banking wird der Auftrag ausgeführt. Durch die Eingabe der Kontonummer und des Betrags am TAN-Generator sowie deren Miteinbeziehung in die TAN-Berechnung ist das Verfahren vor Phishing bzw. Man-in-the-middle-Angriffen geschützt.

Auch wenn die Geräte verschiedener Hersteller anders heißen und aussehen, sind sie innerhalb derselben HHD-Version technisch gesehen gleich und funktionieren mit Karten verschiedener Kreditinstitute.

Bei einem Verlust der Bankkarte können durch den Finder mit einem beliebigen TAN-Generator, der für dieses Verfahren geeignet ist, weiterhin TANs generiert werden. Da für Transaktionen auch die Online-Banking-Zugangsdaten notwendig sind, stellt dies eine überschaubare Gefahr dar. Sobald eine Bankkarte gesperrt wird (z. B. bei Diebstahl), werden mit dieser Karte erzeugte TANs vom Kreditinstitut abgelehnt.

Dieses neue, optische Verfahren findet in Deutschland eine zunehmende Verbreitung. Viele Sparkassen und Volks- und Raiffeisenbanken sowie die Postbank setzen es bereits ein. Die Sparkassen und die Postbank nennen es „chipTAN comfort“, während die Volksbanken die Bezeichnungen „Sm@rtTAN plus“ und „SmartTAN optic“ verwenden. In Österreich ist das System hingegen unter dem Namen „cardTAN“ bekannt, wobei cardTAN einen technisch eigenständigen Standard darstellt.

Die Kunden erwerben hierbei einen TAN-Generator mit Ziffernfeld und Karteneinschub. Auf der Rückseite des TAN-Generators befinden sich fünf optische Sensoren. Nachdem eine Überweisung im Online-Banking erfasst wurde, erscheint am Bildschirm eine Grafik, die fünf flackernde Schwarz-Weiß-Flächen enthält. Nun muss die persönliche Bankkarte in den TAN-Generator eingesteckt werden. Sobald der TAN-Generator am Bildschirm an die Grafik gehalten wird, erfolgt eine Datenübertragung durch Lichtsignale. Hierbei werden Teile der Empfängerdaten übertragen, bei einer Einzelüberweisung beispielsweise der (Start-)Code, die Empfängerkontonummer sowie der Überweisungsbetrag. Auf dem Display des TAN-Generators werden im Anschluss die übermittelten Daten zur Kontrolle und Bestätigung angezeigt. Der TAN-Generator errechnet nun eine auftragsbezogene TAN, die im Online-Banking eingegeben wird.

Das optische Übertragungsverfahren "Flickering" mit seinen schnell blinkenden weißen Balken auf schwarzem Hintergrund ist für Personen mit Epilepsie nicht geeignet, da der optische "Flickercode" epileptische Anfälle auslösen kann. Auf diese Gesundheitsgefahr wird in der Gebrauchsanleitung des optischen TAN-Generators explizit hingewiesen.

Dieses Verfahren schützt vor Phishing- bzw. Man-in-the-Middle-Angriffen, sofern die im Display angezeigten Daten vor der Bestätigung auf ihre Richtigkeit geprüft werden. Durch die optische Übertragung müssen keine Auftragsdaten am TAN-Generator eingegeben werden. Da für Transaktionen zusätzlich auch die Online-Banking-Zugangsdaten notwendig sind, stellt dabei auch der Verlust der Bankkarte eine überschaubare Gefahr dar. Sobald die Bankkarte gesperrt wird (z. B. bei Diebstahl), werden mit dieser Karte erzeugte TANs vom Kreditinstitut ohnehin abgelehnt.

Angriffsmöglichkeiten zeigen sich in Verbindung mit Sammelüberweisungen. Hier wird im Display des TAN-Generators nur die Anzahl der Posten sowie der Gesamtbetrag angezeigt. Auf die Anzeige der einzelnen Empfängerkontonummern wird verzichtet, da dies bei umfangreichen Sammelüberweisungen nicht darstellbar wäre. Bei einem Angriff könnten nun die einzelnen Posten der Sammelüberweisung verändert werden. Solange deren Anzahl und der Gesamtbetrag gleich bleibt, wäre die Manipulation im Display des TAN-Generators nicht erkennbar.

Eine weitere Angriffsmöglichkeit wäre die Umwandlung einer Einzelüberweisung in eine Sammelüberweisung mit einem Posten. Der Nutzer würde in diesem Fall im Display des TAN-Generators nur die Anzahl der Posten (hier „1“) und den Betrag angezeigt bekommen. Nachdem dies bekannt wurde, haben die deutschen Sparkassen die Displayanzeige bei Sammelüberweisungen mit nur einem Posten umgestellt. In diesem Fall werden trotz Sammelüberweisung die Empfängerkontonummer sowie der Betrag angezeigt. Bei Volksbanken im GAD-Umfeld sind hingegen Sammelüberweisungen mit nur einem Posten nicht zulässig und werden abgewiesen.

Kritisiert wird bei diesem Verfahren allerdings von einigen Benutzern, dass das Gerät bei unterschiedlichen Displaytypen den Flickercode nicht zu erkennen scheint. Hier hilft es dann oftmals, das Gerät in einem leicht schrägen Winkel (insb. bei S-PVA-Displays) an den Bildschirm zu halten oder die Bildschirmhelligkeit zu erhöhen. Weiterhin kann es helfen, die Flicker-Geschwindigkeit zu verändern und direkte Einstrahlung von Lichtquellen auf dem Monitor zu verhindern. Auch kann es helfen, die Hardware-Beschleunigung im Browser zu deaktivieren.

In allen Fällen ist die Sicherheit mindestens so hoch wie bei Benutzung von TAN- oder iTAN-Listen. Kontrolliert der Benutzer aber die angezeigten Daten im Display des TAN-Generators und benutzt keine Sammelüberweisung, ist die Sicherheit deutlich erhöht.

Allerdings wird aufgrund der Sicherheitsmaßnahmen von chipTAN inzwischen versucht, den Benutzer mittels „Social Engineering“ dazu zu bewegen, von sich aus freiwillig eine Überweisung zu Gunsten der Betrüger zu tätigen.

Beim photoTAN-Verfahren werden die Transaktionsdaten verschlüsselt als mehrfarbige Mosaikgrafik auf dem Bildschirm angezeigt. Mit einer entsprechenden Smartphone-App wird dieser Code eingelesen und entschlüsselt, die Transaktionsdaten zur Kontrolle auf dem Smartphone angezeigt und die zugehörige TAN generiert. Die so generierte TAN wird dann zur Freigabe der Transaktion eingegeben.

Zur Initialisierung des Verfahrens muss die App auf dem Smartphone ein eindeutiges Erkennungsmerkmal bekommen und dieses Merkmal muss mit dem Bankkonto verbunden werden. Dazu erhält man i. A. einen Brief mit einem Freischaltcode oder einer Freischaltgrafik, welche man mit der App einscannen muss. Gibt man nach der Initialisierung bei einer Überweisung die von der App generierte TAN im Online-Banking-Portal ein, so kann dadurch bewiesen werden, dass die Bestätigung von einer registrierten Smartphone-App erzeugt wurde. Alternativ zur App bieten einige Banken auch spezielle TAN-Generatoren (Lesegeräte) für photoTAN an. Auch hier ist eine initiale Verknüpfung des Gerätes mit dem Bankkonto erforderlich.

In Deutschland wurde das vom britischen Spin-off der Universität Cambridge, Cronto Ltd., entwickelte Verfahren Anfang 2013 von der Commerzbank und deren Direktbank Comdirect eingeführt und wird ebenfalls von den Schweizerischen Raiffeisenbanken sowie der Deutschen Bank eingesetzt. Darüber hinaus wurde es von der niederländischen Rabobank zusammen mit einer weiter entwickelten Version des Lesegeräts ("Rabo Scanner") eingeführt. Cronto Ltd. wurde 2013 von VASCO Data Security International Inc. übernommen.

Ähnlich arbeitet das bereits seit Ende 2012 bei der 1822direkt ausgerollte QR-TAN-Verfahren, das zusammen mit der LSE Leading Security Experts GmbH entwickelt wurde. Dieses basiert auf dem offenen OCRA-Algorithmus (RFC6287) und transportiert die signierten Transaktionsinformationen in einem QR-Code. Der Rollout, bei dem das Smartphone registriert wird, geschieht über die getrennten Kanäle einer HTTPS-Verbindung und eines Briefversands per Post.

Beide Verfahren können im Vergleich zu iTAN aufgrund der PC-unabhängigen Kontrollmöglichkeit der Überweisungsdaten als geschützter gegen Angriffe von Trojanern oder Man-in-the-Browser-Attacken angesehen werden, solange die Sicherheit des Smartphones bzw. des photoTAN-Lesegeräts gewährleistet ist. Für die Verwendung ist eine Internetverbindung des Smartphones nicht zwingend erforderlich, zwischen Onlinebanking und Smartphone besteht keine Verbindung.
Werden allerdings beim mobilen photoTAN-Banking sowohl die Banking- wie auch die photoTAN-Funktion via App auf einem einzigen Smartphone zusammengeführt, sodass die photoTAN-App und die Banking-App direkt miteinander kommunizieren, lassen sich die Überweisungen wieder manipulieren. Solche Apps werden zum Beispiel von der Deutschen Bank, der Commerzbank und der Norisbank bereitgestellt.

Für die Banken haben beide Verfahren den Vorteil, dass trotz hoher Sicherheit die Investitions- und Transaktionskosten verhältnismäßig niedrig sind, da keine eigenen TAN-Generatoren erforderlich sind und z. B. im Unterschied zum mTAN-Verfahren keine SMS versandt werden müssen.

Generell kann ein Betrüger versuchen, eine TAN zu erraten. Bei einer 6-stelligen TAN ist die Wahrscheinlichkeit 1 : 1.000.000, eine bestimmte TAN mit einem Versuch zu erraten. Wenn der Kunde zur Legitimation aus einer Liste von beispielsweise 100 TANs eine beliebige auswählen kann, ist die Wahrscheinlichkeit für den Betrüger, eine dieser TANs zu erraten, 1 : 10.000. Wenn der Betrüger drei Versuche hat, ergibt sich damit eine Erfolgswahrscheinlichkeit von ungefähr 0,03 %.

Anstatt zu raten, kann der Betrüger versuchen, TANs auszuspähen. Des Öfteren wurde bereits versucht, durch Phishing in den Besitz von Transaktionsnummern zu kommen. In einer Variante wird hierbei dem Bankkunden eine E-Mail mit einem Link auf eine falsche Internetadresse der Bank geschickt. Der Text der E-Mail bzw. Internetseite soll den Kunden veranlassen, auf dieser falschen Internetseite seine Kontonummer, seine PIN und auch noch nicht verwendete TANs einzugeben.

Phishing ist erfolgreich, weil viele Nutzer des Online-Bankings nicht genau überprüfen, ob die im Browser angezeigte Seite auch wirklich von der gewünschten Bank stammt. Beim klassischen TAN-Verfahren ist der Betrüger erfolgreich, wenn er Kontonummer, PIN und eine beliebige, noch nicht benutzte TAN erfährt, d. h. der Kunde diese Daten auf der gefälschten Internetseite einträgt. Beim iTAN-Verfahren hat der Betrüger eine geringe statistische Wahrscheinlichkeit, eine erbeutete iTAN verwerten zu können.

Die Sicherheit des iTAN-Verfahrens ist differenziert je nach Bedrohungstyp zu sehen. Während beim klassischen TAN-Verfahren im Mittel 50 TANs auf einem TAN-Bogen gültig sind (der TAN-Bogen enthält 100 TANs, von denen im statistischen Mittel die Hälfte bereits verbraucht sind), ist beim iTAN-Verfahren jeweils nur die einzige TAN gültig, welche die Bank während der Transaktion abfragt. Es bietet daher einen guten Schutz gegen Phishing, sofern der Bankkunde nicht zu viele iTANs in eine Phishing-Seite eingibt. Es bietet keinen Schutz gegen Man-In-The-Middle-Angriffe, worauf verschiedene Veröffentlichungen hingewiesen haben.

Entsprechende Angriffe werden per Trojaner, z. B. durch Ändern des verwendeten DNS-Servers sowie der SSL-Root-Zertifikate, oder durch direktes Eingreifen des Trojaners in die Programme auf dem Rechner, etwa per Man-In-The-Browser-Angriff, durchgeführt. Hierbei werden die tatsächlichen Überweisungsdaten im Hintergrund durch die der Betrüger ersetzt sowie auch die Kontoübersichtsseiten so manipuliert, dass der Betrug nicht auffällt.

Die Sicherheit des klassischen TAN-Verfahrens sowie des iTAN-Verfahrens ist begrenzt, da die TAN nicht direkt mit dem Inhalt der Überweisung verknüpft wird. Dadurch kann die Bank nicht alleine mit Hilfe der übermittelten TAN entscheiden, ob der Auftrag korrekt ist. Dagegen bietet das mTAN-Verfahren einen besseren Schutz gegen Phishing- und Man-in-the-middle-Angriffe. Bei einigen Geldinstituten wird dem Kunden beim Versand der mTAN der Überweisungsbetrag und die Empfänger-Kontonummer im Text mitgesendet. Dadurch hat der Kunde die Möglichkeit, diese Daten mit der bei der Bank eingegebenen Überweisung zu vergleichen und einen möglichen Betrug zu bemerken. Dabei muss der Kunde beim Empfang der SMS aber nicht nur die mTAN lesen, sondern auch tatsächlich Betrag und Empfänger-Kontonummer überprüfen.

Allerdings wurde das mTAN-Verfahren bereits durch parallele Schadsoftware-Infektionen von PC und Mobiltelefon gebrochen. Entsprechende mobile Schadprogramme sind für Symbian OS, Blackberry OS, Windows Mobile und Android bekannt. Darüber hinaus wurde mTAN mehrfach dadurch ausgehebelt, dass sich die Angreifer unter Vorspiegelung falscher Tatsachen Ersatz- oder Zweit-SIM-Karten für die Handynummern der Opfern von den entsprechenden Mobilfunk-Providern ausstellen ließen.

Auch Verfahren, die auf elektronischen Unterschriften basieren, wie das HBCI-Verfahren mit Chipkarte, bieten keinen höheren Schutz. Hier wird aus dem Inhalt der Überweisung mit kryptographischen Verfahren eine Prüfsumme berechnet und an die Bank übermittelt. Die Bank kann hier anhand der Prüfsumme feststellen, ob die Überweisung vom Kunden kommt oder nicht, und auch, ob die Überweisung seit dem Signiervorgang unverändert ist. Sofern der PC des Kunden mit Schadsoftware (Trojanern oder Viren) infiziert ist, könnten Daten bereits manipuliert an den Kartenleser geleitet werden. Sofern nicht ein Secoder-fähiges Kartenlesegerät verwendet wird und die jeweilige Bank sowie die Homebanking-Software die Secoder-Erweiterung mit den Secodervisualisierungstexten für HBCI unterstützen, ist nicht sichtbar, welche Daten zur elektronischen Signatur vorliegen.

Neuere TAN-Verfahren mit einem TAN-Generator (z. B. chipTAN comfort/smartTAN optic) verknüpfen die TAN mit den Auftragsdaten. Die damit erzeugten TANs können nicht für abweichende (betrügerische) Überweisungen genutzt werden. Sofern Überweisungsdaten durch Schadsoftware verändert werden, sind diese veränderten Werte auf dem Display des TAN-Generators sichtbar. Der Bankkunde kann die Transaktion in diesem Fall abbrechen. Da der TAN-Generator nicht an den PC angeschlossen wird, ist eine Manipulation der Displayanzeige ausgeschlossen. Alle bisherigen, technischen Angriffsarten können damit wirkungsvoll verhindert werden. Einzige Gefahrenquelle bleibt der sorglose Umgang mit den angezeigten Auftragsdaten im Display des TAN-Generators.

Allerdings wird aufgrund der hohen Sicherheit von Verfahren mit dedizierten Signaturgeräten mit eigenem Display zur Kontrolle der Überweisungsdaten wie chipTAN oder photoTAN inzwischen versucht, den Benutzer mittels „Social Engineering“ dazu zu bewegen von sich aus, freiwillig eine Überweisung zu Gunsten der Betrüger zu tätigen.

Das PIN/TAN-Verfahren wurde bereits zu Zeiten des Btx-Onlinebankings benutzt. Die Idee wird dem technischen Leiter der damaligen Verbraucherbank (heute Norisbank), Alfred Richter, zugeschrieben. Zunächst war dieser Zugriffschutz 1976 nur für den internen Gebrauch gedacht, ab 1979 führte die Bank Onlinebanking ein.



</doc>
<doc id="5214" url="https://de.wikipedia.org/wiki?curid=5214" title="Tee (Begriffsklärung)">
Tee (Begriffsklärung)

Tee steht für:
Tee ist der Familienname bzw. Künstlername folgender Personen:
tee steht für:
TEE steht als Abkürzung für:
Siehe auch: 


</doc>
<doc id="5216" url="https://de.wikipedia.org/wiki?curid=5216" title="Treuhänderverfahren">
Treuhänderverfahren

Das Treuhänderverfahren ist eine im Versicherungsvertragsgesetz geregelte Möglichkeit des deutschen Rechts, das einer Versicherungsgesellschaft erlaubt, Vertragsbedingungen mit ihren Kunden einseitig zu ändern. Im Treuhänderverfahren stimmt nicht der Vertragspartner (Versicherungsnehmer) zu, sondern ein von der Versicherung bestellter unabhängiger Treuhänder. Dieser genehmigt die Änderungen anstelle des Vertragspartners. Der Versicherungsnehmer als Vertragspartner hat auf die Änderungen des Versicherungsvertrags keinen Einfluss.
Derartige Änderungen sind nur möglich, wenn sie durch eine gesetzliche Regelung oder vertragliche Vereinbarung zulässig sind.

"Siehe auch:" Treuhänder (Versicherungswesen)


</doc>
<doc id="5217" url="https://de.wikipedia.org/wiki?curid=5217" title="Tamanrasset">
Tamanrasset

Tamanrasset (, Tuareg ⵜⴰⵎⴰⵏⵖⴰⵙⴻⵜ "Tamanɣaset", umgangssprachlich "Tam") ist die größte Oase im Süden Algeriens und als Kommune mit rund 93.000 Einwohnern Hauptstadt der Provinz Tamanrasset.

Tamanrasset liegt auf etwa 1400 Meter Höhe am Rande des Ahaggargebirges.

Die Kommune besteht neben dem Hauptort Tamanrasset noch aus den Ortschaften "Amsel, Oukou, Tit, Assekrem, Tahifet, Tarhenanet, Tagmart, Talan Teidit, Efak, Ihelfène, Taghahaouhaout, In Dalag "und" Izernène".

Die Tuareg bilden die größte Bevölkerungsgruppe. Diese wohnen bis heute noch verstreut, während das Geschäftsleben in der Stadt weitgehend von zugewanderten Arabern und Mozabiten aus dem Norden geprägt wird.

Die Oase Tamanrasset diente schon vor Eintreffen der ersten Europäer als Karawanenstützpunkt. Um 1885 ließen die durch diese Region ziehenden Tuareg von Haratin, die sie für diese Arbeit herholten, Gärten anlegen. Daraus entstand eine permanente Siedlung, die 1910 bereits 40 gemauerte Häuser zählte.

In der französischen Kolonialzeit wurde 1919 eine Einheit französischer Kamelreiter von "Fort Motylinski", benannt nach Adolphe de Calassanti Motylinski, dem Erforscher der Berbersprachen, nach Tamanrasset verlegt. Dort errichteten die Soldaten ein Fort, das sie zu Ehren eines Generals der Kamelreiter "Fort Laperrine" nannten. Diesen Namen trug auch die um das Fort gelegene, stetig wachsende Siedlung bis zur algerischen Unabhängigkeit 1962. In diesem Jahr zählte Tamanrasset bereits geschätzt 3.000 Einwohner.

Als 1974 die Provinz Tamanrasset (Wilāyat Tamanrāsat) geschaffen wurde, wurde die namengebende Stadt Provinzhauptstadt. Bis 1978 wurde die 1000 km lange Straße durch das Ahaggar, die Tamanrasset mit den nördlich gelegenen Landesteilen Algeriens verbindet, asphaltiert. Seither ist Tamanrasset gut erreichbar.

Nach Medienberichten ist Tamanrasset "eines der wichtigsten Verteilerzentren für Migranten auf dem Weg nach Europa".

Tamanrasset ist heute ein wichtiges Logistikzentrum für den Transsaharaverkehr zwischen Algerien und Niger. Sie liegt an der Transsahararoute Algier–In Salah–Tamanrasset–Agadez, einem Teil des Algier-Lagos-Highway, der zu den Trans-African Highways gehört. Die Stadt verfügt über die besten Versorgungsmöglichkeiten der Region, mehrere KFZ-Werkstätten und einen Flughafen.

In Tamanrasset befindet sich das Hauptquartier der "Militärregion Sahara" des algerischen Heeres.

Auch der Sahara-Tourismus ist für die Stadt von großer Bedeutung. Sie ist beliebter Ausgangspunkt für Exkursionen ins benachbarte Ahaggargebirge, wo besonders die Eremitage von Charles de Foucauld gerne angefahren wird.

Eine lokale Sehenswürdigkeit ist das Mausoleum des Tuaregfürsten Moussa ag Amastan im Osten der Stadt.

Aufgrund der Höhenlage hat die Oase ein für die Sahara relativ gemäßigtes Klima. Die durchschnittliche Jahresniederschlagsmenge beträgt 46 mm (also mehr als in anderen Wüstenstädten wie In Salah), verteilt auf 16 Regentage. Die durchschnittliche Höchsttemperatur beträgt 28,5 °C, die durchschnittliche Tiefsttemperatur 14 °C.



</doc>
<doc id="5218" url="https://de.wikipedia.org/wiki?curid=5218" title="Tuareg">
Tuareg

Die Tuareg (Singular: Targi (männlich), Targia (weiblich); zu dieser Eigenbezeichnung siehe Abschnitt Etymologie) sind ein zu den Berbern zählendes Volk in Afrika, dessen Siedlungsgebiet sich über die Wüste Sahara und den Sahel erstreckt. Von den Tuareg werden neben ihrer eigenen Sprache mehrere Verkehrssprachen gesprochen, von Songhai über Arabisch und Hassania bis Französisch; ihre Schrift ist das Tifinagh. Sie lebten jahrhundertelang nomadisch im Gebiet der heutigen Staaten Mali, Algerien, Niger, Libyen und Burkina Faso und zählen heute, die Angaben schwanken stark, etwa 1,5 bis 2, nach Eigenangaben bis 3 Millionen Menschen. Seit der Mitte des 20. Jahrhunderts sind viele inzwischen sesshaft geworden. In den letzten Jahren kam es immer wieder zu Aufständen der Tuareg, die sich behindert fühlen, ihre hirtennomadische Lebensweise fortzuführen.

Das Wort "Tuareg" leitet sich von dem Wort "Targa", dem berberischen Namen für die Provinz Fezzan in Libyen, ab. Damit bezeichnete Tuareg ursprünglich die Bewohner des Fezzan. "Targa" ist ein berberisches Wort, das mit „Rinne“ oder „Kanal“, im weitesten Sinne auch mit „Garten“ übersetzt werden kann (Chaker, Claudot-Hawad, Gast 1984:31). Lokaler Ansicht zufolge bezeichnet Targa nicht den gesamten Fezzan, sondern lediglich die Region zwischen den Städten Sebha und Ubari und wird arabisch als „gutes Land“ "(bilad al-khayr)" bezeichnet. Gemeint ist damit das fruchtbare Wadi al-Haya (vormals Wadi al-Ajal), das den gesamten Süden Libyens mit agrarischen Produkten versorgt (Kohl 2007:47).

Die bis heute weit verbreitete arabische Volksetymologie: "Tawariq" (Einzahl: "Tarqi"), „das von Gott verlassene Volk“, dient dazu, eine arabische Überlegenheit über die Tuareg auszudrücken. Grund dafür sind die liberalen religiösen Auffassungen der Tuareg, die von Vertretern einer strengen muslimischen Doktrin als verwerflich angesehen werden.

Der Name Tuareg hat sich seit der Kolonialzeit im deutschen, frankophonen und angloamerikanischen Sprachraum eingebürgert. Die Tuareg selbst bezeichnen sich nicht mit diesem Namen. Die emische Bezeichnung der Tuareg lautet "Imajeghen" im Niger, "Imuhagh" in Algerien und Libyen und "Imushagh" in Mali. Das "gh" wird wie das deutsche Rachen-"r" ausgesprochen und die Betonung liegt auf der ersten Silbe. Diese Eigenbezeichnung (Endonym) bezieht sich auf Menschen mit freier Abstammung, die noble Qualitäten besitzen. Damit wird auf den Ehrenkodex "(asshak)" der Sahara- und Sahelbewohner hingewiesen.
Alle drei Begriffe gehen auf dieselbe Wurzel zurück und sind lediglich infolge der dialektalen Ausformung unterschiedlich. Neben dieser Eigenbezeichnung Imajeghen/Imuhagh/Imushagh findet der Name "Kel Tamasheq," „die Leute, die Tamasheq sprechen“, Verwendung.

In der Literatur werden die Tuareg als "Kel Tagelmust", „die Leute des Gesichtsschleiers“ oder "Das blaue Volk" bezeichnet, da sie mit Indigo gefärbte Kleidung tragen. Beide Begriffe werden von den Tuareg nicht verwendet.

Die Tuareg sind ein Berbervolk. Sie sollen Nachkommen der altberberischen Garamanten sein, die um die Zeitenwende in den Regionen des heutigen Südtunesiens und Libyens ein kriegerisches Kamelnomadentum entwickelt hatten. Im 11. Jahrhundert wurden sie von arabischen Beduinen vom Stamm der Banū Hilāl aus dem Fessan vertrieben und zogen sich abgedrängt in die Gebiete der zentralen Sahara, insbesondere das Tassili n'Ajjer, Aïr und Ahaggar zurück, wo sie seit dieser Zeit leben. Insoweit konnten sie sich einer Arabisierung ihrer Kultur (Schrift, Sprache, Handwerkskultur, matrilineare Sozialstrukturen) entziehen. Gleichwohl übernahmen sie den Islam. Bei dieser Abdrängung vertrieben sie ihrerseits das Wüstenvolk der Tubbu in das Tibestigebirge. Nach dem Untergang des Songhaireichs im Zuge des marokkanischen Eroberungskrieges im 16. Jahrhundert drangen die Tuareg zunehmend auch in die Sahelzone ein und errangen in der Folgezeit unter anderem die Kontrolle über Timbuktu und das Sultanat Aïr mit Sitz in Agadez.

Die Tuareg mussten immer wieder um das Recht kämpfen, als freies Volk anerkannt zu werden und nach ihrer Tradition leben zu dürfen. Im 19. Jahrhundert leisteten sie der vordringenden Kolonialmacht Frankreich in der Saharazone von Westafrika lange Zeit heftigen Widerstand. Erst 1917 wurde ein Friedensvertrag geschlossen. Mit dem Ende der französischen Kolonialherrschaft in Westafrika 1960 wurde das Siedlungsgebiet der Tuareg zwischen den nunmehr unabhängigen Staaten Mali, Niger und Algerien aufgeteilt, wobei kleinere Gruppen der Tuareg zudem in Libyen und Burkina Faso leben. 1990 bis 1995 revoltierten die Tuareg in Mali und Niger aufgrund der Unterdrückung und Ausgrenzung durch die jeweiligen Regierungen. Ein Führer des Tuareg-Aufstandes war Mano Dayak. Mitte der 1990er Jahre wurden die Aufstände nach der Unterzeichnung von Friedensverträgen beendet. 2007 beschuldigte die neu gegründete Tuareg-Rebellengruppe Bewegung der Nigrer für Gerechtigkeit die Regierung, den Friedensvertrag nicht einzuhalten. Außerdem fordern sie einen Anteil des Gewinns aus dem Uranabbau nordwestlich von Agadez für die Tuareg (Uranmine bei Arlit).

Infolge des Bürgerkriegs in Libyen im Jahr 2011 verschärfte sich die Sicherheitslage im Norden Malis, nachdem Tuareg, die auf Seiten Muammar al-Gaddafis kämpften, aus Libyen vertrieben wurden. Die als Nationale Bewegung für die Befreiung des Azawad (MNLA) auftretenden bewaffneten Gruppen drangen ab Ende 2011 über Niger nach Mali ein und brachten Gebiete im Norden des Landes unter ihre Kontrolle. Ob sie dabei in Verbindung zu Al-Qaida im Maghreb stehen, ist umstritten. Soldaten der malischen Streitkräfte warfen der Regierung von Präsident Amadou Toumani Touré Unfähigkeit bei der Bekämpfung des Aufstandes der Tuareg im Norden des Landes vor und übernahmen durch einen Putsch im März 2012 die Macht. Die MNLA nutzte die Situation und eroberte in den Tagen darauf bis Anfang April alle Städte im Gebiet Azawad. Am 6. April rief sie einseitig den unabhängigen Staat Azawad aus.
Als Nomadenvolk, das bis zur Kolonialzeit in mehrere politische Konföderationen unterteilt war, besitzen die Tuareg keine Hauptstadt. Am ehesten kann man Agadez im Niger, mit dem Sitz des Sultans von Aïr als einen zentralen Ort bezeichnen. Für die nördlichen Tuareg (Kel Ajjer und Kel Ahaggar) spielten die südalgerische Oase Djanet und die südlibysche Oase Ghat in früheren Zeiten eine ähnliche Rolle.
Der heutige Hauptort des Ahaggar-Gebirges, Tamanrasset, entstand erst nach 1900, als sich der französische Missionar Charles de Foucauld in der Gegend niederließ. Erst nach der endgültigen Eroberung des Gebirges durch die französischen Kolonialtruppen wuchs der Ort und wurde zum offiziellen Sitz des Amenokal (Königs) der Kel Ahaggar.

Die Kultur der Tuareg wurde von den Afrikaforschern Heinrich Barth und Henri Duveyrier erforscht und ausführlich beschrieben.

Seit der ersten Wanderungswelle der Umayyaden von der Halbinsel nach Nordafrika arabisierte sich der Maghreb und Ägypten. Die Tuareg wurden über die Handelswege zu Muslimen, obwohl sie sich anfangs sehr stark gegen eine Missionierung wehrten, denn die den Islam verbreitenden Araber waren ihre angestammten Feinde. Tuareg sind Muslime der Malikiten-Rechtsschule (wie fast ganz Nordafrika), und gehören diversen Bruderschaften an. An die Regeln des Islams halten sie sich überwiegend streng. Ihren Glauben an gute und böse Geister "(Kel Essuf)" konnten sie in die muslimische Religion einfügen, da auch der Islam das Vorhandensein von Geistern im Koran erwähnt. Zu ihrer Abwehr sind für sie Amulette, in Leder eingebundene magische Zeichen, unverzichtbar. Die Frauen tragen als Amulett-Schmuck die "Chomeissa," eine abstrahierte Form der "Hand der Fatima."

Wie in der gesamten Sahelzone ist zeremonielles Teetrinken ein wichtiger Bestandteil der Alltagskultur. Es werden drei unterschiedlich starke Aufgüsse unterschieden. Ein Gast, der drei Gläser ausgetrunken hat, steht unter dem Schutz der Tuareg.

Die Tuareg sind nomadische Viehzüchter mit einem komplex abgestuften hierarchischen Sozialmodell:

Einige Stämme hatten bis zur kolonialen Eroberung die politische und wirtschaftliche Macht inne. Sie stellten den König, den "Amenokal." Daneben gibt es zugewanderte Stammesgruppen, die in der Literatur mit Begriffen des feudalen Europas beschrieben werden, die "Imaghad." Sie mussten in vorkolonialer Zeit Abgaben liefern, kooperierten jedoch in politischen Belangen mit den Imajeghen/Imuhagh/Imushagh und wurden von ihnen beschützt. "Iklan," „Sklaven“ spielten im traditionellen System eine wesentliche wirtschaftliche Rolle. Sie stellten das Eigentum einer Familie dar, wurden jedoch als fiktive Verwandte integriert. Sklaven konnten freigelassen werden und wurden dann mit unterschiedlichen Termini bezeichnet (unter anderem "Iderafan," "Ikawaren," "Izzegharen"). Die Handwerker und Schmiede "(Inadan)" stellen eine eigene soziale Gruppe dar, die als Personen ohne Scham und Anstand gelten, jedoch für die Wirtschaft unentbehrlich waren, da sie Arbeitsgeräte, Werkzeuge, Waffen, Küchenutensilien und Schmuck herstellten.
Der Vollständigkeit halber seien die "Ineslimen", die Korangelehrten genannt, obwohl sich der Begriff auf alle Muslime bezieht.

Dieses Sozialsystem spielt bis heute eine Rolle und weist den jeweiligen Klassen Wert- und Moralvorstellungen zu, die für die einzelnen Gruppenmitglieder einzuhalten sind 

Die Frau empfängt die Gäste und überwacht die Zubereitung des Tees. Sie entscheidet, wen sie heiratet und sie darf ihren Mann verstoßen. Eine Ehescheidung stellt in dieser Kultur keine Schande dar. Ebenso ist es ihr erlaubt, vor einer Ehe verschiedene Liebhaber gehabt zu haben. Nach einer Scheidung verbleiben die Kinder bei der Frau. Die Söhne der Schwester werden von Männern in der Weitergabe ihrer Besitztümer bevorzugt, da man hier von einer engeren Verbindung ausgeht, als es bei eigenen Söhnen der Fall ist. Man spricht hier von Matrilinearität, womit aber nicht das Matriarchat gemeint ist.

Die verlorene oder versunkene "Oase Gewas" ist in der Tuareg-Kultur ein wichtiges Symbol. Sie steht für die Sehnsucht nach einer vollkommenen, paradiesischen Welt voller Reichtümer und Überfluss. Dieser imaginäre Gegenentwurf zur unbarmherzigen und kargen Wirklichkeit der Wüste dient als eine Art Trost. In der Vorstellung der Tuareg kann nur derjenige diesen legendären Ort finden, der nicht bewusst und gezielt nach ihm sucht.

Die Tuareg besitzen mit dem Tifinagh ein Schriftsystem, das jedoch nicht der alltäglichen Kommunikation dient. Auch in früheren Zeiten war die Kenntnis des Tifinagh auf die „Adelsclans“ (damit werden in der älteren Literatur die Imajeghen/Imushagh/Imushagh bezeichnet) beschränkt, wo sie den Kindern von ihren Müttern bzw. den alten Frauen beigebracht wurde. Heute verwenden viele Handwerker die Tifinagh-Schrift und gravieren ihre Namen auf selbst hergestellte Schmuckstücke.

→ "Artikel: Geschichte des Islam bei den Tuareg"

Die umherziehenden Tuareg leben in Zelten. Die Stämme der Sahelzone bauen ihre Mattenzelte aus Palmwedeln. Wenn die Stämme über längere Zeit an einem Ort bleiben, errichten sie "Seribas". Diese kleinen Hütten aus Schilf besitzen zwei Eingänge, welche für Durchzug sorgen. Als Windschutz dient eine Strohmatte, "Asabar" genannt, die man vor den Eingang stellt. In der Wüste haben die Tuareg Lederzelte, die aus 30–40 Schaf- und Ziegenfellen bestehen. Beim Aufbauen der Zelte errichten sie zuerst die Bogenkonstruktion, danach werden die Möbel platziert und anschließend Dach und Seitenwände darüber geworfen und bespannt.
Viele der Tuareg sind in die Städte gezogen. Andere Tuareg haben sich an Oasen eigene Siedlungen aufgebaut und betreiben Ackerbau. Die meisten Tuareg, die in einer Stadt ein neues Leben beginnen wollen, gehen nach Agadez, eine Stadt im Niger, in der schon viele von ihnen leben.

Die Kleidung der Nomaden ist geschlechtsspezifisch. Männer tragen eine schwarze, am Saum mit weißen oder gelben Fäden bestickte Hose "(ikerbey)," ein langes, bis zu den Knöcheln reichendes Übergewand "(tekatkat)" und den Gesichtsschleier, "tagelmust" oder "eshesh", um den Mund zu verdecken, da Körperöffnungen als unrein gelten. Außerdem ist es üblich, dass sich Männer vor Frauen verschleiern. Nach einer anderen Interpretation müssen sich die Männer, die häufig in der Wüste und in den Bergen unterwegs sind, vor den "Kel Eru", den Geistern der Toten, schützen, die versuchen, auf dem Weg über den Mund Besitz von den Lebenden zu ergreifen. Zur traditionellen Männertracht gehörte, zumindest an hohen Festtagen, auch eine hohe Mütze aus rotem Filz, die als "Tukumbut" bezeichnet wurde. Das Gesicht der Frauen ist, wie bei den Berbern, unbedeckt, sie tragen aber ein Tuch auf dem Kopf, das ihre Würde und ihre Ehre als erwachsene Frau verdeutlicht. Die Kopfbedeckungen der Männer und Frauen haben in erster Linie mit dem Ehrenkodex der Gesellschaft "(asshak)" zu tun und verdeutlichen Respekt, Anstand und Reserviertheit "(takarakit)" .

Frauen sind mit einem Wickelrock "(teri)" und einem lose flatternden, aufwändig bestickten Oberteil "(aftaq)" bekleidet oder tragen ein Wickelgewand "(tasirnest)." Gleich dem "tagelmust" der Männer besitzen Frauen eine Kopfbedeckung, "adeko" oder "afar," die ihre Ehre und Würde unterstreicht und das Frau-sein hervorhebt.

Die Kopfbedeckung der Tuareg beruht weniger auf muslimischen Normen als auf ihren eigenen Wertvorstellungen (vgl. Rasmussen 1995). Zudem bietet sie Schutz vor Sonne, Sand und Wind und verringert die Körperaustrocknung. "Aleschu", das indigoblau gefärbte und per Hand aus vielen Stoffbahnen zusammengenähte Stück Stoff, ist das Markenzeichen schlechthin, wurde jedoch erstmals vor knapp 150 Jahren aus Kano ins Gebiet der Tuareg importiert (Spittler, 2008). Jahrelanges Tragen färbt die Gesichtshaut bläulich, daher das Klischee vom „blauen Ritter der Wüste“.
Seit ungefähr einem Jahrhundert sind auch feine Musselinstoffe in weiß oder schwarz in Verwendung "(eschesch)," da durch die zunehmende Verarmung das "aleschu" nicht mehr bezahlbar war. Der Chèche (auch "Schesch" geschrieben) ist zwischen 2,5 Meter und 15 Meter lang, je nachdem, ob es sich um einen jungen Mann oder eine respektgebietende ältere Persönlichkeit handelt.

Verschiedene Getreidesorten, die von den Frauen angebaut oder gesammelt werden und aus denen sie das Brot der Tuareg, "Taguella," herstellen, bilden die Grundlage der Ernährung. Im Süden wird vor allem Hirse genutzt, im Norden Weizen, außerdem Gerste. Für die umherziehenden Tuareg ist die Kamelmilch wichtig. Ungekocht wird sie mit Wasser zur täglichen Mahlzeit getrunken. In "gedḥān" genannten Holzschalen offen stehengelassen, vergärt sie zu Sauermilch oder Dickmilch. Außerdem benötigen sie Ziegen-, Kuh- und Schafsmilch für Butter und Käse. Wenn die Tuareg auf Wanderschaft sind, gehört die Taguella (insbesondere in Algerien) zum Ernährungsstandard. Fleisch gibt es meist nur bei religiösen und familiären Festen. Die Tuareg verschmähen häufig Eier, Hühner und Fisch. Beeren, Früchte, Wurzeln und Samen werden von den Frauen und Kindern wie Getreide gesammelt. Der von Arabern eingeführte Grüntee ist den Tuareg fast unentbehrlich geworden. Das Ritual des Teekochens gehört zur Teekultur Nordwestafrikas.

Es gibt mehrere traditionelle Musikstile, zum Beispiel "Tendé", "Imzad" und "Esele." Tendé wird auch „Tanz der Kamele“ genannt. Dabei sitzen die Frauen dicht beisammen und singen, eine Vorsängerin trommelt auf dem mit Ziegenhaut bespannten Hirsemörser, der "Tendé" genannt wird, und die Männer umrunden die Frauen auf ihren Kamelen. "Imzad" ist eine einsaitige Fiedel, die vorzugsweise von älteren Frauen gespielt wird. Die dreisaitige Tuareg-Laute "Tahardent" ähnelt der viersaitigen "Tidinit" von Mauretanien, sie hat sich seit den 1960er Jahren in den Städten am Rand der Wüste ausgebreitet. "Esele" ist eine Art „Wüstendisco“, bei der junge Frauen die Männer mit rhythmischem Gesang und Händeklatschen zum Tanz auffordern. Gitarrenmusik ist sehr beliebt. Ein Fest ohne Gitarre ist in manchen Regionen undenkbar.

Hochzeiten und nationale oder religiöse Jahresfeste haben im Leben der Nomaden eine große Bedeutung. Das größte Fest ist die Hochzeit. Frauen und Männer tragen dabei edelste Kleidung, dazu gibt es als Musik meist "Tendé." So heißt auch ein weiteres Fest, bei dem ausschließlich die Musikart "Tendé" gespielt wird. Daneben gibt es viele regionale Feste.

In Djanet in Südalgerien wird jedes Jahr vor dem islamischen Aschura-Tag das zehntägige "Sebiba-"Tanzfest veranstaltet. Bianou ist ein ähnliches Neujahrsfest, das in Agadez im nördlichen Niger stattfindet.

Die Tuareg schmieden von Waffen bis zu Ohrringen die unterschiedlichsten Gegenstände aus Eisen, Silber und Buntmetallen. Eisen gewinnen sie heutzutage in erster Linie aus Industrieschrott, zum Beispiel Halbachsen von Geländewagen, die sie dann zu Äxten weiterverarbeiten.
Für die Herstellung von Gegenständen aus Buntmetall (Kupfer, Messing und Bronze) wird meist das Wachsausschmelzverfahren angewandt, bei dem man zunächst ein Modell des gewünschten Objekts aus Wachs anfertigt. Das Modell wird anschließend in kaltem Wasser gehärtet und danach mit feinem Ton umkleidet. Dabei werden mehrere Löcher freigelassen, um später das Wachs ausschmelzen zu können. Nun wird der Ton erhitzt und das Wachs durch die Öffnungen in eine Schüssel mit Wasser zur Wiederverwertung ausgegossen. Das vorgesehene Metall wurde bereits in einem Tontiegel "(tebent)" geschmolzen. Wenn das Gussmetall dann heiß genug ist, wird es durch das Wachsausgussloch in die Tonform eingegossen. Diese wird nach der Metallhärtung zerschlagen, anschließend wird der abgekühlte Rohling gefeilt und poliert (beispielsweise mit Sand) und ein Muster eingeritzt. Da man beim Gelbguss keine vorgefertigten Gussformen verwendet, fallen schon die unbearbeiteten Objekte sehr unterschiedlich aus.

Die Sahara-Tuareg bringen mit ihren Kamelen Salz aus der Amadror-Ebene und anderen Orten, sowie Datteln auf verschiedene Märkte. Von dem Erlös kaufen sie Getreide, Stoffe, Tee und Zucker. Die Sahara-Tuareg könnten ohne diesen Karawanenhandel nicht leben. Er wird nur von den Männern betrieben, so dass die Frauen manchmal monatelang mit den Kindern und Viehherden allein bleiben. Die Handelsunternehmen der Sahel-Tuareg beschränken sich auf den Verkauf ihres Viehs.







</doc>
<doc id="5220" url="https://de.wikipedia.org/wiki?curid=5220" title="Tastatur">
Tastatur

Eine Tastatur ist ein Eingabegerät, das als Bedien- und Steuerelement eine Anzahl von mit den Fingern zu drückenden Tasten enthält. Heute sind die meisten Tastaturen elektronische Tastaturen. Es gibt aber auch andere Funktionsprinzipien, wie beispielsweise mechanische oder pneumatische Tastaturen. Tastaturen gibt es für eine Vielzahl technischer Geräte, darunter für Computer, Fernbedienungen, Telefone und Handys. Weitere mögliche Eingabegeräte und Varianten sind weiter unten aufgeführt.

Eine Computertastatur hat im Vergleich zu einer Schreibmaschinentastatur mehr Tasten, die zur Bedienung des Computers und ggf. dessen Peripherie dienen. Die Computertastatur hat teilweise eine andere Tastenbelegung und auch Beschriftung. Eine feststehende Anordnung gibt es nicht, jede Systemarchitektur hat hier gewisse Eigenständigkeiten, insbesondere bei Notebooks ergeben sich durch das Platzproblem weitere Unterschiede.
Wesentliche Unterschiede zur Schreibmaschinentastatur sind der geringere Hubweg (Distanz von der Berührung des Fingers bis zum Anschlag) sowie der geringere Energieaufwand beim Herunterdrücken der Tasten. Dadurch ist die Haptik auch anders. Einige Computertastaturen sind hintergrundbeleuchtet, so dass die Tasten auch bei Dunkelheit sichtbar sind.

Grundsätzlich ist es nicht sinnvoll, jede einzelne Taste direkt am Tastaturcontroller anzuschließen, das würde eine große Anzahl von Anschlüssen am Controller und entsprechenden Verdrahtungsaufwand bedingen. Die einzelnen Tasten sind stattdessen in einer elektrischen Matrix aus Zeilen- und Spaltenleitungen angeordnet. Wird eine Taste gedrückt, so wird eine bestimmte Zeile mit einer bestimmten Spalte elektrisch verbunden. Diese Verbindung wertet ein Mikrocontroller (früher zum Beispiel bei XT-Tastaturen ein Mikroprozessor 8042) aus und schickt diese Information zum Rechner – bei der heute vorherrschenden IBM-PC-Architektur (auch beim Apple Macintosh) als sogenannter Tastencode (englisch Scancode). Damit sind zum Beispiel bei einer 16×8-Matrix mit 24 Leitungen bis zu 128 Tasten realisierbar.

Dieses Funktionsprinzip ist nahezu universell auf alle modernen Eingabegeräte mit mehreren Tasten anwendbar. Bei einigen älteren Architekturen übernahm der Hauptprozessor des Rechners selbst die Tastaturabfrage, zum Beispiel beim Commodore PET 2001, Commodore 64, Atari 800 und Sharp MZ-700.

Die Tastatur-Matrix besteht beispielsweise aus 13 Spalten und 8 Zeilen (104 Tasten). Zunächst wird der Ausgang für die erste Spalte aktiviert (aufgrund der negativen Logik auf eine niedrige elektrische Spannung gezogen) und dann überprüft, ob sich an einem oder mehreren der acht Eingänge der Zeilen diese niedrige Spannung wiederfindet. Dann wird der Ausgang für die erste Spalte wieder deaktiviert (hochohmig geschaltet) und der für die zweite Spalte aktiviert, und so weiter. Nach der letzten Spalte wird, eventuell nach einer Pause, wieder bei der ersten Spalte begonnen. Dieser Vorgang wiederholt sich kontinuierlich, mindestens einige dutzendmal pro Sekunde, und nennt sich „Scannen“. Wenn eine Taste gedrückt wird, wird bei einer bestimmten aktiven Spalte die Verbindung zur Zeile erkannt. Aus der Koordinate innerhalb der Spalten- und Zeilenmatrix ermittelt der Tastatur-Prozessor eine Tastennummer, die dann in einen Code für die Tastenposition, den Scancode, umgewandelt wird. Dieser wird zum Rechner übertragen. Dabei spielt es keine Rolle, welche (nationale) Tastaturbelegung (engl. „keyboard layout“) verwendet wird; der Scancode gibt nur die Position der gedrückten Taste für eine Standardtastatur an, unabhängig von der äußerlichen Anordnung, der Sprachauswahl des Betriebssystems oder der Beschriftung der Tasten.

Detailliertere Informationen zu den verschiedenen Scancodes, die speziell von heutigen Windows-PC-Tastaturen verwendet werden, und zu den Tastatur-Steuercodes findet man unter Scancode bzw. Tastaturbelegung.

Allgemein, sei es eine Tastatur eines elektronischen Musikinstrumentes oder eine Computer-Tastatur, kann es vorkommen, dass mehr als eine Taste gleichzeitig gedrückt wird. Je nach Bauart der Tastatur kann diese, im schlechtesten Fall, maximal zwei gleichzeitige Tastendrücke richtig erkennen (2-key rollover) oder im besten Fall alle (n-key rollover).

Abhängig von der Anordnung der Tasten in der Matrix kann es bei der gleichzeitigen Betätigung von drei oder mehr Tasten zu Fehlerkennungen kommen. Dabei werden möglicherweise Tasten als aktiv erkannt, die tatsächlich nicht betätigt sind. Dieser Effekt wird Phantom-Key genannt.

Zur Erklärung wird als Beispiel eine 6-Tasten-Tastatur mit zwei Zeilen (A, B) und drei Spalten (1, 2, 3) herangezogen. Werden u = A1, y = B2 und v = A2 gleichzeitig gedrückt, ist es für den Tastaturcontroller nicht möglich zu unterscheiden, ob nur v oder x gedrückt wird oder beide. Wird Zeile A beim Scannen aktiviert, erkennt der Controller korrekt Spalte 1 und 2 entsprechend u und v. Wenn jedoch Zeile B aktiv wird, erkennt der Controller korrekt Spalte 2 für y und fälschlicherweise Spalte 1 für x, weil über y, v und u (von Zeile B auf Spalte 2, zur Zeile A, zu Spalte 1) eine Verbindung mit Spalte 1 existiert. Im Gegensatz dazu ist eine gleichzeitige Betätigung von w = A3 oder z = B3 mit u und y kein Problem und würde sicher erkannt. Generell tritt dieser Effekt auf, wenn drei Ecken eines Rechteckes innerhalb der Matrix betätigt werden.

Eine verbreitete Gegenmaßnahme ist, die Tastenmatrix so anzuordnen, dass bei den üblichen Tastenkombinationen solche Muster nicht auftreten. Hier wird ausgenutzt, dass zum Beispiel nur sehr selten mehrere Zeichentasten gleichzeitig gedrückt werden müssen, während etwa die Kombination „Zeichen und Hochstelltaste“ häufig ist. Zusätzlich haben einige Tastaturen eine Beschränkung der Anzahl der Tasten, die sie gleichzeitig als gedrückt erkennen, zum Beispiel nur zwei Tasten (2-key rollover).

Intelligenter ist eine Erkennung von (möglichen) Phantom-Keys. Da es sehr unwahrscheinlich ist, dass ein Mensch mehrere Tasten so kurz hintereinander bedient, dass der Tastaturcontroller keine Reihenfolge der Betätigung mehr erkennen kann, besteht die Möglichkeit, so lange weitere gleichzeitig aktive Tasten zu erkennen, bis eine Kombination auftritt, die einen Phantom-Key enthalten könnte. Im obenstehenden Beispiel würde das dazu führen, dass zum Beispiel erst u, dann y als aktiv erkannt werden, hier kann kein Phantom-Key auftreten, also werden beide Tasten akzeptiert. Kommt nun noch v hinzu, entsteht der Phantom-Key-Effekt und der Tastaturcontroller ignoriert die Tasten v und x.

Aus Kostengründen nur in Spezialanwendungen zu finden ist die Option, jede Taste mit einer Diode zu versehen, so dass der Strom nur von Zeile zu Spalte fließen kann und nicht umgekehrt (true n-key rollover). Das ist jedoch, abhängig von der Technik des Tastenfeldes, nicht immer realisierbar und in jedem Fall mit höheren Produktionskosten verbunden. Eine Kompromisslösung ist es, eine Mischung aus günstiger Tastenanordnung und Dioden an einigen Tasten zu suchen.

Sowohl beim Drücken als auch beim Loslassen einer Taste schickt der Tastatur-Controller einen Code zum PC, wodurch in diesem ein Interrupt ausgelöst wird. Die Interrupt-Behandlungsroutine wertet den Scancode der jeweiligen Taste aus, ermittelt, welche der Modifier-Tasten (Umschalttaste, Strg, Alt, Alt Gr, Feststelltaste, Num, Rollen und Einf) zusätzlich aktiv ist, und speichert das Ergebnis im Tastaturpuffer. Das Betriebssystem bzw. aktive Anwendungsprogramm arbeitet mit Software-Interrupts diesen Puffer ab und verknüpft den Scancode nun mit einem Zeichen (Keycode) wie Ziffer, Buchstabe oder Satzzeichen etc. oder einer Aktion (zum Beispiel "Programm beenden"). Das heißt, die Tastatur liefert nur Tastennummern und keine ASCII-Zeichen, diese Verknüpfung findet erst in der Software des PC-Betriebssystems oder Anwenderprogramms statt.

Die Kommunikation zwischen Tastatur und Computer erfolgt über eine serielle Schnittstelle, so dass nur wenige Adern im Kabel und nur ein kompakter Stecker benötigt werden.

Der Vorgänger der PS/2-Schnittstelle war die PC/XT-Tastaturschnittstelle, die artverwandt in der Funktionsweise ist, aber nur den Datentransfer von der Tastatur zum Computer zulässt. Mit der AT-Tastatur wurde die Schnittstelle bidirektional, um bestimmte Zustände wie zum Beispiel Num-Lock, Caps-Lock und Scroll-Lock über LEDs darstellen zu können.

Bei der PC/XT- und der AT-Schnittstelle kam ein fünfpoliger DIN-Stecker zum Einsatz, der mit Einführung der PS/2-Computerfamilie von IBM durch einen sechspoligen Mini-DIN-Stecker („PS/2-Stecker“) ersetzt wurde. Elektrisch sind AT-Schnittstelle und PS/2 identisch, das Protokoll wurde für die PS/2-Tastaturen erweitert.

Eine über USB angeschlossene Tastatur wird vom Host-Controller als Gerät der Klasse Human Interface Device behandelt. Die dafür nötigen Gerätetreiber sind in jedem USB-kompatiblen Betriebssystem bereits integriert. Die Datenübertragung erfolgt im Interrupt-Transfer. Viele Funktastaturen haben eine USB-Schnittstelle, diese melden sich am System genauso an wie kabelgebundene Tastaturen.

Im Gegensatz zur PS/2- und ADB-Schnittstelle (Apple Desktop Bus) sendet eine USB-Tastatur nicht bei jedem Niederdrücken und Loslassen einer Taste die sogenannten Make- bzw. Break-Codes, sondern einen Datenblock mit Statusinformationen (Flags) für die Umschalttasten und Bytes für die sonstigen Tasten, in dem der gesamte aktuelle Status der Tastatur angegeben wird.

Die USB-Schnittstelle hat gewisse Vorteile gegenüber der PS/2-Schnittstelle:


Es gibt jedoch auch Nachteile gegenüber Tastaturen mit PS/2-Schnittstelle:


Mechanische Tastaturen gelten mit bis zu 50 Millionen Anschlägen langlebiger als Rubberdome-Tastaturen, bei denen die Tasten mit einer Art Kuppe aus Silikon oder Gummi in die Ausgangsstellung zurückgeschoben werden.

Ein weiteres Qualitätsmerkmal ist die Laserbeschriftung, die langlebiger ist als beklebte Tasten. Die hochwertigste Beschriftung ist jedoch die nach dem "Double-shot-molding"-Verfahren gefertigte, bei der eigene Lettern ähnlich einer Schreibmaschine physisch die Buchstaben und Zeichen innerhalb der Tasten bilden. Als Material kommt hier z. B. auch höherwertiger PBT-Kunststoff zum Einsatz, der sich nicht verfärbt und seine Oberflächenbeschaffenheit auch über lange Zeit beibehält.

Weitere Qualitätsmerkmale sind der Spritzwasserschutz einer Tastatur, eine Antihaftversiegelung gegen Schmutz sowie ferner auch Anti-Ghosting für Spieler und Musiker, die auch komplexe Tastenkombinationen verwenden wollen.

Auch die Lautstärke spielt eine gewisse Rolle, mechanische lautere Tastaturen können hier durch sogenannte "Gummi-O-Ringe" oder eigene MX-Silent Tasten noch etwas gedämpft werden. An die Dämpfung von Rubberdome-Tastaturen kommen diese bisher jedoch nicht heran.

Spätestens seit den 1970er Jahren gibt es Ansätze, gewissen Problemen, die sich beim Arbeiten mit „normalen“ Tastaturen ergeben können (zum Beispiel RSI-Syndrom), mit neuen, ergonomischen Tastaturen entgegenzuwirken. Diese vermeiden die Anordnung der Tasten in durchgehenden geraden horizontalen Reihen, sondern ordnen sie je nach Konzeption und Bauform in gekrümmten oder geknickten (durchgehenden oder unterbrochenen) Linien oder auch ganz anders an. Auch sind in einigen Konstruktionen Teilbereiche der Tastatur gegenüber der Auflagefläche erhöht.

Eine Entwicklung aus den 1970er Jahren (von Lillian Malt und Stephen Hobday) ist die auch heute (Stand März 2017) noch vertriebene Maltron-Tastatur. Hier sind die Tastenblöcke für linke und rechte Hand getrennt angeordnet mit je einem zusätzlichen Tastenblock für die beiden Daumen. Die Tastatur wird sowohl mit an QWERTY und Dvorak angelehnten Tastaturbelegungen produziert als auch mit einer eigens auf die Ergonomie der Tastatur hin entwickelten.

Die Ergodox-Tastatur verfolgt mit vertikal geradlinig angeordneten Tasten und separaten Tastenblöcken für die Daumen ein ähnliches Konzept wie die Maltron-Tastatur, jedoch sind linke und rechte Tastaturhälfte physikalisch getrennt und somit voneinander unabhängig positionierbar. Hardware und Firmware sind frei lizenziert, die Tastatur kann als Bausatz oder über Dienstleister fertig zusammengebaut erworben werden.

Anfang der 1990er Jahre wurden weitere Tastaturen auf den Markt gebracht (namentlich von der deutschen Firma "Marquardt" in Zusammenarbeit mit der "Fraunhofer-Gesellschaft"), die durch eine natürlichere Stellung der Hände ein ermüdungsfreies und gesundes Arbeiten ermöglichen sollen. Diese Tastaturen sind zweigeteilt und A-förmig gebogen. Außerdem sind die beiden Teile wie ein Schwalbenflügel in sich erhöht, und die Tastatur kann – optional – nach hinten abfallen, wohingegen „normale“ Tastaturen nach hinten ansteigen. Die Handballen liegen auf einem Polster auf. Die ergonomische Tastatur ermöglicht eine natürlichere Schreibhaltung als die normale Tastatur, weil die Hände beim Schreiben nicht nach außen und oben gebogen werden müssen, sondern mit dem Unterarm in einer Linie bleiben. Nachteilig sind solche Tastaturen beim „Zweifingersuchsystem“ von Wenigschreibern und bei gleichzeitiger Verwendung von Maus und Tastatur.

Die Ergonomie solcher Tastaturen kommt am ehesten bei Verwendung mit Computerprogrammen zum Tragen, die eine Bedienung auch ohne Maus ermöglichen, weil dabei die Hände auch während längerer Arbeit in der durch die Tastatur ermöglichten Haltung verbleiben können.

Versuche mit vereinfachten Belegungen durch weniger Tasten, zusätzlich farblich gekennzeichnet, vom Unternehmen "„New Standard Keyboards“" gingen einen anderen Weg der Optimierung: Skepsis gegenüber bisherigen Ergonomieuntersuchungen führte zur alphabetischen Anordnung der Buchstaben auf den Tasten in Verbindung mit speziellen Tastenpositionierungen.

Mit "Velotype" wurde ein völlig anderer Ansatz gewählt: Statt mit einzelnen Tasten Buchstaben einzugeben, werden hier mit Hilfe von Tastenkombinationen ganze Silben geschrieben. Das System, das heute unter der Bezeichnung „Veyboard“ vermarktet wird, soll eine mehrfach höhere Eingabegeschwindigkeit buchstabenorientierte Tastaturbelegung erlauben. Daher hat es sich in einigen Nischenmärkten wie der Live-Untertitelung durchgesetzt.

Eine Großschrifttastatur für Sehbehinderte entspricht einer herkömmlichen PC-Standardtastatur. Die Beschriftung ist allerdings deutlich größer. Die Buchstaben sind hier etwa 10 mm hoch und füllen die Fläche der einzelnen Tasten aus. Dadurch sind sie wesentlich leichter zu lesen als die Standardbeschriftung. Man kann zwischen verschiedenen Gehäuse- und Schriftfarben wählen. Zusätzlich kann die Tastatur mit fühlbaren Markierungen versehen werden.

Wann eine Großschrifttastatur sinnvoll eingesetzt werden kann, hängt von den persönlichen Bedürfnissen des Nutzers ab. Groß beschriftete Tasten sind eine sinnvolle Ausstattung besonders für Computernutzer, die noch nicht lange sehbehindert sind und sich auf der Tastatur nicht sicher fühlen. Die Großschrift eignet sich auch dann gut, wenn man selten benötigte Tasten entspannt finden möchte.

Es gibt Tastaturen aus Edelstahl, Aluminium oder anderen Metallen, die speziell für den Einsatz in rauen, verschmutzten Umgebungen oder in öffentlichen Räumen gedacht sind. Einsatzbereiche sind:

Metalltastaturen sind widerstandsfähiger gegenüber mechanischer Beanspruchung, leichter zu reinigen sowie UV- und temperaturunempfindlich. Nachteilig sind das höhere Gewicht und die höheren Herstellungskosten. Auch zum Schreiben längerer Texte sind sie nicht geeignet, da die Tasten meist sehr schwer zu drücken sind.
Darüber hinaus gibt es verschmutzungsresistente Gummitastaturen. Diese werden häufig in Kfz-Werkstätten oder Firmen verwendet, die mit flüssigen Stoffen arbeiten, zum Beispiel mit Öl.

Kompakttastaturen nehmen eine kleinere Grundfläche ein als herkömmliche Tastaturen und eignen sich damit besonders für Anwendungsfälle, in denen wenig Platz zur Verfügung steht, beispielsweise auf den Knien von Rollstuhlfahrern. Ein anderer Anwendungsfall sind sogenannte Slimline-Tastaturen, die meist eine normale Tastaturbelegung aufweisen, aber wesentlich flacher (slim = „schlank“) sind.

Für Anwendungsfälle, in denen die Mobilität im Vordergrund steht, gibt es Klapptastaturen sowie flexible Tastaturen aus Kunststoff, die man einrollen kann. Aus Gummi gefertigte Modelle sind zusätzlich wasserdicht und auch gegen andere Flüssigkeiten resistent. Klapp- und Gummitastaturen werden als Ergänzung zu den bauformbedingt sehr kleinen Bildschirmtastaturen von PDAs und anderen tragbaren Tablet-Computern wie dem iPad angeboten.

Als Inseltastatur, "Insel-Design, Insel-Look" oder englisch "" wird eine Tastatur bezeichnet, bei der zwischen den Tasten ein Abstand besteht; das Gehäuse der Tastatur hat dementsprechend einzelne Löcher für jede Taste. Daher hat eine derartige Tastatur eine gewisse Ähnlichkeit mit Kaugummitastaturen, welche ebenfalls (konstruktionsbedingt) für jede Taste ein einzelnes Loch haben. Dementsprechend kann der englische Begriff "chiclet keyboard" für beide Arten stehen.

Bereits 1985 wurde eine Tastatur vorgestellt, deren Tasten mit kleinen LCDs versehen waren, wodurch ihre Beschriftung vom Benutzer jederzeit umprogrammiert werden konnte. Ein aktuelles Beispiel solcher Konstruktionen (Stand März 2017) ist die Optimus-Maximus-Tastatur. Wegen des erheblich höheren Preises müssen derartige Geräte als Nischenprodukte angesehen werden. Auch Tastaturen auf Grundlage eines Touchscreens konnten sich bislang als eigenständiges Peripheriegerät nicht durchsetzen.

Bei dieser Form der Tastatur wird ein projiziertes Tastenfeld auf einer beliebigen ebenen Oberfläche erzeugt und die Benutzeraktion darauf mit einer Kamera erfasst. Das Gerät erkennt mittels Bildverarbeitung, welche Felder berührt werden, und leitet die Information an den Computer weiter (nicht zu verwechseln mit der Virtuellen Tastatur).

Dieser Begriff bezeichnet im Zusammenhang mit Tastaturen für Blinde jede Computertastatur mit auf den Tasten aufgedruckten Schriftzeichen im Gegensatz zu der speziell für Blinde entwickelten Punktschriftmaschine. Bei Kenntnis des Tastschreibens und in Verbindung mit einer Braillezeile kann eine solche herkömmliche Tastatur auch von Blinden benutzt werden.

Einhandtastaturen werden verwendet, wenn dem Bediener nur eine Hand zur Tastaturbedienung zur Verfügung steht. Dies kann die Folge körperlicher Einschränkungen sein, oder in Situationen, in denen die andere Hand für ein anderes Bedienelement gebraucht wird.

Letzteres ist beispielsweise bei Computerspielen der Fall, die eine unterbrechungsfreie Bedienung mit der Maus erfordern, so dass nur die andere Hand zur gleichzeitigen Tastatureingabe zur Verfügung steht. Bei solchen Spielen sind deshalb die notwendigen Tastatureingaben in der Regel auf Tasten beschränkt, die auf der linken Seite der Standardtastatur angeordnet sind und somit mit einer Hand ohne Verlagerung des Handballens (und damit verzögerungsfrei) erreichbar sind. Für diese Anwendungen eignen sich die speziellen Gamer-Einhandtastaturen, die nur diese Tasten enthalten (zumeist ergänzt um Tasten für sämtliche Ziffern), diese aber in einer für Computerspielbedienung optimierten Weise anordnen. Für Standard-Computeranwendungen wie Textverarbeitung sind solche Tastaturen jedoch ungeeignet.

Einhandtastaturen mit dem vollständigen Umfang einer Standardtastatur werden in der Regel als Artikel für die berufliche Rehabilitation im entsprechenden Fachhandel angeboten.

Eine spezielle Form der Einhandtastaturen sind die Akkordtastaturen (englisch "chord keyboards"), die nur wenige (zumeist unbeschriftete) Tasten besitzen, und bei denen die Eingabezeichen Kombinationen von gleichzeitigen Tastenbetätigungen zugeordnet sind.

Bereits die Z3 von Konrad Zuse verfügte über Tasten. Zunächst wurden Fernschreiber mit mechanischen Schreibmaschinentastaturen eingesetzt. Später wurden rein elektronische Tasten verwendet. Die Belegung und Anordnung der Tasten wurde von der Schreibmaschine übernommen. Dazu kamen verschiedene Zusatztasten. Jahrzehntelang galt, dass fast jedes Computermodell über seine spezifische Tastatur verfügte. Das führte vor allem in der Zeit um 1980, zur Blütezeit der 8-Bit-Heimcomputer, zu einer riesigen Vielfalt. Neben der Belegung mit Buchstaben, Ziffern und Sonderzeichen verfügten manche Computer über Grafikzeichen, einige sogar über Programmiersprachen-Befehle (zum Beispiel in BASIC) auf der Tastatur. Beispiele dafür sind die Tastaturen des Commodore 64 oder des Sinclair ZX81. Oft war eine Taste mit dem Firmenzeichen des Herstellers vorhanden.

Je mehr sich der IBM-PC und dessen Nachfolger am Markt durchsetzten, verdrängte zunächst die sogenannte "XT-Tastatur", dann "AT-Tastatur" und später die "MF2-Tastatur" alle anderen Tastaturtypen. Neben der PC-Tastatur hat sich heute einzig noch die Apple-Tastatur behaupten können, die jedoch inzwischen eine sehr ähnliche Tastenbelegung aufweist.

Durch die weite Verbreitung des IBM-PCs wurde auch dessen Tastatur zum meistbenutzten Typ. Obwohl die Tastaturen für IBM-PCs einem einheitlichen Standard folgen, haben sich in 25 Jahren PC-Geschichte durch Erweiterungen und lokale Anpassungen (Umlaute etc.) viele verschiedene Tastaturtypen entwickelt:


Die Apple-Tastaturen unterscheiden sich von den PC-Tastaturen nur minimal. Die erweiterte Apple-Tastatur hat 19 Funktionstasten. Diese sind zum Teil mit Funktionen für die Steuerung der Lautstärke (leiser, lauter und stumm) belegt. Zudem existiert eine Taste für das Auswerfen von optischen Datenträgern. Weiter ist der Ziffernblock anders angeordnet und weist eine Sondertaste auf, die programmspezifisch verwendet werden kann. Neben der Leertaste finden sich beiderseits je eine Strg- bzw. Ctrl-Taste (^), eine Wahltaste (⌥) und eine Befehlstaste (⌘), mit deren Hilfe wie auch mit den Umschalttasten nicht nur die Tastatur, sondern auch die Maustasten in vielfältiger Weise mehrfach belegt (bis zu 16-fach) verwendet werden. Die Druck-/Print-Screen-Taste, die Rollen/Scroll Lock-Taste, die Pause-Taste, die Einfg/Insert-Taste, die Alt-Gr-Taste wie auch die Windows-Taste gibt es auf einer Apple-Tastatur nicht. Auch die Belegung des @-Zeichens ist bei Apple-Tastaturen, welche nicht die US-amerikanische Belegung haben, anders. Das Zeichen wird bei der deutschen Tastaturbelegung durch Drücken der Tasten (Wahltaste)+ und mit schweizerischem Layout per + eingefügt.

Für Umsteiger von einer PC-Tastatur auf den Mac wirkt sich am verwirrendsten wohl die Verschiebung der eckigen und der Mengenklammern aus: Letztere werden bei deutschem und schweizerischem Layout durch + bzw. erzeugt, also über die Tasten für die entsprechenden runden Klammern, mit denen aber beim PC die eckigen Klammern erzeugt werden. Diese liegen bei der Apple-Tastatur auf + bzw. . Auch die Pipe, der senkrechte Strich, wird nicht per + erreicht, sondern via +. Über +Shift+ gelangt man zum Backslash.

Generell lassen sich Apple-Tastaturen neuerer Bauart (USB) auch relativ problemlos auf PC-Systemen betreiben, wobei die Befehlstaste (cmd) dann als Windows-Taste fungiert. Dadurch hat die rechte Taste die Funktion von . Die Tastenbelegung ist ansonsten wie am PC gewohnt. Das @-Zeichen ist also nicht auf beziehungsweise , sondern auf auf deutschen beziehungsweise auf schweizerischen Tastaturen.


Werden Tastaturen für einen bestimmten Zweck optimiert, so sind sie unter Umständen für andere Zwecke schlechter geeignet. So sind Tastaturen an Überweisungs- und Geldautomaten, die viele unterschiedliche Menschen verwenden oder der Witterung ausgesetzt sind, aus Metall und weisen einen geringen Tastenspielraum auf. Das ermöglicht Langlebigkeit und nahezu unzerstörbare Eigenschaften, aber kein flüssiges Schreiben.

Auch wenn die Tastatur bis heute das dominierende Eingabegerät ist, haben sich im Lauf der Jahrzehnte doch eine Reihe von Alternativen etabliert.

Eine Alternative bildet der sogenannte Touchscreen. Dabei handelt es sich um einen Bildschirm, auf dem berührungsempfindliche Felder liegen, die zur Eingabe verwendet werden können. Durch die Kombination von Tastatur und Bildschirm in einem Gerät lässt sich vor allem Platz sparen.
Die fehlende taktile Rückmeldung, beispielsweise ein Druckpunkt als fühlbare Tipp-Bestätigung, ist nur wichtig für ungeübte Schreiber und für Dateneingabe ohne sichtbare Rückmeldung und spielt für die Eingabe von Fließtext keine Rolle.

Weitere Alternativen bilden sogenannte Zeigegeräte. Dabei handelt es sich um Geräte, mit denen ein Zeigersymbol über den Bildschirm bewegt werden kann. Beispiele für Zeigergeräte sind: Maus, Touchpad, Joystick, Trackball, Trackpoint, Rollstangenmaus und Lichtgriffel.

Weitere Alternativen sind:










Als Hilfe zum Blindschreiben haben Computertastaturen meist zwei haptische Markierungen auf den Tasten der Buchstaben F und J sowie eine Markierung für das Zentrum des Nummernfelds auf der Taste für die Ziffer 5. Dabei handelt es sich um fühlbare Erhebungen in Form eines Punktes oder eines Striches (Fühlbalken), oder eine fühlbar abweichende Formgebung der Tasten. Diese erleichtern das Auffinden der Tasten (Positionierungen) und werden daher als Orientierungstasten bezeichnet.




</doc>
<doc id="5221" url="https://de.wikipedia.org/wiki?curid=5221" title="Tradition">
Tradition

Tradition (von „hinüber-geben“ oder „Übergabe, Auslieferung, Überlieferung“) bezeichnet die Weitergabe (das "Tradere") von Handlungsmustern, Überzeugungen und Glaubensvorstellungen u. a. oder das Weitergegebene selbst (das "Traditum", beispielsweise Gepflogenheiten, Konventionen, Bräuche oder Sitten). "Tradition" geschieht innerhalb einer Gruppe oder zwischen Generationen und kann mündlich oder schriftlich über Erziehung, Vorbild oder spielerisches Nachahmen erfolgen. Die soziale Gruppe wird dadurch zur "Kultur". Weiterzugeben sind jene Verhaltens- und Handlungsmuster, die im Unterschied zu Instinkten nicht angeboren sind. Dazu gehören einfache Handlungsmuster wie der Gebrauch von Werkzeugen oder komplexe wie die Sprache. Die Fähigkeit zur Tradition und damit die Grundlage für Kulturbildung beginnt bei Tieren, wie beispielsweise Krähen oder Schimpansen, und kann im Bereich der menschlichen Kulturbildung umfangreiche religiös-sittliche, politische, wissenschaftliche oder wirtschaftliche Systeme erreichen, die durch ein kompliziertes Bildungssystem weitergegeben wurden.

Der "sichtbare" Ausdruck der Traditionen einer Ethnie oder eines indigenen Volkes wird als "Folklore" bezeichnet (siehe auch Folklorismus).

Die Redeweise „Es ist Tradition, dass …“ bezieht sich in der Regel auf das Überlieferte "(traditum)", häufig im Sinne von „Es ist seit langer Zeit üblich, dass …“. Umgangssprachlich seltener wird mit "Tradition" der Überlieferungsvorgang an sich "(tradere)" bezeichnet. Zur Unterscheidung wird im Deutschen manchmal von „Tradition“ im Sinne von "traditum" und „Tradierung“ entsprechend dem "tradere" gesprochen. Diese Unterscheidung verweist auf zwei Hauptbedeutungen von Tradition:
Forschungen zum Begriff und zum Verhältnis der beiden Hauptbedeutungen fallen in den Bereich der Traditionstheorie (siehe unten).

Unter Tradition wird in der Regel die Überlieferung der Gesamtheit des Wissens, der Fähigkeiten sowie der Sitten und Gebräuche einer Kultur oder einer Gruppe verstanden. Nach Hans Blumenberg besteht Tradition daher nicht aus Relikten, also dem aus der Geschichte übrig Gebliebenen, sondern aus „Testaten und Legaten“. "Tradition" ist in dieser Hinsicht das "kulturelle Erbe" (Legat), das in Arbeits- und Kommunikationsprozessen von einer Generation zur nächsten weitergegeben wird. Wissenschaftliches Wissen und handwerkliches Können gehören ebenso dazu, wie Rituale, künstlerische Gestaltungsauffassungen, moralische Regeln und Speiseregeln. "Traditionen" im Sinne von Brauchtum und kulturellem Erbe begegnen beispielsweise bei Hochzeiten, Dorffesten und im Zusammenhang mit kirchlichen Feiertagen. Auch Alltagsgesten bei Begrüßung und Verabschiedung sind Brauchtumstraditionen. Die Ethnologie untersucht, wie solches Brauchtum konkret entsteht und "tradiert" wird.

Im deutschsprachigen Raum wird in mancherlei Variationen gern der Aphorismus zitiert: „Tradition ist nicht das Halten der Asche, sondern das Weitergeben der Flamme“. Er soll von Thomas Morus oder anderen Geistesgrößen stammen oder jedenfalls verwendet worden sein. Belege werden dafür regelmäßig nicht präsentiert und sind auch sonst nicht zu finden. Die Gegenüberstellung: Bewahrung der Asche oder der Flamme, benutzte allerdings schon John Denham in seinem Gedicht "To Sir Richard Fanshaw, upon his Translation of Pastor Fido" (1647). Denham vergleicht dort eine poesielose, an den Worten klebende Übersetzung "Wort für Wort und Zeile für Zeile" mit Fanshaws lebendiger, sinngemäßer Übertragung im Geist des Originals: 

Das "Bremer Sonntagsblatt. Organ des Künstlervereins" brachte am 12. Mai 1861 unter der Überschrift "Englische Dichtungen" eine Verdeutschung von :

Darunter folgten Übertragungen Pertz' „nach Th. Moore“. Dies könnte dazu beigetragen haben, dass später, als jemand die Asche/Flamme-Metapher von Übersetzungen auf Traditionspflege übertrug, irrtümlich Thomas Morus zu ihrem Urheber avancierte.

Neben hochkulturellen Inhalten werden zuweilen auch nur temporär gültige Üblichkeiten als "Tradition" bezeichnet. In dem Sinn wird der Ausdruck "traditionell" gebraucht; es ist das Übliche und Gewohnte. Der eher bildungssprachliche Ausdruck "traditional" wird dagegen auf die hochkulturellen Inhalte bezogen.

Seltener bezeichnet "Tradition" die Tradierung, also den Prozess der Überlieferung selbst, auch wenn in systematischer Hinsicht der "Tradition"sprozess die Grundlage für die "Tradition" als kulturelles Erbe bildet. Die ältere Traditionstheorie hat den Traditionsprozess als einen Vorgang beschrieben, bei dem ein Tradent einem Empfänger etwas überliefert. Neuere Ansätze kritisieren diese Auffassung als zu starke Vereinfachung. So wie das schlichte Sender-Empfänger-Modell in der Kommunikationstheorie tatsächliche Kommunikation unsachgemäß beschreibt, ist das vergleichbare Tradent-Empfänger-Modell unzulänglich. Die Entdeckung des Subjekts in der Neuzeit macht es nach dieser Auffassung nötig, eine Wechselbeziehung anzunehmen, wie es beispielsweise der Kultursoziologe Stuart Hall für das Sender-Empfänger-Modell vorgeschlagen hat. Der vormalige „Empfänger“ wird als aktiver Teil von Traditionsprozessen verstanden (Tradent-Akzipient-Modell).

Traditionstheorien gibt es in sehr unterschiedlichen Zusammenhängen: In der Ethnologie, der Volkskunde, der Soziologie, der Philosophie, der Theologie, der Literaturwissenschaft und der Rechtswissenschaft. Dabei konzentrieren sich die einzelnen Wissenschaften jeweils auf Teilaspekte des Phänomens "Tradition". Bislang liegt kein Ansatz für eine systematisch entwickelte Traditionstheorie vor.

Da Tradition zu den Grundlagen des sozialen Lebens und Handels gehört, hat sich insbesondere die Soziologie mit dem Phänomen "Tradition" befasst. Robert Spaemann sieht im "Französischen Traditionalismus" gar eine der Wurzeln der Soziologie selbst. In jedem Fall hat die soziologische Auseinandersetzung mit der Tradition die geistes- und kulturwissenschaftlichen Diskussionen insgesamt geprägt. Insbesondere Max Webers Verständnis von Tradition als einem von vier Grundtypen sozialen Handelns ist wirkungsgeschichtlich kaum zu überschätzen. Weber grenzt die Orientierung an Tradition von der zweck- und wertrationalen Orientierung des Handelns ab. Er greift damit ein Traditionsverständnis auf, das am Ende des 19. Jahrhunderts vorrationale Tradition und rational orientierte Moderne gegenüberstellt. Diese Gegenüberstellung ist auch die Folge einer kritischen Abwendung vom Traditionsverständnis des Traditionalismus.

Neben seinem Versuch, den Traditionsbegriff mit vier Grundtypen sozialen Handelns greifbar zu machen, formuliert Weber gleichsam eine Theorie der politischen Herrschaft, wobei er zwischen charismatischer, rationaler, legaler und traditioneller Herrschaft unterschied. Hierbei knüpfte er den Begriff der Tradition eng an eine herrschende Einzelperson, die über einen von ihm abhängigen Verwaltungsstab verfügt. Merkmal der auf Tradition beruhenden Herrschaft sei Weber zufolge, dass die politische Ordnung primär auf überliefertes Wissen beruhe, auf persönlichen Gehorsam basiere und – im Gegensatz zur charismatischen Herrschaft – einen alltäglichen Charakter habe.

Das Traditionsverständnis von Max Weber ist aber nur bedingt geeignet, das Phänomen der Überlieferung und Übernahme zwischen den Generationen und den Einfluss auf die Bildung sozialer Gruppen angemessen zu beschreiben. Allein die Gegenüberstellung von vorrationaler Tradition und rationaler Moderne greift nicht. Wäre es so, dass der Modernisierungsprozess das Überkommene allmählich abstreifen würde, müsste dieses Phänomen weltweit auch zu beschreiben sein. Tatsächlich bietet der Modernisierungsprozess aber ein differenziertes Bild: Zum Teil werden Traditionen von modernen Entwicklungen und Auffassungen abgelöst (Traditionsabbruch), zum Teil geraten Moderne und Tradition in einen unüberwindbaren Konflikt (Traditionalismus, Fundamentalismus), zum Teil bestehen Tradition und Moderne konfliktlos nebeneinander oder ergänzen sich sogar (Alternativmedizin). Wie wenig sich die Begriffe ausschließen, zeigt sich aber insbesondere daran, dass Modernität selbst zu einer neuen „großen Tradition“ geworden ist. Statt Tradition als vormodern zu betrachten, was zu kurz greifen würde, gilt es darum, die soziale Funktion der Tradition auch in modernen und post-modernen Gesellschaften zu beschreiben. Für Anthony Giddens besteht diese Funktion darin, das kollektive Gedächtnis einer Gesellschaft zu organisieren.

Für die soziologische Analyse des Phänomens Tradition bieten sich nach Edward Shils drei Aspekte an: 1. formal, 2. inhaltlich und 3. strukturell. In formaler Hinsicht ist Tradition abhängig vom Prozess der Tradierung. Inhalte, die nicht tradiert wurden bzw. werden, mögen kulturhistorisch interessant sein, sind aber soziologisch uninteressant für die Betrachtung von Tradition. Inhaltlich zeichnen sich Traditionen durch eine besondere Wertschätzung oder einen besonderen Anspruch aufgrund der Vergangenheitsorientierung aus. Strukturell ist Tradition auf Wiederholung, Weitergabe und Ritualisierung angelegt. In der Perspektive dieser drei Aspekte wird deutlich, wie Tradition kulturelle Leitmuster "(guiding patterns)" ausbildet und so die Vergangenheit in die Gegenwart hineinreicht und diese beeinflusst.

In Anlehnung an Shils definiert der amerikanische Organisationspsychologe Karl E. Weick Tradition als etwas, das in der Vergangenheit erzeugt, durchgeführt oder geglaubt wurde oder von dem [heute] geglaubt wird, dass es existierte, ausgeführt oder in der Vergangenheit geglaubt wurde und das von einer Generation zur nächsten weitergegeben wird oder wurde. Weiter spezifizieren Shils und Weick: „Um als Tradition zu qualifizieren muss ein Muster mindestens zweimal in drei Generationen übergeben werden.“

In der Ethnologie bildete sich ab 1982 eine eigene Debatte zum Thema "Tradition", die durch das Verständnis von Tradition als kulturellem Konstrukt geprägt ist (siehe auch Sozialkonstruktivismus). Ausgangspunkt waren Anfang der 1980er Jahre die Arbeiten des Briten Eric Hobsbawm und des Amerikaners Roger Keesing. Großen Einfluss auf die Diskussion hatte 1983 die These von der „erfundenen Tradition“, welche die beiden Sozialhistoriker Eric Hobsbawm und Terence Osborn Ranger in ihrem Sammelband "The Invention of Tradition" ausführten. Danach sind viele Traditionen, denen eine alte Herkunft zugeschrieben wird, verhältnismäßig jung, aufgezeigt auch am Beispiel schottischer und walisischer Kultur, deren Wurzeln zumeist im 19. Jahrhundert liegen. Bekanntestes Beispiel ist die so genannte "Highlander-Tradition" mit Kilt und Dudelsack, die als Protestkleidung erst nach der Vereinigung mit England aufkam, aber als ursprüngliche Highland-Tradition angesehen wird. Ein Jahr zuvor hatten Roger Keesing und Robert Tonkinson in ihrem Aufsatz "Reinventing Traditional Culture" auf der Basis von ethnologischen Forschungen in Melanesien am Beispiel der Bezeichnung "kastom" (ein Pijin-Wort auf den Salomonen, vom englischen "custom" abgeleitet, übersetzbar als „Tradition“) versucht aufzuzeigen, dass das kulturelle Selbstverständnis stark von kolonialen Einflüssen geprägt ist und sich deutlich vom vorkolonialen Brauchtum unterscheidet.

Jocelyn Linnekin und Richard Handler verstanden 1984 Tradition als symbolische Konstruktion und Repräsentation. Sie grenzten ihren analytischen Gebrauch des Wortes vom Alltagsverständnis ab, wonach Tradition wie eine Sache erscheint, die weitergegeben werden kann. Dagegen betonten Linnekin und Handler, Traditionen seien als symbolische Konstruktionen der aktuellen Generation immer Interpretationen und könnten durch die Interpretation verändert werden. Dadurch entsteht, was Linnekin und Handler das „Paradox der Tradition“ nennen: Der Versuch, eine Tradition authentisch zu bewahren, bedarf der Interpretation dieser Tradition, und genau dadurch verändert sie sich. Kern dieser symbolischen Konstruktion ist die Verwendung von Material aus der Vergangenheit, um Handlungen, Verhalten, Beziehungen und Artefakte in der Gegenwart zu verstehen.

Weitere wichtige ethnologische Positionen vertreten Geoffrey Miles White und Lamont Lindstrom "(Tradition als Diskurs)" sowie Kathleen M. Adams "(Tradition und Agency)".

In der Geschichtswissenschaft wird unter „Tradition“ die mündliche oder schriftliche Überlieferung von Informationen zum Zweck der Erhaltung für die Nachwelt verstanden. Der Begriff dient zur Unterscheidung von Tradition als bewusster Überlieferung vom Überrest als unbewusster Überlieferung, etwa in Form Gebrauchstexten und -gegenständen wie Rechnungen, Bestandslisten etc. (vgl. Artikel Tradition (Geschichtswissenschaft)). Der in der Sozialgeschichte eingeführte Begriff der „erfundenen Tradition“ nimmt im Unterschied zum Begriffspaar „Tradition/Überrest“ die umgekehrte Perspektive der (bewussten oder unbewussten) Traditionskonstruktion der Nachwelt in den Blick und betont die soziale Konstruktion der Geschichtsschreibung selbst.

In der antiken Rechtssprache (Römisches Recht) war Tradition ("traditio") der Übergabeakt einer (beweglichen) Sache zum Beispiel bei der Vererbung und beim Kauf. Daher rührt auch die noch heute manchmal begegnende Verwendung von Tradition als Auslieferung (vergleiche englisch: "trade").

Auch im heutigen deutschen Zivilrecht ist zur rechtsgeschäftlichen Übertragung des Eigentums an einer beweglichen Sache grundsätzlich neben der dinglichen Einigung die Übergabe der Sache erforderlich, es gilt also das "Traditionsprinzip". Jedoch wird das Traditionsprinzip häufig durchbrochen, indem die Übergabe durch eines der gesetzlich vorgesehenen Übergabesurrogate ersetzt wird (z. B. Vereinbarung eines Besitzkonstitutes oder Abtretung des Herausgabeanspruchs).

In der modernen Rechtswissenschaft bezeichnet "Traditionstheorie" einen bestimmten Ansatz zur Abgrenzung des öffentlichen Rechts vom Privatrecht. Die "Traditionstheorie" bezeichnet danach die Auffassung, dass bestimmte Rechtsgebiete "traditionell" dem öffentlichen Recht zugeordnet werden. Dazu gehören zum Beispiel Rechtsstreitigkeiten innerhalb des Polizei-, des Ordnungs- und des Verwaltungsrechtes.
Neben der Traditionstheorie gibt es als weitere Abgrenzungstheorien die Interessentheorie, die Subordinationstheorie (auch: Subjektstheorie) und die Sonderrechtstheorie (auch: modifizierte Subjektstheorie).

Im Bereich der Historischen Hilfswissenschaften ist eine der rechtswissenschaftlichen Bedeutung nahe liegende Verwendung gebräuchlich, wenn die Übertragungen von Grundbesitz an Klöster und ihre Beurkundung als "Tradition" bezeichnet wird (vgl. Traditionsbuch)

In der Philosophie spielt der Traditionsbegriff kaum eine Rolle. Selbst in etablierten Handbüchern fehlt häufig eine Erörterung des Themas und eine Analyse des Begriffs. Der Philosoph Karl Popper sah die Entwicklung einer Traditionstheorie vor allem als Aufgabe der Soziologie, nicht der Philosophie. Insofern wird in der Regel auf soziologische oder sozialanthropologische Begriffsklärungen zurückgegriffen. Dennoch haben sich einige Philosophen wie Josef Pieper, die sogenannte Ritter-Schule und Alasdair MacIntyre mit der Theorie der Tradition befasst. Pieper hat vor allem die Verbindung von mittelalterlicher Philosophie und Katholizismus in den Blick genommen. Die Ritter-Schule hat Tradition vor allem wegen der geschichtlichen Einbettung allen kulturellen Lebens diskutiert. MacIntyre hat als Kommunitarist auf die Notwendigkeit traditionaler und regional gültiger Maßstäbe für die gegenwärtige Ethik und Politik verwiesen. In Abgrenzung zu Pieper und MacIntyre und im Rückgriff insbesondere auf die Diskurstheorie von Jürgen Habermas hat in jüngster Zeit Karsten Dittmann versucht Tradition als Bedingung entgrenzter, generationsübergreifender Diskurse zu verstehen, die langwährende Wandlungsprozesse wie das Projekt der Aufklärung erst verständlich machen.

Der Begriff Traditionelle Religion wird nicht selten als Synonym für die mündlich überlieferten Ethnischen Religionen verwendet, deren Vorstellungen praktisch ausschließlich auf den Tradierungsprozess zurückgehen. Doch auch in den Weltreligionen spielen Traditionen eine wichtige Rolle:

Tradition ist im Judentum immer im Zusammenhang von Tradierung, Lehre und Erinnerung gesehen worden. In Deuteronomium 6 (5. Mose 6) findet sich die Anweisung, das jüdische Glaubensbekenntnis als Summe des (göttlichen) Gesetzes an den Sohn weiterzugeben, dass dieser es an seinen Sohn weitergebe. Außerdem soll die Erinnerung an die Geschichte des eigenen Volkes, seine Entstehung und an den mit Gott am Berge Sinai geschlossenen Bund tradiert werden.

Kern des jüdischen Traditionsverständnisses ist das Gesetz, die Tora. Bei der Überlieferung der Tora wird unterschieden zwischen der schriftlichen Tora (die sogenannten fünf Bücher Mose) und der mündlichen Tora, der (zunächst) mündlich überlieferten Auslegung der schriftlichen Tora. Diese ist wiederum zum Teil verschriftlicht im Talmud.

Einen eigenen Begriff für solche Tradition gibt es im Tanach nicht. Es gibt wohl das Wort "magan", das "überliefern" im Sinne von "ausliefern" meint, nicht aber im hier behandelten Sinn. Ein solches Wort entwickelt sich erst später aus dem Wort "masorät" (das Verpflichtende, Bindende). Daraus leiten sich die Bezeichnung Masoreten ab, die im Speziellen für eine jüdische Gelehrtengruppe des Mittelalters gebraucht wird. Die Masoreten bemühten sich um eine möglichst genaue schriftliche Überlieferung der Tora. Sie erstellten unter hinzufügen der Masora, einem umfangreichen textkritischen Apparat, den sogenannten Masoretischen Text. Masora gilt heute als Kernbegriff des jüdischen Überlieferungsverständnisses.

Eine bekannte Tradition im Judentum ist die Brit Mila (Beschneidung männlicher Neugeborener kurz nach der Geburt). Brit Shalom, die unblutige Variante, ist wenig verbreitet.

In der römisch-katholischen Kirche wird unter Tradition die neben der Bibel stehende, aber genauso verbindliche Glaubenslehre seit den Aposteln und Kirchenvätern verstanden. Als Traditionsprinzip dient diese Glaubenslehre in der römisch-katholischen Exegese zur Auslegung der christlichen Heiligen Schrift; nach römisch-katholischer Auffassung kann die wahre Aussage christlich-biblischer Texte nur durch die Auslegungstradition der Kirche verstanden werden. Das Traditionsprinzip ergänzt demnach das Schriftprinzip.

Seit der Reformationszeit ist der Bezug auf Tradition zu einem besonderen Merkmal vor allem des konservativen Katholizismus geworden. So widmete sich das Tridentinum, das als Beginn der Gegenreformation gilt, in seiner ersten Sitzungsperiode von 1545 bis 1547 dem Verhältnis von Bibel und Tradition. Im "Dekret über die Annahme der heiligen Bücher und der Überlieferungen" wird der Anspruch der Tradition in Abgrenzung zur protestantischen Auffassung dokumentiert. Allerdings wird zu diesem Zeitpunkt der Traditionsbegriff selbst noch nicht ausdrücklich reflektiert. Das geschieht erst mit dem Französischen Traditionalismus, der eine konservative, katholische Reaktion auf die Französische Revolution darstellt, getragen von katholischen Adligen und Gelehrten wie Louis de Bonald und Joseph de Maistre. Der ausdrückliche Bezug auf Tradition und die Vorrangstellung der Tradition gegenüber der Vernunft bringt der Bewegung die Bezeichnung „Traditionalismus“ ein, die seither für viele reform- und aufklärungskritische, anti-moderne Auffassungen steht. Im 20. Jahrhundert steht für solche traditionalistischen Auffassungen des Katholizismus insbesondere die Priesterbruderschaft St. Pius X.

Der Begriff der Orthodoxie verweist bereits auf die beiden wesentlichen Aspekte des orthodoxen Traditionsverständnisses: Orthodoxie heißt zugleich „richtiger Glaube“ und „richtiger Lobpreis“. Die „Rechtgläubigkeit“ bezieht sich vor allem auf die biblische Überlieferung. Für den orthodoxen Glauben ist wichtig, sich dem Ursprünglichen zuzuwenden und diesem Ursprünglichen treu zu bleiben. Der biblische Text gilt als Garant, Herzstück und Kern der Tradition. An diesem Punkt unterscheidet sich die Orthodoxie wesentlich vom römischen Katholizismus, der die kirchliche Lehrtradition eher gleichberechtigt neben die Bibel stellt. In den Anfängen der Reformation sahen die ersten Reformatoren in den orthodoxen Kirchen mögliche Verbündete. Erste Kontaktaufnahmen bereits in der ersten Hälfte des 16. Jahrhunderts blieben am Ende aber folgenlos.

Der „rechte Lobpreis“ bezieht sich auf den liturgischen Gottesdienst. Die sogenannte „Göttliche Liturgie“ geht im Kern auf jüdische und frühestchristliche Formen zurück; seit gut 1000 Jahren wird sie in unveränderter Form gefeiert. Allerdings haben sich unterschiedliche Varianten dieser Liturgie entwickelt. Die bekannteste Form geht auf die Liturgie aus Konstantinopel zurück und ist in allen orthodoxen Kirchen in Gebrauch. Diese liturgische Tradition, zu der neben den Texten auch Melodien, Handlungsabläufe, Gewänder, liturgische Geräte, der Kirchenbau selbst, Ikonen etc. gehören, hat eine ebenso große Bedeutung wie die biblische Lehre und wird auch oft zur Auslegung der Bibel herangezogen.

Seit der Reformationszeit, in der das römisch-katholische Traditionsverständnis kritisiert wurde, entwickelte sich der Begriffsgegensatz von christlicher Heiliger Schrift und Tradition. Das Traditionsprinzip wurde zugunsten des Schriftprinzips als notwendiges Element des wahren Schriftverständnisses aufgegeben; nach evangelischer Lehre ist die heilige Schrift selbsterklärend und deshalb allein die Schrift verbindlich für Fragen des Glaubens (vergleiche sola scriptura). In einer gewissen Spannung hierzu stehen die neuen Traditionen, die sich in den einzelnen evangelischen Konfessionen herausgebildet haben.

Die neuzeitliche Traditionskritik der Aufklärung verdankt sich wesentlich des traditionskritischen Impulses der Reformation, ging aber auch wesentlich darüber hinaus, indem sie auch die Bibel selbst als zu kritisierende Tradition verstand.

"Traditionskritik" ist zum einen der Name einer Methode in der historisch-kritischen Textforschung, zum anderen eine Bezeichnung der Kritik an Tradition und den tradierten Inhalten selbst.


In Europa begann mit der Reformation, später mit Rationalismus und Aufklärung, ein kritisches Infragestellen überlieferter Formen des Wissens, Glaubens und der Moral. Mit der Betonung des Vernunftprinzips (das an die Stelle des reformatorischen Schriftprinzips trat) wurde die Gültigkeit jedes Traditionsprinzips in Frage gestellt. Darauf reagierte schon frühzeitig der "Französische Traditionalismus", Ausdruck der Reaktion. Das Kräftemessen von Tradition und Vernunft hält bis in die Gegenwart an. Zusammen mit der Eigendynamik eines rationalisierenden Kapitalismus und den Folgen kultureller und ökonomischer Globalisierung ist derzeit eine weltweite Revision überkommener Werte und Überlieferungen zu beobachten. Als Gegenreaktion sind ebenfalls weltweit fundamentalistische Tendenzen zu verzeichnen. Wie schon der "Französische Traditionalismus" ist die Reaktion in der Gegenwart häufig religiös motiviert und gewaltbereit.





</doc>
<doc id="5222" url="https://de.wikipedia.org/wiki?curid=5222" title="Tibesti">
Tibesti

Das Tibesti ist ein aus Vulkanen bestehender Gebirgszug im Tschad und zugleich das höchste Gebirge der Sahara.
Seine nördlichen Ausläufer erstrecken sich mehrere hundert Kilometer auf das Territorium von Libyen.
Es erhebt sich am Nordrand des Tschadbeckens und steigt aus der flachen Wüstenlandschaft mit mehreren Schichtstufen auf. Das Hochgebirge ist teilweise stark zerklüftet und zeigt zahlreiche Vulkankrater und Schlackenkegel. Das Tibesti gehört zu den isoliertesten Regionen der Erde und wird von den Tubbu besiedelt. Das regionale Verwaltungszentrum ist die Stadt Bardaï mit rund 1500 Einwohnern.

Das Tibesti-Gebirge bedeckt ein Gebiet von rund 100.000 km² und dehnt sich vom 19. bis zum 23. nördlichen Breitengrad und vom 16. bis zum 19. östlichen Längengrad aus. Die sehr starke vulkanische Tätigkeit kann als ein Beispiel für die Entstehung von kontinentalen Riftsystemen dienen. Seine Entstehung begann im frühen Miozän und setzte ihre Tätigkeit bis in das Quartär fort.
Allerdings scheint sich das Rift nicht weiter auszudehnen und den Zenit seiner vulkanischen Tätigkeit überschritten zu haben, denn es gibt in diesem Gebiet häufig Calderen und eingestürzte Magmakammern, die sich derzeitig nicht mehr auffüllen und zahlreiche Kratersysteme hinterlassen haben.

Die Vulkane des Tibesti bestimmen die Topographie des Gebirges und gehören zu den kontinentalen Riftvulkanen, von denen mindestens drei Vulkane und ein Vulkanfeld als "aktiv" oder als "potenziell aktiv" beschrieben worden sind.
Aufgrund ihrer abgelegenen Lage wurde erst in den 1970er Jahren die aktive vulkanische Tätigkeit aus dem Weltraum entdeckt, als ein sowjetischer Satellit der Kosmos-Serie einen Ausbruch im Thermalquellenfeld Yi Yerra am Südhang des Emi Koussi beobachtete.

Aufgrund seiner Höhe erhält das Gebirge mehr Niederschlag als das Umland. Der höchste Gipfel ist der Vulkan Emi Koussi mit 3445 Metern. Weitere Vulkane sind der Tarso Toussidé mit 3265 Metern, der Tarso Voon mit 3100 Metern und der Tarso Toon mit 2625 Metern Höhe. Im westlichen Teil des Gebirges liegt das ausgedehnte Vulkanfeld Tarso Toh. Eine regionale wirtschaftliche Bedeutung haben die Salzablagerungen in der Caldera Era Kohor des Emi Koussi und in der Caldera Trou au Natron südöstlich des "Tarso Toussidé".

Im zentralen Teil des Tibesti liegt in der Nähe des "Tarso Voon" das Soborom-Solfatarenfeld, das von der lokalen Bevölkerung für medizinische Zwecke aufgesucht wird.

Auf dem Territorium Libyens liegt der 2267 Meter hohe Bikku Bitti und flacht nach dem 1650 Meter hohen Jabal Nuqay in die wüste Ebene Libyens ab.

Im Norden des Tibesti liegt sein einziger Süßwassersee, der Mare de Zoui, der einige wenige Hektar groß ist.

Viele der in der Region des Tibesti verwendeten Namen entstammen dem Arabischen und den Tedaga- und Dazaga-Sprachen, diese gehören zur Gruppe der saharanischen Sprachen.
Der Begriff "Ehi" wird für Berggipfel oder Hügel mit steilen Flanken verwendet. "Emi" wird verwendet für größere Berge, aber auch für Gebirgszüge; der Begriff "Tarso" wird für Hochplateaus oder Berge mit Bergflanken, die ein geringes Gefälle haben, verwendet.
Der Begriff "Ehra" wird für Vulkankrater und Calderen verwendet. In der Standardliteratur über die Vulkane des Tibesti werden diese einheimischen Namen jedoch nur selten verwendet oder werden bis auf wenige Beispiele richtig verwendet, wie bei dem Emi Koussi und dem Tarso Toh.

Die Gebirgsregion des Tibesti gehört zu dem Ariden Klimatyp. Die jährliche Niederschlagsmenge wird mit unter 600 mm pro Jahr angegeben. Das Gebiet kann nur wenige Einwohner ernähren, aus diesem Grunde trägt das Tibesti den Beinamen „Bergland des Hungers“ (Siehe unten: Werner Gartung). Die bekannten maximalen Temperaturen liegen um die 30 °C in den Niederungen und um 20 °C in den Höhenlagen des Gebirges. In den Wintermonaten fällt diese jedoch auf ca. 12 in den Niederungen und 9 °C in den Höhenlagen.

Die Vegetation im Gebirge des Tibesti variiert mit der Höhenlage und dem Gefälle. In den südwestlichen Gebirgshängen liegen die Wadis Enneri Tegaham, Enneri Mi, Enneri Ké die bei größeren Niederschlägen Oberflächenwasser führen und das Wachstum von Bäumen wie der Doumpalme ("Hyphaene thebaica"), den Zahnbürstenbaum ("Salvadora persica"), Tamarisken ("Tamarix articulata"), den Anabaum ("Acacia albida") und anderen tropischen Pflanzen, den Abutilon, Hibiskus und "Tephrosia" ermöglichen.

In den höheren Lagen des Gebirges wachsen an den Süd- und Südwesthängen der endemische "Ficus teloukat", an den westlichen Berghängen die "Myrtus nivellei" und an den nördlichen Berghängen die "Tamarix gallica nilotica".

An größeren Säugetieren kommen in dem Gebirge die Dorkasgazelle "(Gazella dorcas)", der Mähnenspringer "(Ammotragus lervia)" und der Gepard "(Acinonyx jubatus)" vor. Populationen kleinerer Säugetiere umfassen den Klippschliefer "(Procavia capensis)", den Kaphasen "(Lepus capensis)" und die Stachelmäuse "(Acomys spp)".





</doc>
<doc id="5223" url="https://de.wikipedia.org/wiki?curid=5223" title="Tunesien">
Tunesien

Tunesien (; amtlich Tunesische Republik, ) ist ein Staat in Nordafrika, der im Norden und Osten an das Mittelmeer, im Westen an Algerien und im Süd-Osten an Libyen grenzt. Sein Name ist von dem Namen seiner Hauptstadt Tunis abgeleitet. Tunesien gehört zu den Maghreb-Ländern. Die größte vorgelagerte Insel ist Djerba (514 km²). Das Land ist mit einer Fläche von 163.610 km² ungefähr doppelt so groß wie Österreich. Es wird von mehr als zehn Millionen Menschen bewohnt.

Das Land unterlag im Laufe seiner Geschichte dem Einfluss mehrerer Völker. Ursprünglich war es von den Berbern besiedelt. Um 800 v. Chr. gründeten die Phönizier erste Niederlassungen im tunesischen Küstenstreifen. Die Römer gliederten es in ihre Provinz Africa ein. Das Christentum herrschte in der Folge bis zur Arabisierung ab dem 7. Jahrhundert vor. Eine kulturelle Blütezeit erlebte die Region im 12. Jahrhundert. Im 16. Jahrhundert begann die Herrschaft des Osmanischen Reiches, die bis zum Ende des 19. Jahrhunderts andauerte, als das Land französisches Protektorat wurde. Seine Unabhängigkeit erlangte Tunesien im Jahre 1956. Von 1956 bis 2011 wurde es durchgängig autoritär von der Einheitspartei Neo Destour/RCD regiert. Im Zuge der Revolution wurde eine Verfassunggebende Versammlung gewählt, die 2014 eine neue Verfassung verabschiedet hat.

Tunesien ist das nördlichste Land Afrikas und nur 140 Kilometer von Sizilien entfernt. Es erstreckt sich zwischen dem Mittelmeer und der Sahara, zwischen 37° 20’ und 30° 10’ nördlicher Breite sowie zwischen 7° 30’ und 11° 30' östlicher Länge. Die größte Nord-Süd-Ausdehnung zwischen Ras al Abiad (Cap Blanc) und der Grenzstation Bordj el Khadra beträgt rund 780 km, die größte Ost-West-Ausdehnung zwischen der Insel Djerba und Nefta etwa 380 km. Die Mittelmeerküste hat eine ungefähre Länge von 1.300 Kilometern.

Der Nordwesten Tunesiens wird vom Tell-Atlas bestimmt. Parallel zur Nordküste verlaufen von der algerischen Grenze bis zur Bucht von Bizerta die Gebirgszüge der Kroumirie (700–800 m Höhe). Daran schließt sich nordöstlich das Mogod-Bergland (300–400 m Höhe) an, das zum Beispiel am Ras al Abiad in einer meist steilen Felsküste ins Mittelmeer abfällt. Auf der dem Wind abgewandten Seite des Gebirges schließt sich das Talbecken des ganzjährig wasserführenden Medjerda an, dessen Unterlauf zur wichtigsten Agrarzone des Landes gehört.
Die Bergrücken der Dorsale verlaufen von Nordost (am Westrand von Kap Bon beginnend) nach Südwest mit dem höchsten Berg Tunesiens (Djebel Chambi, 1544 m) mit einer Länge von 220 Kilometern. Die nordöstliche Verlängerung dieser Gebirgszüge bildet die Halbinsel Cap Bon mit fruchtbaren Ebenen und einigen Erhebungen (Djebel Beno Oulid, 637 m und Djebel Korbous, 419 m), die jedoch als eigenständige Landschaftsregion aufgefasst wird.

Östlich der Dorsale, entlang der Mittelmeerküste zwischen Hammamet und Skhira, Sousse und Sfax, liegt der Sahel (arabisch für Küste) genannte Küstenstreifen, der durch Regen bringende Ostwinde sehr fruchtbar ist und unter anderem große Olivenbaumkulturen ermöglicht.

Südlich der Dorsale schließt sich die Region des Zentraltunesischen Steppenlandes an, die an ihrem Südrand mit dem "Nördlichen Gebirgssaum" einen Übergang zur Schottsenke (Chott el Djerid und Chott el Gharsa) bildet. Die von Salzseen und Oasen geprägte Landschaft geht weiter südlich am Östlichen Großen Erg in die Wüstenlandschaft der Sahara mit dem "Jebil-Nationalpark" über. In südöstlicher Richtung folgt das bis zu 600 m hohe Kalksteinplateau Dahar, das mit einem Schichtstufenland an die Wüstensteppe der Djeffara-Ebene anschließt. Diese Landschaft erstreckt sich weiter über die Landesgrenze nach Libyen.

Entlang des Mittelmeeres, um den Golf von Gabès liegt die "Litoralzone", die durch sandige Flachküsten, Lagunen und vorgelagerte Inseln (beispielsweise Djerba) gekennzeichnet ist.

Die Gewässer Tunesiens befinden sich fast alle im Norden des Landes. Der wichtigste Fluss ist der Medjerda, er bekommt die meisten Niederschläge (400 mm pro Jahr) und führt 82 % der Wasservorkommen. Daneben gibt es noch einige kleinere Wadis, also Flüsse, die nicht ganzjährig Wasser führen. Wichtigste Seen, Lagunen und Sabcha sind der See von Bizerta, der Ichkeul-See, der See von Tunis, die Lagune von Ghar El Melh, die Sabcha Ariana und die Sabcha Sejoumi.

Die Landesmitte und der Süden Tunesiens sind durch Aridität und Abflusslosigkeit gekennzeichnet. Die Gewässer wie die Sabcha Sidi El Héni haben nur zwölf Prozent bzw. sechs Prozent der Wasserressourcen. Allerdings existieren dort große Grundwasservorkommen, was die Fläche an Oasen in den letzten dreißig Jahren von 15.000 auf 30.000 Hektar zu vergrößern erlaubt hat.

Bereits während der Kolonialzeit wurde mit dem Bau von Stauseen begonnen, damals vor allem, um Tunis mit Trinkwasser zu versorgen. Nach der Unabhängigkeit wurden die Projekte weitergeführt, damals mit dem Ziel der Bewässerung. Seit den 1980er Jahren ist die Verstädterung für den starken Anstieg des Wasserbedarfs verantwortlich. Mittlerweile gibt es in Tunesien 21 große Staudämme, zahlreiche kleinere Stauanlagen, sowie 98 Kläranlagen. 80 % des Wasserverbrauchs entfiel im Jahr 2000 auf die Landwirtschaft. Ab dem Jahr 2030 wird mit ernsthaftem Ressourcendefizit an Süßwasser gerechnet.

In Tunesien stoßen mediterranes und arides Klima aufeinander. Die Niederschläge nehmen von Nord nach Süd ab und von Ost nach West leicht zu. Es lassen sich unterscheiden der winterfeucht-sommertrockene Norden, die vom wechselhaften Klima bestimmte zentraltunesische Steppenregion mit heißen Sommern, kalten Wintern und abnehmenden Niederschlägen, die vom Meer beeinflusste Mittelmeerküste mit ausgeglichenerem Klima und das Wüstenklima südlich der Schotts.

Mit zunehmender Entfernung vom Mittelmeer weicht sein ausgleichender Einfluss einem kontinentalen Klima. Die Mitteltemperaturen liegen im Januar bei 10 °C, im August bei 26 °C (Tunis). Südlich des Atlas herrscht ganzjährig trockenheißes Wüstenrandklima mit sehr unregelmäßigen Niederschlägen. Die Temperaturen erreichen hier Maximalwerte bis 45 °C, wobei es zu 10 °C Temperaturdifferenz im Schatten kommen kann (normalerweise nur 5 °C).
Die extremsten Unterschiede werden in der Sahara mit sommerlichen Temperaturen von 50 °C und Bodenfrösten im Winter erreicht. Unerträgliche Hitze kann der in Tunesien Chehili genannte Saharawind Schirokko bringen.

Niederschläge fallen fast nur in den Wintermonaten und werden meistens von Tiefausläufern des weiter nördlich gelegenen Westwinddrifts herangeführt. Im Sommer liegt das gesamte Land im Bereich der subtropischen Hochdruckzone, welche die Tiefdruckgebiete der Westwinddrift um das Mittelmeer herumleitet. Jedoch kann es in Ausnahmefällen auch im Sommer zu heftigen Regenfällen kommen, die vorher ausgetrocknete Wadis in reißende Ströme verwandeln. Während im Norden die jährliche Niederschlagsmenge bei 500 bis höchstens 1000 mm an der Nordküste und im Gebirge liegt und damit für einen erfolgreichen Regenfeldbau ausreicht, ist im Süden die Verdunstung stärker als die unregelmäßige Niederschlagsmenge von allenfalls 200 mm pro Jahr.

An der Nordküste und im Atlasgebirge wächst mediterraner Laub- und Buschwald (Macchie) mit Steineiche, Korkeiche und Aleppo-Kiefer, wo neben Kleinwild auch Wildschweine Nahrung finden. Zwischen 1990 und 2000 hat der Waldbestand um 0,2 % zugenommen.

Im Nationalpark Djebel Chambi leben neben dem Mähnenschaf die bedrohte Cuviergazelle. In den sich anschließenden südlichen Steppen und Halbwüsten lebt die Dorkasgazelle und vereinzelt auch noch wenige Exemplare der Dünengazelle. Ursprünglich kam auch die Säbelantilope in diesen Trockenzonen vor; diese wurde mittlerweile in weitläufigen, eingezäunten Bereichen im Bou-Hedma-Nationalpark wieder angesiedelt. In den Wüstengebieten kommen darüber hinaus zahlreiche kleinere Tierarten, wie etwa Heuschrecken-, Skorpion-, Schlangen- und verschiedene Vogelarten vor. Die Sumpfgebiete des Ichkeul-Nationalparks im Norden des Landes sind ein bedeutendes Vogelschutzgebiet und zählen zum UNESCO-Weltnaturerbe.

Tunesien überschritt im Jahr 2014 die Schwelle von elf Millionen Einwohnern. Dies bedeutete eine Verdreifachung der Bevölkerung seit 1956 und eine Verdoppelung seit 1970. Seit 1990 verlangsamte sich das Bevölkerungswachstum jedoch. Tunesien hat heute die „älteste“ Bevölkerung Afrikas (bezogen auf den Median, der bei 32,4 Jahren liegt), die niedrigste Geburtenrate in der arabischen Welt (1,9 Kinder pro Frau) und ein Bevölkerungswachstum von etwa einem Prozent.

Die große Mehrheit der Tunesier (98 %) identifiziert sich kulturell mit den Arabern, wenngleich Studien belegen, dass sie aus ethnischer Sicht den Berbern und auch den Iberern näher stehen, während der genetische Anteil der Araber, die die Region im 7. und 8. Jahrhundert besiedelten, geringer ausfällt. Unter den Zivilisationen, die das Gebiet des heutigen Tunesiens besiedelt haben und die zu jeweils unterschiedlichen Graden assimiliert wurden, sind die Phönizier, die Römer, die aus Germanien kommenden Vandalen, die Osmanen und zuletzt die Franzosen. Dazu kamen im 15. Jahrhundert zahlreiche Mauren und Juden aus Andalusien.

Die ersten Ostaraber kamen im 7. Jahrhundert mit der muslimischen Eroberung des Maghreb. Sie islamisierten den Großteil der Ifrīqiya. In dieser Epoche entstanden neue Städte wie Kairouan und Mahdia. Ab dem 11. Jahrhundert kamen die aus Ägypten vertriebenen Banū Hilāl im heutigen Tunesien an und besiegelten die sprachliche und kulturelle Arabisierung des Landes. Die berberische Sprache und Kultur ist nur in einigen geographisch isolierten Gebieten in den Bergen nahe Matmata, Tataouine, Gafsa oder Sbeitla erhalten geblieben. Anders als in Marokko oder Algerien, wo die Berber eine ethnische Minderheit darstellen, ist ihre Zahl in Tunesien eher gering.

Tunesien ist unter den Maghreb-Staaten das aus linguistischer Sicht homogenste Land, weil fast die gesamte Bevölkerung Tunesisch-Arabisch spricht und auch das Schriftarabische, die offizielle Amtssprache des Landes, beherrscht. Für das Tunesisch-Arabisch, das eigentlich eine Mischung mehrerer Dialekte ist, gibt es keine offizielle Regulierung. Es wird vor allem als Alltagssprache verwendet. Nur im Süden des Landes und auf der Insel Djerba werden noch vereinzelt berberische Dialekte benutzt.

Während der Zeit des französischen Protektorats in Tunesien wurde die französische Sprache eingeführt, zum Teil auch mit Zwang, insbesondere in den Bildungseinrichtungen. Nach der Unabhängigkeit wurde in offiziellen Institutionen die arabische Sprache wieder eingeführt. Verwaltung, Justiz und Bildungswesen blieben noch lange Zeit zweisprachig. Tunesien ist dem Einfluss europäischer Sprachen auf Grund seiner geographischen Lage sowie durch Medien und Tourismus stark ausgesetzt, was Kenntnis dieser Sprachen bei den Tunesiern fördert.

In den 1990er Jahren wurde das Französische aus dem öffentlichen Leben in Tunesien wieder zurückgedrängt, um einerseits den Zugang zu höherer Bildung zu vereinfachen und um das arabisch-islamische Flair im öffentlichen Raum zu beleben. Seit Oktober 1999 wird von allen Geschäftstreibenden verlangt, in ihren Werbeaufschriften mindestens doppelt so viel Platz für arabische wie für lateinische Zeichen zu verwenden. Die Verwaltung wurde dazu angehalten, alle Kommunikation auf Arabisch umzustellen, obwohl dies bisher nur im Verteidigungs- und Justizministerium sowie im Parlament gelungen ist. Französisch wird somit zu einem Symbol des höheren Bürgertums. Der Einfluss durch Touristen aus Europa führt dazu, dass neben dem Französischen Englisch als Verkehrssprache vermehrt verwendet wird.

Tunesien investierte 2015 18 % des Staatshaushaltes in das Bildungssystem und weist mit über 80 % eine hohe Alphabetisierungsrate auf. 91 % der Kinder schlossen die Primarschule ab und 71 % die Sekundarschulen. 30 % der Schulabgänger beginnen ein Studium.

Der Islam ist in Tunesien Staatsreligion; 98 % der Bevölkerung bekennen sich zu diesem Glauben. 85 % der tunesischen Muslime gehören dem malikitischen Madhhab der sunnitischen Glaubensrichtung des Islam an. Der Rest sind Hanafiten und Ibaditen. Christen und Juden sind kleine Minderheiten, aber das Land war gegenüber religiösen Minderheiten tolerant. Seit der Revolution 2011 erhalten radikale salafistische Strömungen starken Zulauf.

Im Volksglauben der Tunesier finden sich noch heidnische Überbleibsel wie etwa der Glaube an den Bösen Blick. Das ganze Land ist von "Qubbas" übersät. Diese kleinen, meist weißen Kuppelbauten sind Pilgerorte, häufig Grabstätten von islamischen Heiligen "(Marabouts)", von denen geglaubt wird, dass sie Botschafter zwischen Mensch und Gott sind. Im Volksislam werden Marabouts um Hilfe gebeten, auch wenn dies vom offiziellen Sunnitentum als Abgötterei "(Schirk)" bezeichnet wird. Schwarzafrikanische Sklaven brachten den "Stambali-"Besessenheitskult mit, der sich als gesellschaftlich randständiges Phänomen auch unter arabischen Tunesiern verbreitet hat.

Das Judentum war in Tunesien einst sehr bedeutend, heute gibt es nur noch rund 1500 Juden. Auf der Insel Djerba steht seit wahrscheinlich über 1000 Jahren die al-Ghriba-Synagoge (Die Erstaunliche), eine der ältesten Synagogen der Welt. Jedes Jahr findet dort die größte jüdische Wallfahrt Nordafrikas statt, zu der Gläubige aus der ganzen Welt erwartet werden. Auf Djerba leben mehrheitlich muslimische Kharidjiten.

Die Verfassung Tunesiens sieht die freie Ausübung des Glaubens vor, so lange diese nicht die öffentliche Ordnung stört. Dieses Grundrecht wurde von der tunesischen Regierung in der Regel respektiert. Religiöse politische Parteien waren jedoch nicht zugelassen, Proselytismus und Polygamie sind verboten. Das Tragen des Hidschab war eingeschränkt und in der Verwaltung und öffentlichen Schulen nicht gestattet, dieses Verbot wurde nach dem Sturz des Ben Ali-Regimes im Frühjahr 2011 aufgehoben. Islamische Feiertage (wie etwa das Islamische Opferfest, das Fest des Fastenbrechens oder Mawlid an-Nabi) sind in Tunesien gesetzliche Feiertage.

Für das Jahr 2007 wurde die Zahl der im Ausland lebenden Tunesier auf eine Million Personen geschätzt. Davon entfallen 84 % auf Europa, 600.000 allein auf Frankreich, 143.000 auf Italien und 80.000 auf Deutschland. In Nordamerika leben 26.000 und in den arabischen Staaten insgesamt 140.000 Tunesier, davon 80 % in Maghrebländern (überwiegend in den Nachbarstaaten Libyen und Algerien, wo sie sich als Nachbarn kulturell schnell integrieren können) und etwa 24.655 hochqualifizierte Arbeitskräfte in den Golfstaaten. Die Tunesier in den europäischen Ländern besitzen in der Regel die doppelte Staatsbürgerschaft. Die meisten waren entweder im 19. Jahrhundert während des französischen Protektorats nach Europa ausgewandert oder kamen in den 1950er und 1960er Jahren als Gastarbeiter. Diese Auswanderer haben eine große Bedeutung für die tunesische Wirtschaft: Sie überweisen einerseits hohe Summen, um die daheim gebliebenen Angehörigen zu unterstützen, andererseits investieren Heimkehrer aus dem Ausland viel in heimische Wirtschaftsbetriebe.

Erste Spuren von nomadisch lebenden Jägern und Sammlern aus der Altsteinzeit wurden in der 20 km östlich von Gafsa gelegenen Oase El Guettar gefunden.

Auf das Ibéromaurusien, eine an der nordafrikanischen Küste verbreitete Kultur, folgte das Capsien. Von dieser Kultur wurden 15.000 Jahre alte Skelette und Werkzeuge gefunden, die darauf hinweisen, dass die Capsien-Menschen neben Steinwerkzeugen auch Nadeln aus Knochen zum Nähen von Kleidung aus Tierhäuten herstellten.

Während der Jungsteinzeit formte sich die Sahara mit ihrem heutigen Klima. Diese Epoche ist gekennzeichnet von der Einwanderung der Berber. Es entstanden erste Kontakte mit den Phöniziern in Tyros, die gegen Ende der Jungsteinzeit begannen, das heutige Tunesien zu besiedeln und später das Karthagische Reich gründeten.

Das heutige Tunesien erlebte zu Beginn der geschichtlichen Aufzeichnungen die Gründung von Handelsniederlassungen durch Siedler aus dem östlichen Mittelmeer. Gemäß der Legende war die erste dieser Niederlassungen Utica im Jahr 1101 v. Chr. Im Jahr 814 v. Chr. gründeten aus Tyros kommende phönizische Siedler die Stadt Karthago. Nach der Legende war es die Königin Élyssa, die Schwester des Königs von Tyr, Pygmalion, welche die Stadt gründete.

Karthago wurde innerhalb von 150 Jahren zur größten Macht des westlichen Mittelmeeres. Die Einflussnahme geschah teils durch Kolonisierung, größtenteils jedoch durch Handelsniederlassungen und Verträge. Diese Macht und das hohe landwirtschaftliche Potential des karthagischen Mutterlandes führten dazu, dass das Interesse des jungen, erstarkenden Römischen Reiches geweckt wurde und es kam zur Konfrontation, die in den drei Punischen Kriegen gipfelte. Karthago konnte mit seinen unter anderen von Hannibal geführten Truppen während des Zweiten Punischen Krieges (218–201 v. Chr.) das Römische Reich mehrmals an den Rand einer Niederlage bringen. Am Ende des Dritten Punischen Krieges (149–146 v. Chr.) wurde die Stadt Karthago drei Jahre belagert und letzten Endes zerstört. Das Gebiet des heutigen Tunesien wurde Teil der römischen Provinz Africa mit Hauptstadt Utica. Im Jahr 44 v. Chr. beschloss Caesar, eine Colonia in Karthago zu gründen, was jedoch von Augustus erst mehrere Jahrzehnte später verwirklicht wurde, und im Jahr 14 wurde Karthago Hauptstadt von Africa.

Africa wurde, neben Ägypten, zu einem der bedeutendsten Lieferanten landwirtschaftlicher Produkte Roms, vor allem lieferte Africa Getreide und Olivenöl. Es entstand ein dichtes Netz an römischen Siedlungen, deren Ruinen bis heute noch zu sehen sind, etwa Dougga (römisch "Thugga"), Sbeitla ("Sufetula"), Bulla Regia, El Djem ("Thysdrus") oder Thuburbo Majus. Africa war, zusammen mit Numidien, für sechs Jahrhunderte lang eine sehr wohlhabende Provinz, wo etwa die Mosaikkunst blühte. Dank seiner Rolle als Knotenpunkt der Antike siedelten sich in der Folge auch Juden und die ersten Christen im heutigen Tunesien an.

Das Christentum breitete sich schnell aus, vor allem durch die Ankunft von Siedlern, Händlern und Soldaten. Bekanntheit erlangte Karthago diesbezüglich, dass hier der einflussreiche christliche Apologet Tertullian lebte und wirkte, so dass Nordafrika sich in der nächsten Zeit zu einem von mehreren Zentren des Christentums entwickelte. Die heidnische Bevölkerung widersetzte sich zunächst dem neuen Kult, später wurde die Christianisierung auch mit Gewalt durchgesetzt. Ab 400 durchdrang das Christentum durch die Aktivitäten von Augustinus von Hippo und seiner Bischöfe sämtliche Gesellschaftsschichten, indem sie die städtische Aristokratie und die Landbesitzer auf ihre Seite brachten. Krisen wie etwa das donatistische Kirchenschisma, das mit dem Konzil von Karthago abgewendet wurde, überwand das Christentum dank der guten wirtschaftlichen und sozialen Lage schnell. Davon zeugen Ruinen von Bauwerken wie die Basilika von Karthago oder die zahlreichen Kirchen, die auf heidnischen Tempeln (wie etwa in Sufetula) erbaut wurden.

Am 19. Oktober 439 eroberten die Vandalen und Alanen Karthago und errichteten ein Königreich, das ein Jahrhundert dauerte. Die Vandalen gehörten dem Arianismus an, einer Glaubensrichtung, die auf dem Ersten Konzil von Nicäa zur Häresie erklärt worden war. Sie forderten von der zumeist katholischen Bevölkerung die Treue zu ihrem Glauben und antworteten auf deren Weigerung mit Gewalt. Besitztümer der katholischen Kirche wurden beschlagnahmt. Die Kultur der ansässigen Bevölkerung blieb aber unangetastet und auch das Christentum florierte, soweit es die neuen Herrscher tolerierten. Das Vandalenreich ging nach der verlorenen Schlacht bei Tricamarum unter, bei der die Vandalen unter König Gelimer gegen die oströmischen Truppen von Belisar unterlagen. Kaiser Justinian I. machte aus Karthago eine Diözese und 590 das Exarchat von Karthago, das gegenüber der kaiserlichen Zentralmacht hohe zivile und militärische Autonomie besaß. Heiden, Juden und Häretiker wurden bald darauf aber von der byzantinischen Zentralgewalt, die das Christentum zur Staatsreligion erheben wollte, verfolgt.

Die ersten arabischen Vorstöße auf das heutige Tunesien begannen im Jahre 647. 661 wurde in einer zweiten Offensive Bizerta erobert; die Entscheidung fiel nach der dritten, 670 von Uqba ibn Nafi angeführten Offensive und der Gründung von Kairouan, die später Ausgangspunkt für die arabischen Expeditionen auf den nördlichen und westlichen Maghreb wurden. Der Tod von Uqba ibn Nafi 693 führte nur zu einem vorübergehenden Stillstand der arabischen Eroberung; 695 nahm der Ghassaniden-General Hassan Ibn Numan Karthago ein. Die Byzantiner, deren Seestreitkräfte den Arabern überlegen waren, griffen 696 Karthago an und nahmen es ein, während 697 die Berber unter al-Kahina die Araber in einer Schlacht besiegten. 698 jedoch eroberten die Araber Karthago erneut und besiegten auch al-Kahina.

Anders als vorherige Eroberer gaben sich die Araber nicht damit zufrieden, nur die Küstengebiete zu okkupieren, sondern machten sich auch an die Eroberung des Landesinneren. Nach einigem Widerstand konvertierten die meisten Berber zum Islam, vor allem durch die Aufnahme in die Streitkräfte der Araber. In den neugebauten Ribats wurden religiöse Schulen eingerichtet. Gleichzeitig jedoch schlossen sich zahlreiche Berber der Glaubensrichtung der Charidschiten an, die die Gleichheit aller Muslime unabhängig von ihrer Rassen- oder Klassenzugehörigkeit verkündigte. Das heutige Tunesien blieb eine Provinz der Umayyaden, bis es 750 an die Abbasiden fiel. Zwischen 767 und 776 wurde das gesamte Territorium Tunesiens von den berberischen Charidschiten unter Abu Qurra beherrscht, die sich später in ihr Königreich Tlemcen zurückziehen mussten.

Im Jahre 800 übergab der Abbasidenkalif Harun ar-Raschid seine Macht über Ifrīqiya dem Emir Ibrahim ibn al-Aghlab und übertrug ihm auch das Recht, seine Funktion zu vererben. Somit wurde die Aghlabiden-Dynastie gegründet, die ein Jahrhundert lang den mittleren und östlichen Maghreb beherrschte. Das heutige Tunesien wurde zu einem bedeutenden Kulturraum mit der Stadt Kairouan und seiner Großen Moschee im Mittelpunkt. Tunis wurde bis zum Jahr 909 die Hauptstadt des Emirates.

Das Aghlabiden-Emirat verschwand innerhalb von 15 Jahren (893–909) durch die Aktivitäten des proselytischen Ismailiten Abu Abd Allah asch-Schiʿi, unterstützt durch eine fanatisierte Armee, die sich aus dem berberischen Kutama-Stamm rekrutierte. Im Dezember 909 rief sich Abdallah al-Mahdi zum Kalifen aus und gründete damit die Fatimiden-Dynastie. Gleichzeitig erklärte er die sunnitischen Umayyaden und die Abbasiden zu Usurpatoren. Der Fatimidenstaat breitete seinen Einfluss auf ganz Nordafrika aus, indem er die Karawansereien und damit die Handelswege mit Schwarzafrika unter seine Kontrolle brachte. Eine letzte große Revolte des charidschitischen Banu Ifran-Stammes unter Abu Yazid konnte niedergeschlagen werden. Der dritte Fatimidenkalif Ismail al-Mansur verlegte die Hauptstadt nach Kairouan und eroberte 948 Sizilien. 972, drei Jahre nachdem die Region vollständig erobert war, verlegte die Fatimiden-Dynastie ihre Basis in östliche Richtung. Kalif Abu Tamim al-Muizz legte die Herrschaft über Ifriqiya in die Hände von Buluggin ibn Ziri, der die Ziriden-Dynastie gründete. Die Ziriden erlangten schrittweise die Unabhängigkeit vom Fatimiden-Kalifen, was mit einem kompletten Bruch mit den Fatimiden endete. Diese rächten sich für den Verrat damit, dass sie Beduinenstämme (die Banū Hilāl und Banu Sulaym) aus Ägypten mit Eigentumstiteln auf Land in Ifriqiya ausstatteten und gegen die Ziriden ziehen ließen. Kairouan wurde in der Folge nach fünfjährigem Widerstand erobert und geplündert. 1057 flohen die Ziriden nach Mahdia, während die Eroberer in Richtung des heutigen Algerien weiterzogen. Die Ziriden versuchten danach erfolglos, das inzwischen von den Normannen besetzte Sizilien zurückzuerobern, und 90 Jahre lang versuchten sie, Teile ihres früheren Territoriums zurückzugewinnen. Sie verlegten sich auf Piraterie, um sich am Seehandel zu bereichern.

Diese Migration war das entscheidendste Ereignis in der Geschichte des mittelalterlichen Maghreb. Sie hat das traditionelle Gleichgewicht zwischen nomadischen und sesshaften Berbern zerstört und zu einer Bevölkerungsdurchmischung geführt. Das Arabische, das bis dahin nur von den städtischen Eliten und am Hof gesprochen wurde, begann, die berberischen Dialekte zu beeinflussen.

Ab dem ersten Drittel des 12. Jahrhunderts war Tunesien häufigen Angriffen der Normannen aus Sizilien und Süditalien ausgesetzt. Das Territorium von Ifriqiya wurde gleichzeitig (1159) vom Almohaden-Sultan Abd al-Mu'min von Westen aus erobert. Wirtschaft und Handel blühten auf; Handelsbeziehungen wurden mit den wichtigsten Städten am Mittelmeer aufgenommen. Der wirtschaftliche Aufschwung bewirkte, dass das almohadische Jahrhundert als "goldenes Zeitalter des Maghreb" in die Geschichte einging, als sich große Städte mit prächtigen Moscheen entwickelten und Wissenschaftler wie Ibn Chaldūn arbeiteten.

Die Almohaden legten die Verwaltung des heutigen tunesischen Gebiets in die Hände von Abu Muhammad Abdalwahid, doch bereits sein Sohn Abu Zakariya Yahya I. löste sich 1228 ab und gründete die Dynastie der Hafsiden. Zwischen 1236 und 1574 regierte somit die erste tunesische Dynastie. Die Hauptstadt wurde nach Tunis verlegt, das sich dank des Seehandels schnell entwickelte.

Ab der zweiten Hälfte des 14. Jahrhunderts verloren die Hafsiden langsam die Kontrolle über ihr Territorium und gerieten, speziell nach der verlorenen Schlacht von Kairouan (1348) unter den Einfluss der Meriniden des Abu Inan Faris. Die Pest von 1384 traf Ifriqiya mit voller Wucht und trug zum Bevölkerungsschwund seit den Invasionen durch die Banū Hilāl bei. Gleichzeitig begannen Mauren und Juden aus Andalusien einzuwandern. Die Spanier unter Ferdinand II. und Isabella I. eroberten die Städte Mers-el-Kébir, Oran, Bejaia, Tripolis und die Algier vorgelagerte Insel. Die Hafsidenherrscher sahen sich genötigt, die Hilfe der Korsarenbrüder Khair ad-Din Barbarossa und Arudsch in Anspruch zu nehmen.

In ihrer Bedrängnis erlaubten die Hafsiden den Korsaren, den Hafen von La Goulette und die Insel Djerba als Basis zu benutzen. Nach dem Tod von Arudsch machte sich sein Bruder Khair ad-Din Barbarossa zum Vasallen des Sultans von Istanbul und wurde von ihm zum Admiral des Osmanischen Reiches ernannt. Er eroberte 1534 Tunis, musste sich aber 1535 aus der Stadt zurückziehen, nachdem diese durch eine Armada von Karl V. im Tunisfeldzug erobert worden war. 1574 wurde Tunis wieder von den Osmanen, diesmal unter Führung von Turgut Reis, erobert. Tunesien wurde damit eine Provinz des osmanischen Reiches. Die neuen Herrscher hatten aber wenig Interesse an Tunesien und ihre Bedeutung nahm ständig auf Kosten von lokalen Machthabern ab; es waren nur 4000 Janitscharen in Tunis stationiert. Im Jahre 1590 kam es zu einem Janitscharenaufstand, als dessen Resultat ein Dey an die Staatsspitze gesetzt wurde. Ihm war ein Bey unterstellt, der für die Verwaltung des Landes und die Steuereintreibung verantwortlich war. Der den Bey gleichgestellte Pascha hatte nur die Aufgabe, den osmanischen Sultan zu repräsentieren. Im Jahre 1612 gründete Murad Bey die Dynastie der Muraditen, am 15. Juli 1705 machte Husain I. ibn Ali sich zum Bey von Tunis und gründete die Dynastie der Husainiden. Unter den Husainiden erreichte Tunesien einen hohen Grad an Selbständigkeit, obwohl es offiziell noch immer osmanische Provinz war. Ahmad I. al-Husain, der von 1837 bis 1855 regierte, leitete einen Modernisierungsschub ein mit wichtigen Reformen wie die Abschaffung der Sklaverei oder die Annahme einer Verfassung.

Wirtschaftliche Schwierigkeiten, hervorgerufen durch eine ruinöse Politik der Beys, hohe Steuern und ausländische Einflussnahme zwangen die Regierung 1869, den Staatsbankrott zu erklären und eine internationale britisch-französisch-italienische Finanzkommission ins Leben zu rufen. Aufgrund seiner strategischen Lage wurde Tunesien schnell zum Zielpunkt der französischen und italienischen Interessen. Die Konsuln Frankreichs und Italiens versuchten, aus den finanziellen Schwierigkeiten der Beys ihre Vorteile zu ziehen, wobei Frankreich darauf vertraute, dass sich England neutral verhalten würde (England hatte kein Interesse daran, dass Italien den Seeweg über den Sueskanal in seine Kontrolle bringen würde), und auch darauf, dass Bismarck die Aufmerksamkeit Frankreichs von der Elsaß-Lothringen-Frage ablenken wollte.

Einfälle von Plünderern aus der Kroumirie in das Territorium Algeriens lieferten Jules Ferry den Vorwand, Tunesien zu erobern. Im April 1881 drangen französische Truppen in Tunesien ein und eroberten innerhalb von drei Wochen Tunis, ohne auf nennenswerten Widerstand zu stoßen. Am 12. Mai 1881 wurde Bey Muhammad III. al-Husain zur Unterzeichnung des Bardo-Vertrages gezwungen. Aufstände rund um Kairouan und Sfax wurden einige Monate später schnell erstickt. Das Protektorat wurde mit den Vertrag von la Marsa vom 8. Juni 1883 gefestigt. Sie räumten Frankreich weitreichende Befugnisse in der Außen-, Verteidigungs- und Innenpolitik Tunesiens ein. Frankreich gliederte das Land in sein Kolonialreich ein und vertrat in der Folge Tunesien auf dem internationalen Parkett. Der Bey musste fast seine gesamte Macht an den Generalresidenten abgeben. Auf wirtschaftlichem Gebiet gab es Fortschritte:

Am Beginn des 20. Jahrhunderts begann der Widerstand gegen die französische Besatzung. 1907 gründeten Béchir Sfar, Ali Bach Hamba und Abdeljelil Zaouche die reformistische Intellektuellenbewegung Jeunes Tunisiens. Diese nationalistische Strömung zeigte sich in der Djellaz-Affäre 1911 und im Boykott der Straßenbahn von Tunis 1912. Von 1914 bis 1921 herrschte in Tunesien der Ausnahmezustand und jegliche antikolonialistische Presseäußerung wurde verboten. Trotzdem bekam die nationale Bewegung mehr Zulauf und zu Ende des Ersten Weltkriegs wurde von einer Gruppe um Abdelaziz Thâalbi die Destur-Partei gegründet. Sie verkündete nach ihrer offiziellen Gründung am 4. Juni 1920 ein Acht-Punkte-Programm. Der Anwalt Habib Bourguiba, der schon vorher in Zeitschriften wie "La Voix du Tunisien" oder "L’Étendard tunisien" das Protektoratsregime angeprangert hatte, gründete 1932 zusammen mit Tahar Sfar, Mahmoud Materi und Bahri Guiga die Zeitschrift "L’Action Tunisienne", die neben der Unabhängigkeit auch für den Laizismus eintrat. Diese Position führte zur Spaltung der Destour-Partei auf dem Kongress von Ksar Hellal am 2. März 1934:

Nach dem Scheitern von Verhandlungen mit der Regierung Léon Blum kam es 1937 zu einigen blutigen Zwischenfällen, die in den gewaltsam niedergeschlagenen Unruhen vom April 1938 gipfelten.
Diese Unterdrückung führte dazu, dass der Néo-Destour seinen Kampf im Untergrund fortführte. 1940 lieferte das Vichy-Regime Bourguiba auf Verlangen Mussolinis an Italien aus, der sich erhoffte, damit die Résistance in Nordafrika zu schwächen. Bourguiba rief jedoch am 8. August 1942 zur Unterstützung für die Alliierten auf. Kurz darauf wurde das Land Schauplatz der Schlacht um Tunesien, an deren Ende die Truppen der Achsenmächte am 11. Mai 1943 zur Kapitulation am Kap Bon gezwungen wurden.

Nach dem Zweiten Weltkrieg wurde der bewaffnete Widerstand Teil der Strategie zur nationalen Befreiung. Verhandlungen mit der französischen Regierung wurden geführt und Robert Schuman deutete 1950 sogar eine schrittweise Unabhängigkeit Tunesiens an; nationalistische Auseinandersetzungen führten 1951 jedoch zum Scheitern dieser Verhandlungen.

Nach der Ankunft des neuen Generalresidenten, Jean de Hauteclocque, am 13. Januar 1952 und der Verhaftung von 150 Destour-Mitgliedern am 18. Januar begann eine bewaffnete Revolte, während sich die Fronten auf beiden Seiten verhärteten. Die Ermordung des Gewerkschafters Farhat Hached durch die kolonialistische Extremistenorganisation La Main Rouge führte zu Kundgebungen, Unruhen, Streiks und Sabotageaktionen, wobei das Ziel immer mehr die Strukturen der Kolonisation und Regierung wurden. Frankreich mobilisierte 70.000 Soldaten, um die tunesischen Guerilla-Gruppen unter Kontrolle zu bringen. Diese Situation wurde erst mit der Zusicherung innerer Autonomie an Tunesien durch Pierre Mendès France am 31. Juli 1954 entschärft. Am 3. Juli 1955 wurden schließlich von Tunesiens Premierminister Tahar Ben Ammar und seinem französischen Amtskollegen Edgar Faure die französisch-tunesischen Verträge unterzeichnet. Trotz des Widerstandes von Salah Ben Youssef, der in der Folge aus der Destour-Partei ausgeschlossen wurde, wurden die Verträge vom Kongress des Néo-Destour am 15. November in Sfax ratifiziert. Nach neuen Verhandlungen erkannte Frankreich am 20. März 1956 die Unabhängigkeit Tunesiens an, wobei es die Militärbasis in Bizerta behielt.

Am 25. März 1956 wurde die konstituierende Nationalversammlung des Landes gewählt. Die Néo-Destour gewann alle Sitze, und Bourguiba übernahm den Parlamentsvorsitz. Am 11. April wurde er von Lamine Bey zum Premierminister ausgerufen. Am 13. August wurde das fortschrittliche tunesische Personenstandsgesetz erlassen. Am 25. Juli 1957 wurde die Monarchie abgeschafft, Lamine Bey musste abdanken, und Tunesien wurde eine Republik. Bourguiba wurde am 8. November 1959 zu ihrem ersten Präsidenten gewählt.

Die Rechtsgrundlagen der Verfassung orientierten sich am französischen Recht. Der Islam war Staatsreligion (Artikel 1); Tunesien war aber das einzige arabische Land, das das islamische Rechtssystem Scharia in seiner Verfassung vom 1. Juni 1959 abgeschafft hatte. Lediglich Artikel 38 der tunesischen Verfassung schrieb fest, dass der Präsident ein Muslim sein muss. Nach der Unabhängigkeit waren die Frauen im Familienrecht (Eheschließung, Scheidung, Sorgerecht) den Männern gleichgestellt worden. Tunesien verfügte über ein Parlament, das aus zwei Kammern bestand ("Zweikammersystem"):

Im Jahr 1958 kam es zu einem internationalen Zwischenfall mit vielen zivilen Opfern, als die Franzosen den Grenzort Sakiet Sidi Youssef als Vergeltungsmaßnahme gegen von Tunesien aus operierende Kämpfer der FNL im Rahmen des Algerienkrieges bombardierten. Im Jahr 1961, als das Ende des Algerienkrieges absehbar war, forderte Tunesien die Rückgabe der Militärbasis von Bizerta. Die folgende Bizerta-Krise forderte etwa 1000 Todesopfer, davon die Mehrheit Tunesier. Sie endete mit der Rückgabe der Basis am 15. Oktober 1963.

Nach der Ermordung von Salah Ben Youssef, dem wichtigsten Oppositionellen seit 1955, sowie des Verbots der Kommunistischen Partei am 8. Januar 1963 wurde die tunesische Republik zu einem von der Néo-Destour geführten Einparteienstaat. Auch ihre Nachfolgerin, die 1988 gegründete Konstitutionelle Demokratische Sammlung (RCD), war bis Januar 2011 die dominierende Partei. Sie entsandte zuletzt (2010) 152 der 189 Parlamentarier.

Im März 1963 leitete Ahmed Ben Salah eine sozialistische Politik ein, unter der praktisch die gesamte tunesische Wirtschaft verstaatlicht wurde. Bereits 1969 wurde Ben Salah jedoch entlassen, nachdem es zu Unruhen wegen der Kollektivierung der Landwirtschaft gekommen war; das sozialistische Experiment war damit auch beendet. Die schwächelnde Wirtschaft und der von Muammar al-Gaddafi gepredigte Panarabismus führten zu einem 1974 gestarteten politischen Projekt, das Tunesien und Libyen unter dem Namen Arabische Islamische Republik vereinigen sollte. Dieses Projekt wurde jedoch nach nationalen und internationalen Spannungen wieder fallengelassen.

Die Verurteilung Ben Salahs zu einer hohen Gefängnisstrafe leitete eine Periode ein, in welcher der durch Ahmed Mestiri angeführte liberale Flügel der mittlerweile nach PSD umbenannten Partei die Oberhand gewann. Bourguiba wurde 1975 zum Präsidenten auf Lebenszeit ernannt, der Gewerkschaftsbund UGTT gewann während der Regierung von Hédi Nouira eine gewisse Autonomie, und die Tunesische Menschenrechtsliga konnte 1977 gegründet werden. Die erwachende Zivilgesellschaft konnte auch durch die Gewaltakte gegen die UGTT am Schwarzen Dienstag des Januar 1978 und die Angriffe auf die Bergbaustadt Gafsa im Januar 1980 nicht mehr mundtot gemacht werden.

Zu Beginn der 1980er Jahre geriet das Land in eine politische und soziale Krise, deren Ursachen in Nepotismus und Korruption, in der Lähmung des Staates angesichts der sich verschlechternden Gesundheit Bourguibas, in Nachfolgekämpfen und einer generellen Verhärtung des Regimes zu suchen sind. Im Jahre 1981 erweckte die teilweise Wiederherstellung des pluralistischen Systems Hoffnungen, die jedoch bereits mit der Wahlfälschung im November desselben Jahres zerstört wurden. Die blutige Niederschlagung der Brot-Unruhen im Dezember 1983, die erneute Destabilisierung der UGTT und die Verhaftung ihres Vorsitzenden Habib Achour trugen dann zum Sturz des alternden Präsidenten und zum sich verstärkenden Aufkommen des Islamismus bei.

Am 7. November 1987 setzte Ministerpräsident Zine el-Abidine Ben Ali den Präsidenten Bourguiba aufgrund von Senilität ab, was von der Mehrheit des politischen Spektrums begrüßt wurde. Im Dezember 1987 entließ Ben Ali sechs der neun Politbüromitglieder der regierenden Parti Socialiste Destourien (PSD) und ersetzte sie durch persönliche Vertraute. Nach dem Machtwechsel kehrten auch mehrere Exilpolitiker nach Tunesien zurück. Ende 1987 wurden 2500 Gefangene, darunter auch 600 islamische Fundamentalisten aus den Gefängnissen freigelassen. Außenpolitisch setzte Ben Ali auf eine engere Zusammenarbeit mit den Maghreb-Staaten und nahm auch die 1985 abgebrochenen diplomatischen Beziehungen zu Libyen wieder auf.

Ben Ali wurde am 2. April 1989 mit 99,27 % der Stimmen gewählt und schaffte es in der Folge, die Wirtschaft wieder anzukurbeln. Ben Ali bekämpfte den radikalen Islamismus aktiv und ersparte Tunesien somit die Gewalt, die das benachbarte Algerien erschütterte; die Ennahda-Partei wurde neutralisiert, zehntausende militante Islamisten verhaftet und in zahlreichen Prozessen zu Beginn der 1990er Jahre verurteilt. Der führende Flügel der Ennahda-Bewegung lebte im Exil in Frankreich und Großbritannien. Die laizistischen Oppositionellen gründeten 1988 mit dem "Pacte national" eine Plattform mit dem Ziel, das Regime zu demokratisieren. Die politische Opposition und Nicht-Regierungsorganisationen begannen derweil, das Regime der Einschränkung von Bürgerrechten zu beschuldigen, weil es die Repression über die Bekämpfung des radikalen Islamismus hinaus ausweitete. In den Präsidentschaftswahlen 1994 wurde Ben Ali mit 99,91 % der Stimmen wiedergewählt; im Jahr 1995 unterzeichnete er ein Freihandelsabkommen mit der Europäischen Union. Die Präsidentschaftswahl am 24. November 1999 war die erste pluralistische Wahl in der Geschichte des Landes, wurde jedoch von Ben Ali mit einem ähnlichen Stimmenanteil wie in den vorangegangenen Wahlen gewonnen. Die Verfassungsänderung des Jahres 2002 steigerte noch den Machtumfang des Präsidenten. Im selben Jahr meldete sich der islamische Terrorismus mit dem Anschlag auf die al-Ghriba-Synagoge zu Wort.

2009 wurden die Bürger Tunesiens in ihrem Recht, die Regierung abzuwählen, und ihrem Recht auf freie Meinungsäußerung erheblich eingeschränkt. Die Regierung führte strenge Einschränkungen der Meinungs-, Presse- und Versammlungsfreiheit im Vorfeld der Wahl im Oktober 2009 ein. Öffentliche Kritik wurde nicht geduldet. Es gab zahlreiche Berichte darüber, dass oppositionelle Bürger durch strafrechtliche Ermittlungen, willkürliche Verhaftungen, Reisebeschränkungen und Kontrollen gezielt eingeschüchtert wurden, um Kritik zu verhindern. Lokale und internationale Nichtregierungsorganisationen berichteten, dass Sicherheitskräfte Gefangene misshandelten. Präsident Zine el-Abidine Ben Ali wurde zuletzt im Oktober 2009 mit 89,28 Prozent Stimmenanteil im Amt bestätigt; die nächste Präsidentschaftswahl sollte Ende des Jahres 2014 stattfinden. Zine el-Abidine Ben Ali wurde aufgrund des öffentlichen Drucks durch die massiven Proteste ab Dezember 2010 gestürzt. Nach seiner Flucht nach Saudi-Arabien übernahm Parlamentspräsident Fouad Mebazaa am 14. Januar 2011 vorläufig die Amtsgeschäfte.

Am 4. Januar 2011 starb in einem Krankenhaus in Tunis Mohamed Bouazizi, ein 26-jähriger Mann, an den Verletzungen, die er sich in der Provinzhauptstadt Sidi Bouzid bei einer Selbstverbrennung am 17. Dezember 2010 zugefügt hatte. Der Gemüsehändler hatte sich selbst vor dem Gouvernementsgebäude in Brand gesetzt, um gegen die Konfiszierung seines Obst- und Gemüsestandes durch die Polizei zu protestieren. Es folgten Solidaritätskundgebungen im ganzen Land, die sich zu regimekritischen Kundgebungen ausweiteten. Forderungen nach Presse- und Meinungsfreiheit mischten sich mit Kritik an Korruption und Zensur. Der Ärger der Tunesier richtete sich auch gegen die Kleptokratie in der Umgebung Ben Alis, insbesondere durch die zahlreichen Familienmitglieder seiner Frau, Angehörige der Familie Trabelsi, die aufgrund von politischer Einflussnahme wichtige Unternehmen in Tunesien in Besitz genommen haben.

Während der Unruhen kam es im Januar 2011 zur Verhängung einer Ausgangssperre über die Hauptstadt und Teile ihrer Vororte. Präsident Ben Ali reagierte auf die Unruhen mit der Ausrufung des Ausnahmezustandes. Er löste die Regierung auf und kündigte vorgezogene Neuwahlen an, bevor er, aufgrund immer lauter werdender Proteste, am 14. Januar 2011 fluchtartig das Land verließ. Die Amtsgeschäfte wurden vom Verfassungsrat interimistisch auf den Parlamentspräsidenten Fouad Mebazaa übertragen, nachdem sie kurzzeitig durch den Premierminister Mohamed Ghannouchi geführt wurden. Die von Ghannouchi gebildete Übergangsregierung kündigte Pressefreiheit und die Freilassung aller politischen Gefangenen an. Am 3. Februar 2011 kündigte Interimspräsident Mebazaâ in einer Rede an die Nation die Wahl einer Verfassunggebenden Versammlung an, die den „endgültigen Bruch“ mit dem Ben-Ali-System einleiten sollte. Die tunesische Volkserhebung löste als „Arabischer Frühling“ im fast gesamten arabischen Raum ähnliche Bewegungen aus, die unter anderem in Libyen und Ägypten die dortigen Machthaber stürzten.

Am 23. Oktober 2011 fanden die ersten freien Wahlen zu einer Verfassunggebenden Versammlung statt, aus denen die islamistische Partei "Ennahda" als stärkste mit 90 der 217 Sitze hervorging. Die Versammlung trat am 22. November 2011 erstmals zusammen. Mit Hilfe der Kongresspartei wurde Moncef Marzouki am 12. Dezember 2011 zum neuen Staatspräsidenten gewählt. Er ernannte am 24. Dezember Hamadi Jebali zum Ministerpräsidenten.

In der Verfassunggebenden Versammlung waren u. a. folgende Parteien vertreten:

Die Ennahda-Bewegung wurde auch nach ihrem Wahlsieg zur Verfassungsgebenden Versammlung differenziert eingeschätzt: Deren Mitglieder seien „bürgerlich-konservative Muslime“, „moderate Islamisten“ oder „militante Islamisten“. Zwar hatte die Ennahda die Aktionen der Islamisten stets verurteilt und ihr Wahlprogramm war moderat verfasst (z. B. Geschlechtergerechtigkeit), doch befürchteten nicht wenige Tunesier, dass diese Forderung als Deckmantel nach einem Wahlsieg abgelegt werden könnte.

2012/13 kam es zu Übergriffen auf Abgeordnete und Politiker, die nicht der Ennahda-Partei angehörten. Die Ermordung des linken Oppositionspolitikers Chokri Belaïd am 6. Februar 2013, eines prominenten Kritikers der Ennahda-Partei, und Mohamed Brahmis am 15. Juli 2013 führten zu Massendemonstrationen gegen die Regierungspartei. Auch viele Frauen fühlten sich nach dem Sieg dieser Partei in ihren Rechten gefährdet, die ihnen schon Bourguiba 1956 und danach Ben Ali zugestanden hatten. So sollten sie zum Beispiel dem Mann nicht mehr „gleichgestellt“ sein, sondern ihn „ergänzen“ (Verfassungsentwurf vom August 2012). Dagegen gab es Demonstrationen bis ins Jahr 2013. Ministerpräsident Jebali war bereits am 19. Februar zurückgetreten. Sein Nachfolger wurde der bisherige Innenminister Ali Larajedh, der ein Jahr später, am 29. Januar 2014, im Rahmen eines nationalen Dialogs Mehdi Jomaâ und dessen Regierung von Technokraten Platz machte. Seit Ende 2014 ist Beji Caid Essebsi erster demokratisch gewählter Präsident eines arabischen Landes; er ernannte am 5. Januar 2015 Habib Essid zum Premierminister.

Am 7. Februar 2014 wurde die neue Verfassung, auf die sich am 27. Januar eine Mehrheit von 200 Abgeordneten (von insgesamt 216) aus fast allen Parteien geeinigt hatte, feierlich verabschiedet. Sie garantiert die Glaubens- und Gewissensfreiheit sowie die Gleichstellung von Mann und Frau und ist zum Zeitpunkt ihrer Verabschiedung „einzigartig in der arabischen Welt“.

Die Machtverteilung zwischen Präsident und Premierminister soll ein autokratisches Regime künftig verhindern. Ein neu zu schaffender "Verfassungsgerichtshof" soll über die Rechtmäßigkeit zukünftiger Gesetzesreformen wachen. Damit soll die Gewaltenteilung in Zukunft geschützt werden.

Einer der größten Streitpunkte war bis zum Schluss die Rolle der Religion im neuen Tunesien. Während die Präambel und Artikel 1 der Verfassung zwar den Islam erwähnen, ohne auf seine Bedeutung für den Staat einzugehen, wird der Text an einigen Stellen konkreter. Der Artikel 6 garantiert Glaubens- und Gewissensfreiheit und sogar – undenkbar in anderen arabischen Ländern – das Recht auf gar keinen Glauben, um jedoch nur einen Halbsatz später festzulegen, dass der Staat „das Heilige“ beschützt. Der Islam ist Staatsreligion, aber die Scharia nicht Rechtsquelle.

Tunesien wurde 2015 auf der Freedom Map der Organisation Freedom House 2015 als erstes arabisches Land mit dem Status „frei“ bewertet. 2017 erhielt es die Bestnote 1 bei der Bewertung der politischen Rechte.
2016 wurde das Tunesische Quartett für seine Bemühungen um die Demokratisierung und den nationalen Dialog nach der Revolution mit dem Friedensnobelpreis ausgezeichnet.

Gleichberechtigung von Frauen und Männern war in der Verfassung ein wichtiges Thema. Frauenförderung ist seit Mitte der 1950er Jahre ein Bestandteil der tunesischen Politik. Bereits 1956, nach der Unabhängigkeit, wurden in Tunesien Frauen weitgehend gleichgestellt, sie durften wählen gehen und die Scheidung einreichen. Lediglich das islamische Erbrecht, in dem Söhnen höhere Anteile als Töchtern zustehen, wurde beibehalten. Die neuen Artikel 20 und 45 stellen Männer und Frauen nicht nur vollkommen gleich und garantieren Chancengleichheit, sondern sprechen sich auch dafür aus, dass eine bestimmte Zahl der Sitze in Stadt- und Landräten an Frauen vergeben werden muss. Der ‚Staatsfeminismus‘ wird von tunesischen Frauenbewegungen kritisiert, da trotz aller staatlichen Bemühungen weiterhin Benachteiligung von Frauen bestehe.

In Tunesien kommt es regelmäßig zu Folter durch staatliche Behörden. Laut einem Bericht von Amnesty International aus dem Jahr 2017 kommt es landesweit zu Folter, willkürlichen Verhaftungen, Hausdurchsuchungen, Razzien und Reiseverboten. Verhaftungen erfolgen demnach wegen auffälligen Aussehens, religiöser Äußerungen oder bereits verbüßter Delikte. Die Organisation bemängelte die Straflosigkeit für Menschenrechtsverletzungen, die die Kultur der Gewalt fördere.

Behörden gingen bis zum Ende der Diktatur 2011 vielfach brutal gegen die eigenen Bürger vor. Im Jahr 2016 wurden Gesetze verabschiedet, die vor Menschenrechtsverletzungen schützen sollen. Laut Amnesty International sind allerdings durch ein 2015 verabschiedetes Anti-Terrorgesetz die Behörden mit weitreichenden Befugnissen ausgestattet und fassen den „Begriff des Terrorismus“ sehr weit.

Tunesien ist in 24 Gouvernements gegliedert, deren geographische Größe ihrer Einwohnerzahl angepasst ist:

Die Gouvernement sind wiederum verwaltungstechnisch in insgesamt 264 Delegationen (ähnlich Landkreisen) untergliedert, die ihrerseits die eigentlichen Gemeinden oder, in größeren Städten, die Stadtteile, enthalten.

Nach dem Korruptionswahrnehmungsindex ("Corruption Perceptions Index") von Transparency International lag Tunesien 2016 von 176 Ländern zusammen mit Bulgarien, Kuwait und die Türkei auf dem 75. Platz, mit 41 von maximal 100 Punkten.

2016 lebten 67,0 % der Bevölkerung in Städten oder städtischen Räumen. Die 5 größten Städte sind (Stand 2017):

Im Rahmen der Euro-Mediterranen Partnerschaft kooperiert Tunesien mit der Europäischen Union.

Das Bruttoinlandsprodukt (BIP) ist seit 20 Jahren stetig gestiegen. Dies war aufgrund der politischen Stabilität und Kontinuität im Land möglich. Tunesien wird deshalb von der Organisation für wirtschaftliche Zusammenarbeit und Entwicklung (OECD) als Schwellenland eingestuft und gilt als wettbewerbsfähigstes Land Afrikas. Das BIP Tunesiens betrug im Jahr 2016 41,8 Mrd. US-Dollar. Das Bruttoinlandsprodukt pro Kopf betrug im selben Jahr ca. 3.730,00 US-Dollar. Die größten ökonomischen Herausforderungen liegen für Tunesien insbesondere in der Bekämpfung der seit Jahren hohen Arbeitslosigkeit sowie der Erhöhung des Investitionsniveaus im privaten und öffentlichen Sektor. Zudem werden Strukturreformen für notwendig erachtet. Die Arbeitslosenquote lag 2016 bei ca. 14 %. Bei jungen Menschen und Akademikern ist die Arbeitslosigkeit jedoch noch deutlich höher und liegt bei über 20 %. Die Tunesische Wirtschaft wuchs in den letzten Jahren nicht schnell genug um all die jungen Menschen die neu auf den Arbeitsmarkt stießen zu absorbieren, was zu einem Gefühl der Perspektivlosigkeit bei vielen jungen Leuten führt.

Charakteristisch für die tunesische Wirtschaft ist die starke Ausrichtung auf Europa (Außenhandel und Tourismus). Das Land kann sich deshalb vom Konjunkturzyklus in der EU nicht abkoppeln.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Tunesien Platz 95 von 137 Ländern (Stand 2017–2018). Im Index der wirtschaftlichen Freiheit belegt das Land 2017 Platz 123 von 180 Ländern.

Die Landwirtschaft beschäftigt 18 % der Arbeitskräfte und erwirtschaftete 2007 einen Anteil von 11,5 % am BIP. Im nördlichen Landesteil werden vor allem Getreide (Weizen, Gerste), Zitrusfrüchte, Datteln und Gemüse angebaut sowie Rinder gehalten. Charakteristisch sind die ausgedehnten Olivenkulturen; Tunesien ist einer der bedeutendsten Exporteure von Olivenöl. Bedeutend ist auch der Weinbau. Im Süden gibt es vereinzelt Oasenwirtschaft und extensive Viehzucht (Schafe, Ziegen).

Die Landwirtschaft verbraucht ca. 80 % des Süßwassers des Landes, die bewässerte Fläche ist von 65.000 Hektar (1956) auf heute 345.000 Hektar angestiegen. Der Sektor ist jedoch relativ unproduktiv und stagniert praktisch seit 1992; die Weltbank empfiehlt eine weitere Deregulierung, was jedoch mit den Armutsbekämpfungszielen der Regierung nicht vereinbar ist. Des Weiteren ist die Landwirtschaft von Desertifikation und Bodenerosion betroffen, jedes Jahr gehen 20.000 Hektar landwirtschaftlicher Nutzfläche verloren. Nachdem die Weltmarktpreise für jene landwirtschaftlichen Produkte, auf deren Import Tunesien angewiesen ist, in den letzten Jahren stark gestiegen sind, hat die Regierung das Erreichen der Autarkie zum Ziel erklärt.

Im Jahr 2006 wurden in Tunesien fast 110.000 Tonnen Fisch verarbeitet, das meiste davon in intensiv bewirtschafteten Küstengewässern. Die Regierung versucht, die Hochseefischerei zu entwickeln; die Kühl- und Hafeninfrastruktur dafür steht mittlerweile zur Verfügung.

Die wichtigsten Bodenschätze sind Phosphate, Erdöl, Gold, Erdgas, Eisenerze, Zink sowie Blei. Für Januar 2006 wurden die Erdölreserven Tunesiens auf 308 Millionen Barrel geschätzt. Im Jahr 2005 wurden täglich 75.000 Barrel Öl gefördert. Tunesien ist somit nur ein sehr kleiner Ölproduzent. In Tunesiens Erdöl- und Erdgasförderung wurde in den vergangenen Jahren viel investiert und die Förderung wird 2009 bei etwa 8,4 Millionen Tonnen Öläquivalent liegen. Das bedeutet eine Steigerung gegenüber 2005 von 50 %. Dies hatte für das Jahr 2007 erstmals seit langem eine ausgeglichene Energiebilanz zur Folge. Neben eigener Förderung bekommt Tunesien kostenlose Gaslieferungen als Zahlung für die Pipeline von Algerien nach Italien, die über tunesisches Gebiet verläuft. Die eigene Förderung von Energieträgern hilft dabei, die Effekte der steigenden Weltmarktpreise für Energie abzumildern. Tunesien hat nur eine einzige Raffinerie, die in Bizerta liegt und von der Société Tunisienne des Industries de Raffinage (STIR) betrieben wird. Sie hat aber nur eine Kapazität von 34.000 Barrel pro Tag (~ 1,7 Millionen Tonnen pro Jahr). Eine weitere Raffinerie in Skhira ist in Bau, sie wird eine Kapazität von sechs Millionen Tonnen pro Jahr haben.

Die Gewinnung von Phosphatmineralen (etwa 60 % Calciumphosphat) im Süden des Landes begann um 1899. Die Lagerstätten entdeckte man im Zeitraum 1885–1886. Durch die "Compagnie des phosphates et du chemin de fer de Gafsa" wurde eine zwischen der Hafenstadt Sfax und dem Abbauzentrum Métlaoui eine 200 Kilometer lange Eisenbahnstrecke errichtet. Bis zum heutigen Zeitpunkt wurde die Eisenbahnerschließung in Nähe der Lagerstätten weiter ausgebaut. Der Phosphatabbau besitzt für Tunesien große nationalökonomische Bedeutung. Um die erforderlichen Arbeitskräfte unterbringen zu können, wurden in der semiariden Region zahlreiche neue Siedlungen errichtet.

Im Jahr 2006 wurden in Tunesien 12,85 Milliarden Kilowattstunden elektrische Energie erzeugt. Davon kamen 12,66 Milliarden aus konventionellen Wärmekraftwerken. Diese werden größtenteils mit Erdgas betrieben. Auf der Agenda steht die Installation von Atomreaktoren mit französischer Unterstützung. Erneuerbare Energien spielen nur eine sehr untergeordnete Rolle, vor allem wird in die Gewinnung von Energie aus Wind investiert. Die staatliche Société Tunisienne de l’Electricité et du Gaz (STEG) hatte bis 1996 ein Monopol auf die Stromerzeugung und -Vermarktung, und sie hat bis heute den größten Marktanteil. Sie gibt an, dass 96 % des Landes Zugang zu elektrischer Energie hat.

Der Industriesektor machte 2005 29 % am BIP aus, und 32 % aller Erwerbstätigen sind hier beschäftigt.

Der wichtigste Industriezweig ist die Textil- und Lederbranche. 40 % aller Industrieunternehmen sind dieser Branche zuzurechnen, sie beschäftigen 43 % der in der Industrie angestellten Arbeitskräfte und erwirtschaften 35 % des Exportwertes. Die Textilindustrie hatte 2007 ein Wachstum von 5,6 %, obwohl nach der Aufhebung des Multifaserabkommens eine Krise befürchtet worden war. Es ist zu erwarten, dass ein Teil der Unternehmen die jetzt stärkere Konkurrenz aus der Türkei, Ägypten und Fernost nicht überleben wird. Für den Fall einer ernsthaften Krise der Textilindustrie werden Schwierigkeiten für die gesamte tunesische Wirtschaft und ernste soziale Konsequenzen befürchtet.

Die chemische Industrie ist vor allem auf die Verarbeitung der tunesischen Phosphatvorkommen ausgerichtet. Sie erhält momentan vom Weltmarkt, wo Phosphatdünger (Superphosphate) eine starke Nachfrage erleben, starke Impulse, weshalb mehrere Anlagen zur Herstellung von Phosphorsäure im Bau sind.

Der Maschinenbau- und Elektrotechniksektor hat in den Jahren an Bedeutung stark gewonnen, vor allem durch die Auslagerung der Produktion von Kfz-Teilen. Im Jahre 2009 beschäftigten die 190 Unternehmen der Automobil-Zulieferindustrie 40.000 Mitarbeiter. Für die Bauindustrie wird ein Boom vorhergesagt, weil einige Großprojekte der Regierung und von arabischen Entwicklungsgesellschaften anstehen. Darüber hinaus ist die Nahrungs- und Genussmittelindustrie bedeutsam; sie konzentriert sich auf die Verarbeitung der landwirtschaftlichen Produkte des Landes.

Tunesien hat mit 1300 Kilometern Küste, zumeist mit Sandstrand, und einem reichen kulturellen Erbe ein großes touristisches Potential. Der Fremdenverkehr hat sich seit Anfang der 1970er Jahre auch zu einem wichtigen Wirtschaftszweig entwickelt und erwirtschaftete 2009 5,8 % des BIP. Hatte Tunesien im Jahr 1971 221 Beherbergungsbetriebe mit 41.000 Betten, so waren es im Jahr 2005 816 Betriebe mit fast 230.000 Betten. Diese Zahlen zeigen deutlich, dass es dabei vor allem um Großhotelanlagen handelt. Viele dieser Clubhotels haben über 400 Zimmer. Im Jahr 2007 besuchten 6,7 Millionen Auslandsgäste Tunesien; die Einnahmen beliefen sich auf 3,05 Milliarden Dinar. Ziele sind Küstenorte wie Hammamet, Nabeul, Sousse und Port El-Kantaoui, Monastir und Mahdia sowie die Insel Djerba zur Erholung; von hier aus werden die Wüste Sahara im Süden erkundet oder archäologische Fundstellen wie Karthago, nahe der im Norden des Landes gelegenen Hauptstadt Tunis, besichtigt.

Etwas mehr als die Hälfte der Touristen stammt aus Mitteleuropa, danach folgen die Nachbarländer Libyen und Algerien, die zusammen etwa 20 % der Übernachtungszahlen ausmachen. Demgegenüber stammen 82 % der Tourismuseinnahmen aus der EU. 2001 besuchten etwa eine Million Touristen aus Deutschland Tunesien, diese Zahl hat sich seitdem um 50 % reduziert. Das Tourismusministerium Tunesiens versucht, in Europa gezielt Werbung zu schalten, um das Land vom billigen Image zu befreien. Der Erfolg ist bisher ausgeblieben, direkte Konkurrenten am Tourismusmarkt wie Ägypten, Marokko oder die Türkei haben höhere Zuwächse an Besuchern und Umsätzen zu verzeichnen.

Infolge der instabilen politischen Situation kam es im Tourismussektor Tunesiens 2011 zu einem starken Einbruch, der Mitte des Jahres vom deutschen Auswärtigen Amt auf 60 Prozent beziffert wurde. „Zudem seien seit Jahresbeginn knapp 3.000 Stellen im 400.000 an Beschäftigten umfassenden Tourismussektor gestrichen worden.“ Die Einnahmen durch Touristen beliefen sich 2011 auf 1.805 Millionen US-Dollar.

Die tunesischen Exporte stiegen im Jahr 2007 um 25 % gegenüber dem Vorjahr, während die Importe um 22 % zulegten. Diese Zahlen sind vor allem auf die leichte Abwertung des Dinar gegenüber dem Euro sowie auf die gestiegenen Preise für Rohöl, das exportiert, und für Ölprodukte, die reimportiert werden, zurückzuführen.

Europa ist für Tunesien der mit großem Abstand wichtigste Handelspartner: Etwa drei Viertel der Einfuhren stammen aus Europa, und für 80 % der Ausfuhren ist Europa der Abnehmer. Frankreich, Italien und Deutschland sind, traditionell in dieser Reihenfolge, die bedeutendsten Handelspartner.

Die wichtigsten Importgüter sind Lebensmittel, Raffinerieprodukte, Maschinen, Fahrzeuge, Telekom- und IT-Ausrüstung sowie Stoffe und Leder. Exportiert werden Textilien und Lederprodukte, Rohöl, Phosphatdünger und Phosphorsäure sowie Einzelteile für Kraftfahrzeuge.

Die Handelsbilanz Tunesiens ist negativ: Das Land importiert mehr, als es exportiert. Das Defizit wird durch den Tourismus und die Zahlungen der Tunesier an die zu Hause gebliebenen Verwandten ausgeglichen, so dass das Leistungsbilanzdefizit 3 % des BIP ausmacht, das durch Direktinvestitionen aus dem Ausland gestopft werden muss. Die Direktinvestitionen betrugen 2007 1180,5 Millionen Euro, die größtenteils in die Infrastruktur und den Textilsektor flossen. Seit den 1990er Jahren hat sich Tunesien für ausländische Direktinvestitionen geöffnet. Derzeit gibt es etwa 3.000 Unternehmen, die mit ausländischem Kapital betrieben werden. Sie beschäftigen über 300.000 Arbeitnehmer. Frankreich, Italien und Deutschland sind die wichtigsten Herkunftsländer.

Seit 2008 sind alle Zölle für Industriegüter zwischen Tunesien und der EU abgeschafft. Das Abkommen von Agadir trat im Juli 2006 in Kraft und soll den freien Handel sowie den Abbau anderer Handelshemmnisse zwischen Tunesien, Ägypten, Marokko und Jordanien ermöglichen. Tunesien hat ein Freihandelsabkommen mit der Türkei unterzeichnet und bemüht sich auch um eines mit den USA. Beim wirtschaftlichen Zusammenschluss mit Libyen, Algerien, Marokko und Mauretanien zur Union des Arabischen Maghreb ist aber noch kein Ergebnis abzusehen.

Die Währungsreserven Tunesiens beliefen sich Februar 2016 auf 6,7 Milliarden US$.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 11,77 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 9,88 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 4,5 % des BIP.
Die Staatsverschuldung betrug 2016 ca, 60,6 % der Wirtschaftsleistung.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:


Tunesien hat relativ niedrige Verteidigungsausgaben, die sich 2006 auf etwa 1,4 % des BIP beliefen. Es unterhält eine Armee, die 2002 aus 27.000 Mann bestand, eine Marine mit 4.500 Mann und einer Luftwaffe mit 3.500 Mann. Daneben gibt es eine paramilitärische Nationalgarde, die 12.000 Mann umfasst. Die regulären Streitkräfte sind unter anderem mit 84 M60-Kampfpanzern, 120 M113-Transportpanzern, 15 F-5-Jagdflugzeugen, 12 Aero L-59 und 11 Aermacchi MB 326-Ausbildungsflugzeugen sowie 40 Patrouillenbooten ausgerüstet.

Es besteht Wehrpflicht für alle Männer über 20. Der Militärdienst dauert ein Jahr.

Tunesien ist Unterzeichner des Atomwaffensperrvertrages, der Chemiewaffenkonvention und der Biowaffenkonvention. Das tunesische Militär beteiligte sich an mehreren UN-Missionen in Äthiopien und Eritrea (UNMEE) und im Kongo (MONUC).

Tunesien verfügt über ein Straßennetz von fast 19.000 km, davon 257 km Autobahnen, fast 12.500 km befestigter Straßen, der Rest unbefestigt. Der Bau der Straßen begann in den 1880er Jahren. Ab den 1920er Jahren begann man, die Küstenstraße (die heutige "Route nationale 1") von Tunis über Sfax und Sousse nach Gabès zu bauen. Es folgte eine Vernetzung der Straßen im Norden des Landes, während das Landesinnere bis in die 1950er bis 1970er Jahre auf seine verkehrsmäßige Erschließung warten musste. Die erste Autobahn wurde 1986 eingeweiht.

Der Personenverkehr wird größtenteils per Straße abgewickelt, entweder per Bus, der vor allem von der staatlichen Société nationale de transport inter-urbain angeboten wird. Daneben sind Sammeltaxis beliebt, die in Tunesien "louage" genannt werden.

Tunesien verfügt über ein Eisenbahnnetz von 2145 Kilometern Länge, das größtenteils aus der Kolonialzeit stammt. Es bedient 200 Bahnhöfe. Der Fernverkehr und die Vorortbahnen von Tunis und Sousse werden von der staatlichen SNCFT betrieben, während die Société des transports de Tunis die TGM und die Stadtbahn Tunis verwaltet.

Es gibt 30 Flughäfen in Tunesien, davon sieben internationale Flughäfen. Die wichtigsten sind Flughafen Tunis, Monastir und Djerba. Ende 2009 ist bei der Stadt Enfidha, Governorat Sousse, ein neuer Flughafen mit einer Kapazität von fünf Millionen Passagieren in Betrieb gegangen. Seine Endkapazität wird bei 22 Millionen Passagieren liegen. Tunesien wird im Linien- wie im Charterverkehr von vielen europäischen Flughäfen aus direkt angeflogen. Neben der 1948 gegründeten nationalen Fluglinie Tunisair gibt es eine weitere, private Fluggesellschaft, nämlich Nouvelair Tunisie.

Tunesien hat Handelshäfen in Bizerta, Gabès, La Goulette, Radès, Sfax, Sousse, Skhira und Zarzis. Sie unterstehen alle der staatlichen Hafenbehörde Office de la Marine Marchande et des Ports, gelten aber als nicht besonders effizient. Im 100 km südlich von Tunis gelegenen Enfidha soll deshalb ein Tiefseehafen gebaut werden, der 1,4 Milliarden Euro kosten wird und 5 Millionen TEUs pro Jahr umschlagen kann.

Logistik und Informationstechnologie sind die momentan am schnellsten wachsenden Wirtschaftsbereiche Tunesiens, das Wachstum betrug 2007 14 %. Dieser Sektor hat auch in der langfristigen wirtschaftlichen Strategie der Regierung einen hohen Stellenwert. In den Ausbau der Telekommunikationsinfrastruktur sollen in den nächsten Jahren fast 4 Milliarden Euro investiert werden und es wird erwartet, dass innerhalb der nächsten fünf Jahre der Anteil dieses Sektors am BIP auf 27,5 % steigt. Bereits jetzt hat Tunesien einen sehr hohen Rang im Network Readiness Index; es liegt vor einigen EU-Staaten und nimmt unter den arabischen Staaten den zweiten Platz ein.

Für März 2016 wurde die Zahl der tunesischen Internetnutzer mit 5,47 Millionen angegeben oder 48 % der Bevölkerung. 2008 es gab 204.000 Internetanschlüsse, davon 106.000 ADSL-Anschlüsse. Tunesien verfügt mittlerweile über eine starke und miteinander gut vernetzte Bloggerszene, welche die Jasminrevolution maßgeblich mitorganisiert hat.

Da Tunesien über die Jahrhunderte mehrere Einwanderungswellen aus Arabien, Spanien, Frankreich, der Türkei und den westafrikanischen Berber-Reichen erlebte, unterscheiden sich die Tunesier in ihrem Aussehen und im Kulturleben von anderen arabischen Nationen. Dies zeigt sich im Stadtbild von Tunis (zum Beispiel auf dem Place de Barcelone oder im maurisch-andalusischen Viertel "Sidi Bou Saïd"), in der Töpferei- und Keramikkunst (zum Beispiel in Nabeul), an zahlreichen Bauten verschiedener Epochen (zum Beispiel dem Fort am Golf von Hammamet) und in der tunesischen Küche (zum Beispiel Baguette, Käse, Croissant, „Makkarona“ sowie einigen Berbergerichten wie zum Beispiel Brik).

Die Architektur Tunesiens hat viele externe Einflüsse aufgenommen. Hierbei haben sich europäische und nordafrikanische Stilrichtungen mit Bautraditionen aus dem arabisch beeinflussten Mittelmeerraum gemischt.

Frühe Spuren architektonischer Hinterlassenschaften wurden bei Ausgrabungen in paläolithischen Siedlungen gefunden. Die ältesten städtischen Strukturen hinterließen die Phönizier mit ihren ersten Handelsniederlassungen seit dem 12. Jahrhundert v. Chr. im Land. Davon sind nur wenige Zeugnisse erhalten geblieben. Zu ihren Gründungen zählt die Stadt Karthago.

Die römische Epoche in Tunesien hat die alte Architektur des Landes stark beeinflusst. Sie ist in Form von archäologisch gesicherten Stadtanlagen und Einzelbauten überliefert. Dazu zählen die Ruinen von Sbeitla.

Eine noch heute in besonderer Weise gepflegte und vielfach museal präsentierte Ausdrucksform des römischen Architekturerbes sind die zahlreich erhaltenen Mosaiken. Diese hatte man zum Zwecke der Verzierung von Fußböden und Wänden geschaffen.

Die nachfolgende byzantinische Epoche hinterließ einige Befestigungsanlagen (beispielsweise Gafsa, Sbeitla und Tebessa) und Kirchenbauten, wie die ehemalige Basilika von Sbiba.

Mit den im 7. Jahrhundert beginnenden arabischen Machtstrukturen wandelten sich architektonische und künstlerische Ausdrucksformen. Es mischen sich Gestaltungselemente aus dem Kulturkreis der Berber im Maghreb, aus den ursprünglich römischen und byzantinischen Traditionen und hereinbrechenden orientalischen Einflüssen. Die wichtigsten Bauten in dieser Epoche sind Palaststädte (Qasr al-Qadīm) an den älteren Lagersiedlungen und weitere Befestigungsanlagen in Sfax und Sousse. Eine besondere Form stellen die Ribate dar, wobei ein Befestigungsturm auch als Minarett dienen konnte. Aus dieser Epoche stammen die ersten Moscheen in Tunesien, die in der frühen Phase noch wehrhafte konzipierte Bauten sind. Zu den bedeutendsten dieser Bauwerke zählen die Hauptmoschee in Kairouan und die Ez-Zitouna-Moschee von Tunis. Sie wurden im 7. bzw. 8. Jahrhundert begonnen. Als deren bauliches Vorbild gilt die Umayyaden-Moschee in Damaskus. Die Hauptmoschee von Kairouan lieferte wiederum das Vorbild für weitere Bauten in Spanien und im nordafrikanischen Raum. An mittelalterlichen Bauten wurden Spolien verbaut, wenn sie aus zugänglichen Ruinen unter leichten Umständen geborgen werden konnten. Typisch sind jedoch Fassaden mit zweifarbigen Backsteinen. Im 10. Jahrhundert entwickelten sich unter dem Einfluss der Fatimiden und später der Ziriden immer mehr repräsentative Residenzbauten. Im 12. Jahrhundert regierten im Gebiet des heutigen Tunesiens die Almohaden, die hier Einflüsse des marokkanischen Kulturkreises einbrachten.

Der umfangreichste Beitrag in der historischen Architektur von Tunesien stammt aus der Epoche vom 13. bis 15. Jahrhundert. Die berberisch geprägten Hafsiden übernahmen Formen und Schmuck ihrer Architektur aus den von Tunesien aus westlich gelegenen Regionen Afrikas und von der Iberischen Halbinsel. Hier hatte sich bereits eine eigenständige Richtung im Bauen entwickelt, die sich aus der Kombination marokkanischer und andalusischer Einflüsse zu einer spezifischen Architektur entwickelte, die später als allgemein Maurischer Stil bezeichnet wird. In dieser Epoche entstanden an der Ez-Zitouna-Moschee von Tunis eine islamische Hochschule und an weiteren Orten Medresen. Im Jahr 1420 eröffnet hier das "Maristān", das älteste islamische Krankenhaus. Es erfolgte der Ausbau von Wasserversorgungsanlagen, teilweise unter Nutzung älterer römischer Einrichtungen. Unter Abd al-Aziz II. entsteht der Hafsiden-Palast Bardo in Tunis, eine frühe Gartenanlage, die um 1500 durch neue Gebäude eine Erweiterung erfuhr und danach sogar über eine Bibliothek verfügte.

Im 16. Jahrhundert erlitt Tunesien einen allgemeinen Niedergang. Konflikte mit Spanien behinderten weitere Entwicklungen. Spanische Truppen versuchten die Seeräuberherrschaft auf Inseln (Djerba 1511) und in Hafenstädten zu bekämpfen. Unter ihrer kurzen Herrschaft entstanden einige Festungsbauten, wie jene auf Djerba. Im Jahr 1570 mussten sie Tunis wieder aufgeben und verloren somit ihren Einfluss auf die Region.

Erst als im 17. Jahrhundert eine große Zahl von Auswanderern aus Spanien sich in Nordafrika ansiedelte, erhielt die tunesische Architektur wieder neue Impulse. Die moslemischen und jüdischen Emigranten aus Andalusien brachten ihre Erfahrungen, ästhetischen Auffassungen und handwerklichen Fertigkeiten mit. Zusammen mit den parallel wirkenden türkischen Einflüssen bildete sich in Tunis ein Mischstil heraus. In der Baukultur bedeutete das am Beispiel der Moscheen, dass ihr Minarett kaum noch mit quadratischen, sondern nun mit achteckigem Grundriss errichtet wurde. In diesem Zuge erhielten sie zunehmend eine Galerie und ein Spitzdach. Die Innenbereiche der Paläste wurden durch die neuen Architekturentwicklungen zunehmend mit einer üppigen Dekoration im Maurischen Stil ausgestattet. Später kamen italienische Stilelemente hinzu. Diese Prinzipien zogen sich bis in das 19. Jahrhundert hinein, wobei verstärkt Bauten des zeitgenössischen europäischen Stils im Stadtbild von Tunis und einigen anderen Städten auftraten.

Die traditionell strukturierten alten Stadtkerne mit einer ummauerten Medina, den engen Gassen und den überwiegend zweigeschossigen Wohngebäuden wurden von diesen Stadtentwicklungen weitgehend unberücksichtigt gelassen und blieben bis in die heutige Zeit erhalten.

Durch die französische Kolonialmacht wirkten neue Strömungen und Künstler in den tunesischen Kulturraum hinein. In Karthago ließ Le Corbusier mehrere Villen errichten. Die Viertel der Europäer unterschieden sich durch ihre Architektur markant von jenen der einheimischen Bevölkerung. Auch in diesem Zeitabschnitt mischten sich wieder externe Architekturansätze mit den Erfahrungen regional verwurzelter Baumeister. Diese bauliche Entwicklung spiegelt sich in zahlreichen modernen Bauten sowohl von privaten als auch von öffentlichen Auftraggebern wider. Aus dieser Zeit stammen einige Suqs mit Arkaden und Kuppeldächern. Ein herausragender Einzelbau ist beispielsweise das Zollgebäude von Bizerta. Mit dem beginnenden 20. Jahrhundert dominieren europäische Bauformen in den größeren Städten. Sie sind durch Stahlbetonanwendungen und die typische Kubatur von Stadtvillen gekennzeichnet. Die ländlichen Siedlungen sind weiterhin von traditionellen Bauweisen, wie subterrane Röhrenbauten und befestigte Getreidespeicher (beispielsweise in Tataouine), bestimmt.

In der jüngsten Periode des Landes, nach der Unabhängigkeit im Jahr 1956, erfolgten Stadterweiterungen und die Errichtung zahlreicher öffentlicher Bauten. Eine besondere Rolle spielte dabei die Rekonstruktionsarbeiten in der Innenstadt von Tunis, die nach einem Wettbewerbsergebnis von bulgarischen Architekten durchgeführt wurde.

Bauten aus der jüngsten Architekturepoche Tunesiens binden Anspielungen auf einheimische Ornamente in moderne Formen des Bauens mit Beton und Naturstein ein. Die Nationalbibliothek in Tunis, ein moderner Zweckbau, ist ein Beispiel für diese Verknüpfung.

In Tunesien spielt sich das Literaturleben in zwei Sprachen ab: in arabisch und französisch. Die arabische Literatur existiert seit dem 7. Jahrhundert, als die arabische Zivilisation sich auf das Gebiet Tunesiens ausbreitete; französischsprachige Literatur gibt es erst seit 1881. Heute hat die arabischsprachige Literatur ein höheres Gewicht als die französischsprachige: Von den 1249 literarischen Neuerscheinungen des Jahres 2002 waren 885 auf arabisch; mehr als ein Drittel der Neuerscheinungen waren Kinderbücher. Alles in allem ist das literarische Schaffen in Tunesien also, trotz seiner langen Geschichte, heute sehr überschaubar. Wichtige tunesische Autoren sind Abu al-Qasim asch-Schabbi, Moncef Ghachem und Mahmoud Messadi, weitere finden sich in der Liste tunesischer Schriftsteller.

Die Musik Tunesiens ist das Resultat der kulturellen Vermischung aus arabisch-andalusischer Musik, die Flüchtlinge nach der spanischen Eroberung Andalusiens im 15. Jahrhundert mitbrachten, arabischer und westlicher Musik. Sie hat viele Facetten; die berühmteste klassische Musikrichtung ist der Malouf. Er wird von kleinen Orchestern gespielt, bestehend aus Violine, Kanun, Oud, Violoncello, Kontrabass, Nay, Darbouka und Nagharats (einem Paar kleiner Bechertrommeln). Klassische Gesänge haben bis heute Erfolg beim Publikum. Abgesehen von der Instrumentierung unterscheiden sich städtische und ländliche Musik kaum. Im städtischen Umfeld dominieren Saiteninstrumente wie das Rebec, der Oud und das Kanun sowie Darbouka. Im ländlichen Milieu und den Gesängen der Beduinen dominieren neben der Perkussion Blasinstrumente wie der Mezwed und die Gasba.

Unter den bedeutendsten Sängern des Landes sind Saliha, Khemaïs Tarnane, Ali Riahi, Hédi Jouini, Latifa Arfaoui, Mohamed Jamoussi, Cheikh El Afrit und Dhikra Mohamed zu nennen. Unter den Instrumentalisten sind der Oud-Spieler Anouar Brahem, Lotfi Bouchnak, Salah El Mahdi, Ridha Kalaï, Ali Sriti und Youssef Slama die wichtigsten. El Azifet ist ein reines Frauen-Orchester, eine Seltenheit im arabischen Raum. Baron Erlanger ist eine wichtige Figur der modernen tunesischen Musik. Er sammelte die Regeln und Geschichte des Malouf, welches sechs Bände füllte, und gründete eine Rachidija, ein wichtiges Konservatorium, das heute noch genutzt wird.

Die Bevölkerung Tunesiens wird heute auch von ausländischer Musik angezogen, wobei hier vor allem die ägyptische Musik, libanesische und syrische Musik einflussreich sind. Westliche Musik kommt in Form von Rockmusik, Hip-Hop, Reggae und Jazz in das Land.

Die tunesische Küche spiegelt die berberischen, arabischen, jüdischen, türkischen, französischen und italienischen Einflüsse wider, denen das Land im Laufe seiner Geschichte ausgesetzt war. Die Ernährung beruht auf Getreide, vor allem Weizen in Form von Brot, Nudeln oder Grieß, Oliven und Olivenöl, verschiedenen einheimischen Gemüsesorten (Tomaten, Kartoffeln, Kichererbsen, Bohnen oder Karotten), Hammel- und Rindfleisch sowie Fisch und Meeresfrüchten.

Die tunesische Küche unterscheidet sich von jener der maghrebinischen Nachbarn durch die häufige Verwendung von Tomaten und Paprika (daher die Bezeichnung "rote Küche") und ihre Schärfe, die sie "Harissa" verdankt. Daneben haben, anders als in anderen arabischen Ländern, Käse und Pasta in die tunesische Esskultur Einzug gehalten. Typische Gerichte sind Couscous oder die Tunesische Tajine, das Kichererbsengericht Lablabi, Merguez-Würste, Schakschuka oder die Süßspeise Baklava.

Die Tunesier haben auch traditionell eine relativ liberale Einstellung zum Alkohol. Es gibt daher den Feigenschnaps Boukha oder den Dattellikör Laghmi. Auch wird in Tunesien Bier ("Celtia") gebraut und Wein gekeltert.

Tunesien hat ein reiches handwerkliches Erbe mit vielen regionalen Spezialitäten. Das Kunsthandwerk ist auch ein bedeutender Wirtschaftszweig, in dem geschätzte 300.000 Personen tätig sind. Die Töpferei ist besonders um Guellala verbreitet, während Nabeul berühmt für die Herstellung von Fayence ist. Die Mosaikkunst hat sich seit dem 2. Jahrhundert im Land verbreitet, die weltweit bedeutendste Sammlung von Mosaiken befindet sich im Nationalmuseum von Bardo. Das Schmieden kam mit den Flüchtlingen aus Andalusien nach Tunesien, heute sind besonders die blauen Fenstergitter, die an Maschrabiyya erinnern, berühmt. Die Teppichknüpferei wurde durch die Karthager in Tunesien eingeführt, in der ersten Hälfte des 19. Jahrhunderts kamen noch einmal starke Impulse aus dem osmanischen Reich. Heute ist das Zentrum der Teppichherstellung in und um Kairouan angesiedelt. Im Jahr 2004 wurden 200.000 m² Woll- und 16.500 m² Seidenteppiche hergestellt. Die Tendenz ist, aufgrund sinkender Preise, fallend. Ursprünglich hatten die tunesischen Teppiche weniger als 40.000 Knoten pro Quadratmeter; heute kann er eine Feinheit von bis zu 250.000 Knoten haben. Die traditionelle Tracht des Landes heißt Jebba, an den Füßen trägt man Babuschen, die für Männer aus Leder, für Frauen aus Seide oder Baumwolle mit eingewebten Silber- oder Goldfäden und meist mit Blumenmotiven versehen sind. Berühmt ist auch der Schmuck, besonders der Silberschmuck der Berber im Süden des Landes, in den häufig Münzen eingearbeitet werden.

Daneben gibt es mehrere islamische Feiertage, deren Datum sich nach dem Mondkalender richtet und daher von Jahr zu Jahr schwankt. Dazu zählen der Geburtstag Mohammeds, das Opferfest, das Fest des Fastenbrechens am Ende des Ramadan sowie das islamische Neujahr.

Der wichtigste und meistbetriebene Sport in Tunesien ist der Fußball, sowohl was die Zahl der ihn Ausübenden, als auch was die Berichterstattung betrifft. Danach folgen Taekwondo, Handball, Volleyball, Judo, Karate, Leichtathletik und Tennis. Alle anderen Sportarten, wie etwa der Radsport, sind wenig verbreitet, was an fehlender Infrastruktur, Ausrüstung und geringem Medieninteresse liegt.

Die tunesische Fußballnationalmannschaft kann bisher vier Weltmeisterschaftsteilnahmen (1978, 1998, 2002 und 2006) vorweisen, wobei sie jeweils jedoch in der Vorrunde ausschied. An der Fußball-Afrikameisterschaft nahm Tunesien 13 Mal teil und gewann den Titel 2004. 1963 konnte der Arab Cup im Libanon gewonnen werden. Bedeutende Spieler sind Zoubaier Baya, Hatem Trabelsi und Yassine Chikhaoui.

Der Klub "Espérance Sportive de Tunis" ist der erfolgreichste tunesische Verein, sei es auf nationaler, oder internationaler Ebene mit 27 Meisterschaftstiteln und 15 Pokalerfolgen, sowie 2 CAF Champions League -Triumphen (1994 & 2011). Den ersten afrikanischen Titel als tunesisches Team, verbuchte der Verein Club Athlétique Bizertin 1988 mit dem Gewinn des Afrika Cup der Pokalsieger (Coupe d'Afrique des clubs vainqueurs de coupes). Der Verein "Club Africain" war hingegen der erste tunesische Vertreter, der 1991 die CAF Champions League gewinnen konnte. "Étoile Sportive du Sahel" gelang dieser Erfolg als erstem tunesischen Vertreter nach der Neuorganisation dieses Wettbewerbs, am 9. November 2007, daneben hat noch der "Club Sportif Sfaxien" in regionalen und kontinentalen Wettbewerben zahlreiche Erfolge verzeichnen können. Das wichtigste Fußballereignis ist das Hauptstadtderby zwischen "Club Africain" und "Espérance Sportive de Tunis. E"s findet zweimal jährlich statt und zieht jedes Mal mehr als 60 000 Zuschauer an.

Die wichtigsten Meisterschaften, die in Tunesien ausgespielt werden, sind die tunesische Fußball-, die Handball-, die Volleyball- und Basketballmeisterschaften. Cups werden im Fußball, Handball, Volleyball und Basketball ausgetragen. Daneben gibt es eine Tunesische Radsportmeisterschaft und, unregelmäßig, die Tour de Tunisie. Auch internationale Meisterschaften wurden in Tunesien bereits ausgetragen, etwa die erste Ausgabe der Junioren-Fußballweltmeisterschaft im Jahr 1977. Die Fußball-Afrikameisterschaft war in Tunesien 1965, 1994 und 2004 zu Gast. Darüber hinaus wurde die Handball-Weltmeisterschaft der Herren 2005 in Tunesien ausgetragen.

Im Mai 2007 waren in Tunesien 1673 Sportvereine registriert, davon 250 Fußball-, 206 Taekwondo-, 166 Karate-, 140 Behindertensport-, 85 Handball-, 80 Leichtathletik-, 66 Judo-, 60 Kung Fu-, 59 Kickboxing-, 48 Basketball-, 47 Pétanque-, 45 Tischtennis-, 40 Volleyball-, 37 Box-, 31 Schwimm- und 30 Tennisklubs.

Der bedeutendste Sportler des Landes ist der Leichtathlet Mohamed Gammoudi, der vier olympische Medaillen gewinnen konnte. Weltmeister, die aus Tunesien kommen, sind Anis Lounifi (Judo) und Oussama Mellouli (Schwimmen).

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Tunesien Platz 97 von 180 Ländern.

Es gibt in Tunesien zwei öffentliche Fernsehkanäle namens Télévision Tunisienne 1 und Télévision Tunisienne 2. Privates Fernsehen gibt es erst seit Februar 2005, als der Betrieb von Hannibal TV begann. Seit 2007 sendet des Weiteren Nessma TV. Die Regierung betreibt vier nationale Radiostationen, nämlich Radio Tunis, Radio Tunisie Culture, Radio Jeunes und RTCI sowie fünf lokale Stationen (Gafsa, El Kef, Monastir, Sfax, Tataouine). Seit November 2003 gibt es Privatradio, momentan existieren drei Stationen, nämlich Mosaïque FM in Tunis, Jawhara FM in Sousse und Zitouna FM. Zitouna FM ist größtenteils religiösen Inhalten gewidmet. Die Programme aller dieser Sender werden größtenteils auf arabisch gesendet, ein kleinerer Teil ist auf Französisch. Hinzu kommt der regierungskritische, private Sender ohne Sendelizenz Radio Kalima, dessen Programm über den Satelliten Hot Bird und als Livestream übers Internet ausgestrahlt wird.

Im Jahr 2007 wurden in Tunesien 245 Tageszeitungen und Zeitschriften gezählt, wovon 90 % von privaten Organisationen herausgegeben werden. Einige Zeitungen sind französischsprachig, darunter Le Temps Tunisie.

Die Meinungs- und Pressefreiheit wird von der Verfassung garantiert; in der Praxis jedoch übernahmen bis zur Revolution in Tunesien 2010/2011 die Medien die Regierungslinie, die über die staatliche Nachrichtenagentur TAP verbreitet wurde, und berichteten kritiklos über die Arbeit des Staatspräsidenten, der Regierung, der regierenden Partei RCD. In Tunesien herrschte bis dahin Zensur, und die Regierung beeinflusste auch über die Vergabe von Förderungsgeldern die Berichterstattung der Medien.

Für das Gesundheitssystem wurden im Jahr 2008 2 % des BIP bzw. 8 % der öffentlichen Ausgaben aufgewandt. Es ist relativ gut ausgebaut mit 968 Menschen pro Arzt, mehr als 90 % der Bevölkerung sind sozialversichert, und die Lebenserwartung liegt bei 76 Jahren (Männer: 74 Jahre, Frauen: 78 Jahre), und konnte in den letzten Jahrzehnten enorm gesteigert werden. Dank mehrerer Familienplanungs-Programme der Regierung liegt das Bevölkerungswachstum bei nur 1 %. Die HIV-Prävalenz war 2006 0,11 % der Bevölkerung. Die Kindersterblichkeit liegt bei 12 pro 1.000 Geburten und die Müttersterblichkeit bei 62 pro 100.000 Geburten (Stand 2017).

Quelle: UN

Wer nach Tunesien reist, sollte gegen Tetanus, Diphtherie, Polio, Hepatitis A und Hepatitis B geimpft sein. Bilharziose-Erreger kommen in vielen Gewässern Tunesiens vor.





</doc>
<doc id="5224" url="https://de.wikipedia.org/wiki?curid=5224" title="Tunis">
Tunis

Tunis (, ) ist die Hauptstadt Tunesiens und Provinzhauptstadt des gleichnamigen Gouvernements. Sie ist die größte Stadt Tunesiens und hatte im Jahr 2014 laut Zensus 1.056.247 Einwohner. In der Agglomeration wohnen etwa zwei Millionen Einwohner (Juni 2014). Ihr historischer Kern die Médina steht auf der UNESCO-Welterbe-Liste.

Nachdem die Stadt ein bescheidenes Dorf im Schatten von Karthago, Kairouan und danach Mahdia war, wurde sie am 20. September 1159 (5. Ramadan 554 des muslimischen Kalenders) unter der Regierung der Almohaden als Hauptstadt genannt. Ihr Status wurde danach im Jahr 1228 von den Hafsiden bestätigt und auch nach der Unabhängigkeit des Landen von Frankreich am 20. März 1956.

Tunis ist ebenfalls die ökonomische und kommerzielle Hauptstadt Tunesiens. Ihr dichtes Straßen- und Autobahnnetz und ihre relativ fortgeschrittene Fluganbindung machen sie zu einem Konvergenzpunkt des Nationalverkehrs. Diese Situation ist ein Ergebnis einer langen zentralistischen Politik, die dazu beigetragen hat, dass sich alle wichtigen Institutionen in Tunis befinden.

Im Laufe des 20. Jahrhunderts hat sich die Agglomeration außerhalb der Grenzen der Gemeinde erweitert. Zurzeit erstreckt sich Tunis über 4 Gouvernorats, Tunis, l'Ariana, Ben Arous und La Manouba, und wird Le Grand Tunis (das große Tunis) genannt.

Im Jahr 2017 stand Tunis auf dem 113. Platz der Weltrangliste der lebenswertesten Städte und auf dem 1. Platz in Nordafrika.

Nach Paul Sebag ist « Tunis » die französische Transkription eines Namen, der "tûnus", "tûnas" oder "tûnis" ("û" hat den Wert eines « u » im Deutschen) in den gängigsten tunesischen Dialekten ausgesprochen wird. Die drei Vokalvarianten (u, a und i) wurden schon von dem syrischen Geographen Yāqūt ar-Rūmī in seinem Werk "Mu'jam al-Bûldan" (Wörterbuch der Länder) erwähnt; der Dritte ist der dominanteste, auch bei der Einwohnerbezeichnung "tûnisi" ou "tûnusi" (Tuniser). Diese Stämme ("tûnus", "tûnas" und "tûnis") leiten sich aus dem berberischen Verb "ens ab", der « bei jemandem übernachten » heißt.

Unter Berücksichtigung aller vokalischen Variationen hat der Name von Tunis wahrscheinlich die Bedeutung von « Biwak » oder « Nachtcamp ». Aus dem Namen von Tunis wurde auch der französische Begriff « Tunisie » (Tunesien) abgeleitet. Dieser Name wurde von französischen Historikern und Geographen eingeleitet und hat sich in fast allen europäischen Sprachen etabliert. Der Begriff "Tunes" (auf Tunesisch wie auch auf Arabisch) bezeichnet allerdings sowohl die Stadt als auch das Land. Ob über Tunis oder Tunesien gesprochen wird, ergibt sich aus dem Kontext.

Andere Erklärungen über die Herkunft des Namen von Tunis gibt es noch. Der Name soll aus dem berberischen Begriff "Tinast" kommen, der « Schlüssel der Fruchtbarkeit » bedeutet, in Bezug auf die Fruchtbarkeit des Bodens in diesem Gebiet.

Tunis liegt im Norden des Landes unweit des Mittelmeers. Zwischen der Stadt und dem Golf von Tunis liegt der See von Tunis, eine flache Lagune. Uhrzeit: GMT+1h.

Die Innenstadt von Tunis mit der historischen Altstadt (Médina) und der während der Kolonialzeit angelegten Neustadt liegt zwischen dem See von Tunis und dem See Sebkhet Sedjoumi (). 

Um die Innenstadt herum liegen die inneren Vororte: im Westen das Villen- und Regierungsviertel Le Bardo mit dem gleichnamigen Museum, im Norden das Viertel um die Belvédère-Hügel und die neueren Siedlungen El Menzah und Ariana, im Süden die Industrieviertel Megrine und Ben Arous. 

Während der Kolonialzeit bauten die Franzosen einen 10 km langen Schnellstraßen- und Stadtbahndamm quer durch den See von Tunis, der als Fortsetzung der Avenue Habib Bourguiba die Innenstadt von Tunis mit der Hafenstadt La Goulette (Halk al-Wadi) verbindet. Nördlich von La Goulette reihen sich die wohlhabenden Vororte Qartāj (Carthage, Karthago) mit dem internationalen Flughafen Tunis, Sidi Bou Said, La Marsa und Gammarth an die Küste, südöstlich liegt der Badeort Hammam-Lif.

Tunis liegt in der subtropischen Klimazone und innerhalb des Bereichs winterfeucht-sommertrockenen Mittelmeerklimas im Norden Tunesiens. Die Jahresmitteltemperatur liegt bei 17,7 °C, die jährliche Niederschlagssumme beträgt 465 mm. Der überwiegende Teil dieser Niederschläge fällt im Winterhalbjahr, im Sommer herrscht ein arides Klima. Die heißesten und trockensten Monate sind Juli und August mit Monatsdurchschnittstemperaturen um 26 °C. Im Januar und Februar liegt das Monatsmittel knapp über 10 °C.

Hauptprodukte der Industrie in und um Tunis sind Textilien, Teppiche und Olivenöl. Der Tourismus ist eine weitere wichtige Quelle für das Einkommen der Stadt. Zudem hat in Tunis auch die erste tunesische Automobilmarke Wallyscar ihren Sitz.

Tunis ist der wichtigste Knotenbahnhof der tunesischen Staatsbahn Société nationale des chemins de fer tunisiens und wird sowohl im Fern-, Nah-, als auch im Vorortverkehr bedient. Die Société des transports de Tunis betreibt die Stadtbahn Tunis sowie das 5.836 Kilometer lange städtische Busnetz.
Tunis hat unweit des Zentrums mit dem Aéroport International de Tunis-Carthage auch einen internationalen Flughafen sowie umfangreiche Hafenanlagen mit Fährverbindungen nach Frankreich (Marseille, Toulon) und Italien (u. a. nach Genua, Palermo, Salerno).

Der Name der Stadt geht auf die Göttin Tanit zurück, die in punischer Zeit als Schutzgöttin von Karthago verehrt wurde.

Tunis ist eine der ältesten Städte am Mittelmeer. Die numidische Stadt "Tunes" existierte bereits vor dem Eintreffen der ersten phönizischen Kolonisten im 9. Jahrhundert v. Chr. Jedoch stand Tunis in der Antike stets im Schatten des mächtigen Karthagos.

Erst nach der arabischen Eroberung und der Zerstörung Karthagos Ende des 7. Jahrhunderts gelangte Tunis zu überregionaler Bedeutung. Unter der Herrschaft der Aghlabiden diente Tunis im 9. Jahrhundert kurzfristig als Residenz. Zu dieser Zeit entstand die Médina mit der Ez-Zitouna-Moschee.

Tunis wurde im Jahr 1159 unter der Dynastie der Almohaden die Hauptstadt Ifrīqiyas und war ein führendes Handelszentrum mit Europa. Im Jahr 1270 scheiterte ein Eroberungsversuch des französischen Königs Ludwig IX. während des Siebten Kreuzzugs.

Erstmals gelangte Tunis im Jahr 1534 unter osmanische Herrschaft. Nur ein Jahr später wurde es jedoch von Karl V. im Tunisfeldzug erobert und unterstand danach spanischem Protektorat, bis Tunis im Jahr 1574 endgültig in die Hand der Osmanen fiel. Nach 1591 waren die osmanischen Gouverneure (Beys) relativ unabhängig und die Stadt wuchs als ein Zentrum von Piraten und Handel. Ab 1609 siedelten sich zahlreiche muslimische Flüchtlinge aus Andalusien, darunter auch viele Juden, an und trugen zum wirtschaftlichen und kulturellen Aufschwung von Tunis bei.

Im April 1655 war der englische Admiral Robert Blake beauftragt worden, von den Staaten des Mittelmeers, die englische Schiffe angegriffen hatten, eine Entschädigung einzufordern. Nur der Bey von Tunis widersetzte sich, mit dem Resultat, dass Blakes Schiffe das Arsenal des Beys bei Porto Farina (Ghar el Melh) angriffen, wobei sie neun algerische Schiffe und zwei Küstenbatterien zerstörten. Es war das erste Mal in der Seekriegsführung, dass Küstenbatterien außer Gefecht gesetzt wurden, ohne Landungstruppen einzusetzen.

Nachdem ihnen von Otto von Bismarck während des Berliner Kongresses im Jahr 1878 die Oberhoheit über Tunis zugesichert worden war, machten die Franzosen nach der Annektierung des Landes im Jahr 1881 die Stadt zum Sitz ihrer Protektoratsverwaltung und nahmen zahlreiche städtebauliche Veränderungen vor. Zwischen der Altstadt und dem Meer entstand eine Neustadt im europäischen Stil und der Stadthafen wurde über einen Schifffahrtskanal durch den See von Tunis mit dem neu angelegten Hafen von La Goulette verbunden. Während des Zweiten Weltkriegs wurde Tunis von den Achsenmächten von November 1942 bis Mai 1943 gehalten und war deren letzte Basis in Afrika (siehe Afrikakorps/Erwin Rommel).

Nach der Unabhängigkeit Tunesiens im Jahr 1956 wanderten die meisten Europäer (hauptsächlich Franzosen und Italiener) aus, die zuvor noch fast ein Viertel der Einwohnerschaft ausgemacht hatten. Zugleich führte die Landflucht zu einem großen Bevölkerungszuwachs und dem Bau von zahlreichen Neubaugebieten. Zwischen 1979 und 1990 hatte die Arabische Liga ihr Hauptquartier in Tunis, ebenso wie die PLO von 1982 bis 1993. Im Jahr 1985 wurde das Hauptquartier der PLO im südlichen Strandbad Hammam Plage (auch: Hammam Schatt) durch die israelische Luftwaffe bombardiert, wobei über 60 Menschen ums Leben kamen (→ "Operation Wooden Leg").

Im Jahr 2015 kam es zu mehreren schweren Attentaten in Tunis, darunter im März den Angriff auf das Nationalmuseum und im November den Anschlag auf die Präsidentengarde.

Das Stadtbild von Tunis ist geprägt vom starken Kontrast zwischen der orientalischen Altstadt und der europäisch anmutenden Neustadt. Die Altstadt (Medina) von Tunis wurde im 9. Jahrhundert von den Aghlabiden angelegt und im 13. Jahrhundert von den Hafsiden umgestaltet. Sie gehört seit 1979 zum UNESCO-Welterbe. Die heute von etwa 20.000 Menschen bewohnte Medina hat eine Ausdehnung von 1500 m x 800 m und zeigt den üblichen Aufbau arabischer Altstädte mit einem unregelmäßigen Netz aus verwinkelten Gassen. Im Zentrum der Medina steht die Ez-Zitouna-Moschee, nach der Großen Moschee von Kairouan die wichtigste Moschee Tunesiens. Die Ez-Zitouna-Moschee geht im Kern auf das 9. Jahrhundert zurück, wurde jedoch mehrfach umgebaut und erweitert. Rings um die Moschee erstreckt sich das Marktviertel. Traditionell ist jeder der Souks (Marktgassen) einem bestimmten Wirtschaftszweig zugeordnet, z. B. den Parfümhändlern (Souk el Attarine), Schuhhändlern (Souk el Blaghija) oder Stoffhändlern (Souk des Étoffes). Der zentrale Bereich der Souks ist heute stark auf den Tourismus eingestellt, die Marktgassen in den Randbereichen der Medina werden jedoch vornehmlich von den einheimischen Bewohnern frequentiert.

Der "Platz des Sieges" "(Place de la Victoire)" mit dem ehemaligen Stadttor ("Porte de France", arab. "Bab el Bhar" ‚Hafentor‘) liegt an der Grenze zwischen Altstadt (Medina) und Neustadt (Ville Nouvelle). Die Neustadt liegt zwischen der Medina und dem See von Tunis und wurde im 19. Jahrhundert von den Franzosen angelegt. Die Straßen bilden ein regelmäßiges Schachbrettmuster und auch die Architektur der Gebäude mutet europäisch an. Hauptachse der Neustadt ist die über 1,5 km lange Prachtstraße Avenue Habib Bourguiba, die von Geschäften, Cafés und Hotels gesäumt wird.

Etwa vier Kilometer westlich der Innenstadt liegt der ehemalige Villenvorort Le Bardo. Hier befindet sich der von den Hafsiden im 15. Jahrhundert angelegte und von den türkischen Beys erweiterte Palastbezirk. Die Mitte des 19. Jahrhunderts erbaute Residenz des Beys beherbergt heute das tunesische Parlament. In den Räumen des ehemaligen Harems ist das Nationalmuseum von Bardo untergebracht.





</doc>
<doc id="5226" url="https://de.wikipedia.org/wiki?curid=5226" title="Glucose">
Glucose

Glucose (Abkürzung: Glc) oder Glukose, von ‚süß‘, ist eine natürlich vorkommende chemische Verbindung. Sie ist das häufigste Monosaccharid (Einfachzucker) und gehört damit zu den Kohlenhydraten. Es gibt zwei Enantiomere: -Glucose und -Glucose (für eine Erklärung der Bezeichnungen „“ und „“ siehe Fischer-Projektion). In der Natur kommt ausschließlich -Glucose vor. Diese wird auch als Traubenzucker oder in älterer Literatur als Dextrose bezeichnet. Glucose liegt in fester Form meist als Monohydrat "Hydratdextrose" vor.

Wenn in Texten „Glucose“ ohne weiteren Namenszusatz (Präfix) erwähnt wird, ist -Glucose gemeint. Die synthetisch herstellbare -Glucose besitzt nur untergeordnete Bedeutung.

Für den Körper des Menschen ist Glucose die Hauptenergiequelle, welche als Glykogen in der Leber gespeichert ist.

Der Traubenzucker wurde 1792 von Johann Tobias Lowitz in Weintrauben entdeckt und als von Rohrzucker (Saccharose) verschieden erkannt. Glucose ist der von Jean Baptiste Dumas 1838 geprägte Begriff, der sich in der chemischen Literatur durchgesetzt hat. Von Friedrich August Kekulé stammt der Vorschlag Dextrose (von Lateinisch "dexter" = rechts), da Traubenzucker die physikalische Eigenschaft besitzt, die Ebene des linear polarisierten Lichtes nach rechts zu drehen. Im Gegensatz dazu drehen -Fructose (eine Ketohexose) und L-Glucose polarisiertes Licht nach links. Die Struktur der Glucose und die Strukturbeziehungen zu den anderen Monosacchariden wurden 1891 von Emil Fischer beschrieben und stellten einen Meilenstein der Naturstoffchemie dar. Die Namen beziehen sich zunächst auf die Naturstoffe. Ihre Enantiomeren erhielten mit Einführung systematischer Nomenklaturen denselben Namen mit Berücksichtigung der absoluten Stereochemie (z. B. Fischer-Nomenklatur, /-Nomenklatur).

Traubenzucker ist als Baustein in Zweifachzuckern wie Lactose (Milchzucker) oder Saccharose (Rohr- oder Rübenzucker), in Mehrfachzuckern wie Raffinose und in Vielfachzuckern wie Stärke, Glykogen oder Cellulose enthalten. Er wird durch die vollständige enzymatische Spaltung von Stärke (z. B. aus Mais oder Kartoffeln) hergestellt. Daraus resultiert die früher gängige Bezeichnung „Stärkezucker“. Rohr- oder Rübenzucker ist ein Zweifachzucker, der aus je einem Molekül Glucose und Fructose zusammengesetzt ist.

Glucose wird hauptsächlich von Pflanzen mithilfe der Photosynthese aus Sonnenlicht, Wasser und Kohlenstoffdioxid produziert und kann von allen Lebewesen als Energie- und Kohlenstofflieferant verwertet werden. Normalerweise kommt Glucose aber nicht frei, sondern in Form ihrer Polymere, also Milchzucker, Rübenzucker, Stärke, Cellulose und andere vor, die in Pflanzen sowohl Reservestoffe als auch Bestandteil der Zellstruktur sind. Diese Polymere werden bei der Nahrungsaufnahme durch Tiere, Pilze und Bakterien mithilfe von Enzymen erst zu Glucose abgebaut. Beim Menschen geschieht dies teilweise bereits beim Kauen mittels Amylase, die im Speichel enthalten ist. Alle Lebewesen sind außerdem in der Lage, Glucose aus bestimmten Ausgangsprodukten selbst herzustellen, wenn sich die Notwendigkeit ergibt. Der Glucosegehalt des Blutes beträgt etwa 0,1 % und wird durch die Hormone Insulin und Glucagon geregelt.

Glucose wird in Lebewesen zu mehreren anderen chemischen Verbindungen umgesetzt, die Ausgangsprodukt verschiedener Stoffwechselwege sind. Neben der Phosphorylierung zu Glucose-6-phosphat, die Teil der Glycolyse ist, kann Glucose bei ihrem Abbau zunächst zu Glucono-1,5-lacton oxidiert werden. Glucose dient als Baustein bei der Biosynthese von Trehalose in Bakterien und von Glykogen. Glucose kann außerdem von der bakteriellen Xylose-Isomerase in Fructose umgewandelt werden.

Nicht zuletzt muss Glucose, um in Zellen und Zellkompartimente oder heraus zu gelangen, durch Membranen transportiert werden, wofür es beim Menschen mehrere Dutzend spezielle Transportproteine gibt, die hauptsächlich zur Major-Facilitator-Superfamilie gehören.

Der Stoffwechselweg, der mit Molekülen beginnt, die zwei bis vier Kohlenstoffatome (C) enthalten und im Glucosemolekül endet, das sechs Kohlenstoffatome enthält, heißt Gluconeogenese und kommt in allen Lebewesen vor. Die kleineren Ausgangsstoffe sind das Resultat anderer Stoffwechselwege und stammen bei Pflanzen letztlich von der Assimilation von Kohlenstoffdioxid.

Glucose ist ein Baustein vieler Kohlenhydrate und kann aus diesen mithilfe von bestimmten Enzymen abgespalten werden. So genannte Glycosidasen katalysieren zunächst die Hydrolyse langkettiger Polysaccharide, wobei endständige Glucose oder Disaccharide entfernt werden. Disaccharide wiederum werden meist von speziellen Glycosidasen zu Glucose abgebaut. Die Namen der abbauenden Enzyme sind oft vom jeweiligen Poly- und Disaccharid abgeleitet; so gibt es unter anderem für den Abbau von Polysaccharidketten Amylasen (von Amylose, Bestandteil der Stärke), Cellulasen (von Cellulose), Chitinasen (von Chitin) und mehr; weiters für die Spaltung von Disacchariden die Lactase, Saccharase, Trehalase und andere.

Beim Menschen sind etwa 70 Gene bekannt, die für Glycosidasen codieren. Sie haben Funktionen bei der Verdauung und beim Abbau von Glykogen, Sphingolipiden, Mucopolysacchariden und Poly(ADP-Ribose).

Der physiologische Brennwert von Glucose beträgt 15,7 kJ/g (3,74 kcal/g). Die hohe Verfügbarkeit von Kohlenhydraten durch die pflanzliche Biomasse hat während der Evolution, besonders der Mikroorganismen, zu einer Vielfalt von Methoden geführt, den Energie- und Kohlenstoffspeicher Glucose zu verwerten. Unterschiede bestehen darin, bis zu welchem nicht mehr für die Energiegewinnung nutzbaren Endprodukt der Weg führt. Hier entscheidet das Vorhandensein einzelner Gene, und ihrer Genprodukte, der Enzyme, welche Reaktionen möglich sind (siehe Abbildung). Dabei wird der Stoffwechselweg der Glycolyse von nahezu allen Lebewesen genutzt. Lediglich die an Extrembedingungen angepassten Bakterien und Archaeen haben weitere Abbaureaktionen erschlossen, die im Entner-Doudoroff-Weg zusammengefasst werden. Ein wesentlicher Unterschied dieses Wegs besteht in der Gewinnung von NADP als Reduktionsmittel für den Anabolismus, das sonst indirekt erzeugt werden müsste.

Zuletzt findet Glucose als Baustein bei der Glycosylierung von Proteinen und anderen Stoffen (katalysiert von Glycosyltransferasen) Verwendung und kann von den resultierenden Glycoproteinen, Peptidoglycanen und Glycosiden wieder abgespalten werden.

Traubenzucker kann im Körper über die Glycolyse, die oxidative Decarboxylierung, den Citratzyklus und die Atmungskette vollständig zu Wasser und Kohlenstoffdioxid abgebaut werden. Ist dafür nicht genügend Sauerstoff verfügbar, erfolgt der Glucoseabbau anaerob bis zum Lactat durch die Milchsäuregärung und setzt weniger Energie frei. Bei hohem Angebot an Glucose kann der Metabolit Acetyl-CoA aber auch zur Fettsäuresynthese genutzt werden. Ebenfalls wird durch Glucose der Glykogenspeicher des Körpers wieder aufgefüllt, der vor allem in Leber und Skelettmuskulatur zu finden ist. Diese Vorgänge werden hormonell reguliert. Durch die Gluconeogenese kann der Organismus Glucose aus anderen Stoffwechselprodukten, unter anderem aus Lactat oder bestimmten Aminosäuren unter Energieverbrauch aufbauen. Die Neubildung von bis zu 250 g Glucose am Tag findet hauptsächlich in der Leber statt. Auch die Tubuluszellen der Nieren können Glucose bilden. Eine dauernde Erhöhung des Glucosegehalts im Blut wird als Zuckerkrankheit (Diabetes mellitus) bezeichnet.

Die Glucose im Blut wird als Blutzucker bezeichnet. Der Blutzuckergehalt eines gesunden Menschen beträgt im nüchternen Zustand, d. h. nach Übernachtfasten, etwa 70 bis 100 mg/dl Blut (4 bis 5,5 mM). Im Blutplasma liegen die gemessenen Werte etwa 10–15 % höher. Außerdem liegen die Werte im arteriellen Blut über den Konzentrationen im venösen Blut, da Glucose während der Passage des Kapillarbettes in das Gewebe aufgenommen wird. Auch im Kapillarblut, das häufig zur Blutzuckerbestimmung verwendet wird, liegen die Werte teils höher als im venösen Blut. Nach Nahrungsaufnahme steigt die Blutzuckerkonzentration an. Werte über 180 mg/dl in venösem Vollblut sind sicher pathologisch und werden als Hyperglykämie bezeichnet. Ein wiederholt oder dauerhaft erhöhter Blutzuckerwert weist in der Regel auf Diabetes mellitus hin. Glucosekonzentrationen unter 40 mg/dl in venösem Vollblut werden als Hypoglykämie bezeichnet.

Als Kohlenhydrat mit sechs C-Atomen gehört Glucose zu den Hexosen. Als Aldose hat die Glucose eine Aldehydfunktion am ersten Kohlenstoffatom (im Gegensatz dazu werden Kohlenhydrate mit einer Ketogruppe als Ketosen bezeichnet). Durch eine intramolekulare Halbacetal-Bildung entsteht ein Ring: In der Pyranose-Form wird das erste mit dem fünften, in der selteneren Furanose-Form das erste mit dem vierten Kohlenstoffatom über eine Sauerstoffbrücke verbunden. Der Carbonylsauerstoff der Aldehydgruppe wird dabei zur Hydroxygruppe.

Die oben abgebildete Darstellung der pyranoiden Form – im Beispiel die α--Glucopyranose – wird als Haworth-Projektion bezeichnet. Dabei wird der Ring als eben dargestellt, was nicht der Realität entspricht, aber für viele Zwecke ausreichend ist. Durch den Ringschluss wird das erste C-Atom zu einem neuen, weiteren Chiralitätszentrum, so dass die Bildung von Diastereomeren möglich wird. Die Struktur, bei der die Hydroxy-Funktion des Halbacetals in der Haworth-Projektion nach unten weist, wird als α--Glucose bezeichnet, die mit der Hydroxy-Funktion nach oben als β--Glucose. Allgemein gilt, dass bei der α-Form die beim Ringschluss gebildete Hydroxy-Funktion auf der entgegengesetzten Seite der Ringebene der Haworth-Projektion steht wie die Hydroxymethylengruppe (C-Atom 6), bei der β-Form auf derselben. In dieser Form gilt die Definition sowohl für die - und -Zucker, als auch für Aldosen und Ketosen. α- und β--Glucose sind Beispiele für als Anomere bezeichnete Strukturen. Anomere sind stereoisomere Zucker, die sich nur in der Konfiguration an dem beim Ringschluss gebildeten Chiralitätszentrum unterscheiden. Anomere sind somit ein Spezialfall der Epimere.

Die Fischer-Projektion ist bei den cyclischen Halbacetal-Formen, siehe 1, unübersichtlich. Um die gewinkelte Anordnung der Kohlenstoff-Kette zu verdeutlichen, wird die Sessel-Darstellung 3 gewählt. Auch die Darstellung 4 ist üblich und stereochemisch eindeutig.

In wässriger Lösung kann der Ring geöffnet und geschlossen werden, so dass ein Gleichgewicht zwischen Pyranose- (Sechsring mit endozyklischem Sauerstoffatom, 99,75 %), Furanoseform (Fünfring, in Spuren) und offenkettiger Aldehydform (0,25 %) vorliegt. Die Zugabe von Säure oder Lauge beschleunigt diesen Vorgang. Da beim Ringschluss entweder die Alpha- oder die Beta-Form entstehen kann, liegt auch ein Gleichgewicht zwischen Alpha-Form (36,4 %) und Beta-Form (63,6 %) vor. Das Gleichgewicht liegt, wie an den prozentualen Anteilen zu erkennen ist, auf der Seite der β--Glucose. Sie ist das stabilere Anomer, da alle Hydroxygruppen in der Sesselkonformation äquatorial angeordnet sind und somit den größtmöglichen Abstand voneinander haben. Dass das α-Anomer mit immerhin 36,4 % trotz der axialen OH-Gruppe vorliegt, deutet darauf hin, dass es noch weitere Einflüsse geben muss. Die relative Stabilität der α-Konfiguration wird als anomerer Effekt bezeichnet.

Die Umwandlung zwischen den beiden Anomeren kann im Polarimeter beobachtet werden, da reine α--Glucose einen spezifischen Drehwinkel von +112,2°·ml·dm·g hat, reine β--Glucose von +17,5°·ml·dm·g. Hat sich nach einer gewissen Zeit das Gleichgewicht eingestellt, liegt ein Drehwinkel von +52,7°·ml·dm·g vor. Diese Veränderung des Drehwinkels wird als Mutarotation bezeichnet. Durch Zugabe von Säure oder Base kann diese Umwandlung massiv beschleunigt werden. Die Gleichgewichtseinstellung läuft über die offenkettige Aldehyd-Form.

In verdünnter Natronlauge werden Mannose, Glucose und Fructose ineinander umgewandelt (Lobry-de-Bruyn-Alberda-van-Ekenstein-Umlagerung), so dass sich ein Gleichgewicht zwischen diesen Isomeren ausbildet. Diese Reaktion verläuft über ein Endiol:

Diese Reaktionen haben nur historische Bedeutung:

Die Fehling-Probe ist ein klassischer Nachweis für Aldosen. Aufgrund der Mutarotation liegt Glucose immer zu einem geringen Anteil als offenkettiger Aldehyd vor. Durch Zugabe der Fehling-Reagenzien (Fehling-(I)-Lösung und Fehling-(II)-Lösung) wird die Aldehydgruppe zur Carbonsäure oxidiert, während der Cu-Tartratkomplex zu Cu reduziert wird und als ziegelroter Niederschlag (CuO) ausfällt.

Nach Zugabe von ammoniakalischen AgNO zur Probelösung wird Ag von Glucose zu elementarem Silber reduziert, welches sich an der Gefäßwand als Silberspiegel ablagert.

Bei der Barfoedschen Probe wird eine Mischung aus gelöstem Kupferacetat, Natriumacetat und Essigsäure, mit der Lösung des zu untersuchenden Zuckers versetzt und im Wasserbad einige Minuten erhitzt. Bei Glucose und anderen Monosacchariden bildet sich schnell eine rötliche Färbung und rotbraunes Kupfer(I)-oxid (CuO).

Das Enzym Glucose-Oxidase (GOx) setzt Glucose unter Verbrauch von Sauerstoff zu Gluconsäure und Wasserstoffperoxid um. Ein weiteres Enzym, die Peroxidase, katalysiert eine chromogene Reaktion (Trinder-Reaktion) von Phenol mit 4-Aminoantipyrin zu einem violetten Farbstoff.

Die Teststreifenmethode bedient sich der oben erwähnten enzymatischen Umsetzung von Glucose zu Gluconsäure unter Bildung von Wasserstoffperoxid. Die Reagenzien sind auf einer Polymermatrix immobilisiert, dem sog. Teststreifen, der eine mehr oder weniger starke Farbe annimmt. Diese kann mit Hilfe eines LED-basierten Handphotometers bei 510 nm reflektometrisch ausgelesen werden. Dies ermöglicht die routinemäßige Blutzuckerbestimmung durch Laien. Neben der Reaktion von Phenol mit 4-Aminoantipyrin wurden neue chromogene Reaktionen entwickelt, welche die Photometrie bei höheren Wellenlängen (550 nm, 750 nm) erlauben.

Die Elektroanalytik der Glucose basiert ebenfalls auf der oben erwähnten enzymatischen Umsetzung. Das produzierte Wasserstoffperoxid kann durch anodische Oxidation bei einem Potential von 600 mV amperometrisch quantifiziert werden. Die GOx wird auf der Elektrodenoberfläche oder in einer dicht vor der Elektrode angeordneten Membran immobilisiert. Als Elektroden kommen neben den klassischen Edelmetallen wie Platin oder Gold in jüngster Zeit immer häufiger Kohlenstoffnanoröhren-Elektroden zum Einsatz, welche z. B. mit Bor dotiert wurden. Auch Cu-CuO-Nanodrähte finden als enzymfreie amperometrische Elektroden Verwendung. Es wurde damit eine Nachweisgrenze von 50 µmol/L erzielt. Eine besonders aussichtsreiche Methode besteht im sog. „enzyme wiring“. Dabei wird das bei der Oxidation fließende Elektron direkt aus dem Enzym über einen molekularen Draht auf die Elektrode abgeleitet.

Für die Glucose gibt es eine Vielzahl anderer chemischer Sensoren. Angesichts der Bedeutung der Analytik von Glucose in den Biowissenschaften sind auch zahlreiche optische Sonden für Saccharide entwickelt worden, die auf der Verwendung von Boronsäuren beruhen und vor allem für intrazelluläre, sensorische Anwendungen in Frage kommen, wo andere (optische) Methoden gar nicht oder nur bedingt einsetzbar sind. Neben den organischen Boronsäurenderivaten, welche oft hochspezifisch an die 1,2-Diolgruppen der Zucker binden, gibt es noch weitere nach Funktionsmechanismen klassifizierte Sondenkonzepte, die selektive Glucose-bindende Proteine (z. B. Concanavalin A) als Rezeptor verwenden. Des Weiteren wurden Methoden entwickelt, welche die Glucosekonzentration indirekt über die Konzentration von verstoffwechselten Produkten erfassen, z. B. den Verbrauch von Sauerstoff mit Hilfe fluoreszenz-optischer Sensoren. Schließlich gibt es enzymbasierte Konzepte, die die intrinsische Absorbanz bzw. Fluoreszenz von (fluoreszenzmarkierten) Enzymen als Informationsträger nutzen.

Insbesondere für die Analytik komplexer Mischungen, die Glucose enthalten, wie z. B. beim Honig werden heute chromatographische Verfahren, häufig in Kopplung mit der Massenspektrometrie eingesetzt. Unter Berücksichtigung der Isotopenverhältnisse lassen sich mit dieser Analytik auch zuverlässig Honigverfälschungen durch zugesetzte Zucker nachweisen. Als vorteilhaft erweisen sich dabei Derivatisierungen unter Verwendung von Silylierungsreagenzien. So können auch die Anteile von Di- und Trisacchariden quantifiziert werden.

Glucose ist ein bedeutender biotechnologischer Rohstoff. Folgende Schautafel gibt einen kurzen Überblick über wichtige Produkte (Ein Klick auf die Namen führt zu den entsprechenden Artikeln). Die industriell interessanten Produkte bzw. deren Vorstufen sind fett gekennzeichnet:

Um sich die Glucose-Konfiguration in der Fischer-Projektion zu merken, gibt es folgende Eselsbrücke: Die Positionen der Hydroxygruppen rechts und links der Kohlenstoffkette lassen sich durch Onomatopoeia des Geräusches eines Martinshorns („Ta-Tü-Ta-Ta“) „versinnbildlichen“. Die Anordnung der Hydroxygruppen der Galactose kann man sich hingegen als Blaulicht (siehe Abbildung) merken.

"Um sich die Stellung der Hydroxy-Gruppen bei der Haworth-Projektion zu merken, gibt es folgenden Merksatz: F-l-o-H Fischer-links-oben-Haworth."





</doc>
<doc id="5227" url="https://de.wikipedia.org/wiki?curid=5227" title="Theodizee">
Theodizee

Theodizee [] (französisch "théodicée", griechisch θεοδικία "theodikía" von altgriechisch "theós" ‚Gott‘ und "díkē" ‚Gerechtigkeit‘) heißt „Gerechtigkeit Gottes“ oder „Rechtfertigung Gottes“. Gemeint sind verschiedene Antwortversuche auf die Frage, wie das subjektive Leiden in der Welt vor dem Hintergrund zu erklären sei, dass ein (zumeist christlich aufgefasster) Gott einerseits allmächtig, andererseits gut sei. Konkret geht es um die Frage, warum ein Gott oder Christus das Leiden zulässt, wenn er doch die Omnipotenz („Allmacht“) und den Willen („Güte“) besitzen müsste, das Leiden zu verhindern. Die Bezeichnung "théodicée" (später deutsch „Theodizee“) geht auf den Philosophen und frühen Vordenker der Aufklärung Gottfried Wilhelm Leibniz zurück.

Der Hinweis auf das Leid als religiöse oder religionskritische Frage ist bereits in Kulturen der Antike, z. B. im alten China, in Indien, Iran, Sumer, Babylonien und Ägypten zu finden. Skeptische Philosophen der griechischen Antike argumentierten, dass der Demiurg (wenn er existierte) in der Tat Übel verhindern müsste, und führten teils weitere Argumente zugunsten eines Agnostizismus oder Atheismus an.

Nach moderner Theologie behandelt schon die Geschichte von Hiob aus dem jüdischen Tanach (christliches Altes Testament) die Frage, wie es sein könne, dass ein gerechter Gott dulde, dass guten Menschen Böses widerfahre.

Die klassische Frage der christlichen Theologie nach der Rechtfertigung Gottes stellte sich für viele religiöse abendländische Menschen in besonderer Weise nach den Schrecken des Holocaust (vgl. auch Theologie nach Auschwitz), die aus eigenen Reihen erwuchsen.

Eine prägnante, oft zitierte philosophische Formulierung der "Theodizee" lautet:

Diese Argumentation wurde von dem lateinisch-afrikanischen Rhetoriklehrer und christlichen Apologeten Lactantius (ca. 250 bis nach 317) überliefert, der sie dem Philosophen Epikur zuschrieb; allerdings zu Unrecht, denn sie ist nicht epikureisch, sondern ist wohl in Anlehnung an einen unbekannten skeptischen Philosophen formuliert worden – möglicherweise Arkesilaos oder Karneades. Cicero hatte mit Berufung auf Poseidonios berichtet, Epikur habe die Götter ihrer Untätigkeit wegen geleugnet. Der Skeptiker Sextus Empiricus hat im 2. Jahrhundert n. Chr. eine ähnliche, etwas ausführlichere Überlegung entwickelt, die davon ausgeht, dass Gott für alles sorgen müsste, wonach es kein Übel geben dürfte, es existiert aber Übel; die Konsequenz, dass demnach Gott nicht existiert, wird nicht expliziert, ist aber offensichtlich impliziert. Dem Problem liegt das theistische Gottesbild zugrunde.

Man kann die Theodizee als Widerspruch konstruieren, der sich aus der Annahme ergibt, dass es Übel in der Welt gibt und Gott existiert:


Ähnliche Argumente lassen sich ebenfalls für andere Eigenschaften Gottes konstruieren, d.h. wenn Gott allwissend ist, dann erkennt er das Übel, und wenn Gott allgütig ist, dann will er das Übel verhindern. Das Problem wird nicht wesentlich modifiziert, wenn der Bereich der relevanten Übel spezifisch qualifiziert wird. So rekonstruiert, muss nach üblicher Analyse mindestens eine der obigen Aussagen modifiziert oder negiert werden. Die nachfolgenden Lösungsansätze tun dies auf unterschiedliche Weise.
Es gibt dabei einen Ansichtspunkt, der hier nicht berücksichtigt wird, nämlich dass Gott existiert, jedoch Übel nicht verhindert. Weiter kann man sagen, dass Gott entscheidet, ob er Übel zulassen oder verhindern will.

Schon der frühe christliche Kirchenlehrer und Philosoph Augustinus und später mittelalterliche Denker wie der Dominikaner Thomas von Aquin begründeten die Auffassung, das Übel habe kein eigenständiges Sein, sondern sei nur Mangel an Sein bzw. Mangel am Guten "(privatio boni)". Thomas nannte als Beispiel die Blindheit, die Entbehrung des Augenlichtes sei. Diese philosophische Position geht demnach von einem realen Mangel aus – im Gegensatz zu jener, die behauptet, das Leid bzw. das Übel sei für den davon Betroffenen nicht real.

Diese Privationstheorie hat eine „außerordentliche Erfolgsgeschichte“ hinter sich, schreibt der zeitgenössische Theologe Friedrich Hermanni. Vom 2. bis in das 17. Jahrhundert hinein sei sie in fast allen philosophischen Systemen unumstritten gewesen – zwischen den Kirchenvätern und den spätantiken Philosophen, zwischen Aristotelikern und Platonikern, zwischen Thomisten und Scotisten, zwischen Reformatoren wie Philipp Melanchthon und römisch-katholischen Dogmatikern wie Robert Bellarmin sei dies ein Punkt gewesen, in dem man sich einig war.

Im 17. Jahrhundert und bei einigen sogenannten "Nominalisten" im Universalienstreit bereits im 14. Jahrhundert wurde das Leiden hingegen als ein Seiendes – eine auf empirischen Feststellungen beruhende Tatsache – betrachtet. Daher komme dem Übel auch eine eigene Realität zu. Weiterhin wurde vorgebracht, dass auch ein bloßer Mangel an Gutem, der zu Leid führt, nicht mit der Allmacht und Allgüte Gottes zu vereinbaren sei.

Nach der Monadologie von Gottfried Wilhelm Leibniz gibt es eine unendliche Anzahl möglicher Welten. Von diesen hat Gott nur eine geschaffen, nämlich die vollkommenste, „die beste aller möglichen Welten“. Leibniz argumentierte:
Folglich müsse die Welt, die Gott hervorgebracht hat – also die tatsächlich existierende Welt –, „die beste aller möglichen Welten“ sein, und jede Form des Übels sei letztlich notwendig und erklärbar.

Dagegen brachte der Philosoph Gerhard Streminger verschiedene Einwände vor. Schon in dem Begriff „beste Welt“ sah er eine Schwierigkeit: Dieser Begriff sei „unbestimmt, da sich bei der Endlichkeit alles Geschaffenen über jede bestimmte Welt hinaus noch eine bessere denken lässt, so wie […] über jede größte Zahl noch eine größere“.

Außerdem meinte Streminger, ohne Zusatzüberlegungen enthalte Leibniz’ Argumentation eine Petitio Principii: Leibniz stütze den Hauptsatz seiner Theodizee, dass die vorhandene Welt die beste aller möglichen sei, mit dem Hinweis auf die Weisheit und Güte Gottes. So werde das, was in der Theodizee erst noch zu beweisen sei, nämlich die Güte Gottes, bereits als erwiesen vorausgesetzt.

Der Philosoph Bertrand Russell bekundete „höchstes Erstaunen“ darüber, „dass Menschen glauben können, diese Welt mit allem, was sich darin befindet, und mit all ihren Fehlern sei das Beste, was Allmacht und Allwissenheit in Millionen von Jahren erschaffen konnten“. Er fragte: „Meinen Sie, wenn Ihnen Allmacht und Allwissenheit und dazu Jahrmillionen gegeben wären, um Ihre Welt zu vervollkommnen, dass Sie dann nichts Besseres als den Ku-Klux-Klan oder die Faschisten hervorbringen könnten?“

Eine weithin bekannte Antwort auf Leibniz’ Lösungsvorschlag ist Voltaires satirische Novelle "Candide oder der Optimismus".

„Im Wege des Kontrastes und der Ergänzung“ leisteten Übel „zum optimalen Gesamtbild dieser Welt einen unverzichtbaren Beitrag“, so beschreibt der Philosoph Norbert Hoerster einen anderen Lösungsansatz.

Die Gegenüberstellung von Gegensätzen verschöne die Rede, erklärte Kirchenlehrer Augustinus, um fortzufahren: „so bewirkt die göttliche Redekunst, die statt der Worte sich der Dinge bedient, durch dieselbe Gegenüberstellung von Gegensätzen die Schönheit des Weltalls.“ Der Philosoph Gottfried Wilhelm Leibniz meinte: „Da aber die göttliche Weisheit … das erwählen mußte, was den besten Zusammenklang ergab und das Laster durch diese Pforte eingetreten ist: so wäre Gott nicht vollkommen gut, nicht vollkommen weise gewesen, wenn er es ausgeschlossen hätte.“

Zu Argumenten dieser Art meint der Philosoph Gerhard Streminger: „Selbst wenn im ästhetischen Bereich häßliche Teile zu einem Gesamtschönen zusammengefügt werden können, so ist nicht einzusehen, daß ein gütiger Gott Menschen wie Figuren in einem Schachspiel behandeln sollte – und nicht wie Individuen mit eigenen Antrieben. Menschen besitzen einen Eigenwert und nicht bloß einen Wert in einem übergeordneten Rahmen“, und: „Es existiert keine Analogie zwischen schön und gut. […] die Behauptung, das Vollkommene werde durch Leid erhöht, ist spätestens dann voll Lebensverachtung, wenn dieses Leid über ein gewisses Maß an Stärke und Quantität hinausgeht.“ Eine „christliche Theodizee“, erklärt Streminger, „muß um die moralische Persönlichkeit zentriert sein und nicht um die Idee der Schönheit des ganzen Universums; ihr entscheidender Grundsatz muß ethisch und kann nicht ästhetisch sein.“

Eine Variante dieses Lösungsansatzes stellt Gerhard Streminger vor: „daß Leid […] unerläßlich sei zum Bewußtwerden des Guten“. Streminger bestreitet das. Er weist darauf hin, dass ein allmächtiger Gott uns so geschaffen haben könnte, „daß wir das Gute als solches erkennen, es also auch dann schätzen, wenn wir Schlechtes niemals erfahren haben.“ Zu dem „Prinzip des Gegensatzes, demzufolge kontinuierliche Lust und Freude von niemandem als solche erlebt werden könne“, meint Streminger, mit einem solchen Prinzip könne bestenfalls ein äußerst geringes Maß an Leid erklärt werden. Selbst wenn dies Prinzip plausibel wäre, würde als Gegensatz Langeweile genügen.

Übel und Leiden könnten für ein spirituelles Wachstum notwendig sein. Diese Überlegung wurde von dem Theologen und Religionsphilosophen John Hick entwickelt und nach dem Kirchenvater Irenäus benannt. Das Verhältnis dieser Überlegung zur Willensfreiheit erläutert der Theologe Armin Kreiner folgendermaßen: Die „irenäische Theodizee“ sei keine Alternative zum Argument der Willensfreiheit, sondern setze diese als konstitutiven Bestandteil voraus. Die Existenz der Willensfreiheit ermögliche die Genese der Sittlichkeit.

Ein Problem dabei ist, dass viele Übel nicht dazu beizutragen scheinen, wie das Leiden von jungen, unschuldigen Kindern. Andere genießen ein Leben in Bequemlichkeit und Luxus, in dem es buchstäblich nichts gibt, was zu einer moralischen Entwicklung herausfordern würde. Ein weiteres Problem entsteht bei dieser Art von Theodizee, wenn beim „spirituellen Wachstum“ auf die Nützlichkeit zur Überwindung von Übel abgestellt wird. Denn wenn es kein Übel gäbe, das überwunden werden müsste, dann würde eine solche Fähigkeit ihre Nützlichkeit verlieren. In diesem Fall wäre es nötig, mehr über den inhärenten Wert von spiritueller Gesundheit zu sagen.

Durch die gesamte Bibel findet sich immer wieder der Hinweis, dass Gott durch Leiden Menschen in seine Nähe ziehen möchte: Nachdem Ijob durch das Leid gegangen ist, sagt er am Ende des Buches in : „Ich hatte von dir nur vom Hörensagen vernommen; aber nun hat mein Auge dich gesehen.“

Ähnliche Aussagen sind zu finden in

Es wird gewarnt, dass Menschen, denen es sehr gut geht, dazu neigen, Gott zu vergessen: „Als aber Jeschurun fett ward, wurde er übermütig. Er ist fett und dick und feist geworden und hat den Gott verworfen, der ihn gemacht hat.“ 

Martin Luther schreibt zu : „… werde ein jeglicher auch ein Falke, der sich in solcher Not in die Höhe schwingen könne und wisse aufs erste sicher, zweifle auch nicht, dass ihm Gott solche Not nicht zum Verderben zuschickt, […], sondern dass er ihn damit zum Gebet, zum Rufen und zum Streit treiben will, damit er seinen Glauben übe und Gott erkennen lerne, in einem andern Anblick, als er es bisher getan hat, und gewöhne sich auch, mit dem Teufel und den Sünden zu kämpfen und durch Gottes Hilfe zu siegen. Sonst lernten wir nimmermehr, was Glaube, Wort, Geist, Gnade, Sünde, Tod oder Teufel wäre, wo es immer in Frieden und ohne Anfechtung zugehen sollte. Damit würden wir denn Gott nimmermehr kennenlernen, wir würden nimmermehr rechte Christen, … Er will, dass du zu schwach sein sollst, solche Not zu tragen und zu überwinden, auf dass du in ihm stark werden lernest und er in dir durch seine Stärke gepriesen werde.“

Bei dem lutherischen Theologen Dietrich Bonhoeffer findet sich dieselbe Haltung: „Ich glaube, dass Gott aus allem, auch aus dem Bösesten, Gutes entstehen lassen kann und will. Dafür braucht er Menschen, die sich alle Dinge zum Besten dienen lassen [Hinweis auf ]. Ich glaube, dass Gott uns in jeder Notlage soviel Widerstandskraft geben will, wie wir brauchen. Aber er gibt sie nicht im Voraus, damit wir uns nicht auf uns selbst, sondern allein auf ihn verlassen. In solchem Glauben müsste alle Angst vor der Zukunft überwunden sein. Ich glaube, dass auch unsere Fehler und Irrtümer nicht vergeblich sind, und dass es Gott nicht schwerer ist, mit ihnen fertig zu werden, als mit unseren vermeintlichen Guttaten. Ich glaube, dass Gott kein zeitloses Fatum ist, sondern dass er auf aufrichtige Gebete und verantwortliche Taten wartet und antwortet.“

Menschen, die an Reinkarnation glauben, argumentieren: Gottes Güte bestehe darin, den Menschen einen Zeitrahmen von mehreren Leben zu geben, in welchen sie durch das von ihren falschen Entscheidungen verursachte Leid lernen könnten, sich im göttlichen Sinne zu verhalten. Diesem oder einem sehr ähnlichen Gedankenmodell folgen die Theosophie-Anhänger oder im christlichen Bereich die Anthroposophen, die Geistchristen und die sich „Urchristen“ nennenden Vertreter der Bewegung Universelles Leben, außerdem viele New-Age-Anhänger.

Das Übel ist nur ein notwendiges Durchgangsstadium; nach Hegel dient es der dialektischen Entwicklung der Geschichte, in der eine „göttliche Vorsehung“ den „absoluten, vernünftigen Endzweck der Welt“ verwirklicht. In Perioden des Glücks, so Hegel, fehle der Gegensatz; sie seien „leere Blätter“ in der Weltgeschichte. Das Ergebnis seiner Darlegungen nennt Hegel „die wahrhafte "Theodicee", die Rechtfertigung Gottes in der Geschichte“.

Laut jüdisch-mystischer Zohar-Auslegung des Buches Genesis hat Gott vor der Schöpfung unserer Welt andere Welten erschaffen und wegen ihrer Unvollkommenheit wieder zerstört (soweit herrscht Übereinstimmung mit der Interpretation des Midrasch). Die Reste dieser Welten haben sich laut Zohar als „Hülsen“ (heb. "Qlīpōt") erhalten, die fortdauern und das Böse in der Welt verursachen (die „andere Seite“, heb. "sitra aḥrā"). Da aber auch sie ursprünglich von Gott erschaffen wurden, enthalten sie noch „Funken von Heiligkeit“ (heb. "nīṣōṣōt šēl qədušā").

Gegen diese Theorie kann eingewandt werden, sie stehe im Widerspruch mit einigen Eigenschaften Gottes:

Diesen Widerspruch versuchte Isaac Luria durch die Einführung der Notwendigkeit des "Tzimtzum" aufzulösen. Tzimtzum, wörtlich "Zusammenziehung" oder "Rückzug", ist ein Akt göttlicher Selbstbeschränkung des En Sof ("des Unendlichen"). Aufgrund des Tzimtzum und dem Erscheinen des unendlichen Lichtes kommt es zum "Bruch der Gefäße" (šəvīrat hakəlīm).

Neutestamentliche Theologen wie der Heidelberger Klaus Berger weisen darauf hin, dass die Bibel selbst und damit der christliche Glaube nicht das Ziel habe, eine Antwort auf die Herkunft des Bösen zu geben, sondern eher darauf, dass Gott die Errettung daraus sei. Gott habe das Böse nicht geschaffen, sondern das Böse war bereits gegeben, als Gott zu wirken begann. Im Alten Testament der Bibel schaffe Gott die Welt als einen Bereich der Ordnung, der dem lebensfeindlichen Chaos abgerungen wurde. Das Chaos und die Mächte, die den Menschen und das Leben bedrohen, werden nach diesem Denkansatz hier vorerst zurückgedrängt, die Chaosmächte sind aber weiterhin anwesend und gefährlich, sobald die Anwesenheit Gottes schwindet.

Dadurch wird die Allmacht Gottes als Prinzip in Frage gestellt, welche im biblischen Denken so nicht bekannt sei, sondern eher aus dem Einfluss des griechischen Denkens komme. Gott wäre demnach dabei, in einer bösen Welt und einer unfertigen, schwachen Schöpfung sein Reich aufzubauen, aber dies könne er nicht mit einem Fingerschnippen und in einem Augenblick tun (insofern wäre der Begriff von Allmacht falsch). Allmacht sollte nach dieser Anschauung vielmehr so verstanden werden, als dass letzten Endes die Verheißung des Reiches Gottes und der vollendeten Schöpfung erfüllt wird und Gott mächtiger als alle anderen Mächte in Raum und Zeit ist, nicht aber, dass Gott alles und jedes jederzeit wirkt. Das Geheimnis der Zeit steht nach Berger zwischen der „schwachen Schöpfung“ und der Erfüllung der Verheißung:
Nach dem evangelisch-reformierten Theologen Karl Barth gibt es keine Lösung des Theodizee-Problems. Wir sind nicht berechtigt, Gott anzuklagen. Wir können nur dialektisch vom Paradoxon reden (Karl Barth: Das Böse ist die „unmögliche Möglichkeit“).

Ähnlich äußern sich Theologen von heute, so der ehemalige Präses der Evangelischen Kirche von Westfalen, Alfred Buß: 

Fast 2000 Jahre zuvor wird in den Sprüchen der Väter, einem Teil der Mischna und Hauptwerk der jüdischen Ethik, formuliert: „Rabbi Janai sagt: Es ist uns nicht gegeben zu wissen, warum Frevler in Wohlergehen und Gerechte in Leiden leben.“ (Kap. IV, Vers 19)

Immanuel Kant definierte das Problem wie folgt: „Unter einer Theodicee versteht man die Verteidigung der höchsten Weisheit des Welturhebers gegen die Anklage, welche die Vernunft aus dem Zweckwidrigen in der Welt gegen jene erhebt.“ Zugleich schienen für Kant alle philosophischen Versuche in der Theodizee zum Scheitern verurteilt. Wir seien zu begrenzt, um metaphysische Spekulationen anzustellen. Hier stoße unsere Vernunft an ihre Grenzen.

Eine andere Interpretation der Bibel besagt, dem Menschen "erscheinen" Dinge als übel, aber er kann nicht objektiv urteilen. Für Gott habe das Übel einen Sinn, obwohl es aus menschlicher Sicht unverständlich ist. Gott ist demnach nicht nur für das verantwortlich, was Menschen subjektiv als „gut“ bewerten, sondern für alles, wenn man seine "Allmacht" ernst nehmen will. Dies wird u. a. mit folgenden Stellen der christlichen Bibel begründet:


Nach dem evangelischen Theologen Klaus Koch entwerfe der Autor der dramatischen Geschichte des leidenden Hiob in der alttestamentlichen Weisheitsliteratur eine Theodizee-Antwort, die vor allem negativ sei. Insbesondere argumentiere jener Autor intensiv gegen den sogenannten „Tun-Ergehen-Zusammenhang“ (Glück/Wohlstand einerseits und Leid/Not andererseits gelten als eine Belohnung bzw. Bestrafung Jahwes für gerechtes oder sündhaftes Leben). Dass Hiob unschuldig sei, das wird nicht nur von ihm selbst beteuert – vielmehr sagt das auch Jahwe selbst: Hiob sei „untadelig und rechtschaffen; er fürchtet Gott und meidet das Böse“. Jahwe erkläre, er sei von Satan aufgereizt worden, Hiob „ohne Grund zu verderben“ . In Streitgesprächen mit seinen Freunden weise Hiob den angeblich göttlich garantierten Zusammenhang zwischen Leiden und Schuld plausibel zurück und verlangt eine andere Antwort von Jahwe. In zwei großen Gottesreden am Ende des Buches ergreife Jahwe selbst das Wort und rühme seine Schöpfung als Erweis seiner Macht und seines Wissens, im Gegensatz zu Hiobs Ohnmacht und Unwissenheit. Daraufhin widerrufe Hiob : „So habe ich denn im Unverstand geredet über Dinge, die zu wunderbar für mich und unbegreiflich sind“ . Nach dem katholischen Theologen Klaus Kühlwein erkenne Hiob am Ende „im farbenprächtigen Mosaik der Schöpfung das Antlitz des Schöpfers und einen Plan, der weit entfernt ist von menschlichen, allzu menschlichen Vergeltungsfantasien, von göttlicher Willkür und kosmischer Sinnlosigkeit“.

Theologisch anerkannt relevante Überlieferungen wie die christliche Bibel können laut dieser Sichtweise Klaus Kühlweins nicht den Anspruch erheben, vollständig und widerspruchsfrei zu sein. Die christlich-theologische/philosophische Erkenntnis, die allein aus diesen Quellen geschöpft werden könne, genüge nicht, um ein hinreichend plausibles Bild der Beweggründe, Pläne und Ziele eines höchst vollkommenen Gottes und Christus, welchem Liebe, Weisheit und Macht in höchster Potenz zugeschrieben werden, zu zeichnen. (Mögliche Quellenerweiterung, siehe Mystik, Neuoffenbarung.)

Nach Ansicht des römisch-katholischen Theologen Hans Küng soll „Durch Leiden … der Mensch zum Leben gelangen. Warum das so ist, warum das für den Menschen gut und sinnvoll ist, warum es nicht ohne Leid besser ginge, das kann keine Vernunft erweisen. Das kann aber vom Leiden, Sterben und neuen Leben Jesu im Vertrauen auf Gott schon in der Gegenwart als sinnvoll angenommen werden, in der Gewissheit der Hoffnung auf ein Offenbarwerden des Sinnes in der Vollendung.“

„Unbedingtes und restloses Vertrauen“ zu Gott, trotz „Unfähigkeit, das Rätsel des Leids und des Bösen enträtseln zu können“, dafür wirbt Küng mit dem Versprechen, darin finde „der leidende, zweifelnde, verzweifelte Mensch“ einen „letzten Halt“; so lasse sich das Leid „zwar nicht ‚erklären‘, aber bestehen“. So verschiebt Küng den Akzent des Theodizee-Problems: weg vom Problem eines logischen Widerspruchs zwischen zwei Aussagen, hin zur Frage nach der Qualität der Beziehung des gläubigen Menschen zu seinem Gott und hin zu der Frage, welche Auswirkungen dies Gottvertrauen auf das Leben eines Menschen haben kann, insbesondere auf das Leben eines leidenden Menschen.

Der katholische Professor für Fundamentaltheologie Armin Kreiner nennt es hingegen „völlig abwegig“, aus der Erkenntnis der Fehlbarkeit des menschlichen Ermessens zu folgern, „daß es besser wäre, auf rationale Kontrolle zu verzichten und sein Vertrauen stattdessen ausschließlich auf die göttliche Offenbarung zu setzen. Denn auch die Akzeptanz eines Offenbarungsanspruchs […] ist ein Akt des Glaubenssubjekts – ein Akt, den es, soweit es in seiner Macht steht, vor sich selbst und vor anderen zu verantworten hat. Der bewußte Verzicht auf vernünftige Kriterien der Verantwortbarkeit heiligt nicht den Glaubensgehorsam, sondern entehrt ihn zum blinden Obskurantismus.“

„Gott ist bei den Leidenden. Gott zieht uns nicht plötzlich aus dem Leiden, aber wenn wir leiden und angefochten sind, steht Gott uns bei.“ (Margot Käßmann, von 1999 bis Februar 2010 Landesbischöfin der Evangelisch-lutherischen Landeskirche Hannovers). : „Gepriesen sei der Herr, Tag für Tag! Gott trägt uns, er ist unsre Hilfe.“ Und : „Denn der Herr hat gesagt : ‚Ich will dich nicht verlassen und nicht von dir weichen.‘“

Hans Küng spricht von der „Anmaßung, als neutraler und angeblich unschuldiger Zensor über Gott und die Welt das Urteil sprechen zu wollen“, da es dem Menschen nicht zu stehe, die Theodizee-Frage zu stellen, sondern darum, sich ein Urteil über einen "Glauben" zu bilden: ein Urteil darüber, ob der Glaube an einen allmächtigen und gütigen Gott trotz Theodizee-Problem gerechtfertigt ist.

Für die Theodizee-Frage sind unter den Eigenschaften Gottes die (angenommene) Allgüte, Allmacht, Unbegreiflichkeit und Allwissenheit relevant. Norbert Hoerster vertritt die Meinung, dass der Theist wenigstens eines der Gottesattribute aufgeben müsse. Entweder sei Gott allgütig oder er sei allmächtig. Beide Eigenschaften gleichzeitig anzunehmen betrachtet er als in sich widersprüchlich und damit irrational.

Einige Theologen und Philosophen haben – zum Teil mit Hinweis auf biblische Aussagen – die Meinung vertreten, dass Gott in sich komplex und eben nicht nur gut sei. Der ‚liebe‘ Gott wäre eine Verkürzung des biblischen Gottesbildes, wobei man dennoch auf diesen Aspekt Gottes vertrauen soll. Bekannt sind die Unterscheidungen von Luther und Schelling: Luther hebt den Deus absconditus (verborgenen Gott; Zorn, Gesetz) und den Deus revelatus (offenbarten Gott; Liebe, Evangelium) voneinander ab; Schelling unterscheidet zwischen Grund und Existenz in Gott, wobei Gott qua Grund die Ursache für das Übel sei. Friedrich Nietzsche bestreitet Gottes Gutsein und sieht ihn „jenseits von Gut und Böse“.

Einer noch stärker ausgeprägten ambivalenten Gottesvorstellung begegnet man zum Beispiel im Hinduismus, in der altägyptischen Religion, in der griechischen Mythologie oder in der germanischen Mythologie, wo die Götter nicht als absolut gütig und gut betrachtet werden. Sie vereinen helfende, gebende und friedensbringende Eigenschaften ebenso in sich wie zerstörerisch-wütende und kriegerische. In diesem Sinne wird durch eine ambivalente Gottesvorstellung ebenfalls die Allgüte Gottes relativiert.

Es wird argumentiert, die Gerechtigkeit Gottes mache es erforderlich, dass er nicht immer auf maximales Wohlergehen hinwirken könne. Menschliches Leiden wird gedeutet als „gerechte Strafe“ für menschliches Fehlverhalten und/oder für Ungehorsam gegenüber den Geboten Gottes und/oder für „Sünde“, d. h. die Trennung des Menschen von Gott.

Dieser Theodizee-Versuch ist dem Einwand ausgesetzt, dass menschliches Leiden oft in keinem Verhältnis zur Schuld des Betroffenen stehe, dass auch Unschuldige litten, z. B. Säuglinge. So erhalte man keine Lösung des Theodizee-Problems, sondern ein Theodizee-Problem in etwas veränderter Gestalt: „Verträgt sich die Lehre vom allmächtigen und gerechten Gott mit der Erfahrung einer Welt voller Ungerechtigkeiten?“ Hinzu kommt, nach Bart D. Ehrman, Professor für Neues Testament, ein doppeltes Problem: dass die Vorstellung von Leiden als Strafe Gottes „sowohl falsche Sicherheit als auch falsche Schuld“ erzeugt. Er schreibt:

Der Philosoph Norbert Hoerster wendet sich gegen Argumentationen: Gottes Allgüte sei mit menschlichen Begriffen nicht zu erfassen, der menschliche Begriff der Güte beschreibe die Allgüte Gottes nur unvollkommen und nicht fehlerfrei und der Widerspruch im Theodizee-Problem sei lediglich eine Folge der Fehlerhaftigkeit menschlicher Begriffe: 

Weitere Ansätze bei der Lösung der Theodizee-Frage liegen in der Annahme, dass Gott dem Menschen Freiheit und Eigenverantwortung in seinem Handeln lasse.

Der Philosoph Bertrand Russell vertrat die Meinung, ein allmächtiger Gott sei für alles verantwortlich. Es sei sinnlos anzuführen, das Leiden in der Welt sei durch die Sünde verursacht. Selbst wenn das wahr wäre, würde es nichts bedeuten. Wenn Gott im Voraus gewusst hätte, welche Sünden die Menschen begehen würden, so wäre er eindeutig für alle Folgen dieser Sünden verantwortlich, durch seinen Beschluss, den Menschen zu erschaffen. Von einer anderen Seite beleuchtete der Philosoph John Leslie Mackie das Problem: Wenn die Menschen Freiheit hätten in dem Sinne, dass sie in einigen Fällen tatsächlich so oder so entscheiden könnten – wenn das also weder durch äußere Umstände noch durch die Wesensart dieser Menschen festgelegt wäre –, dann wäre es unmöglich zu wissen, wie sie sich entscheiden würden, bevor sie sich entschieden hätten; niemand könnte das vorher wissen, auch kein allmächtiger Gott, der alles wüsste, was gewusst werden kann. So hätte Gott nicht wissen können, wie die Menschen ihre Freiheit gebrauchen würden. Eine solche Verteidigungsstrategie für Gott, meinte Mackie, gelinge jedoch nur auf Kosten einer sehr ernsten Aushöhlung dessen, was man gewöhnlich unter der Allwissenheit Gottes versteht. Außerdem gab Mackie zu bedenken, dass selbst dann, wenn ein allmächtiger Gott nicht wissen konnte, was Adam, Eva und Satan tun würden, wenn er sie erschüfe, er doch zweifellos wissen würde, was sie tun "könnten". So wäre er ein „höllisches Risiko“ eingegangen, als er Adam, Eva und Satan erschuf; ja, er wäre das Risiko eingegangen, dass die Menschen noch weitaus bösartiger sein könnten, als sie es tatsächlich sind. 

Grundlage dieses Ansatzes, z. B. vertreten durch den protestantischen Pfarrer, Prediger und Schriftsteller Wilhelm Busch (1897–1966), ist die Beobachtung, dass in der westlichen Welt die Säkularisierung stets voranschreitet. Die Gebote Gottes werden nicht mehr beachtet und sind den meisten Menschen nicht einmal bekannt. Dies ist ein klares Nein zu Gott durch die Nichtbeachter. Ein „Nein“ durch Nicht-Kenner ist das noch keineswegs. Gott respektiert diese scheinbar endgültige Entscheidung und zieht sich weitgehend, aber nicht ganz zurück. Gott weiß nämlich, dass der Mensch dazulernt und zu einem geistig fortgeschritteneren Zeitpunkt aufgrund gereifter Einsicht, dass er Gottes Hilfe braucht, eine andere Entscheidung treffen könnte, die eine vertieftere Wissens- und Verstehenslage beinhalten kann.

Dietrich Bonhoeffer trieb solch eine Sicht in einem seiner Briefe auf die Spitze: „Vor und mit Gott leben wir ohne Gott. Gott lässt sich aus der Welt hinausdrängen ans Kreuz, Gott ist ohnmächtig und schwach in der Welt und gerade und nur so ist er bei uns und hilft uns.“

Aus der Erfahrung persönlichen Leides kommt der Rabbi Harold S. Kushner in seinem Buch „Wenn guten Menschen Böses widerfährt“ zu dem Schluss, dass Gott zwar gut, aber nicht allmächtig sei. Die Frage nach dem „Warum?“ des Leides führe zu nichts, da sie entweder Wut auf sich selbst (Was habe ich getan, dass mir das passiert?) oder auf Gott (Warum lässt Gott das zu?) zur Folge habe und diese Wut verhindere, dass der Mensch Hilfe von anderen Menschen und von Gott annehmen könne. Da Gott durch die Menschen wirke, solle die Frage vielmehr lauten „Wenn mir dieses Leid nun schon einmal passiert ist, wer kann mir helfen?“
Dieser Lösungsansatz ist auch im Rahmen der sog. Theologie nach Auschwitz verbreitet. Auch Hans Jonas ist der Meinung, dass der Begriff der Allmacht zweifelhaft sei und dass deswegen auf dieses Gottesattribut verzichtet werden müsse. Jonas glaubt, dass Gott deswegen in Auschwitz nicht eingegriffen habe, weil er es nicht konnte. Jonas schlägt deshalb die Idee eines Gottes vor, der darauf verzichtet (hat), in den Verlauf des Weltgeschehens einzugreifen.

Durch die Kreuzigung Christi sei die Ohnmacht des dreieinigen Gottes deutlich geworden (Dorothee Sölle: „Gott hat keine anderen Hände als die unseren“). Zugleich werde die besondere Nähe des christlichen Gottes zu den Menschen in der Passion beschrieben. Gott entäußere sich selbst und unterwerfe sich menschlicher Grausamkeit, um zugleich eine Perspektive aufzuweisen, die in die Ewigkeit hineinrage.

Das Böse sei durch gefallene Engel, den Teufel, Demiurgen oder miteinander konkurrierende Weltprinzipien (Dualismus) zu erklären. Als Beispiel hierfür kann die altpersische Religion Zarathustras dienen, die davon ausging, dass zwei gleich mächtige Urprinzipien die Welt beherrschen: auf der einen Seite das gute, gebende, göttliche Prinzip, auf der anderen Seite das böse, nehmende, widergöttliche. Auf diese Art und Weise wird die Allmacht Gottes bestritten, und (der gute) Gott ist dann nicht mehr für die Existenz des Bösen verantwortlich.

Auch die neutestamentliche Lehre des Paulus von Tarsus enthält in wichtigen Teilen Aspekte des persischen Dualismus (vgl. : „sündiges Fleisch“, Ursünde), die mit Eigeninterpretationen des Tanach zur paulinischen Theologie vermischt wurden.

In gnostischen Schriften wird die Herkunft des Bösen durch ein für Gott inakzeptables Verhalten einiger Engel beschrieben. Diese sahen Adam, der als Gottes Ebenbild geschaffen wurde, und lachten ihn wegen seiner Schwäche aus. Da Gott diese Engel verstieß, wurden sie zu seinen Feinden. Und da sie Gott selbst nicht bezwingen können, wollen sie Gottes Schöpfung durch einen Abnutzungskampf zerstören. Der Mensch kann sich nun nach seinem Schöpfer ausrichten oder sich unter der Herrschaft dieser Engel der Selbstzerstörung hingeben.

Solche dualistische Gottesvorstellungen finden sich bei den Bogomilen, in der Gnosis und im Manichäismus.

Ein atheistisches Beispiel wäre das Yin-Yang der chinesischen Philosophie, welches die Geschehnisse in der Welt durch dualistische Urprinzipien erklärt.

Die Gnosis sieht den Menschen als für eine vollkommene Gottesbeziehung gedacht. Durch die Sünde wurde die Weiterentwicklung der Schöpfung unvollkommen, wodurch die Beziehung des Menschen zu seinem Schöpfer und Ursprung getrübt wird. Der Mensch leidet unter diesem Umstand. Gott hüllt sich aufgrund der Sünde in einen Nebel, der dem Menschen die Distanz und Freiheit gibt, während der Zeit, die ihm gegeben ist, in Sünde zu leben, wenn er das will. Gottes Allwissenheit wird zur warnenden Botschaft einer absoluten, heiligen Gerechtigkeit, die nicht auf diese Welt beschränkt bleibt.

Peter Knauer hält das Theodizee-Problem für das Ergebnis einer von „vorneherein falsche[n] Fragestellung“. Er betrachtet das Theodizee-Problem als ein spekulatives Problem, bei dem von Gott auf die Welt und von der Welt auf Gott geschlossen werde. Das ist seiner Meinung nach aber unzulässig, da es keine Gott und die Welt übergreifende Wirklichkeit gebe. Er verschiebt die Fragestellung von der spekulativen Ebene zu einer existentiellen Frage. Also nicht mehr „Wie kann Gott das Übel zulassen?“, sondern „Wie kann der Mensch die eigene Endlichkeit aushalten und bestehen?“

Die atheistische Schlussfolgerung aus der, wie man meinte, misslungenen Theodizee gewann Ende des 18. Jahrhunderts an Boden. Als nach dem Erdbeben von Lissabon 1755 die optimistische Leibniz’sche Lösung der Theodizee für viele an Plausibilität einbüßte, war es nur noch ein kleiner Schritt, anstatt Gottes Güte gleich Gottes Existenz zu verneinen.

Viele Atheisten und Agnostiker ziehen aus dem Theodizee-Problem ähnliche Schlüsse wie der Philosoph Norbert Hoerster: „… dass jedenfalls auf dem gegenwärtigen Stand unseres Wissens die Existenz eines ebenso allmächtigen wie allgütigen göttlichen Wesens angesichts der vielfältigen Übel der Welt als "äußerst unwahrscheinlich" gelten muss.“ Joachim Kahl sieht im Theodizee-Problem sogar eine „empirische Widerlegung des Gottesglaubens“. John Leslie Mackie führte aus: Da es nun einmal Übel gebe und „keine plausible Theodizee in Sicht“ sei, spreche viel dafür, „dass sich der Theismus nicht widerspruchsfrei darlegen lässt, ohne dass wenigstens eine seiner zentralen Aussagen wesentlich verändert wird.“ In einer Gesamtschau kommt Mackie „nach Abwägen der Wahrscheinlichkeiten“ zu dem Ergebnis, „dass weitaus mehr gegen die Existenz eines Gottes spricht als dafür“.

Eine besondere Sichtweise findet sich bei Odo Marquard: 
Damit sei die Theodizee in der zweiten Hälfte des 18. Jh. in die Geschichtsphilosophie gemündet.
An Gottes Allgüte, Allwissenheit und Allmacht wird also in vollem Umfang festgehalten. Zur Rettung aller drei klassischen Eigenschaften wird aber die Existenz (des so definierten) Gottes aufgegeben.

Der Mathematiker, Logiker und Philosoph Raymond Smullyan führt in seinem Dialog „Ist Gott ein Taoist?“ zwischen Gott und einem Sterblichen aus, dass die Existenz fühlender Wesen unvermeidlich den freien Willen nach sich zieht und damit die Möglichkeit der Entscheidung für oder gegen das Böse, hier definiert als Handlung, die anderen fühlenden Wesen Leid zufügt. Daher ist es auch Gott logisch unmöglich, eine Welt mit fühlenden Wesen zu erschaffen, in der das Böse nicht existiert, genauso wenig wie es ihm möglich ist, ein Dreieck in der Ebene zu erschaffen, dessen Winkelsumme nicht 180° beträgt.

Das Problem des Bösen, das gute Menschen ohne erkennbare Ursache erleiden, wird nicht direkt thematisiert, allerdings werden die Themen Reinkarnation und Karma indirekt angesprochen. Gott sieht jeden Menschen als in Entwicklung begriffen. Durch seine freien Entscheidungen macht der Mensch notgedrungen Erfahrungen, die zu Leid bei sich selbst oder anderen fühlenden Wesen führen. Um dieses Leid zu vermeiden, wird der Mensch gewissermaßen durch Erfahrung automatisch gut und letzten Endes zu einem Engel, der zu Gott zurückkehrt. Dieser Entwicklungsprozess dauere zu Gottes Bedauern leider sehr lange, und er könne daran ebenfalls nichts ändern.













</doc>
<doc id="5230" url="https://de.wikipedia.org/wiki?curid=5230" title="Tastgrad">
Tastgrad

Der Tastgrad (auch Aussteuergrad, ) gibt für eine periodische Folge von Impulsen gemäß Normung das Verhältnis der Impulsdauer zur Periodendauer an. Der Tastgrad wird als Verhältniszahl der Dimension Zahl mit einem Wertebereich von 0 bis 1 oder 0 bis 100 % angegeben. Dieses wird nachfolgend an idealen Impulsen erläutert, die eine Rechteckschwingung bilden.

Speziell für den Tastgrad formula_2 = 0,5 = 50 % entsteht ein "symmetrischer" Puls.

Der Begriff Tastverhältnis kommt in den genannten Normen nicht vor und wird in der Literatur nicht einheitlich verwendet; er wird sowohl für den Tastgrad als auch dessen Kehrwert als auch für das Verhältnis zwischen Impulsdauer und Impulsabstand verwendet. Die letzte Variante entspricht dem nicht mehr genormten Begriff "Ein-Aus-Verhältnis", also dem Verhältnis formula_3 mit einem Wertebereich von 0 bis formula_4.

Durch Variation des Tastgrades lässt sich der Gleichwert der elektrischen Spannung ändern. Da diese Einstellung aufgrund des Schaltens nahezu ohne Verlustleistung erfolgt – im Gegensatz zur Einstellung mit einem Vorwiderstand –, ist diese digitaltechnische Methode eine verbreitete Methode zur Steuerung elektrischer Spannung und Leistung. Anwendungen sind Schaltnetzteile und die Schwingungspaketsteuerung für Heizungen.

Die zugrundeliegende Pulsweitenmodulation ist auch eine Modulationsart zur Erzeugung stufenlos einstellbarer Gleichspannung, in der die Demodulation durch einen Tiefpass hinter der pulsweitenmodulierenden Schaltung zur Mittelwertbildung führt. Trotz des Ein-Aus-Schaltens entsteht ein Analogsignal, da der Tastgrad eine stufenlos einstellbare Größe ist. Bei genügend trägen Anwendungen wird auf den Tiefpass verzichtet, weil zur Mittelwertbildung alleine schon die Trägheit des Systems ausreicht, zum Beispiel bei Heizungen.

Auch manche Digital-Analog-Umsetzer arbeiten mit einstellbarem Tastgrad. Bei einer Rechteckspannung mit konstanter Periodendauer bestimmt die digitale Eingangsinformation die in diesem Fall schrittweise einstellbare Impulsbreite und somit die dazu proportionale mittlere Ausgangsspannung.

Bei der Phasenanschnittsteuerung werden Motordrehzahlen eingestellt mittels variablen Tastgrades an einer sinusförmigen Spannung. Entsprechendes gilt für die Choppersteuerung.


</doc>
<doc id="5233" url="https://de.wikipedia.org/wiki?curid=5233" title="Theogonie">
Theogonie

Die Theogonie ( "Entstehung der Götter", von "Gott" und "(geboren) werden, entstehen") ist ein Werk von Hesiod, in dem die Entstehung der Welt und der Götter in der Abfolge ihrer Herrschaft geschildert wird. Sie zählt neben der "Odyssee" und der "Ilias" von Homer zu den ältesten Quellen (ca. 700 v. Chr.) der griechischen Mythologie.

Eingeleitet wird Hesiods Theogonie mit dessen Dichterweihe durch die Musen. Diese überreichen Hesiod als Zepter „ein Gesproß frischgrünenden Lorbeers“ und hauchen ihm göttlichen Gesang ein, so weihen sie ihn zum Sänger der Theogonie.

Nach diesem Proömium beginnt Hesiods poetischer Bericht vom Werden der Welt und über die Abstammung der Götter: Der Kosmos nimmt seinen Anfang mit dem Erscheinen von sechs Urgottheiten. Das sind Chaos, Gaia, Tartaros, Eros, Erebos und Nyx. Ob diese sechs unabhängig voneinander entstehen oder ob aus Chaos die weiteren fünf hervorgehen, ist unklar. In der Regel wird jedoch Letzteres angenommen.

Als Nächstes gebiert Gaia durch Eros – ohne Begattung im Schlaf – Uranos, den Himmel, die Ourea, die Berge, und Pontos, das Meer. Nyx und Erebos zeugen Aither, die Luft, und Hemera, den Tag. Von Gaia und Uranos stammen die Titanen, die Kyklopen und die Hekatoncheiren ab. Uranos verbirgt seine Kinder in der Erde und lässt Gaia sie nicht gebären. Dies erbost Gaia und sie stiftet Kronos, einen der Titanen, zum Umsturz an. Kronos entmannt auf ihren Rat seinen Vater mit einer Sichel und wirft das Glied ins Meer. Durch die Entmannung seines Vaters wird er zum Herrscher der zweiten Göttergeneration. Aus dem Blut, das aus Uranos’ Glied auf Gaia fällt, entstehen die Giganten, die Erinnyen und die melischen Nymphen. Aus dem Samen des in den Pontos geschleuderten Gliedes wächst Aphrodite hervor.

Auf diese erste längere Erzählung folgen weitere Genealogien. Es werden die Nachkommen der Nyx aufgezählt, die des Pontos und die Nachkommen der Titanen Okeanos und Tethys, die Okeaniden und die Flüsse. Es folgen die Nachkommen der Titanen Theia und Hyperion, des Titanen Kreios mit der Tochter des Pontos Eurybia und der Titanen Phoibe und Koios. Schließlich werden die Nachkommen des Kronos und der Rhea genannt: Hestia, Demeter, Hera, Hades, Poseidon und Zeus.

Nachdem Kronos von Uranos und Gaia vorausgesagt wurde, dass er von einem seiner Söhne entmachtet würde, verschlingt er seine Kinder. Rhea gelingt es jedoch, Zeus zu verstecken und Kronos stattdessen einen gewindelten Stein verschlingen zu lassen. Durch eine List erbricht Kronos den Stein und die übrigen Kinder, woraufhin Gaia ihnen das Versteck der Kyklopen zeigt. Zeus befreit diese und erhält von den schmiedenden Kyklopen den Donner, den Zündkeil und den Blitz.

Nach der Beschreibung der Geburt des Zeus folgt die Genealogie des Titanen Iapetos mit der Okeanide Klymene. Diese werden alle von Zeus für ihre Missetaten bestraft, darunter auch Prometheus, dem an eine Säule gefesselt ein Adler seine Leber abfrisst, bis Herakles ihn später befreit. (In anderen Überlieferungen, z. B. in der Tragödie des Aischylos, wurde Prometheus auf Befehl des Zeus durch Hephaistos in Ketten an einem Felsen im Kaukasus geschmiedet.) Die Bestrafung des Prometheus wird als einzige erläutert: Prometheus verteilte einen geschlachteten Ochsen, wobei er versuchte Zeus zu täuschen. Das Fleisch legte er auf einen Haufen und bedeckte ihn mit Haut und Magen, auf einen anderen Haufen legte er die Knochen und bedeckte sie mit Fett. Zeus erkannte den Trug und wählte dennoch den Haufen mit den Knochen, seitdem opfern die Menschen den Göttern die Knochen und essen das Fleisch selbst. Zeus verbirgt daraufhin das Feuer vor den Menschen, doch Prometheus stiehlt es und bringt es ihnen zurück. Zeus lässt Hephaistos die Pandora aus Lehm gestalten und schickt sie zu den Menschen, um ihnen Unglück zu bringen.

Nach der Prometheus-Erzählung wird der weitere Ablauf der Titanomachie geschildert. Zeus und seine Geschwister führen zehn Jahre lang Krieg gegen die Titanen, bis Gaia das Versteck der Hekatoncheiren verrät. Diese werden befreit und gemeinsam besiegen sie die Titanen und verbannen sie in den Tartaros, wo sie von den Hekatoncheiren bewacht werden. Dem schließt sich eine längere Beschreibung des Tartaros und seiner Bewohner an. Es folgt eine letzte Auseinandersetzung: Nach der Verbannung der Titanen gebiert Gaia den Typhoeus, um die Götter zu entmachten. Er erringt beinahe den Sieg, wird dann aber von einem Blitz des Zeus bekämpft und in den Tartaros verbannt, wo aus ihm die schädlichen Winde entstehen. Nach diesem letzten Kampf wird Zeus von den anderen Göttern zum Herrscher bestimmt.

Es folgt ein Katalog der Ehen des Zeus, zuerst die mit Metis. Gaia prophezeit Zeus, ein Sohn von Metis werde ihn stürzen, wie auch er Kronos und dieser Uranos stürzte, daraufhin verschlingt er seine Frau. Zeus gebiert daraufhin Athene aus seinem Kopf. Nach den Ehen mit weiteren Göttinnen folgen die Verbindungen von Zeus mit sterblichen Frauen sowie Ehen der anderen Olympier. Mit einem eigenen Prolog folgt zuletzt ein Katalog der Göttinnen, die Nachkommen mit sterblichen Männern haben.

Hesiods Götter werden zwar als überwiegend gut und menschenfreundlich dargestellt, weisen jedoch auch Eigenschaften auf und begehen Taten, die in der griechischen Gesellschaft im Allgemeinen als verwerflich galten. Schon Xenophanes warf Homer und Hesiod daher ihren Anthropomorphismus vor: "Alles haben Homer und Hesiod den Göttern angehängt, was bei Menschen Schimpf und Schande ist: Stehlen, Ehebrechen und sich gegenseitig betrügen!"





</doc>
<doc id="5234" url="https://de.wikipedia.org/wiki?curid=5234" title="Tesla">
Tesla

Tesla steht für:

Personen:

TESLA steht für:
Siehe auch:


</doc>
<doc id="5235" url="https://de.wikipedia.org/wiki?curid=5235" title="Tesla-Turbine">
Tesla-Turbine

Eine Tesla-Turbine (auch "Scheibenläuferturbine") ist eine schaufellose Turbine, die von Nikola Tesla erfunden und am 21. Oktober 1909 von ihm patentiert wurde. Sie findet auch als Pumpe Anwendung, wenn sie angetrieben wird.

Eine Tesla-Turbine besteht im Wesentlichen aus mehreren möglichst glatten, kreisförmigen Scheiben. Diese sind in gleichmäßigem, geringem Abstand zentrisch auf einer Welle montiert. Durch Düsen wird das Fluid tangential auf den äußeren Rand der Scheiben eingebracht und beschleunigt diesen bis nahe seiner eigenen Geschwindigkeit. Die Energieübertragung auf die Scheiben erfolgt dabei durch Viskosität und Adhäsion. Das Fluid wird dadurch abgebremst und in Richtung zur Scheibenmitte hin abgelenkt. Es strömt in Folge dessen spiralförmig nach innen, wobei es weiterhin Bewegungsenergie auf die Scheiben überträgt. In der Welle befinden sich Öffnungen, durch welche das Fluid nach Erreichen derselben abläuft. Durch Bohrungen oder Aussparungen in der Welle wird das Fluid dann nach außen abgeleitet.

Das Drehmoment ist im Stillstand am größten und wird mit der Drehzahl linear kleiner, bis es bei der Leerlaufdrehzahl zu null wird. Das ist auch der Grund, warum die höchste Leistung bei halber Leerlaufdrehzahl erreicht wird.

Der Arbeitsmaschine von Tesla wurden Pumpwirkungsgrade von 95 % bis zu 98 % und mehr nachgesagt. Diese Angaben beruhen jedoch eher auf der Legendenbildung um Nikola Tesla als auf physikalischen Realitäten. Systematische Untersuchungen von Warren Rice von der Arizona State University attestierten einen zu erwartenden Wirkungsgrad im Bereich von 40–60 %, mit einem oberen Limit bei 65 %. Praktisch erreicht die Tesla-Turbine nicht den Wirkungsgrad heutiger Axialturbinen.

Die Vorteile der Tesla-Turbine sind ein einfacher Aufbau, große Vibrationsarmut durch Fehlen von Unwuchten – allerdings ist die Maschine nur dann vibrationsfrei, wenn die Rotoren, wie bei beschaufelten Turbinen auch, präzise ausgewuchtet sind – und eine hohe Zyklenfestigkeit (sehr hohe Toleranz gegenüber Last- und Temperaturwechseln).

Die Tesla-Turbine kam im industriellen Rahmen trotzdem bisher kaum über Prototypen und Kleinserien hinaus (z. B. Pumpen für giftige, korrosive oder verunreinigte Abwässer), da durch die erreichten hohen Drehzahlen das Material der Scheiben sich in technisch unbeherrschbarem Maße verzog. Neue Materialien wie Karbonfaser konnten diese Probleme zumindest teilweise lösen.

Beim Einsatz als Pumpe tritt das Fluid im Zentrum aus den Achsenbohrungen aus und wird von den Scheiben zum Rand hin beschleunigt. Das Pumpengehäuse sollte evolvent geformte Abströmöffungen für das Fluid aufweisen, um eine effiziente Ableitung zu gewährleisten.




</doc>
<doc id="5238" url="https://de.wikipedia.org/wiki?curid=5238" title="Tensor">
Tensor

Ein Tensor ist ein mathematisches Objekt aus der linearen Algebra, das besonders im Bereich der Differentialgeometrie Anwendung findet. Der Begriff wurde ursprünglich in der Physik eingeführt und erst später mathematisch präzisiert. In der Differentialgeometrie und den physikalischen Disziplinen werden meist keine Tensoren im Sinn der linearen Algebra betrachtet, sondern es werden Tensorfelder behandelt, die häufig einfach als Tensoren bezeichnet werden. Ein Tensorfeld ist eine Abbildung, die jedem Punkt des Raums einen Tensor zuordnet. Viele physikalische Feldtheorien handeln von Tensorfeldern. Das prominenteste Beispiel ist die allgemeine Relativitätstheorie. Das mathematische Teilgebiet, das sich mit der Untersuchung von Tensorfeldern befasst, heißt Tensoranalysis und ist daher auch heute noch ein wichtiges Werkzeug in den physikalischen und ingenieurwissenschaftlichen Disziplinen.

Bei einem Tensor handelt es sich um eine mathematische Funktion, die eine bestimmte Anzahl von Vektoren auf einen Zahlenwert abbildet. Die Anzahl von Vektoren, die ein Tensor entgegennimmt, wird als Rang oder Stufe des Tensors bezeichnet. Das bestimmende Merkmal von Tensoren ist, dass Tensoren multilinear sind. Sie weisen also für jeden der formula_1 Vektoren, die als Argument übergeben werden, eine lineare Abbildung auf. Die Multilinearität von Tensoren ermöglicht es, den Wert der Funktion als Funktion auf beliebigen Basisvektoren formula_2 auszudrücken. Die Werte, auf die der Tensor die Basisvektoren abbildet, werden als die "Komponenten" des Tensors bezeichnet.

Für einen Tensor formula_3 mit dem Rang 1 gilt aufgrund der Multilinearität von formula_3 für alle formula_5, formula_6 die Gleichung

Gleichermaßen gelten für einen Tensor formula_3 vom Rang 2 für alle formula_9 die Zusammenhänge:

Das Wort "Tensor" (lat. "tendo" „ich spanne“) wurde in den 1840er Jahren von William Rowan Hamilton in die Mathematik eingeführt; er bezeichnete damit den Absolutbetrag seiner Quaternionen, also keinen Tensor im modernen Sinn. James Clerk Maxwell scheint den Spannungstensor, den er aus der Elastizitätstheorie in die Elektrodynamik übertrug, selbst noch nicht so genannt zu haben.

In seiner modernen Bedeutung, als Verallgemeinerung von Skalar, Vektor, Matrix, wird das Wort "Tensor" erstmals von Woldemar Voigt in seinem Buch "Die fundamentalen physikalischen Eigenschaften der Krystalle in elementarer Darstellung" (Leipzig, 1898) eingeführt.

Unter dem Titel "absolute Differentialgeometrie" entwickelten Gregorio Ricci-Curbastro und dessen Schüler Tullio Levi-Civita um 1890 die Tensorrechnung auf riemannschen Mannigfaltigkeiten; einem größeren Fachpublikum machten sie ihre Ergebnisse 1900 mit dem Buch "Calcolo differenziale assoluto" zugänglich, das bald in andere Sprachen übersetzt wurde, und aus dem sich Albert Einstein die mathematischen Grundlagen aneignete, die er zur Formulierung der allgemeinen Relativitätstheorie benötigte. Einstein selbst prägte 1916 den Begriff "Tensoranalysis" und trug mit seiner Theorie maßgeblich dazu bei, den Tensorkalkül bekannt zu machen; er führte überdies die einsteinsche Summenkonvention ein, nach der über doppelt auftretende Indizes unter Weglassung der Summenzeichen summiert wird.

Ausgehend von einem endlichdimensionalen Vektorraum bezeichnet man Skalare als Tensoren vom Typ formula_12, Vektoren als Tensoren vom Typ formula_13 und Kovektoren als Tensoren vom Typ formula_14. Tensoren höherer Stufe definiert man als multilineare Abbildungen mit Tensoren geringerer Stufe als Argumente und Abbildungswerte. So kann etwa ein Tensor vom Typ formula_15 als lineare Abbildung zwischen Vektorräumen oder als bilineare Abbildung mit einem Vektor und einem Kovektor als Argumente aufgefasst werden.

Beispielsweise ist der mechanische Spannungstensor in der Physik ein Tensor zweiter Stufe – eine Zahl (Stärke der Spannung) oder ein Vektor (eine Hauptspannungsrichtung) reichen nicht immer zur Beschreibung des Spannungszustandes eines Körpers aus. Als Tensor vom Typ formula_16 aufgefasst ist er eine lineare Abbildung, die einem Flächenelement (als Vektor) die darauf wirkende Kraft (als Kovektor) zuordnet, oder eine bilineare Abbildung, die einem Flächenelement und einem Verschiebungsvektor die Arbeit zuordnet, die bei der Verschiebung des Flächenstücks unter dem Einfluss der wirkenden Spannung verrichtet wird.

Bezüglich einer fest gewählten Vektorraumbasis erhält man die folgenden Darstellungen der verschiedenen Typen von Tensoren:
Die Anwendung des Spannungstensors auf ein Flächenelement ist dann z. B. durch das Produkt einer Matrix mit einem Spaltenvektor gegeben. Die Koordinaten von Tensoren höherer Stufe können entsprechend in ein höherdimensionales Schema angeordnet werden. So können diese Komponenten eines Tensors anders als die eines Spaltenvektors oder einer Matrix mehr als ein oder zwei Indizes haben. Ein Beispiel für einen Tensor dritter Stufe, der drei Vektoren des formula_17 als Argumente hat, ist die Determinante einer 3×3-Matrix als Funktion der Spalten dieser Matrix. Bezüglich einer Orthonormalbasis wird er durch das Levi-Civita-Symbol formula_18 repräsentiert.

Die Begriffe "ko-" und "kontravariant" beziehen sich (im Zusammenhang mit der Tensorrechnung) auf die Koordinatendarstellungen von Vektoren, Linearformen und Tensoren höherer Stufe. Sie beschreiben, wie sich solche Koordinatendarstellungen bezüglich eines Basiswechsels im zugrundeliegenden Vektorraum verhalten.

Legt man in einem formula_19-dimensionalen Vektorraum formula_20 eine Basis formula_21 fest, so kann jeder Vektor formula_22 dieses Raumes durch ein Zahlentupel formula_23 – seine Koordinaten – mittels formula_24 dargestellt werden. Mittels der einsteinschen Summenkonvention, bei der das Summenzeichen entfällt und trotzdem über doppelt auftretende Indizes summiert wird, lautet die Darstellung formula_25. In diesem Abschnitt wird ab jetzt nur noch diese Konvention verwendet. Geht man zu einer anderen Basis von formula_20 über, so ändert sich der Vektor selbst nicht, aber die Koordinaten der neuen Basis werden andere sein. Ist also die neue Basis durch formula_27 in der alten Basis bestimmt, so ergeben sich die neuen Koordinaten durch Vergleich in
also formula_29 oder

Dreht man zum Beispiel eine orthogonale Basis in einem dreidimensionalen euklidischen Raum formula_20 um formula_32 um die formula_33-Achse, so drehen sich die Koordinatenvektoren im Koordinatenraum formula_34 ebenfalls um die formula_33-Achse, aber in der entgegengesetzten Richtung um formula_36. Dieses der Basistransformation entgegengesetzte Transformationsverhalten nennt man "kontravariant." Oft werden Vektoren zur Abkürzung der Notation mit ihren Koordinatenvektoren identifiziert, sodass Vektoren allgemein als kontravariant bezeichnet werden.

Eine Linearform oder ein "Kovektor" formula_37 ist dagegen eine skalarwertige lineare Abbildung formula_38 auf dem Vektorraum. Man kann ihr als Koordinaten ihre Werte auf den Basisvektoren, formula_39, zuordnen. Die Koordinatenvektoren einer Linearform transformieren sich wie das Basistupel als
weshalb man dieses Transformationsverhalten "kovariant" nennt. Identifiziert man wieder Linearformen mit ihren Koordinatenvektoren, so bezeichnet man auch allgemein Linearformen als kovariant. Hierbei geht, wie bei Vektoren, die zugrundeliegende Basis aus dem Kontext hervor. Man spricht in diesem Kontext auch von Dualvektoren.

Diese Kurzbezeichnung wird auf "Tensorprodukte" ausgedehnt (Symbol formula_41 für die Tensormultiplikation). Faktoren, die "Vektorräume" sind, nennt man kontravariant, Faktoren, die "Dualräume" sind, nennt man kovariant.

Im Folgenden sind alle Vektorräume endlichdimensional.
Mit formula_42 bezeichne man die Menge aller Linearformen aus dem formula_43-Vektorraum formula_44 in den Körper formula_43.
Sind formula_46 Vektorräume über formula_43, so werde der Vektorraum der Multilinearformen formula_48 mit formula_49 bezeichnet.

Ist formula_44 ein formula_43-Vektorraum, so wird mit formula_52 sein Dualraum bezeichnet. Dann ist formula_53 isomorph zum Tensorprodukt

Setze nun für einen fixierten Vektorraum formula_44 mit Dualraum formula_52
mit formula_1 Einträgen von formula_52 und formula_60 Einträgen von formula_44. Dieser Vektorraum realisiert das Tensorprodukt
Elemente dieser Menge heißen Tensoren, kontravariant der Stufe formula_1 und kovariant der Stufe formula_60. Kurz spricht man von Tensoren vom Typ formula_65. Die Summe formula_66 heißt "Stufe" oder "Rang" des Tensors.

Es gibt natürliche Isomorphismen der folgenden Art:
Das heißt, man kann Tensoren der Stufe formula_68 auch induktiv als multilineare Abbildungen zwischen Tensorräumen geringerer Stufe definieren. Dabei hat man für einen Tensor eines bestimmten Typs mehrere äquivalente Möglichkeiten.

In der Physik sind die Vektorräume in der Regel nicht identisch, z. B. kann man einen Geschwindigkeitsvektor und einen Kraftvektor nicht addieren. Man kann jedoch die Richtungen miteinander vergleichen, d. h., die Vektorräume bis auf einen skalaren Faktor miteinander identifizieren. Daher kann die Definition von Tensoren des Typs formula_65 entsprechend angewendet werden. Es sei außerdem erwähnt, dass (dimensionsbehaftete) Skalare in der Physik Elemente aus eindimensionalen Vektorräumen sind und dass Vektorräume mit Skalarprodukt mit ihrem Dualraum identifiziert werden können. Man arbeitet z. B. mit Kraftvektoren, obwohl Kräfte ohne die Verwendung des Skalarprodukts als Kovektoren anzusehen sind.

Als (äußeres) "Tensorprodukt" oder "Tensormultiplikation" bezeichnet man eine Verknüpfung formula_41 zwischen zwei Tensoren. Sei formula_44 ein Vektorraum und seien formula_72 und formula_73 Tensoren. Das (äußere) Tensorprodukt von formula_74 und formula_75 ist der Tensor formula_76, der durch
definiert ist. Hierbei sind die formula_78 und die formula_79.

Im Folgenden seien formula_44 und formula_81 endlichdimensionale Vektorräume.

Sei formula_44 ein Vektorraum über einem Körper formula_43. Dann ist durch
die sogenannte Tensoralgebra definiert.
Mit der Multiplikation, die auf den homogenen Bestandteilen durch das Tensorprodukt gegeben ist, wird formula_117 zu einer unitären assoziativen Algebra.

Sei formula_44 wie oben ein Vektorraum, dann sind die Räume formula_119 ebenfalls Vektorräume. Weiterhin sei formula_44 nun endlichdimensional mit der Basis formula_121. Die duale Basis wird mit formula_122 bezeichnet. Der Raum formula_119 der Tensoren ist dann ebenfalls endlichdimensional und
ist eine Basis dieses Raumes. Das heißt, jedes Element formula_125 kann durch
dargestellt werden. Die Dimension dieses Vektorraums ist formula_127. Wie in jedem endlichdimensionalen Vektorraum reicht es auch im Raum der Tensoren zu sagen, wie eine Funktion auf der Basis operiert.

Da die obige Summendarstellung sehr viel Schreibarbeit mit sich bringt, wird oft die einsteinsche Summenkonvention verwendet. In diesem Fall schreibt man also
Die Koeffizienten formula_129 werden Komponenten des Tensors bezüglich der Basis formula_122 genannt. Oft identifiziert man die Komponenten des Tensors mit dem Tensor an sich. Siehe dafür unter Tensordarstellungen der Physik nach.

Seien formula_131 und formula_132 jeweils unterschiedliche Basen der Vektorräume formula_133. Jeder Vektor, also auch jeder Basisvektor formula_134 kann als Linearkombination der Basisvektoren formula_135 dargestellt werden. Der Basisvektor formula_136 werde dargestellt durch

Die Größen formula_138 bestimmen also die Basistransformation zwischen den Basen formula_139 und formula_140. Das gilt für alle formula_141. Dieses Verfahren wird Basiswechsel genannt.

Ferner seien formula_142 die Komponenten des Tensors formula_3 bezüglich der Basis formula_144. Dann ergibt sich für das Transformationsverhalten der Tensorkomponenten die Gleichung

Es wird in der Regel zwischen der Koordinatendarstellung des Tensors formula_146 und der Transformationsmatrix formula_147 unterschieden. Die Transformationsmatrix formula_148 ist zwar eine indizierte Größe, aber kein Tensor. Im euklidischen Raum sind das Drehmatrizen und in der speziellen Relativitätstheorie z. B. Lorentz-Transformationen, die sich auch als „Drehungen“ in einem vierdimensionalen Minkowskiraum auffassen lassen. Man spricht in diesem Fall auch von Vierertensoren und Vierervektoren.

Mit Hilfe der Komponenten kann ein Tensor bezüglich einer Basis dargestellt werden. Beispielsweise kann ein Tensor formula_3 mit Rang 2 in einem gegebenen Basissystem formula_150 wie folgt als Matrix dargestellt werden:

Dadurch lässt sich der Wert formula_152 im Rahmen des entsprechenden Basissystems mit Hilfe der Matrixmultiplikation berechnen:

Betrachtet man nun konkret den Trägheitstensor formula_154, so kann mit ihm bezüglich eines gewählten Koordinatensystems die Rotationsenergie formula_155 eines starren Körpers mit der Winkelgeschwindigkeit formula_156 wie folgt berechnet werden:

Neben dem Tensorprodukt gibt es für (r,s)-Tensoren weitere wichtige Operationen.

Das innere Produkt eines Vektors formula_158 (bzw. eines (Ko-) Vektors formula_159) mit einem Tensor formula_160 ist der formula_161 (bzw. formula_162)-Tensor, der durch
bzw. durch
definiert ist. Dies bedeutet, dass der formula_65-Tensor formula_166 an einem festen Vektor formula_107 bzw. festen Kovektor formula_168 ausgewertet wird.

Gegeben sei ein (r,s)-Tensor sowie formula_169 und formula_170. Die Tensorverjüngung formula_171 bildet den Tensor
auf den Tensor
ab. Dieser Vorgang heißt Tensorverjüngung oder "Spurbildung." Im Fall von (1,1)-Tensoren entspricht die Tensorverjüngung
unter der Identifizierung formula_175 der Spur eines Endomorphismus.

Mit Hilfe der einsteinschen Summenkonvention kann man die Tensorverjüngung sehr kurz darstellen. Seien beispielsweise formula_176 die Koeffizienten (bzw. Koordinaten) des zweistufigen Tensors formula_3 bezüglich einer gewählten Basis. Will man diesen (1,1)-Tensor verjüngen, so schreibt man oft anstatt formula_178 nur die Koeffizienten formula_179. Die einsteinsche Summenkonvention besagt nun, dass über alle gleichen Indizes summiert wird und somit formula_179 ein Skalar ist, der mit der Spur des Endomorphismus übereinstimmt. Der Ausdruck formula_181 ist hingegen nicht definiert, weil nur über gleiche Indizes summiert wird, wenn einer oben und einer unten steht. Hingegen ist also formula_182 ein Tensor erster Stufe.

Sei formula_183 eine lineare Abbildung zwischen Vektorräumen, die kein Isomorphismus zu sein braucht. Der Rücktransport von formula_184 sei eine Abbildung formula_185, die durch
definiert ist. Dabei ist formula_187 und formula_188.

Sei formula_189 ein Vektorraumisomorphismus. Definiere den Push-Forward von formula_184 durch formula_191 mit
Dabei ist formula_193, formula_194 und formula_195. Mit formula_196 wird der Rücktransport der Linearform formula_197 notiert. Konkret heißt dies formula_198. Analog zum Rücktransport kann man beim Push-Forward auf die Isomorphie von formula_184 verzichten und diese Operation nur für formula_200-Tensoren definieren.

In diesem Abschnitt werden Tensorprodukträume definiert. Diese werden typischerweise in der Algebra betrachtet. Diese Definition ist allgemeiner als die der (r,s)-Tensoren, da hier die Tensorräume aus unterschiedlichen Vektorräumen konstruiert werden können.

Es seien formula_20 und formula_202 Vektorräume über dem Körper formula_43. Sind formula_204 weitere formula_43-Vektorräume, formula_206 eine beliebige bilineare Abbildung und formula_207 eine lineare Abbildung, dann ist auch die Verknüpfung formula_208 eine bilineare Abbildung. Ist also eine bilineare Abbildung gegeben, so kann man daraus auch beliebig viele weitere bilineare Abbildungen konstruieren. Die Frage, die sich ergibt, ist, ob es eine bilineare Abbildung gibt, aus der auf diese Art, durch Verknüpfung mit linearen Abbildungen, "alle" bilinearen Abbildungen auf formula_209 (auf eindeutige Weise) konstruiert werden können. Ein solches universelles Objekt, d. h. die bilineare Abbildung samt ihrem Bildraum, wird als Tensorprodukt von formula_20 und formula_202 bezeichnet.

Definition: Als Tensorprodukt der Vektorräume formula_20 und formula_202, wird jeder formula_43-Vektorraum formula_215 bezeichnet, zu dem es eine bilineare Abbildung formula_216 gibt, die die folgende universelle Eigenschaft erfüllt:

Gibt es einen solchen Vektorraum formula_215, so ist er bis auf Isomorphie eindeutig. Man schreibt formula_224 und formula_225. Die universelle Eigenschaft kann also als formula_226 geschrieben werden. Zur Konstruktion solcher Produkträume sei auf den Artikel Tensorprodukt verwiesen.

In der Mathematik sind Tensoren Elemente von Tensorprodukten.

Es sei formula_43 ein Körper und es seien formula_228 Vektorräume über dem Körper formula_43.

Das "Tensorprodukt" formula_230 von formula_231 ist ein formula_43-Vektorraum, dessen Elemente Summen von Symbolen der Form
sind. Dabei gelten für diese Symbole die folgenden Rechenregeln:

Die Tensoren der Form formula_236 heißen "elementar." Jeder Tensor lässt sich als Summe von elementaren Tensoren schreiben, aber diese Darstellung ist außer in trivialen Fällen nicht eindeutig, wie man an der ersten der beiden Rechenregeln sieht.

Ist formula_237 eine Basis von formula_238 (für formula_239; formula_240), so ist
eine Basis von formula_242 Die Dimension von formula_243 ist also das Produkt der Dimensionen der einzelnen Vektorräume formula_244

Der Dualraum von formula_243 kann mit dem Raum der formula_60-Multilinearformen
identifiziert werden:

Sind alle betrachteten Vektorräume endlichdimensional, so kann man
miteinander identifizieren, d. h., Elemente von formula_256 entsprechen formula_60-Multilinearformen auf formula_258

Als Invarianten eines ein- oder zweistufigen Tensors bezeichnet man Skalare, die sich unter orthogonalen Koordinatentransformationen des Tensors nicht ändern. Für Tensoren erster Stufe führt die Bildung der vom Skalarprodukt induzierten Norm zu einer Invarianten
wobei hier und im Folgenden wieder die einsteinsche Summenkonvention verwendet wird. Für Tensoren zweiter Stufe im dreidimensionalen euklidischen Raum lassen sich im Allgemeinen sechs irreduzible Invarianten (das heißt Invarianten, die nicht durch andere Invarianten ausgedrückt werden können) finden:
Im Falle von symmetrischen Tensoren 2. Stufe (z. B. dem Verzerrungstensor) fallen die Invarianten formula_261 und formula_262 zusammen. Außerdem lässt sich formula_263 über die anderen 3 Invarianten darstellen (ist also nicht mehr irreduzibel). Die Determinante ist auch eine Invariante, sie lässt sich beispielsweise für formula_264-Matrizen über die irreduziblen Invarianten formula_265, formula_266 und formula_267 darstellen als
Für antisymmetrische Tensoren gilt formula_269, formula_270, formula_271 und formula_263 lässt sich wieder auf formula_266 zurückführen. Somit haben im dreidimensionalen euklidischen Raum symmetrische Tensoren 2. Stufe drei irreduzible Invarianten und antisymmetrische Tensoren 2. Stufe eine irreduzible Invariante.

Man kann das Tensorprodukt formula_274 eines Vektorraumes formula_20 mit sich selbst bilden. Ohne weiteres Wissen über den Vektorraum kann ein Automorphismus des Tensorprodukts definiert werden, der darin besteht, in den reinen Produkten formula_276 die Faktoren zu vertauschen:
Da das Quadrat dieser Abbildung die Identität ist, folgt, dass für die Eigenwerte nur die Werte formula_278 in Frage kommen.

Mittels formula_287 können Tensorpotenzen von formula_20 beliebiger Stufe gebildet werden. Entsprechend können weitere paarweise Vertauschungen definiert werden. Nur sind diese nicht mehr voneinander unabhängig. So lässt sich jede Vertauschung der Stellen formula_289 und formula_290 auf Vertauschungen mit der ersten Stelle zurückführen:

Falls die Vektorräume, die man miteinander tensorieren will, eine Topologie besitzen, so ist es wünschenswert, dass ihr Tensorprodukt ebenfalls eine Topologie besitzt. Es gibt natürlich viele Möglichkeiten, eine solche Topologie zu definieren. Das injektive beziehungsweise das projektive Tensorprodukt sind dafür jedoch eine natürliche Wahl.

Ursprünglich wurde der Tensorkalkül nicht in dem modernen hier vorgestellten algebraischen Konzept untersucht. Der Tensorkalkül entstand aus Überlegungen zur Differentialgeometrie. Insbesondere Gregorio Ricci-Curbastro und sein Schüler Tullio Levi-Civita haben ihn entwickelt. Man nennt den Tensorkalkül daher auch Ricci-Kalkül. Albert Einstein griff diesen Kalkül in seiner Relativitätstheorie auf, was ihm große Bekanntheit in der Fachwelt einbrachte. Die damaligen Tensoren werden heute als Tensorfelder bezeichnet und spielen in der Differentialgeometrie auch heute noch eine wichtige Rolle. Im Gegensatz zu Tensoren sind Tensorfelder differenzierbare Abbildungen, die jedem Punkt des zugrundeliegenden (oft gekrümmten) Raums einen Tensor zuordnen.




</doc>
<doc id="5239" url="https://de.wikipedia.org/wiki?curid=5239" title="Thales">
Thales

Thales von Milet ( ("Thalḗs ho Milḗsios")); * wahrscheinlich um 624/23 v. Chr.; † zwischen 548 und 544 v. Chr. war ein vorsokratischer Naturphilosoph, Geometer und Astronom des archaischen Griechenlands.

Thales hat wahrscheinlich keine Schriften hinterlassen. Die Überlieferung fand durch andere Autoren der Antike statt. Da sich schon früh Legenden um ihn gebildet haben, kann man sich auf über ihn bekannte Details meist nicht verlassen. Es lässt sich jedoch ein grobes Bild zeichnen.

Demnach hat er sich in seiner Heimatstadt Milet politisch betätigt und war einer, der für seine große Weisheit bewundert wurde. So erachtete man ihn als einen der Sieben Weisen und als Begründer der griechischen Naturphilosophie, Astronomie und Geometrie.

Thales, dessen Vater und Mutter Examyes und Kleobuline hießen, war in der Stadt Milet an der Westküste Kleinasiens beheimatet. Ob er phönizischer Herkunft war oder nicht, ist umstritten. Er (oder wenigstens seine Mutter) entstammte laut Diogenes Laertios dem hochadeligen Geschlecht der Theliden.

Wahrscheinlich ist Thales, wie durch die "Chronika" Apollodors von Athen (etwa 180–110 v. Chr.) überliefert, in der 39. Olympiade, um das Jahr 624/623 v. Chr. geboren worden und starb im Alter von 78 Jahren in der 58. Olympiade, also zwischen 548 und 544.
Über Thales’ Familiensituation gibt es widersprüchliche Informationen. Möglicherweise hatte er eine Frau und ein Kind, nach anderen Aussagen war er unverheiratet und hat den Sohn seiner Schwester adoptiert.

Diogenes Laertios berichtet, Thales sei von niemandem angeleitet worden, abgesehen von den Ägyptern und den Priestern. Er zitiert Pamphile, Thales habe von den Ägyptern die Geometrie gelernt. Vermutlich hielt er sich eine Zeit lang zu Forschungszwecken in Ägypten (und auch Kreta) auf und lernte dort von Priestern und Astronomen auf den Gebieten der Mathematik und Astronomie.

Es gibt verschiedene Hinweise darauf, dass Thales in Milet einen gewissen politischen Einfluss hatte.

Spätestens seit Platon wurde meist Thales als Erster der Sieben Weisen genannt, zu denen sonst vor allem Staatsmänner wie der berühmte athenische Gesetzgeber Solon zählten.

Es gibt auch Berichte darüber, dass Thales als politischer Berater hervortrat:

Herodot berichtete etwa, Thales habe den Ioniern den guten Rat gegeben, „ein gemeinsames Versammlungshaus zu errichten, und zwar in Teos, denn Teos sei der Mittelpunkt Ioniens, die anderen Städte sollten aber nach wie vor als eigenständige Kommunen gelten […].“ Dies erscheint unplausibel, da dieser zentralistische Gedanke eher in Perikles’ als in Thales’ Zeit gehört.

Außerdem erzählt Diogenes Laertios, er habe den Milesiern abgeraten, ein Bündnis mit Kroisos, dem Lyder­könig einzugehen, was die Milesier angesichts des Sieges Kyros’, des Perser­königs, später gerettet habe. Dies ist ein Widerspruch zu der Anekdote bei Herodot, Thales habe den Fluss Halys umgeleitet, damit Kroisos’ Heer ihn habe überqueren können.

Im Einzelnen mögen die Berichte erdichtet sein. Im Ganzen erscheint es jedoch plausibel, dass Thales eine Rolle in der milesischen Politik spielte.

Wasser war neben anderen Begriffen schon lange vor Thales ein Begriff, der in Kosmogonien des Alten Orients dazu benutzt wurde, die Entstehung der Welt zu erklären. Die Vorstellung eines Urozeans oder Urwassers, die vor der Entstehung unserer Welt existiert haben sollen, findet sich sowohl in Babylonien wie im Alten Ägypten. Im 8. Jahrhundert v. Chr. hat der griechische Dichter Homer über den Flussgott Okeanos geschrieben, er sei der „Ursprung der Götter“ und der „Ursprung von allem.“ Im 7. Jahrhundert v. Chr. schrieb der Dichter Alkman eine Weltentstehungsgeschichte, an deren Anfang die Gewässer stehen. Es kann angenommen werden, dass einige dieser alten Vorstellungen Thales beeinflusst haben. Jedenfalls ist dieser in die Philosophiegeschichte eingegangen, weil er das Wasser als den Anfang oder Urgrund aller Dinge bezeichnet haben soll:

Aristoteles unterscheidet die Ansichten der Vorsokratiker nach Anzahl und Beschaffenheit ihres Ursprungs "(archḗ)". Thales war einer, der nicht mehrere Ursprünge angenommen habe (wie Empedokles, der die vier Ursprünge Feuer, Wasser, Luft und Erde annahm), sondern bloß einen. Und einer, der einen „materiellen“ Ursprung angenommen habe (keinen „immateriellen“, wie etwa das Unbegrenzte seines Schülers Anaximander). Nach Aristoteles war Thales der erste Philosoph, der die Frage nach einem Urgrund aller Dinge stellte. Ähnliches wie Aristoteles berichten auch Hippolyt von Rom und Diogenes Laertios, wobei Hippolyt von Rom im folgenden Zitat auch die Theologie und Astronomie des Thales, sowie eine Anekdote erwähnt:

Aristoteles berichtet, Thales habe die Ruhelage der Erde damit erklärt, dass die Erde auf Wasser schwimme.

Als Grund für die jährliche Überschwemmung des Nils soll Thales die Etesien genannt haben, denn diese Sommerwinde, die der nördlichen Fließrichtung des Nils entgegengesetzt sind, würden das Abfließen ins Mittelmeer verhindern.

Diogenes Laertios hat uns zwei mathematische Erkenntnisse des Thales überliefert. So sage Pamphile, Thales habe „als Erster das rechtwinklige Dreieck in den Kreis eingetragen.“ Üblicherweise wird diese Stelle bei Diogenes Laertios so interpretiert, dass hier der Satz des Thales gemeint ist. Der Satz des Thales ist ein mathematischer Lehrsatz, nach dem ein Dreieck, von dem eine Seite ein Durchmesser seines Umkreises ist, ein rechtwinkliges Dreieck ist. Man kann die Stelle aber auch so interpretieren, dass Thales das zum Kreis flächengleiche rechtwinklige Dreieck gezeichnet habe, dabei ergibt sich 3 als Näherung für die Kreiszahl. Die zweite Stelle lautet: „Nach Hieronymos von Rhodos hat er die Höhe der Pyramiden […] gemessen, da auch unsere Schattenlänge der Körpergröße gleich ist.“ Dieses Messverfahren kann als früher Vorläufer eines späteren Strahlensatzes angesehen werden, der Wahrheitsgehalt der überlieferten Stelle ist jedoch umstritten. So ist das beschriebene Messverfahren nicht zur Messung der Höhe von jeder Pyramide geeignet (einige haben einen zu flachen Steigungs­winkel), bei einer geeigneten stand auch nur bei günstigem Sonnenstand, also an ein oder zwei Tagen im Jahr, die Spitze des Pyramidenschattens genau senkrecht über einer Pyramidenkante, sodass dieser direkt messbar war. Diese Methode war für die Messung von Pyramidenhöhen also kaum praktisch verwendbar und auch nicht besonders eindrucksvoll, denn die Ägypter konnten die Höhe einer Pyramide ohne Schwierigkeit berechnen.

Weitere mathematische Erkenntnisse des Thales gibt Proklos in seinem Kommentar zu Euklids "Elementen" an: „Dass der Kreis durch den Durchmesser halbiert wird, soll zuerst Thales bewiesen haben.“ Weiter schreibt Proklos: „Es wird gesagt, er habe als erster gewusst und gesagt, dass in jedem gleichschenkligen Dreieck die Winkel an der Grundlinie gleich sind, wobei er in altertümlicher Weise die gleichen [ἴσας, d. h. maßgleich] Winkel ähnlich [ὁμοίας, d. h. gestaltgleich] nannte.“ Nach Eudemos soll Thales auch zuerst gefunden haben, dass Scheitelwinkel gleich sind, aber erst Euklid habe einen Beweis für erforderlich gehalten. Eine weitere Stelle lautet: „Eudemos führt in seiner "Geschichte der Geometrie" diesen Satz [den dritten Kongruenzsatz] auf Thales zurück. Er sagt, die Methode, nach der er die Entfernung der Schiffe auf dem Meer errechnete, müsse notwendig auf Verwendung dieses Satzes beruhen.“ Es ist jedoch bis heute unklar, wie Thales dies durchgeführt haben soll, antike Messverfahren waren dafür kaum geeignet. Eudemos nahm zudem nur an, dass Thales diesen Satz kannte.

Herodot berichtet, Thales habe das Jahr einer Verfinsterung korrekt hervorgesagt, welche plötzlich während einer Schlacht zwischen Lydern und Medern eingetreten sei. Häufig wird dies so gedeutet, dass damit eine totale Sonnenfinsternis gemeint ist. Rückwirkend wurde berechnet, dass am ehesten die Sonnenfinsternis vom 28. Mai 585 v. Chr. zu Herodots wenigen Angaben passe. Herodots Zitat lässt jedoch offen, um was für eine Verfinsterung es sich handelte. Totale Sonnenfinsternisse geschehen äußerst selten, eine atmosphärische Verfinsterung ist viel wahrscheinlicher. Mit dem Kenntnisstand zu Thales’ Zeit war es zudem nicht möglich, den Zeitpunkt einer Sonnenfinsternis im Voraus zu berechnen. Wahrscheinlich ist, dass Thales etwas anderes vorhergesagt hat, das nachträglich von Herodot mit der Verfinsterung bei jener Schlacht identifiziert wurde.

Folgendes hat Diogenes Laertios zur Astronomie des Thales überliefert:

Der Mondkrater Thales ist nach ihm benannt.

Auch von einer möglichen Ingenieursleistung des Thales wurde berichtet. So habe er den Fluss Halys zumindest teilweise umgeleitet, damit ihn das Heer des Kroisos habe überqueren können. Herodot bezweifelte dies und ließ das Heer den Fluss auf bestehenden Brücken überqueren.

Bekannt geworden sind vor allem zwei Anekdoten über Thales. Nach der ersten soll er aufgrund seiner astronomischen Kenntnisse eine große Olivenernte prognostiziert und daraufhin gewinnbringend in Ölpressen investiert haben:

Nach der zweiten soll er von einer Magd verspottet worden sein, weil er beim Sterneschauen in einen Brunnen gefallen sei:

Eine häufig rezipierte Anekdote ist, dass Thales einen Dreifuß erhalten habe, welcher als Preis für den weisesten aller Menschen gedacht gewesen sei. Es existieren einige widersprüchliche Fassungen dieser Geschichte.

Fast alle Forscher gehen davon aus, dass Thales kein Schriftsteller war. Was von ihm bekannt ist, wurde durch andere antike Autoren überliefert. Eine der Hauptquellen zu seinem Leben und Werk ist der antike Schriftsteller Diogenes Laertios, der allerdings etwa 800 Jahre nach Thales gelebt hat. Schon dieser war auf manchmal widersprüchliche Quellen angewiesen. Frühere Autoren, die über Thales berichten, sind der Geschichtsschreiber Herodot sowie die Philosophen Platon und vor allem Aristoteles.


Übersichtsdarstellungen in Handbüchern

Gesamtdarstellungen und Untersuchungen

Rezeption



</doc>
<doc id="5250" url="https://de.wikipedia.org/wiki?curid=5250" title="Theodor Zwinger der Ältere">
Theodor Zwinger der Ältere

Theodor Zwinger der Ältere ("auch Theodore Zwinger oder Theodoro Zuingero"; * 2. August 1533 in Basel; † 10. März 1588 ebenda) war ein eidgenössischer Gelehrter, Arzt, Medizinprofessor und Paracelsist.

Zwinger war Sohn des Kürschners "Leonhard Zwinger" und der Christine Herbster, einer Schwester des Druckers Johannes Oporinus, die in zweiter Ehe den Professor Conrad Lycosthenes heiratete. Er studierte ab 1548 an der Universität Basel und anschliessend in Frankreich (wo er mit Petrus Ramus in Kontakt kam) und in Italien (wo er 1559 an der Universität Padua bei Girolamo Mercuriale zum Doktor der Medizin promoviert wurde). Im selben Jahr nach Basel zurückgekehrt, praktizierte er zuerst als Arzt, heiratete und erhielt 1565 eine Professur für Griechisch und 1571 für Ethik. Ab 1580 war er Professor für theoretische Medizin. 1565 und 1572 amtierte er als Rektor der Universität Basel.

Zwinger war einer der ersten Universitätsgelehrten, der die Werke von Paracelsus aufgriff. Er wurde deswegen angefeindet, konnte aber in Basel einen Kreis von Paracelsisten aufbauen. Zu seinen Schülern gehörten auch Heinrich Khunrath und Johann Arndt.

Zwinger ist Herausgeber der Enzyklopädie Theatrum Vitae Humanae (1565, 1571, 1586, 1604). Er konnte bei der Erstellung des Theatrums zurückgreifen auf die Wissenssammlung seines Stiefvaters Conrad Lycosthenes. Sein Methodus Apodemica von 1577 ist die Grundlegung einer Reisekunst nach den Prinzipien einer ramistischen Topik. 

Zwingers Sohn, Jakob Zwinger, führte das "Theatrum" kurzzeitig fort. Von ihm stammt der Pfarrer und Theologieprofessor Theodor Zwinger der Jüngere (1597–1654) ab.





</doc>
<doc id="5253" url="https://de.wikipedia.org/wiki?curid=5253" title="Toxikologie">
Toxikologie

Die Toxikologie () ist die Lehre von den Giftstoffen, den Vergiftungen und deren Behandlung. Sie ist ein interdisziplinäres Fachgebiet, das meist der Pharmakologie angegliedert ist. Ihr Beschäftigungsfeld überschneidet sich dort mit dem der Biochemie, wo die molekularen Grundlagen der Vergiftungen aufgeklärt werden. Daher sind auch viele Chemiker und Biochemiker als Toxikologen tätig.

Das griechische Wort stammt von des Bogenschützen, mit dem ein (Gift-)Pfeil abgeschossen wird. Es bezieht sich ursprünglich auf das Pfeilgift. Die Pfeilspitze wurde zwecks schneller tödlicher Wirkung mit bakteriell verseuchtem Leichengift oder mit toxisch wirkenden Pflanzenstoffen präpariert. Als Pflanzenstoffe dienten solche, die örtliche Entzündungen hervorriefen, das Herz zum Stillstand brachten und die Muskeln oder die Atmung lähmten. Die Toxikologie, in ihrer modernen Form begründet von Mathieu Orfila, ist damit die Lehre von den schädlichen Wirkungen chemischer Stoffe auf lebende Organismen.

Bei der Frage nach der Giftigkeit (Toxizität) eines Stoffes ist in der Regel die Menge bzw. die Konzentration des betreffenden Stoffes wichtig. Manche Substanzen wirken in geringen Mengen günstig auf den Körper, sind aber in höheren Konzentrationen gefährlich. Alle Substanzen sind ab einer bestimmten, von der Verabreichungsart abhängigen, Dosis tödlich.
Eine andere Form dieses Satzes stammt von dem griechischen Arzt Eryximachos um 350 v. Chr.

Die Anwesenheit einer potentiell giftigen Substanz in einem Organismus führt nicht zwingend zu einer Vergiftung. Gleichwohl kann die übermäßige Einnahme eines medizinischen Medikaments eine Vergiftung verursachen. Die Giftigkeit eines Stoffes wird mit der Toxizitätsbestimmung festgestellt. Die Dosis eines bestimmten Stoffes, die für ein Lebewesen tödlich wirkt, ist die letale Dosis.

Eine Ausnahme von dieser Regel stellen die krebserzeugenden und genverändernden Stoffe dar. Bei ihnen kann theoretisch schon ein Molekül ausreichen, um ihre Schadwirkung zu entfalten. Es ist daher umstritten, ob für diese Substanzen eine Wirkungsschwelle angegeben werden sollte. Eine weitere Ausnahme von dieser Regel bilden die Allergene, auch hier reicht theoretisch ein einziges Molekül aus, eine allergische Reaktion auszulösen. Von den allergischen Reaktionen zu trennen sind die stofflichen Unverträglichkeiten (z. B. Lebensmittel oder Medikamente), bei denen das Dosis/Wirkungsprinzip gilt.





</doc>
<doc id="5254" url="https://de.wikipedia.org/wiki?curid=5254" title="Trojaner">
Trojaner

Trojaner bezeichnet:
Die Trojaner bezeichnet:
Siehe auch:


</doc>
<doc id="5255" url="https://de.wikipedia.org/wiki?curid=5255" title="Trojaner (Astronomie)">
Trojaner (Astronomie)

Als Trojaner wurden ursprünglich die zwei Gruppen von Asteroiden bezeichnet, die die Sonne auf der gleichen Bahn wie der Jupiter umkreisen, ihm jedoch auf den Librations- oder Lagrange-Punkten mit einem mittleren Abstand von 60° auf dem Lagrange-Punkt L vorauseilen beziehungsweise nachfolgen (L). Diese sind die Gleichgewichtspunkte und stellen einen Sonderfall des sonst nur schwer lösbaren Dreikörperproblems dar. Das System Sonne – Planet – Trojaner bildet für jeden dieser Kleinkörper ein stabiles Dreikörpersystem. Als man solche Körper auch bei anderen Planeten fand, wurde der Begriff "Trojaner" auf alle vergleichbaren koorbitalen Objekte ausgedehnt.

Die bislang gefundenen Trojaner sind nach den Personen aus Homers "Ilias" benannt: die vorauseilenden Asteroiden (bis auf Hektor) nach den griechischen, die nacheilenden (bis auf (617) Patroclus) nach den trojanischen Helden.
Bei den Planeten wurden bislang nur von Merkur und Saturn noch keine Trojaner entdeckt.

Im Juli 2013 fand man bei der Venus erstmals einen Trojaner, der die Bezeichnung 2013 ND erhielt. Zuvor, seit 2001, fand man bereits die Quasisatelliten (322756) 2001 CK, 2002 VE und 2012 XE. Der unter 100 Meter große 2013 ND wurde als erster echter Venus-Trojaner identifiziert, jedoch wird der Asteroid innerhalb von etwa 500 Jahren seine Trojaner-Bahn wieder verlassen und erneut zu einem Quasisatelliten werden.

Von der Erde ist bislang nur ein Trojaner bekannt, im Juli 2011 wurde der Asteroid 2010 TK als Trojaner identifiziert. Ferner gibt es den Asteroiden 2002 AA, der zwischen den Lagrange-Punkten L und L hin- und herpendelt.
Nach der die Entstehung des Mondes beschreibenden Kollisionstheorie entwickelte sich im L-Punkt der Erdbahn der Protoplanet Theia, der durch eine nachfolgende Kollision mit der Erde für die Entstehung des Mondes sorgte.

1990 wurde auch im Librationspunkt L des Mars ein Trojaner entdeckt, der Eureka getauft wurde. Mittlerweile sind acht weitere Mars-Trojaner bekannt: (101429) 1998 VF, (311999) 2007 NS, 2001 DH47, 2011 SC191, 2011 UN63, 2011 SL25, 2011 UB256 und (121514) 1999 UJ, davon wurde letzterer im L-Punkt entdeckt. Vom Minor Planet Center sind bislang allerdings erst die vier Nummerierten bestätigt. Ausgenommen (101429) 1998 VF und (121514) 1999 UJ haben alle ähnliche Bahnelemente und könnten eine Familie um Eureka bilden.

Bislang sind bei Jupiter im Punkt L 4.188 und im Punkt L 2.268 Trojaner bekannt . Die Größenordnung der Gesamtzahl dürfte mit der des Hauptgürtels vergleichbar sein, wegen der größeren Entfernung ist aber bisher nur ein geringerer Teil als im Hauptgürtel entdeckt worden. Der erste Trojaner, (588) Achilles, wurde 1906 von Max Wolf entdeckt. Der weitaus größte Trojaner dürfte der 1907 entdeckte (624) Hektor sein, ein unregelmäßig geformter Asteroid von 370 mal 195 Kilometer Ausdehnung.

Jupiter hat durch seine dominierende Masse enormen Einfluss auf das äußere Sonnensystem und verursacht zahlreiche Instabilitäten an den Asteroiden und deren Bahnen. Viele Astronomen meinen deshalb, dass auch im Bereich der Trojaner ein steter, langsamer Wechsel im Gang ist.
Ihre heute unerwartet große Zahl – im Jahr 1960 kannte man erst 20 – wird durch gegenseitige Kollisionen erklärt. Weil sie mit Albedos um 0,04 ähnlich dunkel oder dunkelrot sind wie die Asteroiden des äußeren Sonnensystems, stammen viele Trojaner vermutlich von dort, gehörten also einst zu den Transneptunischen Objekten (TNO).

Vereinzelt dürften sich Trojaner durch Bahnstörungen oder Stöße wieder von den Librationspunkten (Lagrange-Punkten) lösen und in Richtung Marsbahn abdriften.

Mitte des Jahres 2013 wurde der bisher einzige Trojaner entdeckt, der auf der Uranus-Bahn um die Sonne kreist. Der Asteroid 2011 QF läuft seinem Planeten auf Librationspunkt L seit mindestens 700.000 Jahren voraus und behält seine Position in etwa für die kommenden 1.000.000 Jahre bei. Danach wird er neuerlich zu einem Zentauren werden. Die Entdeckung des 60 Kilometer durchmessenden Asteroiden aus Eis und Gestein sorgte in Fachkreisen für Überraschung, da man bisher annahm, es könnten wegen des gravitativen Einflusses von Jupiter und Neptun keine Uranus-Trojaner existieren.

Ende 2001 fand man auch 60° vor Neptun einen Trojaner. Mit dem 4-m-Spiegelteleskop am Cerro Tololo aufgenommen, erhielt der 230-km-Körper die Asteroiden-Nummer 2001 QR, war aber erst nach einem Jahr „gesichert“. Er umrundet die Sonne – genau wie Neptun – in 166 Erdjahren. Mittlerweile sind 8 weitere Neptun-Trojaner bekannt: (385571) Otrera, 2005 TN, (385695) 2005 TO, 2006 RJ, 2007 VL sowie 2004 KV, 2008 LC und 2011 HM. Die sechs erstgenannten befinden sich alle auf L, die letzten drei auf L.

Ursprünglich wurde die L-Position auch von 2008 LC angenommen. Am 12. August 2010 gab das Department of Terrestrial Magnetism (DTM) der Carnegie Institution of Washington in Washington D. C. jedoch bekannt, dass sich 2008 LC auf der Lagrange-Position L bewegt. Es ist der erste nachgewiesene Neptun-Trojaner auf dieser Position.

Am 25./26. August 2014 kreuzte die Raumsonde New Horizons die Neptunbahn nicht allzu weit vom Lagrangepunkt L. Für die Beobachtung der Neptun-Trojaner befand sich die Sonde jedoch zu weit von diesen entfernt, siehe beispielsweise 2011 HM102.

Ein Trojaner-Mond ist ein natürlicher Satellit eines Planeten, der sich in einem der Librations- oder Lagrange-Punkte eines größeren Mondes bewegt. Die Trojaner-Monde bilden eine Untergruppe der koorbitalen Monde.

Gegenwärtig sind vier Trojaner-Monde bekannt, die sich alle im Mond­system des Planeten Saturn befinden:

In den L- und L-Punkten der Bahn des Erdmondes wurden sehr schwache Staubwolken gefunden, die (nicht unabhängig bestätigten) Kordylewskischen Wolken, die noch schwächer als der schwache Gegenschein ausgeprägt sind.

Bei dem Jupiter-Trojaner (617) Patroclus, der sich auf dem Lagrange-Punkt L bewegt, wurde am 22. September 2001 erstmals ein Mond eines Trojaners entdeckt und Menoetius benannt. Beide Körper bewegen sich auf Kreisbahnen um den gemeinsamen Schwerpunkt.

Seither wurden drei weitere Monde nachgewiesen, die Trojaner von Jupiter umkreisen, und zwar bei den L-Trojanern (17365) 1978 VF und (29314) Eurydamas sowie bei dem L-Trojaner (624) Hektor.




</doc>
<doc id="5256" url="https://de.wikipedia.org/wiki?curid=5256" title="Tempel der Artemis in Ephesos">
Tempel der Artemis in Ephesos

Der Tempel der Artemis in Ephesos oder kurz das Artemision von Ephesos war der olympischen Gottheit Artemis (Göttin der Jagd, des Waldes und Hüterin der Frauen und Kinder) in ihrer Sonderform als Artemis Ephesia gewidmet. Er soll von der Amazonenkönigin Otrere gegründet worden sein und gehörte als größter Tempelbau zu den „Sieben Weltwundern“ der Antike. Die antike griechische Stadt Ephesos beim heutigen Selçuk an der Südwestküste der heutigen Türkei war für ihren Reichtum berühmt und später als Hauptstadt der römischen Provinz Asia eine der größten Städte der Antike.

Auf Lateinisch hieß der Tempel "Artemisium Ephesi(n)um" oder "templum Dianae Ephesi(n)ae", auf Altgriechisch ("ho naòs tês Artémidos Ephesíês"), ("ho en Ephésô naòs tês Artémidos") oder ("tò Artemísion Ephésion").

Im Zuge der österreichischen Ausgrabungen seit 1895 sind mehrere Nutzungsphasen des Heiligtums und Bauphasen des Artemistempels nachgewiesen worden. Bereits ab der Bronzezeit wurde das Areal kontinuierlich bis in geometrische Zeit genutzt. Die ersten, einfachen Kultbauten, das Kultbild bergende Naiskoi, entstanden in mittelgeometrischer Zeit. Im 8. Jahrhundert v. Chr. errichtete man mit den Tempeln „A“ und „B“, z. T. als Holzkonstruktionen, die frühesten Peripteraltempel Kleinasiens. Angeblich unter dem kaum bekannten Tyrannen Pythagoras wurde im 7. Jahrhundert v. Chr. der archaische Tempel „C“ errichtet, der noch vor Fertigstellung durch Überschwemmung zerstört wurde.

Der Nachfolgetempel „D“ wurde um 550 v. Chr. begonnen. Architekten waren Chersiphron von Knossos und sein Sohn Metagenes. Die Bauarbeiten an diesem Tempel, für den auch der lydische König Kroisos einige Säulen stiftete, dauerten mit 120 Jahren für antike Verhältnisse ungewöhnlich lang. Die Konstruktion war äußerst schwierig, da das Gebäude auf einem Sumpfgelände errichtet wurde. Erdverfärbungen, die bei Tiefgrabungen beobachtet werden konnten, bestätigen die antiken Nachrichten, nach denen bei der Geländevorbereitung für den Tempelbau Holzkohle und Lederabdeckungen zur Sicherung des moorigen Untergrunds eingebracht wurden. Der Architekt Theodoros von Samos, der schon am Rhoikos-Tempel in Samos mitgewirkt hatte, soll an den Fundamentarbeiten beteiligt gewesen sein. Die auffallenden Ähnlichkeiten zwischen dem Artemistempel und dem Heraion von Samos lassen vermuten, dass seine Mitwirkung nicht allein auf die Fundamentarbeiten beschränkt war.

Der aus einem weiß-bläulichen Marmor der Gegend errichtete archaische Tempel „D“ erhob sich auf einem 111,7 Meter × 57,3 Meter großen, nur zweistufigen Unterbau und vereinte eine dipterale äußere Anlage mit einem hypäthralen, das heißt offenen Sekos. 106 ionische Marmorsäulen von rund 19 Metern Höhe umgaben den Sekos. Die Säulenbasen vom ephesischen Typ standen auf etwa 2,30 Meter breiten Plinthen. Der abschließende Torus der Basen unterlag keiner einheitlichen Gestaltung, sondern wies neben einfacher Riefelung auch weich überfallende Blattreihen auf. Ebenso variierte die Gestaltung der Kanneluren, deren Anzahl mal 40 oder 44, aber auch 48 betragen konnte und mal gleich breit von feinen Graten getrennt wurden, mal im Wechsel breite und schmale Kanneluren aufwiesen. Ein Anthemion schmückte den Säulenhals. An Front- und Rückseite waren die Säulen zwei bzw. drei Reihen tief gestaffelt. Die Säulenschäfte im Eingangsbereich waren als "columnae caelatae" mit Reliefs oberhalb der Basen geschmückt. Die wohl ersten Marmorarchitrave der griechischen Baukunst spannten sich von Säule zu Säule und erreichten die größte je von Griechen erreichte Spannweite. Der Mittelarchitrav der Front wog hierbei etwa 24 Tonnen und musste auf die fast 20 Meter hohen Säulen zentimetergenau verlegt werden. Die rund 86 Zentimeter hohe Sima zeigte mythische Kämpfe in flachem Relief, das als endloses Band Darstellungen von Kriegern, Pferden und Wagen aneinander reihte.

Die Rückseite des Tempels wies wahrscheinlich ein geschlossenes Adyton anstelle eines Opisthodoms auf. Die zwei Meter hohe Artemis-Statue im offenen Sekos war aus Rebholz gefertigt und mit Gold und Silber verkleidet. Das Dach der Peristasis war aus Zedernholz gefertigt. Heraklit weihte sein Buch über den Logos im Tempel auf dem Altar der Artemis.

Der Tempel fiel am 21. Juli 356 v. Chr. einer Brandstiftung durch Herostratos zum Opfer. Er beging die Tat aus Geltungssucht – sein Vorhaben, durch das Niederbrennen des Weltwunders berühmt und somit unsterblich zu werden, ist ihm gelungen. Der Sage nach soll in der Nacht des Brandes Alexander der Große, der später auch große finanzielle Hilfe zum Wiederaufbau des Tempels leistete, geboren worden sein, weswegen Artemis, die dessen Geburt in Pella überwachte, ihr eigenes Heiligtum nicht schützen konnte.

Der spätklassische Neubau (Tempel „E“), mit dem man schon bald darauf begann, wurde durch Cheirokrates oder Deinokrates als bauleitenden Architekten ausgeführt. Es sollte der alte Tempel getreulich wiederhergestellt werden, doch kam es zu einigen Änderungen. So wurde auf der Schuttmasse des Vorgängerbaus ein bereits bei Strabon vermerktes größeres Areal – nun 125,67 Meter × 65,05 Meter, 2,7 Meter Höhe – als Unterbau des neuen Tempels angelegt und der Sockel mit seiner nun zehnstufigen Krepis beträchtlich erhöht. Der Artemistempel „E“ hatte laut Plinius 127 Säulen mit einer Höhe von rund 18 Metern und verfügte über ein Steindach. Nach dem Bericht des Plinius soll er über 36 mit Reliefs verzierte Säulen, "columnae caelatae", verfügt haben. Eines dieser Reliefs soll Skopas gearbeitet haben.

Auch bei diesem Tempel dauerte die Bauzeit mit rund hundert Jahren relativ lang. Gleichwohl kamen bei diesem Tempel alle Bauteile dort zu stehen, wo sie bereits der Vorgänger besaß. Säule saß über Säule und Wand saß über Wand. Die Säulen wiesen nun einheitlich 24, durch Stege voneinander abgesetzte Kanneluren auf. Einige der "columnae caelatae" saßen auf großen kubischen, ebenfalls mit Relief verzierten Sockeln, ohne dass man deren Position am Bau genauer bestimmen könnte. Dem Gebälk fehlt wie seinem Vorgänger der in Kleinasien unübliche Fries. Laut Ausweis von Münzdarstellungen müssen nun riesige Giebel, die drei für kultische Zwecke genutzte Türen aufwiesen, die Fronten geschmückt haben.
Die Bauarbeiten wurden anscheinend nach 250 v. Chr. eingestellt, obwohl einige Bauglieder in Rohform belassen wurden. Antipatros von Sidon beschreibt den Tempel in seinen Epigrammen über die Sieben Weltwunder:
In den Blick der Weltgeschichte geriet der Tempel wieder, als etwa 46 v. Chr. Arsinoë IV., die jüngere Schwester Kleopatra VII., ins Exil in den Tempel kam. Da sie jedoch als Blutsverwandte eine potentielle Bedrohung für den Machtanspruch ihrer Schwester darstellte, wurde sie auf deren Initiative und auf Befehl Marcus Antonius' schon 41 v. Chr. wohl auf den Stufen des Tempels selbst hingerichtet; der Hohepriester, der sie bei ihrer Ankunft als „Königin“ tituliert hatte, wurde nur auf Gnadenersuchen verschont. Das Asylrecht kassierte im Übrigen bald darauf Augustus auch de jure.

Als der Apostel Paulus um das Jahr 55 n. Chr. nach Ephesos kam, hatte er angeblich so großen Zulauf, dass viele Anwohner um den Artemiskult und um ihre wirtschaftliche Existenz fürchteten. Der „Aufruhr des Demetrios“ – eines Silberschmieds und Herstellers von Devotionalien – ist im 19. Kapitel der Apostelgeschichte beschrieben und von Goethe im Gedicht „Groß ist die Diana der Epheser“ vom gegenteiligen Standpunkt aus umgedeutet worden.

Während der Regierungszeit des römischen Kaisers Gallienus wurde der Prachtbau im Jahr 268 n. Chr. von den Goten auf einem Kriegszug zerstört, die Reste von den Einwohnern als Baumaterial verwendet. Die Ephesier gaben den Artemiskult jedoch erst im 4. Jahrhundert auf. Heute zeugt eine wiederaufgerichtete Säule von dem einstigen Weltwunder.

Um 430 v. Chr. schrieb Ephesos einen Wettbewerb aus, für das Heiligtum eine Statue einer Amazone zu schaffen:

Neben weiteren aufzählenden Nennungen der Statuen in der antiken Literatur werden noch zwei Details zu ihnen überliefert: Die Amazone des Kresilas war verwundet "(Kresilas fecit … Amazonam volneratam)", und die Amazone des Phidias stützte sich auf eine Lanze und war vorbildlich in der Form ihres Mundes und schönen Nackens. Die Originale sind verloren, allerdings vermitteln römische Kopien einen guten Eindruck von dreien der genannten Amazonen. Denn unter den erhaltenen Statuentypen gehen die Amazonen vom Typ Sosikles, „Mattei“ und „Sciarra“ auf den Wettbewerb zurück. Umstritten ist bis heute die Zuweisung der Amazonentypen an einzelne Künstler, zumal alle drei Typen die Amazone als verwundet darstellen.

Die ersten Ausgrabungen fanden 1863 bis 1869 durch den Engländer John Turtle Wood statt. Im Zuge der Arbeiten im Stadtgelände von Ephesos nimmt das Österreichische Archäologische Institut seit 1895 immer wieder Grabungen auf dem Gelände des Artemisions vor. Gefundene Architekturfragmente und andere Fundgegenstände sowie mehrere Kopien der Artemisstatue, die im Stadtgebiet gefunden wurden, sind im "Saal der Artemis" im Ephesos-Museum in Selçuk ausgestellt, Funde aus den älteren Grabungen befinden sich im Ephesos Museum in Wien.




</doc>
<doc id="5258" url="https://de.wikipedia.org/wiki?curid=5258" title="Trivium">
Trivium

Das Trivium () bezeichnete die drei sprachlichen Fächer der sieben freien Künste, die zusammen mit den mathematischen Fächern des nach Vorläufern in Antike und Frühmittelalter das Grundstudium an den hochmittelalterlichen Universitäten bildeten. Es bestand aus den – durchwegs lateinisch unterrichteten – Fächern:
Einen Eindruck von den konkreten Inhalten dieser Fächer kann man z. B. aus den (um 630 n. Chr. abgeschlossenen) "Etymologiae" des Isidor von Sevilla gewinnen.

Das Wort "Grammatik" stammt von dem griechischen "", „Geschriebenes, Buchstabe“, und bedeutete „Lesen und Schreiben“. Erstmals wurde dieses Wort von Dionysios Thrax [begrifflich bestimmt] als Sprach- und Literaturwissenschaft, seit Priscianus im Wesentlichen wieder eingeschränkt auf die Lehre [...] der Syntax (de constructione)." Beim Lehrfach Grammatik ging es also um das sprachlich korrekte Reden nach den Regeln der lateinischen Sprache.

Das Wort "Dialektik" stammt wie das Wort "Dialog" von dem griechischen ' mit der Grundbedeutung „sich unterhalten“. Dementsprechend bedeutete in der Antike und im Mittelalter "Dialektik" ' „Kunst der (vernünftigen) Gesprächsführung“ zum Zweck, andere von einer Meinung zu überzeugen. Dieser Begriff beinhaltete Methoden der Argumentation sowie den Bereich, der heute mit Logik bezeichnet wird.

Der Begriff „Dialektik“ hat seinen Ursprung in Formen der Klärung von Begriffen und der argumentativen Beweisführung bei Sokrates (sokratische Dialoge) und Platon. Platon bezieht "richtige Worte" „neben definitionstheoretischen Fragen auch (auf) Grundbegriffe der Logik wie Wahrheit und Falschheit, zwingende Folgerung und Widerspruch“. Bei Aristoteles „findet sich die erste systematische Entwicklung formallogischer Fragen“. Er „macht die Trugschlüsse der Sophisten zum zentralen Thema der Dialektik“. Platon hatte Dialektik als die Bemühung gesehen, „einen einsichtigen Gesprächspartner durch begriffliche Klärung von den Erscheinungen zu den Ideen zu führen“.

Somit sind „Logik (und) Rhetorik [...] die wichtigsten Hilfswissenschaften der Dialektik“, um andere von einer Meinung zu überzeugen.

Das Wort Rhetorik stammt aus dem griechischen "" und bedeutete „Redekunst, Eloquenz, Redetechnik“. Dabei ging es um die Art, wie dialektische Argumentationen vorgetragen wurden, um andere zu überzeugen. „Der grundsätzliche Unterschied zwischen Dialektik und Rhetorik liegt [...] im Bereich der Anwendung der Argumentationen“. Die „Dialektik untersucht ihre Gegenstände im fortlaufenden Wechsel von Frage und Antwort“, etwa in der politischen oder philosophischen Argumentation, die „Rhetorik im Zusammenhang der monologischen Rede“. Als Lehrsystem „ist die Rhetorik erstmals im 5. Jh. v. Chr. in Sizilien nachweisbar“. Diese Lehre bezog sich „ausschließlich auf die Gerichtsrede“, die die Rede in Einleitung (Prooimion), Hauptteil (Agon) und Nachwort (Epilogos) gliederte, „wobei im Hauptteil als dem Ort der Beweisführung die Darstellung des Falles vom eigentlichen Beweis auf der Grundlage plausibler Gründe gesondert wurde“.

Von ist das Adjektiv „trivial“ abgeleitet, das sich im bildungsgeschichtlichen Zusammenhang auf die Inhalte des Triviums bezieht, ansonsten aber schon seit der Antike „allgemein zugänglich, allbekannt, gewöhnlich, gemein“ bedeutet.




</doc>
<doc id="5261" url="https://de.wikipedia.org/wiki?curid=5261" title="TeX">
TeX

TeX ([], im deutschsprachigen Raum auch ), in Eigenschreibweise formula_1, ist ein von Donald E. Knuth ab 1977 entwickeltes und 1986 fertiggestelltes Textsatzsystem mit eingebauter Makrosprache (die ebenfalls "TeX" genannt wird). Im engeren Sinn ist TeX ein einzelnes "Programm" (auf einem Rechner eine ausführbare Datei bzw. der zugrundeliegende Quellcode), das einfache Textdateien und Zeichensatzdateien einliest und eine binäre Dokumentdatei (gewissermaßen eines Grafikformats, genauer einer Seitenbeschreibungssprache) erzeugt, die von anderen Programmen ausgedruckt, angezeigt oder anderweitig für eine Ausgabe weiterverarbeitet werden kann; die Funktionalität dieses Programms ist seit 1990 „eingefroren“, und heutzutage wird es vorwiegend von Weiterentwicklungen wie pdfTeX emuliert. In einem weiten Sinne besteht das "Textsatzsystem" TeX aus Weiterentwicklungen dieses Programms, aus Zeichensatzdateien, tausenden Paketen, die als Eingabetexte verwendet werden können, und vielen weiteren Programmen zur Vor-, Nach- und Zwischenbearbeitung (Sortieren von Registereinträgen, Erzeugen des Literaturverzeichnisses aus einer Datenbank, Bildschirmanzeige, …). Der am einfachsten zu erlernende und auch für unterschiedlichste Kombinationen von Anforderungen am besten gepflegte Zugang zu TeX besteht in der Verwendung des Makropakets LaTeX (samt seinen Erweiterungen), sodass für die Masse der Anwender „TeX“ und „LaTeX“ gewissermaßen dasselbe ist.

"TeX" ist eine Abkürzung des griechischen Τέχνη (aus altgr. τέχνη (téchne), Fähigkeit, Kunstfertigkeit, Handwerk). Die Buchstaben stehen für die griechischen Buchstaben Tau, Epsilon und Chi (Τ, e und Χ). In der Darstellung sind diese nicht selten identisch mit lateinischem T, e und X, jedoch unterscheidet sich Letzteres in der Aussprache.

Laut Knuth soll das X als Ach-Laut wie etwa im deutschen Wort "ach" ausgesprochen werden, also: []. Dieser Laut ist nach einem vorderen Vokal wie "e" für viele deutsche Sprecher ungewohnt. Daher hat sich im Deutschen auch die Aussprache [] mit Ich-Laut wie in "Technik" und anderen von Τέχνη abgeleiteten Wörtern eingebürgert.

TeX kann für alle Arten von Texten verwendet werden, vom kurzen Brief bis zu mehrbändigen Büchern, wobei TeX ursprünglich für längere Texte und wissenschaftliche Arbeiten entwickelt wurde. Viele große wissenschaftliche Verlage nutzen es für den Bücherdruck bzw. Werksatz. Eine besondere Stärke ist der mathematische Formelsatz sowie das erzeugte Schriftbild.

Technisch gesehen handelt es sich bei TeX um einen Interpreter, der rund 300 fest eingebaute Befehle (sogenannte "primitives") kennt und einen komplexen Mechanismus zur Definition eigener Makros bereitstellt. Aus entsprechenden Quelldateien erstellt TeX sogenannte DVI-Dokumente, neuere Versionen können auch direkt PDF-Dateien erzeugen. Der Textsatz erfolgt absatzweise, wobei nach einem eigenen Optimierungsverfahren alle Zeilenumbrüche (inklusive Worttrennung) im Absatz gleichzeitig bestimmt werden. Ein einfacher Algorithmus bestimmt danach den optimalen Seitenumbruch. Die Optimierungsverfahren arbeiten mit Bewertungspunkten "(penalties)," anhand derer die möglichen Umbruchpunkte bewertet werden. Zeilen- und Seitenumbruch werden von einer Vielzahl von Parametern, insbesondere Registern und elastischen Längen, gesteuert, die auch im Dokument geändert werden können (genauere Beschreibung erfolgt unten). TeX selbst kommt mit einem Minimum an Ressourcen aus und stellt nur die Grundfunktionen zur Verfügung.

TeX wird durch eine Vielzahl von Softwarepaketen ergänzt, die eine effiziente und komfortable Nutzung von TeX erst ermöglichen. Eine relativ einfache Software-Zusammenstellung, die die wichtigsten allgemeinen Grundbedürfnisse erfüllt, ist das von Donald E. Knuth selbst entwickelte plain TeX, das in Knuths Einführungsbuch "The TeXbook" ausführlich beschrieben ist. Weitaus bekannter und anwenderfreundlicher ist das ursprünglich von Leslie Lamport entwickelte LaTeX. Darauf aufbauend gibt es mehrere Tausend Zusatzpakete für alle erdenklichen Gebiete. Mit dem Paket "hyperref" lässt sich beispielsweise ohne großen Zusatzaufwand ein Hypertext erstellen, der die Navigation im Inhaltsverzeichnis und im Index in einem PDF-Leseprogramm sowie Links ins Internet ermöglicht. Das Programm BibTeX ermöglicht seit langem die Erstellung und Verwaltung von Quellenangaben in TeX-Texten, MakeIndex erzeugt aus „rohen“ Indexeinträgen der Gestalt ("Terminus", "Seite") Seitenzahllisten für die jeweiligen "Terminus"-Einträge. Inzwischen sind moderne utf8-fähige Alternativen zu diesen "Werkzeugen" entwickelt worden.

„Erweiterungen“, „Formate“, Makros

„Erweiterungen von TeX“ sind folgendermaßen zu unterscheiden:


Ein Makro ist gekennzeichnet durch einen gewöhnlich mit einem Backslash (umgekehrten Schrägstrich) beginnenden „Befehl“ und eine damit verbundene Ersetzungsregel. Beispielsweise wird unter "plain TeX" und "LaTeX" codice_1 durch codice_2 ersetzt. Wenn ein Format als „Makrosammlung“ bezeichnet wird, ist damit eine Sammlung entsprechender Ersetzungsregeln gemeint. Erst durch ein solches Format wird TeX für praktische Anwendungen handhabbar. („Lebensnahe“ Beispiele von Makros findet man unter "LaTeX#Aufbau eines Dokuments".)

Ein "TeX-Format" ist typischerweise folgendermaßen gekennzeichnet:

"Erläuterungen:"

ConTeXt

Neben den TeX-Formaten "plain TeX" und "LaTeX" ist auch ConTeXt recht bekannt. Im Vergleich zu LaTeX hat es mehr Funktionen (wie Hypertext-Unterstützung) direkt eingebaut, andererseits sind weniger Zusatzpakete als für LaTeX verfügbar. ConTeXt ist jedoch "kein Beispiel eines mit Knuths Programm TeX" verwendbaren Formats, es funktioniert nur mit „Binärerweiterungen“ von Knuths TeX, siehe "ConTeXt#mkii und mkiv."

Das folgende Beispiel zeigt die Schritte, um in plain TeX Text zu setzen.

Zuerst wird eine Textdatei (zum Beispiel Text.tex) mit dem folgenden Inhalt erstellt:
Hallo
\bye
Der zu setzende Text ist in diesem Fall Hallo.
Die Anweisung \bye ist ein TeX-Befehl, der das Ende der Datei angibt und in der Ausgabe nicht erscheint.

Danach gibt man in einem Kommandozeileninterpreter den Befehl
ein. TeX erzeugt die Datei Text.dvi (dvi steht für „device independent“, also „geräteunabhängig“).

Die Datei Text.dvi kann (etwa mit dem yap-Programm der MiKTeX-Distribution oder mit xdvi unter Unix) auf dem Bildschirm dargestellt werden. Auf dem Bildschirm erscheint eine Druckseite mit dem Wort „Hallo“.

Die dvi-Datei kann direkt vom Anzeigeprogramm aus ausgedruckt werden oder in ein Druckerformat (etwa PostScript, mit dem dvips-Programm) umgewandelt werden.

Die Stärke von TeX liegt darin, dass für viele übliche Dokumente bereits Schablonen vorliegen, die man verwenden kann.
LaTeX ist ein Softwarepaket mit einer Sammlung von Makros, die in der Sprache TeX geschrieben sind.

Knuth begann mit der Entwicklung des TeX-Systems, weil er mit der schlechter werdenden typografischen Qualität seiner Buchreihe The Art of Computer Programming (TAOCP) unzufrieden war:

Band 1 erschien 1968, Band 2 1969. Die Druckvorlagen wurden mit der Monotype-Technik gesetzt. Diese Art des Formelsatzes war aufwendig. Nach Erscheinen von Band 3 1973 verkaufte Knuths Verleger seine Monotype-Maschinen. Die korrigierten Neuauflagen von Band 1 und 3, die 1975 erschienen, mussten in Europa gesetzt werden, wo noch einige Monotype-Systeme in Gebrauch waren.

Die Neuauflage von Band 2 sollte 1976 mit Fotosatz erstellt werden, doch die Qualität der ersten Proben enttäuschte Knuth ("Digital Typography", Kapitel 1).
Er hatte 15 Jahre Arbeit in die Reihe gesteckt und wollte sie nur fortsetzen, wenn die Bücher entsprechend gut gesetzt waren. Im Februar 1977 bot sich ein Ausweg, als Knuth im Rahmen einer Bücherevaluierung die Ausgabe eines digitalen Drucksystems mit 1000 dpi Auflösung vorgelegt bekam. Pat Winston hatte damit ein Buch über künstliche Intelligenz geschrieben. Als Knuth dies klar wurde, unterbrach er die Arbeit an Band 4, von dem er die ersten 100 Seiten fertiggestellt hatte, und entschloss sich, selbst die Programme zu schreiben, die er und sein Verleger brauchten, um Band 2 neu zu setzen.
Das Design von TeX begann am 5. Mai 1977.

Knuth schätzte die notwendige Arbeit auf wenige Monate ein. Im Mai 1977 schrieb er an seinen Verleger, dass er die ersten Vorlagen im Juli fertig haben würde.

Nach vier Jahren des Experimentierens mit einem Xerox-Satzsystem hatte er noch kein Ergebnis erzielt, das den Fotosatz übertraf. Knuth gab nicht auf und traf bekannte Schriftdesigner, unter anderem Hermann Zapf, von denen er lernte. Nach fünf weiteren Jahren Arbeit hatte er einen Stand erzielt, mit dem er zufrieden war. Die Fertigstellung von TeX wurde am 21. Mai 1986 im Computer Museum, Boston, Massachusetts gefeiert.

In den Büchern der TAOCP-Reihe wollte Knuth bewusst einen Teil des Wissens der Informatik festhalten, von dem er annahm, dass es bereits eine solche Entwicklungsreife erlangt habe, dass dieses Wissen auch in hundert Jahren noch ähnlich dargestellt werde. Daher war es für ihn sehr ärgerlich, dass die typografische Qualität mit den damaligen Techniken von Auflage zu Auflage nachließ. Dass das TeX-Projekt dann fast zehn Jahre seiner Zeit in Anspruch nehmen würde, war nicht geplant.

Zu den Programmen des TeX-Systems und ihrer Benutzung verfasste Knuth eine fünfbändige Reihe:

Versionsnummer

Die Versionsnummer von TeX nähert sich der Kreiszahl formula_2 an, indem bei jeder Fehlerkorrektur eine Ziffer angehängt wird. Die aktuelle Versionsnummer ist 3.14159265. Eine Versionsnummer 4 wird es nach Aussage Knuths nicht geben, da er es – obwohl er selbst schon einige Vorschläge für zusätzliche Funktionen für TeX gemacht hat – als wichtiger erachtet, ein stabil laufendes System zu haben, das auch in 100 Jahren noch dieselben Ausgaben liefert wie heute. Dies hinderte Knuth allerdings nicht daran, anlässlich des formula_3 (also des 32.) Geburtstages von TeX in einer satirischen Rede vor der TeX Users Group das Erscheinen einer neuen Version mit dem Namen „i-TeX*“ zu kolportieren.

Weiterhin hat Knuth verfügt, dass TeX nach seinem Tode von niemandem weiterentwickelt werden soll. Außerdem solle die Versionsnummer zu formula_2 geändert werden, und alle Programmfehler, die zum Zeitpunkt seines Todes noch vorhanden sein würden, sollten als Features anerkannt werden. Analog nähert sich die Versionsnummer von METAFONT der eulerschen Zahl e.

Der von TeX generierte Schriftsatz wies einige Aspekte auf, die zur Zeit des Erscheinens von TeX neuartig oder in anderen Textsatzsystemen von niedrigerer Qualität waren. Einigen dieser Innovationen liegen interessante Algorithmen zu Grunde, die Thema mehrerer Abschlussarbeiten von Knuths Studenten wurden. Bis heute einzigartig sind die Regeln für Abstände in mathematischen Formeln, andere von Knuths Entdeckungen finden inzwischen in verschiedenen Schriftsatzsystemen Verwendung.

Da das Hauptaugenmerk Knuths bei der Entwicklung von TeX auf hochqualitativem Textsatz für sein Buch "The Art of Computer Programming" lag, legte er viel Wert auf gut funktionierende Regeln für die Wahl von Abständen in mathematischen Formeln. Als Grundlage verwendete er Werke, die er für Beispielexemplare hochqualitativer mathematischer Typografie hielt: die Bücher, die bei Addison-Wesley, dem Herausgeber von "The Art of Computer Programming," gesetzt wurden – besonders die Arbeit von Hans Wolf –, Ausgaben der mathematischen Fachzeitschrift "Acta Mathematica" um das Jahr 1910 und ein Exemplar der niederländischen Fachzeitschrift "Indigationes Mathematicae". Aus der genauen Analyse des Textsatzes in diesen Werken entwickelte Knuth einen Regelsatz für das Setzen von Abständen in TeX. Während TeX nur einige grundlegende Regeln für die Abstandhaltung bereitstellt, hängen die exakten Parameter von der verwendeten Schriftart ab, die zum Formelsatz verwendet wird. Die Regeln für Knuths Computer-Modern-Schriftarten beispielsweise wurden in jahrelanger Arbeit feinabgestimmt und sind jetzt eingefroren. Als er aber andere Schriftarten wie AMS Euler zum ersten Mal verwendete, mussten neue Abstandhaltungsparameter definiert werden.

Im Vergleich zu manuellem Schriftsatz lässt sich das Problem des Setzens im Blocksatz mit einem automatisierten System wie TeX prinzipiell leicht lösen. Ein solches System kann automatisch die Wortabstände in einer Zeile anpassen, wenn definiert wurde, an welcher Stelle ein Zeilenumbruch sinnvoll ist. Das eigentliche Problem besteht also darin, die Zeilenumbruchstellen zu finden, die den besten Gesamteindruck erzeugen. Viele Zeilenumbruchsalgorithmen verwenden einen sogenannten "First-Fit"-Ansatz: Die Zeilenumbrüche werden Zeile für Zeile festgelegt, und es wird kein Zeilenumbruchspunkt mehr geändert, nachdem er einmal festgelegt wurde. Ein solches System kann einen Zeilenumbruchspunkt nicht danach beurteilen, welchen Effekt der Umbruch auf darauf folgende Zeilen hat. Im Gegensatz zu solchen Algorithmen zieht der "Total-Fit"-Algorithmus, den Knuth und Michael Plass für TeX entwickelten, alle möglichen Zeilenumbruchspunkte in einem Absatz in Betracht und findet die Kombination derer, die insgesamt das bestaussehende Arrangement hervorbringen.

Formal assoziiert der Algorithmus einen "badness" genannten Wert mit jedem möglichen Zeilenumbruch. Die "badness" wird erhöht, wenn die Wortabstände zu stark gestreckt oder gestaucht werden müssen, um eine Zeile in die richtige Länge zu bringen. Strafpunkte werden hinzuaddiert, wenn ein Zeilenumbruch besonders unerwünscht ist, beispielsweise wenn ein Wort getrennt werden muss, wenn zwei Zeilen hintereinander mit einer Worttrennung enden oder wenn eine sehr gestreckte Zeile unmittelbar auf eine sehr gestauchte folgt. Der Algorithmus findet dann den Satz von Umbruchpunkten, der eine minimale Summe der Quadrate der "Badness"-Werte aufweist. Enthält ein Absatz "n" mögliche Umbruchpunkte, so müssten mit einem naiven Algorithmus 2 mögliche Zeilenumbruchskombinationen überprüft werden. Mit Methoden der dynamischen Programmierung kann die Komplexität auf "O"("n") gesenkt werden. Weitere Vereinfachungen (beispielsweise extrem unwahrscheinliche Umbruchstellen gar nicht zu überprüfen – z. B. eine Worttrennung des ersten Wortes eines Absatzes) führen meist zu einer Laufzeit in der Ordnung "n". Im Allgemeinen zeigte Michael Plass, dass das Problem, den besten Seitenumbruch zu finden, aufgrund der erhöhten Komplexität durch die Platzierung von Abbildungen und Tabellen NP-vollständig sein kann. Ein ähnlicher Algorithmus wird verwendet, um Seitenumbrüche in Absätzen so zu platzieren, dass Hurenkinder und Schusterjungen verhindert werden.

Der Zeilenumbruchsalgorithmus von TeX wurde von mehreren anderen Programmen aufgenommen, zum Beispiel von Adobe InDesign und dem GNU-Kommandozeilenwerkzeug fmt.

Wenn kein passender Zeilenumbruch gefunden werden kann, versucht TeX, ein Wort zu trennen. Die ursprüngliche Version von TeX verwendete einen Worttrennungsalgorithmus, der Wortpräfixe und -suffixe erkannte, bei Bedarf entfernte und Bindestriche zwischen den zwei Konsonanten einer Buchstabenkombination Vokal–Konsonant–Konsonant–Vokal einfügte, was in der englischen Sprache meistens möglich ist. TeX82 verwendet einen neuen Worttrennungsalgorithmus, der von Frank Liang 1983 entwickelt wurde und Umbruchpunkten in Wörtern Prioritäten zuordnet. Zunächst wird eine Liste von Worttrennungsmustern aus einem großen Korpus getrennter Wörter generiert (solche Korpora enthalten 50.000 Wörter und mehr). Wenn TeX dann beispielsweise eine akzeptable Worttrennungsposition im Wort "encyclopedia" finden muss, erzeugt es eine Liste der Teilwörter von „.encyclopedia.“ (der Punkt ist ein Sonderzeichen, das Anfang und Ende des Wortes markiert). Diese Liste umfasst alle Teilwörter der Länge 1 ("., e, n, c, y" usw.), der Länge 2 (".e, en, nc" usw.), bis zur Länge 14, dem Wort inklusive der Punkte selbst. TeX sucht in seiner Liste von Trennungsmustern solche Teilwörter heraus, für die es die Erwünschtheit einer Trennung errechnet hat. In unserem Fall werden 11 solcher Muster gefunden:

Für jede Wortposition errechnet TeX nun den Maximalwert aus allen passenden Mustern, was hier

ergibt. Die ungeraden Zahlen markieren mögliche Trennungspositionen; hier ist das Ergebnis also "en-cy-clo-pe-di-a". Dieses auf Teilwörtern basierende System erlaubt die Definition sehr allgemeiner Muster wie ia mit niedrigen Umbruchzahlen (gerade oder ungerade), die, wenn notwendig, durch spezifischere (längere) Muster wie dia übergangen werden.

Durch geschickte Parameterwahl ist es möglich, Trennmuster so zu erzeugen, dass


Die Nachteile des Liangschen Algorithmus zeigen sich in Sprachen wie Deutsch, wo es eine praktisch beliebige Anzahl von Komposita gibt, die nicht bei Erzeugung der Trennmuster berücksichtigt werden können und daher oft falsch getrennt werden.

Entwicklung abgeschlossen

Die Definition des Befehlsumfangs von TeX steht seit 1990 fest. Knuths Anliegen war es, ein qualitativ hochstehendes Programm zu schaffen; das Programm wird als abgeschlossen betrachtet, es finden nur noch Fehlerkorrekturen statt. Knuth legt Wert darauf, dass echte Weiterentwicklungen (etwa mit einem erweiterten Satz primitiver Befehle) nicht als „TeX“ bezeichnet werden (siehe "Lizenz").

Nachfolgegeneration

In gängigen Distributionen (siehe unten) können mit den Kommandozeilenbefehlen codice_12 bzw. codice_13 direkt PDF-Dateien erzeugt werden:
Grafische TeX-Editoren (siehe unten) bieten Frontends für solche Kommandozeilen. Dazu wird pdfTeX verwendet. Dieses Programm hat einen umfangreicheren Befehlssatz als TeX; die ursprünglichen TeX-Befehle sollen ebenso wirken wie mit TeX, so dass das Makropaket LaTeX und seine Erweiterungen auch mit pdfTeX verwendet werden können (codice_13). Ebenso kann Knuths Makropaket plain TeX mit pdfTeX verwendet werden (codice_12), und "The TeXbook" kann auch hierfür als Referenz herangezogen werden (trifft jedoch hinsichtlich der Zahl der Register nicht mehr zu). – pdfTeX wird heute in gängigen Distributionen auch zur Erzeugung von TeXs ursprünglichem Ausgabeformat DVI verwendet (z. B. per codice_16), TeX kommt nur noch für den Kommandozeilenbefehl codice_17 zum Einsatz. Auf diese Weise hat man Zugriff auf den Befehlssatz von ε-TeX, einer Erweiterung von TeX, die pdfTeX vorherging und z. B. eine größere Zahl von Registern als TeX zur Verfügung stellt. Tatsächlich können heute mit pdfTeX noch weitere, früher selbstständige Weiterentwicklungen von TeX emuliert werden. Unter zusätzlicher Verwendung des Pakets "microtype" ist außerdem eine Verfeinerung von TeXs Zeilenumbruchalgorithmus zugänglich.

Betriebssystemzeichensätze und Unicode

TeX verwendete zunächst ein ganz eigenes System von Zeichensätzen (mit Knuths METAFONT erzeugt), konnte dann auch mit PostScript-Schriftarten umgehen. Letzteres ist schon für Autoren, die das System nicht professionell, sondern für eigene Texte anwenden wollen, anfangs nicht einfach. Die von einem Office-Textverarbeitungsprogramm auf einem Rechner angebotenen Zeichensätze sind nicht zugänglich. Auch die Verwendung von nicht-angelsächsischen Glyphen und noch mehr von nicht-lateinischen Schriftsystemen ist aus der Perspektive von Office-Anwendern etwas unzugänglich. Dies gilt besonders bei Beschränkung auf das Makropaket plain TeX, während LaTeX standardmäßig gut dokumentierte Schnittstellen zur Überwindung dieser Hindernisse bereitstellt. TeXs erste Zeichensätze stellten weniger als 128 Glyphen zur Verfügung, mit LaTeX sind 8-bit-Zeichensätze der üblichen Codierungen inklusive UTF-8 leicht verfügbar. pdfTeX stellt hinsichtlich dieser beiden Punkte keine Verbesserung dar. Natürlicher ist der Zugang zu Schriftarten von außerhalb der TeX-Welt mit XeTeX und LuaTeX, die OpenType verfügbar machen und per Voreinstellung in Unicode kodierte Eingabedateien unterstützen. "The TeXbook" trifft auf XeTeX und LuaTeX nicht mehr völlig zu.

„TeX“ als „Oberbegriff“

Obwohl nun das von Knuth entwickelte Programm als solches wohl kaum noch verwendet wird, gibt es weiterhin „TeX-Distributionen“ und „TeX-Benutzergruppen“ (wie folgt), im Wesentlichen wird auch das Arbeiten mit den aktuellen Weiterentwicklungen von TeX als Arbeit mit TeX betrachtet und entsprechend bezeichnet. Unter Autoren, die praktisch nur lateinische Schriften verwenden, ist XeTeX wohl noch kaum verbreitet, und reinen Autoren (im Gegensatz zu „TeX-Experten“) ist von diesen unterschiedlichen „Engines“ auch nichts bekannt. Da reine Anwender praktisch ausschließlich Makros aus den Formaten LaTeX und plain TeX und ihren Erweiterungen (und nicht primitive Befehle) verwenden, sind sie von den Unterschieden kaum betroffen. Dass bestimmte Makropakete nur mit Weiterentwicklungen von TeX (vor allem ε-TeX) und nicht mit Knuth’s Programm funktionieren, ist ebenfalls von geringer Relevanz, da Letztere (vor allem pdfTeX) mit gängigen Distributionen geliefert werden und ihre Verwendung (auch in TeX-Editoren) voreingestellt ist.

Rund um TeX haben sich schon sehr früh Benutzergruppen organisiert, als erstes die TeX Users Group (TUG), die international agiert. Später kamen sprachbezogene und regionale Benutzergruppen hinzu wie die Deutschsprachige Anwendervereinigung TeX (DANTE) für den deutschen Sprachraum und GUTenberg für den französischen Sprachraum. Insgesamt gibt es zurzeit etwa zwei Dutzend TeX-Benutzergruppen, die untereinander zusammenarbeiten.

Es gibt viele verschiedene Distributionen von TeX, die untereinander vollständig kompatibel sein sollten. Sie enthalten alle zur Arbeit mit TeX nötigen Programme. Auch optionale Ergänzungen wie LaTeX sind enthalten.

Der Verzeichnisbaum "texmf," der durch die verschiedenen aktuellen Installationen erzeugt wird, ist als „TeX Directory Structure“ standardisiert.

Noch umfangreicher als die genannten "Distributionen" ist die Sammlung von TeX-Versionen und -Erweiterungen im Comprehensive TeX Archive Network (CTAN). So ist TeX Live auf als strikt frei lizenzierte TeX-Zusatzpakete beschränkt, so dass selbst Pakete nicht eingeschlossen sind, die „manuell“ aus dem CTAN heruntergeladen werden können, aber keine hinreichend präzise Lizenz beinhalten. Die andere aktuell relevante Distribution "MiKTeX" ist dagegen auf mit dem Betriebssystem Microsoft Windows lauffähige Software beschränkt, während aus dem CTAN auch Pakete heruntergeladen werden können, die ausschließlich auf Unix-artigen Betriebssystemen oder gar nur auf „historischen“/„exotischen“ Betriebssystemen lauffähig sind.

Der Quelltext von TeX steht offen zur Verfügung, er kann und soll nach dem Willen von Donald E. Knuth weiterentwickelt werden. Die Ausgabe einer bestimmten Version von TeX soll aber auf allen Plattformen identisch sein. Weiterentwicklungen von anderen Autoren dürfen daher nicht selbst den Namen TeX tragen, sondern müssen anders benannt werden. Obwohl es bisweilen heißt, TeX und LaTeX seien von Knuth bzw. von Leslie Lamport unter "Public Domain" gestellt worden, ist das also nicht zutreffend.

Die Lizenzbedingungen von TeX dürfen nicht mit denjenigen von LaTeX verwechselt werden – obwohl jene ähnlich sind. Auch die LaTeX Project Public License (LPPL) erlaubt das Kopieren und Verändern des Quelltexts von LaTeX-Paketen nur, solange die geänderten Dateien unter einem anderen Namen verteilt werden als die ursprünglichen Dateien. Die LPPL gilt aber nicht für TeX selbst. Sie ist nur aus der Lizenz, die Knuth für TeX vorgegeben hatte, hervorgegangen.

Die Arbeit von Autoren oder ihren Schreibkräften mit TeX verläuft so (man betrachte zunächst das kurze #Beispiel): Man gibt eine Passage Quellcode (Text, Befehle zur Formatierung und für spezielle Zeichen wie mathematische Symbole) ein (und speichert die Textdatei ab), lässt dann das TeX-Programm (nicht anders ist die Arbeit etwa mit pdfTeX) mit der neuen Textdateiversion laufen, schließlich schaut man sich das Ergebnis mit einem Previewer (mit MiKTeX etwa "yap", mit Linux "Evince") an (eventuell erzeugt man noch mit einem weiteren Werkzeug eine PostScript- oder PDF-Version aus der DVI-Datei). Man hat zwei Fenster: eines für die Texteingabe und eines für die Vorschau. Dies ist ganz anders als bei der Arbeit mit einem Office-Programm, wo man Textänderungen direkt im Vorschaufenster vornimmt. Durch die TeX-Befehle unterscheidet sich das Erscheinungsbild des Codes stark von dem des erzeugten Dokuments, was für Bearbeiter eine gewisse Herausforderung darstellt (vgl. "LaTeX#Kein WYSIWYG").

Immerhin ist LyX ein Editor "für LaTeX", mit dem man Textänderungen in einem Fenster vornimmt, das keine TeX-Befehle zeigt, auch keine echte Vorschau bietet, immerhin eine intuitiv erfassbare Andeutung des Ergebnisses.

Die Arbeitsumgebung mit separaten Fenstern für Ein- und Ausgabe kann unterschiedlich aufwändig gestaltet sein:

Eine ausführlichere Darstellung von Entwicklungsumgebungen für "LaTeX" findet sich unter "LaTeX#Entwicklungsumgebungen."

TeX kann mit Hilfe von TeXML ein anspruchsvolles Layout von in einem XML-Format vorliegenden Daten erzeugen. Damit ist TeX eine Alternative zu XSL-FO.

Einführungen

Andere



</doc>
<doc id="5262" url="https://de.wikipedia.org/wiki?curid=5262" title="Trivialliteratur">
Trivialliteratur

Trivialliteratur (lat. trivialis, „allgemein zugänglich“, „gewöhnlich“) ist eine (schöne) Literatur, die im Gegensatz zur Hochliteratur als einfach, allgemein verständlich und leicht zu erfassen gewertet wird. Der Begriff hat pejorativen Charakter. Alternative Begriffe sind "Schemaliteratur" und "Massenliteratur". Neben dieser Zweiteilung steht ein Dreischichtenmodell, das "Hoch-, Unterhaltungs-," und "Trivialliteratur" unterscheidet.

Der Begriff "Trivialliteratur" wurde 1807 von Joseph Görres in seiner Publikation "Die teutschen Volksbücher" geprägt, in der er vierzig von ihnen nacherzählte. Während der Begriff in dieser Bedeutung nichts Abwertendes hatte, lag das Unbehagen an dem später von dem Begriff abgedeckten Phänomen jedoch bereits vor. So berichtete Heinrich von Kleist am 14. September 1800 in einem Brief an Wilhelmine von Zenge von einem Besuch in einer Würzburger Leihbibliothek, in der er vergeblich nach den Klassikern Ausschau hielt. Auf die Frage, was für Bücher denn in den Regalen ständen, antwortete ihm der Besitzer: „Rittergeschichten, lauter Rittergeschichten, rechts die Rittergeschichten mit Gespenstern, links ohne Gespenster, nach Belieben.“

Seine heutige Bedeutung erhielt der Begriff Trivialliteratur durch Marianne Thalmanns 1923 veröffentlichte Dissertation "Der Trivialroman des 18. Jahrhunderts und der romantische Roman". Trivialliteratur ist demnach eine Form literarischer Unterhaltung. Mit dem Begriff wird seit den 1920er Jahren der Bereich der populären, häufig als minderwertig angesehenen Massenliteratur bezeichnet. In einer groben Aufteilung von Literatur in die drei Felder Dichtung bzw. Hochliteratur, Unterhaltungsliteratur und Trivialliteratur wird Letztere als die mit dem geringsten literarischen Anspruch – bis hin zu Kitsch- und Schundliteratur – gewertet. Allerdings sind die Übergänge zwischen diesen Wertungen fließend und Zuordnungen werden je nach Betrachtung der einen oder anderen Stufe erfolgen können.

Trivialliteratur widmet sich meist großen Themen wie Liebe, Tod, Abenteuer, Verbrechen, Familie oder Krieg, behandelt diese aber in einer vereinfachenden, klischeehaften und oftmals die Vorstellung einer „heilen Welt“ verklärenden Weise. Triviale Texte sind in Sprache, Verständlichkeit und Emotionalität so strukturiert, dass sie den Erwartungen einer möglichst großen Leserschaft gerecht werden (indem sie dieser eine oftmals schöne, durchweg gerechte Welt mit klaren Unterscheidungen zwischen Gut und Böse vermitteln). Wesentliches Merkmal der Trivialliteratur ist – anders als die eher auf kritische Reflexion gängiger Vorstellungen und etablierter Denkweisenden setzende Hochliteratur – den Erwartungshorizont des Lesers nicht zu durchbrechen, was einer Bestätigung (Affirmation) seiner bestehender Meinungen, Gesellschaftsbilder usw. gleichkommt. Das hat zur Bezeichnung "affirmative Literatur" (im Gegensatz zu "kritischer Literatur") als einem weiteren Synonym für Trivialliteratur Anlass gegeben. In dieser Hinsicht werden als "trivial" auch solche literarischen Texte bezeichnet werden können, die gewöhnliche bis primitive Erwartungshaltungen bedienen. Hierzu zählt etwa das wiederholende Ausbreiten emotionaler und sensationeller Inhalte bis hin zu verschiedenen Formen des Voyeurismus. 

Trivialliteratur erfüllt typischerweise Erwartungshaltungen der Leserschaft. Weil diese aber durchaus inhomogen aus den Erfahrungen und Erwartungen einzelner Leser erwachsen und von deren individuellen und gesellschaftlichen Kontexten geprägt sind, wird die Analyse und Beschreibung der sozio-literarischen Bedingungen sowohl für die Möglichkeit der Verbreitung von trivialer Literatur generell als auch für die literatur-ästhetische Wertung des einzelnen Textes weiterhin die Grundlage der Trivialliteraturforschung bilden. 

Der Begriff "Trivialliteratur" war und ist Gegenstand literaturwissenschaftlicher Forschung und Diskussion, da er Texte vorab als von vermeintlich niedrigem ästhetischen Niveau einstuft und teilweise als Oberbegriff von Schundliteratur oder Kitsch fungiert. 

Ein eher strukturanalytischer Ansatz, welcher den stark schematisierten Charakter trivialer Texte und auch Textausschnitte hervorhebt und zudem die abwertende Konnotation des Begriffes vermeidet, charakterisiert Trivialliteratur als "Schemaliteratur". Sie verfügt danach über folgende Merkmale: schematischer Spannungsaufbau, melodramatische und sentimentale Handlungen, Schwarz-Weiß-Zeichnung bei Charakteren, Vermittlung eindeutiger moralischer Ansichten und Vortäuschung eines scheinbar klaren Weltbildes. Ihre starke Bindung an fixe Schemata geht einher mit ihrer Tendenz zur seriellen Erscheinungsweise, wie zum Beispiel in Fortsetzungsromanen, Mainstream-Comic-Heften oder Roman-Heften und -Reihen. 

All diese Werke erfüllen kollektive Leserbedürfnisse, wonach Texte dieser Art gewisse gemeinsame Grundmuster umzusetzen und zu erfüllen vermögen. Dadurch setzt Schemaliteratur sich von den zeitgeschichtlich etablierten, gleichwohl aber epochal veränderlichen Normen „hochliterarischer“ Systeme ab, die durch Intertextualität, Originalität, Singularität und Innovation geprägt erscheinen.

Auch wenn der eigentliche Ursprung der Trivialliteratur im 18. Jahrhundert liegt, können erste Formen bereits in den im 15. Jahrhundert weit verbreiteten Einblattdrucken gesehen werden. In ihnen ging es um meist religiöse Inhalte, die für jedermann klar, anschaulich und deutlich zu vermitteln waren. Im Laufe der Zeit weitete sich das inhaltliche Spektrum der Blätter, wobei das Sensationelle zunehmend an Bedeutung gewann. Viele Heiligendarstellungen wurden mit grausamen, brutalen Geschichten überhöht; Berichte von Naturkatastrophen, von Kriegsgeschehen und Schlachten oder von der Ausbreitung von Seuchen wurden häufige Themen. So vermischten sich Information mit Sensation und Unterhaltung, wobei die nachrichtliche Information zunächst nicht im Vordergrund stand. Diese Einblattdrucke gelten als Vorläufer der Groschenheftliteratur des 20. Jahrhunderts. Ebenfalls im 15. Jahrhundert sind Einblattdruck-Wandkalender entstanden, die sich im 16. Jahrhundert zu Kalenderheften weiterentwickelten. Ab dem 17. Jahrhundert wurden sie immer häufiger mit belehrenden und unterhaltsamen Texten wie zum Beispiel Sentenzen und Anekdoten, Rätsel oder Horoskope versehen, so dass die Kalender zu einem Unterhaltungsmedium wurden, das anspruchslose Lesebedürfnisse befriedigte. Im Zuge der Aufklärung im 18. Jahrhundert dienten Kalender jedoch zunehmend auch pädagogischen Zwecken und wurden zu Vermittlern vernünftigen und sinnvollen Wissens. 

Diese Vorläufer weisen viele Parallelen zu den ab dem 18. Jahrhundert als Trivialliteratur bezeichneten Werken auf, für deren Inhalte die beispielhafte Gegenüberstellungen von „gut“ und „böse“, „schön“ und „hässlich“ oder „klug“ und „dumm“ zu einem Prinzip wurde. Dessen klare Dualität ermöglichte die verdeutlichende Darstellung ganz bestimmter Wertvorstellungen und kam damit zugleich dem Verlangen der Leserschaft nach Information und Sinngebung entgegen.

Die einsetzende Entwicklung und rasche Verbreitung der Trivialliteratur wurde dabei durch den enormen Anstieg der Lesefähigkeit in der Bevölkerung und das damit verbundene Vergnügen am Lesen befördert, dem die (technischen) Fortschritte in der Druck- und Buchproduktion im 18. Jahrhundert entsprachen. Das Lesen konnte so zur unterhaltsamen Freizeitbeschäftigung werden, die Erholung vom oft harten Arbeitsalltag versprach. Die Tatsache, dass Frauen über mehr häusliche Freizeit verfügten als Männer, erklärt, dass sie die Mehrheit des Lesepublikums stellten. Ihre Lesebedürfnisse wurden von empfindsam-sentimentalen Frauen- und Liebesromanen erfüllt, welche größtenteils von englischsprachigen Vorbildern inspiriert waren. Das Pendant hierzu in heutiger Zeit ist der „freche Frauenroman“ wie ihn zum Beispiel Eva Heller vorlegt.

Ebenso entstand das noch heute populäre, trivialliterarische Genre des Schauerromans, der – als Reaktion auf den Rationalismus gedeutet – von der englischen Gothic novel und ihres Begründers Horace Walpole inspiriert war. Bedeutendster deutschsprachiger Vertreter dieses Genres war Christian Heinrich Spieß.

Neben diesen Ausprägungen bildete sich im 18./19. Jahrhundert die Heimatliteratur heraus sowie die Räuberromane, die nicht zuletzt durch ihre vorgeblichen Freiheitsideale und ihren aktionistischen Protest gegen die bestehende Gesellschaftsordnung beträchtliche Popularität gewinnen konnten. So erregte etwa Heinrich Clauren mit dem Erfolg seiner "Mimili" den Unwillen des hochliterarisch orientierten Wilhelm Hauff so sehr, dass dieser ihn zum Ziel satirischer und polemischer Angriffe machte. Auch die Indianer- und Wildwestliteratur fand – inspiriert durch Romane des Amerikaners James Fenimore Cooper – eine breite Leserschaft, da sie das Auswanderungsland Amerika als besondere Thematik der Abenteuer- und Reiseliteratur in diesen Romanen entdecken konnte. Als wohl wichtigster Vertreter ist hierbei – neben Friedrich Gerstäcker oder Charles Sealsfield – Karl May zu nennen, der u. a. durch seine "Winnetou"-Romane bis heute anhaltenden Ruhm sich erschrieb, ebenso wie der historische Roman, der seit dem 19. Jahrhundert – geprägt vor allem durch Walter Scott und Alexandre Dumas – sich anhaltender Beliebtheit und breiter Leserschichten erfreut. 

Die seit der zweiten Hälfte des 18. Jahrhunderts entstehenden kommerziellen Leihbibliotheken waren als Verbreitungsform der Trivialliteratur ebenso wichtig wie die Kolportageliteratur. Daneben trugen insbesondere die Fortschritte in der Drucktechnik des Zeitungswesens dazu bei, dass sich gedruckte Prosaliteratur zum leicht erhältlichen Massenmedium entwickeln konnte, wobei Eugène Sue mit "Les mystères de Paris" zum Begründer des Fortsetzungsromans in Zeitungen wurde. 

Im 20. Jahrhundert wird der historisch-zeitgeschichtliche Roman des 19. Jahrhunderts fortgeschrieben, aber auch zum Vermittler patriotischen Gedankenguts umfunktioniert. Kriegsgeschehen, das heldenhafte Töten und Sterben für das Vaterland, sowie die propagandistische Verunglimpfung des Gegners werden zum kriegsverherrlichenden thematischen Zentrum der Erzählhandlungen. 
Die Abenteuer- und Reiseliteratur entwickelt sich in Richtung auf Science-Fiction weiter. Auch die Comics erlangten nun durch ihre enorme Vielfalt ihren bis heute anhaltenden Erfolg in allen Gesellschaftsschichten. Hervorgegangen aus den am Ende des 19. Jahrhunderts in amerikanischen Zeitungen veröffentlichten Comic-Strips sind sie heute in ihren vielfältigen Varianten und Ausprägungen nur noch bedingt der Trivialliteratur zuzurechnen. So standen Comic-Strips anfangs dem Satirischen sehr nahe, die Erwachsenen-Comics der 1960er Jahre waren stark von der Pop-Art beeinflusst und wandten sich mit sozialkritischem Hintergrund an eine intellektuelle Leserschaft, bis hin zur sensiblen Behandlung sehr komplexe Themen 
wie etwa der Shoah. 

Merkantiles Ziel der Produktion von Trivialliteratur bleibt durchwegs, eine möglichst breite Leserschaft und damit potentiell hohe Verkaufszahlen – bis hin zum Bestseller – zu erreichen.

Der geschichtliche Überblick lässt erkennen, dass erfolgreiche Trivialliteratur über die Jahrhunderte hinweg weitgehend als fiktionale Prosa begegnet, die Gattung Lyrik – mit Ausnahme etwa im 19. Jahrhundert der Gedichte der Kempner oder der Studentenlyrik sowie im 20. Jahrhundert insbesondere der Schlagerliteratur – bleibt dagegen eher eine beachtenswerte Randerscheinung.






</doc>
<doc id="5263" url="https://de.wikipedia.org/wiki?curid=5263" title="Thomas S. Kuhn">
Thomas S. Kuhn

Thomas Samuel Kuhn (* 18. Juli 1922 in Cincinnati, Ohio; † 17. Juni 1996 in Cambridge, Massachusetts) war ein US-amerikanischer Physiker, Wissenschaftsphilosoph und Wissenschaftshistoriker. Er gehört zu den bedeutendsten Wissenschaftsphilosophen des 20. Jahrhunderts.

In seinem Hauptwerk "The Structure of Scientific Revolutions" beschreibt Kuhn die Wissenschaft als eine Folge von Phasen der Normalwissenschaft, unterbrochen von wissenschaftlichen Revolutionen. Ein zentrales Konzept ist hierbei das Paradigma; ein Paradigmenwechsel ist eine Revolution. Das Verhältnis von Paradigmen, zwischen denen eine Revolution liegt, bezeichnet Kuhn als inkommensurabel, was hier bedeutet: nicht mit dem gleichen (begrifflichen) Maß messbar.

Thomas Kuhn wurde 1922 in Cincinnati in eine jüdische, nicht praktizierende Familie geboren. Sein Vater war als Ingenieur in der Industrie und seine Mutter als Korrektorin tätig. 1940 begann er an der Harvard-Universität, an der schon sein Vater studiert hatte, ein Studium der Physik. Während seines Studiums belegte er mehrere Kurse in Philosophie und Literatur und schrieb außerdem für die von Studenten herausgegebene Zeitung "Harvard Crimson".

Nach seinem Bachelorabschluss im Jahr 1943 arbeitete er zunächst in einem Radar-Forschungslabor ("Radio Research Laboratory") in Harvard. Dort war er als Theoretiker an Radar-Gegenmaßnahmen für den Zweiten Weltkrieg beteiligt. 1944 wurde er in Großbritannien und im gerade von den West-Alliierten eroberten Nordfrankreich als Radartechniker eingesetzt. Im Herbst 1944 kehrte Thomas Kuhn nach Harvard zurück, wo er sein Studium fortsetzte: Er erhielt seinen Master und promovierte 1949 bei dem späteren Nobelpreisträger John H. van Vleck in theoretischer Festkörperphysik.

Zu dieser Zeit war sein eigentlicher Mentor bereits der damalige Präsident von Harvard, James Bryant Conant. Conant wurde auf Kuhn wegen seines für einen Physiker ungewöhnlichen Engagements im "Harvard Crimson" und in einem literarisch-philosophischen Club aufmerksam. Auf Conants Initiative hin gab Kuhn bereits vor seiner Promotion einen Kurs in Wissenschaftsgeschichte. Die Arbeit an diesem Kurs beeinflusste Kuhn stark, so dass er sich gegen die Physik und für eine Laufbahn als Historiker und Philosoph entschied.

Von Conant vorgeschlagen, wurde Kuhn Mitglied der "Society of Fellows" in Harvard. Er beschäftigte sich dort mit der Geschichte der Wissenschaft, war aber immer an deren Auswirkungen auf die Philosophie interessiert.

Kuhn nahm 1956 eine Stelle als Assistenzprofessor für Wissenschaftstheorie und Wissenschaftsgeschichte in Berkeley an, einige Jahre später wurde er zum ordentlichen Professor für Wissenschaftsgeschichte. In Berkeley verfasste er unter anderem sein Hauptwerk "The Structure of Scientific Revolutions (Die Struktur wissenschaftlicher Revolutionen)".

Das Buch – er selbst bezeichnet es als Essay – schrieb er anfangs als Teil der International Encyclopedia of Unified Science. Anstoß war die „fast unbekannte Monografie“ "Entstehung und Entwicklung einer wissenschaftlichen Tatsache" von Ludwik Fleck, die einige seiner Gedanken vorwegnimmt.

1963 wurde Kuhn in die American Academy of Arts and Sciences gewählt, 1979 in die National Academy of Sciences. Von 1964 bis 1979 lehrte er an der Princeton University. Danach wechselte er ans Massachusetts Institute of Technology (MIT), wo er die Laurance S. Rockefeller-Professur für Philosophie innehatte, die er bis zu seiner Emeritierung im Jahr 1991 ausfüllte.

Im Jahr 1979 wurde Kuhn zum Mitglied der Leopoldina gewählt. Kuhn wurde 1982 mit der George-Sarton-Medaille ausgezeichnet, dem höchst renommierten Preis für Wissenschaftsgeschichte der von George Sarton und Lawrence Joseph Henderson gegründeten History of Science Society (HSS).

Kuhn war seit 1948 verheiratet. Aus der Ehe gingen drei Kinder hervor. Nach der Scheidung 1979 kam es im Jahr 1982 zu einer weiteren Ehe. Er starb 1996 im Alter von 73 Jahren an Krebs. Bis zu seinem Tod hatte er eine erweiterte Fassung seiner Ideen zur Wissenschaftstheorie unter dem Titel „The Plurality of Worlds: An Evolutionary Theory of Scientific Discovery“ zu etwa zwei Dritteln fertiggestellt. Er beauftragte kurz vor seinem Tod die beiden Philosophen John Haugeland (verstorben 2010) und James Conant (ein Enkel des o. g. James Bryant Conant) mit der Herausgabe des Buchs, was aber bis heute nicht geschehen ist.

Der Begriff des Paradigmas ist ein zentrales Element von Kuhns Philosophie. Während er ihn in "The Structure of Scientific Revolutions" noch sehr frei und in unterschiedlichen Bedeutungen benutzt, bemühte sich Kuhn in späteren Publikationen, den Begriff zu präzisieren.

Kuhn übernahm für seine Theorie den Ausdruck "Paradigma" aus der Linguistik (linguistische Bedeutung). In Kuhns ursprünglicher Verwendung sind Paradigmen „"konkrete Problemlösungen, die die Fachwelt akzeptiert hat"“. Hiermit sind Beispiele wie die Lösung des Problems gemeint, wie eine Kugel auf einer schiefen Ebene herabrollt. Die Lösungen solcher Probleme werden Studierenden in Lehrbüchern erklärt. Solche allgemein akzeptierten Problemlösungen dienen als Anleitung, um andere Probleme zu lösen, indem man sie mit den schon gelösten Problemen analogisiert.

In "The Structure of Scientific Revolutions" erhalten Paradigmen zusätzlich eine globale Bedeutung: Nahezu alles, worüber in der Wissenschaft Konsens besteht, ist paradigmatisch. Gemäß dieser Begriffsausweitung können unter anderem auch ganze Theorien paradigmatisch sein. Kuhn wurde in den Folgejahren für diese philosophisch nicht unproblematische Aufweichung des Paradigmenbegriffes oft kritisiert. Allerdings ist die Allgemeinheit des Paradigmenbegriffs von Kuhn beabsichtigt. Dadurch vermeidet er im Gegensatz zu Karl Popper die methodologische Festlegung auf das, was Wissenschaft ist oder sein soll. Diese Festlegung erfolgt im Rahmen des Paradigmas selbst. Damit ist die Unterscheidung zwischen Wissenschaft und Metaphysik wie die zwischen Entdeckungs- und Begründungszusammenhang hinfällig.

Zu Beginn der 70er Jahre änderte Kuhn seine Terminologie. Paradigmen im weiten Sinne bezeichnete er nunmehr als "disziplinäre Matrix", während er konkrete Problemlösungen fortan "Musterbeispiele" nannte (allerdings gibt Kuhn den Begriff der disziplinären Matrix im Laufe der 70er Jahre wieder auf). Im Postskriptum zu "Structure" von 1969 heißt es zum Paradigmenbegriff:

Die Ausdrücke "Paradigma" und "Paradigmenwechsel" verwendete er nur noch selten. Sie waren inzwischen sowohl in der Rezeption wie auch von Kuhn selbst, und zwar schon sehr früh, abweichend vom ursprünglichen Wortsinne eines Modells in weiterem Sinne unpräzise für alles verwendet worden, was tradiert wird und worüber Konsens unter arbeitenden Wissenschaftlern bestand.

Die Existenz eines Paradigmas ist für Kuhn ein Zeichen reifer Wissenschaften, es ist allerdings nicht ein notwendiges Kriterium für Wissenschaftlichkeit. Kuhn bezeichnet vorparadigmatische Wissenschaft auch als Protowissenschaft.

Mangels anerkannter Musterbeispiele besteht für den Forscher in einer vorparadigmatischen Phase der Wissenschaft ein großer Freiraum in der Wahl seiner Experimente, so dass Wissenschaftler stark unterschiedliche Aspekte ihres Themengebietes untersuchen und die hierbei gefundenen Theorieansätze die Experimente anderer Forscher nicht zu erklären vermögen.

Auf diese Weise entstehen oft viele konkurrierende und inkompatible Ansichten unter Wissenschaftlern. Als Beispiel nennt Kuhn die Elektrizität, welche durch Reibungsphänomene oder natürliche Abstoßung und Anziehung erklärt und von wieder anderen als Flüssigkeit angesehen wurde, bevor zur Zeit Benjamin Franklins eine paradigmatische Theorie der Elektrizität entstand.

Während die Mathematik schon seit der Antike paradigmatischen Charakter habe, seien laut Kuhn andere Wissenschaftsbereiche wie die Genetik erst seit relativ kurzer Zeit paradigmatisch. Wieder andere Bereiche, besonders in den Sozialwissenschaften, befinden sich noch immer in einem vorparadigmatischen Zustand.

"Normalwissenschaft" bezeichnet in der wissenschaftstheoretischen Konzeption von Kuhn eine der beiden möglichen Phasen der Wissenschaftsentwicklung, nachdem eine Wissenschaft die vorparadigmatische Phase hinter sich gelassen hat. Von ihr unterschieden wird die "außerordentliche" oder "revolutionäre" Phase.

Charakteristisch für Normalwissenschaft ist die Akzeptanz eines Paradigmas durch die wissenschaftliche Gemeinschaft, auf dessen Basis Forschung betrieben wird. Zum einen wird der Bereich relevanter Probleme durch das Paradigma drastisch eingeschränkt, dies bedeutet aber auf der anderen Seite die Möglichkeit, in die Tiefe gehende Forschung zu betreiben.

Die Aufgabe des Wissenschaftlers in normalwissenschaftlichen Phasen ist die Lösung von Problemen, deren Lösungsregeln implizit durch das Paradigma gegeben sind. Kuhn bezeichnet diese Tätigkeit als "Lösen von Rätseln", in Analogie zu Puzzles oder Schachproblemen, in denen die Grundregeln fest vorgegeben sind. Als Rätsel werden bevorzugt Probleme angegangen, von denen vermutet wird, dass eine Lösung für sie existiert und mit Hilfe der Lösungsregeln auch gefunden werden kann. Ist dies nicht der Fall, werden Probleme oft als "metaphysisch" abgelehnt.

Im Wesentlichen gibt es drei Sorten von Rätseln:

Weitere normalwissenschaftliche Tätigkeiten, die unter diese Punkte fallen, sind die Bestimmung universeller physikalischer Konstanten, die Formulierung quantitativer Gesetze, Musterbeispiele für die Lösung wissenschaftlicher Probleme und die Inkorporierung neuer Phänomene in das Paradigma.

Prinzipiell geht es dem Forscher dabei nicht um die Überprüfung oder Falsifikation des Paradigmas. Über dieses herrscht Konsens unter den Wissenschaftlern. Ziel der Normalwissenschaft sind also keine fundamentalen Neuerungen, die das Weltbild umstürzen könnten, sondern die schrittweise Verbesserung von Theorien im Rahmen des gegebenen Paradigmas.

Auf keinen Fall sieht Kuhn in normalwissenschaftlicher Forschung eine wenig herausfordernde Routinetätigkeit. Analog zu vielen konstruierten Rätseln sind sowohl Kreativität nötig als auch die Fähigkeit, Methoden auf technisch oder abstrakt-mathematisch hohem Niveau anwenden zu können. Außerdem treten auch innerhalb der Normalwissenschaft Innovationen auf, nur betreffen diese nicht die Grundpfeiler der Theorie.

Sofern Probleme bei der Lösung der Rätsel auftreten, werden sie in den meisten Fällen der mangelnden Qualität des Wissenschaftlers oder der verfügbaren experimentellen Methoden zugeschrieben. Durch diese enge Bindung der wissenschaftlichen Praxis an das Paradigma wird eine Spezialisierung und Tiefe erreicht, die ohne das Vertrauen in eine sichere Basis nicht möglich wäre.

Im Gegensatz zur von Karl Popper vorgeschlagenen Falsifizierbarkeit hält Kuhn die Möglichkeit, Normalwissenschaft zu treiben für das entscheidende Abgrenzungskriterium zu vorwissenschaftlichen oder pseudowissenschaftlichen Theorien.

Kuhn beschreibt das Paradigma als Träger einer wissenschaftlichen Theorie:

Erst wenn über einen längeren Zeitraum hinweg an zentralen Stellen Probleme aufgetreten sind oder überraschende Entdeckungen gemacht worden sind, beginnt die Phase der außerordentlichen Wissenschaft. In ihr wird auch wieder über die Grundlagen selbst diskutiert. Eine solche "Krise" kann zu einem Paradigmenwechsel führen, bei dem das Paradigma der Disziplin verworfen und durch ein anderes ersetzt wird.

Von Kuhn angeführte Beispiele für wissenschaftliche Revolutionen sind unter anderem die Ablösung der Phlogistontheorie durch Lavoisiers Sauerstoffchemie, Einsteins Relativitätstheorie, die die klassische Newtonsche Physik ablöste, und in besonderer Ausführlichkeit die Kopernikanische Wende vom geozentrischen hin zum heliozentrischen Weltbild. Der Wissenszuwachs ist nun im Gegensatz zur Normalwissenschaft nicht kumulativ, da wichtige Teile der alten Theorie aufgegeben werden. Der Inhalt der nachrevolutionären Theorie ist vorher nicht abzusehen, unerwartet.

Mit wissenschaftlichen Revolutionen verändern sich nach Kuhn nicht nur die Theorien, sondern auch das allgemeine Weltbild und die wissenschaftliche Praxis. Dies führte dazu, dass Kuhn in "Structure" wiederholt davon spricht, dass es so ist, als würde sich nicht die Interpretation des Menschen, sondern die Welt selbst ändern. Ein Paradigma wirkt sich auf tieferen Ebenen aus: es betrifft selbst die Wahrnehmung der Wissenschaftler. Vorläufer bezüglich dieser Behauptung sind Ludwik Fleck "(Entstehung und Entwicklung einer wissenschaftlichen Tatsache)", der bereits den Paradigmenwechsel forderte, und Norwood Russell Hanson "(Patterns of discovery)". Aufgrund der kognitiven Dimension von Paradigmen vergleicht Kuhn Paradigmenwechsel mit sogenannten Gestaltwechseln. Diese kennzeichnet ein plötzlicher Wechsel von einer zu einer anderen Wahrnehmung.

Im ausdrücklich formulierten Gegensatz zu dem falsifikatorischen Ansatz Karl Poppers behauptet Kuhn, dass Paradigmen nicht nur deshalb aufgegeben werden, weil sie falsifiziert wurden. Ein Paradigma wird erst dann aufgegeben, wenn es durch ein anderes ersetzt werden kann. Ein Aufgeben des Paradigmas durch die wissenschaftliche Gemeinschaft ohne Ersatz würde, Kuhn zufolge, die Aufgabe der wissenschaftlichen Tätigkeit per se bedeuten. Ebenso wenig kann Evidenz zwischen zwei um die Paradigmavorherrschaft konkurrierenden Theorien entscheiden. So behauptet Kuhn, dass es zur Zeit der Erfindung des Kopernikanischen Systems keine Evidenz gab, die dieses System über das damals etablierte Ptolemäische System erhoben hätte. Dieses Argument ist heute als Unterdeterminierung von Theorien durch Evidenz bekannt und wird insbesondere von Empiristen wie von Bas van Fraassen verwendet.

Einer der umstrittensten und meistdiskutierten Punkte von Kuhns Philosophie ist das auf einer Analogie mit der Mathematik beruhende Konzept der Inkommensurabilität, das er unabhängig von, aber etwa zeitgleich mit Paul Feyerabend in die Wissenschaftsphilosophie eingeführt hat (Kuhns und Feyerabends Begriffe der Inkommensurabilität unterscheiden sich etwas voneinander).
Der Kuhn’sche Begriff der Inkommensurabilität enthält die folgenden, auf den ersten Blick heterogenen Elemente:
Tatsächlich bilden diese drei Elemente für Kuhn aber eine Einheit: Im Kern ist Inkommensurabilität das Resultat einer begrifflichen Veränderung.

Ein zentrales Beispiel für die Inkommensurabilität zweier Theorien ist für Kuhn die Theorie des Sonnensystems. Das Ptolemäische Weltbild kannte folgende „Planeten“: Sonne, Mond, Merkur, Venus, Mars, Jupiter, Saturn (Uranus, Neptun und Pluto waren damals noch unbekannt). Planeten waren die "Wandelsterne", die relativ zu den Fixsternen eine Bewegung ausführten. Im kopernikanischen Weltbild hingegen firmiert eine andere Menge von Himmelskörpern als „Planeten“, nämlich Merkur, Venus, Erde, Mars, Jupiter und Saturn. Jetzt waren Planeten Himmelskörper, die die Sonne umkreisen. "Zudem" werden zwei neue Kategorien eingeführt, nämlich die Sonne als ein Zentralgestirn und die Kategorie Satelliten, in die der Mond der Erde und später die Monde des Jupiters, entdeckt durch Galilei, gehören. Als Konsequenz gilt für Kuhn, für uns überraschend: Der Satz "„Im ptolemäischen System drehen sich die Planeten um die Erde und im kopernikanischen System um die Sonne“" ist kein wirklich sinnvoller Satz, da es keinen einheitlichen Planetenbegriff gibt, der in diesem Satz verwendet werden könnte.

Als weiteres Beispiel nennt Kuhn die Revolution von der Newton’schen Physik zur Relativitätstheorie Einsteins. Beide Theorien seien inkommensurabel, weil in beiden Theorien verwendete Wörter wie z. B. Energie in beiden Theorien unterschiedliche Bedeutungen hätten. Demnach könne die Newton’sche Physik auch nicht als Annäherung an die Spezielle Relativitätstheorie für Geschwindigkeiten, die klein gegenüber der Lichtgeschwindigkeit sind, angesehen werden. Ein sanfter Übergang der einen Lehre in die andere sei somit nicht möglich. Dies ist mit dem Korrespondenzprinzip im Bohr’schen Sinne völlig verträglich: Bestritten wird dabei nicht, dass im Grenzübergang die numerischen Werte bestimmter Variablen ineinander übergehen; das ist eine mathematische Tatsache. Dennoch bleibt, wie auch schon von Bohr für den analogen Fall des Verhältnisses von klassischer Mechanik und Quantenmechanik betont, ein begrifflicher Bruch zwischen beiden Theorien.

Die Hypothese der Inkommensurabilität gibt der Kuhn’schen Auffassung der Wissenschaftsentwicklung ihre eigentliche Brisanz. Die Inkommensurabilitätsannahme ist gegen die Vorstellung gerichtet, dass der wissenschaftliche Fortschritt kumulativ zu verstehen sei: als eine stetige Anhäufung wissenschaftlicher Erkenntnisse ohne wesentliche Rücknahmen und Brüche. Dies war beispielsweise Karl Poppers Auffassung. Kuhn hat aber nie behauptet, dass die Wissenschaftsentwicklung irrational ablaufe. Er hat nur bestritten, dass die traditionelle Auffassung vom rationalen Theorienvergleich angemessen sei, nämlich durch einen Punkt-für-Punkt-Vergleich der verschiedenen Konsequenzen der involvierten Theorien. Tatsächlich ist Kuhn aber irrtümlicherweise vielfach so verstanden worden, als ob er wegen der Inkommensurabilität die Möglichkeit des rationalen Theorienvergleichs und damit die Rationalität der Wissenschaftsentwicklung leugnen wolle.

In den ersten Jahren nach "Structure" stand Kuhns Paradigmenbegriff im Zentrum der Kritik.
Kuhn wurde oft für die Unschärfe seines Paradigmabegriffs kritisiert. Margaret Masterman fand 21 unterschiedliche Verwendungen des Begriffs in "The Structure of Scientific Revolutions", woraufhin Kuhn einen Versuch der Klärung unternahm (siehe oben). In späteren Jahrzehnten verlagerte sich die Kritik zusehends auf Kuhns Vorstellung von Inkommensurabilität.

Nach einem Hauptkritiker Kuhns, dem Wissenschaftstheoretiker Imre Lakatos, umgreifen Paradigmen mehr als einen Leitgedanken, sie sind komplex in ihrer Zusammensetzung. Sie umfassen einen sogenannten harten Kern, der aus den tragenden Theorien (einer Wissenschaftsdisziplin z. B.) besteht, sowie aus einer „Schutzzone“ von Hilfshypothesen, die den „harten Kern“ gegen Widerlegungen abschirmen.

Als dritter Bestandteil der Paradigmen fungiert nach Lakatos ein spezifisch zu diesem „harten Kern“ gehörender oder durch ihn induzierter leistungsfähiger Problemlösungsapparat. Deshalb sei der Ausdruck "Paradigma" durch die treffendere Formulierung "Methodologie wissenschaftlicher Forschungsprogramme" zu ersetzen. Verschiedene Forschungsprogramme können nach Lakatos rational verglichen werden und sind nicht etwa inkommensurabel.

Hiermit wandte sich Lakatos gegen Kuhns Vorstellung von wissenschaftlichen Revolutionen und besonders gegen den Einfluss sozialer und kognitiver Faktoren auf diese. Er warf Kuhn in deutlichen Worten vor, dass für ihn wissenschaftliche Revolutionen irrational seien, eine Sache von "Mob-Psychologie". Gegen diesen Vorwurf wehrte sich Kuhn ausdrücklich.

Während Kuhns Paradigmenbegriff in der Wissenschaftstheorie vielfach aufgegriffen wurde, ist die Inkommensurabilitätshypothese praktisch nicht akzeptiert und wird bis heute stark kritisiert. Beispielsweise wurde eingewendet (etwa von John W. N. Watkins), dass, wenn Paradigmen bzw. Theorien inkommensurabel – also unvergleichbar – seien, sie gar nicht in einer Konkurrenzsituation miteinander stehen könnten. Es würde sich dann also überhaupt nicht die Frage der Verdrängung der einen Theorie durch die andere stellen, was Kuhns ursprünglicher Behauptung widerspricht, wonach neue Theorie und verdrängte Theorie nicht verträglich seien. Ein weiterer Einwand ist, dass Kuhn seine wissenschaftshistorischen Untersuchungen, die ihn zu seinen Auffassungen führten, nur durchführen konnte, indem er selbst die verschiedenen wissenschaftlichen Theorien von einer übergeordneten Position aus betrachtete und verglich, was gemäß seiner Inkommensurabilitätshypothese unmöglich gewesen sein sollte.

Nach Kuhn darf Inkommensurabilität jedoch nicht als totale Kommunikationslosigkeit verstanden werden. Es ändert sich nicht die gesamte Weltsicht, denn nachfolgende Theorien müssen zumindest als solche erkennbar sein, um überhaupt als inkommensurabel bezeichnet werden zu können. Es gibt also einen gemeinsamen Kern auch inkommensurabler Theorien, der einen Vergleich ermöglicht.

Thomas Kuhn war persönlich von der Unübersehbarkeit eines Fortschritts in der Wissenschaft überzeugt. Allerdings sah er das Fortschreiten nicht als zielgerichteten Prozess hin auf eine endgültige, objektive Beschreibung der Wirklichkeit, sondern als einen Prozess ähnlich der Darwin’schen Evolution, in dem alte Theorien zwar durch bessere neue abgelöst werden, der jedoch nicht zielgerichtet ist.

Der amerikanische Physiker und Nobelpreisträger Steven Weinberg kritisierte in einem Essay Kuhns Position als „radikalen Skeptizismus“, der zu der relativistischen Auffassung führe, die Wissenschaft sei, ähnlich wie „Demokratie oder Baseball“, lediglich eine soziale Konstruktion. Wenn inkommensurable wissenschaftliche Theorien nur innerhalb ihres Paradigmas beurteilt werden könnten, würden diese gegenüber anderen, nichtwissenschaftlichen Theorien keine privilegierte Stellung einnehmen. Diese Auffassung hält Weinberg für inakzeptabel und versucht, in seinem Aufsatz Kuhns Thesen von der Inkommensurabilität wissenschaftlicher Revolutionen zu widerlegen.

In eine ähnliche Richtung zielt die Kritik, dass er, wenn es keine objektiven Kriterien für die Theorienwahl gebe, die Wissenschaftsgeschichte als irrationalen Prozess darstelle, der nur Resultat von Macht und Disziplin sei, und dass Kuhns Position letztlich zu einem totalen Methoden- und Theorienrelativismus führe, zum „anything goes“ von Paul Feyerabend.

Kuhn setzte sich in den Jahrzehnten nach dem Schreiben von "Structure" gegen diese Vorwürfe zur Wehr und vertrat die Auffassung, dass sein Bild der Wissenschaftsgeschichte keinesfalls zum Relativismus führe.

Die Berühmtheit von Thomas Kuhns Thesen und seine zum Teil quasi-poetische Sprache hat zu vielen Fehldeutungen in der Rezeptionsgeschichte geführt. Insbesondere der Begriff des "Paradigmenwechsels" wurde später zu einem schillernden und gerne auch außerhalb von wissenschaftlichen Theorien vereinnahmten Schlagwort, da sich mit ihm moderne Werte wie Innovation, Fortschritt, Kreativität u. a. verknüpften.
Beispielsweise verwendet Samuel P. Huntington die These des Paradigmenwechsels in seinem Buch Kampf der Kulturen für die Erklärung des Aufkommens seines Zivilisationenparadigmas.

Die Popularisierung zum Allerweltsbegriff und die „Entwicklung zur Beliebigkeit“ sowie der „Kultstatus“ des Begriffes haben Kuhn immer wieder als einen Wegbereiter der Postmoderne erscheinen lassen, obgleich er sich davon explizit distanziert hat.

Kuhn selber sah schon die Übertragung seiner Befunde aus der Geschichte der Naturwissenschaften auf andere Wissensbereiche, wie die Soziologie, als problematisch an.

Zu Ehren von Thomas Kuhn wurde von der International Academy of Science zusammen mit Yuan T. Lee der Thomas Kuhn Award verliehen.





</doc>
<doc id="5264" url="https://de.wikipedia.org/wiki?curid=5264" title="Thomas Kuhn">
Thomas Kuhn

Thomas Kuhn ist der Name folgender Personen:

<onlyinclude>* Thomas Kuhn (Künstler) (1948–1992), Schweizer Maler und Bildhauer 


</doc>
<doc id="5265" url="https://de.wikipedia.org/wiki?curid=5265" title="Tiflis">
Tiflis

Tiflis ( Tbilissi; bis 1936: ტფილისი "Tpilissi") ist die Hauptstadt Georgiens. Es ist die bevölkerungsreichste und größte Stadt des Landes. Tiflis hat mehr als eine Million Einwohner, die Gesamtfläche beträgt 726 Quadratkilometer, das bebaute Gebiet rund 70 Quadratkilometer.

Die Stadt liegt im Zentrum der Kaukasus-Landenge im östlichen Teil Georgiens. Sie erstreckt sich in einer Gebirgsniederung 21 Kilometer entlang des Flusses Kura ( "Mtkwari"). Im Westen wird Tiflis vom Berg Mtazminda, im Osten von der Hügelkette Machata, im Süden vom Mtabori und dem Gebirgszug Solalaki begrenzt. Entsprechend dem gebirgigen Bodenrelief haben die Stadtbezirke Höhenunterschiede zwischen 380 und . Viele Wohnviertel wurden in Terrassen an die Hänge gebaut.

Das Klima in Tiflis ist gemäßigt. Die Temperatur beträgt im Jahresdurchschnitt 13,3 °C, die durchschnittliche jährliche Niederschlagsmenge 498 Millimeter.
Weil Georgien auf einem seismisch aktiven Gebiet liegt, ist es in Tiflis im Laufe der Geschichte immer wieder zu Erdbeben gekommen. Das letzte schwere Beben am 25. April 2002 hatte eine Magnitude von 4,8 auf der Richterskala. Es tötete sechs Menschen und beschädigte über 10.000 Gebäude, hauptsächlich in den Altstadtgebieten Mtazminda, Tschughureti, Issani und Krzanissi. Der Schaden an den Wohnhäusern betrug rund 62 Millionen US-Dollar.

Der georgische Name "Tbilissi" bedeutet so viel wie ‚warme Quelle‘, von თბილი, "tbili" (). An den Nordosthängen des Mtabori sprudelt bis zu 46,5 °C heißes, kohlensäurehaltiges Schwefel-Quellwasser aus der Erde, das seit Jahrhunderten in Badehäusern genutzt wird.

Die Überlieferung berichtet, dass der georgische König Wachtang I. Gorgassali auf der Jagd in einem bewaldeten Tal einen Fasan erlegte. Das Tier fiel in eine heiße Quelle und wurde vom sprudelnden Wasser sofort gar gekocht. Der König ließ die Umgebung genau erforschen. Als er erfuhr, dass es dort viele heiße Quellen gab, gründete er 485 an diesem Ort die Stadt Tiflis.

Der in Deutschland gebräuchliche Name "Tiflis" wurde bereits im 13. Jahrhundert von deutschen Kartografen, später von Marco Polo verwendet. Er wird heute außer im Deutschen regulär im Türkischen, Griechischen (Τιφλίδα) und Persischen (تفلیس) gebraucht. Bis 1936 war er die offizielle russische Bezeichnung der Stadt (Тифлис) und war international verbreitet.

1936 wurden durch Beschluss der sowjetischen Führung die amtlichen russischen Bezeichnungen verschiedener Städte modifiziert, meist den örtlichen Sprachformen angepasst. In diesem Zuge wurde die georgische Namensform "Tpilissi" gemäß dem Vorschlag georgischer Linguisten modernisiert, indem der altgeorgische Bestandteil ტფილი ("tpili") ‚warm‘ durch die neugeorgische Form თბილი ("tbili") ersetzt wurde. Diese Form wurde zur offiziellen russischen Benennung (Тбилиси). Am 20. September 2006 hielt das georgische Parlament eine Feierstunde zum 70. Jahrestag der Umbenennung ab.

Die meisten Sprachen haben die georgische Namensform übernommen. Im Deutschen ist "Tbilissi" eine häufig verwendete Namensform, in der DDR war es die offizielle Bezeichnung.

Tiflis wurde erstmals im 4. Jahrhundert auf einer römischen Karte unter dem Namen "Pilado" urkundlich erwähnt. Sie lag an einer Kreuzung der Karawanenstraßen vom Schwarzen Meer nach Persien, Indien und China. Die Perser hielten Tiflis besetzt und hatten die Festung Narikala oberhalb der Stadt errichtet.

Im 5. Jahrhundert befreite der georgische König Wachtang I. Gorgassali (dt. "Wolfshaupt") Tiflis, machte sie zu seiner Hauptstadt und baute sie zur Metropole aus. 591 – nach dem Ende des vorletzten der Römisch-Persischen Kriege – wurde Tiflis oströmische Provinzhauptstadt. Im 7. Jahrhundert wurde sie von den Arabern erobert, ging danach in persischen, byzantinischen und 1068 in seldschukischen Besitz über.

1121 wurde sie nach der Befreiung durch David den Erbauer erneut georgische Hauptstadt und dank der befestigten Lage am Schnittpunkt von inzwischen sieben europäisch-asiatischen Handelswegen zu einer der reichsten Städte des Mittelalters. Marco Polo berichtete, es gäbe in Georgien eine "herrliche Stadt namens Tiflissi, die von Vororten und vielen Festungen umgeben ist".

Im 13. Jahrhundert wurde die Stadt von den Choresmiern verwüstet. Von 1386 bis 1402 gehörte Tiflis zum zentralasiatischen Reich Timur Langs.

Im 17. Jahrhundert fiel die Stadt unter türkische Herrschaft, wurde vom georgischen König Irakli II. zurückerobert und befestigt. Im 18. Jahrhundert bemächtigten sich die Türken abermals der Stadt, wurden aber 1735 von Nadir Schah von Persien wieder vertrieben, der den georgischen König Theimuras einsetzte. Dessen Sohn Irakli brachte die Stadt zu hoher Blüte.

1795 marschierten die Perser unter Schah Aga Mohammed Khan (georgisch "Aga Mohammed Chan Irakli") in Georgien ein. Nach der "Schlacht von Krtsanisi" wurde Tiflis völlig zerstört und 22.000 Menschen in Sklaverei verschleppt. Im November 1799 besetzte der russische Generalmajor Lasarus die Stadt.

Nach der Annexion 1801 wurde Georgien Teil des Russischen Reiches und Tiflis wurde nach wiederholten Änderungen der Verwaltungsstruktur 1850 Hauptstadt des gleichnamigen Gouvernements (russisch: "Tiflisskaja Gubernija"). Russland verwaltete von dort aus den Kaukasus.
In der georgisch-orthodoxen Sioni-Kathedrale zwang der russische General Knorring am 12. April 1802 die georgische Aristokratie und Geistlichkeit mit Waffengewalt zum Eid auf die russische Zarenkrone.

Russland förderte andererseits den Handelsplatz, befreite einen Großteil der in Tiflis verkauften Waren sowie Transitgüter von Steuern. Die Stadt erblühte und die Bevölkerung wuchs von 8500 im Jahr 1811 auf fast 30.000 im Jahr 1825.

Das heutige Stadtbild prägte sich in der Mitte des 19. Jahrhunderts aus. Auf Initiative des russischen Vizekönigs Fürst Michail Woronzow wurde die Stadt nach 1845 modernisiert und erweitert. Er berief den Italiener Giovanni Scudieri als Chefarchitekten, gründete das erste Theater und die erste öffentliche Bibliothek in Transkaukasien. Die alten Stadtmauern wurden abgerissen und Tiflis dehnte sich nach Norden und Westen aus.

Im heutigen Stadtzentrum, an der Allee Rustawelis Gamsiri, wurden elegante Paläste, Hotels, Wohnhäuser und Museen im Stil des Klassizismus, des Barock und später des Jugendstils gebaut. Der deutsche Landschaftsarchitekt Heinrich Scharrer und der deutsche Architekt Otto Simonson legten zusammen den "Alexanderpark" im Stadtzentrum an. Um den Handel zu stärken, wurde die Tiflisstraße gebaut und Tiflis 1872 per Eisenbahn mit Poti und 1883 mit Batumi sowie Baku verbunden.

Die Zahl der Industrieunternehmen wuchs von 138 im Jahr 1864 auf 228 im Jahr 1883. 1886 hatte Tiflis 104.024 Einwohner. Es handelte sich hauptsächlich um Armenier, Russen und Georgier, daneben Deutsche, Tataren, Perser und Polen. Bei der ersten gesamtrussischen Volkszählung von 1897 wurde für Tiflis eine Einwohnerzahl von 159.590 ermittelt. Zahlenmäßig stärkste Bevölkerungsgruppe mit knapp 30 Prozent waren damals noch die Armenier, gefolgt von den Russen mit 28 Prozent. Die Georgier stellten nur ein Viertel der Stadtbewohner.

Am 16. Mai 1918 wurde Tiflis Regierungssitz der "Demokratischen Republik Georgien". Am 25. Februar 1921 besetzte die 11. Armee der Roten Arbeiter- und Bauernarmee Tiflis und gliederte es Sowjetrussland an. Bürgermeister Beniamin „Benia“ Tschchikwischwili flüchtete nach Frankreich. Tiflis wurde Hauptstadt der Georgischen SSR und war zwischen 1922 und 1936 gleichzeitig Hauptstadt der Transkaukasischen SFSR.

Unter sowjetischer Herrschaft dehnte sich die Stadt von 53 Quadratkilometern im Jahr 1920 auf 365 Quadratkilometer im Jahr 1989 aus. Durch Industrieansiedlung in den 1940er Jahren wuchs die Bevölkerung von Tiflis enorm an. In der Stadt bestand das Kriegsgefangenenlager "236" für deutsche Kriegsgefangene des Zweiten Weltkriegs. Tote wurden auf zwei Kriegsgefangenenfriedhöfen in Einzel- oder Massengräbern beerdigt. Schwer erkrankte wurden im Kriegsgefangenenhospital "1563" versorgt. Auf dem Hospitalfriedhof wurden bis 1946 bereits mehr als 3400 Verstorbene beerdigt.

1956 demonstrierten Studenten und Schüler gegen die sowjetische Herrschaft in Georgien. Mindestens 80 von ihnen wurden beim Massaker von Tiflis getötet. Ab 1951 entstanden große Neubaugebiete mit einförmigen Wohnblöcken. 1966 wurde die erste, 1979 die zweite U-Bahnlinie eröffnet, 1972 auf dem Berg Mtazminda ein Fernsehturm von 274,5 Meter Höhe errichtet.

1989 kam es in Tiflis zu Demonstrationen gegen die Kommunistische Partei und für staatliche Unabhängigkeit. Das Vorgehen sowjetischer Fallschirmjäger unter Führung von Oberst Alexander Lebed gegen Hungerstreikende im Zentrum von Tiflis am 9. April mit scharf geschliffenen Spaten und Giftgas endete mit 20 Toten.

Nach der Unabhängigkeit des Landes 1991 wurde Tiflis die Hauptstadt Georgiens. Ein Militärputsch gegen Präsident Swiad Gamsachurdia führte zwischen Dezember 1991 und Januar 1992 zum "Tbilisser Krieg", bei dem die Innenstadt um den Rustaweli-Boulevard durch Panzer, Artillerie und Raketen stark beschädigt wurde. Im November 2003 fand vor und im Parlamentsgebäude die samtene Revolution (Rosenrevolution) statt, die eine reformerische Wende in Georgien einleitete.

Die Stadt entwickelt sich in den 2000er und 2010er Jahren schnell. Straßen und öffentlicher Nahverkehr wurden modernisiert, und bedeutende öffentliche Bauten sind nachts farbig und wechselnd beleuchtet. Sehenswürdigkeiten werden restauriert und sind gut erreichbar.

Rund 30 Prozent der Einwohner Georgiens leben heute in Tiflis und Umgebung. Die Einwohnerzahl der eigentlichen Stadt betrug bei der letzten Volkszählung 2014 1.062.282, für das gesamte der Stadtverwaltung unterstellte Gebiet (mit dem Rang einer Region) 1.108.717. Damit ist sie gegenüber der vorangegangenen Volkszählung 2002 leicht zurückgegangen, als in der Stadt 1.073.345 und in der Region 1.121.292 Menschen lebten. Der Bevölkerungsrückgang gegenüber der maximalen Einwohnerzahl von 1.259.692 bei der Volkszählung 1989 hat sich somit erheblich verlangsamt.
In Tiflis waren nach der Zählung 2014 rund 90 Prozent der Bevölkerung Georgier, 4,8 Prozent Armenier, 1,4 Prozent Aserbaidschaner, 1,2 Prozent Russen, 1,0 Prozent Kurden (Jesiden), 0,4 Prozent Osseten, 0,3 Prozent Ukrainer und 0,2 Prozent Griechen. Damit ist gegenüber 2002 der Anteil der Georgier weiter gestiegen (von 84 %), während der Anteil insbesondere der Armenier, Russen und Kurden relativ stark zurückging (2002 entsprechend 7,6, 3,0 und 1,6 %); nur der aserbaidschanische Anteil stieg leicht (von 1,1 %).

Die Religionen sind vielfältig. Tiflis beherbergt alte Kirchengebäude der Georgischen Orthodoxen Apostelkirche, der Armenisch-gregorianischen Apostelkirche und der Römisch-katholischen Kirche. Es gibt eine sephardische und zwei aschkenasische Synagogen, eine Moschee mit zwei Gebetsnischen, einer schiitischen und einer sunnitischen Mihrab, einen zoroastrischen Tempel und eine lutherische Kirche.

Unter dem russischen Kaiser Alexander I. wurden württembergische Pietisten im Südkaukasus angesiedelt, die sich aus religiösen Gründen hier niederließen. Zwei von Kaukasiendeutschen bewohnte Siedlungen wurden im Laufe der Zeit mit Tiflis vereinigt, nämlich die 1817 gegründeten Siedlungen „Neu Tiflis“ und Alexanderdorf, die im heutigen Stadtteil Didube-Tschugureti liegen. 1941 wurden auf Erlass Stalins die deutschstämmigen Bewohner nach Sibirien deportiert, womit deren Präsenz in Tiflis jählings endete. Im Bereich des ehemaligen Neu Tiflis befinden sich heute die Botschaft Deutschlands und das ehemalige Wohnhaus der Friedensnobelpreisträgerin Bertha von Suttner. Da sich zahlreiche Vertretungen von Wirtschaftsunternehmen und Büros von politischen sowie Hilfsorganisationen aus Deutschland, darunter ein Goethe-Institut, in Tiflis befinden, hat sich eine deutsche Gemeinde entwickelt, für die 2010 die Eröffnung einer „Deutschen Internationalen Schule“ erfolgte.

Das Stadtoberhaupt von Tiflis ist der Bürgermeister. Stellvertreter des Bürgermeisters ist der Premier. Von 1991 bis 2005 wurden die Amtsinhaber vom georgischen Präsidenten ernannt. Seit 2006 werden sie vom Stadtrat gewählt.

Bürgermeister ist Giorgi Ugulawa (Vereinte Nationale Bewegung). Er war zuvor Leiter der Präsidialverwaltung. Seine Vorgänger seit der Unabhängigkeit Georgiens waren: Otar Litanischwili (1992–1993), Konstantine Gabaschwili (1993), Nikolos Lekischwili (1993–1995), Badri Schoschitaischwili (1995–1998), Iwane Sodelawa (1998–2004) und Surab Tschiaberaschwili (2004–2005). Amtierender Premier ist Temur Kurchuli. Ihm untersteht die Stadtverwaltung (georgisch "Mtawroba") mit rund 1700 Mitarbeitern in 40 verschiedenen Abteilungen. Es gehört zu seinen Aufgaben, das Stadtbudget festzulegen.

Bürgermeister und Stadtverwaltung werden von dem alle vier Jahre gewählten Stadtrat (georgisch "Sakrebulo") kontrolliert. Er hat 37 Mitglieder und tritt alle drei Monate zusammen. Der am 5. Oktober 2006 gewählte Stadtrat war bis Oktober 2010 im Amt. Mehrheitsfraktion im Stadtrat ist die "Vereinte Nationale Bewegung" mit 34 Sitzen (66,53 Prozent der Wählerstimmen). Jeweils einen Sitz haben das Wahlbündnis von "Republikanischer Partei" und "Konservativen" (12,04 Prozent), die "Georgische Arbeiterpartei" (10,65 Prozent) und die "Rechte Opposition" (6,08 Prozent). Stadtratsvorsteher ist Sasa Begaschwili. Sein Vorgänger von Juni 2002 bis Januar 2004 war Micheil Saakaschwili.

Tiflis hat seit der jüngsten Neugliederung 2013 zehn Stadtrajone (georgisch , "raioni"): Saburtalo und Wake im Westen, Krzanissi und Mtazminda im Süden, Didube und Tschughureti im Norden, Gldani und Nadsaladewi im Osten sowie Issani und Samgori im Südosten. Jeder Rajon hat eine Verwaltung, die von einem Gouverneur (georgisch , "gamgebeli") geleitet wird.

Die Rajone entstanden durch Aufteilung der seit den 1990er-Jahren existierenden Bezirke Didube-Tschughureti, Gldani-Nadsaladewi, Issani-Samgori, Mtazminda-Krzanissi und Wake-Saburtalo. Die seit 2007 bestehenden Rajone Dsweli Ubani (deutsch "Altstadt") und Didgori wurden zugleich aufgelöst und deren Territorien auf die umliegenden, bereits vor 2007 bestehenden Rajone verteilt: die zentral gelegene Altstadt/Dsweli Ubani auf Issani, Krzanissi, Mtazminda, Tschughureti und Wake, das 2007 aus Teilen von Mtazminda und Wake sowie einem größeren, zuvor zum Rajon Gardabani gehörenden Gebiet im Südwesten des Stadtgebietes gebildete Didgori wieder auf die Rajone Mtazminda und Wake.

Neben dem eigentlichen Stadtgebiet umfasst das zur Stadt gehörige, einer Region gleichgestellte Gebiet noch vier Klein- oder Minderstädte (georgisch "daba", ), hervorgegangen aus in der sowjetischen Periode gebildeten Siedlungen städtischen Typs, sowie 22 andere Ortschaften in der Umgebung mit insgesamt 46.435 Einwohnern (2014). Diese Orte standen bis zur Bildung der Munizipalitäten in Georgien 2007 unter Verwaltung der umliegenden Rajonen Gardabani und Mzcheta. Seither sind sie verwaltungstechnisch dem jeweils angrenzenden Stadtrajon von Tiflis unterstellt und liegen auf dem somit erweiterten Gebiet der Hauptstadtregion Tiflis.

2014 verteilten sich die Einwohner wie folgt auf die Rajone und Orte im umliegenden Gebiet (Einwohnerzahlen in Klammern, Minderstädte "kursiv"):

Tiflis unterhält mit folgenden Städten eine Städtepartnerschaft.


Die Verbindung mit Saarbrücken gehörte zu den ersten deutsch-sowjetischen Städtepartnerschaften. Im März 2005 schloss Tiflis ein Kooperationsabkommen mit der kasachischen Hauptstadt Astana.

Das Straßennetz von Tiflis befindet sich allgemein in einem schlechten bis sehr schlechten Zustand. Insbesondere in den Vororten sind einstmals gepflasterte Straßen aufgrund unterbliebener Reparaturarbeiten äußerst ländlich anmutenden ungepflasterten Fahrbahnen gewichen. In der Innenstadt hingegen wird der Zustand der Straßen, insbesondere der der Hauptverkehrsachsen, internationalen Standards gerecht. Wegen der gebirgigen Geographie verträgt das Tifliser Straßennetz nach Berechnungen der UNO höchstens 4200 bis 4500 Fahrzeuge pro Stunde. Weil die Anzahl der privaten Kraftfahrzeuge, die als Statussymbole einen hohen Stellenwert besitzen, seit den 1990er Jahren stetig zugenommen hat, ist das Straßennetz häufig überlastet und Verkehrsstaus gehören zum Alltag. Die Regierung setzte teilweise 25 Hauptverkehrsstraßen und Kreuzungen in Stand.

Der öffentliche Personennahverkehr wird in Tiflis von einer Untergrundbahn, einem Omnibus-Liniennetz – beide von stadteigenen Betrieben unterhalten – und einem städtisch regulierten, aber privat betriebenen Minibus-Liniennetz (Marschrutka) sowie nicht zuletzt von unzähligen privatwirtschaftlich agierenden und offenbar kaum regulierten Taxiunternehmen bestritten. Noch vereinzelt verkehrende Seilbahnen ergänzen dieses öffentliche Transportsystem. Schienengebundenen öffentlichen Nahverkehr gibt es so gut wie nicht. Das Straßenbahnnetz, 1883 als Pferdestraßenbahn begründet und ab 1904 elektrifiziert, wurde am 4. Dezember 2006 stillgelegt. Eine neue Straßenbahn mit neuen Fahrzeugen soll demnächst in Betrieb genommen werden. S-Bahn-Verkehr gibt es mit Ausnahme eines sporadisch vom Zentralbahnhof zum Flughafen verkehrenden Zuges nicht.
Seit 1966 gibt es in Tiflis ein U-Bahn-Netz. Es umfasst zwei Linien und hat eine Länge von 26,3 Kilometern. Eine dritte Linie soll sich ab den späten 1980er Jahren im Bau befunden haben, der aufgrund fehlender Finanzmittel nach 1991 jedoch bis auf Weiteres wieder eingestellt wurde. Die U-Bahn verkehrt von 6 bis 24 Uhr, in Spitzenzeiten im Drei-Minuten-Takt, und verbindet die Vororte mit dem Zentrum. Der Fahrpreis beträgt 50 Tetri und kann an allen Stationen an Schaltern entrichtet werden. 2008 ist eine RFID-Karte eingeführt worden, die nach Aufladung an einem der Schalter einen elektronisch gesteuerten Zugang zur Metro gewährt. Seit 2011 ist diese Karte für die Nutzung der Metro obligatorisch. Sie kann für einen Lari an jedem Schalter erworben und wieder zurückerstattet werden. Innerhalb eines Verkehrstages gibt es für den Nutzung Skaleneffekte bei der Preisberechnung: So kostet die erste Entwertung 50 Tetri, die zweite 30 Tetri und jede weitere 20 Tetri am Tag. Das Ticket ist in der Metro und in Bussen der Tifliser Verkehrsbetriebe gültig. Tages-, Wochen-, Monats-, Jahreskarten oder andere Abo-Angebote werden nicht angeboten.

Das traditionelle öffentliche Verkehrsmittel war seit 1937 der Oberleitungsbus. In den 1970er und 1980er Jahren verkehrten in der Hauptstadt rund 300 Oberleitungsbusse der Marke Škoda. 1990 gab es davon noch 137 Stück, deren Anzahl bis 2004 auf 80 sank, von denen allerdings lediglich 40 funktionsfähig waren. Der Oberleitungsbusverkehr wurde am 4. Dezember 2006 eingestellt.

2004 wurden erstmals wieder neue Omnibusse für ein neu errichtetes Busliniennetz angeschafft. Die Europäische Bank für Wiederaufbau und Entwicklung hat der Tifliser Verkehrsgesellschaft 2005 einen Kredit über 3,1 Millionen Euro gewährt, mit dem Omnibusse aus der Ukraine und aus den Niederlanden erworben worden sind. Diese mit einem gelben Anstrich versehenen Omnibusse bedienen seither ein Liniennetz, das einen preisgünstigen öffentlichen Transport bis in entlegenere Stadtteile garantiert.

Die seit den 1990er Jahren verkehrenden privaten Kleinbusse, "Marschrutki" oder Minibus genannt, stellen das vorletzte Glied des öffentlichen Transportwesens jenseits der Omnibuslinien dar. Es handelt sich meist um aus Westeuropa importierte, mit sechzehn Sitzplätzen ausgestattete, handwerklich umgebaute Kleintransporter. Sie verkehren in von zunächst 72 auf inzwischen 223 von der Stadtverwaltung festgelegten Linien von frühmorgens bis etwa 23 Uhr und stellen ein preisgünstiges öffentliches Transportmittel dar, das teils in Konkurrenz zu den Omnibuslinien steht, teils das Busliniennetz bis in die entlegensten Stadtteile ergänzt. Der behördlich festgelegte Fahrpreis in Höhe von 80 Tetri, für gewisse Kurzstrecken am Stadtrand 40 Tetri, wird bei Erreichen des Ziels beim Fahrer entrichtet. In Marschrutki neueren Typs, erkennbar an ihrer gelben Farbe und dem digitalen Linienschild, ist ebenfalls die Bezahlung mit RFID-Karte möglich.

Als letztes Glied des öffentlichen Transportwesens fungiert das Taxigewerbe. In Tiflis gibt es eine überproportional hohe Anzahl von Taxis, die rund um die Uhr in der gesamten Stadt verkehren. Sie werden üblicherweise vom Straßenrand aus herangewinkt; es gibt kaum feste Taxistehplätze. Der Fahrpreis für das Taxi ist deutlich höher als der für die Omni- und Minibusse und wird mit dem Fahrer, möglichst vor Fahrtantritt, individuell ausgehandelt (innerhalb des Stadtgebietes je nach Entfernung etwa zwischen 2 und 10 Lari). Da sich offenbar jeder Fahrzeugbesitzer als Taxiunternehmer betätigen kann, ist die Zahl der Taxis im Tifliser Straßenbild exorbitant hoch und der technische Zustand der Fahrzeuge höchst unterschiedlich. Seit 2008 erscheinen Taxiketten auf dem Markt, deren Fahrzeuge technisch und bei der Ausstattung einen Standard nach internationalen Maßstäben garantieren wollen und über Taxameter verfügen.
Eine Besonderheit des Tifliser Nahverkehrs waren öffentliche Hängeseilbahnen, die das Stadtzentrum beziehungsweise tiefer gelegene Teile mit den Bezirken auf den Plateaus oberhalb der steilen Berghänge verbanden. Wegen fehlender Mittel zur Instandhaltung war bis 2009 von ehemals einem Dutzend Linien nur noch eine einzige im Stadtteil Wake zum Schildkrötensee hinauf in Funktion, die zudem eher touristischen Zwecken dient. Seit 2010 ist diese außer Betrieb. Am 18. Juni 2012 wurde eine neu errichtete Seilbahn eröffnet, die den Rike-Park mit der Nariqala-Festung verbindet und ebenso eher touristisches Publikum ansprechen dürfte. Gebaut wurde die moderne Gondelbahn vom italienischen Unternehmen Leitner ropeways.

Die Stadtverwaltung ist derzeit bemüht, die hohe Zahl von Kleinbussen und Taxis zu reduzieren, um Staus aufzulösen und die überaus hohe Schadstoffbelastung der Luft insbesondere im tiefer gelegenen Innenstadtbereich zu verringern.

Seit 1872 ist Tiflis an das Eisenbahnnetz angeschlossen, das heute von der Georgischen Eisenbahn betrieben wird. Vom Zentralbahnhof aus, dessen Hauptgebäude bis 2010 grundsaniert und wiedereröffnet wurde, verkehren regelmäßig Züge unter anderem nach Batumi, Bordschomi, Kutaissi, Poti sowie international nach Baku und Jerewan. Von Tiflis aus verkehren von verschiedenen Omnibushöfen (Didube, Zentralbahnhof, Isani) sowohl Busse als auch Minibusse zu zahlreichen Zielorten innerhalb Georgiens.

Der Flughafen von Tiflis liegt 20 Kilometer südöstlich der Innenstadt in Lotschini und verfügt seit 2007 über ein internationalen Standards gerecht werdendes Terminalgebäude. Er ist mit dem Bus an die Innenstadt angebunden. Zusätzlich verkehren seit 2007 Züge zum Flughafen.

Mit der Eisenbahn können Ziele in Armenien und Aserbaidschan erreicht werden. Die Bahnverbindung von Tiflis nach Russland auf dem Landweg ist wegen des Konflikts mit Russland unterbrochen.

Per Omnibus sind von Tiflis aus Ziele in Griechenland (Athen), Russland (Moskau) sowie allen Nachbarländern Georgiens zu erreichen.

Tiflis verfügt über eine vielfältige Theaterszene mit 33 Bühnen. Zu den wichtigsten zählen das 1845 gegründete "Gribojedow Theater für russisches Drama", das 1921 gegründete "Staatliche Akademische Theater Schota Rustaweli", das 1851 gegründete "Staatliche Sachari-Paliaschwili-Theater für Oper und Ballett" sowie die zwischen 1969 und 1971 erbaute
„Große Halle“ des "Staatlichen Sinfonieorchesters Georgiens". Daneben gibt es Theater der armenischen, aserbaidschanischen, ossetischen und jüdischen Volksgruppen. Es gibt zwei Jugend-, ein Komödien-, ein Musical-, ein Filmschauspieler-, ein Pantomimen- und ein Marionettentheater. Am Heldenplatz steht der im stalinistischen Baustil errichtete "Zirkus", Heimat des "Georgischen Staatszirkus".

Am 16. November 1896 wurde in Tiflis das erste Kino Georgiens eröffnet. In den 1920er Jahren wurde die Stadt zum Standort der Studios von "Kartuli Filmi" und Produktionsort aufwändiger Spielfilme. Zwei größere Kinos von internationalem Standard zeigen in der Innenstadt aktuelles internationales Kino. Im Spätherbst findet in ihnen jährlich das "Tbilisi International Film Festival" statt.

1972 wurde in Tiflis die Filmfakultät am "Schota-Rustaweli-Theaterinstitut", das spätere "Staatliche Georgische Institut für Theater und Film" gegründet. Tiflis ist Sitz des "Nationalen Zentrums für Cinematografie".

Tiflis hat zwölf große Museen. Die bedeutendsten sind das "Staatliche Museum der Künste Georgiens" nahe dem Freiheitsplatz und das "Staatliche Simon-Dschanaschia-Museum Georgiens" am Rustaweli-Boulevard. Ersteres beherbergt Gold- und Silberschätze der georgischen Könige, darunter das Brustkreuz der Königin Tamara sowie Zeugnisse georgischer Malerei des 18., 19. und 20. Jahrhunderts. Das zweite zeigt Dokumente der Geschichte Georgiens und des Kaukasus von der Altsteinzeit über die Antike bis zur Gegenwart. Eine umfangreiche Sammlung der georgischen Maler Niko Pirosmani und Lado Gudiaschwili sowie des Bildhauers Iakob Nikoladse befinden sich in der "Staatlichen Galerie der Künste" (Rustaveli Gamsiri 11). 

Das "Geldmuseum" der Georgischen Nationalbank präsentiert eine Schau zur Geschichte regionaler Zahlungsmittel, die vier Jahrtausende zurückreicht. Das "Ethnographische Museum" ist ein Freilicht-Museum, das das georgische Leben in verschiedenen Epochen zeigt. 22 private Kunstgalerien und sechs Ausstellungshallen präsentieren regelmäßig zeitgenössische georgische Kunst.

Tiflis ist Georgiens wissenschaftliches Zentrum. Die "Staatliche Universität Tiflis" (TSU) wurde 1918 gegründet. Heute studieren dort rund 35.000 Studenten an fünf Fakultäten. Die Universitätsbibliothek umfasst über 3,7 Millionen Bücher und Periodika. Daneben beherbergt Tiflis die "Staatliche Ilia-Universität", die "Georgische Technische Universität" (GTU), die "Georgische Agrarwissenschaftliche Universität" (GAU), die "Staatliche Medizinische Universität" und die "Staatliche Universität für Sprache und Kultur".

Außerdem gibt es verschiedene private Hochschulen, darunter die "Georgische Universität für Sozialwissenschaften", die "Grigol-Robakidse-Universität", "Internationale Schwarzmeer-Universität", die "European School of Management", das "Georgian Institute of Public Affairs" (GIPA) und die "Caucasus School of Business " (CSB). In der Hauptstadt sind mehr als 20.000 Menschen in Forschungseinrichtungen beschäftigt.

Die "Georgische Akademie der Wissenschaften" hat zehn wissenschaftliche Abteilungen und 63 Forschungsinstitute. Sie betreibt Grundlagenforschung und koordiniert die Forschung in Georgien.

Seit Dezember 2003 wird die Denkmalpflege in Tiflis mit Hilfe des Europarats reorganisiert, um möglichst viele Baudenkmale der alten und der modernen Stadt zu erhalten. Tiflis bietet dafür sehr günstige Voraussetzungen, da die Stadt seit der Invasion der Perser 1795 kaum beschädigt worden ist. Auf der anderen Seite ist der Zustand der Gebäude derart marode, dass der Zerfall immer wieder zu mehr unwiederbringlicher Zerstörung historischer Bausubstanz führt, als zur Verfügung stehende Finanzmittel für die Restaurierung verhindern können. Ein bedeutender archäologischer Fundplatz befindet sich in Grakliani.

In der Stadt ist der georgische Fußball-Rekordmeister (16 Titel) und zweimalige sowjetische Meister Dinamo Tiflis ansässig, der als bekanntester Verein Georgiens gilt. Ebenfalls lange in der ersten Liga des Landes spielte WIT Georgia Tiflis, der bislang zweimal die Meisterschaft des Landes erringen konnte. Derzeit spielt WIT jedoch in der zweiten Liga.

Auf dem rechten Ufer der Kura, über der Altstadt, liegen die Ruinen der Festung Nariqala aus dem 3. Jahrhundert. Oberhalb der Wehranlagen steht die Monumentalstatue "Kartlis Deda" (dt. "Mutter Georgiens"). Darunter erstreckt sich ein 128 Hektar großer "Botanischer Garten". Er wurde 1845 auf dem Gelände des Festungsparks angelegt.

Die Altstadt mit engen, gewundenen Kopfsteinpflaster-Straßen soll zum Weltkulturerbe der UNESCO erklärt werden. Dort liegen die Sioni-Kathedrale aus dem 5. Jahrhundert, die Metechi-Kirche der georgischen Könige aus dem 13. Jahrhundert, die Antschischati-Basilika aus dem 6. Jahrhundert und die Große Synagoge. Durch die Altstadt führte einst die "Seidenstraße". Noch heute stehen ihre alten Karawansereien. Die Häuser sind zumeist aus Holz und haben geschnitzte überhängende Balkone. Im Bäderviertel "Abanotubani" finden sich Schwefelbäder des 17. Jahrhunderts sowie eine Moschee.

An die Altstadt schließen sich im Stadtteil "Garetubani" die Prachtbauten des 19. Jahrhunderts an. Herzstück ist die mit Platanen gesäumte Flaniermeile Rustaweli-Boulevard zwischen "Freiheitsplatz" und "Platz der Republik". An ihm befinden sich unter anderem die Kaschweti-Kirche, die Staatliche Gemäldegalerie, das Historische Museum, das Staatliche Rustaweli-Theater, die Oper und das Parlamentsgebäude. Die Kura wird von breiten Uferboulevards gesäumt, an denen jedoch das Auto auf mehrspurigen Fahrbahnen dominant gegenüber den Fußgängern auftritt.

Die Stadtteile am Berg Mtazminda sind durch eine Standseilbahn erschlossen, die Anfang der 2010er Jahre modernisiert wurde. Auf halber Höhe liegt das Pantheon, ein Friedhof, auf dem bedeutende Georgier und Russen begraben sind, unter ihnen die Schriftsteller Ilia Tschawtschawadse, Akaki Zereteli und der Diplomat Alexander Gribojedow. Auf dem flachen Gipfel des Berges wurde ein großer Park angelegt, der 2009 zum Vergnügungspark erweitert worden ist. Der Fernsehturm steht als von allen Teilen der Stadt sichtbares Wahrzeichen über der Stadt und ist nachts aufwändig beleuchtet.

Der "Tschawtschawadse Boulevard" im Bezirk "Wake" beherbergt das Hauptgebäude der Staatlichen Universität, Banken, elegante Modeläden und exklusive Bars. Er ist die bevorzugte Einkaufsmeile der neuen Wohlstandschicht in Georgien.

Am linken Ufer des Flusses liegen im 20. Jahrhundert erbaute kulturelle und staatliche Institutionen. Dort stehen das größte Kirchengebäude Transkaukasiens, die Sameba-Kathedrale, die "Staatliche Medizinische Universität", verschiedene wissenschaftliche Institute, das "Boris-Paitschadse-Stadion", Heimspielstätte von Dinamo Tiflis, und der Hauptbahnhof.

Vor den Toren der Stadt, unterhalb des "Kus Tba" (dt. "Schildkrötensee"), liegt das "Ethnographische Museum". Es handelt sich um ein Freilichtmuseum, das in einem weitläufigen Areal Zeugnisse des ländlichen Bauens, Wohnens und Lebens aus allen Teilen Georgiens präsentiert. Hier finden sich historisch und kulturell erhaltenswerte originale Gebäude wie beispielsweise ein swanetischer Wehrturm, die wieder errichtet wurden, weil sie anderenfalls verfallen wären.

Im Flusstal der Were im Stadtzentrum befindet sich der Zoo Tiflis, der älteste und größte Tierpark in Georgien, der am 14. Juni 2015 durch eine Überschwemmung schwer beschädigt wurde.





</doc>
<doc id="5267" url="https://de.wikipedia.org/wiki?curid=5267" title="Tampere">
Tampere

Tampere ( ) ist eine Großstadt im südwestlichen Finnland. Mit Einwohnern ist Tampere die größte Stadt der Region Pirkanmaa und drittgrößte Stadt des Landes (nach Espoo, das zur Agglomeration Helsinki gehört). Wegen ihrer vielseitigen Industrie wird sie häufig auch "Manchester des Nordens" genannt (finnischer Spitzname: ""). 

Im Ballungsraum Tampere beträgt die Einwohnerzahl 330.711 (Stand: 31. Dezember 2016).

Tampere ist die größte Binnenstadt der Nordischen Länder. 

Tampere liegt zwischen zwei Seen, dem Näsijärvi und dem Pyhäjärvi. Die Seen haben einen Niveauunterschied von 18 Metern und werden durch die 945 Meter lange "Tammerkoski"-Stromschnelle verbunden, die als Energiequelle genutzt wird.
Die Stadt befindet sich etwa auf einer Breite mit Söderhamn in Schweden und Lillehammer in Norwegen. Am 21. Juni findet der Sonnenaufgang um 03:41, der Sonnenuntergang um 23:12 statt. Am 22. Dezember hingegen geht die Sonne erst um 09:43 auf und verschwindet um 15:04 wieder hinter dem Horizont. Die Stadt ist Zentrum der Region Pirkanmaa, die auch als "Tampere-Region" bezeichnet wird.

Entfernungen zu anderen finnischen Städten:

Die Gesamtfläche der Stadt beträgt 690,6 km², davon nehmen 522,7 km² Land- und 167,9 km² Wasserflächen ein. Da große Teile des Stadtgebiets ländlich geprägt sind, liegt die Bevölkerungsdichte nur bei 385 Einwohnern je km².

Angrenzende Gemeinden sind im Westen Nokia und Ylöjärvi, im Süden Pirkkala und Lempäälä sowie im Osten Kangasala und Orivesi. Da sich das Gebiet der Stadt sehr weit nach Norden ausdehnt, grenzen dort Kuru und Ruovesi, deren Ortszentren bereits 70 bis 80 Kilometer von Tamperes Innenstadt entfernt sind, an.

Tampere gliedert sich, vereinfacht gesehen, in sechs Stadtbezirke, und zwar in ' (‚Stadtzentrum‘), ' (‚Westbezirk‘), ' (‚Nordbezirk‘), ' (‚Ostbezirk‘), ' (‚Südbezirk‘) und ' (‚Südostbezirk‘).

Die westlich der Stromschnelle Tammerkoski gelegenen Stadtteile "Amuri, Pyynikki, Santalahti" und "Yläpispala" erstrecken sich über den Landrücken "Pispala" und gehören zum "Läntinen piiri", also zum westlichen Bezirk.

Nördlich der Innenstadt befinden sich die Stadtteile "Lapinniemi, Lappi" sowie Petsamo, und östlich "Kauppi, Kaleva, Kissanmaa" und "Vuohenoja". Obwohl sie sich nach Norden erstrecken, gehören "Kolunkylä, Kämmeniemi" und "Teisko" zum Ostbezirk.

Weiter südlich liegt der Stadtteil "Hervanta", der zum "Kaakkoinen piiri" gehört. Zum "Eteläinen piiri" gehören "Härmälä, Hatanpä, Järvensivu."

Seit 2011 entsteht westlich von Hervanta der neue Stadtteil Vuores.

Im Januar 2003 lag die Durchschnittstemperatur bei −11,4 °C, die Höchsttemperatur bei +4,8 °C und die Tiefsttemperatur bei −32 °C. Im Juli 2003 erhöhte sich die durchschnittliche Temperatur auf +20,2 °C, wobei das Thermometer eine Höchsttemperatur von +30,6 °C sowie eine Tiefsttemperatur von +8,8 °C erreichte. Die durchschnittliche Niederschlagsmenge lag im Jahr 2002 bei 500 mm.

Bereits um 1000 war die Region besiedelt. Ab dem 15. Jahrhundert gab es Gutshöfe und Mühlen, im 17. Jahrhundert wurde erstmals ein Marktplatz genannt.

Am 1. Oktober 1779 wurde die heutige Stadt als Nahtstelle zwischen Süd- und Mittelfinnland vom schwedischen König Gustav III. gegründet. Ziel war es, eine neue Handels- und Fabrikstadt zu errichten. Bereits wenige Jahre später, 1783, siedelte hier die erste finnische Papierfabrik, und 1820 gründete der Schotte James Finlayson eine Baumwollfabrik an den damals noch ungenutzten Stromschnellen zwischen Näsijärvi und Pyhäjärvi, die Grundstein für die spätere Textilindustrie wurde und in deren umgebauten Backsteinfabrikhallen sich heute einige Museen, Cafés und ein Kino befinden. Schon im 19. Jahrhundert war Tampere das finnische Innovationszentrum schlechthin. So nahm 1842 die erste finnische Papiermaschine in der Fabrik "J. C. Freckell & Son." ihre Produktion auf, und 1882 wurden bei Finlayson die ersten Glühbirnen Skandinaviens hergestellt.

Die erste Eisenbahnstrecke erreichte Tampere am 22. Juni 1876 aus Richtung Hämeenlinna. Am 29. September 1883 folgte eine Verbindung über Orivesi nach Vaasa und am 4. November 1895 nach Pori.

1865 wütete in Tampere ein Stadtbrand, der das Wachstum jedoch nicht aufhalten konnte. Als Folge der starken Industrialisierung war dann zu Beginn des 20. Jahrhunderts Tampere das finnische Zentrum der Arbeiterbewegung. 1905 und 1906 nahm hier Lenin an Konferenzen der Bolschewiki teil.

Tampere war eine der strategisch wichtigen Städte während des Finnischen Bürgerkriegs (28. Januar bis 15. Mai 1918). Am 6. April nahmen die Streitkräfte der Weißen bei der Einnahme der Stadt etwa 10.000 Soldaten der Roten gefangen.

1923 wurde Tampere Bischofssitz. Im Winterkrieg und im Fortsetzungskrieg erfolgten verschiedene sowjetische Luftangriffe auf die Stadt, die insgesamt 18 Todesopfer unter der Zivilbevölkerung forderten, außerdem wurden 1,2 % der Wohnungen zerstört und 2,1 % schwer beschädigt. In den 1960er Jahren begann der Niedergang der Textilindustrie, die den Anforderungen des Weltmarktes nicht mehr gewachsen war. Ab etwa 1970 wurden die alten Fabriken allmählich stillgelegt und für andere Zwecke genutzt.

1947 wurden "Messukylä", 1950 "Lielahti", 1966 "Aitolahti" und schließlich 1972 "Teisko" eingemeindet.

Interessant ist, dass die Stadt trotz der wirtschaftlichen Umstrukturierung in den 1970er Jahren nie von Abwanderung und einem daraus resultierenden Bevölkerungsrückgang betroffen war, wie das in jener Zeit beispielsweise bei den ähnlich stark industrialisierten Städten des Ruhrgebiets typisch war.

Dennoch gibt es zwei Phasen in der Geschichte der Stadt, die das Wachstum zumindest hemmten. Neben der erwähnten Zeit der wirtschaftlichen Umstrukturierung wären das die Jahre der Revolution und des Bürgerkriegs.
Entwicklung der Einwohnerzahl "(ab 1960 31. Dezember)":

Gilt nur für die Einwohner innerhalb der Stadtgrenzen. In der städtischen Siedlung Tampere wohnen 281.092 Einwohner (Schätzung für den 1. Januar 2005).

Von den 199.823 Einwohnern am 31. Dezember 2002 waren 104.540 (52,3 %) Einwohner weiblich und 95.283 (47,7 %) männlich. Lediglich 990 Einwohner, das entsprach einem Anteil von 0,5 % an der Gesamtbevölkerung, waren schwedischsprachig.

Die natürliche Bevölkerungsentwicklung wies im Jahr 2002 einen Geburtenüberschuss von 215 Einwohnern auf. Auch der Wanderungssaldo war mit 1.445 Einwohnern positiv. Diese Entwicklung ist auf die in Finnland weiterhin anhaltende Urbanisierung zurückzuführen. Ein Vergleich mit der Entwicklung in anderen Groß- und Mittelstädten des Landes befindet sich in der Liste der Städte in Finnland.

Nach Schätzungen wird Tampere im Jahr 2020 229.000 Einwohner haben.

Unter finnischer Präsidentschaft fand am 15. und 16. Oktober 1999 in Tampere ein Sondergipfel der EU zum Bereich „Justiz und Inneres“ statt. Zentrales Thema war die Asyl- und Migrationspolitik.

Wie in allen finnischen Städten ist der Stadtrat die höchste Entscheidungsinstanz in Tampere. Die 67 Abgeordneten dieses Rates werden auf vier Jahre gewählt.

Die stärkste Partei in Tampere ist die konservative Sammlungspartei, gefolgt von den Sozialdemokraten. Auch der Grüne Bund, die rechtspopulistischen Basisfinnen und das Linksbündnis sind mit Wahlergebnissen im zweistelligen Prozentbereich verhältnismäßig stark. Die dritte große Partei des Landes, die finnische Zentrumspartei, spielt wie in anderen Großstädten nur eine untergeordnete Rolle.

Finnische Kommunen haben überwiegend einen obersten Beamten (finn. "kaupunginjohtaja") der ungefähr einem Oberstadtdirektor entspricht. Hierbei handelt sich um ein unbefristetes Beschäftigungsverhältnis (obwohl Entlassungen, oft mit nachfolgendem Rechtsstreit bei politischen Querelen keine Seltenheit sind). Seit einer Gesetzesänderung von 2006 kann die Kommune auch das sogenannte Bürgermeistermodell wählen, was bedeutet, dass der Stadt- oder Gemeinderat einen Bürgermeister (finn. pormestari) für nicht länger als seine eigene Legislaturperiode wählt. Tampere war die erste Stadt in Finnland, die 2007 von diesem Recht Gebrauch machte, nachdem der damalige Oberstadtdirektor in den Ruhestand gegangen war. Von 2007 bis zu seinem Ruhestand 2012 hatte Timo P. Nieminen von der Sammlungspartei diesen Posten inne (Wiederwahl 2009). Anfang 2013 wurde seine Parteifreundin Anna-Kaisa Ikonen als Bürgermeisterin gewählt.

 Tampere unterhält seit 1960 mit Essen eine Städtepartnerschaft, seit 1961 auch mit Chemnitz. Die Städtepartnerschaften mit den beiden deutschen Städten wurden nicht wahllos geknüpft: sowohl Essen als auch Chemnitz zeigen in ihrer industriellen Entwicklung Parallelen zu Tampere auf.

Weitere Partnerstädte sind:

Die einstigen Hauptindustriezweige, namentlich die Textil-, Schuh-, Leder-, Papier- und Holzindustrie, sind bis heute weitestgehend durch andere Industriezweige verdrängt worden. Die meisten industriell Beschäftigten gab es 2001 mit 9.828 Personen (51,9 %) in der Metall- und Elektroindustrie.

Zwischen 1980 und 2001 ging der Anteil der in der Industrie beschäftigten Personen von 42,1 % auf 27,1 % zurück, während der Dienstleistungssektor mit 70,9 % gegenüber 53,9 % im Jahr 1980 auf dem Vormarsch war.

Sehr nachteilig ist die Entwicklung der Arbeitslosenquote verlaufen. Während sie 1970 nur 1,5 % betrug, lag sie auch nach Überwindung der Wirtschaftskrise Anfang der 1990er im Jahr 2001 mit 13,7 % noch über dem finnischen Gesamtdurchschnitt (9,2 %).

Neben Helsinki ist Tampere der wichtigste Verkehrsknotenpunkt Finnlands. Die PKW-Dichte lag 2002 bei 381 Kraftfahrzeugen je 1.000 Einwohner.

Im Oktober 2000 wurde das letzte Teilstück der Autobahn nach Helsinki fertiggestellt. Diese Autobahn gehört zur Europastraße 12, in die in Tampere die Europastraße 63 aus Richtung Nordostfinnland einmündet.
Tampere besitzt zwei Häfen, einen am Pyhäjärvi und einen am Näsijärvi. Die Tammerkoski-Stromschnelle selbst ist nicht schiffbar, deswegen beginnt und endet auch am Pyhäjärvi-Hafen die Silberlinie sowie am Näsijärvi-Hafen der Dichterweg.

Südwestlich der Stadt, bei Pirkkala, befindet sich der Flughafen Tampere-Pirkkala, von dem unter anderem Ryanair-Verbindungen nach Frankfurt-Hahn und Bremen bestehen. Weitere Fluggesellschaften, die den Flughafen anfliegen, sind Nordic Regional Airlines (für Finnair) und die SAS.
Die schnellsten landgebundenen Verbindungen nach Helsinki werden mit weniger als anderthalb Stunden durch die von der VR betriebenen Züge vom Typ Sm3 angeboten. Dabei handelt es sich um fahrplanmäßig bis zu 200 km/h schnelle Pendolinozüge. Der Bahnhof von Tampere verfügt über eine Verladestelle für Autoreisezüge. Einen Güterbahnhof gibt es in Viinikka.

Der Stadtbereich selbst wird von einem dichten Stadtbusnetz mit 27 Linien und einer Gesamtlänge von 370,5 km erschlossen. 2002 wurden 160 Stadtbusse unterhalten. Überlandbusse fahren fast alle finnischen Städte an. Sie beginnen und enden am Busbahnhof "(linja-autoasema)", der südwestlich vom Bahnhof liegt. Eine Straßenbahn existiert noch nicht, jedoch hat der Bau einer neuen Stadtbahn (Straßenbahn Tampere) 2016 begonnen (Eröffnung der ersten Stufe 2021 geplant). Das 238,8 Millionen Euro (mit Fahrzeugen 342 Mio. Euro) teure Bahnprojekt soll im Endausbau ein 56,7 km langes Netz umfassen, von denen 2,7 km Tunnelstrecken wären.

Tampere ist heute Sitz mehrerer High-Tech-Betriebe. Daneben sind in Tampere auch Unternehmen zu finden, welche in den klassischen Industriezweigen tätig sind, beispielsweise die Metsä-Board-Tako-Papierfabrik im Zentrum der Stadt, der Hersteller für Hubrettungs- und Hubarbeitsbühnen Bronto-Skylift oder der Maschinenbauer Glaston.

Zu Beginn der 1920er Jahre wurde die erste finnische Radiosendung in Tampere ausgestrahlt. Heute ist die Stadt Standort des zweiten öffentlich-rechtlichen finnischen Fernsehprogramms YLE TV2.

Die auflagenstärkste Tageszeitung der Stadt und Region ist das Aamulehti.

Im Schuljahr 2002/2003 gab es 57 Volksschulen und zehn Gymnasien.

Tampere verfügt über drei Universitäten und Hochschulen, wovon zwei 13.000 (Universität Tampere & Technische Universität Tampere) und eine 4.000 (PIRAMK University of Applied Sciences) Studenten haben. Neben der 1925 gegründeten Universität Tampere (UTA) im Stadtzentrum und der PIRAMK University of Applied Sciences im Norden der Stadt befindet sich im Stadtteil Hervanta die Technische Universität Tampere. Hervanta selbst ist eine typische Satellitenstadt mit 11.000 Wohnungen. Von den etwa 26.000 Einwohnern sind über 4.500 Einwohner Studenten. Das bekannteste und größte Studentenwohnheim in Hervanta ist das "Mikontalo". Für UTA-Studenten im Erasmus-Programm gibt es in der Innenstadt von Tampere das Studentenwohnheim "Tuomiokirkonkatu" und etwas nördlich des Stadtzentrums das Studentenwohnheim "Lapinkaari".
Im Rahmen des Erasmus-Programms findet seit 2000 jährlich eine Konferenz an der PIRAMK University of Applied Sciences statt, bei der Studierendengruppen aus ca. 10 EU-Ländern sich treffen, um aktuelle Entwicklung in den Bereichen Mobile and Interactive Marketing, IT for Convergence Internet Marketing & eBusiness Management vorzustellen und gemeinsam in interdisziplinären Workshops Projekte zu bearbeiten.

Das berühmteste Freilichttheater Finnlands befindet sich im Pyynikki-Park . Es besitzt eine drehbare Tribüne, die sich immer parallel zu einer Szene dreht.

Die beiden bedeutendsten Theater der Stadt sind jedoch das "Stadttheater", auch als TT-Theater bekannt, sowie das "TTT-Theater". Beide Einrichtungen verfügen über drei Bühnen und bieten sowohl internationale Klassiker als auch lokale Uraufführungen dar. 2004 beging das TT-Theater, ursprünglich ein Arbeitertheater, sein hundertjähriges Bestehen.

Nennenswerte Museen sind das Puppen- und Kostümmuseum, das Turn- und Gymnastikmuseum, das Eishockeymuseum sowie das Spionagemuseum. Ein Bild von den Lebensverhältnissen in der alten Arbeiterstadt Tampere vermittelt ein sozialgeschichtliches Museum, in dem die damaligen beengten Wohnverhältnisse anhand eines Häuserblocks dargestellt werden.

Beliebt ist das „Muumimuseo“ („Mumin-Museum“), das dem Werk der finnlandschwedischen Künstlerin und Autorin Tove Jansson gewidmet ist.

Neben dem TTT-Theater befindet sich ein Lenin-Museum. In den beiden Räumen, in denen man etwas über sein Leben im Allgemeinen sowie über seine Zeit in Finnland im Besonderen erfahren kann, hat sich Lenin tatsächlich aufgehalten. Erwähnenswert ist in diesem Zusammenhang auch, dass 1905 Lenin hier erstmals mit Josef Stalin zusammentraf.

Im Särkänniemi-Park (siehe Parks) befindet sich das "Sara Hildén Art Museum". Dabei handelt es sich um ein Museum für zeitgenössische Kunst.

Die Stadt besitzt mit der Tampere-Halle das größte Konzert- und Kongresszentrum Nordeuropas. Sie liegt gegenüber der Universität Tampere. Bekanntestes Stück, das in der Tampere-Halle aufgeführt wird, ist die Oper „Die letzten Versuchungen“ von Joonas Kokkonen.
Hier findet auch das jährliche Tampere Beatles Happening statt.

Bekannte Musikgruppen sind das Philharmonische Orchester Tampere und Bands wie Negative oder Lovex.

Bemerkenswert ist die schachbrettartige Anlage der Innenstadt . Die übrigen Stadtteile behielten dieses Muster nicht konsequent bei. Der Entwurf der Innenstadt stammt von dem deutsch-finnischen Architekten Carl Ludwig Engel. Er sah u. a. die breite Esplanade "Hämeenpuisto" vor, die damals im Notfall die Ausbreitung von Feuer verhindern sollte und heute von vielen über 100 Jahre alten Bäumen gesäumt wird. Die meisten Gebäude der Innenstadt stammen allerdings aus der Zeit nach 1900. An der Westseite des Zentralplatzes, der noch bis in die 1930er die Funktion des Marktplatzes besaß, befindet sich auch das Rathaus, das heute allerdings nur noch repräsentativen Zwecken dient. Von dem 1890 fertiggestellten Bau herab wurde 1905 das Manifest gegen die russische Unterdrückung verlesen.

Über den Zentralplatz verläuft die "Hämeenkatu", die wichtigste Straße und damit die „Lebensader“ der Stadt. Sie beginnt im Westen an der Hämeenpuisto, an der sich auch die 1986 erbaute Stadtbibliothek, die architektonisch der Form des Auerhahns nachempfunden wurde, und die 1881 errichtete Alexanderkirche "(Aleksanterin kirkko)" befindet. Ihr östliches Ende findet sich am Bahnhof "(asema)", der bereits am anderen Ufer des Tammerkoskis liegt. An der Hämeenkatu 13 liegt die 1901 erbaute und unter Denkmalschutz stehende Markthalle "Kauppahalli" der Stadt. Die "Hämeensilta-Brücke", mit deren Hilfe die Hämeenkatu den Tammerkoski überquert, ist mit vier Statuen von Wäinö Aaltonen verziert. Sie stammen von 1929, bilden eine finnische Jungfrau, einen Jäger, einen Händler sowie einen Steuereintreiber nach und stellen damit das früher im Stadtgebiet ausgeübte Gewerbe dar.

Südlich des Zentralplatzes liegt der "Laukontori", der Hafen des Pyhäjärvi und heutige Marktplatz. Unmittelbar neben dem Laukontori mündet der Tammerkoski in den Pyhäjärvi. Der Tammerkoski selbst fließt durch zwei Wehre mit sich anschließenden Turbinenhallen. An der nördlichen Schleuse befinden sich die ehemaligen Fabrikhallen von Frenckell, Finlayson (mit der Finlayson-Kirche) und "Tampella".

Der Dom von Tampere "(Tuomiokirkko)" ist die Kathedrale des Bistums Tampere und wurde zwischen den Jahren 1902 und 1907 im Stil der finnischen Nationalromantik gebaut und ist ein blau-grauer Granitbau. Er befindet sich am Rande des Zentrums nördlich des Bahnhofs. Der asymmetrische Außenbau lässt die Kirche wie ein Zauberschloss aussehen. Das Altarfresko „Auferstehung“ ist ein Werk von Magnus Enckell, die übrigen Fresken sowie die bunten Fenster stammen von Hugo Simberg.

Älteren Datums ist die Alte Kirche "(Vanha Kirkko)". Die gelbe Holzkirche entstand 1824 im Empirestil. Der vier Jahre später hinzugefügte Glockenturm mit Einzeigeruhr geht auf Pläne von Carl Ludwig Engel zurück. Die Kirche steht am Zentralplatz "(Keskustori)".

Aus neuerer Zeit (1964 bis 1966) stammt die Kaleva-Kirche "(Kalevan kirkko)" im gleichnamigen Stadtteil Kaleva, die als Grundriss das alte christliche Symbol des Fisches hat. Der Betonbau wirkt von außen zwar nicht besonders einladend, jedoch beeindruckt der hohe und helle Innenraum.

Der Friedhof Kalevankangas wurde 1880, der Friedhof Vatiala 1960 errichtet.

Größter Park der Stadt ist der "Pyynikki-Park". Er befindet sich im gleichnamigen Stadtteil, im Süden des Pispala-Landrücken, und beherbergt den 30 Meter hohen "Pyynikki-Aussichtsturm", der allerdings vom Näsinneula in den Schatten gestellt wird.

Eine Touristenattraktion ist der Särkänniemi-Park, der nicht nur über umfangreiche freizeitparkspezifische Einrichtungen verfügt, sondern auch ein Planetarium und einen 173 m hohen Turm mit Drehrestaurant "(Näsinneula)" beherbergt. Er steht auf dem Pispala-Landrücken, am Ufer des Näsijärvi. Früher gab es in dem Park ebenfalls ein Delfinarium, welches nun zu einem Indoor-Spielplatz umfunktioniert wurde.

Einen beliebten Treffpunkt stellen die Parks am linken und rechten Ufer des Tammerkoskis dar. Sie sind im Sommer mit roter Blumenpracht ausgeschmückt und bieten einen hervorragenden Blick auf den stattlichen Schornstein von 1870 sowie auf die heute noch teilweise als Wasserkraftwerke fungierenden Hallen der früheren Papierfabrik Frenckell.

Im Stadtteil Hatanpää, auf einem in den Pyhäjärvi hineinreichenden Landvorsprung, erstreckt sich ein elf Hektar großes Arboretum. Es besteht aus einer alten Parkanlage eines Gutshofs, einem Park mit etwa 500 Pflanzenarten sowie einem Steingarten. Auch ein Rosengarten, der seine volle Pracht im Spätsommer entfaltet, wurde angelegt.

Der Sport wird am stärksten durch die zwei Eishockeymannschaften "Tappara" und "Ilves" geprägt. Beide haben einen großen Einfluss auf die finnische Eishockeykultur. Sie gelten als die überhaupt erfolgreichsten Mannschaften in Finnland. Auch der Fußball braucht sein Licht nicht unter einen Scheffel zu stellen. So gewann das 1998 gegründete Tampere United in den Jahren 2001, 2006 und 2007 die finnische Veikkausliiga. Allerdings findet die Mannschaft international, wie alle übrigen Fußballmannschaften des Landes auch, nur wenig Beachtung. Sie spielt in dem Stadion hinter dem Busbahnhof, gegenüber dem Pyhäjärvi-Hafen. Weitere bedeutende Mannschaften sind die Zweitligisten Tampereen Pallo-Veikot, PP-70 Tampere und der ehemalige Meister Ilves-Kissat Tampere.

1967 trug Tampere gemeinsam mit Helsinki die Basketball-Europameisterschaft aus, und 1995 war die Stadt Austragungsort der Ruder-Weltmeisterschaften. In den Jahren 1979 und 2001 wurden in Tampere die Orientierungslauf-Weltmeisterschaften ausgetragen. Eine Bewerbung für die Olympischen Winterspiele 1976 blieb hingegen erfolglos.

In Tampere gibt es insgesamt zehn Strandbäder. Am nächsten zum Zentrum liegt das Strandbad am Näsijärvi-Ufer in Onkiniemi, unweit westlich des Särkenniemi-Vergnügungsparks, gefolgt von dem Strandbad am Sommertheater von Pyynikki. Es gibt vier Hallenbäder, davon eines in Hervanta.

Im Amateurboxen war Tampere bereits Austragungsort der Junioren-Europameisterschaft 1984, der Europameisterschaften 1981 und 2000, der Baltischen Meisterschaft 1990 und der 7. Weltmeisterschaft 1993. Zudem ist die Stadt jährlich Austragungsort des international stark besetzten Tammer-Turniers.

2004 konnte der SC Classic den Pokal im Floorball erzielen.

2008 war Tampere Gastgeber der Ringer-Europameisterschaften.

2009 hat Tampere das Europäische Olympische Winter-Jugendfestival organisiert.

Seit 2014 wird im Ratina-Stadion von Tampere der Speedway-Einzel-WM Grand Prix von Finnland ausgetragen.


Die bekannteste Spezialität in Tampere ist die "Schwarzwurst" (Mustamakkara), eine Grützwurst mit Blut, die heiß mit Preiselbeermarmelade gegessen wird. Man erhält sie warm u. a. in der Markthalle, am Hafen des Pyhäjärvi (Laukontori) oder auf dem Markt im Stadtteil Tammela (Tammelantori) sowie an der Wursttheke einiger größerer Supermärkte. Eine weitere lokale Spezialität ist auch das Hefebrot, im Tamperer Dialekt genannt „riävä“ (von finn. rievä).

Es gibt 2 Kleinstbrauereien: Die Gaststätte Plevna im Finlayson-Gelände und die Pyynikin käsityöläispansimo (wörtlich übersetzt "Pyynikkier Handwerksbrauerei") , deren Biere in ausgewählten Gaststätten der Stadt erhältlich sind.

Raimo Helminen – Väinö Linna – Matti Nykänen





</doc>
<doc id="5270" url="https://de.wikipedia.org/wiki?curid=5270" title="Thomas Mann">
Thomas Mann

Paul Thomas Mann (* 6. Juni 1875 in Lübeck; † 12. August 1955 in Zürich, Schweiz) war ein deutscher Schriftsteller und einer der bedeutendsten Erzähler des 20. Jahrhunderts.

Dem 1901 erschienenen ersten Roman "Buddenbrooks" folgten Novellen und Erzählungen wie "Tonio Kröger", "Tristan" und "Der Tod in Venedig". Der 1924 veröffentlichte Roman "Der Zauberberg", mit dem er die Tradition des europäischen Bildungsromans fortführte, zeigt Manns Gestaltungskunst: Der Erzähler wahrt eine skeptisch-ironische Distanz zu den Figuren, typische Konstellationen kehren leitmotivisch wieder, und es herrscht ein syntaktisch komplexer, anspruchsvoller Stil. Diese Merkmale prägen auch die folgenden Veröffentlichungen, unter denen die Novelle "Mario und der Zauberer", die Romantetralogie "Joseph und seine Brüder" sowie das Spätwerk "Doktor Faustus" hervorzuheben sind. Für die "Buddenbrooks" erhielt Thomas Mann 1929 den Nobelpreis für Literatur.

Weithin Beachtung fanden auch seine Stellungnahmen zu aktuellen politischen, gesellschaftlichen und kulturellen Fragen. Stand er der westlichen Demokratie zunächst skeptisch gegenüber, wandelte er sich zu Beginn der 1920er Jahre zu einem überzeugten Verteidiger der Weimarer Republik. Während der nationalsozialistischen Herrschaft emigrierte er 1933 in die Schweiz und 1938 in die USA, deren Staatsbürgerschaft er 1944 annahm. Seit 1952 lebte er wieder in der Schweiz.

Thomas Mann entstammte der angesehenen Lübecker Patrizier- und Kaufmannsfamilie Mann. Seine Frau Katia, geborene Pringsheim, inspirierte ihn zu mehreren seiner literarischen Figuren und Werke. Sein älterer Bruder Heinrich und vier seiner sechs Kinder, Erika, Klaus, Golo und Monika, waren ebenfalls Schriftsteller.

Thomas Mann war der zweite Sohn des Kaufmanns und Lübecker Senators Thomas Johann Heinrich Mann. Er wurde am 11. Juni 1875 in der Marienkirche zu Lübeck evangelisch getauft. Seine Mutter Julia (geborene da Silva-Bruhns) war mütterlicherseits brasilianischer Herkunft. Aus der Ehe gingen außer dem Bruder Heinrich (1871–1950) noch die Kinder Julia (1877–1927, Suizid), Carla (1881–1910, Suizid) und Viktor (1890–1949) hervor. Die Familie zählte zu den ersten Kreisen Lübecks. Seine Kindheit hat Thomas Mann später als „gehegt und glücklich“ bezeichnet.

1891 starb Thomas Manns Vater an Blasenkrebs. In seinem Testament hatte er verfügt, Unternehmen und Wohnhaus in Lübeck zu verkaufen. Die Erlöse wurden angelegt, und deren Zinsen standen seiner Frau und den Kindern für ihren Lebensunterhalt zu.

Nach neunjähriger Schulzeit legte Thomas Mann 1894 in Lübeck das eigentlich nur auf sechs Jahre angelegte „Einjährige“ (Mittlere Reife) mit durchgehend mäßigen bis sehr mäßigen Leistungen ab. Seine Schulzeit empfand er als stumpfsinnig. Schon früh begann er zu schreiben und beteiligte sich 1893 mit Prosaskizzen und Aufsätzen an der von ihm mit herausgegebenen Schülerzeitschrift "Der Frühlingssturm". Einen Brief an Frieda L. Hartenstein von 1889 unterschrieb der Vierzehnjährige mit „Thomas Mann. Lyrisch-dramatischer Dichter“. 1894 verließ er als Obersekundaner vorzeitig das Katharineum zu Lübeck und ging nach München, wohin die Mutter schon ein Jahr zuvor mit den Geschwistern gezogen war.

Krafft Tesdorpf, der seit dem Tod des Vaters zum Vormund für die noch nicht volljährigen Kinder bestellt war, bestimmte, dass Thomas Mann nach dem Abgang von der Schule einen bürgerlichen Beruf ergreifen sollte. Thomas nahm deshalb eine Stelle als Volontär in einer Feuerversicherungsgesellschaft an, obwohl die Bürotätigkeit ihn langweilte. Sein Debüt als Schriftsteller gab er 1894 mit der Novelle "Gefallen". Sie wurde in dem literarischen Magazin "Die Gesellschaft" veröffentlicht, die schon 1893 sein Gedicht "Zweimaliger Abschied" publiziert hatte. Daraufhin wurden ihm weitere Veröffentlichungen in der Kunstzeitschrift "Pan" angeboten.

Aufgrund dieses ersten Erfolges beendete Thomas Mann 1895 seine Versicherungstätigkeit und begann, Vorlesungen an der Technischen Hochschule München zu besuchen, um später einen journalistischen Beruf auszuüben. 1896 war er mit 21 Jahren volljährig geworden und erhielt monatlich 180 Mark aus den Zinsen des väterlichen Vermögens, was ihm ein Leben als freier Schriftsteller ermöglichte. Von 1895 bis 1896 verfasste Thomas Mann Beiträge für die nationalchauvinistische Monatsschrift "Das zwanzigste Jahrhundert," deren kurzzeitiger Herausgeber sein Bruder Heinrich war.

1896 folgte er seinem Bruder Heinrich nach Italien. Sie mieteten sich im Juli 1897 in dem östlich von Rom liegenden Ort Palestrina ein. Gemeinsam verfassten sie dort das "Bilderbuch für artige Kinder". Es enthielt parodistische „Kunstgedichte“ und war mit eigenhändigen Zeichnungen illustriert. Die Brüder schenkten es ihrer Schwester Carla zur Konfirmation. Das Unikat gelangte nach Carlas Tod in den Besitz des jüngsten Bruders Viktor, der es später den Kindern von Thomas Mann übergab. Seit der Emigration der Familie 1933 gilt es als verschollen; nur Gedichte, die Viktor Mann in seinen Memoiren "Wir waren fünf" zitierte, und einige Reproduktionen der Zeichnungen blieben von dem einzigen Gemeinschaftswerk der beiden Brüder erhalten.

Thomas Mann schrieb in Palestrina einige Novellen, unter anderen "Der kleine Herr Friedemann", und begann mit dem Roman "Buddenbrooks."

Seine sporadischen Beiträge in der antisemitischen Monatsschrift "Das zwanzigste Jahrhundert" beschränken sich auf die Zeit der Schriftleitung seines Bruders Heinrich (1895/96). Auch wenn Thomas Manns Artikel moderater ausfallen als der Rest der Zeitschrift, enthalten sie doch die antijüdischen Stereotype, die um die Jahrhundertwende auch in seinen eigentlich literarisch zu nennenden Arbeiten zu finden sind. Eine damalige Distanzierung vom Programm des "Zwanzigsten Jahrhunderts" existiert nicht.

Ab 1898 arbeitete er ein Jahr lang in der Redaktion des "Simplicissimus". 1900 wurde er als „Einjährig-Freiwilliger“ zum Dienst im Münchner Leibregiment eingezogen. Seine militärische Laufbahn endete nach drei Monaten wegen Dienstuntauglichkeit – ein Erlebnis, das sich in der Musterungsszene in den "Bekenntnissen des Hochstaplers Felix Krull" widergespiegelt findet.

1901 wurde Manns erster Roman "Buddenbrooks" veröffentlicht. Die zweibändige Erstausgabe stieß zunächst auf nur geringe Resonanz. Die einbändige zweite Auflage von 1903 dagegen brachte den Durchbruch und machte Thomas Mann in der Öffentlichkeit bekannt. Einige Figuren des Romans haben Vorbilder in der Familiengeschichte der Manns, viele Nebenfiguren sind Lübecker Bürgern nachgestaltet. Die meisten Porträtierten waren wegen der ironisierenden Darstellung nicht begeistert, sich im Buch wiederzufinden. Bald kursierte eine Liste, die die lebenden Vorbilder identifizierte und die eine Lübecker Buchhandlung ihrer Kundschaft auslieh. Das Verhältnis der Lübecker zu ihrem prominenten Mitbürger war deshalb lange Zeit gespannt. 1929, 28 Jahre nach dem ersten Erscheinen, erhielt Thomas Mann für die "Buddenbrooks" den "Nobelpreis für Literatur".

1903 zeichneten sich erste Missstimmungen zwischen den Brüdern Thomas und Heinrich ab. Obwohl Thomas Mann sich als Schriftsteller in der Öffentlichkeit etabliert hatte, fühlte er sich von seinem Bruder als Künstler zurückgesetzt und kritisierte seinerseits die „langweilige Schamlosigkeit“ in dessen Büchern. Insbesondere Heinrich Manns gerade veröffentlichter Roman "Die Jagd nach Liebe" erregte bei ihm Abscheu. Der Kontakt brach zwar nicht völlig ab und es kam immer wieder zu Annäherungsversuchen, ein künstlerischer Austausch fand aber nur im regelmäßigen Briefwechsel statt, wobei der jeweilige Briefschreiber die Werke des Empfängers kommentierte.

1904 lernte Thomas Mann Katharina „Katia“ Pringsheim (Tochter des Mathematikers Alfred Pringsheim und Enkelin der Frauenrechtlerin Hedwig Dohm) kennen und begann, um sie zu werben. In seinen Briefen und Tagebüchern sind bis dahin nur homoerotische Schwärmereien dokumentiert. Er lebte seine Homosexualität jedoch nie aus, es blieb bei Schwärmereien für „Jünglinge“, die unter anderem in den "Buddenbrooks" (Hanno/Kai Graf Mölln), in "Tonio Kröger" (Tonio Kröger/Hans Hansen), im "Zauberberg" (Hans Castorp/Pribislav Hippe) und in "Der Tod in Venedig" (Gustav von Aschenbach/Tadzio) ihren Niederschlag fanden.

Mit dem Entschluss, Katia Pringsheim zu heiraten, entschied er sich für ein „geordnetes“ Leben und heiratete in eine der angesehensten Familien Münchens ein. Katia zögerte zunächst, sodass die Ehe erst am 11. Februar 1905 geschlossen wurde. In seinem zweiten Roman "Königliche Hoheit" von 1909 hat Thomas Mann die Brautzeit literarisch verarbeitet. Mit Katia hatte er sechs Kinder: Erika (1905–1969), Klaus (1906–1949, Suizid), Golo (1909–1994), Monika (1910–1992), Elisabeth (1918–2002) und Michael (1919–1977, vermutlich Suizid).

1912 äußerten Ärzte bei Katia den Verdacht auf Tuberkulose, was einen längeren Sanatoriums-Aufenthalt in Davos erforderlich machte. Thomas Mann war, als er sie dort besuchte, beeindruckt von der Atmosphäre des Sanatoriums und fasziniert von den amüsanten Schilderungen, die ihm seine Frau über die Klientel der Klinik gab. Sie inspirierten ihn zu seinem Roman "Der Zauberberg", den er 1913 begann, aber erst 1924 vollendete.

Im Jahr 1914 zog die Familie Mann in die Poschingerstraße 1 am Herzogpark. Als im selben Jahr der Erste Weltkrieg ausbrach, gab es viele Literaten, die der recht euphorischen Stimmung vor allem bürgerlicher Kreise im Deutschen Reich nicht widersprachen – im Gegenteil. Der Kriegsbeginn wurde begrüßt und bejubelt. Auch Alfred Kerr, Robert Musil, Richard Dehmel und Gerhart Hauptmann zeigten sich von dessen Berechtigung überzeugt. Thomas Manns Meinung wird in folgenden Zitaten dargestellt.

Jörn Leonhard zitiert in seiner "Geschichte des Ersten Weltkriegs" die Erinnerung der Kinder an die Worte des Vaters, dass „nun wohl auch gleich ein feuriges Schwert am Himmel erscheinen“ werde, und dessen Erinnerung an Leo Tolstoi, den „Repräsentanten einer radikalen Gewaltlosigkeit“ (Leonhard): „Merkwürdig, aber wenn der Alte noch lebte, er brauchte nichts zu tun, nur da zu sein, auf Jasnaja Poljana, – dies wäre nicht geschehen, – "es hätte nicht gewagt, zu geschehen."“.

Thomas schrieb an seinen Bruder Heinrich: „Ich persönlich habe mich auf eine vollständige Veränderung der materiellen Grundlagen meines Lebens vorzubereiten. Ich werde, wenn der Krieg lang dauert, mit ziemlicher Bestimmtheit das sein, was man ‚ruiniert‘ nennt.“ Und er fährt später fort: „In Gottes Namen! Was will das besagen gegen die Umwälzungen, namentlich die seelischen, die solche Ereignisse im Großen zur Folge haben müssen! Muß man nicht dankbar sein für das vollkommen Unerwartete, so große Dinge erleben zu dürfen?“
Thomas Mann hielt den Krieg prinzipiell für notwendig, galt es aus seiner Sicht doch, den „verworfensten Polizeistaat der Welt“, das zaristische Russland, „zu zerschlagen“.

In seinen "Gedanken im Kriege" – Reflexionen zum Gegenstand des Krieges – verteidigte der Dichter seine militaristischen Standesbrüder. Ganz im damaligen imperialistischen Zeitgeist schrieb er ferner: „Das Gleichgewicht Europas [...] war die Ohnmacht Europas, war seine Blamage gewesen, mehr als einmal... .“ Den Kontakt zu Heinrich, der wie Stefan Zweig, Arthur Schnitzler, Romain Rolland und später auch Hermann Hesse gegen die die öffentliche Meinung bestimmenden chauvinistischen "Ideen von 1914" anschrieb, hatte er inzwischen ganz abgebrochen. Detailliert setzte er sich mit den geistigen Strömungen der Kriegs- und Vorkriegszeit in seinem umfangreichen Werk "Betrachtungen eines Unpolitischen" auseinander, in dem er den Unterschied zwischen der deutschen pessimistischen Selbstironie des Geistes bei gleichzeitiger Liebe dessen zum Leben einerseits, und dem romanischen Radikalismus des Geistes bzw. des Lebens andererseits herauszuarbeiten versucht. Den Gegensatz zu seinem eigenen Verständnis als deutsch-bürgerlicher Künstler bildet sein Bruder Heinrich als frankophiler „Zivilisationsliterat“.

Schon kurze Zeit nach der Drucklegung (Ende 1918) folgte allerdings eine immer stärkere Distanzierung Manns von dieser Phase seines politischen Denkens.

Die Ermordung des Reichsaußenministers Walther Rathenau am 24. Juni 1922 war mitauslösend für Manns Entscheidung, öffentlich für die Weimarer Republik und ihre Werte einzutreten. Mit seiner Rede "Von deutscher Republik" trat er zum ersten Mal als politischer Mahner und Befürworter der neuen Staatsform hervor. Demokratie und Humanität, so Mann, seien eins, und da der Mensch dem Prinzip der Humanität folgen solle, habe er also nach einem demokratischen Zusammenleben zu streben. Er wurde auch Mitglied der liberaldemokratischen "Deutschen Demokratischen Partei". Auch trat er in das Komitee der "Paneuropäischen Union" ein.

1924 veröffentlichte der Dichter seinen Roman "Der Zauberberg". Er war auf Anhieb ein großer Erfolg. Danach folgten "Unordnung und frühes Leid" und "Über die Ehe". 1925 begann der Autor mit der Arbeit an der Tetralogie "Joseph und seine Brüder". Modell für die Konturen Josephs standen die jungen Menschen, von denen der Schriftsteller sich verzaubert fühlte. Auch der siebzehnjährige Klaus Heuser († 1994), der Sohn von Werner Heuser, und ein Freund seiner Kinder, den Thomas Mann 1927 in Kampen auf Sylt kennengelernt hatte und über den er notierte, er sei seine „nach menschlichem Ermessen letzte Leidenschaft“, dürfte in die Figur des Joseph eingeflossen sein.

Thomas Mann beteiligte sich als Gründungsmitglied der "Sektion Dichtkunst" bei der Preußischen Akademie der Künste unmittelbar an Versuchen, das Ansehen der Literatur zu heben. Insbesondere wandte er sich gegen das damals geltende Gesetz zur Bewahrung der Jugend vor Schund- und Schmutzschriften, das die schriftstellerische Freiheit einschränkte.

In einer Rede am 30. November 1926 in der Münchner Tonhalle kritisierte Thomas Mann den Kulturbetrieb Münchens aufs Schärfste. Die Stadt reagierte schnell und setzte ein Komitee zur Förderung der Literatur ein – bereits Anfang 1927 wurde Thomas Mann gemeinsam mit Catherina Godwin, Hans Ludwig Held, Hans von Gumppenberg, Emil Preetorius, Peter Dörfler und Wilhelm Weigand in den neu eingerichteten Literaturbeirat der Stadt München berufen. Für Gumppenberg wurde nach dessen Tod 1928 Benno Rüttenauer eingesetzt. Der Beirat förderte Literaten durch Vergabe von Druckkostenzuschüssen sowie durch den 1928 auf Anregung Thomas Manns gestifteten Dichterpreis der Stadt München. War Thomas Mann anfangs noch zuversichtlich, so machte sich ab 1929 in zunehmendem Maße der Einfluss der politischen Rechten bemerkbar, und er konnte sich mit seinen Vorschlägen immer seltener durchsetzen.

Der Nobelpreis für Literatur war für Mann keine Überraschung. Bereits Jahre zuvor war spekuliert worden, dass er ihn bekommen könnte, er selbst hatte schon 1927 darauf gehofft. Am Nachmittag des 12. November 1929 erreichte ihn die Nachricht aus Stockholm. Er war konsterniert, dass sich das Komitee praktisch nur auf seinen ersten Roman bezog. Verantwortlich dafür war in erster Linie der einflussreiche Stockholmer Königsmacher, der Schwede Fredrik Böök, der dem Roman "Der Zauberberg" keine Wertschätzung entgegenzubringen vermochte und ihn mehrfach verrissen hatte. Das Preisgeld betrug 200.000 Reichsmark. Einen Teil davon verwendete Mann, um die Schulden seiner Kinder Klaus und Erika nach ihrer Weltreise zu tilgen. Außerdem wurden davon der Bau des seit 1996 als Thomas-Mann-Kulturzentrum gepflegten Sommerhauses in Nidden auf dem zu Litauen gehörenden Teil der Kurischen Nehrung und zwei Autos finanziert, der Rest angelegt. Schon in Stockholm hatte ein Journalist den Manns nahegelegt, das Geld „draußen stehenzulassen“, aber sie verstanden nicht, weshalb. Als sie 1933 aus Deutschland emigrierten, verloren sie einen großen Teil ihres Vermögens, namentlich ihren Immobilien- und anderen Sachbesitz.

Die Reichstagswahl 1930 hatte den Nationalsozialisten einen gewaltigen Stimmenzuwachs beschert. Thomas Mann, der, wie viele andere Skeptiker, den wachsenden politischen Einfluss der NSDAP mit Misstrauen beobachtet hatte, entschloss sich zu einem "Appell an die Vernunft", einer Rede, die er am 17. Oktober 1930 im Berliner Beethoven-Saal hielt und die als „Deutsche Ansprache“ in die Geschichte einging. Unter das vorwiegend republikanische und sozialdemokratische Publikum hatten sich Arnolt Bronnen, die Brüder Ernst und Friedrich Georg Jünger sowie etwa ein Dutzend Nationalsozialisten gemischt, die versuchten, durch Zwischenrufe zu stören. Das gelang ihnen jedoch nicht.

Thomas Mann nannte den Nationalsozialismus in nüchterner Unumwundenheit mit . Er fragte, ob das deutsch sei und ob überhaupt verwirklicht werden könne. Der Beifall im Saal war groß, drang aber nicht nach draußen durch. Thomas Mann zählte zu den wichtigsten prominenten Gegnern des Nationalsozialismus.

Im Februar 1933 jährte sich Richard Wagners Todestag zum 50. Mal. Mann erreichten mehrere Einladungen, aus diesem Anlass einen Vortrag zu halten. Am 10. Februar hielt er diesen "(Leiden und Größe Richard Wagners)" zunächst im Auditorium maximum der Universität München, um am folgenden Tag mit seiner Frau eine längere Reise ins Ausland anzutreten: Die Vortragsreise führte sie nach Amsterdam, Brüssel und Paris, danach folgte ein Winterurlaub in Arosa. Nicht zuletzt auf Drängen von Erika und Klaus Mann sollten sie von dieser Reise nicht mehr nach München zurückkehren. Als alle Mitglieder der "Sektion Dichtkunst" bei der Preußischen Akademie der Künste aufgefordert wurden, gegenüber der nationalsozialistischen Regierung eine Treueerklärung abzugeben, erklärte Mann mit einem Schreiben an den Akademie-Präsidenten Max von Schillings vom 17. März 1933 seinen Austritt.

Am Tag der Bücherverbrennung, dem 10. Mai 1933, wurde Thomas Mann aus dem Münchener Literaturbeirat ausgeschlossen. Seine Werke blieben von der Bücherverbrennung verschont, nicht jedoch die seines Bruders Heinrich und seines Sohnes Klaus.

Der Entschluss, Deutschland den Rücken zu kehren, fiel den Manns nicht leicht. Unter anderem mussten sie ihr Sachvermögen zurücklassen. Nur ein Teil davon konnte später auf Umwegen in die Schweiz geschafft werden. Zu finanziellen Engpässen kam es jedoch nicht, da die Familie rechtzeitig einen erheblichen Teil des Stockholmer Preisgeldes und Bargeld aus Deutschland in die Schweiz transferieren konnte. Thomas Manns Verleger hatte ihn inständig gebeten, die Deutschen in dieser schweren Stunde nicht allein zu lassen, und sich bereit erklärt, seine Neuerscheinungen weiterhin zu veröffentlichen.

Die erste Station des Exils war Sanary-sur-Mer in Frankreich. Letztlich zogen die Manns in die Schweiz und wohnten in Küsnacht in der Nähe von Zürich. Die Bewegungsfreiheit des Schriftstellers verringerte sich, da sein deutscher Pass ablief und die Nazis dessen Verlängerung von Manns persönlichem Erscheinen in München abhängig machten. Dort wartete bereits ein „Schutzhaftbefehl“ auf ihn. Das Ausbürgerungsverfahren, von dem alle seit August 1933 emigrierten Prominenten betroffen waren, wurde in seinem Fall zunächst ausgesetzt. Allerdings nutzten die Finanzbehörden die Gelegenheit, um in München sein Haus einschließlich Inventar zu beschlagnahmen. Sie beriefen sich auf Verlagsverträge, aus denen sich eine erhebliche Steuerschuld des Schriftstellers aus den Jahren 1929–1930 ergebe.

1934 und 1935 reisten die Manns die ersten beiden Male in die Vereinigten Staaten. Dort war das Interesse an dem prominenten Schriftsteller groß; die Behörden gewährten ihm ohne gültigen Pass die Einreise. Seinen sechzigsten Geburtstag beging Thomas Mann in Küsnacht, er wurde von den Schweizern überwältigend gefeiert. Am 19. November 1936 wurde ihm auf seinen Antrag hin die tschechoslowakische Staatsbürgerschaft für den Ort Proseč verliehen. Im Tagebuch vermerkt er dazu knapp: „Sonderbares Ereignis.“ Wenige Wochen später wurde ihm – gleichzeitig mit seiner Frau Katia und den Kindern Golo, Elisabeth und Michael – die deutsche Staatsbürgerschaft aberkannt. Das Ausbürgerungsverfahren wurde nach den Erkenntnissen einer unabhängigen Historikerkommission durch die Stellungnahme des damaligen Gesandten Ernst von Weizsäcker begünstigt, der sich im Mai 1936 in einem Brief aus Bern dafür ausgesprochen hatte, weil Thomas Mann neben „höhnischen Bemerkungen [sogar] feindselige Propaganda gegen das Reich im Ausland“ betrieben habe. Zugleich entzog die Universität Bonn Mann die Ehrendoktorwürde, die ihm 1919 verliehen worden war.

In den 1930er Jahren besuchte Mann sechsmal Ungarn und wohnte dort unter anderem beim Grafen Lajos Hatvany in Hatvan bei Budapest. Hier publizierte er mehrmals Texte in der 1854 gegründeten deutschsprachigen Zeitung "Pester Lloyd", so 1936 den Essay "Achtung, Europa!"

In der persönlichen Korrespondenz drückte er in dieser Zeit seine Hoffnungen auf den Erfolg der Volksfrontregierungen in Frankreich (Front populaire) und Spanien (Frente Popular) aus. Auch bezeichnete er die Stalinsche Verfassung von 1936 als „akzeptabel“.

Die endgültige Übersiedlung Thomas Manns und seiner Familie in die USA fiel zeitlich mit dem Berchtesgadener Abkommen zusammen, das im März 1938 zum Anschluss Österreichs an Deutschland führen sollte. Bei der Ankunft in New York am 21. Februar 1938 baten ihn Reporter daher um eine Stellungnahme zu jener Entwicklung und fragten ihn, ob er das Exil als eine schwere Last empfinde. Seine Antwort wurde am nächsten Tag in der "New York Times" abgedruckt:

Erste Station des Exils in den USA war Princeton. Thomas Mann erhielt, vermittelt durch seine Gönnerin Agnes E. Meyer, eine Gastprofessur an der dortigen Universität. Vier Vorlesungen standen auf seinem Lehrplan mit den selbstgewählten Themen "Faust" von Goethe, Wagner, Freud und eine Einführung in den "Zauberberg".

Das erste Jahr in den Vereinigten Staaten verlief erfolgreich. Er war finanziell abgesichert, seine Werke verkauften sich gut, er unternahm einige Lesereisen, traf wichtige Persönlichkeiten und erhielt fünf Ehrendoktorwürden (Columbia, Hobart, Princeton, Rutgers und Yale). Am 6. Juni 1939 startete er zu seiner vorerst letzten Reise nach Europa. Gleichzeitig arbeitete er an seinem Roman über Goethe, den er im Oktober 1939 beendete und der im gleichen Jahr unter dem Titel "Lotte in Weimar" erschien.

Der Ausbruch des Zweiten Weltkriegs am 1. September 1939 löste Bestürzung im In- und Ausland aus und bewog Thomas Mann, der sich gerade in Schweden aufhielt, zu zahlreichen Aktionen. Er war Mitglied in mehreren Ausschüssen, die Emigranten unterstützten, unter anderem im "Unitarian Service Committee" und im "Committee for Jewish and Christian Refugees". Im Oktober 1940 begann er mit den Texten für seine Radiosendung Deutsche Hörer! In monatlichen Abständen ausgestrahlt, wurden seine warnenden und gefühlsbetonten Ansprachen ab März 1941 in Kalifornien auf Platte aufgezeichnet und mit der Luftpost nach New York gebracht. Per Kabel wurden sie von dort nach London übertragen, wo die BBC die fünf- bis achtminütigen Aufnahmen über Langwelle auch in das deutsche Reichsgebiet ausstrahlte. Die Alliierten banden diese Versuche, das Monopol der deutschen Rundfunkanstalt von außen zu durchbrechen, in ihre allgemeine Informationspolitik und Propaganda gegenüber dem Dritten Reich und dessen Bevölkerung ein.

Die Einnahmen aus der Sendung spendete Mann dem "British War Relief Fund". Eine seiner bekanntesten Ansprachen ist die Sendung vom 14. Januar 1945:

Mann wählte nicht von ungefähr eine so apokalyptische Ausdrucksweise. Allerdings machte er aus Hitler und seinen später als „Paladine“ bekannt gewordenen Helfern in bissigen Teilen der Radioansprachen auch Witzfiguren, um eine allzu starke Dämonisierung zu vermeiden: In den Ansprachen wechselten sich moralische und bürgerlich-soziale Distanzierungen häufig ab.

Thomas Mann war einer von nur wenigen in der Öffentlichkeit aktiven Gegnern des Nationalsozialismus, auf die der deutsche Diktator in seinen Hetzreden namentlich einging. Mann revanchierte sich mit Anspielungen auf die rhetorischen Schwächen des „Führers“ und betonte die Richtigkeit seiner eigenen Vorhersagen:

Die unter dem Namen "„Deutsche Hörer!“" bekannt gewordenen Radiosendungen boten nach dem Krieg in Deutschland viel Diskussionsstoff. Während einige behaupteten, Thomas Mann habe in seinen Reden eine Kollektivschuld aller Deutschen suggeriert, vertraten andere die Meinung, er sei lediglich mit der Mentalität der Weimarer Republik und dem sozialen Klima in den ersten Jahren des Nationalsozialismus sehr hart ins Gericht gegangen.

1941 waren die Manns nach Pacific Palisades, nördlich von Los Angeles/Kalifornien zwischen Santa Monica und Malibu, übergesiedelt. Dort lebten sie zunächst in einem gemieteten Haus am Amalfi Drive, bevor sie ein eigens errichtetes Wohnhaus am San Remo Drive beziehen konnten. Es war Mitte 2016 als Verkaufsobjekt vom Abriss bedroht, was zu einer Online-Petition für den Erhalt im Namen der "Gesellschaft für Exilforschung" führte, an der sich unter anderem Herta Müller beteiligte: Das Haus solle „ein Ort der Erinnerung an die Exil-Geschichte, ein Ort des intellektuellen, gesellschaftlichen und kulturellen Austauschs werden.“ Die Bundesrepublik erwarb das Anwesen zu diesem Zweck.

Die Staatsbürgerschaft der Vereinigten Staaten erlangte Thomas Mann erst 1944. In den Jahren 1943 bis 1947 – unterbrochen 1946 durch eine schwere Lungenkrebserkrankung, die in Chicago operativ behandelt wurde – arbeitete Mann an "Doktor Faustus". Für dieses Projekt hatte er im Vorfeld musikwissenschaftliche Lehrbücher sowie Biografien über Mozart, Beethoven, Berlioz, Hugo Wolf bis hin zu Alban Berg studiert. Mit zeitgenössischen Komponisten wie Strawinsky, Hanns Eisler und Arnold Schönberg nahm er Kontakt auf, um sich in Sachen Musikkomposition unterweisen zu lassen. Besonders viel lernte er aber von Adorno, der damals in der Nachbarschaft lebte. Dieser beriet ihn gern und ausführlich, wovon Thomas Mann selbst in seinem autobiographischen Bericht "Die Entstehung des Doktor Faustus – Roman eines Romans" Rechenschaft ablegt und wovon auch Katia Mann in ihren "Ungeschriebenen Memoiren" berichtet. Dokumentarisches und Historiografisches aus der Luther-Zeit und dem Dreißigjährigen Krieg gehörten ebenso zur Vorbereitung des Romans wie Grimmelshausen, Sprichwörtersammlungen des Mittelalters und Fachliteratur zu Nietzsche. Er nannte das Buch seine „Lebensbeichte“ und schrieb am 21. Oktober 1948 an Paul Amann: 

In Kalifornien fand Mann auch Zugang zu den nordamerikanischen Unitariern, deren Mitglied er schließlich werden sollte. Thomas Mann – zuvor Lutheraner – schätzte die Unitarier vor allem als Glaubensgemeinschaft ohne dogmatische Fundamente, wobei er dem christlich ausgerichteten Unitarismus näher stand als neueren humanistischen Ansätzen. Mann trat auch als Gastredner auf der Kanzel auf und veranlasste, dass Frido und seine Enkelin Angelica im Frühjahr 1942 in der First Unitarian Church in Los Angeles getauft wurden, wobei er selbst als Pate fungierte.

Mann hatte zwischen sich und einflussreiche literarisch-publizistische Kreise des westlichen Nachkriegsdeutschlands einen Keil getrieben: In seinem offenen Brief "Warum ich nicht nach Deutschland zurückkehre" vertrat er die These von der Kollektivschuld der Deutschen. Drohbriefe und Verrisse seines "Doktor Faustus" waren die Folge. Die Bombardierung deutscher Städte während des Zweiten Weltkrieges kommentierte er mit den Worten: „Alles muß bezahlt werden.“ Es mussten einige Jahre vergehen, bis sich in der bundesdeutschen Öffentlichkeit wieder eine versöhnlichere Haltung gegenüber Thomas Mann einstellte.

Von der Politik der USA war Thomas Mann nach dem Tod des US-Präsidenten Franklin D. Roosevelt im Jahr 1945 und besonders seit Beginn des Kalten Krieges 1947 zunehmend enttäuscht. Sein Entschluss, nach Europa zurückzukehren, verfestigte sich, als er im Juni 1951 vor dem Repräsentantenhaus im Kongress als bezeichnet wurde. Er musste (wie schon zuvor die deutschen Emigranten Hanns Eisler und Bertolt Brecht) Rechenschaft über seine Aktivitäten vor dem Komitee für unamerikanische Umtriebe ablegen. Genau ein Jahr später, im Juni 1952, kehrten die Manns mit Tochter Erika in die Schweiz zurück, wo sie zunächst in einem gemieteten Haus in Erlenbach bei Zürich, ab 1954 dann in der angekauften Villa in Kilchberg, Alte Landstraße 39, über dem Zürichsee lebten.

Schon 1949 hatte Thomas Mann anlässlich der Feiern zu Goethes 200. Geburtstag Deutschland einen Besuch abgestattet. Er besuchte Frankfurt am Main (Trizone) und Weimar (SBZ), was von der westdeutschen Öffentlichkeit misstrauisch beäugt wurde, jedoch von Mann mit dem Satz kommentiert wurde: In Frankfurt erhielt er den westdeutschen Goethe-Preis. In Weimar traf er Johannes R. Becher, den Präsidenten des Kulturbundes und späteren Kultusminister der DDR, sowie Oberst Tjulpanow, Leiter der Informationsabteilung der SMAD, und es wurde ihm der ostdeutsche Goethe-Nationalpreis verliehen. Die gesamte Reise, die ihn auch nach Stuttgart und in das zerstörte München führte, stand unter polizeilichem Schutz, da es im Vorfeld einige Drohbriefe gegeben hatte. Letztlich wurde er aber enthusiastisch aufgenommen und seine Frankfurter Rede "Goethe und die Demokratie" wurde per Lautsprecher aus der Paulskirche auf den Vorplatz übertragen, wo weitere Zuhörer standen. Das Preisgeld der Frankfurter Ehrung stiftete Thomas Mann mittellosen Schriftstellern, die Summe des Weimarer Preises für den Wiederaufbau der dortigen Herder-Kirche.

Die Deutschland-Besuche von der Schweiz aus wurden zu einer festen Einrichtung. Thomas Mann nahm 1953 die Ehrenpräsidentschaft der Deutschen Schillerstiftung in Weimar (DDR) an. 1954 setzte er die 1909 begonnene Arbeit am Roman "Die Bekenntnisse des Hochstaplers Felix Krull" fort – der letztlich, durch seinen nahen Tod, ein Fragment blieb.

Zum 150. Todestag Friedrich Schillers 1955 veröffentlichte Mann den Essay "Versuch über Schiller" und hielt zu den Feierlichkeiten die Festansprachen; zunächst in Stuttgart und am 14. Mai 1955 in Weimar. An diesem Tag wurde ihm die Urkunde zum Ehrenmitglied der "Deutschen Akademie der Künste" überreicht.

Sechs Tage später, am 20. Mai 1955, besuchte er ein letztes Mal seine Vaterstadt Lübeck und bekam im Rahmen dieses Aufenthaltes die Ehrenbürgerwürde verliehen. In seiner Dankesrede nahm er Bezug auf seinen Vater, den früheren Senator der Stadt: „Ich kann wohl sagen, sein Bild hat immer im Hintergrunde gestanden all meines Tuns, und immer habe ich es bedauert, daß ich ihm zu seinen Lebzeiten so wenig Hoffnung machen konnte, es möchte aus mir in der Welt noch irgend etwas Ansehnliches werden. Desto tiefer ist die Genugtuung, mit der es mich erfüllt, daß es mir gegönnt war, meiner Herkunft und dieser Stadt, wenn auch auf ausgefallene Weise, doch noch etwas Ehre zu machen.“

Im Juli 1955 hielt sich das Ehepaar im holländischen Seebad Noordwijk auf. Am 18. Juli erwähnte Thomas Mann seiner Frau gegenüber erstmals einen ziehenden Schmerz im linken Bein, der ihm „kürzlich angeflogen“ sei und nun beginne, ihm lästig zu fallen. Die hinzugezogenen Ärzte diagnostizierten eine Beinvenenthrombose und verordneten Bettruhe. Am 23. Juli erfolgte die vorzeitige Rückkehr nach Zürich zur weiteren Behandlung. Im dortigen Kantonsspital stellte sich kurzfristig eine Besserung seines Zustandes ein. Voller Vorfreude auf seine Rückkehr nach Kilchberg schrieb er an Theodor W. Adorno: Jedoch folgte innerhalb von Tagen eine stetige Verschlechterung: Er verlor an Gewicht und litt zunehmend unter Kreislaufschwäche. Am 12. August 1955 starb Thomas Mann achtzigjährig im Zürcher Kantonsspital an einer Ruptur der unteren Bauchschlagader (Aorta abdominalis) infolge von Arteriosklerose.

Zur Beerdigung auf dem Kilchberger Friedhof am 16. August erschienen zahlreiche Trauernde aus dem In- und Ausland. Als einer der langjährigen Wegbegleiter des Verstorbenen schrieb Carl Zuckmayer in seinen "Worten des Abschieds": 

Thomas Mann knüpfte an die Erzähltechniken des 19. Jahrhunderts an, vor allem an den weit ausholenden Gestus Tolstois und die Symbole und Leitmotive im Werk Theodor Fontanes und Richard Wagners. Charakteristisch für Thomas Manns Prosa sind Ironie und „heitere Ambiguität“ Bis zu "Der Zauberberg" (1924) überwogen psychologischer Scharfblick und Durchschauen. Danach, in der „zweiten Hälfte“ des Gesamtwerks, wurden mythologische Motive und religiöse Themen gestaltet. Außerdem hinterließ Thomas Mann ein bedeutendes essayistisches Werk. Seine durch Gliedsätze und Einfügungen hoch verschränkte Erzählweise bewahrt Rhythmus und Balance, Sprache und Ton sind der jeweiligen Thematik angepasst.

Thomas Mann hat acht Romane sehr unterschiedlichen Umfangs geschrieben:

Unter der großen Zahl von Novellen sind besonders hervorzuheben: "Tristan", "Tonio Kröger", "Tod in Venedig" und "Mario und der Zauberer".

Die Werke des Dichters (abgesehen von den "Betrachtungen eines Unpolitischen", die während des Ersten Weltkrieges entstanden sind und nach Ansicht des Verfassers ohnehin als „Verirrung“ anzusehen sind) haben folgende Gemeinsamkeiten:

Thomas Mann hat sein Leben lang Tagebuch geschrieben. Nach seiner überstürzten Emigration in die Schweiz blieben die Tagebücher 1933 in München zurück, und Thomas Mann fürchtete, dass sie den Nationalsozialisten in die Hände fallen würden. Die Tagebücher wurden in einer abenteuerlichen Aktion von seinem Sohn Golo in die Schweiz gerettet. Alle Tagebücher aus der Zeit vor März 1933 hat Thomas Mann im Mai 1945 im Garten seines Wohnhauses in Pacific Palisades verbrannt. Allein die Hefte aus der Zeit September 1918 bis Dezember 1921 blieben erhalten, da der Autor sie für die Arbeit am "Doktor Faustus" benötigte. Die noch vorhandenen und heute veröffentlichten Tagebücher umfassen die Zeiträume von September 1918 bis Dezember 1921 und von März 1933 bis Juli 1955. Thomas Mann hatte verfügt, dass die versiegelten Pakete mit den Tagebüchern erst 20 Jahre nach seinem Tod geöffnet werden dürfen.

Die sukzessive Veröffentlichung begann im Jahr 1975, in dem sich der Geburtstag des Autors zum hundertsten Mal jährte. Dementsprechend groß waren die Erwartungen des Publikums. Da der Text der Tagebücher sich als „unliterarisch“ erwies und vom sprachlichen Niveau der Werke weit entfernt war, wurde auf die Veröffentlichung vielfach enttäuscht reagiert. Überraschend war das im Grunde nicht, hatte Thomas Mann doch selbst im kalifornischen Exil den Hinweis „Without literary value“ auf seine Tagebuchsammlung geschrieben, bevor er sie der Nachwelt anvertraute. Erstaunlich war eher, dass Thomas Manns homoerotische Neigung sich hier sehr deutlich offenbarte; denn die zentrale Rolle für sein Leben, die Thomas Mann, laut Tagebuch, seiner Liebe zu jungen Männern beimaß, war so bisher weder aus dem Werk noch aus sonstigen bekannten Äußerungen abzulesen gewesen.

Die hauptsächliche Bedeutung der Tagebücher besteht allerdings nicht so sehr darin, ein privates Bild des Schriftstellers und Einsichten in seine Psyche, sondern vielmehr seinen geistesgeschichtlichen und naturwissenschaftlichen Hintergrund zu vermitteln, die Entstehungsgeschichte seines Werks zu dokumentieren und Hinweise auf beabsichtigte Wirkungen zu geben. Kritiker haben die Tagebücher sowohl als gefühl- und teilnahmslos wie auch als eitel und narzisstisch bezeichnet.

In einer Tagebuchnotiz vom 15. September 1950 spielt der Autor erneut mit dem Gedanken an die Verbrennung seiner Tagebücher. Die Reaktion der noch lebenden Familienmitglieder war insgesamt zurückhaltend. Der jüngste Sohn Michael Mann schien unter der in den Tagebüchern deutlich werdenden Einstellung seines Vaters ihm gegenüber besonders gelitten zu haben. Ob sein Tod zum Jahreswechsel 1976/1977 mit seiner Arbeit an den Tagebüchern seines Vaters in Zusammenhang steht, ist umstritten.

Das Leben und Werk Thomas Manns war schon zu seinen Lebzeiten umstritten und blieb es über seinen Tod hinaus.

Manns Werke entstammen nicht bloßer Kreativität, sondern sind oft das Resultat jahrelanger, täglicher, disziplinierter Kleinarbeit. Die Schilderungen in seinen Werken gehen oft auf reale Gegebenheiten zurück, nur selten sind sie frei erfunden. Deren innovative Integration, überraschende assoziative Verknüpfung sowie präzise sprachliche Ausführung bilden das Kernstück der Arbeit Thomas Manns und machen seine Werke zu dem, was (gelegentlich auch despektierlich) als „bildungsbürgerlich“ bezeichnet wird.

Seine literarischen Erfolge, sein Konservatismus, sein großbürgerlicher Lebensstil und nicht zuletzt seine Fähigkeit zu prägnantem Polemisieren trugen zu Neid und Feindschaften bei. Dementsprechend gespannt war das Verhältnis zu manchen Schriftstellerkollegen. Robert Musil, von Mann durchaus hoch geschätzt, und Kurt Tucholsky bezeichneten ihn als „Großschriftsteller“, Bertolt Brecht nannte ihn einen „regierungstreuen Lohnschreiber der Bourgeoisie“, Alfred Döblin bezeichnete ihn als den Herrn, „der die Bügelfalte zum Kunstprinzip“ erhebt. Auch die Tatsache, dass er die meisten seiner literarischen Gestalten nach realen Vorbildern modellierte, teils aus dem familiären Umkreis, teils aus seinem prominenten Bekanntenkreis, sogar aus dem Umfeld seiner Konkurrenten, trug ihm nicht immer Freunde ein. Freundschaftliche Beziehungen unterhielt Mann dagegen beispielsweise zu Hermann Hesse, Hermann Broch und Jakob Wassermann.

Thomas Manns Verhältnis zum Judentum war ambivalent. Er selbst bezeichnete sich indes als Philosemiten.

Das Verhältnis der Nationalsozialisten zu Thomas Mann, der sich schon seit den frühen 1920er Jahren öffentlich gegen die rechtsextremen politischen Tendenzen in Deutschland gewendet hatte, war nach der Machtergreifung 1933 zunächst nicht völlig eindeutig. In München wurden zwar Haus und Vermögen beschlagnahmt und sogar ein (geheimer) „Schutzhaftbefehl“ erlassen. Doch noch im März 1934 durfte der Roman "Der junge Joseph" beim Berliner S. Fischer Verlag erscheinen, was darauf hindeutet, dass eine Rückkehr des international bekannten Nobelpreisträgers Thomas Mann ins Deutsche Reich zumindest einigen der Machthaber in Berlin ins Konzept gepasst hätte. Thomas Mann zögerte, auch mit Rücksicht auf die Interessen des Verlags, sehr lange, sich öffentlich klar gegen das Regime zu positionieren. Erst nachdem er im Februar 1936, nicht zuletzt auf starken Druck seiner Tochter Erika hin, mit einem offenen Brief an Eduard Korrodi in der "Neuen Zürcher Zeitung" den öffentlichen und eindeutigen Bruch mit den deutschen Machthabern herbeigeführt hatte, erfolgten Ausbürgerung und Versteigerung des zurückgebliebenen Münchener Hausrats.

Warum Mann keine bekennenden Schüler hat, wurde im Jahr seines 100. Geburtstags wissenschaftlich beleuchtet. Der Literaturwissenschaftler Peter Pütz vertritt die Ansicht: Manns Wirkung liege „nicht in der Nachfolge, sondern im Gegenentwurf“. Thomas Mann hat sich keiner literarischen Schule oder Strömung zugerechnet: 

Marcel Reich-Ranicki resümiert: 

Walter Nigg schreibt über Thomas Manns Haltung gegenüber Nietzsche: 

Mehr Verständnis zeigte der ungarische Schriftsteller Sándor Márai, der sich mit Thomas Manns Spannungsverhältnis zu Deutschland beschäftigte: 

Der 1975 vor dem Grundstück des früheren Geburtshauses in der Lübecker Breiten Straße in Buchform errichtete Thomas-Mann-Stein des Bildhauers Ulrich Beier zitiert ihn selbst mit seiner "Rede zur Feier des 50. Geburtstages":







Von folgenden Werken existieren original Tonbandaufnahmen von Thomas Mann, der seine eigenen Werke mit hörbarem Genuss rezitierte.






Belletristik

Die Datenbank der "Thomas-Mann-Sammlung Dr. Hans-Otto Mayer" (Schenkung Rudolf Groth) in der Universitäts- und Landesbibliothek Düsseldorf verzeichnet laufend Monografien und Aufsätze aus Sammelbänden und Fachzeitschriften sowie Zeitungsartikel zum Thema „Thomas Mann und seine Familie“. Die Datenbank enthält über 28.100 Titelsätze von Primär- und Sekundärliteratur und wird laufend aktualisiert. Auf den Internetseiten der Universitäts- und Landesbibliothek Düsseldorf ist die Datenbank frei zugänglich. Einzelne Kapitel aus Büchern, Aufsätze aus wissenschaftlichen Zeitschriften und Zeitungsartikel können über die Datenbank direkt bestellt werden und werden per Post geliefert.

Datenbanken

Über Mann

O-Ton


</doc>
<doc id="5271" url="https://de.wikipedia.org/wiki?curid=5271" title="Themis (Mythologie)">
Themis (Mythologie)

Themis () ist in der griechischen Mythologie Tochter des Uranos und der Gaia und gehört zum Göttergeschlecht der Titanen. Sie gilt als Göttin der Gerechtigkeit und der Ordnung sowie der Philosophie. 

Sie war – nach Metis – die zweite Gattin des Zeus, der mit ihr die Horen (Göttinnen, die das geregelte Leben überwachten) und Moiren (eine Dreiergruppe von Schicksalsgöttinnen) zeugte.

Nach Gaia war sie die zweite Schutzherrin des Orakels von Delphi, das dann von Phoibe oder auch gleich von Apollon übernommen wurde. 

Themis kannte die Zukunft und besaß ein Wissen um die Dinge, das selbst dasjenige des Zeus überstieg. So warnte sie diesen vor einer Beziehung mit Thetis, da einer von deren Söhnen mächtiger werden würde als der Vater. Deukalion und Pyrrha ermöglichte sie, die Sintflut zu überleben und die Erde neuerlich zu bevölkern. Den Atlas warnte sie, dass ein Zeussohn kommen werde, um die goldenen Äpfel der Hesperiden zu stehlen – weshalb dieser dann auch die Bitte des Perseus nach Unterkunft ausschlug und zur Strafe dafür versteinert wurde.



</doc>
<doc id="5273" url="https://de.wikipedia.org/wiki?curid=5273" title="Tangens und Kotangens">
Tangens und Kotangens

Tangens und Kotangens sind trigonometrische Funktionen und spielen in der Mathematik und ihren Anwendungsgebieten eine herausragende Rolle. Der Tangens des Winkels formula_1 wird mit formula_2 bezeichnet, der Kotangens des Winkels formula_1 mit formula_4. In älterer Literatur findet man auch die Schreibweisen formula_5 für den Tangens und formula_6 für den Kotangens.

Die Bezeichnung „Tangens“ stammt von dem Mathematiker Thomas Finck (1561–1656), der sie 1583 einführte. Die Bezeichnung „Kotangens“ entwickelte sich aus "complementi tangens", also Tangens des Komplementärwinkels.

Die Wahl des Namens "Tangens" erklärt sich unmittelbar durch die Definition im Einheitskreis. Die Funktionswerte entsprechen der Länge eines Tangentenabschnitts:

In einem rechtwinkligen Dreieck ist der Tangens eines Winkels formula_8 das Längenverhältnis von Gegenkathete zu Ankathete und der Kotangens das Längenverhältnis von Ankathete zu Gegenkathete:

Daraus folgt unmittelbar:

sowie

Formal kann die Tangensfunktion mittels der Sinus- und Kosinusfunktionen durch
definiert werden, wobei der Wertebereich formula_14 je nach Anwendung die reellen formula_15 oder die komplexen Zahlen formula_16 sind. Um zu verhindern, dass der Nenner formula_17 Null wird, werden beim Definitionsbereich formula_18 die Nullstellen der Cosinus-Funktion weggelassen:
im Reellen bzw.
im Komplexen.

Der Kotangens kann analog dazu durch
definiert werden, wobei sich für dessen Definitionsbereich
im Reellen bzw.
im Komplexen ergibt, wenn gewährleistet werden soll, dass der Nenner formula_25 ungleich Null ist.

Für den gemeinsamen Definitionsbereich von formula_26 und formula_27
gilt

Tangens: Im jeweiligen Intervall streng monoton steigend.

Kotangens: Im jeweiligen Intervall streng monoton fallend.

Punktsymmetrisch zum Koordinatenursprung:

Sowohl die Tangensfunktion als auch die Kotangensfunktion haben Asymptoten, aber keine Sprungstellen oder Extrema.

Durch passende Einschränkung der Definitionsbereiche erhält man eine Bijektion

Ihre Umkehrfunktion
heißt Arkustangens und ist folglich ebenfalls bijektiv.

Ihre Umkehrfunktion
heißt Arkuskotangens und ist folglich ebenfalls bijektiv.


Dabei sind mit formula_40 die Bernoulli-Zahlen bezeichnet.


Die Partialbruchzerlegung des Kotangens lautet für formula_43

Bei der Ableitung von Tangens und Kotangens tauchen die ansonsten eher wenig gebräuchlichen trigonometrischen Funktionen Sekans und Kosekans auf:

Die formula_47-ten Ableitungen lassen sich mit der Polygammafunktion ausdrücken:


Die Additionstheoreme für Tangens und Kotangens lauten
Eine symmetrische Formulierung lautet: Genau dann gilt
bzw.
wenn formula_63 ein Vielfaches von formula_30 ist.

Aus den Additionstheoremen folgt insbesondere für doppelte Winkel

Die Auflösung der bereits aus dem obigen Abschnitt Ableitung bekannten Identitäten

nach formula_25 bzw. formula_17 ergibt bei Beschränkung auf den "ersten Quadranten" zunächst einmal Einfaches:

Die etwas komplizierteren Erweiterungen auf ganz formula_15 lassen sich entweder "kompakt" als Grenzwert mit Hilfe der Floor-Funktion formula_75 oder "elementarer" mittels abschnittsweise definierter Funktionen darstellen:

Der Tangens des halben Winkels kann dazu verwendet werden, verschiedene trigonometrische Funktionen durch rationale Ausdrücke zu beschreiben: Ist formula_78, so ist
Insbesondere ist
eine Parametrisierung des Einheitskreises mit Ausnahme des Punktes formula_81 (der dem Parameter formula_82 entspricht). Einem Parameterwert formula_83 entspricht dabei der zweite Schnittpunkt der Verbindungsgeraden von formula_81 und formula_85 mit dem Einheitskreis (s. a. Einheitskreis#Rationale Parametrisierung).

Der Tangens liefert eine wichtige Kennzahl für lineare Funktionen: Jede lineare Funktion
besitzt als Graphen eine Gerade. Der Tangens des (orientierten) Winkels formula_8 zwischen der positiven x-Richtung und der Geraden ist die "Steigung" formula_88 der Geraden, d. h. formula_89. Dabei ist es egal, welche der beiden Halbgeraden man als zweiten Schenkel wählt.

Auch unter der "Steigung einer Straße" versteht man den Tangens des Steigungswinkels. Das Beispiel im Bild rechts zeigt eine Steigung von 10 % entsprechend einem Steigungswinkel von etwa 5,7° mit dem Tangens von 0,1.

Tangens und Kotangens können benutzt werden, um die zeitliche Abhängigkeit der Geschwindigkeit beim Wurf eines Körpers nach oben zu beschreiben, wenn für den Strömungswiderstand der Luft eine turbulente Strömung angesetzt wird (Newton-Reibung). Das Koordinatensystem werde so gelegt, dass die Ortsachse nach oben zeigt. Für die Geschwindigkeit gilt dann eine Differenzialgleichung der Form formula_90 mit der Schwerebeschleunigung "g" und einer Konstanten "k" > 0. Dann ergibt sich:

wobei formula_92 die Grenzgeschwindigkeit ist, die beim Fall mit Luftwiderstand erreicht wird. Wegen der oben angegebenen engen Zusammenhänge zwischen Kotangens und Tangens kann man diese zeitliche Abhängigkeit auch genauso gut mit Hilfe des Tangens ausdrücken:

Diese Lösung gilt, bis der Körper den höchsten Punkt seiner Bahn erreicht hat (also wenn "v" = 0 ist, das heißt für formula_94), daran anschließend muss man den Tangens Hyperbolicus verwenden, um den folgenden Fall mit Luftwiderstand zu beschreiben.

Der Tangens ist eine Lösung der Riccati-Gleichung

Faktorisiert man die rechte Seite, so erhält man

mit der imaginären Einheit formula_97. Der Tangens (als komplexe Funktion) hat die Ausnahmewerte formula_97, formula_99: Diese Werte werden niemals angenommen, da die konstanten Funktionen formula_97 und formula_99 Lösungen der Differentialgleichung sind und der Existenz- und Eindeutigkeitssatz ausschließt, dass zwei verschiedene Lösungen an derselben Stelle denselben Wert besitzen.



</doc>
<doc id="5274" url="https://de.wikipedia.org/wiki?curid=5274" title="Temperatur">
Temperatur

Die Temperatur ist eine physikalische Größe, die vor allem in der Thermodynamik eine wichtige Rolle spielt. Ihre SI-Einheit ist das Kelvin (K). In Deutschland, Österreich und der Schweiz ist die Einheit Grad Celsius (°C) ebenfalls zulässig.

Die Temperatur kennzeichnet das thermodynamische Gleichgewicht: Wenn zwei Körper dieselbe Temperatur haben, findet zwischen ihnen kein Wärmeaustausch statt, auch wenn sie miteinander in direktem Kontakt stehen. Haben beide Körper unterschiedliche Temperaturen, fließt Wärme vom wärmeren Körper zum kälteren. Dies geschieht so lange, bis sich die Temperaturen einander angleichen. Die Gleichgewichtstemperatur liegt, abhängig von deren Masse und Wärmekapazität, dann zwischen den Ausgangstemperaturen der Beiden.

Die Temperatur ist eng mit der ungeordneten Teilchenbewegung eines Stoffes verknüpft. In idealen Gasen wäre die Temperatur ein direktes Maß für die mittlere kinetische Energie der Teilchen (siehe kinetische Gastheorie).

Die Temperatur ist eine intensive Größe. Das bedeutet, dass sie ihren Wert beibehält, wenn man den betrachteten Körper teilt, während die Innere Energie als extensive Größe Eigenschaften einer Menge hat, die aufgeteilt werden kann.

Viele physikalische Eigenschaften sind direkt von der Temperatur abhängig und können daher zur Bestimmung der Temperatur dienen. Ein Beispiel ist die thermische Ausdehnung von Stoffen. Diese wird für ideale Gase im Gesetz von Gay-Lussac beschrieben und kann daher als Referenz dienen. Fast alle physikalischen und chemischen Eigenschaften von Stoffen sind zumindest schwach temperaturabhängig, beispielsweise der elektrische Widerstand oder die Dichte. Bei Änderungen des Aggregatzustandes bzw. anderer Phasenübergänge kommt es schon bei kleinen Temperaturunterschieden zu sprunghaften Veränderungen von Stoffeigenschaften.

Die Temperatur beeinflusst die Reaktionsgeschwindigkeit von chemischen Prozessen (Verdoppelung etwa alle 10 °C Temperaturerhöhung (van-’t-Hoff’sche Regel)) und somit auch Stoffwechselprozesse von Lebewesen maßgeblich.

Die Temperatur ist eine zentrale Kenngröße bei der Beschreibung des Wetters und des Klimas. Gegenwärtig wird ein Anstieg der Jahresdurchschnittstemperatur der Erde beobachtet (siehe globale Erwärmung).

Das Temperaturempfinden des Menschen unterscheidet sich teilweise erheblich von der physikalischen Temperatur (siehe gefühlte Temperatur).

Alle festen Stoffe, Flüssigkeiten und Gase bestehen aus sehr kleinen Teilchen, den Atomen und Molekülen. Diese befinden sich in ständiger ungeordneter Bewegung und zwischen ihnen wirken Kräfte. Mit „ungeordnet“ meint man in diesem Zusammenhang, dass die Geschwindigkeitsvektoren der Teilchen eines Körpers sich in Betrag und Richtung unterscheiden. Die Vektorsumme der Geschwindigkeiten aller Teilchen eines ruhenden Körpers verschwindet. Der Mittelwert der Geschwindigkeits"beträge" ist jedoch von Null verschieden. Er hängt von der Art des Stoffes, vom Aggregatzustand und vor allem von der Temperatur ab. Für feste, flüssige und gasförmige Körper gilt: "Je höher die Temperatur eines Körpers ist, desto größer ist der mittlere Geschwindigkeitsbetrag seiner Teilchen." Dieser anschauliche Zusammenhang legt nahe, dass es eine tiefste mögliche Temperatur gibt, den absoluten Nullpunkt, an dem sich die kleinsten Teilchen nicht mehr bewegen. Aufgrund der Unschärferelation ist eine völlige Bewegungslosigkeit jedoch nicht möglich (Nullpunktsenergie).

Eine einheitliche Temperatur ist nur für Gleichgewichtssysteme definiert (thermodynamisches Gleichgewicht). Bei Systemen, die nicht im Gleichgewichtszustand sind, werden zur Beschreibung mehrere verschiedene Temperaturen benötigt, etwa Elektronentemperatur und Ionentemperatur in einem Nichtgleichgewichts-Plasma oder Temperaturen für Translation, Rotation und Vibration für einen expandierenden Molekülstrahl.

Das ideale Gas ist eine Modellvorstellung, die gut geeignet ist, um Grundlagen der Thermodynamik und Eigenschaften der Temperatur zu illustrieren. Dem Modell zufolge sind die Teilchen des Gases punktförmig, können aber dennoch elastisch gegeneinander und gegen die Gefäßwand stoßen. Ansonsten gibt es keine Wechselwirkung zwischen den Teilchen. Das ideale Gas ist eine gute Näherung für Gase mit Atomen als kleinste Teilchen. Moleküle können rotieren oder vibrieren und können daher nicht als punktförmige Objekte vereinfacht werden.

Für das ideale Gas ist die Temperatur formula_1 proportional zur mittleren kinetischen Energie formula_2 der Teilchen
wobei formula_4 die Boltzmann-Konstante ist. In diesem Fall ist also die makroskopische Größe Temperatur auf sehr einfache Weise mit mikroskopischen Teilcheneigenschaften verknüpft. Außerdem gilt für das ideale Gas die allgemeine Gasgleichung, die die makroskopischen Größen Temperatur, Volumen formula_5 und Druck formula_6 in Beziehung setzt
wobei formula_8 die Teilchenzahl des Systems ist.

Aus diesen beiden Gleichungen kann man folgern, dass ein absoluter Temperaturnullpunkt existiert, an dem sich die Gasteilchen nicht mehr bewegen, also die mittlere kinetische Energie null haben. Wenn man beim Verringern der Temperatur den Druck konstant hält, wird das Volumen des Gases immer kleiner und am Temperaturnullpunkt würde auch das Volumen null, das Gas würde sich also auf einen Punkt zusammenziehen. Andererseits lässt sich die allgemeine Gasgleichung ausnutzen, um über die Messung von Volumen und Druck die Temperatur zu bestimmen. Dies wird durch die Gasthermometer realisiert. Bei diesen Thermometern reicht im Gegensatz zu anderen Thermometern eine Zweipunkteichung aus, da der Zusammenhang der Größen bekannt ist und sie eignen sich daher dazu, andere Thermometer zu kalibrieren.

Manchmal werden die Größen Temperatur, Wärme und thermische Energie miteinander verwechselt. Es handelt sich jedoch um verschiedene Größen. Die Temperatur und die thermische Energie beschreiben den Zustand eines Systems, wobei die Temperatur eine intensive Größe ist, die thermische Energie jedoch eine extensive Größe. So stellt bei idealen Gasen die Temperatur ein Maß für den "Mittelwert" der kinetischen Energie der Teilchen dar, während die thermische Energie als Teil der inneren Energie die "Summe" aller kinetischen Energien der Teilchen bildet.

Die Wärme hingegen charakterisiert nicht den Zustand, sondern die Änderung des Systemzustandes, genauer: den Energieaustausch zwischen zwei Systemen. Wärme ist dabei der Teil der Energie, der ohne die Wirkung einer makroskopischen verallgemeinerten Kraft übertragen wird. Die abgegebene oder aufgenommene Wärme führt dabei je nach Art der Zustandsänderungen (z. B. isobar oder isochor) zu unterschiedlichen Änderungen von Temperatur und thermischer Energie. Änderungen der inneren Energie und damit der thermischen Energie und der Temperatur werden nämlich nicht alleine durch Wärme hervorgerufen, sondern auch durch Arbeit (siehe Erster Hauptsatz der Thermodynamik). Das jeweilige Verhältnis von Wärme und Temperaturänderung heißt Wärmekapazität. Zudem enthält die innere Energie neben der thermischen Energie auch noch andere Anteile, so dass eine Wärmezufuhr nicht immer mit einer Erhöhung der thermischen Energie und der Temperatur verbunden ist. Beispielsweise schmilzt eine Menge Eis unter Wärmezufuhr, ohne dabei seine Temperatur von 0 °C zu ändern. Zwar wächst die innere Energie des Wassers, nicht jedoch seine thermische Energie.

Wärme bewegt sich immer vom System höherer Temperatur zum System niedrigerer Temperatur, wenn eine Wärmeübertragung zwischen den Systemen möglich ist. Dies führt auch zu einem Temperaturausgleich, wobei die Wärmeübertragung endet, wenn die Systeme sich im thermodynamischen Gleichgewicht befinden, also dieselbe Temperatur haben. Dabei hängt die Endtemperatur von den Wärmekapazitäten der beteiligten Systeme ab. Bei einer höheren Wärmekapazität führt dieselbe Änderung der Wärme zu einer geringeren Änderung der Temperatur. Das bedeutet, dass die Endtemperatur beim Mischen gleicher Mengen zweier Stoffe mit verschiedener Wärmekapazität und Anfangstemperatur näher an der Temperatur des Stoffes mit der höheren Wärmekapazität liegt. Ein anschauliches Beispiel liefert der Vergleich von Wasser und Luft. Wasser hat eine sehr viel höhere Wärmekapazität als Luft, daher kann eine Badewanne voll heißen Wassers ein Zimmer viel mehr erhitzen als dieselbe Menge Luft derselben Temperatur.

Ein thermodynamisches Gleichgewicht zeichnet ein Ruhesystem aus. Thermodynamische Gleichgewichtssysteme sind also nicht invariant unter Lorentztransformationen, da sich beispielsweise aus einem gleichmäßig strömenden Gas mittels eines Windrades Energie entziehen lässt. Ein System, das in seinem Ruhesystem im thermodynamischen Gleichgewicht ist, hat dabei die Eigenschaft, dass die mittels eines Windrades zu entziehende Energie minimal ist. Im Sinne der speziellen Relativitätstheorie ist ein System im thermodynamischen Gleichgewicht außer durch die Temperatur auch durch ein Ruhesystem charakterisiert. Um dies darzustellen kann die Temperatur als zeitartiger Vierervektor dargestellt werden. In einem System sind also die drei Ortskoordinaten formula_9 und die Zeitkoordinate ist die übliche Temperatur. Es ist allerdings im Kontext der Zustandsgleichungen günstiger und daher auch üblicher, die inverse Temperatur, genauer formula_10 als zeitartigen Vierervektor darzustellen.

In der allgemeinen Relativitätstheorie ist die Raumzeit gekrümmt, so dass im Allgemeinen der thermodynamische Limes nicht wohldefiniert ist. Wenn die Metrik der Raumzeit zeitunabhängig, also statisch, ist, kann allerdings ein globaler Temperaturbegriff definiert werden. Im allgemeinen Fall einer zeitabhängigen Metrik, wie sie beispielsweise Grundlage der Beschreibung des expandierenden Universums ist, können Zustandsgrößen wie die Temperatur nur lokal definiert werden. Ein verbreitetes Kriterium dafür, dass ein System zumindest lokal thermisch ist, ist, dass die Phasenraumdichte die Boltzmann-Gleichung ohne Streuung erfüllt.

Die thermodynamische Behandlung von Quantensystemen erfolgt meist mit den Methoden der statistischen Mechanik. Gerade im Kontext quantenfeldtheoretischer Systeme spielt dabei die Dichtematrix eine bedeutende Rolle. Da allerdings die Dichtematrix des kanonischen und großkanonischen Ensembles im thermodynamischen Limes unendlich wird und dadurch ihre Bedeutung verliert, ist für die korrekte Behandlung dieser Systeme einiger Aufwand nötig. In der axiomatischen Quantenfeldtheorie wurde erkannt, dass KMS-Zustände, die auch Gibbs-Zustände für Systeme endlichen Volumens umfassen, auch für den thermodynamischen Limes definierbar sind und sich eignen, um thermische Erwartungswerte zu berechnen. In der Standardtheorie wird das Problem meist durch eine Renormierungsprozedur behoben.

Stehen zwei Körper unterschiedlicher Temperatur in Wärmekontakt, so wird nach dem nullten Hauptsatz der Thermodynamik solange Energie vom wärmeren zum kälteren Körper übertragen, bis beide im thermischen Gleichgewicht stehen und die gleiche Temperatur angenommen haben. Es gibt dabei drei Möglichkeiten der Wärmeübertragung:


Der Mensch kann Temperaturen nur im Bereich um 30 °C fühlen. Genau genommen nimmt man nicht Temperaturen wahr, sondern die Größe des Wärmestroms durch die Hautoberfläche, weshalb man auch von einer gefühlten Temperatur spricht. Dieses hat für das Temperaturempfinden einige Konsequenzen:


Genau genommen gilt dieses nicht nur für das menschliche Empfinden, auch in vielen technischen Anwendungen ist nicht die Temperatur von Bedeutung, sondern der Wärmestrom. So hat die Atmosphäre der Erde oberhalb 1.000 km Temperaturen von mehr als 1.000 °C, dennoch verglühen deshalb keine Satelliten. Auf Grund der geringen Teilchendichte ist der Energieübertrag minimal.

Die formalen Eigenschaften der Temperatur werden in der Thermodynamik behandelt. Man bezeichnet die Temperatur hier als eine systemeigene, intensive Zustandsgröße. Sie lässt sich ebenfalls über die Entropie S definieren, da aus den Eigenschaften dieser Zustandsgröße folgt, dass S konstant bei allen reversiblen Zustandsänderungen ohne Wärmeübertragung Q ist:

mit "T" als Zustandsfunktion. "T" wird dabei so gewählt, dass formula_12 ein Differential einer Zustandsfunktion ist. Nach dem Poincaré-Lemma ist hierfür hinreichend und notwendig

Beim idealen Gas erfüllt die Gastemperatur formula_14 diese Bedingung.

Die statistische Definition der Temperatur lautet nach Boltzmann:

Hierbei bedeuten:


Bei einer sehr großen Ansammlung von Teilchen und dem Vorliegen eines idealen Gases kann man die Maxwell-Boltzmann-Verteilung anwenden und in der Folge die Temperatur wie folgt definieren:

Hierbei bedeuten:


Die Temperatur ist damit ein Maß für den durchschnittlichen ungerichteten, also zufälligen, Bewegungsenergieanteil (kinetische Energie) einer Ansammlung von Teilchen. Die Teilchen sind hierbei die Luftmoleküle bzw. die Moleküle oder Atome eines Gases, einer Flüssigkeit oder eines Festkörpers.

In der statistischen Mechanik steht die Temperatur mit der Energie pro Freiheitsgrad in Zusammenhang, wobei sich die Anteile der Energie auf verschiedene Komponenten verteilen:


Bei einatomigen idealen Gasen (Edelgase, gasförmiges Quecksilber) gibt es als Komponente der Temperatur nur die kinetische Energie und bei Festkörpern nach dem Einstein-Modell nur die 3. Komponente des Oszillators im Gitterverbund des Festkörpers.

Bei Gasen kann man diesen Zusammenhang zwischen Temperatur und Teilchengeschwindigkeit nach obiger Beziehung sogar quantitativ angeben. Eine Verdopplung der Temperatur auf der Kelvin-Skala führt bei idealen Gasen zu einer Erhöhung der quadratisch gemittelten Teilchengeschwindigkeit um den Faktor formula_21. Zwei unterschiedliche Gase haben dann die gleiche Temperatur, wenn das Produkt aus der molaren Masse des jeweiligen Gases und dem Quadrat der quadratisch gemittelten Teilchengeschwindigkeit gleich groß ist.

Im thermischen Gleichgewicht nimmt jeder Freiheitsgrad der Materie (Physik) (Bewegung, potentielle Energie, Schwingungen, elektronische Anregungen usw.) eine der Temperatur entsprechende Menge an Energie auf. Wie viel genau muss aus der kanonischen Verteilung berechnet werden und ist durch das Verhältnis von Energie zu Temperatur mal Boltzmannkonstante "k" bestimmt. Bei der kontinuierlichen (klassischen) kinetischen Energie ist dieses genau "k""T"/2. Die Boltzmannkonstante ergibt einen Zusammenhang zwischen Energie und Temperatur, der 11.606,7 Kelvin pro Elektronenvolt beträgt. Bei Raumtemperatur (300 Kelvin) ergibt dieses 0,0258472 eV. Die durchschnittliche kinetische Energie der Teilchen im idealen Gas beträgt ½"k""T" für jeden der drei Translationsfreiheitsgrade, unabhängig von der Molekülmasse bzw. molaren Masse. Wegen ½ "mv" = ½ "k""T" ist ein Teilchen umso langsamer, je größer seine Masse ist, und zwar im Verhältnis der Quadratwurzel aus seiner Masse. Bei idealen Gasen gleichen sich Massenerhöhung und Geschwindigkeitserniedrigung gegenseitig aus, was zum Gesetz von Avogadro führt.

Die Thermische Energie ist jedoch wie die Temperatur selbst nur ein Mittelwert innerhalb eines Vielteilchensystems und ihr Zusammenhang mit der Teilchengeschwindigkeit lässt sich ebenfalls aus der Maxwell-Boltzmann-Verteilung ableiten:

Das thermische Gleichgewicht hat eine wichtige Eigenschaft, die in der Thermodynamik zur Formulierung des Nullten Hauptsatzes führt.

Wenn ein System A sich mit einem System B sowie B sich mit einem System C im thermischen Gleichgewicht befinden, so befindet sich auch A mit C im thermischen Gleichgewicht. Das thermische Gleichgewicht ist damit transitiv, was es möglich macht, die empirische Temperatur "θ" einzuführen. Diese ist so definiert, dass zwei Systeme genau dann die gleiche empirische Temperatur haben, wenn sie sich im thermischen Gleichgewicht befinden.

Der Temperaturbegriff lässt sich erweitern, so dass sich auch negative Temperaturen definieren lassen.

Die Erweiterung baut darauf auf, dass ein System, welches makroskopisch im thermischen Gleichgewicht erscheint, also eine einheitliche Temperatur hat, mikroskopisch gesehen aus Teilchen besteht, die nicht alle die gleiche Energie haben.

Tatsächlich tauschen diese Teilchen durch Stöße ständig untereinander Energie aus, so dass sich eine Maxwellsche Geschwindigkeitsverteilung einstellt und die Gesamtenergien der einzelnen Teilchen auf unterschiedliche Niveaus verteilt sind (Boltzmann-Statistik). Wie eingangs bereits beschrieben ist die Temperatur ein Mittelwert dieser Energien.

Diese Verteilung ist nicht gleichmäßig, sondern viele Teilchen haben wenig Energie und wenige Teilchen sehr viel Energie. Teilt man die Energien in gleiche Klassen ein und zählt die Teilchen mit einer bestimmten Energie, so ergibt sich eine exponentielle Abnahme der Verteilung zu steigenden Energien. Erhöht man die Temperatur, so nivelliert sich die ungleiche Verteilung der Energieklassen, im hypothetischen Grenzfall der unendlichen Temperatur wäre in jeder Klasse (auf jedem Energie-Niveau) die gleiche Anzahl von Teilchen.

Die Erweiterung des Temperaturbegriffs geht nun davon aus, dass die Energieverteilung der Teilchen nach oben begrenzt ist und so geändert wird, dass die höheren Energieklassen stärker besetzt sind (Besetzungsumkehr, Inversion), als die niedrigen. Dies würde sich in der Gleichung der Boltzmannschen Energieverteilung als negative Temperatur zeigen.

Inzwischen ist es gelungen, entsprechende Gase mit negativer Temperatur unter Laborbedingungen herzustellen. Ebenso kann man die Besetzungsinversion im aktiven Medium eines Lasers als Zustand negativer Temperatur auffassen.

Der Zustand negativer Temperatur ist allerdings instabil, die Energie aus einem solchen System würde bei Kontakt mit einem Körper positiver Temperatur (egal welcher) an diesen abfließen.

Die Temperaturmessung erfolgt hierbei mit Hilfe von Thermometern oder Temperatursensoren. Das Herstellen eines thermischen Kontaktes erfordert ausreichende Wärmeleitung, Konvektion oder ein Strahlungsgleichgewicht zwischen Messobjekt (Festkörper, Flüssigkeit, Gas) und Sensor. Die Messgenauigkeit kann z. B. durch nicht ausgeglichene Wärmestrahlungs-Bilanz, Luftbewegungen oder durch Wärmeableitung entlang des Sensors beeinträchtigt sein. Die Messgenauigkeit wird theoretisch durch die zufällige Brownsche Molekularbewegung begrenzt.

Die Temperaturerfassung durch Wärmekontakt kann in vier Methoden unterteilt werden:


Die Temperatur einer Oberfläche kann berührungslos durch Messung der Wärmestrahlung bestimmt werden, sofern der Emissionsgrad ausreichend genau bekannt ist. Die Messung erfolgt z. B. mit einem Pyrometer oder mit einer Thermografie-Kamera.

Je nach Temperatur kommen dabei verschiedene Wellenlängenbereiche in Frage (siehe hierzu Stefan-Boltzmann-Gesetz oder Wiensches Verschiebungsgesetz). Bei niedrigen Temperaturen kommen Bolometer, Mikrobolometer oder gekühlte Halbleiterdetektoren in Frage, bei hohen Temperaturen werden ungekühlte Fotodioden oder auch der visuelle Vergleich der Intensität und Farbe des Glühens angewendet (Wolframfaden-Pyrometer, Glühfarben).

Rechts ist eine Thermografie zu sehen; hierbei wird eine Falschfarbendarstellung der Strahlungsemission im Mittleren Infrarot (ca. 5…10 µm Wellenlänge) erzeugt, die sich durch Kalibrierung in Form einer Farbskala an die Temperaturskala koppeln lässt. Links im Bild ist die Spiegelung der Strahlung des heißen Bechers zu erkennen.

Messfehler entstehen hierbei wie auch bei Pyrometern durch


Bei Minimierung aller störenden Einflüsse sind Messgenauigkeiten bzw. Kontraste bis herab zu Temperaturdifferenzen von 0,01 K möglich.

Die berührungslose Temperaturmessung anhand der Wärmestrahlung wird auch bei der Fernerkundung und zur Bestimmung der Oberflächentemperatur von Sternen angewendet, sofern die Eigenstrahlung der Lufthülle gering genug ist. IR-Teleskope sind deshalb nur auf hohen Bergen sinnvoll.

"Siehe hierzu auch Messgeräte, Messtechnik, Messung und "

Eine empirische Temperaturskala ist eine willkürliche Festlegung der Größenordnung der Temperatur und gestattet die Angabe der Temperatur in Bezug zu einem Vergleichswert.

Es gibt zwei Methoden, eine Skala zu definieren:

Nach der ersten Methode werden zwei Fixpunkte festgelegt. Diese Fixpunkte sind zweckmäßigerweise in der Natur vorkommende und durch Experimente reproduzierbare Werte. Der Abstand zwischen den Fixpunkten wird dann anhand einer temperaturabhängigen Stoff- oder Prozesseigenschaft gleichmäßig aufgeteilt: Z. B. wurde bei der Celsius-Skala die Volumenänderung von Quecksilber in 100 gleiche Teile geteilt, während die Fahrenheit-Skala sich auf die etwas anders verlaufende Volumenänderung von Alkohol bezieht.

Bei der zweiten Methode genügt ein Fixpunkt, der wie zuvor durch eine Stoffeigenschaft (z. B. Schmelzpunkt des Eises) definiert wird. Nun muss der Abstand (Skalenstrich zu Skalenstrich) bzw. die Größe der Einheit festgelegt werden.

Eine Methode, die sich trotz einiger Vorteile nicht etablieren konnte, orientiert sich an der Volumenänderung von Gasen bei konstantem Druck. Als Einheit wurde von Rudolf Plank der Temperaturunterschied vorgeschlagen, der einer Volumenänderung um den Faktor (1 + 1/273,15) entspricht. Eine solche logarithmische Temperaturskala erstreckt sich von minus Unendlich bis plus Unendlich. Es ist kein absoluter Nullpunkt erforderlich, dessen Bestimmung nicht exakt möglich ist.

Die bekanntesten Temperaturskalen mit ihren verschiedenen Charakteristika sind weiter unten tabellarisch dargestellt. Die heute gültige Temperaturskala ist die „International Temperature Scale of 1990“ (ITS-90).

Die Einheit der thermodynamischen Temperatur (Formelzeichen formula_1) ist das Kelvin mit dem Einheitenzeichen K. Das Kelvin ist eine SI-Basiseinheit. Es ist der 273,16te Teil der thermodynamischen Temperatur des Tripelpunktes von Wasser, bei dem dessen feste, flüssige und gasförmige Phase koexistieren. Der Nullpunkt der Kelvinskala liegt beim absoluten Nullpunkt. Der Wert 273,16 ist so gewählt, weil der Tripelpunkt mit guter Näherung bei 0,01 °C liegt und der absolute Nullpunkt bei −273,15 °C.

Die Celsiustemperatur (Formelzeichen formula_24 oder auch formula_25) gibt nach ihrer modernen Definition nicht mehr die "empirische Temperatur" der historischen Celsius-Skala an, sondern ist die thermodynamische Temperatur der Kelvin-Skala mit um 273,15 kleineren Zahlenwerten:

Die Einheit Grad Celsius (°C) ist eine abgeleitete SI-Einheit. Das Grad Celsius ist identisch zum Kelvin. Temperaturdifferenzen werden in K angegeben; die Differenz zweier Celsiustemperaturen kann auch in °C angegeben werden. Der Zahlenwert ist in beiden Fällen gleich.

In den USA ist die Fahrenheit-Skala mit der Einheit Grad Fahrenheit (Einheitenzeichen: °F) immer noch sehr gebräuchlich. Die absolute Temperatur auf Fahrenheit-Basis wird mit Grad Rankine (Einheitenzeichen: °Ra) bezeichnet. Die Rankine-Skala hat den Nullpunkt wie die Kelvin-Skala beim absoluten Temperaturnullpunkt, im Gegensatz zu dieser jedoch die Skalenabstände der Fahrenheit-Skala.

In der folgenden Tabelle, die nur einen groben Überblick geben soll, sind Beispiele einiger Temperaturen genannt. Spezifische Stoffwerte können Artikeln wie beispielsweise Siedepunkt und Schmelzpunkt entnommen werden.



</doc>
<doc id="5276" url="https://de.wikipedia.org/wiki?curid=5276" title="Äquivalenzprinzip (Physik)">
Äquivalenzprinzip (Physik)

Das Äquivalenzprinzip der Physik drückt aus, dass die schwere und die träge Masse eines Körpers zwei äquivalente Größen sind. Diese Formulierung gibt in moderner Ausdrucksweise die frühen Feststellungen von Galileo Galilei und Isaac Newton wieder, dass beim freien Fall alle Körper gleich beschleunigt werden bzw. dass alle Gravitationswirkungen proportional zur Masse der beteiligten Körper sind. Albert Einstein erkannte ab 1907 hierin ein mögliches Grundprinzip einer Theorie der Gravitation, das ihn schließlich zur allgemeinen Relativitätstheorie leitete.

Das Äquivalenzprinzip existiert in zwei Formen: Nach dem schwachen Äquivalenzprinzip bestimmt von allen Eigenschaften eines Körpers allein seine Masse (also das Maß seiner Trägheit), welche Schwerkraft in einem gegebenen homogenen Gravitationsfeld auf ihn wirkt. Seine weiteren Eigenschaften wie chemische Zusammensetzung, Größe, Form etc. haben keinen Einfluss. Nach dem starken Äquivalenzprinzip gilt, dass Gravitations- und Trägheitskräfte auf kleinen Abstands- und Zeitskalen in dem Sinn äquivalent sind, dass sie an ihren Wirkungen weder mit mechanischen noch irgendwelchen anderen Beobachtungen unterschieden werden können. Aus dem starken Äquivalenzprinzip folgt das schwache; ob das auch umgekehrt gilt, hängt möglicherweise von der genauen Formulierung ab und ist noch nicht abschließend geklärt.

Das schwache Äquivalenzprinzip gilt als Folge des newtonschen Gravitationsgesetzes in der klassischen Mechanik. Als Folge beschreiben alle Körper im freien Fall in einem äußeren Gravitationsfeld (bei gleichen Anfangsbedingungen) in derselben Zeit dieselbe Bahn. Daher gibt es relativ zu einem mitbewegten Bezugssystem während des freien Falls keine Auswirkungen des äußeren Gravitationsfeldes auf die Bewegung der Körper, was als Zustand der Schwerelosigkeit bezeichnet wird.

Als "träge Masse" formula_1 wird die Masse im zweiten Newtonschen Axiom bezeichnet:
Das erste Newtonsche Axiom drückt aus, dass Körper träge sind: Sie verharren in ihrem Bewegungszustand, solange keine Kraft auf sie wirkt. Das zweite Axiom quantifiziert die Trägheit: Je mehr träge Masse ein Körper besitzt, desto größer muss die Kraft sein, um ihm eine bestimmte Beschleunigung zu erteilen. Die träge Masse ist additiv: Setzt man einen Körper aus Bestandteilen zusammen, so addieren sich ihre trägen Massen, wenn man die Bindungsenergien der Bestandteile vernachlässigen kann.

Die "schwere Masse" ist ein Maß für die gravitative Anziehungskraft zweier Körper. In der klassischen Mechanik wird die Gravitationskraft durch Newtons Gravitationsgesetz beschrieben. Ein Körper der schweren Masse formula_3 zieht einen anderen Körper der schweren Masse formula_4 im Abstand formula_5 mit einer Kraft vom Betrag
an. Ebenso wie die träge Masse ist die schwere Masse additiv: Setzt man einen Körper aus Bestandteilen zusammen, so addieren sich ihre schweren Massen, wenn man die Bindungsenergien vernachlässigen kann.

Beide Massenarten sind a priori unabhängig voneinander, wie z. B. die träge Masse eines Teilchens und seine elektrische Ladung. Aber alle bislang durchgeführten Experimente bestätigen, dass die schwere Masse eines Körpers seiner trägen Masse entspricht. Träge und schwere Masse sind "äquivalent." Dieses experimentelle Ergebnis wird "schwaches Äquivalenzprinzip" genannt.

Das schwache Äquivalenzprinzip manifestiert sich in Galileis Fallgesetz, dass alle Körper gleich schnell fallen. Im Erdschwerefeld gilt für einen Körper der schweren Masse formula_7 näherungsweise
als Gravitationsgesetz, mit der Fallbeschleunigung formula_9. Vernachlässigt man Reibungskräfte und den Auftrieb, so ergibt sich die Beschleunigung formula_10 des Körpers durch das zweite Axiom zu
Das Äquivalenzprinzip formula_12 führt nun auf
Alle Körper fallen (im Vakuum) im Erdschwerefeld gleich, unabhängig von ihrer Masse. Wäre das schwache Äquivalenzprinzip verletzt, so würde auch dieses Gesetz ungültig sein. Dies lässt sich experimentell nachprüfen.

Da das Trägheitsgesetz und Newtons Gravitationsgesetz auf voneinander unabhängigen physikalischen Befunden und Axiomen beruhen, bleibt in der klassischen Mechanik das schwache Äquivalenzprinzip unerklärt.

Erste Versuche zu träger und schwerer Masse machten bereits Isaac Newton (dargestellt in seinen Principia, Erstausgabe 1687) und Friedrich Wilhelm Bessel (1832) in Form von Pendelversuchen. Weitere Untersuchungen wurden 1890 und 1909 von dem ungarischen Physiker Loránd Eötvös in dem nach ihm benannten Eötvös-Experiment durchgeführt, das 1964 von Roll, Krotkov und Dicke in Princeton sowie 1972 von Braginsky und Panov in Moskau in verbesserter Form wiederholt wurde. Quantitativ werden solche Messungen zur Äquivalenz von träger und schwerer Masse durch das sog. Eötvös-Verhältnis

beschrieben, wobei formula_15 und formula_16 die gemessenen Beschleunigungen zweier unterschiedlicher Testkörper darstellen. Während die klassischen Pendelversuche von Newton und Bessel eine Obergrenze von formula_17 erreichten, verbesserten die Torsionspendelversuche von Eötvös (1909) diese Grenze auf formula_18. Durch Experimente mit den Laserreflektoren, die bei Apollomissionen auf dem Mond aufgestellt worden waren "(Lunar Laser Ranging)," konnte Irwin Shapiro 1976 die Gültigkeit des Äquivalenzprinzips mit einer Genauigkeit von 10 nachweisen. Eric G. Adelberger u. a. von der Eötwash-Gruppe publizierten 1999 eine Arbeit, die dieses Prinzip mit einer Genauigkeit von 10 bestätigt.

Schärfere Obergrenzen lassen sich durch satellitengestützte Experimente wie z. B. die STEP-Mission "(Satellite Test of the Equivalence Principle)," Gravity Probe A oder Microscope erzielen. Hierbei werden z. B. die relativen Beschleunigungen von im Orbit befindlichen, frei fallenden Testkörpern mit unterschiedlicher chemischer Zusammensetzung gemessen, was zu einer erwarteten Genauigkeit von formula_19 (Microscope) und formula_20 (STEP) führen soll. Hierbei wird auch, direkter als in früheren Experimenten, die Formulierung des Äquivalenzprinzips der allgemeinen Relativitätstheorie (ART) überprüft.

Folge des schwachen Äquivalenzprinzips ist, dass ein Beobachter in einem geschlossenen Labor, ohne Information von außen, aus dem mechanischen Verhalten von Gegenständen im Labor nicht ablesen kann, ob er sich in Schwerelosigkeit oder im freien Fall befindet (siehe dazu nebenstehende Abbildung). Dies ist gleichbedeutend mit der Aussage, dass Gravitationskräfte äquivalent zu Trägheitskräften sind. Daher können Gravitationskräfte durch Wechsel in ein beschleunigtes Bezugssystem lokal eliminiert werden.

Dieses Prinzip wurde von Einstein 1907 verallgemeinert:

Eine äquivalente aber mathematisierte und mit den Begriffen der allgemeinen Relativitätstheorie ausgedrückte Formulierung des starken Äquivalenzprinzips lautet:
Quantitativ lässt sich dies durch riemannsche Normalkoordinaten darstellen, die zeigen, dass die Abweichungen von der flachen Metrik proportional zum Krümmungstensor sind.

Es muss allerdings beachtet werden, dass dieses Prinzip nur lokal gilt:
Da das elektrische Feld geladener Körper ebenfalls eine große Ausdehnung hat, gibt es eine Kontroverse darüber, ob das Äquivalenzprinzip für solche Teilchen gelte.

Das schwache Äquivalenzprinzip ist durch die klassische Mechanik nicht zu erklären. Dagegen wird in der ART das starke Äquivalenzprinzip zum Ausgangspunkt der Theorie erhoben: Testteilchen durchlaufen unabhängig von ihrer Zusammensetzung oder anderen Beschaffenheit dieselbe Fallkurve, wenn anfänglich ihr Ort und ihre Geschwindigkeit übereinstimmen. Im newtonschen Sinne sind also träge und passive schwere Massen, zwischen denen man in der ART nicht unterscheiden kann, äquivalent. Dass alle Testteilchen dieselben Fallkurven durchlaufen, ergibt sich in der ART daraus, dass sich die Lagrangedichte der ART bei Wechsel der Koordinaten nicht ändert. Das Äquivalenzprinzip ist somit als fundamentale Symmetrie in der Theorie vorhanden.

Die Beobachtung einer Verletzung des Äquivalenzprinzips würde daher zeigen, dass die ART nur begrenzt gültig wäre. Mit heutiger Messgenauigkeit hat man keine Abweichungen vom Äquivalenzprinzip beobachten können.

Darüber hinaus gilt in der ART ein aktives Äquivalenzprinzip, dass nämlich verschiedene Materie oder Strahlung dieselbe Gravitation erzeugen, wenn nur ihr Energie-Impuls-Tensor übereinstimmt.




</doc>
<doc id="5277" url="https://de.wikipedia.org/wiki?curid=5277" title="Teilbarkeit">
Teilbarkeit

Teilbarkeit ist eine mathematische Beziehung zwischen zwei ganzen Zahlen. Eine ganze Zahl ist durch eine andere ganze Zahl teilbar, wenn bei der Division kein Rest verbleibt, also die „Geteilt-Rechnung aufgeht“.

So ist beispielsweise die Zahl 8 durch 4 teilbar, da 8 : 4 genau 2 ergibt; somit ist 4, aber auch 2, Teiler von 8. Dagegen ist die Zahl 9 nicht durch 4 teilbar, weil die 4 zweimal in die 9 „geht“, aber ein Rest von 1 übrig bleibt.

Die Zahl 11 hat nur zwei Teiler: 1 und die Zahl 11 selbst. Solche Zahlen nennt man Primzahlen. Die Zahl 12 dagegen hat viele Teiler: 1, 2, 3, 4, 6 und 12. Solche Zahlen nennt man hochzusammengesetzte Zahlen.

Die Funktion, die einer natürlichen Zahl formula_1 die Anzahl ihrer Teiler zuordnet, ist eine zahlentheoretische Funktion (die Teileranzahlfunktion). In der Zahlentheorie ist der Begriff "Teilbarkeit" auf natürliche Zahlen beschränkt.

In der Algebra dagegen wird der Begriff "Teilbarkeit" auf Integritätsringe, kommutative Ringe und nicht-kommutative Ringe erweitert.

Eine ganze Zahl formula_2 "teilt" eine ganze Zahl formula_3 genau dann, wenn es eine ganze Zahl formula_1 gibt, für die formula_5 ist. Man sagt dann „formula_2 ist "Teiler" von formula_3“, „formula_3 ist "teilbar" durch formula_2“, oder „formula_3 ist Vielfaches von formula_2“ und schreibt formal:
Für das Gegenteil, wenn es also "keine" ganze Zahl formula_1 gibt, für die formula_5 ist, schreibt man:

Da formula_16 für alle formula_1 gilt, ist formula_18 ein Teiler von formula_18 und von keiner anderen Zahl, also formula_20 für jedes formula_21

Schreibt man denselben Sachverhalt in der Form formula_22, so erkennt man, dass jede Zahl formula_2 ein Teiler von formula_18 ist.

Die formula_25 ist das neutrale Element der Multiplikation, d. h. die Multiplikation mit formula_25 ändert einen Ausgangswert nicht. Zu den Elementen formula_27 gibt es ein multiplikatives Inverses, nämlich ein Element formula_28 mit formula_29. Solche Elemente werden Einheiten des Rings genannt. Einheiten sind "triviale Teiler" einer jeden ganzen Zahl. Die Einheiten des Rings formula_30 der ganzen Zahlen sind gerade die Zahlen formula_31. (Die Einheiten eines Rings bilden eine multiplikative Gruppe.)

Es gelte formula_12 und formula_33. Ist formula_2 keiner der "trivialen Teiler" formula_35, so nennt man formula_2 einen "nichttrivialen Teiler" oder "echten Teiler" von formula_3. Eine ganze Zahl, die nicht Einheit ist und die nur die trivialen Teiler besitzt, nennt man Primelement und, wenn sie formula_38 ist, Primzahl. Ist formula_2 eine Primzahl, so heißt formula_2 Primteiler oder Primfaktor von formula_3.

Die Menge aller Teiler einer natürlichen Zahl formula_1 nennt man die „Teilermenge von formula_1“. Die Quasiordnung der Teilbarkeit induziert auf ihr die Struktur eines Verbandes, man spricht deshalb auch vom „Teilerverband von formula_1“.

Die Menge aller Vielfachen einer natürlichen Zahl formula_1 heißt entsprechend "Vielfachenmenge". Bei den ganzen Zahlen formula_30 ist die Mächtigkeit dieser Menge abzählbar unendlich.


Seien formula_2, formula_3, formula_52 und formula_53 ganze Zahlen.

Die natürlichen Zahlen formula_74 sind mit der Teilbarkeitsrelation eine quasigeordnete Menge, sogar ein vollständiger distributiver Verband, dessen Verknüpfungen durch kgV und ggT gegeben sind. Das kleinste Element ist die formula_25 (formula_25 teilt jedes andere), das größte ist die formula_18 (formula_18 wird von jedem anderen geteilt).





Will man für eine Zahl formula_90 eine Teilbarkeitsregel mit Quersummen aufstellen, so sucht man nach einem Vielfachen, das entweder formula_91 oder formula_92 für ein beliebiges formula_1 ist. Im ersten Fall kann die Teilbarkeit mit der nichtalternierenden formula_1-Quersumme, im zweiten Fall mit der alternierenden formula_1-Quersumme überprüft werden.

Entsprechende Faktoren existieren für alle Zahlen, die mit 10 teilerfremd sind. Allerdings ist die Prüfung zum Teil schon für relativ kleine Zahlen unpraktisch (siehe zum Beispiel die unten angegebenen Regeln für Teilbarkeit durch 17 und 19).

Ist formula_91 ein Vielfaches der betrachteten Zahl formula_90, dann gilt die Teilbarkeitsregel: „Eine Zahl ist genau dann durch formula_90 teilbar, wenn ihre nichtalternierende formula_1-Quersumme durch formula_90 teilbar ist.“

Beispielsweise ist formula_101 ein Vielfaches von 3, so dass die Teilbarkeit durch 3 anhand der (1er-)Quersumme geprüft werden kann.


Die Quersumme muss nicht vollständig berechnet werden, sondern es genügt, den Rest einer Ziffer (oder Zifferngruppe) bei Division durch formula_90 zu berücksichtigen. Es kann auch nach jeder Addition der Rest bei Division durch formula_90 berechnet werden. Um z. B. zu ermitteln, ob 7654 durch 3 teilbar ist, kann man rechnen:
Da der im letzten Schritt berechnete Rest nicht Null ist, ist 7654 nicht durch 3 teilbar.

Ist hingegen formula_92 ein Vielfaches der betrachteten Zahl formula_90, dann gilt die Teilbarkeitsregel: „Eine Zahl ist genau dann durch formula_90 teilbar, wenn ihre alternierende formula_1-Quersumme durch formula_90 teilbar ist.“

Betrachtet man beispielsweise die Zahl 7, so kann man durch Ausprobieren sehen, dass formula_119. Daraus ergibt sich dann die Teilbarkeitsregel mit einer alternierenden 3er-Quersumme.


Neben der schon genannten Teilbarkeitsregel mittels der alternierenden 3er-Quersumme gibt es für die 7 weitere, teils einfachere, Teilbarkeitsregeln.

Im Babylonischen Talmud findet sich eine Teilbarkeitsregel, bei der man letztlich nur überprüfen muss, ob eine zweistellige Zahl durch 7 teilbar ist. Dazu wird eine Zahl an der vorletzten Stelle in zwei Teile aufgespalten. Die Ziffern vor der vorletzten Stelle bilden die Zahl formula_2 und die letzten beiden Ziffern die Zahl formula_3. 3815 wird beispielsweise in die Zahlen formula_125 und formula_126 zerlegt. Nun zählt man formula_3 und das Doppelte von formula_2 zusammen. Ist die Summe durch 7 teilbar, so ist auch die ursprüngliche Zahl durch 7 teilbar. Für 3815 erhält man so formula_129. Da 91 durch 7 teilbar ist, ist auch 3815 durch 7 teilbar. Bei sehr großen Zahlen kann man dieses Verfahren solange wiederholen, bis man irgendwann eine zweistellige Zahl erhält. Um die Gültigkeit der Teilbarkeitsregel zu zeigen, betrachtet man die Gleichung
Da 98 und damit auch formula_131 durch 7 teilbar ist, ist formula_1 genau dann durch 7 teilbar, wenn formula_133 durch 7 teilbar ist.

Für eine weitere Teilbarkeitsregel spaltet man eine Zahl in ihre letzte Ziffer formula_3 und den Rest formula_2 auf. Zum Beispiel 3815 in die Zahlen formula_136 und formula_137. Dann gilt folgender Satz:
Für 3815 muss man also überprüfen, ob formula_141 durch 7 teilbar ist. Dazu kann man 371 wieder in 37 und 1 zerlegen. Da formula_142 durch 7 teilbar ist, sind auch 371 und 3815 durch 7 teilbar.

Man kann eine Zahl formula_1 auch vor der drittletzten Ziffer spalten, so dass die letzten drei Ziffern die Zahl formula_2 und die Ziffern davor die Zahl formula_3 bilden. Dann zieht man formula_3 von formula_2 ab und prüft, ob diese Differenz durch 7 teilbar ist. Da
und formula_149 durch 7 teilbar ist, ist formula_1 genau dann durch 7 teilbar, wenn formula_151 durch 7 teilbar ist.

Ein Verfahren, um die Teilbarkeit durch 17 festzustellen, beruht auf der Identität 17 · 6 = 102. Deswegen gilt
Man spaltet also die zu prüfende Zahl formula_1 vor der vorletzten Stelle in zwei Teile, nimmt das Doppelte des linken Teils und zieht den rechten Teil ab (oder umgekehrt). Ist das Resultat durch formula_154 teilbar, so gilt dies auch für formula_1.

Beispiel: formula_156. 
Also formula_157, was durch 17 teilbar ist.

Um die Teilbarkeit durch 19 zu überprüfen, spaltet man eine Zahl in ihre letzte Ziffer formula_3 und den Rest formula_2 auf. Zum Beispiel 7904 in die Zahlen formula_160 und formula_161. Dann gilt folgender Satz:

Für 7904 muss man also überprüfen, ob formula_164 durch 19 teilbar ist. Dazu kann man 798 wieder in 79 und 8 zerlegen. Da formula_165 durch 19 teilbar ist, sind auch 798 und 7904 durch 19 teilbar.

Um die Teilbarkeit durch eine beliebige Zahl formula_1 zu überprüfen, verwendet man deren Primfaktorzerlegung. Man überprüft dann die Teilbarkeit durch die einzelnen Primzahlpotenzen dieser Zerlegung.

Beispielsweise ist eine Zahl genau dann durch formula_167 teilbar, wenn sie durch formula_168 und 3 teilbar ist. Das heißt, ihre letzten beiden Ziffern müssen 00, 25, 50 oder 75 sein und die Quersumme durch drei teilbar sein.

In einem Zahlensystem zur Basis formula_169 lassen sich Teilbarkeitsregeln für Teiler formula_170 finden, die sich in eine teilerfremde Faktorenzerlegung möglichst kleiner Zahlen zerlegen lässt, die Teiler von formula_171, formula_172 oder formula_173 sind. formula_1 sollte dabei möglichst klein sein, für Kopfrechnen sind nur Werte bis maximal 4 sinnvoll.

Die folgenden Teilbarkeitsregeln benutzen andere Stellenwertsysteme:

Weitere Teilbarkeitseigenschaften findet man im Artikel Kongruenz (Zahlentheorie).

Der Teilbarkeitsbegriff wird auch wesentlich allgemeiner in kommutativen Ringen betrachtet. Die Definition von Teilbarkeit in natürlichen und ganzen Zahlen wird hier direkt übernommen:

Es sei formula_193 ein kommutativer Ring. Sind formula_194 Ringelemente, dann ist formula_2 ein Teiler von formula_3, falls ein weiteres Ringelement formula_197 mit formula_5 existiert.

In Ringen teilt formula_2 genau dann formula_3, wenn das von formula_2 erzeugte Hauptideal formula_202 das von formula_203 erzeugte umfasst, formal: formula_204.

Ein einfaches Beispiel aus den ganzen Zahlen: Das von formula_205 erzeugte Hauptideal formula_206 ist die Menge aller Vielfachen von formula_205, formula_208 dementsprechend die Menge aller Vielfachen von formula_209. formula_210, also ist formula_205 ein Teiler von formula_209.

Die fruchtbarsten Teilbarkeitseigenschaften erhält man in Integritätsringen, das sind nullteilerfreie kommutative unitäre Ringe.

Bei nicht-kommutativen Ringen formula_213 muss man bei der Teiler- und Vielfachen-Eigenschaft die Seitigkeit (linke, rechte oder zweiseitige) mit angeben. Dies lässt sich mit dem einfachen Teilbarkeitssymbol „formula_214“ (dessen symmetrische Gestalt schon einer Spiegelung mit inverser Bedeutung im Wege steht) des kommutativen Falls nicht mehr ausdrücken.

Von zwei Elementen formula_215 heißt formula_216 linker Teiler von formula_217, falls ein formula_218 mit formula_219 existiert. Dann ist auch formula_217 rechtes Vielfaches von formula_216. Diese Teilbarkeit entspricht der Inklusion der Rechtsideale formula_222. Entsprechend definiert man rechten Teiler, linkes Vielfaches und, wenn für links wie rechts gültig, auch zweiseitigen Teiler, zweiseitiges Vielfaches.

In Strukturen, in denen auch eine allgemeine Division als Umkehr der Multiplikation möglich ist (Körper und Schiefkörper), wie beispielsweise in den reellen Zahlen, ist die Theorie der Teilbarkeit trivial: Jede Zahl (bzw. jedes Körper-Element) ist durch jede andere Zahl außer formula_18 teilbar, d. h. auch: alle von 0 verschiedenen Elemente sind Einheiten.




</doc>
<doc id="5278" url="https://de.wikipedia.org/wiki?curid=5278" title="Tom Wiggins">
Tom Wiggins

Tom Wiggins (* 25. Mai 1849 in Harris County, Georgia; † 13. Juni 1908 in Hoboken, New Jersey) war ein blinder US-amerikanischer Pianist und Komponist mit einer Inselbegabung.

Wiggins wurde 1849 auf der „Wiley Edward Jones Plantage“ in Georgia geboren. Er war von Geburt an blind. Im Herbst 1850 wurde er gemeinsam mit seinen versklavten Eltern Charity und Mingo Wiggins und zwei Brüdern an Colonel James N. Bethune, einen Rechtsanwalt und Herausgeber, verkauft. Wiggins ist daher auch unter dem Namen seines Besitzers, James Bethune, als "Tom Bethune" (Thomas Greene Bethune) bekannt. Da Wiggins nicht arbeiten konnte, hörte er oftmals den Töchtern der Familie Bethune beim Musizieren zu und begann, selbst zu spielen. Schon im Alter von sechs Jahren wurde Wiggins zu Aufführungen in der Nachbarschaft der Bethunes gebracht. Das erste öffentliche Konzert gab Wiggins im Alter von acht Jahren am 7. Oktober 1856 in der "Temperance Hall" in Columbus. In der Folge wurde er zu Auftritten nach Atlanta, Macon und Athens gebracht. In der Zeitung "Athens Southern Watchman" wurde ein durchweg positiver Bericht über Wiggins' Auftritt veröffentlicht.

Kurz nach dem Tod von Colonel Bethunes Frau im Mai 1858 wurde Wiggins als Sklave an den Tabakplantagenbesitzer Perry Oliver ausgeliehen. In dem Drei-Jahres-Vertrag wurde eine Summe von 15.000 US-Dollar für das Recht, Wiggins auch außerhalb Georgias einzusetzen, vereinbart.

Wiggins spielte für Oliver einige Konzerte in Savannah. 1860 gab er ein Konzert in Baltimore, bei dem auch der Klavierbauer William Knabe anwesend war. Dieser zeigte sich so beeindruckt vom Können Wiggins, dass er ihm ein großes Klavier aus Rosenholz schenkte. Im selben Jahr wurden zwei Kompositionen des Zehnjährigen veröffentlicht: "Oliver Galop" und die "Virginia Polka". 1861 spielte er in Washington, D.C. für die ersten japanischen Diplomaten in den Vereinigten Staaten.

Mit Beginn des Sezessionskrieges brachte Perry Oliver den 12-jährigen wieder zurück nach Georgia. Im Oktober 1862 wurde er wieder zurück zu Colonel Bethune gebracht, der dort nach Erzählungen eines Sohnes des Colonels, der für die Konföderation kämpfte, das Stück "Battle of Manassas" komponierte. Gegen Ende des Sezessionskrieges, als General Bethune erkannte, dass die Südstaaten den Krieg verlieren würden, schloss er einen Vertrag mit Wiggins Eltern ab. Tom Wiggins sollte von Bethune gemanagt werden, freie Kost und Logis sowie eine musikalische Ausbildung erhalten, und einen Lohn von 20 US-Dollar im Monat. Wiggins Eltern sollten laut Vertrag 500 US-Dollar im Jahr sowie ebenfalls freie Kost und Logis erhalten. 1866, nach einem vierwöchigen Aufenthalt in New York City, wo Wiggins in der "Irving Hall" Konzerte gab, wurde Wiggins auf eine Europa-Tournee geschickt, wo er unter anderem vor Ignaz Moscheles und Charles Halle spielte, die ihm begeisterte Empfehlungen ausstellten und von einem „musikalischen Wunder“ sprachen. 1868 tourte Wiggins durch Nordamerika und Kanada. 1870 verdienten die Bethunes als Wiggins Manager bereits 50 000 US-Dollar pro Jahr mit Wiggins' Konzertauftritten. Am 25. Juli 1870 ernannte sich General Bethune selbst zu Wiggins Vormund und hob damit den mit Wiggins Eltern geschlossenen Vertrag auf. Wiggins lebte jetzt mit General Bethune in einer Pension in New York City, und verbrachte die Sommer auf der Farm der Bethunes in Virginia. Bethune starb am 16. Februar 1884, als er versuchte, auf einen bereits anfahrenden Zug aufzuspringen und dabei unter die Räder geriet.

Wiggins starb 59-jährig an einem Schlaganfall. Er war, genau wie heute Leslie Lemke, musikalisch außergewöhnlich begabt, aber in allen anderen Lebensbereichen unterdurchschnittlich befähigt und wird teils als Autist gesehen.





</doc>
<doc id="5280" url="https://de.wikipedia.org/wiki?curid=5280" title="Tannhäuser und der Sängerkrieg auf Wartburg">
Tannhäuser und der Sängerkrieg auf Wartburg

Tannhäuser und der Sängerkrieg auf Wartburg ist eine romantische Oper in drei Akten von Richard Wagner, welche auf der traditionellen Volksballade "Tannhauser" und dem Sängerkrieg auf der Wartburg aufbaut. Die Oper thematisiert den Zwiespalt zwischen heiliger und profaner Liebe und die Erlösung durch Liebe – ein Leitthema, welches sich auch durch viele von Wagners späten Werken zieht.

Zu Beginn des Werks befindet sich Tannhäuser im Inneren des Venusberges (d. i. der Hörselberg bei Eisenach). Die Welt der Venus ist ausschließlich der Sinnlichkeit geweiht. Tannhäuser hat als Sterblicher den Weg hierher gefunden, ist des Genusses jedoch zusehends überdrüssig („Wenn stets ein Gott genießen kann, bin ich dem Wechsel untertan“). Venus versucht, ihren Ritter zum Bleiben zu bewegen, und prophezeit, dass die Menschen Tannhäuser sein Verweilen bei der heidnischen Göttin der Liebe nie verzeihen würden: Dort finde er nie sein Heil. Tannhäuser jedoch bleibt bei seinem Entschluss: „Mein Heil ruht in Maria!“ Bei der Anrufung Mariens versinkt die Welt der Venus, und Tannhäuser sieht sich in ein liebliches Waldtal in Thüringen am Fuße der Wartburg versetzt. Er versöhnt sich mit den Rittern, die er einst verlassen hatte, und will an einem Sängerfest, das auf den nächsten Tag bestimmt ist, teilnehmen, auch um das Herz Elisabeths, der Nichte des Landgrafen, zu gewinnen.

Im zweiten Aufzug begegnen sich Tannhäuser und Elisabeth erstmals wieder. In einem Dialog zwischen beiden und einem Gespräch zwischen Elisabeth und dem Landgrafen wird die Wunschverbindung der beiden kaum verhüllt vorbereitet. Das Sängerfest beginnt, wobei jeder der Teilnehmer das Wesen der Liebe besingen soll. Dabei gerät Tannhäuser jedoch, schon bevor er an der Reihe ist, immer mehr in Rage, da er merkt, dass die anderen Minnesänger offensichtlich von wahrer Sinnlichkeit keine Ahnung haben, die er ja im Venusberg erfahren hat. Dies verleitet ihn, als er einen Lobpreis auf die Liebe singen soll, zu dem Bekenntnis, dass er im Venusberg gewesen ist, wodurch er nach den Moralgesetzen der Zeit Abscheu und allgemeine Empörung hervorruft und vom Fürsten verdammt wird. Die Anrufung von oder der Aufenthalt bei heidnischen Göttern war allemal Götzendienst und Abgötterei. Nur auf die Fürsprache Elisabeths wird Tannhäuser gestattet, sich den Pilgern, die nach Rom ziehen, anzuschließen, um Buße zu tun und um Vergebung zu bitten.
Im dritten Aufzug erleben wir, wie Elisabeth vergeblich auf die Rückkehr Tannhäusers wartet; unter den heimkehrenden Pilgern, die in Rom Gnade gefunden haben, ist er nicht. Verzweifelt zieht sie sich zurück, das Geleit Wolframs lehnt sie stumm ab. Wolfram besingt in einem traurigen Lied die Todesahnung, die wie Dämmerung die Lande deckt. Da naht Tannhäuser. Er ist in Rom gewesen, hat jedoch keine Vergebung gefunden (Romerzählung). Der Papst (in der Sage ist es Urban IV.) sprach, auf seinen Priesterstab zeigend: „Wie dieser Stab in meiner Hand nie mehr sich schmückt mit frischem Grün, kann aus der Hölle heißem Brand Erlösung nimmer dir erblühn.“ Tannhäuser ist verzweifelt und sehnt sich nun zurück ins Reich der Venus. Diese erscheint auch, doch Wolfram hält Tannhäuser von dem verzweifelten Schritt ab. Er ruft Elisabeths Namen aus, deren Bitte, für Tannhäusers Vergebung sterben zu dürfen, inzwischen erfüllt wurde. Venus ist augenblicklich verschwunden. Tannhäuser stirbt, mit seinen letzten Worten Elisabeth um Hilfe bei Gott bittend. Pilger bringen den wundersam erblühten Priesterstab aus Rom: „Den dürren Stab in Priesters Hand hat er geschmückt mit frischem Grün“ – ein Zeichen, dass Gott selbst Tannhäuser Erlösung gewährte. Nach dem Lob Gottes durch den Chor: „Hoch über aller Welt ist Gott, und sein Erbarmen ist kein Spott!“ schließt das Werk mit dem Satz: „Der Gnade Heil ist dem Büßer beschieden, er geht nun ein in der Seligen Frieden!“

"Tannhäuser" ist Wagners fünfte vollendete Oper und entstand ab 1842. Sie beruht auf zwei ursprünglich unabhängigen Sagen, der von Heinrich von Ofterdingen und dem Sängerkrieg auf der Wartburg zur Zeit Landgraf Hermanns I. von Thüringen (gestorben 1217) einerseits sowie der vom Tannhäuser, der für sein Verweilen im Venusberg Vergebung bei Papst Urban IV. (Papst 1261–1264) suchte. In Ludwig Bechsteins Sammlung "Die Sagen von Eisenach und der Wartburg, dem Hörselberge und Reinhardsbrunn" von 1835 werden diese beiden - mindestens 50 Jahre auseinanderliegenden, jedoch sehr wahrscheinlich fiktiven - Ereignisse unmittelbarer nacheinander referiert, was Wagner auf die Idee brachte, sie zu einer Handlung und die Figuren des Heinrich von Ofterdingen und des Tannhäuser zu einer Gestalt zu verschmelzen. Bechstein war im Übrigen auch nicht der Entdecker der Stoffe. Beide finden sich bereits in den "Deutschen Sagen" der Brüder Grimm, die vom "Tannhäuser" auch in "Des Knaben Wunderhorn". Nach Wagners Aussage in seiner Autobiographie "Mein Leben" kannte er auch zumindest zwei literarische Fassungen der Sagen: Ludwig Tiecks Erzählung "Der getreue Eckart und der Tannhäuser" und E.T.A. Hoffmanns Novelle "Der Kampf der Sänger" aus "Die Serapionsbrüder".

Textbuch und Partitur entstanden mit Unterbrechungen innerhalb von drei Jahren, die Uraufführung fand am 19. Oktober 1845 im "Königlich Sächsischen Hoftheater" in Dresden statt.

Die Fassung der Uraufführung repräsentiert das Stadium 1 im "Wagner-Werk-Verzeichnis" (WWV). Schon unmittelbar danach nahm Wagner erste Änderungen vor: er verkürzte die Einleitung zum III. Akt ("Tannhäusers Pilgerfahrt") und ließ am Schluss Venus noch einmal auftreten und den Leichenzug der Elisabeth szenisch aufführen. Außerdem gestattete er verschiedene Kürzungen, vor allem aus zwei Gründen: zum einen weil die Titelpartie (nach wie vor) eine der anstrengendsten und schwierigsten des Tenor-Repertoires ist und zum anderen, um Szenen mit stummem Spiel abzukürzen, das die Sänger des 19. Jahrhunderts nur selten zur Zufriedenheit des Komponisten ausführten. Die Änderungen und Kürzungsmöglichkeiten erläutert Wagner 1852 ausführlich in seiner Schrift "Über die Aufführung des Tannhäuser". 1860 gelang es ihm, die Partitur von Breitkopf & Härtel mit den bis dahin für gültig erklärten Änderungen und Varianten neu stechen zu lassen. Hiermit ist Stadium 2 des WWV erreicht. Diese Fassung (nicht die der Uraufführung) wird in der Regel als "bühnengebräuchliche " oder "Dresdener Fassung" bezeichnet.

1861 eröffnete sich für Wagner die Möglichkeit, den "Tannhäuser" an der Pariser Opéra aufzuführen. Dafür erarbeiteten Charles Nuitter und andere in enger Zusammenarbeit mit dem Komponisten eine französische Textfassung. Außerdem war ein Ballett für den II. Akt obligatorisch, was Wagner zunächst kategorisch ablehnte. Er entschloss sich jedoch bald dazu, der Venusberg-Szene ein Bacchanal vorangehen zu lassen (nach den Maßstäben dieser Zeit konnte man es kaum als Ballett bezeichnen) und das nachfolgende Duett vor allem musikalisch neu zu gestalten. Die Neufassung zeigt in Hinsicht auf Harmonik und Instrumentation deutliche Einflüsse der Arbeit an "Tristan und Isolde". Außerdem fügte er zum Chor am Ende des III. Aktes Streicher und Harfe hinzu. Die Pariser Aufführungen in französischer Sprache bilden Stadium 3 im WWV. Dass die Pariser Aufführungen Wagner in Frankreich nicht zum Durchbruch verhalfen, lag nur zu einem geringen Teil daran, dass es kein Ballett im II. Akt gab, sondern viel mehr an der insgesamt ungewohnten und neuartigen Faktur des Werkes.

Für Musteraufführungen in München 1867 und Wien 1875 nahm der Komponist nur noch eine wesentliche Änderung vor: von nun an sollte die erste Szene unmittelbar an die Ouvertüre anschließen, wobei diese in München um einige Takte, in Wien um die zweite Hälfte gekürzt wurde. Die in Paris durch lokale Umstände erfolgten Striche wurden wieder aufgemacht. Das hiermit erreichte Stadium 4 des WWV wird im Allgemeinen "Pariser Fassung" genannt (richtig wäre eigentlich "Wiener Fassung" oder "Fassung letzter Hand").

Wagner plante bereits zu seinen Lebzeiten, den "Tannhäuser" in das Repertoire der Bayreuther Festspiele aufzunehmen, wobei er unschlüssig war, ob er der Dresdener oder der Wiener Fassung den Vorzug geben sollte oder einer neuen, wieder der Dresdener angenäherten (vgl. Cosimas Tagebuch-Einträge 6. November 1877, 13. März 1881 und 23. Januar 1883). "Tannhäuser" wurde in Bayreuth erstmals 1891 gespielt.

Bei den Bayreuther Festspielen war es üblich, die Länge der einzelnen Aufzüge zu dokumentieren, jedoch wurden dort nicht alle Jahre erfasst. Einfluss auf die Dauer hatten auch die Art der Stimme und das Temperament der Sänger. Im Falle des Tannhäuser führten unterschiedliche Fassungen zu größeren Zeitunterschieden als sonst zwischen Aufführungen verschiedener Dirigenten gewohnt.
<nowiki>*</nowiki> Wegen Inszenierung unterschiedlicher Fassungen nicht immer repräsentativ. Prozentangaben beziehen sich auf die kürzeste Dauer.

3 Flöten (3. auch Piccolo), 2 Oboen, 2 Klarinetten, Bassklarinette, 2 Fagotte, 2 Ventilhörner, 2 Waldhörner, 3 Trompeten, 3 Posaunen, Basstuba, Pauke, Große Trommel, Becken, Triangel, Tamburin, Harfe, Streicher

Bühnenmusik: Piccolo, 2 Flöten, Englisch Horn, 4 Oboen, 6 Klarinetten, 4 Fagotte, 12 Waldhörner, 12 Trompeten, 4 Posaunen, Trommel, Becken, Tamburin [zusätzlich in der Pariser Fassung: Kastagnetten, Harfe]

In einer jüngeren und vor allem über die Musik hinausgehenden Rezeption wird Wagners Oper auf der Folie der Spieltheorie betrachtet. Innerhalb einer ökonomischen Untersuchung wird eine Analyse des Tannhäuserschen Dilemmas durchgeführt. Das Grunddilemma des Protagonisten der Oper wird innerhalb dieser Untersuchung als Problem der rationalen Entscheidungsfindung interpretiert. Der Held steht demnach innerhalb des Sängerkriegs vor einem nicht zu lösendem Dilemma: Sollte er den Wettbewerb gewinnen, erschwert er seine Sünden mit der Eheschließung der Elisabeth weiterhin, ohne dass ihm die Erlösung zukommen würde. Sollte er hingegen im Wettbewerb unterliegen, würde er seine Geliebte Elisabeth verlieren.








</doc>
<doc id="5282" url="https://de.wikipedia.org/wiki?curid=5282" title="Tautologie (Logik)">
Tautologie (Logik)

Eine Tautologie ( von "t’autó" [aus τὸ αὐτό] „dasselbe“ und -logie), auch Verum ( „wahr“) genannt, ist in der Logik eine allgemein gültige Aussage, das heißt eine Aussage, die aus logischen Gründen immer wahr ist. Beispiele für Tautologien sind Aussagen wie „Wenn es regnet, dann regnet es“ oder „Das Wetter ändert sich oder es bleibt, wie es ist.“

Teilweise wird der Begriff "Tautologie" für alle Arten von allgemeingültigen Aussagen verwendet, teilweise wird er auf solche Aussagen eingeschränkt, die in der zweiwertigen, klassischen Aussagenlogik allgemein gültig sind. Im letzteren, aussagenlogischen Sinn ist eine zusammengesetzte Aussage genau dann eine Tautologie, wenn sie wahr ist unabhängig davon, ob die Teilaussagen, aus denen sie zusammengesetzt ist, ihrerseits wahr oder falsch sind.

Formal wird die Feststellung, dass eine Aussage formula_1 allgemein gültig beziehungsweise eine Tautologie ist, als formula_2 geschrieben.

Eine aussagenlogische Tautologie ist zum Beispiel die Disjunktion „Entweder es regnet, oder es regnet nicht“: Unabhängig davon, ob die in ihr vorkommende Aussage „Es regnet“ wahr ist oder nicht, ist die ganze Aussage wahr: Ist „Es regnet“ wahr, dann ist „Es regnet, oder es regnet nicht“ wahr, weil der "erste" Teilsatz der Disjunktion wahr ist. Ist „Es regnet“ aber falsch, dann ist damit „Es regnet nicht“ wahr. Dies wiederum ist aber der "zweite" Teilsatz der Disjunktion, sodass der ganze Satz auch in diesem Fall wahr ist.

Wenn man den Begriff "Tautologie" im weiteren Sinn verwendet, dann fallen auch Aussagen darunter, die zwar nicht in der Aussagenlogik, aber in anderen logischen Systemen wie der Prädikatenlogik oder der Modallogik allgemein gültig sind. In diesem Sinn ist zum Beispiel die prädikatenlogisch allgemein gültige Aussage „Alle Schafe sind Schafe“ eine prädikatenlogische Tautologie, die modallogisch allgemein gültige Aussage „Es ist möglich, dass es regnet, oder es ist möglich, dass es nicht regnet“ eine modallogische Tautologie.

In mehrwertigen Logiken, also in nichtklassischen Logiken, in denen es mehr als zwei Wahrheitswerte gibt, verliert der Tautologiebegriff seine – vermeintliche oder tatsächliche – umgangssprachliche Natürlichkeit und muss neu definiert werden. Eine Möglichkeit, den Tautologiebegriff in mehrwertige Logik zu übernehmen, besteht darin, aus den Wahrheitswerten einen oder mehrere herauszugreifen und ihnen besondere Bedeutung zuzumessen. Diese herausgegriffenen Pseudowahrheitswerte werden designierte Pseudowahrheitswerte genannt. Man definiert, dass all jene Aussagen Tautologien sind, die für jede Bewertung der in ihnen vorkommenden Atome einen designierten Wahrheitswert liefern. Bei dieser Lösung bleibt der Tautologiebegriff selber zweiwertig, das heißt eine Aussage ist entweder eine Tautologie, oder sie ist keine.



Von zentraler Bedeutung für die Logik sind Methoden, zu prüfen, ob Aussagen kontingent (also in ihrer Wahrheit von den Wahr- oder Falschheiten ihrer Grundbausteine abhängig) oder tautologisch (in jedem Fall wahr) sind.

Während eine solche Prüfung prinzipiell mithilfe jeder Methode möglich ist, mit der für alle möglichen Fälle die Wahr- oder Falschheit einer Aussage ermittelbar ist, nimmt die sogenannte Baummethode einen besonderen Stellenwert ein, da hier nicht jeder einzelne Fall geprüft werden muss.

In der klassischen Aussagenlogik fällt die Aufgabe der Tautologieprüfung mit dem praktisch bedeutsamen und intensiv untersuchten Erfüllbarkeitsproblem der Aussagenlogik zusammen, weil eine Aussage genau dann eine Tautologie ist, wenn ihre Verneinung unerfüllbar ist: Zu prüfen, ob eine Aussage eine Tautologie ist, fällt damit zusammen, zu prüfen, ob ihre Verneinung erfüllbar ist.



</doc>
<doc id="5283" url="https://de.wikipedia.org/wiki?curid=5283" title="Transneptunisches Objekt">
Transneptunisches Objekt

Als transneptunisches Objekt (TNO) oder auch seltener Transneptun bezeichnet man alle Himmelskörper des Sonnensystems, deren mittlere Umlaufbahn (große Halbachse) jenseits der Bahn des äußersten Gasplaneten Neptun liegt.

Im Kuipergürtel befindliche Objekte sind eine Teilmenge der TNO und werden auch als Kuipergürtelobjekte ("KBO", von ) bezeichnet. Heute kennt man einige hundert TNO, vermutet aber allein einige zehntausend Objekte, deren Durchmesser 100 km überschreitet.

Von etwa 1900 bis 1930 stand das Wort „Transneptun“ für einen hypothetischen neunten Planeten, der (irrtümlich) für kleine Bahnstörungen der Planeten Uranus und Neptun verantwortlich gemacht wurde. Der Marsforscher Percival Lowell (1855–1916) hatte lange selbst nach ihm gesucht und dafür das „Lowell-Observatorium“ bei Flagstaff finanziert.

Pluto wurde am 18. Februar 1930 entdeckt und ist das einzige transneptunische Objekt, das für eine gewisse Zeit als Planet galt. Pluto wurde jedoch 2006 auf den Rang eines Zwergplaneten herabgestuft. Ab etwa 1950 suchte man nach einem Transpluto, jedoch wählte man 1977, nach der Entdeckung des ersten Zentauren Chiron, eine andere Terminologie.

Viele transneptunische Objekte sind nicht sehr groß, dunkel und schwer zu erkennen. Die Erforschung der transneptunischen Objekte befindet sich noch in ihren Anfängen, jedes Jahr werden viele Objekte neu entdeckt. Neue und größere Teleskope und computergestützte Bildauswertung ermöglichen neue Erkenntnisse über diese Objekte in schneller Folge. Die Raumsonde New Horizons konnte Pluto und den Kuipergürtel genauer untersuchen und brachte damit neue Erkenntnisse über diesen Bereich des Sonnensystems.

Die bisher entdeckten transneptunischen Objekte sind in ihrer Zusammensetzung kometenähnlich. Viele bekannte Kometen stammen nach Bahn-Messungen aus den 1970er Jahren eher aus dem Kuipergürtel als, wie lange Zeit vermutet, aus der Oortschen Wolke.

Die Transneptune werden als spezielle Gruppe der Asteroiden angesehen und unterscheiden sich von jenen im Hauptgürtel vor allem durch

Die Transneptune kreisen großteils im Kuipergürtel zwischen 30 und 50 AE, sammeln sich aber vielfach zu speziellen Gruppen.

Ein Drittel aller Kuipergürtel-Objekte steht in verschiedenen Bahnresonanzen zum Planeten Neptun. Ihre Umlaufzeiten stehen also in einem einfachen Zahlenverhältnis zu der des Neptun von 164,79 Jahren. Gemäß dem dritten Keplerschen Gesetz haben Objekte mit gleicher Bahnresonanz auch ähnlich große Halbachsen. Die übrigen Bahnelemente wie deren Form (Exzentrizität) und deren Lage (Inklination, Länge des aufsteigenden Knotens und Argument der Periapsis) sind jedoch sehr verschieden. Mögliche Resonanzen sind:

Die Plutinos sind nach dem hellsten von ihnen, dem Zwergplaneten Pluto benannt. "Twotinos" sind eine Abwandlung dieses Begriffes nach dem Zahlenverhältnis 2:1.

Eine andere wichtige Gruppe sind die Cubewanos (oder klassische KBO, CKBO). Die Gruppe ist benannt nach dem ersten entdeckten Objekt dieser Gruppe Albion (vormals "1992 QB"). Sie bewegen sich mit kleinen Exzentrizitäten auf nahezu kreisförmigen Bahnen zwischen 42 und 50 AE mit Bahnneigungen von bis zu 30°. Etwa 2/3 der bekannten KBO bewegen sich auf einer solchen kreisähnlichen Bahn um die Sonne. Zu dieser Gruppe gehören die 1000-km-Objekte Quaoar und Varuna. Klassische KBO weisen keine Bahnresonanz mit den äußeren Planeten auf.

Gestreute KBO (oder Scattered Disk Objects, SDO) bewegen sich mit großen Exzentrizitäten auf Bahnen mit Periheldistanzen nahe 35 AE und Apheldistanzen bis 1000 AE. Bis jetzt sind erst wenige dieser gestreuten KBO bekannt (zum Beispiel (15874) 1996 TL mit einer stark elliptischen Bahn und einer Bahnneigung von 24°).

Die Bahnen einiger transneptunischer Objekte können nicht allein mittels Streuung durch Neptun erklärt werden. Diese „freistehenden Objekte“ (englisch „Detached Objects“ (DO) oder „Distant Detached Objects“ (DDO)) haben Periheldistanzen von mehr als 40 AE, was nicht durch Neptuns Gravitation verursacht sein kann. Die Erklärungsansätze beinhalten eine Störung von außerhalb des Kuipergürtels, z. B. durch einen vorbeifliegenden Stern oder einen außerhalb des Gürtels befindlichen Planeten.

Ende 2003 wurde mit Sedna ein Objekt in seinerzeit dreifacher Pluto-Entfernung entdeckt, das sich auf einer äußerst langgezogenen Ellipse weit außerhalb des Kuipergürtels, aber noch nicht in der Oortschen Wolke bewegt und einen neuen Prototyp darstellt. Es ist rund 995 km groß und wurde nach der zentralen Meeresgöttin der Inuit Sedna benannt. Man fand 2012 VP und weitere Objekte mit ähnlichen Bahnelementen. Die Ausrichtung ihrer Apsidenlinien und ihre ähnliche Inklination führten Konstantin Batygin und Michael E. Brown zu dem Schluss, ein noch nicht entdeckter „Planet Neun“ erzwinge die gleichförmige Ausrichtung der Umlaufbahnen dieser DDO.
Die hellsten bekannten TNO (mit absoluter Helligkeit ≤ 4,0):





</doc>
<doc id="5284" url="https://de.wikipedia.org/wiki?curid=5284" title="Tonminerale">
Tonminerale

Tonminerale bezeichnet einerseits Minerale, die überwiegend feinstkörnig (Korngröße < 2 µm) vorkommen, andererseits jedoch die Schichtsilikate, die nach ihrer schichtartigen Kristallstruktur aus Silizium und Sauerstoff, sowie Wasserstoff und meist Magnesium und Aluminium benannt sind. Beide Definitionen sind nicht deckungsgleich. Manche überwiegend feinstkörnig vorkommende Minerale, etwa Goethit oder Gibbsit, sind keine Silikate. Andererseits gibt es Schichtsilikate, wie etwa Kaolinit, die oft größer als zwei Mikrometer sind. Tonminerale bezeichnen daher in der Regel solche Minerale, die beide Kriterien erfüllen.

Tonminerale entstehen durch Verwitterung von anderen Mineralen oder Gläsern oder bilden sich neu aus übersättigten Bodenlösungen oder hydrothermalen Wässern. Bei der Diagenese kommt es zu Ordnungsprozessen im Kristallgitter der Tonminerale, die als Maß für die Reife eines Sediments verwendet werden kann.

Tonminerale bestehen aus zwei charakteristischen Bauelementen:
Je nach Anordnung dieser Schichten unterscheidet man:
Durch die Substitution (v. a. von vierwertigem Si durch dreiwertiges Al in der Tetraederschicht oder von dreiwertigem Al durch zweiwertiges Mg in der Oktaederschicht) entsteht eine negative Schichtladung, die durch die Einlagerung von Kationen in der Zwischenschicht neutralisiert wird. Die Schichtladung der 1:1-Tonminerale ist stets Null. Die 2:1-Tonminerale werden nach ihrer Schichtladung x klassifiziert:
Tonminerale mit nicht ganzzahligen Schichtladungen besitzen die Fähigkeit zur Quellung, das heißt zur temporären und reversiblen Wasseraufnahme in ihren Zwischenschichten.

Alternativ kann die Schichtladung in der Oktaederschicht auch dadurch kompensiert werden, dass nur zwei von drei Oktaedern besetzt sind. Daher unterscheidet man:

Tonminerale sind sehr weich (Mohs-Härte 1) und reagieren plastisch auf mechanische Beanspruchung. Sie wandeln sich beim Erhitzen in härtere und festere Minerale um (Keramik). Tonminerale besitzen eine große spezifische Oberfläche, an die Stoffe adsorbiert und desorbiert werden können. Mit der großen Oberfläche ist eine hohe Kationenaustauschkapazität verbunden. Tonminerale haben eine geringe Wasserdurchlässigkeit. Suspensionen von Tonmineralen reagieren thixotrop auf mechanische Beanspruchung.

Die Art und der Anteil der Tonminerale in Böden bestimmt maßgeblich deren Fruchtbarkeit. 2:1-Tonminerale besitzen eine höhere Kationenaustauschkapazität als 1:1-Tonminerale und können daher mehr Nährstoffe wie Kalium- oder Ammoniumionen an Pflanzen abgeben, während sie die von den Wurzeln abgegebenen Hydroniumionen an deren Stelle in ihrer Zwischenschicht einlagern. Der kristallographische Ordnungsgrad des Tonminerals Illit wird von Mineralogen verwendet, um die Zeit zu bestimmen, die seit der Ablagerung eines Sediments vergangen ist. Er nimmt mit fortschreitender Diagenese zu.

Ton ist der wichtigste und älteste Rohstoff für die Herstellung von Keramik. Als Bestandteil von Lehm wird er für die Herstellung von Ziegeln benötigt. Daneben wird er zusammen mit Kalkstein zur Produktion von Zement verwendet. In der Bildenden Kunst dient er der Herstellung von Plastiken. Tonminerale werden als Ionenaustauscher, beispielsweise bei der Säuberung von Trinkwasser und zum Entfärben von Lösungen eingesetzt. Insbesondere Montmorillonit wird wegen seiner Wasseraufnahmefähigkeit genutzt, zum Beispiel im Katzenstreu. Kaolinit wird auch in der Papierindustrie als Appreturmittel verwendet, glättet die Oberfläche und nimmt Tinte auf. Blähton (stark porös gebrannter Ton) dient als isolierender Baustoff und für die Hydrokultur. Andere Tone dienen als Abdichtung in Deponien, sind Füll-, Trenn- und Zuschlagstoffe in Farben, Lebensmitteln und pharmazeutischen Produkten oder werden als Katalysatoren eingesetzt.

Zum Einsatz als Füllstoff, beispielsweise in Kunststoffen, werden Tone zuvor mit organischen Modifikatoren modifiziert, um sie organophil (d. h. hydrophob) zu machen. Dadurch verlieren sie zwar ihre gute Wasseraufnahmefähigkeit, lassen sich aber gut mit organischen Stoffen (z. B. Polymerschmelzen) mischen. Sie können dann als Nanofüllstoff eingesetzt werden.





</doc>
<doc id="5286" url="https://de.wikipedia.org/wiki?curid=5286" title="Transpluto">
Transpluto

Als Transpluto wurde zwischen 1930 und etwa 1985 ein hypothetischer zehnter Planet des Sonnensystems bezeichnet, den man außerhalb der Umlaufbahn des Pluto vermutete und jahrzehntelang suchte. Der Name lehnte sich an die um 1890 entstandene Arbeitsbezeichnung "Transneptun" an, die sich 1930 mit der Entdeckung Plutos zunächst erübrigte und heute der Name einer ganzen Gruppe sonnenferner Asteroiden und Zwergplaneten ist. Bis zur Herabstufung des vormals neunten Planeten Pluto zum Zwergplaneten am 24. August 2006 sprach man bei der Suche nach einem weiteren zehnten Planeten auch von Planet X, wobei das „X“ neben dem Symbol für das Unbekannte auch für die römische Zahl 10 stand.

Nachdem der Planet Neptun durch eine genaue Bahnanalyse des Uranus erfolgreich vorhergesagt wurde, postulierte Percival Lowell jenseits von Neptun einen neunten Planeten, den so genannten Transneptun, um Bahnabweichungen des Uranus und Neptun zu erklären. Er gründete 1894 das Lowell-Observatorium und suchte bis 1915 systematisch nach diesem Planeten. Der später damit beauftragte Clyde Tombaugh fand 1930 den Pluto beim intensiven Durchsuchen eines enger begrenzten Himmelsausschnittes eher durch Zufall. Trotz seiner geringen Größe (sie wurde bis etwa 1950 überschätzt) galt Pluto 76 Jahre lang als der neunte Planet.

Bald stellte sich heraus, dass Pluto zu klein ist, um die Bahn des Neptun merklich zu stören und dessen Bahnabweichungen zu erklären. So begann am Lowell-Observatorium schon bald nach 1930 statt der Suche des Transneptun jene nach dem noch ferneren Transpluto. Ab 1957 führte Henry Lee Giclas eine systematische fotografische Durchmusterung des Himmels durch, die 18 Jahre in Anspruch nahm. Die der Suche zugrunde liegenden Bahnstörungen des Neptun waren lange Zeit nicht nachvollziehbar – man hielt sie für die Auswirkungen kleiner, unvermeidlicher Messfehler.

Die Terminologie der sonnenfernen Objekte erwies sich zwischen 1980 und 1992 als erweiterungsbedürftig, als man an der Position des schon länger postulierten Kuipergürtels weitere transneptunische Objekte wie die Plutinos entdeckte.
Im März 2004 wurde das Thema „Zehnter Planet“ für die Medien wieder aktuell, als man die etwa 1000 km große Sedna in doppelter Pluto-Entfernung entdeckte, in der Fachwelt aber nicht als Planet einstufte. Schließlich wurde am 29. Juli 2005 die Entdeckung von Eris (zunächst als 2003 UB) bekanntgegeben. Dieser ungefähr 2300 km große Körper wurde sowohl von der NASA als auch der Presse zur Zeit der Entdeckung als „zehnter Planet“ eingestuft. Eris erfüllte nach Bahn und Größe den Status eines „Transpluto“ und löste damit die „Planetendiskussion“ der Internationalen Astronomischen Union (IAU) aus. Nach der 2006 beschlossenen Definition von „Planet“ sind aber weder Pluto noch Eris Planeten, sondern gehören in die neu geschaffene Kategorie „Zwergplanet“.

Wenn es neben den (nach der Definition von 2006) acht Planeten einen weiteren geben sollte, müsste dieser per Definition „die Umgebung seiner Bahn bereinigt haben“. In den Außenbereichen des Sonnensystems wurden mittlerweile einige weitere größere Körper entdeckt. Die meisten davon sind den Asteroiden oder Kometen zuzuordnen. Eris und Pluto sind weiterhin die einzigen mit einem Durchmesser über 2000 km. Daher gilt heute die Existenz eines weiteren „wirklichen“ Planeten nach der neuen Planetendefinition als unwahrscheinlich.

Trotzdem wird immer wieder über die Existenz eines solchen Planeten spekuliert. Ein Grund dafür ist der scheinbar unbegrenzte Vorrat an Kometen aus den Zonen des Kuipergürtels und der Oortschen Wolke. Ein weiter außen kreisender Planet könnte mit seiner Schwerkraft dafür sorgen, dass Materie aus der Oortschen Wolke in das innere Sonnensystem gelangt.

Nach heutigem Wissen kann es durchaus einen weiteren Planeten geben. Dieser dürfte allerdings keine merklichen Bahnstörungen auf die bekannten Planeten und Raumsonden im äußeren Sonnensystem verursachen und allen bisherigen Himmelsdurchmusterungen für sich bewegende Objekte entgangen sein. Insbesondere die Ergebnisse des Weltraumteleskops WISE schränken die Größe dieses Planeten stark ein und verlagern seine hypothetische Bahn auf mehrere hundert bis tausend AE. Auch müsste seine Bahn stark elliptisch sein oder weit außerhalb der Ekliptik liegen.

Ein Brauner Zwerg oder ein schwacher Stern („Nemesis“), der unsere Sonne zum Mitglied eines Doppelsternsystems machen würde, kann zwar bis heute (Stand Januar 2016) nicht definitiv widerlegt werden, ist aber nahezu unwahrscheinlich, da er auf Grund seiner Eigenstrahlung durch die bisherigen Himmelsdurchmusterungen sehr wahrscheinlich gefunden worden wäre.

1999 berechneten John B. Murray von der Open University in Großbritannien und John Matese von der University of Louisiana at Lafayette unabhängig voneinander die Bahnen langperiodischer Kometen. Dabei kamen sie zu dem Schluss, dass ein möglicher bisher unbekannter Himmelskörper regelmäßige Bahnstörungen bei den Körpern der Oortschen Wolke verursachen würde und diese dadurch auf Kometenbahnen ins innere Sonnensystem gelenkt würden. Murray postulierte als Verursacher einen großen „Planeten X“ (Tyche) mit einer Masse zwischen 1 und 10 Jupitermassen in einer Entfernung von 30.000 bis 50.000 AE von der Sonne (etwa / bis / der Entfernung zum nächsten Stern). Die Bahn eines solchen Körpers dürfte gemessen am Alter des Sonnensystems nicht stabil sein, da ein Umlauf um die Sonne Millionen Jahre betragen würde. Nach den bisherigen Modellen kann sich allerdings bei der Entstehung des Sonnensystems in dieser Entfernung kein Planet gebildet haben, weshalb Murray vermutet, dass er entweder ein einstmals aus dem interstellaren Raum in unser Planetensystem eingedrungener Planemo ist oder aber durch Migration nach außen wanderte.

Bisher lässt sich Murrays These weder bestätigen noch ausschließen. Jedoch konnte das Infrarot-Weltraumteleskop WISE keinen Planeten von Jupitergröße innerhalb von 26.000 AE Sonnenabstand nachweisen.

Die erste Hypothese über einen möglichen Planet am äußersten Rand des Sonnensystems stammt von Daniel Whitmire. Im Jahr 1985 nannte er ihn noch „Planet X“.
Ergebnisse aus dem Vergleich der Bahndaten des Zwergplaneten Sedna und dem Ende 2012 entdeckten Asteroiden 2012 VP lassen sich gemäß einer Anfang 2014 veröffentlichten Untersuchung einiger Forscher damit erklären, dass es einen unentdeckten Planeten mit mindestens der zehnfachen Erdmasse außerhalb des Kuiper-Gürtels in einer Entfernung von „einigen hundert AE“ geben könnte. Es werden aber auch andere Erklärungen solcher Bahndaten diskutiert. Eine kurz darauf (März 2014) veröffentlichte Arbeit verneint auf Grund der Messung von Bahnstörungen bekannter Objekte des Sonnensystems im Rahmen der heute erzielbaren Messgenauigkeit die Existenz eines Planeten mit doppelter Erdmasse innerhalb von etwa 500 AE Sonnenabstand, und für einen Planeten mit 15-facher Erdmasse innerhalb von etwa 1000 AE. 2016 postulierten die US-Astronomen Batygin und Brown (California Institute of Technology) die Existenz eines Planeten mit mindestens etwa der 10-fachen Erdmasse und einer Umlaufzeit von bis zu 20.000 Jahren anhand mathematischer Berechnungen.





</doc>
<doc id="5288" url="https://de.wikipedia.org/wiki?curid=5288" title="Titan (Mythologie)">
Titan (Mythologie)

Die Titanen (" Titánes", Einzahl: Titan, ) sind in der griechischen Mythologie Riesen in Menschengestalt und ein mächtiges Göttergeschlecht, das in der legendären Goldenen Ära herrschte. Wie die Kyklopen und Hekatoncheiren sind sie Nachkommen der Gaia und des Uranos. Die weiblichen Titanen werden auch Titanide () genannt. Vom Titanenkampf zwischen Olympiern, Hekatoncheiren und einer Reihe von Titanen berichtete das verlorene Epos Titanomachie, in welchem sie nach hartem Kampf schließlich besiegt und in die Tiefen der Unterwelt, den Tartaros, getrieben werden.

Das griechische Wort „τιταίνω/titainō“ bedeutet übersetzt „sich recken“.

Die Titanen bilden das älteste Göttergeschlecht der Theogonie der griechischen Mythologie. So nennt Hesiod zwölf Titanen, sechs Söhne und sechs Töchter, die später sechs Paare bilden:

Titan Iapetos zeugt mit Klymene weitere Titanen:

Titan Kreios zeugte mit Eurybia ebenfalls weitere Titanen:

Von den Titanen binden sich im übrigen vier Schwestern mit vier Brüdern:

Die ersten Kinder, die Gaia dem Uranos gebar, die Zyklopen und Hekatoncheiren (Hundertarmigen), waren diesem von Anfang an verhasst, so dass er sie in den Tartaros verbannte und Freude an diesem schlimmen Werk empfand. Da Gaia darunter litt und erbost war, versteckte sie die folgenden Kinder, brachte den grauen Stahl hervor, machte daraus eine Sichel und überreichte ihnen diese, damit sie ihn bestraften. Sie stachelte die Titanensöhne an zu rebellieren, angeführt von Kronos, dem jüngsten der Titanen, legten sie zu fünft einen Hinterhalt für ihren Vater. Sie packten ihn, als er auf der Erde lag, um ihn an den vier Ecken der Welt festzuhalten, während Kronos in der Mitte den Titanenvater Uranos entmannte.

Das Geschlechtsteil des Uranos warf Kronos ins Meer. Das auf Gaia fallende Blut aus Uranos’ Penis befruchtete sie; Gaia gebar daraufhin die Giganten, die Erinnyen und die melischen Nymphen. Aus dem weißen Schaum, der durch Mischung von Uranos’ Blut und Samen entstand, erwuchs Aphrodite („die Schaumgeborene“). Nach anderen Autoren war Okeanos der einzige der Titanen, der nicht an der Kastration beteiligt war und auch im Titankampf neutral blieb.

Hesiod erzählt, dass nach dem Sieg des Kronos über Uranos zwischen den Titanen und den späteren Olympischen Göttern ein heftiger Kampf entbrannte, der erst entschieden werden konnte, als Gaia letzteren verriet, wie der Sieg zu erringen sei: Die von Kronos gefangengehaltenen Hekatoncheiren („Hundertarmigen“) sollten zur Hilfe herbeigeholt werden, um die Götter im Kampf zu unterstützen. Als der Kampf zwischen Göttern und Titanen neu entbrannte, bewarfen die Hundertarmigen die Titanen mit Steinen, so dass sie unter diesen begraben wurden. Schließlich auch noch gefesselt, wurden sie in den Tartaros gestoßen, von wo sie niemals mehr entkommen können, da Poseidon eine eherne Tür baute und die Hundertarmigen diese nun bewachen.

Okeanos und alle Titaniden sollen an diesem Kampf nach anderen Angaben nicht oder nur passiv beteiligt gewesen sein. So soll ein Teil jüngerer Titanen, unter anderem Helios, auf Seite des Zeus gestanden haben.





</doc>
<doc id="5290" url="https://de.wikipedia.org/wiki?curid=5290" title="Trigonometrische Funktion">
Trigonometrische Funktion

Mit trigonometrischen Funktionen oder auch Winkelfunktionen (seltener: Kreisfunktionen oder goniometrische Funktionen) bezeichnet man rechnerische Zusammenhänge zwischen Winkel und Seitenverhältnissen (ursprünglich in rechtwinkligen Dreiecken). Tabellen mit Verhältniswerten für bestimmte Winkel ermöglichen Berechnungen bei Vermessungsaufgaben, die Winkel und Seitenlängen in Dreiecken nutzen. Die trigonometrischen Funktionen sind außerdem die grundlegenden Funktionen zur Beschreibung periodischer Vorgänge in den Naturwissenschaften.

Die elementaren trigonometrischen Funktionen sind:
sowie deren Kehrwerte:

Zwischen diesen Funktionen bestehen enge Zusammenhänge. Genau genommen würde bereits eine der Funktionen ausreichen, um beliebige trigonometrische Probleme lösen zu können. Die Verwendung mehrerer verschiedener Funktionen ermöglicht jedoch eine Vereinfachung der Rechnungen und Formeln.

Die Kotangensfunktion wird in Tabellen mit Funktionswerten von trigonometrischen Funktionen gerne genutzt, da man cot("x") zusammen mit der Tangensfunktion tabellieren kann. Insofern ist die Bedeutung von cot("x") etwas größer als die von sec("x") und csc("x").

Es gibt weitere – heute eher unübliche – Funktionen, wie z. B. "sinus versus" ("versin"), "cosinus versus" ("coversin"), "exsecant" ("exsec") und "excosecant" ("excsc").

Ursprünglich sind die Winkelfunktionen als Seitenverhältnisse in rechtwinkligen Dreiecken und daher nur für Winkel von 0 bis 90 Grad definiert:
Diese Definition ist unabhängig von der Wahl des rechtwinkligen Dreiecks, das zur Berechnung verwendet wird. In jedem rechtwinkligen Dreieck mit gleichem Winkel formula_2 ergeben diese Verhältnisse den gleichen Wert. Dies lässt sich z. B. mit den Strahlensätzen beweisen.

Aus diesen Beziehungen folgt unmittelbar die Beziehung:
Die Ankathete des Winkels ist gleichzeitig die Gegenkathete des anderen spitzen Winkels formula_4 des rechtwinkligen Dreiecks; da die Winkelsumme im Dreieck 180° beträgt, und der rechte Winkel 90° zu dieser Summe beiträgt, ist dieser Winkel formula_5 und daher

Die Winkelfunktionen können aber als Sekanten- und Tangentenabschnitte am Einheitskreis auch auf größere Winkel erweitert werden. Vom Schnittpunkt des einen Winkelschenkels mit dem Einheitskreis werden die Lote auf die beiden Koordinatenachsen gefällt und liefern Sinus und Kosinus des Winkels. Die Tangenten in den Punkten "x" = 1 bzw. "y" = 1 schneiden den Schenkel ebenfalls und liefern dann in der Projektion auf die Achsen den Tangens und den Kotangens. Dabei muss der Schenkel gegebenenfalls rückwärts verlängert werden, um einen Schnittpunkt zu erzielen. Auf diese Weise können jedem Winkel von 0 bis 360 Grad Werte der Winkelfunktionen zugeordnet werden, die nun freilich auch negativ werden können (siehe Abbildung). Die oben angegebenen Beziehungen gelten dabei weiterhin.

In der Analysis werden Sinus und Kosinus in der Regel über Potenzreihen definiert, wobei der Winkel im Bogenmaß angegeben wird. Näheres siehe in den Artikeln Sinus und Kosinus sowie Tangens.

Die Vorzeichen der trigonometrischen Funktionen in Abhängigkeit vom Quadranten gibt die folgende Tabelle an:
Der Betrag wird wie folgt umgerechnet:
Hauptsächlich werden die trigonometrischen Funktionen im Vermessungswesen genutzt.
Formeln zur Berechnung von Größen am Dreieck → Dreiecksgeometrie.

Weiterhin sind sie in der Analysis und bei vielen Anwendungen der Physik und der Technik wichtig. Es besteht eine enge Beziehung zur Exponentialfunktion, die besonders bei Funktionen komplexer Zahlen und in der Taylorreihe der Funktionen sichtbar wird.

In manchen Situationen werden die trigonometrischen Winkelfunktionen benötigt, um aus Seitenverhältnissen Winkel zu berechnen. Dazu werden die Arkusfunktionen oder inverse Winkelfunktionen arcsin, arccos, arctan und arccot – die Umkehrfunktionen zu den trigonometrischen Funktionen – verwendet. Auf Taschenrechnern sind sie häufig mit sin usw. bezeichnet. Das stimmt mit der Schreibweise formula_7 für die Umkehrfunktion von "f" überein (auch wenn die Arkusfunktionen das genau genommen nicht sind), kollidiert allerdings mit der ebenso üblichen Konvention, formula_8 für formula_9 zu schreiben.

Die Arkusfunktionen werden verwendet, um zu einem Seitenverhältnis den Winkel zu berechnen. Wegen der Symmetrie der trigonometrischen Funktionen ist von Fall zu Fall zu klären, in welchem Quadrant der gesuchte Winkel liegt.




</doc>
<doc id="5292" url="https://de.wikipedia.org/wiki?curid=5292" title="Tafelwasser">
Tafelwasser

Tafelwasser ist ein „künstlich“ hergestelltes Erfrischungsgetränk. In der Regel besteht Tafelwasser aus Trinkwasser, das mit weiteren Zutaten angereichert wurde.

Es ist kein natürliches Mineralwasser und darf verschiedene Wasserarten enthalten. Tafelwasser kann überall zusammengemischt und abgefüllt werden, wobei zwar qualitätshygienische Anforderungen einzuhalten sind, aber keine amtliche Anerkennung erforderlich ist. Eine Etikettierung als natürliches Wasser ist verboten und die Angabe einer Ursprungsquelle oder eines Brunnens nicht zulässig. Somit ist es ein Getränk, das zwar Trinkwasser enthält, aber die Anforderungen an natürliches Mineralwasser nicht erfüllt.

Für industriell hergestellte Tafelwässer werden oftmals dem örtlich zur Verfügung stehenden Wasser weitgehend alle Inhaltsstoffe entzogen (Demineralisierung); anschließend wird wieder eine definierte Menge Mineralstoffe zugegeben, damit der Geschmack nicht vom Abfüllort abhängig ist. Diese Stoffe dürfen nach Maßgabe der Zusatzstoff-Zulassungsverordnung und im Rahmen des Lebensmittelrechts beigemischt werden. So werden z. B. Kohlenstoffdioxid, Speisesalz und andere Mineralsalze zugefügt, wobei stets die Vorgaben der Trinkwasserverordnung zu beachten sind. Die so hergestellten Tafelwässer sind die Grundlage der meisten coffeinhaltigen Erfrischungsgetränke.

„Sodawasser“ ist ein Tafel- oder Mineralwasser mit mindestens 570 mg Natriumhydrogencarbonat pro Liter sowie Kohlendioxid.



</doc>
<doc id="5293" url="https://de.wikipedia.org/wiki?curid=5293" title="Trauer">
Trauer

Der Begriff Trauer bezeichnet die durch ein betrübendes Ereignis verursachte Gemütsstimmung und deren Kundgebung nach außen, etwa durch den Verlust nahestehender oder verehrter Personen oder Tiere, durch die Erinnerung an solche Verluste, oder auch zu erwartende Verluste.

Trauer bezeichnet

Im weiteren Sinn unterscheidet man auch folgende Arten der Trauer: übertriebene, chronische, maskierte und verspätete Trauer (englisch: complicated grief, traumatic grief).

Der "Trauerprozess" selbst besitzt verschiedene Aspekte. (Siehe auch Traurigkeit, Schmerz.) Die Folgen sind unter anderem Bedrückung und Depression.
Der Zustand der Trauer hat beim Menschen neben dem emotionalen Aspekt auch einen Verhaltensaspekt, bei dem es um die Bewältigung und Verarbeitung des seelischen Schmerzes geht, dieser wird durch den Verlust eines nahestehenden Menschen oder Tieres verursacht. Auslöser können der Tod von Freunden, Verwandten oder Haustieren, aber auch Trennungen anderer Art (Haus, Heimat) sein, die den „räumlichen Verlust“ nahestehender oder verehrter Personen bedeuten.

Körperliche Aktivität oder Ablenkung können Trauer verdrängen oder kurzfristig erleichtern. Man kann überdies versuchen, den Verlust zu ersetzen. Trauergebräuche oder -rituale und Trauerfeiern gelten als Teil von Kultur und Kulturgeschichte; einige werden seit Jahrhunderten praktiziert. Solche Möglichkeiten werden durch Erinnerung und symbolisch wiederholtes Zurückholen und erneutes Weggeben des Betrauerten, ein Sich-Einlassen auf die Extremsituation des Verlustes gespielt und können ein allmähliches Akzeptieren und Loslösen erleichtern. Diese sogenannte Trauerarbeit kann helfen. Die Klage und auch Gespräche sind konstitutiv. Bedeutsam können auch der Ort der Trauer und/oder die zugehörige Situation sein.

Die Trauer verläuft gewöhnlich in mehreren Phasen.


Während oder nach der Bearbeitung der Trauer können sich neue Perspektiven eröffnen, die unabhängig vom Trauerfall sind: neue Beziehungen, Verhaltensänderungen. So kann bearbeitete Trauer auch Lernprozesse in Gang setzen oder begünstigen. Wenn die Trauerarbeit und/oder schwere zusätzliche Belastungen noch zu viele Energien beanspruchen, kann dies die Lernprozesse hemmen.

Neuerdings gibt es Trauerreisen. Die Teilnehmer einer solchen Reise(gruppe) (meist unter 20) kommen während der Trauerreise ins Gespräch; dies soll ihre Trauerarbeit fördern und erleichtern.

Elisabeth Kübler-Ross beschrieb 1969 fünf Phasen des Sterbens in einem Modell aus schrittweisem Verhalten, Reflexion, Auflehnung und Akzeptanz der Situation, Ängsten und Wünschen des Sterbenden. Das Phasenmodell bezieht die soziale Umwelt ein und lässt sich im Trauerprozess auch für die Begleitung und Erklärung von Trauer von Angehörigen und Freunden verwenden. 

1970 legten John Bowlby und Collin Murray Parkes ein vierphasiges Modell vor, das 1982 von Verena Kast mit dem Modell von Kübler-Ross verschmolzen und – unter Einbezug von Elementen der analytischen Psychologie – zu einem ebenfalls vierphasigen Modell verarbeitet wurde. 1972 hatte Yorick Spiegel bereits ein psychoanalytisch orientiertes Modell der Trauerphasen vorgelegt. 

J. William Worden legte 1982 ein Modell vor, das aus vier Aufgaben der Trauerarbeit bestand und nicht als Phasenmodell zu verstehen ist. Dieses entwickelte er 1991 und 1996 weiter und ergänzte es um eine fünfte Aufgabe.

Diese Einteilung erfolgt nach Verena Kast und basiert auf Empfehlungen von John Bowlby und Collin Murray Parkes. Diese Theorien rund um den "Trauerprozess" von Verena Kast lehnen sich stark an das Modell der Sterbephasen von Kübler-Ross an und unterscheiden vier Phasen, die meist sukzessive und natürlich nicht streng voneinander getrennt ablaufen.





Der systematische Theologe Yorick Spiegel beschrieb in seiner Habilitationsschrift von 1972 ebenfalls vier Trauerphasen; sie unterscheiden sich jedoch von den Phasen, wie sie Kast beschreibt.





Der Trauerprozess ist kein passiver Vorgang, bei dem etwas mit einem geschieht; vielmehr muss der Trauernde aktiv werden und eine Reihe von Aufgaben lösen. Diese „Arbeit“ gewährleistet erst einen „normalen“ Trauerprozess; wird die Trauerarbeit nicht geleistet, ist der Abschluss des Trauerprozesses nicht mehr möglich. Pathologische Trauerverarbeitung ist die Folge.

Yorick Spiegel nennt folgende Aufgaben, die der Trauernde zu lösen hat:

Es lassen sich übrigens keine eindeutigen Aussagen darüber machen, zu welchem Zeitpunkt welche Aufgabe vom Trauernden in Angriff genommen werden soll. Teilweise überschneiden sich die Bereiche und müssen gleichzeitig angegangen werden; – aber der Trauernde kann ebenso eine ganze Zeit lang auf die Lösung nur "einer" bestimmten Aufgabe fixiert sein.

Des Weiteren ist der Trauerprozess individuell, also bei jedem Menschen anders. Manchmal werden die genannten Phasen nicht oder nur kaum merklich durchlaufen. Die Phasenmodelle sind somit nicht als statische Gegebenheiten anzusehen, sondern als Stütze für die Betroffenen, ihren persönlichen Trauerprozess zu durchlaufen.

Mit der Säkularisierung der Sepulkralkultur wandeln sich im deutschsprachigen Raum seit den 1970er Jahren auch die Trauersitten und die Arten der Bestattung. Trauerkleidung, die traditionell oft während der gesamten Trauerzeit getragen wurde, ist heute meist nur noch am Tage der Bestattung gebräuchlich. Aufwändige Zeremonien und selbst Grabsteine, die an den Verstorbenen erinnern, können entfallen, an die Stelle eines Priesters kann bei Trauerfeiern anlässlich der Bestattung heute ein professioneller und nicht konfessionell gebundener Trauerredner treten.

Neu entstandene Trauerrituale sind das Pflanzen eines Erinnerungsbaumes oder das Aufstellen von Holzkreuzen, Blumensträußen oder Kerzen am Straßenrand nach einem tödlichen Unfall oder einer Gewalttat. Die Gemeinschaft der Angehörigen von AIDS-Kranken hat seit den 1980er Jahren durch gemeinschaftlich handgenähte Quilts (AIDS Memorial Quilt) an ihre Verstorbenen erinnert.

Üblich ist für Trauernde auch heute das Bekanntmachen des Todesfalles in Form einer Familienanzeige oder durch gedruckte Trauerbriefe, in Österreich „Parten“ genannt. Die Bandbreite moderner Formen der Kondolenz reicht von mündlichen Beileidsbekundungen, Beileidskarten und Kondolenzbucheinträgen über Blumen- und Kranzspenden (die auf Wunsch der Hinterbliebenen oft durch Geldspenden an gemeinnützige Einrichtungen ersetzt werden) bis hin zu Kondolenzeinträgen auf virtuellen Friedhöfen. Für empfangene Beileidsbekundungen bedanken die Hinterbliebenen sich in einer weiteren Zeitungsanzeige oder gedruckten Karten oder Briefen.

Trauerbegleitung wird heute vielfach nicht nur von Angehörigen, Freunden, Bekannten und Seelsorgern geleistet, sondern auch von Psychotherapeuten, Selbsthilfegruppen, Trauernetzwerken und von Bestattern, die Mitarbeiter in speziellen Seminaren schulen lassen.


Aus der Sicht des christlichen Glaubens ist Trauer eine menschliche Befindlichkeit und kann durch Gebet eine andere Dimension einschließen. Solidarität mit den Trauernden wird zu einer besonderen Form der Nächstenliebe – und jeder kann dazu beitragen, indem er einer trauernden Person nicht aus Scheu den Kontakt oder ein Gespräch verweigert. Trauernde zu trösten, Tote zu begraben und für Lebende und Tote zu beten sind Werke der Barmherzigkeit.

Zur christlichen Beerdigung gehört die Bitte, dass alle Anwesenden für ihren eigenen Tod bereit sind. Damit wird über das Abschiednehmen vom konkreten Toten der Blick darauf geweitet, dass jeder Mensch sterblich und Abschied/Tod ein durchgängiges Motiv im Leben ist.

Christen verstehen den Tod als Übergang zum ewigen Leben bei Gott.

Mancherorts sind im Ablauf der Zeit der Trauer einzelne verschiedene besondere Gottesdienstformen üblich:

Im späteren Zeitbalauf:

Regelmäßig wiederkehrende Feiern:
Für die Trauernden wird so ein Ritual angeboten einen bestimmten Zeitschnitt zu markieren und der Verstorbenen in gottesdienstlicher Form zu gedenken.

2007 richtete das Bistum Limburg in der Kirche St. Michael in Frankfurt-Nordend erstmals ein spezielles Trauerzentrum ein.

Im Hinduismus wird der Tod nicht als Ende, sondern als Übergang in einen anderen Daseinszustand begriffen. Auf die Bestattung folgt eine etwa 13-tägige Trauerzeit. Die Familie des Verstorbenen gilt in dieser Zeit als unrein. Neben rituellen Bädern ist es üblich, dass männliche Angehörige (besonders Söhne) sich am zehnten Tag den Kopf rasieren lassen. Die Angehörigen dürfen in der Trauerzeit ihren Gefühlen freien Lauf lassen, aber nicht an religiösen Zeremonien teilnehmen. Auch gewisse Lebensmittel (Süßigkeiten) sind ihnen nicht erlaubt. Durch diese Maßnahmen soll sichergestellt werden, dass die Seele des Verstorbenen ungehindert zu ihrer neuen Form aufsteigen kann. Gebräuchlich ist auch das – tatsächliche oder symbolische – Spenden einer Kuh. Die traditionelle Farbe der Trauer ist Weiß. Personen, die dem Verstorbenen oder den Angehörigen nahestehen, kondolieren persönlich und bringen Blumen, die dem Toten, falls er im Hause aufgebahrt ist, zu Füßen gelegt werden; entferntere Bekannte schicken Blumen und eventuell eine Karte. Am ersten Jahrestag nach dem Tod wird die "Shraddha"-Zeremonie durchgeführt, bei der die Trauernden der Seele ("pitṛ") des Verstorbenen eine Opferspeise ("pinda daan") darbringen.

Auch in China gilt der Tod traditionell nicht als Ende, sondern als Übergang der Seele in ein ewiges Dasein. Der Konfuzianismus, der das chinesische Denken stärker geprägt hat als jede andere Philosophie, konzipiert diese ewige Dasein allerdings nicht so sehr als Unsterblichkeit des Individuums, sondern vielmehr als Kontinuität der Familie. Die Familie ist in China alles, und das Individuum ohne Familie nichts. Trauersitten sind in China Ausdruck der kindlichen Pietät und der Verpflichtung der Jungen gegenüber den Vorausgegangenen, denen sie alles verdanken, was sie haben und was sie sind.

Der älteste Sohn der Familie des Verstorbenen hat, weil er den Fortbestand der Familienlinie garantiert, auch bei den Trauersitten eine Schlüsselfunktion. Er geht von Haus zu Haus und überliefert niederkniend den Nachbarn und Angehörigen die Todesnachricht. Es ist sehr üblich und wird erwartet, beim Erhalt einer Todesnachricht und während der Trauer laut zu weinen und zu wehklagen, selbst wenn man nicht wirklich traurig ist. Dies gilt besonders für den ältesten Sohn. Unüblich ist es hingegen, über seine Trauer zu "sprechen"; viel über Gefühle zu sprechen, gilt in China traditionell als ungesund.

Der Sarg mit dem Verstorbenen wurde traditionell im Haus oder Innenhof der Angehörigen zu einer mindestens einwöchigen Totenwache aufgebahrt; die Angehörigen durften in dieser Zeit weder Schmuck noch rote Kleidung tragen. Kinder und Schwiegertöchter – von denen erwartet wurde, dass ihre Trauer am größten sei – trugen Schwarz und eine Haube aus Sacktuch, Enkel trugen Blau und Urenkel Hellblau. Schwiegersöhne wurden als Außenstehende angesehen und durften hellere Farben tragen. Blutsverwandte und Schwiegertöchter hatten während der gesamten Totenwache laut zu weinen und zu wehklagen.

Nach der Bestattung begann traditionell eine 100 Tage dauernde Trauerzeit. Um ihre Trauer anzuzeigen, trugen die Angehörigen in dieser Zeit ein Stück farbigen Stoff am Ärmel: die Kinder Schwarz, die Enkel Blau und die Urenkel Grün. In sehr traditionsbewussten Familien wurden die farbigen Stoffstücke bis zu drei Jahre lang getragen. In den ersten 49 Tagen durften die Nachkommen des Verstorbenen nicht ihr Haar schneiden. Besondere Regeln galten auch für den ältesten Sohn der Familie, der in den ersten sechs Monaten nach der Bestattung weder die Farbe Rot tragen noch heiraten durfte. Verlangt war eine Trauerzeit nur beim Tode von Familienmitglieder aufsteigender Linie; wenn ein Kind oder eine Ehefrau verstarb, konnte die Trauerzeit – weil es hier nicht um Gefühle, sondern um Pflichterfüllung gegenüber den vorausgegangenen Generationen ging – ganz entfallen.

Eine weitere chinesische Besonderheit ist das alljährliche Qingming-Fest, ein Totengedenkfest, bei dem den bestatteten Verstorbenen Blumen und Geschenke gebracht werden.








</doc>
<doc id="5294" url="https://de.wikipedia.org/wiki?curid=5294" title="Thesaurus">
Thesaurus

Ein Thesaurus ( "thesaurós" ,Schatz, Schatzhaus‘; lateinisch dann "thesaurus", daher auch "Tresor") bzw. Wortnetz ist in der Dokumentationswissenschaft ein kontrolliertes Vokabular, dessen Begriffe durch Relationen miteinander verbunden sind. Die Bezeichnung wird gelegentlich auch für linguistische Thesauri oder wissenschaftliche Wortschatzsammlungen einer Sprache verwendet.

Als Thesaurus bezeichnet man ein Modell, das versucht, ein Themengebiet genau zu beschreiben und zu repräsentieren. Es besteht aus einer systematisch geordneten Sammlung von Begriffen, die in thematischer Beziehung zueinander stehen. Der Thesaurus ist ein kontrolliertes Vokabular, auch Attributwertebereich genannt, für das jeweils zu beschreibende Attribut. Es werden in erster Linie Synonyme, aber auch Ober- und Unterbegriffe verwaltet. Oft werden jedoch keine Antonyme (gegenteilige Begriffe) aufgeführt.

Beispiel: Bildnis (Synonym: Abbild, Bild, Spiegelbild); Tischler (Oberbegriff: Handwerker)

Im allgemeinen Wortsinn bezeichnete es zunächst einen „Wissensspeicher“ wie beispielsweise ein Wörterbuch oder eine Enzyklopädie. 1572 erschien der fünfbändige "Thesaurus Graecae Linguae" von Henricus Stephanus (Henri Estienne), das zu seiner Zeit umfassendste Wörterbuch, erwähnt auch in den Tagebüchern des Samuel Pepys (Dezember 1661). Der vor allem im englischen Sprachraum einflussreiche, 1852 von Peter Mark Roget veröffentlichte "Roget’s Thesaurus of English Words and Phrases" rückte die Bedeutung des Begriffs in die Richtung eines linguistischen Thesaurus.

Im Bereich des Information-Retrieval wurde der Begriff erstmals 1957 von Hans Peter Luhn verwendet, als in den 1950er-Jahren verschiedene Systeme zur Indexierung entwickelt wurden. Zu den ersten Thesauri, die in der Praxis zur Erschließung eingesetzt wurden, gehören das System von Du Punt (1959) und der "Thesaurus of ASTIA Descriptors" (1960). Ein einheitliches Format für Thesauri wurde 1967 mit dem "Thesaurus of Engineering and Scientific Terms" (TEST) vorgelegt. Aus den bereits von Anfang an entwickelten Regeln für den Aufbau von Thesauri entwickelten sich mit der Zeit allgemeine Standards, die die Form des klassischen Thesaurus zur Dokumentation festlegen. Dazu gehören die von Derek Austin und Dale entworfenen "UNESCO's Guidelines for the Establishment and Development of Monolingual Thesauri", deren Inhalte in den ISO-Standard 2788 (1986) einflossen.

In der Dokumentationswissenschaft hat sich der Thesaurus als geeignetes Hilfsmittel zur Sacherschließung und zum Auffinden von Dokumenten erwiesen. Dabei dienen Relationen zwischen den einzelnen Begriffen zum Auffinden bei der Indexierung (Vergabe von Schlagworten) und bei der Recherche. Im Gegensatz zu einem linguistischen Thesaurus enthält ein Thesaurus zur Dokumentation ein "kontrolliertes Vokabular," d. h. eindeutige Benennungen (Deskriptoren) für jeden Begriff. Unterschiedliche Schreibweisen "(Photo/Foto)," Synonyme bzw. als gleichbedeutend behandelte Quasi-Synonyme, Abkürzungen, Übersetzungen etc. werden durch Äquivalenzrelationen miteinander in Beziehung gesetzt. Begriffe werden außerdem durch Assoziationsrelationen und hierarchische Relationen vernetzt.

Der Thesaurus dient als Dokumentationssprache zum Indexieren, Speichern und Finden von Dokumenten. Die Relationen ermöglichen es, bei der Indexierung und Recherche passende Benennungen für gesuchte Begriffe zu finden. Bei der Suche können Thesauri durch die automatische Erweiterung der Suchanfrage auf Synonyme und Unterbegriffe hilfreich sein.

Ein Thesaurus kann damit auch allgemein zur Begriffsklärung dienen und hat im besten Fall die Funktion einer Normdatei inne.
Im Gegensatz zu einer monohierarchischen Tabelle oder Datenbank kann der Thesaurus eine polyhierarchische Struktur besitzen (d. h. ein Unterbegriff kann mehrere Oberbegriffe haben).

Die Thesaurusnormen DIN 1463-1 bzw. das internationale Äquivalent ISO 2788 sehen folgende Relationsarten und dazugehörige Abkürzungen vor:
Die häufigsten Relationen in einem Thesaurus sind Äquivalenz-, Assoziations- und hierarchische Relationen.

In der Regel wird ein Element einer Äquivalenzrelation, also eine Benennung, als Vorzugsbenennung festgelegt. Die Nicht-Vorzugsbenennungen erhalten einen Verweis auf die ihnen äquivalente Vorzugsbenennung.

Früher verstand man unter einem Thesaurus ein wissenschaftliches Sammelwerk mit dem gesamten Wortschatz einer Sprache. Bekannt sind unter anderem der "Thesaurus Linguae Graecae" und der "Thesaurus Linguae Latinae". Bei diesen Werken handelt es sich genaugenommen um Wörterbücher.

Die ersten in der Elektronischen Textverarbeitung (EDV) verwendeten Thesauri waren ebenfalls einfache Wörterbücher, welche eingegebene Vokabeln mit enthaltenen Einträgen abgleichen und dem Anwender Rückmeldungen geben konnten. Die Rückmeldungen ließen sich zunächst nur für die Erkennung von einfachen Rechtschreibfehlern verwenden und konnten mit Suchläufen, später aber im Hintergrund ermittelt werden, was dem heutigen Standard entspricht. Ursprünglich entstanden die dazu benötigten Datenbanken aus manuell in Datenformat konvertierten Wort-Sammlungen, die für kommerzielle Programme zunächst fortlaufend durch den Hersteller ergänzt und mit Aktualisierungen an den Kunden ausgeliefert wurden. Mit dem Aufkommen von individuell durch den Benutzer ergänzbaren Worteinträgen entstand die Möglichkeit, große, quasi-kollaborative nutzerbasierte Plattformen zur Sammlung neuer Einträge zu verwenden, wobei die auf einem Server liegende Datenbank durch Rücksendung der individuellen Arbeitskopien von Thesauri verschiedener Benutzer kurzzeitig sehr stark wuchsen. Auch dabei war aber eine Handsichtung notwendig, um den Eintrag häufig falsch geschriebener und deshalb irrtümlich häufig eingesendeter falscher Vokabeln zu verhindern. Aufgrund des begrenzten Vokabulars jeder Sprache sind heute jedoch für die meisten Sprachen nahezu vollständige Datensätze verfügbar, die die jeweilige Sprache erschöpfend wiedergeben. Der Eintrag neuer Wörter entspricht heute nur noch dem "natürlichen" Wachstum der jeweiligen Sprachen.

Zeitgleich wurden die elektronischen Thesauri zu immer komplexeren Programmen weiterentwickelt, welche auch grammatikalische Regeln und Stilregeln kontrollieren sowie Synonyme anbieten können. An ihren Grenzbereichen geben moderne Thesauri heute auch Übersetzungshilfen und lassen Texte automatisch durchsichten, wobei der Anwender zuvor zahlreiche Optionen anwählen kann.

Eine besondere Form der Thesauri bedient Eingabehilfen für Piktogramm-Schriften wie beispielsweise die Chinesische Schrift unter Verwendung einer westlichen Computer-Tastatur. Diese Schriftzeichen lassen sich wegen ihrer Vielzahl oft nicht auf praktisch handhabbaren Tastaturen abbilden, weshalb die Thesauri dem Anwender Zeichen vorschlagen, die dann von ihm angenommen oder abgelehnt werden können. 

So gibt es für die Eingabe japanischer oder chinesischer Schriftzeichen zahlreiche Methoden, die Silben oder Abkürzungen nach thesauriden Datenbankeinträgen in Schriftzeichen umwandeln. Von diesen Methoden konnte sich jedoch bislang keine standardisiert durchsetzen, weil die asiatischen Schriftsprachen sehr komplex aufgebaut sind und die Bedeutung der Zeichen oftmals kontextabhängig ist. 

Der Lernaufwand zur Nutzung dieser Thesauri-basierten Programmlösungen ist für Asiaten extrem hoch, und native Sprecher benutzen meist nur jeweils eine Softwarelösung, mit der sie akzeptabel hohe Schreibgeschwindigkeiten erreichen können, welche jedoch hinter der der lateinischen Buchstabenschrift weit zurückbleibt. Lateinische Schreiber schreiben wesentlich schneller als asiatische, obwohl die Lesegeschwindigkeit bei Piktogrammschriften für kundige Leser höher ist als bei lateinischen Schriften. Einem einheitlichen Thesaurus für Piktogrammschriften stehen traditionelle, konzeptionelle und syntaktische Probleme entgegen.

In einem linguistischen Thesaurus sind statt Begriffen Wörter ähnlicher und verwandter Bedeutung durch Verweise verknüpft. Diese Art von lexikalisch-semantisch organisiertem Nachschlagewerk kann unter anderem als Formulierungshilfe benutzt werden. Es gibt Nachschlagewerke dieser Art in gedruckter Form oder in elektronischer Form, hier zumeist als Hintergrundressource von Textverarbeitungsprogrammen.






</doc>
<doc id="5295" url="https://de.wikipedia.org/wiki?curid=5295" title="Tragschrauber">
Tragschrauber

Ein Tragschrauber, auch Autogyro, Gyrokopter oder Gyrocopter genannt, ist ein Drehflügler, der in seiner Funktionsweise einem Hubschrauber ähnelt. Jedoch wird hier der Rotor nicht durch ein Triebwerk, sondern passiv durch den Fahrtwind in Drehung versetzt (Autorotation). Der Auftrieb ergibt sich dabei durch den Widerstand des sich drehenden Rotorblatts bei nach hinten geneigter Rotorachse. Der Vortrieb erfolgt, wie beim Starrflügelflugzeug, meist durch ein Propellertriebwerk.

Tragschrauber sind interessant für Anwendungen, bei denen geringe Geschwindigkeiten erwünscht, aber Senkrechtstart und -landung nicht notwendig sind. Vorteile sind dabei die Überziehsicherheit (d. h. ein Strömungsabriss ist nicht möglich), die geringen Bau-, Wartungs- und Betriebsmittelkosten, das geringe Gewicht und der geringe Platzbedarf sowie die einfache Erlernbarkeit der Bedienung.

Als Erfinder des Tragschraubers gilt der Spanier Juan de la Cierva, der seinen "Autogiro" als geschützten Markennamen im Jahr 1923 bekannt machte. Das Grundprinzip des Tragschraubers wurde zwar bereits zuvor von dem Mallorquiner Pere Sastre Obrador verwendet, seine Flugversuche scheiterten jedoch.

Der Antrieb erfolgt meist durch Kolbentriebwerke und Propeller. Der Rotor wird an einem Mast befestigt. Ein Schlaggelenk, das eine Bewegung des Rotorblatts nach oben oder unten zulässt, ist zentrales Merkmal eines Tragschraubers. Eine Taumelscheibe wird nicht benötigt; zur Steuerung wird der komplette Rotor geschwenkt. Zusätzlich wird ein herkömmliches Seitenruder zur Steuerung benötigt. Aufgrund der geringen Geschwindigkeit wird meist ein festes Fahrwerk verwendet. Eine geschlossene Kabine ist eher selten. Einige Modelle setzen zusätzliche Tragflächen ein.

Tragschrauber benötigen nur eine sehr kurze Startrollstrecke von wenigen Metern bis etwa 100 m und gehören damit zur Gruppe der ESTOL-Flugzeuge. Die Landerollstrecke liegt zwischen 0 und einigen 10 m. Zur weiteren Verkürzung der Startstrecke kann der Rotor vor dem Start durch einen weiteren Motor oder eine Getriebeeinheit auf die Startdrehzahl beschleunigt oder auch von Hand angeworfen werden.

Die Autorotation kommt dadurch zustande, dass das Rotorblatt im inneren Bereich der Rotorebene einen so hohen Anstellwinkel hat, dass eine das Blatt beschleunigende Kraft resultiert. Im äußeren Durchmesser hingegen bremst die Resultierende das Blatt. Diese Kräfte sind im stationären Flug im Gleichgewicht. Erhöht man den Anstellwinkel der Rotorebene, verschiebt sich die Grenze zwischen beschleunigendem und abbremsendem Bereich nach außen. Es entsteht ein Ungleichgewicht zugunsten der Beschleunigung, ergo: der Rotor erhöht seine Drehzahl. So wird verständlich, wie der Tragschrauberrotor stets automatisch die nötige Rotordrehzahl einnimmt, um die Maschine zu tragen.

Da der Rotor nur durch den Luftstrom angetrieben wird, ist er mechanisch gering belastet, und es ist kein kompliziertes Getriebe notwendig. Der Antriebsausfall ist ebenfalls unkritisch. Tragschrauber haben keine Mindestgeschwindigkeit und können somit nicht in einen überzogenen Flugzustand geraten. Das war auch die Motivation des Erfinders Juan de la Cierva. Der richtige Umgang mit diesen Fluggeräten muss trotzdem gut geschult werden, denn negative Anstellwinkel sind nicht zulässig, da sie – ähnlich wie zu große Anstellwinkel bei anderen Fluggeräten – zu einem kritischen Flugzustand führen können. Negative Anstellwinkel werden durch starkes Drücken am Steuerknüppel erreicht, was zu einer Anströmung des Rotors von oben führen würde; gleichzeitig hohe Vortriebsleistung verstärkt den negativen Effekt.



Eine Sonderform ist der Flugschrauber (engl. "compound autogyro"), bei dem nur bei Start und Landung der Rotor angetrieben wird, etwa mit Blattspitzenantrieb. Beim Vorwärtsflug erzeugt er dann freidrehend den Auftrieb, gegebenenfalls zusammen mit Tragflächen, mit Vortrieb durch normale Propeller-Triebwerke, so beim Fairey Rotodyne.

Erfunden wurde der Tragschrauber von dem Spanier Juan de la Cierva, der sein Fluggerät "Autogiro" nannte. Nachdem ein von ihm entwickelter Bomber abgestürzt war, weil er in einen überzogenen Flugzustand geraten war, entwickelte er ein Fluggerät, bei dem dieser Zustand nicht auftreten kann. 1920 begann er mit „rotierenden Flügeln“, wie er sie nannte, zu experimentieren. Das Resultat war der erste erfolgreiche Flug eines Autogiro, des "C4", am 9. Januar 1923 in Getafe, Spanien. Es folgten 1925 der C.6 und 1928 der C.8, wobei dem Entwickler wesentliche Lösungen zur Stabilisierung des Rotors eines Drehflüglers gelangen, die später auch bei Hubschraubern genutzt wurden, so z. B. die Schlaggelenke.

Ende der 1920er Jahre gab es einen Autogiro-Boom. Harold Pitcairn und sein Kollege Walter Kellett gründeten nach Lizenzierung durch de la Cierva in den USA eine Firma zur Herstellung von Tragschraubern. Sie belieferten das US Post Office mit ihren Produkten. Mit der Rezession brach dieses Geschäft jedoch ein.

In dieser Zeit wurden Tragschrauber auch in Großbritannien (besonders Cierva in enger Kooperation mit Avro), Deutschland (insbesondere Focke-Wulf), der Sowjetunion (ZAGI) und Frankreich (SNCASO) entwickelt.

In Deutschland entwickelte Oberingenieur Engelbert Zaschka 1927 einen kombinierten Trag- und Hubschrauber. Bei der Entwicklung von Zaschka wurden im Unterschied zu den bis damals bekannten Trag- und Hubschraubern die Rotoren des "Zaschka-Rotationsflugzeugs" mit einer durch zwei Kreisel wirksamen Schwungmasse zwangsläufig rotierend verbunden. Durch diese Anordnung konnte mit abgestelltem Motor ein gefahrloser senkrechter Gleitflug ausgeführt werden.

Nach de la Ciervas Tod 1936 konnten andere auf seinen Erfolgen aufbauen und die Entwicklung des Hubschraubers vorantreiben. In Frankreich wurde der Gyroplane-Laboratoire von Louis Bréguet und René Dorand weiterentwickelt, so dass man 1937 bei Probeflügen, die von dem Ingenieur und Piloten Claysse überwacht wurden, neue Weltrekorde aufstellen konnte.

Im Zweiten Weltkrieg führten einige deutsche U-Boote den unmotorisierten Schlepp-Tragschrauber Focke-Achgelis Fa 330 „Bachstelze“ mit. Er war mit einer Person bemannt, die als Ausguck diente, und wurde an einer Leine hinter dem aufgetaucht fahrenden U-Boot geschleppt. In Großbritannien erfolgte die Kalibrierung der Luftraumüberwachung mit Hilfe von Tragschraubern. Dazu wurden Tragschrauber des britischen Musters Cierva C.30 „Rota“ in einer Flugstaffel eingesetzt. In Japan wurden Tragschrauber Ka-1 zur Artilleriebeobachtung eingesetzt. Geplant wurde außerdem, von umgebauten Frachtern aus operierende Tragschrauber zur Abwehr von U-Booten einzusetzen. In der Sowjetunion wurden fünf in einer Staffel zusammengefasste Tragschrauber ZAGI A-7 am Anfang des Kriegs zu Aufklärungsflügen und zum Abwurf von Flugblättern genutzt.

Somit spielten Tragschrauber insgesamt keine wesentliche militärische Rolle, die Militärs wandten sich dem Hubschrauber zu. Dieser versprach neue taktische Möglichkeiten und ein breiteres Einsatzspektrum.

Tragschrauber werden häufig zu Freizeitzwecken geflogen, wo sie wegen des niedrigen Anschaffungspreises und der geringen Betriebsmittelkosten beliebt sind. Im Oktober 2003 wurde in Deutschland erstmals einem Tragschrauber die Musterzulassung als Ultraleichtflugzeug erteilt. So zugelassene Tragschrauber dürfen dort mit der Sportpilotenlizenz geflogen werden. Besitzer einer solchen Lizenz für herkömmliche Ultraleichtflugzeuge müssen dazu eine Zusatzprüfung ablegen, es ist jedoch auch möglich, die Ausbildung ausschließlich auf Tragschraubern zu absolvieren.

Von 2009 bis 2010 wurde erstmals eine Weltreise per Tragschrauber unternommen. Das Ehepaar Melanie und Andreas Stütz flog in 18 Monaten in verschiedenen Tragschraubertypen durch Europa, das südliche Afrika, Australien, Neuseeland, USA und Südamerika. Das Abenteuer wurde im Buch "Weltflug – Zwei Überflieger auf fünf Kontinenten" und im Film "Weltflug.tv – Die Weltreise mit Tragschrauber" dokumentiert.

Tragschrauber sind grundsätzlich für bestimmte polizeiliche Aufgaben (z. B. Luftraumüberwachung) oder zur Unterstützung im Rahmen von Katastrophenhilfe geeignet. In Deutschland fanden Tests statt. Im Irak werden Tragschrauber für polizeiliche Zwecke eingesetzt.

Von Juli bis Dezember 2007 testete die Polizei Brandenburg einen angemieteten Tragschrauber vom Typ HTC MT-03 auf dem Flugplatz Saarmund auf seine Tauglichkeit für Polizeieinsätze. Vier flugerfahrene Beamte der Landespolizei waren mit dem Praxistest befasst. Zu den Testabschnitten in unterschiedlichen Flughöhen gehörten u. a.:

Hauptmotiv für den Einsatz von Tragschraubern sind die gegenüber Hubschraubern niedrigen Anschaffungs-, Betriebs- und Wartungskosten. So kostet eine Hubschrauber-Flugstunde durchschnittlich 1.000 Euro, eine Tragschrauber-Flugstunde dagegen nur 120 Euro.
Ein weiterer Vorteil ist, dass der Boden zu beiden Seiten leicht beobachtet werden kann. Die Spitzengeschwindigkeit im polizeibezogenen Einsatz liegt bei 160 km/h, die Mindestgeschwindigkeit bei 30 km/h.

Als Hauptproblem bei dem getesteten Tragschrauber erwies sich jedoch die geringe Nutzlast von maximal 210 kg. Weiterhin besteht eine rechtliche Hürde in der Luftfahrtbestimmung, wonach bewohntes Gebiet mit Ultraleichtflugzeugen und Tragschraubern grundsätzlich nicht im Tiefflug überflogen werden darf. Das Innenministerium Brandenburg plante daher eine Ausnahmeregelung für Polizeiflüge. In einem umfassenden Abschlussbericht zur mit Erfolg abgeschlossen Phase II des Projekts wurde ein Rechtsgutachten zum „Einsatz von Tragschraubern bei Behörden mit Sicherheitsaufgaben (Polizei)“ erstellt. Im Ergebnis wurden keine rechtlichen Schranken für den Einsatz erkannt. Das Projekt wurde bis zur Entscheidung des Innenministers mit Erfolg in einer dritten Phase fortgeführt.

Im Februar 2010 ließ der neue Innenminister von Brandenburg, Rainer Speer (SPD), das von seinem CDU-Vorgänger gestartete Projekt einstellen.

2012 testete das DLR (Deutsches Zentrum für Luft- und Raumfahrt) zusammen mit dem THW den Einsatz von Tragschraubern für die Bekämpfung von Katastrophen (vor allem als Erkundungshilfsmittel).

Die Hochschule Koblenz forscht an luftgestützter Sensorik mit Hilfe eines Tragschraubers. 

Seit 2009 wurden mehrere Projekte in Kurdistan, Irak, realisiert. 2010 wurde der erste Tragschrauber an den kurdischen Innenminister Karim Sinjari übergeben. Ziel der Projekte ist es, Piloten auszubilden, die für das Innenministerium die An- und Abflugwege der Flughäfen in Erbil und Sulaimaniyya zum Schutz vor terroristischen Übergriffen überwachen.

Diese auf den Tragschraubern ausgebildeten Piloten bilden das Rückgrat der gesamten Piloten-Crew der kurdischen Polizei. In einem nächsten Schritt werden sie zu Helikopterpiloten (auf Eurocopter EC 120 B) ausgebildet.

Schon im Jahre 1934 tauchte ein Autogyro im Film "Es geschah in einer Nacht" auf, ebenso in "Die 39 Stufen" von 1935. Besonders bekannt wurde der Tragschrauber durch den James-Bond-Film "Man lebt nur zweimal" („Little Nellie“) sowie durch "Mad Max 2".

In der Fernsehserie Das A-Team fliegt H. M. Murdock am Anfang der letzten Staffel einen Tragschrauber und begründet das mit den Worten: „Das habe ich mal in einem James-Bond-Film gesehen. Seitdem habe ich immer einen Hubschrauber im Koffer dabei.“





</doc>
<doc id="5299" url="https://de.wikipedia.org/wiki?curid=5299" title="Teufel">
Teufel

Der Teufel (von "Diábolos," wörtlich ‚Durcheinanderwerfer‘ im Sinne von ‚Verwirrer, Faktenverdreher, Verleumder‘ aus "dia" ‚auseinander‘ und "bállein" ‚werfen‘, zusammengesetzt zu "diabállein" Zerwürfnis stiften, verleumden; ) wird in verschiedenen Religionen als eigenständiges, übernatürliches Wesen angesehen. Er spielt im Christentum und im Islam eine besondere Rolle als Personifizierung des Bösen. Dargestellt wird er oft als Engel mit schwarzen Flügeln oder als „Junker“ mit Pferdefuß. Im Buddhismus nimmt Mara oder auch Devadatta die Stelle eines „teuflischen“ Dämonenwesens ein.

Je nach Religion, Kulturepoche und Geographie wird der Teufel mit anderen Namen benannt.

In der Übersetzung der hebräischen Texte von und ins Griechische wurde das jüdische "ha-Satan" zu "diabolos" (‚Teufel‘) der Septuaginta. Die Vorstellungen von Satan sind im Judentum aber deutlich verschieden von den Vorstellungen und der Verwendung des Begriffs Satan im Christentum und im Islam. Aufgrund der Deutung und Auslegung des Tanachs durch die jeweiligen Gelehrten ergeben sich bedeutende Unterschiede.

"Satan" ist im Tanach vor allem der Titel eines Anklägers am göttlichen Gerichtshof (die hebräische Bezeichnung "Satan" (, "Sin-Teth-Nun") bedeutet so viel wie „Ankläger“). Die Bezeichnung kann auch für Menschen verwendet werden, das hebräische Wort wird dann im Allgemeinen ohne den bestimmten Artikel benutzt (; ; ; Psalm ; als Verben im Sinne von „Feind“ oder „Anfeindung“ in den Psalmen: ; ; ). Üblicherweise wird der Titel Satan verschiedenen Engeln verliehen und kann dann auch allein bezeichnend sein.

Satan wird im Judentum nicht als etwas Personifiziertes oder gar als das personifizierte Böse betrachtet. Im Judentum wird sowohl das Gute als auch das Böse als zwei Seiten einer Zusammengehörigkeit gesehen, die beide z. B. in Gott, dem ewigen Wesen, begründet sind. Gut und Böse sind von dieser Welt, der Gott, das ewige Wesen, transzendent gegenübersteht. Der Satan, wenn der Titel einem Engel in einem Zusammenhang oder in einer Erzählung gegeben wurde, handelt dabei stets nicht eigenmächtig und nicht nach eigenem Willen, sondern im Auftrag Gottes und steht voll unter der Kontrolle und dem Willen Gottes. Der Titel Satan wird im Tanach und anderen heiligen Schriften des Judentums verschiedenen Engeln und Menschen verliehen.
Die ausführlichste Darstellung, in der ein Engel mit dem Titel "Satan" in Gottes Auftrag wirkt, findet sich im Buch Hiob. Die Erzählung beginnt mit der Szene am himmlischen Gerichtshof, bei dem Gott und ein Engel anwesend sind. Aufgrund des Einwands des Engels in dieser göttlichen Gerichtshofsrunde, der als Ankläger, also als "Satan" fungiert, kommt es zu einem Vorwurf an Gott. Der fromme und wohlhabende Hiob halte Gott nur deshalb die Treue, weil Gott um ihn herum kein Unglück zulasse. Daraufhin gestattet Gott Satan, Hiobs Gottvertrauen auf die Probe zu stellen. Trotz der Unglücke und trotz der leidvollen Krankheit, die den nichtsahnenden Hiob daraufhin im Auftrage Gottes ereilen, akzeptiert Hiob sein trauriges Los und flucht seinem Gott nicht. Er kritisiert ihn jedoch und besteht darauf, dass er nichts Unrechtes getan habe. Hiobs Freunde sind davon überzeugt, dass er ein Unrecht begangen haben müsse, denn Gott lasse es nicht zu, dass ein Unschuldiger so viel Unglück zu erleiden habe. Damit wird der Einwand des Engels widerlegt, es gebe keinen Menschen, der Gott in jeder Situation treu bleibe oder nicht von Gott abfalle, sobald es ihm aus menschlicher Sicht schlecht ergehe. In zwei weiteren Fällen tritt ein Satan als Versucher oder Ankläger des sündigen Menschen vor Gott auf. In ist der sich in den Weg stellende Engel (Satan) letztlich nicht negativ handelnd, sondern wird von Gott gesandt, um Schlimmeres für Balaam zu verhindern.

In den außerbiblischen volkstümlichen jüdischen Erzählungen des europäischen Mittelalters wird der Titel "Satan" manchmal einem Engel, der von Gott verstoßen wird, weil er sich selbst gottgleich stellen wollte, gegeben. Die Geschichten, in denen dies geschieht, sind dabei in vollem Bewusstsein und Kenntnis der Lehren des Judentums, die solche Vorstellungen stets ablehnten, gegenüberstellend erzählt. Er gilt als Träger des Prinzips des Bösen. Hier wird auf alte Begrifflichkeiten des persischen Kulturkreises, in dem das duale Prinzip des Kampfes Gut gegen Böse eine große Rolle spielt, und die Vorstellungen der umgebenden christlichen Kultur angespielt. Es sind daher eher phantastische Erzählungen oder Schauergeschichten und nicht biblische jüdische Lehren oder lehrhafte jüdische Erzählungen der Tradition. Eventuell werden die Vorstellungen des Christentums auch nur illustratorisch nacherzählt, um die Position der Christen, die in Widerspruch zu denen des Judentums stehen, vorzustellen.

Im Christentum ist der Teufel der Inbegriff des Bösen. Er wird auch (abweichend von der alttestamentlichen Bedeutung dieser Namen) Satan oder Luzifer genannt. Der Teufel wird dabei als ein gefallener Engel angesehen, der gegen Gott rebellierte.

Die christliche Tradition bezieht auch die Schlange in der Schöpfungsgeschichte oft auf den Teufel. Diese Gleichsetzung findet sich schon in der Offenbarung des Johannes. In der Tradition wird der Teufel als Urheber der Lügen und des Bösen in der Welt angesehen. Die Offenbarung nennt ihn den „großen Drachen, die alte Schlange, die Teufel oder Satan heißt und die ganze Welt verführt“ (Offenbarung ). Der Epheserbrief bezeichnet sein Wirken „Herrschaft jenes Geistes, der im Bereich der Lüfte regiert und jetzt noch in den Ungehorsamen wirksam ist“. Besonders ausführlich wird der Teufel im apokryphen äthiopischen Henochbuch als Azazel als einer jener Gottessöhne erwähnt, die mit den Menschentöchtern die Nephilim, die „Riesen der Vorzeit“, zeugten.

Ebenfalls im Neuen Testament wird Satan als Engel bezeichnet, der sich als Engel des Lichts ausgebe (), und als personifiziertes Geistwesen vorgestellt, das stets als Teufel agiere. So heißt es: „Wer die Sünde tut, stammt vom Teufel; denn der Teufel sündigt von Anfang an. Der Sohn Gottes aber ist erschienen, um die Werke des Teufels zu zerstören.“ ().

Im Buch Jesaja findet sich ein Spottlied auf den König von Babylon, von dem eine Stelle später von christlicher Tradition auf den Satan bezogen wurde, ursprünglich aber eine Anspielung auf die Gestalt des Helel aus der babylonischen Religion ist, das Gegenstück zum griechischen Gott Helios. Der Bezug auf den König wird schon anfänglich klargemacht:

Die Kirchenväter sahen darin eine Parallele auf den in beschriebenen Fall Satans („Da sagte er zu ihnen: Ich sah den Satan wie einen Blitz vom Himmel fallen“). Eine theologische Begründung für die Gleichsetzung besteht darin, dass die Stadt Babylon in der Offenbarung mit dem Teufel am jüngsten Tag gemeinsam von Gott vernichtet werde. Andere wenden dagegen ein, dass eine angenommene gleichzeitige Vernichtung keine Identität bedeute.

Auf ähnliche Weise wurden auch Teile von auf den Fall des Satans bezogen. Dort spricht der Prophet vom Ende des Königs von Tyrus, der wegen seines Hochmuts sich für einen Gott hält und daher angeklagt wird. In den Versen 14–15 heißt es dann an den König gerichtet: „Du warst ein vollendet gestaltetes Siegel, voll Weisheit und vollkommener Schönheit. Im Garten Gottes, in Eden, bist du gewesen. Allerlei kostbare Steine umgaben dich […] Aus Gold war alles gemacht, was an dir erhöht und vertieft war, all diese Zierden brachte man an, als man dich schuf. Einem Kerub mit ausgebreiteten, schützenden Flügeln gesellte ich dich bei. Auf dem heiligen Berg der Götter bist du gewesen. Zwischen den feurigen Steinen gingst du umher.“

In den Evangelien bezieht sich Jesus in verschiedenen Gleichnissen auf den Teufel, etwa im Gleichnis vom Unkraut unter dem Weizen:
Vor dem tausendjährigen Reich gibt es nach der Offenbarung des Johannes einen Kampf zwischen dem Erzengel Michael und seinen Engeln und Satan, der damit endet, dass der Teufel und seine Anhänger auf die Erde geworfen werden (Höllensturz). Für die Dauer des tausendjährigen Reichs wird er aber gefesselt, um danach wieder kurz freigelassen zu werden. Er verführt dann für eine gewisse Zeit Menschen, ehe er in einen Feuersee geworfen wird ().

Einige wenige christliche Gemeinschaften, wie die Christadelphians, die Church of the Blessed Hope oder Christian Science, lehnen die Vorstellung der Existenz eines Teufels oder Satans als reales böses Geistwesen ab.

Die Widersagung des Bösen "(Abrenuntiatio diaboli)" gehört in der römisch-katholischen Kirche zum Ritus der Taufe und zur Erneuerung der Taufversprechen in der Feier der Osternacht. Im Katechismus der Katholischen Kirche heißt es in 391–394 über den Satan:

Der katholische Literaturwissenschaftler und Anthropologe René Girard interpretiert das christliche Verständnis Satans in seiner Analyse der neutestamentlichen Texte als eines der Hauptmotive der christlichen Offenbarung. Im Rahmen der von ihm formulierten mimetischen Theorie ist die Teufelsdarstellung in den Evangelien ein Paradigma des mimetischen Zyklus: Der Teufel ist der Versucher und der Stifter des Begehrens und des „Ärgernisses“ "(skándalon)," sein Wirken ist die sich selbst austreibende mimetische (= nachahmende) Gewalt, und er ist der „Mörder vom Anfang“, der das mythische Religionssystem, den kirchlichen Christusmythos, das ist die Gottwerdung und Anbetung des jüdischen Wanderpredigers, Rabbiners und Messias Jesus von Nazareth und die Trennung vom Judentum, hervorbringt. In der Bloßstellung der menschlichen (mimetischen) Gewalt durch die Passion und im darauffolgenden Ende des heilbringenden Opferkultes der archaischen Welt sei der Sinn des Triumphes des Kreuzes über die „Gewalten und die Mächte“ des Kolosserbriefes (2,14–15) sowie jene Täuschung der „Herrscher dieser Welt“ des 1. Korintherbriefes (2,6–8) zu sehen, wenn man diese und ähnliche Begriffe mit Satan gleichsetzt, wie die Kirchenväter es machten. Girards Auffassung wurde von manchen theologischen Kreisen rezipiert, allerdings sind seine Gedanken in der christlichen Dogmatik ungewöhnlich und in der kirchlichen Öffentlichkeit noch kaum bekannt. Er verweist jedoch auf Origenes und dessen These des vom Kreuz getäuschten Satans als Träger „eine(r) wichtige(n) Intuition“, die in der westlichen Kirche „unter den Verdacht, ‚magisches Denken‘ zu sein“, geriet.

Ikonographische Attribute des Teufels gehen teils auf heidnische Götter zurück, etwa mit dem griechischen Gott Pan. Der Teufel wird meist schwarz und behaart, mit einem oder zwei Bocks- oder Pferdefüßen, Widderhörnern und einem Schwanz dargestellt. Bei seinem Verschwinden hinterlasse er zudem einen argen Gestank.

Im Islam kann jedes vernunftbegabte Wesen – aufgrund des ihm von Gott gegebenen freien Willens – sich vom Guten, das heißt Gottes Gesetzen, Gerechtigkeit und Weisheit, abwenden und letztendlich zu einem Teufel (Schaitan, , Plural Shayāṭīn, bedeutungs- und herkunftsgleich mit = Satan) werden. Ausgehend von der Existenz diverser Geistwesen wie den Dschinn, die die Welt neben den Menschen bevölkern, können somit sowohl sichtbare als auch unsichtbare Wesen (Sure 6:112: "al-Ins" und „al-Dschinn“) Teufel sein. Die Teufel verursachen Leid, erschweren das Leben anderer, verursachen Zweifel an Gott und verführen zum Bösen.

Als oberster Teufel gilt Iblis () als Verführer der Menschen und Dschinn. Dieser wurde selbst zum Teufel, als er sich hochmütig weigerte, auf den Befehl Gottes hin sich vor Adam niederzuwerfen, und aus dem Himmel verbannt wurde. Der Name Iblis begründet sich nach islamischer Auffassung in der Gottesferne und daraus resultierende Verzweiflung. Zuvor hieße er gemäß islamischer Tradition Azazel. Iblis’ Arroganz gilt nach islamischer Auffassung dem Menschen (da er sich selbst als „aus Feuer geschaffen“ besser wähnte als Adam), nicht aber Gott. Nach dem strengen Monotheismus des Islams ist es undenkbar, dass ein Wesen danach streben könnte, Gott zu stürzen oder um seine Erhabenheit zu beneiden. Darum bat Iblis um Erlaubnis, die Menschen irrezuleiten. Ob es sich bei Iblis um einem zum Teufel degradierter Engel handelt, der in Gottes Auftrag die Rolle eines für die menschliche Entwicklung notwendigen Verführers einnimmt, oder um einen Dschinn, der aufgrund seiner eigenen Triebseele sich dem Bösen zuwandte, ist unter islamischen Gelehrten umstritten. Unabhängig davon ist dem Islam die Vorstellung des Teufels als Widersacher Gottes fremd, der eine Art Kräfte-Gegenpol darstellt. Das Prinzip Gut gegen Böse als Gegenkräfte ist hier nicht anwendbar, denn nur Gott ist der absolut Mächtige, der Teufel hingegen kann lediglich als Versucher gelten. Es gehört zu den Prüfungen der Menschheit, sich zu entscheiden, d. h. für Gott oder für das Teuflische.

Im Jesidentum existiert die Gestalt des Bösen nicht. Die jesidische Vorstellung ist, dass Gott allmächtig ist und neben Gott keine zweite Kraft existieren kann. Die Jesiden sprechen das Wort des Bösen nicht aus, weil allein der Ausspruch dieses Wortes die Anzweiflung der Einzigartigkeit Gottes sei. Nach jesidischer Vorstellung wäre Gott schwach, wenn er noch eine zweite Kraft neben sich existieren ließe. Diese Vorstellung wäre mit der Allmacht Gottes nicht vereinbar.

Die Religion Zarathustras, der Zarathustrismus, ist dualistisch geprägt: "„Und im Anbeginn waren diese beiden Geister, die Zwillinge, die nach ihrem eigenen Worte das Gute und das Böse im Denken, Reden und Tun heißen. Zwischen ihnen haben die Guthandelnden richtig gewählt.“"

Gérald Messadié sieht den Wandel Satans vom Ankläger in Gottes Rat zum Gegenspieler Gottes als Übernahme des Ahriman aus dem Zoroastrismus an; dort sind der böse Welterschaffer und der gute Gott Ahura Mazda in der Tat Gegenspieler.

Im Zarathustrismus (auch Zoroastrismus) gelangen die Seelen nach dem Tod über die "Činvat-Brücke". Hier wird Gericht gehalten: Für den rechtschaffenen Menschen ist die Brücke breit wie ein Pfad, für den anderen schmal wie eine Messerspitze. Die Guten gelangen in die seligen Gefilde des Paradieses Garodemäna (später Garotman), den „Ort der Lobgesänge“; die Seele des Bösen aber kommt an den „schlechtesten Ort“, d. h. in die Hölle. Die Dämonen des Zoroastrismus werden Daeva, Drudsch und Pairikas "(Peri)" genannt und teils als Unholdinnen gedacht, die mit bösen Menschen in fleischlichem Verkehr stehen und die guten zu verführen trachten, teils als tückische Dämonen, welche Trockenheit, Missernten, Seuchen und andere Plagen über die Welt verhängen.

Die Schöpfungsgeschichte des Zarathustrismus besagt, dass Ahura Mazda (Gott) in den ersten 3000 Jahren durch einen langherrschenden Windhauch zuerst den eiförmigen Himmel und daraufhin die Erde und die Pflanzen erschuf. Im zweiten Zyklus von 3000 Jahren entstanden die Urstiere und danach der Urmensch. Danach erfolgte der Einbruch des Anramainyu (der „Teufel“), welcher den Urmenschen und den Urstier tötete und eine Periode des Kampfes eröffnete, die ihr Ende erst mit der Geburt des Zarathustra erreichte. Dieses Ereignis fiel in das 31. Jahr der Regierung des Königs Vistaspa. Und von da an werden wieder 3000 Jahre vergehen, bis der Heiland Saoschjant geboren wird, welcher die bösen Geister vernichten und eine neue, unvergängliche Welt herbeiführen wird; auch die Toten sollen dann auferstehen.

Statt des einen Messias werden an anderen Stellen deren drei genannt, wodurch sich also diese Lehre von der entsprechenden des Alten Testaments unterscheidet. Dagegen stimmt die Lehre von der Auferstehung sogar in Details mit der christlichen überein, so dass die Annahme einer Entlehnung der letzteren aus der Religion der den Hebräern benachbarten Zarathustristen eine gewisse Wahrscheinlichkeit für sich hat. Speziell die Begriffe Himmel und Hölle waren im alten Judentum nicht bekannt.

Der außerdem oft verwendete Begriff "Luzifer" ist nichtjüdischen Ursprungs: In der Antike war Luzifer der Name für den Planeten Venus; im antiken Babylon wurde die Venus als „Tagesstern“, „Sohn der Morgendämmerung“ oder auch „Morgenstern“ oder „Abendstern“ bezeichnet. Die römische Mythologie kennt Luzifer als Sohn der Aurora, der Göttin der Morgenröte. In der griechischen Mythologie ist die Göttin Eos das Gegenstück zu der römischen Aurora. Und auch hier hatte diese Göttin einen Sohn, welcher Phosphoros oder Eosphóros (griech. ‚Lichtträger‘) hieß. Dieser entspricht also dem römischen Lucifer (lat. ‚Lichtträger‘ beziehungsweise ‚Lichtbringer‘). Da in Jesaja 14,12 ein aus den Himmeln hinabstürzender „Engel [eigentlich Cherub, s. u.] der Morgenröte“ Erwähnung findet, wurde in der Vulgata der „Glanzstern“ von Jesaja 14,12 als „lucifer“ wiedergegeben.

Die nachfolgend aufgelisteten Namen bezeichnen teilweise "den" Teufel, teilweise "einen" von mehreren Teufeln oder eine Erscheinungsform des Teufels. Siehe die jeweilige Erläuterung und die verlinkten Artikel.

Namen aus dem Bereich Judentum, Islam und Christentum (Sprachen: Hebräisch, Arabisch, Griechisch, Lateinisch, Deutsch):


Manche Menschen nehmen an, dass die Nennung des Namens des Teufels dazu führen könne, dass dieser herbeigerufen werde. Es gibt daher eine Vielzahl von verhüllenden Bezeichnungen und Umschreibungen für den Teufel. Ein anderer Grund für die Benutzung einer Umschreibung kann die besondere Betonung eines Aspekts seines Wesens sein. Beispiele:

Es gibt Gottheiten aus anderen Religionen und Mythologien, die innerhalb des Christentums mit dem Teufel identifiziert wurden.

Im Jahr 1922 befasste sich der Psychoanalytiker Sigmund Freud mit dem christlich-volkstümlichen Teufelsglauben. In dem 1923 erschienenen Aufsatz "Eine Teufelsneurose im siebzehnten Jahrhundert" (XIII, S. 317–353) bewertete er die kirchliche Seelsorge als nicht hilfreich im Umgang mit kranken Menschen. Die Diagnose zum Krankheitsbild des bayerischen katholischen Christen, Malers und Teufelsbündlers Christoph Haitzmann, der sich 1669 mit eigenem Blut dem Teufel verschrieben hatte, lautete: „Nicht aufgearbeitete Depression infolge des Verlustes einer nahe stehenden Person“. Die Legende hat folgende Ereignisse hierzu überliefert:
Am 8. September 1677 zum Tag Mariä Geburt erschien während einer exorzistischen Praktik um Mitternacht in der Wallfahrtskirche Mariazell der Teufel als geflügelter Drache dem Christoph Haitzmann im Beisein von Mönchen.

Dem Psychoanalytiker Slavoj Žižek zufolge fungiere nicht nur der Teufel „als diabolos (von diaballein: trennen, das Eine in zwei auseinanderziehen) und Jesus Christus als sein Gegenteil, als Symbol (zu symballein: sammeln und vereinen)“. Vielmehr sei Jesus Christus laut selbst das Trennende (diabolos) und sowohl der Teufel wie auch Judas Iskariot lediglich seine Unterstützer dabei.

Zahlreiche Märchen erzählen – zumeist entgegen der christlichen Dogmatik – von einem Teufel, der oftmals komische Züge trägt. Darunter fallen z. B. KHM 29 Der Teufel mit den drei goldenen Haaren, KHM 100 Des Teufels rußiger Bruder, KHM 125 Der Teufel und seine Großmutter oder KHM 189 Der Bauer und der Teufel.

Viele bekannte Schauspieler haben im Laufe der Zeit den Teufel verkörpert, wobei unterschiedlichste Ansätze, von sehr humoristisch bis ausgesprochen ernsthaft und böse, gewählt wurden:

Viele weitere Filme beschäftigen sich mit dem Teufel:

Der im Mittelalter verpönte Tritonus wurde auch als "Diabolus in musica" (lateinisch: ‚Teufel in der Musik‘) beziehungsweise als "Teufelsintervall" bezeichnet. Seit dem Lied "Black Sabbath" der gleichnamigen Band, das auf dem Tritonus basiert, ist dieser ein Markenzeichen ihres „bösen“ Klangs. Der Titel des Albums "Diabolus in Musica" der Metal-Band Slayer spielt ebenfalls auf den Tritonus an.

Die Teufelsgeige ist ein einfaches Rhythmus- und Lärminstrument.

In folgenden Musikstücken wird der Teufel thematisiert (Auswahl)


Eine heitere Sicht auf den Teufel bietet die Geschichte "Der Teufel in der Weihnachtsnacht" von Charles Lewinsky. In dem Text besucht der Teufel den Papst, um ihn zu verführen. Das tut er mit großer Überzeugungskraft. Das Buch ist eine gelungene Persiflage auf die weihnachtliche Atmosphäre im Vatikan und vor allem auf die katholische Kirche.

Wanders Deutsches Sprichwörter-Lexikon bietet unglaubliche 1700 Sprichwörter mit dem Wort "Teufel," sieht man einmal von den Sprichwörtern zusammengesetzter Worte mit Hilfe von "Teufel" ab. Der Sachverhalt verdeutlicht, dass der "Teufel" in der (deutschen) Sprache sehr lebendig ist und in vielen Erscheinungsformen auftritt.

Im traditionellen Puppentheater tritt der Teufel als Inkarnation des Bösen auf. Er kontrastiert damit Figuren, die das Prinzip des Guten verkörpern, wie den Kasper, Feen oder Engel. Im klassischen Puppenspiel vom Dr. Faustus kommt dem Teufel als Mephistopheles in der Rolle des Gegenparts und Begleiters der Hauptfigur Faust eine tragende Rolle zu. Im modernen Verkehrskasperspiel fungiert der Teufel als Verführer, der die Menschen zu verkehrswidrigem, unfallträchtigem und kooperationsfeindlichem Verhalten verleiten will. In dieser Rolle spiegelt er die destruktiven Tendenzen im Menschen, die von Eigennutzdenken, Vorteilsucht, Machtstreben u. a. Untugenden bestimmt werden und sich bereits im kindlichen Verhalten finden. Die Figuren des Puppentheaters haben im Laufe ihrer langen Theatergeschichte unterschiedliche Interpretationen erfahren. Im pädagogisch ausgerichteten neuzeitlichen Verkehrstheater wird die starre Typologie und die Fixierung auf das Ewig-Böse teilweise aufgegeben und unter Mithilfe des Publikums selbst dem Teufel die Chance einer letztendlichen Bekehrung zum Guten und Nützlichen eingeräumt.

Je nach Art des Puppentheaters findet sich die Figur des Teufels in der Ausführung als Handpuppe, Stabpuppe oder Marionette.





</doc>
<doc id="5300" url="https://de.wikipedia.org/wiki?curid=5300" title="Tein">
Tein

Tein steht für:
TEIN steht für:
Siehe auch:


</doc>
<doc id="5302" url="https://de.wikipedia.org/wiki?curid=5302" title="Geschichte des Urheberrechts">
Geschichte des Urheberrechts

Die Geschichte des Urheberrechts beschreibt die Geschichte des Rechts des Urhebers an seinen Werken.

Die antiken Rechtsordnungen kannten kein Urheberrecht im heutigen Sinne. Sehr wohl bekannt war jedoch eine Beziehung zwischen Autor und Werk, sei sie als echter Stolz auf die eigene Leistung oder als Weitergabe einer göttlichen Gabe verstanden. In der neueren rechtshistorischen Forschung wird ferner auf funktionale Äquivalente im Bereich des Urheberpersönlichkeitsrechts, d. h. des Veröffentlichungsrechts, Namennennungsrechts und
Entstellungsverbots, verwiesen, die jedoch nicht die Schlagkraft einer durchsetzbaren Rechtsnorm erreichten.

Verdeutlicht wird dies durch die Entstehung des Wortes "Plagiat:" Es geht auf eine der ältesten bekannten Urheberrechtsverletzungen aus dem Rom des ersten Jahrhunderts nach Christus zurück. Der römische Dichter Marcus Valerius Martialis prägte den Begriff „Plagiat“. Er verglich seine Epigramme mit freigelassenen Sklaven und bezeichnete einen gewissen Fidentinus, der Gedichte des Martialis fälschlich als eigene ausgegeben hatte, als Menschenräuber (lat. „plagiarius“).

Die Frage, warum das technisch so hoch entwickelte römische Recht keinen dem heutigen Urheberrecht vergleichbaren Schutz gewährte, wird unterschiedlich beantwortet: Einerseits wird auf das „sachgegenständliche Denken“ verwiesen, das zur Abstraktion eines nur „geistigen“ Eigentums nicht fähig gewesen sei. Eine andere Auffassung verweist auf die römische Gesellschaftsstruktur. Die große Masse der Arbeitskräfte wurde aus Sklaven rekrutiert; im Rechtssystem schlug sich dies in der Weise nieder, dass die "artes liberales" ihre Dienste nur in unentgeltlichen Vertragsformen anbieten konnten; lediglich aus Dankbarkeit für die Freundschaftsleistung konnte der Vertragspartner einen Ehrensold, das "honorarium," zahlen. Dem entspreche, dass es dem römischen Bürger als unehrenhaft erschienen wäre, schöpferische Leistungen unter vermögensrechtlichen Schutz zu stellen. Als Grund für das fehlende Urheberrecht der Antike wird ferner die technologische Aufwendigkeit der Vervielfältigung und das daher geringe ökonomische Interesse an ihrer Monopolisierung genannt.

Auch im Mittelalter war ein Recht auf geistige Werke unbekannt. Rechtsregeln gab es nur für die Sachen, in denen sich das Geisteswerk zeigte, insbesondere für das Eigentum hieran. So durfte ein Buch beispielsweise nicht gestohlen, wohl aber abgeschrieben werden. Die Bearbeitung eines Stoffes durch viele verschiedene Künstler und Autoren war der Normalfall, ebenso die Übernahme oder Veränderung von Liedern und Musikstücken durch andere Musiker. Wenn ein Autor keine Veränderung seines Textes wollte, behalf er sich mit einem Bücherfluch – so wünschte Eike von Repgow, der Verfasser des Sachsenspiegels, jedem den Aussatz auf den Hals, der sein Werk verfälschte.

Hier berührt sich die rechtsgeschichtliche mit einer geistesgeschichtlichen Beobachtung: auch die Zitierpraxis war in jenen Zeiten wesentlich weniger streng als heute. Der Rang eines Künstlers bemaß sich mehr nach seinen handwerklichen Fertigkeiten als nach der Originalität seiner Erfindungen.

Mit der Erfindung des Buchdrucks (um 1440) wurde es einfacher, Kopien eines Werkes in größeren Mengen herzustellen. Dem Autor stand immer noch kein „Urheberrecht“ zur Seite. Er musste froh sein, wenn sein Werk nicht nur gedruckt wurde, sondern der Drucker beziehungsweise Verleger ihm etwas für das Manuskript zahlte. Nun kam es dazu, dass andere Drucker Erstdrucke nachdruckten. Das erschwerte dem Erstdrucker das Geschäft, da er mehr Arbeitskraft investiert und eventuell einen Autor bezahlt hat – der Nachdrucker konnte seine Produkte naturgemäß billiger anbieten. Auch ein Autor konnte unzufrieden über Nachdrucke sein – sie waren meist weniger sorgfältig hergestellt: Fehler schlichen sich ein oder der Text wurde gar absichtlich abgeändert:

Um dem Nachdruck entgegenzutreten, erbaten sich Drucker daher Sonderrechte von den Obrigkeiten, die das Nachdrucken eines Werkes zumindest für eine bestimmte Zeit verboten. Die Bezeichnung für diese Sonderrechte ist Privileg, im kirchlichen Bereich Imprimatur – wobei letzteres auch im weltlichen Bereich im Sinne von Druckfreigabe/Druckgenehmigung seitens des Druckereikunden in Gebrauch kam und bis heute blieb. Die Interessen der Drucker trafen sich mit denen der Obrigkeiten, die auf die in ihrem Herrschaftsbereich veröffentlichten Schriften Einfluss haben wollten. Dies gelang besonders in Frankreich mit seiner frühen absolutistischen Struktur, weniger beispielsweise in Deutschland. Hier ignorierten manche Landesfürsten sogar bewusst Verstöße von Verlegern gegen kaiserliche Privilegien, um sie wirtschaftlich zu unterstützen und sich begehrte Literatur billiger ins Land zu holen. Auch die Ideen der Aufklärung verbreiteten sich zu einem großen Teil durch Raubdrucke.

Mit Beginn der Renaissance rückte die Individualität mehr in den Vordergrund und man gewährte auch Autorenprivilegien, mit denen der Schöpfer für sein Werk belohnt wurde.

In Deutschland wurde ein solches Privileg zum Beispiel Albrecht Dürer eingeräumt. Dürer gibt bereits in einem Nachwort seiner Holzschnittpassion 1511 an, ein kaiserliches Privileg bekommen zu haben, welches jedoch nicht erhalten ist. Interessant an jenem Nachwort ist, dass Dürer bereits von „ingenium“, also „Schöpfung“ schreibt, was für damalige Autoren und Künstler noch ungewöhnlich und ein mit der Renaissance einhergehendes neues Selbstverständnis ist. Das erste erhaltene Privileg Dürers stammt aus dem Jahr 1525. Auch seine Witwe erhält nach seinem Tod 1527 noch ein Privileg, was damals unüblich war und für Dürers außergewöhnliche Stellung als Künstler spricht.

Privilegien schützten jedoch den Schöpfer als Person (Persönlichkeitsrecht) und brachten den Urhebern noch keine Einnahmen. Angeknüpft wurde auch weiterhin am Werk als einer Sache. Mitte des 16. Jahrhunderts wurden Territorialprivilegien eingeführt – allgemeine Nachdruckverbote in einem bestimmten Gebiet für einen begrenzten Zeitraum.

Als die Verleger dazu übergingen, den Autoren Honorare zu zahlen, bildete sich die Überzeugung, ihnen (den Verlegern) würde damit ein ausschließliches gewerbliches Schutzrecht zustehen (Lehre vom Verlagseigentum), auch wenn sie kein Privileg für ein Werk besaßen. Der Nachdruck wurde daher verboten, wenn der Verleger die Rechte erworben hatte.

Erstmals wurde im 18. Jahrhundert über eigentumsähnliche Rechte an "geistigen Leistungen" (und das Phänomen des "immateriellen Besitzes") theoretisiert. Ein englisches Gesetz von 1710, das so genannte Statute of Anne, erkannte als erstes ein ausschließliches Vervielfältigungsrecht der Autoren an, die es dann an die Verleger abtraten. Nach Ablauf der vereinbarten Zeit fielen alle Rechte wieder an den Autor zurück. Das Werk musste im Register der Buchhändlergilde eingetragen und mit einem Copyrightzeichen versehen sein, damit es geschützt war. Die Vereinigten Staaten führten das Verfahren 1795 ein (diese Registrierung wurde in England jedoch 1956 und in den Vereinigten Staaten 1978 wieder abgeschafft). Überwiegend wurde die Idee vom geistigen Eigentum mit der Naturrechtslehre begründet. Auch Frankreich führte in zwei Gesetzen von 1791 und 1793 ein "Propriété littéraire et artistique" ein. In Preußen kam es im Jahr 1837 zu einem entsprechenden Schutz. Die Bundesversammlung (Deutscher Bund) beschloss ebenfalls 1837 eine zehnjährige Schutzfrist seit Erscheinen des Werkes – 1845 auf 30 Jahre nach dem Tode des Urhebers "(post mortem auctoris)" verlängert. 1870 wurde im Norddeutschen Bund ein allgemeiner Urheberrechtsschutz eingeführt, den das Deutsche Reich 1871 übernahm und später weiter ausbaute.
1886 wurde mit der Berner Übereinkunft das erste internationale, multilaterale Abkommen zum Urheberschutz geschlossen. Vereinbart wurde eine Mindestschutzfrist aller Werke (außer fotografische und cinematographische Werke) der Verbandsländer von 50 Jahren.

In der Zeit des Nationalsozialismus galt der Urheber als „Treuhänder des Werks“ für die Volksgemeinschaft. Bestehende Gesetze wie das Kunsturheberrechtsgesetz blieben in Kraft.

Am 6. September 1952 wurde in Genf das Welturheberrechtsabkommen beschlossen. Es sollte eine weltweite Regelung zum Schutz der Urheberrechte darstellen und die Verbreitung der Geisteswerke erleichtern. Die unterzeichnenden Staaten verpflichteten sich, ihre eigenen Gesetzesgrundlagen entsprechend anzupassen.

In Deutschland wurde das heute noch gültige deutsche Gesetz über Urheberrecht und verwandte Schutzrechte (Urheberrechtsgesetz – UrhG) am 9. September 1965 verkündet. Es löste insbesondere das Gesetz betreffend das Urheberrecht an Werken der Literatur und der Tonkunst (LUG) vom 19. Juni 1901 und weitgehend das Gesetz betreffend das Urheberrecht an Werken der bildenden Künste und der Photographie (Kunsturheberrechtsgesetz – KUG, KunstUrhG) vom 9. Januar 1907 ab. Unter anderem sah es eine Verlängerung des Urheberrechts von 50 auf 70 Jahre nach dem Tod des Urhebers vor. Damit wurde Deutschland international zu einem Vorreiter im Bezug auf die Verlängerung von Urheberrechtsfristen. Zugleich wurde auch die Privatkopie wieder legalisiert, die in den 50er Jahren durch Gerichtsentscheidungen entgegen dem damaligen Gesetzeswortlaut aufgehoben worden war.

Seit den 1990er-Jahren wurde das Urheberrecht in mehreren Internationalen Vertragswerken behandelt, in denen der Schutz der Urheberrechte den neuen technischen Möglichkeiten des Internets angepasst und zum Teil deutlich gestärkt wurde. Das 1994 im Rahmen der Welthandelsorganisation (WTO) verabschiedete Übereinkommen über handelsbezogene Aspekte der Rechte des geistigen Eigentums (TRIPS) etabliert weltweite Mindeststandards für das Urheberrecht, nach denen Ausnahmen von den ausschließlichen Rechten der Urheber auf wenige Sonderfälle reduziert werden müssen (Drei-Stufen-Test, Art. 9 Abs. 2 RBÜ). Zugleich schränkt es die Vermietung von urheberrechtlich geschützten Werken ein und schreibt eine Mindestschutzdauer von 50 Jahren über den Tod des Urhebers hinaus ("post mortem auctoris") vor.

Im Jahr 1996 wurde im Rahmen der Weltorganisation für geistiges Eigentum (WIPO) der WIPO-Urheberrechtsvertrag (WTC) und der WIPO-Vertrag über Darbietungen und Tonträger (WPPT) unterzeichnet. Sie regeln insbesondere Fragen des Urheberrechts in der Informationsgesellschaft. Im Einzelnen ging es um folgende Themen:


Durch das TRIPS und die anderen Verträge wurden einseitig die wirtschaftlichen Interessen der Urheber und Rechteverwerter gestärkt. Die Rechte der Konsumenten und der Allgemeinheit wurden dagegen eingeschränkt.

Infolge dieser Verträge haben einzelne Staaten nur noch geringe Spielräume in der Ausgestaltung des Urheberrechts. Unübliche Regelungen würden zum Beispiel im Rahmen der WTO als Verzerrungen des freien Welthandels behandelt, die von einem Schiedsausschuss sanktioniert werden können. Darüber hinaus üben die Vereinigten Staaten auch bilateral Druck auf einzelne Staaten aus, die Urheberrecht ihrer Meinung nach nicht intensiv genug schützten (siehe z. B. die Fälle The Pirate Bay und Allofmp3).

Diese Urheberrechtsverträge wurden 1998 in den Vereinigten Staaten mit dem Digital Millennium Copyright Act (DMCA) und 2001 in der EU mit der EG-Urheberrechtsrichtlinie in nationales beziehungsweise supranationales Recht umgesetzt. Sie übernahmen die meisten der oben im TRIPS und dem WCT beziehungsweise WPPT festgelegten Verschärfungen. Darüber hinaus regeln sie auch die Verantwortlichkeit der Internetdiensteanbieter (ISP). Diese sind für die Urheberrechtsverletzungen ihrer Kunden haftbar, wenn sie diese auf Anforderung der Rechteinhaber nicht sofort abstellen. Sie sind darüber verpflichtet, die Identität der Verletzer offenzulegen.

Die Unterhaltungsindustrie hat ein intensives Lobbying zugunsten dieser Regelungen betrieben.

Die EG-Urheberrechtsrichtlinie ist von den Mitgliedstaaten in nationales Recht umzusetzen. In Deutschland gilt in diesem Zuge seit dem 13. September 2003 ein novelliertes Urheberrecht, das unter anderem die Umgehung eines wirksamen Kopierschutzes für kommerzielle, aber auch private Zwecke unter Strafe stellt. Die § ff. UrhG sehen einen „Schutz technischer Maßnahmen“ vor. Gemäß § 95a Abs. 1 UrhG dürfen technische Schutzmaßnahmen (z. B. Kopierschutz) ohne Zustimmung des Rechteinhabers nicht umgangen werden – auch nicht zur Anfertigung einer – an sich zulässigen – Privatkopie. Keine Umgehung in diesem Sinne ist die Herstellung einer analogen Kopie einer digitalen, geschützten Vorlage.

In Österreich trat die Umsetzung der EG-Urheberrechtsrichtlinie bereits am 1. Juli 2003 in Kraft.

In den letzten Jahren gerät das aktuelle Urheberrecht, entgegen allen legislativen Trends, immer mehr in die Kritik der Öffentlichkeit und einiger Rechts- und Wirtschaftswissenschaftler.

Kritisiert wird vor allem, dass sich das Urheberrecht immer mehr zu einem „Rechteverwerterrecht“ gestaltet, ohne Berücksichtigung der Allgemeinheit und der im Wandel befindlichen Internet- und Informationsgesellschaft. Das Urheberrecht sei in seiner aktuellen Form ein Relikt des letzten Jahrhunderts, das in der Zeit von interaktiven Internetanwendungen und Breitbandanbindung eine ganze Generation kriminalisiere und in keiner Weise mehr seiner ursprünglichen Intention gerecht werde. Anstatt kreatives Schaffen zu beflügeln und zu fördern, schränke das aktuelle Urheberrecht die Entwicklung von Kultur und Fortschritt enorm ein.

Kritik geht auch von Trägerinnen und Trägern traditioneller Kulturen aus, in denen Konzeptionen von Werk und individuellem Autor, wie sie dem Urheberrecht zugrunde liegen, keine Entsprechung haben. Traditionelle kulturelle Ausdrucksweisen ("traditional cultural expressions") unterstehen in ihren lokalen Gesellschaften eigenen herkömmlichen Regelungen, sind aus urheberrechtlicher Sicht jedoch gemeinfrei und können entgegen dem Willen vieler ihrer Träger uneingeschränkt genutzt werden. An der Organisation der Vereinten Nationen für Erziehung, Wissenschaft und Kultur (UNESCO) und Weltorganisation für geistiges Eigentum (WIPO) gab es seit den 1960er Jahren immer wieder Initiativen zur Schaffung von Schutzregelungen für traditionelle kulturelle Ausdrucksweisen. Seit dem Jahr 2000 tagt an der Weltorganisation für geistiges Eigentum (WIPO) in dieser Frage erneut ein zwischenstaatliches Komitee.

Die der Kritik folgenden Forderungen reichen von leichten Lösungen wie einer Verkürzung der Mindestschutzdauer, über eine Ausweitung und Reetablierung der (privaten) Schranken des Urheberrechtes bis hin zur Einführung einer Kulturflatrate, welche die exklusiven Besitzansprüche an immaterielle Güter verneint. So fordert z. B. in Deutschland die Piratenpartei eine Beschränkung des Urheberrechts, um auf die neuen Gegebenheiten zu reagieren. Andere Parteien wie Bündnis 90/Die Grünen fordern "eine Kulturflatrate, um Kulturschaffende zu entlohnen". Das bürgerliche Lager aus Union und FDP hält jedoch immer noch strikt an der Vorstellung eines möglichst starken Urheberrechts fest.

Seit einiger Zeit entwickelt sich Open Source zu einer wirtschaftlichen Ergänzung zu traditionell kommerziellen Produkten im Softwarebereich. Besonders steht bei Open Source Kooperation im Vordergrund, da die Lizenzen einen einfachen Codeaustausch erlauben. Freie Lizenzen wie die GPL bauen auf dem Urheberrecht auf, welches sie so nutzen, dass Urheber immer genannt werden müssen und aufbauende Werke nur unter den gleichen Bedingungen verbreitet werden dürfen. Creative-Commons-Lizenzen ermöglichen es, den Nutzern von Texten, Fotos, Musikstücken etc. unter Berücksichtigung der Urheberrechte in verschiedenen Ländern der Erde Copyleft-Bedingungen festzulegen. Diese Möglichkeit wird in den letzten Jahren immer häufiger genutzt. So nutzt z. B. auch Wikipedia das Copyleft-Prinzip.

Einige Tatort-Autoren kritisieren als Mitglieder des Verband Deutscher Drehbuchautoren bei den Grünen, Piraten, Linken und der „Netzgemeinde“ besonders die „demagogische Gleichsetzung von frei und kostenfrei“. Die „Banalität von Rechtsverstößen“ würde zum „Freiheitsakt“ erhoben werden. Sie möchten gerne „klar machen, dass die nachhaltige Produktion qualitativ hochwertiger Kunst und Kultur nicht amateurhaft, also wie Wikipedia organisiert werden kann. Immerhin leben hunderttausende Menschen von kreativer Arbeit und helfen mit ihren (konkurrenzfähigen) Werken, die ideelle und materielle Zukunft einer postindustriellen Bundesrepublik auch international zu sichern“. Denn: „Filme, Musikproduktionen, web- und Werbekampagnen, Architektur- und Designprodukte werden überhaupt erst realisiert, wenn die künstlerischen Ideen der Urheber mit Kapital und Vermarktungsknowhow zusammenkommen.“ Dass das Internet und Anbieter wie Google kriminelle Plattformen wie „kino.to, megaupload, the Pirate Bay“ ermöglichen, solle auf jeden Fall verhindert werden. Dabei sehen sie einen „historischen Kompromiss zwischen Urhebern und Usern“ bevor.

Rockmusiker Sven Regener kritisiert in einem Interview mit dem Bayerischen Rundfunk die Piratenpartei sehr scharf: „Eine Gesellschaft, die so mit ihren Künstlern umgeht, ist nichts wert.“. Die seiner Ansicht nach verlogene Doppelmoral der Piraten stellt er an einem ihm bekannten Beispiel heraus: „Der örtliche Chef hier von der Piratenpartei, der hat eine Firma, die machen Apps fürs iPhone – das ist ein geschlossenes System, das ist hundert Prozent Copyright, mit Anwälten, mit allem Drum und Dran.“

Übersicht

Antike

Renaissance

Neuzeit


</doc>
<doc id="5310" url="https://de.wikipedia.org/wiki?curid=5310" title="Unified Messaging">
Unified Messaging

Unified Messaging bezeichnet ein Verfahren, in jeglicher Form eingehende und zu sendende Nachrichten (z. B. Voice-Mail, E-Mail, Fax, SMS, MMS etc.) in eine einheitliche Form zu bringen und dem Nutzer über verschiedenste Access-Clients Zugang auf diese zu gewähren (Festnetz- oder Mobiltelefon, E-Mail-Client).

Es handelt sich um Dienste, die von den jeweiligen Providern proprietär konfiguriert werden.

Die eingehenden Nachrichten und Informationen werden, soweit nötig, im ersten Schritt digitalisiert. Hierzu kommt für gedruckte Informationen die Texterkennung zum Einsatz. Faxe werden meist direkt von einem Fax-Server entgegengenommen und digital abgelegt bzw. mit OCR verarbeitet und als Text weitergeleitet (Fax2mail). Sprachmitteilungen (beispielsweise vom Anrufbeantworter) werden in Klangdateien und Textdateien gewandelt.

Die so aufbereiteten Nachrichten werden dem entsprechenden Mitarbeiter in einheitlicher Form (= "Unified Messaging"). übermittelt.

Derzeit ist es etwa üblich, dem Mitarbeiter die verschiedenen Dokumente per elektronischer Post zukommen zu lassen oder aber in einer Datenbank vorgangsbezogen abzulegen und dem Mitarbeiter über eine einheitliche Benutzeroberfläche Zugriff auf die Dokumente zu gewähren.

Zudem ist es möglich, dass man sich die Nachricht vom Rechner oder vom Audiotex-System per Fernzugriff vorlesen lässt.

Ziel ist es, alle Nachrichten, Daten und Dokumente zu bestimmten Vorgängen an einem Ort in technisch halbwegs einheitlicher Form nachvollziehbar und jederzeit abrufbar bereitzuhalten.

Grundsätzlich zu unterscheiden sind UM-Lösungen für den privaten Gebrauch und den kommerziellen Einsatz. Letztere lassen sich zur Verbesserung des Workflow u. a. in CRM- und ERP-Systeme (z. B. SAP) sowie Exchange, Lotus Notes, GroupWise o. ä. integrieren.

Die Tiefe und die Güte der Information der Provider über die angebotenen Leistungsmerkmale lässt viele Wünsche offen.

Konzepte zur Präsentation von Information auf dem Medium der momentanen Wahl unabhängig von der gewählten Form des Erstellers und unabhängig vom Kanal der Übertragung sind hilfreich. Der tatsächliche reale Nutzen erwächst aus zwei Merkmalen zur persönlichen Administrierung der Messages:
mit folgenden Nebenbedingungen möglichst nach Kriterien ISO/IEC 15408:
Beispiele sind bekannt, in denen Unified Messaging neue Schwachstellen schafft.

Eine technische Standardisierung der Angebote ist bisher nicht erkennbar.

Bei bundesweit angebotenen UM-Diensten ist durch die verschärfte Durchsetzung des Ortsnetzbezugs von geografischen Teilnehmerrufnummern in Deutschland durch die Bundesnetzagentur die Verwendung von geografischen Festnetznummern als virtuelle UM-Voicebox/Faxnummern nur noch bei vorhandenem Wohnsitz oder Firmensitz der UM-Kunden im jeweiligen Ortsnetz erlaubt. Da für die weitere Verwendung von geografischen Rufnummern gemäß den Vergaberichtlinien der BNetzA eine aufwändige Adressdatenüberprüfung der Kunden notwendig wäre und kostenintensiv Rufnummernblöcke in allen 5200 Ortsnetzen angefordert werden müssten, verzichten UM-Anbieter auf die weitere Verwendung geografischer Rufnummern und vergeben stattdessen nichtgeografische nationale 032-Teilnehmerrufnummern an ihre Kunden.



</doc>
<doc id="5311" url="https://de.wikipedia.org/wiki?curid=5311" title="Unicode">
Unicode

Unicode (Aussprachen: am. Englisch [], brit. Englisch []; dt. []) ist ein internationaler Standard, in dem langfristig für jedes Sinn tragende Schriftzeichen oder Textelement aller bekannten Schriftkulturen und Zeichensysteme ein digitaler Code festgelegt wird. Ziel ist es, die Verwendung unterschiedlicher und inkompatibler Kodierungen in verschiedenen Ländern oder Kulturkreisen zu beseitigen. Unicode wird ständig um Zeichen weiterer Schriftsysteme ergänzt.

ISO 10646 ist die von ISO verwendete, praktisch bedeutungsgleiche Bezeichnung des Unicode-Zeichensatzes; er wird dort als Universal Coded Character Set (UCS) bezeichnet.

Herkömmliche Computer-Zeichensätze umfassen nur einen begrenzten Vorrat an Zeichen, bei westlichen Zeichenkodierungen liegt diese Grenze meistens bei 128 (7 Bit) Codepositionen – wie bei dem sehr bekannten ASCII-Standard – oder 256 (8 Bit) Positionen, wie z. B. bei ISO 8859-1 (auch als "Latin-1" bekannt) oder EBCDIC. Davon sind nach Abzug der Steuerzeichen 95 Elemente bei ASCII und 191 Elemente bei den 8-Bit ISO-Zeichensätzen als Schrift- und Sonderzeichen darstellbar. Diese Zeichenkodierungen erlauben die gleichzeitige Darstellung nur weniger Sprachen im selben Text, wenn man sich nicht damit behilft, in einem Text verschiedene Schriften mit unterschiedlichen Zeichensätzen zu verwenden. Das behinderte den internationalen Datenaustausch in den 1980er und 1990er Jahren erheblich.

ISO 2022 war ein erster Versuch, mehrere Sprachen mit nur einer Zeichenkodierung darstellen zu können. Die Kodierung benutzt Escape-Sequenzen, um zwischen verschiedenen Zeichensätzen (z. B. zwischen Latin-1 und Latin-2) wechseln zu können. Das System setzte sich jedoch nur in Ostasien durch.

Joseph D. Becker von Xerox schrieb 1988 den ersten Entwurf für einen universalen Zeichensatz. Dieser 16-Bit-Zeichensatz sollte nach den ursprünglichen Plänen lediglich die Zeichen moderner Sprachen kodieren:

Im Oktober 1991 wurde nach mehrjähriger Entwicklungszeit die Version 1.0.0 des Unicode-Standards veröffentlicht, die damals nur die europäischen, nahöstlichen und indischen Schriften kodierte. Erst acht Monate später, nachdem die Han-Vereinheitlichung abgeschlossen war, erschien Version 1.0.1, die erstmals ostasiatische Zeichen kodierte. Mit der Veröffentlichung von Unicode 2.0 im Juli 1996 wurde der Standard von ursprünglich 65.536 auf die heutigen 1.114.112 Codepunkte, von codice_1 bis codice_2 erweitert.

Beginnend mit der Version 7.0 im Juni 2014 sollen neue Hauptversionen regelmäßig jedes Jahr im Juni veröffentlicht werden. Die Veröffentlichung zieht sich teilweise über einen längeren Zeitraum hin, sodass zum Veröffentlichungszeitpunkt zunächst nur die Zeichentabellen und einzelne Teile der Spezifikation fertig sind, während die endgültige Veröffentlichung der Hauptspezifikation erst einige Zeit später erfolgt.

Das Unicode-Konsortium stellt mehrere Dokumente zur Unterstützung von Unicode bereit. Neben dem eigentlichen Zeichensatz sind dies des Weiteren auch andere Dokumente, die zwar nicht zwingend notwendig, aber dennoch hilfreich zur Interpretation des Unicode-Standards sind.

Im Gegensatz zu früheren Zeichenkodierungen, die meist nur ein bestimmtes Schriftsystem kodierten, ist es das Ziel von Unicode, alle in Gebrauch befindlichen Schriftsysteme und Zeichen zu kodieren. Der Zeichenumfang ist dazu in 17 Ebenen () gegliedert, welche jeweils 2 = 65.536 Zeichen umfassen. Sechs dieser Ebenen werden bereits verwendet, die restlichen sind für spätere Nutzung reserviert:


Innerhalb dieser Ebenen werden zusammengehörende Zeichen in Blöcken (engl. ) zusammengefasst. Meist behandelt ein Unicodeblock ein Schriftsystem, aus historischen Gründen hat sich allerdings ein gewisses Maß an Fragmentierung eingestellt. Oft wurden später noch Zeichen hinzugefügt und in anderen Blöcken als Ergänzung untergebracht.

Jedes im Unicode-Standard codierte elementare Zeichen ist einem Codepunkt (engl. ) zugeordnet. Diese werden üblicherweise hexadezimal (mindestens vierstellig, d. h. ggf. mit führenden Nullen) und mit einem vorangestellten codice_3 dargestellt, z. B. codice_4 für das ß.

Der gesamte vom Unicode-Standard beschriebene Bereich umfasst 1.114.112 Codepunkte (U+0000 … U+10FFFF, 17 Ebenen zu je 2, d. h. 65536 Zeichen). Davon lässt der Standard jedoch für einige Bereiche die Verwendung zur Zeichenkodierung nicht zu:
Somit stehen für die Zeichencodierung insgesamt 1.111.998 Codepunkte zur Verfügung. Die Anzahl der tatsächlich zugewiesenen Codepunkte ist jedoch deutlich niedriger; eine Übersicht, wie viele Codepunkte in den verschiedenen Versionen jeweils zugewiesen sind und wofür sie genutzt werden, bieten die Tabellen D-2 und D-3 im Anhang D des Unicode-Standards.

Spezielle Bereiche sind für private Nutzung reserviert, d. h. in diesen werden niemals Codepunkte für in Unicode standardisierte Zeichen zugewiesen. Diese können für "privat" definierte Zeichen verwendet werden, die zwischen den Erzeugern und Verwendern der Texte, die sie enthalten, individuell abgesprochen sein müssen. Diese Bereiche sind:

Es haben sich für verschiedene Anwendungen spezielle Konventionen entwickelt, die speziell für den PUA-Bereich der BMP Zeichenbelegungen vorgeben. Zum einen finden sich hier häufig "precomposed characters" aus Grundzeichen und diakritischen Zeichen, da in vielen (speziell älteren) Software-Anwendungen nicht davon ausgegangen werden kann, dass solche Zeichen gemäß den Unicode-Regeln bei Eingabe als Folge aus Grundzeichen und diakritischem Zeichen korrekt dargestellt werden. Zum anderen finden sich Zeichen, die nicht den Regeln für eine Aufnahme in Unicode entsprechen, oder deren Beantragung zur Aufnahme in Unicode aus anderen Gründen erfolglos war oder unterblieb. So findet sich in vielen Fonts auf der Position U+F000 ein Hersteller-Logo (Logos werden in Unicode prinzipiell nicht codiert).

Quellen für PUA-Zeichen sind z. B.:

Neben dem eigentlichen Zeichensatz sind auch eine Reihe von Zeichenkodierungen definiert, die den Unicode-Zeichensatz implementieren und die benutzt werden können, um den vollen Zugriff auf alle Unicode-Zeichen zu haben. Sie werden Unicode Transformation Format (kurz "UTF") genannt; am weitesten verbreitet sind zum einen UTF-16, das sich als interne Zeichendarstellung einiger Betriebssysteme (Windows, OS X) und Softwareentwicklungs-Frameworks (Java, .NET) etabliert hat, zum anderen UTF-8, das ebenfalls in Betriebssystemen (GNU/Linux, Unix) sowie in verschiedenen Internetdiensten (E-Mail, WWW) eine große Rolle spielt. Basierend auf dem proprietären EBCDIC-Format von IBM-Großrechnern ist die UTF-EBCDIC-Kodierung definiert. Punycode dient dazu, Domainnamen mit Nicht-ASCII-Zeichen zu kodieren. Mit dem Standard Compression Scheme for Unicode existiert ein Kodierungsformat, das die Texte gleichzeitig komprimiert. Weitere Formate zur Kodierung von Unicode-Zeichen sind u. a. CESU-8 und GB 18030.

Viele Zeichen, die im Unicode-Standard enthalten sind, sind sogenannte Kompatibilitätszeichen, die aus Unicode-Sicht bereits mit anderen in Unicode kodierten Zeichen bzw. Zeichensequenzen dargestellt werden können, so z. B. die deutschen Umlaute, die theoretisch mit einer Sequenz aus dem Basisbuchstaben und einem kombinierenden Trema (horizontaler Doppelpunkt) dargestellt werden können. Bei der Unicode-Normalisierung werden die Kompatibilitätszeichen automatisch durch die in Unicode vorgesehenen Sequenzen ersetzt. Dies erleichtert die Verarbeitung von Unicode-Texten erheblich, da so nur eine mögliche Kombination für ein bestimmtes Zeichen steht, und nicht mehrere verschiedene.

Für viele Schriftsysteme sind die Zeichen in Unicode nicht in einer Reihenfolge codiert, die einer bei den Anwendern dieses Schriftsystems üblichen Sortierung entspricht. Deshalb kann bei einer Sortierung z. B. in einer Datenbankanwendung üblicherweise nicht die Reihenfolge der Codepunkte verwendet werden. Außerdem sind die Sortierungen in vielen Schriftsystemen von komplexen, kontextabhängigen Regelungen geprägt. Hier definiert der Unicode Collation Algorithm, wie Zeichenfolgen innerhalb eines bestimmten Schriftsystems oder auch schriftsystemübergreifend sortiert werden können.

In vielen Fällen ist jedoch die tatsächlich anzuwendende Reihenfolge von anderen Faktoren (z. B. der verwendeten Sprache) abhängig (z. B. sortiert „ä“ im Deutschen anwendungsabhängig wie „ae“ oder „a“, im Schwedischen jedoch hinter „z“ und „å“), sodass der Unicode-Sortierungsalgorithmus dann anzuwenden ist, wenn die Sortierung nicht von spezielleren Rahmenbedingungen bestimmt wird.

Das gemeinnützige Unicode-Konsortium wurde 1991 gegründet und ist für den Industriestandard Unicode verantwortlich. Von der ISO (Internationale Organisation für Normung) wird in Zusammenarbeit mit IEC die internationale Norm ISO 10646 herausgegeben. Beide Institutionen arbeiten eng zusammen. Seit 1993 sind Unicode und ISO 10646 bezüglich der Zeichenkodierung praktisch identisch. Während ISO 10646 lediglich die eigentliche Zeichenkodierung festlegt, gehört zum Unicode ein umfassendes Regelwerk, das unter anderem für alle Zeichen weitere zur konkreten Anwendung wichtige Eigenschaften (sogenannte Properties) eindeutig festlegt wie Sortierreihenfolge, Leserichtung und Regeln für das Kombinieren von Zeichen.

Seit einiger Zeit entspricht der Codeumfang von ISO 10646 exakt dem von Unicode, da auch dort der Codebereich auf 17 Ebenen, darstellbar mit 21 Bit, beschränkt wurde.

Gegenüber anderen Normen gibt es bei Unicode die Besonderheit, dass einmal kodierte Zeichen niemals wieder entfernt werden, um die Langlebigkeit digitaler Daten zu gewährleisten. Sollte sich die Normierung eines Zeichens nachträglich als Fehler erweisen, wird allenfalls von seiner Verwendung abgeraten. Daher bedarf die Aufnahme eines Zeichens in den Standard einer äußerst sorgfältigen Prüfung, die sich über Jahre hinziehen kann.

Im Unicode werden lediglich „abstrakte Zeichen“ (englisch: "characters") kodiert, nicht dagegen die grafische Darstellung (Glyphen) dieser Zeichen, die von Schriftart zu Schriftart extrem unterschiedlich ausfallen kann, beim lateinischen Alphabet etwa in Form der Antiqua, Fraktur, der irischen Schrift oder der verschiedenen Handschriften. Für Glyphenvarianten, deren Normierung als sinnvoll und notwendig nachgewiesen wird, sind dabei allerdings vorsorglich 256 „Variation Selectors“ reserviert, die ggf. dem eigentlichen Code nachgestellt werden können. In vielen Schriftsystemen können Zeichen außerdem je nach Position unterschiedliche Formen annehmen oder Ligaturen bilden. Von Ausnahmen abgesehen (z. B. Arabisch) werden solche Varianten ebenfalls nicht in den Unicode-Standard übernommen, sondern es wird eine sogenannte Smartfont-Technik wie OpenType vorausgesetzt, die die Formen angemessen ersetzen kann.

Andererseits werden identische Glyphen, wenn sie verschiedene Bedeutungen haben, auch mehrfach kodiert, etwa die Glyphen А, В, Е, K, М, Н, О, Р, Т und Х, die – mit zum Teil unterschiedlicher Bedeutung – sowohl im lateinischen als auch im griechischen und kyrillischen Alphabet vorkommen.

In Grenzfällen wird hart um die Entscheidung gerungen, ob es sich um Glyphenvarianten oder tatsächlich unterschiedliche, einer eigenen Kodierung würdige Zeichen (Grapheme) handelt. Beispielsweise sind nicht wenige Fachleute der Meinung, man könne das phönizische Alphabet als Glyphenvarianten des hebräischen Alphabets betrachten, da der gesamte Zeichenvorrat des Phönizischen dort eindeutige Entsprechungen hat und auch beide Sprachen sehr eng miteinander verwandt sind. Letztlich durchgesetzt hat sich allerdings schließlich die Auffassung, es handele sich um separate Zeichensysteme, in der Unicode-Terminologie „scripts“ genannt.

Anders verhält es sich bei CJK (Chinesisch, Japanisch und Koreanisch): Hier haben sich in den letzten Jahrhunderten die Formen vieler gleichbedeutender Schriftzeichen auseinanderentwickelt. Dennoch teilen sich die sprachspezifischen Glyphen dieselben Codes im Unicode (mit Ausnahme einiger Zeichen aus Kompatibilitätsgründen). In der Praxis werden hier überwiegend sprachspezifische Schriftarten verwendet, wodurch der Platzbedarf der Schriften zusammen hoch ist. Die einheitliche Kodierung der CJK-Schriftzeichen (Han Unification) war eine der wichtigsten und umfangreichsten Vorarbeiten für die Entwicklung von Unicode. Besonders in Japan ist sie durchaus umstritten.

Als der Grundstein für Unicode gelegt wurde, musste berücksichtigt werden, dass bereits eine Vielzahl unterschiedlicher Kodierungen im Einsatz waren. Unicode-basierte Systeme sollten herkömmlich kodierte Daten mit geringem Aufwand handhaben können. Dazu wurde für die unteren 256 Zeichen die weit verbreitete ISO-8859-1-Kodierung (Latin1) ebenso wie die Kodierungsarten verschiedener nationaler Normen beibehalten, z. B. TIS-620 für Thailändisch (fast identisch mit ISO 8859-11) oder ISCII für indische Schriften, die in der ursprünglichen Reihenfolge lediglich in höhere Bereiche verschoben wurden.

Jedes Zeichen maßgeblicher überkommener Kodierungen wurde in den Standard übernommen, auch wenn es den normalerweise angelegten Maßstäben nicht gerecht wird. Hierbei handelt es sich zu einem großen Teil um Zeichen, die aus zwei oder mehr Zeichen zusammengesetzt sind, wie Buchstaben mit diakritischen Zeichen. Im übrigen verfügt auch heute noch ein großer Teil der Software nicht über die Möglichkeit, Zeichen mit Diakritika ordentlich zusammenzusetzen. Die exakte Festlegung von äquivalenten Kodierungen ist Teil des zum Unicode gehörenden umfangreichen Regelwerks.

Darüber hinaus gibt es viele Unicode-Zeichen, denen keine Glyphe zugeordnet ist und die trotzdem als „characters“ behandelt werden. So sind neben Steuerzeichen wie dem Tabulatorzeichen (U+0009), dem Zeilenvorschub (U+000A) usw. allein 19 verschiedene Zeichen explizit als Leerzeichen definiert, sogar solche ohne Breite, die u. a. für Sprachen wie Thai, die ohne Wortzwischenraum geschrieben werden, als Worttrenner eingesetzt werden. Für bidirektionalen Text, z. B. Arabisch mit Lateinisch, sind sieben Formatierungszeichen kodiert. Darüber hinaus gibt es weitere unsichtbare Zeichen, die nur unter bestimmten Umständen ausgewertet werden sollen, etwa der Combining Grapheme Joiner.

Unter Windows (ab Windows 2000) kann in einigen Programmen (genauer in RichEdit-Feldern) der Code dezimal als +<dezimales Unicode> (bei eingeschaltetem Num-Lock) auf dem numerischen Tastaturfeld eingegeben werden. Dabei ist jedoch zu beachten, dass Zeichennummern kleiner als 1000 um eine führende Null zu ergänzen sind (z. B. + für Codepoint 234 [ê]). Diese Maßnahme ist notwendig, da die (immer noch in Windows verfügbare) Eingabemethode +<ein- bis dreistellige dezimale Zeichennummer ohne führende Null> bereits in MS-DOS-Zeiten genutzt wurde, um die Zeichen der Codepage 850 (vor allem bei früheren MS-DOS-Versionen auch Codepage 437) einzugeben.

Eine weitere Eingabemethode setzt voraus, dass in der Registrierungsdatenbank im Schlüssel codice_5 ein Eintrag (Wert) vom Typ REG_SZ ("Zeichenfolge") namens codice_6 existiert und ihm der Wert (das Datum) codice_7 zugewiesen ist. Nach dem Editieren der Registry müssen Benutzer/-innen sich unter Windows 8.1, Windows 8, Windows 7 und Vista vom Windows-Benutzerkonto ab- und wieder anmelden, bei früheren Windows-Versionen ist ein Neustart des Rechners notwendig, damit die Änderung in der Registry wirksam werden.
Danach können Unicode-Zeichen wie folgt eingegeben werden: Zuerst die (linke) Alt-Taste drücken und halten, dann auf dem Ziffernblock die Plus-Taste drücken und wieder loslassen und anschließend den hexadezimalen Code des Zeichens eingeben, wobei für Ziffern der Ziffernblock verwendet werden muss. Abschließend die Alt-Taste wieder loslassen.

Zwar funktioniert diese Eingabemethode prinzipiell in jedem Eingabefeld jedes Windows-Programms, allerdings kann es vorkommen, dass Schnellzugriffstasten für Menüfunktionen die Eingabe hexadezimaler Codepunkte verhindern: Will man beispielsweise den Buchstaben Ø (U+00D8) eingeben, so führt die Kombination codice_8 in vielen Programmen dazu, dass stattdessen das Menü "Datei" geöffnet wird.

Ein weiterer Nachteil besteht darin, dass Windows hier die explizite Angabe der (intern in Windows verwendeten) UTF-16-Codierung statt der Unicode-Kodierung selbst verlangt und daher nur die Eingabe vierstelliger Codewerte zulässt; für Zeichen, die oberhalb der BMP liegen und über Codepunkte mit fünf- oder sechsstelliger Hexadezimaldarstellung verfügen, sind stattdessen sogenannte Surrogate Pairs zu verwenden, bei denen ein fünf- oder sechsstelliger Codepunkt auf zwei je vierstellige Ersatzcodepunkte abgebildet wird. So ist etwa der Violinschlüssel 𝄞 (U+1D11E) als UTF-16-Wertpaar U+D834 U+DD1E einzugeben; eine direkte Eingabe fünf- oder sechsstelliger Codepunkte ist hier also nicht möglich.

Bei Apple OS X kann der Unicode-Wert bei gedrückter -Taste eingegeben werden, nachdem die Unicode-Hex-Tastenbelegung in den Systemeinstellungen unter „Sprache und Text“, „Eingabequellen“ aktiviert wurde.

Unter Microsoft Office (ab Office XP) kann Unicode auch hexadezimal eingegeben werden, indem im Dokument <Unicode> oder U+<Unicode> eingetippt wird und anschließend die Tastenkombination +, bzw. in Dialogfeldern +, gedrückt wird. Diese Tastenkombination kann auch benutzt werden, um den Code des vor dem Cursor stehenden Zeichens anzuzeigen. Eine alternative Möglichkeit, welche auch in älteren Versionen funktioniert, ist, mit „Einfügen“ – „Sonderzeichen“ eine Tabelle mit Unicode-Zeichen aufzurufen, darin mit dem Cursor ein gewünschtes auszusuchen und in den Text einzufügen. Das Programm ermöglicht auch, für häufiger benötigte Zeichen Makros festzulegen, die dann mit einer Tastenkombination abgerufen werden können.

GTK+, Qt und alle darauf basierenden Programme und Umgebungen (wie beispielsweise die Desktop-Umgebung Gnome) unterstützen die Eingabe über die Kombination + bzw. in neueren Versionen + bzw. ++. Nach dem Drücken der Tasten erscheint ein unterstrichenes kleines u. Danach kann der Unicode in hexadezimaler Form eingegeben werden und wird auch unterstrichen, damit man erkennen kann, was zum Unicode gehört. Nach einem Druck der Leer- oder Eingabetaste erscheint dann das entsprechende Zeichen. Auf der Desktop-Umgebung KDE wird diese Funktionalität nicht unterstützt.

Im Texteditor Vim können Unicode-Zeichen mit +, gefolgt von der Taste und dem Unicode in hexadezimaler Form, eingegeben werden.

Seit Windows NT 4.0 ist das Programm "charmap.exe", genannt "Zeichentabelle", in Windows integriert. Mit diesem Programm ist über eine grafische Benutzeroberfläche möglich, Unicode-Zeichen einzufügen. Außerdem bietet es ein Eingabefeld für den Hexadezimalcode.

Unter macOS steht unter "Einfügen" → "Sonderzeichen" ebenfalls eine systemweite Zeichenpalette bereit.

Die freien Programme gucharmap (für Windows und Linux/Unix) und kcharselect (für Linux/UNIX) stellen den Unicode-Zeichensatz auf dem Bildschirm dar und bieten zusätzliche Informationen zu den einzelnen Zeichen.

HTML und XML unterstützen Unicode mit Zeichencodes, die unabhängig vom eingestellten Zeichensatz das Unicode-Zeichen darstellen. Die Notation lautet codice_9 für dezimale Notation bzw. codice_10 für hexadezimale Notation, wobei das 0000 die Unicode-Nummer des Zeichens darstellt. Für bestimmte Zeichen sind auch benannte Zeichen (engl. "named entities") definiert, so z. B. stellt codice_11 das ä dar, das gilt allerdings nur für HTML; XML und das davon abgeleitete XHTML definieren benannte Notationen nur für die Zeichen, die bei normalem Gebrauch als Teile der Auszeichnungssprache interpretiert würden, also codice_12 als codice_13, codice_14 als codice_15, codice_16 als codice_17 und codice_18 als codice_19.

Unicode wird vor allem aus den Reihen der Wissenschaftler und in ostasiatischen Ländern kritisiert. Einer der Kritikpunkte ist hierbei die Han-Vereinheitlichung; aus ostasiatischer Sicht werden bei diesem Vorgehen Schriftzeichen verschiedener nicht verwandter Sprachen vereinigt. Unter anderem wird kritisiert, dass antike Texte in Unicode aufgrund dieser Vereinheitlichung ähnlicher CJK-Schriftzeichen nicht originalgetreu wiedergegeben werden können. Aufgrund dessen wurden in Japan zahlreiche Alternativen zu Unicode entwickelt, wie etwa der Mojikyō-Standard.

Die Kodierung der thailändischen Schrift wird teilweise kritisiert, weil sie anders als alle anderen Schriftsysteme in Unicode nicht auf logischer, sondern visueller Reihenfolge basiert, was unter anderem die Sortierung thailändischer Wörter erheblich erschwert. Die Unicode-Kodierung basiert auf dem thailändischen Standard TIS-620, der ebenfalls die visuelle Reihenfolge verwendet. Umgekehrt wird die Kodierung der anderen indischen Schriften manchmal als „zu kompliziert“ bezeichnet, vor allem von Vertretern der Tamil-Schrift. Das Modell separater Konsonanten- und Vokalzeichen, welches Unicode vom indischen Standard ISCII übernommen hat, wird von jenen abgelehnt, die separate Codepunkte für alle möglichen Konsonant-Vokal-Verbindungen bevorzugen. Die Regierung der Volksrepublik China machte einen ähnlichen Vorschlag, die tibetische Schrift als Silbenfolgen anstelle als einzelne Konsonanten und Vokale zu kodieren.

Auch gab es Versuche von Unternehmen, Symbole in Unicode zu platzieren, die für deren Produkte stehen sollen.

Ob das entsprechende Unicode-Zeichen auch tatsächlich am Bildschirm erscheint, hängt davon ab, ob die verwendete Schriftart eine Glyphe für das gewünschte Zeichen (also eine Grafik für die gewünschte Zeichennummer) enthält. Oftmals, z. B. unter Windows, wird, falls die verwendete Schrift ein Zeichen nicht enthält, nach Möglichkeit ein Zeichen aus einer anderen Schrift eingefügt. 

Mittlerweile hat der Coderaum von Unicode/ISO einen Umfang angenommen (mehr als 100.000 Schriftzeichen), der sich nicht mehr vollständig in einer Schriftdatei unterbringen lässt. Die heute gängigsten Schriftdateiformate, TrueType und OpenType, können maximal 65.536 Glyphen enthalten. Unicode/ISO-Konformität einer Schrift bedeutet also nicht, dass der komplette Zeichensatz enthalten ist, sondern lediglich, dass die darin enthaltenen Zeichen normgerecht kodiert sind. In der Publikation »decodeunicode«, die alle Zeichen vorstellt, werden insgesamt 66 Fonts genannt, aus denen die Zeichentabellen zusammengesetzt sind.


Eine Ersatzschriftart dient der Ersatzdarstellung für Zeichen, für die kein Font mit korrekter Darstellung zur Verfügung steht.

Hier gibt z.B. folgende Fonts:





</doc>
<doc id="5312" url="https://de.wikipedia.org/wiki?curid=5312" title="Unfallchirurgie">
Unfallchirurgie

Die Unfallchirurgie befasst sich mit dem physischen Trauma und wird häufig mit Traumatologie gleichgesetzt. Im engeren bzw. eigentlichen Sinne ist Unfallchirurgie jedoch ein Teil der über die chirurgischen Aspekte hinausgehenden Traumatologie (auch "Unfallheilkunde" oder "Unfallmedizin" genannt). Die Unfallchirurgie und die Wiederherstellungschirurgie bilden die Grundlagen der Unfallheilkunde bzw. Unfallmedizin.

Ein Wendepunkt der neuzeitlichen Chirurgiegeschichte war der Krimkrieg. Alle europäischen Staaten erkannten fundamentale Defizite in der Versorgung von Verwundungen. Systematisch und mit beispielloser Konsequenz verbesserte Preußen seinen Sanitätsdienst. Davon „profitierten“ vor allem die Hygiene und die Chirurgie. Die Kaiser-Wilhelms-Akademie für das militärärztliche Bildungswesen, die Charité und die Chirurgischen Lehrstühle Berlin begründeten die Spitzenstellung der deutschen Chirurgie. Nach dem Ersten Weltkrieg war die Orthopädie die (nichtoperative) Krüppelfürsorge. Das erste Unfallkrankenhaus entstand 1925 auf Betreiben von Lorenz Böhler in Wien. Die schrecklichen Gesichtsverletzungen ließen die Kieferchirurgie entstehen. Ihre Pioniere waren der Chirurg Christian Bruhn in Düsseldorf und der Zahnarzt Georg Axhausen an der Charité. Von einer „Unfallchirurgie“ sprach niemand – nicht einmal im Zweiten Weltkrieg, als sich die führenden Sanitätsoffiziere und Beratenden Chirurgen 1942 in Krasnodar auf die Einführung des Marknagels von Gerhard Küntscher verständigten. Zwar wurde die "Deutsche Gesellschaft für Unfallheilkunde, Versicherungs- und Versorgungsmedizin" schon 1922 unter Erwin Payr in Leipzig gegründet; aber die Unfallchirurgie ist ein ziviles Kind der prosperierenden 1950er Jahre. Angesichts der vielen Arbeits- und Wegeunfälle drängte die Gesetzliche Unfallversicherung in Deutschland auf eine "optimale" (nicht „zweckmäßige“) Versorgung ihrer Versicherten. Die Unfallkrankenhäuser wurden ausgebaut, die ersten Lehrstühle für Unfallchirurgie eingerichtet. Zum chirurgischen Teilgebiet wurde das Fach erst 1968. München bekam in den 1980er Jahren zu den zwei chirurgischen Lehrstühlen (Großhadern und TU) einen dritten, unfallchirurgischen Lehrstuhl in der Nußbaumstraße. Die in Deutschland erste Abteilung für Orthopädische, Unfall und Handchirurgie entstand 1995 in Mecklenburg. Wie die Kinderchirurgie musste die Unfallchirurgie erkennen, dass die Emanzipation von der Chirurgie nicht nur Vorteile bringt. Die allseits für notwendig und wünschenswert gehaltene Zusammenlegung mit der Orthopädie hat daran nichts geändert und die Ausbildung des Nachwuchses noch schwerer gemacht. Auf Anregung von Dietmar Wolter – selbst ein beachtlicher Maler – hat Johannes Grützke dem Fach ein unvergleichliches „Denkmal“ voller Hintersinn und Humor gemalt. Das Wandbild schmückt die ganze Stirnwand des Hörsaals vom BG Klinikum Hamburg.

Das zentrale Betätigungsfeld der Unfallchirurgen sind Verletzungen des Bewegungsapparats, d. h. von Knochen, Muskulatur, Bändern und Sehnen. Knochenbrüche bestimmen den Alltag und gelungene Osteosynthesen sind sichtbare Erfolge; aber „Unfallchirurgie“ umfasst viel mehr als solche „Platten- und Schraubenchirurgie“. Versorgt werden müssen auch Verletzungen der großen Körperhöhlen (Thorax, Abdomen) und der Haut. Exponiert und gefährdet ist die Hand. Als Fortsetzung des Gehirns begründet sie mit der Handchirurgie ein eigenes Fachgebiet – das älter ist als die Unfallchirurgie. In den Rahmen der Unfallchirurgie gehören auch die Kindertraumatologie, die Sporttraumatologie und die Rehabilitation in der Unfallchirurgie. Auch ein großes kommunales Haus kann kaum Kapazitäten für Verbrennungen, Strahlenschäden und Plastische Chirurgie vorhalten. Das bleibt Spezialabteilungen von Unfallkrankenhäusern vorbehalten. 

Wer Unfallchirurgie betreiben will, muss zunächst die Bedingungen der Ärztlichen Weiterbildung erfüllen und Facharzt für Chirurgie oder Facharzt für Orthopädie und Unfallchirurgie sein. Will er von den Landesverbänden der Deutschen Gesetzlichen Unfallversicherung als Durchgangsarzt zugelassen werden, muss er den Schwerpunkt Unfallchirurgie oder die Zusatzbezeichnung Spezielle Unfallchirurgie erlangt haben.

In einem Krankenhaus eine unfallchirurgische Abteilung einzurichten und zu betreiben, ist an bauliche, apparative und personelle Bedingungen geknüpft. Sie werden von der Deutschen Gesetzlichen Unfallversicherung im Rahmen des Siebten Buchs Sozialgesetzbuch vorgegeben und von den sechs Landesverbänden der Berufsgenossenschaften überwacht. Unabdingbare Voraussetzungen sind Hubschrauberlandeplatz, Schockraum (mit Spiral-CT), Blutbank und Intensivmedizin.
In Deutschland, in der Schweiz und in Österreich sind die Berufsgenossenschaften, die Schweizerische Unfallversicherungsanstalt und die Allgemeine Unfallversicherungsanstalt die wichtigsten Partner (und Träger) der Unfallchirurgie. Weder die Gesetzliche Krankenversicherung noch die Private Krankenversicherung wären imstande, vergleichbare Standards in der Akutversorgung und Rehabilitation von Verletzten zu gewährleisten.


Als Grundlage für die Zulassung zur stationären Behandlung von Berufsunfällen sind Verletzungen seit Jahrzehnten definiert. Zur Behandlung von Schwerverletzten nach § 6 der Bestimmungen des Reichsversicherungsamtes vom 19. Juni 1936 (sog. Verletzungsartenverfahren) waren immer nur wenige Krankenhäuser zugelassen. Eine Zulassungsbedingung war die Versorgung einer Mindestzahl von schwerverletzten Patienten (Polytrauma). Bei der letzten Aktualisierung des Verletzungsartenverzeichnisses im Januar 2013 wurden drei Behandlungsebenen eingeführt:

Auf Betreiben der Deutschen Gesellschaft für Unfallchirurgie haben sich 520 Kliniken in 40 zertifizierten „Traumanetzwerken“ zusammengeschlossen. Zu den überregionalen Traumazentren auf der höchsten Versorgungsstufe gehören alle BG-Kliniken. Ihre Ärzte nutzen die Telemedizin für ihre Patienten und Ärzte anderer Kliniken.

Im 19. Jahrhundert waren Carl Thiem, Carl Hansmann, Fritz König, Albin Lambotte und Robert Danis Wegbereiter der Unfallchirurgie. Zu Unrecht vergessen ist Henning Brütt, der im Hafenkrankenhaus „die beste Unfallchirurgie Hamburgs“ betrieb. 1957 kam Gerhard Küntscher für acht Jahre als Ärztlicher Direktor an das Haus. Zeitgenossen Küntschers waren Heinrich Bürkle de la Camp und Werner Wachsmuth. Als Schüler von Erwin Payr brachte Endre Hedri die Unfallchirurgie nach Ungarn. In der Deutschen Demokratischen Republik war Helmtraut Arzinger-Jonasch eine maßgebliche Vertreterin der Traumatologie und der Verbrennungsmedizin. Maurice Edmond Müller war der Patron der Arbeitsgemeinschaft für Osteosynthesefragen. Der erste Ordinarius für Unfallchirurgie in Deutschland war Harald Tscherne. Er machte (wie Hans Georg Borst und Rudolf Pichlmayr) die Medizinische Hochschule Hannover weltberühmt. Nur wenige Ärzte vereinbarten „hot and cold orthopaedics“ – Unfallchirurgie und Orthopädie – in Werdegang und Beruf, z. B. Robert Merle d’Aubigné, Peter Friedrich Matzen, Robert Judet und Alfred Nikolaus Witt. Über 1000 Betten und 168 nachgeordnete Ärzte „herrschte“ der Orthopäde Gawriil Abramowitsch Ilisarow in Sibirien. Dietmar Wolter machte die Jahrhundertgestalt in Deutschland bekannt und widmete sich der Fortentwicklung des genialen Ringfixateurs. Ins 21. Jahrhundert hinein wirken Siegfried Weller, Leonhard Schweiberer, Otmar Trentz, Norbert Haas und der politisch ambitionierte Axel Ekkernkamp.

Dem komplexen Feld der Querschnittlähmungen verschrieben sich Neurologen und Chirurgen. Im englischen Exil errichtete der deutsche, aus Breslau stammende jüdische Neurologe und Neurochirurg Ludwig Guttmann die erste Rehabilitationsklinik für Querschnittgelähmte. Er begründete die Stoke Mandeville Games als Vorläufer der Paralympischen Spiele. In den 1960er Jahren beriet er die Berufsgenossenschaftliche Unfallklinik Murnau beim Aufbau von Deutschlands erstem Zentrum für Querschnittverletzte. Seinem Vorbild folgten Friedrich-Wilhelm Meinecke und Gerhard Exner in Hamburg, Hans Jürgen Gerner in Bad Wildungen und Heidelberg und Jürgen Harms in Langensteinbach. Bei Robert Judet in Paris inaugurierte Raymond Roy-Camille 1963 die transpedikuläre Schraube, die neue Wege in der Behandlung von Verletzungen (und Verformungen) der Wirbelsäule eröffnete.

Es gibt zahlreiche Fachzeitschriften für Knochen- und Gelenkchirurgie.




</doc>
<doc id="5313" url="https://de.wikipedia.org/wiki?curid=5313" title="Ufoglaube">
Ufoglaube

Als Ufoglaube oder UFO-Religion bezeichnet man die neureligiösen Ansichten von unterschiedlich stark organisierten Gruppen und ihren Propheten, die nach eigenem Verständnis in Kontakt mit Außerirdischen stehen. Das UFO-Phänomen erregte erst seit 1947 (UFO-Sichtung von Kenneth Arnold) größere Aufmerksamkeit. In der Folgezeit entstanden verschiedene religiöse Gruppen, die außerirdische Wesen in ihren Glauben integriert haben. Inhalte des religiösen Gedankenguts dieser Gruppen lassen sich teilweise bis zu den okkulten, theosophischen und esoterischen Traditionen des 19. Jahrhunderts zurückverfolgen. Oft werden Elemente christlicher Religion wie zum Beispiel die Figur Jesu Christi in das eigene Weltbild integriert.

Von religiösem Ufoglauben ist der grenzwissenschaftliche Forschungszweig der Ufologie zu unterscheiden.

Die außerirdischen Wesen werden meist als spirituell und moralisch weit überlegen angesehen. Ernst Benz hat nachgewiesen, dass dies ein typischer Zug der Auseinandersetzung mit der Möglichkeit außerirdischer Welten in den vorangehenden Jahrhunderten (seit der kopernikanischen Wende) ist. Verbreitet sind auch Heilsversprechen, die die Rettung der Gläubigen am Jüngsten Tag durch UFOs beinhalten.

Während in manchen Glaubenssystemen ein stark millenaristischer Zug hervortritt (z. B. Ashtar Command), sind andere Gruppen (z. B. Scientology) nicht apokalyptisch. Typisch ist ein deutlicher Technologiebezug, wobei sowohl technologiekritische (der technische Fortschritt der Menschheit sei eine Bedrohung für die Erde) als auch begeisterte Stimmen (der technische Fortschritt diene dem Segen der Menschheit) vorkommen. Manche Bewegungen weisen darin Ähnlichkeiten zu melanesischen Cargo-Kulten auf. Zentral ist meistens der Versuch, Technik und Spiritualität bzw. Wissenschaft und Religion zu versöhnen. Ähnliche Ideen werden in der (nichtreligiösen, parawissenschaftlichen) Prä-Astronautik vertreten.

Die weltweit größte UFO-Religion ist der Raelismus. Weniger verbreitet sind beispielsweise Aetherius Society, Ashtar Command, Unarius, Chen Tao, Eduard Albert Meiers Gruppe "F.I.G.U." oder Uriellas Orden Fiat Lux. Auch Scientologen haben mit Xenu Elemente eines Ufoglaubens in ihrer Lehre. In die Schlagzeilen kamen 1997 Heaven’s Gate durch einen kollektiven Selbstmord und 1998 das Trainingszentrum zur Freisetzung der Atmaenergie durch einen möglicherweise verhinderten kollektiven Selbstmord.




</doc>
<doc id="5314" url="https://de.wikipedia.org/wiki?curid=5314" title="Ute Vogt">
Ute Vogt

Ute Vogt (* 3. Oktober 1964 in Heidelberg) ist eine deutsche Politikerin (SPD). Sie war von 2002 bis 2005 parlamentarische Staatssekretärin beim Bundesminister des Innern. Von 1999 bis 2009 war sie Landesvorsitzende der SPD Baden-Württemberg. Sie ist Mitglied des Deutschen Bundestages. Seit Dezember 2013 ist sie eine der sieben stellvertretenden Vorsitzenden der SPD-Bundestagsfraktion.

Die in Wiesloch aufgewachsene Ute Vogt absolvierte nach dem Abitur am Wieslocher Wirtschaftsgymnasium ein Studium der Rechtswissenschaft in Heidelberg, welches sie mit beiden juristischen Staatsexamina beendete. Zudem studierte Vogt an der Deutschen Hochschule für Verwaltungswissenschaften Speyer. Seitdem ist sie als Rechtsanwältin tätig.

Nachdem sich Ute Vogt schon seit 1983 bei den Jusos engagierte, wurde sie 1984 auch Mitglied der SPD. Von 1991 bis 1994 war sie Sprecherin der Jusos in Baden-Württemberg. Seit 1993 gehörte sie dort dem SPD-Landesvorstand an. 1995 wurde sie zur stellvertretenden Landesvorsitzenden gewählt. Von 1999 bis 2009 war sie SPD-Landesvorsitzende in Baden-Württemberg.

2001 trat sie bei der Landtagswahl als Spitzenkandidatin der SPD gegen Ministerpräsident Erwin Teufel an. Ihr erklärtes Ziel, einen Regierungswechsel zu erreichen, verfehlte sie zwar, jedoch gelang ihr insoweit ein Achtungserfolg, als die SPD sich um 8,2 Prozentpunkte verbesserte und mit 33,3 Prozent der Stimmen ihr bestes Landesergebnis seit 1972 erzielen konnte.

Seit 2001 ist sie zudem Mitglied im Präsidium der SPD. Von 2003 an war Ute Vogt stellvertretende Bundesvorsitzende der SPD. Für den Bundesparteitag im Oktober 2007 wurde sie nicht mehr für das Amt einer Vize-Vorsitzenden nominiert. Auf dem Bundesparteitag am 26. Oktober 2007 in Hamburg wurde sie als Mitglied des Parteivorstands im ersten Wahlgang mit 345 Stimmen gewählt.

Sie war erneut Spitzenkandidatin der SPD für die Landtagswahl in Baden-Württemberg am 26. März 2006, bei der die SPD 25,2 % der Stimmen erreichte und damit auf das Niveau vor Vogts erster Spitzenkandidatur 2001 zurückfiel.

Vogt übernahm nach der Landtagswahl den Vorsitz ihrer Fraktion und damit das Amt der Oppositionsführerin im Landtag. Immer wieder musste sie sich gegen innerparteiliche Kritiker behaupten. Obwohl dem Landesparteitag im September 2007 in Fellbach eine heftige Diskussion um sie vorausging, wurde sie nach einer kämpferischen Rede mit 77,4 Prozent als Landesvorsitzende bestätigt.

Zum Anfang des Jahres 2008 erklärte sie ihren Rücktritt vom Fraktionsvorsitz. Zu ihrem Nachfolger wurde am 10. Januar 2008 Claus Schmiedel gewählt.

Als Reaktion auf das schlechte Ergebnis der Südwest-SPD bei der Bundestagswahl 2009 sowie ihr eigenes Abschneiden im Bundestagswahlkreis Stuttgart I erklärte sie am 30. September 2009, nicht mehr für den Landesvorsitz der SPD kandidieren zu wollen. Auf dem Landesparteitag der SPD in Karlsruhe wurde Nils Schmid am 27. November 2009 zu ihrem Nachfolger gewählt.

Von 1989 bis 1994 war Vogt Stadträtin in Wiesloch. Von 1994 bis 2005 war sie Mitglied des Deutschen Bundestages. Hier war sie von 2000 bis 2002 Vorsitzende des Innenausschusses. Ute Vogt ist 1998 als direkt gewählte Abgeordnete des Wahlkreises Pforzheim und sonst stets über die Landesliste Baden-Württemberg in den Deutschen Bundestag eingezogen. Nach der Bundestagswahl 2002 wurde sie am 22. Oktober 2002 als parlamentarische Staatssekretärin beim Bundesminister des Innern in die von Bundeskanzler Gerhard Schröder geführte Bundesregierung (Kabinett Schröder II) berufen. Am 22. November 2005 schied sie aus dem Amt.

2006 bis 2009 war Ute Vogt Mitglied des Landtages von Baden-Württemberg. Bei der Landtagswahl 2006 erreichte sie im Wahlkreis Bretten 31,2 % der Stimmen und zog über ein Zweitmandat in den Landtag ein, nachdem das Direktmandat an den CDU-Kandidaten Joachim Kößler mit 43,8 % der Stimmen ging. Trotz innerparteilicher Kritik aufgrund des landesweit schlechten SPD-Wahlergebnisses wurde sie am 29. März 2006 mit 24 zu 14 Stimmen zur Vorsitzenden der SPD-Landtagsfraktion gewählt und löste damit Wolfgang Drexler ab. Am 9. Oktober 2007 gab sie bekannt, für diese Position bei den Neuwahlen zum Fraktionsvorstand im Januar 2008 nicht mehr zur Verfügung zu stehen; zu ihrem Nachfolger wurde Claus Schmiedel gewählt. Ute Vogt legte ihr Landtagsmandat am 30. September 2009 nach ihrer erneuten Wahl in den Bundestag nieder.

Seit 2009 gehört Vogt erneut dem Bundestag an. Bei der Bundestagswahl 2009 erreichte sie im Bundestagswahlkreis Stuttgart I mit 18,0 % der Erststimmen nur den dritten Platz nach dem Grünen-Kandidaten Cem Özdemir, der auf 29,9 % der Erststimmen kam. Sie kehrte über die Landesliste in den Bundestag zurück, während Stefan Kaufmann von der CDU mit 34,4 % das Direktmandat erreichte.

Von 2010 bis 2013 war sie Obfrau (Sprecherin) der SPD-Bundestagsfraktion im 1. Untersuchungsausschuss (Gorleben).

Im 18. Bundestag ist Vogt ordentliches Mitglied im Gemeinsamen Ausschuss sowie ordentliches Mitglied der Kommission Lagerung hoch radioaktiver Abfallstoffe (Endlagerkommission) gemäß § 3 Standortauswahlgesetz. Zudem ist sie stellvertretendes Mitglied im Ausschuss für Ernährung und Landwirtschaft, im Ausschuss für Umwelt, Naturschutz, Bau und Reaktorsicherheit, im Ausschuss für Recht und Verbraucherschutz sowie im Vermittlungsausschuss. Seit dem 19. Dezember 2013 ist sie stellvertretende Vorsitzende der SPD-Bundestagsfraktion und in dieser Funktion zuständig für die Bereiche Umwelt, Naturschutz und Reaktorsicherheit sowie Ernährung und Landwirtschaft.

Ute Vogt ist stellv. Vorsitzende des Stiftungsvorstandes der Rotkreuz Stiftung Stuttgart, Mitglied des Kuratoriums der Stiftung Energie & Klimaschutz Baden-Württemberg, Schirmherrin des Vereins MEHRSi und von Blick nach Rechts, Vizepräsidentin der DLRG, Ehrenpräsidentin der THW-Jugend (Baden-Württemberg) sowie Vorstandsmitglied der Deutschen Umweltstiftung. Daneben ist sie Mitglied im Auto Club Europa, AK Asyl Stuttgart, Bergwacht Schwarzwald, BiWu - Beschäftigungsinitiative Wiesloch und Umgebung e. V., Eurosolar, Europa-Union Deutschland e. V. (Landesverband BW - EUD BW), Fördermitglied im BUND, Förderverein für das Kulturhaus Osterfeld e.V., Gegen Vergessen – Für Demokratie, Gewerkschaft ver.di, Kinderzentrum Maulbronn, Lilith e. V., Männergesangverein Arlinger, Mieterverein Stuttgart, Naturfreunde, SGK Baden-Württemberg, Tierschutzverein Stuttgart und weiteren örtlichen Vereinen. Ute Vogt ist katholisch.




</doc>
<doc id="5315" url="https://de.wikipedia.org/wiki?curid=5315" title="Unterwasserarchäologie">
Unterwasserarchäologie

Die Unterwasserarchäologie beschäftigt sich mit allen archäologischen Quellen, die unter Wasserbedeckung erhalten geblieben sind. Diese Quellen finden sich auf dem Grund von Meeren, Seen und Flüssen, Brunnen, Höhlen und Cenoten, aber auch in Mooren, wobei hier meist der Begriff "Feuchtbodenarchäologie" verwendet wird.

Die Bedeutung der Unterwasserarchäologie ist regional unterschiedlich und hängt ursächlich von geografischen und geophysikalischen Umständen ab. In Skandinavien beispielsweise findet seit dem Ende der letzten Eiszeit, der Würm-Eiszeit, die vor rund 12.000 Jahren endete und damit das Holozän einleitete, durch den Wegfall des enormen Gewichtes des Eispanzers eine sogenannte postglaziale Landhebung statt, die bis heute andauert. Das führte dazu, dass die ehemaligen Flachwassergebiete, normalerweise die potentiell interessantesten Gebiete für Unterwasserarchäologen, heute an Land liegen, sodass die Unterwasserarchäologie bis auf wenige Ausnahmen kaum eine Rolle spielt, um beispielsweise die Erforschung der Geschichte der Wikinger voranzutreiben.

Für andere ufernahe Bereiche liegt die Bedeutung dieser Sparte in gegensätzlichen postglazialen Entwicklungen, die durch den Meerwasseranstieg (gebietsweise bis zu 120 m) ehemals trocken gelegene Wohnplätze, besonders aus mesolithischer Zeit, unter die heutigen Wasserlinien verbracht haben, wie beispielsweise Doggerland zwischen den Britischen Inseln und Dänemark oder den Bereich vor der Küste Nordirlands, der erst vor kurzer Zeit in einem aufwändigen Projekt genauer untersucht wurde (Joint Irish Bathymetric Survey Project (JIBS)).

Die besondere Bedeutung dieses noch nicht sehr alten Zweiges der Archäologie ergibt sich aus der besonders guten Konservierung organischer Materialien wie Holz und Textilien, aber auch Speiseresten und anderen organischen Abfällen unter Luftabschluss in Süßwasser. Die Aussage- und Informationskraft der geborgenen Gegenstände ist deshalb sehr hoch und betrifft naturgemäß auch alle Relikte maritimer Art, so dass sich seefahrtsgeschichtliche Einblicke ergeben. Als Sonderzweig hat sich die Schiffsarchäologie entwickelt.

In warmen und salzreichen Meeren, wie dem Mittelmeer oder dem Roten Meer, werden durch den Schiffsbohrwurm Holz und andere organische Materialien stark angegriffen, von älteren Wracks sind selten mehr als 5 % des Schiffsrumpfes erhalten. Der besondere Informationsgehalt liegt aber in den erhaltenen Materialien, aus denen die Schiffsladung bestand, wie Kupferbarren, Keramiken oder aus römischer Zeit auch Architekturelemente oder Marmor-Sarkophage, deren Herkunft manchmal bestimmt werden kann und die dadurch Auskunft über Handelsbeziehungen, Reiserouten, Stand der Technik und auch gesellschaftliche Strukturen geben können. Interessante Beispiele dafür sind das Schiff von Ulu Burun oder das Wrack von Yassi Ada.

Eine Erschließung der Unterwasserquellen ist nicht nur durch Tauchgänge möglich, sondern auch durch das Trockenlegen des Gewässergrunds mit Hilfe von Spundwänden. Auf diese Weise wurde zum Beispiel ein Teil des alten Hafens von Haithabu trockengelegt und ein altes dänisches Handelsschiff geborgen. Andere Beispiele für Unterwasserarchäologie sind die Bergungen der Bremer Hansekogge von 1380, der Vasa, der Mary Rose sowie von Schiffen vor der türkischen Küste vor Uluburun, Bozburun und Küçüven Burnu bei Marmaris und am Kap Gelidonya (bei Antalya), Kyrenia auf Zypern, der Wrackfund vor Alicante, und die Ausgrabungen in La Tène (späte Eisenzeit). Auch im Roten Meer (Sadana Island) werden Schiffswracks untersucht. Dänische Beispiele sind die bei der Insel Dejrø (zwischen Fünen und Ærø) gelegenen Wohnplätze Møllegabet I + II (mesolithische Wohnplätze) und Tybrind Vig (eine neolithische Siedlung). Das sind zwei von etwa 70 bisher registrierten Plätzen.

Antike Schiffswracks und unter Wasser befindliche Ruinen werden durch die "UNESCO 2001 Konvention zum Schutz von Unterwasserkulturerbe" geschützt.

Bis zum Aufkommen des Tauchsports wurden in der Regel Gelegenheitsfunde von Berufstauchern (z. B. Schwammtauchern) gemacht, die sie selbst nutzten (z. B. Amphoren) oder als Schrott (Kanonen etc.) bzw. an Museen oder Sammler verkauften. Der wissenschaftliche Wert der Funde war, da die Stücke aus dem Zusammenhang gerissen wurden und die Bergung auch oft sehr rabiat erfolgte, zumeist nicht mehr vorhanden. Mit dem Aufkommen des Tauchsports kam es vielerorts, besonders im Mittelmeer, zu regelrechten Plünderungen von Wracks im Flachwasser. Ab den 1950er Jahren begann sich langsam die Erkenntnis durchzusetzen, dass mit modernen Schwimmtauchgeräten unter Wasser ernsthafte archäologische Arbeit möglich war, teilweise sogar besser als an Land, da eine Fundstelle, wenn sie einmal mit Sediment bedeckt ist, nur geringen Störungen ausgesetzt ist und insbesondere organisches Material (z. B. Holz) durch den Luftabschluss besser konserviert wird. Um nicht binnen kürzester Zeit zu zerfallen, müssen gerade diese Materialien nach der Bergung im Wasser verwahrt werden und anschließend einem aufwändigen Konservierungsprozess unterzogen werden. In vielen Fällen ist daher ein vorrangiges Ziel einer Ausgrabung insbesondere an Schiffswracks nicht die vollständige Bergung, sondern die Vermessung und Sicherung von Einzelfundstücken, während man das eigentliche Wrack an Ort und Stelle belässt, um später weitere Untersuchungen am Fundort zu ermöglichen.

In letzter Zeit werden auch Tauchroboter in der Unterwasserarchäologie eingesetzt.

Der französische Unterwasserarchäologe Franck Goddio plant akribisch, setzt Magnetometer ein und betont, dass aus Kostengründen der Zufall auf ein Minimum reduziert werden müsse.

Møllegabet I liegt 2,3 Meter unter dem Meeresspiegel. Der Platz wurde von der Ertebølle-Kultur zwischen 4500 und 4000 v. Chr. benutzt. Als erstes wurde ein 60 Meter langer und 0,75 Meter hoher Køkkenmøddinger (Muschelhaufen) mit Resten von Feuerstellen gefunden. Als Werkzeuge wurden Kern- und Scheibenbeile, Klingen und Pfeilspitzen benutzt, aber auch gepickte und polierte Steinbeile und ein Sandsteinteller kamen an die Oberfläche. Angelhaken, Messer, Spitzen und Hacken waren aus Knochen, Horn oder Geweih gefertigt. Elch, Reh, Rothirsch und Wildschwein wurden gegessen, aber auch die Felllieferanten Marder, Otter, Wildkatze und Wolf wurden erlegt, wie auch Robben und Enten, Fischadler, Kormorane, Möwenarten, Schwäne und Säger. Unter den Fischen sind Aal, Dorsch und Plattfische belegt. Austern, Eicheln, Muscheln, Nüsse und Schnecken wurden gesammelt.

Møllegabet II liegt 4,5 Meter unter dem Meeresspiegel und nur etwa 25 Meter entfernt vom ersten Platz. Hier wurden beim Tauchen Holzobjekte entdeckt und zwischen 1987 und 1993 wurden systematische Tauchgänge und Grabungen durchgeführt. Dieser Platz wurde zwischen 5500 und 5000 v. Chr. datiert. Zusätzlich zu den Nahrungsresten am Platz I wurden hier auch Reste von Igel und Hund gefunden. Die Holzreste von Aalstechern, Beilschäftungen, Fischspeeren, Jagdbögen und Reusen waren aussagekräftig. Eine vermutliche Bootsbestattung und ein 3 Meter × 5 Meter großer Fußboden aus einer Rindenschicht auf einem Gitterwerk aus Ästen gehörten zu den spektakulären Funden.

Auch Unterwasserausgrabungen sind in Deutschland genehmigungspflichtig und dürfen nur durch speziell ausgebildete Fachleute durchgeführt werden. Die "Kommission für Unterwasserarchäologie beim Verband der Landesarchäologen in der Bundesrepublik Deutschland" führt entsprechende Ausbildungen durch.





</doc>
<doc id="5319" url="https://de.wikipedia.org/wiki?curid=5319" title="Unternehmen">
Unternehmen

Ein Unternehmen ist eine wirtschaftlich selbständige Organisationseinheit, die mit Hilfe von Planungs- und Entscheidungsinstrumenten Markt- und Kapitalrisiken eingeht und sich zur Verfolgung des Unternehmenszweckes und der Unternehmensziele eines oder mehrerer Betriebe bedient.

Privatrechtlich organisierte Unternehmen werden auch als "Privatwirtschaft" bezeichnet. Dagegen gehören zum Aggregat des öffentlichen Sektors öffentliche Unternehmen, Körperschaften des Privatrechts und Anstalten des öffentlichen Rechts (Kommunalunternehmen); sie stellen eine Mischform dar und unterliegen – wie auch Vereine – meist dem Kostendeckungsprinzip.

In Deutschland gibt es rund drei Millionen umsatzsteuerpflichtige Unternehmen, etwa zwei Drittel davon sind Einzelunternehmen.

Im alltäglichen Sprachgebrauch sind die Begriffe Unternehmen, Gesellschaft, Firma und Betrieb dem "Duden" zufolge Synonyme. In den rechts- und wirtschaftsbezogenen Fachsprachen werden die Begriffe jedoch unterschieden; danach kann z. B. ein Betrieb eine systemunabhängige Wirtschaftseinheit zur Fremdbedarfsdeckung sein, während ein Unternehmen einen oder mehrere Betriebe besitzen kann und eine Gesellschaft lediglich eine Rechtsform oder eine Organisationsform ist. Mit Firma wiederum ist fachsprachlich der Name eines Unternehmens gemeint.

In den verschiedenen Rechtsgebieten wird der Begriff Unternehmen unterschiedlich verstanden. Im Arbeitsrecht wird ein Unternehmen als organisatorische Einheit verstanden, mit welcher der Unternehmer seine wirtschaftlichen oder ideellen Zwecke verfolgt. Ein Unternehmen kann aus einem oder mehreren Betrieben bestehen. Im ersten Fall bezeichnet Unternehmen dann die wirtschaftliche Komponente, während der Betrieb für den arbeitstechnischen Ablauf steht. Im Umsatzsteuerrecht ist Unternehmer, wer eine gewerbliche oder berufliche Tätigkeit selbständig ausübt. Die umsatzsteuerliche Unternehmensdefinition ist daher wesentlich weiter gefasst als im Arbeitsrecht.

Das Unternehmen ist das Erkenntnisobjekt der Betriebswirtschaftslehre, die jedoch keine einheitliche Definition bereithält. Fritz Schmidt stellte 1924 den wirtschaftenden Betrieb in den Blickpunkt ihrer Beobachtung. „Die Betriebe, ob Haushalte oder Unternehmungen, sind die Wirkungszentren und Formungselemente der Wirtschaft.“ Martin Lohmann sieht den Betrieb als produktiven Teil eines Unternehmens, er ordnet ihn dem Unternehmen unter. Ein großer Teil weiterer Wissenschaftler ordnete jedoch den Unternehmensbegriff dem Betriebsbegriff unter, so etwa mit Einschränkungen Erich Gutenberg, der im Betrieb die Kombination von Produktionsfaktoren sah. Diese Unterordnung wird uneingeschränkt übernommen von Heinrich Nicklisch, Eugen Schmalenbach oder Konrad Mellerowicz. Zwischen Gutenberg und Mellerowicz war ein Methodenstreit entstanden, als Gutenberg ab April 1955 Betrieb und Unternehmung gleichsetzte. Im Kern ging es um die Frage, ob der Betrieb oder die Unternehmung Forschungs- und Erkenntnisobjekt sei.

Für Gutenberg besitzt ein Unternehmen drei konstitutive Merkmale:

Gutenberg und Erich Kosiol sehen den Unternehmensbegriff als mit der Marktwirtschaft begriffsnotwendig verbunden an. Gemeinhin wird als „Unternehmen“ eine aus Sachen (Maschinen, Warenlager), Rechten (Forderungen, Patente) und sonstigen Beziehungen (z. B. Goodwill, Organisation, Verbindlichkeiten) bestehende Organisationseinheit bezeichnet, deren Träger eine natürliche oder juristische Person bzw. eine Personenvereinigung ist. Damit wird einem Betrieb die technisch-leistungsorientierte Sphäre, einem Unternehmen die finanziell-rechtliche Sphäre zugeordnet.

Im weiteren Sinne können Betriebe auch dann als Unternehmen gelten, wenn sie nicht nach Gewinn streben. Derartige Unternehmen werden als Non-Profit-Unternehmen bezeichnet. Man unterscheidet wirtschaftliche Non-Profit-Unternehmen, soziokulturelle Non-Profit-Unternehmen, politische Non-Profit-Unternehmen und karitative Non-Profit-Unternehmen. Da die Abkehr vom erwerbswirtschaftlichen Prinzip nicht mit den meisten traditionellen Unternehmensbegriffen vereinbar ist, spricht man in diesem Kontext häufig von Organisationen statt Unternehmen („Non-Profit-Organisationen“).

Im Rechtswesen gibt es keinen einheitlichen Unternehmensbegriff, denn entscheidend ist der jeweilige Normzweck eines Gesetzes. Es hängt mithin vom jeweiligen Regelungsziel eines Gesetzes ab, welcher Inhalt dem Unternehmensbegriff zukommt. Der Unternehmensbegriff spielt insbesondere im Aktien- und GmbH-Recht eine große Rolle etwa im Konzernrecht der „verbundenen Unternehmen“. Allerdings bieten die Gesetze keine Legaldefinition an, sondern setzen den Unternehmensbegriff als bekannt voraus. Dem Gesetzgeber erschien eine Definition in den § AktG ff. AktG zu kompliziert und war zudem mit großen praktischen Schwierigkeiten verbunden. Die Rechtsprechung sieht aber den Unternehmensbegriff nicht als auf juristische Personen institutionalisiert an, sondern dehnt ihn auf natürliche Personen aus, wenn diese eine beherrschende Stellung in einer abhängigen Gesellschaft wahrnehmen und die Besorgnis vorliegt, der „Aktionär könnte um ihretwillen seinen Einfluss zum Nachteil der Gesellschaft geltend machen.“ Man unterscheidet in der Rechtswissenschaft den funktionellen, institutionellen und teleologischen Unternehmensbegriff.

Außerdem wird zwischen dem wirtschaftsrechtlichen und handelsrechtlichen Unternehmensbegriff unterschieden. Während dem wirtschaftsrechtlichen ein institutioneller Gehalt zukommt, ist der zivil- und handelsrechtliche Unternehmensbegriff als Gebilde mit gegenständlicher Qualität gekennzeichnet. Der konzernrechtliche und der in § 1 GWB enthaltene gehen von einem subjektiven Unternehmensbegriff aus, der Rechtssubjekte mit unternehmerischer Betätigung erfasst. Im Juni 2000 wurden die Begriffe Verbraucher und Unternehmer ins BGB eingefügt. Seither versteht Abs. 1 BGB unter dem Unternehmer eine natürliche oder juristische Person oder eine rechtsfähige Personengesellschaft, die bei Abschluss eines Rechtsgeschäfts in Ausübung ihrer gewerblichen oder selbständigen beruflichen Tätigkeit handelt. Aus Sicht des BGB kommt es mithin darauf an, dass die gewerbliche oder selbständige Tätigkeit bei Rechtsgeschäften im Vordergrund steht. Bei seiner Definition hat sich hierbei der Gesetzgeber vom Unternehmerbegriff des § 2 Abs. 1 Umsatzsteuergesetz leiten lassen, wonach Unternehmer ist, wer eine gewerbliche oder berufliche Tätigkeit selbständig ausübt. Nach dieser Bestimmung umfasst das Unternehmen die gesamte gewerbliche oder berufliche Tätigkeit des Unternehmers. Gewerblich oder beruflich ist umsatzsteuerrechtlich jede nachhaltige Tätigkeit zur Erzielung von Einnahmen, auch wenn die Absicht, Gewinn zu erzielen, fehlt. Dadurch werden auch öffentlich-rechtliche Unternehmen wie Anstalten des öffentlichen Rechts erfasst, die keine Gewinnerzielungsabsicht verfolgen. Damit steht die rechtliche Einordnung der Non-Profit-Unternehmen im Einklang mit der betriebswirtschaftlichen Sichtweise.

Für die Zwecke des Unternehmenskaufs wird das Unternehmen definiert als Gesamtheit von materiellen und immateriellen Rechtsgütern, in der Menschen mit dem Ziel zusammenwirken, planmäßig und dauerhaft wirtschaftliche Aktivitäten zu entfalten.

Unternehmen lassen sich international nach folgenden Kriterien unterscheiden:

Die Rechtsform eines Unternehmens umfasst alle gesetzlichen Regelungen, durch die es zur rechtlich fassbaren Einheit wird. Rechtsformen lassen sich anhand einiger zentraler Merkmale unterscheiden. Dazu zählt unter anderem das gesetzlich vorgeschriebene Haftungskapital bei Unternehmensgründung, die Haftungsregelung oder auch die steuerliche Behandlung. Die Rechtsform bestimmt auch, ob ein Unternehmen eine eigene Rechtspersönlichkeit besitzt oder ob ihre Teilhaber als natürliche Personen handeln. Wechselt ein Unternehmen die Rechtsform, spricht man von einer Umwandlung. Dabei sind beispielsweise Auflagen der Kreditgeber, Änderungen der Zahl der Gesellschafter, Änderungen der Steuergesetze oder Änderungen der Unternehmensgröße (durch Wachstum oder Schrumpfung) bedeutende Einflussfaktoren.

Man unterscheidet grundsätzlich privatrechtliche und öffentlich-rechtliche Rechtsformen, die alle gesetzlich festgelegt sind (einige privatrechtliche Mischformen ausgenommen).



Man unterscheidet die Phasen eines Unternehmens nach Gründungsphase (Pionierphase), Umsatzphase (Markterschließung, Diversifikation, Akquisition, Kooperation und Restrukturierung) sowie Auflösungsphase. Wöhe/Döring sprechen im Rahmen der genetischen Gliederung der Betriebswirtschaftslehre von der Gründungs-, Betriebs- und Liquidationsphase. Außerdem kann zwischen der Gründungs-, Entwicklungs- und Krisenphase unterschieden werden.

Sofern ein Unternehmen seine Zahlungsverpflichtungen gegenüber Gläubigern nicht mehr erfüllen kann, kommt es meist zur Insolvenz. Unternehmen können auch in die Auflösungsphase übergehen, ohne die Umsatzphase jemals erreicht zu haben. Ein Unternehmen hat grundsätzlich keinen definierten Endzeitpunkt. Es ist kein einmaliges, sondern ein ständiges Vorhaben mit zunächst unbegrenzten Ressourcen. Dadurch unterscheidet es sich von einem Projekt, das jedoch Bestandteil eines Unternehmens sein kann.

Ein Großunternehmen erreicht im Durchschnitt ein Alter von 75 Jahren. Dennoch gibt es zahlreiche Unternehmen, die mehrere hundert Jahre alt sind. Einige der weltweit ältesten Familienunternehmen schlossen sich in der Association les Hénokiens zusammen. Das älteste deutsche Unternehmen ist die Glasmanufaktur von Poschinger, gegründet im Jahr 1568. Der im Jahr 578 gegründete japanische Tempel- und Burgbauer Kongō Gumi galt bis zu seiner Liquidation im Januar 2006 als ältestes noch bestehendes Familienunternehmen der Welt. Nach einer Untersuchung der Bank of Korea von 2008 gab es 5586 Unternehmen in 41 Ländern die älter als 200 Jahre waren, davon 3146 in Japan, 837 in Deutschland, 222 in den Niederlanden und 196 in Frankreich.

In einer groben Gliederung nach dem Wirtschaftszweig (auch Branchengliederung) ist zwischen Sachleistungsunternehmen und Dienstleistungsunternehmen zu differenzieren. Sachleistungsunternehmen sind insbesondere Industrie- und Handwerksunternehmen. Bei solchen Unternehmen wird nach der Erzeugungsstufe weiter unterschieden: Gewinnungsunternehmen sind Unternehmen, die sogenannte Urprodukte hervorbringen. Dazu zählen mineralische, pflanzliche oder tierische Naturvorkommen sowie die Naturkräfte. Urprodukte stellen den Ausgangspunkt des Wirtschaftsprozesses dar. In derartigen Bereichen tätige Unternehmen werden unter dem Sammelbegriff Primärsektor zusammengefasst. Veredelungs- oder Aufbereitungsunternehmen (sekundärer Sektor) produzieren aus den gewonnenen Urprodukten schließlich Zwischenprodukte, die wiederum von Verarbeitungsunternehmen (ebenfalls sekundärer Sektor) in Endprodukte transformiert werden.

Dienstleistungsunternehmen zählen zum Tertiärsektor und produzieren keine physischen Güter, sondern erbringen immaterielle Dienste. Eine solche Dienstleistung ist nicht lagerbar, kaum übertragbar und benötigt einen externen Faktor (Integration des externen Faktors „Kunde“). Ihre Erzeugung und der Verbrauch fallen meist zeitlich zusammen. Man spricht vom Uno-actu-Prinzip.

Überblick der Unternehmen in Deutschland nach Zuordnung zum Wirtschaftszweig im Jahr 2012:
Es gibt keinen weltweit anerkannten einheitlichen Bewertungsmaßstab für die Größe eines Unternehmens. Das deutsche Handelsgesetzbuch (HGB) unterscheidet in HGB nach kleinen Kapitalgesellschaften, mittelgroßen Kapitalgesellschaften und großen Kapitalgesellschaften. Dabei sind Bilanzsumme, Umsatzerlöse und Beschäftigtenzahl die entscheidenden Maßstäbe.

Nach Umsatz war im Geschäftsjahr 2011 der Ölkonzern Royal Dutch Shell das weltweit größte Unternehmen, nach Marktkapitalisierung am 30. Juni 2012 der Hardware- und Softwarehersteller Apple.

Listen der größten Unternehmen werden unter anderem von den US-amerikanischen Wirtschaftsmagazinen Forbes und Fortune sowie der britischen Wirtschaftszeitung Financial Times aufgestellt. Dazu gehören vor allem Fortune Global 500, eine Rangliste der 500 umsatzstärksten Unternehmen der Welt, und Financial Times Global 500, eine Rangliste der 500 nach Marktkapitalisierung größten Unternehmen der Welt.

Einen alternativen integrativen Ansatz, welcher die vier verschiedenen Indikatoren Umsatz, Gewinn, Aktiva und Marktkapitalisierung gleichzeitig berücksichtigt, bietet Forbes mit der Liste Forbes Global 2000, die aber ausschließlich börsennotierte Unternehmen berücksichtigt. Hier war im Geschäftsjahr 2011 der Ölkonzern ExxonMobil das größte Unternehmen der Welt.


In Deutschland erstellt die Monopolkommission eine Rangliste der größten Unternehmen in Deutschland in ihren alle zwei Jahre erscheinenden Hauptgutachten. Die gemessen an der Wertschöpfung zehn größten Unternehmen in Deutschland waren demnach 2006: Deutsche Telekom, Siemens, DaimlerChrysler, Volkswagen, Deutsche Bahn, Deutsche Post, Deutsche Bank, Robert Bosch GmbH, Bayerische Motoren Werke und BASF.


Lokale Unternehmen sind Unternehmen, die nur einen Betrieb an einem Ort führen. Man spricht von regionalen Unternehmen, wenn innerhalb einer geografischen Region mehrere Betriebsstätten geführt werden. Ein sogenanntes nationales Unternehmen betreibt Stützpunkte innerhalb eines Landes.

Handelt es sich um Unternehmen, die auch auf internationalen Märkten aktiv sind (Internationalisierung), spricht man nach Sumantra Ghoshal und Christopher Bartlett von internationalen Unternehmen, globalen Unternehmen und multinationalen Unternehmen. Multinationale Unternehmen zeichnen sich durch Produktionsstandorte in mehreren Staaten aus. Nationale Gesellschaften erledigen dabei das operative Geschäft und Teile der strategischen Aufgaben. Globale Unternehmen sind zentralisierte Unternehmen, bei denen die einzelnen nationalen Gesellschaften primär Distributionsaufgaben übernehmen. Strategische Entscheidungen und der größte Teil der operativen Entscheidungen werden jedoch im Mutterland beschlossen. Internationale Unternehmen organisieren bestimmte strategische Abteilungen zentral, andere werden dezentral organisiert. Hierbei handelt es sich um eine Mischform von multinationalen und globalen Unternehmen.

Die Existenzbedingungen des Unternehmens sind Liquidität (Existenzbedingung „sine qua non“), Rentabilität und Wachstum. Liquidität muss jederzeit – auch kurzfristig – gesichert sein, um Zahlungsverpflichtungen nachkommen zu können. Rentabilität muss mittel- bis langfristig gesichert sein, da sonst die Liquiditätsbedingung nicht erfüllt werden kann. Wachstum wird an Größen wie Gewinn, Erlös oder Beschäftigtenzahl gemessen. Um Liquidität und Rentabilität zu sichern, muss ein Unternehmen mindestens mit dem Markt mitwachsen.

Unternehmensziele lassen sich in drei Dimensionen darstellen. Dabei wird zwischen der ökonomischen, der sozialen und der ökologischen Dimension unterschieden. Eine vorrangige Stellung der ökonomischen Dimension ergibt sich aus den konstitutiven Merkmalen eines jeden Unternehmens. Innerhalb der ökonomischen Dimension unterscheidet man wiederum Leistungsziele, Finanzziele und Erfolgsziele.

"Internationalisierung" heißt die geografische Dezentralisierung der Unternehmenstätigkeit auf internationalen Märkten. Sie gewinnt durch eine zunehmende Globalisierung der gesamten Unternehmenstätigkeit immer mehr an Bedeutung. Motive für Internationalisierung sind die Sicherung des Absatzes durch größere Marktnähe, die Senkung der Lohn- und Lohnnebenkosten, Umgehen von Importrestriktionen, Realisierung von Transportkostenvorteilen, Investitionsfördermaßnahmen durch die ausländischen Staaten sowie Unabhängigkeit von der Entwicklung der Devisenkurse. In welcher konkreten Form Internationalisierung erfolgt, ist abhängig von der Situation des jeweiligen Unternehmens und seiner Strategie. Die Stufen der Internationalisierung in Abhängigkeit von Kapital- und Managementleistungen sind Export, Lizenzvergabe, Franchising, Joint Venture, Auslandsniederlassung und Tochterunternehmen.

Bei der Internationalisierung von Dienstleistungsunternehmen gelten dagegen andere Schwerpunkte.

Unternehmen sind häufig Teil größerer Wirtschaftseinheiten. Dabei werden „Kooperation“ und „Konzentration“ unterschieden.

Kooperation ist die freiwillige Zusammenarbeit mehrerer rechtlich selbständiger Unternehmen. Dabei werden die drei Typen Kartell, Konsortium und Unternehmensverband unterschieden. Kartelle sind Kooperationen auf vertraglicher Basis, die sich wettbewerbsbeschränkend auswirken sollen. Die Mitglieder eines Kartells streben meist nach Monopolstellung, ohne dabei ihre Selbständigkeit aufzugeben. In Deutschland sind Kartelle im Rahmen des Wettbewerbsrechts verboten. Konsortien sind ähnliche Kooperationen auf vertraglicher Basis, allerdings ohne wettbewerbsrechtliche Relevanz. Häufig werden sie zur Durchführung von Großprojekten gegründet und anschließend wieder aufgegeben. Unternehmensverbände werden zur gemeinsamen Interessenvertretung gegenüber der Öffentlichkeit oder dem Staat gebildet. Bei Unternehmensverbänden ist wiederum nach Wirtschaftsfachverbänden, Kammern und Arbeitgeberverbänden zu differenzieren.

Darüber hinaus zählt auch die gemeinsame Gründung eines neuen Unternehmens durch mehrere bestehende Unternehmen zu den Kooperationen. Ein derartiges Gemeinschaftsunternehmen basiert jedoch im Gegensatz zu den drei klassischen Typen der Kooperation nicht allein auf vertraglicher Basis. Stattdessen ist die Kooperation dabei durch Kapitalbeteiligungen der Gesellschaftsunternehmen gekennzeichnet.

Konzentration ist die freiwillige oder auch unfreiwillige Angliederung eines bereits bestehenden Unternehmens an ein anderes Unternehmen. Dabei wird die wirtschaftliche Selbständigkeit des Unternehmens zugunsten der übergeordneten Einheit eingeschränkt. In seltenen Fällen geht sie sogar komplett verloren. Dabei wird zwischen Fusionen und verbundenen, rechtlich selbständigen Unternehmen unterschieden. Eine Fusion ist die Verschmelzung mehrerer Unternehmen zu einer wirtschaftlichen Einheit. Sie wird entweder durch Übertragung des Unternehmensvermögens mit vorheriger Liquidation oder durch Übertragung des Unternehmensvermögens im Rahmen der Gesamtrechtsnachfolge (gilt nur für Kapitalgesellschaften) vollzogen.

Demgegenüber sind verbundene Unternehmen durch kapitalmäßige Verflechtung oder vertragliche Vereinbarung miteinander verbunden. Dabei kann es sich um eine Minderheitsbeteiligung (bei Beteiligungsquoten bis zu 25 Prozent), eine Sperrminorität (bei Beteiligungsquoten bis zu 50 Prozent), eine Mehrheitsbeteiligung (bei Beteiligungsquoten bis zu 75 Prozent), eine Dreiviertelmehrheitsbeteiligung (bei Beteiligungsquoten bis zu 95 Prozent) oder eine Eingliederungsbeteiligung (bei Beteiligungsquoten im Bereich zwischen 95 und 100 Prozent) handeln.

Übertragungen von Unternehmen führen dazu, dass die Inhaberschaft am Unternehmen wechselt.
Zu Übertragungen kann es insbesondere bei Unternehmenskauf und Erbfall kommen. Daneben kommen etwa Übertragungen aufgrund von Schenkung bei sog. vorweggenommener Erbfolge in Betracht.
Bei Überlassungen von Unternehmen wechselt (nur) die Person des Betreibers des Unternehmens, der Inhaber des Unternehmens bleibt in der Regel derselbe; Anwendungsfälle sind insbesondere die Unternehmenspacht und der Unternehmensnießbrauch.

Als vorübergehende Formen der Zusammenarbeit von Unternehmen existieren etwa das Konsortium, die Arbeitsgemeinschaft oder die strategische Partnerschaft.

Die Frage, warum sich Individuen und Gruppen von Individuen zu Unternehmen zusammenschließen und dadurch Transaktionen außerhalb des Marktes durchführen, ist eine der Grundfragen der Industrieökonomik.

Neben Effizienzgründen (technologische Gründe, Unternehmen als langfristige Beziehung, Unternehmen als Institution zum optimalen Umgang mit unvollständigen Verträgen) kann die Existenz von Unternehmen auch mit Rentenabschöpfung erklärt werden.



</doc>
<doc id="5320" url="https://de.wikipedia.org/wiki?curid=5320" title="Unkündbarkeit">
Unkündbarkeit

Unkündbarkeit (in Österreich: Pragmatisierung) liegt im Arbeitsrecht vor, wenn für den Arbeitgeber eine ordentliche Kündigung des Arbeitsverhältnisses ausgeschlossen ist. 

Die meisten Arbeitsverhältnisse unterliegen einer beiderseitigen Kündigungsmöglichkeit durch Arbeitgeber oder Arbeitnehmer. Davon geht auch die allgemein für alle Arbeitsverhältnisse geltende Vorschrift des Abs. 2 BGB bei unbefristeten Arbeitsverhältnissen aus. Die Unkündbarkeit ist deshalb als Ausnahmeregelung anzusehen, die auch nur auf ganz bestimmte Arbeitsverhältnisse begrenzt ist. Noch im Dezember 1954 ging das Bundesarbeitsgericht (BAG) davon aus, dass selbst eine „ständige Übung“ auch eine ordentliche Unkündbarkeit der Arbeitnehmer begründen könne.

Die ordentliche Unkündbarkeit eines Beschäftigungsverhältnisses ist eine Form der Mitarbeiterbindung und wirkt faktisch wie eine Arbeitsplatz- oder Beschäftigungsgarantie. Die Unkündbarkeit kann für Arbeitnehmer einen sehr starken Anreiz darstellen, dessen Wirkung den Anreiz möglicher Gehaltssteigerungen durch Beförderung sogar noch übertrifft.

Vergleichbar mit der Unkündbarkeit eines Arbeitnehmers ist die Situation von Beamten, die in einem Beamtenverhältnis auf Lebenszeit stehen und die nur bei schweren Dienstvergehen entlassen werden können. Dabei gehen möglicherweise auch die Versorgungsansprüche verloren. Die Lebenszeitstellung des Beamten schließt die Möglichkeit der Versetzung in ein anderes Amt und die gehaltsmindernde zeitweilige oder endgültige Versetzung in den Ruhestand ein.

Die Unkündbarkeit und Versorgung auf Lebenszeit (Alimentation) soll vor willkürlicher Entlassung schützen. 

Die Lebenszeitstellung von Beamten geht historisch auf die Rechtsstellung des Richters in England zur Sicherung der Unabhängigkeit der Justiz („Unabsetzbarkeit des Richters“) entsprechend der Gewaltenteilungslehre zurück. Nur in den Ländern des Deutschen Bundes wurde Anfang des 19. Jahrhunderts dieses Prinzip auf die Beamtenschaft insgesamt übertragen. Der Beamtendienst sollte als Auszeichnung angesehen werden, die den Träger zu einer engagierten Dienstausübung motiviert. Dahinter stand ursprünglich das Bündnis zwischen vermögenslosem Bildungsbürgertum und Reformmonarchie gegen den Adel.

Das aus dem Fürstendienst der Einzelstaaten hervorgegangene Beamtentum führte als erste Berufsgruppe eine Unkündbarkeit ein. Zu Beginn des 17. Jahrhunderts verpflichtete sich der Bedienstete dem Lehnsherrn gegenüber auf privatrechtlicher Grundlage. Im Absolutismus wechselte die Rechtsgrundlage zum öffentlichen Recht und betonte die Unabhängigkeit des Beamtentums. 

Unter König Friedrich Wilhelm I. entstand erstmals ein besonderes Beamtenethos. Treue (gegenüber dem Monarchen), Fleiß, Unbestechlichkeit, Pünktlichkeit und Sparsamkeit waren bereits damals die wichtigen „preußischen“ Tugenden. Das von ihm initiierte, aber erst durch Friedrich II. im Februar 1794 erlassene Allgemeine Preußische Landrecht galt als erste zusammenfassende gesetzliche Regelung des Beamtenrechts, die in Teil II Titel 10 das Kapitel „Von den Rechten und Pflichten der Diener des Staates“ beinhaltete. Hierin war erstmals der Staatsdienst als Lebensberuf vorgesehen.

In Bayern führte die „Haupt-Landespragmatik“ vom Januar 1805 das Beamtenrecht ein und verwandelte das Arbeitsverhältnis erstmals in ein öffentlich-rechtliches Dienstverhältnis, das ab Mai 1808 die "Inamovibilität" (Unabsetzbarkeit der Beamten und Richter) vorsah. Das badische Dieneredikt vom Januar 1819 schrieb den Beamten Unkündbarkeit zu, auch Sachsen garantierte seinen Beamten im März 1835 die Unkündbarkeit. Mit dem "Zivilversorgungsschein" konnten sich Berufssoldaten der preußischen Gendarmerie in den unteren Dienstgraden zum Ende ihrer Militärlaufbahn seit Juni 1871 in der staatlichen preußischen Verwaltung - etwa im Schuldienst oder bei Postämtern - bewerben und konnten damit eine Weiterbeschäftigung im öffentlichen Dienst antreten. Im November 1918 drohte Friedrich Ebert mit der Ablösung der Beamten, was in der Bürokratie eine Furcht vor dem Verlust der Unkündbarkeit auslöste. Die Weimarer Reichsverfassung vom August 1919 führte erstmals für Beamte die Anstellung auf Lebenszeit, Ruhegehalt und Hinterbliebenenversorgung und die Staatshaftung bei Amtspflichtverletzung ein. 

In den USA hat sich die Unkündbarkeit () der Bundesbeamten zwischen 1890 und 1914 durchgesetzt. Das dortige Berufsbeamtentum zeichnet sich durch professionelle Rekrutierung, Aufstiegs- und Karrieremuster und praktische Unkündbarkeit aus. Die in der öffentlichen Verwaltung () Beschäftigten erhalten einen Arbeitsvertrag ohne zeitliche Begrenzung, der jedoch nach Erlöschen der übertragenen Aufgaben endet. Der privilegierte Status der Unkündbarkeit von Hochschullehrern () soll als Tenure-Track der Bewahrung der akademischen Freiheit dienen. Auch ernannte Bundesrichter sind unkündbar. In Griechenland verloren Beamte mit dem Tod oder der Abdankung des Monarchen ihre Stellung.

Am 23. Februar 1934 hob eine Verordnung der Reichsregierung die Unkündbarkeit auf Reichs-, Länder- und Gemeindeebene auf, sie wurde durch das Grundgesetz im Mai 1949 wieder eingeführt ( Abs. 5 GG; hergebrachte Grundsätze des Berufsbeamtentums). Eines der wesentlichsten Lebensrisiken im Sozialstaat ist das der Arbeitslosigkeit, der die DDR ab April 1950 mit dem Recht auf Arbeit eine faktische Unkündbarkeit entgegensetzte, die sämtliche Arbeitsverhältnisse erfasste. Sie galt nur dann nicht, wenn sich jemand der „Verletzung staatsbürgerlicher Pflichten“ oder der „sozialistischen Moral“ schuldig machte. Ansonsten kannte die DDR keine Beamten, sondern lediglich Arbeiter und Angestellte des öffentlichen Dienstes.

Eine absolute Unkündbarkeit gibt es in Deutschland nicht, denn aus der Definition ist zu entnehmen, dass für den Arbeitnehmer weiterhin eine ordentliche oder außerordentliche Kündigung möglich ist und der Arbeitgeber noch die Möglichkeit der außerordentlichen Kündigung gemäß BGB aus wichtigem Grund besitzt. Dem „unkündbaren“ Mitarbeiter kann mithin aus in seiner Person oder in seinem Verhalten liegenden wichtigen Gründen fristlos gekündigt werden. Selbst wenn die ordentliche Unkündbarkeit eines Arbeitnehmers dazu führt, dass eine außerordentliche fristlose Kündigung nicht gerechtfertigt ist, bleibt noch die Möglichkeit einer außerordentlichen Kündigung mit Auslauffrist, die einer ordentlichen Kündigungsfrist entspricht. Bei dauernder Unkündbarkeit wird eine Kündigungsfrist von 18 Monaten unterstellt, bei zeitweiliger Unkündbarkeit - etwa aufgrund Zugehörigkeit zum Betriebsrat - soll die Kündigungsfrist maßgebend sein, die der Arbeitgeber ohne den besonderen Kündigungsschutz einzuhalten hätte. Eine unzulässige ordentliche Kündigung kann aber nicht gemäß BGB in eine außerordentliche befristete Kündigung umgedeutet werden.

Eine derartige „relative“ Unkündbarkeit kann sich aus Gesetz, Tarifvertrag, Betriebsvereinbarung (z. B. Sozialplan) oder Arbeitsvertrag ergeben. Eine gesetzliche Regelung findet sich in Abs. 3 Teilzeit- und Befristungsgesetz, wonach die ordentliche Kündigung eines befristeten Arbeitsverhältnisses für den Arbeitgeber und den Arbeitnehmer ausgeschlossen ist, soweit eine Kündigungsmöglichkeit nicht vereinbart worden ist. Beamte können nur unter erschwerten Bedingungen aus ihrer Stellung entfernt werden (Entlassung, Verlust der Beamtenrechte, Entfernung aus dem Beamtenverhältnis nach den Disziplinargesetzen oder Eintritt oder Versetzung in den Ruhestand), nicht aber durch Kündigung ( Beamtenstatusgesetz). 

Unkündbarkeitsregelungen sind vor allem im öffentlichen Dienst anzutreffen. Die ordentliche Unkündbarkeit stellt ein wesentliches Element des Kündigungsschutzes im öffentlichen Dienst dar und beginnt mit Vollendung eines bestimmten Lebensalters und Erreichen eines bestimmten Dienstalters. Hier können nach § 34 Abs. 2 TVöD Arbeitsverhältnisse von Beschäftigten, die das 40. Lebensjahr vollendet haben, nach einer Beschäftigungszeit von mehr als 15 Jahren durch den Arbeitgeber nur aus einem wichtigen Grund gekündigt werden. Beide Voraussetzungen müssen gleichzeitig erfüllt sein. Im Rahmen der wichtigen Gründe kann nach der ständigen Rechtsprechung des BAG aufgrund tarifvertraglicher Vorschriften eine außerordentliche Kündigung aus wichtigem Grund bei Krankheit in Betracht kommen, und zwar bei Langzeiterkrankungen oder häufigen Kurzerkrankungen. 

Im Falle eines „sinnentleerten Arbeitsverhältnisses“ (Orlando-Kündigung), bei dem der Arbeitgeber dem Arbeitnehmer auf „unabsehbare Dauer“ Arbeitsentgelt fortzahlen müsste, ohne ihn beschäftigen zu können, ist eine außerordentliche betriebsbedingte Kündigung durch den Arbeitgeber möglich. Das trifft auf Fälle zu, wo dem Arbeitgeber Unternehmerfreiheit zugesprochen wird und hierdurch Tätigkeiten wegfallen (etwa Personalabbau durch Outsourcing). Zuvor muss der Arbeitgeber jedoch alle Möglichkeiten ausgeschöpft haben, um eine Beendigungskündigung zu vermeiden; notfalls ist auch ein Arbeitsplatz mit schlechterer Vergütung anzubieten oder ein in absehbarer Zeit durch Fluktuation frei werdender Arbeitsplatz. Der Arbeitgeber kann sogar gehalten sein, eine Weiterbeschäftigung bei anderen öffentlichen Arbeitgebern durch einen Personalgestellungsvertrag auszuschöpfen.

Die - zeitweilige - Unkündbarkeit kann sich aus einer bestimmten Funktion ergeben und gilt nur solange, wie jemand diese Funktion ausübt. "Persönlich ausgeschlossen" (Sonderkündigungsschutz) ist eine Kündigung etwa bei Abgeordneten ( Abs. 2 GG), Vertrauenspersonen der Schwerbehinderten ( Abs. 3 SGB IX), Immissionsschutzbeauftragten ( Abs. 2 BImmSchG), Datenschutzbeauftragten ( Abs. 3 BDSG), Auszubildenden nach Ablauf der Probezeit ( Abs. 2 BBiG), zum Wehrdienst eingezogenen Personen ( ArbPlSchG), Zivildienstleistenden ( Abs. 1 Nr. 1 ZivildienstG) und Mitgliedern des Betriebsrats, der Jugendvertretung und der Auszubildendenvertretung ( Abs. 1 KSchG), Mutterschutz und Elternschaft ( Abs. 1 MuSchG, Abs. 1 BEEG) oder während der Pflegezeit ( Abs. 1 PflegeZG). 

Die tarifliche Unkündbarkeit kann jeden Arbeitnehmerstatus erfassen, gleichgültig, ob jemand Arbeiter, Angestellter, Beamter, Richter oder Soldat ist. Im öffentlichen Dienst wurde mit dem Inkrafttreten des TVöD und des Tarifvertrag für den öffentlichen Dienst der Länder (TV-L) im Oktober 2005 die Unterscheidung zwischen Angestellten und Arbeitern abgeschafft. Beide Gruppen werden jetzt einheitlich als Beschäftigte bezeichnet. Der Arbeitnehmer muss sich in einem Kündigungsschutzprozess ausdrücklich auf die tarifliche oder vertragliche Unkündbarkeit berufen.

Es wird argumentiert, dass durch die Unkündbarkeit im öffentlichen Dienst die Beschäftigten ihre Unabhängigkeit vor ungerechtfertigten politischen Eingriffen und ihre Unparteilichkeit sicherstellen können. Die in der Unkündbarkeit liegende Beschäftigungsgarantie vermindert die betriebliche oder behördliche Flexibilität auf dem Arbeitsmarkt und verursacht bei den Arbeitgebern Personalkosten, die sich in Rezessionen als Kostenremanenz negativ auf den Gewinn auswirken. Die Unternehmen müssen unkündbares Personal vorhalten, das im Extremfall unbeschäftigt ist und deshalb Leerkosten verursacht. Derartige Beschäftigungsgarantien sind in den Arbeitsentgelten eingepreist, so dass unkündbare Arbeitsverhältnisse im Regelfall ein niedrigeres Gehaltsniveau aufweisen als frei kündbare. Je höher mithin das Arbeitsplatzrisiko, um so höher ist im Normalfall die Entlohnung. Die von der Unkündbarkeit Begünstigten müssen keine Arbeitslosenversicherung zahlen, weil sie vom Risiko der Arbeitslosigkeit im Regelfall nicht betroffen sind; sie werden als versicherungsfrei eingestuft ( Abs. 1 SGB III). Oft diskutiert wird die angeblich geringere Leistungsbereitschaft oder Arbeitsproduktivität unkündbarerer Arbeitskräfte im Zusammenhang mit dem zugrunde liegenden Moral Hazard.

Eine wesentliche Ursache für die früheren hohen Verluste der Deutschen Bundesbahn bildete die aus der Unkündbarkeit resultierende Beschäftigungsgarantie, denn etwa die Hälfte der Bahnbediensteten hatte 1979 Beamtenstatus, die übrigen Arbeitnehmer unterlagen ebenfalls der Unkündbarkeit. Das galt repräsentativ für alle ehemaligen Parafisci und den gesamten öffentlichen Dienst. Die Staatstreue der Staatsbediensteten wird mit dem Gegenwert von Unkündbarkeit des Arbeitsverhältnisses belohnt. Die lebenslange Beschäftigungsgarantie hängt auch unmittelbar mit der Insolvenzunfähigkeit der öffentlichen Unternehmen und öffentlichen Verwaltung zusammen. Die lebenslange Beschäftigungsgarantie hat zweifellos einen unbestimmten, aber hohen monetären Wert. 

In der Schweiz und Österreich hat es gravierende Änderungen bei der Unkündbarkeit gegeben. Die Schweizer Bundesbeamten sind seit März 2000 aufgrund des Bundespersonalgesetzes (BPG) mit wenigen Ausnahmen (etwa der Bundesrichter) Angestellte des öffentlichen Rechts, ihr Arbeitgeber besitzt ein ordentliches Kündigungsrecht aus betrieblichen oder wirtschaftlichen Gründen (Art. 10 BPG). In Österreich heißt die Unkündbarkeit Pragmatisierung, die begünstigten Arbeitskräfte sind „definitiv“ gestellt (also verbeamtet). Seit September 2004 sind nur noch bestimmte Berufsgruppen „definitiv“ gestellt, etwa Richter, Polizisten, Bundeslehrer oder Landesbeamte. 

Die Niederlande, Schweden und Großbritannien schufen die Unkündbarkeit ihrer Professoren ab. In Brasilien verabschiedete der Kongress im Februar 1998 eine Verwaltungsreform, die erstmals eine Flexibilisierung der Unkündbarkeit und eine Kappungsgrenze für Bundesbeamte einführte. 


</doc>
<doc id="5321" url="https://de.wikipedia.org/wiki?curid=5321" title="UNESCO">
UNESCO

Die UNESCO (, , auch: "Organisation der Vereinten Nationen für Bildung, Wissenschaft und Kultur") ist eine Internationale Organisation und gleichzeitig eine der 17 rechtlich selbstständigen Sonderorganisationen der Vereinten Nationen. Sie hat ihren Sitz in Paris (Frankreich). Derzeit sind 195 Mitglieder in der UNESCO vertreten und elf assoziierte Mitglieder – neben den 193 Staaten der UN mit Ausnahme von Liechtenstein auch die nicht in den UN vertretenen Cookinseln, Niue und die Palästinensischen Autonomiegebiete.

Zu den Aufgabengebieten der UNESCO gehören die Förderung von Erziehung, Wissenschaft und Kultur sowie Kommunikation und Information. Ihr Gründungsvertrag wurde am 16. November 1945 von 37 Staaten in London unterzeichnet und trat am 4. November 1946 nach der Ratifikation durch 20 Staaten in Kraft. Erster Generaldirektor war Julian Huxley.

Aus den Erfahrungen des Zweiten Weltkrieges zogen die Gründungsstaaten die folgende Lehre:
Weiter aus der Präambel der Verfassung der UNESCO:
Im Bereich der Erziehung setzt sich die UNESCO vor allem dafür ein, bis zum Jahre 2015 weltweit „Bildung für alle“ ("Education for All", EFA) zu erreichen. Hierzu haben sich 164 Länder verpflichtet, sechs Bildungsziele zu erreichen. Auch die Gesundheitserziehung zur Drogen- und AIDS-Prävention sowie der Wiederaufbau des Bildungswesens in Katastrophen- und Krisengebieten gehören zum Wirkungsfeld. Die UNESCO setzt sich auch für eine demokratische Erziehung auf Basis der Menschenrechte ein.

Außerdem entwickelte die UNESCO mit der ISCED (International Standard Classification of Education) eine Klassifikation zur Charakterisierung von Schulsystemen.

Wichtiger Bestandteil der Arbeit der UNESCO ist die Organisation interregionaler und internationaler Konferenzen zum Thema Bildung/Erziehung wie etwa die Konferenz zur Erwachsenenbildung CONFINTEA.

Zum Aufgabenbereich der UNESCO gehören zudem das UNESCO-Schulprojekt und die UNESCO-Lehrstühle.

Die UNESCO koordinierte auch die UN-Dekade Bildung für nachhaltige Entwicklung (2005–2014).

Im wissenschaftlichen Bereich fördert die UNESCO vorrangig die zwischenstaatliche Zusammenarbeit in der Ozeanographie, Hydrologie, Geologie und Umweltwissenschaften mit dem Hauptziel des Erhalts der biologischen Arten und der Trinkwasserressourcen.

Die Folgen des wissenschaftlichen und technischen Fortschritts der Biowissenschaften haben den Bedarf an international übereinstimmenden Werten, Grundsätzen und Normen der Bioethik erhöht. Die UNESCO hat darauf reagiert und in den vergangenen Jahren drei völkerrechtlich nicht bindende Erklärungen im Bereich von Wissenschaft und Menschenrechten ausarbeiten lassen, die von der Generalkonferenz jeweils einstimmig verabschiedet wurden:

Die UNESCO betraut die "Haager Konvention zum Schutz von Kulturgut bei bewaffneten Konflikten" von 1954, das wichtigste Werkzeug zur Bewahrung von Kulturgut, und die "Konvention gegen illegalen Handel mit Kulturgut" von 1970, das Werkzeug gegen Plünderung. 
2001 entstand die "Konvention zum Schutz des Kulturerbes unter Wasser", womit eine wichtige Lücke im geographischen Abdeckungsgebiet der Haager Konvention geschlossen wurde.

Das Welterbekomitee der UNESCO verwaltet das "Welterbe der Menschheit" ("Welterbekonvention" von 1972), welches eine besondere Auszeichnung mit Bewahrungspflicht darstellt. Es setzt sich aus dem Weltkultur- und Weltnaturerbe zusammen, womit die UNESCO ihre Agenden auch in den Bereich der Landschaften und Naturgebilde ausdehnte (entsprechend dem Begriff des Naturdenkmals). Hinsichtlich Kulturgüterschutz besteht eine enge Partnerschaft zwischen der UNESCO und Blue Shield International. Da bei vielen Kriegen und Unruhen die Bewegungsfreiheit des Vereinte-Nationen-Personals wegen Sicherheitsbedenken deutlich eingeschränkt ist, wird Blue Shield aufgrund seiner Struktur als besonders geeignet angesehen, um flexibel und autonom in bewaffneten Konflikten zu handeln. Das betrifft auch die Zusammenarbeit der UNESCO mit Blue Shield zur Erhebung von zu schützenden Kulturgut, die Erstellung von "No-strike lists", die Verknüpfung ziviler und militärischer Strukturen und die Ausbildung von lokalem militärischen Personal hinsichtlich Schutz von Kulturgut. 
Schon 1970 wurde das Programm "Man and Biosphere" (MAB) ins Leben gerufen, welches die enge Verflechtung von Kulturleistungen und Umwelt betont. Zu diesem Programm gehören auch die "Biosphärenreservate" als Modellregionen. Daher ist die UNESCO auch im Umwelt- und Naturschutz engagiert, in Folge des Programms entstanden wichtige weltweite Konventionen, etwa "über die biologische Vielfalt" (Biodiversität).

1982 wurde bei der UNESCO-Weltkulturkonferenz "Mondiacult" in Mexiko ein erneut „erweiterter Kulturbegriff“ festgeschrieben. Die 126 Teilnehmer-Staaten nahmen damit eine konzeptionelle Entwicklung zur Kenntnis; im Grunde ging es darum, den elitären, bildungsbürgerlichen Kulturbegriff und den europalastigen Fokus auf Kulturgut als Bauwerke und Werke der bildenden „Schönen Künste“ im musealen Sinne abzulösen.

2005 hat die UNESCO das "Übereinkommen zum Schutz und zur Förderung der Vielfalt kultureller Ausdrucksformen" verabschiedet - eine „Magna Charta“ der Kulturpolitik.
In diesen Kontext gehören auch das "Weltdokumentenerbe" im Rahmen des Programms "Memory of the World" (MOW) von 1992, das als „Gedächtnis der Welt“ auf die materiellen Zeugnisse bedeutender geistiger Kulturleistungen fokussiert (alte Handschriften, originale Urkunden, Bibliotheken u. ä.).
Im Oktober 2003 wurde die "Konvention zur Erhaltung des immateriellen Kulturerbes" verabschiedet, das Übereinkommen trat am 20. April 2006 in Kraft. Schon das Vorgängerprogramm "Meisterwerke des mündlichen und immateriellen Erbes der Menschheit" schützte von 1997 bis 2006 mündliche Ausdrucksformen wie Mythen, Epen und Erzählungen, sowie darstellende Künste Musik, Tanz, Spiele, Bräuche, handwerkliche Fähigkeiten und andere künstlerische Ausdrucksformen, die sich nicht in einem materiellen Werkbegriff äußern.

Im Bereich „Kommunikation und Information“ setzt sich die UNESCO mit den Problemen der Informationsgesellschaft auseinander. Sie engagiert sich für die Pressefreiheit und den Informationszugang, indem sie unter anderem in Entwicklungsländern Medienkompetenz vermittelt, Journalisten ausbildet und Radiostationen bzw. Nachrichtenagenturen aufbaut.

Die UNESCO betreibt unter dem Kürzel EOLSS eine große wissenschaftliche Online-Enzyklopädie.

Als Vorgängerinstitution gilt das dem Völkerbund angegliederte Institut International de Coopération Intellectuelle (IICI), das im Januar 1926 in Paris seine Arbeit aufnahm. Das IICI wiederum war ins Leben gerufen worden, weil die 1922 gegründete "Commission International de Coopération Intellectuelle" (CICI), der vierzehn Mitglieder, unter anderem Albert Einstein, Henri Bergson, Marie Curie und Gonzague de Reynold angehörten, als wenig schlagkräftig galt.
Das IICI wurde deshalb als Exekutivorgan der weiterhin bestehenden CICI gegründet und wurde vor allem von Frankreich unterstützt, das auch den überwiegenden Teil des Personals stellte und 80 Prozent der Kosten des Instituts trug. Hauptaufgaben des IICI waren Fragen des Urheberrechts und geistigen Eigentums sowie von Übersetzungen, der Statistik im Kulturbereich und der „moralischen Abrüstung“, das heißt, das IICI sollte komplementär zum Völkerbund wirken, dessen Hauptaufgabe die Friedenssicherung durch politische Maßnahmen, wie etwa Abrüstung und Streitschlichtung war, indem es die Friedensbereitschaft der Völker durch erzieherische und kulturelle Maßnahmen stärkte.

Seit 1942 führte der britische Erziehungsminister Lord Butler in London Gespräche mit Amtskollegen aus acht europäischen Exilregierungen. Ziel war der Wiederaufbau von Bildung und Kultur nach dem Ende des Zweiten Weltkrieges in Europa. Die Sowjetunion weigerte sich ausdrücklich, an den Beratungen teilzunehmen. Geplant waren zunächst bilaterale Verträge zwischen den beteiligten Staaten. Nachdem jedoch die Gründung der Vereinten Nationen beschlossen war, begann im Frühjahr 1944 der Kreis um Butler, eine ähnliche Struktur für Bildung und Kultur unter dem Dach der späteren UNO und mit Sitz in London zu entwickeln. Ein erster Entwurf für die UNESCO-Verfassung datiert auf den April 1944. Ab diesem Zeitpunkt beteiligten sich auch die Vereinigten Staaten an den Beratungen. Die europäischen Teilnehmer hofften auf US-Finanzhilfen für die Zeit nach dem Krieg. Auf amerikanischer Seite wollte man die UNESCO für die Gestaltung eines demokratischen Europa nach den eigenen Plänen nutzen.

Ab April 1945 kam es zu Konflikten zwischen der neuen französischen Regierung unter Charles de Gaulle und den übrigen Teilnehmern der Beratungsrunde. Die französische Seite wollte ihr, auf der Französischen Revolution basierendes Kulturverständnis zum Leitbild der UNESCO machen. Als ab November 1945 nach einem Regierungswechsel auf britischer Seite relativ unerfahrene Delegierte an den Beratungen teilnahmen, setzten die Franzosen Paris als zukünftigen Sitz der UNESCO durch. Inhaltlich kam ab diesem Zeitpunkt der Wissenschaft und der internationalen wissenschaftlichen Zusammenarbeit ein größeres Gewicht zu.

Luxemburg trat am 27. Oktober 1947 der UNESCO bei. Österreich folgte am 13. August 1948 als 40. Mitglied. Die Schweiz wurde am 18. Januar 1949 Mitglied. Am 11. Juli 1951 wurde die Bundesrepublik Deutschland Mitglied der UNESCO, vertreten durch die ständige Vertretung der Bundesrepublik Deutschland bei der UNESCO; im November 1972 auch die Deutsche Demokratische Republik. Liechtenstein ist bis heute (Stand: 2017) zwar Mitglied der Vereinten Nationen, aber kein UNESCO-Mitgliedsstaat.

Im Verlauf der Entkolonialisierung in den 1950er bis 1970er Jahren traten zahlreiche neu entstandene Staaten der UNESCO bei, so dass die Zahl der Mitgliedsstaaten von 59 im Jahr 1950 auf 99 (1960), 125 (1970), 153 (1980) und 159 (1990) anstieg. 1955 verließ die Südafrikanische Union – einer der Gründungsstaaten – die UNESCO, da diese zu einem „Forum für anti-südafrikanische Agitatoren geworden“ sei. Nach Ende des Apartheid-Regimes trat Südafrika 1994 wieder bei. Die Republik China, die ebenfalls zu den Gründungsstaaten gehörte, wurde 1971 infolge der Resolution 2758 der UN-Generalversammlung aus der UNESCO gedrängt und an ihrer Stelle die Volksrepublik China aufgenommen. Auch Portugal war zwischen 1972 und 1974, dem Jahr der Nelkenrevolution, nicht Mitglied. Nach dem Zerfall der Sowjetunion 1991 kamen 12 ehemalige Sowjetrepubliken hinzu, während Russland als Rechtsnachfolger der Sowjetunion fungierte. 

Heute zählt die UNESCO 195 Mitgliedstaaten (Stand: Oktober 2017).

Zwischen den Vereinigten Staaten und der UNESCO kam es mehrfach zu ernsthaften Differenzen. Im Jahr 1974 stellte der amerikanische Kongress auf Empfehlung des damaligen Präsidenten Gerald Ford die Zahlungen an die UNESCO vorübergehend ein, nachdem diese in einer Resolution die Palästinensische Befreiungsorganisation (PLO) anerkannt, und den Staat Israel verurteilt hatte. Am 19. Dezember 1983 kündigten die USA ihre UNESCO-Mitgliedschaft. Zur Begründung wurde angeführt, dass die UNESCO „linksgerichtet politisiert“ sei und dass ihre Finanzen völlig in Unordnung seien. Die Kritik fokussierte sich stark auf den damaligen Generalsekretär Amadou-Mahtar M'Bow, einen senegalesischen Muslim, dem ein geldverschwendendes Patronage-System vorgeworfen wurde. Am 31. Dezember 1984 erfolgte der tatsächliche Austritt der Vereinigten Staaten. Zum 31. Dezember 1985 verließ auch das Vereinigte Königreich unter Premierministerin Margaret Thatcher mit ähnlichen Argumenten und Bedenken wegen der befürchteten Einschränkung der Pressefreiheit durch die neue „Weltinformations- und -kommunikationsordnung“ die Organisation, trat ihr aber im Jahr 1997 unter Premierminister Tony Blair wieder bei. Singapur trat zeitgleich mit Großbritannien aus und kehrte am 8. Oktober 2007 wieder in die UNESCO zurück. Auch die USA traten im Jahr 2003 nach 19 Jahren der Abwesenheit wieder der UNESCO bei.

Am 31. Oktober 2011 beschloss die UNESCO-Generalversammlung mit 107 gegen 14 Stimmen bei 52 Enthaltungen Palästina als offizielles Mitglied aufzunehmen. Die 14 Gegenstimmen kamen von Australien, Deutschland, Israel, Kanada, Litauen, den Niederlanden, Palau, Panama, Samoa, den Salomonen, Schweden, der Tschechischen Republik, den Vereinigten Staaten und Vanuatu. Zu den Staaten, die sich enthielten, gehörten die Schweiz und das Vereinigte Königreich. Für die Aufnahme stimmten unter anderen Österreich, Frankreich, Russland, China und Indien. Danach reduzierten die Vereinigten Staaten, die zu diesem Zeitpunkt etwa ein Fünftel des UNESCO-Budgets finanzierten, ihre Beitragszahlungen zur UNESCO. Zur Begründung hieß es, dass die Aufnahme Palästinas „die internationalen Bemühungen um einen gerechten dauerhaften Frieden im Nahen Osten unterminieren würde.“ Wegen des Zahlungsboykotts verloren die USA und Israel im November 2013 ihr Stimmrecht. Für 2016–2017 betrug der reguläre UNESCO-Haushalt insgesamt 653 Millionen US-Dollar. Durch die Aussetzung der Beitragszahlungen der USA und Israels stand allerdings nur ein Ausgabenplan in Höhe von 518 Millionen US-Dollar zur Verfügung.

Am 18. Oktober 2016 verabschiedete die UNESCO auf Antrag arabischer Mitgliedsstaaten eine Resolution unter dem Titel „Sicherstellung des Kulturerbes Palästinas und des eigenständigen Charakters Ost-Jerusalems“, in der die Bedeutung der alten Stadt Jerusalem für die drei monotheistischen Religionen betont wurde. Allerdings wurde der Tempelberg nur mit ihren arabisch-islamischen Namen al-Haram al-Sharif bezeichnet, die viel älteren jüdischen Verbindungen und die Bezeichnung Zion blieben unerwähnt. Israel wurde außerdem wegen angeblicher Gewalt gegen islamische Pilger und wegen archäologischer Ausgrabungen kritisiert. Daraufhin stellte Israel seine Zusammenarbeit mit der UNESCO vorläufig ein. 

Die Entscheidung der UNESCO vom 7. Juli 2017, die Altstadt von Hebron zum palästinensischen Weltkulturerbe zu erklären, führte zu Protesten der israelischen Diplomaten, weil die jüdischen Verbindungen zu Hebron (z. B. Beerdigungsort einiger Erzväter, antike Hauptstadt Israels vor Jerusalem) verschwiegen wurden. Am 12. Oktober 2017 erklärten zunächst die US-Regierung und Stunden später die israelische Regierung den Austritt aus der UNESCO zum 31. Dezember 2018. Als Anlass galt neben den bisherigen Unstimmigkeiten die mögliche Wahl des Katarers Hamad bin Abdulasis al-Kawari, dem Antisemitismus vorgeworfen wird, zum Generaldirektor der Organisation. Kawari unterlag dann im Exekutivrat in der Stichwahl der französischen Kandidatin Audrey Azoulay. Dem vorausgegangen war eine Kritik von US-Präsident Donald Trump, dass die Vereinten Nationen nicht die ihr gesetzten Ziele erreicht hätten. Trump kritisierte auch den nach seiner Ansicht überproportionalen Anteil der USA an den regulären Ausgaben der Vereinten Nationen (22 %) und an den Ausgaben für friedenserhaltende Maßnahmen (28 %). Die USA streben stattdessen den Status eines permanenten Beobachters bei der UNESCO an, um weiterhin ihre Sichtweise und Expertise bei einigen wichtigen Aktivitäten der Organisation einzubringen. Ende Dezember 2017 reichte der israelische Ministerpräsident Benjamin Netanjahu den Austritt Israels aus der UNESCO formell ein.

Am 13. Oktober 2017 wurde die ehemalige französische Kulturministerin Audrey Azoulay mit 30 gegen 28 Stimmen durch den UNESCO-Exekutivrat zur künftigen Direktorin der UNESCO gewählt. Ihr Gegenkandidat war der katarische Diplomat Hamad bin Abdulasis al-Kawari. Die Wahl wurde am 10. November 2017 von der Generalkonferenz der 195 Mitgliedstaaten der UNESCO bestätigt. Am 15. November 2017 begann die Amtsperiode der neuen Generaldirektorin.

Organe der UNESCO sind die Generalkonferenz, der Exekutivrat und das Sekretariat, an dessen Spitze ein Generaldirektor steht.

Die "Generalkonferenz" ist das oberste Entscheidungs- und Kontrollorgan der UNESCO. Sie tritt (seit 1954) alle zwei Jahre zu einer ordentlichen Tagung in Paris zusammen. In der Generalkonferenz gilt der Grundsatz "Ein Staat – eine Stimme" – ein Prinzip, das auch in der Mehrzahl der anderen Sonderorganisationen sowie der Generalversammlung der Vereinten Nationen Anwendung findet.

Die Generalkonferenz ist Entscheidungsträger, denn sie bestimmt die Zielsetzung und die allgemeinen Richtlinien der Arbeit der Organisation und beschließt über die ihr vom Exekutivrat vorgelegten Programme und hat das Budgetrecht. Die politische Zielsetzungen und die Arbeitsrichtlinien werden festgelegt, wobei ein vom Sekretariat in Abstimmung mit den Mitgliedsstaaten vorgelegtes zweijähriges Arbeitsprogramm als Diskussionsgrundlage dient.

Ferner beruft sie internationale Staatenkonferenzen ein, nimmt internationale Empfehlungen oder Übereinkommen an und erörtert die Berichte der Mitgliedstaaten an die Organisation über die Maßnahmen zur Umsetzung von Empfehlungen und Übereinkommen.

Die Generalkonferenz wählt auch die Mitglieder des "Exekutivrates" für eine vierjährige Amtszeit, der seit 1995 aus 58 Mitgliedern besteht und normalerweise zweimal jährlich zusammentritt.

Parallel zur Generalkonferenz tagt diese auch in wichtigen Komitees und Kommissionen:

Der Exekutivrat fungiert als "Bindeglied" zwischen Generalkonferenz und Sekretariat. Er ist für die Vorbereitungen der Tagesordnung der Generalkonferenz, die Prüfung des Arbeitsprogramms und des entsprechenden Haushaltsplans sowie für die Überwachung des vom Sekretariat verabschiedeten Arbeitsprogramms zuständig.
Hinsichtlich der Zusammensetzung des Exekutivrats ist im Laufe der letzten 50 Jahre eine zunehmende "Verstaatlichung" festzustellen. Bis 1954 galten die Mitglieder als Privatpersonen – gewählt als "Vertreter des Geistesleben", die im Auftrag der Generalkonferenz handeln sollten. Gemäß einem Vorschlag der Vereinigten Staaten und des Vereinigten Königreichs gelten die Vertreter nunmehr nicht mehr als unabhängig, sondern zugleich als Politiker, welche die Staaten vertreten, aus denen sie stammen. Seit 1976 können die Regierungen darüber hinaus ihre Vertreter vor dem Ablauf ihrer vierjährigen Amtszeit und unabhängig von deren Zustimmung abberufen und ersetzen lassen. Gewählt werden nach Artikel V Absatz 2 der UNESCO-Verfassung solche Persönlichkeiten, die über die notwendige Erfahrungen und Fähigkeiten verfügen, um die administrativen und exekutiven Pflichten des Rates zu erfüllen.

Das Sekretariat ist in mehrere Abteilungen gegliedert. An seinem Hauptsitz in Paris arbeiten derzeit rund 2.100 Mitarbeiter aus etwa 170 Nationen. Weitere 700 Mitarbeiter sind in den 65 Außenstellen in aller Welt tätig.

Das Sekretariat wird von einem Generaldirektor geleitet, der auf Vorschlag des Exekutivrats von der Generalkonferenz für eine Amtszeit von vier Jahren gewählt wird. Der Direktor nimmt an den Sitzungen der Generalkonferenz, des Exekutivrates und der Ausschüsse ohne Stimmrecht teil und erstellt Berichte über die Tätigkeit der Organisation.

Die UNESCO ordnet ihre Mitgliedsstaaten fünf Regionen zu:
Auch wenn diese Regionen die Bezeichnung von Kontinenten im Namen tragen, sind sie nicht rein geografisch orientiert, sondern richten sich auch nach kulturellen Gesichtspunkten. So können Staaten einer UNESCO-Region zugeordnet sein, die nach einem anderen Kontinent benannt ist als dem, in dem sie geographisch liegen (Beispiel Israel: geographisch Asien, UNESCO-Region Europa und Nordamerika). Auch Staaten, die Gebiete auf mehr als einem Kontinent haben, werden als ganze einer UNESCO-Region zugeordnet (Beispiel Russland: mitsamt seinem asiatischen Teil ganz der UNESCO-Region Europa und Nordamerika zugeordnet, ebenso z. B. Frankreich mit allen Überseegebieten).

Die UNESCO-Nationalkommissionen sind keine Organe der UNESCO, jedoch bereits von der UNESCO-Verfassung vorgesehene Stellen in jedem Mitgliedsstaat. Die Staaten sollen ihre „mit Fragen der Erziehung, Wissenschaft und Kultur befassten maßgeblichen Institutionen mit der Arbeit der UNESCO in Verbindung bringen, vorzugsweise durch Bildung einer Nationalkommission, in der die Regierung und die betreffenden Institutionen vertreten sind.“ Nationalkommissionen existieren in allen Mitgliedstaaten, so auch in Deutschland (Deutsche UNESCO-Kommission), Österreich (Österreichische UNESCO-Kommission) und der Schweiz.







</doc>
<doc id="5323" url="https://de.wikipedia.org/wiki?curid=5323" title="Uranus (Planet)">
Uranus (Planet)

Der Uranus (Latinisierung von altgriechisch "uranós" ‚Himmel‘) ist von der Sonne aus mit einer durchschnittlichen Sonnenentfernung von 2,9 Milliarden Kilometern der siebte Planet im Sonnensystem und wird zu den äußeren, jupiterähnlichen (jovianischen) Planeten gerechnet. Er wurde 1781 von Wilhelm Herschel entdeckt und ist nach dem griechischen Himmelsgott Uranos benannt.

Der Durchmesser dieses Gasplaneten ist mit über 51.000 Kilometern etwa viermal so groß wie der Durchmesser der Erde, das Volumen ist etwa 65-mal so groß wie das der Erde. Uranus ist nur unter günstigen Umständen freiäugig sichtbar. Seine blassgrüne Scheibe ist von der Erde aus betrachtet etwa 3,5″ groß. Physikalisch ist Uranus mit dem Neptun vergleichbar und nimmt nach ihm mit rund 14 Erdmassen in der Massenrangfolge im Sonnensystem unter den Planeten den vierten Platz ein. Hinsichtlich des Durchmessers liegt er knapp vor Neptun auf Rang drei – nach Jupiter und Saturn. Aufgrund von Eisvorkommen im Inneren werden Uranus und Neptun auch „Eisriesen“ genannt.

Das astronomische Symbol des Uranusist dem Marssymbol ähnlich. Im Unterschied zu diesem hat der Kreis einen Zentralpunkt, und der Pfeil auf dem Kreis steht senkrecht. Ein anderes, hauptsächlich in der Astrologie verwendetes Uranussymbol ist .

Am besten ist Uranus derzeit am Herbst- und Winterhimmel zu beobachten. Seine Opposition 2018 ist am 24. Oktober und verlagert sich jährlich um 4 bis 5 Tage nach hinten.

Uranus läuft auf einer annähernd kreisförmigen Umlaufbahn mit einer Exzentrizität von 0,0457 zwischen Saturn und Neptun um die Sonne. Sein sonnennächster Punkt, das Perihel, liegt bei 18,324 AE und sein sonnenfernster Punkt, das Aphel, bei 20,078 AE. Mit fast 3 Mrd. km Abstand hat er etwa die doppelte Entfernung zur Sonne wie der nächstinnere Planet Saturn. Dieser Bahnradius passt genau zur 1766 formulierten Titius-Bode-Reihe, sodass die Entdeckung des Uranus als Bestätigung der damaligen, von Kepler begründeten Sicht einer „Weltharmonie“ galt.

Die Bahnebene ist mit 0,772° nur wenig gegen die Erdbahnebene geneigt und hat damit im Vergleich mit den anderen Planeten die geringste Inklination. Für einen Umlauf um die Sonne benötigt Uranus etwa 84 Jahre. Bei einer mittleren Bahngeschwindigkeit von 6,81 km/s braucht er rund zwei Stunden, um seinen eigenen Durchmesser zurückzulegen (die Erde braucht etwa sieben Minuten). 

Uranus rotiert in 17 Stunden 14 Minuten und 24 Sekunden einmal um seine Achse. Wie bei allen Gasplaneten wehen in der Hochatmosphäre starke Winde in Rotationsrichtung. In südlichen Breiten (etwa 60°) bewegt sich die sichtbare Atmosphäre viel schneller und die Rotationsdauer ist dort mit 14 Stunden entsprechend kürzer.

Als Besonderheit liegt die Rotationsachse des Planeten annähernd in seiner Bahnebene, er „wälzt“ sich gewissermaßen in dieser voran, wenn die Achse in Richtung Sonne zeigt. Die Achsneigung gegen das Bahnebenenlot beträgt 97,77°, so dass Uranus rückläufig rotiert. Als Folge dieser Neigung ist nach jedem halben Umlauf einmal die Nordhalbkugel und einmal die Südhalbkugel der Sonne zugewandt. Mit Ausnahme einer schmalen Äquatorregion herrscht dann auf den jeweiligen Halbkugeln ständig Tag bzw. Nacht (vergleichbar mit Polartag und Polarnacht auf der Erde). In der Nähe der Pole kann ein Sonnentag daher bis zu einem halben Uranusjahr dauern.

Als Voyager 2 am 24. Januar 1986 an Uranus vorbeiflog, stand die Sonne annähernd über dessen Südpol. 2007 lag sie zur Tag-und-Nacht-Gleiche kurz in seiner Äquatorebene.

Die Ursache der starken Achsneigung ist unbekannt. Die verbreitetste Hypothese nimmt eine Kollision mit einem großen Protoplaneten während der Entstehungsphase an.

Nach Computersimulationen müsste Uranus während seiner Entstehung und der Bildung seines Mond- und Ringsystems von zwei oder mehr Himmelskörpern getroffen worden sein, um die Neigung des gesamten Uranussystems zur Umlaufbahn des Planeten zu erklären.

Uranus hat eine für Gasplaneten typisch niedrige Dichte von 1,27 g/cm³. Der Äquatordurchmesser beträgt mit 51.118 km reichlich dem vierfachen Erddurchmesser. Aufgrund der schnellen Rotation, verstärkt durch die geringe Dichte, weist Uranus mit einem Poldurchmesser von 49.946 km eine deutliche Abplattung von 1:44 auf. Er ist nach Jupiter und Saturn der drittgrößte Planet des Sonnensystems, jedoch auf Grund seiner geringen Dichte weniger massereich als Neptun.

Im Strukturmodell wird Uranus als flüssiger Planet mit einer gasförmigen oberen Schicht oder Atmosphäre betrachtet, die nicht klar nach unten begrenzt ist. Da sich der Druck mit zunehmender Tiefe über den kritischen Punkt erhöht, geht die Gashülle ohne Phasenübergang vom gasförmigen in einen flüssigen Zustand über. Als Oberfläche wurde derjenige Bereich definiert, bei dem der Druck 1 bar groß ist. Die Schwerkraft macht auf der Ein-Bar-Ebene rund 90 % der Erdschwere aus.

Obwohl Uranus anteilmäßig mehr schwerere Elemente (schwerer als Wasserstoff und Helium) als Jupiter aufweist, ist seine Dichte auf Grund seiner geringeren Masse und geringeren Drücken im Inneren (800 GPa statt 3000...4500 GPa) geringer als die des Jupiters.

Die Hauptbestandteile der oberen Schichten der Gashülle sind molekularer Wasserstoff mit 82,5 ± 3,3 Vol-%, atomares Helium mit 15,2 ± 3,3 Vol-% und etwa 2,3 Vol-% Methan. Das Massenverhältnis Helium:Wasserstoff ist mit 0,26 sehr nahe dem ursprünglichen Massenverhältnis in der Sonne von 0,27. Als Nebenbestandteil folgt Deuterium mit etwa 148 ppm Volumenanteil. Als Aerosole werden Ammoniakeis, Wassereis, Ammoniumhydrogensulfid und Methaneis diskutiert. Wasserstoff kann (von der Erde aus) im Spektrum des Sonnenlichtes, das durch die planetarischen Wolken gestreut wird, nachgewiesen werden. Das Verhältnis Wasserstoff zu Helium konnte durch die Refraktion (Brechung) der Radiosignale von Voyager 2 durch die Atmosphäre bestimmt werden, als die Sonde den Funkschatten des Planeten durchflog.

Das Sonnenlicht wird von den oberen Wolkenschichten reflektiert. Diese befinden sich unter einer Schicht aus Methangas. Wenn das reflektierte Licht diese Schicht durchquert, wird durch das Methangas der rötliche Teil des Lichtes absorbiert, während der blaue Anteil ungehindert passieren kann. Dadurch erscheint Uranus blaugrün.

Beim Aufbau der Atmosphäre können drei Schichten unterschieden werden: Die "Troposphäre" in Höhen zwischen −300 und 50 km und Drücken von 100 bis 0,1 bar. Die "Stratosphäre" befindet sich in Höhen zwischen 50 und 4000 km und die Drücke betragen 0,1 bis 10 bar. Die "Thermosphäre" (Korona) erstreckt sich von 4000 km bis zu 50.000 km über der Oberfläche. Es gibt keine Mesosphäre.

Die Troposphäre ist der unterste und dichteste Teil der Atmosphäre. Mit steigender Höhe fällt ihre Temperatur ab. Am unteren Ende der Troposphäre, das etwa 300 km unter dem Ein-Bar-Level liegt, beträgt die Temperatur etwa 320 K. Bis zum oberen Bereich der Troposphäre, der sich in 50 km Höhe befindet, fällt die Temperatur auf etwa 53 K ab. Sie enthält fast die gesamte Masse der Atmosphäre und ist auch für den Großteil der planetarischen Wärmeausstrahlung (ferne Infrarotstrahlung) verantwortlich.

Die Wolken bestehen anscheinend aus Partikeln gefrorenen Methans, das als heißes Gas aus tieferen Lagen aufgestiegen und in den äußeren Schichten kondensiert ist. Es wird vermutet, dass Wasser die unteren Wolken bildet, während die oberen Wolken eher aus Methan bestehen. Die Windgeschwindigkeiten betragen bis zu 200 m/s beziehungsweise rund 700 km/h. Die Temperatur beträgt bei 1 bar etwa 76 K (−197 °C), bei 0,1 bar 53 K (−220 °C).

Seine effektive Temperatur liegt bei nur 58,1 K (−214 °C), kaum mehr als der entferntere Neptun. Diese Strahlungstemperatur ist die Temperatur, welche die Uranusatmosphäre im Bereich von 0,4 bar aufweist. Die niedrigste Temperatur in der Atmosphäre wird bei 70 mbar mit 52 K (−221 °C) gemessen.

In der Stratosphäre, der mittleren Schicht der Uranusatmosphäre, erhöht sich im Allgemeinen die Temperatur mit der Höhenlage. An der unteren Grenze bei 50 km (bei der Tropopause) sind es noch 53 K, während die Temperatur in 4000 km Höhe (an der Grenze zur Thermosphäre) schon 800 bis 850 K beträgt. Ursache für die Erhitzung der Stratosphäre ist die Absorption von solarer UV- und IR-Strahlung durch Methan und andere Kohlenwasserstoffe, die sich in diesem Teil der Atmosphäre als Ergebnis der Methanphotolyse bilden. Der Wärmetransport von der heißen Thermosphäre könnte ebenfalls dazu wirken. Die Kohlenwasserstoffe besetzen einen relativ engen Bereich in Höhen von 100 bis 280 km. Dabei beträgt der Druck etwa 10 bis 0,1 mbar und die Temperaturen liegen zwischen 75 und 170 K.

Ethan und Ethin (Acetylen) neigen dazu, im kälteren unteren Bereich der Stratosphäre und in der Tropopause nebelige Schichten zu formen. Sie könnten teilweise für die detailarme Erscheinung des Uranus verantwortlich sein. Die Konzentration von Kohlenwasserstoffen ist in der Stratosphäre Uranus’ oberhalb dieser Nebel wesentlich niedriger als in den Stratosphären der anderen Gasplaneten. Dies und die schwache vertikale Durchmischung über der Nebelschicht machen die Stratosphäre des Uranus durchsichtiger und als Ergebnis kälter als die der anderen Gasplaneten.

Die äußerste Schicht der Atmosphäre Uranus’ ist die Thermosphäre und Korona. Sie weist eine einheitliche Temperatur von 800 bis 850 K auf. Dies ist viel höher als die 420 K in der Thermosphäre des Saturn. Die Wärmequellen hierfür sind nicht bekannt. Weder solares ultraviolettes Licht noch Polarlichtaktivitäten können genug Energie zur Verfügung stellen. Verringerte Wärmeabstrahlung aufgrund des Mangels an Kohlenwasserstoffen in der oberen Stratosphäre könnte zur Aufrechterhaltung der hohen Temperatur beitragen. Zusätzlich zu molekularem Wasserstoff enthalten Thermosphäre und Korona einen großen Anteil an freien Wasserstoffatomen. Deren geringe molekulare Masse könnte zusammen mit den hohen Temperaturen erklären, warum sich die Korona so weit (50.000 km oder zwei Uranusradien) vom Planeten weg ausdehnt. Diese erweiterte Korona ist ein einzigartiges Merkmal von Uranus. Die Korona bremst die kleinen Partikel ab, die Uranus umkreisen. Als Folge dessen sind die Ringe des Uranus sehr staubarm.

Die Ionosphäre des Uranus entspricht seiner Thermosphäre zusammen mit dem oberen Teil der Stratosphäre. Hauptsächlich weiß man über die Ionen durch Messungen von Voyager 2 Bescheid, sowie durch Infrarot-Emissionen des H-Ions, die von erdgebundenen Teleskopen festgestellt wurden. Die Beobachtungen zeigen, dass die Ionosphäre Höhen zwischen 2.000 und 10.000 km besetzt. Sie wird hauptsächlich von der UV-Strahlung der Sonne aufrechterhalten und ihre Dichte hängt von der Sonnenaktivität ab. Die Aktivität der Aurora (Polarlicht) ist nicht so auffällig wie bei Jupiter und Saturn. Die obere Ionosphäre (die Region der Thermosphäre) ist die Quelle der UV-Emission von Uranus, die als „Tagesglühen“ oder „Elektroglühen“ bekannt ist. Diese geht ebenso wie die IR-Strahlung der H-Ionen nur von der sonnenbeleuchteten Seite des Planeten aus. Dieses rätselhafte Phänomen, das bei den Thermosphären aller Gasriesen auftritt, wird nun als eine UV-Fluoreszenz von atomarem und molekularem Wasserstoff gedeutet, die von Sonnenstrahlen mit einer möglichen Beteiligung von Photoelektronen angeregt wird.

Unter der dichten, gasförmigen Wasserstoff-Methan-Hülle besteht Uranus aus teilweise verflüssigten Gasen, Eis und möglicherweise einem kleinen Gesteinskern. Die Gashülle geht durch Kompression in eine „Kruste“ aus Wasserstoff und Helium über, die etwa 30 % des Planetenradius ausmacht. Die Masse dieser oberen Schicht macht etwa die 0,5- bis 1,5-fache Erdmasse aus.

Der etwas dickere Mantel aus Wasser, Methan und Ammoniak hat vermutlich die Konsistenz von Eis und beinhaltet den Großteil der Masse des Uranus. Diese dichte Flüssigkeit, die elektrisch sehr leitfähig ist, wird manchmal auch Wasser-Ammoniak-Ozean genannt.

Dieser Mantel umschließt einen kleinen, eventuell flüssigen Kern aus Silizium und Eisen mit einer der Erde vergleichbaren Masse.

Dieser Aufbau ist mit dem des Neptun vergleichbar, unterscheidet sich aber deutlich von den Riesenplaneten Jupiter und Saturn. Diese haben anteilmäßig mehr Wasserstoff und weniger Helium (ähnlich wie die Sonne), und ihre Mäntel bestehen großteils aus metallischem Wasserstoff. Die Kerne von Uranus und Neptun ähneln jenen von Jupiter und Saturn, jedoch fehlt die stark komprimierte Hülle aus Wasserstoff. Im Zentrum des Uranus dürfte ein Druck von rund acht Millionen bar bei einer Temperatur von etwa 5000 °C herrschen.

Man vermutet, dass die Materie des Uranus relativ gleichmäßig verteilt ist. In Bezug auf interne Wärmequellen ist er eine Ausnahme unter den äußeren Planeten. Es ist aus bisher unerklärlichen Gründen kein Wärmevorrat mehr aus der ursprünglichen Kontraktion und Stofftrennung vorhanden. Eine mögliche Erklärung für das Fehlen der inneren Wärmequelle besteht darin, dass infolge des Einschlags, der seine Rotationsachse kippte, der Großteil der ursprünglichen inneren Hitze verloren ging. Einer anderen Theorie zufolge existieren in den oberen Schichten einige Barrieren, die den Wärmetransport aus dem Inneren behindern. Seine Energiequelle ist lediglich absorbierte Sonnenstrahlung, denn er strahlt im Unterschied zu den anderen Gasplaneten nicht mehr Wärme ab, als er von der Sonne erhält.

Bilder von Voyager 2 zeigten 1986 im sichtbaren Spektrum praktisch keine Oberflächendetails. Man sah kaum Wolkenbänder oder Stürme, wie man sie sonst auf anderen Gasplaneten beobachten kann. Die in Richtung der Rotation schnell wehenden Wolkenbänder waren nur sehr schwach ausgeprägt. Eine mögliche Erklärung für dieses vergleichsweise ruhige Wetter und die unauffälligen Wolkenformationen könnte in der schwachen inneren Wärmequelle des Uranus liegen.
Während des Vorbeifluges von Voyager 2 stand die Sonne über dem Südpol. Dennoch war Uranus aus unbekannten Gründen am Äquator wärmer als am sonnigen Pol. Daraus hatten die Wissenschaftler errechnet, dass sogar der dunkle Pol etwas wärmer ist als der von der Sonne bestrahlte. Die Temperaturen in der Atmosphäre sind durch diese sehr langsame Abkühlung – und andererseits sehr langsame Erwärmung – erstaunlich ausgeglichen.

Die südliche Hemisphäre kann in zwei Regionen aufgeteilt werden: Eine helle Polarkappe und dunklere äquatoriale Bänder. Die Grenze ist etwa bei 45° südlicher Breite. Ein schmales Band, das den Planeten zwischen dem 45. und 50. südlichen Breitengrad umspannt, ist das hellste große Merkmal auf der Oberfläche des Planeten. Es wird der südliche „Collar“ genannt. Die Polarkappe und der „Collar“ sind möglicherweise eine dichte Region von Methanwolken. Jedoch konnten am Beginn des 21. Jahrhunderts, als die Region der Polarkappe ins Sichtfeld kam, das Hubble-Weltraumteleskop und das Keck-Teleskop auf Hawaii weder einen „Collar“ noch eine Polarkappe in der nördlichen Hemisphäre beobachten. Deshalb erscheint Uranus asymmetrisch: hell in der Nähe des Südpols und einheitlich dunkel in der Region nördlich des südlichen „Collars“.

In den letzten Jahren nähert sich Uranus seinem Äquinoktium und damit wird die Nordhalbkugel zunehmend beleuchtet. Als Folge dieser erhöhten Sonneneinstrahlung zeigen neuere Aufnahmen des Hubble-Weltraumteleskops viel stärker ausgebildete Bänder und erhöhte Wetteraktivität in der nördlichen Hemisphäre. Demnach herrschen in der Atmosphäre des Gasplaneten, trotz seiner großen Entfernung von der Sonne, ausgeprägte Jahreszeiten. Er empfängt nur ein Vierhundertstel der Sonnenwärme, die die Erde empfängt. Die Sonne erscheint von ihm aus nur als eine winzige Scheibe. Dennoch strahlt sie immer noch 1100-mal heller, als der Vollmond von der Erde aus erscheint.

Bilder des Keck-Observatoriums zeigten im Jahr 2004, dass sich Wirbelstürme teilweise über viele Monate lang halten. In der nördlichen Hemisphäre entdeckten die Forscher eine rund 29.000 km lange Wolkenformation. Dies war die größte bislang beobachtete Wolkenstruktur. Sie hatte sich jedoch schon einen Monat später wieder aufgelöst. Langlebiger zeigte sich ein großer Sturm in der südlichen Hemisphäre, der sich schon seit mehreren Jahren aus unbekannten Gründen über fünf Breitengrade hinweg auf und ab bewegte.

2014 konnten die Stürme sogar von Amateuren mit dem Teleskop beobachtet werden, so auffällig waren sie geworden.

Das Magnetfeld von Uranus ist ungewöhnlich und hat die Form eines Quadrupols mit 2 Nord- und 2 Südpolen. Ein Polpaar ist um fast 60° gegenüber der Rotationsachse geneigt und hat seinen Ursprung nicht im Zentrum des Planeten, sondern ist um ein Drittel des Planetenradius nach Süden hin versetzt. Vermutlich wird es durch Bewegungen in nicht allzu großer Tiefe erzeugt, möglicherweise durch ionisiertes Wasser. Neptun hat ein ähnlich geformtes und verschobenes Magnetfeld, was darauf hindeutet, dass die starke Abweichung nichts mit der Größe der Achsenneigung zu tun hat. Die Magnetosphäre von Uranus ist über seiner Nachtseite durch die Rotation korkenzieherartig verwirbelt.

Die ungewöhnliche Geometrie resultiert in einer hoch asymmetrischen Magnetosphäre, bei der die Stärke des magnetischen Feldes in der südlichen Hemisphäre bis zu 0,1 Gauß (10 µT) gering sein, in der nördlichen Hemisphäre bis zu 1,1 Gauß (110 µT) stark sein kann. Das durchschnittliche Feld auf der Oberfläche beträgt 0,23 Gauß (23 µT). Im Vergleich dazu ist das magnetische Feld der Erde an beiden Polen etwa gleich stark, und ihr „magnetischer Äquator“ ist annähernd parallel mit ihrem physikalischen Äquator. Das Dipolmoment von Uranus ist 50-mal stärker als das der Erde.

Die Magnetosphäre enthält geladene Teilchen: Protonen und Elektronen und einen kleinen Anteil an H2+ Ionen. Der Partikelfluss ist hoch genug, um ein Verdunkeln oder eine Erosion der Mondoberflächen in einem astronomisch kurzen Zeitraum von 100.000 Jahren zu bewirken. Dies könnte die Ursache für die einheitlich dunkle Färbung der Monde und der Ringe sein.

Uranus hatte beim Vorbeiflug von Voyager 2 relativ gut entwickelte Auroras, die als helle Bögen um die magnetischen Pole gesehen werden.
Das Hubble Space Telescope konnte im Jahr 2011 kleine runde Auroras auf der Uranus-Tagseite beobachten, die nur wenige Minuten andauerten. Damit haben sich Uranus-Auroras seit dem Vorbeiflug von Voyager 2 deutlich verändert, was wohl damit zusammenhängt, dass die Planetenachse und damit das Magnetfeld zur Sonne anders orientiert sind als beim Vorbeiflug von Voyager 2.

Uranus ist wie alle Gasriesen im Sonnensystem von einer Menge sehr kleiner Körper und Teilchen umgeben, die den Planeten in Richtung seiner Rotation umrunden und mit ihren verschieden dicht belegten Umlaufbahnen ein System konzentrischer Ringe bilden. Diese befinden sich zumeist in der Äquatorebene des Planeten und hauptsächlich innerhalb der Roche-Grenze.

Das Ringsystem des Uranus wurde am 10. März 1977 von James L. Elliot, Edward W. Dunham und Douglas J. Mink mit dem Kuiper Airborne Observatory entdeckt. Die Entdeckung war ein Glücksfall. Sie planten die Bedeckung des Sterns SAO 158687 durch Uranus zu beobachten, um dessen Atmosphäre und Durchmesser zu untersuchen. Bei der Analyse ihrer Beobachtungen stellten sie fest, dass der Stern kurz vor und nach der eigentlichen Bedeckung zusätzlich je fünfmal kurzzeitig verschwand. Sie folgerten daraus, dass es ein Ringsystem um den Planeten geben müsse. Die Ringe wurden direkt von Voyager 2 abgelichtet, als die Sonde 1986 Uranus passierte. Es war nach Saturns Ringsystem das zweite, das im Sonnensystem entdeckt wurde.

Uranus hat wie Jupiter ein sehr feines und dunkles Ringsystem. Was die Größe der Teilchen betrifft, besteht es wie bei Saturn sowohl aus groben Partikeln und Brocken mit bis zu 10 Meter Durchmesser, als auch aus feinem, aber anteilmäßig viel geringerem Staub. Im Durchschnitt sind die Teilchen größer als die der Saturnringe, in der Gesamtzahl sind sie hingegen viel weniger. Mit Voyager 2 wurde festgestellt, dass die Gesamtmasse der Uranusringe geringer ist als die Teilchenmasse in der Cassinischen Teilung der Saturnringe. Als auffallendster Unterschied zu den Gebilden der anderen Gasplaneten sind sie zumeist schmal, jedoch scharf begrenzt und durch große scheinbare Leerräume voneinander getrennt. Nicht alle von ihnen sind kreisförmig oder liegen in der Äquatorebene des Uranus. Der hellste von ihnen – von Uranus aus der elfte – wird mit dem griechischen Buchstaben Epsilon (ε) bezeichnet. In seinem dem Planeten nächsten Bereich ist er 20 km breit und fast undurchsichtig, in seinem dem Uranus fernsten Abschnitt ist er jedoch mit 96 km fünfmal breiter und fünfmal so durchsichtig. Die innersten Monde Cordelia und Ophelia halten als Schäfermonde den dichten Epsilon-Ring von innen und außen durch ihre Gravitationswirkung zusammen.

Die zwei vom Hubble-Teleskop im Dezember 2005 zuletzt entdeckten Ringe befinden sich weit außerhalb der schon vorher bekannten elf und sind wesentlich breiter. Aufgrund ihrer großen Distanz zum Uranus werden sie das äußere Ringsystem genannt. Der größere Ring liegt in doppelt so großem Abstand zum Planeten wie die vorher bekannten Ringe. Damit sind 13 Ringe bekannt. Im April 2006 zeigten Bilder vom Keck-Observatorium die Farben der neuen Ringe: einer war blau, der andere rot.

Hubble hatte 2003 auch zwei kleine Monde erspäht, von denen einer, Mab, seinen Orbit mit dem äußersten neu entdeckten Ring teilt. Dieser Ring My (μ) zeigt seine höchste Dichte in einem Abstand von 97.700 km zum Uranuszentrum und seine Ringpartikel könnten vom Mond Mab stammen.

Die Innenkante dieses Ringes liegt 86.000 km vom Planetenzentrum entfernt, an der Umlaufbahn des Mondes Puck. Eine Besonderheit des Ringes ist ein blaues Leuchten, das bislang nur vom E-Ring des Saturn bekannt war. Jener Saturnring besteht offenbar aus sehr feinen Eiskristallen, die das Sonnenlicht reflektieren und die ihre Quelle in Geysiren auf dem Saturnmond Enceladus haben. Das stützt die Vermutung, dass die weniger als 0,0001 Millimeter großen Eispartikel des Uranusrings, die klein genug sind, um blaues Licht zu streuen, von dem sehr eishaltigen Uranusmond Mab stammen und durch Meteoriteneinschläge ins All befördert wurden.

Der weiter innen liegende Ring Ny (ν) ist von rötlicher Farbe und besteht wahrscheinlich neben kleinen auch aus größeren Komponenten als der blaue Ring. Seine höchste Dichte befindet sich in einem Abstand von 67.300 km zum Uranuszentrum. In seinem Dichtemaximum konnte noch kein Mond entdeckt werden. Die inneren Ringe des Planeten erscheinen dagegen grau.

Die Ringe des Uranus liegen anscheinend nicht genau zentrisch um den Planeten, sondern schwingen etwas um ihn. Als Ursachen dafür vermuten die Astronomen die Gravitationswirkung seiner Monde und seiner Abplattung.

Die Ringe des Uranus sind wahrscheinlich relativ jung. Spalten in ihrem Umkreis als auch Unterschiede in ihrer Trübung legen nahe, dass sie nicht mit dem Uranus entstanden. Die Materieteilchen in den Ringen könnten einmal Teile eines Mondes gewesen sein, der durch einen Einschlag mit hoher Geschwindigkeit oder durch Gezeitenkräfte zertrümmert wurde.

Es sind 27 Monde des Uranus bekannt. Ihre Durchmesser liegen zwischen 10 und 1600 km. Vier von ihnen sind so groß, dass sie sich aufgrund der Masse im hydrostatischen Gleichgewicht befinden und daher die Form eines Rotationsellipsoids haben. Bei einem fünften (Miranda) ist das wahrscheinlich auch der Fall.

Die beiden ersten wurden von Wilhelm Herschel 1787 entdeckt und von seinem Sohn John Herschel nach Figuren aus Shakespeares "Sommernachtstraum" Titania und Oberon benannt. Zwei weitere Monde, die William Lassell 1851 entdeckte, wurden Ariel und Umbriel getauft, Gerard Kuiper entdeckte 1948 den Mond Miranda. Alle weiteren Monde von Uranus wurden ebenfalls nach Figuren von Shakespeare oder Alexander Pope benannt. Beim Vorbeiflug der Raumsonde Voyager 2 im Januar 1986 wurden zehn weitere Monde entdeckt. Der Satellit Perdita wurde später auf Bildern von Voyager 2 identifiziert. Zwei weitere kleine innere Monde wurden mit dem Hubble-Teleskop entdeckt. Bis 1997 war Uranus der einzige Gasriese ohne bekannte „irreguläre Monde“. Seit damals wurden mit erdgebundenen Teleskopen neun entfernte irreguläre Monde gefunden.

Die letzten Entdeckungen datieren aus dem Jahr 2003, als mit dem Hubble-Teleskop neben zwei weiteren Ringen auch zwei weitere Monde entdeckt wurden. Einer dieser beiden Monde, der den Namen Mab erhalten hat, zerbröselt vermutlich langsam unter dem ständigen Bombardement von Mikrometeoriten und bildet dadurch einen der beiden neu entdeckten Ringe. Bei dieser Gelegenheit wurde außerdem entdeckt, dass vor allem die Bahnen der dicht gedrängten inneren Monde zwischen Miranda und den Hauptringen keine stabilen Keplerbahnen sind, sondern dass die Monde auf chaotische Weise Energie und Drehmoment austauschen. Nach in Science veröffentlichten Berechnungen könnten in wenigen Millionen Jahren einige der Monde, die sich gegenseitig mittels kreuzender Umlaufbahnen stören könnten, auf Kollisionskurs gehen.

Die Satelliten von Uranus bilden drei verschiedene Gruppen: eine planetennahe Gruppe mit kleinen Durchmessern und kreisförmigen Umlaufbahnen, eine mittlere Gruppe der fünf großen Satelliten, sowie eine äußere Gruppe von kleinen Satelliten mit sehr weiten, ausgeprägt exzentrischen und sehr stark geneigten beziehungsweise größtenteils rückläufigen Umlaufbahnen. Unter den großen Uranusmonden befindet sich jedoch keiner von der Größe der Galileischen Monde des Jupiter beziehungsweise des Saturnmondes Titan, oder auch nur des größten Neptunmondes Triton.

Die fünf Hauptmonde sind Miranda, Ariel, Umbriel, Titania und Oberon. Das Satellitensystem des Uranus ist das masseärmste unter den Gasriesen. Die Gesamtmasse der fünf größten Monde zusammen ist geringer als die Hälfte des Neptunmondes Triton und entspricht etwa 13 % der des Erdmondes. Der größte Satellit, Titania, weist einen Radius von nur 788,9 km auf. Das ist weniger als die Hälfte von Triton, aber ein wenig mehr als Rhea, der zweitgrößte Mond des Saturn. Titania ist damit der achtgrößte Mond im Sonnensystem. Die Monde haben eine relativ geringe Albedo. Diese erstreckt sich von 0,20 für Umbriel bis 0,35 für Ariel. Die Monde sind eine Ansammlung aus etwa 50 % Eis und 50 % Felsen. Das Eis könnte Ammoniak und Kohlendioxid enthalten.

Unter den Monden scheint Ariel mit den wenigsten Einschlagkratern die jüngste Oberfläche zu haben, während Umbriel als der älteste erscheint. Miranda besitzt 20 Kilometer tiefe Canyons mit Verwerfungen, terrassenförmige Schichten, und eine chaotische Variation in Alter und Merkmalen der Oberflächen. Einer Hypothese nach könnte Miranda vor längerer Zeit durch einen massiven Einschlag komplett auseinander gesprengt worden sein und sich dann wieder wahllos zusammengefügt haben. Mirandas jüngste geologische Aktivität wurde vermutlich durch Hitzeentwicklung beeinflusst, hervorgerufen durch Gezeitenkräfte. Damals war die Umlaufbahn bei einer 3:1-Resonanz mit Umbriel exzentrischer als jetzt. Grabenbrüche, verbunden mit aufsteigenden Diapirs, sind anscheinend die Gründe für die ovale Erscheinungsform des Mondes. Auch Ariel hatte wahrscheinlich eine 4:1-Bahnresonanz mit Titania gebildet.

Irreguläre Monde sind eingefangene Satelliten in großem Abstand zum Planeten; sie weisen eine hohe Bahnneigung auf und sind oft rückläufig.

Das Diagramm illustriert die Umlaufbahnen der irregulären Monde, die bis jetzt entdeckt wurden. Die Exzentrizität der Bahnen wird durch gelbe Segmente (die den Bereich vom Perizentrum bis zum Apozentrum überstreichen) und die Inklination durch die Y-Achse dargestellt. Die Satelliten oberhalb der X-Achse bewegen sich prograd (rechtläufig), die Satelliten darunter retrograd (rückläufig). Die X-Achse ist mit Gm (Millionen km) sowie dem betreffenden Bruchteil der Hill-Sphäre beschriftet. Der gravitative Einfluss, innerhalb dessen ein Umlauf um den Planeten möglich ist, reicht bei Uranus etwa 70 Millionen km in den Raum.

Anders als bei Jupiters irregulären Satelliten kann in der bekannten Uranus-Population kein Zusammenhang zwischen Bahnradius und Inklination gefunden werden. Stattdessen können die retrograden Monde basierend auf dem Verhältnis von großer Halbachse und Exzentrizität in zwei Gruppen geteilt werden. Die innere Gruppe umfasst Monde näher bei Uranus (a < 0,15 r) und sind mäßig exzentrisch (~ 0,2), namentlich: Francisco, Caliban, Stephano und Trinculo. Die äußere Gruppe (a > 0,15 r) umfasst Satelliten mit hoher Exzentrizität (~ 0,5): Sycorax, Prospero, Setebos und Ferdinand.

Bei der Entstehung der Eisriesen erreichten sie mit ihren nur wenigen Erdmassen angesammelter Materie von Gasnebeln nie den kritischen Punkt von Jupiter und Saturn, noch mehr Materie anzuziehen. Derzeitige Theorien über die Entstehung und Bildung des Sonnensystems haben Schwierigkeiten, die Existenz von Uranus und Neptun so weit jenseits der Bahnen von Jupiter und Saturn zu erklären. Sie sind zu groß, um sich aus der Materie gebildet zu haben, die im frühen Sonnensystem in dieser Entfernung zu erwarten wäre. Vielmehr vermuten einige Wissenschaftler, dass sich Uranus und Neptun viel näher bei der Sonne geformt haben und durch den Gravitationseinfluss von Jupiter hinausgeschleudert wurden. Jedoch zeigten andere Ende der 1990er durchgeführte Simulationen unter Berücksichtigung der Planetenwanderung die Möglichkeit, dass sich Neptun und Uranus nahe ihren jetzigen Positionen formen konnten.

Nach einer anderen, auf dem Nizza-Modell aufbauenden, Theorie von 2007 sollen sich Uranus und Neptun nicht nur näher bei der Sonne gebildet haben, sondern Uranus wäre auch weiter von der Sonne entfernt als Neptun entstanden, da er leichter als Neptun ist. Später hätten die beiden Planeten die Plätze getauscht, während sie auf ihre jetzigen Umlaufbahnen gelangten.

Die scheinbare Helligkeit schwankte von 1995 bis 2006 zwischen +5,6 und +5,9. Damit war er gerade noch mit freiem Auge sichtbar (die Grenze der Sichtbarkeit liegt bei +6,0). Sein Winkeldurchmesser bewegt sich zwischen 3,4″ und 3,7″ (Im Vergleich: Saturn zwischen 16″ und 20″, Jupiter zwischen 32″ und 45″). Während seiner Opposition ist Uranus in klarer, dunkler Nacht unter günstigen Bedingungen theoretisch mit bloßem Auge zu sehen. Mit einem Feldstecher ist er stets leicht zu finden. In größeren Amateurteleskopen mit einem Objektivdurchmesser zwischen etwa 15 und 23 cm erscheint Uranus als eine blasse cyanfarbene Scheibe mit einer deutlichen Randverdunkelung. Mit einem leistungsfähigeren Teleskop von 25 cm oder mehr könnten auch Wolkenstrukturen und einige der größeren Monde wie Titania und Oberon zu sehen sein.

Uranus ist bei sehr guten Bedingungen freiäugig sichtbar, allerdings entspricht seine Helligkeit nur einem gerade noch erkennbaren Stern 6. Größe. Hingegen zählen alle der Sonne und der Erde näheren Planeten – von Merkur bis Saturn – mit einer Helligkeit von mindestens 1. Größe zu den auffälligsten Objekten am Himmel und sind seit dem Altertum und der Antike als Wandelsterne wohlbekannt. Auf Grund seiner langsamen Bahnbewegung blieb Uranus auch nach der Erfindung des Fernrohrs noch lange als Planet unerkannt und wurde bei vereinzelten Beobachtungen für einen Fixstern gehalten. Das tat auch John Flamsteed, der ihn 1690 als „34 Tauri“ erstmals katalogisierte, oder Tobias Mayer im Jahre 1756.

Sir Friedrich Wilhelm Herschel entdeckte den Planeten zufällig am 13. März 1781 zwischen zehn und elf Uhr abends mit einem selbst gebauten 6-Zoll-Spiegelteleskop, als er von seinem Garten in der englischen Stadt Bath aus eine Himmelsdurchmusterung durchführte, um mit einer stärkeren Vergrößerung versuchsweise Fixsternparallaxen zu messen. Er hielt den sich an der Grenze zwischen den Sternbildern Stier und Zwillinge bewegenden Himmelskörper jedoch zunächst für einen Kometen, da kaum jemand daran gedacht hatte, dass es mehr als die bis dahin sechs altbekannten Planeten geben könnte. Uranus war der erste, der nicht schon in der Antike bekannt war.

Innerhalb von drei Monaten nach seiner Entdeckung erkannte die Wissenschaft Herschels Fund schließlich als neuen Planeten an. Die erste präzise Bahnbestimmung gelang dem Benediktiner Placidus Fixlmillner (1721–1791) auf der Sternwarte Kremsmünster. Die Mathematiker und Astronomen Anders Johan Lexell und Pierre-Simon Laplace hatten mit ihren Berechnungen schon bald zeigen können, dass es sich um einen Planeten handeln muss, der sich in dem 19-fachen Abstand der Erde um die Sonne bewegt. Uranus fügte sich damit genau in die vom deutschen Astronomen Johann Elert Bode veröffentlichte Titius-Bode-Reihe der Bahnradien ein, was die seit Johannes Kepler postulierte „Harmonie des Himmels“ eindrucksvoll zu bestätigen schien. Die Ausdehnung des bekannten Sonnensystems hatte sich damit verdoppelt.

Wilhelm Herschel entdeckte sechs Jahre nach dem Uranus auch dessen zwei größte und auffallendste Monde Titania und Oberon. Die große Neigung der Bahnebenen dieser und aller weiteren Uranusmonde führte durch Analogieschluss von den großen bekannten Saturn- und Jupitermonden für lange Zeit zu der begründeten, inzwischen bestätigten Vermutung, dass die Rotationsachse des Uranus den Mondbahnen entsprechend ebenfalls sehr stark geneigt sein müsse.

Die zunehmenden Abweichungen der beobachteten Positionen des Uranus von den berechneten ließen Bahnstörungen durch einen noch unbekannten weiteren Himmelskörper vermuten und führten zur gezielten Suche nach einem noch ferneren Planeten, die 1846 mit der Auffindung des Neptun erfolgreich war.

Mit der Entdeckung eines neuen Planeten begann eine über sechzig Jahre dauernde Debatte um seine Benennung. Herschel selbst benannte ihn zu Ehren des englischen Königs Georg III. "Georgium Sidus" – Georgs Stern. Der Jesuit und Astronom Maximilian Hell hatte "Urania", den Namen der Muse der Astronomie vorgeschlagen. In Frankreich bezeichneten ihn die Astronomen als "Herschel", bis Bode vorschlug, ihn nach dem griechischen Gott Uranos zu benennen. Der Name setzte sich aber erst gegen 1850 durch und wurde, entsprechend den römischen Namen der anderen Planeten, der lateinischen Schreibweise angepasst. In der römischen Mythologie ist Uranus der Vater von Saturn, der wiederum der Vater von Jupiter ist.

Dadurch wurde es der einzige Planet im Sonnensystem, der nicht direkt nach einer römischen Gottheit benannt wurde bzw. in den meisten Sprachen den Namen einer römischen Gottheit trägt. Die noch weiter entfernteren Neptun und Pluto wurden wieder nach dem bisherigen Schema benannt. Die bis dahin bekannten Planeten wurden in der Antike mit Göttern gleichgesetzt. Die Römer übernahmen die Bezeichnungen der Griechen, setzten aber ihre eigenen, den griechischen entsprechenden, Götter ein.

Gleich den sieben klassischen Planeten sollte dem neuen ebenfalls ein Planetenmetall zugeordnet werden, wofür das Platin in Betracht gezogen wurde. Von ihm hatte Bode das Symbol nach einem Vorschlag von Johann Gottfried Köhler als astronomisches Zeichen für Uranus entlehnt.

In dieser moderneren Zeit benannte aber 1790 der Chemiker Martin Heinrich Klaproth das von ihm im Jahr zuvor entdeckte Element „Uranium“ (heutige Bezeichnung: Uran) nach dem neuen Planeten.

In Frankreich und auch in England wurde ein Uranussymbol verwendet, das eigens entworfen worden war und mit der Initiale H auf Herschel hindeutet. In einem Brief an Herschel stellte der französische Astronom Jérôme Lalande 1784 das Symbol mit den Worten
vor. Das Symbol wird heute, obwohl es auf diese beiden Astronomen des 18. Jahrhunderts zurückgeht, vorwiegend in der Astrologie verwendet.
Wilhelm Herschel hatte in seinen Manuskripten von 1797 bereits einen Ring um den Uranus beschrieben, doch diese Beobachtung wurde als eine Täuschung aufgefasst. Auch nach der Entdeckung von 1977 hat niemand den historischen Aufzeichnungen getraut, da sich die Ringe viel zu lichtschwach zeigten, als dass er sie mit seinen Mitteln hätte sehen können. Bis der britische Forscher Stuart Eves die Notizen untersuchte und in den Angaben Übereinstimmungen mit der Größe, der Lage und der Farbe des Epsilon-Rings fand. Im April 2007 präsentierte er eine These, nach der die Uranusringe vor 200 Jahren heller gewesen sein können. Er begründet dies mit ähnlichen Veränderungen der Saturnringe, die diffuser und dunkler werden.

Die bisher einzige Raumsonde, die Uranus besuchte, war Voyager 2. Sie startete am 20. August 1977. Auf ihrer "Grand Tour" zu allen vier Riesenplaneten vollführte sie 1979 am Jupiter einen Swing-by zum Saturn, an dem sie 1981 weiteren Schwung in Richtung Uranus nahm. Den passierte sie am 24. Januar 1986 und übermittelte die meisten der heute von ihm bekannten Bilder und Daten. Ihre Signale vom Uranus – wie auch dessen reflektiertes Licht – erreichten die Erde erst nach zwei Stunden und 45 Minuten.

Während des Anfluges wurden mit der Sonde neben den inzwischen neun bekannten Ringen und fünf Monden zwei weitere Ringe und zehn neue Monde entdeckt. Der 16. Satellit wurde noch 13 Jahre später auf ihren fotografischen Aufnahmen entdeckt und konnte nach weiteren vier Jahren mit dem Weltraumteleskop Hubble bestätigt werden. Da der Uranus der Sonne während der Passage seine Südpolregion zuwandte, flog Voyager 2 zwischen die konzentrischen Bahnen seiner Monde wie durch die Kreise einer aufrechten Zielscheibe, und weil sie einen Swing-by in Richtung Neptun nehmen musste, konnte sie durch diesen Umstand nicht mehrere Uranusmonde hintereinander anfliegen. So lieferte sie hoch aufgelöste Fotos nur von Miranda, die sich als der geologisch interessanteste der fünf größeren Monde erwies. Es wurde noch das magnetische Feld, die unregelmäßige Struktur, die Neigung und seine einzigartige korkenzieherartige Verwirbelung (Magnetschweif), hervorgerufen durch die seitwärtige Bewegung von Uranus, untersucht. Eine weitere Raumsonde zum Uranus ist zurzeit nicht geplant.

Uranus befand sich im Jahr 2007 in einer seiner seltenen „Kantenstellungen“ – ein Ereignis, das nur alle 42 Jahre vorkommt. Dies gab den Forschern auch mit erdgebundenen Teleskopen besondere Gelegenheiten für Messungen. Zum einen versuchte man, Veränderungen, die durch den Wechsel der Jahreszeiten auf Uranus verursacht werden, in seiner Atmosphäre zu finden. Zum anderen kam es durch die spezielle Geometrie zu gegenseitigen Bedeckungen und Verfinsterungen der Uranusmonde. Würden diese Ereignisse gemessen, ließen sich die Bahnparameter der Monde erheblich genauer bestimmen, als es bisher der Fall war – so die Vermutung der Forscher.

In den Vereinigten Staaten läuft eine Studie einer Uranussonde, die nach dem Abwurf einer Atmosphärenkapsel den Planeten und seine Monde während mehrerer Umläufe erforschen soll. Sie würde nach einem Start im Jahr 2020 unter Verwendung eines solarelektrischen Antriebs und eines Swing-bys an der Erde 2033 ihr Ziel erreichen. Ob diese Studie letztendlich in einer konkreten Mission realisiert wird, ist unklar.






</doc>
<doc id="5324" url="https://de.wikipedia.org/wiki?curid=5324" title="Rutherfordium">
Rutherfordium

Rutherfordium, früher auch Kurtschatovium (Ku), ist ein ausschließlich künstlich erzeugtes chemisches Element mit dem Elementsymbol Rf und der Ordnungszahl 104. Es zählt zu den Transactinoiden. Im Periodensystem der Elemente steht es in der 4. IUPAC-Gruppe, der Titangruppe. Alle zehn bekannten Isotope des Rutherfordiums sind radioaktiv.

Eine erste Synthese des Elementes erfolgte 1964 durch den sowjetischen Forscher Georgi Fljorow am Kernforschungszentrum bei Dubna. Dort wurde Plutonium mit Neonkernen beschossen:

Es wurde in der Sowjetunion und anderen Ländern nach Igor Kurtschatow Kurtschatovium (Ku) benannt. US-amerikanische Forscher lehnten den Namen aus politischen Gründen ab, beanspruchten den 1969 erzielten ersten Nachweis des Elements für sich und schlugen den Namen "Rutherfordium" (Rf), nach Ernest Rutherford vor. Ein weiterer Vorschlag war die Bezeichnung "Dubnium" (Db). (In Ergänzung: die 1967 erfolgte Entdeckung des Elements 105, ebenfalls et al. durch Georgi Fljorow trägt seit 1997 den Namen Dubnium – benannt nach dem Kernforschungszentrum bei Dubna.) 

Die systematische Bezeichnung ist "Unnilquadium" (Unq). Erst 1997 kam eine Einigung auf "Rutherfordium" zustande (siehe Elementnamensgebungskontroverse).

Die ersten später bestätigten Synthesen wurden 1968/69 durch die amerikanischen Forscher Albert Ghiorso, Matti Nurmia, James Harris, Kari Eskola und Pirrko Eskola in Berkeley, USA mit Hilfe des Schwerionenbeschleunigers HILAC durchgeführt:

Rutherfordium gehört zu den Übergangsmetallen. Es hat wahrscheinlich die Elektronenkonfiguration [Rn]5f 6d 7s und somit ähnliche chemische Eigenschaften wie Titan, Zirconium und Hafnium; mit diesen wird es zur Titangruppe gerechnet. Wegen der kurzen Zerfallszeit – das langlebigste Isotop 261 hat nur eine Halbwertszeit von 65 Sekunden – und den geringen zur Verfügung stehenden Mengen kommt den chemischen Eigenschaften bisher keine Bedeutung zu. Vermutlich bildet es leicht hydratisierte Rf -Ionen in starker Säure-Lösung und auch Komplexe in Salzsäure, Bromwasserstoffsäure- oder Flusssäure-Lösungen.

Es gibt keine Einstufung nach der CLP-Verordnung oder anderer Regelungen, weil von diesem Element nur wenige Atome gleichzeitig herstellbar sind und damit viel zu wenige für eine chemische oder physikalische Gefährlichkeit.


</doc>
<doc id="5325" url="https://de.wikipedia.org/wiki?curid=5325" title="Dubnium">
Dubnium

Dubnium (früher auch: "Hahnium", "Nielsbohrium") ist ein ausschließlich künstlich erzeugtes chemisches Element mit dem Elementsymbol Db und der Ordnungszahl 105. Es zählt zu den Transactinoiden. Im Periodensystem der Elemente steht es in der 5. IUPAC-Gruppe, der Vanadiumgruppe. Alle Dubnium-Isotope sind radioaktiv.

Entdeckt wurde Dubnium 1967 von Georgi N. Flerow et al. am Vereinigten Institut für Kernforschung bei Dubna in der Nähe von Moskau bzw. 1970 von Albert Ghiorso et al. an der Universität von Kalifornien, Berkeley. Wie alle Transactinoide wird Dubnium ausschließlich durch Teilchenbeschuss künstlich hergestellt. Die sowjetische Arbeitsgruppe arbeitete dabei mit dem Beschuss von Americium durch Neon-Kerne und schlug dafür den Namen "Nielsbohrium" vor: 


</doc>
<doc id="5328" url="https://de.wikipedia.org/wiki?curid=5328" title="Unna">
Unna

Unna ist eine Große kreisangehörige Stadt und Kreisstadt des Kreises Unna im östlichen Ruhrgebiet im Land Nordrhein-Westfalen. Sie ist (nach Lünen) die zweitgrößte Stadt des Kreises Unna.

Unna liegt etwa 15 km östlich von Dortmund am östlichen Rand des Ruhrgebiets, am Westfälischen Hellweg und damit in der Hellwegbörde, die sich zwischen den Flüssen Ruhr und Lippe erstreckt. Das Stadtgebiet umfasst Höhenlagen von 65 m (Afferder Bach) bis zu 225 m (Hemmerder Schelk) über NN.

Die westlichen und nördlichen Ortschaften Massen und Königsborn sind ähnlich dem Ballungsraum Ruhrgebiet strukturiert und weisen zum Teil typische Zechensiedlungen auf. Afferde sowie die östlichen und südlichen Stadtteile Alte Heide, Uelzen, Mühlhausen, Lünern, Stockum, Westhemmerde, Hemmerde, Siddinghausen, Kessebüren und Billmerich haben hingegen eher ländlichen Charakter. Dies gilt auch für deren Verkehrs- und Wohnsituation.

Erste Siedlungen auf dem heutigen Stadtgebiet gab es seit der Jungsteinzeit. In römischer Zeit bestand in der Nähe eine germanische Siedlung. Ob diese, wie erst ab dem frühen Mittelalter gesichert, auch schon früher als Station am Hellweg diente, ist unsicher.

Unna wurde erstmals am 6. August 1032 in einer Urkunde des Kölner Erzbischofs Pilgrim erwähnt, der die Kirche in Unna mit anderen Besitztümern an den Abt Radolfus von Deutz übertrug. Auch die weltlichen Herren waren zunächst noch vom Kölner Erzbischof abhängig. Zu Beginn des 13. Jahrhunderts war Graf Friedrich von Altena-Isenberg vom Kölner Erzbischof neben anderen Gütern auch mit Unna belehnt. Nachdem er 1225 seinen Onkel, den Erzbischof Engelbert von Berg erschlagen hatte, wurde er hingerichtet, und sein Besitz fiel an seinen Vetter, Graf Adolf von Altena, der sich nach seiner neuen Residenz Burg Mark an der Lippe künftig Graf von der Mark nannte und auf den die Befestigung des Ortes zurückgeht. 1264 erstürmten erzbischöfliche Truppen Unna und brannten es nieder, worauf der Ort wiederaufgebaut wurde. Nach der Schlacht bei Worringen 1288, mit der sich die Grafen von der Mark von der kölnischen Vorherrschaft befreiten, bekam Unna vom Grafen von der Mark das Stadtrecht verliehen. Gleichzeitig erhielt Unna das Marktrecht und durfte die Niedere Gerichtsbarkeit ausüben. Das älteste Stadtsiegel sowie die früheste Erwähnung eines Rates datieren von 1290. Die Stadt wurde in den fortdauernden Auseinandersetzungen mit den Erzbischöfen von Köln mehrfach wieder teilweise niedergebrannt (so zum Beispiel 1303 und 1308), bei den anschließenden Wiederaufbauten jedoch vergrößert und stärker befestigt, so dass sie 1324 einen erneuten Angriff der erzbischöflichen Truppen abwehren konnte. Im späten Mittelalter wurde die Stadt von mehreren Bränden und kriegerischen Ereignissen heimgesucht, so auch ab 1419 im märkischen Bruderkrieg (1417–1461).

Trotz vieler Kriege erlebte die Stadt im 14. Jahrhundert einen beachtlichen Aufschwung, der auf die Kaufleute von Unna zurückgehen soll, die auch regen Fernhandel betrieben. Ab 1347 ließen die märkischen Grafen in Unna Münzen prägen. Vom Wohlstand der Stadt und ihrer Bewohner zeugt auch der um 1375 vergrabene „Goldschatz von Unna“, den man 1952 bei Ausgrabungsarbeiten fand und der aus über 70 Goldmünzen aus Deutschland, England, Frankreich, Böhmen und Ungarn besteht. Seit 1389 wurde im Gebiet des heutigen Königsborn Sole gefördert und daraus Kochsalz gewonnen. Im 15. Jahrhundert begann der Zusammenschluss der Handwerker und Kaufleute in Zünften. Von 1469 bis 1518 und wieder ab 1540 war die Stadt Unna Mitglied der Hanse. Unna wurde 1549 gemeinsam mit Hamm hansische Prinzipalstadt.

Gegen Ende des 16. Jahrhunderts endete wegen des Niedergangs der Hanse, der Glaubensspaltung und zahlreicher Kriege und Seuchen die wirtschaftliche Blüte. 1597 verlor Unna infolge der Pest mehr als die Hälfte seiner Einwohner. 1598 und 1604 waren im Zuge des Spanisch-Niederländischen Krieges spanische Reiter in Unna einquartiert. Mit dem Tod Herzog Johann Wilhelms von Kleve im Jahr 1609 starb die alte Linie der Grafen von der Mark aus, und die Grafschaft Mark mit der Stadt Unna fiel an Markgraf Ernst von Brandenburg und Pfalzgraf Wolfgang Wilhelm, deren Bündnis jedoch 1614 aufgrund von Glaubensunterschieden zerbrach. Unna und andere kleve-märkische Städte wurden 1614 von niederländischen Truppen eingenommen, die jedoch beim erneuten Ausbruch der Pest 1615 wieder abzogen. Im Dreißigjährigen Krieg litt die Stadt vielfach unter Belagerungen, Einquartierungen und Kontributionen, so zum Beispiel durch Niederländer (1622 und 1641), Spanier und Italiener (1622/23), kaiserliche Truppen (1628), „Pappenheimer“ (1628), Schweden (1633/34), Hessen (1634), Lüneburger (1635) und Kaiserliche (1636/41). Besonders die vielen Kontributionen und Einquartierungen waren „uber alle maßen beschwerlich“. Zudem wurden am 4. April 1640 durch „ein ungewöhnlich Erdbeben viele Häuser in der Stadt verdorben.“ Hatte Unna vor dem Dreißigjährigen Krieg jährlich 4.000 Reichstaler Einnahmen aus Höfen, Ländereien und Renteneinkünften zu verbuchen, so musste es währenddessen alle Güter veräußern und sich auch noch mit 30.000 Talern verschulden. Die wirtschaftliche Blütezeit des 14. bis 16. Jahrhunderts war für lange Zeit dahin.

1666 fiel Unna an Brandenburg-Preußen, das später im Königreich Preußen aufging. In Unna stand bis 1756 ein Regiment zu Fuß, das seinen Ersatz aus der Grafschaft Mark erhielt. Im Niederländisch-Französischen Krieg (1672–1679) ließ der französische Marschall Turenne die Stadt am 4. Februar 1673 beschießen, wodurch 220 Häuser zerstört wurden (etwa 54 % des Gesamtbestandes). 1678 und 1723 kam es zu weiteren Stadtbränden. Im Siebenjährigen Krieg (1756–1763) hatte Unna weitere Kampfhandlungen, Einquartierungen und Durchzüge zu erdulden; aber es hatte – verglichen mit den beiden vorhergehenden Kriegen – weniger unter der Willkür fremder Soldaten und Kontributionen zu leiden. Planmäßige Zerstörung war eine Ausnahme. Der Siebenjährige Krieg hatte in seinen Auswirkungen für Unna keine so dramatischen Folgen wie der Niederländisch-Französische Krieg (s. o.). Die Ziegelei der Stadt wurde zerstört und die Wasserleitung „größtenteils ruiniert“. Die Inflation wuchs bis 1761 auf das Dreifache gegenüber dem Vorkriegsstand und fiel dann 1763, bei Kriegsende, auf nur noch 145 % pro Jahr. Während des ganzen 18. Jahrhunderts stagnierte die Bevölkerungszahl: 1919 Bewohner (1719) und 2037 Bewohner (1796). Der Handel stagnierte, und Versuche, Fabriken anzusiedeln, scheiterten „beim gänzlichen Mangel des Wassers“. Die nach der Revolution von 1789 einsetzende Einwanderung aus Frankreich berührte auch Unna und setzte neue Impulse in Wirtschaft und Lebensart.

Im 18. Jahrhundert ein unbedeutendes Ackerbürgerstädtchen, gewann Unna erst durch die Industrialisierung und die Erhebung zur Amtsstadt wieder an Bedeutung. 1734 wurde durch den preußischen Staat die „Saline Königsborn“ gegründet, die später zur wichtigsten Saline in Westfalen wurde. 1799 wurde in Unna die erste Dampfmaschine in den Westprovinzen Preußens zur Soleförderung in Betrieb genommen. Mit dem Ausbau der Straße von Unna nach Kamen 1752, der Schiffbarmachung der Oberen Ruhr zwischen 1780 und 1801, dem Ausbau des Hellwegs 1817 und dem Anschluss an das bergisch-märkische Eisenbahnnetz 1855 erfolgte die Anbindung der Stadt an die neuen Zentren im Ruhrgebiet.

Nach dem Ende des Königreichs Preußen infolge der Napoleonischen Kriege wurde Unna am 21. Januar 1808 mit der Grafschaft Mark dem neu geschaffenen Großherzogtum Berg zugeschlagen und wurde Sitz der Munizipalität Unna, die mit anderen Munizipalitäten den Kanton Unna im Arrondissement Dortmund bildete. 1809 wurde im Zuge der Säkularisation das „Katharinenkloster“ in Unna aufgehoben. Nach Napoleons Niederlage zogen im November 1813 wieder die Preußen ein, die die französische Verwaltungsgliederung zunächst bestehen ließen und 1817 den Kreis Hamm durch den Zusammenschluss der Bürgermeistereien Hamm, Pelkum, Rhynern, Kamen, Unna und Fröndenberg neu gründeten. Durch Annahme der revidierten Städte-Ordnung von 1831 im Jahr 1837 schied Unna aus der ländlichen Bürgermeisterei aus. Das Amt Unna vereinigte sich 1844 mit dem Amt Kamen zum Amt Unna-Kamen.

Die Industrialisierung setzte 1870 mit dem Beginn der Steinkohleförderung in Massen, Billmerich, Ringebrauck und Königsborn ein. Auch Bergbauzulieferbetriebe sowie Fabriken für Maschinenbau und Metallverarbeitung, Ziegeleien und Brauereien siedelten sich an. Im Jahr 1860 wurde das städtische Gaswerk gegründet; in den Jahren 1887/1888 wurde das Ruhrwasserwerk erbaut.

Nach dem Ersten Weltkrieg besetzte im Frühjahr 1920 die Rote Ruhrarmee Unna für kurze Zeit. 1927 wurde die Verlegung der Kreisverwaltung des Kreises Hamm nach Unna beschlossen und nach Einweihung des neuen Kreishauses im September 1930 vollzogen. Am 17. Oktober wurde der Kreis Hamm schließlich in Kreis Unna umbenannt.

Zur Zeit des Nationalsozialismus wurde in Unna 1934 eine Führerschule der Allgemeinen SS errichtet, die wenig später zu einer allgemeinen Kaserne ausgebaut wurde. An der Kamener Straße entstand 1937 das Heereszeugamt für die Panzertruppen. Die 1933 noch aus 156 Personen bestehende jüdische Gemeinde wurde bis 1943 zur Auswanderung gezwungen oder ausgelöscht. 142 Unnaer Juden fielen – direkt oder indirekt – der NS-Gewaltherrschaft zum Opfer, z. B. in Theresienstadt, Belzec und Auschwitz. Während des Zweiten Weltkriegs wurde die Arbeitskraft der zum Wehrdienst eingezogenen Männer u. a. durch Zwangsarbeiter aus Frankreich, Polen, Südosteuropa und der Sowjetunion ersetzt. Die sogenannten „Ostarbeiter“ aus der Sowjetunion standen dabei auf der untersten Stufe. 1944 betrug ihre Zahl 1790, d. h. jeder zwölfte Bewohner der Stadt war damals Zwangsarbeiter, davon die Hälfte in der Landwirtschaft, die übrigen im Bergbau, in der Industrie, bei der Reichsbahn, beim Bau von Luftschutzanlagen und in Privathaushalten. Ihre Unterbringung erfolgte teils auf den Höfen, teils in bewachten Barackenlagern, die vor allem für „Ostarbeiter“ und Polen errichtet wurden. Die Situation der Zwangsarbeit war im Allgemeinen gekennzeichnet durch lange Arbeitszeiten, geringe Entlohnung, miserable Verpflegung und schlechte Behandlung. 1943 verzeichnete der städtische Verwaltungsbericht 416 „flüchtige und kontaktbrüchige Ausländer“, die festgenommen und der Gestapo übergeben wurden; das waren mehr als ein Drittel der damals 1133 Zwangsarbeiter. In den letzten beiden Kriegsjahren kam es zu mehreren Luftangriffen auf Unna, die überwiegend dem militärisch bedeutsamen Heereszeugamt galten, aber auch große Zerstörungen in der Stadt verursachten. Der erste Großangriff erfolgte am 19. September 1944, weitere große Angriffe folgten am 16. und 28. Februar sowie am 5. und 23. März 1945. Dieser letzte galt den Reichsbahnanlagen und dem Messingwerk, dauerte nur 15 Minuten und tötete 113 Menschen. In den letzten 101 Kriegstagen (1. Januar bis 11. April 1945) wurde die Stadt 17-mal angegriffen, wodurch 174 Menschen getötet und mehr als 1200 Häuser zerstört oder beschädigt wurden. Beim Näherrücken der Front wurde Unna Lazarettstadt, die dem Gegner kampflos hätte übergeben werden sollen. Allerdings wurden die Verwundeten in letzter Minute nach Hemer überführt. Um die Stadt entbrannte ein mehrtägiger Kampf, der erst mit der Einnahme durch Truppen der aus östlicher Richtung kommenden 95. US-Infanterie-Division am 11. April 1945 beendet wurde.

In der Nachkriegszeit nahm Unna bis 1. Januar 1950 rund 2260 Flüchtlinge und Vertriebene auf. Unna wurde nach Auflösung des Landes Preußen durch Gesetz des Alliierten Kontrollrats 1947 Teil des neuen Bundeslandes Nordrhein-Westfalen. Die frühere SS-Kaserne wurde bis April 1956 von belgischen Truppen benutzt; anschließend zog dort die Bundesgrenzschutzabteilung Eschwege ein. 1956 hatte Unna erstmals mehr als 30.000 Einwohner.

Mit der Schließung der Schachtanlagen während der Strukturkrise im Ruhrgebiet in den 1960er Jahren endete die durch den Bergbau geprägte Entwicklung. Seitdem wandelt sich die Stadt zu einem Dienstleistungszentrum.

Am 1. Januar 1968 wurden die bisherigen Gemeinden Afferde, Billmerich, Hemmerde, Kessebüren, Lünern, Massen (am 1. April 1911 als Zusammenschluss aus den damaligen Gemeinden Niedermassen und Obermassen gebildet), Mühlhausen, Siddinghausen, Stockum, Uelzen und Westhemmerde mit der Stadt Unna zusammengeschlossen. Das bisherige Amt Unna-Kamen wurde aufgelöst.

Bis Anfang des 19. Jahrhunderts war Unna eine Stadt mit nur wenigen tausend Einwohnern. Die Bevölkerung war im Laufe der Jahrhunderte nur langsam gewachsen und ging infolge zahlreicher Kriege, Seuchen und Hungersnöte immer wieder zurück. So starben 1597 beim Ausbruch der Pest von 2500 Bewohnern 1400. Auch im Dreißigjährigen Krieg (1618–1648) sank die Einwohnerzahl. Erst mit dem Beginn der Industrialisierung beschleunigte sich das Bevölkerungswachstum. Lebten 1818 erst 2400 Menschen in der Stadt, so waren es 1900 bereits 15.000. Bis 1956 verdoppelte sich diese Zahl auf 30.000.

Einen Zuwachs um 18.000 Personen auf 50.000 Einwohner brachten die Eingemeindungen vom 1. Januar 1968. Im Jahre 2000 erreichte die Bevölkerungszahl mit 71.375 ihren historischen Höchststand. Am 31. Dezember 2010 betrug die amtliche Einwohnerzahl für Unna nach Fortschreibung des Landesamtes für Datenverarbeitung und Statistik Nordrhein-Westfalen 66.502 (nur Hauptwohnsitze nach Abgleich mit den anderen Landesämtern). Die in der Volkszählung „Zensus 2011“ ermittelte Einwohnerzahl von 59.145, bezogen auf Ende 2011, weicht mit einer Differenz von über 7000 Einwohnern oder elf Prozent signifikant von der vorgenannten, amtlichen Zahl von 66.502 Einwohnern ab. Die Stadt Unna hat Zweifel an dem Ergebnis der Volkszählung und hat, gemeinsam mit mehreren anderen Städten in NRW, gegen die Ergebnisse der Volkszählung auf Basis Zensus 2011 geklagt. Allerdings ist dieser festgestellte deutliche Einwohnerschwund, setzt man andere vergleichbare Parameter an, nicht erklärbar, es sei denn, dass Zu- und Abgänge des Aufnahmelagers Unna-Massen sowie die Tatsache, dass Unna Bundeswehrstandort (Fluktuation/unterschiedliche Wohnorte der Soldaten) war und bleibt, die Zahlen deutlich positiv oder negativ beeinflusst haben bzw. beeinflussen. Das Ergebnis der Klage bleibt abzuwarten.

Mit der für 2013 angegebenen Einwohnerzahl von 58.856 hat Unna rechnerisch bereits 5.556 Einwohner weniger als die vom IT.NRW erst für 2025 prognostizierte Zahl von 64.412.

In ihrer Studie von 2009 prognostiziert die Bertelsmann-Stiftung für Unna einen Rückgang der Bevölkerung von derzeit rd. 67.000 Einwohnern auf rd. 44.000 Einwohner im Jahr 2030. Das entspricht einem realen Rückgang von 23.000 Einwohnern (oder jährlich 1000) bzw. 34 %. Dieser dramatische Rückgang wird ausschließlich für Unna vorhergesagt; in anderen Ruhrgebietsstädten liegt der prozentuale Rückgang im Vergleichszeitraum bei durchschnittlich nur fünf bis zehn Prozent.

Neuere Prognosen zeigen für Unna weiterhin gravierende Differenzen. So sollen es laut IT.NRW auf Basis des Zensus 2011 im Jahr 2030 63.513 Einwohner sein, während die Bertelsmann-Stiftung in ihrer Prognose von Juli 2015 für 2030 mit 54.250 Einwohnern rechnet – mithin ein Unterschied von fast 10.000 Einwohnern. Es ist zu vermuten, dass der in der Bertelsmannstudie von 2009 vorhergesagte Rückgang der Einwohnerzahl auf nur rd. 44.000 so nicht eintreffen wird.

Die folgende Übersicht zeigt die Einwohnerzahlen nach dem "jeweiligen" Gebietsstand. Bis 1819 handelt es sich meist um Schätzungen, danach um Volkszählungsergebnisse (¹) oder amtliche Fortschreibungen des Statistischen Landesamtes. Die Angaben beziehen sich ab 1871 auf die „ortsanwesende Bevölkerung“, ab 1925 auf die Wohnbevölkerung und seit 1987 auf die „Bevölkerung am Ort der Hauptwohnung“. Vor 1871 wurde die Einwohnerzahl nach uneinheitlichen Erhebungsverfahren ermittelt.

¹ Volkszählungsergebnis

Zusammensetzung des Stadtrats nach dem derzeitigen Stand:
Ergebnisse der Kommunalwahlen ab 1975

In der Liste werden nur Parteien und Wählergemeinschaften aufgeführt, die bei der jeweiligen Wahl mindestens 1,95 Prozent der Stimmen erhielten.

Derzeitiger Bürgermeister (2017) ist seit 2004 Werner Kolter (SPD).

Ehemalige Bürgermeister (bis 1917)

Ehemalige Bürgermeister (ab 1926)

Ehemalige Stadtdirektoren

Die Stadt Unna führt das mit Urkunde des Innenministers des Landes Nordrhein-Westfalen vom 13. November 1967 genehmigte Wappen:
Beschreibung der Flagge: „Die Stadtflagge zeigt die Farben rot und weiß. Sie kann das Stadtwappen enthalten.“

Die Stadt Unna führt Dienstsiegel mit dem Stadtwappen. Das Dienstsiegel gleicht in Form und Größe dem der Hauptsatzung beigedruckten Siegel.

Unna unterhält partnerschaftliche Beziehungen mit den folgenden Städten:

Des Weiteren unterhält die Kreisstadt Unna eine partnerschaftliche Beziehung zu der Stadt Enkirch an der Mosel sowie zu dem in Unna stationierten Logistikbataillon 7 der Bundeswehr.

Die Burg Unna ist seit 1936 Sitz des Hellweg-Museums. In der Ortschaft Kessebüren besteht das Ernst-Oldenburg-Haus mit einer ständigen Ausstellung von Werken des Malers und Bildhauers Ernst Oldenburg, der hier die letzten Jahre seines Lebens lebte und arbeitete. In Stockum gibt Horst Höfers privates Bergbaumuseum „Fröhliche Morgensonne“ einen anschaulichen Einblick in die Bergbaugeschichte des östlichen Ruhrgebiets und das Leben der Bergleute in der ersten Hälfte des 20. Jahrhunderts.

Der Kunstverein Unna begann in der Mitte der 1960er Jahre, zeitgenössische Kunst des Ruhrgebietes und Künstler aus dem europäischen Ausland zu zeigen. Er war bis in die Mitte der 1970er Jahre kultureller Mittelpunkt der Kunstszene der Region.

Im Mai 2001 ist in Unna auf dem Gelände der ehemaligen Lindenbrauerei das Zentrum für Internationale Lichtkunst eröffnet worden. Seit 2002 ist die Stadt Unna im Kulturprojekt "Hellweg – ein Lichtweg" vertreten.

Die "Evangelische Stadtkirche" ist eine spätgotische Hallenkirche, die ab 1322 errichtet wurde. Der Grundstein des Chors datiert auf 1389. Er soll im Jahr 1396 vollendet worden sein. Der Turm wurde im 15. Jahrhundert erbaut. Der Turmhelm wurde nach Zerstörung durch Blitzschlag nach Plänen des Kölner Dombaumeisters Ernst Friedrich Zwirner 1863 neu errichtet. Die Kirche war von 1596 bis 1601 Wirkungsort des Liederdichters Philipp Nicolai. An der Stadtkirche befand sich einst noch eine 1501 errichtete Marienkapelle, die jedoch zu Beginn des 19. Jahrhunderts abgerissen wurde. Außerdem bestand in Unna neben der lutherischen Stadtkirche noch die reformierte Hospitalkirche, die jedoch nach der Vereinigung der lutherischen und der reformierten Gemeinde Unna um 1820 verkauft und später abgerissen wurde.

Nachdem die Stadt Unna in der zweiten Hälfte des 16. Jahrhunderts evangelisch geworden war, konnte sich erst gegen Ende des 17. Jahrhunderts wieder eine katholische Gemeinde bilden. Ihr wurde durch einen Vergleich zwischen dem Kurfürsten von Brandenburg und dem Pfalzgrafen von Neuburg im Jahr 1672 die Klosterkapelle an der heutigen Klosterstraße zugesprochen. Sie war um 1468 erbaut worden und nach Katharina von Alexandrien benannt. Sie diente von 1683 bis 1848 als katholische Pfarrkirche; danach war das Gebäude bis zur Pogromnacht am 9. November 1938 die Unnaer Synagoge; heute befindet sich darin eine Druckerei. Die zweite Katharinenkirche wurde 1848 südwestlich der heutigen Kirche an der späteren Bahnhofstraße errichtet. Sie wurde jedoch schon um 1900 wegen des starken Wachstums der Gemeinde zu klein. Die Planungen für einen Neubau zogen sich bis 1933 hin.

Das am Nordrand des Stadtzentrums stehende Kirchengebäude wurde von 1933 bis 1934 nach Plänen der Architekten Flerus und Konert aus Dortmund als doppeltürmiger Wandpfeilerbau im Stil der Neoromanik errichtet. Die nüchternen Wände aus Stahlbeton sind mit einer Fassade aus grünlichem Anröchter Dolomit verkleidet. Der Grundstein wurde am 26. November 1933 gelegt; die Konsekration erfolgte am 7. Oktober 1934 durch den Paderborner Erzbischof Caspar Klein. Die beiden Großplastiken über dem Westportal sind Arbeiten des Bildhauers Josef Baron; sie zeigen die Heiligen Bonifatius und Petrus Canisius. Vom selben Künstler stammt ein Relief hoch am südlichen Turm, das den „Unnaer Esel“ zeigt.

Im Jahr 2002 wurde eine umfangreiche Sanierung erforderlich, bei der auch die Ausgestaltung des Innenraumes neu geplant wurde. Der klare Charakter der Architektur sollte erhalten werden, die Stimmung des Innenraumes verbessert werden. Unter der Leitung von Thomas Kesseler wurde ein Konzept entwickelt, das die Ausstattungsstücke in einen neuen Zusammenhang brachte und ergänzte. Die Alabasterfigur der Hl. Katharina, die zuvor lange Zeit im Diözesanmuseum in Paderborn gestanden hatte, erhielt einen neuen Platz in einer eigens dafür gebauten roten Kapelle. Die vorherige Trennung von Altar und Gemeinderaum sollte aufgelöst werden; der Altarraum fand seinen Platz in den ersten beiden Achsen des Langhauses, um vier Stufen erhöht. Die auskragenden Stufen bestehen aus Anröchter Steinplatten, die 1,00 × 1.00 Meter groß sind. Sie korrespondieren mit den Raumabmessungen, den Seitenschiffmaßen und den Pfeilern der Kirche. Es wurde die Idee einer Wegekirche aufgegriffen, so dass Tabernakel, Altar, Taufstein und Hauptportal eine Mittelachse bilden. Ein zwei Meter breiter Streifen aus grünem Dolomit unterstreicht dies. Zwei neun Meter hohe und drei Meter breite Glasbilder mit Schwarzrotbemalung sind im Bereich der ehemaligen Altarebene an Flachstahl aufgehängt. Sie weisen Elemente der frühchristlichen Apsismosaiken auf; im Hintergrund werden Arkanthusornamente durch freie Malereien überlagert, die eine kosmische Lichtvision erzeugen sollen. Vor diesen Malereien hängt frei im Raum ein Kreuz. Als weitere Anlehnung an frühchristliche Raumgestaltung wurden 2,5 Meter hohe, weiße Wandblöcke gemauert, die als Chorschranken dienen. In den oberen Chorschranken befinden sich zwei Glasbilder, die Maria und Johannes den Täufer beim Gebet zeigen. Sie bilden zusammen mit der Darstellung der Kreuzigung eine sogenannte "Deesis Gruppe"; dieser Bildtypus wies schon in der byzantinischen Kunst auf das Weltgericht hin. Der Innenraum ist zurückhaltend in hellen Mineralfarben gefasst; die Holzdecke zeigt verschiedene intensiv gehaltene Blaugrautöne auf. Die Seitenkapellen und die Werktagskapelle sind nach der Palette der Farbklaviatur von Le Corbusier in Blau-, Rot- und Gelbtönen gehalten. Die ehemaligen Türen der Windfänge fanden keine Wiederverwendung; sie wurden durch eine Glaswand unter der Orgelempore, die eine Vorhalle bildet, ersetzt. Die Glaswand ermöglicht gleichzeitig die Öffnung der Kirche und die Sicht auf den Innenraum, ohne Zutritt zu gewähren. In der Anbetungskapelle sind die Wandfelder in gelben Farbabstufungen gehalten. Auf dem gläsernen Mittelportal ist die Inschrift "„Tritt ein in diesen Raum. Lass dich berühren vom wahren Licht, zu dem Christus die Pforte ist“" zu lesen.

Die evangelische "Christuskirche" im Stadtteil Königsborn ist eine in den Jahren 1904/1905 errichtete Saalkirche in neuromanischen, vom Jugendstil beeinflussten Formen.

"Eine vollständige Liste der registrierten Bau- und Bodendenkmäler mit Fotos enthält die Liste der Baudenkmäler in Unna."





Der Eselsbrunnen wurde 1978 als Ersatz für den alten Marktbrunnen nach einem Entwurf von Josef Baron errichtet. Der Esel ist kein Wahrzeichen im eigentlichen Sinn, hat sich aber als Symbolfigur für Unna durchgesetzt. Der Ursprung dürfte in sogenannten Ortsübernamen liegen, die im 19. Jahrhundert in Westfalen weit verbreitet waren, wie beispielsweise (plattdeutsch) „Kömsche Bleier“ für die benachbarten Kamener, hier eben „Iasel Unna“. 1924 entstand aus dieser Tradition eine erste offizielle Eseldarstellung am Rathaus am Markt, 1934 ein Relief am Turm der neu erbauten Katharinenkirche, das die Auseinandersetzung der katholischen Kirchengemeinde mit der nationalsozialistischen Verwaltung um den Bauplatz symbolisieren soll. Mit einer Skulptur im Stadtpark 1954 (ebenfalls Josef Baron), dem Stadtbrunnen und zuletzt 2009 einer Tierparade pflegt Unna diese Tradition.

Als die Aluminiumwerk Unna AG 1989 ihr 75-jähriges Bestehen feierte, schenkte das Unternehmen der Stadt die von Hans-Oiseau Kalkmann entworfene Wasserskulptur, die der Bildhauer mit den Auszubildenden in einer viermonatigen Kontakt-Kunst-Aktion im Werk erstellte. Die Skulptur ist ein Hinweis auf den an dieser Stelle verrohrten Kortelbach. Sie steht am Innenstadtring, wo sich früher das Wassertor befand. Daran erinnert auch der Name der Wasserstraße.

In Unna fand von 1956 bis 2011 alljährlich an Weiberfastnacht, angeführt von Helmut Scherer, der kleinste Karnevalsumzug der Welt statt. Er bestand nur aus einem einzigen handgezogenen Bollerwagen.

Regelmäßige Veranstaltungen leiten sich her einerseits aus Tradition und Brauchtum, andererseits aus innovativer und aktueller Programmplanung unterschiedlicher Veranstalter in städtischer und anderer Trägerschaft.

Nach umfassender Bestandsaufnahme und gemäß einem Gesamtkonzept für die Bereiche Bildung-Kultur-Freizeit in der Stadt Unna wurden Grundlagen geschaffen für die Förderung der mannigfaltigen sozial-kulturellen Aktivitäten und Initiativen an möglichst vielen Plätzen der Stadt mittels wirkungsvoller Bündelung und Vernetzung zu einem kulturellen „Informationszentrum für alle“, zu einem „Forum für zeitgenössische Künste“. Es gibt eine bemerkenswerte Vielfalt kultureller Veranstaltungen: Regelmäßige Ausstellungseröffnungen des "Kunstvereins" Unna, im städtischen "Hellweg-Museum" sowie durch die von der Stadt Unna eingerichtete und nach dem Unnaer Künstler Carlernst Kürten, einem der bekanntesten Bildhauer der Region des Ruhrgebietes, benannte "Carlernst Kürten-Stiftung" in Unna-Alte Heide, die Konzerte der "Philipp-Nicolai-Kantorei", des "Musikvereins" der Stadt Unna und der diversen "Chöre" des regen Sängerkreises, die "Jugendkonzerte" der Partnerstädte Palaiseau und Unna, "Kindermalfeste" auf den Marktplätzen, "Festprogramme" der Sportvereine, "Vormieten" der Kulturverwaltung, besondere "Filmreihen" der Volkshochschule, zudem die mit Rückenwind des Kulturausschusses eingerichtete Reihe "JAZZ AKTUELL", die regelmäßigen Veranstaltungen von in der Stadt befindlichen Einrichtungen wie der "Internationalen Komponistinnen-Bibliothek" und dem "Internationalen Zentrum für Lichtkunst" sowie von anderen überregional bedeutsamen Institutionen, beispielsweise dem "Westfälischen Literaturbüro in Unna e. V." mit seiner internationalen Biennale "Mord am Hellweg", überdies das Veranstaltungsprogramm der "Kulturbetriebe der Kreisstadt Unna" (beispielsweise jährlich: "Summertime (Kultursommer Open Air)").

Alle zwei Jahre findet „Un(n)a Festa Italiana“, das größte italienische Fest nördlich der Alpen, in der Innenstadt statt. Im Jahr 2001 fanden etwa 200.000 Besucher den Weg nach Unna, 2007 wurden etwa 250.000 Gäste erwartet. Besondere Attraktion sind aufwendige Illuminationen aus Bari.

An jedem ersten Wochenende im September findet in Unna das Stadtfest statt. Auf bis zu sieben Bühnen werden im gesamten Innenstadtbereich verschiedene Musikrichtungen und Künstler präsentiert. Neben lokalen Größen wie die Gruppe Cantus Brutalis treten auch bekannte Musikgruppen wie die Kölner Gruppe Brings (2004) auf. Das Fest wird am Freitagabend mit dem Altbierabend und dem traditionellen Fassanstich durch den Bürgermeister eröffnet.

Unna war bis Mitte des 19. Jahrhunderts noch eine überwiegend landwirtschaftlich geprägte Stadt. 1828 wurden Bierbrauereien, Branntweinbrennereien, Töpfereien und eine Ziegelfabrik als „Nahrungszweige“ der Stadt genannt. Daneben hatte die Salzgewinnung in Königsborn große Bedeutung. Ab Beginn der Industrialisierung hat sich – vor allem durch den Kohlebergbau – ein Wechsel zum Industriestandort vollzogen. Im Gegensatz zu den meisten Ruhrgebietsstädten sind in Unna die meisten Arbeitsplätze nach wie vor in der Industrie (Eisen- und Metallverarbeitung, Maschinenbau) oder im Handwerk angesiedelt. Seit 1972 betreibt VDM Metals im Stadtteil Königsborn ein Schmelzwerk für Nickellegierungen und Sonderedelstähle. Das Unternehmen beschäftigt in Unna rund 450 Mitarbeiter. Als bedeutender Arbeitgeber gilt auch die Unternehmenszentrale und das Möbelhaus des Unternehmens Zurbrüggen Wohnzentrum. Im Bereich der Elektronik ist die Firma WIMA in Unna auf dem Gebiet der Folienkondensatoren Weltmarktführer. Dies ist für Unternehmen im Elektronikbauteilesektor mit Standort Deutschland eine Rarität.

Ein zunehmend an Bedeutung gewinnender wirtschaftlicher Schwerpunkt ist der Logistikbereich. Das ehemalige Karstadt-Zentrallager (heute "DHL"), das Zentrallager des Pumpenherstellers WILO SE, ein Verteilzentrum der DPD und das Metro-Hochregallager Varena sind die bekanntesten Vertreter dieser Branche. Ein weiterer großer Logistikkomplex der DHL wurde 2008 in Betrieb genommen.

Daneben ist Unna aber auch Wohnort für viele Pendler, die in benachbarten Ruhrgebietsstädten arbeiten.

Der Bahnhof Unna liegt an den Bahnstrecken Hagen–Hamm und Dortmund–Soest, und er ist die Endstation der nördlichen Verlängerung der Hönnetalbahn über Fröndenberg und Menden nach Neuenrade. Er ist außerdem Endpunkt der S-Bahn-Linie 4 der S-Bahn Rhein-Ruhr.
Weitere Bahnhöfe bzw. Haltepunkte im Stadtgebiet von Unna sind Lünern und Hemmerde an der Strecke nach Soest sowie Unna West, Unna-Königsborn und Massen an der S-Bahn-Linie nach Dortmund.

Unna besitzt ein Stadtbusnetz (StadtBus Linien C40–C45). Dieses wird, wie auch die Anbindung an die Nachbargemeinden, durch die Verkehrsgesellschaft Kreis Unna betrieben.

Unna liegt direkt an den Bundesautobahnen 1 (Köln–Bremen) sowie 44 (Dortmund–Kassel) und ist so sehr gut an das Autobahnnetz angebunden.

Die Bundesstraße 1 durchquert das Stadtgebiet in West-Ost-Richtung, die B 233 verlief ursprünglich von Werne über Unna in Richtung Iserlohn. Der Abschnitt von der Unnaer Stadtmitte bis zur Autobahnauffahrt Kamen wurde zu einer Landesstraße (L 678) zurückgestuft.

Westlich von Unna befindet sich der Flughafen Dortmund "(Dortmund Airport 21)".
Von hier aus werden zahlreiche Ziele in Osteuropa sowie in der Türkei angeflogen. Weitere Verbindungen sind vorhanden mit Palma de Mallorca und London-Stansted. Im Inland wird nur München angeflogen. Der Platz wird von zahlreichen Geschäfts- und Privatfliegern genutzt.

Die Einflugschneise aus östlicher Richtung verläuft quer über das Stadtgebiet von Unna. Die Start-/Landebahn endet/beginnt wenige hundert Meter von der Stadtgrenze (Stadtteil Unna-Massen) entfernt.
Mehrere Verlängerungen der Startbahn und Ausweitungen von Start- und Landezeiten wurden und werden von vorwiegend Unnaer Bürgern, die sich in einer „Schutzgemeinschaft Fluglärm“ zusammengeschlossen haben, bekämpft.

Unna beherbergt die Glückauf-Kaserne, den Standort des Logistikbataillons 7 der deutschen Bundeswehr.

Das Amtsgericht Unna ist das für die Gemeinden Unna, Fröndenberg, Bönen und Holzwickede zuständige Amtsgericht. Das Schöffengericht des AG Unna ist auch für die Stadt Kamen zuständig.

Auf dem Gelände der ehemaligen Lindenbrauerei sind im Zentrum für Information und Bildung (ZIB) verschiedene Einrichtungen der Stadt Unna zusammengefasst. Unter anderem befinden sich hier die Volkshochschule, die Stadtbibliothek und das Stadtarchiv seit 2004 unter einem Dach.

Unna weist ein fast vollständiges Angebot an allgemeinbildenden Schulen auf. Unna hat insgesamt elf Grundschulen. Das Angebot an weiterführenden Schulen umfasst zwei Gesamtschulen, zwei Realschulen, drei Gymnasien, eine Förderschule (Schwerpunkte Lernen sowie Emotionale und soziale Entwicklung, Sekundarstufe I) und ein Weiterbildungskolleg in Form einer Abendschule (Realschule und Gymnasium). Als Besonderheiten sind ein bilingualer Zweig am Geschwister-Scholl-Gymnasium sowie ein Waldorfkindergarten zu nennen. Unna hat ebenfalls ein Berufsschulzentrum. Die drei Kollegs haben Schwerpunkte in den Bereichen Handwerk und Technik (Hellweg Berufskolleg), Hauswirtschaft, Sozial- und Gesundheitswesen (Märkisches Berufskolleg) sowie Handel, Wirtschaft und Verwaltung (Hansa Berufskolleg).

In Unna gibt es zwei Krankenhäuser der Grundversorgung, das Katharinen-Hospital, im Volksmund auch „Katholisches Krankenhaus“ genannt, und das Evangelische Krankenhaus.

Zudem ist das Lebenszentrum Königsborn ein überregional bedeutsamer Träger im Gesundheitswesen, unter anderem mit einem der größten Sozialpädiatrischen Zentren Deutschlands und der Klinik für Neuro- und Sozialpädiatrie. Wegen der überregionalen Bedeutung wurde diese Einrichtung unter anderem schon von Angela Merkel und Bundespräsident Horst Köhler besucht.

Unna ist eine relativ waldarme Stadt; aber es gibt in der Massener Heide, im Bornekamptal, im Kurpark und in der Uelzener Heide / Mühlhauser Mark einige stadtnahe Erholungsgebiete, die zu Fuß oder mit dem Fahrrad leicht erreichbar sind. In Massen kann eine Beachvolleyball­anlage von jedermann nach Absprache angemietet werden. Weiterhin existieren eine Eissporthalle, zwei Freibäder, Skateanlagen, ein städtisch betriebenes sowie ein durch einen Trägerverein (HSM) vor der Schließung bewahrtes Hallenbad. Auch dieses in Massen (auf dem Gelände der Realschule an der Königsborner Straße) gelegene Hallenbad kann von privater Seite angemietet werden; sonntags werden unentgeltliche Schwimmzeiten angeboten. Bei den Sportvereinen dominiert der Fußball; jedoch gibt es auch Schwerpunkte beim Hallenhandball, Eishockey, Radrennsport und beim Volleyball. Darüber hinaus werden auch Kampfsportarten wie Judo, Jiu-Jitsu, Karate, Boxen und viele mehr für alle Altersgruppen in verschiedenen Vereinen angeboten.

Die Golf-Clubs Unna-Fröndenberg e. V. (GCUF) und Gut Neuhaus – beide jeweils auf Fröndenberger Gebiet, ca. 5 km von der Unnaer Stadtgrenze entfernt – haben ihren Einzugsbereich schwerpunktmäßig in den Regionen Unna und Fröndenberg.

In Unna gibt es fünf Tennisvereine mit insgesamt 29 Plätzen und ca. 850 Mitgliedern (Stand der angegebenen Mitgliedszahlen: Frühjahr 2016):

Konzerte und andere größere Veranstaltungen finden in der Erich-Göpfert-Stadthalle und in der Lindenbrauerei statt.







</doc>
<doc id="5331" url="https://de.wikipedia.org/wiki?curid=5331" title="Ural (Begriffsklärung)">
Ural (Begriffsklärung)

Ural heißen folgende geographischen Objekte:
Ortschaften:
Ural heißen außerdem:



</doc>
<doc id="5332" url="https://de.wikipedia.org/wiki?curid=5332" title="U-Boot">
U-Boot

Ein U-Boot (kurz für Unterseeboot; militärische Schreibweise Uboot ohne Bindestrich) ist ein Boot, das für die Unterwasserfahrt gebaut wurde. Moderne große U-Boote, die eine Masse bis zu 26.000 Tonnen haben können, werden auch U-Schiffe genannt.

Der Ausdruck "U-Boot" bezeichnet speziell ein militärisch verwendetes Unterwasserboot. Zivile U-Boote, ob kommerziell oder für die Forschung, werden meist als "Tauchboot" bezeichnet.

Der Wunsch des Menschen, länger und tiefer zu tauchen als es seine Lungenkapazität zulässt, ist etwa ebenso alt wie der Wunsch zu fliegen. Deswegen beschäftigten sich schon immer Menschen damit, entsprechende Vorrichtungen oder Instrumente zu entwickeln, die dies ermöglichen sollten. Aus der Antike liegen diesbezüglich Berichte von Aristoteles und Plinius dem Älteren vor. Selbst Alexander der Große soll bereits Tauchversuche im Mittelmeer unternommen haben (siehe Tauchglocke). Detailliertere Beschreibungen eines „Colymphas“ (griechisch für „Taucher“) genannten und für militärische Zwecke geeigneten Unterseebootes stammen aus dem 7./8. Jahrhundert von Pseudo-Hieronymus in seiner Aethicus zugeschriebenen Kosmographie, einer Mischung aus Fakten, Mythen, technischen und geographischen Ausführungen sowie christlichen Weisheiten.

Eine jüngere Beschreibung eines Tauchfahrzeugs in einer Erzählung befindet sich im etwa 1180/90 entstandenen Heldenepos „Salman und Morolf“.

Eine frühe technische Zeichnung eines U-Bootes stammt von Guido da Vigevano, der Ende des 13. Jahrhunderts geboren wurde, sodass diese aus dem frühen 14. Jahrhundert stammen dürfte.

Die Geschichte des technisch geprägten Tauchens bzw. der Entwicklung eines Tauchboots begann mit dem 15. Jahrhundert. So entwarf beispielsweise 1405 der Nürnberger Kriegsbaumeister Konrad Kyeser in seinem Werk "Bellifortis" einen ersten Tauchanzug. Roberto Valturio zeichnete 1472 sein U-Boot und Leonardo da Vinci zeichnete 1515 ein Ein-Mann-Tauchboot.

Diese Ideen wurden weiter vorangetrieben und 1604 beschrieb Magnus Pegel erstmals in einem Buch die Grundgedanken und Voraussetzungen für den Bau eines Tauchbootes. Der niederländische Erfinder Cornelis Jacobszoon Drebbel war der erste, der über die bloße Theorie hinausging und im Jahre 1620 das erste manövrierbare Unterwasserfahrzeug – ein mit Leder überzogenes Holzruderboot – baute.

Das Rotterdammer Schiff war das erste für den militärischen Einsatz konzipierte Tauchboot der Geschichte. Es wurde im Jahre 1653 vom Franzosen De Son im südholländischen Rotterdam konstruiert.

Im Auftrag des Landgrafen Karl von Hessen-Kassel konstruierte 1691 der französische Physiker Denis Papin, der auch Professor an der Philipps-Universität Marburg war, ein Tauchboot, dessen erstes Exemplar jedoch 1692 in der Fulda in Anwesenheit einer großen Schar von Schaulustigen zerstört wurde. Der Zweitversuch wies mit einer brennenden Kerze, die brennend wieder auftauchte, vermeintlich nach, dass genügend Atemluft für Menschen im Boot vorhanden sei. Trotz der Fehlschläge hatte die Idee, ein funktionstüchtiges Unterwasserfahrzeug zu bauen, inzwischen weltweit Tüftler motiviert. 1772 wurde im Steinhuder Meer das erste Unterwasserfahrzeug in Deutschland getestet. Es bestand aus Holz und hatte die Form eines Fisches, weshalb es den Namen "Hecht" erhielt. Mit dem Boot wurde etwa zwölf Minuten getaucht. Der US-Amerikaner David Bushnell baute 1776 die "Turtle" („Schildkröte“), eine Konstruktion aus Eisen und Eichenholz. Sie gilt als erstes richtiges U-Boot, da als Antrieb zwei über Handkurbeln betriebene Schrauben dienten – im Gegensatz zu ihren beiden Vorläufern, die von Segeln oder mit Rudern an der Wasseroberfläche angetrieben wurden. 1799 beschrieb der Bergmeister Joseph von Baader eine Konstruktion für ein Zwei-Mann-U-Boot.

Die Erfindung von Akkumulator und Elektromotor ermöglichte einen Unterwasserantrieb, der von Muskelkraft unabhängig ist. Auch die industrielle Produktion von Stahl leistete einen wichtigen Beitrag zum Fortschritt des U-Boot-Baus, indem sie einen enorm haltbaren Baustoff an Stelle des leichten und gegenüber Verfall und Parasiten anfälligen Holzes setzte. Darüber hinaus stand mit der Erfindung des Torpedos durch Giovanni Luppis im Jahre 1860 auch eine brauchbare Waffe für den Einsatz von U-Booten aus zur Verfügung.

Insgesamt ermöglichte somit der technische Fortschritt der Industrialisierung den Wandel des U-Bootes zu einem auch für die Marinen kleiner Staaten interessanten und brauchbaren Fahrzeug.

Der US-Amerikaner Robert Fulton entwarf 1801 das U-Boot "Nautilus". Es besaß einen Handkurbelantrieb für eine Schraube, neu allerdings waren nun Ruder zur Seiten- und Tiefensteuerung sowie ein Druckluftsystem zur Versorgung der vierköpfigen Besatzung mit Atemluft. Die "Nautilus" erregte sogar die Aufmerksamkeit Napoleons, galt aber schließlich für militärische Einsätze als zu langsam.

Am 18. Dezember 1850 ließ der bayerische Artillerie-Unteroffizier Wilhelm Bauer in Kiel das erste in Deutschland gebaute U-Boot, den sogenannten "Brandtaucher", zu Wasser. Da der Entwurf unter enormem Kostendruck gebaut wurde, war auf den Einbau von Tauchzellen verzichtet worden. Der Tauchvorgang sollte durch das Fluten von Wasser in das Boot erfolgen. Beim ersten Tauchversuch am 1. Februar 1851 in der Kieler Innenförde verschob sich jedoch der Ballast nach achtern, wobei das geflutete Wasser ebenfalls ins Heck floss. Das Boot sackte daraufhin durch, und weiteres Wasser drang durch die Nähte der Außenhaut und das Einstiegsluk. Das Boot sank bis auf den Grund in sieben Metern Wassertiefe. Die dreiköpfige Besatzung, unter ihnen Wilhelm Bauer, wartete, bis der Innendruck so groß war wie der Außendruck, öffnete das Einstiegsluk und trieb an die Oberfläche, wo sie gerettet wurde. Der verunglückte "Brandtaucher" wurde erst am 6. Juli 1887 geborgen. Nach verschiedenen Museums-Stationen hat das Tauchboot nun seine Heimat im Militärhistorischen Museum der Bundeswehr in Dresden. Ein Modell des Brandtauchers steht im Deutschen Museum in München. Ein Modell vom Bug des Brandtauchers in Originalgröße befindet sich im Kieler Schifffahrtsmuseum.

Während des Sezessionskrieges wurden mehrere handgetriebene U-Boote gebaut, unter anderem die CSS "H. L. Hunley". Am 17. Februar 1864 versenkte sie das gegnerische Schiff "USS Housatonic" und gilt somit als erstes U-Boot der Welt, das in Kriegszeiten unter Gefechtsbedingungen ein anderes Schiff zerstört hat (vorherige U-Boote hatten lediglich zu Testzwecken Schiffe versenkt). Bei diesem Einsatz ging das U-Boot mitsamt seiner achtköpfigen Besatzung allerdings verloren. Erst am 4.Mai 1995 wurde die "Hunley" von der National Underwater and Marine Agency (NUMA) gefunden und 2000 geborgen.

1863 stellte die französische Marine mit der Plongeur eines der weltweit ersten im getauchten Zustand nicht mit Muskelkraft betriebenen U-Boote in Dienst. Das Boot nutzte eine mit Druckluft betriebene Kolbenmaschine, konnte unter Wasser eine Strecke von bis zu 9 km zurücklegen und war mit einem Spierentorpedo bewaffnet. Der Druckluftantrieb benötigte sehr große Tanks, weshalb das U-Boot mit einer Länge von 43 m und einer Verdrängung von 426 ts wesentlich größer als alle anderen U-Bootkonstruktionen seiner Zeit war. Aufgrund des Antriebskonzeptes und der geringen Reichweite konnte das Boot nicht autark operieren und brauchte ein dampfbetriebenes Überwasser-Begleitschiff, das die "Plongeur" in das Zielgebiet schleppen und mit der notwendigen Druckluft versorgen musste.

 Am 2. Oktober 1864 wurde von Narcís Monturiol mit der "Ictíneo II" eines der ersten U-Boote mit einem maschinellen Antrieb zu Wasser gelassen. Das Boot bestand aus mit Kupferzargen verstärktem Holz und war komplett mit etwa zwei Millimeter dicken Kupferplatten beschlagen. Es wurde durch einen Magnesiumperoxid, Zink und Kaliumchlorat verarbeitenden Motor angetrieben.

Als erstes funktionsfähiges U-Boot der Welt gilt die "Sub Marine Explorer", da es das erste Boot war, das aus eigener Kraft wieder auftauchen konnte. Das Boot wurde 1865 von dem Deutsch-US-Amerikaner Julius Kröhl in New York hergestellt. Die moderne Konstruktion mit ihrem stromlinienförmigen Rumpf hatte ähnlich wie heutige Boote ein System von Ballastkammern für das Tauchen und Presslufttanks für das Auftauchen. Der Einsatzzweck des Bootes war das Sammeln von Perlen vom Meeresgrund, wofür es drei Ausstiegsluken nach unten hatte. Nach erfolgreichen Tests wurde es in Einzelteile zerlegt und nach Panama verschifft, wo Kröhl nach Perlen tauchte. Bereits 1867 verstarb er, genauso wie die gesamte Mannschaft, vermutlich an der Taucherkrankheit. Erst 2006 wurde das Schiff wiederentdeckt. Bis dahin hielten es die Einheimischen für ein zerstörtes japanisches Kleinst-U-Boot aus dem Zweiten Weltkrieg. Es liegt vor der Küste Panamas auf Grund und kann noch heute bei Niedrigwasser zu Fuß erreicht werden. Das Boot ist trotzdem unwiederbringlich verloren, da die starke Korrosion eine Bergung oder Restaurierung unmöglich macht.

Gegen Ende des 19. Jahrhunderts begannen sich die Marinen verschiedener Staaten für U-Boote zu interessieren. Die Marineministerien vieler Staaten – vornehmlich Spaniens, Frankreichs und der USA – schrieben Wettbewerbe für U-Boote aus und ließen sich Erfindungen und Entwicklungen vorführen. 1878/79 baute der englische Pfarrer und Erfinder George Garrett (1852–1902) auf eigene Rechnung zwei Tauchboote, die mit Kohlensäure bzw. Dampf angetrieben wurden. 1881 führte der Franzose Goubet den Elektromotor als Unterwasserantrieb ein. 1885 baute der Schwede Thorsten Nordenfelt zusammen mit Garrett ein mit einer Dampfmaschine angetriebenes U-Boot, das von der griechischen Marine erworben wurde. 1886/87 folgten zwei weitere 30 Meter lange Boote mit einem 250-PS-Dampfantrieb, die Nordenfelt bei der Barrow Shipbuilding Company, einem Vorreiter des U-Boot-Baus, für die osmanische Marine bauen ließ. Die Boote blieben beim Tauchen mit einem halbkugelförmigen Cockpit über Wasser. Der Kessel musste geschlossen werden, unter Wasser erfolgten Antrieb und Navigation mit Druckluft. Die 100 Tonnen schweren Boote waren 30,5 Meter lang und erreichten eine Geschwindigkeit von 6 Knoten über und 4 unter Wasser. Sie waren mit zwei Torpedorohren und zwei Maschinengewehren bewaffnet. Die "Abdul Hamid" war das erste U-Boot, dem es gelang, ein altes Zielschiff mit einem Torpedo zu versenken. Ein Problem war die Ausbalancierung des Bootes beim Abschuss der Torpedos.

Ebenfalls 1886 stellte die spanische Marine ein von einem Marineoffizier namens Isaac Peral entworfenes elektrisch betriebenes U-Boot namens "Peral" in Dienst, konnte jedoch die primitive Akkumulatortechnik nicht weiterentwickeln. Ab 1888 wurden in Frankreich U-Boote gebaut und in den Dienst der Marine gestellt. Henri Dupuy de Lôme und Gustave Zédé entwickelten zunächst ein batteriebetriebenes U-Boot namens "Gymnote", das in Toulon gebaut wurde. Dort entstanden in der Folgezeit weitere und größere Boote: Die 48,5 m lange "Sirene", 1892 gefolgt von einem 36,5 m langen Boot namens "Morse". Beide Boote waren ebenfalls batteriebetrieben und mit modernen Whitehead-Torpedos bewaffnet. Den größten Schritt tat das französische Marineministerium mit der von Maxime Laubeuf entwickelten "Narval", die 1899 gebaut wurde. Sie hatte einen Dampfantrieb, der bei der Überwasserfahrt die Batterien auflud. Dieses Boot wurde zur Grundlage der "Sirene"-Klasse, von der ab 1900 vier Exemplare in den Dienst der französischen Marine gestellt wurden. 1904 ersetzte Frankreich mit der Einführung der Aigrette-Klasse den für U-Boote ungeeigneten Dampfantrieb durch den wesentlich wirksameren und zuverlässigeren Dieselmotor.

In den USA verrichtete der emigrierte Ire John Philip Holland Pionierarbeit. Zunächst konstruierte er ab 1879 vier U-Boote für die "Fenian United Brotherhood", die mit dieser neuartigen Unterwasserwaffe die Royal Navy bezwingen und Irland zur Unabhängigkeit verhelfen wollte. Hollands Boote wurden bereits bei der Überwasserfahrt von einem Ottomotor angetrieben. 1888 schrieb die US Navy einen Wettbewerb für U-Boot-Konstruktionen aus, den Holland gewann. Wegen finanzieller Probleme konnte die Navy Holland erst ab 1895 Geld zum Bau eines Prototyps übermitteln. So entstand zunächst 1897 die 40 m lange "Plunger" (auch als "Holland V" bezeichnet), die jedoch wegen der hochgesteckten Ziele der Navy zahlreiche technische Mängel vor allem in der Antriebstechnik aufwies. Hollands nächste Konstruktion, die mit 25,4 m deutlich kleinere "Holland VI", konnte jedoch 1898 die Navy so sehr begeistern, dass ab 1900 die ersten sechs Boote der ähnlich konstruierten "Adder"-Klasse gebaut wurden. Die anderen Marinen, vor allem die Royal Navy, standen der schnellen Entwicklung von U-Booten allerdings kritisch gegenüber und verweigerten sich zunächst dem U-Boot-Bau. In Russland lief das erste U-Boot, die von Iwan Bubnow entwickelte "Дельфин" ("Delfin)", erst 1902 vom Stapel.

Mit dem Einsatz der Hunley 1864 begann auch ein wachsendes Interesse an der Nutzung von U-Booten zu Kriegszwecken. Im deutschen Kaiserreich blieb man zunächst zurückhaltend. Das Versuchs-U-Boot (1897) wurde von Howaldt in Kiel noch auf eigene Rechnung gebaut und als Fehlschlag bereits um 1902 verschrottet.

Im Jahre 1902 wurde in Deutschland ein Prototyp eines 200 Tonnen schweren Experimental-U-Bootes namens "Forelle" gebaut und intensiv getestet. Das kleine U-Boot stellte sich als durchaus interessant und kriegstauglich heraus und es wurden drei weitere Boote der gleichen Klasse für den Export nach Russland angefertigt. Nun wurde auch in Deutschland über den Einsatz militärischer U-Boote nachgedacht, und schließlich erteilte nach langem Zögern am 4. April 1904 das Reichsmarineamt dem Marineingenieur Gustav Berling den Auftrag, ein U-Boot zur Seekriegsführung zu konstruieren und zu bauen. Berling wandte sich daraufhin an die Germaniawerft in Kiel. Sein Entwurf lehnte sich an die nach Russland exportierten U-Boote an. Da es allerdings einige bedeutsame Änderungen bei der Konstruktion gab, verzögerte sich die Auslieferung des U-Bootes, und erst im April 1905 wurde mit dem Bau begonnen. Die wesentlichen Neuerungen betrafen den Druckkörper, die horizontale Anordnung der Torpedorohre und den Antrieb, da man anstatt eines potenziell gefährlicheren Benzinmotors einen Petroleumantrieb einsetzen wollte, der jedoch noch nicht ausgereift war. Am 14. Dezember 1906 wurde nach mehreren Testfahrten das erste deutsche Militär-U-Boot von der Kaiserlichen Deutschen Marine als "U 1" in Dienst gestellt. Heute befindet sich "U 1" im Deutschen Museum in München.

Mit Beginn des Ersten Weltkriegs (1914–1918) wurden U-Boote erstmals im größeren Umfang im Handelskrieg (Handels-U-Boot) oder zu militärischen Zwecken (siehe U-Boot-Krieg) eingesetzt. Die U-Boote griffen fast immer aufgetaucht an und versenkten Handelsschiffe meistens mit der Bordkanone. Abtauchen sollte das U-Boot nur, um sich einer Verfolgung zu entziehen, weil es unter der Wasseroberfläche im Ersten Weltkrieg für gegnerische Kriegsschiffe unauffindbar war. Große Tauchtiefen waren deshalb bedeutungslos.

Die Kaiserliche Marine schätzte die U-Boote zu Beginn des Krieges nur wenig und setzte stärker auf die großen Schlachtschiffe. Das änderte sich, als SM U 9 am 21. September 1914 vor der niederländischen Küste einen aus den drei Panzerkreuzern „HMS Aboukir“, „HMS Cressy“ und „HMS Hogue“ bestehenden Blockadeverband komplett versenkte. Auf den Panzerkreuzern glaubte man nicht an eine mögliche Gefahr durch deutsche U-Boote und erkannte die Torpedos nicht, obwohl sie pressluftbetrieben waren und deutliche Spuren an der Wasseroberfläche hinterließen. Nach den ersten Explosionen nahmen die Schiffsführungen Minen als Ursache an und ignorierten Berichte über Torpedo-Blasenspuren. Diese Fehleinschätzung kostete tausende Seeleute das Leben. Der unerwartete Erfolg machte die deutschen U-Boot-Fahrer zu Helden und begünstigte den raschen Ausbau der deutschen U-Boot-Waffe. Das Ansehen der U-Boot-Fahrer gegenüber den Besatzungen auf den teuren Großkampfschiffen, die kaum zum Einsatz kamen und nur geringe Erfolge erzielten, stieg beträchtlich.

Die zu Kriegsbeginn gegenüber den U-Boot-Verbänden Großbritanniens oder Frankreichs nur kleine deutsche U-Boote-Waffe wuchs sehr schnell und erlangte gegenüber der anderer Staaten eine technische Überlegenheit. Das galt besonders für die Qualität der Periskope und Torpedos, aufgrund derer sie zu einer äußerst ernst zu nehmenden Gefahr für die Flotten und Handelsschiffe der Gegner wurden.

Nach dem Ende des Ersten Weltkriegs verlangsamte sich die Entwicklung militärischer U-Boote. Deutschland, dem inzwischen größten Hersteller, war die Entwicklung und Produktion im Friedensvertrag von Versailles verboten worden. Die Siegermächte hingegen sahen im Besitz einer großen offensiven U-Boot-Waffe keine Notwendigkeit.

Vor dem Zweiten Weltkrieg sah sich die Führung der deutschen Kriegsmarine zu Kriegsbeginn einer recht starken alliierten Flotte gegenüber. Da Großbritannien und Frankreich als Garantiemächte Polens auftraten, hoffte man, mit den relativ billig herzustellenden U-Booten maximale Versenkungserfolge zu erzielen. Die U-Boote wurden so die Hauptbedrohung für sämtliche Handelsrouten. Man ließ sie vor allem Frachtschiffe angreifen, mit dem Ziel, Großbritannien als Inselstaat von dringend benötigten Rohstoffen abzuschneiden. Trotz ihrer technischen und logistischen Grenzen und ihrer geringen Anzahl von nur 57 Booten zu Beginn des Zweiten Weltkrieges war die U-Boot-Waffe anfangs sehr erfolgreich. Diese Erfolge überzeugten den ursprünglich skeptischen Hitler, einem verstärkten U-Boot-Bauprogramm zuzustimmen. Mehr und mehr U-Boote wurden in Dienst gestellt und ihre Anzahl näherte sich der Grundforderung des Befehlshabers der U-Boote (BdU) Karl Dönitz nach 300 Booten für einen erfolgreichen Blockadekrieg gegen England. Von den Erfolgreichsten unter den Kommandanten – den "Assen" – wurden teilweise enorme Versenkungsziffern erzielt. Einer der bekanntesten war Günther Prien, der 1939 als Kommandant von "U 47" in die Bucht von Scapa Flow, dem stark gesicherten Heimathafen der britischen Home Fleet eindrang und dort das Schlachtschiff "HMS Royal Oak" versenkte.

Weitaus wichtiger war allerdings die Versenkung von Handelsschiffen. Hierbei waren nächtliche Überwasserangriffe der bei Nacht schwer zu sichtenden U-Boote am erfolgreichsten. Nach den anfänglichen Erfolgen spürte die britische Wirtschaft schnell die Auswirkungen der vielen tausend Tonnen versenkten Schiffsraumes und es wurden umfangreiche Gegenmaßnahmen taktischer und logistischer (Geleitzugsystem) wie auch rein technischer Art eingeleitet. Der schnelle Fortschritt in der Radartechnik und die Ausstattung der Sicherungszerstörer der Konvois hiermit machten aufgetauchte U-Boote auch bei Nacht weithin erkennbar und bekämpfbar. Entzog sich das U-Boot durch Tauchen, konnte es mit ASDIC geortet und mit Wasserbomben bekämpft werden.

Aufgrund der geringen Batteriekapazitäten konnten sich die überwiegend zum Einsatz gekommenen U-Boote vom Typ VII und IX unter Wasser nicht schnell genug von Sicherungseinheiten absetzen und erlitten zunehmend Verluste. Die deutsche Entwicklung und Fertigung der sogenannten „Elektroboote“ der Typen XXI und XXIII, die ihrer Zeit weit voraus waren und in hoher Zahl gebaut werden sollten, kamen durch das Ende des Krieges nicht mehr oder nur noch sporadisch zum Einsatz. Der Typ XXI war der erste U-Boot-Entwurf, der für einen überwiegenden Unterwassereinsatz konzipiert war. Die Boote dieser Typen liefen unter Wasser mit E-Maschinen schneller als aufgetaucht mit Dieselmaschinen und hatten (durch hohe Batteriekapazitäten und die Möglichkeit zum Schnorchel-Betrieb) die Fähigkeit, lange getaucht operieren zu können. Er ließ alle anderen U-Boot-Typen auf einen Schlag veralten und wurde zum Ausgangspunkt der gesamten U-Boot-Entwicklung nach 1945.

Auch Italien verfügte über eine große U-Boot-Flotte (im Juni 1940 über 100 U-Boote), und schon im Sommer 1940 operierten die ersten italienischen U-Boote im Atlantik. Die Schiffe der Königlich Italienischen Marine waren bis zur Kapitulation Italiens im September 1943 im Einsatz. Anders als die deutschen erfüllten sie aber die in sie gesetzten Erwartungen kaum, da sowohl die Konstruktion der Boote (zu großer Turm, der selbst bei Nacht weit zu sehen war) wie auch die Ausbildung der Besatzungen nicht den Erfordernissen des Handelskrieges entsprachen. Insgesamt entsprachen die italienischen Erfolge nur einem Bruchteil derer, welche die Deutschen erzielten.

Im Gegensatz zu den deutschen U-Booten waren die britischen U-Boote ursprünglich nicht für den Einsatz im Handelskrieg auf hoher See entwickelt worden. Sie dienten meist zur Überwachung der Häfen und Marinebasen unter deutscher Kontrolle. Die vorhandenen Boote der H-Klasse und L-Klasse waren Einhüllen-Unterseeboote, deren Entwürfe noch aus dem Ersten Weltkrieg stammten. Zweihüllen-Hochseeboote waren unter anderem die Boote der Thames- und T-Klasse. Von den von der Royal Navy neuentwickelten modernen Zweihüllen-Hochseebooten der A-Klasse wurden vor dem Kriegsende nur die beiden Boote "Anchorite" und "Astute" fertiggestellt, die nicht mehr zum Kriegseinsatz kamen. Militärisch bedeutend waren vor allem die im Mittelmeer operierenden britischen U-Boote, die von ihren Basen in Malta, Gibraltar und Alexandria aus erfolgreich Schiffe der Achsenmächte, die Nachschub zum nordafrikanischen Kriegsschauplatz transportieren sollten, torpedierten. Ein Großteil der Nachschubgüter für die deutsch-italienische Afrika-Armee wurde dabei anhand der Informationen des britischen "Ultra Secret" versenkt. Die Entzifferung des Enigma-M-Funkverkehrs ermöglichte den Briten, feindliche Marineoperationen früh zu lokalisieren und Gegenmaßnahmen einzuleiten. Der erfolgreiche Abschluss der Operation "„Ultra“", bei der sich der britische Zerstörer "HMS Somali" gezielt auf die Jagd nach deutschen Wetter- und Versorgungsschiffen machte, um deren Chiffriermaschinen und -schlüssel zu erbeuten, lieferte diese Möglichkeit Ende Mai 1941.

Erst gegen Kriegsende griffen sowjetische U-Boote in der Ostsee in das Kriegsgeschehen ein, wo sie die deutschen Schiffstransporte von und zum ostpreußischen Kessel bedrohten. Dabei verursachten sie drei der verheerendsten Schiffskatastrophen aller Zeiten: Am 30. Januar 1945 versenkte "S-13 (С-13)" die "Wilhelm Gustloff", wobei mehr als 9.000 Menschen ums Leben kamen. Am 10. Februar versenkte "S-13" die "Steuben" (ca. 3.400 Tote), am 16. April wurde die "Goya" Opfer des sowjetischen U-Bootes "L-3 (Л-3)" (über 7.000 Tote).

Im Pazifikkrieg verfügten sowohl Japan wie auch die USA über bedeutende U-Boot-Flotten, neben denen auf diesem Kriegsschauplatz auch einige britische und niederländische U-Boote im Einsatz standen. Während die japanische Marineführung die Hauptaufgabe ihrer U-Boote in der Sicherung der eigenen Überwasser-Flottenoperationen und der Bekämpfung feindlicher Kriegsschiffe sah, konzentrierten sich die US-Amerikaner auf die Versenkung von Handelsschiffen. In Japan kam es auch zur Entwicklung und zum Einsatz von Kleinst-U-Booten, welche von den großen „Unterwasserkreuzern“ in die Nähe des Zielgebietes gebracht wurden. Außerdem baute Japan Unterwasser-Flugzeugträger, welche in einem Druckkörper bis zu drei Flugzeuge aufnehmen konnten. Geplant war, mit diesen Flugzeugen beispielsweise die Schleusen des Panamakanals oder San Francisco zu bombardieren. Zu Beginn des Krieges hatte die japanische Handelsflotte einen Schiffsraum von 6 Millionen BRT. Von diesen waren bis zum Kriegsende 5.053.491 BRT (1178 Schiffe) versenkt worden. Die aufgrund dieser Verluste eingetretenen Engpässe beim japanischen Nachschub wie auch bei der Rohstoffversorgung Japans trugen zum alliierten Sieg im Pazifik bei. Die japanische U-Boot-Waffe erlitt durch den Einsatz des Sonars bei den US-Amerikanern hohe Verluste; von insgesamt 190 U-Booten gingen 127 verloren. Oft wurden die japanischen U-Boote angegriffen, bevor sie sich überhaupt dem Ziel nähern konnten. Die US-amerikanische Marine verlor 52 U-Boote, was knapp 16 % aller im Dienst befindlichen Boote entsprach.

Obwohl sich der U-Boot-Krieg als sehr verlustreich herausgestellt hatte, gewann der strategische Wert der U-Boot-Waffe mehr und mehr an Bedeutung im Kalten Krieg. Ziel der U-Boot-Entwicklung war es nun, die Schwächen der Modelle des Zweiten Weltkriegs zu verbessern. Dies zielte besonders auf extrem lange – und auch schnelle – Unterwasserfahrten sowie große Tauchtiefen ab.

Die Entwicklung gipfelte in der Konstruktion von nukleargetriebenen U-Booten, welche die geforderten langen Tauchzeiten erfüllten. Die USA waren bei dieser Entwicklung führend, und am 21. Januar 1954 lief das erste nukleargetriebene U-Boot, die "USS Nautilus", vom Stapel. Am 3. August 1958 passierte sie als erstes Wasserfahrzeug bei einer Tauchfahrt unter der Arktis den geographischen Nordpol. Am 23. Januar 1960 erreichte das Forschungs-U-Boot "Trieste" mit 10.916 Metern Tiefe den zweittiefsten Punkt der Erde.

In den folgenden Jahren entwickelten sich die U-Boote schnell weiter. Sie wurden immer größer und schlagkräftiger gebaut. Da es kaum noch spektakuläre „öffentliche“ Entwicklungen in der U-Boot-Technik zu vermelden gab und die U-Boot-Waffe insgesamt als sehr geheim eingestuft wurde, erfuhr die Öffentlichkeit in den folgenden Jahrzehnten nur noch in Form von „Katastrophen“ etwas über die modernen U-Boote.

Seit dem Zweiten Weltkrieg machen U-Boote vor allem durch spektakuläre Unfälle Schlagzeilen:


Auch nach dem Zweiten Weltkrieg kam es vereinzelt zu Kampfhandlungen, an denen U-Boote beteiligt waren. Die ersten fanden noch mit konventionellen U-Booten im Bangladesch-Krieg des Jahres 1971 statt, als Indien im Krieg zwischen Bangladesch und Pakistan intervenierte. Dabei wurde am 9. Dezember 1971 die indische Fregatte "INS Khukri" vom pakistanischen U-Boot "PNS Hangor" versenkt, einem Boot der französischen Daphné-Klasse. Elf Jahre später griff erstmals ein Atom-U-Boot ein Kriegsschiff an: Am 2. Mai 1982 wurde der argentinische Kreuzer "General Belgrano" im Falklandkrieg durch einen Torpedo des britischen U-Boots "HMS Conqueror" versenkt.

Außerdem werden U-Boote zu Aufklärungszwecken eingesetzt. Zu einem internationalen Eklat kam es im Oktober 1981, als das mit Nukleartorpedos bewaffnete sowjetische U-Boot "W-137" (Whiskey-Klasse) vor dem schwedischen Marinehafen Karlskrona auf eine Schäre lief und von der schwedischen Marine aufgebracht wurde. Die sowjetische Führung bestritt anschließend einen Spionageeinsatz gegen das neutrale Schweden und führte den Zwischenfall auf einen Navigationsfehler zurück.
Die größten U-Boote, die jemals gebaut wurden, sind die des sowjetischen Projektes 941 (Nato-Bezeichnung: Typhoon-Klasse), Vorbild des sowjetischen U-Boots aus dem Spielfilm "Jagd auf Roter Oktober".
Da sich nach dem Zweiten Weltkrieg die Großmächte fast gänzlich auf den Einsatz von Atom-U-Booten verlegten, blieb es kleineren Marinen (hauptsächlich Deutschland, Italien, Schweden und Niederlande) überlassen, die Technik für konventionell betriebene U-Boote weiterzuentwickeln. Momentaner Stand der Technik ist die Einführung außenluftunabhängiger Antriebsanlagen, beispielsweise in Form von Brennstoffzellen, Kreislaufantrieben oder Stirlingmotoren. Beispiele dafür sind die deutsche U-Boot-Klasse 212 A, deren erstes Boot "U 31" im März 2004 an die Deutsche Marine übergeben wurde, sowie die schwedische Gotland-Klasse, deren Boote seit 1996 im Einsatz stehen. "U 31" verfügt als erstes U-Boot über einen Hybridantrieb aus Elektro- und Brennstoffzellen-Antrieb und ermöglicht so wochenlange Tauchfahrten ohne die Nachteile eines Atomantriebs (Pumpen- und Turbinengeräusche, Wärmeabgabe (Wärmeschleppe), Sicherheitsrisiken).

U-Boote können nicht nur an der Wasseroberfläche schwimmen, sondern auch ganz unter Wasser tauchen. Wenn U-Boote an der Wasseroberfläche schwimmen, sind sie wie normale Schiffe im Mittel leichter als das umgebende Wasser. Für eine Tauchfahrt erhöhen sie ihre Dichte, indem Ballasttanks mit Wasser geflutet werden. Wenn auf diese Weise ihre Masse größer als das Wasser ist, das sie verdrängen, sinken sie unter die Wasseroberfläche. Dies wird "statisches Tauchen" genannt.

Während der Tauchfahrt wird angestrebt, dass ihre gesamte Masse gleich der des verdrängten Wassers ist. Dann schweben sie gemäß dem Archimedischen Prinzip im Wasser, ohne Energie für das Halten der Tiefe zu benötigen. Dieser Zustand wird allerdings nie genau erreicht. Einerseits wirken sich selbst kleinste Unterschiede zwischen der U-Boot-Masse und der des verdrängten Wassers aus. Andererseits verändert sich die Dichte des umgebenden Wassers laufend durch Änderungen des Salzgehaltes, der Menge von Schwebestoffen (Plankton) und der Temperatur des Wassers. Das U-Boot hat also immer eine Tendenz zu steigen oder zu fallen. Es muss daher "eingesteuert" werden. Dazu wird Wasser in Regelzellen eingelassen oder ausgedrückt.

Ein gut eingesteuertes U-Boot manövriert unter Wasser in der Vertikalen durch "dynamisches Tauchen". Dazu erzeugt es während der Vorwärtsfahrt mit Hilfe von waagerechten Tiefenrudern dynamischen Auftrieb oder Abtrieb. Die Tiefenruder wirken dabei ähnlich wie die Flügel eines Flugzeugs. Bei historischen U-Booten war meist jeweils ein Paar Tiefenruder vorn und achtern angebracht. Moderne U-Boote tragen die vorderen Tiefenruder häufig seitlich am Turm.

Die ersten Unterwasserfahrzeuge aus dem 15. bis 18. Jahrhundert bestanden nahezu ausnahmslos aus Holz und wurden – wenn überhaupt – nur durch Eisenzargen oder Nägel zusammengehalten. Oftmals wurden die Boote so gefertigt, dass man sinnbildlich auf ein normales Holzboot ein anderes Holzboot kielaufwärts montierte. In der Regel wurden die Holzplanken solcher Unterwasserfahrzeuge durch Pech versiegelt und zusätzlich zur Abdichtung komplett mit einer Haut aus Leder überzogen. Bei diesen „U-Booten“ handelte es sich meist um "Einhüllenboote", bei denen die Tauchzellen innerhalb des Druckkörpers angebracht waren. Da die Zellen mit dem Außenwasser in Verbindung standen, mussten auch sie druckfest gebaut werden bzw. entsprechende Pumpen vorhanden sein.

Erst als es Mitte des 19. Jahrhunderts technisch gelang, die Antriebsschraube sowie die Steuerruder derart an den Rumpf anzubringen, dass die Fahrzeuge selbstständig fortbewegt und gesteuert werden konnten, ohne an der Oberfläche von einem Begleitfahrzeug gezogen zu werden, veränderte sich auch die Bauweise des Rumpfes. Nun wurden die Konstruktionen der Hüllen vermehrt durch Metalleinsätze verstärkt und Anfang des 20. Jahrhunderts wurden die ersten U-Boote mit einem kompletten Stahlrumpf gebaut.

Tauchzellen und Tanks verlagerte man ziemlich bald aus dem Druckkörper nach außen; es ergaben sich somit "Einhüllenboote mit Satteltanks". Aus dem Streben nach guter Seetauglichkeit bei Überwasserfahrt entstand daraus schließlich das "Zweihüllenboot", bei dem die Tauchzellen um den zylindrischen Druckkörper herumgelegt wurden. Das Boot erhielt damit eine zweite Hülle in Bootsform. Da diese im Tauchzustand innen wie außen unter gleichem Druck stand, brauchte sie nicht besonders stark zu sein. Den durch Brennstoffverbrauch bedingten Gewichtsveränderungen begegnete man dadurch, dass das Treiböl in nicht druckfesten, unten offenen Bunkern auf Seewasser schwimmend gefahren wurde.

Mit der nach bzw. während des Zweiten Weltkrieges zunehmenden technischen Entwicklung verschwand nach und nach der Überwasseraspekt des U-Bootes. Die Boote erhielten zunächst eine hydrodynamisch saubere geglättete Form, und US-amerikanische Entwicklungen rund um das Versuchs-U-Boot "USS Albacore" führten schließlich zur heute überwiegend gebauten Tropfenform mit zylindrischem Mittelstück. Diese wird normalerweise dadurch erreicht, dass der zylindrische Druckkörper durch freiflutende Aufbauten vorne und achtern stromlinienförmig gemacht wird. Auch das Oberdeck und der Turm sind freiflutend, es gibt aber keine durchgehende zweite Hülle. Die heute gängigen Boote sind somit weder Einhüllen- noch Zweihüllenboote und werden manchmal "Anderthalbhüllenboote" genannt.

Bei modernen Booten werden die Einbauten wie etwa Mannschaftsunterkünfte, Kommandozentrale, Antrieb usw. zunehmend akustisch entkoppelt, das heißt, mit passiver und aktiver Dämpfung und Zwischenträgern am Rumpf aufgehängt bzw. angebracht. Mehrere herkömmliche Propeller wurden durch einen einzigen vielflügligen Sichelpropeller bzw. einen Düsenpropeller oder Pumpjet ersetzt. Ziel ist eine weitergehende Minimierung der Schallemission an das umgebende Wasser und die Lautlosigkeit des Bootes, wodurch es quasi „unsichtbar“ wird (vergl. Tarnkappentechnik). Berichte über magnetohydrodynamische Antriebe („Raupenantrieb“ bzw. MHA-ähnliche Technik) dürften allerdings eher der Belletristik zuzuordnen sein.

Folgende Grafik vermittelt einen Eindruck von der Größe älterer und moderner U-Boote im Vergleich zu einem Boeing-747-Passagierflugzeug (für die Abkürzungen siehe Militärische Klassifizierung von U-Booten):
Die Druckkörper moderner militärischer U-Boote halten normalerweise dem Wasserdruck in 600 Meter Tiefe stand. In Anbetracht der Tiefe der Ozeane bedeutet dies, dass sie eigentlich nur knapp unter der Wasseroberfläche operieren können. Einige sowjetische Atom-U-Boote besaßen Druckkörper aus Titan und waren in der Lage, ca. 900 Meter tief zu tauchen. U-Schiffe des Typs Projekt 685 kamen angeblich sogar unter 1.200 Meter. Spezielle zivile Tiefsee-U-Boote sowie Bathyscaphen sind in der Lage, jeden Punkt des Meeresbodens zu erreichen.

U-Boote müssen in drei Dimensionen manövrieren können.

Zur Feinabstimmung bei Sehrohrtiefe siehe: Papenberg-Instrument.

Für die Fahrt über Wasser können im Prinzip alle Antriebe verwendet werden, die für Schiffe in Frage kommen. Gewöhnliche Schiffsaggregate (Dieselmotoren, Gasturbinen) sind Verbrennungsmotoren und benötigen große Mengen Sauerstoff für den Verbrennungsvorgang, der bei Überwasserfahrt oder Schnorchelfahrt aus der Luft angesaugt werden kann.

Das eigentliche Antriebsproblem stellt sich aber auf Tauchfahrt, da hier nicht genug Luft für den Betrieb von Verbrennungsmotoren zur Verfügung steht und bei größeren Tauchtiefen auch Abgase nicht mehr abgeleitet werden können. Es müssen also luftunabhängige Antriebe zur Anwendung kommen.

Auch nach dem Schließen der Außenluken zur Tauchfahrt wird durch die Atmung von Besatzung und Passagieren im Bootsinneren Kohlendioxid (CO) freigesetzt und Sauerstoff verbraucht. Abhängig vom Luftraum im Bootsinneren, der Personenanzahl und ihrer körperlichen Aktivität steigt der CO-Gehalt der Atemluft binnen einiger Stunden von der heutigen Außenluft-Konzentration von etwa 0,04 % auf noch akzeptable 1,0 … 1,5 %. In etwa der doppelten Zeit sinkt der Sauerstoffgehalt der Luft von anfänglich 21 % auf noch akzeptable 17 %.

Eine höhere CO-Konzentration von 4 % kann nur kurzfristig ertragen werden, 5 % sind giftig. Ohne Lufterneuerung wird deshalb nach einigen Stunden die Zuführung von Frischluft durch Durchlüften oder eine maschinelle Vorrichtung notwendig, welche die Außenluft mit der Innenluft des Bootes austauscht. Dies konnte früher nur aufgetaucht, später auch über einen Schnorchel erfolgen.

Ansonsten müssen, um eine Vergiftung der Personen an Bord zu vermeiden, nach einiger Zeit Filter eingesetzt werden, die das CO-Gas binden. Diese basieren auf der chemischen CO-Absorption durch Ätznatron oder Ätzkali. Für lange Tauchfahrten ohne Durchlüftung muss hiervon eine Menge in der Größenordnung des Proviantgewichts mitgeführt werden.

Der Sauerstoff, der von Besatzung und Passagieren eines U-Bootes im Tauchbetrieb benötigt wird, muss zusätzlich an Bord mitgeführt oder erzeugt werden. Bereits 1620 entwickelte Cornelis Jacobszoon Drebbel die Idee, den Sauerstoff zu ergänzen, indem er Kaliumnitrat verwendete, bei dessen Erhitzung Sauerstoff freigesetzt wird. Heute ist das Mitführen eines zusätzlichen Vorrates von Sauerstoff in Gasflaschen üblich, die sich feiner dosieren lassen.

Bei mit Kernenergie angetriebenen U-Booten kann der von der Besatzung verbrauchte Sauerstoff auch durch Sauerstoff ersetzt werden, der an Bord erzeugt wird. Dazu wird Energie aus dem Antriebssystem verwendet, um mittels Elektrolyse Wasser (HO) in seine Bestandteile – Wasserstoff und Sauerstoff – aufzuspalten, so dass ein Auftauchen zum Luftaustausch nicht mehr nötig ist.

Wenn ein U-Boot sämtliche seiner Tauch- und Regelzellen mit der an Bord befindlichen Druckluft anbläst, leitet es damit einen schnellen Auftauchvorgang ein, den man Notauftauchen nennt. Verglichen mit quasistatischem (regulär langsamem) Auftauchen durchbricht bei diesem Vorgang aufgrund der Massenträgheit ein vergleichsweise großer Teil des Bootes die Wasseroberfläche.

Wenn das U-Boot in steilem Winkel zur Wasseroberfläche steigt, geht der Auftauchvorgang am schnellsten.
Beispiele:

Viele Staaten besitzen militärische U-Boote, genaue Daten über die Zahlen sind jedoch oft geheim.

Die Stärke von U-Booten gegenüber Überwasserschiffen liegt darin, dass sie versteckt operieren und nur schwer entdeckt werden können.

Da U-Boote nicht optisch erfassbar sind, weil das Meer in größeren Tiefen dunkel ist und Radar unter Wasser nicht funktioniert, können sie auf größere Entfernungen nur akustisch lokalisiert werden, auf kurze Entfernungen auch durch die Erwärmung des Wassers durch den Antrieb oder eine Verzerrung des Erdmagnetfeldes durch die Stahlhülle.

Deshalb wird bei der Konstruktion besonders darauf geachtet, dass ein U-Boot so leise wie möglich ist. Dies wird durch einen stromlinienförmigen Bootskörper, speziell geformte Propeller akustische Entkopplung insbesondere von Kolbenmaschinen und Aussenhülle (samt Schraube) und Dämmung der Aussenhülle mit Elastomer erzielt.

Die ursprüngliche Aufgabe von U-Booten war die Bekämpfung von Überwasserschiffen. In dieser Rolle erlangten die U-Boote in beiden Weltkriegen ihre Bedeutung. Mit Beginn des Nuklearzeitalters kamen zwei weitere Hauptaufgaben hinzu: Strategische U-Boote wurden mit nuklearen Raketen ausgerüstet und dienten der nuklearen Abschreckung. Sie bildeten einen Teil der sogenannten Erstschlagskapazität, konnten aber auch zur Zweitschlagskapazität gerechnet werden, die einen gegnerischen Angriff auf das eigene Land überleben und für einen Gegenschlag bereitstehen sollten. Gleichzeitig wurden zur Jagd auf gegnerische strategische U-Boote spezielle Jagd-U-Boote entwickelt. Für beide Aufgaben verwendete man in erster Linie, aber nicht ausschließlich, atomgetriebene U-Boote. In jüngster Zeit wurden Jagd-U-Boote mit nichtnuklearem, außenluftunabhängigem Antrieb entwickelt. Bei der Deutschen Marine und einigen Verbündeten werden derzeit Boote mit dem in Deutschland entwickelten Brennstoffzellen-Antrieb beschafft. In der Deutschen Marine sind es die U-Boote der Klasse 212 A, die nach und nach in Dienst gestellt werden.

Neben diesen klassischen Aufgaben hat die Aufklärung mit U-Booten an Bedeutung gewonnen. Aufgrund ihrer Fähigkeit, ungesehen operieren und mit akustischen Sensoren sehr weit horchen zu können, bieten U-Boote gerade in Szenarien unterhalb der Schwelle offener Konflikte den Vorteil, wichtige Erkenntnisse sammeln zu können. Eine weitere Sonderaufgabe ist der Einsatz von Kampfschwimmern vom U-Boot aus. Beide Aufgaben können von herkömmlichen oder speziellen U-Booten wahrgenommen werden.

U-Boote unterscheiden sich in unterschiedliche militärische oder zivile Typen, je nachdem, welcher Zweck und welcher Auftrag dem jeweiligen U-Boot zukommt. Da U-Boote heute jedoch überwiegend militärisch eingesetzt werden, überwiegt in der nachfolgenden Liste der Anteil der diversen militärisch genutzten U-Boot-Typen:

Zur Bezeichnung von U-Boot-Typen werden in der Fachliteratur meistens die Standards der US Navy benutzt. Diese geben Aufschluss über Antrieb und Verwendungszweck eines U-Bootes.

Die frühere sowjetische und heutige russische Marine verwendet ein ähnliches System, das Kombinationen aus der Abkürzung für U-Boot (PL) ergänzt um Kürzel für Antriebsart und Bewaffnungstyp zulässt:
Für Boote mit Dieselantrieb ergibt sich so:

U-Boote verfügen über verschiedene Sensoren und Beobachtungsgeräte, mit denen sie Objekte orten können.

An oder direkt unter der Wasseroberfläche kann bei modernen U-Booten ein Radarsensor oder ein Sehrohr aus der Oberseite des Turms ausgefahren werden:
Unter Wasser kann ein U-Boot andere Schiffe nur akustisch über deren Geräuschabstrahlung orten. Die entsprechenden Sensoren werden als Sonarsensoren bezeichnet.

Die Wichtigkeit von Sonarsensoren führte dazu, dass sie bei der Konstruktion von U-Booten eine immer bedeutendere Rolle spielen. Um möglichst wenig in ihrer Leistung durch Störgeräusche beeinträchtigt zu werden, müssen Hydrophone so weit wie möglich vom Propeller und der Antriebsanlage entfernt montiert werden, so dass sich der Hauptsensor des Sonars im Bug eines U-Bootes befindet. Diese Sensoren im Bug setzen sich aus vielen einzelnen Hydrophonen zusammen, die in einer zylindrischen oder kugelförmigen Struktur montiert sind.

Da die eigenen Antriebsgeräusche aber die Ortung von Geräuschen hinter dem Boot erschweren, kann in vielen Fällen an mehreren hundert Meter langen Kabeln ein so genanntes "Schleppsonar" (engl. Towed Array / TAS) hinter dem U-Boot hergezogen werden. Dies bringt einige Vor-, aber auch Nachteile mit sich. So vergrößert sich die Empfindlichkeit des passiven Sonars erheblich, da einerseits wesentlich mehr Hydrophone am Schleppkabel angebracht werden können, und andererseits der Abstand zum Antrieb des U-Bootes die Störgeräusche reduziert. Dies führt zu einer signifikant gesteigerten Empfindlichkeit, welche eine erhöhte Horchreichweite und Peilgenauigkeit gewährleistet. Ein Nachteil des Schleppsonars besteht in seiner Länge (manche bis über einen halben Kilometer lang) und seinem Gewicht. Die Manövrierfähigkeit des U-Bootes wird dadurch eingeschränkt und ebenfalls die Geschwindigkeit, wobei letzteres das geringere Problem ist, da das Schleppsonar sowieso nur bei langsamer Fahrt oder Schleichfahrt angewendet wird. Die Einholdauer des Schleppsonars ist abhängig von der Länge des Kabels und kann durchaus länger als eine Minute dauern, was in kritischen Situationen aber schon „zu lange“ sein kann. Muss in einer Krisensituation schnell die Geschwindigkeit erhöht, ein enges Wendemanöver eingeleitet oder die Tauchtiefe rapide verändert werden, bleibt oftmals nichts anderes übrig, als das Schleppsonar zu kappen.

Grundsätzlich gilt, dass ein U-Boot umso schwerer zu lokalisieren ist, je kleiner und leiser es ist. Dieselelektrisch betriebene U-Boote haben deswegen im getauchten Zustand oft Vorteile gegenüber den wesentlich größeren Atom-U-Booten. Der Hauptvorteil von Atom-U-Booten sind ihre Ausdauer und Geschwindigkeit. Hohe Geschwindigkeiten verringern allerdings die Sensorenreichweite erheblich und vergrößern den Geräuschpegel. Zusätzlich verursacht die hohe Temperatur des Reaktors zahlreiche Probleme. Bei modernen Kernreaktoren kann bei geringer Leistungsabgabe die Kühlung allein durch Konvektion erfolgen. Ansonsten sind Kühlwasserpumpen notwendig, welche Geräusche erzeugen, die sich über den Schiffskörper bis ins Wasser ausbreiten und dort zu lokalisieren sind. Die Abwärme aus dem Kühlwasser von Kernreaktoren ist sogar durch Satelliten zu orten. Eine weitere Möglichkeit, die Eigengeräusche eines U-Bootes zu dämpfen, besteht darin, alle Maschinen auf einer freischwingenden, gummigelagerten Plattform aufzubauen, um so die Geräuschübertragung auf den restlichen Schiffskörper zu vermindern. Speziell geformte Propeller sorgen für eine Minimierung von Kavitationsgeräuschen.

Neben der Dämpfung der Eigengeräusche kommen auch Maßnahmen zum Einsatz, welche die Ortung durch feindliches Sonar erschweren sollen. So dämpft eine Opanin-Hülle – eine etwa 4 mm dicke Gummibeschichtung – die Schallrückstrahlung im Frequenzband zwischen 10 und 18 kHz bis auf 15 %. Die Wirkung des Schutzmittels ist dabei stark abhängig von Salzgehalt, Luftgehalt und Temperatur des Wassers. Diese Technik wurde erstmals 1943 bei dem deutschen U 480 angewandt. Durch die spezielle Gestaltung des Bootsrumpfes lässt sich die Sonarrückstrahlfläche eines U-Bootes reduzieren, so dass ein einfallender Sonarimpuls abgelenkt oder gestreut wird und nur noch ein sehr schwaches Echo in Richtung des Senders zurückgestrahlt wird.

Die Schiffshülle besteht bei einigen U-Boot-Klassen aus einem nicht magnetisierbaren Stahl. Damit wird die Ortung durch die Erfassung der vom U-Boot erzeugten Verzerrung des Erdmagnetfeldes so gut wie unmöglich.

Seit dem Zweiten Weltkrieg werden auch Funkmessbeobachtungsgeräte auf U-Booten eingesetzt, welche die Besatzung des U-Bootes vor einer möglichen Radarortung durch gegnerische Flug- und Seeziele warnen sollen.

Ein Schutzmittel besteht im Ausstoßen von Täuschkörpern („Bolden“). Ein Täuschkörper kann dabei ein Auftriebskörper sein, der Calciumhydrid (CaH) enthält und vom U-Boot ausgestoßen werden kann. Er schwebt im Wasser und erzeugt dabei Wasserstoffblasen, die für die aktive Sonar-Ortung ein Scheinziel vortäuschen sollen, hinter dem das gefährdete U-Boot ablaufen kann. Ein anderes Mittel ist das Ausstoßen oder Nachschleppen von Täuschkörpern, welche die Geräusche des U-Bootes bzw. dessen Antriebs imitieren und so die passive Sonarortung herannahender Torpedos in die Irre führen sollen.

Die Kommunikation mit getauchten U-Booten ist problematisch. Nur sehr langwellige Radiosignale (VLF, Very Low Frequency, Längstwelle), wie zum Beispiel die der Marinefunksendestelle Rhauderfehn, können etwa 10 bis 30 Meter tief ins Meerwasser eindringen. Im Zweiten Weltkrieg nutzte die Kriegsmarine den Längstwellensender Goliath auf 16,55 kHz (Hauptfrequenz) zur Übermittlung von Nachrichten an getauchte U-Boote.

Ausschließlich die Supermächte verfügen heute über die Möglichkeit, wenige Daten an U-Boote sogar in Tiefen bis zu 300 Meter zu senden. Das geschah auf 76 Hz (USA, Längstwellensender Sanguine) und 82 Hz (Russland, Längstwellensender ZEVS), also auf SLF (Super Low Frequency). Die dabei nur geringe mögliche Datenrate erlaubte nur eine Art „Anrufsignal“, um U-Boote zum Beispiel aufzufordern, bis ca. 15 Meter unter die Wasseroberfläche aufzusteigen, um dort auf Längstwelle (VLF, 3–30 kHz) mit höherer Datenrate Meldungen entgegenzunehmen, ohne dabei Antenne, Bojen etc. über der Wasseroberfläche positionieren zu müssen. Zur VLF-Übertragung dient den USA die Marinefunkstelle Cutler. Die Aktivitäten im SLF-Bereich wurden von den USA jedoch im September 2004 wieder aufgegeben und auch auf der russischen 82-Hz-Frequenz werden seit Längerem keine Aktivitäten mehr beobachtet.

Falls große Datenmengen auszutauschen sind oder das U-Boot nicht nur empfangen, sondern auch senden muss, ist es aber gezwungen, die Wasseroberfläche mit konventionellen Antennenmasten oder Bojen zu durchdringen. Dies aber erleichtert die Ortung des U-Bootes, das vor dem militärischen Feind verborgen gehalten werden soll.
Längere Nachrichten an ein U-Boot werden auf einem Satelliten gespeichert und heruntergeladen (in Sekundenschnelle). Für ein getauchtes U-Boot gibt es noch die Möglichkeit, eine Funkboje mit einer gespeicherten Nachricht aufsteigen zu lassen, die dann zum Beispiel an einen Satelliten gesendet wird. Das ist auch das übliche Verfahren bei Notsituationen, in denen das Boot auf den Meeresgrund gesunken ist und Hilfe von außen benötigt wird.
Versuche, das Kommunikationsproblem durch satellitengestützte Laser zu lösen, die ins Meerwasser bis zu einem gewissen Grad eindringen können, wurden wahrscheinlich nach dem Ende des Kalten Krieges aufgegeben.

Den Wissenschaftlern Maurice Green und Kenneth Scussel vom US Office of Naval Research (ONR) ist es 2007 zudem gelungen, ein Unterwasser-GPS-System zu entwickeln, das eine genaue Positionsbestimmung von U-Booten ermöglichen soll. Das System ist in der Lage, anhand von akustischen Signalen und Computerberechnungen die Position von U-Booten und in Zukunft möglicherweise auch von Tauchern zu orten. Hierzu werden am Meeresgrund fest verankerte genau positionierte GPS-Basisstationen eingerichtet. Ein U-Boot kann über Sonarimpulse mit der GPS-Basisstation am Meeresboden „kommunizieren“. Durch das Antwortsignal der GPS-Meeresbodenstation, das die genaue Tiefe und den Peilwinkel des empfangenen Schall-Impulses errechnet, kann ein Computersystem an Bord eines U-Bootes mit den GPS-Daten die eigene Position unter Wasser berechnen.

Während elektromagnetische Datenübertragung nur bis 10 m oder im günstigsten Fall über bis zu 300 m Unterwasserdistanz funktioniert, reicht akustische Unterwassertelefonie (Gertrude) der NATO heute bis 10 km und kann vielleicht auf bis zu 100 km Reichweite weiterentwickelt werden.
Informationen via Wasserschall lassen sich auch in Form von Morse-Nachrichten austauschen, was bei geringerer Informationsdatenrate die Reichweite erhöhen kann. Bei zivilen Tauchbooten bietet es sich oft an, eine Kabelverbindung hinauf zu einem Roboter-Satelliten oder zu einem Versorgungsschiff etwa für elektrische Telefonie zu verwenden. Bathysphären und Unterwasserplattformen hängen an einem Stahlseil, das auch von Leitungen für Energie, Information und Atemluft begleitet sein kann.

Torpedos sind die bekannteste Waffe militärischer U-Boote. Sie werden über Torpedorohre aus dem Rumpf ausgestoßen und von einem Schraubenantrieb, neuerdings auch von einem Wasserstrahl- oder einem zu Superkavitation führenden Raketentriebwerk angetrieben. Moderne Torpedos werden meist von den sie abschießenden U-Booten aus über einen Draht ferngelenkt, können aber auch selbstständig Ziele erkennen. Die Torpedoräume, in denen die Torpedos und andere Waffen gelagert werden, befinden sich meist im Bug des U-Bootes. Bei neueren Entwicklungen, zum Beispiel der US-amerikanischen Los-Angeles-Klasse, wurden dagegen die Waffen eher mittschiffs untergebracht und die Torpedorohre schräg nach vorne gerichtet; auf diese Weise konnte ein leistungsfähigeres Aktivsonar im Bug untergebracht werden. Torpedorohre im Heck eines U-Bootes waren noch bis nach dem Zweiten Weltkrieg üblich, werden heute jedoch nicht mehr verwendet, da sie für fernlenkbare oder autonom zielsuchende Torpedos nicht erforderlich sind.

Aus den Torpedorohren moderner U-Boote können auch Flugkörper gestartet werden. Das gängigste Prinzip hierbei ist es, einen Flugkörper, der auch von Überwasserschiffen gestartet werden kann, in einen zylindrischen Container zu verstauen. Dieser Container verlässt das U-Boot auf die gleiche Art und Weise wie ein Torpedo und durchstößt die Wasseroberfläche; danach gibt er den Flugkörper frei. Solche Flugkörper werden überwiegend gegen Schiffe eingesetzt.

Auch Marschflugkörper gegen Landziele können aus Torpedorohren gestartet werden. Allerdings werden sie überwiegend aus senkrechten Startschächten abgefeuert, um die Anzahl der mitgeführten Torpedos nicht reduzieren zu müssen. Auf die Verwendung von Anti-Schiff-Lenkflugkörpern spezialisierte U-Boot-Typen werden im Allgemeinen mit den Kürzeln SSG bzw. SSGN klassifiziert. Neben den erwähnten Vertikalstartern fanden auch andere Startverfahren Verwendung; so war die US-amerikanische USS Halibut mit einer Startrampe auf dem Vordeck ausgerüstet, während auf den sowjetischen Klassen Juliett und Echo die Flugkörper in im Winkel von 20° aufstellbaren Startbehältern untergebracht waren. Im Gegensatz zu modernen Entwürfen mussten diese frühen Flugkörper-U-Boote allesamt zum Abfeuern der Waffen auftauchen.

Ballistische Flugkörper ("Submarine-launched ballistic missile", SLBM) werden aus senkrechten Schächten gestartet. Sie haben wesentlich größere Durchmesser als Torpedos und sollen möglichst schnell das Wasser verlassen. Die meisten modernen U-Boote mit ballistischen Raketen (Klassifizierung SSBN oder SSB) sind dazu mit einer Anzahl von Raketensilos ausgerüstet, die sich mittschiffs hinter dem Turm befinden. Ausnahmen sind die russische Typhoon-Klasse, bei der sich der Turm am Rumpfende und die Raketen davor befinden, sowie die älteren, mittlerweile außer Dienst gestellten Klassen Golf und Hotel, bei denen die Raketen im Turm untergebracht waren. Nachdem die ersten ballistischen Raketen, die von U-Booten aus abgefeuert werden konnten, noch als Mittelstreckenraketen klassifiziert wurden (zum Beispiel UGM-27 Polaris), verfügen modernere Raketen wie die Trident mittlerweile über die Reichweiten von Interkontinentalraketen. Nur auf den erwähnten älteren U-Booten der Golf- und Hotel-Klasse kamen als ballistische Raketen anfangs Kurzstreckenraketen vom Typ Scud mit einer Reichweite von 150 km zum Einsatz. U-Boot-gestützte ballistische Raketen sind meist nuklear bestückt und sollen in der Theorie des Atomkriegs als Zweitschlagwaffen zum Einsatz kommen.

Im Gegensatz zu früheren Zeiten, in denen U-Boote mit an Deck montierten Geschützen bewaffnet waren, haben moderne U-Boote keine oder kaum Überwasserbewaffnung. Da U-Boote heutiger Zeit ausschließlich unter der Wasseroberfläche operieren, wird schlichtweg keine solche Bewaffnung gebraucht. Darüber hinaus wurden bereits gegen Ende des Zweiten Weltkrieges Decksgeschütze von U-Booten entfernt, um den hydrodynamischen Widerstand zu senken und die Unterwassergeschwindigkeit zu steigern. Die Tatsache allerdings, dass sich U-Boote fast nicht gegen U-Jagd-Hubschrauber und -Flugzeuge verteidigen können, verlangt nach der Entwicklung von Flugabwehrwaffen, die von getauchten U-Booten aus einsetzbar sind. Es existieren lediglich verschiedene schultergestützte Flugabwehr-Raketenstarter ähnlich der bekannten FIM-92 Stinger, die vom Turm abgefeuert werden. Beispielsweise ist die russische Sierra-Klasse mit Startvorrichtungen für Raketen der Typen SA-N-5 Grail oder SA-N-8 Gremlin ausgestattet. Die deutsche Marine entwickelt zurzeit mit dem System IDAS für die U-Boot-Klasse 212 A allerdings eine Flugabwehrwaffe, die auch von einem getauchten U-Boot aus einem Torpedorohr ausgestoßen und auf ein Ziel über der Wasseroberfläche abgefeuert werden kann.

Wie Katastrophen wie bei der Thresher, der Scorpion oder der Kursk zeigen, kommt es auch in Friedenszeiten immer wieder zu Unglücksfällen. Um die Besatzung zu retten, wurden verschiedene Rettungsmittel entwickelt:

Die Deutsche Marine als Teilstreitkraft der Bundeswehr verfügt nur über U-Boote mit Diesel- und mit Brennstoffzellenantrieb, nicht jedoch über Atom-U-Boote. Da die Aufgaben der Deutschen Marine im NATO-Bündnis anfangs auf reine Küstenüberwachung festgelegt waren und als Operationsfeld lediglich die „flache“ Ostsee sowie die Nordsee in Frage kamen, waren vor allem sehr kleine, leise und nicht für große Tiefen ausgelegte U-Boote relevant. Daher spielten während der Zeit des Ost-West-Konflikts die seinerzeit 24 U-Boote der damaligen Bundesmarine eine wichtige Rolle bei der Verteidigung der westdeutschen und dänischen Ostseeküste gegen amphibische Landungen der Marinen des Warschauer Pakts. Außerdem gab es eine internationale Beschränkung, dass Deutschland nur über U-Boote (Tauchboote) bis maximal 500 Tonnen Wasserverdrängung verfügen darf. Mit den veränderten politischen Verhältnissen haben sich jedoch auch die Aufgaben der Deutschen Marine verändert. Dennoch wurde bisher auf Atom-U-Boote zu Gunsten der Fortentwicklung der konventionellen U-Boote verzichtet. Die neuen Boote mit Brennstoffzellenantrieb der Klasse 212 A dienen vornehmlich der Bekämpfung anderer U-Boote sowie der unbemerkten Aufklärung und operieren je nach Bedarfsfall weltweit. Weiterhin waren bis zum Juni 2010 U-Boote der Klasse 206A im Dienst, deren Einsatzgebiet von der Nord- und Ostsee bis in den Mittelmeerraum reichte. Die Kommandanten der deutschen U-Boote haben die Dienstgrade Kapitänleutnant, Korvettenkapitän oder Fregattenkapitän.

Neben der militärischen Nutzung gibt es zivile Aufgaben für U-Boote.

"Forschungs-U-Boote" werden zur systematischen Untersuchung der Meeresböden oder Meeresströmungen eingesetzt. Sie erfüllen geologische, meeresbiologische, ozeanografische oder archäologische Aufgaben.

"Such-U-Boote", sollen oftmals unbemannt Objekte auf dem Meeresgrund aufspüren und untersuchen. Bekanntheit erlangten zum Beispiel die Expeditionen zu den Wracks der "Titanic" (mit der "Alvin") oder der "Bismarck". Das einzige nukleargetriebene Forschungs-U-Boot war die "NR-1" der US Navy.

Daneben existieren auch ferngesteuerte U-Boot-Modelle, die von Modellbauern gebaut werden oder auch als Spielzeug verkauft werden. Ihre Tauchtiefe beträgt höchstens einige Meter.
"Andere zivile Aufgaben":





</doc>
<doc id="5333" url="https://de.wikipedia.org/wiki?curid=5333" title="Umkehrfunktion">
Umkehrfunktion

Die Umkehrfunktion oder inverse Funktion einer bijektiven Funktion ist die Funktion, die jedem Element der Zielmenge sein eindeutig bestimmtes Urbildelement zuweist. 

Eine Funktion formula_1 ordnet jedem formula_2 ein eindeutig bestimmtes Element formula_3 zu, das mit formula_4 bezeichnet wird. Umgekehrt kann es sein, dass für ein formula_3 kein formula_2 mit formula_7 existiert oder es kann mehr als ein formula_2 mit formula_7 geben. Eine Funktion formula_10, bei der für jedes formula_3 genau ein formula_2 mit formula_7 existiert, wird bijektiv genannt. Für solche Funktionen kann man eine Funktion formula_14 bilden, die jedem formula_3 dieses eindeutig bestimmte formula_2 mit formula_7 zuordnet. Diese Funktion formula_18 ist dann die Umkehrfunktion von formula_10. 

Eine Funktion, deren Umkehrfunktion existiert, wird auch als "invertierbar" bezeichnet.

Wenn formula_20 eine bijektive Funktion ist, dann bezeichnet formula_21 die Umkehrfunktion. Dabei ist die hochgestellte formula_22 nicht mit einer negativen Potenz bezüglich der Multiplikation zu verwechseln; es handelt sich vielmehr um die Umkehrung bezüglich der Hintereinanderausführung (Verkettung) von Funktionen.

Der Funktionswert formula_23 ist definiert als das (eindeutig bestimmte) formula_24, das die Gleichung formula_25 erfüllt.

Eine alternative Schreibweise ist formula_26 ("f quer"), was allerdings leicht mit der komplexen Konjugation verwechselt wird.















Ist formula_78 eine Funktion und gelingt es, die Gleichung formula_79 durch Äquivalenzumformung in die Form formula_80 zu bringen, also äquivalent nach formula_81 "aufzulösen" (wobei formula_24, formula_83 und formula_84 gilt), dann ist formula_10 als bijektiv nachgewiesen und die Umkehrfunktion von formula_10 (nämlich formula_58) bestimmt.

Beispiele:



Für allgemeinere Anwendungen ist der oben eingeführte Begriff der Umkehrfunktion als Inverses
einer Bijektion zu eng. Entsprechend existieren Verallgemeinerungen für solche Gegebenheiten,
von denen zwei nachfolgend vorgestellt werden.

Für eine Funktion formula_106 heißt eine Funktion formula_107 Linksinverse (oder Retraktion),
wenn

Das heißt, die Funktion formula_58 erfüllt

formula_58 muss also gleich der Umkehrfunktion von formula_10 im Wertebereich von formula_10 sein, kann aber beliebige Werte für Elemente aus formula_114 annehmen, die nicht Resultat von formula_10 sind. Eine Funktion formula_10 hat Linksinverse genau dann, wenn sie injektiv (linkseindeutig) ist.

Eine Funktion kann mehrere Linksinverse haben; es gibt jedoch nur eine eindeutige Funktion, die zugleich Links- als auch Rechtsinverse ist.

Linksinverse treten oft als Inverse von Einbettungen auf.

Zum Beispiel sei formula_10 eine Funktion, die jedem Farbnamen ('rot', 'grün', 'blau' usw.) seine Farbe zuweist. Dann wäre ein Retrakt eine Funktion formula_58, die für jede Farbe einen Farbnamen ergibt.

Als numerisches Beispiel sei formula_10 die Einbettung von formula_120 in formula_121. Dann kann formula_58 z. B. die größte ganze Zahl liefern, die kleiner oder gleich dem Argument ist.

Eine Rechtsinverse (Koretraktion) von formula_106 (oder, bei Faserbündeln, ein Schnitt von formula_10) ist eine Funktion formula_125, so dass

Das heißt, die Funktion formula_127 erfüllt

formula_129 kann also jedes Element von formula_130 sein, das von formula_10 auf formula_132 abgebildet wird.
Eine Funktion formula_10 hat Rechtsinverse genau dann, wenn sie surjektiv (rechtstotal) ist.
Im allgemeinsten Fall wird für Existenz und »Konstruktion« einer Rechtsinversen das Auswahlaxiom benötigt.

Eine Rechtsinverse ist injektiv. Denn ist formula_134, dann ist formula_135.

Eine Funktion kann mehrere Rechtsinverse haben. Es gibt aber nur eine einzige Rechtsinverse, wenn sie zugleich Linksinverse ist; sie ist dann "die" Umkehrfunktion von formula_10.

Hat formula_10 mehrere Rechtsinverse, dann kann keine davon surjektiv sein, da die sonst bijektiv wäre und folglich einzig. Außerdem ist formula_10 dann nicht injektiv – denn sonst wäre es bijektiv, da es für die bloße Existenz einer Rechtsinversen schon surjektiv sein muss.

Rechtsinverse treten oft als Funktionen auf, die Repräsentanten einer Menge bestimmen.

Sei beispielsweise formula_139 eine Funktion, die jeder Art ihre Gattung zuweist. Das Rechtsinverse formula_127 ist eine Funktion, die für jede Gattung eine typische Art benennt. Politische Vertretung liefert viele Beispiele. Hier könnte formula_10 etwa die Staatsangehörigkeit eines Menschen sein, formula_127 das Staatsoberhaupt eines Staates.

Die Hilbert-Kurve bildet das Einheitsintervall stetig (daher die Bezeichnung "Kurve") auf das Einheitsquadrat ab. In der praktischen Anwendung wird aber häufig der "Hilbert-Index" benötigt, nämlich eine Linearisierung zweidimensionaler Daten (eine Umkehrung der Hilbert-Kurve). Dazu nimmt man eine der Rechtsinversen der Hilbert-Kurve, von denen es mehrere gibt – denn die Hilbert-Kurve kann als stetige Abbildung zwischen zwei Räumen unterschiedlicher Dimension nach dem Satz von der Invarianz der Dimension nicht bijektiv sein.



</doc>
<doc id="5334" url="https://de.wikipedia.org/wiki?curid=5334" title="Uster">
Uster

Uster (im zürichdeutschen Ortsdialekt ) ist eine politische Gemeinde und zugleich der Hauptort des gleichnamigen Bezirks im Schweizer Kanton Zürich. Die Bedeutung der Stadt Uster ist mit dem Bau des S-Bahn-Netzes des Zürcher Verkehrsverbunds stark gewachsen. Mit über 30 000 Einwohnern ist sie die drittgrösste Stadt im Kanton.

Uster liegt im Zentrum des Kantons Zürich und ist mit 28,56 km² die grösste Gemeinde im Bezirk Uster, sowie drittgrösste Stadt des Kantons Zürich. Die Gemeinde liegt östlich des Greifensees, 13 Kilometer entfernt von Zürich.

Der grösste Anteil von Uster wird von Kulturland eingenommen, das 44,5 % des Gemeindegebietes umfasst. Dazu kommen 27 % Wald, 18.4 % Siedlung, 7.8 % Verkehrsfläche und 0.4 % Gewässer.

Im Norden liegt Volketswil, sowie ein kleiner Zipfel von Fehraltorf. Im Osten befinden sich die Gemeinden Pfäffikon und Seegräben. Im Süden liegen die Gemeinden Gossau und Mönchaltorf, im Nordwesten liegt die Gemeinde Greifensee.

Uster unterteilt sich nach der Vereinigung vom 9. Juli 1927 in elf Ortsteile, die früher eigenständige Zivilgemeinden waren. Sechs sogenannte Aussenwachten liegen noch heute als Dörfer in freier Landschaft rund um das Kerngebiet der Stadt Uster, die anderen sind inzwischen baulich zusammengewachsen.

Der Ortsteil Kirchuster wurde erstmals 1416 erwähnt. Er hat rund 10 000 Einwohner und ist somit der grösste Ortsteil der Stadt.

Der Name des Ortes wurde mehrmals erwähnt, zunächst 1227 als "de Inferiori Ustero" (lat.), 1300 als "ze Nidern Ustre" (dt.) und 1416 als "ze Nidern Ustra". Westlich aber nahe dem Greifensee und südlich des Bahnhofs gelegen, leben in etwa 6400 Einwohner.

Das Quartier unterteilt sich in Oberuster und Nossikon. Oberuster wurde zum ersten Mal 1325 erwähnt. Es liegt östlich und leicht erhöht zum Bahnhof. Nossikon, als "Nossinchoven" erstmals 903 erwähnt, liegt südöstlich vom Bahnhof. Zusammengenommen wohnen dort etwa 4 700 Einwohner.

Der Ortsteil liegt im nordöstlichen Teil von Uster und hat in etwa 3200 Einwohner. Der Name setzt sich aus den zwei Namen der Anhöhen Rehbüel und Hegetsberg zusammen. Die dichte Besiedlung des Gebietes entstand nach dem Bau der Glatttalbahn und deren Eröffnung am 1. August 1856, als in der Nähe des Bahnhofs die ersten Häuser gebaut wurden. Die öffentliche Hand entschied sich für den Bau von Gesundheitseinrichtungen in diesem neuen Stadtteil, in dem sich heute die Gesundheitsmeile mit dem Spital Uster, dem Alters- und Pflegezentrum "Im Grund" und der "Stiftung Wagerenhof" für Menschen mit einer geistigen und mehrfachen Beeinträchtigung befindet.

In der südlich vom Stadtzentrum gelegenen Ortschaft leben zirka 770 Einwohner in der bewahrten dörflich-ländlichen Struktur nahe dem Ufer am Greifensee. Erstmals im Jahr 741 urkundlich erwähnt, ist Riedikon die älteste Siedlung im Gebiet der heutigen Stadt Uster.

Zwischen dem Kerngebiet der Stadt Uster und dem Dorf Nänikon liegt der Weiler Werrikon. Sie ist durch ein Naturschutzgebiet von den Quartieren des Zentrums getrennt. In der Aussenwacht leben etwa 270 Einwohner.

Dieses Quartier liegt nördlich des Zentrums und hat in etwa 3550 Einwohner. Der Ort wurde erstmals um 1325 als "Winckhen" erwähnt, 1361 als "Winninkon".

Nördlich der Oberlandautobahn und nordwestlich von Wermatswil liegt die Aussenwacht Freudwil. Fast vollständig von Wäldern umgeben, leben dort etwa 130 Einwohner. Das Uhrtürmchen auf dem alten Schulhaus gilt als Wahrzeichen des kleinen Dorfes.

Zwischen dem Kerngebiet der Stadt Uster und Gemeinde Pfäffikon ZH liegt Wermatswil. Auf der Anhöhe leben etwa 1200 Einwohner. Berühmtester Sohn des Dorfes ist Jakob Gujer.

Das Dorf Nänikon liegt drei Kilometer westlich von Uster und hat in etwa 2250 Einwohner. Die zwei Wahrzeichen der Aussenwacht sind das Türmli-Schulhaus und das an den Alten Zürichkrieg erinnernde Denkmal auf der Bluetmatt. Das Dorf ist eng mit der Gemeinde Greifensee verflochten; beide Gemeinden haben einen gemeinsamen Bahnhof Nänikon-Greifensee und eine gemeinsame Postleitzahl.

Das Dorf mit bäuerlichem Ursprung liegt zwischen Uster und Gossau. Es hat etwa 400 Einwohner.

Auf Ustemer Gemeindegebiet, bei Riedikon, fanden sich zahlreiche Spuren von Ufersiedlungen der neolithischen Pfyner Kultur. Von bronzezeitlicher Besiedelung zeugen einige Grabhügel.
Römische Gutshöfe standen in Nänikon, Riedikon und im Oberusterwald. Der alemannische Siedlungskern entstand im Frühmittelalter an den Ufern des Aabachs im heutigen Oberuster (im Bereich der «Insel»). Es kreuzten sich dereinst dort die Römerwege vom Glatttal zum Oberland (dem Aabach entlang) und von Winterthur zum Zürichsee.

Das Kloster St. Gallen verfügte um 741 über Besitz in Riedikon. Die Siedlung Uster wird im Jahr 775 erstmals in St. Galler Urkunden erwähnt, als "Ustra villa". Spätere Schreibweisen sind "in Ustramarcha" (902), "Ustera" (952), "Ustire" (1217/8), "Uster" (1219), "Ustere" (1244), "Ostrum" (1263), "Ustir" (1268), "Ostra" (1363), "Ustrach" (1426). Der Ortsname ist sehr wahrscheinlich ein ursprünglicher Bachname, der später auf die Siedlung übertragen wurde, und dürfte auf althochdeutsch "*uster-aha" «gefrässiger Bach» zurückgehen.

Auf der Insel, die auf der einen Seite noch heute vom Aabach und auf der anderen Seite von einem künstlichen Kanal (heute die Strasse) eingefasst ist, stand die erste Mühle. Sie wurde später etwas nach Süden und auf die andere Kanalseite verlegt, wo sie heute noch – eher unscheinbar – steht (das Mühlerad existiert nicht mehr). Die Mühle in Niederuster bestand seit dem 14. Jahrhundert (1350 "molendinum dictum die Ziegelmüli"). Eine von den Rapperswilern gestiftete Pfarrkirche St. Andreas ist erstmals 1099 erwähnt (abgebrochen 1823). Laut Bucelin ("Historia Agilolfingica et Guelfica," 1662) behaupteten die dem Geschlecht der Rapperswiler angehörenden "Herren von Uster" eine Abstammung von den schwäbischen Welfen. Die Kirchgemeinde kam nach 1369 zu den Landenbergern und 1438 an das Kloster Rüti.

Um das Jahr 1100 errichteten die Herren von Winterthur mitten im Gebiet des Grafen von Rapperswil das Schloss Uster, das sich heute als Wahrzeichen über der Stadt erhebt. Um 1267 war die Burg im Besitz der Freiherren von Bonstetten. Im Hochmittelalter gehörte Uster zur Herrschaft Greifensee; Wermatswil und Teile von Freudwil gehörten zu Kyburg, Sulzbach und Riedikon zu Grüningen und grosse Teile der Ländereien um Oberuster dem Kloster Rüti. Der Ortskern selber aber gehörte zum grössten Teil freien Bauern, was zu jener Zeit unüblich war. 1473 wurde in der Kirchgemeinde ein Jahrzeitbuch angelegt, das zu den besterhaltenen des Kantons gehört. Dadurch, dass das Land und Schloss unterschiedlichen Herren gehörten, bildete sich in Uster kein eigentlicher Altstadtkern. Im Gebiet der heutigen Zentralstrasse, unterhalb des Burghügels, liessen sich im ausgehenden Mittelalter Handwerker nieder.

Die Grenze der Blutgerichtsbarkeit verlief entlang des Aabaches mitten durch Uster. Linksufrig (Riedikon, Sulzbach) lag sie bei den Herren von Grüningen, rechtsufrig (Wermatswil) bei der Grafschaft Kyburg. In Nossikon ist für das 13. und 14. Jahrhundert ein separates Gericht von Freien belegt, das durch die Öffnung von 1431 bestätigt wurde, seine Sonderstellung aber spätestens im 16. Jahrhundert verlor.

Das Anniversar von Uster entstand zwischen 1469 und 1473.

Bis 1544 gehörte auch Greifensee zur Kirchgemeinde Uster, bis 1638 ausserdem Volketswil, Hegnau, Zimikon, Kindhausen und Isikon, bis 1767 Gutenswil und bis 1770 Heusberg. Die Siedlung Uster selbst wurde zur Unterscheidung von Ober-Uster und Unter-Uster auch Kirch-Uster genannt. Da die Pfarrkirche dem Heiligen Andreas geweiht war, fand jährlich am Andreastag (30. November) ein grosser Jahrmarkt statt.

Die 1492 niedergebrannte Burg Uster wurde nach 1526 wieder aufgebaut. 1535 verliessen die Herren von Bonstetten den Ort. Der Herrschaftsinhaber Hans Vogler verkaufte 1544 die niedere Gerichtsbarkeit an die Stadt Zürich. Ab 1560 gehörte die Burg den Freiherren von Hohensax, deren Linie mit dem Tod von Christoph Friedrich von Hohensax 1633 ausstarb. Die Rechte des Klosters Rüti fielen nach seiner Aufhebung 1525 an die Stadt Zürich. 1668 fielen ein Drittel der Einwohner der Kirchgemeinde einer Seuche zum Opfer.

Im 18. Jahrhundert entstand im oberen Glattal eine ausgedehnte Heimindustrie. Daneben bestand in Uster eine namhafte Produktion von Kirschwasser.
Um 1800 war gegen die Hälfte der Bevölkerung im Baumwollgewerbe tätig.

Uster wurde 1798, unter der Helvetischen Republik, Distriktshauptort. Diesen Status, nun Bezirkshauptort genannt, behielt es auch unter der Mediationsverfassung des Kantons Zürich 1803 sowie unter der liberalen Kantonsverfassung 1831 und gilt seither unverändert.

Im frühen 19. Jahrhundert erlangte das Dorf durch die zunehmende Industrialisierung mehr und mehr Bedeutung und zählte mit dem Zürcher Oberland zu den am dichtesten industrialisierten Gegenden Europas. Die Geschichte der Baumwollspinnerei und der industriellen Revolution in Uster und im Zürcher Oberland kann heute anhand des Industriepfades an 49 Objekten zwischen Uster und Bauma nacherlebt werden.

Die alte Pfarrkirche wurde 1823 abgebrochen und durch einen monumentalen Neubau im klassizistischen Stil ersetzt (siehe Reformierte Kirche Uster).

Am 22. November 1830 forderten 10 000 Landbewohner mehr demokratische Rechte, einen Rücktritt der alten Gewalten und die Ausarbeitung einer neuen Verfassung. Dies war der erste Schritt zu einem modernen Kanton Zürich. 1831 wurde als Ergebnis dieses sogenannten "Ustertages" eine neue liberale Kantonsverfassung auf der Basis der Volkssouveränität, der Gewaltentrennung und der Gleichberechtigung der Landbewohner sowie die Presse- und Gewerbefreiheit im Kanton Zürich eingeführt.

1832 geriet Uster in die Schlagzeilen, als am 22. November, enttäuscht über nicht eingehaltene Versprechen der Regierung, Heimweber die Fabrik der Firma Corrodi & Pfister, die mit den ersten mechanischen Webstühlen ausgestattet war, aus Angst um ihre Arbeitsplätze niederbrannten. Gegen fünfzig Beteiligte an dieser Tat wurden daraufhin zu Ketten- und Gefängnisstrafen von bis zu 24 Jahren verurteilt. Dieses Ereignis ging später als "Brand von Uster" oder "Usterbrand" in die Geschichtsbücher ein. Die Tat ist das bekannteste Beispiel eines Maschinensturms in der Schweiz.

1856 wurde die Bahnstrecke Wallisellen–Uster der damaligen Glatthalbahn-Gesellschaft eröffnet. Der Bahnhof in der Stadt Uster gehört zu den ältesten Bahnhöfen in der Schweiz.

1909 erhielt Uster eine Strassenbahn (Uster–Oetwil-Bahn). Am 1. Oktober 1949 fuhr die Bahn zum letzten Mal die Strecke Uster–Oetwil; anschliessend wurde sie durch einen Autobusbetrieb ersetzt.

Ende des 19. Jahrhunderts und ganz zu Beginn des 20. Jahrhunderts wurde Uster auch international bekannt durch zwei Schweizer Hersteller von Automobilen. So hat die Firma Weber & Cie. ab 1899 zunächst Dreiräder und ein Jahr später vierrädrige Automobile produziert. 1906 musste sie die Produktion einstellen.

Der Uhrmacher Martin Fischer gründete 1904 mit seinem Partner Paul Vorbrodt Turicum AG in Uster. Das Unternehmen hatte im Jahr 1913 etwa 140 Mitarbeiter, die Fahrzeuge wurden weltweit exportiert. Die Produktion musste 1914 nach etwa 1000 gebauten Fahrzeugen eingestellt werden. Die Bushaltestelle "Turicum" in Uster erinnert noch heute an das Unternehmen.

Von beiden Unternehmen gibt es Fahrzeuge im Verkehrshaus der Schweiz in Luzern zu besichtigen.

Trauriges Aufsehen erregte ein schwerer Unfall am 9. Mai 1985, als gegen 20:25 Uhr im Hallenbad Uster die Stahlanker der untergehängten Decke aufgrund von Spannungsrisskorrosion versagten. 12 Personen starben, 19 wurden verletzt. Der verantwortliche Ingenieur hatte die aggressive Natur von Chlorverbindungen in feuchtwarmer Luft unterschätzt.

Quellen zur Geschichte von Uster werden im Stadtarchiv Uster und in der Paul-Kläui-Bibliothek aufbewahrt. Im Stadtarchiv befindet sich das historische Archiv der Stadt Uster mit einem Aktenvolumen von ca. 2 Laufkilometern. Zu den Archivschätzen gehören auch ungefähr 15 Privatarchive, die Dokumente von Privatpersonen, Vereinen oder Firmen, die für die Stadt Uster von Bedeutung waren und ein vergangenes Alltagsleben abbilden, beinhalten. Die älteste mittelalterliche Urkunde im historischen Archiv ist von 1371. Sie besteht aus Pergament und wurde in deutscher Sprache verfasst. Zum weiteren Bestand gehören Dossiers von Verwaltungsstellen, welche die behördlichen Vorgänge in der Stadt Uster dokumentieren. Die Paul Kläui-Bibliothek ergänzt als wissenschaftliche Bibliothek mit ihrer regionalen Literatur aus Uster und dem Zürcher Oberland den Bestand des Stadtarchivs. Eine Dokumentationsstelle mit Zeitungsartikeln, Landkarten und Fotos für Uster ist dort integriert.

Das Buchdruckmuseum Graphos zeigt die 500-jährige Geschichte des Buchdrucks auf. Bei Führungen lernt man Handsatz wie zu Gutenbergs Zeiten, den Maschinensatz und den Druck an funktionierenden Originaleinrichtungen kennen.

Legislative ist der Gemeinderat mit 36 Sitzen.

Die Exekutive ist der siebenköpfige Stadtrat. Sitz des Stadtrates war bis 2007 das 1962 nach den Plänen von Bruno Giacometti erbaute Stadthaus. Seit Juni 2007 tagt der Stadtrat in einem Neubau gleich neben dem Stadthaus.



Im Kantonsrat des Kantons Zürich ist die Stadt Uster durch acht Personen vertreten:

Bei den Nationalratswahlen 2015 betrugen die Wähleranteile in Uster: SVP 27.7 %, SP 23.6 %, FDP 13.1 %, glp 9.5 %, Grüne 8.0 %, BDP 4.5 %, EVP 4.0 %, CVP 3.5 %, EDU 2.4 %, AL 1.5 %.

In der Stadt Uster leben 34'550 niedergelassene Personen "(Stand Ende März 2018)".

"Einwohnerschaft nach Nationalität:"

In der lokalen Sektion der Evangelischen Allianz sind beteiligt: Chrischona-Gemeinde (gegr. 1990), Evangelisch-reformierte Kirche, Evangelisch-methodistische Kirche (gegr. 1880), Freie Kirche Uster (gegr. 1863), Freie Missionsgemeinde, Heilsarmee (gegr. 1890), die Pfingstgemeinde (gegr. 1936) und die Iglesia Cristiana Cuerpo de Cristo (in der lokalen Allianz seit 2017).

In Uster gibt es zudem eine Neuapostolische Kirche sowie den Moschee-Verein in Werrikon (gegr. 1992).

Die zentrale Lage und die gute Erschliessung mittels öffentlichen Verkehrs hat seit 1990 einen Bauboom und steigende Immobilienpreise ausgelöst. Die 1350 in der Stadt ansässigen Unternehmen, zum grossen Teil im Dienstleistungssektor zu Hause, bieten rund 13 000 Arbeitsplätze. So haben zum Beispiel die Firma Uster Technologies, Distrelec und die Bank BSU ihren Sitz in Uster.

Uster ist sehr gut mit öffentlichen Verkehrsmitteln erschlossen. So führen die folgenden vier Linien der S-Bahn Zürich über Uster:

Zur Gemeinde Uster gehört zudem der Bahnhof Nänikon-Greifensee mit Anschluss an die und die .

Am Wochenende verkehren zudem die Nachtlinien auf der Fahrstrecke der , sowie die mit Halt an allen Stationen und Endstation Uster. Die einzelnen Linien verkehren im Halbstundentakt, so dass stündlich acht Züge nach Zürich sowie sechs Züge ins Zürcher Oberland fahren. Mit den Linien und gelangt man in 14 Minuten ins Zentrum von Zürich.

In Uster selbst stehen den Reisenden sieben lokale und sechs regionale Buslinien zur Verfügung.

Der Greifensee wird durch das Kursschiff Maur–Uster–Maur erschlossen.

Trotz seiner Grösse ist in Uster nur eine Verkehrsampel zu finden. Alle bis auf eine wurden in den letzten Jahren durch teilweise phantasievoll gestaltete Verkehrskreisel ersetzt, was Uster auch überregional als „Kreiselstadt“ bekannt machte.

Ebenfalls überregional bekannt sind die zehn Bahnübergänge, welche mehr als 40 Minuten in der Stunde geschlossen sind. Als einzige Alternative dazu befindet sich vor dem Feuerwehrhaus eine Unterführung, die allerdings nur 3.25 m hoch ist. Dies führte in den Achtzigerjahren zum sarkastischen Slogan: "«Uster – Die lebendige Stadt hinter der Barriere»".

Die im Norden angrenzende Oberlandautobahn (A53) erschliesst die Stadt mit drei Abgängen, welche ausserhalb der Stadt in die drei Hauptverkehrsachsen münden.

Als einer der zwölf regionalen Feuerwehrstützpunkte hat die Ustermer Feuerwehr die Aufgabe, die örtlichen Feuerwehren der umliegenden 14 Gemeinden bei grösseren Schadensereignissen und im Oel-/Chemiewehrbereich logistisch und personell zu unterstützen.

Neben der Kantonspolizei besitzt Uster eine eigene Stadtpolizei, die für kommunale Aufgaben zuständig ist und die Kantonspolizei unterstützt. Der Seerettungsdienst auf dem Greifensee wird von allen Anrainer-Gemeinden gemeinsam betrieben und hat seinen Sitz im Bootshaus Niederuster.

Das Spital Uster ist für die medizinische Versorgung des oberen Glatttales und des Zürcher Oberlandes mit insgesamt rund 135 000 Einwohnern zuständig. Mit rund 1100 Angestellten ist das Spital Uster der grösste Arbeitgeber der Region. Organisiert ist es als Zweckverband der zwölf Trägergemeinden. Am 8. März 2015 befanden die Stimmbürger über die Umwandlung des Zweckverbandes in eine Aktiengesellschaft. Das Anliegen wurde in der Stadt Uster mit 57 Prozent Nein-Stimmen abgelehnt, womit die Umwandlung scheiterte.

Auf dem Sportgelände Buchholz sind eine Sporthalle, Hallenbad, Reitanlage mit Halle, Tennis-, Squash- und Badmintonplätze, Kletterhalle, sowie Boccia- und Minigolfbahnen beheimatet. Zudem bietet die Stadt zwei Freibäder.

Trotz der Nähe zur Stadt Zürich besitzt die Stadt Uster ein eigenes kulturelles Angebot.

Im Zellwegerpark stehen die folgenden Kunstobjekte:
Neben den Konzerten der Kulturgemeinschaft Uster (KGU) beleben verschiedene Konzertreihen das Kulturleben der Landstadt am Greifensee. Hier seien das Orgelfestival Uster mit den fünf Orgelkonzerten im Spätsommer, die Reihe "must – Musiker aus Uster für Uster," das "PAM – Platz für andere Musik" sowie die Reihe "Sonntagskonzerte" der reformierten und der katholischen Kirchgemeinde erwähnt.

Insbesondere für Jazzliebhaber gehört Uster zu den wichtigsten Zentren, da Uster nicht nur einen der bekanntesten Jazzclubs besitzt, sondern auch das Schweizerische Jazzmuseum, das SwissJazzOrama, beheimatet.

In den letzten Jahren konnten sich in Uster und Umgebung mehrere Festivals etablieren, an denen vorwiegend Rockmusik gespielt wird.











Für individuelle sportliche Aktivitäten bietet die Region den See mit den Möglichkeiten für Radtouren, Inlineskating, Schwimmen, Rudern, Segeln, Surfen, Tauchen sowie Laufparcours im Wald, Vitaparcours, Finnenbahn, 400-m-Laufbahn, Hallenbad und Freibäder, Golfrange, Minigolf, Squash-, Tennis- & Badmintonplätze. Überdies bietet Uster zwei Fussballplätze, Sportstadion, und Sporthalle, ist Austragungsort für Geländesportarten wie Triathlon und Langstreckenlauf und beheimatet mehr als 71 Sportvereine.

Sehr erfolgreich schneiden jeweils die Sportler des Ustermer Schwimmvereins "Schwimmclub Uster-Wallisellen" an nationalen und internationalen Wettkämpfen ab. An der jährlichen, nationalen Ruderregatta auf dem Greifensee nimmt der Ruderclub Uster als örtlicher Verein teil.

Der Judo Club Uster, gegründet 1964, ist einer der grössten Judo Clubs der Schweiz und nimmt in der Region Zürich eine führende Rolle ein. Er ist sowohl bei den Damen als auch bei den Herren mit einem Team in der Nationalliga A, die höchste Schweizer Liga, vertreten.

Der städtische Unihockeyverein UHC Uster bestreitet seine Spiele in der höchsten Schweizer Spielklasse, der Nationalliga A. Auch im Handball ist Uster mit der Frauenmannschaft des TV Uster in der Swiss Premium League vertreten. Der Fussballverein FC Uster spielt in der 2. Liga interregional.

Im Jahr 1998 fanden in Uster die offiziellen Halbmarathon-Weltmeisterschaften statt. Seit 1980 findet jährlich Mitte September der Greifenseelauf mit Start und Ziel in Uster statt, gelaufen wird seit 1992 auf Halbmarathon-Distanz und zusätzlich auch kürzere Distanzen.

Uster ging im Jahr 2000 eine Städtepartnerschaft mit Prenzlau (Deutschland) ein. Der Partnerschaftsvertrag wurde am 29. Oktober 2000 vom damaligen Bürgermeister Prenzlaus, Jürgen Hoppe, und der damaligen Stadtpräsidentin Usters, Elisabeth Surbeck-Brugger, unterzeichnet. Am 19. Oktober 2004 wurde dieser Vertrag von Hans-Peter Moser, Bürgermeister von Prenzlau, Elisabeth Surbeck-Brugger, Stadtpräsidentin von Uster, und Hansjörg Baumberger, Stadtschreiber von Uster, um fünf Jahre bis Ende Juni 2010 verlängert. Anschliessend im Sommer 2010 um weitere vier Jahre.

Zur Naherholung stehen den Ustermern der See, das Stadtparkgebiet mit zwei Weihern und mehr als sieben Wälder zur Verfügung.

Neben dem Frühlingsmarkt, und der "Uster-Messe" im Herbst, findet jeweils am letzten Donnerstag im November und dem anschliessenden Freitag der "Uster Märt" statt. Dieser seit 1099 bestehende Jahrmarkt war ursprünglich Bauernmarkt und Tierschau. Heutzutage gibt es neben dem klassischen Marktbetrieb auch einen grossen Karussell- und Fahrgeschäftebetrieb, so dass dieser Anlass inzwischen zu den grössten Jahrmärkten der Schweiz zählt.

2001 wurde die Stadt Uster für ihre Stadt- und Raumplanung mit dem Wakkerpreis des Schweizerischen Heimatschutzes (SHS) ausgezeichnet:

2014 wurde die Stadt Uster für ihre langfristige Planung und Realisierung von Grünräumen mit dem Schulthess Gartenpreis des Schweizerischen Heimatschutzes (SHS) ausgezeichnet.
Der "Usterapfel" ist eine nach Uster benannte Apfelsorte. Sie ist älteren Generationen auch als "Zitrönler" bekannt. Der aus Holland stammende Apfel gelangte 1760 unter anderem nach Uster und wurde erstmals auf dem Gelände des dortigen Schlosses gepflanzt. Im 19. Jahrhundert war der Usterapfel die ertragreichste und häufigste Sorte im Kanton Zürich; damals machte er einen Zehntel bis einen Viertel der gesamten Apfelproduktion im Kanton aus.

Blasonierung
Dieses Wappen wurde von der Gemeinde 1917 eingeführt, basierend auf einem bei Gerold Edlibach (1486) abgebildeten Wappen.

Im 19. Jahrhundert galt ein anderes Wappen: «drei grüne Linden- oder Apfelbäume», heute das Wappen von Kirchuster. Es erscheint erstmals auf einer Schützenfahne von etwa 1835.





</doc>
<doc id="5335" url="https://de.wikipedia.org/wiki?curid=5335" title="Umberto Eco">
Umberto Eco

Umberto Eco (* 5. Januar 1932 in Alessandria, Piemont; † 19. Februar 2016 in Mailand, Lombardei) war ein italienischer Schriftsteller, Kolumnist, Philosoph, Medienwissenschaftler und wohl der bekannteste zeitgenössische Semiotiker. Durch seine Romane, allen voran "Der Name der Rose", wurde er weltberühmt.

Seine Kindheit und Jugend verbrachte Eco als Sohn einer kleinbürgerlichen Familie – der Vater Giulio Eco war Buchhalter – in der südpiemontesischen Provinzhauptstadt Alessandria und deren Umgebung, die ihn stark prägten. Über die Stadt und die Landschaft, den Charakter und die Grundstimmung der dort lebenden Menschen sowie den Alltag in den dreißiger und frühen vierziger Jahren unter dem Regime des italienischen Faschismus ließ Eco sich an mehreren Stellen direkt oder indirekt aus, so z. B. unverhüllt autobiographisch in dem Text „Das Wunder von San Baudolino“ (am Ende des Bandes "Wie man mit einem Lachs verreist") und in diversen „Streichholzbriefen“, aber auch literarisch eingekleidet in den Romanen "Das Foucaultsche Pendel" (wo unter anderem die Zeit des Partisanenkrieges 1944–1945 mit konkreten Situationen aus dem Leben des jungen Eco erzählt wird), "Die Insel des vorigen Tages" und "Baudolino" (deren Protagonisten beide aus der Gegend von Alessandria stammen, der eine im barocken 17. Jahrhundert und der andere im mittelalterlichen 12. Jahrhundert, wo er unter anderem die Gründung der Stadt im Jahre 1168 miterlebt) und besonders ausführlich in seinem fünften Roman, "Die geheimnisvolle Flamme der Königin Loana", worin die Kindheits- und Jugendjahre Ecos geschildert werden. Prägend waren dabei offenbar die Erfahrungen, die der 12- bis 13-jährige Eco in einem kleinen Bergdorf im südlichen Piemont machte, wo seine Familie 1943–1945 Schutz vor den Bombardierungen suchte und er im letzten Kriegsjahr Kämpfe zwischen Partisanen und Faschisten aus der Nähe miterlebte.

1948 begann Eco – entgegen dem Wunsch seines Vaters, der ihn zu einer Karriere als Rechtsanwalt drängte – ein Studium der Philosophie und Literaturgeschichte an der Universität Turin, das er 1954 bei Luigi Pareyson mit einer Dissertation über die Ästhetik bei Thomas von Aquin abschloss. Danach ging er zum damals noch ganz jungen italienischen Fernsehen (RAI) in Mailand, wo er ein Kulturprogramm aufzubauen versuchte. 1956 erschien sein erstes Buch, eine erweiterte Fassung seiner Dissertation unter dem Titel "Il problema estetico in San Tommaso" („Das ästhetische Problem beim heiligen Thomas“). Drei Jahre später verließ Eco das Fernsehen und wurde Sachbuchlektor in dem Mailänder Verlag Bompiani, für den er bis 1975 tätig blieb (und in dem seither fast alle seine Bücher erschienen sind). Zugleich war er im Umfeld des Gruppo 63 aktiv, einer der literarischen Bewegung der Neoavanguardia zugerechneten Gruppierung. Mit dem 1962 erschienenen Buch "Opera aperta" (deutsch "Das offene Kunstwerk", 1973) wurde er schlagartig als brillanter Kulturtheoretiker bekannt, der 1963 seine akademische Karriere als Dozent für Ästhetik und visuelle Kommunikation am Polytechnikum in Mailand begann, um sie über eine Zwischenstation an der Universität in Florenz schließlich an der Universität Bologna (der ältesten Universität Europas) zu beenden. Sein schon 1968 (deutsch 1973) erschienenes Buch "Einführung in die Semiotik" gilt bis heute auch international als Standardwerk.

1975 erhielt er eine ordentliche Professur für Semiotik mit Lehrstuhl an der Universität Bologna. Seit 1999 leitete er die dortige "Scuola Superiore di Studi Umanistici". Im Oktober 2007 zog er sich aus der aktiven Lehrtätigkeit zurück und war ab 2008 Professor emeritus der Universität Bologna.

Eco erfuhr zahlreiche Ehrungen und Auszeichnungen sowohl gesellschaftlicher wie akademischer Art, darunter allein (bis 2014) von 39 Universitäten weltweit die Ehrendoktorwürde – in Deutschland von der FU Berlin 1998 – und die Mitgliedschaft des Ordens Pour le Mérite für Wissenschaften und Künste (1999) sowie das Große Verdienstkreuz mit Stern des Verdienstordens der Bundesrepublik Deutschland (2009).

Mit seinem 1980 (deutsch: 1982) erschienenen ersten Roman "Der Name der Rose" erregte Eco weltweites und seinerzeit völlig überraschendes Aufsehen als Romancier. Auch sein 1988 erschienener Roman "Das Foucaultsche Pendel" sowie die vier folgenden Romane (1994, 2000, 2004 und 2010, s. u.) wurden in alle Weltsprachen übersetzt. Einem breiteren Publikum ist der Name Umberto Eco daher vor allem durch diese literarischen Werke bekannt, in denen er bei aller Freude am farbigen Erzählen und an spannenden Plots ausgiebig von Zitaten und Montagetechniken Gebrauch macht, was zu ihrer Charakterisierung als "den" postmodernen Romanen schlechthin geführt hat. Er selbst stand dem Begriff der Postmoderne eher skeptisch gegenüber und zog es vor, von Intertextualität zu sprechen, d. h. von der inneren Verflechtung und Verwobenheit aller literarischen Texte miteinander. Sehr plastisch wird dieser Gedanke an einer zentralen Stelle in "Der Name der Rose" ausgedrückt, wo der Erzähler Adson sagt:

Eco nannte Jorge Luis Borges und James Joyce als die beiden modernen Autoren, „die ich am meisten geliebt habe und von denen ich am stärksten beeinflusst worden bin“.
Als Bürger und politischer Autor war Eco zudem ein aktiver und vehementer Gegner von Silvio Berlusconi. In zahlreichen Zeitungs- und Zeitschriftenartikeln hat er dessen Politik scharf kritisiert. Noch kurz vor der Wahl im April 2006, die Berlusconi dann knapp verlor, veröffentlichte Eco seine gesammelten politischen Schriften nochmals in Buchform unter dem Titel "Im Krebsgang voran: Heiße Kriege und medialer Populismus" (deutsch: Frühjahr 2007).

Seit 1985 schrieb er regelmäßig – erst wöchentlich, ab 1998 vierzehntäglich – eine Kolumne in der Wochenzeitschrift "L’Espresso" unter dem Titel "La Bustina di Minerva" (deutsch: „Streichholzbriefe“). Er war Mitgründer und -herausgeber der Internetzeitschrift "Golem l’Indispensabile". 2002 gründete er zusammen mit Freunden und Gleichgesinnten (darunter die Architektin Gae Aulenti und der Schriftsteller und Germanist Claudio Magris) die Gruppe "Libertà e Giustizia", die sich – in Anlehnung an die Widerstandsbewegung "Giustizia e Libertà" gegen den Mussolini-Faschismus – als intellektuelle Opposition gegen die Politik von Silvio Berlusconi versteht.

2005 wurde Umberto Eco in dem englischen Magazin "Prospect" nach Noam Chomsky und vor Richard Dawkins zum zweitwichtigsten Intellektuellen weltweit gewählt. Im April 2010 erschien die erste Biographie (Leben und Werk) über ihn.

Nachdem sich Eco schon in dem Roman "Das Foucaultsche Pendel" ausführlich dem Thema "Verschwörungstheorien" gewidmet hat, ist dies auch in dem 2010 erschienenen Roman "Der Friedhof in Prag" das zentrale Thema. Eco schafft mit seinem fiktiven Ich-Erzähler Simon Simonini einen berufsmäßigen Fälscher, den er als Hauptautor der "Protokolle der Weisen von Zion" einführt.

Nachdem Silvio Berlusconis Verlagsgruppe Mondadori im Oktober 2015 die Rizzoli-Verlagsgruppe „RCS Libri“ mit dem Verlag Bompiani, bei dem Umberto Eco bisher veröffentlichte, übernommen hatte, veröffentlichte Eco unter dem neu gegründeten Verlag "La Nave di Teseo" (zu deutsch "Schiff des Theseus"), der auf einen Vorschlag Ecos hin in Anspielung auf das Paradoxon benannt wurde. Verlegerin von "La Nave di Teseo" ist die ehemalige Bompiani-Chefin Elisabetta Sgarbi.

Von 1962 bis zu seinem Tod war Eco mit der gebürtigen Deutschen Renate Ramge, einer in Frankfurt am Main geborenen Expertin für Museums- und Kunstdidaktik, verheiratet. Sie bekamen einen Sohn und eine Tochter.

Umberto Eco starb am späten Abend des 19. Februar 2016 im Alter von 84 Jahren in seiner Mailänder Wohnung an den Folgen einer Krebserkrankung.






Alle illustriert von Eugenio Carmi












</doc>
<doc id="5336" url="https://de.wikipedia.org/wiki?curid=5336" title="Ursula K. Le Guin">
Ursula K. Le Guin

Ursula Kroeber Le Guin [] (* 21. Oktober 1929 in Berkeley, Kalifornien als Ursula Kroeber; † 22. Januar 2018 in Portland, Oregon) war eine amerikanische Autorin hauptsächlich phantastischer Literatur. Bekannt ist sie vor allem durch die Science-Fiction-Romane des Hainish-Zyklus und die Fantasy-Romane der Erdsee-Welt.

Ursula Kroeber war die Tochter der Schriftstellerin und Anthropologin Theodora Kroeber, Autorin von "Ishi in Two Worlds", der Biografie des letzten Überlebenden der Yahi-Indianer, und des Professors für Anthropologie Alfred Louis Kroeber. Die Großeltern waren deutschstämmig, väterlicherseits stammte die Familie aus Kröbern in Thüringen, der Geburtsname der Mutter war Kracaw, und aus der Gegend von Krakau stammten die Vorfahren, die Anfang des 19. Jahrhunderts eingewandert waren. 
Während der Schulzeit wohnte die Familie in der Nähe des Campus der University of California in Berkeley, wo Kroeber die "Berkeley High School" besuchte. In den Sommermonaten aber wohnte man in "Kishamish", einem Besitz der Familie im Napa Valley. 

Sie begann schon früh zu schreiben. In einem ihrer wenigen autobiographischen Essays schreibt sie über den Eindruck, den Lord Dunsanys "A Dreamer’s Tale" auf die kindliche Leserin machte, wie sie mit acht Jahren ihre erste Geschichte schrieb von einem Mann, der von übelwollenden Elfen verfolgt wird, und wie sie mit 11 Jahren eine erste Zeitreisegeschichte an "Amazing Stories" sandte, die abgelehnt wurde. Sie las viel, es gab ein Haus voller Bücher und eine gute öffentliche Bibliothek, Science-Fiction las sie aber für lange Zeit nicht mehr – „I was busy with Tolstoy and things“, außerdem gab es immer nur die Geschichten von den Raumschiffkapitänen mit kantigen Gesichtern und seltsamen Waffen, es war die goldene Zeit der Space Operas – erst 1960 oder 1961 auf Anregung eines Freundes eine Erzählung von Cordwainer Smith, die sie davon überzeugte, dass man es machen könne, das heißt, dass es Science-Fiction von Interesse gibt und dass man dergleichen auch schreiben könnte.

Zunächst kamen aber Schule und Studium. Nach der High School ging Kroeber an die Ostküste und studierte Literatur am Radcliffe College in Cambridge, wo sie 1951 den Bachelor machte und mit einer Mitgliedschaft bei Phi Beta Kappa ausgezeichnet wurde. Anschließend setzte sie ihre Studien der italienischen und französischen Renaissance an der Columbia University in New York fort, wo sie "Faculty Fellow" war und mit einer Arbeit über Pierre de Ronsard 1952 mit dem Master abschloss. Den Hintergrund der mediterranen Renaissance-Welt verwendete sie später in ihren "Geschichten aus Orsinien" und in "Malafrena".
Mehrere Fulbright-Stipendien erlaubten ihr Forschungsaufenthalte in Paris (1953–1954) und London (1968–69 und 1975–76). In Frankreich lernte sie 1953 ihren späteren Ehemann kennen, den Professor für Geschichte Charles A. Le Guin. Im Dezember 1953 heirateten Kroeber und Le Guin. Mit ihrem Mann hatte sie zwei Töchter (geb. 1957 und 1959) und einen Sohn (geb. 1964). 

Le Guin unterrichtete in den folgenden Jahren Französisch an der "Mercer University" in Macon, Georgia, und an der "University of Idaho" in Moscow. 1955 war sie "Department Secretary" (stellvertretender Fachbereichsleiter) an der Emory University in Atlanta. Die Familie ließ sich schließlich 1958 an der Westküste in Portland, Oregon nieder, wo ihr Mann an der Portland State University unterrichtete und Le Guin bis zu ihrem Tod hauptsächlich lebte. In den Sommermonaten verbrachten sie aber immer wieder Zeit im Napa Valley.

Im September 1962 erschien eine erste Erzählung, "April in Paris", in dem SF-Magazin "Fantastic Stories of Imagination". Seither lebte sie als Schriftstellerin in Portland (Oregon). Ihr erster Roman, "Rocannon’s World ", erschien 1966. Die folgenden Jahre bis 1974 waren die schriftstellerisch produktivsten, in denen die meisten der bekanntesten Werke, vor allem der Großteil der Romane aus den "Hainish"- und "Erdsee"-Zyklen erschienen. Bald schon folgte auch Anerkennung, sie gewann Preise, ihre Bücher wurden enthusiastisch besprochen und ihre Kurzgeschichten fanden den Weg in wichtige Anthologien.
Neben ihren belletristischen Werken verfasste sie eine Reihe von literaturkritischen und theoretischen Arbeiten und war immer wieder Dozentin in Lehrveranstaltungen und Workshops für kreatives Schreiben, unter anderem an der "Pacific University", Forest Grove, Oregon (1971), "University of Washington", Seattle (1971–1973), "Portland State University", Oregon (1974, 1977, 1979), in Melbourne, Australien (1975), an der "University of Reading" in England (1976), der "Indiana Writers Conference" in Bloomington (1978, 1983), an der University of California in San Diego (1979) und über viele Jahre hin an der "Portland State University".

Im Januar 2018 starb Le Guin im Alter von 88 Jahren in ihrem Heim in Portland.

Le Guin beschrieb ihre Arbeitsweise einmal folgendermaßen als eine Art visionären Prozess bzw. visionäre Aufgabe:

Das heißt die bestimmenden Elemente in Le Guins Werken sind die Protagonisten als Teil und in Beziehung zu der sie umgebenden Welt.
Dementsprechend lassen sich ihre Erzählungen in vier Hauptgruppen einteilen, entsprechend den vier fiktiven Welten Le Guins, denen ein Großteil ihrer Werke zugeordnet werden kann. Diese sind

Le Guin hatte wenig übrig für die Aufteilungen und Abgrenzungen, die mit den Genrebegriffen der Science-Fiction verbunden sind, wie sie in ihrem Essay "Genre: A Word Only the French Could Love" formulierte. Dabei beanstandet sie vor allem Genre-Zuordnungen, wenn sie als Merkmal literarischer Qualität missbraucht werden:

Ansonsten hält sie Genre-Zuordnung in vielen Fällen für durchaus nützlich, vor allem für den Leser. Aber:

Dennoch exemplifizierte sie die Genre-Unterscheidungen in ihren vier Welten geradezu schulbuchmäßig, etwa entsprechend Samuel R. Delanys Definition, nach der ein Bericht schildert, was geschehen ist, ein realistischer Roman etwas, was hätte geschehen können, die Science-Fiction das, was nicht geschehen ist (aber geschehen könnte) und die Fantasy schließlich das, was nicht geschehen könnte – außer, wenn alles geschehen kann.

Die „Ökumene“ (im Original "Ekumen") ist ein relativ loser Verbund von bewohnten Welten, die vor langer Zeit von den Humanoiden der Welt "Hain" besiedelt wurden. Zu diesen Welten gehört auch "Terra", die Erde, die Menschen sind also Nachfahren hainischer Kolonisten. Der Kontakt ging mit dem Niedergang der hainischen Zivilisation verloren und wurde erst wiederhergestellt, als interstellare Raumfahrt erneut möglich wurde. Es gibt allerdings keinen überlichtschnellen Antrieb, weshalb Reisen und auch der Austausch von Nachrichten zunächst Jahre dauern, auch wenn aufgrund der Zeitdilatation den Reisenden selbst die Zeit wesentlich kürzer erscheint. Erst durch die Erfindung des "Ansible", eines interstellaren Kommunikators, der den Austausch von Nachrichten ohne Zeitverzögerung möglich macht – der Hintergrund der Erfindung wird im Roman "The Dispossessed" geschildert – wird eine engere Zusammenarbeit und ein Austausch und damit die Ökumene als Zusammenschluss der bewohnten Welten erst möglich.

Es handelt sich bei den Erzählungen aus dem Ekumen-Universum also um Science-Fiction im klassischen Sinn: Akzeptiert man die Voraussetzungen (also etwa die Möglichkeit instantaner Kommunikation über interstellare Entfernungen oder von Telepathie), so folgt alles weitere durchaus den Linien des realistischen Romans.

Weniger klar verhält es sich mit der Welt der Erdsee. Zwar gibt es Zauberer und Drachen, also zwei klassische Elemente der Fantasy. Es kann allerdings nicht in jedem Augenblick alles passieren, vielmehr unterliegt die Magie Regeln, die – entsprechende Begabung vorausgesetzt – erlernt werden können, und zwar in der Schule der Magier auf der Insel "Rok". Zentral für deren Studium ist die Kenntnis der wahren Namen der Dinge, die der wahren, ursprünglichen und einzigen Sprache entstammen. Diese magische Sprache ist zugleich die Muttersprache der Drachen, die dadurch magische Geschöpfe schlechthin sind.
Das Inselreich der Erdsee wird von verschiedenen Völkern bewohnt, mit von Insel zu Insel verschiedenen Sitten und Gebräuchen. Es gibt Rok als kulturelles Zentrum und den jährlichen Langtanz als auf fast allen Inseln geübtes Ritual, doch diese kulturelle Klammer ist relativ lose.
Weiterhin gibt es eine nicht im Einzelnen ausgeführte, aber immer wieder angedeutete Geschichte des Inselreichs und es gibt ein Königtum, das allerdings zu dem Zeitpunkt, als "Der Magier der Erdsee" beginnt, schon lange erloschen ist. "Der Magier der Erdsee" ist der erste Band der für den Erdsee-Zyklus zentralen Trilogie, die weiteren Bände sind "Die Gräber von Atuan" und "Das ferne Ufer". Der Protagonist dieser drei Romane ist "Ged", genannt "Sperber", und die Romane schildern die Laufbahn Geds vom Ziegenhirten auf der Berginsel "Gont" bis zum Erzmagier der Erdsee.

Man hat also eine konsistente Welt, deren Grundvoraussetzungen ganz andere sind als die der uns bekannten Welt. Allerdings gibt es ja durchaus zahlreiche menschliche Gemeinschaften und Kulturen, in denen Magie für eine existierende, wirkende Macht gehalten wird. In der Le Guinschen Erdsee kommt zu einer derartigen Weltsicht dann das offensichtlich, sehr handgreifliche „Funktionieren“ solcher Magie.

Ein zentrales Thema der Fantasy ist der Einbruch des völlig Unbekannten, des Namenlosen, in die geordnete, benannte Welt der High Fantasy. Diese Konfrontation ist Thema in "Der Magier der Erdsee", wo Ged als Student der Magie unbeabsichtigt ein finsteres, namenloses Wesen heraufbeschwört, das ihn fortan verfolgt und seine gesamte Existenz zu verschlingen droht, und ebenso im dritten Roman "Das ferne Ufer", wo die Magie insgesamt bedroht ist und zu verlöschen droht, und damit auch die Kultur des Inselreichs vor dem Untergang steht. Der Einbruch des Unbekannten ist hier buchstäblich Folge einer Grenzüberschreitung, indem nämlich die Grenze zwischen dem Ort der Lebenden und dem Reich des unumkehrbaren, unwiderruflichen Todes durchbrochen wird. Durch die so entstandene Bresche blutet die Welt der Magie aus und Ged gelingt es nur unter Aufbietung seiner sämtlichen Kräfte und mit Hilfe des künftigen Königs, die Lücke wieder zu schließen und die Welt der Magie zu restituieren,

Neben Romanen und Kurzgeschichten schrieb Le Guin mehrere Kinderbücher, zahlreiche Essays, Literaturrezensionen und übersetzte Lao Zis "Daodejing".

Sie hat für ihre Bücher eine Reihe von Auszeichnungen erhalten, darunter mehrfach die beiden bedeutendsten internationalen Preise für Science-Fiction-Literatur, den Nebula Award und den Hugo Award. "Die linke Hand der Dunkelheit" und "Planet der Habenichtse" gewannen jeweils beide Preise, daneben "Tehanu" und "Powers" den Nebula Award und "Das Wort für Welt ist Wald" den Hugo Award. 1974 erhielt "Die Omelas den Rücken kehren" den Hugo für die beste Kurzgeschichte. 1985 erhielt sie den Janet Heidinger Kafka Prize der University of Rochester. Zwischen 1972 und 2009 gewann sie außerdem insgesamt vier Locus Awards. 1988 und 2002 erhielt sie den World Fantasy Award. Für ihr Lebenswerk erhielt sie 1979 den Gandalf Grand Master Award, 2003 den Damon Knight Memorial Grand Master Award der Science Fiction and Fantasy Writers of America, 2004 den Margaret A. Edwards Award der American Library Association sowie 2014 die Medal for Distinguished Contribution to American Letter der "National Book Federation".
2001 wurde sie in die Science Fiction Hall of Fame aufgenommen.

Hainish-Zyklus

Erdsee-Zyklus

The Annals of the Western Shore

Einzelromane

Kurzgeschichtensammlungen


Kinderbücher

Catwings (Kinderbuchreihe)

Adventures in Kroy (Kinderbuchreihe)

Gedichte

Übersetzungen

Essays und Sachliteratur









</doc>
<doc id="5338" url="https://de.wikipedia.org/wiki?curid=5338" title="Urdbohne">
Urdbohne

Die Urdbohne ("Vigna mungo"), auch Linsenbohne genannt, ist eine Pflanzenart aus der Unterfamilie der Schmetterlingsblütler innerhalb der Familie der Hülsenfrüchtler (Fabaceae oder Leguminosae). Diese Nutzpflanze ist nahe verwandt mit einer Reihe anderer „Bohnen“ genannter Feldfrüchte, insbesondere zur Mungbohne ("Vigna radiata").

Die Urdbohne wird seit 3000 bis 4000 Jahren auf dem indischen Subkontinent angebaut und ist heute in ganz Süd- und Südostasien verbreitet.

Die Kulturpflanze Urdbohne wächst als niedrige, aufrechte oder hängende, einjährige Pflanze. Diese wurde aus Wildformen selektiert, die als ausdauernde krautige Pflanzen mit 2 bis 4 m langen Stängeln wachsen. Die Kulturpflanzen erreichen je nach Sorte Wuchshöhen von meist nur 20 bis 30 cm, es können auch 60 bis 90 cm werden.

Die behaarten Laubblätter sind dreiteilig und gestielt. Die Blattstiele weisen Längen von etwa 10 cm auf. Die drei breit-lanzettlichen bis spitzovalen Teilblätter weisen eine Breite von 5 bis 7 cm und eine Länge 5 bis 10 cm auf.

Die achselständig an etwa 15 cm langen Blütenstandstielen stehenden, meist zwei- bis dreimal verzweigten Blütenstände, enthalten Teilblütenstände, die jeweils fünf bis sechs Blüten enthalten. Die Blütenfarbe ist leuchtend-gelb. Jede Blüte blüht nur wenige Stunden. Meist erfolgt Selbstbestäubung.
Je Teilblütenstand entwickeln sich meist nur zwei bis drei Hülsenfrüchte. Die rau behaarten, geraden Hülsenfrüchte weisen eine Länge von 4 bis 7 cm und eine Breite von etwa 0,6 cm auf. Jede Hülse enthält vier bis zehn Samen. Die glänzenden, kleinen, quadratischen und an den Enden abgerundeten Samen weisen einen Durchmesser von etwa 4 mm auf. Die Farbe der Samen ist meist sehr dunkelgrün bis schwarz, solche werden deshalb „Black gram“ genannt; es kommen auch grüne Formen vor. Die Samen sind im Inneren weiß, was ein deutliches Unterscheidungsmerkmal zur Mungbohne ist, die im Inneren gelb ist. Das weiße, konkav gewölbte Hilum weist eine Länge von 1,2 bis 2,3 mm und eine Breite von bis zu 1 mm auf. Das Tausendkorngewicht liegt bei 15 bis 40 Gramm.
Die Samen keimen epigäisch (oberirdisch).

Man kann die frischen Hülsen, die Bohnenkeimlinge oder die getrockneten Bohnen verwenden.
In Indien ist die Urdbohne eine der beliebtesten Bohnen. Wegen gleicher Kochzeit lässt sie sich gut mit Reis kombinieren. In Südindien ist ein Teig aus ihr die Grundlage für die pikanten Frühstückskrapfen (Vada), und zusammen mit Reismehl für Klöße (Idli) und Pfannkuchen (Dosa). Sie kann auch anstelle von Linsen in Papadam-Fladen verwendet werden.
Die Urdbohne hat mit etwa 20 bis 24 % (vom Trockengewicht) einen relativ hohen Eiweißanteil.

Die Erstveröffentlichung als "Phaseolus mungo" erfolgte 1767 in "Mantissa Altera", 1, 101 durch Carl von Linné. Der aktuell gültige Name wurde 1956 von Frank Nigel Hepper in "Kew Bull." 11,128 veröffentlicht. 
Synonyme von "Vigna mungo" sind: "Azukia mungo" , "Phaseolus radiatus" , "Phaseolus roxburghii" . Dabei muss man beachten, dass "Phaseolus radiatus" ein Synonym von "Vigna radiata" ist und "Phaseolus radiatus" ein Synonym von "Vigna mungo" ist.

"Vigna radiata" gehört zur Untergattung "Ceratotropis" in der Gattung "Vigna".

Es gibt zwei Varietäten:



</doc>
<doc id="5339" url="https://de.wikipedia.org/wiki?curid=5339" title="Umlaufmotor">
Umlaufmotor

Der Umlaufmotor ist in der Regel ein Verbrennungsmotor, bei dem das Kurbelgehäuse und die Zylinder um die starre Kurbelwelle rotieren; daher wird gelegentlich auch die Bezeichnung Rotationsmotor verwendet. Die Zylinder sind bei vielen Modellen sternförmig um die Kurbelwelle angeordnet und stellen damit eine Variante des Sternmotors dar, es sind aber auch Boxer- und Einzylinderanordnungen konstruiert worden. Der Bewegungsablauf von Umlaufmotoren ist gegenüber herkömmlichen Hubkolbenmotoren kinematisch umgekehrt. Die meisten Modelle von Umlaufmotoren hatten eine feststehende Kurbelwelle mit daran befestigten, drehbar gelagerten Pleueln und ein umlaufendes Kurbelgehäuse mit den Zylindern. Dabei sind Zylinder und Hubzapfen exzentrisch zueinander angeordnet, wodurch der Hub der einzelnen Kolben innerhalb der Zylinder zustande kommt. Bei einigen späten Modellen rotierte die Kurbelwelle gegenläufig zum Zylinderstern, um die absolute Drehzahl des Zylindersterns zu reduzieren. 

Eine weitere Variante eines Umlaufmotors ist die Elbow engine, eine Kraftmaschine, die Dampfdruck oder Pressluft in mechanische Energie umwandelt.

Umlaufmotoren wurden hauptsächlich in der Frühzeit der Fliegerei eingesetzt. Da man auf der Suche nach leistungsfähigen, aber leichten Motoren war, wollte man die aufwendige und schwere Wasserkühlung durch die einfache Luftkühlung ersetzen. Um eine genügend große Luftströmung für die Kühlung zu erhalten und das Schwungrad einzusparen, ließ man die Zylinder um die Kurbelwelle rotieren. Umlaufmotoren hatten bis zu 14 Zylinder, wobei diese Varianten mit ihrem doppelten Zylinderstern wegen der Kühlungsprobleme keine große Verbreitung fanden. Zudem traten wegen der hohen rotierenden Massen große Drehimpulse auf, welche die Manövrierfähigkeit der Maschinen beeinträchtigten.

Der Umlaufmotor wurde 1908 in Gennevilliers (Frankreich) von den Gebrüdern Laurent und Louis Seguin erfunden. Ihr Motor wurde unter dem Namen „Gnôme“ bekannt. Im Deutschen Reich baute ab 1913 die Motorenfabrik Oberursel die Konstruktion in Lizenz unter der Bezeichnung Oberursel-Gnôme U O (7 Zylinder; Hubraum 11,8 l; 60 kW/80 PS bei 1200/min). Dessen Weiterentwicklung, der Doppelsternmotor U III von 1914/15 mit 14 Zylindern und 115 kW/160 PS konnte im praktischen Einsatz nicht überzeugen, da er die oben erwähnten Probleme zeigte.

Nach dem Oberursel U I (9 Zylinder; Hubraum 16,3 l; 70 kW/95 PS) wurde ab Ende 1916 in der Bauart "Le Rhône" der UR IIa (9 Zylinder; Hubraum 15,1 l; 90 kW/120 PS) in mehreren Jagdflugzeugen (Fokker Dr.I, Pfalz Dr.I) der Fliegertruppe des Deutschen Heeres verwendet. Als letzte Oberursel-Entwicklung bestand im April 1917 der UR IIIa die Abnahmeprüfung des Heeres. Dieser Motor mit elf Zylindern und einem Hubraum von 18,4 l leistete 88 kW/120 PS.

Die "Gandenberger’sche Maschinenfabrik Georg Goebel" in Darmstadt konnte nach dreijähriger Entwicklungszeit Anfang 1917 die Bauabnahme für ihren Umlaufmotor Goe II erzielen. Die Konstruktion mit sieben Zylindern, 15,7 Litern Hubraum und 75 kW/100 PS Leistung wurde nur in geringen Stückzahlen gebaut und ab Mai 1918 vom Goe III ersetzt, der mit neun Zylindern und einem Hubraum von 27 Litern eine Leistung von 118 kW/160 PS hatte. Die meisten der 229 ausgelieferten Goe III verwendete Fokker in den Jagdeinsitzern.

Umlaufmotoren benötigen keine zusätzlichen Schwungmassen, da das Kurbelwellengehäuse und die daran befestigten Zylinder als Schwungrad wirken. Dadurch waren sie bei mechanisch ruhigem Lauf um etwa ein Drittel leichter als herkömmliche Motoren. Der Gewichtsvorteil war vor allem beim Einsatz in Jagdflugzeugen des Ersten Weltkrieges ein entscheidender Faktor.

Als Nachteile, die zum Aussterben der Motorenbauart führten, werden verschiedene Eigenschaften angeführt:

Andererseits ergaben sich auch Vorteile, beispielsweise:

Spätere Modelle hatten wegen der Aufheizung eine vom Kurbelgehäuse abgetrennte Gemischzuführung. Einige Muster saugten das Frischgas über Flatterventile an, was bei Drosselung zu heftigen Flammrückschlägen führt, weswegen Umlaufmotoren oft über Aus- und Einschalten der Zündung geregelt wurden. Wenn bei Flugschauen Repliken der alten Flugzeuge mit diesen Triebwerken zur Landung einschweben, ist das Absterben und Wiederanspringen durch das Aus- und Einschalten der Zündung zum Verringern der Geschwindigkeit deutlich zu hören. 

Da das Kraftstoff-Luft-Gemisch durch das Kurbelgehäuse geführt wurde, bestand die Gefahr der Ölverdünnung durch den Kraftstoff mit damit verbundenen Schmierproblemen, weshalb bevorzugt das benzinunlösliche Rizinusöl verwendet wurde. Durch die Rotation der Zylinder traten an den Kolben außerdem Corioliskräfte auf, die seitlich wirkten und einen hohen Wartungsaufwand zur Folge hatten.

Einige späte Muster (z. B. der Siemens & Halske Sh.III von 1917) wurden als Gegenläufer ausgelegt, bei denen sich über ein Getriebe die Kurbelwelle gegenläufig zum Zylinderstern drehte. Bei einer Übersetzung von 1:1 konnte dadurch bei gleicher Relativgeschwindigkeit zwischen Stern und Kurbelwelle und damit gleicher Leistung die Drehzahl des Zylindersterns gegenüber dem Flugzeug halbiert werden. Der Propellerwirkungsgrad wurde damit deutlich gesteigert bei gleichzeitiger Verringerung der negativen Auswirkungen der Rotation wie der Kreiseleffekte und Gaswechselverluste.

Bereits 1892 gab es ein französisches Motorrad von Millet mit Umlaufmotor. In den 1920er Jahren gab es die "Megola", ein Motorrad mit einem Fünfzylinder-Gegenumlaufsternmotor in der Vorderradnabe, dessen Gehäuse (zum Teil zwischen den Speichen herausragend) zusammen mit dem Rad und dessen Kurbelwelle in die entgegengesetzte Richtung rotiert. Die Megola hatte weder Schaltgetriebe noch Kupplung, war aber sportlich sehr erfolgreich und wurde 1924 gegen die BMW-Werksmannschaft deutscher Meister.

Es gab aber auch Umlaufsternmotoren in Motorradrahmen, am bekanntesten und kommerziell am erfolgreichsten war die Redrup Radial von 1912 mit einem Gegenumlauf-Dreizylinder und im Gegensatz zur Megola einem ansonsten konventionellen Antriebsstrang mit externem Schaltgetriebe und Riemenantrieb.

Bei einigen der 1904 in Wales von Barry gebauten Motorrädern war zwar auch ein Umlaufmotor eingebaut, jedoch kein (bei nur zwei Zylindern wie ein V-Motor schüttelnder) Sternmotor, sondern ein Zweizylinder-Boxermotor, der in der Rahmenmitte rotierte.

In den 1940er Jahren konstruierte Cyril Pullin sogar einen Einzylindermotor, dessen Gehäuse in der Radnabe zusammen mit dem Rad rotierte. Als Gewichtsausgleich waren noch Kupplung und Trommelbremse in die "Powerwheel" genannte Radnabe eingebaut. Das "Powerwheel" kam allerdings nicht über das Prototypenstadium hinaus.

Auch in Automobilen amerikanischer Automobilmarken Anfang des 20. Jahrhunderts gab es Umlaufmotoren. Die Hersteller Adams-Farwell, Badger Motor Car Company, Bailey Automobile Company und Intrepid hatten allerdings nur regionale Verbreitung.

In Flugzeugen kamen Umlaufmotoren hauptsächlich in der Zeit vor und während des Ersten Weltkrieges zum Einsatz, wie beispielsweise in den Fokker-Modellen E1- E3 sowie Dr.I. In Großbritannien stammte der erfolgreichste Umlaufmotor von Walter Owen Bentley.



</doc>
<doc id="5340" url="https://de.wikipedia.org/wiki?curid=5340" title="Urgeschichte">
Urgeschichte

Die Urgeschichte (Synonyme Vorgeschichte oder Prähistorie) ist ein Teilgebiet der archäologischen Disziplin Ur- und Frühgeschichte. Sie bezeichnet den ältesten Zeitabschnitt der menschlichen Geschichte, in der noch keine Schriftquellen vorliegen. Die Urgeschichte erstreckt sich vom Auftreten der ersten Steinwerkzeuge vor etwa 2,5 Millionen Jahren (vgl. Stammesgeschichte des Menschen) bis zum regional sehr unterschiedlichen Auftreten von Schrift­zeugnissen. Die Erforschung der prähistorischen Epochen ist allein Aufgabe der Archäologie und ihrer Hilfswissenschaften.

Die nachfolgende Frühgeschichte ist die Periode, in der zumeist indirekte Schriftquellen und archäologische Quellen gleichwertig zur Rekonstruktion des Geschichtsbildes Verwendung finden. Auf diese folgt die Geschichte im engeren Sinne, in der Erkenntnisse vor allem durch Auswertung schriftlicher Quellen erlangt werden und die materielle Kultur zusätzlich archäologisch erschlossen wird (Mittelalterarchäologie und Historische Archäologie).

Die Urgeschichte bzw. Vorgeschichte wird 1836 erstmals von dem dänischen Archäologen Christian Jürgensen Thomsen in einem publizierten Dreiperiodensystem in Steinzeit, Bronzezeit und Eisenzeit gegliedert. Seine auf dänisch verfasste Schrift "Ledetraad til nordisk Oldkyndighed" erschien 1837 in deutscher Übersetzung. Diese bis heute gültige Periodisierung bezieht sich auf die Alte Welt. Die "historische Zeit" beginnt beispielsweise in Nordamerika nicht vor der Entdeckung durch die Spanier.

Der älteste Abschnitt, die Steinzeit, wird in die Altsteinzeit und die mit der Entstehung der Landwirtschaft beginnende Jungsteinzeit gegliedert. Nur in Mittel- und Nordeuropa liegt dazwischen die Epoche der Mittelsteinzeit, die dem jüngeren Epipaläolithikum des Mittelmeerraumes entspricht. Erdgeschichtlich fällt die Altsteinzeit mit dem Pleistozän (Eiszeitalter) zusammen, die folgenden Epochen mit dem Holozän. Das Ende des Pleistozäns bzw. der letzten Eiszeit (Würm III) fand vor etwa zehn Jahrtausenden statt. Ein weiterer tiefer Einschnitt war die Entdeckung der Metallurgie (Kupferzeit). In den meisten Regionen kam es zunächst zur Verarbeitung von Gold und Kupfer und erst später zur Legierung von Bronze (Bronzezeit) und danach zur Eisenverhüttung (Eisenzeit). Durch die Fähigkeit, Metalle mit höheren Schmelzpunkten zu verarbeiten, konnten schließlich leistungsstärkere Werkzeuge und Waffen erzeugt werden, was den produzierenden Regionen erhebliche Vorteile verschaffte. Mit der Entwicklung der städtischen Hochkulturen und der Schrift, erstmals in Vorderasien (Sumer), vollzieht sich regional der Übergang von der Ur- zur Frühgeschichte.

Die Steinzeit ist die früheste Epoche der Menschheitsgeschichte und durch die dominierende Überlieferung von Steinwerkzeugen gekennzeichnet. Sie begann – nach heutigem Kenntnisstand – mit den ältesten gefundenen Werkzeugen vor etwa 2,6 Millionen Jahren in Afrika.

Die Altsteinzeit – fachsprachlich Paläolithikum, von "palaios" ‚alt‘ und "lithos" ‚Stein‘ – war die erste und längste Periode der Urgeschichte und bezeichnet in Afrika, Europa und Asien jeweils den ältesten Abschnitt der Steinzeit.

Die Menschheitsgeschichte beginnt in Afrika vor ca. 6 Millionen Jahren, als Pongide wie "Ardipithecus ramidus" den aufrechten Gang entwickelten. Die nachfolgenden Australopithecinen verfeinerten diese Fortbewegungsweise und gelten als Hominoide; aus ihnen gingen schließlich die ersten Vertreter der Gattung "Homo" hervor: "Homo rudolfensis" und "Homo habilis". Als Kriterium, sie der Gattung "Homo" zuzurechnen, gilt neben anatomischen Unterschieden zu den Australopithecinen vor allem, dass sie vermutlich schon vor 2,6 Mio. Jahren einfache Steinwerkzeuge (Geröllgerät, sog. Choppers und Chooping tools) benutzten. Diese ersten Steinwerkzeuge gehören der Kulturstufe des Oldowan an. Der Zeitabschnitt wird zumeist als Frühpaläolithikum bezeichnet.

"Homo erectus" lebte in jener Kultur, die als Altpaläolithikum bezeichnet wird. Schon für seine frühesten Vertreter ist der Gebrauch von Steingeräten nachgewiesen; spätere Individuen nutzten auch das Feuer, wie die 790.000 Jahre alte Fundstelle Gesher Benot Ya’aqov – die älteste sicher datierte – im heutigen Israel belegt. Vor etwa 1,5 Million Jahren wurde in Afrika die Stufe des Acheuléen erreicht. Vor etwa 500.000 Jahren kam diese Technik auch nach Europa. Sie zeichnet sich durch beidseitig zugeschlagene Artefakte aus, die sehr sorgfältig gefertigt wurden.

"Homo erectus" war so anpassungsfähig, dass er sich von Afrika über Vorderasien einerseits nach Indien, ins heutige China (Peking-Mensch) und nach Südostasien (Java-Mensch) ausbreiten konnte, andererseits nach Süd- und Mitteleuropa (Out-of-Africa-Theorie). Die späten Vertreter von "Homo erectus" – sie werden jedoch häufig zu "Homo heidelbergensis" gestellt – können bereits als Jäger und Sammler bezeichnet werden, da im Braunkohletagebau Schöningen in Niedersachsen acht hölzerne, mindestens 270.000 Jahre alte Wurfspeere (die „Schöninger Speere“) entdeckt wurden.

In Europa entwickelte sich aus "Homo erectus" über die "Homo heidelbergensis" und "Homo steinheimensis" genannten Zwischenstufen der Neandertaler, eine an die spezifischen Umweltbedingungen der Eiszeit hervorragend angepasste Art der Gattung "Homo". Die Neandertaler stellen sich heute (entgegen früheren Annahmen) als kulturell entwickelt dar: Bei ihnen sind in Mitteleuropa zum ersten Mal kultische Praktiken nachweisbar, so sind unter anderem Bestattungen mit Grabbeigaben belegt (z. B. La Ferrassie). Die Neandertaler entwickelten eine Technik der Steinbearbeitung, bei der nicht mehr nur Kerne aus Feuerstein, Quarzit oder anderen Gesteinen zu Werkzeugen modifiziert wurden, sondern auch die scharfkantigen Abschläge weiterbearbeitet wurden (Breitklingenkulturen). Ein spezieller Fall der Bearbeitung sogenannter „präformierter“ Kerne ist die Levalloistechnik. Worauf das Verschwinden der Neandertaler zurückzuführen ist, ist ungeklärt. Er dürfte langsam von Mitteleuropa nach Westeuropa verdrängt worden sein. Die jüngsten bzw. letzten Neandertalfundstellen befinden sich auf der Iberischen Halbinsel.

Nicht aus den nach Europa und Asien ausgewanderten, sondern aus den in Afrika verbliebenen Populationen von "Homo erectus" entwickelte sich in Afrika – zur gleichen Zeit, in der sich in Europa die Entwicklung von "Homo erectus" zum Neandertaler vollzog – eine neue Menschenform, genannt "Homo sapiens". Sie breitete sich, beginnend vor ca. 100.000 Jahren, in einer zweiten Auswanderungswelle der Gattung "Homo" ebenfalls nach Asien und Europa aus und entwickelte sich zum Menschen der Gegenwart fort. Anhaltspunkte für eine aggressive Verdrängung des Neandertalers durch "Homo sapiens" sind nicht nachweisbar. In Europa könnten sich die beiden Arten vor 40.000 bis 35.000 Jahren begegnet sein.

"Homo sapiens" verfeinerte die Methoden der Steinbearbeitung (Schmalklingenkulturen). Bemerkenswert sind die ersten Zeugnisse abstrakten (symbolischen) Denkens, das sich am prägnantesten in Höhlenmalereien ausdrückt, deren älteste auf etwa 35.000 Jahre datiert wurden. Die ältesten bekannten künstlerischen Erzeugnisse sind mit Gravuren verzierte Knochenobjekte aus der Blombos-Höhle in Südafrika, deren älteste auf etwa 77.000 Jahre datiert wurden.

Durch das Ende der letzten Eiszeit verschoben sich die Klimagürtel der Erde nach Norden; im Gebiet des Fruchtbaren Halbmonds in Vorderasien entstand vor ca. 12.000 Jahren erstmals die Landwirtschaft (nach neueren Schätzungen gab es erste Anfänge evtl. sogar vor 15.000 Jahren). Die Domestikation von Tieren und Pflanzen brachte tiefgreifende sozio-ökonomische Veränderungen mit sich, weshalb auch seit dem Archäologen Vere Gordon Childe von der „Neolithischen Revolution“ gesprochen wird. Es gab einige weitere unabhängige Entstehungszentren der Landwirtschaft (zum Beispiel Südchina, Südamerika). Von diesen Zentren breiteten sich die Neuerungen mehr oder weniger schnell aus (nach Mitteleuropa gelangten sie vor ca. 7000 Jahren). Weitere Merkmale der Jungsteinzeit sind die Sesshaftigkeit und die Herstellung von Keramik. Auch in der Steintechnologie gab es Neuerungen; man entdeckte den Schliff von Stein. Die wichtigsten sozialen Auswirkungen der Landwirtschaft sind darin zu sehen, dass durch die höhere Produktivität nicht nur die in der Landwirtschaft tätigen Menschen ernährt werden konnten. Es kam zur Spezialisierung der verschiedensten Berufsgruppen (horizontale Differenzierung) und zur Entstehung von Herrschaft, zunächst in Häuptlingsreichen, dann in Staaten (vertikale Differenzierung).

Der Zeitabschnitt zwischen der Jungsteinzeit und der frühen Bronzezeit, in dem Menschen mit dem Kupferbergbau begannen und grundlegende Techniken der Metallurgie erfanden, wird Kupfersteinzeit, "Chalkolithikum" oder "Äneolithikum" genannt. Kupfer bleibt jedoch knapp und Steinwerkzeuge herrschen weiterhin vor. Ötzi, die Leiche des Mannes vom Tisenjoch, stammt aus der mitteleuropäischen Kupfersteinzeit und hatte ein Beil bei sich, dessen Klinge aus 99 % Kupfer aus dem Salzburger Land besteht.

Bronze, eine metallurgische Legierung von 90 % Kupfer mit 10 % Zinn, wurde im Ende des 4. Jahrtausend v. Chr. erfunden. Sie ist weitaus härter und schnitthaltiger als Kupfer und gibt der Epoche ihren Namen. In Israel ist Bronze ab 3300 v. Chr. belegt. Die Bronzezeit endet in Kleinasien ab 1700 v. Chr. und im Mittelmeerraum spätestens um 1200 v. Chr. mit dem Gebrauch verhütteten Eisens. In Mitteleuropa beginnt die Bronzezeit erst ab 2200 v. Chr. und reicht noch bis 800 v. Chr. Die Hebräer hatten vor allem Bronzewaffen und eroberten Kanaan und die griechischstämmigen Pelischti an der Küste des später nach ihnen benannten historischen Palästina, welche vor allem Eisenwaffen hatten.

Die Eisenverhüttung begann in Kleinasien ab dem 17. Jahrhundert v. Chr. bei den Hethitern. Mit dem Untergang ihres Reiches endet auch ihr Monopol auf die Herstellung von Eisen. Im 12. Jahrhundert v. Chr. ist die Eisenzeit im Vorderen Orient und dem Mittelmeerraum verbreitet und die Bronzezeit endet hier.

Werkzeuggebrauch wurde bei vielen Primaten beobachtet. Am Anfang der Technikentstehung bei den Vorfahren des Menschen stand also die - zunächst wenig ausgefeilte - Werkzeugbearbeitung; von besonderem Interesse ist die Bearbeitung in zeitlichem Abstand von der Werkzeugbenutzung (eine Vorfertigung), was bei Primaten nicht beobachtet werden konnte (außer dem Zurücklegen bereits positiv eingesetzter Materialien). Während Steine als Werkzeuge sogar namensgebend für die technologische Epoche wurden und seit 2,5 Millionen Jahren nachweisbar sind (siehe Steinzeit), so gibt es keinen Grund, anzunehmen, damalige Menschen hätten andere Materialien in ihrer Umgebung nicht verwendet, auch wenn es archäologisch aus der Frühzeit nicht nachweisbar ist (Holz, Knochen, Häute, Pflanzenschalen). Anfänglich erstreckte sich die Benutzung eher auf Wurfsteine, Steine als Amboss oder als Hammer, sowie Schlag- und Stützstöcke. Der Neandertaler und "Homo sapiens" fertigten aus ausgesuchten Steinen hochspezialisierte Speere und Keilmesser an.

Ein weiterer Impuls ging von der Beherrschung des Feuers aus (siehe prähistorische Feuernutzung). Der zielgerichtete Gebrauch des Feuers durch den Menschen gilt für "Homo erectus" vor 790.000 Jahren als nachgewiesen. Die Erwärmung am Feuer half, in der Eiszeit auch kältere Bereiche Europas und Asiens zu besiedeln. Weitreichende Folgen auch auf der Ebene der Evolution könnte das Garen von Speisen mit sich gebracht haben: Die Nahrungsdichte pro Mahlzeit stieg, aber der erforderliche Kaudruck nahm ab, daher brauchten Zähne und Kaumuskulatur nur vermindert ausgebildet zu werden. Demgegenüber konnten weitere (auch festere) Nahrungsstoffe erschlossen werden, für deren Bewältigung andere Spezies erhebliche Zeit und Energie (Wiederkäuer) investieren müssen. Eines der Resultate ist, dass der Mensch eine der wenigen Spezies (neben Wespen) ist, welche den Geruch verbrannten Fleisches (Grillen) appetitanregend finden. Der Mensch ist zudem die einzige Spezies, die den Geruch gerösteter Samen (Backduft, Popcorn, Sesamöl) besonders attraktiv findet. In diesem Zusammenhang wurden sogar Vermutungen formuliert, dass das Volumenwachstum des sehr energiehungrigen Gehirns mit der Garung der Nahrung begründet werden könne, allerdings setzte die Hirnentwicklung bereits in einem Zeitraum ein, in dem die Feuerbenutzung noch kaum anzunehmen ist. Die Annahme intensiven Kochens vor "Homo erectus" entbehrt jeglicher Anhaltspunkte. Neandertaler produzierten aus Birkensaft bei einigermaßen kontrollierter Temperatur von etwa 350 °C Birkenpech als Klebstoff zum Beispiel um Werkstücke (Speerspitze an Speerschaft) zuverlässig zu verbinden.

Angehörige der Gattung "Homo" konnten bereits Hütten bauen. Materialverwendung, Vorfertigung von Werkzeugen und Feuerbeherrschung wurden bereits durch die Vorfahren des "Homo sapiens" entwickelt. Die gemeinsame Jagd begünstigte Techniken der Verständigung und der Strategiefindung. Der Neandertaler fertigte bereits Kleidung, vermutlich als erster Mensch, eine Lebensnotwendigkeit im eiszeitlichen Europa. Möglicherweise parallel dazu oder bei Erstkontakt mit dem Neandertaler erlernte "Homo sapiens" vor mindestens 75.000 Jahren ebenfalls, Kleider herzustellen. Seither gilt auch die Kleiderlaus als nachgewiesen.

Die Befähigung des "Homo sapiens" zum Handel erkennt man daran, dass er Feuerstein (auch entfernten Ursprungs) standardmäßig nutzte und sich auch fernab der Küsten mit Muschelketten schmückte. Die Steinwerkzeuge des Neandertalers mögen denen des modernen Menschen äußerlich ähneln, aber das Material des Neandertalers ist immer regionaler Herkunft.

Die Erfindung der Nadel kam nach dem vermuteten Aufbruch aus Afrika während der Eiszeit bald hinzu. Die Domestikation des Hundes konnte archäologisch noch nicht eindeutig zeitlich eingeordnet werden. Sie erfolgte jedoch sehr früh, lange vor sämtlichen anderen Domestikationen. Diese erste Domestikation gelang vermutlich aus zentralasiatischen Wölfen, zu denen der Mensch dort seit ungefähr 40.000 Jahren Kontakt hatte und wird heute in die Frühzeit der Besiedelung des asiatischen Kontinents datiert, das heißt in die Zeit vor 30.000 bis 40.000 Jahren.

Vor etwa 35.000 Jahren wurde die (Höhlen-)Malerei im Süden Frankreichs entwickelt (Cro-Magnon-Mensch). In diese Zeit fallen früheste Funde von Elfenbeinschnitzereien von Figürchen in Europa. Der älteste Nachweis einer Knochenflöte wird ebenfalls mit etwa 35.000 Jahren datiert (siehe auch Geißenklösterle). Während der Eiszeit wurde Keramik (aus gebranntem Ton oder Lehm) erfunden und entwickelt, nach gängiger Vorstellung als Zufallsprodukt nach Lagerfeuer auf Lehm- oder Tonboden. Die ersten Keramikfiguren (Venus von Dolní Věstonice) können auf mindestens 24.000 Jahre datiert werden. Die ältesten bekannten Keramikgefäße sind etwa 18.000 Jahre alt und wurden zunächst meist in Spiralwulsttechnik ausgeführt (siehe auch Töpferei). Vermutlich vor 16.000–18.000 Jahren wurde der Speerwerfer (Atlatl, durch Armbewegung geschleudert) entwickelt (den Speer gab es bereits viel früher), daraus oder parallel wurden Bogen und Pfeil entwickelt.

Nach Ende der Eiszeit, etwa vor 10.000 Jahren, gelang vermutlich an verschiedenen Orten unabhängig voneinander (in Syrien und China) der Anbau von Grassamen. Entwicklungen in Süd- und Mesoamerika folgten viel später. Zuvor oder parallel zur Domestikation der Pflanzen erfolgte die Domestikation von Tieren, als lebend bevorrateter Fleischlieferant (zur Chronologie siehe Domestizierung). Infolge der Sesshaftigkeit entstanden vorübergehend größere Siedlungen (Jericho, Çatalhöyük).

An verschiedenen Orten der Welt wurde, insbesondere nach Rückzug der Gletscher, in den Kreide- und Juraablagerungen Feuerstein gewonnen, neben Obsidian ein besonders wichtiger Rohstoff. Dazu wurden auch metertiefe Schächte (erste Bergwerksminen) angelegt (Angaben zu steinzeitlichen Minen siehe Feuerstein). Das Material wurde über große Entfernungen verbreitet (siehe Feuersteinstraße in Europa). Parallel wurde Obsidian wegen seiner besonders scharfen Bruchkanten geschätzt, über das Meer transportiert und in Europa verbreitet. Fundorte für Obsidian gibt es nicht viele. Vor etwa 9.000 Jahren begann die Gewinnung auf Melos, Sardinien und den Liparischen Inseln, aber auch in anderen Teilen der Welt.

Die Erfindung der Töpferdrehscheibe erfolgte vielleicht vor oder parallel zur Erfindung des Rades (mindestens vor 6.000 Jahren). Beide Erfindungen gelangen möglicherweise an verschiedenen Orten (aber nicht auf dem amerikanischen und australischen Kontinent). Zur Entwicklung des Rades siehe: Geschichte des Transportwesens im Altertum.





</doc>
<doc id="5341" url="https://de.wikipedia.org/wiki?curid=5341" title="Unehrlicher Beruf">
Unehrlicher Beruf

Unehrliche Berufe waren in der Ständegesellschaft des europäischen Mittelalters und bis weit in die frühe Neuzeit Erwerbsweisen ohne gesellschaftlich zuerkannte Ehrbarkeit.

In der frühneuzeitlichen Gesellschaft waren die Unbescholtenheit des Leumunds und die persönliche Ehre ein wesentliches soziales Kapital. Sie begründeten den Status in der jeweiligen sozialen Gruppe wie insgesamt in der kommunalen Gemeinschaft. Andererseits zog deren Verlust durch Infamierung soziale Ausgrenzung und damit einhergehend Stigmatisierung in unterschiedlichen Graden nach sich.

Unehrliche Berufe trugen den Makel der gesellschaftlichen Verachtung. "Unehrlich" bedeutete, anders als heute, nicht „betrügerisch“, sondern „ehrlos“, „nicht ehrenwert“, ohne ständisches Ansehen. Die Vorstellungen darüber, was Ehrlosigkeit ausmachte, welche Tätigkeiten zu den unehrlichen zu rechnen seien, waren nach Raum und Zeit unterschiedlich, so dass es einen allgemeingültigen Katalog der unehrlichen Erwerbsweisen nicht geben kann.

Zu unterscheiden sind „unehrliche Leute“, die durch bestimmte Erwerbsweisen in Verruf gerieten, nach drei Kategorien:

In der feudalen Ständeordnung standen die Angehörigen unehrlicher Tätigkeitsgruppen als „Unterständische“ am ständegesellschaftlichen Rand. Ihren Paria­status konnten sie nur sehr schwer überwinden. Ein ihnen aufgenötigtes endogames Heiratsmuster, das Verbot der Ausübung gesellschaftlich anerkannter Berufe, das damit einhergehende Zugangsverbot zu den Zünften, häufig auch schlicht Armut hielten diesen Status aufrecht und verhinderten den Wechsel in andere soziale Schichten. Immer neuen Zuzug erhielten sie von Absteigern aus dem sesshaften Unterschichtenmilieu.
Wer in eine Familie mit unehrlichem Beruf hineingeboren wurde, wurde darin in der Regel zeitlebens festgehalten.

Fließend waren die Übergänge in das Milieu der rechtlich, gesellschaftlich und wirtschaftlich ausgeschlossenen Menschen („herrenloses Gesindel“, „Janhagel“) aus der Mehrheitsbevölkerung, die keinem Untertanenverband angehörten und zur Dauermigration gezwungen waren. Sie mussten ihr Leben mit Betteln und mit wenig angesehenen oder verachteten Noterwerbsmethoden wie ambulant ausgeübtem Flickhandwerk, Hausierhandel und Dienstleistungen mit niedrigem Status (Scherenschleiferei, Maulwurffang, Kammerjägertätigkeit) zu bestreiten versuchen. Ein legendärer Repräsentant dieser vagierenden unterständischen und außerständischen Bevölkerung war in Westdeutschland gegen Ende des 18. Jahrhunderts der Schinderhannes Johannes Bückler, der als Sohn eines Abdeckers als Hausierer tätig war.

Eine besondere Position nahmen die Abdecker und die Scharfrichter („Nachrichter“) ein. Zwar standen auch sie in der gesellschaftlichen Rangordnung ganz unten. Dafür steht das Verbot des öffentlichen sozialen Kontakts, wie es in einem separaten Tisch im Gasthaus oder in der Lage des Wohnhauses außerhalb der Stadtmauer Ausdruck fand. Andererseits waren vor allem Scharfrichter, die auch für die Durchführung der Tortur verantwortlich waren, gute Kenner des menschlichen Körpers und abweichender Körperverfassungen. Ihre medizinischen Kompetenzen waren gefragt, sie traten hierdurch in Konkurrenz zu Badern und Ärzten, und es kam im 18. Jahrhundert wiederholt zu Verordnungen, in denen ihnen „alles innerliche und äußerliche Curieren, bei hoher fiscalischer Strafe, gänzlich verboten“ oder nur unter bestimmten Auflagen erlaubt wurde. Nachdem im Laufe des 18. Jahrhunderts der Rechtsstatus der Unehrlichkeit allgemein und auch in ihrem Fall aufgehoben wurde, wechselten sie oder ihre Söhne nicht selten in den Beruf des Arztes.

Unter den Angehörigen „unehrlicher“ Berufe traten Roma nur sehr selten in Erscheinung. Als Scharfrichter oder Abdecker sind sie nicht belegt. Jenische Familiennamen findet man hingegen in den einschlägigen Quellen. So war z. B. Jacob-Peter Huber (* 1771), ein Stammvater des jenischen Politikers Robert Huber, Wasenmeister im bündnerischen Oberhalbstein.

Paria, Buraku (in Japan)



</doc>

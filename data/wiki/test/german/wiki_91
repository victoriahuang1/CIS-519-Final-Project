<doc id="13603" url="https://de.wikipedia.org/wiki?curid=13603" title="Regen">
Regen

Regen ist die am häufigsten auftretende Form flüssigen Niederschlags aus Wolken. Er besteht aus Wasser, das nach Kondensation von Wasserdampf infolge der Schwerkraft auf die Erde fällt.

Regentropfen binden Staub und Aerosole, die in die Atmosphäre aufgestiegen sind. Diese Bestandteile bestimmen den pH-Wert des Regens.

Die Regenformen werden nach Entstehung, Dauer, Intensität, Wirkung und geografischem Vorkommen unterschieden. Fester Niederschlag, z. B. Hagel, Graupel oder Schnee, besteht aus gefrorenem Wasser und Kondensationskeimen und tritt auch gemischt mit Regen auf.

Das gemeingerm. Wort mhd. "regen", ahd. "regan" ist unklarer Herkunft.

Die Kondensation des Wasserdampfes in der Atmosphäre tritt durch Abkühlung und durch
Aerodynamik ein. Zusätzlich bestimmen der Staubgehalt und die Aerosole den Taupunkt abweichend vom Phasendiagramm der theoretischen Thermodynamik.

Ausgangspunkt jedes Regens sind Wolken, die aus feinen Eiskristallen oder "Wolkentröpfchen" (Wassertropfen mit 5 bis 10 μm Durchmesser) bestehen. Sie bilden sich infolge der Abkühlung einer feuchten Luftmasse beim Aufstieg in der Erdatmosphäre, wenn der Taupunkt unterschritten wird. Je nach Höhe und herrschender Temperatur bilden sich entweder Eiskristalle an Kristallisationskeimen durch Resublimation oder Wolkentröpfchen mit Hilfe von Kondensationskeimen durch Kondensation. Diese Primärkörper können, in Abhängigkeit von der Aufenthaltsdauer in der Wolke, weiteren Wasserdampf, andere Wolkentropfen oder auch Eiskristalle an sich binden und dadurch anwachsen. Erreichen Eiskristalle eine wärmere Umgebung, so schmelzen sie wieder zu Tropfen. Wird das Gewicht der Tropfen so groß, dass sie weder durch die Luftreibung (Reibung im Fluid nach dem Gesetz von Stokes) noch von den in einer Wolke vorherrschenden Luftströmungen (Aufwinden) „in Schwebe“ gehalten werden können, beginnen sie aufgrund der Schwerkraft langsam zu Boden zu sinken, und es entsteht der uns bekannte Regen. Das Zusammenwachsen vieler kleiner Wassertröpfchen zu größeren und schwereren beschleunigt diesen Vorgang und erhöht die Fallgeschwindigkeit. In der Regel besteht der am Boden auftreffende Regen aus Tropfen mit einem Durchmesser von 0,6 bis 3 mm.

"Warme Wolken" sind Wolken, in denen nur flüssiges Wasser vorkommt. Je nach Beschaffenheit der Kondensationskerne kann Wasser auch unter 0 °C flüssig bleiben; auch diese unterkühlten Wolken werden "warme Wolken" genannt und sind nach Definition Wasserwolken.

Das Anwachsen von Wolkentröpfchen zu Regentröpfchen allein durch das Aufsammeln von Wasserdampf (Dampfdiffusion) ist wenig effizient und recht langsam. Daher gelten das Zusammenstoßen (Kollision) und nachfolgende Zusammenfließen (Koaleszenz) von Wolkentröpfchen als weitere entscheidende Schritte bei der Entstehung von Regen. Zu Kollisionen kommt es, weil große Tropfen schneller absinken als kleine. Dennoch kommt es nicht immer zur Kollision; häufig werden kleine Tropfen von der Luftströmung um rasch fallende große Tropfen herumgeleitet. Erfolgt nach einer Kollision auch Koaleszenz, so spricht man von "Akkretion", also einem Anwachsen durch Aufsammeln. Dabei führt nicht jede Kollision zwangsläufig zur Koaleszenz. Man spricht in diesem Zusammenhang von Kollisions- und Koaleszenzeffizienz. Die "Kollisionseffizienz" ist für Tropfen ähnlicher Größe mit einem Radius von mindestens 30 µm sehr hoch, die "Koaleszenzeffizienz" hingegen ist höher bei Tropfen mit unterschiedlichen Radien. Große Tropfen kollidieren somit häufiger, jedoch bleiben sie dabei meist unverändert, wohingegen kleine Tropfen eher mit großen zusammenwachsen. Das Produkt aus Kollisions- und Koaleszenzeffizienz nennt man auch "Akkretionseffizienz"; sie ist ein Parameter für die Regenwahrscheinlichkeit von Wolken. Je größer die Tropfen werden, desto schneller wachsen sie. Begünstigt wird dieser Prozess durch einen hohen Feuchtegehalt der Luft (Tropen, Subtropen) oder große Kondensationskerne, wie zum Beispiel in maritimen Luftmassen.

Wenn Wolken während ihres Lebenszyklus ganz oder nur teilweise aus Eispartikeln bestehen, so werden sie "kalte Wolken" genannt. Mischformen werden zum Teil auch als "kühle Wolken" bezeichnet. Der Wachstumsmechanismus ist jedoch gänzlich anders als in "warmen Wolken".

Zwischen unterkühlten Wassertropfen und dem Wasserdampf in ihrer Umgebung besteht ein Gleichgewicht. Wenn die Luft viel Feuchtigkeit enthält, wachsen die Tropfen an, in trockener Luft geben sie Wasser ab. Ein ähnliches Gleichgewicht gibt es zwischen Eiskristallen und Wasserdampf, mit dem Unterschied, dass Eiskristalle die Feuchtigkeit effizienter aufnehmen und weniger dazu neigen, sie wieder abzugeben. Sind in einer Wolke Eiskristalle in der Nähe unterkühlter Wassertropfen, so stellt eine Sättigung des Wasserdampfes bezüglich der Wassertropfen eine Übersättigung bezüglich der Eiskristalle dar. Die Eiskristalle wachsen durch Sublimation (techn. Resublimation) auf Kosten der Tropfen an (Bergeron-Findeisen-Prozess). Es folgt eine Kettenreaktion, die Eiskristalle fallen schneller und zerbrechen bei Kollision mit Wassertropfen zu Splittern, die wiederum anwachsen und somit zur Vereisung der unterkühlten Wasserwolke führen. Am häufigsten vereisen Wolken in einem Temperaturbereich von rund −5 °C bis −15 °C, in diesem Bereich herrscht der maximale Unterschied zwischen dem Sättigungsdampfdruck über Wasser und dem über Eis (siehe dazu auch: Kristallbildung). Damit es bei vereisten Wolken zu Regen (und nicht Schneefall, Graupel etc.) kommt, müssen die Eispartikel beim Sinken wieder wärmere Luftschichten passieren und tauen. Dabei durchqueren sie unter Umständen nochmals Schichten von Wolken mit Wolkentropfen (bei unterkühlten Wolken oder Gewitter-Wolken) beziehungsweise Wasserdampf. Durch Akkretion wachsen sie dort weiter an, der Prozess ähnelt dann jenem in "warmen Wolken".

Bei der Messung gefallenen Regens wird die Menge in Liter pro Quadratmeter oder, wie in der Meteorologie üblich, die Höhe in „Millimetern“ angegeben. Es gilt:

Ein Millimeter Niederschlagshöhe entspricht somit einem Liter Niederschlagsmenge auf einem Quadratmeter. Diese Angabe entspricht auch der Höhe, um die der Wasserspiegel in einem Auffanggefäß (z. B. einer leeren Konservendose) steigen würde.

Der klassische Regenmesser ist daher auch ein Gefäß, das herabfallende Regentropfen auffängt. Aus der Wassermenge im Behälter und der Größe seiner Öffnung kann der Niederschlag berechnet werden. Die ersten Regenmessungen wurden vor etwa 2000 Jahren in Indien vorgenommen, in Europa verwendete man Regensammler zum ersten Mal im 17. Jahrhundert. In den 1670er Jahren bewies Pierre Perrault mit Hilfe eines Regenmessers den Zusammenhang zwischen Regen und der Abflussmenge von Bächen und Flüssen. Bei starkem Wind sind Regensammler ungenau und erfassen tendenziell zu geringe Regenmengen, da Turbulenzen Regentropfen vom Sammler wegtragen. Die ersten Regenmesser wurden auf Dächern montiert, heute stellt man sie in Bodennähe auf, um den Windeinfluss zu verringern. Größe und Form der offiziellen Regenmesser sind von Land zu Land unterschiedlich. Ihre Vereinheitlichung scheiterte bisher daran, dass dadurch die Vergleichbarkeit mit langjährigen Aufzeichnungen eingeschränkt würde.

Neuere Methoden zur Regenmessung sind das Niederschlagsradar und Wettersatelliten, ein dichtes Netz von Regensammlern liefert allerdings genauere Werte.

Weiterhin gibt es noch Regensensoren – diese dienen nicht der Messung, sondern lediglich der Steuerung technischer Prozesse.

Abhängig von meteorologischen und geografischen Bedingungen gibt es unterschiedliche Formen von Regen. Die Klassifikation von Regen kann nach Dauer oder Intensität beziehungsweise nach Entstehung, räumlichen Vorkommen, Wirkung am Boden oder dem Empfinden eines Betrachters erfolgen. Man kann ein und dasselbe Regenereignis in verschiedene Kategorien einordnen, abhängig von der Perspektive des Beobachters – hier einige Beispiele:
Im Allgemeinen entsteht Regen durch einen primären Entstehungsprozess, nach dem die Regenform benannt werden kann. Folgende Formen sind möglich:

 
"Steigungsregen (Steigungsniederschlag)" oder auch "Stauregen" (flüssige Form orografischen Niederschlags) entsteht, wenn Wind feuchte Luft vom Meer oder Flachland an Gebirgszügen oder anderen orografischen Erhebungen (Luv-Seite) aufsteigen lässt. Steigungsregen kommt in den Tropen, Subtropen und gemäßigten Zonen vor. Er kann Stunden bis wenige Tage andauern, in seltenen Fällen auch mehrere Wochen.

Die Luft wird mit zunehmender Höhe immer weiter abgekühlt, dabei sinkt jedoch auch ihre Wasserdampfkapazität und die Lufttemperatur nähert sich immer weiter dem Taupunkt. Zunächst kühlt sich die Luft nach dem Prinzip der trockenadiabatischen Abkühlung um ein Grad Celsius pro 100 Höhenmeter ab ("adiabatisch" bedeutet „ohne Wärmeaustausch mit der Umgebung“, siehe Adiabatische Zustandsänderung). Sobald eine relative Luftfeuchtigkeit von 100 Prozent erreicht ist, kühlt sich die Luft nach dem Prinzip der feuchtadiabatischen Abkühlung nur noch um ungefähr 0,6 °C pro 100 Meter ab. Dabei kondensiert der Wasserdampf der Luftmasse unter Freisetzung latenter Wärme zu Wasser (Wolkentröpfchen), was zur Wolkenbildung führt. Je nach Intensität der Aufwärtsströmung kommt es in der Folge oft zu heftigen Niederschlägen. Diese konzentrieren sich an den jeweiligen orografischen Hindernissen, wo oft hohe Niederschlagsmengen erreicht werden.

Nach der Thermodynamischen Föhntheorie kann der weitere Verlauf wie folgt aussehen: Auf der windabgewandten Lee-Seite erwärmt sich die absinkende Luft, sofern sie komplett ausgeregnet ist, wieder trockenadiabatisch um ein Grad Celsius pro 100 Meter, also schneller als die Abkühlung beim Aufstieg erfolgte. Das kann in tiefen Lagen zu einer wärmeren Luftströmung auf der Lee-Seite führen, die als Föhn bekannt ist.

Gebirge und andere geografische Erhöhungen haben aufgrund dieser Vorgänge und allgemein vorherrschender Hauptwindrichtungen meist eine Regen- oder Wetterseite mit erhöhter Niederschlagsintensität. Das kann in den gemäßigten Breiten ebenfalls zur Entstehung von Regenwäldern führen, man spricht dann vom "gemäßigten Regenwald". Die Gebiete auf der windabgewandten Seite des Gebirges erhalten weniger Niederschläge, sie liegen aufgrund des Lee-Effekts im Regenschatten.

"Konvektionsregen" ist Regen aus Wolken, die sich aufgrund von Konvektionsströmungen bilden. Konvektionsregen kommt vornehmlich in den Tropen und Subtropen, zur warmen Jahreszeit aber auch in den gemäßigten Breiten, also auch in Deutschland, Österreich und der Schweiz vor. Abhängig von der geografischen Lage, kann er zwischen mehreren Minuten (Wolkenbrüche) und mehreren Tagen (Tropenregen) dauern.

Bei warmer Witterung verdunsten große Mengen des im Boden oder auf Wasserflächen vorhandenen Wassers. Die dabei entstehenden bodennahen feuchten Luftmassen werden, aufgrund von ebenfalls durch die Wärme am Boden verursachten Luftströmungen (Wärmeströmungen), in die Höhe transportiert. Erreichen sie ihre Sättigung, so bilden sich Wolken. Die Größe und Art der gebildeten Wolken hängt von der Intensität der Strömungen, der Luftmasse und ihrer Feuchtigkeit, der Temperatur und Bodenbeschaffenheiten (Geografie) ab. Bei optimalen Bedingungen bilden sich so in nur wenigen Stunden oft sehr starke Konvektionsgewitter. Diese treten vor allem in tropischen, aber auch vielen anderen Gebieten der Erde (speziell zur warmen Jahreszeit), häufig am frühen bis späteren Nachmittag auf. Je nach Intensität, Anzahl der Kondensationskeime in der Luft und vorhandener feuchter Luftmassen können sich kleine Wolken oder äußerst große Gewitterwolken bilden.

"Frontregen" (Zyklonenregen, stratiformer Regen) entsteht in einer Warm- oder Kaltfront und kommt in den Subtropen und gemäßigten Zonen vor. Die Dauer des Frontregens ist unmittelbar abhängig von der Aufenthaltsdauer der Front über dem Beobachtungsstandort und von der Temperaturdifferenz an der Front. Frontregen wandert mit der Front mit, Frontregen tritt auch an der Rückseite von Wolkenfeldern auf, die durch Winde anderer Mitteltemperatur in ein Frontensystem "geschoben" werden. Dann setzt der Regen kurzzeitig ein, wenn kurz vor Ende der Passage des Wolkenfeldes der Himmel bereits wieder aufklart.

Frontregen tritt auf, wenn warme und feuchte Luftmassen (oft aus tropischen Gebieten) auf kalte (polare) Luftmassen treffen. Bei einer Warmfront gleitet die leichtere Warmluft auf die schwerere Kaltluft auf, bei einer Kaltfront schiebt sich die schwerere Kaltluft unter die vorhandene Warmluft.

Beim Aufsteigen kühlt sich die feuchtwarme Luft ab, der gespeicherte Wasserdampf kondensiert, Wolken bilden sich und es regnet. Das Entstehungsprinzip ähnelt dem des Steigungsregens, mit dem Unterschied, dass Luftmassen statt fester Hindernisse den Transport der feuchten Luft in die Höhe bewirken.

Diese Formen beschreiben meist die Auswirkung und das Empfinden durch den Beobachter am Boden, der primäre Entstehungsprozess wird bei der Betrachtung meist vernachlässigt.

Als "Dauerregen" oder "Landregen" bezeichnet man ein lang andauerndes Niederschlagsereignis. In den gemäßigten Breiten fällt er fast ausschließlich aus Nimbostratuswolken. Dauerregen kann in den Tropen, Subtropen und gemäßigten Breiten beobachtet werden und mehrere Stunden bis Tage dauern, selten jedoch auch mehrere Wochen. In den gemäßigten Breiten tritt er meist im Zusammenhang mit einer Warmfrontpassage auf. Die jeweilige Definition eines Dauerregens kann je nach Klimagebiet unterschiedlich sein. In Mitteleuropa spricht man im Allgemeinen dann von einem Dauerregen, wenn er mit ununterbrochenen Regenfällen und einer Heftigkeit von über 0,5 Millimeter Niederschlag pro Stunde über einen Zeitraum von mindestens sechs Stunden anhält.

Mit "Starkregen" werden in der Meteorologie große Mengen Regen bezeichnet, die in kurzer Zeit fallen. Diese Art des Regens ist somit nach seiner Intensität und Dauer definiert. Starkregen kommt in den Tropen, Subtropen und gemäßigten Breiten vor und kann von wenigen Minuten bis zu einigen Stunden dauern. Die folgende amtliche Definition ist variabel, da sie sich jeweils auf einen bestimmten Ort bezieht:

Von Starkregen wird im deutschen Sprachraum ab einer Menge von mehr als 5 Litern auf den Quadratmeter in 5 Minuten, mehr als 10 Liter auf den Quadratmeter in 10 Minuten oder mehr als 17 Liter pro Quadratmeter und Stunde gesprochen. Starkregenereignisse können jedoch wesentlich heftiger ausfallen. Ereignisse bei Gewittern, bei denen in 30 Minuten 30 Liter auf den Quadratmeter fallen, sind in Mitteleuropa relativ selten, können aber unter Umständen bereits zu überfluteten Kellern führen. Je stärker und länger anhaltend diese Ereignisse sind, desto geringer ist die Wahrscheinlichkeit ihres Auftretens. Kurze, aber heftige Niederschläge sind wahrscheinlicher als langanhaltende kräftige Niederschläge, die in wenigen Tagen in Mitteleuropa bis zu 200 mm Niederschlag bringen können. Längeranhaltender Starkregen fällt in Europa insbesondere bei Vb-Wetterlagen (sprich „5 b“).

Beispiele: Am 3. Juli 1975 fielen in Shangdi, Nei Monggol, China, 401 Liter auf den Quadratmeter in einer Stunde und am 26. November 1970 38 Liter Regen pro Quadratmeter in einer Minute auf Basse-Terre, einem Inselteil von Guadeloupe. Am 6. Juni 2011 fielen in einigen Stadtteilen Hamburgs 80 Liter Niederschlag pro Quadratmeter innerhalb von 45 Minuten. Am 28. Juli 2014 hat es in Münster 292 l/m² innerhalb von sieben Stunden geregnet.

In den Tropen ist die Neigung zu Starkregen sehr hoch, insbesondere während der Regenzeit in der innertropischen Konvergenzzone. Auch tropische Wirbelstürme führen zu hohen Niederschlagsmengen, vor allem über dem Meer und an den Küsten. In vegetationsarmen Gebieten der wechselfeuchten Tropen und trockenen Subtropen können durch Starkregen („Ruckregen“) katastrophale, stark abtragende Schichtfluten ausgelöst werden. In Europa sind subkontinentale oder kontinentale Bereiche betroffen. In den Küsten- oder Seeklimaten der gemäßigten Zone treten Starkregenereignisse nur sehr selten auf.

Die seit 1998 bekannten atmosphärischen Flüsse können große Wassermengen in nicht-tropische Küstengebiete der Ozeane verlagern und dort extreme Starkregen erzeugen, wie sie beispielsweise im Arkstorm-Szenario des United States Geological Survey verarbeitet werden.

Im Zusammenhang mit dem weltweiten Klimawandel infolge der anthropogenen Zunahme von Treibhausgasen wird auch eine mögliche Zunahme von Starkregenereignissen diskutiert. Dabei zeigte sich 2011 in einer Studie des Gesamtverbandes der Deutschen Versicherungswirtschaft (GDV), dass es regionale Unterschiede gibt. So ist im Süden Deutschlands eine Zunahme zu beobachten, während die Zahl der Ereignisse in Norddeutschland rückläufig ist.

"Platzregen" bezeichnet einen Regen, der sich zeitlich und räumlich nur auf einem kleinen Gebiet abregnet. Er dauert meistens nur wenige Minuten und betrifft oft weniger als einen Quadratkilometer. Das Gebiet ist dabei durch seine Topografie nicht besonders prädestiniert für Regen, so dass es in der Regel weder vorhersehbar noch im Nachhinein erklärbar ist, warum sich diese einzelne Wolke ausgerechnet hier und jetzt abregnet, eine benachbarte Wolke aber nicht. Typische Wetterlagen, die das Auftreten von "Platzregen" befördern, sind die Rückseiten von langsam durchziehenden Kaltfronten, die noch von der Warmfront übriggebliebene Wolkenreste zum Abregnen bringen. Ebenso können starke Vertikalbewegungen der Luft zum Abregnen von an sich stabilen Wolken führen. "Platzregen" können sehr heftig sein "(Starkregen)" und sind dann schwer vom Schauer abzugrenzen. Im Gegensatz zu den verschiedenen Arten des Schauers ist der "Platzregen" aber nicht frühzeitig an seiner Wolkenentwicklung erkennbar. Je nach regionaler Gepflogenheit wird umgangssprachlich nicht zwischen Schauer und "Platzregen" unterschieden. In populärwissenschaftlichen Wettervorhersagen werden "Platzregen" oft mit Formulierungen wie „heiter bis wolkig mit möglicher lokaler Schauertätigkeit“ oder „örtliche Schauerneigung“ angekündigt.

"Sprühregen" oder "Nieselregen" wird nach seiner Form definiert. Er kommt in den Tropen, Subtropen und gemäßigten Breiten vor und kann, abhängig vom Hauptereignis, Stunden bis Tage dauern. Sprühregen besteht aus kleinen Tröpfchen, die üblicherweise aus Stratuswolken fallen. Die Tröpfchen haben einen Durchmesser, der kleiner als 0,5 Millimeter ist. Die Sicht ist bei Sprühregen oft eingeschränkt. Sprühregenschauer kommen nur über der See vor, fallen aus Stratocumuluswolken und werden auch als Miniaturschauer bezeichnet. Bei einer Niederschlagsintensität von bis zu 0,2 Millimeter je Stunde spricht man von einem "leichten", bei 0,2 bis 0,5 Millimeter je Stunde von einem "mäßigen" und bei über 0,5 Millimeter je Stunde von einem "starken" Sprühregen. In Österreich, insbesondere im Salzkammergut, wird ein wegen der Nordstaulage meist länger anhaltender "Nieselregen" auch "Schnürlregen" genannt.

"Unterkühlter Regen" (allgemein "Eisregen" und "Blitzeis", wie das Folgende) besteht aus unterkühlten Regentropfen, die wesentlich kälter als 0 °C sind, in flüssigem Zustand fallen und die beim Auftreffen sofort gefrieren ("Klareis" oder "Glatteis" im engeren Sinne, "Glaze"). Er wird nach seiner Form und Wirkung am Boden definiert und kann am ehesten in den Subpolargebieten, im Winter auch in den gemäßigten Breiten, vorkommen. 

Unterkühlte Tropfen entstehen, wenn saubere Regentropfen durch kalte und sehr reine Luftschichten fallen, wobei sie sich bis weit unter 0 °C abkühlen, jedoch mangels Kristallisationskeimen flüssig bleiben. Trifft so ein unterkühlter Regentropfen auf ein festes Hindernis, nutzt er dies als Kristallisationskeim und gefriert schlagartig, so dass beregnete Gegenstände schon nach kurzer Zeit von einem bis zu mehrere Zentimeter starken kompakten Eispanzer bedeckt sind.

Auf Fahrbahnen führt Eisregen wie gefrierender Regen zu gefährlicher Straßenglätte, auf der selbst Autos mit Winterreifen kaum Halt finden. Gelegentlich kommt es nach Eisregen zum Bruch von Freileitungsmasten, wenn diese dem zusätzlichen Gewicht des Eispanzers auf den Leiterseilen nicht mehr gewachsen sind.

Treffen einige der unterkühlten Regentropfen bereits in der Luft auf Kristallisationskeime (zum Beispiel Staubkörner), ist der Eisregen mit Eiskörnern durchsetzt.

Im Gegensatz zu Eis- oder gefrierendem Regen stehen bereits gefrorene Niederschläge wie Hagel, Graupel und Griesel oder Schnee. Diese entstehen bereits in den Wolken und fallen als fester Niederschlag zu Boden.

"Gefrierender Regen" (allgemein "Eisregen" und "Blitzeis", wie das Vorhergehende) hat seinen Namen nach der Wirkung am Boden, es entsteht beim Auftreffen auf dem Boden dann plötzliches "Raueis".

Solche Ereignisse kommen primär in den gemäßigten Breiten und Subpolargebieten vor und können einige Minuten bis wenige Stunden dauern. In den Tropen und Subtropen kann gefrierender Regen nur im Gebirge auftreten. Gefrierender Regen hat eine Temperatur von über 0 °C, ist also nicht unterkühlt, und gefriert erst nach dem Auftreffen auf eine wesentlich kältere Oberfläche. Diese bildet oft ein nicht durch eine Schneedecke isolierter Boden, der über einen längeren Zeitraum hinweg bei starkem Frost bis in tiefe Lagen ausgekühlt ist. „Gefrierender Regen“ und „Eisregen“ werden oft fälschlich für bedeutungsgleich gehalten. Massives Glatteis auf Fahrbahnen ist in den gemäßigten Breiten oft durch gefrierenden Regen verursacht (weitaus häufiger aber durch überfrierende Luftfeuchtigkeit oder Nebel). Gefährlich ist gefrierender Regen auch für Flugzeuge, da die Eisschicht das Flugzeug schwerer macht (und dadurch den Verbrauch des vor dem Start wohlkalkulierten Treibstoffs und das Landegewicht erhöht) und das Tragflächenprofil verändert, was den Auftrieb der Flügel vermindert (siehe Flugzeugenteisung).

"Warmer Regen" ist nach seiner Wirkung am Boden (gefühlte Temperatur durch den Beobachter) definiert. Er entsteht, wenn tief liegende, warme und feuchte Luftmassen nur gering angehoben werden müssen, um ihre Sättigung zu erreichen und sich dabei fast nicht abkühlen. Dieses Phänomen kann am ehesten in den Tropen und Subtropen, in den Sommermonaten fallweise auch in gemäßigten Breiten, beobachtet werden. Warmer Regen tritt in gemäßigten Breiten meist bei Front- oder Steigungsregen auf. In den Tropen hingegen kann er sich auch unabhängig davon bilden, wenn warme, bodennahe und feuchte Luftmassen durch geringe lokale Strömungen (oder Konvektion) erneut angehoben werden.

Ein wärmeres Klima soll demzufolge mehr warmen Regen nach sich ziehen, das begünstigt extremere Wetterereignisse. Laut einer Studie entfallen momentan rund 31 Prozent des gesamten globalen Niederschlags auf warmen Regen, in den Tropen sogar 72 Prozent.

Lokale Formen sind Regenereignisse bzw. Regenformen, die an ganz bestimmte Gebiete der Erde gebunden sind.

Allgemein versteht man unter einem "Tropenregen" einen oft lange anhaltenden warmen Regen mit mäßiger Intensität, der in den Tropen oder Subtropen vorkommt. Er kann durch unterschiedliche Prozesse gebildet werden, als Hauptursachen gelten jedoch Zenitalregen der ITC und Steigungs- oder Konvektionsprozesse in der sekundären ITC, in manchen Fällen auch Ausläufer von tropischer Wirbelstürmen. Alexander von Humboldt beschrieb Tropenregen als Konvektionsregen, der nur innerhalb der Wendkreise vorkommt. Seiner und der allgemeinen Definition zufolge befinden sich tropische Regenwälder im Gebiet des Tropenregens. In der Literatur wird aber warmer Regen teilweise mit Tropenregen gleichgesetzt. 

"Monsunregen" wird durch den Monsun hervorgerufen und kommt vor allem im Raum des Indischen Ozeans (Indien, Bangladesch, Ost-Australien, Ostafrika, das Dhofar auf der Arabischen Halbinsel) vor. Die Bezeichnung Tropenregen wird oft auch für monsunartigen Regen verwendet. Laut Definition handelt es sich bei Monsunregen um ein langfristiges Ereignis, das nach seiner Entstehungsform am ehesten dem Stauregen zuzuordnen ist. Monsunregen fällt über eine Periode von mehreren Wochen. Dabei sind mehrere abgesetzte und wenige Stunden dauernde, intensive Regenereignisse am Tag typisch. Er kann jedoch als leichter Dauerregen auftreten (siehe Regionale Monsunphänomene).

Hauptbestandteil von Regen ist Wasser in flüssiger Form. Das Wasser kann eine Temperatur zwischen −40 °C (unterkühlt, aber nicht gefroren) und über 20 °C haben. Daneben kann der Regen je nach Entstehungsort weitere chemische Elemente und Verbindungen enthalten. Die Anreicherung des Regens mit zusätzlichen Stoffen reinigt die Luft, kann aber für das Regenwasser die Verunreinigung mit unerwünschten Substanzen mit sich bringen.

Die im Regen enthaltenen Stoffe können sowohl natürlichen Ursprungs als auch anthropogen, das heißt vom Menschen verursacht, sein.

Mit aufgewirbelter Gischt gelangen Na, Cl, Mg und K als Seesalz-Aerosol in die Atmosphäre. Im Regenwasser nehmen die Konzentrationen dieser Ionen landeinwärts ab. Dagegen stammen Ca, NH, HCO und NO im Niederschlag überwiegend aus dem über Landoberflächen fortgewehten Staub.
Aufgrund des gelösten Kohlenstoffdioxids hat unbelastetes Regenwasser einen pH-Wert von 5,6.
In erster Linie natürlichen Ursprungs sind auch die im Regenwasser enthaltenen Spuren von Sauerstoff, Stickstoff, Ozon, Pollen und einigen organischen Verbindungen, z. B. Ameisensäure.

Durch den Menschen gelangen weitere Emissionen in die Atmosphäre, wie etwa Staub, Rauch und Verbrennungsabgase aus Industrie, Verkehr und Hausbrand. Sie können direkt oder in Form ihrer Umwandlungsprodukte die Zusammensetzung des Regenwassers beeinflussen.

In den überwiegend von Menschen verursachten Emissionen kommen auch Stoffe vor, die mit Wasser eine neue Verbindung eingehen können und Regen zu einer leicht sauren Lösung machen. Schwefeloxide (SO) bilden mit Wasser Schweflige Säure (HSO), Stickoxide (NO) bilden Salpetersäure (HNO). Bekannt ist dieses Phänomen als saurer Regen, es kann in der Regel zu etwa zwei Dritteln auf die Verunreinigung mit Schwefliger Säure und zu einem Drittel auf den Gehalt an Salpetersäure zurückgeführt werden.
In Mitteleuropa ging die Intensität des sauren Regens seit den frühen 1980er Jahren zurück. An den Messstationen des deutschen Umweltbundesamts stieg der pH-Wert des gesammelten Regenwassers zwischen 1982 und 2014 von 4,1–4,6 wieder auf 5,1–5,2 an.

Als basischen Regen bezeichnet man Niederschlag, dessen pH-Wert höher ist als der pH-Wert, der sich in reinem Wasser durch den natürlichen Kohlenstoffdioxid-Gehalt der Erdatmosphäre einstellt (pH = 5,6). Basischer Regen ist örtlich sowie zeitlich begrenzt und stellt das Gegenstück zu saurem Regen dar. Ursache für basischen Regen ist zumeist die Emission von größeren Mengen Alkalienstaub in die Atmosphäre. Diese kann z. B. verursacht werden durch:

Der kondensierende Wasserdampf bildet zunächst feinste Tröpfchen, die mit zunehmender Größe immer schwerer werden. Je nach seiner Größe hat ein Regentropfen eine unterschiedlich große Sinkgeschwindigkeit in Luft. In Wolken gibt es Zonen mit aufsteigenden (Aufwind) oder fallenden (Abwind) Luftströmungen. Ein Regentropfen fällt erst zur Erdoberfläche, wenn die Geschwindigkeit der aufsteigenden Luftströmung kleiner als seine Sinkgeschwindigkeit ist. Seine Auftreffgeschwindigkeit auf der Erdoberfläche hängt von seiner Sinkgeschwindigkeit und von der Luftströmung ab, in der er sich befindet. Einen starken Platzregen gibt es in einer fallenden Luftströmung.

Das Gesetz von Stokes kann für kleine Tropfen bis 1 mm mit guter Näherung verwendet werden. Die Sinkgeschwindigkeit eines Tropfens mit einem Durchmesser von 1 mm beträgt ca. 6 m/s. Größere Tropfen verändern ihre Form aufgrund des Luftwiderstands und werden flachgedrückt, sie fallen turbulent. In diesem Fall ist der Strömungswiderstandskoeffizient (c-Wert) geschwindigkeitsabhängig. Er verändert sich permanent während der Beschleunigung. Der Luftwiderstand der Tropfen nimmt mit dem Quadrat der Fallgeschwindigkeit so lange zu, bis die Gewichts- und Widerstandskräfte gleich groß geworden sind, dann fällt der Regentropfen mit (fast) konstanter Geschwindigkeit.

Tropfen bis etwa 1 mm Durchmesser behalten ihre sphärische Form (Kugel), dann beginnen sie sich allerdings durch die Luftgeschwindigkeit im Fallen immer stärker zu verformen. Dadurch nimmt ebenfalls der Luftwiderstand weiter zu, und die Fallgeschwindigkeit bleibt nahezu konstant. Die Tropfengröße ist variabel (A), der größte bisher fotografierte Tropfen hatte einen Durchmesser von 9 mm, in der Regel jedoch zerplatzen Tropfen bereits ab 6 mm zu kleineren. Die Tropfenform ist anfangs kugelförmig (B), mit zunehmender Größe und dadurch resultierender Fallgeschwindigkeit verändert sie sich zu einem "kugelschalenförmigen" (fallschirmartigen, bzw. hamburgerförmigen) Körper (C). Diese Form kann solange beibehalten werden (D), bis der Druck (hervorgerufen durch den Luftwiderstand) an der Innenseite (das ist die der Fallrichtung zugewandte Seite) so groß wird, dass er die Oberflächenspannung des Wassers überwindet (E). Die maximal erreichbare Größe eines Tropfens ist somit auch von der Zusammensetzung und Temperatur des ihn bildenden Wassers abhängig.

Das Kräftegleichgewicht von Gewichtskraft und Reibung bei konstanter Fallgeschwindigkeit bildet den Ansatz für die Berechnung mittels C-Wert, oder mittels Gesetz von Stokes. Zur Vereinfachung werden keine Vorzeichen oder Vektoren verwendet, die Fallrichtung ist immer in Richtung Erde und der Luftwiderstand wirkt entgegen. Zusätzliche Einflüsse wie Luftströmungen (Auftrieb), Temperatur, Oberflächenspannung des Tropfens (Materialbeschaffenheit) oder veränderliche Form des Tropfens werden hier nicht berücksichtigt.

Kräftegleichgewicht, Ansatz für folgende Betrachtungen:

Folgende Größen werden dabei verwendet:

Die Fallgeschwindigkeit von Partikeln bis ~1 Millimeter nach dem Gesetz von Stokes ergibt sich aus folgender Kräftegleichung:

Wenn formula_4, dann folgt für die Geschwindigkeit:

Mit dieser Formel lassen sich auch Sinkgeschwindigkeiten von Staubpartikeln in der Luft berechnen. Diese können durch starke Winde (Wüstenstürme), Vulkanausbrüche, Kernwaffenversuche oder Meteoriteneinschläge in große Höhen (bis 30 Kilometer) der Atmosphäre gelangen. Bei langer Aufenthaltsdauer infolge geringer Sinkgeschwindigkeit und einer großen Menge an Partikeln kann es zu starker Abkühlung kommen. Man spricht dann, im Falle von Großereignissen (Supervulkanausbruch, große Meteoriteneinschläge, Atomkrieg), auch vom Nuklearen Winter.

"Beispiel:"
Die Absinkzeit eines Staubpartikels mit einer Größe von einem µm, der in eine Höhe von 20 Kilometer geschleudert wurde, beträgt nach obiger Formel somit 1,8 Jahre. Das deckt sich recht gut mit allgemeinen Beobachtungen.

Für die Fallgeschwindigkeit von Partikeln zwischen ~1 Millimeter bis 3 Millimeter muss die Kräftegleichung angepasst werden. Je nach Gewicht und Tropfenform – die ja selbst wieder geschwindigkeitsabhängig ist – variiert der C-Wert hier zwischen 0,35 (Kugel) bis 1,3 (fallschirmartig oder offene Halbkugel), aus:

folgt für die Geschwindigkeit:

Als grobe Abschätzung empfiehlt sich folgende Faustformel: Fallgeschwindigkeit in "m/s" ≈ 6 · Tropfendurchmesser in Millimeter (nur in einem Bereich von 0,5 bis max. 1,5 mm Tropfengröße annähernd richtig). Ein Tropfen der Größe 1 mm fällt mit einer Geschwindigkeit von etwa 6 m/s ≈ 20 km/h.

Regen ist die häufigste Form von Niederschlag und trägt dazu bei, den Wasserkreislauf zu schließen, der für das Leben auf der Erde ein entscheidender Faktor ist. Langfristig tragen die durch Regen gespeisten Bäche und Flüsse ganze Gebirge ab. Bei entsprechenden geologischen Verhältnissen können Schluchten und Canyons entstehen. Regen reinigt die Luft und wäscht Staub, Pollen und sonstige Partikel aus. Er löst weiterhin Sauerstoff, Stickstoff, Kohlensäure, Schwefelsäure und Salpetersäure aus der Luft. Die gelösten Stoffe führen zu einer erhöhten Erosion und der Verwitterung von Gestein und Boden, sowie zu einer erhöhten Regenerosion bei Gebäuden, Maschinen und Anlagen (zum Beispiel an Flugzeugflügeln). Regen löst außerdem Mineralien aus Gestein und Boden, die als Nährstoff für Pflanzen sowie andere Lebensformen dienen.

"Übermäßiger Regen" kann langfristig zu einer Veränderung des lokalen Klimas (Mikroklima und Mesoklima), und damit auch zu einer Veränderung von Fauna und Flora führen. Ebenso kann dadurch eine Abspülung (Denudation), beziehungsweise flächenhafte Erosion oder Vernässung des Bodens erfolgen. Kurzfristiger übermäßiger Regen kann lokal zu Sturzbächen und Überflutungen führen. Bei Hanglagen und im Gebirge kann er Hang- oder Erdrutsche und Gerölllawinen hervorrufen.

"Ausbleibender Regen" führt langfristig zu Dürre und somit zu einer Veränderung des lokalen Klimas, was ebenso Veränderungen bei Fauna und Flora hervorrufen kann. Dieser Prozess fördert die Desertifikation. Durch die verringerte Regenerosion bleiben aber Bauwerke, Anlagen und Maschinen unter Umständen länger erhalten; die Pyramiden von Gizeh sind ein Beispiel für geringe Erosion über Jahrtausende. Kurzfristig ausbleibender Regen (Austrocknung) verändert das lokale Klima nicht und stellt somit keine Bedrohung für Fauna und Flora dar.

Karl August Wittfogel These von der Hydraulischen Gesellschaft prägte lange die Vorstellung von Gesellschaften, bei denen die Verteilung und Regulierung der Wasservorkommen und seltener Regenfälle zentral war. Zentral war diesen Gesellschaften ein Staatskult (mit einer mächtigen Beamten- und Priesterschaft) und zentralisierte typische Herrschaftsformen eines „Hydraulischen Despotismus“.

Er nannte dabei die im Altertum das chinesische Kaisertum zur Zähmung des Huang Hes, die im Punjab am Indus früh erscheinende Hochkultur, die Regulierung des Euphrat und Tigris in Mesopotamien (vgl. Babylonisches Reich), das ägyptische Pharaonentum am mittleren und unteren Nil und – mit Abstrichen – das Aztekenreich in Mexiko (vgl. Tenochtitlán) bzw. Inkareich in Peru vor ihrer Zerstörung durch den spanischen Imperialismus. Technische Kenntnisse, in der Wasserbewirtschaftung wie im Bereich der Astronomie (bzw. Astrologie) spielten dabei eine zentrale Rolle.

In altorientalischen Regionen und Epochen wurden Gewitter und Sturm als numinose Gewalt empfunden, mit wichtigen Unterschieden in der jeweiligen Mythologie. So spielte der Wettergott im vom Bewässerungsfeldbau geprägten Babylonien weniger eine Rolle als Regenspender, sondern stärker als Herr der Stürme. In den stärker vom Regenfeldbau geprägten Gebieten des Alten Orients, also in Obermesopotamien, Syrien, Anatolien und auch in Assyrien, nahm er eine bedeutendere Stellung unter den großen Gottheiten ein als in Babylonien.

In China war der Regen Symbol für Fruchtbarkeit und Zeugung. Nach alten mythologischen Vorstellungen erzeugte ihn der Drache mit Hilfe von Bällen. Unter "Wolken-und-Regen-Spiel" verstand man damals in China auch die geschlechtliche Vereinigung von Mann und Frau.

Der Beginn der modernen Wetterkunde wird auf den Bau des ersten Thermometers durch Galileo Galilei um 1600 datiert. Zuvor versuchte man dem erhofften Niederschlag auch durch magische Praktiken nachzuhelfen; ein Beispiel dafür sind die Regentänze verschiedener afrikanischer und indigener Völker. Der Regenmacher ist ein in Chile solchen Praktiken entstammendes Musikinstrument. Scherzhaft wird die seit dem letzten Jahrhundert beschriebene technische Regenerzeugung durch mit Hagelfliegern verbreitetes Silberjodid auch so genannt. Bei den Olympischen Sommerspielen in Peking 2008 wurde Silberiodid mit Hilfe von Raketen in Regenwolken eingebracht, um diese an der Störung der Eröffnungsfeierlichkeiten zu hindern. In Deutschland wird die Regenerzeugung im Landkreis Rosenheim und in Österreich in der Süd-, West- und Ost-Steiermark regulär zur Hagelabwehr verwendet. In Thailand spielt die auf eine Initiative von König Bhumibol zurückgehende Erzeugung von Fon luang (Thai: , "königlicher Regen") eine zentrale Rolle im Verhältnis zur dortigen Monarchie.

In Deutschland ist Münster für sein häufig regnerisches Wetter bekannt. Obwohl die Niederschläge im Jahresmittel nicht aus der Reihe fallen, gilt als sprichwörtlich „In Münster regnet’s, oder es läuten die Glocken, und wenn beides ist, ist Sonntag“. Darüber hinaus wird mit "meimeln" im lokalen Dialekt Masematte ein flüchtiger leichter Dauerregen bezeichnet. Im niederbayrischen Regen wird gegenüber dem lokalen Rivalen Zwiesel gern angeführt: „In Zwiesel konns reign, aba in Reign konns nit zwieseln.“ Sprichwörtlich wird überregional „Auf Regen folgt Sonnenschein“ verwendet. Ein bekanntes Palindrom der deutschen Sprache „Ein Neger mit Gazelle zagt im Regen nie“ enthält ebenfalls einen Regenbezug. Im Mittelalter hielt sich die Theorie, dass Blattläuse im Sommer durch Regen, den sogenannten Neffenregen, gehäuft Nutzpflanzen befielen.

In Österreich ist insbesondere Salzburg und das angrenzende Salzkammergut für seinen lang anhaltenden Schnürlregen bekannt.

Im insbesondere katholischen Christentum gilt der Heilige Georg als einer der Vierzehn Nothelfer und ist unter anderem für gutes Wetter zuständig, die Tradition der Georgiritte geht unter anderem darauf zurück. Eine zentrale Rolle als Hoffnungssymbol und besondere Naturerscheinung spielt in vielen Kulturen der Regenbogen, im Christentum als zentrale Verheißung Gottes, die Sintflut nicht zu wiederholen und den Bund mit den Menschen zu erneuern.





</doc>
<doc id="13604" url="https://de.wikipedia.org/wiki?curid=13604" title="Chimborazo">
Chimborazo

Der inaktive Vulkan Chimborazo ([], []), auch "Tschimborasso", ist mit (nach neueren Angaben ) Höhe über dem Meeresspiegel der höchste Berg in Ecuador. Sein Gipfel hat von allen Punkten der Erdoberfläche die weiteste Entfernung zum Erdmittelpunkt.

Der Chimborazo liegt in der Westkordillere der Anden, in der nach ihm benannten Provinz. Sein Nachbargipfel ist der hohe Carihuairazo. Der majestätische Gipfel des Chimborazo ragt 2500 m aus der etwa 3500 bis 4000 m hohen ihn umgebenden Hochebene. Sein Durchmesser beträgt an der Basis etwa 20 km. Bei idealen Verhältnissen, meist in den Wintermonaten (Dezember–April), kann der Gipfel von der Küstenstadt Guayaquil aus gesehen werden. Die wichtigsten Städte in seiner Umgebung sind Riobamba (etwa 30 km südöstlich), Ambato (etwa 30 km nordöstlich) und Guaranda (etwa 25 km südwestlich des Berges). Der Chimborazo liegt innerhalb des Naturreservates "„Reserva de Produccion Faunistica Chimborazo“", das dazu dient, den Lebensraum für die in den Anden heimischen kameliden Vicuña, Lama und Alpaca zu schützen.

Der obere Teil des Berges ab zirka 5100 m ist vergletschert. Einzelne Gletscherarme reichen bis 4600 m hinunter. Die Gletscher des Chimborazo stellen die Wasserversorgung für große Teile der Provinzen Bolívar und Chimborazo. Die Gletscher haben in den letzten Dekaden aufgrund von globaler Erwärmung, Aschebedeckung infolge der aktuellen vulkanischen Aktivität seines östlichen Nachbars Tungurahua (Schotterer "et al." 2003) und dem El-Niño-Phänomen signifikant an Masse verloren.

Wie bei anderen ecuadorianischen Bergen wird auch das Eis der Gletscher des Chimborazo von sogenannten "Hieleros" (vom spanischen "Hielo" für Eis) abgebaut, um auf den Märkten von Guaranda und Riobamba verkauft zu werden. Früher wurde das Eis bis hinunter in Küstentieflandstädte wie Babahoyo oder Vinces transportiert (Borja 2004). Baltazar Ushca steigt als einziger verbliebener Hielero noch regelmäßig zu den Eisminen des Chimborazo hinauf.

Der Vulkanismus am Chimborazo ist Folge der Subduktion der Nazca-Platte unter den südamerikanischen Kontinent und hat überwiegend andesitisch-dazitischen Charakter. Der Schichtvulkan ist um das Jahr 550 (± 150 Jahre) zum letzten Mal ausgebrochen.

Mit ist der Chimborazo der höchste Berg von Ecuador und höher als alle nördlicheren Berge Amerikas. Bei einer 1993 durchgeführten Differential-GPS-Messung wurde eine Höhe von nur festgestellt. Auch SRTM-Daten weisen darauf hin, dass diese Höhe eher wahrscheinlich ist als die weitverbreitete Höhenangabe von .

Der Chimborazo galt vor der Vermessung des Himalaya als der höchste Berg der Erde überhaupt. Die Messungen George Everests im Jahr 1856 zeigten jedoch, dass viele Himalaya-Gipfel, insbesondere der Mount Everest deutlich höher liegen als der Chimborazo. Selbst in den Anden kennt man heute viele höhere Berge und Vulkane. Der höchste Berg in den Anden ist der Aconcagua mit Höhe, der höchste Vulkan der Erde ist der ebenfalls in den Anden gelegene Nevado Ojos del Salado mit Höhe.
Der Gipfel des Chimborazo ist wegen seiner Nähe zum Äquator der Punkt der Erdoberfläche, der am weitesten vom Erdmittelpunkt entfernt ist. Dass er hierin den wesentlich höheren Mount Everest übertrifft, liegt daran, dass die Erde aufgrund der Rotation und der sich daraus ergebenden Fliehkraft keine Kugel ist, sondern ein Rotationsellipsoid, dessen Radius an den Polen kleiner und am Äquator größer ist. Nimmt man den Erdmittelpunkt als Bezugspunkt, so übertrifft nach Senne (2000) der Chimborazo (1° südl. Breite, 6384,557 km vom Erdmittelpunkt) den Mount Everest (28° nördl. Breite, 6382,414 km vom Erdmittelpunkt) um mehr als zwei Kilometer.

Einerseits ist wegen der maximalen Entfernung von der Erdachse die Zentrifugalbeschleunigung durch die Erddrehung hier am größten, andererseits wegen der maximalen Entfernung vom Erdmittelpunkt die Gravitationsbeschleunigung am geringsten, was beides dazu beiträgt, dass am Gipfel des Chimborazo die Fallbeschleunigung den sehr niedrigen Wert 9,767 m/s² besitzt.

Bezüglich der Herkunft des Namens "Chimborazo" existieren verschiedene Theorien. Es kann eine Kombination des Cayapa-Wortes für Frau – "Schingbu" – und des Kichwa-Wortes für Eis/Schnee – "Razo" – sein, was in etwa "Eisige Frau" ergeben würde, oder mit "Chimbo" für Thron/Gottesthron aus dem Shuar ergäbe es "Eisiger Thron Gottes". Von der in der Umgebung lebenden indigenen Bevölkerung wird der Berg auch "Urkurasu (Urcorazo)" genannt; mit dem Quichuawort "urku" – Berg – ergibt das einfach "Berg-Eis".

Im lokalen indigenen Mystizismus etwa der Puruhá ist der Chimborazo ein heiliger Berg. "Taita Chimborazo" ("Taita" ist Kichwa für Vater) repräsentiert als Mann von "Mama Tungurahua" den Stammvater der Puruhá.

Der Chimborazo wurde 1565 zum ersten Mal von einem Europäer, Girolamo Benzoni, beschrieben. Die Franzosen Charles Marie de La Condamine und Pierre Bouguer unternahmen 1742 Forschungen am Berg. Einen ersten richtigen Besteigungsversuch wagte bereits Alexander von Humboldt zusammen mit Aimé Bonpland und Carlos Montúfar am 23. Juni 1802; sie erreichten eine Höhe von zirka 5600 m (ihre eigene damalige Schätzung betrug 5900 m). Der Beschreibung des Aufstiegs durch Humboldt verdanken wir die erste genaue Schilderung der Symptome von Höhenkrankheit. Humboldt verbrachte mehrere Tage am Berg. Er skizzierte ihn und ließ sich mit ihm im Hintergrund abbilden. Im Dezember 1831 scheiterte auch der Naturforscher Jean Baptiste Boussingault aus Frankreich. Den Gipfel erreichte als erste eine britisch-italienische Seilschaft bestehend aus Edward Whymper und den Brüdern Jean-Antoine und Louis Carrel am 4. Januar 1880. Die Schutzhütte auf der Südwestseite unterhalb des Gletschers auf 5000 m wurde zu Ehren des Erstbesteigers „Edward-Whymper-Hütte“ benannt. Da viele Kritiker die gelungene Erstbesteigung anzweifelten, bestieg Whymper noch im selben Jahr den Berg ein zweites Mal über eine neue Route (von Pogyos im Westen her), mit den zwei Ecuadorianern David Beltrán und Francisco Campaña (Whymper 1892).

Am 15. August 1976 ging SAETA-Flug 232 mit 55 Passagieren und 4 Besatzungsmitgliedern auf der 309 km langen Route von Quito nach Cuenca verloren. Unverzügliche Suchaktionen in möglichen Absturzregionen blieben erfolglos. Obwohl ein Absturz am Chimborazo als am wahrscheinlichsten galt, konnte kein Wrack gefunden werden. Nachdem das abgestürzte Flugzeug und seine Insassen über 26 Jahre als vermisst gegolten hatten, wurden die Überreste am 17. Oktober 2002 von ecuadorianischen Bergsteigern, die die selten begangene Integralroute benutzten, auf einer Höhe von zirka östlich des Chimborazohauptgipfels gefunden.


Die Besteigung des Chimborazo ist bei Bergsteigern sehr beliebt. Trotz seiner Höhe ist er gut zugänglich und die Normalroute vergleichsweise einfach zu besteigen.


Die einfachsten und am meisten benutzten Routen sind die Normalroute (Schwierigkeitsgrad I/F-PD-) und die Whymperroute (II/PD+). Beide Routen starten bei der Whymper-Hütte und führen via den Westgrat und den Vorgipfel "Ventimilla" (6.228 m) zum Hauptgipfel ("Whymper" oder "Ecuador") (6.268 m).

Es existieren verschiedene andere, weniger benutzte und meist schwierigere Routen über die verschiedenen Seiten und Grate zu einem der folgenden Gipfel: Hauptgipfel ("Whymper", "Ecuador"), Zentralgipfel ("Politecnico") und Ostgipfel ("N. Martinez").

Es sind zwei Hütten in Betrieb, die Carrel-Hütte () und die ein wenig weiter oben liegende Whymper-Hütte (). Die Carrel-Hütte ist durch eine Straße erschlossen und kann von Riobamba, Ambato oder Guaranda erreicht werden. Die Zurita-Hütte () an der "Pogyos"-Route ist nicht mehr in Betrieb.






</doc>
<doc id="13606" url="https://de.wikipedia.org/wiki?curid=13606" title="Brocken (Begriffsklärung)">
Brocken (Begriffsklärung)

Brocken (althochdt. "broccho" ‚Abgebrochenes‘) steht für:

Berge:

Sonstiges:
Siehe auch:


</doc>
<doc id="13608" url="https://de.wikipedia.org/wiki?curid=13608" title="Eishockey">
Eishockey

Eishockey ist eine Mannschaftssportart, die mit fünf Feldspielern und einem Torwart auf einer etwa 60 m langen und 30 m breiten Eisfläche gespielt wird. Ziel des Spiels ist es, das Spielgerät, den Puck, eine kleine Hartgummischeibe, in das gegnerische Tor zu befördern. Die Spielzeit beträgt üblicherweise dreimal 20 Minuten netto. Da bei jeder Spielunterbrechung die Uhr angehalten wird, dauert ein Spiel etwa zwei bis zweieinhalb Stunden.

Verbreitung fand der Sport insbesondere durch die Erfindung und Errichtung von Kunsteisbahnen seit Anfang des 20. Jahrhunderts.

Eishockey ist durch die Internationale Eishockey-Föderation (IIHF) international organisiert. Der Weltverband hat bis heute 64 Mitgliedsverbände. Als spielerisch beste Eishockeyliga der Welt gilt die nordamerikanische National Hockey League (NHL).

Vorläufer des Eishockeysports finden sich in Friesland oder auch in den Niederlanden. Künstlerische Abbildungen von ähnlichen Sportarten datieren bis ins 16. Jahrhundert. Ein anderes Spiel aus jener Zeit, welches dem heutigen Eishockey sehr ähnelt ist bis heute unter dem Namen Bandy bekannt. Andere Spuren verweisen auf Dänemark im Jahr 1134. Die Schlittschuhe waren bis in die Zeit der Industrialisierung aus Knochen.

In Nordamerika kannten schon die indianischen Ureinwohner "Kanattas" im 16. Jahrhundert verschiedene Ballspiele. Durch die französische Kolonisation Kanadas in der Mitte des 16. Jahrhunderts vermischten sich jene Ballspiele mit denen der Soldaten zum heute bekannten Lacrosse. Der "Camburca", ein Krummstock, entwickelte sich zu einer Art Hockey- oder Eishockeyschläger. Mitte des 18. Jahrhunderts kamen die Engländer nach Kanada und die Soldaten brachten die ihnen bekannten Spiele Hurling und Shinty, auch Shinney genannt, mit. Im Lauf der Zeit wurden das Shinney-Spiel aufgrund der vorherrschenden Bedingungen schnell aufs Eis übertragen. Anfangs spielten nur die Soldaten, doch es kamen Studenten aus Montréal hinzu, die feste Spielregeln entwickelten, sodass sich allmählich ein fester Ligenbetrieb entwickelte. Das älteste belegbare „erste Eishockeyspiel in einer Halle“ fand am 3. März 1875 im Victoria Skating Rink in Montreal statt und wurde von James Creighton organisiert, einem Studenten der McGill University. 1917 wurde die Profiliga National Hockey League gegründet, die zunächst nur kanadische, später auch US-amerikanische Mannschaften umfasste. In Kanada wird Eishockey als Nationalsymbol beansprucht; europäische Traditionen des Spiels werden hierbei ausgeklammert.

In Europa, das bereits lange vergleichbare Spiele kannte, verbreiteten sich die Spielidee und die Regularien bereits gegen Ende des 19. Jahrhunderts, wo diese sich zunächst vor allem in den klimatisch günstigen Gebieten wie der Alpenregion oder Skandinavien etablierten, sowie in Großstädten, sofern diese über eine Kunsteisbahn verfügten.

Die Herkunft des Worts "Hockey" ist ungeklärt, möglich erscheint eine Herleitung von dem französischen Wort für Stock oder dem englischen Begriff „hook“ (gekrümmt). Die deutsche Sprache kennt mit "Hocken" (Althochdeutsch: "hocchan") ein phonetisch ähnliches Wort, aus dem sich das früher in Deutschland für das Spiel verwendete Wort "Eishocken" ableitet.

Geschichtliche Bedeutung erlangten oftmals die Spiele zwischen der damaligen Sowjetunion und den USA oder Kanada vor allem in der Phase des Kalten Krieges als Ausdruck der Rivalität der Systeme. So wurde auch das Spiel der USA gegen die UdSSR beim Olympischen Eishockeyturnier 1980 zu einer Art „Show-down“ der beiden Weltmächte. Die USA konnten es gegen die seinerzeit übermächtige Sowjetunion gewinnen, das Spiel ging als „Miracle on Ice“ in die Geschichte ein.

Die Bedeutung des Eishockey in der Sowjetunion nahm ab den 1950er Jahren erheblich zu. So gelang es dem sowjetischen Team, bereits bei der ersten Weltmeisterschaftsteilnahme 1954 wie auch bei der ersten Teilnahme am Olympischen Eishockey-Turnier die Goldmedaille zu gewinnen. Weiterhin gab es zwischen der UdSSR und der ČSSR bedeutende Spiele, in denen die sowjetische Mannschaft mit herausragenden tschechoslowakischen Spielern konkurrierte. Auch hier wurde, insbesondere nach dem Prager Frühling 1968, die unterschiedliche politische Gesinnung und die politischen Befindlichkeiten in den Sport getragen.

Nach dem Zusammenbruch der Sowjetunion gingen einige der besten Spieler in die "National Hockey League", in der Mannschaften der USA und Kanadas spielen. Die NHL stellt heute die bedeutendste Liga der Welt dar, mit vielen der besten Spieler aus Europa und anderen Teilen der Welt. Bei den alljährlich stattfindenden NHL All-Star Games zeigt sich, dass viele europäische Spieler heute zu den wichtigsten Leistungsträgern der Liga gehören. Die Sportlandschaft in Nordamerika ist pluralistischer aufgestellt und anders als beispielsweise in Europa nicht von einer einzigen Sportart dominiert. Folglich fallen Spielereinkünfte in vielen Sportarten höher aus und ziehen wie im Fall des Eishockey Spieler aus Europa an.

Das Eishockey für Frauen entwickelte sich aus dem Herreneishockey und unterscheidet sich von diesem in verschiedenen Regelanpassungen. Die wichtigste davon ist, dass bei den Frauen das Drücken gegen die Bande sowie Body-Checks grundsätzlich verboten sind. Diese Regelung wurde nach der ersten Weltmeisterschaft 1990 eingeführt, da es dort zu schweren Verletzungen aufgrund des Aufeinandertreffens von unterschiedlich robust gebauten Frauen gekommen war. Darüber hinaus sind die Frauen, genauso wie alle Nachwuchsspieler, verpflichtet, mit Helmen mit Gesichtsschutz zu spielen.

Das erste olympische Fraueneishockey-Turnier fand 1998 in Nagano statt. Genau wie die Weltmeisterschaften werden diese Wettbewerbe vor allem durch die Mannschaften aus Nordamerika dominiert. Dort gibt es auch ein ausgeprägtes Ligasystem, das dem der Männer gleicht. Populär ist Fraueneishockey auch im nördlichen Europa und in Russland.

Bei den Winterspielen von 2006 in Turin konnte mit Schweden zum ersten Mal eine nichtamerikanische Mannschaft die Silbermedaille gewinnen.

Beim Eishockey muss ein Puck, eine flache Hartgummischeibe, mit Schlägern in das gegnerische Tor geschoben oder geschossen werden. Eishockey gilt als sehr schnelle Mannschaftssportart und zudem sehr körperbetonter Sport, bei dem es auch zu handfesten Auseinandersetzungen zwischen den Spielern kommen kann. In der Regel werden diese Auseinandersetzungen schnell unterbunden und enden ohne Verletzungen, jedoch gab und gibt es auch immer wieder Vorfälle, die schwere Verletzungen nach sich zogen (→ Gewalt im Eishockey). Mittels so genannter "Bodychecks" ist es möglich, den Gegner den Regeln entsprechend seitlich zu verdrängen oder aus dem Weg zu schaffen, um den Puck zu erobern.

Das weltweit geltende Regelwerk wird regelmäßig im Regelbuch der Internationalen Eishockey-Föderation (IIHF) herausgegeben. Die aktuelle Version ist bis 2018 gültig. Das Regelbuch ist in zwölf Abschnitte unterteilt, die die Themen "Spielbetriebliche Vorschriften", "Spielfeld", "Teams und Spieler", "Spielerausrüstung", "Spielregeln/Allgemein", "Spielregeln/Spielunterbrüche", "Spielregeln/Spielerwechsel", "Spielregeln/Tore", "Spielregeln/Dauer und Situationen", "Strafenbeschreibungen", "Strafschüsse und zugesprochene Tore" sowie "Spezielle Regeln für Torhüter" beschreiben.

Die NHL gibt ein eigenes Regelbuch heraus, dessen Inhalte zum Teil von den Vorschriften der IIHF abweichen.

Das Spielfeld ist eine rechteckige Eisfläche von 61 m Länge und 30 m Breite mit abgerundeten Ecken. Es ist von einer ca. 1,20 m hohen Holz- oder Hartplastikbande umgeben, über der aus Sicherheitsgründen – insbesondere gegen fliegende Pucks – Schutzglasscheiben oder Fangnetze angebracht sind, und reicht um die Tore herum. Gegliedert wird es durch fünf Querlinien:


Die Tore haben eine Höhe von 1,22 m und eine Breite von 1,83 m: Pfosten und Latte sind rot gefärbt. Vor dem Tor ist ein halbkreisförmiger Torraum markiert. Insgesamt gibt es neun Anspiel- oder Bullypunkte: neben dem in der Mitte des Feldes jeweils zwei in den Verteidigungszonen. Gegenüber den Boxen der Spielerbänke gibt es weitere für Zeitnehmung und Strafbänke.

In der nordamerikanischen NHL weicht das Spielfeld von den internationalen Vorgaben ab. Es ist mit 200 Fuß zwar fast gleich lang (60,96 m), aber nur 85 Fuß (25,91 m) breit und verfügt über eine andere Verteilung der Spielfeldzonen. Dadurch wird das Spiel in der Regel schneller und aggressiver.

Die Deutsche Eishockey Liga (DEL) schreibt als maximale Abmessungen 61 m Länge und 30 m Breite und als minimale Abmessungen 56 m Länge und 26 m Breite vor. Die Ecken müssen abgerundet sein, mit einem Radius von 7 m bis 8,5 m. Insofern wäre auch ein Spielfeld mit etwa NHL-Maßen in der DEL zulässig. Jedoch orientieren sich bisher alle DEL-Clubs und Betreiber am internationalen Maß, um auch Länderspiele austragen zu können.

Anmerkung: Shorttrack, die 111,12-m-Kurzbahnvariante des Eisschnelllaufs wird ebenfalls auf einem Eishockeyfeld ausgetragen.

Eine Mannschaft besteht für gewöhnlich aus bis zu 22 Spielern. Während eines Spiels dürfen sich höchstens sechs Spieler gleichzeitig auf dem Eis befinden. In der Regel bestehen diese aus fünf Feldspielern und einem Torwart, in besonderen Situationen wird der Torwart aber auch durch einen weiteren Feldspieler ersetzt.

In der Regel wird in Linien, Reihen oder Blöcken gespielt, das bedeutet, dass Stürmer und Verteidiger möglichst immer mit den gleichen Partnern spielen. Eine Mannschaft sollte drei bis vier Verteidigungsreihen (4 mal 2 Spieler = 8 Spieler) und vier Sturmreihen (4 mal 3 Spieler = 12 Spieler) besitzen. Mit zwei Torhütern erreicht man dann die Anzahl von 22 Spielern pro Team.

Ein Team kann einen Mannschaftskapitän und mindestens einen Assistenzkapitän bestimmen. Zur Erkennung tragen sie auf dem Trikot ein „C“ (Captain) oder „A“ (Alternate-Captain).

Das Auswechseln von Feldspielern ist nicht nur während Spielunterbrechungen möglich, sondern kann auch „fliegend“, also während des laufenden Spiels, erfolgen.

Um Verletzungen vorzubeugen, ist eine spezielle Schutzausrüstung vorgeschrieben. Da die Torhüter durch die auf das Tor abgefeuerten Schüsse einer erhöhten Gefahr ausgesetzt sind, haben sie eine noch umfassendere Schutzausrüstung.

Die Schiedsrichter zählen zu den Offiziellen. Diese sind unterteilt in "On-Ice-Offizielle" und "Off-Ice-Offizielle". Die On-Ice-Offiziellen bestehen aus einem Hauptschiedsrichter ("Referee") und zwei Linienrichtern ("Linesmen"). In unteren Spielklassen wird das System mit zwei Schiedsrichtern ohne Linienrichter verwendet. In höheren Spielklassen kommt manchmal auch das System mit zwei Schiedsrichtern und zwei Linienrichtern zur Anwendung, beispielsweise in der NHL, internationaler Ebene oder teilweise auch in den Play-off-Spielen bedeutender europäischer Ligen. Der Schiedsrichter hat die allgemeine Aufsicht über das Spiel sowie die Kontrolle über Spieler und Offizielle. Die Linienrichter haben die Kontrolle über Linienverstöße (Abseits und Icing) und übernehmen die Bullys. Der oder die Hauptschiedsrichter übernimmt bzw. übernehmen das Bully nur zu Drittelbeginn oder nach erzielten Toren. Schieds- und Linienrichter haben beim Eishockey einen erheblich größeren Einfluss als beispielsweise beim Fußball. Sie können durch das Aussprechen von Strafzeiten unmittelbar auf das Spielgeschehen einwirken.

Zu den "Off-Ice-Offiziellen" zählen neben dem Stadionsprecher und dem Spielzeitnehmer ein Punktrichter, ein Video-Torrichter, zwei Strafbankbetreuer sowie zwei Torrichter. In der DEL wird kein Torrichter mehr eingesetzt; der Stand-By-Schiedsrichter übernimmt die Aufgabe des Video-Torrichters.

Weiter wird zwischen "Spiel-Offiziellen" und "Team-Offiziellen" unterschieden. Zu den Spiel-Offiziellen zählen neben den Schieds- und Linienrichtern auch der Video-Torrichter, die Torrichter, die Punkterichter und der Stand-By-Schiedsrichter. Zu den Team-Offiziellen gehört der Trainerstab der Mannschaften, die Betreuer sowie der Ordnerdienst.

Ein Eishockeyspiel dauert netto 60 Minuten (drei Drittel mit je 20 Minuten effektiver Spielzeit, dazwischen in den meisten Ligen jeweils 15 Minuten Pausenzeit). Da bei jeder Spielunterbrechung die Uhr angehalten wird, dauert ein Eishockeyspiel jedoch brutto meist erheblich länger, etwa zwei bis zweieinhalb Stunden.

Beim Eishockey gibt es formell keine Eigentore, der Treffer wird dem Spieler der angreifenden Mannschaft zugeschrieben, der den Puck zuletzt berührt hat.

Ein Tor gilt nur, wenn der Puck die Torlinie in vollem Durchmesser überquert und der Schiedsrichter den Puck hinter der Torlinie gesehen hat. Kann nicht mit Sicherheit eine Aussage darüber getroffen werden, ob der Puck tatsächlich die Torlinie in vollem Durchmesser überschritten hat, so ist auf „kein Tor“ zu entscheiden. Sofern das Spiel aufgezeichnet wird, darf der Schiedsrichter den Videobeweis zu Hilfe nehmen.

Es ist zulässig, den Puck mit Hilfe des Schlittschuhs fortzubewegen, sofern man dadurch kein Tor erzielt. Wird der Spieler am Schlittschuh getroffen, ohne dass eine aktive Kickbewegung zu sehen ist, und überquert der Puck in Konsequenz die Torlinie, ist auf reguläres Tor zu entscheiden.

Den Puck mit dem hohen Stock (Stock über Schulter oder über der Latte) zu spielen ist unzulässig, erlaubt ist es aber, den Puck mit der Hand zu stoppen oder wegzuschlagen. Außerdem darf man innerhalb des eigenen Verteidigungsdrittels den Puck mit der Hand seinem Mitspieler zupassen. Handpässe in der neutralen Zone und im Angriffsdrittel sind verboten. Das Werfen des Pucks, wenn dieser in der Hand eingeschlossen ist, wird bestraft.

Wird ein Torhüter durch einen Schuss an der Maske getroffen, wird das Spiel unterbrochen. Kommt es jedoch nach einem Maskentreffer zu einem direkten Nachschuss, welcher zum Tor führt, bevor der Schiedsrichter das Spiel unterbricht, ist das ein regulärer Treffer.

Steht bei einem Entscheidungsspiel nach regulärer Spielzeit kein Sieger fest, so gibt es eine Verlängerung, genannt Overtime. Endet auch diese unentschieden, wird mit einem „Shootout“ per Penalty-Schießen ein Sieger ermittelt.


Strafen werden vom Schiedsrichter ausgesprochen. Die häufigsten Gründe dafür sind "Behinderung" (interference), "Beinstellen" (tripping), "Hoher Stock" (high-sticking), "Haken" (hooking), "Spielverzögerung" (delaying the game), "Stockschlag" (slashing), "Bandencheck" (boarding), "Stock-Check" (Cross-Check), "Check gegen den Kopf" (checking to the head), "Check von hinten" (checking from behind), "Ellbogencheck" (elbowing), "Unsportliches Verhalten" (unsportsmanlike conduct), "Übertriebene Härte" (roughing), "Unerlaubter Körperangriff" (charging), "Halten" (Holding), "Unkorrekte Ausrüstung" (illegal equipment), "Unkorrekter Spielerwechsel" (too many men), "Halten des Stockes" (holding the stick), "Stockstich" (spearing), "Kniecheck" (kneeing) und "Check gegen das Knie" (checking to the knee).

Um auf Verletzungen zu reagieren, hat der Weltverband IIHF neue Strafen eingeführt, die helfen sollen, schwere Verletzungen zu vermeiden. Dazu zählen Check gegen den Kopf- und Nackenbereich oder Check von hinten. Beide Fouls ziehen mehr als eine normale 2-Minuten-Strafe nach sich und werden zusätzlich mit einer persönlichen Disziplinarstrafe verhängt (2+10 Minuten).

Um die Zuschauer zu schützen und das Spiel flüssiger zu gestalten, wurde eine neue, bei den Spielern und Mannschaften zuerst umstrittene, Regel eingeführt: Verlässt der Puck in direkter Folge eines Schusses aus der Verteidigungszone des schießenden Spielers das Spielfeld über die Scheibe, wird eine kleine Strafe gegen den verfehlenden Spieler wegen Spielverzögerung ausgesprochen. Schießt man den Puck jedoch auf die Spielerbank, so wird, seit der Regeländerung 2007, keine Strafe mehr ausgesprochen.

Die Höhe der Strafe liegt, innerhalb eines gewissen Rahmens, im Ermessen des Schiedsrichters. Sieht der Schiedsrichter ein Foul, zeigt er das durch Heben des Arms an; das Spiel läuft jedoch so lange weiter, bis die zu bestrafende Mannschaft in Puck-Besitz gelangt. In dieser Zeit kann der Torhüter der nicht bestraften Mannschaft das Tor verlassen, um einen weiteren Mann auf das Spielfeld zu schicken. Erzielt die gefoulte Mannschaft in dieser Phase ein Tor, ist die Strafe hinfällig.

Mögliche Strafen und Strafzeiten:

Eine Spieldauer-Disziplinarstrafe oder Matchstrafe hat den sofortigen Ausschluss des Spielers für den Rest des Spiels zur Folge. Im Spielbericht werden aber nur 20 Minuten (Spieldauer-Disziplinarstrafe), respektive 25 Minuten (Matchstrafe) eingetragen.
Die zweite Spieldauer-Disziplinarstrafe im gleichen Spiel oder innerhalb des gleichen Wettbewerbs zieht eine automatische Sperre von einem Spiel nach sich. Die zuständige Disziplinarstelle kann den Spieler zudem für weitere Spiele sperren. Eine Matchstrafe bedeutet, dass der Spieler automatisch „bis auf Weiteres“ gesperrt ist, das heißt für mindestens ein weiteres Spiel. Der Fall wird von der zuständigen Disziplinarstelle beurteilt.

Gelingt der in Überzahl spielenden Mannschaft ein Tor, darf der Spieler, sofern er eine kleine Strafe verbüßt, sofort wieder auf das Eis. Auf große Strafen oder Disziplinarstrafen hat ein Tor keinen Einfluss. Sitzen zwei Spieler auf der Strafbank, darf derjenige, dessen Strafe die geringere Restzeit aufweist, wieder auf das Eis. Eine Ausnahme besteht hier seit der Saison 2006/07 in der DEL. Bekommt jeweils ein Spieler beider Mannschaften gleichzeitig eine 2-Minuten-Strafe, egalisieren sich die Strafen. Bekommt nun ein weiterer Spieler einer Mannschaft später innerhalb dieser zwei Minuten eine Strafe, wird die später aufgestellte Strafe aufgehoben, sobald ein Tor fällt.

Die Zeitstrafen werden auf der Strafbank abgesessen. Der Torhüter wird bei kleinen Strafen oder der ersten Disziplinarstrafe durch einen Feldspieler vertreten, der zur Zeit des Vergehens auf dem Eis war. Größere Strafen können je nach Liga oder Wettbewerb Spielsperren nach sich ziehen.

Wird gegen beide Mannschaften gleichzeitig eine gleiche Anzahl von Strafen ausgesprochen, so müssen die betreffenden Spieler zwar die Strafe auf der Strafbank absitzen, die beiden Mannschaften bleiben aber im Feld in der gleichen Spielstärke wie vor den Vergehen (Mit der Ausnahme von je einer kleinen Strafe gegen beide Teams bei Vollbestand (5 gegen 5). In diesem Fall wird 4 gegen 4 gespielt und beide Strafen laufen auf der Uhr). Eine Mannschaft kann durch Strafen nie auf weniger als drei Feldspieler reduziert werden. Im Falle der dritten Strafe, die zu einem Mann weniger auf dem Eis führen würde, muss der betreffende Spieler zwar auf die Strafbank, er wird aber durch einen Mannschaftskollegen auf dem Eis ersetzt und die Strafzeit beginnt erst zu laufen, nachdem eine Strafzeit eines vorher bestraften Spielers abgelaufen ist (aufgeschobene Strafe). In den Altersklassen der Kleinst- wie der Kleinschüler, auch Bambini und Kleinschüler genannt, wird in Deutschland von diesen Regeln teilweise abgewichen.

Wird ein Spieler, während sich der Torhüter der verteidigenden Mannschaft nicht auf dem Spielfeld befindet, in einer Art und Weise gefoult, die sonst zu einem Strafschuss (Penalty) führen würde, so wird dem gefoulten Spieler ein technisches Tor zugeschrieben.

Das taktische Denken im Eishockey begann erst in den 1950er Jahren. Vordenker auf diesem Gebiet waren die Osteuropäer und Sowjets, die langsam ein Spielsystem etablierten. In der Zeit des Kalten Krieges konnte man strikt zwischen sowjetischem Eishockey, das von Kurzpassspiel bis zur Torchance geprägt war, und einer kanadisch-nordamerikanischen Spielweise mit mehr Körpereinsatz trennen. So strikt sind die Unterschiede heute nicht mehr zu erkennen. Vor der Entwicklung einer Taktik gab es noch keine richtige Trennung der Positionen. Jeder konnte so spielen, wie er wollte. Mit guter Taktik gelang es technisch schwächeren Mannschaften schon oft, gegen talentiertere Teams zu gewinnen.

So gibt es diverse Möglichkeiten, einen Angriff aufzuziehen. Eine beliebte Variante, vor allem in Nordamerika, ist das "Dump’n’Chase"-Spiel, bei dem der Spielzug mit einem weiten Pass an die Hintertorbande eröffnet wird. Technisch versierte Spieler können durch einen Lauf durch die neutrale Zone zur Torchance gelangen. Daneben gibt es noch verschiedene Passwege, über die der Center einen Außenstürmer erreichen kann. Des Weiteren ist der Konter oder das Break eine beliebte Alternative.

Im Abwehrverhalten unterscheidet man fünf Varianten:
Auch dieses Verhalten ist nicht starr, und man kann diese Abwehrtechniken miteinander kombinieren.

Eishockey ist vor allem dort sehr verbreitet, wo auch vor Erfindung der Kältemaschine – und der damit verbundenen Möglichkeit, Kunsteisbahnen zu schaffen – genügend Eisflächen für eine regelmäßige Ausübung dieser Sportart vorhanden waren und sind. Ein Betrieb solcher Kunsteisbahnen ist meistens sehr teuer. Allerdings werden mittlerweile kostengünstigere Alternativen aus synthetischem Eis angeboten, die einer herkömmlichen Eisbahn sehr nahekommen. Somit findet der Eissport nun auch in wärmeren Regionen wie Südafrika eine größere Verbreitung.

Weit verbreitet ist Eishockey vor allem in Kanada, Russland (der ehemaligen Sowjetunion) und den USA, Tschechien und der Slowakei sowie den nordischen Ländern (vor allem Schweden und Finnland), teilweise auch in Deutschland. Einen sehr hohen Stellenwert hat Eishockey auch in der Schweiz, Lettland und in Österreich und gehört dort zu den beliebtesten Sportarten. Die Gesamtsumme der Eishockey-Zuschauer in einer Saison liegt in diesen Ländern weit über derjenigen der Fußball-Zuschauer. Ein traditionsreiches internationales Highlight für Clubmannschaften stellt auch der seit 1923 ausgetragene Spengler Cup dar, der in Davos, im Kanton Graubünden, stattfindet und eine wesentlich längere Geschichte als der Europapokal und dessen Folgewettbewerbe aufweisen kann.


Ende des 19. Jahrhunderts breitete sich der in Kanada entstandene Eishockeysport allmählich in ganz Europa aus, wobei die Entwicklung in den nordischen Ländern aufgrund der besseren Bedingungen für das damals noch unter freiem Himmel ausgetragenen Eishockey günstiger waren.

Die Meisterschaft wird heute in fast allen Ligen durch eine Play-off-Runde ermittelt, für das sich die besten Mannschaften des Grunddurchganges qualifizieren. In der ersten Runde trifft die nach dem Grunddurchgang am besten platzierte Mannschaft gegen die am schlechtesten platzierte, die zweitbeste auf die zweitschlechteste usw. – die Gewinner spielen in der nächsten Runde weiter, bis die zwei verbliebenen Mannschaften das Finale austragen. Die Begegnungen werden in der Regel als Serie von Spielen ausgetragen, wobei die Teilnahme an der nächsten Runde durch den Best-of-Modus geregelt ist.

Die wirtschaftliche Bedeutung des Eishockeys ist in Europa zwar noch nicht so groß wie in Nordamerika, das Marketing erlangte jedoch in den letzten Jahren auch in den Top-Ligen Europas eine immer größere Bedeutung. In den 1960er und 1970er Jahren brauchte man eine gute Jugendarbeit, um erfolgreich zu sein, heute ist eine professionelle Struktur nötig. So betrieben die Hamburg Freezers zum Beispiel bis 2005 keine Nachwuchsarbeit, weil sie mit der Anschutz-Gruppe einen finanzkräftigen Investor haben, der seine Mannschaften in Europa nach dem nordamerikanischen Franchise-System betreibt.

In Deutschland war zunächst Berlin Ursprungsort des Eishockeys, später folgten vor allem die süddeutschen Gebiete in Bayern und Teilen Baden-Württembergs als „Hochburgen“ der neuen Sportart. Nach den beiden Weltkriegen erlebte der Sport einen erneuten Aufschwung, auch bedingt durch die Gründung der eingleisigen Eishockey-Oberliga als höchste deutsche Spielklasse im Jahr 1948, die schließlich 1958 durch die Eishockey-Bundesliga abgelöst wurde. Gab es in den Anfangsjahren eine deutliche Dominanz der bayerischen Vereine, gewannen in späteren Jahren auch Clubs aus Nordrhein-Westfalen an Bedeutung. Heute gilt Eishockey in weiten Teilen des Landes als eine der wichtigsten Mannschaftssportarten.

Die höchste Spielklasse in Deutschland ist seit 1994 die Deutsche Eishockey Liga (DEL), die den Beinamen „1. Bundesliga“ trägt. Im Gegensatz zu anderen Sportarten gibt es in der Deutschen Eishockey Liga kein Unentschieden, um rein taktische Spiele zu vermeiden. Bis zur Saison 2005/06 wurde ein Vorrundenspiel, bei welchem es nach regulärer Spielzeit unentschieden stand, mit einem Penalty-Schießen entschieden. Zwischen der Spielzeit 2006/07 und Spielzeit 2016/17 wurden zunächst fünf Minuten 4 gegen 4 gespielt. Seit der Spielzeit 2016/17 wird die Verlängerung mit 3 gegen 3 gespielt. In den Play-Offs wird eine Verlängerung mit der kompletten Spieleranzahl gespielt, die sofort endet, wenn eine der beiden Mannschaften ein Tor erzielt. Es gibt kein Penaltyschießen, sondern immer wieder Verlängerungen von jeweils 20 Minuten, bis eine Mannschaft ein Tor erzielt. Bei einem Sieg innerhalb der regulären Spielzeit erhält der Gewinner drei Punkte, der Verlierer null, bei einem Sieg durch Verlängerung oder Penalty-Schießen wird der Sieger mit zwei Punkten, der Verlierer mit einem Punkt belohnt.

In der Zeit der österreichisch-ungarischen Monarchie entwickelte sich der Eishockeysport vor allem in Wien, nach dem Ersten Weltkrieg und dem Zerfall der Monarchie gewann das österreichische Eishockey durch internationale Erfolge an Bedeutung. Nach dem Anschluss an das Deutsche Reich wurde der nationale Verband aufgelöst, im Zweiten Weltkrieg kam der Spielbetrieb schließlich zum Erliegen. Erst 1945 wurde der Verband neu gegründet, 1947 konnte die Nationalmannschaft erneut die Bronzemedaille bei einer Weltmeisterschaft erobern.

Die höchste Spielklasse des österreichischen Eishockeys existiert in ihrer aktuellen Form seit der Saison 1965/66. Der österreichische Meister wird jedoch mit Unterbrechungen seit dem Jahr 1923 ausgespielt. Rekordmeister und gleichzeitig der älteste Verein im aktuellen Teilnehmerfeld ist der EC KAC aus Klagenfurt mit 30 Meistertiteln. Die heute den Namen "Erste Bank Eishockey Liga" tragende Spielklasse hat sich im Lauf der letzten Jahre hin zu einer internationalen Liga entwickelt, wobei vier von zwölf teilnehmenden Mannschaften aus dem benachbarten Ausland stammen. Die Regeln sind nahezu identisch zu denen im restlichen Mitteleuropa, wobei der Gewinner bei einem Sieg innerhalb der regulären Spielzeit drei Punkte erhält, bei einem Unentschieden erhalten beide Mannschaften einen Punkt, der Sieger nach Verlängerung oder Penaltyschießen einen Zusatzpunkt.

In der Schweiz verlief die Entwicklung des Eishockeys in Analogie zu den Nachbarländern, der nationale Verband wurde im Jahr 1908 gegründet. Die National League A (früher Nationalliga A) ist heute höchste Eishockey-Liga in der Schweiz. Sie besteht im Moment aus zwölf Mannschaften. Gespielt wird die Meisterschaft in einer ersten Phase (Qualifikation) als Rundenturnier. Danach ermitteln die besten acht Mannschaften den Schweizer Meister im Play-off-Stil, wobei der 1. Platzierte gegen den 8. Platzierten spielt, der 2. Platzierte gegen den 7. Platzierten usw. Der Name „National League“ wurde in der Saison 2007/08 zusammen mit vielen zusätzlichen Neuerungen eingeführt. Zu den Änderungen gehören unter anderem ein neuer Pokal und die Erweiterung der Qualifikation auf 50 Runden: Jedes Team spielt viermal gegen jedes andere Team plus zusätzliche sechs Gruppenspiele. Dazu werden die zwölf Clubs in drei Vierergruppen eingeteilt, deren Zusammensetzung aufgrund der geografischen Lage erfolgt. In jeder Vierergruppe gibt es eine Hin- und Rückrunde, wobei diese Ergebnisse in die Gesamtrangliste der Qualifikation, die also 50 Spiele zählt, übernommen werden.

Mit der Verbreitung des Eishockeys in Europa fanden vor allem die nordischen Länder in diesem Spiel eine neue Nationalsportart, da unter anderem die Bedingungen in diesen Ländern für das damals noch unter freiem Himmel ausgetragenen Eishockey optimal waren. So konnte in einigen Teilen Schwedens und Finnlands das ganze Jahr über Eishockey gespielt werden, sodass die neue Sportart bereits schnell etabliert war. Vor allem die Nationalmannschaften dieser beiden skandinavischen Staaten entwickelten sich in kurzer Zeit zu internationalen Größen, die beiden höchsten Spielklassen Svenska Hockeyligan und SM-liiga gelten heute als zwei der bedeutendsten Spielklassen der Welt.

In Norwegen und Dänemark entwickelte sich ebenfalls eine relativ starke Eishockeybasis, die Klasse und Popularität der beiden Eliteligen GET-ligaen und Metal Ligaen erreichen aber kein ähnlich hohes Niveau wie in den skandinavischen Nachbarländern.

Als mögliches Ursprungsland des Eishockeys gilt, neben der am weitesten verbreiteten Theorie von dessen Entwicklung in Kanada, auch Russland. Allerdings kam es in der damaligen Sowjetunion erst in den 1940er Jahren zu einer wirklichen Entwicklung und Verbreitung des Eishockeys. 1947 wurde die erste Meisterschaft der UdSSR ausgetragen, 1952 trat die Sowjetunion der Internationalen Eishockey-Föderation (IIHF) bei. Wurde in der Sowjetunion bis in die 1950er Jahre auf dem Eis vor allem Bandy gespielt, entwickelte sich das sowjetische Eishockey fortan mit einem enormen Tempo.

In der Zeit des Kalten Krieges entwickelte sich eine starke russische Eishockeydominanz mit internationalen Erfolgen in Serie, was auch der unklaren Profi-Situation der russischen Spieler im Gegensatz zu den nordamerikanischen Amateuren bei internationalen Turnieren geschuldet war. Aufeinandertreffen von Ost- und Westmannschaften wie das Miracle on Ice stellten eine sportliche Variante des politischen Konflikts zwischen den beiden Lagern dar. Die nationale sowjetische Liga galt lange als eine der stärksten Spielklassen der Welt, nach dem Zusammenbruch der UdSSR gingen ein Großteil der besten Spieler jedoch in die "National Hockey League", um in den NHL-Mannschaften der USA und Kanadas zu spielen.

Die höchste Spielklasse in Russland ist heute die Kontinentale Hockey-Liga, welche 2008 die Superliga ablöste und sich durch ihre Öffnung für Teams aus ganz Europa und Nordasien (es nehmen Mannschaften aus verschiedenen ehemaligen SU-Staaten, aber auch aus Tschechien, Kroatien und der Slowakei am Spielbetrieb teil) zum Gegenpol zur nordamerikanischen NHL entwickeln soll.

Auf dem Gebiet der ehemaligen Tschechoslowakei entwickelte sich der Eishockeysport ab etwa 1920 sehr stark und viele Mannschaften des sogenannten „"Kanadischen Hockeys"“ wurden in dieser Zeit gegründet. Seit 1929 wird in Poprad der Tatra Cup ausgetragen, der damit das zweitälteste europäische Eishockeyturnier darstellt. Zu wichtigen Zentren des Eishockeys in der Tschechoslowakei entwickelten sich neben der Hauptstadt Prag vor allem Großstädte wie Bratislava, Budweis oder Pilsen sowie die Ballunsgräume entlang der Hohen Tatra, was auch Einfluss auf die Entwicklung des Sports im Nachbarland Polen hatte, wo bis heute die Mehrheit der professionellen Eishockeyteams in den Ballungsräumen südpolnischer Großstädte wie Kattowitz, Krakau, Tichau oder Oppeln sowie entlang der Beskiden anzutreffen sind.

Nach dem Zweiten Weltkrieg gewann Eishockey in der Tschechoslowakei zusätzlich an Popularität und wurde zur Sportart Nummer eins im Land. Das tschechoslowakische Nationalteam wurde mehrfach Weltmeister und gehörte zu den dominierenden Nationalteams der 1960er und 1970er Jahre, während der Armeeklub Dukla Jihlava fünfmal den Spengler Cup gewann und insgesamt achtmal das Finale des Europapokals erreichte. Nach der Auflösung der Tschechoslowakei 1992 entstanden in Tschechien und der Slowakei separate Verbände mit eigenen höchsten Spielklassen (siehe Extraliga). Während das tschechische Nationalteam das Startrecht der Tschechoslowakei übernahm und Ende der 1990er Jahre mehrfach die Weltmeisterschaft gewann, musste sich das slowakische Nationalteam erst aus der dritten Division an die Weltspitze zurückkämpfen, um 2002 selbst Weltmeister zu werden.

Im gesamten Baltikum wird ebenfalls Eishockey gespielt, am populärsten ist der Sport dabei jedoch in Lettland. In den übrigen Ländern Mittel- und Osteuropas besitzt der Eishockeysport innerhalb der Mannschaftssportarten einen ähnlich hohen Stellenwert, ohne dass die jeweiligen Spielklassen in Polen (Ekstraliga), Weißrussland (Extraliga), Bulgarien (A-Gruppe), Rumänien (Nationale Eishockeyliga), Serbien (Serbische Eishockeyliga), Slowenien (Prva Liga) oder Ungarn (OB I. Bajnokság) die Qualität und Zuschauerzahlen der Ligen in Tschechien oder der Slowakei erreichen.

Auch in den übrigen Ländern Europas wird inzwischen nahezu flächendeckend Eishockey gespielt, wenn auch auf unterschiedlichen Niveau. Während in Norditalien, vor allem im deutschsprachigen Südtirol der Eishockeysport eine ähnlich hohe Popularität besitzt wie in den nördlich angrenzenden Nachbarländern, ist der Sport im Süden des Landes nur sehr schwach vertreten. Die nationale Liga "Serie A" wurde bereits 1924 gegründet und gehört damit zu den ältesten Eishockeyspielklassen Europas. In Frankreich gehört Eishockey eher zu den Randsportarten, wobei wichtige Pokalspiele wie das in Paris stattfindende Endspiel um die Coupe de France von teilweise mehr als 12.000 Zuschauern besucht werden.

Im Vereinigten Königreich und Irland wird ebenfalls Eishockey gespielt, allerdings lediglich als Randsportart, was sich vor allem in Zuschauerzahlen und Qualität bei internationalen Vergleichen abzeichnet, wobei es im Vereinigten Königreich teilweise parallel mehrere Profiligen gab und gibt. Die höchsten Spielklassen sind die Elite Ice Hockey League in Großbritannien und die Irish Ice Hockey League in Irland. Auch in anderen, für den Wintersport eher untypischen Regionen, wie Spanien (Superliga), Türkei (Superliga) oder Griechenland, entwickelte sich im Laufe der Jahre regelmäßiger Ligenbetrieb.

Im kanadischen und amerikanischen Englisch wird Eishockey generell als "Hockey" bezeichnet. Schwerpunkt des nordamerikanischen Eishockeys ist Kanada, wo der Sport Nationalsportart ist und die nördlichen Gebiete der USA. In den USA steht Hockey wegen seiner nur regionalen Stärke hinter Baseball, American Football und Basketball, zusammen mit diesen Sportarten bildet es aber "die großen Vier des US-Sports".

Die 1917 gegründete "National Hockey League" (NHL) gilt als beste Liga der Welt und der Stanley Cup als die begehrteste Eishockey-Trophäe.

Seit den 1940ern wurde aber auch der Ligenunterbau der NHL stetig professionalisiert und ausgebaut. So gibt es heute mit der American Hockey League (AHL) eine Elite-Minor League, in der die Top-Farmteams der NHL-Franchises spielen. Darunter gibt es noch die ECHL (früher East Coast Hockey League), Central Hockey League (CHL), West Coast Hockey League (WCHL) und International Hockey League (IHL). Des Weiteren gibt es professionell arbeitende Nachwuchsligen in Kanada: die Western Hockey League (WHL), Ontario Hockey League (OHL) und Ligue de hockey junior majeur du Québec (LHJMQ). Die "Minor Leagues" übernehmen unter anderem die Jugendarbeit für die NHL-Franchises.

In den 1990er Jahren ist die wirtschaftliche Bedeutung des Hockeysports in Nordamerika radikal angestiegen. Die NHL-Franchises zahlten übertrieben hohe Gehälter, so dass einige noch heute hohe Schulden haben. Das zeigte sich auch in der Verhandlung über ein neues NHL Collective Bargaining Agreement zur Saison 2004/05, die in einem Spielerstreik und Lockout endete, so dass die Spielzeit abgesagt wurde. Nach über einem Jahr Verhandlungen wurde eine Gehaltsobergrenze (Salary Cap) beschlossen, die die Liga ausgeglichener und spannender machen soll.

Eine Besonderheit im nordamerikanischen Profisport ist der Entry Draft, der auch im Eishockey durchgeführt wird (NHL Entry Draft). Beim Draft sichern sich die NHL-Teams die Rechte an talentierten Juniorenspielern. Ein Kuriosum dabei: Der von vielen als bester Spieler aller Zeiten betrachtete Wayne Gretzky ist einer der wenigen seiner Generation, die nicht gedraftet wurden. Die meisten „Draft picks“ spielen erst in der AHL oder einer Juniorenliga, bis sie von ihrem NHL-Team „befördert“ werden.

Eishockey ist in Lateinamerika eine absolute Randsportart. Einen geregelten Spielbetrieb gibt es lediglich in Mexiko, dessen Nationalmannschaft derzeit auch an offiziellen Turnieren des internationalen Eishockeyverbandes IIHF teilnimmt. Des Weiteren gibt es Eishockeyverbände in Argentinien, Brasilien, Chile und Ecuador, die sich jedoch meist nur dem Inlinehockey widmen und den Begriff „Eishockey“ nur der Form nach im Namen tragen. Trotzdem gibt es einige Hobbyspieler im Süden von Argentinien und Chile, wo im Winter der südlichen Hemisphäre (ca. Mai bis August) gelegentlich auf zugefrorenen Flüssen oder Seen gespielt werden kann. Das bleibt jedoch die Ausnahme – einerseits, da die zum Eishockey notwendige Ausrüstung relativ teuer (und in Südamerika schwer zu beschaffen) ist, andererseits, weil diese Sportart in den betreffenden Ländern relativ unbekannt ist.

Das erste Profieishockeyspiel auf lateinamerikanischem Boden fand am 23. September 2006 in Puerto Rico statt: Zur Saisoneröffnung der nordamerikanischen Profiliga NHL trafen sich die Teams der Florida Panthers und der New York Rangers im José Miguel Agrelot Coliseum in San Juan.

Auch in anderen Teilen der Welt fand das Eishockey allmählich seine Verbreitung. Die Asia League Ice Hockey gilt als spielstärkste Eishockeyliga außerhalb Nordamerikas und Europas und beheimatet Teams aus Japan, China und Südkorea. In Australien (Australian Ice Hockey League) und Neuseeland (New Zealand Ice Hockey League) sowie in Südafrika wird zum Teil ebenfalls seit vielen Jahrzehnten Eishockey gespielt, wenn auch in Form einer absoluten Randsportart. Nichtsdestoweniger spielten bereits mehrere ehemalige NHL-Profis zeitweise in Australien.

Über den regulären Ligenbetrieb hinaus nehmen einige afrikanische und asiatische Länder am offiziellen Spielbetrieb der IIHF teil. So sind dies in Afrika Algerien und Marokko, sowie in Asien unter anderem die Vereinigten Arabischen Emirate, Kuwait, Macao oder Thailand.

Für die Eishockeynationalmannschaften existieren verschiedene bedeutende internationale Turniere. Der offiziellen Rangfolge der Internationalen Eishockey-Föderation ("International Ice Hockey Federation – IIHF") entsprechend ist hierbei das Olympische Eishockey-Turnier, welches seit 1924 bei den Winterspielen stattfindet, das bedeutendste Turnier für Nationalmannschaften. Darüber hinaus finden seit 1920 offizielle Weltmeisterschaften der "IIHF" statt, welche vor allem für die europäischen Teams eine große Bedeutung besitzen.

Da die nordamerikanische "National Hockey League" für die Weltmeisterschaften der Herren ihre Saison normalerweise nicht unterbricht und auch für die Olympischen Spiele bislang nur 1998, 2002, 2006, 2010 und 2014 eine Pause eingelegt hat, ist das Ansehen dieser Turniere in Kanada und den USA relativ gering. Vor allem den Weltmeisterschaften wird dabei vorgehalten, dass bei ihnen nicht die besten Spieler der Welt spielen würden und sie deshalb keinen echten Weltmeister küren könnten. Hinzu kommt, dass bis 1976 nur Amateure an den Weltmeisterschaften teilnehmen durften, so dass beispielsweise Kanada lange Zeit ihren Amateurmeister zur Weltmeisterschaft schickte. Aus diesem Grund wurde der Meister der kanadischen und nordamerikanischen Profiliga lange Zeit als „World Champion“ tituliert.

Neben dem Olympischen Eishockey-Turnier ist deshalb heute der World Cup of Hockey zum wichtigsten Turnier im Eishockey geworden. Er wird von der "NHL" in Zusammenarbeit mit der IIHF ausgerichtet und fand bislang 1996, 2004 sowie 2016 statt. Da das Turnier vor Beginn der NHL-Saison stattfindet, können hier die besten Spieler aller Nationen teilnehmen, wodurch die Bedeutung vor allem in Nordamerika deutlich erhöht wird.

Das wohl bekannteste und traditionsreichste Vereinsturnier ist der Spengler Cup, der schon seit 1923 jährlich in Davos in der Schweiz ausgetragen wird. Rekordsieger ist der Gastgeber HC Davos. Der zweitälteste Pokalwettbewerb Europas, der Tatranský pohár, wird seit 1929 im slowakischen Poprad ausgetragen.

Ein weiterer in Europa wichtiger Vereinswettbewerb war der IIHF European Champions Cup. Die Landesmeister der nach IIHF-Weltrangliste sechs besten europäischen Eishockeyverbände trafen dabei zwischen 2005 und 2008 jährlich aufeinander. Der erste Cup fand im Januar 2005 im russischen Sankt Petersburg statt. Sieger wurde der russische Meister HK Awangard Omsk. Der European Champions Cup war Nachfolger der European Hockey League, des wichtigsten Vereins-Wettbewerbs Europas von 1996 bis 2000. Diese war wiederum Nachfolger des Europapokals, der von 1965 bis 1996 über 30 Jahre lang jährlich stattfand. Die Initiierung eines neuen kontinentalen Wettbewerbs, der Champions Hockey League, ist der in der Saison 2008/09 erstmals von der IIHF ausgetragene Europapokal-Wettbewerb für Eishockeyklubs.

Verschiedene Spielerinnen und Spieler haben weltweite Bekanntheit erlangt und sind in die internationale oder eine nationale Hockey Hall of Fame aufgenommen worden. Berühmte Spieler der NHL waren Wayne Gretzky ("„The Great One“"), Mario Lemieux ("„Super Mario“"), Bobby Orr, Gordie Howe, Bobby Hull, sein Sohn Brett Hull, Mark Messier, Patrick Roy, Joe Sakic und Steve Yzerman (alle Kanada); sowie Mike Modano (USA), Jari Kurri (Finnland), Nicklas Lidström und Peter Forsberg (Schweden) sowie Pawel Bure (Russland). Außergewöhnliche sowjetische Spieler waren unter anderem Boris Michailow, Wladimir Petrow, Waleri Charlamow, Wladislaw Tretjak, Wladimir Krutow, Igor Larionow, Sergei Makarow (letztere drei bildeten zusammen die berühmte KLM-Reihe) sowie Wjatscheslaw Fetissow und Alexei Kassatonow.

Unter den Aktiven genießen Spieler wie die Kanadier Sidney Crosby und Steven Stamkos, der Tscheche Jaromír Jágr und die Russen Alexander Owetschkin, Jewgeni Malkin und Pavel Dazjuk große Popularität.

Bei den Frauen haben vor allem die Kanadierinnen Manon Rhéaume und Hayley Wickenheiser weltweiten Ruhm erfahren.

In Deutschland wurde Erich Kühnhackl zum „Eishockeyspieler des Jahrhunderts“ gewählt, bekannt sind auch der ehemalige Bundestrainer Hans Zach sowie Gerd Truntschka, Dieter „Didi“ Hegen und Udo Kießling, welcher auch deutscher Rekordnationalspieler ist.

Mit Uwe Krupp, der als erster Deutscher den Stanley Cup gewonnen hat und zwischen 2005 und 2011 Bundestrainer war, sowie den aktuellen NHL-Profis Dennis Seidenberg, Tobias Rieder, Leon Draisaitl, Thomas Greiss, Tom Kühnhackl, Philipp Grubauer und Korbinian Holzer erfreuen sich auch deutsche Eishockeyspieler internationaler Popularität. Weitere deutsche Eishockeyspieler in der NHL waren Marco Sturm, Christoph Schubert und Jochen Hecht.

Bekannte Spieler aus der Schweiz sind die ehemaligen bzw. aktiven NHL-Torhüter, Martin Gerber, Reto Berra, Jonas Hiller sowie David Aebischer, der in der Saison 2000/01 als erster Schweizer den Stanley Cup gewonnen hat. Mit Mark Streit konnte zu Beginn der Saison 2005/06 erstmals ein Schweizer Feldspieler in der NHL Fuß fassen. Hinzu kommen Spieler wie Luca Sbisa, Raphael Diaz, Roman Josi, Nino Niederreiter und Damien Brunner, die aktuell (Stand 2014) in der NHL aktiv sind. Weitere bekannte Spieler mit Schweizer Hintergrund sind in der Liste der Schweizer Spieler in der NHL ersichtlich.

Ein bekannter österreichischer Eishockeyspieler aus den siebziger Jahren ist der mittlerweile in die Politik gewechselte Franz Voves, der insgesamt 75 Spiele für die Österreichische Eishockeynationalmannschaft bestritt. Heutzutage sind aktive und ehemalige NHL-Spieler wie Thomas Pöck, Thomas Vanek, Michael Grabner, Michael und Thomas Raffl sowie Andreas Nödl sehr bekannt in Österreich.

Zu den bekanntesten Vereinen weltweit zählen in erster Linie die Clubs aus der "National Hockey League". Über die größte Tradition verfügen dabei die „Original Six“ (Boston Bruins, Chicago Blackhawks, Detroit Red Wings, Canadiens de Montréal, New York Rangers, Toronto Maple Leafs) – die Gründungsmitglieder sind, oder in den Anfangsjahren in die Liga aufgenommen wurden. Die Mighty Ducks of Anaheim, welche erst Anfang der neunziger Jahre ihr Debüt in der NHL gaben, haben hingegen zuerst durch den Sportfilm "The Mighty Ducks", dem sie letztendlich ihren Namen zu verdanken haben, einen gewissen Popularitätsschub erhalten. Inzwischen heißt das Team „Anaheim Ducks“.

Neben den nordamerikanischen Eishockeyvereinen zählt auch der mehrfache sowjetische Meister ZSKA Moskau zu den ruhmreichen Eishockeyvereinen. Dieser mit 20 Titeln erfolgreichste Teilnehmer im Europapokal verfügte in den 1970er und 1980er Jahren auch weltweit über eine der spielstärksten Mannschaften. Bei mehreren Spielen gegen verschiedene NHL-Klubs (Super Series), welche allesamt auf nordamerikanischem Boden ausgetragen wurden, hatte der ZSKA zum Abschluss ausnahmslos eine positive Bilanz.

In Deutschland sind vor allem die rheinischen Rivalen Düsseldorfer EG (von 2000 bis 2012 "DEG Metro Stars") und Kölner Haie (ehemals "Kölner EC") bekannt, welche in den 1990er Jahren mehrere spannende Duelle um den Deutschen Meistertitel austrugen. Weiterhin genießen auch Vereine wie die Krefeld Pinguine (Meister 1952, 2003), Frankfurt Lions (Meister 2004), Adler Mannheim (Meister 1980, 1997, 1998, 1999, 2001, 2007 und 2015) und DEL-Rekordmeister Eisbären Berlin (Meister 2005, 2006, 2008, 2009, 2011, 2012 und 2013) große Popularität. Vereine wie der EV Füssen (16-facher deutscher Meister), EC Bad Nauheim, EC Bad Tölz, SB Rosenheim, der EV Landshut, der SC Riessersee (10-facher deutscher Meister) oder der BSC Preussen, genauer gesagt, der Berliner Schlittschuhclub, welcher bis heute Rekordmeister ist, spielen heutzutage sportlich eine eher untergeordnete Rolle, haben vorrangig durch ihre mittlerweile weit zurückliegenden Erfolge jedoch noch eine gewisse Bekanntheit.

In der DDR gewann beim „ewigen Duell“ zwischen dem SC Dynamo Berlin und die SG Dynamo Weißwasser letztere 25 Mal die Meisterschaft und Dynamo Berlin 15 Mal.

Zu den bekanntesten Schweizer Eishockeyclubs zählen der Rekordmeister HC Davos, der SC Bern mit dem seit Jahren höchsten europäischen Besucherzuspruch zwischen 15- und 16-Tausend Zuschauern pro Spiel, die ZSC Lions aus Zürich, die Kloten Flyers und der HC Lugano. Daneben gibt es auch kleinere Klubs, welche in den jeweiligen Regionen einen großen Stellenwert haben, wie zum Beispiel die SCL Tigers aus Langnau, der EV Zug, der EHC Biel, Fribourg-Gottéron oder der HC Ambrì-Piotta.

Die wohl bekanntesten österreichischen Eishockeyclubs sind der Rekordmeister EC KAC sowie der EC VSV, der EHC Liwest Black Wings Linz, der EC Red Bull Salzburg und die Vienna Capitals.
Ein Vorläufer des Eishockeys ist Bandy, eine Eis-Sportart, die in mehrfacher Hinsicht eher dem Feldhockey und dem Fußball ähnelt und besonders in Nord- und Osteuropa und Nordamerika betrieben wird.

Aus dem Eishockeysport hat sich eine Reihe heute eigenständiger Sportarten entwickelt: Zum einen das Sledge-Eishockey, auch Schlitteneishockey genannt, das den Eishockeysport für körperlich beeinträchtigte Sportler ermöglicht, bei welchen die Beweglichkeit der unteren Gliedmaßen eingeschränkt ist, und heute als Sportart im festen Programm der Winter-Paralympics steht. Auch gibt es das Chneblen, bei dem man ohne Ausrüstung und aus Spaß auf dem Eisfeld kleine Turniere macht. Zum anderen existieren gleich mehrere „Sommer-Eishockey“-Sportarten, wie Floorball/Unihockey, Inlinehockey, Inline-Skaterhockey oder Streethockey, von welchen Floorball, Inlinehockey und Streethockey über eigene Weltmeisterschaften verfügen, wobei die beiden letzteren auch von der Internationalen Eishockey-Föderation durchgeführt oder unterstützt werden. Floorball/Unihockey ist insbesondere in Skandinavien, der Schweiz und Tschechien eine sehr populäre Variante, die in normalen Sporthallen ausgetragen wird und die auch als Mixed gespielt werden kann. Streethockey wird immer beliebter bei jungen Menschen, weil man es auch außerhalb und ebenso wie Floorball ohne viel Ausrüstung spielen kann.

Eine weiter verwandte, jedoch in Deutschland noch nicht sehr bekannte Sportart ist Broomball. Die größten Unterschiede im Vergleich zum Eishockey sind, dass man den Sport mit Spezialschuhen auf dem Eis spielt und anstelle von Eishockeyschlägern und Puck einen „Broom“ sowie einen Ball benutzt.

Eine spezielle Variante ist Unterwassereishockey.





</doc>
<doc id="13611" url="https://de.wikipedia.org/wiki?curid=13611" title="Gotland">
Gotland

Gotland ist eine schwedische Insel und historische Provinz. Die nach Seeland (Dänemark) und vor Fünen (Dänemark) sowie Saaremaa (Estland) zweitgrößte Insel der Ostsee liegt nordöstlich von Öland. Ihren Namen hat sie vom Germanenstamm der Goten, die die Insel laut der Gutasaga um die Zeitenwende zumindest teilweise verließen, um auf dem Kontinent, später als Ost- und Westgoten, große Reiche im mediterranen Raum zu errichten. 

Die Insel Gotland bildet zusammen mit einigen benachbarten kleineren Inseln die Provinz Gotlands län, die historische Provinz Gotland sowie die Gemeinde Gotland.

Gotland besteht zu weiten Teilen aus einem Kalkstein­plateau, im Süden liegt ein größerer Bereich auf einer Sandstein­formation. Hauptort der Insel ist die frühere Hansestadt Visby. Entsprechend seinen Ressourcen ist Gotland von der Stein- und Zementindustrie, vom Mergel- und Tonabbau und vom Fischfang geprägt worden. Die Kalkbrennerei (Kalköfen von Bärlast und Kyllaj) trug zur Entwaldung der Insel bei. Im Jahr 1730 wurde die Zahl der Öfen auf 18 halbiert. Heute wird nur noch an wenigen Orten Kalkstein abgebaut. Die Zementherstellung ist auf den Ort Slite konzentriert, wo Schwedens größtes Werk steht. In Bläse befindet sich ein Kalkwerksmuseum (schwed. Bläse kalkbruksmuseum).

Die höchste Erhebung ist unter 82 m hoch. Es gibt auf Gotland insgesamt etwa 50 ständig mit Wasser gefüllte Seen, von denen vier eine Fläche von mehr als einem Quadratkilometer haben: Bäste träsk, Tingstädeträsk, Fardumeträsk und Bogeviken.

Gotland ist bekannt für seine sehr artenreiche Naturlandschaft. Besonders die Vogelwelt und die Vielfalt an Orchideen sind hervorzuheben. Zahlreiche Naturreservate wurden auf der gesamten Insel eingerichtet, eines davon auf der Torsburg, zwei andere auf den Inseln Lilla Karlsö und Stora Karlsö (kleine und große Karlsinsel) oder das Uppstaigreservat. Das Guteschaf (schwed. "Gutefår"), eine kleine robuste Hausschaf­rasse, ist die älteste schwedische Rasse. Bis in die Neuzeit gehörte die winterliche Robbenjagd auf Gotland zur Ernährungsgrundlage der Inselbevölkerung.

Gotland ist eine Insel mit einigen Nebeninseln (u. a. Fårö, Lilla und Stora Karlsö und Östergarnsholm), die nach der Eiszeit aufgrund der Landhebung und unterschiedlicher Meeresstände der Ostsee in mehreren Etappen aus der Ancylussee emporwuchs. Die Insel wurde zunächst von Jägern und Sammlern besiedelt. Die Skelettfunde von Stenkyrka und Lummelunda sind etwa 8000 Jahre alt. Damit sind sie nicht nur die ältesten in Gotland, sondern gehören auch zu den ältesten in Schweden. Um 2000 v. Chr. wurden die Jäger von Ackerbauern verdrängt. Wenig später gelangte bereits die Bronze auf die Insel. Die Eisenzeit begann wie im übrigen Norden um 500 v. Chr. Die Siedlungen dienten offenbar auch als Handelsplätze. An mehreren von ihnen haben Archäologen Feuerstein gefunden, der aus Südschweden stammt; Pfeilspitzen aus Schiefer kamen aus Mittelschweden oder Norrland und Bernstein vom südlichen Ostseeufer. 

Aus der Vorzeit haben sich viele Überreste als Bodendenkmale erhalten. Schiffssetzungen und Bildsteine sind Elemente, deren Ursprung offenbar auf Gotland lag. Radgräber (bei Linde und Stenkyrka), beinahe 400 Runensteine, auch Menhire und Steinkisten sind sehr zahlreich. Steinhügelgräber, die auf Schwedisch Rojr (dt. "Röser") heißen, und Felsritzungen vervollständigen die Relikte der Vorzeit. Das Bulverket, ein Pfahlbau im Tingstädeträsk, ist eine einmalige Anlage aus dieser Zeit. Seltsam sind auch die Trullhalsar (Trollhälse), vendelzeitliche Grabkreise, die sich ähnlich auch im heutigen Polen finden. Nicht zuletzt prägten sechs große Gräberfelder (z. B. Gräberfeld von Lilla Bjärs, Gräberfeld von Lilla Ihre) und frühzeitliche Burganlagen, darunter Stora Havor, Grogarnsberget, Herregårdsklint, Styrmansberg und die Torsburg, die jüngere Vorzeit der Insel ebenso wie die Trojaburgen, die nordischen Labyrinthe.

Das Freilichtmuseum von Bunge und Gotlands Fornsal in Visby zeigen viel davon. Die über 800 auf der Insel gefundenen wikingerzeitlichen Hortfunde, darunter die drei im Jahr 2000 geborgenen Horte von Spillings (allein 65 kg Silber mit einem Materialwert von 600.000 €), sind hingegen in Stockholm zu sehen. Das gilt auch für den vorwikingerzeitlichen Hortfund von Havor. Bereits Mitte des 7. Jahrhunderts war das Baltikum Ziel von schwedischen Aktivitäten. Sagas, die allerdings erst im 13. Jahrhundert aufgezeichnet wurden, schildern die Taten der Könige Ivar Vidfamne (Ívarr inn víðfaðmi – 655–695 n. Chr.) und Harald Hildetand. Ivar Vidfamne soll das Baltikum und die Gegend um Gardarike in Karelien erobert haben. Von dauerhafter Landnahme kann aber keine Rede sein, denn das Reich zerfiel mit seinem Tod. Dass kurische Waffen und Schmuckstücke nach Gotland gelangten, belegen Ziernadeln, Fibeln und Schwerter aus dem 10. Jahrhundert. Man fand Utensilien, wie sie in der Umgebung von Klaipėda und Kretinga vorkommen, an der gotländischen Küste. Das Grab in Hugleifs belegt die Anwesenheit von Kuren auf der Insel. Die Funde deuten Handelsbeziehungen zu den Balten an, allerdings erst im 10. und 11. Jahrhundert. 
Eine Besonderheit ist, dass das Gotische (gutiska razda), das auf der Insel noch im frühen Mittelalter gesprochen wurde, eine eigenständige Sprache war und dass auf Gotland ein eigenes Landesrecht (Landskapslag) galt, das erst in der frühen Neuzeit ersetzt wurde. So kennt das Gutalag (Gotlandsgesetz) bis 1595 keinen Lagman. Da auch die Runensteine und die Grabfelder auf Gotland älter sind als jene am Mälaren, muss die Insel in der Geschichte weitaus früher eine wichtige Rolle gespielt haben als das Zentrum der Svear.

Die Reste der ältesten Siedlungsstruktur auf Gotland stammen aus der Zeit um Christi Geburt. Es sind große Steinfundamente, die bis zu 60 m lang sein können und Mauerstärken bis zu 1,5 m zeigen. Auf der Insel gibt es annähernd 1800 in Gruppen liegende Fundamente. Die großen Häuser boten Platz für Wohnteil und Stall. Auf den Fundamenten ruhte ein steiles Dach, abgestützt durch Doppelreihen kräftiger Pfosten. Das Dach war vermutlich mit Reet (gotländ. „Ag“) gedeckt, das bis in unsere Zeit als Deckmaterial für ländliche Wirtschaftsgebäude dient. Man baute primär Gerste, Weizen und Roggen an. Die Saat wurde durch Mauern aus Stein vor Vieh und Wild geschützt. Einige Reste der alten Ackersystemgrenzen, so genannte „Fornäckrar“ gibt es noch. Vieh wurde auch auf den nicht umzäunten Weiden, den Allmenden, gehalten. Rinder, Ziegen, Schafe, Hausschweine und Hühner sind in der prähistorischen Siedlung Vallhagar belegt. Einige der Plätze, darauf deuten reiche Funde hin, waren eisenzeitliche Handelszentren.

Schon in heidnischer Zeit war Gotland in 20 Thingbezirke eingeteilt. Diese Einteilung bestand bis 1745 fort, wurde dann aber den Pfarrbezirken angepasst.

Mitte des 11. Jahrhunderts gehörte die Insel noch zum Reich der heidnischen mittelschwedischen Svear, unter deren Schutz sie sich gestellt hatte. Der Sage nach soll der Norwegerkönig Olav der Heilige die Insel 1029 n. Chr. christianisiert haben. Die Grabplatte aus der Johanneskirche von Visby dokumentiert den Übergang. Gotland war bereits vor und während der Zeit der Wikinger sowie im frühen und späten Mittelalter ein wichtiger Platz für den Ostseehandel. Später hatte die Hanse maßgeblichen Anteil daran. Lange bevor Lübeck und andere Städte an der Ostsee gegründet wurden, waren die Inselhäfen Paviken und Fröjel, später dann Visby Drehpunkt des Warenverkehrs zwischen Avaldsnes auf Karmøy und Kaupang in Norwegen, Birka und Sigtuna in Schweden, Dorestad in den Niederlanden, Haithabu, Ribe und Tissø im damaligen Dänemark, Quentovic in Frankreich, Jomsburg (Vineta), Ralswiek, Reric, Truso und Wiskiauten an der südlichen Ostseeküste, Nowgorod in Russland und Seeburg im Baltikum. Bäuerliche Händler brachten begehrte Güter über das Meer. In ihrem Gefolge, wohl als Gäste oder als Partner, kamen im 12. Jahrhundert immer mehr Kaufleute aus den neu gegründeten Städten an der Ostsee und aus dem Rheinland und Westfalen auf die Insel. Schnell übernahmen sie mit eigenen Schiffen den Großteil des Handelsvolumens. Die deutschen Händler, zum größten Teil ansässig in Visby, wo sie großen Einfluss auf die Stadtentwicklung mit dem Bau prachtvoller Höfe, Häuser und der Marienkirche hatten, wurden alsbald zur ernsthaften Konkurrenz für die ländliche Bevölkerung; Spannungen waren daher unausweichlich. Dieser Gegensatz zwischen Kaufleuten und Landbevölkerung mündete 1288 in einen militärischen Konflikt.

In mehreren Kämpfen musste sich die Stadt Visby, die dem schwedischen König Magnus Ladulås unterstand und ihm zur Heeresfolge verpflichtet war, gegen die Bauern behaupten und erlitt dabei trotz ihrer Befestigungsanlage – die noch heute weitgehend erhaltene Stadtmauer war knapp 3,6 km lang und hatte neben drei Toren auch 44 Wehrtürme – schwere Zerstörungen. In der zweiten Julihälfte 1361 landete der dänische König Waldemar Atterdag mit einer 3000 Mann starken Streitmacht auf der Insel. Ein eilig zusammengetrommeltes Heer der Landbevölkerung versuchte ihn und seine Ritter aufzuhalten, war aber nach zwei Tagen aufgerieben. Am 27. Juli trafen die Dänen in Sichtweite der Stadtmauern auf das letzte Aufgebot. Es kam zu einem blutigen Gemetzel. Rund 2000 Menschen fanden den Tod, da die Stadtbewohner es nicht wagten, ihnen die Tore zu öffnen. Nach der Schlacht ergab sich Visby und öffnete „freiwillig“ den Truppen Waldemars seine Tore, erhielt aber im Gegenzug zwei Tage später die Bestätigung seiner alten Rechte und Privilegien.
Seitdem war Gotland dänisch. Im Krieg Dänemarks gegen Schweden besetzten 1394 die Vitalienbrüder die Insel als Operationsbasis, wo sie sich als Freibeuter unter der Losung „Gottes Freunde, aller Welt Feinde!“ allmählich verselbstständigten und zu gefürchteten Seeräubern entwickelten. Auf Vivesholm liegen noch die Reste einer Befestigungsanlage, die Albrecht von Mecklenburg nach seiner Absetzung als schwedischer König als Anführer der Vitalienbrüder bauen ließ. Schließlich vertrieb der Deutsche Orden unter Konrad von Jungingen 1398 die Vitalienbrüder von Gotland, das dem Ritterorden von Schweden verpfändet worden war. 1408 wurde die Insel Margarethe von Dänemark zugesprochen. Erich von Pommern begann 1411, am Südende Visbys eine Burg zu errichten. 1439 wurde er als dänischer König abgesetzt, herrschte aber noch elf Jahre über Gotland, bevor er die Burg 1448 dem neuen dänischen König übergab.

In der Folgezeit spielten die Gebrüder "Olof" und "Ivar Axelsson Tott" eine herausragende Rolle auf der Insel. Der dänische König Christian II., der Søren Norby als Lehnsmann über Gotland eingesetzt hatte, wurde aus seinem Land vertrieben und ging nach Gotland, das er gegen dänische und Lübecker Ansprüche verteidigte. 1525 beschossen die Lübecker Visby; sie konnten die Feste Visborg aber nicht bezwingen. Trotzdem geriet die Insel wieder unter dänische Herrschaft.

Der Übergang zum lutherischen Bekenntnis erfolgte auf Gotland im 16. Jahrhundert, ist aber nicht im Einzelnen dokumentiert.

1645 kam Gotland im Frieden von Brömsebro nach beinahe 300 Jahren wieder zu Schweden. Die Insel wurde 1654 der Exkönigin Christina als Unterhaltsland zugesprochen. Im dänisch-schwedischen Krieg von 1675–1679 wurde sie wieder von den Dänen besetzt, die sie aber am Ende räumen mussten, wobei sie die Stadtburg von Visby, die Visborg, sprengten. Im Nordischen Krieg 1700–1721 und im Finnischen Krieg 1808 wurde die Insel durch russische Truppen in Mitleidenschaft gezogen, bevor friedlichere Zeiten anbrachen.

Die Bedeutung Gotlands als Handelsmetropole der Ostsee ging bald verloren, weil die Buchten als natürliche Häfen für größere und schwerere Schiffe zu flach waren.

In jüngerer Zeit wurden auf der Insel Gotland einige neue Techniken zur elektrischen Energieübertragung erprobt. So ging 1954 zwischen Gotland und dem schwedischen Festland die erste operationelle Anlage zur Hochspannungs-Gleichstrom-Übertragung in der westlichen Welt in Betrieb, die HGÜ Gotland. 1999 wurde auf Gotland erstmals die HGÜ-Anbindung eines Windparks durchgeführt (HGÜ Visby-Nas).

Das Wappen und die offizielle Flagge Gotlands zeigen in Blau einen silbernen, goldbewehrten und goldgezungten Widder, der eine rote Fahne am goldenen Langkreuz mit einem goldenen Flugteil mit dem rechten Lauf hält. Daneben gibt es eine inoffizielle Version der Flagge Gotlands, die auf gelbem Grund mit einem grünen Kreuz belegt ist.

Neben dem Standardschwedischen wird auf Gotland noch heute eine besondere Sprache, Gutamål, gesprochen. Sie hat sich aus dem Altgutnischen entwickelt und wird heute allgemein als schwedische Mundart angesehen.

Die gotländischen Fischerdörfer (schwedisch Gotländska fiskelägen) sind für die Insel Gotland typisch. Heute gibt es noch etwa 150 Fischerdörfer verschiedener Größe, die früher in erster Linie von den an der Küste wohnenden Bauern genutzt wurden (Hallshuk, Helgumannen, Kovik). Davon stehen elf unter Denkmalschutz. Besonders von Fårö aus wurden auch Robben gejagt.

Es gibt fast hundert zumeist gut erhaltene Landkirchen, die überwiegend aus dem Mittelalter stammen. Die ältesten sind die Kirchen in Atlingbo, Fardhem und Stenkyrka (Steinkirche). Als Beispiel für die vor 1150 n. Chr. genutzten Holzkirchen ist eine partielle Rekonstruktion der Kirche von Hemse im Staatlichen Museum von Stockholm zu sehen. Die gotländischen Kirchen überraschen neben ihren Reliefs, Fresken und Taufbecken auch durch eine Vielzahl von Holzkreuzen von hoher künstlerischer Qualität. Die Gotländischen Triumphkruzifixe sind eine besonders schöne Gattung. In der Kirche von Öja hängt ein Triumphkruzifix aus dem 13. Jahrhundert, das in seiner Art einzigartig in Skandinavien ist.

Die Ortschaft Broa auf Gotland gab die Bezeichnung für eine frühmittelalterliche Kunstphase des Wikingerstils ab, die so genannte „Broa-Phase“ in der zweiten Hälfte des 8. Jahrhunderts, die eine Unterart der Oseberg-Stils ist. Sie ist gekennzeichnet durch ein Konglomerat von Motiven und Stilen. Lange und fadige Abzweigungen schaffen ein Netzwerk von lockeren Schlingen um die Haupttiere. Diese bandförmigen Tiere sind ein später Ausläufer der skandinavischen Tierornamentik, die im 6. Jahrhundert entstand.

Im Jahr 1900 zogen der Maler William Blair Bruce und die Bildhauerin Carolina Benedicks-Bruce in ihr Landhaus Brucebo in Väskinde. In den Folgejahren kamen dort verschieden bildende Künstler zu Besuch, aber auch Schriftsteller und Musiker. Seit 1937 sind dort junge Künstler als Stipendiaten untergebracht. Nach dem Zweiten Weltkrieg entstanden in Brucebo verschiedene Filme des Regisseurs Torbjörn Axelman. Seit 2012 ist das Wohnhaus als Künstlermuseum geöffnet.

Auf Gotland entstand der letzte Film von Andrei Tarkowski, "Opfer". Der genaue Ort, an dem das im Film gezeigte Haus zweimal abgebrannt wurde, ist 56°59′52,30″ N, 18°22′38,86″ E; man findet vom Haus aber keine Spuren mehr, nur das nahe Wäldchen ist noch unverändert erhalten. Hierhin pilgern jedes Jahr viele Tarkowski-Fans. Zudem spielen auf Gotland die Kriminalfilmserien "Maria Wern, Kripo Gotland" (2008–2016) und "Der Kommissar und das Meer" (seit 2007).



Heute ist Gotland ein beliebtes Ferienziel, speziell für die Schweden; denn das Klima der Ostseeinsel ist mild. Bei Fahrradtouristen und Jugendlichen ist die Insel besonders beliebt. Zu den Sehenswürdigkeiten auf Gotland wie auch auf der kleineren Nachbarinsel Fårö gehören die Raukar. Dies sind eigentümlich geformte, bis über zehn Meter hohe Kalksteinsäulen an den „Klappersteinfelder“ genannten Steinstränden von Digerhuvud und Langhammars, in den Wäldern bei Lickershamn, im Süden bei Hoburgen und im Osten bei Ljugarn.

Der Hauptanziehungspunkt der Insel ist die mittelalterliche Stadt Visby. Eines der größten Mittelalterfestivals des Nordens findet hier alljährlich in der 32. Kalenderwoche statt. Medeltidsveckan (Mittelalterwoche) lockt Jahr für Jahr rund 40.000 Besucher nach Gotland. Visby verwandelt sich dann für eine Woche in eine lebendige Hansestadt aus dem Jahre 1361 mit Kostümen, Musik, Theater, Markt und Gauklerei.

Am zweiten Wochenende im Juli findet in Stånga die Olympiade von Gotland (Gutarnas olymp) statt, eine mit den Highland-Games vergleichbare Veranstaltung.

Auf der Insel befindet sich in Kneippbyn, nicht weit von Visby, auch die "Villa Kunterbunt", bekannt aus Astrid Lindgrens Pippi-Langstrumpf-Büchern. Es ist das Originalgebäude aus den Filmen, die alle auf der Insel gedreht wurden. In dem Gebäude ist das Jäckchen von Herrn Nilsson, Pippis Äffchen, zu sehen, aber auch die Schreibmaschine, auf der Astrid Lindgren ihre Geschichten schrieb.

Seit 1995 finden jährlich in Rone die „offiziellen“ Kubb-Weltmeisterschaften statt. Dabei kamen im Jahr 2002 insgesamt 192 Teams mit jeweils sechs Mitgliedern zusammen, um den Titel zu erringen. Das Alter der Teilnehmer lag zwischen 8 und 85 Jahren.

Die größte Enduroveranstaltung der Welt, das "Gotland Grand National", findet jährlich am ersten Novemberwochenende mit über 2200 Teilnehmern in Tofta, 15 km südlich von Visby, statt.

Von den Festlandhäfen Oskarshamn und Nynäshamn sowie – saisonal – Västervik existieren Fährverbindungen der Reederei Destination Gotland nach Visby. Innerhalb des Archipels besteht eine kostenlose Fährverbindung zur Insel Fårö.

Visby verfügt über einen Flughafen, der von Stockholm und einigen anderen schwedischen Städten angeflogen wird. Es bestehen auch Verbindungen nach Helsinki und Oslo durch die Fluggesellschaft Gotlandsflyg. Von 2009 bis 2011 bestand während der Sommermonate eine Direktflugverbindung mit Berlin durch Air Berlin.

In Visby verkehren mehrere Stadtbuslinien. Der Rest der Insel ist durch Regionalbusverkehre erschlossen, die meist vom Busbahnhof in Visby starten. Ergänzend gibt es bedarfsgesteuerte Verkehre.

Im Jahre 1878 wurde die erste Eisenbahn auf Gotland eingeweiht. Die Strecke ging von Visby nach Hemse. Bis 1921 wurde die Strecke nach Norden und Süden verlängert. Daneben baute man ergänzende Strecken sowie wenige kurze Feldbahnen für den Kalksteintransport. Mit Ausnahme je einer rekonstruierten kurzen Museumsbahn und Feldbahn sind die Eisenbahnstrecken heute nicht mehr vorhanden.

Kalkstein wurde seit der Vorzeit als Baumaterial verwendet. Trockenmauerwerk (schwed. Kallmur genannt) ist Teil einiger prähistorischer Rösen (Kauparve) und Fornburgen (Torsburg). Besonders viele Kalksteinbauten wurden während des Mittelalters errichtet. In Visby zeugen vor allem die Ringmauer, die Kirchenruinen und die Speicher davon. Die mittelalterlichen Steinbrüche lagen in Bro, Hejdeby und nördlich von Visby. Der Kalksteinbruch wurde bis Mitte des 17. Jahrhunderts von den Bauern durchgeführt. Steine wurden wohl bereits vor dem Dombau in Lund (1103–1145) in die Städte an der Ostsee exportiert. Im Mittelalter wurde mit dem Brennen von Kalk begonnen. Es erfolgte zunächst in kleinen Meilern für den Hausbedarf. Ein mittelalterlicher Brennofen wurde während der 1970er Jahre außerhalb von Nordergravar entdeckt. Man nimmt an, dass er im Zusammenhang mit dem Bau der Ringmauer von Visby genutzt wurde. Mit dem 1161 erteilten Handelsprivileg von Heinrich dem Löwen eigneten sich die Visbyer Hanseleute die einträgliche Kalkindustrie an. Zuvor unbedeutende Plätze wie Barläst, Bläse, Fårösund, Kappelshamn, Katthammarsvik, Kyllaj, Länna, Lauters, Lergrav, Lörge, Sankt Olofsholm, Storugns und Värnevik blühten auf. Voraussetzung für eine lukrative Kalkherstellung war ein Steinbruch in der Nähe eines Hafens. Dort wurden Kalköfen errichtet, die vom 17. bis in die Mitte des 19. Jahrhunderts für die Insel prägend wurden. Zu Beginn des 17. Jahrhunderts wurde der erste dieser großen Brennöfen auf St. Olofsholm erbaut. Es gibt eine Beschreibung des vorindustriellen Brennvorganges von Carl von Linné, der die Insel im Jahre 1741 besuchte. Restaurierte Kalköfen stehen heute als Industriedenkmäler in Barläst und Kyllaj. 

Sandstein gibt es in dem Gebiet von Grötlingbo bis zur Südspitze der Insel. Die Steinbrüche, in denen seit frühgeschichtlicher Zeit Steine gebrochen und bearbeitet wurden, lagen hier nahe beieinander. Im Mittelalter wurden Baumaterial für Kirchen und andere Häuser sowie Steinblöcke für die Herstellung von Taufsteinen – sowohl für die eigenen Kirchen als auch für den Export – gewonnen.

Aufgrund des Klimawandels kam der ehemalige Softwarespezialist Lauri Pappinen im Jahre 2000 auf die Idee, auf Gotland Wein anzubauen. Nach ersten Versuchen setzte er auf die Rebsorten Rondo, Phoenix und Solaris. Sein Weingut „Gutevin“ professionalisierte sich und bekam mehrere Konkurrenten, darunter „Vinhuset Halls Huk“. Gemeinsam bilden sie, weit nördlich der früher angenommenen Weingrenze von 52° N, eines der nördlichsten Weinanbaugebiete Europas.





</doc>
<doc id="13613" url="https://de.wikipedia.org/wiki?curid=13613" title="Landtag">
Landtag

Landtag steht für (und Landtagswahl siehe jeweils entsprechend):

heutige Parlamente:

historische Parlamente und Organe (nach Gründungszeit):

historische Gerichte (Schweiz):
Siehe auch:


</doc>
<doc id="13616" url="https://de.wikipedia.org/wiki?curid=13616" title="Reformatio in peius">
Reformatio in peius

Reformatio in Peius (orthographisch auch Kleinschreibung zulässig; aus lat. "reformatio" „Veränderung“ und "peius" „das Schlechtere“; deutsche Begriffe: "Verschlechterung, Verböserung") ist ein juristischer Begriff. Er bedeutet, dass

Die gesetzliche Untersagung einer solchen Schlechterstellung wird als Verschlechterungsverbot bezeichnet.

Im Zivilprozess ist die Reformatio in Peius nur zulässig, soweit die andere Partei ebenfalls ein Rechtsmittel eingelegt hat.

Im Strafprozess gilt dies gleichermaßen bis auf eine prozessuale Situation: Die Staatsanwaltschaft hat eine Doppelrolle. Stellt sie einen Rechtsmittelantrag zu Gunsten des Angeklagten, ist eine reformatio in peius unzulässig, ebenso bei einem Einspruch gegen einen Strafbefehl, wenn der Einspruch gemäß Abs. 1 Satz 3 StPO auf die Höhe des einzelnen Tagessatzes beschränkt wird und das Gericht über diesen Einspruch im Beschlusswege (ohne Hauptverhandlung) entscheidet.

Wird gegen den Verwaltungsakt einer Behörde Widerspruch eingelegt, dann wird er – wenn die Ausgangsbehörde dem Widerspruch nicht abhilft – von der Widerspruchsbehörde überprüft, die einen Widerspruchsbescheid erlässt (Zuständigkeit kraft aufschiebend bedingtem Devolutiveffekt). Unstreitig ist, dass die Ausgangsbehörde den Verwaltungsakt im Widerspruchsverfahren nicht mit einer zusätzlichen Beschwer versehen darf. Sie kann dem Widerspruch nur abhelfen. Sehr streitig ist, ob der Widerspruchsbescheid gegenüber dem Ausgangsbescheid eine zusätzliche selbständige Beschwer enthalten darf.

Wenn die Widerspruchsbehörde eine Entscheidung fällt, die für den Widerspruchsführer gegenüber der ursprünglichen Entscheidung belastender ist, kommt entweder ein Fall der Reformatio in peius oder des Selbsteintritts in Betracht. Keine Verböserung liegt vor, wenn der Widerspruchsführer im Widerspruchsbescheid nicht zusätzlich, sondern in einem neuen Verwaltungsakt, der mit dem Widerspruchsbescheid verbunden sein kann, erstmals beschwert wird. Abzugrenzen ist danach, ob der Widerspruchsführer qualitativ (dann: erstmalige Beschwer) oder quantitativ (dann: zusätzliche Beschwer) zusätzlich verpflichtet wird. Bei einer erstmaligen Beschwer wird die Widerspruchsbehörde als sachlich zuständige Fachbehörde bei Gelegenheit des Widerspruchsverfahrens tätig.

Dazu sind folgende Aspekte zu beachten:

Nach VwGO gilt, dass das Gericht an den Antrag des Antragstellers gebunden ist. Das entspricht dem Grundsatz "ne ultra petita". Daraus könnte man sachlogisch folgern, dass auch die Widerspruchsbehörde an den Widerspruch des Widerspruchsführers gebunden sei. Dann dürfte nur dem Antrag gemäß der (gegebenenfalls teilweisen) Beschwer abgeholfen werden. Dieser Ansicht steht allerdings entgegen, dass VwGO nur für Gerichtsverfahren ab dem ersten Rechtszug gilt. Das Widerspruchsverfahren unterliegt einer rechtlichen Doppelnatur, was bedeutet, dass es auch Verwaltungsverfahren ist. Das wird schon dadurch klar, dass das Widerspruchsverfahren nicht durch eine gerichtliche Spruchkammer, sondern durch eine Verwaltungsbehörde wahrgenommen wird. Selbst wenn man das Widerspruchsverfahren ausschließlich als gerichtlichen Vorschaltbehelf qualifizierte (was kaum vertretbar ist), stünde dem VwGO der Abs. 2 VwGO entgegen, der offenbar von der Möglichkeit einer Verböserung ausgeht.

Bei der Zulässigkeit einer Verböserung durch den Widerspruchsbescheid ist zu besorgen, dass der Betroffene vom Gebrauch des Widerspruchs abgeschreckt werde. Das könnte die Effektivität des Rechtsschutzes ( Abs. 4 GG) einschränken.

Das Vertrauen des Widerspruchsführers nicht schlechter zu stehen, als er stünde, wenn er das Widerspruchsverfahren nicht eingeleitet hätte, ist nicht schutzwürdig. Ab Wirksamkeit des Verwaltungsakts hat der Adressat eine Art „Anwartschaft“ auf die zukünftige Bestandskraft des Bescheids, weil die Behörde die zukünftige Bestandskraft des Verwaltungsaktes einseitig nicht mehr verhindern kann (abgesehen von den VwVfG, die der Behörde auch nach Bestandskraft zur Verfügung stehen). Diese Position hat der Widerspruchsführer selbst aufgegeben, indem er durch Einlegung des Widerspruchs die Angelegenheit der Widerspruchsbehörde zur nochmaligen Entscheidung gegeben hat.

Die Widerspruchsbehörde hat eine umfassende Recht- und Zweckmäßigkeitskontrolle durchzuführen. Wegen der Bindung der Verwaltung an das Gesetz ( Abs. 3 GG) muss die Widerspruchsbehörde auch die Möglichkeit haben, zulasten des Widerspruchsführers zu entscheiden.

Der herrschenden Meinung gemäß regelt § 79 Abs. 2 VwGO nur die prozessualen Folgen einer Verböserung für den Fall, dass nach dem Verwaltungsverfahrensrecht eine Verböserung zulässig ist. Das ist dann der Fall, wenn die Widerspruchsbehörde in der Sache ermächtigt ist gegenüber dem Bürger tätig zu werden. Woraus sich die Zuständigkeit der Widerspruchsbehörde ergibt, in der Sache dem Bürger eine selbständige zusätzliche Beschwer aufzuerlegen, ist unterschiedlich zu beurteilen.

Die heute herrschende Auffassung folgt dem Bundesverwaltungsgericht und sieht bei Verschiedenheit von Ausgangs- und Widerspruchsbehörde die Widerspruchsbehörde dann zur Verböserung durch den Widerspruchsbescheid in der Sache zuständig, wenn sie gleichzeitig Fachaufsichtsbehörde der Ausgangsbehörde ist. Die Berechtigung, in die Rechte des Bürgers einzugreifen, ergibt sich nach umstrittener Auffassung entweder analog aus den Regeln über den Widerruf und die Rücknahme von Verwaltungsakten oder aus derselben Eingriffsgrundlage, welche für die Ausgangsbehörde in Betracht kommt.

Gegen die reformatio in peius durch die Widerspruchsbehörde kann sich der Widerspruchsführer durch Anfechtungsklage wehren. In der Zulässigkeit bestehen folgende Besonderheiten:


In der Begründetheit sind folgende Besonderheiten zu beachten:
Verfahrensrechtlich ist gemäß VwGO eine Anhörung hinsichtlich der Verschlechterung in allen Fällen erforderlich.

Eine Reformatio in Peius durch Verwaltungsgerichte ist grundsätzlich unzulässig. Ausnahmsweise zulässig ist sie in folgenden Fällen:

Auch im Verfahren vor den Finanzgerichten darf nicht verbösert werden ( FGO).

Die Regelungen für Verwaltungsgerichte gelten entsprechend im Sozialprozess ( SGG).

Im Verfahren vor den Arbeitsgerichten gelten über die Verweisung in Abs. 2 Satz 1 ArbGG die gleichen Grundsätze wie allgemein im Zivilprozess.

Im Beschwerdeverfahren dürfen grundsätzlich nur Widerrufsgründe geprüft werden, die Gegenstand der ersten Instanz waren. Die Missachtung dieser begrenzten Anfallwirkung würde zu einem Verstoß gegen den im Rechtsmittelrecht geltenden Grundsatz des Verschlechterungsverbots (reformatio in peius) führen.

Nach einer Grundsatzentscheidung des Bundesgerichtshofs kann das Deutsche Patentamt jedoch aus prozessökonomischen Gründen nach pflichtgemäßem Ermessen anstelle dieser Gründe oder zusätzlich von Amts wegen oder mit Einverständnis des Patentinhabers analog ZPO auch weitere Widerrufsgründe nach PatG in das Verfahren einbeziehen und gegebenenfalls zur Grundlage eines Widerrufs machen. Das gilt insbesondere, wenn Dritte nach Ablauf der Einspruchsfrist als Einsprechende dem Einspruchsverfahren beitreten ( Abs. 2 PatG).

Das Bundespatentgericht hingegen hat keine Verfügungsbefugnis über das Beschwerdeverfahren. Es darf deshalb eine Entscheidung im Rechtsmittelverfahren gem. PatG, §§ 308, 528, 557 ZPO wie ein Zivilgericht nur insoweit abändern, als eine Änderung beantragt ist. Es darf dem Beschwerdeführer auch nicht mehr zuerkennen, als dieser beantragt.

Für die Beschwerde gegen Entscheidungen des Europäischen Patentamts gelten nach dem Europäischen Patentübereinkommen (EPÜ) eigene Regeln.

Ist der Patentinhaber der alleinige Beschwerdeführer gegen eine Zwischenentscheidung über die Aufrechterhaltung des Patents in geändertem Umfang, so kann weder die Beschwerdekammer noch der nicht beschwerdeführende Einsprechende die Fassung des Patents gemäß der Zwischenentscheidung in Frage stellen. Ist der Einsprechende der alleinige Beschwerdeführer, so ist der Patentinhaber primär darauf beschränkt, das Patent in der Fassung zu verteidigen, die die Einspruchsabteilung ihrer Zwischenentscheidung zugrunde gelegt hat. Änderungen, die der Patentinhaber selber vorschlägt, können von der Beschwerdekammer abgelehnt werden, wenn sie weder sachdienlich noch erforderlich sind. In einer weiteren Entscheidung entschied die Große Beschwerdekammer, eine reformatio in peius sei aufgrund des Fehlens einer Bestimmung zur Anschlussbeschwerde nach dem EPÜ nicht völlig ausgeschlossen, weil sie der Vermeidung unnötiger Streitigkeiten dienen könne und gleichzeitig den Anspruch der Beteiligten auf rechtliches Gehör befriedige.

In Österreich bezieht sich das Verbot der reformatio in peius stets auf ein vom Beschuldigten eingebrachtes Rechtsmittel. Ein solches darf die Ausgangsposition des Beschuldigten nie verschlechtern. Erhebt hingegen die Gegenpartei (Bsp. Staatsanwaltschaft im Strafverfahren) ein Rechtsmittel, so kann die Strafe sehr wohl erhöht werden.

Steuerrecht:

Im österreichischen Steuerrecht bedeutet das Verschlechterungsverbot, dass keine Schlechterstellung durch eine oberstgerichtliche Rechtsauslegung in der steuerlichen Auswirkung eintreten darf.

Das Verschlechterungsverbot des der Bundesabgabenordnung (BAO) wurde vom Verfassungsgerichtshof als verfassungswidrig aufgehoben.

Strafrecht:

Im Strafverfahren ist die reformatio in peius in StPO explizit untersagt, wenn ein Rechtsmittel nur zu Gunsten des Angeklagten erhoben wird.

Verwaltungsstrafrecht:

Im Verwaltungsstrafverfahren muss grundsätzlich zwischen einem ordentlichen Verfahren und den abgekürzten Verfahren des VStG unterschieden werden.

"Ordentliches Verfahren gem. §§ 40ff VStG:"

Wird eine Strafe im ordentlichen Verfahren (Straferkenntnis) durch den Beschuldigten vor den Landesverwaltungsgerichten angefochten, so ist eine reformatio in peius gemäß VwGVG untersagt, wenn das Rechtsmittel vom Beschuldigten oder zu seinen Gunsten erhoben wird.

"Abgekürztes Verfahren gem. §§ 47ff VStG:"

Nicht in allen Arten des abgekürzten Verfahrens ist das Verschlechterungsverbot verankert.

Bei Strafverfügungen ist ein Verschlechterungsverbot in Abs. 2 VStG explizit angeführt.

Bei Anonymverfügungen nach VStG und Organstrafverfügungen nach VStG sind keine Rechtsmittel vorgesehen. Ein Nichtbezahlen der vorgeschriebenen Geldleistung führt zu einer Strafverfügung oder der Einleitung des ordentlichen Verfahrens. In beiden Fällen ist die Verhängung einer höheren Strafe zulässig.

Im Zivilprozess gilt das Verbot der reformatio in peius. Es ist Ausfluss der Dispositionsmaxime. Durchbrochen wird dieses Prinzip bei Geltung der Offizialmaxime (Streitigkeiten betreffend Kindesunterhaltszahlungen) oder wenn die Gegenpartei eine Anschlussberufung stellte gemäss Art. 313 E ZPO.

Im Militärstrafprozess folgt auf eine Einsprache gegen ein Strafmandat ein ordentliches Verfahren vor einem Militärgericht, in welchem eine reformatio in peius zulässig ist.




</doc>
<doc id="13617" url="https://de.wikipedia.org/wiki?curid=13617" title="Theorbe">
Theorbe

Die Theorbe (ital. "tiorba", franz. "théorbe", engl. "theorbo") ist ein Musikinstrument und gehört zur Familie der Lauteninstrumente. Ihr bautechnisches Kennzeichen ist der zweite Wirbelkasten an einem verlängerten Hals. Typen der Theorbe sind der italienische "chitarrone", die französische "théorbe des pièces" und die "English theorbo".

Theorbe wird auch eine Reihe unterschiedlicher Basslauten genannt, deren gemeinsames Merkmal ein zweiter Wirbelkasten (zur Aufnahme von Basssaiten) ist, die aber nicht Theorben im engeren Sinne sind: Liuto attiorbato, Arciliuto, Archlute, deutsche Barocklaute, Angelica (Angélique).

Die Etymologie des Namens "Theorbe" ist bisher nicht hinreichend geklärt. Laut Athanasius Kircher war der Name zunächst scherzhaft gemeint und bezeichnete eigentlich im neapolitanischen Dialekt das Mahlbrett, auf dem die duftenden Essenzen und Kräuter der Parfümeure und Apotheker zerrieben wurden.

Das ältere Synonym "chitarrone", in Anlehnung an die antike Kithara als Augmentativ von "chitarra" abgeleitet (einer fünfchörigen italienischen Laute), war bis ca. 1650 in Gebrauch. In Deutschland wird seit dem 18. Jahrhundert auch der neu erfundene Name Erzlaute verwendet. Die Theorbe im engeren Sinn unterscheidet sich jedoch grundsätzlich von der Laute durch ihre Stimmung.

Die neue Musik ab 1600 (Monodie) erforderte Instrumente mit einem tiefen Bassregister zur Begleitung. Damit Darmsaiten bei gleicher Spannung tiefer klingen, muss ihre Masse erhöht werden. Die Erhöhung der Masse erfolgt, indem die Saiten dicker oder länger hergestellt werden. Die bautechnische Lösung zur Aufnahme längerer Saiten war der zweite Wirbelkasten an einem verlängerten Hals.

Die meisten erhaltenen Theorben zeichnen sich durch ihre Größe und die damit verbundene lange Griffbrett-Mensur aus, die zwischen ca. 80 und 100 cm variieren kann. 

Durch die lange Mensur des Griffbretts entstand ein Problem bei den hoch klingenden Saiten. Die erforderlichen Darmsaiten sind so dünn, dass sie sehr leicht reißen. Deswegen wird der erste und zweite Chor der Theorbe eine Oktave tiefer eingestimmt, so dass der dritte Chor der am höchsten klingende ist (reentrant tuning, rückläufige Stimmung). Auch durch diese Stimmung unterscheidet die Theorbe sich von der Laute.

Im 17. und 18. Jahrhundert war die Theorbe unter den Zupfinstrumenten das bevorzugte Generalbass-Instrument.

Die Kleinform der Theorbe ist das "Tiorbino", das eine Oktave höher als die Theorbe in der gleichen rückläufigen Stimmung gestimmt ist. Zwei Autoren haben für dieses Instrument geschrieben: Bellerofonte Castaldi ("Capricci a 2 stromenti cioè tiorba e tiorbino", Modena 1622) und Jean-Baptiste Besard ("Novus Partus," 1617).

Die prominentesten Vertreter des Instrumentes in Italien waren Johann Hieronymus Kapsberger, Bellerofonte Castaldi und Alessandro Piccinini. Aus England ist vorläufig keine Solomusik für Theorbe bekannt, aber William Lawes u. a. setzte sie in seiner Kammermusik ein. In Frankreich wurden Theorben bis in das erste Drittel des 18. Jahrhunderts geschätzt und sowohl für kammermusikalische als auch orchestrale Musik eingesetzt (Nicolas Hotman, Robert de Visée, François Campion). In den Hoforchestern von Wien, Bayreuth, Berlin und Brüssel waren Theorbisten bis nach 1750 beschäftigt (Ernst Gottlieb Baron, Francesco Bartolomeo Conti, Adam Falckenhagen, Paul Carl Durant, Giovanni Paolo Foscarini). Auch der als „letzter Lautenist“ bezeichnete Christian Gottlieb Scheidler (1747–1829) spielte Theorbe. Zeitgenössische Spieler sind Christina Pluhar und Anton Birula.

Solomusik für Theorbe wurde meist in Tabulatur notiert.





</doc>
<doc id="13621" url="https://de.wikipedia.org/wiki?curid=13621" title="See (Begriffsklärung)">
See (Begriffsklärung)

See bezeichnet:

See als Ortsname:

Gemeinde in Österreich

Gemeindeteile in Bayern
Gemeindeteile in Sachsen
Gemeindeteile in Österreich
See als Familienname:
Sée bezeichnet
SEE steht als Abkürzung für

SEE als Unterscheidungszeichen auf Kfz-Kennzeichen:

SE-E steht für

Siehe auch:



</doc>
<doc id="13622" url="https://de.wikipedia.org/wiki?curid=13622" title="See">
See

Ein See ist ein Stillgewässer mit oder ohne Zu- und Abfluss durch Fließgewässer, das vollständig von einer Landfläche umgeben ist. Er stellt ein weitgehend geschlossenes Ökosystem dar.

Ein See ist ein Binnengewässer, das eine (größere) Ansammlung von Wasser in einer Bodenvertiefung einer Landfläche darstellt und im Gegensatz zu einem Binnenmeer (zum Beispiel dem Mittelmeer) auf der 0-Meter-Höhenlinie keine direkte Verbindung zum Weltmeer hat. Damit weist er keinen durch Meeresströmungen bedingten Zu- und/oder Abfluss auf. Zu- und Abflussmenge sind in der Regel gegenüber der Gesamtwassermenge eines Sees gering. Im Gegensatz zu einem Fließgewässer weist ein See kein Gefälle auf.

Der Begriff "Binnensee" wird gebraucht, um Seen des Binnenlandes von "Küstenseen" (Strandseen, küstennahen "Brackwasserseen" oder durch Eindeichung der Küste entstandene Seen) abzugrenzen, aber auch allgemein zur Bezeichnung von Seen.

Ein See im Sinn der limnologischen Definition ist in der Regel wesentlich tiefer als ein Teich, Tümpel oder Weiher, so dass sich eine über Tage bis Monate stabile Temperaturschichtung ausbilden kann. Die Frequenz ihrer Durchmischung wird zu einer Einteilung der Seen benutzt, da sie auch weitreichende ökologische Folgen hat (siehe Ökosystem See). In dieser Hinsicht gelten auch die flachen Steppenseen wie der Neusiedler See oder der Plattensee nicht als „echte“ Seen.

Allerdings ist die genaue Abgrenzung zwischen Seen und Tümpeln/Weihern etc. unscharf und immer subjektiv. Deshalb bezeichnen einige Limnologen jede mit Wasser gefüllte Senke als See. Für ihre Kategorisierung wäre dann unerheblich, ob ein See ständig, periodisch oder episodisch mit Wasser gefüllt ist und ob er eine permanente Schichtung ausbildet.

Umgangssprachlich ist die Zuordnung oft abhängig von der Salinität, diese ist jedoch kein Kriterium. Ein See enthält zwar meistens Süßwasser, es gibt aber auch große Salzseen, wie z. B. das Kaspische Meer (treffender ist deshalb der früher geläufige Name "Kaspisee"), den Aralsee und das Tote Meer. Auch sodahaltige Seen gibt es, zum Beispiel die des Rift Valley im Ostafrikanischen Grabenbruch wie der Nakurusee oder einige der "Lacken" um den Neusiedler See.

Eine weitere Definition kann über die Größe erfolgen. Die Mindestgröße eines Sees beträgt etwa einen Hektar.

In der Astronomie spricht man auch dann von Seen, wenn diese eine andere Flüssigkeit als Wasser enthalten, etwa bei den Methanseen auf Titan.

Geologisch und geomorphologisch unterscheidet man folgende Seearten:

Natürlich entstandene Seen kann man nach der Art ihrer Entstehung weiter untergliedern:

Abgesehen von der Möglichkeit, dass Seen durch rasche Ereignisse verschwinden können, etwa durch Verschütten seiner Mulde oder Verschwinden seines Damms, haben Seen von ihrem Stoffhaushalt die typische Tendenz zu vergehen. Zuflüsse bringen Sediment ein, das sich im strömungsarmen See sukzessive absetzt. Von Bergen langsam herunterkommende Schuttkegel, Lawinen und Muren tragen ebenso zum Verfüllen eines Sees bei. Schon allein der Eintrag von Staub, Pollen, Blättern und Pflanzenteilen bis Schwemmholz sinkt zu einem großen Teil im Wasser ab, sinkt Material mit organischem Kohlenstoff in tiefe, sauerstoffarme Schichten wird es auch nicht oxidiert und zersetzt, sondern setzt sich materiell als kohlenstoffreicher Schlamm ab. Dieser Vorgang ist abhängig von der Eintragungsrate mit Luftströmungen und durch über den See auskragenden Uferbewuchs. In trockenen Gebieten oder Zeiten können Seen ohne ausreichenden Zufluss auch austrocknen.

Natürliche und künstlich angelegte Seen bieten neben ihrer Bedeutung für die Natur auch einige Nutzungsmöglichkeiten für den Menschen.

Die meisten Seen werden entweder von Berufs- oder Angelfischern bewirtschaftet. Ferner können Seen als Badesee für Freizeit und Erholung, Schwimmen und Baden genutzt werden. Größere Seen bieten Möglichkeiten zum Wasserskifahren, Windsurfen und Segeln. Auf vielen großen Seen wird auch Binnenschifffahrt betrieben. Stauseen dienen oft der Stromerzeugung in Wasserkraftwerken. Aus Stauseen und hinreichend sauberen Naturseen wird auch oft Trinkwasser gewonnen.

Weltweit gibt es 1,4 Millionen Seen mit Oberflächen größer als zehn Hektar. Das Land mit den meisten Seen ist Kanada mit über 900.000 Seen. Finnland hat 187.888 Seen. Das gesamte Wasser aller Seen würde die gesamte Landoberfläche 1,30 Meter hoch bedecken. Durchschnittlich tauscht sich das Wasser eines Sees alle fünf Jahre komplett aus. Die zehn größten Seen der Welt enthalten 85 Prozent des Wassers aller Seen. Der Baikalsee enthält alleine ein Fünftel des gesamten Süßwassers der Erde.

Der Lhagba Pool in Tibet lag 6.368 m hoch, seine Klassifizierung als See (statt eines Schmelzwasserpools) war allerdings umstritten. Der See ist heute nicht mehr vorhanden. Danach gilt der Kratersee des 5920 m hohen Vulkans Licancabur an der Grenze zwischen Bolivien und Chile als höchster See der Erde.

Der Titicaca-See liegt 3810 m über dem Meeresspiegel und ist das höchstgelegene kommerziell schiffbare Gewässer der Erde.


Der Blue Lake in der neuseeländischen Region Tasman hat mit 70–80 Metern die höchste Sichtweite aller natürlichen Süßwassergewässer. Destilliertes Wasser für Laborzwecke hat eine Sichtweite von etwa 80 m.

Der Anstieg der CO-Konzentration in der Erdatmosphäre im Anthropozän führt neben der Versauerung der Weltmeere auch zur Versauerung von Seen.

Aufnahmen der Raumsonde Cassini zeigten, dass auf dem Saturnmond Titan Seen aus flüssigem Methan und Ethan existieren, die von Flüssen gespeist werden. Die Durchschnittstemperatur des Titan beträgt –179 °C, dadurch bleibt das Methan flüssig. Der größte See des Titan ist mit rund 400.000 Quadratkilometern das Kraken Mare.

Im Niederdeutschen (und ebenso im Niederländischen) sind die Wortbedeutungen von „Meer“ und „See“ gegenüber dem Hochdeutschen vertauscht: Die an Norddeutschland angrenzenden Meere heißen Nordsee und Ostsee (jeweils die See); im Landesinneren liegen dagegen z. B. das Steinhuder Meer, das Zwischenahner Meer, das Große Meer und andere; in den Niederlanden wurde die Zuiderzee nach ihrer Eindeichung in IJsselmeer umbenannt.

Raue oder hohe See (die …) bedeutet hohen Seegang, also hohen Wellengang am Meer.




</doc>
<doc id="13623" url="https://de.wikipedia.org/wiki?curid=13623" title="Nomenklatur (Chemie)">
Nomenklatur (Chemie)

Unter Nomenklatur versteht man in der Chemie die möglichst systematische und international möglichst einheitliche Namensgebung für chemische Stoffe. Dabei gilt heute als wichtig, dass ein Verbindungsname "eindeutig" ist und nur zu einer einzigen Strukturformel führt. Die Bezeichnung „Ethanol“ bezeichnet beispielsweise "nur" die Verbindung CH-CH-OH und keine andere. Umgekehrt haben chemische Verbindungen aber keinen eindeutigen Namen, z. B. kann man die Verbindung CH-CH-OH nach verschiedenen Nomenklatursystemen sowohl als „Ethanol“ als auch als „Ethylalkohol“ bezeichnen.

Bis ins 18. Jahrhundert war die Bezeichnung chemischer Stoffe sehr uneinheitlich. Einen wichtigen Schritt in Richtung Systematisierung stellte 1787 das Buch "Méthode de nomenclature chimique" von Louis Bernard Guyton de Morveau, Antoine Laurent de Lavoisier, Claude Louis Berthollet und Antoine François de Fourcroy dar. Jöns Jakob Berzelius führte um 1825 die chemische Zeichensprache mit Buchstaben für chemische Elemente ein. 1860 schlug ein Komitee unter Leitung von Friedrich August Kekulé ein internationales Bezeichnungssystem für organische Verbindungen vor. 1919 wurde die International Union of Pure and Applied Chemistry (IUPAC) gegründet. Seitdem betrachtet sie die Festlegung internationaler Standards für die chemische Nomenklatur als ihre Hauptaufgabe.

Um die Bezeichnungsweisen für chemische Verbindungen zu vereinheitlichen, gibt es die als international verbindlich vereinbarten Richtlinien der IUPAC (International Union of Pure and Applied Chemistry) und der IUBMB (International Union of Biochemistry and Molecular Biology) sowie deren als Ausgleichskommission eingesetzte "Joint Commission on Biochemical Nomenclature". Diese regeln den englischen Sprachgebrauch. Die Bezeichnungen in anderen Sprachen werden von den nationalen Chemikerverbänden entsprechend übertragen. So ist im deutschsprachigen Raum der Deutsche Zentralausschuss für Chemie unter Geschäftsführung der Gesellschaft Deutscher Chemiker (GDCh) im Einvernehmen mit den nationalen IUPAC-Mitgliedsgesellschaften der Schweiz und Österreich für die Umsetzung zuständig. Auch die IUPAC selbst verwendet in ihren Elementlisten viele englische Namen statt der den Elementkürzeln zugrunde liegenden (z. B. Potassium, Sodium, Tungsten, Mercury). Die Bedürfnisse verschiedener Sprachen und sogar des Englischen selbst werden von der IUPAC ausdrücklich anerkannt. Besonders stringente Nomenklaturregelungen sind insbesondere für Index-Werke für chemische Stoffe wie Beilsteins Handbuch der Organischen Chemie und Chemical Abstracts bis vor kurzem notwendig gewesen, da deren System zur Auffindung von Einträgen bis zur Einführung elektronischer Recherche hauptsächlich danach erfolgte.

Da die systematische Bezeichnung von chemischen Verbindungen nach diesen Regeln oft sehr kompliziert ist, wird von den Chemikern im Alltagsgebrauch bis hin zu wissenschaftlichen Publikationen weiterhin eine große Anzahl von traditionellen Namen oder neu geschaffenen, anerkannten Kurznamen verwendet. Die IUPAC unterscheidet zwischen Trivialnamen, die keinen Bezug zur systematischen Nomenklatur haben (z. B. Wasser, Harnstoff oder Glaubersalz), semisystematischen Namen oder Semitrivialnamen, die zumindest einen Teil eines systematischen Namens verwenden (z. B. Kohlendioxid statt Kohlen"stoff"dioxid, Trityl für die Triphenylmethyl-Gruppe oder Glycerin für Propan-1,2,3-triol) und den bereits erwähnten systematischen Namen.
Auch für die Erfindung neuer Trivialnamen, z. B. von neu entdeckten Naturstoffen, gibt es IUPAC-konforme Regeln.

Ferner herrschen bei den Elementnamen die nationalen Gewohnheiten vor und selbst die IUPAC-Namensstamm entsprechen nicht durchgängig dem für die Formelkürzel maßgebenden Namen (Beispiel Hg = Hydrargyrum, dt. Quecksilber, IUPAC-Wurzel „mercur“ wie engl. mercury und lat. Mercurius).

Die Namen der chemischen Elemente werden von den Entdeckern festgelegt. Für unbekannte oder neue Elemente, die noch keinen Namen erhalten haben, gibt es systematische Elementnamen, die sich von der Kernladungszahl ableiten. Eine systematische Anordnung der Elemente nach ihrer Elektronenkonfiguration bietet das Periodensystem der Elemente. Für einige Elemente existieren alte deutsche Bezeichnungen, die in mehreren Revisionen von der IUPAC an die im englischsprachigen angepasst wurden. Dies betraf vor allem Elementnamen, in denen die Buchstaben "k" und "z" gegen "c" ausgetauscht wurden. Beispiele sind Kalzium – Calcium, Silizium – Silicium oder Kobalt – Cobalt. Aber auch einige andere Schreibweisen wie Jod, das zu Iod verändert wurde oder Wismut zu Bismut wurden geändert. Während in der Chemie überwiegend die neuen Bezeichnungen verwendet werden, werden in anderen Bereichen und im Allgemeinen Sprachbereich vielfach noch die alten Namen genutzt.

Für jedes Element existiert ein Kürzel aus ein bis drei Buchstaben (Elementsymbol). Die Elementsymbole sind international gültig, sie werden also beispielsweise auch auf Japanisch durch lateinische Buchstaben wiedergegeben. 

Will man ein bestimmtes Isotop eines Elements bezeichnen, so stellt man dessen Massenzahl hochgestellt vor das Elementsymbol, zum Beispiel C für das Kohlenstoff-12-Isotop, U für Uran 235, etc. Die schweren Isotope des Wasserstoffs, H (Deuterium) und H (Tritium), besitzen mit D bzw. T auch ein eigenes Elementsymbol.

Um Verbindungen von verschiedenen Elementen untereinander zu benennen, werden die Elementnamen teilweise abgewandelt und mit Nachsilben versehen. Dazu verwendet man den Namensstamm, welche aus den lateinischen bzw. griechischen Elementnamen abgeleitet sind. So wird beispielsweise der Sauerstoff in der Verbindung Aluminium"ox"id (AlO) durch seine Namensstamm "(ox)" und die Endung "-id" angegeben.

Falls eine Art von Atomen oder Atomgruppen in einem Molekül mehrfach vorkommt, wird die Anzahl durch ein entsprechendes Zahlenpräfix (Vorsilbe) angegeben, das von den griechischen Zahlwörtern abgeleitet ist und dem Namen des entsprechenden Atoms bzw. der entsprechenden Atomgruppe vorangestellt wird. Ausgenommen sind "nona" und "undeca", welche aus dem Lateinischen abgeleitet sind. 

Die Präfixe "mono" und "di" werden nur für Eins und Zwei verwendet. In Verbindung mit anderen Zahlwörtern (21, 101, etc.) werden "hen" und "do" verwendet. Die Bildung des kompletten Zahlwortes erfolgt dabei "von hinten nach vorne". 
Beispiele:

Bei Metallverbindungen nennt man nur die Wertigkeit bzw. Oxidationszahl, die das Metall in dieser Verbindung (Ionenbindung) besitzt: z. B.: CrO Chrom(VI)-oxid, gelesen Chrom-"sechs"-oxid, anstatt Chromtrioxid (Stocksche Nomenklatur). Die Wertigkeit bzw. Oxidationszahl wird dabei mit römischen Zahlen angegeben.
Falls der Name einer Verbindung dadurch eindeutig bleibt, kann man die Wertigkeit auch weglassen. So gibt es z. B. nur ein einziges Oxid des Aluminiums, nämlich AlO, weshalb man statt Aluminium(III)-oxid auch einfach Aluminiumoxid schreiben kann.

Sehr oft wird die Vorsilbe "mono-" weggelassen, z. B. NaCl = Natriumchlorid und nicht Natrium"mono"chlorid.

Falls mehrere identische Gruppen vorhanden sind, bei denen die Verwendung der obigen Vorsilben missverständlich wäre, werden die folgenden aus dem griechischen hergeleiteten Präfixe verwendet. Ab Vier wird das einfache Zahlenpräfix zusammen mit der endung "-kis" verwendet. 

Beispiele:

Für die direkte Verknüpfung von identischen Einheiten verwendet man die folgenden Vorsilben, welche von den lateinischen Zahlwörtern abgeleitet sind:

Beispiel:

Beim Schreiben von Formeln von chemischen Verbindungen folgt man im Wesentlichen der Elektronegativitätsskala der chemischen Elemente. Man beginnt immer mit dem elektropositiveren Verbindungspartner, deshalb schreibt man etwa AgCl, AlO, PCl und nicht umgekehrt. Eine Ausnahme von dieser Regel sind die Wasserstoffverbindungen.

Wasserstoffatome schreibt man in den Formeln "an letzter Stelle" (NH, SiH, etc.). Handelt es sich jedoch um aciden Wasserstoff (d. h. die Verbindung reagiert in wässriger Lösung sauer), so schreibt man den Wasserstoff "am Anfang der Formel" (HF, HCl, HBr, HI, HO, HO, HS, HSe, HTe). Bei der Benennung dieser Verbindungen wird beispielsweise neben dem Namen „Hydrogenfluorid“ für HF oder „Hydrogenchlorid“ für HCl auch der insbesondere im Labor wesentlich gebräuchlichere Name Fluorwasserstoff bzw. Chlorwasserstoff verwendet. Letztere sind aufgrund der eindeutigeren Bezeichnung insgesamt zu bevorzugen, da es sich bei diesen Verbindungen nicht um Salze mit den entsprechenden Anionen handelt, sondern aufgrund der hohen Elektronegativitätszahlen nur um stark partiell geladene Dipolverbindungen, in denen eine stark polare kovalente Bindung (Atombindung) vorliegt. Außerdem ist aufgrund der Eindeutigkeit bei der Benennung zu beachten, dass beispielsweise unter „Hydrogenfluorid“ Salze des Typs MHF bzw. MF×HF (mit M = einwertiges Metall) verstanden werden, somit ist Fluorwasserstoff bevorzugt zu verwenden. Auch bei anorganischen Oxosäuren schreibt man den Wasserstoff am Anfang der Formel, obwohl er eigentlich am Sauerstoff gebunden ist, also für Schwefelsäure zum Beispiel HSO statt SO(OH).
Zur Benennung des Radikals wird dem Stammnamen die Endung -yl angehängt. Dies gilt sowohl in der organischen wie auch in der anorganischen Chemie.

Beispiele:
HO: Hydroxyl (Stamm: Hydrox-) und
CH: Methyl (Stamm: Meth-)

Einige Radikale haben, besonders bei den Sauerstoffverbindungen, spezielle Namen.

Für die Benennung von organischen Verbindungen nach dem IUPAC-System geht man üblicherweise von einem Stammsystem aus, das unter Umständen weitere Substituenten (Reste) trägt. Ein Substituent ist dabei ein Atom oder eine Atomkombination, welche ein Wasserstoffatom des Stammsystems ersetzt (substituiert). Für die Benennung der Verbindung wird der Name des Stammsystems unverändert übernommen und die Namen der substituierenden Gruppen werden dem Stammsystem in abgewandelter Form angefügt (substitutive Nomenklatur).

Als Ergänzung zum Stammsystem wird bei entsprechenden Verbindungen ein Deskriptor genannter Präfix vor dem systematischen Substanznamen ergänzt [z. B. "cis"-, "trans"-, ("E")-, ("Z")-, "o"-, "m"-, "p"-, "n"-, "iso"-, "neo"-, "cyclo"-, "sec"-, "tert"-, -, -, "meso"-, (±)-, (+)-, (–)-, ("RS"), ("R")-, ("S")-], der die Konfiguration oder die Stereochemie des Moleküls beschreibt. Häufig werden Deskriptoren in Kombination mit Lokanten (z. B. "O"-, "N"-, "S"-, α-, β-, [3.3]) zur genauen Beschreibung bestimmter Positionen von Atomen oder Bindungen verwendet, um eine chemische Struktur eindeutig zu benennen.

Um eindeutige Formeln und Namen zu erhalten, werden in der Nomenklatur Namensteile und spezielle Angaben in Klammern gesetzt. Drei Arten von Klammern werden verwendet: runde ( ), eckige <nowiki>[ ]</nowiki> und geschweifte <nowiki>{ }</nowiki>. Die Verwendung von eckigen Klammern ist in der Anorganischen und Organische Chemie allerdings unterschiedlich. Mehrfach vorkommende Einheiten werden in runde Klammern gesetzt, mit Ausnahme von Koordinationseinheiten, welche immer in eckigen Klammern stehen. Im Namen organischer Verbindungen werden von innen nach außen runde, eckige und geschweifte Klammern gesetzt: <nowiki>{[( )]}</nowiki>, <nowiki>{[({[( )]})]}</nowiki>, usw. In Formeln wird die abweichende Reihenfolge [], [( )], [{( )}], [({( )})], [{({( )})}] verwendet.

Bei unvollständig isotop substituierten Verbindungen werden die Positionsziffern und Nuklidsymbole in runde Klammern gesetzt, zum Beispiel bei Dichlor(H)methan. Bei spezifischen, unselektiven, selektiv markierten, isotop defizitären oder angereicherten Verbindungen werden die Nuklidsymbole mit Multiplikationssubskripten in eckige Klammern gesetzt, zum Beispiel bei [C,H]Methan, ["def"C]Chloroform, [C]Chloroform.

Bei polycyclischen Kohlenwasserstoffen werden die Verschmelzungspositionen von Teilstrukturen in eckigen Klammern dazwischengeschoben, zum Beispiel bei Benzo["a"]anthracen.

Bei verbrückten polycyclischen Kohlenwasserstoffe im Baeyer-System wird nach dem Begriff cyclo die Anzahl der C-Atome der beiden Zweige des Hauptringes, der Hauptbrücke und eventuell vorhandener Sekundärbrücken in absteigender Reihenfolge in eckigen Klammern angegeben, zum Beispiel Bicyclo[4.4.0]decan. Bei ungesättigten Brücken werden die Lokanten der Mehrfachbindungen in eckigen Klammern innerhalb des Brückenterms angegeben.

Bei Spiro-Kohlenwasserstoffen folgen dem Term Spiro die Summen der an das Spiroatom gebundenen C-Atome in eckigen Klammern, zum Beispiel Spiro[2.4]heptan. Muss man, um überhaupt eine Spiroverknüpfung realisieren zu können, erst eine formale (Di)-Hydrierung durchführen, wird der zusätzliche indizierte Wasserstoff direkt hinter der die Spiroverknüpfung betreffenden Ziffer in runde Klammern gesetzt.

Bei Oligosacchariden mit freier Halbacetal-Gruppe werden die Verknüpfungslokanten in runden Klammern zwischen die einzelnen Komponentennamen gesetzt, zum Beispiel β-D-Galactopyranosyl-(1→4)-α-D-glucopyranose.

Die einfachsten Stammsysteme sind lineare Ketten aus Kohlenstoffatomen, bei denen alle übrigen Bindungen mit Wasserstoffatomen gesättigt sind. Solche gesättigte Kohlenwasserstoffe nennt man Alkane, sie erhalten die Endung -an. Für die vier kleinsten Alkane werden die Namen Methan, Ethan, Propan und Butan beibehalten, für die übrigen Alkane ergibt sich der genaue Name der Verbindung nach der folgenden Tabelle aus der "Anzahl der Kohlenstoffatome". Man kombiniert das Zahlwort der ersten Dekade mit den Zahlwörtern für die folgenden Dekaden. Am Ende folgt ein n, sodass man die Alkan-typische Endung "-an" erhält.

Beispiele:

Ausnahmen von der Benennung nach der obigen Tabelle gibt es bei:
Falls eine Doppelbindung in der Verbindung vorhanden ist, spricht man von Alkenen und verwendet statt der Endung "-an" die Endung -en. Die Position der Doppelbindung wird durch eine Nummer angegeben siehe unten bei "Nummerierung", z. B.

Bei Ketten, die eine Dreifachbindung enthalten (= Alkinen), wird die Endung -in verwendet, z. B.

Falls mehrere Doppel- oder Dreifachbindungen vorkommen, verwendet man die multiplizierenden Vorsilben di, tri, tetra, penta, hexa, hepta, …

Die Hauptkette (Stammsystem) ist jene Kette, welche

Hinweis zum "Lokantensatz": Ein Lokantensatz ist die Aufzählung der Lokanten wie z. B. 2,4 im 2,4-Dimethyl-heptan. Der „niedrigste Lokantensatz“ "bedeutet nun nicht" die kleinste Summe der Lokanten, vielmehr vergleicht man die Lokanten der Reihe nach. Der kleinste Lokantensatz ist der, der an der ersten unterscheidbaren Stelle den kleineren Lokanten aufweist.

Bei cyclischen Systemen ist im Allgemeinen ein Cyclus das Stammsystem.

Falls es sich um eine monocyclische Verbindung handelt, erfolgt die Benennung wie bei linearen Ketten, und zusätzlich wird die Vorsilbe Cyclo- vorangestellt, also z. B. "Cyclo"hexan. Für Benzol wird der Trivialname beibehalten. Monocyclische Verbindungen mit mehr als sechs C-Atomen, die die maximale Anzahl nichtkumulierter Doppelbindungen aufweisen, können als (n)-Annulene bezeichnet werden (n = Anzahl der C-Atome). 
Cyclische Systeme werden bevorzugt nach dem Hantzsch-Widman-System bezeichnet.

Bei "kondensierten polycyclischen Kohlenwasserstoffen" (d. h. die einzelnen Ringe sind jeweils über genau eine gemeinsame Bindung verknüpft) ist jene Komponente das Basissystem, welche
Dabei werden folgende Polycyclen als eigene Systeme aufgefasst (in ansteigender Priorität, in Klammern die Anzahl der Ringe): Pentalen (2), Inden (2), Naphthalin (2), Azulen (2), Heptalen (2), Biphenylen (3), as-Indacen (3), s-Indacen (3), Acenaphthylen (3), Fluoren (3), Phenalen (3), Phenanthren (3), Anthracen (3), Fluoranthen (4), Acephenanthrylen (4), Aceanthrylen (4), Triphenylen (4), Pyren (4), Chrysen (4), Naphthacen (4), Pleiaden (4), Picen (5), Perylen (5), Pentaphen (5), Pentacen (5), Tetraphenylen (5), Hexaphen (6), Hexacen (6), Rubicen (7), Coronen (7), Trinaphthylen (7), Heptaphen (7), Heptacen (7), Pyranthren (8), Ovalen (10).

Alle übrigen Ringe werden als Vorsilben vorangestellt, wobei die Endsilbe "-en" in -eno umgewandelt wird (z. B. "Benzo"cycloocten). Die Art der Verknüpfung wird durch Zahlen und Buchstaben angegeben, was aber hier nicht näher erläutert werden soll.

Zur Benennung von gesättigten oder teilweise gesättigten Derivaten der oben angeführten Polycyclen gibt es die Möglichkeit, beim Wegfallen einer Doppelbindung die beiden zusätzlichen Wasserstoffatome durch die Positionsnummern und die Vorsilbe dihydro- anzuzeigen. Analog gibt es tetrahydro-, hexahydro- usw. Vollständig gesättigte Systeme erhalten die Vorsilbe perhydro-. Einzelne Wasserstoffatome werden durch das sogenannte indizierte H angegeben, welches in kursiver Schrift vorangestellt wird (z. B. 4"H"-Pyrazol).

Cyclophane können nach den gleichen Regeln benannt werden, obwohl es für diese auch eine eigene Nomenklatur gibt.

Bei "verbrückten polycyclischen Kohlenwasserstoffen" (d. h. die einzelnen Ringe sind jeweils über mehr als eine gemeinsame Bindung verknüpft) wird das von-Baeyer-System verwendet.

In Spiroverbindungen sind die Ringe über ein gemeinsames Atom verbunden.
Nomenklatur: "Substituenten"-spiro["Anzahl Atome im kleineren Ring" . "Anzahl Atome im grössern Ring"]Stammname (Ringgrösse wird ohne Spiro-Atom angegeben). Beispiel: 1-Brom-3-chlor-spiro[4.5]decan-7-ol.

Die Entscheidung, was nun als Stammsystem betrachtet wird, ist bei komplizierteren Verbindungen nicht mehr ganz einfach.

Sofern keine Trivialnamen vorliegen, benennt man monocyclische Heterocyclen mit bis zu 10 Ringgliedern meist nach dem Hantzsch-Widman-System.

Bei kondensierten Polycyclen haben Heterocyclen Vorrang gegenüber Carbocyclen (= Ringen, die nur aus Kohlenstoffatomen bestehen). Auch für Heterocyclen gibt es dabei Systeme mit Trivialnamen, welche als eigene Stammsysteme aufgefasst werden (ohne Reihung und unvollständig):

Ansonsten folgt die Benennung von Heterocyclen weitgehend den oben angeführten Regeln für cyclische Systeme ohne Heteroatome. Die Art und Position der Heteroatome wird dann mit Hilfe der Austauschnomenklatur oder „a“-Nomenklatur angegeben.

Viele Einzelverbindungen und Stoffgruppen enthalten die Endung -idin (z. B. Pyrrolidin und Anisidin). Dabei handelt es sich in den meisten Fällen um aromatische Verbindungen, die Stickstoff enthalten. Die Namensgebung folgt jedoch keiner durchgehenden Systematik.

Ein Substituent kann z. B. eine funktionelle Gruppe sein, oder wiederum ein (kleineres) Stammsystem, etwa eine Seitenkette. Die Bezeichnungen für Substituenten werden dem Namen des Stammsystems als Vorsilben (Präfixe) oder Endungen (Suffixe) angefügt. Die genaue Position des Substituenten wird durch Ziffern präzisiert (siehe unten bei "Nummerierung").

Falls es mehrere Vorsilben (Präfixe) gibt, werden diese in alphabetischer Reihenfolge aufgelistet.

Falls es sich beim Rest wiederum um ein Stammsystem handelt, zum Beispiel eine Seitenkette oder einen Ring, so wird an dessen Namen die Silbe -yl angehängt und das Ergebnis als Vorsilbe (Präfix) vorangestellt. Die Benennung von Seitenketten erfolgt nach den gleichen Regeln wie für die Grundkette, bis auf folgende Ausnahmen:

Beispiele:
Die Namen Element-Wasserstoff-Gruppen werden analog aus der Stammverbindung gebildet:

Wenn man beispielsweise an die Verbindung Propan (CH-CH-CH) in der Mitte noch einen Methanbaustein anhängt, heißt die entstehende Verbindung CH-CH("CH")-CH dann "2-Methyl"propan. Die Verbindung CH-CH-CH("CH")-CH-CH("CHCH")-CH-CH heißt "3-Ethyl"-"5-methyl"heptan.

Seitenketten mit Doppelverbindung zur Grundkette erhalten die Endung -ylen (Methylen: =CH), bei einer Dreifachverbindung -ylidin (Methylidin: ≡CH).

Die ranghöchste funktionelle Gruppe wird als Endung (Suffix) hinten angestellt, übrige funktionelle Gruppen als Vorsilben (Präfixe) vorangestellt:


Für die Bezeichnungen einzelner funktioneller Gruppen und ihre Rangfolge siehe das Stichwort Funktionelle Gruppe.

Für manche Substituenten gibt es Trivialnamen, welche z. T. auch verbindlich sind, wie z. B.:

Die Nummerierung des Stammsystems erfolgt so, dass die erhaltenen Nummern möglichst klein sind. CH-CH-CH-CH(CH)-CH heißt also 2-Methylpentan und nicht 4-Methylpentan.

Falls es nur eine mögliche Kombination gibt, können die Nummern weggelassen werden (z. B. 2-Methylpropan = Methylpropan, da es kein anderes Methylpropan gibt).

Falls Seitenketten nummeriert werden müssen, ist die Verbindungsstelle zur Hauptkette immer die Position 1.

Für natürlich vorkommende Derivate des Glycerins gilt für die Nummerierung der C-Atome nach IUPAC die sn-Nomenklatur.

Bei kondensierten polycyclischen Systemen existieren ggf. verbindliche Nummerierungsschemata, die jeweils nachgeschlagen werden müssen (siehe z. B. Steran-Grundgerüst).

Die Positionsnummern werden Lokanten genannt.

Für mehrfach vorkommende gleiche Gruppen werden die "multiplizierenden Vorsilben" di, tri, tetra, penta, hexa, hepta, … (siehe oben) verwendet:

Falls die Verwendung von di, tri, tetra usw. missverständlich wäre, etwa bei identischen weitersubstituierten Seitenketten, muss man wie oben beschrieben die entsprechenden alternativen Vorsilben bis, tris, tetrakis usw. verwenden. Für direkt verknüpfte identische Einheiten sind die Vorsilben bi, ter, quater usw. in Verwendung.

Nach IUPAC-Nomenklatur muss zum Beispiel die Verbindung

NH-CH-CH-OH

den Namen 2-Aminoethanol erhalten.

Auf folgende Weise gelangt man zu diesem Namen:


Zur Unterscheidung von chiralen Verbindungen gibt es die kursiv geschriebenen Vorsilben ("R")- und ("S")-. Ihre Verwendung wird durch die Cahn-Ingold-Prelog-Regel (CIP-Regel) und ihre Nebenregeln festgelegt. Wenn eine chirale Verbindung als 1:1.Gemisch der Enantiomere vorliegt – also ein Racemat ist – benutzt man die Vorsilbe ("RS")-. Wenn ein Stereozentrum in einer chiralen Verbindung einheitlich vorliegt, aber aus irgendwelchen Gründen die Konfiguration ("R")- "oder" ("S")- unklar ist, benutzt man als Vorsilbe (Ξ)- (griechischer Buchstabe Xi).

Bei biochemischen Substanzen wie Kohlenhydraten und Aminosäuren wird auch noch häufig die Fischer-Nomenklatur verwendet, welche die Vorsilben - und - verwendet (wobei und als Kapitälchen geschrieben werden).

Zur Unterscheidung des Drehsinns bei optisch aktiven Verbindungen verwendet man die Vorsilben (+)- und (−)-, wobei kein Zusammenhang zwischen der optischen Aktivität (Drehsinn) und der „Richtung“ Chiralität besteht.

Es sei darauf hingewiesen, dass sich die unterschiedlichen Bezeichnungsweisen ("R", "S" bzw. , und +, −) nach den verschiedenen Nomenklaturarten "nicht" von den jeweils anderen Bezeichnungen ableiten lassen. Zur systematischen Bezeichnung von Verbindungen mit mehreren Chiralitätszentren eignen sich nur die CIP-Regeln, wobei die Fischer-Nomenklatur beispielsweise für Zucker wesentlich kompakter ist.

Bei der "cis"-"trans"-Isomerie unterscheidet man bei der Nomenklatur zwischen Verbindungen, die nur zwei verschiedene Substituenten haben, und Verbindungen mit mehr als zwei. Erstere werden mit den kursiv geschriebenen Vorsilben cis"- oder trans"- gekennzeichnet. cis"-Doppelbindungen werden meist – aber nicht durchgängig – nach IUPAC mit einem vorangestellten, kursiven ("Z") („Zusammen“) und trans"-Doppelbindungen mit einem ("E") („Entgegengesetzt“) gekennzeichnet. Genau genommen stehen bei einem ("Z")-Isomer jene zwei Substituenten an benachbarten Atomen einer Doppelbindung auf derselben Seite des Moleküls, die die höchste Priorität im Cahn-Ingold-Prelog-System haben, beim ("E")-Isomer stehen die Substituenten mit der höchsten CIP-Priorität also auf entgegengesetzten Seiten des Moleküls.

Links ist das "trans"-1,2-Dibromethen, rechts die "cis"-Version dargestellt. Auch kann man hier die ("E","Z")-Nomenklatur anwenden,
bezeichnet.

Auch hier am Ring sind die beiden Bromatome in "trans"- (links) und "cis"-Stellung („Zusammen“ auf einer Seite) dargestellt.

Das ist ein ("Z")-3-Methylpent-2-en, da die ranghöheren Substituenten (siehe Stereochemie) auf einer Seite liegen.

Bei Kohlenhydraten unterscheidet man Anomere durch die kursiven Vorsilben α"- bzw. β"-.

Für die Nomenklatur von Enzymen gibt es gemeinsame Richtlinien der IUPAC und der IUBMB (International Union of Biochemistry and Molecular Biology). Nach dieser Nomenklatur enden Enzymnamen mit -ase und enthalten eine Information über die Funktion des Enzyms. Details unter dem Stichwort Enzym und auf der Website der IUBMB.

Außerdem wurde ein Codesystem ("siehe" EC-Nummern) entwickelt, in dem die Enzyme unter einem Zahlencode aus vier Ziffern zu finden sind.

Für Nukleinsäuren gilt die Nukleinsäure-Nomenklatur.

Die alternative N-X-L-Nomenklatur dient der Bezeichnung hypervalenter Verbindungen. Für den praktischen Umgang mit chemischen Stoffen im Alltag gibt es zudem Normen, Nummernsysteme und Stoffdatenbanken unterschiedlicher Ausrichtung:





</doc>
<doc id="13624" url="https://de.wikipedia.org/wiki?curid=13624" title="International Union of Pure and Applied Chemistry">
International Union of Pure and Applied Chemistry

Die International Union of Pure and Applied Chemistry (IUPAC; ) wurde im Jahr 1919 von Chemikern aus der Industrie und von Universitäten gegründet. Ziel war es, die weltweite Kommunikation der Chemiker untereinander zu ermöglichen und zu fördern. Die IUPAC ist seit langem als die bestimmende Institution anerkannt, wenn es sich um Empfehlungen zu Nomenklatur, Symbolen, Terminologie, standardisierten Messmethoden, Werten für molare Massen der chemischen Elemente in natürlicher Isotopengemisch-Zusammensetzung und viele andere Themen in Bereichen der Chemie handelt. Die Vereinigung gibt die Zeitschrift Pure and Applied Chemistry heraus.

Einige Mitglieder engagieren sich ehrenamtlich im Rahmen von Projekten für die IUPAC, die sich in die folgenden acht Abteilungen gliedern:

Unter den Chemikern bestand bereits lange vor der Gründung der Wunsch, die internationale Zusammenarbeit zu fördern. So gab es schon eine Vorläufer-Organisation, die "International Association of Chemical Societies (IACS)", die sich 1911 in Paris traf und sich unter anderem bereits um Fragen der Nomenklatur und der Standardisierung in der Chemie kümmern sollte.
Versuche, die chemische Nomenklatur zu standardisieren, begannen allerdings schon 1860, als Friedrich August Kekulé von Stradonitz die ersten internationalen Treffen organisierte, die schließlich 1892 zur sogenannten Genfer Nomenklatur für organisch-chemische Verbindungen führten.

Präsident ist Qi-Feng Zhou. Er löste Natalia Tarasova ab.




</doc>
<doc id="13625" url="https://de.wikipedia.org/wiki?curid=13625" title="MIT (Begriffsklärung)">
MIT (Begriffsklärung)

MIT steht als Abkürzung für:

Einrichtungen:

chemische Verbindungen:
Siehe auch:


</doc>
<doc id="13626" url="https://de.wikipedia.org/wiki?curid=13626" title="Massachusetts Institute of Technology">
Massachusetts Institute of Technology

Das Massachusetts Institute of Technology (MIT, ) ist eine Technische Hochschule und Universität in Cambridge, Massachusetts, in den USA. Die Hochschule liegt am Charles River in Cambridge, direkt gegenüber von Boston und stromabwärts von der Harvard University. Das MIT wurde 1861 gegründet und ist eine private, nicht-konfessionelle technische Universität, die als erste Chemie-Ingenieure ausbildete und die Wirtschafts-, Sozial- und Geisteswissenschaften in die Ingenieurausbildung einbezog. Derzeit studieren am MIT über 10.000 Studenten. 

Das MIT rühmt sich für das hohe Niveau der Ausbildung, wobei die Studenten schon früh in die Forschungsaktivitäten eingebunden werden. Es gilt als eine der weltweit führenden Eliteuniversitäten und erreicht in internationalen Vergleichen regelmäßig einen Spitzenplatz. Die Hochschule ist Mitglied der Association of American Universities, einem seit 1900 bestehenden Verbund führender forschungsintensiver nordamerikanischer Universitäten.

Rund um das MIT hat sich ein Netz aus Hochtechnologie-Kleinfirmen angesiedelt: In den späten 1990er Jahren war Risikokapital im Überfluss vorhanden, sodass der bevorzugte Karrierewunsch vieler Studenten darin bestand, eine Hightech-Startup-Firma zu gründen.

Die Hochschule ist zudem Gründungsorganisation und Sitz des World Wide Web Consortium (W3C), dem Standardisierungsgremium für das World Wide Web. Seit 2002 macht das MIT sukzessive seine gesamten Kursunterlagen über das Internet öffentlich zugänglich und unterstützt damit die OpenCourseWare. Alleine im MIT-OpenCourseWare-Projekt wurden auf diese Weise fast 2000 Kurse in 33 Fächern verfügbar gemacht.

Gegründet wurde das MIT nach dem Vorbild deutsch- und französischsprachiger polytechnischer Hochschulen am 10. April 1861 als dreigliedrige Einrichtung, bestehend aus „a society of arts, a museum of arts [industrial arts], and a school of industrial science.“ Der Gründer William Barton Rogers, ein bekannter Naturforscher, wollte eine unabhängige Universität schaffen, mit Ausrichtung auf die Erfordernisse eines zunehmend industrialisierten Amerika.
Wegen des Bürgerkrieges konnten die ersten Studenten erst 1865 aufgenommen werden. In den Folgejahren erlangte das MIT einen erstklassigen Ruf.

Wegen der andauernden Finanzierungslücken wurde um 1900 ein Zusammenschluss mit der benachbarten Harvard-Universität geplant. Dies konnte jedoch wegen massiver Proteste ehemaliger MIT-Studenten nicht durchgesetzt werden. 1916 wurde der Campus von Boston nach Cambridge am gegenüberliegenden Flussufer verlegt.

Nach dem Zweiten Weltkrieg, in dem das MIT zur Entwicklung der Radartechnik beitrug, stieg das Ansehen des MIT weiter an. Das Wettrüsten und die Raumfahrt in der Zeit des Kalten Krieges erzeugten eine staatlich geförderte Nachfrage nach Hochtechnologie. Bekannt wurde das MIT auch durch den Digitalrechner Whirlwind, der dort von 1944 bis 1952 unter der Leitung von Computerpionier Jay Wright Forrester entwickelt und gebaut wurde.

Weitere Entwicklungen aus den MIT-Labors der Nachkriegszeit waren der Ferrit-Kernspeicher sowie die automatische Raumsondensteuerung des Apollo-Programms. Seit dem Aufkommen des Personal Computers hat das MIT auch eine zentrale Rolle in den Schlüsseltechnologien des Informationszeitalters besetzt.

2001 konstatierte MIT-Präsident Charles Marstiller Vest, dass das MIT als Institution die Karriere von weiblichen Fakultätsmitgliedern und Forschern in diskriminierender Weise behindert habe. Er kündigte organisatorische Schritte zur Gleichstellung der Geschlechter an. Am 6. Dezember 2004 trat Susan Hockfield, eine Wissenschaftlerin auf dem Gebiet der molekularen Neurobiochemie, nach 15 männlichen Präsidenten als erste Präsidentin dieses Amt an. Im Juli 2012 folgte ihr Leo Rafael Reif im Amt.

Am 18. April 2013 wurde vor dem Stata Center ein Polizist der Campus-Polizei von Tamerlan und Dschochar Zarnajew erschossen, die drei Tage zuvor den Anschlag auf den Boston-Marathon verübt hatten.

Der Campus des MIT liegt in Cambridge unweit der Harvard University direkt am Charles River mit Blick auf die Skyline von Boston. Am östlichen Ende grenzt er an die Longfellow Bridge, welche nach Boston führt.

Aus den Gründerjahren des MIT sind die »Maclaurin Buildings« zu erwähnen, die den Eingangsbereich zum Campus bilden. Sie wurden von Welles Bosworth im neoklassizistischen Stil der Zeit entworfen. Fertiggestellt wurden sie 1916. Kennzeichnend sind die großzügige Lichtführung und besonders der zentrale Kuppelbau.

Die Friese tragen die Namen großer Naturforscher, zum Beispiel Aristoteles, Isaac Newton, Benjamin Franklin, Louis Pasteur, Antoine Lavoisier, Michael Faraday, Archimedes, Leonardo da Vinci, Charles Darwin und Nikolaus Kopernikus, jeweils umgeben von Gruppen mit Namen passender Forscher, die zum jeweiligen Forschungsgebiet der „Großen“ beigetragen haben.

Lavoisier zum Beispiel befindet sich im Kreise von Boyle, Cavendish, Priestley, Dalton, Gay-Lussac, Berzelius, Woehler, Liebig, Bunsen, Mendelejew, Perkin und van’t Hoff.

Späteren Gebäuden aus den Jahren 1950 bis 1970 mangelt es dagegen an Ausstrahlung, auch wenn einige von ihnen vom MIT-Absolventen I. M. Pei entworfen wurden, so das "Green Building" (Hauptgebäude der Fakultät für Geo-, Atmosphären- und Planetenphysik), das Institut für Chemische Verfahrenstechnik als höchstes Gebäude auf dem Campus und das Wiesner Building, in dem das MIT Media Lab untergebracht ist.

Ein umfangreiches Bauprogramm in jüngster Zeit umfasste das »Stata Center«, entworfen von Frank Gehry und 2004 fertiggestellt, das Simmons Hall Studentenwohnheim, entworfen von Steven Holl, das Zeisiger-Sportzentrum und ein neues Gebäude, entworfen von Charles Correa, für das »Picower Center for Learning and Memory«, das »Institute for Brain and Cognitive Science« und das »McGovern Institute for Brain Research«.

Für das »Stata Center« musste 1998 ein altes Gebäude weichen, das im Zweiten Weltkrieg als Provisorium errichtet wurde. Eigentlich sollte es spätestens sechs Monate nach Kriegsende abgerissen werden, aber es erwies sich trotz seiner Hässlichkeit lange Jahre als eine Brutstätte für kreative Projekte.

Zu den neuesten und architektonisch interessanten Gebäuden gehören:

Das MIT verfügt über fünf Fakultäten, "Schools" genannt:

Das Lernpensum am MIT ist in den unteren Semestern sehr groß. Dennoch ist die Quote erfolgreicher Kursabschlüsse hoch. Dies erklärt sich aus der antiautoritären Kultur und dem Paradigma, dass erworbenes Wissen geteilt werden muss. In der Praxis heißt das, dass ältere Studenten und Professoren den jüngeren hilfreich zur Seite stehen.

Regelmäßig während des Semesters finden schriftliche Prüfungen statt. Dabei wird weniger konkretes Wissen abgefragt als vielmehr die Fähigkeit der Studenten überprüft, komplexe Probleme zu lösen. So gibt es kaum Multiple-Choice-Tests, die Arbeitsergebnisse sind frei zu formulieren. Die Analyse und Korrektur dieser Tests ist dementsprechend aufwändiger.

Es werden auch praktische Aufgaben zur Lösung gestellt. Die Studenten bekommen eine Konstruktionsaufgabe und wetteifern um den besten Entwurf zur Lösung.

Im Rahmen des "Undergrade Research Opportunities Program" (UROP) werden bereits niedrige Semester in die Forschungsaktivitäten ihres Instituts eingebunden. Die praktischen Arbeiten hierzu finden größtenteils am Freitagnachmittag und am Wochenende statt, wenn der normale Lehrbetrieb ruht.

Seit 2007 werden Vorlesungen auch über das Internet übertragen. Der 71-jährige niederländische Physiker Walter Lewin – Physikdozent am MIT – „ist mittlerweile zum Star im Internet avanciert.“

Neben dem Schwerpunkt Naturwissenschaften und Technik hat das MIT auch Institute für Philosophie, Volkswirtschaft, Betriebswirtschaft, Linguistik und Anthropologie.


Unter den bekanntesten Forschungsinstituten sind zu nennen:

Es gibt einige traditionelle Kooperationspartner:

Heutzutage gibt es abgestimmte Studienpläne für untere Semester, so dass Studenten Kurse am MIT und an Harvard miteinander kombinieren können. Das Gleiche gilt auch für die Studentinnen am Wellesley College, einer traditionsreichen Hochschule nur für weibliche Studenten.

Seit einigen Jahren geht das MIT verstärkt Partnerschaften mit diversen Universitäten sowie öffentlich und privat finanzierten Forschungslabors ein. Die Projekte sind im Allgemeinen vom externen Partner zu finanzieren und dienen somit der Geldmittelbeschaffung des MIT. Das MIT als privatwirtschaftliche Organisation vermarktet auf diese Weise sein Renommee und die Forschungskapazität der Fakultätsmitglieder und Studenten.

Der Universitätsverlag MIT Press ist seit 1962 ein von der Universität unabhängiges Unternehmen, das für die Universität publiziert. Der Verlag publiziert ungefähr 200 Bücher und 40 wissenschaftliche Journale pro Jahr.

Von den 10.206 Studenten sind ungefähr 4000 „Undergraduates“ und 6000 „Graduates“. 43 % der Studentenschaft sind weiblich (29 % der Graduates). Es gibt Studenten aus allen 50 US-Bundesstaaten und aus 110 verschiedenen Ländern. 9 % der „Undergraduates“ und 40 % der „Graduates“ sind ausländische Studenten.

45 % der „Undergraduates“ (17 % der „Graduates“) gehören einer amerikanischen Minderheit an:

Herkunft der internationalen Studenten (2006):

Die Studiengebühren lagen 2010–2011 bei 39.212 US$

Die Studenten sind mit hohen Anforderungen konfrontiert, allerdings zum überwiegenden Teil hoch motiviert. Das MIT hat auf Vorwürfe reagiert, die hohen Anforderungen würden Studenten sogar bis in den Suizid treiben: Eine intensivere psychologische Betreuung soll die Situation verbessern.

Wie an vielen amerikanischen Universitäten leben die Studenten meist recht beengt in Wohnheimen auf dem Campus. Es gibt traditionell deutliche Unterschiede zwischen verschiedenen Wohnheimen: In einigen ist die Hippie-Kultur präsent, während andere einen betont bürgerlichen Stil pflegen.

Für eine nicht musisch geprägte Universität ist der Anteil aktiv musizierender Studenten recht hoch: Es gibt ein Symphonieorchester und mehrere klassische Chöre mit studentischen Musikern.

Das Ethos des MIT ist ausgeprägt antiautoritär, gleichzeitig glaubt man an den Sinn einer Leistungselite, deren sozialer Status sich aus intellektueller Überlegenheit heraus rechtfertigt, nicht aus der Herkunft oder formaler hierarchischer Positionen.

Am MIT ist es Usus, dass Informationen offengelegt werden sollen und nicht verdeckt werden dürfen. Jede Behauptung ist möglicher Gegenstand einer kritischen Überprüfung, ihre Akzeptanz darf sich nicht auf allgemein gültige Ansicht oder Direktiven „von oben“ berufen.

Diese ethische Einstellung wird auch bei vielen Hackern angenommen: Am MIT wird der Begriff "Hack" weit definiert, im Sinne eines überraschenden technisch-ausgefeilten Kunstgriffs, wobei die besten Hacks einen humoristischen Aspekt besitzen. Der Begriff "Hacker" wurde am MIT geprägt, einige Wurzeln der Hacker-Kultur lassen sich zum MIT der 50er und 60er Jahre zurückverfolgen: Am MIT arbeiteten Urväter der Hacker-Szene wie Richard Stallman, Jay Sussman und Tom Knight.

Die Sportteams des MIT sind die "MIT Engineers". Die Hochschule ist Mitglied in der "New England Women’s and Men’s Athletic Conference" und nehmen damit an der dritten Division der National Collegiate Athletic Association teil.


siehe: , u. a.:





</doc>
<doc id="13627" url="https://de.wikipedia.org/wiki?curid=13627" title="Leo XIII.">
Leo XIII.

Papst Leo XIII. ("Vincenzo Gioacchino Pecci"; * 2. März 1810 in Carpineto Romano; † 20. Juli 1903 in Rom) war von 1878 bis 1903 Papst der römisch-katholischen Kirche.

Papst Leo XIII. ist als politischer Papst in die Geschichte eingegangen. Die von vielen gefürchtete Dogmenhäufung nach der Unfehlbarkeitserklärung aus dem Jahr 1870 unter Papst Pius IX. blieb aus. Wohl aber kann man Leo XIII. den ersten Enzyklikenpapst nennen, er verfasste 86 dieser päpstlichen Rundschreiben, darunter sieben zur Marienverehrung. Sein Ziel war es, die Kirche aus ihrer selbst gewählten Isolierung gegenüber den neuzeitlichen gesellschaftlichen und politischen Entwicklungen herauszuführen, jedoch war er von der Notwendigkeit einer „zeitlichen Macht“ (Kirchenstaat) des Papstes überzeugt. Einerseits orientierte er sich an der hochmittelalterlichen Ordnung von Kirche und Staat, andererseits verfasste er die erste explizite Sozialenzyklika der römisch-katholischen Kirche und wertete damit die katholische Soziallehre auf. Wegen seiner Anteilnahme an sozialen Fragen wurde er mit dem Attribut „Arbeiterpapst“ und dem Beinamen „der Soziale“ bekannt.

Leo XIII. starb im Alter von 93 Jahren, kein Papst wurde bisher älter. Zudem war mit ihm zum ersten Mal ein Camerlengo (Kardinalkämmerer) zum Papst gewählt worden.

Gioacchino Pecci entstammte dem niederen Landadel. Sein Vater war Kriegskommissar und Oberst. 

Bereits als Junge galt er als hochbegabt und entwickelte eine Vorliebe für das Lateinische. Er studierte von 1818 bis 1824 am Jesuitenkolleg in Viterbo, von 1824 bis 1832 folgte das Theologiestudium am Collegium Romanum. Die Ausbildung für den päpstlichen Verwaltungs- und Diplomatendienst an der Accademia dei Nobili Ecclesiastici in Rom dauerte von 1832 bis 1837. Pecci promovierte 1835 zum Doktor beider Rechte (Dr. iur. utr.).

1837 empfing der Kirchenjurist und Theologe durch Kardinalvikar Carlo Odescalchi die Priesterweihe. Bereits von 1838 bis 1841 war er päpstlicher Delegat in Benevent, danach in der gleichen Funktion in Perugia.

1843 ernannte ihn Gregor XVI. zum Titularerzbischof von "Tamiathis" und sandte ihn als Nuntius nach Belgien, von wo er allerdings auf Wunsch des Königs wieder abberufen wurde. Von 1846 bis 1878 war er Bischof von Perugia, wo er den Dom im neugotischen Stil umgestalten ließ. Am 19. Dezember 1853 wurde er zum Kardinal mit der Titelkirche San Crisogono erhoben. Er vertrat dort zunächst eine streng konservative und wissenschaftsfeindliche Linie. Zur Stärkung der Traditionen sollte seine Reform des Theologiestudiums dienen.

In der Umbruchphase der Loslösung Umbriens vom Kirchenstaat (1860) war Pecci Anführer der Bischöfe gegen das italienische Staatskirchentum. Mitte der 1870er Jahre öffnete er sich allmählich gegenüber der modernen Kultur und Technik. Nach dem Tod des bisherigen Camerlengos Filippo de Angelis wurde er 1877 zum Nachfolger ernannt. In dieser Funktion führte Kardinal Pecci die Amtsgeschäfte während der Sedisvakanz im Jahr 1878. Er wurde, gegen den bisherigen Brauch, als Kandidat der Gemäßigten am 20. Februar nach zweitägigem Konklave, dem ersten in der Sixtinischen Kapelle, zum Nachfolger von Pius IX. gewählt.

Die Krönung Leos XIII. erfolgte am 3. März 1878 in der Sixtinischen Kapelle. Seine angegriffene Gesundheit ließ ein eher kurzes Übergangspontifikat erwarten.

Seine Wahl des Papstnamens Leo war ein Zeichen der Verehrung für Leo XII., aber auch ein Signal für den von ihm angestrebten Wandel in der Stellung des Papsttums.

Ohne einschneidend mit der Politik seiner Vorgänger zu brechen, erstrebte er als Antwort auf die Nöte seiner Zeit die Restauration der von ihm als vorbildlich erachteten hochmittelalterlichen Ordnung von Kirche und Staat. Dabei stand die Reform des Theologiestudiums mit ihrer Orientierung an Thomas von Aquin an erster Stelle. Sein persönliches Vorbild war Innozenz III. (1198–1216). 1891 ließ er dessen Leichnam nach Rom überführen und ihn in San Giovanni in Laterano beisetzen.

Die Hinwendung zum Mittelalter fand ihren symbolischen Ausdruck im Kirchenbau jener Zeit; vornehmlich wurden neogotische Gotteshäuser errichtet. Dieses eher rückwärts gewandte Programm Leos XIII. war insofern zum Scheitern verurteilt, als er die unwiderruflichen Folgen des gesellschaftlichen Wandels im 19. Jahrhundert nicht beachtete. Mit einer Negation der Europäischen Revolutionen von 1848/49 seien ihre Folgen nicht aus der Welt zu schaffen. Das nachrevolutionäre Europa verurteilte außerdem seine Konzeption eines universalen Papsttums mit geistlichem Führungsanspruch.

Die berühmte Enzyklika "Rerum Novarum" (dt.: "Geist der Neuerung") 1891 begründete den Ruf Leos XIII. als "„Arbeiterpapst“." Er prangerte die Ausbeutung der Arbeiter an und wies auf ihre Verelendung infolge der Industrialisierung hin. Zudem beschrieb er deren negative Auswirkungen auf Wirtschaft und Staat und zeigte einen Weg zur Besserung der Verhältnisse auf. Gleichzeitig wandte er sich gegen den Sozialismus als Ausweg aus der Misere und befürwortete das Privateigentum. Der Papst entwickelte mit dieser Enzyklika eine Lehre von der menschlichen Person und ihren Rechten, von der Ordnung der Wirtschaft, von der Koalitionsfreiheit der Arbeiter und der sozialen Verpflichtung des Staates. Arbeitsschutz sei eine staatliche Aufgabe, ebenso der gesetzliche Rahmen für die Arbeiterrechte. Seitdem kann man von einer lehramtlich fundierten kirchlichen Soziallehre sprechen. Diese Enzyklika wird als die „Mutter aller Sozialenzykliken“ betrachtet; die nachfolgenden Päpste bezogen sich darauf mit „Fortentwicklungsenzykliken“. Laut André Habisch, Professor für Wirtschafts- und Sozialethik an der Katholischen Universität Eichstätt-Ingolstadt, ist die Enzyklika bis in die Gegenwart prägend für die deutsche Wirtschaftsordnung.

In der Auseinandersetzung mit dem italienischen Staat untersagte der Papst den Katholiken die parlamentarische Mitwirkung. Durch Zugeständnisse an Bismarck (gegen den Willen der Zentrumspartei) beendete Leo XIII. den Kulturkampf. Er forderte die französischen Katholiken zum Frieden auf, denn er wollte seine Kräfte auf die Auseinandersetzung mit Italien konzentrieren, was letztlich scheiterte.

Bei der Beilegung der Streitigkeiten mit der Schweiz und den lateinamerikanischen Ländern war er erfolgreicher. Er knüpfte engere Kontakte zu den USA und Russland, die Beziehungen zu England und Spanien verbesserten sich. Die Vermittlung im Konflikt um die Karolinen (1885), ein geschickter taktischer Zug Bismarcks, wertete zusätzlich die internationale Stellung des Papstes auf. Das von ihm angestrebte Maß von politischer Mitsprache und Einflussnahme konnte er jedoch nicht erreichen.

Sein Engagement für eine Beendigung der Schismen zu den Orthodoxen Kirchen und zu den Anglikanern hatte keinen Erfolg, da er von ihnen forderte, dass sie seinen Primat anerkennen und die Anglikaner ihre Weiheriten für ungültig erklären sollten.

Leo XIII. forderte mit dem Apostolischen Schreiben "Orientalium dignitas" (1894) von den anderen Kirchen, dass sie sich in der Frage des Primats dem Papst unterwerfen sollten.

Die apostolische Bulle "Apostolicae curae" erklärt die Weihe von Diakonen, Bischöfen und Priestern in den Anglikanischen Kirchen (einschließlich der Church of England) für ungültig. Gleichzeitig erkannte er die Weihen der Orthodoxen und Orientalischen Kirchen an. Die Freimaurerei verurteilte er in derselben Bulle.

Im Bewusstsein von der universalen Stellung des Papsttums verstärkte Leo XIII. den römischen Zentralismus. Die Bischöfe erhielten häufig genaue Instruktionen, und päpstliche Interventionen in den einzelnen Ländern wurden immer häufiger. Dazu wurde die Stellung der Nuntien gegenüber den Bischöfen gestärkt. Auch die zunehmenden Pilgerfahrten nach Rom und Reformen in der Organisationsstruktur der Orden (z. B. Franziskaner, Benediktiner) dienten zur Verstärkung der Bindungen von Klerus und Laien an den Hl. Stuhl.

Unter Leo XIII. wurde die Weltmission auf eine neue organisatorische Grundlage gestellt und ausgeweitet. Es gab gleichsam einen Globalisierungsschub bei den kirchlichen Strukturen: 48 Apostolische Vikariate und 248 Diözesen wurden neu eingerichtet.

Der Papst unterstützte die historische Forschung und öffnete 1881 das Vatikanische Archiv für Gelehrte aller Konfessionen. 1891 gründete er die vatikanische Sternwarte "Specola Vaticana" in der päpstlichen Sommerresidenz Castel Gandolfo, die einzige naturwissenschaftliche Forschungseinrichtung des Vatikans. In der Enzyklika Providentissimus Deus (1893) ermutigte Leo zum Bibelstudium und warnte gleichzeitig vor rationalistischen Interpretationen, die die Inspiration der Schrift leugneten.

1897 approbierte Leo XIII. außerdem die Gründung der Catholic University of America in Washington, D.C. Im apostolischen Brief "Testem Benevolentiae" (1899) verurteilte er die Häresie des Amerikanismus, einer theologischen Reformbewegung in den Vereinigten Staaten, da diese den Katholiken in den USA eine Anpassung an die Zivilreligion der US-amerikanischen Kultur vermitteln wollte. Diese Reformbewegung war aus päpstlicher Sicht zu liberal, da sie die Bedeutung von Glaubensinhalten zugunsten praktischen Verhaltens vernachlässige.

Am 20. April 1884 veröffentlichte Leo XIII. die Enzyklika "Humanum genus", der zufolge die Menschheit aus zwei gegnerischen Gruppen besteht; die eine kämpfe für Wahrheit und Tugend, die andere für Lüge und Laster. Die eine entspreche dem Reich Gottes auf Erden, der Kirche Jesu Christi, die andere dem Königreich Satans, das durch die Freimaurerei geleitet oder gefördert werde. Im Jahr 1885 wurde er in diesem Zusammenhang Opfer des Taxil-Schwindels. Spätere von ihm verfasste Schriften gegen die Freimaurerei waren "Dall’alto dell’Apostolico Seggio", "Custodi di quella fede" und "Inimica vis".

Leo XIII. förderte die Verehrung des Herzens Jesu, dem er am 11. Juni 1899 die gesamte Menschheit weihte. Er erläuterte diese Weihe in seiner Enzyklika Annum sacrum (deutsch: "Heiliges Jahr", bezogen auf das damals bevorstehende Jahr 1900).

Leo XIII betonte die Mittlerfunktion Marias und förderte insbesondere den Rosenkranz, dem er sieben Enzykliken widmete. „Gnade und Wahrheit (kämen) durch Jesus Christus“.() Nur durch Maria würden „die Gnaden aus diesem Schatz“ uns verliehen. Niemand könne „zum Vater im Himmel kommen als durch den Sohn“. Er fuhr fort: „so ähnlich kann niemand zu Christus kommen als durch seine Mutter.“ Hauptmotiv der Marienfrömmigkeit sei, dass Gläubige sich an Maria wendeten, weil sie „Gottes Gerechtigkeit“ fürchteten, denn Maria als „die Mutter des allmächtigen Gottes“ sei „ganz gütig, nachsichtig und barmherzig“. Jeder Gläubige solle die „Marienverehrung zu seiner liebsten und teuersten Angelegenheit machen“.

Mit der Marienverehrung Leos XIII. setzte sich Franz Graf-Stuhlhofer auseinander und äußerte Bedenken gegen das Gottesbild: „Hier wird Gott als zwar streng, aber glücklicherweise von der ihm zur Seite stehenden Maria leicht beeinflussbar dargestellt.“ Außerdem erscheine Maria „als die eigentliche Mittlerin zwischen Gott und den Menschen“. Es entstehe der Eindruck, wir „sollen uns an Maria wenden, die unsere Anliegen an Gott weiterleitet.“ 

Leo XIII. wurden nach seinem Tode als bisher letztem Papst sämtliche Organe entnommen. Sein Nachfolger, Pius X. (1903–1914), wünschte dies ausdrücklich nicht. Seitdem ist diese Praxis nicht mehr gängig. Allerdings flammte die Diskussion 2005 wieder auf, als Polen das Herz des verstorbenen Johannes Paul II. begehrte. Das Kardinalskollegium blieb bei der von Pius X. getroffenen Entscheidung und lehnte derartige Wünsche ab. Die Entnahme von Organen ist somit erst nach einer Kanonisation möglich. Diese werden dann als Reliquien verehrt.

Leo XIII. wurde zunächst im Petersdom bestattet. Sein heutiges Grab ist in der Basilika San Giovanni in Laterano.

Die 86 leoninischen Enzykliken und weitere apostolische Schreiben im chronologischen Überblick:



Papst Leo XIII. hat folgende Personen seliggesprochen:


Folgende Personen wurden von ihm heiliggesprochen:


Cyrill von Alexandrien wurde 1882 von Papst Leo XIII. zum Kirchenlehrer ernannt, 1890 folgte Johannes von Damaskus, den er außerdem zum Patron der Theologiestudenten des Ostens ausrief. Und 1893 erhob er auch Cyrill von Jerusalem zum Kirchenlehrer.

Bonaventura wurde von Papst Leo XIII. als „Fürst aller Mystiker“ bezeichnet.

Sowohl von den Zeitgenossen als auch von der Nachwelt wird das Pontifikat dieses Papstes kontrovers beurteilt. Dies betrifft vor allem die zum Teil recht widersprüchliche Haltung gegenüber den Erscheinungen der Moderne.

Gemessen an seinem eigenen Anspruch erreichte Leo XIII. seine Ziele nur in begrenztem Umfang. Die katholische Kirche ging mit neuer Orientierung aus seinem Pontifikat hervor.

Leo XIII. verhalf der Kirche an der Wende zum 20. Jahrhundert zu neuem Selbstvertrauen. Es war mit sein Verdienst, dass die Kirche in den traditionell katholischen Ländern außer Frankreich, aber auch in Deutschland, zu einer relevanten Kraft der modernen Gesellschaft wurde.

Das Wappen des Papstes ist das Stammwappen der Familie Pecci: In Blau eine schlanke grüne Zypresse, darüber ein silberner Querbalken; die Zypresse unter dem Balken begleitet von zwei silbernen Lilien, über dem Balken rechts (= optisch links) ein goldener Stern mit Schweif. Der Baum wächst in zeitgenössischen Wappenzeichnungen meist aus einem grünen oder auch erdfarbenen Schildfuss. Manche Darstellungen des Wappens sind von einem Schriftband mit dem Wahlspruch „Lumen de coelo“ (Licht vom Himmel) begleitet. Frühere Darstellungen des Familienwappens zeigen den Querbalken abweichend nicht über den gesamten Schild gelegt, sondern hinter dem Baum.

Papst Leo XIII. verlieh dem Vin Mariani "(Mariani-Wein)" eine Goldmedaille und ließ sich auf einem Werbebild für das Getränk abbilden. Damals wusste man noch nicht, dass das Getränk den Ethylester des Benzoylecgonins enthält, der dem Methylester des Benzoylecgonins "(Kokain)" strukturell sehr ähnlich ist.

Der Papst hat aus seiner Leidenschaft für Schnupftabak nie einen Hehl gemacht.

Er war der erste Papst, dessen Stimme aufgenommen wurde. Die 45-sekündige Aufnahme hat Giovanni Bettini am 5. Februar 1903 – also fünf Monate vor seinem Tod – mit einem Phonograph auf einer Wachsrolle gemacht, während der 93-jährige Papst das Ave Maria betete. Man kann sie heute noch auf einer CD mit Gesängen des Kastraten Alessandro Moreschi hören. (Alessandro Moreschi. The Last Castrato. Complete Vatican Recordings. OPAL CD 9823, 1984; 1987)

Außerdem war er der erste Papst, der mit einer Kamera gefilmt wurde, und zwar von William Kennedy Laurie Dickson. Dickson hatte die Kamera selbst erfunden, die nach der Aufnahme vom Papst gesegnet wurde. Die Aufnahme ist noch vorhanden und in einen Film neueren Datums eingefügt worden.

Mit seiner Amtszeit von mehr als 25 Jahren gehört Leo XIII. zu den am längsten regierenden Päpsten. Sein unmittelbarer Vorgänger, Pius IX., war mit mehr als 31 Jahren noch länger im Amt. Später hatte Johannes Paul II. eine Amtszeit von mehr als 26 Jahren.

Im Jahre 1883 wurde auf seinen Erlass hin das Vatikanische Geheimarchiv öffentlich zugänglich gemacht. Nur wenige konnten zuvor die dort vorhandenen archivalischen Quellen nutzen.

Leo XIII. ließ ab 1887 die "Cappella Pecci" in der Kirche Santissime Stimmate di San Francesco neu als Grablege für Familienmitglieder der Familie Pecci ausgestalten. Dort beerdigt sind seine Mutter und sein Bruder sowie später verstorbene Angehörige der Familie.

Stefan George verfasste ein Gedicht auf Leo XIII. Nach Wolfgang Frommel ist dies das einzige bedeutende deutsche Gedicht auf einen Papst.




</doc>
<doc id="13628" url="https://de.wikipedia.org/wiki?curid=13628" title="Stahl">
Stahl

Stahl ist ein Werkstoff, der zum größten Teil aus Eisen besteht. Er lässt sich warm oder kalt umformen, also durch Walzen oder Schmieden bearbeiten. 

Stahl ist eine Eisen-Kohlenstoff-Legierung mit einem Kohlenstoff-Massenanteil von maximal 2 %. Eisen-Kohlenstoff-Legierungen mit höheren Kohlenstoffanteilen werden Gusseisen genannt und sind nicht umformbar. Stahl enthält neben Kohlenstoff noch weitere Legierungselemente, die bewusst zulegiert werden, um seine Eigenschaften zu verändern oder als Verunreinigung enthalten sind. Im Register europäischer Stähle sind über 2400 Stahlsorten (Stand: 2017) aufgelistet.

Stahl ist einer der vielseitigsten Konstruktionswerkstoffe und ist unbegrenzt rezyklierbar. Seine Produktion (im Jahr 2016: 1629 Millionen Tonnen) übertrifft die Menge aller übrigen metallischen Werkstoffe zusammen um mehr als das zehnfache. Stahl ist in großen Mengen und zu geringen Kosten verfügbar und seine Eigenschaften lassen sich durch Legieren und Wärmebehandeln in weiten Bereichen variieren. Er lässt sich gut durch Walzen, Schmieden, Fräsen und Schweißen verarbeiten und verfügt über eine hohe Festigkeit (einfacher Stahl zwischen 180 und 350 N/mm², hochfester Stahl bis weit über 1200 N/mm²), gute Härtbarkeit, Steifheit (E-Modul) und Bruchdehnung.

Kohle und Stahl (Montanindustrie) waren lange Zeit Hauptsäulen der Schwerindustrie und Grundlage für die politische Macht eines Staats.

Das Wort „Stahl“ entwickelte sich aus dem mittelhochdeutschen "stahel, stāl," dem althochdeutschen Wort "stahal", dem mittelniederdeutschen "stāl," mittelniederländischen "stael" und dem an. "stál;" daneben die j-Bildung as. "stehli" ‚Axt‘ und ae. "stīle."

In der DIN EN 10020:2000–07 "Begriffsbestimmungen für die Einteilung der Stähle" wird unter Punkt 2.1 folgendes ausgeführt:
Eisenwerkstoffe sind Werkstoffe, deren Massenanteil an Eisen von keinem anderen Element übertroffen wird. Als Stahl werden nur Eisenwerkstoffe bezeichnet, deren Massenanteil an Kohlenstoff unter 2 % liegt. Anders als in der DIN EN 10020 werden in der Fachliteratur nur solche Werkstoffe zu den Stählen gezählt, die sich umformen lassen, also sich durch Schmieden bearbeiten lassen.

Der Grenzgehalt an Kohlenstoff von 2 % leitet sich direkt aus dem Eisen-Kohlenstoff-Diagramm ab. Bis zu einem Gehalt von 2,06 % kann der Werkstoff in Form von Austenit vorliegen, der sich gut umformen lässt.

Diese allgemeine, seit dem frühen 20. Jahrhundert gebräuchliche Definition umfasst mit dem Begriff "Stahl" auch das damals kaum mehr produzierte Schmiedeeisen, das ebenfalls einen Kohlenstoff-Gehalt von unter 2 % hat. Zwar hat Schmiedeeisen ähnliche Kohlenstoffanteile wie Stahl, ist aber aufgrund verschiedener Verunreinigungen nicht identisch mit Stahl. Wenn in modernen Werken (ab dem 20. Jh.) im historischen Kontext von "Stahl" die Rede ist, so ist damit "Schmiedeeisen" gemeint. Selbst in Werken der Technikgeschichte wird für frühere schmiedbare Eisenwerkstoffe der Begriff "Stahl" verwendet. Der Begriff "Schmiedeeisen" diente in der Antike als Abgrenzung gegenüber dem Roheisen, das noch sehr stark verunreinigt war, und ab dem Mittelalter zusätzlich gegenüber dem nicht schmiedbaren Gusseisen.

Einfache schmiedbare Eisenwerkstoffe wurden bereits bei den Hethitern vor ca. 3500 Jahren hergestellt, z. B. für Waffen. Die frühe Verhüttung von Eisenerz ist bereits für das 2. Jahrtausend v. Chr. im damaligen Hethiter-Reich belegt, wo auch um die Mitte des 1. Jahrtausends v. Chr. wohl erstmals ein einfacher härtbarer Stahl hergestellt wurde. Eisen verdrängte allmählich die zuvor genutzten Kupferwerkstoffe (Bronze), da es härter und fester ist. Genutzt wurde Eisen vor allem für Waffen und Rüstungen, sowie Werkzeuge, weniger in der Landwirtschaft. Eisenerze waren nahezu überall zu finden, während die zur Bronze-Herstellung benötigten Metalle Kupfer und Zinn selten waren und nicht an denselben Orten vorkamen. Zur Eisengewinnung wurde Holzkohle benötigt, die aus Holz gewonnen werden konnte. 

In der Antike und im Mittelalter wurde das Erz in mit Holzkohle beheizten Rennöfen bei Temperaturen von etwa 1250 °C verhüttet. Das Eisenerz wandelte sich dabei in festes Eisen, sowie in flüssige Schlacke um, die verschiedene unerwünschte Bestandteile der Erze enthält. Der Ofen wurde am Boden angestochen, sodass die Schlacke herausrinnen konnte (daher die Bezeichnung "Renn"ofen, von "rinnen"). Das Produkt war poröses, festes Roheisen, das noch Schlackereste enthielt, die durch Schmieden entfernt wurden. Danach wurde es als Schmiedeeisen bezeichnet, das beinahe keinen Kohlenstoff enthielt und dem heutigen Stahl recht nahe kam, aber auf einem anderen Weg gewonnen wurde und daher stärker mit Begleitelementen verunreinigt war. Aristoteles unterschied jedenfalls bereits in der "Meteorologica" das Roheisen vom Schmiedeeisen, das keine Schlackereste mehr enthält. Da das kohlenstoffarme Eisen für Werkzeuge und Waffen zu weich ist, wurde es auf Holzkohlen, die zum größten Teil aus Kohlenstoff bestehen, geglüht, wodurch es das Element Kohlenstoff vor allem in den Randschichten aufnahm. Auch das Härten durch Abschrecken in Wasser oder Öl war bekannt und wird von Plutarch (45–125 n. Chr.) korrekt durch das schnelle Abkühlen erklärt. Zuvor war Plinius der Ältere (24–79 n. Chr.) noch der Meinung, die Wasserqualität hätte den entscheidenden Einfluss.

Als neue Berufe entstanden Grob- und Feinschmiede, die nun Eisen nutzten, im Gegensatz zu den Kupferschmieden. Andere Schmiede spezialisierten sich auf bestimmte Produkte oder Produktgruppen. Dazu zählen der Werkzeug-, Messer-, Sichel-, Nagel-, Schwert-, Helm-, Lanzen- oder Pfeilschmied und Schlosser. In der Antike wurden die notwendigen Arbeiten häufig von Sklaven verrichtet.

Für die Landwirtschaft produzierte im Frühmittelalter das dörfliche Handwerk Sensen, Sicheln und Pflüge. Durch Wind- oder Wassermühlen angetriebene Hammerwerke und Blasebälge wurden entwickelt. Im 14. Jahrhundert n. Chr. wurde in Europa der Holzkohle-Hochofen (Stückofen) entwickelt. Er erreichte höhere Temperaturen und benötigte weniger Kohle. Das Eisenerz reagierte darin mit dem Kohlenstoff der Holzkohle. Die Schmelztemperatur lag dank des Kohlenstoffs nun unter der Hochofentemperatur, sodass erstmals flüssiges Roheisen entstand, das jedoch wegen des hohen Kohlenstoffgehaltes von etwa 4 % nicht schmiedbar war. Es ähnelte dem heutigen Gusseisen und wurde auch in Formen gegossen. Um schmiedbares Eisen zu erhalten, wurde das Roheisen aus dem Hochofen nochmals im Frischfeuer geschmolzen. Die Eigenschaften des Eisens konnten gezielt durch Anlassen, Aufkohlen, Abschrecken und Glühen beeinflusst werden. Der Benediktinermönch Theophilus Presbyter beschrieb im 11. Jahrhundert das Härten von Feilen: Zunächst wurde Ochsenhorn verbrannt und mit Salz vermischt und über die Feilen gestreut, die dann im Ofen geglüht wurden. Anschließend wurden sie in Wasser abgeschreckt und im Ofen angelassen. Draht wurde ursprünglich wie in der Antike gedreht und geschmiedet. Im Mittelalter ging man zum Drahtziehen mit Zieheisen über, um die großen Drahtmengen herzustellen, die vor allem für Ringpanzer („Kettenhemden“) benötigt wurden.

Zu Beginn des Mittelalters war die Arbeitsteilung wenig ausgeprägt. Grob- und Feinschmiede arbeiteten häufig in derselben Schmiede. Später war die Arbeitsteilung ähnlich wie in der Antike, wozu neue Berufe wie Huf-, Pfannen-, Pflug- und Zangenschmiede sowie Feilenhauer beitrugen. Neben der beruflichen Arbeitsteilung gab es auch regionale Unterschiede. In der Gegend um Solingen lagen an Nebenläufen der Wupper zahlreiche durch Wassermühlen getriebene Hammerwerke, die Klingen schmiedeten. Die Schleifmühlen für die geschmiedeten Klingen benötigten für die Fertigbearbeitung mehr Energie und lagen daher direkt an der Wupper. Diese getrennten Fertigungstufen erforderten jedoch den Transport der geschmiedeten Klingen von den Nebenläufen bis zur Wupper, wodurch bis zu einer Wegstunde für den Transport aufgebracht werden musste.

Die Herstellung von gebrauchsfähigem Eisen aus Erzen geschah in mehreren Schritten:
Neu war die technische Umsetzung dieser Prozesse, die nun vor allem auf Steinkohlebasis beruhte und daher Eisenwerkstoffe günstiger werden ließ, und die Produktionsmengen erhöhte. Innerhalb der Produktionstechnik konnten Eisen oder Stahl nun als Konstruktionswerkstoff für Werkzeugmaschinen genutzt werden, die dadurch präziser und leistungsfähiger wurden. Der Großteil des Eisens wurde jedoch für Dampflokomotiven und Schienen sowie Brücken verwendet.

Für das Schmelzen der Erze im Hochofen war es notwendig, dass das Erz mit Kohle in Berührung kam, da der Kohlenstoff für chemische Reaktionen benötigt wurde. Bei Holzkohle, die fast ausschließlich aus Kohlenstoff besteht, war dies kein Problem, die günstigere Steinkohle jedoch war stark mit Schwefel und anderen Elementen verunreinigt, die das Produkt schlecht werden ließen. Abraham Darby kam auf die Idee, Steinkohle zu verkoken. Als Hauptprodukt entstand dabei Koks, der für das Schmelzen der Erze genutzt werden konnte, als Nebenprodukt Teer. Als sich die Teergruben in der Nähe der Eisenhütten immer weiter füllten, wurden Chemiker darauf aufmerksam und fanden Methoden, daraus Teerfarben und Medikamente herzustellen. Ein deutlich niedrigerer Kohle-/Koksverbrauch war mit dem Heißluftblasen von James Beaumont Neilson möglich. Hierbei wurde die dem Hochofen zugeführte Luft erhitzt, was höhere Temperaturen zur Folge hatte und besseren Stahl lieferte.

Für das Frischen mit Steinkohle gab es zwei verschiedene Methoden, um das Eisen vor dem Schwefel der Steinkohle abzuschirmen. Ab 1740 entwickelte Benjamin Huntsman den Tiegelgussstahl, bei dem Stahl in Tiegel gegeben und mehrere Tage lang im Kohlefeuer erhitzt wurde. Das Ergebnis war ein homogener Stahl ausgezeichneter Qualität. Wegen seiner relativ hohen Härte wurde er bevorzugt für Schneidwerkzeuge und Scheren genutzt. Er war jedoch sehr teuer und konnte nur in geringen Mengen erzeugt werden.
Das bedeutendere Verfahren war das Puddeln, das 1784 von Henry Cort erfunden wurde. Beim Puddeln wurde das Roheisen auf einen Herd gegeben, unter dem Steinkohle verbrannt wurde. Das flüssige Roheisen kam dabei mit dem Sauerstoff der Luft in Kontakt und verbrannte den im Eisen enthaltenen Kohlenstoff. Da das kohlenstoffarme Eisen einen höheren Schmelzpunkt hat, bildeten sich Klumpen, die schließlich zu Luppen anwuchsen und aus dem Herd geholt wurden. Um sicherzustellen, dass alle Teile der Schmelze gleichmäßig mit Sauerstoff in Kontakt kamen, musste ein Arbeiter die Schmelze ständig kräftig umrühren, was viel Erfahrung erforderte. Von den Puddlern hing somit entscheidend die Qualität des erzeugten Stahles ab. Das Puddeln ermöglichte zwar größere Produktionsmengen als zuvor, war jedoch ein manuelles Verfahren, das sich nicht mechanisieren ließ, und bildete den Engpass in der gesamten Prozesskette vom Erz bis zum fertigen Produkt.

Das durch Puddeln gefrischte Eisen wurde anschließend unter dem Schmiedehammer bearbeitet, um die letzten Schlackereste zu entfernen und den Werkstoff homogener zu machen. Dieser Schmiedeprozess hatte ebenfalls großen Einfluss auf die Qualität des Stahls. Erst danach wurde er zu Blechen oder Schienen gewalzt.

Die erste deutsche Gussstahlfabrik gründete Friedrich Krupp 1811 in Essen. Der Impuls für die sprunghafte Zunahme der Stahlproduktion erfolgte etwa in der Mitte des 19. Jahrhunderts durch die gleichzeitige Anwendung mehrerer technischer Erfindungen: Die Dampfmaschine stellte der Industrie eine leistungsstarke und flexible Arbeitskraft zur Verfügung, in den Kokereien wurde die Steinkohle zu Koks verarbeitet und die Entwicklung des Eisenbahnwesens sowie der Dampfschifffahrt förderten die Erreichbarkeit neuer, großer Absatzmärkte für Stahl.

Die Stahlindustrie hatte in allen Ländern, unabhängig von ökonomischen Erwägungen, eine enorme politische Bedeutung, da sie neben einem Indikator für die technisch-wirtschaftliche Entwicklung und der Bedeutung für die Rüstungsindustrie auch eine nationale Prestigefrage war. Die Bedeutung des Stahls für die damalige Zeit symbolisiert der Eiffelturm (allerdings wegen der hohen Anforderung an Bruchsicherheit noch aus Puddeleisen), der anlässlich der Pariser Weltausstellung von 1889 als ein Monument des technischen Fortschritts aus Stahl erbaut wurde.

Die Hochindustrialisierung ist Teil der ersten Industrialisierungswelle, in der sich die führenden Staaten endgültig vom Agrar- zum Industriestaat wandelten. Es existieren verschiedene Definitionen und Abgrenzungen darüber, was genau unter der Hochindustrialisierung zu verstehen ist: Teilweise ist damit die Hochindustrialisierung in Deutschland zwischen 1870 und 1914 gemeint, teilweise ist damit auch die Zweite industrielle Revolution gemeint. Die wichtigsten Neuerungen betreffen die Entwicklung von Verfahren zur Massenproduktion von günstigem und gleichzeitig hochqualitativem Stahl und die Rationalisierungsbewegung, die mit einer wirtschaftlicheren Produktion einherging.

Engpass der Stahlproduktion war nach wie vor das Frischen im Puddelofen. Roheisen konnte in guter Qualität und in ausreichenden Mengen in den stetig größer werdenden Hochöfen geschmolzen werden. Die Weiterverarbeitung des Puddeleisens in mechanisierten Walzwerken geschah ebenfalls zügig. Um die große Nachfrage aus dem Eisenbahnwesen zu befriedigen, unternahm man einige Versuche, das Puddeln ebenfalls zu mechanisieren, was jedoch nicht erfolgreich war. Die Erfahrung der Puddler konnte nicht einfach in Maschinen übertragen werden. Abhilfe kam durch drei konkurrierende Verfahren: Die beiden bodenblasenden oder windfrischenden Verfahren von Bessemer und Thomas sowie das Herdfrischen von Siemens und Martin.

Henry Bessemer kam in den 1850er Jahren auf die Idee, das flüssige Roheisen in einen Konverter zu geben und durch Düsen im Boden Luft zu blasen. Der in der Luft enthaltene Sauerstoff verbrannte den Kohlenstoff und andere unerwünschte Begleitelemente in nur 20 Minuten und erhitzte gleichzeitig auch das Roheisen, sodass der gefrischte Stahl nun erstmals flüssig entstand und gegossen werden konnte. Der Vorgang, Luft durch Roheisen zu blasen, wird auch als Windfrischen bezeichnet. Nur mit Luft konnte man nun mit dem Bessemer-Verfahren die bis dahin höchsten Temperaturen im Hüttenwesen erzeugen und halten und hatte dabei nicht etwa wie früher Brennstoffe verbraucht, sondern auch noch Wärme erzeugt. Das Verfahren war daher deutlich günstiger. Außerdem war der Bessemer-Stahl von sehr guter Qualität: Er war sehr rein und homogen und hielt durch seine große Härte den Belastungen stand, denen er als Schienenmaterial ausgesetzt war. Schienen aus Puddeleisen mussten dagegen meist nach bereits 10 Jahren ausgetauscht werden. Vor allem in den USA entstanden in den späten 1860er und frühen 1870er Jahren zahlreiche Bessemeranlagen. Das Verfahren hatte allerdings zwei Nachteile. Es eignete sich nur für phosphorarme Erze, die vor allem in Deutschland selten waren, und der chemisch neutrale Stickstoff in der Luft löste sich im Gefüge des Stahls und führte dazu, dass er hart, aber auch spröde war. Außerdem verbrannte beim Frischen fast der gesamte Kohlenstoff, sodass Bessemer-Stahl nicht besonders fest war.

Das Thomas-Verfahren von Sidney Thomas und Percy Gilchrist war seit 1878 eine Variante des Bessemer-Verfahrens, die sich für phosphorreiche Erze eignete und daher vor allem in Regionen an Rhein und Ruhr, in Belgien, Luxemburg und Lothringen bevorzugt genutzt wurde. Es benötigte allerdings auch einen gewissen Mindestgehalt an Phosphor, sodass man in England und Amerika wenig Interesse daran zeigte, da hier entsprechende Erze nicht vorkamen. Thomas-Stahl war sogar noch etwas härter und spröder als Bessemer-Stahl und eignete sich eher für wenig belastete Fälle wie Draht oder Rohre und weniger für den Brücken- oder Schiffsbau.

Eine Alternative zu den beiden bodenblasenden oder windfrischenden Verfahren war das Siemens-Martin-Verfahren, das zum Herdfrischen gezählt wird und nach den drei Brüdern des berühmten Werner von Siemens, Friedrich, Otto und Wilhelm sowie dem französischen Eisenhüttenmann Pierre Martin benannt ist. Das Verfahren beruhte auf einem speziellen Ofen, bei dem die zugeführte Luft stark erhitzt wurde, bevor sie entzündet wurde. Anstatt mit dieser heißen Luft nun den Herd zu heizen, erhitzte man damit einen weiteren Luftstrom, der nun noch heißer wurde als der Erste. Damit konnten nun dauerhaft Temperaturen gehalten werden, die über der Schmelztemperatur von Stahl lagen. Nach mehreren Stunden war der Stahl dann von den Begleitelementen befreit. Durch den langsameren Prozess konnte der gewünschte Kohlenstoffgehalt sehr genau eingestellt werden. Außerdem löste sich kein Stickstoff im Stahl, sodass das Siemens-Martin-Verfahren einen qualitativ höherwertigen Stahl ergab, der jedoch wegen des aufwendigeren Prozesses etwas teurer war. Der Großteil des Stahls wurde jedoch bis 1960 mit diesem Verfahren hergestellt, da man damit auch ausgezeichnet Schrott verwerten konnte.

Das nach dem Puddeln notwendige Schmieden der Luppen, um den Werkstoff zu homogenisieren, konnte mit den neuen Verfahren entfallen, da sie alle flüssigen Stahl erzeugten, der schon sehr viel homogener war, als Puddeleisen je werden konnte. Dennoch zögerten die Stahlproduzenten damit, das Schmieden aufzugeben, da ein gründlicher Schmiedeprozess bisher Kennzeichen eines guten Stahls war. Vor allem die Kunden konnten lange nicht glauben, dass mit weniger Aufwand ein besseres Produkt möglich war. Krupp war in Deutschland der letzte Industrielle, der das Schmieden aufgab, verbot seinen Vertretern aber, zu verraten, dass der kruppsche Stahl, der für seine hohe Qualität bekannt war, nur noch gewalzt wurde.

In Amerika etablierte sich Ende des 19. Jahrhunderts allmählich eine Rationalisierungsbewegung, die im frühen 20. Jahrhundert ihren Höhepunkt fand. Unter Rationalisierung verstand man damals vor allem die Erhöhung der wirtschaftlichen Effizienz der Produktion. Man wollte also mit den vorhandenen Arbeitern und Anlagen möglichst viel produzieren oder eine bestimmte Produktionsmenge zu möglichst geringen Kosten herstellen. Erst mit den daraus entstandenen Organisationsprinzipien wurde das wahre Leistungspotential der neuen Technologien vollständig ausgeschöpft – nicht nur im Bereich der Stahlindustrie, sondern in allen Gewerben.

Obwohl in den Bessemer-Werken in einem Konverter der Stahl nach nur 20 Minuten gefrischt war, konnte man nur fünf bis sechs Chargen pro Tag ausbringen. Die restliche Zeit stand der Konverter still. Dies lag vor allem an der hohen Reparaturbedürftigkeit der Konverterböden, die nach spätestens sechs Chargen verschlissen waren und etwa 10 Stunden lang repariert werden mussten. In Europa versuchte man daher neue Materialien zu verwenden, die hitzebeständiger sind. In Amerika, wo ein sehr großer Bedarf nach Stahl herrschte, hielt man sich damit nicht lange auf. Man wechselte einfach den gesamten Boden in wenigen Minuten aus und produzierte dann weiter. Daher stieg die Ausbringung pro Konverter innerhalb weniger Jahre auf 48 pro Tag und später an Spitzentagen sogar auf 72. Die Kosten für die Böden spielten dagegen keine große Rolle. In amerikanischen Stahlwerken wurde nun rund um die Uhr produziert und damit auch zum ersten Mal sowohl schnell als auch gut, was Beobachter aus Europa besonders beeindruckte. Denn bisher hieß gut produzieren vor allem langsam und gründlich produzieren. Diese Produktionsweise in der Stahlindustrie wurde in Amerika als hard driving und in Deutschland als Schnellbetrieb bezeichnet.

Die Elektrizität ermöglichte mit den neuen Elektro-Öfen die Herstellung von sogenanntem Elektrostahl. Diese Stahlwerke waren ausgezeichnete Schrottverwerter, spielten aber auf dem Gesamtmarkt nur eine untergeordnete Rolle. Hier konkurrierten nach wie vor drei verschiedene Verfahren: Das Bessemer- und das Thomasverfahren mit dem etwas günstigeren Stahl und das Siemens-Martin-Verfahren mit dem qualitativ besseren Stahl. In den Bessemer- und Thomashütten bemühte man sich daher die Qualität zu verbessern und damit die ersehnte „Siemens-Martin-Gleichheit“ zu erreichen, was jedoch nicht gelang. Alle Verfahren waren jedoch ausgesprochen produktiv, sodass es erstmals zu Überkapazitäten kam. Bisher hatte man versucht die Kosten einzelner Anlagen zu optimieren; die Produktionsmenge war eine daraus resultierende Größe. Nun war sie ebenso wie der Marktpreis vorgegeben durch Kartellierung, Konzernbildung, Schutzzölle und weitere wirtschaftliche Einflüsse. In den vertikal integrierten Stahlkonzernen, mit ihren Erzgruben, Hochöfen, Bessemer- oder Siemens-Martin-Hütten und den Walzwerken ging es nun darum, die Kosten für das gesamte Unternehmen zu minimieren.

Zur Effizienzsteigerung entstanden bald Integrierte Hüttenwerke, um Nebenprodukte zu verwerten. Im Hochofen entsteht beispielsweise das sogenannte Gichtgas, das schon lange genutzt wurde, um den Hochofenwind zu erwärmen. Dabei wurde jedoch nur etwa 20 % des Gases verbraucht. Nun versuchte man es weitergehend zu nutzen: Zunächst wurde es in den Dampfmaschinen der Walzwerksantriebe verbrannt. Dadurch entstand eine starre technische Kopplung zwischen der Anzahl der Hochöfen und der Anzahl der Walzwerke. Als man zu elektrischen Antrieben überging, trieb man stattdessen Generatoren damit an: Das Gas wurde „verstromt“. Außerdem begann man das flüssige Roheisen aus dem Hochofen direkt in die Konverter (Bessemer- und Thomas-Verfahren) oder Siemens-Martin-Öfen zu geben ohne es erneut einzuschmelzen, was energiesparender ist. Das gefrischte Eisen ließ man gerade soweit abkühlen, dass es fest wurde und walzte es dann. Im Idealfall genügte die im Hochofen erzeugte Hitze für den gesamten Prozess, was als „Walzen in einer Hitze“ bezeichnet wurde. Die schon immer abfallende Schlacke wurde nun zu Sand, Stein und Zement weiterverarbeitet. Besonders begehrt war die Schlacke der Thomas-Werke, da sie einen hohen Anteil von Phosphorsäure aufweist und daher zu Dünger weiterverarbeitet werden kann. Die Thomas-Werke zählten sogar zu den größten Düngemittelherstellern. Diese "Thomasgutschrift" trug wesentlich zu den Kostenvorteilen des Verfahrens bei, da der Thomasstahl um diese Gutschrift günstiger wurde.

In der chemischen Industrie wurden bei einigen Prozessen wie dem damals neuen Haber-Bosch-Verfahren zur Herstellung von Ammoniak sehr hohe Drücke und Temperaturen von bis zu 330 bar und 550 °C benötigt. Der am Prozess beteiligte Wasserstoff diffundierte in den Stahl der Reaktorwände, löste den darin enthaltenen Kohlenstoff und verringerte dadurch die Festigkeit des Stahls, was zu Reaktorexplosionen führte. In der Folge entwickelte man hochlegierte Stähle, die ihre Festigkeit nicht über den Kohlenstoff, sondern über andere Legierungselemente erhalten und daher chemisch beständiger sind. Der wichtigste Vertreter ist der austenitische, rostfreie Chrom-Nickel-Stahl. Die neuen Stähle und chemischen Verfahren verhalfen sich somit gegenseitig zum großtechnischen Durchbruch.

Für die deutschen Nationalsozialisten, die 1935 ein umfangreiches Rüstungsprogramm gestartet hatten, war Stahl ein kriegswichtiger Werkstoff. So galt der Norwegenfeldzug unter anderem der Sicherung des Nachschubs von schwedischem Eisenerz, das für die damalige Stahlerzeugung ein unverzichtbarer Rohstoff war. Die Alliierten bombardierten das Ruhrgebiet, die größte stahlproduzierende Region Europas. Am Ende des Kriegs hatten die Luftangriffe ca. 20 % der Produktionskapazitäten zerstört. Erst 1957 wurde der Vorkriegsstand mit einer Rohstahlproduktion von 16 Millionen Tonnen wieder erreicht.

Die auf der Potsdamer Konferenz beschlossene Demilitarisierung des Deutschen Reichs beinhaltete auch eine Demontage der Stahlindustrie. Ein Teil der demontierten Betriebe ging an die Sowjetunion, die diese zum Wiederaufbau des durch den Krieg zerstörten Landes benötigte. In den westlichen Besatzungszonen regte sich bald Widerstand gegen die Demontage, und so stellten die Alliierten die Demontage schon 1949 wieder ein. Eine weitere Maßnahme der alliierten Kontrollbehörde war die sogenannte „Entflechtung“ der Stahlindustrie. Damit sollte das neuerliche Aufkommen von marktbeherrschenden Unternehmenszusammenschlüssen wie den „Vereinigten Stahlwerken“ verhindert werden.

Um eine gemeinsame Kontrolle der Kohle- und Stahlproduktion sicherzustellen, wurde 1952 auf französische Initiative hin die Montanunion gegründet. Aus der Montanunion entwickelte sich dann schrittweise die Europäische Union. In der Folge erlebte die Stahlindustrie in der Bundesrepublik Deutschland einen großen Aufschwung. 1961 produzierten 420.568 Beschäftigte 33 Millionen Tonnen Rohstahl, was einen Höchststand bei der Mitarbeiterzahl bedeutete. Einen Produktionsrekord stellte die westdeutsche Stahlindustrie 1974 auf, als sie über 53 Millionen Tonnen Stahl fertigte. Heutzutage benötigt die Stahlindustrie im wiedervereinigten Deutschland etwa 76.500 Mitarbeiter, um rund 46 Millionen Tonnen Stahl (Stand 2008) herzustellen. Diese enorme Produktivitätssteigerung war nur durch bedeutende technische Innovationen möglich.

Nach Erwartungen der Stahlindustrie wird die Stahlnachfrage der Automobilbranche durch die aufkommende Elektromobilität bis 2050 um 4,2 Mio. t zunehmen.

Stahl ist der Standardwerkstoff im Maschinenbau und ein wichtiger Baustoff im Bauwesen. Die Teildisziplin des Bauingenieurwesens, die sich mit den Besonderheiten von Stahlkonstruktionen befasst, ist der Stahlbau. Von dem in Deutschland genutzten Stahl entfallen 25 % auf das Baugewerbe, 25 % auf den Automobilbau, 13 % auf den Maschinenbau, 11 % auf Rohre, 9 % auf Metallwaren und 8 % auf den Stahlbau (Stand: 2014).

Stahl wird verwendet für zahlreiche verschiedene Maschinen, darunter Pumpen, Krane, Förderanlagen, Turbinen oder Fräsmaschinen, für Stahlseile, Brücken und den Hochbau, im Stahlbeton, für Waffen und Werkzeuge aller Art, für Rohre und chemische Apparate, Druckbehälter, Schienen, Schiffe, Autos und Motorräder.

In Industriezweigen, bei denen Wert auf Leichtbauweise gelegt wird (insbesondere der gesamte Fahrzeugbau), können anstelle von Stahl Werkstoffe von geringerer Dichte, beispielsweise Aluminium, Magnesium, Kunststoffe und Faserverbundwerkstoffe verwendet werden. Da die anderen metallischen Werkstoffe aber häufig eine geringere Festigkeit und Härte im Vergleich zu Stahl aufweisen, kann der Gewichtsvorteil durch gezieltes Verwenden von hochfesten Stählen und konstruktiven Maßnahmen – etwa die Verarbeitung von dünnerem Blech mit Aussparungen und Sicken – ausgeglichen werden. Faserverbundwerkstoffe haben zwar teilweise eine wesentlich höhere Festigkeit und Steifigkeit in Faserrichtung, Konstruktion und Verarbeitung unterscheiden sich jedoch deutlich von der metallischer Werkstoffe und sind vor allem deutlich aufwändiger.

Weltweit wurden 2016 1629 Millionen Tonnen Stahl produziert. Das ist mehr als das zehnfache aller anderen metallischen Werkstoffe zusammen. Von Aluminium, dem zweitwichtigsten metallischen Werkstoff, wurden 2016 nur 115 Mio. Tonnen produziert. Das mit großem Abstand bedeutendste Herstellerland war die Volksrepublik China mit einem Anteil von 50 Prozent. Größter Produzent nach China ist Japan mit 6,4 %. In der EU werden 10 % und in Nordamerika (NAFTA) 6,8 % der Weltproduktion hergestellt. Siehe auch Stahl/Tabellen und Grafiken.

In Deutschland wurden mit 87.000 Beschäftigten im Jahr 2014 ca. 43 Mio. t Rohstahl hergestellt.

Die Produktion von Stahl verlief lange Zeit auf relativ geringem Niveau: Schätzungen zufolge wurden im Mittelalter im deutschsprachigen Raum zwischen 20.000 t und 30.000 t an Stahl jährlich erzeugt. Gegen 1950 überstieg die Weltproduktion erstmals 200 Mio. Tonnen, bis Mitte der 1970er-Jahre stieg sie weiter bis auf 700 Mio. Tonnen und verweilte bis zur Jahrtausendwende mit geringen Schwankungen auf diesem Niveau. Seitdem stieg sie weiter auf über 1000 Mio. Tonnen, wobei der Zuwachs fast ausschließlich auf China zurückgeht.

Aus Eisenerzen wird Roheisen gewonnen, indem es zusammen mit Kohle in einen Hochofen gegeben wird. Die Kohle erhitzt einerseits durch Verbrennung das Erz und dient andererseits als Reduktionsmittel für das Erz, das chemisch gesehen aus Eisenoxid besteht. Das dadurch entstandene Roheisen, dient als Ausgangsmaterial für die Stahlerzeugung. Es enthält etwa 4 % Kohlenstoff und verschiedene Verunreinigungen.

Der Vorgang, bei dem der Gehalt an Kohlenstoff und anderen Elementen im Roheisen gesenkt wird, wird als Frischen bezeichnet, was nichts anderes bedeutet, als dass die unerwünschten Begleitelemente Silicium, Mangan, Schwefel und Phosphor durch Zugabe von Sauerstoff verbrannt werden. Heute relativ unbedeutende Frischverfahren sind das Siemens-Martin-, Bessemer- und das Thomas-Verfahren, bei denen die Oxidation durch Luft vonstattengeht. Sie waren im von der Mitte des 19. Jahrhunderts bis in die 1960er weit verbreitet. Davor wurde das Puddelverfahren genutzt.
Technisch weit verbreitet (72 % der Welterzeugung) ist aber das Linz-Donawitz-Verfahren (LD-Verfahren). Bei diesem wird das flüssige Roheisen aus dem Hochofen in einen großen, schwenkbaren Behälter gefüllt. Dieser Behälter, der Konverter, fasst ungefähr 300 t flüssiges Roheisen. Die Reaktion, die zur Umwandlung von Roheisen in Stahl führt, ist exotherm. Damit der Konverter durch zu hohe Temperaturen keinen Schaden nimmt, muss er gekühlt werden. Zu diesem Zweck wird zusätzlich zum Roheisen Eisen- bzw. Stahlschrott beigemischt. Die zum Schmelzen des Eisen- bzw. Stahlschrottes nötige Energie entzieht dem Prozess einen Teil der Wärme. Dennoch steigen die Temperaturen im Konverter von ca. 1250 °C auf etwa 1600 °C.

Der Prozess der Rohstahlerzeugung startet durch das Einfahren einer wassergekühlten Sauerstofflanze in die Schmelze. Durch diese Lanze wird reiner Sauerstoff mit einem Druck von etwa 10 bar in die Schmelze geblasen. Er oxidiert die Begleitelemente, die entstehenden gasförmigen Oxide (Kohlenmonoxid, Kohlendioxid und Schwefeldioxid) entweichen durch die Konverteröffnung in den Abgaskamin. Feste oder flüssige Oxide lagern sich an der Oberfläche der Schmelze ab, wo sie zusammen mit zuvor zugegebenem Kalkstein die sogenannte Schlacke bilden. Nach etwa einer halben Stunde ist der Gehalt an Fremdelementen in der Schmelze stark gesunken. Die Schlacke und die Stahlschmelze (jetzt Rohstahl genannt) werden getrennt voneinander aus dem Konverter in Transportkübel gegossen.

Das zweite wichtige Stahlherstellungsverfahren ist das Elektrostahlverfahren. Mit Graphitelektroden werden im Lichtbogenofen Temperaturen von bis zu 3500 °C erzeugt. Da diese Temperaturen nur lokal an den Spitzen der Graphitelektroden entstehen, kann zur beschleunigten Homogenisierung der Temperaturverteilung mit Lanzen Sauerstoff eingeblasen werden. Dadurch wird der Erschmelzungsprozess deutlich beschleunigt und so können ca. 100 to Stahlschrott in ca. einer Stunde erschmolzen werden. Für die Erschmelzung werden der Eigenschrott, der aus der Stahlherstellung stammt, als auch Fremdschrott, z. B. Automobilschrott, eingesetzt. Bereits während der Erschmelzung können Legierungsmittel der Stahlschmelze zugefügt werden. Die verfeinerte Einstellung der gewünschten chemischen Analyse wird im Legierungsofen vollzogen, nachdem die Stahlschmelze in den sogenannten Legierungsstand verbracht wurde.

Darüber hinaus lässt sich die Qualität des Stahls zusätzlich erhöhen, indem man ihn einer Desoxidation unterzieht. Dabei werden der Stahlschmelze Ferrosilicium und Aluminium beigesetzt, die den Sauerstoff in der Schmelze binden; dies verhindert Sauerstoffeinschlüsse und erhöht damit die Festigkeit des Stahls. Bei dem Vorgang der Desoxidation, also der Erstarrung (Abkühlung) des Stahls, ist die Randzone genauso temperiert wie der Kern des Stahlblocks. Dies mindert die Spannungsenergie im Gefüge des Stahls, somit erlangt der Stahl ein gleichmäßiges Gefüge. Je nach Grad der Desoxidation unterscheidet man beruhigte Stähle und vollberuhigte Stähle.

Für kleinere Tonnagen oder beim Einsatz in Gießereibetrieben finden Induktionsöfen häufig Anwendung.

Eisen als Hauptbestandteil des Stahles ist, auch wenn es korrodiert oder weggeworfen wird, für Umwelt, Tier, Mensch und Pflanzen nicht toxisch. Die Stahlerzeugung ist ein großer Energieverbraucher. Im Jahr 2013 entfielen ca. 18 % des gesamten weltweiten industriellen Endenergieverbrauchs auf den Eisen- und Stahlsektor. Bei der Ökobilanz von Stahl müssen zwei Herstellungsrouten unterschieden werden:


In der Praxis wird Stahl zuerst aus Erz hergestellt und dann oft mehrfach recycelt (einmalig Primärerzeugung und mehrfach Stahlrecycling). Damit ergibt sich ein durchschnittlicher CO-Ausstoß von etwa 1 kg CO pro kg hergestellten Warmbandstahles. Zum Vergleich: Bei der Herstellung von 1 kg Roh-Aluminium werden 10 kg CO freigesetzt (bei Verwendung eines durchschnittlichen Energiemixes). Aus geschreddertem Mischschrott kann über Magnetscheider eine Eisenfraktion zurückgewonnen werden. Legierungselemente können, müssen aber nicht beim Recycling entfernt werden. Hochlegierte Stähle werden daher dementsprechend von Metallaufkäufern extra erfasst und vergütet. Hingegen wird das Recycling dünnwandiger, beschichteter, restentleerter Gebinde (Dosenschrott) teilweise als Downcycling bezeichnet.

Stahl ist mit 500 Mio. t pro Jahr der weltweit meistrecycelte Industriewerkstoff. Die Recyclingquote von Stahl liegt bei 70 %, die von einzelnen Stahlanwendungen z. T. bei deutlich über 90 %.

Für den Korrosionsschutz von Eisen und Stahl werden Stoffe eingesetzt, die das Recycling stören, verloren gehen oder als umweltrelevante Stoffe entweichen oder zurückgehalten werden müssen. Dazu gehören insbesondere die Legierungselemente Chrom und Nickel sowie als Beschichtungen Lacke, Zinn (Weißblech) und Zink. In Europa werden daher die Stahlwerksstäube recycelt, um das darin enthaltene Zink zurückzugewinnen.

Nach EN 10020:2000 wird zwischen drei Hauptgüteklassen unterschieden:
Die Kurznamen der Stähle sind in der EN 10027 festgelegt. Heute werden ca. 2500 verschiedene Stahlsorten hergestellt, von denen etwa 2000 erst in den letzten zehn Jahren entwickelt wurden.

Die Stahlwerkstoffe werden nach den Legierungselementen, den Gefügebestandteilen und den mechanischen Eigenschaften in Gruppen eingeteilt.

Weitere wichtige Eigenschaften für den Anwender sind die Einsatzbereiche und Verwendungsmöglichkeiten der Stähle. Daher ist auch eine Kennzeichnung sinnvoll, aus der diese hervorgeht:

Die Dichte von Stahl beträgt 7,85–7,87 g/cm (7850–7870 kg/m).

Der Schmelzpunkt von reinem Eisen liegt bei 1536 °C, durch Zugabe von Legierungselementen verringert sich in der Regel der Schmelzpunkt von Stahl und liegt bei 2 % Kohlenstoff nur noch bei 1400 °C. Aus dem Eisen-Kohlenstoff-Diagramm ersichtlich, hat Stahl genaugenommen wie die meisten Legierungen keinen genauen Schmelzpunkt: Bei Temperaturen oberhalb der Liquiduslinie (oberste Linie im Diagramm) ist der Werkstoff vollständig flüssig, zwischen der Liquidus- und Soliduslinie (zweitoberste Linie) liegt er teilweise flüssig und fest vor. Erst bei Unterschreiten der Soliduslinie liegt nur noch fester Werkstoff vor.

Die elektrische Leitfähigkeit von Stahl ist etwas geringer als die von reinem Eisen mit 10 · 10 A/(V · m). So hat "Stahl C15" 9,3 · 10A/(V · m), "Stahl C35" 8,6 · 10 A/(V · m) und "Stahl C60" 7,9 · 10 A/(V · m). Die Leitfähigkeit sinkt also merklich mit steigendem Anteil von Kohlenstoff, bleibt aber deutlich über der von rostfreiem Stahl mit 1 · 10 A/(V · m).

Stahl gilt als sehr fester, aber auch „weicher“ Werkstoff, während das verwandte Gusseisen als hart und spröde gilt. Festigkeit ist die auf den Querschnitt bezogene Kraft, die der Werkstoff ertragen kann, bevor er versagt (Reißen, Brechen etc.). Bauteile aus Stahl können also hohe Kräfte übertragen. Stahl gilt als „weich“ also verformbar: Bevor Stahl bricht, verformt er sich, wobei diese Verformung bei Stahl sehr groß sein kann. Gusseisen dagegen bricht ohne vorherige Verformung. Stahl hat daher große Sicherheitsreserven gegenüber Bruch, weshalb er bei wichtigen Fahrzeugteilen genutzt wird (Lenkung, Fahrwerk, Antrieb).

Die Festigkeit liegt bei den am häufigsten verwendeten Stählen, den unlegierten Baustählen, zwischen 180 und 350 N/mm². Sie nimmt mit steigendem Gehalt an Kohlenstoff und sonstigen Legierungselementen zu. Das Verformungsverhalten von Werkstoffen wird in Spannungs-Dehnungs-Diagrammen festgehalten. Dabei wird die Kraft auf eine Materialprobe langsam erhöht und die Längenänderung gemessen. Im Diagramm wird die Mechanische Spannung (Kraft geteilt durch Querschnittsfläche) und die Dehnung (Längenänderung relativ zur Ausganglänge) aufgetragen. Baustähle weisen eine ausgeprägte Streckgrenze auf: Die Spannung steigt zunächst proportional zur Dehnung und fällt dann plötzlich geringfügig ab. Das Maximum der Geraden im Diagramm ist die Streckgrenze, bis zu der Stähle im Gebrauchsfall genutzt werden sollen. Für die Bearbeitung durch Walzen und Schmieden muss sie überschritten werden.

Hochfeste Stähle können Festigkeiten über 1000 N/mm² erreichen. Manche besondere Sorten, wie die für Klaviersaiten, erreichen sogar über 1800 N/mm². Stahl weist somit gegenüber Holz, Beton und Stein eine hohe Festigkeit auf. Die auf die Dichte bezogene Festigkeit, die Spezifische Festigkeit, ist bei Stahl sehr hoch gegenüber diesen Werkstoffen. Konstruktionen aus Stahl sind somit bei gegebener Tragfähigkeit leichter. Übertroffen wird Stahl nur noch von einigen Leichtmetallen wie Aluminium, Magnesium oder Titan. Von allen bekannten Werkstoffen zählen Stähle zu denen mit der höchsten Festigkeit. Ähnliche, aber geringere Werte erreichen neben Aluminium-, Magnesium- und Titanlegierungen noch CFKs, mit Kohlenstofffasern verstärkte Kunststoffe.

Die Bruchdehnung, also die Dehnung beim Bruch (Ende der Kurve im Spannungs-Dehnungs-Diagramm), kann bei Tiefziehstahl mit geringer Festigkeit 50 % betragen, höherfeste Stähle haben in der Regel dagegen geringere Bruchdehnungen; Baustähle dehnen sich also sehr weit, bevor sie brechen. Im Gegensatz dazu brechen Gusseisen und Keramik bei Überschreiten der Festigkeit ohne vorherige plastische Verformung.

Der Elastizitätsmodul von gewöhnlichen ferritischen Stählen beträgt 210 GPa (2,1·10 N/mm). Im Spannungs-Dehnungs-Diagramm ist er als Steigung der Geraden zu erkennen. Der E-Modul ist damit etwas höher als der von Gusseisen (170 GPa) und deutlich höher als der von Aluminiumlegierungen (70 GPa). Übertroffen wird Stahl nur von wenigen Werkstoffen, darunter Hartmetalle (ca. 500 GPa) und Diamant (900 GPa).

Die Härte kann bei Stahl in großen Bereichen variieren und Vickershärten zwischen 80 und 940 HV erreichen. Weichgeglühte Vergütungsstähle erreichen Härten von 150 bis 320 HV (150 bis 300 Brinell, 1 bis 33 Rockwell), vergütete (gehärtete) Vergütungsstähle liegen bei etwa 210 bis 650 HV. Werkzeugstahl erreicht im gehärteten Zustand bis 840 HV. Im Vergleich dazu liegen Kupfer- und Alumiumwerkstoffe zwischen 40 und 190 HV, während Hartmetalle 780 bis 1760 HV erreichen. Typische Keramiken sind noch härter.

Technologische Eigenschaften beziehen sich auf die Be- und Verarbeitung. Im Einzelnen handelt es sich um die Gießbarkeit, Schmiedbarkeit, Zerspanbarkeit und Schweißbarkeit. Mit Ausnahme der Gießbarkeit sind sie bei den häufig genutzten Sorten gut bis sehr gut.

Gießbarkeit ist die Eignung eines Werkstoffes, durch Gießen verarbeitet zu werden. Gemeint ist hier vor allem das Formgießen, bei dem die Formen schon die Gestalt der späteren Endprodukte enthalten, nicht das Gießen zu Barren.

Stahl lässt sich vergleichsweise schlecht gießen, weshalb er von allen in der Gießerei verwendeten Werkstoffen einen geringen Massenanteil hat und sowohl von Gusseisen als auch von Aluminium deutlich übertroffen wird, da sich beide viel besser gießen lassen. 2011 wurden in Deutschland ca. 220.000 Tonnen Stahl in Gießereien genutzt, während es bei Gusseisen ca. 4,2 Mio. Tonnen und bei Aluminium 840.000 Tonnen waren.

Spezielle Stahlsorten für Gießereien werden als Stahlguss bezeichnet. Er neigt zu Warmrissen, die nur mit gießtechnischer Erfahrung beherrschbar sind. Außerdem ist der Schmelzpunkt mit 1580 °C bis 1680 °C sehr hoch (Gusseisen 1100 °C, Aluminiumgusslegierungen um 600 °C), was zu einem hohen Energiebedarf beim Schmelzen führt und zu hohen thermischen Belastungen der Formen und Anlagen. Stahl neigt beim Formgießen zu Oberflächenanbrennungen mit der Form und es sind große Speiser nötig um den Volumenverlust bei der Abkühlung in der Form auszugleichen. Nach dem Erstarren lassen sich die Speiser nur schwer wieder abtrennen. Gegossene Werkstücke aus Stahl sind wegen des hohen Fertigungsaufwandes etwa dreimal teurer als solche aus Gusseisen, obwohl wegen der höheren Festigkeit weniger Material benötigt wird.

Umformbarkeit ist die Eignung eines Werkstoffes, sich durch die Verfahren der Umformtechnik bearbeiten zu lassen. Das mit Abstand wichtigste Verfahren der Gruppe ist das Schmieden, weshalb auch von Schmiedbarkeit gesprochen wird. Zu der Gruppe zählen aber auch das Biegen, Walzen, Tiefziehen, Fließpressen und viele weitere.

Die Umformbarkeit ist umso besser, je geringer die nötigen Kräfte sind und je stärker sich der Werkstoff verformen kann, ohne zu brechen oder reißen. Die zur Umformung benötigte Kraft wird üblicherweise auf die Querschnittsfläche bezogen und als Fließspannung angegeben. Die maximale Dehnung, die ein Werkstoff ertragen kann, ist die Bruchdehnung.

Bei einfachen Baustählen ist die Fließspannung vergleichsweise gering und die Bruchdehnung sehr hoch. Bei hochfesten Stählen ist die Fließspannung naturgemäß höher, es werden aber auch deutlich festere Werkstoffe geschmiedet, darunter Titan-, Nickel- und Kobalt-Legierungen. Die Bruchdehnung ist meist umso kleiner je fester ein Stahl ist. Eine Ausnahme sind die TRIP-Stähle mit geringer bis mittlerer Fließspannung und hoher Bruchdehnung. Bei den meisten Stahlsorten ist die Fließspannung als gering einzustufen. Dazu zählen neben den Baustählen die Warmarbeitsstähle und Automatenstähle. Aluminium- und Magnesiumlegierungen liegen in einem ähnlichen Bereich. Die Bruchdehnung kann jedoch stärker schwanken: Bei Automatenstählen ist sie sehr gering, bei Warmarbeitsstählen fast genauso gut wie bei Baustählen.

Beim Kaltumformen steigt die Fließspannung und somit auch die nötige Kraft je höher der Umformgrad (die Verformung) ist. Der Effekt wird als Kaltverfestigung bezeichnet und kann genutzt werden um besonders feste Werkstücke zu schmieden. Der genaue Zusammenhang zwischen Fließspannung und Umformgrad wird in Fließkurven festgehalten. Bei höheren Temperaturen sinkt bei fast allen Stählen sowohl die Fließspannung als auch die Verfestigung. Beim Warmumformen steigt die Fließspannung bereits bei geringen Umformgraden gar nicht mehr. Bei Stählen tritt dies bei Temperaturen von etwa 1100 °C auf.
Die Zerspanbarkeit ist die Eignung eines Werkstoffes, sich durch Zerspanen (Fräsen, Bohren, Schleifen) bearbeiten zu lassen. Sie hängt ab vom Kohlenstoffgehalt, den sonstigen Legierungselementen und dem Wärmebehandlungszustand. Stähle mit einem sehr niedrigen Kohlenstoffgehalt neigen zum Verkleben mit der Schneide und bilden lange Bandspäne, die sich in der Maschine verfangen können. Sie führen jedoch zu geringen Zerspankräften, aber auch zu schlechten Oberflächen. Bei mittleren Kohlenstoffgehalten (0,2 % bis 0,6 %) gibt es keine Probleme mit Verklebungen. Die Schnittkräfte steigen, die Oberflächen werden besser und die Späne kürzer. Dafür nimmt der Verschleiß der Werkzeuge zu. Stähle mit einem hohen Kohlenstoffgehalt führen zu hohen Kräften und Temperaturen sowie zu einem hohen Verschleiß. Die Oberflächenqualität und der Spanbruch sind jedoch gut. Elemente wie Phosphor, Blei und Schwefel begünstigen die Zerspanbarkeit, festigkeitssteigernde Elemente wie Nickel verringern sie. Im weichen (normalgeglühten) Zustand sind die meisten Stähle relativ gut zu zerspanen, im vergüteten oder gehärteten Zustand ist der Verschleiß dagegen sehr hoch, was teure Werkzeuge aus Schneidkeramik oder Bornitrid erfordert.

Die Schweißeignung gibt an, wie gut sich ein Werkstoff schweißen lässt. Vor allem die un- und niedrig legierten Baustähle lassen sich sehr gut schweißen, was ein wichtiger Grund für ihre weite Verbreitung ist, da Verbinden durch Schweißen deutlich kostengünstiger ist als durch andere Verbindungstechniken wie Schrauben oder Nieten. Höherlegierte Stähle können beim Schweißen problematisch sein. Als grobe Abschätzung, ob ein Stahl geschweißt werden kann, kann das Kohlenstoffäquivalent genutzt werden, das den unterschiedlichen Einfluss der verschiedenen Legierungselemente berücksichtigt. Aluminium lässt sich meist deutlich schlechter schweißen als Stahl.

Die Mikrostruktur bestimmt bei Stahl, wie bei vielen Werkstoffen, in hohem Maße die mechanischen Eigenschaften, vor allem die Härte und Festigkeit. Die meisten Stähle bestehen neben Eisen und Kohlenstoff noch aus zahlreichen weiteren Elementen, die zumindest als Verunreinigung enthalten sind – die Stahlbegleiter – oder gewollt zulegiert werden, die Legierungselemente. Die Mikrostruktur ist daher relativ kompliziert, beruht aber im Wesentlichen auf der Struktur der reinen Eisen-Kohlenstoff-Legierungen (insbesondere bei un- und niedrig legiertem Stahl). Deren Struktur basiert wiederum auf derjenigen von reinem Eisen.

Eisenatome liegen wie bei allen Metallen im festen Zustand in einer regelmäßigen Anordnung vor. Unter 911 °C befinden sich die Eisenatome in den Ecken eines gedachten Würfels, in dessen Mitte sich ein weiteres Eisenatom befindet. Diese Struktur wiederholt sich in sämtliche Richtungen theoretisch beliebig oft und wird allgemein als kubisch raumzentriert bezeichnet, im Falle von Eisen auch als formula_1-Eisen. Oberhalb von 911 °C liegt Eisen in der sogenannten kubisch flächenzentrierten Form vor, bei der wieder in den Ecken eines gedachten Würfels Atome sitzen, aber diesmal zusätzlich in der Mitte jeder Würfelfläche ein weiteres, aber keines in der Mitte des Würfels. Diese Variante wird als formula_2-Eisen bezeichnet. Der für Stahl wesentliche Unterschied ist die unterschiedliche Dichte: In der kubisch flächenzentrierten Form sind die Lücken zwischen den Eisenatomen größer; sie können also leichter durch Atome der Legierungselemente besetzt werden.

Bei reinen Eisen-Kohlenstoff-Legierungen befindet sich immer Kohlenstoff in den Lücken zwischen den Eisenatomen. formula_1-Eisen mit Kohlenstoffatomen wird allgemein als formula_1-Mischkristall bezeichnet, bei Stahl häufig auch als Ferrit (von lateinisch "ferrum" = Eisen), während das formula_2-Eisen mit eingelagertem Kohlenstoff als formula_2-Mischkristall bezeichnet wird und bei Stahl Austenit genannt wird nach William Austen. Austenit kann je nach Temperatur sehr viel Kohlenstoff enthalten, maximal 2,06 Massenprozent, während Ferrit nur maximal 0,03 % Kohlenstoff enthalten kann. Die Temperatur, bei der sich Austenit in Ferrit umwandelt, hängt vom Kohlenstoffgehalt ab und lässt sich aus dem Eisen-Kohlenstoff-Diagramm entnehmen. In beiden Fällen kommt es zu einer Mischkristallverfestigung, also einer Steigerung der Festigkeit. Außer im Eisen-Mischkristall kann Kohlenstoff noch in zwei weiteren Formen vorliegen, insbesondere wenn mehr Kohlenstoff vorhanden ist als im Mischkristall aufgenommen werden kann: Grafit und Zementit. Als Grafit werden Bereiche bezeichnet, die nur aus Kohlenstoff bestehen, während Zementit eine chemische Verbindung aus Eisen und Kohlenstoff mit der Formel FeC ist. Grafit entsteht vor allem bei langsamer Abkühlung nach dem Gießen oder Glühen, während der harte und spröde Zementit bei schneller Abkühlung entsteht. Sie führen zu keiner Festigkeitssteigerung.

Weitere Legierungselemente können in verschiedenen Formen im Stahl vorliegen:

Die mechanischen Eigenschaften des Stahls (Härte, Festigkeit) können auf verschiedene Weisen verändert werden:

Die Wärmebehandlungsverfahren werden in mehrere Gruppen eingeteilt:

Thermische Verfahren ändern die Mikrostruktur durch rein thermische Einflüsse, ohne zusätzlich die chemische Zusammensetzung zu ändern und ohne gleichzeitige mechanische Bearbeitung.

Sie verändern den Stahl auch chemisch. Folgende Verfahren werden angewandt:

Thermomechanische Verfahren basieren auf einer mechanischen Bearbeitung (Schmieden, Walzen) kombiniert mit einer Wärmebehandlung. Von Bedeutung ist das Austenitformhärten, das zur Festigkeitsteigerung dient.




</doc>
<doc id="13629" url="https://de.wikipedia.org/wiki?curid=13629" title="Blechblasinstrument">
Blechblasinstrument

Ein Blechblasinstrument ist ein Blasinstrument, bei dem die Töne mit einem Kessel- oder Trichtermundstück nach dem Prinzip der Polsterpfeife angeblasen werden. Die schwingenden Lippen des Musikers erzeugen den Ton durch Ankopplung an eine konisch-zylindrische Röhre, deren Luftsäule als Resonator dient. Terminologisch korrekter ist der Begriff „Lippentoninstrument“, welches entsprechend der Hornbostel-Sachs-Systematik zur Klasse der eigentlichen Blasinstrumente in der Gruppe der Aerophone zählt.

Die Mehrheit der Blechblasinstrumente wird aus Blech von Metalllegierungen wie Messing oder Neusilber hergestellt. „Blechblasinstrument“ ist aber nur eine Art Trivialname; andere Materialien sind möglich. Bei großen Instrumenten wie dem Sousaphon kommen moderne Faserverbundwerkstoffe zur Gewichtsersparnis zum Einsatz. Die neuzeitliche Vuvuzela wird aus Kunststoff gefertigt. 

Hölzerne Instrumente wie das Alphorn oder das Didgeridoo funktionieren zwar nach dem gleichen Prinzip, werden aber im allgemeinen Sprachgebrauch genauso wie die mit Tonlöchern ausgestatteten Serpente und Zinken historisch nicht zu den Blechblasinstrumenten, wohl jedoch zu den Lippentoninstrumenten gezählt.

Das Klappenhorn und die Ophikleide zählen dagegen allgemein anerkannt im Hinblick auf ihre Entwicklungsgeschichte zu den Blechblasinstrumenten. Auch sie werden mit einem Kesselmundstück angeblasen.

Das Saxophon und die Querflöte hingegen gehören aufgrund ihrer Tonerzeugung zur Gruppe der Holzblasinstrumente, obwohl ihr Korpus meist aus Metall hergestellt wird.

Die meisten Musikinstrumente bestehen aus einem "Schwingungserzeuger" (Generator) und einem "Schwingungsverstärker" (Resonator). Die Besonderheit der Blechblasinstrumente liegt darin, dass die Schwingungserzeugung durch die Lippen des Bläsers erfolgt und somit ein menschliches Organ Teil des Instruments wird. Die Luft wird dazu gleichmäßig durch die gegen die Strömung leicht vorgespannten Lippen geblasen. Bei ausreichender Strömungsgeschwindigkeit gerät das Lippengewebe durch den Widerstand der Muskulatur in Schwingung. Der Übergang der Schwingung der Lippen zur Tonsäule geschieht durch das Mundstück.

Ein Blechblasinstrument funktioniert als Polsterpfeife. Der Ton entsteht, indem die Schwingung der Lippen des Bläsers sich auf eine der durch die Rohrlänge bestimmten Eigenfrequenzen der Luftsäule im Instrument einstellt. Durch Resonanz mit der Luftsäule im Rohr gerät diese in Schwingung und es entsteht eine stehende Welle. Deren Schwingung wird am offenen Rohrende über den Schallbecher an die Umgebungsluft übertragen. Um das bestmögliche Klangergebnis (möglichst viele Teiltöne/Obertöne) zu erzielen, muss die Lippenfrequenz deckungsgleich mit der Frequenz des jeweiligen Naturtones sein. Ist, ohne den Naturton zu verlassen, die initialisierende Frequenz der Lippen zu hoch oder zu tief, wird die stehende Welle verkürzt oder verlängert, wodurch die Frequenz des Tones höher oder tiefer „verfälscht“ wird.

Das Instrument ist zwar meist aus verschiedenen konischen und zylindrischen Stücken zusammengesetzt, wirkt aber physikalisch im Wesentlichen wie ein konisches, am engen Ende durch den Mund des Spielers verschlossenes Rohr. Der tiefste erzeugbare Ton hat daher eine Wellenlänge, die etwa das Doppelte der Instrumentlänge beträgt. Bei der nächsthöheren Eigenfrequenz, also dem zweiten Naturton, ist die Wellenlänge gleich der Instrumentlänge, der Ton liegt also eine Oktave höher.

Insgesamt ergeben die Eigenfrequenzen die bekannte Naturtonreihe. Sie ist identisch mit der Teiltonreihe des Grundtones. Durch Anpassung der Schwingungsfrequenz mittels Beschleunigung der Strömungsgeschwindigkeit der Luft durch den Lippenspalt überblasen Blechblasinstrumente jeweils zu dem Ton, dessen Frequenz das nächste ganzzahlige Vielfache der Frequenz des Grundtons bildet. Zwei, drei, vier oder mehr Halbwellen entstehen im Rohr. Ob der Grundton praktisch brauchbar ist, hängt unter anderem von der Bauart und Mensur (s. unten) des Instruments ab (siehe unten, Halbinstrument/Ganzinstrument).

Der höchste spielbare Ton ist vom Können des Bläsers abhängig; das Mundstück hat allerdings einen starken Einfluss darauf. Kleinere Mundstücke mit engerer Bohrung begünstigen die Ansprache höherer Töne, führen aber zu einem schärferen Klang vor allem in den tieferen Lagen.

Die Klangqualität hängt von vielen Faktoren ab. Neben der Bauart und Materialbeschaffenheit des Instrumentes sind die Geschicklichkeit und physische Konstitution des Bläsers wesentliche Faktoren. Um die oben unter "Physik der Tonerzeugung" beschriebene Kongruenz der initialisierenden Schwingung der Lippen mit der von der Rohrlänge bestimmten Frequenz des Naturtones herzustellen, bedarf es einer optimalen Balance zwischen

Je unbelasteter durch innere Muskelspannung sowie Mundstückdruck die Lippen entsprechend der erforderlichen Tonhöhe schwingen, desto sauberer und klarer klingt der Ton. Der durch die Atemstütze, von Zwerchfell, Bauchmuskulatur (Beckenbodenmuskulatur, Bauchdecke) sowie der Rückenmuskulatur (Flanken) aufgebaute und gesteuerte Luftstrom als Energieträger kommt hierbei die größte Bedeutung zu, denn die Schwingung wird durch die Energie der den Lippenspalt durchströmenden Luft initialisiert und getragen. Um eine Oktave zu überblasen, muss die Luftgeschwindigkeit etwa verdoppelt werden. Ebenso wichtig, aber in seiner Spannung und der Gewichtung zur Bedeutung der Geschwindigkeit der Luft oft überschätzt, ist der Ansatz. Als Ansatz bezeichnet der Bläser die Stellung der Lippen gegen die Luft und die Balance zwischen Lippenspaltgröße und Muskelspannung. Der Lippenspalt ist gegen den Luftstrom stets offen zu halten. Damit die Lippen frei schwingen können, darf der Lippenspalt nicht zugepresst und das Mundstück nicht angepresst werden (druckloses bzw. druckschwaches Blasen). Im Gegenzug muss die Lippenspannung gerade so groß sein, um ohne zusätzlichen Mundstückdruck der Energie des Luftstromes zu widerstehen. So können die Lippen frei schwingen, ohne sich durch Presskraft des Mundstückes oder Kontakt gegenseitig zu behindern (Nebengeräusche, unnötig hoher Widerstand).

Ebenso ist die Dauer eines auszuhaltenen Tones abhängig einerseits vom Lungenvolumen des Bläsers, andererseits vom Wirkungsgrad seiner Blastechnik. Das Optimum ist erreicht, wenn die durch den Luftstrom transportierte Energie die Lippen mit so wenig wie möglich, aber zum Erreichen der Tonhöhe und Dynamik so viel wie nötig gespannter Lippenmuskulatur zum Schwingen bringt. Erreicht der Bläser das Optimum, benötigt er zur Erregung der Schwingung weniger Luft und kann den Ton in Abhängigkeit von der Lautstärke Dynamik (Musik) länger halten: Töne bis ca. 60 Sekunden sind bei geringer Lautstärke möglich (mit Zirkularatmung beliebig länger).

Je entspannter die Lippenmuskulatur im unteren und mittleren Bereich arbeitet, desto mehr Spielraum hat der Bläser nach oben. Gelingt es, die Lippen durch mehr Entspannung mit wenig Energie zum Schwingen zu bringen, kann der Bläser unter ausreichender Beschleunigung der Luft höhere Töne erreichen (Wirkungsgrad). Ist die Lippenspannung zu hoch und die Öffnung zu klein, erhöht sich der Widerstand am Lippenspalt entsprechend. Dadurch wird für die Tonerzeugung mehr Luft (Energie) benötigt und die Töne sprechen schwerer oder gar nicht an.

Um den Ton klangschön anzublasen, sollte er immer von der Luft initialisiert beginnen. Nicht etwa durch einen Fremdimpuls, z. B. dem Zungenstoß. Der zeitlich dazu koordinierte Stoß präzisiert nur den Beginn der Lippenschwingung. Er stellt kein Ventil im Sinne von Verschluss dar.

Eine wichtige Abweichung vom einfachen konischen Rohr ist durch den Schalltrichter oder Schallbecher gegeben. Dieser instrumententypische Exponentialtrichter transformiert die Wellenimpedanz vom sehr hohen Wert des Rohres zur wesentlich geringeren Schallkennimpedanz der Luft und bewirkt dadurch eine wirksamere Abstrahlung der Schall-Energie in die Umgebung. Nur der Rest der Energie wird zur Erzeugung der stehenden Welle wieder ins Rohr reflektiert. Der Trichter wirkt sich auf die Klangfarbe aus, aber auch auf den Intervallabstand der Naturtöne, denn durch die allmähliche Querschnittsänderung entsteht der Effekt der variablen akustischen Rohrlänge: Der Reflexionspunkt außerhalb des Trichters kann mit Hilfe der Strömungsgeschwindigkeit der Luft und des Ansatzes etwas verschoben werden. Dieses Phänomen ist derzeit noch nicht ausreichend erforscht; einen Einfluss haben z. B. auch die Form des Mundinnenraumes des Bläsers und die Stellung der Zunge.

Instrumente mit nur leicht geöffnetem, flachem Trichter sind die Bügelhörner. Sie sprechen sehr leicht an, klingen aber relativ leise und weich, da ihr Obertonanteil gering ist. Die Tonhöhe kann vom Bläser mit dem Ansatz gut variiert werden (+10/–50 Cent), weil der flache Trichter eine ungenaue Abrisskante für die Reflexion darstellt. Der Trichter gibt relativ wenig Schallenergie an die Umgebungsluft ab; dadurch wird dadurch mehr Energie ins Instrument reflektiert, so dass die stehende Welle sich leicht bildet.

Steilere Trichter haben Trompete oder Posaune, die relativ schwer ansprechen, aber einen obertonreichen, hellen bis scharfen Ton haben, der schwieriger intonierbar ist. Die Tonhöhe lässt sich durch die Blastechnik nur geringfügig verschieben, Intonationskorrekturen sind also nur in eng begrenztem Umfang möglich. Steile Trichter geben mehr Schallenergie ab, die Instrumente klingen also relativ lauter. Dies verringert aber gleichzeitig die reflektierte Energie zur Bildung der stehenden Welle.

Das Kessel- oder Trichtermundstück steckt in einem meist konischen Mundrohr. Die nachfolgenden zylindrischen Rohre, wo sich auch die Ventilbögen befinden, definieren die „Bohrung“. Mitunter folgt darauf noch ein konischer „Anstoß“, bevor das Instrument im Schallstück mit dem Schalltrichter mündet. Die Längen und Durchmesser dieser einzelnen Segmente bestimmen insgesamt die Mensur des jeweiligen Blechblasinstruments. Dieser Begriff bezeichnet also die Steigung des Rohrdurchmessers zur jeweiligen Position in der Grundrohrlänge. Ein exaktes metrisches Maß kann daher nicht definiert werden, sondern man vergleicht meistens gleich lange Instrumente miteinander.

Die Mensur bestimmt einerseits die Klangfarbe des Instruments: Eine Posaune klingt heller als ein Baritonhorn, eine Trompete heller als ein Flügelhorn. Bei der Trompete sind bis zu 40 Teil-Obertöne nachweis- und mit dem Oszillograph darstellbar. Ihr Anteil sowie ihre Stärke in diesem Spektrum bestimmen die Klangfarbe des Instrumentes und deren Abweichungen bei gleicher Bauart. Andererseits beeinflusst die Mensur, wie gut der 1.Naturton (der „Grundton“) anspricht. Bei Waldhörnern ist der tiefste „zuverlässig“ spielbare Ton der Naturtonreihe erst der zweite Naturton, eine Oktave über dem eigentlichen Grundton. Bedingt durch die vergleichsweise sehr engen Mensur des Waldhornes, sowie die kleine Mensur und Bohrung des trichterförmigen Hornmundstückes (es ist in seinem Randdurchmesser dem tulpenförmigen Kessel des Trompetenmundstückes vergleichbar) ist der Grundton (F = 46 Hz) nur mit außerordentlicher Lockerheit möglich zu blasen, während derselbe Ton mit der gleichen Rohrlänge auf der F-Tuba den Normalfall bildet. Abgesehen von dieser Problematik können geübte Bläser auf allen Blechblasinstrumenten die Grundtöne intonationsrein blasen.

Weiterhin hat die Mensur Einfluss auf den exakten Intervallabstand der Naturtonreihe. Durch gezielte punktuelle Durchmesserveränderungen können bestimmte Naturtöne in ihrer Intonation verändert werden. Die saubere Intonation der Naturtonreihe ist ein wesentliches Qualitätsmerkmal.

Die Mensur hat keinen Einfluss auf die Höhe des Grundtones. Diese wird ausschließlich durch die Länge der Luftsäule bestimmt.


"Veränderung der Resonanzrohrlänge:"

Um eine chromatische Spielweise zu ermöglichen, stattete man "Blechblasinstrumente" bereits im 14. Jahrhundert mit der Möglichkeit aus, die Rohrlänge durch einen "Zug" (Teleskop-Rohr) zu verlängern (Zugtrompete, Posaune). Dadurch erschlossen sich weitere proportional verschobene Naturtonreihen. Das Gegenteil dazu bilden die danach entstanden Instrumente mit Tonlöchern oder Klappen (Klappenhorn, Ophikleide), bei denen die Luftsäule entsprechend verkürzt wird.

Die bedeutendste Innovation bildet jedoch die Erfindung der Ventilinstrumente um 1813 durch Friedrich Blühmel und Heinrich Stölzel, die seither die überwiegende Mehrzahl aller gängigen "Blechblasinstrumente" bilden.

Bald darauf setzte sich die klassische Konfiguration mit drei Ventilen durch, die den Grundton um jeweils zwei, einen und drei Halbtöne erniedrigen. Mit einem solchen dreiventiligen Instrument ist es möglich, ab einer Quinte über dem Grundton eine durchgehende chromatische Tonleiter zu spielen.

Ist noch ein weiteres Ventil vorhanden, so handelt es sich in der Regel um ein Quartventil (fünf Halbtöne). Historisch wurden manche Instrumente aus Gründen der Intonation auch mit fünf, sechs oder mehr Ventilen gebaut, eine Praxis, die sich bis heute bei der Tuba erhalten hat. (Weitere Informationen dazu finden sich unter Ventil (Blasinstrument) sowie den Artikeln zum jeweiligen Instrument selbst.)

Heutzutage werden vorwiegend Posaunen mit einem Zug (teilweise ergänzt durch ein oder zwei Ventile) gespielt. Klappeninstrumente werden überwiegend nur noch im Sinne der historischen Aufführungspraxis verwendet.

Zur Hilfe beim Intonieren schlecht stimmender Töne vor allem bei der Kombination mehrerer Ventile, wird bei diesen der Ventilverlängerungszug (mitunter auch der Hauptstimmzug) während des Blasens mit Hilfe einer Vorrichtung oder in Form eines sogenannten Trigger ausziehbar und somit veränderlich gestaltet.

Als das Grab des Pharao Tutanchamun aus dem Jahre 1323 v. Chr. entdeckt wurde, fand man auch zwei Exemplare des ältesten heute noch erhaltenen Blechblasinstrumentes, des "Scheneb". Diese trompetenartigen Instrumente sind ca. 58 cm lang, haben einen Durchmesser von 17 mm (Anblasseite) bis 26 mm und einen anschließenden Schalltrichter mit bis 88 mm. Gefertigt sind beide Instrumente aus getriebenem und verlötetem Blech: das eine aus teilweise vergoldetem Silber, das andere aus einer Kupferlegierung. Über Blastechniken und eine konkrete Verwendung ist nichts schriftlich überliefert, bildliche Darstellungen (vermutlich bereits ab ca. 2300 v. Chr.) stellen sie in einen militärischen oder repräsentativen Zusammenhang.

Ein weiteres Instrument aus diesem Kulturkreis ist die jüdische Chazozra. Im Kontext des Alten Testaments (Tanach) wird Mose von Gott nach dem Auszug aus Ägypten aufgefordert , zwei Trompeten aus getriebenem Silber zu fertigen. Verbunden damit ist an gleicher Stelle eine relativ ausführliche Vorschrift zur Anwendung. Der religiöse Gebrauch oblag den Leviten im Jerusalemer Tempel. Allerdings sind auch diese Trompeten reine Signalinstrumente und weder zum eigentlichen Musizieren gefertigt, noch dazu geeignet. Originale Instrumente sind wahrscheinlich nicht erhalten, die letzten dürften gemäß der Abbildung im Titusbogen der Plünderung des Tempels zum Opfer gefallen sein.

"Scheneb" und die "Chazozra" bestanden aus geschmiedeten und verlöteten Blechen. Gleichzeitig war auch die Kunst des Wachsausschmelzverfahrens bereits ab dem 4. vorchristlichen Jahrtausend bekannt. Mehrere Instrumente entstanden so:
Die Römer übernahmen ab ca. 300 v. Chr. aus der Kultur der Etrusker auch verschiedene Blechblasinstrumente aus gegossener Bronze mit abnehmbaren Mundstücken.

Ob die Kunst des Biegens dünnwandiger Rohre von der Antike durch das Mittelalter überliefert wurde oder im Abendland neu entdeckt werden musste, ist nicht mit Sicherheit geklärt. Frühmittelalterliche Instrumente waren gestreckt, die früheste Abbildung einer S-förmig gewundene Form ist auf einer Miniatur von 1377 (Cronicles of France) in der British Library zu sehen.

Als Standardform bildete sich ab ca. 1500 die einmal gewundene "Langtrompete" heraus, die als Barocktrompete bis zum Ende des 18. Jahrhunderts praktisch unverändert blieb. Da auf festen Röhren nur Naturtonreihen möglich sind und das Clarino-Spiel in der hohen Lage unüblich wurde, kam der Wunsch nach spielbaren Tönen zwischen den (tieferen) Naturtönen auf. Das Stopfen war für Trompeten nur beschränkt praktikabel. Auch der ständige Wechsel zwischen Instrumenten unterschiedlicher Stimmung war keine Verbesserung. Ein erster Schritt war die Veränderung der Rohrlänge durch aufgesteckte Rohrstücke (Setzstücke). Zur Erzeugung schnell aufeinander folgender Halbtonschritte wurde an den Instrumenten "(Tromba da tirarsi)" wahrscheinlich ein teleskopartig ausziehbares Mundrohr verwendet (Gemälde von Hans Memling, ca. 1480).

Die Entwicklung des ausziehbaren Doppelzuges und somit der eigentlichen Posaune fand wahrscheinlich Mitte des 15. Jahrhunderts in Burgund (Südfrankreich) statt. Zeitgleich entstanden dünnwandige Instrumente mit dem wesentlichen Merkmal des heutigen Waldhorns, der kreisrund gebogenen Röhre. Es finden sich Abbildungen solcher „Hörner“ auf Darstellungen in Worcester oder in Terlan im Tirol.

In heutigen Sinfonieorchestern sind die Blechblasinstrumente meistens in der Mitte hinten angeordnet.

Aus der Familie der Zinken, die in der Renaissance verbreitet war und nach heutiger Terminologie zu den Blechblasinstrumenten gehört, entwickelten sich weitere Formen von Grifflochhörnern. Die Tonhöhenveränderung des Grifflochhorns erfolgt analog einem Holzblasinstrument: Über Grifflöcher oder Klappen erreicht der Bläser eine Verkürzung der schwingenden Luftsäule. Der Klangcharakter der erzeugten Töne war nach den Maßstäben des 19. Jahrhunderts jedoch nicht so befriedigend wie der eines natürlichen Tons. Trotzdem hielten sich Serpent, Basshorn und die modernere Ophikleide bis weit ins 19. Jahrhundert. Mit der Ophikleide und der Tuba wurden Instrumente entwickelt, die auch im Bassbereich chromatisch spielbar waren. Aus dem italienischen Raum kamen Klappenhorn und Klappentrompete, die sich als volkstümliche Instrumente lange Zeit hielten.

Die Erfindung der Ventile seit den 1810er-Jahren veränderte die Bedeutung der Blechblasinstrumente und deren Stellenwert in der Musik. Die Oper "Rienzi" von Richard Wagner (UA 1842 in Dresden) verwendete bereits Ventiltrompeten, nach dem Vorbild der Cornets à pistons in der französischen Oper. Das chromatisch spielbare Kornett, in deutschen Sprachgebiet oft „Posthorn“ genannt, wurde zum beliebten Soloinstrument.

Über "Halbinstrumente" berichtete Karl Emil von Schafhäutl 1854 in einem Bericht von einer Industrieausstellung in München:

Als "Halbinstrument" bezeichnete er somit 20 Jahre nach Erfindung der Ventile ein engmensuriertes Blechblasinstrument, bei welchem der Pedalton (bzw. erste Naturton) schlecht anspricht, nicht spielbar und somit normal nicht verwendbar ist. Im Gegensatz dazu gibt es "Ganzinstrumente", bei denen der Pedalton gut verwendbar ist. Die Begriffe "Halbinstrument/Ganzinstrument" haben instrumentenbaupraktisch seit etwa 1900 keine Relevanz mehr, wahrscheinlich waren sie in den Anfangsjahren der Blechblasinstrumentenentwicklung nur entsprechend zitierte Werbeattribute.

Das Phänomen des extrem schlecht stimmenden (also „fehlenden“) Grundtons begründet sich aus den physischen Eigenschaften der Instrumentenform. Ein stark konischer Verlauf des Hauptrohrs, also eine weite Mensur unterstützt das gewünschte Frequenzverhältnis von 1:2 (Oktave) der ersten zwei Naturtöne, während eine weitgehend zylindrische Mensur (gleichbleibendes Rohr ohne Schallstück) dieses Verhältnis zu etwa 1:2,5 verschiebt. Der gewünschte Ton ist fünf Halbtöne zu tief, aber trotzdem (wenn auch schwierig) erzeugbar.

Dieser Begriff bezeichnet in der Praxis bei Blechblasinstrumenten den Notennamen des (1.,) 2., 4., 8. usw. Naturtones, unabhängig von dessen absoluter Oktavlage. Bläst beispielsweise eine B-Trompete und ein B-Tenorhorn den 3. Naturton, klingen beide Instrumente im Oktavabstand. Der Gesamtklang wird allgemein als angenehm empfunden. Bläst eine B-Trompete und eine C-Trompete beispielsweise den 2. Naturton, klingen beide Instrumente im Sekundabstand. Der Gesamtklang wird gemeinhin als unangenehm empfunden.

Die Grundstimmung wird festgelegt durch die Grundrohrlänge

Die Grundtonhöhe "f" in Hertz ist physikalisch abhängig von der Instrumentenrohrlänge "l" in Metern und der Schallgeschwindigkeit "c" der Luft.

Mit der Formel: formula_1 kann näherungsweise die Länge oder in der Umkehrung auch die Frequenz berechnet werden.

Alle Instrumente mit dem gleichen Grundton haben deshalb auch etwa die gleiche Rohrlänge. Beispielsweise sind die Rohrlängen des Waldhorns in B (274 cm), der Posaune (270 cm), des Tenorhorns (266 cm) und des Baritonhorns beziehungsweise Euphoniums in B (262 cm) fast gleich. Das Waldhorn in F ist mit 370 cm etwas länger als die Tuba in F (354 cm). Diese Längendifferenzen innerhalb der gleichen Grundstimmung hängen von der Bauweise des Instrumentes ab, insbesondere von der Mensur und dem Öffnungswinkel und Durchmesser des Schallstückes.

Das kürzeste gebräuchliche Blechblasinstrument ist die B-Piccolotrompete mit einer Grundrohrlänge von 65 cm. Die B-Tuba mit vier Ventilen ist das üblicherweise tiefste Instrument mit einer Grundrohrlänge von 541 cm, werden noch die Verlängerungen von normalerweise vier Ventilen hinzugerechnet, ergibt sich dabei eine Rohrlänge von 930 cm.

In einem normalen "separaten F/B-Doppelhorn" sind 704 cm Rohr verbaut, dabei werden dem B-Horn beim Umschalten in die F-Stimmung zusätzlich 96 cm hinzugefügt. Um Intonationskorrekturen vornehmen zu können, werden die F-Ventilzüge (Gesamtlänge 102 cm) unabhängig von den B-Ventilzügen verwendet.

"Siehe auch:" Grundstimmung (Blasinstrument).

Umgangssprachlich haben moderne Blechblasinstrumente unabhängig von ihrer Größe überwiegend die gleichen Baugruppen:
Der Stimmzug beeinflusst die Gesamtlänge des Instrumentes und besteht aus zwei parallel verlaufenden Teleskoprohren, verbunden durch einen Bogen. Deshalb ist auch die Bezeichnung "Stimmbogen" gebräuchlich. Zieht man den Stimmbogen weiter heraus, also verlängert man die Gesamtlänge des Instruments, dann werden die Töne tiefer. Schiebt man den Stimmbogen dementsprechend weiter hinein, werden die Töne höher.

Durch eine oder mehrere "Wasserklappen" (wenn vorhanden) kann in Pausen während des Musizierens das Kondenswasser aus dem Instrument schnell entfernt werden. Die Position der Wasserklappe(n) ist so gewählt, dass sie sich in Blashaltung am tiefsten Punkt eines Rohres befinden.

Blechblasinstrumente werden gewöhnlich aus Messingblech und -rohren mit einer Wandstärke von 0,4 bis 0,6 mm gefertigt. Schallstücke und große konische Rohre werden aus Blechen mit silberhaltigem Hartlot zusammengelötet und mit entsprechenden Werkzeugen in die gewünschte Form gedrückt. Zylindrische Rohre werden nahtlos industriell gefertigt. Die einzelnen eventuell gebogenen Rohrteile werden mit kurzen Rohrstücken („Zwingen“) überlappend mit Weichlot verlötet.

Traditionell werden von einem Metallblasinstrumentenmacher zu biegende Rohre mit flüssigem Blei gefüllt und nach Erkalten „per Hand“ gebogen. Mit speziellen Techniken wird die Oberfläche geglättet, dabei verdichtet und gehärtet. Anschließend wird das Blei verflüssigt und restlos entfernt. Dieser Biegevorgang kann auch mittels Metallen mit niedrigem Schmelzpunkt oder verflüssigbaren Kunstharzen erfolgen.

In der modernen industriellen Massenproduktion wird der Schallbecher aus einer Ronde tiefgezogen und bündig mit entsprechend konischem Stängel hartverlötet. Gebogen werden Rohre oft mit Wassereis-Füllung, geglättet werden diese danach hydraulisch in einer teilbaren Matrizenform. Das erfordert fertigungstechnisch größere Materialstärken (bis 1 mm), die Haltbarkeit fertiger Instrumente ist mitunter extrem gering: Durch extremes „Aufblasen“ zerreißen homogene Kristallstrukturen und bekommen Kapillarrisse, durch die bei einem fertigen Instrument unabdingbar Kondenswasser diffundiert. Lackierte Oberflächen verhindern das Verdunsten, es kommt zu irreversiblen Schäden am Instrument.

Fertiggestellte Einzelteile werden oftmals bereits vor dem Zusammenbau geschliffen und poliert. Das fertige Instrument kann abschließend lackiert oder galvanisch versilbert, vernickelt beziehungsweise vergoldet werden.



Wie die meisten Musikinstrumente werden Blechblasinstrumente nicht nur von großen Unternehmen hergestellt, sondern auch von kleinen, handwerklich hoch spezialisierten Fachbetrieben, die mitunter nur aus einem einzigen Metallblasinstrumentenmachermeister bestehen.




</doc>
<doc id="13631" url="https://de.wikipedia.org/wiki?curid=13631" title="Harz (Mittelgebirge)">
Harz (Mittelgebirge)

Der Harz, bis ins Mittelalter "Hart" (‚Bergwald‘) genannt, ist ein Mittelgebirge in Deutschland und das höchste Gebirge Norddeutschlands. Er liegt am Schnittpunkt von Niedersachsen, Sachsen-Anhalt und Thüringen. Anteil am Harz haben im Westen die Landkreise Goslar und Göttingen, im Norden und Osten die Landkreise Harz und Mansfeld-Südharz und im Süden der Landkreis Nordhausen. Der Brocken ist mit der höchste Berg des Harzes und Sachsen-Anhalts.

Im Harz, der von artenreicher Flora und Fauna geprägt ist, gibt es ausgedehnte Wälder, teils landwirtschaftlich genutzte Hochflächen, tief eingeschnittene Täler mit wilden Flussläufen und Wasserfällen sowie Stauteiche und Stauseen. Vielerorts gibt es Zeugnisse einer langen Siedlungsgeschichte. Zudem sind Wintersportgebiete vorhanden, und der Harz ist Wandergebiet.

Der Harz enthält den Nationalpark Harz, drei Naturparks (Harz (Niedersachsen), Harz/Sachsen-Anhalt und Südharz) und das Biosphärenreservat Karstlandschaft Südharz. Das Mittelgebirge liegt im Südteil des Geoparks Harz – Braunschweiger Land – Ostfalen.

Im Harz und in seiner unmittelbaren Umgebung befinden sich mit Goslar, Quedlinburg, der Lutherstadt Eisleben und dem Oberharzer Wasserregal zahlreiche UNESCO-Weltkulturerbestätten.

Das Gebirge ist 110 Kilometer lang und 30 bis 40 Kilometer breit, bedeckt eine Fläche von 2.226 km² und reicht von Seesen (im Westen) bis zur Lutherstadt Eisleben (im Osten). Der größte Anteil des Harzes liegt in Sachsen-Anhalt (Landkreise Harz und Mansfeld-Südharz), der Westteil in Niedersachsen (Landkreise Goslar und Göttingen); nur ein kleiner Teil im Süden liegt in Thüringen (Landkreis Nordhausen).

Der Harz ist eine eigenständige naturräumliche Großregion 3. Ordnung und Haupteinheitengruppe; er trägt im Handbuch der naturräumlichen Gliederung Deutschlands die Kennziffer "38", beim Bundesamt für Naturschutz unter Übernahme derselben Grenzen die Kennziffer "D37".

Den deutlich herausmodellierten Höhenschwerpunkt des Mittelgebirges bildet der "Hochharz" um den Brocken (), der alle Gipfel über 800 Metern Höhe enthält, darunter insbesondere den Wurmberg () bei Braunlage, den Höhenzug der ineinanderübergehenden Erhebungen Auf dem Acker () und Bruchberg (ca. ) und die Achtermannshöhe ().

Eine bereits relativ alte Bezeichnung ist die des Oberharzes, die historisch eigentlich das Gebiet der sieben Bergstädte mit Clausthal-Zellerfeld als Zentrum meinte, wobei das "Ober-" eine Abgrenzung von Orten an Randlagen darstellte. Im engeren physisch-geographischen Sinne indes bezeichnet er den geologisch scharf umgrenzten, über Söse, Innerste nebst Grane und obere Oker nebst Abzucht entwässerten Nordwestteil des Harzes. Damit läge die Bergstadt St. Andreasberg als einzige außerhalb des physischen Oberharzes.

Als Gegensatz zur Bezeichnung Oberharz etablierte sich die Bezeichnung "Unterharz" für den östlich des Hochharzes gelegenen, hauptsächlich zur Bode, im Süden auch zu linken Zuflüssen der Helme entwässernden Teil des Harzes.

In den Arbeiten des Instituts für Landeskunde im Rahmen des Handbuchs der naturräumlichen Gliederung Deutschlands wurde in den 1950er Jahren schließlich die Einheit "Mittelharz" eingeführt, die neben dem Hochharz seine nördliche, östliche und südliche Abdachung zur Oker (ohne Quellläufe) mit Radau, Ecker und Ilse; oberer Holtemme nebst Zillierbach; Kalter und Warmer Bode; oberer Oder und Sieber beinhaltete. Überdies wurde der zur Wipper entwässerte Ostteil unter dem Namen "Östliche Harzabdachung" vom Südharz abgetrennt. Diese Gliederung in vier bzw. fünf sogenannte "Haupteinheiten" (Oberharz, Hochharz, Mittelharz, Unterharz, Östliche Harzabdachung) wird auch heute noch vom Bundesamt für Naturschutz zu Grunde gelegt. Sie ist indes in ihren Grenzen deutlich unschärfer als z. B. die des Thüringisch-Fränkischen Mittelgebirges, innerhalb der jede Haupteinheit gleichzeitig eine in sich geschlossene geologische und orographische Einheit darstellt. Insbesondere in den zentralen Hochflächen sind die Übergänge zwischen den drei östlicheren Einheiten des Harzes fließend; lediglich Ober- und Hochharz sind physisch überall deutlich von den Nachbareinheiten abgegrenzt.

Auch in der Geologie hat sich inzwischen eine Gliederung in Ober-, Mittel- und Unterharz etabliert, welche indes deutlich von den landläufigen und naturräumlichen Zuordnungen abweicht. Entspricht der Oberharz hier noch weitgehend dem naturräumlichen, enthält der "Mittelharz" hier neben dem Mittelharz im engeren Sinne um das Brockenmassiv auch noch die komplette mittlere Nordabdachung nach Osten bis einschließlich des Rambergs.

Einige der geologischen Einheiten, speziell etwa die Massive von Brocken und Ramberg, der Oberharzer Devonsattel (mit Wolfshagener Ausraumbecken im Westen und stärker angehobenem Ostteil), der Acker-Bruchberg-Zug und das Ilfelder Becken, formen geomorphologisch deutlich abgegrenzte Landschaftseinheiten. Dieses gilt jedoch speziell für die Einheiten des Dinantium nicht. So existieren im Oberharz deutliche Unterschiede zwischen dem Westrand, dem Innerstetal, den Hochflächen und der Sösemulde. Ähnlich deutlich stellen sich im geologischen Unterharz die Gegensätze zwischen der zentralen Hochfläche und den stark reliefierten Abdachungen nach Süden dar. Da die Lebensräume im Harz überdies stark durch die Flusssysteme geprägt sind, dürfte speziell die geologische Abgrenzung zwischen Mittel- und Unterharz wenig mit der tatsächlichen Vegetation sowie dem Landschaftserleben zu tun haben – zumal nur ein kleiner Teil aller anstehenden Gesteine wirtschaftlich von Bedeutung ist und deshalb die Landschaft durch den Bergbau nicht entscheidend verändert wurde.

In den Jahren 1963 (äußerster Westen) und vor allem 1970 wurde der Harz, vom äußersten Ostrand abgesehen, im Maßstab 1:200.000 in Einzelnaturräume gegliedert. Da die Nummernverteilung der beiden Einzelbearbeiter voneinander abweicht, sind die unten angegebenen Naturraumnummern nur als Hilfe beim Lesen der "jeweiligen" Quelle (verlinkte Karten) zu verstehen.

Hauptquellen der naturräumlichen Grenzziehung sind:
Dabei wurde aufgrund der z. T. einander konträren Quellenlage wie folgt eingeteilt:
Es ergibt sich insgesamt folgende Einteilung (Haupteinheiten von Westen nach Osten, innerhalb derselben je von Nord nach Süd bzw. im Uhrzeigersinn geordnet):

Der Brocken () ist der höchste Berg des gesamten Mittelgebirges Harz und von Sachsen-Anhalt, der Wurmberg () ist der höchste im niedersächsischen Harzteil und zugleich im gesamten Niedersachsen und der Große Ehrenberg () ist der höchste im thüringischen Harzteil.

Im Harz gibt es einen Nationalpark und drei rechtlich eigenständige Naturparks. Innerhalb der Parks befinden sich zahlreiche Natur- und Landschaftsschutzgebiete. Die Parks liegen im Geopark Harz – Braunschweiger Land – Ostfalen.

Der Nationalpark Harz, der 2006 gegründet wurde und 247,32 km² (24.732 ha) groß ist, liegt in Niedersachsen (Landkreise Goslar und Göttingen) und Sachsen-Anhalt (Landkreis Harz). Er wurde als erster länderübergreifender Nationalpark Deutschlands aus den bis dahin bestehenden Nationalparks "Harz" (ca. 158 km² in Niedersachsen) und "Hochharz" (ca. 89 km² in Sachsen-Anhalt) gebildet.

Der Naturpark Harz (Niedersachsen) (NSG-Nr. 319376), der 1960 gegründet wurde und etwa 790 km² (79.000 ha) groß ist, erstreckt sich in den niedersächsischen Landkreisen Goslar und Göttingen; er grenzt an den Nationalpark Harz. Der Naturpark Harz/Sachsen-Anhalt (NSG-Nr. 33370), der 2003 gegründet wurde und etwa 1660 km² (166.054 ha) groß ist, liegt in den sachsen-anhaltischen Landkreisen Harz und Mansfeld-Südharz; er enthält Teile des Nationalparks Harz. Der Naturpark Südharz (NSG-Nr. 390676), der 2010 gegründet wurde und etwa 267 km² (26.700 ha) groß ist, breitet sich im thüringischen Landkreis Nordhausen aus.

Das Biosphärenreservat Karstlandschaft Südharz, das 2009 gegründet wurde und 300,34 km² (30.034 ha) groß ist, liegt im Landkreis Mansfeld-Südharz (Sachsen-Anhalt). Es schützt eine Gipskarstlandschaft mit Zechsteinablagerungen.

Der Harz gilt als das geologisch vielfältigste der deutschen Mittelgebirge, wobei Sedimentgesteine, die schwach metamorph verändert wurden, bei weitem überwiegen. Die häufigsten, an der Oberfläche anstehenden Gesteine sind Tonschiefer, geschieferte Grauwacken und der in zwei Plutonen anstehende Granit. Die im Harz weit verbreitete Gießen-Harz-Decke des Rhenoherzynikums besteht zu großen Teilen aus Flysch. Bekannt und wirtschaftlich bedeutend sind die Kalksteinvorkommen um Elbingerode und der Gabbro von Bad Harzburg. Die Landschaften des Harzes sind durch steile Bergketten, Blockhalden, vergleichsweise flache Hochebenen mit vielen Hochmooren und langgestreckte, schmale Kerbtäler charakterisiert, von denen das Bodetal, das Oker- und Selketal die bekanntesten sind. Ein repräsentativer Querschnitt aller Harzer Gesteine wird auf der Jordanshöhe bei Sankt Andreasberg nahe dem Parkplatz gezeigt.

Die Auffaltung des Harzes vollzog sich im Oberkarbon im Zuge der variszischen Gebirgsbildung vor 320 bis 300 Millionen Jahren. Der Harz gehört zur so genannten rhenohercynischen Zone der europäischen Varisziden. Im Verlauf der Erdgeschichte wurde das Gebirge stark erodiert und später teilweise von mesozoischen und känozoischen Gesteinen überdeckt. Ab dem Oberen Jura kommt es infolge der Fernwirkung der Alpidischen Gebirgsbildung zu verstärkten tektonischen Bewegungen und entlang großer Bruchstrukturen der so genannten Harznordrandverwerfung zu einer Heraushebung des Harzes (Saxonische Gebirgsbildung, subherzyne Phase).

Neue Forschungen aus dem Jahr 2011 datieren den Brockengranit auf ein Alter von 293 Millionen Jahren. Die Entstehung des Brockengranits ist demnach mit der Entstehung und dem beginnenden Zerfall des Superkontinents Pangäa in Zusammenhang zu bringen. So soll die Entstehung des Brockengranits nicht mit der sogenannten variszischen Gebirgsbildung in Verbindung stehen, die bereits 40 Millionen Jahre früher (also 330 Millionen Jahre vor heute) abgeschlossen war.

Der Harz ist ein (Pult-)Schollengebirge, das nach Westen und Nordosten verhältnismäßig steil abfällt und sich nach Süden allmählich abflacht. Es wird von zahlreichen tiefen Tälern durchschnitten. Nördlich des Gebirges liegen kreidezeitlichen Schichten der subhercynen Mulde im ausgedehnten Harzvorland; südlich des Gebirges lagern permische Sedimente flach auf den nach Südwesten einfallenden paläozänen Sedimenten.

Aufgrund der Harzrandverwerfung und der senkrecht oder teilweise sogar überkippten geologischen Schichten ist in dem nur relativ wenige Quadratkilometern umfassenden Gebiet eine teilweise häufig wechselnde geologische Beschaffenheit zu beobachten. Infolge dieser Tatsache bezeichnet man Teile des Nordwestharzes auch als „Klassische Quadratmeile der Geologie“.

Klimatisch hebt sich ein Gebirge durch geringere Temperaturen und größere Niederschläge von den Randlandschaften ab. Regelmäßige Niederschläge während des ganzen Jahres prägen den Mittelgebirgscharakter des Harzes. Regenreichen atlantischen Westwinden frei ausgesetzt, fallen auf der Luv-Seite bis zu 1.600 mm Regen im Jahr (Westharz, Oberharz, Hochharz), hingegen fallen, gebirgstypisch, auf der Lee-Seite durchschnittlich nur 600 mm Niederschlag im Jahr (Ostharz, Unterharz, Östliche Harzabdachung).

Mit einer durchschnittlichen Erwärmung von über 1 Grad in den vergangenen 100 Jahren auf dem Brocken und den entsprechenden Folgen für die Ökosysteme wie die Borkenkäferentwicklungen lässt sich der globale Klimawandel auch im Harz feststellen.

Die Flüsse des Harzes haben aufgrund der Geographie und des Klimas stark schwankende Wasserführungen und können bei den hohen Niederschlägen große Wassermengen führen. Der lukrative Silberbergbau im Oberharz führte dort insbesondere zwischen dem 16. bis zum 19. Jahrhundert zu umfangreichen Veränderungen der Flüsse im Quellgebiet von Innerste, Oker, Oder und Söse: Das Oberharzer Wasserregal mit 143 kleinen Talsperren, die zu einem großen Teil noch heute in Betrieb sind, prägt nicht nur die Gewässer, sondern die gesamte Landschaft im Westharz. Zu den Oberharzer Teichen gehören die ältesten noch in Betrieb befindlichen Talsperren Deutschlands. Diese Teiche und Gräben stehen seit 1977 unter Denkmalschutz und wurden 2010 als UNESCO-Weltkulturerbe anerkannt.
Auch im Unterharz finden sich viele Spuren montaner Wasserwirtschaft. Neben wasserführenden Gräben und Teichen existieren auch längst aufgegebene Gräben und trockene Teiche. Die erhaltenen Teile davon wurden 1991 als Flächendenkmal Unterharzer Teich- und Grabensystem unter Schutz gestellt und dienen heute zum Teil als Trinkwasserreservoir. Teile der Anlagen können bis ins Jahr 1320 zurückdatiert werden.

Im 20. Jahrhundert wurde ein System von 16 Talsperren im Harz errichtet, das zwölf Harzflüsse anstaut. Bis heute dienen die Talsperren überwiegend der Trinkwassergewinnung, der Stromerzeugung, dem Hochwasserschutz sowie der Niedrigwasseraufhöhung. Mit der Sösetalsperre, die von 1928 bis 1931 erbaut wurde, begann der moderne Talsperrenbau im Harz.

Die größten Flüsse des Harzes sind im Norden die Innerste, die Oker und Bode, im Osten die Wipper und im Süden die Oder. Die Innerste speist die Leine und hat als Zuflüsse die Laute, Grane und Nette. In die Oker münden die Flüsse Radau, Ecker und Ilse. Die Bode wird durch die Holtemme, in die wiederum der Zillierbach mündet, Hassel und Selke gespeist. Die Wipper nimmt die Eine auf. In die Rhume münden die Söse und Oder, welcher die Sieber zufließt; in letztere fließt die Lonau ein. Die südlich des Harzes verlaufende Helme wird von der durch die Steina verstärkte Ichte gespeist, von der aus dem Gebirge kommenden Zorge, welche die von der Uffe verstärkte Wieda und die Bere aufnimmt, und von der Thyra, welcher der Krebsbach zufließt.

Die Vegetationszonierung des Harzes umfasst sechs Höhenstufen– sortiert nach Höhe in Meter (m) über Normalhöhennull (NHN):

Vom Harzrand bis Höhe dominieren Buchenwälder, insbesondere die Hainsimsen-Buchenwälder ("Luzulo-Fagetum") der schwach nährstoffversorgten Standorte mit Rotbuche ("Fagus sylvatica") häufig als alleiniger Baumart. In tieferen, trockenen Lagen kommen Stieleiche ("Quercus robur") und Traubeneiche ("Quercus petraea") hinzu. Auf feuchteren Standorten tritt Bergahorn ("Acer pseudoplatanus") auf. In den lichtreichen Zerfalls- und Verjüngungsphasen spielen auch lichtbedürftige Pioniere wie Eberesche ("Sorbus aucuparia"), Hänge-Birke ("Betula pendula") und Sal-Weide ("Salix caprea") eine Rolle. Der Perlgras-Buchenwald ist an den wenigen nährstoff- und basenreicheren Standorten über Diabas und Gneis zu finden und weist eine arten- und blütenreichere Krautschicht auf. Auch hier dominiert die Rotbuche, beigemischt Bergahorn, Gewöhnliche Esche ("Fraxinus excelsior"), Hainbuche ("Carpinus betulus") und Bergulme ("Ulmus glabra"). Durch das zunehmende Kontinentalklima am östlichen Harzrand wird dort die Rotbuche zugunsten von Traubeneichen-Mischwäldern verdrängt.

In den mittleren Lagen zwischen 700 und Höhe würde man unter natürlichen Bedingungen von Fichte ("Picea abies") und Rotbuche dominierte Mischwälder antreffen. Diese sind jedoch bewirtschaftungsbedingt seit langem bis auf Reste Fichtenbeständen gewichen. Weiterhin tritt in diesen Wäldern der Bergahorn auf.

In den höchsten Lagen von etwa 800 m bis zur Waldgrenze bei Höhe gedeihen Fichtenwälder, in denen auch Laubgehölze wie Ebereschen, Hänge- und Moorbirke ("Betula pendula" und "Betula pubescens") sowie Weiden ("Salix" spec.) zu finden sind. Die hohe Luftfeuchtigkeit ist die Ursache für eine reiche Moos- und Flechtenflora. Trotz der Naturnähe findet man nur noch wenige heimische, genetisch angepasste (autochthone) Fichten. Wollreitgras-Fichten-Wälder ("Calamagrostio villosae-Piceetum") dominieren. Auf frischen, aber keineswegs nassen und nur mäßig gesteinsreichen Böden gedeiht eine gut entwickelte Bodenvegetation, die in ihrem Erscheinungsbild vor allem durch Gräser wie Wolliges Reitgras ("Calamagrostis villosa") und Draht-Schmiele ("Deschampsia flexuosa") geprägt ist. Die Böden in den Hochlagen sind wie auch im überwiegenden Teil des gesamten Harzes vergleichsweise nährstoff- und basenarm, so dass nur wenige krautige Pflanzen wie das Harzer Labkraut ("Galium saxatile") vorkommen. Dafür sind es eher Farne, Moose, Flechten und Pilze, die neben der Fichte die Eigenart dieser Wälder bestimmen. Im Bereich verwitterungsresistenter Gesteine in der hochmontanen und montanen Stufe kommen häufig Felsen und Blockhalden vor – Extremstandorte der Vegetation. Aufgrund des Mangels an Erdmaterial gedeihen auf ihnen nur schwachwüchsige, sehr licht stehende Block-Fichtenwälder. Sie zeichnen sich durch einen besonders hohen Strukturreichtum aus und lassen mehr Raum für lichtliebende Arten wie Hänge-Birke, Eberesche, Bergahorn, Weiden und Zwergsträucher wie die Heidelbeere ("Vaccinium myrtillus"). Auch Moose und Farne sind hier häufig. Als Besonderheit ist die Karpatenbirke ("Betula pubescens subsp. carpatica") zu nennen. In der Umgebung der Hochmoore auf Sumpf- und Moorböden finden sich die Moor-Fichtenwälder. Auf diesen Standorten können Fichtenwälder ausnahmsweise auch in tieferen Lagen die natürliche Waldgesellschaft bilden. Diese durch besondere Nässe geprägten Moorwälder weisen bereits einen hohen Anteil an Torfmoosen ("Sphagnum" spec.) auf. Die Bodenvegetation kann aber auch durch ein reiches Vorkommen von Zwergsträuchern wie Preiselbeere ("Vaccinium vitis-idaea") geprägt sein. Ebenso sind umfangreiche Bestände des Blauen Pfeifengrases ("Molinia caerulea") für diese Waldgesellschaft typisch. Charakteristische Pilzarten der natürlichen Fichtenwälder sind der Dünne Feuerschwamm ("Phellinus viticola") und der Olivgelbe Holzritterling ("Tricholomopsis decora").

Nur kleinflächig treten Schlucht-, Au- und Quellwälder auf. Die Rotbuche tritt hier zugunsten anspruchsvollerer Laubbaumarten wie Bergahorn, Sommer-Linde ("Tilia platyphyllos"), Berg-Ulme oder Gewöhnliche Esche zurück. Die Krautschicht ähnelt der von besser nährstoffversorgten Buchenwäldern. Auffällige Vertreter der hier beheimateten Pflanzengesellschaften sind Alpen-Milchlattich ("Cicerbita alpina"), Mondviole ("Lunaria rediviva"), Dorniger Schildfarn ("Polystichum aculeatum") und Buchenfarn ("Phegopteris connectilis").

Die Harzer Moore zählen zu den besterhaltenen Mitteleuropas. Ihre Entstehung geht zurück bis auf das Ende der letzten Eiszeit vor über 10.000 Jahren. Wesentlichen Anteil an der Vegetation der Hochmoore haben die Torfmoose. Die feuchteren Bereiche (Schlenken) und die höherliegenden trockeneren Bulten werden von unterschiedlichen Arten besiedelt. In Schlenken findet sich das Spieß-Torfmoos "(Sphagnum cuspidatum)", auf den Bulten Magellans Torfmoos "(Sphagnum magellanicum)". Die Torfmoospolster werden von den Zwergsträuchern Heidelbeere, Preiselbeere und weiteren Zwergsträuchern durchwachsen. Die Rosmarinheide ("Andromeda polifolia") ist ein Eiszeitrelikt. Weitere Eiszeitrelikte sind Zwerg-Birke ("Betula nana") oder Wenigblütige Segge ("Carex pauciflora"). Von Mai bis Juni blüht die Gewöhnliche Moosbeere ("Vaccinium oxicoccus"). An den schwarzen Früchten ist die Schwarze Krähenbeere ("Empetrum nigrum") zu erkennen. Auf den trockeneren Bulten ist die Besenheide ("Calluna vulgaris") zu finden. Vereinzelt tritt die Glockenheide ("Erica tetralix") auf. Typische Gräser sind das Scheiden-Wollgras ("Eriophorum vaginatum"), bekannt durch die leuchtend weißen Fruchtstände und die Rasige Haarsimse ("Scirpus cespitosus"), die im Herbst rostrot ist. Die faszinierendste Moorpflanze ist der Rundblättrige Sonnentau ("Drosera rotundifolia"). Am trockeneren Moorrand ist die Moor- oder Rauschbeere ("Vaccinium uliginosum") zu finden.

In den Harzer Buchenwäldern findet eine Vielzahl von Tieren Lebensraum. Über 5.000 Tierarten, die meisten davon Insekten, sind auf den Buchenwald angewiesen. Darunter befinden sich viele Arten, die im Boden und in der Streuschicht für die Zersetzung und Einarbeitung des Laubes sorgen, wie zum Beispiel Springschwänze, Hornmilben, Asseln, Fadenwürmer, Tausendfüßer, Regenwürmer und Schnecken. Charakteristische Brutvögel der altholzreichen Buchenwälder sind Schwarzspecht ("Dryocopus martius") und Hohltaube ("Columba oenas"). Zeichen für die Naturnähe der Buchenwälder im Harz ist auch die Rückkehr des Schwarzstorchs ("Ciconia nigra"). Dieser scheue und störanfällige Bewohner reich strukturierter Laub- und Mischwälder war durch Beeinträchtigungen seines Lebensraums (fehlende Altbäume, Mangel an naturnahen Bächen) in Mitteleuropa sehr selten geworden. Durch Lebensraumverbesserungen infolge der Renaturierung von Fließgewässern sowie der Förderung von störungsarmen Ruhezonen hat sich der Schwarzstorchbestand heute erholt. Eine typische Säugetierart dieser Laubwälder ist die Europäische Wildkatze ("Felis silvestris silvestris"), die über eine stabile Population im Harz verfügt. Sie bevorzugt die strukturreicheren Waldgebiete, die ein reiches Nahrungsangebot aufweisen.

Auch die Tierwelt der Buchen-Fichtenmischwälder ist vielfältig. Besonders Arten, die auf strukturreiche Wälder angewiesen sind, kann man hier antreffen. So ist der Bergmischwald der natürliche Lebensraum des Auerhuhns ("Tetrao urogallus"). Lebensraum findet hier auch der Raufußkauz ("Aegolius funereus"). Er brütet fast ausschließlich in Schwarzspechthöhlen in alten Buchen und braucht den im Vergleich zum Fichtenwald lichteren Buchenwald mit seiner höheren Kleinsäugerdichte für seine Nahrungssuche. Zur Deckung bevorzugt er jedoch die dunkleren Fichten.

Ein großer Teil der in den natürlichen Fichtenwäldern lebenden Tierarten ist an die besonderen Lebensbedingungen in den Harzhochlagen angepasst. Für die Vogelwelt gelten Haubenmeise ("Lophophanes cristatus"), Winter- und Sommergoldhähnchen ("Regulus regulus" und "Regulus ignicapillus"), Erlenzeisig ("Carduelis spinus"), Waldbaumläufer ("Certhia familiaris"), Tannenmeise ("Periparus ater") und Fichtenkreuzschnabel ("Loxia curvirostra") als typische Bewohner. Besonders hervorzuheben ist hier der Sperlingskauz ("Glaucidium passerinum"), der die submontane bis subalpine Stufe mit struktur- und nadelbaumreichen Wäldern sowie eingestreuten Freiflächen bewohnt. Als Brutbaum wird die Fichte bevorzugt, als Nahrungsgebiete dienen lichte Waldbestände oder Moorbereiche. Ähnlich wie für den Schwarzstorch gilt auch für den vor langer Zeit aus dem Harz verschwundenen Sperlingskauz, dass er Ende der 1980er Jahre von selbst zurückgekehrt ist, als sich sein angestammter Lebensraum wieder zu mehr Naturnähe entwickelt hatte, so dass für ihn neben genügender Nahrung (Insekten, Kleinsäuger, Kleinvögel) auch stehendes Totholz (Fichten mit Spechthöhlen) vorhanden waren.

Neben vielen Vogelarten gibt es in den verschiedenen Fichtenwäldern auch eine Reihe von Großschmetterlingen, die außerhalb des Harzes stark gefährdet sind oder gar nicht vorkommen. Beispielhaft seien hier zwei Arten genannt. In alten, lichten Wollreitgras-Fichtenwäldern, zum Teil in Verbindung mit Blockhalden oder Moor-Fichtenwäldern, tritt der Braungraue Bergwald-Steinspanner ("Elophos vittaria") auf, in heidelbeerreichen Moor-Fichtenwäldern dagegen eher der Bläuliche Heidelbeer-Blattspanner ("Entephria caesiata").

Nur wenige Tierarten konnten sich an die extremen Bedingungen in Hochmooren anpassen. Beispiele dafür sind die Alpen-Smaragdlibelle ("Somatochlora alpestris"), die im Harz ihr einziges Vorkommen in Niedersachsen hat und in Deutschland stark gefährdet ist, und die vom Aussterben bedrohte Hochmoor-Mosaikjungfer ("Aeshna subarctica").

Felsen und Blockhalden sind wesentlicher Bestandteil des Lebensraumes von Wanderfalke ("Falco peregrinus") und Ringdrossel ("Turdus torquatus"). Der Wanderfalke braucht steile, aus der Landschaft herausragende, wenig bewachsene Felsen. Nachdem seine Population im Harz erloschen war, gibt es dort nun wieder Brutpaare. Entscheidend dazu beigetragen haben umfangreiche Maßnahmen zur Förderung von Ruheräumen in angestammten Brutgebieten dieser scheuen Vogelart. Bereits 1980 siedelte sich im Ostharz ein Brutpaar aus einem Auswilderungsprojekt an. Die Ringdrossel bevorzugt halboffene Blockhalden und locker bewaldete Übergangsbereiche zwischen baumfreien Hochmooren und Wald. Im Harz liegt eines ihrer wenigen isolierten Brutvorkommen Mitteleuropas. Ihr Hauptverbreitungsgebiet erstreckt sich auf Nordwesteuropa einschließlich großer Teile Englands und Schottlands sowie auf die Hochgebirge Süd- und Osteuropas.

Die Fließgewässer mit ihrem ausgeprägten Bergbachcharakter spielen eine bedeutende Rolle im gesamten Harz. Im Vergleich zu anderen naturräumlichen Regionen in Niedersachsen sind sie noch sehr naturnah und zeichnen sich durch hohe Strukturvielfalt und sauberes Wasser aus. Aufgrund der zumindest zeitweise hohen Fließgeschwindigkeit der Harzbäche können Blütenpflanzen in den Gewässern nur sehr selten Fuß fassen. Auch die Tiere der Harzer Fließgewässer müssen sich an die hohen Fließgeschwindigkeiten anpassen. Nur wenige Arten, zum Beispiel Fische, schwimmen aktiv gegen die Strömung an. Häufigste Arten sind Bachforelle ("Salmon trutta fario") und Groppe ("Cottus gobio"). Weit vielfältiger ist dagegen das Artenspektrum des Lückensystems unter der Gewässersohle. Neben sich hier entwickelnden Insekten- und Fischlarven findet man Einzeller, Strudelwürmer und Wassermilben. Andere Tierarten haften an Steinen fest, Köcherfliegenlarven und Schnecken, oder können durch extrem abgeflachte Körperform die verringerte Strömungsgeschwindigkeit am Grund des Gewässers oder an Steinen ausnutzen, Steinfliegenlarven. In strömungsberuhigten Bereichen hinter Steinen oder in Moospolstern findet man außerdem Wasserkäfer und Flohkrebse.

An den Fließgewässern des Harzes kann man vereinzelt die Großlibelle Zweigestreifte Quelljungfer ("Cordulegaster boltonii") sowie die Blauflügel-Prachtlibelle ("Calopteryx virgo"), eine Kleinlibelleart, antreffen.

Die Wasseramsel ("Cinclus cinclus"), die überall an den Harzgewässern zu finden ist, kommt fast ausschließlich im Bergland vor. Ihr Lebensraum sind schnell fließende, klare und mit Ufergehölzen bestandene Gebirgsbäche. Sie kann tauchen und unter Wasser auf dem Grund laufen. Zur Nahrungssuche dreht sie dort Steine um. Auch die Gebirgsstelze ("Motacilla cinerea") nutzt die reichen Nahrungsvorräte der Bergbäche.

Seit 2000 werden vom Nationalpark Harz erfolgreich Eurasische Luchse ausgewildert, die sich mittlerweile gut in die Ökologie einpassen. Durch gezielte Schutzmaßnahmen der vergangenen Jahre konnte ein Rückgang der Fledermausbestände im Harz gestoppt werden. Von den jagdbaren Säugetieren sind Rothirsch, Reh, Wildschwein und Europäischer Mufflon zu nennen.

Vor 700.000 bis 350.000 Jahren jagte der Homo erectus bilzingslebensis im und um den Harz bei Bilzingsleben (Thüringen), Hildesheim und Schöningen (Niedersachsen). Der Neandertaler erschien vor rund 250.000 Jahren in der Region und jagte Auerochsen, Wisente, Braun- und Höhlenbären, Mammuts, Nashörner, Pferde, Rentiere und Waldelefanten. Nachgewiesen wurden die Werkzeuge der Neandertaler in der Einhornhöhle im Südharz (100.000 Jahre vor heute) und in den Rübeländer Höhlen. Funde von Birkenpech bei Aschersleben am Nordharzrand wiesen die Verwendung dieses vorgeschichtlichen Klebstoffs durch Neandertaler vor 50.000 Jahren nach. Die Paläolithische Revolution brachte vor 40.000 Jahren den Menschen (Homo sapiens) aus Afrika nach Europa und auch in die Harzregion; er verdrängte den Neandertaler und wurde später auch hier sesshaft.

Viele Funde im Harz, wie beispielsweise die Bronzekeule von Thale, die bei der Roßtrappe gefunden wurde, könnten auf eine frühe kultische Nutzung des Harzes hinweisen. Bei Ührde im Südwestlichen Harzvorland wurden steinzeitliche Siedlungsspuren entdeckt.

Bis zum 3. Jahrhundert v. Chr. siedelten keltische Stämme um den Harz, bevor diese von Rhein-Weser- und Elbgermanen verdrängt wurden. Vermutlich lebten westlich vom heutigen Wernigerode Cherusker, während östlich davon vermutlich die Fosen lebten. Einflüsse von Slawen sind im Harz nicht vorhanden. Im Südharz lebten auch die Thüringer.

Archäologische Untersuchungen belegen einen frühen Bergbau im Harz, der sich sicher bis in das 3. Jahrhundert n. Chr. datieren lässt, aber wohl deutlich älter ist und schon in der Bronzezeit begann. Ptolemäus erwähnt den Harz mit der Bezeichnung Μηλίβοκον ("Mēlíbokon").

Der Harzgau selbst wird zuerst in einer Urkunde des Kaisers Ludwig des Frommen aus dem Jahre 814, in der hochdeutschen Form "Hartingowe", genannt. Nach den Jahrbüchern von Fulda zum Jahre 852 wurde der Harzgau von den Haruden bewohnt und nach ihnen der Harudengau ("Harudorum pagus") genannt. Harud, woraus Hard, Hart, Harz wurde, bedeutet Wald, Waldgebirge, und die Haruden sind die An- oder Bewohner des Harud.

Jüngeren Ursprungs sind die Siedlungen mit der Namensgebung –rode, die erst seit der Mitte des 9. Jahrhunderts im Harzgau nachweisbar sind. Woher die Gründer dieser Orte kamen, ist unbekannt.

Karl der Große erklärte den Harz zum Reichsbannwald. Der "Sachsenspiegel", das älteste deutsche Rechtsbuch, um 1220/30 wohl auf der Burg Falkenstein im Selketal verfasst, schrieb den Reichsbann später fest: Wer durch den Harzwald ritt, der hatte Bogen und Armbrust zu entspannen und die Hunde anzuleinen – nur gekrönte Häupter durften hier jagen. Der "Sachsenspiegel" des Eike von Repgow, nach dem über Jahrhunderte deutsches Recht gesprochen wurde, bezeichnet den Harz als Stätte, "wo den wilden Tieren Schutz in des Königs Bannforsten gewährt wird"; zu den drei beschriebenen Bannforsten im Lande der Sachsen hatte daher nicht jedermann freien Zutritt.

Für ewig hielt dieser Bann allerdings nicht. Bergbau, Hüttenindustrie, Wasserwirtschaft, zunehmende Besiedlung, Rodungen, Vieheintrieb, Landwirtschaft und später der Fremdenverkehr untergruben den kaiserlichen Schutz.

Bereits 1224 erwarben die 1129 in Walkenried eingezogenen Mönche umfangreichen Waldbesitz im Westharz, um das ihnen 1157 von Friedrich Barbarossa zugesprochene Viertel der Rammelsberger Erzausbeute wirtschaftlich zu sichern. Man kann daher bereits zu dieser Zeit von einer Holzverknappung ausgehen. Vom 12. bis zum Ende des 14. Jahrhunderts waren große Teile des Harzes wirtschaftlich vom Zisterzienserkloster Walkenried geprägt. Neben Ackerbau und Fischzucht wurde von dort aus auch der Silberbergbau im Oberharz und in Goslar gesteuert.

Mitte des 14. Jahrhunderts wurden infolge der Mittelalterlichen Pest die Siedlungen im Harz weitgehend entvölkert. Eine organisierte Wiederbesiedlung der Bergbauorte im Oberharz erfolgte erst wieder Anfang/Mitte des 16. Jahrhunderts.

1412 bis 1413 tobte der Fleglerkrieg im Südharz, in dem Teile der Grafschaft Hohnstein verwüstet wurden.

Von etwa 1527 bis Mitte des 16. Jahrhunderts sind Täufer im Harz bezeugt.

1588 veröffentlichte der Nordhäuser Arzt Johannes Thal mit der "Silva hercynia" die erste Regionalflora der Welt und beschrieb die floristischen Besonderheiten des Harzes. 1593 starb der letzte Nachkomme der Hohnsteiner, Graf Ernst von Hohnstein. Der Herzog Heinrich Julius von Braunschweig zog die Grafschaft Hohnstein als erloschenes Lehen ein und verlehnte sie an das Fürstentum Braunschweig-Wolfenbüttel, dessen Fürst er selbst war.

Im Dreißigjährigen Krieg (1618–1648) verwüsteten und entvölkerten schwedische Söldner große Teile des Harzes. Die Harzschützen leisteten 1624 bis 1627 erbitterten Widerstand. 1668 erließ Rudolf August, Herzog zu Braunschweig und Lüneburg, eine erste Schutzverordnung für die Baumannshöhle. Es heißt in dem herzoglichen Erlass unter anderem, dass diese Höhle jederzeit von allen verständigen Leuten für ein sonderbares Wunderwerk der Natur gehalten worden sei. In demselben dürfe nichts verdorben oder vernichtet werden, auch dürfe kein fremdes loses Gesindel unangemeldet hineingelassen werden. Ein ansässiger Bergmann wurde mit der Aufsicht über das Naturdenkmal betraut. Bis zum Erlass dieser Schutzverordnung hatte es nur Waldschutzverordnungen gegeben, die aus rein praktischen Erwägungen heraus von den Fürsten verkündet worden waren. Die Höhlenverordnung von 1668 berücksichtigte erstmals ethisch-ästhetische Gesichtspunkte; 1668 war das Geburtsjahr des klassischen, konservierenden Naturschutzes im Harz. Ausgelöst worden war die Verordnung durch vorhergehende, schwere Zerstörungen des Höhleninventars durch Rowdys.

1705 wurde der letzte Harzer Bär am Brocken erlegt.

Der ständig zunehmende Holzbedarf der Gruben und Hütten führte zur Übernutzung der Wälder und ab 1700 zur regelrechten Waldzerstörung. Allein 30.000 Meilerplätze soll es im Harz gegeben haben. 1707 verbot eine Verordnung des Grafen Ernst zu Stolberg den Brockenführern, Fremde oder Einheimische ohne besondere Erlaubnis auf den Brocken zu führen; das Feuermachen wurde untersagt. Erste Waldschutzbemühungen im Harz rankten sich von Anfang an um den Brocken und sind auf Graf Christian Ernst aus dem Hause Stolberg zurückzuführen. Er ließ 1736 auch das „Wolkenhäuschen“ auf dem Brocken errichten.

Johann Wolfgang von Goethe besuchte als junger Mann mehrmals den Harz und hatte dort wesentliche biografische Erlebnisse. Dazu gehören seine Wanderungen auf den Brocken, sowie der Besuch des Bergwerkes im Rammelsberg. Später flossen seine Beobachtungen der Gesteine am Brocken in seine geologischen Forschungen ein. Mit dem ersten Besuch des Harzes wurde bei ihm ein starkes Interesse an den Naturwissenschaften geweckt (siehe Goethe: "Dichtung und Wahrheit"). 1777 bestieg Goethe, von Torfhaus kommend, den Brocken. Zu dieser Zeit gab es auf dem Brocken noch keinen Massentourismus; für das Jahr 1779 sind nur 421 Wanderer belegt. Goethe beschrieb seine Gefühle auf dem Gipfel später wie folgt: "So einsam, sage ich zu mir selber, indem ich diesen Gipfel hinabsehe, wird es dem Menschen zumute, der nur den ältesten, ersten, tiefsten Gefühlen der Wahrheit seine Seele öffnen will." Mit seinem "Faust I" und seiner Ballade "Die erste Walpurgisnacht" trug Goethe später wesentlich zur Verbreitung des Blocksberg-Mythos bei.

Für die deutschen Romantiker bildete der Harz eine heimatliche Sehnsuchtslandschaft, ebenso ursprünglich wie sagenumwoben. Verschiedene Reiseberichte und Gedichte, etwa von Novalis, Ernst Schulze und Adelbert von Chamisso, geben Zeugnis von dem verklärenden Blick der Romantiker auf das ""Muttergebürg, welchem die andere Schar wie das Laub entspross"" (Novalis). Bekannt ist besonders der Reisebericht "Die Harzreise" von Heinrich Heine.

Am 23. März 1798 wurde der letzte Wolf des Harzes bei der Plessenburg erlegt. Das gräfliche Logierhaus auf der Heinrichshöhe war zu klein geworden und litt unter Überbelegung; 1799 brannte es ab. 1800 wurde daher ein neues Gasthaus auf dem Brocken errichtet.

Um 1800 waren weite Teile des Harzes entwaldet. Die in der Folge des Bergbaus entstandenen, wenig widerstandsfähigen Fichtenmonokulturen des Oberharzes wurden durch Borkenkäferkalamitäten und ein orkanartiges Unwetter im November 1800 großenteils zu Boden gelegt. Diese größte bekannt gewordene Käferkalamität im Harz wurde die „Große Wurmtrocknis“ genannt – sie vernichtete 30.000 ha Fichtenwald und dauerte etwa 20 Jahre an. Die Wiederaufforstung geschah größtenteils mit Fichte. Ständige Borkenkäferprobleme und Sturmkatastrophen waren die negativen Begleiterscheinungen der Fichtenwirtschaft des Harzer Bergbaus.

1818 erlegte der reitende Förster Spellerberg aus Lautenthal am Teufelsberge den vorerst letzten Eurasischen Luchs ("Lynx lynx") des Harzes.

Zu Beginn des 19. Jahrhunderts, mit der zunehmenden Umgestaltung der Naturlandschaft durch den wirtschaftenden Menschen, und nach der Ausrottung der großen Säugetiere Braunbär, Wolf und Luchs, wurde man sich der Gefährdung der Natur bewusster. 1852 stellte der Landrat von Quedlinburg die Teufelsmauer bei Thale „als einen Gegenstand der Volkssage und eine als seltene Naturmerkwürdigkeit berühmte Felsgruppe“ unter Schutz, da die Bewohner der benachbarten Gemeinden die Felsen als Steinbruch benutzten. Diese Schutzanordnung blieb allen Widersprüchen der benachbarten Gemeinden zum Trotz bestehen. Ein hochwertiges Naturdenkmal blieb so vor der Zerstörung bewahrt. Hierbei wurden von der Behörde ausdrücklich romantische Motive für die Sicherstellung geltend gemacht.

1890 legte Professor Dr. Albert Peter den Brockengarten an. Es handelte sich damals um den ersten Alpenpflanzengarten auf deutschem Boden; der Brockengarten war in seiner wissenschaftlichen Konzeption und Aufgabenstellung die erste Einrichtung dieser Art weltweit. 1899 erfolgte die Inbetriebnahme der Brockenbahn, gegen die es schon damals starke Bedenken der Naturschützer gab. So wollte der Botaniker Bley Züge auf den Brocken verhindern, da die Brockenflora bedroht war.

1907 stieß Hermann Löns angesichts des anlaufenden Massentourismus auf dem Brocken seinen bekannten Stoßseufzer „Mehr Schutz für den Brocken“ aus. 1912 forderte Löns in der Broschüre "Der Harzer Heimatspark," ohne ihn so zu nennen, die Einrichtung eines Harzer Nationalparks. Der Harz spielte eine besondere Rolle im Leben des bekannten Heimatdichters, Naturforschers und Heimatfreundes, sicherlich nicht zuletzt wegen seiner zweiten, aus Barbis im Südharz stammenden Frau Lisa Hausmann.

Um 1920 starb die Harzer Auerhuhnpopulation aus. 1926 schrieb der Wernigeröder Rektor W. Voigt im bekannten „Brockenbuch“: "In Amerika ist es längst eine Sache des Volkes geworden, der heimischen urwüchsigen Natur in Nationalparks eine geheiligte Zufluchtstätte zu schaffen. Nord- und Süddeutschland haben ihren Heide- und Alpenpark. Möge es nun auch in Mitteldeutschland den gemeinsamen Bemühungen der Fürstlichen Verwaltung und der Landjägerei, des Wernigeröder Naturschutzvereins und einzelner Brockenfreunde gelingen, durch liebevolle Pflege und Aufklärung in weitesten Kreisen, auch den Brocken zu einem kleinen, aber einzigartigen Naturschutzgebiet des deutschen Volkes zu machen und als solches zu erhalten."

In den 1930er Jahren wurden die Nationalparkplanungen in Deutschland wieder konkret; es existierten belegbare Pläne für die Nationalparks Lüneburger Heide, Bayerisch-Böhmischer Wald, Hohe Tauern, Höllengebirge, Neusiedler See und Kurische Nehrung. Der Zweite Weltkrieg verhinderte den Fortgang dieser Nationalparkpläne, doch es kam 1937 noch zur Ausweisung des „Naturschutzgebietes Oberharz“.

In der NS-Zeit wurde der Harz zu einem wichtigen Standort der Rüstungsindustrie. Zahlreiche kriegswichtige Betriebe waren hier angesiedelt, die mit dem näherrückenden Ende des Zweiten Weltkriegs immer mehr Zwangsarbeiter beschäftigten. Der Harz war daher in dieser Zeit Standort von mehreren hundert Zwangsarbeiterlagern und KZs. Bekannt wurde das KZ Mittelbau-Dora bei Nordhausen im Südharz.

Am 8. April 1945 erklärte das Oberkommando der Wehrmacht den Harz angeblich zur „Festung“ und rief zur Verteidigung Mitteldeutschlands vor den West-Alliierten auf. Das Hauptquartier der „Harzfestung“ lag bei Blankenburg. Zuvor wurde im westlichen Harzvorland aus allen noch verfügbaren Einheiten das Armeeoberkommando 11 mit etwa 60.000 Mann unter dem stellvertretenden Oberbefehl von Otto Hitzfeld neu gegründet. Der Oberbefehl ging wenige Tage später an General Walther Lucht über. Als der Generalstab der 11. Armee im Kloster Michaelstein bei Blankenburg in Gefangenschaft ging, hatten die US-Einheiten etwa 75 % des Harzgebietes erobert; am 11. April 1945 besetzte die 1st US Army kampflos Nordhausen im Südharz und am 14. April 1945 Halberstadt im nördlichen Harzvorland. Am 20. April 1945 erfolgte gegen Widerstand die Besetzung des Brocken-Plateaus. Am 23. April 1945 wurde General Lucht gefangen genommen und 50.000 deutsche Soldaten gingen in Kriegsgefangenschaft. Tatsächlich war der Harz zum Ende des Zweiten Weltkrieges keine militärisch ernstzunehmende Festung. Die West-Alliierten umgingen den Harz relativ mühelos auf ihrem Weg in Richtung Elbe und die Verzögerung des Vormarsches fiel gering aus.

Im Zuge der Verhandlungen auf der Potsdamer Konferenz und der Neuordnung Deutschlands wurden die beiden östlichen Drittel des Harzes und der Brocken im Juli 1945 von sowjetischen Truppen besetzt.
Durch das westliche Drittel des Harzes verlief von 1949 bis 1990 die Innerdeutsche Grenze. Das Brockenplateau und weitere grenznahe Harz-Gipfel wurden militärisches Sperrgebiet, in das erstmals am 3. Dezember 1989 demonstrierende Wanderer kamen. Der Tourismus zum Brocken ist seitdem sehr intensiv geworden – eine Million Menschen besuchen jährlich die Brockenkuppe. Das ehemalige Sperrgebiet weist heute zahlreiche schutzwürdige Lebensräume auf, so dass es als Grünes Band entwickelt wird.

Der Bergbau im Harz begann vor 3000 Jahren in der Bronzezeit. Dem Oberharzer Bergbau und dem Hüttenwesen verdanken die sieben Oberharzer Bergstädte (Clausthal, Zellerfeld, Bad Grund, Sankt Andreasberg, Lautenthal, Altenau und Wildemann) und rund 30 weitere Ortschaften im Harzinnern und am Harzrand ihre Blüte. Auch die ehemalige Reichsstadt Goslar, deren Glanz von den Erzschätzen des Rammelsberges abhing, förderte über mehrere Jahrhunderte silberhaltige Bleierze. Der Bergbau bestimmte maßgeblich das Harzer Wirtschaftsleben und sein Landschaftsbild. Die Bergleute schufen das berühmte technische System der Oberharzer Wasserwirtschaft, das Oberharzer Wasserregal, von dem noch 70 km Wassergräben und 68 Stauteiche (mit 8 Millionen Kubikmeter Inhalt) genutzt werden. Ohne deren Wasserkraft hätte der Silberbergbau niemals seine hohe wirtschaftliche Bedeutung erlangen können.

Der Bergbau im Unterharz gilt als der älteste Abbau von Bodenschätzen im Harz. Auf Grund der geringeren Vorkommen und der langen territorialen Zersplitterung sowie der teils gegen den Bergbau gerichteten Interessen der stolbergischen Fürsten erlangte der Bergbau nicht die gleiche Bedeutung wie im Oberharz. Der Bergbau konzentrierte sich dabei vor allen auf Elbingerode, Straßberg, Neudorf und Silberhütte. Dennoch prägte der Erz- und Flussspatabbau nachhaltig die Landschaft im gesamten Unterharz.

Im östlichen Harzvorland (Mansfelder Land und Sangerhäuser Mulde) wurde bis 1990 Kupferschieferbergbau betrieben. Dessen Anfänge wurden zwar erst urkundlich um 1199 erwähnt, begannen aber wohl nach neuesten Forschungen auch schon in der Bronzezeit. Er zählte in seiner Blütezeit, Ende des 15. Jahrhunderts, zu den bedeutendsten Europas. Weiterhin befindet sich in Ilfeld das ehemalige Bergwerk Rabensteiner Stollen, das einzige Steinkohlebergwerk im Harz. Im Nordthüringer Revier gab es zahlreiche Kalibergwerke, in der Nähe von Röblingen wurden durch einen Montanbetrieb geologische Wachse abgebaut.

Das letzte Bergwerk im Harz – die Grube „Wolkenhügel“ bei Bad Lauterberg – stellte im Juni 2007 die Förderung wegen Unwirtschaftlichkeit ein. Zuletzt arbeiteten dort 14, von vormals 1000 Mitarbeitern, die mit modernster Technik den Schwerspat zu Tage förderten. Mit der Stilllegung dieser Anlage fand der schon im Mittelalter und seit dem 16. Jahrhundert ununterbrochen betriebene Bergbau auf Silber, Blei und Zink sein Ende. Davon zeugen die Kulturdenkmale und die negativen Folgewirkungen des Bergbaus für die Umwelt wie die Schwermetallbelastungen der Ökosysteme.

Der in vergangenen Jahrhunderten im Harz blühende Bergbau – besonders nach Silber, Eisen, Kupfer, Blei und Zink – ist stark zurückgegangen. Geblieben sind jedoch die zum Teil stark schwermetallhaltigen Rückstände in den Böden des Oberharzes, welche heute ein großes Umweltrisiko darstellen.

Bedeutend ist auch heute noch die Kupferverarbeitung im Gebiet von Mansfeld. Letzte Schwerpunkte des Bergbaus waren der Rammelsberg bei Goslar (stillgelegt 1988) und die Grube Hilfe Gottes bei Bad Grund (stillgelegt 1992). In Bad Lauterberg wurde bis Juli 2007 auf der Grube Wolkenhügel als letztem Bergwerk des gesamten Harzes Schwerspat gewonnen, der heute in erster Linie in der Farbherstellung und dem Schallschutz Verwendung findet. Darüber hinaus werden in zahlreichen Steinbrüchen Massenrohstoffe abgebaut, Diabas, Dolomit, Gabbro, Gips und Grauwacke. Kalkstein wird bei Bad Grund (Winterberg) und rund um Elbingerode in den drei großen Tagebauen (Werk Rübeland, Werk Kaltes Tal und Werk Hornberg) abgebaut. Ein wichtiger Arbeitgeber im Oberharz ist die TU Clausthal. Neben den klassischen Fächern Aufbereitung, Bergbau, Geologie und Metallurgie werden hier viele ingenieur- und naturwissenschaftliche Fächer sowie Betriebswirtschaftslehre unterrichtet und beforscht.

Aufgrund der Wälder des Harzes spielt die Forstwirtschaft eine wirtschaftliche Rolle sowie die dazugehörende verarbeitende Industrie. Wuchsen noch im ersten Jahrtausend nach Christus in den Höhenlagen – für einen natürlichen Bergwald typisch – überwiegend Harthölzer (in erster Linie Rotbuchen; man ging damals „in die Harten“ = Harz) so sind heute in den bewirtschafteten Flächen meistens Monokulturen von Fichten anzutreffen. Wesentliche Ursachen dieser Entwicklung lagen in der Bergbaugeschichte der Harzregion mit ihrem hohen Holzbedarf und den damit einhergehenden Übernutzungen und Devastierungen der Waldbestände. Dazu kamen klimatische Veränderungen in der sogenannten „Kleinen Eiszeit“. Die Wiederaufforstung mit der relativ einfach anzubauenden und anspruchslosen Fichte seit Mitte des 18. Jahrhunderts geht wesentlich auf die Anregung des Oberforst- und Jägermeisters Johann Georg von Langen zurück, hat aber auch zu den heutigen Borkenkäferproblemen geführt – eine typische Folge von Monokulturen, verstärkt durch den Klimawandel.

Der Harz wird heute nur noch durch die Harzer Schmalspurbahnen (HSB) erschlossen, die Wernigerode, Nordhausen, Quedlinburg und den Brocken miteinander verbinden. Bis zur Befestigung der innerdeutschen Grenze schloss sich daran das Netz der Südharz-Eisenbahn-Gesellschaft nach Braunlage an.
Eine Reihe von Stichstrecken führten und führen zum Teil heute noch in den Harz, derzeit mit regelmäßigem Personenverkehr die Bahnstrecke Halberstadt–Blankenburg, die Bahnstrecke Magdeburg–Thale und die Bahnstrecke Klostermansfeld–Wippra. Alle in Niedersachsen liegenden Stichbahnen (Innerstetalbahn, Odertalbahn) sowie in Sachsen-Anhalt die Bahnstrecke Berga-Kelbra–Stolberg (Harz) sind aufgegeben worden. Die Rübelandbahn dient derzeit nur dem Güter- und gelegentlichem Museumseisenbahnverkehr.

Der Harz ist (vom Norden im Uhrzeigersinn) von den Strecken Bahnstrecke Heudeber-Danstedt–Bad Harzburg/Vienenburg, Bahnstrecke Halberstadt–Vienenburg, Bahnstrecke Halle–Halberstadt, Berlin-Blankenheimer Eisenbahn, Halle-Kasseler Eisenbahn, der Südharzstrecke, Bahnstrecke Herzberg–Seesen, Bahnstrecke Neuekrug-Hahausen–Goslar und der Bahnstrecke Vienenburg–Goslar umgeben.

Neben den Regionalzügen auf diesen Strecken ist der Harz im Jahresfahrplan 2017 im Fernverkehr angebunden:


Der Harz wird im Westen von der A 7 und im Süden von der A 38 (Südharzautobahn) gestreift. Nicht weit entfernt liegt im Osten die A 14. Entlang des Südwestharzrandes verläuft eine vierspurige und autobahnähnliche Schnellstraße (B 243) von Seesen über Osterode bis Nüxei bei Bad Sachsa. Besonders das nördliche Harzvorland profitiert durch die autobahnähnliche B 6 (geplant als A 36 (Nordharzautobahn)) von Goslar nach Bernburg. Auch die B 4 ist teils autobahnähnlich (von B 6 bis südlich Bad Harzburg sowie Königskrug bis südlich Braunlage) oder vierspurig (von B 6 bis Torfhaus) ausgebaut. Sie überquert den Harz von Bad Harzburg auf der Nord-Süd-Achse über Torfhaus und Braunlage bis nach Ilfeld und Nordhausen am Südharzrand. Auch der übrige Harz ist mit Bundesstraßen gut erschlossen. Von Bedeutung sind besonders die Harzhochstraße (B 242), die den gesamten Harz in West-Ost-Richtung (von Seesen bis Mansfeld) quert, sowie die B 241, die von Goslar im Norden einmal über den Oberharz und Clausthal-Zellerfeld bis nach Osterode im Süden führt (siehe auch: Alte Harzstraße), genau wie die B 498.

Die heutige Technische Universität Clausthal (TUC) ist national wie international eine renommierte technische Universität. Sie ist ein Zentrum naturwissenschaftlicher und ingenieurtechnischer Ausbildung und Forschung mit den Schwerpunkten Energie und Rohstoffe. Die Ursprünge der 1775 gegründeten TUC liegen im Bergbau, namentlich in der Clausthaler Berg- und Hüttenschule.

Die Hochschule Harz verfügt über zwei Standorte. In Wernigerode befinden sich die Fachbereiche Automatisierung und Informatik sowie Wirtschaftswissenschaften, in Halberstadt der Fachbereich Verwaltungswissenschaften. Momentan verzeichnet die junge Hochschule 3.300 angehende Akademiker. Das Profil der Hochschule Harz ist geprägt durch innovative, international ausgerichtete Lehrinhalte und Kooperationen mit Partnerhochschulen weltweit.

In Nordhausen befindet sich die Fachhochschule Nordhausen.

Der Fremdenverkehr stellt im Harz einen bedeutsamen Erwerbszweig dar. Es gibt viele Kurorte und nahezu jeder Ort im Harz und Harzvorland ist touristisch geprägt. Bekannte Ziele sind der Nationalpark Harz mit Brocken und die historischen Städte am Harzrand. Übernachtungsstärkste Stadt des Harzes ist Wernigerode; danach folgt Braunlage. Konzepte wie die Westernstadt "Pullman City Harz" oder die Rockopern auf dem Brocken sollen vor allem auswärtige Touristen ansprechend unterhalten. Zuständig für die touristische Vermarktung des gesamten Harzes ist der Harzer Tourismusverband (HTV), die meisten Kommunen betreiben zusätzlich örtliche Kurbetriebsgesellschaften.

Auch wenn der Harz für den Wintersport nicht die Bedeutung anderer deutscher Mittelgebirge – wie Rothaargebirge, Thüringer Wald, Erzgebirge und Schwarzwald – oder gar der Alpen erreicht, gibt es etliche Wintersportstätten und -orte. Zu nennen sind hier vor allem die Orte Altenau mit Ortsteil Torfhaus, Benneckenstein, Braunlage (mit Ortsteil Hohegeiß), Goslar-Hahnenklee-Bockswiese, Hasselfelde, Sankt Andreasberg (mit den Ortsteilen Sonnenberg und Oderbrück) und Schierke. Neben den alpinen Skisport am Bocksberg, Matthias-Schmidt-Berg, Ravensberg, Sonnenberg, Großen Wiesenberg und Wurmberg sowie an vielen anderen kleineren Skigebieten und -hängen hat wegen der Höhenlagen und der Länge der Strecken auch der nordische Skisport eine große Bedeutung. Internationale Wintersport-Wettbewerbe finden in der Biathlonanlage am Sonnenberg statt; früher gab es solche auch auf der Wurmbergschanze (2014 abgerissen) bei Braunlage.

Erwähnenswert sind die zahlreichen Loipen im Harz. Ihre Qualität und Ausstattung werden von den Grundeigentümern, in Teilen des Nationalpark Harz, und auch einzelnen Kommunen und Fördervereinen gewährleistet. Bekannt wurde der Förderverein Loipenverbund Harz e. V. Er wurde 1996 auf Initiative des Nationalparks Harz von Harzer Wintersportgemeinden, den Seilbahn- und Liftbetrieben sowie Hotels und Verkehrsunternehmen gegründet. Der Verein verfolgt das Ziel, den nordischen Skitourismus im Harz zu fördern und die Belange des Naturschutzes zu berücksichtigen.

Den Bergrettungsdienst in den Loipen, auf den Rodelhängen, Wanderwegen und Alpin-Skipisten, sowie im unwegsamen Gelände gewährleistet die Bergwacht Harz.

Im Sommer wird im Harz vor allem gewandert. Seit einigen Jahren wird auch Nordic Walking vermehrt betrieben.

Auf mehreren Talsperren im Harz wird vielfältiger Wassersport betrieben. Auf einigen im Harz entspringenden Flüssen sind Kanufahren und verwandte Sportarten im Wildwasser möglich. Auf der Oker unterhalb der Okertalsperre finden auch nationale und internationale Kanu- und Kajak-Wettkämpfe statt. Das Wildwasser entsteht dort durch zeitweise erhöhte Wasserabgabe aus der Okertalsperre und ist somit weitgehend witterungsunabhängig.

Einige Berge bieten eine gute Basis für die Fliegerei (Segel-, Drachenfliegen), namentlich vom Rammelsberg bei Goslar aus. Der Harz bietet verschiedene Klettergebiete, wie das Okertal mit seinen Klippen, wobei dort die Adlerklippen besonders stark frequentiert sind. Der Harz hat sich in den vergangenen Jahren zu einem sehr guten Mountainbike-Revier entwickelt mit 62 ausgeschilderten Mountainbike-Strecken und fünf Bikeparks mit Liftbetrieb in St. Andreasberg, Braunlage, Hahnenklee, Schulenberg und Thale. Die Bikeparks verfügen über Freeride-, Downhill- und Fourcrossstrecken. Sowohl die ausgeschilderten Strecken als auch die Bikeparks sind für jeden Leistungs- und Konditionsbereich passend.

Die Straßen des Harzes werden trotz teilweise hoher Kfz-Belastung von Renn- und Tourenradfahrern befahren, da es im gesamten Norden Deutschlands kein Revier mit vergleichbar langen und zum Teil sehr steilen Anstiegen gibt. Zudem bestehen zahlreiche Bahnanbindungen mit Fahrradmitnahme an den Harzrand.

Auch im Sommer sichert die Bergwacht Harz die Rettung von verunfallten Personen aus unwegsamem Gelände.

Die Bergwelt des Harzes wurde schon in früheren Zeiten für ausgedehnte Wanderungen genutzt (u. a. von Johann Wolfgang von Goethe, Heinrich Heine und Hans Christian Andersen). Ein umfangreiches Wanderwegenetz wird heute insbesondere durch den Harzklub e. V. unterhalten. Darüber hinaus führen durch den Harz die Europäischen Fernwanderwege E6 und E11 und die Fernwanderwege Harzer Hexenstieg, Kaiserweg, Karstwanderweg, Selketalstieg und Via Romea. Als überregionales Projekt gibt es die Harzer Wandernadel mit 222 Stempelstellen; letztere wird seit 2006 als Wanderabzeichen verliehen. Im Unterharz liegt Deutschlands erster offizieller FKK-Wanderweg – der 18 Kilometer lange Harzer Naturistenstieg.

Neben dem Okertal und der Roßtrappe bei Thale sind auch die Hohneklippen (Höllenklippe und Feuerstein bei Schierke) das Ziel von Kletterern.

Auf dem Gebiet des Harzes werden vorwiegend ostfälische und thüringische Mundarten gesprochen.

Eine Besonderheit des Oberharzes ist die Oberharzer Mundart. Im Gegensatz zu den ostfälischen und thüringischen Mundarten des Umlandes handelt es sich hier um eine erzgebirgische Mundart, die auf die Ansiedlung von Bergleuten im 16. Jahrhundert zurückgeht.

Die Oberharzer Mundart beschränkt sich auf wenige Orte. Die bekanntesten sind Altenau, Sankt Andreasberg, Clausthal-Zellerfeld, Lautenthal und Hahnenklee. Heute hört man im Oberharz die Mundart im täglichen Leben nur mehr wenig. Hauptsächlich Angehörige der älteren Generationen beherrschen sie noch. Zur Aufrechterhaltung werden in den Lokalzeitungen gelegentlich Artikel in Oberharzer Mundart abgedruckt.

Zur Verdeutlichung folgt der Refrain eines St. Andreasberger Heimatliedes:

<poem style="margin-left:3em;">
Eb de Sunne scheint, ebs stewert, schtarmt, ebs schneit,
bei Tag un Nacht ohmds oder frieh
wie hämisch klingst de doch
du ewerharzer Sproch
O Annerschbarrich wie bist de schien.
</poem>

In den Gips-, Dolomit- und Kalksteinschichten des Harzes haben geologische Prozesse zur Entstehung von Höhlen geführt. Solche Höhlen sind die Baumannshöhle, die Einhornhöhle, die Hermannshöhle, die Iberger Tropfsteinhöhle und am Südrand des Harzes die Heimkehle.

Da der Harz zahlreiche Bodenschätze enthält, wurden sie schon seit der Bronzezeit durch Bergbau ergründet. Einige der Bergwerke sind zu Schaubergwerken umgestaltet. So war die Grube Samson lange Zeit das tiefste Bergwerk der Welt. Andere Schaubergwerke sind die Büchenberg, das „Drei Kronen & Ehrt“, der Röhrigschacht, das Bergwerk „Lange Wand“ in Ilfeld, der Rabensteiner Stollen in Netzkater, die Grube Glasebach bei Straßberg, die Grube Lautenthals Glück oder die Grube Rammelsberg bei Goslar, die zum UNESCO-Weltkulturerbe zählt. Die Grube Roter Bär in Sankt Andreasberg diente bis ins 20. Jahrhundert auch als Lehrbergwerk und ist heute ebenfalls zeitlich beschränkt Besucherbergwerk. Den meisten Schaubergwerken sind mehr oder weniger umfangreiche Bergbaumuseen angegliedert, die größten davon am Rammelsberg und in Clausthal-Zellerfeld Oberharzer Bergwerksmuseum.

Die folgenden Städte und Gemeinden liegen ganz oder teilweise im Harz.
Im nördlichen Randgebiet des Harzes befinden sich die mittelalterlichen Klöster Wöltingerode, Drübeck, Ilsenburg und Michaelstein, am Südrand liegt das Kloster Walkenried. In Hahnenklee steht die 1908 geweihte, hölzerne Gustav-Adolf-Stabkirche und in Clausthal-Zellerfeld die größte Holzkirche Deutschlands, die Marktkirche zum Heiligen Geist.

Im Harz gibt es an verschiedenen Punkten aufgrund der erhöhten Lage Funk- und Aussichtstürme, wie die Carlshaushöhe bei Trautenstein, der Aussichtsturm auf dem Großen Knollen und das Josephskreuz.
In früheren Zeiten wurden erhöhte Kamm- und Spornlagen zur Anlage von Burgen genutzt, und so finden sich im Harz die Burgruine Anhalt, die Burg Falkenstein, auf der Eike von Repkow wahrscheinlich den Sachsenspiegel verfasste, die Burgruine Harzburg, die Burg Hohnstein bei Neustadt/Harz, die Ruine Königsburg, die Burg Lauenburg bei Stecklenberg, die Plessenburg, die Burgruine Scharzfels und die Burg Stecklenburg.
Neben diesen Wehranlagen wurden aber auch Schlösser errichtet, wie Schloss Herzberg, Schloss Blankenburg, Schloss Ilsenburg, Schloss Stolberg und Schloss Wernigerode.

Im Miniaturenpark Kleiner Harz in Wernigerode sind auf einer Fläche von 1,5 ha detailgetreue Nachbildungen bedeutender Bauwerke und Attraktionen der Harzregion ausgestellt.

Unter den vielen Malern, die den Harz abgebildet haben, waren Caspar David Friedrich, Ludwig Richter, Georg Heinrich Crola, Ernst Helbig, Hermann Hendrich, Edmund Kolbe, Wilhelm Pramme, Adolf Rettelbusch, Wilhelm Ripe, Hermann Bodenstedt, Walther Hans Reinboth und Rudolf Nickel.
Zeichnungen fertigte u. a. Lyonel Feininger besonders bei seinen Urlaubsaufenthalten in Braunlage.








</doc>
<doc id="13632" url="https://de.wikipedia.org/wiki?curid=13632" title="Harz (Material)">
Harz (Material)

Harze sind, je nach Temperatur und Alter, mehr oder weniger flüssige Produkte, die sich aus verschiedenen organischen Stoffen zusammensetzen. 

In natürlicher Form werden Harze als Naturharz von Tieren und Pflanzen, insbesondere Bäumen abgesondert. Bei Bäumen dienen sie in erster Linie zum Verschließen von Wunden an der Pflanze. Traditionell sind Harze gelbliche bis bräunliche, klare bis trübe, klebrige und nichtkristalline Materialien natürlichen Ursprungs, die in den gängigen organischen Lösungsmitteln löslich sind, nicht jedoch in Wasser. Naturharze haben nur noch praktische Bedeutung für Naturfarben und Spezialanwendungen. Im süddeutschen Raum und in Österreich wird Naturharz auch als (Baum)pech bezeichnet.

Im industriellen Bereich werden heute mengenmäßig vorwiegend Kunstharze, also synthetische Materialien, verwendet. Sie dienen als reaktive Zwischenstufe zur Herstellung von duroplastischen Kunststoffen und sind Komponenten in Lacken und Klebstoffen. Sie sind weiche Feststoffe oder hochviskose Substanzen, die üblicherweise Prepolymere mit reaktiven funktionellen Gruppen enthalten. Entgegen den Empfehlungen der IUPAC werden in der Kunststoffindustrie gelegentlich auch vernetzte Kunststoffe (Duroplaste) als „Harze“ bezeichnet.

Natürliches Harz oder Naturharz ist eine Sammelbezeichnung für eine von Pflanzen oder Tieren abgesonderte zähe, nicht wasserlösliche Flüssigkeit. Pflanzen, vor allem Bäume, produzieren diese Ausscheidungen (Exsudate) unter anderem nach Verletzungen, um mit der meist klebrigen Masse die Wunde zu verschließen. Als einziger tierischer Lieferant gilt die in Süd- und Südostasien beheimatete Lackschildlaus ("Kerria laccifera"), die den Schellack liefert. Historisch wurden diese Materialien vielseitig verwendet, unter anderem in der Kunst, der Medizin und im Schiffbau. Heute werden Naturharze, vor allem in der Industrie, weitestgehend durch Kunstharze ersetzt, die zu den Kunststoffen zählen.

Naturharze sind eine Mischung verschiedener chemischer Substanzen. Die mengenmäßig vorherrschenden Verbindungen sind Harzsäuren, die zu den Carbonsäuren zählen. Frische Harze bestehen weiterhin zu einem nicht unwesentlichen Teil aus flüchtigen und aromatischen Verbindungen. Verdunsten diese, wird das verbleibende Material zäher und härter. Daneben führen Polymerisations-, Vernetzungs- und Oxidationsreaktionen zum Erstarren der Ausscheidung.

Das in den meisten Nutzungen vorherrschende Harz von Nadelbäumen ist eine zähe, klebrige und stark riechende Flüssigkeit. Es ist in Öl leicht und in Alkohol gut, in Benzin teilweise löslich, Edelterpentine auch in Salmiakgeist.

Man unterscheidet rezente, rezentfossile bzw. halbfossile und fossile Harze. Während rezente Harze von noch heute lebenden Bäumen entstammen (Terpentin, Balsame, Gummilack, Kolophonium, Sandarak und Mastix), sind rezentfossile Harze aus früheren Vertretern von Baumarten entstanden, die teilweise aber auch heute noch existieren. Diese nennt man Kopale. Fossiles Harz wird als Bernstein bezeichnet. Die Ursprungsbäume sind meist nicht mehr feststellbar. Für Baltischen Bernstein nimmt man an, dass er von einer prähistorischen Koniferenart stammt. Fossile Harze sind in Alkohol schlecht löslich.

Pflanzliche Ausscheidungen ohne Harzfraktion setzen sich meist aus Polysacchariden zusammen und sind wasserlöslich. Sie zählen nicht zu den Naturharzen. Mischformen aus wasserlöslichen und harzhaltigen Komponenten stellen die Gummiharze dar.

Baumharze sind sekundäre Stoffwechselprodukte der Pflanzen, die über Harzkanäle an die Pflanzenoberfläche geleitet werden (zur Biosynthese siehe hier). Im normalen Lebenszyklus bilden harzerzeugende Bäume „physiologisches Harz“. Nach Verletzungen steigt die gebildete Menge, das „pathologische Harz“ dient dem Wundverschluss.

Die systematische Gewinnung von Baumharz geschieht durch das Harzen. Dabei werden künstliche Verletzungen durch Anritzen der Rinde herbeigeführt und das austretende Harz in einem Behälter gesammelt. Verwendete Bäume sind unter anderem Kiefer, Lärche und der Sandarakbaum. Fossile Baumharze wie Bernstein werden durch Absuchen vorkommenreicher Flächen (z. B. Strände), durch Prospektion oder Bergbau gewonnen.

Das wohl bekannteste natürliche Harzprodukt ist Kolophonium, das vorwiegend aus dem Harz von Kiefern und Fichten gewonnen wird und in vielen Produkten Verwendung findet, z. B. als Klebstoff für Heftpflaster, in Kaugummi und zur Behandlung der Bogenhaare bei Streichinstrumenten. Kolophonium ist der feste Rückstand, der beim Erhitzen von Nadelbaumharz nach Abdestillieren des Terpentinöls anfällt. 
Mit Alkali verseiftes oder durch eine Diels-Alder-Reaktion mit Maleinsäureanhydrid modifiziertes Kolophonium wird in der Papierherstellung eingesetzt, um dieses zu hydrophobieren. Durch diesen, Leimung genannten Prozess wird die Beschreibbarkeit und Bedruckbarkeit des Papiers verbessert.

In der europäischen Ölmalerei spielten Harze sowohl von Nadelbäumen (Terpentine, Mastix) als auch die von Laubbäumen (Dammar) eine große Rolle. Sie dienten seit dem 15. Jahrhundert in Kombination mit anderen Substanzen als Bindemittel der Farbpigmente. Die Qualität der Öl-Harz-Farben hatte mehrere Vorteile gegenüber den davor üblichen Malfarben, vor allem ermöglichte sie aufgrund besserer Mischbarkeit einen größeren Nuancenreichtum durch weichere Farbübergänge. Die Temperamalerei, deren Bindemittel Emulsionen sind, und die noch frühere Wachsmalerei wurden somit verdrängt. Außerdem werden Harze schon seit der Antike für die Herstellung von Lacken verwendet.

In Griechenland wird das Harz der Aleppo-Kiefer zum Wein gegeben, was ihm ein besonderes Aroma verleiht. Dieser Wein wird Retsina genannt. Einige tropische Harze wie Elemi und Copal sowie vor allem Myrrhe und Weihrauch werden bis heute als Räucherwerk verwendet.

Als Resine werden Extraktstoffe aus dehydrierten Naturharzen bezeichnet. Sie werden als Zwischenprodukte in der chemischen Industrie eingesetzt, zum Beispiel als Synthesekautschuk, für Schiffsfarben oder zur Pigmentherstellung. Seifen aus Resinen, ebenfalls zur industriellen Verwendung, heißen Dresinate. Resorcin, ein Destillat aus Naturharzen, wird als "Haftvermittler" im Reifenbau verwendet, daneben auch zur Herstellung von Farbstoffen, Kunststoffen, Klebstoffen und Flammschutzmitteln sowie in Pharmazeutika.


Der jährliche Bedarf der chemischen Industrie in Deutschland an Naturharzen wird auf 31.000 t geschätzt (einschließlich Naturwachse). Jährlich werden 5.000 bis 16.000 t dehydrierte Naturharze nach Deutschland importiert. Die mengenmäßig vorherrschende Verwendung von Naturharzen ist die Herstellung von Farben, Lacken und Klebstoffen.

Kunstharze (auch "synthetische Harze" genannt) sind nach ISO 4618:2014 "Beschichtungsstoffe — Begriffe" durch Polymerisations-, Polyadditions- oder Polykondensationsreaktionen synthetisch hergestellte Harze. Nach den Konventionen der IUPAC sind sie weiche Feststoffe oder hochviskose Substanzen, die üblicherweise Prepolymere mit reaktiven funktionellen Gruppen enthalten. Synthetische Harze bestehen bei der Verarbeitung in der Regel aus zwei Hauptkomponenten. Die Vermischung beider Teile (Harz und Härter) ergibt eine reaktionsfähige Harzmasse. Bei der Härtung steigt die Viskosität an und nach abgeschlossener Härtung erhält man einen unschmelzbaren Kunststoff (Duroplast). Nach der IUPAC sollen die duroplastischen Produkte (engl.: "thermosets") nicht als Harze bezeichnet werden.

Kunstharze können durch Naturstoffe, zum Beispiel pflanzliche oder tierische Öle beziehungsweise natürliche Harze, modifiziert sein, wie z. B. bei Alkydharzen. Als Kunstharze werden jedoch auch natürliche Harze bezeichnet, die durch Veresterung oder Verseifung modifiziert wurden. Im weiten Sinn und entgegen den Empfehlungen der IUPAC werden in der Industrie (besonders im englischen Sprachraum) alle Polymere, die die Basis für Kunststoffe, organische Beschichtungen oder Lackierungen u. ä. sind, als Harze bezeichnet. So wird ohne Notwendigkeit Polyethylen als "Polyethylenharz" bzw. im Englischen als "polyethylene resin" bezeichnet. ISO 472:2013 "Kunststoffe — Begriffe" schafft ein hoch inhomogenes, kaum nützliches Begriffsfeld. Das Wort "Harz" wird dort direkt oder indirekt mit dem Wort "Kunststoff" sowie die in einem Produktionsverfahren verwendete "Formmasse" oder allgemein mit "Polymer" gleichgesetzt. 

Synthetische Harze sind in der Regel flüssige oder feste amorphe Produkte ohne scharfen Siede- oder Schmelzpunkt. Für die technische Anwendung sind die Harze oft in Form einer Emulsion oder Suspension erhältlich bzw. werden in dieser Form hergestellt. Viele dieser Harze sind prinzipiell auch als echte Lösungen einsetzbar, da es sich jedoch bei den dafür meist notwendigen Lösungsmitteln um flüchtige organische Verbindungen handelt, wird dieser Anteil immer geringer. Bei klassischen Harzen, wie Phenolharzen, erfolgt die Härtung über eine Polykondensation; sie werden daher Kondensationsharze genannt und müssen unter hohen Drücken verarbeitet werden. Wichtig für Industrie sind Harze, die ohne Abspaltung flüchtiger Komponenten zu Duroplasten aushärten. Diese Harze werden Reaktionsharze genannt. Ein Beispiel hierfür sind photoinitiiert härtende Acrylate, wobei eine radikalische Polymerisation durch UV- oder sichtbares Licht erfolgt. Epoxide härten hingegen über eine Polyaddition, ebenfalls ohne Abspaltung.

Als härtbare Formmassen bezeichnet man meist rieselfähige Massen, die in einem Warmformungsvorgang mit unmittelbar anschließender irreversibler Aushärtung bei erhöhter Temperatur zu Formteilen und Halbzeugen verarbeitet werden. Hierbei sind häufig hohe Drücke zur kompletten Füllung der Form notwendig. Die Verarbeitung von Kunstharz erfolgt häufig im Gussverfahren. Hierbei wird das Gießharz in eine wiederverwendbare oder eine verlorene Form gegossen.




Entsprechend der Vielzahl unterschiedlicher Harztypen existiert ein breites Anwendungsspektrum. Typisch sind Verwendungen für Leime, Klebstoffe, Lacke, aber auch zur Herstellung von Formteilen.

Harze werden unter anderem in folgenden Anwendungsbereichen verwendet:





</doc>
<doc id="13633" url="https://de.wikipedia.org/wiki?curid=13633" title="Vi">
Vi

vi („vi“ für „visual“) ausgesprochen [] oder [], im deutschen Sprachraum gelegentlich auch [], jedoch nicht „sechs“ oder „six“ (wie die römische Zahl VI) ist ein 1976 von Bill Joy auf einem ADM-3A-Computerterminal für eine frühe BSD-Version geschriebener und von POSIX standardisierter Texteditor. Der Name stammt vom Befehl codice_1 des Editors ex. Mit diesem Befehl konnte man den Zeileneditor in einen visuellen Modus umschalten.

Bis Anfang der 1970er Jahre wurden hauptsächlich zeilenorientierte Editoren benutzt, wobei ein weit verbreiteter ed war. Joy baute auf diesem auf, zunächst mit ebenfalls einem Zeileneditor, "ex". Darauf baute später wiederum der Editor vi auf. vi wurde schnell zum De-facto-Standardeditor unter Unix.

1991 benutzten ungefähr die Hälfte aller Teilnehmer einer Usenet-Umfrage den vi. Auch heutzutage ist die Verwendung von vi bzw. dessen Erweiterungen zumindest in der Unix- und Linuxwelt noch sehr verbreitet. Außerdem kann man mit diesem Editor in Kombination mit ssh (früher mit Telnet oder rsh) im Netzwerk auf anderen Rechnern arbeiten.

Aufgrund seiner relativen Ressourcenfreundlichkeit starten vi bzw. fast alle seine Klone schneller und benötigen deutlich weniger Speicherplatz als etwa Emacs. Auf einer „Rettungsdiskette“ hat vi auch heute noch seinen Platz, so dass er Bestandteil fast aller Unix-/Linux-Distributionen ist.

Die originale Version von Bill Joy war ursprünglich weder im Quelltext noch sonst frei verfügbar, so dass mittlerweile eine Reihe von Klonen mit zum Teil wesentlichen Erweiterungen existiert, wie z. B. Vim, Nvi, elvis und WinVi, die teilweise auch für nicht Unix-artige Systeme verfügbar sind.

vi besitzt drei grundsätzlich unterschiedliche Arbeitsmodi. Die drei Modi sind:
 | Start mit

Aufgrund der verschiedenen Arbeitsmodi ist die Bedienung von vi, verglichen mit anderen Terminaleditoren wie GNU nano oder heute üblicheren grafischen Editoren, zunächst gewöhnungsbedürftig. Ein großer Vorteil von vi ist hingegen, dass mehrere Befehle nacheinander ohne gleichzeitiges Betätigen der -, - oder sonstiger Modifikator-Tasten abgesetzt werden können. So ist es auch möglich, mit einem einzigen Befehl mehrere Wörter oder Sätze zu löschen.

Im Zuge des sogenannten Editor War gründeten die Anhänger von vi den „Cult of Vi“ als Reaktion auf die von Richard Stallman alias St. IGNUcius gegründete Church of Emacs. Daraufhin wurden sie von den Emacs-Anhängern als Nachahmer („ape their betters“) bezeichnet.




</doc>
<doc id="13634" url="https://de.wikipedia.org/wiki?curid=13634" title="Heiliger">
Heiliger

Als Heiliger wird ein Mensch bezeichnet, der als einer Gottheit besonders nahestehend beziehungsweise als in religiöser und ethischer Hinsicht vorbildlich angesehen wird. Die Anerkennung eines Heiligen kann religiösen oder politischen Autoritäten vorbehalten sein oder sich in der Akklamation und Verehrung durch das gläubige Volk vollziehen; eine wichtige Rolle kann dabei das Auftreten von übernatürlichen Phänomenen (Wunder) im Zusammenhang mit dem Heiligen spielen. Die darauf folgende – zumeist posthume – kultische Verehrung eines Heiligen bezeichnet man als Heiligenverehrung.

Im allgemeinen Sprachgebrauch verweisen die Begriffe des Heiligen und der Heiligenverehrung gemeinhin auf die entsprechenden christlichen Vorstellungen. Auch wenn die Begriffe eng mit der christlichen Volksreligiosität assoziiert werden, so sind doch beide Phänomene in anderen Weltreligionen ebenso zu finden.

Der Begriff des Heiligen ist religionswissenschaftlich bisher nicht befriedigend definiert. Zum einen ist aufgrund der differierenden Anforderungen, die verschiedene Religionen an einen Heiligen stellen, keine für alle Religionen allgemeingültige Definition möglich. Zum anderen überschneidet sich der religiöse Typ des Heiligen mit mehreren anderen religiöser Autoritätstypen, und es ist bisher nicht gelungen, eine deutlich unterscheidende Charakteristik zu finden.
Die Grenzen der im Diskurs religiöser Autoritäten skizzierten Typen sind fließend und können sich in wichtigen Punkten überschneiden. Ein für alle Religionen allgemeingültiger Automatismus ist jedoch nicht gegeben.

Typologische Gemeinsamkeiten dieser Art weist der Heilige besonders mit dem Märtyrer und dem Heros auf: Sein Grab bzw. der Aufbewahrungsort seiner Reliquien entwickelt sich zu einem kultischen Zentrum besonderen Ranges. Es ist das Ziel allgemeiner Verehrung, von Pilgerreisen und wird oft als Zentrum einer Nekropole genutzt. Allen drei Typen wird eine Funktion als Fürsprecher der Gläubigen gegenüber der göttlichen Autorität zugeschrieben.

Eine Verehrung ist oft wie beim Heros bereits zu Lebzeiten gegeben, kann aber wie beim Märtyrer auch erst nach dem Tod erfolgen. Ein Unterschied zum Typen des Märtyrers liegt darin, dass der Letztere die religiöse Vollkommenheit per definitionem nicht durch seinen Lebenswandel, sondern durch die Art seines Sterbens erlangt, während beim Heiligen sich die Vollkommenheit auch ohne ein solches Martyrium wesentlich durch sein voraufgegangenes Leben erweist. Im Gegensatz zum Heros fehlt ihm die göttliche oder halbgöttliche Abstammung.

Der Heilige kann zwar Kleriker sein oder dem gottgeweihten Stand angehören, muss dies aber nicht. Weiterhin kann der Heilige das Charisma des Religionsstifters oder Reformators besitzen, im Gegensatz zu diesen ist aber sein Ziel nicht Verkündigung einer (Glaubens-)lehre und anschließende Bildung einer Gläubigenschar, sondern das Hervortreten durch sein religiös vorbildliches Leben.

Vom mythischen Heilsbringer schließlich unterscheidet ihn sein Wirken in der real überlieferten, wenn auch oft in der Tradierung unzuverlässigen, Geschichte und der fehlende Erlösungsaspekt seines Lebens.

Die Deklaration und Verehrung von Heiligen erfüllt ein urreligiöses Bedürfnis der Menschen nach Vorbildern in ihrem Glauben und gleichzeitige Bestätigung desselben. Die als vorbildlich anerkannten Mitglieder der Glaubensgemeinschaft verlassen zwar die diesseitige – menschliche – Gemeinschaft. Sie bieten jedoch die Möglichkeit, den Kontakt zwischen Diesseits und Jenseits zu halten, denn obwohl sie in die jeweilige göttliche Herrlichkeit aufgenommen worden sind, bleiben sie über ihr Grab, ihre Reliquien und ihre Verehrung im Diesseits präsent und bilden so eine Verbindung zu der von den noch lebenden Gläubigen selbst angestrebten Erlösung. Über die ihnen während oder nach ihrem Leben zugeschriebenen Wundertaten geben sie den Gläubigen eine positive Antwort auf die Frage nach der Sinn- und Wahrhaftigkeit der jeweiligen Religion.

Die christliche Theologie ist geprägt von einem Doppelkonzept von Heiligkeit: Das Heilige schlechthin ist Gott selbst, jedoch nicht im Sinne einer transzendenten Statik, also eines Zustandes in göttlichen Sphären ohne Auswirkung auf das Diesseits. Vielmehr wird Gottes Heiligkeit als immanente Dynamik verstanden, die alle irdischen Dinge für sich aussondern kann und damit Grund ihrer Heiligkeit ist. Im Neuen Testament wird diese Sicht modifiziert. Nun ist es Jesus Christus, der in seiner einzigartigen Beziehung zum Vater durch seinen Tod und seine Auferstehung Heiligkeit in denen bewirkt, die ihm nachfolgen.

Christliche Heiligkeit tritt in zwei Komponenten auf. Einerseits erwählt sich Gott sowohl im Alten als auch im Neuen Testament ein „heiliges Volk“: Das Volk Israel und das so bezeichnete „neue heilige Volk“ der Kirche. Andererseits tritt auch immer das Konzept der individuellen Heiligkeit einer Einzelperson auf. Die individuelle Heiligkeit ist dabei aber stets nur Manifestation einer Heiligkeit als Glied der Kirche, die in ihrer Gesamtheit ja die „communio sanctorum“, also die „Gemeinschaft der Heiligen“, darstellt. Im Katechismus der Katholischen Kirche heißt es dazu: „Wenn die Kirche gewisse Gläubige heiligspricht, das heißt feierlich erklärt, daß diese die Tugenden heldenhaft geübt und in Treue zur Gnade Gottes gelebt haben, anerkennt die Kirche die Macht des Geistes der Heiligkeit, der in ihr ist. Sie stärkt die Hoffnung der Gläubigen, indem sie ihnen die Heiligen als Vorbilder und Fürsprecher gibt.“

Die frühchristliche Heiligenverehrung schloss sich an die aus dem jüdischen Glauben bekannten Formen an. Dort waren seit langem der Hohepriester als „amtlicher Fürbitter“ der Menschen, die Mittlerschaft der Engel zwischen Gott und den Menschen, die Verehrung großer Gestalten der Vergangenheit sowie das Martyrium bekannt.

Die hohepriesterliche Mittlerfunktion wurde ganz auf Christus übertragen und erst nach der theologischen Klarstellung früher Väter der Kirche, dass die Verehrung anderer Menschen, die Christus nachgefolgt waren, die Einzigartigkeit der Mittlerfunktion Christi nicht beeinträchtige, begann die Urkirche Märtyrer und die Apostel anzurufen.

Der erste Beleg einer Märtyrerverehrung ist der um 160 geschriebene Bericht über Polykarp von Smyrna, in der westlichen Kirche breitete sich die Märtyrerverehrung wahrscheinlich während der Verfolgungen im 3. Jahrhundert aus und verband sich unter dem Einfluss Tertullians zu einer Verehrung der Märtyrer als Heiliger. Anfänglich war diese Verehrung auf den Todestag und die Grabstätte des Märtyrers beschränkt, mit dem Aufkommen der Reliquienverehrung vervielfachten sich aber die räumlichen und zeitlichen Möglichkeiten der Verehrung. Der erste greifbare Beleg des Verständnisses der Heiligen als Fürsprecher bei Gott findet sich in einem Graffito an der römischen Kirche San Sebastiano aus dem Jahr 260.

Mit dem Wandel des Christentums zur Staatsreligion des Römischen Reiches weitete sich auch der Heiligenbegriff, da das Martyrium wegen der abgestellten Verfolgungen nun nicht mehr höchstes Zeugnis eines christlichen Lebens sein konnte. Nach und nach wurden – unter dem bestimmenden Einfluss Clemens’ von Alexandria – sogenannte „confessores“, also Bekenner, die zwar verfolgt worden, aber dem Martyrium entgangen waren, und Menschen mit einem „engelgleichen Leben“, deren radikal asketisch-jungfräuliches Leben als ständiger Kampf gegen die Verführungen des Satans verstanden wurde, in den Kreis der verehrungswürdigen „Heiligen“ aufgenommen.

Seit dem Frühmittelalter wurden zunehmend entweder große Lichtgestalten der Christenheit (Kirchenlehrer, Könige, sog. „Ritter- und Soldatenheilige“ usw.) oder Menschen, die ein Alternativkonzept zum alltäglichen christlichen Leben boten (Franziskus, Benedikt), vom Volk regional als Heilige verehrt. Bei den sog. „Adelsheiligen“, also Herrschern, Bischöfen oder Ordensgründern, ging die Initiative der Verehrung in den meisten Fällen von deren Nachfolgern im Amt oder Mitgliedern ihrer Dynastie aus, die dadurch auch sich selbst eine stärkere Legitimität zu verschaffen hofften. Die kirchliche Anerkennung folgte im Allgemeinen erst später. Um von offizieller Seite Beliebigkeit und Ausufern der Heiligenkulte zu verhindern, bemühten sich die Päpste, das alleinige Recht zur Heiligsprechung und damit die Kontrolle der Heiligenverehrung zu erlangen, zumal diese aufgrund ihrer Bedeutung für die Beglaubigung politischer und dynastischer Legitimität und nicht zuletzt auch aufgrund ihrer wirtschaftlichen Bedeutung für die Kult- und Wallfahrtsorte auch einen gewichtigen machtpolitischen Faktor darstellte. Im Jahr 993 fand die erste päpstliche Heiligsprechung (Ulrich von Augsburg) statt, im Verlauf des 11. und 12. Jahrhunderts konnten sich die Päpste schließlich gegen die konkurrierenden Instanzen der Konzilien und Ortsbischöfe durchsetzen. Alexander III. dekretierte 1171 die alleinige Zuständigkeit des Papstes für Heiligsprechungen. Aber allgemein verbindlich wurde diese Alleinzuständigkeit erst durch den "Liber Extra" von 1234, einem Nachtrag zum Decretum Gratiani. Im Mittelalter handhabte die römische Kurie die Heiligsprechung sehr zurückhaltend und kanonisierte nur 79 Personen, während die Volksfrömmigkeit auch ohne päpstliche Beteiligung zur selben Zeit Hunderte neuer Heiliger hervorbrachte.

Die christliche Theologie unterscheidet gemäß der sachlichen und terminologischen Klärung des Zweiten Konzils von Nicäa im Jahre 787 die Anbetung (griech. "λατρεια", lat. "adoratio"), die allein Gott vorbehalten ist, von der Verehrung (griech. "δουλεια", lat. "veneratio"), die den Heiligen und ihren Reliquien zukommt. Die sogenannte Dulia ist grundsätzlich von der Latrie, der Anbetung, zu unterscheiden. Innerhalb der Dulia, der Verehrung, wird noch die "Hyperdulia "(„Hochverehrung“) unterschieden, die ausschließlich der Jungfrau Maria zukommt.

Bereits Ambrosius von Mailand hatte im 4. Jahrhundert den römischen Begriff des „patronus“ für die Heiligen verwendet, der die Schutzfunktion des Patrons im Klientelwesen der römischen Gesellschaft beinhaltete. Der im Hochmittelalter zur vollen Ausbildung gelangte Gedanke, sich für Nationen und Diözesen, Kirchen und Städte (Stadtpatron), später gar Stände und Berufe eigene Schutzpatrone zu erwählen, unter deren Schutz und Hilfe man sich stellen wollte, machen das transformierte Verständnis der „Heiligen“ deutlich; Reliquienanhäufung und Drang nach Wundern waren die theologisch unerwünschten Folgen. Das Vierte Laterankonzil verurteilte zwar, „dass die Gläubigen mit phantastischen Geschichten oder gefälschten Dokumenten getäuscht werden, wie es an sehr vielen Orten aus Gewinnsucht zu geschehen pflegt.“ Aber es konnte die Entwicklung in der Praxis nicht aufhalten. Der Charakter der Heiligen als Vorbilder im christlichen Leben ("Imitatio Christi") trat zugunsten der zugeschriebenen Funktionen als Helfer zurück. Die Gläubigen wählten sich zur Fürbitte gezielt Heilige (häufig als Krankheitspatrone) aus, denen man bestimmte Attribute zuschrieb. Blasius half beispielsweise gegen Halskrankheiten, Sebastian gegen die Pest. Auch die Entwicklung des Kultes der vierzehn Nothelfer fällt in diesen Zusammenhang.

Erst die Reformation brachte deutliche Kritik an der herrschenden Situation vor. Eine Rolle der Heiligen als direkte Mittler des Erbetenen wurde mit Verweis auf die Bibel strikt abgelehnt und die Einzigartigkeit der Heilsmittlerschaft Christi wieder in den Vordergrund gerückt. Nach der theologischen Festigung des Luthertums wurde in der Pflege des Gedächtnisses verschiedener altkirchlicher Heiliger keine Gefahr mehr gesehen. Das Heiligengedächtnis wurde in der Confessio Augustana XXI als Moment der persönlichen Stärkung im Glauben befürwortet und anerkannt. Zu den anerkannten „alten“ Heiligen traten zusätzlich Vorreformatoren wie Jan Hus und dann auch Akteure der Reformation – insbesondere Luther selbst – hinzu, so dass verschiedene Theologen Züge einer „Lutherverehrung“ zu erkennen glauben, die sich u. a. in den Lutherbildern in protestantischen Gottesdiensträumen manifestiere.
Im Gegensatz zu den lutherisch geprägten Protestanten lehnten die Reformierten die Heiligenverehrung insgesamt ab. Ulrich Zwingli und Johannes Calvin sahen in Wallfahrten und Reliquienverehrung ein Werk des Satans und betonten die Gültigkeit des alttestamentlichen Bilderverbots, gegen das die Heiligenverehrung verstoße.

Das Konzil von Trient legte 1563 die römisch-katholische Dogmatik in der Frage der Heiligenverehrung genauer fest: Da die Heiligen im Himmel mit Christus herrschten, sei es „gut und nützlich“, sie demütig um Beistand anzurufen, um von Gott durch den alleinigen Erlöser und Heiland Jesus Christus Wohltaten zu erlangen (DH 1821). Ziel der Heiligenverehrung ist damit Gott. Das Zweite Vatikanische Konzil bestätigte diese Lehre und verwies nochmals darauf, dass die Fürbitte der Heiligen bei Gott nicht „heilskonstitutiv“ wie die hohepriesterliche Mittlerfunktion Christi sei (LG 48–69). Die apostolische Konstitution Lumen gentium führt aus, dass die Heiligen „zwar Schicksalsgenossen unserer Menschlichkeit“ seien, dennoch aber „vollkommener dem Bilde Christi gleichgestaltet“ würden. "Wie die christliche Gemeinschaft unter den Erdenpilgern uns näher zu Christus bringt, so verbindet auch die Gemeinschaft mit den Heiligen uns mit Christus, von dem als Quelle und Haupt jegliche Gnade und das Leben des Gottesvolkes selbst ausgehen." (LG 50). 
Das Direktorium über die Volksfrömmigkeit und Liturgie hält fest, die Heiligenfeste verkündigten Christus „in seinen Knechten“, indem sie als Feste der Glieder des Leibes Christi, dessen Haupt, Christus selbst, verherrlichten.

In den Ostkirchen ist die Verehrung von Heiligen ein selbstverständlicher Bestandteil des geistigen Lebens. Seit dem 4. Jahrhundert ist die Darstellungen von Heiligen in Ikonen belegt. Die Verehrung äußert sich bis heute im Malen und Verehren von Ikonen, dem Verfassen und Lesen von Heiligenviten sowie der wieder verstärkt auftretenden Kanonisation. Wie in der katholischen Kirche auch werden die Gräber und Reliquien besucht und verehrt, Menschen, Kirchen und Orte nach ihnen benannt und ihr Gedenktag im Kirchenjahr liturgisch gefeiert. Die Wallfahrt des Pilgers zum Heiligengrab und zuletzt das Sehen, Berühren und Küssen der Reliquie oder der Ikone ist in den Ostkirchen präsenter als im Westen und dient dazu, an der besonderen Gottesnähe des Heiligen selbst teilzuhaben.

Die altkatholische Kirche betrachtet die Verehrung von Heiligen als sinnvoll, unabhängig davon, wie einzelne als Heilige anerkannt und zur Verehrung empfohlen werden. Dabei bleibe allerdings wichtig, dass sich die Heiligenverehrung deutlich von der Form der Anbetung und des Kultes unterscheide, die allein Gottvater, Jesus Christus und dem Heiligen Geist zustehe. Im alt-katholischen Gottesdienst und Gebetsleben wird daher in der Regel nur Gott direkt angeredet. Von den Heiligen wird lediglich erwähnt, dass die Gläubigen in Gemeinschaft mit ihnen stehen, und sie werden als Vorbilder im Glauben vorgestellt. Verehrt werden dabei vor allem Heilige der ungeteilten Kirche des ersten Jahrtausends sowie aus der späteren Zeit Christen, die als besondere Vor- und Leitbilder ökumenische Anerkennung erreicht haben, wie beispielsweise Franz von Assisi, Teresa von Avila, Dietrich Bonhoeffer, Martin Luther King, Edith Stein oder Oscar Romero.
Im haitianischen Voodoo werden vereinzelt christliche Heilige wie Maria, Simon Petrus, Jakobus der Ältere, Philomena von Rom, Patrick von Irland und Ulrich von Augsburg in Gestalt von Voodoo-Geistwesen verehrt; hierbei handelt es sich um einen Fall von Synkretismus.

Die kubanische Santería setzt zahlreiche Orisha (gute Geister) in analoger Weise mit christlichen Heiligen gleich, wovon deutlich mehr Heilige als im Voodoo betroffen sind und die Gleichsetzung den Kern der Religion bildet.

Im Judentum allgemein ist „קדוש“ („kaddosch“, hebräisch: "heilig") ein Wort, das vor allem die einfache Bedeutung von "besonders" oder "das Besondere" hat und damit im Gegenteil zu "profan" (im Sinne von "weltlich", "normal", "alltäglich") steht.

Im orthodoxen Judentum wird auf eine persönliche Heiligkeit nur äußerst zurückhaltend eingegangen. In der religiösen Praxis bildete sich aber de facto trotzdem bereits in alttestamentlicher Zeit die Heiligenverehrung heraus, was sich an der Existenz vieler Heiligengräber festmachen lässt.

Einer gewissen Verehrung der Propheten (besonders Mose) wurde auch von offizieller Seite kein Widerstand entgegengebracht, seit der Zeit des makkabäischen Widerstandskampfes gewann auch das Märtyrertum an Bedeutung. Seit der Spätantike entwickelte sich in der Volksfrömmigkeit ein regelrechter Gräberkult um Grabstätten besonders frommer Juden, oft werden sogar Synagogen über oder in der Nähe eines Grabes erbaut. Besonders stark trat der Typ des Heiligen im osteuropäischen Chassidismus auf, der im „Zaddik“ einen Heilsbringer mit einer besonders engen Gottesbeziehung und einer Mittlerqualität von Gottes Gnade für die Menschen verehrte.

Auch im heutigen Judentum spielen Heiligengräber als Wallfahrtsziele eine Rolle. Prominente Beispiele hierfür sind die Gräber der Erzväter in Hebron, das Davidsgrab in Jerusalem, das Grab des kabbalistischen Rabbiners Schimon ben Jochai in Meron oder des Chabad-Führers Menachem Mendel Schneerson.

Die alttestamentlichen Patriarchen und Propheten wurden auch in die Reihe der christlichen und islamischen Heiligen aufgenommen.

Im Islam hat sich eine Verehrung Heiliger, die dem christlichen Verständnis eines Heiligen nahekommt, bereits früh herausgebildet. Schon bald nach ihrem Tod wurden etwa in der schiitischen Richtung ʿAlī ibn Abī Tālib, der Schwiegersohn Mohammeds, und seine Söhne Hasan ibn Ali und Husain ibn Ali als Heilige verehrt. Auch bei den Sunniten treten solche Heilige auf, unter anderem al-Chidr (al-Ḫiḍr, „der grüne Mann“), der von Muslimen mit den Propheten Elischa und Elija sowie von orientalischen Christen mit dem heiligen Georg identifiziert wird. 

Viele Heilige wurden als „Freunde Gottes“ ("auliyāʾ Allāh") bezeichnet. Hierbei handelt es sich um einen Begriff, der schon im Koran (Sure 10:62) vorkommt. Auch wenn der „Freund Gottes“ ein durchgehend gehorsames und gottgefälliges Leben geführt hat, rückt er nicht durch eigene Leistung, sondern vielmehr erst durch Allahs Wirken in eine Nähe zu jenem. Es gibt kein offizielles Heiligsprechungsverfahren, und die Verehrung einer Person als Heiligem ergibt sich aus dem Konsens der Gläubigen. Daher kann nicht nur Menschen aus der Zeit nach Mohammed, sondern auch Propheten und Patriarchen aus der Zeit zuvor die Heiligkeit zugesprochen werden. 

Das Bild des Heiligen im Islam ist davon geprägt, dass Heilige Fürsprecher und Mittler zwischen den Gläubigen und dem verborgenen Allah sind, Wunder wirken können und als Wächter des Glaubens gelten. Viele Gräber von islamischen Heiligen sind bis heute Ziel von Ziyāra-Wallfahrten. Andere Orte werden besucht, weil sie in irgendeiner anderen Beziehung zum betreffenden Heiligen stehen. Die Wallfahrtsorte werden von den Pilgern als Kraftquelle gesehen werden, da die spirituelle Energie "(Baraka)" eines Heiligen nach muslimischer Auffassung auch über den irdischen Tod hinaus wirkt, teilweise sogar für stärker gehalten wird als zu Lebzeiten. Der Heilige erhält seine Baraka über eine spirituelle Kette "(Silsila)," die ihn mit der Familie des Propheten verbindet.

Die spätere islamische Heiligenverehrung bezieht sich meist auf bekannte Mystiker (Sufis). Häufig wirkten diese auch als Oberhaupt "(Scheich)" eines Sufiordens "(Tariqa)," wie sie verstärkt ab dem 12. und 13. Jahrhundert entstanden. Zu jener Zeit, die als eine erste Blütezeit des Sufismus gilt, fanden die islamischen Mystiker eine große Resonanz auch in der breiten Bevölkerung, wodurch sich noch heute die starke auf diese Personen bezogene Verehrung erklären lässt. Einer der international bekanntesten Heiligen ist ʿAbd al-Qādir al-Dschīlānī, dessen Grab in Bagdad Pilger aus der ganzen islamischen Welt angezogen hat. Träume von ʿAbd al-Qādir al-Dschīlānī spielen auch eine große Rolle in der islamischen Mystik. So soll er zum Beispiel dem westafrikanischen religiösen Anführer Usman dan Fodio erschienen sein und ihm das „Schwert der Wahrheit“ verliehen haben.

In einigen sunnitischen Gruppen wie der Bewegung der Wahhabiten (Salafismus) und den Ahl-i Hadīth wird eine Heiligenverehrung explizit bekämpft, da sie dem Prinzip der absoluten Einzigartigkeit und Erhabenheit Gottes "(tauhid)" zuwiderlaufe und ein nicht auf Gott, sondern auf Menschen gerichteter Kult sei. Bei der Ablehnung der Heiligenverehrung beruft man sich in diesen Kreisen auf den Koran (Sure 9:31; 10:19). 

→ "Siehe auch:" Marabout, Derwisch, , Volksislam

Heilige der indischen Religionen des Hinduismus, Buddhismus und Jainismus lassen sich grob dadurch charakterisieren, dass sie in radikaler Askese und Meditation einen höheren Bewusstseinsstand (Erleuchtung) erreicht haben sollen. Der Mittlercharakter zwischen göttlicher Autorität und Menschen tritt bei den verbreiteten atheistischen oder agnostischen Konzepten entsprechend nicht auf.

Die ungenaue Kategorie des „Hinduismus“ macht eine allgemein gültige Definition eines „hinduistischen Heiligen“ praktisch unmöglich. Es lässt sich aber eine relativ weit verbreitete Verehrung bestimmter religiöser Lehrer, die in ihrer Zeit das Gesicht des Hinduismus prägten, wie Shankara, Ramakrishna oder auch Gandhi, beobachten.
Im Buddhismus ist die Vorstellung von Heiligen konkreter vorhanden. Der Hinayana sieht die individuelle Heiligkeit darin gegeben, dass ein Mensch, der Arhat, nach streng asketischem Leben und Beachtung der Lehren Buddhas bereits zu Lebzeiten das Nirvana erreicht und damit aus dem Kreislauf der Wiedergeburten ausscheidet. Auch Siddhartha Gautama, der die vier edlen Wahrheiten erkannt und in der Meditation zu vollkommener innerer Ruhe gefunden hat, fällt unter die Kategorie des Heiligen.

Der bereits im Hinayana präsente Gedanke einer Verehrung der Reliquien Buddhas setzte sich im Mahayana verstärkt fort. Hier werden zusätzlich die Bodhisattvas als Heilige verehrt, weil sie zwar die Erleuchtung bereits erlangt haben, aus Altruismus („Mitgefühl“) aber auf das Nirvana verzichten und andere Menschen ebenfalls zur Erleuchtung führen wollen. Über ihren Gräbern und Reliquien wurden Stupas errichtet, die auch heute noch beispielsweise in Thailand in Ehrerbietung barfuß rechtsherum andächtig umschritten wird, zumeist verbunden mit Blumen-, Weihrauch- und Kerzen-Opfern. Berühmte Heilige des tibetischen Buddhismus sind z. B. Padmasambhava, Milarepa und Tsongkhapa.

Im Jainismus schließlich werden 63 exemplarische Menschen, darunter die 24 sogenannten Tirthankaras („Furtbereiter, Bahnbrecher“), als Heilige verehrt, weil sie, obwohl sie selbst bereits Erlösung aus dem Kreislauf der Wiedergeburten gefunden haben, in immer wiederkehrenden Abständen den Menschen den Weg zur Erleuchtung aufgezeigt haben.

Im Konfuzianismus war der Begriff des „Heiligen“ stets mit dem des „Edlen“ konnotiert, der die fünf konfuzianischen Kardinaltugenden, Menschlichkeit (Ren, 仁), Gerechtigkeit (Yi, 義), Ethisches Verhalten (Li, 礼), Weisheit (Zhi, 智) und Aufrichtigkeit (Xin, 信) in sich vereint. Neben Konfuzius selbst und seinen Schülern zählten dazu vor allem ideale mythische Herrscher und die regierenden Kaiser.

Der Daoismus dagegen verehrte verschiedene historische Gestalten, denen zugeschrieben wurde, in Übereinstimmung mit dem Dao gelebt zu haben (z. B. die sogenannten „Acht Unsterblichen“). Sie werden oft als mit übernatürlichen Fähigkeiten versehen vorgestellt, die auch vor Krankheit und Tod bewahren können, und sie sind Unsterbliche. Sie gelten im Pantheon des Daoismus häufig auch als Gottheiten.





</doc>
<doc id="13637" url="https://de.wikipedia.org/wiki?curid=13637" title="Pythons">
Pythons

Die Pythons (Pythonidae; altgr. "Pythōn"; Einzahl "der", allgemeinsprachlich auch "die" Python) sind eine Familie von Schlangen aus der Überfamilie der Pythonoidae. Aufgrund von körperlichen Merkmalen wurden sie lange mit den Boaschlangen als Riesenschlangen zusammengefasst, wobei aktuelle genetische Untersuchungen an der nahen Verwandtschaft der beiden Familien zweifeln lassen. Bezogen auf Familie oder Unterfamilie spricht man auch von Pythonschlangen. Ebenfalls als Pythons werden die Vertreter der Gattung Eigentliche Pythons ("Python") bezeichnet.

Die Pythons sind Bewohner der Alten Welt, wo sie vor allem in Afrika, Süd- und Südostasien sowie in Australien vorkommen. Die Verbreitungsgebiete liegen hauptsächlich zwischen den Wendekreisen: Der nördliche Wendekreis wird von den beiden Asiaten "Python molurus" und "Python bivittatus" erreicht, in Afrika lebt eine Art, in Australien mehrere südlich des südlichen Wendekreises. Im Miozän lebten auch in Europa Pythons.
Seit einigen Jahren hat sich der Tigerpython durch illegale Aussetzungen als Neozoon in Florida etabliert.

Die kleinste Art der Gattung Südpythons ("Antaresia perthensis") wird nur etwa 70–90 Zentimeter lang. Einige Arten der Gattungen der Eigentlichen Pythons ("Python") und "Malayopython" zählen mit gesicherten Maximallängen über sechs Meter zu den größten Schlangen der Welt.

Pythons sind ungiftig und töten ihre Beute durch Umschlingen. Alle Pythons haben ein Grubenorgan, können also Infrarotstrahlen erfassen und so auch im Dunkeln jagen. Bei den Schlangen der Gattung "Antaresia" sind die Labialgruben, anders als bei allen anderen Schlangen dieser Familie, von außen nicht sichtbar. Während bisher davon ausgegangen wurde, dass bei "Antaresia" keine Labialgruben vorhanden sind und die Gattung deshalb eine frühe Entwicklungsstufe der Pythons darstellt, zeigen neuere Untersuchungen, dass auch bei diesen Schlangen eine Labialgrube unter der Rostralen vorhanden ist.

Pythons sind ovipar, also im Gegensatz zu den ovoviviparen Boidae eierlegend. Pythonweibchen legen je nach Art zwei bis über 100 Eier ab. Die Weibchen betreiben eine spezielle Form der Brutpflege. Sie liegen bis zum Schlupf der Jungtiere in Schlingen um ihr Gelege. Dabei lässt sich zeigen, dass die Temperatur im Innern des Geleges über der Außentemperatur liegt. Eine Reihe von Arten erzeugt Wärme durch Muskelzittern.

Früher wurden die Pythons aufgrund von körperlichen Merkmalen mit den Familien Boaschlangen (Boinae) und den Sandboas (Erycinae) typischerweise als Würgeschlangen in die Überfamilie der Riesenschlangenartigen zusammengefasst, doch widerlegen aktuelle genetische Untersuchungen diese enge Verwandtschaft. Abschließend ist die Systematik der Schlangen noch nicht geklärt, allerdings stimmen etliche Untersuchungen darin überein, dass die nächsten Verwandten der Pythonschlangen (Pythonidae) die Familie der Spitzkopfpythons (Loxocemidae) und die Familie der Erdschlangen (Xenopeltidae) darstellen. Zusammen formen sie demnach die Überfamilie Pythonoidae. Die beiden Familien Loxocemidae und Xenopeltidae waren bisher eine zu den Riesenschlangen basale Gruppe, für die beispielsweise Lee et al. 14 morphologische Unterschiede beschreibt. Welche dieser Unterschiede für die neue Gruppierung noch gültig sind, müssen Untersuchungen noch zeigen.

Die Familie Pythonidae selbst umfasst insgesamt 40 Arten in 8 Gattungen:

Die Gattung "Python" ist die artenreichste. Die Gattungen "Malayopython" und "Simalia" wurden erst Anfang 2014 von Reynolds et al. eingeführt.

Das folgende Kladogramm zeigt die innere Systematik der Pythons mit den Erdschlangen und dem Spitzkopfpython als Außengruppe:
Die meisten Arten der Familie Pythonidae sind in der Europäischen Artenschutzverordnung gelistet und dürfen ohne Genehmigung gehalten werden. Gemäß der Bundesartenschutzverordnung ist die Haltung der zuständigen Landesbehörde gegenüber jedoch meldepflichtig.

"Python molurus molurus", der helle Tigerpython ist im Anhang A der Europäischen Artenschutzverordnung geführt und darf ohne Genehmigung nicht gehalten werden.




</doc>
<doc id="13638" url="https://de.wikipedia.org/wiki?curid=13638" title="Python (Programmiersprache)">
Python (Programmiersprache)

Python ([], [], auf Deutsch auch []), ist eine universelle, üblicherweise interpretierte höhere Programmiersprache. Sie hat den Anspruch, einen gut lesbaren, knappen Programmierstil zu fördern. So werden beispielsweise Blöcke nicht durch geschweifte Klammern, sondern durch Einrückungen strukturiert. Wegen ihrer klaren und übersichtlichen Syntax gilt Python als einfach zu erlernen.

Python unterstützt mehrere Programmierparadigmen, z. B. die objektorientierte, die aspektorientierte und die funktionale Programmierung. Ferner bietet es eine dynamische Typisierung. Wie viele dynamische Sprachen wird Python oft als Skriptsprache genutzt.
Die Sprache hat ein offenes, gemeinschaftsbasiertes Entwicklungsmodell, das durch die gemeinnützige Python Software Foundation, die "de facto" die Definition der Sprache in der Referenzumsetzung CPython pflegt, gestützt wird.

Die Sprache wurde Anfang der 1990er Jahre von Guido van Rossum am Centrum Wiskunde & Informatica in Amsterdam als Nachfolger für die Programmier-Lehrsprache "ABC" entwickelt und war ursprünglich für das verteilte Betriebssystem Amoeba gedacht.

Der Name geht nicht etwa (wie das Logo vermuten ließe) auf die gleichnamige Schlangengattung (Pythons) zurück, sondern bezog sich ursprünglich auf die englische Komikertruppe Monty Python. In der Dokumentation finden sich daher auch einige Anspielungen auf Sketche aus dem Flying Circus. Trotzdem etablierte sich die Assoziation zur Schlange, was sich unter anderem in der Programmiersprache Cobra sowie dem Python-Toolkit „Boa“ äußert.

Die erste Vollversion erschien im Januar 1994 unter der Bezeichnung Python 1.0. Gegenüber früheren Versionen wurden einige Konzepte der funktionalen Programmierung implementiert, die allerdings später wieder aufgegeben wurden. Von 1995 bis 2000 erschienen Updates, die fortlaufend als Python 1.1, 1.2 etc. bezeichnet wurden.

Python 2.0 erschien am 16. Oktober 2000. Neue Funktionen umfassten eine voll funktionsfähige Garbage Collection (automatische Speicherbereinigung) und die Unterstützung für den Unicode-Zeichensatz. In Version 2.6 wurde eine Hilfe eingebaut, mit der angezeigt werden kann, welche Code-Sequenzen vom Nachfolger Python 3 nicht mehr unterstützt werden und daher in darauf aufbauenden Versionen nicht mehr lauffähig sind.

Python 3.0 (auch Python 3000) erschien am 3. Dezember 2008 nach längerer Entwicklungszeit. Es beinhaltet einige tiefgreifende Änderungen an der Sprache, etwa das Entfernen von Redundanzen bei Befehlssätzen und veralteten Konstrukten. Da Python 3.0 hierdurch teilweise inkompatibel zu früheren Versionen ist, beschloss die Python Software Foundation, Python 2.7 parallel zu Python 3 vorerst weiter durch die Versorgung mit Updates zu unterstützen.

Am 23. Dezember 2016 wurde die neueste Version 3.6 veröffentlicht.

Python wurde mit dem Ziel größter Einfachheit und Übersichtlichkeit entworfen. Dies wird vor allem durch zwei Maßnahmen erreicht. Zum einen kommt die Sprache mit relativ wenigen Schlüsselwörtern aus. Zum anderen ist die Syntax reduziert und auf Übersichtlichkeit optimiert. Dadurch lassen sich Python-basierte Skripte deutlich knapper formulieren als in anderen Sprachen.

Van Rossum legte bei der Entwicklung großen Wert auf eine Standardbibliothek, die überschaubar und leicht erweiterbar ist. Dies war Ergebnis seiner schlechten Erfahrung mit der Sprache ABC, in der das Gegenteil der Fall ist.

Durch dieses Konzept wurde es auch ermöglicht, Python-Programme in anderen Sprachen als Module einzubetten. Dadurch können Schwächen von Python umgangen werden. Beispielsweise können für zeitkritische Teile Routinen in maschinennäheren Sprachen wie z. B. C aufgerufen werden. Umgekehrt lassen sich mit Python Module und Plug-ins für andere Programme schreiben, die die entsprechende Unterstützung bieten. Dies ist z. B. bei Blender, Cinema 4D, GIMP, Maya, OpenOffice bzw. LibreOffice, PyMOL, SPSS, QGIS oder KiCad der Fall.

Python ist eine Multiparadigmensprache. Das bedeutet, Python zwingt den Programmierer nicht zu einem einzigen Programmierstil, sondern erlaubt, das für die jeweilige Aufgabe am besten geeignete Paradigma zu wählen. Objektorientierte und strukturierte Programmierung werden vollständig unterstützt, funktionale und aspektorientierte Programmierung werden durch einzelne Elemente der Sprache unterstützt.

Die Datentypen werden dynamisch verwaltet, eine statische Typprüfung wie z. B. bei C++ gibt es nicht. Die Freigabe nicht mehr benutzter Speicherbereiche erfolgt durch Referenzzählung. Unicode-Unterstützung wurde in der Version 2.0 eingeführt.

Die wesentlichen Ziele und Regeln der Sprache wurden mit einer Reihe kurzer, humorvoller Phrasen unter dem Titel "The Zen of Python" im Jahr 2004 herausgegeben.

Python besitzt eine größere Anzahl von grundlegenden Datentypen. Neben der herkömmlichen Arithmetik unterstützt es transparent auch beliebig große Ganzzahlen und komplexe Zahlen.

Die Sprache verfügt über die übliche Ausstattung an Zeichenkettenoperationen. Zeichenketten sind in Python allerdings unveränderliche Objekte (wie auch in Java). Daher geben Operationen, die das Ändern einer Zeichenkette bewerkstelligen sollen – wie z. B. das Ersetzen von Zeichen – immer eine neue Zeichenkette zurück.

In Python ist alles ein Objekt; Klassen, Typen, Methoden, Module etc. Der Datentyp ist jeweils an das Objekt (den "Wert") gebunden und nicht an eine Variable, d. h. Datentypen werden dynamisch vergeben, so wie bei Smalltalk oder Lisp – und nicht wie bei Java.

Trotz der dynamischen Typverwaltung enthält Python eine gewisse Typprüfung. Diese ist strenger als bei Perl, aber weniger strikt als etwa bei Objective CAML.
Implizite Umwandlungen nach dem Duck-Typing-Prinzip sind unter anderem für numerische Typen definiert, so dass man beispielsweise eine komplexe Zahl mit einer langen Ganzzahl ohne explizite Typumwandlung multiplizieren kann.
Mit dem Format-Operator codice_1 gibt es eine implizite Umwandlung eines Objekts in eine Zeichenkette. Der Operator codice_2 überprüft zwei Objekte auf (Wert-)Gleichheit. Der Operator codice_3 überprüft die tatsächliche Identität zweier Objekte.

Python besitzt mehrere "Sammeltypen", darunter Listen, Tupel, Mengen (Sets) und assoziative Arrays (Dictionaries). Listen, Tupel und Zeichenketten sind "Folgen" (Sequenzen, Arrays) und kennen fast alle die gleichen Methoden: Über die Zeichen einer Kette kann man ebenso iterieren wie über die Elemente einer Liste. Außerdem gibt es die unveränderlichen Objekte, die nach ihrer Erzeugung nicht mehr geändert werden können. Listen sind z. B. erweiterbare Felder (Arrays), wohingegen Tupel und Zeichenketten eine feste Länge haben und unveränderlich sind.

Der Zweck solcher Unveränderlichkeit hängt z. B. mit den "Wörterbüchern" zusammen, einem Datentyp, der auch als "assoziatives Array" bezeichnet wird. Um die Datenkonsistenz zu sichern, müssen die "Schlüssel" eines Wörterbuches vom Typ „unveränderlich“ sein. Die ins Wörterbuch eingetragenen "Werte" können dagegen von beliebigem Typ sein.

"Sets" sind Mengen von Objekten und in CPython ab Version 2.4 im Standardsprachumfang enthalten. Diese Datenstruktur kann beliebige (paarweise unterschiedliche) Objekte aufnehmen und stellt Mengenoperationen wie beispielsweise Durchschnitt, Differenz und Vereinigung zur Verfügung.

Das Typsystem von Python ist auf das Klassensystem abgestimmt. Obwohl die eingebauten Datentypen genau genommen keine Klassen sind, können Klassen von einem Typ erben. So kann man die Eigenschaften von Zeichenketten oder Wörterbüchern erweitern – auch von Ganzzahlen. Python unterstützt Mehrfachvererbung.

Die Sprache unterstützt direkt den Umgang mit Typen und Klassen. Typen können ausgelesen (ermittelt) und verglichen werden und verhalten sich wie Objekte – in Wirklichkeit sind die Typen (wie in Smalltalk) selbst ein Objekt. Die Attribute eines Objektes können als Wörterbuch extrahiert werden.

Eines der Entwurfsziele für Python war die gute Lesbarkeit des Quellcodes. Die Anweisungen benutzen häufig englische Schlüsselwörter, wo andere Sprachen Symbole einsetzen. Darüber hinaus besitzt Python weniger syntaktische Konstruktionen als viele andere strukturierte Sprachen wie C, Perl oder Pascal:

Beim letzten Punkt bieten andere Programmiersprachen zusätzlich codice_7 und/oder codice_8. Diese wurden zugunsten der Lesbarkeit in Python weggelassen und müssen durch codice_9-Konstrukte oder andere Verzweigungsmöglichkeiten (Slices, Wörterbücher) abgebildet werden. Im Gegensatz zu vielen anderen Sprachen können codice_4- und codice_5-Schleifen einen codice_12-Zweig haben. Dieser wird nur ausgeführt, wenn die Schleife vollständig durchlaufen wurde und nicht mittels codice_13 abgebrochen wird.

Python benutzt wie Miranda und Haskell Einrückungen als Strukturierungselement. Diese Idee wurde erstmals von Peter J. Landin vorgeschlagen und von ihm off-side rule („Abseitsregel“) genannt. In den meisten anderen Programmiersprachen werden Blöcke durch Klammern oder Schlüsselwörter markiert, während verschieden große Leerräume außerhalb von Zeichenketten keine spezielle Semantik tragen. Bei diesen Sprachen ist die Einrückung zur optischen Hervorhebung eines Blockes zwar erlaubt und in der Regel auch erwünscht, aber nicht vorgeschrieben. Für Programmierneulinge wird der Zwang zu lesbarem Stil aber als Vorteil gesehen.

Hierzu als Beispiel die Berechnung der Fakultät einer Ganzzahl, einmal in C und einmal in Python:

Fakultätsfunktion in C:

int fakultaet(int x)

Die gleiche Funktion in Python:

def fakultaet(x):
Es ist jedoch darauf zu achten, die Einrückungen im gesamten Programmtext gleich zu gestalten. Die gemischte Verwendung von Leerzeichen und Tabulatorzeichen kann zu Problemen führen, da der Python-Interpreter Tabstops im Abstand von acht Leerzeichen annimmt. Je nach Konfiguration des Editors können Tabulatoren optisch mit weniger als acht Leerzeichen dargestellt werden, was zu Syntaxfehlern oder ungewollter Programmstrukturierung führen kann. Als vorbeugende Maßnahme kann man den Editor Tabulatorzeichen durch eine feste Anzahl von Leerzeichen ersetzen lassen. Die Python-Distribution enthält in der Standardbibliothek das Modul "tabnanny", welches die Vermischung von Tabulator- und Leerzeichen zu erkennen und beheben hilft.

Man kann die Fakultätsfunktion aber auch wie in C einzeilig mit ternärem Operator formulieren:

Die Fakultätsfunktion in C:

int fakultaet(int x)

Die Fakultätsfunktion in Python:

def fakultaet(x):
Coconut und andere Erweiterungen erleichtern das funktionale Programmieren in Python, darüber hinaus lässt sich dies auch mit dem herkömmlichen Python realisieren:

Ausdrucksstarke syntaktische Elemente zur funktionalen Programmierung vereinfachen das Arbeiten mit Listen und anderen Sammeltypen. Eine solche Vereinfachung ist die Listennotation, die aus der funktionalen Programmiersprache Haskell stammt; hier bei der Berechnung der ersten fünf Zweierpotenzen:

zahlen = [1, 2, 3, 4, 5]
zweierpotenzen = [2 ** n for n in zahlen]

Weil in Python Funktionen als Argumente auftreten dürfen, kann man auch ausgeklügeltere Konstruktionen ausdrücken, wie den Continuation-passing style.

Pythons Schlüsselwort codice_14 könnte manche Anhänger der funktionalen Programmierung fehlleiten. Solche codice_14-Blöcke in Python können nur Ausdrücke enthalten, aber keine Anweisungen. Damit werden solche Anweisungen generell nicht verwendet, um eine Funktion zurückzugeben. Die übliche Vorgehensweise ist stattdessen, den Namen einer lokalen Funktion zurückzugeben. Das folgende Beispiel zeigt dies anhand einer einfachen Funktion nach den Ideen von Haskell Brooks Curry:

def add_and_print_maker(x):
Damit ist auch Currying auf einfache Art möglich, um generische Funktionsobjekte auf problemspezifische herunterzubrechen. Hier ein einfaches Beispiel:

def curry(func, knownargument):
Wird die codice_16-Funktion aufgerufen, erwartet diese eine Funktion mit zwei notwendigen Parametern sowie die Parameterbelegung für den zweiten Parameter dieser Funktion. Der Rückgabewert von codice_16 ist eine Funktion, die dasselbe tut wie codice_18, aber nur noch einen Parameter benötigt.

Anonyme Namensräume (sog. Closures) sind mit den o. g. Mechanismen in Python ebenfalls einfach möglich. Ein simples Beispiel für einen Stack, intern durch eine Liste repräsentiert:

def stack():

pop, push, is_empty = stack()
Auf diese Weise erhält man die drei Funktionsobjekte codice_19, codice_20, codice_21, um den Stack zu modifizieren bzw. auf enthaltene Elemente zu prüfen, ohne codice_22 direkt modifizieren zu können.

Python nutzt ausgiebig die Ausnahmebehandlung (engl. "exception handling") als ein Mittel, um Fehlerbedingungen zu testen. Dies ist so weit in Python integriert, dass es teilweise sogar möglich ist, Syntaxfehler abzufangen und zur Laufzeit zu behandeln.

Ausnahmen haben einige Vorteile gegenüber anderen beim Programmieren üblichen Verfahren der Fehlerbehandlung (wie z. B. Fehler-Rückgabewerte und globale Statusvariablen). Sie sind Thread-sicher und können leicht bis in die höchste Programmebene weitergegeben oder an einer beliebigen anderen Ebene der Funktionsaufruffolge behandelt werden. Der korrekte Einsatz von Ausnahmebehandlungen beim Zugriff auf dynamische Ressourcen erleichtert es zudem, bestimmte auf Race Conditions basierende Sicherheitslücken zu vermeiden, die entstehen können, wenn Zugriffe auf bereits veralteten Statusabfragen basieren.

Der Python-Ansatz legt den Einsatz von Ausnahmen nahe, wann immer eine Fehlerbedingung entstehen könnte. Nützlich ist dieses Prinzip beispielsweise bei der Konstruktion robuster Eingabeaufforderungen:

while True:
Dieses Programmstück fragt den Benutzer so lange nach einer Zahl, bis dieser eine Zeichenfolge eingibt, die sich per codice_23 in eine Ganzzahl konvertieren lässt. Durch die Ausnahmebehandlung wird hier vermieden, dass eine Fehleingabe zu einem Laufzeitfehler führt, der das Programm zum Abbruch zwingt.

Ebenso kann auch das hier nicht berücksichtigte Interrupt-Signal (codice_24, häufig Strg+C) mittels Ausnahmebehandlung in Python abgefangen und behandelt werden (codice_25).

Python verfügt über eine große Standardbibliothek, wodurch es sich für viele Anwendungen gut eignet.
Sie ist eine der größten Stärken von Python. Der überwiegende Teil davon ist plattformunabhängig, so dass auch größere Python-Programme oft auf Unix, Windows, macOS und anderen Plattformen ohne Änderung laufen.
Die Module der Standardbibliothek können mit in C oder Python selbst geschriebenen Modulen ergänzt werden.

Die Standardbibliothek ist besonders auf Internetanwendungen zugeschnitten, mit der Unterstützung einer großen Anzahl von Standardformaten und -protokollen (wie MIME und HTTP). Module zur Schaffung grafischer Schnittstellen, zur Verbindung mit relationalen Datenbanken und zur Manipulation regulärer Ausdrücke sind ebenfalls enthalten.

Mit Hilfe des mitgelieferten Moduls Tkinter kann in Python (wie in Perl und Tcl) schnell eine grafische Benutzeroberfläche (GUI) mit Tk erzeugt werden. Es gibt darüber hinaus eine Vielzahl von weiteren Wrappern von anderen Anbietern. Sie stellen Anbindungen () zu GUI-Toolkits wie z. B. PyGTK, PyQt, wxPython, PyObjC und PyFLTK zur Verfügung.

Neben Tkinter wird auch ein Modul zum Zeichnen von Turtle-Grafiken mitgeliefert.

from tkinter import *
fenster = Tk()
fenster.geometry("200x100")
label = Label(fenster, text="Hallo Welt!")
label.pack()
def befehl():
button = Button(fenster, text="OK", command=befehl)
button.pack()

from turtle import *
speed(0)
reset()
x = -200
y = 200
while y != -200:

Als nicht triviales Beispiel sei hier der kompakte Sortieralgorithmus Quicksort angegeben:
def quicksort(liste):
Hier ermöglicht insbesondere die Listennotation für die Variablen "links" und "rechts" eine kompakte Darstellung. Zum Vergleich eine iterative Formulierung dieser zwei Zeilen:

Dies ist nur ein Beispiel für die gesparte Schreibarbeit durch die Listennotation. Tatsächlich ist in diesem Fall die iterative Formulierung die schnellere, da pro Durchgang nur einmal über das Array „liste“ iteriert wird, und nicht zweimal wie in der Listennotation.

So wie Lisp, Ruby, Groovy und Perl unterstützt der Python-Interpreter auch einen "interaktiven Modus", in dem Ausdrücke am Terminal eingegeben und die Ergebnisse sofort betrachtet werden können. Das ist nicht nur für Neulinge angenehm, die die Sprache lernen, sondern auch für erfahrene Programmierer: Code-Stückchen können interaktiv ausgiebig getestet werden, bevor man sie in ein geeignetes Programm aufnimmt.

Darüber hinaus steht mit Python Shell ein Kommandozeileninterpreter für verschiedene unixoide Computer-Betriebssysteme zur Verfügung, der neben klassischen Unix-Shellkommandos auch direkte Eingaben in Python-Form verarbeiten kann. IPython ist eine populäre interaktive Python-Shell mit stark erweiterter Funktionalität.

Neben der Referenzimplementierung CPython gibt es einen in Java implementierten Python-Interpreter namens Jython, mit dem die Bibliothek der Java-Laufzeitumgebung für Python verfügbar gemacht wird. Außer den Interpretern existieren Compiler, die Python-Code in eine andere Programmiersprache übersetzen: Mit Cython kann Python-Code in effiziente C-Erweiterungen übersetzt oder externer C/C++-Code angebunden werden. Ebenso existiert der Compiler IronPython für die .NET- bzw. Mono-Plattform. Um Python als Skriptsprache für Programme in C++ zu nutzen, werden zumeist die Boost-Python-Bibliothek oder (in neueren Projekten) Cython verwendet. Ein Python-Parser für Parrot und ein in Python geschriebener Interpreter für Python, PyPy, welcher von der EU gefördert wurde, sind ebenfalls in Entwicklung. Es existiert ein Python-Interpreter für Mikrocontroller namens MicroPython.

Neben IDLE, das oft mit Python installiert wird und im Wesentlichen aus einer Textumgebung und einer Shell besteht, wurden auch einige vollwertige Entwicklungsumgebungen (IDEs) für Python entwickelt, beispielsweise Eric Python IDE oder PyCharm. Des Weiteren existieren Plug-ins für größere IDEs wie Eclipse, Visual Studio und NetBeans. Texteditoren für Programmierer wie Vim und Emacs lassen sich gegebenenfalls auch für Python anpassen. Allerdings ist keine IDE notwendig, da Pythoncode nicht kompiliert werden muss, sodass sich Skripte grundsätzlich auch mit einem beliebigen Texteditor schreiben lassen.

Für die verschiedenen GUI-Toolkits, wie z. B. Tkinter (GUI-Builder), WxPython (wxGlade), PyQt (Qt Designer), PySide, PyGTK (Glade), Kivy oder PyFLTK gibt es teils eigene Editoren, mit denen sich grafische Benutzeroberflächen auf vergleichsweise einfache Art aufbauen lassen.

Python unterstützt die Erstellung von Paketen; dabei helfen distutils und setuptools. Die Pakete werden auf PyPI, dem Python Package Index, gespeichert und von dort zur Installation abgerufen. Als Paketmanager wird üblicherweise pip oder auf alten Systemen auch easy_install eingesetzt.

Python ist für die meisten gängigen Betriebssysteme frei erhältlich und bei den meisten Linux-Distributionen im Standardumfang enthalten. Um Python in Webserver einzubinden, wird Webserver-umgreifend WSGI verwendet, welches die Nachteile von CGI umgeht. WSGI stellt eine universelle Schnittstelle zwischen Webserver und Python(-Framework) zur Verfügung.

Eine Reihe von Web-Application-Frameworks nutzt Python, darunter Django, Pylons, SQLAlchemy, TurboGears, web2py, Flask und Zope. Ferner gibt es einen Python-Interpreter für das Symbian-Betriebssystem, so dass Python auf verschiedenen Mobiltelefonen verfügbar ist. In der Version 2.5.1 ist Python ein Bestandteil von AmigaOS 4.0. Außerdem basieren mehrere bekannte kommerzielle Projekte, etwa Google und YouTube, in Teilen auf Python. Auch in der Spieleindustrie findet die Sprache bisweilen Einsatz, etwa in "EVE Online", "World in Conflict" und "Civilization IV".

Im Rahmen des Projektes 100-Dollar-Laptop wird Python als Standardsprache der Benutzeroberfläche verwendet. Da der 100-Dollar-Laptop für die Schulausbildung von Kindern konzipiert ist, soll bei Benutzung der dafür gestalteten grafischen Benutzeroberfläche „Sugar“ auf Knopfdruck der gerade laufende Python-Quellcode angezeigt werden. Damit soll Kindern die Möglichkeit gegeben werden, die dahinter liegende Informationstechnologie real zu erleben und nach Belieben „hinter die Kulissen“ zu schauen.

In der Wissenschaftsgemeinde genießt Python große Verbreitung, hauptsächlich wegen des einfachen Einstiegs in die Programmierung und der großen Auswahl wissenschaftlicher Bibliotheken. Numerische Rechnungen und die visuelle Aufbereitung der Ergebnisse in Graphen werden meist mit NumPy und der Matplotlib erledigt. Es existiert ein Projekt namens SciPy, welches viele wissenschaftliche Bibliotheken Pythons bündelt und somit einfacher zugänglich macht. Mit TensorFlow gibt es eine große Bibliothek zur Forschung und Nutzung von künstlicher Intelligenz und maschinellem Lernen.

Bei der Definition (aber nicht beim Aufruf) von Methoden muss der Parameter "self", der der Instanz entspricht, deren Methode aufgerufen wird, explizit angegeben werden. Dies wird oft als unelegant und nicht objektorientiert empfunden. Es ist aber nötig, um bestimmte wichtige Konstrukte zu ermöglichen; außerdem entspricht es dem Python-Grundsatz „Explicit is better than implicit“.

Bis zur Version 3.0 wurde kritisiert, dass in einer Methodendefinition der Aufruf der Basisklassenversion derselben Methode die explizite Angabe der Klasse und Instanz erfordert. Dies wurde als Verletzung des DRY-Prinzips („Don’t repeat yourself“) gesehen, außerdem behinderte es Umbenennungen. In Python 3.0 wurde dieser Kritikpunkt behoben.

Ferner wird die mangelnde statische Typsicherheit der Programmiersprache kritisiert.

Auf Multiprozessor-Systemen behindert der sogenannte "Global Interpreter Lock (GIL)" von CPython die Effizienz von Python-Anwendungen, die softwareseitiges Multithreading benutzen. Diese Beschränkung existiert unter Jython oder IronPython allerdings nicht. Bislang ist von offizieller Seite nicht geplant, den GIL zu ersetzen. Stattdessen wird empfohlen, mehrere miteinander kommunizierende Prozesse anstelle von Threads zu verwenden.

In den aktuell vorherrschenden Implementationen ist die Geschwindigkeit niedriger als bei vielen kompilierbaren Sprachen, aber ähnlich wie bei Perl, PHP, Dart und Ruby. Das liegt zum Teil daran, dass bei der Entwicklung von CPython der Klarheit des Codes gegenüber der Geschwindigkeit Vorrang eingeräumt wird. Man beruft sich dabei auf Autoritäten wie Donald Knuth und Tony Hoare, die von verfrühter Optimierung abraten. Wenn Performanceprobleme auftreten, die nicht durch Optimierung des Python-Codes gelöst werden können, werden stattdessen JIT-Compiler wie PyPy verwendet oder zeitkritische Funktionen in maschinennähere Sprachen wie C oder Cython ausgelagert.

Für den Einstieg

Referenzen

Weiterführendes



</doc>
<doc id="13648" url="https://de.wikipedia.org/wiki?curid=13648" title="Yacht">
Yacht

Eine Yacht beziehungsweise Jacht (aus gleichbedeutend , dies verkürzt aus mittelniederdeutsch "jachtschip" „Jagdschiff“, „schnelles Schiff“) ist (heute) ein Wasserfahrzeug für Sport- und/oder Freizeitzwecke, das – von Sonderfällen abgesehen – mit einem Deck und einer Kajüte ausgestattet ist. Entsprechend der Antriebsart werden Motoryachten und Segelyachten unterschieden. Wesentlich für eine Segelyacht ist – in Abgrenzung zur Jolle – ein fester, in der Regel mit Ballast versehener Kiel.

Im Alltagsgebrauch wird üblicherweise erst ab einer gewissen Länge des Fahrzeugs von einer Yacht gesprochen. Unter etwa 7 m spricht man eher von einem Boot, über 10 m von einer Yacht. Eine typische Yacht in europäischen Küstengewässern ist heute um die 10 bis 17 Meter lang (30 bis 56 Fuß) und mit mehreren Kabinen ausgestattet. Auf deutschen Binnenseen herrschen bei Yachten Bootslängen von 6 bis 15 Meter vor.

Größere Yachten werden auch als "Maxiyachten" bezeichnet, je nach Größe als "Mini-Maxis", "Maxis" oder "Supermaxis". Die Grenzen zwischen diesen Kategorien sind keineswegs fest, als Anhaltspunkt kann aber beispielsweise die Klasseneinteilung des Yacht Club Costa Smeralda dienen, der bei seinen Regatten Yachten von 18–24 m als "Mini-Maxis", Yachten über 30,5 m als "Supermaxis" und alle dazwischen als "Maxis" starten lässt.

Sehr große Yachten, bei denen kein Wert auf sportliche Nutzung gelegt wird, sondern die als reine Luxusobjekte dienen, werden als "Mega-Yachten" oder "Superyachten" bezeichnet.

Übliche Baumaterialien für Yachten sind heute faserverstärkte Kunststoffe (meist GFK oder CFK). Holz war früher der einzig verfügbare Baustoff, wurde aber von Kunststoffen praktisch vollständig abgelöst. Stahl und Aluminium werden sehr selten verwendet. Beton (Ferrozement) wurde in den 1970er Jahren erprobt, konnte sich aber nie durchsetzen. Der Bau einer modernen Yacht ist eine sehr komplexe und anspruchsvolle Arbeit, die viel Erfahrung erfordert.

Bei Maxi- und Megayachten spielt Komfort eine große Rolle, sie ähneln eher privaten Kreuzfahrtschiffen als Sportbooten.

Bauart, Einrichtung, Motorisierung und Ausrüstung einer Yacht richten sich sehr nach dem bevorzugten Revier und der Stärke der Nutzung. Yachten, die in der EU in Betrieb genommen werden, müssen der CE-Norm entsprechen und gemäß ihrer Konstruktion und Ausrüstung in eine der Kategorien A bis D eingeordnet werden.

In der Regel befinden sich Yachten, egal ob Motor- oder Segelyachten, ab einer Rumpflänge von 21 Metern (70 Fuß) in der Verantwortung von semi- oder vollprofessionellen Besatzungen (Yachtmatrose). Ab zwei Personen spricht man von einer Crew, dabei handelt es sich zumeist um Schiffsführer („Skipper“) und Steward. Der Anzahl der Crewmitglieder sind mit steigender Größe der Yacht kaum Grenzen gesetzt – sie kann bei Megayachten mehr als 60 Personen umfassen, darunter z. B. Köche und Ingenieure. Der Skipper ist dann oft Kapitän mit Patent für große Fahrt.

Insbesondere wegen der hohen Personalkosten ist die Charter von Yachten zu einer beliebten Alternative zur eigenen Yacht geworden. Damit können auch weitere Nachteile einer eigenen Yacht vermieden werden (Anschaffungskosten, Distanz zum Liegeplatz, Klima- und Revierabhängigkeit).

Die größten Privatyachten erreichen über 180 Meter Länge. Sie können 20–36 Knoten erreichen.

Die breite Masse fährt eher Yachten mit einer Länge zwischen 10 und 20 Metern. Segelyachten sind ab etwa 12 Metern sicher hochseetauglich, Motoryachten wegen der geringeren Stabilität erst deutlich darüber. Zudem begrenzt der Treibstoffvorrat einer „normalen“ Motoryacht von zwischen 500 und 1.500 Litern die Reichweite, denn 100 Liter pro Stunde können auch hier verbraucht werden. Die Reichweite einer Segelyacht ist nicht durch den Treibstoff begrenzt und ihre Höchstgeschwindigkeit wird durch die Rumpfgeschwindigkeit bestimmt.

Es sind in der Vergangenheit viele Schiffsformen als "Jachten" bezeichnet worden. Neben kleinen dreimastigen Jagten mit Spiegelheck um 1600 wurden in der ersten Hälfte des 17. Jahrhunderts auch speziell für Lustfahrten mit einer Frühform des Hochsegels versehene Schiffe für Binnen- und Küstengewässer so genannt. Im weiteren Verlauf des 17. Jahrhunderts wurde der Begriff "Jacht" eher für administrative und herrschaftliche Fahrzeuge verwendet. Diese sind als Statenjacht oder Herenjacht bekannt. Die Schenkung zweier solcher Jachten an den englischen Hof markiert den Beginn der Nutzung dieser Form in England als "Yacht". In Dänemark wurde im Laufe des späten 17. Jahrhunderts eine Frachtschiffform mit dieser Bezeichnung entwickelt. Es handelte sich um einen schnellen Segler mit scharfer Spantform bei runderem Bug, schmalerem Heck und einmastiger Gaffeltakelage mit Topprahen. Diese Form wurde noch im ganzen 19. Jahrhundert in Dänemark und Norddeutschland gebaut.







</doc>
<doc id="13649" url="https://de.wikipedia.org/wiki?curid=13649" title="Jolle">
Jolle

Die Jolle ist ein formstabiles Schwertboot, dessen Konstruktionsschwerpunkt meist über der Wasserlinie liegt. Im Gegensatz zum gewichtsstabilen Kielboot gewinnt eine Jolle ihr aufrichtendes Moment durch den Wasserdruck, der auf die flache Form des Bootsbodens wirkt. "Jolle" ist auch eine traditionelle Bezeichnung für diverse Arten kleinerer Ruder- oder Segelboote. Je nach Verwendungszweck können Jollen in "Rennjollen" oder "Wanderjollen" unterschieden werden und bilden in Bootsklassen einheitliche Klassentypen (siehe auch: Liste von Bootsklassen).

Eine krängende Jolle richtet sich nur bei sehr begrenzten Krängungswinkeln von allein wieder auf, sobald die Krafteinwirkung (zum Beispiel durch Winddruck) endet. Bei stärkerem Wind oder in Böen verlagert die Crew ihr Körpergewicht in Luv nach außen, indem sie ausreitet oder „in das Trapez geht“. Dadurch verlagert sich der Masse-Schwerpunkt der Jolle nach Luv, die Jolle stabilisiert sich wieder. Reagiert die Besatzung nicht oder nicht rechtzeitig auf Veränderungen des Winddrucks (zum Beispiel durch Ausreiten oder Fieren des Segels), kann die Jolle nach Luv (bei plötzlich nachlassendem Winddruck, einem Windloch) oder Lee (bei plötzlich steigendem Winddruck, das heißt in Böen) kentern.

Jollen haben viel eingebauten Auftrieb, der durch Schwimmkörper oder ausgeschäumte Hohlräume erreicht wird. Dadurch gehen sie auch bei einer Kenterung nicht unter. Die meisten Jollen besitzen außerdem einen Doppelboden, der nach der Wiederaufrichtung Wasser im Innenraum in kurzer Zeit ablaufen lässt.

Als "Jolle" wurde früher ein kleines, rundspantiges Boot ohne Balkenkiel bezeichnet. Der Name wurde von der norwegischen Bezeichnung "jöll" für einen ausgehöhlten Trog abgeleitet. Bis zum 19. Jahrhundert wurden Jollen als Spitzgattboote, später mit dem typischen Spiegelheck gebaut. Bei der Marine war die "Jolle" ein kleines rundgebautes Beiboot im Unterschied zur größeren, völligeren Barkasse und zum schnellen, schlanken Gig.

Jollensegeln ist in Deutschland ein beliebter Sport. An fast jedem Gewässer von ausreichender Größe finden sich Segelvereine, die das Jollensegeln betreiben. Neben vielen Freizeitseglern gibt es während der Saison von März bis Oktober an den Wochenenden Segelregatten für die verschiedenen Klassen.

Eine Jollenklasse bezeichnet Jollen mit einer einheitlichen Bauvorschrift, welche die Boote bei Regatten direkt vergleichbar macht. Entsprechend werden für Jollenklassen von Segelvereinen oder Yachtclubs Klassenregatten veranstaltet.

Die Bauvorschrift bezieht sich üblicherweise mindestens auf Rumpfform, Länge, Breite, Gewicht und Segelfläche. Meist sind auch andere Maße des Bootes beschränkt (zum Beispiel Länge des Mastes, Schwertlänge usw.). In Deutschland gibt es seit etwa 1900 überregionale Jollenklassen.





</doc>
<doc id="13650" url="https://de.wikipedia.org/wiki?curid=13650" title="Wasserfahrzeug">
Wasserfahrzeug

Wasserfahrzeuge sind Fahrzeuge, die zur Fortbewegung auf dem oder im Wasser bestimmt sind. Wesentlichste Unterscheidung, mit auch rechtlichen Konsequenzen, ist die Art des Antriebs, die grob in der Reihenfolge ihres Aufkommens einzuteilen ist:

Von Ausnahmen wie Luftkissenbooten und Tragflächenbooten abgesehen, wird der notwendige Auftrieb bei mechanisch betriebenen Fahrzeugen durch das archimedische Prinzip erzeugt.

Der Begriff Wasserfahrzeug dient als zusammenfassende Kategorie für Boots- und Schiffstypen. Die meisten Wasserfahrzeuge werden als Boot, Floß oder Schiff bezeichnet. Obwohl die meisten Schiffe größer als viele Boote sind, ist die Unterscheidung zwischen diesen beiden Kategorien nicht immer eine Sache der Größe.


Nach den Statuten der International Maritime Organization ist ein Wasserfahrzeug ein solches, wenn es sich nicht höher als zwei Meter über dem Wasser bewegen kann. Nach dieser Definition wären beispielsweise Bodeneffektfahrzeuge, nicht aber Flugboote oder Verkehrsflugschiffe als Wasserfahrzeuge anzusehen.

Wasserfahrzeuge waren zum Überqueren von Meerengen, Flüssen und Seen, aber auch bei der Ausbreitung des Menschen entlang Küsten, über Fließgewässer und zum Erreichen von Inseln erforderlich, aber auch für den Fischfang in Binnengewässern und im Meer.

"Homo erectus" kolonisierte vor etwa 850.000 Jahren die indonesische Insel Flores, welche in der letzten Million Jahre nie auf dem Landweg erreichbar war, und entwickelte sich auf der Insel zum "Homo floresiensis". Die dazu erforderliche Überquerung der Lombokstraße gilt als der erste erfolgreiche Einsatz von Wasserfahrzeugen über eine Meeresdistanz von mehreren Kilometern (durch die Gattung "Homo").

"Homo sapiens" hat vor mindestens 45.000 Jahren ozeantaugliche Wasserfahrzeuge konstruiert und erfolgreich zu Passagen von etwa 100 Kilometer Entfernung ohne Landsicht genutzt bei der Besiedlung Sahuls (Neuguinea und Australien) und bis zu entfernt liegenden Inseln Melanesiens. Als erste Typen werden Floße aus Bambus, als Antriebsmittel Ruder, Staken (im Flachwasser), Windansatzstellen (Palmblattmatten) und längs schwimmende Begleiter vermutet. Diese Wasserfahrzeuge entstanden aus bereits vorher entwickelten Meeresfahrzeugen für den küstennahen und küstenferneren Fischfang.

Mit Floßen wurden ab dem 9. Jahrtausend v. Chr. auch die landfernen Inseln des Mittelmeeres (Kreta, Malta, Zypern) besiedelt.

Mit Erfindung der Axt war der Bau von Einbäumen aus Baumstämmen möglich. Ab etwa 6000 v. Chr. sind solche Wasserfahrzeuge in Mitteleuropa nachzuweisen.

Wasserfahrzeuge aus über Rahmen aus Holz und Knochen gespannte zusammengenähte Felle traten erstmals in der Steinzeit in Nordamerika auf. Die Besiedlung arktischer Gebiete, wo besonders Gewässer Beute für die Jäger boten, verlangte hochwertige Boote, wie etwa die Baidarka auf den Aleuten, Kajak und Umiak in Grönland. Solche Boote verfügen über einen Rahmen aus Holz und/oder Knochen und sind mit Fellen bespannt.

Um den westlichen Pazifik herum wurden vor über tausend Jahren seetüchtige Auslegerboote und das notwendige navigatorische Können entwickelt.

Einbäume wurden im Laufe der Entwicklung durch Anbringen von Brettern beplankt, so dass Formen wie die Piroge entstanden. Die dabei entwickelten Techniken der Befestigung und Abdichtung ermöglichten die beplankten Schiffe, für den Verkehr im Meer bis zum Aufkommen der Eisenschiffe vorherrschend.



</doc>
<doc id="13651" url="https://de.wikipedia.org/wiki?curid=13651" title="Crew">
Crew

Crew // (deutsch „Mannschaft“) steht für:

Crew ist der Familienname folgender Personen:
CREW steht als Abkürzung für:
Siehe auch: 



</doc>
<doc id="13652" url="https://de.wikipedia.org/wiki?curid=13652" title="Mannschaft">
Mannschaft

Mannschaft (von mittelhochdt. "manschaft" ‚Lehns-/Gefolgsleute‘) bezeichnet:


Mannschaften als Pluralwort

Siehe auch:



</doc>
<doc id="13653" url="https://de.wikipedia.org/wiki?curid=13653" title="Surfbrett">
Surfbrett

Ein Surfbrett ist ein aus einem schwimmfähigen Material hergestelltes Brett, das als Sportgerät zum Wellenreiten oder in Verbindung mit einem Segel zum Windsurfen dient.

Das Kernmaterial bei Surfbrettern ist Hartschaum. Die Beschichtung ist meist aus Polyester- oder Epoxydharz-getränktem Glasfasergewebe. Bei modernen Wellenreitbrettern wird ein Kern verwendet, der in der Mitte eine heliumgefüllte Blase enthält, was das Gewicht der Surfbretter zusätzlich reduziert und den Auftrieb erhöht. Vereinzelt werden Surfbretter auch aus Balsaholz hergestellt. Diese sind, wenn zusätzlich mit einem Styroporkern versehen, leichter als herkömmlich hergestellte Bretter. Daneben gibt es noch Surfbretter aus Holz, die – in der Bauweise vergleichbar einem Flugzeugflügel – innen hohl sind.

Um die Längsstabilität zu gewährleisten, wird mittig in Längsrichtung eine dünne Sperrholzplatte eingebaut, der "Stringer". Er gleicht die Lastverteilung aus, damit das Brett nicht auseinanderbricht.

Je nach Anwendungszweck gibt es unterschiedliche Bauformen. Diese bestimmen Drehverhalten, Auftrieb und Handhabbarkeit, und damit letztendlich die Verwendung des Brettes.

Die Größe eines Surfbrettes wird üblicherweise in Feet (Fuß) und Inch (Zoll) angegeben, beim Windsurfen sind auch Zentimeter üblich. Charakteristisch bei Windsurfbrettern ist das in Litern angegebene Volumen, das den statischen Auftrieb bestimmt.

Die Finne ist in den Finnenkasten im Heck des Brettes eingeschraubt und dient der Richtungsstabilität. Es gibt unterschiedliche Normen für den Finnenkasten beim Windsurfen: Powerbox, US-Box, Tuttle Box, deep Tuttle Box, Powertrim Box, Tiga Conic Box. Wellenreit- und Windsurf Wave Bretter haben oft zwei oder drei Finnen. 

Windsurfbretter der Longboardklasse und Anfängerbretter besitzen ein bewegliches Schwert nahe der Mitte des Brettes. In ein Windsurfbrett ist eine Schiene zur Befestigung des Mastfußes integriert. Diese dient zur Aufnahme des Power-Joint und damit des Mastfußes und ist je nach Segelgröße in Längsrichtung verstellbar.

Die Fußschlaufen, mindestens zwei vorne und eine oder zwei hinten, sind je Fußgröße einstellbar und können meist auch am Brett versetzt werden.

Windsurfbretter bezeichnet man nach ihren unterschiedlichen Einsatzbereichen:


Surfbretter zum Wellenreiten besitzen - im Gegensatz zu Windsurfbrettern - keine Fußschlaufen. Um die Standfestigkeit auf der nassen Oberfläche zu erhöhen, bietet der Markt verschiedene Surfwachse, welche auf die Standfläche des Surfbretts aufgetragen werden. Je nach Wassertemperatur gibt es verschiedene Sorten mit den Bezeichnungen "Cold" (bis 14 °C), "Cool" (13 bis 20 °C), "Warm" (19 bis 26 °C) und "Tropic" (ab 24 °C Wassertemperatur).

Beim Wellenreiten werden grundsätzlich zwei verschiedene Brettarten unterschieden: das "Longboard" und das "Shortboard" (auch "Funboard" genannt). Von einem Longboard ist die Rede, wenn die Länge 8 Fuß (ca. 2,44 Meter) überschreitet. Kürzere Surfbretter werden als Shortboard bezeichnet.

Beim Longboard werden zusätzlich diejenigen unter 9 Fuß (ca. 2,74 Meter) oft auch als "Malibu" bezeichnet. Longboards sind meist relativ dick und haben eine gerundete Spitze, auch "Nase" genannt. Einsteiger und leicht Fortgeschrittene sollten zum Malibu (Funboard) greifen, mit dem das „Erwischen“ der Welle und das Halten des Gleichgewichts am leichtesten ist. 

Historisch gesehen wurden bis in die 1960er Jahre ausschließlich Longboards gesurft, deshalb gilt Longboardsurfen als Old School und wird meist mit Stilbewusstsein betrieben. Der beliebteste Trick des Longboards ist der "Hang Ten", auch "Nose Ride" genannt, wobei alle zehn Zehen über die Spitze des Brettes ragen. Bekannte Longboarder sind Joel Tudor aus den USA und Bonga Perkins aus Hawaii.

Aufgrund seiner relativ schwierigen Handhabung ist das Shortboard eher für den fortgeschrittenen Surfer geeignet. Auch hier bestimmen die verschiedenen Bauformen die unterschiedlichen Eigenschaften der Bretter.


Eine Sonderform bilden Bretter der Kategorie Gun (Gewehr), die eine Länge von 7 Fuß bis ca. 12 Fuß aufweisen. Sie sind sehr schlank und vorne spitz zulaufend gestaltet und werden für sehr große Wellen verwendet. Die geringe Breite ermöglicht hohe Geschwindigkeiten, und aufgrund ihrer Länge liegen sie auch bei hohen Geschwindigkeiten stabiler im Wasser. 

Als Bodyboard (oder auch "Boogieboard") wird eine besondere Art Surfbrett zum Wellenreiten bezeichnet. Im Gegensatz zum Surfbrett, das stehend benutzt wird, surft man auf einem Bodyboard im Liegen. Bei der "Drop-Knee"-Variante kniet man mit einem Bein auf dem Board, während das andere aufgestellt ist. Selten wird auch ganz auf dem Bodyboard gestanden. Die Vorteile von Bodyboards sind die schnelle Erlernbarkeit, der einfache Transport und die relativ geringe Verletzungsgefahr durch das Board selbst.

Es gibt einfache Anfängerboards für Kinder bis hin zu "Professional Boards", die bis zu 400 Euro kosten. Ein Bodyboard besteht aus Schaumstoff bzw. aus Hartschaum wie Polyethylen oder Polypropylen. Je nach Ausführung wird zur Verstärkung, ähnlich wie beim Surfbrett, ein Stringer aus kohlenstofffaserverstärktem Kunststoff eingelassen. Für Anfänger eignet sich ein Brett, das in der Länge vom Boden bis kurz unter den Bauchnabel reicht und, in der Breite in der Hand gehalten, komfortabel unter die Armbeuge passt. Das Brett wird mittels Leash − einer Fangleine − mit dem Körper verbunden. Die Leash wird meist am Unter- oder Oberarm mit einem Klettverschluss befestigt. Zum Bodyboarden benötigt man spezielle, kurze Schwimmflossen, die in Surfshops erhältlich sind. Diese Flossen werden zum Erreichen der Welle mit kurzen, kräftigen Stößen bewegt. Als Insidertrick für die anfangs möglicherweise unbequem erscheinenden, aber zum Bodysurfen unabdingbaren Schwimmflossen gelten Tennissocken, die im Fußteil getragen werden.

Das Bodyboard kann in fast allen Wellen von 0,5 - 8 Meter benutzt werden. Es eignet sich vor allem für steile Beach- und Reefbreaks mit schnell brechenden Wellen, wie sie z. B. in Puerto Escondido in Mexiko vorkommen. In Deutschland eignet sich die Nordsee auf Sylt bei Westerland und Wenningstedt am besten zum Bodyboarden. In Westerland ist auch Bodyboardausrüstung erhältlich. Andere beliebte Bodyboardreviere in Europa sind Peniche und Ericeira in Portugal und die Nordküsten von Lanzarote und Fuerteventura, sowie die Atlantikküste Frankreichs. In Portugal und weiten Teilen Lateinamerikas ist das Bodyboard vielfach beliebter als das Surfbrett.

Erfunden wurde diese Brettform von Tom Morey in den 1970ern, der mit seiner Firma "morey-boogie" 25 Jahre ein Patent darauf besaß. Inzwischen sind zahlreiche andere Hersteller am Markt.

Ein in der Szene bekannter Protagonist des Bodyboardens ist der Hawaiier Mike Stewart, der als Junge in der Nachbarschaft von Tom Morey wohnte. Er gewann insgesamt 11 Weltmeistertitel im Bodyboarden. Er galt Anfang der 1990er Jahre als einer der besten Wellenreiter, ungeachtet der Ausrüstung, und gewinnt heute noch im Alter von 42 Jahren Wettkämpfe. Eine jüngere Generation um den Weltmeister von 2006, Jeff Hubbard, konzentriert sich auf Aerials (Luftsprünge), wobei Drehungen bis zu 720 Grad in der Luft erreicht werden.

Beim Kitesurfen unterscheidet man zwischen "Directional-Boards" und "Bi-Directional-Boards".




</doc>
<doc id="13654" url="https://de.wikipedia.org/wiki?curid=13654" title="Windsurfen">
Windsurfen

Windsurfen (früher auch "Brettsegeln" oder "Stehsegeln" genannt) ist eine Wassersportart, bei der man, auf einem Surfbrett stehend, ein Segel zur Fortbewegung nutzt. Das Segel ist dreh- und kippbar mit dem Brett verbunden, was spektakuläre Manöver und Tricks ermöglicht. Die in den USA entwickelte Sportart wurde zur Trendsportart und hat sich weltweit etabliert.

Das Windsurfen ist aus dem Wellenreiten und dem Segeln entstanden: Die Nutzung der Kraft des Windes ermöglichte es, sich das mühsame Paddeln gegen die Wellen zu ersparen. Im November 1964 zeichnete Newman Darby sein Darby Sailboard und veröffentlichte in Popular Science, das in einer Auflage von 1,5 Millionen Exemplaren in den USA erschien, eine bebilderte Selbstbauanleitung für sein Segelbrett. Er verwendete dabei ein Segel ähnlich einem Kinderdrachen, bei dem ein Mast beweglich mit dem Surfboard verbunden war und eine horizontale Spiere zum Halten des Segels diente. Newman Darby baute mehrere dieser Sailboards und es existieren Filme über seine Probefahrten.

Der Amerikaner Jim Drake, Ingenieur des US-Verteidigungsministeriums, versah ein Surfboard mit einem Segel, um das lästige Paddeln durch die Wellen zu vermeiden und entwickelte dazu das Bauprinzip des Windsurfers mit einem „Paar gekrümmter Bäume, welche querab zur Spiere verlaufen und zwischen sich das Segel halten“, wie der Gabelbaum umschrieben wird. Finanziert wurde das Projekt von seinem damaligen Freund Hoyle Schweitzer, mit dem er gemeinsam 1968 ein Patent bei dem US-amerikanischen Patentamt anmeldete. Drake startete in der Jamaica Bay in New York am 21. Mai 1967 zum ersten Mal mit seinen Windsurfer „Old Yeller“. Vor ihm hatte der Engländer Peter Chilvers mit der Idee gespielt, ein Surfbrett mit einem Segel zu verbinden. Jedoch hatte Drake schließlich die entscheidende Idee, das Segel mit einem Gabelbaum zu spannen und moderne Materialien für Brett, Mast und Segel einzusetzen. Am 6. Januar 1970 wurde dem Patentantrag „für ein windbetriebenes Fahrzeug“ (US-Patent Nr. 3487800) vom USPTO stattgegeben.

Hoyle Schweitzer erkannte die wirtschaftlichen Möglichkeiten des Windsurfens und trieb die Entwicklung weiter voran. Zusammen mit seiner Frau Diana gründete er das Unternehmen "Windsurfing International Inc." und übernahm 1973 auch Drakes Anteile an den Patentrechten. Ein Jahr, nachdem Windsurfen erstmals olympische Disziplin – jedoch zu aller Erstaunen auf dem deutschen Windglider – geworden war, lief das Patent aus.

In Europa wurde das Patent in einem Verletzungsverfahren vor dem Münchner Patentgericht eingeschränkt. Ursprünglich hatten Schweitzer und Drake das Kardangelenk des Mastfußes und das Rigg (die Einheit aus Segel, Mast, Mastfuß und Gabelbaum) geschützt. Das Gericht würdigte aber die frühere Erfindung von Newman Darby, der bereits sein Segel beweglich mit dem Board verbunden hatte und beließ Schweitzer nur den Gabelbaum als Patentinhalt, da der Gabelbaum im Gegensatz zu dem kinderartigen Drachensegel von Darby mit nur einer Haltespire eine deutliche Verbesserung darstellte. Das Patent zum Gabelbaum reichte ihm aber aus, um praktisch in allen patentgeschützten Märkten Lizenzen eintreiben zu können.

Zusammenfassend waren vier Personen maßgeblich an der Entwicklung des Sports beteiligt: Newman Darby als eigentlicher Erfinder der Sportart, Jim Drake als Erfinder des Gabelbaumes und Hoyle Schweitzer, der das Windsurfen zu einem spektakulären Trendsport entwickelte und damit wirtschaftlich erfolgreich war. Neben den Amerikanern hatte auch der deutsche Fred Ostermann ein Board, den Windglider entwickelt, der in Europa und später in der ganzen Welt die Märkte beherrschte. Der Windglider wurde so das einzig zugelassene Surfbrett anlässlich der ersten olympischen Windsurfwettbewerbe bei der Olympiade 1984 in Los Angeles. 
Fred Ostermann ist auch der Erfinder des Tandemsurfens.

In den folgenden Jahren setzte parallel zu neuen Materialien und Innovationen eine starke Verbreitung der neuen Sportart ein. Wesentliche Meilensteine setzte „Windsurfing Hawaii“ in den Jahren 1976–1977 mit der Entwicklung des Trapezes zur Entlastung der Hände, Fußschlaufen für höhere Standfestigkeit auf dem Brett, leichten und agilen Brettern, die Sprünge ermöglichten, sowie mit kürzeren Gabelbäumen. Parallel zu diesen frühen Funboardaktivitäten in Hawaii hat sich Ende der 1970er, Anfang der 1980er Jahre in Europa eine wachsende Gemeinde von Longboardenthusiasten gebildet. So konnten zum Beispiel das Windglider Board von Fred Ostermann, auf dem später (1984) um olympisches Gold gesegelt wurde oder auch der "Mistral Competition", der mit 270.000 Exemplaren eines der meistverkauften Surfboards wurde, dazu beitragen, dass in Europa Mitte der achtziger Jahre etwa 2,8 Millionen Surfer ihrem Sport frönten. Die Mutter aller Trendsportarten war damit geboren.

Windsurfer umgab das Image von Freiheit und Naturverbundenheit. Rund um das Windsurfen und seine Idole wurde in den 1980ern bis Mitte der 1990er Jahre ein regelrechter Kult betrieben. Der "Surfertyp" wurde mit hübschen Mädchen, eigener Mode und Lebenseinstellung in Verbindung gebracht. Die Trendsportart wurde bestens vermarktet und fand Anhänger in aller Welt. Eigene Magazine wie „Surf“ und „Stehsegelrevue“ verbreiten im deutschen Sprachraum die neuesten Trends und Informationen.

Diese Blütezeit des Surfens war als Massensport eine kurze, denn die einseitige Berichterstattung in den Fachmagazinen über die Kurzbrettszene (engl. "Funboard"), die 1980–1983 aus Hawaii nach Europa herüberschwappte und 1982 Jürgen Hönscheid zum ersten deutschen Windsurf-Profi machte, überforderte viele Longboardsurfer in Europa. Kaum einer wollte als „Stehsegler“ gelten, wenn andere über meterhohe Wellen sprangen. 1986 wurden in Deutschland noch 180.000 Surfboards verkauft. Seitdem ist die Zahl bis zum heutigen Tag konstant rückläufig. 2005 gingen in Deutschland lediglich 9.000 Surfboards über den Ladentisch. Damals wie heute erlernen etwa 40.000–50.000 Menschen das Windsurfen (belegt durch den VDWS, Verband der deutschen Windsurfing Schulen), allerdings mit dem Unterschied, dass in den achtziger Jahren praktisch alle Neueinsteiger „on Board“ blieben, während heute 95 Prozent der Anfänger nach dem Erwerb des „Windsurfing Grundscheines“ diesen Sport wieder aufgeben. Das passiert in keinem anderen Sport.

Dies wird von Kritikern als Signal an die Industrie verstanden, wieder Surfbretter zu konzipieren, die von der breiten Masse gewünscht werden bzw. dieser verhilft beim Sport zu bleiben. Handel und Hersteller halten dem entgegen, dass Anfängermodelle verfügbar sind, aber kaum Absatz finden, da jeder Sportler so rasch wie möglich auf die kurzen, leichten und agilen Bretter umsteigt. Diese sind jedoch deutlich schwerer zu handhaben als die früheren Longboards, es dauert wesentlich länger und die athletischen Voraussetzungen sind deutlich höher, um damit richtig surfen zu können. Außerdem ist auch mehr Wind erforderlich, um die Leistungsfähigkeit des Kurzbretts auszuschöpfen. Das alles hat dazu geführt, dass weniger athletische beziehungsweise weniger geduldige Surfer den Sport wieder aufgaben.

Gleichzeitig verdankt der Sport dem Funboard eine atemberaubende Akrobatik und völlig neue Fahrtechniken wie die Faszination des „Gleitens“ und die Beherrschung der Welle.

Üblicherweise gleiten Surfbretter mit 30 bis 45 Kilometer pro Stunde über das Wasser. Die Rekordgeschwindigkeiten über 90 km/h werden nur bei Sturmstärken auf sehr glattem Wasser mit speziellen, ca. 25 cm breiten Surfbrettern (sogenannten "Speedneedles") erreicht. Die Kurzstrecken-Weltrekorde werden in der Regel auf einer künstlichen Wasserfläche am Strand von Saintes-Maries-de-la-Mer in der französischen Camargue aufgestellt; dort wurde eigens ein 1.100 m langer und 15 m breiter Kanal ("le canal", auch "French Trench") angelegt, um auch bei hohen Windgeschwindigkeiten die – Geschwindigkeit reduzierende – Wellenbildung auf dem Wasser fast vollständig zu verhindern.

Der absolute Geschwindigkeitsrekord wurde seit dem 6. März 2008 mit 49,09 Knoten (90,9 km/h) vom Franzosen Antoine Albeau gehalten.
Im Oktober 2004 holte der Ire Finian Maynard den Weltrekord für segelgetriebene Wasserfahrzeuge über 500 m vom Trimaran "Yellow Pages Endeavour" zu den Surfern. Am 10. April 2005 verbesserte er seinen Rekord noch auf 48,7 Knoten (90,2 km/h), was vom World Sailing Speed Record Council am 11. April 2005 ratifiziert wurde. Der schnellste Segler ist auf diese Strecke der Trimaran "L'Hydroptère" mit 44,81 Knoten; anders als die Surfer segelt die "Hydroptère" ihre Rekorde allerdings nicht auf einem künstlichen, optimierten Gewässer, sondern in Küstengewässern mit natürlichen Wellen.

Den Weltrekord für segelgetriebene Wasserfahrzeuge über eine Seemeile (1852 m) hielt ab dem 15. Oktober mit 39,97 Knoten ebenfalls Maynard. Am 31. Oktober 2006 übernahm der vorherige Weltrekordler Bjørn Dunkerbeck mit 41,14 Knoten wiederum den Rekord. Am 24. April 2007 brachte der Trimaran "L'Hydroptère" mit 41,69 Knoten pro Seemeile den Rekord jedoch wieder zu den Seglern.

Vom 28. Oktober 2010 bis zum 16. November 2012 galt der Kite-Surfer Rob Douglas (USA) als neuer Weltrekordhalter. Während der Lüderitz Speed Challenge (Namibia) fuhr er 55,65 Knoten, das sind 103,06 km/h, allerdings holte Paul Larson den Geschwindigkeitsrekord wieder zu den Seglern. Er erreichte mit der Vestas Sailrocket 2 eine Spitzengeschwindigkeit von 63 Knoten (117 km/h) und fuhr dabei mit einer Durchschnittsgeschwindigkeit von 59,23 Knoten (110 km/h) über eine Distanz von 500 m. 10 Tage später brach Paul Larson seinen eigenen Rekord wieder und setzte einen neuen Rekord bei 65,45 Knoten auf 500 m.

Im November 2012 stellte der französische Windsurfer Antoine Albeau im Lüderitz-Kanal in Namibia mit 52,05 Knoten (96,39 km/h) einen neuen Geschwindigkeits-Weltrekord im Windsurfen über 500 Meter auf. Die britische Windsurferin Zara Davis erreichte dort 45,83 Knoten (84,87 km/h) und war damit die weibliche Geschwindigkeits-Weltrekordhalterin. 2015 verbesserte Albeau seinen Rekord auf 53,27 Knoten (98,66 km/h), bei den Damen erreichte Karin Jaggi 46,31 Knoten (85,77 km/h).

Aus dem Windsurfen und dem Wakeboarding ist das Kitesurfen entstanden. Es ist seit etwa 2001 in Mitteleuropa verbreitet und wurde um 1995 in den USA erfunden. Dabei wird das Segel durch einen großen Lenkdrachen ersetzt. Die Surfbretter zum Kiten, die sogenannten Kiteboards, sind viel kleiner und haben kaum Auftrieb. Sie gleichen am ehesten den Wakeboards beim Wasserskilaufen.

Im neuen Jahrtausend ist der Medienrummel um die Sportart zurückgegangen. Einerseits haben die Kitesurfer den Surfern beim Publikum den Rang abgelaufen, andererseits war Windsurfen kein attraktiver Sport für Zuschauer. Die Abhängigkeit von Wind und Wetter machten Live-Übertragungen im Fernsehen kaum planbar und die Zuschauer vor Ort bekamen am Strand oftmals nicht viel von dem mit, was draußen auf dem Wasser passierte. Zudem gehört Windsurfen zu den nur schwer zu erlernenden Sportarten, was die Anzahl der Ausübenden in Grenzen hält und mit verhindert, dass Windsurfen zum Massensport wird. Windsurfen ist kein Hobby, das nebenbei erlernt wird wie zum Beispiel Radfahren oder Laufen. Insbesondere die perfekte Beherrschung moderner Surfbretter ist im Gegensatz zum Windsurfen mit den Schulungsbrettern nur schwer zu erlernen.

Ein Breitensport und Funsport ist Windsurfen aber längst geworden. Quer durch alle Berufs- und Altersgruppen – vom 10-jährigen Schüler bis zum 75-jährigen Rentner – haben sich an den Surfspots Gemeinschaften entwickelt. Jedoch fällt auf, dass der Sport zu gut 90 Prozent von Männern betrieben wird.

In Deutschland hat Peter Raatz im Jahre 1972 den heute ältesten Windsurfingverein der Welt gegründet (WSeV Berlin). Dieser Verein etablierte Windsurfen als eine Segeldisziplin im Regattasport. 1984 und 1988 stellte der Verein den deutschen Teilnehmer bei den Olympischen Sommerspielen.

2011 hat die Professional Windsurfers Association Live-Wertungen und kommentiertes Live-Streaming für die Events der World-Tour eingeführt. Sowohl am Strand als auch im Internet können die Zuschauer damit das Geschehen auf dem Wasser besser verfolgen.

Das Sportgerät besteht aus einem stromlinienförmigen Schwimmkörper, dem Surfbrett, dessen Volumen sich nach dem Können und Gewicht des Sportlers richtet. Da kleinere Bretter weniger kippstabil, voluminösere dagegen weniger wendig sind, liegt das Volumen der Surfbretter meist zwischen 65 und etwa 150 Litern und wird je nach Einsatzzweck ausgewählt. Die Brettlänge liegt dabei zwischen 2,2 und 2,80 Metern, bei einer Breite von 48 bis 101 cm. Für ungeübte Sportler sollte das Surfbrett möglichst viel Volumen und damit Auftrieb haben, um besonders kippstabil zu sein.

Die Variationen der Brettformen sind seit den Anfängen um 1975 stark angestiegen – für fast jedes denkbare Einsatzgebiet wurden Bretter entwickelt. Bis Mitte der 1980er waren die Verdränger-Typen mit etwa 20 kg Gewicht und teilweise einem Kiel, ansonsten mit einem Schwert, verbreitet. Diese wurden schnell von kleinen, gleitfähigen Typen abgelöst, die nur noch etwa 7 kg Gewicht haben. Bei der Produktion haben auch hier leichte und sehr steife Materialien wie kohlenstofffaserverstärkter Kunststoff („Carbon“, „Kohlefaser“) oder mit Aramidfasern „Kevlar“ verstärkter Kunststoff Einzug gehalten.

Fast alle Serienbretter werden heute in einem Werk der Firma Cobra in Thailand hergestellt, die übrigen häufig in Vietnam oder Tunesien.

Der Mastfuß am Surfbrett ist über eine freibewegliche Verbindung – den "Powerjoint" – mit dem "Rigg" verbunden. Das Rigg besteht aus einem biegefähigen Mast (gegebenenfalls mit einer Mastverlängerung), einem Gabelbaum zum Festhalten und dem Segel. Während zunächst Mast und Gabelbaum noch recht umständlich nur durch Tampen verbunden waren, wurde zurückgehend auf eine Erfindung von Karl Robert Kranemann aus 1984 durch die Fa. Schütz Werke GmbH & Co. KG, Selters mit den Marken ART und Fanatic eine Schnellspannverbindung am Markt eingeführt. Kranemann erteilte weiterhin auch der Fa. Mistral eine Lizenz. Die Schnellspannverbindung hat sich im weiteren Verlauf durchgesetzt und ist heute Standard.

Der Mast besteht aus glasfaserverstärkten Kunststoffen und kann zur Gewichtsersparnis mit Carbon verstärkt sein. Die Kennzahl IMCS („Indexed Mast Check System“) des Mastes bezeichnet die Masthärte und -steifigkeit und muss auf die Vorgabe des Segelherstellers abgestimmt sein. Je niedriger der Wert ist, desto weicher ist der Mast. Je nach Segel werden meist Masten von 360 cm bis 580 cm Länge verwendet.

Neben dem SDM-Mast ("Standard Diameter Mast") gibt es seit etwa 2000 deutlich dünnere Masten, die als RDM-Mast ("Reduced Diameter Mast") bezeichnet werden. Die RDM-Masten zeichnen sich durch einen verkleinerten Radius bei erhöhter Wandstärke aus. Ein SDM-Mast hat am Mastfuß einen Innendurchmesser von 48.5 mm, beim RDM-Mast beträgt er 33 mm. Bei gleicher Steifigkeit erfordert der kleinere Durchmesser eine größere Wandstärke des Masts, wodurch er schwerer ist.

Beispiel:

Der Hauptvorteil eines RDM-Masts besteht in der besseren Anströmung des Segels. Viele Windsurfer schätzen auch das bessere „Flexverhalten“ bei Freestylemanövern.

Der Gabelbaum besteht aus Aluminium- oder Carbonholmen und dient zum Aufspannen des Segels und als Haltegriff für den Sportler. Die Länge des Gabelbaumes ist verstell- und arretierbar, um das Segel optimal abstimmen zu können.

Ein modernes Windsurfsegel besteht aus Monofilm (durchsichtige Polyester-Folie, Mylar) und Dacron (gewebtes Polyester), teils Mylar-beschichtet. Je nach Preisklasse des Segels werden besonders beanspruchte Teile mit Kevlar-Gewebe verstärkt. Für einen besseren Vortrieb und einen stabilen Druckpunkt verfügen sie über mehrere, teilweise durchgehende, Segellatten. Sehr leistungsfähige Segel haben "Camber", mit denen sich die Segellatten am Mast abstützen. Dadurch wird die Druckpunkt- und Form-Stabilität nochmals erhöht; das Handling in Manövern verschlechtert sich jedoch. Die Segelfläche kann zwischen 1,5 m² (für Kinder) und 12,5 m² liegen und richtet sich nach dem Körpergewicht, dem Können und maßgeblich der Windstärke.

Die Schwierigkeit beim Windsurfen besteht hauptsächlich in der Fähigkeit, das Gleichgewicht des eigenen Körpers mit der Segelstellung zum Wind zu kontrollieren. In Surfschulen kann man heute das Windsurfen in entsprechenden Kursen bereits in 10–12 Stunden erlernen. Dies wird durch besonderes Anfängermaterial ermöglicht. So bieten Bretter für Anfänger heute ein hohes Maß an Kippstabilität, was es dem Schüler einfacher macht, sich auf die Segelsteuerung zu konzentrieren.

Für einen Anfänger ist ein Brett mit Schwert die bessere Wahl, da es für bessere Kippstabilität sorgt und einem Abtreiben zur windabgewandten Seite entgegenwirkt. Bei kleineren Brettern findet man nur noch die Finne vor, da das Schwert beim Gleiten stört und die Geschwindigkeit verringert. Erfahrenere Windsurfer wählen meist ein möglichst kleines Surfbrett mit weniger Auftrieb, da dieses eine höhere Drehfreudigkeit aufweist. Dabei kann der Auftrieb geringer als das Gewicht des Sportlers sein, so dass das Brett erst beim Fahren durch den dynamischen Auftrieb an die Wasseroberfläche gehoben wird und auf dieser gleitet. Solche kleinen Bretter werden auch als Sinker bezeichnet. Verständlicherweise ist ein Aufholen des Segels durch die Startschot (Aufholleine) bei Sinkern nicht mehr möglich, deshalb muss man auf diesen kleinen Brettern den Wasserstart beherrschen (siehe Kapitel Starttechnik).

Die grundlegenden Techniken des Windsurfens werden in Surfschulen bereits in wenigen Tagen vermittelt, sodass ein Anfängerbrett sicher bei leichtem Wind gesteuert werden kann. Die richtige Verwendung von Fußschlaufen und Trapez kann in Fortgeschrittenenkursen erlernt werden und gestaltet sich schon zeitaufwendiger. Die höchste Könnensstufe wird durch die Beherrschung des Kurzbrettes erreicht.

Um die Fahrtrichtung zu beeinflussen, wird das Segel nach vorne oder hinten geneigt. Beim nach vorne Neigen (Abfallen) dreht sich der Bug vom Wind weg und umgekehrt beim nach hinten Neigen (Anluven) des Segels in den Wind hinein.

Bei mittlerer Stellung des Segels (weder nach vorne oder nach hinten geneigt), befindet sich der Druckpunkt des Segels über dem Druckpunkt des Schwertes, wie es Einsteiger-Boards aufweisen. Wird nun das Segel nach vorne geneigt, wandert der Druckpunkt (Windkraft) des Segels vor den Druckpunkt des Schwertes (Wasserwiderstand) und die Windkraft bewirkt eine Drehung des Surfbrettes mit der Spitze vom Wind weg. Dagegen wird bei einer Neigung des Segels nach hinten der Druckpunkt des Segels hinter den Druckpunkt des Schwertes verschoben und das Surfbrett dreht mit der Nase gegen den Wind. Die Neigung des Mastbaumes zusammen mit dem Segel ist möglich, da er mit einem Gelenk (dem sogenannten Powerjoint) auf dem Brett befestigt ist. Diese Möglichkeit – den Mast zu neigen – unterscheidet das Surfbrett vom normalen Segelboot. Dagegen hat das normale Segelboot ein Ruder zur Steuerung.

Bei höheren Geschwindigkeiten, wenn das Board Gleitfahrt hat, wird die Fahrtrichtung fast ausschließlich über das Ankippen des Brettes mit den Füßen gesteuert. Durch tieferes Eintauchen der belasteten Brettseite erhöht sich auf dieser der Strömungswiderstand, während er sich auf der gegenüberliegenden Seite verringert. Durch diesen Bremseffekt dreht sich das Surfbrett in die gewünscht Richtung. Vor allem in der Welle entstehen damit spektakuläre Manöver wie der "cut back".

Surfer verringern die Beanspruchung der Arme beim Halten des Segels am Gabelbaum durch ein Trapez. Dabei handelt es sich entweder um einen kompakten Hüftgürtel oder eine Art Sitzhalterung, an denen vorne ein Metallhaken befestigt ist. Dieser unten offene Trapezhaken wird in ein kurzes Seil – den Trapeztampen – eingehängt, welcher am Gabelbaum befestigt ist und den größten Teil der Zugkraft des Windes aufnimmt. Zum Aushaken des Trapeztampens wird der Gabelbaum kurz zum Körper gezogen, sodass der Tampen aus dem Haken herausfällt. Die Hände am Gabelbaum werden, bei richtigem Trimm, nur noch für Korrekturen und im Manöver beansprucht. Und um einen festen Stand zu gewährleisten, befinden sich am Heck des Brettes drei bis sechs fest verschraubte Fußschlaufen aus weichem Material, in die die Füße bis zum Rist gesteckt werden.

Der Surfer ist so mit den Händen, mit der Hüfte und mit den Füßen mit dem Sportgerät verbunden und kann eine stabile und relativ kräfteschonende Lage einnehmen.

Als Anfänger übt man zunächst den Schotstart. Dabei liegt das Segel in Lee und das Brett, möglichst auf Halbwindkurs, in Luv. Auf dem Brett stehend wird das Rigg mit einem dicken (griffigen) Kunststoff-Seil, der Startschot, aus dem Wasser gezogen. Dies ist sehr anstrengend, da das Segel gegen die Windkraft hochgezogen wird und das Board durch den zunehmenden Winddruck im Segel unkontrolliert zu fahren beginnt. Den Moment, in dem das Gabelbaumende ruckartig das Wasser verlässt und das Segel nach Lee schwingt, gilt es auszubalancieren. In einer koordinierten Abfolge von Ergreifen und Stellen des Mastes, Heranziehen des Gabelbaumes und Gewichtsverlagerung durch Änderung der Fußposition, alles in Abhängigkeit von der Windstärke, beginnt die kontrollierbare Vorwärtsbewegung des Brettes.

Als Nachfolgetechnik wird der Beachstart gelehrt. Dabei liegt das Brett in Lee und idealerweise auf Halbwindkurs, um den Segeldruck möglichst leicht kontrollieren zu können. Das Rigg wird in Fahrtstellung gehalten und das Surfbrett aus knie- bis hüfttiefem Wasser von Luv her in Richtung Mastfuß mit dem hinteren Fuß zuerst bestiegen. Hierbei arbeitet man mit dem Wind und nicht dagegen, wie beim Schotstart. Je tiefer das Wasser ist, umso mehr Wind wird benötigt, um sich vom Wind auf das Brett ziehen zu lassen.

Als fortgeschrittene (und bei Sinkern allein mögliche) Startmethode lernt man den Wasserstart. In der Regel ist hier entsprechend mehr Wind als beim Beachstart notwendig, geübte Windsurfer beherrschen den Wasserstart jedoch auch bei Windstärken, die gerade ausreichen, um das Rigg aus dem Wasser zu heben. Das Ausrichten des Brettes und Segels muss beim Wasserstart schwimmend geschehen. Stimmt die Position, kann man durch Andrehen des Riggs Wind fangen, setzt zuerst den hinteren Fuß auf das Brett, verlagert das Körpergewicht möglichst weitgehend auf den Mastfuß und lässt sich vom Segel aus dem Wasser ziehen. Bei wenig Wind wird das Rigg möglichst senkrecht aufgestellt und der Surfer zieht sich in einer Art Klimmzug hoch, bei höheren Windstärken ist das Verlagern des Körpergewichtes vom Mastfuß weg erforderlich.

Während man nun Fahrt aufnimmt, wird der Trapezhaken eingehängt und erst der vordere, dann der hintere Fuß in die Schlaufen gestellt.

Es kann beim Wasserstart hilfreich sein, wenn man das Segel mit dem Gabelbaum auf das Heck des Brettes legt, da es dann leichter gegen den Wind aus dem Wasser zu heben ist. Bei moderneren Brettern ist der Mastfuß jedoch meist so weit hinten platziert, dass man den Gabelbaum höchstens auf den Unterarm legen kann, während die Hand das Heck des Brettes ergreift.

Als erste zu erlernende Manöver gelten die Wende und die Halse – die meisten Windsurfer geben sich dann mit diesem Niveau voll und ganz zufrieden. Um die Windsurfmanöver der Freestyler oder Profis zu erlernen, ist reichlich Übung und eine gewisse Begabung notwendig.

Die Manöver, von Surfern "Moves" genannt, werden in folgende Kategorien unterteilt:

Damit die Komponenten des Sportgerätes optimal funktionieren, kommt dem Trimm eine besondere Bedeutung zu. Dazu gibt es im Wesentlichen folgende Einstellmöglichkeiten:

Beim sogenannten "spin out" reißt an der Finne der Wasserstrom ab und Luftbläschen bilden Verwirbelungen, die die richtungsstabilisierende Wirkung der Finne nicht mehr gewährleisten. Dies hat für den Sportler den Effekt, dass beim Gleiten das Brett plötzlich seitlich wegrutscht. Damit wird das Surfbrett unsteuerbar, was oft zu einem Sturz führt. Die Ursache ist ein zu hoher Druck auf die Seitenfläche der Finne. Als Gegenmaßnahme kann eine größere Finne eingebaut werden, der Mastfuß nach vorne verschoben oder der Fahrstil bzw. der Kurs zum Wind geändert werden. Mit einigem Geschick kann man das Brett bei einem „spin out“ wieder auf Kurs bringen, wenn man das Heck mit dem Fuß in der hinteren Fußschlaufe ruckartig zu sich heranzieht und das Gewicht zum Mastfuß hin verlagert, was zum nachfolgenden Fehler führen könnte.

Bei Starkwind besteht die Gefahr eines "Schleudersturzes", wenn man zu nah am Mastfuß steht und durch eine plötzliche "Böe" mit dem Rigg über das Surfbrett geschleudert wird. Dabei wird man u. U. aus den Fußschlaufen gehoben, vom Trapez kann man sich jedoch nicht mehr losmachen. Um Kopfverletzungen bei einem Aufschlag auf Gabelbaum, Mast oder Brett vorzubeugen, lässt man den Gabelbaum beim Schleudersturz keinesfalls los und hält dabei die Arme ausgestreckt. Dieser Fehler passiert vor allem unerfahrenen und ungeübten Windsurfern, mit fortgeschrittener Könnensstufe erlebt man einen Schleudersturz nur noch als Folgewirkung eines missglückten Manövers. Der so beliebte Vorwärts-Looping "(front loop)" ist übrigens nichts anderes als ein kontrollierter Schleudersturz.

Im Bereich der Binnengewässer und der BinSchStrO gelten Windsurfer nach § 1.01 Nr. 14 BinSchStrO als „Kleinfahrzeug unter Segeln“. Sie sind damit Segelbooten gleichgestellt. Sie müssen ausweichen:
Vorrang besteht gegenüber Kleinfahrzeugen mit Motorantrieb und Kleinfahrzeugen, die weder mit einer Antriebsmaschine noch unter Segel fahren.

Im Bereich der SeeSchStrO haben nach § 31 Abs. 2 SeeSchStrO Wassersportgeräte (Zugboote der Wasserskiläufer und von Wassersportanhängen, Wassermotorradfahrer, Kite- und Segelsurfer) allen anderen Fahrzeugen auszuweichen. Für die Wassersportgeräte untereinander gelten die allgemeinen Kollisionsverhütungsregeln. Auf den Seeschifffahrtstraßen sind Windsurfer also ausweichpflichtig gegenüber:

Gegenüber anderen Wind- und Kitesurfern gelten die Ausweichregeln zwischen Segelfahrzeugen:

Offizielle Windsurfwettkämpfe werden erst ab einer bestimmten Windgeschwindigkeit (z. B. 10 Knoten) veranstaltet. Das Windlimit ist jeweils so festgelegt, dass mit dem zugelassenen Material die Kurse sinnvoll befahren werden können. Bei einem Slalom 42 beispielsweise muss der Wind so stark sein, dass die Bretter den Gleitzustand erreichen und durchglittene Halsen gefahren werden können.

"Freestyle" (Vielfalt, Originalität und Ausführung artistischer Elemente wie Loopings, Drehungen und Sprünge) und "Waveriding" (Sprünge über die Wellen und das Abreiten der Wellen) sind durch Kampfrichter bewertete Wettbewerbe. "Olympische Klasse", "Formula Klasse", " Kursrennen" und "Slalom 42" sind Rennen, bei denen viele Teilnehmer einen festgelegten Kurs absolvieren. "Indoor" sind Hallenwettbewerbe und "Long Distance" sind Langstrecken-Wettfahrten.

Das Windsurfen wurde für Männer 1984 in Los Angeles olympische Disziplin, die Frauen folgten 1992 in Barcelona.

Die Ausrüstung der "olympischen Klasse" ist für alle Teilnehmer gleich. Für die Olympischen Spiele 1984 setzte sich bei der IYRU (International Yacht Racing Union) der deutsche Windglider als einzig zulässiger Surfbretttyp gegen den weltweit meistgefahrenen Typen "Windsurfer" durch. 1996, 2000 und 2004 wurden die Windsurfregatten auf dem Mistral One Design gefahren. Auf der ISAF-Jahreshauptversammlung 2005 wurde der Neilpryde-Vorschlag „“ zum neuen Olympiaboard für die Olympischen Spiele 2008 in Peking gewählt. National und international wichtige RS:X Regatta-Termine und Ergebnisse kann man beim DWSV abrufen.

Im Mai 2012 gab die ISAF bekannt, nach den Olympischen Spielen 2012 Windsurfen durch Kitesurfen zu ersetzen. Diese Entscheidung wurde 6 Monate später zurückgezogen.

Jährlich wird mit den Windsurf World Cups von der Professional Windsurfers Association (PWA) die Surfweltmeisterschaft in den Disziplinen Wave, Freestyle und Slalom 42 ausgetragen. Von anderen Organisationen werden Weltmeister für Speed, Racing und in der Formulaklasse gekürt.

Außerdem gibt es die Europameisterschaften im Freestyle, ausgetragen durch die EFPT "(European Freestyle Pro Tour)".

Im deutschsprachigen Raum finden Wettbewerbe auf Sylt und Podersdorf statt. Der Windsurf World Cup Sylt ist die weltweit größte Veranstaltung dieser Art.

Das Speedsurfen gilt als Formel 1 der windbetriebenen Wasserfahrzeuge. Bei hohen Windgeschwindigkeiten muss ein Kurs von 250 oder 500 Metern möglichst schnell durchfahren werden. Ein bekannter Wettbewerb ist die Dunkerbeck GPS Speed Challenge, der für alle Altersklassen offen ist und an dem mit zertifizierten GPS-Datenloggern auch dezentral teilgenommen werden kann.
Das Windsurf-Foiling ist eine Renn-Disziplin, die schon bei zwei bis drei Windstärken ausgetragen werden kann. Dabei gleiten die Sportler ab einer bestimmten Geschwindigkeit auf einer schwertartigen Verlängerung mit einem unter Wasser liegenden Tragflügel (Foil) im Finnenkasten. Das Board wird durch das Foil aus dem Wasser gehoben und kommt wegen des dann geringen Wasserwiderstandes schneller ins Gleiten als ein normales Windsurf-Brett. Bei dieser Race-Disziplin werden keine Vor- oder Zwischenläufe ausgetragen. Es dürfen maximal vier Wettfahrten am Tag und insgesamt 15 während eines World Cups ausgetragen werden. 2016 wurde auf Sylt wurde erstmals ein Preisgeld im Rahmen eines Word Cups vergeben (7500 €).

Der Red Bull Storm Chase (RBSC) ist ein weltweit stattfindender Wettbewerb, bei dem von Januar bis März Stürme ab Windstärke 10 und einer Wellenhöhe von mindestens 4 Metern abgeritten werden. Wie bei der Disziplin Wave werden Wellenritte und Sprünge gewertet. Die vier Finalisten des letzten RBSC sind automatisch gesetzt, vier weitere Teilnehmer können sich online bewerben. Die Vorwarnzeit für die Fahrer beträgt 120 Stunden. Mindestens 60 Stunden vor dem Event können sich die Teilnehmer auf die Reise zum Austragungsort begeben.

Zu den Legenden der Sportart zählen unter anderem der US-Amerikaner Robby Naish und der niederländisch-dänische 42-malige Weltmeister Bjørn Dunkerbeck sowie die Moreno Twins bei den Damen. Sie haben neben ihren Erfolgen maßgeblich zur Weiterentwicklung des Sports und Materials beigetragen. Zu den bekanntesten deutschen Fahrern gehörten in den 1990er-Jahren der mehrfache Deutsche Meister Bernd Flessner und Ex-Weltmeisterin Jutta Müller. In den letzten zehn Jahren machten der deutsche Waverider Klaas Voget, sowie Steffi Wahl im Weltcup von sich reden. Mit vier WM-Titeln gehört Philip Köster zu den großen Nachwuchstalenten.




</doc>
<doc id="13655" url="https://de.wikipedia.org/wiki?curid=13655" title="Liste von Bildhauern">
Liste von Bildhauern

Die Liste von Bildhauern führt möglichst umfassend Bildhauer aller Epochen auf. Die Liste ist alphabetisch nach Nachnamen geordnet. Die Einträge enthalten neben dem Namen auch Informationen über Geburtsjahr, Sterbejahr und Herkunft.




</doc>
<doc id="13656" url="https://de.wikipedia.org/wiki?curid=13656" title="Stehaufmännchen">
Stehaufmännchen

Ein Stehaufmännchen ist ein Gegenstand, meist in Gestalt einer menschlichen Figur, der sich von selbst immer in aufrechte Lage bringt.

Die Stehauffigur besitzt in der Regel eine abgerundete Unterseite und einen sehr tief liegenden Schwerpunkt. Jede Veränderung der Lage eines Stehaufmännchens führt dadurch zu einem Anheben des Schwerpunktes, so dass sich das Stehaufmännchen durch die Schwerkraft von selbst wieder aufrichtet. Besitzt das Stehaufmännchen eine halbkugelförmige Unterseite, so muss der Schwerpunkt unterhalb des Mittelpunkts der Kugel liegen.

Nach dem gleichen physikalischen Prinzip gibt es auch Gegenstände, die auf einem „Stehaufbein“ befestigt sind, beispielsweise ein Punchingball, ein Globus oder ein einbeiniger Hocker. Einige Schiffs­typen, zum Beispiel Segelyachten, werden ebenfalls mit einem sehr niedrig liegenden Schwerpunkt konstruiert, damit sie bei starkem Seegang oder Seitenwind von alleine wieder aus ihrer Schräglage zurückkehren und nicht kentern können.

Umgangssprachlich wird der Begriff Stehaufmännchen für solche Personen verwendet, die sich nicht durch Niederlagen oder Misserfolge entmutigen lassen, diese überwinden und sich immer wieder neu „auf das Leben einlassen“ und versuchen, es selbst zu meistern. In der Psychologie wird diese Fähigkeit als Resilienz bezeichnet.




</doc>
<doc id="13657" url="https://de.wikipedia.org/wiki?curid=13657" title="Wahrer und scheinbarer Wind">
Wahrer und scheinbarer Wind

Wahrer Wind und scheinbarer Wind sind Begriffe aus der Seefahrt.

Als wahrer Wind wird in der Seefahrt die Richtung und Windgeschwindigkeit des meteorologischen Windes bezeichnet, wie sie beispielsweise auf einem vor Anker liegenden Schiff oder einer Küstenmessstation gemessen werden. In der Schifffahrt wird die Windrichtung mit kardinalen Himmelsrichtungen angegeben. Die Windgeschwindigkeit wird entweder in Beaufort oder in Meter pro Sekunde angegeben.

Der Fahrtwind ist der durch die Bewegung eines Fahrzeugs (Fahren) hervorgerufene „Gegenwind“. Er kommt immer genau von vorn, ist also der Bewegungsrichtung des Fahrzeugs (um 180°) entgegengesetzt gerichtet. Dieser Fall lässt sich bei Windstille leicht beobachten, dann sind Fahrtwind und scheinbarer Wind genau übereinander liegende Vektoren.

Der scheinbare Wind (auch "relativer Wind" oder Bordwind genannt) ist in der Schifffahrt und insbesondere beim Segeln von Bedeutung. Es handelt sich um den am fahrenden Schiff wahrgenommenen Wind, der sich aus der vektoriellen Addition des "wahren Windes" und des "Fahrtwindes" ergibt. Der scheinbare Wind fällt immer vorlicher ein als der wahre Wind, außer man fährt genau vor dem Wind (womit dann beide genau von hinten kommen).

Fahrzeuge mit Segelantrieb nutzen immer den scheinbaren Wind. Bei einem Segelschiff richtet sich die Stellung der Segel ebenfalls nach dem scheinbaren Wind.

Je schneller ein Segelboot fährt, umso vorlicher fällt der scheinbare Wind ein, umso dichter müssen die Schoten also geholt werden. Die maximal mögliche Höhe am Wind, die ein Boot segeln kann, hängt vom kleinsten Winkel zum scheinbaren Wind ab, auf dem noch Vortrieb erzeugt wird. Je schneller das Boot segelt, umso größer wird daher der Winkel zum wahren Wind. Sehr schnelle Boote (z. B. Katamarane oder Jollen) haben daher häufig einen schlechteren Wendewinkel als ansonsten eher plumpe schwere Yachten.

Aber auch auf dem Fahrrad lässt sich der scheinbare Wind erleben. Wenn man in die Richtung fährt, in die der Wind weht und dabei genauso schnell ist wie der Wind weht, dann steht die Luft „scheinbar“, obwohl man den Wind durch die Bäume wehen sieht (Fahrtwind und wahrer Wind stehen vektoriell genau gegeneinander und neutralisieren sich).

Physikalisch gesehen handelt es sich um die Luftbewegung, die man feststellt, wenn man als Bezugssystem das Ruhesystem des jeweiligen Fahrzeugs wählt.




</doc>
<doc id="13660" url="https://de.wikipedia.org/wiki?curid=13660" title="Wirtschaftsingenieurwesen">
Wirtschaftsingenieurwesen

Wirtschaftsingenieurwesen ist ein interdisziplinäres Studium, das aus ingenieur- und wirtschaftswissenschaftlichen Teilen besteht. Es verbindet technisch-naturwissenschaftliche sowie Wirtschafts- und Rechtsinhalte miteinander. Nach Abschluss des Studiums darf der Absolvent die Bezeichnung Wirtschaftsingenieur führen.

Verschiedene Universitäten boten diese Studienrichtung bereits im späten 19. Jahrhundert an. In den USA wurde dieses Studium vermutlich erstmals 1908 eingeführt. 1927 führte die damalige Technische Hochschule Berlin den Studiengang als erste deutsche Hochschule ein. Der von Willi Prion entwickelte Studiengang wurde damals noch als „Wirtschaft und Technik“ bezeichnet und sollte Nachkommen von Unternehmern eine adäquate Ausbildung bieten. 

Bis in die 1970er Jahre boten dieses Studium relativ wenige Universitäten an. Als erste bundesdeutsche Fachhochschule hatte die 1971 gegründete Fachhochschule Würzburg-Schweinfurt in ihrer Schweinfurter Abteilung von Anfang an den Studiengang Wirtschaftsingenieurwesen im Angebot.

In den Hochschulen und Ingenieurschulen der DDR war ein ähnlicher Studiengang als "Ingenieurökonomie" etabliert.

Das Wirtschaftsingenieurwesen befasst sich mit Betriebsabläufen, das heißt mit Theorien, Methoden, Werkzeugen und intersubjektiv nachprüfbaren Erkenntnissen und Zusammenhängen, zwischen verschiedenen wirtschafts-, ingenieur- und rechtswissenschaftlichen Disziplinen. Daher fungiert der Studiengang als Schnittstelle zwischen der Betriebswirtschaft und dem Ingenieurwesen. Die Zielstellung des Wirtschaftsingenieurwesens ist die Optimierung der Betriebsabläufe hinsichtlich technischer Prozesse auf der einen und größtmöglicher Produktivität und Wirtschaftlichkeit auf der anderen Seite.

Inhalt des Wirtschaftsingenieurwesens sind interdisziplinäre und damit oft komplexe Systeme. Dies beinhaltet die Entwicklung, Implementierung, Realisierung, Optimierung sowie den realen Betrieb solcher Systeme unter Einhaltung rechtlicher Anforderungen und Vorschriften, wobei die wirtschaftlichen oder technischen Systeme nicht isoliert, sondern als ganzheitliche Systeme betrachtet werden.

Vielmehr modelliert und gestaltet das Wirtschaftsingenieurwesen unter ökonomischen wie rechtlichen Gesichtspunkten und unter Verwendung der Methoden der Systemtheorie, der Statistik, des Operations Research und weiteren reale wirtschaftlich-technische Systeme und leitet aus entsprechenden Modellen Anforderungen für Produktions-, Fertigungs-, Vermarktungs- und Informationssysteme zur Implementierung in existierende oder neue reale Systeme ab.

Das Studium des Wirtschaftsingenieurwesens kann an einer Hochschule (Universität, Fachhochschule oder Dualen Hochschule), das mit einem akademischen Grad abschließt, oder einer Berufsakademie erfolgen. Das Wirtschaftsingenieurwesen beinhaltet als interdisziplinäres Studium sowohl ingenieurwissenschaftliche als auch wirtschaftswissenschaftliche Komponenten, wird aber an den meisten Universitäten eher den ingenieurwissenschaftlichen Fakultäten zugeordnet. Der Schwerpunkt des Studiengangs und die Zusammenstellung der Module können sich je nach gewähltem Schwerpunkt und abhängig von der Forschungsausrichtung der jeweiligen Hochschule deutlich unterscheiden. Während bei Wirtschaftsingenieur-Studiengängen an technisch ausgerichteten Universitäten der ingenieurwissenschaftliche Teil meist deutlich überwiegt, bieten andere Hochschulen und Fachhochschulen auch Wirtschaftsingenieur-Studiengänge mit wirtschaftswissenschaftlichem Schwerpunkt an. Im späteren Stadium beeinflussen die Studierenden selbst durch die Wahl von Wahlpflichtmodulen und Vertiefungsrichtungen den Schwerpunkt des weiteren Verlaufs selbst.

In den ersten vier Semestern des Studiums, bei Diplomstudiengängen also im Grundstudium, sollen technische und wirtschaftliche Grundlagen vermittelt werden. Da sich die Studieninhalte in dieser Phase aufgrund der Interdisziplinarität stark mit den Inhalten anderer wirtschafts- und ingenieurwissenschaftlicher Studiengänge überschneiden, besuchen Studierende des Wirtschaftsingenieurwesens normalerweise einige der Vorlesungen für Studierende der Informatik, der Betriebswirtschaftslehre, des Maschinenbaus, des Bauingenieurwesens oder der Elektrotechnik. Vermehrt werden wie in allen technischen Studiengängen auch im deutschsprachigen Raum Vorlesungen in englischer Sprache gehalten; zudem werden auch Vorlesungen und Kurse zu technischem Englisch angeboten. Außerdem sind im Regelfall Praktika, häufig in Form eines Grund- und eines Fachpraktikums, Bestandteil des Studiums.

Das Wirtschaftsingenieurwesen hat sich als eigenständiger, wissenschaftlicher Studiengang sowie auch als Aufbaustudium an privaten und öffentlichen Hochschulen fest etabliert.

Abhängig von der Universität verläuft das Studium entweder allgemein als „Wirtschaftsingenieurwesen“ oder wird bereits von Beginn an in verschiedene Spezialisierungen unterteilt (beispielsweise „Wirtschaftsingenieur Maschinenbau“; weitere Spezialisierungen sind unter anderem „Energietechnik“, „Energiemanagement“, „operatives Management“ und „strategisches Management“, „Verfahrenstechnik“ sowie „Elektrotechnik“).

Seit der Bologna-Reform wird das Studium des Wirtschaftsingenieurwesens meistens als Bachelor- und Masterstudiengang angeboten; die Lehrveranstaltungen sind durch Module strukturiert. Der Master ist ein Aufbaustudium und setzt den Abschluss des Bachelors voraus. An wenigen Hochschulen bestimmter Bundesländer wird noch ein Diplomstudiengang angeboten.

Das Wirtschaftsingenieurwesen zählt verschiedenen Statistiken zufolge zu den Studiengängen mit den meisten immatrikulierten Studierenden in Deutschland, wobei die Zuordnung der verschiedenen Wirtschaftsingenieur-Studiengänge je nach Statistik unterschiedlich erfolgt. Das statistische Bundesamt differenziert zwischen Wirtschaftsingenieurwesen mit technischem und wirtschaftlichem Schwerpunkt.

In Deutschland wird das Studium des "Wirtschaftsingenieurwesens" an 101 Fachhochschulen, 32 Universitäten und technischen Universitäten sowie an 15 Berufsakademien angeboten. Ähnliche Studienmöglichkeiten finden sich in Österreich. In der Schweiz wird dieser Studiengang nur an Fachhochschulen angeboten. Auch im europäischen und nichteuropäischen Ausland existieren teilweise entsprechende Studienmöglichkeiten.

Zum Abschluss des Studiums ist eine Studienarbeit (je nach Studiengang eine Diplomarbeit, Bachelorarbeit oder Masterarbeit) zu verfassen und je nach Studienordnung in einem Kolloquium zu verteidigen. In manchen Studiengängen, insbesondere die Diplomstudiengänge, ist zudem vorher eine Abschlussprüfung zu bestehen.

Das erfolgreich absolvierte Studium endet mit dem akademischen Diplom-, Bachelor- oder Mastergrad:


Beim Simultanstudium, dem Regelfall, sind während der gesamten Studiendauer sowohl ökonomisch-rechtswissenschaftliche als auch technisch-naturwissenschaftliche Fächer im Stundenplan verankert. Zusätzlich existieren Veranstaltungen, die speziell auf die Interdisziplinarität von Wirtschaft und Technik eingehen.

Das Wirtschaftsingenieurwesen umfasst vorwiegend die Kernfächer des betriebswirtschaftlichen und des jeweiligen Ingenieurstudiums, hingegen wird auf einige Randdisziplinen, welche reine Betriebswirte oder Ingenieure belegen müssen, teilweise verzichtet.

Absolventen von technischen Studiengängen bietet ein Aufbaustudium die Möglichkeit, die ökonomisch-rechtswissenschaftlichen Anteile „nachzuholen“. Die Fernhochschule Hamburg bietet seit 2006 auch einen Aufbaustudiengang für Wirtschaftswissenschaftler. Die Hochschule Mittweida bietet seit 2000 einen akkreditierten Fernstudiengang mit dem Abschluss des Diplomwirtschaftsingenieurs an.

Wirtschaftsingenieure werden meist innerhalb von technisch-wirtschaftlichen Querschnittsfunktionen, wie z. B. für logistische oder planerische Aufgaben (z. B. im Fertigungsbereich), technisches Marketing, Vertrieb bzw. Einkauf für technische Produkte, Projektmanagement oder Qualitätssicherung tätig.

Sie werden hauptsächlich im Maschinenbau, der Elektrotechnik, im Fahrzeugbau, im Bauwesen und in Beratungsgesellschaften eingesetzt. Aufgrund der breiten Wissensbasis werden Wirtschaftsingenieure immer öfter bei Versicherungen, Kreditinstituten, im Einzelhandel und in der Informatik eingesetzt.

Wirtschaftsingenieure sind beispielsweise in den Bereichen Vertrieb (siehe auch: Vertriebsingenieur), Produktentwicklung, Instandhaltung, Produktion, Logistik, Einkauf, Finanzen/Verwaltung, Unternehmensführung/Unternehmensleitung, Qualitätsmanagement, Umweltmanagement, Controlling und Consulting tätig.


Im Englischen gibt es keine allgemeingültige Übersetzung des Begriffs.

Um den Inhalt des Studiums passend wiederzugeben, wird eine der folgenden, englischen Übersetzungen verwendet:


Diese beinhalten sowohl die Ingenieurwissenschaft als auch den englischen Begriff für BWL. Allerdings sind einige der derzeit verbreiteten Übersetzungen international irreführend. „Industrial Engineering“ wird beispielsweise im angloamerikanischen Sprachraum öfters mit Arbeitswissenschaften gleichgesetzt. Der Verband Deutscher Wirtschaftsingenieure (VWI) merkt hierzu beispielsweise an: „Insbesondere die bislang weit verbreitete Benutzung des „Industrial Engineers“ als Umschreibung des deutschen Wirtschaftsingenieurs ist eher vergleichbar mit Produktions- und Fertigungsingenieuren und verdeutlicht nicht das komplexe und integrative deutsche Konzept des Studiums.“ Der VWI empfiehlt deshalb den Beisatz „in Business Administration and Engineering“ zum akademischen Grad.

Je nach Fachrichtung und Hochschule finden weiterhin auch die folgenden Bezeichnungen Verwendung:


Zu beachten ist dabei jedoch, dass Business Engineering eigentlich nur die Planung, Durchführung und Kontrolle von Geschäftsideen und Innovationen umfasst und Product Engineering sich nur auf Konstruktion und Planung neuer Produkte beschränkt.





</doc>
<doc id="13661" url="https://de.wikipedia.org/wiki?curid=13661" title="Operette">
Operette

Die Operette (ital., wörtlich: „kleine Oper“) ist ein musikalisches Bühnenwerk. Die Bezeichnung gibt es seit dem 17. Jahrhundert. Bis zum 20. Jahrhundert hat sie einen erheblichen Bedeutungswandel erfahren. Die Operette seit dem 19. Jahrhundert hat eher leichte, eingängige Musik, eine heitere oder sentimentale Handlung und gesprochene Dialoge zwischen den Musiknummern.

Im 18. Jahrhundert bedeutete Operette die „kleine“ Oper, entweder weil sie kürzer war als andere Werke (vor allem Einakter wurden als „Operette“ bezeichnet), weil sie „bloß“ eine Komödienhandlung hatte im Unterschied zu Opera seria oder Tragédie lyrique, oder weil nur wenige Figuren ohne Chor in ihr auftraten. Außerdem wurden manche musikalischen Theaterwerke „Operette“ genannt, weil sie keine Gesangsvirtuosen erforderten, sondern von singenden Schauspielern ausgeführt werden konnten. Eine einfachere Struktur der Gesangseinlagen konnte ebenfalls den Ausschlag für diese Bezeichnung geben: In die Vaudeville-Komödien der Pariser Jahrmarktstheater wurden bekannte Melodien mit neuen Texten eingelegt, was sich in der spanischen Operette (Zarzuela) bis ins 20. Jahrhundert erhalten hat.

Deutschsprachige, auch anspruchsvollere Opern wurden manchmal deshalb „Operetten“ genannt, weil sie gegenüber den italienischen und französischen Opern einen geringeren Stellenwert hatten. Das Deutsche wurde gegenüber dem Französischen, der internationalen Sprache der Aristokratie, noch gering geschätzt. Deutschsprachige Opern hatten zumeist eine Komödienhandlung und damit sozial niedrig stehende Figuren (siehe Ständeklausel, Rührende Komödie).

Der Ruf der deutschsprachigen Operette als „bürgerliche deutscher Oper“, die endlich eine Aufwertung verdient hätte, kam Ende des 19. Jahrhunderts in Wien auf, wobei die damit gemeinten Wiener Werke von Johann Strauß (Sohn) oder Millöcker als bewusster Gegenentwurf zu den Pariser Werken Jacques Offenbachs gesehen wurden, die als grotesk-frivol galten; nationalistische und antisemitische Tendenzen spielen dabei eine Rolle.

In gewissem Maß spiegelt der Gegensatz zwischen Oper und Operette auch die Konkurrenz zwischen den subventionierten Hoftheatern und den privatwirtschaftlichen Theatern beziehungsweise den Wandertruppen wider. 

Dieser Gegensatz wurde oft durch „Reformen von oben“ abzuschwächen versucht, wie durch die Errichtung eines "Nationalsingspiels" 1777 im Wiener Burgtheater durch Joseph II.

Ende des 18. Jahrhunderts wurden Stücke, die aus der französischen Opéra comique hervorgegangen waren, als Operetten bezeichnet, also Opern, die gesprochene Dialoge anstelle gesungener Rezitative enthielten und vom Pariser Jahrmarktstheater stammten statt von den Hofbühnen. So galt etwa Mozarts "Die Entführung aus dem Serail" als Operette.

Das Genre, das heute im engeren Sinn als Operette bezeichnet wird, entstand als eigenständige Kunstform um 1848 in Paris, wurde damals jedoch nicht „Operette“ genannt, sondern „opérette bouffe“ (im Fall von Einaktern, daher der Diminutiv) und „opéra bouffe“ (für Mehrakter), „bouffonnerie musicale“ oder „folie musicale“. Direktes Vorbild war die Opéra comique der 1830er- und 40er-Jahre wie etwa François Aubers "Le Cheval de bronze" (1835). Die ersten „Operetten“ waren ursprünglich kurze Werke mit grotesk-frivolem Inhalt. Zu den ersten Komponisten gehörte Florimond Ronger, genannt Hervé (1825–1892), zu dessen bekanntesten Stücken "Don Quichotte et Sancho Pança" (1848), "Le Petit Faust" (1869) und "Mam’zelle Nitouche" (1883) zählen.

Berühmt wurde aber ein anderer Komponist mit dieser Art Musiktheater. Der Cellist Jacques Offenbach, der Kapellmeister an der Comédie-Française gewesen war, eröffnete anlässlich der Weltausstellung von 1855 in Paris ein Theater, das Théâtre des Bouffes-Parisiens, das ausschließlich dieser Gattung gewidmet war. "Entrez Messieurs, Mesdames" bildete zusammen mit "Les Deux Aveugles" und "Une nuit blanche" am 7. Juli 1855 den Auftakt. Bald folgten so berühmte Werke wie "Le Violoneux", die "Chinoiserie musicale" "Ba-ta-clan" (beide 1855), die Kreuzritter-Persiflage "Croquefer, ou le dernier des Paladins", die bukolische Komödie "Le Mariage aux lanternes" (beide 1857) und die übersprudelnden Marktweiber in "Mesdames de la Halle" (1858), die das Genre poissard der Jahrmärkte neu belebten. Genau wie Hervés frühe Werke waren auch Offenbachs frühe Farcen dem Vaudeville verpflichtet und teils eher Schauspiele mit Musik, beziehungsweise – wie "Meyers Konversations-Lexikon" 1877 bemerkte – „eine Art von Posse, die man mit dem Namen des höheren Blödsinns zu bezeichnen pflegt, auf das musikalische Gebiet übertragen“.

Bereits in den frühen Einaktern parodiert Offenbach wiederholt die Belcanto-Oper sowie die Grand opéra à la Meyerbeer. Deren Stilmittel, die Offenbach gut beherrschte, wirkten im kleinen Theaterrahmen lächerlich. Um Kritikern den Wind aus den Segeln zu nehmen, erklärte Offenbach in einer 1856 veröffentlichten Abhandlung, dass er mit seinen Werken die alte "Opéra comique" des 18. Jahrhunderts wiederbeleben wolle. Jedoch liefen Offenbachs Stücke (von wenigen Ausnahmen wie "Barkouf" und "Vert-Vert" abgesehen) nicht im bürgerlich-respektablen Rahmen der "Opéra-Comique", sondern vor dem Halbweltpublikum der Pariser Operettenbühnen. Zu diesem Publikum gehörte der Jockey Club, aber auch Mitglieder der Kaiserfamilie, wie Charles de Morny, der nicht nur Patenonkel von Offenbachs Sohn war, sondern unter einem Pseudonym auch Libretti für Offenbach schrieb ("Monsieur Choufleuri"). In "Meyers Konversations-Lexikon" heißt es 1877, Offenbachs Werke seien „so vom Geiste der Demi-monde durchsetzt, daß sie mit ihren schlüpfrigen Stoffen und sinnlichen, zumeist trivialen Tonweisen eine entschieden entsittlichende Wirkung auf das größere Publikum ausüben müssen“. Ähnliche Äußerungen finden sich in beinahe allen zeitgenössischen Kritiken, auch in England, Deutschland und Österreich. Sie wirkten aber eher erfolgsfördernd als negativ auf die Rezeption der offenbachschen Werke. Allerdings erklärt sich vor diesem Hintergrund, warum Offenbachs Operetten lange nicht in den etablierten Hoftheatern und an den bürgerlichen Bühnen liefen, sondern vielfach in Spezialetablissements, die von Großteilen der Öffentlichkeit skeptisch beäugt wurden. In London, und später auch in Paris und Wien, brach ab 1850 die Zeit der preisgünstigen Kleintheater an, die als Music Hall in großer Zahl eröffnet wurden und einen subbürgerlichen Teil des Publikums von einem großbürgerlichen trennte, der nach wie vor die etablierten Theater besuchte.

Ein wesentliches Gestaltungsmerkmal aller Offenbach-Operetten war die Groteske, die ins Lächerliche verzerrte Abbildung der Realität, oft nur leicht verkleidet in Kostüme der Antike, des Mittelalters, der Arbeiterwelt (wie bei "Mesdames de la Halle") oder des Landlebens (wie in "Mariage aux lanternes"). Die Stücke sind damit teils Vorbilder des Dadaismus der 1920er und des Absurden Theaters der 1950er Jahre, werden jedoch in den meisten literaturgeschichtlichen Abhandlungen in dieser Eigenschaft übergangen. Dieses Konzept der Operette erlaubte ein auffallend freizügiges Ausspielen von Erotik auf der Bühne, was unter normalen oder realistischen Umständen von der Zensur in Paris niemals erlaubt worden wäre, aber unter dem Deckmantel der Parodie möglich war. Im deutschen Sprachgebiet war man solche Freizügigkeit auf der Bühne noch weniger gewohnt. Hier sprach die Presse von „der ungeheuren Frivolität der […] offenbach’schen musikalischen Farcen“, von der „Liederlichkeit […] des ganzen Genres“ und urteilte mit „Besorgnis über den sittengefährdenden Komponisten“ Offenbach, dessen Werke die „Negation aller sittlichen u. rechtlichen Ordnung“ darstellten.

Da die Einakter oft in Kombination gespielt wurden, bildeten sich verschiedene Formen der Operette heraus, die kontrastierend gegeneinander gesetzt wurden. Im Fall Offenbachs sind die beiden Haupttypen entweder „ländliche Komödien“ wie "Le Mariage aux lanternes" (in Wien ein Sensationserfolg unter dem Titel "Die Hochzeit beim Laternenschein", mit drei Damen im eng geschnürten Dirndl als Milchmädchen, die „melkend“ durch die Handlung stolpern und einen reichen Mann fürs Leben suchen) oder überdrehte Farcen wie "Croquefer" oder "Ba-ta-clan", die eine neue Dimension des Slapstick auf die Bühne bringen. All diese Werke werden durch drei Merkmale gekennzeichnet, die unterschiedlich stark im Vordergrund stehen: Groteske, Frivolität und Sentimentalität.

1858 wurde Offenbachs erste mehr-aktige Operette mit Chor und erweiterter Solistenbesetzung, "Orphée aux enfers" (Textbuch von Hector Crémieux und Ludovic Halévy), in Paris uraufgeführt und trat einen Siegeszug um die Welt an. Zusammen mit "La Belle Hélène" (1864) und "La Grande-Duchesse de Gérolstein" (1867) – beide mit Textbüchern von Henri Meilhac und Halévy – ist das Stück noch Teil des heutigen Operettenrepertoires. Seit dem Höllengalopp aus "Orpheus" ist der „Skandaltanz“ Cancan eng mit der Operette verbunden.

Aber auch andere französische Komponisten wie Charles Lecocq, Hervé oder Robert Planquette konnten sich mit dem neuen Genre profilieren und entwickelten Offenbachs Modell der frivolen Grotesk-Operette weiter.

Mit dem Deutsch-Französischen Krieg 1870/71 verblasste Offenbachs Ruhm in Frankreich. Die aberwitzigen Satiren des „deutschen“ Komponisten wurden nach der französischen Niederlage weniger geschätzt. Das für das Zweite Kaiserreich typische Halbwelt-Publikum Offenbachs verlagerte sich in die vielen neu entstehenden Music-Halls mit ihren Varieté-Programmen, und das bürgerliche Publikum der „richtigen“ Theater wollte weniger groteske und frivole Stücke sehen. Fortan widmete Offenbach sich ausladenden Ausstattungsstücken, die aber in vielen Fällen noch Merkmale der typischen Offenbachiade aufweisen, zum Beispiel wenn in "Le Voyage dans la lune" (1875) die Mondbewohner mit Hilfe eines verbotenen Apfels zum Sündenfall verleitet werden.

In den 1870er-Jahren waren auch Emmanuel Chabrier und später André Messager mit Operetten erfolgreich, ersterer mit stark an Offenbach orientierten aberwitzigen Slapstick-Komödien wie "L’étoile" (1877), die dem neuen Typ des Schwanks entsprachen, letzterer mit sentimentaleren Stücken wie "Véronique" (1898). Aufgrund der starken antifranzösischen Ressentiments blieben zahlreiche französische Werke im deutschsprachigen Raum nach 1871 unbeachtet. "La fille de Madame Angot" (1873) und "Giroflé-Girofla" (1874) von Lecocq sowie "Mam’zelle Nitouche" (1883) von Hervé gehören zu den weltweit erfolgreichsten Operetten, sind aber im deutschen Sprachgebiet fast unbekannt geblieben. Besser funktionierte die Kommunikation zwischen Paris und London – und mehr und mehr auch mit New York.

Im Bestreben, den Erfolg der von Karl Treumann (1823–1877) und Johann Nepomuk Nestroy (1801–1862) nach Wien importierten (und übersetzten) Offenbach-Operetten zu kopieren, entstanden ab 1860 eigenständige Wiener Werke nach Pariser Vorbild. Den Auftakt machte Franz von Suppé mit dem Einakter "Das Pensionat", bald folgten weitere Komponisten, von denen Johann Strauß, Karl Millöcker und Carl Zeller die bekanntesten und einflussreichsten sind.
Die frühen Wiener Operetten wurden in den traditionellen Wiener Vorstadttheatern gespielt, die sich nicht mehr dem Handwerkerpublikum widmen wollten, für das die „Posse mit Gesang“ einst gedacht war, sondern sich nun um ein zahlungskräftiges Geldbürgertum bemühten. Zum Carl-Theater und dem Theater an der Wien gesellten sich neue Spielstätten wie das Theater am Franz-Josefs-Kai, und das Strampfer-Theater. Allein schon auf Grund der Preispolitik war es der breiten Bevölkerung nicht möglich, diese Theater zu besuchen. Es versammelten sich dort neben den Mitgliedern der kaiserlichen Familie jene Zirkel, die sich nach den Aufständen von 1848 neu etabliert hatten: „Die Finanzwelt, der wohlhabende Mittelstand, die im Sonnenschein des volkswirthschaftlichen Aufschwungs sich pilzartig vermehrenden Parvenus der Börse und die üppig in die Halme schießende Halbwelt, welche ihren Luxus in der vanity fair der (Prater-)Hauptallee zur Schau stellten“, wie die "Illustrirte Zeitung" bemerkte (21. Mai 1881). Die Wiener Operette war stark geprägt vom Wiener Walzer, der bis weit ins 20. Jahrhundert hinein zu einem besonderen Erkennungsmerkmal des Genres in seiner österreichischen Variante werden sollte.

Wegweisende Werke der frühen, stark an Offenbach orientierten Wiener Operette waren Franz von Suppès moderat frivole Farcen "Das Pensionat" (1860), "Zehn Mädchen und kein Mann" (1862, wegen des großen Erfolgs erweitert zu "Fünfundzwanzig Mädchen und kein Mann"), "Die schöne Galathée" (1865), beziehungsweise Mehrakter wie "Boccaccio" (nach dem "Decamerone", 1879) so wie die Erfolgstitel von Johann Strauß, allen voran "Die Fledermaus" (in der jeder fremdgehen will, 1874), "Das Spitzentuch der Königin" (über die Eheprobleme des Königs von Portugal, 1880) und "Eine Nacht in Venedig" (über eine erotisch verwirrte Karnevalsnacht, 1883). Diese Operetten waren opernhafter und auch im Orchester „vollklingender“ als die französischen Vorbilder. Das Theater an der Wien hatte andere Traditionen und war wesentlich größer als Offenbachs „Bouffes“. Suppés Operettenpartituren unterschieden sich nicht erheblich von den Spielopern, die er als Hauskomponist seit Jahren für dieses Theater schreiben musste, und wurden von der Kritik auch nicht als neues Genre wahrgenommen.

Aufgrund der Popularität des neuen Genres wurden neue Publikumsschichten erschlossen, die oft weniger liberale Moralvorstellungen hatten als das Urpublikum der Operette. Auch der Deutsch-Französische Krieg 1870/71 dämpfte die Frivolitäten. Damit verschob sich der Akzent der Operette weg vom Frivol-Grotesken hin zum Sentimentalen und Rührseligen. Außerdem diente der Unterschied zwischen Moralischem und Amoralischem einem „national“ gestimmten Publikum als Gegensatz zwischen Deutschem und Französischem. Dieses Publikum wollte den „blödsinnigen“ und „irrwitzigen“ französischen Operetten das „vernünftige“ Wienerische entgegenhalten, dem „Lasziven“ das „Volkstümliche“ und in letzter Konsequenz dem „Fremden“ das „Vaterländische“. „Die politischen Implikationen wurden unübersehbar: Die Forderungen an den Operettenspielplan waren vom Umsichgreifen des Nationalismus und Antisemitismus nicht mehr zu trennen.“ So wurden die französischen Werke, beziehungsweise nach französischem Muster geschaffenen Operetten, zunehmend verdrängt von einer „altdeutschen Mode“, für die Millöckers Volksoper "Die sieben Schwaben" (1887) als Paradebeispiel gelten kann. Der Journalist und spätere Begründer des Zionismus Theodor Herzl schuf zusammen mit Adolf Müller junior die Operette "Des Teufels Weib" (1890), während der Wiener Kulturpolitiker und Theatergründer Adam Müller-Guttenbrunn gegen den „jüdischen Journalismus“ im Theaterleben zu Felde zog.

Fortan strebte die Wiener Operette zunehmend in Richtung Spieloper – mit "Der Zigeunerbaron" (1885) als bis heute berühmtesten Beispiel – und nach Seriosität. Es traten in den Stücken plötzlich „Helden mit Vorbildcharakter“ fürs Publikum auf, wie der Barinkay im "Zigeunerbaron", Symon in "Der Bettelstudent" (1882) oder Conte Erminio in "Gasparone" (1884). Auch Johann Strauß’ "Simplicius" (1887) ist eine Art nationale Volksoper mit durchaus ernsthaften Momenten. Das war das Gegenteil dessen, was den individuellen Reiz der Gattung einmal ausgemacht hatte. In seinem Standardwerk "Die Oper" meint Oscar Bie (1914): „In der Wiener Operette […] lebt offenbar ein starker Trieb, sich zur komischen Oper aufzuschwingen; man konnte das bisher merken, an ihrem Streben nach edlerer musikalischer Form und leider auch an ihren aus Missverständnis ihrer Aufgabe hervorgegangenen Uebergriffen in das Gebiet der großen Oper.“ Dennoch waren die neu entstandenen Wiener Operetten der Epoche sehr erfolgreich und wurden auch im Ausland (bis hin nach Amerika) vielfach nachgespielt.
Die erste Glanzzeit der spezifisch Wienerischen Operette dauerte bis zur Jahrhundertwende, mit Richard Heubergers "Der Opernball" (1898) als Schlusspunkt. Nach dem sensationellen Erfolg von Franz Lehárs "Die lustige Witwe" (1905) wurde eine zweite Glanzzeit eingeläutet, in der fast alle deutschsprachigen Operetten dem Handlungsmodell der "Witwe" folgten, mit jeweils ähnlicher Figurenkonstellation, aber unterschiedlichen Kostümen. „Die Operette […] hat sich eigentlich überlebt, weil sie seit der "Lustigen Witwe" immer dieselben Bücher hat“, schrieb Karl Farkas.

Nach dem Ersten Weltkrieg begann für die Kunstform eine dezidiert neue Ära, auch im eher traditionsbewussten Wien; die dabei entstandenen Stücke nennt man oft Silberne Operetten (im Gegensatz zur sogenannten Goldenen Operetten des 19. Jahrhunderts), obwohl diese Terminologie ideologisch problematisch ist. Die Operette wurde zunehmend von der Revue, dem Kabarett und dem Kino verdrängt beziehungsweise ging kreative Mischformen mit diesen Genres ein. Jedoch konnten auch in den 1920er-Jahren und bis nach dem Zweiten Weltkrieg noch gigantische weltweite Operetten-Erfolge erzielt werden. Modernere Formen der Tanz- und Unterhaltungsmusik gewannen neben traditionellen Walzer-, Polka- oder Marsch-Motiven stark an Einfluss, etwa Shimmy und Foxtrott. Schlager begannen ihren Siegeszug nicht mehr unbedingt auf der Bühne, sondern konnten von den neueren Medien Grammophon, Hörfunk und später dem Tonfilm ausgehen und von dort aus auf die Bühne zurückwirken. Seit den 1930er-Jahren gab es originale Filmoperetten (zum Beispiel "Die drei von der Tankstelle") bei denen der Übergang zum Filmmusical (wie man es aus Hollywood kennt) fließend ist.

Kompositionen nach dem Ersten Weltkrieg vermieden oft die Bezeichnung Operette und die entsprechenden Werke erschienen als Singspiel, „musikalische Komödie“ oder „musikalisches Lustspiel“ auf dem Programm (zum Beispiel "Der Vetter aus Dingsda, Im weißen Rößl"). In den 1920er Jahren rückte der Operette zunehmend das Broadway- oder West End-Musical zur Seite, die für Wiener und Berliner Operettenkomponisten oft Vorbildcharakter hatten und deren typische Charakteristika in deutschsprachige Operetten eingebaut wurden (zum Beispiel in Emmerich Kálmáns Charleston-Operette "Die Herzogin von Chicago", 1928).

Franz Lehár hingegen näherte sich in seinen späteren Operetten immer mehr dem Genre der Oper, so ganz besonders in "Giuditta", seinem letzten Werk, das 1934 an der Wiener Staatsoper mit Jarmila Novotná und Richard Tauber unter Lehárs Leitung uraufgeführt wurde. Lehár war ein großer Bewunderer Puccinis – die Wertschätzung war gegenseitig –, der eine Operette für das Wiener Carltheater schreiben wollte. Infolge des Ersten Weltkriegs kam es aber nicht zur Uraufführung in Wien, das für Wien gedachte Werk "La rondine" erlebte seine Uraufführung in italienischer Sprache in der Oper von Monte Carlo.

Einige spätere Musicals waren moderne Weiterentwicklungen der Operette, zum Beispiel "My Fair Lady", und eigneten sich deshalb besonders zur Aufnahme in das Repertoire der deutschsprachigen Stadttheater. Die deutsche Fassung von "My Fair Lady" wurde von Robert Gilbert erstellt, der zuvor mit seinen Liedtexten zum "Weißen Rössl" große Bekanntheit erlangt hatte. Komponist Frederick Loewe wiederum war der Sohn eines österreichischen Operetten-Buffo und wuchs mit der Tradition der grotesk-sentimentalen Wiener- und Berliner Operette auf, was man allen seinen Werken anhört (speziell in den süßlichen Liebesliedern).

Die nach zirka 1920 geschaffenen deutschsprachigen Werke verzichteten oft auf die alten Walzer-, Polka- und Marschklänge, die Komponisten orientierten sich stattdessen an den neuen Unterhaltungsklängen aus den USA (Shimmy, Foxtrott, Charleston). So entstanden für Wiener Bühnen radikal moderne, transatlantische Stücke wie Emmerich Kálmáns "Die Bajadere" (mit „Fräulein, bitte woll’n Sie Shimmy tanzen“), oder Bruno Granichstaedtens "Der Orlow" und "Reclame". Konservative Wiener Operettenfans reagierten entsetzt auf die neuartigen Klänge. Als Gegenreaktion begann Erich Wolfgang Korngold eine Renaissance der Strauß-Operette, die in seinen Bearbeitungen (v. a. "Eine Nacht in Venedig", 1923) neu zur Diskussion gestellt und vor allem von der konservativen Wiener Presse (z. B. Neue Freie Presse) gefeiert wurden.

Diese walzerseligen Stücke (z. B. Korngolds Strauß-Pasticcio "Walzer aus Wien", 1930) konnten die Vorliebe eines Großteils des Publikums für moderne Klänge jedoch nicht ersetzen. So erzielte beispielsweise Ralph Benatzky noch 1936 mit der jazzigen Hollywood-Operette "Axel an der Himmelstür" im Theater an der Wien einen richtungsweisenden Erfolg (mit Max Hansen und Zarah Leander in den Hauptrollen). Auch Paul Abraham schrieb nach 1933 seine neuen Stücke für Wien, z. B. die frivole Fußball-Operette "Roxy und ihr Wunderteam", 1937 voller Blackwalks und Foxtrotts, 1938 verfilmt mit Rosy Barsony und Oscar Denes.

Nach dem Anschluss Österreichs 1938 verschwanden die nunmehr als „entartet“ abgestempelten Jazz-Werke von den Spielplänen der Wiener Theater; sie wurden ersetzt von sogenannten Klassikern des 19. Jahrhunderts und Kompositionen, die sie imitierten.

Als Novitäten-Gattung behielt die Wiener Operette über die letzten Werke von Robert Stolz ("Frühjahrsparade"), Ludwig Schmidseder ("Abschiedswalzer") und Gerhard Winkler in der zweiten Jahrhunderthälfte hinaus ungebrochene Kontinuität. Im Fernsehen wurde die Wiener Operette von Sängern wie Rudolf Schock oder Anneliese Rothenberger gefördert.
Bis in die 1970er-Jahre entstanden noch Operetten des Linzer Komponisten Igo Hofstetter, wie etwa "Roulette der Herzen", "Alles spricht von Charpillon" und "Schach dem Boss", die in mehrere Fremdsprachen übersetzt wurden und in verschiedenen Theatern mehrere Jahre auf den Spielplänen waren.

Auch in Berlin hatten die Pariser und Wiener Operettentitel Erfolg und bald machten sich lokale Komponisten wie Paul Lincke, Jean Gilbert und Walter Kollo daran, einen typisch berlinerischen Tonfall in der Operette einzuführen. Beispiele für die frühe „Berliner Operette“ sind Paul Linckes "Frau Luna" und "Im Reiche des Indra" (beide 1899), Walter Kollos "Drei alte Schachteln" (1917) und Jean Gilberts "Die keusche Susanne" (1910) oder "Die Kinokönigin" (1913). Kennzeichnend für den „Berliner Stil“ ist die Bevorzugung von zackiger Marschmusik (berühmtestes Beispiel ist Linckes „Das ist die Berliner Luft, Luft, Luft“).

Schon im 19. Jahrhundert dienten Berliner Theater wie das Woltersdorff-Theater der Wiener Operette für Tryout-Aufführungen. Nach dem Ende des Ersten Weltkriegs entwickelte sich Berlin zunehmend zum Zentrum der deutschsprachigen Operette, und viele Wiener Komponisten vergaben Uraufführungen ihrer Werke in die deutsche Hauptstadt (berühmtestes Beispiel: Lehárs "Das Land des Lächelns", 1929 mit Richard Tauber im Metropol-Theater uraufgeführt) oder siedelten ganz an die Spree um (Ralph Benatzky, Oscar Straus usw.). Mit Beginn der Zwanziger Jahre setzte sich in Berlin auch ein radikal neuer Ton für Operetten durch, stark orientiert an den synkopierten Tänzen aus den USA. Stärker als in Wien wurde der transatlantische Sound ein Markenzeichen der Berliner Operetten. Dabei entstanden Klassiker wie Eduard Künnekes "Der Vetter aus Dingsda" (1920) mit dem „Batavia Foxtrott“, Erik Charells am Broadway orientierte Revueoperetten "Casanova" (1928), "Drei Musketiere" (1929) und "Im weißen Rössl" (1930), für die Ralph Benatzky musikalisch verantwortlich war, sowie als krönender Höhepunkt Paul Abrahams wirbelwindartige Hits "Viktoria und ihr Husar" (1930), "Die Blume von Hawaii" (1931) sowie "Ball im Savoy" (1932). Die "Blume von Hawaii" war das erfolgreichste Bühnenwerk der Weimarer Ära.

Mit dem Aufkommen des Tonfilms Ende der 1920er Jahre entwickelte sich die fürs neue Medium geschaffene „Tonfilmoperette“ als eigenständiges Genre, besonders bei der Berliner Filmfirma UFA, als deren Hauptvertreter Werner Richard Heymann gelten kann, mit Film-Klassikern wie "Die Drei von der Tankstelle (1930)" oder "Der Kongreß tanzt" (1931, Regie: Erik Charell).

Neben den opulenten Produktionen für Bühne und Film entwickelte sich speziell in Berlin das Genre der satirisch angehauchten, intimen Kabarett-Operette, repräsentiert durch Stücke wie Mischa Spolianskys "Wie werde ich reich und glücklich?" (1930).

Nach 1933 wurde der internationale, an den USA ausgerichtete Jazz-Stil der deutschsprachigen Operette (in Berlin und Wien) ersetzt von einem – mit wenigen Ausnahmen – biederen Sound, kreiert von „arischen“ Komponisten wie Nico Dostal, Fred Raymond, Ludwig Schmidseder und Friedrich Schröder, die unter anderem Remakes oder sogar Plagiate der nunmehr verbotenen „jüdischen“ Werke schufen ("Saison in Salzburg" für "Im weißen Rössl", "Die ungarische Hochzeit" für "Gräfin Mariza" usw.). Eine Ausnahme stellen Ludwig Schmidseders Operetten dar, bei denen „entartete Tendenzen“, wie jazzige Rhythmen, in seiner Musik deutlich zu hören sind. Die betont unpolitische Haltung der Operette bewahrte ihren Werken noch nach dem Zweiten Weltkrieg eine jahrzehntelange Wirkung wie bei "Maske in Blau" (1937, als Ersatz für Ábraháms "Ball im Savoy") oder "Hochzeitsnacht im Paradies" (1942).
Ferner wurden von den Nationalsozialisten die Wiener Werke von Suppé, Strauß, Millöcker und Zeller auf die Bühne zurückgeholt, als Beispiele für „arische“ und nicht „entartete“ Operetten (meist ohne Nennung der jüdischen Textdichter). Die wertende Unterteilung in Goldene und Silberne Operette geht auf die Kulturpolitik der Nazis zurück.

Nach dem Zweiten Weltkrieg schufen in der DDR Komponisten wie Gerd Natschinski, Conny Odd, Herbert Kawan, Eberhard Schmidt, Gerhard Kneifel und Guido Masanetz neue Operetten (siehe Heiteres Musiktheater (DDR)).

In England sind die klanglich üppigen Savoy Operas von Gilbert und Sullivan beliebt geworden, und in Spanien erlebte die musikalisch schlankere Zarzuela nach der Wende zum 20. Jahrhundert eine Blütezeit, zum Beispiel von Enrique Granados oder Amadeo Vives.

Seit den 1910er-Jahren konnte sich auf der New Yorker Broadway-Bühne eine eigene Operettentradition etablieren, die etwa von Victor Herbert ("Babes in Toyland", 1903, "Mlle Modiste", 1905, "The Red Mill", 1906, "Naughty Marietta", 1910, "Sweethearts", 1913, "Eileen", 1917, "Orange Blossoms", 1922), Sigmund Romberg ("The Student Prince", 1924) und Rudolf Friml ("The Vagabond King", 1925) vertreten wurde und teils auch in Europa, speziell Großbritannien, sehr erfolgreich war. Auch wenn diese US-Operetten vielfach europäischen, vor allem Wiener Vorbildern folgten, behandelten sie doch oft Stoffe, die nicht von adligen Personen handelten und amerikanische Geschichten erzählten (wie die Kolonialoperette "Naughty Marietta" oder die in Kanada spielende Friml-Operette "Rose-Marie").

Auch wenn US-Komponisten vielfach Lieder im Walzertakt schrieben, war der Dreivierteltakt in Amerika nicht das kennzeichnende Merkmal der Operette wie in Wien. Stattdessen wurden speziell Marschnummern (wie „Stouthearted Men“) und Vaudeville-Nummern populär und zum Vorläufer und Vorbild späterer Musicalsongs von Jerome Kern oder George Gershwin. Deren Werke (wie "Show Boat", 1927) entstanden teils parallel zu den Broadwayoperetten und sind mit diesen inhaltlich und musikalisch eng verwandt. Auch die Musical Plays nach den Textbüchern von Oscar Hammerstein II wie "Oklahoma!" (1943) oder "The Sound of Music" (1959) werden bisweilen zur Operette gezählt.

Der Regisseur und Produzent Erik Charell konnte in München 1950 mit einer Revue-Version der Schweizer Operette "Feuerwerk" von Paul Burkhard noch einen Erfolg feiern. Darauf wurden die Uraufführungen im großen Rahmen seltener.

In der DDR versuchte man sich gleichermaßen von den bürgerlichen Stoffen der Operette und von der kommerziellen Produktionsweise des Musicals abzugrenzen, was zur Gattung des Heiteren Musiktheaters führte, in der von den 1950er-Jahren an zahlreiche neue Werke produziert und aufgeführt wurden. Internationalen Erfolg hatte etwa Gerd Natschinskis "Mein Freund Bunbury" (1964) nach Oscar Wilde.

In neuester Zeit sind wieder Operetten-Uraufführungen bzw. eine Annäherung des Musicals an die Operette zu beobachten. Darunter etwa "The Beastly Bombing: A Terrible Tale of Terrorists Tamed by the Tangles of True Love" (Los Angeles 2006) von Julian Nitzberg (Libretto) und Roger Neill (Musik), im Stil von Gilbert & Sullivans grotesken Polit-Satiren.

Nach der Machtübernahme der NSDAP 1933 erfuhr das Genre Operette eine radikale Umdefinierung. Fast alle an den USA orientierten Jazz- und Revueoperetten der 1920er Jahre wurden verboten, wegen des angeblich skandalösen Einflusses sogenannter „Niggermusik“, vor allem aber wegen der überwiegend jüdischen Autoren. Wegen seiner jüdischen Glaubenszugehörigkeit wurde speziell auch das Œuvre Jacques Offenbachs vollständig von deutschsprachigen Bühnen verbannt.

Da Jazzmusik und somit auch Jazz-Operetten unerwünscht waren, da als „typisch jüdisch“ angesehen, und da Frivolität und gesellschaftspolitische Aktualität ebenso unwillkommen waren, musste nach 1933 Ersatz für die vielen Erfolgsstücke der Weimarer Republik gefunden werden. Dabei gingen die Nazis auf drei verschiedenen Ebenen vor, die Entwicklungen der 20er Jahre aufgriffen, aber mit neuen Inhalten füllten. Wenn man von „Operette“ im Nationalsozialismus spricht, dann eigentlich von folgenden Varianten:


Da die Titel der 1920er Jahre und überhaupt die Unterhaltungstheaterkultur der Weimarer Republik als „entartet“ und als „Verwesungserscheinung“ betrachtet wurde, und weil selbst bei Lehárs Opern-Operetten "Land des Lächelns" und "Giuditta" die Textbücher von jüdischen Autoren wie Fritz Löhner-Beda („Ausgerechnet Bananen“) stammten, empfahl die Reichsdramaturgie die ‚klassischen‘ Walzer-Operetten des 19. Jahrhunderts, die fortan im Sprachgebrauch zu „Goldenen Operetten“ wurden, welche turmhoch über der „Silbernen Epoche“ des 20. Jahrhunderts thronen (eine Terminologie und Ideologie, die bis heute gebraucht wird). So erlebten plötzlich lange vergessene Werke wie "Prinz Methusalem", "Spitzentuch der Königin", "Tausendundeine Nacht", "Der Obersteiger", "Das Heiratsnest", "Der Landstreicher", "Der Fremdenführer", "Der Kellermeister", "Der arme Jonathan", "Das Pensionat" usw. Revivals. Sie wurden mit den besten damals verfügbaren Opernsängern und Orchestern eingespielt, darunter die Berliner und Wiener Philharmoniker, als moderne „Spielopern“, wie Hans Severus Ziegler in seinem Geleitwort zu "Reclams Operettenführer" 1939 darlegt:

Diese geschmacksbildende ‚Niveau-Hebung‘ der Operette in Richtung Singspiel war auch deshalb wichtig, weil Privattheater im Nationalsozialismus bald völlig verschwanden und Operetten entsprechend an staatlichen Bühnen gespielt wurden, mit Ensemblesängern, deren Talente auf anderen Gebieten lagen als bei den hochspezialisierten Operettenstars der 1920er Jahre.

Zum „Singspiel“ umfunktioniert, dominierten die Werke von Strauß und Zeitgenossen, aber auch etliche Lehár-Operetten fortan (bis heute) den Spielplan, teils unter Weglassung des Namens der Textdichter.

Auch die wenigen „arischen“ Komponisten, die vor 1933 Karriere im Jazzbereich gemacht hatten, wie Eduard Künneke, schrieben fortan opernhafte Werke für Opernsänger, etwa "Die große Sünderin", die 1935 an der Berliner Staatsoper Unter den Linden herauskam, mit der Wagner-Diva Tiana Lemnitz und dem Verdi-Tenor Helge Roswaenge in den Hauptrollen. Mit diesem Ansatz der ‚Veredelung‘ und ‚Veroperung‘ der Operette und dem Zerstören privater Theater zerstörte man nicht nur eine blühende Unterhaltungsindustrie in Deutschland, die vergleichbar war mit dem, was man heute noch vom Broadway oder Londoner West End kennt. Man vernichtete so die kommerzielle Kunstform Operette und machte sie zu einem Staatsakt, von Kammersängern vorgetragen und Staatsorchestern begleitet.

Die Spezialisierung, die man einst von Operettendarstellern forderte, gibt es nur noch selten. Die Fähigkeit zum Sprechen pointierter Dialoge fehlt manchen Opernsängern. Gesangsfächer wie die Soubrette, der Spieltenor oder Spielbass sind eher eine Ausnahme unter den professionellen Sängern. Heute reduziert sich das landauf, landab gespielte Operettenrepertoire im deutschsprachigen Raum (nunmehr an staatlich subventionierten Bühnen, nicht mehr an spezialisierten Privattheatern) im Kern auf wenig mehr als ein halbes Dutzend „klassischer“ Werke, in Österreich sind es etwas mehr. Amerikanische Operetten werden nur an vereinzelten Bühnen wie der Ohio Light Opera gepflegt. Lediglich in Frankreich wird – speziell Offenbachs Œuvre – regelmäßig an größeren Bühnen gegeben, teils mit sehr prominenten (Opern)-Besetzungen.

Ehemals überwiegend der Operette gewidmete Theater wie das Staatstheater am Gärtnerplatz München oder die Volksoper Wien haben sich mittlerweile mehr der Oper zugewandt, obwohl Operetten an diesen Häusern weiterhin einen wichtigen Bestandteil des Repertoires ausmachen. Außer in Dresden mit der Staatsoperette Dresden gibt es neuerdings auch in Hamburg mit dem Engelsaal ein auf Operetten spezialisiertes Theater. Es sind die einzigen selbstständigen Theater dieser Art in Deutschland. Die "Musikalische Komödie" in Leipzig als Bestandteil der Oper Leipzig mit eigenem Ensemble widmet sich ebenfalls überwiegend der Operette. In Baden bei Wien laufen regelmäßig Operetten, vielfach auch Titel jenseits des Standardrepertoires, in konventionellen Inszenierungen, die insbesondere seit der Intendanz von Robert Herzl weitum guten Ruf genießen. Besonders im osteuropäischen Ausland existieren viele reine Operettentheater, die das Repertoire pflegen, so z. B. das Operettenhaus in Budapest, das mit seinen Produktionen auch auf Tournee geht, beispielsweise in Deutschland.

Nach wie vor gibt es auf Operette spezialisierte Festivals: Einige wegen ihrer enormen Medienpräsenz bedeutende Operettenfestivals sind in Österreich die Seefestspiele Mörbisch (unter der Intendanz von Dagmar Schellenberger), das Lehár Festival Bad Ischl und in Deutschland die Elblandfestspiele Wittenberge (Gründungsintendant: Heiko Reissig), die alljährlich in den Sommermonaten stattfinden. 

Von 2011 bis 2015 veranstaltete die Staatsoperette Dresden alljährlich im Frühjahr das Johann Strauss Festival Dresden, in dessen Mittelpunkt das mit seinen Raritäten weltweit einmalige Johann-Strauss-Repertoire des Hauses stand. Ergänzt wurde das Programm jeweils um aktuelle Höhepunkte aus dem Spielplan, um den Bogen von den Anfängen des Genres bis in die Gegenwart hinein zu öffnen und verschiedene Präsentationsformen zu zeigen. Durch den Umzug in die neue Spielstätte bedingt, ist es für 2016 und 2017 ausgesetzt worden.

Die Festspiele im Schlossgarten (vormals Schlossgartenfestspiele Neustrelitz) in Neustrelitz sind die größten Operettenfestspiele Deutschlands.

Ferner bietet das US-amerikanische Ohio Light Opera Festival jeden Sommer sechs Operetten, wovon die Hälfte Ausgrabungen vergessener Werke sind. Durch Fernsehübertragungen, CD- und DVD-Veröffentlichungen erlangten besonders die Festspiele in Mörbisch und Ohio überregionale Beachtung.

Zahlreich sind auch die Laienbühnen, die sich der Operette widmen. In der Schweiz gibt es etwa eine "Musiktheatervereinigung" als Zusammenschluss dreier Theatergesellschaften.





</doc>
<doc id="13662" url="https://de.wikipedia.org/wiki?curid=13662" title="Der Bettelstudent">
Der Bettelstudent

Der Bettelstudent ist eine Operette in drei Akten von Carl Millöcker. Das Libretto verfassten gemeinsam F. Zell und Richard Genée. Es basiert auf dem Stück "Les Noces de Fernande (Fernandos Hochzeit)" von Victorien Sardou (Musik: Louis Deffès, Théâtre de l’Opéra-Comique Salle Favart, 19. November 1878) und weiteren Motiven aus Edward Bulwer-Lyttons fünfaktigen „romantischem Melodrama“ "The Lady of Lyons; or, Love and Pride" (erstmals aufgeführt am Londoner Covent Garden Theater am 15. Februar 1838). Die Uraufführung von Millöckers Operette fand am 6. Dezember 1882 im Theater an der Wien statt.

Zwei Flöten, zwei Oboen, zwei Klarinetten, zwei Fagotte, vier Hörner, zwei Trompeten, drei Posaunen, großes Schlagwerk und Streicher. Für die Bühnenmusik werden Musikanten gebraucht, die eine Stadtkapelle spielen.

Die Operette spielt im Jahr 1704 in Krakau. Damals war August der Starke zugleich Kurfürst von Sachsen und König von Polen.

Gefängnishof

Im Kerker der Zitadelle von Krakau sind viele Polen inhaftiert, die gegen die sächsische Herrschaft aufbegehrt haben. Eine Gruppe Frauen fleht den Gefängniswärter Enterich an, ihre Männer besuchen zu dürfen. Nachdem Enterich die mitgebrachten Speisen und Getränke konfisziert hat, lässt er ein paar Gefangene zu ihren Frauen in den Hof. Das Wiedersehen wird jedoch abrupt beendet, als der aufgeblasene Gouverneur, Oberst Ollendorf, mit ein paar Offizieren eintrifft. Ollendorf kocht vor Wut, weil ihn am Vortag bei einem Ball die polnische Komtesse Laura vor allen Leuten mit dem Fächer ins Gesicht schlug. Dabei hatte er die junge Schöne ja nur auf die Schulter geküsst. Jetzt will er sich für dieses Verhalten rächen. Aus einem abgefangenen Brief von Lauras Mutter, der Gräfin Nowalska, weiß er, dass für sie nur ein polnischer Fürst als Schwiegersohn in Frage kommt. Ollendorf beabsichtigt daher, die Gräfin und ihre Tochter vor der Krakauer Gesellschaft zu blamieren. Dazu braucht er zwei Gefangene. Einer sollte überzeugend einen polnischen Fürsten spielen können und der andere dessen Sekretär.

Enterich führt seinem Chef die beiden befreundeten Studenten Symon Rymanowicz und Jan Janicki vor. Ollendorf verspricht ihnen die Freiheit und eine zusätzliche Belohnung in Geld, wenn sie die ihnen zugedachten Rollen spielten. Beide sind einverstanden.

Verwandlung – Platz in Krakau

Bei der Krakauer Frühjahrsmesse herrscht eine ausgelassene Stimmung. Auch Oberst Ollendorf ist mit seinen Offizieren da. „Zufällig“ trifft das Grüppchen auf die Gräfin Nowalska und ihre beiden Töchter. Ollendorf verwickelt die Damen in ein Gespräch und bemerkt so ganz nebenbei, ein gut begüterter polnischer Fürst namens Wybicki sei in Krakau eingetroffen, wo er sich nach einer Braut umschauen wolle. Damit hat der Oberst sofort die Neugier der Gräfin geweckt. Wenige Minuten später macht er die Damen mit dem vermeintlichen Fürsten und dessen Sekretär bekannt. Für Laura und Symon ist es Liebe auf den ersten Blick, und Lauras Schwester Bronislawa fühlt sich zu Jan Janicki hingezogen, obwohl dieser nicht von Adel ist. Im Geiste sieht sich die Gräfin schon als Schwiegermutter eines reichen Fürsten, mit dessen Hilfe sie ihre angeschlagenen Finanzen sanieren kann. Als bald darauf die beiden Paare ihre Verlobungen bekannt geben, schwelgt die Gräfin im Glück.

Saal im Palast der Gräfin Nowalska

Heute soll eine Doppelhochzeit gefeiert werden. Symon wird von Gewissensbissen geplagt. Er liebt seine Verlobte aufrichtig und macht sich Gedanken darüber, wie er sie am besten mit der Wahrheit konfrontieren könne. Ihr offen ins Gesicht zu sagen, dass er nur ein Bettelstudent sei, wagt er nicht. Er entschließt sich aber, ihr schriftlich die Wahrheit zu gestehen. Der Brief wird geschrieben und auf den Weg gebracht.

Allmählich treffen die Hochzeitsgäste ein. Alles, was in Krakau Rang und Namen hat, will das Ereignis nicht verpassen. Nach der Trauungszeremonie finden sich aber noch viele ungebetene Gäste ein: Enterich kommt mit den in Lumpen gekleideten Gefangenen und begrüßt mit ihnen den Bräutigam, einen „Bettelstudenten“. Laura und ihre Mutter echauffieren sich, der Skandal ist perfekt. Symon war der Auffassung, seine Braut habe gewusst, wer er sei und fragt sie, ob sie seinen Brief denn nicht gelesen habe. Da gesteht Ollendorf, dass dieser ihm zugespielt worden sei und seine Empfängerin nie erreicht habe. Während sich der Oberst in Schadenfreude suhlt, bringt Enterich die Häftlinge – außer Symon und Jan – in den Kerker zurück. Der „Bettelstudent“ wird des Hauses verwiesen.

Im Schlossgarten

Symon fühlt sich vogelfrei und zöge am liebsten weit weg, dorthin, wo ihn keiner kennt. Jan versucht, ihn zum Bleiben zu bewegen, weil er ihn für seine patriotischen Pläne noch brauche. Er gibt jetzt unter dem Siegel der Verschwiegenheit seine wahre Identität preis: Er sei kein Student, sondern Graf Opalinski. Seine geheime Mission sei, eine Verschwörung gegen die Besatzer vorzubereiten. Ihm fehlten nur noch 200.000 Taler, um den italienischen Kommandanten der Krakauer Zitadelle bestechen zu können. Für diese Summe sei dieser geneigt, sich mit den Rebellen einzulassen.

Inzwischen ist Ollendorf zugetragen worden, wer Jan Janicki in Wirklichkeit ist und was er vorhat. Vom König hat er den Befehl erhalten, ihn mit 200.000 Talern zu bestechen, damit er den Aufenthalt des polnischen Herzogs Kasimir verrate, um den Anführer der Aufständischen endlich dingfest machen zu können. Der vermeintliche Student geht zum Schein darauf ein, kassiert die Belohnung und behauptet (mit dessen Einverständnis), sein Freund Symon sei der gesuchte Herzog. Symon wird verhaftet und abgeführt.

Laura ist inzwischen bewusst geworden, dass sie Symon aufrichtig liebt. Als sie hört, dass er hingerichtet werden soll, fleht sie um sein Leben. Plötzlich ertönen Kanonenschüsse. Nun wird allen klar, dass die Revolte der Polen gelungen ist. Die Krakauer Zitadelle ist wieder in ihrer Hand. Ollendorf und seine Offiziere werden entwaffnet und gefangen genommen. Symon wird vom neuen König Stanislaus Leszczynski zum Dank für seinen patriotischen Einsatz in den Adelsstand erhoben und erhält die Grafenwürde. Nun steht einer Vermählung mit der polnischen Komtesse nichts mehr im Wege. Auch Lauras Schwester geht nicht leer aus, wird sie sich in Bälde mit dem Titel Gräfin Opalinski schmücken dürfen.

Es ist in erster Linie Millöckers Musik zu verdanken, dass dieses Stück zu einer der beliebtesten und erfolgreichsten deutschsprachigen Operetten wurde. Besonders bekannt ist u. a. Ollendorfs beleidigter Gesang "Ach ich hab sie ja nur auf die Schulter geküsst" und das Duett "Mit der Liebe Fessel binden".

Zwischen 1896 und 1921 war sie mit 4940 deutschsprachigen Aufführungen unter den meistgespielten Operetten an vierter Stelle. In der Saison 1990/91 gab es zwölf Inszenierungen in Deutschland, Österreich und der Schweiz mit insgesamt rund 180.000 Besuchern.







</doc>
<doc id="13663" url="https://de.wikipedia.org/wiki?curid=13663" title="Orpheus in der Unterwelt">
Orpheus in der Unterwelt

Orpheus in der Unterwelt (französisch "Orphée aux enfers") ist eine Opéra bouffe in zwei Akten bzw. vier Bildern von Ludovic Halévy und Hector Crémieux. Die Musik komponierte Jacques Offenbach, die Uraufführung fand am 21. Oktober 1858 in Offenbachs Théâtre des Bouffes-Parisiens in Paris statt. Eine erweiterte Fassung als Opéra Féerie in 4 Akten und 12 Bildern erarbeiteten der Komponist, Crémieux und Halévy 1874 für eine Neuinszenierung der Oper in dem von Offenbach inzwischen geleiteten Théâtre de la Gaîté. Die Erstaufführung der erweiterten Fassung fand am 7. Februar 1874 in Paris statt. Für diese Fassung wurde das Werk von ca. 90 Minuten Spielzeit auf runde vier Stunden verlängert und der Bestand der Musiknummern von 16 auf 30 ergänzt.

Nachdem Offenbach für sein Theater wegen der Lizenzbestimmungen drei Jahre lang Einakter komponiert hatte, war "Orpheus" das erste abendfüllende Werk und ein sensationeller Erfolg. Die Handlung persifliert die griechische Sage von Orpheus und Eurydike. Mit den Göttern des Olymp, die den Hades besuchen, um sich zu amüsieren, wird gleichzeitig die Doppelmoral der besseren Gesellschaft des Zweiten Kaiserreichs karikiert. Zur Zeit der Uraufführung konnten sich viele Personen der Pariser Gesellschaft in dem Stück wiedererkennen. Die griechische Mythologie war ein beliebtes Gesprächsthema der feinen Leute, und Offenbach nahm mit seinem Orpheus den Antikenkult gehörig auf die Schippe. Selbst der regierende Kaiser Napoléon III. blieb nicht verschont. Er konnte sich in der Figur des liebestollen obersten Gottes Jupiter wiederfinden. Die Oper gefiel dem Kaiser; er nahm Offenbach die Anspielungen anscheinend nicht übel und applaudierte laut.

Das bekannteste Musikstück ist der sogenannte "Höllen-Cancan" (im Original allerdings als „Galop infernal“ bezeichnet) im zweiten Akt, ein Gassenhauer, der auch heute noch bekannt ist und häufig auch separat aufgeführt wird.

Daneben finden sich in dem Stück zahlreiche musikalische Zitate: die französische Nationalhymne, die Arie "Che farò senz’ Euridice" (Ach, ich habe sie verloren) aus Orfeo ed Euridice von Christoph Willibald Gluck, und ein Fugenthema von Johann Sebastian Bach.

Die 1860 uraufgeführte Wiener Bearbeitung des "Orpheus" stammt vermutlich von Johann Nestroy, der auch die Rolle des Jupiter übernahm. Bei dieser wurde auch die Ouvertüre uraufgeführt, die Carl Binder komponierte.

Das Werk ist neben den Gesangssolisten besetzt mit vierstimmigem Chor und einem Orchester mit 2 Flöten (2. mit Piccolo), 1 Oboe, 2 Klarinetten, 1 Fagott, 2 Hörnern, 2 Pistons, 1 Posaune, Pauken, Schlagzeug und Streichern (Violinen 1, Violinen 2, Violen, Violoncelli, Kontrabässe).

Die Handlung spielt auf der Erde, bei Theben im antiken Griechenland.

Das Ehepaar Orpheus und Eurydike hat sich auseinandergelebt. Der Musiklehrer und Geiger Orpheus betrügt seine Frau mit der Nymphe Chloé. Er hätte sich längst von seiner ungeliebten Gattin getrennt, wäre da nicht die Öffentliche Meinung. Eurydike, die ein gelangweiltes Leben führt, weiß das, und es stört sie nicht weiter. Auch sie hat einen Geliebten, den Schäfer und Imker Aristäus. Eurydike weiß jedoch nicht, dass ihr Liebhaber Aristäus tatsächlich Pluto, der Herr der Unterwelt, ist. Pluto will seine Geliebte in die Unterwelt entführen und wartet auf einen günstigen Zeitpunkt. Nach einem heftigen Streit zwischen den Eheleuten sieht Pluto seine Zeit gekommen. Er beißt Eurydike in den Hals, und dieser Kuss des Todes liefert ihm Eurydike aus.

Als Eurydike wieder zu sich kommt, schreibt sie gemeinsam mit Pluto einen „Abschiedsbrief“ an Orpheus, ihren Ehemann:

Als Orpheus ihre Nachricht liest, ist er erfreut. Er denkt, endlich frei zu sein von seiner Frau, und will die gute Nachricht sofort seiner Geliebten überbringen. Doch da tritt ihm die Öffentliche Meinung in den Weg und fordert ihn auf, seine Ehefrau von Jupiter, dem obersten Gott, zurückzufordern. Wieder kann sich die Öffentliche Meinung durchsetzen, und sie begleitet Orpheus hinauf auf den Olymp.

Auf dem Götterberg Olymp.

Auch bei den Göttern herrscht Langeweile und Überdruss. Jupiter vergnügt sich ungeniert mit jungen Frauen. Diana ist traurig, weil sie den schönen Sterblichen Aktäon bei ihren Aufenthalten unten auf Erden nicht mehr gefunden hat. Juno, die Gemahlin Jupiters, macht ihrem Göttergatten eine Szene. Auf Erden sei eine wunderschöne Frau von einem Gott entführt worden. Jupiter streitet ab, etwas mit dieser Entführung zu tun zu haben.

Da kommt Merkur, der Götterbote, mit der Nachricht, Pluto sei eben von einem Aufenthalt auf Erden mit einer wunderschönen Frau namens Eurydike in die Unterwelt zurückgekehrt. Jupiter ist erfreut, ist er doch durch diese Nachricht vorerst der Vorwürfe enthoben. Um seiner Unschuld Nachdruck zu verleihen, zitiert er Pluto aus der Unterwelt auf den Olymp. Pluto erscheint vor dem obersten Gott, doch er leugnet die Entführung.

Da erscheint Orpheus zusammen mit der Öffentlichen Meinung und fordert seine Frau zurück. Jupiter beschließt, die Sache in der Unterwelt genauer zu untersuchen. Er will Eurydike aus der Unterwelt holen, aber nicht für Orpheus, sondern für sich selbst. Die gesamte Götterschar folgt ihm in Plutos Höllenreich.

Unterwelt, in Plutos Boudoir.

Hier hält Pluto die entführte Eurydike versteckt. Bewacht wird sie von Hans Styx, dem stets betrunkenen Diener Plutos. Hans Styx umwirbt die Schöne und erzählt ihr von seiner Zeit in Reichtum und Pracht als Prinz von Arkadien. Doch Eurydike lässt das kalt.

Sie sehnt sich zurück zu ihrem Mann auf Erden. Der Reiz des Abenteuers ist bereits verblasst. Die vom Olymp in der Unterwelt eingetroffenen Götter können das Versteck der Eurydike zunächst nicht finden. Doch Jupiter ist misstrauisch. In Gestalt einer Fliege kommt er durchs Schlüsselloch und entdeckt Eurydike. Er scharwenzelt um sie herum, gibt sich als oberster Gott zu erkennen und verspricht ihr, sie zu befreien und mit auf den Olymp zu nehmen.

Pluto gibt ein Höllenfest. Es wird getanzt und getrunken.

Jupiter erntet allgemeinen Beifall mit einem Menuett, das sich alsbald zu einem wilden Cancan steigert. Eurydike ist als Bacchantin auf dem Fest.
Abermals wird die göttliche Gesellschaft von den Sterblichen gestört. Wieder fordert Orpheus in Begleitung der Öffentlichen Meinung von Jupiter seine Frau zurück. Jupiter gibt dem Wunsch nach, aber er stellt eine Bedingung: wenn Orpheus vor Eurydike in die Oberwelt hinaufsteige, dürfe er sich nicht nach seiner Gattin umwenden.

So beginnt der Marsch in Richtung Oberwelt: die Öffentliche Meinung, dann Orpheus und Eurydike, von Hans Styx geführt. Doch als sie das Tor erreichen, schleudert Jupiter einen Blitz. Orpheus dreht sich erschrocken herum und hat damit seine Frau verloren.

Aber auch Pluto soll Eurydike nicht haben, und so bestimmt Jupiter:

In französischer Sprache:
In deutscher Sprache:

"Das Gift im Lift – warum Orpheus ganz nach unten fuhr". Kinderoperette von Kay Link nach Jacques Offenbachs "Orpheus in der Unterwelt." Uraufführung: 18. März 2012. Auftragswerk der "Bayer.Kultur", Leverkusen, erschienen im Bühnenverlag Boosey & Hawkes.

Bereits 1909 wurde die Oper unter demselben Titel verfilmt.
Im Jahr 1974 wurde die Operette als "Orpheus in der Unterwelt" von der DEFA als Musik-Komödie verfilmt. Mitwirkende waren Wolfgang Greese und Dorit Gäbler in den Hauptrollen. Weiterhin wirkten Schauspieler wie Rolf Hoppe, Fred Delmare und Gerry Wolff mit. Die Regie übernahm Horst Bonnet.
Die Gesangseinlagen wurden zum Teil von den Schauspielern selbst gesungen. Unterstützt wurden sie vom Chor und von Mitgliedern der Deutschen Staatsoper Berlin, vom Ballett der Komischen Oper Berlin und des Metropol-Theaters sowie dem DEFA-Sinfonieorchester unter der Leitung von Robert Hanell.




</doc>
<doc id="13664" url="https://de.wikipedia.org/wiki?curid=13664" title="Boccaccio (Suppé)">
Boccaccio (Suppé)

„Boccaccio, oder Der Prinz von Palermo“ ist eine komische Oper bzw. Operette in drei Akten von Franz von Suppé. Das Libretto schrieben Camillo Walzel alias Friedrich Zell und Richard Genée. Das Werk wurde am 1. Februar 1879 am Carltheater in Wien uraufgeführt und gilt als das erfolgreichste Bühnenwerk des Komponisten. Es handelt von Giovanni Boccaccio, dem berühmten Dichter des "Decamerone", der hier selbst Held einer Liebesgeschichte wird.

Zwei Flöten, zwei Oboen, zwei Klarinetten, zwei Fagotte, vier Hörner, zwei Trompeten, drei Posaunen, großes Schlagwerk und Streicher

Bild: Platz mit Kirche und dem Hause Scalzas

Ausgelassen feiert das Volk von Florenz das Fest des Schutzpatrons der Stadt auf den Straßen und Plätzen. Vor der Kirche preist ein Buchhändler die neuesten Novellen Giovanni Boccaccios an, die das lockere Leben der Florentiner Frauen zum Gegenstand haben. Darüber sind die Ehemänner sehr erbost, glauben sie doch fest an die Treue ihrer Gattinnen. Sie warten nur auf die Gelegenheit, des Dichters habhaft zu werden, um ihn verprügeln zu können. Drei ehrbare Bürger, der Fassbinder Lotteringhi, der Gewürzkrämer Lambertuccio und der Barbier Scalza sind die Wortführer. Letzterer ist gerade von einer Reise in seine Heimatstadt zurückgekehrt, sehr zur Überraschung seiner Frau Beatrice, die ihn noch in der Fremde wähnte. Sie hat gerade Besuch von dem Studenten Leonetto, der sie sehr verehrt. Ihm hat sich dessen Freund Boccaccio angeschlossen. Beatrice gaukelt nun ihrem Mann vor, zwei sich streitende Studenten hätten sich gewaltsam Einlass in ihr Haus verschafft. Um Beatrices Bericht glaubhaft erscheinen zu lassen, kämpfen die „Eindringlinge“ zum Schein mit ihren Degen und beschimpfen sich wüst. Scalza fürchtet, in den Streit hineingezogen zu werden, und ergreift die Flucht.

Boccaccio ist in Fiametta, die Pflegetochter der Lambertuccios, verliebt. Die Schöne erwidert seine Liebe, ohne zu wissen, dass er der berühmte Dichter ist. Er macht sich auf die Suche nach ihr. Dabei trifft er auf Pietro, einen Prinzen aus Palermo. Der Herzog von Palermo möchte ihm gerne seine nichteheliche Tochter, die in Florenz lebt, zur Frau geben, und hat ihn deshalb zur Brautwerbung nach dort entsandt. Als Pietro erkennt, dass der junge Mann der von ihm so hoch geschätzte Dichter Boccaccio ist, will er gleich sein Schüler werden.

Die Ehemänner von Florenz glauben, in dem Prinzen den ihnen verhassten Dichter erkannt zu haben, und verabreichen ihm eine Tracht Prügel. Als sie merken, dass sie sich geirrt haben, machen sie ihrem Ärger Luft, indem sie sich auf den Buchhändlerkarren stürzen und ihn umkippen. Einem herumstehenden Bettler befehlen sie, die Schriften anzuzünden. Der Bettler ist aber kein anderer als Boccaccio selbst, der Verkleidungen liebt. Notgedrungen muss er seine eigenen Werke in Flammen setzen.

Bild: Platz vor den Häusern des Fassbinders und des Gewürzkrämers

Pietro hat ein Auge auf Isabella, die schöne Gattin des Fassbinders, geworfen und bringt ihr ein Ständchen dar, während Boccaccio Fiametta den Hof macht. Leonetto ist dazu ausersehen worden, sich einstweilen um die ältliche Petronella zu kümmern. Boccaccio und Pietro nutzen die Gunst der Stunde, solange die Hausherren in der Schänke zechen. Als Lotteringhi unerwartet nach Hause kommt, versteckt sich Pietro in einem Fass. Trotzdem wird er sofort von dem Ankommenden erkannt. Isabella ist jedoch um keine Ausrede verlegen; sie behauptet einfach, der Fremde sei ein Kunde und wolle die Qualität des Fasses genau prüfen.

Boccaccio hat sich als Bauernbursche verkleidet und dient sich dem gleichfalls heimkommenden Lambertuccio als Olivenpflücker an. Dabei macht er dem Leichtgläubigen weis, der Olivenbaum sei verhext. Von ihm aus sähe man nur sich küssende Pärchen. So kommt es, dass Lambertuccio seine Nachbarin Isabella mit dem Prinzen, seine Pflegetochter mit Boccaccio und – zu seinem großen Erstaunen – seine eigene Frau mit Leonetto traut vereint sieht. Plötzlich taucht Scalza auf und bereitet Boccaccios Schwindel ein jähes Ende. Er berichtet Lambertuccio, der junge Mann, den er auf seinem Anwesen sehe, sei tatsächlich der vermaledeite Dichter.

Auf der Jagd nach Boccaccio fällt den drei ehrsamen Florentiner Bürgern erneut ein Unschuldiger in die Hände: ein vom Herzog Beauftragter, der Fiometta, seine leibliche Tochter, abholen soll. Boccaccio, Pietro und Leonetto verhüllen rasch ihre Gesichter mit Teufelsmasken und kommen so ungeschoren davon.

Bild: Schlosspark mit Gartenterrasse

Der Abgesandte war kein Geringerer als der Herzog selbst. Weder Pietro noch Fiametta ist glücklich darüber, dass sie der Herzog füreinander bestimmt hat. Pietros Herz schlägt immer noch für Isabella, und Fiametta geht der Fremde, den sie für einen Studenten hält, nicht aus dem Kopf. Die Ehemänner von Florenz wollen den Herzog ersuchen, Boccaccio des Landes zu verweisen, lassen aber davon ab, als sie erfahren, dass der Dichter des Herzogs volle Gunst genieße. Nach der Aufführung einer von Boccaccio verfassten Commedia dell’arte wird der Dichter zum Professor an der Florentiner Universität ernannt und darf seine geliebte Fiametta bald zum Traualtar führen.

Der Musik gelingt es, den komödiantischen Übermut des Textes brillant umzusetzen. Sie ist voller Einfälle, glänzend orchestriert und sehr eingängig. Der Abstand zwischen Operette und komischer Oper ist an vielen Stellen aufgehoben. Musikalische Höhepunkte sind:




</doc>
<doc id="13665" url="https://de.wikipedia.org/wiki?curid=13665" title="Madame de Pompadour">
Madame de Pompadour

Jeanne-Antoinette Poisson, dame Le Normant d’Étiolles, marquise (Markgräfin) de Pompadour, duchesse (Herzogin) de Menars (* 29. Dezember 1721 in Paris; † 15. April 1764 in Versailles), kurz Madame de Pompadour, war eine Mätresse des französischen Königs Ludwig XV.

Jeanne-Antoinette Poisson wurde in Paris als Tochter von François Poisson (1684–1754) geboren, eines wohlhabenden bürgerlichen Heereslieferanten und Verwalters bei den Brüdern Pâris. Ihr leiblicher Vater war jedoch wahrscheinlich der reiche Bankier und Hauptsteuerpächter Charles François Paul Le Normant de Tournehem (1684–1751). Dieser übernahm, nachdem Poisson infolge eines Skandals im Jahre 1725 ins Exil verbannt worden war, die Position ihres Vormundes und nahm sie zusammen mit ihrer Mutter, Louise-Madeleine de La Motte (1700–1745), und ihrem Bruder, Abel-François Poisson de Vandières (1727–1781), bei sich auf.

Der Vater François Poisson wurde als jüngstes von neun Kindern eines Webers in Provenchère unweit von Langres geboren, ihre Mutter entstammte demselben Dorf. Er trat in den Dienst der Pariser Finanziers Jean Pâris de Montmartel (1690–1766) und Joseph Pâris-Duverney (1684–1770). Zu Wohlstand gekommen, ehelichte er am 29. Juli 1715 Anne-Geneviève-Gabrielle de Carlier de Roquaincourt (1695–1718). Diese Verbindung blieb kinderlos und seine Gattin starb im Jahre 1718. Noch im selben Jahre, am 6. Oktober 1718 heiratete Poisson erneut, seine zweite Frau war eben Louise Madeleine de la Motte. Im vierten Ehejahr gebar seine zweite Frau am 30. Dezember 1721 eine Tochter. Da ihr Ehemann neun Monate zuvor für längere Zeit in der Provence weilte, wurden über den leiblichen Vater von Jeanne-Antoinette Poisson zwei Namen genannt, Jean Pâris de Montmartel und Charles François Paul Le Normant de Tournehem.

François Poisson kam in den Strudel der Spekulationsblase von John Law of Lauriston und wurde am 23. April 1727 zur Rückzahlung von 232.430 Livres und einer Bestrafung verurteilt, doch er zog die Flucht in das Ausland vor. Seine Tochter wurde in das Ursulinenkloster von Poissy, "couvent des Ursulines de Poissy", gesandt.

Jeanne-Antoinette Poisson war eine regelmäßige Besucherin des samstäglichen Gesprächskreises im Club de l’Entresol, welcher von Pierre-Joseph Alary (1689–1770) und Charles Irénée Castel de Saint-Pierre gegründet worden war und von 1720 (bzw. 1724) bis 1731 in der Hochparterrewohnung am Place Vendôme in Paris von Charles-Jean-François Hénault (1685–1770) stattfand.

Am 9. März 1741 wurde Jeanne-Antoinette mit dem reichen Untersteuerpächter Charles-Guillaume Le Normant, seigneur d’Étiolles (1717–1799), dem Neffen ihres Vormundes, in der Kirche St. Eustache getraut. Von ihm bekam sie einen Sohn, der jedoch schon ein Jahr nach seiner Geburt starb, sowie die Tochter Alexandrine-Jeanne, genannt "Fanfan", die am 10. August 1744 zur Welt kam, jedoch ebenfalls früh, mit knapp 10 Jahren, starb.

Es ist historisch belegt, dass ihr im Alter von neun Jahren prophezeit worden war, dass sie eines Tages die Mätresse von Ludwig XV. sein werde. Der betreffenden Wahrsagerin, einer "Madame Lebon", setzte sie später eine Rente von 600 Livres aus.

Seitdem versuchte zuerst ihre Mutter und dann sie selber die Aufmerksamkeit Ludwigs XV. zu erregen. So ging sie, wenn sie sich in Étiolles aufhielt, in den nahegelegenen Wald von Sénart, wo sich die königliche Jagdgesellschaft zu treffen pflegte. Dies nützte jedoch wenig, da die damalige Mätresse Marie-Anne de Mailly-Nesle, Marquise de La Tournelle und Duchesse de Châteauroux (1717–1744), ihr verbot, weiterhin bei den Jagden des Königs zu erscheinen. Deshalb blieb ihr nichts anderes übrig, als den Tod der königlichen Mätresse abzuwarten, der schließlich am 8. Dezember 1744 eintrat.

Der Kardinal de Bernis schlug Ludwig XV. Jeanne-Antoinette Poisson vor, die ihm während der Jagden aufgefallen war, und lud sie zu einem Maskenball, am 28. Februar 1745, anlässlich der Hochzeit des Kronprinzen Ludwig Ferdinand ein, wo sie und der König sich trafen. Er machte sie nicht nur zu seiner offiziellen Mätresse (frz. "maîtresse en titre"), der ersten Bürgerlichen mit diesem Status am französischen Hof überhaupt, sondern erhob sie im Juli desselben Jahres auch zur Marquise de Pompadour mit Landsitz und eigenem Wappen. Am 14. September 1745 fand ihre offizielle Vorstellung am Hof von Versailles statt.

Obwohl ihr vertrauter Verkehr mit dem König nur bis 1751 dauerte, behielt sie die Stellung als offizielle Mätresse bis zu ihrem Tod 1764. Dies ist vor allem auf ihr diplomatisches Geschick zurückzuführen. Sie machte nicht, wie andere königliche Geliebte, den Fehler, sich die Königin zur Feindin zu machen, sondern tat alles, sie nicht zu brüskieren und ihr gefällig zu sein. Sie brachte sie sogar dazu, sie zu ihrer Hofdame und zur Herzogin von Menars zu ernennen (1756).

Zum anderen knüpfte sie am Hof ein Netz von Verbündeten, die ihre Stellung absicherten. Zudem verstand sie den König nicht nur in intimer Hinsicht an sich zu binden, sondern auch sein Unterhaltungsbedürfnis zu befriedigen und sich damit unentbehrlich zu machen. So sang sie für ihn mit prominenten Künstlern aus Paris geistliche Konzerte und Motetten, um seine Neugier für das Theater zu wecken. Schließlich erfüllte der König ihr den Wunsch nach einem kleinen Theater, welches „Theater des kleinen Appartements“ genannt wurde. Die Einweihung fand am 17. Januar 1747 mit dem Stück "Tartuffe" von Molière statt. Als Direktor wählte die Mätresse den Herzog de La Vallière aus, der der beste Sachwalter der französischen Komödie war. Die Aufführungen waren begehrt, doch das Publikum war klein und wurde sorgfältig von der Mätresse ausgewählt.

Sie brachte den König dazu, den Staatssekretär Jean-Frédéric Phélypeaux, Graf de Maurepas, 1749 nach 15 Jahren zu entlassen, obwohl dieser sich für unersetzlich hielt. Dies erreichte sie, indem sie dem König berichtete, dass Maurepas sie vergiften wolle.
Außerdem nutzte Madame de Pompadour ihre Position als offizielle Mätresse zur Förderung zahlreicher Intellektueller und Künstler, unter anderem der Autoren der "Encyclopédie" Denis Diderot und Jean Baptiste le Rond d’Alembert, des Schriftstellers Jean-Jacques Rousseau und der Maler François Boucher, Jean-Marc Nattier und François-Hubert Drouais. Voltaire gehörte zu ihren Favoriten. Durch ihren Einfluss wurde er 1745 zum Historiografen Frankreichs ernannt und ein Jahr später in die "Académie française" aufgenommen. Schließlich wurde er dank der Marquise zum ordentlichen Kammernherrn erhoben. Ihrem ehemaligen Lehrer Prosper Jolyot de Crébillon setzte sie eine Pension über 100 Louisdors aus, als sie erfuhr, dass er sich in Not befand. Nach seinem Tod 1762 überredete sie Ludwig XV. von Frankreich, in der Kirche Saint-Gervais ein Mausoleum für Crébillon in Auftrag zu geben.
In den letzten Jahren ihres Lebens musste die Marquise ihren Platz immer wieder verteidigen. So war Anne Couppier de Romans, eine neue Verbindung des Königs, eine große Gefahr für sie, da sie einen Sohn namens Louis-Aimé de Bourbon (* 1761) von ihm hatte, den Ludwig XV. anerkannte. Die Marschallin de Mirepoix beruhigte sie jedoch, dass der König keine Frau mehr liebe als die Marquise. Nach ihrem Tod gab Ludwig XV. bekannt, dass er sich nur aus Barmherzigkeit nicht von ihr getrennt habe, da er ihren Selbstmord befürchtete.

Die Mätresse wollte ihre Familie an ihrem Wohlstand teilhaben lassen. So kaufte sie ihrer im Dezember 1745 verstorbenen Mutter ein Grabgewölbe in der Kapuzinerkirche an der Place Vendôme, in das sie den Leichnam überführen ließ. Ihrem Vater verschaffte sie das Landgut Marigny, später Marquisat, wo er 1754 mit 70 Jahren an Herzinsuffizienz starb. Außerdem ernannte sie ihren jüngeren Bruder Abel-François Poisson (1725–1781) zum Monsieur de Vandières und verschaffte ihm Ämter bei Hofe. Ihre Tochter Alexandrine schickte sie in ein Kloster, in dem nur die Töchter aus den besten Familien des Königreichs erzogen wurden. Dort wurde das Mädchen wie eine Prinzessin behandelt und auch die anderen Mädchen strebten nach ihrer Freundschaft. Mit 13 Jahren sollte sie den Herzog de Picquigny, Sohn des Herzogs von Chaulnes, heiraten. Die Heirat fand jedoch nie statt, da Alexandrine im Kloster an einer Peritonitis erkrankte, an der sie mit fast zehn Jahren starb.

Die Gesundheit der Marquise war von Geburt an schlecht. Sie spie in ihrer Jugend Blut und machte deswegen Milchkuren, die ihr angeblich halfen, auf die sie später aber, mit Rücksicht auf die Gewohnheiten des Königs, verzichten musste. Es folgten Erkältungen und Fieberschübe. Außerdem war ihre Gesundheit durch zahlreiche Fehlgeburten geschwächt.

Ab 1748 verschlechterte sich ihr Gesundheitszustand. D’Argenson bemerkte ihre ausgezehrten Wangen und die ungesunde Gesichtsfarbe, wenig später war sie abgemagert und ihre Haut hatte sich gelb verfärbt. Wegen ihrer Krankheit setzte die Marquise im November 1751 ihr Testament auf. 1764 verschlechterte sich ihr Zustand dramatisch: Ihr Husten nahm zu und sie litt unter Erstickungsanfällen. Während dieser Zeit besuchte Ludwig XV. sie fast täglich oder informierte sich über ihren Zustand durch Kuriere. Als nach drei Wochen plötzlich eine Besserung eintrat, beschloss man eine Rückkehr nach Versailles. Dort starb sie am 15. April 1764, an einem Palmsonntag, in den "Petits Appartements" gegen sieben Uhr abends nach etwa zweimonatiger Krankheit. Laut Erzählungen gab es schlechtes Wetter, als der Leichenwagen Versailles verließ, um nach Paris zu fahren. Daraufhin soll Ludwig XV. gesagt haben: „La marquise n’aura pas beau temps pour son voyage“ (deutsch: „Die Marquise wird kein gutes Wetter haben für ihre Reise“).

Die Marquise de Pompadour wurde in der Kapelle des Kapuzinerinnenklosters in Paris beigesetzt. Das Kloster wurde 1806 zerstört.

Die Marquise nahm vor allem auf die auswärtige Politik Einfluss, unter anderem auf die militärische Führung, auf Gesetze oder auch auf strategische Planungen. Auch riet sie dem König im Siebenjährigen Krieg zum Bündnis mit Österreich gegen England und Preußen. Hier erfüllte sie alle Erwartungen der Österreicher. Sie drängte zur Ratifizierung des zweiten Vertrags von Versailles und erhielt die Erlaubnis, drei Armeen, anstatt der geplanten 24.000 Mann, zur Unterstützung nach Österreich zu schicken. Nach der verlorenen Schlacht bei Roßbach wollte sie dennoch keinen Frieden schließen, da der Frieden den Ruin ihres Werks bedeutet hätte. Ihr Ausspruch „Après nous le déluge“ (deutsch: „Nach uns die Sintflut“) nach der verlorenen Schlacht ist legendär. Deshalb machte man sie nach ihrem Tod für alle Fehlschläge des Siebenjährigen Kriegs verantwortlich. Den Herzog von Choiseul unterstützte sie.

Aufgrund ihrer bürgerlichen Herkunft war sie trotz ihres Adelstitels eine Außenseiterin am Hof, ihr Status hing vollständig von der wankelmütigen Gunst des Königs ab. Auch hatte sie sich ständig gegen zahlreiche Rivalinnen und missgünstige Höflinge zu behaupten.

Die Gründung der Manufacture royale de porcelaine de Sèvres ist ebenfalls auf die Mätresse zurückzuführen, um damit das sächsische Porzellan zu übertrumpfen. Die ersten Proben waren jedoch nicht von Erfolg gekrönt, bis man 1765 das Kaolin von Saint-Yrieix entdeckte. Daraufhin wurde Sèvres zur königlichen Manufaktur erklärt. Aber der Durchbruch gelang erst, als die Mätresse selbst das Sèvres-Porzellan durch Ausstellungen im Versailler Schloss vorstellte. Dabei versuchte sie mit allen zur Verfügung stehenden Mitteln, Käufer zu werben, und machte das Porzellan sogar zu einer patriotischen Angelegenheit: „Es heißt seine Bürgerpflicht verkennen, wenn man nicht dieses Porzellan kauft, solange man Geld besitzt.“

Aber es gab weitere Projekte, die die Marquise weitaus mehr faszinierten: der Bau der Place Louis XV und der Militärakademie in Saint-Cyr. Sie wollte eine Militärakademie einrichten, für Söhne, deren Angehörige im Krieg gefallen oder verwundet worden waren. Ein Institut für junge Mädchen aus verarmten, adligen Familien gab es dort bereits. Für die Söhne sollte Ludwig XV. von Frankreich die stellvertretende Vaterschaft übernehmen. Im Juli 1756 wurde die Militärakademie schließlich fertiggestellt.

Neben ihrem Landsitz Pompadour erhielt sie zahlreiche weitere Besitzungen vom König, darunter den jetzigen Élysée-Palast und das Petit Trianon (dessen Fertigstellung sie nicht mehr erlebte), die sie oft aufwändig umbauen ließ. Ihre ständigen Bauvorhaben und die Beschäftigung zahlreicher Künstler kosteten Frankreich rund 36 Millionen, was ihr den Vorwurf der Verschwendungssucht sowohl beim Adel als auch bei der breiten Bevölkerung eintrug.

Die Marquise besaß folgende Schlösser und Wohnsitze:





</doc>
<doc id="13666" url="https://de.wikipedia.org/wiki?curid=13666" title="Cottbus">
Cottbus

Cottbus, [], ist eine kreisfreie Stadt im Land Brandenburg. Nach dessen Landeshauptstadt Potsdam ist sie die zweitgrößte Stadt und neben Brandenburg an der Havel und Frankfurt (Oder) eines der vier Oberzentren des Landes. Obwohl in Cottbus selbst nur eine kleine sorbische Minderheit wohnt, gilt sie als das politische und kulturelle Zentrum der Sorben in der Niederlausitz. Die Stadt ist ein Dienstleistungs-, Wissenschafts- und Verwaltungszentrum.

Bis Anfang des 20. Jahrhunderts war die Schreibweise des Stadtnamens strittig. Während bei Berliner Straßennamen die modernere Schreibweise mit "K" angewandt wurde und auch noch teilweise wird („Kottbusser Tor“), hielt man vor Ort am traditionellen "C" fest. Weil die amtliche Eigenbezeichnung der Stadt schon den vor 1996 geltenden Regeln widersprach, bleibt sie nach der "dringenden Empfehlung" des Ständigen Ausschusses für Geographische Namen für die Anwendung der Rechtschreibreform auf geografische Namen weiterhin die gültige Schreibweise. In dem Zusammenhang sei erwähnt, dass sowohl die Schreibweise "Cottbuser(in)" als auch "Cottbusser(in)", also mit einem oder zwei "s", zulässig ist.
Gemäß Hauptsatzung der Stadt trägt sie den amtlichen Namen „Cottbus/Chóśebuz“. Die Stadt trägt zusätzlich zu ihrem Namen die Zusatzbezeichnung „Universitätsstadt/Uniwersitne město“.

Neben dem amtlichen Ortsnamen in Deutsch und Niedersorbisch gibt es auch in den slawischen Nachbarsprachen jeweils eigene Bezeichnungen für Cottbus, so "Choćebuz" im Obersorbischen, "Chociebuż" im Polnischen und "Chotěbuz" im Tschechischen.

Cottbus ist die größte Stadt in der Niederlausitz und liegt an der mittleren Spree zwischen dem Lausitzer Grenzwall im Süden und dem Spreewald im Norden. Die Stadt dehnt sich in Ost-West-Richtung 15,6 km aus, in Nord-Süd-Richtung 19,2 km. Die Spree erreicht in Cottbus eine Breite von 36 m. Sie fließt in einer Länge von 23 km durch die Stadt.

Die Gesamtfläche der Stadt beträgt 164,2 km², davon sind 35,2 km² Waldfläche, weitere 3 km² Wasserfläche. Die nächstgelegenen Großstädte sind Dresden, ca. 90 km südwestlich, Zielona Góra in Polen, ca. 100 km nordöstlich, und Berlin, ca. 100 km nordwestlich von Cottbus.

Das Stadtgebiet von Cottbus ist in 19 Ortsteile gegliedert. Die angegebenen Einwohnerzahlen beziehen sich auf den 31. Dezember 2017. Ausgehend vom Stadtzentrum handelt es sich um folgende Ortsteile (niedersorbische Bezeichnungen in Klammern):

In den Ortsteilen, die 1993 und später eingemeindet worden sind, sind gemäß der Hauptsatzung der Stadt Cottbus Ortsbeiräte zu wählen. Diese sollen u. a. die Stadtverordneten und die Stadtverwaltung in ihrer Arbeit unterstützen.

Darüber hinaus gibt es 19 weitere Gemeindeteile und sonstige Siedlungsplätze.

Die Stadt Cottbus wird als kreisfreie Stadt vollständig vom Landkreis Spree-Neiße umschlossen und grenzt im Norden und Nordosten an die Gemeinden Drachhausen, Drehnow und Turnow-Preilack, die Stadt Peitz sowie die Gemeinden Teichland und Heinersbrück. Im Osten und Süden grenzt sie an die Gemeinden Wiesengrund und Neuhausen/Spree, im Südwesten an die Stadt Drebkau mit Klein Oßnig und im Westen an die Gemeinden Kolkwitz, Briesen und Dissen-Striesow.

Die Stadt Cottbus befindet sich in der gemäßigten Klimazone. Die Jahresmitteltemperatur betrug in der CLINO-Periode 1971–2000 9,3 Grad Celsius. Der wärmste Monat ist der Juli mit durchschnittlich 18,6 Grad Celsius. Der kälteste Monat ist der Januar mit −0,6 Grad Celsius im Mittel. Die Differenz zwischen dem Maximum und dem Minimum, die so genannte Amplitude, beträgt 19,2 Grad Celsius.

Die mittlere jährliche Niederschlagsmenge im Zeitraum 1971–2000 betrug 559 Millimeter. Der meiste Niederschlag fällt im Juli mit 74 Millimetern im Durchschnitt, der wenigste im Februar mit 34 Millimetern im Mittel. Es gibt ganzjährig Niederschläge, im Sommer sind diese jedoch stärker. Das Klima ist damit ganzjährig humide.

Die Cottbuser Siedlungsgeschichte im heutigen Altstadtgebiet kann fast 2000 Jahre zurückverfolgt werden. Im 3. und 4. Jahrhundert siedelten sich im Altstadtbereich germanische Siedler an. Seit dem 6. Jahrhundert wanderten aus dem Südosten slawische Stämme in das Gebiet zwischen Elbe/Saale und Oder ein. Im 8. Jahrhundert folgten die Lusitzi, ein westslawischer Stamm. Sie erbauten auf einer Talsandinsel am Westufer der Spree einen mittelslawischen Burgwall. Im Schutze der slawischen Burg legten die Wenden eine Vorburgsiedlung an, die sich im 11. und 12. Jahrhundert zu einer frühstädtischen Siedlung entwickelte. Am 30. November 1156 fand der Name „Cottbus“ seine erste urkundliche Erwähnung. Die Stadtrechte scheint Cottbus zwischen 1216 und 1225 erhalten zu haben. Im 14. Jahrhundert wurde die Cottbuser Stadtmauer angelegt.

Die „Herren von Cottbus“, ein fränkisches Adelsgeschlecht, herrschten von 1199 bis 1445. Das Geschlecht Kotebuz wurde auch Kottwitz genannt, in alten Karten des 15. und 16. Jahrhunderts wurde deshalb auch der Ortsname KOTTEVITZ, KOTWITZ und Kottwitz gebraucht und geschrieben. Die von Cottbus/Kottwitz gründeten noch fünf weitere Orte Namens Kottwitz, Chotěvice in Sachsen, Schlesien und Böhmen. 1304 mussten die Wettiner wegen Geldschwierigkeiten die Lausitz verkaufen. Bis 1370 kam es aus diesem Grund zu einem häufigen Besitzwechsel der Stadt Cottbus. In den Jahren 1405 und 1406 erteilte Johann III. den Tuchmacher- und Leineweberzünften ihr Privileg.

Seit 1445 stand Cottbus unter brandenburgischer Herrschaft, ausgenommen die Zeit von 1807 (Tilsiter Frieden) bis 1815 (Wiener Kongress), in der die Stadt dem Königreich Sachsen angegliedert war. 1468 schlug der Blitz in die Stadt ein und legte ganz Cottbus mitsamt der Oberkirche in Schutt und Asche. 1479 vernichtete ein Feuer die Stadt abermals.

Auch Pestwellen und Verheerungen während des Dreißigjährigen Krieges brachten Zerstörung, Not und Elend für die Stadt und ihre Bevölkerung. Wallenstein zog mit seinen Truppen durch Cottbus. Die Stadt erlebte mehrfach Besatzung, Plünderungen und Zerstörungen. Am Ende des Krieges 1648 lebten nur noch wenige hundert Menschen in Cottbus.

Im 18. Jahrhundert siedelten sich die französischen Hugenotten an, und Cottbus erlebte einen wirtschaftlichen Aufschwung. Im selben Jahrhundert wurde auch ein Teil der Wehranlagen zurückgebaut, und die Cottbuser nutzten das Gelände, um Maulbeerbäume für die Seidenspinnerzucht zu pflanzen. Gärten wurden angelegt, und die Stadt begann sich in alle Richtungen auszudehnen. Von 1756 bis 1763 tobte der Siebenjährige Krieg. Dieser machte sich auch in Cottbus bemerkbar. Wenn auch keine direkten Kampfhandlungen stattfanden, so gab es doch Durchzüge und Einquartierungen von Truppen. Durch die Bestimmungen des Wiener Kongresses 1815 erfolgte der Anschluss des Kreises Cottbus mitsamt der Niederlausitz an Preußen. Davor war Cottbus eine Exklave im sächsischen Territorium gewesen.

Mit der Industrialisierung im 19. Jahrhundert nahm die Stadt einen bedeutenden Aufschwung. Cottbus wurde zu einem Zentrum der Niederlausitz – zu einer Industriestadt mit moderner Infrastruktur, mit Kultur- und Sozialbauten. Sie entwickelte sich durch den Bau der Eisenbahnen zu einem bedeutenden Verkehrsknotenpunkt. In dieser Zeit wurden viele neue Gewerbe in Cottbus gegründet. Dazu gehören beispielsweise eine Wollgarn-Spinnerei und eine Baumkuchen-Bäckerei. Im Zuge des Verwaltungsaufbaus erhielt Cottbus im Oktober 1824 ein Landgericht. Am 17. März 1831 wurde die revidierte Städteordnung eingeführt. Die vom Landrat, dem Bürgermeister und Vertretern der Bürgerschaft ausgearbeitete Stadtverfassung erhielt am 14. Dezember 1831 die Regierungsgenehmigung. Vom 12. bis 15. Februar 1832 fanden dann die Wahlen zum ersten Stadtparlament statt. Im Oktober 1835 erhielt der Tuchmacher Heinrich Kittel eine Fabrikkonzession. Er vereinigte Spinnerei, Weberei, Walke und Appretur unter einer Leitung. Noch wurden die alten Maschinen von einem Pferdegöpel angetrieben. Anfang der 1840er Jahre wurden aber Dampfmaschinen und der Jacquardwebstuhl vorherrschend. Es waren die Anfänge der Großbetriebe in der Cottbuser Textilindustrie, an denen der englische Textilmaschinen- und Wollfabrikant William Cockerill, Junior maßgeblichen Anteil hatte.

Im Jahr 1861 gründete sich ein Turnverein, dessen Mitglieder nach dem Großbrand vom 9. Mai 1862 eine Turnerfeuerwehr aufstellten, die sich im Februar 1863 konstituierte, was als Gründungsjahr der Cottbuser Feuerwehr gilt. Sie hatte bis 1877 Bestand, als Cottbuser eine Freiwillige Feuerwehr gründeten.

Am 1. August 1914 nahm man auch in Cottbus den Beginn des Ersten Weltkrieges jubelnd auf. Am Gymnasium fanden Notreifeprüfungen statt, und einige Tage später zog das Infanterieregiment Nr. 52 unter dem Jubel Tausender Cottbuser zum Bahnhof. Im September wurde auf der Rennbahn im Norden der Stadt ein Lager für 10.000 Gefangene eingerichtet. Am 4. September 1914 traf der erste Transport mit 7.000 russischen Kriegsgefangenen ein. 1915 kam noch ein Gefangenenlager im Osten der Stadt dazu.

Wirtschaftlich dominierte nach dem Weltkrieg weiterhin die Textilindustrie, allerdings war die Arbeitslosigkeit teilweise hoch. Ab 1938 wurde in Cottbus von den Zittauer Phänomen-Werken das Kettenfahrzeug "ZKW" für die Wehrmacht hergestellt. Etwa 1.000 Arbeiter produzierten monatlich etwa 150 Kettenfahrzeuge. 1939 verlegten die Flugzeugwerke Focke-Wulf Teile ihrer Produktion nach Cottbus, wo erst Aufklärungsflugzeuge Fw 200 und später bis 1945 Jagdflugzeuge des Typs Focke-Wulf Fw 190 montiert wurden. Während der Zeit des Nationalsozialismus entstanden außerdem eine Deutsche Verkehrsfliegerschule und ein Hydrierwerk.
Im Herbst des Jahres 1940 erlebten die Cottbuser die ersten Luftangriffe auf die Stadt. Am 15. Februar 1945 zerstörte ein Luftangriff von 459 US-amerikanischen B-17-Bombern große Teile der Stadt. Dabei wurden rund 4.000 Sprengbomben auf das Bahnhofsgelände, die östlichen und südlichen Stadtbezirke, die Branitzer Siedlung und mehrere Industriebetriebe geworfen. Der Angriff forderte mehr als 1.000 Todesopfer. Am 22. April 1945 nahmen Truppen der 1. Ukrainischen Front der Roten Armee nach dreitägigen verlustreichen Kämpfen die Stadt ein.

Cottbus war ab 1952 Hauptstadt des Bezirks Cottbus der DDR. Das Gebiet um Cottbus wurde ab 1957 zum wichtigsten Kohle- und Energielieferanten. Aber auch das Bauwesen, die Textil- und Möbelindustrie sowie die Nahrungsmittelproduktion bestimmten die Wirtschaftsstruktur der Stadt, die 1976 den Status einer Großstadt erhielt.

Mit dem Vollzug der deutschen Einheit im Oktober 1990 begann durch die Privatisierung der Wirtschaft ein tiefgreifender Strukturwandel in Stadt und Region. Cottbus wurde zu einem Dienstleistungs-, Wissenschafts- und Verwaltungszentrum. Im Zuge der brandenburgischen Kreisreform von 1993 wurde der Landkreis Cottbus Teil des neu gebildeten Landkreises Spree-Neiße. Die Stadt selbst blieb aber kreisfrei. Im Jahr 2006 beging die Stadt die 850-Jahr-Feier der urkundlichen Ersterwähnung. Seit dem 1. Januar 2007 ist Cottbus der Sitz des Finanzgerichts Berlin-Brandenburg.

In der Geschichte der Stadt wurden folgende Gemeinden bzw. Gemarkungen nach Cottbus eingegliedert:

Die Eingemeindungen seit der Zugehörigkeit zur Bundesrepublik Deutschland dienten vor allem dem Zweck, den Status einer Großstadt (mit mindestens 100.000 Einwohnern) und die damit verbundenen finanziellen Zuwendungen zu erhalten. Zum Teil erfolgten diese Eingemeindungen, insbesondere die der südlichen Ortsteile im Jahr 2003, gegen den erklärten Willen der Bewohner. Bis heute wurden keine weiteren Eingemeindungen vorgenommen.

Die Bevölkerungsentwicklung von Cottbus ist starken Schwankungen ausgesetzt. Die Schwankungen der Einwohnerzahl zwischen dem 14. und 17. Jahrhundert sind Folgen der Pest. Die Bevölkerungszahl von Cottbus überschritt am 4. September 1976 die Grenze von 100.000, wodurch sie zur Großstadt wurde. In nur 13 Jahren bis 1989 erreichte sie vor allem durch das Braunkohlekombinat mit fast 130.000 ihren historischen Höchststand. Seit der Wende in der DDR hat die Stadt in ihrem Stadtgebiet von 1990 wegen der hohen Arbeitslosigkeit und des Geburtenrückgangs bis 2007 rund 46.000 Einwohner verloren. Es gab mehr Sterbefälle als Geburten und auch mehr Wegzüge als Zuzüge. Der Status als Großstadt mit über 100.000 Einwohnern konnte in den ersten 13 Jahren nach der Wende nur durch Eingemeindungen von etwa 17.000 Einwohnern aus dem Umland erhalten werden. Für Anfang 2009 vermeldete die städtische Statistik noch 100.068 Einwohner. Ende Januar 2009 fiel die Zahl der Einwohner einem Pressebericht zufolge auf unter 100.000. Für 2010 vermeldete man erstmals wieder mehr Zuzüge (4.978) nach Cottbus als Wegzüge (4.305) und den Anstieg der Geburten auf über 800 im Jahr, die Bevölkerungszahl stieg wieder über 100.000. Als Ergebnis des Zensus 2011 wurde die Bevölkerungszahl zum 9. Mai 2011 mit 99.984 angegeben. Damit verlor Cottbus den Status einer Großstadt. Diesen Status erlangte Cottbus 2016 wieder zurück, als laut Statistischem Landesamt 100.416 Einwohner zum 31. Dezember 2016 gezählt wurden.

Als Folge des Einwohnerrückgangs und der Vergrößerung des Stadtgebietes ging die Einwohnerdichte stark zurück. Während sie am 31. Dezember 2000 noch bei 720 Personen je km² lag, lebten am 31. Dezember 2012 nun 608 Personen auf einem km².

Der Anteil von Menschen mit nicht-deutscher Staatsbürgerschaft lag Ende 2000 bei 2,8 %, Ende 2011 bei 3,4 %, Ende 2015 bei 2,2 % und stieg dann bis Anfang 2018 auf 8,5 % (insgesamt etwa 8.000 Personen, 3.400 davon Flüchtlinge). Ähnlich wie zuvor in Salzgitter, wurde daraufhin ein „Zuzugstop“ verhängt. 2011 hatten 6,1 % der Cottbuser einen Migrationshintergrund.

Die bevölkerungsreichsten Ortsteile mit jeweils mehr als 10.000 Einwohnern sind Sandow, Ströbitz, Schmellwitz, die Spremberger Vorstadt und Sachsendorf. Die bevölkerungsärmsten Ortsteile mit jeweils weniger als 1.000 Einwohnern sind Skadow, Saspow und Willmersdorf. Im Jahr 1991 betrug das Durchschnittsalter der Stadtbevölkerung 35,5 Jahre. Im Jahr 2000 lag es bereits bei 40,9 Jahren, 2011 bei 45,7 Jahren. „Jüngster Ortsteil“ Ende 2008 ist Sielow mit durchschnittlich 41,8 Jahren, den höchsten Altersschnitt gibt es im Ortsteil Madlow mit 49,9 Jahren, gefolgt von Sandow (48,9).

Die überwiegende Anzahl (82,3 %) der Einwohner der Stadt Cottbus gehört keiner Religionsgemeinschaft an. 11,9 % bekennen sich zur Evangelischen Kirche und 3,5 % zur Römisch-katholischen Kirche. 0,6 % gehören der Orthodoxen Kirche an, 1,1 % sonstigen Religionen.

Die Christianisierung der Lausitz erfolgte von Meißen aus und war etwa um 1100 abgeschlossen. Bischof Eido von Rochlitz konnte bei seinen Missionen von 992 bis 1015 durch seine Kenntnis der slawischen Sprachen erfolgreich in der Niederlausitz predigen und auch unter Benno von Meißen wurde die Mission ab 1058 erfolgreich fortgeführt. Die Stadt Cottbus gehörte so anfangs zum Bistum Meißen. Wegen der großen Entfernung der dünn besiedelten Niederlausitz zum sächsischen Meißen gab es in Lübben einen Offizial als Stellvertreter des Bischofs. Cottbus war Sitz eines Erzpriesters.

1522 wurde ein erster Versuch unternommen, in der Stadt die Reformation einzuführen. Die endgültige Etablierung der evangelischen Konfession gelang erst Markgraf Johann von Küstrin 1537. Die Stadt war dann über Jahrhunderte überwiegend protestantisch. Gotteshäuser waren die Pfarrkirche St. Nikolai (Oberkirche) und die Kirche des um 1300 gegründeten Franziskanerklosters (Klosterkirche). Außerdem gab es an der Stelle der heutigen Schlosskirche eine Katharinenkirche, die 1600 abbrannte. Vorherrschend war das lutherische Bekenntnis, doch gab es ab 1620 auch eine reformierte Gemeinde auf dem Schloss. 1714 wurde die Schlosskirche erbaut.

1830 wurden durch König Friedrich Wilhelm III. beide Konfessionen innerhalb Preußens zu einer einheitlichen Landeskirche (Unierte Kirche) vereinigt. Somit gehörten die protestantischen Gemeinden von Cottbus zur „Evangelischen Kirche in Preußen“ bzw. deren Provinzialkirche Brandenburg, deren Oberhaupt der jeweilige König von Preußen als „summus episcopus“ war. Als Reaktion auf diese staatliche Zwangsvereinigung entstand im gesamten Königreich Preußen die evangelisch-lutherische (altlutherische) Kirche. Sie forderte ihr Recht auf Religionsfreiheit, indem sie uneingeschränkt lutherische Verfassung, Gottesdienst und Lehre in Geltung wissen wollte. So entstand in Cottbus 1846/47 zunächst eine Kirchengemeinde, die ihre evangelisch-lutherische Kreuzkirche jedoch erst 1878/79 errichten konnte. Bis heute sind die Altlutheraner mit einem Pfarramt und der Kreuzkirche in der Stadt präsent. Heute gehört die Evangelisch-Lutherische Kreuzkirchengemeinde zum Kirchenbezirk Lausitz der Selbständigen Evangelisch-Lutherischen Kirche.

Nach Wegfall des Landesherrlichen Kirchenregiments 1918 war die Provinzialkirche Brandenburgs Gründungsmitglied der Evangelischen Kirche der Altpreußischen Union. 1947 wurde sie eine selbstständige Landeskirche mit einem Bischof an der Spitze. 2004 fusionierte die Kirche mit der Evangelischen Kirche der schlesischen Oberlausitz zur Evangelischen Kirche in Berlin-Brandenburg-schlesische Oberlausitz. Die protestantischen Kirchengemeinden von Cottbus gehören – sofern es sich nicht um Freikirchen handelt – zum Kirchenkreis Cottbus innerhalb des gleichnamigen kirchlichen Amtsbezirks, dessen Sitz sich ebenfalls in Cottbus befindet.

Neben den landeskirchlichen Gemeinden gibt es mehrere Freikirchen in Cottbus, beispielsweise eine Evangelisch-Freikirchliche Gemeinde (Baptisten), eine Evangelisch-methodistische Kirche, die Apostolische Gemeinschaft, die Biblische Glaubensgemeinde Cottbus e. V. und die Freie Christliche Gemeinde Sachsendorf.

Als Gotteshäuser stehen den evangelischen Gläubigen heute u. a. die Oberkirche St. Nikolai, die Klosterkirche, die Lutherkirche und die Madlower Martinskirche zur Verfügung. Die Schlosskirche wurde 2014 umgewidmet.

Nach der Reformation blieb nur ein geringer Rest katholischer Gläubiger in Cottbus und Umgebung. Diese wurden vom Kloster Neuzelle betreut. Gelegentliche Gottesdienste fanden in der Katharinenkirche bis 1590 statt. Ab 1646 gestattete der Rat der Stadt an zwei Sonntagen im Jahr Gottesdienste in der Gottesackerkirche „Ad sanctam portam“.

Nach mehreren Gesuchen an die zuständigen staatlichen und kirchlichen Stellen wurde 1848 für die wachsende Zahl der Katholiken der Grundstein der heutigen Christuskirche gelegt und die Kirche „Zum Guten Hirten“ am 27. Oktober 1850 geweiht. Insbesondere in und durch die Industrialisierung wuchs die Zahl der Katholiken bald auf über 2500 an und so wurde für die gewachsene Gemeinde am 7. Oktober 1934 ein neues Gotteshaus durch den Breslauer Kardinal Adolf Bertram auf das im Kontext der Zeit wohlgewählte Patrozinium „St. Maria Friedenskönigin“ geweiht. 1964 wurde die kriegsbeschädigte Kirche „Zum Guten Hirten“ wiederhergestellt und es bildete sich eine zweite Cottbuser Pfarrgemeinde um die „Christuskirche“. Seit 2012 sind beide Gemeinden wieder zusammengeführt und führen den alten Titel „Zum Guten Hirten“. Die Gemeinde gehört heute zum Bistum Görlitz.

Den römisch-katholischen Christen stehen die Christuskirche (Zum Guten Hirten), die Edith-Stein-Kirche und die Propstei- und Pfarrkirche „St. Maria Friedenskönigin“ als Gotteshäuser zur Verfügung.
Um die pädagogische und medizinische Versorgung der Bevölkerung überkonfessionell zu unterstützen, siedelte sich am 01. Dezember 1886 der sozial tätige, und insbesondere krankenpflegende katholische Orden Arme Dienstmägde Jesu Christi (auch: Dernbacher Schwestern) aus Dernbach im Westerwald an. Seine Tätigkeit der ambulanten Krankenpflege, im Kindergarten und im Altenheim dauerte bis zum 01. Mai 1965.

Auch die Neuapostolische Kirche, die Kirche Jesu Christi der Heiligen der Letzten Tage (Mormonen) und die Zeugen Jehovas sind in Cottbus vertreten.

Die älteste Nachricht über Juden in Cottbus stammt aus dem Jahr 1448, als Joachim II. ihnen Wohnrecht in der Stadt erteilte und sie unter seinen Schutz stellte. 1510 mussten alle Juden nach dem angeblichen Hostienfrevel von Knoblauch Cottbus verlassen.

Erst 1692 ist wieder der erste Antrag auf Ansiedlung einer jüdischen Familie in Cottbus erhalten, 1740 sind jüdische Bürger erwähnt.
1811 wurde erstmals eine Betstube im Hinterhaus eines Tuchmachers an der Mauerstraße genannt. 1814 lebten nur 17 Juden in Cottbus. Mit dem Jahr 1816 und der Zugehörigkeit zu Preußen, wo seit 1812 das Judenedikt galt, wuchs auch die jüdische Gemeinde langsam. Im Jahr 1847 wurde von den Juden der Stadt und ihrer Umgebung beschlossen, die Bildung einer Jüdischen Gemeinde in Angriff zu nehmen. 1858 galt die Gemeinde schließlich als gegründet. 1866 gehörten ihr 31 Personen an, 1902 waren es bereits 90 Mitglieder.

Bereits im Jahr 1933 begann die systematische Entrechtung, Diskriminierung, Verfolgung und Vernichtung jüdischer Bürger durch die Nationalsozialisten. Allein in diesem Jahr wurden 315 Gesetze und Verordnungen zu ihren Ungunsten erlassen. Des Weiteren gründete sich ebenfalls 1933 die ausschließlich für so genannte „Judenangelegenheiten“ zuständige Ortspolizeibehörde Cottbus VI.

Am 31. März 1933 erschien in einer Cottbuser Tageszeitung der Boykottaufruf, der neben Lebensmittelläden auch sämtliche Büroräume jüdischer Rechtsanwälte, die Niederlassungen jüdischer Ärzte und Tuchversandgeschäfte betraf und ab dem 1. April 1933 gelten sollte. Darüber hinaus wurden zahlreiche Einbürgerungen, die zwischen 1918 und 1933 erfolgt waren, rückgängig gemacht. 1936 lebten 334 jüdische Bürger in Cottbus, darunter 87 Kinder, 128 Frauen und 119 Männer. Im Februar 1937 lebten bereits 499 Juden in Cottbus. Viele von ihnen waren aus den umliegenden Gemeinden nach Cottbus geflohen, da sie darauf hofften, in einer größeren Stadt anonymer leben zu können. Gleichzeitig lief eine von der Regierung gewollte Auswanderungswelle, unter Zahlung der geforderten „Reichsfluchtsteuer“, einhergehend mit Zwangsenteignungen von Häusern, Geschäften und Fabriken. So emigrierten am 1. Oktober 1936 34 Juden, hauptsächlich nach Südafrika und Brasilien. Während der Novemberpogrome 1938 brannten die Nationalsozialisten auch die Cottbuser Synagoge nieder. Sie wurde später abgetragen und an ihrer Stelle in den 1960er Jahren ein Kaufhaus errichtet. Zur Erinnerung an die jüdische Gemeinde und ihre Synagoge steht auf dem Vorplatz der Stadtwerke in der Karl-Liebknecht-Straße eine Tafel, die 1988 aufgestellt und 1998 erneuert wurde. Die Nacht des 9. Novembers war Auftakt für die Deportationen jüdischer Bürger in Konzentrationslager. Schon Mitte November 1938 verließ der erste Transport die Stadt. Nach dem Kriegsende waren nur noch zwölf Mitglieder der ehemaligen Gemeinde am Leben.

Am 15. Juli 1998 wurde die jüdische Gemeinde in Cottbus wiedergegründet. Sie wirkt gemeinnützig als eingetragener Verein. Gegenwärtig zählt sie etwa 350 Mitglieder, die alle aus der ehemaligen Sowjetunion nach Deutschland gekommen sind. Die Gemeinde hatte jedoch bis Anfang 2015 keine würdige Synagoge. Erschwerend kam hinzu, dass die Räumlichkeiten nicht mehr die nötigen Kapazitäten hatten. Nach etlichen Jahren der Bemühung um bessere Gemeinderäume wurde am 18. September 2014 die evangelische Schlosskirche der jüdischen Gemeinde zur Umnutzung als Synagoge übergeben. Am 27. Januar 2015 erfolgte die offizielle Weihung der neuen Synagoge unter Teilnahme des Landesrabbiners und des Vizepräsidenten des Zentralrates der Juden in Anwesenheit von etwa 1.000 Cottbusern.

Die Stadtverordnetenversammlung hat neben dem Oberbürgermeister Holger Kelch (CDU) 46 Mitglieder. Seit der Kommunalwahl vom 25. Mai 2014 setzt sie sich wie folgt zusammen:


In der Stadtverordnetenversammlung bilden AUB, SUB und FLC eine Fraktionsgemeinschaft.

Durch die Nichtaufnahme eines gewählten Stadtverordneten der LINKEN in die Fraktion verfügt diese nur noch über neun Sitze.

DIE LINKE und SPD gingen wieder eine Kooperation ein, haben jedoch nur noch 19 von 47 Stimmen (inkl. OB) Stimmen.

An der Spitze der Stadt Cottbus stand wohl schon seit dem 13. Jahrhundert ein Bürgermeister, doch sind nur einige Namen überliefert. Sie fungierten als Sprecher der Bewohner und waren dem Schlossherrn unterstellt. Spätestens seit dem 16. Jahrhundert gab es auch einen Rat, der aus Ratsmännern und vier Bürgermeistern bestand. Später änderte sich die Zahl der Bürgermeister. Seit dem 19. Jahrhundert trug das Stadtoberhaupt meist den Titel „Oberbürgermeister“. Der Rat trug dann die Bezeichnung Stadtverordnetenversammlung.

Der Oberbürgermeister wird heute direkt vom Volk gewählt. Am 14. September 2014 fand die letzte Wahl statt. Holger Kelch (CDU) wurde mit 50,7 % der Stimmen zum Oberbürgermeister gewählt. Seine Gegenkandidaten waren der bisherige Amtsinhaber Frank Szymanski (SPD) mit 37,3 % und Lars Krause (Die PARTEI) mit 12,0 %.

Das Lokale Bündnis für Familie läuft im Jahr 2008 noch als eines von bundesweit mehreren Modellprojekten. Es dient der Förderung der Familien und des bürgerschaftlichen Engagements in Zusammenarbeit mit Politik, Verwaltung, Bürgern, Bildungseinrichtungen und Vereinen.

Der Krebs als heraldisches Symbol ist relativ selten. Die Behauptung, dass Cottbus den Krebs als einzige Stadt im Wappen führt, ist aber falsch. Ebenfalls einen Krebs als Wappentier haben die Städte Bad Wurzach, Bernkastel-Kues, Kreßberg, Vörstetten und Pram in Oberösterreich. Außerdem haben auch die Landkreise Bernkastel-Wittlich und Spree-Neiße den Krebs im Wappen. Ursprünglich führten ihn streng gläubige Ritter in ihrem Schild. Nach Cottbus kam dieses Wappentier wahrscheinlich aus dem Fränkischen. Fredehelm von Cottbus, welcher 1307 starb, stammte vom fränkischen Adelsgeschlecht Kottwitz ab, deren Wappensymbole neben dem Krebs auch ein Widderhorn waren. Im ältesten bildhauerischen Kunstwerk der Stadt, dem Grabstein in der Klosterkirche mit dem Bildnis Fredehelms und seiner Gemahlin Adelheid ist der Krebs auf dem Brustharnisch und dem Schild des Ritters abgebildet. Von dort kam der Krebs wahrscheinlich in das älteste erhaltene Stadtsiegel aus dem 14. Jahrhundert. Dies ist die älteste Überlieferung des Wappens. 

Die Flagge der Stadt Cottbus ist dreistreifig Rot-Weiß-Rot im Verhältnis 1:8:1 und mit dem Stadtwappen im Mittelstreifen.

Cottbus unterhält Städtepartnerschaften mit folgenden Städten:

Besonders intensiv ist dabei die Beziehung zu Zielona Góra, mit dem es Jahresvereinbarungen über die Zusammenarbeit gibt und für welches auch in der örtlichen Touristeninformation geworben wird. Zielona Góra wie auch Saarbrücken, Gelsenkirchen und Lipezk sind jeweils Straßennamen in Cottbus gewidmet.

Die Stadt Cottbus verfügt über eine Vielzahl von Theatern, Bühnen und Ensembles. Das bekannteste ist wohl das "Staatstheater Cottbus", erbaut nach Entwürfen des Architekten Bernhard Sehring. Es ist das einzige staatliche Theater in Brandenburg und stellt Schauspiel, Musiktheater und Ballett dar. Außerdem bietet die Stadthalle von Cottbus ungefähr 2000 Menschen Platz. In diesem Veranstaltungssaal gastieren regelmäßig internationale Ensemble wie der Chinesische Nationalcircus, das Russische Staatsballett sowie Stars aus Pop, Folk und Schlager: z. B. Harry Belafonte, Rosenstolz, Gitte Hænning. Das kleine Theater, die "TheaterNative C", wurde als Privattheater 1989 gegründet und ist zu einer bestimmenden Größe in der Cottbuser Kunstszene geworden. Es bietet Schauspiel, Kabarett, Boulevard sowie Kindertheater und experimentelle Inszenierungen. Das "piccolo-Theater" ist seit 1991 ein Theater für Kinder und Jugendliche. Das Publikum der "Puppenbühne Regenbogen" besteht überwiegend aus Kindern im Alter von vier bis zehn Jahren, für die der Besuch oft das erste Theatererlebnis ihres Lebens ist und bleibende Eindrücke hinterlässt.

Musikalisch sind in der Stadt das "Philharmonische Orchester" Cottbus, die "Singakademie" Cottbus, das "Cottbuser Kindermusical" und an Ensembles das "Kinder- & Jugendensemble Pfiffikus", das "Studententheater Bühne 8", sowie das "Theater an der Wendeschleife" im "Gladhouse" ansässig.

Mit dem "Filmtheater Weltspiegel" besitzt Cottbus den nach dem Burg Theater in Burg (bei Magdeburg) zweitältesten Kinozweckbau Deutschlands. Dieser wurde im Jugendstil erbaut. Das "Obenkino" im Gladhouse und das "KinOh Stadthalle" sind weitere kleine Kinos im Stadtzentrum. Zudem gibt es eine "UCI Kinowelt" im Ortsteil Groß Gaglow, in der Nähe des Einkaufszentrums Lausitz Park.

Jährlich im Herbst findet seit 1991 in Cottbus das "Filmfestival Cottbus" mit einem Schwerpunkt auf das osteuropäische Kino statt. Das Festivalzentrum befindet sich in der Stadthalle Cottbus. Spielstätten sind weiterhin das Staatstheater, die Kammerbühne, das Filmtheater Weltspiegel, das Obenkino und das Zelig.

Die Stiftung Fürst-Pückler-Museum Park und Schloss Branitz bringt dem Besucher im Schloss sowie in der Multimedia-Ausstellung im Gutshof die Geschichte des Parks und dessen Schöpfer Fürst Hermann von Pückler-Muskau näher.

Das "Wendische Museum" vermittelt Einblicke in Kultur und Geschichte der Wenden der Niederlausitz. Zahlreiche Exponate zur Trachtenkunde, Schrifttum und Literatur, Kunst und Musik sowie zu Brauchtum und Lebensweise belegen die einzigartige Kultur der slawischen Bevölkerung.

Sowohl das "Stadtmuseum" als auch das "Stadtarchiv" gelten als historisches Gedächtnis der Stadt Cottbus. Sie sind Ansprechpartner für geschichtsinteressierte Bürger, Heimatforscher und Historiker. Beide Institutionen widmen sich der Stadtgeschichte. Von 2010 bis Ende 2014 war das Stadtmuseum für die Öffentlichkeit geschlossen. Mit Sonderausstellungen im Rathaus und weiteren städtischen Einrichtungen werden zudem interessante Einzelthemen vorgestellt.

Das "Brandenburgische Apothekenmuseum" am Altmarkt ist das einzige Apothekenmuseum des Landes Brandenburg. Komplette Apothekeneinrichtungen aus der Zeit um 1830 und der ersten Hälfte des 20. Jahrhunderts werden dort gezeigt.

Das "Kunstmuseum Dieselkraftwerk", bis April 2006 Brandenburgische Kunstsammlungen Cottbus, beherbergt Werke aus den Gattungen Malerei, Skulptur, Grafik, Fotografie und Plakat und sie arbeitet vornehmlich mit dem Themenkomplex von Landschaft, Raum, Natur und Umwelt. Die Galerie Haus 23, die Galerie Fango und die Galerie auf Zeit bieten Bildhauern, Malern, Fotografen und Filmemachern aus Cottbus und der Region die Möglichkeit, ihre Werke auszustellen.

Das "Flugplatzmuseum" zeigt 80 Jahre Geschichte der Cottbuser Flugplätze und der Luftfahrt in der Lausitz.

Weitere Museen sind das "Wassermuseum" im Wasserwerk Cottbus, das Technische Denkmal "Spreewehrmühle" und das Technische Denkmal "Parkschmiede Cottbus-Branitz".

Das "Raumflugplanetarium „Juri Gagarin“" wurde am 26. April 1974 am heutigen Lindenplatz eröffnet und es befand sich bis Ende 2012 der originale Sternprojektor "Spacemaster – Raumflugplanetarium" von Carl Zeiss aus Jena im Einsatz. Seine Kuppel mit einem Durchmesser von 12,5 m bietet 91 Besuchern Platz. Seit dem 19. Juni 2013 wird ein neues Hybrid-Projektionssystem vom Typ "Chronos II − InSpace" verwendet.


Der 31 m hohe "Spremberger Turm" wurde im 13. Jahrhundert als Teil der 1.200 m langen Wehranlage erbaut und bildet mit Bastei und Torhaus das südliche Stadttor. Die Zinnenkrone erhielt er in den Jahren 1823 bis 1825.
Der "Münzturm" ist der älteste Turm der Stadt. Die „Herren von Cottbus“ ließen hier wahrscheinlich schon 1483 den Cottbuser Heller mit dem Cottbuser Wappentier, dem Krebs, prägen. Türme, Tore und Wiekhäuser entlang der mittelalterlichen Stadtmauer lassen den Grundriss der Altstadt erkennen. Die Lindenpforte entstand, um schneller von der Altstadt den Markt in der Neustadt erreichen zu können. Dazu wurde im Jahre 1879 der Mauerturm in der Mauerstraße durchbrochen.




Im Stadtgebiet befinden sich vier ausgewiesene Naturschutzgebiete (Stand Februar 2017).

Der "Branitzer Park" ist der wohl bedeutendste und bekannteste Park in Cottbus. Branitz gelangte 1696 in Besitz der Grafen von Pückler. Im Jahr 1845 begann Hermann von Pückler-Muskau mit dem Bau der neuen Parkanlage. Der von ihm geschaffene Landschaftspark, der unter seinem Nachfolger vollendet wurde, ist ein Gartenkunstwerk von internationaler Bedeutung. Der namhafte Schriftsteller und Weltreisende Fürst Pückler war neben Peter Joseph Lenné und Friedrich Ludwig Sckell einer der bekanntesten deutschen Gartengestalter des 19. Jahrhunderts. Der Branitzer Park wurde als zonierter Landschaftspark mit gestalterisch differenzierten Parkbereichen angelegt.

Im Zentrum der Anlage befindet sich das 1770 bis 1772 erbaute Schloss. Das Schloss ist umgeben vom sehr reich mit Blumenbeeten, Plastiken, weiteren Schmuckelementen und Ziergehölzen ausgestatteten Pleasureground. Hier verwendete Pückler auch fremdländische Gehölze, während er in den weiteren Parkbereichen nur heimische pflanzen ließ.

Der anschließende „Innenpark“ mit einer Fläche von ungefähr 100 Hektar umfasst unter anderem die Gutsökonomie, die Gärtnerei, die Parkschmiede, das Cottbuser und das Branitzer Torhaus sowie die Pyramidenebene. Fürst Pückler gestaltete außerdem die den Park umgebende Feldflur, den „Außenpark“, als Ornamental Farm auf einem Gesamtareal von etwa 600 Hektar. Für die Gestaltung des Parks nutzte Fürst Pückler den hohen Grundwasserstand und die in der Nähe gelegene Spree, um in seinem Park ein künstliches Gewässersystem zu schaffen. Mit dem Aushub aus den Seen und Kanälen ließ er das gestalterisch vollendete Geländerelief des Parks anlegen. Besonders schön ist die Schilfseepartie modelliert.

Die Pyramidenebene beeindruckt mit den beiden einzigartigen Erdpyramiden, der ehemals stufenförmig erbauten Landpyramide (erbaut 1860–1863) und der Seepyramide, dem Tumulus (erbaut 1856). Fürst Pückler ließ sich 1871 im Tumulus beisetzen. 1884 wurde auch seine 1854 verstorbene Frau und Lebensgefährtin, Lucie von Pückler-Muskau, dorthin umgebettet.

Durch meisterhafte Gruppierung der Gehölze, künstlerisch gelungene Ausformung des Reliefs und geschickte Wegeführung schuf der Fürst mit dem Branitzer Park eine Art Bildergalerie, in der sich dem Betrachter beim Spazierengehen eine Folge dreidimensionaler Landschaftsbilder darbietet.

1898 entstand auf der feuchten Niederung der Mühleninsel die erste Cottbuser Parkanlage, der "Goethepark", durch Initiative des Oberbürgermeisters Paul Werner und des Verschönerungsvereins. Der Amtsteich innerhalb des Parks wurde bereits um 1600 zur Fischzucht angelegt. 1914 bis 1935 erfolgte ein weiterer Ausbau des einst sumpfigen Geländes. Die Gestaltung der Uferbereiche mit Staudenpflanzungen an den Bachläufen nahe dem Dieselkraftwerk wurde 1954 zur Ausstellung „Grünen und Blühen an der Spree“ vorgenommen. Der "Carl-Blechen-Park", mit seltenen Bäumen und Blütenstauden an der Ostseite der Spree, entstand in den 1930er Jahren. Seine 1934 und 1935 geschaffene Uferpromenade ist mit dem nach Süden verlaufenden „Rosenufer“, der heutigen Ludwig-Leichhardt-Allee, fortgeführt worden.

Der "Eliaspark" entstand 1902 durch eine Stiftung des Kommerzienrates Elias. Dieser dreieinhalb Hektar große Park wurde im Rahmen der ersten Bundesgartenschau in den neuen Ländern 1995 neu gestaltet. Der mit 55 Hektar deutlich größere "Spreeauenpark" hingegen wurde erst im Vorfeld der Bundesgartenschau 1995 geschaffen. Inmitten von Wiesenflächen unter schattenspendenden Bäumen entstanden neue Wege, Spiel- und Sportanlagen, ein Wasserspielplatz und das Spielhaus als Treff für Kinder und Jugendliche. Der Spreeauenpark erfreute seit der BUGA Millionen Besucher. Um den 1,2 Hektar großen Parkweiher gruppiert sich ein Rosengarten, ein Rhododendronhain, Wiesenlandschaften mit Wechselpflanzungen und ein Apotheker- und Bauerngarten. Für Kenner ist der Tertiärwald mit Pflanzen und Gehölzen aus verschiedenen erdgeschichtlichen Epochen, Findlingen aus der Eiszeit und einem fossilen Mammutbaumstubben eine besondere Attraktion.

Im Jahr 1954 eröffnet grenzt der "Tierpark Cottbus" heute an den Spreeauenpark und den Branitzer Park. Mit mehr als 1.200 Tieren in über 170 Arten aus allen Erdteilen ist er der größte zoologische Garten Brandenburgs, unter anderem bekannt durch die Züchtung von Wasservögeln. Durch die Unterstützung der Stadt Cottbus, verschiedener Firmen und des Tierpark-Fördervereins (gegründet 1994) wird der Tierpark stets modernisiert und erweitert. Im Sommer 2014 soll der Bau eines neuen Raubtiergeheges beendet werden, welches als Domizil von Sumatra-Tigern geplant ist.

Der mit Abstand bekannteste Verein der Stadt ist "Energie Cottbus" (Fußball). Der FCE, derzeit in der Regionalliga Nordost aktiv, schaffte in der Saison 1999/2000 den Aufstieg in die erste Bundesliga sowie den zweimaligen Klassenerhalt, stieg in der Saison 2002/2003 allerdings in die zweite Bundesliga ab. In der Saison 2005/2006 konnte der Verein den erneuten Einzug in die erste Bundesliga erreichen, aus der er in der Saison 2008/2009 wieder abstieg. Das Stadion der Freundschaft hat aktuell ein Fassungsvermögen von 22.528 Zuschauern. Es bietet 10.949 überdachte Sitzplätze, 7.795 überdachte und 3.630 nicht überdachte Stehplätze, sowie 154 Plätze im Rollstuhl-Handicapbereich.

Weitere größere Vereine sind der Handballverein "LHC Cottbus", welcher in der Saison 2007/2008 in der 2. Handball-Bundesliga spielte, die "White Devils" (Basketball), die "Cottbus Crayfish" (American Football), die "Crabettes" (Cheerleading), die "Cottbus Cannibals" (Lacrosse), sowie die 1. Damenmannschaft des "SV Energie (Volleyball)", die seit Jahren erfolgreich in der Regionalliga-Nordost spielen. Der Radsportverein "RSC Cottbus" brachte zahlreiche Weltmeister und Olympiasieger hervor.

Insgesamt gibt es mehr als 120 Sportvereine in Cottbus. In der Stadt gibt es vier Dachorganisationen: den "PSV Cottbus 90 e. V.", den "SCC Breitensport e. V.", den "Stadtsportbund Cottbus e. V." und die "Versehrtensportgemeinschaft Cottbus e. V."

Auf dem "Weg des Ruhmes" vor dem Neuen Rathaus werden die Medaillengewinner Olympischer und Paralympischer Spiele geehrt. Nach den Spielen in London befinden sich dort nun 37 in den Boden eingelassene Ehrenplaketten.

Cottbus ist Olympiastützpunkt für die Sportarten Radsport, Turnen, Fußball (m), Leichtathletik, Boxen (m), Handball (m) und Volleyball (w). Für den aktiven Freizeitsport stehen allen Interessenten 50 Sporthallen, 49 Sportplätze und Stadien, 20 Tennisplätze, 70 Kegel- und Bowlingbahnen, fünf Schießstände, vier Badeseen, eine Reitsportanlage, eine Schwimmhalle mit Freibad und ein Bootshaus zur Verfügung.

Seit 2013 wird auch ein Paralympischer Trainingsstützpunkt für Leichtathletik, mit behindertengerechten Trainingsanlagen aufgebaut.

Sportzentrum

Das "Sportzentrum", welches vom Sportstättenbetrieb der Stadt Cottbus verwaltet wird, ist eine der größten und modernsten Sportanlagen in der Region Südbrandenburg. Es wird für den Kinder- und Jugendsports, den Nachwuchs- und Spitzensport sowie den Breiten- und Behindertensport genutzt. Zum Komplex gehören das "Max-Reimann-Stadion", ein Leichtathletikstadion, das mit allen seinen Anlagen internationalen Anforderungen erfüllt. Des Weiteren das "Cottbuser Radstadion", mit seiner überdachten Radrennbahn finden nationale und internationale Wettkämpfe mit großem Anklang statt, beispielsweise der Weltcup im Bahnradsport 1995 und 1996. Darüber hinaus befinden sich noch eine Leichtathletik-Halle, zwei Turnhallen, zwei Fußballfelder und eine Boxhalle auf dem Gelände. Mit der "Lausitz-Arena" verfügt es zusätzlich noch über eine Sportmehrzweckhalle für rund 2000 Zuschauer.


Beim "Turnier der Meister" – der wohl sportlich hochwertigsten Sportveranstaltung in Cottbus – treten jährlich rund 200 Turner aus rund 40 Nationen im Gerätturnen gegeneinander an und kämpfen um die begehrten Titel beim F.I.G.-Weltcup.

Im Sommer beherbergte das Sportzentrum in Cottbus von 1991 bis 2011 alljährlich das "Internationale Lausitzer Leichtathletik-Meeting", bei dem regelmäßig neue Rekorde in allen Disziplinen der Leichtathletik aufgestellt wurden. Bei den Männern liegt der Meetingrekord im 100-Meter-Lauf bei 10,00 Sekunden, bei den Frauen bei 11,14 Sekunden. Seit 2003 gibt es alljährlich Ende Januar das "Internationale Springer-Meeting" mit den Disziplinen Hochsprung der Frauen (Meetingrekord 2,01 m) und Stabhochsprung der Männer (Meetingrekord 5,90 m). Weitere leichtathletische Veranstaltungen in Cottbus sind das der "Lausitzer Citylauf" (2017 AOK City Run@Bike )sowie der "Spreewald-Marathon".

Seit 1999 gehört das "24 Stunden-Schwimmen" fest zum Sportkalender der Stadt. Dieses wird durch die Deutsche Lebens-Rettungs-Gesellschaft Cottbus traditionell im November durchgeführt. Mit 1876 Teilnehmern im Jahr 2016 gehört es zu den bundesweit größten Breitensportereignissen im Wassersport. Während der letzten Veranstaltung wurden insgesamt knapp 5500 Kilometer geschwommen.

Mit der "Drachenbootregatta" auf der Spree, dem "DAK-Firmen-Lauf" und dem "Frühlingsradeln" haben sich noch weitere Breitensport-Veranstaltungen in Cottbus etabliert.

Die Messen und Ausstellungen finden in der Messe Cottbus mit multifunktionalen Ausstellungsflächen von 6.500 m² statt. Die Messe liegt verkehrsgünstig am Stadtring und in unmittelbarer Nähe zum Spreeauenpark.

Jedes Jahr im Januar findet der "Cottbuser Reisemarkt" statt. Diese Messe bietet Angebote rund um Ferien, Tourismus, Freizeit, Caravan, Camping und Boote. Mit bis zu 250 Ausstellern und 15.000 Besuchern gehört diese Ausstellung zu den größten in Cottbus. Parallel dazu findet jedes Jahr die Ausstellung „"Fit+Gesund"“ statt, welche sich dem Themenbereich rund um Wellness, Kuren und Gesundheit widmet. Ende Januar beherbergt das Messegelände alljährlich eine Handwerkerausstellung. Hierbei handelt es sich um die einzige Ausstellung zum Handwerk in Südbrandenburg.

Die Messe "Impuls", welche jedes Jahr im Februar stattfindet, befasst sich mit Ausbildung, Weiterbildung, Existenzgründung, Unternehmenssicherung und Arbeit. Im März findet die Ausstellung "Cars & Bikes" in den Messehallen statt. Dies ist die größte Automobil- und Zweiradausstellung im Land Brandenburg. Mit 27.000 Besuchern im Jahr 2005 war diese Ausstellung die besucherstärkste seit der Bundesgartenschau 1995. Auf der "Tattoo-Convention" zeigen internationale Künstler ihr Können. Regelmäßig findet im März die Ausstellung "CottbusBau" statt, bei der es sich um die größte Baufachmesse im Land Brandenburg handelt.

Im Oktober beherbergt Cottbus die "Herbstmesse". Mit mehr als 330 Ausstellern ist dies die besucherstärkste Verbraucherausstellung im Land Brandenburg.

Weitere Veranstaltungen auf dem Messegelände sind die Ausstellungen "Vital & Co." und die "Erotik Messe".

Zu den alljährlich stattfindenden Großveranstaltungen zählen u. a. die "Altstadtnacht" (April), die "Cottbuser Umweltwoche" (Mai/Juni), die "Nacht der offenen Kirchen" (Pfingsten), das "Stadtfest" (Juni), die "Spreeauennacht" (August), der "Tag der Vereine" (August/September), das "Töpferfest" (September), der "Lausitzer Bauernmarkt" (September/Oktober), die "Nacht der kreativen Köpfe" (Oktober), das FilmFestival Cottbus – Festival des osteuropäischen Films (November) sowie der "Weihnachtsmarkt" (Dezember).

An jedem Tulpensonntag (Februar/März) findet zudem mit dem "Zug der fröhlichen Leute" der größte Karnevalsumzug in Ostdeutschland statt. Zahlreiche Motivwagen, Kapellen und Tanzgruppen von Karnevalsvereinen aus Cottbus und Umgebung ziehen jedes Jahr rund 100.000 Besucher an. Jährlich findet in der Stadthalle die Karnevalsgala "Heut steppt der Adler" statt, die vom Rundfunk Berlin Brandenburg aufgezeichnet wird.

In der Stadt gibt es derzeit(Stand: Schuljahr 2013/2014)zwölf Grundschulen, zwei Oberschulen, zwei Gesamtschulen, vier Gymnasien, drei Förderschulen sowie ein Oberstufenzentrum die sich in städtischer Trägerschaft befinden und auf die 9835 Schülerinnen und Schüler gehen. Daneben gibt es noch weitere Privatschulen u. a. auch eine Waldorfschule und ein evangelisches Gymnasium.

Mit dem "Pückler-Gymnasium" und dem "Oberstufenzentrum II Spree-Neiße" gibt es zudem noch zwei weitere öffentliche Schulen im Stadtgebiet, welche sich allerdings in Trägerschaft des Landkreises Spree-Neiße befinden.

In Cottbus gibt es nach der Fusion der Brandenburgischen Technischen Universität Cottbus und der Hochschule Lausitz zum 1. Juli 2013 zur Brandenburgischen Technischen Universität Cottbus-Senftenberg (BTU) nur noch eine staatliche Hochschule. Als Beauftragter für die Gründung wurde der Hochschulexperte Birger Hendriks ernannt.

Gegen das Vorhaben der Landesregierung zur Hochschulfusion hat sich seitens beider Hochschulen frühzeitig reger Widerstand gebildet. Eine von der Initiative "Hochschulen erhalten" gestartete Volksinitiative mit über 42.000 Unterschriften (davon rund 33.000 gültig) wurde vom Brandenburgischen Landtag mit den Stimmen der Regierungskoalition am 15. November 2012 abgelehnt. Als Reaktion auf diese Ablehnung wiederum, wurde seitens der Protestinitiative ein Volksbegehren gestartet. Hierfür wären 80.000 Unterschriften erforderlich gewesen, um das Gesetz erneut durch den Landtag prüfen zu lassen. Das Vorhaben ist per 9. Oktober 2013 am geringen Zuspruch von nur rund 18.000 Unterschriften durch die Brandenburger Wahlbevölkerung gescheitert.

Die BTU gliedert sich in sechs Fakultäten:


Die BTU unterhält des Weiteren mehrere Forschungszentren u. a.:

Zudem gibt es eine Fachschule für Wirtschaft und eine Medizinische Schule am Carl-Thiem-Klinikum, das ein Lehrkrankenhaus der Berliner Charité ist. Des Weiteren verfügt Cottbus über eine Volkshochschule und die "Schule für Niedersorbische Sprache und Kultur".

Öffentliche Bibliotheken in der Stadt sind die "Stadt- und Regionalbibliothek" in der Berliner Straße, mit einem Medienbestand von über 250.000 Einheiten () sowie die "Bücherei" in Sandow mit rund 6000 Einheiten.

Daneben unterhalten die BTU, mit dem "Informations-, Kommunikations- und Medienzentrum (IKMZ)", mit über 890.000 Medieneinheiten, wie auch die Hochschule Lausitz mit rund 180.000 Einheiten (davon ca. 80.000 in Cottbus) eigene Hochschulbibliotheken. Daneben gibt es noch weitere Fach- und Behördenbibliotheken, wie z. B. die des Carl-Thiem-Klinikums, des Landesamts für Arbeitsschutz oder des Finanzgerichts Berlin-Brandenburg.

Weitere besondere Bibliotheken sind die "Pückler-Bibliothek" im Schloss Branitz, als reine Präsenzbibliothek mit den Sammelschwerpunkten Literatur von und über Pückler, Garten- und Reiseliteratur sowie Kunst- und Kulturgeschichte des 19. und 20. Jahrhunderts, deutsche Geschichte des 19. Jahrhunderts und Regionalgeschichte sowie die "Niedersorbische Bibliothek" mit den Schwerpunkten Geschichte, Sprache, Kunst und Kultur der Sorben.

In Cottbus ist eine Vielzahl von Unternehmen ansässig. Dazu gehört z. B. die LEAG, welche von Cottbus aus die Verwaltung von Tagebauen und Kraftwerken in Ostdeutschland steuert. ABB ist ein Konzern der Elektrotechnik und ebenfalls in Cottbus ansässig. Die Deutsche Bahn besitzt in der Stadt ein Fahrzeuginstandhaltungswerk. Die envia Mitteldeutsche Energie AG (enviaM) ist ein regionaler Energie- und Kommunikationsdienstleister in Ostdeutschland. Einen Standort hat dieses Unternehmen auch in Cottbus. Die Deutsche Post AG betreibt in Cottbus eines ihrer 82 Briefzentren in Deutschland.

Die zehn größten Unternehmen 2013, nach Bilanzsumme :

Weitere Unternehmen kommen vorrangig aus den Bereichen Architektur, Chemie und Pharmazie, Dienstleistungen, Einzelhandel, Energie, Finanzwesen, Forschung, Gesundheitswesen, Handel, Maschinenbau und Telekommunikation.

Der Wirtschaftsstandort ist einer von 15 Regionalen Wachstumskernen im Land Brandenburg. Dadurch werden ausgewählte zukunftsorientierte Branchen gefördert.

Mit Stand vom 30. Juni 2017 gab es in Cottbus 45.718 Arbeitnehmer, welche sich wie folgt verteilen:

Die Arbeitslosenquote beträgt mit Stand vom Dezember 2017 8,5 %, das heißt es gab insgesamt 4.410 Arbeitslose. Die Stadt liegt damit über dem Landesdurchschnitt von 6,6 %. 

Durch das südliche Stadtgebiet von Cottbus führt die Bundesautobahn 15, die vom Dreieck Spreewald (A 13 Dresden–Berlin) kommend und als Teil der Europastraße 36 in Richtung Polen/Ukraine führt. Die Autobahn ist vierspurig und hat zwei Autobahnanschlussstellen in Cottbus: Cottbus-West und Cottbus-Süd. Cottbus wird ferner von den Bundesstraßen 97, 168 und 169 durchzogen. Dabei bildet die B 168 zugleich den südlichen und östlichen Teil des Stadtrings.

Mit der in Planung und Bau befindlichen Ortsumgehung im Osten der Stadt, welche durch die B 97n und B 168n gebildet werden sollen, soll künftig ein Großteil des Schwerlasttransports und Durchgangsverkehrs aus der Innenstadt ferngehalten werden. Ein 6,7 km langer erster Abschnitt der Ortsumgehung, von Peitz bis zur L 49 in Kahren, wurde am 3. September 2012 freigegeben. Der zweite Abschnitt befindet sich indes noch in der Planungsphase. Ein Baubeginn ist noch nicht bekannt. Mit Fertigstellung des zweiten Abschnitts wird Cottbus auch eine dritte Autobahnanschlussstelle (Cottbus-Ost) in Höhe des Ortsteils Kahren erhalten. Ein dritter Abschnitt soll außerhalb des Stadtgebiets von der Anschlussstelle nach Süden verlaufen und nördlich von Groß Oßnig in die B 97 münden. Ob der dritte Abschnitt tatsächlich realisiert wird, ist noch nicht abschließend entschieden.

Die Dichte an Privat-PKW liegt erheblich unter dem brandenburger Durchschnitt (2014:=510). Trotz fallender Bevölkerung verändert sich die Zahl der zugelassenen PKW kaum. Die private Motorisierung hat zugenommen, wenn auch weniger stark als im Landesschnitt.

Von Cottbus aus führen Bahnlinien in alle Richtungen: Regional-Express und Regionalbahn-Linien der DB Regio und der ODEG nach Wismar über Berlin sowie in Richtung Görlitz und Zittau (Bahnstrecke Berlin–Görlitz), Dresden (Bahnstrecke Priestewitz–Cottbus), über Finsterwalde und Falkenberg (Elster) nach Leipzig (Bahnstrecke Halle–Cottbus), Frankfurt (Oder) (Bahnstrecke Cottbus–Guben) und nach Forst (Lausitz) (Bahnstrecke Cottbus–Żary). Daneben gibt es auch täglich eine innerdeutsche Fernverbindung nach Emden und Norddeich Mole, außerdem gab es bis Dezember 2014 internationale Zugverbindungen in die polnischen Städte Wrocław (Breslau), Kraków (Krakau) und Żagań (Sagan). Es ist jedoch ab April 2016 geplant, die Verbindung mit Triebwagen der DB-Baureihe 628 wieder aufleben zu lassen. Neben dem Bahnhof Cottbus (im Volksmund und bei Cottbusverkehr "Hauptbahnhof" genannt) befinden sich noch drei weitere Bahnhöfe der Deutschen Bahn im Cottbuser Stadtgebiet:

Der Haltepunkt Kiekebusch wird seit 2006 nicht mehr bedient. Von 1898 bis 1970 existierte zudem eine Zugverbindung mit der Spreewaldbahn vom Cottbuser Spreewaldbahnhof über Burg nach Lübben. Die Gleise dieser Schmalspurbahn sind seit 1983 nahezu vollständig zurückgebaut. Lediglich das Empfangsgebäude des Spreewaldbahnhofs ist heute noch unweit des Hauptbahnhofs zu finden. Nach der Stadt ist der Intercity-Express "Cottbus/Chóśebuz" benannt.

Den öffentlichen Personennahverkehr (ÖPNV) bedienen Straßenbahnen und Busse der Cottbusverkehr GmbH und Neißeverkehr GmbH, beide Mitgliedsunternehmen im Verkehrsverbund Berlin-Brandenburg (VBB). Insgesamt gibt es 49 Linien, davon fünf Cottbuser Straßenbahn- und 44 Omnibuslinien. Das Liniennetz verfügt über 634 Haltestellen und es ist ca. 934 km lang. Insgesamt sind 21 Straßenbahnen und 54 Omnibusse im Einsatz.

Außerdem verbindet eine Parkbahn (600 mm Spurweite) den Bahnhof Cottbus-Sandow mit dem Fußballstadion Stadion der Freundschaft, dem Messezentrum, dem Tierpark Cottbus und dem Branitzer Park. Der Verkehr beschränkt sich auf die Sommermonate.

Cottbus verfügt über zwei regionale Flugplätze. Der Verkehrslandeplatz Neuhausen ist ca. 15 km entfernt. Mit 16.000 bis 20.000 Flugbewegungen pro Jahr gehört der Flugplatz zu den fünf größten Verkehrslandeplätzen in Brandenburg. Angeboten werden europaweite Charter- und Taxiflüge, Ausbildungen sowie Rund- und Wirtschaftsflüge mit dem Flugzeug und Hubschrauber. Luftsport, wie beispielsweise Fallschirmspringen und Segelfliegen, wird durch ortsansässige Vereine angeboten.

Auch der ca. 25 km entfernte Flugplatz Cottbus-Drewitz bietet europaweite Charter- und Taxiflüge an. Der ehemalige Flugplatz Cottbus-Nord wurde von der NVA genutzt und ist heute geschlossen. Dort befindet sich nun der "Technologie- und Industriepark (TIP)", ein rund zwei Quadratkilometer großes Gewerbegebiet, in Planung und Aufbau.

In größerer Entfernung zu Cottbus liegen die drei internationalen Flughäfen:

Im Städteranking beim Fahrradklimatest 2012 des ADFC konnte Cottbus in der Kategorie Städte mit 100.000 bis 200.000 Einwohnern den fünften Platz (von 42) direkt nach der Landeshauptstadt Potsdam erringen. Nach einer Erhebung der Stadt machen Fahrradfahrer mittlerweile 22 Prozent des Gesamtverkehrsaufkommens aus.

Fernwanderwege

Radfernwege

Die Stadt wird von einigen bedeutenden Radfernwegen durchlaufen und ist Ausgangspunkt einiger Fahrradtouren in den Spreewald oder die Tagebau-Region. Folgende Fahrradwege durchlaufen Cottbus:

Cottbus ist Sitz eines Regionalstudios des "Rundfunk Berlin-Brandenburg" (RBB). Bereits seit dem 2. August 1925 erfolgten regelmäßige Sendungen des Deutschen Radio-Klubs aus Cottbus. Sie hatten eine Empfangsreichweite von bis zu 600 km. Im Jahr 1946 wurde das Studio Cottbus des Landessenders Brandenburg gegründet. Es strahlte ausschließlich Rundfunkprogramme aus. Die DDR-Regierung unterstellte das Studio ab dem Jahr 1952 dem Nationalen Komitee für Rundfunk. In der Folgezeit wurden tägliche bis zu sechsstündige, regionale Hörfunksendungen auf Radio DDR II in deutscher und niedersorbischer Sprache produziert. Im Jahr 1989 erhielt das Studio Cottbus ein eigenes Außenstudio in Bautzen für die Produktion von Hörfunksendungen in obersorbischer Sprache. Das Studio Bautzen wurde am 1. Januar 1991 vom MDR übernommen.

Im Frühjahr 1990 gründeten die Regionalstudios von Radio DDR II in Cottbus, Potsdam und Frankfurt (Oder) das gemeinsame Hörfunkprogramm "Antenne Brandenburg". Am 1. Januar 1992 wurden Programm und ein Teil der Redaktionen vom "Ostdeutschen Rundfunk Brandenburg" (ORB) und später von dessen Rechtsnachfolger RBB übernommen. In seinen Cottbuser Hörfunkstudios produziert der RBB täglich rund fünf Stunden Hörfunksendungen in deutscher und für das "Bramborske Serbske Radijo" in niedersorbischer Sprache.

Das RBB-Regionalstudio Cottbus mit seinen rund 50 Mitarbeitern fertigt außerdem Fernsehsendungen und einzelne Fernsehbeiträge. So werden in Cottbus u. a. die Sendungen "RBB regional", "THEODOR – Geschichte(n) aus der Mark" und die niedersorbischsprachige Sendung "Łužyca" ("Lausitz") produziert . Darüber hinaus erfolgen Zulieferungen für "Das Erste", die dritten Fernsehprogramme sowie Produktionshilfen für das ZDF.

Regelmäßig erfolgt seit der Schließung des alten Berliner Friedrichstadtpalastes im Jahr 1980 die Produktion großer Fernsehshows in Cottbus, dazu gehören beispielsweise: "Ein Kessel Buntes" (DFF), "Die Goldene Note" (DFF), "Musikanten sind da" (DFF), "Melodien für Millionen" (ZDF), "Musikantenstadl" (DFF/ARD/SRF/ORF), "Fest der Volksmusik" (ARD) und "Musik für Sie" (MDR). Fernsehgeschichte schrieb der Entertainer Harald Juhnke mit seinem überraschenden Auftritt in der ersten deutsch-deutschen Unterhaltungsshow "Musikantenstadl" am 17. Dezember 1989. Jeweils im Januar produziert der RBB in Cottbus für die ARD die Karnevals-Sendung "Heut’ steppt der Adler".

Neben dem öffentlich-rechtlich organisierten Rundfunk sind in Cottbus auch private Radiosender vertreten. Ganz auf Cottbus und Umgebung fokussiert sendet der Sender 94.5 Radio Cottbus. Mit zeitweise regionalen Programmfenstern bzw. Nachrichten senden "BB Radio" und "94,3 rs2". Daneben lassen sich noch weitere Sender über UKW wie "Radio B2" und der "Berliner Rundfunk 91.4" empfangen. Mit "Lausitz TV" (LTV) gibt es zudem einen im Kabelnetz frei empfangbaren Lokalfernsehsender.

In Cottbus und Umgebung erscheint die regionale Tageszeitung "Lausitzer Rundschau", in Monopolstellung. Die Zeitung "20cent" gehörte, über den Saarbrücker Zeitungsverlag als Mutterunternehmen, zur Lausitzer Rundschau. Sie wurde zum 28. Februar 2009 eingestellt. "Der Märkische Bote", die "Lausitzer Woche" und der "Wochenkurier" sind Anzeigenblätter der Region. Außerdem erscheinen die Veranstaltungsmagazine "Blicklicht" und "Hermann". Während die Zeitung "Konturmagazin" auf eine jugendliche Zielgruppe abzielt, erscheint die Wochenzeitung "Nowy Casnik" in niedersorbischer Sprache für die Lausitzer Minderheiten der Wenden und Sorben.

Cottbus war von 1868 bis 1918 Garnisonsstadt der preußischen Armee. 1886 war die neu erbaute "Alvensleben-Kaserne" vom "Infanterie-Regiment 52" bezogen worden. Nach einer Unterbrechung in der Zwischenkriegszeit wurde Cottbus bei der Aufrüstung der Wehrmacht erneut Militärstandort, als 1938 der Neubau "Hermann-Löns-Kaserne" vom Heer bezogen wurde. Sie war nach dem Krieg bis 1958 von den sowjetischen Truppen belegt und wurde anschließend an die Nationale Volksarmee übergeben. In der nun "Paul-Hornick-Kaserne" genannten Unterkunft war bis 1990 das Panzerregiment-15 untergebracht.

Außerdem befand sich im 1950 eingemeindeten südlichen Vorort Sachsendorf ein ebenfalls in den 1930er Jahren für die Wehrmacht errichtetes Kasernenareal, das bis in die 1980er Jahre von den sowjetischen Truppen (zuletzt: 35. Luftsturmbrigade) belegt blieb.

Berühmte Persönlichkeiten aus Cottbus sind unter anderem der Maler Carl Blechen, der Physiologe Gustav Theodor Fritsch, der Résistance-Angehörige Adolphe Low, die Leichtathletin Ulrike Bruns, der Diskuswerfer Robert Harting sowie der Radrennfahrer Tony Martin.



Denkmaltopographie:





</doc>
<doc id="13668" url="https://de.wikipedia.org/wiki?curid=13668" title="Der liebe Augustin">
Der liebe Augustin

Der liebe Augustin steht für:



</doc>
<doc id="13669" url="https://de.wikipedia.org/wiki?curid=13669" title="Paganini (Operette)">
Paganini (Operette)

Paganini ist eine Operette in drei Akten von Franz Lehár. Sie entstand 1925 mit Texten von Paul Knepler und Bela Jenbach. Lehár setzte hier dem Violinvirtuosen Niccolò Paganini ein Denkmal. Die Uraufführung fand am 30. Oktober 1925 im Johann Strauß-Theater in Wien statt. Die deutsche Premiere fand im Januar 1926 mit Richard Tauber (Paganini) und Vera Schwarz (Anna Elisa) am Deutschen Künstlertheater in Berlin statt.


Fürstentum Lucca, Anfang des 19. Jahrhunderts

1. Akt: Idyllische Gegend in der Nähe des Dorfes Capannori bei Lucca.
Paganini hat auf seiner Konzertreise hier Station gemacht. Die Dorfbewohner hören staunend den Geiger üben, dessen dämonische Virtuosität manchen von ihnen unheimlich ist. Auf einem Jagdausflug kommt die Fürstin Elisa zufällig hier vorbei und vernimmt das erregende Geigenspiel. Sie ist Napoleons Schwester, eine schöne, leidenschaftliche, Glanz und Genuss liebende junge Frau. An der Seite ihres Gatten, des Fürsten Felice, fühlt sie sich wenig glücklich, denn ihn fesseln stets andere Frauen. Auch heute ist er wohl wieder bei der Opernsängerin Bella Giretti. Mag er immerhin tun, was ihm beliebt – aber wie sehnt sie selbst sich nach einem rechten Mann „voll Mark und Seele!“ Da kommt eben Paganini aus seinem Zimmer, stimmt die aufgeregten Bauern durch seine Liebenswürdigkeit ganz zu seinen Gunsten und singt bei einem Becher Wein ein Loblied auf die Glückselemente seines Lebens: die Heimat, die Kunst, die Frauen! Er unterhält sich mit Elisa, die sich nicht zu erkennen gibt. Das Gespräch wird von Paganinis Impresario Bartucci unterbrochen, der die Nachricht bringt, dass der Künstler wegen des Verdachts, einen Mann im Zweikampf getötet zu haben, in Lucca nicht auftreten dürfe. Zornig lehnt es Paganini ab, sich zu rechtfertigen, und will sofort abreisen. Elisa, entzückt von seinem Temperamentsausbruch, hofft, ihn zurückhalten zu können. Ihr Charme bezaubert ihn so, dass er sogleich stürmisch versucht, sie zu küssen. Da erfährt er durch Huldigungen des Landvolks, wer die fremde Dame ist. Auch Fürst Felice taucht nun auf und verbietet aufs Neue das Auftreten des „Abenteurers“. Aber Elisa, die ihm ja seine Affäre mit Bella Giretti nachsehen muss, setzt ihren Willen durch: Paganini wird spielen!

2. Akt: Festsaal im Schloss zu Lucca.
Elisas Liebe hält Paganini schon seit sechs Monaten in Lucca fest. Beim Glücksspiel hat er soeben seine kostbare Geige eingesetzt und verloren. Der Kammerherr Pimpinelli verschafft sie ihm wieder, erbittet jedoch dafür sein „Rezept“, schöne Frauen zu erobern. Aber Paganini weiß nur: "Gern hab ich die Fraun geküsst, hab nie gefragt, ob es gestattet ist." Elisa lebt in Angst, Paganini wieder zu verlieren; er beschwichtigt sie aber mit einem soeben komponierten Liebeslied. Auch den Fürsten quält die Eifersucht, da er Bellas Sympathien für Paganini bemerkt hat. Dem Impresario Bartucci gefällt es gar nicht, dass der Künstler, dem die ganze Welt zu Füßen liegen könnte, hier an dem kleinen Fürstenhof seine Zeit vergeudet. Auch fürchtet er einen Skandal, wenn Paganini jetzt dem Fürsten nach der Gattin womöglich noch die Geliebte wegnimmt. Paganini muss ihm recht geben, doch vermag er sich dem Zauber Elisas noch nicht zu entziehen. Da kommt von außen ein Anstoß zur Änderung der Verhältnisse: Graf Héouville übermittelt Elisa den Befehl Napoleons, Paganini sofort zu entlassen, denn man klatscht bereits in Paris über ihre Beziehung zu dem Geiger. Elisa wehrt sich gegen die Trennung – aber der Geliebte ist ihr ja schon entfremdet: zärtlich nähert er sich Bella und widmet ihr sogar das zuerst der Fürstin zugedachte Liebeslied. Eifersüchtig will Elisa die Sängerin zur Abreise nötigen, da zeigt ihr diese triumphierend Paganinis Geschenk. Jetzt will sich die Fürstin rächen und ihn sogar verhaften lassen. Als er sie jedoch am Abend durch sein Spiel aufs Neue betört, stellt sie sich schützend zwischen ihn und die Schergen.

3. Akt: In einer Schmugglerschenke an der Grenze des Fürstentums.
Paganini ist vom Hofe geflohen und will mit Hilfe der Schmuggler rasch über die Grenze. Bartucci hat ihn eingeholt und mahnt ihn, diesmal nicht vergeblich, von den Frauen zu lassen und nur seiner Kunst zu leben. So bleibt er, als die verliebte Bella, die ihm nachgereist ist, vor ihm erscheint, standhaft, und Bella tröstet sich bald mit ihrem Begleiter Pimpinelli. Als Straßensängerin verkleidet, kommt auch Elisa noch einmal zu Paganini, aber nur, um Abschied zu nehmen und ihn freizugeben.


Die Operette wurde 1934 unter dem Titel "Gern hab’ ich die Frau’n geküßt" von E. W. Emo verfilmt.
Eugen York verfilmte "Paganini" 1973 für das ZDF. Darsteller waren Antonio Theba (Paganini), Teresa Stratas (Fürstin Anna Elisa), Johannes Heesters (Fürst Felice Bacciocchi), Dagmar Koller (Bella Giretti), Peter Kraus (Pimpinelli), Fritz Tillmann (Conte Carcasona), Wolfgang Lukschy (Graf Hedouville).


</doc>
<doc id="13670" url="https://de.wikipedia.org/wiki?curid=13670" title="Bolero">
Bolero

Bolero steht für:

Bolero steht für:

Bolero steht für:

Bolero steht ferner für (Dazu sind allerdings noch keine Wikipedia-Artikel vorhanden):
Siehe auch:


</doc>
<doc id="13671" url="https://de.wikipedia.org/wiki?curid=13671" title="Jan Hus">
Jan Hus

Jan Hus (nach seinem wahrscheinlichen Geburtsort Husinec, Prachiner Kreis, Königreich Böhmen; * um 1370; † 6. Juli 1415 in Konstanz), auch "Johannes Hus(s)" genannt, war ein böhmischer christlicher Theologe, Prediger und Reformator. Er war zeitweise Rektor der Karls-Universität Prag. Nachdem er während des Konzils von Konstanz seine Lehre nicht widerrufen wollte, wurde er auf dem Scheiterhaufen verbrannt. Die nach Jan Hus benannte Bewegung der Hussiten geht zum Teil auf sein Wirken zurück. In Tschechien gilt Hus als „Nationalheiliger“.

Neuere Forschungen geben den 1. Juli 1372 als Geburtsdatum an.

Jan Hus war der Sohn von Joh. Joseph Huß (* 1330 in Husinec) und dessen Ehefrau von Kowisckcy. Er hatte zwei Brüder namens Hironimus (* 1370) und Benedictus (* 1382). Jan Hus, dessen Vater vermutlich Fuhrmann war, besuchte die Lateinschule in der Handelsstadt Prachatice in Westböhmen und studierte ab ca. 1390 in Prag. Nach dem Studium an der Karls-Universität Prag erlangte er 1396 den akademischen Grad eines Magister Artium, wurde Hochschullehrer und gilt als Verfasser des anonymen Traktats "Orthographia Bohemica", in dem erstmals das diakritische System der tschechischen Rechtschreibung vorgeschlagen wurde (mit dem Akut für lange Vokale und dem Punkt für weiche Konsonanten).

Durch Hieronymus von Prag wurde Hus ab 1398 mit den Lehren des Oxforder Theologen John Wyclif vertraut, die er begeistert aufnahm. Tschechische Adelige, die seit der Vermählung der Schwester König Wenzels, Anne von Böhmen, mit Richard II. von England (1382) an der Universität Oxford studierten, brachten von dort Wyclifs Schriften nach Prag – zuerst die philosophischen, später auch die theologischen und kirchenpolitischen. Wyclif forderte aufgrund der sittlichen Verfallserscheinungen des Klerus in England und in Böhmen die Abkehr der Kirche von Besitz und weltlicher Macht.

Jan Hus begann 1398 Theologie zu studieren und wurde 1400 zum Priester geweiht. 1401 wurde er zum Dekan der philosophischen Fakultät ernannt. 1402 wurde er Professor und übte das Amt des Rektors der Prager Universität 1409–1410 aus. Dort lehrte er Theologie und Philosophie.

Ab 1402 predigte Hus in tschechischer Sprache in der Bethlehemskapelle in der Prager Altstadt und führte das gemeinsame Singen während des Gottesdienstes in der tschechischen Landessprache ein. Er hielt dort jährlich rund 200 Predigten auf Tschechisch und förderte so auch das tschechische Nationalbewusstsein. Hus, der zunächst unter Erzbischof Zbynko Zajíc von Hasenburg großes Ansehen genoss, wurde von diesem mehrfach zum Synodalprediger bestimmt. Er wurde Beichtvater der Königin Sophie von Bayern. Hus predigte eine strenge, tugendhafte Lebensweise und eiferte gegen Zeitgeist und Mode, so dass er gelegentlich die Zünfte der Schuster, Hutmacher, Goldschmiede, Weinhändler und Wirte gegen sich aufbrachte.

Beeinflusst durch die Lehren Wyclifs, kritisierte er den weltlichen Besitz der Kirche, die Habsucht des Klerus und dessen Lasterleben. Er kämpfte leidenschaftlich für eine Reform der verweltlichten Kirche, trat für die Gewissensfreiheit ein und sah in der Bibel die einzige Autorität in Glaubensfragen, im Gegensatz zu der Doktrin der Amtskirche, dass der Papst die letzte Instanz bei Glaubensentscheidungen sei. Von John Wyclif übernahm Hus zudem die Lehre der Prädestination und setzte sich für die Landessprache als Gottesdienstsprache ein.

1408 erfuhr der Prager Erzbischof von Hus’ Predigten und enthob ihn daraufhin seiner Stellung als Synodalprediger. Das Lesen der Messe und das Predigen wurden ihm verboten. Er hielt sich aber nicht an diese Verbote, predigte weiterhin gegen Papsttum und Bischöfe und brachte in kurzer Zeit große Teile Böhmens auf seine Seite.

Um der Reformbestrebungen Herr zu werden, unterwarf sich der Prager Erzbischof Alexander V., einem der damaligen drei Päpste, und erwirkte von ihm eine Bulle, die die Auslieferung der Schriften Wyclifs und den Widerruf seiner Lehren forderte. Außerdem sollte das Predigen außerhalb der Kirchen verboten werden. Nachdem diese Bulle am 9. März 1410 veröffentlicht wurde, ließ der Erzbischof über 200 Handschriften Wyclifs öffentlich verbrennen und verklagte Jan Hus in Rom. Hus, der sich dort erfolglos durch Abgesandte vertreten ließ, wurde daraufhin im Juli 1410 mit dem Kirchenbann belegt. Gegenpapst Johannes XXIII. bannte ihn im Februar 1411. Hus wurde exkommuniziert und der Stadt Prag verwiesen. Als Folge davon brachen in Prag Unruhen aus.

Aufgrund seiner Beliebtheit, die in Volksdemonstrationen gipfelte, lehrte Hus unter dem Schutz des Königs zunächst noch ein Jahr weiter. Er verurteilte nun die Kreuzzugs- und Ablassbullen von Johannes XXIII. 1412 jedoch musste Hus fliehen.

Böhmen war das einzige Königreich im Heiligen Römischen Reich. Prag war zu Hus’ Zeit kaiserliche Residenzstadt. Neben dem Deutschen König und/oder „Römischen“ Kaiser gab es also den Böhmischen König, wenn diese Würden nicht gerade in Personalunion zusammenfielen.

Als die Prager Karls-Universität zum Abendländischen Schisma Stellung nehmen sollte, war Hus Wortführer der Tschechen. Die Universität war nach den vier „Nationalitäten“ Bayern, Sachsen, Polen und Böhmen gegliedert. König Wenzel hatte sich seit 1408 bereiterklärt, das Konzil von Pisa, das das päpstliche Schisma zu überwinden suchte, zu unterstützen, ebenso wie die böhmische Nation der Universität. Die deutschen Nationen sowie Erzbischof Zbyněk hingegen hielten an ihrer römischen Obedienz fest. Die Fronten verhärteten sich, als sich die Magister der böhmischen Nation zum Wyclifschen Realismus bekannten, der die philosophische Grundlage für die theologische Kritik Hussens und anderer böhmischer Theologen bildete.

Diese Oppositionsbildung führte schließlich zum Kuttenberger Dekret von 1409, das die Stimmenverteilung an der Universität grundlegend änderte. Mit einer Stimmenmehrheit der deutschen Nationen wäre eine neutrale Position gegenüber den beiden Päpsten in Avignon und Rom nicht durchzusetzen gewesen. Wenzel erteilte daher den Böhmen drei Stimmen, den Bayern, Polen und Sachsen zusammen dagegen nur eine. Die Tschechen erklärten sich zusammen mit König Wenzel für neutral, während die Deutschen zusammen mit Erzbischof Zbyněk an Gregor XII. festhielten.

Neben Jan Hus hatte Hieronymus von Prag, der 10 Monate nach Hus auf dem Konzil von Konstanz als Häretiker verbrannt wurde, wesentlichen Einfluss auf die Durchsetzung des Dekrets.
Zum ersten Mal spielten bei einem Aufbegehren des tschechischen Volkes nationalistische Motive eine Rolle, die maßgeblich für die Ausbildung des Hussitischen Engagements waren. Infolge des Kuttenberger Dekrets verließen wenigstens 1000 deutsche Studenten mit ihren Professoren Prag und veranlassten die Gründung der Universität Leipzig.

Als der Gegenpapst Johannes XXIII. einen neuen Kreuzzug gegen den König von Neapel verkündete und jedem „Kreuzträger“ vollkommenen Ablass versprach, verurteilte Hus öffentlich diese Praxis, wodurch er großen Zulauf erfuhr. Jedoch zerbrach dadurch endgültig das Verhältnis zum König, der selbst finanzielle Interessen am geplanten Ablasshandel hatte.
In Prag brachen neue Unruhen aus, als am 14. Juli 1412 drei junge Männer, die sich öffentlich gegen den Ablasshandel gewandt hatten, hingerichtet wurden. In der Reformbewegung wurden sie sofort als Märtyrer verehrt.

Aufgrund des größer werdenden Drucks floh Hus 1412 aus Prag und lebte bis 1414 auf der Ziegenburg in Südböhmen und auf der Burg Krakovec in Mittelböhmen. Dort verfasste er mehrere seiner Werke und leistete damit einen wesentlichen Beitrag zur Weiterentwicklung der tschechischen Schriftsprache. In dieser Zeit setzte er seine Mitwirkung an der Bibelübersetzung in die Landessprache fort (eine neue vollständige Übersetzung des Alten Testaments und Überarbeitung von älteren Übersetzungen des Neuen Testaments entstand in seiner Umgebung). Erste Veröffentlichung der neuen Textteile erfolgte in seinem Werk "Postila" (1413).

Hus begab sich nun nach Husinec, seinem Geburtsort. In dieser Phase verfasste er zahlreiche Schriften und Pamphlete. Er erreichte, dass der mit der Kirche in Widerspruch liegende Teil des böhmischen Adels ihn und seine Anhänger schützte. Einige hatten sich für den Fall, seine Ideen seien erfolgreich, vermutlich auch Hoffnungen auf die Kirchenbesitztümer gemacht, weil der Klerus nach Wyclifs Lehren bei Unwürdigkeit zu enteignen sei.

Hus durchzog das Land als Wanderprediger und fand zahlreiche Anhänger. 1413 schrieb Hus "De Ecclesia" ("Über die Kirche"). Darin vertrat er die Ansicht, dass die Kirche eine hierarchiefreie Gemeinschaft sei, in der nur Christus das Oberhaupt sein könne. Ausgehend vom augustinischen Kirchenbegriff, definierte er die Kirche als Gemeinschaft der Prädestinierten, also aller von Gott erwählten Menschen. In der sichtbaren Kirche gebe es jedoch zudem auch die nicht erwählten Menschen, die das "corpus diaboli" bildeten. Hus vertrat die Ansicht, dass viele Häupter der Kirche in Wahrheit Glieder des Teufels seien.

Die Unruhen und theologischen Streitigkeiten in Böhmen beschäftigten auch das Konzil von Konstanz ab 1414. Es galt, den Ruf des Landes wiederherzustellen und sich vom Vorwurf, Häresie zu dulden, zu befreien.
Der deutsche König Sigismund sicherte Hus freies Geleit (einen "salvus conductus" für Hin- und Rückreise und die Zeit des Aufenthalts) zu und stellte ihm einen Geleitbrief in Aussicht. Hus machte sich aber schon vorher auf den Weg, um seine Ansichten vor dem Konzil darzustellen. Trotz seiner Exkommunizierung und dem gegen ihn ausgesprochenen Großen Kirchenbann wurde er auf seinem Weg nach Konstanz überall freundlich empfangen. Er erreichte am 3. November Konstanz. Der Papst hob am 4. November 1414 die Kirchenstrafen gegen ihn auf. Zunächst predigte er drei Wochen in einer Herberge in der St.-Pauls-Gasse – heute Hussenstraße. (Der Standort der Herberge lässt sich nicht mehr eindeutig klären. Das heutige Hus-Museum Konstanz ist in einem Haus aus der damaligen Zeit untergebracht.)

Am 28. November wurde er zur Bischofspfalz beim Münster gebracht und im Haus des Domkantors eine Woche gefangengehalten.
Am 6. Dezember wurde er in einen halbrunden Anbau des Dominikanerklosters auf der Dominikanerinsel im Verlies festgesetzt. Hier durchlebte er einige qualvolle Wochen. Bei Tage wurde er gefesselt und nachts in einen Verschlag gesperrt. Er war dem Gestank einer Kloake ausgesetzt, wurde schlecht ernährt und war von Krankheit gepeinigt. Da mit seinem Tode nicht gedient war – er sollte seine Lehren widerrufen –, wurde er ab 24. März 1415 in ein etwas erträglicheres Quartier, den Barfüßerturm an der späteren Stefansschule, verlegt. Danach wurde er im Gefängnisturm des Schlosses Gottlieben eingekerkert.

Als Sigismund am 24. Dezember 1414 eintraf, gab er sich über den Bruch des Geleitbriefes zornig, tat aber nichts, um Hus zu helfen. Da er die böhmische Krone seines Bruders Wenzel beerben wollte, war ihm stärker daran gelegen, den Ruf Böhmens zu rehabilitieren. Die Geleitzusage Sigismunds wurde für nichtig erklärt, da Hus seine Ansichten nicht zurücknehmen wolle und deshalb nicht mehr die weltliche Ordnung für ihn zuständig sei, sondern die kirchliche (nach damaliger Auslegung war die Zusage ohnehin nichtig, da es gegenüber einem Häretiker keine verpflichtende Zusage geben konnte).

Im März 1415 floh Papst Johannes XXIII., als dessen Gefangener Hus galt, aus Konstanz. Hus kam am 24. März in den Gewahrsam des Bischofs von Konstanz. Papst Johannes XXIII. wurde bald selbst gefangen genommen, nach Konstanz zurückgebracht und selbst im Schloss Gottlieben eingekerkert.

Am 4. Mai 1415 verdammte das Konzil auch John Wyclif und seine Lehre. Da Wyclif zum Zeitpunkt der Verurteilung bereits 30 Jahre tot war, konnte das Urteil nicht mehr vollstreckt werden. Dafür wurde die Verbrennung seiner Gebeine angeordnet und 1428 tatsächlich durchgeführt.

Hus kam am 5. Juni in das Franziskanerkloster. Dort verbrachte er die letzten Wochen seines Lebens. Vom 5. bis 8. Juni wurde Hus im Refektorium des Klosters verhört. Hus unterstützende böhmische und mährische Adlige erreichten, dass er auf dem Konzil sich und seine Lehren in aller Öffentlichkeit zumindest ansatzweise verteidigen konnte. Das Konzil verlangte von ihm den öffentlichen Widerruf und die Abschwörung seiner Lehren. Hus lehnte dies ab und blieb auch bis Ende Juni standhaft.

Am Vormittag des 6. Juli 1415 wurde Hus in feierlicher Vollversammlung des Konzils im Dom, dem späteren Konstanzer Münster, auf Grund seiner Lehre von der „Kirche als der unsichtbaren Gemeinde der Prädestinierten“ als Häretiker zum Feuertod verurteilt. Beteiligt am Konzil im Dom waren als Repräsentanten der weltlichen Mächte König Sigismund, Friedrich von Hohenzollern, Ludwig III. von der Pfalz und ein ungarischer Magnat. Die Beteiligten am kirchlichen Schuldspruch waren der Kardinalbischof von Ostia, der Bischof von Lodi, der Bischof von Concordia und der Erzbischof von Mailand. Da Papst Gregor XII. zuvor abgedankt hatte und Papst Johannes XXIII. (Gegenpapst) kurz zuvor abgesetzt worden war, erfolgte die Verurteilung ohne päpstliche Beteiligung.

Hus wurde der weltlichen Gewalt übergeben. Der Weg führte vom Münster über die heutige Wessenbergstraße (damals noch Plattengasse), den Obermarkt und das Paradieser Stadttor ein kurzes Stück Richtung Gottlieben zum Brühl. Kurz vor der Hinrichtung kam Reichsmarschall Haupt II. von Pappenheim angeritten und forderte Hus im Namen von König Sigismund zum letzten Mal zum Widerruf auf. Hus weigerte sich. „Der Reichsmarschall schlug zum Zeichen der Exekution in die Hände. Die Fackel wurde an den Holzstoß gelegt“. Im Auftrag des Königs vollstreckte Pfalzgraf Ludwig das als Reichsgesetz geltende Urteil. Jan Hus wurde am Nachmittag des 6. Juli 1415 auf dem Brühl, zwischen Stadtmauer und Graben, zusammen mit seinen Schriften verbrannt. Die Phasen der Hinrichtung beschrieb Ulrich Richental in seiner Chronik. Seine Asche streuten die Henker in den Rhein. Seit 1863 erinnert ein Gedenkstein am mittelalterlichen Richtplatz am Ende der danach benannten Sackgasse "Zum Hussenstein" daran.

In seinem Abschiedsbrief hatte Hus an seine Freunde geschrieben:
Die Verurteilung von Jan Hus fiel in eine Zeit, in der um die weltliche und um die kirchliche Vormachtstellung mit allen Mitteln gekämpft wurde.

Sigismund gewann den Machtkampf gegen seinen Vetter Jobst von Mähren nach dem Tod König Ruprechts. Drei Papstanwärter kämpften um den Anspruch, Papst zu sein: Gregor XII. in Rom, Benedikt XIII. in Avignon sowie Alexander V. (nach ihm Johannes XXIII.) in Pisa. Die Machtfragen wurden geregelt, die unter anderem von Hus eingeforderten Reformen wurden jedoch nicht durchgeführt. Die bestehenden Ordnungen galten nach der Absetzung des Papstes Johannes XXIII. und der Hinrichtung von Jan Hus mit der Wahl des neuen Papstes Martin V. im Konzilsgebäude am Hafen von Konstanz im Jahr 1417 als bestätigt.

Die Hinrichtung löste den ersten Prager Fenstersturz und die Hussitenkriege (1419–1434) aus. Fünf Kreuzzüge wurden gegen die aufständischen Taboriten entsandt. Die Kriege verwüsteten in der ersten Hälfte des 15. Jahrhunderts nicht nur Böhmen und Mähren, sie griffen auch auf die Nachbarländer über, bis die Hussiten zuerst durch Zugeständnisse, später auch durch innere Zerrüttung besiegt wurden.

Hus war stark beeinflusst von den Lehren John Wyclifs. In seinen überwiegend kompilatorischen Schriften sind Wyclifs Anschauungen zum Teil wörtlich wiedergegeben, was der Schriftstellermanier des Mittelalters durchaus entsprach. Einiges hat Hus von Wyclif auch nicht übernommen. So hielt er an der Messe, der Lehre von der Transsubstantiation und der Lehre vom Fegefeuer fest, lehnte die Fürbitte der Maria und der Heiligen jedoch ab.

Nach Jan Hus ist die Kirche die Gesamtheit aller Prädestinierten (der Vorherbestimmten) ("ecclesia est universitas praedestinatorum"). Ihre Prädestination macht sie zu Mitgliedern der heiligen Kirche. Christus ist das Haupt – und kein Haupt außer ihm – der Kirche, das ihr selbst und jedem einzelnen Mitglied geistliches Leben vermittelt. Es gibt nach Hus von Anfang an nur "eine" Kirche, deren Mitglieder vorherbestimmt sind und nicht vor dem Tag des Gerichtes Gottes bekanntwerden. Für Hus ist der Begriff Kirche vorwiegend ein geistlicher und weniger ein institutioneller.

Hus unterscheidet zwischen Kirchenmitgliedern der Sache und dem Namen nach. Ein Mitglied der Institution Kirche muss nicht zu den Prädestinierten gehören, genauso wie ein Nichtmitglied der Institution Kirche zur geistlichen Kirche der Prädestinierten gehören kann. Ein Mensch zeigt seine Prädestination durch sein Verhalten.

Hus teilt die Kirche in drei Teile ein: Das Volk, die weltliche Herrschaft und den Klerus. Der weltlichen Herrschaft komme die Aufgabe zu, die Diener Gottes zu beschützen und das Gesetz Gottes zu verteidigen. Die Diener Gottes sollen „die Welt verbessern, die Kirche beleben als die Seele derselben und nach allen Seiten Christus am nächsten folgen“.

Hus verlangt von einem Geistlichen ein wahrhaftiges und heiliges Leben mit dem Ziel, den Gläubigen zu dienen. Er beklagt, dass die Geistlichen seiner Zeit Gott verachteten und durch Gewinnsucht und Heuchelei die Kirche in Verruf brächten. Statt dem Volke zu helfen – so Hus –, berauben sie es, statt es zu verteidigen, unterdrücken sie es noch grausamer als die weltlichen Herren.

Die Geistlichkeit habe die Aufgabe, das Evangelium zu verkünden und dem Volk mit den Sakramenten zu dienen. Auch hier sieht Hus den Gegensatz zur damaligen Priesterschaft, welche nach seinen Worten nicht aus „göttlichem Trieb“ predige, sondern um des Gewinnes willen. Viele forderten Geschenke oder Geld für Salbung, Taufe, Kommunion, Ordination, Konsekration der Altäre und Begräbnisse. Hus kritisiert den Ablasshandel, erfundene Reliquien, Bilderverehrung und erfundene Wunder. Die Gnade Gottes dürfe nicht käuflich sein.

„Die Priester predigen wohl gegen unsere Unzucht und unsere Laster“, so beklagt Hus, „aber von den ihrigen sagen sie nichts, also ist es entweder keine Sünde, oder sie wollen das Privilegium haben“. Die Geistlichen, die im Heer der Gläubigen in vorderster Linie stehen, müssen nach seiner Auffassung auch von allen übrigen Gläubigen ermahnt und bestraft werden können, wenn sie irren oder sündigen.

Für Hus war der Begriff Papst genauso wenig ein institutioneller wie sein Begriff der Kirche. Nicht das Amt, sondern das Verhalten befähige einen Papst. Er wandte sich gegen Lehren, dass dem Papst unbegrenzte Autorität zukomme, dass er weder Gott noch Mensch sei, dass der Papst einen Bischof ohne Grund absetzen dürfe und dass er von apostolischen Vorschriften in der Bibel Abstand nehmen dürfe. Mit „der heiligste Vater auf Erden“ könne nur jemand gemeint sein, der auf heilige Weise lebe, Christus in Armut, Demut, Friedfertigkeit und Keuschheit nachfolge, nicht aber jemand, der in offenkundiger Habgier, in offenem Hochmut und in anderen Sünden lebe. Auch hier zeigt sich Jan Hus’ Grundhaltung, dass sich Inhaber von kirchlichen Ämtern, inklusive des Papstamtes, an den Aussagen und Werten der Bibel messen lassen müssen, eine Auffassung, die er von Wyclifs Lehre bestätigt sah.

Hus sah die Bibel als „ganz wahr und hinreichend zur Seligkeit des Menschengeschlechts“ an. Sie sei der Maßstab, nach dem sich das Leben richten müsse. Alle religiöse Wahrheit sei in ihr enthalten. Die Schrift sei eine Waffe gegen den Teufel, die auch schon Christus gebraucht habe, indem er dem Teufel nicht befohlen, sondern argumentiert habe. Er wandte sich gegen die Lehre, dass die Autorität der Kirche über der Bibel stehe. Die so lehrten, wollten sich selbst von Kritik freihalten und das Volk über die Heilige Schrift in Unkenntnis halten, damit es gefügig bleibe.

Hus forderte, nichts zu glauben, festzuhalten, zu behaupten und zu predigen, was nicht durch die Aussagen der Bibel begründbar sei. Die Schrift, so Hus, müsse geglaubt werden, sie sei der Zugang zum Himmelreich.

Das Abendmahl gehörte für Hus zu den „tiefsten und geheimsten und höchsten Mysterien unseres Glaubens“. Es könne von einem Menschen nicht voll begriffen werden. Die geistliche Erfahrung müsse als die wichtigere der sakramentalen Erfahrung immer vorausgehen. Christus habe dieses Sakrament eingesetzt zum Gedächtnis seines Leidens, seines Lebens und Wirkens, seiner Auferstehung und Himmelfahrt. Dies solle der Priester im Gedächtnis haben, wenn er das Sakrament spende. Entgegen der vorherrschenden Lehre seiner Zeit betonte Hus, dass das Abendmahl in Brot und Wein auch für Laien bestimmt sei. Er könne aus der Schrift eine Einschränkung nicht herauslesen. Das Ziel des Abendmahls sei, „In Christo bleiben und ihn bleibend in sich haben; in Ewigkeit nicht sterben; das ewige Leben haben“.

Die Praxis des Abendmahls gehört noch immer zu den theologisch diskutierten Punkten innerhalb der Christenheit. Hus betonte zunächst die Notwendigkeit des Glaubens an die Worte Jesu, welcher sagte, das Brot sei sein Leib und der Wein sei sein Blut. Darüber hinaus würden Brot und Wein durch die vom Priester verlesenen Einsetzungsworte geweiht, so dass das Brot in den wahren Leib Christi und der Wein in das wahre Blut Christi transsubstanziiert (verwandelt) würden.

Häresie habe drei Ursachen: Abkehr vom Gesetz Gottes, Lästerung und Ämterkauf. Eine Lästerung sei es, wenn ein Mensch Gott beschuldigt, wenn Gott hartnäckig in Gedanken beleidigt werde, indem man ihm seine Macht nicht zutraut, oder wenn man das, was Gott allein gebühre, einer menschlichen Kraft oder einer anderen Kreatur zuerkenne. In seiner Schrift über Häresie und Simonie (Ämterkauf) wies Hus darauf hin, dass auch Jesus als Lästerer beschuldigt und hingerichtet worden sei. Besonders heftig stritt Hus gegen den Verkauf kirchlicher Ämter, die andere Häresien nach sich ziehe, nicht die Fähigsten auf die Posten bringe und die Menschen verderbe.





In der Prager Bethlehemskapelle erinnern Wandmalereien an das Schicksal von Hus, der von der Kanzel dieses in der Altstadt gelegenen Gotteshauses von 1402 bis 1412 vor bis zu 3.000 Besuchern predigte.

Vor der Burg Krakovec erinnert eine von Milan Vácha geschaffene Statue an Jan Hus.

Mit der Bildung der Tschechoslowakei erklärte diese 1925 den 6. Juli zum Staatsfeiertag, worauf der Heilige Stuhl für drei Jahre die diplomatischen Beziehungen unterbrach.

Die Evangelische Kirche in Deutschland und die Evangelisch-Lutherische Kirche in Amerika erinnern am 6. Juli mit einem Gedenktag an Jan Hus.

Am Tag der vollzogenen Tötung von Jan Hus, dem 6. Juli, wird in der Konstanzer Lutherkirche ein ökumenischer Gedenkgottesdienst mit den Vertretern christlicher Kirchen aus Konstanz und Tschechien und mit tschechischen Pilgern und weiteren Besuchern abgehalten. Am Hussenstein, in der Gegend des Stadtteil Paradies, wo er bei lebendigem Leib verbrannt wurde, legen die Bürgermeister von Konstanz und seiner Partnerstadt Tábor zur Erinnerung an ihn einen Kranz nieder. Das Hussitenlied wurde vom Chor der Musikschule Bedřich Smetana gesungen.

Zum Gedenken an den Reformator wurde 1915, an seinem 500. Todestag, ein monumentales Hus-Denkmal auf dem Altstädter Ring in Prag eingeweiht. Ein bereits zweiter Wettbewerb für dieses National-Denkmal wurde im Jahre 1900 ausgeschrieben, 1903 wurde schon feierlich ein Grundstein zum Sockel des Denkmals gelegt. In der Nähe war der Standort der 1918 zerstörten Mariensäule (Prag).

Im kleinen Ort Husinec in Tschechien steht zu seiner Erinnerung eine Bronzestatue.
Die Stadt Jičín ließ 1872 ein Hus-Denkmal aufstellen.
In Horní Blatná ehrt ein Denkmal vor der Laurentiuskirche den Reformator.

Statuen sind für den 600. Todestag von Jan Hus, am 6. Juli 2015, in Planung und Bearbeitung für die Orte Husinec u Řeže, Prag, Kozí Hrádek (bei Tábor), Lidice und Konstanz unter dem gemeinsamen Thema "Jan Hus – Weg zur Versöhnung".

Die bedeutendsten Medaillen zum Gedenken an den Tod von Jahn Hus sind die sogenannte Hustaler. Die zahlreichen Nachgüsse und Nachprägungen dieser Stücke und der fast 200-jährige Herstellungszeitraum sind bei Medaillen wohl einmalig und zeugen von großem Interesse an diesen um 1537 erstmals in der der Werkstatt von Hieronymus Magdeburger geprägten und danach auch als Silberguss um 1717 ausgeführten Kleinkunstwerken. Der Spruch in der Umschrift dieser Medaillen CENTVM. REVOLVTIS. ANNIS. DEO. RESPONDEBITIS. ET. MIHI, übersetzt – „Wenn hundert Jahre vergangen sind, werdet ihr Gott und mir antworten.“ – ist jedoch von Hus nicht geäußert worden. Die Ausgabe der Medaillen wurden durch die lutherische Reformation veranlasst.



Vom 9. Dezember 2007 bis zu seiner Einstellung im Jahr 2012 trug der Regionalexpress/Schnellzug R/RE 451 und 452 Nürnberg-Prag und zurück den Namen „Jan Hus“.

Seit 1998 gibt es die "Vereinigung der Städte mit hussitischer Geschichte und Tradition". Sie hat grenzüberschreitend 17 Städte aus Deutschland und der Tschechischen Republik als Mitglieder. Sie setzt sich ein für das Bewahren des hussitischen Erbes und als Konsequenz für die Völkerverständigung. Konstanz gehört seit 2002 zur Vereinigung. Jährlich wird eine Pilgerwanderung auf den Spuren von Jan Hus unterstützt.

Die "Route der Toleranz" verläuft auf der Zugroute von Jan Hus von der Burg Krakovec nach Konstanz. Zunächst folgt sie der Goldenen Straße von Prag nach Nürnberg. Weiter verläuft sie über Ulm, Biberach, Ravensburg, Meersburg nach Konstanz.

Die Deutsch-Tschechische Vereinigung e. V. hat nahe dem Hussenstein in Konstanz eine Begegnungsstätte im Palmenhaus, Am Hussenstein 12.

Über eine Rehabilitierung in der römisch-katholischen Kirche wird seit dem Ende des 20. Jahrhunderts diskutiert. 1996 äußerte Kardinal Miloslav Vlk die Meinung, dass das Urteil gegen Hus widerrufen werden müsse. 1999 erklärte Papst Johannes Paul II. anlässlich eines Historikerkongresses über den Reformator:

Jedoch ist bis heute (2017) eine Rehabilitierung nicht erfolgt.

In den protestantischen Kirchen genießt Jan Hus hohes Ansehen als Vorläufer des Reformators Martin Luther, der im folgenden Jahrhundert seine Ideen und Ideale aufgriff.

Vor seiner Hinrichtung soll Hus gesagt haben: „Heute bratet ihr eine Gans, aber aus der Asche wird ein Schwan entstehen“. "Husa" bedeutet tschechisch "Gans". Später brachten Historiker diesen Ausspruch mit Luther in Zusammenhang und machten deshalb den Schwan zu dessen Symbol. Johannes Bugenhagen erwähnte diesen Bezug in seiner Trauerrede für Martin Luther am 22. Februar 1546 in der Schlosskirche Wittenberg. Dargestellt wird dieser Bezug Luthers zu Hus auch in dem Bild von Jacob Jacobs "Martin Luther mit dem Schwan", das seit 1603 in der Hamburger Hauptkirche St. Petri an einem Pfeiler im linken Seitenschiff hängt. Ein weiteres Bild von Hans Stiegler in der Amanduskirche (Freiberg am Neckar) zeigt ebenfalls Martin Luther mit dem Schwan.

Die Herrnhuter Brüdergemeine berücksichtigt die Denkansätze von Jan Hus.








</doc>
<doc id="13672" url="https://de.wikipedia.org/wiki?curid=13672" title="Die Fledermaus">
Die Fledermaus

Die Fledermaus ist eine Operette von Johann Strauss. Sie wurde 1874 in Wien uraufgeführt (Wiener Operette) und gilt als Höhepunkt der Goldenen Operettenära.

Gabriel von Eisenstein muss eine Arreststrafe wegen Beleidigung einer Amtsperson antreten. Da befolgt er gerne den Rat seines Freundes Dr. Falke, sich in der Nacht zuvor noch beim Prinzen Orlofsky zu amüsieren. In Wirklichkeit hat Dr. Falke vor, sich für einen früheren Streich Eisensteins zu revanchieren (die Operette hat den vollen Namen „Die Rache einer Fledermaus“). Rosalinde von Eisenstein lässt ihren Gemahl gern ziehen, als der vermeintlich ins Gefängnis aufbricht. Auch dem Kammermädchen Adele, das vorgibt, eine kranke Tante besuchen zu wollen, gibt sie frei.

Als alle weg sind, kommt Alfred, um sich mit Rosalinde zu vergnügen. Leider wird das Techtelmechtel vom Gefängnisdirektor Frank gestört, der Eisenstein abholen will: Da bleibt Alfred aus Rücksicht auf Rosalinde nichts übrig, als deren Gemahl zu spielen und sich ins Gefängnis abführen zu lassen.

Im Gartensalon bei dem jungen Prinzen Orlofsky verspricht Dr. Falke dem Prinzen, dass er heute noch viel zu lachen haben werde. Eisenstein tritt als „Marquis Renard“ bei ihm auf, Adele wird als die junge Künstlerin Olga vorgestellt. Eisensteins Verdacht, sie sei sein Stubenmädel, weist sie zurück. Gefängnisdirektor Frank wird als „Chevalier Chagrin“ in die Gesellschaft eingeführt, und selbst die als ungarische Gräfin verkleidete Rosalinde erscheint – Dr. Falke hat sie kommen lassen mit dem Hinweis, ihr Ehemann sei dort. Es gelingt ihr, dem von ihr faszinierten Eisenstein (der sie nicht erkennt) seine Taschenuhr zu entwenden, die sie benötigt, um ihrem Gemahl (den sie natürlich erkannt hat) später seine Untreue zu beweisen.

Vom Champagner angeheitert, erzählt Eisenstein vor allen Gästen, wie er einst Dr. Falke blamierte, als er ihn in seinem Fledermauskostüm (sie waren auf einem Maskenball) dem Spott der Marktfrauen und Gassenbuben aussetzte.

In der Morgenfrühe will der schwer bezechte Frank seinen Dienst als Gefängnisdirektor antreten. Der noch schwerer betrunkene Zellenschließer Frosch soll berichten, was inzwischen vorgefallen ist, und nutzt diesen Bericht zu einer mehr oder minder improvisierten Persiflage aktueller örtlicher Ereignisse.  Da zeigt sich, dass Adele (mit ihrer Schwester Ida) Frank gefolgt ist. Adele gibt zu, wer sie wirklich ist, und bittet den vermeintlichen Chevalier, sie für die Bühne ausbilden zu lassen. Jetzt erscheint auch Eisenstein, der seine Strafe antreten will und nun von Frosch erfährt, dass er, Eisenstein, doch schon gestern eingeliefert worden sei. Es stellt sich aber heraus, dass sein Doppelgänger kein anderer ist als "Alfred"; und als auch noch "Rosalinde" auftaucht, durchschaut Eisenstein das Verhältnis zwischen Alfred und seiner Frau, wird jedoch kleinlaut, als Rosalinde ihm die Uhr vorweist, die sie ihm in Gestalt der „ungarischen Gräfin“ bei Orlofskys Fest abgenommen hat.

Schließlich trifft die ganze Festgesellschaft mit Prinz Orlofsky und Dr. Falke ein. Jetzt wird klar: Die gesamte Inszenierung war die gelungene „Rache der Fledermaus“ Falke. Der köstlich amüsierte Prinz verspricht Adele, sie als Mäzen zu fördern.

Der männlichen Hauptrolle, "Gabriel von Eisenstein", stehen die zwei ebenbürtigen weiblichen Hauptrollen "Rosalinde" und "Adele" gegenüber. Sprechanteile und gesangliche Schwierigkeit der drei Hauptrollen sind etwa gleichwertig. Die Rolle des "Eisenstein" ist von Strauss für einen Spieltenor geschrieben, allerdings haben auch einige bedeutende Baritone die Partie eingespielt. Die Rolle der "Adele" ist eine klassische Soubrette.

Die wichtigsten Nebenrollen sind "Dr. Falke" (alias "die Fledermaus"), der Gefängnisdirektor "Frank" sowie "Prinz Orlofsky". Letzterer wurde von Johann Strauss als Hosenrolle für einen Mezzosopran angelegt, in einigen Inszenierungen wird die Partie auch von einem Tenor gesungen.

Das Werk ist neben den Gesangssolisten besetzt mit vierstimmigem Chor und einem Sinfonieorchester mit 2 Flöten (2. mit Piccolo), 2 Oboen, 2 Klarinetten, 2 Fagotten, 4 Hörnern, 2 Trompeten, 3 Posaunen, Pauken, Schlagzeug und Streichern (Violinen 1, Violinen 2, Violen, Violoncelli, Kontrabässe).



Das Libretto der Operette geht auf eine literarische Quelle, die Komödie "Das Gefängnis" des Leipziger Schriftstellers Roderich Benedix zurück. Aus dieser wiederum entstand das Lustspiel "Le Réveillon" des französischen Autorenduos Henri Meilhac und Ludovic Halévy. Mit Réveillon wird in Frankreich das Fest am Heiligen Abend bezeichnet, welches durchaus ausufern kann (ein ähnliches Fest spielt am Rande im zweiten Akt von Giacomo Puccinis Oper La Bohème eine Rolle). Motive und Inhalte bearbeitete Karl Haffner, wobei dessen Vorlage sich als nicht musiktauglich erwies. Der in Wien tätige Librettist Richard Genée erweiterte diese Vorlage zu einem kompakten, operettentauglichen Stück. Insbesondere machte er ein rauschendes Fest bei einem russischen Großfürsten zum Mittelpunkt des Werkes, um das sich die Intrigen von Eisenstein und Falke entwickeln.

Die Musik soll in den wesentlichen Teilen innerhalb von 42 Tagen im Sommer 1873 in Strauss' damaliger Wohnung (1870–1878) in der Maxingstraße 18 in Hietzing (seit 1892 13. Wiener Bezirk) entstanden sein, wobei Strauss hauptsächlich als Urheber der Melodien in Erscheinung trat, während große Teile der Instrumentierung von Genée ausgeführt wurden. Ein Musikstück aus dem neuen Werk wurde bei einem Wohltätigkeitskonzert im Oktober 1873 erstmals dem Wiener Publikum vorgestellt, dies war der "Csárdás" aus dem zweiten Akt. Dieser und die Ouvertüre sind die einzigen musikalischen Teile, die vollständig von Johann Strauss komponiert wurden.

Wegen des großen Erfolges dieser "Csárdás"-Aufführung wurde die Uraufführung der gesamten Operette rasch vorangetrieben, musste aber infolge der inzwischen ausgebrochenen Wirtschaftskrise („Gründerkrach“) mehrfach verschoben werden. Schließlich ging sie am 5. April 1874, unter der musikalischen Leitung des Komponisten, im Theater an der Wien über die Bühne. Nach späteren Behauptungen sei sie in Wien kein „Sensationserfolg“ gewesen, in Wirklichkeit fand sie durchwegs anerkennende Zustimmung bei Publikum und Presse. Bis 1888 folgten weitere 199 Aufführungen in demselben Theater. In anderen Städten war allerdings die Aufnahme erheblich besser, zum gleichen Zeitpunkt war sie in Berlin bei einem späteren Startzeitpunkt bereits über 300-mal aufgeführt worden.

Die erste Aufführung in einem Opernhaus erfolgte 1894 unter dem Dirigat von Gustav Mahler im Stadt-Theater Hamburg (Staatsoper).

"Die Fledermaus" ist neben dem "Zigeunerbaron" und "Eine Nacht in Venedig" eine der drei berühmtesten Strauss-Operetten und zudem eine der wenigen Operetten, die regelmäßig auch an großen internationalen Opernhäusern gespielt werden (meist zu Silvester und im Fasching).

Der Grund hierfür ist vor allem die ausgesprochen feinsinnige, mitreißende und meisterhaft orchestrierte Komposition. Höhepunkte sind das Uhren-Duett (Rosalinde/Gabriel von Eisenstein), der Csárdás, die Arie des Prinzen Orlofsky, die Arie "Mein Herr Marquis" (Adele) und der Chorwalzer "Brüderlein und Schwesterlein – Du und du" im zweiten Akt.

Der Text ist eingängig und voller Ironie mit zeitlosen Wahrheiten. Als Beleg kann erneut das Ensemble „Brüderlein und Schwesterlein“ dienen:

Eine besondere Stellung nimmt die Ouvertüre ein, die, in freier Sonatenhauptsatzform geschrieben, zu den größten Schöpfungen von Johann Strauss zählt. Sie fasst die zahlreichen musikalischen Höhepunkte des gesamten Werkes zusammen und ist mit ihrer abwechslungsreichen Dynamik auch für Spitzenorchester immer wieder eine Herausforderung.

1999 erschien im Rahmen der "Neuen Johann Strauss Gesamtausgabe" eine zweibändige Neuausgabe der "Fledermaus" mit dem revidierten Notentext und dem nachkomponierten „Neuen Csárdás“ sowie Entstehungsgeschichte, Revisionsbericht und Textbuch.

Nahezu völlig vergessen ist, dass es um 1879 Bemühungen gab, Johann Strauss zu einer Fortsetzung der Operette zu bewegen. Dazu hatte Leon Treptow das Libretto verfasst. Nachdem Strauss endgültig ablehnte (er komponierte anschließend die Operette „Das Spitzentuch der Königin“) wurde „Prinz Orlofsky“ schließlich von Carl Alexander Raida vertont und erlebte am 8. April 1882 im Berliner Viktoria-Theater seine Uraufführung. Bekannt ist daraus bestenfalls noch der „Prinz-Orlofsky-Walzer“. Nach 23 Aufführungen wurde das Werk vom Spielplan genommen und scheint keine weitere Aufführung mehr erlebt zu haben.


Verfilmungen der Operette:




</doc>
<doc id="13673" url="https://de.wikipedia.org/wiki?curid=13673" title="Die schöne Helena">
Die schöne Helena

Die schöne Helena () ist eine Opéra-bouffe bzw. eine Buffo-Oper in drei Akten von Jacques Offenbach und den Librettisten Henri Meilhac und Ludovic Halévy.
Die Uraufführung fand am 17. Dezember 1864 im Théâtre des Variétés in Paris statt. Offenbach konnte damit an den Erfolg seiner Opéra-bouffe "Orpheus in der Unterwelt" (1858) anknüpfen, die ebenfalls einen Stoff der klassischen Antike persifliert. Im Unterschied zu "Orpheus" steht und fällt der Erfolg der "Schönen Helena" jedoch mit der weiblichen Titelrolle, in der Uraufführung dargestellt von Hortense Schneider, bei der Wiener Erstaufführung von Marie Geistinger. Berühmte weitere Interpretinnen waren u. a. Emily Soldene im anglo-amerikanischen Raum.

Hortense Schneider (1833–1920) war die gefeierte Sängerin und Kurtisane, für deren spezielle Talente Offenbach "La belle Hélène" konzipierte sowie später auch weitere Operetten wie "Barbe-bleue" 1866, "La Grande-Duchesse (de Gerolstein)" 1867, "La Périchole" 1868 und "La Diva" 1869. Schneider zeichnete sich mehr durch ihre enorme Bühnenpräsenz und erotischen Reize aus als durch klassische Gesangskünste. Der Romancier Emile Zola porträtiert sie – ironisch zugespitzt – in "Nana" (1880) als Darstellerin ohne sonderliche Begabungen, die es aufgrund ihrer körperlichen Reize schafft, das Pariser Gesellschaftspublikum mit Nacktauftritten in den Bann zu schlagen.

Das Publikum der Offenbach-Operetten bestand – wie u. a. Kracauer in "Jacques Offenbach und das Paris seiner Zeit" schreibt – aus höchsten Adelskreisen und der Halbwelt. So fand man im Publikum Bankiers, Schriftsteller, Diplomaten, Kurtisanen aber auch kaiserliche Würdenträger, die großen Amüsierbedarf (sowohl intellektuell als auch sexuell) hatten und deutlich lockerere Moralvorstellungen als das Bürgertum.

Offenbach und seine Librettisten machen sich in den sogenannten „Offenbachiaden“ besonders über die Mittelschicht, Neureiche, Emporkömmlinge und deren konservative Moralvorstellungen lustig; nicht, wie fälschlich oft angenommen, über die Herrschenden insgesamt, im Sinn von Brechts klassenkämpferischem Theater. Es war ein Lachen von oben nach unten, kein Protest von unten nach oben. Somit kann auch die Figur der Sparta-Königin Helena – der „schönsten Frau der antiken Welt“ – nicht als Abbild von Kaiserin Eugénie gesehen werden, wie manchmal behauptet wird.

Anhand der Tagebücher des Librettisten Halévy lässt sich feststellen, dass er als Staatsbeamter zwar eine zwiespältige Haltung gegenüber dem Regime Napoleons III hatte, jedoch sind keine Hinweise auf Untergrabungen gegen Napoleon III. zu finden. Anhaltspunkte für den Verfall der Sitten und Kritik am Zweiten Kaiserreich sucht man ebenso in den Tagebüchern vergebens. Was man aus ihnen sicher ableiten kann ist, dass sich die Operette aktiv an der Polemik gegen die falschen Moralisten (jene, die die Staatsmacht als oberste Instanz und Hüter der Moral ansahen, in Verkörperung des Kaisers, der für seine halbweltlichen Eskapaden bekannt war) beteiligt hatte.

Hans Jörg Neuschäfer begründet in seinem Artikel „Die Mythenparodie in La Belle Hélène“ in "Jacques Offenbach und seine Zeit", wie Offenbach mit dem Aspekt des Scherzhaften spielt, jedoch den Mythos der Helena-Sage bewahrt. Er begründet seine Aussage mit zwei Prinzipien, die für ihn vor allem den Aspekt der Burleske (des Scherzhaften) aufzeigen. Ein Prinzip besteht für ihn darin, dass die antiken Schauplätze der Operette mit der Gegenwart verbunden werden. So wird aus Sparta Paris und aus Nauplia das mondäne Seebad Trouville. Somit rückt die Handlung der "Belle Hélène" in Reichweite des Pariser Publikums der 1860er Jahre. Das zweite Prinzip ist die Reduktion des Übermaßes an moralischer und physischer Kraft, sowie dass er den Heroismus der antiken Sagenvorlage auf das Mittelmaß menschlicher Schwäche herabsetzt, um komischen Effekt zu erzielen (die Helden der Geschichte werden alle als lächerliche Könige und Krieger dargestellt).

Offenbachs Operetten sind indirekt, wie auch direkt, mit der Entwicklung des Moraltheaters verbunden. Indirekt, da sie sich die große Kontroverse mit dem Sprechtheater teilen, die in den 1850er Jahren das Feuilleton bestimmte. Direkt, indem sie Motive und Themen des Moraltheaters aufgreifen. Die Rolle des Geldes und die Rolle der Ehe in der Gesellschaft sind die zentralen Themen, die Offenbach aus dem Moraltheater übernommen hat.

"Die schöne Helena" gewann ihre provozierende Wirkung nicht nur dadurch, dass viele historische Darstellerinnen in dem Stück nackt bzw. fast nackt zu sehen waren, sondern auch aus der Behandlung des Themas Ehebruch, das Helena als schicksalhaft und unvermeidlich nennt. Dies wurde von verschiedenen Seiten als Befürwortung der Wiedereinführung der Scheidung gesehen, meint Ralph-Günther Patocka in seinem Buch "Operette als Moraltheater, Jacques Offenbach Libretti zwischen Sittenschule und Sittenverderbnis".

Helena, die „schönste Frau der Welt“, sehnt sich nach Abwechslung von ihrem Ehealltag und weiß, dass die Göttin Venus dem Prinzen Paris die Liebe der schönsten Frau auf Erden versprochen hat, also Helena. Mit Spannung erwartet sie Paris, der sich ihr als Schäfer verkleidet nähert. Im 2. Akt erscheint Paris in Helenas Schlafgemach. Da sie glaubt, dies alles sei nur ein Traum – in dem alles erlaubt sei – gibt sie sich Paris in einer wilden Liebesszene hin. Diese Nachtszene mit dem erotisch aufgeladenen Traumduett «Ce n’est qu’un rêve» erhitzte die Gemüter der Moralisten, da die Darstellerin der Helena bei der Uraufführung und später der Wiener Erstaufführung nackt auf der Bühne stand. In einem Eintrag in den Münchner Polizeiakten heißt es im Zusammenhang mit einer Aufführung in Bayern: „Bei alledem verdient Anerkennung, daß die Direction des Theaters [in München, Anm.] sichtlich bestrebt war, das Stück möglichst dezent zu geben. Die Costüme enthüllten bei weitem nicht die Blöße so, wie in Paris am Vaudevilletheater oder in Wien am Carltheater der Fall ist. In Paris u. Wien entkleidete sich Helena in der Nachtszene des II. Actes fast vollständig auf der Bühne.“

Wenn man sich die Biographien von Sängerinnen und Tänzerinnen der Epoche ansieht, fällt auf, wie häufig sie Verbindung zu gewissen Logenbesuchern pflegten. Daher kann man durchaus daraus schließen, dass die Theater, wo Frivoles, Laszives und sexuell Angespieltes gezeigt wurde, auch als Edelbordell genutzt wurde und die Sängerinnen, Chorsängerinnen und die Tänzerinnen als Edelprostituierte fungierten. Dies wird u. a. von Zola in "Nana" ausführlich dargestellt, wo der Direktor des Vaudeville-Theaters von seinem Haus wiederholt als „Bordell“ spricht.

Ihre Götter bildeten die Griechen oft mit blonden Haaren ab. So verwundert es nicht, dass auch Helena in der Operette mit blonden Haaren dargestellt wird. Betrachtet man jedoch die Darstellung der Helena als „Blondine“ mit dem Hintergrundwissen, dass in den 1860er Jahren blond gefärbte Haare bei Frauen der Mittelschicht als „barbarisch“ angesehen wurden und als Erkennungszeichen von Prostituierten galten, erscheint Offenbachs Helena sofort in einem anderen Licht. In ihrer Arie singt Helena durchaus provokant in diese Richtung anspielend: «On me nomme Hélèna la blonde». Natürlich ist auch die Titelfigur Nana in Zolas Roman blond und eine gefeierte Kurtisane.

In dem Genre der Operette, das Offenbach pflegte, konnten unter dem Deckmantel der Parodie und Groteske viele Übertreibungen aller Art und sehr freizügige Anspielungen von Erotik auf die Bühne gebracht werden, die unter realistischen oder normalen Umständen von der Zensur auf einer öffentlichen Bühne in Paris niemals erlaubt worden wären.
Über die Darstellung der Helena durch Marie Geistinger in Wien bemerkt Zeitzeugin Bertha Glöckner: „Heute sehe ich noch diese Helena vor mir, in ihren durchsichtigen Tarlatangewändern, ich sehe ihre junonische Gestalt, ihre klassischen Beine, ihr reizvolles Profil mit dem leicht ironischen Lächeln um den Mundwinkel. Wenn sie den Paris bei der ersten Begegnung mit der ihr eigenen schwungvollen Handbewegung begrüßte und dann anlorgnettierte, wenn sie in der Traumszene die Tunique abwarf … das war ein Bild unnachahmlicher Grazie.“ 
Es verwundert angesichts des schlüpfrigen Themas Ehebruch und der „in verschiedenem Sinne florierenden Nacktheit“ im Stück nicht, dass der bei der Pariser Uraufführung anwesende Fürst Metternich beim Verlassen des Theaters zu seiner Ehefrau gesagt haben soll: „Wir haben unrecht daran getan, der Premiere beizuwohnen. […] Unser Name wird in allen Zeitungen stehen, und es ist nicht angenehm für eine Frau, gewissermaßen offiziell in einem solchen Stück gewesen zu sein.“

Die Operette spielt im mythologischen Griechenland (Sparta und Nauplia) kurz vor Beginn des Trojanischen Krieges, vermischt mit Elementen der Gegenwart zur Zeit der Uraufführung.

"Bild: Tempelplatz in Sparta"

Helena, die Gattin des Königs Menelaos, gilt als die schönste Frau der Welt, und sie glaubt das auch von sich selbst. Weil ihr etwas trotteliger Ehemann schon sehr betagt ist, kann er seine Frau nicht mehr befriedigen. Helena bittet deshalb Venus, die Göttin der Liebe, ihr endlich mal wieder einen richtigen Liebhaber zu senden. Dabei denkt sie an jenen Schäfer, dem Venus einst auf dem Berge Ida die schönste Frau der Welt versprochen hat. Auch Menelaos hat von dieser Geschichte gehört und sorgt sich seither sehr um die Treue seiner schönen Frau.

In Sparta findet gerade ein geistiger Wettkampf statt. Einer der Teilnehmer ist Prinz Paris aus Troja, der sich – getarnt als Schäfer – unter die Teilnehmer gemischt hat. Weil er auf jede Frage die richtige Antwort weiß, hat er bald Helenas Interesse geweckt. Paris erkennt rasch, dass der Großaugur Kalchas vor allem auf seinen eigenen Vorteil bedacht ist, und besticht ihn, damit er bei seinem Werben um Helena für günstige Umstände sorge. Kalchas verkündet dem Volk, die Götter hätten befohlen, dass sich Menelaos nach Kreta begeben müsse. Schweren Herzens tritt er die Reise an.

"Bild: Gemach der Helena"

Kalchas hat Helena für die kommende Nacht einen wunderschönen Traum versprochen. Als die schöne Frau in ihrem Gemach Paris erblickt, glaubt sie, dass jetzt der Traum wahr werde. Beide verbringen eine ausgelassene Liebesnacht und stillen ihr Verlangen. Doch womit sie nicht gerechnet haben: Menelaos kehrt früher als erwartet von seiner Reise zurück. Er ertappt sein Weib beim Seitensprung und will den Rivalen verhaften lassen. Doch bevor es seinen Häschern gelingt, ihn zu ergreifen, gelingt ihm die Flucht.

"Bild: Strandpromenade in Nauplia"

Alles, was in Griechenland Rang und Namen hat, erholt sich in Nauplia. Zurzeit beehrt auch König Menelaos mit seiner Gattin dieses Seebad. In seiner Verzweiflung hatte Menelaos postalisch ein Bittgesuch beim Großauguren der Göttin Venus eingereicht, damit die Schuldfrage endlich geklärt werde. Seine Gattin beharre nämlich eisern, völlig unverschuldet in die „Notlage“ geraten zu sein. Zur Antwort erhielt er, er möge sich nebst Gattin in Nauplia einfinden, dann würden ihm die Augen geöffnet. Es dauert auch nicht lange, da naht doch tatsächlich mit einem Schiff der weißhaarige und ehrfurchtgebietende Großaugur. Als er Helena auffordert, mit ihm nach Cythere zu kommen, um dort im Tempel hundert weiße Schafe zu opfern, ist es Menelaos, der seine Gattin auffordert, gleich das Schiff zu besteigen und dem Befehl Folge zu leisten. Es dauert aber nicht lange, bis er merkt, dass er hereingelegt worden ist. Denn kaum ist das Schiff ein paar Meter vom Strand entfernt, enttarnt sich der Großaugur als Prinz Paris, der die schöne Helena entführt. Und diese Entführung – das weiß man inzwischen – war die Ursache für den Beginn des Trojanischen Krieges!

Die deutschsprachige Erstaufführung fand schon drei Monate nach der Premiere am 17. März 1865 in Wien am Theater an der Wien statt. Die Rolle der Helena übernahm Marie Geistinger. Inszeniert wurde diese Erstaufführung vom damaligen Direktor des Theaters, Friedrich Strampfer. Camillo Walzel (Pseudonym F. Zell) und Julius Hopp übersetzten das französische Libretto ins Deutsche. In den ersten neun Monaten der Spielzeit brachte es das Stück auf 65 Aufführungen, trotz einer Opposition, die sich aus Kunstexperten und Journalisten zusammensetzte, die gegen die Darstellung der Antike Stimmung machte. Marie Geistinger verkörperte bis Ende des Jahres 1875 in bis zu 200 Vorstellungen die Helena. Offenbach selbst meinte nach der Wiener Erstaufführung entzückt über Geistinger: «Voilà la belle Hélène de mes rêves!»

Besetzung der deutschsprachigen Erstaufführung in Wien
„(Theater an der Wien) Gestern kam Offenbach’s […] vielgenannte „schöne Helena“ unter des Kompositeurs persönlicher Leitung zur ersten Aufführung […]. Vorzüglich war Fr. Geistinger als Helena […]“

„Die Musik steht an komischem Gehalt und satyrischer Würze jener des ‚Orpheus‘ nach, doch enthält sie zahlreiche Schönheiten, die sich durch wirkliche Erfindung, melodischen Reiz, graziöse Mache und pikante Instrumentation auszeichnen. […] Die Darstellung, die fasst in allen Theilen eine hochgelungene zu nennen war, hat Vieles zum Erfolge beigetragen. Der Löwenanteil desselben gebührt Fr. Geistinger, eine Künstlerin im besten Sinne des Wortes. […] Sie spielt mit Sicherheit und Ruhe, und verfügt über eine so schätzenswerthe Menge von Ausdrucksmitteln, die zur Feststellung eines Charakters und dessen stylvoller Behandlung gehören.“

Man liest aber auch von Marie Geistinger als „höchst routinierter Schauspielerin“, die eine „leidliche Sängerin“ sei, aber imstande, „ein Stück zu tragen“. Zur Inszenierung wurde bemerkt, dass „einzelne Damen“ durch ihren „tadellosen Körperbau“ aufgefallen seien und sich verdient gemacht hätten in einem Stück, dass voller „derbe[r] Zweideutigkeiten“, ja auch „in verschiedenem Sinne florierend[e] Nacktheit“ vorkäme – das Publikum angeblich dennoch „gelangweilt“ war.

"Die schöne Helena" wurde auch von Regisseur Max Reinhardt und Komponist Erich Wolfgang Korngold neu bearbeitet. Es war nicht die erste Zusammenarbeit der beiden für eine Operettenneubearbeitung (vorangegangen war die erfolgreiche Bearbeitung der Fledermaus von Johann Strauß im Jahr 1929.) Durch Egon Friedell und Hanns Sassmann ließ Reinhardt das "Helena"-Libretto von Henri Meilhac und Ludovic Halévy radikal umarbeiten. Die klassische Struktur von drei Akten löste Reinhardt in eine Revue von sieben Einzelbildern auf. Diese Fassung wurde am 15. Juni 1931 zum ersten Mal in Berlin aufgeführt. Die Rolle der Helena übernahm Jarmila Novotná, die später bei Lehárs Giuditta bei der Uraufführung an der Wiener Staatsoper die Titelpartie sang. Menelaos verkörperte Hans Moser. Die Hosenrolle des Orest wurde von Friedel Schuster dargestellt. Diese Rolle sollte der Beginn einer großen Operettenkarriere für sie sein. Die musikalische Bearbeitung von Korngold kam gut an, obwohl er dem Werk viel von seiner elementaren Wirkung nahm, da er mit dem Wienerwalzeroperettenstil die Operette versüßlichte. Etliche Bühnen in Europa spielten diese Version nach, unter anderem 1932 das Adelphi Theatre in London. In der Emigration brachten Korngold und Reinhardt ihre Bearbeitung der "Schönen Helena" unter dem Titel "Helen Goes To Troy" 1944 in New Yorker zurück auf die Bühne.

Eine weitere Neubearbeitung als „Operette für Schauspieler“ von Peter Hacks wurde 1964 in einer Inszenierung von Benno Besson am Deutschen Theater Berlin erstaufgeführt.

Die schöne Helena wurde erfolgreich in weiteren Ländern auf die Bühne gebracht.
Originalfassung:
Zwei Flöten (2. auch Piccolo), eine Oboe, zwei Klarinetten, ein Fagott, zwei Hörner, zwei Trompeten, eine Posaune, Pauken, Schlagwerk und Streicher.

Wiener Bearbeitung:
Zwei Flöten, zwei Oboen, zwei Klarinetten, zwei Fagotte, vier Hörner, zwei Trompeten, drei Posaunen, eine Basstuba, großes Schlagwerk und Streicher
Wiener Bearbeitung oder Wiener Fassung bedeutet, dass Offenbach selbst Veränderungen und Erweiterungen im Orchester vorgenommenen hat. Lange Zeit war bei Einspielungen von Wiener Fassungen leider bedauerlich, dass eher ein deutscher als ein pariserischer musikalischer Geist im Werk mitschwang. Da etliche Dirigenten Schwerfälligkeit mit instrumentalem Reichtum gleichsetzten, waren die Wiener Fassungen lange Zeit, zu Unrecht, als zu massig verschrien.

Offenbach hat eine Fülle eingängiger Melodien über sein Werk gestreut. Die Musik sprudelt geradezu lebensschäumend daher. Außer der berühmten Ouvertüre, die oft losgelöst vom eigentlichen Werk im Konzertsaal und im Rundfunk zu hören ist, seien noch folgende Gesangsnummern besonders hervorzuheben:

Gesamtaufnahmen gibt es in französischer, deutsch und russischer Sprache. Querschnitte existieren auch in Englisch, Polnisch sowie Tschechisch.

DVDs

TV-Filmversionen



</doc>
<doc id="13674" url="https://de.wikipedia.org/wiki?curid=13674" title="Der Zarewitsch">
Der Zarewitsch

Der Zarewitsch ist eine Operette in drei Akten von Bela Jenbach und Heinz Reichert, die Musik wurde von Franz Lehár komponiert.

Die Uraufführung fand am 21. Februar 1927 im Deutschen Künstlertheater in Berlin statt.
Die für Richard Tauber geschriebene Operette gilt als Franz Lehárs Spätwerk. „Der Zarewitsch“ hat wenig von beschwingt-kitschiger Operettenseligkeit. Seine Musik kommt mit opernhafter Wucht daher – und zum Schluss kriegen sie sich nicht. Es ist die sensible Sicht auf die Tragödie eines Mannes, dessen privates Glück auf dem Altar der Staatsräson geopfert wird.

Der junge, ängstliche und kontaktscheue Alexej wird zum Thronfolger Russlands, zum Zarewitsch erwählt. Da er keine Frauen in seiner Nähe duldet, beschließt der Großfürst, gleichzeitig sein Onkel, ihn durch eine "eingeschmuggelte" Geliebte, Sonja, ein Ballettmädchen, auf den Ehestand vorzubereiten. Alexej, zunächst wütend, kann von Sonja besänftigt werden, indem sie ihm vorschlägt, seine Geliebte lediglich zu spielen und so dem Zarenhofe Theater vorzuspielen. Bei beiden erwächst Liebe. Da jedoch Sonja für eine Heirat nicht standesgemäß ist, soll sie verschwinden, damit Alexej nun eine Prinzessin ehelichen kann, der aber eigentlich nur Sonja will. Zahlreiche Intrigen lassen das Paar nach Neapel in Italien fliehen, lediglich von dem treuen Dienerpaar Iwan und Mascha und den Goldfischen begleitet.

Der Aufenthaltsort wird entdeckt, der Großfürst appelliert an Pflichtgefühl und Unterordnung persönlichen Glücks zum Wohle des Vaterlandes. Da ereilt alle die Nachricht vom Tode des Zaren. Alexej gehorcht der Staatsräson und die Geliebten trennen sich.


Das "Wolgalied" wurde durch die Verbindung von melodischer Schlichtheit mit ergreifender Sentimentalität zu einem der bekanntesten Schlager der Operettengeschichte. Es fand einen literarischen Niederschlag in Günter Grass’ Erzählungsband "Mein Jahrhundert".

Allein! wieder allein!
Einsam wie immer.
Vorüber rauscht die Jugendzeit
In langer, banger Einsamkeit.
Mein Herz ist schwer und trüb mein Sinn,!
Ich sitz' im gold'nen Käfig drin.

Es steht ein Soldat am Wolgastrand,
Hält Wache für sein Vaterland.
In dunkler Nacht allein und fern
Es leuchtet ihm kein Mond, kein Stern!
Regungslos die Steppe schweigt,
Eine Träne ihm ins Auge steigt:
Und er fühlt, wie's im Herzen frißt und nagt,
Wenn ein Mensch verlassen ist, und er klagt,
Und er fragt:!

Hast du dort oben vergessen auf mich?
Es sehnt doch mein Herz auch nach Liebe sich.
Du hast im Himmel viel Engel bei dir!
Schick doch einen davon auch zu mir.

Bei der Erstaufführung 1927 in Berlin hatte Angelos Grimanis sein Debüt als Tänzer.


</doc>
<doc id="13675" url="https://de.wikipedia.org/wiki?curid=13675" title="Wiener Blut">
Wiener Blut

Wiener Blut ist eine „komische Operette“ in drei Akten von Johann Strauss (Sohn), zusammengestellt und bearbeitet von Adolf Müller junior. Der Titel stammte von dem Konzertwalzer "Wiener Blut" op. 354 (1873). Die Uraufführung der Operette fand am 26. Oktober 1899 im Carltheater Wien statt. Das Libretto stammt von Victor Léon und Leo Stein.

Nach einem imposanten Lebenswerk und mitten in seiner Arbeit am Ballett "Aschenbrödel" war der 74-jährige Johann Strauss auch gesundheitlich nicht (mehr) in der Lage, eine neue Operette in Angriff zu nehmen. Gleichwohl gab er dem Drängen des Theaterdirektors Franz Jauner, seiner geschäftstüchtigen Ehefrau Adele und natürlich auch den Librettisten nach, eine Operette mit seinem Namen zu autorisieren, nachdem der Kapellmeister des Theaters an der Wien, Adolf Müller jun., eine Kompilation bereits vorliegender Tanzmelodien ins Gespräch gebracht hatte.

Diese Kompilation übernahm schließlich Müller jun. selbst, der sich hier erneut als erfahrener und hochbegabter Theaterpraktiker erwies: Er stellte aus verschiedensten Werken von Strauss effektvoll die Musik zusammen, wobei Strauss selbst nur beratend tätig war (siehe Abschnitt "Kompilation"). Müller griff auf nachweisbar 31 Werke zurück, deren bekannteste sich sehr schnell erschließen (wie zum Beispiel die Walzer "Wiener Blut" oder "Morgenblätter" oder auch die Polka schnell "Leichtes Blut"), er griff aber auch auf sehr frühe, biedermeierlich wirkende Tänze von Strauss aus den 1840er-Jahren zurück, die er geschickt mit den nostalgischen Momenten der Operette verband: „Gerechterweise müsste es also heißen: ‚Wiener Blut‘, Operette von Adolf Müller nach Motiven Strauss’scher Werke.“ Diese Art Zweitverwertung von Tanzkompositionen als Pasticcio auf der Bühne war 1899 neu, und verbreitete sich erst später in der Wiener Operette nach dem Ersten Weltkrieg.

Victor Léon und Leo Stein wiederum verfassten den Handlungsstrang und stellten sich der auch nicht einfachen Aufgabe, fertigen Musikstücken, zum Teil von hohem Bekanntheitsgrad, singbare Texte zu unterlegen, die zudem zusammen auch noch eine brauchbare Handlung ergeben sollten. Aus beidem entstand schließlich ihr Libretto über die amourösen Eskapaden des Grafen Zedlau zur Zeit des Wiener Kongresses.

Mit einem neuen Werk aus der Feder des Walzerkönigs glaubte Franz Jauner, der Direktor des Carltheaters, sich vor dem drohenden Konkurs retten zu können. Zunächst hatte Strauss allerdings das Werk für das Theater an der Wien bestimmt, dort kam es allerdings zu Streitigkeiten über Honorarforderungen. Johann Strauss setzte schließlich dem Drängen von Jauner keinen Widerstand mehr entgegen, dass es daraufhin dem Carltheater angeboten wurde und vier Monate nach Strauss’ Tod wurde dort die Verwechslungskomödie aus der Taufe gehoben.

Zeitungen kommentierten die Premiere ironisch als „Urlaub, den sich Meister Strauss vom Himmel genommen“ habe, und verglichen ihn mit "Titurel", der in Richard Wagners "Parsifal" aus dem Grabe singt. Das Fiasko der Uraufführung führte unter anderem zu Jauners Selbstmord. Dies lag allerdings wohl auch an dem Zeitgeist begründet, der das Jahr 1899 beherrschte und der gebannt auf das 20. Jahrhundert blickte und erhoffte, bessere Zeiten zu erleben. In einer solchen Situation war der Blick zurück auf den Wiener Kongress nicht gefragt.

Zu dem erhofften Kassenerfolg wurde erst die Neueinstudierung der Operette 1905 im Theater an der Wien, der zu einem Siegeszug um die Welt wurde, der bis heute anhält.

Die lebenslustige Wienerin Gabriele ist mit Balduin Graf Zedlau, einem Gesandten von Reuß-Schleiz-Greiz getraut worden. Schon nach kurzer Zeit entpuppt sich Zedlau als sehr spießig und da ihm das "Wiener Blut" fehlt, zieht Gabriele wieder auf das Schloss ihrer Eltern. Der einsame Zedlau beginnt daraufhin eine Affäre mit der schönen Franziska Cagliari. Zudem wirft Balduin ein Auge auf die Freundin Pepi Pleiniger seines Dieners Josef, die als Probiermamsell arbeitet.

Gabriele erfährt vom Treiben ihres Mannes und kehrt in seine Villa zurück. Nach turbulenten Verwechslungen treffen alle einander in Hietzing beim Heurigen. Gabriele lässt sich von Fürst Ypsheim-Gindelbach, dem Premierminister von Reuß-Schleiz-Greiz und Chef von Graf Balduin, geleiten. Balduin vergnügt sich mit der Probiermamsell Pepi Pleiniger und Joseph kommt mit der schönen Franziska Cagliari. Trotzdem kommt es zum Happy End, die jeweiligen Paare (Gabriele und Balduin, Pepi und Joseph, Cagliari und Ypsheim-Gindelbach) finden wieder zueinander und alle erkennen, dass daran nur das Wiener Blut schuld sein kann, und singen einen großen Schlusschor.

Die kurzen Textzeilen zeigen, dass der Walzer ursprünglich nicht für Singstimmen, sondern instrumental gedacht war. So war man gezwungen, als Gesangstext etwas Knappes, aber doch Verständliches und sich Reimendes zu finden:

Der Titel „Wiener Blut“ und diese Verszeilen über ihn wurden zum geflügelten Wort: Geschickt wird auf diese Weise die Exklusivität des „blauen Bluts“ auf die ganze Einwohnerschaft der multikulturellen Stadt Wien übertragen.

Für die Zusammenstellung verwendete Adolf Müller jun. nachgewiesen 31 Werke in unterschiedlichen Situationen, wobei das Tonträgerbeispiel sich auf die einzige, bisher vorhandene Johann-Strauss-Gesamtausgabe der "Marco Polo Edition" (Naxos) bezieht und nur des Nachhörens wegen aufgenommen wurde. Die Angaben folgen "Helmut Reichenauer: Gedanken zu Wiener Blut" im Almanach des Kulturvereins „Wiener Blut“ (Nr. 1, Oktober 2011).

Wiener Blut (1942): Die populäre Verfilmung von Willi Forst von 1942 wurde bereits in Deutschland produziert und präsentierte mitten im Zweiten Weltkrieg eine unversehrte österreichische Welt. Die enge Bindung von Mussolinis Diktatur an das nationalsozialistische Deutschland schuf die Rahmenbedingungen, dass die Filmfestspiele von Venedig den Film noch im selben Jahr mit dem "Premio della Biennale" auszeichnen konnten. 

Eine weitere Verfilmung unter der Regie von Hermann Lanske mit Benno Kusche, René Kollo, Ingeborg Hallstein, Fritz Muliar und Dagmar Koller in den Hauptrollen erschien 1972 als Koproduktion von ZDF und ORF.

Der Rocksänger Falco benannte sein fünftes Album und den dazugehörigen Titelsong 1988 nach der Operette, die deutsche Metalband Rammstein tat es ihm mit dem achten Lied des Albums „Liebe ist für alle da“ gleich: Dieses heißt ebenfalls „Wiener Blut“, benutzt den Titel aber als zynische Metapher für den Fall Josef Fritzl. Die slowenische Industrial-Band Laibach adaptierte 1987 den Wiener-Blut-Walzer für die Bühnenmusik zum avantgardistischen Theaterstück „Krst pod Triglavom“ („Die Taufe unter dem Triglav“) der Neue-Slowenische-Kunst-Theatergruppe „Rdeči Pilot“ („Der rote Pilot“), welches die Geschichte Sloweniens inklusive der vergangenen Zugehörigkeit zu Österreich thematisierte.

2008 präsentierte die Regisseurin Cordula Däuper "Wiener Blut" am Berliner Hebbel-Theater in einer rein weiblichen Cross-Dressing-Produktion, die das Stück in Bezug auf die „heterosexuelle Matrix der Geschlechterrollen“ im Sinn von Judith Butler neu durchleuchtete. Über den Gender-Aspekt sowie die Frage, ob Librettist Victor Léon in dieser Operette mit der Gräfin Zedlau ein neues, modernes Frauenideal entwarf, das zu den Maximen der Zeitschrift "Die Hausfrau" passt, wo er langjähriger Redakteur war, wurde bei der Konferenz Tanz-Signale in Wien 2015 erstmals diskutiert. Ebenso zur Sprache kam dort im Vortrag von Kevin Clarke das Thema, dass man den Grafen Zedlau als Sexsüchtigen im modernen Sinn des Wortes interpretieren kann, der unter der „Tyrannei der Lust“ leide („Klopft ein Versucher an die Tür, dann ist vergessen die Moral. Ich denke: ‚Nur noch dies eine Mal! Von morgen an werd’ ich solid!‘ Ach, lieber Gott! ’s ist ein altes Lied! Und morgen, ach, ja dann … Fang ich von vorne an!“).

Friedrich Schlögl veröffentlichte Anfang 1873 einen ersten Teil seiner Schriften, die großenteils unter dem Titel „Kleine Culturbilder aus Wien“ im Feuilleton des Neues Wiener Tagblatts erschienen waren, unter dem Titel „‚Wiener Blut‘: Kleine Culturbilder aus dem Volksleben der alten Kaiserstadt“. Das Buch wurde ein großer Erfolg. Ferdinand Kürnberger und Ludwig Anzengruber stimmten in das Lob vieler anderer ein. „Schlögl habe beste belletristische Ethnographie über die Wiener geschrieben, sei ein ‚sozialer Wegweiser‘, der Autor ‚das Gewissen Wiens‘.“

Am 20. April 1873 fand die Hochzeit von Leopold von Bayern mit Gisela von Österreich statt. Rund um diesen Anlass wurde eine Reihe glanzvoller Feste gefeiert. Das Personal des k.k. Hof-Operntheaters veranstaltete am 22. April 1873 im Goldenen Saal des Wiener Musikvereins einen Hof-Opernball. Dabei wurde der Walzer mit dem von Schlögl entlehnten Titel von Johann Strauss uraufgeführt.


</doc>
<doc id="13676" url="https://de.wikipedia.org/wiki?curid=13676" title="Eine Nacht in Venedig">
Eine Nacht in Venedig

Eine Nacht in Venedig ist eine „komische Operette“ in drei Akten von Johann Strauss. Das Libretto stammt von Camillo Walzel (Pseudonym: Friedrich Zell) und Richard Genée. Am 3. Oktober 1883 fand die Uraufführung im Neuen Friedrich-Wilhelmstädtischen Theater in Berlin statt. Die Operette dauert knapp drei Stunden. Die Handlung spielt in Venedig um die Mitte des 18. Jahrhunderts und ist eine Verkleidungs- und Verwechslungskomödie mit Liebesverwirrungen zwischen zwei Ständen. 1931 wurde sie durch Erich Wolfgang Korngold erheblich umgearbeitet, eine weitere Fassung stammt von Walter Felsenstein (1954), original wird sie nurmehr sehr selten aufgeführt.

Friedrich Zell und Richard Genée hatten zwei Textbücher für eine neue Operette zur Auswahl: "Eine Nacht in Venedig" und "Der Bettelstudent". Johann Strauss wählte das erste Buch; mit dem anderen begann Karl Millöckers Karriere. Wie viele andere Wiener Operetten ist das Libretto die Bearbeitung einer französischen Opéra-comique (daher die Bezeichnung „komische Operette“): "Le Château trompette" (1860) von François-Auguste Gevaert. Ein Plagiats-Vorwurf aus Paris zeigte jedoch, dass solche Zweitverwertungen nicht mehr so unproblematisch waren wie noch eine Generation zuvor.

Weil Strauss’ Ehefrau ein Verhältnis mit Franz Steiner, dem Direktor des Theaters an der Wien, begonnen hatte, wollte Strauss die Operette nicht dort uraufführen lassen und nahm das Angebot aus Berlin an. Die Premiere in Berlin wurde zum Misserfolg. Das lag zum Teil an der verworrenen Handlung, zum Teil aber auch an den unterlegten Texten: Das spätere Kopfmotiv des „Lagunenwalzers“ hatte zum Beispiel den Text "Nachts sind die Katzen ja grau, da tönt es zärtlich Miau" erhalten, was das Premierenpublikum zum lautstarken „Miauen“ animierte.

Für die Erstaufführung am Theater an der Wien wurde das Stück daher umgearbeitet, insbesondere dieser Text eliminiert und auf die vorhandene Melodie "Ach wie so herrlich zu schauen, sind all’ die lieblichen Frauen" von Genée getextet. Das Werk wurde auf diese Weise zu einer auch demonstrativ bejubelten Aufführung gebracht. 

Die bekanntesten Musiknummern sind der "Lagunenwalzer" und das "Gondellied". Die Wiener Venedig-Mode hatte später ihren Höhepunkt im Themenpark Venedig in Wien.

Zwei Flöten, zwei Oboen, zwei Klarinetten, zwei Fagotte, vier Hörner, zwei Trompeten, drei Posaunen, eine Harfe, Schlagwerk und Streicher

"Die Operette spielt während des Karnevals in Venedig Mitte des 18. Jahrhunderts."

Bild: Platz am Canal Grande

Herzog Guido von Urbino ist in Venedig dafür bekannt, dass er in puncto Frauen nichts anbrennen lässt. Besonders während des Karnevals ist er darauf erpicht, seine Frau mit einer anderen Schönen zu betrügen. Zu diesem Zwecke hat er die Senatoren der Stadt mit ihren Gattinnen zu einem Maskenball eingeladen. Weil aber die Senatoren wissen, was der Lüstling im Schilde führt, haben sie beschlossen, ihre Frauen zu Hause zu lassen. Bartolomeo Delaqua geht sogar noch einen Schritt weiter: Er will seine ihm Angetraute während des Balles im außerhalb der Stadt gelegenen Kloster unterbringen und statt ihrer deren Zofe Ciboletta als Begleitperson mitnehmen. Dieser Plan ist Herzog Guido zu Ohren gekommen. Ausgerechnet die rassige Barbara soll nicht an seinem Fest teilnehmen? Dies muss unter allen Umständen verhindert werden, ist sie doch in diesem Jahr das Objekt seiner Begierde! Er hat sie zwar noch nicht gesehen, aber was man so alles über sie berichtet, hat seine Neugierde geweckt.

Caramello ist nicht nur Guidos Leibbarbier, sondern auch sein Faktotum, das ihm schon so oft in peinlichen Situationen geholfen hat. Er erhält den Auftrag, Barbara Delaqua aufzuspüren und seinem Palazzo zuzuführen.

Die schöne Barbara wiederum verspürt überhaupt keine Lust, den Plänen ihres Mannes Folge zu leisten. Sie ist vielmehr ebenfalls darauf aus, sich während des Karnevals zu amüsieren. In solchen Notsituationen pflegt sie mit dem Fischermädchen Annina die Kleider zu wechseln, um in deren Rolle zu schlüpfen. Mit Annina verbindet sie eine alte Freundschaft, schließlich hatten beide schon als Säuglinge dieselbe Amme gehabt. Barbara hat einen Verehrer, von dem ihr Gatte nichts weiß. Es ist sein Neffe, der charmante Seeoffizier Enrico Piselli. Mit ihm will sie heute das Karnevalstreiben genießen.

Caramello hat herausgefunden, dass ein Gondoliere Delaquas Frau zum Kloster bringen soll. Also drückt er dem jungen Mann ein erkleckliches Sümmchen in die Hand, und schon ist Caramello zu dem bestellten Gondoliere geworden. Jetzt hat er leichtes Spiel, die Schöne seinem Herrn und Meister zuzuführen.

Bild: Prunksaal im Palazzo des Herzogs

Bei der Ankunft im Palazzo stellt sich heraus, dass Caramello einem Irrtum aufgesessen ist; denn das Mädchen in der Gondel ist nicht des Senators Gattin, sondern das Fischermädchen Annina, das er nur allzu gut kennt. Schließlich hat er ihr schon mehrere Male die Ehe versprochen, aber immer gekniffen, wenn es ernst zu werden drohte. Und nun soll er Annina als Barbara Delaqua seinem Herrn vorstellen? Caramello hat kein gutes Gefühl dabei; doch er sieht keinen anderen Ausweg. Mit eifersüchtigen Blicken registriert er, dass seine Freundin in der Rolle der Senatorengattin eine glänzende Figur macht, und diese selbst kokettiert genüsslich mit ihrer Rolle. Auch der Herzog lässt sich täuschen und ist von der kessen Person aufs Höchste entzückt. Es gelingt ihm allerdings nicht, mit der Angebeteten auch nur eine Minute alleine zu sein.

Als neue Gäste werden Senator Delaqua mit Gattin angemeldet. Der Herzog ist zunächst verwirrt, glaubt er doch, dass letztere schon längst da sei. Annina aber besänftigt ihn: sie selbst sei schon die richtige. Die Begleiterin des Herzogs sei nur die Zofe Ciboletta. Aber diese scheint Pfeffer im Blut zu haben. Gleich macht sie sich an den Herrn des Hauses ran. Ihre Absicht ist, bei ihm eine Stelle für ihren Liebhaber, den Makkaronikoch Pappacoda, zu ergattern. Dies wiederum passt Delaqua überhaupt nicht. Er hat sie schließlich nur deshalb mitgenommen, damit sie für ihn beim Herzog einen Verwalterposten erbitte.

Bild: Auf dem Markusplatz

Inzwischen hat die Festgesellschaft ihr Karnevalstreiben vom Palazzo auf den Markusplatz verlagert. Senator Delaqua staunt nicht schlecht, als er dort seine liebe Gattin am Arm seines Neffen Enrico Piselli flanieren sieht. Eigentlich sollte sie doch noch im Kloster sein! Empört stellt er sie zur Rede. Und wie zieht sich die elegante Dame aus dem Schlamassel? Sie gaukelt ihrem Gatten einfach vor, der Gondoliere habe sie gegen ihren Wunsch ans falsche Ziel gebracht, ja, er habe sie regelrecht entführen wollen. Der teure Enrico aber habe dies bemerkt und sie aus der fatalen Lage befreit. Deshalb müsse auch er Enrico dankbar sein.

Als Senator Delaqua nun seine echte Ehefrau dem Herzog präsentiert, ist dieser verblüfft; schließlich ist sie heute bereits die dritte Dame, die Barbara Delaqua sein soll. Aber wie dem auch sei, er hat an Annina einen solch großen Gefallen gefunden, dass er sie unbedingt auf Dauer in seiner Nähe haben möchte. Da sie aber offenbar mit Caramello liiert ist, ernennt er diesen kurzerhand zu seinem Verwalter. Auf diese Weise wird Annina zwangsläufig des Öfteren im Palazzo auftauchen. Und wenn mal Caramello durch einen Auftrag von ihm verhindert ist, sich um seine Freundin zu kümmern, dann böte sich vielleicht doch noch die Gelegenheit,…

Die Fassung der Uraufführung wurde bereits für die Erstaufführung in Wien abgeändert. Dazu gehört auch die Ouvertüre, die gegenüber der „Berliner Fassung“, die kaum noch zu hören ist, von Strauss in Teilen neu komponiert wurde („Wiener Fassung“) und in dieser auch heute noch gespielt wird. Verlagsseitig wurde (und wird) allerdings über Jahre hinweg die Variante von Korngolds Bearbeitung 1931 angeboten, die in der Instrumentation erheblich verdickt wurde und gegenüber beiden Fassungen um ein Drittel der Musik eingekürzt wurde.

Gleichwohl wurden und ließen sich die dramaturgischen Mängel nicht so ohne Weiteres beseitigen.

Trotzdem wurde das Werk immer wieder gespielt, allerdings nunmehr zumeist in Bearbeitungen. Die wohl bekannteste der Zeit war die von Carl Hagemann 1916 in Mannheim.

Die im westlichen Europa bekannteste dieser Bearbeitungen stammt von Erich Wolfgang Korngold (1931). Im Zusammenhang mit einer Medien-Debatte um den angeblichen Niedergang der Wiener Operette bearbeitete er als Parteigänger der Konservativen das Stück im Jahr 1931 für das Theater an der Wien. Der Tenor Richard Tauber erhielt darin eine Paraderolle. Diese Fassung ist etwa an der Wiener Volksoper bis in die jüngste Zeit häufig gespielt worden und auch auf Tonträgern präsenter als das Original. Korngold verdickte die Instrumentation und fügte Musiknummern ein, die nicht zum originalen Bestand gehören. Das bekannteste dieser Stücke ist das Tenorlied "Sei mir gegrüßt, du holdes Venezia", dessen Übernahme aus dem "Simplicius" erst in den 1970er Jahren schlüssig nachgewiesen werden konnte.

Im östlichen Europa wurde die Bearbeitung von Walter Felsenstein (1954) wesentlich häufiger gespielt. Diese hielt sich, anders als die Bearbeitung von Korngold, deutlicher am Original und vermied die bei Korngold eingearbeiteten Nummern aus anderen Operetten von Strauss. Allerdings wurde in der originalen Bearbeitung von Felsenstein die Ouvertüre eliminiert (die allerdings dann meist von den Theatern trotzdem gespielt wurde), das Stück auf zwei Akte reduziert (was den Entfall einiger Musiknummern bedeutete) und vor allem die Figur der Annina als selbstbewußte und emanzipierte Frau in den Mittelpunkt gerückt.

Durch die zahlreichen Bearbeitungen wurde nach Auffassung von Fritz Racek und dem Strauss-Forscher Hans-Ullrich Barth überdies das Original auf einen Torso reduziert, „… verwässert oder musikalisch verfälscht, im Wert gemindert“.

Trotz der Bearbeitungen gehört der musikalische Bestand der Operette mit zu den bekanntesten Werken von Johann Strauss:

Aus der Musik der Operette schuf Strauss mehrere eigenständige Musikwerke, von denen das bekannteste Stück der „Lagunen-Walzer“ (op. 411) wurde.

Das Werk wurde dreimal verfilmt:




</doc>
<doc id="13677" url="https://de.wikipedia.org/wiki?curid=13677" title="Produktion">
Produktion

Produktion ( ‚hervor führen‘), insbesondere bei Gegenständen auch Fertigung, Fabrikation oder Verarbeitung, Bearbeitung, im rechtlichen Sprachgebrauch die Herstellung, sind die von Arbeitskräften ("Produzenten") mittels Arbeit bewirkten Prozesse der Transformation, die aus natürlichen wie bereits produzierten Ausgangsstoffen (Werkstoffe) unter Einsatz von Energie und bestimmten Produktionsmitteln lagerbare Wirtschafts- oder Gebrauchsgüter ("Ökonomisches Gut") erzeugen.

In seinen Beiträgen zur "Politischen Ökonomie" definiert Karl Marx die "Produktion" philosophisch im Rahmen der Gesellschaftssysteme als ökonomischen Prozess neben der "Distribution", der "Verteilung/Aneignung" und der "Konsumtion".

Vom Standpunkt der Betriebswirtschaftslehre ist die Produktion betriebsorganisatorisch als Teil des Geschäftsprozesses eine betriebliche Funktion in Unternehmen. Eine umfassende Betrachtung der Produktion erstreckt sich nicht nur auf organisatorische und technologische Gesichtspunkte, sondern auch auf soziokulturelle und ethisch-normative Wertvorstellungen über die Arbeit, denen sie als einer der grundlegenden Vorgänge zur Deckung der menschlichen Bedürfnisse unterliegt.

Der Begriff der Produktion bezeichnet die "Herstellung" von "Gütern" im Allgemeinen. Produziert wird in der Industrie, im Handwerk, in der Land- und Forstwirtschaft, im Bergbau, oder auf künstlerischem Gebiet. Diese Produktion gliedert sich allgemein in Urproduktion (Primärsektor der Wirtschaft), also Herstellung von Wirtschaftsgut aus Naturressourcen, und die "Herstellung von Waren." Diesbezüglich sind die Begriffe "Herstellung" und "Verarbeitung" fließend, so wird beispielsweise wirtschaftssystematisch prinzipiell in Metallhersteller (Bergbau und Verhüttung) und Metallverarbeiter (Warenproduktion) unterschieden, die dazwischenstehende Produktion von Halbzeug (wie Blech), an und für sich auch schon Handelsware, wird meist noch den Herstellern zugeordnet, obschon fertigungstechnisch schon eine Verarbeitung von Rohmetall vorliegt. Die Ausdrücke "Fertigung" oder "Fabrikation" sind auf handwerkliche und industrielle Güter und Waren beschränkt.

"Bearbeitung" bezieht sich auf die Veränderung der äußeren Gestalt eines Werkstückes; bei der "Verarbeitung" wird das Rohteil verbraucht und liegt in chemisch anderer Form vor, beispielsweise verbrannt bei Brennstoffen oder ausgeschmolzen bei der Verarbeitung von Erzen zu Metallen.

Auch in der Landwirtschaft wird zum Beispiel im Obstbau die Handelsware bei Tafelobst direkt ohne jede Verarbeitung (ur-)produziert, während der Sektor Lebensmittelherstellung primär die Verarbeitung von Tieren und Pflanzen umfasst.

Dienstleistungen sind auch "produzierbar", bei ihnen erfolgt jedoch die Produktion und der Konsum gleichzeitig. In Verlagen bleibt die traditionelle Bezeichnung Herstellung als großer Arbeitsbereich, zuständig für Papier und Online-Organisation, Satz etc. Nicht lagerbare Wirtschaftsgüter, wie beispielsweise Elektrizität oder Druckluft werden "bereitgestellt".

In der industriellen Produktionstechnik wird die "Fertigung" (von Stückgütern), die "Verfahrenstechnik" (von Fließgütern / von Rohstoffen zu Gütern) und die "Energietechnik" unterschieden.

Die Betriebswirtschaftslehre führt die Produktion als eine der klassischen Funktionen im Betrieb (Beschaffung, Produktion, Absatz). Die Produktion lässt sich noch in Beziehung setzen mit Entwicklung, Konstruktion, Arbeitsvorbereitung, Fertigung und Montage. Diese Prozesse müssen koordiniert und gemanagt werden. Mit ihr befassen sich vor allem die Produktionswirtschaft und die Industriebetriebslehre. Die Betrachtung der Produktion ist teils auf die industrielle Fertigung ausgerichtet, teils werden auch handwerkliche und weitere Fertigungsmethoden mit einbezogen. Manche Wissenschaftler betrachten nur die Erzeugung von Sachgütern als Produktion, andere beziehen ausdrücklich Dienstleistungen mit ein. Da bei „Produktion“ also nicht klar ist, ob Dienstleistungen inbegriffen sind, verwenden manche Wissenschaftler den Begriff „Leistungserstellung“, um zu verdeutlichen, dass Dienstleistungen "und" Sachgüter gemeint sind. Analog wird auch von der Leistungsverwertung für den Absatz gesprochen. Hintergrund ist, dass immer mehr Sachgüter zusammen mit Dienstleistungen verkauft werden, etwa im Falle eines Herstellers einer komplexen Maschine, der die Schulung der Mitarbeiter im Kundenunternehmen übernimmt, sowie die Programmierung, Wartung und Lieferung von Ersatzteilen.

Es existieren mehrere Definitionen von Produktion

Die verschiedenen Produktionsverfahren werden aus betriebswirtschaftlicher Sicht unterschieden nach der Anzahl der gefertigten Teile als Einteilungskriterium (Einzel-, Serien-, Massenfertigung) und der "Fertigungsorganisation" (Werkstatt- oder Reihenfertigung).

Volkswirtschaftlich werden Begriffe wie Produzierendes Gewerbe und Produktionswert verwendet. In der Volkswirtschaftslehre beschreibt die Produktionstheorie die Herleitung der Angebotskurve (Angebot (Volkswirtschaftslehre)) im Marktmodell. Von einer Technologie ausgehend, die alle technisch machbaren Kombinationen von Inputfaktoren beschreibt, lässt sich die effizienteste Faktorkombination – für gegebene Preise – herleiten (sogenannte Gewinnmaximierung). Daraus lassen sich die Faktornachfrage und das Güterangebot herleiten.




</doc>
<doc id="13679" url="https://de.wikipedia.org/wiki?curid=13679" title="Akt">
Akt

Akt (von lat. "actus" „Tat“, zu "agere" „handeln“, Partizip "actum") steht für:


Im juristischen Bereich:

Außerdem:

Akt ist der Familienname folgender Personen:

akt. steht als Abkürzung für:

Akt. steht als Abkürzung für:

AKT steht als Abkürzung für:

Siehe auch:


</doc>
<doc id="13680" url="https://de.wikipedia.org/wiki?curid=13680" title="33">
33









</doc>
<doc id="13681" url="https://de.wikipedia.org/wiki?curid=13681" title="Marx Augustin">
Marx Augustin

Marx Augustin oder Der liebe Augustin (eigentlich "Markus Augustin"; * 1643 in Wien; † 11. März 1685 ebenda) war ein Bänkelsänger, Sackpfeifer und Stegreifdichter. Er wurde durch die Ballade „O du lieber Augustin“ sprichwörtlich.

Zum Leben Augustins ist wenig gesichert. Augustin soll sehr beliebt gewesen sein, weil er mit seinen zotigen Liedern vor allem während der Pest in Wien im Jahr 1679 die Bevölkerung der Stadt aufheiterte, weshalb er im Volksmund nur als „Lieber Augustin“ bekannt war.

Augustin soll als Sohn eines heruntergekommenen Wirts aufgewachsen sein und war demnach schon früh darauf angewiesen, mit seinem Dudelsack von einer Spelunke zur nächsten zu ziehen, wobei nur wenig von dem verdienten Geld die jeweilige Kneipe verlassen haben soll – der Überlieferung nach soll er auch ein „tüchtiger Trinker“ gewesen sein. 

Der Legende nach war der 36-jährige Augustin 1679 während der Pestepidemie wieder einmal betrunken und schlief irgendwo in der Gosse seinen Rausch aus. Siech-Knechte, die damals die Opfer der Epidemie einsammeln mussten, fanden ihn, hielten ihn für tot und brachten die Schnapsleiche zusammen mit den Pest-Leichen auf ihrem Sammelkarren vor die Stadtmauer. Dort warfen sie ihre ganze Ladung in ein offenes Massengrab. Diese Pestgrube soll sich in der Nähe der Kirche St. Ulrich im siebten Wiener Gemeindebezirk befunden haben, gleich neben dem Platz, an dem heute der Augustinbrunnen steht. Wie in der damaligen Situation üblich, wurde das Grab nicht sofort geschlossen, sondern provisorisch mit Kalk abgedeckt, um später weitere Pestopfer aufzunehmen. Am folgenden Tag habe Augustin inmitten der Leichen so lange krakeelt und auf seinem Dudelsack gespielt, bis Retter ihn aus der Grube zogen.

Danach soll Augustin sein Erlebnis als Bänkelsänger vorgetragen und davon recht gut gelebt haben. So ist die Legende vom lieben Augustin vielleicht seinem eigenen Bericht zu verdanken. Bereits zeitgenössische Quellen berichten von dem der Leichengrube entstiegenen Augustin. Abraham a Sancta Clara erwähnt das Ereignis in seinem „Wohlangefüllten Weinkeller“, um vor der Trunksucht zu warnen. Urkundliche Stütze für die Legende ist nur ein Eintrag im städtischen Totenschauprotokoll, das einen „Augustin N.“ verzeichnet.

Augustin wurde auf dem Wiener Nikolai-Friedhof beerdigt, nach dessen Auflassung wurden seine sterblichen Überreste vermutlich auf den Sankt Marxer Friedhof überführt.

Das Volkslied "O du lieber Augustin" ist erst um 1800 in Wien nachgewiesen. Die sehr verbreitete Melodie ist jedoch älter, so ist sie 1720 in einer Musikhandschrift belegt. Teilweise wird Augustin selbst als Verfasser genannt, der Ursprung ist jedoch unklar. Der spöttische Text gibt aber den Galgenhumor wieder, der den Wienern in Erinnerung geblieben ist:

Johann Nepomuk Hummel (1778–1837) komponierte aus dem Lied ein Werk mit acht Variationen. 1924 gestaltete Dietzenschmidt seine "Volkskomödie mit Musik, Gesang und Tanz ‚Vom lieben Augustin‘" (Bühnenmusik u. a. Ernst Krenek) zu einer mythischen Begegnung, in welcher sich der weltzugewandte Triebmensch Augustin nach einigen Episoden von Gewinn und Verzicht am Ende dem Liebreiz des „Pestmädels“ ergibt (einer Wiener „Frau Tod“). 1940 zeigte der Film "Der liebe Augustin" die Figur unabhängig von der Legende als Bänkelsänger in der Metternich-Zeit (Regie: E. W. Emo, Titelrolle: Paul Hörbiger). Franz Karl Ginzkey dichtete eine "Ballade vom Lieben Augustin". 1981 haben Wolfgang Ambros, Manfred Tauchen und Joesi Prokopetz dem Bänkelsänger eine Rockoper mit dem Titel "Augustin" gewidmet.

Auf dem Strohplatzl in Wien wurde am 4. September 1908 zu Augustins Ehren ein Denkmal eingeweiht. Der Augustin-Brunnen steht an der Ecke Neustiftgasse-Kellermanngasse. Während der Zeit des Nationalsozialismus wurde die Statue Augustins gestohlen. Kurz darauf wurde an ihre Stelle ein Schild mit der Aufschrift angebracht:

Seit einiger Zeit sitzt eine Puppe, die den lieben Augustin darstellen soll, als Touristenattraktion im Eingang des Griechenbeisls am Wiener Fleischmarkt, wo Augustin einst regelmäßig auftrat. 

Im Jahr 2008 wurde in Wien Neubau (7. Bezirk) der "Augustinplatz" nach ihm benannt, wobei sich die Benennung auch auf die Sängerin Liane Augustin (1928–1978) bezieht.

Bis heute ist er ein Inbegriff dafür, dass man mit Humor alles überstehen kann.

Daniel Defoe schildert eine recht ähnliche Person in seinem 1722 erschienenen Buch "Die Pest zu London": Einen namenlosen Flötenspieler zur Zeit der Pest in London 1665, der die Leute während der Zeit der Seuche mit fröhlichen Liedern und Späßen unterhielt und im Alkoholrausch oder infolge übermäßigen Essens schlafend für einen Pesttoten gehalten wurde und auf einem Karren mit den anderen Leichen transportiert wurde, aber erwachte, kurz bevor er ins Massengrab geworfen wurde.


Eine ganz andere Figur, nämlich die eines Spieluhrenmachers im späten achtzehnten Jahrhundert am Bodensee, gestaltet der Roman



</doc>
<doc id="13683" url="https://de.wikipedia.org/wiki?curid=13683" title="Vanuatu">
Vanuatu

Vanuatu (auf Bislama: "Ripablik Blong Vanuatu") ist ein souveräner Inselstaat im Südpazifik.

Der aus 83 Inseln bzw. Inselgruppen bestehende Staat ging 1980 aus dem seit 1906 bestehenden britisch-französischen Kondominium Neue Hebriden hervor und hat heute etwa 267.000 Einwohner.

Das Inselgebiet von Vanuatu erstreckt sich über 1300 km des Südpazifiks und zählt zu Melanesien. Zum Staat gehören 83 Inseln (davon 67 bewohnte Inseln), meist vulkanischen Ursprungs, welche überwiegend zur Inselgruppe Neue Hebriden zählen. Weiterhin gehören die Banks- und die Torresinseln zu Vanuatu.

Nur wenige dieser vanuatuischen Inseln haben eine Größe, die sie bedeutend macht. Die größten sind Espiritu Santo (3955,5 km²) und Malakula (2041,3 km²). Der höchste Punkt auf Vanuatu ist der Tabwemasana (Tabwémasana) mit 1879 m auf der Insel Espíritu Santo.

Bemerkenswert ist der aktive Vulkan Mount Yasur auf der Insel Tanna sowie der Lombenben auf der Insel Ambae, der im November 2005 Aktivität zeigte. Immer wieder erschüttern Erdbeben die Inseln, so auch 1999 und 2002. Letzteres richtete in der Hauptstadt Port Vila erheblichen Schaden an. Auf beide Erdbeben folgte ein Tsunami.

Viele der Inseln von Vanuatu sind schon seit Jahrtausenden bevölkert. Die ältesten Funde werden auf das Jahr 2000 v. Chr. datiert. Der portugiesische Seefahrer Pedro Fernández de Quirós erreichte am 3. Mai 1606 Espíritu Santo. Im Glauben, den „verlorenen“ südlichen Kontinent gefunden zu haben, nannte er die Insel nach dem Heiligen Geist "Terra Australis del Espiritu Santo" und nahm sie und alles bis zum Südpol liegende Land im Namen des spanischen Königs und der katholischen Kirche in Besitz.

1768 segelte Louis Antoine de Bougainville auf der Fregatte "La Boudeuse" zwischen Espíritu Santo und Malakula und widerlegte somit Quirós Theorie, es handle sich um den Teil eines südlichen Kontinents.

Nach der zweiten Reise des britischen Entdeckers James Cook ließen sich ab 1839 europäische Siedler auf den Inseln nieder. Ab 1887 standen die Inseln offiziell unter britisch-französischer Kontrolle.

Franzosen und Engländer einigten sich 1906 auf die Gründung des Kondominiums Neue Hebriden auf den Neuen Hebriden.

Auf Grund verschiedener Infektionskrankheiten, die vor allem durch die europäischen Siedler ins Land gebracht wurden, fiel die Bevölkerung bis 1935 auf 45.000 Einwohner.

Während des Zweiten Weltkrieges wurden die Inseln Éfaté und Espíritu Santo von den Alliierten als Militärbasen genutzt. In den 1960er Jahren drängte die Bevölkerung zu mehr Selbstbestimmung und später nach Unabhängigkeit. Volle Souveränität erlangte der Inselstaat am 30. Juli 1980 durch die Zustimmung der beiden europäischen Schutzmächte. 1981 trat Vanuatu den Vereinten Nationen bei und zwei Jahre später der Bewegung der blockfreien Staaten.

Die 1990er Jahre waren geprägt von politischer Instabilität, was zu einer größeren Dezentralisierung im politischen System des Inselstaates führte.

Zwischen dem 14. und 15. März 2015 wütete in Vanuatu der Zyklon Pam, der weite Teile des Inselstaates zerstörte, weswegen der Inselstaat am 15. März 2015 den nationalen Notstand ausrief. Der Zyklon war, mit Windgeschwindigkeiten von mehr als 300 km/h, einer der stärksten je gemessenen Zyklone.
In der Hauptstadt Port Vila wurden 90 % aller Gebäude zerstört oder stark beschädigt.

Der Zyklon wird als die schwerste Katastrophe in der Geschichte von Vanuatu bezeichnet.
Laut Angaben der Vereinten Nationen kamen mindestens 24 Menschen durch den Zyklon ums Leben, 3300 Menschen wurden obdachlos. Präsident Baldwin Lonsdale gab dem Klimawandel eine Mitschuld am Ausmaß der Zerstörung.

Vanuatu ist eine parlamentarische Republik mit einem Präsidenten als Staatsoberhaupt. Der Präsident, der hauptsächlich repräsentative Funktionen ausübt, wird alle fünf Jahre von einem gemeinsamen Gremium aus Mitgliedern des Parlaments und den Präsidenten der Regionalparlamente gewählt. Vom 22. September 2014 bis zu seinem Tod war Baldwin Lonsdale aus der Provinz Torba nach insgesamt acht Wahlgängen, bevor er die notwendige Zweidrittelmehrheit mit 46 von 58 Stimmen erreichte, Präsident des Landes. Er löste den vom 2. September 2009 bis 2. September 2014 amtierenden Iolu Abil und dessen danach amtierenden Interimspräsidenten Philip Boedoro ab. Nachfolger des in der Region angesehen Baldwin Lonsdale wurde im Juli 2017 Tallis Obed Moses.

Der Regierungschef Vanuatus ist der Premierminister, der vom Parlament mit Dreiviertelmehrheit gewählt wird. Der Premierminister bestimmt selbst die Mitglieder seines Kabinetts.

Seit dem 16. Februar 2016 ist Charlot Salwai Premierminister und löste seinen Vorgänger Sato Kilman ab.

Vanuatu hat ein Einkammersystem. Das Parlament hat 52 Mitglieder, die alle vier Jahre in Mehrpersonenwahlkreisen direkt gewählt werden. Der Premierminister kann das Parlament vorzeitig vom Präsidenten auflösen lassen.

Neben dem Parlament gibt es den "Malvatu Mauri", auch "National Council of Chiefs" genannt. Dieser Häuptlingsrat hat aber nur beratende Funktion.

Das oberste rechtsprechende Staatsorgan von Vanuatu ist der Oberste Gerichtshof, der aus einem Obersten Richter und bis zu drei weiteren Richtern besteht. Das Rechtssystem basiert auf dem britischen Recht.

Die kleinen Matthew- und Hunterinseln, welche etwa 250 km südöstlich von Anatom (Provinz Tafea), der südlichsten Hauptinsel von Vanuatu, liegen, werden sowohl von Neukaledonien (Frankreich) als auch von Vanuatu beansprucht.

Seit dem 26. Oktober 2011 ist Vanuatu Mitglied der WTO.

Vanuatu verfügt über keine regulären Streitkräfte. Für die Sicherheit ist neben der Polizei (Vanuatu Police Force) auch die paramilitärische Vanuatu Mobile Force zuständig, die auch über einen maritimen Arm, den Police Maritime Wing, verfügt. Vanuatuische Sicherheitskräfte haben bereits an Einsätzen der Vereinten Nationen teilgenommen.

Vanuatu besteht aus den sechs Provinzen Malampa, Penama, Sanma, Shefa, Tafea und Torba, deren Namen sich aus den Namen der Einzelinseln zusammensetzen.

Vanuatu hat ca. 267.000 Einwohner (2014), etwa 98,5 % der Bevölkerung sind Melanesier.

32,6 % der Bevölkerung sind bis 14 Jahre alt, von 15 bis 64 Jahre sind es 63,7 %, über 64 Jahre sind lediglich 3,7 % der Einwohner. Vanuatu hat ein jährliches Bevölkerungswachstum von etwa 1,5 %, wobei die Geburtenrate bei 22,72 pro 1000 Einwohner und die Sterberate bei 7,82 pro 1000 Einwohner liegt. Die Säuglingssterblichkeit liegt bei 5,4 %. Die Lebenserwartung der Bevölkerung liegt im Zeitraum von 2010 bis 2015 bei 71,4 Jahren (Männer: 69,4 Jahre, Frauen: 73,6 Jahre). 26 % der über 15-jährigen sind Analphabeten.

Quelle: UN

72,6 % der Bevölkerung geben als Muttersprache eine der 110 Sprachen Vanuatus an. Mit dieser Menge an Sprachen hat Vanuatu die höchste Sprachendichte (Sprachen pro Einwohner) der Welt. Alle diese Sprachen zählen zum melanesischen Zweig der ozeanischen Sprachgruppe. Bislama, eine in der britisch-französischen Kolonialzeit entstandene Kreolsprache, wird von knapp 23,1 % als erste Muttersprache angegeben, tatsächlich stellt es aber die tägliche Sprache der Einwohner des Inselstaates dar. Neben Bislama gelten auch Englisch und Französisch als Amtssprachen, werden aber kaum noch aktiv gesprochen: Englisch wird von 1,9 % der Einwohner, Französisch von 1,4 % gesprochen. Andere Sprachen werden von 0,3 % der Gesamtbevölkerung gesprochen.

Einwohner ohne gemeinsame Sprache verständigten sich früher über Zeichnungen, die in den Sand gezeichnet wurden. Rituelle Sandzeichnungen, die aus einer durchgehenden Linie bestehen, wurden 2003 von der UNESCO als Kulturerbe der Menschheit anerkannt.

31,4 % der Bevölkerung gehören der presbyterianischen Kirche an und 13,4 % der anglikanischen Kirche. 13,1 % der Einwohner sind römisch-katholisch, 10,8 % sind Siebenten-Tags-Adventisten und 13,8 % gehören einer anderen christlichen Konfession an. Daneben gibt es noch etliche einheimische Glaubensrichtungen, zu denen sich aber nur noch 5,6 % der Gesamtbevölkerung bekennen. Die bedeutendsten sind Cargo-Kulte wie die John-Frum-Bewegung und die Prinz-Philip-Bewegung. (Alle demografischen Angaben sind auf dem Stand von 2006)

Die Wirtschaft von Vanuatu besteht überwiegend aus Landwirtschaft, Fischerei und Tourismus. Zu den wichtigsten Kulturpflanzen gehören Kokospalmen, Erdnüsse, Bananen und Mais, die hauptsächlich für den Inlandsbedarf kultiviert werden. Im Jahr 2010 wurden auf einer Fläche von 96.000 ha Kokospalmen (Ertrag: 170.000 t), auf 2428 ha Erdnüsse (Ertrag 2011: 2616 t), auf 1455 ha Bananen (Ertrag: 10.735 t) und auf 1402 ha Mais (Ertrag: 783 t) angebaut.

Auf einigen Farmen werden Rinder und Schweine gehalten, die 2010 2500 t Rindfleisch und 3417 t Schweinefleisch erbrachten. Vereinzelt werden auch Kaffee (2011: 25 ha), Kakao (2011: 500 ha) und Gewürze (2011 auf sieben Hektar mit einem Ertrag von 95 t) angebaut. Die Wirtschaft ist trotz eines expandierenden Dienstleistungssektors noch stark landwirtschaftlich geprägt und somit anfällig für Naturkatastrophen und Wetterschwankungen. Es existieren keine nennenswerten Bodenschätze.

Etwa 65 % der Bevölkerung bestreiten ihren Lebensunterhalt durch den primären Sektor. Durch diverse Wirtschaftsreformen wie die Einführung einer Mehrwertsteuer von 12,5 % gelang es, den Dienstleistungssektor zu stärken, so dass der Tourismus weiter expandieren konnte. 2011 waren es ca. 94.000 Besucher. Auf Kreuzfahrtschiffen erreichten 125.000 Tagesbesucher die Inseln.

Vanuatu erlangt auch zunehmend Bedeutung als Offshore-Finanzplatz. In Vanuatu gibt es keine Einkommenssteuern, Körperschaftssteuern oder Kapitalertragssteuern. Der Staatshaushalt wird durch Einfuhrsteuern, die Mehrwertsteuer (12,5 %) und durch diverse Gebühren finanziert.

Exportiert werden in erster Linie landwirtschaftliche Produkte, wobei die Mengen jährlich stark schwanken können. 2010 wurden Agrargüter im Wert von 24 Mio. US-$ exportiert, jedoch mussten Nahrungsmittel im Wert von 56,3 Mio. US-$ importiert werden.
Auch in den Jahren davor überstieg der Wert der Lebensmittelimporte (2009: 53,1 Mio. US-$) stets den Wert der Lebensmittelexporte (2009: 28,1 Mio. US-$) im erheblichen Maße. Vanuatu exportiert u. a. Rindfleisch (2010: 352 t nach 904 t im Vorjahr), Kopra (2010: 5693 t nach 15.107 t im Vorjahr und 3205 t 2008), Kakao (2010: 1978 t nach 1480 t im Vorjahr) sowie geringe Mengen von Kaffee (2010: 1 t nach 4 t im Vorjahr) und Bananen.

Am 28. Januar 2016 legte die EU-Kommission ein Maßnahmenpaket "zur Bekämpfung von Steuerflucht" vor, bei dem unter anderem Vanuatu auf der "schwarzen Liste" der Steueroasen auftaucht.

Die wichtigsten Handelspartner sind Neukaledonien, Australien und Neuseeland.

Als Währung fungiert auf Vanuatu der Vatu (VUV). Am betrug der Wechselkurs 1 EUR =  VUV, entsprechend 100 VUV =  EUR.

Die Betreiber des Kazaa-Netzwerkes von Sharman Networks haben sich dort niedergelassen, um Verfahren in den Niederlanden zu entgehen und von Steuervorteilen zu profitieren. Auch WinMX, eine Peer-to-Peer-Software und der OnlineTVRecorder wechselten ihren Standort dorthin.

Der Staatshaushalt umfasste 2005 Ausgaben von umgerechnet 72,2 Mio. US-Dollar, dem standen Einnahmen von umgerechnet 78,7 Mio. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsüberschuss in Höhe von 1,8 % des BIP.

Die Staatsverschuldung betrug 2008 106 Mio. US-Dollar oder 18,5 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Auf der Insel Pentecost wird noch immer der traditionelle Tauschhandel betrieben. Die Tanbunia-Bank hat sich darauf spezialisiert. So besitzt jeder Gegenstand – zum Beispiel eine Schilfmatte, eine Muschel, ein Wildschwein, der Stoßzahn eines Wildschweins oder einfach ein wohlgeformter Stein – einen Wert, der nach einer in der Bank befindlichen Tabelle in die Währung Livatu umgerechnet wird. Selbst ein guter Rat oder eine alte Geschichte können auf diese Weise in Geldeswert umgerechnet werden.

So gesehen „besitzt“ jeder Einwohner etwas, das er eintauschen kann. Der Tanbunia-Bank ist sogar daran gelegen, dass niemand Mangel leidet oder verarmt.

Der Wert eines Livatu entspricht dem Wert eines guten Schweinestoßzahnes und der Staat Vanuatu garantiert die Umtauschmöglichkeit in die offizielle Landeswährung Vatu.

Das knapp eintausend Kilometer lange Verkehrsnetz Vanuatus besteht zu 75 % aus nicht asphaltierten Straßen und Wegen. Für die Überwindung von Mittel- und Langstrecken stehen auf Vanuatu insgesamt 31 Flugplätze bereit, davon drei mit asphaltierten Start- und Landebahnen. Einer der wichtigsten Flughäfen ist der Flughafen Bauerfield. Die nationale Fluglinie ist Air Vanuatu. Die Handelsmarine verfügt über etwa 50 Schiffe, von denen keines einen inländischen Besitzer hat. Größere Häfen befinden sich in Forari, Port Vila und Espiritu Santo.

Die Kulturlandschaft Chief Roi Mata’s Domain ist seit 2008 UNESCO-Welterbe. Auch die Sandzeichnungen, die früher der Kommunikation zwischen den einzelnen Inseln und Stämmen dienten, wurden von der UNESCO als Weltkulturerbe anerkannt.

Einer der bedeutendsten zeitgenössischen Künstler Vanuatus ist der von der Insel Wallis stammende Aloi Pilioko, der u. a. das farbenfrohe Relief an der Fassade der Hauptpost in Port Vila schuf. Unweit davon ist ein interessantes Wandgemälde auf der Fassade des Verwaltungsgebäudes gegenüber der Markthalle zu sehen.
Eine Kurzwellenstation, die mit 10 kW ein Inlandsprogramm in Englisch, Französisch und Bislama ausstrahlt, kann bei günstigen Empfangsbedingungen gelegentlich auch in Mitteleuropa empfangen werden. Die Sendefrequenzen sind: 3.945, 5.055 und 7.260 kHz. Die Top-Level-Domain von Vanuatu ist ".vu". Mit dem Verkauf der ".vu"-Domain verdiente Vanuatu mehr als 42 Millionen Euro.




</doc>
<doc id="13687" url="https://de.wikipedia.org/wiki?curid=13687" title="Regenbogen">
Regenbogen

Der Regenbogen ist ein atmosphärisch-optisches Phänomen, das als kreisbogenförmiges farbiges Lichtband in einer von der Sonne beschienenen Regenwand oder -wolke wahrgenommen wird. Sein radialer Farbverlauf ist das mehr oder weniger verweisslichte Spektrum des Sonnenlichts. Jeder der annähernd kugelförmigen Regentropfen bricht das Sonnenlicht beim Ein- und beim Austritt und reflektiert es innen an seiner Rückwand. Das in Richtung auf die Sonne zurückgeworfene Licht wird dabei zum überwiegenden Teil in einem Kegelmantel konzentriert. Der Beobachter hat die Schicht reflektierender Wassertropfen vor sich und die Sonne im Rücken. Diejenigen Tropfen, aus deren Kegelmantel ihn ein farbiger Lichtstrahl erreicht, befinden sich auf einem kreisförmigen Band, dessen bogenförmigen Ausschnitt der Regenbogen darstellt.

Unter guten Bedingungen ist über dem kräftigen "Hauptregenbogen" ein weiterer, schwächerer mit umgekehrter Farbfolge sichtbar, als "Nebenregenbogen." Gelegentlich werden auf der blauen Seite des Regenbogens weitere schmale Lichtbänder sichtbar, die Interferenzstreifen oder "überzähligen Regenbögen."

Das Sonnenlicht ist ein kleiner Teil des Spektrums aller elektromagnetischen Wellen. Bei hochstehender Sonne erreichen alle Anteile des Sonnenspektrums die Erdoberfläche und ihre Mischung wird als weißliches Tageslicht wahrgenommen. Bei tiefstehender Sonne ist die Lichtfarbe rötlicher, da der kurzwellige blaue Anteil des Sonnenspektrums in der Atmosphäre stärker gestreut wird als der langwellige rote, was zum Beispiel zu Morgenrot führt.

Die Farben des Regenbogens entstehen durch Brechung des Sonnenlichts in den Wassertropfen, wobei dieses wie in einem Prisma (Abbildung rechts) wellenlängenabhängig unterschiedlich stark abgelenkt wird. Im Regenbogen sind im Allgemeinen die Farben weniger rein und weniger deutlich voneinander getrennt als im zum Beispiel mit Hilfe eines Prismenspektroskops erzeugten Lichtspektrum (zweite Abbildung rechts). Ursache ist die teilweise Mischung der beim Eintritt in den Wassertropfen getrennten farbigen Lichter durch ihre innere Reflexion an unterschiedlichen Stellen der kugelförmigen Tropfenfläche und ihre erneute Ablenkung beim Austritt. Farbig gleiche Lichtstrahlen aus benachbarten Eingangsstrahlen können vereinigt werden, wobei sie sich durch Interferenz verstärken oder auslöschen können.

Das während oder kurz nach einem Regenereignis parallel auf die fallenden, eng benachbarten Regentropfen wie auf eine Wand treffende Sonnenlicht wird durch jeden von ihnen in einem Kegelmantel konzentriert gegen die Sonne zurückgeworfen. Ein solcher Kegel besteht aus ineinander steckenden Kegeln unterschiedlicher Kegelwinkel für die unterschiedlichen Lichtfarben. Beim Hauptregenbogen mit einmaliger innerer Reflexion hat der weniger abgelenkte äußere rote Lichtkegel einen Winkel von etwa zweimal 42°, der stärker abgelenkte innere blaue einen Winkel von etwa zweimal 40,2°.

Blickt der Beobachter zur „Regenwand“, so empfängt er rotes Licht aus Tropfen, die sich von ihm aus gesehen ebenfalls auf einem Kegelmantel mit einem Winkel von etwa zweimal 42° befinden. Der Beobachter befindet sich in der Spitze dieses Kegels, dessen Achse von der Sonne durch den Beobachter zum Sonnengegenpunkt führt. Das blaue Licht am inneren Rand des Regenbogens kommt aus Tropfen, die sich auf einem Kegelmantel mit einem Winkel von etwa zweimal 40,2° befinden. Aus jedem Tropfen stammt jeweils nur ein Kegelmantelstrahl, der beim Beobachter ankommt. Weil die Zahl der Regentropfen aber unvorstellbar groß ist, kommt ein aus farbigen Streifen bestehender Regenbogen in auffallender Helligkeit zustande.

Der Hauptregenbogen entsteht durch Sonnenlicht, das in einen kugelförmigen Wassertropfen eindringt, im Innern einmal reflektiert wird und dann wieder aus dem Tropfen austritt.

Wenn formula_1 der Eintrittswinkel, Winkel zur Senkrechten, ist und
formula_2 der Winkel zur Senkrechten im Wassertropfen dann gilt nach dem Brechungsgesetz

wobei formula_4 die wellenlängenabhängige Brechungszahl des Wassers ist.

Der Winkel zur Senkrechten beim Eintritt innerhalb des Tropfens, der entsprechende Winkel bei der Reflexion und auch beim Austritt aus dem Tropfen formula_2 tritt in gleichschenkligen Dreiecken mit zwei Seitenlängen gleich dem Radius des Tropfens auf. Diese Winkel sind daher alle identisch.

Die gesamte Winkeländerung beim Durchgang durch den Tropfen ergibt damit als

Maximale Intensität tritt auf, wenn der Winkel der gesamten Ablenkung sich bei Variation des Einfallswinkels nicht ändert. Dies geschieht, wenn die Ableitung nach dem Eintrittswinkel null wird, also

Diese Bedingung ist für formula_8 etwa gleich 180° minus 42° erfüllt.

Der Nebenbogen entsteht bei zwei Reflexionen innerhalb des Tropfens. Die Winkeländerung kann völlig analog für eine beliebige Zahl an Reflexionen "k = 1, 2, 3, …" berechnet werden:

Maximale Intensität tritt beim Eintrittswinkel

auf, bei dem die Ableitung der Gesamtablenkung nach dem Einfallswinkel gleich null ist.

Ein Regentropfen ist transparent und während des Falls in guter Näherung eine kleine Kugel. Die Abbildung links zeigt den Weg eines Sonnenstrahls durch einen Regentropfen. Beim Ein- und Austritt sind die am Tropfenrand reflektierten Teile und bei der inneren Reflexion die austretenden Teile des Strahls nicht gezeichnet. Diese Strahlteile sind an der Entstehung des Regenbogens nicht beteiligt, sie reduzieren lediglich dessen Intensität.

Beim Eintritt werden die verschiedenen farblichen Anteile des Sonnenstrahls nach dem farbabhängigen Brechungsgesetz verschieden stark abgelenkt, rot am wenigsten, violett am stärksten. Innerhalb des Tropfens werden die entstandenen Farbstrahlen an nicht genau gleichen Stellen der kugelförmigen Rückwand reflektiert. Ihr Austritt erfolgt ebenfalls nicht an einer einzigen, genau gleichen Stelle am Tropfenrand. Die erneute Ablenkung durch Brechung ist zudem noch von der Farbe jedes Teilstrahls abhängig.

In der Abbildung rechts ist gezeigt, wie das gesamte einen Tropfen passierende Licht an der Mantelfläche eines Kegels konzentriert von diesem zurückgeworfen wird. Zur Förderung der Übersichtlichkeit ist der Vorgang für rotes Licht, das heißt nur für eine der im Sonnenlicht enthaltene Farben, dargestellt. Die Bilder für die anderen Farben weichen geringfügig davon ab. Typischer Unterschied ist der Winkel des begrenzenden Kegelmantels ("Regenbogenwinkel:" 2 mal 42° für Rot; 2 mal 40,2° für Blau). Die Darstellung ist ein Schnitt durch Tropfen- und Kugelmitte. Um die horizontal gezeichnete Mittenachse besteht Rotationssymmetrie.

Die Abbildung links zeigt die Winkelbeziehungen zwischen Beobachter, Regentropfen und Sonne. Da der in den Tropfen eintretende und der den Beobachter passierende Sonnenstrahl parallel sind, schneidet ein Strahl zwischen Regenbogen und Beobachter beide Sonnenstrahlen unter gleichen Wechselwinkeln. Im Bild sieht der Beobachter einen aus einem Tropfen austretenden roten Strahl (Wechselwinkel 42°). Um den den Beobachter passierenden Sonnenstrahl besteht bezüglich Licht aus weiteren Regentropfen Rotationssymmetrie. Die ihn aus vielen Tropfen erreichenden Lichtstrahlen befinden sich auf einem Kegelmantel mit gleichen Öffnungswinkel wie der Öffnungswinkel der Kegelmantel-Spots der Regentropfen.

Die Abbildung rechts (untere Strahlen) enthält für den Hauptregenbogen zwei Regentropfen mit einmaliger innerer Reflexion. Vom Licht aus dem oberen roten Kegelmantel-Spot und von dem auf dem unteren blauen Kegelmantel-Spot erreicht nur je ein Strahl den Beobachter. Die Wassertropfen sind übertrieben groß gezeichnet. In Realität wird jede Lichtfarbe aus vielen übereinander liegenden Tropfen und von nahezu unendlich vielen auf einem kreisförmigen Band liegenden Tropfen gesehen.

Die Vorstellung, dass gemäß Abbildung links oben eigentlich Blau die oberste Farbe im Hauptbogen sein müsste, ist irrig – da Blau unter einem kleineren Winkel reflektiert wird, sind die Tropfen, die für einen Beobachter das Blau liefern, dem Zentrum des Regenbogens näher.

Die Auffächerung des Hauptregenbogens durch Dispersion beträgt zwischen Rot und Blau etwa 1,8°. Wegen der räumlichen Ausdehnung der Sonne von etwa 0,5° beträgt die Breite jeder Farbe ebenfalls etwa 0,5°. Diese Unschärfe liegt deutlich unter der Auffächerung, weshalb der Beobachter noch eine relativ reine rote äußere Farbe sieht. Die anderen Farben sind durch Mischung weniger gesättigt beziehungsweise rein. Die Addition der endlichen Sonnenausdehnung und der Auffächerung ergibt die Gesamtbreite des Hauptregenbogens von etwa 2,2°. Bei einer Entfernung des Regenschauers von 1 km sind Regentropfen über eine radiale Strecke von etwa 35 m am Regenbogen beteiligt.

Von der Erdoberfläche aus gesehen kann der Regenbogen im Maximum nur ein Halbkreis sein (Scheinbare Größe von Höhe 42 Grad, Breite 84 Grad). Er tritt bei im Horizont stehender Sonne auf. Der Mittelpunkt des Halbkreises ist die im Gegen-Horizont stehende Gegen-Sonne. Bei höher stehender Sonne wird der Regenbogen kleiner. Da sich jetzt sein Mittelpunkt unter dem Horizont befindet, wird der Scheitel zum Orientierungspunkt. Im Scheitel ist in der Regel der Regenbogen auch am deutlichsten ausgeprägt. Falls an einer bestimmten Stelle keine Regentropfen vorhanden sind, hat der Regenbogen dort eine Lücke.

Wenn die Sonne höher als 42° steht, liegt auch der Scheitelpunkt des Bogens unter dem Horizont und kann so nur mehr von einem erhöhten Beobachtungsort aus gesehen werden, zum Beispiel beim Blick von der Spitze eines Berges oder Turmes auf eine tiefer liegende Regenwand (siehe Bild links).

Um einen zum Kreis geschlossenen Hauptregenbogen sehen zu können, müssen Regentropfen in voller radialer Ausdehnung um den Sonnengegenpunkt vorhanden sein und von der Sonne beschienen werden. Diese Möglichkeit besteht im Allgemeinen nur von einem Flugzeug oder einem Ballon aus (siehe Bild rechts).

Bisher wurden Sonnenstrahlen betrachtet, die einmal im Inneren der Regentropfen reflektiert werden. Der oberhalb des Hauptbogens sichtbare Nebenregenbogen entsteht aus dem kleineren Lichtanteil, der erst nach zwei inneren Reflexionen die Tropfen verlässt. Er ist entsprechend schwächer als der Hauptregenbogen. Eine weitere Schwächung entsteht durch die größere Auffächerung des Lichtstrahls in farbige Teilstrahlen infolge flacheren Ein- und Austritts am Tropfenrand. Der Nebenregenbogen kann daher nur bei günstigen Lichtverhältnissen beobachtet werden (siehe Abbildung links).
Das nach zweimaliger innerer Reflexion austretende Licht ist in einen zum Sonnengegenpunkt gerichteten Kegelmantel-Spot konzentriert. Der doppelte Kegelwinkel ist aber größer als 180° – der Kegelmantel ähnelt einem vom Wind umgestülpten Regenschirm –, so dass Spot-Licht auch rückwärts zum Beobachter fällt (siehe Abbildung links, unten). Die halben Kegelwinkel sind 129° (51° von rückwärts gesehen) für rotes Licht (siehe Abbildung rechts) und 126° (54°) für blaues Licht. Weil als Wechselwinkel die Komplementärwinkel der Kegelmantel-Spot-Öffnungswinkel zu 180° gelten, sieht der Beobachter im Nebenregenbogen die umgekehrte Farbreihenfolge im Vergleich zum Hauptregenbogen. Der Nebenregenbogen ist innen rot und außen blau (siehe Abbildung oben rechts: obere Strahlen).

Im Bild rechts mit einem Haupt- und Nebenregenbogen fällt auf, dass der Himmel im Innern des Hauptbogens deutlich heller als außerhalb erscheint, und dass der Bereich zwischen Haupt- und Nebenregenbogen deutlich dunkler als seine Umgebung ist. Dieser Helligkeitskontrast entsteht, weil sich die Farben im Inneren der Kegelmantel-Spots überlagern und schließlich jenseits des blauen weißes Licht von den Regentropfen zum Beobachter reflektiert wird. Haupt- und Nebenregenbogen sind einander mit ihrer roten Seite zugekehrt. Hier fehlt das zusätzliche weiße Licht. Der Raum zwischen ihnen wird dunkler gesehen. Dieses dunkle Band wird zu Ehren seines Entdeckers Alexander von Aphrodisias als "Alexanders dunkles Band" bezeichnet. Beim Nebenregenbogen enthalten die Kegelmantel-Spots im Inneren auch weniger Licht (vergleiche die Abbildung oben rechts mit der Abbildung weiter oben), so dass die Aufhellung über ihm weniger stark als unter dem Hauptregenbogen ist.

Nebenregenbögen höherer Ordnung, also mit mehr als zwei Reflexionen innerhalb eines Regentropfens, sind wegen der oben beschriebenen Abschwächung mit bloßem Auge nicht mehr erkennbar; sie wurden erstmals von Félix Billet (1808–1882) beschrieben, der auch die zugehörigen Winkelabstände vom Sonnengegenpunkt dafür berechnete. Nachdem deren Existenz jedoch theoretisch begründet wurde, ist in jüngeren Jahren auch der Nachweis mit fotografischen Mitteln gelungen.

Es handelt sich um den tertiären Regenbogen unter einem Winkel von etwa 40° gegen die Sonne und den quartären Regenbogen unter etwa 45°. Diese Bögen entstehen durch Licht, das drei- oder viermal innerhalb der Regentropfen reflektiert wurde.

Mondregenbogen heißt ein Regenbogen bei Nacht, der das Mondlicht als Grundlage hat. Er ist naturgemäß wesentlich seltener als ein Regenbogen und erscheint dem Beobachter aufgrund seiner Lichtschwäche weiß. Zu sehen ist er, weil das menschliche Auge Helligkeitsunterschiede viel empfindlicher wahrnimmt als Farben (siehe Nachtsehen). Bei klarer Luft und ausgeprägtem Vollmond können die Regenbogenfarben sichtbar werden. Außerdem sieht man sie prinzipbedingt immer bei farbfotografischen Aufnahmen, wenn das Verfahren lichtempfindlich genug ist, so dass die Abbildung des Mondregenbogens gelingt.

In Tropfen mit Durchmesser kleiner als 50 Mikrometer ist die Zerlegung des Sonnenlichtes in ihre farbigen Bestandteile zu klein. Nebel enthält entsprechend kleine Wassertropfen, weshalb dieser weiß erscheinende Regenbogen Nebelbogen genannt wird.

Beim Taubogen findet die Lichtbrechung an Tautropfen statt, beispielsweise dem Tau auf einer Wiese oder an Spinnweben, selten dem Tau an kleinen auf einem See schwimmenden Partikeln. Der Taubogen erscheint aber dem Beobachter nicht als Kreis, sondern elliptisch oder hyperbelförmig, je nach Sonnenstand und Neigung der Ebene in der sich die Tautropfen befinden. Der Effekt ergibt sich dadurch, dass sich der 42-Grad-Kegel des zurückgeworfenen Lichts an der Oberfläche des Bodens in einer Hyperbel oder Ellipse schneidet. Durch den schräg verlaufenden Kegelschnitt ergibt sich die Vorstellung, die Lichterscheinung erstrecke sich in horizontaler Ebene, was nur scheinbar richtig ist. Tatsächlich ist der Bogen im Auge des Betrachters immer in einem 42-Grad-Winkel vom Sonnengegenpunkt entfernt.
Wenn das Sonnenlicht an einer Wasserfläche gespiegelt wird, bevor es auf die Regentropfen trifft, kann ein zweiter Bogen entstehen, der am Horizont mit dem Hauptbogen zusammentrifft, weiter oben aber wie ein zweiter, den Hauptbogen kreuzender Bogen erscheint. Darüber hinaus gibt es Beobachtungen von seitlich versetzten, sich überschneidenden Regenbögen, deren Entstehung bislang unklar ist.

Die sehr seltenen gespaltenen Regenbögen oder Zwillingsregenbögen unterscheiden sich von doppelten Regenbögen aus Haupt- und Nebenregenbogen dadurch, dass sie einen gemeinsamen Ursprung haben, sich dann aber (zumindest teilweise) in zwei Regenbögen aufspalten, und dass die Farbabfolge der beiden Bögen sich nicht umkehrt. Im Gegensatz zum doppelten Regenbogen, der ein zusammenhängendes Phänomen darstellt, das durch unterschiedliche Brechung innerhalb derselben Menge an Wassertropfen entsteht, handelt es sich bei einem Zwillingsregenbogen tatsächlich um zwei „unabhängige“ Regenbögen, die zur selben Zeit an unterschiedlichen Mengen von Wassertropfen entstehen. In besonders seltenen Fällen kann jeder der Zwillingsbögen auch selbst wieder einen Nebenregenbogen zeigen.
Zwillingsregenbögen können entstehen, wenn unterschiedlich große Wassertropfen gleichzeitig vom Himmel fallen, etwa, wenn zwei Regenschauer sich vereinen. Die Wassertropfen flachen durch den Luftwiderstand umso mehr ab, je größer sie sind und brechen somit das Licht in leicht unterschiedliche Richtungen, was dazu führt, dass auch die sich je nach Wassertropfenform ergebenden Regenbögen leicht deformiert erscheinen und somit einen Zwillingsregenbogen bilden können.

Beim Austritt des Lichtes aus den Tropfen fallen nicht nur Strahlen unterschiedlicher Farbe zusammen, wobei durch additive Mischung die Farbreinheit des Regenbogens geschwächt wird. Es fallen auch Strahlen derselben Wellenlänge zusammen, die durch unterschiedlich lange Laufwege im Tropfen gegenseitig phasenverschoben sind. Bei ihrer Überlagerung findet Interferenz statt, sie löschen sich gegenseitig aus oder verstärken sich. Die für Interferenzerscheinungen typischen Muster begleiten vor allem den Hauptregenbogen an dessen blauer Seite als helle gegenüber dunklen abgesetzte Streifen, die als Interferenz- oder "überzählige Bögen" bezeichnet werden.

Der Unterschied zwischen den Laufwegen ist eine Funktion der Tropfengröße. Überzählige Regenbögen treten erst bei Regentropfen in Erscheinung, deren Durchmesser kleiner als einen halben Millimeter ist.

Die Tropfengröße und die Tropfenform haben generell Einfluss auf die farbliche Erscheinung des Regenbogens.

Häufig sind die Enden des Bogens besonders hell. Dieser Effekt wird ebenfalls durch Interferenz verursacht, die außer von der Tropfengröße auch von Abweichungen von der Kugelform abhängt. Generell lässt sich feststellen, dass große Tropfen mit Durchmessern von mehreren Millimetern besonders helle Regenbögen mit wohldefinierten Farben erzeugen. Bei Größen von weniger als 1,5 mm wird zunächst die Rotfärbung immer schwächer. Sehr kleine Tropfen, wie beispielsweise in Nebelschwaden, in denen der Durchmesser oft nur etwa ein Hundertstel Millimeter beträgt, liefern nur noch verwaschene Farben.

Das von einem Regenbogen reflektierte Licht hat einen sehr hohen Polarisationsgrad. Mit Hilfe eines Polarisationsfilters kann ein Regenbogen, je nach Drehwinkel des Filters vor dem Beobachterauge oder der Kamera, entweder weitgehend gelöscht oder im Kontrast gesteigert werden.

Der optische Effekt der Dispersion des Sonnenlichts lässt sich auch bei anderen optischen Phänomenen als den Regenbogen beobachten. Bekannt sind vor allem die Haloerscheinungen.

Einige Erscheinungen sind anders als beim Regenbogen durch Beugung des Sonnenlichtes verursacht.

Natürliche Regenbögen entstehen meist dann, wenn nach einem Regenschauer der Himmel schnell aufklart und die tief stehende Sonne das abziehende Niederschlagsgebiet beleuchtet. In gemäßigten Klimazonen mit einer westlichen Vorzugswindrichtung wie in Mitteleuropa sind diese Bedingungen häufig am späten Nachmittag im Anschluss an ein Wärmegewitter erfüllt. Zu diesen kommt es meist bei Kaltfrontaufzügen, wobei am Vormittag im Mittel weniger Regen fällt als am Nachmittag, was dann auch zur höheren Wahrscheinlichkeit führt, auf einen Regenbogen zu treffen.

Im Sommer ist um die Mittagszeit herum kein Regenbogen zu beobachten, da die Sonne hierfür zu hoch steht. Im Winter besteht aber auch zu diesem Zeitpunkt die Möglichkeit einen flachen Regenbogen zu erkennen. Unabhängig davon kann ein Regenbogen recht häufig in einem Sprühnebel beobachtet werden, vor allem bei Springbrunnen, Sprinklern und Wasserfällen. Da solche Regenbögen nicht auf ein Niederschlagsereignis angewiesen sind, beobachtet man sie viel einfacher und häufiger. Bei entsprechendem Sonnenstand ist die Beobachtung von Regenbogenfragmenten auch in der Gischt von größeren Wellen möglich.

Bei Wetter ohne bewölkten Himmel mit strahlendem Sonnenlicht kann der „Regenbogen“ so selbst erzeugt werden. Ein solcher künstlicher Regenbogen beruht auf den gleichen beschriebenen physikalischen Prinzipien. Der einzige Unterschied mag die Größe der Reflexionsfläche sein. Um den Scheitelpunkt des Regenbogens zu finden, muss man dabei seinen Blick in Richtung des eigenen Schattens richten.

Der Regenbogen wird von beiden Augen des Beobachters stets unter demselben Beobachtungswinkel (dem "Regenbogenwinkel") gesehen. Vom stereoskopischen (räumlichen) Sehen wird er deshalb als ein Objekt in "unendlicher Entfernung" interpretiert. Diese Täuschung wirkt insbesondere dann irritierend, wenn sich „hinter“ einem „nahen“ Regenbogen (beispielsweise im Sprühnebel eines Gartenschlauches) noch Objekte im Gesichtsfeld befinden, deren Entfernung aufgrund des stereoskopischen Sehens als "kleiner als unendlich" eingeschätzt werden können. Ebenso irritierend wirkt die Tatsache, dass sich der Regenbogen mit dem Beobachter mitbewegt: Man kann deshalb bekanntlich nie zum "Ende des Regenbogens" gelangen.

Der Regenbogen beflügelt nicht nur die Fantasie des Menschen, verschiedene Erklärungsversuche haben auch den Erkenntnisprozess in der Physik und dort speziell in der Optik wesentlich vorangetrieben.

Die oben angeführte physikalische Erklärung des Regenbogens, beruht in ihrem grundlegenden strahlenoptischen Teil auf 1637 veröffentlichte Arbeiten von René Descartes. Sie sind unter der Überschrift "DE L’ARC-EN-CIEL" im Anhang "Les Météores" seiner philosophischen Schrift "Discours de la méthode" beschrieben. Er griff darin die bereits um 1300 von Dietrich von Freiberg in seinem Werk "De iride et de radialibus impressionibus" entwickelte Idee auf, wonach ein Regenbogen durch die Brechung und Reflexion von Sonnenstrahlen innerhalb einzelner Tröpfchen erklärt werden kann. Seine „mysteriöse“ Erklärung der Regenbogenfarben war unzutreffend. Er wendete das kurz vorher von Willebrord Snell entdeckte Brechungsgesetz an, ohne die Dispersion (die wellenlängen-abhängige Brechung des Lichts) zu kennen.

Aus dem Jahre 1700 stammt eine den Regenbogen betreffende Arbeit von Edmond Halley, und 1704 brachte Isaac Newtons Theorie des Lichtes die Dispersion ins Spiel und machte so die Farbenpracht verständlich.

War es zu Newtons Zeiten noch Thema kontroverser Diskussionen, ob Licht nun korpuskularen oder wellenartigen Charakter besitze, so war auch hier der Regenbogen ein wichtiger Ideengeber.
Das Rätsel der überzähligen Bögen veranlasste 1801 Thomas Young zur Durchführung seines berühmten Doppelspaltexperimentes. Er wies damit die Wellennatur des Lichtes nach und konnte anschließend das Rätsel durch die Betrachtung von Interferenzerscheinungen lösen (1804).

Youngs Theorie wurde 1849 von George Biddell Airy weiter verfeinert. Er erklärte die Abhängigkeit des exakten Farbverlaufs von der Tröpfchengröße. Die eigens entwickelten mathematischen Verfahren spielen im Rahmen der WKB-Näherung noch heute eine wichtige Rolle für die moderne Quantenmechanik.

Moderne physikalische Beschreibungen des Regenbogens und ähnlich gearteter Probleme basieren im Wesentlichen auf der von Gustav Mie 1908 entwickelten und als Mie-Streuung nach ihm benannten Theorie.

Der "Regenbogenwinkel" hängt – wie oben beschrieben – bei kugeligen Flüssigkeitströpfchen nicht von der Tropfengröße ab, sondern lediglich vom Brechungsindex. Diese wiederum ist bei einer bestimmten Wellenlänge eine temperaturabhängige Materialkonstante der tropfenbildenden Flüssigkeit. Deshalb kann durch Messung des Regenbogenwinkels, unter dem monochromatische Laserstrahlung von einem Nebel reflektiert wird, die Temperaturverteilung innerhalb des Nebels berührungslos bestimmt werden, falls – wie in technischen Anlagen meist der Fall – bekannt ist, welche Flüssigkeit den Nebel bildet.

Als ein nicht alltägliches und beeindruckendes Naturschauspiel haben Regenbögen ihre Spuren in der Kulturgeschichte der Menschheit hinterlassen und sind zudem ein in unzähligen Kunstwerken dargestelltes Bildmotiv. Da der Regenbogen weltweit bekannt und mit zahlreichen positiven Attributen versehen ist, hat er auch immer wieder Einzug in die Symbolik gehalten.

Der Regenbogen ist von jeher ein wichtiges Element zahlreicher Mythologien und Religionen über alle Kulturen und Kontinente hinweg. Die Mythen sprechen ihm dabei oft die Rolle eines Mittlers oder einer Brücke zwischen Götter- und Menschenwelt zu. Mythologien ohne Regenbogen sind selten. Der Regenbogen als Mythos findet sich auch in den Erzählungen relativ isolierter Kulturen; daraus lässt sich schließen, dass dieser Mythos auf der Erde an verschiedenen Orten und zu verschiedenen Zeiten eigenständig erdacht und überliefert worden ist. Es geht nicht allein auf den Verkehr und den Austausch unter den großen Kulturen der Menschheit zurück, wenn der Regenbogen-Mythos heute überall auf der Erde aufgefunden werden kann.

Die australischen Ureinwohner, die Aborigines, verehren in ihrer Schöpfungsgeschichte eine Regenbogenschlange als den Schöpfer der Welt und aller Lebewesen. Die griechische Mythologie sah ihn als Verbindungsweg, auf dem die Göttin Iris zwischen Himmel und Erde reist. Nach der irischen Mythologie hat der Leprechaun seinen Goldschatz am Ende des Regenbogens vergraben. In der germanischen Mythologie war er die Brücke Bifröst, die Midgard, die Welt der Menschen, und Asgard, den Sitz der Götter, miteinander verband. Während des Ragnarök, des Weltuntergangs der nordischen Mythologie, wird der Regenbogen zerstört. Regenbogen tauchen auch in der Schöpfungsgeschichte der Diné auf. Bei den Inka vertrat der Regenbogen die Erhabenheit der Sonne.


Im jüdischen Tanach (Altes Testament der christlichen Bibel), 1. Buch Mose 9, ist der Regenbogen ein Zeichen des Bundes, den Gott mit Noach und den Menschen schloss. Laut biblischer Erzählung versprach Gott nach dem Ende der Sintflut: „Ich will hinfort nicht mehr die Erde verfluchen um der Menschen willen, denn das Dichten und Trachten des menschlichen Herzens ist böse von Jugend auf.“ Der Regenbogen als Zeichen des Friedens zwischen Mensch und Gott nimmt damit eine altorientalische Tradition auf, nach der das Phänomen als abgesenkter, also nicht schussbereiter Bogen Gottes interpretiert wurde. Aufgrund dieser Stelle ist der Regenbogen im Judentum bis heute ein wichtiges religiöses Symbol.
Im Christentum wird ein anderer Traditionsstrang wichtig. In Ezechiel 1 sieht der Prophet einen gewaltigen Thronwagen. Oben auf dem Thron ist ein heller Schein 
Im griechisch verfassten Neuen Testament kommt der Regenbogen nur ein einziges Mal vor. In der Offenbarung des Johannes erscheint ein Engel mit einem Buch vom Himmel herab, er ist in eine Wolke gehüllt und über seinem Kopf ist ein Regenbogen. Dieses Bild basiert auf Ezechiel 1,28. Das griechische Wort für diese Erscheinung heißt „iris“, und hier wird deutlich, dass die antike Vorstellung des Kriegsbogens vergessen ist. Wichtig an der Erscheinung ist die schillernde Farbenpracht, die Himmel und Erde verbindet. Das griechische Wort bezeichnet neben dem Regenbogen auch ganz allgemein einen farbigen Ring (oder Halbring). In steht in vielen deutschen Übersetzungen zwar Regenbogen, aber hier heißt es ausdrücklich, dass es sich um einen grünlich schimmernden Lichtkranz handelt – also einen Heiligenschein, der Gottes Gegenwart anzeigt. In der folgenden christlichen Tradition lebt das Symbol auf Ikonen und in der mittelalterlichen Malerei und Bildhauerei. Auf Altären und auf den Darstellungen des Jüngsten Gerichts über dem Eingangsportal einer Kirche wird Christus manchmal als der auf (oder in) einem Regenbogen sitzende Richter dargestellt werden – eine freie Aufnahme der Stellen in der Offenbarung vermischt mit Ezechiel. Der Regenbogen symbolisiert hier die Göttlichkeit Christi. Seit dem 12. Jahrhundert wird auch Maria in einem Regenbogen oder auf einem Regenbogen sitzend dargestellt und dadurch ihre Heiligkeit zum Ausdruck gebracht.

Der Regenbogen als Bildmotiv findet sich früher oder später bei nahezu allen Landschaftsmalern, stellt aber auch ein begehrtes Ziel vieler Naturfotografen dar. Zu nennen sind hier beispielsweise Caspar David Friedrich, Joseph Anton Koch oder Peter Paul Rubens. Dabei ist der Regenbogen auch ein beliebtes Laienmotiv und in künstlerischen Lehreinrichtungen aller Altersstufen zu finden.

Auch in der Musik finden sich viele Motive rund um den Regenbogen. So besingt Judy Garland 1939 in Das zauberhafte Land, einer Verfilmung des Zauberers von Oz, eine Gegend „irgendwo über dem Regenbogen“ "(somewhere Over the Rainbow)," wo „Träume wahr werden“. Dieses Lied von Harold Arlen und E. Y. Harburg wurde 1994 als Coverversion von Marusha zu einer Techno-Hymne. Zum gleichen Genre zählt auch "Rainbow To The Stars" von Dune.

Im Bereich des Metal ist der Song "At The End Of The Rainbow" der schwedischen Band Hammerfall zu nennen, wo man am „Ende des Regenbogens mit Gold in den Händen“ stehen will (auf ihrem 1998 erschienenen Studioalbum Legacy of Kings). Und die deutsche Band Axxis singt "Touch the Rainbow" (auf ihrem 1990 erschienenen Studio-Album Axxis II).

Die deutsche Band Scorpions nannte ihr zweites Studioalbum von 1974 "Fly to the Rainbow," worauf sich am Ende das gleichnamige Stück befindet.

Rainbow war eine Hardrockband, die 1975 vom Gitarristen Ritchie Blackmore gegründet wurde. Auf dem Debüt von Rainbow war der Song "Catch the Rainbow" zu finden.

Die Rolling Stones schilderten 1967 in ihrem Song „She’s A Rainbow“ diverse Drogenerfahrungen und bedienten sich dabei der Farbenpracht des Regenbogens als Metapher für die Weiblichkeit.

Bezugnehmend auf den sprichwörtlichen Topf mit Gold am Ende des Regenbogens sang die Gruppe ABC um Martin Fry 1982 in dem Titel "„All Of My Heart“:" „No I won’t be told there’s a crock of gold at the end of the rainbow.“

Der französische Komponist Olivier Messiaen, ein Synästhetiker, komponierte in seinem 1944 entstandenen „Quartett auf das Ende der Zeit“ (Quatuor pour la fin du temps) einen Satz mit dem Titel „Tanz der Regenbogen für den Engel, der das Ende der Zeit ankündigt“ (Fouillis d’arc-en-ciel, pour l’Angel qui annonce la fin du temps).


Die Regenbogenfahne ist ein in der Geschichte vielfach und in verschiedenem Sinne verwendetes Symbol:





</doc>
<doc id="13689" url="https://de.wikipedia.org/wiki?curid=13689" title="Beugung (Physik)">
Beugung (Physik)

Die Beugung oder Diffraktion ist die Ablenkung von Wellen an einem Hindernis. Durch Beugung kann sich eine Welle in Raumbereiche ausbreiten, die auf geradem Weg durch das Hindernis versperrt wären. Jede Art von physikalischen Wellen kann Beugung zeigen. Besonders deutlich erkennbar ist sie bei Wasserwellen oder bei Schall. Bei Licht ist die Beugung ein Faktor, der das Auflösungsvermögen von Kamera-Objektiven und Teleskopen begrenzt. Manche technische Komponenten, wie Beugungsgitter, nutzen die Beugung gezielt aus.

Zur Beugung kommt es durch Entstehung neuer Wellen entlang einer Wellenfront gemäß dem huygens-fresnelschen Prinzip. Diese können durch Überlagerung zu Interferenzerscheinungen führen.

Im Gegensatz zur Beugung findet bei der Streuung eine Ablenkung von Strahlung durch Interaktion von Teilchen statt. Bei gleichgerichteter, kohärenter Streuung spricht man auch von Reflexion.
Bei der Brechung beruht die Ablenkung einer Strahlung auf der Änderung der Ausbreitungsgeschwindigkeit bei Änderung der Dichte oder der Zusammensetzung des Ausbreitungsmediums, am deutlichsten beim Durchtritt durch eine Phasengrenze. 

Christiaan Huygens bemerkte bereits um 1650, dass mit einer Lichtausbreitung in Wellenform bestimmte bis dahin unerklärliche Phänomene beschrieben werden können. Er formulierte das Huygenssche Prinzip und begründete damit die Wellenoptik. Der Effekt der Beugung von Licht an einem optischen Spalt wurde schließlich 1662 von Francesco Maria Grimaldi beobachtet, der das Licht in seinem Werk "De lumine" als Welle beschrieb. 1802 führte Thomas Young entsprechende Experimente am Doppelspalt durch. Eine vollständige physikalische Beschreibung der Beugung konnte 1818 durch Augustin Jean Fresnel erbracht werden, die von Siméon Denis Poisson zunächst in Zweifel gezogen wurde, kurz darauf jedoch von François Arago durch den experimentellen Nachweis der von Poisson selbst theoretisch vorhergesagten Poisson-Flecken bei der Beugung an einer Kugel bestätigt werden konnte.

1835 untersuchte Friedrich Magnus Schwerd Beugungserscheinungen an regelmäßigen Gittern, die mit Hilfe der Wellenoptik ebenfalls beschrieben werden konnten. 1909 konnte Geoffrey Ingram Taylor im Taylor-Experiment zeigen, dass auch Licht mit äußerst geringer Intensität, also auch einzelne Photonen gebeugt werden, womit der Welle-Teilchen-Dualismus nachgewiesen werden konnte.

1924 entwickelte Louis-Victor de Broglie die Theorie der Materiewellen, und bereits drei Jahre später konnten Clinton Joseph Davisson und George Paget Thomson durch Versuche zur Elektronenbeugung zeigen, dass auch Elementarteilchen mit Ruhemasse gebeugt werden. Weitere drei Jahre später konnten Otto Stern, Otto Robert Frisch und Immanuel Estermann diesen Effekt auch bei der Beugung von Strahlen aus Heliumatomen und Wasserstoffmolekülen an einem Lithiumfluoridkristall demonstrieren. Claus Jönsson führte 1961 schließlich auch Experimente zur Beugung von Elektronen an Einzel- und Doppelspalten aus.

Wegen der Wellennatur des Lichtes weicht sein reales Verhalten teilweise stark von jenem ab, was die geometrische Optik erwarten ließe. So ist bei der Fotografie beugungsbedingt die Auflösung eines Fotos durch den Durchmesser (Apertur) der Linse begrenzt.

Das physikalische Modell für Beugung ist das huygens-fresnelsche Prinzip. Zur Berechnung von "Beugungsbildern" wird das kirchhoffsche Beugungsintegral verwendet, dessen zwei Grenzfälle die Fresnel-Beugung (divergierende Punktstrahlungsquelle) und die Fraunhofer-Beugung sind (parallele Lichtstrahlen als Strahlungsquelle). Die Überlagerung der Elementarwellen kann zu gegenseitiger Verstärkung (konstruktive Interferenz) oder gegenseitiger Abschwächung (destruktive Interferenz) oder gar Auslöschung führen, siehe auch bei Gangunterschied.
Beugung kann unter anderem gut beobachtet werden, wenn geometrische Strukturen eine Rolle spielen, deren Größe mit der Wellenlänge der verwendeten Wellen vergleichbar ist. Optische Blenden werden je nach Anwendung so dimensioniert, dass sie Beugungseffekte bewirken – also bei Abmessungen im Bereich und unterhalb der Lichtwellenlänge, oder mit hinreichender Genauigkeit keine – dann mit Abmessungen deutlich über der Lichtwellenlänge.

Beugung am Einfachspalt: Teilt man in Gedanken ein Lichtbündel, das an einem Einfachspalt in eine bestimmte Richtung abgelenkt wird, in zwei Hälften, können sich diese beiden Anteile des Lichtbündels konstruktiv oder destruktiv überlagern. An einem Spalt ergibt sich so wieder eine Reihe von Beugungsmaxima.

An Blenden anderer Form ergeben sich teilweise stark abweichende Beugungsmuster.

Gitter sind Blenden mit periodischen Spalten. Die Beugung am Gitter ist damit ein wichtiger Spezialfall der Beugung an Blenden.


Prinzipiell gelten Gesetzmäßigkeiten, die für die Beugung von Lichtwellen gelten, auch für andere Wellenerscheinungen.




</doc>
<doc id="13690" url="https://de.wikipedia.org/wiki?curid=13690" title="Multitasking">
Multitasking

Der Begriff Multitasking [] (engl.) bzw. Mehrprozessbetrieb bezeichnet die Fähigkeit eines Betriebssystems, mehrere Aufgaben (Tasks) (quasi-)nebenläufig auszuführen. Im Allgemeinen bietet der Prozessor hierzu auch unterstützende Hardware-Strukturen. Die verschiedenen Prozesse werden in so kurzen Abständen immer abwechselnd aktiviert, dass der Eindruck der Gleichzeitigkeit entsteht. Multitasking ist somit ein Synonym für "Zeit-Multiplexverfahren". Besitzt ein Computer mehrere CPU-Kerne, so dass er mehrere Aufgaben "echt-gleichzeitig" ausführen kann, so spricht man von Multiprocessing. In modernen Computern werden beide Verfahren kombiniert eingesetzt.

Multitasking kann bei verschiedenen Anforderungen nützlich sein, insbesondere bei der Optimierung der Auslastung und für eine je nach Zielsetzung ausgeglichene oder prioritätsbasierte Ressourcenverteilung.

Der Grundgedanke hinter der „Optimierung der Auslastung“ ist der, dass in einem durchschnittlichen Rechner der überwiegende Teil der Rechenzeit nicht genutzt werden kann, weil häufig auf verhältnismäßig langsame, externe Ereignisse gewartet werden muss (beispielsweise auf den nächsten Tastendruck des Benutzers). Würde nur ein Prozess laufen (zum Beispiel die wartende Textverarbeitung), so ginge diese Wartezeit komplett ungenutzt verloren (siehe „aktives Warten“). Durch Multitasking kann jedoch die Wartezeit eines Prozesses von anderen Prozessen genutzt werden.

Ist ein Rechner bzw. seine Rechenzeit demgegenüber größtenteils ausgelastet, beispielsweise durch einzelne rechenintensive Prozesse, so können dennoch mehrere Benutzer oder Prozesse anteilige Rechenzeit erhalten, anstatt auf das Ende eines anderen Prozesses warten zu müssen.
Dies kommt insbesondere auch der Interaktivität zugute.

Da das System zugleich für die verschiedenen Prozesse Prioritäten berücksichtigen kann, ist eine entsprechende Gewichtung möglich, je nach Zielsetzung. Ein Server kann zum Beispiel die Dienste bevorzugen, welche er anbieten soll, jedoch direkte Benutzer-Interaktionen niedrig priorisieren. Ein Desktop-PC wird umgekehrt vor allem die Ein- und Ausgaben von/an den Benutzer bevorzugen, und dafür Hintergrund-Prozesse etwas zurückstellen.

Vorläufer des Multitasking ist die Multiprogrammierung mit dem Ziel einer höheren CPU-Auslastung im Gegensatz zur sequenziellen Ausführung der Aufgaben bei Stapelverarbeitung. Bei der Multiprogrammierung findet der Kontextwechsel der Programme mit dem Zugriff auf periphere Geräte statt, da dabei zwangsläufig Wartezeit entsteht. Erste Ansätze basieren auf dem Konzept von Christopher Strachey aus dem Jahr 1959. Praktisch umsetzen ließen sich solche Konzepte aber erst mit leistungsfähiger Hardware, als mit der Interruptsteuerung die Entwicklung von TSR-Programmen möglich wurde.

Der technische Ablauf beim Multitasking ist im Prinzip immer gleich.

Als wichtige Grundvoraussetzung des Multitaskings gilt im Allgemeinen, dass ein Prozess, der zugunsten eines anderen unterbrochen wird, nichts über diesen anderen (oder ggf. auch mehrere andere) „wissen“ muss. Dies wird meist erreicht, indem jeder Prozess einen eigenen sogenannten "Prozesskontext" besitzt, der seinen Zustand beschreibt. Ein Prozess ändert immer nur seinen eigenen Prozesskontext, niemals den eines anderen Prozesses.

In der Regel wird der gesamte Prozesskontext (der Zustand des Prozesses) beim Unterbrechen gespeichert, z. B. auf dem Stapelspeicher (engl. Stack). Er bleibt so lange gespeichert, bis der betreffende Prozess wieder Rechenzeit erhalten soll. Unmittelbar bevor dieser Prozess wieder aktiv wird, wird der gespeicherte Zustand wieder geladen, sodass es für den Prozess so erscheint, als sei er überhaupt nicht unterbrochen worden; unabhängig davon, ob, wie viele und was für Prozesse in der Zwischenzeit ausgeführt worden sind. Dieses Umschalten zwischen einzelnen Prozessen wird mit Taskwechsel bezeichnet.

So kann ein Prozess bei der weiteren Ausführung nach der Unterbrechung wieder eine definierte Umgebung vorfinden, auch wenn zwischenzeitlich andere Prozesse ausgeführt wurden.

Beim kooperativen Multitasking ähnelt der Taskwechsel stark dem Aufruf von Prozeduren bzw. Funktionen in der prozeduralen Programmierung.

Beim „kooperativen Multitasking“ wird das Multitasking durch eine zentrale Prozessverwaltung im Systemkernel realisiert: ein einfacher, sogenannter Scheduler. Der Scheduler sichert den Prozesskontext des gerade unterbrochenen Tasks, wählt den nächsten Prozess aus, der Rechenzeit erhalten soll, stellt dessen Prozesskontext her und gibt den Prozessor dann an diesen neuen Prozess ab. Der Scheduler kann Listen mit verschieden priorisierten Tasks führen, und niedrig priorisierte entsprechend selten aufrufen. Dabei kann auch die bereits verbrauchte Rechenzeit eines Tasks berücksichtigt werden. In der Regel werden Betriebssystem-interne Aufgaben zuerst erledigt, bevor ein neuer Task den Prozessor erhält. Es ist jedem Prozess selbst überlassen, wann er die Kontrolle an den Kern zurückgibt; in der Regel wird zumindest jede Dienst-Anforderung an das Betriebssystem mit einem Taskwechsel verbunden.

Vorteil dieser Methode ist, dass viele Systemfunktionen (z. B. die Ausgabe) nicht wiedereintrittsfähig sein müssen und daher nicht synchronisiert sein müssen, was eine erhebliche Vereinfachung für den Hersteller bedeutet. (Unterbrechungsroutinen müssen jedoch stets dieses Problem lösen.) Diese Form des Multitasking hat wie auch das TSR-Konzept den Nachteil, dass Programme, die nicht kooperieren, das restliche System zum Stillstand bringen. Gründe für solches Programmverhalten können sein:

Das Konzept wurde zum Beispiel eingesetzt

Diese Form des Multitasking ist prinzipiell schnell und ressourcenschonend, sowie technisch verhältnismäßig einfach realisierbar. Für multiuserfähige Großrechner war es nie eine praktikable Alternative und wurde z. B. unter Unix nie eingesetzt, da ja ein Benutzer mittels unkooperativem Programm alle anderen blockieren könnte. Auch in den neueren Windows-Betriebssystemen der NT-Linie und in Mac OS X wird diese inzwischen als veraltet geltende Technik nicht eingesetzt. Teilweise unterstützen sie ähnliche Funktionalität begrenzt zum Beispiel als User Mode Threads, jedoch in jedem Fall nur eingebettet in präemptivem Multitasking.

Basis der heutzutage standardmäßig angewendeten Methode ist das "präemptive Multitasking". Die Abarbeitung der einzelnen Prozesse wird ebenfalls gesteuert durch den Scheduler, einen Bestandteil des Betriebssystemkerns (siehe unten). Jeder Prozess wird nach einer bestimmten Abarbeitungszeit unterbrochen. Dabei spricht man auch von so genannten Zeitschlitzen (bzw. "Zeitscheiben", engl. "time slices"). Dann „schläft“ der Prozess (ist inaktiv) und andere Prozesse werden bearbeitet. Erhält er wieder eine Prozessorzuteilung, so setzt er seine Arbeit fort (ist aktiv). Meist wird jedem Prozess eine „absolute“ Zeitscheibe zugewiesen (alle Zeitscheiben haben die gleiche, feste Dauer); alternativ wird ihm pro definierter Zeiteinheit ein bestimmter Prozentteil dieser Zeiteinheit zugewiesen (z. B. abhängig von seiner Priorität), den er höchstens nutzen kann (die Länge der Zeitscheibe wird also jedes Mal neu bestimmt). Endet seine Zeitscheibe („seine Prozessorzuteilung ist zu Ende“), dann unterbricht ihn ein Hardware-Timer, er wird wieder „schlafen gelegt“ und das Betriebssystem erlangt wieder Kontrolle. Sollte er bereits vor Ablauf seiner Zeitscheibe eine Funktion des Betriebssystems benötigen, so wird er sogleich angehalten und als „nicht rechenbereit“ markiert, bis das Betriebssystem den gewünschten Dienst erbracht hat. Nur als „rechenbereit“ markierte Prozesse erhalten Prozessorzeit-Zuteilungen.

Eine beliebte Umsetzung des präemptiven Multitaskings ist die Verwendung einer Vorrangwarteschlange in Verbindung mit der Round-Robin-Scheduling-Strategie. Es gibt auch die Prozessorzuteilung abhängig von der Taskpriorität, vor allem bei Echtzeitsystemen z. B. MicroC/OS-II. Für das Multitasking spielt das nur eine untergeordnete Rolle, da präemptives Multitasking die Kernel- bzw. Prozessorkontrolle über die Prozesse beschreibt.
Hardwareseitig benötigt präemptives Multitasking im Gegensatz zur kooperativen Variante (vergl. TSR-Programm als Vorläufer) zwingend einen Interrupterzeuger (meist ein Zeitgeber) im geeigneten Prozessor, da das System softwareseitig keine Möglichkeit hat, Prozessen die Kontrolle über den Prozessor zu entziehen. Der Zeitgeber sendet regelmäßig oder nach Ablauf einer eingestellten Zeit ein Signal (Interrupt) an die CPU, was sie zur Unterbrechung des aktuell laufenden Tasks und zur Ausführung des Schedulers veranlasst. Dieser speichert den gerade unterbrochenen Prozesskontext, übernimmt nötige Verwaltungsaufgaben und aktiviert dann wieder einen (anderen) Prozess.

Moderne Betriebssysteme arbeiten darüber hinaus mit einem Speicherschutz, der verhindert, dass verschiedene Prozesse sich im Speicher gegenseitig beeinflussen oder gar überschreiben. Diese Schutzfunktion übernimmt im PC die Memory Management Unit (MMU), welche die Virtualisierung des Hauptspeichers und verschiedene Berechtigungslevel (Ringe) oder auch Modi (Kernel-Mode versus User-Mode) ermöglicht und so dem Betriebssystem erlaubt, verschiedene parallele Prozesse innerhalb des Rechners voneinander strikt abzukapseln. Im PC kam die MMU erstmals in Rechnern mit i286-Prozessoren von Intel zum Einsatz. Diese Technik ist aber für Multitasking im engeren Sinne nicht zwingend notwendig.

Die ersten weit verbreiteten Computersysteme, die präemptives Multitasking beherrschten, waren der Sinclair QL (1984) und der Commodore Amiga (1985) im Heimbereich (beim Amiga ohne Speicherschutz/Privilegierung und somit „aushebelbar“), sowie zuvor die unter Unix betriebenen Großrechenanlagen. Windows beherrscht erstmals in den 3.x-Versionen teilweise präemptives Multitasking, dort allerdings nur für DOS-Programme und das auch nur dann, wenn sie auf einem System mit einem i386-kompatiblen Prozessor ausgeführt werden, da dieser in solchen Fällen hardwareseitige Virtualisierung ermöglicht. Moderne Betriebssysteme, die präemptives Multitasking vollständig unterstützen, sind Windows NT (und alle Nachfolger), QNX, BeOS und alle auf Unix basierenden Systeme wie Linux, HP-UX, Solaris, Mac OS X u.v.m.

Außerdem muss man zwischen Time slicing (Zeitscheiben-Verfahren) und Time-Sharing unterscheiden, letzteres gestattet mehreren Benutzern bzw. deren Prozessen (z. B. auf Datenbankservern oder Großrechnern mit Terminalzugriff) sich automatisch anteilig die verfügbare Rechenzeit zu teilen.

Eine Sonderform des präemptiven Multitasking ist das weniger bekannte Präemptible Multitasking (englische Schreibweise Preemptible Multitasking), das erstmals im Betriebssystem OS/2 implementiert wurde. Viele Betriebssystem-eigene Kernel-Routinen werden als Scheduler-Threads geführt; somit können Anwendungsprozesse auch Zeitschlitze erhalten, während eigentlich eine Betriebssystem-Aktion ausgeführt wird (mit Ausnahmen für atomare OS-Prozesse). Das Konzept ermöglicht schnellere Reaktionszeiten. Mit Version 2.6 hat es auch in den Linux-Kernel Eingang gefunden.




</doc>
<doc id="13691" url="https://de.wikipedia.org/wiki?curid=13691" title="Softwareseitiges Multithreading">
Softwareseitiges Multithreading

Softwareseitiges Multithreading (auch nur "Multithreading", oder selten auch: "Mehrfädigkeit") wird die Aufteilung eines Programmes in mehrere gleichzeitig abzuarbeitende Bearbeitungsstränge (Threads) genannt. Im Allgemeinen verwenden die Threads eines Programmes gemeinsame Ressourcen wie Adressraum, Datei-Handles etc.

Softwareseitiges Multithreading unterscheidet sich von hardwareseitigem Multithreading, welches eine scheinbare oder echte gleichzeitige Abarbeiten mehrerer Threads in nur einem vollständigen Prozessor-Kern eines Prozessors unterstützt.

Ob die Gleichzeitigkeit real oder scheinbar ist, hängt dabei vom verwendeten Betriebssystem und der Hardware ab. Um reale Gleichzeitigkeit zu erreichen, muss ein Mehrprozessorsystem, ein Mehrkernprozessor oder ein Prozessor mit Core MultiThreading hardwareseitigem Multithreading zur Verfügung stehen, und das Betriebssystem muss es einem Prozess erlauben mehrere CPUs gleichzeitig (nebenläufig) für verschiedene Threads zu verwenden.

Threads werden verwendet, um einzelnen Programmteilen eine schnelle Reaktion auf Ereignisse zu erlauben.

Nur durch die Aufteilung eines Programms in mehrere Threads ist es möglich, die durch mehrere oder "mehrfädige" Prozessoren gebotene Leistung in einer einzelnen Anwendung auszunutzen.

Multithreading kann die Programmierung von Systemen wesentlich vereinfachen, sofern adäquate Unterstützung durch Programmiersprache oder Frameworks geboten wird. Da Quelltext und Laufzeit in ihrer Sequenzialität übereinzustimmen scheinen, erhalten Entwickler einen intuitiven Zugang.

Bei Multitasking wird die Nebenläufigkeit mehrerer Prozesse gefordert, während sich Multithreading auf die Nebenläufigkeit von Bearbeitungssträngen innerhalb eines Prozesses bezieht.

Einige Betriebssysteme, z. B. Linux vor Kernel Version 2.6, realisieren Multithreading innerhalb des Multitasking. Hier ist jeder Thread ein eigener Task, der selbständig in der Prozesstabelle aufgeführt wird. Im Gegensatz zu eigenständigen Prozessen teilen sich die verschiedenen Threads eines Prozesses jedoch den Speicher (Heap) und die vom Scheduler zugeteilte Rechenzeit.



</doc>
<doc id="13692" url="https://de.wikipedia.org/wiki?curid=13692" title="Beugung">
Beugung

Beugung steht für:

Siehe auch:


</doc>
<doc id="13698" url="https://de.wikipedia.org/wiki?curid=13698" title="Selbstbestimmungsrecht der Völker">
Selbstbestimmungsrecht der Völker

Das Selbstbestimmungsrecht der Völker ist eines der Grundrechte des Völkerrechts. Es besagt, dass ein Volk das Recht hat, frei über seinen politischen Status, seine Staats- und Regierungsform und seine wirtschaftliche, soziale und kulturelle Entwicklung zu entscheiden. Dies schließt seine Freiheit von Fremdherrschaft ein. Dieses Selbstbestimmungsrecht ermöglicht es einem Volk, eine Nation bzw. einen eigenen nationalen Staat zu bilden oder sich in freier Willensentscheidung einem anderen Staat anzuschließen.

Heute wird das Selbstbestimmungsrecht der Völker allgemein als gewohnheitsrechtlich geltende Norm des Völkerrechtes anerkannt. Sein Rechtscharakter wird außerdem durch Artikel 1 Ziffer 2 der UN-Charta, durch den Internationalen Pakt über bürgerliche und politische Rechte (IPBPR) sowie den Internationalen Pakt über wirtschaftliche, soziale und kulturelle Rechte (IPWSKR), beide vom 19. Dezember 1966, völkervertragsrechtlich anerkannt. Damit gilt es universell.

Das Prinzip der Selbstbestimmung wurde in der Philosophie der Aufklärung ausdrücklich formuliert und war zunächst ein individuelles Recht, eng verbunden mit dem Kantischen Begriff der „Mündigkeit“. Die Wandlung zum Gruppenrecht begann bereits mit dem Ringen um die Religionsfreiheit.

1659 erschien die Schrift "Gentis Felicitas" des Johann Amos Comenius (frei übersetzt bedeutet der Titel: „Volkswohlfahrt“). Die Schrift beginnt mit der Definition des Begriffes „Volk“ und leitet im zweiten Absatz aus dem individuellen Glücksstreben auch das Nationale her:

Danach stellt Comenius (jeweils mit Begründung und Erläuterung) 18 Merkmale für „Volkswohlfahrt“ zusammen, darunter einheitliche Bevölkerung ohne Mischung mit Fremden, innere Eintracht, Regierung durch Herrscher aus dem eigenen Volk und Reinheit der Religion.

Im späten 18. Jahrhundert wurde das Selbstbestimmungsrecht der Völker als „Volkssouveränität“ formuliert und errang in der Französischen Revolution und im Amerikanischen Unabhängigkeitskrieg den Sieg über das bis dahin als gültig anerkannte dynastische Prinzip. Diese Interpretation des kollektiven Selbstbestimmungsrechts ist somit eine Fortführung seines individuellen (kantschen) Gegenparts und negiert die ethnische Dimension des Volksbegriffs, die obige Definition beinhaltet. „Volk“ ist im Zusammenhang der bürgerlichen Revolutionen als politische Kategorie zu verstehen, die sich in „vertikaler“, das heißt zu den klassischen Herrschaftseliten (Adel, König), nicht aber in „horizontaler“ (im Gegensatz zu anderen ethnischen Volksgruppen) Abgrenzung manifestiert. In diesem Zusammenhang finden wir hier bereits den entscheidenden Unterschied zwischen einer ethnischen und einer politischen Definition des Begriffs „Nation“.

In dieser Tradition ist das Selbstbestimmungsrecht mit der Idee der Volkssouveränität eng verbunden. Voraussetzung für die Idee der politischen Selbstbestimmung war die Herausbildung des politischen Volksbegriffes im 19. Jahrhundert.

Bereits in den von der französischen Revolution motivierten Bestrebungen zur Bildung von Nationalstaaten im Europa des 19. Jahrhunderts setzte sich jedoch die ethnische Interpretation des Volksbegriffs immer mehr durch. Somit entwickelte sich nach der Durchsetzung des politischen Volksbegriffes nach der Revolution von 1848 die Idee des Nationalitätenprinzips, wonach jede Volksgruppe das Recht auf einen (eigenen) Staat habe. Dies richtete sich vor allem gegen die vielvölkerstaatlichen Königs- und Kaiserreiche des damaligen Europas.

Die Idee des Selbstbestimmungsrechts der Völker wurde von Lenin im Oktober 1914 propagiert und nach der Oktoberrevolution mit dem "Dekret über die Rechte der Völker Russlands" vorerst auch konsequent umgesetzt. Zudem forderte Leo Trotzki 1915 im Zimmerwalder Manifest, dass das „Selbstbestimmungsrecht der Völker […] unerschütterlicher Grundsatz in der Ordnung der nationalen Verhältnisse sein“ müsse.

Der damalige US-Präsident Woodrow Wilson legte seinen Friedensbemühungen (→ 14-Punkte-Programm) am Ausgang des Ersten Weltkriegs die Idee des Selbstbestimmungsrechts der Völker zugrunde, wenngleich er mit ihr einen anderen Inhalt verband als Lenin.

Nach dem Zweiten Weltkrieg findet sich das Selbstbestimmungsrecht in verschiedenen UN-Dokumenten. Auch in der sowjetischen Völkerrechtslehre ist zumindest in späteren Jahren vom „Selbstbestimmungsrecht der Völker und Nationen“ die Rede. Ihr zufolge ist unter „Volk“ die jeweilige Bevölkerung eines bestimmten Territoriums (unabhängig von der historischen Entwicklung, wie erforderlich für „Nation“ im Sinne Josef Stalins) zu betrachten. Erforderlich sind nur ein gemeinsames Gebiet und weitere Gemeinsamkeiten geschichtlicher, kultureller, sprachlicher und religiöser Art und die Verbindung durch gemeinsame Ziele, die sie mit Hilfe des Selbstbestimmungsrechtes erreichen will. Dem wurde auch durch das Ende der Entstellung des Selbstbestimmungsrechtes (Breschnew-Doktrin) Rechnung getragen. Im konkreten Fall, so bei der Frage der Rechtsstellung Gesamtdeutschlands nach dem Zweiten Weltkrieg, unterschieden sich freilich die Auslegungen gemäß dem jeweiligen Verständnis der Parteien deutlich.

Das Selbstbestimmungsrecht der Völker ist eines der Grundaxiome der Charta der Vereinten Nationen. Es wird in den Artikeln 1, 2 und 55 erwähnt und als eine Grundlage der Beziehungen zwischen den Staaten bezeichnet.

Eine bindende Verpflichtung der Vertragsstaaten zur Einhaltung des Rechts auf Selbstbestimmung geht aus den beiden Menschenrechtspakten der Vereinten Nationen hervor, die 1966 von der UN-Generalversammlung angenommen wurden und nach Erreichen der nötigen Anzahl an Ratifizierungen 1977 in Kraft traten. Der "Internationale Pakt über bürgerliche und politische Rechte" (IPbpR) sowie der "Internationale Pakt über wirtschaftliche, soziale und kulturelle Rechte" (IPwskR) gleichen Datums erklären das Selbstbestimmungsrecht für die Vertragsstaaten als verbindlich. In beiden Pakten heißt es gleichlautend in Artikel I:

Für die Überwachung der Einhaltung dieser Vertragspflicht sind der UN-Menschenrechtsausschuss und der UN-Ausschuss für wirtschaftliche, soziale und kulturelle Rechte verantwortlich. Der konkrete Gehalt dieser Rechtsnorm ist in einem "General Comment" des Menschenrechtsausschusses aus dem Jahre 1984 formuliert.

Das Selbstbestimmungsrecht der Völker ist "ius cogens" (vgl. die Kodifikation in Art. 53 Wiener Vertragsrechtskonvention (WVRK)). Es handelt sich mithin um eine Norm, von der nicht abgewichen werden darf, und die nur durch eine spätere Norm des allgemeinen Völkerrechts geändert werden könnte. Verträge, die gegen existierendes "ius cogens" verstoßen, sind nichtig (vgl. die in Art. 53 WVRK kodifizierte Regel).

Das Selbstbestimmungsrecht schafft grundsätzlich gerade keine Individualrechte, sondern bietet zunächst den Rahmen für deren Entfaltung oder jedenfalls die freie Gruppenbildung; ein Recht des Individuums darauf, dass der Gruppe, deren Mitglied es ist, dieses Recht gewährt wird, besteht freilich.

Das Selbstbestimmungsrecht der Völker ist auch in verschiedenen IGH-Urteilen als „universelles“ und „völkergewohnheitsrechtliches Prinzip“ mit "erga omnes"-Charakter anerkannt. Die Definition des Selbstbestimmungsrechtes wurde von der Generalversammlung der UNO in einer Erklärung seiner Prinzipien, der "Friendly Relations Declaration" vom 24. Oktober 1970, noch verfeinert. Sie besitzt zwar nicht den Charakter einer Rechtsnorm, wird aber von der Völkerrechtslehre verwendet, um den Inhalt des Selbstbestimmungsrechts zu bestimmen.

Auch weil das geltende Völkerrecht die territoriale Integrität aller Staaten, die, wie es heißt, „eine Regierung besitzen, welche "die gesamte Bevölkerung" des Gebiets ohne Unterschied der Rasse, des Glaubens oder der Hautfarbe vertritt“ schützt, gibt es keine völkerrechtliche Norm, „die ein Sezessionsrecht ausdrücklich bejahen oder verbieten würde“. Selbst aus der Staatenpraxis ist kein Sezessionsrecht abzuleiten. "Externe Selbstbestimmung" und Autonomie sind darüber hinaus fallabhängig zu gewähren und zu gestalten.

Wenn der Staat die nach Autonomie oder gar einem eigenen Staat strebende Minderheit durch seine Herrschaftsausübung diskriminiert, kann er seinen Anspruch auf territoriale Integrität nach dem gegenwärtigen Völkerrecht verwirken.

Nach Karl Doehring hat das Selbstbestimmungsrecht den Charakter eines Notwehrrechts: wenn eine ethnische Gruppe in fundamentaler Weise diskriminiert werde und zwar gerade aufgrund ihrer Gruppeneigenschaften, dann habe sie ein Recht auf Sezession. Es muss aber im Einzelfall genau geprüft werden, ob eine "evidente und fundamentale" Verletzung von Menschenrechten vorliegt und ob der Minderheit politische und rechtliche Möglichkeiten eingeräumt werden, um eigene Interessen zu vertreten oder sich notfalls gegen Benachteiligung zur Wehr setzen zu können. Es muss entschieden werden, ob eine so erhebliche Beeinträchtigung des diskriminierten Volkes vorliegt, die es rechtfertigt, das Selbstbestimmungsrecht des Staates teilweise einzuschränken. Der Staat hat durch seine Staatsgewalt also „das ganze [Staats-]Volk zu repräsentieren“. Werden einzelne Volksgruppen „"per definitionem" von der Teilhabe an staatlichem Leben ausgeschlossen“, dann erst verwirkt der Staat die Treuepflicht seiner Bürger.

Eine Definition von „Volk“ ist in den Menschenrechtspakten nicht getroffen worden und kann in rechtlicher Hinsicht in allgemeiner Form auch nicht getroffen werden. Was unter einem "Volk" zu verstehen ist, wird immer in einem konkreten historischen, politischen und kulturellen Umfeld konstruiert. Eine kleinere Gruppe innerhalb existierender Staaten kann als „Volk“ verstanden werden, wenn bestimmte Kriterien (z. B. eine gewisse Homogenität, gemeinsame Geschichte und die Selbstidentifikation als distinkte Gruppe) gegeben sind. Identitätsstiftende Merkmale haben inkludierende und exkludierende Funktionen, manche Menschen werden einbezogen, andere ausgeschlossen.

Auch indigene Völker berufen sich in ihren Forderungen zentral auf das Selbstbestimmungsrecht. Aus diesem Grund stellen sich zahlreiche Staaten auf den Standpunkt, es gebe keine indigenen Völker ("indigenous peoples"), sondern nur indigene Menschen ("indigenous people"). Den umgekehrten Weg ging Bolivien, als die Republik 2009 offiziell in "Plurinationaler Staat Bolivien" umbenannt wurde. Damit wurde immerhin betont, dass der Staat den Rechten der indigenen Bevölkerung Rechnung tragen müsste, um die Legitimität des Vielvölkerlandes zu rechtfertigen. Allerdings gibt es bisher keinen Staat, in dem das angestammte Land einer autochthonen Bevölkerung in eine autonom verwaltete Provinz mit denselben Rechten wie andere Provinzen verwandelt worden wäre.

Der Begriff "Selbstbestimmungsrecht der Völker" wird neben dem juristischen Ansatz auch im Sinne eines ethisch-moralischen Anspruchs verstanden, der gelegentlich zur Untermauerung politischer Ziele herangezogen wird und in vielen Konfliktfällen ein möglicher Lösungsweg für schwelende Konflikte sein könnte. Dabei handelt es sich allerdings dann noch nicht unbedingt um kodifiziertes oder allgemein durchgesetztes Völkerrecht.

So leiten beispielsweise Minderheiten daraus das Recht ab, sich als Volk zu definieren und Autonomie für sich zu beanspruchen, wobei unter Autonomie vom Recht auf Sprache und Brauchtum bis hin zur politischen Eigenstaatlichkeit alles verstanden werden kann. Eine solche Interpretation ist jedoch umstritten.

Die faktische Durchsetzbarkeit oder Durchsetzung des gültigen Rechtes hängt von dem jeweils aktuellen tatsächlichen Machtgefüge und den darin verwobenen Interessen ab.

Ein häufiges Problem sind überlappende Gebietsansprüche mehrerer Ethnien, die sich dabei jeweils auf ihr Selbstbestimmungsrecht berufen oder die Entstehung neuer Minderheiten durch die Begründung von Staaten, die unter Berufung auf das Selbstbestimmungsrecht entstanden sind. Wenn solche Minderheiten nun ihrerseits ihr Selbstbestimmungsrecht realisieren wollen, kann dies zu weiteren Konflikten führen. Konfliktträchtig wird der Anspruch auf Selbstbestimmung auch dann, wenn die natürlichen Ressourcen eines Landes in einem Gebiet besonders konzentriert sind und die in diesem Gebiet dominierende Bevölkerungsgruppe einen größeren Anteil an diesen Ressourcen einfordert.

Ein aktuelles Beispiel ist das Kosovo im ehemaligen Jugoslawien. Mit der kosovarischen Staatsgründung und Sezession von Serbien ergibt sich ein praktisches Problem der Anwendung des Selbstbestimmungsrechtes, wer überhaupt an der Willensäußerung der Sezession von einem bereits bestehenden Staat teilnehmen darf. Dies gilt insbesondere bei zahlenmäßig relativ kleinen Völkern, bei denen von einem Kolonisator durch Ansiedlungen eigener Volksangehöriger das Erlangen einer Mehrheit zur Abspaltung in Ausübung des Selbstbestimmungsrechtes in Volksabstimmungen nach dem Mehrheitsprinzip rasch effektiv verhindert werden kann bzw. bereits die Abhaltung der Volksabstimmung institutionell verhindert würde.

Bis zum Ersten Weltkrieg standen die meisten Völker Mitteleuropas unter der Herrschaft europäischer Imperien: des Russischen Reichs, des bereits seit dem 18. Jahrhundert im Zerfall begriffenen Osmanischen Reichs und Österreich-Ungarns. Das Deutsche Reich umfasste zudem einen Teil der polnischen, dänischen und französischen Siedlungsgebiete. Ergebnis des Ersten Weltkriegs war der Zerfall dieser Imperien und die Entstehung neuer Nationalstaaten.

Woodrow Wilsons Konzept bezog sich in allererster Linie auf die „historischen Nationen“, z. B. der Polen und Tschechen, die ihre frühere Eigenstaatlichkeit durch Teilung beziehungsweise Unterwerfung eingebüßt hatten und nun unter der Herrschaft der Kontinentalmächte Russland, Österreich-Ungarn und Deutschland standen. Bevölkerungsgruppen ohne eigenstaatliche Vergangenheit wurden in geringerem Maße berücksichtigt.

Dass aufgrund der ethnischen Gemengelage in den betroffenen Ländern nicht jede Nation einen eigenen, ökonomisch lebensfähigen Nationalstaat bilden konnte, war Wilson durchaus bewusst. Er verstand seinen Begriff der "„self-determination“", wie einige Historiker glauben, weniger als „nationale Selbstbestimmung“ denn als „demokratische Selbstbestimmung“ ("self-government"). Die Völker Europas sollten Demokratien werden, weil diese Herrschaftsform, so glaubte der US-amerikanische Präsident, prinzipiell friedfertiger sei als andere politische Ordnungen. Dieser demokratiepolitische Aspekt wurde aber von den jungen Nationen Ostmittel- und Südosteuropas und namentlich von den Deutschen geflissentlich übersehen. So entstanden aus den zerfallenen Großreichen überwiegend neue Nationalitätenstaaten, die aber teilweise wie Nationalstaaten regiert wurden.

Hierzu gehören die erste Tschechoslowakische Republik, die die historischen Kronländer Böhmen und Mähren, die zuvor als „Oberungarn“ bekannte Slowakei und die Karpatoukraine umfasste, das als Königreich der Serben, Kroaten und Slowenen begründete Jugoslawien, Rumänien mit der ungarischen und deutschen Bevölkerung in Siebenbürgen wie auch Polen, das große Teile Litauens, Weißrusslands und der Westukraine unter seine Herrschaft brachte. Die prozentual größten Gebietsverluste erlitt dabei Ungarn, das auf ein Drittel des Territoriums verkleinert wurde, das es in der habsburgischen Doppelmonarchie umfasst hatte.

Sowohl Ungarn als auch Deutsche sahen sich in den Nachfolgestaaten der Österreichisch-Ungarischen Monarchie plötzlich in einer Minderheitenposition und forderten dementsprechend – überwiegend erfolglos – eine Umsetzung ihres Selbstbestimmungsrechts ein. Dies betraf insbesondere die Tschechoslowakei, in der die Deutschen die zweitgrößte Bevölkerungsgruppe darstellten.

Der Zusammenschluss Österreichs mit dem Deutschen Reich wäre nur durch die Zustimmung der Alliierten möglich gewesen. Diese lehnten aber eine Union beider Staaten im Vertrag von Saint-Germain ab, weil dies zur Bildung einer Kontinentalmacht geführt hätte. Die Staatsbezeichnung „Deutschösterreich“ wurde von den Alliierten ebenfalls abgelehnt und musste in „Republik Österreich“ geändert werden.

Auch die deutschen Gebietsabtretungen an Polen, Belgien, Frankreich und Dänemark infolge des Versailler Vertrags wurden aus deutscher Sicht als Missachtung des Selbstbestimmungsrechts gewertet, da ein Teil der Abtretungen entweder ohne Volksabstimmung erfolgte oder ihr Ergebnis ignoriert oder manipuliert wurde.

Der afrikanische Kontinent befand sich bis zur Mitte des 20. Jahrhunderts weitgehend unter europäischer Kolonialherrschaft. Auch die Unabhängigkeitsbewegungen dieses Kontinents stützten sich in ihren Bestrebungen auf das Selbstbestimmungsrecht der Völker, jedoch nicht in derselben Weise wie die neuen Nationalstaaten Mitteleuropas. So einigten sich die Mitglieder der Organisation für Afrikanische Einheit frühzeitig darauf, die überwiegend auf der Berliner Kongokonferenz von 1884/1885 festgelegten kolonialen Staatsgrenzen nicht anzutasten. Die meisten afrikanischen Staaten sind nach europäischem Verständnis Vielvölkerstaaten. Ein Selbstbestimmungsrecht, das sich auf einzelne Ethnien bezogen hätte, wurde hier nicht realisiert. Dies geschah angesichts der Befürchtung, dass Grenzrevisionen entlang ethnischer Linien eine nicht endende Kette von Kriegen in Gang gesetzt hätten. Die einzigen Fälle einer Anerkennung eines später entstandenen Staates sind Namibia, das 1990 seine Unabhängigkeit von Südafrika erlangte, Eritrea, das sich nach mehreren Jahrzehnten des Kriegs von Äthiopien lossagte, und der Südsudan, der sich vom Sudan abgespalten hat. Das seit 1991 de facto unabhängige Somaliland bleibt dagegen ohne internationale Anerkennung und gilt als stabilisiertes De-facto-Regime.

Gleichzeitig existieren in verschiedenen afrikanischen Ländern sezessionistische Tendenzen, die durch den Ressourcenreichtum einzelner Regionen motiviert sind.

Bisher unerfüllt blieb zudem die Forderung der indigenen Sahrauis nach der Unabhängigkeit der Westsahara. Diese stellt heute die letzte Kolonie auf dem afrikanischen Kontinent dar.

Die Renaissance der Nationalbewegungen in den Unionsrepubliken gehörte zu den wichtigsten Triebkräften, die das Ende der Sowjetunion herbeiführten. Die Vorreiterrolle hatten hierbei die baltischen Staaten, die 1941 im Zuge des Deutsch-sowjetischen Nichtangriffspaktes von der UdSSR okkupiert und dann annektiert worden waren.

Die nationalen Bewegungen in diesen drei Republiken konnten nicht nur beträchtlichen Zulauf bei der Durchsetzung und Anerkennung der souveräner Rechte erreichen, sondern auch die letztendlich erfolgreiche einseitige Loslösung von der Sowjetunion. Es ist zu bemerken, dass bei baltischen Republiken die Frage des Selbstbestimmungsrechts der Völker im Sinne des Völkerrechts beim Zerfall der Sowjetunion gar nicht in Frage gestellt werden kann, da die Staatlichkeit der baltischen Republiken schon vor der Okkupation der UdSSR existierte und 1990–1992 die (politische) Unabhängigkeit nur zu verkünden war. Neue baltische Staaten sind nach dem Zerfall der UdSSR nicht entstanden, da sie als Staaten auch früher gegolten haben.

Das Vorbild der Staaten im Baltikum machte zunächst in denjenigen Teilrepubliken Schule, die ihrerseits auf die Tradition von Nationalbewegungen zurückblicken konnten, etwa Georgien und die Ukraine, wurde jedoch auch von Staatsführern adaptiert, deren Staatsnationen überwiegend eine Kreation der Stalin-Ära waren, z. B. Turkmenistan.

Das Konzept des Selbstbestimmungsrechts konnte auch deshalb so in manchen ehemaligen sowjetischen Teilrepubliken einflussreich werden, da Lenin es bereits vor der Oktoberrevolution zum Kern seines Programms gemacht hatte. Seiner Theorie nach war die Sowjetunion ein freies Bündnis freier Völker die ihr Selbstbestimmungsrecht verwirklicht hatten.

Dieser positive Bezug auf das Selbstbestimmungsrecht wurde während der gesamten sowjetischen Epoche beibehalten. Die Ukraine und Weißrussland wurden auf diese Weise – als theoretisch selbstständige Staaten – so Gründungsmitglieder der Vereinten Nationen.

Besonders in den ersten Jahrzehnten wandte das sowjetische Regime erhebliche Mittel für das "nation building" in den neu gegründeten Republiken Turkestans auf, in dem bis dato keinerlei nationalstaatliche Tradition existierte, sondern der Bezug auf Stammesidentität, Religion und Lebensweise (nomadisch vs. sesshaft) identitätsprägend gewirkt hatte.

Während Lenins Theorie besagte, dass die Nationen, wenn ihnen größtmögliche Selbstbestimmung gewährt würde, auf die Dauer von selbst verschwinden würden, trat historisch das Gegenteil ein: Die Sowjetunion zerbrach 1991 exakt entlang jener Grenzen, die von Lenin und seinem Nationalitätenkommissar Stalin gezogen worden waren.

Dieses Zerbrechen führte nun zur Entstehung erheblicher russischer Minderheiten in den Nachbarstaaten, die nun ihrerseits teilweise die Frage der Selbstbestimmung stellten. Hierzu gehört die gewaltsame russische Abspaltung Transnistriens von Moldawien sowie die Autonomiebewegung auf der mehrheitlich russisch besiedelten Halbinsel Krim. Eine bedeutende russische Minderheit gibt es in Kasachstan. Die Verlegung des Regierungssitzes aus der Metropole Almaty in die nördliche Provinzstadt Akmola durch den autoritär regierenden Präsidenten Nursultan Nasarbajew wird auch als Zugeständnis gegenüber der russischen Bevölkerung erklärt.

Andere blutige Konflikte, in denen sich Parteien auf das Selbstbestimmungsrecht beriefen und berufen sind:

Auch die Staatsideologie der Sozialistischen Föderativen Republik Jugoslawien ging nominell von einer Eigenständigkeit der Teilrepubliken aus, die etwa in kultureller aber auch in wirtschaftlicher Hinsicht einen gewissen Spielraum genossen. Auch hier gehören einander ausschließende Bezüge auf das Selbstbestimmungsrecht durch mehrere auf demselben Territorium siedelnde Ethnien zu den Faktoren, die das blutige Auseinanderbrechen des Staatsverbands während der Jugoslawienkriege herbeiführten. Jedoch hatten auch die Nachfolgestaaten des ehemaligen Jugoslawiens wieder Probleme mit den verschiedenen nationalen Minderheiten in den neuen Staaten. Ein Beispiel ist das Verhältnis der Serben in Kroatien.

Die Missachtung des freien Selbstbestimmungsrechtes der Völker kann Konflikte auslösen, intensivieren oder gegebenenfalls auch nur verdeutlichen. Beispiele aus jüngerer Vergangenheit und in der Gegenwart sind:

Eines der bekanntesten Beispiele ist die friedliche Wahrnehmung des Selbstbestimmungsrechtes des indischen Volkes unter der Führerschaft Gandhis gegenüber Großbritannien.
Argumente des Selbstbestimmungsrechtes der Völker spielen auch eine zentrale Rolle bei der Begründung der Rechtsposition der tibetischen Exilregierung (CTA) hinsichtlich Tibets. Die Exilregierung geht dabei mit der Haltung des Dalai Lama bewusst einen gewaltlosen Weg, der sich an buddhistischen Prinzipien ausrichtet.

Kritiker verweisen darauf, dass das Selbstbestimmungsrecht oft verweigert wurde, und halten es für kaum mehr als ein politisches Schlagwort. Ein Rechtsanspruch könne erst bestehen, wenn eine genauere Definition des Rechtes und seiner Träger die Gefahr der „Atomisierung“ der Staaten dadurch, dass sich eine unzufriedene Gruppe plötzlich zum Volk erkläre, zumindest verringert habe. Weil dies weder in den Menschenrechtspakten noch in der Prinzipienerklärung vom 24. Oktober 1970 geschehen sei, könnten die Pakte das Recht nicht gewähren. Dem kann jedoch u. a. entgegengehalten werden, dass weder die bloße fehlende positivrechtliche Durchnormierung (es gibt ja noch andere Quellen außer dem vertraglichen Völkerrecht) noch die Strittigkeit einzelner Bereiche bei grundsätzlicher Konturierung die Rechtsnormqualität hindern, und es widerspricht übrigens auch dem klaren Wortlaut der existierenden vertraglichen Normen.

Einige Wissenschaftler und Politiker lehnen das Selbstbestimmungsrecht der Völker prinzipiell ab. So beschrieb es Ralf Dahrendorf 1989 als „barbarisches Instrument“:
Götz Aly erklärte, beim Selbstbestimmungsrecht der Völker habe es sich ursprünglich um eine „nationalistische Kampfparole des 19. Jahrhunderts“ gehandelt, und bezeichnet es als „zutiefst vergiftet“. Nach seiner Auffassung zertraten „immer wieder […] Mehrheiten, die sich zum „Volk“ erklärten, unter dem Motto Selbstbestimmung die Rechte von Minderheiten und die das Individuum schützenden unveräußerlichen Grundrechte.“ Deshalb zählt er „das Selbstbestimmungsrecht der Völker zu den Ursachen der Katastrophen des 20. Jahrhunderts.“





</doc>
<doc id="13699" url="https://de.wikipedia.org/wiki?curid=13699" title="Boccaccio">
Boccaccio

Boccaccio bezeichnet:



Boccaccio ist der Familienname folgender Personen:



</doc>
<doc id="13701" url="https://de.wikipedia.org/wiki?curid=13701" title="Plejaden">
Plejaden

Die Plejaden (auch Atlantiden, Atlantiaden, Siebengestirn, Taube, Sieben Schwestern, Gluckhenne) sind ein offener Sternhaufen, der mit bloßem Auge gesehen werden kann. Im Messier-Katalog hat er die Bezeichnung M45. Sie sind Teil unserer Galaxie, der Milchstraße.

Da die Plejaden bereits lange vor Erfindung des Teleskops als Sterngruppe bekannt waren, werden traditionell auch oft nur die hellsten Hauptsterne als Plejaden bezeichnet. In manchen Kulturen und historischen Darstellungen werden nur sechs Sterne zu den Plejaden gerechnet. Der Grund dafür ist Pleione, der ein veränderlicher Stern ist.

Seine scheinbare Helligkeit schwankt langsam, aber unregelmäßig zwischen der von Taygeta und Celaeno, so dass Pleione manchmal erst dann gesehen wird, wenn Celaeno auch schon erkannt werden kann.
Mit bloßem Auge sind daher, je nach Sichtbedingungen, sechs bis neun Sterne zu erkennen. Der Sehungsbogen ist bei klarem Himmel mit 14,5° bis 15,5° anzusetzen; bei trüber Witterung mit 19,5° bis 20,5°. Der heliakische Aufgang ist bei guten Sichtbedingungen ab einer Horizonthöhe von 6° bis 7° beobachtbar; die Sonne befindet sich zu diesem Zeitpunkt etwa 9° unter dem Horizont.

Die Plejaden sind etwa von Anfang Juli bis Ende April am nördlichen Sternhimmel sichtbar.

Der Sternhaufen liegt knapp 140 Parsec entfernt im Sternbild Stier, umfasst mindestens 1200 Sterne und ist etwa 125 Millionen Jahre alt.

Im NGC-Katalog sind die Plejaden nicht aufgeführt, jedoch gibt es im Bereich der Plejaden mehrere Reflexionsnebel mit eigenen NGC-Nummern. Hierzu gehören der "Maja-Nebel" NGC 1432 und der "Merope-Nebel" NGC 1435. Nur etwa eine halbe Bogenminute oder 0,06 Lichtjahre von Merope entfernt befindet sich eine Konzentration von interstellarem Staub, die als IC 349 oder "Barnards Merope-Nebel" bekannt ist und kinematisch unabhängig von den Plejaden ist.

Der offene Sternhaufen erscheint mit einer Ausdehnung von ca. 2° etwa viermal so groß wie der Mond, der zum Vergleich unten links in die Aufnahme kopiert wurde. Unberücksichtigt ist die physiologische Wahrnehmung, helle Objekte am Himmel in ihrer Größe zu überschätzen.

Die Plejaden befinden sich mit etwa 400 Lichtjahren Entfernung nahe genug, damit aufgrund des Umlaufs der Erde um die Sonne im Laufe eines Jahres für die einzelnen Sterne eine messbare jährliche Parallaxe auftritt. Mit Hilfe dieser Methode und Messungen weiterer Methoden ergab sich für die Plejaden ein Abstand von etwa 135 Parsec (entspricht etwa 440 Lichtjahre). Unter Verwendung der trigonometrischen Parallaxe wurde dann allerdings mit dem 1989 gestarteten Satelliten Hipparcos die Entfernung der Plejaden 1999 abweichend zu anderen vorangegangenen Messungen auf 120 Parsec (entspricht 390 Lichtjahre) bestimmt. 2009 wurden in einer Publikation unter Verwendung der Hipparcos-Daten sogar weniger als 120 Parsec angegeben. Die Daten des Hipparcos-Satelliten wichen damit deutlich von früheren Messungen ab. Die aus den Hipparcos-Daten ermittelte Entfernung bedeutete jedoch, dass die physikalischen Modelle für junge Sterne korrigiert werden mussten: Wegen der nun kleineren Distanz bei gleicher scheinbarer Helligkeit müssten die Sterne in den Plejaden tatsächlich mit geringerer absoluter Helligkeit leuchten. Um dies in den physikalischen Modellen zu berücksichtigen, müssten die Sterne der Plejaden einen weitaus höheren Heliumanteil besitzen, welcher so allerdings nicht nachgewiesen wurde. Als 2014 eine erneute trigonometrische Messung durch Very Long Baseline Interferometry die ursprünglichen Messungen von rund 135 Parsec Entfernung bestätigten und damit die bisherigen physikalischen Modelle untermauerten, wurden Zweifel an den Berechnungen mit den Hipparcos-Daten laut. Auch zur Nachfolgemission Gaia, welche die Genauigkeit der Messungen von Hipparcos übertreffen soll und deren Mission 2018 enden wird, gab es daraufhin kritische Stimmen, da bei Gaia dieselbe Methodik verwendet wird wie bei Hipparcos. Erwähnenswert ist, dass andere Entfernungsmessungen von Hipparcos mit anderen Daten übereinstimmen und die Datenlage allein bei denen der Plejaden abweicht. Vorläufige Ergebnisse der Gaia-Mission, die im September 2016 veröffentlicht wurden, geben nun die Entfernung der Plejaden mit 134 ± 6 Parsec an, bestätigen also ebenso wie die 2014 vorgenommene trigonometrische Messung durch die Very Long Baseline Interferometry die älteren Distanzbestimmungen.

Die Plejaden galten in vielen Kulturen als besondere Sterne. So wird z. B. eine Gruppe sechs gezeichneter Punkte in den Höhlen von Lascaux als Darstellung der Plejaden gedeutet.

Ein bedeutendes, vermutlich der Astronomie dienendes Objekt aus Mitteleuropa ist die Himmelsscheibe von Nebra. Eine Gruppe von sieben eng beieinander liegenden Punkten wird mit den Plejaden identifiziert.

Das Siebengestirn wurde erstmals in Sumer schriftlich als Sternbild erwähnt (mul.mul) und als "Siebengottheit der großen Götter" bezeichnet.

In der Bibel werden die Plejaden im Buch Hiob erwähnt: "Kannst du knüpfen das Gebinde des Siebengestirns, oder lösen die Fesseln des Orion?".

Im biblischen Mythos werden die Plejaden als Taube symbolisiert, welche als Frühlingsgestirn die Wiederaufweckung der Natur ankündigen.

Sie galten als "Sterne des Enki" oder "Sterne, die dort stehen, woher der Ostwind kommt". Im Astrolab B, das aus dem 12. Jahrhundert v. Chr. stammt, repräsentieren die Plejaden das zweite Tierkreiszeichen Stier.

Als bildliches Glyptik-Symbol und Darstellung als "Siebengottheit" finden sich die Anfänge bei den Assyrern in der Zeit vom 15. zum 14. Jahrhundert v. u. Z., der Mitanni-Zeit. Häufig zierten die Plejaden assyrische Denkmäler und wurden in prophetischen Texten angerufen. In Babylonien spielte das Siebengestirn eine mehr untergeordnete Rolle und wurde daher kaum bildlich dargestellt. Die Babylonier sahen in ihm die magische Zahl vierzig, da die Plejaden für 40 Tage von der Sonne verdeckt wurden.

Die hellsten Sterne sind nach Gestalten der griechischen Mythologie benannt, dem Titanen Atlas (daher der andere Name), seiner Frau Pleione sowie ihren sieben Töchtern Alkyone, Asterope, Celaeno, Elektra, Maia, Merope und Taygete. Die Plejaden, die als Nymphen einzuordnen sind, erzogen Dionysos und Zeus.
Der Mythologie nach wurden sie von Orion verfolgt. Zeus versetzte sie als Sternbild an den Himmel, doch auch dort werden sie noch immer von Orion verfolgt, dessen Sternbild sich etwa 30° südöstlich der Plejaden befindet.
In Japan sieht man die sechs hellsten Sterne der Plejaden als das Sternbild "Subaru" an, wovon der Name und das Markenzeichen der japanischen Automobilmarke Subaru abgeleitet ist.

In der arabischen Literatur werden die Plejaden genannt. Der Name wurde auch zu einem weiblichen Vornamen im türkischen (als "Surayya") und im arabischen Sprachraum (z. B. Soraya Obaid). Es ist auch der Name des Satellitentelefonsystems Thuraya, das seinen Sitz in den Vereinigten Arabischen Emiraten hat.

Für die Beduinen signalisiert der Aufgang der Plejaden den Sommer und der Untergang den Winter: „Die Plejaden gehen auf über dürrer Getreidegarbe und unter, wenn das Tal zum Bach wird.“ Das entspricht der jüdischen Anschauung: „Die Welt kann wegen der Kälte der Plejaden nur deshalb bestehen, weil der Sirius mit seiner Hitze für Ausgleich sorgt.“

Die Griechen und Römer (lat. Vergiliae) betrachteten den Frühuntergang des Siebengestirns Anfang November als das Zeichen der Feldbestellung und das Ende der Schifffahrt. Mit dem Frühaufgang um den damaligen 20. Mai galten die Plejaden als Signalgeber für die beginnende Ernte (siehe auch Gezer-Kalender).

Flavius Josephus erwähnt, dass beim Niedergang des Siebengestirns um die Zeit des Laubhüttenfestes im November der einsetzende Regen dem Wassermangel ein Ende macht. Die Massai in Afrika benutzen die Plejaden in der heutigen Zeit als "Regenzeitsignalgestirn".
Die von Gladys Dickson herausgegebene "arabische Astrologie" nennt den 20. Mai für den Frühauf- und den 17. November für den Frühuntergang (siehe auch: Heliakisch); in der alten griechischen Tradition erwähnt die "Geoponica (Kap. 1)" die entsprechenden Daten für den 10. Juni und 4. November.

Für die Blackfoot-Indianer Nordamerikas war das Sternbild der Plejaden von entscheidender Bedeutung. Die Blackfoot waren nomadische Jäger und Sammler. Sie wohnten in kleinen Gruppen in Tipis aus Bisonfellen. Zu Jagdzügen schlossen sich manchmal einige Gruppen oder gar ein gesamter Unterstamm zusammen. Der Stand der Plejaden zu Beginn der Trockenzeit war das Startsignal für eine aufwendige Treibjagd der riesigen Bisonherden. Sind dann die Plejaden am Sternenhimmel Ende April verschwunden, sind auch die Bisons verschwunden.

In pazifischen Kulturen bestimmt der Aufgang der Pleiaden das Neujahrsfest. In Neuseeland ist Matariki eines der wichtigsten Feste der Māori. Auf der Inselwelt Französisch-Polynesien feiert man einmal jährlich das Plejadenfest. Es ist eine Art Neujahrsfest, ein Fest der Fülle und des Wandels.

Alle 18,6 Jahre werden die Plejaden über einen längeren Zeitraum regelmäßig vom Mond bedeckt. Die letzte Serie ging von 2005 bis 2009 (hier die in Mitteleuropa beobachtbaren Ereignisse):

Danach kommt es erst wieder ab dem Jahr 2024 für einen Beobachter auf der Erde zu zeitweiligen Verdeckungen der Plejaden durch den Mond. Die Plejaden bilden zusammen mit den Hyaden das sogenannte Goldene Tor der Ekliptik.





</doc>
<doc id="13706" url="https://de.wikipedia.org/wiki?curid=13706" title="Klassizismus">
Klassizismus

Klassizismus bezeichnet als kunstgeschichtliche Epoche den Zeitraum etwa zwischen 1770 und 1840. Der Klassizismus löste den Barock bzw. das Rokoko ab. Eine Form des Klassizismus ist das Biedermeier. Die Epoche wurde in der Malerei und Literatur von der Romantik begleitet und in der Architektur vom Historismus abgelöst.

Im Verhältnis zum Barock kann der Klassizismus als künstlerisches Gegenprogramm aufgefasst werden. Gegen Ende des 18. Jahrhunderts gelangte er nach einer ersten Phase der Koexistenz durch die anhaltenden Diskussionen über die ästhetischen Leitbilder des Barocks zur Vorherrschaft. Der Klassizismus in der Architektur basiert auf dem Formenkanon des griechischen Tempelbaus, lehnt sich teilweise aber auch an die italienische Frührenaissance an.

Außerhalb des mittel- und osteuropäischen Raums wird der Klassizismus als „Neoklassizismus“ bezeichnet, während im deutschsprachigen Raum unter Neoklassizismus eine Strömung des Historismus im frühen 20. Jahrhundert verstanden wird.

Der Begriff findet auch in dem Sinn eines künstlerischen Rückgriffs auf antike griechische oder römische Vorbilder seine Verwendung. So trat er bereits seit dem 17. Jahrhundert in den europäischen Künsten in verschiedenen Strömungen, Themenstellungen und unterschiedlichen regionalen Ausprägungen in Architektur, Malerei und Plastik in Erscheinung (siehe Classicisme).

Der Begriff ist im europäischen Sprachraum mehrdeutig und bezieht sich meist nicht auf ein und dieselbe Kunstepoche. So bezeichnet man beispielsweise die Baukunst Palladios (1508–1580) und seiner Nachfolger als Klassizismus (siehe Palladianismus). Als Klassizismus benennt man ferner die Kunst Frankreichs, Hollands und Englands im 17. Jahrhundert. Seit der Renaissance entstanden klassizistische Unterströmungen, die auch in der Zeit des Barocks immer wirksam waren. Besonders ausgeprägt ist diese Strömung in Frankreich und England (siehe klassizistischer Barock). So wird für die im deutschsprachigen Raum als "Klassizismus" bezeichnete Epoche in England, Frankreich, Spanien und Italien der Begriff "Neo-Klassizismus" verwendet, der teilweise auch im Deutschen übernommen wurde.

Im späten 18. Jahrhundert galt der Klassizismus mit einer purifizierenden Vereinfachung der Formen als Gegenmodell zur Kunst des Barocks, die mit dem Feudalismus assoziiert wurde. Gegenüber dem vorangegangenen Rokoko zeichnet sich der Klassizismus durch eine Rückkehr zu geradlinigen, klaren Formen und einer stärkeren Anlehnung an klassisch-antike Vorbilder aus.

Als geistiger Begründer im deutschsprachigen Raum gilt Johann Joachim Winckelmann.

Der Übergang von spätbarocken Formen zum Klassizismus wird vor allem in der älteren deutschen Kunstgeschichte bisweilen als Zopfstil bezeichnet. Benannt ist er nach dem Zopf, in dem die barocke Blumengirlande zu einem dünnen Band reduziert wird.

In Frankreich beginnt die Epoche des "Klassizismus", die in Frankreich als "neo-classicisme" bezeichnet wird, gegen Ende der Regierungszeit von Ludwig XV. Der vergleichbare Stil wird Louis-seize (vorrevolutionärer Klassizismus) genannt. Während einer Übergangszeit von 1750 bis 1760, die als "style transition" bezeichnet wird, finden sowohl Elemente des Rokoko, des "goût pittoresque" als auch klassische Formen Verwendung. Der Frühklassizismus wird in Frankreich auch als "goût grec" bezeichnet, geht nach 1770 in den "goût étrusque" des Louis-seize aus der Regierungszeit Ludwigs XVI. über.
In Österreich fällt dies mit der Regierungszeit Josephs II. zusammen, der auch neue Bauaufgaben initiiert (Kirchen für neue Pfarrsprengel, Krankenhäuser, öffentliche Schulen und Parks; siehe Josephinismus).

In Großbritannien nennt man die frühklassizistische Phase Late Georgian.

Ab den 1790er Jahren galt der Klassizismus als der „Stil der Revolution“, vor allem in der Architektur, wo wuchtige Formen bevorzugt werden. Mit der Vereinnahmung der Revolution durch Napoleon Bonaparte kommt es dann zum dekorativeren "Empirestil", der sich mit der Herrschaft des Kaisers über ganz Westeuropa ausbreitet. Auch Jacques-Louis David, der Begründer des Klassizismus in der Malerei, wird zum Anhänger der Revolution und später Hofmaler Napoleons.

In Großbritannien fasst man diese Zeit als Regency zusammen (nach der Herrschaft des Prinzregenten und künftigen Königs Georg IV.).

Die Architektur und Malerei des Biedermeier stellt ihm gegenüber eine weitere Wendung ins Dekorative dar, die gleichwohl keine grundsätzliche ästhetische Abwendung bedeutet. In der Malerei hält sich diese Ästhetik bis in die 1870er Jahre, in der Architektur wird sie schon in der ersten Jahrhunderthälfte durch alternative Bauformen, am frühesten von der Neugotik in Frage gestellt. Gesellschaftlich werden die neuen Bauformen mit dem aufstrebenden Bürgertum und seiner Wünsche nach Repräsentation assoziiert. Paul Sprenger, ein wichtiger Repräsentant der späten klassizistischen Architektur in Österreich, wurde geradezu als „Metternich der Architektur“ bezeichnet.

Um die Mitte des 19. Jahrhunderts setzte eine Entwicklung vom Klassizismus hin zum Historismus ein. Eine prägende Stilform dieses Übergangs ist der Rundbogenstil, der ab etwa 1828, gedanklich untermauert durch die Schrift „In welchem Style sollen wir bauen?“ von Heinrich Hübsch, eine erste stilistische Transformation des Klassizismus einleitete.

Die Abgrenzung des Klassizismus zum Historismus ist weder chronologisch noch stilistisch ganz einfach. Einerseits ist der Klassizismus selbst ein „historisierender“ Stil, der sich an die Antike und ihrer Interpretation in der Renaissance anlehnt. Andererseits teilt der Historismus zum Teil dasselbe Formenrepertoire, besonders deutlich in der Neorenaissance. Dazu kommt noch, dass der Spätklassizismus durchaus eine Vorliebe für bestimmte Dekorationsformen, etwa aus der byzantinischen oder arabischen Kunst, zeigt. Der Grundzug des Historismus ist dann auch nicht so sehr die „Ablösung“ vom Klassizismus, sondern sein Einfügen in einen pluralistischen Kanon von Stilen – daher auch der Alternativbegriff Eklektizismus. Der schlagendste Unterschied ist die weitaus größere Dekorfreudigkeit der historistischen Bauten und Ausstattungen, die dem in der Gründerzeit reichgewordenen Bürgertum eher zusagte als der spartanische Stil der ersten Jahrhunderthälfte.

Als Übergangsbauwerk zwischen Klassizismus und Historismus in Österreich gilt die Altlerchenfelder Pfarrkirche, bei deren Bau eine Debatte über den „richtigen Stil“ geführt wurde, was schon die Geisteshaltung des Historismus ankündigt.

Der programmatische Schwerpunkt auf der klassischen Antike unterscheidet den Klassizismus vom Historismus.

Im Gegensatz zum Klassizismus greift der Historismus auf zahlreiche andere Strömungen zurück: Neuromanik, Neugotik, Neorenaissance, Neobarock, Neorokoko. Ebenso existiert eine Neudeutung seiner selbst im Neohistorismus. Ferner findet der Klassizismus zur Wende des 19. zum 20. Jahrhundert als "Neoklassizismus" eine Neugeburt.

Im Historismus fehlt der Bezug auf die theoretischen Konzeptionen, wie sie etwa Vitruv und andere römische Bauforscher entwickelt haben, und die im Klassizismus als Kanon zugrunde gelegt werden. Der Zugang des Historismus zur klassisch-antikisierenden Formensprache ist eklektisch und auf formale Aspekte beschränkt.

In der Malerei lösten sich die Künstler von dem allegorischen Programm der Barockzeit und malten Szenen aus der griechischen und römischen Antike, die oft einen „patriotischen“ Hintersinn haben. Die Konturen werden klarer und die pastose Farbgebung verschwindet zugunsten eines flächigen Farbauftrages. Die koloristischen Aspekte der Malerei traten in den Hintergrund. Auf Farbigkeit konnte ein strenger Klassizist im Prinzip auch verzichten. Daher wirkt die Farbgebung eher kühl. Körpergrenzen werden zeichnerisch scharf abgegrenzt. Eine klar überschaubare und harmonische Komposition der Figuren, ein ruhiges Zeitmaß waltet in allen Gebärden.

In Illustrationen sind Umrissradierungen für den Klassizismus charakteristisch.

Die Architektur des Klassizismus orientiert sich stärker als vorherige Stile an dem antiken Bauten, vornehmlich griechischen Vorbildern. Portikus und Säulenordnung sind nun häufiger anzutreffen. Anwendung findet der Stil in fürstlichen und bürgerlichen Repräsentationsbauten, aber auch bei Bauwerken in traditionellen Bautechniken wie im Fachwerkbau. Seltener sind klassizistische Kirchen, hierbei dient der achteckige Turm der Winde oder das Pantheon als Vorbild.

Künstler, die dem Klassizismus zugeordnet werden, siehe:



 


</doc>
<doc id="13710" url="https://de.wikipedia.org/wiki?curid=13710" title="Verdammnis">
Verdammnis

Verdammnis (althochdeutsch "firdammon"; von lat. "damnare", „büßen lassen“, „verurteilen“, „verwerfen“; zu lat. damnum „Buße“, „Verlust“, „Schaden“), auch in der Erweiterung "ewige Verdammnis", bedeutet das Verworfensein vor Gott und die Verurteilung zur Pein einer Höllenstrafe aufgrund begangener Taten. Von jemandem, der sprichwörtlich „der Verdammnis anheimgefallen“ ist, wird angenommen, dass er entweder in der Hölle ist, sich nicht im Stand der Gnade befindet oder von Gott verworfen worden sei.

Im Christentum wird die Existenz einer Hölle gelehrt. Dabei gilt die Hölle als Ort ewiger Verdammnis, an den die Seelen jener, die aus freiem Entschluss in Todsünde sterben, nach dem Partikulargericht gelangen. Die Strafe wird im letzten Gericht vervollständigt, „denn nach ihm werden die Gottlosen an Leib und Seele zugleich gepeinigt“. Die Hölle steht im Gegensatz zu einem Ort absoluter Glückseligkeit (Paradies, ewiges Leben, Himmel). Das Purgatorium (Fegefeuer) als Ort der abschließenden Läuterung der Seele ist von der Bestrafung der Verdammten völlig verschieden.


</doc>
<doc id="13711" url="https://de.wikipedia.org/wiki?curid=13711" title="Zellteilung">
Zellteilung

Die Zellteilung oder Cytokinese, auch Zytokinese (von altgr. κύττος "kytos" ‚Zelle‘ und κίνησις "kinesis" ‚Bewegung‘), ist der biologische Vorgang der Teilung einer Zelle. Das Plasma und andere Bestandteile der Mutterzelle werden auf die Tochterzellen aufgeteilt, indem zwischen ihnen Zellmembranen eingezogen oder ausgebildet werden. Dabei entstehen meistens zwei, manchmal auch mehr Tochterzellen.

Bei eukaryotischen Zellen geht einer Zellteilung in den meisten Fällen eine Kernteilung (Mitose) voraus. Zell- und Kernteilungen können aber auch unabhängig voneinander stattfinden, zum Beispiel bei der Endoreplikation, wo sich nach einer Kernteilung die Zelle nicht teilt. Die Kernteilung oder "Karyokinese" wird daher von der Zellteilung oder "Zytokinese" unterschieden.

Da in vielen Eukaryoten die Tochterzellen Kopien aller wesentlichen Zellbestandteile erhalten müssen, ist die Zellteilung stark reguliert. Im Speziellen muss sichergestellt sein, dass das Genom vollständig repliziert wurde. Bei Organismen mit Zellkernen, den Eukaryoten, ist die Zellteilung in der Regel mit einer direkt zuvor stattfindenden Kernteilung (Mitose oder Meiose) zeitlich und regulatorisch gekoppelt. Die Zellteilung kann dabei schon eingeleitet werden, während die Kernteilung durchgeführt wird. Kernteilung und Zellteilung werden zum Zellzyklus zusammengefasst.

Zellen, die sich im Zellzyklus befinden, bei denen sich also Zellwachstum und Zellteilung fortwährend abwechseln, werden als proliferierend bezeichnet. Die Anzahl der Zellteilungen pro Zeiteinheit ist die Teilungsrate. Sie ist für den jeweiligen Zelltyp spezifisch. Bei einzelligen Lebewesen entspricht die Zeitdauer zwischen zwei Teilungen der Generationszeit.
Zellen von Eukaryoten, die sich nach Differenzierung nicht mehr teilen, werden als "postmitotisch" bezeichnet, so etwa Neuronen.

Beispiele für eine Zellteilung, die nicht Teil des normalen Zellzyklus ist, sind Knospung und Schizogonie.

Da die Prokaryoten, zu denen die Bakterien und Archaea zählen, keinen Zellkern besitzen, findet hier keine Mitose statt. Hier heften sich die Bakterienchromosomen nach der Replikation an die Zellmembran, und über eine Einschnürung dieser Membran folgt eine Teilung, durch die zwei Tochterzellen entstehen. Diese sind meist identisch in Größe und Gestalt. Bei manchen Arten erfolgt die Zellteilung jedoch durch Knospung (auch: Sprossung), so dass eine kleine Tochterzelle, die Knospe, entsteht und eine größere, die den Hauptteil der ursprünglichen Zelle behält.

Bei den Eukaryoten beginnt die Zellteilung gewöhnlich während der späten Phasen der Kernteilung, also der Anaphase oder der Telophase (siehe Abbildungen). Sie muss aber nicht im direkten Anschluss an eine Mitose oder Meiose erfolgen. Auch eine erneute Replikation des Erbguts, also der DNA, kann in bestimmten Fällen ohne zwischengeschaltete Zellteilung stattfinden, etwa bei Polytänchromosomen.

Bei tierischen Zellen kommt es bei der Teilung in zwei Tochterzellen zur Bildung eines kontraktilen Ringes in der Höhe der Metaphaseplatte: die Zellmembran wird zwischen den Tochterkernen nach innen gezogen. Der kontraktile Ring besteht aus Aktin- und Myosinfilamenten. Die Kontraktion verläuft analog zur Muskelkontraktion über den sogenannten molekularen Ruderschlag, bei dem sich die Filamente gegeneinander verschieben.

Bei der Fruchtfliege "Drosophila melanogaster" finden sich Ausnahmen von der Regel, dass auf eine Verdopplung des Genoms eine Zellteilung folgt. Am Beginn der Embryonalentwicklung kommt es zunächst zu einer raschen Abfolge von synchronen mitotischen Kernteilungen, ohne dass sich zwischen den Kernen Zellmembranen ausbilden. Die Kerne wandern an die Oberfläche, es bildet sich ein „synzytiales Blastoderm“. Synzytium bezeichnet eine vielkernige Zelle. Nach einigen weiteren Kernteilungen werden schließlich Zellmembranen zwischen den Kernen ausgebildet und die nächste Entwicklungsphase, die Gastrulation, beginnt. In den Larven der Fliege kommt es zur Ausbildung von Polytänchromosomen, bei denen eine Vervielfachung des Genoms innerhalb eines Zellkerns stattfindet.

Nicht alle Synzytien entstehen durch Kernteilungen ohne Zellteilungen. Beispielsweise Muskelfasern entstehen durch die Fusion einkerniger Zellen unter Erhaltung aller Kerne.
Bei pflanzlichen Zellen erfolgt die Cytokinese, indem eine neue Zellwand gebildet wird. Dies geschieht durch Verschmelzung von Golgi-Vesikeln in der Teilungsebene von innen nach außen fortschreitend über eine vesikuläre Zwischenstufe, dem Phragmoplasten. Parallel zur Zellwand wird dabei eine neue Zellmembran angelegt. In beiden bleiben jedoch kleine Lücken, die Plasmodesmen, erhalten, durch welche alle Zellen der Pflanze im sogenannten Symplasten miteinander verbunden bleiben und eine Stoffverteilung durch alle Zellen hindurch möglich ist.

Entsprechend der großen Vielfalt der Pilze kommen hier unterschiedliche Zellteilungsmechanismen vor. Bei der Bäcker- und Bierhefe "Saccharomyces cerevisiae", auch Sprosshefe genannt, entsteht eine Tochterzelle durch Sprossung aus der Mutterzelle. Bei der Spalthefe "Schizosaccharomyces pombe" erfolgt die Teilung dagegen durch Spaltung in zwei gleich große Zellen. Beim Schleimpilz "Dictyostelium discoideum" schnürt ein kontraktiler Ring die gleich großen Tochterzellen voneinander ab, ähnlich wie bei tierischen Zellen (siehe Abbildung).
Die Begriffe antiklin und periklin beschreiben in der Entwicklungsbiologie die Orientierung einer Zellteilung zur nächsten Oberfläche des Organs, in dem diese Zellteilung stattfindet. Zellteilungen, die senkrecht zur nächsten Oberfläche erfolgen, nennt man "antiklin". Findet die Zellteilung parallel zur Oberfläche statt, so bezeichnet man diese als "periklin".

Als Begründer der modernen Embryologie gilt der deutsche Arzt Robert Remak. Er beschrieb 1842 die drei Keimblätter Ektoderm, Mesoderm und Endoderm. Er erkannte vor Rudolf Virchow und Theodor Schwann den Zellkern als Grundstruktur der Zellteilung. Remak beschrieb die Grundstruktur des Axons und das Remak-Ganglion. Später arbeitete er auf dem Gebiet der Galvanotherapie.


Zellproliferation


</doc>
<doc id="13713" url="https://de.wikipedia.org/wiki?curid=13713" title="Genealogie">
Genealogie

Genealogie (von altgriechisch "genealogía" „Geschlechtsregister, Stammbaum“; zurückgehend auf "geneá" „Geburt, Abstammung, Sippschaft, Familie“ und "lógos" „Lehre“) bezeichnet im engeren Sinne die historische Hilfswissenschaft der Familiengeschichtsforschung, allgemeinsprachlich Ahnenforschung (Thema dieses Artikels). Genealogen oder Familienforscher befassen sich mit menschlichen Verwandtschaftsbeziehungen und ihrer Darstellung. Verallgemeinernd wird als Genealogie einer Person oder Familie die Auflistung ihrer namentlich bekannten Vorfahren verstanden.

Im weiteren Sinne bezeichnet Genealogie den genetischen Zusammenhang einer Gruppe von Lebewesen, die biologische Abstammung eines Lebewesens von anderen Lebewesen; in der Tierzucht ist sie die Voraussetzung für eine Abstammungsbewertung. Von dieser Bedeutung abgeleitet wird auch von Genealogien in ideengeschichtlichen Zusammenhängen gesprochen, so erfasst das "Mathematics Genealogy Project" die veröffentlichten Doktorarbeiten in der Mathematik.

Im übertragenen Sinne wird in den Geisteswissenschaften unter "Genealogie" eine historische Methode verstanden, welche die geschichtliche Entwicklung verschiedener Sachverhalte der Gegenwart untersucht. So rekonstruierte beispielsweise 1887 der deutsche Philosoph Friedrich Nietzsche in seiner "Genealogie der Moral", dass sich Moralvorstellungen nicht aus absoluten Werten herleiten, sondern etwas im Laufe der Geschichte Gewordenes sind. Für den französischen Soziologen Michel Foucault (1926–1984) war "Genealogie" ein zentraler Begriff in seinen Entwicklungsanalysen von psychischer Krankheit oder des Gefängniswesens.

Von einer bestimmten Person als "Ego" „Ich“ oder "Proband" „Testperson“ ausgehend, erforscht die Genealogie in aufsteigender Linie die Abstammung ihrer Vorfahren (Ahnen, daher „Ahnenforschung“), und in absteigender Linie deren Nachkommen. Personen, die genealogisch miteinander verknüpft sind, gehören zu einer Verwandtschaft. Sobald die Beschreibung der Zusammenhänge über die reine Darstellung der Abstammung hinausgeht, spricht man von „Familiengeschichtsforschung“ – zum Beispiel mit dem Ziel, die Lebensumstände entfernter Vorfahren herauszufinden.

Während die Genealogie oft aus persönlichem oder familiärem Interesse und als Freizeitbeschäftigung betrieben wird, ist die Genealogie bei der Ermittlung von Erben von Bedeutung. Während manche Länder Erbansprüche auf die Großeltern und deren Nachfahren beschränken, sind nach deutschem Erbrecht Erbansprüche möglich, falls der gemeinsame Vorfahre im Mittelalter oder sogar früher gelebt hatte.


Da die Genealogie ein Teilgebiet der geschichtlichen Forschung darstellt, werden häufig auch weitere verwandte oder naheliegende Bereiche wie Namen- und Wappenkunde, Heimat- und Militärgeschichte, Kriegsgräber, aber auch Verwandtschaftsgrade behandelt.

Ein selbstständiger Bereich der Genealogie ist die Namenforschung zur Herkunft, Verbreitung und Bedeutung von Familiennamen.

Das Interesse an der Genealogie erwacht meist an der eigenen Familie. Man beginnt mit Fragen an Eltern, Großeltern und Verwandte nach familiären Zusammenhängen und der Herkunft der Vorfahren. Familienbücher, Familienfotos und ein möglicherweise noch vorhandener Ahnenpass liefern weitere Informationen. In einigen Regionen gibt es auch schon seit Jahrzehnten die Tradition der Sterbebildchen oder Totenzettel, die sich hervorragend für die Ahnenforschung eignen, da sie oft neben einem Foto des Verstorbenen auch Geburts- und Sterbedaten sowie weitere Informationen (Namen von Verwandten, Geburtsname, Hinweise auf die Art des Todes) enthalten. Außerdem wird man, insbesondere in den letzten Generationen, auch auf dem Friedhof fündig. Auf den Grabsteinen stehen häufig ebenfalls weitere Daten. Fotos, urkundliche Belege und Dokumente sowie die Biografien und Lebensbilder der Großeltern, Urgroßeltern und weiterer Verwandter sind der Grundstock für eine Familienchronik.

Die weitere Forschung erfordert allerdings die Beschäftigung mit den Quellen. Hierzu ist Fachwissen nötig, das sich jeder Genealoge im Laufe seiner Forschungstätigkeit aneignet.
In diesem Zusammenhang wurde auch auf die Tücken personengeschichtlicher Forschungen zum Mittelalter hingewiesen und „an den zum Teil etwas kühnen Hypothesen über Verwandtschaftsbeziehungen (…) deutliche Kritik geäußert.“

Die Forschung an älteren Quellen wie den Kirchenbüchern oder Gerichtsbüchern erfordert die Fähigkeit des Lesens alter Schriften (siehe Paläografie) und in katholischen Gebieten zumeist Lateinkenntnisse. Veränderlichkeit der Familiennamen und ein ausgedehnter Heiratskreis der zu erforschenden Personen sind zu berücksichtigen. Die Forschung gelangt bisweilen an den sogenannten Toten Punkt, den es zu überwinden gilt. Mit der Verdopplung der Zahl der Vorfahren in jeder Generation weitet sich das Bild von der persönlichen Ahnenschaft aus zu Themen wie Heimatgeschichte, Sozialgeschichte, Wirtschaftsgeschichte und Bevölkerungsgeschichte ganzer Orte (siehe Ortsfamilienbuch) oder Regionen.

Anstatt der eigenen können auch die Vorfahren und Nachkommen historischer Persönlichkeiten oder herausragender Vertreter bestimmter Berufsgruppen erforscht werden. In einem reiferen Stadium kommt der Forscher zu einer immer größeren Genauigkeit und Detailliertheit bei der Erfassung der Daten. Beispielsweise kann man die Geschwister der Vorfahren einbeziehen, ihre Ehepartner, ihre Kinder und die soziale Stellung ihrer jeweiligen Schwiegereltern, wodurch wissenschaftliche Sekundäranalysen der Daten sinnvoll und besonders aussagekräftig werden.

Ein wichtiges Qualitätsziel einer weitgehend von Laienforschern betriebenen Datenerhebung und -darstellung in der Genealogie besteht darin, die Forscher so weit mit wissenschaftlichen Standards zu versehen und zu motivieren, dass die erhobenen Daten den Kriterien der Qualität und Wissenschaftlichkeit gerecht werden, in den wissenschaftlichen Diskurs eingegliedert (Publikation, Darstellung, evtl. Internet) und in einen historischen Kontext gestellt werden können.

Mit dem Boom des Internets hat parallel auch die Genealogie einen starken Aufschwung erfahren. Durch das Medium Internet können weltweite Kontakte zwischen Forschern schnell und kostengünstig hergestellt werden. In genealogischen Datenbanken im Internet sind heute viele Millionen erforschter Ahnentafeln und Stammbäume zu finden. Mit GEDCOM hat sich zudem ein Standard für die Abbildung und Strukturierung von genealogischen Daten gebildet, der von einer Vielzahl genealogischer Programme unterstützt wird.

Bei einem Teil der Genealogen wird die Haltung beobachtet, diese Arbeitsweise sei die Genealogie an sich. So wird dabei teils vernachlässigt, dass nur durch gründliche Arbeit an den Quellen das Material für derartige Datenbanken entsteht.

Einige amerikanische und auch deutsche Firmen nutzen das Thema Ahnenforschung dazu, kostengünstig personenbezogene Daten zu ermitteln. Nutzer von Web-Portalen geben etwa Adressen und Geburtsdaten über ihre Verwandten ein – die jedoch im Zuge des viralen Marketings oder von Affiliate-Netzwerken missbraucht werden können. Personenbezogene Daten werden so in ungewöhnlich großer Menge über lebende und verstorbene Personen vermarktbar. Das Datenschutzrecht greift hier häufig nicht, wenn etwa der Nutzer in den Geschäftsbedingungen der grenzüberschreitenden Verarbeitung zugestimmt hat; somit das deutsche Recht nicht anwendbar ist.

Da wissenschaftliche Forschungen bei vielen Fragen der Repräsentativität bedarf, galten genealogische Quellen lange Zeit als ungeeignet. Beispielsweise in den Arbeiten von Jacques Dupaquier zur Sozialgeschichte Frankreichs wurden jedoch repräsentative Stichproben erhoben, wobei sich Dupaquier auf Stammlisten stützte.

Wissenschaftlichkeit der Arbeitsmethoden bedeutet auch für die Genealogen die Objektivität der Forschung, unabhängig von der Person, die sie betreibt. Abstammungen gelten nur dann als belegt, wenn andere Forscher, die von den vorhandenen Quellen ausgehen, zu denselben Ergebnissen gelangen müssen. Bestehen Zweifel und Unsicherheiten, so sind diese in den Ahnenlisten als solche zu kennzeichnen. Errechnete Werte oder bloße Vermutungen müssen als solche erkennbar sein.

Auch etablierte akademische Disziplinen besitzen in der Regel keine ständigen Kontrollgremien, sondern setzen das Streben nach Wahrhaftigkeit aller Forscher voraus. Das Kriterium, das den Forscher vom Phantasten (etwa beim unbekannten Vater für ein uneheliches Kind) oder gar Betrüger trennt, ist die Wiederholbarkeit des Abstammungsnachweises durch andere Forscher. Sorgfältigeres Arbeiten, etwa durch die Einbeziehung neuer, bisher unbekannter Quellen und Methoden (siehe auch Vaterschaftsgutachten) kann dabei in Einzelfällen durchaus zu Revisionen bisher als ausreichend belegt geltender Abstammung führen.

Zwischen der Begriffsgeschichte und der Genealogie besteht eine gegenseitige Beziehung, die bisher wenig beachtet wurde. Denn Sprache und Begriffe sind in Raum und Zeit veränderlich, über die sich genealogische Forschungen erstrecken. Familiennamen, Ortsnamen, Flurnamen, Berufsbezeichnungen, Verwandtschaftsbezeichnungen, Rechtsbegriffe und volkskundlich wichtige Begriffe – einschließlich der Formeln, mit denen die Pfarrer vorehelichen Geschlechtsverkehr und uneheliche Geburt brandmarkten – sind in guten Ahnenlisten zu Tausenden enthalten. Kartiert man zum Beispiel aus hunderten solcher Listen die Bezeichnungen der Berufe, Jahrzehnt für Jahrzehnt getrennt, dann lässt sich die regionale Verbreitung, etwa für die Bezeichnung von Bauern und der Begriffswandel belegen, was wiederum die Voraussetzung für richtige Zuordnungen der Sozialgeschichte ist.

Der Genealoge kann dazu beitragen, die Aussagekraft seiner Arbeiten zu erhöhen, indem er Angaben zu verschiedenen Schreibweisen von Familiennamen und zu Berufen in seinen Arbeiten quellengetreu wiedergibt und nicht modernisiert oder zu stark generalisiert. Dazu gehört etwas heimatgeschichtliche Erfahrung und Fingerspitzengefühl: „Bäcker“ oder „Becker“ zu unterscheiden, ist fast bedeutungslos, „Fleischer“ von „Fleischhauer“ aber sprach- und begriffsgeschichtlich bedeutsam und die Grenze zwischen „Wagner“ und „Stellmacher“ trennt sogar Mundarten-Räume.

Familienbeziehungen können mit Hilfe von Genogrammen veranschaulicht werden.

Der Beginn des 20. Jahrhunderts war von der naiven Vorstellung geprägt, dass mit genealogischen Daten ein direkter Beitrag zu leisten wäre, die Vererbung zahlreicher Merkmale zu klären („Genetische Genealogie“). Man nahm einfach vorgegebene sprachliche Ganzheiten für psychische Variablen, etwa „Ehrgeiz“ und „Gutgläubigkeit“, so wie man „blondes Haar“ und „blaue Augen“ nahm, und untersuchte den Erbgang von „Ehrgeiz“ und „Gutgläubigkeit“.

Durch diese Methoden konnten keine seriösen Ergebnisse erzielt werden, da die Auswirkungen der Erziehung und anderer Umwelteinflüsse auf die Entwicklung psychischer Eigenschaften außer Acht gelassen wird. Nur wenige, zumeist monogene Merkmale (wie etwa die Bluterkrankheit) folgen einem auch genealogisch nachvollziehbaren Erbgang. Bei vielen komplexeren (polygenen) Sachverhalten hat es sich als schwierig oder bisher unmöglich erwiesen, einzelne Genwirkungen zu erkennen.

Meist ist der Genealoge nicht nur Kenner der Heimatgeschichte bestimmter Gebiete, sondern erfasst bei seiner Tätigkeit ein lebendiges Geschichtsbild und erkundet das historische Erbe. Fast in jeder Ahnenliste häufen sich die Ahnen im 16. bis 18. Jahrhundert in bestimmten Gemeinden, ja stellen in manchen Dörfern einen beträchtlichen Prozentsatz der Einwohnerschaft. Für die Einordnung und Bewertung der Berufe, der Kaufpreise der Güter und Häuser oder der landschaftsgebundenen Begriffe wird damit ein heimatgeschichtliches Grundwissen unentbehrlich. In vielen Fällen ist die bereits vorhandene heimatgeschichtliche Literatur (Chroniken; Beilagen der Tageszeitungen; Reihe Werte unserer deutschen Heimat) eine wertvolle genealogische Quelle, in anderen Fällen bearbeitet gerade der Genealoge das Ortsfamilienbuch, die Ortschronik oder erarbeitet heimatgeschichtliche Beiträge und Lebensbilder. Heimatgeschichte verbunden mit Genealogie und mit persönlichem Bezug zur Gegenwart, ist keine abstrakte Geschichte. Durch die Verbindung von Personen, Ereignissen, Daten, Häusern und den Lebensumständen der Vergangenheit mit ihren sozialen Konflikten und Kämpfen, oft auch unter Einbeziehung von Herkunftssagen entsteht ein umfassendes Bild.

Mitteleuropa gehört zu denjenigen Teilen der Welt, in denen seit dem 16. Jahrhundert in Form der Kirchenbücher und der Gerichtshandelsbücher, seit Ende des 18. Jahrhunderts auch in Form der Personenstandsbücher, geeignete Quellen für die Familiengeschichtsforschung vorhanden sind, in denen die Hauptlebensdaten für jede Person nachgewiesen werden können, sofern die entsprechenden Quellen nicht vernichtet worden sind.

Weitere wichtige Quellengruppen der Genealogie sind zum Beispiel Bürgerbücher, Leichenpredigten und Personalschriften, Universitätsmatrikel, Pfarrerverzeichnisse, Testamente und andere Akten, aus denen die verwandtschaftliche Stellung der Personen zueinander oder wenigstens – damit sich der Tote Punkt der Nachforschungen überwinden lässt – ihr Heimatort erkennbar ist, wie beispielsweise die Passagierlisten der Auswandererschiffe aus dem 19. und 20. Jahrhundert und die Musterungslisten. Eine weitere Quellengruppe sind Listen und Akten, die die Existenz von Personen an einem bestimmten Ort und zu einer bestimmten Zeit nachweisen und ihre soziale Stellung, wie zum Beispiel Steuerlisten und Adressbücher. Oftmals sind diese und andere Quellen nur für bestimmte Bevölkerungsgruppen vorhanden, wie der sozialen Oberschicht.

Auf der Grundlage der bereits genannten und weiterer Quellen sind dann Hilfsmittel erarbeitet worden: Karteien, Dateien und Bücher. Dazu gehören die Ortsfamilienbücher, Häuserbücher, Güterchroniken und Dienerbücher, aber auch die Ahnenstammkartei des deutschen Volkes.

Mit Hilfe der Internet-Technologie werden viele dieser Quellen nach und nach in Online-Genealogie-Datenbanken veröffentlicht.

Kirchenbücher befinden sich in den Pfarrarchiven der jeweiligen Kirchgemeinde und Glaubensgemeinschaft. In einigen Territorien sind die Originale der Kirchenbücher oder ihre Kopien und Verfilmungen in zentralen Archiven konzentriert und dort für die Nutzung zugänglich. Diese zentralen Archive können kirchliche oder staatliche Archive sein, im zuständigen Bistum, wie beispielsweise in Münster, im zuständigen Landeskirchenarchiv, wie zum Beispiel in Kassel, oder auf Grund einer Vereinbarung mit der Kirche im Landesarchiv, wie beispielsweise in Innsbruck für Tirol, in den Archiven der Schweizer Kantone und im Elsass. Die jeweilige Zuständigkeit und den Lagerungsort gilt es in jedem Falle zu ermitteln.

Gerichtshandelsbücher und andere wichtige Quellen sind in den zuständigen Staatsarchiven zu finden, weitere Quellengruppen in den Stadtarchiven. Seit 1875 werden in Deutschland Personenstandsbücher in den Standesämtern geführt.

Das mit Abstand größte genealogische Archiv wird von der 1894 gegründeten genealogischen Gesellschaft von Utah unterhalten. Die Erforschung der Familiengeschichte hat innerhalb der Kirche Jesu Christi der Heiligen der Letzten Tage (→ Mormonen) nicht nur eine wichtige religiöse Bedeutung (siehe Totentaufe). Deshalb archiviert die Genealogische Gesellschaft von Utah Kirchenbücher und andere genealogisch wichtige Dokumente einerseits auf Mikrofilm und andererseits mittlerweile auch auf digitalen Medien. Die Kirchenbuch-Filme können in vielen familien-genealogischen Zentren auf der ganzen Welt öffentlich eingesehen werden; auch über das Internet sind Personendaten (von bereits verstorbenen Personen) und Verwandtschaftsverhältnisse einsehbar.

Zahlreiche Kirchenbuchverfilmungen, vor allem aus den früheren deutschen Ostgebieten, sind auch in der Zentralstelle für deutsche Personen- und Familiengeschichte Leipzig zu finden.

Unter anderem die evangelischen landeskirchlichen Archive stellen Kirchenbücher inzwischen zentral über die Internetplattform Archion zur Verfügung.

Die Forschungsergebnisse werden in "genealogischen Tafeln" dargestellt, die sowohl mit aufsteigenden (Aszendenz, Vorfahren) als auch absteigenden (Deszendenz, Nachfahren) Inhalten auftreten. Bei beiden Richtungen ist sowohl die Form einer Tabelle als auch die einer Liste möglich. Bei der aufsteigenden Linie wird von Ahnentafel oder Ahnenliste, bei der absteigenden von Nachkommentafel oder Nachkommenliste gesprochen. Eine Kombination beider Tafeln, bei denen alle Vor- und Nachfahren einer ausgewählten Person aufgezeigt werden, werden auf Grund ihrer Form im Allgemeinen auch „Sanduhr“-Tafeln genannt.

Werden nur die Nachkommen einer Person erfasst, die den gleichen Familiennamen tragen oder einmal trugen oder mit diesen Personen verheiratet waren (wobei ein stringentes Durchhalten dieser Regel, zum Beispiel aufgrund von Namensänderungen, Adoption, ausländischem Namensrecht und anderem, nicht immer möglich ist), so ist es eine Stammtafel oder Stammliste. In Nachschlagewerken ist der Familienname Sortierkriterium und somit die Stammtafel oder Stammliste die natürliche Darstellungsform, ebenso in „Familiengeschichten“. In Monographien, die eine bestimmte Person und deren Nachkommen behandeln, herrschen Nachkommentafeln und -listen vor.

Ob bei der Darstellung genealogischer Ergebnisse die Tabellen- oder Listenform gewählt wird, hängt unter anderem davon ab, wie umfangreich das Datenmaterial ist und wie übersichtlich es dargestellt werden soll. Grundsätzlich gilt, je mehr Generationen darzustellen sind, umso eher bietet sich die Listenform an.

Über die Darstellung alleine der Vorfahren oder Nachkommen hinaus sind bekannt:

Die verwandtschaftlichen Zusammenhänge der Einwohner eines Ortes werden in einem Ortsfamilienbuch dargestellt; nur auf die Hausbesitzer beschränkt, in einem Häuserbuch.

Die Sicherung verlangt die dauerhafte, der öffentlichen Benutzung zugängliche Aufbewahrung von Forschungsergebnissen. Von allen im 20. Jahrhundert von Genealogen erarbeiteten Materialien (Ahnenlisten, Kirchenbuchverkartungen) dürfte die Hälfte inzwischen wieder vernichtet und verloren sein. Beim gegenwärtigen Stand des rechnergestützten Druckes und der jedermann zugänglichen Kopiertechnik sollte das heute kein Problem mehr sein.

Wenn keine Drucklegung der Arbeit in einer Zeitschrift oder Buchreihe sinnvoll oder möglich ist, sollten von jeder genealogischen Arbeit mindestens ein halbes Dutzend Ausdrucke und Kopien des Originals angefertigt werden. Zwei davon soll und muss die Deutsche Bibliothek (die für derartige Einsendungen auch Geldmittel zur teilweisen Kostenerstattung zur Verfügung hat) erhalten, ein Exemplar gehört in die zuständige Landesbibliothek des jeweiligen Bundeslandes, eines in die Zentralstelle für deutsche Personen- und Familiengeschichte Leipzig, weitere Exemplare in das regional zuständige Staatsarchiv, das zuständige Pfarramt (bei einem Ortsfamilienbuch) und in mindestens eine wichtige regionale Wissenschaftliche Bibliothek und ein Stadtarchiv. Auf dem Titelblatt sollte rechts oben dieser Verteilungsschlüssel der Standorte angegeben werden. Werden derartige, nicht im Buchhandel erhältliche Arbeiten zitiert, dann sollte stets der Standort angegeben werden.

Im Nachlass sollten geeignete (d. h. geordnete und mit Quellenverzeichnis versehene) Materialien durch klare, zu Lebzeiten getroffene, schriftliche Festlegungen an Archive, Museen oder Bibliotheken übergeben werden. Nach allen Erfahrungen gehen im privaten Besitz (bei den leiblichen Erben) verbliebene Materialien der öffentlichen Benutzung und damit der weiteren Forschung häufig völlig verloren. Auch Karteien, selbst wenn sie in Archive gelangen, sind als Unikate nicht gegen Unordnung und Diebstahl einzelner Karten gesichert. Ihre Benutzung ist an einen einzigen Standort gebunden und damit erschwert. Auch hierfür ist ein zusammenhängendes Manuskript mit mehreren Ausdrucken die sicherste Lösung. Nur auf diese Weise wird die immense Arbeit für die weitere Forschung nutzbar. Karteien, die als ungeordneter Nachlass in irgendein Archiv gelangen, bleiben erfahrungsgemäß oft für Jahrzehnte unauffindbar und praktisch verloren.

Sicherstellung heißt nicht nur Aufbewahrung, sondern vor allem auch Gewährleistung der weiteren öffentlichen Benutzung, die ja für den Genealogen auch die Voraussetzung seiner eigenen Arbeit war.

Bei der Sicherung wichtiger Dokumente und Familienstammbäume bietet die genealogische Gesellschaft in Utah der Kirche Jesu Christi der Heiligen der Letzten Tage (→ Mormonen) Hilfestellung. Über die entsprechenden örtlichen Forschungsstellen oder das Internet lassen sich diese Daten kostenlos digital archivieren. Diese Daten sind nach einer bestimmten Zeit der Bearbeitung weltweit einsehbar. Auch hier gilt, dass nur Daten von verstorbenen Personen einsehbar gemacht werden können.

Zum Thema Sicherstellung der genealogischen Ergebnisse europäischer Adelsfamilien wurde eine spezielle genealogische Datenbank, die sogenannte "WW-Person", angelegt.

Der deutsche Geograph und Universalgelehrte Johann Gottfried Gregorii betrachtete ganz im Zeitgeist des beginnenden 18. Jahrhunderts die Genealogie als Hilfswissenschaft von Geschichte und Geographie und veröffentlichte zwischen 1715 und 1733 seine fünfbändige genealogische Beschreibung des europäischen Adels unter dem Titel: "Das jetzt lebende EUROPA". Die mit den Homannschen Erben verbundene Kosmographische Gesellschaft schrieb dazu 1750: „Ein Weltbeschreiber muß die Genealogie und Wappenkunst inne haben“, und: „Die Genealogie enthält den Grund der meisten Veränderungen der Herrschaften und der daher rührenden Landabtheilungen.“

, sagte bereits der Historiker Johann Christoph Gatterer (1727–1799), der 1788 einen "Abriss der Genealogie" veröffentlichte. In den alten Hochkulturen war die Genealogie der Helden und Könige die Form der Geschichtschronologie schlechthin (man denke an die ersten Kapitel der Bibel). Die frühe mittelalterliche Genealogie war vor allem eine Geschichte der Stammreihen des Hochadels. Der Adel insgesamt brauchte den Nachweis der Abstammung, um Besitzansprüche geltend zu machen oder die Qualifikation für bestimmte Ämter nachzuweisen.

Erst an der Wende zur Neuzeit begannen auch wohlhabende bürgerliche Geschlechter damit, ihre Ahnen aufzuschreiben. Die Zünfte verlangten von jedem Auswärtigen, der ein Handwerk in der Stadt erlernen oder ausüben wollte, einen Geburtsbrief. Mit den Vereinen "Der Herold" (Berlin 1869) und "Der Adler" (Wien 1870) entstanden die ersten genealogischen Vereine für Heraldik und Genealogie. 1902 wurde "Der Roland" in Dresden als erster bürgerlicher Verein der Welt gegründet.

Parallel dazu entwickelte sich die Abstammungsbewertung in der Tierzucht. Seit dem 18. Jahrhundert werden Stammbücher zum Beispiel auch für Rennpferde geführt, später gefolgt von den Herdbüchern zahlreicher Nutztier-Rassen.

Um die Wende zum 20. Jahrhundert begann die eigentliche Entwicklung der Genealogie in Breite und Tiefe. Die "Gothaischen Genealogischen Taschenbücher" ("Almanach de Gotha", kurz: "Der Gotha"), die ursprünglich schon seit 1763 als "Genealogischer Hofkalender" in Gotha erschienen und von 1785 bis 1944 vom Verlag Justus Perthes in Gotha herausgegeben wurden, öffneten sich nunmehr auch für bürgerliche Familien und gaben deren Herkunft an, zum Teil aus bäuerlicher und anderer Wurzel. 1904 wurde in Leipzig die Zentralstelle für Deutsche Personen- und Familiengeschichte gegründet. 1913 erschien das "Handbuch der praktischen Genealogie". In dieser Pionierzeit war die junge Genealogie von zukunftsweisenden und interdisziplinär denkenden Persönlichkeiten geprägt, die die Genealogie in den Dienst der Sozialwissenschaften stellen wollten. In der weitgehend auf Amateurforschung beruhenden Genealogie blieb jedoch die Resonanz auf diese Anregungen gering.

In den zwanziger Jahren begann der Anthropologe Walter Scheidt mit seinen Mitarbeitern, Kirchenbücher populationsgenetisch auszuwerten, wozu er die Mitarbeit von Genealogen suchte. Von mehreren Pfarrern angeregt, begann parallel dazu unter dem Stichwort „Volksgenealogie“ eine Arbeitsrichtung zu entstehen, die nicht mehr nur die Genealogie der begüterten Schichten im Auge hatte, sondern der gesamten Bevölkerung.

Karl Förster (1873–1931) hatte die Notwendigkeit erkannt, die genealogische Laienforschung besser zu organisieren und Daten für Forschungszwecke zentral zu sammeln. Bereits 1921 hatte er den Ahnenlistenumlauf gegründet, dessen Daten in die "Ahnenstammkartei des deutschen Volkes" eingearbeitet wurden. Vor 1933 gab es im deutschen Sprachraum bereits eine große Zahl regionaler genealogischer Vereine und Zeitschriften. In ihren Vorträgen und Publikationen waren Schlagworte wie Vererbung, Rasse und Heimat verbreitet.

Ab 1933 versuchte die nationalsozialistische Politik zielstrebig, die genealogischen Vereine gleichzuschalten, und die Genealogie wurde in den Dienst der Blut-und-Boden-Ideologie und des Antisemitismus gestellt. Das Berufsbeamtengesetz verlangte den Nachweis der so genannten arischen Abstammung (zum Beispiel durch den Ahnenpass), und die Genealogie wurde zur Sippenforschung. Die Kirchen erhielten den Auftrag, zu ermitteln, welche Juden im 19. und 20. Jahrhundert zum Christentum konvertiert waren und sich hatten taufen lassen. Mit Hilfe entsprechender Informationen konnten die Nachkommen der Täuflinge „als Juden entlarvt“ werden. Allein in der evangelischen Kirche Schleswig-Holsteins waren in 17 Kirchenämtern rund 150 Angestellte tätig, die täglich recherchierten, Abstammungsnachweise ausstellten und Register erstellten. Auf dem Gebiet der damaligen Nordelbischen Kirche wurden mithilfe der kircheneigenen Ahnenforschung 7731 Christen jüdischer Herkunft identifiziert, ausgesondert und getötet. 1939 lief in 3.000 Gemeinden Deutschlands die Arbeit an Dorfsippenbüchern. 

1934 wurde in München das Kaiser-Wilhelm-Institut für Genealogie und Demografie gegründet, in dem eine Reihe Arbeiten über die Erbgänge psychischer Erkrankungen, aber auch die Genealogie von Hochbegabungen fertiggestellt wurden. Das hatte die Folge, dass 1945 fast die gesamte organisatorische Basis der Genealogie aufgelöst wurde.

Bis 1945 hatte die Entwicklung der sachlichen Bezüge der Genealogie zur Bevölkerungsgeschichte, Wirtschaftsgeschichte und Sozialgeschichte im deutschen Sprachraum einen zeitlichen Vorsprung. Um 1950 hatten die Genealogen in Deutschland und Österreich begonnen, alte Vereine, Verlage und Zeitschriften aus der Zeit vor 1933 zu reaktivieren oder neue zu gründen. 1969 wurde in der DDR in Magdeburg eine erste Arbeitsgemeinschaft Genealogie im Verbund des Kulturbundes gegründet.
Obwohl seit 1929 „Internationale Kongresse für Genealogie“ stattfinden, hat es der betont regionale und nationalsprachliche Charakter der Quellen bisher verhindert, dass es zur Entwicklung einer international und theoretisch umfassenden Genealogie gekommen ist. Zweifellos bewirkt aber die Entwicklung von genealogischen Computerprogrammen eine zunehmende Internationalität.
Nach 1945 gingen neue Anstöße aus von Frankreich, den Niederlanden, Schweden, Großbritannien und den USA, wo sich die Familiengeschichtsforschung in den letzten Jahrzehnten zu einer weit verbreiteten Freizeitbeschäftigung entwickelt hat.

In den USA war insbesondere John Farmer (1789–1838) führend.
Zuvor dienten den amerikanischen Kolonisten Ahnentafeln dazu, ihre soziale Positionierung innerhalb des Britischen Empires nachzuweisen. Farmer vertrat ein stärker egalitäres, republikanisches Ethos. Die amerikanische Genealogie diente nun zunehmend dazu, Bezüge zu den Gründervätern der Vereinigten Staaten und Helden des Amerikanischen Unabhängigkeitskriegs hervorzuheben. Eine wichtige weibliche Bezugsperson war die Indianerin Pocahontas, deren zahlreiche Nachfahren größtenteils Mitglieder der weißen Oberschicht wurden, auf die bis zum heutigen Tag viele Vertreter der „ersten Familien Virginias“ "(FFV)" ihre Abstammung zurückführen. Bereits ihr Taufname Rebekka spielte an auf die ihr zugewiesene Rolle als Erzmutter des nordamerikanischen Neuenglands. Um 1900 wurden diese Bezüge mit etlichen Ausstellungen bedacht, so der "Jamestown Exposition" von 1907, sowie in historischen Gesellschaften wie der Preservation Virginia erforscht. Farmers Anstrengungen führten zur Gründung der New England Historic Genealogical Society (NEHGS), die sich in Neuengland um die Erhaltung von historischen Aufzeichnungen und Familienbüchern engagiert und das "New England Historical and Genealogical Register" herausgibt.

Aus religiösen Gründen hat die Genealogische Gesellschaft von Utah bei der Anwendung des Computers in der Genealogie international eine organisatorische Führungs- und Spitzenrolle eingenommen. Sie wurde 1894 mit dem Ziel gegründet, den Mitgliedern der Kirche Jesu Christi der Heiligen der Letzten Tage (Mormonen) beim Zusammentragen familiengeschichtlicher Angaben zu helfen. Bei den Mormonen sind die stellvertretende Taufe und andere Zeremonien für verstorbene nicht-mormonische Vorfahren Teil der religiösen Praxis. Als gemeinnützige Organisation stellt die Genealogische Gesellschaft aber ihre Einrichtungen und Materialien Familienforschern allgemein zur Verfügung und baut ihre Datenbasis systematisch und weltweit aus.

Im Judentum hat die Genealogie eine besondere Rolle. In der Tora wird Genealogie übersetzt mit "toledot" („Generationen“). Im Hebräischen beziehen sich Bezeichnungen wie "yiḥus" und "yuḥasin" auf die Legitimität oder den Geburtsadel, im modernen Hebräisch "shorashim" („Wurzeln“) oder "genealogi". Bis heute ist den Nachfahren von Leviten und Kohanim sowie verschiedenen Rabbinerfamilien eine besondere Anerkennung verbunden.

Das Judentum ist eine Religionsgemeinschaft, bei der ebenso ein gemeinsamer ethnischer Hintergrund behauptet wird. Das Interesse an Genealogie rührt aus der schriftlichen Überlieferung der biblischen Stammlinien, wie es vor dem Hintergrund einer langen Verfolgungs- und Vertreibungsgeschichte zu sehen ist. Im 20. Jahrhundert führte der Holocaust zu einer verstärkten Rolle der jüdischen Genealogie, weil Überlebende versuchten, vermisste Familienmitglieder zu finden oder das Andenken der Verlorenen zu bewahren. Dazu wurden verschiedene genealogische Einrichtungen gegründet, darunter der Internationale Suchdienst (ITS) in Arolsen, das Search Bureau for Missing Relatives in Jerusalem oder zuletzt die Erstellung der zentralen Datenbank der Namen der Holocaustopfer in der Gedenkstätte Yad Vashem.

Im deutschen Sprachraum gibt es etwa 100, zumeist auf geographische Regionen spezialisierte, genealogische Vereine, von denen die Mehrzahl dem im Jahre 1949 gegründeten Dachverband Deutsche Arbeitsgemeinschaft genealogischer Verbände e. V. (DAGV) angehören, der in der Nachfolge der Arbeitsgemeinschaft deutscher familien- und wappenkundlicher Vereine steht, die 1924 gegründet worden war.

Für überregionale Interessen von allgemeiner Bedeutung und das Thema Computergenealogie im Besonderen ist der Verein für Computergenealogie (CompGen) mit über 3.700 Mitgliedern zuständig. Dieser Verein widmet sich schwerpunktmäßig der Veröffentlichung genealogischer Forschungsergebnisse im Internet. Neben vielen Datenbanken wird mit dem "GenWiki" ein Wiki betrieben, das sich ausschließlich mit Genealogie beschäftigt.

Die Genealogen treten oft Vereinen in den Regionen bei, aus denen ihre Vorfahren stammen. Wohnen sie selbst heute in einem anderen Gebiet, so sind sie häufig Mitglied im genealogischen Verein oder Heimatverein ihres Wohnortes und in dem Verein, der für die Heimat ihrer Vorfahren zuständig ist.

Zu den nach Mitgliederzahl oder bearbeiteter Gegend größten und aktivsten regionalen genealogischen Vereinen für den deutschsprachigen Raum zählen:

Die ehemaligen deutschen Siedlungsgebiete im Osten und Südosten Europas hat die Arbeitsgemeinschaft ostdeutscher Familienforscher e. V. (AGoFF, etwa 1000 Mitglieder) als Forschungsgebiet. Einzelne Teilgebiete werden von eigenen Vereinen bearbeitet, die zum Teil aus der AGoFF hervorgegangen sind oder mit ihr zusammenarbeiten:

Einige Vereine widmen sich den Nachkommen von Flüchtlingen, die wegen religiöser Verfolgungen nach Deutschland gekommen sind:

Es gibt auch einige Vereine im Ausland, deren Mitglieder nach ihren Vorfahren im deutschsprachigen Raum forschen:

Darüber hinaus gibt es auch Familienverbände und Vereine, in denen die Nachkommen einer bestimmten Person, die Träger eines Familiennamens oder zueinander in einer bestimmten Verwandtschaftsbeziehung stehende Personen organisiert sind:





</doc>
<doc id="13714" url="https://de.wikipedia.org/wiki?curid=13714" title="Neotenie">
Neotenie

Neotenie oder Neotänie (gr. "neos" ‚jung‘ und "teínein" ‚strecken‘, ‚ausdehnen‘) bezeichnet in der Zoologie den Eintritt der Geschlechtsreife im Larvenzustand ohne Metamorphose, z. B. bei Schwanzlurchen. Der Begriff wurde 1885 durch den Zoologen Julius Kollmann bei der Untersuchung von Entwicklungsverzögerungen bei Kaulquappen geprägt.

Später wurde der Begriff im Bereich der Domestikationsforschung für das Phänomen der Verjugendlichung, die Beibehaltung von Jugendmerkmalen, verwendet. 

Als bekanntestes Beispiel gilt der Axolotl, "Ambystoma mexicanum", ein im Xochimilco-See in Mexiko lebender Molch aus der Familie der Querzahnmolche. Diese Art wird bereits in einem späten Larvenstadium – mit entwickelten Beinen, aber noch mit Kiemen – geschlechtsreif. Physiologisch wird die Neotenie durch eine Unterfunktion der Schilddrüse ausgelöst, die genetisch bedingt eine zu geringe Menge an Reifungshormonen herstellt, oder aber durch einen starken Mangel an mineralischem Jod im Wasser. Füttert man Axolotl mit solchen Hormonen, reifen sie zu erwachsenen Tieren heran und begeben sich, wie die nahe verwandten Tigersalamander, an Land.

Neotenie kommt bei vielen Schwanzlurchen unterschiedlicher Familien vor, allerdings gibt es hier verschiedene Ausprägungen dieser Erscheinung. In jodarmen Gebirgsgewässern kommen gelegentlich Dauerlarven von Molchen und Salamandern vor, die sich aber mithilfe von Schilddrüsenhormonen zur Metamorphose bringen lassen. Dieses Phänomen tritt nicht nur bei den heimischen Molchen wie etwa dem Bergmolch auf, sondern auch bei den amerikanischen Querzahnmolchen, zu denen auch der Axolotl gehört. Dagegen sind einige andere Arten unabhängig vom Jodgehalt des Wassers zu einer Lebensweise als Dauerlarven übergegangen, die auch durch Hormongaben nicht dazu zu bewegen sind, eine Umwandlung zum Landtier durchzuführen.
Ein typisches Beispiel für dieses Phänomen ist der europäische Grottenolm.

Neotenie kommt auch bei einigen Insektenarten vor, beispielsweise bei Motten, Käfern und Fächerflüglern.

Eine beschleunigte Entwicklung der Reproduktionsfunktionen wird auch als Pädogenese bezeichnet.

Auch bei Säugetieren spricht man von Neotenie, wenn diese im Zusammenhang mit der Domestizierung jugendliche Merkmale, wie z. B. eine verkürzte Schnauze, eine spezielle Fellzeichnung oder Schlappohren beibehalten. Durch die neotenischen Merkmale und das dadurch verkörperte Kindchenschema wird beim Menschen ein Fürsorge- und Kümmerungsverhalten aktiviert, das zu einem Selektionsvorteil für das entsprechende Tier führt. Insbesondere Haustiere zeigen daher oft solche Merkmale. 

Neotenische Merkmale bei Säugetieren (insbesondere hervorgerufen durch die Domestizierung) wurden von Hermann von Nathusius, Louis Bolk und verschiedenen modernen Forschergruppen deutlich beschrieben. So beschreibt beispielsweise eine russische Arbeitsgruppe, die über 40 Generationen Füchse auf Zahmheit selektiert hat, eine große Anzahl neotenischer Merkmale, die in dieser selektierten Fuchspopulation auftraten.

Dennoch ist Neotenie nicht, wie seit den 1920er Jahren häufig unterstellt, ein generelles Phänomen der Domestikation.

Neotenie sagt aus, dass Jugendstrukturen auf Dauer erhalten bleiben können. Zum Beispiel können Kräuter als fixierte Jugendstadien der Holzgewächse angesehen werden.
Ein Beispiel ist Paeonia (Pfingstrose), bei der die ursprünglichen Formen verholzt sind und sekundäres Dickenwachstum zeigen. In der Phylogenie wurde immer weniger Sekundärholz gebildet und parenchymatisches Mark sowie Markstrahlen verbreitert. Schließlich blieb nur das primäre Xylem erhalten und es entstanden krautige Formen.

Bei einigen wenigen karnivoren Pflanzen entwickeln sich Fallenmechanismen lediglich im Jugendstadium, bei den meisten bleibt die Karnivorie jedoch das ganze Leben lang erhalten. Die Karnivorie bringt den Pflanzen gegenüber nichtkarnivoren Pflanzen immense Vorteile. Während Pflanzen, die nicht karnivor sind, in nährstoffarmen Gebieten in einem ständigen Konkurrenzkampf um Nährstoffe sind, haben karnivore Pflanzen ein viel weiteres Spektrum an Nahrungsmitteln, daher entwickeln sich die Fangmechanismen bei manchen karnivoren Pflanzen extrem früh, teilweise sogar vor den Keimblättern. Diese Beobachtungen lassen vermuten, dass der Ursprung der Karnivorie mit der Keimlingsetablierung zu tun hat und die Ausbildung von Fallen bei erwachsenen Pflanzen ein Fall von Neotenie ist.

Emile Devaux wendet ab 1921 diesen Begriff auf die Entstehung des Menschen an. Etwa zeitgleich entwickelte der niederländische Anatom Lodewijk (Louis) Bolk seine Fetalisationstheorie. Demnach sei der Mensch ein in seiner Entwicklung stark verzögerter Affe – bzw. ein vorzeitig geschlechtsreif gewordener Affenfötus. Auch wurde behauptet, dass die „Mongoliden“ stärker „pädomorph“ seien („rassische Neotenie“).

Als Argumente dafür nannte Bolk folgende Merkmale am Schädel:

und weitere anatomische Besonderheiten beim Menschen:

Außerdem bewertete Bolk die lange Lebensspanne des Menschen als Indiz für diese Theorie.

Die Neotenie-Hypothese der Hominisation steht nicht im Gegensatz zu anderen Erklärungsmodellen für beispielsweise den aufrechten Gang. Während unter anderem die Savannen-Hypothese ein Erklärungsmodell zu liefern versuchte, unter welchen Selektionsbedingungen dieses spezielle Merkmal des Menschen entstand, erklärt die Neotenie-Hypothese, wie diese Merkmale physiologisch zustande gekommen sein könnten. Dabei stellt der Mechanismus der Neotenie eine Form der Präadaptation dar.


</doc>
<doc id="13715" url="https://de.wikipedia.org/wiki?curid=13715" title="Qi">
Qi

Der chinesische Begriff Qì (), auch als Ch'i, in Japan als Ki (jap. / ) und in Korea als "Gi" (kor. / ) bekannt, bedeutet Energie, Atem oder Fluidum, kann aber wörtlich übersetzt auch Luft, Gas (in der Chemie / Physik), Dampf, Hauch, Äther sowie Temperament, Kraft oder Atmosphäre bedeuten. Außerdem bezeichnet Qi die Emotionen des Menschen und steht nach moderner daoistischer Auffassung auch für die Tätigkeit des neurohormonalen Systems.

Qi ist ein zentraler Begriff des Daoismus. Der Begriff findet sich bereits im 42. Kapitel des Daodejing; der daoistische Philosoph Zhuangzi beschrieb den Kosmos als aus Qi bestehend. Darüber hinaus ist die Vorstellung vom Qi die ideelle Grundlage der traditionellen chinesischen Medizin (TCM) und der sogenannten inneren Kampfkünste.

Die Vorstellung vom Qi prägt bis heute das Weltverständnis vieler Menschen in Asien und zunehmend auch im Westen und hat Bedeutung für verschiedene Religionen. In adaptierter Form findet das mit dem Begriff verbundene Konzept seit dem 19. Jahrhundert auch Eingang in das westliche Denken, insbesondere als Bestandteil esoterischer Lehren.

Nach Auffassung der Kultur des Alten China und des Daoismus durchdringt und begleitet das Qi alles, was existiert und geschieht.

Als Substanz, aus der das ganze Universum sowohl in physischer als auch geistiger Hinsicht besteht, wird es vorgestellt als vitale Energie, Lebenskraft oder eines alles durchdringenden kosmischen Geistes, ist dabei aber weder physischer noch geistiger Natur. In einer sich ständig verändernden Wirklichkeit stellt das Qi die einzig konstante Größe dar.

Nach daoistischer Vorstellung entstand die Welt aus dem ursprünglichen Qi (Yuanqi), in dem Yin und Yang noch vermischt waren. Himmel und Erde bildeten sich erst durch Trennung des Einen: Was Yangqi empfing, stieg hell und klar empor und wurde Himmel, was Yinqi erhielt, wurde dunkel und schwer und sank zur Erde. Und was Yin und Yang in gerechtem und ausgewogenem Maße erhielt, war der Mensch in der Mitte.

Nach diesen Vorstellungen atmen wie der Mensch auch Himmel und Erde. Ihr Fluss ist wie beim Menschen beim Einatmen rein und unverbraucht und beim Ausatmen verbraucht. Daher teilt sich der Tag in zwei Abschnitte: Zwischen Mitternacht und Mittag ist die Zeit, in der Himmel und Erde einatmen. Nur in diesem Zeitraum sollten Atemübungen ausgeführt werden, da nur dann positive Energie aufgenommen werden kann, nicht jedoch in der Zeit zwischen Mittag und Mitternacht, weil dann Himmel und Erde ausatmen.

Eine besondere Bedeutung hat der Fluss des Qi für die belebte Welt. So trägt z. B. das Qi der Sonne zum Wachstum der Pflanzen bei, das Qi des Funktionskreises Leber verteilt das Blut/Xue im Körper, das Qi der Mutter behütet das Kind, das Qi der Erde trägt das Haus usw.

Der Begriff "Neiqi" steht für den „Inneren Atem“ und bezeichnet die im Inneren des Körpers gespeicherte Energie. Hierzu steht im Gegensatz "Waiqi", der „Äußere Atem“, also die eingeatmete Luft. Das Neiqi ist die bei der Geburt übernommene Energie des Ur-Atems, des Yuanqi (s. o.). Bei der Geburt des Menschen bilden sich durch Aufnahme des Ur-Qi Geist, Körper, Speichel und Samen des Mannes.

Nach daoistischer Auffassung kommt es darauf an, das Neiqi im Inneren des Körpers zu stärken, zu formen und zu erhalten beziehungsweise möglichst in seinen ursprünglichen, reinen Zustand zurückzuführen. Hierzu dienen zahlreiche daoistische Atemübungen. Bis in die Tang-Dynastie herrschte die Meinung vor, dass bei Atemübungen die Luft anzuhalten sei, um die Energie im Körper zu erhalten und zirkulieren zu lassen. Diese Auffassung änderte sich dann in der Mitte der Tang-Dynastie. Es setzte sich nun die Meinung durch, dass beim Zirkulieren des Atems nicht das äußere Qi, sondern das innere Qi im Körper kreist, wodurch man von der gefährlichen Übung des Atemanhaltens für bis zu 200 Herzschläge Abstand nehmen konnte.

Naturgemäß wurde dem Qi des Menschen schon immer besonderes Interesse entgegengebracht. Es bildeten sich daher eine Reihe von Lehren und Techniken, die besondere Wirkungen durch eine gezielte Beeinflussung des Qi herbeizuführen versuchten.

Dabei wurde der allgemeine Begriff „Qi“ weiter verfeinert, wenn von speziellen Phänomenen oder Prozessen die Rede ist. So stammt z. B. das obengenannte „Leber-Qi“ aus dem Wortschatz der traditionellen chinesischen Medizin und beschreibt das Qi, das dem Funktionskreis Leber (das nicht dem Organ der Leber nach westlichem Verständnis entspricht, sondern weit mehr umfasst) erlaubt, seine Funktion im menschlichen Körper auszuüben.

Eine bedeutende Rolle spielte das Qi in der Lehre des neokonfuzianischen Philosophen Zhu Xi, der die beiden großen traditionellen Lehren des alten China, den Daoismus und den Konfuzianismus, miteinander zu verbinden versuchte. Zhu Xi unterschied Qi, den materiellen Aspekt der Wirklichkeit, und Li, die vorgegebene feste Ordnung, also den formellen Aspekt. Die Verbindung beider Wirklichkeitsaspekte führt seiner Auffassung nach zur Entstehung der sichtbaren Welt.

Als Meditations-, Konzentrations- und Bewegungsform zur Kultivierung von Körper und Geist beschäftigt sich Qigong („Arbeit am Qi“) mit der Stärkung und Harmonisierung des Qi im menschlichen Körper. Qigong gilt ebenfalls als eine der fünf Säulen der traditionellen chinesischen Medizin. Siehe auch Faqi.

Im Feng Shui wird die Beziehung des Menschen zu seiner Umwelt betrachtet. Es gilt diese so zu gestalten, dass sie dem Menschen angenehm und förderlich ist und dadurch der Kreislauf des Qi im Körper günstig beeinflusst wird. Ebenso sollen ungünstige oder schädliche Wirkungen beseitigt werden. So wird im Feng Shui beispielsweise vom „schlechten Qi des Badezimmers“ gesprochen, wenn die schädlichen Einflüsse, die von einem Badezimmer ausgehen, betrachtet werden.

In vielen fernöstlichen Kampfkünsten spielt die bewusste Wahrnehmung und Kontrolle über das Qi eine Rolle. Beispiele sind insbesondere die inneren Kampfkünste wie das Taijiquan und Aikidō, aber auch die Shaolin-Kampfkünste.
Dabei soll einerseits das Praktizieren der Kampfkunst den Fluss des Qi stärken und harmonisieren, andererseits soll der Praktizierende das Qi auch für die Kampfkunst verwenden können. Beispielsweise wird die Fähigkeit eines Kämpfers, bei einem Bruchtest dicke Bretter mit einem Schlag zerteilen zu können und sich dabei nicht zu verletzen, darauf zurückgeführt, dass dieser durch langes Training in der Lage ist, das Qi auf einen schmalen Bereich der Handkante zu konzentrieren. Die Stärke des Qi zeige sich neben der Freisetzung von Kraft auch in der Aufmerksamkeit für den Qi-Fluss in einer Konfliktsituation, was den Kampfkünstler in die Lage versetze, die Intentionen des Kontrahenten frühzeitig wahrzunehmen. Manche Kampfkünste wie das Aikidō entwickelten daraus das Prinzip des Aiki, d. h. der Abstimmung der Bewegung auf das universelle Qi zum Zwecke der Harmonisierung kontrahenter Energien.

Qi wird in der traditionellen chinesischen Medizin (TCM) als generelle Lebensenergie oder Energie des Spirituellen angesehen. Das Qi im Körper wieder in seinen natürlichen, ausgeglichenen Zustand zu bringen, ist das Grundprinzip jeder traditionellen chinesischen Therapieform.

Bei einer perfekten Harmonie beider Kräfte ist auch der Qi-Fluss im Körper ausgeglichen. Das Modell der traditionellen chinesischen Medizin geht davon aus, dass der menschliche Körper im Inneren Funktionskreise beziehungsweise „Elemente“ aufweist, die mit einem Energiefluss korrespondieren, der teilweise an der Körperoberfläche und teilweise leicht darunter verläuft. Nach daoistischer Auffassung sind die wichtigsten Bahnen das Diener- und das Lenkergefäß. Man nennt diese Kanäle des Energieflusses „Leitbahnen“ oder „Meridiane“. Diese Vorstellungen widersprechen wissenschaftlichen Erkenntnissen über Funktion und Aufbau des menschlichen Körpers.

Krankheit ist ein Produkt der Unterbrechung dieses harmonischen Flusses. Nach dieser Auffassung kann Krankheit u. a. durch mangelnden Qi-Fluss, durch Stockung, durch Mangel an Qi selbst oder durch verbrauchtes Qi, das nicht abgeleitet wurde, entstehen. Die TCM versucht daher, physische Krankheiten durch verschiedene Praktiken zu kurieren, die ein Ausbalancieren des Qi-Flusses im Körper zum Ziel haben. Einige dieser Techniken enthalten Pflanzenmedizin, spezielle Diäten und Ernährungslehren sowie Akupunktur. Da ein so genanntes "vorgeburtliches Qi" nicht vermehrt werden kann, steht die TCM Hungerkuren sehr kritisch gegenüber. Sie sollten nicht im Alltag durchgeführt werden, sondern nur spirituellen Zwecken dienen, etwa zur Meditation.

Die Idee eines den Körper durchströmenden Qi-Stromes ist wesentlicher Teil des daoistischen Weltbildes und basiert auf sehr frühen chinesischen Vorstellungen, die auch heute noch von vielen Menschen in Asien getragen werden. Da das traditionelle daoistische Denken nicht in gleichem Maße wie die heutige naturwissenschaftliche Sicht zwischen objektiv-äußerer und subjektiv-innerer Wirklichkeit unterscheidet, stellen die unterschiedlichen Bedeutungsinhalte des Begriffs (Emotionen des Menschen, Atem, Dampf, Energie usw.) für Menschen, die von der Existenz des Qi überzeugt sind, keinen Widerspruch dar. Da das traditionelle Wissen eher auf Heil- und Wirksamkeit ausgerichtet ist als auf Gewinn an objektiver Erkenntnis, genügt es, die Wirkung des Qi in der Welt wahrzunehmen bzw. in den Wirkungen der auf dem Konzept aufbauenden Techniken zu spüren bzw. zu erahnen.

Eine Assimilation neuer naturwissenschaftlicher Erkenntnisse ist daher meist erfolgreich. Diese werden in das vorhandene Weltbild integriert, sofern sie für dessen Verständnis nützlich sind. Beispielsweise überraschte die Entdeckung von „Bazillen“ als Krankheitserreger die traditionelle chinesische Medizin nicht, da sie aus daoistischer Sicht phänomenologisch schon seit über 2000 Jahren funktional bekannt waren. Das Konzept eines „Abwehr-Qì“ konnte ebenfalls um die Erkenntnis der Immunabwehr erweitert werden.

Durch die Beschäftigung mit den traditionellen chinesischen Lehren und die Übernahme der genannten Gesundheitslehren und Techniken hat sich das Qi-Konzept seit den 1970er Jahren auch zunehmend in den Vorstellungen von Menschen des westlichen Kulturkreises verbreitet. Dabei kann es zu einer Vereinfachung des komplexen daoistischen Systems kommen. Besonders in der Esoterik wird das Qi dann als eine Art feinstoffliche Energie verstanden. Diese Ansicht wird durch die vereinfachte Übersetzung von Qi als "Lebensenergie" oder dergleichen verstärkt. Von naturwissenschaftlich geprägten Menschen wird diese Erklärung abgelehnt, da die Existenz einer solchen Energieform naturwissenschaftlich nicht belegt ist. Andere sehen im Qi ein nützliches Konzept, das dabei hilft, verschiedene Phänomene zu verstehen und die Fähigkeiten zu entwickeln, diese zu beeinflussen. In diesem Erklärungsmodell hat das Qi keine physikalische Realität, sondern es handelt sich lediglich um eine phänomenologische Beschreibung der Realität. Diese Erklärung steht nicht im Widerspruch zu naturwissenschaftlichen Erkenntnissen.

Das Konzept des Qi kommt in vielen anderen Kulturen in ähnlicher Form vor:


Zudem bestehen neuere, teils esoterische Konzeptionen, in denen auch – entweder implizit oder sogar "expressis verbis" – Bezug auf das Qi genommen wird:

Im westlichen Athletentraining spielt die klassische Sichtweise des Qi kaum eine Rolle. Bei physiologisch orientierten Versuchen, in denen asiatische Kampfkünstler angaben, ihr Qi in bestimmten Körperteilen z. B. in den Armen oder Beinen zu konzentrieren, zeigten Wärmebildkameras, dass genau dort eine erhöhte Muskelspannung vorlag, die für besondere Leistungen, wie kräftige Schläge auszuhalten oder auszuführen, vorbereitet war. Unter einem rein physiologischen Gesichtspunkt betrachtet, kann Qi demnach auch als einfache Muskelanspannung beschrieben werden, die man bewusst durch Nervenimpulse kontrolliert und vor allem konzentriert.

Bei dem Spiel Scrabble spielt der Begriff "Qi" (beziehungsweise sein Genitiv "Qis") insofern eine wichtige Rolle, dass er im Deutschen eines von ganz wenigen mit dem Buchstaben "Q" beginnenden Wörtern ist, für den man kein "u" als zweiten Buchstaben benötigt. Somit bietet der Begriff häufig die einzige Möglichkeit, das "Q" abzulegen.




</doc>
<doc id="13716" url="https://de.wikipedia.org/wiki?curid=13716" title="Kirchenstrafe">
Kirchenstrafe

Unter Kirchenstrafen ("disciplinae ecclesiasticae") werden Sanktionen gegenüber Laien und Klerikern verstanden, die die römisch-katholische Kirche verhängt, um straffällig gewordene Gläubige durch Strafmittel zurechtzuweisen. 

Die Kirchenstrafen sind im 6. Buch des Codex Iuris Canonici (CIC) geregelt (seit 1983 in den cann. 1311–1399). 

Davon zu unterscheiden sind die kirchlichen Disziplinarmaßnahmen ohne Strafcharakter wie die Amtsenthebung. 

Im 6. Buches des CIC sind die Anwendbarkeit der kirchlichen Strafbestimmungen, die persönlichen Voraussetzungen der Strafbarkeit wie Strafmündigkeit oder Schuldfähigkeit sowie die Strafzumessung einschließlich möglicher Strafausschließungsgründe oder Straferlass geregelt, außerdem die Strafverhängung (Strafverfahren). 

Mit einer Kirchenstrafe werden belegt Straftaten gegen die Religion und die Einheit der Kirche wie Schisma und Häresie, Straftaten gegen die kirchlichen Autoritäten und die Freiheit der Kirche, beispielsweise wer dem Apostolischen Stuhl, dem Ordinarius oder dem Oberen, der rechtmäßig gebietet oder verbietet, nicht gehorcht und nach Verwarnung im Ungehorsam verharrt, ferner Amtsanmaßung und Amtspflichtverletzung, sog. Fälschungsdelikte wie Verleumdung oder Urkundenfälschung, Straftaten gegen Leben und Freiheit des Menschen (auch Abtreibung) und schließlich Straftaten gegen besondere Verpflichtungen wie der Versuch, eine Zivilehe einzugehen oder dem Zölibat zuwider in einem eheähnlichen Verhältnis zu leben.

Zu unterscheiden sind die Spruch- und die Tatstrafen. 

Strafmittel sind die Besserungs- oder Beugestrafen sowie Sühnestrafen. Vorbeugend können auch Strafsicherungsmittel (Verwarnung, Verweis) und Bußen (Auflage, irgendein Werk des Glaubens, der Frömmigkeit oder der Caritas zu verrichten wie eine Bußwallfahrt oder Fasten) ausgesprochen werden. Gegenüber Laien können sowohl die Kirchenstrafen als auch eine eventuelle Absolution durch den Priester (Beichtvater) ausgesprochen werden, im übrigen durch die Kirchengerichtsbarkeit.

Beispiele für bekannte Kirchenstrafen sind:

Etliche der oben genannten Strafen werden heute nicht mehr oder nicht in der früher üblichen Weise angewandt. So hat der Kirchenbann seit langem keine weltlichen Rechtsfolgen mehr und entspricht seit Abschaffung der Unterscheidung zwischen „kleinem“ und „großem“ Bann im Jahre 1869 der Strafe der Exkommunikation. Die Straffolge der Infamie wurde 1983 abgeschafft. Das Interdikt beschränkt sich heute im Regelfall auf die persönliche Untersagung der Mitwirkung an Gottesdiensten und sakramentalen Handlungen gegenüber einem Kleriker oder Amtsträger, während so genannte Lokalinterdikte über ganze Orte, Territorien oder Gebiete im geltenden Recht nicht mehr ausdrücklich vorgesehen sind.

Für ihre Kirchenkritik wurden unter anderen Jan Hus, Martin Luther, Hans Küng oder Eugen Drewermann bestraft. 

Auch die Inquisitionsverfahren waren kirchliche Strafverfahren, wohingegen die Hexenprozesse überwiegend nach weltlichem Recht geführt wurden.



</doc>
<doc id="13719" url="https://de.wikipedia.org/wiki?curid=13719" title="Louis Pasteur">
Louis Pasteur

Louis Pasteur (* 27. Dezember 1822 in Dole, Département Jura; † 28. September 1895 in Villeneuve-l’Étang bei Paris) war ein französischer Chemiker, Physiker, Biochemiker und Mitbegründer der medizinischen Mikrobiologie, der entscheidende Beiträge zur Vorbeugung gegen Infektionskrankheiten durch Impfung geleistet hat.

Pasteur begann seine Karriere mit einer Entdeckung auf dem Gebiet der Chemie: Aus zwei asymmetrischen, spiegelbildlichen Kristallformen eines Salzes der Traubensäure sowie ihrer optischen Aktivität, wenn sie getrennt in Lösung gebracht wurden, schloss er auf ihre zugrunde liegende molekulare Asymmetrie. Damit wurde er zum Begründer der Stereochemie. Optische Aktivität war in den Augen Pasteurs eine Eigenschaft, die die Moleküle von Lebewesen charakterisiert. Da bei der Gärung optisch aktive Substanzen entstehen, vermutete er, dass sie von Mikroorganismen verursacht wurde. Dies konnte er in einer Reihe von Experimenten belegen und damit die konkurrierende Hypothese ausschließen, die etwa von Justus Liebig vertreten worden war, es handele sich um rein chemische Reaktionen ohne Beteiligung von Lebewesen. Gleichzeitig galt damit die seit der Antike diskutierte Frage, ob unter Alltagsbedingungen Leben spontan entstehen kann, als entschieden. Im Rahmen seiner Studien zur Gärung entdeckte Pasteur, dass es Mikroorganismen gibt, die ohne Sauerstoff auskommen, und er fand das erste Beispiel für eine Stoffwechselregulation, als er beobachtete, dass Hefezellen unter Ausschluss von Sauerstoff Zucker schneller verbrauchen. Pasteur beschrieb verschiedene Formen der Gärung und erkannte, dass dies verschiedenartige Mikroorganismen voraussetzt. Eine praktische Konsequenz dieser Arbeiten war ein Verfahren zur Haltbarmachung flüssiger Lebensmittel, die Pasteurisierung.

Im Auftrag der französischen Regierung erforschte Pasteur verschiedene Krankheiten der Seidenraupen und erkannte sie als Infektionskrankheiten. Ab 1876 widmete er sich vollständig human- und veterinärmedizinischen Fragen. Er entwickelte einen Impfstoff aus abgeschwächten Krankheitserregern zum Schutz vor Geflügelcholera und baute damit die Impfung – für die es in der Humanmedizin bis dahin nur das Beispiel der Pockenschutzimpfung gegeben hatte – überhaupt erst zu einem allgemeinen Prinzip aus. Weitere Impfstoffe gegen Milzbrand, Schweinerotlauf und Tollwut zeigten, dass man zumindest theoretisch fortan beliebigen Infektionskrankheiten vorbeugen konnte. Mit seinen Arbeiten zur Gärung und Impfung demonstrierte Pasteur das wirtschaftliche und medizinische Potenzial experimenteller Biologie. Die Produktion des Milzbrandimpfstoffs stand am Anfang der Impfstoff-Industrie. Eine Spendenwelle nach der ersten spektakulären Tollwut-Impfung eines Jungen erlaubte die Gründung des "Institut Pasteur", bis heute die führende Wissenschaftsinstitution Frankreichs in der biomedizinischen Forschung.

Die Wissenschaftshistoriker Gerald L. Geison und Antonio Cadeddu haben anhand der Labortagebücher von Pasteur nachgewiesen, dass seine Veröffentlichungen sich nicht immer mit den tatsächlich im Labor vorgenommenen Experimenten deckten.

Louis Pasteur kam aus einer Familie von Gerbern (sein Vater war der Gerber Jean-Joseph Pasteur) und wuchs nach mehreren Umzügen in Arbois auf, wo er das Gymnasium besuchte. Das dritte von fünf Kindern fiel in der Schule, wenn überhaupt, zunächst durch sein künstlerisches Talent auf. 1837/38 errang er jedoch so viele Schulpreise, dass ihm nahegelegt wurde, sich auf die " École normale supérieure", die Pädagogische Fakultät, in Paris vorzubereiten. Der erste Versuch scheiterte an zu starkem Heimweh. 1842 absolvierte er das Baccalauréat (Note in Chemie: „mittelmäßig“) und wurde an der "École Normale" zugelassen, lehnte jedoch ab, weil er mit seinem Rang (15. von 22 Kandidaten) unzufrieden war. Er absolvierte ein weiteres Vorbereitungsjahr und erreichte diesmal Rang 4.
Die folgenden fünf Jahre studierte Pasteur an der "École Normale" und arbeitete zugleich als wissenschaftlicher Mitarbeiter ("préparateur"). 1846 absolvierte er in Paris die Lehramtsprüfung für physikalische Wissenschaften. 1847 wurde er zum Doktor der Naturwissenschaften ("docteur-ès-sciences") auf Grundlage von zwei Doktorarbeiten in Physik und Chemie promoviert. Nach einem kurzen Aufenthalt Ende 1848 als Gymnasialprofessor für Physik am von Dijon ging er im Januar 1849 als Assistenzprofessor für Chemie ("professeur suppléant") an die Universität Straßburg. Hier verliebte er sich in Marie Laurent, die Tochter des Rektors der Akademie Straßburg. Er heiratete sie bereits am 29. Mai 1849. Seine Frau brachte fünf Kinder zur Welt, von denen allerdings drei in früher Jugend starben. Bis 1853 war sein wissenschaftliches Ansehen so gewachsen, dass ihm die Pharmazeutische Gesellschaft ("Société de Pharmacie") einen Preis von 1500 Francs zuerkannte und er in die Ehrenlegion aufgenommen wurde.

1854 wurde er als Professor für Chemie sowie als Dekan an die neu gegründete Fakultät für Wissenschaften in Lille berufen. Pasteur vertrat hier eine stark anwendungsbezogene Forschung im Interesse der lokalen Industrie; auch setzte er sich für eine Innovation ein, die durch ein Kaiserliches Dekret im selben Jahr eingeführt worden war, dass Studenten der Naturwissenschaften im Labor ausgebildet werden sollten.

1857, im Alter von 34 Jahren, wurde Pasteur zum Direktor für wissenschaftliche Studien sowie zum Administrator (vergleichbar einem Kanzler an einer deutschen Hochschule) an der "École Normale" in Paris ernannt. Pasteur setzte die Zahl der "agrégé-préparateurs" – Laborassistenten, die die "École Normale" absolviert hatten – herauf, reduzierte aber andererseits ihre Vertragsdauer von sieben oder acht Jahren auf zwei. Mit dieser Maßnahme ermutigte er mehr Studenten zur Promotion. Außerdem gründete er eine neue Zeitschrift, die "Annales scientifiques de l’École Normale Supérieure", als Forum für die Forschungsergebnisse seines Hauses. Diese Zeitschrift redigierte er persönlich bis 1871. Unter Pasteurs Ägide verbesserte sich der Ruf der "École Normale" enorm. Hatten sich zuvor etwa 50 bis 70 Personen jährlich auf die 15 Studienplätze beworben, so waren es zum Schluss 200 bis 230. Viel Aufsehen erregte, dass erstmals Bewerber, die zugleich an der "École polytechnique" angenommen worden waren, sich für die "École Normale" entschieden. Pasteur war auch für die Disziplin unter den Studenten zuständig, was ihn überforderte. Nach 1867 ausgebrochenen Studentenunruhen war er an der "École Normale" nicht mehr haltbar und wechselte als Chemie-Professor an die Sorbonne.

Obwohl mit seinen beiden Positionen an der "École Normale" kein Forschungsauftrag verbunden war, hatte Pasteur sich sofort zwei Dachräume als Labor eingerichtet, wo er seine in Lille begonnenen Studien zur Gärung fortsetzte. Er erlangte direkten Zugang zu Kaiser Napoleon III., was ihm erlaubte, seine Tätigkeit zunehmend auf die Forschung zu verlagern. 1862 wurde Pasteur in die Akademie der Wissenschaften gewählt. Ab 1865 nahmen ihn die Forschungsarbeiten zu den Krankheiten der Seidenraupen, um die ihn die Regierung gebeten hatte, stark in Anspruch. Bis 1870 verbrachte er deswegen jeden Sommer in einem Feldlabor in Südfrankreich.
Dank der Unterstützung des Kaisers wurde ihm der Neubau eines Labors genehmigt, der sich allerdings durch den Deutsch-Französischen Krieg verzögerte. Den Krieg verbrachte Pasteur im heimischen Jura, wo er das Bierbrauen studierte, ein Industriezweig, in dem er Frankreich gegenüber Deutschland als unterlegen ansah. Bei seiner Rückkehr nach Paris bat er darum, von allen Lehrverpflichtungen entlastet zu werden. Er setzte eine Leibrente von 12.000 Francs, die ihm noch vom Kaiser versprochen worden war, unter den veränderten politischen Bedingungen der Dritten Republik durch. Die nächsten fünf Jahre verbrachte er damit, seine Forschungsarbeiten über Gärung, Bier, Krankheiten der Seidenraupen und die spontane Entstehung des Lebens abzuschließen.
1876, mit 54 Jahren, wechselte der Chemiker auf ein neues Forschungsgebiet: die Infektionskrankheiten von Haustieren und Menschen. Als Außenseiter auf dem Gebiet der Veterinär- und Humanmedizin begann er mit der Erforschung des Milzbrands. Nachdem ihm ein Impfstoff zum Schutz vor Milzbrand gelungen war, wurde zu dessen Produktion ein zusätzliches Labor in der rue Vauquelin gebaut – es stellt den Beginn der Impfstoff-Industrie dar. Für seine Tollwut-Forschung wurden Pasteur außerdem die alten Ställe des Schlosses Saint-Cloud im Park von Villeneuve l’Etang zur Verfügung gestellt. Die vom französischen Staat eingeworbenen Drittmittel wuchsen an, bis Pasteur zeitweise zehn Prozent der gesamten französischen Forschungsausgaben vereinnahmte.
Gegenüber seinen Schülern und Mitarbeitern verhielt sich Pasteur autoritär, und er galt als völlig humorlos. Sein Labor führte er wie ein Familienvater, wobei er darauf achtete, dass seine Angestellten auch verwandtschaftlich verbunden waren. Obwohl Pasteur in seiner Jugend kurzfristig an der Februarrevolution von 1848 teilgenommen hatte, war seine spätere politische Haltung konservativ bis reaktionär. Louis Napoleons Staatsstreich von 1851 begrüßte er und suchte engen Kontakt zum Kaiser. Auch nach dessen Abdankung verschwieg er nicht, wie sehr er ihm verpflichtet gewesen war. 1875 kandidierte Pasteur für die Konservativen für einen Sitz im Senat für seine Heimatstadt Arbois, scheiterte aber weit abgeschlagen, weil er als Bonapartist galt. Abgesehen von diesem Ausflug in die Politik lebte Pasteur ausschließlich für die Wissenschaft, verfolgte auch keine Hobbys; in seiner Pariser Zeit verließ er nur selten das Quartier Latin, wo die für ihn wesentlichen Wissenschaftsinstitutionen lagen.
Pasteur war ein glühender Patriot. Einem Briefpartner schrieb er während des Deutsch-Französischen Kriegs von 1870/71, er werde künftig alle seine Werke mit „Hass auf Preußen. Rache. Rache.“ zeichnen. Einen Ehrendoktor der Universität Bonn gab er aus diesem Grund 1870 zurück. Seine heftigsten Kontroversen, die zumindest von der Presse auf beiden Seiten mit stark nationalistischen Untertönen begleitet wurden, focht er mit deutschen Wissenschaftlern – unter ihnen Justus Liebig und Robert Koch – aus. Noch kurz vor seinem Tod weigerte er sich, den preußischen Orden Pour le Mérite anzunehmen.

Seine wissenschaftliche Leistung wurde durch zahlreiche Preise aus aller Welt anerkannt, darunter das Großkreuz der Ehrenlegion. 1882 erhöhte der französische Staat seine Leibrente auf 25.000 Francs (vererbbar auf seine Frau und seine Kinder), was dem Doppelten des Gehalts eines Universitätsprofessors entsprach. Im selben Jahr wurde Pasteur als „Unsterblicher“ in die "Académie française" gewählt. Mit dem triumphalen Erfolg der Tollwut-Impfung traf eine Vielzahl von Spenden ein. Eine Subskription zum Bau des "Institut Pasteur" über zwei Millionen Francs wurde sogar noch übertroffen. Es wurde im November 1888, als Pasteur 65 Jahre alt geworden war, eingeweiht. Allerdings hatte seine Gesundheit nach mehreren Schlaganfällen so stark gelitten, dass er keine wichtigen Beiträge zur Forschung mehr leisten konnte. Bei seinem Tod war Pasteur praktisch vollkommen gelähmt. Sein Körper – und später der seiner Frau – wurde in einer Krypta unter dem "Institut Pasteur" beigesetzt.

In den Jahren von 1847 bis 1857 unternahm Pasteur eine Reihe bedeutender Experimente zum Zusammenhang zwischen optischer Aktivität, Kristallstruktur und chemischer Zusammensetzung von organischen Verbindungen insbesondere der Weinsäure und der Traubensäure.

Bereits 1841 hatte Frédéric de la Provostaye die Kristallformen rechtsdrehender Weinsäuresalze (Tartrate) beschrieben. Ausgangspunkt für Pasteurs Forschungsprojekt war die Entdeckung von Eilhard Mitscherlich, dass die Natrium-Ammonium-Salze der Weinsäure und der Traubensäure in allen ihren Eigenschaften identisch zu sein schienen, nur dass eine Lösung der Weinsäure die Ebene des polarisierten Lichts nach rechts drehte, während eine Traubensäure-Lösung optisch inaktiv blieb. Pasteur konnte Mitscherlichs Behauptung widerlegen, dass die Natrium-Ammonium-Salze der Weinsäure und der Traubensäure in ihrer Kristallstruktur identisch wären. Es gelang ihm unter dem Mikroskop, im Falle der Traubensäure zwei verschiedene Kristallstrukturen zu identifizieren, die sich wie Bild und Spiegelbild zueinander verhielten. In mühsamer Handarbeit trennte er die beiden Kristallformen voneinander. Separat aufgelöst, drehten sie die Ebene des polarisierten Lichts um denselben Betrag in entgegengesetzte Richtungen. Die rechtsdrehende Form (L-(+)-Weinsäure) war identisch mit der bekannten Weinsäure, die linksdrehende Form (D-(−)-Weinsäure) war bis dahin unbekannt gewesen. Kombinierte er gleiche Volumina und Konzentrationen der beiden Lösungen miteinander, hoben sich die gegenläufigen optischen Aktivitäten auf, und die Lösung wirkte optisch inaktiv (Racemat). Diese Entdeckung stellte er am 22. Mai 1848 im Alter von 25 Jahren vor der Akademie der Wissenschaften vor. 1860 äußerte Pasteur in einem Vortrag vor der Chemischen Gesellschaft zu Paris ("Société Chimique") die kühne Hypothese, dass die asymmetrische Kristallform optisch aktiver Verbindungen ihre Ursache in der asymmetrischen Gruppierung der Atome im Molekül haben müsse. Er spekulierte bei dieser Gelegenheit, ob zum Beispiel eine rechtsdrehende Verbindung die Form einer rechtshändigen Spirale oder eines irregulären Tetraeders haben könnte. Damit wurde er zum Begründer der Stereochemie, auch wenn das Konzept des tetraedrischen Kohlenstoff-Atoms Kekulé zuzuschreiben ist.

Die soeben gegebene Darstellung folgt dem Vortrag von 1860. Der Wissenschaftshistoriker Gerald L. Geison hat durch das Studium von Pasteurs Labortagebüchern einen anderen Ablauf rekonstruiert. Pasteur war demnach stark von seinem Kollegen Auguste Laurent an der "Ecole Normale" beeinflusst, der glaubte, dass die Form eines Kristalles durch die interne (Molekül-)Struktur bestimmt wird. Allerdings waren Beispiele von „Dimorphismus“ bekannt, also Substanzen mit gleicher chemischer Summenformel, aber unterschiedlichen Kristallformen, wie Calcit und Aragonit. Pasteur untersuchte insgesamt acht verschiedene Salze von Weinsäuren (Tartrate), wobei er prüfen wollte, ob sie miteinander kristallisieren könnten. Unter Laurents Einfluss achtete er besonders auf den Gehalt an Kristallwasser und minimale Änderungen der Kristallstruktur. Ihm war bereits aufgefallen, dass die Kristalle der Natrium-Ammonium-Salze der Traubensäure zwei verschiedene Formen aufweisen, und er hatte sie bereits auseinandersortiert, bevor er sich mit Mitscherlichs Arbeit zur optischen Aktivität beschäftigte. Pasteurs Überzeugung, dass nur optisch aktive Substanzen eine asymmetrische Kristallstruktur zeigen, war also nicht Ausgangspunkt, sondern Ergebnis seiner Forschung; die Frage der optischen Aktivität war nicht erkenntnisleitend. Geison wertet dies als ein Beispiel dafür, wie Erinnerung trügen kann und auch die Selbstauskünfte von Wissenschaftlern nicht unbedingt zuverlässig sind. Den Einfluss von Auguste Laurent hat Pasteur später heruntergespielt, möglicherweise, weil Laurents Sympathie für die Radikalen bald nicht mehr politisch opportun war.

Pasteur fand noch zwei weitere Methoden zur Racemat-Trennung: Eine optisch aktive Base bildet mit dem Racemat einer optisch aktiven Säure ein diastereomeres Salzpaar, das durch Kristallisation getrennt werden kann. Und Mikroorganismen verstoffwechseln unter Umständen bevorzugt eine der beiden optisch aktiven Formen eines Racemats. Ein schönes Beispiel fand Pasteur 1860 in dem Pinselschimmel "Penicillium glaucum", der selektiv die rechtshändige Form einer Lösung des Ammonium-Salzes der Traubensäure, welcher etwas Phosphat hinzugefügt wurde, verstoffwechselt. Beide Methoden ließen sich in zahlreichen Fällen anwenden. Allerdings musste Pasteur seine Überzeugung revidieren, dass optisch aktive Verbindungen notwendigerweise eine asymmetrische Kristallstruktur haben, nachdem er sie bei einem optisch aktiven Isomer des Amylalkohols nicht nachweisen konnte.

Nachdem Pasteur 1854 nach Lille gewechselt war, wurde von ihm erwartet, eine stark anwendungsbezogene Forschung zu leisten. Speziell die Alkoholproduktion aus Rübenzucker spielte für die lokale Industrie eine große Rolle, wodurch der Chemiker erstmals mit Fragen der Gärung konfrontiert wurde.

Pasteur war überzeugt davon, dass optische Aktivität und molekulare Asymmetrie organischer Verbindungen in irgendeiner Weise mit dem Leben zusammenhängt, eine Intuition, die sich seitdem im Wesentlichen als richtig erwiesen hat (Chiralität). In der Natur grenzte er einen lebendigen Bereich chemischer Verbindungen (optisch aktiv mit asymmetrischen Kristallformen) gegen einen toten Bereich (optisch inaktiv mit symmetrischen Kristallen) ab. Philosophisch gesehen war Pasteur also Vitalist. Er glaubte an eine „kosmische asymmetrische Kraft“, die er zeit seines Lebens experimentell nachzuweisen hoffte. Da die Produkte der Gärung häufig optisch aktiv waren, vermutete er, dass die Gärung selbst von Mikroorganismen verursacht wurde.

Für die Gegenposition einer rein abiotisch verursachten Gärung standen Chemiker wie Jöns Jakob Berzelius oder Justus Liebig. Vor allem Liebig war für Pasteur ein ständiges Ärgernis. „Alle Experimente, die ich seit 23 Jahren dieser Akademie mitgeteilt habe, dienten direkt oder indirekt dazu, die Ungenauigkeit der Ansichten Liebigs aufzuzeigen“, sagte er bei einem der wichtigsten Vorträge seines Lebens am 10. Februar 1880 vor der Akademie der Wissenschaften anlässlich der Vorstellung seines ersten Impfstoffs und hielt sich lange bei der in seinen Augen fatalen Rolle Liebigs auf. Dessen Theorie zur Gärung besitze nicht die geringste Grundlage.

Pasteur begann seine Studien zur Gärung mit der Milchsäuregärung (der Prozess, in dem mikrobiell Zucker zu Milchsäure abgebaut wird, wobei Sauerbier oder Sauermilch entsteht). 
Möglicherweise war er auf diese spezielle Form der Gärung durch sein Interesse an Amylalkohol gestoßen, weil dieser hierbei in großen Mengen entsteht. 1857 hielt er einen inzwischen berühmt gewordenen Vortrag zur Milchsäuregärung vor der Gesellschaft für Wissenschaften, Landwirtschaft und Künste zu Lille. In diesem Vortrag fasste er die Überzeugungen zusammen, die seine Forschungsarbeiten zur Gärung leiteten:

Pasteurs Studien zu Bier, Wein und Essig fallen in zwei verschiedene Zeiträume von 1857 bis 1865 und von 1871 bis 1876. Dass die alkoholische Gärung von Hefen verursacht werde, ist nicht von Pasteur entdeckt worden, wie manchmal falsch zu lesen ist. Diese Idee war bereits 1837 unabhängig voneinander von Charles Cagniard-Latour, Theodor Schwann und Friedrich Kützing publiziert worden. Pasteur konnte zeigen, dass bei der alkoholischen Gärung nicht nur Ethanol und Kohlendioxid, sondern zahlreiche Nebenprodukte wie Glycerin, Bernsteinsäure, Zellulose und Fette entstehen. Der Prozess ließ sich in seiner vollen Komplexität nicht in Form einer einfachen Reaktionsgleichung aufschreiben, was es in seinen Augen unwahrscheinlich machte, dass er ohne Beteiligung von Lebewesen ablaufen sollte.

Trotzdem ging die Mehrheit der Wissenschaftler noch von einer abiotischen Theorie der Gärung aus. Liebig hatte dafür eine „instabile organische Substanz“ zur notwendigen Voraussetzung erklärt. Pasteur widerlegte diese Theorie, indem er eine alkoholische Gärung in einem künstlichen Medium, das frei von organischem Stickstoff war, in Gang brachte. Zu einer Lösung von Rohrzucker gab er Ammoniumtartrat sowie die Mineralien aus veraschter Hefe und startete die Gärung mit etwas Bierhefe. Wenn er eine der Zutaten wegließ, blieb die Gärung aus.

1858 schlugen Moritz Traube und 1860 Marcelin Berthelot eine Theorie vor, die zwischen den Extrempositionen Pasteurs und Liebigs vermittelte: Die Gärung war demnach nicht unmittelbar das Produkt von Lebewesen, sondern von „Fermenten“ (Enzyme), die von Lebewesen ausgeschieden werden, selbst aber nicht leben. Liebig reagierte lange Zeit nicht, bis er 1868 und 1869 zwei Vorlesungen zu dem Thema hielt, die überdies wegen des Deutsch-Französischen Kriegs erst 1871 ins Französische übersetzt wurden. In seiner letzten Vorlesung gab Liebig zu, dass die alkoholische Gärung im eben skizzierten Sinne auf die Aktivität der Hefe zurückging. Pasteur wollte die Existenz „löslicher Fermente“ nicht vollkommen ausschließen, aber auch noch nicht akzeptieren, als die Frage durch eine Publikation Berthelots aus dem Nachlass von Claude Bernard noch einmal aufkam. Trotzdem beschuldigte er französische Anhänger von Liebig, wie Edmond Frémy, des mangelnden Patriotismus, weil sie es gewagt hatten, „deutscher Wissenschaft“ den Vorzug zu geben.

Im Rahmen seiner Bierstudien entdeckte Pasteur 1861 zum ersten Mal eine Stoffwechselregulation. Bei Sauerstoffmangel konsumieren Hefen mehr Zucker. In heutigen Begriffen decken sie dann ihren Energiebedarf durch den anaeroben Abbau von Kohlenhydraten (Glykolyse); bei ausreichender Versorgung mit Sauerstoff schalten sie auf den oxidativen Kohlenhydratabbau um (oxidative Phosphorylierung). Der Effekt ist inzwischen nach ihm als Pasteur-Effekt benannt. Seine Erkenntnisse fasste er 1876 in dem Buch "Etudes sur la bière" zusammen.

Ab 1861 veröffentlichte Pasteur eine Reihe von Artikeln zur „Essigsäuregärung“ (aus heutiger Sicht handelt es sich nicht um eine Gärung), die er 1864 in einer langen Abhandlung zusammenfasste. Als Pasteur anfing, wurde die Essigsäuregärung von der Mehrheit der Wissenschaftler als abiotischer Prozess betrachtet, analog zur katalytischen Oxidation von Alkohol zu Acetaldehyd und Essigsäure durch fein verteiltes Platin. Pasteur war sich dagegen sicher, dass auch die Essigsäuregärung auf einen biologischen Prozess zurückzuführen sein musste, und untersuchte mit diesem Ziel die feine Haut auf der Oberfläche von Essig, die Essigmutter. Friedrich Kützing hatte bereits 1837 einen Zusammenhang zwischen der Aktivität von Mikroorganismen in der Essigmutter und der Entstehung von Essig hergestellt. Pasteur gelang es, wie schon in seinem Experiment zur alkoholischen Gärung, Essig aus einem Medium aus verdünntem Alkohol, Ammonium, mineralischen Salzen und dem Organismus aus der Essigmutter (in damaliger Nomenklatur "Mycoderma aceti") zu gewinnen. Nur an einer Oberfläche in Gegenwart von reichlich Sauerstoff produzierte dieser Organismus Essig. Pasteurs Studien hatten für Essighersteller die praktische Folge, dass sie die Produktion steuern konnten. Bis dahin hatten sie manchmal wochenlang warten müssen, bevor die Essigmutter spontan auftauchte.

Pasteur beschrieb 1861 auch eine neuartige Form der Gärung, die Buttersäuregärung. Die Mikroorganismen, die er hier als Verursacher ausmachte, waren beweglich, weshalb er sie dem Tierreich zuordnete. Ihm fiel auf, dass diese „Infusorien“ unter dem Mikroskop am Rande des Deckglases ihre Beweglichkeit verloren, was er auf hier eindringende atmosphärische Luft zurückführte. Wenn Pasteur Luft durch einen Behälter leitete, in dem Buttersäuregärung ablief, konnte er die Gärung sofort stoppen. Die Mikroorganismen waren danach abgestorben. Kohlensäure behinderte dagegen die Buttersäuregärung nicht. Pasteur hatte damit ein Beispiel für einen anaeroben Organismus gefunden. Später verallgemeinerte er diese Beobachtung, indem er Gärung generell als „Leben ohne Luft“ definierte. Über den Widerspruch, dass die „Essigsäuregärung“ seiner Ansicht nach die Anwesenheit von Sauerstoff voraussetzte, sah er lange Zeit hinweg. Außerdem musste er 1872 zugeben, dass auch die alkoholische Gärung nicht ganz ohne Sauerstoff ablaufen kann, weil sonst Hefe nicht keimen kann.

Fäulnis war zu dieser Zeit als Zersetzung von Substanzen pflanzlicher oder tierischer Herkunft mit Entwicklung von übelriechenden Gasen definiert. 1863 dehnte Pasteur seine Erkenntnisse zur Gärung auf die Fäulnis aus. Obwohl er nur unzureichende Belege lieferte, behauptete er, dass auch die Fäulnis auf die Aktivität lebender Organismen zurückgehe. Fäulnis sei nichts anderes als die Gärung von Substanzen mit einem relativ hohen Schwefelanteil, und die Freisetzung dieses Schwefels in gasförmigen Verbindungen erzeuge den üblen Geruch.

Pasteurs Studien über die Gärung waren von entscheidender Bedeutung für eine Frage, die seit der Antike diskutiert worden war: Kann Leben unter Alltagsbedingungen spontan entstehen? (Spontanzeugung; der in der deutschsprachigen Literatur häufig verwendete Begriff „Urzeugung“ suggeriert, es gehe um die erste Entstehung des Lebens, die hier nicht gemeint ist.) Zu Pasteurs Zeit war die Debatte bereits auf die Fragestellung reduziert worden, ob mikroskopisch kleine Lebewesen aus toter organischer Materie entstehen können. Ab 1860 veröffentlichte Pasteur dazu in kurzer Folge fünf Arbeiten, die er 1861 in einem Vortrag vor der Chemischen Gesellschaft zu Paris zusammenfasste.

Zu den Experimenten, die hier nicht vollständig beschrieben werden, gehören:

Für den Vortrag von 1861 verlieh die Akademie der Wissenschaften Pasteur ein Preisgeld von 2500 Francs, das für denjenigen ausgelobt worden war, der wichtige Beiträge zur Frage der spontanen Entstehung von Leben leisten würde. Félix Archimède Pouchet (1800–1872) hatte 1845 nachgewiesen, dass weibliche Tiere Eizellen unabhängig vom Kontakt mit Männchen produzieren. Er vertrat eine gemäßigte Variante der Spontanzeugung (zwar entstehen nicht erwachsene Organismen spontan, wohl aber ihre Eier). Pouchet wiederholte Pasteurs Experiment in den französischen Alpen mit dem Unterschied, dass er statt hefehaltigem Zuckerwasser einen Heuaufguss verwendete. In allen acht Fällen veränderte sich der Flascheninhalt, was so wirkte, als ob nur Sauerstoff nötig wäre, um Leben entstehen zu lassen. Als Pasteur verächtlich reagierte, verlangten Pouchet und seine Mitarbeiter eine Untersuchungskommission der Akademie, die 1864 zusammentrat, allerdings mit so vielen Pasteur-Sympathisanten besetzt war, dass ein faires Verfahren nicht gesichert war. Die Kommissionssitzungen zogen sich ergebnislos hin, während sich unter französischen Wissenschaftlern der Eindruck festsetzte, dass die Frage in Pasteurs Sinne entschieden sei. 1876 entdeckten jedoch Ferdinand Cohn und John Tyndall die Tatsache, dass bestimmte Mikroorganismen eine Phase mit Endosporen – die sogar kochendes Wasser überstehen – durchlaufen, was Pouchets Ergebnisse zum Teil erklären würde. Allerdings hatten Pouchet und seine Kollegen auch Mikroorganismen beschrieben, die definitiv nicht so entstanden sein können, wie Myzelien, verschiedene Bakterien und Amöben. Das spricht dafür, dass ihre Versuche auch auf eine andere Weise kontaminiert gewesen sein müssen.

Anhänger der Spontanzeugung konnten immer noch einwenden, dass durch die Erhitzung eine „Lebenskraft“ oder eine andere wesentliche Voraussetzung für die spontane Entstehung von Leben zerstört werde. 1863 gelang es Pasteur, zwei Körperflüssigkeiten zu konservieren, ohne sie zu erhitzen: Urin und Blut. Er gewann sie direkt aus den Venen beziehungsweise der Harnblase von Tieren. Solange er sie nur keimfrei gemachter Luft aussetzte, veränderten sie sich nicht. Pasteur leistete damit einen wesentlich Beitrag zur Technik des aseptischen Arbeitens.

1877 wurde Pasteur erneut herausgefordert, diesmal von dem britischen Wissenschaftler Henry Charlton Bastian, der die spontane Entstehung von Leben in sterilem Urin beobachtet haben wollte. Diesmal war es Pasteur, der eine Untersuchungskommission der Akademie der Wissenschaften anregte. Obwohl Bastian sogar nach Paris reiste, trat die Kommission nie wie geplant zusammen und Bastian fuhr mit leeren Händen nach Hause zurück. Sein Protest führte allerdings dazu, dass Pasteurs Mitarbeiter Jules Joubert und Charles Chamberland sich die Frage noch einmal vornahmen und auf die erstaunliche Hitzeresistenz mancher Mikroorganismen stießen. Ein praktisches Ergebnis dieser Forschungen war der Autoklav.

In einer unveröffentlichten Notiz von 1878 spekulierte Pasteur darüber, dass die spontane Entstehung von Leben doch möglich sein müsse, weil sie am Anfang des Lebens gestanden haben müsse.

Nicolas Appert hatte bereits 1831 in seinem Buch "Le livre de tous les ménages" (deutsch: Das Buch aller Haushalte) eine Methode zur Haltbarmachung von Wein durch Erhitzen veröffentlicht, von der Pasteur später erklärte, dass sie ihm unbekannt gewesen sei. Diese Erklärung ist glaubhaft, weil Appert den Abschnitt erst in einer späteren Auflage hinzugefügt hatte.

Das Konservierungsverfahren für flüssige Lebensmittel, das heute als Pasteurisierung bekannt ist, ergab sich aus Pasteurs Forschungsarbeiten zur Gärung. Dabei werden Lebensmittel auf eine Temperatur unterhalb von 100 °C erhitzt. Es handelt sich nicht um eine Sterilisation, da nur die meisten vegetativen Formen von Mikroorganismen, nicht aber ihre Sporen abgetötet werden. Allerdings erfasst die Pasteurisierung nahezu alle pathogenen Keime. 1867 wurde Pasteur für seine Methode, Wein durch Erhitzen zu konservieren, auf der Weltausstellung zu Paris mit dem Großen Preis ausgezeichnet. 1868 prüfte die Französische Marine den Prozess und übernahm ihn für die Flotte und zur Weinversorgung der Kolonien. Auch Essig ließ sich durch Erhitzen auf 55 °C konservieren, wie Pasteur zeigte. In Österreich und Deutschland wurde die Pasteurisierung von Flaschenbier bei 55 °C populär. Dagegen geht die Pasteurisierung von Milch und Milchprodukten nicht auf Pasteur zurück. Dieser hatte 1861 festgestellt, dass selbst Kochen bei 100 °C Milch nicht zuverlässig sterilisiert, was er auf ihre basische Natur zurückführte. Erst Franz von Soxhlet verwirklichte dann auch die Pasteurisierung von Milch.

In einer Zeit ohne künstliche Textilfasern war die Seide-Industrie für Spanien, Frankreich und Italien von großer Bedeutung, sodass die Krankheiten der Seidenraupen schon früh von Forschern untersucht wurden. Agostino Bassi hatte für die "muscardine" bereits 1835 festgestellt, dass der Erreger ein Pilz war, und damit das erste Beispiel überhaupt für eine Infektionskrankheit geliefert. 1865 befand sich die französische Seide-Industrie in einer schwierigen Lage, weil in den vorangegangenen zwei Jahrzehnten die Produktion wegen einer Krankheit namens "pébrine" auf ein Sechstel geschrumpft war.

In diesem Jahr beauftragte die französische Regierung Pasteur, diese Fleckenkrankheit zu untersuchen. Nach seiner eigenen Aussage hatte er zuvor noch nie mit Seidenraupen zu tun gehabt. Die Forschungsarbeiten zogen sich wegen einer Reihe von persönlichen Unglücken hin (Ende 1865 starb Pasteurs Vater; 1866 starb seine zweijährige Tochter; 1867 brachen Studentenunruhen an der "Ecole Normale" aus, die dort zu seiner Entlassung führten; 1868 erlitt er einen schweren Schlaganfall). Bei der von Microsporidien (in heutiger Nomenklatur: "Nosema bombycis") verursachten Krankheit sind die Körper der Raupen von braunen, pfefferartigen Punkten – wovon sich der französische Name "pébrine" ableitet – übersät. Ihnen entsprechen winzige Kügelchen, die unter dem Mikroskop im Inneren der Raupen sichtbar werden. Nach langen vergeblichen Versuchen kam Pasteur auf die Idee, gesunde Seidenraupen mit Maulbeerblättern zu füttern, die mit den Ausscheidungen von kranken Raupen bestrichen waren. Tatsächlich starben die gefütterten Raupen, allerdings ohne die gefürchteten Punkte zu zeigen, sodass Pasteur den Versuch zunächst für gescheitert hielt. Erst als er ihn von seinem Assistenten Désiré Gernez an einem anderen Ort wiederholen ließ, war Pasteur überzeugt, dass die Punkte nicht nur ein Symptom, sondern Ursache der Krankheit waren. Sie enthielten lebende Krankheitserreger, die nicht aus dem Inneren der Raupen stammten – wie Pasteur ursprünglich angenommen hatte –, sondern von außen die Raupen infizierten.

Nachdem der Charakter der "pébrine" als Infektionskrankheit erkannt war, fand Pasteur heraus, dass sich ein Teil seiner widersprüchlichen Versuchsergebnisse dadurch erklären ließ, dass viele Raupen an einer zweiten Krankheit – der Schlaffsucht (französisch: "flacherie" oder "morts-flats") – gelitten hatten. Die meisten Spezialisten hatten "flacherie" bis dahin für ein Stadium der "pébrine" gehalten. Pasteur fand einen "vibrio", der sich in Fütterungsversuchen übertragen ließ und den er für den Erreger der Krankheit hielt. Auch im Falle der "flacherie" war Béchamp jedoch Pasteur zuvorgekommen und hatte als Krankheitserreger eine Mikrobe identifiziert, die er seinerseits "Microzymas aglaiae" nannte. Heute ist bekannt, dass es sich bei bestimmten Formen der "flacherie" um eine Viruskrankheit handelt, andere Formen durch starke Hitze hervorgerufen werden. Wahrscheinlich haben Pasteur (und Béchamp) damals "Bacillus bombycis" gesehen, eine bakterielle Sekundärinfektion infolge der Viruserkrankung.

Die Vorbeugungsmaßnahmen, die Pasteur empfahl, waren unabhängig vom angenommenen Krankheitsmechanismus: Zu der Zeit, als Pasteur noch von einer konstitutionellen, vererbbaren Krankheit ausging, empfahl er ebenso wie später, als er an eine Infektionskrankheit glaubte, für die Brut nur Elterntiere zu verwenden, die nachweislich frei von dem Krankheitserreger waren. Folgt man den Angaben von Pasteur, so gelang es, die "pébrine" auszurotten, während ähnliche Versuche bei der "flacherie" scheiterten. Aber auch in Bezug auf die "pébrine" erfüllten sich Pasteurs Hoffnungen nicht, weil die Krankheitserreger noch andere Wirte als die Seidenraupen haben. Unabhängig von seinen Forschungsarbeiten ließ sich der Niedergang der französischen Seide-Industrie auch deswegen nicht aufhalten, weil die französische Seide Konkurrenz durch billigere orientalische Seide bekam.

In der älteren Literatur über Pasteur wird fast durchweg behauptet, dass die Fleckenkrankheit der Seidenraupen das erste Beispiel für eine Krankheit gewesen sei, bei der Mikroorganismen als Ursache entdeckt werden konnten. Dies ist falsch. Das erste Beispiel für eine Infektionskrankheit hatte Agostino Bassi bereits 1835 mit der "muscardine" der Seidenraupen geliefert. Sie wird von dem Pilz "Beauveria bassiana" – einem mehrzelligen Organismus – verursacht. Pasteur selbst erwähnte lobend Casimir Davaine, der 1863 gezeigt hatte, dass der Milzbrand-Erreger die Krankheit Milzbrand verursacht, und damit zum ersten Mal eine Bakterie für eine Krankheit verantwortlich gemacht hatte. (Davaine war seinerseits durch die Lektüre von Pasteurs Arbeiten zur Buttersäuregärung angeregt worden.) Die von Pasteur untersuchten Krankheiten der Seidenraupen waren nur weitere Beispiele für ein Konzept, das sich zwar noch nicht durchgesetzt hatte, aber zunehmend glaubwürdiger wurde.
Allerdings glaubte Pasteur, dass noch Zweifel geblieben waren, und untersuchte deswegen ebenfalls den Milzbrand-Erreger, dessen Entwicklungszyklus zu diesem Zeitpunkt jedoch schon weitgehend von Robert Koch aufgeklärt worden war. Pasteurs wichtigster Beitrag war hier der Hinweis auf die Rolle der Regenwürmer, die Milzbrand-Sporen aus begrabenen Tierkadavern wieder an die Erdoberfläche bringen. Daraus ergab sich der Ratschlag an Bauern, an Milzbrand verstorbene Tiere niemals in Boden zu vergraben, den sie als Weide vorgesehen hatten. Dank der Arbeiten Kochs, aber auch Pasteurs wurde Milzbrand zur ersten Krankheit von großen Nutztieren, die allgemein als Infektionskrankheit anerkannt wurde.

1878 hielt Pasteur einen Vortrag über die „Keimtheorie“. Hier machte er erstmals am Beispiel des Erregers der Sepsis ("vibrion septique", heute "Clostridium septicum"; Pasteur ging zunächst davon aus, dass es nur diesen einen Erreger einer Sepsis gebe) die Beobachtung, dass beim selben Erreger Variationen der Virulenz vorkommen. Diese Erkenntnis war eine wichtige Voraussetzung für die Entwicklung seiner Impfstoffe. 1880 fasste Pasteur das Konzept der Infektionskrankheiten in dem großen Vortrag "Über die Erweiterung der Keimtheorie auf die Ätiologie bestimmter allgemein verbreiteter Krankheiten" vor der Akademie der Medizin zusammen. Hier führte er Furunkel, Osteomyelitis und Kindbettfieber auf die Tätigkeit von Mikroorganismen zurück, obwohl er für die ersten beiden Beispiele nur je einen einzigen Patienten untersucht hatte. Als Verursacher machte er eine Bakterie verantwortlich, die später Staphylokokkus genannt wurde.

Pasteurs Terminologie schwankte zunächst zwischen Begriffen wie „Vibrionen“ oder „Infusorien“, bevor er einen Vorschlag des Chirurgen Charles Emmanuel Sédillot (1804–1883) unterstützte, sämtliche Mikroorganismen „Mikroben“ zu nennen. Hiervon leitete er den neuen Fachbegriff „Mikrobiologie“ ab, den er 1881 auf dem Internationalen Medizinkongress in London statt des deutschen Begriffs „Bakteriologie“ vorschlug. Dieser Begriff war tatsächlich angemessener, weil Pasteur nicht nur Bakterien, sondern auch Hefen und – im Falle der Tollwut – sogar Viren untersuchte.

Joseph Lister hatte seine Anregung zur Antisepsis 1865 aus der Lektüre der Schriften Pasteurs über Gärungs- und Fäulnisprozesse erhalten, wie er Pasteur in einem Dankbrief bestätigte. Pasteur war auf diese Anerkennung stolz und ließ den Brief bei verschiedenen Gelegenheiten reproduzieren. Er selbst propagierte schon früh antiseptisches Arbeiten. So sagte er etwa 1874: „Hätte ich die Ehre, ein Chirurg zu sein, würde ich nie ein Instrument irgendeiner Art in den menschlichen Körper einführen, ohne es vor der Operation kochendem Wasser oder besser noch einer Flamme ausgesetzt und dann schnell abgekühlt zu haben.“ Persönlich entwickelte Pasteur eine derartige Angst vor Infektionen, dass er nur widerwillig Hände schüttelte und vor dem Essen sein Geschirr putzte.

1877 beobachteten Pasteur und Joubert, dass eine Milzbrand-Infektion nicht angeht, wenn sie gleichzeitig andere Bakterien injizierten. Sie schienen Stoffe abzugeben, die die Milzbrand-Erreger töteten. Obwohl Pasteur die Hoffnung äußerte, aus dieser Entdeckung ein therapeutisches Prinzip zu entwickeln, wurde sie in ihrer Bedeutung erst 1939 verstanden, als René Dubos das erste Antibiotikum entdeckte, das von einem Bakterium produziert wird.

Vor den Arbeiten Pasteurs zur Geflügelcholera war die einzige bekannte Impfung in der Humanmedizin Edward Jenners Pockenschutzimpfung, das heißt, unter einer „Impfung“ wurde ein besonderes Verfahren zum Schutz vor Pocken verstanden. Ihre Funktionsweise war unklar. 1880 zeigte Pasteur am Beispiel der Geflügelcholera – die nichts mit der Cholera des Menschen zu tun hat –, dass man auch anderen Krankheiten durch eine Impfung vorbeugen kann. Pasteur erweiterte damit den Impfgedanken zu einem allgemeinen Prinzip. Bei Pasteurs erstem Impfstoff handelte es sich um einen Lebendimpfstoff aus abgeschwächten Erregern der Krankheit. Pasteur setzte also eine Impfung gegen eine Krankheit ein, von der bekannt war, dass sie durch einen Erreger verursacht wurde, der außerhalb eines lebenden Organismus kultiviert werden konnte.
In fast allen Büchern über Pasteur wird der Hergang der Ereignisse in einer verklärten Form wiedergegeben, die zuerst von René Vallery-Radot geprägt worden ist. Demnach habe bei der Entwicklung des Impfstoffs ein glücklicher Zufall eine Rolle gespielt, als Pasteur auf eine alte Kultur des Erregers der Geflügelcholera aufmerksam geworden sei, die nicht fortlaufend kultiviert worden, sondern unverändert liegen geblieben war. Pasteur habe diese alte Kultur einigen Hühnern spritzen lassen, die daraufhin nicht erkrankt seien. Beim Spritzen einer neuen, frischen Kultur seien die Hühner weiterhin gesund geblieben. An dieser Erzählung fallen drei Elemente auf: der Zufall, die Zuspitzung auf ein Schlüsselexperiment und die geniale Beobachtungsgabe Pasteurs. Die Darstellung Vallery-Radots ist in fast alle späteren Bücher über Pasteur übernommen worden.

Der italienische Wissenschaftshistoriker Antonio Cadeddu konnte durch das Studium der Labortagebücher Pasteurs (insbesondere „Heft 89“) nachweisen, dass die Ereignisse weniger dramatisch abgelaufen sind. Demnach war der Impfstoff das Ergebnis eines ausgedehnten Forschungsprogramms, das von Pasteurs Mitarbeiter Emile Roux unternommen worden war. Insbesondere war es nach einer Notiz, die Pasteur am 4. März 1880 angefertigt hat, Roux, der während eines Sommerurlaubs seines Chefs die alte Kultur zwei Hühnern gespritzt hatte. Diese Hühner hatten zwar überlebt, waren aber bei der wiederholten Injektion der "alten" Kultur gestorben. Das von Pasteurs Biografen René Vallery-Radot behauptete Schlüsselexperiment hat es also laut Cadeddu nie gegeben, die Idee zu dieser Art von Versuchen kam von Roux, der erste Versuch war erfolglos und auch weitere Versuche verliefen widersprüchlich.

Hervé Bazin, emeritierter Medizinprofessor an der Universität Löwen, hat ebenfalls die Labortagebücher einer genauen Analyse unterzogen. Demnach begann Pasteur sein Forschungsprogramm mit dem festen Ziel, einen Impfstoff zu finden. Anfang 1879 unternahm er dazu zunächst Fütterungsversuche, suchte also nach einer Schluckimpfung. Auch die weiteren Experimente, die Roux vornahm, geschahen auf Anweisung von Pasteur. Als „Schlüsselexperiment“ bezeichnet Bazin einen Versuch vom 5. Januar 1880. In ihm wurden 12 Hühner mit einer frischen Kultur, 12 Hühner mit einer älteren, bereits sauer gewordenen Kultur und weitere 12 Hühner mit einer alten, ebenfalls sauer gewordenen Kultur infiziert. Nach acht Tagen hatte von den mit der älteren Kultur infizierten Hühnern vier überlebt, von der mit der alten Kultur infizierten Hühnern elf. Pasteur konnte also durch die Länge der Kulturpausen die Virulenz des Erregers steuern. Dass eine Kultur mit einem abgeschwächten Erreger sich als Impfstoff eignet, wies er in einem Versuch vom 23. Januar nach, als acht geimpfte Hühner nach Konfrontation mit dem virulenten Erreger gesund blieben.

Typisch für Pasteur war die Voreiligkeit, mit der er an die wissenschaftliche Fachöffentlichkeit ging. Bereits am 22. Januar 1880 machte Pasteur eine vage Ankündigung auf einem veterinärmedizinischen Fachkongress. Am 9. Februar 1880 trug Pasteur über das Thema vor der Akademie der Wissenschaften und am folgenden Tag vor der Akademie der Medizin vor, konnte (laut Cadeddu) oder wollte (laut Bazin) aber noch nicht angeben, wie er den Impfstoff hergestellt hatte, sondern sprach nur von einer „gewissen Änderung in der Kulturweise“.

Pasteurs Verhalten führte in der Akademie der Medizin zu einem Eklat, weil einige Mitglieder glaubten, Pasteur wolle absichtlich die Herstellungsmethode seines Impfstoffs verheimlichen. Der Alterspräsident der Akademie Jules Guérin protestierte scharf, worauf Pasteur in einem Briefentwurf bereits seine Mitgliedschaft in der Akademie kündigen wollte, den Brief aber nicht abschickte. Im Oktober kam es vor der versammelten Akademie zwischen Pasteur und Guérin zu einem wütenden Wortwechsel, der damit endete, dass Guérin Pasteur zum Duell forderte. Pasteur nahm die Herausforderung des 80-jährigen jedoch nicht an. Erst am 26. Oktober 1880 sprach Pasteur die Vermutung aus, dass es der Sauerstoff gewesen war, der während der langen Kulturpausen die Krankheitserreger abgeschwächt hatte. („Abschwächung“ darf nicht so verstanden werden – wie es noch Pasteur tat –, dass der Erreger durch die Kulturbedingungen in seiner Virulenz abgeschwächt worden wäre, sondern durch die Kulturbedingungen wurden einzelne, von vornherein vorhandene, weniger virulente Erreger selektiert und konnten sich vermehren.)

In der Praxis hatte der Impfstoff gegen Geflügelcholera starke Nebenwirkungen, die geimpften Tiere konnten unbemerkt die Krankheit weiterverbreiten, der Impfschutz hielt nur kurz vor, und der Impfstoff war teuer. Geflügelcholera wurde und wird aus diesen Gründen weiterhin durch Keulen der Bestände bekämpft. Seine Bedeutung ist wissenschaftshistorischer Art: Pasteurs Geflügelcholera-Impfstoff war das erste Beispiel für einen Impfstoff, der künstlich im Labor produziert und nicht – wie Jenners Pockenimpfstoff – der Natur entnommen worden war.

Pasteur besaß ein heute eigentümlich wirkendes Verständnis von Immunität. Er betrachtete einen Körper als Kulturmedium für die Krankheitserreger, die nur so lange wachsen konnten, wie das Kulturmedium die dafür notwendigen Nährstoffe enthielt. Waren diese Nährstoffe erschöpft, konnten die Krankheitserreger nicht mehr wachsen, womit der Körper immun geworden war. Die abgeschwächten Erreger in einem Impfstoff vermittelten Immunität, indem sie die für sie notwendigen Nährstoffe aufbrauchten. Impfstoffe konnten in dieser Sicht grundsätzlich nur Lebendimpfstoffe sein. Nur vor diesem Hintergrund wird die Kontroverse um Pasteurs nächstes Projekt – einen Milzbrand-Impfstoff – verständlich.
Als Sohn eines Gerbers muss Pasteur den Milzbrand – eine wichtige Berufskrankheit von Menschen, die mit tierischen Häuten umgehen – gekannt haben. Milzbrand war auch veterinärmedizinisch eine bedeutende Krankheit: Die jährlichen Verluste wurden für Frankreich auf 20 bis 30 Millionen Francs geschätzt. Wieder stellte Pasteur einen Impfstoff her – zumindest erweckte er diesen Anschein –, indem er Milzbrand-Bakterien längere Zeit dem Sauerstoff der Luft aussetzte.

Am 28. Februar 1881 verkündete Pasteur vor der Akademie der Medizin, dass „nichts leichter“ sei, als Schafe, Kühe oder Pferde vor Milzbrand zu schützen, wie er es bereits mit großem Erfolg an Schafen bewiesen habe. Trotzdem reagierte er überrascht, als er – veranlasst von dem Tierarzt Hippolyte Rossignol – vom Landwirtschaftsverein von Melun ("Société d’agriculture à Melun") zu einem öffentlichen Demonstrationsversuch herausgefordert wurde. Der Versuch auf Rossignols Hof in Pouilly-le-Fort (bei Melun, Département Seine-et-Marne) war weltweit der erste öffentliche Demonstrationsversuch eines im Labor entwickelten Impfstoffs. Von insgesamt 50 Schafen impften seine Mitarbeiter an zwei aufeinander folgenden Terminen 25, die übrigen 25 Tiere dienten als Kontrollgruppe. Zuletzt spritzten sie allen Schafen hochvirulente Milzbrand-Bakterien. Zwei Tage später, am 2. Juni 1881, traf Pasteur zum triumphalen Abschluss des Versuchs ein: Von den 25 geimpften Schafen waren 24 gesund geblieben und lediglich ein Mutterschaf – wahrscheinlich an einer anderen Ursache – erkrankt. Von den ungeimpften Tieren waren zu diesem Zeitpunkt 23 gestorben und die beiden übrigen Schafe dem Tod nahe. Zu den Zuschauern gehörte der Pariser Korrespondent der Londoner "Times", der den Versuchsausgang international bekannt machte.

Louis Pasteur hat die Zusammensetzung des in Pouilly-le-Fort verwendeten Impfstoffs nie veröffentlicht, erweckte aber den Eindruck, dass er ihn genauso wie seinen ersten Impfstoff gegen Geflügelcholera hergestellt hätte: als einen durch Sauerstoff abgeschwächten Lebendimpfstoff. Diese Darstellung ist – mit Ausnahme der zunächst wenig beachteten Erinnerungen seines Neffen Adrien Loir – von den gängigen Pasteur-Biografen übernommen worden. Loir schrieb dagegen 1937, dass Pasteur den Impfstoff für Pouilly-le-Fort mit Kaliumdichromat habe versetzen lassen. Dieser Impfstoff sei von Chamberland und Roux hergestellt worden.
Die Wissenschaftshistoriker Antonio Cadeddu und Gerald L. Geison haben unabhängig voneinander durch Auswertung von Pasteurs Labortagebüchern (in diesem Fall „Heft 91“) nachgewiesen, dass Loirs Darstellung tatsächlich zutrifft. Heikel wird dieses Ergebnis dadurch, dass Pasteur und seine Mitarbeiter damit auf ein Verfahren zurückgriffen, das von Pasteurs Konkurrenten Henry Toussaint entwickelt worden war (Toussaint hatte zuvor Pasteurs Forschungen zur Geflügelcholera ermöglicht, indem er ihm eine Kultur des erst kurz zuvor entdeckten Erregers überlassen hatte). Anders als Pasteur verfolgte Toussaint das Konzept eines Totimpfstoffs, für den er die Bakterien durch Hitze oder Chemikalien abtötete. 1880 hatte er einen Milzbrand-Impfstoff aus erregerhaltigem Blut erprobt, für den er in einer Variante die Bakterien mit Phenol versetzt, in einer anderen Variante die Bakterien mit Hitze abgetötet hatte. Geison konnte anhand von Pasteurs Labortagebüchern und privater Briefe zeigen, wie groß Pasteurs Irritation war, als dieser erfuhr, dass Toussaints Konzept eines Totimpfstoffes funktionierte, während sein eigener Impfstoff noch unbefriedigende Resultate lieferte. „Dies stellt alle Vorstellungen, die ich über Krankheitserreger oder Impfstoffe hatte, auf den Kopf. Ich verstehe nichts mehr“, schrieb er an seinen Kollegen aus der Akademie der Medizin, den Veterinärmediziner Henri Bouley. Pasteur veranlasste sofort, dass seine Mitarbeiter die Versuche Toussaints wiederholten. Sie erhitzten erregerhaltiges Blut zehn Minuten lang auf 55 °C, fanden aber noch lebensfähige, wenn auch abgeschwächte Bakterien, wie es ins Impfstoff-Konzept von Pasteur passte.

Roux hatte aber auch das Toussaintsche Verfahren mit Phenol und in einer Abwandlung mit Kaliumdichromat erprobt. Erst zwei Jahre später veröffentlichten Roux und Chamberland diese Methode ohne jeglichen Hinweis darauf, dass sie in Pouilly-le-Fort zum Einsatz gekommen war. Durch die Ergebnisse von Pouilly-le-Fort galt Pasteur in der Öffentlichkeit als eindeutiger Sieger im Wettlauf um einen Milzbrand-Impfstoff. Obwohl Toussaints Lehrer Auguste Chauveau 1882 noch einmal auf die Priorität seines Schülers hinwies, geriet Toussaints Name bald in Vergessenheit. Geison hat Pasteurs Verhalten in diesem Fall als Wissenschaftsbetrug gewertet.

Allein schon der Umstand, dass Pasteur seine Labortagebücher nicht vernichtet hat, lässt jedoch daran zweifeln, dass er Wissenschaftsbetrug beabsichtigte. Zu einer anderen Einschätzung als Geison kommt Hervé Bazin. Pasteurs Irritation rührte demnach aus seiner Überraschung, dass in Person von Henry Toussaint so schnell ein Konkurrent auf dem Feld der Impfstoff-Entwicklung aufgetaucht war, obwohl er in seinem Vortrag vom 9. Februar 1880 bewusst vage geblieben war. Auch Bazin gesteht die Priorität für den Milzbrand-Impfstoff Toussaint zu. (Außer Toussaint kommt allerdings noch der Brite William Greenfield in Frage, der nach dem Vorbild der Pockenschutzimpfung Rinder mit Milzbrand-Erregern aus Nagetieren impfte.) Pasteurs Impfstoff sei jedoch objektiv überlegen gewesen, weil Toussaint kein Kulturverfahren für Milzbrand-Bakterien besaß und deswegen als Grundlage für den Impfstoff auf das Blut infizierter Tiere zurückgreifen musste. Die Qualität von Toussaints Impfstoff wechselte aus diesem Grund stark, während Pasteur einen stabilen Impfstoff garantieren konnte.

Toussaint revidierte später unter nicht ganz klaren Umständen die Interpretation seines Impfstoffs als Totimpfstoff und erklärte öffentlich, dass er die Bakterien wohl nur abgeschwächt, aber nicht abgetötet habe. Pasteur sah also keine Notwendigkeit, aufgrund der Toussaintschen Versuche seine eigenen Vorstellungen von der Funktionsweise eines Impfstoffs zu revidieren. Zum Zeitpunkt des Versuchs von Pouilly-le-Fort hatte Pasteur mehrere verschiedene Impfstofftypen für die Milzbrand-Schutzimpfung in der Entwicklung (mit Hitze und Sauerstoff bzw. mit Kaliumdichromat abgeschwächt), sodass es nach Ansicht von Bazin vernünftig erscheine, wenn er sich öffentlich noch nicht auf ein Verfahren festlegen wollte. Pasteur habe Toussaint nicht plagiiert. Cadeddu hat noch auf ein mögliches Motiv für Chamberland und Roux hingewiesen, sich mit Kaliumdichromat zu beschäftigen. Pasteur war es bis zum Versuch von Pouilly-le-Fort nicht gelungen, durch Aussetzen an Sauerstoff sporenfreie Kulturen des Milzbrand-Erregers zu erzeugen. Mit Kaliumdichromat gelang es dagegen Chamberland und Roux, sporenfreie Kulturen herzustellen, die für die Impfstoffproduktion geeignet und gleichzeitig durch die Chemikalie abgeschwächt worden waren.

Noch 1881 bot Pasteur dem französischen Staat an, eine staatliche Fabrik für Milzbrand-Impfstoff aufzubauen, wenn er im Gegenzug aller materieller Sorgen enthoben würde. Der französische Staat lehnte damals ab, sodass Pasteur den Impfstoff weiter in seinem Labor produzierte. Die Nachfrage war jedoch bald so groß, dass Pasteur und seine Mitarbeiter in industrielle Dimensionen vorstießen. Für die Produktion wurde ein eigenes Labor unter der Leitung von Charles Chamberland in der rue Vauquelin eingerichtet. Eine eigens gegründete Firma, die "Société de Vulgarisation du Vaccin Charbonneux", später umbenannt in "Société du Vaccin Charbonneux Pasteur", übernahm den Vertrieb. Die Zahl der verkauften Impfstoffdosen (eine vollständige Impfung erforderte zwei Dosen) nahm steil zu: 1881 – noch im Jahr des Versuchs von Pouilly-le-Fort – wurden 164.000 Dosen verschickt, 1882 bereits 700.000, 1885 900.000. Dabei kam es 1882 laborintern zu einer schweren Krise, weil sich der Impfstoff nicht als so stabil wie erhofft herausstellte, was zahlreiche weitere Versuche und die Einführung einer strikten Qualitätskontrolle erforderte. Nach französischem Recht war es nicht möglich, Impfstoffe patentieren zu lassen, aber der Produktionsprozess war so kompliziert, dass das Unternehmen über viele Jahre ein Monopol wahren konnte. Der Verkaufspreis für eine Impfstoffdosis betrug 2,50 Francs. Zwei Fünftel des Gewinns standen Pasteur zu, je ein Fünftel Chamberland und Roux, das letzte Fünftel wurde einer Rücklage zugeführt.

Anfangs wurde ein Impfstoff produziert, der mit Kaliumdichromat abgeschwächt worden war. Er wurde durch einen Impfstoff ersetzt, den Pasteur nach seinem eigenen Verfahren entwickelt hatte: Bei ihm wurden die Bakterien dem Sauerstoff der Luft ausgesetzt – allerdings bei einer genau einzuhaltenden Temperatur von 42 bis 43 °C (bei dieser Temperatur entwickelten die von Pasteur verwendeten Milzbrand-Bakterien keine Sporen). Innerhalb von acht Tagen verloren die Bakterien so ihre Virulenz vollständig. Wurde der Prozess früher abgebrochen, so konnte jeder beliebige Grad von Virulenz eingestellt werden und laut Pasteur auch beliebig lange konserviert werden.

Mit seinen Studien zum Milzbrand hatte sich Pasteur auf ein Gebiet begeben, das von Robert Koch beherrscht wurde. 1877 erkannte Pasteur die Leistungen Kochs (und Davaines) an. Andererseits beanspruchte er aber auch wiederholt, die bakteriellen Endosporen zuerst entdeckt zu haben, und zitierte dafür seine Arbeiten über die "flacherie". In seinen Augen hatte Koch also nur eine Entdeckung bestätigt, die er zuvor gemacht hatte. 1881 enthielt der erste Band der „Mittheilungen aus dem Kaiserlichen Gesundheitsamte“ mehrere Artikel von Robert Koch und seinen Schülern, in denen sie die Pasteurschen Milzbrandversuche in zahlreichen Punkten angriffen: Die Pasteurschen Milzbrandimpfstoffe seien mit anderen Bakterien kontaminiert, Pasteur habe zahlreiche Krankheiten miteinander verwechselt, er habe Prioritätsansprüche von Koch missachtet, und Regenwürmer spielten nicht die von Pasteur behauptete Rolle im Lebenszyklus des Milzbrand-Erregers.

Als Pasteur 1881 auf dem Internationalen Medizinkongress in London Koch traf, war er sich dieses Angriffs anscheinend noch nicht bewusst, und er lobte Kochs feste Kulturmedien als großen Fortschritt. 1882, auf dem Internationalen Kongress für Hygiene und Demographie in Genf, griff er dagegen die Koch-Schule scharf an. Koch verzichtete auf eine Erwiderung und versprach, schriftlich zu reagieren, was er auch tat. In dieser Veröffentlichung lobte er plötzlich das Konzept der Impfstoffe aus abgeschwächten Krankheitserregern, sprach jedoch die Priorität Toussaint zu. Nach wie vor hielt er die Pasteurschen Impfstoffe für unsauber. Pasteur antwortete 1882 wiederum in einem sarkastisch gehaltenen offenen Brief. Bei der Heftigkeit der Auseinandersetzung muss der Altersunterschied – Koch war 21 Jahre jünger – und der teilweise unverschämte Ton, den Koch anschlug, berücksichtigt werden.

Pasteur entwickelte noch einen Impfstoff gegen eine weitere veterinärmedizinisch bedeutende Krankheit, den Schweinerotlauf. (Der Erreger, der heute "Erysipelothrix rhusiopathiae" genannt wird, war 1882 von seinem Mitarbeiter Louis Thuillier isoliert worden.) Hierfür setzte er erstmals eine neuartige Methode der Abschwächung ein, indem er den Erreger fortlaufend von Kaninchen zu Kaninchen übertrug. Der Impfstoff gegen Schweinerotlauf war wirtschaftlich – abgesehen von Ungarn – kein großer Erfolg.

Pasteur zögerte lange, Impfstoffe an Menschen anzuwenden, sodass Jaime Ferrán ihm 1885 mit einer Schutzimpfung gegen Cholera zuvorkam. Dessen Impfstoff aus Cholerabakterien war nach der Pockenschutzimpfung der erste Menschen verabreichte Impfstoff; ob er wirksam war, ist unter Medizinhistorikern umstritten. Pasteur arbeitete an seinem ersten humanmedizinischen Impfstoff ab 1881. Dafür wählte er eine auf den ersten Blick ungewöhnliche Krankheit, die Tollwut. Da Viren im modernen Sinne des Wortes noch unbekannt waren, gab es hier keine sichtbaren Krankheitserreger, mit denen er hätte experimentieren können. Tollwut war humanmedizinisch unbedeutend, jedoch wegen des grausamen Todes gefürchtet: Durchschnittlich starben in Frankreich pro Jahr etwa 30 Menschen daran. Pasteurs Wahl wird häufig auf ein Kindheitserlebnis von 1831 zurückgeführt, als in seiner Heimatstadt Arbois mehrere Menschen von einem tollwütigen Wolf gebissen worden waren. Der achtjährige Louis Pasteur beobachtete damals die traditionelle Behandlung – Ausbrennen der Bisswunde mit einem glühenden Eisen durch den Dorfschmied –, was er für den Rest des Lebens nicht mehr vergessen haben soll.
Tollwut hatte den experimentellen Vorteil, dass die Krankheit bei Menschen und Tieren vorkam, sodass Hunde als Versuchstiere benutzt werden konnten. Die Inkubationszeit betrug ein bis zwei Monate, was Zeit für eine Intervention durch Impfung ließ. Außerdem etablierte Pasteur als Versuchstier das Kaninchen, das gegenüber dem Hund eine nur etwa halb so lange Inkubationszeit aufwies und sich erheblich sicherer handhaben ließ. Kaninchen waren aus diesen Gründen bereits von Pierre Victor Galtier, Professor an der Tierarzneischule von Lyon, vorgeschlagen worden. Auch hatte Galtier 1881 einen Tollwut-Impfstoff an Schafen erprobt, wofür er – analog zur Pockenschutzimpfung – den Wild-Erreger selbst übertragen hatte und nicht einen künstlich im Labor abgeschwächten Erreger. Da Galtier seine Ergebnisse in den „Abhandlungen der Akademie der Wissenschaften“ publizierte, muss Pasteur Kenntnis von seinen Arbeiten gehabt haben. Allerdings erwähnte er ihn nur ein einziges Mal, und auch das nur, um ihn zu kritisieren.

Pasteur entwickelte zunächst eine Methode, den Tollwut-Erreger durch aufeinander folgende Übertragungen auf Affen abzuschwächen; als Maß der Abschwächung diente die Inkubationszeit. Bis 1884 hatte er auf diese Weise einen Impfstoff entwickelt, der Hunde vor einer Infektion durch Tollwut schützte, was er sich durch eine Kommission des Unterrichtsministers bestätigen ließ. Allerdings waren bei diesen Versuchen die Hunde immer erst geimpft und dann mit dem Krankheitserreger infiziert worden.

Der Wissenschaftshistoriker Gerald L. Geison hat als Erster anhand der Labortagebücher von Pasteur nachgewiesen, dass Pasteurs Arbeitsgruppe diesen Impfstoff an zwei Menschen erprobt hat, ohne dass die Öffentlichkeit je davon erfuhr. Im ersten Fall erhielt ein Mann namens Girard am 2. Mai 1885 eine einzige Injektion, bevor das übergeordnete Ministerium reagierte und Pasteur jede weitere Behandlung untersagte. In seinem Labortagebuch – das für diesen Fall die einzige Quelle ist – verfolgte Pasteur das weitere Schicksal Girards, der nach einer schweren Krise wieder genas, nur bis zum 25. Mai. Er selbst zeigte sich überzeugt, dass Girard aufgrund der einmaligen Behandlung genesen war, angesichts der langen Inkubationszeit kann aber tatsächlich nicht beurteilt werden, ob der Patient nicht doch später an Tollwut erkrankte. Im zweiten Fall – für den es außer Pasteurs Labortagebuch auch noch einen Augenzeugenbericht des behandelnden Arztes gibt – war ein elfjähriges Mädchen namens Antoinette Poughon erst sechs Wochen nach der Infektion, als es bereits die ersten Symptome zeigte, in die Behandlung gekommen. Es starb, nachdem es zwei Spritzen erhalten hatte. Verwunderlich ist hier vor allem, dass Pasteur bei diesem offensichtlich aussichtslosen Fall überhaupt eine Therapie versuchte. Für die hier angewandte Reihenfolge – Impfung nach einer bereits erfolgten Tollwut-Infektion – hatte Pasteur bis dahin mit einer kleinen Ausnahme keinen Tierversuch unternommen. Nur ein einziges Mal hatte er versucht, ein bereits tollwütiges Kaninchen mit einer Serie von Impfungen zu heilen; das Kaninchen war aber gestorben.
Hätte die Öffentlichkeit von diesen vorausgegangenen Therapieversuchen erfahren, hätte sie wahrscheinlich längst nicht so enthusiastisch auf die angebliche oder tatsächliche Heilung des jungen Joseph Meister reagiert, die Pasteur am 26. Oktober 1885 bekannt gab. Die Nachricht erzeugte eine Sensation. Aus aller Welt strömten von vermeintlich tollwütigen Tieren gebissene Menschen nach Paris, und ein gutes Jahr später waren allein dort bereits 2500 Patienten behandelt worden. Nach Pasteurs eigenen Angaben werden nur 10 Prozent der von tollwütigen Hunden gebissenen Menschen tatsächlich infiziert. Die „Heilung“ von Joseph Meister war also keinesfalls ein Beleg für die Wirksamkeit des Impfstoffs, auch wenn sie von der Öffentlichkeit so verstanden wurde. Erst die statistische Auswertung einer großen Zahl von Fällen ergab zweifelsfrei, dass Pasteurs Tollwut-Impfstoff tatsächlich wirksam war.
Nach den gescheiterten bzw. unklaren Ergebnissen in den ersten beiden Therapieversuchen hatte Pasteur für die Herstellung „eine andere Methode“ – wie er sie schlicht in seinem Labortagebuch nennt – verwendet. Der bei Meister eingesetzte Impfstoff bestand aus dem emulgierten Rückenmark – das zwei Wochen lang an der Luft getrocknet hatte – eines an Tollwut verstorbenen Kaninchens. Pasteur behauptete, dass er den Impfstoff an 50 Hunden erprobt hätte und in allen Fällen erfolgreich gewesen sei. Tatsächlich belegen die Labortagebücher, dass zu der speziellen Methode, mit der Joseph Meister behandelt worden war, die Tierexperimente noch nicht abgeschlossen waren. Meister erhielt eine Serie von 13 Injektionen, wobei Pasteur zunehmend frischeres Rückenmark mit zunehmend virulenteren Tollwut-Erregern verwendete.
Von Pasteur nie öffentlich anerkannt wurde der Anteil seines wichtigsten Mitarbeiters am Tollwut-Impfstoff Emile Roux. Wie sich ebenfalls erst aus den Erinnerungen von Adrien Loir ergab, hatte Roux das Verfahren entwickelt, das Rückenmark eines tollwütigen Tiers an einem Faden hängend in einer Flasche aufzubewahren und auf diese Weise das Rückenmark zu trocknen, ohne dass es verfaulte (Pasteur verbesserte noch die Methode, indem er Kaliumhydroxid als Trocknungsmittel hinzufügte). Roux war 1883 zum Dr. med. promoviert worden, weil Pasteur selbst als Nicht-Mediziner keine Menschen behandeln durfte, und hätte eigentlich die Impfungen vornehmen sollen. Im konkreten Fall des Joseph Meister scheint Roux sich geweigert zu haben, einen Menschen mit einem Impfstoff zu behandeln, den er persönlich noch nicht für ausgereift hielt, sodass Pasteur die Hilfe anderer Ärzte in Anspruch nehmen musste. Etwas anders bewertet Hervé Bazin das Geschehen. Er kann nachweisen, dass Pasteur das Trocknungsverfahren für erregerhaltiges Rückenmark schon vor dem Zeitpunkt verwendet hatte, an dem er es nach Angaben von Loir von Roux kopiert haben soll. Aber auch laut Bazin weigerte sich Roux, die Veröffentlichung vom 26. Oktober 1885 zu unterzeichnen, weil er die Anwendung am Menschen für voreilig hielt.

Insgesamt hat Pasteur vier verschiedene Impfstoffe entwickelt und damit nachgewiesen, dass man – zumindest im Prinzip – fortan vor beliebigen Infektionskrankheiten durch eine Impfung schützen konnte. Auch wenn die Umstände bis heute umstritten sind, hat Pasteur in allen Fällen am Ende ein wirksames Produkt geschaffen.

Durch den spektakulären Erfolg der „Heilung“ von Joseph Meister traf eine Flut von Spenden ein. Der für die Gründung eines "Institut Pasteur" aufgelegte Fonds schwoll auf 2,6 Millionen Francs an. Am 14. November 1888 wurde es in Anwesenheit von Präsident Sadi Carnot eingeweiht, Pasteur sein erster Direktor. Von seinem Status her war das "Institut Pasteur" eine private Institution, die Leiter der fünf Sektionen sowie Pasteur selbst erhielten ihr Gehalt jedoch weiterhin vom Staat.

Um Spenden war mit dem Motiv geworben worden, ein Zentrum der Tollwutschutzimpfung zu schaffen, doch das "Institut Pasteur" war von Anfang weit mehr: das erste Forschungsinstitut für Medizinische Mikrobiologie. Sein Vorbild wurde in aller Welt kopiert, so mit dem 1891 gegründeten Preußischen Institut für Infektionskrankheiten in Berlin, dem Lister-Institut von 1891 in London, dem Gamaleya-Institut von 1891 in Moskau und dem Kitasato-Institut in Tokio, das auf einen Vorläufer von 1892 zurückgeht. Die Institutionalisierung der Mikrobiologie drückte sich auch in Fachzeitschriften aus, darunter die 1887 von Emile Duclaux gegründeten "Annales de l’Institut Pasteur".

Ebenso entstanden in aller Welt Tollwut-Impfdienste, die häufig zum Keim eines weiteren "Institut Pasteur" wurden. In Russland, wo Tollwut ein großes Problem war, wurde bereits ab 1886 in Odessa gegen Tollwut geimpft. Weitere Institute entstanden in St. Petersburg, Moskau, Samara und Warschau. Ebenfalls schon 1886 öffnete ein Tollwut-Impfdienst in New York, aber auch zum Beispiel 1891 unter Albert Calmette in Saigon und 1893 in Tunis. Heute tragen mehr als 100 medizinische Institute und Wissenschaftszentren den Namen Louis Pasteurs, häufig auch ohne dass sie unmittelbar mit dem "Institut Pasteur" zu tun haben.

Es ist häufig festgestellt worden, dass bei den Arbeiten von Pasteur eine strikte Trennung zwischen Grundlagenforschung und angewandter Forschung nicht möglich ist. Pasteur bearbeitete mit großem Elan anwendungsbezogene Probleme und stieß dabei regelmäßig zu Erkenntnissen von grundsätzlicher Bedeutung vor. Seine Karriere war von Kontroversen begleitet, wobei zu berücksichtigen ist, dass die Diskussionskultur im Wissenschaftsbetrieb des 19. Jahrhunderts generell stärker von Polemik geprägt war als heute. Er behauptete zwar, Kritik gegenüber offen zu sein, reagierte aber, wenn er tatsächlich kritisiert wurde, empfindlich. Seine Methode der Beweisführung hatte eine starke rhetorische Komponente. So veranstaltete er öffentliche Demonstrationen von Experimenten, um einen Sachverhalt zu belegen, und verlangte nach Untersuchungskommissionen, um einen wissenschaftlichen Streit zu entscheiden. Auf die Auswahl der Kommissionsmitglieder nahm er allerdings Einfluss, sodass für seinen wissenschaftlichen Gegner kein faires Urteil gesichert war. Wissenschaftler der Tierarzneischule von Turin, mit denen Pasteur über die Frage der Wirksamkeit des Milzbrand-Impfstoffs in Konflikt geraten war, verglichen Pasteur mit einem „Duellanten, der jeden herausfordert, der ihm zu widersprechen wagt oder ihn auch nur scharf anschaut, der aber die Gewohnheit hat, sich die Wahl der Waffen vorzubehalten, und seine Gegner verpflichtet, mit gebundenen Händen zu kämpfen“.

Als Reaktion auf ein für ihn unangenehmes Erlebnis bat Pasteur 1878 seine Familie, seine Labortagebücher nie jemandem zu zeigen. Nach dem Tod des Physiologen Claude Bernard hatte einer von dessen Schülern Notizen veröffentlicht, wonach Bernard an Pasteurs Theorie zur Gärung gezweifelt habe. Dies zwang Pasteur dazu, gegen den eigentlich von ihm verehrten Bernard öffentlich Stellung zu beziehen. Um nicht selbst eine ähnliche Situation zu provozieren, verhängte er das Veröffentlichungsverbot über seine Labortagebücher.

1964 übergab Pasteur Vallery-Radot – damals der letzte überlebende der direkten männlichen Nachkommen – die Labortagebücher der Französischen Nationalbibliothek. Sie wurden mit dem Tod von Pasteur Vallery-Radot 1971 zugänglich, allerdings erst mit dem Katalog von 1985 praktisch nutzbar. Insgesamt bestehen sie aus 144 Notizbüchern, von denen 42 mit Zeitungsausschnitten, Vorlesungsnotizen etc. gefüllt sind. Die übrigen 102 Notizbücher sind die eigentlichen Labortagebücher und dokumentieren 40 Jahre Forschungsarbeit.

Anlässlich des 100. Todestages von Pasteur 1995 veröffentlichte der US-amerikanische Wissenschaftshistoriker Gerald L. Geison das Buch "The Private Science of Louis Pasteur", in dem er anhand der Labortagebücher nachwies, dass die Geschichte von Pasteurs Versuchen in einigen Fällen anders abgelaufen ist, als seine Veröffentlichungen nahelegen. Das Buch verursachte in Frankreich, obwohl es nie übersetzt wurde, einen Skandal. Eine ähnliche Stoßrichtung haben die Veröffentlichungen des Italieners Antonio Cadeddu. Die Debatte wird nach wie vor von der nationalen Herkunft bestimmt: Während die Kritiker aus dem Ausland kommen, wird Pasteur von Franzosen wie Patrice Debré und Hervé Bazin in Schutz genommen.

1883 wurde Pasteur in die American Academy of Arts and Sciences und die National Academy of Sciences gewählt. 1887 wurde das Institut Pasteur gegründet. Im ersten Gebäude des Instituts wohnte Pasteur in seinen letzten Lebensjahren (ab 1888). In einem Teil dieses Gebäudes ist seit 1936 das "Musée Pasteur" eingerichtet. Weitere Museen existieren in seinen ehemaligen Wohnhäusern in Dole und Arbois (siehe Maison de Louis Pasteur).

Zahlreiche Denkmale sind zu seiner Ehre errichtet worden. Zeitweise galt Pasteur in französischen Umfragen noch vor Napoleon als der bedeutendste Franzose, der je gelebt hat.

Während sein Andenken in Deutschland zurückhaltend gepflegt wurde, war Pasteur vor allem auch in Russland populär. Zar Alexander III. gehörte mit einem Beitrag von 100.000 Francs zu den großzügigsten Spendern für das "Institut Pasteur". Zahlreiche russische Wissenschaftler kamen nach Paris, unter ihnen der künftige Nobelpreisträger Ilja Metschnikow, in dessen Sektion am "Institut Pasteur" sich zeitweise eine russische Kolonie bildete.

Nach Pasteur benannt ist die Bakterienfamilie der Pasteurellaceae mit der Gattung "Pasteurella". Die von "Pasteurella multocida" verursachten Krankheiten werden als „Pasteurellosen“ bezeichnet. Auch der Asteroid (4804) Pasteur trägt seinen Namen.

Nach Pasteur wurde sein "Collège" in Arbois, ein Ort in Algerien sowie ein Bezirk in Kanada benannt. Mehr als 2000 Straßen Frankreichs tragen seinen Namen, darunter der "Boulevard Pasteur", eine große Verkehrsader von Paris. Die Métro fährt dort die Station "Pasteur" an. Ihm zu Ehren tragen auch die Pasteur-Halbinsel und die Pasteur-Insel in der Antarktis seinen Namen.

Pasteur Vallery-Radot (Hrsg.): "Œuvres de Pasteur". Masson, Paris

Pasteur Vallery-Radot (Hrsg.): "Correspondance de Pasteur". Flammarion, Paris.

Ein Bestandteil des wissenschaftlichen Werks sind auch Pasteurs Labortagebücher.

Maßgeblich und bis heute die Diskussion bestimmend, wenn auch der Autor nicht den Anspruch erhebt, eine vollständige Biografie zu liefern, ist Gerald L. Geison: "The Private Science of Louis Pasteur". Princeton University Press, Princeton 1995.







"The Story of Louis Pasteur" (deutsch: "Louis Pasteur"). Regie: William Dieterle. 1936. Mit einem Oscar für Paul Muni in der Rolle als Pasteur.



</doc>
<doc id="13723" url="https://de.wikipedia.org/wiki?curid=13723" title="EG">
EG

EG steht für:

EG als Unterscheidungszeichen auf Kfz-Kennzeichen:

eG bzw. e.G. steht für:


eg steht für:

e.g. steht für:

Siehe auch:


</doc>
<doc id="13724" url="https://de.wikipedia.org/wiki?curid=13724" title="Europäische Gemeinschaft">
Europäische Gemeinschaft

Die Europäische Gemeinschaft (EG) war eine supranationale Organisation, die mit dem Vertrag von Maastricht 1993 aus der 1957 gegründeten Europäischen Wirtschaftsgemeinschaft (EWG) hervorging. Sie war als eine der drei Europäischen Gemeinschaften Teil der ersten und wichtigsten der . Der Rechtskörper der Europäischen Gemeinschaft war damit das Kernstück der Europäischen Union (EU). Grundlage der EG war der Vertrag zur Gründung der Europäischen Gemeinschaft (EG-Vertrag).

Bereits vor Inkrafttreten des Vertrages von Lissabon hatte die Bezeichnung "Europäische Union" in der Umgangssprache die "Europäische Gemeinschaft" ersetzt, jedoch blieben EU und EG juristisch unterschiedliche Institutionen. Anders als die „Dachorganisation“ EU verfügte die EG über eine eigene Rechtspersönlichkeit und damit völkerrechtliche Handlungsfähigkeit. Mit Inkrafttreten des Vertrages von Lissabon am 1. Dezember 2009 wurde die Existenz der EG beendet. Ihre Rechtsnachfolgerin wurde die Europäische Union, die durch den Vertrag nun selbst Rechtspersönlichkeit erhielt. Der EG-Vertrag wurde in "Vertrag über die Arbeitsweise der Europäischen Union" (AEU-Vertrag) umbenannt.

Die Vorgängerorganisation der Europäischen Gemeinschaft, die Europäische Wirtschaftsgemeinschaft (EWG), wurde am 25. März 1957 in Rom von den sechs Mitgliedstaaten (Belgien, Bundesrepublik Deutschland, Frankreich, Italien, Luxemburg, Niederlande) der Europäischen Gemeinschaft für Kohle und Stahl (EGKS, oft auch Montanunion genannt) gegründet. Grundlage war der Vertrag zur Gründung der Europäischen Wirtschaftsgemeinschaft (abgekürzt EWG-Vertrag), dessen Inhalte im Wesentlichen auf der Konferenz von Messina erarbeitet worden waren. Gleichzeitig gründeten die Staaten auch die Europäische Atomgemeinschaft (EAG oder Euratom); EWG-Vertrag und Euratom-Vertrag werden daher zusammen als die Römischen Verträge bezeichnet. Zusammen mit der bereits 1951 gegründeten EGKS bestanden nun also drei Gemeinschaften, die zusammen auch als Europäische Gemeinschaften (EG) bezeichnet werden. 1967 wurden die Organe dieser drei Gemeinschaften durch den EG-Fusionsvertrag zusammengelegt. Am 11. Mai 1967 beantragte die britische Regierung (unter Harold Wilson (Labour), Premierminister von Oktober 1964 bis Juni 1970) den Beitritt zur EG; am 27. November 1967 kündigte Charles de Gaulle sein bzw. Frankreichs Veto gegen den Beitritt Großbritanniens an und legte es in der Sitzung des EWG-Ministerrates am 19. Dezember 1967 auch ein.

Zum 1. Januar 1973 traten Großbritannien, Irland und Dänemark der EG bei („EG-9“). Am 1. Januar 1981 trat Griechenland bei („EG-10“), am 1. Januar 1986 Spanien und Portugal („EG-12“); weiteres siehe hier.

Mit der Gründung der Europäischen Union (EU) durch den 1993 in Kraft getretenen Vertrag von Maastricht wurde die EWG in „Europäische Gemeinschaft“ (EG) umbenannt, aus dem EWG-Vertrag wurde der EG-Vertrag. Mit dieser Änderung sollte die qualitative Veränderung der EWG von einer reinen Wirtschaftsgemeinschaft hin zu einer umfassenden politischen Organisation, die etwa auch umwelt- und sozialpolitische Fragen behandelte, zum Ausdruck gebracht werden. An der Existenz der drei Teilgemeinschaften (EGKS, EAG, EG) änderte diese Umbenennung allerdings nichts, da mit ihr keine formelle Vereinigung der drei Gemeinschaften verbunden war. Die Europäische Union selbst war als Dachorganisation konstruiert, die neben den drei Gemeinschaften noch als weitere Politikbereiche die Gemeinsame Außen- und Sicherheitspolitik und die Zusammenarbeit im Bereich Justiz und Inneres umfasste. Für diese galten jedoch andere Entscheidungsverfahren als für die EG.

Im Zuge der Gründung der EU benannten sich auch einige Organe der EG um:
Die von den einzelnen Organen erlassenen Rechtsakte blieben allerdings weiterhin Rechtsakte der jeweiligen Gemeinschaft.

Da die Bedeutung der EGKS immer geringer wurde und die Euratom nur eine spezialisierte Aufgabe hat, bildete die Europäische Gemeinschaft das Herz der Europäischen Gemeinschaften. Die Abkürzung „EG“ konnte die drei Gemeinschaften als Ganzes "oder" die Europäische Gemeinschaft allein bezeichnen. Die drei Gemeinschaften wiederum bildeten die erste und wichtigste der drei Säulen der Europäischen Union. Ziel der EG war die Errichtung eines Binnenmarktes und – darauf aufbauend – einer europäischen Wirtschafts- und Währungsunion. Daneben hatte sie Zuständigkeiten in weiteren Politikbereichen wie Verkehr, Soziales, Umwelt, Forschung und Technologie, Gesundheit, Bildung, Kultur, Verbraucherschutz, Entwicklung. In den Vertragsreformen von Amsterdam 1997 und Nizza 2001 wurden zudem verschiedene Bereiche der Zusammenarbeit im Bereich Justiz und Inneres in den EG-Vertrag aufgenommen, d. h. „vergemeinschaftet“.

Der Vertrag über die Europäische Gemeinschaft für Kohle und Stahl lief nach 50 Jahren Laufzeit 2002 aus. Durch den Vertrag von Nizza von 2001 (2003 in Kraft getreten) wurden die EGKS-Bestimmungen in den EG-Vertrag eingegliedert. Die EGKS selbst wurde aufgelöst.

Mit Inkrafttreten des Vertrages von Lissabon, am 1. Dezember 2009, wurde auch die Europäische Gemeinschaft schließlich aufgelöst. Ihre Rechtsnachfolgerin wurde die Europäische Union, die durch denselben Vertrag erstmals Rechtspersönlichkeit erhielt. Formal erfolgte diese Fusion von EG und EU dadurch, dass im EG-Vertrag durchgängig die Bezeichnung „Europäische Gemeinschaft“ durch „Europäische Union“ ersetzt wurde; der Vertrag selbst wurde in Vertrag über die Arbeitsweise der Europäischen Union umbenannt.
Der Versuch, EG- und EU-Vertrag zu einem einheitlichen Vertragstext zusammenzulegen, war 2005 mit dem EU-Verfassungsvertrag gescheitert.

Die Organe der EG waren gemäß Abs. 1 EG-Vertrag (vor Inkrafttreten des Vertrages von Lissabon):


Diese Organe waren gleichzeitig für die gesamte Europäische Union ("einheitlicher institutioneller Rahmen") tätig.

Der Rat und die Kommission wurden gemäß Art. 7 Abs. 2 EG-Vertrag vom Wirtschafts- und Sozialausschuss sowie vom Ausschuss der Regionen unterstützt. Diese Institutionen hatten und haben im Gegensatz zu den Organen jedoch lediglich beratende Aufgaben.

Die EG besaß, basierend auf EG-Vertrag, eine eigene Kompetenz, sekundäres Gemeinschaftsrecht als Teil des Europarechts zu setzen. Dabei handelte es sich um Richtlinien der EG, Verordnungen der EG oder Entscheidungen der EG. In der Öffentlichkeit wurden diese schon seit 1993 oft nicht als Richtlinien, Verordnungen oder Entscheidungen der EG, sondern der EU bezeichnet, was formal unkorrekt war. Erst seit der Fusion von EG und EU durch den Vertrag von Lissabon 2009 erhielten sie auch formal diese Bezeichnung. Ältere Rechtsakte behalten in ihrem amtlichen Kürzel allerdings die Kennzeichnung als EG-Rechtsakte bei, so hat die Dublin-II-Verordnung von 2003 etwa das Kürzel "Verordnung (EG) Nr. 343/2003".

Vor Inkrafttreten des Lissaboner Vertrages besaß die EG anders als die EU Rechtsfähigkeit und konnte völkerrechtlich verbindliche Verträge abschließen. Diesen Status sprach ihr der Europäische Gerichtshof 1971 und nochmals 1976 in der AETR-Doktrin aufgrund der "Parallelität von internen und externen Rechtsetzungskompetenzen" zu:
Beispielsweise war die EG ein vollwertiges Mitglied der EBWE und hatte ein Stimmrecht entsprechend ihrer Einlagen. An den Organisationen WTO, FAO und Eurocontrol war sie ebenfalls beteiligt. Jedoch musste sie vor den Sitzungen mit den EU-Mitgliedstaaten, die auch in den Organisationen vertreten sind, ihr Abstimmungsverhalten mitteilen. Wenn sie ihr Mandat wahrnahm, vertrat sie diese Staaten und gab statt ihrer ihre Stimme ab.





</doc>
<doc id="13725" url="https://de.wikipedia.org/wiki?curid=13725" title="EAG">
EAG

EAG steht als Abkürzung für:

Siehe auch:


</doc>
<doc id="13728" url="https://de.wikipedia.org/wiki?curid=13728" title="Europäische Wirtschaftsgemeinschaft">
Europäische Wirtschaftsgemeinschaft

Die Europäische Wirtschaftsgemeinschaft (EWG) war der ursprüngliche Name eines Zusammenschlusses europäischer Staaten zur Förderung der gemeinsamen Wirtschaftspolitik im Rahmen der europäischen Integration. Am 25. März 1957 wurde die EWG mit der Unterzeichnung der Römischen Verträge durch Belgien, Frankreich, Italien, Luxemburg, die Niederlande und die Bundesrepublik Deutschland gegründet. 

Durch den am 7. Februar 1992 unterzeichneten Vertrag von Maastricht wurde die EWG angesichts ihrer mittlerweile erweiterten Aufgabenstellung mit Wirkung zum 1. November 1993 in Europäische Gemeinschaft (EG) umbenannt; am 1. Dezember 2009 wurde sie mit Inkrafttreten des Vertrags von Lissabon aufgelöst.

Die Idee zur Schaffung eines Gemeinsamen Marktes reicht bis in die Zeit der gescheiterten EVG-Verträge 1952 zurück. Verschiedene europäische Politiker wie Jean Monnet, der belgische Außenminister Paul-Henri Spaak sowie sein niederländisches Pendant Willem Beyen waren maßgeblich an der Wiederbelebung des europäischen Gedankens beteiligt. Sie sahen die beste Möglichkeit der europäischen Kooperation auf wirtschaftlichem Gebiet, da nach der Ablehnung der EVG durch die Französische Nationalversammlung (30. August 1954) diese Form der europäischen Zusammenarbeit im militärischen und politischen Bereich vorerst fehlgeschlagen war. 

Auf der Konferenz von Messina im Juni 1955 beschlossen die Außenminister der EGKS eine allgemeine wirtschaftliche Einigung der Volkswirtschaften, die Schaffung gemeinsamer supranationaler Institutionen, eine Sozialharmonisierung durch Verwirklichung allgemeiner Sozialstandards und eine Zusammenarbeit auf dem Nuklearsektor. Man beschloss auf der Konferenz von Messina die Einsetzung eines Regierungsausschusses unter Vorsitz von Paul-Henri Spaak („Spaak-Kommission“) zur Ausarbeitung der Grundlagen und Möglichkeiten des Gemeinsamen Marktes (Frage nach Einbeziehung verschiedener Wirtschaftssektoren). Innerhalb der deutschen Bundesregierung gab es unterschiedliche Strömungen; zwei dominierten: 

Die sechs Staaten der EGKS einigten sich bei den Regierungsverhandlungen auf Grund des Berichts der Spaak-Kommission auf die Vereinheitlichung des Gemeinsamen Marktes 

Man erreichte auch eine Einigung hinsichtlich der zivilen Nutzung der Atomenergie (Euratom). Die Verhandlungen über den Gemeinsamen Markt standen unter dem Eindruck des Ungarnaufstandes (1956) und der Suezkrise; diese führten den Regierungschefs die Notwendigkeit der europäischen Zusammenarbeit eindringlich vor Augen. Der "Vertrag zur Gründung der Europäischen Wirtschaftsgemeinschaft" (EWG-Vertrag) und jener der "Europäischen Atomgemeinschaft" (EAG-Vertrag/Euratom) wurden am 25. März 1957 in Rom von den sechs Mitgliedern der Montanunion – Frankreich, Italien, Bundesrepublik Deutschland, Belgien, Niederlande und Luxemburg – unterzeichnet ("Römische Verträge"). Am 1. Januar 1958 traten die Verträge in Kraft; Walter Hallstein wurde erster Präsident der EWG-Kommission.

Zum 1. Januar 1961 kam es zu einer ersten Teilangleichung der nationalen Zollsätze der EWG-Staaten mit dem Ziel eines einheitlichen Außenzolls. Die Verwirklichung der Zollunion und die Einführung eines gemeinsamen Außenzolls erfolgte am 1. Juli 1968. Im Juni 1961 wurde ein Assoziierungsabkommen der EWG mit Griechenland unterzeichnet. 

Im Sommer 1961 stellten die drei Staaten Irland (31. Juli), Großbritannien (9. August) und Dänemark (10. August) den Antrag auf Beitritt zur EWG. Am 30. April 1962 beantragte auch Norwegen den Beitritt. 
Am 14. Januar 1963 sprach sich der französische Staatspräsident de Gaulle gegen Großbritanniens EWG-Beitritt aus. Sein Veto überraschte die EWG-Kommission und die fünf anderen Mitgliedsstaaten.
Am 29. Januar 1963 wurden die Beitrittsverhandlungen der EWG mit Großbritannien abgebrochen. Am 20. Juli 1963 wurde das Yaoundé-Abkommen (ein Assoziierungsabkommen frankophoner afrikanischer Staaten und Madagaskar mit der EWG) unterzeichnet und am 12. September 1963 mit der Türkei. Am 8. April 1965 wurde Vertrag zur Einsetzung eines gemeinsamen Rates und einer gemeinsamen Kommission der Europäischen Gemeinschaften („Fusionsvertrag“) unterzeichnet; damit wurden die Exekutivorgane der Europäischen Gemeinschaften (EGKS, EWG und Euratom) zusammengelegt. 

1967 beantragten Großbritannien (10. Mai), Dänemark (11. Mai) und Norwegen (24. Juli) zum zweiten Mal den Beitritt zur EWG und Schweden am 28. Juli erstmals. Ein Assoziierungsabkommen der EWG mit Marokko und Tunesien wurde am 4. März 1969 geschlossen. Am 29. Juli 1969 wurde ein zweites Yaoundé-Abkommen unterzeichnet, das am 1. Januar 1971 in Kraft trat. 

Am 1. und 2. Dezember 1969 fassten die Staats- und Regierungschefs der EG (inzwischen war Georges Pompidou französischer Staatspräsident) auf ihrem Gipfeltreffen in Den Haag Beschlüsse zur beschleunigten Integration, zur Einführung einer Wirtschafts- und Währungsunion (WWU) bis 1980 und zur politischen Zusammenarbeit sowie die Aufnahme von Beitrittsverhandlungen mit Dänemark, Großbritannien, Irland und Norwegen. Diese Verhandlungen begannen am 30. Juni 1970. Bei einer Volksabstimmung im September 1972 lehnten 53,5 Prozent der Abstimmenden einen EWG-Beitritt Norwegens ab. Dänemark, Großbritannien und Irland traten zum 1. Januar 1973 bei. 

Mit dem Vertrag von Maastricht wurde 1992 die EWG, eine der drei Europäischen Gemeinschaften (EG), in Europäische Gemeinschaft (EG) umbenannt und war eine der drei Säulen der Europäischen Union.





</doc>
<doc id="13733" url="https://de.wikipedia.org/wiki?curid=13733" title="Teilgraph">
Teilgraph

Der Begriff Teilgraph beschreibt in der Graphentheorie eine Beziehung zwischen zwei Graphen. Ein anderes Wort für Teilgraph ist Untergraph. Ein Graph formula_1 ist Teilgraph des Graphen formula_2, wenn alle Knoten und Kanten von formula_1 auch in formula_2 enthalten sind.
Anders gesagt: Ein Teilgraph formula_1 entsteht aus einem Graphen formula_2, indem einige Knoten und Kanten aus formula_2 entfernt werden. Dabei müssen beim Entfernen eines Knotens auch alle inzidenten Kanten mit entfernt werden.

Ein Graph formula_8 heißt Teilgraph oder Untergraph oder Subgraph von formula_9, wenn seine Knotenmenge formula_10 Teilmenge von formula_11 und seine Kantenmenge formula_12 Teilmenge von formula_13 ist,
also formula_14 und formula_15 gilt.

Umgekehrt heißt formula_16 dann Supergraph oder Obergraph von formula_17.

Bei einem knotengewichteten bzw. kantengewichteten Graphen formula_16 wird von einem Teilgraphen formula_17 zudem verlangt, dass die Gewichtsfunktion formula_20 von formula_17 kompatibel zu der Gewichtsfunktion formula_22 von formula_16 sein muss, d. h. für jeden Knoten formula_24 gilt formula_25 bzw. für jede Kante formula_26 gilt formula_27.

Gilt für einen Teilgraphen formula_17 zusätzlich, dass formula_12 alle Kanten zwischen den Knoten in formula_10 enthält, die auch in formula_31 vorhanden sind, so bezeichnet man formula_17 als den durch formula_10 induzierten Teilgraphen von formula_16 und notiert diesen auch mit formula_35.
Ein induzierter Teilgraph ist also immer eindeutig durch den Obergraphen und die gewählte Knotenmenge bestimmt.
Für eine Knotenmenge formula_36 bezeichnet man mit formula_37 den induzierten Teilgraphen,
der aus formula_38 durch Weglassen der Knoten aus formula_39 und aller mit diesen Knoten inzidenten Kanten entsteht, also
formula_40.

Ein Teilgraph formula_41 von formula_42, der alle Knoten seines Obergraphen enthält, wird aufspannender Teilgraph oder Faktor genannt.

In der folgenden Abbildung sind die Graphen formula_17, formula_16 und formula_45 Teilgraphen von formula_2, aber nur formula_16 und formula_48 sind induzierte Teilgraphen. formula_45 entsteht dabei aus formula_2 durch Weglassen der Knoten formula_51 und ihrer inzidenten Kanten, also ist formula_52. Gleichzeitig ist formula_45 auch ein induzierter Teilgraph von formula_16.




</doc>
<doc id="13734" url="https://de.wikipedia.org/wiki?curid=13734" title="Filzlaus">
Filzlaus

Die Filz- oder Schamlaus ("Pthirus pubis", Syn.: "Phthirus pubis", vulgär "Sackratte") ist eine am Menschen parasitierende Tierlausart (Pedikulose). Sie ist mit bloßem Auge noch erkennbar und wird über Kleidungsstücke, Bett- und Handtücher oder bei engem körperlichen Kontakt übertragen. Einmal vom Körper entfernt, können Filzläuse nur bis zu 24 Stunden überleben, deren Eier aber länger.

Die Filzlaus wird etwa 1 bis 1,5 Millimeter lang und ist von kurzer, aber breiter Gestalt mit einem grauen Körper. Der Körper trägt sechs paarige, zapfenartige Auswüchse. An den Enden ihrer sechs Beine befinden sich kräftige Halteklauen, mit denen sie sich an den Haaren des Menschen festhält. An den Haaren befestigt sie auch ihre Eier, die "Nissen".

In Abhängigkeit von der beim Menschen unterschiedlich ausgeprägten sensorischen Sensibilität und allergischen Sensitivität verursacht der Stich der Filzlaus in der Regel einen starken Juckreiz und eine blaue Verfärbung der betroffenen Hautpartie.

Filzläuse kommen vor allem in der Schambehaarung (Leistengegend, je nach Befallintensität auch an den Innenseiten der Oberschenkel) vor, seltener in den Achsel- und Barthaaren, sehr selten in den Augenwimpern ("Cilien") der Ober- und Unterlider ("Phthiriasis palpebrarum") und noch seltener in den Augenbrauen. Der Befall der Augenhaare wird meist durch die hygienisch schlechten Bedingungen in Krisen- und Kriegsgebieten ausgelöst.

Filzläuse befallen nicht die Kopfhaare. Die Filzlaus ist extrem stark auf den Menschen spezialisiert und stirbt spätestens nach 24 Stunden, wenn sie vom Körper entfernt wird. Filzlausbefall wird in der Medizin als "Phthiriasis" bezeichnet.

Wahrscheinlich bekamen die Vorfahren des Menschen erst vor etwa 3,3 Millionen Jahren Probleme mit Filzläusen, da zu dieser Zeit der Parasit den Sprung von anderen Affenarten zu den menschlichen Vorfahren schaffte. Ein Forscherteam um David Reed von der University of Florida in Gainesville (Florida) verglich mit molekularbiologischen Untersuchungen (Erbgut-Analysen) Läuse von Gorillas ("Phthirus gorillae") und Filzläuse von Menschen ("Phthirus pubis") und fand dabei heraus, dass beide Läusearten vor 3,3 Millionen Jahren gemeinsame Vorfahren hatten und sich anschließend unabhängig voneinander weiterentwickelten. Nach Ansicht der Wissenschaftler kamen die Vormenschen damals durch die Gorillajagd, das Verzehren toter Gorillas oder das Nächtigen in den Nestern der Menschenaffen mit den Ahnen der Filzläuse in Kontakt.

In den letzten Jahren ist die Verbreitung der Filzlaus in westlichen Ländern stark zurückgegangen. So fand sich in einer britischen Studie ein signifikanter Rückgang zwischen den Jahren 1997 und 2003. Diese Entwicklung wird größtenteils auf die in letzter Zeit zunehmend verbreitete Achsel- und Schamhaarentfernung zurückgeführt. Gynäkologen und Urologen im australischen Sydney berichteten, dass sie seit 2008 bei Frauen keinen Filzlausbefall mehr beobachtet hätten, bei Männern betrage der Rückgang 80 Prozent.

Lindan (in Deutschland nicht mehr zugelassen) und Malathion (in Deutschland und der Schweiz vom Markt genommen) werden in der medikamentösen Behandlung eines Filzlausbefalls eingesetzt. Pyrethrumpräparate sind Mittel der zweiten Wahl. Medizinisches Shampoo gegen Kopf- und Filzläuse ist in jeder Apotheke vorrätig und rezeptfrei erhältlich. Diese Mittel wirken insektizid. Bei Befall der Augenbrauen oder Wimpern dürfen diese aber nicht angewendet werden. Hier eignet sich neben einer vollständigen Entfernung aller Augenwimpern einschließlich der Läuse und ihrer Eier (Nissen) auch die Anwendung verschiedener Wirkstoffe beziehungsweise Präparate wie beispielsweise Fluorescein 20 %, Physostigmin 0,25 %, Vaseline, gelbe Quecksilberoxidsalbe 1 % und Pilocarpin 4 %.

Eine alternative bzw. ergänzende Therapie besteht darin, die Scham- und Achselbehaarung, aber auch das Barthaar wegzurasieren. Außerdem sollten die Kleidung und die Bettbezüge bei mindestens 60 Grad gewaschen und – falls möglich – heiß getrocknet werden. Kleidung, die nicht so heiß gewaschen werden kann, sollte in einer fest verschnürten Plastiktüte oder einem luftdichten Sack 14 Tage separat aufbewahrt werden. Dadurch werden die Filzläuse und aus den Nissen geschlüpfte Nachkommen ausgehungert und sterben. Anschließend kann die Kleidung auch bei geringeren Temperaturen gewaschen werden. In den ersten drei bis vier Tagen sollte im Schambereich hauteng getragene Kleidung wie Unterwäsche und Schlafanzug täglich gewechselt und wie oben beschrieben separat behandelt werden. Um eine Neuansteckung durch einen Ping-Pong-Effekt zu verhindern, sollte in der Behandlungsphase auf sexuelle Kontakte mit anderen sowie auf gemeinsame Benutzung von Handtüchern verzichtet werden.

Anders als die Kopf- und Kleiderlaus, die gelegentlich in Not- und Krisenzeiten als Überträger von gefährlichen Krankheitserregern auffallen, spielt die Filzlaus diesbezüglich in Mitteleuropa keine Rolle mehr.



</doc>
<doc id="13737" url="https://de.wikipedia.org/wiki?curid=13737" title="Europäische Gemeinschaft für Kohle und Stahl">
Europäische Gemeinschaft für Kohle und Stahl

Die Europäische Gemeinschaft für Kohle und Stahl, kurz offiziell EGKS, oft auch Montanunion genannt, war ein europäischer Wirtschaftsverband und ein Vorläufer der EG. Er gab allen Mitgliedstaaten Zugang zu Kohle und Stahl, ohne Zoll zahlen zu müssen. Eine besondere Neuheit war die Gründung einer Hohen Behörde, die im Bereich der Montanindustrie, also der Kohle- und Stahlproduktion, gemeinsame Regelungen für alle Mitgliedstaaten treffen konnte. Die EGKS war damit die erste supranationale Organisation überhaupt; anfangs wurde ihr supranationaler Charakter (dt. Fassung: „überstaatlicher“ Charakter) ausdrücklich in Artikel 9 des EGKS-Vertrages vom 18. April 1951 erwähnt.

Die Gründerstaaten des EGKS-Vertrages waren Belgien, die Bundesrepublik Deutschland, Frankreich, Italien, Luxemburg und die Niederlande. Der EGKS-Vertrag, der für eine Dauer von 50 Jahren geschlossen wurde, lief am 23. Juli 2002 aus. Er wurde nicht verlängert; seine Regelungsmaterie wurde fortan dem EG-Vertrag, seit 2009 dem Vertrag über die Arbeitsweise der Europäischen Union, zugerechnet.

Die Organisation wurde am 18. April 1951 durch den Vertrag von Paris gegründet und trat am 23. Juli 1952 in Kraft. Der EGKS-Vertrag ging auf den Schuman-Plan, eine Initiative des französischen Außenministers Robert Schuman, zurück, in der er dem deutschen Kanzler Konrad Adenauer einen Vorschlag machte, dem dieser sofort zustimmte: gemeinsame Kontrolle der Montanindustrie der Mitgliedstaaten ohne Zoll. Das bedeutete, dass das Ruhrgebiet, das damals unter der Kontrolle der Internationalen Ruhrbehörde und britischer Besatzung stand und dessen Anlagen bis 1949 zum Teil als Reparationen demontiert wurden, eine Chance für neues Wachstum bekam.

Die Montanunion galt einige Jahre lang als ein „Schwungrad“ des wirtschaftlichen Neuaufbaus in Westdeutschland. Die Idee, die deutsche und französische Kohle- und Stahlpolitik zu harmonisieren, war aber nicht neu. In der OEEC gab es bereits Diskussionen über eine Neustrukturierung der Kohle- und Stahlindustrien. Auch die Internationale Rohstahlgemeinschaft von 1926/31, ein Kartell der deutschen, französischen, belgischen und luxemburgischen Stahlproduzenten, hatte z. T. vergleichbare Aufgaben gehabt.

Hauptziel des Vertrages war in der Argumentation Schumans die Sicherung des innereuropäischen Friedens durch die „Vergemeinschaftung“, also die gegenseitige Kontrolle der kriegswichtigen Güter Kohle und Stahl, sowie die Sicherstellung dieser für den Wiederaufbau nach dem Zweiten Weltkrieg entscheidenden Produktionsfaktoren.

Das deutsche Bundeswirtschaftsministerium unter der Ägide von Ludwig Erhard zeigte sich ausgesprochen skeptisch. Erhard war nicht bereit, den politischen Zielen einer Integration alle volkswirtschaftlichen Grundsätze hintenanzustellen. Nach Nikolaus Bayer war das „Hauptziel“, das Bundeskanzler Adenauer mit der Montanunion verfolgte, „eine schnelle Rückführung Deutschlands als gleichberechtigtes Mitglied in die westliche Staatengemeinschaft“. Dafür sei Adenauer durchaus auch bereit gewesen, in Detailfragen Kompromisse einzugehen oder Nachteile in Kauf zu nehmen. Ludolf Herbst beschreibt die Situation der deutschen Bundesregierung zu der Zeit mit den Worten: „Da Bonn über nationale Souveränität noch nicht verfügte, bedeutete Supranationalität an sich keinen Verzicht.“ Großbritannien lehnte den Plan ab; es hatte Angst, dass dieser Plan seine Souveränität beeinträchtigen würde. 

In Deutschland selbst lehnte die Sozialdemokratische Opposition den Plan zunächst ab, da dieser die junge Bundesrepublik auf eine Westintegration festgelegt hätte. Die SPD hatte jedoch die Hoffnung auf eine Wiedervereinigung Deutschlands als blockfreier bzw. neutraler Staat noch nicht aufgegeben. Der DGB hingegen, der an den Verhandlungen zur EGKS ebenfalls teilnahm, setzte sich im Laufe des Prozesses von den SPD-Positionen ab: man stimmte der Westintegration als Teil einer Europäischen Integration zu und erreichte damit eine gewerkschaftliche Mitbestimmung in den Institutionen der Montanunion. 

Nach vielen Verhandlungen mit der deutschen Regierung stellte Jean Monnet am 20. Juni 1950 einen Vertragsentwurf vor. Die nationalen Delegationen sollten zuerst die Vorschläge prüfen. So wurde im Bundeskabinett eigens ein Ausschuss für den Schuman-Plan gegründet. Am 29. Juni wurden dann Empfehlungen vom Bundeskabinett verabschiedet. Vor dem Hintergrund internationaler Entwicklungen, im Fernen Osten war gerade der Koreakrieg ausgebrochen und wegen der niederländischen Opposition gegen die Kompetenzen der Hohen Behörde, wollte Monnet die Beteiligten zur Unterschrift drängen. Bundeskanzler Adenauer forderte mehr Zeit, um den Entwurf in Ruhe analysieren zu können. Die Bundesregierung, im Besonderen Wirtschaftsminister Erhard, machte klar, dass Deutschland diesem Plan nur zustimmen würde, wenn die Kontrollen über die Ruhrindustrie abgeschafft würden. Nach Erhards Meinung könne im Rahmen einer freieren Wirtschaftspolitik selbst in normalen Zeiten nicht auf eine hoheitliche Regelung der Preise für Kohle und Stahl verzichtet werden. Am 14. März 1951 wurde schließlich ein Kompromiss erreicht, so dass der EGKS-Vertrag am 18. April 1951 unterzeichnet werden konnte.

In Deutschland lagen damals die reichsten Kohlevorkommen der sechs Länder. Frankreich erhielt damit vor allem Zugang zum Ruhrgebiet, welches in der vormals britischen Besatzungszone lag und dessen Rohstoffproduktion und wirtschaftliche Entwicklung bis dahin noch unter den Sanktionen der Siegermächte zu leiden hatte. Da Frankreich auch schon großen Einfluss im damals unabhängigen Saarland hatte, war dies eine weitere Möglichkeit, von Rohstofflagerstätten zu profitieren. Um den Zugang zu den deutschen Industriegebieten zu realisieren, forderte Frankreich vor allem die Kanalisierung der Mosel.

Die Organe der EGKS, der Europäischen Wirtschaftsgemeinschaft (EWG) und der Europäischen Atomgemeinschaft (EURATOM) wurden am 8. April 1965 durch den sogenannten Fusionsvertrag zusammengelegt. Die rechtliche Selbständigkeit der drei Gemeinschaften blieb hiervon jedoch unberührt.

Laut Bayer war die Montanunion bei ihrer Gründung eine beispiellose supranationale Organisation, an die die Mitgliedsstaaten freiwillig Teile ihrer Hoheitsrechte abtraten. Sie markierte damit den Beginn des Prozesses des europäischen Zusammenwachsens und hat maßgeblich auf die folgenden Schritte eingewirkt. Einerseits wurden in der von den USA stark befürworteten Montanunion Elemente des liberalen Kapitalismus übernommen und umgesetzt, andererseits markiert die Montanunion einen der ersten Schritte in der Emanzipation Europas von den USA. Der Historiker Ludolf Herbst schrieb 1989, „daß entscheidende Impulse zur Fortsetzung der europäischen Integration von der Montanunion ausgingen.“

Ab etwa 1952 erlebte Öl in vielen Bereichen einen bis dahin unvorstellbaren Aufschwung: 1955 deckten die 151 Millionen Tonnen Kohle, die in Westdeutschland gefördert wurden, knapp 80 Prozent des westdeutschen Energiebedarfs. 1965 erreichte der westdeutsche Energieverbrauch 268 Millionen Tonnen Steinkohleeinheiten. Die Kohle deckte davon nur noch 42,1 Prozent (113 Millionen Tonnen); Erdöl 41,2 Prozent (110,5 Millionen Tonnen SKE). Ölheizungen verdrängten in vielen Haushalten Kohleheizungen und Diesellokomotiven verdrängten Dampflokomotiven.

Nach Bergarbeiterstreiks in Belgien forderten Anfang 1959 die drei Benelux-Staaten, in der Montanunion offiziell eine Krise im Sinne des Artikel 58 des EGKS-Vertrages auszurufen. Die drei anderen – Deutschland, Frankreich und Italien – lehnten dies ab.

In den 1950er Jahren erschloss Frankreich in seiner Kolonie Algerien große Erdöl- und Erdgasvorkommen. Im Frühjahr 1959 forderte die französische Regierung unter Charles de Gaulle – auch mit Verweis darauf – eine Revision der Montanunion.

Clarence B. Randall, ein ehemaliger leitender Mitarbeiter der "Economic Cooperation Administration" bezichtigte in der Sommerausgabe 1951 der Zeitschrift Atlantic Monthly die Montanunion in einem langen Artikel des „Super-Sozialismus“. Die anstehende parlamentarische Absegnung der EGKS-Verträge war in seinen Augen ein Gang der freien Marktwirtschaft zum Schafott.

Die "Wirtschaftsvereinigung Eisen- und Stahlindustrie" (WVESI) versendete im Juni 1950 ein vertrauliches Rundschreiben an ihre Mitglieder mit einem achtzehnseitigen Exposé mit einer kritischen Untersuchung der Schuman-Deklaration vom 9. Mai. Es sah in der geplanten EGKS letztlich nichts anderes als eine „große kartellähnliche Organisation“ mit dem Ziel, den Wiederaufbau der französischen Nachkriegsindustrie abzusichern:
Adenauer war mehr auf die politischen Vorteile des Schuman-Plans bedacht. Die Chancen, die deutsch-französischen Beziehungen neu zu gestalten und durch Gleichberechtigung und Gleichbehandlung das internationale Renommée der Bundesrepublik aufzuwerten, überwogen alle anderen Überlegungen.

Adenauer wies den Chef der deutschen Unterhändler Walter Hallstein unmissverständlich an, den Ausgang der Verhandlungen auf keinen Fall durch Sonderwünsche der „Ruhrkapitäne“ zu gefährden. Das war der französischen Seite recht. Durch den Koreakrieg war Frankreich einem immer stärker werdenden Druck der USA ausgesetzt, einer Wiederbewaffnung Deutschlands zuzustimmen. Monnet lag sehr daran, möglichst rasch ein unterschriftsreifes Vertragswerk abzuschließen. Die Lösung technischer Fragen sollte später erörtert werden. Wirtschaftliche Belange spielten für Paris nur eine untergeordnete Rolle. Die Industrie bekam nicht die von ihr nachdrücklich verlangten Sicherheiten.

Nicht nur in Deutschland, sondern auch in allen übrigen EGKS-Ländern attackierte die Unternehmerschaft den Plan. Sie fühlten sich bei der Formulierung der ökonomischen und institutionellen Bestimmungen völlig übergangen. Die Hohe Behörde wurde unter anderem als Instrument einer Art Verstaatlichung gesehen; der Vertreter Luxemburgs bezeichnete sie am 7. Mai 1951 als „Verwaltungsmonster“. Der französische Branchenverband "Chambre Syndicale de la Sidérurgie Française" (CSSF) befürchtete einen „Dirigismus übelster Art.“

Die nationalen Branchenverbände kooperierten bei ihrem Widerstand gegen den EGKS nur punktuell. Die CSSF nahm fälschlich an, dass Bonn dem EGKS nicht zustimmen werde. Als sie das Gegenteil erfuhren, beschlossen sie zusammen mit dem Verband der luxemburgischen Hüttenherren (GISL) die Taktik „den Plan annehmen, aber sofort eine Revision der wirklich wichtigen Punkte verlangen“. Eile schien geboten, da mit dem Zustandekommen der obersten Unionsbehörde die alte Internationale Ruhrbehörde höchstwahrscheinlich verschwinden und die für Franzosen und Luxemburger mit ihren großen Minette-Vorkommen sehr wichtige Frage der Kohleverteilung sich demnach von Anbeginn stellen würde. Die Revision der Texte müsse stattfinden, noch bevor die Hohe Behörde überhaupt zum Einsatz gelangte.

Deutschlands Stahlwerken wurde der Import billiger amerikanischer Kohle mit Rücksicht auf den Ruhrbergbau verboten. 1965/66 war die US-Kohle pro Tonne 15 D-Mark billiger als Ruhrkohle.
Hans-Günther Sohl, WVESI-Vorsitzender und Generaldirektor der August Thyssen-Hütte, äußerte, dass deshalb in der Stahlindustrie Kurzarbeit und Entlassungen drohten. Notfalls müsse die deutsche Stahlindustrie ins EWG-Ausland abwandern. Der Kohle wegen auf Jahre hinaus mit roten Zahlen zu arbeiten, sei niemandem zuzumuten.

Auf Ruhrkohle und Hollandkohle erhob die Hohe Behörde einen Zuschlag, der in die Modernisierung des belgischen Bergbaus fließen sollte, um ihn effizienter und damit konkurrenzfähiger zu machen. Die Behörde griff nicht ein, als dort nichts Nennenswertes geschah. 

Die Finanzierung geschah ursprünglich über die EGKS-Umlage – faktisch eine Steuer – auf Kohle- und Stahlunternehmen, die direkt der Hohen Behörde der EGKS zugutekam. Die vertraglich festgelegte Maximalhöhe dieser Umlage lag bei einem Prozent. Sie wurde später eingestellt. Außerdem hatte die EGKS die Möglichkeit, Anleihen aufzunehmen, die sie jedoch selbst nur für die Vergabe von Krediten nutzen durfte.

Die Organe der Gemeinschaft waren:





</doc>
<doc id="13743" url="https://de.wikipedia.org/wiki?curid=13743" title="Kolmogorow-Komplexität">
Kolmogorow-Komplexität

Die Kolmogorow-Komplexität (nach Andrei Nikolajewitsch Kolmogorow) ist ein Maß für die Strukturiertheit einer Zeichenkette und ist durch die Länge des kürzesten Programms gegeben, das diese Zeichenkette erzeugt. Dieses kürzeste Programm gibt somit eine beste Komprimierung der Zeichenkette an, ohne dass Information verloren geht.

Wenn die Kolmogorow-Komplexität einer Zeichenkette mindestens so groß ist wie die Zeichenkette selbst, dann bezeichnet man die Zeichenkette als unkomprimierbar, zufällig oder auch strukturlos. Je näher die Kolmogorow-Komplexität an der Länge der Zeichenkette liegt, desto 'zufälliger' ist die Zeichenkette (und desto mehr Information enthält sie).

Das Prinzip der Kolmogorow-Komplexität wurde unabhängig im Jahre 1964 von Ray Solomonoff, im Jahre 1965 von Andrei Kolmogorow und 1969 von Gregory Chaitin entwickelt, und hat Bezüge zur Shannonschen Informationstheorie.

Die Kolmogorow-Komplexität wird manchmal auch Algorithmische Komplexität oder Beschreibungskomplexität genannt, darf aber nicht mit der Zeit- oder Raumkomplexität von Algorithmen verwechselt werden. Etwas präziser ist die Bezeichnung Algorithmischer Informationsgehalt, die auch die Verbindung zu dem Begriff des Informationsgehalts nach Shannon herstellt. Ein verwandter, aber deutlich abzugrenzender Ansatz ist die "Algorithmische Tiefe", die sich auf den Aufwand bezieht, der betrieben werden muss, um eine bestimmte Nachricht zu erzeugen oder zu entschlüsseln. Die Algorithmische Informationstheorie von Gregory Chaitin präzisiert den Ansatz Kolmogorows in Bezug auf das Maschinenmodell. Jorma Rissanen beschreibt mit der Minimum Description Length ein ähnliches Konzept, das aber auf Komprimierung der Daten aufbaut.

Ein Beispiel zur Erzeugung einer Folge von 1000 Nullen mag die Kompression veranschaulichen:
Die Zahl 1000 lässt sich (in Binärform) durch 10 Bit darstellen.
Bei einem gegebenen (konstanten) Programm zum Ausdrucken einer Nullfolge kann man die Kolmogorow-Komplexität einer Folge von "n" Nullen als "Konstante + log("n")" angeben:
Das obige Programm kann mit einer konstanten Anzahl an Bits kodiert werden, z. B. als Maschinencode oder als einfache Textdatei. Die Kodierung der Zahl "n" benötigt "log(n)" Bits. Die gesamte Kodierung benötigt also zusammengerechnet "Konstante + log(n)" Bits und damit für große "n" wesentlich weniger als "n" Bits. Daher ist die Nullfolge komprimierbar.

Die folgende Darstellung verdeutlicht die Komprimierbarkeit:
Das Programm, das die Folge mit 1000 Nullen erzeugt, nimmt kaum mehr als 5 % der Folge selber ein.

Es gibt allerdings (in diesem Sinne) auch nur scheinbar zufällige Zahlenfolgen.
Beispielsweise gibt es ein kurzes Programm, welches die Dezimalentwicklung der Kreiszahl Pi in beliebiger Genauigkeit erzeugt.
Damit ergibt sich ebenfalls eine Komplexität der Form "Konstante + log("n")", wobei "n" die Genauigkeit der Darstellung angibt.

Die Kolmogorow-Komplexität ist nicht berechenbar, sie ist allerdings von oben rekursiv aufzählbar.

Heute findet die Kolmogorow-Komplexität Anwendung in der Informatik, der Biologie und anderen Wissenschaften, die Strukturen oder Informationen betrachten.





</doc>
<doc id="13746" url="https://de.wikipedia.org/wiki?curid=13746" title="Zusammenhang (Graphentheorie)">
Zusammenhang (Graphentheorie)

Der Zusammenhang ist ein mathematischer Begriff aus der Graphentheorie. Ein Graph heißt zusammenhängend, wenn die Knoten paarweise durch eine Kantenfolge des Graphen verbunden sind.

Ein ungerichteter Graph formula_1 heißt zusammenhängend, falls es zu je zwei beliebigen Knoten formula_2 und formula_3 aus formula_4 einen ungerichteten Weg in formula_5 mit formula_2 als Startknoten und formula_3 als Endknoten gibt.

Einen maximalen zusammenhängenden Teilgraphen eines beliebigen Graphen nennt man eine Komponente oder Zusammenhangskomponente.
Ein nicht zusammenhängender Graph zerfällt in seine Zusammenhangskomponenten.

Ein gerichteter Graph formula_1 heißt (stark) zusammenhängend von einem Knoten formula_2 aus, falls es zu jedem Knoten formula_3 aus formula_4 einen gerichteten Weg in formula_5 von formula_2 nach formula_3 gibt. formula_5 heißt stark zusammenhängend, falls formula_5 von jedem Knoten aus stark zusammenhängend ist. Anders formuliert heißt formula_5 stark zusammenhängend, falls es zwischen zwei beliebigen Knoten formula_18 und formula_2 aus formula_5 sowohl einen gerichteten Weg von formula_18 nach formula_2 wie auch einen gerichteten Weg von formula_2 nach formula_18 in formula_5 gibt.

Ein gerichteter Graph heißt (schwach) zusammenhängend, falls der zugehörige ungerichtete Graph (also der Graph, der entsteht, wenn man jede gerichtete Kante durch eine ungerichtete Kante ersetzt) zusammenhängend ist.

Ein induzierter Teilgraph formula_26 für eine Teilmenge formula_27 heißt starke Zusammenhangskomponente von formula_5, falls formula_26 stark zusammenhängend ist und nicht zu einem größeren stark zusammenhängenden Teilgraphen von formula_5 erweitert werden kann.

Relativ leicht zeigt man folgende Aussagen:

Mittels Tiefensuche lässt sich leicht ein linearer Algorithmus implementieren, der die Zusammenhangskomponenten eines Graphen berechnet und so einen einfachen Test impliziert, ob der Graph zusammenhängend ist.
Der Test, ob ein gerichteter Graph von einem Knoten "v" aus zusammenhängend ist, funktioniert analog.
Von Tarjan (1972) stammt ein linearer Algorithmus zur Bestimmung starker Zusammenhangskomponenten, der ebenfalls auf Tiefensuche basiert und in gerichteten Graphen die starken Zusammenhangskomponenten und leicht modifiziert in ungerichteten Graphen die Blöcke und Artikulationen berechnet. Siehe auch Union-Find-Struktur.

Eine wesentliche Verallgemeinerung des Begriffs stellt der Begriff des k-fachen Knotenzusammenhangs, der Kantenzusammenhang und der Bogenzusammenhang dar.




</doc>
<doc id="13750" url="https://de.wikipedia.org/wiki?curid=13750" title="Zusammenhangskomponente">
Zusammenhangskomponente

Zusammenhangskomponente bezeichnet in verschiedenen Bereichen der Mathematik ein zusammenhängendes Teilobjekt. Speziell bezeichnet es:



</doc>
<doc id="13755" url="https://de.wikipedia.org/wiki?curid=13755" title="Ischim (Fluss)">
Ischim (Fluss)

Der 2450 km lange Ischim (/Есиль; /Jessil) ist ein linker Nebenfluss des Irtysch in Kasachstan und Russland (Asien).

Der Ischim entspringt in Kasachstan am Nordwestrand der Kasachischen Schwelle und fließt zunächst in Richtung Westen.

50 km südöstlich der kasachischen Hauptstadt Astana wurde das Vyacheslaver Reservoir zur Bewässerung und Stromerzeugung errichtet, 2002 der Irtysch-Karaganda-Kanal zur weiteren Wasserversorgung der schnell wachsenden Stadt angeschlossen. 40 km weiter befindet sich ein weiteres Wasserreservoir. Beim Erreichen der Hauptstadt erhält der Fluss aus dem hier einmündenden Nura-Ischim-Kanal Wasser zur Versorgung der ansässigen Industrie und lokalen Fischzucht. 

Anschließend durchfließt der Ischim das seit den 1990er Jahren neu errichtete Regierungsviertel und Geschäftszentrum von Astana und ist hier künstlich verbreitert, um einen repräsentativeren Eindruck zu erzeugen. 

Westlich Astanas fließt erneut Wasser - hier auf natürlichem Wege und ca. ein Drittel ihrer gesamten Wassermenge - aus dem südlich gelegenen Binnendelta der Nura in den Ischim. 

Weiter westlich bei Derschawinsk knickt der Fluss scharf nach Norden ab und fließt wenig später an der im Kasachischen namensgebenden Stadt Jessil vorbei. Hier wird er vom südlichen Ast der Trans-Eurasia-Strecke überquert, die Teil der "Neuen Seidenstraße" von China nach Europa ist. Weitere 140 km flussabwärts wurde 1969 die Sergejew-Talsperre zur Stromerzeugung und Bewässerung errichtet. In seinem schiffbaren Unterlauf passiert er Petropawl und, bereits auf russischem Gebiet, die im Russischen namensgebende Stadt Ischim. Bei Ust-Ischim mündet er in den Irtysch.

Für die Größe seines Einzugsgebietes von 177.000 km² hat der Ischim eine eher geringe Wasserführung. Sie beträgt 215 Kilometer vor der Mündung 56,3 m³/s (Pegel Wikulowo) und gut 35 Kilometer vor der Mündung rund 83 m³/s (Pegel Orechowo). Bei Wikulowo wurde ein maximaler Durchfluss von 686 m³/s registriert. Der Oberlauf liegt im Bereich der winterkalten kontinentalen Steppen Innerasiens, zur Mündung hin umgibt den Ischim die weniger trockene südliche Taiga. Hierdurch erklärt sich die deutliche Volumenzunahme des Flusses kurz vor der Mündung.

"Siehe auch:" Liste der längsten Flüsse der Erde



</doc>
<doc id="13756" url="https://de.wikipedia.org/wiki?curid=13756" title="Journalistische Darstellungsform">
Journalistische Darstellungsform

Journalistische Darstellungsformen gelten für alle Medien, also Zeitungen, Zeitschriften, Hörfunk, Fernsehen und Internet.
Kennzeichnend für die bundesdeutsche Journalismuslehre nach 1945 ist die vom amerikanischen Journalismus übernommene Trennung von Information und Meinung (Trennungsregel).

Nachrichten und Berichte zu schreiben, ist das journalistische Kernhandwerk. Die Auswahl der Themen erfolgt nach dem Nachrichtenwert, der sich zusammensetzt: a) aus der Aktualität und b) aus dem Wissens-, Unterhaltungs- und Nutzwert.
Mehr als alle anderen Darstellungsformen sind Nachrichten und Berichte am Ziel der Objektivität orientiert; sie müssen sich jeder Wertung enthalten.


Die längeren informationsorientierten Darstellungsformen folgen in der Regel nicht dem Prinzip 'Das Wichtigste zuerst', sondern vielmehr einem roten Faden und einem Spannungsbogen. Im Gegensatz zu den streng "objektiven" Formen Kurzmeldung, Nachricht und Bericht tritt vielfach die Person des Journalisten in Erscheinung – mit subjektiven Beobachtungen und Einschätzungen. Eckart Klaus Roloff nannte Formen wie Reportage, Feature, Porträt und Essay deshalb interpretierende Darstellungsformen. Wolf Schneider und Paul-Josef Raue sprechen von unterhaltenden, Michael Haller von erzählenden Formen.


Während in den informierenden Formen die Meinung des Journalisten nichts zu suchen hat, ist sie das Kennzeichen folgender Darstellungsformen:



Zunächst prägte Emil Dovifat den Begriff der journalistischen „Stilformen“. Zur Abgrenzung bevorzugt die aktuelle deutschsprachige Journalistik nach 1945 die Bezeichnung „journalistische Darstellungsformen“.

Seit den 1950er-Jahren entstand in der Bundesrepublik nach US-amerikanischem Vorbild eine professionelle Journalistenausbildung. Hier stand an, Darstellungsformen zu definieren und zu kategorisieren. Pionierarbeit für Deutschland leistete Walther von La Roche, dessen „Einführung in den praktischen Journalismus“ dies 1975 grundlegend beschrieb. Parallel unterschied in der DDR eine differenzierte journalistische Genre-Theorie informatorische, analytische und bildhaft konkrete Mitteilungsweise.

Wolf Schneider und Paul Josef Raue erweiterten die informierenden und meinungsäußernden Formen um die Kategorie "Unterhaltung". Eine ähnliche wissenschaftliche Definition der Darstellungsformen findet sich bei Siegfried Weischenberg („Hamburger Schule“). Nach der „Mainzer Schule“ (Fischer Lexikon) kommen als dritte Kategorie die "phantasiebetonten Formen" wie Hörspiel und Kurzgeschichte (also literarische Formen) hinzu.

In der Praxis bewährt hat sich die Definition von Michael Haller („Leipziger Schule“), die zwischen "objektiven" und "subjektiven" Formen unterscheidet. Dagegen sieht Eckart Klaus Roloff als dritte Form die interpretierenden Textgattungen wie Porträts, Features, Interviews, Essays und Reportagen.

Um die längeren informationsorientierten Formen wie Interview, Reportage und Feature in Aufbau und Erzählhaltung zu beschreiben, haben Michael Haller und Christoph Fasel vorgeschlagen, sie als „erzählende“ Darstellungsformen innerhalb der informierenden Darstellungsformen zu benennen, vgl. auch Storytelling.

In unterschiedlicher Ausprägung finden sich die journalistischen Darstellungsformen in den journalistischen Formaten wieder.

Beispiele dafür sind

"Ein Bild sagt mehr als 1000 Worte": Diese alte Weisheit (auch in Redaktionen) ist im visuellen Zeitalter der Bilderfluten elektronischer Medien immer noch wichtig. Mit dem Fotojournalismus beschäftigt sich deshalb ein eigener Beitrag.

Die meisten Tageszeitungen sind für den Mantel-Teil auf Agentur-Fotos angewiesen. Ein Großteil der eingehenden Agentur-Fotos verteilt sich entsprechend den Nachrichtenfaktoren auf die Kategorien: Prominente aller Art, Katastrophen (Unfälle, Wetter, Erdbeben) und Konflikte.
Im Lokalbereich resultiert ein Großteil der Fotos aus (offiziellen) Terminen. Diese werden von manchen ein wenig spöttisch „Schüttelbilder“ genannt und zeigen z. B. Ehrungen, Einweihungen, kommunalpolitische und vereinsorientierte Anlässe. Illustrierte legen seit jeher Wert auf qualitativ gute Bilder. Sie beschäftigen eigene und freie Fotografen, welche es möglich machen, Reportagefotos und ganze Fotostrecken zu bringen.


Ein informierender journalistischer Beitrag (im Druck oder auf Webseiten) besteht meist aus einer Überschrift, einem Vorspann und dem eigentlichen Text.


Zusätzlich kann der Beitrag enthalten

Kommentare sind immer namentlich oder mit einem eindeutig zuzuweisenden Autorenkürzel gekennzeichnet.





</doc>
<doc id="13757" url="https://de.wikipedia.org/wiki?curid=13757" title="Fraunhofer-Gesellschaft">
Fraunhofer-Gesellschaft

Die Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e. V. (Fraunhofer) ist mit über 25.000 Mitarbeitern die größte Organisation für angewandte Forschungs- und Entwicklungsdienstleistungen in Europa, gefolgt vom niederländischen Institut TNO. Sie stellt einen wichtigen Teil der deutschen Forschungslandschaft dar, die unter anderem aus Hochschulen (insbesondere den Universitäten), Max-Planck-Gesellschaft, Helmholtz-Gemeinschaft Deutscher Forschungszentren, Wissenschaftsgemeinschaft Gottfried Wilhelm Leibniz und der Deutschen Forschungsgemeinschaft besteht. Der Sitz der Zentrale ist in München.

Namensgeber ist Joseph von Fraunhofer (1787–1826). Dessen hervorragende Leistung bestand in der Verbindung von exakter wissenschaftlicher Arbeit und deren praktischer Anwendung für neue, innovative Produkte. Joseph von Fraunhofer war als Forscher, Erfinder und Unternehmer gleichermaßen erfolgreich und wurde deshalb zum Vorbild und Namenspatron der heutigen Fraunhofer-Gesellschaft gewählt.

Gegründet im Jahr 1949 ist es das Ziel der Fraunhofer-Gesellschaft, anwendungsorientierte Forschung zum unmittelbaren Nutzen für Unternehmen und zum Vorteil der Gesellschaft durchzuführen. Die Fraunhofer-Gesellschaft betreibt derzeit mehr als 80 Forschungseinrichtungen, davon 69 Institute, an über 40 Standorten in ganz Deutschland. Über 25.000 Mitarbeiter, überwiegend mit natur- oder ingenieurwissenschaftlicher Ausbildung, bearbeiten das jährliche Forschungsvolumen von 2,3 Milliarden Euro. Davon fallen knapp 2,0 Milliarden Euro auf den Leistungsbereich Vertragsforschung. Über 70 % dieses Leistungsbereichs erwirtschaftet die Fraunhofer-Gesellschaft mit Aufträgen aus der Industrie und mit öffentlich finanzierten Forschungsprojekten. Der Rest wird von Bund und Ländern beigesteuert, auch um damit den Instituten die Möglichkeit zu geben, Problemlösungen vorzubereiten, die in fünf oder zehn Jahren für Wirtschaft und Gesellschaft aktuell werden (Vorlaufforschung). Mitglieder der als gemeinnützig anerkannten "Fraunhofer-Gesellschaft" sind namhafte Unternehmen und private Förderer. Circa 3 Prozent des Finanzvolumens der Fraunhofer-Gesellschaft fallen auf den Leistungsbereich Ressortforschung für das Bundesministerium der Verteidigung.

Rund 30 Prozent ihrer Aufwendungen erhält die Gesellschaft von Bund (90 Prozent) und Ländern (10 Prozent) als institutionelle Förderung, um Vorlaufforschung zu betreiben. Planungssicherheit durch kontinuierliche Etatsteigerungen ist mit dem Pakt für Forschung und Innovation gegeben. Die übrigen etwa 70 Prozent der Aufwendungen muss sie durch eigene Erträge decken, wobei dies sowohl Aufträge aus der Industrie als auch öffentlich finanzierte Forschungsprojekte (Bund, Länder, EU) einschließt. Dabei kommt das so genannte Fraunhofer-Modell zum Tragen: Seit 1973 bemisst sich die Höhe der Grundfinanzierung weitgehend erfolgsabhängig nach der Höhe der Wirtschaftserträge. Dieses Prinzip gilt für die Gesellschaft im Ganzen ebenso wie für die einzelnen Institute. Damit wird der politische Wille umgesetzt, die Fraunhofer-Gesellschaft zum führenden Anbieter der angewandten Forschung zu machen. Gleichzeitig erhalten die Gesellschaft und ihre Institute die Flexibilität, auf sich ändernde Rahmenbedingungen in der Wissenschaft und auf den Märkten autonom zu reagieren.

Die Institute sind rechtlich keine selbständigen Einheiten. Faktisch gewährt das Fraunhofer-Modell den Instituten dennoch einen sehr hohen Grad an Unabhängigkeit von der Münchner Zentrale. Die Institute und ihre Leiter tragen selbständig die Verantwortung für Projektergebnisse, für die eigene Bedeutung in der wissenschaftlichen Welt und vor allem für die eigene Finanzierung. Daraus ergibt sich einerseits ein hohes Maß an Eigenständigkeit in der fachlichen Schwerpunktsetzung, in der Allokation der Ressourcen, in der Projektakquisition und im Projektmanagement, andererseits auch ein gewisser wirtschaftlicher Druck und ein Zwang zur Kunden- und Marktorientierung. In diesem Sinne handeln die Institute und ihre Mitarbeiter unternehmerisch und verbinden im Idealfall die Forschung, die Innovation und das Unternehmerische so, dass die Orientierung am Namenspatron Joseph von Fraunhofer erfüllt wird.

Zahlreiche Innovationen in Produkten und Verfahren gehen auf Forschungs- und Entwicklungsarbeiten in Fraunhofer-Instituten zurück. Die Institute arbeiten dazu auf praktisch allen anwendungsrelevanten Technologiefeldern, so z. B. in Mikroelektronik, Informations- und Kommunikationstechnik, Life Sciences, Werkstoffforschung, Energietechnik oder Medizintechnik. Eine der bekanntesten Fraunhofer-Entwicklungen ist das Audiodatenkompressionsverfahren MP3. Im Jahr 2011 hat die Fraunhofer-Gesellschaft 673 neue Erfindungen gemeldet. Dies entspricht etwa drei Erfindungen pro Werktag. Davon wurden 494 Entwicklungen zum Patent angemeldet. Der Bestand aktiver Schutzrechte und Schutzrechtsanmeldungen erhöhte sich auf 6131.

Am 26. März 1949 erfolgte die Gründung in München durch Vertreter der Industrie und Wissenschaft, des Landes Bayern und der gerade entstehenden Bundesrepublik.

1952 erklärten das Bundeswirtschaftsministerium und der Stifterverband die Gesellschaft zur dritten Säule in der außeruniversitären deutschen Forschungslandschaft neben der Deutschen Forschungsgemeinschaft und der Max-Planck-Gesellschaft. Das Ziel der "Fraunhofer-Gesellschaft", angewandte Forschung auch mit eigenen Einrichtungen zu unterstützen, blieb aber lange umstritten.

Ab 1954 entstanden die ersten eigenen Institute, ab 1956 auch solche im Bereich des Verteidigungsministeriums. 1959 verfügte die Fraunhofer-Gesellschaft über neun eigene Institute mit 135 Mitarbeitern und einem Finanzvolumen von 3,6 Millionen Mark (ca. 6,6 Millionen Euro 2013).

1965 empfahl der Wissenschaftsrat den allgemeinen Ausbau der außeruniversitären Forschungseinrichtungen und insbesondere der Fraunhofer-Gesellschaft als Trägerorganisation der angewandten Forschung. 1968 geriet die Fraunhofer-Gesellschaft wegen der von ihr betriebenen militärischen Forschung in die öffentliche Kritik.

1969 arbeiteten mehr als 1200 Mitarbeiter in 19 Instituten und der Zentralverwaltung. Das Budget der Fraunhofer-Gesellschaft lag bei 33 Millionen Mark. Eine „Kommission zur Förderung des Ausbaus der Fraunhofer-Gesellschaft“ plante die weitere Entwicklung der FhG. Die Kommission entwickelte das später sogenannte Fraunhofer-Modell, das die Grundfinanzierung der Fraunhofer-Gesellschaft von ihrem Erfolg bei der Akquisition von Forschungsaufträgen abhängig macht.

1973 wurde das „Fraunhofer-Modell“ vom Bundeskabinett und der Bund-Länder-Kommission verabschiedet. Im selben Jahr zogen Vorstand und Zentralverwaltung in der Leonrodstraße 54 in München in ein gemeinsames Gebäude. Das Fraunhofer-Programm zur Förderung der Vertragsforschung für kleinere und mittlere Unternehmen lief an und gewann in den Folgejahren immer mehr an Bedeutung. Forschungs- und Verteidigungsministerium teilten sich 1977 die politische Verantwortung für die FhG. Bund und Länder teilten im Bereich der zivilen Forschung die Förderung im Verhältnis 9 zu 1 auf.

1984 hatte die Fraunhofer-Gesellschaft 3500 Mitarbeiter in 33 Instituten mit einem Forschungsvolumen von 360 Millionen Mark. 1988 lag der Anteil der Verteidigungsforschung am gesamten Aufwand der Fraunhofer-Gesellschaft nur noch bei 10 Prozent. 1989 hatte die FhG dann fast 6400 Mitarbeiter in 37 Instituten mit einem Gesamtvolumen von 700 Millionen Mark im Jahr.

1991 wurden nach der Deutschen Wiedervereinigung zahlreiche Forschungseinrichtungen der ehemaligen DDR als befristete Einrichtungen oder als Außenstellen bereits existierender Institute in die Fraunhofer-Gesellschaft integriert.

1993 überschritt die Gesellschaft ein Gesamtfinanzvolumen von einer Milliarde Mark. Das von ihr vorgelegte „Leitbild 2000“ definiert die Gesellschaft als markt- und kundenorientierte, national und international aktive Trägerorganisation für Institute der angewandten Forschung.

Von 2000 bis 2001 wurden auf Initiative des Bundesministeriums für Bildung und Forschung die GMD-Forschungszentrum Informationstechnik GmbH und die Fraunhofer-Gesellschaft fusioniert.

Im Jahr 2000 gelang ein besonderer Erfolg am Fraunhofer-Institut für Integrierte Schaltungen IIS: MP3, das heute weltweit verbreitete Verfahren zur Kodierung und Komprimierung von Musikdaten, stieg zum meistbenutzten Codec für Audiodateien auf und machte Fraunhofer weltweit auch außerhalb des Forschungsbereichs bekannt.

Nach zweijähriger Pilotphase wurde 2001 die Fraunhofer-Venture-Gruppe als eigenständige Abteilung in der Zentrale der Fraunhofer-Gesellschaft in München etabliert. Sie ist dort für das Ausgründungs- und Beteiligungsprogramm zuständig.

2002 wurde das Heinrich-Hertz-Institut für Nachrichtentechnik Berlin GmbH (HHI) aus der Wissenschaftsgemeinschaft Gottfried Wilhelm Leibniz e. V. (GWL) in die Fraunhofer-Gesellschaft überführt. Mit dieser Integration überschritt die Fraunhofer-Gesellschaft erstmals die Grenze von 1 Milliarde Euro Finanzvolumen.

2003 bezog die Zentrale der Fraunhofer-Gesellschaft ein eigenes Hochhaus im Stadtteil Sendling-Westpark in München. In einer »Mission« werden die grundsätzlichen Unternehmensziele zusammengefasst, in den »Werten und Leitlinien« wird die angestrebte Unternehmenskultur beschrieben. Damit soll den Mitarbeitern eine bessere Möglichkeit gegeben werden, sich mit dem Unternehmen zu identifizieren und das eigene kreative Potential freizusetzen.

2004 erhielt die bisherige Fraunhofer-Arbeitsgruppe für elektronische Medientechnologie am Fraunhofer-Institut für Integrierte Schaltungen IIS den Status eines selbstständigen Instituts. Sie wurde zum Fraunhofer-Institut für Digitale Medientechnologie IDMT. Neue Allianzen und Themenverbünde helfen dabei, die Marktpräsenz der Fraunhofer-Institute in bestimmten Kompetenzbereichen zu verstärken.

2005 entstanden in Leipzig das Fraunhofer-Institut für Zelltherapie und Immunologie IZI und das Fraunhofer-Center Nanoelektronische Technologien CNT in Dresden. 2006 wurde in Leipzig das Fraunhofer-Zentrum Mittel- und Osteuropa MOEZ gegründet.

2006 wurden die Aktivitäten der Fraunhofer-Gesellschaft im Bereich der wissenschaftlichen Weiterbildung in der Fraunhofer Academy gebündelt. Die Geschäftsstelle befindet sich in München. Die Fraunhofer Academy bietet Weiterbildung für Fach- und Führungskräfte aus der Wirtschaft.

2007 wurde in Portugal das "Fraunhofer-Forschungszentrum für Assistierende Informations- und Kommunikationsumgebungen" gegründet.
Dieses ist jetzt als "Fraunhofer Portugal Research Center for Assistive Information and Communication Solutions" (FhP-AICOS) das erste Institut des 2008 gegründeten portugiesischen Ablegers der Fraunhofer-Gesellschaft "Associação Fraunhofer Portugal Research".

2008 wurde mit der Fraunhofer Austria Research GmbH eine weitere Tochtergesellschaft in Europa gegründet; Sitz ist Wien. Fraunhofer Austria umfasst zwei Geschäftsbereiche: "Produktions- und Logistikmanagement" in Kooperation mit der Technischen Universität Wien und "Visual Computing" in Kooperation mit der Technischen Universität Graz.

Zum 1. Januar 2009 wurde die MeVis Research GmbH, Bremen als Fraunhofer-Institut für Bildgestützte Medizin MEVIS (Fraunhofer MEVIS) in die Fraunhofer-Gesellschaft aufgenommen.
In Bremerhaven wurde zum gleichen Datum aus der Fraunhofer-Einrichtung CWMT ein eigenständiges Institut, das im Laufe des Jahres 2009 mit dem Institut für Solare Energieversorgungstechnik zum neuen Institut für Windenergie und Energiesystemtechnik (IWES) zusammengeführt wurde. Am 17. August 2009 wurden die drei wehrtechnischen Institute der FGAN Fraunhofer-Institute.

2011 wurde mit dem Modell „Fraunhofer-Anwendungszentrum“ die Kooperation mit ausgewählten forschungsstarken Fachhochschulen begonnen. Die deutschlandweit erste Einrichtung dieser Art ist das Fraunhofer-Anwendungszentrum Industrial Automation in Kooperation mit dem Institut für industrielle Informationstechnik der Hochschule OWL in Lemgo, Nordrhein-Westfalen.

2012 wurde die Zusammenarbeit mit Fachhochschulen weiter ausgebaut. Es entstanden im Sommer zwei neue Fraunhofer-Anwendungszentren. 
Durch eine Kooperation zwischen dem Braunschweiger Fraunhofer-Institut für Holzforschung (WKI) und der Fachhochschule Hannover wurde das Fraunhofer-Anwendungszentrum für Holzfaserforschung HOFZET im Sommer gegründet. 
Zur gleichen Zeit wurde durch das ebenfalls in Braunschweig ansässigen Fraunhofer-Institut für Schicht- und Oberflächentechnik und die HAWK Göttingen das Anwendungszentrum für Plasma und Photonik gegründet.

Am 1. Januar 2017 wurde in Ostwestfalen-Lippe das Fraunhofer-Institut für Entwurfstechnik Mechatronik IEM gegründet. Es startete 2011 als Projektgruppe, angelehnt an das Fraunhofer IPT. Nach mehr als 20 Jahren wurde damit in Nordrhein-Westfalen wieder ein neues Fraunhofer-Institut eröffnet.

Die Gesellschaft besteht heute aus mehr als 80 Forschungseinrichtungen, darunter 69 Institute.




</doc>
<doc id="13759" url="https://de.wikipedia.org/wiki?curid=13759" title="Nachricht">
Nachricht

Nachricht bezeichnet alltagssprachlich den Inhalt einer Information. Die Mitteilung einer Nachricht ist die Benachrichtigung. In der Informationstheorie ist eine Nachricht eine Information, die sich quantitativ als Signal und qualitativ als bedeutungstragendes Zeichen oder Zeichenfolge beschreiben lässt. Sie wird im Prozess der Kommunikation nach Maßgabe eines Codes von einem Sender an einen Empfänger übermittelt. Für den Empfänger hat sie einen Neuigkeitswert und ruft über die Stimulation hinaus bei ihm eine interpretative Reaktion hervor.

Nachrichten sind das Grundelement der Kommunikation und damit Gegenstand der Betrachtung durch verschiedene Wissenschaften, unter anderem der Semiotik, der Informationstheorie, der Kommunikationswissenschaften, der Nachrichtentechnik und der Verhaltensforschung.

Laut dem Deutschen Wörterbuch ist die Nachricht eine „mitteilung zum darnachrichten“.
Seit etwa 1600 steht Nachricht für eine ‚Mitteilung‘. Im 16. bis 18. Jahrhundert ist "Nachrichtung" für etwas ‚wonach man sich zu richten hat, Anweisung‘ bezeugt, was aber allmählich verdrängt wird. Im Plural werden seit dem 20. Jahrhundert mit Nachrichten ‚(über Rundfunk, Fernsehen gesendete) aktuelle, besonders politische Meldungen‘ bezeichnet. Die neueste Nachrichtendefinition stammt von Dietz Schwiesau und Josef Ohler: „Die Nachricht ist eine direkte, auf das Wesentliche konzentrierte und möglichst objektive Mitteilung über ein neues Ereignis, das für die Öffentlichkeit wichtig und/oder interessant ist.“

In der Semiotik entfalten Zeichen auf der Semantik-Ebene ihre Bedeutung.

Eine Nachricht wird im Allgemeinen durch Signale übertragen, die, wenn sie verstanden wurde, dem Empfänger Wissen bringt. Dabei ist es entscheidend, dass der Empfänger in der Lage ist, die Nachricht zu entschlüsseln, dass also die Kodierung (Sprache, Datenformat etc.) der Nachricht bekannt ist, und dass ein semantischer Kontext vorhanden ist, so dass die "Bedeutung" der Nachricht verstanden werden kann. Eine Nachricht ist in diesem Sinne Information im Zustand der Übertragung. Wird zusätzlich Information übermittelt, die hilft, den Inhalt der Nachricht zu dekodieren, so spricht man von einer Metabotschaft.

Die technische Übertragung bzw. Übermittlung ist die Aufgabe der Nachrichtentechnik. In der Netzwerktechnik versteht man unter "Nachricht" ein Datenpaket.

Häufig steht "Nachrichten" synonym für "Neuigkeiten" und Informationen über Ereignisse. Im allgemeinen Sprachgebrauch steht deshalb der Plural "Nachrichten" für Berichte über aktuelle Ereignisse in den Massenmedien, wie Fernsehen, Radio oder Zeitungen und Zeitschriften. Dabei unterscheidet man zwischen „guten Nachrichten“, wie zum Beispiel die Rettung oder Genesung von Menschen, und „schlechten Nachrichten“, wie Berichten über Katastrophen, Sterbefälle oder Unglücke.

In der Bibel steht „Gute Nachricht“ gleichbedeutend für die vier Evangeliumsberichte über das Leben und Wirken Jesu von Nazaret.





</doc>
<doc id="13760" url="https://de.wikipedia.org/wiki?curid=13760" title="Transsibirische Eisenbahn">
Transsibirische Eisenbahn

Die Transsibirische Eisenbahn (, Transkription "Transsibirskaja magistral"; früher auch als "Sibirische Eisenbahn" bezeichnet, amtlich jedoch nur für die Teilstrecke vom Ural bis zum Baikalsee), kurz Transsib genannt, ist mit 9288 km die längste Eisenbahnstrecke der Welt, mit mehr als 400 Bahnhöfen zwischen Moskau und Wladiwostok am Pazifik. Sie ist die Hauptverkehrsachse Russlands.

Der Regelbetrieb der Transsibirischen Eisenbahn wird von der staatlichen Russischen Eisenbahngesellschaft (RŽD) durchgeführt. Wie die meisten Eisenbahnstrecken des Landes wurde sie in Breitspur mit einer Spurweite von 1520 mm errichtet.

In der zweiten Hälfte des 19. Jahrhunderts konnte Russland zur Ausbeutung der sibirischen Reichtümer unmöglich länger auf Pferdefuhrwerke und Lastkähne setzen, so dass in den 1870er Jahren Planungen für eine Eisenbahn durch ganz Sibirien begannen. Nachdem die russische Eisenbahn 1886 den Ostrand des Ural erreicht hatte, wurden verschiedene Trassenführungen erwogen. Finanzminister Iwan Alexejewitsch Wyschnegradski wollte Inselbetriebe bauen lassen und mit einer modernisierten Flussschifffahrt kombinieren. Aber Zar Alexander III. entschied sich auf Anraten von Verkehrsminister (ab 1892 Finanzminister) Sergei Juljewitsch Witte, der selbst Erfahrungen in der Eisenbahnwirtschaft hatte, für eine durchgehende Bahnstrecke, die Transsib.

Witte kalkulierte, dass Russland durch die Transsib einen leichteren Zugang zum chinesischen Markt hätte, so dass auch der europäische Handel mit China zum Teil auf diesen Weg verlagert werden könnte. So war z. B. beabsichtigt, den chinesischen Teehandel, den Großbritannien durch den indischen Tee zerstört hatte, wieder zu beleben. Ebenso wurde es durch eine Eisenbahn wirtschaftlich, sibirisches Getreide in den europäischen Teil Russlands und nach Russisch-Mittelasien zu transportieren. Dazu kam die Erwartung, dass die Bahn die sibirische Wirtschaft ankurbeln und ausländische Investitionen anlocken würde.

Das Projekt zum Bau der Transsib wurde von Finanzminister Witte geleitet. Das Investitionsvolumen wurde anfangs auf 325 Millionen Rubel geschätzt, was angesichts der russischen Staatsschulden nur durch die Aufnahme von Anleihen im Ausland, insbesondere in Frankreich und Belgien, möglich war. Diese Kredite wurden buchhalterisch als „Einnahmen“ verbucht. Zum Bau wurden russische Geräte und einheimisches Material verwendet, wodurch die einheimische Produktion an Eisen, Stahl, Kies, Zement und Holz einen starken Aufschwung nahm. Ein Drittel der russischen Jahresproduktion an Roheisen wurde zum Bau der Transsib verwendet.

Über die endgültigen Baukosten gibt es abweichende Angaben. Ähnlich wie bei anderen staatlichen Großinvestitionen in Sibirien und anderswo überschritten die endgültigen Kosten von mehr als einer Milliarde Rubel das prognostizierte Investitionsvolumen bei weitem. Die durchschnittlichen Baukosten sollen 72.000 Rubel pro Kilometer betragen haben, bei der Baikalsee-Umgehung 197.000 Rubel pro Kilometer.

Im März 1891 proklamierte Zar Alexander III. den Baubeginn für die Transsib und der damalige Zarewitsch Nikolai, der spätere letzte Zar von Russland, führte in der Nähe von Wladiwostok den ersten Spatenstich durch.

Im Oktober 1916 wurde die Transsib mit der Einweihung der Amurbrücke bei Chabarowsk fertiggestellt.

Aufgrund der riesigen Entfernungen wurde der Bau der Strecke in verschiedenen Regionen zeitgleich durchgeführt. Die heute noch existierenden Eisenbahnverwaltungen erhielten ihre Namen nach diesen Bauabschnitten:

Dieser Abschnitt wurde von 1891 bis 1897 gebaut und erstreckt sich von Wladiwostok aus über 800 km.
Er war mangelhaft geplant. Am Amur mussten ganze Streckenabschnitte neu vermessen werden, als sich herausstellte, dass die Trasse im Überflutungsbereich des Flusses verlief, der im Frühjahr 10 Meter Hochwasser führte. Am Ussuri verschütteten Erdrutsche bereits fertiggestellte Bahndämme, Oberbau und Gleise versanken, wenn der Permafrostboden oberflächlich taute.

Von Tscheljabinsk am Ural aus startete 1893 ein zweiter 1920 km langer Bauangriff von Westen. 1894 war Omsk am Irtysch erreicht, im Folgejahr Nowosibirsk am Ob. In Krasnojarsk am Jenissej traf der erste Zug am 6. Dezember 1896 ein, in Irkutsk am 16. August 1898.
Zu den Schwierigkeiten dieses Bauabschnittes gehörten als besondere Ingenieurbauwerke Brücken über die großen sibirischen Ströme und die Notwendigkeit, in der zentralsibirischen Barabasteppe Brunnen ausheben zu müssen, da das Oberflächenwasser für Dampflokomotiven nicht verwendbar war.

Im Bereich des Baikalsees wurde – von Westen kommend – das Gelände gebirgig und schwieriger. Erstmals mussten Tunnel angelegt werden – mehr als 30 – und 200 Brücken waren auf einer Länge von nur 260 km erforderlich. Der Bauabschnitt vom Ostufer des Baikalsees war 1072 km lang und wurde in den Jahren von 1895 bis 1900 errichtet. Auch hier war die Planung mangelhaft. 1897 wurden durch eine Überschwemmung 300 km Strecke samt 15 Brücken weggespült.

Der Streckenverlauf am Baikalsee war lange Zeit umstritten. Zwei Streckenverläufe standen zur Auswahl. Der eine – zwischen Irkutsk und Baikalsee etwa dem heutigen Verlauf entsprechend – war wegen der starken Steigungen umstritten, die damals zur Verfügung stehenden Lokomotiven hätten ihn vermutlich nicht bewältigt. Diese Strecke hatte den Nachteil hoher Kosten aufgrund vieler Brücken, Tunnels und Uferbefestigungen und beinhaltete einen erheblichen Umweg.

Der umgesetzte Alternativvorschlag war die Baikalbahn von Irkutsk entlang des Ufers der Angara zum Baikalsee. Von dort verkehrten im Sommer zwei Dampfschiffe mit Eisbrecherqualitäten, die als Trajekte Wagen zum gegenüberliegenden Ufer des Sees übersetzten. Die Schiffe wurden in England gebaut, in Einzelteile zerlegt, an den Baikalsee transportiert und dort zusammengebaut. Ab 1900 transportierten die Eisenbahnfähre "Baikal" (im Bürgerkrieg schwer beschädigt und unweit des Hafens Myssowaja versenkt) und die Personenfähre "Angara" (heute Museum in Irkutsk) Wagen, Ladung bzw. Reisende über den See. Im Winter wurden Ladung und Reisende mittels Pferdeschlitten über den zugefrorenen Baikal gebracht. Ab Januar 1901 wurden auch Schienen auf dem Eis des Baikalsees verlegt. Dabei wurden jedoch die Wagen und gelegentlich auch in zwei Teile zerlegte Lokomotiven einzeln von Pferden über den See gezogen. Eine Lokomotive versank dabei im Baikalsee. Während des Russisch-Japanischen Kriegs reichte diese Lösung wegen der zu geringen Kapazität nicht mehr aus: Der Bau der Baikalsee-Umfahrung wurde forciert und im Herbst 1904 fertiggestellt.

Aufgrund der extremen klimatischen Bedingungen – bis zu −50 °C im Winter und Bodenfrost bis in den Juni hinein – war und ist der mögliche Zeitraum für Arbeiten kurz. Brücken wurden zunächst nur aus Holz und erst im Nachhinein aus Stein oder Stahl errichtet, um schneller voranzukommen. Das hatte den Nachteil, dass sie durch Funkenflug in Brand geraten konnten. Viele Baumaterialien (außer Holz und Steinen) mussten den Seeweg über Odessa nach Wladiwostok nehmen.

Zunächst wurde die Strecke eingleisig ausgebaut. Aus Kostengründen wurden bei Qualität von Material und Ausbau die unteren Grenzen des Vertretbaren gewählt. Vom Planungskomitee wurden dafür die technischen Anforderungen abgesenkt. Die Gleise waren nur halb so schwer wie üblich und bogen sich (andernfalls wären sie bei Tauwetter aber auch schneller eingesunken) und Schwellen verfaulten im Boden. Tunnelbau wurde zugunsten starker Neigungen und enger Bögen vermieden, so dass die Höchstgeschwindigkeit stellenweise nur 20 km/h betrug. Nach einem Frühlingsregen „hüpften die Züge wie Eichhörnchen vom Gleis“, wie ein verbitterter Ingenieur bemerkte, so dass es im ersten Betriebsjahr zu bis zu drei Unfällen pro Tag kam.

Je weiter die Baustellen im Osten lagen, desto häufiger ersetzten die um die Hälfte billigeren Saisonarbeiter aus China, Korea und Japan russische Lohnarbeiter (45 Rubel Monatslohn). Auch Strafgefangene und Zwangsarbeiter wurden dort erstmals eingesetzt. Nur 29 Prozent der Arbeiter stammten aus Sibirien. Jeder vierte Steinmetz für den Brückenbau kam aus Italien. Die Gesamtzahl der 1895 tätigen Bauarbeiter betrug fast 30.000. Schätzungen zufolge waren an den verschiedenen Streckenabschnitten bis zu 90.000 Arbeiter gleichzeitig mit dem Bau beschäftigt.

Unzureichende Arbeitssicherheit und zahlreiche in Asien noch weit verbreitete Krankheiten, deren Auswirkungen sich durch chronischen Ärztemangel und fehlende sanitäre Anlagen verschlimmerten, dezimierten die Bautrupps. Zehntausende kamen bei dem Bahnbau ums Leben.

Bereits im Februar 1903 wurde als Abkürzung der Transsib zwischen Tschita und Wladiwostok die Chinesische Osteisenbahn (auch "Transmandschurische Eisenbahn") über chinesisches Staatsgebiet eröffnet. Eine rasche Abfolge verschiedener Schwierigkeiten ließ deren Baukosten in die Höhe schnellen: 1899 und 1901 brach Beulenpest und 1902 Cholera aus. 1900 zerstörten Bauarbeiter, die sich dem Boxeraufstand angeschlossen hatten, rund 700 Kilometer Gleise.

Zu Beginn des Russisch-Japanischen Kriegs im Februar 1904 war die Kapazität der Transsib auf zehn Züge pro Tag und Richtung beschränkt. Bis zum Kriegsende konnte die Kapazität jedoch mehr als verdoppelt werden. Doch auch dies reichte militärisch nicht aus: Russland unterlag. Das hatte zur Folge, dass ab 1908 streckenweise begonnen wurde, ein zweites Gleis zu errichten. Für den entsprechenden Ausbau der Gesamtstrecke wurden mehr als 500 Millionen Rubel veranschlagt. Der komplette zweigleisige Ausbau konnte allerdings erst nach dem Zweiten Weltkrieg fertiggestellt werden. Auch wurde 1908 erneut mit dem Bau der Transsib entlang des Amur begonnen. Dessen Niedrigwasser verhinderte, dass hier ganzjährig Schifffahrt möglich war.

Die Transsibirische Eisenbahn hatte unmittelbar positive Auswirkungen auf die Wirtschaft des Gebietes, das sie erschloss: Auslandsinvestitionen in Bergbau, Handel, Eisenbahnen und Fabriken, verbunden mit der Errichtung von Konsulaten und Außenhandelsbüros in Wladiwostok waren Zeichen des wirtschaftlichen Aufschwungs. Gehandelt wurde mit Holz, Kohle und Lebensmitteln.

Ein weiteres Zeichen des wirtschaftlichen Aufschwungs war die Zuwanderung. Bei Baubeginn (1891) hatte Sibirien rund fünf Millionen Einwohner. Aber allein zwischen 1903 und 1914 siedelten sich rund vier Millionen Bauern entlang der Trasse an. Der Fahrpreis für Zuwanderer betrug pro Familie nur fünf bis zehn Rubel, da die Zuwanderung im staatlichen Interesse lag.

Der zweispurige Ausbau der Transsib wurde nach dem Zweiten Weltkrieg fertiggestellt. Im Zusammenhang mit der Projektierung des Irkutsker Stausees an der Angara wurde die ursprünglich verworfene Direktverbindung von Irkutsk bis Sljudjanka über einen Pass des Baikalgebirges mit mehreren Eisenbahntunneln errichtet. Die neue Strecke ging 1949 in Betrieb. Beide Strecken wurden zunächst parallel betrieben, wobei die neue weiter ausgebaut und bis 1956 elektrifiziert wurde. Nach Flutung des Irkutsker Stausees, der 1959 fertiggestellt war, wurde die alte Strecke zwischen Irkutsk und dem Baikalsee stillgelegt. Der Abschnitt Sljudjanka – Port Baikal der Bahn wurde damit zu einer Stichstrecke von nur noch lokaler bzw. touristischer Bedeutung.

In den 1950er und 1960er Jahren wurden von der Transsib ausgehend mehrere Stichbahnen nach Norden und Süden angelegt, um die Holzeinschlaggebiete der Taiga und die Getreidekammern der Steppe besser anzubinden. So entstanden etwa die "Südsibirische Bahn" von Jurga über Nowokusnezk und Abakan bis Taischet, die einen Gürtel von zwei- bis fünfhundert Kilometern um die Transsib verkehrstechnisch erschließen. Aus militärstrategischen Gründen (die Transsib läuft stellenweise unweit der russisch-chinesischen Grenze) wurde eine zweite, nördlicher trassierte Strecke, die Baikal-Amur-Magistrale, verlegt. Sie zweigt in Taischet von der Transsib ab und verläuft etwa 600 Kilometer nördlich von ihr parallel zum Pazifik. 

Weil Wladiwostok früher militärisches Sperrgebiet war, mussten Ausländer von Ussurijsk (km 9177) nach Nachodka (Ausreisehafen für Ausländer nach Yokohama, Japan) fahren. Mit dieser Variante ist die Gesamtstrecke sogar 9438 km lang.

Die durchgehende Elektrifizierung wurde nach 74 Jahren am 25. Dezember 2002 abgeschlossen. Diese erfolgte über Jahrzehnte abschnittsweise:

Die Strecke verläuft über 7000 km von West nach Ost und 1400 km von Nord nach Süd. Sie wird landschaftlich vorwiegend durch Taiga geprägt. Bei Kilometer 1777 (im Ural) markiert ein Obelisk südlich der Gleise die Grenze zwischen Europa und Asien.

An ihrem Verlauf liegen 89 Städte, unter anderem Nischni Nowgorod, Kirow, Perm, Jekaterinburg, Omsk, Nowosibirsk, Krasnojarsk, Irkutsk, Ulan-Ude, Tschita und Chabarowsk. Ein wichtiger Parallelzweig zur Hauptstrecke durchquert um die Stadt Petropawl ("Petropawlowsk") auf einem etwa 180 Kilometer langen Abschnitt kasachisches Gebiet.

Die Transsibirische Eisenbahn überquert 16 große Flüsse (Wolga, Wjatka, Kama, Tobol, Irtysch, Ob, Tom, Tschulym, Jenissei, Oka, Selenga, Seja, Bureja, Amur, Chor und Ussuri). Darüber hinaus verläuft sie 207 km entlang des Baikalsees und 39 km entlang der Amurbucht.

Ursprünglich verlief die Strecke zwischen Moskau und Omsk weiter südlich über Tula, Rjaschsk, Pensa, Samara, Ufa und Petropawlowsk. Erst ab Mitte der 1930er Jahre fuhren die durchgehenden Züge über die noch heute genutzte Strecke.

Von der Transsibirischen Eisenbahn zweigen Strecken nach Zentralasien, die Transmongolische Eisenbahn von Ulan-Ude in die Mongolei und die Volksrepublik China ab. In ihrer Fortsetzung jenseits von Wladiwostok führt die Bahnstrecke Chassan–Rajin nach Nordkorea.

Über die Transsibirische Eisenbahn verkehrt auch die längste durchgehende Zugverbindung der Welt (Moskau–Wladiwostok–Pjöngjang). Jeden zweiten Tag verlässt ein Zug 1/2 ("Rossija") den Jaroslawler Bahnhof in Moskau und in der Gegenrichtung Wladiwostok, um 144 Stunden später in Wladiwostok am Japanischen Meer anzukommen. Zusätzlich verkehrt in ebenfalls zweitägigem Rhythmus Zug Nr. 99/100 bei rund 160 Stunden Fahrzeit. Neben diesen Zugpaaren verkehrt eine Vielzahl anderer Züge auf der Strecke. Bei Touristen beliebt sind die beiden Zugpaare nach Peking. Eines fährt über die Transmongolische Eisenbahn (Nr. 3/4), das andere über die Mandschurei (Nr. 19/20).

Nahezu jede Stadt entlang der Transsibirischen Eisenbahn oder im Umfeld der Strecke hat ein eigenes Zugpaar nach Moskau: Moskau–Omsk, –Nowosibirsk, –Nowokusnezk, –Kemerowo, –Tomsk, –Krasnojarsk, –Abakan, –Irkutsk, –Ulan-Ude, –Sewerobaikalsk, –Tschita oder –Chabarowsk. Aber nicht jeder Fernzug, der die Strecke befährt, fährt nach oder kommt von Moskau, wie die Zugpaare Nowosibirsk–Wladiwostok, Omsk–Nowosibirsk, Nowosibirsk–Krasnojarsk, Krasnojarsk–Irkutsk oder Charkow–Wladiwostok zeigen. Züge verkehren täglich, alle zwei Tage oder wöchentlich. Im Sommer gibt es zusätzlich Saison-Züge von vielen sibirischen Städten ans Schwarze Meer (Adler) sowie in den Kaukasus (Kislowodsk).

Die Fernzüge führen in der Regel zwei Klassen:

Tagsüber verbinden vereinzelt Schnellzüge mit Sitzplätzen Städte entlang der Transsibirischen Eisenbahn, zum Beispiel Omsk-Nowosibirsk. Dieser Markt entwickelt sich jedoch erst. Außerdem gibt es für den Nahverkehr tagsüber Elektritschkas, die alle Haltepunkte bedienen. Theoretisch könnte man von Moskau bis Wladiwostok fast ausschließlich mit Elektritschkas fahren – müsste dafür jedoch über 50-mal umsteigen und einige Wochen Fahrzeit einplanen. Allein zwischen Omsk und Nowosibirsk muss man beispielsweise zweimal umsteigen, ohne direkten Anschluss.

Die Wagen für die Züge sowie das Wagenpersonal stellt prinzipiell der dezentralere der beiden Abfahrtsbahnhöfe. Die Wagen für den Zug Moskau–Tomsk und umgekehrt sind also beispielsweise in Tomsk stationiert. Die Wagen werden auf der tagelangen Fahrt regelmäßig geprüft. Die Bahnhöfe in Moskau, am Schwarzen Meer und im Kaukasus haben kaum eigene Wagen – bei der Vielzahl der Verbindungen wäre kein Platz dafür. Die Lokomotiven hingegen werden unterwegs mehrmals gewechselt, was allein wegen der verschiedenen Stromsysteme entlang der Strecke notwendig ist. Jedes Bahnbetriebswerk betreut etwa 500 Kilometer Strecke.

Eine Fahrkarte von Moskau nach Wladiwostok kostete im durchgehenden Zug beispielsweise im Januar 2013 im 2er-Schlafwagen umgerechnet 922 Euro, im 4er-Schlafwagen 493 Euro und im Großraum-Liegewagen 243 Euro. Allerdings gibt es starke saisonale Schwankungen. 2014 wurden bis zu 50-prozentige Frühbucherrabatte eingeführt.

Der Zustand der russischen Eisenbahnen ist allgemein gut bis befriedigend, die Transsibirische Eisenbahn als Hauptmagistrale der RŽD ist in sehr gutem Zustand. Es gibt kaum Langsamfahrstellen, jedoch auch keine Hochgeschwindigkeitsabschnitte. Mehrfach wurde der Ausbau einzelner Abschnitte zur Hochgeschwindigkeitsstrecke geplant; oft wird der Abschnitt zwischen Omsk und Nowosibirsk genannt, der auf 650 Kilometer Länge nahezu keine Kurven oder Steigungen aufweist. Siemens hatte bereits Vorverträge für den Ausbau und den späteren Wagenpark unterschrieben, im Moment liegt das Projekt jedoch auf Eis.

Die durchschnittliche Reisegeschwindigkeit der Personenzüge auf der Transsib beträgt 60 bis 70 km/h. Güterzüge fahren langsamer.

Mit Fertigstellung der Fernstraße M58 "Amur" 2004 (durchgehend asphaltiert seit 2010) zwischen Tschita und Chabarowsk verlor die Bahn ihr Monopol der Anbindung des russischen Fernen Ostens an den Rest des Landes im Landverkehr.

Zusammen mit der Deutschen Bahn wurden seit 1997 Pläne entwickelt, die Transsibirische Eisenbahn als Transportweg für Güter aus dem Fernen Osten nach Europa zu nutzen. Wegen der Wirtschaftskrise und damit stark gesunkenen Frachtraten auf dem Seeweg wurden die Pläne aber zunächst gestoppt. Seit 2010 werden nun mehrere Güterzüge zwischen China und Europa angeboten. Deren Fracht wird an den Systemschnittgrenzen zwischen Normalspur und Breitspur umgeladen. Seit Ende November 2011 fährt ein täglicher Zug des Trans-Eurasia-Express für BMW vom Werk Leipzig nach Shenyang.

Die chinesische und die russische Regierung beabsichtigen, die Fahrtzeit von Moskau nach Peking von jetzt sechs Tagen auf unter zwei Tage zu verkürzen. Im Oktober 2014 haben sie dazu ein Memorandum unterzeichnet, das den Neubau einer rund 7000 km langen Schnellzugstrecke vorsieht. Sie soll 180 Milliarden € kosten.







</doc>
<doc id="13763" url="https://de.wikipedia.org/wiki?curid=13763" title="Wladiwostok">
Wladiwostok

Wladiwostok (, wiss. Transliteration "Vladivostok" – übersetzt: Beherrsche den Osten; chinesisch 海參崴 / Hǎishēnwǎi – übersetzt: Seegurkenmarsch) ist eine Großstadt am Japanischen Meer mit Einwohnern (Stand: ). Sie ist Russlands wichtigste Hafenstadt am Pazifik, bedeutender Wirtschaftsstandort und Verwaltungszentrum der Region Primorje.

Wladiwostok liegt im Süden der zum Föderationskreis Ferner Osten gehörenden Region Primorje, ist eine Hafenstadt am Japanischen Meer, einem Randmeer des Pazifischen Ozeans, und liegt am südlichen Ende der rund 30 Kilometer langen und 13 Kilometer breiten Murawjow-Amurski-Halbinsel, die die Peter-der-Große-Bucht in Amur- und Ussuribucht teilt. Das Stadtzentrum liegt an der Bucht Goldenes Horn. Südlich von Wladiwostok trennt der Östliche Bosporus die Stadt von der Insel Russki. Das Stadtgebiet umfasst eine Fläche von 56.154 Hektar, darunter sind etwa 50 Inseln. Die höchste Erhebung der Stadt ist mit 257 m der Berg Cholodilnik.

Die Entfernung nach Moskau beträgt über die Transsibirische Eisenbahn 9.288 und über die Luftlinie 6.430 Kilometer. Wladiwostok liegt sieben Zeitzonen östlich von Moskau und grenzt im Norden an den Stadtkreis Artjom.

Wladiwostok liegt etwa auf demselben Breitengrad wie Florenz, dennoch sind die Winter kalt und trocken. Die Sommer sind aufgrund der Nähe zur Monsunzone regnerisch und mild.

Wladiwostok bildet einen gleichnamigen Stadtkreis "(gorodskoi okrug)" und unterteilt sich in folgende Stadtrajons (Stadtbezirke):


Die Region um Wladiwostok gehörte ursprünglich zur chinesischen Äußeren Mandschurei und war von Jurchen und Mandschu bevölkert. Ein französischer Walfänger, der 1852 in die Solotoi-Rog-Bucht einlief, entdeckte am Ufer Hütten chinesisch-mandschurischer Fischer. Der Ort hieß zu dieser Zeit "Haishen-wie". Ab Mitte des 19. Jahrhunderts begann Russland seine Vorherrschaft in Zentralasien und im Fernen Osten auszuweiten. Mit dem Vertrag von Aigun wurde China 1856 gezwungen, sein Territorium links des Heilong Jiangs vom Argun bis zur Ozeanmündung an das Russische Kaiserreich abzutreten. Diesem „Ungleichen Vertrag“ folgte am 18. Oktober 1860 die Pekinger Konvention, womit China sein Küstengebiet östlich vom Ussuri und Unterem Amur vollständig verlor. Bereits am hatten russische Matrosen "Haishen-wie" besetzt und dem Ort den herausfordernd imperialistischen Namen "Wladiwostok" („Beherrsche den Osten“) gegeben.

1862 bekam Wladiwostok einen Hafen. Ein durchdachtes System von Festungen entstand zwischen den 1870er- und 1890er-Jahren. 1880 erhielt Wladiwostok den Stadtstatus. Das städtische Wappen mit dem sibirischen Tiger wurde im März 1883 angenommen. Die Wirtschaft erlebte ab 1903 einen Aufschwung mit der Fertigstellung der Transsibirischen Eisenbahn, die Wladiwostok mit Moskau und Europa verbindet. Schnell entwickelte sich Wladiwostok zu einem internationalen Handelszentrum. Um die Jahrhundertwende siedelten sich dort viele ausländische Kaufleute an, deren Bauten teilweise noch das Stadtbild prägen (z. B. das 1864 gegründete Handelsunternehmen Kunst und Albers). Im engen Zusammenhang mit der wirtschaftlichen Entwicklung ist die rasante Zunahme der Bevölkerungszahl zu sehen.

Während des Russisch-Japanischen Krieges 1904–1905 wurde Wladiwostok von japanischen Kriegsschiffen belagert. Nach dem Ausbruch der Oktoberrevolution und des Russischen Bürgerkrieges wurde die Stadt durch Ententetruppen besetzt. Im April des Jahres 1918 landeten einzelne japanische und britische Verbände. Ihnen folgte ein 8.000 Soldaten starkes amerikanisches Expeditionskorps, die American Expeditionary Force Siberia. In der Folge entstand hier ein Sammelplatz der Weißen Armee und ihrer Unterstützer (vgl. Fernöstliche Republik und Sibirische Intervention). Nach der Einnahme der Stadt durch die Truppen der Rote Armee unter Leitung von Jeronimas Uborevičius im Jahre 1922, der umfangreiche Kampfhandlungen vorausgingen, wurde Wladiwostok zum Hauptquartier der sowjetischen Pazifikflotte und zum Standort wichtiger militärisch-industrieller Einrichtungen. 1937 wurde die gesamte koreanische Minderheit in der Region um Wladiwostok auf Anordnung Josef Stalins nach Zentralasien deportiert und konnte erst Jahrzehnte später zurückkehren.

Während des Bestehens der Sowjetunion stieg die Einwohnerzahl der Stadt rasant an, von etwas über 100.000 im Jahr 1926 auf fast 650.000 Anfang der 1990er-Jahre. Die Stadt blieb bis 1991 für Ausländer gesperrt. Dennoch trafen sich 1974 der sowjetische Staatschef Leonid Breschnew und der US-amerikanische Präsident Gerald Ford zu Gesprächen über Strategic Arms Limitation Talks (SALT) in Wladiwostok.

Nach dem Zerfall der Sowjetunion herrschte zunächst in ganz Russland eine wirtschaftliche Rezession, von der auch Wladiwostok stark betroffen war. Obwohl die Pazifikmetropole geöffnet wurde, mussten zahlreiche Industriebetriebe schließen und die Einwohnerzahl der Stadt sank. Inzwischen hat sich der Trend jedoch umgekehrt. Unter Ausnutzung seiner guten verkehrsgeographischen Lage knüpft Wladiwostok wieder zunehmend an seine früheren Handels- und Industriefunktionen an und profiliert sich zu einem wichtigen Wirtschaftszentrum im ostasiatischen Raum.

Ein prestigeträchtiges Symbol der wirtschaftlichen „Renaissance“ der Stadt ist die Russki-Brücke, eine Schrägseilbrücke, die mit 1104 Metern die weltweit größte Stützweite aufweist. Sie wurde im Juni 2012 für den Verkehr freigegeben und verbindet die vorgelagerte Insel Russki mit der Stadt.

Von den 1,96 Millionen Bewohnern Primorjes, des südöstlichsten Subjektes der Russischen Föderation, leben laut den vorläufigen Ergebnissen des Zensus von 2010 592.069 in Wladiwostok, neben der Mehrheit der Russen auch Koreaner (Korjo-Saram) und Ukrainer. Seit Anfang der 1990er-Jahre sind außerdem viele Chinesen zugewandert.

Erst seit 1991 dürfen Ausländer die Stadt wieder ohne Sondergenehmigung besuchen; aufgrund der hohen Militärpräsenz und dem Hafen, der als Hauptstützpunkt der Pazifikflotte genutzt wird, war dies bis 1991 aus Gründen der militärischen Sicherheit untersagt.

Wie in vielen russischen Städten stieg auch die Bevölkerungszahl Wladiwostoks bis zum Zerfall der Sowjetunion konstant an. 1989 lebten knapp 634.000 Menschen in der Stadt. In den Folgejahren fiel die Einwohnerzahl relativ stark ab und betrug 2005 nur noch etwa 586.000. In den letzten Jahren setzte aber wieder eine deutliche Erholung ein, 2013 wurde die Marke von über 600.000 Einwohnern wieder erreicht.

Wladiwostok ist als Hauptstadt der Region Primorje Sitz der Gebietsverwaltung. Das Verwaltungsgebäude, auch „Weißes Haus“ genannt, befindet sich am Hafen.

Bürgermeister ist seit Mai 2008 Igor Puschkarjow, der Stadtrat besteht aus 35 Abgeordneten.

In Wladiwostok befinden sich zahlreiche General- und Honorarkonsulate.

Wladiwostok listet folgende dreizehn Partnerstädte auf:

Folgende Staaten unterhalten in Wladiwostok ein Konsulat:
Im Range eines Generalkonsulats

Im Range eines Konsulats
Im Range eines Honorarkonsulats

Wladiwostok ist einer der wichtigsten Pazifikhäfen Russlands. Hauptwirtschaftszweige sind der Hafen, die Fischereiindustrie und der Marinestützpunkt. Die Hauptexportprodukte sind Fisch, Holz und Metalle. Importiert werden hauptsächlich Gebrauchtwagen, Kleidung, Schuhe und andere Konsumgüter.

Wirtschaftlich bedeutsam ist Wladiwostok aufgrund seiner Grenznähe zur Volksrepublik China sowie der Fährverbindung nach Japan.

Die Stadt ist rund 100 Kilometer von der Volksrepublik China entfernt, und so stellen Chinesen einen bedeutenden Faktor im Wirtschaftsleben dar. Einen Wirtschaftsfaktor bildet das Glücksspiel, welches in der Volksrepublik China verboten ist.

Im Oktober 2015 wurde das Kasino "Tigre de Cristal" mit 67 Spieltischen und 498 Spielautomaten eröffnet. Seit 2009 ist jedes Glücksspiel in Russland verboten, ausgenommen wurden 2010 per Erlass vier für den Glücksspieltourismus attraktive Sonderspielzonen: Neben Wladiwostok auch Asow Kaliningrad (Königsberg), die Region Altai in Sibirien und das südrussische Gebiet Rostow. 2014 kamen noch Krim und Sotschi am Schwarzen Meer dazu.

Bauherr (110 Mio. €) und Betreiber ist die Summit Ascent Holding Ltd. mit den Casinobetreibern NagaCorp aus Kambodscha und Melco Crown aus China. Dahinter steckt der Casino-Investor Lawrence Ho Yau Lung.

Der Bahnhof Wladiwostok ist östlicher Endpunkt der Transsibirischen Eisenbahn. Von hier verkehren unter anderem Züge nach Nachodka und über eine 866 Kilometer lange Strecke nach Changchun in China.

40 Kilometer nördlich nahe Artjom liegt der internationale Flughafen Wladiwostok (IATA-Code VVO, ICAO-Code UHWW), der die Stadt mit zahlreichen Flughäfen in Russland, Ost- und Südostasien verbindet. Er war Hauptsitz der Fluggesellschaft Vladivostok Avia und wird seit 2009 ausgebaut; das Inlandsterminal war 2006 erneuert worden.

Wladiwostok ist Endpunkt der Fernstraße M60 von Chabarowsk. Die A188 führt in Richtung Osten nach Nachodka. 2012 wurden zwei neue Brücken fertiggestellt: Die Solotoi-Brücke über das Goldene Horn verbindet die Stadtteile "Stadtmitte" und "Tschurkina". Über die Russki-Brücke ist die Russki-Insel mit dem Auto erreichbar.

Vom Fährhafen Wladiwostok neben dem Bahnhof fährt ein Fährschiff der Reederei DBS Cruise Ferry regelmäßig nach Donghae, Südkorea und von dort weiter nach Sakaiminato auf der japanischen Hauptinsel Honshū.

Der ÖPNV der Stadt wird sowohl von Bus- und Trolleybuslinien als auch von Straßenbahnen (siehe Straßenbahn Wladiwostok) durchgeführt, hinzu kommen wie überall in Russland eine Vielzahl an Kleinbuslinien, sogenannte „Marschrutki“.

Die meisten Fahrzeuge werden aus Japan oder Südkorea importiert. Wegen des in Japan üblichen Linksverkehrs haben diese Fahrzeuge meist Rechtslenkung.

Am Ort befindet sich eine Monitoring-Station des SDCM-Systems.


In Wladiwostok, insbesondere in den Vororten, gibt es eine starke Luft- und Umweltverschmutzung. Die Erstellung des Ecocenter-Umweltreports dauerte zehn Jahre. Er basiert auf 30.000 Proben der Luft, des Wassers, des Schnees und von menschlichem Gewebe, die zwischen 1985 und 1993 genommen wurden. Die Proben zeigen über den Untersuchungszeitraum signifikante Anstiege bei den Schwermetallen, so bei Cadmium, Cobalt, Arsen und Quecksilber, die das Atmungs- und das zentrale Nervensystem angreifen.

Die Umweltverschmutzung hat laut Ecocenter verschiedene Ursachen. In Wladiwostok haben sich 80 industrielle Unternehmen angesiedelt. Das ist zwar nicht viel im Vergleich zu den großen Industriestandorten in Russland, jedoch gibt es in der Umgebung der Stadt sehr umweltbelastende Industrieunternehmen, wie etwa Schiffbau und -reparatur, Kraftwerke, Bergbau und Pelzfarmen. Wladiwostoks Geographie verstärkt den Verschmutzungseffekt: der Wind kann die Luftverschmutzung aus den am stärksten belasteten Teilen der Stadt nicht wegwehen, da sie in einer Art Becken liegen. Außerdem fällt im Winter recht wenig Schnee, der die Schmutzstoffe binden könnte.

Museen in Wladiwostok sind unter anderem das Wladimir-Arsenjew-Museum, die Gemäldegalerie, das Heimatkundemuseum, das Grenztruppenmuseum, das Suchanow-Museum, das Oldtimermuseum und das Museumsschiff Krasny Wympel.

Mit Mumi Troll wurde in Wladiwostok eine russische Rockband gegründet, die rasch zu einer der populärsten wurde.

Höchstklassige Fußballmannschaft der Stadt ist der in der Zweiten Liga spielende Verein "FK Lutsch-Energija Wladiwostok", der seine Heimspiele im 10.500 Zuschauer fassenden Stadion Dinamo austrägt. In der höchsten russischen Basketballliga PBL spielt der "BK Spartak Primorje". Seine Heimspielstätte ist der Sportkomplex Olimpijez, der 1.100 Zuschauern Plätze bietet.

2013 wurde mit "HK Admiral Wladiwostok" ein Eishockeyclub gegründet, der zur Saison 2013/14 in die KHL aufgenommen wurde. Seine Heimspielstätte ist die 2013 fertiggestellte Mehrzwecksporthalle "Fetissow-Arena", die knapp 5.500 Zuschauerplätze zu bieten hat und außer für Eishockeyspiele auch für Konzerte mit eine Kapazität bis zu 7.500 Plätzen genutzt wird.

Die "Russian Open" sind die offenen internationalen Meisterschaften von Russland im Badminton. Sie werden seit 1992 ausgetragen. Seit 2007 gehören die Meisterschaften zum BWF Grand Prix und damit zur Oberklasse der Badmintonwettbewerbe.

Der Großteil der Bewohner Wladiwostoks gehört der Russisch-Orthodoxen Kirche an. Wladiwostok ist Sitz einer orthodoxen Eparchie.
Das älteste Kirchengebäude von Wladiwostok ist die im Jahre 1907 erbaute evangelisch-lutherische Pauluskirche. Sie steht an der Uliza Puschkinskaja 14 und wurde vom Architekten Georg Junghändel erbaut. Sie gehört zu den bedeutendsten Architekturdenkmälern der Stadt – mit einer bizarren mehr als hundertjährigen Geschichte.
Es gibt in der Stadt auch einige Kirchen anderer christlicher Konfessionen, ein buddhistisches Zentrum sowie eine Synagoge, deren Gemeinde etwa 300 aktive Mitglieder hat.

Bedingt durch die antireligiöse Politik der Sowjetunion ist der Anteil der aktiv praktizierenden Gläubigen aller Religionen relativ niedrig, ebenso gibt es eine große Zahl an Konfessionslosen.





</doc>
<doc id="13768" url="https://de.wikipedia.org/wiki?curid=13768" title="Ulaanbaatar">
Ulaanbaatar

Ulaanbaatar (, in mongolischer Schrift ; übersetzt „Roter Held“; verbreitete Schreibweise nach : Ulan-Bator oder Ulan Bator; gegründet als Örgöö, in Europa früher auch Urga genannt) ist die Hauptstadt der Mongolei.

Knapp die Hälfte der mongolischen Gesamtbevölkerung, rund 1.462.973 Menschen, lebt in der mongolischen Hauptstadt.

Die Hauptstadt ist das politische, wirtschaftliche und kulturelle Zentrum der Mongolei. Die Stadt bildet eine eigenständige Verwaltungseinheit und gehört keinem Aimag (Provinz) an. Das Verwaltungsgebiet von Ulaanbaatar stellt kein zusammenhängendes Stadtgebiet dar, sondern ist mit seiner recht geringen Bevölkerungsdichte und der außerhalb der eigentlichen Stadt dominierenden ländlichen Siedlungsstruktur eher mit einer kleinen Provinz vergleichbar. Zudem zählen noch die Exklaven "Bagakhangai" und "Baganuur" zum Stadtgebiet.

Touristisch bedeutsam sind beispielsweise das Naturkundemuseum, das Süchbaatar-Denkmal auf dem gleichnamigen Platz, der Winterpalast des Bogd Khan und das Dsaisan-Denkmal, das an den Zweiten Weltkrieg erinnert. Es liegt auf einem Hügel südlich der Stadt, von dem aus man einen schönen Blick hat. Die öffentliche Bücherei stellt eine einzigartige Sammlung von Sanskrit-Manuskripten aus dem 11. Jahrhundert aus. Auch die buddhistischen Tempelmuseen und das Gandan-Kloster werden gern besucht.

Ulaanbaatar liegt in 1350 Meter Höhe am Fluss Tuul und am Fuß des 2256 Meter hohen Berges Bogd Khan Uul. Das Verwaltungsgebiet von Ulaanbaatar hat eine Fläche von 4704,4 Quadratkilometern und ist damit etwa doppelt so groß wie das Saarland.

Gegründet wurde die Stadt 1639 unter dem Namen "Örgöö" (deshalb in Europa bis ins 20. Jahrhundert hinein auch als "Urga" bekannt) als Sitz des Oberhaupts des Lamaismus in der Mongolei, des Jebtsundamba Khutukhtu (auch Bogd Gegen genannt). In den ersten anderthalb Jahrhunderten wechselte sie über 25-mal ihren Standort, seit 1778 befindet sie sich an ihrer heutigen Stelle. Außerdem wechselte sie mehrmals den Namen: Ab 1706 wurde sie "Ich-Chüree" () genannt, von 1911 bis 1924 hieß sie "Niislel-Chüree", ab 1924 schließlich "Ulaanbaatar", das in der russischen Schreibweise "Ulan-Bator" () in der Welt bekannt wurde. Der Grund für den Unterschied liegt darin, dass die kyrillische Schrift in der Mongolei erst 17 Jahre später eingeführt wurde und sich zu dem Zeitpunkt in Russland bereits eine an der russischen Aussprache orientierte phonetische Schreibweise etabliert hatte.

Infolge der Unterzeichnung des Vertrages von Aigun 1858 wuchs die wirtschaftliche Bedeutung der Stadt ab den 1860er-Jahren durch den Handel zwischen Russland und China, außerdem war sie der Sitz des chinesischen Amban. 1911 erklärte der 8. Bogd Gegen die Äußere Mongolei für unabhängig. Im gleichen Jahr erfolgte eine Umbenennung der Stadt in "Niislel Chüree". China erkannte die Sezession nicht an, räumte der Äußeren Mongolei 1915 mit dem Vertrag von Kjachta jedoch gewisse Autonomierechte ein.

1920/21 besetzten Truppen der russischen Weißen Armee das Land und riefen am 13. März 1921 in "Niislel Chüree" eine unabhängige Monarchie aus. Kurz darauf eroberte die sowjetische Rote Armee die Stadt und etablierte am 3. Juli 1921 eine Marionettenregierung. Am 13. Juli 1924 wurde die Äußere Mongolei als erstes sowjetisches Satellitenregime dieser Art zur „Volksrepublik“ erklärt. In diesem Zusammenhang erhielt die Stadt den Namen "Ulaanbaatar" (russisch: Ulan Bator; zu deutsch: „Roter Held“) und wurde am 26. November 1924 zur Hauptstadt der Mongolischen Volksrepublik erklärt.

Mit dem Zusammenbruch der Sowjetunion war zwangsläufig der Zusammenbruch ihres Marionettenregimes in Ulan Bator verbunden. Nach Massendemonstrationen im Winter 1989/90 verlor die kommunistische Staatspartei mehr und mehr die Kontrolle. Im März 1990 trat das gesamte Politbüro zurück, im Mai erfolgte die Aufhebung des Einparteiensystems, womit der Demokratisierungsprozess und die Wendung zur Marktwirtschaft begannen. Am 29. Juli 1990 fanden die ersten freien Wahlen in einem Mehrparteiensystem statt. Am 12. Februar 1992 besiegelte das Parlament mit der Annahme einer neuen Verfassung das Ende des kommunistischen Systems. Zugleich verzichtete die verfassungsgebende Gewalt auf die Bezeichnung "Volksrepublik". Damit wurde Ulaanbaatar die Hauptstadt des neuen Staates Mongolei.

Ulaanbaatar ist eine Stadt zwischen Moderne und Tradition. Zentrum der Stadt ist der "Süchbaatar-Platz". Die markantesten Bauten auf diesem großräumigen Platz sind das Parlamentsgebäude, das Rathaus der Stadt, das Haus der Kultur, die Staatsoper, die Mongolische Börse sowie einige moderne Hochhäuser mit Hotels, Restaurants und Geschäften. Vor dem Eingang des Parlamentsgebäudes befinden sich große Skulpturen: in der Mitte Dschingis Khan, rechts und links sein Sohn und Nachfolger Ögedei Khan und sein Enkel Kublai Khan, einst Kaiser von China. 

In der Stadt gibt es verschiedene Zeugnisse des Lamaismus, wie das Gandan-Kloster, den Winterpalast des Bogd Khan oder das Tschoidschin-Lama-Tempel-Museum. Das Gandan-Kloster liegt westlich des Stadtzentrums. Es ist das größte Kloster des Landes und eines der wenigen original erhaltenen, die den stalinistischen Terror in der Mongolei überstanden haben. Dennoch zerstörten auch hier sowjetische Truppen mehrere Gebäude und ließen unter anderem die 26 Meter hohe goldene Statue der Göttin Janraisig (Sanskrit: Avalokiteshvara) vor dem Kloster demontierten und einschmelzen. Durch Spenden der buddhistischen Gemeinde in Höhe von umgerechnet 5 Millionen US-Dollar konnte nach 1990 eine neue vergoldete Janraisig errichtet werden. Für den Dalai Lama, das eigentliche Oberhaupt des Klosters, wurde ein Thronsessel neu erbaut.

In der Stadt gibt es zahlreiche Museen, von denen das "Naturkundemuseum" das größte ist, mit einer paläontologischen Abteilung mit mehreren Saurierskeletten. Erwähnenswert ist auch das "Nationale Historische Museum". Es ist bekannt wegen der völkerkundlich-historischen Ausstellung, die einen Einblick in das Nomadentum und die Geschichte der Mongolen gibt. Das "G.-Dsanabadsar-Kunstmuseum" zeigt die alte Kunst der Mongolei. Im Mittelpunkt stehen die buddhistischen Kunstwerke von Dsanabadsar. Skulpturen, Bilder, Tankas geben einen Überblick über den Buddhismus der Mongolei.

Das "Museum der Schönen Künste" zeigt klassische und moderne Malerei, sowie Kunsthandwerk der Mongolei. Das "Natsagdordsch-Museum" wurde zu Ehren des bedeutendsten Schriftstellers der Mongolei Daschdordschiin Natsagdordsch errichtet. In den 1920er Jahren war Natsagdordsch zum Studium in Deutschland. Sein früher Tod 1937 mit nur 31 Jahren ist von vielen Gerüchten umgeben. Das Museum befindet sich unmittelbar neben dem Tschoidschin-Lama-Tempel.

Das "Ulaanbaatarer Stadtmuseum" gibt einen Einblick in die Geschichte der Stadt Ulaanbaatar. Das "Kamel-Museum" beherbergt 250 Ausstellungsstücke rund um das Baktrische Kamel. Das "Spielzeugmuseum" zeigt beliebte Spiele und Spielsachen aus der Mongolei. Das Museum ist im Gebäude des „Mongolian National Centre for Children“ in der obersten Etage untergebracht. 

Das "Jagdtrophäenmuseum" gibt einen kleinen Einblick in die Tierwelt der Mongolei. Einen Besuch wert ist auch das "Eisenbahnmuseum". Das "Mongolische Theatermuseum" ist berühmt für seine Marionettenausstellung. Das Museum befindet sich auf der dritten Etage des Kulturpalastes. Das "Museum der Künste und Kreativität der Kinder" zeigt die Kreativität der Kinder und Jugendlichen der Mongolei. Das "Mongolische Militärmuseum" wurde zum 50. Jahrestag des Endes des Zweiten Weltkrieges eingeweiht. Es sind Exponate aus allen Zeiten der mongolischen Militärgeschichte zu sehen.

Das "Museum der politisch Verfolgten" wurde von einer Tochter von Peldschidiin Genden, einem Premierminister, der in den 1930er Jahren in der Sowjetunion exekutiert wurde, eingerichtet. Es erinnert an den stalinistischen Terror in den 1930er Jahren, dessen Zahl der Opfer mit 35.000 bis 36.000 Toten beziffert wird. Insgesamt geht die Forschung von mehr als 100.000 Verfolgten aus, sehr wahrscheinlich sind die Opferzahlen deutlich höher. 

In Ulaanbaatar sind mehrere Theater und Theaterensembles zu Hause, wie zum Beispiel das staatliche Schauspielhaus, die Staatsoper, das Ballett und das Volkslied- und Tanzensemble. 



Weitere bedeutende Bildungseinrichtungen sind:

Im Allgemeinen ist das Gesundheitswesen der Mongolei staatlich geregelt. Jeder offiziell erwerbstätige Mongole zahlt in eine staatliche Pflichtversicherung ein, die Medizinversorgung ist für alle Mongolen kostenfrei. Das Gesundheitswesen ist im Vergleich zum Westen mit einem geringen Budget ausgestattet und wird in vielen Bereichen aus dem Ausland unterstützt. Die Tibetische Medizin ist in einigen Regionen verbreitet und kann an der buddhistischen Universität, auch von Ausländern, als Lehrfach belegt werden. 

Mit dem Grand Med Hospital, das mehr als eine halbe Million Patienten pro Jahr versorgt, befindet sich in Ulaanbaatar eines der führenden Krankenhäuser der Mongolei. Weitere große Krankenhäuser in Ulaanbaatar sind:

Des Weiteren gibt es noch viele kleine Spitäler und einige private Krankenhäuser.

Das veterinärmedizinische Institut hat sich seit den 1950er Jahren zu einem wichtigen Element der nomadischen Viehwirtschaft entwickelt. Es untersucht aufgetretene Krankheiten und entwickelt ständig neue Impfstoffe, die in landesweiten Kampagnen an die Hirten verteilt werden.

Ulaanbaatar wird aus nahegelegenen Revieren mit Kohle versorgt und bildet auch das industrielle Zentrum des Landes. Am Stadtrand gibt es u. a. Elektrizitätswerke.

In der Stadt bündelt sich das mongolische Straßen- und Flugnetz, auch die Transmongolische Eisenbahn führt durch Ulaanbaatar. Sie verbindet die Stadt mit der Transsibirischen Eisenbahn und dem chinesischen Bahnnetz. Es gibt direkte Züge nach Moskau und nach Peking. Bei Bujant-Uchaa, etwas außerhalb von Ulaanbaatar, befindet sich der Chinggis Khaan International Airport. Die Fluggesellschaft MIAT betreibt regelmäßige Liniendienste von Ulaanbaatar nach Berlin, Moskau, Peking, Hongkong, Tokio und Seoul. 

Innerhalb der Stadt verkehren der Oberleitungsbus Ulaanbaatar sowie zahlreiche Buslinien. Das städtische Straßennetz wird kontinuierlich verbessert, bedarf jedoch noch großer Investitionen.

Mit einer Jahresdurchschnittstemperatur von −2 °C gilt Ulaanbaatar als die kälteste Hauptstadt der Welt. Das liegt vor allem an den extrem kalten Wintermonaten mit Temperaturen von durchschnittlich −17 bis −21 °C (tagsüber −10 bis −15 °C, nachts um −25 °C). Im Sommer ist es hingegen mit Durchschnittswerten von 15 bis 17 °C recht warm (tagsüber rund 20 °C, nachts 10 °C), im Hochsommer liegen die Höchsttemperaturen bei bis zu 30 °C.

Nach wie vor ziehen während der Sommermonate Familien als traditionelle Nomaden aufs Land und leben nur in der kalten Jahreszeit in der Stadt.

Ulaanbaatar ist in neun "Düüreg" genannte Distrikte unterteilt, und diese wiederum in Unterdistrikte – "Choroos". 

Die Stadt wird von einem Rat der Volksvertreter mit 40 Mitgliedern regiert, welche jeweils für eine Amtszeit von vier Jahren gewählt sind.




</doc>

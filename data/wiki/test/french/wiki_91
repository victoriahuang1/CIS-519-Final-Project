<doc id="17689" url="https://fr.wikipedia.org/wiki?curid=17689" title="Jeanne Grey">
Jeanne Grey

Jeanne Grey ou Lady Jane Grey en anglais (octobre 1537 — 12 février 1554) est brièvement reine régnante (ne pas confondre avec reine consort) du royaume d'Angleterre en juillet 1553, entre la mort de son cousin Édouard VI et sa déposition au profit de Marie. Son court règne lui a valu le surnom de ', « la reine de neuf jours ».

Arrière-petite-fille d'Henri VII par sa plus jeune fille Marie, Jeanne Grey est choisie comme héritière par Édouard VI, qui exclut de la succession ses demi-sœurs Marie et Élisabeth afin d'éviter que la catholique Marie ne monte sur le trône. Cependant, la jeune reine est rapidement évincée par sa cousine Marie, qui la fait enfermer à la Tour de Londres, tout en semblant vouloir l'épargner. Elle la fait cependant exécuter en raison de la participation supposée de Jeanne à un complot contre la reine et de la révolte menée par son père, le duc de Suffolk.

Lady Jeanne, en dépit de son très jeune âge au moment de sa mort, avait déjà été repérée par ses contemporains comme « une dame de bonne réputation ». Elle est d'ailleurs décrite par l’historienne Alison Weir comme « un des esprits féminins les plus érudits du ».

Lady Jeanne naît en octobre 1537 à Bradgate près de Leicester. Elle est la fille aînée d’Henry Grey, marquis de Dorset, et de son épouse Frances, elle-même fille du duc de Suffolk Charles Brandon. Par leur mère, Jeanne et ses deux sœurs Catherine et Marie sont les petites-nièces du roi d'Angleterre Henri VIII. Leur père est également un descendant d’Élisabeth Woodville, l’épouse d’Édouard IV.

On lui enseigne très tôt les langues anciennes et contemporaines, ainsi que les vertus de la religion protestante.

Son enfance est rendue difficile par une mère exigeante et volontiers abusive. Elle tente en effet d’endurcir Jeanne, dont le comportement réservé et la soumission l’irritent, en la maltraitant. Privée d’amour maternel, sa fille se consacre aux livres. Cependant, elle ne se croit pas capable de satisfaire ses parents. Jeanne confie ainsi à Roger Ascham, le tuteur de sa cousine Élisabeth Tudor :

Au printemps 1547, alors qu'elle n'a pas encore dix ans, ses parents en font une pupille de la reine Catherine Parr, la dernière épouse du roi Henri VIII. L’affection de Catherine aide Jeanne à s'épanouir enfin. Elle est présentée dans le même temps à ses cousins royaux, Édouard, Marie et Élisabeth.

Lorsque Catherine Parr meurt en couches en 1548, son mari Thomas Seymour, par ailleurs oncle du roi Édouard VI, propose que Jeanne épouse le souverain qui est son cousin. Cependant, le frère de Thomas, Edward, Lord Protecteur et duc de Somerset, a d'ores et déjà arrangé un mariage entre Édouard et Élisabeth de France, la fille d’Henri II. Aucune de ces deux unions n’est toutefois conclue à cause de la mauvaise santé du nouveau roi.

Jeanne souhaite quant à elle se fiancer à Edward Seymour (1539–1621), comte de Hertford, le fils aîné du Lord Protecteur. Mais, dans le même temps, sa mère est en pourparlers avec le duc de Northumberland John Dudley, désireux de marier son propre fils Guilford. Lady Jeanne s’alarme de l'union qui lui est proposée car elle exècre les Dudley. Lady Frances parvient toutefois à « persuader » sa fille (qu'elle n'hésite pas à frapper pour l'occasion) et le mariage est finalement célébré le 25 mai 1553. Le même jour, sa sœur Catherine épouse en premières noces le comte de Pembroke , avant de convoler sept ans plus tard avec l'ancien prétendant de Jeanne, Edward Seymour lui-même.

Jeanne Grey est en droit de revendiquer le trône anglais par sa mère Frances, descendante d'Henri VII puisqu'elle était la fille de Marie Tudor. Le testament du roi Édouard VI stipule d'ailleurs que Jeanne doit lui succéder. Conformément à la règle de primogéniture, Jeanne ne peut cependant monter sur le trône, sauf si les héritières directes d’Henri VIII, Marie et Élisabeth, s’avèrent incapables d'accéder à cette position.

La fondation de l’Église anglicane par Henri VIII a engendré une élite non catholique, enrichie par la dissolution des monastères. Quand la nouvelle leur parvient que le roi Édouard est mourant, les personnalités de cette nouvelle aristocratie, le duc de Northumberland John Dudley (beau-père de Jeanne Grey) en tête, conspirent contre Marie qui préconise un retour au catholicisme, le rétablissement de l’autorité de l’Église catholique romaine en Angleterre étant seul à même de priver la noblesse anglicane de sa toute récente richesse. C'est dans le but de déjouer ce plan qu'Édouard, sur son lit de mort, privilégie Jeanne afin de lui succéder tout en excluant Marie, l'héritière légitime.

Les bases juridiques sur lesquelles reposent les prétentions de Jeanne, bien décidée à honorer les dernières volontés du défunt souverain, sont toutefois très fragiles. Édouard a en effet transgressé la loi anglaise en déshéritant Marie, et le roi n’avait pas encore seize ans à l’époque de sa mort (il était né la même année que Jeanne, sa seule parente anglicane).

Édouard VI meurt le et le duc de Northumberland proclame Jeanne reine d'Angleterre quatre jours plus tard. Elle élit domicile à la Tour de Londres, où les monarques anglais séjournaient habituellement entre leur accession au pouvoir et leur couronnement. Elle refuse toutefois de conférer le titre de « roi » à son époux Guilford Dudley, proposant de le créer duc de Clarence à la place.

De son côté, le duc de Northumberland doit faire face à plusieurs défis afin de consolider le pouvoir acquis par les anglicans. Il souhaite notamment arrêter et mettre au secret la princesse Marie avant qu’elle ne récolte suffisamment d’appuis pour la cause qu'elle défend. Informée des intentions du duc, l'intéressée s’enferme derrière les murs du château de Framlingham, dans le Suffolk. En seulement neuf jours, elle réussit à mobiliser assez de membres de la noblesse pour retourner à Londres le 19 juillet, à la tête d'une procession triomphale. Le Parlement est alors contraint de reconnaître Marie en tant que reine légitime. La nouvelle souveraine commence par faire emprisonner Jeanne Grey et son époux dans les geôles de la tour de Londres sous l'inculpation de haute trahison, et elle commande l'exécution du duc de Northumberland, le 21 août 1553.

L’ambassadeur au Saint-Empire romain germanique est chargé d'annoncer à Charles Quint que Marie a l’intention d’épargner Jeanne ainsi que le tout nouveau duc de Clarence, son conjoint. Cependant, la rébellion anglicane dirigée par Thomas Wyatt en janvier 1554, bien qu’ils n’y soient pas impliqués directement, achève de décider de leur sort. La révolte de Wyatt est en effet la conséquence directe du mariage entre Marie et le futur roi catholique d’Espagne, Philippe II.

Parmi les anglicans exigeant le retour de Jeanne Grey sur le trône figure le propre père de la jeune femme, le duc de Suffolk. Les conseillers de Marie la poussent alors à exécuter la « reine de neuf jours » afin d'étouffer dans l’œuf ce soulèvement politique. Cinq jours après l’arrestation de sir Thomas Wyatt, la reine signe les ordres d’exécution de lady Jeanne Grey et lord Guilford Dudley.

Au matin du 12 février 1554, les autorités compétentes mènent Guilford Dudley à l’échafaud afin qu'il soit décapité en public. On renvoie ensuite son corps dans l'enceinte de la Tour de Londres, pour qu'il soit visible depuis l'endroit où Jeanne est retenue captive. Sur ordre de la reine, on entraîne ensuite la jeune veuve jusqu'à Tower Green, une petite étendue de gazon dans l'enceinte de la tour, afin qu'elle y soit exécutée à son tour, à l'abri des regards du plus grand nombre (une telle procédure ne s’appliquant en principe qu’aux personnalités de sang royal). En montant sur l’échafaud, elle s’adresse aux quelques personnes autorisées encore présentes à ses côtés :

Elle récite ensuite le psaume 50 "Miserere mei Deus" (« "Ô Dieu, aie pitié de moi" ») en anglais et donne ses gants ainsi que son mouchoir à une dame d’honneur. John Feckenham, chapelain catholique, n'ayant pas réussi à convertir Jeanne Grey, reste malgré tout près d'elle. Elle est mise à genoux et avant de se bander elle-même les yeux, elle pardonne d'avance à son bourreau tout en le suppliant de la . Ayant résolu de mourir avec dignité mais s'avérant incapable de se diriger droit vers le billot à cause du bandeau, elle s'exclame : ().

Après qu’une main inconnue l'y a menée (sans doute sir Thomas Brydges, sous-lieutenant de la Tour de Londres), elle cite les dernières paroles du Christ telles que les rapporte l"'Évangile selon Luc" : Le bourreau abat ensuite sa hache sur le cou de Jeanne, séparant son corps de sa tête, puis il saisit cette dernière par les cheveux et s'écrie : 

À ce moment-là, la reine a déjà emprisonné le père de la suppliciée, le duc de Suffolk, pour sa participation à la révolte de Wyatt. Il est exécuté une semaine plus tard, le 19 février.

Lady Jeanne et son mari reposent dans la chapelle royale de Saint-Pierre-aux-Liens ("St Peter-ad-Vincula") dans l'enceinte de la Tour de Londres.

L'ascension et la chute de lady Jeanne Grey sont racontées, sur fond de sa relation amoureuse avec son mari lord Guilford Dudley, dans le film britannique de 1986 "Lady Jane", réalisé par sir Trevor Nunn, avec Helena Bonham Carter dans le rôle de lady Jane Grey.

La femme de lettres, journaliste et féministe italienne Clelia Romano Pellicano utilisait le pseudonyme « Jane Grey ».




</doc>
<doc id="17690" url="https://fr.wikipedia.org/wiki?curid=17690" title="Le Monde diplomatique">
Le Monde diplomatique

Le Monde diplomatique est un journal mensuel français d’information et d’opinion, fondé en par Hubert Beuve-Méry comme supplément au quotidien "Le Monde".

Même si "Le Monde SA" détient 51 % du capital du "Monde diplomatique", ce dernier disposant malgré tout d'une rédaction indépendante, est édité par une société distincte du "Monde". À ses débuts, il est destiné aux « cercles diplomatiques et aux grandes organisations internationales », créé sur l’initiative de François Honti, journaliste et ancien consul de Hongrie à Genève dans l'immédiat après-guerre ; le journal a pris une tendance altermondialiste à partir de 1973, sous la direction de Claude Julien.

C’est aujourd’hui une filiale indépendante du Groupe Le Monde. En 2007, l’édition française tire à une moyenne de exemplaires, tandis que le tirage de ses quarante éditions internationales en vingt-six langues s’élève à 2,4 millions d’exemplaires. À ce titre, "Le Monde diplomatique" est le mensuel français dont les articles sont les plus diffusés dans le monde.

Le journal est couramment appelé « Le Diplo ».

Autrefois simple supplément du quotidien, le « Diplo » a acquis progressivement son autonomie. À la suite de l'accession à la direction du "Monde" de Jean-Marie Colombani, il en devient en 1996 une filiale à hauteur de 51 %.

Le reste du capital est détenu par l'association des Amis du Monde diplomatique représentant les lecteurs (25 %) et par l'équipe rédactionnelle du journal (24 %) regroupée au sein de l'Association Günter-Holzmann, du nom d'un généreux donateur qui permit le lancement de cette opération. Ensemble, ces parts sont supérieures à la minorité de blocage (33,34 %) et confèrent au journal une relative indépendance politique vis-à-vis du groupe "Le Monde". Par exemple, le directeur de la publication n'est éligible que sur proposition du personnel du journal.

Bien que l'indépendance économique du "Monde diplomatique" vis-à-vis du groupe "Le Monde" soit limitée, la ligne éditoriale du journal est devenue largement autonome de celle du quotidien depuis l'arrivée de Claude Julien à la direction de la rédaction en 1973. Par ailleurs, le « "Diplo" » affirme préserver sa ligne éditoriale vis-à-vis des pressions des annonceurs en limitant la part de ses revenus générée par la publicité. De fait, la part de revenu provenant de la publicité est limitée à 5 %, chiffre largement inférieur à la moyenne de la presse française, qui tire entre 40 et 50 % de son chiffre d’affaires de la publicité.

Une équipe de neuf journalistes permanents (en 2006) assure la rédaction d'une petite partie des articles, la majorité étant écrite par des journalistes indépendants ou des intellectuels (universitaires, écrivains) d'origines et de nationalités variées.

À partir de 1989, l'impression sur les nouvelles rotatives du "Monde" à Ivry et le passage au format berlinois ont permis d'introduire la couleur. À l'initiative de Claude Julien, le mensuel a dès lors illustré ses articles de reproductions d'œuvres d'art contemporaines, longtemps choisies par Solange Brand. Le , "Le Monde diplomatique" est le premier journal français à avoir une édition en ligne. Hébergé par le Cyberport de l’INA, il propose alors les articles de l’année passée. Depuis juillet 2002, le siège du journal se trouve au 1, avenue Stéphen-Pichon dans le de Paris.

Le « Diplo » est présent au capital des éditions Cybermonde (33 % de l'édition en Espagne) et "Le Monde diplomatique éditions arabes". Une revue thématique bimestrielle appelée "Manière de voir" compile des articles parus dans le "Monde diplomatique" et des articles inédits écrits à l'occasion de la publication de cette revue.

À rythme triennal, le « Diplo » publie également trois atlas, traitant respectivement de sujets d'ordre environnemental, géopolitique et historique.

Le "Monde diplomatique" a publié, fin octobre 2010, une édition hors série "le Monde diplomatique en bande dessinée", sélectionnée pour le Prix France Info de la Bande dessinée d’actualité et de reportage. 

Le journal a la particularité d'avoir numérisé sur un unique DVD-rom accessible à l'achat l'ensemble des articles publiés : dans son édition en langue française (depuis sa fondation en 1954), allemande ("idem" depuis 1995), anglaise ("idem" depuis 1996), espagnole ("idem" depuis 1997), italienne ("idem" depuis 1997) et portugaise ("idem" depuis 1999). Il a pour principe professionnel de recouper ses informations.

Le directoire est composé de Serge Halimi (directeur de la publication), Vincent Caron, Bruno Lombard, Pierre Rimbert & Anne-Cécile Robert.

Parmi les anciens membres du journal, on peut citer Ignacio Ramonet, Philippe Rekacewicz, Bernard Cassen, Alain Gresh.

En février 2009, Le "Monde diplomatique" est publié en 26 langues, dont l'espéranto, à travers 72 éditions internationales, dont 46 imprimées (avec un tirage total de 2,4 millions d’exemplaires) et 26 électroniques, qui couvrent l'essentiel de l'Europe, de l'Amérique du Sud et du monde arabe avec, entre autres, une édition palestinienne.
En février 2013, il a 47 éditions internationales en 28 langues.

Dès les années 1975, deux éditions sont apparues au Portugal et en Grèce, suivies dans les années 1980 par une édition en espagnol et une édition en arabe. À la fin des années 1990, le mouvement se développe : allemand et italien depuis 1995, édition Cône sud en Amérique du Sud, puis grec. Le mouvement s'amplifie avec le russe, le polonais, l'hindi, le coréen, etc. Aux versions imprimées s'ajoutent de nouvelles éditions électroniques (farsi, japonais, catalan, espéranto, etc.). Les éditions étrangères prennent différentes formes : mensuel, supplément mensuel ou hebdomadaire d'un autre titre de presse, trimestriel, etc. À la simple traduction des articles de l'édition française s'ajoutent jusqu'à 20 % d'articles rédigées par l'édition locale. L'édition anglophone est née en 1999 d'un partenariat avec "The Guardian Weekly". Les éditions en arabe sont maintenant réalisées à Paris par une filiale partenaire de "A Concept Mahfoum".

Le "Monde diplomatique" traite d'une grande variété de sujets :







La ligne éditoriale du journal, en raison de son caractère nettement engagé en faveur d'une gauche de rupture avec le capitalisme, lui vaut de virulentes critiques. 
Certains détracteurs lui reprochent, par exemple, des positions qualifiées de « propalestiniennes » et d'« antisémites », ou encore des articles jugés favorables à Fidel Castro ou Hugo Chávez. Mais à l'inverse, l'américain Edward Herman qualifie "Le Monde diplomatique" de « média dissident » et le considère comme « probablement le meilleur journal au monde ».

En avril 2016, "Le Figaro" désigne le "Monde diplomatique" comme la matrice idéologique du mouvement de contestation sociale et politique Nuit debout.

Jadis tenant d'une ligne éditoriale tiers-mondiste, caractérisée dans les années 1960 par l'intérêt porté aux nouveaux États nés de la décolonisation, le journal se veut critique de tout impérialisme, entre autres américain. Depuis la fin de la guerre froide, le journal s'est rapproché du mouvement alter-mondialisation, se faisant l'un des hérauts de la critique de la mondialisation « néo-libérale ». Il a ainsi soutenu la lutte des zapatistes, mouvement de guérilla mexicaine s'étant soulevé le janvier 1994, le jour même de l'entrée en vigueur de l'ALENA (Accord de Libre-Échange d'Amérique du Nord), entre autres en publiant des articles du sous-commandant Marcos.

L'éditorial célèbre d'Ignacio Ramonet, publié en 1995, a ainsi mis en circulation le terme de « pensée unique » pour critiquer le dogme néolibéral. 
Ainsi, Ignacio Ramonet pouvait écrire :

La rédaction a pris une part active dans l'émergence, en France, du mouvement altermondialiste. Ainsi, c'est à la suite de la parution d'un éditorial écrit par Ignacio Ramonet en décembre 1997 que fut créée l'association ATTAC. Le journal a relayé des campagnes d'ATTAC (par exemple contre les paradis fiscaux et le secret bancaire). Il est également à l'initiative et membre fondateur de l'Observatoire français des médias, créé à la suite du Forum social mondial de Porto Alegre en 2002. Adepte des grandes enquêtes, le journal s'est montré très critique envers les nouvelles stratégies boursières sacrifiant l'emploi à la rentabilité et le Théorème de Schmidt voulant que l'emploi dépende de la rentabilité des entreprises.

"Le Monde diplomatique" entretient un rapport ambigu avec la philosophie de Toni Negri et Michael Hardt et leur concept « d'Empire » néolibéral qui englobe non seulement les États-Unis ou la Triade (États-Unis, Union européenne, Japon) mais aussi l'ensemble des institutions internationales (FMI, Banque mondiale, OMC, etc.). Si le philosophe de la Gauche radicale italienne a pu présenter sa pensée dans les colonnes du Diplo et a été invité à prendre la parole à l'occasion de la célébration des 50 ans du journal, il a été critiqué par André Bellon au nom de la défense des États comme « expression de la souveraineté populaire». "Le Monde diplomatique" publie aussi régulièrement des articles critiquant l'oligarchie française ou l'« hyperbourgeoisie » mondiale. Certains articles dénoncent un (supposé) manque d'empressement de la COB (Commission des opérations de bourse) à signaler à la justice les opérations douteuses, ou bien les façons multiples qu'ont certains milliardaires (dont François Pinault) d'éviter de payer l'impôt sur le revenu.

C'est dans son édition de février 2007, dans un article de Frédéric Lordon, que l'idée d'un impôt innovant appelé SLAM est née.

Par ailleurs la rédaction du Diplo et l'association des lecteurs du journal ont activement participé à la création des Rencontres déconnomiques d'Aix-en-Provence qui rassemblent depuis 2012, annuellement et dans un esprit clairement satirique, les économistes s'opposant au néo-libéralisme. Un article de Renaud Lambert, publié dans "Le Monde diplomatique" de mars 2012 dénonçant les liens entre économistes néo-libéraux et groupes bancaires fut d'ailleurs un des éléments déclencheurs de la création des "Rencontres déconnomiques".

La ligne anti-impérialisme américain se développe tout naturellement en Amérique du Sud, champ privilégié de l'influence américaine. Le journal défend Castro et Chavez, quitte à prêter le flanc à une critique l'accusant de complaisance excessive. Le journal désapprouve les violations des droits de l'homme à Cuba, mais il les relativise (par rapport à d'autres pays), les explique et les justifie par les pressions américaines et le « blocus » américain sur Cuba.

Philippe Val, rédacteur en chef de "Charlie Hebdo", accuse la rédaction du "Monde diplomatique", et Ignacio Ramonet en particulier, d'une amitié avec les dirigeants Fidel Castro et Hugo Chávez. Bernard-Henri Lévy dénonce lui aussi une position qui serait, selon lui, modérée vis-à-vis du régime communiste de Fidel Castro à Cuba.

Au sujet de ces accusations, Ignacio Ramonet dénonce un « anticastrisme primaire » et répond en avril 2002 : 

Il est par ailleurs arrivé à plusieurs reprises au "Monde diplomatique" de critiquer la politique cubaine.

Le journal critique la « pression sécuritaire », notamment celle qui pèse sur les « jeunes issus de l'immigration » en France, et plus généralement dans le monde celle dont les attentats du 11 septembre 2001 a été l'occasion au motif de l'antiterrorisme.

Le mensuel se veut engagé dans la lutte contre le révisionnisme historique, notamment pour rappeler les réalités du génocide des Juifs européens, les massacres ou déshumanisations liés au colonialisme (massacre du 17 octobre 1961), critiquer les zoos humains ou la façon dont l'État français gère les archives. Le journal a aussi donné une tribune à l'historienne communiste Annie Lacroix-Riz qui critique l'interprétation de l'Holodomor.

En septembre 2014, le "Monde diplomatique" a édité un « anti-manuel d'histoire ». Ce manuel vise explicitement à réhabiliter le rôle des prises de conscience collectives des peuples dans la fabrication de leur histoire. Il critique dès lors l'importance donnée en ce domaine aux grands personnages par les médias spécialisés.

Au sujet du conflit israélo-palestinien, le "Monde diplomatique" a adopté une ligne très critique à l'égard de la politique de l'État d'Israël. En particulier, le journal reproche à ce dernier de ne pas avoir respecté les différentes résolutions du Conseil de sécurité et de l'Assemblée générale de l'ONU depuis 1947 ainsi que sa politique de peuplement des territoires palestiniens occupés.

Il ouvre régulièrement ses colonnes à des personnalités pro-palestiniennes, comme le journaliste Michel Warschawski, la cinéaste Simone Bitton, le médecin et ancien président de Médecins sans frontières Rony Brauman, le journaliste Uri Avnery et l'historien post-sioniste Ilan Pappé. Le "Monde diplomatique" donne également la parole à plusieurs tendances de la gauche israélienne : Amram Mitzna ou Yossi Beilin du Parti travailliste israélien mais aussi à des intellectuels palestiniens : Edward Saïd, Mahmoud Darwich ou Fayçal Husseini.

Une étude de Samuel Ghiles-Meilhac, parue en 2006, retrace l'histoire du "Monde diplomatique" et de ses prises de position, en particulier à l'égard du Moyen-Orient. Samuel Ghiles-Meilhac rappelle qu’en 1954 le « Journal des cercles consulaires et diplomatiques », mensuel au service des diplomates, était favorable à Israël, de même que le Ministère des Affaires étrangères. Mais tout comme le Ministère des Affaires étrangères après la guerre des Six Jours, le journal a changé après 1967. Sous la direction de Claude Julien, il est devenu, selon Samuel Ghiles-Meilhac, un journal de la gauche radicale, . D'après Samuel Ghiles-Meilhac, beaucoup de collaborateurs du journal sont engagés dans le soutien de la cause palestinienne : Amnon Kapeliouk, Joseph Algazy, Michel Warchawski, Samir Kassir, Éric Rouleau, Edward Saïd, Étienne Balibar, Alain Gresh, Dominique Vidal et Serge Halimi.

La position du "Monde diplomatique" sur le conflit israélo-palestinien est vue par Alexandre Del Valle comme une accusation à l'égard d'Israël d'être l'unique responsable des problèmes d'une paix qui tarde à venir. L'auteur fait le reproche au journal de partager les vues pro-palestiniennes d'un certain nombre de personnalités qui interviennent régulièrement dans ses colonnes.

Au sujet du sionisme, Alain Finkielkraut a émis une critique virulente, écrivant que 

Dans un éditorial publié sur le site du journal, Dominique Vidal précise clairement la position du "Monde diplomatique" à l'égard du sionisme : 
Concernant l'antisémitisme, le magazine de la communauté juive "L'Arche" dénonce en l'attitude de l'association "Les Amis du Monde diplomatique" pour son soutien au livre d'Alain Ménargues, "Le Mur de Sharon", que "L'Arche" juge antisémite. Le rédacteur en chef de "L'Arche", Méir Waïntrater, reprochait le silence de Dominique Vidal depuis la sortie du livre. Cela dit, Dominique Vidal dénonce trois mois plus tard, dans les colonnes du "Monde diplomatique", les passages du livre reprenant des thèmes jugés antisémites par "L'Arche", écrivant que, 

L'auteur Alain Ménargues dénonce alors ce qu'il considère comme : 
Il a dit s'étonner qu'un mensuel qui se veut ouvert au débat comme le "Monde diplomatique" cède à ce qu'il juge être des .

Ces accusations, reprises par la rédaction, ont eu pour effet :

Fin 2005, des désaccords apparaissent au sein de l'association ATTAC, recoupant ceux au sein du "Monde diplomatique". Les divergences entre Bernard Cassen, Jacques Nikonoff, Ignacio Ramonet et Maurice Lemoine d'une part, Dominique Vidal et Alain Gresh d'autre part, amènent ces derniers à démissionner en janvier 2006 de leur poste de directeurs de rédaction du "Monde diplomatique", restant membres de la rédaction comme journalistes.

Le quotidien "Libération" estime que : « Alain Gresh et Dominique Vidal se situent dans un courant de « gauche internationaliste » qui s'oppose à une mouvance chevènementiste ou « nationale-républicaine », où l'on retrouve, avec des nuances, Bernard Cassen et le nouveau rédacteur en chef, Maurice Lemoine ». Selon le même journal, les tensions viennent notamment : de divergences sur la question de la laïcité et du voile, la position de Ignacio Ramonet au sujet du régime cubain ; et de désaccords au sujet des FARC colombiens.

Des critiques sur l'apparition d'annonces publicitaires dans le journal émanent parfois d'une partie des lecteurs. Le plus souvent, les reproches concernent des publicités pour des activités dont le journal, par ailleurs, critique le mode de fonctionnement, par exemple les complémentaires santé, les services bancaires ou les produits pharmaceutiques. Ces lecteurs estiment que ces annonces pourraient affecter la ligne éditoriale et, en particulier, limiter la liberté d'expression sur les thèmes en question.

Deux campagnes publicitaires ont notamment rencontré un flot important de critiques. En novembre et , des annonces publicitaires pour IBM et pour Renault occupent deux pages complètes. Dans les éditions de février et mars 2004 apparaissent des annonces de Microsoft, pourfendeur du logiciel libre, alors même que le mensuel publie des articles favorables aux logiciels libres et qu'il les utilise pour son site internet (SPIP).

En réponse à ces critiques, l'équipe éditoriale apporte les éléments suivants :

Le journal a également publié des articles critiques sur la publicité.

En 2015, le Monde diplomatique compte 37 éditions internationales en 20 langues : 32 imprimées et 5 électroniques (non comprises celles qui dépendent des éditions imprimées), dont une édition en espéranto.

Voici la diffusion mensuelle moyenne du "Monde diplomatique", selon les données de l'OJD.
Le , le journal devient le premier en France à avoir une présence sur internet.

"Le Monde Diplomatique" possède une plateforme de réseau social dédiée aux Amis du Monde Diplomatique.


Le site du journal a été l'un des premiers à utiliser SPIP.


</doc>
<doc id="17703" url="https://fr.wikipedia.org/wiki?curid=17703" title="Théorie des groupes">
Théorie des groupes

La théorie des groupes est une discipline mathématique. C'est la partie de l'algèbre générale qui étudie les structures algébriques appelées groupes. Le développement de la théorie des groupes est issu de la théorie des nombres, de la théorie des équations algébriques et de la géométrie.

L'une des origines de l'idée de groupe est l'étude des équations algébriques par Joseph-Louis Lagrange (1771). La terminologie de « groupe » est mise en évidence pour la première fois par Évariste Galois (1830) : on peut « grouper » les automorphismes du corps de décomposition d'un polynôme séparable. L'idée de groupe tient aussi ses sources de l'étude de nouvelles géométries, Felix Klein (1872), et de la théorie des nombres : Leonhard Euler, Carl Friedrich Gauss.

La théorie des groupes est très utilisée en chimie. Elle sert par exemple à simplifier l'écriture de l'hamiltonien d'une molécule en exploitant ses symétries. Elle permet de calculer les orbitales moléculaires comme somme d'orbitales atomiques et de prédire le type de déformation que va subir une molécule en spectroscopie infrarouge (IR). En spectroscopie, elle permet de savoir si une transition sera visible dans un spectre infrarouge et/ou dans un spectre Raman, selon la symétrie de sa déformation.

Chaque molécule possède une symétrie qui peut être déterminée à l'aide du synoptique dans la ci-dessous. Une fois le groupe ponctuel de symétrie trouvé, on utilise la table de caractères correspondante.
Dans " les structures élémentaires de la parenté" l’ethnologue Claude Lévi-Strauss, aidé du mathématicien André Weil, dégage le concept de structure élémentaire de parenté en utilisant la notion de groupe (en particulier le groupe de Klein). Dans "La Structure des mythes," Lévi-Strauss réutilisera les groupes de Klein pour établir la « formule canonique du mythe ».

La théorie des groupes est aussi très utilisée en physique théorique, notamment pour le développement des théories de jauge.

Les groupes donnent lieu à des tables de représentation irréductibles. Par exemple, pour l'eau, les symétries se combinent selon :
et la table de caractère liée : 
Chaque mode de vibration moléculaire peut être ramené à une combinaison des représentations irréductibles dont les caractéristiques permettent ensuite d'établir s'ils relèvent de la spectroscopie Raman ou infrarouge.



</doc>
<doc id="17705" url="https://fr.wikipedia.org/wiki?curid=17705" title="Produit matriciel">
Produit matriciel

Le produit matriciel désigne la multiplication de matrices, initialement appelé la « composition des tableaux ».

Il s'agit de la façon la plus fréquente de multiplier des matrices entre elles.

En algèbre linéaire, une matrice A de dimensions "m" lignes et "n" colonnes (matrice "m"×"n") représente une application linéaire ƒ d'un espace de dimension "n" vers un espace de dimension "m". Une matrice colonne V de "n" lignes est une matrice "n"×1, et représente un vecteur "v" d'un espace vectoriel de dimension "n". Le produit A×V représente ƒ("v").

Si A et B représentent respectivement les applications linéaires ƒ et "g", alors A×B représente la composition des applications ƒo"g".

Cette opération est utilisée notamment en mécanique lors des calculs de torseur statique, ou en informatique pour la matrice d'adjacence d'un graphe.

Le produit de deux matrices ne peut se définir que si le nombre de colonnes de la première matrice est le même que le nombre de lignes de la deuxième matrice, c'est-à-dire lorsqu'elles sont de type compatible.

Si formula_1 est une matrice de type formula_2 et formula_3 est une matrice de type formula_4, alors leur "produit", noté formula_5 est une matrice de type formula_6 donnée par :

La figure suivante montre comment calculer les coefficients formula_8 et formula_9 de la matrice produit formula_10 si formula_11 est une matrice de type formula_12, et formula_13 est une matrice de type formula_14.
formula_15

formula_16

En général, la multiplication des matrices n'est pas commutative, c'est-à-dire que formula_10 n'est pas égal à formula_20, comme le montre l'exemple suivant.

Si l'on considère les matrices formula_23 et formula_24, où formula_25 et formula_26 sont des matrices vérifiant :

on a alors l'égalité

On remarquera l'analogie entre le produit de matrice par blocs et le produit de deux matrices carrées d'ordre 2.

N.B. : on ne définit pas ainsi une nouvelle forme de multiplication de matrices. Cela correspond simplement à une méthode de calcul du produit matriciel ordinaire pouvant simplifier les calculs.

Pour deux matrices de même type, nous avons le produit d'Hadamard ou produit composante par composante. Le produit d'Hadamard de deux matrices formula_1 et formula_3 de type formula_2, noté , est une matrice de type formula_2 donnée par 
Par exemple :

Ce produit est une sous-matrice du produit de Kronecker (voir ci-dessous).
Pour deux matrices arbitraires formula_1 et formula_13, nous avons le produit tensoriel ou produit de Kronecker qui est défini par 

Si formula_11 est une matrice de type formula_2 et formula_13 est une matrice de type formula_48 alors est une matrice de type formula_49. À nouveau cette multiplication n'est pas commutative.

Par exemple 

Si formula_11 et formula_13 sont les matrices d'applications linéaires et , respectivement, alors représente le produit tensoriel des deux applications, .

Les trois multiplications matricielles précédentes sont associatives

distributives par rapport à l'addition :
et compatibles avec la multiplication par un scalaire :

La multiplication par un scalaire formula_57 d'une matrice formula_1 donne le produit

Si nous travaillons avec des matrices sur un anneau, alors la multiplication par un scalaire est parfois appelée la "multiplication à gauche" tandis que la "multiplication à droite" est définie par :

Quand l'anneau fondamental est un anneau commutatif, par exemple, le corps des réels ou des complexes, les deux multiplications sont identiques.

Cependant, si l'anneau n'est pas commutatif, tel que celui des quaternions, alors ils peuvent être différents. Par exemple

Le problème qui consiste, étant donné deux matrices carrées, à les multiplier rapidement, est un problème important en algorithmique. L'algorithme qui découle de la définition a une complexité en temps en formula_62, où formula_63 est le nombre de lignes des matrices. Une borne inférieure est formula_64 (car chacun des formula_65 coefficients de la matrices doit être écrit). L'exposant optimal pour la complexité est donc compris entre 2 et 3 mais sa valeur exacte est un problème ouvert. De nombreux algorithmes ont été inventés pour ce problème, citons par exemple l'algorithme de Strassen en formula_66, le premier à avoir été découvert, et l'algorithme de Coppersmith-Winograd en formula_67. En 1993, Bahar et al. ont donné un algorithme de multiplications de matrices pour des matrices représentées symboliquement à l'aide d'une structure de données appelée Algebraic Decision Diagrams (ADD), qui est une généralisation des diagrammes de décision binaire.

On se donne une suite de matrices rectangulaires formula_68 et on souhaite en calculer le produit formula_69 (on suppose que toutes les matrices ont une taille compatible, c'est-à-dire que le produit est bien défini). Le produit matriciel étant associatif, n'importe quel parenthésage du produit donnera le même résultat. Cependant le nombre de multiplications scalaires à effectuer dépend du parenthésage retenu si les matrices sont de tailles différentes.

Ainsi si l'on prend formula_70, formula_71 et 
formula_72 on a bien formula_73 = formula_74 mais le calcul de formula_73 nécessite 6 multiplications scalaires tandis que celui de formula_74 en nécessite 18.

Il peut donc être utile d'exécuter un algorithme d'optimisation de produit matriciel enchaîné afin d'identifier le parenthésage le plus efficace avant d'effectuer les produits proprement dits.

Pour vérifier un produit matriciel, il existe des algorithmes plus efficaces que de simplement le recalculer.

Par exemple l'algorithme de Freivalds est un algorithme probabiliste qui permet de vérifier le résultat d'un produit matriciel en formula_64 avec une probabilité d'erreur aussi faible que voulue.

Addition matricielle



</doc>
<doc id="17706" url="https://fr.wikipedia.org/wiki?curid=17706" title="Note de musique">
Note de musique

En musique, une note est un symbole permettant de représenter un fragment de musique par une convention d'écriture de la hauteur et de la durée d'un son, ou bien, souvent dans un contexte plus large, la note est la hauteur elle-même d'un son.

Une note est constituée d'une « tête », d'une « hampe » et d'une « durée » : 
La « tête » peut être évidée (ronde, blanche…) ou pleine (noire, croche…). Pour certains instruments, elle peut prendre des formes diverses comme pour la percussion (par exemple, croix pour les cymbales, triangle pour l'instrument du même nom, rectangle pour le tambourin…).

La « ronde » n'a pas de hampe, c'est à partir de la blanche pointée qu'elle est utilisée. De longueur constante pour les notes à une seule tête, la hampe peut être plus ou moins longue suivant l'intervalle, l'accord ou le regroupement rythmique auxquels elle appartient. D'une manière générale, sa direction est au-dessus et à droite de la tête pour les notes placées en dessous de la troisième ligne, et en dessous et à gauche pour les notes placées au-dessus de la troisième ligne.

À partir de la croche, le nombre de crochets (pour un note) ou de barres (pour un ensemble de notes) détermine la « durée » rythmique de la note : un pour la croche, deux pour la double-croche, trois pour la triple-croche… aucun pour la noire et la blanche, pointées ou non.

Dans le solfège, la note de musique est littéralement « notée » sur une partition afin d'être lue par le musicien interprète. On lui attribue quatre caractéristiques principales : la hauteur, la durée, l'intensité et le timbre.

De par sa position sur la portée, la note représente une hauteur (dimension verticale), et par son dessin, elle représente une durée (dimension horizontale). 

Tout son musical (ou note) possède une fréquence fondamentale (nombre de vibrations par seconde calculé en hertz) correspondant à sa hauteur. Deux notes dont les fréquences fondamentales ont un rapport qui est une puissance de deux (c'est-à-dire la moitié, le double, le quadruple…) donnent deux sons très similaires et portent le même nom. Cette observation permet de regrouper toutes les notes qui ont cette propriété dans la même catégorie de hauteur.

Dans la musique occidentale, les catégories de hauteurs sont au nombre de douze. Sept d'entre elles sont considérées comme les principales et ont pour noms : "do", "ré", "mi", "fa", "sol", "la" et "si". L'intervalle compris entre deux hauteurs dont la fréquence de l'une vaut le double (ou la moitié) de l'autre s'appelle une octave. Pour distinguer deux notes de même nom dans deux octaves différentes, on numérote les octaves et donne ce numéro aux notes correspondantes : par exemple, le "la" a une fréquence de dans la norme internationale (bien qu'en pratique, cela puisse parfois varier). Cette fréquence de référence est donnée par un diapason.

Dans la gamme tempérée, la formule permettant de mesurer la fréquence d'une note par rapport à une note de départ est : formula_1. Avec formula_2 le nombre de demi-tons au-dessus de la note de départ formula_3. On s'aperçoit que la fréquence croît de manière géométrique par rapport à la note.

Dans la théorie de la musique, on parle de « degré ». Celui-ci représente une hauteur relative appartenant à une échelle musicale donnée. En effet, il existe de nombreuses possibilités pour choisir les fréquences des notes dans une octave. Le choix des échelles dépend des époques, des instruments et des types de musique.

Selon l'échelle, on obtiendra des gammes musicales différentes. La musique classique en utilise deux : l'échelle diatonique et l'échelle chromatique.

Pour la musique occidentale, il existe, depuis le Moyen Âge, deux types de notation :

« UT queant laxis

REsonare fibris

MIra gestorum

FAmuli tuorum

SOLve polluti

LAbii reatum

Sancte Iohannes »


Parmi les systèmes musicaux non occidentaux, certains ont adopté les nomenclatures ci-dessus, d'autres ont conservé des appellations spécifiques. Par exemple, la musique indienne utilise les "svara". Les noms des sept "svara" sont : स ("Sa"), रे ("Ré"), ग ("Ga"), म ("Ma"), प ("Pa"), ध ("Dha"), नि ("Ni").

Depuis Guido d'Arezzo, les notes de musique peuvent être désignées par "ut, ré, mi, fa, sol, la". Cette pratique a été standardisée par les recommandations du pape Jean XX. Auparavant, en Occident, divers systèmes de notation existaient. La nouvelle méthode permettait d'apprendre en un jour ce qu'il fallait un an pour apprendre avec la méthode grecque utilisant des lettres pour noter tant les tons que les échelles.

En général, la musique est souvent représentée par une ou des notes, que ce soit dans la bande dessinée, pour symboliser un chant, ou dans l'informatique, pour spécifier que le fichier est un fichier musical.

La représentation des symboles musicaux en informatique existe dans différents jeux de caractère (Unicode, LaTeX, Lilypond…). Par exemple, l'encodage en Unicode, pour ♩ (une noire), ♪ (une croche), ♫ (deux croches) et ♬ (deux doubles-croches) :
Cependant, cet encodage ne permet pas le positionnement sur la portée, ni aucune autre manière de distinguer les hauteurs de son. Pour cela, on emploie d'autres normes. Par exemple, un des aspects de la norme MIDI est la numérotation des notes. On dit alors que le la 440 Hz est le numéro 69 et que toute différence de numéro par rapport à cette note est comptée en demi-tons. Par exemple, tous les multiples de 12 sont des do. À l'origine, seuls les numéros de 0 à 127 étaient permis, mais selon la variante de cette échelle, des notes plus graves ou plus aigües peuvent aussi être permises. À partir du moment où on suppose un tempérament égal (gamme tempérée), on peut aussi représenter des notes intermédiaires en utilisant des fractions (voir la formule de fréquence mentionnée ci-haut).

D'autres notations informatisées utilisent les lettres de notes anglaises de A à G et le symbole # tenant lieu de dièse, mais pas de symbole bémol car non-nécessaire (tout bémol a un équivalent dièse). Les notes non-dièse peuvent être suivies d'un trait d'union ou d'une espace. On termine ensuite avec le numéro d'octave, qui augmente de 1 à chaque do comme dans la table ci-haut, mais qui peuvent être décalés (le la 440 pourrait être écrit A-2, A-3 ou A-4 selon l'échelle choisie). C'est ce qui est utilisé visuellement dans les éditeurs de type tracker (à la Amiga), quoique à l'interne, leurs formats utilisent des périodes (inverses de fréquences) comme le format MOD, ou une combinaison note de la gamme & octave (par exemple, les deux parties de la division avec reste d'une note MIDI par 12) comme le format S3M.





</doc>
<doc id="17710" url="https://fr.wikipedia.org/wiki?curid=17710" title="Polyèdre">
Polyèdre

Un polyèdre est une forme géométrique à trois dimensions (un solide géométrique) ayant des faces planes polygonales qui se rencontrent selon des segments de droite qu'on appelle arêtes. 

Le mot "polyèdre", signifiant "à plusieurs faces", provient des racines grecques "πολύς" ("polis"), « beaucoup » et "ἕδρα" ("hedra"), « base », « siège » ou « face ».

Comme beaucoup d'autres concepts, la notion de polyèdre a été formellement introduite par les Grecs. Leur étude occupe une place tout à fait significative dans les "Éléments" d'Euclide et a, pour ce qui est des mathématiques, constitué l'une des préoccupations importantes de Platon.

Il suffit cependant de contempler les pyramides pour réaliser que cette notion est perçue depuis des temps encore plus anciens.

Après Platon, Euclide et Archimède dans l'Antiquité, l'étude des polyèdres a occupé nombre de bons esprits des temps modernes, et notamment ceux de Kepler, Euler, Poincaré, Hilbert

La définition donnée en introduction peut sembler suffisamment claire pour la plupart d'entre nous. Elle ne l'est pas pour un mathématicien. Aussi étrange que cela puisse paraître, dans la mesure où le concept de "polyèdre" ne fait pas référence à la dimension de l'espace dans lequel il se trouve, il n'existe pas de définition universellement agréée sur ce qui fait que « quelque chose » soit un "polyèdre" (le cœur du problème vient de ce que la notion intuitive de polyèdre n'est pas exactement la même selon qu'on a dans l'idée une surface ou un volume).

Afin d'obvier à cette difficulté, on est conduit à introduire la notion de simplexe. On peut la considérer comme équivalente à celle de polyèdre en dimension 3, et elle permet des généralisations aux dimensions supérieures. Un polyèdre formula_1 de dimension formula_2 est alors la réunion d'un ensemble fini de simplexes formula_3 de dimension formula_4 tel que chacune des d-faces (formula_5) d'un simplexe formula_3 est un élément de formula_7 et tel que pour tout couple de simplexe formula_3, formula_9 l'intersection formula_10 est soit vide soit une (d-1)-face commune à formula_3 et formula_12. 

Ainsi un simplexe représente-t-il une définition généralisable de la notion intuitive de polyèdre :

Il est la réunion de ses d-faces et l'intersection de deux d-faces quelconques d'un simplexe est soit vide soit une face de dimension formula_13. 

Par exemple, un triangle, qui est un 2-simplexe, est la réunion de segments et l'intersection de deux segments adjacents est un point qui est un sommet du triangle.

Un polyèdre apparait ainsi comme construit à partir de différentes sortes d'éléments ou d'entités, présentant un nombre différent de dimensions :

Il s'ensuit que l'objet ci-dessous n'est pas un polyèdre au sens de cette définition ; en effet, la face supérieure de la boîte n'est pas limitée par un mais par deux circuits d'arêtes : l'un qui la limite extérieurement et l'autre qui la limite intérieurement. 

Plus généralement en mathématiques et dans d'autres disciplines, le terme « polyèdre » est utilisé pour faire référence à une variété de constructions reliées, certaines géométriques et d'autres purement algébriques ou abstraites.

En particulier, un polytope est un polyèdre convexe et borné.

Dans "", et définissent les polyèdres par un ensemble fini de polygones planaires tels que chaque arête d’un polygone est partagée par un seul autre polygone, et aucun autre sous-ensemble des polygones ne possède cette propriété. Cette définition implique des contraintes strictes : par exemple, les polyèdres ne doivent pas présenter d’autointersections.

Les polyèdres sont en général nommés selon leur nombre de faces. La nomenclature est basée sur le grec classique. On a ainsi, par exemple : tétraèdre (4 faces), pentaèdre (5 faces), hexaèdre (6 faces), (7 faces), triacontaèdre (30 faces), et ainsi de suite. Cette méthode de désignation a son équivalent dans la nomenclature des polygones.

Les arêtes ont deux caractéristiques importantes (à moins que le polyèdre ne soit complexe) :
Ces deux caractéristiques sont duales.

Un polyèdre est dit convexe si tout point de tout segment joignant deux points quelconques du polyèdre appartient au polyèdre.
Autrement dit, un polyèdre est convexe si toutes ses diagonales sont entièrement contenues dans son intérieur.
Il est possible de donner une définition barycentrique d'un tel polyèdre : c'est l'enveloppe convexe d'un ensemble fini de points non coplanaires.

Soit un polyèdre. Si l'on note :
on appelle caractéristique d'Euler le nombre

Pour un polyèdre convexe, cette caractéristique vaut toujours 2.
C'est la relation d'Euler

Pour chaque polyèdre, il existe un polyèdre dual ayant des faces à la place des sommets originaux et vice versa. Dans la plupart des cas, le dual peut être obtenu par le processus de réciprocité sphérique.
Le dual d'un polyèdre régulier peut se construire en joignant les centres des faces adjacentes.
Un polyèdre est une forme tridimensionnelle qui se compose d'un nombre fini de faces polygonales qui sont des parties de plans ; les faces se rencontrent le long des arêtes qui sont des segments de droite, et les arêtes se rencontrent aux points nommés sommets. Les cubes, les prismes et les pyramides sont des exemples de polyèdres. 

Le plus souvent, le polyèdre délimite un volume limité de l'espace à trois dimensions. Quelquefois, ce volume intérieur est considéré être une partie du polyèdre ; d'autres fois, seule la surface est considérée.
Les polyèdres traditionnels incluent les cinq polyèdres convexes réguliers que l'on nomme les solides de Platon : le tétraèdre (4 faces), le cube (ou hexaèdre) (6 faces), l'octaèdre (8 faces), le dodécaèdre régulier (12 faces) et l'icosaèdre (20 faces). Les autres polyèdres traditionnels sont les quatre polyèdres non convexes réguliers (les solides de Kepler-Poinsot), les treize solides d'Archimède convexes (cuboctaèdre, icosidodécaèdre, tétraèdre tronqué, cube tronqué, octaèdre tronqué, dodécaèdre tronqué, icosaèdre tronqué, cuboctaèdre tronqué, icosidodécaèdre tronqué, rhombicuboctaèdre, cube adouci, dodécaèdre adouci et rhombicosidodécaèdre) et les 53 polyèdres uniformes restants.

Un polyèdre possède au moins 4 faces, 4 sommets et 6 arêtes. Le plus petit polyèdre est le tétraèdre.

On peut définir diverses classes de polyèdres présentant des symétries particulières :

On appelle solide uniforme un solide dont toutes les faces sont régulières et tous les sommets identiques. Ainsi sont donc tous les solides réguliers et semi-réguliers précédents. Ils sont en tout 75, auxquels il faut ajouter les deux familles infinies des prismes et des antiprismes.

Bien sûr, il est facile de tordre de tels polyèdres, de telle façon qu'ils ne sont plus symétriques. Mais, lorsqu'un nom de polyèdre est donné, tel que l'icosidodécaèdre, la géométrie la plus symétrique est toujours impliquée, sauf indication contraire. 

Les groupes de symétrie polyédriques sont tous des groupes ponctuels et incluent :

Les polyèdres à symétrie chirale n'ont pas de symétrie axiale et par conséquent ont deux formes énantiomorphes qui sont les réflexions l'un de l'autre. Les polyèdres adoucis ont cette propriété.

Un polyèdre régulier possède des faces régulières et des sommets réguliers. Le dual d'un polyèdre régulier est aussi régulier.

Partons d'un sommet et prenons les points situés à une distance donnée sur chacune des arêtes. Relions ces points, nous obtenons le polygone du sommet. Si celui-ci est régulier on dit que le sommet est régulier.
Un polyèdre est régulier s'il est constitué de faces toutes identiques et régulières, et que tous ses sommets sont identiques. Ils sont au nombre de neuf, classiquement répartis en deux familles :


Les polyèdres quasi réguliers sont à faces régulières, de sommet uniforme et d'arête uniforme. Il en existe deux convexes :

Les polyèdres duaux quasi réguliers sont d'arête uniforme et de . Il en existe deux convexes, en correspondance avec les deux précédents :

Le terme semi-régulier est diversement défini. Une définition consiste en « des polyèdres de sommet uniforme avec deux sortes ou plus de faces polygonales ». Ils sont effectivement les polyèdres uniformes qui ne sont ni réguliers, ni quasi réguliers.

Un polyèdre est semi-régulier si ses faces sont constituées de plusieurs sortes de polygones réguliers, et que tous ses sommets sont identiques. Ainsi sont par exemple les solides d'Archimède, les prismes et les antiprismes réguliers. La terminologie ne paraît pas tout à fait arrêtée. On parle parfois de "solides semi-réguliers de la première espèce" pour désigner ceux de ces solides qui sont convexes, et de "solides uniformes" pour le cas général. Les polyèdres de Catalan ne sont pas semi-réguliers, mais ont des faces identiques et des sommets réguliers. On dit parfois de tels polyèdres qu'ils sont "semi-réguliers de la seconde espèce".

Les polyèdres "convexes" et leurs duaux incluent les ensembles des :

Il existe aussi beaucoup de polyèdres uniformes non convexes, incluant des exemples de divers sortes de prismes.

Un est à la fois (faces égales) et isogonal (de coins égaux). En plus des polyèdres réguliers, il existe beaucoup d'autres exemples. 

Le dual d'un polyèdre noble est aussi un polyèdre noble.

Quelques familles de polyèdres, où chaque face est un polygone de même sorte :


Il n'existe pas de polyèdre dont les faces sont toutes identiques et qui sont des polygones réguliers avec six côtés ou plus car le point de rencontre de trois hexagones réguliers définit un plan. (voir polyèdre oblique infini pour les exceptions).

Un deltaèdre est un polyèdre dont les faces sont toutes des triangles équilatéraux. Il en existe une infinité, mais seuls huit sont convexes :


Norman Johnson a cherché les polyèdres non uniformes ayant des faces régulières. En 1966, il publia une liste de 92 solides convexes, maintenant connus comme les solides de Johnson, et leur donna leurs noms et leurs nombres. Il ne prouva pas qu'ils n'étaient que 92, mais il conjectura qu'il n'y en avait pas d'autres. , en 1969, démontra que la liste de Johnson était complète.

Les pyramides sont autoduales.

La stellation d'un polyèdre est le processus d'expansion des faces (dans leurs plans), c’est-à-dire qu'elles se rencontrent pour former un nouveau polyèdre.

C'est la réciproque exacte du facettage qui est le processus d'enlèvement de parties d'un polyèdre sans créer de nouveau sommets quelconques. Le facettage permet d'obtenir, entre autres, de nombreux nouveaux solides semi-réguliers concaves. On construit de nouvelles faces régulières en regroupant les arêtes d'un polyèdre semi-régulier. Le plus simple est un héptaèdre construit à partir de l'octaèdre, constitué de trois faces carrées et de quatre faces triangulaires.

C'est l'opération qui consiste à raboter un sommet ou une arête. Elle conserve les symétries du solide.

Cette opération permet d'obtenir sept des solides d'Archimède à partir des solides de Platon. On remarque en effet qu'en rabotant de plus en plus les arêtes d'un cube on obtient successivement le cube tronqué, le cuboctaèdre, l'octaèdre tronqué et enfin l'octaèdre. On peut aussi suivre cette série dans l'autre sens.

En partant du dodécaèdre régulier on obtient le dodécaèdre tronqué, l'icosidodécaèdre, l'icosaèdre tronqué (qui donne sa forme au ballon de football), puis l'octaèdre.

Le tétraèdre donne le tétraèdre tronqué.

On peut appliquer cette opération au grand dodécaèdre ou au grand icosaèdre et obtenir des solides uniformes concaves.

À partir d'un cube, cette opération donne successivement un cuboctaèdre, puis un dodécaèdre rhombique.

À partir d'un dodécaèdre régulier, on obtient l'icosidodécaèdre puis le triacontaèdre rhombique.

Les composés polyédriques sont formés comme des composés de deux polyèdres et plus.

Ces composés partagent souvent les mêmes sommets que les autres polyèdres et sont souvent formés par stellation. Certains sont listés dans la .

Un zonoèdre est un polyèdre convexe où chaque face est un polygone avec une symétrie inverse ou, de manière équivalente, des rotations à 180°.

Le mot « polyèdre » a été employé pour une variété d'objets ayant des propriétés structurelles similaires aux polyèdres traditionnels.

Un est un polyèdre qui est construit dans un espace à trois dimensions complexes. Cet espace possède six dimensions : trois dimensions réelles correspondant à l'espace ordinaire, avec une dimension imaginaire accompagnant chacune.

Certains champs d'étude permettent aux polyèdres d'avoir des faces et des arêtes courbées.

La surface d'une sphère peut être divisée par des arcs de grands cercles (délimitant des régions appelées polygones sphériques) pour former un . Ce point de vue est très adapté pour démontrer une grande partie de la théorie des polyèdres symétriques.

Les deux types importants sont :

Plus récemment, les mathématiciens ont défini un polyèdre comme un ensemble dans un espace affine réel (ou euclidien) de dimension quelconque "n" qui possède des côtés plats. Il peut être défini comme l'union d'un nombre fini de polyèdres convexes, où un "polyèdre convexe" est un ensemble quelconque qui est l'intersection d'un nombre fini de demi-espaces. Il peut être borné ou non borné. Dans ce sens, un polytope est un polyèdre borné.

Tous les polyèdres traditionnels sont des polyèdres généraux, et en plus, il existe des exemples tels que :







</doc>
<doc id="17711" url="https://fr.wikipedia.org/wiki?curid=17711" title="Usage des majuscules en français">
Usage des majuscules en français

L’usage des majuscules en français est encadré par des conventions orthographiques et typographiques. Il en découle que le non-respect de celles-ci, par l'usage incorrect d'une minuscule ou d'une majuscule, peut être une faute d'orthographe. Pour certains auteurs, qui font la différence entre majuscule et capitale, celle-ci n'est pas régie par ces conventions. En français, une majuscule est un repère visuel qui facilite la lecture d'un texte.

Traditionnellement, la majuscule ne peut être que la première lettre d'un mot, sauf dans le cas de noms composés ("Pays-Bas", "le Très-Haut").

En outre, si la première lettre est ligaturée, alors toute la ligature est en capitale ("Œuvre").

Le fait que la première lettre d'un mot soit une majuscule ou une minuscule dépend de la nature du mot et de sa place dans la phrase ou dans le texte.

En français, « l'accent a pleine valeur orthographique ». L'Académie française recommande donc l'usage d'accent ou tréma sur une majuscule, tout comme l'utilisation de la cédille et de la ligature. Ainsi les publications de qualité écrivent-elles les majuscules (tout comme les capitales) avec les accents et autres diacritiques, au même titre que les minuscules. En effet, les signes diacritiques ont un rôle important dans les langues qui les utilisent.

Au Québec, « On doit mettre tous les accents et tous les signes diacritiques sur les capitales, excepté sur les sigles et les acronymes quand ils sont écrits en capitales ». Cependant, en Suisse romande, « On ne met pas d'accent à la lettre initiale (majuscule) d'un mot écrit en minuscules. En revanche, on met les accents dans une phrase entièrement en capitales ».

Les signes diacritiques et ligatures restent reproduits par les éditeurs de publications académiques et dictionnaires.

La pratique de l'accentuation a connu une évolution dans la langue française. Elle existe à la fin du Moyen Âge et se normalise tardivement. Dès les débuts de l'imprimerie, les imprimeurs s'efforcent de graver et reproduire les signes diacritiques tels qu'ils apparaissent dans les manuscrits. La bible de Gutenberg les reproduit déjà et la question est réglée dès les années 1470 pour les alphabets plus compliqués comme l'alphabet grec.

La pratique tendant à ne pas indiquer les accents sur les majuscules et les capitales trouve sa source dans l'utilisation de caractères de plomb à taille fixe en imprimerie. La hauteur d'une capitale accentuée étant supérieure, la solution était alors soit de graver des caractères spéciaux pour les capitales accentuées en diminuant la hauteur de la lettre, soit de mettre l'accent après la lettre, soit simplement de ne pas mettre l'accent. Les machines à composer étant d'origine anglo-saxonne (Monotype, Linotype), il n'était pas prévu de mettre des accents sur les capitales. En revanche, en composition manuelle, il existait des capitales accentuées avec un accent en crénage débordant du corps du caractère. Il existait aussi – dans les gros corps – des « accents postiches » qui pouvaient être placés, dans l'interligne, au-dessus des capitales.

Les systèmes informatiques et certains claviers d’usage national laissent subsister des problèmes. Par exemple, sur le clavier AZERTY fourni avec Microsoft Windows pour la France, l’accent aigu est associé à sa lettre ("é") et n’a pas, comme l’accent grave, de touche morte. La raison est que sur cette disposition de clavier, les touches mortes ‹ accent grave › et ‹ tilde › ont été ajoutées par détournement de caractères informatiques  ; or, aux débuts de l’informatique, le caractère associé à l’accent aigu fut le guillemet simple générique « ' », utilisé pour représenter l’apostrophe en français. Sur cette disposition de clavier sous Windows, l’« É » ne peut être saisi qu’à l’aide du pavé numérique  ; +144, ou +0201. D’autres solutions, comme la table des caractères et le clavier virtuel, sont accessibles dans le système ou dans certains logiciels. Un raccourci clavier existe dans Microsoft Word : +, . Pour une utilisation universelle, des raccourcis peuvent être ajoutés via un utilitaire spécialisé, notamment Clavier+. Avec un clavier sous GNU/Linux ou avec un clavier Macintosh, l’« É » est accessible par , et pendant que les capitales sont verrouillées.

Si la touche morte pour l’accent aigu n’existe pas sur le clavier AZERTY français, elle est en revanche présente sur le clavier AZERTY belge, le clavier QWERTY canadien et le clavier QWERTZ suisse. Le clavier « canadien français » a des touches mortes pour tous les accents ainsi que la capitale de l’É en de sa minuscule. Le seul pilote de clavier fourni par Microsoft avec Windows qui permette d’écrire directement en français (y compris les ligatures), est le "clavier canadien multilingue standard", de type QWERTY, qu’il suffit d’activer via la Barre des langues de Windows. Dans l'environnement X Window (utilisé par GNU/Linux, *BSD et autres systèmes apparentés à UNIX), l'utilisation de la touche compose permet l'obtention des majuscules accentuées.

D'autre part, la disposition bépo, disponible en standard sur les distributions récentes de Linux, et téléchargeable pour la plupart des autres systèmes, permet de faire aisément tous les caractères de la langue française (y compris les ligatures e dans l’o et e dans l’a, et les guillemets français « … »). Il possède en particulier les lettres É, È, À, Ç (et Ê pour les claviers de 105 touches) en accès direct.

La possibilité de plus en plus grande offerte par les systèmes d'exploitation de changer à sa guise de disposition de clavier, ajoutées au développement d’Unicode désormais implémenté dans tous les systèmes, estompe actuellement ces difficultés. En effet, pour pallier les insuffisances des dispositions de clavier proposées avec Windows (et à l’origine, pour aider les agents de la NSA à transcrire dans des langues tenues secrètes), Microsoft propose un logiciel nommé MSKLC (Microsoft Keyboard Layout Creator) qui permet de créer ses propres pilotes de clavier et ajoute les installateurs nécessaires. Toutefois, cette installation de pilotes de claviers supplémentaires (jusqu’à 60 par machine) nécessite de disposer de droits d’administrateur. Ces derniers peuvent être éludés en installant sur tout poste le logiciel Portable Keyboard Layout (PKL), basé sur plusieurs scripts AutoHotkey permettant un fonctionnement des touches en surcouche, indépendamment du pilote Windows déjà installé.

Les majuscules s'utilisent :

Quand la majuscule est due à la place du mot, elle ne se place qu'à la première lettre d'un nom composé dont les éléments sont reliés par des traits d'union. Exemple : « Avant-hier, je me suis couché tard. »

Les majuscules s'utilisent :

Quand la majuscule est due à la nature du mot, elle se place à la première lettre d'un nom composé dont les éléments sont reliés par des traits d'union, ainsi qu'aux premières lettres de tous les substantifs, adjectifs et verbes formant ce nom composé. Exemples : le Très-Haut, les Pays-Bas, la Grande-Bretagne, Saint-Jacques-de-Compostelle ou la rue du Cherche-Midi.

Certains noms propres sont devenus des noms communs. Le processus s'appelle antonomase. Dans ce cas, ils perdent la majuscule, sauf si le rapport avec la valeur primitive est toujours perceptible. Ainsi, les appellations génériques de certains vins ou fromages sont des noms communs, alors qu'elles viennent de noms de région ou de ville. Par exemple, on écrit "un bordeaux" pour désigner un vin de Bordeaux et "un cantal" pour désigner un fromage du Cantal. Des noms de personnes sont également touchés par le phénomène de l'antonomase : "un browning" désigne une arme inventée par Browning. En revanche, on écrira "un Van Dyck" pour un tableau peint par Van Dyck.
Le procédé d’antonomase inverse consiste à transformer un nom commun en un nom propre pour désigner une réalité ou une personne en particulier, et non plus seulement la chose générale définie par le nom commun. Ce nom propre, mis à la place de ce qu'il désigne dans la phrase, peut être composé (voir ci-dessous les règles qui leur sont propres). Le mot prend alors la valeur d’un nom propre, y compris pour l'usage de la majuscule. C’est, par exemple, le cas de « État » et « Homme ».

Un « état » est une manière d’être. En revanche l’autorité qui gouverne un territoire est l’« État ». Par contre, le mot « états » au sens d'« assemblée provinciale chargée de voter l'impôt en dehors des pays d'élection » garde une minuscule (les états de Bourgogne, les états du Languedoc) :

En science, on met une majuscule à « homme » lorsque celui-ci désigne l’ensemble du genre "Homo", mammifère de l’ordre des Primates :

Autres exemples :

Au théâtre ou à l'opéra, lorsqu'un protagoniste n'est pas désigné par son nom, et n'est connu que par sa fonction, il prend la majuscule s'il désigne une personne unique dans la distribution, par exemple le Jardinier, le Soldat, la Fée, mais un garde, une fée.

La majuscule est utilisée pour le premier mot d'un nom composé comme le requiert la règle générale et pour les mots qui, à l’intérieur d’un nom composé, requièrent en eux-mêmes la majuscule :

L'adjectif d'un nom composé ne prend de majuscule que dans les cas suivants :

Cette convention souffre des exceptions :

Elle suit les différences entre les conventions sur les noms composés. Par exemple, en Suisse romande, l’usage est de lier par un trait d’union l'adjectif aux mots « mont, aiguille, bec, cime, dent, pierre, pointe, rocher, tête, tour » alors qu'en France, l'usage est de ne pas utiliser le trait d'union. Ces usages donnent, par exemple :

Il existe plusieurs conventions d'usage des majuscules pour les noms des institutions françaises.

Dans la plupart des ouvrages scientifiques, ces noms s’écrivent sans majuscule pour les institutions qui ne sont pas uniques mais avec une majuscule au premier mot de l’entité pour les institutions qui ont un caractère unique :

Le "Journal officiel de la République française (JORF) utilise peu de majuscules : « directeur » et non « Directeur » (mais « Direction », « Premier ministre », « Conseil d'Etat », « Haute Autorité… », « Haut Conseil… ») et écrit le titre d'un ministre ou le nom d'un ministère entièrement en minuscules : « ministre des affaires étrangères et du développement international », « ministère de la défense » (mais écrit « Etat » avec une majuscule non accentuée, contrairement aux règles de la langue), au contraire du portail du Gouvernement, qui met une voire plusieurs majuscules : « secrétaire d’État chargé des Anciens Combattants et de la Mémoire, auprès du ministre de la Défense », « ministre des Affaires étrangères et du Développement international ».

Un moyen couramment utilisé dans la presse est de faire suivre de telles appellations par leur sigle entre parenthèses afin d’en marquer la fin : « le Parti socialiste (PS), la Banque centrale européenne (BCE), la Société nationale des chemins de fer français (SNCF), l’Union européenne (UE) ».

Ces conventions ne sont cependant pas suivies par tous les éditeurs. L’usage commercial consiste à mettre une majuscule sur chaque mot autre qu'un mot de liaison.

L'usage des majuscules pour les noms de marques commerciales qui ne sont pas utilisées comme noms communs est celui des noms propres.

Pour les marques utilisées comme nom commun, cet usage n'est pas une règle. C'est par exemple le cas de Kleenex, Klaxon, Frigidaire, Frigo, Scotch ou Rimmel lorsqu'ils font référence à « mouchoir en papier », « avertisseur », « réfrigérateur », « ruban adhésif » et « fard à cils ». En effet, l'usage de la majuscule se retrouve dans les dictionnaires Larousse et Universalis, dans "Le Ramat de la typographie" (québécois) et dans le alors qu'il n'en est pas fait état dans des ouvrages tels que "Le Petit Robert", le Trésor de la langue française informatisé (TLFi), et le .

Les noms et adjectifs désignant une langue ou le locuteur d'une langue ne prennent pas de majuscule. Le locuteur d'une langue (un francophone, par exemple) ne doit pas être confondu avec le gentilé (un Français, par exemple). Cela permet, dans certains cas, une meilleure compréhension ; ainsi, « Le Français (un gentilé) est compliqué » ne veut pas dire la même chose que « Le français (la langue) est compliqué ». Ces distinctions apparaissent dans les exemples suivants :

Les conventions typographiques sur l’usage des majuscules pour les dénominations désignant le régime politique d’un pays ou d’une zone géographique peuvent varier selon l’usage suivi.

L’usage traditionnel est celui qui est notamment préconisé dans plusieurs guides typographiques dont notamment le , le "Code typographique", le "Mémento typographique" ou le "Dictionnaire des règles typographiques".

Dans ce cadre, les dénominations de pays ou de zone géographique prennent une majuscule initiale s'ils sont immédiatement suivis d'un nom commun (terme spécifique ou complément du nom générique) ou encore d'un ou de plusieurs adjectifs (eux aussi termes spécifiques) :
En revanche, le même type de dénomination conserve la minuscule au terme générique lorsqu'il est immédiatement suivi d'un nom propre complément (du générique) :
Le terme générique garde sa minuscule si le nom composé ne représente pas une entité unique, plus facile à définir avec un article indéfini surtout au pluriel :
Dans les dénominations utilisant un trait d'union, les noms et/ou adjectifs faisant partie du spécifique, prennent une majuscule :

Utilisés seuls, les différents génériques se voient appliquer la règle de l’antonomase inverse suivant le sens :

Un usage simplifié, recommandé par la division francophone du Groupe d'experts des Nations unies pour les noms géographiques (GENUNG), la diplomatie et le gouvernement français selon l’arrêté français du , l’Académie française, ou l’Office québécois de la langue française, et utilisé par certains groupes de presses comme "Le Monde", préconise de capitaliser le nom propre et de toujours capitaliser le nom générique pour les noms de pays, en particulier les noms officiels :

Dans une dénomination désignant un événement historique, on met une minuscule au nom générique et une majuscule au nom spécifique : 

Lorsqu’il n’y a pas de nom spécifique dans la dénomination historique, le générique prend la majuscule (ainsi que le ou les adjectif(s) qui le précède éventuellement, mais pas le ou les adjectifs qui le suit) : 

Les divisions géologiques (ères, périodes, étages) et archéologiques prennent une capitale :

Par contre, les mouvements littéraires (ou philosophiques) et les courants artistiques prennent la minuscule, car ils ne sont pas considérés comme des événements historiques :

On emploie une majuscule au premier substantif de la dénomination des grandes manifestations d'ordre artistique, commercial, sportif, etc., ainsi qu’à l'adjectif qui le précède, mais pas à celui qui le suit.

L'usage général considère que les gentilés (noms des habitants d’un lieu, d’une région, d’une province, d’un pays, d’un continent, ou une identité nationale ou ethnique) et les membres de dynastie constituent des noms propres, qui prennent une majuscule : 

Les noms de gentilés, membres de dynastie employés comme adjectifs prennent toujours une minuscule. Il en va de même pour les fidèles d'une idéologie, d'une philosophie :

Les mots composés ayant un rapport avec un gentilé, un membre de dynastie, ne sont pas reliés par un trait d'union quand ils sont formés à la fois d'un nom (substantif prenant une majuscule) et d'un adjectif placé après (prenant une minuscule) :

Les mots composés ayant un rapport avec un gentilé sont reliés par un trait d'union quand ils sont formés soit de deux noms ou de deux adjectifs, soit d’un nom ou d’un adjectif précédés d'un nom de point cardinal (nord, sud, est, ouest).

Les points cardinaux, quelle que soit leur place dans le texte, prennent une majuscule :

Cependant, les points cardinaux prennent une minuscule s'ils désignent une direction, une exposition, une orientation, une situation relative :

Selon certaines sources, les points cardinaux prennent une minuscule s'ils sont employés adjectivement (à l'exception des trois cas « pôle Nord », « pôle Sud », « cap Nord » où ils ont fonction de nom propre géographique). 

Selon d'autres sources, la valeur adjectivale ne leur confère pas la majuscule:


Les règles pour les mots "madame", "mademoiselle" et "monsieur" sont complexes. Historiquement, l'usage de la majuscule est destiné à marquer dans le discours direct le nom propre d'une personne, celui-ci pouvant comporter plusieurs mots: un patronyme, un prénom, avec des attributs comme un titre, une qualité, un surnom (Philippe le Hardi, Gatsbi le Magnifique) qui portent alors tous une majuscule. Lorsqu'il n'y a pas d'ambiguïté, le prénom et le patronyme peuvent se trouver sous-entendu pour ne conserver qu'un attribut qui formera à lui seul le nom propre, et conservera sa majuscule. La distinction entre par exemple un titre qui désigne en propre une personne (le Roi, le Comte, le Maréchal, le Président), et le même titre qui la désigne en général (le roi, le comte, le maréchal, le président), est assez subtile à faire.

Jean-Charles de Laveaux indique en 1846 dans son "Dictionnaire raisonné des difficultés grammaticales et littéraires de la langue française" : 

De même pour Caspar Hirzel dans sa "Grammaire pratique française" (1869) : 

À la même époque, Émile Littré, dans son dictionnaire, n'utilise pas de majuscule de déférence. La majuscule n'est imposée que dans l'emploi où "Monsieur" n'est plus un nom commun mais désigne le frère du roi.

Cet usage s'est peu à peu perdu dans le temps, tant du fait de l'expansion éditoriale que de la généralisation des formes abrégées , toujours pourvues d'une majuscule. Ainsi, Grevisse écrit dans "Le Bon Usage" : 

Albert Doppagne reste tout aussi prudent : 

Tout en reconnaissant qu'« il ne s'agit pas toujours d'une règle figée et son usage, comme celui de la langue en général, évolue. Cet usage est même parfois flottant, et les codes typographiques eux-mêmes divergent sur bien des points » ; de nombreux grammairiens préconisent l'usage modéré de la majuscule afin de préserver cette notion de déférence, ce que Doppagne résume ainsi : 

Dans le texte courant, les mots "madame, mademoiselle" et "monsieur" s’abrègent généralement lorsqu'ils sont suivis d'un nom de personne ou de qualité en , et ', et au pluriel en , et '. Il s'agit alors de citer des gens à la troisième personne.

Dans des dialogues, lorsque la personne est appelée à la première personne, les termes "madame, mademoiselle" et "monsieur" ne s'abrègent pas et s'écrivent en minuscule.

Albert Doppagne précise que . Les abréviations « M » et « M » pour "monsieur" et "messieurs", utilisées jusqu'au milieu du , sont généralement considérées aujourd'hui comme fautives.

Lorsqu'ils sont écrits au long (c'est-à-dire en entier), le préconise la majuscule lorsque :

Il préconise la minuscule lorsque :
En revanche, le préconise la majuscule dans ces mêmes cas :

Dans les autres cas, ces mots prennent une minuscule, notamment lorsque :

Les mots caractérisant une fonction ou un titre civil ou administratif prennent généralement une minuscule :

Dans le "Journal officiel" de la République française, les titres uniques prennent la majuscule, mais pas les noms de ministère :

Mais les publications non officielles utilisent souvent une majuscule pour les mots caractérisant la fonction d'un ministre car c'est là sa caractéristique et en quelque sorte son nom propre, elles conservent la minuscule pour le titre lui-même (ministre) :

Quand plusieurs éléments différents ont fusionné en un seul titre, la règle du parallélisme implique que l'on mette alors une majuscule à tous ces éléments (ou aucune) :


Les noms de religions ainsi que leurs membres prennent toujours la minuscule : 

Le titre de fonction des dirigeants et de leurs hiérarchies, lorsqu'il désigne une personne précise en se substituant à son nom propre prend une majuscule, mais utilisé au sens généraliste, le titre de la fonction prend toujours la minuscule :

Si on s'adresse à ces mêmes personnes oralement (transcrit), ou par écrit, le titre de fonction prend une majuscule :

De plus en plus de religieux se font appeler plus simplement Père/Mère ou Mon Père/Ma Mère. Ces formules sont à employer seulement pour les personnes qui vous en ont fait personnellement la demande.

Les noms des textes sacrés prennent une majuscule :

Le mot « église » prend une minuscule pour désigner un bâtiment mais une majuscule pour désigner une institution. Cette règle s’applique au pluriel :
Dans les toponymes (noms de lieu) et les odonymes (voies de circulation), seul le terme spécifique prend la majuscule initiale, le terme générique, pour sa part, conservant la minuscule :

Les noms de fêtes religieuses prennent une majuscule. Si le nom de la fête est suivi d'un adjectif, ce dernier prend une minuscule. Mais s'il est précédé d'un adjectif, ce dernier prend une majuscule :

Les noms de fêtes religieuses composés de deux noms prennent une minuscule au générique et une majuscule au spécifique :

En revanche, les noms des temps liturgiques prennent une minuscule :

Dans les religions monothéistes, le terme dieu est devenu un nom propre (antonomase inverse) puisqu’il ne désigne plus qu’une seule entité unique (ainsi que tous les autres termes qui le désignent) ; il prend donc une majuscule.

Dans le même esprit, pour certains termes désignant une entité ayant un rapport avec Dieu, la règle s'applique :

Quand on parle de la personne, le mot "saint" est un adjectif, qui suit donc les règles pour les adjectifs. Il ne prend pas de majuscule. La même règle est valable pour les dénominations, moins fréquentes, de « vénérable » et « bienheureux ». Par ailleurs, on ne met pas de trait d'union. On peut éventuellement abréger « saint » en « S » (auquel cas le "S" est en majuscule), toujours sans trait d'union :

On écrit toutefois Sainte Vierge. Certains grammairiens comme Adolphe Victor Thomas font aussi une exception de "Saint Louis" (Louis IX), probablement par imitation des autres surnoms de souverain, qui prennent la majuscule : "Philippe le Bel", "Charles le Chauve".

Par contre, dans les noms de lieux, de fêtes (sauf les fêtes fictives qui prennent le trait d'union, mais pas la majuscule), d'églises, d'institutions, il est intégré au nom du saint. Il prend donc une majuscule, et est lié avec un trait d'union à ce nom :

Enfin, en cas d'antonomase, surtout pour les vins ("saint-émilion") et les fromages ("saint-paulin"), ainsi que quelques autres noms ("saint-bernard" [chien], "saint-honoré" [pâtisserie], "saint-pierre" [poisson], etc.), le nom obtenu est un nom commun, et ne doit donc plus prendre de majuscule.

La règle générale dit que, pour un titre d'œuvre ou de périodique, les règles applicables aux noms propres s'appliquent et que les mots autres que les noms propres ne prennent une majuscule que s'ils sont le premier mot du titre. On écrira, par exemple, "Mon oncle", "Une saison en enfer" ou "Voyage au centre de la Terre". Grevisse est à cet égard le plus radical : il indique dans "Le Bon Usage" que « pour éviter l'arbitraire et les discordances, l'usage le plus simple et le plus clair est de mettre la majuscule au premier mot seulement, quel qu'il soit. » (123).

Cependant les conventions d'usage des majuscules pour les titres d'œuvres restent mal établies. Par exemple, les règles typographiques édictées par le sont contredites dans certains cas par l'usage flottant et parfois excessif de la capitalisation parmi les éditeurs. Jacques Leclerc indique à ce sujet que : « sur la couverture d’un livre, par exemple, le graphiste peut décider de n'employer que des bas de casse (minuscules d’imprimerie), même dans les noms propres ; il peut mettre des majuscules à tous les mots ou même utiliser systématiquement les capitales sur toute la page. […] Il ne convient pas, dans un texte, de restituer l’effet visuel, esthétique ou calligraphique, car il faut demeurer fonctionnel et neutre. Pour cette raison, on ne doit jamais se fier à la façon dont on a orthographié ou présenté le titre d'un livre ou d'une revue sur la page de couverture, voire le titre d’un film dans le générique. Il est préférable d'appliquer intégralement les règles de la majuscule, qui régissent l’emploi des titres dans un texte ».

Les règles traditionnelles d’usage des majuscules pour les titres d’œuvres varient selon les cas de figure.

Si le titre forme une phrase, alors seul le premier mot prend une majuscule :

Si le titre est composé seulement d'un adjectif suivi d'un substantif, alors le substantif prend également une majuscule :

Si le titre est composé seulement de deux substantifs successifs, alors chaque substantif prend une majuscule :

Si le titre commence par un article défini "(le, la, les)" et qu'il ne constitue pas une phrase verbale :

Si le titre est constitué de substantifs énumérés ou mis en opposition "(et, ou, ni)", chaque substantif prend une majuscule :
En cas de sous-titre, les principes précédents s'appliquent à chaque partie :
Les titres professionnels (« professeur », « docteur », « avocat », etc.), officiels (« ministre », « député », « président », etc.), religieux (« abbé », « rabbin », etc.) ainsi que les grades militaires (« général », « capitaine », etc.) ou honorifiques (« chevalier », « commandeur », etc.) prennent une minuscule sauf lorsqu'ils sont placés en début de titre.
Quand l'auteur a clairement choisi une typographie originale, il est préférable de la respecter si cette graphie est justifiée. Exemple : "eXistenZ" de David Cronenberg. 

Des règles simplifiées pour les titres d’œuvres, s’appliquant à tous les cas de figure, sont aussi dans l’usage.

La majuscule est limitée au premier mot du titre, quel qu’il soit, ainsi qu’aux noms propres figurant dans ce titre.

Dans une phrase, lorsque le mot commençant le titre est élidé ou supprimé, le premier mot cité prend la majuscule  : « J’ai relu quelques chapitres des "Misérables" ».

Pour Guéry, .
Pour Doppagne, la majuscule au seul premier mot s’applique aussi aux titres en deux parties, donnant comme exemple "Le rouge et le noir".

Les titres de journaux et périodiques font exception à ces règles, correspondant plus ou moins à des noms propres. Pour certains, ceux-ci gardent leur majuscule au substantif : "le Soir", "le Monde", "la Presse", "la Revue musicale". Pour d’autres, ils ont une majuscule au premier mot, au premier substantif et, le cas échéant, à un adjectif précédant ce substantif : "Le Soir", "Le Monde", "La Presse", "La Revue musicale".

Qu'il soit écrit en minuscule ou en capitale, le sigle suit les règles d'usage des majuscules applicables aux noms propres.

À l'inverse des règles typographiques utilisées en anglais, les noms de jours ou de mois ne prennent pas de majuscule en français.








</doc>
<doc id="17714" url="https://fr.wikipedia.org/wiki?curid=17714" title="Instrument à cordes pincées">
Instrument à cordes pincées

Un instrument à cordes pincées est un instrument de musique dont les cordes sont pincées, le plus souvent manuellement (à mains nues ou à l'aide de plectre(s)), ou mécaniquement (clavecin par exemple).

Ceci inclut par exemple :




</doc>
<doc id="17715" url="https://fr.wikipedia.org/wiki?curid=17715" title="Sanguine">
Sanguine

La sanguine désigne une famille de pigments de couleur rouge terre. La sanguine se décline également en orange, ocre, marron, beige.

On trouve des craies, des crayons et des pastels de couleur sanguine. La couleur sanguine est produite historiquement à partir de l'hématite, une roche contenant de l'oxyde de fer.

Par extension, une œuvre (monochrome) exécutée avec de la sanguine porte le nom de sanguine.

On trouve des traces de l'utilisation de la sanguine dès la Renaissance pour la coloration ou pour l'exécution de dessins. L'apogée de son utilisation se situe au , puis la technique de la sanguine connaît un net déclin.

Parmi les peintres utilisant la sanguine, on peut citer Nicolas Poussin, Antoine Watteau, Jean Honoré Fragonard, Jacques-Louis David, Dominique Ingres et Léonard de Vinci (utilisée dans son ).

La sanguine trouve son utilisation naturelle dans la production de croquis, de modèles vivants et de scènes rustiques. Elle est idéale pour le rendu des modelés et des volumes.

La sanguine sous forme de craie s'étale facilement et a une utilisation proche de celle du fusain ou du pastel. Elle nécessite ainsi d'être fixée à la fin de l'exécution de l'œuvre.

Comme pour le pastel, le ton du papier est primordial pour l'exécution d'une sanguine. Ainsi, une technique picturale ayant été mise au point pendant la Renaissance, la technique des trois crayons, consiste à représenter un modèle vivant à l'aide d'une craie sanguine, d'une pierre noire et d'une craie blanche sur un papier teinté (couleur crème par exemple). La combinaison de ces couleurs permet de rendre toutes les nuances carnées du modèle vivant avec le plus grand réalisme.



</doc>
<doc id="17716" url="https://fr.wikipedia.org/wiki?curid=17716" title="Peinture à l'huile">
Peinture à l'huile

La peinture à l'huile est une peinture dont le liant ou véhicule est une huile siccative qui enveloppe complètement les particules de pigment.

On appelle aussi "peinture" les travaux d'enduction d'une surface par ce genre de produit. Les autorités normatives françaises demandent qu'on dise "peinturage", mais ce terme n'a jamais pris. La "peinture à l'huile" est donc aussi l'activité de nombreux artistes peintres passés et présents, ainsi qu'une technique picturale.

Enfin, une "peinture à l'huile" est un tableau peint avec cette technique.

Utilisée autrefois pour tous les revêtements de protection et de décoration qui devaient résister à l'eau, en même temps que pour les beaux-arts, la peinture à l'huile a été remplacée partout au cours de la seconde moitié du , sauf en peinture artistique. Considérée en Occident comme la technique picturale reine, elle a montré une solidité remarquable ; des œuvres exposées depuis cinq cents ans et plus sont encore en bon état. La formulation des peintures à l'huile affecte la facilité d'application et l'aspect de l'ouvrage terminé et s'adapte à l'ouvrage envisagé. Les pigments et les charges déterminent l'opacité ou la transparence de la pellicule de peinture sèche ; les médiums à peindre lui donnent une consistance plus ou moins liquide ou pâteuse, influant sur son état de surface.

La fabrication de la peinture, à partir d'ingrédients achetés chez les apothicaires, est demeurée le domaine des ateliers d'artistes jusqu'à la fin du , pour être progressivement remplacée par des produits industriels . Au milieu du , l'invention du tube de peinture souple a facilité la peinture de plein-air et la peinture d'amateur.

La peinture à l'huile a supplanté la technique de la tempera à la fin du Moyen Âge ; les primitifs flamands en ont généralisé l'usage, qui s'est ensuite répandu en Occident. À partir du , les artistes se répartissent entre deux approches : la superposition de couches transparentes ou glacis, et le travail en pâtes opaques. Au fil des époques, la technique de la peinture à l'huile a connu des changements considérables, liés aux progrès techniques et aux évolutions esthétiques. La peinture d'un seul coup, alla prima, autrefois caractéristique des pochades, a supplanté la lente superposition de couches transparentes.

Bien qu'aujourd'hui, la peinture à l'huile désigne exclusivement celle , autrefois, le mot « huile » désignait aussi bien les huiles « fixes » ou grasses que les huiles dites « essentielles », ou volatiles que nous appelons essences. L'« huile » de la peinture pouvait être toute espèce de liant non aqueux. L'intérêt de l'huile grasse et de diverses résines pour la peinture était connu dès le , mais son emploi malaisé, la consistance de la peinture, la nécessité d'attendre longuement entre chaque couche, s'opposaient à son emploi.

Vasari attribue au peintre flamand Jan van Eyck (1390-1441) l'invention de la peinture à l’huile, suivi par de nombreux auteurs. Il est certain que le procédé existait avant lui. Le moine Theophilus Presbyter mentionne ses difficultés au . Van Eyck l'a perfectionné associant des résines transparentes, durables et souples, à l'huile, entraînant son adoption générale. Ce procédé à permis d'utiliser l'huile pour des commandes pérennes, et plus tard de peindre sur châssis et non plus sur panneau. Il est vraisemblable que le modèle oriental des laques chinoises ait influencé les artistes européens

L'avènement de la peinture à l'huile en Occident a été progressif. Les peintres du Moyen Âge utilisèrent la tempera qu'ils recouvraient parfois d'une couche huileuse protectrice. Au fil des générations, cette couche d'huile s'est progressivement chargée en pigment donnant ce que l'on peut qualifier de premier glacis. On retrouve d'ailleurs dans les tableaux des frères Van Eyck, sous d'innombrables couches de glacis cette sous-couche "a tempera". Les panneaux destinés à être peints étaient imprégnés de plusieurs couches de colle et d’enduit, lorsque le bois était imparfait, ce qui était souvent le cas dans les pays du sud (Italie, Espagne) ; ils étaient préalablement marouflés d’une fine toile afin de limiter les effets de dilatation ou de rétraction du bois.

Le passage de la tempera à l'huile voit aussi le passage du bois à la toile. Le bois avait pour inconvénient de limiter les dimensions des tableaux, d'une part à cause de la grandeur maximale qui pouvait être atteinte avec des planches, d'autre part par le poids des œuvres.

La toile montée sur châssis fait son apparition à partir du . On en trouve les premières utilisations sur des volets d'orgue à Venise. Ce sont d'ailleurs les Vénitiens qui diffuseront cette pratique en Italie dans le courant du et en Flandres par Rubens. La toile, généralement de lin, doit être recouverte d’une couche d’enduit qui permet à la peinture de s’accrocher. La peinture à l'huile qui produit un film souple convient parfaitement à ce support souple, qu'il est alors possible de rouler pour son transport.

Jusqu'au , les peintres prépareront leurs couleurs dans leurs propres ateliers. Les procédés sont issus de l'expérience, relèvent d'un savoir transmis par l'apprentissage dans l'atelier d'un maître, et varient de l'un à l'autre, par le choix des huiles, essences et résines, et les méthodes de préparation et d'application. La formulation varie selon l'usage qu'on en fait. Le précepte "gras sur maigre" en témoigne : les fonds se peignent avec des mélanges plus riches en résine (maigres), le dessus et notamment les glacis peuvent incorporer plus d'huiles (au sens moderne du terme, grasses). Van Eyck superpose de fines couches transparentes sur un fond clair, et utilise plus de résines que d'huile grasse. Rubens emploie du blanc, opaque, pour les lumières. Léonard de Vinci utilise moins de résines, et peint en couches très fines de pâte lisse et opaque. Titien abandonne les résines, sauf pour les glacis, peint opaque, revient sur ses peintures après des mois de séchage. Watteau peint plus rapidement, avec beaucoup de siccatifs, d'où un noircissement progressif des tableaux. Cette tendance s'amplifie au . .

Les couleurs à l'huile sont composées de pigments qui forment la matière colorée et d'un liant composé d'huile de lin purifiée ou d'œillette qui les lie et les agglomère. Le diluant ou solvant de la peinture à l'huile est l'essence de térébenthine ou l'essence de pétrole (ou des équivalents modernes non allergènes).

L'huile utilisée est généralement l'huile de lin ou l'huile d'œillette, voire l'huile de carthame ou de l’huile de noix. Ce que l'on appelle « séchage » est en réalité un phénomène de siccativation ou oxydation de l'huile, qui se polymérise et durcit, sans changer l'aspect de l'œuvre, et en quelque sorte, emprisonne les pigments et permet la conservation de la peinture.

La peinture à l'huile est une technique lente à sécher (on dit siccativer), par opposition à la peinture acrylique ou à l'aquarelle, qui sont des techniques aqueuses. Cette particularité permet à l'artiste de prendre le temps de mélanger ses couleurs, de récupérer une erreur et de retravailler son motif pendant plusieurs jours jusqu'à obtenir le fondu, le modelé de la forme, la touche qu'il désire.

Ces huiles naturellement siccatives pouvaient être additionnées de substances (toxiques comme la céruse) visant à accélérer le séchage. Les frères van Eyck créèrent la technique associant des résines transparentes, durables et souples, à l'huile. 

Il est également possible d'obtenir des effets de matière ou de reliefs avec une pâte assez consistante. L'utilisation d'une spatule appelée aussi couteau permet d'obtenir du relief et d'augmenter ainsi la matière de l'œuvre.

On peut améliorer la consistance de la pâte par l'ajout de médiums à peindre, eux-mêmes fabriqués à partir du liant (huile) et de solvant (essence) auxquels on rajoute, éventuellement, pour améliorer la souplesse du film, une résine. Le médium rend la matière plus malléable et donc plus facile à étaler. La présence de résines ne change pas la dénomination courante de "peinture à l'huile".

Les médiums à peindre permettent aussi de respecter la règle du « gras sur maigre » (propre à la peinture à l'huile) qui veut que chaque couche de couleur soit plus grasse que la précédente afin que l'accroche soit solide et durable. L'explication en est très simple : les couches maigres, qui mettent peu de temps à sécher, entreraient en conflit avec les précédentes plus grasses et toujours en train de sécher, provoquant un phénomène variant entre la peau d'orange et celle du reptile au cours de la mue... À éviter, selon les traditions. Dans les premières étapes, la pâte sera donc maigre, par adjonction d'essence et progressivement deviendra plus grasse, par ajout d'huile ou de médium. L'œuvre sera finalement vernie grâce à un vernis à retoucher puis un vernis définitif.

L'utilisation de résines (gomme dammar, gomme mastic, térébenthine et térébenthine de Venise...) rend la peinture durable. Sans ces médiums la couche picturale se dégrade rapidement.

En peinture à l'huile, on appelle "broyage" le mélange du pigment en poudre avec le liant. Broyer un matériau, c'est le réduire en poudre ; autrefois, cette opération se faisait sur une pierre polie, avec adjonction d'un liquide pour maintenir la poussière de pigment. Pour la peinture à l'huile, on broyait généralement à l'eau, puis on laissait sécher les pains de poudre pigmentaire, avant de les broyer à nouveau, avec de l'huile ; certains pigments comme le massicot ou le minium devaient se broyer directement à l'huile pour ne pas perdre leur couleur.

Jusqu'au , les peintres, ou leurs élèves, broyaient eux-mêmes les pigments en poudre avec le liant et ils les employaient rapidement Chacun développait sa technique, à base de différentes huiles, plus ou moins jaunissantes, utilisées crues ou cuites. Ainsi l'huile de lin, siccative et peu jaunissante, fut adoptée devant l'huile d'œillette et l'huile de noix, plus claires, mais moins siccatives.

Au sont apparues les premières couleurs industrielles, présentées dans des vessies de porc puis dans des tubes de peinture souple à partir de 1841. Aujourd'hui, la fabrication des couleurs à l'huile est principalement industrielle (Lefranc et Bourgeois, Sennelier, Winsor et Newton, Talens). Quelques fabricants ont gardé ou repris des manières traditionnelles afin de produire des couleurs plus proches de celles d'autrefois (Leroux, Blockx, Old Holland, Isaro, Michael Harding).

Les contraintes de stockage ont cependant eu une influence regrettable : les peintres de la Renaissance faisaient cuire des huiles, qui étaient dés le départ faites de graines torréfiées. Les peintures à l'huile prêtes à l'emploi modernes contiennent une huile crue, à base de graines étuvées, peu adhésive et peu siccative, qui permettent une plus longue conservation.

La technique est restée longtemps immuable : le peintre dessinait sa composition sur la toile ou sur le panneau de cuivre ou de bois préparé puis, après une éventuelle grisaille, montait son sujet avec les couleurs à l'huile, en couches minces, en donnant l'effet de lumière par le jeu des ombres et des reflets. Puis, une fois ces premières couches bien sèches, il les recouvrait de glacis teintés, transparents, qui harmonisaient la coloration générale. Le tout formait une surface bien unie, comme une toile cirée.

La technique a ensuite évolué, dès la fin de la Renaissance, les peintres commençant à expérimenter la pâte afin d'accentuer les lumières en leur donnant par exemple plus d'épaisseur. Ce procédé devint général et de nouvelles techniques sont nées : peinture en pleine pâte, à la touche, par touches séparées, avec ou sans ébauche préparatoire. Les peintres baroques (Rubens, Van Dyck) puis rococo (Boucher, Fragonard) et les Romantiques (Delacroix, Géricault) ont su exploiter avec brio cette écriture enlevée qui s'oppose à une manière plus lisse et « léchée » de traiter le sujet (peinture néo-classique, style pompier, surréalistes). La peinture à l'huile a la particularité de permettre les deux approches, entre autres.

Les Impressionnistes, et la peinture moderne par la suite, abandonne le traditionnel procédé par couches superposées pour une technique plus spontanée et directe, considérée autrefois comme pochade, dite "alla prima" - autrement dit, peindre en une seule séance, sans séchage entre les couches. Ces courants n'ont cessé d'explorer les limites de la peinture à l'huile.

Né des ateliers classiques et des grands formats, le métier traditionnel du peintre fut et demeure la base référentielle de la technique de l'huile. Les couches picturales du tableau sont superposées selon le principe du et exploitent les transparences de certains pigments, alliées à celle des médiums. On les appelle « jus » (très peu de pigment et beaucoup de diluant), « glacis » (très peu de pigment et beaucoup de médium corsé en résine), « vélatures » (très peu de pigment, beaucoup de médium corsé en résine et un petit peu de blanc). Par opposition à « pâte », « matière », « charge ».

La peinture en plein air date du début du ; les peintres de l'école de Barbizon adoptent cette pratique sous l'influence des aquarellistes anglais comme Constable et Bonington. Elle se généralise avec l'invention du tube de peinture. Dès le début du apparurent des vessies de porc destinées à contenir et à conserver les couleurs à l'huile. Les tubes d'étain ont été inventés en 1841. Ces nouveaux récipients étaient beaucoup plus pratiques que les vessies de porc et permettaient de conserver les couleurs intactes plus longtemps. Les tubes d'étain ont permis aux peintres impressionnistes de sortir de leur atelier pour aller peindre des paysages « sur le motif », c'est-à-dire dans la nature. La conséquence directe sur la révolution impressionniste est cependant à nuancer : le brevet d'invention date de 1841 tandis que la première exposition impressionniste date de 1874.

La peinture à l'huile a une action corrosive ou interagit chimiquement avec la plupart des supports (toile, bois, papier). C'est pourquoi une préparation de la surface est indispensable avant de peindre.
Il existe deux grands types de préparation pour la peinture à l'huile :

Aujourd'hui, les supports du commerce sont déjà enduits (préparation universelle ou synthétique) : on peut donc peindre directement.




</doc>
<doc id="17717" url="https://fr.wikipedia.org/wiki?curid=17717" title="Fusain">
Fusain

Le fusain est une branche de saule ou de fusain d'Europe carbonisée en vase clos.

Bien que concurrencé par des produits plus élaborés (craies et mines de différents types), le fusain reste un outil de dessin irremplaçable.

Si beaucoup d'artistes depuis la Renaissance ont utilisé le fusain (Léonard de Vinci, Verrocchio, Dürer, Pontormo), peu d'œuvres ont été conservées parmi lesquelles celles de Carrache, Baroche, Reni ou Dominiquin. Le mot "fusain" ou "fusin", comme instrument de dessin, est attesté en français depuis 1704. Les artistes le désignaient aussi sous le nom de "charbon de Garais".

Cependant, pour , . Cet usage n'est pas sans rapport avec le goût de l'époque pour le rendu des lumières. Plus que les crayons, la pierre noire, la sanguine, en effet, le fusain se prète aux aplats et au rendu du modelé ().

Classiques (Prud'hon) et Romantiques (Delacroix, Goya) s'en servirent comme instrument de dessin.

Les post-impressionnistes en firent un usage plus approfondi, tels Degas, Redon et surtout Seurat.
Ce dernier réalisa de nombreuses études préparatoires à ses œuvres pointillistes et (et c’est la majorité) de dessins indépendants (série des 'Noirs') au fusain qui lui permettaient de travailler la composition par plans de valeurs, recherchant les volumes sans avoir recours à la ligne et analysant les jeux d’ombres et de lumières au seul moyen des gris.

Auguste Allongé fut un des maîtres du fusain au . Il enseigna le dessin au fusain et publia en 1873 un traité sur cet art qui fut traduit en plusieurs langues.

Le fusain est depuis le l'outil de dessin le plus simple et le plus utilisé dans le dessin d'art, les études, les esquisses, car il est bon marché et permet d'obtenir des noirs très profonds, des tracés précis, fins ou au contraire très larges, selon la façon dont il est utilisé.

Les peintres esquissent au fusain sur la toile destinée à recevoir la peinture. L'excès de poudre s'enlève d'un coup de chiffon pour laisser un dessin léger dont la trace disparaitra sous la couleur.

Les traces laissées par le fusain naturel peuvent être enlevées ou atténuées avec une gomme mie de pain, gomme la mieux adaptée, puisqu'elle n'étale pas le carbone très poudreux.

La marque du fusain sur le support est fugace, ce qui a l'avantage de permettre repentirs et corrections, mais oblige, pour conserver un dessin au fusain, à utiliser un fixatif, afin d'éviter que le carbone ne se décolle, lorsqu'un quelconque objet est frotté sur le support. Autrefois, on fixait le travail en imprégnant le papier par derrière avec un vernis fluide. Il existe aujourd'hui des produits en bombe ou en flacons (à utiliser avec un petit vaporisateur à bouche). On peut par économie utiliser de la laque à cheveux mais, n'étant pas destinée à cet usage, elle peut entraîner des désagréments comme, à terme, le jaunissement de la feuille.

S'il n'est pas encadré, le dessin au fusain sur papier sera conservé entre deux feuilles de papier cristal.

Le bois le plus utilisé aujourd'hui est le saule car il permet une grande variété de diamètres, une homogénéité de tendreté et une bonne densité de noirs.
D'autres arbres peuvent servir à leur fabrication : fusain d'Europe bien sûr, bouleau, épicéa (en Finlande), tilleul mais aussi noyer, figuier, prunier, myrte (en Grèce) ou romarin (en Italie) et buis.
Des imitations (chinoises notamment) de fusain proviennent d'arbres divers : les branches plus épaisses sont coupées dans leur longueur pour imiter la taille de fusains. On reconnait un fusain naturel à l'anneau entourant son rond central (marque de son âge : un an).

Selon la partie de la branche dans laquelle il a été découpé, les bâtonnets peuvent être de différentes grosseurs/diamètres : fin ou mignonette (2-3 mm), moyen ou petit buisson (4-6 mm), gros ou moyen buisson (7-9 mm), très gros ou gros buisson (12-14 mm) jusqu'à géant pour la scénographie (16-24 mm).

Le fusain peut être plus ou moins tendre. Comme pour le crayon mine, plus il sera sec et moins il marquera le support, et à l'opposé, plus il sera tendre, plus il le noircira.

Il existe également du fusain compressé ou comprimé : plus dur, il se compose de poudre de fusain mélangée à un liant. Il est aussi plus difficile à effacer.
Le mot « fusain » (), dont une variante passée est « fusin » (), est tiré du latin "fusago", "fusaginis".

Dans le projet de calendrier républicain, le mot Fusain correspondait au du mois de floréal.


</doc>
<doc id="17718" url="https://fr.wikipedia.org/wiki?curid=17718" title="Peinture acrylique">
Peinture acrylique

La peinture acrylique est un type de peinture utilisant des pigments mélangés à des résines synthétiques et les technique picturales correspondantes. 

Les premières peintures synthétiques sont des nitro-celluloses à l'huile qui apparaissent au milieu des années 1930 à New York pour l'industrie automobile et le bâtiment (peintures utilisées par les peintres mexicains et Charles Pollock). Le second type de peinture, qui apparaît en 1927, comporte de l'alkyde pour le bâtiment (utilisée par De Kooning en particulier). Les polyvinyle-acétate seront introduits également dans les années 1930. 

La première marque commerciale Magma est développée par les imprimeurs américains Leonor Colour et Sam Golden, et mise sur le marché en 1949. Cette peinture se dilue avec de l'essence de térébenthine, elle est utilisée par les peintres Rothko, de Kooning, Barnett Newman, Kenneth Noland ou Morris Louis.

La peinture acrylique pour artiste diluable à l'eau est créée en 1963 par la marque Liquitex du chimiste Henry Levinson. Elle est immédiatement utilisée par les peintres Andy Warhol, David Hockney.

Au Mexique elle avait été inventée vers 1950. Des chimistes de l'Institut National Polytechnique de Mexico, en collaboration avec les maîtres peintres muralistes mexicains, l'ont mise au point lors de la réalisation des fresques sur les façades de l'université de Mexico. Les écrits de David Alfaro Siqueiros (peintre-muraliste mexicain) : « L'art et la Révolution », racontent dans le détail la mise au point de cette technique picturale mise sur le marché en 1950. 

Elle n'apparaît pas en Europe avant les années 1960 : Pierre Alechinsky la découvre à New York en 1965.

La peinture acrylique est constituée de deux éléments : 



À cette pâte pourra ensuite être ajoutée une charge afin d'en augmenter le volume.

La principale qualité de la peinture acrylique est sa docilité : dilution à l'eau (sans excès), miscibilité, mélanges faciles à préparer, facilité d'application, polyvalence de supports, faible odeur. Elle est très solide et indélébile.

Elle a la particularité de sécher très vite, en quelques minutes. C'est un avantage lorsqu'il s'agit de travailler plus rapidement les différentes couches, mais peut constituer un inconvénient en empêchant les retouches. Aujourd'hui, on peut trouver des acryliques à séchage ralenti (d'une heure à plusieurs jours). 

Elle se différencie ainsi de la peinture à l'huile, très lente à sécher mais qui permet les fondus et les repentirs. 

La véritable limitation de l'acrylique est face à un corps gras. Ainsi on ne peut pas la mélanger ou la diluer avec de l'huile, ni de l'essence. Toutefois, selon la règle du « gras sur maigre », il est possible de peindre à l'huile sur une couche d'acrylique. On peut ainsi commencer un tableau à l'acrylique et le continuer à l'huile (mais non l'inverse). 

Certaines marques spécialisées dans le maquettisme utilisent des médiums à base d'alcool isopropyle (isopropanol) pour une utilisation avec pistolet ou aérographe. 

Une peinture acrylique, une fois sèche, macule irrémédiablement un support. On ne pourra la nettoyer qu'avec des solvants puissants. Si elle est encore fraîche, il est assez simple de faire partir immédiatement l'acrylique de tissus avec de l'eau chaude et des savons végétaux de type savon de Marseille ou d'Alep.

L'acétone permet de nettoyer les ustensiles de peinture laissés même plusieurs jours en l'état, précaution prise avec certaines matières plastiques solubles à son contact.

Beaucoup d'effets sont possibles avec la peinture acrylique. D'où un grand éventail de médiums proposés par les fabricants. 







</doc>
<doc id="17719" url="https://fr.wikipedia.org/wiki?curid=17719" title="Communauté d'agglomération">
Communauté d'agglomération

Une communauté d'agglomération est un établissement public de coopération intercommunale (EPCI) français à fiscalité propre, qui prévoit une importante intégration des communes membres.

Elle est définie comme étant :
Par la population comme par le degré de coopération, elle se trouve à un niveau intermédiaire entre la communauté de communes et la communauté urbaine.

Si les syndicats de communes existent depuis 1890 et les syndicats intercommunaux à vocation multiple (SIVOM) depuis le , il faut attendre 1992 pour qu'une nouvelle conception de l'intercommunalité fasse place à la liberté de négociation contractuelle et à la libre association de communes. La loi du crée deux nouvelles catégories d'EPCI à fiscalité propre : les « communautés de communes » et les « communautés de villes ». 

La loi relative au renforcement et à la simplification de la coopération intercommunale du , dite « loi Chevènement ».contribue à accélérer la création de nouvelles structures. Elle supprime les districts et les communautés de villes. Ces dernières n'avaient pas rencontré le succès escompté : cinq communautés de villes seulement avaient été créées depuis 1992. Elle crée une nouvelle catégorie d'EPCI à fiscalité propre, les communautés d'agglomération, réservée aux groupements de plus de . Elle recentre les communautés urbaines sur les ensembles de population les plus importants : au lieu de . Enfin, elle élargit les compétences des communautés de communes.

Leur régime est modifié, comme pour toutes les communautés, par la "loi du 16 décembre 2010 de réforme des collectivités territoriales", qui prévoit l'élection directe des conseillers communautaires des communes de plus de à partir des élections municipales de 2014. La loi est également assoupli les conditions démographiques de création des communautés d'agglomération, en les réduisant notamment à lorsqu'elles comprennent le chef-lieu de département.

L'évolution du nombre de communautés d'agglomération depuis 2002 est la suivante :

Les communes concernées et le préfet de département peuvent créer une communauté d'agglomération si le critère de continuité territoriale est respecté (une zone géographiquement d'un seul tenant et sans enclave) et si l'ensemble territorial répond à l'une des configurations suivante : 


L'article du code général des collectivités territoriales précise que ces conditions ne sont pas exigées si les communautés d'agglomération sont issues de la transformation d'un EPCI à fiscalité propre existant à la date de publication de la loi (district, communauté de communes ou communauté de villes). Par exemple, la création de l'une des toutes premières, la communauté d'agglomération du pays de Flers (janvier 1994), ne satisfaisait pas à la première condition car elle comportait moins de avant 2017. 

La communauté d'agglomération est gérée par un "conseil communautaire" ou "conseil de communauté", composé de conseillers municipaux des communes membres.

Jusqu'aux élections municipales de 2014, les conseillers communautaires étaient des conseillers municipaux élus par chaque conseil municipal des communes membres de la Communauté. Ce système était critiqué, étant donnée l'importance des compétences transférées, et l'absence de débat sur ces politiques en raison de l'élection des conseillers communautaires au suffrage indirect. C'est ainsi qu'à l'unanimité, les présidents des communautés se sont prononcés lors des journées communautaires de Strasbourg en 2007 pour l'élection au suffrage universel direct dès 2014, et ce pour renforcer la légitimité des communautés et leur transparence de fonctionnement.

La loi du 16 décembre 2010 de réforme des collectivités territoriales a prévu que les conseillers communautaires des communes de plus de seront élus au suffrage universel direct, dans le cadre des élections municipales. Les représentants des communes de plus petite taille resteront élus en leur seins par les conseils municipaux. Ces dispositions ont été modifiées par la loi du 17 mai 2013, qui a défini le régime suivant :

À compter des élections municipales de 2014, chaque commune est représentée au conseil communautaire par un nombre de représentants tenant compte de sa population défini aux articles L. 5211-6-1 et L. 5211-6-2 du code général des collectivités territoriales :

L'article L. 5216-5 du Code général des collectivités territoriales impose aux communautés d'agglomération l'exercice de certaines compétences : 

La communauté doit par ailleurs exercer au moins trois des six compétences suivantes :

Elle peut se donner compétence en matière de droit de préemption urbain ou recevoir délégation du département pour exercer des fonctions d'aide sociale.

Les communes peuvent, par ailleurs, déléguer à la communauté d'autres compétences.

L'exercice de certaines compétences nécessite que soient définies les actions et équipements « reconnus d'intérêt communautaire ». cette déclaration d'intérêt communautaire est faite par une délibération du conseil communautaire prise à la majorité des deux tiers du conseil de la communauté d'agglomération.

C'est une différence importante par rapport aux communautés de communes, où la déclaration d'intérêt communautaire résulte du vote d'une majorité qualifiée des conseils municipaux, qui donne ainsi plus de pouvoir aux communes.

Chaque intercommunalité exerce des compétences en nombre inégal et de niveaux inégaux. Là où le SIVU ne gère qu'une seule et unique compétence, la communauté d'agglomération peut dépasser la trentaine. De façon plus générale, elles se distinguent sous les appellations intercommunalité de service (compétence unique et technique) et intercommunalité de projet (compétences multiples au service d'un projet de territoire).

Les recettes des communautés d'agglomération sont :

Il est à noter que, de 1999 à 2009, la ressource principale des communautés d'agglomération fut la taxe professionnelle, dont le taux devait devenir unique sur son territoire, après une période transitoire — dite de « lissage » — de quelques années. Depuis la mise en place de la contribution économique territoriale en 2011 (2010 étant une année transitoire à régime spécial), les communautés d'agglomération perçoivent une partie de la cotisation foncière des entreprises et de la cotisation sur la valeur ajoutée des entreprises. 

La communauté d'agglomération offre une nouvelle conception du pouvoir local, en intégrant l'idée de projet, là où il n'y avait que de la gestion. En effet, les SIVU ou les SIVOM n'ont d'autres vocations que de gérer des équipements ou infrastructures, souvent de réseau, tels le gaz, l'électricité, l'eau ou les déchets. Cependant, il est fréquent pour une commune de rester membre d'un ou deux SIVU, d'un SIVOM ou d'un syndicat mixte et d'une communauté d'agglomération. Une commune adhère en général à plusieurs structures intercommunales, mais ne peut appartenir qu'à un seul EPCI à fiscalité propre. Si la communauté d'agglomération acquiert une compétence gérée par une autre intercommunalité, celle-ci est dissoute si elle ne gérait que cette compétence (SIVU), ou est retirée des compétences de ladite intercommunalité, au titre du principe de spécialité et d'exclusivité des EPCI à fiscalité propre.

La communauté d'agglomération, avec sa fiscalité propre à TPU a évidemment des compétences de gestion, mais également d'élaboration, de création, bref, de projet. Cet état de fait est encore plus valable pour les communautés urbaines, mais moins développé au sein des communautés de communes. L'intercommunalité a donc évolué, puisque le projet, à l'exception de l'ancien district, ancêtre des communautés d'agglomération, n'a jamais été une vocation intercommunale, et ce depuis les premières formes en 1837 et les commissions syndicales de gestion pour les biens indivis entre communes.





</doc>
<doc id="17721" url="https://fr.wikipedia.org/wiki?curid=17721" title="Communauté urbaine">
Communauté urbaine

Une communauté urbaine est un établissement public de coopération intercommunale (EPCI) français à fiscalité propre, qui prévoit une importante intégration des communes membres, bien davantage que les communautés de communes ou les communautés d'agglomération.

Les communautés urbaines étaient, jusqu'à la "loi du 16 décembre 2010 de réforme des collectivités territoriales", la forme la plus intégrée des intercommunalités françaises. Ce n'est plus le cas avec la création, par cette loi, des métropoles, qui reçoivent des compétences déléguées par les communes, mais également par le ou les départements et régions où elles sont situées.

La loi de modernisation de l’action publique territoriale et d’affirmation des métropoles, votée en 2013, abaisse le seuil démographique de création des communautés urbaines de à .

Les premières communautés urbaines ont été créées par la loi 66-1069 du 31 décembre 1966, pour plusieurs villes (Bordeaux, Lille, Lyon et Strasbourg). À l'époque, l'objectif était de remédier au décalage entre les structures administratives existantes et la réalité géographique de ces agglomérations. Ces premières communautés ont été imposées.

Dunkerque a ensuite innové : en effet, la communauté urbaine de Dunkerque est créée en 1968, sur le modèle lillois, à l'initiative des élus. Ont suivi d'autres créations avec, en 1970, la communauté urbaine Creusot-Montceau et la communauté urbaine de Cherbourg, puis celle du Mans en 1972 et celle de Brest en 1973.

La loi 95-1350 du 30 décembre 1995 a permis de transformer les districts urbains en communautés urbaines, ce qui a été le cas de la communauté urbaine du Grand Nancy en 1996, de celle d'Alençon en 1997 et de la communauté urbaine d'Arras en 1998.

Les communautés urbaines restent régies, pour l'essentiel, par les dispositions de la Loi relative au renforcement et à la simplification de la coopération intercommunale (dite loi Chevènement) du 12 juillet 1999, qui réservait toutefois ces structures aux territoires de plus de .

En 2000, la communauté urbaine Marseille Provence Métropole a remplacé une communauté de communes. À Nantes, la communauté urbaine Nantes Métropole a été créée en 2001, remplaçant un district urbain. Les communautés urbaines les plus récentes datent de 2008 : communauté urbaine Nice Côte d'Azur et communauté urbaine du Grand Toulouse, auparavant communautés d'agglomération.

Depuis la "loi du 16 décembre 2010 de réforme des collectivités territoriales", la communauté urbaine est définie comme étant :

La "loi du 16 décembre 2010 de réforme des collectivités territoriales", qui crée les métropoles, et leur fixe une taille minimale de , a corrélativement abaissé à le seuil de création des communautés urbaines, tout en prévoyant que celles créées antérieurement conserveraient leur statut, même si elles ne respectaient pas ce seuil.

La communauté urbaine Nice Côte d'Azur, créée le 29 décembre 2008 et qui regroupait 27 communes, en fusionnant avec les communautés de communes de La Tinée, des stations du Mercantour et de Vésubie-Mercantour ainsi qu'avec la commune de La Tour, s'est transformée le 31 décembre 2011 en Métropole, sous le nom de Métropole Nice Côte d'Azur.

La loi de modernisation de l’action publique territoriale et d’affirmation des métropoles du 27 janvier 2014 abaisse le seuil démographique de création des communautés urbaines de habitants à habitants.

Les communautés urbaines de Bordeaux, Lille, Nantes, Strasbourg et Toulouse ont accédé au au statut de métropoles, du fait de l'automaticité de la création de ces dernières depuis le vote de la loi du 27 janvier 2014. Cette loi a ouvert également cette possibilité, de façon volontaire, à la communauté urbaine de Brest, qui est effectivement devenue une métropole à cette date. Le Grand Lyon est devenu à cette même date une métropole à statut particulier, la métropole de Lyon. Les EPCI de l'agglomération d'Aix-Marseille, dont la communauté urbaine de Marseille, ont été intégrées à une métropole à statut particulier, la métropole d'Aix-Marseille-Provence, le .

À la suite de l'adoption de la loi du 28 février 2017 qui assouplit les conditions de création des métropoles, les communautés urbaines de Tours, Orléans, Clermont-Ferrand, Saint-Étienne et Dijon sont devenues des métropoles.

La communauté urbaine est gérée par un "conseil communautaire" ou "conseil de communauté", composé de conseillers municipaux des communes membres.

Jusqu'aux élections municipales de 2014, les conseillers communautaires étaient des conseillers municipaux élus par chaque conseil municipal des communes membres de la Communauté. Ce système était critiqué, étant donnée l'importance des compétences transférées, et l'absence de débat sur ces politiques en raison de l'élection des conseillers communautaires au suffrage indirect. C'est ainsi qu'à l'unanimité, les présidents des communautés se sont prononcés lors des journées communautaires de Strasbourg en 2007 pour l'élection au suffrage universel direct dès 2014, et ce pour renforcer la légitimité des communautés et leur transparence de fonctionnement.

La loi du 16 décembre 2010 de réforme des collectivités territoriales a prévu que les conseillers communautaires des communes de plus de seront élus au suffrage universel direct, dans le cadre des élections municipales. Les représentants des communes de plus petite taille resteront élus en leur seins par les conseils municipaux. Ces dispositions ont été modifiées par la loi du 17 mai 2013, qui a défini le régime suivant :

À compter des élections municipales de 2014, chaque commune est représentée au conseil communautaire par un nombre de représentants tenant compte de sa population défini aux articles article L. 5211-6-2 code général des collectivités territoriales :

La communauté urbaine exerce de plein droit, au lieu et place des communes membres, les compétences suivantes :

Le conseil de la communauté urbaine est consulté lors de l’élaboration, de la révision et de la modification des schémas et documents de planification en matière d’aménagement, de développement économique et d’innovation, d’enseignement supérieur et de recherche, de transports et d’environnement.

Les communautés urbaines créées avant la loi Chevènement restent soumises à l'ancien régime, moins étendu.

La communauté urbaine peut recevoir d'autres compétences de la part des communes si celles-ci le souhaitent. Elle peut gérer tout ou partie de l'aide sociale, en cas d'accord avec le département.

Les recettes des communautés urbaines sont :

Il est à noter que, de 1999 à 2009, la ressource principale des communautés d'agglomération fut la taxe professionnelle unique (TPU) dont la perception était transférée des communes à la communauté. Son taux devait devenir unique sur son territoire, après une période transitoire - dite de « lissage » - de quelques années. Depuis la mise en place de la contribution économique territoriale en 2011 (2010 étant une année transitoire à régime spécial), les communautés urbaines perçoivent une partie de la cotisation foncière des entreprises et de la Cotisation sur la valeur ajoutée des entreprises.

La loi abaissant le seuil de création des communautés urbaines de habitants à habitants, huit communautés d'agglomération remplissent les critères pour pouvoir devenir des communautés urbaines.

Sous certaines conditions, les EPCI comprenant une commune ayant perdu la qualité de chef‑lieu de région (Amiens, Besançon, Châlons-en-Champagne, Limoges) peuvent déroger au seuil des habitants. Poitiers est ainsi devenu une communauté urbaine, tandis que Metz et Montpellier sont même devenus des métropoles..

À la suite de l'adoption de la loi du 28 février 2017 qui assouplit les conditions de création des métropoles, les communautés d'agglomération de Toulon et Metz sont devenues des métropoles le janvier 2018.

L'Association des communautés urbaines de France regroupait en 2015 les vingt-et-une communautés urbaines et métropoles. Depuis le 20 mai 2014 et pour un mandat de trois ans renouvelable, son président était Gérard Collomb, président du Grand Lyon. Elle fusionne en novembre 2015 avec l'Association des maires de grandes villes de France au sein de France urbaine, dirigée par Jean-Luc Moudenc, maire de Toulouse et président de Toulouse Métropole. 4 communautés urbaines sont également membres de l'Assemblée des communautés de France, avec d'autres intercommunalités.

Si elle recouvre des réalités institutionnelles différentes, l'appellation « communauté urbaine » possède des équivalents en Europe et dans le monde. L'intitulé a été inspiré en général par l'exemple français auquel les structures sont postérieures.

Les plus anciennes, les communautés urbaines québécoises, ont disparu à la suite des fusions municipales de 2002 comme la communauté urbaine de Montréal (CUM) (ou "Montreal Urban Community" en anglais), créée en 1970, qui a été remplacée par un « nouveau Montréal » composé d'anciennes municipalités fusionnées, et étendu à la périphérie.

Le continent africain a, de par son histoire, fréquemment suivi la réalité institutionnelle française. De nombreuses métropoles y sont organisées en communautés urbaines. Le président de Bamako est le « maire central » de la capitale malienne. Yaoundé, Douala et Ngaoundéré, métropoles camerounaises sont aussi organisées en communautés urbaines créées par la loi en 1987. Il en est de même pour Niamey (Niger), Tananarive (Madagascar) et Abidjan (Côte d'Ivoire). Quant à capitale sénégalaise, précédemment communauté urbaine, elle a connu des modifications institutionnelles faisant d'elle la communauté d'agglomération de Dakar (CADAK). Fès, Casablanca, les grandes métropoles marocaines sont aussi organisées en communautés urbaines.

En Belgique francophone, ce type d'organisation date des années 1990 avec quatre communautés urbaines en Région wallonne : régions de Charleroi, Liège, le Centre et Mons-Borinage. Sous statut associatif, les missions sont ponctuelles.




</doc>
<doc id="17722" url="https://fr.wikipedia.org/wiki?curid=17722" title="Bizanos">
Bizanos

Bizanos (en béarnais "Visanòs" (graphie classique), "Bisanos" (graphie fébusienne)) est une commune française, située dans le département des Pyrénées-Atlantiques en région Nouvelle-Aquitaine.

Le gentilé est "Bizanosien".

Commune de l'aire urbaine de Pau située dans son unité urbaine au sud-est de Pau.

La commune est desservie par les routes départementales 100, 937 et 938.

La ligne 804 du réseau interurbain des Pyrénées-Atlantiques, qui part de Pau et mène jusqu'à Asson, s'arrête à Bizanos.

Le réseau de bus Idelis possède les arrêts suivants :

Situées dans le bassin versant de l’Adour, les terres de la commune sont traversées par le gave de Pau, affluent de l'Adour et par ses tributaires, l'Ousse (rejoint sur la commune par le ruisseau Merdé), le canal des Moulins et le Lagoin.


Le toponyme "Bizanos" apparaît sous les formes 
"Bisanos" (, fors de Béarn), 
"Bisanoss" (1270, cartulaire du château de Pau), 
"Visanos" (1385, censier de Béarn), 
"Sent-Gran de Bisanos" (1491, notaires de Pau), 
"Vissanos" (1539, notaires d'Assat), 
"Bizenos" et "Visenos" (respectivement 1546 et 1683, réformation de Béarn) et 
"Bisanos" (1793 et Bulletin des lois en 1801).

"Les Garaux" était un hameau de Bizanos.

Le village existait déjà à l'époque gallo-romaine, les premières références au nom Bizanos remontant au début du avec la famille de Raymond de Bisanos.

Paul Raymond note qu'en 1385, Bizanos comptait treize feux et dépendait du bailliage de Pau. Le fief de Bizanos relevait de la vicomté de Béarn.
Jusqu'au milieu du c'est un village maraîcher et agricole comptant moins de 450 habitants.

Après la Révolution française, la construction de la route reliant Pau à Nay contribue au développement démographique et économique de la ville.

Bizanos fait partie de neuf structures intercommunales :

Bizanos accueille le siège du syndicat d'aménagement hydraulique du bassin de l'Ourse, du syndicat intercommunal pour la construction et le fonctionnement du C.E.S. de Bizanos et du syndicat mixte de la crèche l'Arche.

Bizanos fait partie de l'aire urbaine de Pau.

La commune fait partiellement partie de la zone d'appellation de l'ossau-iraty.

Parmi les éléments marquants du village, il faut noter le château de Franqueville, ainsi que la mairie construite en 1928.

L'église Saint-Magne fut édifiée en 1874. Elle est inscrite à l'Inventaire général du patrimoine culturel.

Le chemin Henri- est un chemin de randonnée qui relie le château de Franqueville à Bizanos au lac de Lourdes (Hautes-Pyrénées). Il alterne pistes forestières et chemins de terre et offre aux randonneurs une vue imprenable sur la chaîne des Pyrénées, le piémont et les plaines.
Long d'environ 35 kilomètres, le parcours du chemin peut être fractionné grâce à diverses routes qui le croisent. Il est possible de l'arpenter à pied, à cheval ou à vélo mais il est interdit à tout véhicule à moteur.

le Vignoble du Château : ce vignoble a été créé sous forme associative, il a pris la place d'un vieux vignoble détruit par l'oïdium en 1833. Il est composé de trois parties :


Plus de 400 adhérents y participent 65 d'entre eux entretiennent la parcelle. La production est vinifiée par le Domaine Bellegarde de Monein et commercialisée sur place le mercredi matin sous le nom d'où Bi de la Casta

Bizanos dispose de deux écoles primaires et d'un collège, le collège des Lavandières, qui regroupe 582 élèves en 2016, pour 22 classes .

Bizanos évolue dans le championnat de France de 3e division fédérale de rugby.



</doc>
<doc id="17725" url="https://fr.wikipedia.org/wiki?curid=17725" title="Bouloc (Haute-Garonne)">
Bouloc (Haute-Garonne)

Bouloc est une commune française située dans le sud-ouest de la France, dans le département de la Haute-Garonne, en région Occitanie. La commune se localise entre Toulouse et Montauban, dans le nord de la Haute-Garonne, au cœur de l'aire urbaine de Toulouse. 

Bouloc connaît depuis la fin du XXème siècle une importante croissance démographique. Jusqu'au milieu du siècle, la commune n'était qu'un petit village. Aujourd'hui, c'est une ville à part entière. Cette croissance s'explique par la proximité de la commune avec Toulouse, mais aussi par l'étalement urbain, la périurbanisation, et l'apparition de zones d'emplois importantes à proximité, comme Eurocentre par exemple. 

Ses habitants sont appelés les Boulocains.

Bouloc se situe au nord de la Haute-Garonne, dans l'AOC Fronton et dans le Pays Toulousain. La ville se situe au milieu de l'axe Toulouse-Montauban, à 20,5 km de Toulouse, 26,5 km de Montauban et 570 km de Paris.

Bouloc est la ville-centre d'une unité urbaine, l'unité urbaine de Bouloc, qui compte la commune et celle de Villeneuve-lès-Bouloc. La commune est également membre de l'aire urbaine de Toulouse, et de la zone d'emploi et du bassin de vie de Toulouse.

Bouloc est limitrophe de six autres communes.

Bouloc fait partie du bassin versant de la Garonne. Elle est arrosée par le Lègue ou Ruisseau de Magnanac et son affluent le Ruisseau de Sayrac.

La superficie de la commune est de 1 855 hectares. Son altitude varie de 127 à 226 mètres.

La commune connaît un climat océanique dégradé, à influences méditerranéennes et continentales, comme la majeure partie nord de la Haute-Garonne.

La commune est traversée par la route départementale 4, qui relie Fronton à Barrière de Paris, à Toulouse, en passant par le centre-ville. On compte également la route départementale 30, qui relie la zone Eurocentre à Montpitol, et la route départementale 77, qui relie le centre-ville à Saint-Rustice.

Bouloc est traversée au sud-ouest par l'autoroute A62, accessible à partir de la .

La gare la plus proche est la gare de Castelnau-d'Estretefonds, située sur la ligne de Bordeaux à Sète, et desservie par le réseau TER Occitanie.

La commune est traversée également par le réseau interurbain de la Haute-Garonne, le réseau Arc-en-Ciel, avec les lignes :
Enfin, Bouloc est située à 22 km de l'aéroport de Toulouse-Blagnac.

L'essentiel des constructions sont situées au sud du territoire communal, autour de la mairie. La ville s'étendant de plus en plus, le centre-ville commence à gagner du terrain, particulièrement au nord-ouest de la mairie.

En 2014, le nombre total de logements dans la commune était de 1 791, alors qu'il était de 1 601 en 2009.

Parmi ces logements, 93,9 % étaient des résidences principales, 0,7 % des résidences secondaires et 5,4 % des logements vacants. Ces logements étaient pour 89,2 % d'entre eux des maisons individuelles et pour 10,6 % des appartements.

La proportion des résidences principales, propriétés de leurs occupants était de 79,7 %, en légère baisse par rapport à 2009 (82,1 %). La part de logements HLM loués vides était de 2,7 % contre 2 %, leur nombre étant croissant 45 contre 30.

Bouloc est situé en zone inondable. Cependant, la commune n'est pas un territoire à risque important d'inondation, n'étant pas traversée par un fleuve ou une rivière.

La commune est également concerné par un risque de mouvement de terrain et de retrait-gonflement des sols argileux. On compte également un risque de séisme de 1/5 (très faible).

"Bou-", altération de "bon" « bon » et "loc" « lieu » en gascon, d'où le sens global de « bon lieu », c'est-à-dire « lieu agréable » ou « lieu propice ».

Homonymie avec les Bonlieu (Jura, "[Conventui] Boni Loci" 1319; Drôme, "Bonus locus" 1170), ainsi que Bonloc.

Bouloc fut l'hôte de visiteurs célèbres, parmi lesquels Alphonse de Poitiers en 1144, le pape Calixte II en 1119, Philippe le Bel en 1303, le roi Charles IX accompagné de Catherine de Médicis et de Michel de l'Hospital en 1565, et enfin Louis XIII le , jour de l'exécution du Duc de Montmorency à Toulouse.

La commune fait partie de la région historique de l'Occitanie et du Languedoc.

Bouloc est une commune avec une sensibilité de gauche, malgré une montée récente de l'extrême droite.

A titre d'exemple, lors de l'élection présidentielle de 2017, à Bouloc, au premier tour, c'est Emmanuel Macron qui l'avait emporté avec 26,07 % des voix, suivi par Marine Le Pen avec 20,48 % des voix, par Jean-Luc Mélenchon avec 19,72 % des voix et par François Fillon avec 16,66 % des voix. Au second tour, c'est Emmanuel Macron qui l'avait emporté avec 65,87 % des voix face à Marine Le Pen qui avait obtenu 34,13 % des voix.

Le nombre d'habitants au recensement de 2011 étant compris entre et , le nombre de membres du conseil municipal pour l'élection de 2014 est de vingt sept.

Bouloc fait partie du canton de Villemur-sur-Tarn. Avant le redécoupage départemental de 2014, Bouloc faisait partie de l'ex-canton de Fronton. La commune fait également partie de la cinquième circonscription de la Haute-Garonne.

La commune est membre de la communauté de communes du Frontonnais.

La collecte et le traitement des déchets des ménages et des déchets assimilés ainsi que la protection et la mise en valeur de l'environnement se font dans le cadre de la communauté de communes du Frontonnais. Depuis septembre 2015, la communauté de commune mit en place la collecte du tri sélectif en porte à porte.


La commune compte une école élémentaire et une école maternelle. À la rentrée 2013-2014, 489 enfants étaient scolarisés en 18 classes. 7 à l'école maternelle et 11 à l'école élémentaire avec également 1 CLIS (CLasse d'Inclusion Scolaire de 12 places). Bouloc compte également un centre de loisirs et une crèche.

Les élèves de la commune dépendent du collège Alain Savary de Fronton.

Les lycées les plus proches sont :

Plusieurs fois dans l'année, l'association « APOIRC » organise son « Salon Musical » dans la salle des fêtes de Bouloc, le dimanche après-midi à 16 h.

L'association « Les Amis de la Radio » gère une Webradio installée sur la commune et diffuse des émissions culturelles et sportive sur CETA Radio.

Bouloc possède plus de cinquante associations. La commune est reconnue pour avoir une vie associative très riche et dynamique.

On compte quatre médecins généralistes sur la commune, ainsi qu'un centre médico-social géré par le département et une pharmacie.

Les hôpitaux les plus proches sont situés sur Toulouse et Montauban.

Bouloc possède plusieurs associations sportives : basket, football, judo/ju-jitsu, futsal, tennis, sport équestre, etc.

La commune est couverte par le journal local La Dépêche du Midi et son édition locale nord-est de la Haute-Garonne, ainsi que par France 3 Occitanie.

La commune compte une église catholique, l'église Notre-Dame.

En 2014, le revenu fiscal médian par ménage était de 23 475 €. 63,6 % des foyers fiscaux étaient imposables.

En 2014, la population âgée de 15 à 64 ans s'élevait à 2 868 personnes, parmi lesquelles on comptait 78 % d'actifs dont 71,6 % ayant un emploi et 6,4 % de chômeurs.

On comptait 639 emplois dans la zone d'emploi, contre 585 en 2009. Le nombre d'actifs ayant un emploi résidant dans la zone d'emploi étant de 2 068, l'indicateur de concentration d'emploi est de 30,9 %, ce qui signifie que la zone d'emploi offre un peu moins d'un emploi pour trois habitants actifs.

Au 31 décembre 2015, Bouloc comptait 379 établissements : 17 dans l’agriculture-sylviculture-pêche, 25 dans l'industrie, 65 dans la construction, 232 dans le commerce-transports-services divers et 40 étaient relatifs au secteur administratif.

En 2016, 49 entreprises ont été créées à Bouloc, dont 29 par des autoentrepreneurs.





</doc>
<doc id="17726" url="https://fr.wikipedia.org/wiki?curid=17726" title="Potentiel hydrogène">
Potentiel hydrogène

Le potentiel hydrogène, noté pH, est une mesure de l'activité chimique des hydrons (appelés aussi couramment protons ou ions hydrogène) en solution. Notamment, en solution aqueuse, ces ions sont présents sous la forme de l'ion hydronium (le plus simple des ions oxonium).

Plus souvent, le pH mesure l’acidité ou la basicité d’une solution. Ainsi, dans un milieu aqueux à :

En 1893, le chimiste danois Søren Sørensen, qui travaille alors au laboratoire Carlsberg à Copenhague sur les effets des concentrations de quelques ions sur des protéines lors des processus de fabrication de la bière, remarque l’importance des ions hydrogène et décide d’introduire le concept de pH. Dans l’article où est évoqué le pH pour la première fois, Sørensen utilise la notation p. Dans cette publication, il donne au sigle la signification en latin ' (« poids de l’hydrogène ») ; mais dans les comptes-rendus de travaux qu’il rédige au sein du laboratoire Carlsberg de l’université de Copenhague la même année, p est l’abréviation du mot allemand ' (potentiel) et H est le symbole de l’hydrogène. Sørensen définit alors l’acidité d’une solution comme étant le cologarithme décimal de la concentration molaire (exprimée en moles par litre) en ions hydrogène : 

Le principe d’une telle échelle de pH est accepté par la communauté scientifique, notamment grâce au chimiste allemand Leonor Michaelis, qui publie en 1909 un livre sur la concentration en ion hydronium (HO). 
En 1924, à la suite de l’introduction du concept d’activité, Sørensen publie un nouvel article précisant que le pH dépend plutôt de l’activité que de la concentration en H. Entre temps, la notation pH a été adoptée, sans que l’on sache vraiment qui en a été l’initiateur :

Par la suite, la lettre « p » est reprise dans plusieurs notations usuelles en chimie, pour désigner le cologarithme : p"K", pOH, pCl La signification du sigle « pH » a été adaptée par chaque langue. Ainsi, par pH, on entendra « potentiel hydrogène » en français, « ' » en allemand, « ' » en anglais, ou « "" » en espagnol.

La notion d’acidité, qui était à la base uniquement qualitative, s’est vue dotée d’un caractère quantitatif avec les apports de la théorie de Brønsted-Lowry et du pH. Alors qu’au début du on utilisait uniquement des indicateurs de pH pour justifier du caractère acide ou basique d’une solution, les évolutions en électrochimie ont permis à l’IUPAC de se tourner dans les années 1990 vers une nouvelle définition du pH, permettant des mesures plus précises.

Depuis le milieu du , l’IUPAC reconnaît officiellement la définition de Sørensen du pH. Elle est utilisée dans les programmes scolaires (études supérieures) et les dictionnaires :

où "a" (également noté "a"(H) ou {H}) désigne l’activité des ions hydrogène H, sans dimension. Le pH est lui-même une grandeur sans dimension.

Cette définition formelle ne permet pas des mesures directes de pH, ni même des calculs. Le fait que le pH dépende de l’activité des ions hydrogène induit que le pH dépend de plusieurs autres facteurs, tels que l’influence du solvant. Toutefois, il est possible d’obtenir des valeurs approchées de pH par le calcul, à l’aide de définitions plus ou moins exactes de l’activité.

L’IUPAC donne aujourd’hui une définition du pH à partir d’une méthode électrochimique expérimentale. Elle consiste à utiliser la relation de Nernst dans la cellule électrochimique suivante :

À l’aide de mesures de la force électromotrice (notée fem ou f.e.m.) de la cellule avec une solution X et une solution S de référence, on obtient :

avec

Cette définition du pH a été standardisée par la norme ISO 31-8 en 1992.

Les manipulations liées au pH en chimie étant le plus souvent réalisées en milieu aqueux, on peut déterminer plusieurs définitions approchées du pH en solution aqueuse. En utilisant deux définitions différentes de l’activité chimique, on peut écrire les deux relations ci-dessous. Elles sont valables dans le domaine limité des solutions aqueuses de concentrations en ions inférieures à et n’étant ni trop acide, ni trop basique, c’est-à-dire pour des pH entre 2 et 12 environ.

où
et
où

Pour des concentrations encore plus faibles en ions en solution, on peut assimiler l’activité des ions H à leur concentration (le coefficient d’activité tend vers 1). On peut écrire :

Par abus d’écriture, l’écriture n’est pas homogène, La concentration standard "C" étant souvent omise pour simplifier la notation. Cette relation est la plus connue et est la plus utilisée dans l’enseignement secondaire.

Pour des acides forts en solution aqueuse à des concentrations supérieures à , l'approximation précédente n'est plus valable : il faut se ramener à la définition 
formula_8 où l'activité des ions oxonium formula_9 tend vers 1 quand la concentration augmente, soit un pH qui tend vers formula_10.

De même pour des bases fortes en solution aqueuse à des concentrations supérieures à , l'activité des ions hydroxyde HO tend vers 1 ; or, par définition de "K", produit ionique de l'eau valant 10 à , on a formula_11 donc formula_12 ne peut être inférieure à "K", soit un pH qui tend vers 14 quand la concentration en base forte augmente.

Brønsted et Lowry ont donné une définition simple des concepts d’acide et de base comme étant respectivement un donneur et un accepteur de proton. D’autres conceptions de l’acidité sont utilisées dans les milieux non protiques (milieux où l’espèce échangeable n’est pas le proton), telles la théorie de Lewis :

Exemples :

Le pH varie dans l’intervalle défini grâce à la constante d’auto-protolyse du solvant.

En solution aqueuse, à température et pression standard (CNTP), un pH de 7,0 indique la neutralité car l’eau, amphotère, se dissocie naturellement en ions H et HO aux concentrations de . Cette dissociation est appelée autoprotolyse de l’eau :


Dans les conditions normales de température et de pression (), le produit ionique de l’eau ([H][HO]) vaut , d’où . On peut également définir le pOH (-log a), de sorte que .

Le pH doit être redéfini – à partir de l’équation de Nernst – en cas de changement de conditions de température, de pression ou de solvant.

Le produit ionique de l’eau ([H][HO]) varie avec la pression et la température : sous et à (), le produit ionique vaut , d’où p"K" = 13,995 ; sous et à , p"K" n’est que de 7,68 : le pH d’une eau neutre est alors de 3,84 ! Sous une atmosphère de (pression de vapeur d’eau saturante), on a : 

Par conséquent, le pOH varie de la même façon et pour la même raison : la plus grande fragmentation de l'eau en proton H (en réalité ion hydronium ) et en OH. Dire que l'eau devient « plus acide » est donc assurément vrai, mais il est non moins vrai qu'elle devient en même temps et pour des raisons de parité « plus basique ». Néanmoins le résultat est bien qu'elle devient plus corrosive, problème étudié avec soin pour les échangeurs de centrales thermiques.

Le produit ionique de l’eau varie selon l’équation suivante :

dans laquelle "K" = "K"/(mol⋅kg) et "d"="d"/(g⋅cm).

Avec :
Domaine d'application de la formule : "T" compris entre , "P" compris entre abs. 

Une autre formulation pour le calcul du p"K" est celle de l'IAPWS

Dans d’autres solvants que l’eau, le pH n’est pas fonction de la dissociation de l’eau. Par exemple, le pH de neutralité de l’acétonitrile est de 27 () et non de 7,0.

Le pH est défini en solution non aqueuse par rapport à la concentration en protons solvatés et non pas par rapport à la concentration en protons non dissociés. En effet, dans certains solvants peu solvatants, le pH d’un acide fort et concentré n’est pas nécessairement faible. D’autre part, selon les propriétés du solvant, l’échelle de pH se trouve décalée par rapport à l’eau. Ainsi, dans l’eau, l’acide sulfurique est un acide fort, tandis que dans l’éthanol, c’est un acide faible. Travailler en milieu non aqueux rend le calcul du pH très compliqué.

Un pH moins élevé que celui de la neutralité (par exemple 5 pour une solution aqueuse) indique une augmentation de l’acidité, et un pH plus élevé (par exemple 9 pour une solution aqueuse) indique une augmentation de l’alcalinité, c’est-à-dire de la basicité.

Un acide diminuera le pH d’une solution neutre ou basique ; une base augmentera le pH d’une solution acide ou neutre. Lorsque le pH d’une solution est peu sensible aux acides et aux bases, on dit qu’il s’agit d’une solution tampon (de pH) ; c’est le cas du sang, du lait ou de l’eau de mer, qui renferment des couples acido-basiques susceptibles d’amortir les fluctuations du pH, tels anhydride carbonique / hydrogénocarbonate / carbonate, acide phosphorique / hydrogénophosphate / phosphate, acide borique / borate.

Le pH d’une solution dite physiologique est de 7,41.

Pour des concentrations ioniques importantes, l’activité ne peut plus être assimilée à la concentration et on doit tenir compte de la force ionique, par exemple grâce à la théorie de Debye-Hückel. Le pH d’une solution décamolaire d’acide fort n’est donc pas égal à -1, tout comme le pH d’une solution décamolaire de base forte n’est pas égal à 15. L’agressivité de telles solutions et leur force ionique importante rend la mesure du pH délicate avec les habituelles électrodes de verre. On a donc recours à d’autres méthodes s’appuyant sur les indicateurs colorés (spectroscopie ou ). Pour des concentrations élevées de H, on peut définir par analogie d’autres échelles de mesure d’acidité, telles l’échelle de Hammett H.

D’après la loi de Nernst établie plus haut :

dans laquelle X est la solution dont le pH est inconnu et S, la solution de référence ; avec à ("R" est la constante des gaz parfaits, "T", la température et "F", la constante de Faraday).

Généralement, le pH est mesuré par électrochimie avec un pH-mètre, appareil comportant une électrode combinée spéciale, dite électrode de verre, ou deux électrodes séparées. L’électrode de référence est en général au calomel saturé ().

Il existe de nombreuses façons de mesurer l’acidité, on utilise fréquemment des indicateurs de pH.

À p"K" = 14.

Cette relation n’est pas valable pour des concentrations inférieures à et ne devrait s’appliquer qu’avec des concentrations supérieures à . Son application à une solution diluée à 10 donne en effet , ce qui est absurde puisque la solution est acide et non alcaline (le pH d’une telle solution est de 6,98).

Dans le cas d’un monoacide, le pH se calcule en résolvant l’équation du troisième degré suivante : .

Dans le cas limite formula_17, l’équation précédente devient alors formula_18 d’où on déduit que formula_19. Lorsque formula_20, on a formula_21.

Cette relation est soumise aux mêmes remarques que pour le cas d’un acide fort.

Cette formule est très approximative, notamment si les acides ou bases utilisés sont faibles, et devrait être utilisée avec la plus grande prudence.

Dans des solutions assez peu concentrées (on dit « solution diluée »), l’acidité est mesurée par la concentration en ions hydronium (oxonium) ou [], car les ions H s’associent avec []. Cependant, aux fortes concentrations, cet effet est en partie contrebalancé par les coefficients d’activité qui s’effondrent aux concentrations élevées. Néanmoins, il est possible d’obtenir des pH négatifs, y compris dans le contexte des séquelles minières en cas de drainage minier acide extrême. 

L'échelle 0-14 pour le pH est une limite conventionnelle. Ainsi une solution concentrée d'acide chlorhydrique à a un pH d'environ quand une solution saturée d'hydroxyde de sodium a un pH d'environ .

Les produits plus acides que l’acide sulfurique à 100 %, sont qualifiés de superacides. Ceux-ci sont couramment utilisés, notamment comme catalyseurs pour l’isomérisation et le craquage des alcanes. De même, l'acide chlorhydrique concentré possède un pH négatif.

Le pH d’un sol est le résultat de la composition du sol (sol calcaire, résineux) et de ce qu'il reçoit (pluie, engrais). Il a une influence sur l’assimilation des nutriments et oligo-éléments par une plante.




</doc>
<doc id="17727" url="https://fr.wikipedia.org/wiki?curid=17727" title="Base">
Base

Dans le langage courant, la base est la partie inférieure d'un objet sur laquelle il repose.







</doc>
<doc id="17731" url="https://fr.wikipedia.org/wiki?curid=17731" title="Sion (Valais)">
Sion (Valais)

Sion, appelée en allemand "", est une ville et une commune de Suisse, chef-lieu et ville la plus peuplée du canton du Valais et du district homonyme.

La ville de Sion fut occupée dès le néolithique (nécropole du Petit-Chasseur), mais celle-ci semble avoir pris son essor surtout à l'époque celte.

Elle tire son nom actuel du latin "Sedunum", lui-même dérivé de celui du peuple celte qui vivait là, les Sédunes (en latin : "Sedunii"). Ceux-ci construisirent sur le site de Sion un oppidum habituellement identifié avec le Drousomagos, signifiant peut-être "marché de Drusus" ou "marché des buissons", cité par Ptolémée et qu'il situe en amont de Martigny.

Jusqu'à la fin de l'époque romaine, Sion reste dans l'ombre de Massongex puis de Martigny, alors appelée Octodure, qui ont l'avantage de se trouver sur la route stratégique du Grand-Saint-Bernard. Ce n'est qu'au , lorsque l'évêque y déplace le siège épiscopal, que la ville devient le centre socio-culturel de la région.

Dès 999, l'évêque de Sion devint comte du Valais. En 1032, le comté est intégré au Saint-Empire romain germanique. À partir de 1189, le comté devient la principauté épiscopale de Sion. Le Moyen Âge et la Renaissance voient s'affronter, sur le terrain de l'unification du Valais et de la lutte de pouvoir, l'évêque et la Diète, qui siège aussi à Sion.

La ville de Sion fut détruite et pillée à plusieurs reprises jusqu'en 1475, date à laquelle les troupes savoyardes furent repoussées à ses portes lors de la bataille de la Planta.

La ville croît alors lentement jusqu'au terrible incendie de 1788. Elle est reconstruite, mais ses remparts sont abattus au (il n'en reste aujourd'hui plus guère que la Tour des Sorciers et la Tour de Guet).

Elle connaît un essor important avec l'arrivée du train.

En 1968, la commune et le village de Bramois ont fusionné avec la municipalité de Sion, tout comme celle de Salins en 2013 et des Agettes en 2017.

Sion tenta à trois reprises d'obtenir les Jeux olympiques d'hiver, mais fut trois fois le dauphin du vainqueur (Denver qui finalement refusa en 1976 au profit d'Innsbruck, Salt Lake City en 2002 et Turin en 2006).

Selon l'Office fédéral de la statistique, Sion mesure .

La basilique (mineure) ou château de Valère (ancienne résidence des chanoines du chapitre) et le Château de Tourbillon surplombent la ville. 

La "Vieille-ville" est un bourg médiéval particulièrement bien conservé : outre les châteaux sis sur les deux collines, de nombreux bâtiments importants s'y trouvent. On peut citer : le château de la Majorie, l'hôtel de ville, la Maison Supersaxo, le casino de Sion abritant le parlement cantonal valaisan, la cathédrale Notre-Dame (ou du Glarier) de Sion avec en face l'Évêché qui est le siège du diocèse de Sion, la Maison du Chapitre, l'église Saint-Théodule, le palais du gouvernement valaisan, la maison de la Diète et la Tour des Sorciers, un des rares vestiges du dernier mur d'enceinte avec la Tour du Guet située au pied du rocher de Valère.

La vallée est traversée par le Rhône.
On compte 93.6 jours par an de gel à Sion. La température maximale y est inférieure à 7,6 jours par an. Elle y est supérieure à + 68,8 jours par an et supérieure à + 16 jours par an. La durée d'ensoleillement moyen est de 2091.6 heures par année.

Selon l'Office fédéral de la statistique, Sion possède habitants fin . Sa densité de population atteint hab./km². ; .

Le conseil municipal est le pouvoir exécutif de la commune. Ses 9 membres, non permanents à l'exception du Président, sont élus tous les 4 ans par le peuple.

Le 30 novembre 2008, lors des élections, le libéral-radical Marcel Maurer est élu à la présidence de la ville de Sion avec voix, devenant le premier président non démocrate-chrétien de la ville. Le 16 octobre 2016, c'est Philippe Varone qui est élu à ce poste avec voix.


Liste des Vice-Présidents

Le conseil général est le pouvoir législatif de la commune. Ses 60 membres sont élus tous les 4 ans par le peuple.

La commune de Sion comporte aussi une corporation de droit public issue de la commune médiévale: la Bourgeoisie. Le Conseil bourgeoisial compte sept personnes : un président, un vice-président et cinq conseillers; un chancelier les assiste.

Le secteur tertiaire est la principale activité de la ville, en raison notamment de la présence de l'administration cantonale, du parlement valaisan, et du tribunal cantonal. Le tourisme (nombreux châteaux et musées) est également une activité importante.

Le secteur secondaire est peu représenté, même si une certaine activité horlogère existe (sous-traitance).

Le secteur primaire, bien que marginalisé, n'est pas négligeable: Sion est la troisième commune viticole de Suisse, la culture maraîchère y est également notable. Néanmoins, les surfaces consacrées à l'agriculture et à la viticulture sont en constante régression, à mesure que l'urbanisation progresse.

Finalement, il convient de signaler la présence d'un important site médical, l'hôpital de Sion-Région, qui regroupe les disciplines médicales de pointe, est membre du Réseau Santé Valais, et collabore avec le Centre hospitalier universitaire vaudois de Lausanne.

Toujours dans le secteur médical, on trouve à Sion l'Institut central des Hôpitaux valaisans (ICHV), la Clinique romande de réadaptation physique de la SUVA, et l'Institut de recherche en ophtalmologie (IRO - laboratoires en optique, biophysique et oculogénétique).

La commune abrite une usine d'incinération d'ordures qui dessert 44 communes avoisinantes. Cette usine est équipée d'un catalyseur qui ramène les émissions de au-dessous des valeurs limites tolérées. En outre, la chaleur de combustion est convertie en énergie électrique qui est injectée dans le réseau local.

Dotée de plusieurs cinémas et théâtres y compris, en saison, de théâtres et spectacles de plein air, comme "Sion en Lumières", la vie culturelle de Sion est animée. De nombreux musées cantonaux, tels que musée d'histoire, d'histoire naturelle, d'archéologie, des Beaux-Arts, le Trésor de la Cathédrale, la Maison de la nature de Montorge ou La Galerie de la Grenette, s'y trouvent. De plus, divers festivals de musique y ont lieu, dont les renommés festival Tibor Varga et son concours international de violon ou encore le festival de l'orgue ancien, dédié au plus vieil orgue jouable du monde (c. 1390-1430) qui se trouve dans la Basilique de Valère. Le chœur Novantiqua de Sion s'est fait connaître en Suisse et à l'étranger, interprétant les grandes pages du répertoire classique, mais également de la musique contemporaine. La Compagnie de danse INTERFACE est notamment basée à Sion.
Depuis mars 2003, la vieille ville de Sion est également dotée d'un marché traditionnel tous les vendredis de 8h à 14h. Le carnaval de Sion est le plus grand carnaval du Valais central, attirant chaque année près de 60'000 personnes.

Chaque deuxième samedi de décembre a lieu la Course Titzé de Noël, une course populaire à travers les rues de la vieille-ville qui comprend également un plateau élites. 

Le club de la ville est Sion Basket et évolue en LNB féminine ( suisse). 

La Société d'escrime de Sion existe depuis 1945. Fondée par Michel Evéquoz, elle a formé plusieurs escrimeurs qui ont obtenu des médailles lors de la tenue des jeux olympiques:

Elle a également remporté 9 fois le championnat suisse à l'épée masculine, 3 fois à l'épée féminine, et 4 fois au fleuret masculin.

La ville de Sion a posé sa candidature pour les Jeux Olympiques de 2002 et de 2006 mais a perdu contre Salt Lake City (Utah, USA) et contre Turin (Italie).

Les clubs de la ville sont le FC Sion, le FC Bramois et le FC Châteauneuf.
Le FC Sion possèdait la particularité d'avoir remporté 13 finales de la Coupe de Suisse sur 13 participations, dont la dernière en 2015 contre le FC Bâle. Cependant, le FC Sion fut battu par le FC Bâle 3 à 0 lors de la finale de la Coupe de Suisse 2017. Ce nombre est symbolique, représentant ainsi les 13 étoiles du drapeau du Valais. Certains d’ailleurs disent que le FC Sion n'aurait pu gagner une quatorzième Coupe de Suisse, étant donné que le drapeau valaisan ne comporte que 13 étoiles, d'autres disent que seul un changement de président permettra d'obtenir une quatorzième Coupe de Suisse.

Le club évolue actuellement en première division (Super League) et joue ses matches au Stade de Tourbillon.

Le club de la ville est le HC Sion-Nendaz 4 Vallées (fusion entre le HC Sion et le HC Nendaz en 2013). Il évolue dans le championnat de Suisse de hockey sur glace D3. Il dispute ses rencontres à domicile à la patinoire de l'Ancien Stand (près de ).


Le club de Sion Pétanque organise chaque année le Grand Prix du Valais.


La ville de Sion possède deux écoles de maturité pour la formation des étudiants :

L'École des métiers du Valais est également implantée à Sion (ainsi qu'à Viège).


Le Centre de Formation Professionnelle de Sion (CFPS) est implanté à Sion sur 2 sites : un bâtiment principal avenue de France et un bâtiment technique Chemin St-Hubert.




Sion est un nœud intermodal (aéroport civil et militaire de Sion, gare CFF et gare routière). La gare de Sion se trouve sur la ligne du Simplon (Lausanne-Milan). L' y correspond à la route européenne 62, sorties : et .



</doc>
<doc id="17735" url="https://fr.wikipedia.org/wiki?curid=17735" title="Assainissement en France">
Assainissement en France

Cet article traite essentiellement des aspects institutionnels de l'assainissement en France. 
Pour les aspects techniques, voir les articles épuration des eaux et hydraulique urbaine.

Si les techniques d’assainissement ont été mises en œuvre depuis l’époque romaine (l'égout le plus ancien du monde romain est la fameuse "cloaca maxima" de Rome), les pot de chambre, latrines ont été utilisés durant des siècles. Du Moyen Âge, jusqu'au , tous les déchets domestiques étaient dispersés dans la rue … Ce n'est que lors de la seconde moitié du que s'élabore la conception moderne de l'assainissement, lorsque John Snow découvrit la véritable origine du choléra lors de l'épidémie de 1854 à Londres.

Les techniques d'assainissement évoluent en permanence : on a réalisé des puits perdus, des fossés d'irrigation, puis des réseaux souterrains et enfin des installations d'assainissement de plus en plus performantes qu’elles soient individuelles, semi collectives ou collectives.

Un bilan annuel est publié chaque année depuis 2009 (108 pages en 2015).

Depuis, la loi sur l'eau et les milieux aquatiques du 30 décembre 2006, la loi dite Grenelle II et les trois arrêtés du 07 septembre 2009 sont venus compléter cette réglementation. 

On distingue deux grands types de réseaux :
Diverses approches intermédiaires sont possibles (pseudo-séparatif).

Les deux solutions présentent des avantages et des inconvénients tant techniques que financiers. 
C’est souvent ce second point, et notamment le régime des subventions, qui détermine les choix.

C’est à la commune qu’il appartient de choisir, après enquête publique, les secteurs de son territoire qui seront équipés d’un réseau collectif ou de dispositifs individuels tels que la fosse septique, à travers les zonages d'assainissement. La loi confie aux collectivités le soin de vérifier la conformité des installations individuelles et les a donc obligées de mettre en place un service public d'assainissement non collectif (SPANC) avant le janvier 2006. Elles peuvent aussi décider d'assurer l’entretien des ouvrages non collectifs.

Il est assuré par la perception d’une redevance qui couvre à la fois les investissements et le fonctionnement du service. 
Elle est perçue par l’intermédiaire de la facture d’eau. 
Cette redevance doit correspondre à la réalité des prestations apportées à l’usager en distinguant le domaine collectif du domaine individuel (donc au besoin deux redevances différentes doivent être instituées). 
Si un usager ne bénéficie d’aucun service (pas de réseau dans sa rue ou pas d’intervention sur son installation individuelle), il ne doit donc pas être assujetti. 

Le Code de la santé publique français oblige les riverains à se raccorder dans les deux ans qui suivent la construction d'un nouveau réseau dans leur rue. Un arrêté interministériel détermine les catégories d'immeubles pour lesquelles un arrêté du maire, approuvé par le représentant de l'État dans le département, peut accorder soit des prolongations de délais qui ne peuvent excéder une durée de dix ans, soit des exonérations de l'obligation. La redevance est exigible dès la mise en place du nouvel équipement. 
Elle peut être majorée si le raccordement n'intervient pas dans le délai prescrit.

La gestion du service assainissement peut être assurée directement par la collectivité (régie) ou déléguée à une société privée (affermage) ou concession).

L'assainissement constitue une problématique majeure en Île-de-France compte tenu de la très forte densité de la région. Cette tâche est gérée par le Syndicat interdépartemental pour l’assainissement de l’agglomération parisienne (SIAAP), établissement public à caractère administratif français créé en 1971 par le conseil municipal de Paris et les conseils généraux des départements des Hauts-de-Seine, de la Seine-Saint-Denis et du Val-de-Marne dans le cadre de la suppression du Département de la Seine.
Le SIAAP traite 2,3 millions de mètres cubes par jour en temps sec et 3 millions de mètres cubes en temps de pluie.

Le volume peut par ailleurs atteindre 7 millions de mètres cubes pendant un fort orage dans quatre stations régionales de traitement :

À la suite du Grenelle de l'environnement (2007), le projet de loi Grenelle II a retenu de nombreuses propositions concernant la trame bleue et trois propositions sur l'assainissement déclinés par 3 articles qui sont (sous réserve de modification lors du processus d'examen de la loi) :



</doc>
<doc id="17736" url="https://fr.wikipedia.org/wiki?curid=17736" title="Clément V">
Clément V

Bertrand de Got est le premier des sept papes qui siégèrent en Avignon entre 1309 et 1377, et naquit vers 1264 en Guyenne, près de Villandraut (actuellement en Gironde), est élu pape en 1305, et meurt le , à Roquemaure (actuellement dans le Gard). Son tombeau se trouve dans l'église collégiale (qu'il avait fait bâtir) à Uzeste. Il fut évêque de Saint-Bertrand-de-Comminges puis archevêque de Bordeaux, avant de devenir pape sous le nom de . Sous son égide furent aussi construits dans le sud de l'actuel département de la Gironde les châteaux dits « clémentins » : Villandraut, Roquetaillade, Budos, Fargues, La Trave. 

On retient de lui l'image d'un pape de bonne foi qui manquait toutefois d'audace et d'esprit de décision, autant par tempérament qu'en raison d'une santé déliquescente.

Il a donné son nom au vin de Bordeaux Château Pape Clément.

Après un court pontificat, la mort de , en , fit ouvrir le conclave de Pérouse et laissa éclater les dissensions du Sacré Collège entre cardinaux bonifaciens et anti-bonifaciens. Les discussions durèrent jusqu’au mois de , et les cardinaux se mirent d'accord pour choisir un pontife hors de leurs rangs, qui par le même coup n'aurait pas été mêlé aux problèmes de la politique bonifacienne. Ainsi, le , ils désignèrent l’archevêque de Bordeaux, dont le nom fut choisi par Napoléon Orsini, parmi trois prélats choisis hors du Sacré Collège.

Différents récits se contredisent sur le déroulement de l'élection. Selon le récit du chroniqueur Jean Villani, le parti italien du Sacré Collège, bonifacien, devait nommer trois évêques de France, et le parti français choisir celui des trois qu'il préférerait. Mis au courant de la liste, Philippe le Bel alla trouver Bertrand de Got pour s'accorder avec lui, en échange de son élection au trône pontifical. Selon Villani, ceux-ci se seraient rencontrés en forêt près de Saint-Jean-d'Angély et le roi de France aurait fait promettre au cardinal de réaliser six actions dès lors qu'il débuterait son pontificat : révoquer les actions de à son encontre, redonner aux Colonna leur honneur et dignité, ou encore accorder à la France les décimes du Clergé pour une durée de cinq ans. Cependant, cette rencontre n'a pu être vérifiée et serait contredite par plusieurs chroniques prouvant que les deux hommes ne se trouvaient pas à Saint-Jean-d'Angély à cette date. D'après Ferretto de Vicente, les habitants de Pérouse, las de voir les cardinaux préférer leurs maisons personnelles au palais pontifical et à son conclave, les poussèrent à se réunir à nouveau au palais, les y enfermèrent, et les privèrent du toit et des vivres tant qu'ils ne se seraient pas accordés. Un troisième récit fait intervenir Robert d’Anjou à la tête de « trois cents cavaliers aragonais armés et d’une multitude d’Almogavres qui ne l’étaient pas moins ». Impressionnés par tant de lances, les cardinaux français et italiens qui étaient représentés à égalité dans le conclave s’empressèrent de se mettre d’accord sur une seule chose : choisir un pontife hors de leurs rangs. Selon Jean Favier, enfin, la nomination de Bertrand de Got est à la fois désirée par le roi de France, et considérée comme acceptable par Francesco Caetani, neveu de et chef de file de son parti depuis la sortie du conclave de Matteo Rosso, malade, qui s'opposait par contre à un candidat extérieur au Sacré Collège. Napoléon Orsini, allié des Colonna, convainc Caetani de favoriser de Got, et de rallier son parti à cette candidature.

Le nouveau pape choisit de régner sous le nom de Clément, le cinquième, le .

Bertrand de Got aurait voulu se faire couronner à Vienne (Dauphiné) comme son lointain prédécesseur Gui de Bourgogne qui en fut l’archevêque de 1083 à 1110 et qui régna, jusqu’en 1124, sous le nom de . Mais Philippe le Bel préféra Lyon et le nouveau pape obtempéra. Dans un froid glacial, se dirigeait vers la vallée du Rhône, puis remonta vers Lyon, pour son couronnement. La cérémonie déploya ses fastes devant le roi de France, en la basilique Saint-Just, le dimanche . 

C'est alors que ce produisit un événement qui devait coûter la vie au duc de Bretagne Jean II, venu à Lyon pour le sacre du pape Clément V afin de régler ses différends avec l'épiscopat breton. Au retour de l'église Saint-Just, alors que le duc tenait la bride de la mule pontificale, un mur sur lequel une foule de spectateurs s'est placée s'effondre, renversa le souverain pontife et ensevelit Jean II : on l'en retira mourant, il expira quatre jours plus tard, entre le 16 et le .
Peu décidé à se rendre à Rome, où régnait le marasme le plus total (troupes pontificales en guerre contre Venise pour le contrôle de Ferrare), il semblerait que la prime intention de fut de passer son pontificat à Bordeaux. Rome au n'était pas encore la capitale politique et administrative de l'Église, la cour pontificale étant itinérante, mais elle gardait la prééminence car elle conservait les reliques des apôtres Pierre et Paul. Entre février et mars, il séjourna à Cluny, Nevers et Bourges avant de rejoindre son ancien archevêché, traînassant d’abbayes en diocèses de France et nommant des cardinaux à sa dévotion.

Quand il s'approcha de Bordeaux, les Gascons tout au long du chemin le saluaient et l'acclamaient. Il trouva une ville en liesse lors de son arrivée en juillet 1306. Cela entraîna la prise de mesures de sécurité et de ravitaillement par le sénéchal d'Aquitaine. Le , le pape quitta son ancienne ville épiscopale et, en chemin, passa à Villandraut, où il était né et dont il était le seigneur.

 fut d'abord le pape du procès de l'ordre du Temple. Philippe le Bel, le , avait fait parvenir à les aveux de hauts dignitaires templiers. En Guyenne, le pape, malade, malgré ces premiers aveux et la promesse royale de lui remettre tous les coupables, manquait d’enthousiasme. Cela était insupportable à Guillaume de Nogaret. Le garde des Sceaux pensa avoir trouvé la parade en faisant prononcer par Pierre Dubois, avocat de Coutances, une diatribe en place publique « contre ceux qui refusaient de faire manger le pain du roi » aux chevaliers du Temple.

Troublé par la tournure des événements et pour tenter d’amadouer le roi de France, décida de s’installer provisoirement en pays plus neutre que la Guyenne anglaise. Il choisit le Comtat Venaissin, fief pontifical. Le Comtat Venaissin avait été cédé, en 1229 selon les termes du traité de Paris, par de Toulouse à la papauté. Rome en prit officiellement possession quelques décennies plus tard, en 1274, après la mort d’Alphonse de Poitiers et de son épouse, Jeanne de Toulouse, fille du comte Raymond.

Malgré les positions favorables à son égard du comte de Provence et du Dauphin du Viennois, n’avait pas pris en compte que la pression française sur ses fiefs de la rive gauche du Rhône devenait de plus en plus prégnante. Au bas de Villeneuve-lès-Avignon elle était matérialisée par la tour Philippe "le Bel", véritable donjon contrôlant le pont Saint-Bénézet. Elle venait d’être achevée, en 1307, après quinze ans de travaux. Quant au pont, qui reliait Avignon (terre d’Empire) à Villeneuve-lès-Avignon (royaume de France), il avait été construit entre 1177 et 1184. Cet ouvrage mesurait neuf cent quinze mètres de long, avait vingt-deux piles et de nombreuses arches de bois. Personne ne se doutait à cet instant que neuf pontifes allaient se succéder dans cette ville pendant plus d’un siècle.

, traînait tellement les pieds, qu’en définitive le concile, au cours duquel le premier pape du Comtat devait condamner les templiers, ne fut convoqué qu’en 1311 et encore sur ferme injonction du roi de France. Le Souverain Pontife quitta sa résidence de Notre-Dame du Groseau, près de Malaucène, le pour rejoindre Vienne.

Le verdict si prévisible fut seulement proclamé le , en séance plénière du concile de Vienne, et en présence du roi de France le Bel, il promulgua la bulle « "Vox in excelso" » qui supprimait l'ordre du Temple. Ce qui n’était pas prévu par le roi de France était que tous les biens des templiers fussent attribués aux chevaliers de Rhodes.

Un mois plus tard, le Pape décréta, par la bulle « "Ad providam" » la dévolution des biens du Temple à l'ordre de l'Hôpital Saint-Jean de Jérusalem, et régla le sort des Templiers par la bulle « "Considerantes dudum" » fulminée le .

Mais en Provence, qui jouxtait Avignon et le Comtat Venaissin, ni ni son successeur Robert d’Anjou ne cédèrent de bonne grâce les commanderies templières aux Hospitaliers. Certaines d’entre elles étaient encore en possession du comte de Provence en 1319.

Mais pour l’instant, dans le Comtat Venaissin, était au plus mal, sans doute atteint d'un cancer de l'intestin. Ses « physiciens » (médecins), pour tenter d’apaiser ses douleurs, lui faisaient ingurgiter des émeraudes pilées. Rongé par la maladie, il publia le les décrétales du concile et quitta sa retraite de Monteux, avec l’espoir de rejoindre Villandraut, le fief de sa famille près de Langon. Le pape atteignit les rives du Rhône le pour s’éteindre, quinze jours plus tard, le , à Roquemaure dans la demeure du chevalier Guillaume de Ricavi qui l’avait hébergé.

La dépouille de fut ramenée à Carpentras pour des hommages solennels. Durant la veillée funèbre, un cierge renversé mit le feu au catafalque et carbonisa le mollet du pontife défunt.

Le , le Sacré Collège arriva à Carpentras pour élire un nouveau pape. On sait que le jeune Pétrarque assista au défilé préalable au conclave qui réunissait vingt-trois cardinaux dont quinze cisalpins et huit transalpins. Les luttes de tendances entre Italiens, Gascons et Français furent telles que deux longs mois passèrent sans qu’un accord fût possible pour trouver un successeur à . 
Sous l’inventif prétexte de donner une vraie sépulture au pontife défunt, le , le conclave fut attaqué aux cris de « Patria Venaissini ! Mort aux Italiens ! Nous voulons un pape ! ». 
Les responsables de ce coup de force étaient Bertrand de Got, seigneur de Monteux, et Raymond Guilhem de Budos, recteur du Comtat, neveux de . Ils pillèrent la ville, incendièrent nombre de demeures et surtout emportèrent avec eux le trésor de guerre de leur oncle, un million de florins destinés à la croisade. Affolés, les cardinaux s’égaillèrent comme des moineaux.

À la fin du mois d’, ses restes furent transférés à Uzeste, dans la collégiale comme il en avait exprimé le souhait dans son codicille du . Dante Alighieri, qui ne l’aimait pas, le plaça en son "Enfer" tout en le traitant, pour sa servilité face au roi de France, de pasteur sans principe capable des œuvres les plus basses.


Ne purent participer au conclave : 

Se trouvaient dans l’impossibilité de participer les cardinaux déposés par :

Le , à Lyon, le nouveau pontife désigne ses premiers cardinaux. Il remet leurs chapeaux à cinq de ses neveux : Bérenger Frédol "le Vieux", Arnaud Frangier de Chanteloup, Arnaud de Pellegrue, Raymond de Got et Guillaume Ruffat de Fargues. Sont aussi de la promotion : Pierre de La Chapelle-Taillefert, Pierre Arnaud, Thomas Jorz, dit "Anglicus", confesseur d’, Nicolas Caignet de Fréauville, confesseur de Philippe "le Bel" et Étienne de Suisy, vice-chancelier du roi de France. 

Le , il procède à sa seconde nomination de cardinaux. Ils sont au nombre de cinq : Arnaud de Faugères (ou Falguières), Bertrand des Bordes, Arnaud Nouvel, Raymond-Guilhem de Fargues, son neveu, et Bernard Jarre (ou Garve) de Sainte-Livrade, son parent.

Le , pour la troisième fois, il désigne ses cardinaux. Entrent dans le Sacré et Antique Collège : Guillaume de Mandagout, Arnaud d’Aux de Lescout, Jacques Arnaud Duèze, le futur , Béranger Frédol "le Jeune", petit-cousin du pape, Michel de Bec-Crespin, Guillaume-Pierre Gaudin, Vital du Four et Raymond Pierre.

Il ne reste que deux statues le représentant : l'une a la tête mutilée et est située sur son tombeau à Uzeste, et l'autre à Bordeaux.







</doc>
<doc id="17737" url="https://fr.wikipedia.org/wiki?curid=17737" title="Søren Sørensen">
Søren Sørensen

Søren Peder Lauritz Sørensen ( - ) est un chimiste danois, né à Havrebjerg au Danemark. Il est connu pour l'introduction du concept de pH, l'échelle permettant de mesurer l'acidité ou la basicité.

Sørensen commence d'abord à étudier la médecine à l'université de Copenhague mais se tourne rapidement vers la chimie, domaine dans lequel il obtient son doctorat en 1899. Il travaillait alors sous la direction de Sophus Mads Jørgensen sur les synthèses inorganiques.

De 1901 à 1938, il dirige le laboratoire de Carlsberg à Copenhague. Il commence bientôt à étudier les acides aminés, les protéines, les enzymes et notamment l'effet de la concentration des ions dans l'analyse de ces protéines. La concentration des ions hydrogène jouant un rôle central dans les réactions enzymatiques, il trouve un moyen simple d'exprimer celle-ci : il s'aperçoit qu'une échelle convenable peut être établie en prenant l'opposé du logarithme décimal de la concentration de ces ions. C'est ainsi qu'en 1909, il introduit le concept du potentiel hydrogène. Dans l'article où il introduit cette échelle, il utilise la notation p et décrit deux nouvelles méthodes pour mesurer l'acidité. La première utilise des électrodes tandis que la seconde consiste à comparer la couleur d'échantillons à un ensemble d'indicateurs de référence.

Par la suite, Sørensen devient un protagoniste dans l'application de la thermodynamique à la chimie des protéines. Il est aidé dans ce travail par sa femme, Margrethe Høyrup Sørensen. Il développe également des solutions tampons permettant de maintenir constant le pH d'une solution.

Avant que Sørensen ne développe l'échelle de pH, il n'y avait aucune méthode communément admise pour exprimer la concentration en ions hydrogène.


</doc>
<doc id="17738" url="https://fr.wikipedia.org/wiki?curid=17738" title="Festival international du film d'animation d'Annecy">
Festival international du film d'animation d'Annecy

Le Festival international du film d'animation d'Annecy, créé en 1960, se déroule au début du mois de juin dans la ville d'Annecy, en Haute-Savoie. D'abord tous les deux ans, le festival devient annuel en 1997. CITIA est la structure organisatrice du Festival.

Le Festival propose une sélection officielle avec un panel de films d'animation utilisant des techniques diverses : dessins animés, papiers découpés, pâte à modeler, stop motion, 3D... Classé dans différentes catégories : 

Ce festival est un rendez-vous important de l'image en mouvement. En parallèle à la compétition, se tiennent des avant-premières mondiales, des rétrospectives, des hommages, des rencontres autour des films et des auteurs, des dédicaces, des expositions, des projections en plein air chaque soir sur le Pâquier.

Concomitamment, un Marché international du film d'animation, est organisé également à Annecy, depuis 1986.

Dans les années 60, l'existence sur le territoire savoyard d'un ciné-club très actif, combinée à la rencontre de ses animateurs avec l'équipe des Journées du cinéma, ont facilité l'installation des Journées internationales du cinéma d'animation (Jica) à Annecy. Pierre Barbin, André Martin et Michel Boschet en sont les membres fondateurs. En 1956, les dirigeants du ciné-club sont présents à Cannes pour assister à la 1 édition des Jica en marge du grand festival. Les journées de l'animation organisées pendant le Festival de Cannes ne fonctionnaient pas : les stars du cinéma accaparaient l'attention du public et des journalistes.De la rencontre des deux équipes germe l'idée de l'installation d'un festival d'animation à Annecy.

En 1968, le festival, comme beaucoup d'autres cette année-là, est interrompu par les événements de mai. En 1971, des discussions entre les organisateurs et l'ASIFA (Association internationale du film d'animation) portent sur le mode de sélection des films présentés. Jusqu'en 1975, le Festival connaît une forte hausse de participation, aussi bien du côté des professionnels, délégations étrangères, abonnés et spectateurs. L'arrivée des premières images réalisées à l'aide de l'ordinateur crée une scission entre les artistes conventionnels et les plus modernes. À la fin des années 70, il est évident qu'il faut se tourner vers l'avenir, changer d'orientation.

En 1982, trois objectifs sont présentés à l'Assemblée Générale : "contribuer au développement du cinéma d'animation; assurer à Annecy une manifestation culturelle de niveau international, être un moyen d'action culturelle pour la ville, la région et la France". Jean-Luc Xiberras est engagé pour mettre en place ces nouvelles orientations. Le secrétariat permanent est ramené de Paris à Annecy.

En 1983, l'édition se tient au centre culturel de Bonlieu, permettant ainsi des projections simultanées, dans différents formats de films et sur différents supports. En parallèle, les prémices d'un marché du film apparaissent, ainsi que quelques conférences thématiques. Cette édition, en s'ouvrant à toutes les fonctions du cinéma d'animation, à toutes les cultures et techniques, se révèlent être un véritable succès. À la suite de l'ouverture de la compétition aux films de commande en 1983, une compétition est également créée pour les films de télévision en 1985.

En 1985, la première édition du Marché international du film d'animation (MIFA) est ainsi concomitamment, avec un rôle complémentaire. Les studios américains deviennent de plus en plus visibles sur la manifestation, avec notamment une diffusion auprès du public d'un programme de 9 films de Disney oscarisés, avec un hommage à la Warner en 1987, et avec l'accueil d'une délégation importante de Walt Disney Pictures pour la première fois en 1989. Entre 1983 et 1997, le nombre de participants passe de 900 accrédités en 1983 à 4300 en 1997 et le nombre de films reçus de 386 à 1271. Ces facteurs entraînent une couverture médiatique de plus en plus conséquente avec près de 300 journalistes présents à la fin des années 90. Dès 1983, le succès de la manifestation entraîne une augmentation du nombre de professionnels d'orientation culturelle ou économique.

En 1993, un écran géant en plein air est installé, sur le Pâquier pour des projections publiques.

En 1997, le conseil d'administration vote l'annualisation de l'événement pour plusieurs raisons : augmentation du volume de production, difficulté de la sélection, concurrence des autres événements, besoin d'un marché annuel et nécessité d'une équipe organisatrice permanente. Par la suite, Annecy conforte sa position de leader international des festivals compétitifs consacrés au cinéma d'animation.

Dans les années 2000, le festival d'Annecy est en plein essor, les avant-premières se multiplient et entraînent ainsi une plus grande couverture médiatique de l'événement. La France et l'Europe se lancent dans la production de films d'animation.

En 2006 est créé la CITIA, la cité de l'image en mouvement. Son projet s'articulant autour de 3 axes (culture, économie, formation), diverses actions sont mises en place au niveau local : exposition permanente sur le cinéma d'animation, développement des opérations d'éducation artistique, implantation de formations supérieures avec les Gobelins, l'école de l'image, création d'un événement dédié aux contenus et nouveaux supports, le Forum Blanc, mise en place d'un fonds d'aide à la production d’œuvres numériques, etc.

Le Festival compte











</doc>
<doc id="17739" url="https://fr.wikipedia.org/wiki?curid=17739" title="Santé">
Santé

La santé est un état de complet bien-être physique, mental et social, et ne consiste pas seulement en une absence de maladie ou d'infirmité.

Cette définition est inscrite au préambule de 1946 à la constitution de l'Organisation Mondiale de la Santé (OMS). Cette définition de l'OMS n'a pas été modifiée depuis 1946.
Elle implique que tous les besoins fondamentaux de la personne soient satisfaits, qu'ils soient affectifs, sanitaires, nutritionnels, sociaux ou culturels et du stade de l'embryoN

Elle se présente donc plutôt comme un objectif.

La santé est une notion relative, parfois non présentée comme corollaire de l'absence de maladie : des personnes porteuses d'affections diverses sont parfois jugées "en bonne santé" si leur maladie est contrôlée par un traitement. Dès le milieu du , des spécialistes du diabète ont ainsi parlé de « "santé insulinienne" ». Aujourd'hui, cet état de fait est même majoritaire dans les pays développés : il devient exceptionnel à partir d'un certain âge de ne pas avoir par exemple un trouble de la réfraction oculaire ou des problèmes d'hypertension.
"A contrario" certaines maladies peuvent être longtemps asymptomatiques, ce qui fait que des personnes qui se "sentent" en bonne santé peuvent ne pas l'être réellement.

« État de santé ressentie »: c'est l'un des indicateurs d'état de santé. Il est publié tous les deux ans depuis 2002, pour les pays de l'OCDE. Après une tendance à la hausse de 2002 à 2008, il a chuté de plusieurs points en 2010 "". En 2008, 74,9 % des hommes se jugeaient en bonne ou très bonne santé, contre 70,6 % en 2010. Pour les femmes ce taux est passé de 70,1 % à 66,5 %.

La santé est un état de complet bien-être physique, mental et social, qui ne consiste pas seulement en l'absence de maladies ou d'infirmité.

Pour l'ONU, la santé reproductive est un droit, comme les autres droits de l'Homme.Cette notion récente évoque la bonne transmission du patrimoine génétique d'une génération à l'autre. Elle passe par la qualité du génome, des spermatozoïdes et des ovules, mais aussi par une maternité sans risque, l'absence de violences sexuelles et sexistes, l'absence de maladies sexuellement transmissibles (MST), la planification familiale, l'éducation sexuelle, l'accès aux soins, la diminution de l'exposition aux perturbateurs endocriniens, etc.

Un certain nombre de polluants (dioxines, pesticides, radiations, leurres hormonaux, etc.) sont suspectés d'être, éventuellement à faibles ou très faibles doses, responsables d'une délétion de la spermatogenèse ou d'altération des ovaires ou des processus de fécondation puis de développement de l'embryon. Certains sont également cancérigènes ou mutagènes (ils contribuent à l'augmentation du risque de malformation et d'avortement spontané).

Les soins de santé reproductive recouvrent un ensemble de services, définis dans le "Programme d’action de la Conférence internationale sur la population et le développement" (CIPD) tenue au Caire (Égypte) en septembre 1994 : conseils, information, éducation, communication et services de planification familiale ; consultations pré et postnatales, accouchements en toute sécurité et soins prodigués à la mère et à l’enfant; prévention et traitement approprié de la stérilité ; prévention de l’avortement et prise en charge de ses conséquences ; traitement des infections génitales, maladies sexuellement transmissibles y compris le VIH/SIDA ; le cancer du sein et les cancers génitaux, ainsi que tout autre trouble de santé reproductive ; et dissuasion active de pratiques dangereuses telles que les mutilations sexuelles féminines.

La santé mentale peut être considérée comme un facteur très important de la santé physique pour les effets qu'elle produit sur les fonctions corporelles. Ce type de santé concerne le bien-être émotionnel et cognitif ou une absence de trouble mental. L'Organisation mondiale de la santé (OMS) définit la santé mentale en tant qu'. Il n'existe aucune définition officielle de la santé mentale. Il existe différents types de problèmes sur la santé mentale, dont certains sont communément partageables, comme la dépression et les troubles de l'anxiété, et d'autres non-communs, comme la schizophrénie ou le trouble bipolaire.

Un déterminant de santé est un facteur qui influence l’état de santé d'une population soit isolément, soit en association avec d’autres facteurs.
L'hygiène est l'ensemble des comportements concourant à maintenir les individus en bonne santé. Ils demandent de pouvoir notamment faire la part entre les "bons microbes" et ceux qui sont pathogènes ou peuvent le devenir dans certaines circonstances. Ces circonstances l'hygiène cherche à les rendre moins probables, moins fréquentes ou supprimées. 
Après une phase hygiéniste, dont l'efficacité de court terme est indiscutable, sont apparus une augmentation des allergies, des maladies auto-immunes, des antibiorésistances et des maladies nosocomiales jugées préoccupantes. La recherche de juste équilibre entre exposition au risque et solution médicale usuelle est rendue difficile dans un contexte d'exposition accrue à des cocktails de polluants complexes (pesticides en particulier) et perturbateurs hormonaux, de modifications sociétales et climatiques planétaires (cf. maladies émergentes, risque pandémique, zoonoses, risque de bioterrorisme, etc.).

La lutte contre les infections nosocomiales à l'hôpital, ou contre les toxi-infections alimentaires par exemple, est née après la découverte de l'asepsie sous l'influence par exemple de Ignàc Semmelweiss ou Louis Pasteur.
Les comportements individuels et collectifs sont de toute première importance dans la lutte contre les épidémies ou les pandémies.

Cette discipline de l'hygiène vise donc à maîtriser les facteurs environnementaux pouvant contribuer à une altération de la santé, comme la pollution par exemple, avec des problèmes paradoxaux à gérer : par exemple, l'amélioration des conditions d'hygiène semble avoir paradoxalement pu favoriser la réapparition de maladies comme la poliomyélite et diverses maladies auto-immunes et allergies.

De nombreux facteurs de risque sont intrinsèquement liés au mode de vie. Les soins corporels, l'activité physique, l'alimentation, le travail, les problèmes de toxicomanie, notamment, ont un impact global sur la santé des individus.

De nombreux risques et dangers sont liés au domaine de la santé , l'évolution de l'Homme et également les changements de son mode de vie ne sont pas sans conséquences. L'alimentation et les nouvelles technologies sont également des facteurs de risques en France et dans le reste du monde.
Les rythmes, les cadences de travail ; les gestes inadaptés sont des facteurs très importants sur la santé. Ils entraînent des troubles psychosomatiques et parfois des handicaps pour la vie.

Quatre facteurs permettraient d'allonger considérablement la durée de la vie : absence de tabac, consommation d'alcool égale ou inférieure à un demi verre par jour, consommation de 5 fruits et légumes par jour, exercice physique d'une demi-heure par jour. Le tout donnerait une majoration de l'espérance de vie de 14 ans par rapport au non-respect de ces facteurs.

Du strict point de vue de l'alimentation, de nombreuses études concordantes concluent qu'une alimentation exclusivement végétarienne permet de limiter les risques de cancer et de maladies cardio-vasculaires, et donc d'avoir une espérance de vie en bonne santé plus longue. Les études mettent à la fois en évidence les bénéfices d'une alimentation riche en légumes et fruits et les risques relatifs liés à la consommation de viande, poisson et produits laitiers. Les compléments alimentaires synthétiques ne seraient absolument pas nécessaires.

D'autres pistes sont explorées pour allonger la durée de vie en bonne santé: le jeûne, le jeûne intermittent et la restriction calorique.

Par ailleurs, l'« hygiénisme moral » trans-national débuté au (à ne pas confondre avec la médecine alternative créée par Herbert Shelton) est une doctrine contre le « relâchement des mœurs », ce qui serait le meilleur moyen de garantir la santé. C'est ce courant qui a par exemple déclaré la lutte contre la syphilis ou l'alcoolisme comme priorité nationale. (C'est également lui qui déclare que si les obèses sont gros, c'est qu'ils sont gourmands et paresseux, ou encore que les fumeurs n'ont pas de volonté; Il semble persister dans certaines politiques et campagnes d'information et d'éducation des citoyens à l'hygiène).

C'est un domaine (parfois nommé « "santé environnementale" ») qui se développe depuis la fin du , à la suite de la prise de conscience du fait que l'environnement, notamment lorsqu'il est pollué, est un déterminant majeur de la santé.

La pollution aiguë ou chronique, qu'elle soit biologique, chimique, due aux radiations ionisantes, ou due aux sons ou la lumière (ces facteurs pouvant additionner ou multiplier leurs effets) est également une source importante de maladies.

Dans l'Union européenne, la Commission a adopté (11 juin 2003) une « "stratégie Communautaire en matière de santé et d'environnement »", traduite le 9 juin 2004, en un « "Plan d'Action" » (2004-2010), qui vise notamment les maladies dites "environnementales". Cela concerne l'asthme et les allergies respiratoires, en cherchant plus généralement à « "mieux prévenir les altérations de la santé dues aux risques environnementaux »" (dont l'exposition aux pesticides et à leurs résidus). Des systèmes de veille sanitaire permanente doivent identifier les (dont nanotechnologies, OGM, maladies émergentes, impacts des modifications climatiques, etc.) et en évaluer l'impact sanitaire selon des actions réalisées au niveau communautaire mais aussi national. Un « "plan d'action environnement et santé" » va être développé afin de mettre en œuvre cette stratégie; De plus un processus de consultation a été initié. Le plan d'action vise à faire le point sur les connaissances scientifiques existantes et à évaluer la cohérence et les progrès réalisés dans l'installation du cadre législatif communautaire en matière de santé et d'environnement. Un nouveau système d'information sur la santé est prévu « "qui fonctionnera également dans le domaine de l'environnement" » et veut devenir « "la plus importante source de données fiables pour l'évaluation de l'impact des facteurs environnementaux sur la santé" ». Ces aspects seront coordonnés avec les systèmes de réaction rapide et une approche intégrée « "visant à juguler les déterminants environnementaux de la santé" ».

En ce qui concerne plus spécifiquement la France, un premier Plan national santé-environnement a été lancé en 2004 et un second en 2009, à la suite du Grenelle de l'environnement. Le bilan des actions menées devrait être fait en 2013.

La santé publique désigne à la fois l'état sanitaire d'une population apprécié via des indicateurs de santé (quantitatifs et qualitatifs, dont l'accès aux soins) et l'ensemble des moyens collectifs susceptibles de soigner, promouvoir la santé et d'améliorer les conditions de vie.

Dans les sociétés traditionnelles ("primitives"), la santé relève généralement autant de l'individu que du groupe. Elle est intriquée avec les croyances animistes et religieuses, et le rôle des guérisseurs (chamans, sorciers, etc.) qui utilisent à la fois la pharmacopée locale, le toucher et des pratiques relevant de la magie, de la divination, ou de la psychologie.

En Europe, l'organisation des soins est restée jusqu'au très majoritairement dépendante d'initiatives privées et d'œuvres caritatives : le rôle des institutions religieuses a été longtemps prédominant, l'assistance aux malades étant considérée comme une œuvre de charité.

Cependant, en Flandre par exemple, des mesures de salubrité sont prises par les magistrats (équivalent du maire) de différentes villes : l'Ordonnance de Bruges de 1464 impose le nettoyage des rues une ou deux fois par semaine. Et tous les jours dès 1632, ainsi que l'obligation de dégager les égouts, l'Ordonnance de Lille de 1470 demande que les immondices soient dégagées des chemins encerclant Lille (cette tâche sera prise en charge par la ville en 1668) et le magistrat de Bruges fait démolir des maisons pour cause d'insalubrité en 1485.

Les cimetières étant source de miasmes et d'infection, Louis XVI en France prend un Édit le 10 mars 1776 qui défend d'enterrer dans les églises et les chapelles.

À partir du , la maladie cesse progressivement d'être considérée comme une fatalité et le corps redevient un sujet de préoccupation. Ce mouvement concerne d'abord les élites, puis s'étend progressivement à l'ensemble de la société. La santé devient alors un droit que les États se doivent de garantir.

Le développement de l'industrialisation est un second facteur qui tend à expliquer le développement de la santé publique : d'une part pour de simples critères de productivité des ouvriers (médecine du travail), d'autre part par crainte des émeutes avec la pression des syndicats. Enfin la Première et la Seconde Guerre mondiale contribueront au développement de la prise en charge médicale de masse et à la mise en place de politiques d'assistance sociale : c'est la naissance de la notion d'État-providence. Après la pandémie de grippe espagnole de 1918, la santé publique prend une dimension mondiale avec l'Organisation mondiale de la santé (OMS). L'éco-épidémiologie se développe pour mieux suivre les zoonoses transmissibles à l'homme, via notamment une collaboration avec l'organisation des Nations unies pour l'alimentation et l'agriculture plus connue sous le terme anglais de « Food and Agriculture Organization » (FAO) et de l'Organisation mondiale de la santé animale préalablement créée par l'arrangement international du 25 janvier 1924 sous le nom d'Office international des épizooties (OIE) sous l'égide de l'ONU. L’Europe tend à prendre plus en compte l'importance du domaine de la santé.

La notion de santé publique regroupe plusieurs champs :

Les règles en matière de santé font l'objet de textes internationaux édictés par l'OMS ou la FAO (Codex alimentarius pour l'alimentation).

L'Union européenne a produit de nombreuses directives, règlements ou décisions pour protéger la santé des consommateurs ou d'animaux consommés.

La promotion de la santé telle que définie par l'OMS est le "processus qui confère aux populations les moyens d'assurer un plus grand contrôle sur leur propre santé, et d'améliorer celle-ci". "Cette démarche relève d'un concept définissant la "santé" comme la mesure dans laquelle un groupe ou un individu peut d'une part réaliser ses ambitions et satisfaire ses besoins, et d'autre part évoluer avec le milieu ou s'adapter à celui-ci."

La santé est prise en compte par le droit, y compris du point de vue des Conditions de travail.

Les crises sanitaires sont des pandémies importantes, qui touchent entre une dizaine de personnes (cas des crises très médiatisées qui touchent les pays développés, comme certaines crises alimentaires) et des millions de personnes. Elles peuvent avoir des coûts économiques, sociaux et politiques considérables.

L'OMS a d'ailleurs été créée pour qu'une pandémie telle que celle produite par la grippe espagnole ne se reproduise pas avec les mêmes effets (30 à 100 millions de morts selon les sources).

Les sommes en jeu dans le domaine de la santé sont considérables, tant pour les coûts induits par les maladies, les pollutions et l'absentéisme, que par le marché des soins et des médicaments. (En 2002, le marché mondial du médicament a été évalué à 430,3 milliards de dollars, contre 220 milliards en 1992). Le marché pharmaceutique a augmenté de 203 milliards d'euros. Et la consommation médicale progresse plus rapidement que le PIB dans les pays développés.

Des crises sanitaires telles qu'une pandémie peuvent avoir des coûts économiques, sociaux et politiques considérables.

Différents médias, se sont spécialisés sur les thèmes de la santé.






</doc>
<doc id="17743" url="https://fr.wikipedia.org/wiki?curid=17743" title="Varicelle">
Varicelle

La varicelle, classique sous sa forme de maladie infantile éruptive fréquente, en milieu tempéré, touche plus tardivement l'adulte en milieu tropical où elle est tout aussi caractérisée par sa très grande contagiosité, exposant ainsi femme enceinte et fœtus. Elle traduit la primo-infection par le virus varicelle-zona ou VZV, virus de la famille "Herpesviridae". Ce n'est qu'en milieu tempéré et sans doute urbain, loin de l'équateur, sauf vaccination, qu'elle survient spontanément dans plus de 90 % des cas chez l'enfant entre 1 et 15 ans. Sa période d’incubation est de 14 jours en moyenne (de 10 à 21 jours). Il existe un vaccin qui n'est pas recommandé en routine en France. Généralement bénigne chez l'enfant bien portant, elle peut être redoutable et mortelle chez l'adulte non immunisé, l'immunodéprimé, la femme enceinte et le nouveau-né.

En 1553, l'italien Giovanni Ingrassia aurait été le premier à distinguer la varicelle de la scarlatine, d'autres attribuent cette distinction à l'allemand Daniel Sennert, dans les années 1610. En 1764 ou 1772, , professeur de l'Université de Göttingen lui donne son nom de "varicella" en la considérant, comme tous les auteurs de son temps, comme une forme atténuée de variole ("variola lymphatica" selon Boissier de Sauvages).

L'anglais William Heberden est le premier, en 1785, à donner une description précise de la varicelle pour la distinguer de la variole ; la démonstration de cette distinction est faite par Desoteux et Valentin ("Traité de l'inoculation", 1799) : la variole ne protège pas de la varicelle, de même la vaccine ne protège pas non plus de la varicelle. 

En 1832, Jean-Louis Alibert inclut la varicelle dans son groupe des dermatoses à exanthème contagieux.

En 1924, T.M. Rivers et W.S.Tillett démontrent la nature virale de l'agent de la varicelle. En 1952, le virus de la varicelle est isolé à partir de cultures cellulaires par Thomas Weller.

Dès 1892, Janus von Bokay signale que des cas de varicelle surviennent en contact étroit avec des personnes atteintes de zona. En 1925, Karl Kundratitz montre que l'inoculation du contenu de vésicule de zona provoque une varicelle chez le sujet non immunisé. En 1943, Joseph Garland suggère que le zona pourrait être une réactivation du virus varicelle acquis plus tôt dans la vie. En 1958, Thomas Weller démontre que les virus de la varicelle et du zona sont les mêmes (virus varicelle-zona).

En 1974, Takahashi, de l'Université d'Osaka, réussit à atténuer une souche de virus varicelle, dite souche OKA, souche vaccinale utilisée dans les vaccins contre la varicelle.

À partir de 1984, les études de génétique moléculaire confirment que le virus du zona est bien une réactivation d'un virus varicelle latent.

Le virus de la varicelle-zona, comme son nom l'indique, est à l'origine de la varicelle et du zona. Il fait partie du groupe des "Herpesviridae". La première infestation entraîne le tableau de la varicelle puis le virus se réfugie dans les ganglions nerveux sensitifs où il peut rester latent durant des décennies. Sa réactivation secondaire est responsable du zona.

Le virus est présent dans le nez et la gorge avant l'éruption et dans les vésicules au cours de cette dernière. La contagiosité débute entre un et deux jours avant l'éruption et se poursuit pendant la phase d'apparition de l'éruption. Elle peut être prolongée dans les formes graves.

Le virus pénètrerait dans l'organisme à travers les voies respiratoires, rejoignant les ganglions lymphatiques pour s'y multiplier, puis se disséminerait après la période d'incubation, dans la gorge et la peau.

Dans les zones tempérées, plus de 90 % des adultes ont eu la varicelle durant l'enfance ou l'adolescence (le plus souvent entre 1 et 9 ans). La maladie se déclare souvent plus tard dans les zones tropicales.

L'incidence est plus élevée en hiver et au printemps. La surveillance de l'évolution de l'incidence en France est effectuée par le réseau Sentinelles de l'INSERM.

Les formes les plus tardives sont sensiblement plus graves.

Le malade est contagieux pendant une période allant de 1 à 4 jours avant l'apparition des boutons jusqu'à l'assèchement des boutons soit une période allant de 5 à 7 jours après l'éruption.

Le malaise général et la fièvre peu élevée (environ ) peuvent apparaître quelques heures avant l’éruption cutanée. Cette éruption cutanée inclut : petites macules (rosées sur les peaux blanches) apparaissent initialement. Elles vont vite se recouvrir de vésicules en gouttes de rosée, qui dans les trois jours vont se dessécher et former une croûte. Il peut rester des lésions hypopigmentées transitoires, ou des cicatrices. Généralement, ces lésions apparaissent en premier sur le cuir chevelu, puis sur le thorax et les muqueuses, ensuite sur les membres, avec respect des régions palmo-plantaires, et enfin au visage. Les différentes séries de lésions décalées dans le temps font que coexistent les différents types de lésions sur tout le corps. L'importance de l'éruption est très variable d'un individu à l'autre.

Les autres symptômes peuvent inclure : difficulté à s’alimenter en raison des vésicules qui se forment parfois dans la bouche, fortes démangeaisons (prurit) et toux importante dans certains cas. Des symptômes similaires à ceux de la grippe peuvent s'y associer : céphalées, douleur abdominale et sensation générale de fatigue et parfois même une conjonctivite.

Il est le plus souvent clinique devant l'aspect de l'éruption et la notion d'un contact dans les deux semaines qui précèdent avec un autre malade. À titre exceptionnel, le diagnostic peut être confirmé par la recherche du virus dans les vésicules.

La recherche d'anticorps contre la varicelle (sérologie) peut être faite mais il existe quelques réactions croisées avec les anticorps dirigés contre les autres herpèsvirus.
Cette recherche peut être utile afin de cibler les personnes à vacciner (absence d'anticorps).


Bien que bénigne dans la très grande majorité des cas, la varicelle peut se compliquer, en particulier chez les sujets immunodéprimés, les nourrissons, les adultes, les femmes enceintes.

La complication la plus fréquente est la surinfection bactérienne favorisée par le grattage des lésions cutanées, comme l'impétigo ou l'érysipèle.


Certains évoquent, à terme, une discrète majoration du risque de survenue de sclérose en plaques. Cependant en 2013, il n'existe pas vraiment d'argument pour une piste infectieuse dans la genèse de la sclérose en plaques.

Chez la femme enceinte, le risque, dans les 20 premières semaines, est de contaminer le fœtus qui peut développer une varicelle congénitale. Après la , si l'enfant est contaminé, il est susceptible de présenter un zona dans les premières semaines ou mois de sa vie.

Chez la femme enceinte cette maladie est grave pour la femme et le fœtus si la mère n'a pas eu la varicelle, sachant que la maladie reste souvent inapparente. Entre 97 et 99 % des femmes sont en fait immunisées à l'âge adulte. Chez le fœtus, la varicelle peut provoquer des malformations si la maladie est contractée avant cinq mois. Chez le nouveau-né, une varicelle congénitale néonatale peut survenir si sa mère a eu la varicelle quelques jours avant ou après la naissance. Cette varicelle congénitale néonatale est très grave avec une mortalité de 20 %.

Chez les formes banales de l'enfance, la maladie n'est pas grave et ne relève que de la prise en charge des symptômes : fièvre, démangeaisons.
Lors de la phase éruptive, il est important d'éviter que l’enfant se gratte (couper les ongles au ras, voire utilisation de moufles chez le petit enfant), car cela peut provoquer une surinfection et . Un antihistaminique peut éventuellement être prescrit par un médecin ou un professionnel de la santé contre les démangeaisons.

Il est également important d'éviter le contact prolongé avec l'eau (l'eau favorisant la macération et donc le risque de surinfection), privilégier les douches aux bains. Informer l'entourage du cas. Donner à boire en abondance. Éviter le contact avec des immunodéprimés, des femmes enceintes et adultes n’ayant pas été infectés lors de leur enfance, et des personnes âgées ayant au contraire été infectées par le passé (risque de zona).

En France, l’éviction scolaire légale, ou de collectivité, a été supprimée. La contagion commence deux à quatre jours avant l'éruption et jusqu'au stade de croûtes. La durée d'incubation totalement silencieuse dure de quatorze à seize jours. Si un médecin est consulté, il prescrira des soins locaux à l'eau tiède et au savon, éventuellement des antiseptiques. En cas de surinfection (impétiginisation) uniquement seront envisagés des antibiotiques. Il est très important de ne pas donner d'anti-inflammatoires contenant de l'acide acétylsalicylique, type aspirine, qui est formellement contre-indiqué du fait du risque – rare – de syndrome de Reye ni d'anti-inflammatoire stéroïdien (corticoïdes, aussi bien en application locale que par voie orale – ce qui arrive chez les enfants souffrant d’eczéma sévère –), les anti-inflammatoires non stéroïdiens de type ibuprofène sont déconseillés en rapport avec un risque de survenue ou d'aggravation d'infections microbiennes.

Ne pas utiliser de crèmes, gels, talc, pommades, qui augmentent le risque de surinfection par macération.

Dans les formes graves, un traitement antiviral est prescrit : l'aciclovir est régulièrement efficace, avec des résistances exceptionnelles.

La vaccination se fait en une injection chez l'enfant de moins de 12 ans, et en deux injections espacées d'un à deux mois, chez l'enfant plus âgé. Elle peut être faite de manière isolée, ou groupée (vaccination anti-varicelle, rubéole, oreillons et rougeole).

Chaque année aux États-Unis, la vaccination évite quelques milliers d'hospitalisations ; le taux d'hospitalisations est passé de 2,5/100 000 cas en 1995 à 1/100 000 cas en 2002. De même, une vaccination faite précocement après un contact avec une personne porteuse du virus peut diminuer sensiblement le risque de développer la maladie et faire en sorte que cette dernière soit moins grave.

L'efficacité varie de 95 % à 100 %, et en cas de varicelle, cette dernière serait sensiblement moins grave.

D’autres alternatives sont possibles dans le but de prévenir et d’atténuer l’infection le virus. L’immunisation passive est l’une de ces solutions. Le premier type d’immunisation passive est faite avec du plasma humain qui contient des anticorps antiviraux à des titres élevés. Cela va permettre à des personnes non immunisées d’être protégées contre le virus sans risquer des complications. Le second type d’immunisation, IVIG, est semblable au premier. Ce qui les distingue, ce sont la concentration d’anticorps ainsi que la voie d’administration. Le VZIG est donné par voie intramusculaire donnant une absorption d’environ 50 % alors que le IVIG est donné de façon intraveineuse, à 100 % d’absorption.

Des antiviraux peuvent également être donnés. L’aciclovir, par exemple, est un analogue de nucléoside bloquant la réplication virale. Son action est principalement au niveau de la phosphorylation de la thymidine kinase. Cela va causer une inhibition de la polymérase du virus. Leur but est principalement d’atténuer les symptômes liés à l’infection par le VVZ.



</doc>
<doc id="17744" url="https://fr.wikipedia.org/wiki?curid=17744" title="Joannes Brønsted">
Joannes Brønsted

Joannes Nicolaus Brønsted est un chimiste danois né le à Varde et mort le à Copenhague. Il est connu pour ses travaux sur la réaction chimique, et la formulation en 1923 de la théorie de Brønsted-Lowry des acides qui étend la théorie d'Arrhenius. Celle-ci définit les acides comme substances qui ont tendance à donner un proton et les bases comme substances qui ont tendance à accepter un proton. Cette théorie a été publiée à quelques mois d'intervalle par Brønsted et Lowry. Comme ils obtinrent les mêmes conclusions de manière indépendante, leurs deux noms furent associés à celle-ci.

Il reçut son diplôme de chimie réactionnelle en 1899 et son doctorat en 1908 à l'université de Copenhague, où il fut directement nommé professeur.

Pendant la Seconde Guerre mondiale, il s'opposa aux nazis, et fut élu à cet égard au parlement danois en 1947. Il n'y siégea cependant pas pour cause de maladie et mourut peu après, le à Copenhague (Danemark)


</doc>
<doc id="17749" url="https://fr.wikipedia.org/wiki?curid=17749" title="Affaissement et effondrement miniers">
Affaissement et effondrement miniers

Les affaissements et effondrements miniers sont des phénomènes mécaniques résultant des comblements spontanés ou provoqués des vides souterrains laissés par l'exploitation minière. Ils font partie des risques miniers (et quand le risque s'est exprimé des « séquelles minières »).

Ces comblements conduisent à deux types de désordres en surface :

L'affaissement est relativement lent et progressif (pouvant durer des décennies). 

Il se produit lorsque les terrains sont plutôt plastiques et que la profondeur d'exploitation est importante par rapport à l'épaisseur de la "taille". Il résulte le plus souvent d'un choix délibéré d'exploitation (méthode dite "« du foudroyage »" (exploitation totale du minerai à l’aide d’un coffrage marchant). Ce phénomène est par exemple observé à grande échelle dans le bassin minier du Nord-Pas-de-Calais.

L'effondrement est au contraire plus rapide, voire brutal.

Il se traduit en surface par une variation instantanée de la topographie locale (cuvette d’affaissement), voire - lorsque la profondeur de l’excavation est faible relativement à son épaisseur - à un trou béant en surface qualifié de « "fontis" » (à ne pas confondre avec les dolines qui sont dues à un phénomène naturel de dissolution karstique). 

L’effondrement peut être délibéré mais il est généralement la conséquence accidentelle de l’évolution des chambres à piliers après l’abandon de l’exploitation (cas de certaines mines lorraines ou des carrières souterraines de craie en Normandie par exemple). La capacité de résistance des piliers se dégrade en effet sensiblement à long terme : leur aire de soutènement peut diminuer par écaillement et leurs propriétés mécaniques peuvent changer en présence d'eau. Selon l'Ineris , en France, les "piliers" et cavités souterraines situés au niveau de la battance du plafond de la nappe pourraient être rendus plus vulnérables dans un futur proche, à cause des dérèglements climatiques qui selon les météorologues induiront des pluies plus fortes et fréquentes en hiver, et des sécheresses plus fréquentes en été.

Pour le public, sur le moment, un effondrement peut être confondu avec un tremblement de terre. Les sismographes et l'identification de l'épicentre permettent de les différencier des vrais séismes. le BRGM a listé les "faux séismes" survenus en France, ils sont souvent dû à des affaissements miniers.

L'effondrement peut intervenir par rupture brusque du toit de l'exploitation. 

Il peut se produire longtemps après la fin de l'exploitation (comme avec l'« effondrement tardif » du Petit Clos près de Saint-Étienne qui s'est produit plus de 100 ans après l'exploitation de couches situées à moins de de profondeur, avec dans ce cas, en outre, un feu de mine (combustion du charbon dans des cavités abandonnés). Des effondrements et affaissements sont fréquents dans tous les types de mines au-dessus de cavités anciennes ou récentes, ou de cavités naturelles, mais les feux de mines aussi appelés feux de « vieux travaux » sont plus rares et spécifiques des gisements d'hydrocarbures.

L’exploitation minière exige souvent un pompage d’assèchement ("exhaure") pour rabattre la nappe phréatique et éviter l'inondation des galeries et autres excavations. 
Au terme de l’exploitation, si l'on ne poursuit pas l’exhaure, le niveau de la nappe remonte jusqu'à trouver un nouvel état d’équilibre hydrogéologique (le temps nécessaire au rééquilibrage du niveau phréatique pouvant varier de quelques mois à quelques décennies, voire plus d'un siècle dans les mines profondes (celles du Nord-Pas-de-Calais par exemple).

L'effet de l'eau sur la stabilité mécanique des exploitations minières abandonnées est complexe, car pouvant se traduire positivement et négativement. 

L'exploitation houillère a duré plus de 270 ans ; du premier puits ouvert en 1620 (dans le Boulonnais) à la production industrielle massive des années 1720 à 1990. Elle a concerné des veines épaisses de quelques mètres au maximum, et souvent à grande profondeur (plusieurs centaines de mètres). On estime qu'environ 2,3 milliards de tonnes de charbon ont été ainsi extraites de ce bassin, laissant des cavités plus ou moins importantes sous environ 280 communes (122 dans le Nord et 158 dans le Pas-de-Calais) . 

Sauf très localement quelques exploitation en galeries soutenues par des piliers épargnés, la méthode d’exploitation consistait à enlever tout le charbon facilement disponible. Les galeries étaient initialement remblayés (jusqu’en 1930), période à partir de laquelle les galeries ont été traités selon la "technique du foudroyage", plus rapide et bien moins coûteuse que le remblayage. On laissait simplement s'affaisser le plafond des galeries après enlèvement de leur soutènement. Selon la DRIRE, l'essentiel des mouvements de sol (retrait - gonflement des sols) étaient stabilisés cinq ans après l’arrêt des travaux, ce que semblent confirmer les relevés topographiques faits dans les années 1990. 

Au-dessus des grandes zones d'effondrement, de vastes cuvettes se sont constituées, atteignant jusqu'à de profondeur (en règle générale, on estime que l'affaissement total atteint 80 % des épaisseurs cumulées des différentes veines exploitées) se sont alors formées au-dessus des veines exploitées. 

Ces déformations sont si larges qu'elles n'ont pas eu conséquences majeures sur les constructions (habitations, ouvrages d'art, etc.) situées au cœur des zones affaissements. Mais en périphérie, au contraire, des mouvements de bascule et de traction et l'étirement des sols ont entraîné de graves dommages et souvent la ruine des structures bâties. Les canalisations ont été touchées aussi (certains réseaux d'assainissement ont même vu une inversion de leur sens d'écoulement, et se sont rompus, aggravant la pollution du sous-sol et de la nappe phréatique de l'ancien bassin minier du Nord et du Pas-de-Calais, notamment par les nitrates. Cela d'autant plus que, conscientes de la vulnérabilité des réseaux et de leur manque d'efficacité en de pareilles circonstances, les compagnies minières s'étaient souvent dispensées d'en construire (hors centres urbains). 

Les puits ont théoriquement tous été remblayés. Mais des « "bouchons" » de remblais s'effondrent parfois, notamment à la suite de la remontée de la nappe. Ils sont alors stabilisé par injection de béton à la base du cône d'effondrement. 14 des 600 puits répertoriés dans ce bassin se sont ainsi effondrés, sans victimes à ce jour. 10 à 15 % des puits sont situés sous des zones aujourd'hui habitées et 200 environ ne sont pas géoréférencés avec exactitude, à cause de pertes d'archives (« "Certains sont remblayés depuis le , les plans dont nous disposons pour les situer sont très approximatifs.".».
D'une façon générale, les affaissements miniers sont à l'origine de modifications irréversibles des écoulements de surface qui ont nécessité la construction de stations de relevage des eaux usées et pluviales. Cette situation a compliqué le développement urbain puisque les populations sont exposées aux inondations en cas de défaillance des équipements. 

Tout au début du , Charbonnages de France gérait encore 54 stations de dénoiement par relevage des eaux. Les problèmes hydrauliques dans l'ancien Bassin minier du Nord-Pas-de-Calais sont encore suffisamment présents pour que le schéma directeur d'aménagement et de gestion des eaux (SDAGE) lui consacre un volet spécifique. Il s'agit à la fois de préoccupations liées aux affaissements miniers, eaux de surface (postes de relèvements) et « remontée » de nappe phréatique mais également de préoccupations liées à la mise en communication de la nappe du carbonifère avec la nappe phréatique de la craie.

À titre d'exemple, à Noyelles-sous-Lens, il a fallu rehausser plusieurs fois les berges du canal de Lens (la Deûle) pour maintenir son écoulement en dépit des affaissements miniers. Les terrains voisins sont désormais six mètres en dessous du niveau du canal. Une station de relèvement a dû être construite ; elle pompe non seulement les eaux pluviales qui n'ont plus d'exutoire gravitaire, mais aussi la nappe phréatique aujourd'hui résurgente dans ce secteur et donc source potentielle d'inondation elle aussi.

La situation est identique sur l'autre rive, à Loison-sous-Lens où existe un autre poste de relèvement.

L'exploitation minière (environ 800 millions de tonnes extraites de 1856 à 2004 est désormais terminée, mais elle a laissé, du fait de la méthode employée "par chambres et piliers" de grands volumes de galeries abandonnées (environ 200 millions de m de vides résiduels).

Ces galeries et cavités se trouvent à des profondeurs variant, selon les zones de quelques mètres à près de deux cents mètres de profondeur. La nappe remontante les ennoie progressivement depuis l'arrêt des pompages d'exhaures (en 2006). 

La méthode d'exploitation était basée sur le principe que, en laissant des piliers de dimensions suffisantes, ces piliers seraient capables de résister dans le temps, sans s'effondrer. Les experts de l'époque avaient cependant sous-estimé la perte de résistance de ces piliers dans le temps.

Des sinistres récents, à partir de 1996 à Auboué notamment, ont amené à reconsidérer complètement le problème et à étudier le risque de mouvements et affaissements de terrain. 
Des études menées depuis 1997 ont permis de caractériser plusieurs types de risques :

L'exploitation du charbon en Belgique remonte au Moyen Âge pour les bassins du Borinage et de Liège. À partir du et jusqu'à la seconde moitié du , le charbon est extrait dans les bassins du Borinage, du Centre, de Charleroi-Basse Sambre et de Liège. En Flandre, le charbon est extrait à partir de 1907 dans la Campine limbourgeoise. L'exploitation cesse de manière générale dans les années 1950-1960 et définitive en 1984 en Wallonie et en 1992 en Flandre.

Durant leur période d'activité, ces charbonnages ont un impact important sur la surface, causant de nombreux "dégâts miniers". L'affaissement de terrain constitue le type de "dégât minier" qui suscite le plus de réaction de la part des habitants de la surface, notamment à cause des dommages causés par ces affaissements aux terrains et aux constructions.

Les personnes lésées peuvent recourir aux tribunaux civils pour régler le différend qui les lient aux charbonnages. Les charbonnages, soit par conciliation, soit par contrainte juridique, sont tenus de réparer financièrement ou physiquement les dégâts causés par leur activité extractive. Toutefois, en 1934, la situation change.

En 1934, la ville de Gosselies est déclarée « sinistrée » à la suite des affaissements causés par le Charbonnage du Grand-Conty et Spinois. Les deux tiers des bâtiments de la ville sont déclarés comme sévèrement endommagés et menaçant ruines. Le charbonnage se trouve dans l'incapacité de payer le montant des dommages. L'État décide d'assurer une grande partie des dédommagements afin de faire face à cette catastrophe.

La catastrophe de Gosselies se trouve à l'origine de la création du Fonds national de garantie pour la réparation des dégâts houillers et de la prise en charge par l'État belge de la question des affaissements de terrains et des « dégâts miniers ». 

La loi du 12 juillet 1939 institue un Fonds national de garantie pour la réparation des dégâts houillers « destiné exclusivement à pourvoir, en cas d’insolvabilité des concessionnaires de mines de charbon, à l’exécution des obligations qui leur incombent en vertu de l’article 58 des lois sur les mines, minières et carrières, coordonnées par l’arrêté royal du 15 septembre 1919 ». Le Fonds de garantie est un établissement d’utilité publique géré par un conseil d’administration. La moitié de ses membres sont nommés par les concessionnaires, l’autre moitié par le ministre chargé des mines. Ce ministre préside le conseil et est assisté dans ses décisions par un comité permanent des dommages miniers.

Ce fonds est alimenté par les concessionnaires des charbonnages en fonction de
leur production. Neuf dixièmes de leurs contributions alimentent leur compte personnel (dit Fonds A), le dernier dixième est versé dans un compte commun (dit Fonds B) qui intervient si le fonds A s’avère insuffisant. Le Fonds national de garantie pour la réparation des dégâts houillers intervient dans les demandes d’indemnisation si la concession responsable des dommages a participé à l’alimentation du fonds pendant au moins trois ans et si cette concession a cessé ses activités extractives sur le site concerné. 

Les compétences du Fonds national de garantie sont transférées en 1947 au ministère du Combustible et de l’Énergie. En 1949, le Fonds est transféré au ministère de la Coordination économique. Après la suppression de ce ministère la même année, le Fonds passe au sein du ministère des Affaires économiques.

À cause des événements liés à la Seconde guerre mondiale et à la Reconstruction, le Fonds ne devient véritablement actif qu'à partir de 1952, date à partir de laquelle il est régulièrement alimenté par les charbonnages. 

Le Fonds national de garantie pour la réparation des dégâts houillers est supprimé le 31 décembre 1997.

Les archives relatives à ces thématiques ont fait l'objet d'une recension dans le Guide GARDEN des Archives de l'État en Belgique (Guide des archives relatives à l'environnement en Belgique entre 1700 et 1980, produites par les institutions publiques).

Le fontis est l'apparition soudaine en surface d'un entonnoir de quelques mètres de rayon et quelques mètres de profondeur. Les dimensions du fontis dépendent de l'importance du vide et de la nature des terrains qui le séparent de la surface. Le fontis fait suite à une dégradation progressive de la voûte d'une galerie qui remonte peu à peu dans le recouvrement, jusqu'à percer au jour. Le fontis ne se produira pas si la galerie est suffisamment profonde, car le foisonnement des blocs du toit vient combler le vide avant qu'il n'atteigne la surface. Le risque de fontis peut également être écarté si un banc épais et résistant arrête la dégradation progressive.

L'affaissement progressif peut survenir au-dessus d'une exploitation par chambres et piliers. Il se traduit par la formation en surface d'une cuvette de quelques dizaines à quelques centaines de mètres de diamètre. Au centre de la cuvette les terrains descendent verticalement. Sur les bords, les terrains se mettent en pente avec un étirement sur les bords extérieurs (ouverture de fractures) et un raccourcissement sur les bords intérieurs (apparition de bourrelets).
L'affaissement de la surface se produit généralement progressivement en quelques jours ou en quelques mois selon une dynamique propre au contexte minier et géologique.

Les bâtiments en surface sont sensibles à la mise en pente des terrains ainsi qu'aux effets d'extension dans la zone d'étirement et de compression dans la zone de raccourcissement.
Les effets sont d'autant plus élevés que l'amplitude de l'affaissement au centre de la cuvette est grande et que la profondeur des travaux miniers est faible.

Les bâtiments sont d'autant plus vulnérables qu'ils sont longs et élancés.

Dans certains cas, la ruine de l'édifice minier ne se fait pas progressivement mais on observe l'effondrement en bloc de l'ensemble des terrains compris entre le fond et la surface. L'effondrement de la surface se produit alors de manière dynamique, en quelques secondes. Une forte secousse tellurique est ressentie. Les bords de la zone affectée sont plus abrupts que dans le cas de la cuvette d'affaissement, des crevasses ouvertes y apparaissent.

Pour qu'un effondrement brutal se produise, deux conditions au moins doivent être remplies :

De nombreux affaissement deviennent des points bas topographiques qui - sauf pompage et relevage des eaux - peuvent devenir des zones humides d'intérêt écologique. 

Ces zones, si elles sont pérennes, sont alimentées passivement (ou activement, dans le cas d'un pompage d'exhaure) par au moins l'une des trois voies suivantes :
Ces eaux et zones sont souvent plus vulnérables à diverses pollutions, étant donné leur origine et contexte de fonctionnement écologique, mais elles peuvent aussi jouer un rôle épurateur important pour les nitrates, phosphates et matières organiques (fonction de lagunage naturel). 
Par contre les métaux lourds ou polluants organiques persistants peuvent s'accumuler dans les sédiments et contaminer le réseau trophique(bioaccumulation, bioconcentration...)

Toutes les infrastructures locales aériennes (poteaux électrique, routes), terrestres, aquatiques ou souterraines (réseaux de câbles et fibres optiques enterrées, tuyaux d'eau et de gaz, égouts, etc.) 

Les bâtiments en surface sont plus ou moins affectés selon leur type de fondation et la gravité du phénomène. Les dégâts présentent des formes différentes selon qu'ils adviennent sur une zone d'étirement (fentes ouvertes) ou sur une zone de compression (raccourcissement/écrasement).

Plus l'amplitude de l'affaissement au centre de la cuvette est importante, et moins la couche de sous-sol située au-dessus des travaux miniers est épaisse, plus les dégâts seront marqués et importants .

Le risque d'effondrement brutal est jugé relativement stabilisé dans les bassins houillers français, sans certitude absolue, mais les mouvements de terrain dans les bassins ferrifères peuvent encore se produire. Ils constituent donc des risques que l'État doit gérer : cela se traduit en particulier par 
Il ne semble pas y avoir de problème d'acidification minière en France, mais ce phénomène peut avoir de graves conséquences là où il existe (sur un site aux États-Unis, un lac extrêmement acide (pH 1) s'est ainsi formé, l'acidité du milieu favorisant la circulation des métaux lourds toxiques).

Restent des problèmes liés à la pollution des sols et donc des nappes, et au fait que les affaissements ont souvent fracturé les réseaux de distribution d'eau potable et les réseaux d'égouts qui fuient abondamment.

Enfin dans les bassins houillers dont celui du Nord-Pas-de-Calais, une production continue de méthane (CH, dit "grisou" en zone minière) perdurera longtemps. Dans le Nord de la France (plus grand bassin minier souterrain exploité du monde), une partie est récupérée par Méthamine (GIE racheté par Gazonor en 2007) et injectée dans le réseau de gaz, mais aux extrémités est et ouest du bassin, une certaine quantité de grisou s'enfuit probablement dans l'air, or le méthane est 21 fois plus actif pour l'effet de serre que le CO.




</doc>
<doc id="17750" url="https://fr.wikipedia.org/wiki?curid=17750" title="Tiers monde">
Tiers monde

L'expression tiers monde, ou tiers-monde, lancée en 1952, se rapporte à l'ensemble des pays africains, asiatiques, océaniens ou du continent américain en carence de développement. Ce terme est considéré comme obsolète par certains au profit de celui de pays les moins avancés (PMA).

Les termes premier monde, second monde et tiers monde ont été employés pour regrouper les nations de la Terre en trois grandes catégories. Ces trois termes ne sont pas apparus simultanément. Après la Seconde Guerre mondiale, l’OTAN et le Pacte de Varsovie ont été considérés comme les deux grands blocs. Le nombre de pays faisant partie de ces deux blocs n’étant pas fixé de manière précise, on s’est finalement aperçu qu’un grand nombre de pays ne rentraient dans aucune de ces deux catégories. 

En 1952, le démographe français Alfred Sauvy invente le terme « tiers monde » pour désigner ces pays. La fameuse expression « Tiers Monde », est initialement publiée dans un article de Sauvy : . L'expression « le tiers monde », du fait de son caractère générique ne doit toutefois pas occulter les spécificités historiques et le contexte socio-politique de chacun des pays censés y correspondre.

Le tiers-monde décrit la réalité complexe, transitoire et chaotique s'inscrivant dans le décalage croissant qui nait entre monde traditionnel et monde moderne à partir de la révolution industrielle (qui débute en Angleterre vers la fin du ). On remarque cependant qu'à cette époque, si en Amazonie, en Afrique, en Asie, les hommes vivent dans un âge proche de l’âge de la pierre taillée, d’autres en Chine et en Inde se trouvent à un niveau de vie supérieur à celui de l’Angleterre du -. L’historien Christopher Alan Bayly l’a éminemment montré dans son ouvrage « La naissance du monde moderne ». 

Certains insistent sur le fait qu'il s'agit d'une réalité très hétérogène, et concluent à l'existence de « plusieurs » tiers mondes. Cela en fonction des perspectives envisagées.

Dans l'inégalité économique, l'expression correspond à l'ensemble des pays pauvres, soit les pays les moins avancés et les pays en développement. Dans cet esprit, le quart monde (proposé par Joseph Wresinski en 1969) fait référence à cette couche de population la plus défavorisée, ne disposant pas des mêmes droits que les autres, et existant dans tous les pays, qu'ils soient riches ou pauvres.

Dans les rapports nord-sud avec « des Suds » faisant face à « un Nord » plutôt occidental et compris comme « développé », l'expression fait alors référence à des « pays dépendants du monde capitaliste », ou des « pays appauvris et surexploités ». Ils « ont le trait commun de n'avoir pas connu, pour des raisons diverses, la révolution industrielle au », ou la prospérité qui a suivi la Renaissance en Europe, et favorisé la colonisation ou la domination des autres territoires. On notera également que cette vision doit être réactualisée avec l'apparition des pays émergents et des nouveaux pays industrialisés, ainsi que des organisations ou des regroupements à caractère économique et/ou politique (comme les pays pétroliers).

Dans la géopolitique comme Georges Balandier (en 1956), l'expression désigne « la revendication des tierces nations qui veulent s'inscrire dans l'Histoire ». À la suite de la décolonisation et de la Conférence de Bandung, certains de ces pays se sont regroupés au sein de l'organisation internationale du mouvement des non alignés.

L'expression « tiers monde » apparait, comme une formule, dans la chute d'une chronique de l'économiste et démographe français Alfred Sauvy en 1952, en référence au tiers état (de l'abbé Sieyès) français sous l'Ancien Régime. 

L'auteur de l'expression la désavoue cependant en 1988 dans un article du "Monde" : « Que l'on permette au créateur de l'expression tiers-monde, il y a déjà près de quarante ans, de la répudier, tant elle fait oublier la diversité croissante des cas. Englober dans le même terme les pays d'Afrique noire et « les quatre dragons » ne peut mener bien loin ».

Le terme est à nouveau très discuté après sa reprise par Georges Balandier en 1956, dans leur publication à l'INED (voir en bibliographie). Il désigne les pays du globe considérés alors comme « sous-développés ».

On interprète dès le début leur proposition, à tort (Balandier, 2003), comme le regroupement des pays n'appartenant ni au bloc occidental (Amérique du Nord, Europe de l'Ouest, Japon, Israël, Australie…), ni au bloc communiste (URSS, Chine, Europe de l'Est…). La chute du mur de Berlin et la dislocation de l'Union soviétique ont de toute façon rendu ce caractère obsolète.

En 2003, dans sa réponse à une question de Jean-Marc Biais « Peut-on encore parler de « tiers-monde », mot que vous avez inventé, en 1956, avec Alfred Sauvy ? », Balandier maintient son terme :

« Cette expression a connu un succès planétaire. Mais, souvent, elle a suscité des malentendus. Pour nous, il ne s'agissait pas de définir un troisième ensemble de nations, à côté des deux blocs (capitaliste et soviétique) en guerre froide. Non, c'était une référence au tiers état de l'Ancien Régime, cette partie de la société qui refusait de « n'être rien », selon le pamphlet de l'abbé Sieyès. Cette notion désigne donc la revendication des tierces nations qui veulent s'inscrire dans l'Histoire. Après une longue éclipse, l'initiative est reprise aujourd'hui par quelques pays en cours de modernisation : le Brésil, l'Inde, l'Afrique du Sud. Lors de la récente conférence de Cancun, ils ont affirmé une forte identité face aux puissances occidentales. N'est-ce pas le début d'une renaissance du tiers-monde ? ». 

Cependant, dans le cadre des sciences géographiques, démographiques, sociales ou économiques, l'expression « Tiers-Monde » est désuète depuis 1997 : on parle de "pays les moins avancés" (PMA).

Certains hommes politiques et économistes s'interrogent sur « la fin du tiers monde » dans la perspective d'un monde multipolaire où la pauvreté serait « combattue » (Robert Zoellick).

Effectivement, l'expression tiers monde est de plus en plus rarement utilisée en économie (voir la typologie économique des pays), bien que l'on parle toujours de la dette du tiers monde. Cependant, son usage perdure dans divers contextes (politiques, historiques, anthropologiques, sociologiques), mais y est critiquée comme étant, alternativement, idéaliste, révolutionnaire ou néo-impérialiste.

Plusieurs réunions, dont certaines sont dites « Sommets du mouvement des non alignés » ou d'autres « Conférences Tricontinentales », ont parfois réuni ces pays autour d'une politique commune : Conférence asiatique de New Delhi de 1947, Conférence asiatique de New Delhi de 1949, Conférence de Bandoeng de 1955, Conférence de Brioni de 1956, Conférence du Caire de 1957, Conférence de Belgrade de 1961, Conférence tricontinentale de La Havane de 1966, et Conférence d'Alger de 1973.

L'agriculture est, dans les pays du tiers-monde, un facteur économique primordial.







</doc>
<doc id="17752" url="https://fr.wikipedia.org/wiki?curid=17752" title="Bière">
Bière

La bière est une boisson alcoolisée obtenue par fermentation alcoolique d'un moût de produits végétaux amylacés tels que l'orge, le maïs, le riz (saké), la banane, le manioc... Ce moût est obtenu à l'issue d'une étape importante de la fabrication de la bière, le brassage, opération à l'origine des vocables brasseur et brasserie.
C'est la plus ancienne boisson alcoolisée connue au monde et la boisson la plus consommée après l'eau et le thé.

La bière actuelle (à partir du Moyen Âge) est généralement produite à partir d’eau, de malt et d'orge (parfois additionnée d'autres céréales). Le houblon, en particulier, apporte le parfum et l'amertume nécessaires à chaque bière et agit comme conservateur naturel. Cette boisson est consommée à la pression, en bouteille ou en canette. La consommation de bière est à l'origine de nombreux évènements festifs tels que la Fête de la bière, le Mondial de la bière, la Journée internationale de la bière ; mais suscite également de nombreux excès tels que des jeux à boire comme le bière-pong, le , le , la ou le barathon. Des versions très faiblement alcoolisées (variant de 2° à 0°) sont présentes sur le marché. Contrairement aux autres boissons « sans alcool », elles sont élaborées par les mêmes procédés que la bière classique. La production de bière est réalisée industriellement ou artisanalement dans une brasserie, tout en étant possible par le particulier.

L'histoire de la bière est intimement liée à celle de ses ingrédients, ainsi qu'aux avancées technologiques qui firent de cette boisson le breuvage que l'on connaît aujourd’hui. Les premières cultures de céréales, notamment de l'orge et de l'épeautre (une variété de blé), ont été attestées en 8000 en Mésopotamie. Tous les ingrédients étant disponibles dès cette époque, la bière pouvait donc exister et l'on estime son invention/découverte à 6000 Cependant, les preuves formelles de son existence, découvertes dans la province de Sumer, remontent au À cette époque, la bière, alors appelée « sikaru» (dont la traduction littérale est "pain liquide") était à la base de l'alimentation quotidienne. On la fabriquait par cuisson de galettes à base d'épeautre et d'orge que l'on mettait à tremper dans de l'eau, afin de déclencher la fermentation nécessaire à la production d'alcool, et que l'on assaisonnait avec de la cannelle, du miel ou toutes autres épices en fonction des préférences des clients. La bière, connue des peuples de Chaldée (maintenant Irak, Koweït) et d'Assyrie (Irak, Syrie, Liban, Palestine), devenue monnaie d'échange, commença sa dissémination. Des recherches archéologiques ont pu démontrer que les Provençaux brassaient déjà leur bière au .

Consommée en famille et utilisée comme moyen de paiement à Babylone, boisson des dieux en Égypte, la bière devint dans la Grèce antique (Diodore de Sicile dit qu'elle fut inventée par Dionysos) et dans l’Empire romain celle du pauvre, et le vin celle des dieux. Elle resta cependant la boisson de choix des peuples du Nord, Celtes et Germains. La préférence pour le vin se confirma dans l’Europe chrétienne au début du Moyen Âge, notamment grâce au concile d’Aix-la-Chapelle de 816 qui encouragea les viticultures épiscopales et monastiques dans le but de célébrer l’eucharistie. Il fallut attendre le pour voir le brassage de la bière y reprendre de l’importance, en particulier en Bavière. Par la suite, aux environs du , certains monastères (par exemple en Alsace et en Bavière) se spécialisèrent dans le brassage de la bière, bue par la population à la place d’une eau souvent non potable.

Aujourd’hui, la bière jouit d’un succès mondial en tant que boisson désaltérante et de dégustation. Ce succès remonte au où la maîtrise de la fermentation basse grâce à la réfrigération et la pasteurisation permirent la production de nouvelles variétés de bière ainsi que leur exportation.

La bière est une boisson qui intervient également dans de nombreuses recettes de cuisine à la bière comme ingrédient premier (exemple : soupe à la bière) ou secondaire apportant une caractéristique particulière au mets (exemple : carbonade flamande).

La bière est utilisée pour le lavage ou l’affinage de certains fromages (exemple : le Cochon 'nez et le Bergues). Elle a aussi été utilisée pour la conservation de la viande.

Elle sert de badigeon après chaulage des murs.

L'élaboration de la bière a évolué à travers les âges. Ce que l’on considérait comme de la bière il y a est sans aucun doute très éloigné de ce que nous connaissons aujourd’hui. Les « migrations » de ce breuvage à travers le monde et le temps ont obligé les brasseurs à adapter le mode de fabrication en fonction des évolutions techniques et des matières premières disponibles. Ce qui n’était autrefois qu’une sorte de « bouillie » alcoolisée plus proche des aliments solides que des boissons, est devenu, notamment grâce aux progrès de la microbiologie et des techniques industrielles au , la boisson limpide que l’on connaît aujourd’hui. Les méthodes de fabrication actuelles sont cependant très proches de celles de ces derniers siècles ce qui dénote une normalisation dans le processus de fabrication.

Pour produire de la bière, il faut certaines matières premières qui vont être transformées tout au long du processus de fabrication. Il est nécessaire de disposer :

D’autre part, on peut aussi utiliser :

Pour transformer ces matières premières par voies enzymatiques et microbiologiques, on utilise diverses techniques de chauffage et de trempage, ainsi que des levures afin de permettre la fermentation du moût (production d’alcool).

On distingue quatre types de fermentations :

Certaines bières, notamment en Belgique, subissent une nouvelle fermentation après la mise en bouteille. Une levure, potentiellement différente de la première, peut être ajoutée à cette occasion.

Il existe des bières triples ou "tripel", qui ne sont pas des bières ayant subi une triple fermentation, mais des bières pour lesquelles on a ajouté du sucre par rapport à une bière classique, cela vaut aussi pour les bières doubles ou "dubbel". Ces bières atteignent généralement un pourcentage relativement élevé d'alcool (plus de 7 %).

La couleur résulte des types de malts de spécialité utilisés. À peine 10 % de malt "black patent", mélangé à un malt de base, est suffisant pour produire une bière noire comme l’ébène :

Les bières sont naturellement troubles, cependant, la tendance actuelle tend à généraliser la filtration de la bière en post-fermentation, ce qui explique la limpidité de nos boissons actuelles. C’est notamment le cas des "pils" ou de la "Kölsch". À l’opposé de cette logique de marché, certains brasseurs continuent de produire des bières non (ou peu) filtrées. Les bières trappistes, qui sont fermentées une nouvelle fois durant leur période de garde, font partie de ces bières troubles. Une garde prolongée sans filtration permet d’obtenir une bière parfaitement limpide sans toutefois occasionner la perte de saveurs souvent obtenue lors de la filtration.

En France, on note généralement la bière par son degré d’alcool, mais également par d’autres degrés indiquant la proportion de céréales dans le moût :

Il ne faut pas confondre ces degrés entre eux. Par exemple l’Eku 28 titre 28° Balling et environ 11° d’alcool. On peut retenir que le degré alcoolique est généralement un peu plus du tiers du degré Balling.

On peut regrouper les bières par structure de goût :

La bière en tant qu'aliment possède à la fois une valeur nutritive et énergétique qui dépend du type de bière et du type de consommation qui en est fait : boisson ou ingrédient culinaire. 

Issue de l'orge commune, une céréale peu panifiable, la bière a toujours eu ce caractère nutritif de pain liquide depuis son usage par les moines lors du carême, ou encore sa confection domestique par des ménagères telle que Katharina von Bora au Moyen Âge. Certains brasseurs perpétuent cette tradition en refusant toute filtration et en proposant ainsi des bières plus rustiques (Kellerbier, Zwickelbier, Zoigl). 

Une tendance récente, représentée notamment par le gastronome suisse Harry Schraemli favorise en outre le développement de la cuisine à la bière.

En Égypte ancienne, les femmes utilisaient la bière à des fins cosmétiques ou dermatologiques (cette tradition est toujours vivante en République tchèque sous forme de bain de bière). En Grèce antique, Hippocrate utilisait la bière pour faciliter la diurèse et combattre la fièvre. Arétée la conseillait en cas de diabète et de migraine.

Au Moyen Âge, cet alcool était réputé pour stimuler l’humeur et l’appétit, calmer et favoriser le sommeil. La bière remplaçait aussi avantageusement l'eau souvent contaminée en ce temps, et jusqu'aux réformes des hygiénistes au , car les germes infectieux étaient détruits lors du brassage.

Au , la bière était encore fabriquée et vendue en pharmacie, additionnée de plantes telles que le gruit aux vertus diverses.

Au , la médecine dénonce les conséquences nocives de l’abus d’alcool et les médicaments ont remplacé l’alcool en tant que remède.

Pour des raisons pratiques, les consommateurs ont rapidement classé les bières afin de s’y retrouver parmi le nombre important de bières sur le marché. Il existe deux types principaux de classement : le « classement par couleur », et le « classement par fermentation ». Le « classement par couleur » correspond "uniquement" à la couleur de la bière, indépendamment de sa méthode de fabrication, de sa composition, ou de sa provenance. Au contraire, le « classement par fermentation » correspond au type de fermentation de la bière, ainsi que, dans certains cas, de la couleur.

Certaines catégories sont sans rapport avec les caractéristiques intrinsèques de la bière :

Certains spécialistes ainsi que diverses associations ont tenté d’établir un classement le plus complet possible des différents types de bières existants (voir article détaillé). Le classement créé conjointement par le Beer Judge Certification Program (BJCP) et l’Institut de la bière décrit par exemple 23 types et 78 sous-types de bières.

Il existe sept grands types de verres. Les flûtes pour servir les "pils", les verres calices pour servir les bières d’abbaye et trappistes, les godets de différentes grosseur pour la witbier, le lambic ( et gueuze) pour les ales britanniques, les chopes (ou bocks) pour les ales anglaises et pour le service de grandes quantités de bières lors de festivals, les tulipes pour certaines blanches, les ballons pour les bières liquoreuses et les verres fantaisistes (verre de cocher, verre en forme de botte).

La contenance des verres à bière oscille entre et . La contenance standard varie d’une région à l’autre, tout comme les appellations que l’on donne aux différentes contenances.

La bière est parfois servie au mètre : un présentoir de de long est percé de façon à pouvoir y présenter une douzaine de verres de (). Dans les pubs et les bars, on peut également trouver un mode de service original : la « girafe » ( photo ci-contre). La « girafe » est un cylindre transparent, d’une contenance de , monté sur un présentoir et muni d’un robinet pour assurer le service. Bien que le mot soit une expression entrée dans le langage courant, elle est en fait une marque déposée par la société PMP Innovation.

Il existe également des verres « trompeurs » en forme de botte, de spirale, que l’on peut trouver en Allemagne par exemple et qui sont utilisés lors des fêtes de la bière. Le contenu du verre se renverse sur le buveur si celui-ci n’y prend pas garde.

Depuis l’origine de la bière, le problème majeur a été de conserver et transporter ce liquide fragile. Au début, les Égyptiens et les Romains utilisaient des amphores en terre cuite, ce qui a permis le commerce de la « bière ». Mais le transport était délicat du fait de la relative fragilité de la terre cuite. L’utilisation par la suite du tonneau en bois inventé par les Gaulois permit d’améliorer la transportabilité.

Par la suite, les moyens de stockage n’ont guère évolué jusqu’au où les premiers fûts métalliques furent utilisés. Les fûts métalliques sont toujours utilisés par les débitants de boissons (de ) et par certaines brasseries pour les périodes de garde (jusqu’à ).

Le véritable engouement pour la consommation à domicile de la bière est arrivé grâce à l’invention de la bouteille de en 1949 suivi de près par la bouteille de et de la canette métallique en 1953. Mais, il existe de nombreuses autres déclinaisons de ces contenants individuels qui sont parfois endémiques à certaines régions du globe.

La bouteille de bière s’appelle également une canette.

La lumière, en provoquant la photolyse des isomères de l’humulone (un composé du houblon) contenu dans la bière, donne à cette dernière une odeur de mouffette. Seul un contenant de verre coloré ou – mieux encore – opaque, protège la bière adéquatement contre ce phénomène, ce qui explique la coloration de la plupart des bouteilles.

La chimie ayant permis la production de dérivés du houblon plus stables à la lumière, certaines marques vendent désormais leurs produits dans des bouteilles transparentes, dans le but de développer un marketing plus efficace.

Évolution des principaux pays producteurs de bière entre 2002 et 2013 en millions d'hectolitres :

La France est le cinquième producteur européen de bière avec une production de d’hectolitres en 2004 dont elle exporte 10 %. Le secteur brassicole génère près de d’euros de chiffre d’affaires annuel et entretient plus de . L’essentiel de la production est assurée en Alsace (60 % de la production française), en Lorraine et dans le Nord-Pas-de-Calais mais aussi en Bretagne et en Bourgogne.
En 2010, quatre entreprises (Kronenbourg, Heineken, Champigneulles, Saint-Omer) dépassent la production annuelle du million d'hectolitre et cumulent ensemble 88,6 % de la production nationale avec d’hectolitres.

Malgré un marché de petite taille et une forte tradition vinicole, plus de différentes sont produites sur le sol français. Il faut également noter qu’au , plus de ont existé en France, dont au moins rien que pour la région Nord/Pas-de-Calais. À noter également qu’avec en 1889, le Pas-de-Calais présentait une concentration brassicole par habitant jamais égalée depuis, ni en France, ni ailleurs dans le monde. Avec les d’hectolitres d’importation en 2004, la consommation moyenne par an et par habitant atteint les ce qui situe les Français parmi les plus faibles consommateurs de bière d’Europe. En effet, la consommation de bière en France a chuté de 30 % entre 1980 et 2010.

La Belgique est souvent reconnue comme étant le pays de la bière. Il s’y produit plus de mille bières différentes dont six des dix bières trappistes reprenant le logo officiel: Orval, Chimay, Westvleteren, Rochefort, Westmalle et Achel. La consommation annuelle par habitant est de en 2003. Le plus important brasseur en quantité est : Anheuser-Busch InBev (souvent appelé AB InBev) issu de la fusion de Anheuser-Busch et InBev.

La production mondiale, bien qu’en baisse ces dernières années, avoisine actuellement les d’hectolitres, dont 268 millions d´hectolitres consommés en Chine. Cette production est aux mains d’un nombre de plus en plus réduit de firmes internationales. 

En 2009, les ventes de bières en grande distribution se concentrent sur les marques suivantes :

En 2014 :

En 2007
En 2005 :

Compte tenu de la baisse de consommation dans les pays développés, le secteur connaît une opération de concentration rapide :

L'industrie de la bière connaît de nombreuses innovations, tant sur le produit, que sur le packaging.
Concernant les innovations autour du produit, les bières sans alcool ont vu le jour en 2000, les bières aromatisées (ex. la Pietra rouge en juillet 2014) et le concentré en sachet en février 2014
Concernant les innovations autour du packaging il y a eu les tonnelets sous pression et le kit de préparation à domicile



</doc>
<doc id="17754" url="https://fr.wikipedia.org/wiki?curid=17754" title="Thomas Lowry">
Thomas Lowry

Thomas Martin Lowry ( - ), était un chimiste anglais.

Il étudia la chimie de 1896 à 1912 au Central Technical College à Londres sous la direction de Henry Armstrong dont il fut l'assistant, un chimiste anglais qui s'intéressait tout particulièrement à la chimie organique mais aussi à la nature des ions et aux solutions aqueuses.

Puis il dirigea à partir de 1913 le département de chimie du Guy's Hospital à Londres, devenant ainsi le premier professeur de chimie de l'histoire des universités de Londres.

En 1914, il fut élu Membre de la Royal Society, et en 1920 il fut nommé premier professeur de chimie physique de l'université de Cambridge. Il est connu pour sa théorie des acides-bases qu'il formula en 1923 en même temps, mais de manière indépendante, que le chimiste danois Joannes Nicolaus Brønsted.

Il resta à Cambridge jusqu'à la fin de ses jours.


</doc>
<doc id="17755" url="https://fr.wikipedia.org/wiki?curid=17755" title="Tempera">
Tempera

Le terme tempera ou tempéra (du , « détremper ») désigne une technique de peinture basée sur une émulsion, qu'elle soit grasse ou maigre : peinture « a tempera ». Pour préciser la nature de l'émulsion, on énonce simplement les composants : tempera à l'œuf, tempera grasse à la colle de peau, etc. 

Cette technique de peinture a longtemps suscité une extrême confusion, les uns confondant détrempe et tempera, les autres restreignant uniquement ce terme à la peinture à l'œuf ou à la graisse. Aujourd'hui, la tempera, évoquant l'idée d'émulsion, est distinguée de la détrempe, peinture strictement aqueuse (mais incluant nécessairement un liant). Ce débat n’existe toutefois qu’en français, la plupart des autres langues utilisent le mot « tempera », quel que soit le liant.

Le terme « tempera » est également employé actuellement par quelques fabricants de peinture pour désigner la peinture ordinaire utilisée pour des affiches et qui est une forme bon marché de gouache (peinture à l'eau opaque) qui n'a rien à voir avec la véritable peinture tempera à l’œuf. 

La tempera est la principale technique de peinture à l'eau utilisée depuis des temps immémoriaux, notamment en Égypte, puis par les peintres d'icônes byzantines, puis en Europe durant le Moyen Âge.

Le procédé original est celui d'une peinture utilisant le jaune d'œuf, émulsion naturelle, ou l'œuf entier comme médium pour lier les pigments. On l'utilise sur du plâtre ou sur des panneaux de bois recouverts de nombreuses couches de gesso, enduit à base de colle de collagène et de carbonate de calcium dans le nord de l'Europe, ou sulfate de calcium dans le sud de l'Europe. 

Quand la peinture à l'huile se développa vers la fin du Moyen Âge, jusqu'en 1500 la tempera continua encore à être employée pendant un certain temps en tant que sous-couche recouverte par un vernis à l'huile translucide ou transparent. Cette technique transitoire mixte fut suivie par une technique de peinture à l'huile pure, qui remplaça presque totalement la tempera au . Cette technique était principalement utilisée pour des peintures religieuses.



</doc>
<doc id="17760" url="https://fr.wikipedia.org/wiki?curid=17760" title="Amber">
Amber

Amber ou "Amer"' (आमेर en hindi et en rajasthani) est une petite ville de l'Inde, l'ancienne capitale de l'État de Dhundhar, renommée en 1727 Jaipur lors du déplacement de sa capitale, dans le Rajputana.

Le nom d'Amber est mentionné pour la première fois par Ptolémée. Fondée par la tribu des Minas, elle est prise, en 1037, par les Rajputs Kachhwâhâ, qui en font leur capitale jusqu'à ce qu'ils l'abandonnent au profit de Jaipur.

Amber est située au débouché d'une gorge de montagne, dans laquelle se niche un lac.

Le vieux palais a été commencé par Man Singh. Le bâtiment principal est le Diwan-i-Khas construit par Mirza Râja.

La ville connaît depuis les années 1960 de nouveau une hausse de population : 6500 en 1961, 20.460 en 1991. Elle comprend de nombreux temples hindouistes comme le Jagat Shiromani (vers 1610), jaïns et la mosquée d'Akbar (1569) restaurée par Aurangzeb.


</doc>
<doc id="17762" url="https://fr.wikipedia.org/wiki?curid=17762" title="Administration territoriale">
Administration territoriale

L'administration territoriale est l'organisation institutionnelle et administrative d'une zone géographique, d'un pays ou d'une confédération de pays.

Elle est constituée d’autorités dont les décisions sont valables sur des portions du territoire appelées circonscriptions. Ainsi donc, c'est la portée de leurs décisions qui différencie l'administration centrale de l’État et l'administration territoriale de l’État. L'existence de l'administration territoriale de l’État se justifie par un constat d’évidence : un État ne saurait être administré uniquement à partir de sa capitale par les autorités administratives centrales. Celles-ci s'appuient donc sur des autorités placées à la tête de différentes portions du territoire national dénommées circonscriptions. Les autorités qui dirigent ces circonscriptions forment l’administration territoriale de l’État ; elles sont soumises au contrôle hiérarchique des autorités administratives centrales.

La façon dont est conçue l'administration territoriale peut fortement varier d'un pays à l'autre,

Le mode d'organisation territoriale peut aussi amener à des conflits de pouvoirs entre différents échelons. De même, il peut exister dans certains cas des échelons superposés sur un même territoire, sans que ces échelons n'aient les mêmes limites géographiques.

Une réflexion est actuellement en cours à l'échelle de l'Union européenne, pour instaurer un système d'eurodistricts, également appelés eurorégions. Ce dispositif aurait pour objectif de créer des liens administratifs entre des régions transfrontalières reflétant des intérêts communs.

Plus généralement, dans le cadre de la mondialisation, les grandes agglomérations prennent conscience de leurs intérêts propres et de leur potentiel international, au niveau économique (géographie économique). Par souci d'autonomie, elles commencent - comme l'avait prévu le futurologue Alvin Toffler - à tisser des relations transversales entre elles, indépendamment des régions et des états. Cela peut contribuer à faire évoluer l'organisation territoriale, dans les pays comme dans le monde, vers un système de pôles multiples et de maillage. Une architecture en réseau apparaît, même si sur le papier elle reste pyramidale.

Par la Charte européenne de l’autonomie locale, Le Congrès des pouvoirs locaux et régionaux a mis en place des règles communes aux pays membres du Conseil de l’Europe ayant apposé leur signature sur la Charte, le but étant de garantir l’acceptation et la protection de l’autonomie politique, administrative et financière des pouvoirs locaux. L’application de ces règles est réalisée selon le droit interne de chaque pays. Cette charte constitue une protection importante des droits des pouvoirs locaux qui occupent une place fondamentale dans le développement de la démocratie locale et l’efficacité dans l’administration. Dans ce sens, la Charte insiste sur plusieurs points dont l’insistance sur le fait que l’autonomie du pouvoir local doit être assurée par la loi et la Constitution, et sur le fait que les pouvoirs locaux doivent disposer de ressources financières proportionnelles à leurs besoins.

Les principales divisions administratives traditionnelles du territoire français sont les suivantes :
Les États-Unis étant une république fédérale, l'organisation territoriale dépend de la loi de chacun des 50 États fédérés.

Comtés : dans 48 États, la subdivision principale est le comté ("county"), appelé "parish" en Louisiane et "borough" en Alaska. Cependant, dans les États de Nouvelle-Angleterre, ce niveau n'a plus de rôle de gouvernement territorial. Excepté en Alaska, ce niveau de gouvernement couvre l'ensemble du territoire américain.

Municipalités : le niveau de gouvernement local en dessous du comté varie lui grandement selon les États, et possède des degrés d'autonomie divers. Excepté en Nouvelle-Angleterre, il ne couvre pas l'ensemble du territoire. Dans ce cas, c'est au comté que revient l'intégralité du gouvernement local.




</doc>
<doc id="17763" url="https://fr.wikipedia.org/wiki?curid=17763" title="Kurt Wüthrich">
Kurt Wüthrich

Kurt Wüthrich, né le à Aarberg en Suisse, est un chimiste suisse. Il est colauréat du prix Nobel de chimie de 2002.

Né à Aarberg en Suisse, Wüthrich étudie la chimie, la physique et les mathématiques à l'université de Berne, avant de poursuivre un doctorat sous la direction de Silvio Fallab à l'université de Bâle, où il soutient sa thèse en 1964. Il continue après son doctorat un travail avec Fallab pendant un court moment avant de partir travailler à l'université de Californie à Berkeley) de 1965 à 1967 avec , puis aux laboratoires Bell à Murray Hill de 1967 à 1969.

Wüthrich retourne en Suisse en 1969, à Zurich, où il commence sa carrière à l'École polytechnique fédérale de Zurich, et où il devient professeur de biophysique en 1980. Il reçoit le Prix Louis-Jeantet de médecine en 1993 et est lauréat du prix de Kyoto en 1998.

Il est lauréat de la moitié du prix Nobel de chimie de 2002 (l'autre moitié a été remise à Kōichi Tanaka et à John B. Fenn) .

Il est élu associé étranger à l'Académie française des sciences, le 3 avril 2000.



</doc>
<doc id="17764" url="https://fr.wikipedia.org/wiki?curid=17764" title="Louis-Philippe">
Louis-Philippe

Louis-Philippe (avec ou sans tiret) est un prénom composé français, formé à partir des prénoms et .

Ce prénom fut porté par un souverain français et par divers princes :



</doc>
<doc id="17766" url="https://fr.wikipedia.org/wiki?curid=17766" title="Vignoble de la vallée du Rhône">
Vignoble de la vallée du Rhône

Le vignoble de la vallée du Rhône est un vignoble français s'étendant de part et d'autre du Rhône, de Vienne au nord jusqu'à Avignon au sud, sur un total de 1317 communes.
C'est le deuxième vignoble en France quant au volume de production de vins d'appellation d'origine contrôlée, après le Bordelais.
Il s'étend sur six départements : Rhône, Loire, Ardèche et Gard, sur la rive droite du Rhône, Drôme et Vaucluse, sur la rive gauche. Ces départements font partie des régions Auvergne-Rhône-Alpes, Occitanie et Provence-Alpes-Côte d'Azur.

Du nord au sud, ce vignoble se divise en deux ensembles d'appellations :

S'y rajoutent les vallées des affluents du Rhône, d'abord celle de la Drôme qui forme le Diois (appellations clairette de Die, crémant de Die, coteaux-de-die et châtillon-en-diois), ensuite celle du Calavon (appellation ventoux), la rive droite de la Basse-Durance (appellation luberon) et enfin une partie du Gard (appellations costières-de-nîmes et clairette de Bellegarde).

Par vocation la vallée du Rhône a toujours été un passage privilégié entre le monde méditerranéen et l'Europe septentrionale ou atlantique.
Dès l'Antiquité, les Grecs s'infiltrent au cœur de la Gaule où ils pratiquent des échanges commerciaux. La culture de la vigne et du vin se poursuit avec l'arrivée des Romains en 125 avant notre ère.

Dès le , la concurrence entre les vignes reprend entre l'Italie et la Gaule narbonnaise. C'est dans ce contexte qu'il convient de dater la construction de la villa gallo-romaine du Mollard, à Donzère et les ateliers d'amphores de la région.
La plus importante unité viti-vinicole de l'antiquité, la "villa du Mollard" a été mise à jour au sud de Donzère. Elle s’étendait sur deux hectares. L’entrepôt des vins de 70 x contenait deux travées abritant 204 dolia disposés en six alignements ayant chacune une contenance de 1,2 hectolitre. À chaque extrémité, un grand fouloir de , y étaient adjoints deux pressoirs.

L’exploitation, qui a été datée entre 50 et 80 de notre ère, produisait hectolitres de vin par an. Le rendement des vignes romaines ayant été estimé à 12 hl/ha, le domaine possédait 300 hectares ce qui nécessitait le travail de 150 esclaves. Tout ou partie de sa production était expédiée par le Rhône en tonneaux, à l’exemple de la scène représentée sur la stèle de Saint-Pierre-ès-Liens de Colonzelle () toute proche. Située sur le porche d’un prieuré clunisien, elle représente le levage de quatre tonneaux et leur embarquement sur un navire marchand.

Ces amphores fabriquées sur place, servaient au transport des vins et des sauces de poisson. Ces découvertes archéologiques, alliées à une étude historique déjà ancienne, permettent de situer l'origine des Côtes du Rhône comme antérieure à bien d'autres régions viticoles françaises.

Les Romains créent la ville de Vienne, puis le vignoble de Vienne dont la renommée est grande.
Ils mettent en valeur la campagne viennoise avec d'immenses travaux de défonçage, de plantation de la vigne et de construction de murettes protégeant les terrasses. Les coteaux très accidentés de la rive droite séduisent les Romains - de la Côte Rôtie à Saint-Joseph - et s'annexent plus tard ceux de la rive gauche - Hermitage - Ils font de cette région une des plus belles de la Gaule narbonnaise. Dès le ils ont donné l'impulsion d'un vignoble commercial.

En 611 est fondé le monastère de Prébayon, réservée aux moniales. La charte accordée par Artemius, évêque de Vaison, à l'abbesse Rusticule, de Saint-Césaire d'Arles et compagne de la reine Radegonde, mentionne la présence de vignes dans ses domaines. C'est la plus ancienne trace écrite d'un vignoble lié à une appellation « Sablet » dans la région.

À Saint-Péray, le vignoble est attesté depuis 936. Le Cartulaire de Saint-Chaffre mentionne la donation à cette abbaye d'une "villa" et de ses vignes sise sous le "castrum" de Crussol.

Dès le , installés à Avignon, les papes firent appel aux vignobles de proximité pour leurs besoins. Clément V s’installa au pied du Ventoux, à Malaucène, près de la fontaine du Groseau, où il fit planter le premier vignoble pontifical. Jean XXII, le deuxième pontife avignonnais, fit bâtir le château de Châteauneuf-du-Pape. Il avait amené avec lui à Avignon des banquiers et des vignerons de Cahors dans le but de renforcer les richesses de la papauté. Les Cadurciens récupérèrent à Châteauneuf d’anciennes parcelles laissées par les templiers chassés par Philippe le Bel et plantèrent les premiers vignobles pontificaux. Au tout début, le vignoble de Châteauneuf ne fournit que quatre puis six tonneaux par an de vin papalin. Dès 1325 la production atteignit douze tonneaux. Trois ans plus tard Jean XXII pouvait partager sa récolte avec son neveu Jacques de Via, le cardinal-évêque d’Avignon. Les spécialistes ont calculé que le vignoble pontifical devait alors couvrir huit hectares. Ce pape fit venir son "vin nouveau" de Tournon (Hermitage), des "Costières" (Saint-Gilles, Nîmes, Beaucaire avec son cru renommé de "Cante-Perdrix"), de la "Côte du Rhône" (Roquemaure, Saint-Laurent-des-Arbres), du Comtat Venaissin (Carpentras), de l’État d’Avignon (Bédarrides) et de l’Enclave des Papes (Valréas). Son "vin vieux" provenait de Malaucène dont le vignoble fournissait chaque année sept saumées de vin liquoreux.

Benoît XII, son successeur, ancra encore plus radicalement la papauté en Avignon en décidant la construction du premier palais des papes dominé par la "Tour du Trouillas" (du pressoir). Homme austère et sévère, il garnit sa table uniquement des vins de la rive droite du Rhône.

Ce fut sous le pontificat de Clément VI, en 1344, que le premier terroir connu de Châteauneuf-du-Pape fut répertorié. Il était dit "Vieille Vigne" (de nos jours "Bois de la Vieille"). Innocent VI apprécia fort le Châteauneuf autant blanc que rouge comme en témoignent les comptes de la Révérende Chambre Apostolique, au cours de son pontificat. Aux vins de ses prédécesseurs, il ajouta ceux de Pont-Saint-Esprit, Bellegarde, Rochefort-du-Gard, Villeneuve-lès-Avignon et Tavel (Prieuré de Montézargues).

Urbain V donna une nouvelle impulsion au vignoble de Châteauneuf en ordonnant qu’y fut planté du raisin muscat. Ce qui n’empêcha point le pontife et sa Cour de découvrir et d’apprécier le vin d’Apt lors du concile qui s’y tint en juin 1365. De plus il donna une nouvelle impulsion au vignoble de Châteauneuf en ordonnant qu’y fut planté du raisin muscat.

Afin de préparer son départ à Rome, il fit passer un accord avec Marco Cornaro, le Doge de Venise, pour le libre passage des vins pontificaux dans les ports vénitiens. Ce qui lui permit, lors de son séjour italien, de 1367 à 1369, d’approvisionner la Cour romaine de vin de Saint-Gilles.

Grégoire XI resta fidèle aux muscats de Beaumes-de-Venise, Velorgues et Carpentras, dans le Comtat Venaissin, continua à commander des vins d’Apt, de Saint-Gilles et de la Côte du Rhône (Laudun, Bagnols-sur-Cèze). Et lors de son retour à Rome "Les vins paraissent avoir tenu une grande place et, à la veille du départ, on s’occupa tant d’assurer le service de la bouteillerie durant le voyage, que de garnir, en prévision de l’arrivée, les caves du Vatican". Le retour de la papauté à Rome n’empêcha point les différents pontifes qui se succédèrent sur le trône de Saint-Pierre de conserver l’habitude de se fournir en vins de Provence et du Comtat Venaissin.

Les marquent le progrès de la viticulture rhodanienne.
Au , la « Côte du Rhône » est le nom d'une circonscription administrative de la Viguerie d'Uzès (département du Gard) dont les vins sont réputés. Une réglementation intervient en 1650 pour protéger leur authenticité de provenance et garantir leur qualité.

Un édit du roi de France prescrit, en 1737, que tous les fûts destinés à la vente et au transport doivent être marqués au feu par les lettres « C.D.R. ». 
Ce n'est qu'au milieu du que la Côte du Rhône devient les Côtes du Rhône en s'étendant aux vignobles situés sur la rive gauche du Rhône. Cette notoriété, acquise au fil des siècles, est validée par les Tribunaux de Grande Instance de Tournon et d'Uzès en 1936.

Au dans les années 1930, sous l'impulsion du Baron Le Roy - homme audacieux et visionnaire - cette notoriété s'accentuera et prendra forme en 1937 par la consécration de l'AOC - appellation d'origine contrôlée - côtes-du-rhône.

L'AOC, c'est la reconnaissance d'une réalité et d'une tradition rassemblant plusieurs éléments : un ou plusieurs cépages, un terroir, un savoir-faire viti-vinicole.

Un organisme officiel national, l'INAO, fixe et contrôle les règles qui garantissent que les produits d'AOC sont conformes aux critères de : production, délimitation parcellaire, cépages, méthodes culturales, récoltes, vinification.
Pour avoir le droit à l'AOC, un vin doit de plus être soumis obligatoirement à une analyse et à une dégustation qui contrôlent sa typicité et sa qualité.

L'appellation côtes-du-rhône, créée par un décret de 1937, se répartit sur hectares sur les départements du Rhône, de la Loire, de l'Ardèche, de la Drôme, du Vaucluse et du Gard.

L'appellation côtes-du-rhône villages, créée par un décret de 1966, concerne 95 communes de l'aire géographique des départements :Ardèche, Drôme, Vaucluse et Gard.



En plus de ces appellations, il existe des vins de pays.

Le vignoble septentrional s’étend de Vienne à Valence au sud. Deux unités se distinguent au sein de ce vignoble. L’une se situe de Vienne à Montélimar. Ses vignes dominent le fleuve, c’est le secteur des Côtes du Rhône. L’autre se situe sur les versants face à la Drôme, c’est le secteur du Diois.

Le vignoble méridional est regroupé autour de la ville d’Orange entre Montélimar au nord et Avignon au sud. Il débute après le défilé de Donzère. Il repose sur des terroirs complexes et variés. Ce vignoble est délimité par des hauts reliefs :


Le climat de Lyon est de type semi-continental avec des influences méditerranéennes : les étés sont chauds et ensoleillés et les hivers rigoureux, la sensation de froid est renforcée par la bise. À Bron, la température moyenne annuelle a été, entre 1920 et 2008, de avec un minimum de en janvier et un maximum de en juillet. La température minimale y a été de - le et la plus élevée de le .
Le 19 août 2009, la température enregistrée à Lyon Bron est . 
La ville fut ce jour-là parmi les 5 villes les plus chaudes d'Europe.

L'ensoleillement y est de heures par an en moyenne, soit environ 164 jours par an.

Tournon-sur-Rhône bénéficie d'un climat tempéré dont la principale caractéristique est un vent quasi permanent qui souffle et assèche l'air le long du couloir rhodanien. Baptisé "Mistral" lorsqu'il vient du nord, il apporte beau temps et fraîcheur en été, mais une impression de froid glacial en hiver. Lorsqu'il provient du sud, il annonce généralement l'arrivée de perturbations orageuses. Il s'appelle alors "le vent du midi" ou "le vent des fous" car, pour certaines personnes, il rend l'atmosphère pénible à supporter, surtout en été.

À partir de cette latitude, l'influence du climat méditerranéen se fait directement sentir. L'ensoleillement annuel est élevé (environ heures à Valence, (estimation de Météofrance). Les étés y sont chauds et secs. La température moyenne du mois de juillet est de (Montélimar ). Les hivers froids sans excès s'inscrivent plutôt dans un climat de type semi-continental dégradé. La température moyenne du mois le plus froid (janvier) est ainsi de .

La pluviométrie annuelle est modérée : environ . Les pluies sont particulièrement importantes à la fin de l'été (particulièrement en septembre à cause de l'effet cévenol ou orage cévenol qui déverse des trombes d'eau).

Avignon, ville située dans la zone d’influence du climat méditerranéen, est soumise à un rythme à quatre temps : deux saisons sèches, dont une brève en fin d'hiver, une très longue et accentuée en été ; deux saisons pluvieuses, en automne, avec des pluies abondantes sinon torrentielles, et au printemps. Les étés sont chauds et secs, liés à la remontée des anticyclones subtropicaux, entrecoupés d’épisodes orageux parfois violents. Les hivers sont doux. Les précipitations sont peu fréquentes et la neige rare.

Selon Météo-France, le nombre par an de jours de pluies supérieures à par mètre carré est de 45 et la quantité d'eau, pluie et neige confondues, est de par mètre carré. Les températures moyennes oscillent entre 0 et selon la saison. Le record de température depuis l'existence de la station de l'INRA est de lors de la canicule européenne de 2003 le 5 août (et le ) et le . Les relevés météorologiques ont lieu à l'Agroparc d'Avignon.

Le vent principal est le mistral, dont la vitesse peut aller au-delà des 110 km/h. Il souffle entre 120 et 160 jours par an, avec une vitesse de 90 km/h par rafale en moyenne. Il souffle depuis le nord vers le sud de la vallée du Rhône.

En Provence et dans le Comtat Venaissin aucun vigneron ne se plaint du mistral - même violent - car celui-ci a des avantages bénéfiques pour le vignoble. Appelé le "mango-fango", le mangeur de boue, il élimine toute humidité superflue après les orages, dégage le ciel et lui donne sa luminosité, préserve les vignes de nombre de maladies cryptogamiques et les débarrasse d'insectes parasites.

La réglementation de l'appellation côtes-du-Rhône admet l'utilisation de 21 cépages dont 13 noirs et 8 blancs, certains à titre principal, d'autres à titre secondaire. Le muscat à petit grain est uniquement réservé à l'appellation Beaume-de-Venise à l'exclusion de tout autre.



Les décrets ont abouti à caractériser ... types d’appellation :
La surface de production est de hectares
La production annuelle est en moyenne de 3,5 millions hectolitres (465 millions de bouteilles) et provient de exploitations viticoles, dont la superficie moyenne est de 10 ha. Seules de ces exploitations sont des caves indépendantes ; les autres sont regroupées en coopératives.

C'est l'ensemble des opérations nécessaires à la transformation du moût (nom du jus de raisin) et à l'élaboration du vin. Certaines de ces opérations sont nécessaires, telle la fermentation alcoolique, et d'autres permettent d'affiner le profil du vin, tant au niveau aromatique (olfactif) que gustatif (goûts).
La vinification en rouge consiste à faire un pressurage après que la fermentation est commencée. Pendant toute cette phase, le moût est en contact avec les matières solides de la vendange. Celles-ci sont très riches en tanins, matières colorantes, odorantes, minérales et azotées. Ces substances vont alors se dissoudre plus ou moins dans le moût et se retrouver dans le vin.

C'est la cuvaison pendant laquelle les sucres se transforment en alcool (fermentation alcoolique) et le jus se voit enrichi par les composants du moût. Plus la macération est longue, plus la coloration du vin sera intense. Se disolvent également les tanins, leur taux sera aussi fonction du temps de la cuvaison. Plus elle sera longue, plus les vins seront aptes à vieillir. Durant cette phase, se produit une forte élévation de la température. Celle-ci est de plus en plus contrôlée par la technique de maîtrise des températures.

Dans la vinification en blanc la fermentation se déroule en dehors de tout contact avec les parties solides de la vendange (pépins, peaux du raisin, rafles). Le but de cette vinification est de faire ressortir le maximum des arômes contenus d'abord dans le raisin, ensuite en cours de fermentation, enfin lors du vieillissement.

L'extraction du jus et sa séparation des parties solides peuvent être précédés par un éraflage, un foulage et un égouttage, pour passer ensuite au pressurage. Mais ces phases sont évités par nombre de vinificateurs pour éviter l'augmentation des bourbes. Le choix se porte sur une extraction progressive du jus puis un débourbage qui permet d'éliminer toute particule en suspension. Là aussi, encore plus que pour une vinification en rouge, s'impose la maîtrise des températures lors de la fermentation alcoolique. Elle se déroule entre 18 et 20° et dure entre 8 et 30 jours selon le type de vin désiré.

La vinification en rosé se produit par mécération, limitée dans le temps, de cépages à pellicule noire avec possible ajout de cépages blancs. Le vin rosé n'a pas de définition légale. Mais ses techniques de vinification sont très strictes et n'autorisent en rien en Europe le mélange de vin rouge et blanc. La première se fait par saignée. C'est le jus qui s'égoutte sous le poids de la vendange - entre 20 à 25 % - et qui va macérer durant 3 à 24 heures. La seconde est le pressurage. Une vendange bien mûre pourra colorer le jus et sa vinification se fait en blanc. La troisième méthode implique une courte macération à froid. Puis sont assemblés jus de goutte (première méthode) et jus de presse (seconde méthode). Obtenu par ses trois types de vinification, où la maîtrise des températures est une nécessité, un vin rosé a une robe qui s'apparente à celle d'un vin rouge très clair, plus le fruit et la fraîcheur des vins blancs.

La vinification des vins effervescents (champagne, mousseux, crémant) a pour but de permettre d'embouteiller un vin dont les sucres et les levures vont déclencher une seconde fermentation en bouteilles. Celle-ci et son bouchon doivent pouvoir résister au gaz carbonique qui se forme sous pression. C'est lui au débouchage qui provoquera la formation de mousse.

On utilise un vin tranquille auquel est ajouté une liqueur de tirage, constituée de levures, d'adjuvants de remuage (pour faciliter la récupération et l'éjection du dépôt au dégorgement) et de sucre (de 15 à /l) selon la pression finalement désirée. La bouteille est rebouchée hermétiquement et déposée sur des clayettes afin que les levures transforment le sucre en alcool et en gaz carbonique. 
La vinification des vins doux naturel se fait à partir de moûts de raisins frais auxquels est rajouté de l'alcool. C'est le mutage. Il doit être fait pendant la fermentation pour obtenir des vins doux naturels. Avec cette façon de procéder, les vins sont d'une grande richesse alcoolique (15° acquis minimum) et d'un fort taux de sucre.

Les titres alcoométriques volumiques (TAV) sont exprimés en pourcent volume (ou : % vol). Ce sont les degrés du vin.

L'œnotourisme recouvre de nombreuses activités de découverte : dégustation des vins, visite de caves, rencontre avec les propriétaires, découverte des métiers et techniques de la vigne, connaissance des cépages, des terroirs, des appellations, de la gastronomie locale. À cet aspect festif s'ajoutent les activités sportives et de loisirs : promenades et randonnées dans les vignobles.

Pour les touristes, une charte de qualité des caveaux de dégustation a été mise en place dans la vallée du Rhône pour l'ensemble des vignobles par Inter Rhône. Elle propose trois catégories différentes d'accueil en fonction des prestations offertes par les caves.

La première - dite accueil de qualité - définit les conditions de cet accueil. Un panneau à l'entrée doit signaler que celui-ci est adhérent à la charte. Ce qui exige que ses abords soient en parfait état et entretenus et qu'il dispose d'un parking proche. L'intérieur du caveau doit disposer d'un sanitaire et d'un point d'eau, les visiteurs peuvent s'asseoir et ils ont de plus l'assurance que locaux et ensemble du matériel utilisé sont d'une propreté irréprochable (sols, table de dégustation, crachoirs, verres).

L'achat de vin à l'issue de la dégustation n'est jamais obligatoire. Celle-ci s'est faite dans des verres de qualité (minimum INAO). Les vins ont été servis à température idéale et les enfants se sont vu proposer des jus de fruits ou des jus de raisin. Outre l'affichage de ses horaires et des permanences, le caveau dispose de fiches techniques sur les vins, affiche les prix et offre des brochures touristiques sur l'appellation.

La seconde - dite accueil de service - précise que le caveau est ouvert cinq jours sur sept toute l'année et six jours sur sept de juin à septembre. La dégustation se fait dans des verres cristallins voire en cristal. Accessible aux personnes à mobilité réduite, il est chauffé l'hiver et frais l'été, de plus il dispose d'un éclairage satisfaisant (néons interdits). Sa décoration est en relation avec la vigne et le vin, une carte de l'appellation est affichée. Il dispose d'un site internet et fournit à sa clientèle des informations sur la gastronomie et les produits agroalimentaires locaux, les lieux touristiques et les autres caveaux adhérant à la charte. Des plus les fiches techniques sur les vins proposés sont disponibles en anglais.

La troisième - dite accueil d'excellence - propose d'autres services dont la mise en relation avec d'autres caveaux, la réservation de restaurants ou d'hébergements. Le caveau assure l'expédition en France pour un minimum de vingt-quatre bouteilles. Il dispose d'un site Internet en version anglaise et le personnel d'accueil parle au moins l'anglais.

Rouges Secs
Blancs Secs
Roses
Mousseux
Doux Naturels

Ils sont notés : année exceptionnelle , grande année , bonne année ***, année moyenne **, année médiocre *.

Soit sur 90 ans, 24 années exceptionnelles, 26 grandes années, 16 bonnes années, 22 années moyennes et 2 années médiocres.

En dépit d'une conjoncture économique mondiale difficile, les ventes se maintiennent à un niveau élevé. Il a été commercialisé 376 millions de bouteilles de vins de la Vallée du Rhône. Toutes appellations confondues, la production a mis en marché : 
Dans ce cadre les ventes sont assurées par trois grands secteurs :
Sur le marché français la demande reste forte avec 75, 8 % des ventes, soit 282 millions de bouteilles commercialisées. La restauration absorbe 47 millions de cols, soit 14, 7 %, tandis que la consommation par ménage représente 271 millions, soit 85, 3 % des ventes. Les principaux secteurs de distributions sont :
Ce marché absorbe 24,2 % des ventes soit 101 millions de bouteilles. L'Union Européenne reste le plus important client avec une demande qui dépasse 50 %. Les principaux pays acheteurs sont :
L'association Vins en fêtes de la vallée du Rhône





</doc>
<doc id="17767" url="https://fr.wikipedia.org/wiki?curid=17767" title="Commune (France)">
Commune (France)

En France, la commune est une collectivité territoriale, consistant en un territoire administré par une municipalité, formée par un conseil municipal, le maire ainsi que, le cas échéant, un ou plusieurs adjoints. Elle constitue l'échelon de base des divisions administratives du territoire français. Les communes sont régies par le de la Constitution de 1958 et la deuxième partie du code général des collectivités territoriales. Les trois plus importantes communes, Paris, Lyon et Marseille, ont un statut particulier visé par la loi du , dite « loi PLM » : elles ont notamment été découpées en arrondissements dits "municipaux", mais l'essentiel des pouvoirs a été laissé aux municipalités centrales.

La commune conduit des politiques publiques dans le cadre de l'exercice des compétences déterminées par la loi (notamment l'état civil, la gestion des écoles maternelles et élémentaires, la voirie municipale, la fourniture du service de l'eau) ainsi que toutes celles qu'elle juge nécessaire pour l'intérêt général, en vertu de la clause de compétence génétale (par exemple la garde d'enfants via les crèches municipales, des services de bibliothèques et médiathèques, les piscines municipales, etc.). Leurs compétences sont aujourd'hui largement partagées avec les intercommunalités, notamment les établissements publics de coopération intercommunale tels que les communautés de commune, communautés d'agglomérations et métropoles. Ces groupements de communes conduisent un nombre croissant de politiques et interrogent l'avenir des communes elles-mêmes en tant que collectivités autonomes.

Historiquement, la commune française est l'héritière de la paroisse de l'Ancien Régime ou de la « communauté » (circonscription fiscale qui portait aussi le nom de « paroisse fiscale », dans les villes à deux ou plusieurs clochers, ou de « collecte »). Au , la commune peut correspondre à une pluralité d'espaces humains : espace rural avec un centre (bourg ou village), ville isolée ou partie d'agglomération multicommunale, notamment.

Sa superficie et sa population peuvent ainsi varier considérablement. Paris est la commune la plus peuplée avec au recensement de 2015, tandis que la commune habitée la moins peuplée, Rochefourchat, a un seul habitant. , soit 85,0 % d'entre elles, possèdent moins de , et représentent 23,2 % des habitants du pays.

D'après la direction générale des collectivités locales (DGCL), au , la France comptait dont en France métropolitaine (Corse comprise) et 129 dans les départements et régions d'outre-mer (Mayotte comprise). Leur nombre est en constante diminution depuis 2014 ( cette année-là) du fait de l'accroissement du nombre de fusions de communes. De plus, il convient d'ajouter au nombre total de communes , situées dans les deux collectivités d'outre-mer de Saint-Pierre-et-Miquelon (deux communes) et de Polynésie française (quarante-huit communes), ainsi qu'en Nouvelle-Calédonie (trente-trois communes).

Le décret de l'Assemblée nationale du disposait . La loi du proclamait : 

Ainsi furent créées les communes françaises telles qu'elles existent encore aujourd'hui.

Avant la Révolution il existait plusieurs circonscriptions administratives de base dont les ressorts ne correspondaient pas toujours.

La paroisse ecclésiastique concernait le domaine religieux. Elle servait de base à l'administration du culte et à la levée de la dîme. Regroupant une communauté de fidèles autour d'un curé, d'une église et d'un cimetière, elle était investie d'une forte identité symbolique. Mais il existait deux autres entités territoriales de base.

D'une part la seigneurie, circonscription judiciaire et fiscale pour les impôts seigneuriaux comme le champart. D'autre part la communauté, ou "paroisse fiscale", ou collecte, ou taillable. Elle avait été créée à la fin du Moyen Âge pour la levée de l'impôt royal, la taille. Dans certaines provinces, ses limites avaient été calquées sur celles de la paroisse, mais dans d'autres, c'était les seigneuries qui avaient servi de référence (en particulier dans le Midi, où le mouvement communal amorcé au avait créé des institutions municipales dans le cadre de la seigneurie). Les communautés étaient souvent administrées par des représentants de la population élus ou cooptés annuellement (consuls, échevins, jurats, etc.) et parfois par des conseils.

Au , la monarchie absolutiste avait progressivement uniformisé les institutions municipales (consulat) tout en les privant de la plupart de leurs prérogatives, pour n'en faire que les relais de la perception des impôts royaux.

Les territoires des seigneuries relevant de seigneurs laïcs étaient soumises à de nombreuses mutations (ventes, divisions, démembrements, regroupements), alors que les communautés et les paroisses étaient beaucoup plus stables. Ainsi à la veille de la Révolution française, il était fréquent que les trois découpages ne correspondent pas : plusieurs paroisses dans une communauté, plusieurs communautés dans une paroisse, de nombreux chevauchements, des enclaves parfois très éloignées de leur chef-lieu. Ainsi, de nombreuses villes ne constituaient qu'une paroisse fiscale mais se divisaient en plusieurs paroisses ecclésiastiques. Au moment où la Révolution éclata, selon les dépouillements opérés dans de nombreuses archives départementales par Antoine Follain, le nombre de paroisses religieuses était proche du nombre de communes tandis que le nombre de paroisses fiscales excédait largement le nombre de clochers (paroissiaux) dans les campagnes. Le Rouergue comptait environ et 635 paroisses en 1788.

Sous l'Ancien Régime et depuis le Moyen Âge, les villes tenaient leurs statuts d’autonomie du roi, du comte ou du duc local. Ainsi la ville de Toulouse tenait sa charte des comtes de Toulouse. Les villes étaient constituées de plusieurs paroisses (plusieurs centaines pour Paris) et étaient généralement entourées de remparts. Elles ont obtenu leur émancipation du pouvoir féodal vers les . Elles se sont alors dotées de structures municipales et d'une administration ce qui leur a donné une ressemblance avec les communes instituées par la Révolution. Il y avait toutefois deux différences : la municipalité n’était pas élue démocratiquement et était généralement dirigée de manière ploutocratique par de riches familles bourgeoises qui ont été ensuite anoblies. Il convient donc de parler d’oligarchie plutôt que de démocratie communale.

Les chartes communales n'étaient pas homogènes, chaque ville ayant la sienne et son organisation.

Dans le nord de la France, les villes étaient généralement administrées par des échevins (du francique "skapin", terme germanique désignant un juge) alors qu'au sud elles étaient administrées par des consuls (titre repris de l’Antiquité romaine). Bordeaux était gouvernée par des jurats (étymologiquement des « jurés ») et Toulouse par des capitouls (membres du chapitre). Il n’y avait pas de maire au sens actuel. Tous les échevins ou tous les capitouls étaient égaux et décidaient de manière collégiale. Toutefois, sur certains sujets, un échevin ou un consul prenait le pas et devenait une sorte de maire sans avoir l'autorité et les pouvoirs exécutifs d'un maire actuel. Ils étaient appelés « prévôt des marchands » à Paris et à Lyon, maire à Marseille, Bordeaux, Rouen, Orléans, Bayonne et dans beaucoup de villes. À Lille on parlait du mayeur, du premier capitoul à Toulouse, du viguier à Montpellier, du premier consul dans nombre de villes du sud, d'ammeister (francisé en Ammestre) à Strasbourg, du maître échevin à Metz, du maire royal à Nancy, du prévôt à Valenciennes, du vicomte-mayeur (ou maïeur) à Dijon et Dole et du Vierg à Autun.

Le soir du , après la prise de la Bastille, le prévôt des marchands de Paris Jacques de Flesselles était tué sur les marches de l’Hôtel de Ville.

Après cet évènement une « Commune de Paris » fut immédiatement mise en place pour remplacer l’ancienne organisation de Paris datant du Moyen Âge. Pour protéger la ville de toute manœuvre contre-révolutionnaire on créa une garde municipale. Plusieurs autres villes suivirent rapidement cet exemple comme de nombreuses communautés.

Le , l'Assemblée nationale constituante décrète qu'. Puis elle décrète : « 1° que chaque département sera divisé en districts ; 2° que chaque département ne sera pas nécessairement divisé en neufs districts, conformément au plan du comité ; 3° que chaque département sera nécessairement divisé dans un nombre ternaire [de districts] ; 4° que le nombre des districts ne sera pas nécessairement le même pour tous les départements ; 4° que le nombre des districts pour chaque département sera fixé par l'Assemblée nationale [constituante], après avoir entendu les députés de chaque province, suivant la convenance et le besoin de chaque département ».

Le , la Constituante votait une loi créant les municipalités ou communes désignées comme la plus petite division administrative en France et c’est ainsi qu'était officialisé le mouvement d'autonomie communale révolutionnaire.

La décision de l’Assemblée nationale était révolutionnaire car, en plus de transformer les chartes des cités et des bourgades, elle érigea en communes presque toutes les anciennes communautés ou paroisses. Certains révolutionnaires, imprégnés d’idées cartésiennes et de la philosophie des Lumières, souhaitaient rompre avec le passé et bâtir une société nouvelle où chacun serait égal et où la raison primerait sur la tradition et le passé. Aussi prévirent-ils des divisions administratives identiques dans tout le pays. Le territoire fut divisé en départements, districts, cantons et communes. Toutes ces communes eurent le même statut, avec un conseil municipal élu par les habitants et un maire. Une maison commune, la mairie, devait être construite pour accueillir les réunions du conseil et l’administration municipale. Des membres de l’Assemblée nationale étaient opposés à une telle fragmentation du pays, mais la proposition de Mirabeau l’emporta : une commune pour chaque communauté ou paroisse. Localement, les habitants pouvaient choisir entre les deux ressorts préexistant. Dans l'ancien Rouergue devenu département de l'Aveyron, la majorité des communes sont issues des communautés plutôt que des paroisses, car c'était le cadre administratif du cadastre et du prélèvement des impôts. Dans l'ouest normand, dans le nord et dans plusieurs départements de l'est, les paroisses fiscales obtinrent bien souvent l'érection en municipalité. Résultat: en 1790, le nombre des municipalités créées y est en moyenne de plus de 800 par département (jusqu'à 1036 dans la Seine-Inférieure). Ailleurs, les paroisses ecclésiastiques servirent de modèle aux nouvelles municipalités plutôt que les paroisses fiscales, jugées trop émiettées.

Le , le registre des naissances, des mariages et des décès tenu par le curé de la paroisse passa sous la responsabilité d'un officier public élu. Un mariage civil était institué et célébré dans les mairies ; la cérémonie n’était pas très différente de celle célébrée à l’église, la phrase « Au nom de la loi, je vous déclare unis par les liens du mariage » remplaçait celle du prêtre (« Au nom de Dieu, je vous déclare unis par les liens du mariage »). Les prêtres durent remettre à la mairie leurs registres des baptêmes, des mariages et des sépultures (BMS) qui furent remplacés par des registres des naissances, mariages et décès (NMD) entre les mains des préposés municipaux. Ce recul de prérogatives de l’Église n'était pas bien accepté partout et, dans l’ouest et au centre du pays, des prêtres furent relativement réfractaires.

Le terme « commune », au sens de l’administration territoriale actuelle, fut imposé par le décret de la Convention nationale du () : .

Sous le Directoire, il existait des « municipalités de canton » et, dans les grandes villes, trois administrations municipales ou plus. Napoléon Bonaparte stabilisa les structures administratives, notamment en réussissant à faire accepter la loi du 28 pluviôse an VIII. Les membres du conseil municipal furent alors élus au suffrage censitaire. Le maire fut nommé par le pouvoir central pour les communes les plus peuplées et par le préfet pour les autres.

Des changements importants eurent lieu en 1831, avec un retour au principe d’élection du conseil, et en 1837, avec la reconnaissance de la capacité légale. La fondamentale loi municipale du prévoit que le conseil serait élu au suffrage universel direct, qu'il siègerait à la mairie et serait présidé par le maire désigné en son sein. C'était la fin de constantes modifications de statut opérées par les différents régimes en place depuis la Révolution française, mais surtout le début de l'autonomie promise par les républicains sous l'Empire.

Le 16 octobre 1793, la convention nationale décréta que les communes qui avaient changé de nom depuis 1789, feraient connaître au comité de division la nouvelle dénomination qu'elles avaient adoptée. Elle invita les communes à s'occuper incessamment de changer les noms qui pouvaient rappeler les souvenirs de la royauté, de la féodalité et de la superstition.

La même assemblée décréta, le 31 octobre 1793, que les dénominations de "ville", "bourg", "village", seraient supprimées
et remplacées par le nom de "commune". Plus tard, un arrêté consulaire du 9 fructidor an IX ordonna qu'à l'avenir il ne serait plus donné aux communes d'autres noms que ceux portés au tableau de la division de la France en justices de paix.

En 1845, l'administration générale des postes prenait encore en compte le fait que plusieurs localités avaient deux noms, pour celles qui en avaient un en français et un autre en allemand (standard ou dialectal selon les cas). Ce fait concernait à l'époque les départements de la Meurthe, de la Moselle, du Bas-Rhin et du Haut-Rhin.

Constituée au départ autour de villages centrés sur leur paroisse et de centres urbains commerciaux importants, les communes français peuvent correspondre aujourd'hui à une grande diversité de réalité territoriale. Quelques idéaux-types peuvent cependant être dégagés. Une commune peut ainsi correspondre :

Les communes françaises existent depuis la Révolution.
Elles constituent une des collectivités territoriales dont l'existence est garantie par la Constitution du 4 octobre 1958 insituant la République et représentent le premier niveau d'administration territoriale. Leur nom est fixé par l'État et tout changement procède d'un décret en Conseil d'État. Les compétences des communes sont essentiellement régies par les parties législative et réglementaire du Code général des collectivités territoriales (CGCT), notamment dans sa deuxième partie.

Malgré les disparités de population et de superficie entre les communes, toutes ont la même structure administrative et les mêmes compétences légales (à l’exception de Paris, Lyon et Marseille, régies par la loi PLM). Les communes d'Alsace et de Moselle ont des spécificités juridiques héritées de la période 1871-1919, lorsque l’Alsace-Moselle était allemandes. Les communes des départements de la petite couronne parisienne ont également des règles particulières, notamment en ce qui concerne la police administrative.

Une commune est administrée par un conseil municipal dont les membres sont élus au suffrage universel direct pour six ans. Le mode de scrutin pour les conseillers des communes de moins de consiste en un scrutin majoritaire plurinominal à deux tours avec possibilité de créer une liste qui pourra subir un panachage. Les conseillers des communes de plus de sont élus via un scrutin de liste proportionnel bloqué à deux tours avec prime majoritaire de 50 %.

Le conseil élit en son sein un maire chargé de préparer et d’appliquer les décisions du conseil, et qui dispose de compétences propres. Le maire est assisté d'un ou de plusieurs adjoints, qui peuvent recevoir des délégations.

Le nombre de conseillers municipaux est fonction de la population de la commune, le minimum étant de 7. Les séances du conseil sont publiques mais seuls les élus peuvent s’exprimer.

Il y a en France environ municipaux (maires inclus). Ils exercent également les fonctions de grands électeurs élisant les sénateurs.

Une commune étant une collectivité territoriale, elle est une personne morale de droit public et dispose ainsi d’un budget propre.

Elles sont constituées pour l’essentiel :

Les communes gèrent l’administration locale (gestion de l'eau, des permis de construire, etc.). En tant que représentant de l’État dans la commune, le maire a la charge des actes d’état civil (naissance, mariage, divorce, décès). De plus il dispose d’un pouvoir de police administrative et, en vertu de son statut d'officier de police judiciaire, d'un pouvoir de police judiciaire, exercé par le biais de la police municipale. Exception est faite de Paris où la compétence relève du préfet de police, sous l’autorité du gouvernement.

Les décisions des conseils municipaux et des maires peuvent être contestées devant le tribunal administratif.

Les trois communes les plus peuplées, Paris, Marseille et Lyon sont divisées en arrondissements municipaux (qui sont distincts des arrondissements départementaux, une autre division administrative française) mais ces derniers présentent des compétences réduites et ne disposent pas de budget propre, par exemple.


Au , la France compte dont en France métropolitaine et 129 dans les DOM-ROM.

Depuis l'entrée en vigueur de la du portant dispositions statutaires et institutionnelles relatives à l'outre-mer, une collectivité territoriale unique a été substituée, sur le territoire de l'île de Saint-Barthélemy et des îlots qui en dépendent, à la commune de Saint-Barthélemy ainsi qu'au département et à la région d'outre-mer de la Guadeloupe. Il en est de même sur le territoire de la partie française de l'île de Saint-Martin et des îlots qui en dépendent, où une collectivité territoriale unique a été substituée à la commune de Saint-Martin ainsi qu'au département et à la région d'outre-mer de la Guadeloupe. Saint-Barthélemy et Saint-Martin continuent néanmoins d'être comptés, dans les statistiques, comme deux communes.

Deux collectivités d'outre-mer ne sont pas divisées en communes mais ont des divisions qui sont traitées statistiquement à un niveau équivalent : Wallis-et-Futuna avec trois circonscriptions et les Terres australes et antarctiques françaises (TAAF) avec cinq districts.

Quant à l'île de Clipperton, elle constitue également une unité statistique mais il s'agit d'une propriété du domaine public de l'État qui n'est pas réellement une commune, ni même une collectivité territoriale.

Il existe neuf communes dites « mortes pour la France », dont 6 sans habitant, et dont le maire est nommé par le préfet de département.

Il existe, en outre, des communes associées et des communes déléguées qui ont des "maires délégués".
Les rattachements aux départements évoluent également. Ainsi :

Les données de population sont les « populations légales des communes en vigueur au », données issues du recensement 2015 et selon la géographie en vigueur au ( en métropole + 129 dans les DOM-ROM), publiées par l'Insee le .

En 2018, la population médiane des communes de France métropolitaine est de .

La commune de France métropolitaine la plus peuplée est Paris ().

En 2012, la commune la plus densément peuplée est Levallois-Perret (Hauts-de-Seine, . L'unité urbaine (c’est-à-dire une commune comportant une zone de plus de où aucune habitation n’est séparée de la plus proche de plus de ) la moins densément peuplée est Saintes-Maries-de-la-Mer (Bouches-du-Rhône, ).

En 2018, (données du recensement de 2015), ont ou moins, dont six communes totalement dévastées après la bataille de Verdun en 1916. Villages français détruits durant la Première Guerre mondiale, ils ne furent jamais reconstruits et ne comptent aucun habitant. Chacune de ces communes qualifiés de communes « mortes pour la France », est administrée par un conseil municipal de trois membres nommés par le préfet de la Meuse.

La taille moyenne d’une commune de France métropolitaine est de . La taille médiane des communes de France métropolitaine n’est que de , à cause du nombre élevé de communes de faible superficie (là encore, la France fait figure d’exception en Europe : en Allemagne, la taille médiane des communes de la plupart des Länder est supérieure à , en Italie elle est de , en Espagne , en Belgique ). Dans les départements d’outre-mer, les communes sont généralement plus grandes qu’en France métropolitaine et peuvent regrouper des villages relativement distants.

En France métropolitaine, la répartition des communes est la suivante :

La commune la plus étendue est Maripasoula (Guyane avec . Sur le territoire métropolitain, Arles () (dans les Bouches-du-Rhône) et Val-Cenis () (en Savoie) sont les deux communes les plus étendues. La plus petite commune est Castelmoron-d'Albret (Gironde) avec . Vaudherland (Val-d'Oise) avec , est la deuxième plus petite commune de France.

Le terme « intercommunalité » désigne différentes formes de coopération entre les communes. Ce type de coopération est apparu dès le avec la loi du qui régit les associations intercommunales. De très nombreux syndicats de communes ont été créés pendant toute la durée du , notamment afin de gérer des activités à une échelle plus large que celle de la commune. C'est ainsi que, dès les années 1920, la banlieue parisienne s'est dotée de grandes structures telles que le "Syndicat des communes de la banlieue pour l’électricité" (devenu le Syndicat intercommunal de la périphérie de Paris pour l'électricité et les réseaux de communication (SIPPEREC) en 1924), chargé d'assurer la distribution de l'électricité dans ces communes, ou le Syndicat des eaux d'Île-de-France en 1923, qui produit et distribue, en gestion déléguée, l'eau potable aux habitants.

Toutefois, le gouvernement a noté, à la fin des années 1960, que la France était le pays d'Europe qui comptait le plus grand nombre de communes, dont l’immense majorité avait moins de , le nombre de petites communes croissant d'ailleurs avec l'exode rural. C’est pourquoi le gouvernement a décidé de réduire le nombre de communes par un mécanisme de fusion. Instauré lors de la loi du , dite « loi Marcellin », ce mécanisme devait donner naissance à une commune nouvelle et unique regroupant les communes fusionnées ou bien il confèrait aux communes fusionnées les statuts de, respectivement, "chef-lieu" ou "commune associée". Dans la dynamique de cette loi, il avait été envisagé environ regroupant mais, en 2009, le nombre de communes effectivement fusionnées n'était que :

Les petites communes préfèrent en effet conserver leurs prérogatives quitte à les confier à des "établissements publics de coopération intercommunale", à des "sociétés d’économie mixte" ou à des associations.

Selon les dispositions de la loi du 16 juillet 1971 dite loi Marcellin ce qui est sa faiblesse, les communes qui acceptent de prendre le statut de commune associé en contribuant à fusionner leur territoire bénéficient d'une priorité et reçoivent une majoration des subventions obtenues de 50 %.

Néanmoins sont créés à partir de 1966, et de manière ressentie comme autoritaire, les communautés urbaines et les districts. Ces structures restent peu nombreuses, et ces structures intercommunales intégrées ne sont que moins de 250 en 1992, année où est créée par la loi du 6 février 1992 relative à l’administration territoriale de la République la communauté de communes.

Le développement de l’intercommunalité a été relancé par la Loi relative au renforcement et à la simplification de la coopération intercommunale du , dite « loi Chevènement ». Désormais, les principales structures intercommunales sont les communautés urbaines, communautés d’agglomération et communautés de communes, financées par une fiscalité propre.

Cette loi a connu un grand succès, et de très nombreuses communautés de communes ou communautés d'agglomération ont depuis été créées.

Dans ce cadre, la "loi -1563 du 16 décembre 2010 de réforme des collectivités territoriales" se fixe comme objectif d'achever et de rationaliser la carte intercommunale afin de parvenir à une couverture intégrale du territoire par des intercommunalités à fiscalité propre avant le . Elle crée également de nouvelles structures intercommunales, la métropole et le pôle métropolitain, tout en incitant à nouveau au regroupement de communes, par la création des communes nouvelles.

Il y a deux sortes de structures intercommunales.

Elles correspondent à la forme d’intercommunalité la moins contraignante. Les syndicats de communes en sont la forme la plus connue. Les communes s’associent et contribuent financièrement au syndicat mais ce dernier ne peut lever ses propres taxes. Les communes peuvent le quitter à n’importe quel moment. Les syndicats peuvent être mis en place pour un sujet précis ou traiter différents problèmes. Ces structures sans pouvoir fiscal n’ont pas été touchées par la loi Chevènement et voient leur intérêt diminuer.

Ce sont ces structures qui ont été créées ou modifiées par la loi Chevènement. Elles sont au nombre de quatre :

Ces quatre structures disposent de différents niveaux de compétences fiscales. Les métropoles, les communautés urbaines et les communautés d’agglomération ont plus de pouvoir en matière fiscale, elles disposaient, jusqu'en 2010, de la taxe professionnelle qui doit être au même niveau dans toutes les communes membres. Comme un syndicat intercommunal, elles gèrent le traitement des ordures ménagères et les transports mais elles s’occupent aussi du développement économique, des projets d’urbanisme et de la protection de l’environnement. Les communautés de communes ont moins de compétences et laissent beaucoup plus d’autonomie aux communes. la loi ne prévoit pas de territoire d'une communauté, en sorte qu'il existe des communautés de deux ou trois communes seulement.
Pour encourager la création de communautés de communes l’État leur alloue des subventions dont le montant dépend du niveau de population. Plus il y aura de communes unies et plus les subventions seront importantes. Cet encouragement est déterminant dans le choix que font les communes de s’unir.

La loi Chevènement est un succès car la grande majorité des communes françaises ont rejoint la nouvelle structure intercommunale, chaque commune conservant selon la loi sa structure municipale. Au on comptait de communes en France métropolitaine (y compris cinq syndicats d’agglomérations nouvelles). Cela représente (91,1 % du total métropolitain) et plus de (86,7 % de la population française métropolitaine).

Toutefois en zone rurale de nombreuses communes n’ont rejoint une communauté que dans le but de bénéficier des subventions de l’État. Souvent, ce sont de simples syndicats intercommunaux qui ont été transformés en communautés de communes. Celles-ci se limitent alors aux deux attributions minimum prévues par la loi et aux activités de la structure précédente, ce qui bien sûr est contraire à l’objectif initial.

Dans les zones urbaines les nouvelles structures intercommunales sont beaucoup plus vivantes. Le plus souvent leurs créateurs avaient dès le départ la volonté de s’unir et de travailler ensemble. Malgré tout, de vieilles haines ne manquent pas de ressurgir ici ou là. Il n’est pas rare de voir une communauté urbaine incomplète car telle ou telle commune a refusé d’y adhérer, quitte à créer une communauté parallèle ; ainsi, la ville de Marseille est partagée en quatre districts intercommunaux. Dans plusieurs endroits, les communes les plus riches se sont regroupées entre elles et n’ont pas accepté l’adhésion des communes plus pauvres pour ne pas avoir à les financer. D’autre part, il faut bien admettre que de nombreuses communautés restent fragiles. Il y a souvent des tensions entre communes, les villes centrales sont souvent suspectées de vouloir dominer ou même absorber les communes limitrophes, sans oublier, bien sûr, les querelles entre partis politiques.

Toulouse et Paris sont deux bons exemples de cette situation :

Le principal défaut des structures intercommunales est qu’elles ne sont pas dirigées par des représentants directement élus. Ce sont les élus des communes qui siègent au conseil intercommunal. La nécessité de concilier les points de vue de chacune des communes membres prend donc le pas sur l'efficacité de la prise de décision (de manière analogue aux institutions européennes) et crée en définitive une carence à la fois de bonne administration et de démocratie. Depuis plusieurs années a lieu un débat sur l'intérêt de voir élire les membres des conseils des communautés au suffrage universel direct lors des élections municipales, ce qui aboutirait toutefois nécessairement à une baisse du poids politique des maires et des communes par rapport à celui des intercommunalités et de leurs présidents.

La loi du 16 décembre 2010 de réforme des collectivités territoriales a réformé le mécanisme, et, à partir des élections municipales de 2014, les conseillers communautaires des communes de plus de habitants seront élus au suffrage universel direct, dans le cadre des élections municipales. Les représentants des communes de plus petite taille (c'est-à-dire environ 90 % de l'ensemble des communes) resteront élus en leur sein par les conseils municipaux.

Depuis la loi Marcellin de 1971, s'élèvent pour réclamer une diminution autoritaire du nombre des communes. Le président de la Cour des comptes . Mais jusqu’à présent . Le parlement français n’a jamais fait aucune proposition de loi obligeant les communes à fusionner. La loi Marcellin du 16 juillet 1971 offrait une assistance et une aide financière pour inciter les communes à se regrouper librement. Quoiqu'elle ait amené des discussions entre élus locaux et provoqué un débat utile, cette loi n'aboutit qu'à réduire le nombre des communes d'un peu plus de 1000 d'entre elles qui acceptèrent de se dissoudre dans une entité plus grande, compte tenu des annulations d'associations survenues ultérieurement. Sont créées par la loi des communes associées comportant un maire délégué.

Les partisans du regroupement des communes font remarquer que les villes françaises ont un faible poids en regard de leurs sœurs européennes. Selon ces mêmes partisans leurs limites, définies il y a , ne seraient plus représentatives de la réalité. Par exemple, la ville de Lyon intra-muros est une commune de faible superficie qui ne comptait que en 2010. Elle se classe loin derrière de nombreuses villes européennes alors que la population de son agglomération comptait en 2009. À ce titre, elle fait partie des plus grandes villes d’Europe et se situe à un niveau comparable à celui de Munich. Par comparaison, la population de la commune (Gemeinde) de Munich était de soit presque trois fois celle de la commune de Lyon et sa superficie est de soit plus que celle de Lyon ().

À l’autre bout de la chaîne, de nombreuses communes éloignées ont été pratiquement vidées de leur population par l’exode rural. Elles sont maintenant dans l’incapacité de financer les services de base tels que l’adduction de l’eau courante, le ramassage des ordures ménagères ou l’entretien des chaussées.

La loi -1563 du 16 décembre 2010 de réforme des collectivités territoriales se fixe pour objectif d'achever et de rationaliser la carte intercommunale, en créant des structures adaptées pour les très grands ensembles urbains, les métropoles, et en organisant l'intégration de toutes les communes dans des intercommunalités, voire en facilitant leur regroupement au sein des communes nouvelles.

La loi n 2015-292 du 16 mars 2015 "relative à l'amélioration du régime de la commune nouvelle, pour des communes fortes et vivantes" doit faciliter la création de communes nouvelles en instaurant un pacte financier qui garantit pendant trois ans le niveau des dotations de l’État aux communes fusionnant en 2015 ou 2016. Cela a entraîné un mouvement inédit de fusions, faisant diminuer de plus de 1000 le nombre de communes entre 2014 et 2017.

Depuis la loi Marcellin de 1971, qui avait l'objectif de réduire le nombre des communes, les ministres de l'Intérieur ont mis en place et encouragé le développement de nouvelles structures intercommunales exerçant un nombre croissant de compétences, en laissant subsister la commune qui progressivement se viderait de ses attributions et tendrait en milieu rural à devenir une « coquille vide ». Ils ont ainsi créé un niveau supplémentaire d'administration et de gestion entre la commune et le département, avec un coût total de fonctionnement plus élevé qu'auparavant. Ce que Raymond Marcellin, qui avait écarté une formule de regroupement, voulait éviter.

Altitude la plus haute :

Altitude la plus basse : la commune française la plus basse est Quimper (Finistère) dont une partie du territoire est située au-dessous du niveau de la mer, jusqu’à -.

À vol d'oiseau, la commune française la plus éloignée de Paris est l’Île des Pins (en Nouvelle-Calédonie, à de la capitale. Sur le territoire métropolitain, il s’agit de Bonifacio, située à .

La commune la plus au nord est Bray-Dunes, Nord.

Les communes les plus à l’ouest sont :

Les communes les plus à l’est sont :

Les communes les plus au sud sont :

La nomenclature officielle des collectivités locales, et notamment des communes de France, fait l'objet du Code officiel géographique (COG).

Les communes françaises possédant le nom le plus long sont Saint-Remy-en-Bouzemont-Saint-Genest-et-Isson (Marne, ), et Beaujeu-Saint-Vallier-Pierrejux-et-Quitteur (Haute-Saône, ) et possèdent toutes les deux .
Si l'on ne prend que les noms formés d'un seul tenant, les communes aux noms les plus longs sont Niederschaeffolsheim et Mittelschaeffolsheim avec (les deux sont situées dans le Bas-Rhin).
La commune française avec le nom le plus court est Y (Somme). Quinze communes possèdent un nom de deux lettres (Ay, Bû, By, Eu, Fa, Gy, Oô, Oz, Py, Ri, Ry, Sy, Ur, Us et Uz).

La première commune par ordre alphabétique est Aast (Pyrénées-Atlantiques). La dernière commune par ordre alphabétique est Zuytpeene (Nord).

Il existe qui commencent par « Saint » (10,7 % des communes françaises), y compris Le Saint. Le « Saint » le plus courant est (), suivi de Saint-Jean () et Saint-Pierre (). communes possèdent le terme « Saint » à l’intérieur de leur nom. Seulement quatre préfectures (dont une d'outre-mer) commencent par « Saint » : Saint-Brieuc, Saint-Étienne, Saint-Lô, Saint-Denis.

Il existe qui commencent par « Sainte » (0,9 % des communes françaises), y compris Saintes. La sainte la plus courante est (, y compris Saintes-Maries-de-la-Mer), suivie de () et (). possèdent le terme « Sainte » à l'intérieur de leur nom.

Les noms des communes (les toponymes) ont des étymologies très diverses : préceltiques, celtiques, gallo-romanes, germaniques, scandinaves (en Normandie) ou plus généralement romanes (voir toponymie française). Bien que la plupart aient subi un phénomène de romanisation ou de francisation, certains conservent cependant l'aspect graphique, voire phonétique, de leur langue d'origine, par exemple :

La plupart des communes corses portent un nom officiel italianisé à l’époque des dominations génoises et pisanes. Par exemple : San-Gavino-di-Carbini (Corse-du-Sud), en corse "San Gavinu di Càrbini" ; Porto-Vecchio (Corse-du-Sud, ), en corse "Portivechju" ; exceptions notoires : Saint-Florent, L'Île-Rousse, Sartène.

Communes jumelles : certaines communes sont voisines avec leurs (presque) homonymes séparées par les vicissitudes de l'histoire :

Le gentilé est le nom donné aux habitants d’une commune.

Des communes couvrent entièrement une ou plusieurs îles situées en mer, on peut citer les îles suivantes :

Il existe également des communes situées sur des îles fluviales, comme L'Île-Saint-Denis ou Béhuard.

L'association des îles du Ponant regroupe les îles sans liaison physique fixe avec le continent.
Par contre :

Les communes, à travers l'aménagement du territoire, l'éclairage public, et leurs incitations à mieux construire, se déplacer et consommer ont un rôle important en matière de gestion et économie de l'énergie. En France, dans les années 2000, les bâtiments que les communes doivent entretenir, chauffer, éclairer etc. représentent 75 % de la consommation d’énergie des communes ( en 2005). L’éclairage public et la signalisation viennent juste derrière. 4 % (en moyenne) du budget de fonctionnement des communes sont des dépenses en gaz, fioul et électricité.

En 2005, ont été dépensés pour éclairer (éclairage public principalement), chauffer et alimenter les matériels électriques du patrimoine communal, c'est 26 % de plus qu'en 2000, alors que l’accroissement des consommations s’élevait à 7,3 %.

L'éclairage public et les véhicules des collectivités (dont intercommunalités, départements et régions) ont nécessité environ par habitant. En 2009, le parc bâti devant être entretenu par les collectivités était de plus de , avec une forte prépondérance des bâtiments scolaires au sein de ce patrimoine. Ces derniers comptent pour (53 % du parc des collectivités), alors que les équipements de sports, loisirs et culture comptent pour 16 % et ceux de l’action sociale pour 13 %. Les locaux (bureaux, salles de mairies, etc.) même des collectivités ne comptent que pour 10 %. Par contre en termes de consommation d'énergie par mètre carré, ce sont les équipements sportifs, culturels et de loisirs (, soit 28 %) et les maisons de retraite (, soit 22 % du total des dépenses énergétique des collectivités) qui consomment le plus, devant les bâtiments scolaires (, 13 %), les bureaux des collectivités (, 17 %) et l'action sociale (, 20 %). Les communes jouent en outre un rôle d'exemple pour les habitants.

Elles doivent décliner à leur échelle au Facteur 4 (réduction par 4 des émissions de gaz à effet de serre avant 2050) et à l'objectif intermédiaire de la loi Grenelle I du « 3 fois 20 » :





</doc>
<doc id="17768" url="https://fr.wikipedia.org/wiki?curid=17768" title="Orthotypographie">
Orthotypographie

L’orthotypographie est l’ensemble des règles qui permettent d’écrire de façon correcte, selon une norme établie, à l’aide de types (caractères). C’est donc l’ensemble des règles de l’orthographe des mots et des règles typographiques (utilisation des majuscules et des minuscules, des espacements, de la ponctuation, de l’italique).

Le terme semble être apparu en 1608 et désignait alors un document destiné aux correcteurs ou à ceux qui vont publier leurs écrits. Pour Jean Méron, le terme excluait donc toute référence à la simple rédaction manuscrite. La notion a été reprise par Nina Catach (« orthographe typographique ») qui s’était intéressée également à la ponctuation comme à ce qu’elle nommait « la mise en page ». Jean-Pierre Lacroux revendique le mot-valise avec un sens distinct de celui évoqué par Méron : tout ce qui concerne « l’armada des prescriptions à la fois orthographiques et typographiques ; par exemple, celles qui concernent l’écriture des titres d’œuvres ».

De fait, le terme correspond à une intersection (nécessairement) floue entre orthographe et typographie. Distincte des « marches typographiques » propres à une publication, un éditeur, une collection , l’orthotypographie répondrait au besoin de repères qui est celui des rédacteurs-composeurs-éditeurs (souvent auto-imprimeurs) d’aujourd’hui. Après cinq siècles de composition typographique et un demi-siècle seulement de photocomposition, le développement des outils bureautiques (matériels et logiciels), mais aussi de l’impression personnelle contribuent à faire émerger dans le public le besoin de connaître les règles de présentation de documents structurés.

L’orthotypographie se distingue donc du simple respect de la norme orthographique et grammaticale commun à l’ensemble des productions écrites (y compris les productions courantes). Son but est d’appliquer des normes ortho- et typo-graphiques applicables à l’édition « composée » qui participent à la compréhension visuelle d’un texte structuré, qu’il s’agisse d’impression sur papier ou de mise en ligne.

Tant le préfacier, Fernand Baudin, que Jean Méron, auteur d"'Orthotypographie, recherches bibliographiques" (Convention typographique, Paris, avril 2002), attribuent la première apparition du mot (composé en caractères grecs) à Hieronymus Hornschuch, auteur d'un court traité latin intitulé "Orthotypographia : instruction utile et nécessaire pour ceux qui vont corriger des livres imprimés & conseils à ceux qui vont les publier" (Leipzig : Michaël Lantzenberger, 1608). Il en existe des traductions allemande, anglaise et française. Jean Méron estime que le mot "désigne donc l’acte d’écrire de façon correcte, selon une norme établie, à l’aide de types (caractères)". Cela exclut qu’une orthotypographie conforme aux usages établis puisse être obtenue en rédigeant manuellement.

Dans "L’Orthographe", Nina Catach (1928-1997) avait employé le terme . Et elle écrit, à propos des pères fondateurs (les Lefèvre d’Étaples, Robert Estienne, Geoffroy Tory et autres Étienne Dolet) :

"Les Délires de l’orthographe" (Plon, 1989) rappelaient :
Dans son "Histoire de l’orthographe française", elle précisait au :

Dans l’avant-propos de son œuvre posthume, Jean-Pierre Lacroux (1947-2002) définit l’orthotypographie :
Orthotypographie est un beau néologisme. Sa formation, fort différente de celle d’"orthotypographia" (rareté néolatine forgée il y a quatre siècles : "ortho" + "typographia" = typographie correcte) ne doit rien à la préfixation. C’est un mot-valise subtil : "orthograph[e]" + "typographie". Il est parfait pour désigner l’armada des prescriptions à la fois orthographiques et typographiques, par exemple celles qui concernent l’écriture des titres d’œuvres.

Initialement diffusée le sur la Liste typographique francophone, cette définition inspira à l’un de ses colistiers, Jean Fontaine, la réflexion suivante : 

L’orthotypographie reste un terme en attente d’une définition précise car il ne ressortit pas à un domaine particulier bien circonscrit. Ses composants, "ortho" et "typographie", pourraient laisser supposer qu’il s’agit d’une discipline indiquant la manière de bien "typographier", verbe absent de la plupart des dictionnaires. S’agit-il simplement de "composer" en utilisant des caractères dits encore d’"imprimerie" alors qu’ils sont couramment utilisés de nos jours sans qu’il soit nécessairement procédé à leur impression ? Ou de composer et publier, ce qui implique d’organiser la composition, de la mettre en forme, de réaliser une mise en page ? Ce qui implique de multiples opérations telles la détermination des marges (ou "empagement"), l’étalonnage, la justification Or, une large part de ces opérations sont négligées par les "orthotypographes", qui n’en traitent pas.

L’observation montre qu’il est généralement admis par les professionnels qu’une marche est un ensemble de règles, fait d’un individu ou d’un groupe d’individus, qui ne sera appliqué que pour la production de l’ensemble des publications d’une personne, d’une entreprise, d’une collection, voire d’un seul titre publié. Ainsi, le "" serait une marche, de même que le "Chicago Manual of Style", qui est à la fois un guide de rédaction (recommandant des tournures de styles, des formes orthographiques à préférer à d’autres) et une marche de composition dont le respect s’impose aux correcteurs des éditions de l’université de Chicago.

Ces deux ouvrages, tout comme "Le Ramat de la typographie", d’Aurel Ramat, auteur unique, n’en sont pas moins des ouvrages de référence dont les règles sont observées par des auteurs et correcteurs dont les publications ne seront pas confiées à l’Imprimerie nationale ou aux Presses de l’université de Chicago. L’observation révèle qu’un "code" serait le fait de plusieurs auteurs se réunissant dans un cadre syndical ou interprofessionnel pour élaborer des règles s’imposant à l’ensemble des entreprises (maisons d’édition, de presse, imprimeries, correcteurs indépendants fédérés dans une association ou un syndicat).

Pourtant, la marche de l’Office des publications de l’Union européenne s’intitule "Code de rédaction interinstitutionnel" et le "Nouveau code typographique" de la Fédération de la communication CFE/CGC, qui se fonde, certes, sur les dix-sept éditions successives du "Code typographique" (première parution en 1928), ouvrage collectif, est-il le fait d’un auteur unique, Robert Guibert. Quant au "Guide du typographe" (ex-"Guide du typographe romand"), dont l’éditeur est le groupe de Lausanne de l’Association suisse des typographes, ouvrage collectif, il serait un code qui ne s’intitulerait pas de la sorte.

Se constate aussi l’existence de documents officiels dont les recommandations s’imposent au moins à celles et ceux qui ont l’autorité de rédiger pour le compte des organismes ou administrations dont ils émanent. On remarquera ainsi que l’Office québécois de la Langue français diffuse, conjointement avec le Bétel (Banc d’évaluation technolinguistique), depuis le début du siècle un document dont le titre est « Word 2002, l’odyssée de l’espace! ou les espacements avant et après les signes de ponctuation et d’autres signes ou symboles courants » (et, non, l’absence d’espace avant le point d’exclamation n’est pas, ici, une coquille puisque le document n’en prévoit pas plus devant ce signe que devant le point d’interrogation). De même, la norme belge intitulée "Classification et frappe de documents", laquelle présente des incohérences (telle cette énumération dont les lignes antérieures à la finale sont suivies de virgules et telle autre dont les lignes sont suivies de points-virgules non précédés d’espaces), et des approximations, voire des erreurs factuelles (le code ISO du yen étant JPY et non JPJ), peut être, du seul fait d’être une norme, assimilée à un code.

La consultation de ces ouvrages tendrait à circonscrire l’orthotypographie : il ne s’agirait que de la fixation des règles de « composition horizontale », le ligne à ligne, par opposition aux règles, techniques ou principes d’occupations de l’espace-page. Pourtant, les indications relatives à la division des mots en fin de ligne, aux listes (énumérations supposant de chasser les entrées successives à la ligne suivante), à l’alinéation (citations dialoguées, tirades, poèmes ?), aux tableaux (sens de lecture) relèvent aussi de la composition verticale. Relève aussi de l’orthotypographie ce qui se rapporte à des normes internationales de composition (composition des toponymes, des codes postaux, abréviations des unités de mesure).

Il serait tentant aussi de réduire le domaine d’application de l’orthotypographie au tronc commun de ces codes, manuels et marches : règles d’emploi des capitales, de l’italique, composition des nombres, abréviations, signes de ponctuation et blancs d’accompagnement, emploi de caractères spéciaux (puces, astérisques, marques de paragraphes) ; l’emploi des ligatures et des caractères dits "experts", tombés en désuétude mais redevenus plus faciles à composer (avec des polices de caractères au format OpenType et des logiciels de composition sachant les gérer), pourrait aussi être intégré dans cette énumération. Relevons incidemment que, dans ce tronc commun, nombre d’entrées rassemblent des règles qui sont observées (ou ignorées) tant pour la composition typographique que pour la rédaction manuscrite ou la composition calligraphique.

Dans la préface d’"Orthotypographie", Jean-Pierre Lacroux rappelait que :

Dans la vie privée comme dans la vie professionnelle, il y a de plus en plus de rédacteurs-composeurs-éditeurs qui sont, de surcroît, « auto-imprimeurs ». Le succès public du "Manuel de typographie élémentaire" d’Yves Perrousseaux comme celui du "Lexique des règles typographiques en usage à l’Imprimerie nationale" attestent du souhait que peuvent avoir les particuliers ou des professionnels ayant affaire avec l’édition, sans être eux-mêmes des professionnels de l’édition (des documents d’entreprise aux publications universitaires), mais ayant à cœur de « bien composer ».

Le code orthographique est admis : qu’il soit violenté parfois ne remet pas en cause sa légitimité, et l’utilisateur lambda sait pouvoir ou devoir recourir si nécessaire à des aides diversifiées en fonction de ses besoins, du "Petit Larousse illustré" des familles aux dictionnaires encyclopédiques ; des dictionnaires des difficultés à cet arbitre que reste "Le Bon Usage" de Maurice Grevisse (1895-1980). De la même manière, il existe un besoin d’outils orthotypographiques, sur papier ou en ligne ; un besoin aussi d’"éducation orthotypographique" : indispensable à haut niveau pour les professionnels de la chose imprimée (ou même éditée : la question est posée pour les textes et documents « mis en ligne ») ; nécessaire aussi pour les autres.

Au-delà de l’exactitude orthographique , il existe un besoin de connaître les règles de présentation de documents structurés : des titres aux citations, des index aux bibliographies

Pour le moment, en anglais comme pour les langues romanes, seuls sont attestés, dans les textes les employant, les deux mots « orthotypographie » et « orthotypographe » et les traductions d’orthotypographie (nous n’avons pas encore trouvé « orthotypographer » ni de forme du type « iste »).

Faut-il envisager, selon les méthodes de la lexicographie, deux entrées pour le mot « orthotypographe » :

« Orthotypographe » comme « orthographe » apparaît peu évident en raison de l’existence du couple « typographe/typographie ». C’est pourquoi « orthotypographie » a été employé et a, selon toute vraisemblance plus d’avenir. « Orthographe » est d’ailleurs une exception, quand il s’agit de désigner un « objet de connaissances théoriques ou pratiques », si l’on considère l’ensemble des termes en « ~graphie ». Une rigueur complète imposerait de ne parler que d’"orthographie", mais l’histoire a ses aléas… On n’oublie pas le hiéroglyphe, qui a subi le même sort

C’est le nom de métier qui devrait être seul retenu, et plus vraisemblablement pour celui ou celle qui fixe ou définit les règles : l’application des règles s’effectue et se vérifie (en principe) tout au long de la chaîne éditoriale.





</doc>
<doc id="17770" url="https://fr.wikipedia.org/wiki?curid=17770" title="Sous-matrice">
Sous-matrice

Une sous-matrice est une matrice obtenue à partir d'une matrice en ne gardant que certaines lignes ou colonnes.

Par exemple
Alors
est une sous-matrice de "A" constituée des lignes 1 et 2, et des colonnes 1,3 et 4. Nous pouvons dire aussi que cette sous-matrice est formée en "supprimant" la ligne 3 et la colonne 2.

Il n'y a pas de notation normalisée pour désigner une sous-matrice - aussi les conventions de notations seront toujours précisées.


</doc>
<doc id="17772" url="https://fr.wikipedia.org/wiki?curid=17772" title="Marius et Jeannette">
Marius et Jeannette

Marius et Jeannette est un film français réalisé par Robert Guédiguian et sorti en 1997. Sélectionné au Festival de Cannes 1997 dans la section Un certain regard.

Quartiers Nord de Marseille, L'Estaque, Marius vit seul dans une immense cimenterie désaffectée qui domine le quartier. Il est le gardien de cette usine en démolition.
Jeannette élève, seule, ses deux enfants avec un maigre salaire de caissière. Elle habite une minuscule maison ouverte sur une courette typique de l'habitat méditerranéen. Ses voisins de cour, Caroline et Justin, Monique et Dédé, l'encouragent avec force, éclats de rire et coups de gueule. La rencontre de Marius et de Jeannette n'est pas simple car, outre les difficultés inhérentes à leur situation sociale, ils sont blessés par la vie. Le film décrit la renaissance de leur capacité à être heureux.







</doc>
<doc id="17777" url="https://fr.wikipedia.org/wiki?curid=17777" title="Le Poulpe (collection)">
Le Poulpe (collection)

« Le Poulpe » est une collection de romans policiers publiée aux éditions Baleine, inaugurée en 1995 avec "La petite écuyère a cafté" de Jean-Bernard Pouy, également directeur de collection originel. Bien que chacun des épisodes soit écrit par un auteur différent, on y suit les aventures d'un même personnage, Gabriel Lecouvreur, un détective surnommé « Le Poulpe » à cause de ses longs bras semblables aux tentacules d'un poulpe. La collection a été adaptée au cinéma en 1998 ("Le Poulpe, le film"), et certains numéros ont été adaptés en bande dessinée à partir de 2000 ("Le Poulpe en bande dessinée").

La « bible » de la collection a été écrite conjointement par les trois premiers auteurs : Jean-Bernard Pouy, Serge Quadruppani et Patrick Raynal.

Jean-Bernard Pouy, qui a fondé et dirigé la collection à ses débuts, déclarait ne pas faire de sélection dans les manuscrits, les publiant dans leur ordre d'arrivée pour rendre compte sans filtre de ce qui s'écrit. De cette façon la collection a rapidement dépassé les 100 épisodes, très inégaux mais attirant des signatures d'horizons très divers : maîtres du roman noir, habitués des "collections blanches" ou encore des amateurs, des collectifs. 

De janvier 2009 à janvier 2013, la collection a été dirigée par Stéfanie Delestré. Elle est ensuite dirigée par Gwenaëlle Denoyers. Cinq à six titres inédits paraissent chaque année.
Pour 2010 : Maïté Bernard, Marin Ledun, JP Jody, Sébastien Gendron, Sergueï Dounovetz, Antoine Chainas... Pour 2012 : Stéphane Pajot. Pour 2013 : Gilbert Gallerne, Christian Zeimer et Margot D. Marguerite, Philippe Franchini, Franz Bartelt…

Les illustrations de la collection « Le Poulpe » sont de Miles Hyman, qui a inauguré un nouveau style graphique avec le Poulpe de Christian Zeimert.



La plupart des titres sont des jeux de mots tirés d'expressions classiques, de titres de films ou de livres, des palindromes ("Une valse de slave nu" de Vladimir Bodiansky)...

"(les numéros qui manquent correspondent aux ouvrages parus dans les autres collections des éditions Baleine)"

Hors collection :

Certains numéros ont été réédités aux éditions J'ai lu, dans la collection « Librio noir » :


Autres rééditions :

Chéryl, la compagne du Poulpe est le personnage principal de quelques romans de la collection :

Plusieurs ouvrages sont collectifs :

"Babel ouest" de Gérard Alle (239) est écrit en français puis en breton.

Marcus Malte et Maïté Bernard font chacun référence dans leur Poulpe au "Faucon maltais" de Dashiell Hammett.

Les éditions Baleine ont commercialisé une bière, « La Poulpeuse », fabriquée en Bretagne, par la coopérative Tri Martelod.

L'idée de ce type de héros récurrent que s'approprient des auteurs différents a été copiée avec plus ou moins de bonheur. Citons :


Une bande dessinée "Le Ploupe" de Thon paraît régulièrement dans le "Psikopat".

Le groupe punk Zampano a réalisé en collaboration avec les écrivains Jean-Bernard Pouy et Jean-Christophe Pinpin "Le Bruit des boucliers" (Bakalao Producto), EP 6 titres consacré au Poulpe.




</doc>
<doc id="17781" url="https://fr.wikipedia.org/wiki?curid=17781" title="Syndicat intercommunal">
Syndicat intercommunal

Un syndicat intercommunal est, en France, un établissement public de coopération intercommunale organisé en vue de coopérer sur des services d'intérêt intercommunal. Doté d'une structure propre gérée de façon indépendante il est régi par des règlements et lois qui en fixent les cadres juridiques et réglementaires.

Le syndicat intercommunal est la structure la plus souple en matière de coopération intercommunale. Cette forme de coopération est régie par les parties législatives et réglementaires du code général des collectivités territoriales, dont l'article L.5212-1 dispose que : 
Les syndicats de communes se classent en deux catégories :

Le rapport «Solidarité et performance» de décembre 2006, adressé au ministre délégué au Budget et à la réforme de l'État par Pierre Richard, président du conseil d'administration de Dexia, a recommandé la suppression des syndicats intercommunaux, sauf décision préfectorale contraire.

Début 2011, il y avait syndicats intercommunaux, nombre en diminution de près de 2,8 % par rapport à 2010 et de 13,2 % par rapport à 2007, compte tenu de la croissance régulière du nombre d'EPCI à fiscalité propre (tels que les communautés de communes), qui absorbent des compétences antérieurement assurées par des syndicats de communes.

Ces syndicats se répartissent comme suit : 

Les compétences des syndicats sont généralement liées à des activités en réseaux, comme la collecte et le traitement des ordures ménagères, la distribution d'énergie, les communications électroniques, l'exploitation d'un centre de ressources informatiques.

Ces syndicats peuvent exercer les compétences suivantes :



</doc>
<doc id="17783" url="https://fr.wikipedia.org/wiki?curid=17783" title="FireWire">
FireWire

FireWire est le nom commercial donné par Apple à une interface série multiplexée, aussi connue sous la norme IEEE 1394 et également connue sous le nom d'interface i.LINK, nom commercial utilisé par Sony. Il s'agit d'un bus informatique véhiculant à la fois des données et des signaux de commandes des différents appareils qu'il relie.

, on peut l'utiliser pour brancher toutes sortes de périphériques friands en bande passante et qui nécessitent que le débit de données soit stable, notamment dans le cadre des disques durs et des caméscopes numériques. Elle permet d'alimenter un périphérique, ainsi que de raccorder 63 périphériques par bus, garantissant leur branchement/débranchement pendant que le système est en marche (dit « à chaud » familièrement). On peut raccorder jusqu'à par l'intermédiaire de passerelles.

Apple songea dès 1986 à créer un bus de communication pour fédérer l'industrie, confrontée à plusieurs normes concurrentes. Pour l'entreprise, il s'agit également de trouver le successeur de l'Apple Desktop Bus. C'est l'ingénieur de chez National Semiconductor Michael Johas Teener qui fut embauché dans cette tâche. Les premières démonstrations eurent lieu en 1993 au COMDEX, par Apple, IBM, Maxtor, Adaptec et Western Digital.

Les entreprises furent divisés sur son nom : i.LINK pour Sony, Lynx pour Texas Instrument (même si le fabricant n'est pas grand public) et FireWire, avec un "camel case", pour la majorité des entreprises. Lors des années 1990, les divisions en interne font que le port est quasiment à l'abandon mais fut finalement maintenu. Les premiers Mac ayant le port sont l'iMac G3 et le Power Macintosh G3 (Bleu et Blanc) en 1999. Le principal argument marketing était lié à l'essor des caméscopes numériques. Mais au fil des années, la norme USB soutenue par Intel prit l'ascendant, Apple se retrouve isolé. En fait, Apple voulut faire payer une licence pour l'utilisation de son port, ce qui provoqua la rupture avec Intel. Les caméscopes utilisant le FireWire se raréfient, Steve Jobs en avait conscience dès 2008, où le MacBook Air ne le prenait pas en charge. Lors des révisions de la gamme des Mac de 2012 à 2014, le port FireWire est supprimé, obligeant aux utilisateurs de la norme à l'acquisition d'un adaptateur, Apple préférant alors le Thunderbolt.

FireWire utilise un multiplexage temporel : le temps est découpé en tranches de 125 microsecondes ( cycles par seconde), les données étant découpées en paquets. Dans chaque tranche sont tout d'abord transmis les paquets isochrones (son, vidéo…) puis les paquets asynchrones (données informatiques). Ce système garantit la bande passante pour les flux vidéo évitant ainsi des effets de saccades et autres pertes de qualité. Les flux isochrones sont identifiés par un canal (), et doivent tous avoir un paquet par tranche ; une fois les paquets isochrones émis le reste du cycle est utilisé pour les paquets asynchrones identifiés non pas par un canal mais par l'identifiant du périphérique émetteur et l'identifiant du périphérique destinataire.

FireWire est dit (branchement à chaud) ; la connexion ou la déconnexion d'un périphérique déclenche un événement chez tous les autres périphériques : ainsi tout le monde sait à tout moment qui est présent sur le bus. À chaque les périphériques reçoivent un numéro d'identification de 0 à 62 ; celui qui a le plus grand numéro est élu chef du bus ou , et c'est lui notamment qui est chargé de marquer le début des cycles de 125 microsecondes. Tout périphérique peut ainsi être contrairement à l'USB où ce rôle est assuré par l'hôte auquel les périphériques sont reliés.

Bien qu'il serve le plus souvent à connecter des disques durs ou des caméscopes pour réaliser des montages vidéo, ou pour réaliser des captures audio via des cartes son externes, le port FireWire peut aussi, pour des besoins ponctuels, servir à relier deux machines en réseau ; il apparaît donc comme faisant partie des périphériques de « Connexions réseau » sous Windows XP et comme interface réseau sous les systèmes utilisant un noyau Linux ou UNIX.

Deux brochages distincts existent en s400 et s800 : le format à 6 broches permettant l'alimentation des périphériques et le format à 4 broches sans alimentation.
Le format à quatre broches est celui des PC portables et des caméscope à bandes mini DV.

En s800 les connecteurs ont 9 broches. s400 et s800 sont compatibles : on peut connecter un périphérique s800 avec un s400 en utilisant un câble 9 broches vers 6 broches.

Le câble le plus répandu est constitué de fils de cuivre torsadés. Sa longueur maximale pour tous les protocoles FireWire est de . Il existe également une transmission par fibre optique, très coûteuse mais permettant d'atteindre .

Le FireWire permet de disposer de débits théoriques atteignant :

La norme IEEE 1394b peut également être appelée FireWire Gigabit, FireWire2 ou Firewire 800.

Le s1600 et s3200 ont été adoptés par l'IEEE en .


</doc>
<doc id="17785" url="https://fr.wikipedia.org/wiki?curid=17785" title="Communauté de communes">
Communauté de communes

Une communauté de communes est un établissement public de coopération intercommunale (EPCI) français à fiscalité propre, qui prévoit une intégration limitée des communes membres.

Elle est définie comme étant :
Par la population comme par le degré de coopération, elle constitue la forme la moins intégrée des EPCI à fiscalité propre, et est conçue pour faciliter la gestion locale de l'espace peu urbanisé.

Si les syndicats de communes existent depuis 1890 et les syndicats intercommunaux à vocation multiple (SIVOM) depuis le , il faut attendre 1992 pour qu'une nouvelle conception de l'intercommunalité fasse place à la liberté de négociation contractuelle et à la libre association de communes. La loi du crée deux nouvelles catégories d'EPCI à fiscalité propre : les « communautés de communes » et les « communautés de villes ». Ces groupements disposent de compétences élargies et sont obligatoirement compétents en matière d'aménagement de l'espace et de développement économique. Cette loi dote, sur le plan fiscal, ces nouvelles structures d'un régime destiné à favoriser une coopération plus intégrée. Aux communautés de communes, mais également aux communautés urbaines et aux districts existant à la date de publication de cette loi, trois régimes sont accessibles :

Les communautés de communes exercent ainsi en lieu et place des communes membres un certain nombre de compétences définies par la loi et par leurs statuts. Ce régime juridique a été modifié à plusieurs reprises, notamment par la Loi relative au renforcement et à la simplification de la coopération intercommunale du 12 juillet 1999, puis la loi du 27 février 2002, la loi n 2010-1563 du et enfin la loi n 2015-991 du , dite loi portant nouvelle organisation territoriale de la République.

Cette dernière augmente en particulier le seuil démographique pour les communautés de communes de à , afin d'en diminuer drastiquement le nombre, avec toutefois quatre possibilités d’adaptation :

Cette loi modifie également les compétences obligatoires et optionnelles attribuées aux différents groupements intercommunaux.

La communauté de communes offre une nouvelle conception de l'administration territoriale, en intégrant l'idée de projet là où il n'y avait que de la gestion. En effet, les SIVU ou les SIVOM n'ont d'autre vocation que de gérer des équipements ou infrastructures, souvent de réseau, tels le gaz, l'électricité, l'eau ou les déchets. Une seule et même commune adhère en général à plusieurs structures intercommunales, mais ne peut appartenir qu'à un seul EPCI à fiscalité propre. Une commune peut ainsi être membre d'un ou deux SIVU, d'un SIVOM, d'un SICTOM, d'un syndicat mixte et d'une communauté de communes. Si la communauté de commune acquiert une compétence gérée par une autre intercommunalité, celle-ci est dissoute si elle ne gérait que cette compétence (SIVU), ou est retirée des compétences de ladite intercommunalité, au titre du principe de spécialité et d'exclusivité des EPCI à fiscalité propre.

En 2012, il existait de communes en France (contre en 2007 et 756 en 1995). La loi n°2010-1563 du 16 décembre 2010 de réforme des collectivités territoriales impose l'élaboration d'un schéma départemental de coopération intercommunal visant à couvrir l'intégralité du territoire national par des "intercommunalités plus cohérentes", schéma devant être mis en œuvre avant le 30 juin 2013. En 2014, avec la mise en place de cette réforme, on compte ainsi de communes.

L'évolution du nombre de communautés de communes depuis 1995 est la suivante :
La communauté de communes est créée par arrêté préfectoral ou inter-préfectoral, doit concerner un territoire d'un seul tenant et sans enclave, et doit regrouper au moins , ce seuil pouvant être abaissé sans pouvoir être inférieur à pour les établissements publics de coopération intercommunale à fiscalité propre : 
"« a) Dont la densité démographique est inférieure à la moitié de la densité nationale, au sein d'un département dont la densité démographique est inférieure à la densité nationale ; le seuil démographique applicable est alors déterminé en pondérant le nombre de 15 000 habitants par le rapport entre la densité démographique du département auquel appartiennent la majorité des communes du périmètre et la densité nationale ;
"b) Dont la densité démographique est inférieure à 30 % de la densité nationale ;
"c) Comprenant une moitié au moins de communes situées dans une zone de montagne délimitée en application de l'article 3 de la loi n° 85-30 du 9 janvier 1985 relative au développement et à la protection de la montagne ou regroupant toutes les communes composant un territoire insulaire ;
"d) Ou incluant la totalité d'un établissement public de coopération intercommunale à fiscalité propre de plus de 12 000 habitants issu d'une fusion intervenue entre le janvier 2012 et la date de publication de la loi n° 2015-991 du 7 août 2015 portant nouvelle organisation territoriale de la République" ».

La création d'une communauté de communes doit également favoriser le respect du périmètre des unités urbaines, des bassins de vie et des schémas de cohérence territoriale, l'accroissement de la solidarité financière et de la solidarité territoriale, la rationalisation et la réduction du nombre de syndicats de communes et le transfert de leurs compétences à des EPCI à fiscalité propre.

La communauté de communes est gérée par un "conseil communautaire" ou "conseil de communauté", composé de conseillers municipaux des communes membres.

Chaque commune dispose au minimum d'un siège et aucune commune ne peut avoir plus de la moitié des sièges.

Jusqu'aux élections municipales de 2014, les conseillers communautaires étaient des conseillers municipaux élus par chaque conseil municipal des communes membres de la Communauté. Ce système était critiqué, étant donnée l'importance des compétences transférées, et l'absence de débat sur ces politiques en raison de l'élection des conseillers communautaires au suffrage indirect. C'est ainsi qu'à l'unanimité, les présidents des communautés se sont prononcés lors des journées communautaires de Strasbourg en 2007 pour l'élection au suffrage universel direct dès 2014, et ce pour renforcer la légitimité des communautés et leur transparence de fonctionnement.

La loi n°2010-1563 du 16 décembre 2010 de réforme des collectivités territoriales a prévu que les conseillers communautaires des communes de plus de seront élus au suffrage universel direct, dans le cadre des élections municipales. Les représentants des communes de plus petite taille resteront élus en leur seins par les conseils municipaux. Ces dispositions ont été modifiées par la loi du 17 mai 2013, qui a défini le régime suivant :

À compter des élections municipales de 2014, chaque commune est représentée au conseil communautaire par un nombre de représentants tenant compte de sa population défini aux articles L. 5211-6-1 et L. 5211-6-2 du code général des collectivités territoriales :
Conformément aux dispositions de l'article du CGCT, la communauté de communes exerce des compétences obligatoires et des compétences optionnelles, ainsi que des compétences supplémentaires qui lui sont transférées par les communes membres. 

Le conseil communautaire peut également choisir des compétences facultatives qu'il définit lui-même, après accord des communes membres. De plus, la communauté de communes peut, avec accord du département, exercer directement certaines compétences d'action sociale qui relèvent normalement de celui-ci.

Elle peut se donner compétence en matière de droit de préemption urbain, notamment en matière de politique locale de l'habitat, ou recevoir délégation du département pour exercer des fonctions d'aide sociale.

Les communes peuvent, par ailleurs, transférer ou déléguer à la communauté d'autres compétences.

L'exercice de certaines compétences nécessite que soient définies les actions et équipements « reconnus d'intérêt communautaire ». Cette déclaration d'intérêt communautaire résulte du vote d'une majorité qualifiée des conseils municipaux, ce qui est une différence fondamentale par rapport au régime des communautés d'agglomération, où cette déclaration d'intérêt communautaire est faite par une délibération du conseil communautaire prise à la majorité des deux tiers du conseil de la communauté d'agglomération, donnant ainsi un pouvoir important aux instances communautaires.

À partir du moment où les compétences sont transférées à la communauté, les communes ne peuvent plus les exercer, sauf en matière de logement social, où la commune et l'intercommunalité peuvent toutes deux intervenir pour financer des opérations ou en garantir les emprunts.

La loi pour l'accès au logement et un urbanisme rénové n°2014-366 du 24 mars 2014 , dite loi ALUR, fait évoluer de manière significative les compétences en matière d'urbanisme, les règles et les documents d'urbanisme. Elle transfère en particulier la compétence "documents d'urbanisme et de planification" aux intercommunalités :

Les établissements publics de coopération intercommunale (EPCI) peuvent opter pour un transfert volontaire de compétence, par délibération entre le et le , dans les conditions définies par l'article L5211-17 du Code général des collectivités territoriales. Dans le cas contraire, le transfert interviendra de fait le , sauf si un quart des communes représentant au moins 20 % de la population s'y oppose par délibération, entre le et le .

La loi portant nouvelle organisation territoriale de la République promulguée le fait évoluer les compétences des intercommunalités, aussi bien obligatoires qu’optionnelles, avec des incidences sur l’articulation avec les syndicats intercommunaux ou mixtes. Concernant les communautés de communes, les modifications concernant les compétences obligatoires sont les suivantes :

Concernant les compétences optionnelles, au moins trois compétences devront être choisies parmi neuf jusqu’au , et ensuite parmi sept, en raison d’un basculement de certaines compétences. Les compétences optionnelles regroupent la création et la gestion des maisons de service public avant le , ainsi que l’assainissement et l’eau, qui seront optionnels jusqu’au . En effet, elles deviendront ensuite des compétences obligatoires.

L'évolution des compétences obligatoires et optionnelles de l'application de la Loi NOTRe pour les communautés de communes se traduit par le tableau suivant :

La Communauté de communes est un établissement public de coopération intercommunale à fiscalité propre.

Afin de financer l'exercice de ses compétences, la communauté de communes peut opter pour : 





</doc>
<doc id="17786" url="https://fr.wikipedia.org/wiki?curid=17786" title="Jean-Pierre Darroussin">
Jean-Pierre Darroussin

Jean-Pierre Darroussin, né le à Courbevoie (Seine), est un acteur et réalisateur français.

Jean-Pierre Darroussin est élevé avec sa sœur dans une famille modeste à Courbevoie. Il est le fils d’un artisan ouvrier étameur et communiste, et d'une mère au foyer. Proche de la Gauche prolétarienne après mai 1968, il vit « de débrouilles plus ou moins légales » et fonde une coopérative militante de coursiers pour livrer la presse. Il se découvre une passion pour le théâtre pendant ses études au lycée Paul-Lapie. Il entre en 1974 au cours Florent puis, en 1975, à l'école de la rue Blanche, formation qui lui permet d'intégrer en 1976, à la troisième tentative, le Conservatoire national supérieur d'art dramatique de Paris dans la classe de Marcel Bluwal, avec Catherine Frot et Ariane Ascaride. 

Il se fait connaître du grand public en 1980 dès son second film, "Psy" de Philippe de Broca.

De 1978 à 1986, il travaille au théâtre, entre autres pour la « Compagnie du Chapeau Rouge » de Pierre Pradinas avec sa complice Catherine Frot. Après l'éclatement de la troupe, il change de métier et devient instituteur à la campagne pendant un an et demi, puis revient au cinéma après le casting du film "Mes meilleurs copains" de Jean-Marie Poiré.

Il devient au fil des années l'un des acteurs fétiches de Robert Guédiguian. 

En 1997, il est récompensé du César du meilleur acteur dans un second rôle pour son interprétation dans "Un air de famille" de Cédric Klapisch, d'après la pièce de théâtre du même nom d'Agnès Jaoui et Jean-Pierre Bacri.

En 1998, il joue son premier « premier rôle » dans "Le Poulpe" de Guillaume Nicloux, puis "15 août", "Le Cœur des hommes" et poursuit depuis avec succès une carrière d'acteur connu du cinéma français.

De 2015 à 2017 il campera le personnage d'Henri Duflot, le chef de la DGSE, un des rôles principaux dans la série Le Bureau des légendes diffusée par Canal+.

Il a deux filles, Marie et Juliette, nées d'une première union. Il partage ensuite, pendant plusieurs années, la vie de Valérie Stroh, actrice, scénariste et réalisatrice française. Il est en couple, depuis 2009, avec la réalisatrice franco-suédoise Anna Novion qu'il a épousée, ils ont eu un fils, Vincent, né en mai 2014.

Lors de la campagne pour l'élection présidentielle de 2007, il soutient Ségolène Royal.






-Philipe







</doc>
<doc id="17788" url="https://fr.wikipedia.org/wiki?curid=17788" title="Anarcho-syndicalisme">
Anarcho-syndicalisme

L'anarcho-syndicalisme, syndicalisme libertaire ou syndicalisme anarchiste est un syndicalisme basé sur les principes de fonctionnement de l'anarchisme. Il propose une méthode, le syndicalisme, couplée à l'action directe et à la grève générale expropriatrice, comme moyens de lutte et d'accès vers une société libertaire.

Il pose le primat de la logique syndicale face à l'action politique ou partisane dans le développement du mouvement ouvrier et l'émergence d'une société décentralisée, libre de toute forme de coercition étatique et fondée avant tout sur l'autogestion des unités de production.

L'idéologie de l'anarcho-syndicalisme procède pour l'essentiel de la pensée de Michel Bakounine. Il peut être compris comme synonyme de socialisme libertaire ou de communisme libertaire. Il est parfois confondu avec le syndicalisme révolutionnaire.

L'anarcho-syndicalisme devient le courant dominant au sein de l'anarchisme après la faillite de sa tendance insurrectionnelle au cours des années 1880-1890. Il ne se configure en tant que mouvement cohérent qu'au début du . À partir de ce moment et jusqu'en 1914, les anarcho-syndicalistes français influencent de façon décisive les orientations de la Confédération générale du travail (CGT). En Espagne, ils jouent un rôle plus éminent encore au sein de la Confédération nationale du travail.

Le projet des anarcho-syndicalistes est l'établissement d'un nouvel ordre social juste et émancipateur (et non pas le « désordre » social), grâce à l'abolition conjointe du capitalisme et de l’État. Les anarcho-syndicalistes proposent de substituer à la propriété privée, la « possession individuelle » ne garantissant aucun droit concernant l'accumulation des biens « non utilisés ».

Dans ce courant de philosophie politique, il n'y a pas de centralisme économique ou politique. Les formes d'organisations sont multiples et s'appuient sur la liberté politique grâce au mandatement impératif, à l'autogestion, au fédéralisme et à la démocratie directe.

Pour ses partisans, l'anarcho-syndicalisme est donc organisé et structuré : il pose le syndicat comme forme d'organisation émancipatrice des travailleurs et refuse le principe de parti politique ou de regroupement corporatiste. Le syndicat est la structure qui permet aux classes opprimées de s'organiser à la base et de mener la lutte selon les choix des individus regroupés en collectifs et non selon des directives hiérarchiques données par un bureau politique (en d'autre termes, du bas vers le haut et non du haut vers le bas, ou à l'horizontale, telle l'utopie de l'égalité).

Selon le "Dictionnaire de la science politique et des institutions politiques" (2010) : « Globalement, l'anarcho-syndicalisme érige les syndicats en organismes centraux de l'action politique et des transformations sociales dans la perspective d'une rupture révolutionnaire fondée en même temps sur une prise de conscience progressive de leur pouvoir par les masses populaires. De ce fait, il se présente à la fois comme un projet d'organisation et comme le vecteur d'une nouvelle éthique de la responsabilité à diffuser dans l'ensemble de la société ».

Le syndicalisme révolutionnaire est apparu vers la fin du , marquant certaines des pages essentielles de l'histoire du mouvement ouvrier. Il est l'un des courants fondateurs de la CGT française au tournant du siècle, avec de nombreux militants anarchistes ou ex-anarchistes comme Émile Pouget, rédacteur en chef de la revue "Le Père Peinard", ou Fernand Pelloutier, fondateur des Bourses du travail. Après la Première Guerre mondiale et la Révolution bolchévique, la CGT se scinde en 1921, une partie créant la Confédération générale du travail unitaire (CGTU) qui appuie les bolcheviques.

Or, le terme même d'« anarcho-syndicalisme » n'apparaît qu'au Congrès, fondateur, de Saint-Étienne de la CGTU, en 1922, lorsque celle-ci discute de son adhésion à l'Internationale syndicale rouge (ISR). Alexandre Lozovski, secrétaire général de l'ISR, utilise en effet ce néologisme pour ridiculiser les « minoritaires » de la Commission exécutive, opposés à Gaston Monmousseau et accusés de « scissionnisme » et de collusion avec la CGT. Au même moment, tentant de rejeter les « minoritaires » opposés à l'adhésion à l'ISR bolchevique du côté de la CGT, et ainsi de créer le clivage réformistes/révolutionnaires, Lozovski parle d'« anarcho-réformisme ». Cette conjonction, entre l'« anarcho-syndicalisme » et l'« anarcho-réformisme », est à nouveau théorisée, l'année suivante, par Andrés Nin, secrétaire général adjoint de l'ISR, qui se réclame du « communisme » et du « syndicalisme révolutionnaire » contre l'« anarchisme », peu ou prou assimilé au socialisme utopique. L'une des clefs du conflit, à ce moment, réside dans le refus tactique de la minorité à poursuivre « la grève pour la grève », tandis que les futurs bolcheviques misent au contraire sur une intensification de la grève révolutionnaire.

L'anarcho-syndicalisme devient un des courants importants du syndicalisme français, en réaction à la montée en puissance du Parti communiste français durant les années 1930, au sein de la CGT-U. Il en résultera une scission et la création par des militants syndicalistes purs (anarchistes) et des syndicalistes révolutionnaires d'une éphémère Confédération générale du travail - Syndicaliste révolutionnaire, dont la Charte fonde l'anarchosyndicalisme français. Cette organisation regroupera environ 2000 adhérents selon un de ses animateurs, Paul Lapeyre (interview pour la revue anarchiste "Les Œillets rouge" en 1986). La CGT-SR, interdite en 1939, ne survivra pas à la Seconde Guerre mondiale, ses adhérents rejoignant la CGT en 1945, préparant la fondation de Force ouvrière ou créant la CNT française (Confédération Nationale du Travail).

Mais l'heure de gloire européenne de l'anarchosyndicalisme est espagnole : c'est en 1936, lors de l'insurrection des militaires franquistes et des milices d'extrême droite que la CNT espagnole, confédération anarchosyndicaliste forte de deux millions d'adhérents, lance un vaste mouvement de collectivisation des terres et des industries dans les zones qu'elle contrôle. Les militants de la CNT sont parmi les premiers à se rendre au front et à donner un coup d'arrêt à l'avancée des troupes franquistes, côte à côte avec les soldats restés fidèles à la république et des militants marxistes.
La suite de la guerre verra l'affaiblissement de la CNT face aux manœuvres hégémoniques du parti communiste stalinien, et la fin de la guerre en 1938 verra une répression brutale s'abattre sur les militants espagnols, pour beaucoup contraints de se réfugier en France. Ces derniers formeront la base des maquis anarchistes du sud de la France, et seront à l'origine de la création en 1946 de la CNT française.

Le mouvement anarchosyndicaliste (ou plus exactement anarchiste ouvrier) a également eu une influence prépondérante en Amérique Latine, où il est à l'origine du mouvement syndical dans de nombreux pays. En Argentine, la Fédération ouvrière régionale argentine (FORA) a représenté une organisation de masse capable d'inquiéter l'État et le patronat, avant d'être laminée par une répression féroce.

Confrontés à l'omniprésence de militants marxistes dans les milieux syndicaux, l'anarchosyndicalisme n'arrivera jamais à retrouver l'influence idéologique dont il jouissait au début du siècle ; quoique ces dernières années, on assiste à un retour en force des idéologies autogestionnaires, antiautoritaires et anticapitalistes dans les discours militants.

Les militants anarchosyndicalistes ont théorisé nombre de pratiques syndicales. S'ils ont beaucoup réfléchi sur la grève générale comme moyen pour la classe ouvrière de se réapproprier ses outils de production, ils ont aussi popularisé l'action directe (occupations, piquets de grève), le sabotage (refus de produire des marchandises de qualité, et boycott par les prolétaires des produits en question) comme moyens d'action, ainsi que, dans certains cas, la réappropriation directe des richesses produites.

Prônant l'antiautoritarisme et le libre choix des travailleurs en lutte quant aux modalités de l'organisation et du suivi des conflits, et refusant toute idée d'État, fut-il prolétarien, les anarchosyndicalistes (et les anarchistes en général) se sont très souvent trouvés férocement opposés aux militants d'obédience marxiste.

Un principe majeur de l'anarchosyndicalisme est de lier fortement la lutte contre les formes d'exploitation et d'aliénation dans la société actuelle avec l'objectif visé de construire une société communiste libertaire. En d'autres termes, l'anarcho-syndicalisme en tant que théorie et pratique, intègre une vision d'un projet politique global. Ce lien fort entre anarchosyndicalisme et communisme libertaire se traduit par une identité de principes et de pratiques : démocratie directe, rotation des tâches, antiautoritarisme, solidarité, fédéralisme. On observe cependant, à des titres divers selon les organisations qui se réclament de l'anarcho-syndicalisme, des écarts entre ces principes et les pratiques concrètes. Dans certains cas, l'écart devient si grand, l'attachement à certains de ces principes si faible, que l'identité « anarcho-syndicaliste » ou encore « anarchiste » est régulièrement niée : un travail de redéfinition identitaire et généralement stratégique est alors à l'œuvre. 

À la différence du courant industrialiste nord-américain (les IWW principalement, souvent qualifiés à tort d'anarcho-syndicalistes alors que l'industrialisme se veut a-idéologique), le courant européen et d'Amérique latine s'est orienté vers une démarche globaliste : ses militants ont étendu leur réflexion et leurs pratiques dans de nombreux domaines, bien au-delà de la stricte (et nécessaire selon eux) action syndicale : éducation, formation, bibliothèques, libération sexuelle, etc.

Les militants anarchosyndicalistes escomptent mettre en œuvre un tel projet politique dès qu'un rapport de forces favorable permet d'enclencher un processus révolutionnaire. 

L'essentiel des anarchosyndicalistes est organisée au niveau international au sein de l'Association internationale des travailleurs (AIT), reconstruite en 1922 et héritière de la Première Internationale du même nom: Association internationale des travailleurs.

Depuis le début des années 1990, l'AIT a connu un renouvellement : les groupes qui se sont éloignés des positions anarchosyndicalistes au profit de tactiques syndicalistes révolutionnaires (au sens marxiste du terme) ont été exclus (CNT dite Vignoles en France, groupe de Rome de l'USI Italienne). Mais elle a été rejointe par de nouveaux groupes, notamment d'Europe de l'Est (Russie, Tchéquie, Slovaquie) et ce quelques années à peine après le Chute du mur de Berlin. Après le congrès de 2000, ce redéploiement, timide mais réel se confirme (Serbie, Brésil, etc.).

En France, il existe depuis 1993 deux organisations dénommées « CNT », mais une seule est reconnue comme section de l'AIT (depuis le congrès de décembre 96) et s'intitule « CNT-AIT ».

Aujourd'hui le mouvement anarchosyndicaliste se développe en France, on doit par exemple citer: l'Union des Anarcho-Syndicalistes (UAS), le Syndicat intercorporatif anarchosyndicaliste (SIA) et le Groupement d'Action et de Réflexion AnarchoSyndicaliste (GARAS).

Sur le plan international, la CGT espagnole (CGTe), La CNT espagnole, la SAC suédoise, le groupe de Rome de l'USI en Italie, les Industrial Workers of the World ou IWW essentiellement dans les pays anglo-saxons, se revendiquent également - au moins partiellement - de l'anarchosyndicalisme.

Le drapeau rouge et noir est le symbole des mouvements anarcho-syndicalistes et anarcho-communistes. Le noir étant la couleur traditionnelle de l'Anarchisme et le rouge celle du Socialisme. Le drapeau rouge et noir réunit ces deux couleurs en parts égales, séparées par une diagonale. La partie rouge est habituellement placée en haut à gauche, et la noire en bas à droite. Symbole de la cohabitation des idéaux anarchiste et socialiste au sein du mouvement anarcho-syndicaliste, il représente également les moyens sociaux mis en œuvre par le mouvement pour arriver à des fins anarchistes.

Une des versions les plus connues de ce drapeau anarchiste est celle du syndicat Espagnol "Confederación Nacional del Trabajo" (Confédaration Nationale du Travail, CNT), qui existe aussi en France. La CNT, ainsi que la "Federación Anarquista Ibérica" (Fédération Anarchiste Ibérique, FAI), étaient totalement impliquées dans le mouvement anarchiste populaire de la fin du XIX, début XX siècles. Ce groupe a sa propre version du drapeau rouge et noir, arborant ses initiales. Les lettres CNT apparaissent dans le rouge tandis que les lettres FAI sont dans le noir (couleur de l'anarchie) puisque le FAI a été fondé en 1927 pour rappeler à la CNT que ses principes de base sont anarchistes.

Le Zabalaza Anarchist Communist Front (ZACF ou ZabFront), Une organisation politique anarchiste Sud-Africaine utilise également cette symbolique rouge et noire à travers son logo. Le ZACF s'est inspiré du "Organisational Platform of the Libertarian Communists". Les membres du ZAFC partage les mêmes principes d'unité, de responsabilité collective et de fédéralisme que ceux de la plate-forme. D'un point de vue historique, la tradition plate-formiste aurait débuté avec "The Organizational Platform of the Libertarian Communists", auquel ce serait ajouté, pendant la période d'après guerre, des documents tel que le pamphlet de Georges Fontenis "Manifesto of Libertarian Communism".

Dessiné avec le dos arqué, la queue ébouriffée, montrant les griffes et les dents, le "Sabo-Cat" ou "Tabby-Cat" a longtemps été un symbole de l"'Industrial Workers of the World". Il a été dessiné par Ralph Chaplin, un des précurseurs des illustrations surnommées « agitations silencieuses » au sein de l'IWW. Ces illustrations étaient étroitement liées à la vie des sans domicile fixe. Bien qu'aujourd'hui généralement associé à l'IWW (parfois même en tant que mascotte), il s'agissait au départ d'un appel à l'action directe sur le lieu de production, en particulier au sabotage. Comme sa position le suggère, le chat symbolise la grève et le syndicalisme radical. L'IWW (ou les "Wobblies") était un syndicat important, il fut notamment le premier syndicat américain à recruter et à collaborer avec des femmes et des personnes de couleurs, de plus il joua un rôle crucial dans la lutte pour les 8 heures de travail par jour et la liberté d'expression au travail dans tous les États-Unis au début du . À partir de 1905, les "Wobblies" connurent une période de notoriété significative, jusqu'à ce qu'ils soient réduits à néant par les "Palmer Raids".

Les origines de ce symbole ne sont pas claires, mais selon une anecdote, il proviendrait d'une grève de l'IWW qui aurait dégénéré. Plusieurs membres ont été tabassés et conduits à l'hôpital, au même moment un chat noir maigrelet s'installa dans le camp des grévistes. Les grévistes nourrirent le chat et il reprit force à mesure que la grève tournait en faveur des travailleurs. Finalement, les travailleurs virent quelques-unes de leurs demandes satisfaites et adoptèrent le chat comme mascotte.

Le nom de « Chat Noir » a été adopté par de nombreux organismes et collectifs se revendiquant du mouvement anarchiste, ainsi que par une scène musicale culte à Austin, Texas (lieu qui a fermé à cause d'un incendie le 6 juillet 2002).

Le sabot était un symbole anarchiste au , bien que son utilisation ait progressivement disparue depuis. Le mot sabotage provient sans doute du mot sabot, en référence à une tactique de syndicalistes hollandais, qui lançaient leurs sabots dans les engrenages des usines, ce qui engendrait un arrêt du travail jusqu'à réparation de la panne. Une technique similaire, adoptée par les Américains pour empêcher les non-grévistes de remplacer les syndiqués en grève, consistait à lancer des clés à molettes à la place des sabots ("monkeywrenching").





</doc>
<doc id="17789" url="https://fr.wikipedia.org/wiki?curid=17789" title="Un air de famille (film)">
Un air de famille (film)

Un air de famille est un film français, réalisé par Cédric Klapisch en 1996, d'après la pièce de théâtre du même titre d'Agnès Jaoui et de Jean-Pierre Bacri.

La famille Ménard se réunit toutes les semaines « Au Père Tranquille », le café tenu par Henri, le fils aîné. Cette fois-ci, elle s'y rassemble pour célébrer le de Yolande, épouse de Philippe, le cadet. Pendant que l'on attend Arlette, la femme d'Henri qui, on l'apprendra en début de film, vient tout juste de quitter le domicile conjugal, les petites préoccupations de Philippe, cadre dans une société d'informatique, débutant une carrière politique, prennent rapidement le pas sur les politesses d'usage. Comme sa mère ne manque jamais de le rappeler, celui-ci occupe une position importante dans une grande entreprise de programmation de la région. Elle s'inquiète également du célibat de sa fille Betty, la benjamine, qui sort secrètement avec Denis, le serveur du « Père tranquille ». Alors que les vieilles rancunes ressurgissent, le ton ne cesse de monter jusqu'à l'avènement d'un nouvel ordre familial.





Les scènes d'intérieur du film ont été tournées en studio, aux studios SETS de Stains.

Les scènes d'extérieur du film ont été tournées à Stains (18, rue Hennequin). On reconnait aisément le café et la voie de chemin de fer en arrière-plan, à l'angle de la rue des Parouzets et de la rue Hennequin. Le passage à niveau est aujourd'hui supprimé.

Jean-Pierre Bacri essaye d'interpeller sa compagne depuis le passage Elisabeth de Saint-Ouen (Seine-Saint-Denis).

Le film est inspiré de la pièce de théâtre "Un air de famille" de Jean-Pierre Bacri et Agnès Jaoui, deuxième pièce du couple d'auteurs, après "Cuisine et Dépendances". Ces deux pièces ont chacune obtenu le Molière du meilleur spectacle comique (respectivement en 1992 et 1995).

La distribution du film est identique à celle de la pièce.



</doc>
<doc id="17790" url="https://fr.wikipedia.org/wiki?curid=17790" title="Mes meilleurs copains">
Mes meilleurs copains

Mes meilleurs copains est un film français réalisé par Jean-Marie Poiré, sorti en 1989.

Cinq potes approchant la quarantaine (Richard, Jean-Michel, Guido, Antoine et Dany) se retrouvent dans la maison de Richard, à l'occasion de la venue à Paris de Bernadette Legranbois, l'ancienne chanteuse du groupe qu'ils formaient tous les six au début des années 1970, et qui les avait quittés vingt ans auparavant pour faire carrière à Montréal.

Entre éclats de rire, souvenirs des années 1970, flash-back, et remises en questions, quelques vieilles rancunes et histoires ressurgissent.







</doc>
<doc id="17791" url="https://fr.wikipedia.org/wiki?curid=17791" title="Robert Guédiguian">
Robert Guédiguian

Robert Guédiguian, né le à Marseille, est un réalisateur de cinéma, producteur et scénariste français.

En 1997, il est récompensé par le prix Louis-Delluc pour son film "Marius et Jeannette", sélectionné au festival de Cannes 1997 dans la section Un certain regard.

Né à Marseille, Robert Guédiguian est d'origine arménienne et fils d'un ouvrier électricien travaillant à bord des bateaux dans le port de la ville. Il fréquente assidument les salles de cinéma durant son enfance et son adolescence. Il quitte ensuite sa ville natale, dont il fait cependant le décor de prédilection de ses films. 

C’est à travers Marseille et, particulièrement le quartier de L'Estaque, qu'il scrute l’histoire de ceux qu’il appelle, en référence à Victor Hugo, les « pauvres gens » : ouvriers, salariés, petits patrons, chômeurs, déclassés.

Dans presque tous les films de Robert Guédiguian se trouvent trois comédiens récurrents – ensemble ou deux par deux : sa compagne et « muse » Ariane Ascaride, Gérard Meylan et Jean-Pierre Darroussin. Il entretient également une relation continue avec le scénariste Jean-Louis Milesi, une troupe d’acteurs et une équipe technique (le monteur Bernard Sasia, l’ingénieur du son Laurent Lafran, le décorateur Michel Vandestien, etc.). 

En dehors des fictions ayant pour cadre la cité phocéenne, il a réalisé une commande sur les derniers jours de François Mitterrand ("Le Promeneur du Champ-de-Mars", 2005) et rendu hommage à un pays, l'Arménie, qu’il n’a jamais revendiqué comme sien, mais qu’il découvre dans "Le Voyage en Arménie" (2006) sans jamais se départir d’un point de vue personnel.

Producteur indépendant, Robert Guédiguian est partenaire d'une maison de production en nom collectif (Agat Films & Cie / Ex Nihilo) qui intervient de façon militante dans la totalité du champ de la création audiovisuelle et du spectacle vivant. Dans son cinéma comme dans son activité de producteur plane l’idée utopique que l’art conscient de lui-même peut changer le monde, sans que l’artiste lucide néglige d’intervenir directement dans le débat public, à l’instar d’un Pier Paolo Pasolini, auteur de référence.

En 2018, il est membre du jury au Festival de Cannes, présidé par Cate Blanchett, aux côtés des actrices Léa Seydoux et Kristen Stewart, de la réalisatrice Ava DuVernay, de la chanteuse Khadja Nin, de l'acteur Chang Chen et des réalisateurs Denis Villeneuve et Andreï Zviaguintsev.

Robert Guédiguian adhère au Parti communiste français à 14 ans, en 1968 (il a même vendu "L'Humanité Dimanche" à la criée dans la rue avec Alexandre Adler). Rêvant de devenir un intellectuel communiste, il dévore le "Manifeste du parti communiste" et "La Guerre civile en France". Il est très motivé, jusqu'à l'abandon du programme commun et l'éclatement de la gauche française, à l'automne 1977. Il rend sa carte deux ans plus tard et réalise son premier film.

Robert Guédiguian soutient, sans y adhérer, le Parti de gauche (PG) dès sa fondation en novembre 2008 et la liste du Front de gauche aux élections régionales françaises de 2010.

En 2011, il soutient la candidature de Jean-Luc Mélenchon à l'élection présidentielle de 2012.

En 2017, il annonce dans un billet posté sur "Huffington Post" qu'il votera Mélenchon en écrivant : 

Robert Guédiguian est l'époux de l'actrice française Ariane Ascaride. Ils se sont rencontrés à la faculté d'Aix-en-Provence dans les années 1970 alors qu'elle était militante au sein de l'UNEF. En 1980, Guédiguian la fait jouer dans son premier long métrage, "Dernier été". Ascaride sera ensuite à l'affiche de tous ses films, à l'exception du "Promeneur du Champ-de-Mars", en 2005.






Trois bandes dessinées ont paru chez Emmanuel Proust Éditions, inspirées de films de Robert Guédiguian :



</doc>
<doc id="17794" url="https://fr.wikipedia.org/wiki?curid=17794" title="Marie-Jo et ses deux amours">
Marie-Jo et ses deux amours

Marie-Jo et ses deux amours est un film français réalisé par Robert Guédiguian (2002).

Marie-Jo aime profondément son mari Daniel ainsi que son amant Marco... Mais elle ne peut les vivre simultanément.
Elle ira même jusqu'à provoquer la destruction de ce fragile équilibre, avec une fin inéluctable et tragique.







</doc>
<doc id="17795" url="https://fr.wikipedia.org/wiki?curid=17795" title="Ariane Ascaride">
Ariane Ascaride

Ariane Ascaride est une actrice française, née le à Marseille. 

En 1998, elle obtient le César de la meilleure actrice pour son rôle dans "Marius et Jeannette" de Robert Guédiguian, avec qui elle a tourné dix-huit films (soit la moitié de sa filmographie).

Fille d’Henriette, une employée de bureau, et d’un représentant, lui-même fils d’immigré napolitain, et sœur du metteur en scène Pierre Ascaride et de l'écrivain Gilles Ascaride, Ariane Ascaride assiste très tôt aux spectacles amateurs auxquels participe son père.

Elle étudie la sociologie à l’université d’Aix-en-Provence où elle s'engage à l'Union nationale des étudiants de France (UNEF). Elle rencontre alors Robert Guédiguian qui deviendra son mari.

Elle entre au Conservatoire national d'art dramatique de Paris où elle suit les cours d'Antoine Vitez et Marcel Bluwal. Dans les années 1970, elle débute au théâtre dans les pièces de son frère Pierre Ascaride, l'un des inventeurs du théâtre à domicile, puis joue dans des petits rôles au cinéma. Son premier véritable rôle lui est offert par René Féret dans "La Communion solennelle" (1977).

À partir de 1980, Ariane Ascaride va apparaître dans tous les films de son mari. Elle est la seule comédienne professionnelle de son premier film, "Dernier été". Si "À la vie, à la mort !" est plébiscité par la critique en 1995, elle n’est véritablement révélée au grand public qu’avec "Marius et Jeannette" qui lui vaut le César de la meilleure actrice en 1998.

À la fin des années 1990, d'autres cinéastes tels que Dominique Cabrera ("Nadia et les hippopotames" en 1999) ou encore Olivier Ducastel et Jacques Martineau ("Drôle de Félix" en 2000) font également appel à elle.

En 2006, on la retrouve à l’affiche du "Voyage en Arménie" de Robert Guédiguian, dont elle est coscénariste mais aussi de "Miss Montigny" de Miel Van Hoogenbemt et de "Changement d'adresse" d’Emmanuel Mouret.

Ariane Ascaride est membre du comité de parrainage de la Coordination française pour la Décennie de la culture de paix et de non-violence.

Ariane Ascaride est la sœur de Pierre Ascaride et de Gilles Ascaride et est l'épouse du réalisateur français Robert Guédiguian. Sa fille Anaïs Ascaride collabore à Agat Films & Cie - Ex Nihilo en tant qu'assistante de production et directrice de production.













</doc>
<doc id="17798" url="https://fr.wikipedia.org/wiki?curid=17798" title="Shanghai">
Shanghai

Shanghai ou Shanghaï (, ÉFÉO : Chang-haï, ; shanghaïen : "Zanhe") est la ville la plus peuplée de Chine (en population urbaine). Elle constitue aussi l'une des plus grandes mégapoles du monde avec plus de 24,15 millions d'habitants (2015). Elle se situe sur la rivière Huangpu près de l'embouchure du Yangzi Jiang, à l'est de la Chine.

L'émergence de la ville comme centre financier de l'Asie-Pacifique, au , s'est faite dans la douleur, avec l'occupation étrangère de la ville pendant plusieurs décennies. Dans les années 1920 et 1930, Shanghai a été le théâtre d'un formidable essor culturel qui a beaucoup contribué à l'aura mythique et fantasmatique qui est associée à la ville depuis cette époque.

Après la fondation de la République de Chine et la guerre sino-japonaise, l'avènement de la République populaire de Chine a muselé la ville économiquement et culturellement, considérée comme un foyer de bourgeoisie et de dépravation, jusqu'à ce que Deng Xiaoping en 1992 décide de promouvoir le développement de la ville.
Il semble aujourd'hui que la ville soit en passe de retrouver la place de centre financier de l'Asie qu'elle occupait auparavant. Sa croissance à deux chiffres, les 18,9 millions d'habitants de sa région urbaine, sa mutation cosmopolite et son essor culturel, l'appellent à devenir une métropole mondiale aux côtés de villes comme New York ou encore Tokyo. Elle a accueilli l'Exposition universelle de 2010.

La transcription « Shanghai » est souvent prononcée // ou // en français (on voit moins souvent les graphies Shangaï ou Shanghaï, Changaï et Chang-Hai), mais en chinois mandarin le nom se prononce "shàng hǎi" // - avec tonèmes : / /. En dialecte shanghaïen, le nom de la ville se prononce "zanhe" //.

Au temps de la concession française, le nom français de la ville s'écrivait « Changhaï » en cohérence avec la transcription de l'EFEO.

Les deux sinogrammes dans le nom « Shanghai » (, ; et , ) signifient littéralement « sur, au-dessus de » et « mer » . La première apparition de cette dénomination remonte à la dynastie Song (), époque à laquelle il existe déjà une confluence et une ville à cet endroit. Il y a des différends sur la façon dont ce nom doit être interprété, mais l'histoire locale officielle a toujours dit que cela signifie « le cours supérieur de la mer ». À cause du changement du littoral, les historiens chinois ont conclu que durant la dynastie Tang, Shanghai était littéralement sur la mer, d'où l'origine du nom. Une autre lecture, en particulier en mandarin standard, suggère également le sens de « aller sur la mer », qui est cohérent avec le statut de port de la ville. Un nom plus poétique pour Shanghai intervertit l'ordre des deux caractères, "" (), et il est souvent utilisé pour les termes liés à l'art et la culture de Shanghai.

Shanghai est communément abrégée en chinois par ' (). Ce caractère apparaît sur toutes les plaques d'immatriculation des véhicules provenant de la ville. Il est dérivé de ' (), le nom de l'ancien village de pêche qui se tenait pendant la dynastie Tang au confluent de la rivière Suzhou et de la rivière Huangpu. Le sinogramme Hu est souvent combiné avec le sinogramme ' () (de l'ancien nom de la rivière Suzhou) pour former le surnom ' (). Par exemple, l'attaque japonaise de Shanghai en août 1937 est communément appelée la Bataille de Songhu. Un autre ancien nom pour Shanghai était ' (), qui est maintenant le nom d'un luxueux hôtel de la ville. Un autre surnom commun est ' () qui vient du nom de " "(), un noble et héros local du royaume de Chu au cours du , dont le territoire incluait Shanghai. Les équipes sportives et les journaux utilisent souvent le sinogramme ' () dans leurs noms. Shanghai est également appelée ' (, « la cité de Shēn »). En Occident, Shanghai est également surnommée la « Perle de l'Orient » ou le « Paris de l'Orient ».

Shanghai est située sur la rivière Huangpu, et se compose donc de deux parties distinctes, Puxi et Pudong (qui signifient respectivement à l'ouest et à l'est du Pu). La ville s'est développée tout d'abord exclusivement à Puxi mais , sous l'impulsion du gouvernement, Pudong est devenu une zone de construction de hautes technologies où les entreprises et autres gratte-ciels se multiplient.

L'avenue Nanjing (cinq kilomètres) fut autrefois la grande artère de la concession dite étrangère. Elle est considérée maintenant comme le vrai centre de Shanghai et elle offre souvent dans sa partie est, près du fleuve, le spectacle d'une indescriptible cohue de piétons.

Le paysage urbain se transforme rapidement depuis quelques années. Des quartiers entiers, comme celui de Dun Hui Fang, sont rasés pour être reconstruits. Les expulsions concernent des dizaines de milliers de personnes depuis le milieu des années 1990 et un total de 20 millions de mètres carrés d'habitations ont été démolis. Les habitants reçoivent en échange de leur départ une compensation dérisoire et sont confrontés aux méthodes violentes et illégales des sociétés de démolition ou de la police.

Shanghai est situé dans un vaste delta, formé par l'embouchure du fleuve Yangzi Jiang qui se jette dans la mer de Chine orientale. Les basses terres qui se trouvent des deux côtés du fleuve sont composées de lœss d'alluvions, qui est formé par les sédiments du Yangzi. Construit de boue, sillonné de canaux et de barrages, le delta est l'une des zones les plus fertiles de Chine, et également son principal fournisseur de coton.

La formation de la terre est probablement due au remplissage d'une ancienne partie de la mer, et les nombreuses petites montagnes sur les îles de la région étaient à l'origine de vraies îles. La formation du delta a renvoyé Shanghai, une ville portuaire à l'origine construite sur la mer, à à l'intérieur des terres.

Shanghai bénéficie d'un climat subtropical humide. Les étés sont très chauds et humides et les hivers sont doux mais peuvent être parfois froids. En été, les températures peuvent facilement dépasser les avec un taux d'humidité très important qui donnera un ressenti indice de chaleur pouvant dépasser les ; de plus les températures baissent peu la nuit. De fortes averses très chaudes peuvent se produire, combinées à des températures élevées. Le record de chaleur est de le 6 et le 8 août 2013, néanmoins un record de fut enregistrée le 7 août 2013 à l'aéroport international hongqiao, et le record de froid est de le 19 janvier 1977.

La municipalité de Shanghai est un territoire administratif ayant le statut de province : elle comprend plusieurs villes dont Shanghai et compte environ 23 millions d'habitants dans son agglomération (en chinois : "chengqu") d'après le recensement de 2010. Shanghai comptait 16,7 millions d'habitants en 2000. 
La municipalité de Shanghai exerce sa juridiction sur dix-huit subdivisions : seize districts et un "xian".

Huit districts sont situés dans le quartier de Puxi, zone urbaine centrale de Shanghai à l'ouest du Huangpu :

Un district gouverne le quartier de Pudong, à l'est du Huangpu :

Les sept districts restants correspondent à des banlieues, à des villes satellites et à des zones rurales éloignées du centre urbain :

L'île de Chongming, située dans l'estuaire du Yangzi Jiang (Chang Jiang), est gouvernée par un seul "xian" :

Ces districts et "xian" sont eux-mêmes divisés (en 2003) en 220 subdivisions de niveau canton, comprenant 114 bourgs, 3 cantons et 103 sous-districts.

À l'origine port modeste et village de tisserands, Shanghai ne semblait pas promise à pareil essor cosmopolite.

Shanghai ne s'est pas toujours appelée Shanghai. Jusqu'à la dynastie Sui (581-618), c'était le village de Hua Ting(華亭鎮). Elle est devenue la préfecture de Huating avant de prendre son nom actuel sous la dynastie Song (960-1234).

Étant donnée sa situation stratégique à l'embouchure du Yangzi Jiang, au centre de la Chine, et la proximité avec des villes aux productions artisanales réputées (Suzhou, Hangzhou), Shanghai est devenue très tôt un centre d'échanges économiques importants.

Cependant, ce n'est qu'après les guerres de l'opium et la présence étrangère que le développement économique de la ville a pris l'envergure qui a fait sa réputation. 
Pendant la première guerre de l'opium, les forces britanniques ont temporairement tenu la ville. Américains et Français suivront, précédant les Russes et les Japonais. La guerre a cessé en 1842 avec le traité de Nankin, établissant l'ouverture commerciale des ports chinois, dont Shanghai. Les Britanniques vainqueurs y aménagent l'un des cinq ports ouverts qui leur seront alors concédés. Avec le traité du Bogue, en 1843, et le traité sino-américain de Wangxia, en 1844, des nations étrangères ont eu le droit de s'établir sur le territoire chinois : c'est le début des concessions étrangères.

La petite enclave française de Shanghai s'est établie sur une zone marécageuse en 1849. Elle fut à la fois un havre pour les réfugiés de toutes les nationalités et un lieu de culture et de plaisirs.

Avec la révolte des Taiping en 1850, Shanghai fut occupée par une triade associée au mouvement appelé Société des Petites Épées. La guerre faisant rage dans les campagnes, de nombreux Chinois se réfugièrent dans la ville, qui leur était théoriquement inaccessible : en 1854, de nouvelles lois permirent aux Chinois d'y acquérir des terrains, provoquant une inflation immobilière. Cette année-là eut également lieu la première réunion du conseil municipal de Shanghai, afin de gérer les concessions étrangères établies "de facto". En 1863, les concessions américaine et britannique se rejoignirent pour former la Concession internationale, alors que les Français établirent leur propre concession.

En mars 1854, l'empire chinois a signé un accord avec les Européens présents dans les concessions leur demandant de construire rapidement de nombreux logements, une grande partie de la ville ayant été détruite par une révolte. C'est ainsi que se construiront les lilongs, jusqu'en 1949.

Jeu, opium et prostitution sont alors les activités les plus lucratives de cette ville qualifiée alors de « plus grand bordel du monde ». Son parrain le plus connu, Du Yuesheng, menait ses trafics en collaborant étroitement avec la police de la concession française.

Après la guerre sino-japonaise de 1894-1895, le traité de Shimonoseki permit aux Japonais de s'ajouter aux forces occupantes. Ils établirent à Shanghai les premières usines de la ville.

Cette période d'occupation a profondément marqué l'identité culturelle de la ville, tout en contribuant dans les années 1920 et 1930 à l'essor des arts, cinéma, théâtre, et la naissance du premier groupe de jazz chinois. En 1920, on y recensait un million d'habitants, dont vingt six mille huit cents étrangers de nationalités diverses. Ils façonnèrent les rues à leur goût, mêlant les styles néogothique, classique, victorien, Art déco... La chanteuse et actrice Zhou Xuan, fille de Weiwei Wang, était sans doute la figure la plus emblématique de cette période. C'est aussi à Shanghai que fut créé le Parti communiste chinois en 1921 et qu'ont été organisées les premières grèves ouvrières. La plupart, coolies et ouvriers, demeurèrent dans la pauvreté et vinrent grossir les rangs du Parti communiste chinois. En 1927, dans le cadre de l'expédition du Nord de pacification de la Chine, les ouvriers chinois, mobilisés par les communistes, prirent Shanghai aux seigneurs de la guerre avant même l'arrivée des troupes gouvernementales. Tchang Kaï-chek, inquiet de la mobilisation réussie par les communistes, décida de se retourner contre ses alliés et lança les triades contre les ouvriers, déclenchant le massacre de Shanghai, qui signa le début de la guerre civile chinoise.

Sous le régime de la République de Chine, Shanghai devint une ville spéciale en 1927, et une municipalité en mai 1930. Elle fut alors le centre financier de l'Asie, où les dollars mexicains par exemple s'échangeaient en masse après la crise boursière de 1929. La marine japonaise bombarda la ville le , officiellement pour réprimer les protestations étudiantes ayant suivi l'incident de Mandchourie, déclenchant la . 

À compter du mois d', à l'aube de la seconde guerre sino-japonaise, Shanghai fut soumise par la marine et l'armée nippones à une série de bombardements qui entraînèrent la mort et l'évacuation de plusieurs milliers de civils. Disposant de forces terrestres et navales bien supérieures à l'armée chinoise, les troupes impériales prirent possession de la ville en novembre (bataille de Shanghai), puis se dirigèrent vers Nankin où elles se livrèrent à un terrible carnage (massacre de Nankin).

Selon les travaux de l'historien Zhiliang Su, au moins 149 « maisons de confort » hébergeant des esclaves sexuelles furent établies à Shanghai pendant l'occupation nipponne.

En 1938, Shanghai fut considérée comme le cinquième port mondial; les plus grandes firmes occidentales y étaient désormais représentées.
Durant la Seconde Guerre mondiale, Shanghai devint temporairement un centre pour les réfugiés d'Europe : c'était alors la seule ville ouverte inconditionnellement aux Juifs. En 1941, sous pression de leurs alliés nazis, les Japonais reçurent les réfugiés juifs dans un ghetto, où les maladies pullulaient. L'immigration juive fut finalement stoppée par les Japonais le .

Les Japonais prirent le contrôle total des concessions le . Durant l'occupation japonaise, les citoyens des pays Alliés travaillant pour l'administration municipale demeurèrent à leur poste jusqu'en , date à laquelle ils furent internés.

Entre 1942 et 1945, sous l'effet combiné de la corruption du Gouvernement de Nankin et de l'occupation japonaise, le nombre de banques atteint 300, soit le double de celui de 1936.

Durant la guerre, le conseil municipal des concessions étrangères fut aboli deux fois, à quelques mois d'intervalle, par deux gouvernements ennemis. En , le gouvernement du Royaume-Uni signa avec la République de Chine un traité acceptant le principe d'une rétrocession. En juillet de la même année, les Japonais rétrocédèrent le conseil municipal au gouvernement collaborateur de Wang Jingwei. Après la guerre, une commission de liquidation fut mise en place pour gérer la rétrocession à la République de Chine.

Les huit années d'occupation, puis la victoire, en 1949, de Mao Zedong sur les troupes du général Tchang Kaï-chek précipitèrent le déclin de la ville.

Après la victoire des communistes, la ville a été considérée comme le symbole du capitalisme étranger, elle sommeillait, et le monde l'avait presque oubliée, avant d'être revalorisée à la suite du mouvement de réformes de Deng Xiaoping.

Autrefois tête de pont des puissances coloniales dans une Chine agonisante, Shanghai est devenue le premier centre industriel du pays, en même temps que l'une des plus grandes métropoles du monde.

Pendant la Révolution culturelle, Shanghai connut des troubles politiques et sociaux : à la fin , la municipalité fut renversée. Les plus importantes grèves de l'histoire de la ville paralysèrent la vie économique. Les rebelles et les gardes rouges désiraient mettre en place un système semblable à la Commune de Paris. Le bilan de la Révolution culturelle fut considérable : logements furent confisqués rien qu'à Shanghai. Entre 1968 et 1976, un million de Shanghaiens furent ruralisés de force.

Au début des années 1990, en une décennie, la « Perle de l'Orient » est redevenue un centre économique de première importance, qui compte en 2005 pour 20 % de la production industrielle nationale pour seulement 1,5 % de la population. Elle se destine aujourd'hui à devenir le centre financier de la Chine, grâce au quartier de Lujiazui.

Le , Chen Liangyu (46 ans) a été élu maire de Shanghai par les délégués de la cinquième session du Congrès du peuple de la Municipalité de la grande métropole de l'Est de la Chine. Il est ensuite devenu Secrétaire du Parti de la Municipalité autonome en octobre 2002, à la place de Huang Ju. Ce poste particulièrement important va habituellement de pair avec un siège au Bureau politique du Parti. C’est le cas pour Chen Liangyu depuis le Congrès du Parti communiste chinois. En , Chen Liangyu est limogé à la suite d'un scandale de corruption.

Avant cela, le , la métropole chinoise a été désignée pour organiser l'Exposition universelle de 2010, qui se tient donc, pour la première fois depuis 151 ans, dans un pays en voie de développement. Depuis l'Exposition universelle de 2010, rien ne semble arrêter le développement de Shanghai. Parmi les grandes métamorphoses, le district de Pudong dont la superficie devrait doubler dans les cinq prochaines années, passant de à km². Avec la création d'un jardin digne de Central Park et un opéra prévu pour 2015, ce quartier d'affaires veut aussi devenir le poumon vert de la ville et un temple de la culture. Sur les dix nouvelles lignes de métro qui desserviront Shanghai en 2012, cinq passeront par ce district. Le réseau, au total, se hissera alors parmi les trois plus longs du monde.

En 2013 y survient l'apparition de l'influenzavirus A sous-type H7N9.

Le maire actuel de Shanghai s'appelle Yang Xiong.

La clique de Shanghai est le nom donné à un groupe d’officiels du Parti communiste chinois qui ont dû leur promotion à leur appartenance à l’administration municipale de Shanghai sous l’égide de l’ancien maire de Shanghai et président Jiang Zemin.

La population de la municipalité de Shanghai est de habitants. D'après la population totale de la municipalité, Shanghai est la seconde plus grande municipalité de la République populaire de Chine, après Chongqing et devant Pékin. En RPC, une municipalité ( en pinyin: ) est une ville avec un statut équivalent aux provinces chinoises. Le recensement de 2000 positionnait la population de Shanghai à 16,738 millions, dont 3,871 millions de migrants. Par rapport au recensement de 1990, la population totale avait augmenté de 3,396 millions d'individus, soit une croissance de 25,5 %. Les hommes représentent 51,4 % et les femmes 48,6 % de la population. 12,2 % des Shanghaïens sont âgés de 0 à 14 ans, 76,3 % entre 15 et 64 et 11,5 % ont plus de 65 ans.

Selon le Bureau municipal des statistiques de Shanghai, la ville comptait étrangers en 2007. On compte également un nombre important d'habitants de Taïwan qui résident à Shanghai pour les affaires (entre environ et ). En 2009, les communautés sud-coréennes à Shanghai atteignaient ressortissants.

En 2006, l'espérance de vie était de 80,97 ans (78,67 pour les hommes et 82,29 pour les femmes). Le revenu moyen annuel des résidents de Shanghai, basé sur les trois premiers trimestres de 2009, est de .

La ville a longtemps été l'un des principaux centres de production textile de la République populaire de Chine. Les autres secteurs manufacturiers importants comprennent la fabrication de produits chimiques et pharmaceutiques, les véhicules (notamment des navires), les machines, l'acier, le papier et les produits d'impression. En outre, la ville produit à grande échelle des systèmes électriques et électroniques ainsi que des équipements tels que les ordinateurs, les radios et les appareils photo.

Avec le début de réformes économiques chinoises au début des années 1980, Shanghai a d'abord été dépassée par certaines provinces du sud, telles que Guangdong. Avec le début des années 1990, grâce à l'action du gouvernement par l'intermédiaire de Jiang Zemin, les investissements ont fortement augmenté à Shanghai, dans le but d'établir un nouveau centre économique en Asie orientale.

Hong Kong constitue le principal rival de Shanghai dans le titre honorifique de plus grand centre économique en Chine. Hong Kong possède l'avantage d'une plus grande expérience, notamment dans le secteur bancaire. Shanghai a des liens plus étroits avec l'arrière-pays chinois et le gouvernement central de Pékin. De plus, Shanghai possède plus de terrains pour accueillir les nouveaux investissements, alors qu'à Hong Kong, l'espace est très limité.

Fondée en 1990 à Shanghai, la Bourse de Shanghai représente aujourd'hui la bourse la plus importante en Chine continentale. Depuis 1991, la croissance économique à Shanghai est à deux chiffres. La ville est donc la seule région de Chine dans ce cas sur une telle durée. La croissance économique annuelle à Shanghai est actuellement d'environ 12 %. Le PIB pour 2006 s'élève à 1,03 billions de yuans (environ 128,8 milliards de dollars). Le PIB par habitant était d'environ (la moyenne chinoise se situe à ) et constitue le troisième plus élevé du pays, derrière Hong Kong et Macao. En 2010, le PIB par habitant est prévu à .

En 1984, à Anhui, une coentreprise avec le constructeur automobile Volkswagen constitue la première usine automobile construite avec une marque occidentale. Volkswagen Shanghai représente une part de marché d'environ 60 % sur les véhicules étrangers en Chine, ce qui est en baisse constante en raison d'une concurrence accrue. Les droits d'importation élevés sur les voitures étrangère les rendent encore plus chères. Ainsi, après l'adhésion à l'OMC de la République populaire de Chine, la conférence de l'APEC en 2001 a réduit progressivement les droits à l'importation.

Shanghai traduit l'envol économique de la Chine. Un dollar sur vingt du PIB chinois provient de cette ville et 1/5 des exportations du pays - qui ont augmenté de 500 % en valeur réelle entre 1992 et 2008 - transite par sa zone portuaire.

Le , le nouveau maire de Shanghai, Chen Liangyu a déclaré qu'il voulait « faire de sa ville, dans les trois années à venir, le centre du marché financier intérieur, des circulations des capitaux et de gestion de fonds, et l'un des centres financiers internationaux les plus importants pour une durée de dix à vingt ans. »

Cela dépend directement de la réforme du système financier chinois, encore très archaïque, mené par les autorités centrales de Pékin.

Actuellement, on compte plus de étrangers dans la métropole chinoise, en plus d'environ Taiwanais. Ces travailleurs étrangers sont principalement originaires du Japon, des États-Unis, de la Corée du Sud, de Singapour, de l'Allemagne, de la France et du Canada. La plupart d'entre eux travaillent dans des sociétés à capital permanent à l'étranger ou dans le cadre de missions à l'étranger.

Une maquette géante de la ville est visible au musée de l'urbanisme sur la Place du Peuple. Elle donne une idée de la valeur de prestige accordée au développement immobilier à Shanghai.

Il y aurait actuellement , dont de plus chaque année, et permanents. Le "", en est l'exemple le plus éclatant, avec ses de hauteur, il était le plus haut bâtiment de Chine, détrôné par la Shanghai Tower, ouverture prévue en 2015. En 2006, un appartement de la Tomson Riviera, située à Pudong, s'est vendu pour 190 millions de yuans, soit environ 19 millions d'euros.

Toutefois, il convient d'ajouter un bémol face à cet engouement spéculatif. Le taux d'occupation des bureaux est très bas dans la ville. Certains analystes redoutent une bulle immobilière comparable à la bulle japonaise des années 1980. En Chine, l'immobilier est une des activités les plus opaques, ce qui explique la fragilité du secteur qui pourrait éclater si la croissance économique montre des signes de ralentissement. Enfin, la multiplication des gratte-ciel fait peser un danger sur le sol de la ville. Les spécialistes constatent que depuis 1921, le sol de la métropole s'affaisse à une vitesse estimée à par an. Un tiers des affaissements des constructions dans le centre-ville est dû à ces grandes tours, d'après le Bureau de la planification de la ville de Shanghai.

Shanghai est également un centre important de raffineries de pétrole. La plus grande aciérie de Chine, et l'une des plus modernes, se situe à Baoshan, en bord de mer. La ville est donc sujette à une pollution importante sous la forme de nuages de fumée de soufre que les usines émettent en permanence. Environ quatre millions de tonnes d'eaux usées industrielles et domestiques non filtrée sont versées quotidiennement dans la rivière Huangpu, la principale source d'eau potable de la ville, et dans le canal de Suzhou dont les eaux sont fréquemment noires et nauséabondes. Un autre problème est le chômage, qui est supérieur à Shanghai par rapport à d'autres grandes villes du pays.

L"'Université Fudan" est l'une des universités de premier plan en République populaire de Chine. Elle a été fondée par le jésuite Joseph Ma Xiangbo en 1905 sous le nom de "Collège catholique Fudan". Ma Xiangbo lui donne ce nom d'après une citation des classiques confucéens. En 1917, elle est transformée en université privée. Au début de la guerre anti-japonaise en 1937, l'université est transférée à Chongqing, à l'intérieur de la Chine. Elle prend son nom actuel en 1946 quand elle revient à Shanghai. Elle fusionne avec l'université l'Aurore en 1952, après le départ des jésuites.

L"'École normale supérieure de l'Est de la Chine", ou plus simplement ECNU, est l'une des plus prestigieuses universités en Chine. Fondée en 1951 à Shanghai, elle fut la première école normale supérieure de la République populaire de Chine. Le premier établissement sino-américain d'enseignement supérieur - Université de New York à Shanghai (NYU Shanghai) – a été cocréé par l'Université de New York et l'ECNU.

L"'Université Tongji" est l'une des plus célèbres universités chinoises de Shanghai. Elle a été fondée en 1907 à l'initiative du Consul Général allemand Wilhelm Knappe comme une école allemande médicale et dirigée par le médecin Erich Paulun. En 1923, elle devient une université et en 1937 elle est déménagée à cause de la guerre, d'abord dans la province de Zhejiang. Lorsque le front approche, elle déménage vers la province de Jiangxi, puis Yunnan, et plus tard même pour le Sichuan. Après la fin de la Seconde Guerre mondiale, elle revient de nouveau à Shanghai, en 1946.

L"'Université des études internationales de Shanghai" est une institution importante dans le pays. Elle est issue de l'Institut des langues étrangères de Shanghai, fondé en 1949. Depuis 1983, l'Université entretient une coopération active avec l'Université de Heidelberg. Depuis 2002 il existe un programme allemand des affaires, qui a été conçu conjointement avec l'Université de Bayreuth.

Voici une liste des autres principaux institut et universités présentes à Shanghai :

La langue officielle de Shanghai, comme dans l'ensemble de la Chine est le mandarin. Cependant, la langue parlée est, dans le delta du Yangzi Jiang (长江) et les régions environnantes, le wu. La variété parlée à Shanghai est le shanghaïen. Les campagnes de promotion du mandarin et la scolarité effectuée exclusivement en mandarin conduisent à un recul progressif de l'usage du dialecte. Celui-ci reste cependant largement utilisé dans la communication informelle. Il est à noter dans le domaine de la communication informelle le basculement du shanghaïen vers le mandarin chez la jeune génération de Shanghaïens, qui ne maîtrise guère plus la langue locale, ou de façon erratique.

Dans le centre de Shanghai, près de l'hôtel de ville et de la rue de Nankin se trouvent le Musée de Shanghai, l'opéra de Shanghai et le Centre d'exposition de la planification urbaine de Shanghai.

Au cœur de la vieille ville, le jardin Yuyuan (ou jardin du madarin Yu) est le plus beau jardin chinois traditionnel de Shanghai.

Le long de la rivière Huangpu se trouve le Bund d'où l'on peut voir le quartier d'affaires de Pudong et ses gratte-ciels dont les plus hauts sont la Perle de l'Orient, la " et le ". La Tour Shanghai, qui a ouvert au public en 2015, est le plus haut gratte-ciel de Chine.
En ce qui concerne les religions asiatiques, on trouve trois principaux temples : le temple de Jing'an, le temple du Bouddha de jade et le temple du dieu de la ville, ce dernier se situant près du jardin Yuyuan.
Plusieurs églises catholiques sont situées à l'intérieur de la ville, comme la cathédrale Saint-Ignace, l'église Saint-Joseph, l'église Saint-François-Xavier, l'église orthodoxe Saint-Nicolas, l'église Sainte-Thérèse-de-l'Enfant-Jésus et l'église Saint-Pierre, mais également assez loin en dehors de l'agglomération, avec la basilique de She Shan, lieu de pèlerinage marial fort fréquenté. Shanghai est la ville où l'on voit le plus d'églises catholiques. L'église Notre-Dame-de-Lourdes de Pudong est l'une des cent dix églises catholiques de l'agglomération de Shanghai. Construite en 1896-1899 dans l'est de la ville par les jésuites français, elle a été rénovée en 2010. L'église de l'Immaculée-Conception de Zhang Pu se trouve également en dehors du centre de la ville.
Shanghai compte plusieurs mosquées, parmi lesquelles celle de Songjiang, la plus ancienne, celle du jardin au pêcher (小桃园清真寺), la plus grande, celle de Huxi (沪西清真寺), celle de Pudong (浦东清真寺), ou celle de Fuzhou Road (福佑路清真寺).

Enfin, certains ponts sont remarquables, comme le pont de Nanpu et le pont de Yangpu, qui se situent parmi les plus longs du monde avec respectivement plus de et plus de de portée. Le pont de Lupu, quant à lui, est le deuxième plus long pont en arc du monde, avec de portée.

La cuisine de Shanghai est en en partie tournée vers les crustacés, coquillages et les poissons, de mer ou d'eau douce, du fait de sa position géographique. Ainsi, le crabe poilu de Shanghai ("shàng hǎi máo xiè", 上海毛蟹) est une célèbre spécialité délicate, prisée pour les qualités aphrodisiaques des ovaires du crabe femelle.

Cette cuisine se caractérise par l'utilisation du vin de cuisine qui sert à mariner les poissons ou le poulet. Une fois saoulée, la viande est cuite rapidement ou servie crue. Une autre particularité de la cuisine dans cette région est l'utilisation à quantité presque égale du sucre et de la sauce soja. Bien qu'abondamment utilisé, le sucre ne donne pas de goût particulièrement sucré aux plats, mais sert à rehausser le goût, comme dans les « travers de porc en sauce aigre-douce » 'táng cù páigǔ", 糖醋排骨).

La cuisine de Shanghai est également réputée pour la cuisson « braisée en rouge » ("hóng shāo", 紅燒), qui consiste à faire cuire à feu doux viandes et légumes. L'utilisation de sauce soja ou de sucre permet alors d'obtenir la fameuse couleur rouge.
Les habitants de la ville de Shanghai sont réputés pour manger de petites portions. Par exemple, les bouchées à la vapeur () sont beaucoup plus petites que leurs cousines baozi (包子) que l'on trouve ailleurs en Chine.

Voici une liste de spécialités de la cuisine de Shanghai :


Et aussi : musée national de Shanghai, Musée de Shanghai, musée des Beaux-Arts de Shanghai, centre de sculpture de Shanghai, musée Lu Xun de Shanghai (dans le parc Lu Xun), le Mémorial du siège du Congrès du Parti communiste chinois de Shanghai.

L'imprimerie, introduite par les missionnaires protestants, avait fait de Shanghai un centre majeur de l'édition. Un lectorat nombreux dû à un taux d'alphabétisation élevé favorisait le développement de la littérature populaire. La langue de wu est ainsi introduite dans les dialogues des romans, et le roman "Haishang hua liezhuan" ("Fleurs de Shanghai", 1894) de est même entièrement écrit dans cette langue. Le « roman de courtisanes » de son côté est souvent lié à la ville de Shanghai, à l'instar du "Haishang fanhua meng" ("Rêves de splendeur shanghaienne", 1898-1906) de Sun Yusheng. Dans ce genre de roman, le romantisme habituel des histoires d'amour se mêle parfois au réalisme de la vie urbaine, mâtiné d'un exotisme occidental issu des concessions.

À la fin de l'Empire apparaît à Shanghai un courant littéraire appelé école des canards mandarins et des papillons, produisant une littérature populaire à thématique amoureuse. Les romans de cette école, dont l'appellation est à l'origine péjorative, racontent dans la traditions des romans populaires antérieurs les aventures compliquées de couples d'amoureux (symbolisés par les canards mandrins et les papillons), généralement un jeune homme au talent méconnu et une jeune fille à la beauté éthérée. Après la suppression des examens impériaux en 1905, cette littérature était produite par des lettrés cherchant à vivre de leur plume. Si les intellectuels méprisaient ce genre, la petite bourgeoisie en était friande. "Le Fantôme de la poire de jade" (1911) de est un exemple type de ce genre de romans, qui a ses prolongements au cinéma et qui perdure jusque dans les années 1930. est quant à lui le premier écrivain spécialisé dans le genre policier, avec son héros Huosang, imitation de Sherlock Holmes.

Après le mouvement du 4 mai 1919, la « Nouvelle Littérature », dont Pékin est le fer de lance depuis 1915, a des répercussions à Shanghai. Elle est ainsi le siège de la Société Création, fondée au Japon en 1921 par Guo Moruo et Yu Dafu, qui mettent en avant un individualisme romantique et rebelle, influencé par la poésie occidentale. Mais c'est avant tout avec l'école néosensationniste que l'esprit de Shanghai (le "") trouve son expression dans la littérature. Liu Na'ou, fondateur du groupe, Mu Shiying et Shi Zhecun en sont les principaux représentants. Écrivains bohèmes, les néosensationnistes fréquentent cafés, dancings et cinémas et trouvent leur inspiration dans la modernité urbaine. Ils innovent dans le domaine des techniques et procédés littéraires, s'inspirant du cinéma et d'exemples venus du Japon ou d'Occident. Les recherches formelles et la volonté de se tenir à l'écart des problèmes politiques et sociaux des modernistes suscitent l'hostilité des écrivains engagés, généralement à gauche.

La politique répressive des seigneurs de la guerre avait conduit durant les années 1920 nombre d'écrivains à quitter Pékin pour se réfugier à Shanghai, comme Lu Xun en 1927. La ville était ainsi devenue un haut lieu de la littérature engagée, comme l'illustre la conversion au marxisme du groupe Création. Après le massacre des communistes par le Guomindang en avril 1927, les intellectuels de gauche tentent de s'organiser. La Ligue des écrivains de gauche est ainsi créée en 1930, sous l'égide de Lu Xun. Regroupant essentiellement des militants, la Ligue, tout comme d'autres organisations similaires, s'attache à promouvoir une littérature prolétarienne et révolutionnaire. L'exemple le plus achevé de cette tendance est le roman "Minuit" (1933) de Mao Dun, dans lequel est racontée la lutte entre capitalistes nationaux et compradores, et la défaite des premiers. La modernité de Shanghai s'y montre sous un aspect négatif. L'épisode le plus fameux de la brutalité de la répression du Guomindang envers les écrivains engagés est celui de l'exécution de cinq écrivains communistes, les cinq martyrs de la Ligue des écrivains de gauche, en 1931.

Avec le déclenchement de la guerre sino-japonaise (1937), les écrivains sont nombreux à quitter Shanghai. Parmi ceux qui restent, certains se compromettent avec le régime collaborateur de Wang Jingwei, comme Liou Na'ou et Mu Shiying, tous deux assassinés en 1939-1940 dans des circonstances mal élucidées. Des reproches de collaboration sont aussi adressés après la guerre à Zhang Ailing (alias Eileen Chang) et Su Qing, les deux écrivains les plus représentatifs de cette période. Le nom d'Eileen Chang est étroitement associé à la ville de Shanghai, où elle est née. Cependant Shanghai n'est que peu présente dans son œuvre, la ville n'y apparaît que dans quelques nouvelles de nature intimiste. Dans ses écrits, Su Qing aborde sans fard la vie quotidienne et conjugale d'un point de vue féminin. Toutes deux attestent la place nouvelle que les femmes ont acquise dans la vie littéraire.

Après 1949, beaucoup d'écrivains ont la prudence de délaisser la création littéraire pour lui préférer la recherche ou la traduction, activités politiquement moins compromettantes. La vie shanghaïenne est toutefois un sujet abordé par Zhou Erfu, l'auteur d'un roman-fleuve en quatre volumes, "Shanghai de zaochen" ("Le Matin de Shanghai"), qui suit les traces du "Minuit" de Mao Dun : le roman montre comment les capitalistes de Shanghai cherchent à s'accommoder du régime communiste. Le sujet vaut au roman d'être condamné pendant la Révolution culturelle (les deux derniers volumes ne paraissent qu'après 1979). Durant la Révolution culturelle Shanghai est d'ailleurs le quartier général des gauchistes les plus radicaux, autour de Jiang Qing, l'épouse de Mao : c'est le « groupe de Shanghai », plus tard appelé Bande des quatre. Une critique de Yao Wenyuan, l'un des « Quatre », contre la pièce "La Destitution de Hai Rui" de Wu Han, parue dans un journal de Shanghai en 1965, avait servi de prélude au déclenchement de la Révolution culturelle. Les intellectuels, ici comme ailleurs, ont alors leur part de persécutions et d'exils, voire de suicides, comme celui de Fu Lei, célèbre traducteur. Ba Jin, qui vit à Shanghai, a laissé des souvenirs de cette période dans ses mémoires.

La légende du Shanghai d'avant-guerre, occultée en Chine même après 1949, se perpétue dans le recueil de l'écrivain Bai Xianyong, exilé à Taiwan. Dans son recueil de nouvelles "Gens de Taipei" (1971), les personnages se souviennent du Shanghai magnifié de leur vie avant l'exil. La ville y est aussi présentée comme la capitale d'un monde déchu. Avec les années 1990, deux écrivaines shanghaiennes, Chen Danyan et Cheng Naishan explorent à nouveau la passé de la ville pour le mettre en miroir avec le présent. Ce retour au passé est aussi l'occasion d'un exotisme facile, fait de sexe et de violence, dont les romans "Mengui" ("Shanghai Triad") de Li Xiao et "Shanghai wangshi" ("Les Triades de Shanghai", inspiré du précédent) de Bi Feiyu sont des exemples. Le cinéma y trouve son compte, avec le film de Zhang Yimou "Shanghai Triad", adapté du roman de Li Xiao, ou celui de Hou Hsiao-hsien, "Les Fleurs de Shanghai", adaptation du roman de Han Bangqing. En revanche, c'est avec un personnage ordinaire, dans une histoire romantique et mélancolique, que Wang Anyi trace un portrait de la ville entre 1945 et 1985 : son roman "Le Chant des regrets éternels" (1995) est considéré comme l'un des meilleurs romans jamais écrits sur Shanghai. Plus récemment se sont fait connaître les « belles écrivaines », Wei Hui et Mian Mian. Wei Hui est l'auteur du roman autobiographique "Shanghai Baby", où le cosmopolitisme traditionnel de Shanghai se mêle au narcissisme de l'héroïne. Ce même cosmopolitisme se retrouve dans "Les Bonbons chinois" de Mian Mian, roman explorant les milieux marginaux de la ville.

"La Condition humaine" d'André Malraux se déroule dans le décor de la ville. Soutenus par les étrangers des concessions, le parti nationaliste du Guomindang de Tchang Kaï chek s'apprête à écraser les communistes chinois dans la ville. Tableau du conflit interne chinois historique, et réflexions sur la guerre.

L'album de Tintin "Le Lotus bleu" dépeint notamment la lutte des chinois pour leur indépendance, avec les enjeux du marché de l'opium en toile de fond.

Stéphane Fière dépeint, dans "La Promesse de Shanghai", le destin d'un paysan contemporain arrivant à Shanghai pour y devenir manœuvre.

Le journaliste Albert Londres écrit "La guerre à Shanghai", avant un dernier reportage, fatal, sur les mafias de la ville.

Noël Coward rédige en 1930 "Private Lives".

"L'Empire du soleil", de J. G. Ballard relate la prise de Shanghai par les troupes japonaises, au lendemain de Pearl Harbor. Un enfant de la ville passe trois années dans un camp de détention.

Un détective enquête sur une mystérieuse disparition dans la concession française de Shanghai, sous la plume de Kasuo Ishiguro, dans "Quand nous étions orphelins".

Shanghai est le lieu de naissance de l'industrie cinématographique du cinéma chinois.

Shanghai, ville de cinéma, a inspiré les cinéastes. 
Quelques acteurs/actrices shanghainais connus en Chine :
Quelques autres films où le décor (réel) de Shanghai à diverses époques joue un rôle majeur :

En revanche, non seulement "La Dame de Shanghai", d'Orson Welles (1946), ne se déroule pas à Shanghai, mais le rapport du film avec la ville est on ne peut plus lointain.

Shanghai possède d'importantes infrastructures sportives. Le stade de Shanghai peut ainsi accueillir et constitue le troisième plus grand stade en Chine. Il a été utilisé au cours des jeux olympiques d'été de 2008 pour accueillir plusieurs matchs du tournoi de football. Le stade de Hongkou compte quant à lui .

En 2005, la SMP subventionne un vaste chantier pour construire le plus grand Skate Park au monde: le SMP Park (ou SMP Skate Park). Ce dernier, d'une superficie totale de regroupe 4 zones de glisse dont une zone de compétition vaste de visant à accueillir des compétitions internationales.

La ville organise également chaque année les Masters de Shanghai, une compétition de tennis masculin, qui fait partie des Masters 1000 de l'ATP World Tour depuis 2009, au même titre que les Masters de Madrid, Masters de Monte-Carlo ou encore Masters de Paris-Bercy. Chaque année, les meilleurs joueurs de tennis mondiaux se retrouvent donc en octobre pour s'affronter dans la salle du Qizhong Forest Sports City Arena.

Depuis 2010, Shanghai accueille également la seconde étape de la Ligue de diamant avec le meeting Shanghai Golden Grand Prix. Cette ligue réuni les meilleurs athlètes du monde qui, au cours de 14 meeting dans le monde, s'affrontent pour engranger le plus de points possibles et gagner en fin de saison un diamant de d'une valeur d'environ .

La ville possède également plusieurs clubs sportifs professionnels qui évoluent dans les principales compétitions sportives du pays :

Le métro de Shanghai comprend en 2017 quatorze lignes (lignes 1 à 13 et ligne 16) en fonctionnement mises en service à l'occasion de l’Exposition universelle de 2010 qui s'est tenue dans la ville.

Shanghai s'enorgueillit de son métro flambant neuf mais aussi de ses de routes à grande vitesse.

Deux lignes de chemin de fer se croisent à Shanghai, Pékin-Shanghai (京沪) et Shanghai-Hangzhou (沪杭), sur lesquelles sont établies plusieurs gares : gare de Shanghai, gare de Shanghai-Hongqiao, gare de Shanghai-Ouest et gare de Shanghai-Sud.

Depuis le 2004, une ligne de Transrapid, un train à sustentation magnétique, relie la ville au nouvel aéroport international de Pudong. Ce train est ainsi la ligne commerciale la plus rapide au monde ().

La compagnie aérienne chinoise est basée sur cet aéroport : China Eastern Airlines.

L'aéroport international de Shanghai-Hongqiao qui se trouve à Puxi (l'Ouest du Pu), dans le quartier de Hongqiao, autrefois principal aéroport, aujourd'hui majoritairement consacré aux vols intérieurs et aux vols internationaux avec les villes voisines : Séoul, Tokyo, Macau, Taiwan et Hong Kong. Il est relié à l'aéroport international de Pudong par la du métro qui le dessert depuis 2010.

Le a été inauguré le plus long pont du monde, le pont de Donghai, qui relie la ville au nouveau port en eau profonde sur les îles Yangshan.

Un moyen de transport très répandu à Shanghai est le taxi. En effet, la ville est remplie de milliers de taxis et les chauffeurs sont adeptes d'un modèle en particulier, la Volkswagen Santana. Elle est robuste et très facile d'entretien. Cependant, la production de ce modèle va s'arrêter en 2012 et les Santana Taxi de Shanghai seront peu à peu remplacées par un autre modèle, la Volkswagen Lavida.

En 2010 a été inauguré le pôle Hongqiao (le premier pôle multimodal de transports au monde), comprenant le de l'aéroport international de Shanghai Hongqiao et la gare de Honqiao, où se croiseront les lignes de TGV CR Shanghai-Pékin, Shanghai-Hangzhou, Shanghai-Nankin et Shanghai-Chengdu (en construction). Cette nouvelle gare de Hongqiao est une des principales gares desservant la métropole de Shanghai, avec la gare de Shanghai, la gare sud de Shanghai et la gare ouest de Shanghai.

Depuis le 31 décembre 2009, Shanghai possède également un tramway.

La route nationale chinoise 318 (ou G318), d'une longueur de , relie la ville à la frontière népalaise.

Après avoir dépassé le port de Rotterdam en 2003, celui de Hong Kong en 2004, et celui de Singapour en 2005, Shanghai est devenu le port le plus actif du monde, aussi bien en termes de tonnage total traité qu'en nombre de conteneurs. Le port est très engorgé, malgré l'ouverture de l'avant-port de Yangshan, avec une croissance annuelle de son trafic de 30 % : en 2008, Shanghai enregistrait un trafic de 508 millions de tonnes, contre 650 millions de tonnes en 2010. La croissance du trafic conteneurisé a été plus faible avec 28 millions d'EVP (Équivalent Vingt Pieds) en 2008 contre 29 millions en 2010.

Une bonne partie du trafic s'effectue avec l'intérieur du pays, par les kilomètres navigables du Yangzi Jiang : les bateaux peuvent aller de Shanghai jusqu'à Chongqing.

Ne pouvant plus s'étendre, en 2000/2001, il fut décidé de construire un nouveau port en eau profonde sur les îles Yangshan au large de Shanghai. Ce nouveau port devant être relié au quartier de Guoyuan par un pont gigantesque — le pont de Donghai — le plus long du monde ondulant en pleine mer sur pas moins de avant d'atteindre son objectif, afin de suivre les hauts-fonds capables de soutenir les fondations.

S'il s'agit d'un pont, pas moins de 470 piliers, et 15 portuaires, ont été posés dont certains à cent mètres de profondeur, d'un coût de 14 à 15 milliards d'euros.

Selon le quotidien financier The Financial Times, daté du 3 juillet 2006, l'opérateur public du port de Shanghai, le Shanghai International Ports Group, voudrait à présent s'étendre à l'étranger, via des acquisitions en Europe, en Asie et aux États-Unis. L'un des responsables de son conseil d'administration a cependant reconnu que les projets pourraient se heurter à des oppositions politiques.







</doc>
<doc id="17799" url="https://fr.wikipedia.org/wiki?curid=17799" title="Portage informatique">
Portage informatique

Le portage informatique consiste à "porter", c'est-à-dire mettre en œuvre, un logiciel, une fonctionnalité, voire un système d'exploitation dans un autre environnement que celui d'origine. Cet environnement est donc soit logiciel, soit matériel.

La portabilité d'un code source est sa qualité d'être aisément porté.

Le portage informatique revient souvent à reprendre le code source du composant existant dans son environnement initial, puis à lui apporter les modifications nécessaires pour qu'il puisse fonctionner sur la plate-forme de destination. Dans ce type de cas, le développeur sera reconnaissant à ceux ayant conçu ledit composant d'avoir utilisé des pratiques visant à la portabilité, par exemple en évitant toute violation de la norme du langage de mise en œuvre.

On peut porter un noyau de système d'exploitation sur une autre architecture matérielle, comme c'est le cas avec les systèmes de type Unix, et parfois on portera des utilitaires, comme ce fut le cas pour le projet GNU.

Des bibliothèques logicielles sont aussi souvent portées pour être disponibles dans de nouveaux environnements. C'est par exemple le cas de la bibliothèque Qt ou encore de la bibliothèque de langage de script Python qu'on trouve, par exemple, sous la forme d'une bibliothèque chargée dynamiquement dans l'environnement Windows.

Dans le domaine du jeu vidéo, on parle de portage lorsqu'un jeu est adapté d'un système à un autre.



</doc>
<doc id="17801" url="https://fr.wikipedia.org/wiki?curid=17801" title="Portage">
Portage

Le portage est l'action de porter quelque chose. L'action est effectuée par un porteur ou portageur dans certains contextes.





De nombreuses localités d'Amérique du Nord portent le nom de Portage, généralement en référence au portage (transport fluvial).



</doc>
<doc id="17804" url="https://fr.wikipedia.org/wiki?curid=17804" title="District">
District

Un district est une division administrative plus ou moins importante dans certains pays. Les formes de gouvernance sont variables, depuis une simple subdivision sans autonomie jusqu'à un territoire autonome avec une représentation élue. De même, la superficie va de celle d'un quartier jusqu'à celle d'une région.

Dans certains pays, le "district fédéral" est un territoire qui abrite la capitale fédérale. Géré directement par le gouvernement fédéral, il n'a pas le statut d'État fédéré. Voir :

Le "district" (en allemand "Regierungsbezirk") est une division territoriale de certains länder allemands. Le district est lui-même subdivisé en "arrondissements" ou "cercles" (en allemand "Kreis" ou "Landkreis").

En Belgique, le district est soit :

Les "districts municipaux" du Brésil sont des territoires en lesquels se subdivisent les municipalités et qui, eux-mêmes, peuvent être subdivisés en quartiers (“bairros”. Dans les municipalités plus grandes, ils peuvent aussi être le siège de subpréfectures ou d’administration régionale comme dans le cas propre à Rio de Janeiro. Les districts dans la législation brésilienne, remplacent les anciennes “ Freguesias du Brésil Colonial” qui existent encore dans la constitution portugaise.
Ces districts sont soumis au pouvoir de la préfecture qui a le pouvoir constitutionnel de les créer ou les éliminer. En beaucoup de municipalités, ils ont peu d’importance et parfois ils n’existent même pas (district unique). Normalement, une municipalité ne se divise en plus d’un district que lorsqu'il s’y trouvent des peuplements expressifs en nombre d’habitants et qu'ils sont éloignés de l’aire urbaine principale. En général, ces districts lorsqu’ils ne sont pas absorbés par l’accroissement naturel de la cité, tendent à vouloir se transformer en de nouvelles municipalités.

Le nom de district a aussi au Brésil, des sens différents :

Au Cameroun, un district est une subdivision départementale agglomérant des petits villages ruraux trop petits pour être érigés en commune.

Dans les Territoires du Nord-Ouest les districts étaient une division territoriale du territoire, ils ont disparu lors de la formation du Nunavut.

En Ontario, un district est une subdivision de base dans le nord-est démographiquement peu dense de la province, plus étendue que les comtés, les régions municipales et les municipalités situées dans la partie sud-est urbanisée de la province.

En Colombie-Britannique un district est une division sur deux niveaux :

En Nouvelle-Écosse nous y trouvons un district municipal.

Le mot "district" désigne en français la subdivision dénommée 区 "(pinyin : qū)" en chinois. Ce terme chinois est également utilisé pour traduire la notion française d'arrondissement municipal qui est proche du district chinois.

Le district urbain est une subdivision administrative de la République populaire de Chine.

À Djibouti, le district est le premier niveau de subdivision du pays.


La Hongrie est organisée en districts et en comitats.

En Inde, les "districts" sont des subdivisions des États et territoires.

En Indonésie, l'équivalent du district, le "kecamatan", est une subdivision du "kabupaten", lui-même une subdivision de la province. Dans le cadre de la loi no. 21 de 2001 portant autonomie régionale, les "kecamatan" des provinces de Papouasie et Papouasie occidentale ont été renommés "distrik".

Israël compte six districts dirigés par des commissaires nommés par le Ministre de l'Intérieur. Ils sont divisés en quinze sous-districts (en hébreu : "nafot" נפות; singulier : "nafa") qui sont eux-mêmes divisés en 50 régions naturelles.

Au Japon, les sont des subdivisions situés entre les préfectures et les villes.

Au Kenya, les districts ("wilaya", "mawilaya" au pluriel) sont, jusqu'en 2012, les premières subdivisions administratives des 7 provinces ("mkoa", "mikoa" au pluriel) et de la zone de Nairobi ("mkoa Nairobi"). Après les élections législatives d'août 2012, elles seront les premières subdivisions administratives des 47 comtés. Les provinces « administratives » seront alors abandonnées au profit des comtés à la fois « exécutifs et législatifs ».

Les districts sont eux-mêmes partagés en divisions administratives ("tarafa", "watarafa" au pluriel) elles-mêmes divisées en localités ("mtaa", "wataa" au pluriel) puis en sous-localités ("Kijiji", "vijiji" au pluriel).

Les districts sont, depuis 2010, au nombre de 256, bien que, en , la Haute Cour de justice kényane ("Hight Court of Kenya") ayant compétence en matière de constitution et de révision des lois, a déclaré que tous les districts constitués après 1992 sont illégaux et ont été créés « in complete disregard of the Law » (« dans le mépris complet de la loi »). Pour la cour, seuls les 46 districts créés avant 1992 ont une valeur juridique.

Au Luxembourg, cette subdivision administrative regroupant les cantons a été supprimée en 2015.

En Nouvelle-Calédonie les districts sont une subdivision des Aires coutumières.

En Pologne, on peut désigner en français "district" la subdivision dénommée "powiat" en polonais correspondant à l'échelon administratif inférieur à la voïvodie.

Le mot français "district" peut s'appliquer selon les auteurs aux "raions" ou aux "okrougs" de Russie.

En Polynésie française, les districts étaient à l'époque coloniale le nom des subdivisions des îles des cinq archipels. Ces districts sont aujourd'hui devenus des communes ou des communes associées.

Le mot "district" désigne en français la subdivision dénommée "okres" en slovaque, correspondant à l'échelon administratif inférieur à la région ("kraj" en slovaque).

En Suisse, les districts sont une subdivision des cantons regroupant les communes. Il n'y en a pas dans tous les cantons car selon la Constitution tous les cantons sont souverains et décident donc de leur organisation territoriale.

Au Royaume-Uni, les districts sont une subdivision des comtés regroupant les paroisses civiles.



</doc>
<doc id="17814" url="https://fr.wikipedia.org/wiki?curid=17814" title="Tai-chi-chuan">
Tai-chi-chuan

Le tai-chi-chuan ou tai chi ou taiji quan (, également prononcé en japonais "taikyoku ken") est un art martial chinois dit « interne » ("neijia") souvent réduit à une gymnastique de santé. Il peut aussi comporter une dimension spirituelle. Il a pour objet le travail de l'énergie appelée "chi."

Les origines du tai-chi-chuan sont encore mal connues et sources de nombreuses controverses. Pour mieux marquer son origine, il convient d'abord de le distinguer d'autres pratiques corporelles chinoises plus anciennes liées ou non au taoïsme. Plusieurs hypothèses existent alors, certaines relevant des mythes et d'autres mieux fondées historiquement. 

Certaines légendes attribuent l'invention du tai-chi-chuan au taoïste semi-légendaire Zhang Sanfeng, vers le début de la dynastie Ming (- siècle). Le "Livre complet sur les exercices du tai-chi-chuan", écrit par Yang Chengfu (1883-1936), raconte que Zhang Sanfeng créa le tai-chi-chuan vers la fin de la Dynastie Song (960-1279) puis le transmit à Wang Zongyue, Chen Zhoutong, Zhang Songxi et Jiang Fa. Un peu plus tôt, Li Yishe (1832-1891) écrivit dans sa "Brève introduction sur le tai-chi-chuan" : « Le tai-chi-chuan fut fondé par Zhang Sanfeng des Song. » Zhang créa l'école intérieure (）par un syncrétisme néo-confucianiste des arts martiaux du bouddhisme Chan du monastère Shaolin et de sa maîtrise du "daoyin" () taoïste. Il s'installa dans le temple du mont Wudang, province de Hubei, pour enseigner sa discipline.

À partir des années 1930, Tang Hao, pionnier des recherches historiques sur les arts martiaux, démontre l'absence de fondements historiques concernant la création du tai-chi-chuan par Zhang Sanfeng. Ses conclusions furent reprises à la même époque par Xu Jedon, et sont encore validées de nos jours par les recherches historiques contemporaines.

Wang Zongyue, qui aurait vécu sous la dynastie Qing (1644-1911), occupe une place importante dans l'histoire du tai-chi-chuan. Son influence a été reconnue par les maîtres de différentes époques. Son "Traité du tai-chi-chuan" (太極拳論) a grandement contribué à la compréhension théorique de cette boxe. Toutefois, des doutes subsistent sur l'identité réelle de l'auteur de ce texte. Il pourrait en fait s'agir de Wu Yu-hsiang, qui prétendit avoir trouvé ce manuscrit à Pékin au milieu du . 

C'est malgré tout l'hypothèse retenue dans le "Manuel de taijiquan" () de Shen Shou (, né en février 1930), publié en 1991 par l'Association chinoise de wushu. Selon cet ouvrage, il aurait ainsi été le premier à exposer la théorie et les techniques du tai-chi-chuan de manière systématique. Des documents administratifs attesteraient que Wang Zongyue transmit le tai-chi-chuan à Jiang Fa puis que ce dernier le diffusa à Chenjiagou. C'est cet ensemble de pratiques qui aurait été enfin transmis à Yang Luchan.

Les premières traces historiques apparaissent véritablement avec Chen Wangting vers la fin de la Dynastie Ming (1368-1644). Elles sont notamment issues de travaux menés par Tang Hao et Gu Liuxin, praticiens et historiens du "wushu" (). Tang Hao soutient cette hypothèse à la suite d'investigations menées au village de Chenjiagou, district de Wenxian, province du Henan, et en se référant aux "Annales du district" et au "Registre généalogique de la famille Chen". Selon ce registre, Chen Wangting était « expert en boxe de style Chen et fondateur du jeu de l'épée et de la lance ». Les différentes écoles contemporaines de tai-chi-chuan (Yang, Wu, Sun) seraient originaires ou héritières de la boxe de style Chen, bien que les principes de cette boxe soient antérieurs à l'appellation tai-chi-chuan.

Un autre registre (dont l'authenticité n'est pas entièrement prouvée) découvert très récemment démontrerait que le lieu originel du Tai-chi-chuan ne serait pas le village de Chenjiagou mais plutôt Tang Cun (Henan), village de la famille Li.

Les écoles classiques sont : 

Fondé par Chen Wangting au , le style Chen () connut une évolution avec Chen Changxing (1771-1853) puis fut rendu public grâce à des maîtres tels que Chen Zhaopi (1883-1972) ou Chen Fake (1887-1957), représentant officiel du style familial à la .

Le style Chen a conservé une martialité sans équivoque et demande des qualités corporelles qui séduisent souvent les adeptes des arts martiaux. Contrairement aux autres styles, ses enchaînements se pratiquent en variant puissance et vitesse. Il se caractérise par des spirales manifestes qui animent chaque mouvement.

Le style Yang () est devenu le plus populaire en Occident. Son créateur Yang Luchan (1799-1872) apprit d'abord le tai-chi-chuan Chen dans le village de Chenjiagou, auprès de Chen Changxing. Selon la légende, il modifia le style pour le rendre accessible au plus grand nombre . Il enseigna son style dans la ville de Yongnian, province du Hebei et le transmit à ses fils : 
Yang Chengfu diffusa le style et institua la pratique lente et relâchée qui caractérise le style Yang. Ainsi, dans la forme de Yang Chengfu, les "fajing" (force souple, "jing", qu’on oppose à la force brute, "li") et les sauts sont supprimés, les prises d’appui violentes et les mouvements difficiles sont simplifiés ou remplacés. Au fil des enseignants successifs, la forme de Yang Luchan subit de nombreuses modifications et emprunts à d'autres styles. Le dernier élève connu de Yang Chengfu se nomme Fu Zhongwen et a été filmé. Les écoles issues du tai-chi-chuan Yang sont très nombreuses et proposent un style personnalisé.

Le style Wu provient du travail de Wu Quanyu (1832-1902), militaire mandchou qui étudia avec Yang Luchan et son fils Yang Banhou. Pendant un temps les familles Yang et Wu furent liées et leurs pratiques non distinguées. C'est après l'installation de Wu Jianquan (1870-1942) — fils de Quanyu — à Shanghaï en 1928 que le style Wu se mit à apparaître en tant que tel. En 1935 fut officiellement fondée l'association de Taiji de Jianquan à Shanghaï. Le dernier grand maître reconnu de ce style fut Ma Yueliang, gendre de Wu Jianquan.


Formes associées à d’autres styles internes : 

Les tai-chi associés à des styles « externes » :

Le tai-chi-chuan en tant qu'art martial interne insiste sur le développement d'une force souple et dynamique appelée "jing" (劲), par opposition à la force physique pure "li" (力). 

Une des règles du tai-chi-chuan est le relâchement "song" (). Ce relâchement garantit la fluidité des mouvements et leur coordination. Une fois la relaxation "song" installée, le pratiquant va développer le "pengjing", force interne consistant à relier chaque partie du corps en restant relaxé. Selon un dicton : « Une partie bouge, tout le corps bouge ; une partie s'arrête, tout le corps s'arrête ». Le "pengjing" est la force caractéristique du tai-chi-chuan ; on peut lui trouver une analogie avec une boule élastique. Frappez la boule et votre coup sera retourné contre vous. .

Lors des frappes, l'énergie est tout d'abord concentrée dans le "dantian" inférieur (), qui est un des centres fondamentaux du "qi" (aussi connu sous la désignation hindouiste « second chakra »). Puis elle est libérée, accompagnée d'une onde de choc propagée par l'ondulation des articulations du pratiquant, tel un fouet. On appelle cette action faire jaillir la force, ou "fajing" ().

Le tai-chi-chuan porte une attention particulière à l'enracinement. L'énergie doit aussi s'élancer des « racines » que constituent les pieds, puisque ce sont généralement eux qui, dans la majorité des cas, vont amorcer le coup que transmettra la main, ou tout autre partie frappante. On dit parfois, « le pied donne le coup, la hanche dirige et la main transmet ». L'énergie provient des pieds, puis elle est dirigée par la taille avant d'être transmise par les mains.

Le tai-chi-chuan peut aussi être vu comme un "qigong". Il implique un travail sur le souffle et non sur la force brute. C'est pourquoi l'entraînement est tout d'abord exécuté lentement pour sentir les flux du souffle "qi", en vue d'exercices d'alchimie interne plus approfondis. Le centre de gravité et la respiration doivent être amenés au niveau de l'abdomen, au "dantian" inférieur. 

Les exercices de poussées de mains permettent d'appliquer les principes du tai-chi-chuan avec un partenaire, et ceci de manière progressive. Ils développent la sensibilité du pratiquant et ainsi sa capacité à transformer une action de l'adversaire à son avantage. Ils sont un prélude au combat libre "sanshou".

Les applications peuvent être exécutées de différentes manières :

Le tai-chi-chuan se pratique généralement à mains nues, mais il existe des formes de tai-chi avec éventail, poignard, épée, bâton, sabre, que le pratiquant pourra apprendre après quelques années d'expérience.

La position des jambes, primordiale, accompagne tous les mouvements. Le tai-chi-chuan en utilise trois principales qui sont le pas du cavalier "mǎbù" (), le pas de l'arc "gōngbù" () et le pas vide "xūbù" (). Les pas s'exécutent de manière plus ou moins accentués selon les styles. Les déplacements restent axés sur huit directions principales, équivalentes à celles de la rose des vents, issues du "taiji" et des huit trigrammes.

Le tai-chi-chuan comme pratique de combat utilise huit techniques principales, qui sont appuyer "an" () ; cueillir "cai" () ; presser "ji" () ; heurter "kao" () ; séparer "lie" () ; tirer "lu" () ; parer et projeter "peng" () ; le coup de coude "zhou" (). 

Outre la frappe du coude, le tai-chi-chuan utilise la frappe avec le poing détendu et la frappe avec l'index replié et soutenu par le pouce. Les pieds infligent le coup de talon, le fouet de la pointe du pied, et les coups de pied circulaire vers l'extérieur ou l'intérieur. Le genou frappe également. Il existe aussi des techniques de frappe avec la paume et les doigts (en forme de pique).

En dehors de l'apprentissage des mouvements, postures et respirations, la pratique du tai-chi-chuan comprend des exercices d'assouplissement et de relâchement des muscles et des articulations, destinés à favoriser la circulation du "qi" et appelés "daoyin fa" () ; littéralement technique ("fa") pour entretenir ("yin") la voie ("dao"). Il existe également des exercices nommés "yiyin fa" (), qui consistent en des mouvements visant à développer la sensation de coordination entre les jambes, le bassin, la colonne vertébrale et les bras qui donnent au tai-chi-chuan son efficacité martiale. 

L'enchaînement proprement dit se nomme "taolu" (), encore nommé "gongjia" () ; perfectionnement du style. Il peut être pratiqué à trois vitesses ; une fois à vitesse normale pour corriger les mouvements, une seconde fois un peu plus rapidement pour habituer le corps à l'unité dynamique du début à la fin, et une troisième fois lentement, comme une phase méditative, pour travailler la circulation du "qi". 

Les exercices à deux se nomment : "tuishou" (), qui consiste à apprendre à sentir la force et les mouvements d'autrui en poussant puis absorbant, avec les mains comme point de contact ; et "sanshou" (), forme de combat libre qui met en application les mouvements du tai-chi-chuan.

Les "baduanjin" (), huit pièces de brocart, sont une série d'exercices de "qigong" utilisés dans certaines écoles pour préparer le corps à la pratique du tai-chi-chuan. Le but est d'ouvrir les trois portes (), c’est-à-dire dénouer les épaules, la taille et les hanches afin de faciliter la circulation du "qi". Popularisés par le général Yue Fei au pour entretenir ses troupes, ils évoquent le brocart, longue étoffe de soie brodée portée par les nobles, et symbole de bonne santé. Ils enchaînent huit mouvements aux noms évocateurs : soutenir le ciel par les mains, bander l'arc et viser l'aigle, séparer le ciel et la terre, la chouette regarde vers l'arrière, l'ours se balance, toucher les pieds des deux mains, serrer les poings, ébranler la colonne de jade. Les premières traces écrites de ces exercices peuvent se retrouver dans des textes de l'époque Song, le Dao Shu () et le Yijian Zhi ().
Le grand enchaînement ou « forme longue » se compose de 75 à 108 mouvements (selon la façon de les décompter des différentes écoles) correspondant à une ou plusieurs applications martiales. Il s'exécute lentement et vise à développer une forme de corps particulière. Il doit s'exécuter dans le respect des grands principes théoriques du taijiquan (port de tête, détente de la poitrine, des aines et de la taille, poids dans les coudes et les épaules, coordinations, intention, vide et plein, fluidité, calme, etc.).

Le style Chen comporte en sus un enchaînement plus court et plus rapide incluant de nombreux mouvements explosifs, les poings canons.

Le "tuishou" () est la forme principale de travail à deux du tai-chi-chuan. Son but est d'apprendre à « écouter » le partenaire, à comprendre la force qu'il exerce, puis à la transformer à son avantage. Les bras doivent toujours rester en contact et s'adapter aux mouvements du partenaire. Il peut prendre des formes codifiées à pas fixes ou pas mobiles ou des formes libres qui ne sont pas sans évoquer la lutte, notamment en Chine.

La pratique des armes (bīngqì, 兵器) fait partie de la grande tradition du tai-chi-chuan. Pour chaque arme, on étudie un enchaînement fondamental. Voici une liste d'armes utilisées dans les tai-chi d'armes :


Le tai-chi-chuan a été testé et s'avère efficace sur les symptômes de plusieurs maladies, même si un effet placebo ne peut être exclu du fait de l'absence de comparaison en aveugle. C'est le cas pour la fibromyalgie, pour la polyarthrite rhumatoïde et l'arthrose du genou, ainsi que dans la maladie de Parkinson et dans la lutte contre la douleur. 
Il est également associé à la longévité. Le pratiquant de Bagua Zhang et de Taji Quan nommé "Lu Zijian" a vécu jusqu'à 118 ans et a pratiqué jusqu'à l'âge de 116 ans. D'autres pratiquants de cet art sont très âgés, parfois centenaires, comme Li Zi Ming et Wu tunan.





</doc>
<doc id="17815" url="https://fr.wikipedia.org/wiki?curid=17815" title="Nappe phréatique">
Nappe phréatique

La nappe phréatique (gr. "puisable") est une nappe d'eau que l'on rencontre à faible profondeur. Elle alimente traditionnellement les puits et les sources en eau potable. C'est la nappe la plus exposée à la pollution en provenance de la surface.

Par "nappe", on entend la partie saturée en eau du sol, c'est-à-dire celle où les interstices entre les grains solides sont entièrement remplis d'eau, ce qui permet à celle-ci de s'écouler. Au-dessus, on peut trouver des terrains non saturés, dans lesquels les interstices contiennent aussi de l'air. Cette couche est appelée la "zone non saturée" ou encore zone vadose. 
Il peut suffire d'un petit apport supplémentaire d'eau en provenance de la surface pour faire basculer la couche "non saturée" à l'état "saturé". Si l'épaisseur de cette tranche de terrain est importante, et si la topographie s'y prête, ce mécanisme peut déclencher une inondation par remontée de la nappe phréatique. Ce phénomène a aggravé les crues de la Somme en 2001.


Il existe également les nappes semi-captives ou à drainance.
Le toit ou le substratum (parfois les deux) de l'aquifère sont fréquemment constitués par des formations semi-perméables. Lorsque les conditions hydrodynamiques sont favorables, il peut y avoir échange d'eau avec l'aquifère superposé ou sous-jacent, c'est le phénomène de drainance.

Dans certains contextes hydrogéologiques, des nappes communiquent directement avec les cours d'eau dans un système de relations parfois complexes: renfort des cours d'eaux en période de sécheresse, accompagnement des phénomènes de crues ou échanges et dissémination des polluants issus des activités humaines

Un forage permet de repérer le : c'est le niveau piézométrique, niveau au-dessus duquel les interstices de la roche ne sont pas saturés en eau. Les variations de ce niveau renseignent sur le degré de remplissage de la roche-réservoir.

Le pompage d'eau dans une nappe à une vitesse qui dépasse la vitesse de recharge de la nappe entraîne la baisse du niveau de nappe phréatique, appelée rabattement de nappe.

Un phénomène courant et source d'importants désordres sous les grandes villes et dans les régions industrielles (Bassin minier du nord de la France par exemple) est que des nappes pompées durant plusieurs décennies ou siècles par l'industrie ou pour des besoins en eau potable on cessé de l'être en raison du recul des besoins industriels ou de la pollution de l'eau n'autorisant plus son usage en eau potable. Il s'ensuit une remontée de nappe source d'inondation et de désordres dans les sous-sols construits à l'époque où le plafond de la nappe avait été artificiellement rabattu.

La zone saturée des nappes distingue deux types d'eau :


L'eau liée correspond à :


Au-delà, les forces d'attraction sont encore plus modestes, et on parle d'eau libre.

Contrairement à la zone saturée qui contient deux phases (liquide pour l'eau, solide pour les grains), il y a ici existence d'une troisième phase : l'air. La saturation est alors la part des pores occupée par un type de fluide

La zone non saturée distingue quatre états en fonction de la saturation du sol en eau :





En France, la plus vaste nappe est celle de Beauce dont la surface est de près de sur six départements. Ses réserves sont estimées à près de 20 milliards de mètres cubes.

La plus grosse est la nappe de la Plaine du Rhin en Alsace qui s'étend sur un petit territoire mais dont les réserves sont estimées à 35 milliards de mètres cubes sur la partie alsacienne seulement.

La plus grande nappe aux États-Unis est la nappe d'Ogallala, d'une superficie comparable à celle de la France, qui s'étend du Dakota du Sud au Texas.

Il est effectué via des modélisations et des réseaux de mesures automatiques (piézomètres)

En France, en 2017, le BRGM publie 11 fois par an des bulletins de situation hydrologique des nappes à partir de 1700 points suivis en France depuis au moins 40 ans dans la plupart des cas, et il prépare (avec Méteo France) un outil « "MétéEAU des nappes" » de prévision et modélisation des niveaux de nappes souterraines. Ceci nécessite un dispositif de télétransmission rapide de l'information et de validation rapide de la donnée. En mars 2017, 300 points de surveillance sont déjà disponibles à une fréquence journalière et à la fin de 2018 tout le réseau devrait être équipé pour la télétransmission.

La biophysicogéochimie des nappes réagit aux modifications de leur environnement, et aux modifications climatiques et anthropiques notamment, avec trois principales menaces pour les nappes actuelles : 

Les forages peuvent permettre le transit de pollutions superficielles vers les nappes souterraines car ils perforent la couche superficielle imperméable et rendent finalement cette surface perméable aux éventuels polluants qui peuvent ensuite se retrouver dans les eaux pompées et consommées. 

La plus grande partie des pollutions a pour origine les activités agricoles qui utilisent de nombreux produits (fertilisants, phytosanitaires) sources de pollutions et en concentrations importantes.

Certaines pollutions sont dites naturelles, provenant du passage des eaux dans des zones minéralisées (particulièrement en domaine cristallin). Certaines concentrations anormales en éléments nocifs comme l'arsenic ou le plomb peuvent être détectées dans la nappe et mettre en danger les populations locales.

La valeur attribuée à une nappe ou aux services écosystémiques (nécessaires à sa bonne conservation) change beaucoup selon la rareté de l'eau dans la région considérée et plus généralement selon les lieux et les époques, selon ses usage (eau potable, eau industrielle, irrigation, etc.) et selon que la personne interrogée est usager de cette nappe ou non. 




</doc>
<doc id="17816" url="https://fr.wikipedia.org/wiki?curid=17816" title="Antoine Lavoisier">
Antoine Lavoisier

Antoine Laurent Lavoisier, ci-devant "de Lavoisier", né le à Paris et guillotiné le à Paris, est un chimiste, philosophe et économiste français, souvent présenté comme le père de la chimie moderne, qui se développera à partir des bases et des notions qu'il a établies et d'une nouvelle exigence de précision offerte par les instruments qu'il a mis au point. Il a inauguré la méthode scientifique, à la fois expérimentale et mathématique, dans ce domaine qui, au contraire de la mécanique, semblait devoir y échapper.

Au-delà de la découverte de l'oxydation, des composants de l'air et de l'eau, de l'état de la matière, ses contributions à la révolution chimique sont à la fois techniques, expérimentales et épistémologiques. Elles résultent d'un effort conscient d'adapter toutes les expériences dans le cadre d'une théorie simple dans laquelle, pour la première fois, la notion moderne d'élément est présentée de façon systématique. Lavoisier a établi l'utilisation cohérente de l'équilibre chimique, a utilisé ses découvertes sur l'oxygène, dont il a inventé le nom, ainsi que sur l'azote et l'hydrogène, pour renverser la théorie phlogistique, et a développé une nouvelle nomenclature chimique qui soutient, ce qui se révélera inexact, que l'oxygène est un constituant essentiel de tous les acides. Précurseur de la stœchiométrie, il a surtout traduit des réactions dans les équations chimiques qui respectent la loi de conservation de la matière, donnant à celle-ci une solide assise expérimentale.

Financier de son métier, soucieux d'établir des statistiques précises utiles à ce qu'il appelle à la suite de Condorcet l'arithmétique politique, il a été sollicité par l'administration royale puis révolutionnaire sur de très nombreux sujets depuis l'instruction publique jusqu'à l'hygiène en passant par le système monétaire. Il a aussi produit dans la lancée de Joseph Black la première théorie expérimentale de la chaleur, à travers l'étude non seulement de la combustion mais aussi de la respiration et de la fermentation des sols. Ses œuvres majeures restent le "Traité élémentaire de chimie" (1789), et la "Méthode de nomenclature chimique" (1787).

Né dans une famille aisée, Antoine Laurent de Lavoisier est baptisé le jour de sa naissance en l’église Saint-Merri. Orphelin de mère à l'âge de cinq ans, il hérite d'une grande fortune.

En 1754, à l'âge de onze ans, il intègre le collège des Quatre-Nations à "Paris". Les cours incluent chimie, botanique, astronomie et mathématiques. En classe de philosophie, lors de sa dernière année, il a pour professeur l'astronome Nicolas-Louis de Lacaille, auprès duquel il s'enthousiasme pour la météorologie, passion qui ne le quittera jamais.

À l'automne 1761, il s'inscrit à la Faculté de droit civil et canonique. Il y suit le cursus habituel, qui lui délivre en deux ans le baccalauréat en droit et l'année suivante, en 1764, la licence qui lui permet de s'inscrire au barreau de Paris. Il ne plaidera cependant jamais.

Durant ces études de droit, il assiste à des conférences sur les sciences naturelles. Paris frisonne alors de l'esprit encyclopédique. L'université est agitée par l'abolition du monopole qu'exercent les jésuites sur l'enseignement, la sécularisation de leurs écoles et la création d'écoles d'application, telle, en 1747, l'École des ponts et chaussées, dans lesquelles l'enseignement moral et religieux s'efface devant celui des sciences et des techniques. L'étudiant Lavoisier adhère à la démarche expérimentale que, dans la ligne cartésienne, professe Étienne de Condillac depuis une dizaine d'années dans les salons parisiens. Il est un lecteur avide du "Dictionnaire de la chymie" que vient de publier Pierre Macquer, le théoricien des affinités électives entre corps chimiques, qui préfigurent les équations chimiques. Son premier essai portera sur l'hydratation du gypse et fait l'objet d'une conférence qu'il donne en 1764 à l’Académie des sciences.

Deux ans plus tard, il est lauréat du concours de l’Académie des sciences pour un essai sur l'éclairage public des salles de spectacle et reçoit au nom du roi une médaille d'or. Cette même année 1766, il assiste Jean-Étienne Guettard, botaniste de l'Académie des sciences, dans l'élaboration de l'atlas minéralogique de la France. Il fait des relevés minéralogiques depuis déjà trois ans. Entre juin et novembre 1767, ils travaillent ensemble à une étude géologique de l’Alsace et de la Lorraine. Leur collaboration se prolongera jusqu'en 1780. Parrainé par Henri Louis Duhamel du Monceau, grand ami de son père, Antoine de Lavoisier est élu membre de l’Académie des sciences le et siège au Louvre à l’âge de vingt-quatre ans, soit deux ans avant un autre jeune collaborateur de Jean-Étienne Guettard avec lequel il a appris à travailler, Balthazar Georges Sage, lequel fondera en 1778, l'École des mines.

Ses études de droit sont d'une importance capitale dans la vie de Lavoisier. Elles l'amènent en effet à s'intéresser à la politique française, et lui permettent d'acquérir en 1770 une charge de fermier général. Âgé de vingt-six ans, il entre ainsi au conseil d'administration de la compagnie privée à laquelle le roi délègue le monopole de la collecte des impôts. C'est ce poste de fermier général qui est à l'origine de ses principales découvertes scientifiques en chimie. Affecté au secrétariat chargé de la perception des impôts à l'octroi de Paris, il y dispose en effet d'une balance qui sert à détecter les fraudes, la plus précise d'Europe, et c'est cette balance qu'il utilise pour procéder à des pesées moléculaires de divers gaz avec une marge d'erreur inégalée jusqu'alors.

Toutefois ce poste le tiendra éloigné pendant trois années de ses recherches. Il n'abandonnera cependant jamais son rôle d'expert en finance. Il proposera en 1790, à la faveur de la Révolution, une réforme du système monétaire français et en 1791 un changement d'assiette du système d'imposition. Dans son travail pour le gouvernement de 1791, il participe au développement du système métrique qui uniformise les poids et mesures.

Le , il épouse, à l'église Saint-Roch de Paris, Marie-Anne Pierrette Paulze, la fille d'un fermier général, alors âgée de treize ans. Au fil du temps, celle-ci se révèle une aide et une collaboratrice scientifique précieuse pour son époux. Elle traduit pour lui des ouvrages anglais, parmi lesquels l"'Essai sur le Phlogistique" de Richard Kirwan et les recherches de Joseph Priestley. Elle réalise de nombreux croquis et gravures des instruments de laboratoire utilisés par Lavoisier et ses collègues. Elle écrit et publie également les mémoires de Lavoisier, et accueille des soirées où d'éminents scientifiques débattent des questions liées à la chimie.

À l'automne 1772, Lavoisier se lance dans une recherche de plusieurs années sur ce qui cause la combustion. Reproduisant les expériences de Joseph Black, il rencontre à Paris, en octobre 1774, Joseph Priestley, qui a observé, le août précédent le dégagement d'un mystérieux « air déphlogistiqué ». Par la suite, Lavoisier expose, en avril 1775, dans un fameux mémoire appelé "Mémoire de Pâques," que la combustion a une cause nécessaire, état la présence de cet air déphlogistiqué, qu'il baptisera en 1779 oxygène. À partir de ces informatons, il démontre la nature composée de l'air, et nommera, également en 1779, la partie qui n'est pas de l'oxygène, azote. C'est en 1778 qu'il publiera une description de l'effet de cet oxygène, l'oxydation, effet qu'il appelle acidification, et en 1783, qu'il montrera que l'eau est composée d'un gaz observé par Henry Cavendish, gaz qu'il baptise hydrogène.

La Ferme générale est chargée depuis 1633 d’administrer pour le roi la Surintendance des Poudres et Salpêtres. C’est à ce titre qu’elle propose et obtient de créer en 1775, la Régie Royale des Poudres et Salpêtres, ancêtre de la SNPE et de l’actuel SIMu. Lavoisier en est un des quatre fermiers délégués aux postes de régisseurs. Il est logé dans un hôtel du Grand Arsenal situé le long de l’actuel rue Bassompierre, où il dispose d’un laboratoire. Ses travaux portent sur l’amélioration de la production de la poudre et s’étendent au domaine de l’agrochimie. Il crée un nouveau procédé de production du salpêtre en utilisant la potasse d’Alsace. Son action se traduit par un redressement financier spectaculaire. Les bénéfices reversés par la Régie à l’État lui valent d’être reconnu.

Régisseur des Poudres par délégation de la Grande ferme, Lavoisier n'en continue par moins d'exercer sa charge au sein de celle-ci. Il y joue, comme le fait Goethe auprès du Duc de Saxe, un rôle de conseiller ministériel. À la suite de la « guerre des farines », il s'oppose ainsi au nouveau ministre des finances, le physiocrate Turgot, et à l'inspecteur aux Monnaies que celui-ci vient de nommer, Nicolas de Condorcet, dans leur projet d'une taxation des navires de commerce à la jauge. Pour lui, le seul impôt qui vaille est territorial et porte sur les revenus du capital.

Son travail d'académicien reste néanmoins primordial. En 1777, il lit, devant l'Académie des Sciences, un premier rapport sur la physiologie de la respiration.

En 1778, peu après la mort de son père, il acquiert le domaine et le château de Freschines à Villefrancœur, dans le Blésois. Sa femme en assure, depuis Paris, l'administration et le couple se rend sur place régulièrement, quelques semaines par an, pour rencontrer l'intendant et mesurer les progrès. C'est là que le savant acquiert la conviction que l'humus ne produit pas spontanément la végétation, mais que celle-ci à besoin de deux sources de chaleur, le soleil et le fumier. Sa ferme est d'abord pour lui un objet d'étude de la rentabilité d'une exploitation et lui sert de modèle économétrique.
En 1779 commence une fructueuse coopération avec un professeur de mathématiques trentenaire en poste à l'École des cadets gentilshommes qui a été distingué par l'Académie des Sciences, Pierre-Simon de Laplace.

En 1784, Lavoisier fait partie d'une commission nommée par Louis XVI pour étudier la pratique du magnétisme animal avec le médecin Joseph Ignace Guillotin, l'astronome Jean Sylvain Bailly et l'ambassadeur des États-Unis en France, Benjamin Franklin.

Quand le roi convoque les États généraux, le 27 novembre 1788, c'est à lui, seigneur blésois et scientifique déjà rendu célèbre par les multiples mémoires et rapports qu'il a publiés, que la classe des nobles de Blois confie la rédaction de leur cahier de doléances. Il y reprend l'idée formulée par Thomas Jefferson dans le préambule de la Déclaration d'indépendance des États-Unis que le bien commun est le bonheur : 

Député suppléant d'Alexandre de Beauharnais, Antoine de Lavoisier, très au fait, en tant que fermier général, de ce qu'a été, au début du siècle, le système de Law et du fonctionnement du dollar continental, est celui qui, à l'automne 1789, propose à la Constituante une monnaie d'escompte qui fluidifie les échanges à un moment où la sécularisation des biens du clergé provoque une inflation de ceux-ci. Ce sera l'assignat. Favorable à une réforme profonde de l'Ancien régime et l'instauration d'une monarchie constitutionnelle, il renonce à sa particule et adhère, au printemps 1790, au second parti politique après le « Club breton », le Société de 1789, fondé par son collègue démissionnaire de la Monnaie Nicolas de Condorcet. Fondé à l'imitation du Club breton, le club de 1789 cherche à en contrecarrer l'influence. Tout en continuant ses recherches au laboratoire des Poudres, Lavoisier adresse, en 1792, un projet d'éducation nationale à la Convention.
Il est l'un des trois commissaires du Comité des finances de la Convention chargé de réformer le système de perception des impôts quand la Terreur éclate. Lavoisier, dont l'image est associée à la dévaluation qui a suivi la transformation des assignats en monnaie de nécessité et qui aurait profité aux émigrés, est dénoncé aux autorités révolutionnaires avec les vingt-sept autres fermiers généraux comme traître à la nation par Antoine Dupin, lui-même ancien employé de la ferme. Il est incarcéré avec son beau-père, Jacques Paulze, le 28 novembre 1793 à la prison de Port Libre et accusé d'avoir spéculé contre l'intérêt des citoyens. "L'Ami du peuple" le vilipende comme trafiquant de tabac frelaté par de mauvaises conditions de stockage. Il est condamné à mort, cinq mois après son arrestation, le 5 mai 1794, malgré la courageuse défense de son disciple et collaborateur Jean Noël Hallé.

Ayant demandé un sursis pour pouvoir achever une expérience, il s’entend répondre par le président du tribunal révolutionnaire, Jean-Baptiste Coffinhal : . Il est guillotiné place de la Révolution le , à l’âge de cinquante ans, en même temps que 26 anciens fermiers généraux. Son corps, dépouillé, est empilé dans la fosse commune des Errancis. Le lendemain de l'exécution de Lavoisier, le grand mathématicien Louis de Lagrange commente : .

Son matériel et ses notes sont saisis mais ses travaux d'économétrie, dont il avait fait don à l'Assemblée constituante, peuvent être repris et publiés en 1796 par Lagrange. Après maintes tribulations, sa femme et collaboratrice, Marie Anne Lavoisier, rassemble ses papiers personnels. Ils sont conservés aux Archives nationales sous la cote 129AP. Avec la collaboration de ses amis savants, elle édite ses derniers travaux en forme d'exposé de la méthodologie de la chimie moderne.

L'une de ses plus importantes recherches a été de déterminer la nature du phénomène de combustion, ou oxydation rapide. Ses expériences permettent de démontrer que la combustion est un processus qui implique la combinaison d'une substance avec du dioxygène. À travers cette découverte, c'est toute la conception de la chimie qui est bouleversée.

À l'automne 1772, doutant que la matière, comme l'enseigne Aristote, prenne des formes différentes – fluides, solides, gazeuses – par une seule loi générale de dégénérescence, il se lance dans une recherche sur la combustion des métaux qui, paradoxalement, gagnent du poids au terme de leur calcination. Ses résultats font l'objet de deux publications dans le bulletin de l'Académie des sciences, sur les exemples de la production d'acide phosphorique et de la calcination des sulfures. Lavoisier cherche une cause au processus de combustion qui puisse expliquer qu'elle ne soit pas qu'une dégradation d'état, cause qu'il n'appelle pas encore « oxygène ».

Il consacre l'année 1773 à reproduire les expériences de Joseph Black et finit par découvrir que le gain de poids des métaux calcinés est dû à l'absorption par ceux-ci de l'« air fixe », découvert quelques années plus tôt par son aîné écossais. Les comptes-rendus sont publiés l'année suivante dans "Opuscules physiques et chimiques".
En octobre 1774, il rencontre Joseph Priestley en visite à Paris et résout son problème de calcination de l'oxyde mercurique, qui dégage un gaz mystérieux. Il démontre dans son célèbre "Mémoire de Pâques", présenté à l'Académie des sciences le 26 avril 1775 que, lorsque la combustion est faite au charbon de bois, ce qui se dégage est l'« air fixe », et que ce dernier est produit par la combustion du carbone en présence de l'« air déphlogistiqué » observé par Joseph Priestley. Il en déduit que le gaz mystérieux est un composant présent dans l'air en permanence dans une certaine proportion et le renomme « air vital ».

En 1778, dans les ouvrages "Sur la combustion en général" et "Considérations générales sur la nature des acides", il démontre que l'« air déphlogistiqué », responsable de la combustion, est aussi une source d'acidité. Ce n'est qu'en 1779 qu'il nomme cette partie « vitale » de l'air : « oxygène » (du grec signifiant « formeur d'acide »), et l'autre partie : « azote » (du grec signifiant « sans vie »).

À partir de 1780, il démontre également le rôle du dioxygène dans la respiration végétale et animale, ainsi que son rôle dans la formation de la rouille, autre forme d'oxydation lente.

L'explication de Lavoisier sur la combustion remplace la théorie phlogistique, qui postule que les matériaux relâchent une substance appelée "phlogiston" lorsqu'ils brûlent dans le récipient en question.

Les contemporains de Lavoisier sont en effet convaincus de la théorie aristotélicienne, défendue jusqu'après 1787 devant la Société royale de Londres par Richard Kirwan et son collègue Joseph Priestley, selon laquelle la matière est composée de quatre éléments fondamentaux – la terre, l'air, l'eau et le feu –, dont les variations de dosage détermineraient la nature des corps. Pour expliquer les échanges entre ces éléments et leurs variations, le bon sens a dû construire l'hypothèse "ad hoc" d'un cinquième élément, le phlogistique, sorte d'éther, dans lequel baignerait tout corps et qui échapperait à toute observation directe.

Dès 1774, Lavoisier s'attaque à cette théorie en démontrant devant ses collègues de l'Académie que le dépôt formé par l'évaporation n'est pas une mutation de l'eau en terre, mais le résidu de matières déjà présentes dans le récipient. Il sera le premier à infirmer l'antique théorie, mais ce n'est qu'en 1780 qu'il établit expérimentalement, avec Laplace, dans un célèbre mémoire, que la chaleur n'est pas un fluide, mais le résultat de l'agitation de ce que les savants appellent déjà des molécules

Il abonde ainsi dans le sens de l'hypothèse d'une « chaleur latente », que suppose la théorie du calorique avancée en 1761 par Joseph Black devant ses collègues de la future Société royale d'Edimbourg. Lavoisier n'ira cependant pas jusqu'à rejeter le concept de fluide calorique bien que celui-ci conserve à la chaleur le caractère d'éther et que les concepts d'état de la matière et de chaleur latente, qu'il n'a pas su tirer lui-même, n'ont pas besoin de supposer un tel éther. Ce sera Joule qui le fera en 1843.

En 1783, dans ses "Réflexions sur le phlogistique", Lavoisier, comme Galilée 170 ans plus tôt avec la conception aristotélicienne du mouvement, démontre que cette théorie phlogistique, si elle répond aux impressions ordinaires, n'est pas conforme à l'expérience scientifique.

Avec Laplace, il réalise l'expérience qui met en évidence l'air inflammable, découvert par Henry Cavendish qu'il baptise « hydrogène » (du grec « formeur d'eau »). Ce gaz réagit avec l'oxygène et forme la rosée, qui est de l'eau, comme l'avait déjà remarqué Priestley, sans toutefois l'expliquer. La synthèse de l'eau démolit deux mil cinq cents ans de dogme aristotélicien, selon lequel l'eau est un élément, et réhabilite la théorie épicurienne de Lucrèce sur les atomes. Elle démontre aussi qu'un corps qui se liquéfie n'est pas un corps qui se transforme en un autre, comme le postule la théorie aristotélicienne, mais que le même élément chimique peut, selon les conditions de pression et de température, changer d'état. Le concept sous-jacent d'état de la matière est, quant à lui, totalement nouveau, et ouvre la voie, insoupçonnée par Lavoisier, à une thermodynamique statistique.

Les expériences de Lavoisier sont parmi les premières expériences chimiques véritablement quantitatives jamais exécutées: c'est en ce sens qu'il assure le passage de l'alchimie, discipline symbolique à visée spirituelle plus qu'expérimentale, à la chimie, dont il est le fondateur. Il a prouvé que, bien que la matière change d'état dans une réaction chimique, la masse totale des réactifs et des produits reste identique du début jusqu'à la fin de la réaction. Il brûla du phosphore et du soufre dans l'air, et montra que les produits pesaient plus que les réactifs de départ. Néanmoins, le poids gagné était perdu par l'air. Ces expériences ont été des preuves à la base de la loi de conservation de la matière. Lavoisier a aussi étudié la composition de l'eau, et il appelle ses composants « oxygène » et « hydrogène ».
La maxime attribuée à Lavoisier, est simplement la paraphrase du philosophe grec présocratique Anaxagore : . Dans son "Traité élémentaire de chimie" de 1789, Lavoisier parle de la matière en ces termes : 

Sous la plume de Lavoisier, « quantité de matière » désigne la quantité d’éléments chimiques mis en jeu lors d’une réaction. En posant la réaction chimique en termes laplaciens d’équation, Lavoisier rend possible ce qu’en 1792, le berlinois Jeremias Richter appellera « stœchiométrie » mais ce ne sera qu’en 1802 qu’un disciple de Lavoisier, Claude Louis Berthollet, définira l’équilibre chimique lui permettant d’établir la première classification des éléments.

Avec le chimiste Claude Louis Berthollet et d'autres, Lavoisier conçoit une nomenclature chimique ou un système des noms qui sert de base au système moderne. Il la décrit dans la "Méthode de nomenclature chimique" (1787). Ce système est toujours en grande partie en service au , y compris des noms tels que l'acide sulfurique, les sulfates et les sulfites.

Son "Traité élémentaire de chimie" (1789) est considéré comme le premier manuel chimique moderne, et présente une vue unifiée des nouvelles théories de chimie, fournit un rapport clair de la loi de la conservation de la masse et nie l'existence du phlogiston. En outre, Lavoisier clarifie le concept d'un élément comme substance simple qui ne peut être décomposée par aucune méthode connue d'analyse chimique, et conçoit une théorie de la formation des composés chimiques des éléments.

De plus, son ouvrage contient une liste d'éléments ou substances qui ne peuvent être décomposés davantage, incluant l'oxygène, l'azote, l'hydrogène, le phosphore, le mercure, le zinc et le soufre. Dans sa liste figurent aussi la lumière et la chaleur, toutes deux qui ne sont plus considérées comme étant de la matière selon la physique moderne.

À partir de 1780, Lavoisier collabore avec le mathématicien Pierre-Simon Laplace. Ensemble, ils poursuivent des expériences, entre autres, sur la respiration. Ces expériences sur l'oxydation lente font suite à celles sur l'oxydation rapide.

Les deux académiciens ont placé un cochon d'Inde dans un calorimètre maintenu à zéro degré par de la glace fondante, la chaleur dégagée par l'animal était mesurée en pesant la quantité de glace fondue et dans le même temps la quantité de dioxyde de carbone produit par sa respiration a été mesurée. Ils remarquent que pour une même quantité de dioxyde de carbone dégagée la respiration et la combustion produisent autant de chaleur, ce qui signifie que la respiration est une production de chaleur continue semblable à une combustion lente. Ils démontrent que la respiration est une étape de la thermogenèse nécessaire à l'homéostasie.

Les expériences sont poursuivies à partir de 1789 avec l'ingénieur Armand Seguin, l'inventeur de la première usine, mais le projet d'une description complète de cet aspect de la physiologie animale sera interrompu par la Révolution.

En 1778, trois ans après la mort de son père, Lavoisier achète le domaine de Fréchines près de Blois et se prend d'une passion discrète pour l'agriculture. Il se rend trois fois par an sur ses terres, pour deux à trois semaines, en compagnie de Madame Lavoisier qui assure la correspondance avec le gestionnaire local, le notaire Lefebvre.

Ce domaine est l'occasion pour Lavoisier de mettre en pratique les travaux de Duhamel du Monceau. Après dix ans, Lavoisier rédige un compte-rendu de ses recherches pour la Société royale d'agriculture et déclare qu'il lui faudra encore une décennie pour confirmer ses résultats. Peu avant sa mort sur l'échafaud, il rédige un traité d'agriculture qu'il déclare pratiquement terminé en 1793.

Ses idées sur l’utilisation du fumier sont très conventionnelles pour l’époque. Ainsi, il montre que des apports massifs permettent d’augmenter lentement les rendements. Vers la fin de sa vie, il est confronté à la théorie de l’humus soutenue par Jean Henri Hassenfratz. Cette théorie, qui postule que seul l’humus est capable de nourrir les végétaux, est fausse et prévalut jusqu’aux travaux de Justus von Liebig en 1840.

Lavoisier rédige anonymement un programme de recherche que l’Académie des sciences aurait dû proposer au concours en 1794, date où la Convention supprime l’Académie et condamne Lavoisier. Dans ce programme, Lavoisier décrit le cycle des composants de la matière à la surface de la terre (le cycle réduction-oxydation) et oppose la « végétalisation » (la photosynthèse) à la combustion et aux fermentations. En ce sens, il annonce les grandes découvertes agronomiques du .











</doc>
<doc id="17817" url="https://fr.wikipedia.org/wiki?curid=17817" title="Espace-temps">
Espace-temps

En physique, l'espace-temps est une représentation mathématique de l'espace et du temps comme deux notions inséparables et s'influençant l'une l'autre. 

Cette conception de l'espace et du temps est l'un des grands bouleversements survenus au début du dans le domaine de la physique, mais aussi pour la philosophie. Elle est apparue avec la relativité restreinte et sa représentation géométrique qu'est l'espace de Minkowski ; son importance a été renforcée par la relativité générale.

Le "continuum espace-temps" comporte quatre dimensions : trois dimensions pour l'espace, « x », « y », et « z », et une pour le temps, « t ». Afin de pouvoir les manipuler plus aisément, on s'arrange pour que ces quatre grandeurs soient homogènes à une distance en multipliant « t » par la constante « c » (célérité de la lumière dans le vide).

Un événement se positionne dans le temps et l'espace par ses coordonnées « ct », « x », « y », « z », qui dépendent toutes du référentiel. Il est très difficile de s'imaginer que le temps ne soit pas le même suivant le référentiel dans lequel on le mesure, mais c'est bien le cas : il n'est donc pas absolu ; il en va de même pour l'espace : la longueur d'un objet peut être différente selon le référentiel de mesure.

Dans l'état actuel des connaissances, seul l'espace-temps comme concept unifié, qui est mathématiquement un espace de Minkowski en relativité restreinte et un espace courbe quelconque en relativité générale, est invariant quel que soit le référentiel choisi, tandis que ses composantes d'espace et temps en sont des aspects qui dépendent du point de vue (référentiel).

Le rapport entre les mesures d'espace et temps donné par la constante universelle "c" permet de décrire une distance "d" en matière de temps : "d" = "ct" avec "t" le temps nécessaire à la lumière pour parcourir "d". Le Soleil est à environ 150 millions de kilomètres c'est-à-dire à environ 8 minutes-lumière de la Terre. En disant « minutes-lumière », on parle d'une mesure de temps multipliée par "c", et on obtient une mesure de distance, dans ce cas-ci des kilomètres. Autrement dit, le facteur "c" sert à convertir des unités de temps en unités de distance. Kilomètres et minutes-lumière sont donc deux unités de mesure de distance.

Ce qui unifie espace et temps dans une même équation, c'est que la mesure du temps peut être transformée en mesure de distance (en multipliant "t", exprimé en unités de temps, par "c"), et "t" peut donc de ce fait être associé aux trois autres coordonnées de distance dans une équation où toutes les mesures sont en unités de distance. En ce sens, on pourrait dire que le temps, c'est de l'espace !

Cependant John Wheeler tient à rappeler que le temps et l'espace ont de grandes différences de nature, ne sont pas complètement identifiables et ne se transforment que "partiellement" l'un en l'autre dans un changement de repère.

La notion d'espace-temps intéresse grandement les philosophes, comme Prigogine, Stengers, Bergson, Kant, etc.

Ce concept, ou du moins son nom, est souvent utilisé dans les dialogues et scénarios de romans ou films de science-fiction (exemple : "Interstellar" de Christopher Nolan), illustré par les notions de « vortex spatio-temporel », « univers parallèle », boucle spatio-temporelle, voyage dans le temps, etc.

Le concept est également sujet d'humour, d'aventure ou d'effroi, dans des bandes dessinées, imprimées ou en ligne, telles que "xkcd" de Randall Munroe ou "Vortex" de Stan et Vince ou Bob Morane et Les Tours de Cristal de Dino Attanasio et Henri Vernes, ou encore dans des romans tels que La Patrouille du Temps .de Poul Anderson ou " le Cycle du Temps" des aventures de Bob Morane de Henri Vernes .

Contrairement à la notion d'espace ou de temps, la notion d'espace-temps a globalement du mal à s’ancrer en tant que réalité physique dans la culture générale et l’inconscient collectif. Le temps et l'espace ont toujours tendance à être dissociés et le temps à être perçu uniquement comme un concept qui n'a pas de réalité physique (contrairement à l'espace).

La culture inca ne distingue pas l'espace et le temps ; l'espace-temps est appelé « "pacha" », en quechua et en aymara.

Selon Catherine J. Allen, .

Elle choisit donc de traduire "pacha" par « world-moment » (moment-monde).




</doc>
<doc id="17819" url="https://fr.wikipedia.org/wiki?curid=17819" title="Chronologie des dynasties chinoises">
Chronologie des dynasties chinoises

Cet article propose une chronologie des dynasties majeures dans l'histoire de la Chine ().

L'histoire de la Chine est rarement aussi nette que ne le laisse entendre son étude systématique par dynastie : il est en effet rare qu'une dynastie s'éteigne calmement et laisse la place rapidement et en douceur à la dynastie suivante. La seconde dynastie est souvent établie avant la chute finale du pouvoir précédent et elle continue souvent un temps encore après sa chute, sous la forme de prétendants isolés sur les marges, espérant encore faire renaître la dynastie de leurs grands ancêtres récemment déposés.

De plus, la Chine a eu de longues périodes de divisions, différentes régions étant alors gouvernées par des groupes différents, et habitées par des populations et des cultures différentes. Lors de ces périodes de divisions, il n'y avait aucune dynastie régnant sur l'ensemble de la Chine, mais plutôt un ensemble de royaumes ayant chacun sa dynastie, aux dates de début et de fin propres. Pour les périodes mythiques, les dates sont claires. Cela devient plus complexe pour les périodes plus documentées, faisant apparaitre la complexité de chaque période de transition et faisant donc naître des interprétations différentes chez les experts. C'est le cas pour et à partir des Zhou occidentaux. Un exemple de la confusion possible va suffire :

Par convention, la date de 1644 marque la prise de Beijing par les armées mandchoues de la (future) dynastie Qing, qui permit alors le règne des Mandchous sur l'ensemble Chine, succédant ainsi à la dynastie Ming qu'ils détruisaient. Pourtant, la dynastie des Qing fut établie en 1636 (ou même en 1616, sous un autre nom définissant le même groupe), tandis que le dernier empereur Ming ne fut pas déposé avant 1662 (voire 1683). Le passage d'une dynastie à une autre est une affaire longue et complexe, avec des avancées et des reculs, il fallut ainsi près de 20 ans au pouvoir dominant des Qing pour se faire reconnaître, s'imposer, et devenir incontesté sur l'ensemble du territoire chinois. Aussi, il est factuellement incorrect de considérer que le transfert de légitimité de la dynastie Ming à la dynastie Qing s'effectua en 1644.

Pour plus de détails sur les dynasties listées ci-dessous et leurs empereurs, cliquer sur le lien adéquat dans le tableau ci-dessous. Cliquer sur H pour obtenir l'article historique de la dynastie, et sur E pour la liste de ses empereurs (ou gouverneurs).




</doc>
<doc id="17824" url="https://fr.wikipedia.org/wiki?curid=17824" title="Alliance 90 / Les Verts">
Alliance 90 / Les Verts

L’Alliance 90 / Les Verts () est un parti politique allemand de centre gauche.

Le parti vert allemand () est fondé le à Karlsruhe, en Allemagne de l'Ouest, issu du mouvement écologiste et pacifiste de la fin des années 1970. En 1983, les Verts obtiennent leurs premiers sièges au Bundestag (où ils se distinguent des autres députés, arborant cheveux longs, baskets et pulls en laine), puis un groupe parlementaire en 1987. Cependant, lors des élections générales qui suivent la réunification allemande, fin 1990, ils enregistrent un revers et ne sont plus représentés. En revanche, Alliance 90 (), un groupe oppositionel de l'ex-Allemagne de l'Est de tendance alternative, obtient quelques sièges sur le quota réservé à l'ex-RDA à la faveur des dispositions transitoires spécifiques à ce scrutin de la réunification. Verts et Alliance 90 décident de fusionner en 1993, le parti prenant alors son nom actuel.

Joschka Fischer participe à la fondation des Verts en 1982, avec notamment Otto Schily, l’un des avocats de la Fraction armée rouge, qui deviendra quant à lui ministre de l’intérieur du gouvernement Schröder. Ces deux hommes auront avec Cohn-Bendit une influence décisive au début de la coalition rouge-verte : la suppression du droit du sang dans l’attribution de la nationalité allemande et l’engagement dans le maintien de la paix, avec l’envoi de soldats au Kosovo.

En 2000, à l’université Humboldt de Berlin, Joschka Fischer appelle à la relance de l’Union européenne grâce à l’adoption d’une constitution et à une avant-garde d’une fédération européenne à venir. Après la chute des talibans en Afghanistan, il aide à la reconstruction avec l’organisation de la conférence de Petersberg à la fin de l’année 2001. En 2003, il semble intéressé par le poste de ministre des Affaires étrangères de l’UE.

À la conférence sur la sécurité de Munich, il désapprouve l’intervention américaine en Irak en lançant en anglais à Donald Rumsfeld : ().

À propos de la coalition rouge-verte, Fischer déclare en 2005 : 

Après un score historique de 10,7 % des voix aux élections fédérales de septembre 2009, Les Verts s'associent au SPD pour reprendre au centre droit le Land de Rhénanie-du-Nord-Westphalie, en 2010. 

L'année suivante, l'écologiste Winfried Kretschmann, grâce à une coalition avec les sociaux-démocrates, devient ministre-président du Bade-Wurtemberg et le premier Vert à diriger un gouvernement en Allemagne. À la fin de l'année, ayant dépassé les 5 % des voix en Mecklembourg-Poméranie-Occidentale, le parti est présent dans tous les parlements du pays.

Au cours de l'année 2012, leur candidat Fritz Kuhn remporte la mairie de Stuttgart.

En octobre 2012, dans une élection sans précédent, les membres du parti choisissent Jürgen Trittin et Katrin Göring-Eckardt pour figurer en tête de la liste des candidats pour les élections au Bundestag prévues pour 2013.

Historiquement positionné à gauche et allié naturel du SPD, le parti évolue cependant au début du vers des positions plus centristes, ce qui ne rend dès 2013 plus improbable la création d'une coalition fédérale entre les Verts et les conservateurs de la CDU, ce type d'alliance ayant, de plus, déjà eu lieu localement. Les positions écologistes de la chancelière Angela Merkel sur le nucléaire jouent aussi dans cette évolution.

En septembre 2013, les Verts se retrouvent au centre d'une polémique sur la pédophilie alors qu'un chercheur révèle que le parti et plusieurs personnalités notables, dont son porte-parole au Bundestag, Jürgen Trittin, ont milité dans les années 1980 pour la dépénalisation des relations entre enfants et adultes ; des archives révèlent également que le parti a financé des associations poursuivant cet objectif. En dépit du mea-culpa de Trittin, les Verts pâtissent de ce scandale alors que la campagne pour les élections législatives suit son cours ; crédités de 15 % un an auparavant, ils chutent alors à 9 %. Ils obtiennent finalement 8,4 % des voix, soit 2,3 points de moins qu'en 2009 et ne sont plus que la quatrième force politique du pays, derrière Die Linke. En conséquence de cet échec, Jürgen Trittin et Katrin Göring-Eckardt démissionnent de leurs fonctions à la tête du parti.

L'année 2016 est marquée par des performances contrastées dans les Länder. Si le parti se maintient au pouvoir dans le Bade-Wurtemberg, au moyen d'une coalition avec les chrétiens-démocrates, il régresse en Rhénanie-Palatinat, où il reste au gouvernement dans le cadre d'une coalition avec les sociaux-démocrates et les libéraux, en Saxe-Anhalt, où il accède au pouvoir avec une coalition avec la CDU et le SPD, et Mecklembourg-Poméranie-Occidentale, où il se trouve exclu du Landtag. À cette occasion, il n'est plus représenté dans l'ensemble des assemblées parlementaires allemandes.

En gras, le meilleur résultat et en italique le moins bon résultat dans chaque Land. 

Bündnis 90: 6,4 %, Grüne: 2,8 %
<br>Grüne: 4,2 %, Neues Forum: 2,9 %, Bündnis 90: 2,2 %
<br>Précisément 4,97 % donc sous la barre des 5 % nécessaire pour obtenir une représentation






</doc>
<doc id="17834" url="https://fr.wikipedia.org/wiki?curid=17834" title="Cuisine coréenne">
Cuisine coréenne

La Corée est une région reconnue pour la variété et la qualité de sa cuisine. Ses grands classiques ne sauraient éclipser un répertoire unique à la gloire du goût sous toutes ses formes et sous tous les modes de préparation. Cette cuisine se distingue de celles des pays voisins, notamment le Japon et la Chine, cependant on retrouve des parfums assez proches dans la cuisine de la province pourtant relativement éloignée de Hubei, en Chine. La cuisine coréenne utilise beaucoup de piment (notamment dans le "kimchi" (김치) ou avec le "gochujang" (고추장)), le sésame, sous forme d'huile et de graines y est une quasi constante, l'ail, ainsi qu'une grande variété de légumes, fruits de mer, légumes sauvages, etc. Il existe des différences selon les régions de la péninsule dans la gastronomie coréenne, mais en général la cuisine des régions situées plus au nord est moins épicée que celle des régions situées au sud, notamment en ce qui concerne le "kimchi".

Tous les plats et accompagnements se partagent sur la table, le convive disposant généralement d'un bol de riz et d'un autre de soupe « privatifs » pour pouvoir picorer à loisir. Le plat principal peut être servi de façon individuelle et certains repas peuvent même suivre un enchaînement gastronomique plus classique vu des Occidentaux, mais l'avantage est de pouvoir alterner les goûts de façon libre et souvent créative.

Les féculents les plus courants sont :





Il existe également une tradition des gamelles appelée localement "dosirak" () en Corée du sud ou "kwakpap" () en Corée du Nord.



Les Coréens sont parmi les plus grands consommateurs d'ail de la planète, qu'il soit cru ou cuit. Pour un Occidental non averti, le piment également employé en relativement grande quantité domine beaucoup les autres saveurs de cette cuisine. Un autre aspect typique est l'utilisation abondante, et sous des formes variées, de tous les produits de la mer : poissons, fruits de mer, algues et animaux moins connus en Occident, tels que les concombres de mer ou encore les outres de mer et les "meongge" (멍게), des sortes d'ascidies.




</doc>
<doc id="17835" url="https://fr.wikipedia.org/wiki?curid=17835" title="Acide carboxylique">
Acide carboxylique

Le terme acide carboxylique désigne une molécule comprenant un groupement carboxyle (–C(O)OH). Ce sont des acides et leurs bases conjuguées sont appelées ions carboxylates.

En chimie organique, un groupe carboxyle est un groupe fonctionnel composé d'un atome de carbone, lié par une double liaison à un atome d'oxygène et lié par une liaison simple à un groupe hydroxyle : -OH.

En chimie, les acides carboxyliques R-COOH constituent avec les acides sulfoniques R-SOH les deux types d'acides de la chimie organique. On les trouve de manière abondante dans la nature, sous la forme d'acide gras (lipide) et ils sont très importants en chimie industrielle. Par exemple, l'acide acétique est non seulement une brique importante pour les molécules complexes que l'on trouve en biologie, mais est aussi une molécule produite industriellement et qu'on retrouve dans le vinaigre. Un des plus connus est l'acide acétylsalicylique ou aspirine. La brique de base des protéines, les acides aminés sont des acides carboxyliques.

Le groupe fonctionnel caractéristique est le groupe carboxyle, où R est un hydrogène ou un groupe organique :

Les acides carboxyliques ont pour formule brute CHO lorsque le R est un groupe alkyle. Le calcul du nombre d'insaturation donne : formula_1. Cette insaturation traduit la liaison double carbone-oxygène.

On écrit souvent les groupements carboxyles sous la forme réduite : -COOH (forme non ionisée du groupement). La forme ionisée du groupement est : -COO.

Celui-ci est toujours situé en fin de chaîne carbonée. L'ajout d'un groupement "carboxyle" à un composé organique est une carboxylation, l'élimination de ce même groupement est une "décarboxylation".

Ce sont les bases conjuguées R-COO des acides carboxyliques. Ces bases sont en général plutôt faibles. La charge négative sur la molécule est délocalisée sur les deux atomes d'oxygène du groupe carboxyle par mésomérie, ce qui explique la stabilité relative de ce type de molécules.

L'ion carboxylate est un tensioactif amphiphile, c'est l'espèce détergente du savon. En effet, le groupe carboxylate _COO- est hydrophile car très polaire. En revanche, la chaîne carbonée R est apolaire et donc hydrophobe et lipophile.



NB : un moyen mnémotechnique pour se souvenir des noms des diacides linéaires, dans l'ordre croissant du nombre de carbones, est la phrase suivante : « On Mange Saucisse Grillée A Point » (Oxalique, Malonique, Succinique, Glutarique, Adipique, Pimélique). Les diacides sont utilisés pour la synthèse de polyamides et de polyesters.

D'autres types d'acides carboxyliques peuvent être cités : les acides dicarboxyliques, les acides tricarboxyliques, les acides alpha hydroxylés, les cétoacides, les acides aminés et les acides gras.

Les acides carboxyliques sont liquides dans les conditions normales tant que leur chaine carbonée présente moins de 8 atomes de carbone. Ils sont solides au-delà.

Les acides de faible masse molaire possèdent une forte odeur ; par exemple l'acide butanoïque est responsable de l'odeur du beurre rance.

La fonction acide carboxylique est fortement polaire et est à la fois donneur et accepteur de liaisons hydrogène. Ceci permet la création de liaisons hydrogène par exemple avec un solvant polaire comme l'eau, l'alcool, et d'autres acides carboxyliques.

De par cette propriété les acides carboxyliques de petite taille (jusqu'à l'acide butanoïque) sont complètement solubles dans l'eau. Les molécules d'acides sont aussi capables de former des dimères stables par pont hydrogène, ce qui permet d'expliquer pourquoi leur température d'ébullition est plus élevée que celle des alcools correspondants.

En solution dans l'eau, l'acide se dissocie partiellement en ion carboxylate,
selon l'équation-bilan :

Ce sont des acides faibles dans l'eau (pK entre 4 et 5).

Comme les alcools, les acides carboxyliques montrent un caractère acide et basique : la déprotonation en ions carboxylates est facile, mais la protonation est plus difficile. Ils possèdent donc un pK plus faible que celui des alcools.
En fait l'acidité des acides carboxyliques s'explique par l'effet inductif dans le groupement carboxyle : la liaison C=O est très polarisée (électronégativité de l'oxygène supérieure à celle du carbone) ce qui fait que le carbone est électrophile, et il attire donc les électrons de l'autre oxygène. Or cet autre oxygène est lui-même lié à un hydrogène, et cette liaison est aussi polarisée, donc l'électron de l'hydrogène qui s'est rapproché de l'oxygène est attiré à son tour par le carbone électrophile. Cet hydrogène devient donc très facilement mobile, d'où l'acidité du groupement carboxyle.

La solubilité de l'acide carboxylique croit avec le pH.

En infrarouge (IR), l'acide carboxylique présente 2 bandes de valence :

D'après la théorie VSEPR :

L'acide carboxylique possède plusieurs formes mésomères.

Comme le montre, entre autres, les différentes formules mésomères de l'acide carboxylique :

Les acides carboxyliques comptent de nombreux dérivés :

En termes de groupe partant (nucléofuge), l'ordre de facilité est :

Cl (chlorure d'acyle), RCOO (anhydride), RO (ester), NH et NRR (amides).



formula_2formula_3
formula_4
formula_5

Il s'agit simplement des hydrolyses des différents dérivés d'acides.

formula_6
formula_7 puis formula_8
formula_9 
formula_10
formula_11
formula_12

La synthèse a lieu à basse température (). Le dioxyde de carbone est alors sous forme solide, dite carboglace. Il est mis en excès. Après réaction, on effectue une hydrolyse en milieu acide pour obtenir l'acide carboxylique.

Première étape : addition de l'organomagnésien sur CO

Deuxième étape : hydrolyse en milieu acide

La synthèse malonique est un ensemble de réactions permettant de synthétiser de nombreux acides carboxyliques primaires ou secondaires à partir du malonate de diéthyle.

Elle est composée :


Cette synthèse est d'autant plus intéressante qua priori", elle permet de synthétiser n'importe quel acide carboxylique, puisque, à part un groupe tertiaire, il semble qu'on puisse mettre ce que l'on veut à la place de R"'.



</doc>
<doc id="17836" url="https://fr.wikipedia.org/wiki?curid=17836" title="Grec ancien">
Grec ancien

Le grec ancien est l’étape historique de la langue grecque qui s'étend du IX siècle avant notre ère au VI siècle de notre ère.

À l’origine, il existait une grande variété de dialectes, regroupés en quatre groupes : arcadochypriote, occidental, éolien et ionien-attique. Parler "du" grec ancien n’a pas grand sens lorsqu'on veut se référer à un des idiomes antiques : dans les faits, cependant, "le" grec ancien désigne l’attique (dialecte du groupe ionien-attique), langue de l’Athènes antique. C'est en effet la langue dans laquelle est écrite la majorité de la littérature grecque classique. Pendant la période hellénistique et le brassage des populations hellénophones en résultant, la "koinè", langue commune (c’est le sens de l'adjectif /) issue de plusieurs dialectes du groupe ionien-attique, s'est progressivement imposée au détriment des dialectes, devenant ainsi la "lingua franca" de l’Antiquité, en concurrence avec le latin.

La "koinè" est ensuite devenue langue officielle de l’Empire romain d'Orient avant de continuer d’évoluer pour donner naissance au grec moderne d’aujourd’hui.

La première forme d'écriture attestée pour noter un dialecte grec est le linéaire B, un syllabaire sans rapport avec l'alphabet grec, servant à transcrire le mycénien, forme archaïque d'un dialecte arcadochypriote utilisée en Grèce continentale et en Crète entre environ 1550 et 1200 av. J.-C. Entre 800 et 200 av. J.-C., une écriture proche, le syllabaire chypriote, a été utilisée à Chypre pour transcrire le grec et l'étéochypriote (une langue non indo-européenne partiellement déchiffrée, peut-être apparentée au lemnien et à l'étrusque).

Des écritures plus anciennes ont existé en Grèce, mais n'ont vraisemblablement pas servi à noter du grec :

Toutes ces écritures étaient vraisemblablement de nature syllabique.

C'est ensuite l'alphabet grec, hérité des Phéniciens et de leur alphabet, qui a été utilisé sous différentes versions (dites épichoriques) à partir du ou du puis a été normalisé et imposé au reste du monde hellénophone par Athènes en 403 av. J.-C. En ajoutant des voyelles à cet abjad sémitique, les Grecs sont les inventeurs des alphabets occidentaux. En effet, emprunté par les Étrusques (cf. Alphabet étrusque), qui l'ont transmis aux Romains, il a donné naissance à l'alphabet latin mais aussi, sans passer par les Étrusques, à l'alphabet gotique, à l'alphabet cyrillique, à l'alphabet copte, etc.

L'histoire de l'alphabet grec constitue un article séparé.

Le grec ancien est une langue à accent de hauteur possédant deux (ou trois, selon les interprétations) intonations : aiguë et circonflexe. Il se caractérise aussi par un système de consonnes aspirées et par un jeu d'oppositions de quantités vocaliques. Il existe plusieurs règles de sandhi, tant internes qu'externes.

En passant de l'indo-européen commun au grec ancien, la langue a subi de nombreuses modifications phonétiques dont les plus flagrantes sont décrites par la loi de Grassmann, la loi d'Osthoff et la loi de Rix. On note d'autre part qu'il permet de restituer dans de nombreux cas la coloration des laryngales indo-européennes. Enfin, c'est une langue centum.

Comme d'autres langues indo-européennes anciennes, le grec est hautement flexionnel. Outre l'utilisation de désinences, le grec se caractérise par des procédés hérités de l'indo-européen commun comme l'alternance vocalique et l'utilisation du redoublement.

Les noms possèdent cinq cas (nominatif, vocatif, accusatif, génitif et datif), trois genres (masculin, féminin et neutre, parfois réduits à une opposition animé/inanimé) et trois nombres (singulier, duel, pluriel et collectif pour les neutres). Le grec moderne n'utilise plus le datif, excepté dans quelques expressions comme "en taxei", mais les autres cas sont généralement conservés.

On compte trois grands types de déclinaisons, tant pour les noms que les adjectifs (type en -α / -η, type thématique en -ος et type athématique), lesquels possèdent plusieurs sous-types. Les pronoms suivent un système qui leur est propre et qui, ayant influencé les types nominaux, n'en sont pas très éloignés.

Les verbes ont trois voix (active, moyenne et passive), trois personnes et trois nombres. Il se conjugue selon six modes : quatre personnels (indicatif, impératif, subjonctif et optatif) et deux impersonnels (infinitif et participe). Il existe sept temps (présent, imparfait, aoriste, futur simple, parfait, plus-que-parfait, et futur antérieur, ces deux derniers étant rarement usés), qui n'existent toutefois pas à tous les modes.

Outre le temps, le verbe exprime surtout trois aspects (imperfectif, perfectif et statique) et, comme toutes les langues, plusieurs modes de procès (inchoatif, itératif, fréquentatif, etc.). Seul l'indicatif marque toujours le temps ; aux autres modes, c'est l'aspect qui est généralement indiqué.

Il existe deux grandes catégories de conjugaisons : les thématiques (ou "verbes en -ω") et les athématiques (dits "verbes en -μι"): les verbes thématiques se caractérisent par la présence d'une voyelle avant la désinence, absente dans les verbes athématiques. Ces catégories se divisent en un grand nombre de sous-catégories. Le système verbal est très complexe car la flexion met en œuvre de nombreux procédés comme l'alternance vocalique, la suffixation par le jeu de désinences, l'utilisation d'une voyelle thématique, celle de l'augment et du redoublement. À tous ces procédés s'ajoutent des modifications phonétiques importantes au sein d'un même paradigme.

En sorte, il n'est pas exagéré de dire qu'il existe plus de verbes irréguliers que de verbes réguliers, si du moins on s'en tient à la définition de verbe irrégulier ayant cours dans la grammaire française.

Tout ceci est donné à titre indicatif, car comme toute langue flexionnelle, le grec ancien s'accorde une grande liberté dans la place des groupes.

Pour les verbes, le grec met souvent le verbe en fin de proposition, qu'elle soit principale ou subordonnée, mais bien moins systématiquement que le latin. Il existe une exception pour les impératifs et les verbes à tournure impersonnelle (comme le verbe « être » : "ἐστί", « il/elle est », traduisible par « il y a ») qui sont généralement en tête de proposition.

Règle du génitif enclavé : le génitif se place sous l’article, soit entre l’article et le mot désignant le possesseur, soit après répétition de l’article. Dans le groupe nominal « le fils du citoyen » on écrira en grec : "Ὁ τοῦ πολίτου υἱός", littéralement « le du citoyen fils » ; mais il est également possible de positionner le génitif après répétition de l'article, par exemple : "Ὁ υἱός ὁ τοῦ πολίτου", « le fils le du citoyen ». L’adjectif quant à lui, se place généralement soit entre l'article et le nom (τὸ μικρὸν ἄνθος : la petite fleur), ou bien après le nom avec une répétition de l’article ("τὸ ἄνθος τὸ μικρόν", littéralement « la fleur la petite »).

Pour les particularités de la négation en grec ancien : voir "Négation (linguistique)".

Le grec ancien se caractérise également par le maintien d’une règle de l’indo-européen commun, qui stipule qu’un verbe dont le sujet est un nom neutre pluriel ne se conjugue pas au pluriel mais au singulier. Voir l’article consacré à la règle dite « "" ».

Un grand nombre de mots en latin, français et anglais, pour ne citer que ceux-là, sont d'origine grecque, et la majorité des néologismes savants utilisés de par le monde est bâtie sur des radicaux grecs (souvent mêlés de radicaux latins). Seules quelques langues européennes, comme l'islandais, de manière systématique, et, dans une moindre mesure, l'allemand, le turc, le tchèque et le croate, n'utilisent pas ces radicaux mais traduisent par calque les termes savants grecs au moyen de radicaux qui leur sont propres.

Des mots comme « boutique », « caractère » ou « beurre » viendraient aussi du grec ancien. Passés par le latin et hérités comme tel dans la langue française (via d’autres langues, comme l’occitan), ils ont subi les mêmes modifications phonétiques que les autres mots hérités et sont maintenant très éloignés de leur étymon grec puisqu'il faut reconnaître derrière chacun d’entre eux : /, / et /.

Voici, pour illustrer l’omniprésence du grec dans les langues occidentales, un texte de Xenophón Zolótas () dans lequel chaque mot (hormis les mots-outils) est d’origine grecque :
En France, l'enseignement du grec ancien est proposé dans quelques collèges et lycées. Les élèves peuvent le débuter dès la troisième ou la seconde et le passer en option pour le baccalauréat. Il s'apprend aussi dans l'enseignement supérieur pour que les universitaires puissent avoir accès aux textes originaux et en établir des éditions scientifiques.

Le grec ancien est une option spécifique dans les établissements d'enseignements secondaires préparant à la maturité gymnasiale en Suisse, et peut être choisi comme sujet d'examen pour ce diplôme.

Au Québec, le grec ancien est toujours enseigné à l'Université Laval, l'Université de Montréal, l'Université Concordia et l'Université McGill au sein des premier et second cycles selon les universités. Anciennement une matière obligatoire du cours classique aux côtés du latin, son enseignement fut abandonné dans les années 1960 après la création des cégeps.












</doc>
<doc id="17839" url="https://fr.wikipedia.org/wiki?curid=17839" title="Max Planck">
Max Planck

Max Planck, né Max Karl Ernst Ludwig Planck le à Kiel, dans le duché de Schleswig (dorénavant en Allemagne), et mort le à Göttingen, en Allemagne pendant l'occupation alliée, est un physicien allemand. Il fut lauréat du prix Nobel de physique de 1918 pour ses travaux en théorie des quanta. Il a reçu la médaille Lorentz en 1927 et le prix Goethe en 1945. Max Planck fut l'un des fondateurs de la mécanique quantique. De ses travaux fut conceptualisée l'ère de Planck, période de l'histoire de l'Univers au cours de laquelle les quatre interactions fondamentales étaient unifiées.

Max Planck, né le 23 avril 1858 à Kiel dans le duché de Schleswig, est issu d’une famille nombreuse et bourgeoise. Ses arrière-grand-père et grand-père paternels sont professeurs de théologie, son père professeur de droit (il participa à la rédaction du code civil allemand), tandis que sa mère est issue d'une famille de pasteurs.

Max Planck fait ses études secondaires à Munich où son père enseigne.

Il hésite alors entre se consacrer à la science ou à la musique. En 1874, il entame des études de mathématiques et de physique à l’université. Il obtient son baccalauréat à dix-sept ans et, trois ans plus tard, il conclut son cursus universitaire à Berlin avec Hermann von Helmholtz et Gustav Kirchhoff comme professeurs.

En 1878, il soutient sa thèse de doctorat sur « le second principe de la thermodynamique » et la notion d'entropie. Ses professeurs ne sont guère convaincus. Il passe néanmoins son habilitation en 1881 sur « les états d'équilibre des corps isotropes aux différentes températures », aboutissant aux mêmes résultats que ceux obtenus auparavant par l'Américain Josiah Willard Gibbs, dont les travaux étaient restés confidentiels.

Jusqu'en 1885, il recherche un poste d'enseignant en physique théorique, discipline peu à la mode à l'époque. Il obtiendra enfin un poste de professeur adjoint à l'université de Kiel en 1885.

À la mort de Gustav Kirchhoff, et sur recommandation de Helmholtz, il est appelé à l’université Humboldt de Berlin comme professeur adjoint puis titulaire en 1892. Un poste qu'il gardera environ quarante ans.

À Berlin, il poursuit des travaux en thermodynamique, en électromagnétisme et en physique statistique.

Planck rejette, dans un premier temps, le modèle atomiste des gaz de Maxwell et Boltzmann. Pour lui, la théorie atomique s’effondrera à terme en faveur de l’hypothèse de la matière continue. Il se rallie devant l'évidence à l'atomisme à partir des années 1890.

À cette même époque, Lord Kelvin identifie le rayonnement du corps noir comme l'un des problèmes à résoudre. Jožef Stefan, Ludwig Boltzmann, Wilhelm Wien s'y attaquent ainsi que , Ernst Pringsheim, Heinrich Rubens, , Friedrich Paschen et Lord Rayleigh.

Travaillant à formuler avec exactitude le second principe de la thermodynamique, Planck s’intéresse dès 1894 au rayonnement électromagnétique du corps noir. Il adopte les méthodes statistiques de Boltzmann.

En octobre 1900, il détermine la loi de répartition spectrale du rayonnement thermique du corps noir en introduisant la constante de Planck, sans en maîtriser l'interprétation physique.

C’est à la fin de 1900 qu’il présente sa découverte à la société de physique de Berlin. C’est la naissance de la théorie des quanta, qu'il ne contribuera pas beaucoup à approfondir, laissant Albert Einstein l'étayer solidement. Planck a du mal à accepter sa propre hypothèse, rendant la matière « discontinue ». 

Planck devint, par la suite, l'un des premiers soutiens d'Einstein, bien que ce dernier fût très critique vis-à-vis des théories de Planck avant de reconnaître ses positions novatrices.

Avec Walther Nernst, Planck organise en novembre 1911 à Bruxelles le premier congrès Solvay qui réunit les sommités de la physique de cette époque. Vers la même époque, il s'oppose au positivisme logique d'Ernst Mach.

Il prend sa retraite universitaire en 1927 mais continue à enseigner par la suite. Il reçoit, cette année-là, la médaille Lorentz, prix décerné par l'Académie royale des arts et des sciences néerlandaise.

Depuis 1894, il est membre de l'Académie royale des sciences et des lettres de Berlin dont il est nommé secrétaire perpétuel du comité de physique en 1912, impulsant une certaine dynamique à cette institution. Il y a fait notamment admettre Einstein.

Après avoir été proposé à deux reprises, en 1907 et en 1908, il reçoit enfin le prix Nobel de physique de 1918 (remis en 1919 pour cause de guerre).

En 1913, il est nommé recteur de l'université de Berlin.

En 1921, il est lauréat de la médaille Liebig puis en 1927 de la médaille Franklin pour sa notion de quantum d'énergie et de la médaille Copley en 1929.

La « médaille Max-Planck » de physique est créée, elle lui sera conjointement attribuée avec Einstein en 1929.

L’année suivante, à la mort de von Harnack, Planck est nommé président de la société KWG ("Kaiser Wilhelm Gesellschaft", en l'honneur du kaiser Guillaume) qui deviendra après la Seconde Guerre mondiale la Société Max-Planck ("Max-Planck-Gesellschaft"), l'une des grandes institutions de la recherche allemande.

Dans le même temps, il rédige des traités de physique théorique et travaille sur des ouvrages de vulgarisation réputés pour leur accessibilité. Il s'intéresse beaucoup à la pédagogie. Il a été le directeur de thèse de deux lauréats du prix Nobel, Max von Laue en 1903 et Walther Bothe en 1914, mais également du philosophe Moritz Schlick (1904).

Max Planck meurt le 4 octobre 1947 à Göttingen.

Planck est reconnu par les plus grands scientifiques, même avant sa mort. Einstein dit de lui qu’il est . Louis de Broglie affirme : 

Il se marie avec Marie Merck (1861-1909) en 1887 et devient père de famille dès 1888. Ils s'installent alors à Grunewald, dans la banlieue de Berlin. Il aura au total quatre enfants, tous morts avant lui. Trois moururent lors de la Première Guerre mondiale : son fils aîné, Karl, devant Verdun en 1916 et ses deux jumelles en 1917 et 1919, de suites de couches. Erwin, son cadet, est fait prisonnier en France. Ce dernier est resté très proche de son père durant l'entre-deux-guerres, occupant des fonctions administratives importantes dans la République de Weimar. Il est arrêté en 1944, accusé de tentative d'assassinat sur Hitler dans le cadre du complot du 20 juillet 1944. Erwin est exécuté en février 1945.

Sa première femme meurt en 1909 et il se remarie avec Marga von Hößlin (1882-1948).

Planck a toujours conservé de sa jeunesse un attrait marqué pour la musique : il a ainsi composé quelques pièces et maîtrise le piano avec lequel il joue parfois avec le violoniste Joseph Joachim, ou plus tard avec Albert Einstein.

Planck a toujours été respectueux de la hiérarchie mais n'hésite pas à défendre ses convictions contre les opinions du moment. Il a témoigné à plusieurs reprises de son patriotisme et de son soutien à la monarchie avant et pendant la Première Guerre mondiale.

Il défend l'universitaire Léo Arons en 1895 qui appartient à un parti d'opposition, et ce contre l'avis du ministre du Culte et de l'Éducation de l'époque. De même, il favorise l'accès à l'enseignement supérieur aux femmes, dont Lise Meitner.

En 1914, il signe le « Manifeste des 93 », proclamant sa solidarité avec l'armée allemande. Il réitère à plusieurs reprises des discours patriotiques mais modère dès 1915 son attitude en refusant le boycott des publications britanniques préconisé par Vienne. Il pense alors à l'après-guerre en évoquant la situation désastreuse de la science allemande en cas de défaite et lutte contre toutes les tentatives d'isolationnisme en faisant preuve de modération.

Dans l'entre-deux-guerres, il participe activement à la reconstruction de la vie intellectuelle allemande en réussissant à obtenir d'importantes subventions de l'État ou de fondations privées. Politiquement, il reste plutôt conservateur, défendant le pouvoir en place et étant défavorable au suffrage universel. Il refuse toutefois, à plusieurs reprises, de s'exprimer à propos de sujets en dehors de la sphère scientifique. Il plaide fortement en faveur de la recherche fondamentale, s'opposant en cela à Stark dont l'influence grandit avec celle des nazis.

La montée de l'antisémitisme commence à atteindre plusieurs grands savants dont le plus célèbre reste Einstein. En 1933, Hitler devient chancelier du Reich. Planck occupe alors des postes clés dans plusieurs institutions, dont l'institut "Kaiser-Wilhelm", société savante possédant un certain pouvoir financier. Il pense alors pouvoir modérer la politique du Führer par un certain degré de pragmatisme. Il ne s'oppose donc pas directement au pouvoir en place et prône la discrétion, plusieurs de ses interventions publiques sont imprégnées de modération. En mars 1933, Einstein, en voyage aux États-Unis, annonce qu'il ne retournera pas en Allemagne pour des raisons politiques. Planck manifeste en privé son désaccord avec cette décision, estimant que ses effets risquaient d'être délétères pour les scientifiques juifs encore sur place. Il rencontre en mai 1933 Adolf Hitler pour essayer de défendre ses collègues juifs dans l'intérêt de l'Allemagne, sans succès. Ses discours ultérieurs restent dans la ligne choisie, mêlant une certaine ambiguïté dans l'opposition : il fait ainsi plusieurs éloges de la relativité sans en citer pourtant l'auteur. Les résultats sont néanmoins positifs dans les premières années : il fait échouer la nomination de Stark à la tête d'un institut important, parvient à obtenir des fonds pour la recherche et à conserver des membres juifs. Sous la pression, la société savante sous la direction de Planck doit cependant s'aligner progressivement sur le pouvoir, le savant étant obligé de discourir en l'honneur du Führer et de faire le salut nazi. Planck finit par abandonner toute fonction officielle en 1938. Il continue cependant de donner des conférences sur des thèmes sensibles comme "Science et religion" où il avoue croire en Dieu, mais pas en celui des chrétiens.

Sa maison, à Grunewald, est détruite par un bombardement aérien le 15 février 1944 alors qu'il résidait à Rogätz, près de Magdebourg. À plus de 80 ans, il est obligé de fuir les bombardements alliés. À la libération, il se réfugie à Göttingen avec sa femme et sa nièce. À la demande des survivants, il devient un temps le président de l'institut "Kaiser-Wilhelm", transformé en "Institut Max Planck" le 11 novembre 1946.

En 1900, Max Planck découvre la loi spectrale du rayonnement d'un corps noir (publiée en 1901) en essayant de réconcilier la loi de Rayleigh-Jeans qui fonctionne aux grandes longueurs d'onde (basses fréquences) et la loi de Wien qui fonctionne aux petites longueurs d'onde (hautes fréquences). Il estime que sa propre fonction correspondait remarquablement bien aux données pour toutes les longueurs d'ondes.

La correction de la loi de Rayleigh-Jeans est particulièrement importante, car elle est construite sur une base théorique forte : la thermodynamique telle qu'elle était connue à l'époque ; mais souffre d'un défaut majeur aux longueurs d'ondes courtes : la catastrophe ultraviolette. Ce point suggère que la thermodynamique est fausse. Planck essaye donc de produire une nouvelle théorie fondamentale destinée à remplacer la thermodynamique.

La loi de Rayleigh-Jeans et la loi de Planck utilisent le théorème d'équipartition et font correspondre un oscillateur à chaque fréquence. Rayleigh suppose que tous les oscillateurs sont également excités, sa loi prédit que les oscillateurs de très courtes longueurs d'onde sont fortement excités même à température ambiante.

Planck déduit sa loi de façon empirique. Il la justifie en postulant que l'énergie émise ou absorbée par les oscillateurs ne se fait que par petits paquets d'énergie E. Ces paquets seraient directement reliés à la fréquence des oscillations selon la formule qu'il expose le 14 décembre 1900 :

où :

Cette hypothèse permet de limiter l'excitation des oscillateurs aux courtes longueurs d'onde, puisqu'ils ne peuvent absorber qu'une énergie au moins égale à formula_1.

Bien qu'il soit facile maintenant d'interpréter cela en termes de quantification de la lumière en photons, Planck ne propose pas cette quantification. Cela apparaît clairement dans son article de 1901, dans les références qu'il y donne sur le travail qu'il a effectué sur le sujet, ainsi que dans ses "" ("Cours sur la théorie du rayonnement thermique", éditées en 1906 à Leipzig) où il explique que sa constante concerne les oscillateurs.

À l'époque, cette relation n'est considérée que comme un artifice de calcul mathématique. L'idée de quantification est développée par d'autres, notamment Einstein qui, en étudiant l'effet photoélectrique, propose un modèle et une équation dans lesquels la lumière est non seulement émise mais aussi absorbée par paquets ou photons. C'est l'introduction de la nature corpusculaire de la lumière.

À la fin de sa vie, il conclut :

Planck écrivit de nombreux articles scientifiques mais publia également plusieurs ouvrages et recueils de cours dont "Le Principe de la conservation de l'énergie" (1887), le "Précis de thermochimie" (1893), le "Cours sur la théorie du rayonnement thermique" (1906) et son "Cours de thermodynamique" (9 éditions entre 1897 et 1930).

Il écrivit également des ouvrages de vulgarisation scientifique, comme "L'image du monde dans la physique moderne" (Éd. Gonthier, 1933) ou les "Initiations à la physique" (Flammarion, Bibliothèque de philosophie scientifique, 1941).

Vers la fin de sa vie, il fit de nombreuses conférences sur des thèmes plus philosophiques comme "Le Concept de causalité en physique" (1937), "Science et religion" (1937), "Signification et limites de la science" (1941) ou "Les faux problèmes de la science" (sa dernière conférence donnée en 1946) . Il rédigea en 1945 une "Autobiographie scientifique", court fascicule d'une trentaine de pages résumant son parcours.



</doc>
<doc id="17841" url="https://fr.wikipedia.org/wiki?curid=17841" title="Conséquences de la guerre d'Irak">
Conséquences de la guerre d'Irak

Les principales conséquences de l'invasion de l'Irak en 2003 les plus cités sont :

Comme l'avaient prévu plusieurs analystes, la disparition du régime de Saddam Hussein et l'occupation du pays par les forces américano-britannique entraîne une augmentation importante de l'activité terroriste d'origine islamiste.

Mais il est important de relativiser : l'augmentation du nombre d'attaques terroristes n'est pas forcément due entièrement à l'occupation de l'Irak. Elles ont augmenté depuis que la structure pyramidale du réseau Al-Qaïda a été détruite lors de l'invasion de l'Afghanistan. Les cellules terroristes sont donc maintenant plus autonomes et n'attendent plus les ordres des responsables d'Al-Qaïda.

Ainsi, une crise humanitaire due au manque de nourriture, d'eau potable, de médicaments ou d'autres produits de première nécessité est apparue. En outre, les dommages collatéraux occasionnés à l'infrastructure civile, en particulier les hôpitaux, les routes et les centrales électriques, empêchent le bon fonctionnement de l'économie et des services publics. Enfin, la rébellion, le sabotage des installations et le terrorisme sont particulièrement fréquents.

Pour ces raisons, les États-Unis ont annoncé le qu'ils resteraient plus longtemps que prévu initialement en Irak.

Le , des images de foules en colère à Falloujah ont été diffusées par les médias occidentaux. Elles montrent la foule frappant, traînant à travers la ville puis pendant à un pont les corps de quatre employés américains de la société militaire privée Blackwater, tués par des grenades.

Ces images ont rappelé celles de 1993, à Mogadiscio en Somalie. La révolte de Falloujah a fortement influencé l'opinion publique américain, jusqu'alors globalement favorable à l'intervention en Irak, en la faisant prendre conscience de la violence de ce conflit.

L'invasion et l'occupation de l'Irak ont joué un rôle non négligeable dans la vague d'attentats du 7 juillet 2005 à Londres. Elle a non seulement provoqué l'apparition d'une nouvelle génération de volontaires pour le terrorisme islamiste, mais il apparaît également que les explosifs utilisés lors des attentats de Londres proviennent d'Irak.

La décision prise par les États-Unis d'envahir l'Irak sans l'aval du Conseil de sécurité des Nations unies, constitue, selon la diplomatie européenne, un recul du droit international. D'après les représentants de certains pays européens, comme la France, l'Allemagne ou la Russie, l'invasion de l'Irak en dehors de tout mandat délivré par l'ONU constitue un acte d'agression pur et simple, et non une libération ou un acte d'auto-défense. De plus, d'après ces mêmes pays, les États-Unis donnent le mauvais exemple en envahissant un État sans gestion multilatérale, gestion qui a prévalu dans les années 1990.

De nombreux journaux et ONG déplorent un recul des droits de l'homme à l'occasion de la guerre d'Irak. La situation provoquée par l'invasion de l'Irak empêche l'exercice de la liberté de la presse en Irak ; de plus, le gouvernement irakien en est même venu à expulser certains journalistes, et à interdire à d'autres de venir en Irak.

À la fin de la période de guerre conventionnelle, l'Irak a connu une courte période de désordre totale. Les troupes d'occupation ne sont guére intervenues, ni pour empêcher les destructions des souvenirs liés au régime de Saddam Hussein, ni pour empêcher les pillages des richesses du pays : banques, musées, etc. 

Suite aux différents affrontements et actes de terrorisme après 2003, le désordre a connu un pic lors des violences confessionnelles de 2006-2007. Mais les attentats demeurent quotidiens en 2011, notamment à Bagdad, Diyala et dans certaines zones du Nord, disputées entre le gouvernement central et celui de la région autonome du Kurdistan.

L'invasion de l'Irak et les menaces des États-Unis vis-à-vis de l'Iran, qui cherche à acquérir l'arme atomique, ont en partie provoqué la défaite des réformistes à l’élection présidentielle de 2005 en Iran. Celle-ci a cependant d'autres raisons :

Le prix Nobel de la Paix, Shirin Ebadi, dénonce également la croisade intégriste de Georges Bush, qui aggrave la situation des réformistes et des femmes dans son pays, l'Iran, et dans le reste du monde musulman.

L'attitude menaçante des États-Unis aurait accéléré un raffermissement du régime syrien, accusé de soutenir le Hamas et de fermer les yeux sur l'infiltration de terroristes en Irak, après une brève et timide détente à la mort deHafez el-Assad.

La Syrie a cependant été contrainte d'évacuer le Liban après l'assassinat de Rafic Hariri, dirigeant libanais. Dans un contexte trouble, ce dernier essayant de se débarrasser de la Syrie pourrait avoir été assassiné par le gouvernement de ce dernier, ce que tant à démontrer l'enquête menée sous l'égide de l'ONU.

Courant novembre 2005, le président Syrien a fait une allocution publique retransmise sur différentes chaînes à travers le monde, dont la chaîne européenne privée Euronews. Il rappelle l'histoire coloniale de la Syrie, pays issu de la division par les colons français et britanniques de la Syrie et du Liban pour les français, et de l'Irak et du Koweït pour les britanniques. Et la pression constante qu'ils ont exercé sur ce pays depuis qu'il a réussi à obtenir son indépendance. Prônant une vision plus équitable des relations internationales, et un plus grand respect du monde arabe dans ses diversités culturelles.

Les États-Unis tentent de pousser les régimes du Moyen-Orient, d'Asie centrale et d'Afrique du Nord à plus de souplesse et de démocratie (premières élections municipales en Arabie saoudite, droit de vote pour les femmes au Koweït, entrebâillement de l’élection présidentielle en Égypte, soutien de l'opposition libanaise contre la présence syrienne, coopération économique renforcée avec le continent africain, dégel des relations avec la Jamahiriya arabe libyenne).

La situation sanitaire du pays est désastreuse. Bien que déjà très peu satisfaisante avant l'invasion, du fait d'une décennie de blocus, elle a incontestablement été aggravée par la guerre : stations de pompage et d'épurations détruites ou inactives du fait des coupures de courant, hôpitaux ayant subi le même sort, manque de nourriture.

Les gouvernements de la Coalition estimaient avant le début de cette guerre le coût des opérations militaires et de la reconstruction entre 100 et 200 milliards de $. Les évaluations les plus récentes sont bien plus élevées.

Coûts totaux de la guerre en Irak entre 2003 et 2005


Pour le Royaume-Uni, le coût financier de l'opération militaire est de 847 millions de livres (1 milliard et 176 millions d'euros) depuis 2003.

Coûts prévisibles pour la période 2005-2015

Il ne s'agit que d'une projection à long terme si l'engagement en 2005 se maintenait :


Coûts « évités » par l'intervention

Coût auquel il faut soustraire les économies générées par ces opérations (arrêt de l'application des résolutions de l'ONU, relance du commerce international)


"sources : Scott Wallsten et Katrina Kosec de l'AEI-Brookings Joint Center for Regulary Studies"

L'année 2005 a vu un revirement de l'opinion publique dans les démocraties qui ont mené la guerre ; ce qui conduit notamment aux États-Unis à une reprise en main du Sénat qui somme le président de fournir des perspectives claires concernant la suite à donner aux opérations sur le terrain de la lutte contre le terrorisme, ainsi que de donner un calendrier de retrait des troupes.

En 2004, le Programme des Nations unies pour l'environnement lance un programme de restauration de l'environnement concernant les marais de Mésopotamie détruits à 90 % par l'ancien régime ; en 2006, environ 10 000 km² ont été restaurés et 100 000 personnes sont revenues habiter la zone.




</doc>
<doc id="17847" url="https://fr.wikipedia.org/wiki?curid=17847" title="Pipa">
Pipa

Pipa peut faire référence à :

PIPA est un sigle pouvant faire référence à :


</doc>
<doc id="17851" url="https://fr.wikipedia.org/wiki?curid=17851" title="Planche à laver">
Planche à laver

La planche à laver (en anglais : "washboard") est un instrument de musique frotté apparu à La Nouvelle-Orléans. C'est un ustensile sanitaire détourné de sa fonction originelle et adapté à un jeu musical par le détournement d'autres objets usuels, tels des dés à coudre.

À l'origine, il s'agissait d'une vraie planche à laver et à battre le linge ; aujourd'hui il en existe des versions modernes, spécialement adaptées à la musique. Elle est composée d'une section plane en aluminium, en inox ou en bois, avec des cannelures, sur laquelle se fixent parfois des mini-cymbales, des cloches ou d'autres petits idiophones.

Cet instrument de musique se porte également comme un plastron et se joue directement sur soi. Il prend alors le nom de "vest-frottoir" ou frottoir en cajun louisianais.

Il se joue en le plaçant contre sa poitrine et en frottant ou tapant dessus avec des dés à coudre placés sur chaque doigt. C'est l'instrument de percussion de prédilection de la musique cadienne. De même, dans le jazz Nouvelle-Orléans ou le blues, les premiers batteurs les utilisaient pour la partie rythmique.

Cet instrument a été utilisé dans les années 1950 dans un style de musique anglo-saxonne, le skiffle, mélange de jazz, folk et blues. Avant les Beatles, le premier groupe de John Lennon, les Quarrymen, jouait du skiffle et Pete Shotton tenait la planche à laver.

Aujourd'hui, nombre d'orchestres de ce type apprécient le côté pratique et portatif de cet instrument et les sons particuliers qu'il produit entre des mains expertes (Cf. Washboard Sam, Gilbert Leroux, Michel Cousin...) Ce sont souvent les musiciens qui fabriquent eux-mêmes leurs instruments selon leurs besoins. Certains musiciens des pays de l'Est pratiquent aussi ce genre d'instrument. En France Mathieu Péquériau, aussi harmoniciste, l'utilise dans Red Cardell. 

En Belgique, Luc Brughmans l'utilise dans l'orchestre « La planche à jazz ».Il. en joue horizontalement, avec fixation sur un harnais de caisse claire.

Chez les washboardistes actuels, outre les « puristes » qui préconisent d'être le plus proche possible de l'origine, une simple planche avec le moins possible d'accessoires, on rencontre aussi des «créatifs» qui ajoutent nombre d'accessoires, voire une caisse claire. Ils ne jouent plus seulement avec les doigts et des dés à coudre mais aussi avec des baguettes, et s'approchent de plus en plus de la batterie.


</doc>
<doc id="17853" url="https://fr.wikipedia.org/wiki?curid=17853" title="Diode électroluminescente">
Diode électroluminescente

Une diode électroluminescente (abrégé en DEL en français, ou LED, de l'), est un dispositif opto-électronique capable d’émettre de la lumière lorsqu’il est parcouru par un courant électrique. Une diode électroluminescente ne laisse passer le courant électrique que dans un seul sens (le sens passant, comme une diode classique, l'inverse étant le sens bloquant) et produit un rayonnement monochromatique ou polychromatique non cohérent à partir de la conversion d’énergie électrique lorsqu'un courant la traverse.

Elle compte plusieurs dérivées, principalement, l'OLED, l'AMOLED ou le FOLED (pour flexible oled). En raison de leur rendement lumineux, les LED pourraient représenter 75 % du marché de l'éclairage domestique et automobile avant 2020. Elles sont aussi utilisées dans la construction des écrans plats de télévision : pour le rétroéclairage des écrans à cristaux liquides ou comme source d'illumination principale dans les télévisions à OLED.

Les premières LED à être commercialisées ont produit de la lumière infrarouge, rouge, verte puis jaune. L'arrivée de la LED bleue, associée aux progrès techniques et d'assemblage permet de couvrir . De nombreux appareils sont munis de LED composites (trois LED réunies en un composant : rouge, vert et bleu) permettant d'afficher de très nombreuses couleurs.

La première émission de lumière par un semi-conducteur date de 1907 et fut découverte par . En 1927, dépose le premier brevet de ce qui sera appelé, bien plus tard, une diode électroluminescente. 

En 1955, Rubin Braunstein découvre l'émission infrarouge de l'arséniure de gallium, semi-conducteur qui sera ensuite utilisé par Nick Holonyak Jr. et pour créer la première LED rouge en 1962. Durant quelques années, les chercheurs se sont limités à quelques couleurs telles que le rouge (1962), le jaune, le vert et plus tard le bleu (1972) . 

Dans les années 1990, les recherches, entre autres, de Shuji Nakamura et Takashi Mukai de Nichia, dans la technologie des semi-conducteurs InGaN permirent la création de LED bleues de forte luminosité, ensuite adaptées en LED blanches, par adjonction d'un luminophore jaune. Cette avancée permit de nouvelles applications majeures telles que l'éclairage et le rétroéclairage des écrans de téléviseurs et des écrans à cristaux liquides. Le , Shuji Nakamura, Isamu Akasaki et Hiroshi Amano ont reçu le prix Nobel de physique pour leurs travaux sur les LED bleues.


L'intérêt des lampes à LED en termes de consommation électrique, de durée de vie et de sécurité électrique s'est rapidement confirmé pour l’automobile (dans l'habitacle et pour les phares et clignotants où les LED se montrent parfois plus performantes que les sources xénon ou halogène), l'éclairage urbain, l'éclairage d'infrastructures, les usages dans la marine et l’aéronautique. Cet intérêt a au début des années 2000 dopé le marché, qui a dépassé en 2010 le seuil des de dollars américains (USD), soutenu par une croissance annuelle globale de 13,6 % de 2001 à 2012, et devrait atteindre avant la fin 2015. Dans ce marché la part de l’éclairage augmente régulièrement de 2008 à 2014 et devrait se stabiliser en 2018, alors que la part du rétro-éclairage devrait décroître dès 2014 en raison d'évolutions techniques.

La part destinée à l'automobile semble dans les années 2010-2015 stable (environ 10 % du marché global et pourrait le rester jusqu'à 2020. Les LED ont d'abord équipé des véhicules de luxe (Audi, Mercedes) puis de moyenne gamme (Seat Léon, Volkswagen Polo en 2014).

En 2016, les principaux fabricants, leaders sur ce marché sont Nichia et Toyoda Gosei au Japon, notamment pour les LED GaN de « forte » puissance, plus de ), Philips Lumileds Lighting Company et OSRAM Opto Semiconductors GmbH en Europe, Cree et General Electric aux États-Unis. Samsung Electronics et produisent des LED pour l'automobile.

La recombinaison d'un électron et d'un trou d'électron dans un semi-conducteur conduit à l'émission d'un photon. En effet, la transition d'un électron entre la bande de conduction et la bande de valence peut se faire avec la conservation du vecteur d’onde formula_1. Elle est alors radiative (émissive) c'est-à-dire accompagnée de l’émission d’un photon. Dans une transition émissive, l'énergie du photon créé est donnée par la différence des niveaux d’énergie avant (E) et après (E) la transition :

Une diode électroluminescente est une jonction P-N qui doit être polarisée en sens direct lorsqu’on veut émettre de la lumière. Le potentiel imposé aux bornes doit être supérieur à celui imposé par la jonction P-N. La plupart des recombinaisons sont radiatives. La face émettrice de la LED est la zone P car c'est la plus radiative.

La longueur d’onde du rayonnement émis dépend de la largeur de la « bande interdite » et donc du matériau utilisé. Toutes les valeurs du spectre lumineux peuvent être atteintes avec les matériaux actuels. L’infrarouge est obtenu grâce à l’arséniure de gallium (GaAs) dopé au silicium (Si) ou au zinc (Zn). Les fabricants proposent de nombreux types de diodes aux spécificités différentes. Les diodes à l’arséniure de gallium sont les plus économiques et les plus utilisées. Les diodes à l’arséniure de gallium-aluminium (AlGaAs) offrent une plus grande puissance de sortie mais nécessitent une tension directe plus élevée et ont une longueur d’onde plus courte (, ce qui correspond au maximum de sensibilité des détecteurs au silicium) ; elles présentent une bonne linéarité jusqu’à . Enfin, les diodes à double hétérojonction (DH) AlGaAs offrent les avantages des deux techniques précédentes (faible tension directe) en ayant des temps de commutation très courts (durée nécessaire pour qu’un courant croisse de 10 % à 90 % de sa valeur finale ou pour décroître de 90 % à 10 %), permettant des débits de données très élevés dans les transmissions de données numériques par fibres optiques. Les temps de commutation dépendent de la capacité de la jonction dans la diode.

L'efficacité lumineuse varie selon le type de diodes, de , et atteignant en laboratoire les . Une grande disparité de performances existe selon la couleur (température de couleur pour le blanc), la puissance ou encore la marque.
Les bleues n’excèdent pas alors que les vertes ont une efficacité lumineuse atteignant .

La limite théorique d’une source qui transformerait intégralement toute l’énergie électrique en lumière visible est de , mais il faudrait qu’elle possède un spectre monochromatique de longueur d’onde .L'efficacité lumineuse théorique d’une LED blanche est de l’ordre de . Ce chiffre est inférieur à du fait que le maximum de sensibilité de l’œil se situe vers .

L'efficacité lumineuse des LED blanches de dernière génération est supérieure à celle des lampes à incandescence mais aussi à celui des lampes fluocompactes ou encore de certains modèles de lampes à décharge. Le spectre de la lumière émise est presque intégralement contenu dans le domaine du visible (les longueurs d’onde sont comprises entre et ). Contrairement aux lampes à incandescence et aux lampes à décharge, les diodes électroluminescentes n’émettent quasiment pas d’infrarouge, sauf celles fabriquées spécifiquement dans ce but.

L'efficacité lumineuse dépend de la conception de la LED. Pour sortir du dispositif (semi-conducteur puis enveloppe externe en époxy), les photons doivent traverser (sans être absorbés) le semi-conducteur, de la jonction jusqu’à la surface, puis traverser la surface du semi-conducteur sans subir de réflexion et, notamment, ne pas subir la réflexion totale interne qui représente la grosse majorité des cas. Une fois arrivé dans l’enveloppe externe en résine époxy (quelquefois teintée pour des raisons pratiques et non pour des raisons optiques), la lumière traverse les interfaces vers l’air à incidence proche de la normale ainsi que le permet la forme de dôme avec un diamètre bien plus grand que la puce ( au lieu de ). Dans les diodes électroluminescentes de dernière génération, notamment pour l’éclairage, ce dôme plastique fait l’objet d’une attention particulière car les puces sont plutôt millimétriques dans ce cas et le diagramme d’émission doit être de bonne qualité. À l’inverse, pour des gadgets, on trouve des LED quasiment sans dômes.

Aux fortes intensités, l'efficacité lumineuse des LED chute. Il a été suspecté en 2007-2008, mieux compris en 2010-2011 puis confirmé début 2013 que cette diminution est attribuable à un « effet Auger » qui dissipe une partie de l'énergie sous forme de chaleur. Des projets de recherche visent à limiter ou contrôler cet effet.

Ce bilan est discuté, car le développement massif des LED pourrait augmenter les tensions sur le marché sur certaines ressources non renouvelables (terres rares ou métaux précieux) et parce que la conversion des éclairages urbains aux LED semble souvent susciter une augmentation de l'illumination globale du ciel nocturne et donc de la pollution lumineuse visible de l'espace.

D'un autre côté les LED ont un fort potentiel d'économie en énergie, si leur utilisation est raisonnée pour éviter le risque d'effet rebond.

Des préoccupations sérieuses existent concernant les impacts sanitaires de lampes mal utilisées, et surtout via leurs effets de pollution lumineuse. 
Ainsi, selon une étude publiée en 2014 dans la revue "Ecological Applications", alors que l’éclairage nocturne municipal et industriel a déjà changé la répartition des différentes espèces d'invertébrés autour des sources lumineuses et semble contribuer à la régression ou la disparition de nombreuses espèces de papillons, l'éclairage public tend à utiliser à grande échelle les diodes électroluminescentes (LED) ; la question de l’impact des spectres lumineux des lampes prend donc de l’importance. Ces spectres lumineux ont récemment beaucoup changé, et ils changeront encore avec le développement des LED. Or, il apparaît que le spectre lumineux émis par les LED mises sur le marché dans les années 2000-2014 attire les papillons de nuit et certains autres insectes plus que la lumière jaune des ampoules à vapeur de sodium, en raison d’une sensibilité élevée de ces invertébrés nocturnes aux parts vert-bleue et UV du spectre. Des pièges lumineux à insectes volants équipés de LED capturent 48 % plus d'insectes que les mêmes pièges utilisant des lampes à vapeur de sodium, avec un effet également lié à la température de l’air (les invertébrés sont des animaux à sang froid, naturellement plus actif quand la température s’élève). Lors de cette étude plus de ont été capturés et identifiés : les espèces les plus fréquemment piégées étaient des papillons et des mouches.

Ces lampes sont froides et ne brûleront pas les insectes comme pouvaient le faire des lampes halogènes, mais le caractère très attractif des LED pour de nombreux invertébrés peut leur être fatal ; leur vol est perturbé et dans la zone d'attraction ils sont mis en situation de « piège écologique », car largement surexposés à des prédateurs de type araignées et chauve-souris, avec de possibles effets écologiques plus globaux si ces lampes étaient utilisées à grande échelle (perturbation des réseaux trophiques et possible renforcement des infestations de certaines cultures ou sylvicultures par des « ravageurs phytosanitaires » attirés par ces lampes, tels que le Bombyx disparate qui est source de dégâts importants depuis qu’il a été introduit aux États-Unis et qui se montre très attiré par la lumière (les auteurs pointent les ports où un éclairage LED pourrait directement attirer des ravageurs ou des espèces exotiques envahissantes accidentellement apportées par des bateaux). Ces espèces anormalement favorisées pouvant à leur tour mettre en péril des espèces natives rares ou menacées. 

L’étude de 2014 n’a pas pu conclure que manipuler la température de la couleur des LED diminuaient leur impact ; mais les auteurs estiment qu'utiliser des filtres ou une combinaison de LED rouges, vertes, et bleues pourraient peut-être diminuer cette fatale attraction, mais alors avec des coûts en termes de consommation électrique et d’énergie ou de terres rares. Ils concluent qu’il existe un besoin urgent de recherche collaborative entre écologues et ingénieurs de la lumière pour minimiser les conséquences potentiellement négatives des développements futurs de la technologie LED. L'écoconception en amont des LED pourrait faciliter le recyclage des lampes usagées et en aval le ré-usage de LED d'objets désuets ou en fin de vie. De même des systèmes intelligents d'asservissement de l'éclairage aux besoins réels sont possibles (lampes équipées de filtres limitant les émissions dans le bleu-vert et le proche-UV, mieux bafflées c'est-à-dire produisant moins de halo et moins éblouissantes, ne s'allumant qu'à l'intensité nécessaire et uniquement quand on en a besoin, via un processus d"'éclairage intelligent" incluant la détection de présence et de luminosité ambiante, si possible intégré dans un smartgrid ou un système écodomotique plus global. En 2014, quatre villes dont Bordeaux, Riga en Lettonie, Piaseczno en Pologne et Aveiro au Portugal testent ce type de solution dans le cadre du programme européen « LITES » (à l'installation ces systèmes sont 60 % plus cher, mais ce surcoût doit être rapidement récupéré par les économies d'électricité et l'amélioration de la qualité de l'environnement nocturne.

Ce composant peut être encapsulé dans divers boitiers destinés à canaliser le flux de lumière émis de façon précise : cylindrique à bout arrondi en 3, 5, 8 et de diamètre, cylindrique à bout plat, ou de forme plate (LED SMD), rectangulaire, sur support coudé, en technologie traversante ou à monter en surface (CMS).

Les LED de puissance ont des formes plus homogènes : la luxeon ci-contre est assez représentative. Ces types de LED sont également disponibles en version « multicœur », « multipuces » ou « multichips » en anglais, dont la partie émissive est composée de plusieurs puces semi-conductrices.

L'enveloppe transparente, ou "capot", est généralement en résine époxy, parfois colorée ou recouverte de colorant.

L’intensité lumineuse générale des diodes électroluminescentes est assez faible, mais suffisante pour la signalisation sur tableau, ou bien les feux de circulation (feux tricolores, passages piétons). Les bleues sont également suffisamment puissantes pour signaliser les bords de route, la nuit, aux abords des villes. Le bâtiment du NASDAQ, à New York possède une façade lumineuse animée entièrement réalisée en LED (quelques millions).

Les LED de puissance sont aussi utilisées dans la signalisation maritime comme sur les bouées permanentes. Deux de ces diodes sont situées l’une par-dessus l’autre et suffisent à un éclairement important et visible par les bateaux de nuit.
Des LED de forte puissance ont vu le jour au début des années 2000. Dans la première décennie du , des rendements lumineux d'environ . Par comparaison, les ampoules à filament de tungstène de atteignent un rendement lumineux d'environ .

Les LED sont, en 2014, suffisamment puissantes pour servir d'éclairage dans le secteur de l'automobile. Employées d'abord pour les feux de stop, clignotants ou de recul, celles-ci remplaceront probablement, à terme, toutes les ampoules classiques.

La couleur d’une diode électroluminescente peut être générée de différentes manières :

Voici quelques colorations en fonction du semi-conducteur utilisé :

Pour le blanc, on ne parle pas de longueur d’onde mais de température de couleur proximale. Celle des diodes électroluminescentes est assez variable en fonction du modèle.

Les diodes électroluminescentes sont polarisées : on raccordera le pôle « - » à la cathode « - » et donc le pôle « + » à l'anode « + ». Les diodes ont généralement trois détrompeurs : la cathode est plus courte, l'électrode à l'intérieur du dôme est plus grosse et le bord extérieur du dôme est plat. Inversement, l'anode est plus longue, l'électrode à l'intérieur du dôme est plus petite et le bord extérieur du dôme est arrondi (dessins en haut de page).

Il est indispensable de ne pas dépasser l’intensité admissible (typiquement : 10 à pour une LED de faible puissance et de l'ordre de 350 à pour une LED de forte puissance) et donc d’intercaler une résistance en série ou d'utiliser une limitation en courant. Utiliser les données du fabricant pour calculer la résistance en fonction de cette intensité désirée I, de la tension d’alimentation, de la tension directe de la LED et du nombre n de LED en série (loi d'Ohm : R = (V - n × V) / I). Une méthode peu dispendieuse en énergie consiste à utiliser un circuit de régulation de courant basé sur des principes analogues à ceux mis en œuvre dans les alimentations électriques à découpage. Pour les applications d’éclairage, on pourra regrouper plusieurs diodes dans un schéma série-parallèle : il faudra dans ce cas tenir compte de la chute de tension provoquée par les diodes en série pour calculer la résistance en série : plus il y aura de diodes en série, plus forte sera la chute de tension ; ce qui permettra de diminuer la résistance en série et donc d’augmenter le rendement du dispositif. Le courant maximal admissible sera, quant à lui, multiplié par le nombre de groupes de diodes en parallèle.

Il est également primordial d'apporter un soin particulier à l'alimentation électrique des LED pour conserver leurs caractéristiques colorimétriques (température de couleur proximale, IRC…).



En , le Laboratoire d'électronique et de technologie de l'information (LETI) et son voisin, l'Institut des nanosciences et cryogénie (INAC) ont reçu le prix EARTO ' () dans la catégorie « Impact attendu » pour leur mise au point d'une diode électroluminescente quatre fois moins chère à produire et produisant trois fois plus de lumière.

Il existe plusieurs manières de classer les diodes semi-électroluminescentes :

La première est un classement par puissance :

Une autre manière de les classer est de considérer la répartition de l'énergie dans la gamme de longueur d'onde couvrant le visible (longueurs d'ondes de l'ordre de 380 à ) ou l'invisible (principalement l'infrarouge). La raison de la distinction réside dans le fait que certaines diodes peuvent servir à éclairer, ce qui est l’une des applications phares du futur (proche) :

D'autres classements sont possibles, par exemple selon le caractère monopuce ou multipuce, la durée de vie, la consommation d'énergie ou encore la robustesse en cas de sollicitations sous contraintes (comme pour certains matériels industriels, militaires, spatiaux…)




L’amélioration du rendement des LED permet de les employer en remplacement de lampes à incandescence ou fluorescence, à condition de les monter en nombre suffisant :

En 2006, le groupe américain Graffiti Research Lab a lancé un mouvement nommé "Led throwies" (lancer de LED) qui consiste à égayer les lieux publics en ajoutant de la couleur sur des surfaces magnétiques. Pour ceci, on combine une LED, une pile au lithium et un aimant, et on lance l’ensemble sur une surface magnétique.

En 2007, Audi et Lexus bénéficient de dérogations de la Commission européenne pour commercialiser des modèles munis de feux avant à base de LED. En 2009, la Ferrari 458 Italia innove elle aussi avec des phares à LED.

Plusieurs villes remplacent leur éclairage public par des LED dans le but de diminuer leur facture d’électricité et la pollution lumineuse du ciel (éclairage dirigé vers le bas). Le recours aux LED est aussi courant dans les feux tricolores. L’exemple de Grenoble est le plus souvent cité : la ville a réalisé son retour sur investissement en trois ans seulement. En effet, les LED permettent des économies d’énergie, mais ce sont surtout les coûts de maintenance qui baissent, du fait de leur robustesse.

En 2010, La RATP expérimente l'éclairage des espaces du métro parisien, notamment à la station Censier-Daubenton première station de métro entièrement éclairée par cette technologie. En 2012 estimant le produit mature la RATP (réseau de transport en commun de Paris) décide de modifier la totalité de son éclairage vers la technologie LED. C'est plus de qui seront modifiés, faisant ainsi du métro parisien le premier réseau de transport en commun d'envergure à adopter le « tout LED ».

Les LED sont utilisées pour réaliser des écrans vidéo de très grande taille (plateaux TV salon dans des grands halls, stade…).

Le rétroéclairage de l’écran par des diodes électroluminescentes permet de fabriquer des écrans plus fins, plus lumineux, ayant une étendue colorimétrique plus importante et plus économes que son prédécesseur ACL à rétroéclairage par tube fluorescent (technologie CCFL). À noter que les constructeurs restent assez flous sur le fait que les LED dégagent plus de chaleur.




</doc>
<doc id="17854" url="https://fr.wikipedia.org/wiki?curid=17854" title="École nationale supérieure d'informatique pour l'industrie et l'entreprise">
École nationale supérieure d'informatique pour l'industrie et l'entreprise

L'École nationale supérieure d'informatique pour l'industrie et l'entreprise (ENSIIE, anciennement Institut d'informatique d'entreprise, IIE) est l'une des françaises accréditées au à délivrer un diplôme d'ingénieur.

Créée en 1968, elle est située à Évry dans l'Essonne. Elle dépend du Ministère de l'Enseignement supérieur et de la Recherche. Son recrutement se fait sur le Concours Mines-Télécom depuis 2016. Elle est membre de la Conférence des grandes écoles et membre associé de l'Université Paris-Saclay.

L'ENSIIE est une des plus anciennes écoles d'informatique en France. Elle dispense un enseignement technique et scientifique de autour du triptyque informatique- mathématiques - organisation des entreprises et finance, une proximité avec le monde de l'entreprise.

L'ENSIIE propose, en plus de sa formation initiale, une formation continue d'ingénieurs (formation d'ingénieur en partenariat) FIP en deux années par alternance, ouverte aux salariés ayant au moins trois ans d'expérience en informatique et disposant d'un diplôme bac+2. L'ENSIIE propose également une formation en alternance d'ingénieur.

Créé en 1968 au sein du Conservatoire national des arts et métiers, l'Institut d'informatique d'entreprise est tout d'abord installé à Paris, dans les locaux du CNAM. Les promotions sont très réduites au départ, constituées d'une trentaine d'élèves au plus, mais ne tarderont pas à s'enrichir. Quatre années plus tard, en 1972, l'IIE est habilité, après avis de la Commission des titres d'ingénieurs, à délivrer le diplôme "d'ingénieur de l'Institut d'Informatique d'Entreprise". En 1974, l'école rejoint le concours Centrale–Supélec, qui servait encore de base principale à son recrutement jusqu'en 2014. En 1984, l'IIE est déplacé dans la ville nouvelle d'Évry, et hérite de locaux plus vastes, permettant d'étendre les promotions à plus de 140 élèves aujourd'hui. En 2006, des travaux importants ont été réalisés, afin de rénover et d'agrandir les locaux de l'école.

En juillet 2006, l’école devient l’École nationale supérieure d'informatique pour l'industrie et l'entreprise et devient un établissement public à caractère administratif rattaché à l’Université d'Évry-Val-d'Essonne (c’est le même statut que les anciennes ENSI).

Entre 2009 et 2017, elle a disposé d'une antenne à Strasbourg, en Alsace. Elle comptait 600 élèves mais a été fermée faute de financement. 

Le 13 septembre 2011, l'ENSIIE rejoint le réseau des écoles associées de l'Institut Mines-Télécom. Une fusion avec Télécom SudParis, est envisagée finalement l’école s’« associe » à l'Université Paris-Saclay et à l'Institut Mines-Télécom.

En 2015, l'école rejoint le Concours INT Télécom, devenu Concours Mines-Télécom en 2016.

Les élèves admis à l'ENSIIE peuvent être issus :

Le recrutement en deuxième année se fait sur admission sur titre, pour les titulaires d'une maîtrise, d'un Master1, Bachelor à dominante informatique ou mathématiques ou d'un diplôme équivalent.

La scolarité à l'ENSIIE dure six semestres, à temps plein. Elle comporte des phases d'enseignement et d'importantes phases de stages en entreprise.

Les et , les étudiants suivent tous les mêmes enseignements, articulés en quatre grands modules :

Ces enseignements ont pour objectif de fournir aux étudiants les bases scientifiques et les outils fondamentaux pour leur futur métier d'ingénieur. Ces bases leur permettront de s'adapter ou de suivre les évolutions rapides du monde informatique.

Les et comprennent un choix d'options options à choisir parmi les catégories suivantes : 

À l'issue des deux premières années à l'ENSIIE, les élèves ont la possibilité de compléter leur cursus au sein de Télécom Ecole de Management pour se voir décerner un double diplôme ingénieur-manager.

Les élèves peuvent ainsi compter plus d'un an de stage durant leur scolarité, à raison d'un par année d'étude, et ce d'une durée minimale de :

Au cours de la deuxième ou de la troisième année d'études, l'ENSIIE propose aux élèves d'effectuer leur scolarité dans une université étrangère, le plus souvent dans le cadre d'une formation en bi-cursus, pouvant mener à l'obtention d'un diplôme étranger, tel qu'un Master of Science (MSc).

La dernière année peut aussi faire l'objet d'une scolarité en bi-cursus : certains élèves peuvent en effet choisir de suivre les cours d'un master, en parallèle de leur scolarité à l'école. La plupart du temps, ce master remplace le choix d'une option du cursus classique.

Une grande partie des enseignants de l'ENSIIE sont des enseignants-chercheurs, et ont donc une mission de recherche à accomplir. Ces enseignants-chercheurs font partie de divers laboratoires en Île-de-France.

Les équipes de recherche associées à l’école travaillent sur les thématiques principales suivantes : Contrôle stochastique en finances, Optimisation combinatoire, Spécification et vérification de programmes, Apprentissage statistique. Ces travaux se font dans des laboratoires associés à l'ENSIIE: le LaMME (Laboratoire de Mathématiques et Modélisation d’Evry) et SAMOVAR (Services réparties, Architectures, Modélisation, Validation, Administration des Réseaux) .

Il existe aussi des relations avec d'autres laboratoires dans lesquels travaillent des enseignants-chercheurs de l'école, par exemple le laboratoire IBISC (Informatique, Biologie Intégrative et Systèmes Complexes de l'Université d'Evry).

Plus de 35 associations se partagent les locaux de l'école, certaines d'entre elles étant reconnues au niveau national.



</doc>
<doc id="17856" url="https://fr.wikipedia.org/wiki?curid=17856" title="Wayne's World">
Wayne's World

Wayne's World ou Le monde selon Wayne au Québec est un film américain réalisé par Penelope Spheeris, sorti en 1992.

Dans la banlieue de Chicago (Aurora, Illinois), Wayne Campbell et Garth Algar animent une émission qu'ils émettent depuis le sous-sol de leurs parents. Benjamin Kane, producteur d'une chaîne de télévision locale, tombe par hasard sur leur programme et les engage, en réalité pour servir les intérêts de marketing de Noah Vanderhoff, propriétaire de bornes d'arcade. Parallèlement à la « professionnalisation » de leur émission, Wayne rencontre Cassandra, bassiste d'un groupe de hard rock, dont il tombe amoureux. Mais il va devoir se confronter à Benjamin, qui semble lui aussi être intéressé par la jeune femme, et avoir ses propres idées quant à l'émission…



Plusieurs acteurs et personnalités font des caméos dans le film :

Le film a connu un important succès commercial, rapportant environ au box-office mondial, dont en Amérique du Nord, pour un budget de , ce qui le classe à la du box-office mondial en 1992. En France, il a réalisé .

Il a reçu un accueil critique favorable, recueillant 85 % de critiques positives, avec une note moyenne de 6,8/10 et sur la base de 46 critiques collectées, sur le site agrégateur de critiques Rotten Tomatoes.







</doc>
<doc id="17860" url="https://fr.wikipedia.org/wiki?curid=17860" title="Satellite naturel">
Satellite naturel

Un satellite naturel est un objet céleste en orbite autour d'une planète ou d'un autre objet plus grand que lui-même qui n'est pas d'origine humaine, par opposition aux satellites artificiels. Ils peuvent être de grosse taille et ressembler à de petites planètes. De tels objets sont également appelés lunes, par analogie avec la Lune, le satellite naturel de la Terre.

Techniquement, le terme pourrait s'appliquer à une planète orbitant une étoile, ou même une étoile orbitant un centre galactique, mais une telle utilisation est rare. En temps normal, il désigne les satellites naturels des planètes, planètes naines et petits corps.

On suppose que les satellites naturels orbitant relativement proches d'une planète sur une orbite prograde se sont formés dans la même région du disque protoplanétaire à l'origine de cette planète. Par opposition, les satellites irréguliers (orbitant généralement sur des orbites distantes, inclinées, excentriques ou rétrogrades) seraient des objets étrangers capturés et éventuellement fragmentés lors de collisions.

Il y a trois causes permettant la création d'un satellite : l'accrétion, la capture et la collision.

Lors de la formation d’une planète, on retrouve des morceaux de roches, des poussières de glace et des gaz tourbillonnants en forme de disque autour. Les bouts de roches s’agglutinent pour former un grumeau, qui sous les chocs d’autres fragments rocheux crée une sphère de roche qui grossit et absorbe les éventuels grumeaux voisins. Elle finit par dominer le disque et reste seule en orbite, donnant naissance à un satellite.

Un modèle permet d'expliquer que la grande majorité des satellites réguliers du système solaire est formé à partir de l'accrétion d'anneaux planétaires. Au cours du temps, ces anneaux « visqueux » entourant des planètes géantes ou des planètes dites « telluriques » telles que la Terre ou Pluton (la prédiction de ce modèle ne fonctionne pas uniquement pour la répartition des satellites de Jupiter) s'étalent (près de la planète les forces gravitationnelles tendent à faire s'accréter les grains qui les constituent mais les forces de marée les en empêchent) et lorsqu'ils atteignent une certaine distance de la planète (appelée « limite de Roche »), la gravité l'emporte sur l'effet de marée, ils forment ainsi de petits agrégats qui se détachent progressivement (sur des millions d'années) et s'éloignent. Les anneaux donnent ainsi naissance à des satellites en orbite autour de la planète, certains de ces anneaux ayant depuis disparu alors que le processus se poursuit sur Saturne par des cycles de confinement et de déconfinement des anneaux de quelques millions d'années : lorsqu'un gros satellite s'éloigne, par le mécanisme d'action et réaction, il repousse l'anneau sous la limite de Roche (confinement) ; lorsque le satellite est suffisamment éloigné, l'anneau s'étale à nouveau pour redépasser la limite de Roche (déconfinement).

Cas le plus rencontré : il faut que deux astéroïdes (rarement un seul), gravitant l’un près de l’autre s'approchent suffisamment d'une planète pour que son champ gravitationnel ne soit pas négligeable. Dès lors, l'astéroïde le plus proche ou le plus lourd de la planète se retrouve happé dans le champ gravitationnel de cette dernière. Sa trajectoire est alors modifiée par la force de l’attraction de l’astre qui s'ajoute aux autres forces exercées sur celui-ci (inertie, attraction de l'autre astéroïde, etc.). Si l’attirance de l’astre est la plus forte, la liaison entre les deux astéroïdes cède, le jumeau reçoit ainsi une impulsion d’énergie et file dans l’espace tandis que l’autre astéroïde commence son premier tour d’orbite ainsi que sa vie de satellite.

Dernier cas, qui est aussi le plus rare : il se produit lorsqu’un astéroïde de taille monstrueuse heurte une planète. Lors de ce choc titanesque, un panache de matière jaillit de l’impact, contenant roche et même des fragments du noyau de la planète. Cette matière va s’agglomérer, formant un nouveau corps. Mais ce dernier, trop lourd à cause de la masse métallique extraite du noyau de la planète, chute à nouveau et va percuter une seconde fois la planète. Cette fois, la partie arrachée du noyau se fond quasi-totalement avec celui de la planète. Le panache ainsi formé sera donc exempt de particules métalliques, plus lourdes. Ce dernier va tout de même se scinder en deux, une partie retournera à l’astre, l’autre commencera une orbite. Cette matière va s’agglomérer de façon à créer un nouveau satellite. Toute cette séquence peut prendre seulement vingt-quatre heures.

Il existe des exceptions ou des variations à ce modèle standard de formation. En particulier, les couples Terre-Lune et peut-être Pluton-Charon tireraient leur origine de la collision de deux grands objets proto-planétaires. La matière éjectée en orbite autour du corps central aurait alors formé un ou plusieurs objets par accrétion. On pense par ailleurs que les satellites d'astéroïdes se forment principalement par ce processus.

Le terme de « satellite » ne possède pas de définition scientifique précise. En particulier, l'existence de couples Pluton-Charon et Terre-Lune, où le rapport des masses entre le corps central et son satellite n'est pas aussi prononcé que dans la plupart des autres systèmes, rend difficile la détermination d'une limite séparant un système satellitaire d'une planète double. Une définition commune suppose qu'un système satellitaire doit posséder un barycentre situé sous la surface du corps le plus grand mais elle n'est pas officielle et reste arbitraire.

À l'autre bout de l'échelle, les systèmes annulaires autour des géantes gazeuses du Système solaire sont composés de petits morceaux de glace et de roche ; il n'existe aucune limite définissant une taille à partir de laquelle un tel morceau est suffisamment grand pour être considéré comme un satellite à part entière.

Le premier satellite naturel connu était la Lune. Jusqu'à la découverte des satellites galiléens en 1610, aucune occasion ne s'était donc présentée pour caractériser de tels objets. Galilée choisit pour sa part le terme grec latinisé "stellae planetæ" (« étoiles errantes », par opposition aux étoiles fixes) pour les désigner. C'est Képler qui les nommera « satellites » en 1611, du latin "satelles" signifiant « gardien » ou « compagnon », le satellite semblant accompagner la planète dans ses déplacements.

Christian Huygens, le découvreur de Titan, fut le premier à utiliser le terme « lune » pour ce type d'objet, appelant Titan "Luna Saturni" ou "Luna Saturnia" (« la lune de Saturne » ou « la lune saturnienne »).

Au fil des découvertes, le terme fut abandonné ; Jean-Dominique Cassini utilisait parfois le terme de « planètes » pour ses découvertes, mais plus souvent celui de « satellites ».

Le terme de « satellite » devint la norme pour décrire un objet en orbite autour d'une planète, permettant d'éviter l'ambiguïté de « lune ». Cependant, en 1957, le lancement de Spoutnik 1, le premier objet artificiel en orbite autour de la Terre, rendit nécessaire la distinction entre les satellites artificiels et les satellites naturels. Le terme simple de « satellite » tendit à désigner principalement les objets artificiels et le terme « lune » fut à nouveau souvent employé. Par contre, on appelle toujours « satellite » les nouveaux corps détectés autour d’objets eux-mêmes en orbite autour du Soleil (leur désignation provisoire est « S » suivi d'un numéro, comme S/2004 N 1).

À la mi-janvier 2014, on connaît plus de 600 satellites naturels dans le Système solaire dont au moins 220 dont l'existence est confirmée. 173 satellites confirmés orbitent autour des planètes du Système solaire : Jupiter en compte 67, Saturne 62 (plus environ 150 lunes mineures), Uranus 27, Neptune 14, Mars 2 et la Terre 1. On connaît 9 lunes orbitant autour de planètes naines : 5 pour Pluton, 2 pour Hauméa, 1 pour Makémaké et 1 pour Éris. Environ 200 autres (dont au moins une quarantaine ayant une désignation provisoire ou définitive) ont été découvertes autour d'astéroïdes et autres petits corps. estiment que 15 % des objets transneptuniens possèdent au moins un satellite.

Les lunes du Système solaire d'un diamètre supérieur à sont la Lune (satellite de la Terre), les lunes galiléennes de Jupiter (Io, Europe, Ganymède et Callisto), Titan (lune de Saturne) et Triton (lune de Neptune). Toutes ces lunes sont plus grandes que Pluton. Ganymède et Titan sont plus grandes que Mercure, la plus petite des planètes du Système solaire.

Les géantes gazeuses possèdent des systèmes entiers de satellites naturels, dont plusieurs ont une taille comparable à la Lune. Parmi les planètes internes, Mercure et Vénus n'ont aucun satellite, la Terre en possède un unique de grande taille (la Lune) et Mars deux lunes minuscules (Phobos et Déimos). Parmi les planètes naines, Cérès n'en possède pas (au contraire de nombreux autres objets de la ceinture d'astéroïdes) ; Éris en possède un (Dysnomie) ; Makémaké en possède également un (S/2015 (136472) 1) ; Hauméa deux (Hi'iaka et Namaka) ; Pluton cinq (Nix, Hydre, Charon, Kerbéros et Styx). Les candidats parmi d'autres au statut de planète naine Orcus et Quaoar en possèdent aussi chacun un.

La plupart des satellites naturels proches sont en rotation synchrone avec le corps autour duquel il tourne, ce qui signifie qu'ils tournent sur eux-mêmes en autant de temps qu'ils effectuent une révolution complète autour de la planète, et présentent ainsi toujours la même face vers la planète (c'est le cas par exemple de la Lune). Parmi les exceptions, Hypérion, une lune de Saturne, tourne de façon chaotique à cause de plusieurs influences extérieures.

En revanche, les satellites extérieurs des géantes gazeuses en sont trop éloignés pour être en rotation synchrone. Par exemple, Himalia (lune de Jupiter), Phœbé (lune de Saturne) et Néréide (lune de Neptune) ont une période de rotation de 10 h et une période orbitale de centaines de jours.

On ne connaît aucun satellite naturel d'un autre satellite naturel. On ne sait pas si de tels objets sont stables à long terme. Dans la plupart des cas, les effets de marée causés par la primaire rendraient un tel système instable.
En théorie, un satellite secondaire pourrait exister à l'intérieur de la sphère de Hill d'un satellite primaire, mais aucun objet de ce type n'a encore été détecté.
Des recherches ont été effectuées pour trouver un satellite de la Lune, sans succès.

Mais même si aucun satellite naturel n'a été découvert autour d'un autre satellite des objets suivant une trajectoire de quasi-satellite peuvent orbiter temporairement autour d'un satellite. C'est ce qu'a réalisé l'Union soviétique avec leur sonde Phobos 2 en 1989 autour du satellite martien du même nom.

Si le projet se concrétise, la NASA prévoit de mettre un astéroïde de taille moyenne en orbite autour de la Lune et pourrait être le premier satellite d'un satellite connu.

Deux lunes possèdent des petits compagnons à leur point de Lagrange L et L, appelés lunes co-orbitales par analogie avec les astéroïdes troyens de Jupiter :

Une lune astéroïdale est un astéroïde en orbite autour d'un autre astéroïde. On considère généralement qu'ils sont formés des débris résultant d'un impact impliquant l'astéroïde primaire. D'autres systèmes pourraient avoir été formés par de petits objets capturés par la gravité d'un corps plus grand. Au début des années 1990, la découverte de la petite lune Dactyle orbitant autour de (243) Ida a confirmé que les astéroïdes peuvent également posséder des satellites naturels. Certains corps, comme (90) Antiope, sont des astéroïdes doubles formés de deux composants de taille similaire. Plusieurs astéroïdes ayant deux satellites sont aujourd'hui connus, dont (87) Sylvia.

La table suivante regroupe les lunes du Système solaire selon leur diamètre moyen et le corps autour duquel elles orbitent. La colonne de droite inclut certains autres objets notables (planètes, planètes naines, astéroïdes, transneptuniens) à titre de comparaison.




</doc>
<doc id="17863" url="https://fr.wikipedia.org/wiki?curid=17863" title="Io">
Io



Io est le symbole désuet de l’élément chimique ionium, autre nom du thorium-230.








</doc>
<doc id="17870" url="https://fr.wikipedia.org/wiki?curid=17870" title="Aquarelle">
Aquarelle

L’aquarelle est une technique picturale fondée sur l'utilisation de pigments broyés, agglutinés avec de l'eau gommée pour constituer une peinture à l'eau également appelée "aquarelle", ainsi que les ouvrages de peintures réalisées par ce procédé. Elle se pratique généralement sur papier.

Depuis la fin du , on différencie l'aquarelle, transparente, de la gouache, opaque.

Le faible encombrement du matériel et la possibilité d'une exécution technique rapide la font souvent servir à la réalisation de pochades et d'études, et à la peinture en extérieur.

L'aquarelle a servi historiquement et sert encore beaucoup pour des travaux mise en couleurs d'impressions monochromes ou de dessins et, avec la gouache, pour les livres de coloriage.

Au , les éditeurs proposaient des versions des lithographies en couleurs, fabriquées en passant à la main des couleurs sur une impression demi-teinte peu contrastée. Le même procédé a servi pour la mise en couleur de photographies. Au , le procédé pouvait servir pour la mise en couleurs de la bande dessinée par des coloristes. Le contour était alors préalablement tracé à l'encre indélébile à la plume ou au pinceau.

En peinture, on considère aujourd’hui généralement que le travail à l'aquarelle se fait sur papier vierge, avec tout au plus une mise en place légèrement tracée au crayon. Cela n'exclut pas que des artistes puissent lever des croquis coloriés sur le motif, avant d'exécuter l'aquarelle proprement dite sur papier vierge.

Lorsque d'un trait de pinceau, on dépose l'aquarelle sur le support, les pigments se retrouvent d'abord en suspension dans le milieu aqueux. Ils se déposent ensuite progressivement au creux des aspérités du papier tout comme des sédiments charriés par une rivière en crue. Tant que le papier reste humide, des pigments flottent encore dans le liquide. Il est toujours possible d'intervenir si l'on ne perturbe pas la couche des pigments déjà déposés .

Lorsque le papier est sec, la transparence de l'aquarelle s'impose. Elle résulte des différences d'épaisseur des strates de pigments sur le papier. Peu de pigments sur les crêtes et davantage dans les creux. C'est ce gradient qui crée cette « vibration » si particulière.

Sa simplicité n'est qu'apparente. Les difficultés, réelles, ne doivent cependant pas décourager le novice qui, s'il a bien assimilé ces spécificités techniques, saura en tirer profit pour produire un travail de qualité.

Le maximum d'intensité lumineuse correspond au blanc du papier. Les techniciens les plus habiles savent ménager dans leur tableau ces éclats lumineux naturels aux endroits les plus opportuns. De nombreux artifices techniques permettent aussi de préserver le fond du support : la paraffine (bougie) ou la gomme à masquer ( "drawing gum").

On décrit habituellement deux techniques qui peuvent s'associer dans un même travail.



Dans tous les cas, la couleur de l'aquarelle ternit assez notablement au séchage . La disparition de l'eau change le trajet des rayons lumineux, et les couleurs perdent de leur éclat. L'artiste en tient compte. Un phénomène du même ordre peut se produire si, le travail fini, on y applique un vernis fixatif ou protecteur .

Les couleurs d'aquarelle se présentent sous deux conditionnements : godets de couleur sèche, tubes de couleur pâteuse.

Elles sont constituées :

La composition de l'aquarelle en godet et en tube est presque la même. L'aquarelle en tube peut comporter plus de miel afin que le produit reste plastique plus longtemps . Il est possible de remplir les godets vidés avec des tubes, moins onéreux, ceux-ci durciront en séchant. Il est cependant recommandé de procéder en plusieurs couches. Si la peinture d'un tube a séché, on peut découper l'enveloppe et utiliser le contenu comme de l'aquarelle en godets.

Les tubes contiennent de 5 à . Les godets mesurent environ et contiennent environ de peinture ; les demi-godets sont moitié moins larges avec . Cette taille uniforme permet de placer dans les alvéoles des boîtes de voyage les produits de tous les fabricants.

Les crayons et craies « aquarellables » et encres-aquarelles liquides ont le plus souvent une composition très différente de ce celle des couleurs d'aquarelle. Bien qu'on puisse les travailler à l'eau, ils s'appliquent d'une façon très différente.

L'aquarelle se pratique habituellement à l'aide d'un pinceau ayant un bon pouvoir de rétention d'eau (trempe).

Le poil de petit-gris (de l'écureuil du même nom), dont la capillarité reste insurpassée, est le plus adapté. La forme du pinceau mouilleur est parfaite pour les lavis et les fonds, car son ventre (ou réservoir) permet de contenir une grande quantité de liquide .

Le poil de martre, souple et nerveux, est apprécié pour sa trempe et la finesse de sa pointe. La meilleure qualité est la variété de martre Kolinsky, en réalité vison de Sibérie.

Les pinceaux chinois, qui peuvent combiner deux sortes de poils, conviennent, mais ils ne se tiennent qu'en position verticale, alors qu'on peut utiliser la tranche des autres pour un trait plus épais.

Les pinceaux en fibres synthétiques souples, moins absorbants mais d'une bonne élasticité, sont utiles pour poser les fonds et ouvrir les blancs.

Les brosses plates servent à mouiller ou peindre de grandes surfaces.

Il existe également de nos jours des pinceaux à recharge d'eau. Lors de leur apparition, ces pinceaux étaient de deux types :
Aujourd'hui différentes sociétés de par le monde fabriquent des pinceaux à recharge d'eau. Il en existe également à recharge d'encre. Il faut utiliser des encres à colorants qui restent solubles dans l'eau, comme celles utilisables avec les stylos à plume. Si on remplit les pinceaux à recharge d'eau d'encre de Chine ou de brou de noix plus ou moins dilués, ils se bouchent rapidement, et souvent sans remède. Un trempage dans de l'eau tiède à chaude permet quelquefois de libérer l'obstruction. Les dépôts provenant du calcaire de l'eau s'éliminent au vinaigre, ou avec les produits prévus pour les machines à laver.

Un pinceau à recharge s'utilise différemment du pinceau traditionnel avec un godet d'eau. Le flux d'eau venant de la réserve dilue l'aquarelle prise au godet au fur et à mesure qu'on passe le pinceau sur le papier ; et seul un pinceau sec peut retirer, en la pompant, un excès de peinture humide. Les pinceaux à réserve d'eau peuvent s'utiliser seuls pour des croquis colorés rapides, ou en complément des pinceaux classiques et des godets.

D'autres outils sont indispensables, comme les godets à eau et chiffons pour nettoyer les pinceaux ; d'autres peuvent s'avérer utiles à l'aquarelle tels que palettes à godets pour préparer des mélanges, éponges, boules de coton, brosse à dent pour les projections de couleur, lame ou plume pour les grattages, gomme pour protéger des .

Le crayon aquarellable (ou crayon aquarelle) ressemble à un crayon de couleur, il permet de dessiner des détails précisément ou bien de placer les couleurs avant de les étaler avec un outil humide comme le pinceau à aquarelle.

Le pastel aquarellable (ou pastel aquarelle) se présente sous la forme d'un bâtonnet rond ou carré, donc plus épais que le crayon. Il permet de remplir plus rapidement des zones de couleur.

Le support de l'aquarelle est le plus souvent un papier spécialement encollé, appelé papier aquarelle.

Le papier à aquarelle est généralement :

Les papiers diffèrent par leur grain et par leur capacité à retenir les pigments. Plus le papier fixe les pigments, et plus on peut appliquer des couches successives sans perturber celles déjà posées ; mais on peut moins retirer de la couleur en mouillant, puis en pompant avec un pinceau essoré .

Bien que soient vendues depuis quelques années des toiles pour aquarelle, le papier est le support le plus courant de l'aquarelle.

Les papiers aquarelle sont majoritairement fabriqués à partir de coton. 

Le coton reste la matière première de choix, en raison de son fort taux de cellulose. La qualité 100 % coton reste la plus précieuse, bien qu'il existe de nombreuses fibres, telles que le lin ou la chanvre qui sont également utilisées pour cette technique de peinture.

La cellulose que l’on tire du bois (hêtre, bouleau, eucalyptus, pin, épicéa, etc.) devance aujourd’hui largement le coton. Malgré quelques défauts, en partie maîtrisés tels que le jaunissement dû à l'acidité, et sa bonne tenue dans le temps, c’est la fibre la plus économique à produire. À côté des papiers 100 % coton sont aujourd’hui proposés des mélanges coton/bois (50/50 ou 25/75), produits tant sur forme ronde que sur table plate. 

De nouvelles fibres naturelles apparaissent, telle la canne à sucre et le bambou. 
Le bambou est doté de longues fibres, d'où une grande résistance du papier et des effets de gondolement de la feuille limités. Il est moins gourmand écologiquement parlant que le bois, car il grandit très vite et il évite l’utilisation de pesticides ou de produits chimiques. 

Les papiers aquarelles sont fabriqués sur forme ronde ou sur table plate. 

La fabrication proto-industrielle sur forme ronde est la plus commune pour les aquarellistes. Plus lente et onéreuse, elle garantit un papier relativement haut de gamme, bien que d'une qualité inférieure au papier produit manuellement et artisanalement. Le tamis rond de cette méthode à l’ancienne, grâce au mouvement rotatif de son cylindre, répartit les fibres sur le feutre de manière aléatoire, pour une surface plane et stable, qui, en pratique, donne un plus grand contrôle de la couleur à l’artiste. 

La fabrication moderne sur table plate, plus rapide et économique, permet de produire un papier de bonne qualité. Le tamis long des machines modernes assure une orientation régulière ,(mais dans un seul et même sens) des fibres : le papier sera bien calibré, mais sensible aux gondolements et à l'étalement de la couleur. 

La fabrication manuelle et artisanale, feuille par feuille et à l'aide de formes à papier, existe toujours. Elle est encore pratiquée dans certaines régions de Chine, notamment à Lijiang et par certains aquarellistes soucieux de maîtriser tous les éléments de leur œuvre, des matériaux à la composition, ainsi qu'en Europe, chez une petite vingtaine d'artisans-papetiers.

Le grain du papier joue un rôle important à l'aquarelle.

Le papier humidifié tend à s'allonger. Au contact de l'eau qui porte les pigments de l'aquarelle, le papier peut gondoler, formant des creux dans lesquels la couleur s'accumule. Pour limiter cet inconvénient, les fabricants de matériel de peinture proposent des blocs de papier où les feuilles sont encollées les unes aux autres sur leurs quatre bords, permettant de conserver une certaine planéïté. L'artiste détache la feuille du bloc une fois l'œuvre terminée.

Il existe deux manières de travailler à l'aquarelle :

Dans la technique sur papier humide, la tension du papier est indispensable. Elle ne nuit en rien dans la technique sur papier sec, et évite des tracas. On utilisait autrefois un "stirator", dispositif destiné à maintenir le papier dans un état d'humidité et de tension.

Pour tendre le papier, on l'humidifie des deux côtés à l'aide d'une éponge ou d'un pinceau mouilleur, puis on le fixe sur une planche rigide à l'aide de bandes de kraft gommé. Une fois sec, le papier pourra être (re)mouillé sans risquer de gondoler. La planche doit être très rigide, car le papier exerce une grande force en séchant. On peut aussi tremper le papier dans l'eau ou on le mouiller profondément avec une douchette, avant de l'agrafer le papier humide sur un châssis. Certains peintres, comme Oga Kazuo, décorateur des dessins animés du studio Ghibli, étendent leur feuille abondamment mouillée sur du bois vernis, tandis que d'autres utilisent une plaque de plexiglas. Si l'eau ne s'évapore pas à travers la face inférieure, il conserve une humidité résiduelle pendant plus longtemps, ce qui se répercute sur la dynamique de l'eau et des couleurs

Les crayons aquarellables rendent très différemment sur papier sec et sur papier humine. Pour les appliquer à sec, il faut laisser le papier sécher plus ou moins d'une heure après avoir préparé le papier.

Des pigments en suspension dans l'eau ont probablement été utilisés pour créer des peintures retrouvées dans des cavernes préhistoriques et de nombreuses peintures murales, notamment celles des chambres funéraires, en Égypte.

La peinture chinoise classique est tout entière faite dans des techniques apparentées à l'aquarelle sur fond sec. Dès le , les Chinois peignaient sur de la soie avec des encres et des colorants solubles dans l’eau.

Dans l'Occident du on l'utilise en association avec la gouache et les applications de feuilles d'or dans les enluminures qui ornaient les manuscrits des monastères de l'Europe médiévale. On peignait sur du vélin ou du papier avec ses pigments solubles dans l'eau.

L'invention de la peinture à l'huile (ou sa diffusion) au laisse à la peinture à l'eau des domaines spécialisés, un peu à l'écart des courants artistiques dominants.

Au et au , le voyageur naturaliste, l'explorateur, le simple observateur ou le peintre amateur choisissent souvent ce médium. La peinture à l'huile reste fragile pendant le long processus de séchage, tandis que l'aquarelle sur papier peut rapidement se ranger dans un carton pour le transport. Les explorateurs étaient accompagnés de cartographes et de topographes qui étaient souvent des artistes amateurs. En 1577, John White accompagna l’expédition de Sir Martin Frobisher en quête du Passage du Nord-Ouest. Les aquarelles de White, représentant des hommes et des femmes inuits, constituent un témoignage exceptionnel des premiers contacts établis entre les cultures européenne et nord-américaine, et se rangent parmi les œuvres canadiennes les plus anciennes de ces explorateurs. 

En Allemagne, Albrecht Dürer développe cette technique, la mélangeant souvent à de l'encre et de la gouache. Il l'utilise en lavis pour des études de paysages durant son voyage en Italie en 1490, puis pour représenter de façon très détaillée des herbes, des fleurs, des oiseaux…
À l'époque classique, la peinture à l'eau, surtout adaptée au petits formats, se désigne sous le nom de "miniature" : .

Particulièrement adaptée aux notations précises, Holbein l'utilise au pour réaliser des portraits en miniatures, et Gaston d'Orléans l'intègre à ses planches naturalistes.

Au , peintres de fleurs et paysagistes flamands (Hendrick Avercamp, Albert Cuyp, Jan Van Goyen, Adriaen Van Ostade) traduisent quelquefois par l'aquarelle leur observation minutieuse de la nature. Rubens et Jordaens en ponctuent parfois leurs dessins de touches aquarellées souples et transparentes. L'aquarelle sert aussi au coloriage de gravures comme celles des ouvrages de Buffon.

Mais la peinture à la caséine et la peinture à l'huile gardent la préférence des artistes et surtout celle de leur commanditaires. Les peintres réservent alors l'aquarelle aux études préparatoires et certains travaux personnels. Cette éclipse relative se prolonge jusqu'au .

Jusque vers 1760, le vocable lui-même n'est pas encore bien défini et Diderot emploie indifféremment les termes de gouaches et d'aquarelle.

En 1766, William Reeves lance en Angleterre la première fabrication commerciale d'aquarelles, et l'Angleterre devient le pays où tant la production des couleurs à l'eau que leur utilisation artistique se développe alors. C'est à cette époque que les fabricants commencent à sélectionner des pigments transparents qui différencient l'aquarelle, les pigments opaques, caractérisés par un indice de réfraction et un taux de diffusion élevés destinés à la gouache.

La multiplication des petits paysages des peintres vénitiens favorise peu à peu le renouveau de la peinture à l'eau.

Utilisée pour les études, notamment lors de voyages en Italie, elle est pratiquée par Jean Honoré Fragonard, Hubert Robert ou Louis Durameau.

Gabriel de Saint-Aubin, Jean-Baptiste Lallemand, Louis-Joseph Watteau, s’en servent dans leurs scènes de genre (peinture de genre). Louis-Gabriel Moreau, l’utilise dans ses paysages de plein air, les soulignant d'un trait de plume. 

À partir du règne de Louis XVI, les aquarellistes sont acceptés au sein de l'Académie royale de peinture et de sculpture.

C'est en Angleterre, où l'enseignement de la peinture est moins encadré qu'en France, que l'aquarelle, par l'intermédiaire de la Royal Watercolour Society, fondée en 1804 à Londres, acquiert une dimension nouvelle. Samuel Palmer, Richard Bonington, William Turner en sont ses principaux acteurs. La maison Winsor et Newton, célèbre fabrique d'aquarelles, est fondée en 1834. Une section des aquarellistes anglais à l'exposition universelle de 1855 à Paris connaît un succès considérable.

En France, Eugène Delacroix, Théodore Géricault, Paul Huet et Théodore Rousseau s'en servent en voyage et pour leurs croquis de paysage. Les Impressionnistes (Boudin, Jongkind) l'apprécient aussi pour sa spontanéité.

Les études de danseuses d'Auguste Rodin et les nus de Georges Rouault montrent la liberté que l'on peut atteindre avec l'aquarelle. En témoignent aussi les œuvres de Vassily Kandinsky, Egon Schiele, Emil Nolde, August Macke ou Paul Klee.

Dans les années 1960, un renouvellement de cette technique est apparu avec Raoul Dufy, Jean Bazaine, Maurice Estève, Zao Wou-Ki, poursuivie dans les années 1970 par Pierre Risch, qui met au point une technique d'aquarelle sur papier de très grand format, entièrement mouillé à l'éponge et détourne un produit destiné à la sérigraphie, ( drawing gum ) afin de préserver le blanc du papier et ne pas gouacher l'aquarelle.

Jusqu'à la fin du , la majorité des dessinateurs de bande dessinée colorient à l'aquarelle ou à la gouache ainsi que les illustrateurs de livres, notamment pour enfants.




</doc>
<doc id="17872" url="https://fr.wikipedia.org/wiki?curid=17872" title="Bopomofo">
Bopomofo

Le bopomofo (ㄅㄆㄇㄈ) ou zhuyin fuhao (caractères traditionnels : ; caractères simplifiés : ; pinyin : Zhùyīn fúhào ; Tongyong pinyin : Jhùyin fúhào ; Wade-Giles : Chu-yin fu-hao) est un alphabet créé pour être utilisé dans la transcription du mandarin à des fins pédagogiques et didactiques. Aujourd'hui, à Taïwan il est, avec le "cāngjié" et le "dayi" l'une des trois méthodes les plus utilisées pour la saisie de l'écriture sur les claviers taïwanais. Il peut permettre la notation d'autres langues chinoises, notamment le hakka et minnan, deux autres principales langues de Taïwan. En Chine continentale, ce système a été abandonné au profit du pinyin fondé sur une translittération latine au moment de la prise du pouvoir du Parti communiste chinois.

Le mot "bopomofo" vient des quatre premières lettres de cet alphabet :

Les Chinois nomment cet alphabet de transcription 注音符號 (注音符号), "zhùyīn fúhào", c'est-à-dire « symboles phonétiques ».

Le système bopomofo été créé au en Chine, afin de simplifier l'apprentissage de la phonétique du mandarin. Jusqu'à sa création, des caractères très utilisés étaient assemblés pour représenter la phonétique des autres caractères. L'idée était d'apprendre la prononciation du mandarin à tous les Chinois, ceux-ci lisant tous la même écriture, mais ayant une prononciation différente, en fonction de la langue.

Par exemple le caractère « ㄖ » est dérivé du caractère soleil ancien. La forme moderne étant « 日 ». Ce caractère se prononçant "rì" en mandarin (transcription pinyin), il sert à représenter la consonne « r ». La majorité des zhuyin fonctionnent sur le même principe.

C'est à peu près le même principe qui est utilisé dans les kana japonais. En effet, si la grammaire et la prononciation du japonais sont très différentes de celles du chinois mandarin, l'écriture japonaise est tout de même fondée sur les sinogrammes.

Bopomofo/zhuyin (les colonnes "zhuyin" et "pinyin" montrent les équivalences)
Trois des signes notant les voyelles finales, ㄣ /en/, ㄤ /ang/ et ㄥ /eng/, sont aussi utilisés pour noter les consonnes nasales seules quand les voyelles ㄧ /i/ et ㄨ /u/ les précèdent : ainsi ㄧㄣ se lit /in/, ㄧㄥ /ing/ et ㄨㄥ /ung/ (écrit 
"-ong "en pīnyīn). Le pīnyīn "yong" / "-iong" est rendu par ㄩㄥ /üng/, en conformité avec l'histoire phonétique : /ü/ provenant historiquement de /iu/, la notation par /üng/ revient à écrire /iung/. L'on prononce bien sûr normalement [jʊŋ]. De la même manière, ㄦ /er/ sert aussi à indiquer la rétroflexion d'une voyelle : ㄅㄧㄢㄦ se lit /bianr/, soit [b̥jə˞] (voir aussi Suffixe -er).

La transcription en bopomofo suit parfois les mêmes conventions qu'en pīnyīn : elle n'indique pas les variantes allophoniques des voyelles. Ainsi ㄧㄢ /ian/ vaut [jɛn] et non [jan]. Les différences sont surtout dues au fait que le bopomofo est plus une transcription phonétique (l'on note ce que l'on entend) que phonologique (l'on note les phonèmes en tant qu'unités fonctionnant en opposition).

Certaines "irrégularités" du pīnyīn disparaissent : alors que, par exemple, la finale complexe /jou/ est notée en pīnyīn par "you" mais "-iu" après une consonne, ce sont toujours les signes ㄧㄡ que l'on utilise en bopomofo. De même, /wei/ (pīnyīn : "wei "/" -ui") est toujours rendu par ㄨㄟ, /wǝn/ (pīnyīn : "wen" / -"un") par ㄨㄣ, et la voyelle /ü/ est écrite ㄩ quelle que soit sa position dans la syllabe (en pīnyīn, on peut l'écrire "yu", "u" dans "ju", "qu", "xu", et "ü" dans "lü" et "nü").

Enfin, le bopomofo distingue clairement les allophones de /e/, ㄝ [ɛ] et ㄜ [ɤ], tous deux notés par "e" en pīnyīn ; on ne trouve ㄝ [ɛ] qu'après une voyelle palatale - /i/ ou /ü/ ─ ou bien quand il constitue la syllabe à lui seul.

Les syllabes à voyelle fermée sans consonne initiale, celles notées en pīnyīn par les digrammes "yi" /i/, "wu" /u/ et "yu" /ü/, sont rendues en bopomofo par la voyelle seule, soient ㄧ, ㄨ et ㄩ respectivement.

De la même façon, les syllabes sans voyelle finale mais dont la consonne est vocalisée (soient en pīnyīn "zhi" [ɖʐ̥ʅ], "chi" [ʈʂʰʅ], "shi" [ʂʅ], "zi" [ʣ̥ɿ], "ci "[ʦʰɿ], "si "[sɿ] et "ri "[ʐʅ]) sont simplement transcrites en bopomofo par les lettres simples ㄓ, ㄔ, ㄕ, ㄗ, ㄘ, ㄙ et ㄖ, sans signe de voyelle.

Note : il est d'usage, dans la transcription des langues chinoises, de rendre la vocalisation d'une consonne au moyen des symboles [ɿ] et [ʅ], ce dernier après une consonne rétroflexe ; c'est, en pīnyīn, la voyelle -"i" qui joue ce rôle. En API, enfin, il faut utiliser le symbole souscrit [ˌ]. Dans tous les cas, le symbole utilisé ne transcrit pas une voyelle mais une absence de voyelle : c'est la consonne seule qui est vocalisée, c'est-à-dire rendue syllabique.

Table d'exemple sinogramme-zhuyin-pinyin.

王之渙 《登鸛雀樓》(Wáng Zhīhuàn, "Dēng Guànquè Lóu")
Note : les marques tonales peuvent aussi s'écrire au-dessus de la voyelle (ㄌㄧ̌, ㄧ̀). Le ton 1 n'est pas noté et le cinquième ton est noté par un point, alors qu'en pinyin le premier ton est un macron et que le cinquième ton n'est pas noté.

Clavier de la République de Chine (Taïwan), utilisant bopomofo (zhuyin, en haut à droite), Cangjie (en bas à gauche) et dayi (en bas à droite).





</doc>
<doc id="17874" url="https://fr.wikipedia.org/wiki?curid=17874" title="Erhu">
Erhu

L’erhu (, du chinois , puisqu'il a deux cordes, et ) est un instrument de musique traditionnel chinois. Inventé il y a plus de mille ans, l'erhu appartient à la famille des huqin (), les instruments à cordes d'origine « "barbare" ». Il a probablement pour ancêtre un instrument d'Asie centrale ou de Mongolie. Il existe d'ailleurs des instruments mongols très proches, comme le "morin khuur" (en chinois, ), l'igil. La variante à quatre cordes, appelée sihu (, ), est également utilisée dans la musique mongole où elle est appelée "khuuchir".

On retrouve cet instrument sous différents noms et variantes de forme (notamment pour la caisse de résonance) partout dans le sud de l'Asie : au Viêt Nam (Dan Nhi), au Cambodge (Tro), au Laos (So), en Corée, en Thaïlande, en Indonésie… .

L’erhu est composé d'une caisse de résonance en bois ouverte au dos et recouverte d'une peau de serpent sur sa face avant. Un manche en bois orné de deux grosses chevilles à l'arrière sort verticalement du dessus de la caisse. Deux cordes ("ré" et "la") relient les chevilles au bas de la caisse. Il existe également une version légèrement plus grande et plus grave, appelée "zhonghu" ().
La table de résonance peut aussi être faite d'une fine planche de bois, et la caisse faite d'une calebasse ajourée au dos (pour que le son puisse ressortir) ou encore d’une noix de coco. Les modèles les plus courants ont la plupart du temps la table d'harmonie faite en peau animale, qui transmet le son de façon plus forte. Il en existe de plusieurs tailles.

Un archet dont le crin est coincé entre les cordes est utilisé pour faire vibrer celles-ci. On peut faire varier la tension de ce crin en l'écartant avec les quatre doigts de la main et en prenant appui sur l'extrémité de l'archet avec le pouce.

Les doigts de l'autre main servent à presser les cordes pour faire varier la hauteur du son.

L’erhu fait partie des instruments traditionnels utilisés pour la musique de l'opéra chinois. Récemment, durant le , les styles et les possibilités de jeu ont été diversifiés. Il existe maintenant aussi des morceaux entiers pour erhu seul, des concertos classiques et des pièces de musique de chambre.



</doc>
<doc id="17875" url="https://fr.wikipedia.org/wiki?curid=17875" title="Plessix-Balisson">
Plessix-Balisson

Plessix-Balisson est une ancienne commune du département des Côtes-d'Armor en région Bretagne.

Elle est devenue commune déléguée de Beaussais-sur-Mer le 

Avec une superficie de 8 hectares seulement, jusqu'en 2017, Plessix-Balisson a été la commune la moins étendue de son département et la deuxième plus petite commune de France (après Castelmoron-d'Albret). Depuis 2017, Plessix-Balisson est la plus petite commune déléguée de France et Vaudherland est la seconde plus petite commune de France (, Val-d'Oise).

Elle était entièrement enclavée dans la commune de Ploubalay.

Le nom de la localité est attesté sous les formes "Baluçon del Pleseiz" en 1139, "Plessiacus Juhelli" à la fin du siècle, "Pleseiz" à la fin du siècle, "Pleisseiz" en 1201, "Le Plessiz Baluczon" en 1387, "Le Plexeiz" en 1405, "Le Plessix Balliczon" en 1428, "Le Plessis Baliczon" en 1451, "Ecclesia Parochia de Plessis Balisson" au XVe siècle, "Le Plessis Balizon" en 1709.

Plessix-Balisson vient de l'ancien français "plesse" (« parc clos de haies d’épines ») et "Baluçon" (surnom porté par Geffroy Brient, seigneur du lieu en 1184).

Le premier seigneur du Plessis s'appelait Geoffroy Baluçon, fils du vicomte Alain Brient et de Muliel. Il venait de la vicomté de Poudouvre qui était un grand fief féodal dont la capitale était probablement la Vicomté, en Dinard, tandis que le siège du doyenné fut, au moins primitivement, Corseul, puis Saint-Enogat vers la fin du .

Geoffroy avait un grand esprit de foi, partagé d'ailleurs par son épouse. On pouvait lire : « Qu'il soit connu de tous que moi, Geoffroy Baluçon, ai donné et concédé, en pure aumône, à la bienheureuse Marie de Saint-Aubin-des-Bois, avec l'assentiment de mon fils Alain, une mine de froment sur la ferme de la Rogerais en Ploubalay et une autre mine sur la dîme du Trèfle on Corseul pour la rédemption de mon âme et de l'âme de mes prédécesseurs. »

C'est donc du nom de son premier seigneur, qui avait reçu en apanage ce démembrement de la vicomté paternelle, que le Plessis devint Le Plessis-Baluçon ou Balisson.

Nous ne savons pas quelles furent les limites du Plessis à cette époque. Il n'est pas douteux qu'une partie de Corseul relevait de cette seigneurie ainsi que Créhen, Ploubalay presque en entier lui appartenait ainsi que de larges enclaves en Lancieux et autres paroisses environnantes. Geoffroy Baluçon était donc un haut et puissant seigneur.

Il fit construire dans son fief du Plessis un château fort dont il ne subsiste plus que l'emplacement. C'est autour de cette forteresse que se groupèrent les éléments constitutifs d'une grande seigneurie du Moyen Âge, une justice avec tribunal et juges, des finances avec des officiers fiscaux, une organisation militaire à peu près complète. Ainsi est né le bourg du Plessis-Balisson.

La forteresse du Plessis était pour les habitants le sûr dépôt de leurs récoltes et de leurs biens. En cas d'incursion, elle donnait un abri. C'était le salut de la région. D'autant plus que ce château était édifié sur un mamelon au confluent de petits ruisseaux dont la réunion formait un vaste étang qui permettait aux défenseurs de se couvrir d'eau pour empêcher l'abord de la forteresse. Aujourd'hui, cet étang a été transformé en prairies et en marécages.

La forme du château était triangulaire. De profonds fossés qu'on distingue toujours et qui sont plantés d'arbres aujourd'hui, l'entouraient aux trois quarts. L'étang alimentait les fossés. Malheureusement, les démolitions ont été telles qu'il est impossible de fixer le nombre et l'emplacement des tours. Un puits profond, creusé au centre de l'une d'entre elles, là où devait être la cour d'honneur, demeure le seul vestige du château.

Un folkloriste bien connu écrivait en 1912 qu'il s'était trouvé à passer au Plessis peu de temps après la mise au jour des substructures du château, il y a de cela cinquante ans environ. La base des tours montrait qu'elles devaient être moins grosses que celles du Guildo. On lui dit qu'on avait trouvé beaucoup d'ossements, de chauves-souris, en les déblayant. La base de l'une des tours qui n'avait plus que quelques assises lui avait fait songer à celles du château de Léhon.

On ne connaît pas la date de la mort du premier seigneur du Plessix. Il laissa un fils, Alain, mais on ne sait pas grand-chose sur sa descendance. Certains disent que c'était un Baluçon, ce Guillaume du Plessix compté parmi les 34 bannerets bretons qui accompagnèrent Philippe Auguste à la bataille de Bouvines en 1214.

C'était encore un Baluçon, ce Geoffroy qui devint chancelier de l'Église de Tours et protonotaire apostolique. Il fut chancelier du roi Philippe le Bel et mêlé à l'expédition des plus grandes affaires de son temps. Le pape Clément V le chargea, même de plusieurs missions importantes. En 1295, ce Geoffroy Baluçon possédait des dignités et des bénéfices.

En mai 1296, il est employé dans un diplôme royal comme mandataire des exécuteurs testamentaires de la reine Isabelle d’Aragon, mère de Philippe le Bel. En tant que secrétaire de Philippe le Bel, il fut souvent employé comme conseiller et négociateur. C'est ainsi qu'il fit partie d'une mission envoyée à Rome pour obtenir du pape Benoît XI, l'annulation des bulles fulminées contre Philippe le Bel par Boniface VIII. En 1306-1307, Geoffroy servit plusieurs fois d'intermédiaire entre le roi et le pape Clément V.

Geoffroy Baluçon fit nommer l'un de ses neveux à l'évêché d'Évreux. Vers la fin de sa vie, il se retira dans l'hôtel qu'il possédait en haut de la rue Saint-Jacques à Paris. Il fit transformer cet hôtel en collège sous le nom de Saint-Martin-au-Mont de Paris, ce qui permit à 40 boursiers de faire leurs études gratuitement.

En fin Geoffroy Baluçon mourut en 1332, à l'abbaye de Marmoutier de Tours, où il se retira après avoir fait profession religieuse. Les abbés de Marmoutier dirigèrent par la suite pendant trois cents ans le collège du Plessix. Ce collège, devenant trop coûteux, fut cédé à la Sorbonne (1646) aux trois conditions suivantes :


Jusqu'au , les seigneurs du Plessix se paraient encore du titre de fondeurs du collège du Plessix. Ainsi, Madame de Launay du Pont-Cornou, en Ploubalay, faisait de nombreuses démarches pour que l'un de ses fils, en qualité de descendant des Baluçon obtînt une bourse.

Peu à peu, les Baluçon se dispersèrent et la famille seigneuriale connut l'extinction. Le résultat fut la ruine du château féodal au . Peut-être eut-il déjà à souffrir des guerres que se firent vers 1390, le Connétable de Clisson et Jean IV de Bretagne. Mais la fin du château s'explique surtout par l'extinction de la branche aînée des Baluçon et le passage par alliance du château, à des étrangers qui, résidant loin du Plessix, négligèrent d'entretenir les murailles, lesquelles finalement s'effondrèrent.

Cependant la race des Baluçon ne disparut pas aussi vite que croulaient les murs du château. On cite un Guillaume Baliczon de Saint-Potan à une revue d'armes à Moncontour en 1469. Un Ollivier du Plessis propriétaire de « Karpostant » en Ploubalay en 1448. Un Rolland Baliczon à Créhen en 1461.

Les branches cadettes des Baluçon se sont perpétuées jusqu'à notre époque, entre autres les Launay du « Bois es Lucas » en Saint-Cast qui sont des descendants des Launay du « Pont-Cornou » et de Launay-Comats, puînés des Balissons. Le manoir de la Roche en Lancieux fut aussi la résidence d'autres cadets qui prirent le nom de cette terre. Leur écusson avec des Léopards comme celui des Baluçon en témoigne. La « Guérais de l'Argentais » en Ploubalay furent des juveigneuries des Baluçon.





</doc>
<doc id="17878" url="https://fr.wikipedia.org/wiki?curid=17878" title="Edsger Dijkstra">
Edsger Dijkstra

Edsger Wybe Dijkstra (prononciation : ), né à Rotterdam le et mort à Nuenen le , est un mathématicien et informaticien néerlandais du . Il reçoit en 1972 le prix Turing pour ses contributions sur la science et l’art des langages de programmation et au langage Algol. Juste après sa mort, en 2002, il reçoit le prix PoDC de l'article influent, pour ses travaux sur l'autostabilisation. L'année suivant sa mort, le prix sera renommé en son honneur prix Dijkstra.

Après des études de physique théorique, il s'engage dès 1955 dans le domaine de l'informatique alors naissante, dont il est l'un des pionniers les plus éclairés.

Dijkstra avait une écriture manuscrite très lisible et a toujours refusé d'utiliser un traitement de texte, malgré son domaine d'activité, préférant la lettre manuscrite photocopiée. Luca Cardelli a créé une fonte « Dijkstra » en son honneur, qui imite son écriture régulière. Dijkstra référençait toutes ses lettres par "EWD" suivi d'un nombre, la dernière étant la lettre "EWD 1318".

Enseignant à l'université technique d'Eindhoven, il commence à se faire connaître en matière de systèmes avec "THE Operating system", un système construit en couches d'abstraction successives et idéal pour l'enseignement (« THE » est un jeu de mot sur l'acronyme de son université "Technische Hogeschool Eindhoven", école polytechnique d'Eindhoven). Fort de l'expérience d'écriture de ce système, il formalise le concept, avant lui diffus, de sémaphore puis introduit le concept de « section critique » avec deux exemples devenus classiques : le problème des lecteurs et des rédacteurs et le dîner des philosophes.

Constatant les dégâts provoqués par l'usage incontrôlé de l'instruction goto en programmation, il rédige en 1968 pour les "Communications of the ACM" un article qu'il nomme (). Voulant publier rapidement l'article sous la forme d'une lettre à l'éditeur, l'éditeur Niklaus Wirth le rebaptise « "Go To Statement Considered Harmful" » (« L'Instruction Go To considérée comme nuisible »).

Ce nouveau titre, tout autant que le propos de l'article, devient alors célèbre dans le milieu de l'informatique. Les titres de la forme se multiplient, jusqu'à un ».

L'instruction goto est rapidement marginalisée, et presque éliminée, par la programmation structurée (concept de Wirth et Dijkstra, présenté entre autres dans EWD 268). En programmation structurée, le "goto" est remplacé par des instructions comme "if … then … else …", "while … do", "repeat … until" qui furent introduites par Wirth dans Algol W : chaque instruction contient "une seule" entrée et "une seule" sortie, ce qui rend enfin possible des tests systématiques exhaustifs impossibles avec le « code spaghetti ».

Des conditions peuvent aussi être imposées à l'entrée unique et des caractéristiques postulées à la sortie unique, ce qui ouvre la porte à des outils ajoutés à la syntaxe, comme "assert" (voir Logique de Hoare) et plus tard à la programmation par contrat du langage Eiffel.

Dijkstra avait joué un rôle important dans le développement du langage Algol à la fin des années 1950 et développé ensuite , contribuant grandement à notre compréhension de leur structure, de leur représentation et de leur implémentation. C'est aussi un adepte du bel algorithme, y compris pour des sujets difficiles à traiter en programmation structurée comme les perles de Dijkstra (disposer une par une des perles de trois couleurs sur un fil de façon à ce qu'il n'y ait jamais deux séquences adjacentes identiques).

Il est également à l'origine de l'algorithme éponyme, l'algorithme de Dijkstra, permettant de calculer des plus courts chemins dans un graphe orienté.

Le discours qu'il prononce en 1972 lorsqu'il reçoit le prix Turing, "The Humble Programmer", est également resté célèbre. Il s'agit également d'un exercice d'autodérision, le professeur Dijkstra s'étant toujours montré très conscient de la valeur de ses travaux.

En 1974, Dijkstra publie l'article fondateur de l'autostabilisation, propriété d'un système réparti à retrouver un comportement correct après toute défaillance transitoire. En 2002, il reçoit le prix PoDC de l'article influent pour cet article. Il meurt peu après. Ce prix est renommé "prix Dijkstra" en son honneur dès l'année suivante.

Dijkstra, connu pour son caractère difficile et son intransigeance, était réputé pour ses aphorismes, lesquels résumaient sa vision de la science informatique.





</doc>
<doc id="17882" url="https://fr.wikipedia.org/wiki?curid=17882" title="Subrahmanyan Chandrasekhar">
Subrahmanyan Chandrasekhar

Subrahmanyan Chandrasekhar ( à Lahore, Punjab, Indes britanniques - à Chicago) est un astrophysicien et un mathématicien d'origine indienne. Il est lauréat du prix Nobel de physique de 1983.

Chandrasekhar, né dans une famille tamoule hindoue installée à Lahore, plus connu sous le nom de « Chandra », fait ses études au "" de Madras (maintenant appelée Chennai). Il effectue son doctorat sous la direction de Ralph H. Fowler au "Trinity College" de l'université de Cambridge au Royaume-Uni et le termine en 1933 à l'âge de . Ses travaux concernent l'évolution des étoiles.

Il est connu pour avoir déterminé au-delà de quelle limite une naine blanche devient instable sous certaines conditions et s'effondre en étoile à neutrons, initiant le processus de supernova. Cette limite, désormais connue sous le nom de limite de Chandrasekhar, lui vaut à l'époque une vive controverse avec l'astronome anglais Sir Arthur Eddington. Il émigre ensuite aux États-Unis et obtient un poste à l'université de Chicago de 1937 jusqu'à sa mort en 1995, à l'âge de . Il obtient la nationalité américaine en 1953.

Durant sa carrière scientifique de plus de , il a rédigé plus de 300 articles scientifiques. Chandrasekhar a reçu de nombreuses distinctions. Il est devenu membre de la "Royal Society" le . Ses travaux sur le transfert de rayonnement lui valent le prix Rumford en 1957. Il est également lauréat de la médaille Bruce en 1952, de la médaille d'or de la Royal Astronomical Society en 1953, de la "National Medal of Science" (la plus haute distinction académique aux États-Unis) en 1966, de la médaille Henry Draper en 1971 et de la médaille Copley en 1984. Il obtient en 1983 le prix Nobel de physique (qu'il partage avec William Fowler) . Le gouvernement indien l'a également honoré en lui attribuant la Padma Vibhushan.

L'astéroïde (1958) Chandra a été baptisé en son honneur. En 1999, la NASA lance un concours pour baptiser son télescope spatial de rayons X (anciennement appelé AXAF), mis en orbite par la navette spatiale Columbia le . Après plus de de noms reçues de plus de 50 pays, c'est Chandra qui fut retenu en l'honneur de Chandrasekhar.

Chandrasekhar était le neveu de Chandrasekhara Venkata Raman, lauréat du prix Nobel de physique de 1930.

Chandrasekhar est considéré comme le premier scientifique à avoir appliqué les lois de la relativité restreinte à l'astrophysique et en particulier à l'étude de la structure interne des étoiles, travail qu'il entreprend lors du voyage en bateau qu'il effectue en 1930 pour rejoindre l'Angleterre et entamer sa thèse. Il a l'occasion de croiser les grands scientifiques de son époque, en particulier Dirac, Eddington, Jeans, Milne et Plaskett.

La principale conclusion de ses travaux est que les naines blanches ne peuvent excéder une certaine masse : au-delà de cette masse, la pression de dégénérescence des électrons ne peut contrer l'effet de la gravité. Commentant la conclusion de ses travaux, Chandrasekhar affirme à propos de ce que peut devenir une naine blanche dont la masse dépasse la limite qu'il vient de calculer qu' (). Une telle conclusion provoque un refus de la part du plus grand astrophysicien anglais de l'époque, Arthur Eddington, à l'approbation duquel Chandrasekhar soumet ses travaux. Eddington les commente par un lapidaire (). Cette phrase connue est prononcée le lors d'une conférence à la "Royal Astronomical Society", à laquelle Chandrasekhar venait communiquer ses résultats. Le président de la "Royal Astronomy Society" demande alors à Eddington de parler sur le même sujet (sans que cela ait été prévu au départ). Eddington, tout en reconnaissant l'exactitude des travaux de Chandrasekhar, les rejette en argumentant qu'une naine blanche dépassant la masse limite n'aurait d'autre choix que de devenir ce qu'il décrivit alors comme étant un trou noir (sans utiliser ce terme qui date des années 1960), hypothèse qu'il juge trop absurde pour être raisonnable. Incidemment, ni Eddington ni Chandrasekhar ne mentionnent la possibilité qu'une naine blanche puisse se transformer en étoile à neutrons, comme l'avaient proposé Walter Baade et Fritz Zwicky quelques mois plus tôt.

Il a souvent été dit que Chandrasekhar avait été très affecté par cette controverse avec Eddington et c'est cette thèse que présente son biographe , suggérant même que ce jugement définitif par une des sommités de son époque porta longtemps ombrage à Chandrasekhar, à tel point qu'il jouera un rôle dans sa décision d'émigrer vers les États-Unis pour pouvoir exercer ses talents. Cependant William McCrea, témoin de la controverse et présent lors de la conférence de 1935 tempère notablement ces propos, estimant que sa décision d'émigrer aux États-Unis était surtout motivée par la perspective d'obtenir un poste permanent dans la recherche, et que les deux hommes sont toujours restés en bons termes et avaient d'ailleurs régulièrement discuté du problème de la masse maximale des naines blanches plusieurs fois auparavant. D'après Mac Crea, Chandrasekhar aurait été surpris de voir à quel point ses conclusions troublaient Eddington. Malgré ce différend, Chandrasekhar a consacré à Eddington un ouvrage : "Eddington, the most distinguished astrophysicist of his time", (litt. « Eddington, le plus éminent astrophysicien de son temps »). Un autre témoignage, du lauréat du prix Nobel de physique James Cronin (qui connut Chandrasekhar les dix dernières années de sa vie), semble cependant indiquer qu'au contraire Chandrasekhar aurait montré sur la fin de sa vie une certaine forme de rancœur envers Eddington.

À la fin des années 1930, Chandrasekhar cesse de s'intéresser à la structure interne des étoiles. Il décide alors de changer de sujet, non sans avoir publié un ouvrage sur le sujet ("Introduction to the Study of Stellar Structure", en 1939). C'est cette façon de procéder qu'il adoptera durant toute sa carrière : il s'investira quelques années dans un sujet donné, publiant de nombreux articles scientifiques (plus de en moyenne pendant plus de ), et concluant le tout par une monographie de qualité. Il en publiera près d'une dizaine : outre celle déjà mentionnée, les plus marquants seront ses travaux sur l'évolution stellaire ("Principles of Stellar Dynamics", 1943), le transfert radiatif ("Radiative Transfert", 1950), l'hydrodynamique ("Hydrodynamic and Hydromagnetic Stability", 1961), les configurations d'équilibre des ellipsoïdes ("Ellipsoidal Figures of Equilibrium" 1968), et les trous noirs ("The Mathematical Theory of Black Holes", 1983).

De tous ces travaux, seuls les premiers seront mentionnés par le Comité Nobel pour justifier l'attribution de son prix. Il semble que Chandrasekhar ait quelque peu souffert de voir que ce n'était pas l'ensemble de son œuvre qui ait été récompensé. Il se permet en tout cas d'évoquer ses autres travaux dans le discours qu'il prononce lors de la remise de son Prix Nobel.

Le dernier sujet auquel s'intéresse Chandrasekhar est celui des trous noirs. Sa contribution majeure repose sur l'étude mathématique des perturbations autour des solutions de type trou noir des équations d'Einstein, notamment celle du mathématicien néo-zélandais Roy Kerr. Chandrasekhar sera séduit par la beauté de cette solution dont il parlera en termes extrêmement élogieux (voir Roy Kerr). Ses travaux sur le sujet seront publiés dans une monographie impressionnante qui illustre sans doute le mieux ses capacités hors pair, "The Mathematical Theory of Black Holes".

Chandrasekhar fut également rédacteur en chef de la revue "Astrophysical Journal" de 1952 à 1971, période pendant laquelle il contribua grandement à l'essor de ce journal qui est maintenant parmi les meilleurs journaux scientifiques traitant d'astrophysique. Durant sa carrière, il a supervisé les thèses de 51 étudiants de l'Université de Chicago.


Les éditions indiquées ne sont pas toujours les éditions originales. Les dates données dans la section « Travaux » correspondent en revanche à la première édition.







</doc>
<doc id="17883" url="https://fr.wikipedia.org/wiki?curid=17883" title="Lénaïc (prénom)">
Lénaïc (prénom)

Lénaïc est un prénom breton francisé, diminutif de Lena, qui signifie en breton ancien « grand troupeau », puis « riche » par dérivation. Comme beaucoup de prénoms bretons, c'est un prénom mixte.

L'origine grecque "Léna", qui donna "Hélène" et ses dérivés, a souvent été attribuée à tort à tous les prénoms bretons dérivés de "Lena", ce qui explique que l'on fête les "Lénaïc" le 18 août.

Autres écritures : Lenaig, Lénaïk, Lenaïck, Laenaïc



</doc>
<doc id="17884" url="https://fr.wikipedia.org/wiki?curid=17884" title="Communautés européennes">
Communautés européennes

Les Communautés européennes étaient des organisations internationales régionales qui avaient la particularité d'avoir des institutions communes.

Les communautés européennes étaient au nombre de trois :

Les structures exécutives des trois Communautés européennes ont été fusionnées par le Traité de fusion des exécutifs communautaires, dit « traité de Bruxelles », signé le et entré en vigueur le . 

Les Communautés européennes formaient l'un des trois piliers de l'Union européenne. Les deux autres piliers étaient la politique étrangère et de sécurité commune (PESC), ainsi que la coopération policière et judiciaire en matière pénale (CPJP).

Les Communautés européennes étaient notamment membres de l'OCDE et de l'OMC.

La CECA fut la première communauté créée par six États : l'Allemagne, la France, l'Italie, et les trois pays du Benelux (Belgique, Pays-Bas, Luxembourg). La Communauté européenne du charbon et de l'acier (CECA) a pour but de combiner les industries du charbon et de l'acier de ses membres pour créer un marché commun autour de ces ressources. L'objectif était d'augmenter la prospérité et diminuer les risques d'une nouvelles guerres entre ces États à travers le processus d'intégration européenne.

Fondée en 1951, elle est considérée comme l'ancêtre de l'Union européenne car elle est la première des communautés européennes. Le traité fondateur de cette communauté avait une validité de 50 ans, et il expira en 2002. Ces activités ont alors été transférées à la Communauté européenne.

Elle fut fondée par les membres de la CECA, en 1957, grâce au traité de Rome. La CEE avait pour but de créer une union douanière et une coopération économique. Cela mena à la création du marché unique européen.

La CEE a disparu en tant que telle, et est devenue la Communauté européenne, après la signature du Traité de Maastricht en 1992. La Communauté européenne a elle-même disparu après l'entrée en vigueur du Traité de Lisbonne le décembre 2009 qui abolit la structure en pilier de l'Union.

La CEEA fut créée le même jour que la CEE en 1957. Son objectif était d'établir un marché commun de l'énergie nucléaire et son développement parmi ses membres. 

Contrairement à la CECA, la CEEA n'avait pas de limite de validité et continue à exister. Du fait de la sensibilité de l'opinion publique en matière de nucléaire, le traité n'a pas été amendé depuis sa signature et n'aurait pas été changé par la Constitution européenne bien qu'elle tentait d'abroger les autres traités.




</doc>
<doc id="17887" url="https://fr.wikipedia.org/wiki?curid=17887" title="Communauté européenne">
Communauté européenne

La Communauté européenne était une organisation supranationale réunissant la plupart des États européens et caractérisée par des transferts de compétence importants consentis dans de nombreux secteurs par les États membres. L'entrée en vigueur du traité de Lisbonne le décembre 2009 a mis fin à la Communauté européenne en tant qu'entité juridique, sa personnalité juridique étant transférée à l'Union européenne qui en était dépourvue jusque-là.

La dénomination « Communauté européenne » a remplacé celle de « Communauté économique européenne » le 1993. L'adjectif « économique » a été retiré de son nom par le traité de Maastricht en 1992. Elle était, avec la Communauté européenne de l'énergie atomique (Euratom), un élément de ce qu'on appelle les Communautés européennes (au pluriel). Ces communautés européennes formaient, avant l'entrée en vigueur du traité de Lisbonne, un des trois piliers de l'Union européenne.

Le traité de Maastricht est basé sur l'Acte unique européen et la Déclaration solennelle sur l'Union européenne dans la création de l'Union européenne. Le Traité fut signé le 7 février 1992 et prit effet le novembre 1993. Il a remplacé les Communautés européennes, les transformant en un des trois piliers de l'Union. Le premier président de la Commission de l'Union européenne fut Jacques Delors, qui conserva brièvement son mandat de la CEE avant l'arrivée de Jacques Santer en 1994.

Le traité d'Amsterdam a transféré les responsabilités de la liberté de mouvement des individus (tel que les visas, l'immigration illégale, l'asile) du pilier Justice et affaires intérieures (JAI) à la Communauté européenne (JAI fut renommé Coopération policière et judiciaire en matière pénale). Les traités d'Amsterdam et de Nice ont aussi étendu la procédure de codécision à presque tous les domaines politiques, conférant au Parlement des pouvoirs égaux au Conseil dans la Communauté.

En 2002, le traité de Paris qui avait créé la Communauté européenne du charbon et de l'acier (une des trois Communautés européennes) prit fin, ayant atteint sa limite de 50 ans (en tant que premier traité, il était le seul limité dans le temps). Aucune tentative de remplacement ne fut amorcée car il était considéré comme superflu ; et au lieu de ça, le traité de Nice transféra ses composantes au traité de Rome et, de là, son effet continu en tant que partie des attributions de la CEE.

Le 2009, l'entrée en vigueur du traité de Lisbonne a mis fin à la structure en pilier de l'Union, et par conséquent à l'existence de la Communauté européenne.

Le but de la CE est d'établir une union économique et monétaire parmi ses membres. Elle se fonde sur « quatre libertés » :

Les directives européennes se basent sur le traité de la Communauté européenne ; couramment on les attribue à l'Union européenne, dont la Communauté européenne n'est qu'une des communautés.

L'adhésion à certaines communautés de l’Union européenne restait optionnelle notamment :

Ce pilier communautaire couvrait les domaines suivants :

La structure en piliers de l'Union permettait l'augmentation de la coopération entre les domaines de l'Union, sans qu'il n'y ait de personnes détenant trop de pouvoirs sur les institutions internationales. La Coopération policière et judiciaire en matière pénale devint un nouveau pilier tandis que la Coopération politique européenne devint le second pilier (la Politique étrangère et de sécurité commune).

Les institutions de la CEE devinrent celles de l'UE mais le rôle de celles-ci entre les piliers est différent. La Commission le Parlement, et la Cour de Justice étaient en grande partie coupés des activités des deuxième et troisième piliers, tandis que le Conseil les domine. Ceci est relaté notamment au travers des noms des institutions, le Conseil est officiellement appelé le « Conseil de l’"Union européenne" » tandis que le nom de la Commission était « Commission des "Communautés européennes" ». Ceci permet à ces nouveaux domaines d'être basés sur l'intergouvernementalisme (accords unanimes entre les gouvernements) plutôt que sur la vote à la majorité et les institutions indépendantes.

Toutefois, depuis Maastricht, le Parlement a gagné un rôle plus important. L'augmentation des pouvoirs des institutions supranationales et le vote à la majorité qualifiée au conseil permettait de décrire le pilier communautaire comme empreint de fédéralisme.

Avec le traité de Lisbonne la structure en piliers pourrait être abolie, les fusionnant en une seule Union européenne, dans lesquelles les institutions communautaires auraient plus de pouvoirs. Ceci inclurait la personnalité morale de la Communauté qui serait transféré à l'Union. Ceci était auparavant prévu par le projet constitution européenne qui n'a pas été ratifié en 2005.

Le traité Euratom, contrairement à la CECA, n'expira pas et, en dépit des propositions de fusion dans l'Union, il continuera d'exister comme entité indépendante de celle-ci.





</doc>
<doc id="17888" url="https://fr.wikipedia.org/wiki?curid=17888" title="Communauté économique européenne">
Communauté économique européenne

La Communauté économique européenne (CEE) était une organisation supranationale créée en 1957 pour mener une intégration économique (dont le marché commun) entre l'Allemagne de l'Ouest, la Belgique, la France, l'Italie, le Luxembourg et les Pays-Bas.

Elle connut des élargissements pour inclure douze États (soit six de plus). À partir de 1967, les institutions de la Communauté économique européenne dirigeaient la Communauté européenne du charbon et de l'acier (CECA) et la Communauté européenne de l'énergie atomique (désignées alors Communautés européennes). Quand l'Union européenne fut créée le , la CEE devint la Communauté européenne (CE), un des trois piliers de l'Union européenne. Avec la fin de la structure en piliers, les institutions de la CEE perdurent en tant qu'institutions de l'Union.

En 1951, la signature du Traité de Paris créa la Communauté européenne du charbon et de l'acier (CECA). Il s'agissait d'une communauté internationale basée sur le supranationalisme et les lois internationales, dont le but était de soutenir l'économie de l'Europe et d'empêcher une guerre future par l'intégration de ses membres.

Dans le but de créer une Europe fédérale deux autres communautés furent proposées : la Communauté européenne de défense (CED) et la Communauté politique européenne (CPE). Tandis que le traité pour cette dernière était rédigé par le Parlement européen, la chambre parlementaire de la CECA, la CED fut rejetée par le Parlement français le 30 août 1954. Le Président Jean Monnet démissionna de la Haute Autorité en signe de protestation et commença à travailler sur des communautés alternatives, basées sur l'intégration économique plutôt que l'intégration politique. Après la Conférence de Messine en 1955, Paul-Henri Spaak eut la tâche de préparer un rapport sur l'idée d'union douanière. Le rapport Spaak fut la première pierre menant aux négociations intergouvernementales à Val Duchesse en 1956 ce qui, avec le rapport Ohlin, jeta les bases du traité instituant la Communauté économique européenne.

En 1956, Paul-Henri Spaak mena la Conférence intergouvernementale pour le Marché commun et l'Euratom au château de Val Duchesse, préparant la conférence conduisant à la signature, le 25 mars 1957, du Traité de Rome instituant la CEE. Celle-ci prend corps le suivant.

Les communautés naissant à la suite de la signature du traité de Rome étaient la Communauté économique européenne (CEE) et la Communauté européenne de l'énergie atomique (CEEA), plus connue sous le nom d'Euratom. Ces dernières avaient un niveau de supranationalité inférieur aux précédentes en raison des protestations de certains pays à propos de leur souveraineté. La première réunion formelle de la Commission Hallstein eut lieu le 16 janvier 1958 au Château de Val Duchesse. La CEE permettrait la création d'une union douanière tandis que l'Euratom devait promouvoir la coopération dans l'énergie nucléaire. La CEE devint rapidement la plus importante et étendit ses activités. Un des premiers accomplissements les plus importants de la CEE fut l'établissement de niveaux communs des prix pour les produits agricoles en 1962. En 1968, les droits de douanes à l'intérieur de la communauté furent supprimés sur certains produits.

Une autre crise a été déclenchée par les propositions concernant le financement de la Politique agricole commune, qui est entrée en vigueur en 1962. La période transitoire durant laquelle les décisions étaient prises à l'unanimité prenait fin et le vote à la majorité avait pris effet au Conseil. Ensuite l'opposition du président français Charles de Gaulle au supranationalisme conduisit à la politique de la chaise vide qui fut résolue par le compromis de Luxembourg. Ce dernier, datant du , par le biais d'un gentlemen's agreement, permit d'utiliser le droit de veto dans des domaines d'intérêt national.

En 1967 le Traité de fusion prit effet, ce qui fit fusionner les institutions de la CECA et de l'Euratom dans celles de la CEE, bien que les trois communautés partageaient déjà l'Assemblée parlementaire et la Cour de justice. Ensemble, elles étaient appelées "Communautés européennes". Malgré cette fusion, chaque communauté possédait encore une personnalité propre. Les traités suivants garantissaient de nouveaux pouvoirs aux communautés au-delà du seul domaine économique. Ils augmentaient ainsi l'intégration politique en une Europe unie et pacifiée que Mikhaïl Gorbatchev décrivait comme une "maison commune européenne".

Le premier élargissement eut lieu le avec l'adhésion du Danemark, du Royaume-Uni et de l’Irlande. À l'origine, quatre pays étaient candidats : le Danemark, l'Irlande, la Norvège et le Royaume-Uni. Ils avaient entamé une procédure pour rejoindre les trois Communautés. Toutefois, le président Charles de Gaulle voyant l'entrée du Royaume-Uni comme un cheval de Troie américain utilisa son droit de veto, et les demandes des quatre pays furent rejetées.

Ces quatre pays firent une nouvelle demande d'adhésion le et, Georges Pompidou ayant pris en juin 1969 la suite de Charles de Gaulle à la présidence, le veto fut levé. Les négociations commencèrent en 1970 sous le gouvernement anglais pro-européen d'Edward Heath, qui devait subir des désaccords concernant la politique agricole commune et les relations entre le Royaume-Uni et le Commonwealth of Nations. Néanmoins, deux ans après que les traités d'adhésion furent signés tous, sauf la Norvège, entrèrent dans la communauté.

Le traité de Rome statuait que le Parlement européen devait être élu au suffrage direct, toutefois ceci nécessita un accord du Conseil sur le système de vote. Le conseil remit ceci à plus tard et le Parlement resta sous le principe de nomination.

Le Parlement fit pression pour des accords et le le Conseil tomba d'accord sur une partie des instruments nécessaires aux élections, certain détails restent différents aujourd'hui encore. Durant la présidence de Jenkin, en juin 1979, les élections se tinrent dans tous les États membres (voir Élections européennes de 1979).

Peu après son élection, le Parlement devint la première institution communautaire à proposer que la communauté adopte le Drapeau européen. Le Conseil européen donna son accord et la Communauté adopta ses symboles en 1984.

La Grèce déposa sa demande d'adhésion le , après la restauration de la démocratie, et entra dans la communauté le . Suivant la Grèce, et après la restauration de la démocratie, l'Espagne et le Portugal firent leur demande d'adhésion aux communautés en 1977 et y entrèrent ensemble le . En 1987 la Turquie devint officiellement candidate à l'adhésion ce qui marqua le début du plus long processus d'adhésion d'un pays.

Avec les projets de futurs élargissements, et le désir d'augmenter les domaines de coopération, l'Acte unique européen fut signé par les ministres des affaires étrangères les 17 et 28 février 1986 respectivement à Luxembourg et à La Haye. Ce document a entrainé des réformes institutionnelles, une extension des pouvoirs, de la coopération en politique étrangère et le marché unique. Il prit effet le . L'acte a été influencé par les travaux de ce qui deviendra le Traité de Maastricht, qui a été accepté le , ratifié l'année suivante, et qui prit effet le établissant l'Union européenne.

L'Union européenne a absorbé la CEE comme l'un des trois piliers. Les domaines de compétences de la CEE devinrent le pilier dénommé Communauté européenne, continuant à suivre la structure supranationale de la CEE. Les institutions de la CEE devinrent celles de l'UE, certaines changeant leur nom en conséquence. Toutefois la Cour, le Parlement et la Commission avaient des apports limités dans les nouveaux piliers, puisqu'ils répondent plus à une logique intergouvernementale que la Communauté européenne. L'entrée en vigueur du Traité de Lisbonne le décembre 2009 a mis fin à cette structure en pilier.

Le but principal de la CEE, comme le présente son préambule, était « d'établir les fondements d'une union sans cesse plus étroite entre les peuples européens » et « d'assurer par une action commune le progrès économique et social en éliminant les barrières qui divisent l'Europe ». Pour cela, trois éléments étaient prévus :

En ce qui concerne l'union douanière, le traité prévoyait une réduction de 10 % des tarifs douaniers et une augmentation de 20 % des quotas d'importation. Les progrès de l'Union douanière se firent plus rapidement que les 12 ans initialement prévus, cependant la France a subi quelques rechutes en raison de la guerre d'Algérie.

La CEE avait pour mission de dépasser l'approche sectorielle (celle du charbon et de l'acier) pour étendre l'intégration à tout le domaine économique afin de créer un véritable marché commun et un rapprochement économique des États membres. Cela passait par la libre circulation des personnes (Convention de Schengen), des biens, des capitaux et des services; par l'abolition des restrictions douanières entre ses membres; par l'instauration d'une politique agricole commune (PAC), etc.

Elle laissait également déjà entrevoir une Union économique et monétaire, la zone euro.

Les membres fondateurs de la CEE, de même que des deux autres communautés étaient au nombre de six, par opposition aux sept États fondateurs de l'AELE. Ces six pays fondateurs étaient l'Allemagne de l'Ouest, le Benelux (c'est-à-dire la Belgique, le Luxembourg et les Pays-Bas), la France et l'Italie. Le premier élargissement a eu lieu en 1973, avec l'adhésion du Danemark, de l'Irlande et du Royaume-Uni. La Grèce, l'Espagne et le Portugal la rejoignirent durant les années 1980. Après la création de l'Union européenne en 1993, elle s'est élargie à 15 autres pays en 2007.

Les États membres sont représentés dans chaque institution. Le Conseil est composé des ministres nationaux qui représentent leur gouvernement national. Chaque État a aussi droit à un Commissaire européen, bien que ces derniers ne sont pas supposés représenter leurs intérêts nationaux mais ceux de la Communauté. Avant 2004, les membres les plus étendus (Allemagne, France, Italie et Royaume-Uni) avaient deux commissaires. Au Parlement européen, le partage des sièges se fait en fonction de la population, cependant, depuis les élections européennes de 1979, les eurodéputés sont élus directement et ils siègent non plus en fonction de leur pays d'origine mais en fonction de leurs idées politiques. La plupart des autres institutions, dont la Cour de justice des Communautés européennes, possèdent une certaine forme de division nationale de ses membres.

La CEE disposait de ses propres institutions :

Le Conseil représente les gouvernements, le Parlement représente les citoyens et la Commission représente les intérêts européens. Le Conseil, le parlement ou un autre parti dépose une demande de législation à la Commission. La commission la rédige et la présente au Conseil pour qu'elle soit approuvée et au Parlement pour obtenir son opinion (dans certains cas il peut y avoir un veto, suivant la procédure législative en usage). Le devoir de la Commission est d'assurer le fonctionnement quotidien de l'Union et d'amener devant la Cour de justice ceux qui ne se soumettent pas. À la suite du traité de Maastricht en 1993, ces institutions sont devenues celles de l'Union Européenne, quoique limitées dans certains secteurs en raison de la structure en piliers. Néanmoins, le Parlement a gagné des pouvoirs sur la Commission en ce qui concerne la législation et la sécurité. La Cour était l'autorité judiciaire la plus importante, résolvant les disputes au sein de la Communauté, tandis que la Cour des comptes n'avait d'autres pouvoirs que celui d'enquêter.

La CEE a hérité de certaines institutions de la CECA parmi lesquelles Assemblée commune et la Cour de justice de la CECA (dont les autorités avaient été étendues à la CEE et à l'Euratom). Toutefois la CEE, et l'Euratom, possédaient des corps exécutifs différents de ceux de la CECA. À la place du Conseil des ministres de la CECA se trouvait le Conseil de la Communauté économique européenne, et à la place de la Haute Autorité se trouvait la Commission des Communautés européennes.

En vertu du Traité de fusion en 1967, les exécutifs de la CECA et de l'Euratom fusionnèrent dans ceux de la CEE, créant ainsi une unique structure institutionnelle pour diriger les trois Communautés séparément. Dès lors, le terme de "Communautés européennes" a été utilisé pour les institutions (par exemple, la "Commission de la Communauté économique européenne" est devenue "Commission des Communautés européennes").

Le Conseil des Communautés européennes était un organisme détenant les pouvoirs législatifs et exécutifs ; il s'agissait donc de l'organe décisionnel principal de la Communauté. Sa Présidence changeait d'États tous les six mois. Il est lié au Conseil européen, qui est une réunion informelle des leaders nationaux (depuis 1961) sur la même base que le Conseil.

Le conseil était composé des ministres nationaux de chaque État. Toutefois sa forme variait suivant le sujet abordé lors des réunions. Ainsi, si l'agriculture était le sujet de la réunion, alors le Conseil était composé de ministre de l'agriculture. Les votes se faisaient soit à la majorité (avec des voix attribuées en fonction de la population), soit à l'unanimité.

La Commission des Communautés européennes était l'organe exécutif de la communauté, rédigeant les lois communautaires, traitant du fonctionnement quotidien de la communauté et soutenant les traités. Il a été conçu pour être indépendant et pour représenter l'intérêt Communautaire. Cependant il était composé de représentants nationaux (deux pour les grands états, un pour les plus petits). Un des membres était nommé Président par le Conseil. Il présidait et représentait la Commission.

Sous la Communauté, le Parlement européen (anciennement Assemblée parlementaire européenne) avait un rôle consultatif au Conseil et à la Commission. Il y avait un certain nombre de procédures législatives Communautaires, au début uniquement la procédure de consultation, ce qui signifiait que le Parlement devait être consulté, bien qu'il fût souvent ignoré. L'Acte unique européen donna plus de pouvoir au Parlement, notamment l'avis conforme qui lui donna le droit de veto sur les propositions, et la procédure de coopération qui lui conférait des pouvoirs égaux au Conseil.

En 1970 et 1975, les traités de l'Union européenne donnèrent au Parlement le pouvoir en ce qui concerne le budget de l'Union européenne. Les membres de Parlement, jusqu'à 1979, était des députés nationaux étant à temps partiel au Parlement. Le TCEE prévoyait la tenue d'élections au Parlement dès que le Conseil aurait trouvé un système de vote, mais ce fut retardé jusqu'en 1979 (voir Élections européennes de 1979). Après cela, les élections se tenaient tous les cinq ans.

La Cour de justice des Communautés européennes était la haute cour en matière de droit communautaire et était composé d'un juge par État et d'un président élu parmi eux. Son rôle était d'assurer l'application du droit communautaire de façon identique dans tous les États et d'arranger les litiges entre des institutions ou des états. Elle est devenue une institution puissante puisque les normes communautaires prévalent sur les normes nationales.

La cinquième institution est la Cour des comptes européenne. Elle s'assure que les fonds du contribuable au Budget communautaire sont correctement dépensés. La cour a fourni un rapport d'audit pour chaque exercice financier au Conseil et au Parlement et donne des avis et des propositions en ce qui concerne la législation financière et des actions anti-fraude. C'est la seule institution qui n'est pas mentionnée dans les traités initiaux. Elle en est devenue une en 1975.





</doc>
<doc id="17889" url="https://fr.wikipedia.org/wiki?curid=17889" title="Christopher Nolan">
Christopher Nolan

Christopher Edward Nolan est un réalisateur, scénariste, monteur et producteur de cinéma britanno-américain, né le à Westminster (Londres).

Il se fait connaître à la fin des années 1990 par un premier long métrage, "Following" (1998), tourné en noir et blanc. Son deuxième film, "Memento" (2000), lui apporte une renommée, qui lui permet d'obtenir pour le troisième, "Insomnia" (2002), la collaboration d'Al Pacino et Robin Williams. Il est choisi ensuite pour réaliser la trilogie des Batman : "Batman Begins" (2005), "" (2008) et "The Dark Knight Rises" (2012). Tout en travaillant sur cette saga, il met en scène "Le Prestige" (2006), et deux films de science-fiction, "Inception" (2010) et "Interstellar" (2014). 

La plupart de ses films ont été écrits avec son cadet, Jonathan Nolan. Ils ont obtenu un total de vingt-six nominations et sept récompenses aux Oscars du cinéma , et ont généré plus de 4,2 milliards de dollars de recettes à travers le monde. 

Nolan a créé avec sa femme Emma Thomas la compagnie de production Syncopy Films. Nourrie de préoccupations philosophiques, sociologiques ou éthiques, son œuvre explore la moralité humaine, la construction du temps, et la malléabilité de la mémoire et de l'identité personnelle. Elle se singularise par la présence d'éléments métafictifs, de changements temporels, de perspectives solipsistes, de narrations non-linéaires, d'effets spéciaux pratiques et de relations analogues entre le langage visuel et les éléments narratifs.

Christopher Edward Nolan est né le dans le quartier de Westminster, à Londres, d'un père britannique, Brendan James Nolan, directeur de publicité, et d'une mère américaine, Christina (née Jensen), successivement agent de bord et professeur d'anglais. Son enfance a été partagée entre Londres et Chicago, il a les deux nationalités, britannique et américaine. Il a un frère aîné, Mathew, et un frère cadet, Jonathan, créateur entre autres de la série télévisée "Person of Interest".

Il commence à faire des films à l'âge de sept ans, empruntant la caméra Super 8 de son père et filmant des figurines. Ayant grandi, Nolan, grand admirateur de "Star Wars" (1977), réalise en hommage une animation en ' nommée '. Son oncle, qui travaillait à la NASA sur le système de guidage des fusées Apollo, lui enverra la vidéo d'un lancement de fusée, avec laquelle Nolan s'amusera à faire des montages. Dès l'âge de onze ans, il aspire à être cinéaste professionnel. Lorsque sa famille a déménagé à Chicago au cours de ses années d'études, il a commencé à faire des films avec Adrien et Roko Belic. Il poursuit sa collaboration avec eux et reçoit un crédit pour son aide à la rédaction de leur documentaire nominé aux Oscars du cinéma nommé "Genghis Blue" (1999). Il a également travaillé aux côtés de Roko sur la documentation d'un safari à travers quatre pays africains, organisée par le photojournaliste Dan Eldon dans le début des années 1990.

Élève du Haileybury and Imperial Service College, une école privée à Hertford Heath dans le Hertfordshire, il a ensuite étudié la littérature anglaise au University College de Londres (UCL). Il utilisera d'ailleurs la "Flaxman Gallery" de cette même école pour tourner une scène dans son film "Inception" (2010). Il choisira l'UCL spécifiquement pour ses installations de production de films, qui comprenait une salle de montage Steenbeck et des caméras . Nolan a été président de l'Union's Film Society et avec Emma Thomas (sa future épouse et productrice) il projetait des films en durant l'année scolaire et ont utilisé tout l'argent récolté pour produire des films en pendant leurs vacances d'été. Au cours de sa scolarité, Nolan a produit deux courts-métrages. Le premier était "Tarantella" (1989), un film surréaliste tourné en et qui a été diffusé sur Image Union (une sorte de vitrine indépendante spécialisée dans les films et la vidéo appartenant à la chaîne Public Broadcasting Service). Le second était "" (1995), filmé sur un week-end en noir et blanc et avec des moyens limités. Financé par Nolan et tourné avec l'équipement de la société, ce dernier est apparu au Festival du Film de Cambridge en 1996 et est considéré comme l'un des meilleurs courts-métrages de l'UCL. Après l'obtention de son diplôme, Nolan a dirigé des vidéos d'entreprises et des films industriels. Il a également fait un troisième court-métrage, "Doodlebug" (1997), l'histoire d'un homme chassant un insecte autour d'un plat avec une chaussure, pour finalement découvrir que c'est une miniature de lui-même. À cette période de sa carrière, Nolan n'a quasiment aucune chance de faire voir le jour à ses projets cinématographiques : il appellera cette période « la pile de lettres de refus », ce qui marqua sa vision de l'industrie cinématographique. Il critiquera d'ailleurs le manque d'investissements dans le cinéma en Grande-Bretagne ainsi que le manque d'ouverture d'esprit des investisseurs.

En 1998, Nolan réalise son premier long-métrage, "Following", qu'il a lui-même financé avec quelques amis. Le film raconte l'histoire d'un jeune romancier en panne d'inspiration, qui, pour pallier son manque d'imagination, suit des inconnus choisis au hasard dans les rues de Londres. Ne parvenant pas à garder ses distances, il est progressivement aspiré dans un milieu criminel. Le film est inspiré de l'expérience de Nolan pour la vie à Londres et qui a vu son appartement cambriolé : « Il existe un lien intéressant entre l'étranger qui prend vos biens et le concept de suivre les gens au hasard dans une foule, les deux étant peu ordinaires pour des relations sociales. » "Following" a été réalisé avec un budget très modeste de 3000 livres sterling et a été tourné sur plusieurs week-ends au cours d'une année. Pour économiser la pellicule, chaque scène dans le film a été préalablement répétée de façon intensive afin de ne faire qu'une ou deux prises pour le montage final. Co-produit avec Emma Thomas et l'interprète principal, Jeremy Theobald, écrit, photographié et monté par Nolan lui-même, "Following" a remporté de nombreuses récompenses et a été plutôt bien reçu par la critique, dont celle du "New Yorker", qui écrit qu'il « fait écho aux classiques de Hitchcock… en plus subtil et plus méchant ». Le 11 décembre 2012, le film est sorti en DVD et Blu-Ray et a intégré la Criterion Collection.

Le succès de "Following" permet à Nolan de réaliser un projet plus ambitieux ; ce sera "Memento" (2000). Au cours d'un voyage sur la route de Chicago à Los Angeles, son frère Jonathan a lancé l'idée de ""Memento Mori"" racontant l'histoire d'un homme atteint d'une amnésie antérograde, qui utilise des notes et des tatouages pour traquer l'assassin de sa femme. Nolan développe alors un scénario et a l'idée de raconter l'histoire à l'envers. Aaron Ryder, un cadre de chez Newmarket Films a dit que c'était "peut-être le script le plus innovant que j'ai jamais vu". Le film a été doté d'un budget de 4,5 millions de dollars, une somme énorme pour Nolan : "Tourner "Following" avec des amis portant nos propres vêtements et ma mère qui leur fait des sandwichs pour passer à un budget de 4 millions de dollars payés par quelqu'un d'autre avec "Memento" et avoir une équipe d'une centaine de personnes est, à ce jour, le plus grand saut que j'ai jamais fait." "Memento", mettant en scène Guy Pearce et Carrie-Anne Moss, est diffusé pour la première fois en septembre 2000 au Festival international du film de Venise et est acclamé par la critique. Joe Morgenstern écrit dans sa critique du Wall Street Journal : "Je ne me souviens pas qu'un film ait paru aussi intelligent, étrangement touchant et sournoisement drôle à la même époque. Basil Smith, dans le livre "The Philosophy of Neo-Noir", dessine une comparaison avec l'Essai sur l'entendement humain de John Locke, qui fait valoir que nos souvenirs conscients constituent notre identité, un thème que Nolan explore dans le film. Le film, ayant totalisé près de 40 millions dollars au box-office, est un grand succès. Ce dernier a reçu un certain nombre de distinctions, notamment les nominations aux Oscars et aux Golden Globes pour le meilleur scénario, mais aussi deux lauréats au Independant Spirit Award pour le meilleur réalisateur et le meilleur scénario, ainsi qu'une nomination à la Directors Guild of America. "Memento" a été considéré par de nombreux critiques comme l'un des meilleurs films des années 2000.

Impressionné par son travail sur "Memento", Steven Soderbergh a recruté Nolan pour réaliser le thriller psychologique "Insomnia" (2002). Le film met en scène les vedettes oscarisées Al Pacino, Robin Williams et Hilary Swank. Warner Bros. voulait initialement un réalisateur plus expérimenté, mais Soderbergh et la Section Eight Productions se sont battus pour choisir la réalisation (Nolan), la photographie (Wally Pfister) et l'éditeur (Dody Dorn). Avec un budget de 46 millions de dollars, il a été décrit comme "un film de Hollywood plus conventionnel que ce que le réalisateur avait fait avant". Remake d'un film norvégien du même nom sorti en 1997, "Insomnia" raconte l'histoire de deux policiers de Los Angeles envoyés dans une ville d'Alaska pour enquêter sur le meurtre méthodique d'une adolescente. Les critiques sont globalement positives et le film obtient de bons résultats au box-office, récoltant près de 113 millions de dollars dans le monde entier. Le critique de film Roger Ebert félicita le film pour son introduction de nouvelles perspectives et idées sur les questions de la morale et de la culpabilité : "Contrairement à beaucoup de remakes, le "Insomnia" de Nolan n'est pas une pâle reprise, mais un réexamen de la matière, comme une nouvelle production d'un bon jeu". Erik Skjoldbjærg, le réalisateur du film original, était satisfait de la version de Nolan, qualifiant le film "intelligemment conçu, avec une bonne direction du réalisateur". Richard Schickel du "Time" estima que "Insomnia" était "un digne successeur" pour "Memento" et que c'était "un triomphe de l'atmosphère non-trop mystérieuse du mystère". Après "Insomnia", Nolan avait planifié un film biographique sur Howard Hughes mettant en vedette Jim Carrey. Il avait écrit un scénario, mais quand il apprit que Martin Scorsese était déjà en train de réaliser un biopic sur ce même personnage ("Aviator" sorti en 2004), il abandonna le scénario à contrecœur et se dirigea vers d'autres projets. Après avoir refusé une offre pour réaliser le film historique "Troie" (2004), Nolan a travaillé sur l'adaptation du roman "The Keys to the Street" de Ruth Rendell dans un scénario qu'il avait l'intention de réaliser pour la Fox Searchlight Pictures, mais a finalement abandonné le projet en citant les similitudes avec ses précédents films. 

En parallèle, il fonde et gère avec son épouse sa propre société de production Syncopy Films, qui va, à partir de 1998, participer à la coproduction de tous ses films.

Au début de l'année 2003, Nolan a approché Warner Bros. avec l'idée de faire un nouveau film Batman. Fasciné par le personnage et son histoire, il a voulu faire un film dans un monde « racontable » rappelant plus un drame classique qu'un fantasme de bande dessinée. "Batman Begins" (2003) était le plus grand projet entrepris par Nolan depuis le début de sa carrière. Sorti dans les salles en juin 2005, ce fut un succès critique et commercial. Avec pour vedettes Christian Bale dans le rôle principal, ainsi que Michael Caine, Liam Neeson, Gary Oldman, Morgan Freeman ou encore Cillian Murphy, le film a relancé la franchise, annonçant un "reboot" sombre de la saga. Il raconte les origines de l'histoire du personnage : Bruce Wayne qui a peur des chauve-souris, la mort de ses parents, son épopée pour devenir Batman et enfin son combat contre Ra's al Ghul qui souhaite détruire Gotham City. Il est notamment applaudi pour sa profondeur psychologique et sa pertinence contemporaine. "Batman Begins" était le huitième plus grand succès cinématographique de l'année 2005 aux États-Unis et le neuvième dans le monde. Il a été nommé aux Oscars pour la meilleure photographie et a également remporté trois BAFTA Awards. Un article publié dans le magazine "Forbes" à l'occasion du dixième anniversaire du film, en décrit l'influence sur la production postérieure : « Le terme "reboot" fait maintenant partie de notre vocabulaire courant, les films d'origine de super-héros ont surfé sur la vague du sombre et du dur, et nous avons maintenant une nouvelle perception de narration d'une adaptation en film d'une bande dessinée. » 

Avant de revenir à la trilogie Batman, Nolan a réalisé, co-écrit et produit "Le Prestige" (2006), adaptation d'un roman de Christopher Priest qui raconte l'histoire de deux prestidigitateurs rivaux au . En 2001, lorsque Nolan était en post-production pour "Insomnia", il demanda à son frère Jonathan Nolan de l'aider à écrire le script pour le film. Le scénario était en fait né de la collaboration entre les deux frères depuis cinq ans. Nolan initialement destiné à faire le film dès 2003, le projet est reporté après qu'il a accepté de réaliser "Batman Begins". Dans le film, nous pouvons voir les acteurs Hugh Jackman, Christian Bale ou encore Scarlett Johansson. "Le Prestige" a reçu des retours élogieux (y compris des nominations aux Oscars pour la meilleure photographie et la meilleure direction artistique) et à généré 109 millions de dollars dans le monde entier. Avec un conte sombre et mouvementé, Roger Ebert a décrit le film comme « atmosphérique, obsessionnel, presque satanique ». Pour le Los Angeles Times, le film est un mélodrame déconcertant et ambitieux, qui donne à comprendre « le prix à payer pour l'immortalité dans un domaine créatif ». Après la sortie du "Prestige", Nolan envisageait de réaliser une adaptation cinématographique de la série télévisée britannique "The Prisoner", mais il finit par abandonner le projet. 

En juillet 2006, Nolan a annoncé que la suite de "Batman Begins" s'intitulera (2008). Le film raconte l'histoire de Batman essayant d'arrêter le personnage du Joker (joué par Heath Ledger) dans sa frénésie criminelle. En faisant cette suite, Nolan a voulu insister sur la noirceur du premier film et raconter « la dynamique de l'histoire d'une ville, avec une grande organisation criminelle et dans laquelle vous êtes à la recherche de la police, du système judiciaire, d'un justicier, des criminels, des riches et des pauvres ». Sorti en 2008, très acclamé par la critique, "" a été considéré comme l'un des meilleurs films des années 2000, et, plus encore, comme l'un des meilleurs films de super-héros de l'histoire du cinéma. Le "New York Times" écrit que, d'un point de vue artistique, le film est supérieur à beaucoup de blockbusters : « Situé entre l'art et l'industrie, entre la poésie et le divertissement, il est plus sombre et plus profond que tous les autres films hollywoodiens dans le genre comic-book. » Roger Ebert le décrit de même comme « un film hanté, qui saute au-delà de ses origines et devient un drame captivant ». Le cinéaste Kevin Smith l'a surnommé « "Le Parrain" [Partie 2] des films de super-héros ». Le film a enregistré, au cours de son exploitation, un certain nombre de records au box-office, gagnant 535 millions de dollars en Amérique du Nord et 470 à l'étranger. C'est le premier long-métrage tourné, au moins en partie, avec le format 15/ des caméras IMAX. À la Cérémonie des Oscars, il a été nommé huit fois et a remporté deux prix : celui du meilleur montage sonore et, à titre posthume, celui du meilleur acteur dans un second rôle pour Heath Ledger. 

Nolan est reconnu par ses pairs après ses nominations aux Directors Guild of America (DGA), Writers Guild of America (WGA) et Producers Guild of America (PGA). 

Après le triomphe de "The Dark Knight", Nolan signe un contrat avec Warner Bros. pour l'écriture, la coproduction et la réalisation d"'Inception" (2010), qu'il décrit comme « un film d'action avec de la science-fiction contemporaine mis dans l'architecture de l'esprit ». Avant sa sortie en salle, certains critiques se demandaient si la complexité intellectuelle des films de Nolan n'allaient pas amputer les résultats au box-office. Avec un casting prestigieux mené par Leonardo DiCaprio, le film est sorti le 16 juillet 2010, et a été un succès critique et commercial. Le "Chicago Sun-Times" décerne au film un "A+" et le consacre comme « l'un des meilleurs films du siècle" ». Pour Mark Kermode, il s'agit du meilleur de l'année 2010 : « "Inception" est la preuve que les gens ne sont pas stupides, que le cinéma n'est pas "trash", et qu'il est possible pour les blockbusters et l'art d'être la même chose. » Le très respecté producteur John Davis a parié que le succès du film pourrait inspirer des studios pour rendre leur contenu plus original : « Je peux vous promettre que les chefs de studios sont déjà en pleine réunion pour rassembler des idées originales pour leurs films d'été. » Le film finit par générer plus de 820 millions de dollars à travers le monde et a été nommé pour huit Oscars, dont celui du meilleur film. Il a remporté les Oscars de la meilleure photographie, du meilleur mixage de son, du meilleur montage de son et des meilleurs effets visuels. Le 20 juillet 2012, veille de la présentation de "The Dark Knight Rises" à Paris, une fusillade a lieu pendant une avant-première du film dans une salle de cinéma d'Aurora (Colorado). La visite à Paris est annulée. 

Au cours de discussions sur le scénario de "The Dark Knight Rises" en 2010, Goyer indique à Nolan son idée de remettre Superman dans un contexte moderne. Impressionné par le concept de premier contact proposé par Goyer, Nolan lance alors l'idée de "Man of Steel" (2013) à Warner Bros, qui a alors embauché Nolan pour la production et Goyer pour l'écriture. Nolan propose alors Zack Snyder pour réaliser le film, en se basant sur ses adaptations stylisées comme "300" (2007) ou (2009) ainsi que son "aptitude innée pour traiter les héros comme des personnages réels". Mettant en vedette Henry Cavill, Amy Adams, Kevin Costner, Russel Crowe et Michael Shannon, "Man of Steel" a rapporté plus de 660 M $ au box-office, mais a engrangé une réaction critique divisée. Nolan et Thomas ont été producteurs exécutifs sur le film "Transcendance" (2014), le tout premier film de Wally Pfister, le chef opérateur de longue date de Nolan. Sur la base d'un scénario de Jack Paglen, le film tourne autour de deux scientifiques qui travaillent à créer une machine qui possède une intelligence artificielle très développée, et qui va entraîner la singularité technologique. Avec les acteurs Johnny Depp, Rebecca Hall et Paul Bettany, "Transcendance" est sorti en salles le 18 avril 2014 et a été un échec critique et commercial. AA Dowd, du site d'informations The A.V. Club, a donné au film la note de C en indiquant que "[Pfister] n'a pas le talent de Nolan pour tisser un grand spectacle pop mêlé à des anxiétés culturelles". 

En janvier 2013, il est annoncé officiellement que Nolan va réaliser, écrire et produire un film de science-fiction intitulé "Interstellar". Les premiers jets du scénario ont été écrits par Jonathan Nolan, et le film était censé être dirigé par Steven Spielberg à la base. Sur la base des travaux scientifiques du célèbre physicien Kip Thorne, le film dépeint "un voyage interstellaire héroïque aux frontières les plus lointaines de notre compréhension scientifique". On peut apercevoir dans "Interstellar" les acteurs Matthew McConaughey, Anne Hathaway, Michael Caine, Matt Damon, Jessica Chastain ou encore Mackenzie Foy, et c'est également la première collaboration de Nolan avec le directeur de la photographie Hoyte van Hoytema. Paramount Pictures et Warner Bros. ont co-financé et co-distribué le projet, sorti en salles le 5 novembre 2014. Le film a reçu des critiques positives et a engrangé de solides résultats au box-office : près de 670 M $ de recettes dans le monde entier. "Interstellar" a été particulièrement félicité pour sa précision scientifique, qui a conduit à la publications de deux articles scientifiques et au fait que l'American Journal of Physics recommande désormais aux écoles de montrer le film pendant les cours de sciences. Il a été nommé parmi les meilleurs films de l'année par l'American Film Institute (AFI). À la 87e cérémonie des Oscars, le film a remporté l'Oscar des meilleurs effets visuels et a reçu quatre autres nominations (Musique originale, Mixage son, Montage sonore, Décors). Nolan a organisé le court-métrage "Emic : A Time Capsule From The People Of Earth" (2015) qui a été spécialement inspiré d"'Interstellar" et "tente de saisir et de célébrer l'expérience humaine sur Terre". 

En 2015, la société de production de Nolan Syncopy forme une coentreprise avec Zeitgeist Films, dans l'optique de sortir des éditions Blu-Ray des films de Zeitgeist les plus prestigieux. Leur premier projet commun a été la sortie en Blu-Ray d'"Elena" (2011) du réalisateur Andreï Zviaguintsev. À l'occasion de la sortie en Blu-Ray de tout le travail des frères Quay, Nolan réalise le court documentaire "Quay". Il a également lancé une tournée cinématographique à l'honneur des deux frères, en ressortant notamment "In Absentia", "The Comb" ou encore "Street of Crocodiles" dans certaines salles de cinéma. Les initiatives ainsi que le court-métrage de Nolan ont été salués par la critique. "Indiewire" écrit d'ailleurs dans sa critique que les frères "auront des centaines, si ce n'est des milliers de fans en plus grâce à Nolan, et que "The Quay Brothers" en sera toujours l'une des contributions les importantes de ce dernier au cinéma". Toujours en 2015, Nolan rejoint le conseil d'administration de la Film Foundation, une association dédiée à la préservation des films. En mars 2015, après avoir contribué à la préservation de vieux films, Nolan a invité une trentaine de représentants de l'American Film Archives à assister au sommet "Reframing the Future of Film" au Getty Museum. 

En décembre 2015, les frères Nolan sont vus à Dunkerque en train de faire des repérages pour le film "Dunkirk", qui conte l'évacuation de Dunkerque en mai 1940. La sortie a eu lieu le 19 juillet 2017 en Europe et le 21 juillet 2017 aux États-Unis.

À l'âge de dix-neuf ans, Christopher Nolan a fait la rencontre d'Emma Thomas au University College de Londres. Ils se sont mariés en 1997 et ont eu quatre enfants : Magnus, Oliver, Savannah et Rory. Emma Thomas a travaillé en tant que productrice sur tous les films de son mari. En 2001, après le succès de "Memento", ils ont décidé de fonder leur propre société de production, nommée Syncopy Films (en référence à la syncope, ou perte de conscience). Ils habitent aujourd'hui à Los Angeles. Désireux de protéger sa vie privée, Nolan ne l'évoque que rarement dans ses interviews.

Il a cependant révélé dans un entretien que s'il n'avait pas été père, le film "Interstellar", dans lequel les liens familiaux sont fortement évoqués, n'aurait sans doute jamais vu le jour.

Nolan ne possède ni téléphone portable ni adresse e-mail : « Ce n'est pas que je sois luddiste ou que je déteste la technologie, mais ça ne m'a jamais intéressé [...] Quand j'ai emménagé à Los Angeles, en 1997, personne ou presque n'avait de portable, et depuis j'ai continué. » Nolan interdit l'usage des mobiles sur ses tournages.

Christopher Nolan a cité Stanley Kubrick, Terrence Malick, Orson Welles, Fritz Lang, Sidney Lumet, David Lean, Ridley Scott, Terry Gilliam et John Frankenheimer comme influences. Les films que Nolan préfère le plus à titre personnel sont "Lawrence d'Arabie" (1962), "2001 : l'Odyssée de l'espace" (1968), "Chinatown" (1974), "L'homme qui voulut être roi" (1975), "Star Wars" (1977) et "Blade Runner" (1982). En 2013, Criterion Collection a publié une liste des dix films que Nolan préfère dans leur catalogue, qui comprend "Greed" ("Les Rapaces", 1924) de Erich von Stroheim, "Le Testament du docteur Mabuse" (1933), "Dossier secret" (1955), "Douze hommes en colère" (1957), "Enquête sur une passion" (1980), "Koyaanisqatsi" (1982), "Furyo" (1983), "The Hit" (1984), "For All Mankind" (1989) et "La Ligne rouge" (1998).

L'habitude de Nolan pour l'emploi de scénarios non-linéaires a été particulièrement influencée par le roman de Graham Swift nommé "Waterland", qui "a fait des choses incroyables avec des timelines parallèles, et a raconté une histoire dans différentes dimensions qui était extrêmement cohérente". Il a également été influencé par le langage visuel du film "The Wall" (1982) et la structure de "Pulp Fiction" (1994), déclarant qu'il était "fasciné par ce que Quentin Tarantino avait fait". Le récit de Dante Alighieri intitulé "Enfer" a servi comme influence au film "Inception" (2010) notamment pour ses références au Minotaure et au Labyrinthe. Quant à "Interstellar" (2014), Nolan a cité des influences littéraires, notamment "Flatland" par Edwin Abbott Abbott, "The Wasp usine" par Iain Banks et "" de Madeleine L'Engle. Nolan a aussi de nombreuses autres influences comme le peintre anglais Francis Bacon, l'architecte allemand Ludwig Mies van der Rohe, l'artiste graphique MC Escher et les auteurs Raymond Chandler, James Ellroy, Jim Thompson, Jorge Luis Borges et Charles Dickens "Le Conte de deux cités" a beaucoup influencé "The Dark Knight Rises" (2012).

Au fur et à mesure de sa carrière, Christopher Nolan s'est entouré d'une équipe de collaborateurs. Sa femme Emma Thomas a été productrice de tous ses films. Aaron Ryder, William Tyrer et Chris J. Ball ont produit "Memento" et "Le Prestige" ; tandis que Michael E. Uslan, Benjamin Melniker et Charles Roven ont produit ses trois "Batman".

Pour ses scénarios, Christopher Nolan travaille avec son frère Jonathan Nolan avec qui il a rédigé "Memento", "Le Prestige", "" et "The Dark Knight Rises". David S. Goyer, le scénariste de "Batman Begins" participera également à l'écriture des deux autres volets de la trilogie.

Wally Pfister a été le directeur de la photographie de tous ses films excepté "Doodlebug", "Following" et "Interstellar".

Peter Lando a été le chef décorateur d’"Insomnia" et de "The Dark Knight : Le Chevalier noir".

David Julyan a composé des musiques pour tous ses films jusqu'à la trilogie Batman, composée par James Newton Howard et Hans Zimmer. Tous ses films suivants ont été composés par Zimmer.

Dody Dorn a monté "Memento" et "Insomnia", tandis que Lee Smith a monté "Batman Begins", "Le Prestige", "The Dark Knight : Le Chevalier noir", "Inception", "The Dark Knight Rises" et "Interstellar".

De plus, il travaille régulièrement avec certains acteurs : 


Toutes organisations officielles confondues, Christopher Nolan a reçu 38 nominations pour 19 lauréats. Son film le plus récompensé est "Inception" (2010) avec 18 nominations pour 7 lauréats. Arrive en seconde position "Memento" (2000) avec 9 nominations et 5 lauréats suivi en troisième position par (2008) avec 6 nominations pour 2 lauréats. Enfin, "Batman Begins" (2005) obtient 3 nominations pour 2 lauréats et "Interstellar" (2014) fait carton plein en recevant 2 nominations pour 2 lauréats.




</doc>
<doc id="17891" url="https://fr.wikipedia.org/wiki?curid=17891" title="Couche présentation">
Couche présentation

La couche présentation est la du modèle OSI.

La couche présentation est chargée du codage des données applicatives. Les couches 1
à 5 transportent des octets bruts sans se préoccuper de leur signification. Mais ce
qui doit être transporté en pratique, c'est du texte, des nombres et parfois des
structures de données arbitrairement complexes. Un protocole de routage par exemple
doit transporter un graphe représentant au moins partiellement la topologie du
réseau. Le rôle de la couche présentation est donc de convertir les données
applicatives manipulées par les programmes en chaînes d'octets


Dans le monde ISO, la règle consiste à définir les données en ASN.1 ("Abstract Syntax Notation") et à réaliser dans la couche de présentation le codage/décodage en BER ou DER.

Dans le monde IP, historiquement, la méthode canonique et implicite est de tout transformer en texte: codage des nombres entiers en décimal, non-utilisation des flottants, utilisation de délimiteurs comme le guillemet ou la marque de fin de ligne (octets 13 et 10) avec des mécanismes d'échappements compliqués pour les cas où le délimiteur apparaît dans les données.
Ces transformations sont spécifiées, et souvent répétées, pour chaque protocole applicatif, puisque le «modèle» IP ne contient pas de couche de présentation.

Cela ne résout le problème qu'à moitié puisque le codage des caractères en octets n'est pas défini. Soit on utilise le codage US-ASCII en ignorant les langues utilisant des caractères non ASCII (cf. par exemple le protocole SMTP

Lorsque cela s'est avéré insuffisant au vu de la complexité de la structure des données à transporter, notamment pour le protocole applicatif Remote Procedure Call de Sun Microsystems, on a inventé XDR ("External Data Representation"),
jusqu’à l’introduction de XML en 1998, dont la vocation était précisément d’étendre le mode de représentation SGML conçu pour des fichiers de type « document » (consommables par des humains) à des jeux d’éléments de données (à traiter par des machines) transitant sur Internet.

On notera qu'aucune de ces représentations n'est un protocole proprement dit.
Cela illustre le fait que s'il n'y a pas de négociation du mode de codage, tout mécanisme utilisé pour l’enregistrement peut aussi faire office de fonction de présentation.




</doc>
<doc id="17893" url="https://fr.wikipedia.org/wiki?curid=17893" title="Fréquence">
Fréquence

En physique, la fréquence est le nombre de fois qu'un phénomène périodique se reproduit par unité de mesure du temps. Son unité dans le Système international d'unités est le hertz (Hz).

La notion de fréquence s'applique aux phénomènes périodiques ou non. L'analyse spectrale transforme la description d'un phénomène en fonction du temps en description en fonction de la fréquence.

Dans plusieurs domaines technologiques, on parle de fréquence spatiale. Dans cet usage, une dimension de l'espace prend la place du temps. S'il existe une variation périodique dans l'espace, la fréquence spatiale est l'inverse de la distance minimale à laquelle on retrouve la forme identique, par exemple en imprimerie la linéature. Qu'il en existe ou pas, on peut appliquer à l'espace les règles de l'analyse spectrale, comme on le fait dans les systèmes de compression numérique des images. Dans le cas des ondes progressives, la fréquence spatiale ou nombre d'onde est le quotient de la fréquence par la vitesse de l'onde.

La pulsation d'un phénomène périodique est la valeur de la vitesse de rotation qu'aurait un système en rotation de même fréquence : pour une fréquence "f", la pulsation est donc ω = 2π."f" (rad/s).

La fréquence, dans ce qu'elle a de plus accessible intuitivement, mesure un phénomène périodique. Plus le phénomène est fréquent, plus sa fréquence est grande.

Inversement, pour mesurer le temps, on fait appel à des phénomènes périodiques qu'on sait stables.

C'est ainsi que le Système international d'unités définit la seconde comme .

En conséquence, on peut définir une fréquence comme le rapport entre deux unités de temps différentes, exprimée en général par le nombre d'unités de l'une pour une de l'autre.

La décomposition en série de Fourier montre que tout signal décrivant un phénomène périodique peut se décomposer en une somme de sinusoïdes, dont la fréquence est un multiple entier de la fréquence du phénomène. La transformation de Fourier étend le concept de série de Fourier à des phénomènes non-périodiques ː elle permet de passer de la description d'un phénomène en fonction du temps à sa description en fonction des fréquences qu'il contient, appelée spectre de fréquences, et inversement. La transformation de Fourier est un procédé mathématique qui suppose que la valeur qui décrit le phénomène est connue à chaque instant. De même, elle suppose que les valeurs de la fréquence peuvent être quelconques, de moins l'infini à plus l'infini. Elle connaît donc des fréquences négatives.

Les phénomènes ont à la fois une extension dans le temps, entre un début et une fin, et une dimension fréquentielle, dans la mesure où ils se répètent périodiquement entre ce début et cette fin. On peut les décrire par l'évolution de leur amplitude dans le temps, ou par les fréquences de leur spectre.

Une description temporelle ne contient aucune information fréquentielle ; une description fréquentielle ne contient aucune information temporelle. La transformation suppose qu'on connaisse le signal à l'infini.

Pour décrire adéquatement un phénomène, on peut le découper dans le temps en segments dont on puisse déterminer à peu près le . La relation d'incertitude 
formula_1
décrit le fait que plus la durée Δ"t" du segment est longue, et donc plus l'incertitude sur la durée est grande, plus l'incertitude sur la fréquence Δ"f" est faible, et vice-versa.

Cette approche mathématique décrit avec précision des faits connus de l'expérience. Pour définir avec précision une fréquence, il faut observer l'oscillation pendant une longue durée. C'est ainsi que l'horloger, pour régler la fréquence du balancier, doit observer la pendule, qui compte ces oscillations, pendant une longue durée. En procédant ainsi, il obtient la moyenne de la durée des balancements, mais perd toute information sur les éventuelles irrégularités. Inversement, en observant le mouvement pendant une brève période, en soumettant l'horloge à divers mauvais traitements comme le remontage du ressort, des courants d'air ou des vibrations, il reconnaît leur conséquence éventuelle sur le balancement, mais n'acquiert aucune notion précise de sa fréquence. En acoustique musicale, on a depuis longtemps remarqué qu'on ne peut définir la tonie des sons brefs. Identifier un ton implique de discriminer précisément une fréquence fondamentale, ce qui n'est possible qu'avec un minimum de temps d'écoute.

La pulsation d'un phénomène périodique est la valeur de la vitesse de rotation, ou vitesse angulaire, qu'aurait un système en rotation de même fréquence : pour une fréquence formula_2 en hertz, la pulsation associée est donc formula_3 ː son unité SI est le radian par seconde (rad·s).

La pulsation est parfois nommée « fréquence angulaire », par traduction littérale de l'anglais « "angular frequency" » ː ce terme est fréquemment employé dans des ouvrages traduits d'auteurs anglophones et est déconseillé par de nombreux auteurs francophones.

L'analogie avec un système mécanique en rotation est intéressante car la description mathématique est très similaire à celle d'une grandeur évoluant de façon sinusoïdale formula_4, où formula_5 est l'amplitude, "formula_6" la vitesse angulaire, "formula_2" la fréquence et "formula_8" le temps. La différence avec une véritable vitesse de rotation est que le phénomène décrit n'est pas une rotation, mais une variation périodique ; la rotation n'est pas ici une rotation physique, mais est celle de la phase dans l'espace réciproque. 

Les coordonnées d'un point en rotation dans un plan décrivent en fonction du temps des sinusoïdes de la forme

où formula_10 est l'abscisse, formula_11 est l'ordonnée. Dans de nombreux domaines de la physique, il est plus simple d'utiliser les nombres complexes avec "x" comme partie réelle et "y" comme partie imaginaire. D'après la formule d'Euler, formula_12, où formula_13 est la constante de Neper, base du logarithme naturel et formula_14 l'unité imaginaire. Avec cette notation, la formule de Moivre permet de traiter avec élégance les élévations à une puissance, nécessaires dans de nombreux calculs. La position d'un point en rotation en fonction du temps peut ainsi s'écrire

où "formula_16" est un nombre complexe dont la partie réelle est l'abscisse et la partie imaginaire l'ordonnée du point. Cette notation est fréquente en analyse spectrale. Dans certains cas, seule la partie réelle a un sens physique, dans d'autres, c'est l'amplitude qui peut porter l'information. 

Quand le phénomène périodique est une onde, la fréquence temporelle et la longueur d'onde sont liées par la vitesse de propagation (célérité) de l'onde.
formula_17
où "f" est la fréquence de l'onde (en hertz), "c" la célérité de l'onde (en mètres par seconde) et formula_18, la longueur d'onde (en mètres).

Le rayonnement électromagnétique peut se définir soit en termes d'onde de propagation d'une perturbation électromagnétique à la vitesse de la lumière, caractérisée par une fréquence et dont l'énergie dépend de l'amplitude, soit en termes de particules sans masse appelées photon, se déplaçant à la vitesse de la lumière.

Dans ce contexte, on désigne la fréquence par la lettre grecque formula_19 ("nu").

L'énergie d'un photon est proportionnelle à la fréquence :
où formula_21 est la constante de Planck.

En électromagnétisme, physique quantique et relativité, on désigne la fréquence par formula_22, la lettre nu de l'alphabet grec. On y parle aussi de fréquence pour la quantité formula_23, avec la lettre grecque oméga.

Dans la technologie et l'ingénierie, on utilise plus couramment la lettre "f", et on appelle la grandeur 2π"f" "pulsation" ou "vitesse angulaire". 

Dans le Système international d'unités dit SI, l'unité de temps est la seconde dont le symbole est s. La fréquence est alors en hertz dont le symbole est Hz (unité SI), et on a = 1 s.

Le hertz ne s'utilise que pour les signaux périodiques. Lorsque le compte d'occurrences par seconde concerne un phénomène aléatoire, on le note explicitement ; par exemple en physique statistique ou en thermodynamique, on compte les « collisions par seconde ». Ainsi, le nombre de désintégrations d'un radionucléide par seconde, représentant son activité, s'exprime en becquerels, et non en hertz.

En mécanique, en médecine, en musique, et en général dans des domaines où la mesure de la fréquence ne sert qu'à des comparaisons, on exprime souvent la fréquence « par minute » : tours par minute (voir vitesse angulaire), pouls en battements par minute, comme la graduation du métronome.

Dans le domaine de la physique ondulatoire on parlera d'une fréquence :


Dans le traitement du signal numérique, la fréquence d'échantillonnage détermine la bande passante admissible pour le système.

Dans les technologies numériques synchrones, les circuits communiquent entre eux en suivant un signal d'horloge dont la fréquence détermine les capacités de transfert du système, toutes choses étant égales par ailleurs.

Un fréquencemètre est un instrument de laboratoire destiné à mesurer la fréquence de signaux électriques périodiques simples. L'appareil détecte les occurrences d'une transition caractéristique de ces signaux, et compare leur fréquence à celle d'un oscillateur aussi stable que possible appelé "base de temps" :

La musique se caractérise par un déroulement assez régulier dans le temps ; les notes reviennent à des instants particuliers. La fréquence de ces instants est déterminée par une grandeur appelée "tempo", qui est une fréquence exprimée en battements par minute.

En musique, les sons sont caractérisés par la hauteur, une perception dont on a depuis l'Antiquité remarqué qu'elle correspond à la longueur des cordes ou des tuyaux des instruments de musique, dont l'étude est à l'origine de l'acoustique.

La théorie de la musique résume ces recherches en affirmant :

Les recherches psychoacoustiques ont montré le caractère schématique de cette définition, mais la correspondance entre la fréquence fondamentale d'un son et la perception d'une hauteur est indiscutée.

Le solfège note les hauteurs sur la portée ; on peut aussi indiquer une note de musique par son nom, avec éventuellement une altération, en précisant l'octave.

Le diapason le plus courant fixe la fréquence du « la » de la troisième octave à la fréquence fondamentale de .

Selon le modèle présenté par la théorie de la musique, les intervalles musicaux correspondent à des rapports harmoniques, c'est-à-dire que le quotient des fréquences est proche de rapports de nombres entiers : l'octave correspond à un rapport 2, la quinte juste à un rapport de 3/2, la tierce majeure à un rapport de 5/4

Il est ainsi possible de déterminer les fréquences fondamentales des notes musicales de proche en proche. Une fréquence doublée donne une octave, tandis qu'ajouter à une fréquence celle de son octave inférieure donne une quinte. Ensuite, l'addition d'une fréquence de 2 octaves inférieures donne une tierce majeure :
Ces intervalles sont purs et non-tempérés. En tempérament égal, les fréquences des demi-tons sont espacés régulièrement en progression géométrique dans une octave. La petite différence entre ces calculs peut s'exprimer en cents. 

Les humains perçoivent les sons de quelques hertz à , mais la plage dans laquelle une personne entraînée peut distinguer les tons s'étend d'environ à environ . Hors de ces limites, qui correspondent au registre du piano, la sensation de hauteur est de moins en moins précise.


</doc>
<doc id="17897" url="https://fr.wikipedia.org/wiki?curid=17897" title="Joseph Henry">
Joseph Henry

Joseph Henry (né le à Albany (New York), mort le à Washington) est un physicien américain qui découvrit l'auto-induction et le principe de l'induction électromagnétique des courants induits.

En 1831, il créa l'unité de mesure d'induction électrique qui fut nommée le henry en son honneur. Henry expérimenta et améliora l'électroaimant, inventé en 1823 par l'Anglais William Sturgeon. Dès 1829, il avait développé des électroaimants d'une grande puissance de levée. En 1831, il fabriqua le premier télégraphe électromagnétique opérationnel. Henry conçut et construisit également l'un des premiers moteurs électriques…

En 1847, alors qu'il était secrétaire de l'Institut Smithsonian des États-Unis, il instaure un système d'observations météorologiques. Les bulletins télégraphiques de tous les observatoires du pays sont centralisés à l'institut, et les informations analysées tous les jours. Une grande carte est établie, et un bulletin est adressé au "Washington Evening Post".

Il a effectué de nombreux travaux sur l'électromagnétisme.

Il a découvert le courant de rupture.

Il a perfectionné l'électroaimant.



</doc>
<doc id="17898" url="https://fr.wikipedia.org/wiki?curid=17898" title="Harnes">
Harnes

Harnes est une commune française située dans le département du Pas-de-Calais en région Hauts-de-France. Elle fait partie de la Communaupole de Lens-Liévin (communauté d'agglomération). La Compagnie des mines de Courrières y a exploité les fosses 9 - 17 et 21 - 22.

Ses habitants sont appelés les "Harnésiens". Leur nom jeté est "Les Claquots".

Harnes : d'abord Hamas, Harnis, enfin Harnes, au . Il paraît que le nom proviendrait du flamand "Hearn" : Marais.
Aussi loin que l'on remonte dans le temps, bien avant que naisse une bourgade, les lieux étaient couverts de forêts géantes. Puis après maints bouleversements, les eaux formèrent des lacs et des étangs, et les marais apparurent.

"Harne" en flamand.

"Article de fond : Histoire du Nord-Pas-de-Calais" Harnes possède une histoire longue et riche. Au cours des siècles, que d'invasions et de pillages, de renaissances et de reconstructions.

Premières huttes installées sur pilotis. Pêche, chasse. Alors abondaient le cerf, le chevreuil, l'ours, le loup, le sanglier. On dit que les lieux étaient déjà habités avant l'âge de la pierre polie.

Les Celtes, venus de Perse et d'Asie Centrale, apportent le bronze, le cuivre et le fer.

Harnes, pendant la période gauloise, fait partie de l'Atrébatie.

De la période gallo-romaine, ont été mis au jour, hors du terrain bourbeux, des fragments de vaisselle, des urnes cinéraires, des vases, des lances, des objets en fer, de nombreux ossements. Dans le musée municipal de Harnes, on peut voir, entre autres merveilles, le « trésor de Harnes » : des pièces de monnaie du , enfouies au bord de la rivière, des matériaux de construction d'une ferme gallo-romaine, et ces fameuses poteries rougeâtres vernissées, aux proportions admirables, décorées d'aigles, de lions, de sphinx.

Les Barbares, Huns et Vandales, envahissent et détruisent Harnes en 406.

Vers 430, les Francs, originaires de Germanie, s'installent dans la région. Leur roi, au nom évocateur de Clodion le Chevelu, après avoir fait la conquête de l'Artois, offre à son neveu, Flandebert, la partie de la Gaule qui constitue la Belgique actuelle. Ainsi naît la Flandre.

À la fin du , Harnes vit la fusion des mœurs et des coutumes flamandes et artésiennes, à la suite du mariage de Judih, fille de Charles le Chauve, avec , comte de Flandre ; l'Artois est donné en dot. Le rattachement du comté de Harnes à l'abbaye de Saint-Pierre-de-Gand en 963, par Arnoult le Vieux, comte de Flandre (donation confirmée par le pape , en 1145) durera, sans interruption .

En 1180, c'est le rattachement glorieux, grâce à la dot d'Isabelle de Hainaut, fille du comte de Flandre, à la couronne de France.

Au début du , , seigneur de Harnes, participe à la « guerre sainte » de Jérusalem. Il revient de la croisade contre les Albigeois en 1208 : c'est l'un des dix-neuf chevaliers d'Artois porte-bannière. Il repart, neuf ans plus tard, pour la croisade.

En 1304, Harnes est ravagée et brûlée par les Flamands. Sous les comtes de Bourgogne (1384-1482), guerres civiles et étrangères se succèdent. Combats, famine, peste et épidémies s'abattent sur la région.

En 1438, Harnes est réduite à et . Le village est si pauvre qu'il est exempté d'impôts. L'Artois abandonné à l'Autriche par , en 1477, puis à partir de la mort de Marie de Bourgogne, revendiqué par les rois de France, , , .

En 1493, sous le règne de Philippe le Beau, fils de Marie de Bourgogne, les Autrichiens attaquent la France, campent à Harnes et rançonnent les habitants. le château est détruit.

En 1526, Charles-Quint, après la défaite de Pavie, contraint à renoncer à sa suzeraineté sur la Flandre et sur l'Artois.

De 1605 à 1665, sous les règnes de et , rois d'Espagne, des villes de l'Artois et le secteur du Comté de Harnes sont prises et pillées.

En 1648, Condé gagne la bataille de Lens. Les Robespierre, Robert Père et fils chargés de l'administration locale gagnent Carvin siège de la principauté voisine d'Epinoy où ils fondent la branche carvinoise de la famille

Le , le traité des Pyrénées met fin à l'occupation espagnole.

1713, le traité d'Utrecht confirme la possession par de l'Artois et de la Flandre. Les États d'Artois sont formés. Ils votent les impôts et les dépenses. Leurs députés, qui exercent une grande autorité aux États généraux, sont convoqués à Versailles en 1789.

C'est le que, l'Assemblée nationale ayant voté la loi de nationalisation de tous les biens religieux, Harnes se trouvé libérée de la tutelle de l'abbaye de Saint-Pierre-de-Gand. Toutes les terres qui appartenaient au monastère sont mises sous séquestre, pour être vendues, en 1793, comme biens nationaux. Les fermiers, qui les occupaient, les achètent.

La ville de Harnes se trouve entièrement sur la concession de la Compagnie des mines de Courrières.

Sur la commune de Harnes à proximité de Fouquières-lez-Lens, le puits 9 est creusé de 1891 à 1896. Le fonçage débute comme les autres puits mais ce n'est pas l'eau abondante qui pose des difficultés mais le terrain qui se désagrège. On décide de foncer par congélation des sols. Le houiller est atteint à . Le puits 17 date de 1909. À la nationalisation, elle devient la première concentration des charbons gras de Courrières de par sa position centrale dans le groupe mais aussi par son exploitation de faible profondeur (). En avril 1949, le 17 s'arrête pour moderniser son chevalement. Le bâtiment d'extraction ainsi que la machine sont démontés. Le nouveau chevalement ainsi que la machine d'extraction proviennent de la fosse 2 de Billy-Montigny. De l'ancienne à la nouvelle machine on passe de à . Le puits 17 reprend du service en 1951 équipé de deux skips d'une capacité de . Le faux carré du chevalement est étanche pour permettre le retour d'air. En décembre 1951, c'est le 9 qui s'arrête pour moderniser la recette et aménager les cages pour accueillir des berlines de au lieu de . En janvier 1952, le nouveau siège est opérationnel, ce qui permet la concentration de la fosse 23 sur le 9/17. Mais le 9/17 est concentré sur la fosse 21/22 de Harnes en 1967 et s'arrête d'extraire en 1968. Le puits 9, profond de , et le 17, profond de , sont remblayés en 1970. Les chevalements sont abattus en 1973. De nombreux bâtiments sont encore visibles sur place notamment la salle de paye, la conciergerie, la lampisterie, les bains-douches, les bureaux.

Le puits est ouvert le janvier 1913 et il est prêt à fonctionner en 1914. C'est cette année que commencent les travaux du puits .
À la fosse sont adjoints les lavoirs et la cokerie. Les installations sont à la veille d'être mises en service lorsque la Première Guerre mondiale éclate. Il n'en restera qu'un tas de ruine en 1918.

La Fosse 21/22 d'Harnes sera un important site d'extraction et de traitement de la houille exploité par la Compagnie des mines de Courrières, puis les Houillères du bassin du Nord-Pas-de-Calais (HBNPC-Groupe d'Hénin-Liétard). Sur le site figureront les deux puits, deux lavoirs à charbon, la cokerie, l'usine Courrières-Kühlmann. Le charbon extrait par les puits 21/22 et les fosses à proximité est traité dans les lavoirs en fonction de la qualité, il peut être ensuite commercialisé ou transformé en coke. Les sous-produits issus de la carbonisation sont envoyés dans l'usine chimique. En 1952, le puits subit une modernisation afin de pouvoir fonctionner avec des berlines de . Le puits est modernisé l'année suivante. Les puits 21 et 22, respectivement profonds de 546 et , sont remblayés en 1977. Le chevalement du puits est démoli en 1979, celui du puits l'année suivante. Subsistent sur le carreau, l'ancienne loge du concierge, la salle de paiement, les bureaux et garages, les deux châteaux d'eau, ce sont les derniers de la Compagnie des mines de Courrières encore visibles. Sur le site de la cokerie, seuls la grille d'entrée et les bureaux sont encore visibles. L'usine chimique Courrières-Kuhlmann deviendra dans les années 1980 Noroxo. Cette usine a fermé en 2004 et sera démolie totalement entre 2009 et 2011.

Un élément phare de l'exploitation charbonnière subsiste encore à Harnes, le terril conique de la Fosse 21. Le terril domine les cités minières : des corons aux cités-jardins, on dénombre dans la région d'habitats ouvriers.

En 2011, la commune d'Harnes a été récompensée par le label « Ville Internet @@@ ».

 
Au 26 octobre 2013, Harnes est jumelée avec :

Par ailleurs, la commune a signé un contrat de coopération avec Kabouda (Burkina Faso) en 2006.

La population de la commune est relativement jeune. Le taux de personnes d'un âge supérieur à 60 ans (21 %) est en effet supérieur au taux national (21,6 %) tout en étant toutefois inférieur au taux départemental (19,8 %). 
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (52,1 %) est du même ordre de grandeur que le taux national (51,6 %). 
La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante : 
 


Au pied du terril de l'ancienne fosse n°9 Harnes - Fouquières sur une superficie de 5,5 hectares la Communaupole de Lens-Liévin a réalisé un lagunage paysagé dont l'objectif est de compléter l'épuration des eaux usées sortant de la station d'épuration de Fouquières par un traitement tertiaire (élimination des germes pathogènes, traitement du phosphore).

Cinq traitements successifs ; filtration dans les taillis de saule ; lagunage dans des bassins plantés de végétaux aquatiques (roseaux, iris, scirpes, typha) et étanchés par de l'argile ; oxygénation et exposition aux ultra violets (l'énergie nécessaire au pompage est fournie par quatre éoliennes rejetant l'eau à plus de deux mètres de hauteur) ; lagunage dans des bassins plantés de végétaux ; finition du lagunage.

La durée du parcours de l'eau est d'environ un mois avant de rejoindre le canal de Lens. Cette réalisation (1999-2004) a reçu le prix Rosa Barba à la quatrième biennale européenne du paysage de Barcelone.



Le Musée de l’école et de la Mine, fondé en 1984, rend hommage à la condition de nos « gueules noires » comme au milieu scolaire du début du . Pour cultiver les valeurs de courage, de lutte ouvrière chères à ces travailleurs, la reconstitution des galeries, les salles d’exposition retracent la vie de mineur, au coron et à la fosse. Les visites sont placées sous la conduite d’une équipe d’animateurs bénévoles, tous ayant bien grandi dans l’ère du charbon. Le récit des hommes du fond, les vestiges des fosses 9/17 et 21/22 de Harnes, l’estaminet, la cuisine, la salle de télégrisoumétrie. Bref, c’est une exploration entière qui retranscrit l’histoire de la mine. La visite de la salle d’école, avec l’exercice de la dictée à la clé, est tout aussi éducative et chargée de souvenirs. La salle de matériel didactique jouxtant la classe illustre la même période et l’enseignement de l’année 1900. L’exposition de matériel et de documents pédagogiques rappelle aux visiteurs l’école d’antan.
En 1969, la municipalité achète la propriété de l’ancien maire, député et sénateur André Deprez, décédé en 1900. En 1971, l’immeuble est mis à la disposition de l’« association des amis du vieil Harnes » par le maire, monsieur Rainguez. Il s’agit de valoriser les collections à caractère militaire, rassemblées depuis 1919 par les anciens combattants et de présenter les vestiges archéologiques qui apparaissent sur les terrains de la zone industrielle en plein développement. Le musée inaugure le 11 novembre 1972 ses deux premières salles. Au fil des années, il va se développer grâce à l’aide de la municipalité et au dévouement tenace d’un groupe de bénévoles. On peut actuellement visiter des salles de la guerre 1914-1918 et de la guerre 1939-1945, une salle sur la Résistance et une sur la Déportation, et trois salles d’archéologie locale. À partir de documents originaux recueillis auprès des Harnésiens, c’est l’impact local des grands évènements du qui a été évoqué et mis en valeur.




Cinéma : ciné-goûters (tous les deux mois), cinéma de Noël des écoles (décembre).

Arts de la Scène : création de cirque contemporain (février), concert de la Saint-Patrick (mars), festival Les enchanteurs (avril), restitution de l'Atelier Théâtre municipal (avril/mai), création annuelle de la Cie Tassion (avril/mai), concert de gala de l'Harmonie de Harnes (juin), fête de la Musique (juin), festival Harnes de Vive Voix (novembre), concert de Noël de l'Harmonie de Harnes (décembre).

Concert de gala de l'Harmonie de Harnes (juin), fête de la Musique (juin), festival Harnes de Vive Voix (novembre), concert de Noël de l'Harmonie de Harnes (décembre).

Jumelages : Feria de Vendres (juin), Gala Day de Loanhead (juin), semaine culturelle de la polonité (octobre).

Lecture publique : salon d'éveil culturel Tiot Loupiot (octobre).

Patrimoine : journées européennes du patrimoine (septembre).






</doc>
<doc id="17900" url="https://fr.wikipedia.org/wiki?curid=17900" title="Bully-les-Mines">
Bully-les-Mines

Bully-les-Mines est une commune française située dans le département du Pas-de-Calais, en région Hauts-de-France. Elle est chef-lieu de canton dans l'arrondissement de Lens. Elle fait partie de la communaupole de Lens-Liévin (communauté d'agglomération) qui regroupe et comptait en 2010.

La commune est à proximité de l'A21 et de l'A26, et près de Lens dans la banlieue de Liévin. Elle est située en Gohelle.

Le nom de Bully a changé au cours des siècles : "Bulgi" 1135, "Bugi" 1152, "Builli" 1157, "Bullia" 1198, "Bully" 1270, "Boulli" 1303, "Buylly" 1410, "Builly-lez-Aix" 1486, "Builly-lez-Grenay" 1511, "Builly-en-Gohelle" 1569, "Bully-en-Gohelle" 1709, "Bully-Grenay" 1750, "Bully-en-Gohelle" 1782 et enfin Bully-les-Mines en 1925.

Le nom, selon plusieurs sources, serait d’origine gauloise. La terminologie actuelle ‘’les-Mines’’ s’explique par la vigueur de l’activité minière. La gare a gardé le nom de Bully-Grenay ce qui entraîne parfois des confusions.

Bien que la région ait été habitée dès la préhistoire on ne trouve rien à Bully. La plus ancienne découverte est un bracelet celte. Par contre les découvertes gallo-romaines sont nombreuses. Bully appartient au Pagus Silvinus région administrative des Atrébates, la future Gohelle. En 2006, des fouilles effectuées lors de la construction d'un lotissment ont mis au jour une nécropole gallo-romaine.

Au , Bully est rattaché spirituellement à l'évêque de Cambrai-Arras en résidence à Cambrai et civilement à l'Artois primitif, chef-lieu Arras.

Bully, partie intégrante de l'Artois, a subi lui aussi les avatars de sa mère de 862 à 1191, c'est la domination des comtes de Flandre en 1191 a lieu le passage à la France jusqu’en 1384 avec gouvernement direct par la Couronne de 1191 à 1237. De 1384 à 1477, on subit la domination bourguignonne, puis retour bref à la France de 1477 à 1492 après la mort de Charles le Téméraire, puis période espagnole de l’année 1492 au 4 novembre 1659, date de la réunion définitive à la France par le traité des Pyrénées.

Le voisinage de trois places fortes, Arras, Béthune et Lens, est souvent funeste à Bully qui sert de terrain de combat ou de passage des armées. En 1213, le village est ravagé. En 1303, il est rasé, même les arbres sont abattus. En 1348, un tiers de la population meurt de la peste noire. La peste sévira encore quatre fois le siècle suivant alternant avec des disettes et des guerres.

En 1537, Bully est ravagée par les troupes de Louis XII qui emportent tout et les malheurs continueront jusqu'à la prise de Lens par les Français en 1556-1557. Le receveur ne peut prélever aucun impôt à Bully, tant la population est éprouvée. En 1648, le village supporte la présence des armées pour la bataille de Lens.

De 1709 à 1712, Bully subit les marches et contremarches des armées lors de la guerre de succession d’Espagne. La situation est aggravée par une épidémie qui fit vingt-quatre morts. En 1796, un incendie détruit la moitié du village (en souvenir un lieu-dit sera nommé ‘’Chemin brûlé’’).

Quelques propriétaires encaissant des revenus de terres à Bully en 1792 (hors seigneurs et particuliers) :

La ville se développe avec l'exploitation des mines de charbon.

Lors de la grève de 1948, le maire et un mineur jaunes sont tabassés par des grévistes.

La population de la commune est relativement âgée. Le taux de personnes d'un âge supérieur à 60 ans (21,7 %) est en effet supérieur au taux national (21,6 %) et au taux départemental (19,8 %). 
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (53,2 %) est supérieur au taux national (51,6 %). 
La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante : 
 







</doc>
<doc id="17902" url="https://fr.wikipedia.org/wiki?curid=17902" title="Pinceau">
Pinceau

Le pinceau (du latin "peniculus", petite queue) est un instrument de peinture, de dessin et d'écriture composé d'un manche ou hampe (en bois ou plastique) munie à son extrémité d'une touffe de poils maintenue grâce à une virole. Avant l'apparition de la virole métallique, les poils étaient maintenus grâce à une plume ligaturée par du fil en laiton : on trouve ces pinceaux, très populaires, sous l'appellation de pinceau 'monté sur plume'. 

Le pinceau sert à étaler la matière picturale (peinture ou encre) sur le support (papier, toile, bois, mur, etc.). Il est aussi utile pour appliquer un enduit ou un vernis sur les mêmes supports : on se sert alors du "spalter". Pour la peinture à l'huile ou à l'acrylique, on parle plutôt de "brosse". 

Il est utilisé en peinture, illustration, dessin et en calligraphie, européenne ou chinoise. L'appellation chinoise du pinceau en mandarin est máobĭ, littéralement « crayon à poils ».

Un pinceau est composé de 3 éléments :

La qualité et spécificité d'un pinceau dépend de la nature des poils de sa touffe. 
Ces poils présentent en effet différentes caractéristiques (dureté, souplesse, nervosité, absorption…) et sont appropriés à des usages différents. Aujourd'hui, les fibres synthétiques tentent d'imiter les qualités des poils naturels ou d'en proposer des nouvelles.



Elles se caractérisent par leur fermeté, leur élasticité et leur résistance.

Contrairement aux poils fins, l'extrémité d'une soie est à fleur multiple (comme une fourche) et non unique. Elle va donc bien retenir la matière pâteuse et l'étaler de manière uniforme, d'où son utilisation fréquente dans les techniques à l'huile et acrylique.

La soie est naturellement courbée ou cambrée, ce qui permet également une bonne accroche et un bon contrôle de la pâte. 


Les premiers pinceaux en fibres artificielles furent les pinceaux en nylon blanc, très fermes et peu élastiques. Depuis, d'autres fibres, toujours des dérivées de polyamide, ont fait leur apparition. On dispose aujourd'hui d'une grande variétés de fibres en termes de souplesse et de fermeté. Les fabricants proposent aussi des mélanges synthétique/naturels.


Chaque fabricant a mis au point ses propres fibres synthétiques : Kaërell (Raphaël), Nova (Da Vinci), Orion (Pébéo), Similaire (Léonard).

Le pinceau de calligraphie, quelle qu’elle soit, retient bien l'encre, tandis que le pinceau à peinture chinoise est plus adapté à des mélanges d'eau avec soit de l'encre de chine pour les lavis, soit des peintures à l'eau en couleur typiquement chinoises, proches des caractéristiques de la gouache ou de l'aquarelle et eau. Ils sont utilisés dans les arts traditionnels d'Extrême-Orient.

Il existe différentes classifications de tailles selon les fabricants, selon le diamètre de la touffe.


Les pinceaux ont différentes tailles et formes selon l'utilisation que l'on veut en faire : plats, ronds, en éventail, etc.

D'autres outils peuvent servir, à l'instar du pinceau, à étaler la matière picturale :



</doc>
<doc id="17908" url="https://fr.wikipedia.org/wiki?curid=17908" title="Méricourt (Pas-de-Calais)">
Méricourt (Pas-de-Calais)

Méricourt est une commune française située dans le département du Pas-de-Calais, dans la région Hauts-de-France. Elle fait partie de la Communaupole de Lens-Liévin (communauté d'agglomération) qui regroupe et comptait en 2010. La Compagnie des mines de Courrières y a ouvert la fosse 3 - 15, et la Compagnie des mines de Drocourt la fosse 4 - 5. Celles-ci sont devenues de grands sièges de concentration, et ont fermé dans les années 1980.
Cette commune est partagée entre les cantons d'Avion et de Rouvroy.

Ses habitants sont appelés les "Méricourtois".

Le nom de « Méricourt » viendrait d'un nom d'origine germanique, "Médéric", et du suffixe "-court", dérivé du latin "curtis", domaine.

La Fosse 3 des Mines de Courrières est ouverte à partir d'août 1858, sur le faisceau des veines du puits et à l'ouest. La fosse est mise en exploitation en 1860. Le terrain houiller est atteint à . Le diamètre est de quatre mètres, le cuvelage possède vingt côtés (ou pans). Le maximum d'eau fourni par le niveau a été de cent hectolitres par minute. La houille contient 34 à 40 % de matières volatiles.
Le puits est commencé en 1905. Alors que se déroule la catastrophe minière dite "catastrophe de Courrières" qui fit morts le sur les territoires de Billy-Montigny, Méricourt, Noyelles-sous-Lens et Sallaumines, le puits n'est pas encore assez profond pour desservir les chantiers. Dès sa mise en service, le puits est entrée d'air, le puits assure le retour d'air.

Les communes voisines commencent à recruter des mineurs à l'étranger après la catastrophe. Au début de l'année 1913, des familles polonaises s'installent à Méricourt, dans les vieilles cités appelées "Méricourt-Coron", selon le témoignage d'un mineur polonais de l'époque.

La fosse est modernisée en 1953. L'année suivante, la fosse est concentrée dessus. En 1963, des ingénieurs évoquent la possibilité de creuser un troisième puits sur le site, équipé d'une tour d'extraction semblable à Barrois , au 10 d'Oignies ou au 19 de Lens, mais l'idée est abandonnée. En 1965, la fosse est concentrée, quatre ans plus tard, c'est au tour de la fosse . En 1971, le chevalement et la recette du puits sont détruits, et remplacés par une nouvelle recette, et le chevalement du puits de Liévin.

La concentration fonctionne jusqu'en 1983, date à laquelle l'extraction est reprise par la fosse 4 - 5 de Drocourt. Le puits est remblayé en 1983, il est profond de . Le puits assure le service pendant deux ans, puis il est remblayé en 1985. Il est lui profond de .

Les deux chevalements sont détruits en 1988. Subsistent sur le site cinq bâtiments en 2011, les bains douches, les bureaux, le magasin, le poste électrique et les garages.

La commune a la particularité d'être sans discontinuer une municipalité communiste.

La population de la commune est relativement jeune. Le taux de personnes d'un âge supérieur à 60 ans (19,8 %) est en effet supérieur au taux national (21,6 %) tout en étant toutefois inférieur au taux départemental (19,8 %). 
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (52,3 %) est du même ordre de grandeur que le taux national (51,6 %). 
La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante : 
 







</doc>
<doc id="17910" url="https://fr.wikipedia.org/wiki?curid=17910" title="Sallaumines">
Sallaumines

Sallaumines est une commune française située dans le département du Pas-de-Calais, en région Hauts-de-France. Elle fait partie de la Communaupole de Lens-Liévin (communauté d'agglomération) qui regroupe et comptait en 2010. La commune a été de la fin du jusqu'à la fin des années 1980 un grand centre minier, puisque la Compagnie des mines de Courrières y a ouvert ses fosses 4 - 11, 5 - 12 et 13 - 18. Une avaleresse 5 a même été tentée sans succès.

Ses habitants sont appelés les "Sallauminois".

Sallau était un petit village agricole de l'Artois, sur les bords de la Souchez, avant la découverte du charbon, qui a provoqué une industrialisation et une poussée démographique subites.

L'histoire de la région reste marquée par la catastrophe minière dite "Catastrophe de Courrières" qui fit morts le sur les territoires de Billy-Montigny, Méricourt et "Sallaumines", puis par la Première Guerre mondiale qui a ravagé une grande partie de la région (classée zone rouge lors de la reconstruction).

La ville de Sallaumines se situe entièrement sur la concession des mines de Courrières. Cette compagnie y a exploitée trois fosses, Fosse 4, Fosse 5 et Fosse 13.

Les travaux du puits n°4 ont commencé en mars 1865 à Sallaumines. La fosse est terminée à la fin de 1867. Le diamètre du puits est de quatre mètres. La houille contient 34 à 40 % de matières volatiles. Le puits n°11 est ajouté à partir de 1898, et est fonctionnel en 1901. La fosse est touchée en 1906 par la "Catastrophe de Courrières", dont le souvenir est encore marqué aujourd'hui, notamment par plusieurs odonymes dans les communes concernées (cf. ).

Une seconde catastrophe se produit le lundi 19 avril 1948, une explosion a lieu dans le puits 11. Les installations du jour sont endommagées à cause de l'effet de souffle, la cage est coincée dans le chevalement. Cette catastrophe provoque la mort de seize personnes, et en blesse grièvement 33.

Après que l'exploitation du gisement a été concentrée sur la fosse n°3/15 en 1954, la fosse cesse son activité, le puits n°11, profond de 531 mètres, est remblayé et sa machine d'extraction installée au puits n°24 à Estevelles. Les chevalements ont été abattus en 1962, date à laquelle le puits n°4 est remblayé. Le carreau a ensuite servi de parc à bois central pour le Groupe d'Hénin-Liétard jusqu'à l'arrêt du dernier siège du Groupe, en 1990/91, date à laquelle les derniers bâtiments de la fosse sont détruits. Une voie rapide a été aménagée et coupe la fosse en deux parties. En 2011, l'ancien carreau de mine est réhabilité avec la construction de nouveaux logements.

Un premier puits, dit avaleresse n°5, est tout d'abord creusé sur le site, à Sallaumines, mais il est abandonné à la profondeur de 24 mètres. Un second puits, dit fosse n°5, a été ouvert à cinquante mètres du premier, à l'aide d'une tour en tôle que l'on a descendu jusqu'à 35 mètres du sol. On épuise jusqu'à 1 300 mètres cubes d'eau à l'heure, au moyen de trois pompes, dont deux de 55 centimètres et une d'un mètre de diamètre, marchant avec une course de trois mètres. Le diamètre du puits est de 4,50 mètres. Le siège est ouvert à partir du 15 mai 1877. Il porte le nom de Constant Mathieu, directeur de la Compagnie. Le puits n°12 est ajouté en 1905.

Les puits n°5 et 12, respectivement profonds de 676 et 823 mètres, ont été remblayés en décembre 1988. Les principales installations ont été démolies en 1990. 
Subsistent encore de nos jours, les bains douches et la lampisterie, occupés par les services techniques de la ville de Sallaumines, les magasins de stockage, la salle de paye et le logement de concierge sont encore visibles mais abandonnés et vandalisés. L'ancien site minier est en cours de réhabilitation est intégré à la Zone Industrielle de la Galance.

Les élections du président de la République Française à élu,en 2017 Emmanuel Macron.

Le maire de Sallaumines se nomme M. Christian Pedowski.

La population de la commune est relativement jeune. Le taux de personnes d'un âge supérieur à 60 ans (19,4 %) est en effet inférieur au taux national (21,6 %) et au taux départemental (19,8 %). 
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (52,3 %) est du même ordre de grandeur que le taux national (51,6 %). 
La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante : 
 

Une zone de sécurité prioritaire a été classée le 14 février 2014 à Lens-Sallaumines et dans le quartier la Grande Résidence, posant la question de la délinquance, de l'économie souterraine et de la hausse des cambriolages , réclamée par les syndicats de police.
Les catholiques de Sallaumines dépendent de la paroisse Bienheureux Marcel-Callo-en-Mines du doyenné de Lens-Liévin du diocèse d'Arras qui regroupe également les clochers de Saint-Vaast de Loison-sous-Lens, Saint-Martin de Méricourt, Sainte-Barbe de Méricourt, et Saint-Amand de Noyelles-sous-Lens.


Les statistiques de l'INSEE montrent un emploi toujours fortement ouvrier et une activité économique orientée vers l'automobile.
"Source : Insee, RP2011 exploitation complémentaire lieu de travail"

"Source : Insee, CLAP, 2013"

"Source : Insee, CLAP, 2013"

Sallaumines est le siège et le principal établissement de la Société Durisotti qui fut fondée le 15 mars 1956 à Sallaumines par deux frères, Bruno et Louis Durisotti après leur apprentissage dans la carrosserie paternelle à Amiens. Après s'être spécialisée dans la réparation des autocars, l'entreprise se tourne à partir de 1968 vers la transformation et à l'aménagement des véhicules utilitaires légers des grands constructeurs automobiles. À la même époque, les mines commencent à fermer, le textile accentue son déclin et le parc des autocars consacrés notamment au transport des ouvriers vers les mines ou les grandes usines diminue fortement. Pour sa reconversion, Durisotti achète l'ancien carreau de la fosse 13, soit 10 hectares de friche industrielle. Six mois plus tard, un premier bâtiment de est construit et Durisotti commence, avec l'accord du constructeur, l'allongement en empattement et en porte à faux du fourgon J7 Peugeot qui rencontre un grand succès et lance définitivement Durisotti sur le marché de la transformation des véhicules utilitaires légers.

En trente ans, la friche minière est devenue le plus important site industriel français pour la transformation de véhicules utilitaires légers, avec de bâtiments sur 13 hectares et 410 salariés. 

L'entreprise Durisotti poursuit son activité à ce jour et, bien qu'ayant fondé des succursales ailleurs, reste le premier employeur de la commune.


La ville est jumelée avec Trbovlje, ville de Slovénie.




</doc>
<doc id="17912" url="https://fr.wikipedia.org/wiki?curid=17912" title="PRI">
PRI

PRI est le sigle qui peut signifier :

PRI est un code qui peut signifier :


</doc>
<doc id="17920" url="https://fr.wikipedia.org/wiki?curid=17920" title="Réaction de Diels-Alder">
Réaction de Diels-Alder

La réaction de Diels-Alder est une réaction chimique utilisée en chimie organique, dans laquelle un alcène (diénophile) s'additionne à un diène conjugué pour former un dérivé de cyclohexène. Otto Diels et Kurt Alder reçurent le prix Nobel de chimie de 1950 pour les travaux relatifs à cette réaction.

Dans le domaine des matériaux avancés et de la robotique souple on parle de « Polymères Diels-Alder » ("" pour décrire des polymères constitués de réseaux covalents thermoréversibles. Un tel matériau a été récemment (2016-2017) expérimentés avec succès pour trois actionneurs pneumatiques de robotique souple (une pince souple, une main et des muscles artificiels) auto-cicatrisantes après lésions par percement, déchirement ou coups portés sur le polymère en question.

La réaction de Diels-Alder est un cas spécial d'une classe de réactions plus générale : les réactions de cycloaddition entre systèmes π. Dans la réaction de Diels-Alder, les 4 électrons π du diène réagissent avec la double liaison de l'alcène contenant 2 électrons π. Pour cette raison, cette réaction est appelée cycloaddition [4+2].
La réalisation de ces réactions nécessite, en général, un chauffage.

La facilité de la réaction de Diels-Alder dépend fortement de la nature des substituants du diène et du diénophile. La réaction prototype ci-dessous, entre l'éthène et le but-1,3-diène, se fait difficilement et donne un rendement en cyclohexène assez faible.

La règle d'Alder permet de préciser les conditions qui facilitent la réalisation de ces cycloadditions : la réaction s'effectue plus facilement entre un diène riche en électrons et un diénophile pauvre en électrons. En d'autres termes, un « bon » diène est substitué par des atomes ou des groupes d'atomes donneurs d'électrons, un « bon » diénophile par des atomes ou groupes d'atomes attracteurs (accepteurs) d'électrons.

Ces caractères, attracteurs ou donneurs, peuvent être des effets inductif, mésomère, ou d'hyperconjugaison. Ci-dessous, la réaction entre le 2,3-diméthylbuta-1,3-diène (2 groupements méthyles donneurs d'électrons), et le propénal (groupement aldéhyde attracteur d'électrons), se fait avec un bon rendement.

Les alcynes substitués par des groupements attracteurs sont de bons diénophiles et font l'objet d'une réaction de Diels-Alders. En effet, la réaction d'un éthyne substitué avec 2 groupements attracteurs (par ex, CHOOC)-C≡C-(COOCH) avec le buta-1,3-diène donne une réaction possible. En revanche, un composé comportant deux liaisons π conjuguées dont au moins une est un alcyne est un mauvais diène, car il ne peut se positionner en conformation s-cis. La réaction nécessite une haute température.

Cette réaction de cycloaddition [4+2] est sous contrôle orbitalaire, ce qui entraîne généralement la formation d'un stéréoisomère "endo" lorsque le diène est cyclique. On justifie fréquemment cette sélectivité par des considérations frontalières secondaires stabilisant l'approche endo :

La réaction est réversible. Le composé exo est généralement le plus stable thermodynamiquement. Si on laisse la réaction se prolonger un temps très long, l'équilibre sera alors en faveur du produit exo.

Cette réaction est renversable. Les réactions inverses, dites de "rétro-Diels-Alder" peuvent être utilisées pour préparer des composés. Par exemple, le craquage thermique du cyclohexène permet d'obtenir du butadiène et de l'éthène. 

Certains auteurs utilisent à tort le terme "réversible" à la place de "renversable" alors que toute transformation chimique est créatrice d'entropie.

La réaction de Diels-Alder peut être problématique, par exemple dans le cadre du stockage du cyclopentadiène, qui se dimérise lentement à température ambiante, pour former un composé tricyclique, par réaction de Diels-Alder sur lui-même. Le composé obtenu, stable à la température ambiante (T = ), doit être distillé lentement pour récupérer le cyclopentadiène (T = ) par réaction de "rétro-Diels-Alder".


</doc>
<doc id="17926" url="https://fr.wikipedia.org/wiki?curid=17926" title="Dièse">
Dièse

En musique, dans le solfège, un dièse est un symbole graphique () appartenant à la famille des altérations et dont la fonction est d'indiquer, sur la partition, que la hauteur naturelle de la note associée à ce dièse doit être élevée d'un demi-ton chromatique.

La note affectée d'un dièse est dite « diésée ». Une note peut être diésée pour toutes sortes de raisons : modulation, transposition, note de passage, ornementation, utilisation de l'échelle chromatique…

En musique ancienne, dans le système musical grec, le dièse, plus couramment employé sous sa forme ancienne « diésis », est le plus petit intervalle utilisé.

Au Moyen Âge, à l'époque du Roman de Fauvel, apparaît le dièse dont la forme provient du bécarre.

L'effet du dièse est différent suivant que celui-ci est accidentel (il peut se trouver à divers endroits sur une portée musicale), ou constitutif (lorsqu'il se trouve à l'armure, c'est-à-dire en début de portée, placé à la droite de la clef). Mais il hausse de toute façon la note d'un demi-ton chromatique.

L'effet d'une altération accidentelle, quelle qu'elle soit, est temporaire. Elle altère toutes les notes "de même nom et de même hauteur" se trouvant après elle, et ce jusqu'à la prochaine barre de mesure, sauf si entre-temps apparaît une autre altération modifiant la hauteur de cette même note.

Les dièses constitutifs, communément appelés « dièses à la clé », sont habituellement rappelés au début de chaque ligne de portée, et à chaque changement de clé (lorsque sur la même portée on passe de la clé de "sol" à la clé de "fa" par exemple). Ils constituent l’armure de la section du morceau. Leur effet est permanent pour toute la durée de la portée, sauf si l'armure est modifiée. Ces dièses altèrent toutes les notes de même nom quelle qu'en soit l'octave, sauf si entre-temps intervient une autre altération (accidentelle) modifiant temporairement (pour la durée de la mesure en cours) la hauteur de la note en question, ou bien sûr si l'armure à la clé est modifiée.
L'ordre des dièses (selon le cycle des quintes montantes) est immuable : "fa", "do", "sol", "ré", "la", "mi", "si". De sorte que s'il n'y a qu'un dièse à la clé, c'est toujours le "fa" qui est altéré ; s'il y en a deux ce sont toujours le "fa" et le "do" ; trois dièses à la clé affectent toujours "fa", "do" et "sol", et ainsi de suite.

Il existe aussi :
\new Staff = "haut" \with {\remove "Time_signature_engraver"}{
\cadenzaOn
</score>

\new Staff = "haut" \with {\remove "Time_signature_engraver"}{
\cadenzaOn
</score>

\new Staff = "haut" \with {\remove "Time_signature_engraver"}{
\cadenzaOn
</score>

Le dièse (♯) est un caractère différent du croisillon (« # ») ; ce dernier a les deux barres transversales horizontales, et ses barres « verticales » légèrement en oblique à droite, alors que les barres verticales du dièse sont bien verticales mais ses « horizontales » sont, elles, légèrement ascendantes. Unicode distingue les deux caractères : le dièse est codé à l'emplacement U+266F, tandis que le croisillon l'est à U+0023. On utilise cependant souvent le croisillon (#) pour représenter le dièse (♯) pour des raisons de facilité, car le croisillon est disponible sur le clavier.

LaTeX permet de tracer le symbole du dièse facilement. La syntaxe est codice_1 et le résultat est formula_1 .

Linux aussi permet de taper les dièses (♯) très facilement  (voir ici pour cela). Si l'on a défini une touche Compose, il suffit de taper Compose # #.

Compose # b donnera un bémol () et Compose # f un bécarre ().




</doc>
<doc id="17928" url="https://fr.wikipedia.org/wiki?curid=17928" title="Cornemuse">
Cornemuse

La cornemuse est un instrument de musique à vent et plus particulièrement à anches. Il en existe plus d'une centaine de types dans le monde. Sa répartition géographique correspond à l'Europe entière, au Caucase, au Maghreb, au golfe Persique et va jusqu'à l'Inde du Nord.

Les origines de la cornemuse sont lointaines et difficiles à déterminer étant donné le peu de preuves archéologiques dont on dispose aujourd'hui. Elle est mentionnée dès l'époque gréco-romaine : les Grecs l'appelaient / et chez les Romains elle se nommait . On suppose que la cornemuse prendrait ses origines en Égypte antique car de nombreuses représentations de chalumeaux doubles, tant chez les Grecs ("aulos bicalame", de "calamus" = roseau en latin) que chez les Égyptiens montrent l'importance de cet instrument. Des débris de ce dernier ont été retrouvés dans des pyramides égyptiennes datant d'environ 300 ans Aristophane (≈450-386 ) poète comique d'Athènes s'en moquait déjà. En théorie, il serait arrivé en Europe grâce aux Grecs, puis aux Romains et au commerce avec les Celtes tout autour du bassin méditerranéen. En effet, d'après Procope ( ) cet instrument aurait été l'instrument de marche des légions romaines. Mais rien ne permet de conforter cette théorie au vu du peu de témoignages dont nous disposons.

Rechercher une origine commune entre les différentes cornemuses est aussi hypothétique qu'illusoire ; il est d'ailleurs possible que cet instrument ait été créé simultanément ou à des époques différentes, et cela dans des régions fort éloignées.

Instrument pastoral à l'origine, elle a développé au cours des siècles un répertoire à part entière qui culmine avec la musique de cour et la musique militaire.

L'adjonction d'un réservoir (poche) à un hautbois à anche double ou à anche simple, constitue l'une des particularités de l'instrument qui permet alors un jeu continu (similaire au souffle continu) et puissant, une autre étant l'adjonction de tuyaux complémentaires à anche simple ou double (semi-mélodique ou bourdon) amplifiant encore la puissance sonore et l'effet polyphonique.

Le joueur de cornemuse est appelé « cornemuseux » et « sonneur » ou "biniaouer" en Bretagne, "piper" en Irlande ou en Grande-Bretagne.

Elles sont toutes constituées sur la base d'un hautbois à doigté plus ou moins complexe, équipé d'une anche double, et d'un ou plusieurs bourdons, produisant une note tenue, à l'aide d'une anche simple − traditionnellement en roseau − emboutie dans un tuyau à coulisse réglable.
Les instrumentistes des hautbois précurseurs des cornemuses jouent le hautbois directement dans la bouche, en respiration circulaire ; les joues servant de réserve d'air pendant que l'instrumentiste regonfle ses poumons.
Il peut jouer simultanément de deux hautbois, ou d'un bourdon et un hautbois, tous deux directement tenus par les lèvres.

Le sac, initialement dans une peau ou une vessie de bête est une invention permettant au musicien de s'affranchir du souffle continu, et qui a permis de complexifier l'instrument, en y ajoutant d'autres bourdons, hautbois, voire des régulateurs actionnés au poignet.

La poche, ou sac, est un réservoir étanche (sac en peau animale ou en Gore-Tex soit encore la combinaison du cuir extérieur et gore-tex intérieur) dans lequel de l'air est insufflé soit par la bouche de l'instrumentiste soit par un soufflet (ce qui est plus rare). L'air contenu dans le réservoir s'échappe ensuite de manière continue vers les tuyaux de bois (ébène, grenadille du Mozambique ou fruitier) souvent formés de segments emboîtés dont l'extrémité interne possède une anche simple ou double qui produit le son. Ces tuyaux sont, ou non, percés de trous de jeu qui, comme sur une flûte, sont fermés ou ouverts par les doigts ou par des clefs (plus rarement), afin de produire la mélodie. Quand ils sont percés de trous, on parle de « tuyaux mélodiques », mais aussi de « tuyaux semi-mélodiques » selon leur rôle dans la production musicale. Un tuyau dépourvu de trou de jeu s'appelle « bourdon », et il donne une note continue de hauteur fixe. Il y a souvent des décorations de passementerie.

Le tuyau mélodique est équipé d'une anche simple battante (une languette vibrante, comme sur la clarinette) ou double (deux languettes vibrantes, comme sur le hautbois). Ce dernier cas est le plus courant en France (sauf pour la boha landaise) d'où son appellation de hautbois. On utilise également les termes de chalumeau, chanterelle, ' en anglais, ' en breton… Le terme « pied » est aussi utilisé pour parler du tuyau mélodique mais son usage n'est pas approprié dans tous les cas : le pied est, dans le cas de la cornemuse d'Auvergne (cabrette), l'ensemble « tuyau mélodique et tuyau bourdon » disposés parallèlement l'un à l'autre, ou bien, dans le cas de la musette baroque, c'est le double tuyau mélodique. Le terme pied ne devrait s'appliquer qu'à ces deux seules cornemuses où deux tuyaux parallèles (soit mélodique et bourdon, soit deux mélodiques) peuvent être démontés en un geste car ils sont fixés sur la même pièce de bois, elle-même reliée au réservoir. Dans tous les autres cas, on peut parler de hautbois, si le tuyau est bien muni d'une anche double. Car le tuyau mélodique peut être équipé d'une anche simple (Gaïdas, Boha). Ce cas est très fréquent pour les cornemuses de l'Est de l'Europe, en Suède, en Méditerranée, dans le Caucase, au Proche et Moyen-Orient et jusqu'en Inde (où on joue aussi la cornemuse écossaise laissée par les Britanniques).

Certaines cornemuses sont munies d'un tuyau mélodique qui sert à accompagner et ornementer la mélodie principale, et que l'on appelle tuyau semi-mélodique pour cette raison. Comme le tuyau mélodique, à côté duquel il est la plupart du temps placé (et même, il est souvent percé dans le même bloc de bois), il possède des trous de jeu. La hongroise, la boha landaise et la zampogna italienne sont équipées d'un tel tuyau. Il peut y avoir de un à quatre trous (voire cinq plus rarement, sachant que le tuyau mélodique en a toujours plus, c'est-à-dire au moins six et jusqu'à une douzaine sur le '). Les uilleann pipes irlandais, possèdent plusieurs tuyaux semi-mélodiques. Appelés ' en anglais, régulateurs en français, ils sont au nombre de trois, rarement quatre voire cinq. Ils permettent de réaliser des accords d'accompagnement et sont munis de clefs que l'on actionne avec le poignet de la main droite.

Le nombre de bourdons, ces tuyaux, qui servent aussi à l'accompagnement mais dont on ne modifie pas la note produite, donc l'accord, en cours de jeu, est très variable : de un à quatre, qui sont accordés le plus souvent à l'octave ou deux octaves sous la tonique du tuyau mélodique, mais aussi en quinte ou quarte. La cornemuse écossaise en a trois, certaines cornemuses de Serbie également. Mais toutes les cornemuses n'ont pas forcément un bourdon. C'est le cas par exemple du mezwed tunisien ou de la tsambouna grecque. Mais elles possèdent soit un double tuyau mélodique (deux tuyaux strictement jumeaux, placés côte à côte, les doigts bouchant deux trous à la fois), soit un tuyau semi-mélodique.
Par exemple, la cornemuse écossaise Great Highland Bagpipe comporte les pièces suivantes (globalement les mêmes sur toutes les cornemuses, dans le principe tout au moins) :

Le tuyau d'insufflation est muni d'un clapet anti-retour (soupape), permettant à l'air introduit dans le réservoir de ne pas en ressortir.
Toutes les cornemuses ont au moins un tuyau mélodique, pour jouer la mélodie. La différence se fait sur la présence et le nombre de bourdons, la présence et le nombre de tuyaux semi-mélodiques, la présence d'un tuyau d'insufflation ou d'un soufflet.

Sur le réservoir sont fixées une ou plusieurs souches, ligaturées de manière étanche. Dans les souches, on vient introduire les tuyaux de jeu. les souches servent d'intermédiaire entre le réservoir et le tuyau : on peut ainsi détacher les tuyaux pour accorder les anches sans devoir tout défaire. Sur la "grande cornemuse d'Écosse" il y a une souche par tuyau de jeu alors que sur d'autres cornemuses, comme la zampogna, il peut y avoir une souche commune à plusieurs tuyaux. Dans certains cas (cornemuse de Turquie par exemple), le tuyau d'insufflation est raccordé directement, sans souche.

Le réservoir ou poche, ou sac, est généralement fait à partir d'une peau animale presque entière, telle que la chèvre (qui a donné son nom à l'instrument comme c'est le cas pour la "cabrette" auvergnate ou la "koza" polonaise) ou le chien (anciennement pour le biniou kozh). Il est aussi fait dans une pièce de cuir bovin ou ovin (ce qui est le cas dans presque toute l'Europe occidentale). Pour garantir l'étanchéité, cette peau est travaillée de différentes manières. Dans le cas de la peau de chèvre, il est fréquent que les poils qui ont été coupés courts soient conservés à l'intérieur et enduits de sel qui absorbera l'humidité du souffle. Dans le cas de l'utilisation d'une pièce de cuir bovin ou ovin cousue, la surface intérieure est enduite d'une préparation à base de poix ou d'un produit adapté qui en assurent l'étanchéité et absorbent la condensation due au souffle (insufflation buccale) . D'autres réservoirs encore peuvent être constituées d'une vessie (celles que l'on trouve dans la région de la Volga en Russie, par exemple). Les poches les plus récentes ont une poche en matériau synthétique : gore-tex, ou en composite gore tex + cuir à l'extérieur en fonction des souhaits des musiciens, NB : les réservoirs en synthétique doivent être munis d'un système de récupération de la condensation due au souffle du musicien. Le caoutchouc a été abandonné car il vieillissait très mal et gardait l'humidité, nuisible aux anches et à l'hygiène bactérienne interne. Souvent, on glisse le réservoir dans un tissu que l'on appelle la robe ou la housse.

Les tuyaux "sonnants" de la cornemuse fonctionnent grâce à une anche qu'il est (parfois) nécessaire de mouiller quelques minutes avant de pouvoir jouer. Selon le type de cornemuse, on trouve une anche simple, sur le tuyau mélodique et le (les) bourdon(s), comme sur le "koziol" polonais ; ou bien des anches doubles (par ex. certaines "zampogna" italiennes et la musette baroque. D'autres cornemuses encore, fonctionnent avec une anche double pour le tuyau mélodique et une anche simple pour le (les) bourdon(s). C'est le cas par ex. du "sac de gemecs" catalan, de la "veuze" nantaise, de la "cabrette" auvergnate, etc.

Les anches simples :
Elles sont constituées d'une section de roseau (Canne de Provence), fermée à une extrémité et fendue de sorte à dégager une lamelle qui est la partie vibrante, la longueur et le diamètre de l'anche donnent la hauteur de son. C'est sur ces paramètres qu'il faudra jouer pour accorder l'anche au moyen d'une bride en fil poissé qui permet de varier la longueur de la lamelle vibrante et donc hauteur de son et puissance sonore. 
Parfois la lamelle est faite dans une autre matière (comme du bronze sur la musette Béchonnet) et elle est alors liée par de la filasse ou du fil de chanvre poissé et enduit de poix sur le tube sur lequel on a pratiqué au préalable un orifice rectangulaire correspondant à la lamelle.
Il existe sur le marché spécialisé, depuis une quinzaine d'années, un nombre important d'anches simples en matériaux synthétiques tels que ABS, avec lamelle en fibre de carbone, fibre de verre, polycarbonate ou même roseau (anches composites)
NB : Joseph Béchonnet, créateur de la musette du même nom au !!!, fut un précurseur, il a inventé la première anche simple composite : corps en ébène creusé et lamelle en bronze comme sur un accordéon ou un harmonica

L'anche est enfoncée dans un siège à l'extrémité du tuyau de jeu, mélodique (lamelle vers le haut) ou bourdon (lamelle vers le bas). L'air fait vibrer la lamelle en s'engouffrant dans l'anche, puis dans le tuyau, et le tuyau se met à sonner.

L'anche simple est analogue à celles de la clarinette et du saxophone.

Les anches doubles :
Elles sont constituées (comme l'anche d'un hautbois) de deux lamelles de roseau trapézoïdales, affinées (grattées) sur la partie la plus large, et déposées sur un petit tube (le canon, que l'on enfoncera dans son siège au bout du tuyau), et tenues l'une contre l'autre avec du fil, de matière naturelle (lin, coton) ou synthétique, qui est ensuite verni afin de fixer les lamelles, ce qui permet aussi de les accorder (car plus on recouvre les lamelles, plus on raccourcit la surface vibrante et inversement). Il y aussi, dans le cas des anches plus complexes (et plus récentes), une petite barrette de laiton, qui sert à accorder, et qui s'appelle la rasette "(Uilleann pipes)", par ailleurs, sur les "Small pipes" en général, une bride en fil de laiton permet de régler l'ouverture des lamelles de l'anche double, (à l'instar du hautbois classique), cela permettant de modifier la puissance sonore et la hauteur de son
Le canon est garni de fil ou de liège pour ajuster l'anche dans son siège sur le tuyau sonore.

Les anches doubles sont aussi utilisées par le basson, la chalémie, le hautbois, la bombarde, ou encore le cor anglais.

La cornemuse se joue généralement debout car elle demande la pleine capacité des poumons, sauf les modèles à soufflet, qui se jouent assis. S'il suffit d'insuffler le sac pour qu'un son sorte aussitôt par les tuyaux sonnants, il est absolument nécessaire, pour des raisons de stabilité et de tenue de la tonalité, que la poche soit mise à pleine pression afin de procéder à l'accord de l'instrument. Une fois la poche gonflée on peut reprendre une inspiration (parfois certains chantent) car c'est le bras qui sert de régulateur de pression (on souffle donc par alternance), cela permet donc d'avoir un son continu et puissant, le processus de fonctionnement est le même sur un instrument alimenté au moyen d'un soufflet ("Northumbrian pipes, uilleann pipes", cabrette, etc.).
La poche permet aussi d'augmenter la pression en cas de passage à l'octave supérieure si le type de cornemuse le permet (par exemple: Uilleann pipe, Cabrette, Musette du centre France, Gaïtas, etc.).

Elle se joue en solo, en couple avec une bombarde, une clarinette, une vielle ou un accordéon (Centre de la France, Cabrette) en formation de cornemuses, en pipe band (Écosse) ou encore en bagad (Bretagne) accompagnée de bombardes. On y joue tout autant des danses que de la musique militaire ou religieuse, etc.
D'autres cornemuses moins puissantes, telles que la musette de cour ou les uilleann pipes se jouent comme un autre instrument, en solo ou en groupe.

Suivant les cornemuses, le jeu est dit "ouvert" (on lève un doigt de plus pour chaque nouvelle note supérieure), "semi-ouvert" comme sur la cornemuse écossaise-(on lève des doigts et on en abaisse d'autres pour obtenir la note juste), "fermé" comme sur le Northumbrian Small Pipes (tous les doigts restent posés, on lève le doigt correspondant à la note voulue).

La réserve d'air produisant un son continu, sauf pour certains chalumeaux fermés à l'extrémité (cornemuse du Northumberland) ou qu'on joue posés sur la cuisse ("uilleann pipes", qui possèdent une clé destinée à fermer ou ouvrir la pression de l'air sur les bourdons et un STOP pour couper l'air sur l'anche du chanter ou hautbois), il est impossible de détacher les notes par des coups de langue.
Le musicien ne peut utiliser que le jeu de ses doigts pour détacher les notes, soit en staccato (Uilleann pipes, Northumbrian small pipes), ou au moyen d'ornementations, comprenant une, voire plusieurs notes rapides, plus aigües ou plus graves que la note mélodique,
certaines étant d'une extrême complexité comme les Crunluath (7 notes à la suite) dans les Pìobaireachd... (musique originelle de la grande cornemuse écossaise, prononcer pibroch comme le son CH en langue Allemande), la technique de doigté peut donc être très complexe afin de donner de l'expression et du rythme à l'air exécuté sur le hautbois.
L'instrument peut aussi permettre d'utiliser le "glissando" (ouverture progressive d'un trou en glissant le doigt, vers une note plus aigüe) "le vibrato" ou le "trémolo" afin de colorer le morceau de musique.











Angleterre :

Écosse :

Irlande:


Dans la saga de fantasy A comme Association, coécrite par Erik L'homme et Pierre Bottero, Jasper (l'un des héros) est joueur de cornemuse dans un groupe de rock médiéval.





</doc>
<doc id="17930" url="https://fr.wikipedia.org/wiki?curid=17930" title="Otto Diels">
Otto Diels

Otto Paul Hermann Diels (Hambourg - Kiel ) était un chimiste allemand. Lui et Kurt Alder reçurent conjointement le prix Nobel de chimie en 1950.

Otto Diels a étudié la chimie de 1895-1899 à l'Université Humboldt de Berlin, où après avoir occupé différentes positions il devint professeur et chef du département de chimie en 1915. L'année suivante, il prit le poste de professeur de chimie à l'université de Kiel, qu'il ne quitta plus jusqu'à sa retraite en 1945.

Son plus célèbre travail fut fait en collaboration avec Kurt Alder, sur la réaction qui porte leurs deux noms : la réaction de Diels-Alder. Les deux hommes obtinrent conjointement le prix Nobel de chimie de 1950 .



</doc>
<doc id="17931" url="https://fr.wikipedia.org/wiki?curid=17931" title="Vimy">
Vimy

Vimy est une commune française située dans le département du Pas-de-Calais, en région Hauts-de-France. Elle fait partie de la Communaupole de Lens-Liévin (communauté d'agglomération) qui regroupe et comptait en 2010. Elle est traversée par la RN 17 qui relie Arras et Lens.

Par arrêté préfectoral du , la commune est détachée le de l'arrondissement d'Arras pour intégrer l'arrondissement de Lens.

Vimy est située à équidistance de Lens et d'Arras. Son plateau domine le bassin minier, tandis que la partie basse de la ville est située dans la plaine de la Gohelle, qui s'étend au pied de la crête de Vimy.

La côte de Vimy correspond à une faille (la faille de Marqueffles) qui a abaissé les terrains crayeux du nord par rapport à des terrains de même nature au sud. Les terrains, sensibles à l'érosion, ont donc connu cette perturbation récemment (à l'échelle des temps géologiques).
Vimy possédait autrefois le château d'Adam de Vimy datant de 1249, qui s'élevait à l'emplacement de l'actuel hôtel de ville, point culminant de la partie basse de la ville. Celui-ci a notamment été utilisé comme refuge pour accueillir les blessés de la bataille de Lens du 20 août 1648. Vimy et son château furent l'enjeu de plusieurs batailles, comme en 1349, lorsque Vimy fut attaquée par les Anglais, puis entre 1708 et 1712, lors de la guerre de Succession d'Espagne. Le château fut détruit en 1833. On découvrit alors des centaines de squelettes dans l'ancienne cour du château. Leur présence fut attribuée à la bataille de Lens et à l'utilisation du château comme hôpital militaire par les troupes de Condé, mais il semblerait plutôt qu'ils proviennent d'une nécropole romaine sur l'emplacement de laquelle fut élevé le château.

Du 9 au 12 avril 1917, les soldats du corps canadien du général Julian Byng attaquent la crête de Vimy pour la reprendre aux Allemands. Cette bataille est une victoire pour les Canadiens, qui réussissent à prendre la cote 145 et à réaliser tous leurs objectifs, au prix de morts. Cette victoire, là où les armées britanniques et françaises avaient échoué pendant plus de deux ans, donne aux troupes canadiennes le statut de troupes d'élite, permet au Canada d'avoir une position indépendante lors de la signature du traité de Versailles, et marque l'émergence de la nation canadienne.

Une grande partie des mares que l'on voit bien en avion autour de Vimy sont en fait d'anciens trous d'obus, mais ce ne sont pas les seules séquelles de guerre. Le phénomène de remontée naturelle des obus (Cf. effet gel/dégel et dynamique du sol) fait qu'on trouve encore couramment des munitions non explosées dans les champs et les jardins.

Une zone boisée, interdite au public et encore non déminée existe encore sur le secteur canadien du mémorial canadien de Vimy, en partie pâturée par des moutons, avec possibilité de présence d'armes chimiques. 144 chambres souterraines et 30 tunnels ont été identifiés, et 13 effondrements ont eu lieu, rien qu'entre 2002 et 2005, date à laquelle les historiens, malgré les efforts d'une dizaine d'historiens anglais n'avaient pas encore retrouvé les plans de toutes les sapes, tranchées et tunnels du côté allemand.

À la suite d'un rapport d'expertise du alertant sur l'état préoccupant du stock d'obus chimiques de Vimy (jugé dans un « état de dégradation extrême », en raison du « danger d'une explosion imminente »), habitants ont été évacués le , pour le transfert sécurisé de 55 tonnes de munitions chimiques réfrigérées, en camions blindés vers le camp de Suippes (Marne). Pour respecter les conventions internationales, les pays n'ont plus de droit de rejeter de munitions anciennes à la mer, ni de les "pétarder" sur la côte (comme cela s'est fait durant des décennies dans l'estuaire de la Somme).

Le projet français Sécoia de construction d'une usine de démantèlement d'armes chimiques a pris beaucoup de retard et est finalement lancé avec la Loi de programmation militaire de 2003-2008, le site pourrait être opérationnel en 2016, et les capacités belges et allemandes suffisent à peine à leurs propres besoins.

Le caractère calcaire des sols de ce secteur a limité les transferts de métaux lourds issus des munitions, mais il existe des poches un peu plus acides, en forêt notamment, et l'observation des billes de plomb des obus shrapnell dans le sol montre qu'elles ont perdu une partie de leur plomb dans l'environnement. Aucune étude écotoxicologique ne semble avoir dans ce secteur porté sur le devenir du plomb et du mercure ou d'autres éléments chimiques faisant partie des séquelles de guerre. Il est possible que localement, les champignons (et, donc, certaines espèces gibier qui s'en nourrissent), ou le bois aient pu bioconcentrer certains de ces toxiques. Il serait par exemple intéressant d'analyser les foies et reins de sangliers, faisans, bécasses, écureuils ou moutons pour évaluer une éventuelle contamination de l'écosystème.

C'est sur le territoire de la commune voisine de Givenchy-en-Gohelle que se trouve le mémorial de Vimy, le plus important monument canadien aux victimes de la Première Guerre mondiale. Le monument s'élève au sommet de la cote 145 pour laquelle se sont battus les soldats canadiens en avril 1917. Il rend hommage au rôle des Canadiens lors de ce conflit, au moyen de figures de pierre symbolisant les valeurs défendues et les sacrifices faits. Érigée entre 1925 et 1936 sur le site de la bataille de la crête de Vimy, cette œuvre d'art est le fruit du travail d'artistes canadiens, l'architecte et sculpteur canadien Walter Seymour Allward.

Les deux pylônes, représentant le Canada et la France, culminent 40 mètres au-dessus de la base du monument. En raison de l'altitude du site, la figure la plus élevée domine la plaine de Lens d'environ 110 mètres.

Le terrain d'assise du mémorial ainsi que la centaine d'hectares qui l'entoure ont été donnés au peuple canadien par la France en 1922. Cela en signe de gratitude pour les sacrifices faits par plus de Canadiens au cours de la Grande Guerre et notamment pour la victoire remportée par les troupes canadiennes en conquérant la crête de Vimy au cours du mois d'.

En s'avançant à l'avant du monument, on peut remarquer une statue de femme voilée, tournée vers l'est, vers l'aube d'un nouveau jour. Elle représente le Canada, une jeune nation, pleurant ses fils tombés au combat. L'arête de Vimy est aujourd'hui boisée, chaque arbre a été planté par un Canadien et symbolise le sacrifice d'un soldat.

Les pierres calcaires choisies par Walter Allward viennent de Croatie. Elles sont montées sur une structure en béton. Les pierres d'origine s'étant abîmées avec le temps, des travaux de restauration ont été entrepris en 2005 et se sont achevés en 2007. La reine Élisabeth II a participé à l'inauguration le .

La population de la commune est relativement jeune. Le taux de personnes d'un âge supérieur à 60 ans (21,6 %) est en effet identique au taux national (21,6 %) tout en étant toutefois supérieur au taux départemental (19,8 %). 
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (51,8 %) est du même ordre de grandeur que le taux national (51,6 %). 
La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante : 







</doc>
<doc id="17933" url="https://fr.wikipedia.org/wiki?curid=17933" title="Koxinga">
Koxinga

Koxinga (en ), dont le titre honorifique est Zhèng Chénggōng, né en 1624 à Hirado et mort en 1662 à Taïwan, est un pirate et général chinois.

Koxinga naît en 1624 à Hirado au Japon. Son père, Zheng Zhilong (鄭芝龍) est un pirate, un marin et un marchand originaire d'une famille de pêcheurs du Fujian. Il va souvent à Hirato pour faire du commerce et c'est là qu'il rencontre la mère de Koxinga, la Japonaise (田川松). Il est élevé jusqu'à ses sept ans par sa mère puis son père l'emmène à Nan'an, près de Quanzhou au Fujian pour son éducation. Il y reçoit l'enseignement des érudits confucéens afin de pouvoir passer l'examen impérial.

La dynastie Ming tombe en 1644 au profit des Mandchous venus du Nord qui fondent la dynastie Qing. Cependant les princes du Sud restent fidèles aux Ming et continuent la guerre afin de prolonger le pouvoir politique des Ming. Le prince Tang est un de ces princes combattants, très influents dans la région de Fuzhou. Il reçoit l'appui de Zheng Zhilong qui s'arrange pour que Zheng Chenggong serve à ses côtés. Le prince accorde alors à ce dernier le droit de porter le nom de la famille impériale Ming, Zhu (朱), ce qui vaudra à Zheng Chenggong son surnom populaire de Guó Xìng yé (國姓爺/国姓爷) soit « Grand père au nom national ». Approximativement transcrit par les Néerlandais, ce surnom devient Koxinga dans les pays occidentaux. La fidélité aux Ming n'est pas la seule raison qu'ont les pirates tels que Koxinga et Zheng Zhilong de combattre les Qing. Ceux-ci ont en effet restreint le commerce par différentes mesures défavorables et la situation des pirates marchands devient beaucoup plus délicate que sous les Ming.

En 1646, le prince Tang meurt, trahi par Zheng Zhilong qui s'était rendu compte que la lutte était sans espoir et que les Qing allaient l'emporter. Koxinga n'est pas de cet avis et continue le combat. Il prend comme base les deux cités de Xiamen et Jinmen et soutient le prince Gui (le futur empereur Yongli) par des raids dans les régions de Fujian, Guangdong et Zhejiang. Il reçoit de l'empereur Yongli le titre de prince et la préfecture de Yanping. Il pousse ses razzias jusqu'à Nankin mais est repoussé progressivement jusqu'à se sentir acculé.

Il se rend compte qu'il lui faut une base plus solide pour assurer ses arrières. Il décide donc de prendre l'île de Taïwan et d'en chasser les Hollandais. Il débarque à Taïwan près de Tainan le 30 avril 1661 avec hommes, et entreprend le siège de Fort Zeelandia. Les Hollandais se rendent le février 1662, et Koxinga signe un traité avec le gouverneur hollandais Frederick Coyett. Koxinga meurt quelques mois plus tard le 23 juin 1662, Son fils Zheng Jing lui succède.

Koxinga est considéré à Taïwan comme un héros national et le père de la nation taïwanaise.



</doc>
<doc id="17934" url="https://fr.wikipedia.org/wiki?curid=17934" title="George Boole">
George Boole

George Boole, né le à Lincoln (Royaume-Uni) et mort le à Ballintemple (Irlande), est un logicien, mathématicien et philosophe britannique. Il est le créateur de la logique moderne, fondée sur une structure algébrique et sémantique, que l'on appelle algèbre de Boole en son honneur.

Il a aussi travaillé dans d'autres domaines mathématiques, des équations différentielles aux probabilités en passant par l'analyse. Autodidacte, il publia ses premiers travaux d'algèbre tout en exerçant son métier d'instituteur et de directeur d'école dans la région de Lincoln. Ses travaux lui valurent en 1844 la "Royal Medal" de la "Royal Society", puis une chaire de mathématiques à l'université ("Queen's College") de Cork en 1849.

De 1844 à 1854, il crée une algèbre binaire, dite booléenne, n'acceptant que deux valeurs numériques : 0 et 1. Cette algèbre aura de nombreuses applications en téléphonie et en informatique, notamment grâce à Claude Shannon en 1938, près d'un siècle plus tard.

George Boole est né à Lincoln le 2 novembre 1815. Son père, John Boole, est cordonnier, sa mère, Mary Ann Joyce, femme de chambre. Les revenus modestes de sa famille ne lui permettent pas d'effectuer sa scolarité dans des écoles prestigieuses. Il étudie donc dans des écoles locales. C'est son père, passionné par les sciences, les mathématiques et les instruments optiques qui lui transmet et l'initie aux mathématiques. 

Ses capacités intellectuelles sont remarquables : il apprend le latin avec l'aide de William Brooke, un libraire, et de manière autonome l'allemand, le français et le grec. À l'âge de quatorze ans il traduit du grec un poème de Méléagre, , publié dans le "Lincoln Herald", un journal local. Son âge étant précisé dans l'article, un professeur écrit au journal estimant impossible qu'un jeune garçon soit capable d'une telle traduction. Cette première "controverse" et les critiques faites à sa traduction le poussent à intensifier ses efforts pour maîtriser les langues anciennes.

À seize ans, obligé de travailler pour soutenir sa famille, il devient enseignant adjoint dans une école de Doncaster. Il enseigne ensuite à Liverpool et à Waddington. En 1834, il revient à Lincoln et ouvre sa propre école. Bénéficiant d'une certaine réputation locale, l'allocution pour la présentation à Lincoln d'un buste d'Isaac Newton lui est confiée. Publié dans la "Gazette Office" en 1835, ce premier article scientifique de George Boole montre à la fois sa connaissance des œuvres de Newton et, de par les critiques qu'il formule, un certain aplomb. En 1838, à la mort de Robert Hall, son ancien employeur à Waddington, George Boole lui succède à la tête de l'école.

Pendant toute cette période il poursuit, en autodidacte, son apprentissage des mathématiques en débutant, à seize ans, par la lecture du "Traité du calcul différentiel et du calcul intégral" de Lacroix. Bénéficiant des moyens de l'Institut de mécanique de Lincoln fondé en 1834, dont son père est le premier conservateur, il se confronte aux œuvres d'Isaac Newton ("Principia"), Pierre-Simon de Laplace ("Mécanique céleste") et Joseph-Louis Lagrange ("Mécanique analytique"). Bien plus tard il estimera avoir perdu près de cinq ans à progresser lentement, tentant d'apprendre seul, sans professeur pour le guider.

En 1839, il écrit son premier article de mathématiques (ce n'est pas le premier publié) qui trouve son origine dans la "Mécanique analytique" de Lagrange; au cours de sa lecture il prend des notes et envisage des améliorations. 

Il entre alors en contact avec D. F. Gregory, qui vient tout juste de créer le "Cambridge Mathematical Journal" ("CMJ"). Il lui soumet d'abord un autre article, lui aussi inspiré par ses lectures de Lagrange : . D. F. Gregory lui répond, dans une lettre datée du 4 novembre 1839, qu'une fois quelques corrections apportées, il serait heureux de le publier ainsi que son article sur le calcul des variations qu'il lui a précédemment mentionné. Ces deux articles et deux autres sont publiés dans le volume 2 du "CMJ". Influencé par Gregory, bénéficiant aussi de son soutien et de ses conseils, George Boole commence à publier régulièrement dans ce journal. 

En 1842 il commence à correspondre avec Auguste De Morgan, avec qui il devient ami. Suite à la publication de deux articles dans le volume 3 du "CMJ", Arthur Cayley lui écrit une première lettre en 1844 pour complimenter son travail. C'est là aussi le début d'une longue amitié.

En 1844, après la publication d'un mémoire dans les "Philosophical Transactions" sur une approche algébrique de la théorie des équations différentielles, la "Royal Society" lui décerne une médaille.

Il commence alors une série de travaux posant les bases de ce qu'on nommera plus tard l'algèbre de Boole. En 1847 sort "Mathematical Analysis of Logic", puis "An Investigation Into the Laws of Thought, on Which are Founded the Mathematical Theories of Logic and Probabilities" en 1854. Boole y développe une nouvelle forme de logique, à la fois symbolique et mathématique. Le but : traduire des idées et des concepts en équations, leur appliquer certaines lois et retraduire le résultat en termes logiques. Pour cela, il crée une algèbre binaire, dite booléenne, n'acceptant que deux valeurs numériques : 0 et 1. Cette algèbre est définie par la donnée d'un ensemble E (non vide) muni de deux lois de composition interne (le ET et le OU) satisfaisant à un certain nombre de propriétés (commutativité, distributivité...). Les travaux de Boole, s'ils sont théoriques, n'en trouveront pas moins des applications primordiales dans des domaines aussi divers que les systèmes informatiques, la théorie des probabilités, les circuits téléphoniques, hydrauliques et pneumatiques, grâce à des scientifiques comme Peirce, Frege, Russell, Turing et Shannon.

En 1849, George Boole se voit proposer une chaire de professeur des mathématiques au "Queen's College" de Cork, en Irlande. 

En 1857, il est nommé membre de la Royal Society. Il s'intéresse ensuite aux équations différentielles à travers deux traités qui auront une influence certaine : "" (1859) et "Treatise on the Calculus of Finite Differences" (1860).

Il épouse le , elle-même mathématicienne autodidacte. Mary Everest était la nièce de sir George Everest, le responsable de la mission cartographique qui baptisa le mont Everest.

George Boole meurt d'une pneumonie le 8 décembre 1864. Il avait pris froid après s'être rendu au "College". Croyant au principe d'analogie, au sens de « soigner le mal par le mal », Mary l'avait alité et aspergé d'eau pour le guérir.

George et Mary ont eu cinq filles : 

Un cratère de la Lune porte le nom de Boole.








</doc>
<doc id="17935" url="https://fr.wikipedia.org/wiki?curid=17935" title="Lois du mouvement de Newton">
Lois du mouvement de Newton

Les lois du mouvement de Newton ont été énoncées dans son ouvrage "Philosophiae naturalis principia mathematica" en 1687. Il s'agit en fait des "principes" à la base de la grande théorie de Newton concernant le mouvement des corps, théorie que l'on nomme aujourd'hui mécanique newtonienne ou encore mécanique classique. À ces lois générales du mouvement fondées en particulier sur le principe de relativité des mouvements, Newton a ajouté la loi de la gravitation universelle permettant d'interpréter aussi bien la chute des corps que le mouvement de la Lune autour de la Terre.

L'énoncé originel de la première loi du mouvement est le suivant :

Dans la formulation moderne de la loi, on parle de mouvement rectiligne uniforme, et on remplace la notion de force (unique) par celle, plus générale, de résultante des forces appliquées sur le corps. Autrement dit, s'il n'y a pas de force qui s'exerce sur un corps (corps isolé), ou si la somme des forces (ou force résultante) s'exerçant sur lui est égale au vecteur nul (corps pseudo-isolé), la direction et la norme de sa vitesse est constante ou, ce qui revient au même, son accélération est nulle. Cette première loi infirme la conception héritée d'Aristote, .

Le mouvement considéré par Newton a lieu par rapport à un espace mathématique abstrait qu'il suppose absolu. Sa première loi s'applique également dans des référentiels en translation uniforme par rapport à cet espace absolu, ce qui donne naissance à la notion de référentiel galiléen. Au , la notion d'espace absolu est peu à peu abandonnée au profit des seuls référentiels galiléens. La première loi de Newton se reformule donc aujourd'hui sous la forme :

La définition d'un référentiel galiléen apparaît fondamentale et est souvent formulée ainsi :

Ainsi la première loi de Newton ne s'applique que dans un référentiel galiléen et un référentiel galiléen est un référentiel où la première loi de Newton s'applique… ce qui apparaît comme une définition circulaire. Pour éviter ce problème, on réécrit le principe d'inertie sous la forme axiomatique suivante :

La détermination d'un "bon" référentiel galiléen est en réalité expérimentale et comme souvent en physique, seule la cohérence entre la théorie (ici la première loi de Newton) et la mesure (mouvement rectiligne uniforme) valide le choix "a posteriori".

L'énoncé original de la deuxième loi de Newton est le suivant :
Dans sa version moderne, on la nomme principe fondamental de la dynamique (PFD), parfois appelée relation fondamentale de la dynamique (RFD), et s'énonce ainsi :
Cette expression se simplifie dans le cas où la masse est constante :

Ceci est souvent récapitulé dans l'équation :
où :

Pour un corps soumis à une résultante des forces nulle, on retrouve bien la première loi de Newton, c’est-à-dire un mouvement rectiligne uniforme. En première analyse, on peut se demander quelle est l'utilité de la première loi puisqu'elle semble être une conséquence de la deuxième. En réalité, dans l'énoncé de Newton, il n'en est rien car la première loi n'est pas présentée comme un cas particulier de la deuxième mais comme une "condition suffisante" à l'application de cette dernière.

En effet, énoncer la première loi, c'est tout d'abord affirmer l<nowiki>'</nowiki>"existence" des référentiels galiléens. Cela constitue un postulat extrêmement fort qui permet, dans les exposés modernes de la mécanique classique, de définir les repères galiléens qui sont les seuls repères dans lesquels la seconde loi est valide. En l'absence de la première loi, la seconde loi est inapplicable puisqu'on ne peut pas définir son domaine de validité. Par conséquent, l'ordre logique dans lequel les lois sont énoncées n'est pas le fruit du hasard mais bien celui d'une construction intellectuelle cohérente.

Ensuite, cette première loi énonce le principe d<nowiki>'</nowiki>"isolement" du solide : on considère les forces "extérieures" qui agissent sur lui, et on ne prend pas en compte ce qui se passe en interne.

Certains étudiants éprouvent de grandes difficultés pour utiliser les lois de Newton telles qu’elles sont traditionnellement énoncées et ce n’est pas sans raison. En effet, les forces semblent s’exercer comme si elles existaient en elles-mêmes, "ex abrupto".

Il s'agit essentiellement d'un problème conceptuel, celui de modélisation. Dans la réalité, il n'y a pas de vecteur, il y a :
Il faut donc d'une part maîtriser les notions de dérivée seconde et de vecteur, mais aussi comprendre comment on peut passer de phénomènes très divers — chocs des molécules d'un fluide, répulsion des nuages électroniques, interactions de Van der Waals, interaction avec des photons, gravitation — à une « flèche » (vecteur force).

L'application de ce principe doit suivre une procédure rigoureuse :
Cette dernière étape impose de connaître les ordres de grandeur des différentes interactions entre les objets matériels en présence afin de les hiérarchiser ; dans la modélisation de la situation où se pose le problème à résoudre, on ne retiendra que les plus intenses.

L'énoncé original est le suivant :
De manière moderne, on exprime que :

A et B étant deux corps en interaction, la force formula_8 (exercée par A sur B) et la force formula_9 (exercée par B sur A) qui décrivent l'interaction sont directement opposées :
Ces forces ont la même droite d'action, des sens opposés et la même norme. Ces deux forces sont toujours directement opposées, que A et B soient immobiles ou en mouvement.

Il faut là encore revenir sur la modélisation, c'est-à-dire sur le passage de la réalité à la description vectorielle. Dans le cas d'une action de contact, c'est assez simple : si Albert pousse de sur Béatrice, alors Béatrice pousse également de sur Albert ; Albert et Béatrice peuvent être sur un sol adhérent ou de la glace, immobiles ou en train de patiner. Il est souvent plus difficile de comprendre que si Albert s'appuie sur le mur, alors le mur pousse aussi sur Albert ; le mur n'a pas de « volonté motrice », il fléchit sous l'effet de l'action d'Albert mais cette flexion est indécelable sauf pour une paroi souple, et Albert subit donc un « effet ressort ». Il est de même pour la notion de sol qui soutient Albert ; en particulier, en cas de saut, il est difficile d'imaginer que c'est le sol qui propulse Albert, toujours par effet ressort.

Le cas des actions à distance est également difficile à conceptualiser, en particulier le fait qu'Albert attire lui aussi la Terre...

Cette loi est parfois appelée "loi d'action-réaction", en référence à l'énoncé original ; une formulation au mieux imprécise, au pire entraînant de nombreuses confusions. En particulier, cette ancienne formulation véhicule l'idée qu'il y a toujours une force qui est la « cause » (l'action), l'autre n'étant qu'une sorte de conséquence (la réaction).

Une autre difficulté rencontrée par les étudiants est l'oubli que ces deux forces formula_8 et formula_9 s'exercent sur deux corps différents. Elles ne peuvent donc pas « s'annuler mutuellement ». L'effet d'annulation n'intervient que lorsqu'on considère un système constitué de différents corps et que l'on s'intéresse à la résultante des forces : dans ce cas, les forces intérieures s'annulent en effet et seule la somme des forces extérieures]] est à prendre en compte (ce qui est heureux pour étudier le mouvement d'un solide constitué de plus de 10 éléments).

La loi des actions réciproques a l'inconvénient de supposer l'application des forces comme instantanée (ce qui est abandonné en relativité restreinte). Dans le cas des forces à distance, il convient dans certains cas d'effectuer des transformations pour tenir compte du retard de propagation.

Cette correction ne relève pas de la relativité. Comme les forces électromagnétiques s'appliquent à distance, on avait mis en évidence que ces forces se propagent à la vitesse de la lumière et non à vitesse infinie et inclus cette nuance dans les équations avant la révolution de la relativité restreinte.

Certains auteurs (minoritaires) appellent "quatrième loi de Newton" sa loi universelle de la gravitation. Cette dénomination est très contestable, mais elle est mentionnée ici à cause de la parenté historique des lois : si cette loi ne fait pas partie des principes de la mécanique au même titre que les trois autres et le principe de relativité, la première réussite de Newton fut d'utiliser ses lois mécaniques plus sa loi d'interaction gravitationnelle pour démontrer les lois empiriques de Kepler. Ce sont ces premiers succès qui établirent pour longtemps la domination des lois de Newton sur la science.

Notons qu'en combinant cette loi et le principe fondamental de la dynamique, on démontre la prédiction de Galilée selon laquelle, dans le vide, tous les objets tombent à la même vitesse (en admettant implicitement que masse inertielle et masse gravitationnelle sont égales).

Newton dans ses Principia a mis en évidence la notion de relativité du mouvement dans les définitions précédant le livre premier. Il introduit dans les scholies II et IV la notion d'espace absolu et indique dans le corollaire V des lois que :

ce qui préfigure la notion de référentiel galiléen telle qu'elle est définie aujourd'hui. Cependant, Newton ne fait aucune référence au cas où un référentiel n'est pas en mouvement rectiligne uniforme par rapport à ce qu'il appelle "l'espace absolu", et aucune infirmation de la validité de ses lois dans les référentiels accélérés n'est donnée dans les "Principia". Il faudra attendre les travaux de Gaspard Coriolis et de Foucault au pour que la notion de référentiel galiléen telle qu'elle est connue aujourd'hui se dégage et pour que les formules de changement de repère vers (ou depuis) un référentiel non galiléen soient établies.

Le principe de relativité s'énonce comme suit :

On pourra le vérifier en admettant les trois premières lois, l'invariance du temps, de la masse et des forces (implicite en physique pré-einsteinienne). C'est pourquoi ce principe est appelé ici corollaire.

Ce principe est dit principe de relativité galiléenne car on en trouve la trace dans le célèbre Dialogue de Galilée, quoique Galilée ait supposé qu'il en était de même pour une rotation uniforme.

Une formulation plus moderne affirme que toutes les lois de la physique sont les mêmes pour deux référentiels d'espace en translation rectiligne uniforme l'un par rapport à l'autre. C'est cette formulation forte qui est à la base de la relativité restreinte.


Isaac Newton a énoncé ses lois dans le premier volume de son "Philosophiae Naturalis Principia Mathematica" en 1687 et, à l'aide des nouveaux outils mathématiques qu'il a développés, il a prouvé beaucoup de résultats au sujet du mouvement des particules idéalisées.

Il convient de nuancer : si Newton avait connaissance des travaux de Galilée, son rôle a été de formaliser les idées de Galilée et d'en tirer les conséquences qui ont permis de construire la mécanique. Quand Newton affirme « Si j'ai vu plus loin que les autres, c'est parce que j'ai été porté par des épaules de géants. », le lecteur averti est censé comprendre que le travail s'inscrit dans la continuité de celui de Galilée. En fait, on pourrait même dire que Newton n'a pas précisé que le principe d'inertie et le principe de relativité, sur lesquels il s'est fondé pour construire toute la mécanique, ont été édictés par Galilée, tout simplement parce qu'il estime que le lecteur est censé le savoir !

Les deux premiers volumes sont mathématiques. Dans le troisième volume, la philosophie naturelle (ancienne dénomination de la physique des phénomènes naturels) est expliquée : il a montré comment ses lois du mouvement combinées à sa loi universelle de la gravitation expliquent le mouvement des planètes et permettent de dériver les lois de Kepler.

Les lois sus-citées ont été mises en forme et édictées par Newton. Mais les fondements proviennent de travaux antérieurs : Galilée, Torricelli, Descartes, Huygens, Hooke, « J'ai été porté par des épaules de géants. » reconnaissait lui-même Newton.

D'autre part, comme l'a fait remarquer Ernst Mach :

Dans cette critique, Mach fait référence à la définition IV des "Principia", laquelle introduit la notion de force, fondamentale en physique :

Mais on peut aller encore plus loin : la conservation de la quantité de mouvement de systèmes peut être érigée en principe premier de la mécanique. Cette démarche présente l'avantage de reposer sur un concept, la quantité de mouvement, permet de traiter des problèmes de mouvements relativistes.

De plus la troisième loi permet d'introduire le concept d"'interaction" qui n'est pas trivial mais, lui aussi, fondamental en physique. À l'époque, cette loi est une absurdité, si l'on se réfère par exemple au point de vue d'Aristote chez qui la magie et autres actions à distance n'existent pas dans le cadre de la physique. Rappelons que le magnétisme est interprété depuis le "de Magnete" de Gilbert par des « lignes spectrales », ou tourbillons. De même, la cause de la gravitation est interprétée par Descartes "via" une théorie (fausse) de tourbillons, si contradictoire que . Par contre, Newton déclarera dans une phrase restée célèbre : "hypotheses non fingo", je ne chercherai pas la cause ultime de la gravitation. La gravitation « s'exprime » au travers de la loi centripète qu'il énonce, il ne fait aucune supposition sur la nature de cette force.

Newton sortait donc hardiment du cadre imposé par la physique de l'époque, d'où une critique véhémente, l'action à distance étant récusée (elle gênait d'ailleurs Newton lui-même), comme insensée (Rømer venait de montrer la finitude de la célérité de la lumière). En 1906, Poincaré proposera une hypothèse moins choquante : la gravitation se propage à la vitesse limite c.

Les lois de Newton peuvent être construites à partir de thèses plus abstraites.

Les lois de Newton ont subi l'analyse critique de Laplace, puis Ernst Mach, puis Poincaré, puis de Kolmogorov.

Selon leur analyse le principe fondamental de la dynamique peut être ramené à une conséquence du déterminisme énoncée par Laplace dans son traité sur les probabilités : 
Ainsi l'orbite hamiltonienne de l'électron dans le plan des phases ["x"("t" ), "p"("t" )] est déterminée par le PFD. C'est tout ce qu'affirme ce principe, puisque, par ailleurs, il faut trouver expérimentalement la loi F("x, v, t" ).

Même si le déterminisme tel que le définit Laplace souffre de limites, il est tout de même possible de montrer que le théorème de la quantité de mouvement repose sur les principes mêmes de la physique : c'est en effet une conséquence du théorème de Noether.

Newton avait postulé : il existe un espace et un temps absolu.

En fait, on pouvait étendre à toute une classe de référentiels dits « inertiels » la notion d'espace absolu : quête sans fin, mais de plus en plus précise. . Mais entièrement Galilée qui défendait l'équivalence entre un référentiel et un autre évoluant à vitesse constante par rapport au premier.

Par contre, Newton se méfiait du temps absolu : il savait qu'en changeant l'échelle de temps, l'expression de son PFD changeait. Il l'a même savamment utilisé. Mais évidemment, il fallait prendre une décision : quelle échelle de temps choisir ? Ce qui paraissait le plus simple était la fameuse loi de Kepler. Et tout était cohérent.

Les notions de temps relatif, de finitude des vitesses, de synchronisation et de transport du temps allaient nécessiter encore beaucoup de découvertes avant d'être entrevues. Il a donc opté pour le temps dynamique absolu et édicté : le temps absolu s'écoule uniformément. C'est cette variable "t" qui intervient quand on écrit
puis
et donc :
Ce temps absolu est généralement admis tant qu'on n'emploie pas la relativité restreinte. Mais il constitue néanmoins une hypothèse philosophique forte qui a été régulièrement discutée par Leibniz notamment :

Une des grandes difficultés des théories de Newton, mise à jour dès le , est la notion d'action "instantanée" à distance. Newton lui-même par cette supposition présente tout aussi bien dans sa théorie de la gravitation que dans sa troisième loi. 
Plus tard au cours du , un certain nombre de difficultés, concernant l'électromagnétisme notamment, indiquèrent également que les principes de Newton ne pouvaient pas rendre compte en l'état de tous les problèmes mécaniques ou cinématiques.

La relativité restreinte démontre qu'aucune interaction ne se propage plus vite que la vitesse de la lumière dans le vide et remet donc définitivement en cause les interactions instantanées. De plus, elle montre que pour des objets dont la vitesse est proche de celle de la lumière, les lois de Newton ne sont absolument plus fidèles à l'observation.

Cependant, les formules de la relativité restreinte permettent de considérer la physique newtonienne comme une approximation en supposant la vitesse de la lumière infinie. Ainsi, la relativité permet de justifier les équations de Newton dans les cas de faibles vitesses en la rendant démontrable à partir d'une théorie plus générale qui l'englobe. Les lois de Newton s'appliquent donc à la plupart des applications quotidiennes de la mécanique, que l'on qualifie alors de « classique » (chute des corps, mouvement des véhicules, moteurs, etc.).

En revanche, il existe des situations où les résultats sont radicalement modifiés, par exemple celles créées au sein des accélérateurs de particules (comme celui du CERN). L'énergie cinétique apportée à une particule de charge "q" par une tension V vaut "q" V. Les énergies cinétiques mises en jeu dans les accélérateurs de particules peuvent actuellement monter jusqu'à l'ordre du téravolt ( milliards de volts). 
On calculerait par exemple, selon les équations newtoniennes, pour un électron ayant acquis une telle énergie cinétique, une vitesse supérieure à celle de la lumière. 
La vitesse réelle, calculée dans le cadre relativiste est celle d'une fraction de la vitesse de la lumière légèrement inférieure à l'unité. 
Il est donc essentiel de bien distinguer les situations où les lois de Newton restent de très bonnes approximations de celles où elles perdent toute pertinence.

En relativité restreinte, les forces respectent toujours un théorème de la quantité de mouvement mais adapté, faisant apparaître le facteur de Lorentz. Le théorème de la quantité de mouvement est donc un théorème très puissant, puisqu'il permet de déduire les lois de Newton dans le cas où les faibles vitesses le permettent. Dans le cas contraire il s'inscrit dans les résultats de la relativité restreinte.

La mécanique newtonienne étudie surtout les systèmes macro-physiques. Dans ce contexte, l'espace et l'énergie sont implicitement considérés comme étant continus. Or, le monde de la mécanique quantique est celui des systèmes micro-physiques, pour lesquels . 
La mécanique newtonienne s'appuie notamment sur le concept de force, sachant que la force dérive d'un potentiel (pour un système mécanique isolé). Toutefois, pour les systèmes micro-physiques (relevant de la mécanique quantique), la notion de force ne peut pas être définie puisque l'énergie potentielle comme les coordonnées d'espace sont quantifiées. En effet, en mathématique, la dérivée d'une fonction discontinue n'est pas définie.
La mécanique de Newton trouve donc ses limites pour l'étude des systèmes micro-physiques, puisque l'hypothèse implicite fondée sur un espace et une énergie continus est mise à mal pour ces systèmes.

Si on connaît la trajectoire formula_16 d'un corps, on connaît formula_17 à chaque instant avec formula_18.
À l'inverse, si on connaît la vitesse formula_17 d'un corps et la position initiale formula_20, on connaît formula_16 à chaque instant avec formula_22.

Dans ces deux cas, si on s'intéresse à un seul axe, formula_23 par exemple, on voit qu'il est possible de connaître en même temps la position formula_24 et la vitesse formula_25 avec une précision infinie, ce qui est contraire au principe d'incertitude de la physique quantique.



</doc>
<doc id="17939" url="https://fr.wikipedia.org/wiki?curid=17939" title="Instrument à cordes frottées">
Instrument à cordes frottées

Les instruments à cordes frottées sont utilisés avec un archet, à l'exception de quelques instruments comme la vielle à roue, dont les cordes sont frottées par le bord d'un disque.




</doc>
<doc id="17940" url="https://fr.wikipedia.org/wiki?curid=17940" title="Steel-drum">
Steel-drum

Un steel-drum ou steeldrum, c'est-à-dire « tambour d'acier » en anglais, plus couramment appelé pan (« casserole ») ou steelpan, est un instrument de percussion idiophone mélodique.

Il est originaire de Trinité-et-Tobago (Caraïbes) et répandu dans des orchestres "steelbands", typiquement composés de plusieurs de ces instruments différents. Les pans constituent donc une famille d'instruments.

Un pan est fait à partir de fûts en métal de utilisés par l'industrie pétrolière pour stocker et transporter de l'essence ou de l'huile, ou encore de la compote, des extraits de parfums... Ils sont sectionnés et la face inférieure de ces bidons est emboutie puis martelée pour y réaliser un ensemble de facettes se comportant chacune comme une cloche. Les différentes facettes sont accordées sur une gamme tempérée.

Il existe de nombreux types de pans, regroupés en sections qui vont des graves aux aigus en passant par les médiums (traditionnel, "pan around the neck" un seul bidon par musicien, ou conventionnel, chaque section chromatique donc plusieurs bidons par musiciens). Dans les orchestres conventionnels, les pans aigus, appelés , comportent une trentaine de notes sur un ou deux bidons, les médiums comportent vingt à trente notes sur deux à quatre bidons, les basses comportent une vingtaine de notes sur quatre à douze bidons. Les pans médiums et basses sont appelés .

Les steeldrums sont construits en utilisant de la tôle d'une épaisseur comprise entre et . Traditionnellement, des steelpans ont été construits avec des tonneaux à huile, des boites de biscuits ou des poubelles usagées.
De nos jours, certains fabricants n'utilisent plus de bidons mais du métal sous forme de tôle plate qu'ils dessinent en cuvette. 
Dans une première étape, le fond du bidon est enfoncé en cuvette. Ce processus est habituellement fait avec plusieurs marteaux, manuellement ou sous la pression de l'air. Le modèle de note est alors marqué sur la surface, et les notes de différentes tailles sont formées et moulées dans la surface. Après le gâchage, les notes doivent être ramollies et accordées (accord initial). Le ramollissement fait partie de ce premier processus d'accord.

Plus la taille de la note est grande, plus la tonalité est grave. La (la pièce cylindrique du bidon) voit varier sa taille selon la tessiture : plus les notes sont graves, plus on a besoin d'une grande pour faire résonner les fréquences graves.
Ainsi le tenor pan, parfois appelé , très aigu avec des petites notes, est constitué d'un seul fût avec une petite jupe (entre 20 et 30cm), alors que le joueur de basse est entouré de quatre, six, neuf, voire douze bidons entiers (avec chacun 3 notes)!
Les pans peuvent être chromés ou peints (ou passées au bichromate de potassium).

Plus on joue, plus les pans se désaccordent, les "steelbands" se chargent de faire accorder régulièrement leurs instruments (en général une fois ou deux par an). Un tuner (accordeur) doit pouvoir parvenir à faire sonner de manière homogène toutes les notes d'un même instrument. Tout le travail d'accordage est effectué en utilisant des marteaux de différentes tailles.

Le musicien joue du pan en frappant ces facettes avec des petites mailloches ou "sticks".

Plusieurs pans sont généralement utilisés simultanément dans un orchestre appelé "steelband" (composé en outre d'une importante section rythmique : un batteur et d'autres percussionnistes), et l'accord des instruments permet d'obtenir une mélodie et un arrangement de type symphonique. En effet, tout comme le vibraphone ou le xylophone, le pan peut être utilisé pour jouer toutes les parties d'une œuvre musicale ou d'un morceau de musique.

Un musicien de pan est appelé un "paniste", et le lieu de répétition du steelband s'appelle un "panyard". Un fabricant s'appelle un "pan maker" et un accordeur s'appelle un "pan tuner".


William R. Aho, 1987, "Steel Band Music in Trinidad and Tobago: The Creation of a People's Music" in Latin American Music Review 8 (1): 26-56.

Shannon K. Dudley :

Aurélie Helmlinger : 

Ulf Kronman

Stephen Stuempfle, 1995, "The steelband movement. The forging of a national art in Trinidad and Tobago" University of Pennsylvania Press 287 p.

Daniel Verba


</doc>
<doc id="17944" url="https://fr.wikipedia.org/wiki?curid=17944" title="Région administrative spéciale">
Région administrative spéciale

Une région administrative spéciale, abrégé RAS, est une dénomination utilisée pour désigner une subdivision politique de la République populaire de Chine (RPC) ou de la République populaire démocratique de Corée (RPDC, ou Corée du Nord) ayant un statut administratif spécial, notamment dans le domaine économique. Hong Kong et Macao sont les seules RAS chinoises actuelles.

Actuellement, la Chine inclut deux RAS ("tèbié xíngzhèngqū"; 特別行政区) 

Le statut de RAS a été proposé aussi pour Taïwan pour tenter de régler le conflit qui oppose la Chine populaire à la République de Chine, qu’elle considère comme sa vingt-troisième province, mais le gouvernement de Taïwan l’a refusé. Les gouvernements des deux républiques reconnaissent et revendiquent chacun l’unité de la Chine, mais ne se reconnaissent pas mutuellement l’autorité politique sur celle-ci.

Les principaux points de désaccord tiennent dans la compétence des gouvernements en matière de défense et de sécurité (intérieure et extérieure), de délimitation territoriale entre les provinces chinoises sous leur autorité, de gestion des ressources pour les zones économiques maritimes, de citoyenneté et de compétence judiciaire, de politique fiscale, ou encore d’autonomie financière et de personnalité juridique des collectivités territoriales (des domaines d’autonomie qui sont exclus du champ d’action des RAS). Au niveau international, seule la République populaire jouit aujourd’hui de la reconnaissance de sa compétence en matière de traités internationaux légalement contraignants dans ces domaines.

Cependant, le gouvernement de Taïwan dispose d’une compétence limitée, reconnue "de facto" sur les territoires qu’il administre directement, et qui lui permet de participer de façon souvent volontaire et non contraignante à des programmes interrégionaux de collaboration, voire d’être partie dans certains accords internationaux de nature économique ou environnementaux, avec une compétence plus étendue que ce que permet actuellement le statut de RAS au sein de la Chine populaire, la République populaire pouvant toutefois mettre un veto à la signature de tels accords.




</doc>
<doc id="17951" url="https://fr.wikipedia.org/wiki?curid=17951" title="Loubens (Gironde)">
Loubens (Gironde)

Loubens ("" en gascon) est une commune du sud-ouest de la France, située dans le département de la Gironde (région Nouvelle-Aquitaine).

Ses habitants sont appelés les "Loubatons".

Bordée au nord par le Dropt, la commune se trouve dans l'Entre-deux-Mers, à à l'est-sud-est de Bordeaux, chef-lieu du département, à au nord-est de Langon, chef-lieu d'arrondissement et à au nord de La Réole, chef-lieu de canton.

Les communes limitrophes en sont Landerrouet-sur-Ségur au nord sur moins d', Mesterrieux au nord-est, Roquebrune à l'est, Saint-Hilaire-de-la-Noaille au sud-est, Saint-Sève au sud, Bagas à l'ouest et Saint-Martin-de-Lerm au nord-ouest.

La commune est principalement traversée, dans le bourg, par la route départementale D126 qui relie Bagas à l'ouest à Roquebrune à l'est et, dans l'ouest du territoire communal, par la route départementale D21 qui mène vers le sud à Saint-Sève et au-delà à La Réole et vers le nord à Landerrouet-sur-Ségur et au-delà en direction de Castelmoron-d'Albret.
L'accès le plus proche à l'autoroute A62 (Bordeaux-Toulouse) est celui de distant de par la route vers le sud.<br>
L'accès à l'autoroute A65 (Langon-Pau) se situe à vers le sud-sud-ouest.<br>
L'accès le plus proche à l'autoroute A89 (Bordeaux-Lyon) est celui de l'échangeur autoroutier avec la route nationale 89 qui se situe à vers le nord-ouest.

La gare SNCF la plus proche est celle, distante de par la route vers le sud, de La Réole sur la ligne Bordeaux-Sète du TER Aquitaine.

À la Révolution, la paroisse Saint-Vincent de Loubens forme la commune de Loubens.

Le janvier 2014, la communauté de communes du Réolais ayant été supprimée, la commune de Loubens s'est retrouvée intégrée à la communauté de communes du Réolais en Sud Gironde siégeant à La Réole.





</doc>
<doc id="17954" url="https://fr.wikipedia.org/wiki?curid=17954" title="Route de la soie">
Route de la soie

La route de la soie désigne un réseau ancien de routes commerciales entre l'Asie et l'Europe, reliant la ville de Chang'an (actuelle Xi'an) en Chine à la ville d'Antioche, en Syrie médiévale (aujourd'hui en Turquie). Elle tire son nom de la plus précieuse marchandise qui y transitait : la soie.

La route de la soie était un faisceau de pistes par lesquelles transitaient de nombreuses marchandises, et qui monopolisa les échanges Est-Ouest pendant des siècles. Les plus anciennes traces connues de la route de la soie, comme voie de communication avec les populations de l'Ouest, remontent à « 2000 avant notre ère au moins ». Les Chinois en fixent l'ouverture au voyage de Zhang Qian en 138-126. Mais elle s'est développée surtout sous la dynastie Han (221 av. J.-C. - 220 ap. J.-C.), en particulier Han Wudi. Puis sous la dynastie Tang (618-907). À partir du , la route de la soie est progressivement abandonnée, l'instabilité des guerres turco-byzantines, puis la chute de Constantinople poussent en effet les Occidentaux à chercher une nouvelle route maritime vers les Indes. L'abandon de la route de la soie correspond ainsi au début de la période des « Grandes découvertes » durant laquelle les techniques de transport maritime deviennent de plus en plus performantes. Du côté chinois, les empereurs Ming Yongle, puis Ming Xuanzong chargent, à la même époque, l'amiral Zheng He d'expéditions maritimes similaires.

La route doit son nom à la marchandise la plus chère qui y transitait : la soie, dont les Chinois furent pendant longtemps les seuls à détenir le secret de fabrication. Cette dénomination, forgée au , est due au géographe allemand Ferdinand von Richthofen.

Le préhistorien André Leroi-Gourhan considère cette route comme un espace d'échanges actifs dès le paléolithique. Héritière de la Route de jade dont les vestiges datent de , elle n'est cependant évoquée dans les chroniques chinoises qu'à partir du . Cette route est aussi mentionnée par Michèle Pirazzoli-t'Serstevens « depuis 2000 avant notre ère au moins » comme voie de passage à des populations d'agro-pasteurs des steppes eurasiatiques, surtout éleveurs de chevaux. Et des études publiées en 2012 et 2014 sur la culture de Qijia (une culture du néolithique final du Nord-ouest) ont démontré que ces échanges portaient aussi sur l'apport de la technologie du bronze en Chine.

Cet itinéraire serait le résultat de la curiosité de l'Empereur de Chine Wudi (141-87 av. J.-C.) de la dynastie Han pour les peuples civilisés lointains que l'on disait habiter les contrées occidentales, au-delà des tribus barbares.

Les Grecs, puis les Romains, commencent à parler du à partir du pour désigner la Chine. Vers le début de l'ère chrétienne, les Romains deviennent de grands amateurs de soie après en avoir acquis auprès des Parthes qui sont alors les organisateurs de ce commerce.

De nombreux autres produits voyagent sur ces mêmes routes : musc, pierres précieuses, porcelaine, étoffes de laine ou de lin, jade, ambre, ivoire, laque, épices, verre (en particulier : les perles de verre avant qu'elles ne soient produites en Chine), corail, métaux précieux et armes, etc.

Les Sogdiens, un peuple indo-européen d'origine scythe établi en Sogdiane, dans l'ouest de l'actuel Ouzbékistan et les pays limitrophes, ont assuré depuis l'antiquité et surtout entre le l'essentiel du commerce des voies d'Asie centrale entre la Chine, la Perse, l'Occident et l'Inde, et en particulier la Route de la Soie. Polyglottes, ils ont fourni bon nombre d'espions, de traducteurs ou d'agents diplomatiques à qui voulait bien les employer. Maîtres de la Transoxiane (région située entre les fleuves Oxus et Syr-Daria), ils prélevaient de nombreuses taxes qui enrichirent leurs prestigieuses cités de Samarcande et Boukhara. La majorité des caravansérails d'Asie centrale étaient des établissements sogdiens. Ils contribuèrent également à la diffusion de religions en Chine comme le nestorianisme, le manichéisme et le bouddhisme. La soie, pour les producteurs chinois, était, non un objet de profit, mais, tout simplement, une monnaie qui servait à payer les fonctionnaires et à gratifier les souverains étrangers, dont les menaçants nomades. Ce furent les marchands sogdiens qui la captèrent en route et en firent un objet économique. Même de leur point de vue, il ne semble pas qu'elle ait toujours été perçue comme formant l'essentiel de leur activité. Les marchandises qu'ils transportaient, de toute façon en très faibles quantités, étaient plutôt le musc et le santal.

Xi'an est, du côté chinois, l'extrémité est de la route de la soie. Le parcours a été considéré comme officiellement « ouvert » par le général chinois Zhang Qian au . Les empereurs Han assiégés par des barbares nomades (les Xiongnu) décident d'ouvrir au commerce et au monde extérieur la soie, alors monopole d'État : Ils ont en effet besoin d'alliés et de chevaux.

À l'apogée de la Dynastie Tang (618-907), la ville capitale de Chang-An (Xi'an) abrite deux millions d'habitants, soit dix fois plus que Constantinople ou Cordoue, mille fois plus qu'Aix-la-Chapelle au temps de Charlemagne.

Les convois de caravanes partent de Xi'an, Lanzhou ou Xining et empruntent le corridor du Gansu. Ils contournent ensuite le désert du Taklamakan, l'un des plus arides du monde, soit par la voie du nord ou par celle du sud. Ces deux itinéraires possibles possèdent chacun différentes variantes, et sont jalonnés de villes et caravansérails, dont les noms et l'importance varient au fil des temps. Toutes les pistes progressent le long d'un chapelet d'oasis-forteresses situées à la périphérie du désert et au pied des hautes montagnes des Tian Shan ou des Kunlun :
À partir de Kachgar et Yarkand, les pistes rejoignent la Perse ou l'Inde à travers les hautes montagnes de l'Asie centrale (Pamir, Hindū-Kūsh et Karakoram), puis par la Sogdiane (Samarcande, Boukhara, Merv), la Bactriane (Balkh) ou le Cachemire (Srinagar). En réalité, très rares sont ceux qui ont eu l'occasion de parcourir l'intégralité du trajet: Marco Polo, son père et son oncle furent de ceux-ci.

Les marchandises venues d'Orient ou d'Occident s'échangent dans les oasis, devenues d'importants comptoirs fréquentés non seulement par les commerçants mais aussi par les pèlerins, les soldats et les espions. À son apogée, la Route de la soie relie – côté ouest – l'Empire byzantin et – côté est – une vaste région qui va des Trois royaumes jusqu'aux territoires de la dynastie Yuan en zone chinoise.

La longueur du parcours, les nombreux intermédiaires, les multiples dangers encourus par les voyageurs sur ces pistes soumises aux incursions de peuples belliqueux et aux attaques des brigands (surtout après la fin de la Pax Mongolica et la dislocation de l'empire mongol au et l'ouverture par les Européens de la route maritime des Épices), vont finir par contribuer au déclin de l'itinéraire terrestre de la « route de la soie ». Ainsi par exemple, la région du « Turkestan chinois » est sous la souveraineté théorique de l'empereur de Chine, mais cette domination subit en réalité de fréquentes éclipses, dues à son grand éloignement et à la difficulté d'y maintenir des garnisons suffisantes.

L'extrême rigueur du climat (torride en été et glacial en hiver), complique l'acheminement qui progresse cahin-caha pendant parfois plus d'un an, à dos de yacks ou en caravanes de cinquante à mille chameaux.

Au total, l'ensemble de ces facteurs renchérissent le prix des produits qui transitent entre l'Extrême-Orient et le bassin méditerranéen. Ces raisons incitent les Européens à rechercher et à pratiquer une route maritime (aussi appelée routes des épices ou ) pour commercer avec les pays d'Orient.

Par ailleurs, les soies chinoises intéressent moins les Européens car la fabrication de la soie se développe en Europe même.

Au , la Route de la soie est progressivement abandonnée.


Par une culture internationale, elle a permis des échanges matériels, culturels, religieux et scientifiques entre peuples aussi divers et mutuellement lointains que les Turcs, les Tokhariens, les Sogdiens, les Perses, les Byzantins et les Chinois.
Dans les régions qu'elle traverse, les richesses qu'elle génère représente une force d’attraction et ouvre des horizons pour des tribus qui vivent jusque-là de façon isolée. Ces peuples sont attirés par les richesses et les opportunités qui se présentent à eux et deviennent maraudeurs ou mercenaires. Beaucoup de leurs membres deviennent ainsi des guerriers redoutables, capables de conquérir des cités riches, des terres fertiles et de forger des empires.

Elle évoque pour certains un processus assimilable à la mondialisation. Elle est à l'évidence un sujet intéressant pour ceux qui veulent observer un phénomène précoce d'intégration politique et culturelle, causé par le commerce international.

Elle a :

L'unité politique de cette région ne survit pas à la chute de l'Empire mongol, la culture et l'économie de la région en souffrent également. Les seigneurs turcs extorquent à l'Empire byzantin décadent l'extrémité ouest de la route et posent les fondations du futur Empire ottoman. De même, à la suite de l'islamisation de la région, les Chinois deviendront durablement méfiants vis-à-vis de cette voie commerciale, dès la fin de la dynastie Tang, et préféreront la fermeture puis le commerce maritime, qui commencera alors à prendre son essor.

Ce projet serait notamment constitué du tronçon d'autoroute de entre Kashgar et Erkeshtam est entré en service en septembre 2013.

C'est un chantier titanesque. L'Europe, la Chine et les pays d'Asie centrale sont engagés dans la construction d'un nouvel axe commercial majeur. La nouvelle route de la soie pourrait redistribuer les cartes, à l'heure où la mondialisation de l'économie fait pencher la balance vers l'Est. Plusieurs tronçons ont déjà été transformés en autoroute.

La partie chinoise de cette route sera constituée des passages par Lianyungang, dans la Province du Jiangsu, et Xi'an, dans la Province du Shaanxi, et par la région autonome ouïghour du Xinjiang.

Cette route pourrait alors rejoindre l'Europe en passant par le Kirghizistan, l'Ouzbékistan, le Tadjikistan, le Turkménistan, l'Iran et la Turquie. Côté chinois, on achève le Xinsilu, une quatre-voies de qui relie la mer Jaune aux monts Tian. Un axe qui a pour but de délester la route maritime, par laquelle transitent des millions de conteneurs par an.

Deux autres routes sont envisagées pour rejoindre l'Europe : une passant par le Kazakhstan et la Russie, et l'autre traversant le Kazakhstan via la mer Caspienne. Les travaux ne sont pas financés par l'Union européenne, qui n'apporte aucune aide logistique. Les bailleurs sont la Banque européenne de développement, la Banque asiatique de développement, la Banque de développement islamique.

Cette route permettra notamment de faciliter le commerce entre la Chine populaire et les pays d'Asie centrale, dont les échanges s'élevaient à de dollars américains en 2008.
Une liaison ferroviaire allant de la région autonome ouïghour à l'Iran et desservant le Tadjikistan, le Kirghizistan et l'Afghanistan est également envisagée.

La route du sud, via la Turquie et l'Iran, est pour l'instant délaissée en raison des sanctions de l'ONU imposées à l'Iran. Ce pays est par ailleurs en conflit avec ses voisins sur le partage des eaux de la mer Caspienne.

Un nouveau terminal pour ferries, tankers et cargos est en chantier à Alat, le nouveau terminal portuaire de Bakou. Cette gigantesque plate-forme, dotée des meilleurs équipements, assurera toutes les jonctions possibles entre bateaux et trains, wagons-citernes et pipelines, conteneurs et camions. « Alat pourra traiter jusqu'à 25 millions de tonnes de fret par an contre 7 millions en 2012, depuis le vieux port de Bakou », confirme Mousa Panahov, le vice-ministre des Transports de Chine.

En 2012, il faut compter au minimum un mois pour acheminer des marchandises depuis Shanghai jusqu'à Rotterdam par la mer, via le canal de Suez; moins de trois semaines en train, et environ quinze jours en camion. Les experts estiment que ces deux derniers voyages terrestres pourraient être réduits de moitié en améliorant les infrastructures et en harmonisant les législations. Un programme dont l'Azerbaïdjan se verrait bien le champion. Le budget consacré par Bakou aux transports (trois milliards d'euros en 2010) est d'ailleurs le premier poste de dépense de l'État azéri. Élément essentiel de cette stratégie, la voie ferrée Bakou-Tbilissi-Kars a été rénovée, des rails et traverses aux locomotives et à l'alimentation électrique. Fort de ses pétrodollars, l'Azerbaïdjan a prêté 400 millions d'euros à son voisin géorgien pour ce chantier.

La route de la soie du troisième millénaire, de même que son modèle original, ne suit pas un trajet unique. Depuis la rive orientale de la mer Caspienne, trois itinéraires différents permettront de rejoindre les contreforts des monts Tian, puis d'entrer en Chine par une dizaine de points de passage.

Plus que la qualité du bitume ou de l'écartement des rails des voies ferrées, c'est le temps et l'argent perdus aux frontières (environ 40 % de la durée et du coût du voyage) et le cout des transports terrestres qui sont les principaux obstacles à la reconstruction de la route de la soie, qui ne peut être une voie économiquement adaptée aux transport des énormes tonnages de marchandise de masse caractérisant la mondialisation moderne qui n'a finalement rien de commun avec le commerce de la route de la soie historique qui n'étaient qu'une voie de commerce et de négoce fondée sur des produits rares et précieux.

Le 14 mai 2017, la Chine ouvre un sommet consacré au projet de nouvelle route de la soie.








</doc>
<doc id="17955" url="https://fr.wikipedia.org/wiki?curid=17955" title="Cédric Klapisch">
Cédric Klapisch

Cédric Klapisch, né le à Neuilly-sur-Seine (France), est un réalisateur, scénariste et producteur de cinéma français.

Il a notamment écrit et réalisé "L'Auberge espagnole", "Les Poupées russes", "Paris" et aussi adapté la pièce de théâtre "Un air de famille" de Jean-Pierre Bacri et Agnès Jaoui plusieurs fois récompensée aux Césars. On lui doit en outre "Le Péril jeune", film aujourd’hui devenu culte et la découverte de Romain Duris, acteur désormais incontournable de la scène cinématographique française.

Né en 1961 à Neuilly-sur-Seine, il est le fils du physicien Robert Klapisch et de Françoise Meyer, psychanalyste. Il a une sœur aînée, Coline. Ses grands-parents maternels sont Robert et Raymonde Meyer, couple de résistants juifs, déportés à Auschwitz via Drancy.

Cédric Klapisch vit à Paris et suit des études au lycée Rodin. Après son bac, il fait deux années de préparation littéraire (hypokhâgne et khâgne) option philosophie au Lycée Lakanal. Il tente le concours d'entrée de l'IDHEC où il n'est pas reçu. Il s'inscrit alors à l'université Paris-III en études de cinéma et l'année suivante à l'université Paris-VIII, où il obtient une Maîtrise de cinéma. Son mémoire, qui porte sur Tex Avery, Woody Allen et les Marx Brothers, s'intitule "Le non-sens au cinéma, du ".

À 23 ans, après un deuxième échec au concours d'entrée de l'IDHEC où on lui reproche son goût trop peu prononcé pour le cinéma français de l'époque, il part aux États-Unis où il étudie le cinéma à l'Université de New York (NYU) pendant deux ans. En 1984 il tourne son premier court métrage, "Glamour toujours". La même année, il réalise "Un, deux, trois mambo", puis "Jack le menteur", et l'année suivante "In transit", dans lequel apparaît le cinéaste Todd Solondz, lui aussi étudiant à NYU.

À son retour en France, Cédric Klapisch travaille tout d'abord comme électricien sur quelques longs métrages, dont "Mauvais sang" de Leos Carax, avant de réaliser en 1989 un nouveau court métrage, "Ce qui me meut", produit par Adeline Lecailler pour la société Lazennec. Ce court métrage fait beaucoup parler de lui, reçoit plusieurs prix dans différents festivals et son titre deviendra plus tard le nom de la maison de production qu'il crée (et qu'il dirige aujourd’hui en collaboration avec Bruno Lévy).

Il travaille ensuite comme scénariste ou réalisateur pour des films d'entreprise ou des documentaires pour la télévision ; il réalise notamment "Masaiitis", un documentaire de 52 minutes pour Canal+ sur les Maasaï du Kenya.

En 1992, Cédric Klapisch passe au long métrage et réalise avec le soutien de Lazennec, "Riens du tout", une comédie sociale sur les déboires d'un patron d'entreprise aux prises avec son personnel, qui réalise entrées en France. Lazennec vient alors de produire "Un monde sans pitié" d'Éric Rochant et "La Discrète" de Christian Vincent, et s'apprête à produire "La Haine" de Mathieu Kassovitz. Le film choral "Riens du tout" s’inscrit donc dans l’arrivée d’une nouvelle génération de cinéastes qui rompt avec le cinéma de la Nouvelle Vague et celui des années 1980. Pour ce premier long, Cédric Klapisch réunit une trentaine d'acteurs différents dont Karin Viard, Antoine Chappey, Odette Laure, Zinedine Soualem et surtout Fabrice Luchini.

L'année suivante, Cédric Klapisch accepte de faire un téléfilm pour Arte qui s’inscrit dans une collection lancée par Pierre Chevalier : « Les Années lycée ». "Le Péril jeune", produit par Vertigo et diffusé à la télévision en 1994, gagne le Fipa d’or ainsi que le prix de l’humour au festival de Chamrousse. Grâce à Pierre-Ange Le Pogam, aujourd’hui associé de Luc Besson dans la société EuropaCorp, alors en poste chez Gaumont, "Le Péril jeune" bénéficie d'une sortie en salle en 1995. Ce film, à petit budget, qu’il a coécrit avec deux amis de lycée, Santiago Amigorena et Alexis Galmot, rencontre alors un certain succès, tant critique que public ( entrées en France). À l'affiche, on trouve alors le jeune Romain Duris ainsi que Vincent Elbaz, qui tourneront plusieurs fois par la suite avec Cédric Klapisch. "Le Péril jeune" est aujourd'hui largement considéré comme l'un des films les plus importants et les plus marquants de la carrière du cinéaste et se range désormais dans la catégorie de films "cultes" de toute une génération.

Après avoir assisté à une représentation de leur pièce de théâtre intitulée "Un air de famille", Cédric Klapisch rencontre Jean-Pierre Bacri et Agnès Jaoui qui lui proposent de mettre en scène l’adaptation cinématographique. En parallèle de la préparation d"'Un air de famille", il décide de transformer un projet de court en long métrage avec Vertigo Productions et réalise en 1996 "Chacun cherche son chat" ( entrées en France). Prix de la Critique Internationale au Festival de Berlin, le film reçoit un excellent accueil lors de sa sortie américaine. On retrouve dans cette comédie parisienne Romain Duris, Zinedine Soualem et Simon Abkarian, ainsi que de nombreux habitants du quartier de la Bastille, dans leur propre rôle. La même année sort "Un air de famille" qui connaît un grand succès ( entrées en France) et reçoit le César du meilleur scénario original, mais aussi deux Césars pour les acteurs Catherine Frot et Jean-Pierre Darroussin.

Si ses quatre premiers films ont permis à Cédric Klapisch de se faire connaître dans le monde du cinéma français, il met tout de même plusieurs années à mener à bien le film "Peut-être", projet dont il a déjà entamé l'écriture quelques années plus tôt. Le scénario et le coût du film font peur aux producteurs et c'est finalement la Warner Bros., associée à Vertigo, qui finance ce film « d'anticipation » sur la paternité, situé dans un Paris futuriste et recouvert de sable. "Peut-être" réunit dans les rôles principaux Jean-Paul Belmondo, Romain Duris et Géraldine Pailhas, mais aussi Vincent Elbaz, Julie Depardieu, Zinedine Soualem, Léa Drucker ou encore Olivier Gourmet. Cette belle distribution rencontre son public lors de sa sortie en salle en 1999 entrées en France) mais reçoit un accueil critique mitigé.

Klapisch écrit ensuite un film policier dont la préparation est retardée de quatre mois et décide alors de réaliser très vite "L'Auberge espagnole", comédie sur les déboires d'un jeune Français qui part finir ses études à Barcelone. Tourné très rapidement, en HD, ce film passe la barre des d'entrées, devenant le plus grand succès du réalisateur. Il fait le tour du monde et inscrit définitivement Cédric Klapisch dans la lignée des réalisateurs à succès du cinéma français contemporain. 

En janvier 2003 sort "Ni pour ni contre (bien au contraire)" ( entrées en France), un polar avec Marie Gillain et Vincent Elbaz (qui tourne pour la troisième fois avec Cédric Klapisch après "Le Péril jeune" et "Peut-être"). On y retrouve aussi Pierre-Ange Le Pogam, ainsi que Diane Kruger dans un de ses premiers rôles.

Le 15 juin 2005, sort "Les Poupées russes", pour lequel Cédric Klapisch retrouve cinq ans plus tard les personnages de "L'Auberge espagnole" et notamment Xavier, le personnage principal interprété par Romain Duris. Le film est un nouveau succès avec 3 millions d’entrées et vaut à Cécile de France son deuxième César. 

En 2008 sort "Paris", un film choral sur la capitale et ses habitants, avec dans la distribution Juliette Binoche, Romain Duris, Karin Viard, François Cluzet, Mélanie Laurent, Albert Dupontel ou encore Fabrice Luchini. Ce film, dans la lignée de "Short Cuts" de Robert Altman ou "Magnolia" de Paul Thomas Anderson, est par ailleurs par "Chacun cherche son chat". C’est à nouveau un succès pour Klapisch avec 2 millions d'entrées en France.

En 2010 est diffusé à la télévision "L'espace d'un instant", un documentaire réalisé par Cédric Klapisch sur la danseuse étoile Aurélie Dupont. La même année, il entame le tournage d’une nouvelle comédie sociale, "Ma part du gâteau", avec au générique Karin Viard et Gilles Lellouche.

En 2013, à propos de l'affaire de la liste de Falciani, il est annoncé dans la presse que Cédric Klapisch possédait un compte en Suisse, d'un montant de euros, non déclaré en France, ce qui était alors illégal.

Il revient dans les salles de cinéma le 4 décembre 2013 pour conclure la trilogie des aventures de Xavier Rousseau, cette fois entre Paris et New York. "Casse-tête chinois", marque le retour d'une partie du casting : Romain Duris, ainsi que les femmes de la vie de son personnage, interprétées par Kelly Reilly, Audrey Tautou et Cécile de France. Le film fait entrées en France et se vendra dans plus de 50 pays.

En 2014, il est membre du comité de soutien à la candidature d'Anne Hidalgo à la mairie de Paris.

Le réalisateur revient au documentaire très peu de temps après : en février 2014, "Élévation", documentaire pour Canal+ sur le sauteur à la perche Renaud Lavillenie. Il filme le en direct à Donetsk le nouveau recordman du monde avec un saut à . Et en avril, la chaîne payante diffuse "Mon livre d'histoire", qui lui permet de raconter le parcours de ses grands-parents pendant la Seconde Guerre mondiale.

Et en septembre, une exposition de photos à la Galerie Paris Cinéma, intitulée "Paris-New-York", retrace son parcours d'artiste, entre ces deux mégalopoles.

Fin 2015, il revient à la télévision, en collaboration avec Dominique Besnehard, pour sa première série télévisée, "Dix pour cent". Le programme, diffusé sur France 2, et dont il met en scène deux des six épisodes de la première saison, connaît un large succès critique et d'audiences.

En novembre de cette même année, Cédric Klapisch initie aux côtés d'Alain Rocca, président d'Universciné, Pascale Ferran et Laurent Cantet, LaCinetek, une plateforme de VOD dédiée au cinéma de patrimoine.

Il partage sa vie avec Lola Doillon, fille du cinéaste Jacques Doillon et de la monteuse Noëlle Boisson. Ils se sont mariés le 26 juillet 2014 et ont deux enfants, Thaïs née en 2000 et Emile, né en 2007.



Cédric Klapisch fait un caméo, ou petit rôle de figuration, dans ses films :

Les données ci-dessous proviennent de l'European Audiovisual Observatory et de JPbox-office.com.




</doc>
<doc id="17957" url="https://fr.wikipedia.org/wiki?curid=17957" title="Catherine Frot">
Catherine Frot

Catherine Frot est une actrice française, née le à Paris. 

Elle est la sœur de la comédienne Dominique Frot.

Depuis 1986, elle a été nommée dix fois aux César du cinéma, obtenant celui de la meilleure actrice dans un second rôle en 1997 pour "Un air de famille" et celui de la meilleure actrice en 2016 pour "Marguerite" de Xavier Giannoli. Elle est également lauréate de deux Molières : meilleure comédienne dans un second rôle en 1995 pour la pièce "Un air de famille" et meilleure comédienne dans un spectacle de théâtre privé en 2016 pour "Fleur de cactus".

Sa famille est originaire de Rochefort en Charente-Maritime. Fille d'un père ingénieur et d'une mère professeur de mathématiques financières, Catherine Frot passe son enfance dans différentes villes de province.

De vocation précoce, avec une prédilection sensible pour le comique, elle suit dès l'âge de quatorze ans les cours du conservatoire de Versailles, tout en poursuivant sa scolarité. Elle entre en 1974 à l'école de la rue Blanche puis au Conservatoire. À la même époque, elle fait partie des fondateurs de la Compagnie du Chapeau Rouge, fort remarquée au festival d'Avignon off en 1975. C'est initialement au théâtre qu'elle va se consacrer le plus souvent, avec en point d'orgue le rôle de la Présidente de Tourvel dans une adaptation des "Liaisons dangereuses" mise en scène par Gérard Vergez (1987). Sur les planches, elle joue aussi de nombreux classiques : "La Cerisaie" et "La Mouette" de Tchekhov, respectivement mis en scène par Peter Brook en 1982 et Pierre Pradinas en 1985, ou encore "John Gabriel Borkman" d'Ibsen, dirigé par Luc Bondy en 1993, et des créations comme "C'était comment déjà ?" de Jean Bouchaud, qui lui vaut le Prix de la révélation théâtrale de l'année du Syndicat de la critique en 1980.

Nommée en 1986 pour le César de la meilleure actrice dans un second rôle pour son interprétation de Béatrice dans "Escalier C", il lui a fallu attendre le rôle de Yolande dans le film de Cédric Klapisch, "Un air de famille" pour se faire connaître du grand public. Elle avait précédemment déjà tenu ce rôle dans la pièce dont le film est adapté et avait remporté le Molière de la comédienne dans un second rôle en 1995. Pour ce même rôle au cinéma, elle reçoit ensuite le César de la meilleure actrice dans un second rôle en 1997. Elle obtient son premier grand rôle dans "La Dilettante" de Pascal Thomas en 1999, qui la confirme comme une actrice pouvant jouer simultanément dans les tons burlesque et tragique. Elle alterne depuis de nombreux premiers rôles dans ces deux registres.

En juillet 2008, elle participe aux fêtes locales de l'île-d'Aix pour y lire des lettres d'amour écrites par Napoléon Bonaparte. 

En 2016, après six précédentes nominations, elle remporte le César de la meilleure actrice pour son rôle de cantatrice qui chante faux dans "Marguerite" de Xavier Giannoli. La même année, elle reçoit le Molière de la comédienne dans un spectacle de théâtre privé pour la pièce "Fleur de cactus". 

Catherine Frot a été, en début de carrière, la compagne de Pierre Pradinas. Elle a épousé Michel Couvelard en 1987 ; ils ont adopté tous deux une petite fille : Suzanne.




Catherine Frot est :




</doc>
<doc id="17958" url="https://fr.wikipedia.org/wiki?curid=17958" title="Jean-Pierre Bacri">
Jean-Pierre Bacri

Jean-Pierre Bacri est un acteur, scénariste et dramaturge français, né le à Castiglione (aujourd'hui Bou Ismaïl), en Algérie.

Il est connu pour son association avec Agnès Jaoui, avec laquelle il a joué et coécrit plusieurs pièces de théâtre et films.

Au théâtre, il reçoit le Molière de l'auteur en 1992 pour "Cuisine et Dépendances" et le Molière du comédien dans un spectacle de théâtre privé en 2017 pour son rôle dans "Les Femmes savantes". Au cinéma, il a reçu quatre fois le César du meilleur scénario original, a remporté une fois celui du meilleur acteur dans un second rôle et a été nommé six fois pour le César du meilleur acteur.

Jean-Pierre Bacri est issu d'une famille juive d'Algérie. Grâce à son père facteur en semaine et ouvreur le week-end dans le cinéma "Star de la ville", il découvre le septième art. Arrivé avec ses parents à Cannes en 1962, au lycée Carnot, où il croise Cyril de La Patellière, il veut devenir professeur de français et de latin. Mais à 25 ans, Jean-Pierre Bacri monte à Paris et décide de travailler dans la publicité. Il devient placeur à l'Olympia pour gagner sa vie. Parallèlement, il suit une formation d'acteur au Cours Simon et au Cours de Jean Périmony, tout en écrivant des pièces de théâtre. "Le Doux Visage de l'amour" reçoit d'ailleurs le prix de la Vocation en 1979. La même année, il joue son premier rôle au cinéma dans "Le Toubib", mais c'est sa prestation de proxénète dans "Le Grand Pardon" en 1981 qui le fait connaître aux yeux du grand public. Il multiplie ensuite les seconds rôles, dont un inspecteur ahuri dans Subway de Luc Besson en 1985 avant de se retrouver en tête d'affiche de "Mort un dimanche de pluie" en 1986, mais surtout "L'Été en pente douce" en 1987. Il joue la même année dans la pièce "L'Anniversaire" de Harold Pinter, dans la mise en scène de Jean-Michel Ribes, auprès de sa future collègue et compagne Agnès Jaoui.

Au-delà des rôles de personnages râleurs, bougons ou un peu beaufs qui ont fait sa popularité, il est reconnu pour son travail d'écriture scénaristique avec Agnès Jaoui. Jean-Pierre Bacri et Agnès Jaoui se sont rencontrés en 1987 et ont formé un couple jusqu'en 2012. Leur collaboration a été récompensée par le César du Meilleur scénario pour "Smoking / No Smoking", "Un air de famille", "On connaît la chanson" et "Le Goût des autres". Hormis "Smoking / No Smoking", Jean-Pierre Bacri a interprété des personnages dans tous les autres films et a même obtenu le César du meilleur acteur dans un second rôle dans "On connaît la chanson". Jean-Pierre Bacri joue dans des comédies comme "La Cité de la peur" et "Didier" et prête sa voix, en 2002, pour "".

En 2005, il décide de parrainer le collectif Devoirs de Mémoires. En 2006, il joue dans "Selon Charlie" de Nicole Garcia. Le tandem Bacri-Jaoui revient au cinéma avec "Parlez-moi de la pluie", sorti en 2008 et réalisé par Agnès Jaoui, accompagnés par Jamel Debbouze. Il s'agit du sixième long-métrage écrit et interprété par le duo (après "Cuisine et dépendances", "Un air de famille", "On connaît la chanson", "Le Goût des autres" et "Comme une image", les deux derniers ayant été également réalisés par Agnès Jaoui). Le dernier film en date écrit par le duo (ou « Jabac » selon l'expression d'Alain Resnais) a pour titre "Au bout du conte" et est sorti en 2013. Il y joue un moniteur d'auto-école croyant irrémédiablement que sa dernière heure est arrivée.

En 2011, Jean-Pierre Bacri est à l'affiche du thriller de Raphaël Jacoulot, "Avant l'aube" et y incarne Jacques Couvreur, le propriétaire d'un luxueux hôtel. Il est à l'affiche en septembre 2012 avec "Cherchez Hortense" de Pascal Bonitzer.

Après leur séparation en 2012, Agnès Jaoui et Jean-Pierre Bacri sont demeurés proches et continuent d'écrire et de jouer ensemble au cinéma et au théâtre.

En 2014, il est membre du comité de soutien à la candidature d'Anne Hidalgo à la mairie de Paris.









</doc>
<doc id="17962" url="https://fr.wikipedia.org/wiki?curid=17962" title="Oxydation de Swern">
Oxydation de Swern

L’oxydation de Swern est l'oxydation douce des alcools primaires et secondaires en aldéhyde et en cétone dans un milieu contenant du chlorure d'oxalyle, du diméthylsulfoxyde et de la triéthylamine.

L'avantage principal de cette méthode réside dans la douceur des conditions qui compense l'utilisation de réactifs toxiques (chlorure d'oxalyle, diméthylsulfoxyde, dégagement de monoxyde de carbone). Il est de plus impossible d'observer l'oxydation supplémentaire en acide carboxylique dans le cas de l'oxydation d'un alcool primaire. Cette méthode est très souvent utilisée dans la synthèse totale de produits naturels.

Les sous produits sont le sulfure de diméthyle (MeS), le monoxyde de carbone (CO), le dioxyde de carbone (CO) et – quand la triéthylamine est utilisée comme base – le chlorure de triéthylammonium (EtNHCl). Deux d'entre eux, MeS et CO sont très toxiques et volatils : la réaction doit donc être réalisée sous hotte. MeS est un liquide volatil (T = ) à l'odeur déplaisante : un moyen de limiter cette odeur est de le traiter à l'ozone pour le transformer en diméthylsulfoxyde, inodore.

La première étape de l'oxydation de Swern est la réaction, à basse température, du diméthylsulfoxyde (DMSO) 1a et 1b avec le chlorure d'oxalyle, 2. Le premier intermédiaire, "3", se décompose rapidement, libérant des oxydes de carbone (CO et CO) et produisant du chlorure de diméthylchlorosulfonium, 4.

Après addition de l'alcool 5, le chlorure de diméthylchlorosulfonium 4 réagit avec l'alcool pour donner l'intermédiaire ionique alkoxysulfonium, 6. L'ajout d'au moins deux équivalents de base – typiquement la triéthylamine – va déprotoner l'ion alkoxysulfonium pour donner l'ylure de soufre, 7. Dans un état de transition comportant un cycle à 5 membres, l'ylure de soufre se décompose pour donner le sulfure de diméthyle (MeS) et l'aldéhyde ou la cétone recherchée, 8.

Lorsqu'on utilise le chlorure d'oxalyle comme agent de déshydratation, la réaction doit être maintenue à une température inférieure à pour éviter les réactions secondaires. En utilisant l'anhydride trifluoroacétique à la place du chlorure d'oxalyle, la température peut être augmentée à sans réactions secondaires. On peut également activer le DMSO pour initier la formation de l'intermédiaire 6 avec des carbodiimides (oxydation de Pfitzner-Moffatt) ou un complexe trioxyde de soufre-pyridine (oxydation de Parikh-Doering).

Dans certains cas, l'utilisation de triéthylamine en tant que base peut conduire à l'épimérisation du carbone α du carbonyle nouvellement formé. L'utilisation du diisopropyléthylamine (i-PrNEt, base de Hünig), plus encombrant, peut tempérer cet effet non désiré.

Le sulfure de diméthyle, sous-produit de cette réaction, a une odeur très désagréable. Les glandes olfactives humaines sont d'ailleurs capables de détecter ce composé dans des concentrations de l'ordre de quelques parts par milliard. Une solution simple à ce problème incommodant est de rincer la verrerie à l'eau de Javel, capable d'oxyder le sulfure de diméthyle et ainsi d'éliminer l'odeur.



</doc>
<doc id="17963" url="https://fr.wikipedia.org/wiki?curid=17963" title="Kurt Alder">
Kurt Alder

Kurt Alder, né le à Königshütte, Allemagne et décédé le , est un chimiste organicien allemand. Il est colauréat avec Otto Diels du prix Nobel de chimie de 1950.

Kurt Alder effectue ses études à l'université de Kiel où il est l'élève puis le collaborateur de Otto Diels. Il soutient sa thèse de doctorat en 1926. Il continue son travail à Kiel où il est nommé maître de conférences en 1930 et professeur en 1934.

De 1936 à 1940, il est directeur des recherches de Bayer Werke à Leverkusen, une branche de IG Farben.

Il est ensuite professeur de chimie expérimentale et technique à l'université de Cologne où il est nommé professeur et directeur de l'institut de chimie de l'université à partir de 1940 jusqu'à sa mort.

Sa principale contribution a été l'élaboration, avec Diels, de la réaction qui porte leurs noms. Cette réaction, très générale, conduit à la formation d'un cycle par réaction d'un diène, composé comportant le motif C=C-C=C, avec un diénophile, composé possédant une double liaison C=C convenablement activée. La réaction de Diels-Alder reste une des très grandes méthodes générales de synthèse organique, très souvent utilisée dans la synthèse de produits naturels et des protéines à structure complexe telles la cantharidine, la réserpine ou la morphine.

En 1950, Otto Diels et lui reçoivent conjointement le prix Nobel de chimie .

Kurt Alder est enterré au Deutzer Friedhof de Cologne.



</doc>
<doc id="17964" url="https://fr.wikipedia.org/wiki?curid=17964" title="Manifestations mondiales contre la guerre d'Irak">
Manifestations mondiales contre la guerre d'Irak

Cet article recense quelques grandes manifestations mondiales contre l'intervention de la coalition internationale qui, sous l'égide de l'USCENTCOM, a mené à la guerre d'Irak.

Le la plus importante manifestation mondiale enregistrée à ce jour a lieu.
Plusieurs millions de personnes ont manifesté dans plus de 600 villes autour de la planète, avec par exemple :
Les gouvernements de ces trois pays ont été les plus fervents défenseurs de la guerre d'Irak.

Les 22 et les manifestations mondiales contre la guerre continuent, après le déclenchement de celle-ci.


Il y a eu des témoignages de conflits massifs entre les manifestants et la police l'État du Golfe 'Bahrain' pour la deuxième journée consécutive.

Le , plusieurs dizaines de milliers de manifestants s'étaient rassemblés à Washington D.C. pour protester contre l'engagement américain en Irak. Dans ce contexte difficile (ouragan Katrina) pour George W. Bush, un sondage Gallup montrait que 63 % des Américains voulaient le retour des soldats dans leur pays. D'autres manifestations de moindre ampleur eurent lieu dans d'autres pays.

Le , à l'occasion du quatrième anniversaire de la guerre en Irak, plus de personnes ont manifesté le 17 mars 2007 à Washington DC contre le maintien des troupes américaines ; ils ont défilé à l'appel d'Answer ("Act Now to Stop War and End Racism") de la Maison-Blanche au Pentagone. Des manifestations ont également été organisées dans d'autres pays du monde comme en Espagne par exemple.
Un sondage du 10 juillet 2007 publié par "USA Today", montre que 62 % d'américains pensent que les États-Unis ont commis une erreur en envoyant des troupes en mars 2003.

Pendant l'émission en direct des Oscars du cinéma 2003, plusieurs présentateurs et personnes récompensées ont fait différents commentaires contre la guerre, allant de Susan Sarandon faisant un simple signe de paix, jusqu'à Michael Moore qui a dénoncé publiquement George W. Bush au moment de recevoir son prix.

Le plasticien Phil Hansen a, lui aussi, stigmatisé le conflit au travers d'un portrait-charge de George W. Bush comportant les noms de soldats américains morts en Irak.



</doc>
<doc id="17966" url="https://fr.wikipedia.org/wiki?curid=17966" title="Histoire de la république de Chine">
Histoire de la république de Chine

L’histoire de la république de Chine débute avec la chute de la dynastie Qing et la formation d'une république constitutionnelle en 1912. Dans ses premières décennies, la république de Chine connait une histoire mouvementée, marquée par la perte de territoires situés aux confins de l'ancien empire, les conflits internes, la dominination des seigneurs de guerre, la guerre civile et l'occupation japonaise. Suite à la capitulation japonaise et la fin de la Seconde Guerre mondiale en 1945, la république de Chine devient un membre fondateur de l'Organisation des Nations unies et l'un des cinq membres permanent de son Conseil de sécurité.

Après la reprise de la guerre civile en 1945 et la défaite face à la république populaire de Chine nouvellement proclamée, la république de Chine se replie sur l'île de Taïwan en 1949. Bien qu'ayant perdu le contrôle de la Chine continentale ainsi que son siège au Nations unies (en 1971), la république de Chine revendique toujours la souveraineté sur l'ensemble du territoire chinois.

Une réforme agraire réussie puis un développement économique rapide et soutenu pendant la deuxième moitié du ont transformé Taïwan, l'un des en un pays industrialisé développé jouissant d'un niveau de vie élevé. Le processus de réformes politiques engagé dans les années 1980 transforma le système polique auparavant dominé par un parti unique (le Kuomintang) en une démocratie multipartiste.

Pendant les siècles qui précédèrent 1800, la Chine était une puissance clef, en tête du monde. Mais au , l'Empire chinois se voit dramatiquement dépassé par les puissances occidentales et leur dynamisme capitaliste, colonial et militaire. La Dynastie Qing (1644-1911) n'arrive plus à faire face et ne parvient plus à garantir l'intégrité du territoire de l'Empire.

Les Européens prennent une avance technique considérable grâce à la mécanisation, à l'artillerie, à l'organisation de leurs banques et à l'idéologie capitaliste d'initiative commerciale. L'Europe s'est créée par la force de vastes empires, et les Britanniques entendent exporter vers la Chine leur production indienne d'opium tandis que la Chine stagne dans sa situation féodale pré-industrielle. Les guerres de l'Opium sont des victoires complètes des Occidentaux, même si les nobles mandchous se voilent la face en prétendant avoir « géré » la menace barbare. L'Occident, bientôt rejoint par le Japon modernisé sous l'Ère Meiji (cf. Guerre sino-japonaise), impose ses choix à la Chine, qui doit s'ouvrir davantage à l'extérieur et payer des indemnités de guerre. L'opposition modernisatrice chinoise met ses espoirs dans la Réforme des cent jours de 1898 ; le brillant mais utopiste Kang Youwei la séduit. Mais la réforme est finalement abattue dans le sang par les conservateurs. L'opposition se radicalise alors : elle devient anti-monarchique et prône une « république de Chine » selon le modèle occidental. Les réformes qu'entreprend tardivement et trop lentement Cixi vers 1905 ne sont pas suffisantes et sa mort est l'occasion d'une dernière contraction des conservateurs. En 1911, la révolte du Double 10 sonne le départ de l'insurrection républicaine. La république de Chine est rapidement proclamée à Nankin. Elle contrôle tout le sud de la Chine.

Alors que la Chine est en effervescence politique, tant dans les hautes sphères que dans les regroupements de partisans libéraux et républicains, des groupes armés, secrets, s'organisent un peu partout. À Wuchang, et alors que ce n'était pas encore l'heure d'une révolte organisée, des explosions ont accidentellement lieu dans une cache d'armes républicaine de la ville. Les révolutionnaires doivent prendre précipitamment les armes et l'emportent finalement dans cette ville : c'est le soulèvement de Wuchang (武昌起義 "wǔchāng qǐyì") du . La nouvelle traversant la Chine, la révolution chinoise de 1911 est lancée. Un peu partout, les groupes secrets prennent les armes et appellent à leur tête Sun Yat-sen, qui revient précipitamment en Chine.

L'Empire s'effondre rapidement et Puyi, l'enfant empereur couronné en 1908, abdique. Le Kuomintang, parti nationaliste républicain fondé par Sun Yat-sen, accède au pouvoir. Mais Yuan Shikai, maître des principales forces armées de la dynastie, parvient à s'imposer comme arbitre. En échange de son ralliement, il succède à Sun Yat-sen à la présidence de la République. Mais lorsque Yuan Shikai ruine les espérances démocratiques et libérales en congédiant le parlement nouvellement établi, il provoque l'opposition des républicains. S'ensuit une période d'instabilité, de soulèvements républicains, de sécessions et de répressions visant notamment le Kuomintang. Au début de 1915, l'Empire du Japon présente à Yuan Shikai ses Vingt et une demandes, qui visent à faire de la Chine un protectorat japonais de fait. Yuan Shikai tente de se faire proclamer Empereur de Chine : la cérémonie officielle ne peut avoir lieu, et il meurt peu de temps après.

À sa mort commence une nouvelle période d'instabilité : c'est la période dite des « Seigneurs de la guerre ». Les principaux généraux et chefs de guerre chinois se battent pour se tailler leur propre domaine de souveraineté. Ces Seigneurs de la guerre se partagent et disputent le nord de la Chine tandis que le gouvernement républicain contrôle le sud, et tente de reprendre le contrôle de l'ensemble. Le parti du Kuomintang est recréé et bénéficie notamment de l'aide matérielle de l'Union soviétique, qui envoie des conseillers politiques du Komintern. Sun Yat-sen dirige différents gouvernements, basés dans le sud de la Chine, qui visent à réunifier le pays. En 1919, le Japon fait accepter à la conférence de paix de Paris ses visées sur l'ancienne concession allemande du Shandong, après avoir conclu un traité secret en ce sens avec le gouvernement chinois, alors dominé par la faction de Duan Qirui. Cette nouvelle provoque en Chine une réaction nationaliste de grande ampleur, connue sous le nom de Mouvement du 4 mai : en juin, le gouvernement de la république de Chine finit par refuser de signer le traité de Versailles. Le Parti communiste chinois (PCC), créé en 1921, est allié au Kuomintang et, sur les conseils des soviétiques, ses membres pratiquent souvent la double appartenance au PCC et au KMT.

À la mort de Sun Yat-sen en 1925, le Kuomintang doit affronter une incertitude quant à sa succession : le général Tchang Kaï-chek fait partie des chefs émergents du parti. Il s'affirme notamment grâce à la création de l'Armée nationale révolutionnaire, une puissante force armée mise sur pied avec l'aide des soviétiques. En 1926, il lance l'opération dite de l'expédition du nord, pour reprendre le contrôle du nord du pays aux seigneurs de la guerre.

Durant cette période, Tchang Kaï-chek affirme son autorité sur le parti : en 1927, inquiet de voir les communistes gagner en influence, il rompt l'alliance et déclenche la purge du parti, dont le début est marqué par le massacre de Shanghai en avril. Le chef de l'aile gauche du Kuomintang, Wang Jingwei, déplace en janvier le gouvernement à Wuhan pour lutter contre les tendances autocratiques de Tchang. Mais ce dernier, dès avril, crée son propre gouvernement à Nankin. Pendant quelques mois, trois gouvernements se disputent la légitimité : celui de la faction de Tchang à Nankin, celui de Wang Jingwei à Wuhan, et celui de Zhang Zuolin à Pékin, soutenu par les Japonais. Mais Wang doit bientôt abandonner la partie et, au début 1928, Pékin est atteinte par les troupes du Kuomintang. Zhang Zuolin prend la fuite et est tué peu après dans un attentat organisé par ses anciens alliés japonais. Nankin devient la capitale du pays et Tchang Kaï-chek devient Chef de l'État avec le titre de "Président du gouvernement central de la république de Chine".
L'opposition interne au Kuomintang ne désarme cependant pas : en 1930, la faction de Li Zongren, alliée à Wang Jingwei et aux seigneurs de la guerre Feng Yuxiang et Yan Xishan, affronte militairement les troupes de Tchang Kaï-chek, lequel remporte cependant la victoire. Les communistes n'ont pas abandonné les armes et continuent d'affronter le Kuomintang. En 1931, ils créent dans le Jiangxi l'enclave de la République soviétique chinoise.

Tchang Kaï-chek suscite de nouvelles oppositions au sein du Kuomintang en faisant arrêter Hu Hanmin, le chef du comité central, au début 1931. Le pays semble au bord d'un nouveau conflit. Mais, en septembre 1931, l'Empire du Japon profite des derniers restes de chaos dans le nord de la Chine pour stopper la réunification chinoise qui se faisait au profit du gouvernement de Nankin. Les Japonais prennent le contrôle de la Mandchourie : l'État-client du Mandchoukouo est créé sur son territoire. Puyi y est à nouveau proclamé Empereur, tout en restant sous le contrôle du pouvoir japonais. Tchang Kaï-chek, devant cet échec, démissionne de son poste de président de la République. Il demeure cependant chef de l'armée et la politique expansionniste du Japon le fait bientôt apparaître comme un chef militaire indispensable. Dès 1935, il prend la tête du gouvernement.

Considérant que ses troupes sont encore trop faibles pour affronter les Japonais, Tchang concentre ses efforts contre les communistes : en 1934, son armée réussit enfin à briser la place-forte de la République soviétique chinoise au Jiangxi. Les communistes doivent fuir à travers le pays, dans l'épisode dit de la Longue Marche, au cours de laquelle Mao Zedong émerge comme chef incontesté du parti.

Le Japon continue sa politique expansionniste et multiplie les opérations pour étendre son influence, soutenant notamment les indépendantistes de Mongolie-Intérieure. À la fin 1936, Zhang Xueliang séquestre Tchang Kaï-chek pour le contraindre à conclure avec les communistes une trêve, aboutissant à un front commun contre les Japonais : c'est l'Accord de Xi'an, qui donne naissance au deuxième front uni chinois.

En 1937, le Japon envahit le reste de la Chine, déclenchant un nouveau conflit, particulièrement sanglant, qui s'intègre à partir de 1941 au théâtre asiatique de la Seconde Guerre mondiale. Nationalistes, communistes et seigneurs de la guerre régionaux affrontent l'envahisseur mais subissent d'importantes défaites en 1937 et 1938. Le Japon contrôle la partie orientale du pays mais échoue à briser les enclaves nationalistes à l'ouest : le conflit s'enlise. À partir de 1940, le Gouvernement national réorganisé de la république de Chine, créé par Wang Jingwei au service des Japonais, dispute la légitimité au gouvernement du Kuomintang, dont il usurpe le nom et l'emblème. La république de Chine ayant rejoint les Alliés à la fin 1941, les États-Unis apportent leur aide et font de la Chine une base pour leur aviation, d'où ils bombardent les positions japonaises. Pendant ce temps, les communistes privilégient les actions de guérilla et consolident leurs forces dans les zones rurales. À partir de 1942, les troupes de la république de Chine interviennent dans la campagne de Birmanie aux côtés du Royaume-Uni, utilisant les bases alliées en Inde comme position de repli. Le 15 mai 1945, la route de Birmanie est rouverte par les Alliés, ouvrant une voie terrestre pour le ravitaillement de la Chine.

En 1945, le Japon capitule, grâce surtout à l'effort américain et aux bombes d'Hiroshima et Nagasaki. L'Union soviétique a également hâté la reddition japonaise en envahissant la Mandchourie, dont elle laisse ensuite le contrôle au Parti communiste chinois.

Taïwan est rendue, avec les Pescadores à la Chine par le Japon à la suite du traité de San Francisco (1951), signé entre les États-Unis et le Japon. Le Traité de Taipei, traité de paix signé en 1952 entre la république de Chine et le Japon, confirme également cela.

À la suite de la reddition japonaise, les troupes gouvernementales chinoises investissent le nord de l'Indochine française, pour participer au désarmement des Japonais présents dans le pays. La France est obligée de négocier le départ des troupes chinoises de sa colonie; la Chine obtient en contrepartie le renoncement des Français à tous leurs privilèges en Chine, depuis les traités inégaux, comprenant les concessions territoriales et les avantages commerciaux. La Chine obtient de son côté des avantages commerciaux en Indochine.

Tchang Kaï-chek exerce un contrôle militaire sur l'essentiel du pays, en particulier les grandes villes, tandis que Mao contrôle d'importantes zones rurales du Nord du pays. Après quelques affrontements en 1945, la guerre civile reprend de manière ouverte dès 1946. De 1945 à 1948, Mao reprend progressivement toutes les campagnes de Chine. Le régime tente de se stabiliser et de se légitimer : alors que la Chine n'avait connu depuis 1912 que des constitutions provisoires, une nouvelle constitution est adoptée début 1947 et entre officiellement en vigueur à la fin de l'année. Mais les communistes continuent leur avancée et les armées nationalistes subissent revers sur revers. En janvier 1949, Tchang démissionne de son poste de président. Li Zongren assure l'intérim, mais il est bien vite évincé par Tchang, qui reprend le pouvoir dans les faits.

En octobre 1949, les communistes proclament la république populaire de Chine à Pékin. En décembre, les dirigeants du Kuomintang et environ deux millions de continentaux s'exilent massivement sur l'île de Taïwan, reprise au Japon en 1945. Le gouvernement nationaliste chinois y installe, de manière censément provisoire, la république de Chine.

Les derniers combats ont lieu au printemps 1950, quand le régime communiste annexe l'île de Hainan et quelques autres territoires, Taïwan demeurant le principal bastion nationaliste.

Le parti continental du Kuomintang, installé sur Taïwan, reste longtemps le parti unique, et l'appellation chinoise de « république de Chine » est encore de nos jours le nom officiel du régime de Taïwan. Étant initialement le seul régime chinois reconnu par le monde non-communiste, la république de Chine conserve après la retraite à Taïwan le siège de la Chine aux Nations unies.

Le régime de la « république de Chine » n'est présent que sur l'île de Taïwan et quelques autres petites îles des environs, mais continue à prétendre une souveraineté sur l'ensemble de la Chine. La période 1945-1949 est particulièrement difficile. Les continentaux du Kuomintang sont largement corrompus, l'île jusque-là japonaise et relativement en paix est vampirisée pour les derniers efforts de guerre sur le continent, l'économie de l'île est coulée par l'inflation du continent.

Le renouveau est rendu possible par la défaite sur le continent : les liens sont coupés. Un autre point décisif est la réforme agraire de Chen Cheng (1949), pour que les paysans taiwanais soient insensibles à la propagande communiste, le Kuomintang permet ici de répartir les terres agricoles entre les paysans, affaiblissant du même coup les grands propriétaires taiwanais (tout de même indemnisés).

Le Kuomintang de Tchang a désormais le plein contrôle de l'île, qu'il place sous un régime de loi martiale (-), et fait la chasse aux communistes, mais les « lois contre la subversion » sont finalement utilisées contre tous les opposants, jusqu'à « ceux qui savent mais ne dénoncent pas ». La guerre froide, et tout particulièrement la guerre de Corée, permettent au régime du Kuomintang de se positionner à nouveau en allié incontournable des États-Unis en Asie. L'importante aide américaine (1951-1965) permet au Kuomintang de poursuivre facilement la politique de modernisation entreprise sous le gouvernorat japonais. Après la première crise du détroit de Taïwan, un traité d'assistance militaire est signé entre la Chine nationaliste et les États-Unis.

Le miracle industrio-économique de Taïwan et le temps passé permet l'émergence d'une nouvelle élite intellectuelle démocrate. Lorsqu'en 1956, Tchang Kaï-chek se déclare « sans préjugé et à l'écoute de la nation » , Lei Zhen et l'équipe de "La Chine Libre" s'engage dans la rédaction d'analyses-critiques de la politique du Kuomindang, puis de la constitution même. Les 5 élus taiwanais rejoignent logiquement le mouvement, qui prend le nom de "Parti Démocratique Chinois", mais le 4 septembre 1960, ses leaders sont emprisonnés, tuant l'alliance des élites démocrates chinoise et taiwanaise.

Par ailleurs, la république populaire de Chine et l'Union soviétique étant désormais en opposition franche, les É.-U. se rapprochent de la Chine populaire. En 1971, la résolution 2758 de l'ONU retire le siège de membre permanent au gouvernement de république de Chine pour l'« offrir » au gouvernement de la Chine populaire, ce qui entraine une crise diplomatique entre la république de Chine et les États-Unis.
Les dernières années de Tchang (mort en 1975), permirent l'apparition d'élus « Hors parti ». Mais c'est surtout lorsque son fils Chiang Ching-kuo arriva au pouvoir -selon sa volonté- que des Taiwanais de souche accèdent aux postes de responsabilité politique. Li Tenghui devient ainsi maire de Taipei en 1978, gouverneur de la province en 1981, puis vice-président de la République en 1984.

Les élus locaux n'étaient en fait pas libres de tout leur choix, tandis que le pouvoir central était encore sous le contrôle des parlementaires élus en terre chinoise, et promis à retrouver leur place après la reconquête prévue. Mais les opposants et le mouvement « Hors parti » profitent de chaque élection locale comme d'une tribune, et certaines fraudes du Kuomintang ravivèrent la conscience politique et les aspirations démocrates des habitants. À partir de 1969, des réformes permettent de timides percées de l'opposition aux législatives de 1972, mais surtout de 1975, puis de 1977 et sa participation record, marquée par l'émeute de Zhongli, et les « Douze grands travaux politiques » de leur programme commun. 1979 est un pas de plus dans le sens des « Hors parti », des incidents éclatèrent ("l'Affaire Formosa"). Le gouvernement lance alors une vague d'arrestations chez les « Hors parti », ruinant la construction du mouvement. Le procès qui suit, avec l'accusation de leaders tel Annette Lu, les mit durablement en lumière, et permit l'apparition d'avocats brillants, tel Chen Shui-bian, et lance un soutien populaire envers les leaders restant éligibles: Su Zhengchang, Xie Zhangting, Cheng Shui-bian élus en 1980. En 1983, une « Amicale des Hors parti » est créée, se comportant comme des permanences de parti politique, mais sans enfreindre les points de la loi anti subversion. Mais en 1986, la décision est prise de se faire dénommer désormais Parti démocrate progressiste (ou "Minjindang"), un défi lancé au Kuomintang, et Chiang Ching-kuo répondit en déclarant « Les temps changent, le contexte change, les grandes tendances changent », encourageant à la réforme du Kuomintang. L'avancée du PDP aux législatives de 1986 confirma la « tendance », le KMT ne semble pas pouvoir répondre, et la population de l'île s'agite. Le , la loi martiale est levée par Chiang Ching-kuo, la censure de la presse est levée en janvier 88. Chiang meurt le , et Li Denghui, taiwanais de souche, lui succède.

Li Tenghui (ou Lee Teng-hui), taiwanais de souche, continue l'ouverture entreprise, insuffle au KMT une direction progressiste qui fait le jeu des réformes et de l'opposition, Li Tenghui se plaçant en médiateur entre les conservateurs du KMT et les réformateurs populaires du PDP. Sous la pression populaire, une loi pour la « démission volontaire » des parlementaires de « l'assemblée nationale [chinoise] éternelle » est proposée le -avec indemnités-, et en 1989, les démocrates progressent encore avec 40 % des votes. Le Kuomintang se divisa (02/1989) et les étudiants se lancent alors dans des sittings pacifistes pour la démocratie, pour l'élection présidentielle au suffrage universel, et pour la démission des vieux parlementaires en poste depuis 1947. Li Tenghui, réformateur, en sort renforcé, et propose une révision de la constitution et une réforme en faveur du pluripartisme. Mais une partie du KMT souhaite un raffermissement, et Li Tenghui nomme donc Hao Bocun -un militaire- au poste de Premier ministre, créant une réaction populaire, qui affaiblit donc la ligne dure du KMT, et légitime la modération de Li Tenghui. La question des vieux parlementaire est finalement réglée en limitant les dernières indemnités jusqu'au . Beaucoup de vieux parlementaires quittent le parlement, dissolvant la ligne la plus dure du KMT, et débloquant la vie parlementaire. La démocratisation continue, les taiwanais de souche investissant de plus en plus la politique du pays, amenant davantage représentativité, les divergences n'en disparaissent pas pour autant. En 1996, les premières élections présidentielles au suffrage universel reconduisent Li Tenghui.

Mais le changement est sans doute ailleurs, la démocratisation a également amenée la discussion sur le « Statut de Taiwan », de l'île, car l'état continue de se prétendre un état chinois, légitime souverain de l'ensemble de la Chine. Des voix s'élèvent pour demander la clarification de ce statut en faveur de la déclaration d'un état taiwanais, purement insulaire, reflétant l'état de fait, et qui abandonnerait donc l'appellation de « république de Chine » pour quelque chose du type « république de Taïwan ». D'abord, de tels militantismes ont été réprimés dans le cadre de la loi contre la subversion (années 1970, 1980), les auteurs de tels idées étant arrêtés. Mais la réalité des faits, et la volonté de se dissocier de l'état communiste de la république populaire de Chine, actrice de la répression de Tian'anmen conduit taiwanais de souche et certains immigrés chinois à militer en faveur de cette officialisation de l'indépendance, le PDP se montrant officieusement favorable à ceci, sans trop se mouiller.

Avec l'élection du candidat du PDP Chen Shuibian à la présidence, en 2000, devenant ainsi le premier président hors KMT, l'idée d'indépendance et de fin de la « république de Chine » est renforcée, mais le KMT et la Chine Populaire s'opposent vivement à cette idée, et la réaction s'organise. Le KMT empêche la tenue d'un référendum, tandis que la Chine Populaire -qui souhaite toujours récupérer l'île- menace l'île de représailles militaires en cas de déclaration d'indépendance.
En 2004, Chen Shuibian est réélu, mais ne parvient pas à imposer ses vues indépendantistes à la population qui, majoritairement, se satisfait d'un statu quo pacifique qui permet la stabilité et la prospérité économique de l'île. En mars 2005, la Chine populaire rationalise et officialise sa position par les 10 points de la Loi anti sécession chinoise de 2005.

De nos jours, la « république de Chine » existe donc encore, à Taïwan, et se pose la question de son existence en tant que future province de la république populaire de Chine ; en tant qu'entité étatique non reconnue de la république de Chine (statu quo) ; ou en tant qu'État de la « république de Taiwan » réclamant une reconnaissance d'État.

Le 3 décembre 2005, le Kuomintang sort large vainqueur des élections régionales.

Lors de l'élection présidentielle taïwanaise de mars 2008, Ma Ying-jeou, candidat du Kuomintang est élu à la présidence.

Le , au terme de son mandat, Chen Shui-bian perd son immunité et redevient simple citoyen avec interdiction de quitter Taïwan. Le 11 novembre de la même année, il est mis en examen pour corruption et arrêté. Le 11 septembre 2009, Chen Shui-bian est jugé coupable de corruption, détournement de fonds et blanchiment d'argent pour un montant de 490 millions NT$. Il est condamné par la justice taïwanaise à la réclusion criminelle à perpétuité et doit en outre verser une amende de 200 millions NT$. Il est ainsi le premier président taïwanais à être condamné à une peine de prison. Le 11 juin 2010, la Haute Cour de justice commute sa peine de prison à 20 ans, l'ayant reconnu innocent du détournement de certains fonds diplomatiques trois jours plus tôt. Le 19 août 2010, le parlement taïwanais vote un amendement proclamant que les anciens présidents et vice-présidents reconnus coupables de corruption ou de sédition se verraient destitués de leurs avantages (rente mensuelle de l'état, notes de frais, gardes du corps, etc.).

Le Kuomintang signe des accords avec la Chine continentale. Des lois facilitant les échanges de marchandises et de la circulation des personnes entre les deux parties du détroit. Des lignes aériennes entre différentes grandes villes du continent et de l'île.

Le 14 janvier 2012, Ma Ying-jeou est réélu à la présidence et le Kuomintang prend une majorité de 64 sièges sur 113 au parlement.





</doc>
<doc id="17967" url="https://fr.wikipedia.org/wiki?curid=17967" title="Françoise d'Eaubonne">
Françoise d'Eaubonne

Françoise d'Eaubonne, née Françoise Marie-Thérèse Piston d'Eaubonne le à Paris et morte le à Paris, est une femme de lettres française et une féministe libertaire. Elle est la sœur de l'écrivain Jehanne Jean-Charles.

Troisième enfant d'Étienne Piston d'Eaubonne, un anarchiste chrétien originaire de Bretagne, co-fondateur du Parti fasciste révolutionnaire et membre du Sillon, et de Rosita Martinez Franco, une fille de révolutionnaire espagnol carliste, son enfance toulousaine est marquée par le déclin physique de son père dû aux effets des gaz dans les tranchées de la Première Guerre mondiale. Elle a seize ans quand éclate la guerre d'Espagne, dix-neuf ans quand elle voit arriver les républicains en exil. De 20 à 25 ans, elle subit les privations propres à l'époque et rencontre à la Libération, dans une grande gare parisienne, les rescapés juifs de retour des camps. Elle résumera plus tard son sentiment sur cette période de sa vie sous le titre évocateur de "Chienne de jeunesse".

Cette enfance plaquée sur une personnalité hypersensible la conduit à porter sur le monde un regard critique qui façonnera la militante radicale et féministe. Un temps membre du Parti communiste français, elle milite activement contre la guerre d'Algérie et en septembre 1960, signe le Manifeste des 121. 

Cofondatrice du Mouvement de libération des femmes (MLF) dans les années 1960, signataire du Manifeste des 343 pour le droit à l'avortement, elle lance le FHAR (Front homosexuel d'action révolutionnaire) avec l'écrivain et journaliste Guy Hocquenghem et Anne-Marie Grélois en 1971. Au sein du MLF, elle anime également le groupe « Écologie et féminisme ». À l'origine du mot « phallocrate », du terme écoféminisme en 1974, elle fonde l'association Écologie-Féminisme en 1978. Cette vie littéraire et militante se croise avec celles de Colette, Jean Cocteau, Simone de Beauvoir dont elle fut une amie très proche, et de Jean-Paul Sartre.

Elle est mère de deux enfants, Indiana et Vincent. Elle meurt à Paris le et est incinérée au cimetière du Père-Lachaise, à Paris.


« Pas un jour sans une ligne » : c'est sous la férule de ce mot d'ordre que l'auteur a produit plus de 50 ouvrages, de "Colonnes de l'âme" (poèmes, 1942) à "L'Évangile de Véronique" (essai, 2003) en passant par quelques romans de science-fiction ("L'Échiquier du temps", "Rêve de feu", "Le Sous-marin de l'espace", "Les Sept Fils de l'étoile"…). Parmi ses ouvrages, on peut distinguer :






Pamphlets ("20 ans de mensonges", contre Longo Maï), traductions (poèmes d'Emily Brontë), édition critique des lettres de Flaubert, nombreuses préfaces, etc.

Elle aura laissé avant de mourir un impressionnant volume de souvenirs, sous le titre "Mémoires irréductibles", qui regroupe : "Putain de jeunesse" (antérieurement publié sous le titre plus correct que son éditeur avait voulu, "Chienne de jeunesse"), "Les Monstres de l'été", "L'Indicateur du réseau" et "Les Feux du crépuscule", ce dernier inédit jusque-là.

Le documentaire de Alessandro Avellis, "La Révolution du désir" (2006), lui rend un hommage tout en retraçant l’histoire du FHAR et ses liens avec Guy Hocquenghem.




</doc>
<doc id="17969" url="https://fr.wikipedia.org/wiki?curid=17969" title="Bugle">
Bugle

Le bugle (en allemand et par extension en anglais) est un instrument de musique de la famille des cuivres, plus exactement des saxhorns mis au point par Adolphe Sax au XIXe siècle. 

Le modèle courant est celui en siformula_1. Il a théoriquement le même registre que la trompette en siformula_1 et demande l'application des mêmes doigtés. Sa perce conique, caractéristique des saxhorns, rend l'accès au registre aigu difficile, et le classe dans les « cuivres doux ». Cette conicité lui donne une réponse rapide et surtout un son très doux et très « rond ». On retrouve cet instrument dans les brass-bands, les harmonies et les fanfares où il est utilisé pour ses qualités sonores dans le grave et le médium. 

Parfois, mais aujourd'hui de plus en plus rarement, un « petit bugle » en miformula_1 est utilisé pour des parties aiguës en fanfare ou en harmonie. Florent Schmitt, par exemple, l'emploie dans ses "Dionysiaques".

Le bugle est très rarement utilisé dans l'orchestre classique. On le trouve dans le solo de bugle de "" d'Igor Stravinsky, dans la de Vaughan Williams et dans les symphonies n°3 et n°4 de Villa-Lobos (dans les fanfares). Il est aussi utilisé en substitution dans la Symphonie nº 3 de Mahler, mais est souvent joué au cor de postillon (le bugle à palettes allemand ressemblant très fortement au cor de postillon). On le trouve aussi dans "Les Pins de Rome" et "Les Fêtes romaines" de Respighi, lorsque l'orchestre ne dispose pas de buccin. Charles Koechlin aussi l'emploie dans certaines de ses œuvres orchestrales : il y a par exemple un bugle en siformula_1 dans La Loi de la Jungle ( opus 175 ), d'après Rudyard Kipling.

Le répertoire concertant fait peu à peu son apparition, citons Aroutiounian et son "Élégie pour bugle et orchestre à cordes" ou encore William-Himes et le "Concertino pour bugle et brass band". En musique de chambre, c'est en général dans le quintette de cuivres que le bugle fait son apparition (les trompettistes jouant de l'instrument) ; on peut aussi noter une "Sonate pour cornet et piano" de Maurice Emmanuel, qui peut être jouée au bugle.

En jazz, le bugle est souvent utilisé comme instrument soliste. En général, il est joué par les trompettistes, et rares sont les jazzmen qui n'utilisent que cet instrument.





</doc>
<doc id="17970" url="https://fr.wikipedia.org/wiki?curid=17970" title="Instrument à cordes">
Instrument à cordes

Un instrument à cordes est un instrument de musique dans lequel le son est produit par la vibration d'une ou plusieurs cordes. Dans la classification musicologique, les instruments à cordes correspondent à la catégorie des cordophones. 

L'histoire des instruments à cordes est vieille de plusieurs milliers d'années. Les premiers n'avaient probablement qu'une seule corde, comme l'arc musical. Dès l'Égypte ancienne, on connaissait les joueurs de harpe. Au Moyen Âge, les ménestriers s'accompagnaient au luth, etc.

La vibration de la corde seule est peu audible. Une plaque couplée aux cordes, la table d'harmonie, prélève une partie de l'énergie vibratoire de la corde pour la transmettre à l'air et obtenir un son. La table d'harmonie peut être une peau de tambour, comme dans le banjo ou la kora.

La fréquence fondamentale de la vibration dépend des caractéristiques de longueur, de masse et de tension de la corde. 

Les cordes ont subi de nombreuses évolutions, du fait notamment des techniques disponibles et de critères esthétiques et symboliques. On a utilisé des fibres végétales, des produits animaux comme le crin de cheval, le boyau ou la soie, des fils métalliques, et plus récemment les fibres synthétiques comme le nylon.

Pour augmenter la masse de la corde sans en détériorer l’élasticité, les luthiers fabriquent des cordes où une âme souple est entourée d'un fil métallique (souvent un alliage de cuivre) qui en augmente la masse. Cette augmentation permet des sons plus puissants et plus graves.

Il existe trois modes de jeu principaux sur les cordophones, correspondant aux possibilités d'excitation de la vibration de la corde :


Dans chacun de ces trois cas, il existe des instruments où un mécanisme excite la corde.

Certains instruments comportent de plus des cordes excitées indirectement, sans que le musicien ne les actionne, qu'on appelle "cordes sympathiques" (viole d’amour, Sitar).

On peut aussi classer les instruments à corde par disposition. On distingue alors

La guitare, la basse, le banjo, la mandoline, le luth... On joue de ces instruments en pinçant les cordes avec les doigts ou avec un plectre.

La harpe et le clavecin utilisent des cordes chromatiques tendues sur une table d'harmonie en bois de résonance (souvent épicéa). Le clavecin possède de par son coffre un résonateur avec une rosace comme le luth.

Dans la Bible, les civilisations orientales, la Grèce ainsi qu'au Moyen Âge, on retrouve les traces du psaltérion. Le psaltérion est joué avec une plume, c'est l'ancêtre de la famille d'instruments du genre clavecin. L'ajout d'un clavier au psaltérion donne naissance à l'épinette, terme qui désignait autant le clavecin que l'épinette contrairement à l'acception moderne.

En y adaptant un clavier et un mécanisme nommé sautereau muni d'un plectre pour pincer les cordes, le clavecin et l'épinette sont apparus au . Les claviers de trois octaves se sont agrandis jusqu'à cinq octaves (63 notes) au cours des siècles ce qui a donné de grands instruments ; qui parfois ont voulu rivaliser avec l'orgue en multipliant les registres et en ajoutant, un deuxième clavier (Flandre, France) voire un troisième clavier ou un pédalier (Allemagne).

Par exemple le piano, le cymbalum, le clavicorde. Les cordes sont frappées avec un marteau lorsqu'on appuie sur la touche.

Le piano utilise des cordes tendues sur une caisse de résonance en bois. Pour faire sonner les cordes, le piano les frappe avec des petits marteaux. Le tympanon, le cymbalum apparaissent au Moyen Âge. Le tympanon, joué à l'aide de mailloches, donnera par la suite au le clavicorde muni d'un clavier. Au bout de la touche du clavier de clavicorde est fichée une lame métallique qui vient directement percuter la corde. Cette pièce est appelée tangente car elle divise la corde en deux parties, dont l'une est étouffée pour ne pas vibrer. Le clavicorde est le premier instrument à clavier et à cordes frappées. 

Au Bartolomeo Cristofori (1655-1731) invente le piano, mais ce piano-forte équipé d'une transmission clavier→marteau est très éloigné du piano actuel : à la place de la tangente est disposée une fourche dans laquelle s'articule un levier dont la grande extrémité est munie d'un marteau garni de peau, la petite extrémité est retenue par une barre fixe, il faut relâcher la touche pour répéter la note. 

Ce système sera perfectionné plus tard avec l'invention de l'échappement simple qui porte le nom de "mécanique viennoise" ; la barre fixe est remplacée par un élément muni d'un ressort qui se retranche dès que le marteau a frappé et permet ainsi de rejouer la note aussitôt. Ce système d'échappement va s'améliorer dans le courant du avec le double échappement. Ceci est la première génération de piano, qui en compte trois, jusqu'au piano actuel.

Ce sont les violons et les instruments similaires de l'orchestre symphonique européen, et de nombreux instruments de musique populaire ou érudite de par le monde, comme le erhu chinois.

Dans la plupart des cas, on frotte les cordes avec un archet. La surface légèrement adhésive déplace la corde, jusqu'à ce que la force de rappel à sa position de repos dépasse la limite d'adhérence. La corde revient alors vers sa position de repos, la dépasse et revient dans l'autre sens ; à un certain moment, la vitesse de la corde par rapport à l'archet est nulle, et elle adhère de nouveau. Ce processus produit une vibration, qui a la particularité d'avoir une fréquence fondamentale légèrement différente de celle de la corde vibrant sans excitation. Cette différence dépend des autres paramètres, comme la nature de l'enduit sur l'archet, en général, une résine appelée colophane, la force d'appui, la vitesse du mouvement, contrôlés par l'instrumentiste, comme le paramètre principal, la longueur de la corde, que le musicien règle, dans le cas du violon, en appuyant une ou plusieurs cordes sur le manche. Les vibrations se transmettent par le chevalet à la caisse de résonance.

La corde à vide, dont la longueur et la tension sont fixes, varie un peu de fréquence fondamentale avec la force d'appui et la vitesse de l'archet, en même temps que change le volume sonore. Les instruments à cordes frottées de la musique orchestrale occidentale, de la famille des violons, n'ont pas de frettes. C'est ce qui permet à l'instrumentiste de jouer juste les différentes notes, tout en faisant varier les trois causes qui affectent la puissance et la fréquence fondamentale des vibrations qui se transmettent par le chevalet à la caisse de résonance.

La vielle à roue frotte les cordes avec la tranche d'un disque mu par une manivelle. La construction et le réglage de l'instrument déterminent la force d'appui sur la corde.



</doc>
<doc id="17971" url="https://fr.wikipedia.org/wiki?curid=17971" title="Luth">
Luth

Le luth est un instrument à cordes pincées. Le terme désigne aussi de manière générale tout instrument ayant les cordes parallèles à un manche. Bien que voisin de la guitare, le luth a connu une histoire différente et distincte, les deux instruments ayant coexisté au cours des périodes principales de la musique. Il est d'origine persane (barbat) pour la forme générale et arabe pour la caisse en lamellé-collé.

Il faut distinguer aujourd'hui dans le langage courant, deux grands types de luths :



Le luth est presque entièrement en bois. La table est faite d'une fine planche de bois résonnant (le plus souvent de l'épicéa) en forme de poire. Les luths ont parfois trois rosaces décorées. Elles ne sont pas ouvertes comme sur une guitare classique actuelle, mais constituées d'une grille décorative sculptée dans le bois de la table lui-même.

Le dos de l'instrument est un assemblage de fines planches de bois appelées côtes assemblées (avec de la colle) bord à bord pour former le corps très arrondi de l'instrument. L'intérieur de cette coque est renforcé par des bandes de parchemin collées.
Le manche est réalisé dans un bois léger et couvert de bois dur (en général de l'ébène) pour la touche. Contrairement à la plupart des instruments actuels, la touche est au même niveau que la table. La tête des luths avant la période baroque faisait un angle avec le manche de près de 90°, probablement pour que les cordes de faible tension restent fermement plaquées au début du manche. Les chevilles sont coniques et sont maintenues en place par friction dans les trous qui les reçoivent. La forme et les bois employés font du luth un instrument très léger pour sa taille.

Les frettes sont réalisées à l'aide de cordes en boyau nouées autour du manche. Quelques frettes supplémentaires en bois sont généralement collées sur la table, où elles ne peuvent être nouées derrière le manche.

Les cordes en boyau (plus rarement en métal) sont groupées en chœurs, généralement deux cordes par chœur (plus rarement trois) à l'exception du chœur le plus aigu, la chanterelle, constitué d'une seule corde ; dans les luths baroques tardifs, les deux chœurs les plus aigus sont généralement deux cordes simples. Les chœurs sont comptés en groupe, ainsi la corde la plus aiguë est appelée premier chœur, les deux cordes suivantes (pour un luth renaissance) deuxième chœur etc. Un luth renaissance à huit chœurs aura donc habituellement quinze cordes (sept fois deux cordes et une corde simple) et un luth baroque à treize chœurs, vingt-quatre cordes (deux fois onze cordes et deux cordes simples).

Les chœurs sont accordés à l'unisson dans l'aigu et le médium, les chœurs graves ayant une des cordes accordée à l'octave supérieure (selon les époques, cette octave est rajoutée à partir de hauteurs différentes). Les deux cordes d'un chœur sont normalement jouées à la même hauteur (dans la même case) et simultanément, à l'exception de cas rarissimes où elles sont jouées séparément ou dans des cases différentes. 

L'accord du luth est très peu standardisé et assez complexe. Les luths ont été construits dans des tailles très variables et sans standard permanent d'accord, le nombre de cordes et de chœurs ayant lui aussi beaucoup changé.

Cependant, on peut dire que l'accord du luth Renaissance ci-dessous est "en général" celui qui était employé : pour un luth à 6 chœurs, on retrouve l'accord de la viole de gambe ténor, les chœurs ayant entre eux un intervalle de quarte juste, à l'exception de l'intervalle entre les et chœurs qui est une tierce majeure (on peut retrouver l'accord du luth Renaissance sur une guitare en abaissant la troisième corde au fa# - au lieu de sol - et en utilisant un capodastre à la troisième case ; sans capodastre, les notes jouées depuis la tablature sonnent une tierce mineure au-dessous du luth).

Pour les luths de plus de 6 chœurs, les chœurs sont ajoutés vers le grave. En raison du grand nombre de cordes et de la largeur du manche, il est alors difficile de modifier le son des chœurs les plus graves en plaçant un doigt de la main gauche sur la touche ; ces chœurs sont donc plutôt utilisés de façon diatonique pour permettre leur utilisation en cordes à vide, comme pour le luth pré-baroque à 10 chœurs - voir ci-dessous.

Il s'agit ici de l'accord habituel, et il arrive que le compositeur donne d'autres indications sur la manière d'accorder - par exemple : chœur en Fa.

Le accroît encore la diversité d'accords, tout particulièrement en France. À la fin du siècle, une certaine norme se met cependant en place, l'accord dit "en ré mineur" s'imposant. Les basses indiquées ci-dessous pour le luth baroque sont modifiées selon la tonalité jouée.

Le diapason peut varier de la=392 Hz à la=470 Hz selon le pays, le répertoire, l'instrument et le diapason utilisé par les autres instruments dans le jeu d'ensemble. Il n'existe pas de standard universel à l'époque, seuls des standards locaux peuvent être établis.

Malgré sa quasi-disparition au , l'instrument continue à survivre tant en France qu'aux États-Unis. On trouve ainsi des traces du théorbe et du chitarrone au salon des Champs-Élysées de 1895. On parle aussi d'un luthiste aux États-Unis accompagnant les films muets, et composant pour son instrument.

Quelques compositeurs français du ont écrit pour le luth à 10 cordes : Jacques Chailley, Yvonne Desportes grand prix de Rome, Jean Loubier à la demande de Michel Faleze, luthiste français et élève de Michael Schäffer. À signaler une pièce du compositeur hongrois Georg Aranyi-Aschner et une messe en latin avec luth, hautbois solo, quatuor à cordes, orgue et chœurs.

Au , certains luthistes composent pour leur instrument une musique originale et idiomatique : Jozef Van Wissem, Ronn McFarlane et Tsiporah Meiran ont ainsi entrepris de composer une musique contemporaine pour cet instrument initialement réservé au répertoire ancien.





Le luth se voit aussi dans les célèbres tapisseries de Charles le Téméraire




</doc>
<doc id="17972" url="https://fr.wikipedia.org/wiki?curid=17972" title="Clavecin">
Clavecin

Un clavecin est un instrument de musique à cordes muni d'un ou plusieurs claviers dont chacune des cordes est « pincée » par un dispositif nommé sautereau.

Terme générique, il désigne différents instruments d'une même famille, distincts par leurs structures, leurs formes, leurs dimensions ou leurs timbres, chacun d'entre eux ayant souvent un nom spécifique. Le mot « clavecin », au sens restrictif, désigne alors le plus grand, le plus complet et le plus techniquement développé d'entre eux, généralement appelé « grand clavecin ».

Instruments spécifiques de la musique européenne, les clavecins ont connu leur apogée et suscité un très large répertoire au cours des , avant de connaître une longue éclipse pendant tout le . Ils ont retrouvé la faveur des musiciens et du public depuis le début du .

Comme pour l'orgue, la puissance des sons émis ne dépend pas de la force avec laquelle le claveciniste frappe les touches ; c'est la présence de registres affectés à chacun des claviers qui permet de varier les timbres. Pendant toute la période « baroque », le clavecin a été l'un des instruments privilégiés de l'écriture en contrepoint, et de la réalisation de la basse continue. Mais ses possibilités expressives se sont révélées moins appropriées au style du classicisme naissant, et surtout, par la suite, à la sensibilité du romantisme : les compositeurs lui ont préféré le piano-forte, puis le piano, nouvellement inventés. C'est à l'occasion de la redécouverte de la musique ancienne que le clavecin a connu son actuel renouveau. Contrairement à ce que beaucoup de gens disent, le clavecin n'a jamais été l'ancêtre du piano. En effet, ces deux instruments ne font pas partie de la même famille (le clavecin fait partie des cordes pincées car ses cordes sont "pincées" par les sautereaux et le piano fait partie des cordes frappées car ses cordes sont "frappées" par de petits marteaux).

Cet article traite par priorité le grand clavecin. Tous les instruments de la famille partagent une histoire et des techniques de facture communes, ainsi qu'un répertoire en grande partie commun ; les points qui leur sont propres sont traités dans des articles séparés.

Les descriptions ci-dessous s'appliquent généralement avec de nombreuses variantes, aux clavecins historiques fabriqués au cours des ainsi qu'aux instruments contemporains dont la facture s'inspire aujourd'hui, le plus souvent, de leur modèle. Les clavecins « modernes » fabriqués au début et pendant la première partie du pouvaient avoir des caractéristiques assez différentes, au point que certains vont jusqu'à leur contester le nom de clavecin : un paragraphe spécial leur est consacré.

Le grand clavecin a la forme d’une harpe disposée horizontalement. Cette forme est proche de celle d'un triangle rectangle dont l'hypoténuse serait concave. Le ou les claviers sont placés sur le petit côté de l'angle droit. Les cordes sont disposées horizontalement, dans une direction perpendiculaire au(x) clavier(s).

L'instrument mesure environ de 2 à de long sur un mètre de large. Son étendue couvre ordinairement de 4 octaves et demie à cinq octaves et n'a jamais été normalisée. La structure est en bois : contrairement au piano, le clavecin à l'ancienne ne comporte pas de cadre métallique ; léger, il peut aisément être déplacé par deux personnes.

La caisse (ou coffre) constitue la structure principale du clavecin et définit sa forme extérieure et son volume. Elle est indépendante du piètement sur lequel elle repose.

C'est un volume presque entièrement clos, en bois, qui joue le rôle de caisse de résonance. Elle est construite autour d'une pièce de bois massive (généralement en chêne), disposée parallèlement au(x) clavier(s) : le "sommier". À gauche (notes graves), la paroi (l’"échine") est rectiligne. À droite (notes aiguës) elle est concave, c'est l"'éclisse courbe" qui rejoint l’échine par une "queue" (ou "pointe") rectiligne ou convexe. Une paroi rectangulaire (la "joue") la prolonge à droite des claviers. Des éléments internes en bois (barres de fond, arcs-boutants, équerres, renforts divers) rigidifient la caisse pour contrer la tension importante exercée par les cordes et éviter toute déformation ; leur agencement varie selon les différentes traditions de facture. La caisse est fermée vers le bas par le « "fond" ».

À la partie supérieure, sous les cordes, se trouve la table d'harmonie, qui couvre presque en totalité la surface de l'instrument.

Un couvercle rabattable, articulé à l'échine, permet de refermer celui-ci quand il est inutilisé afin de protéger de la poussière et des chocs : les cordes et la table d'harmonie. Le couvercle joue aussi un rôle important quand il est ouvert, par la réflexion du son vers l'horizontale. Il peut être d'une seule pièce, ou, beaucoup plus fréquemment, en deux parties articulées. Il est maintenu en position ouverte par une "béquille", simple baguette de bois non fixée à l’instrument. Par ailleurs, un panneau amovible (le "portillon"), peut venir enfermer par l'avant l'espace des claviers.

Les cordes consistent en un simple fil métallique de faible diamètre et, contrairement à celles du piano, elles ne sont pas « filées ». Elles peuvent être en fer, en laiton, en cuivre ou en bronze, et sont disposées dans le sens de la plus grande longueur, du clavier vers la pointe. 

Vers la pointe, chacune d'elles est fixée à une pointe d'accroche située près de l'éclisse courbe. Près du clavier, chacune s'enroule sur une cheville d'accord qui permet de régler finement la tension, donc la hauteur du son émis. Entre ces deux points fixes, chaque corde enjambe deux pièces de bois dur : sillet (fixé sur le sommier), et chevalet, (collé sur la table d'harmonie). Sur le sillet comme sur le chevalet, la corde est guidée par des pointes métalliques qui permettent de fixer précisément sa position. La longueur utile ainsi établie entre pointe de sillet et pointe de chevalet détermine la hauteur du son émis.

À chaque note, correspondent une ou plusieurs cordes, groupées en nappes imbriquées ou superposées au-dessus de la table d'harmonie : l'ensemble des cordes d'une même nappe constitue un « rang » ou « jeu » et peut posséder un sillet et/ou un chevalet particulier. Selon la disposition de l'instrument, les sillets et chevalets peuvent donc être uniques ou multiples.

Chaque sillet est rectiligne ou presque ; la longueur des cordes croît de la droite vers la gauche - c'est-à-dire de l'aigu vers le grave - déterminant la forme du chevalet et, grossièrement, celle de l'instrument. Si elles étaient toutes de même diamètre et de même matière, une étendue (usuelle) de 5 octaves impliquerait une longueur excessive de la corde la plus grave. Pour l'éviter, on fait varier leur diamètre (de pour les plus aiguës, à pour les plus graves, chiffres indicatifs), ainsi, éventuellement, que leur matière (fer pour l'aigu, bronze pour l'intermédiaire, cuivre pour le grave).

Du fait de ces corrections, plus importantes vers les graves, le chevalet à une forme en S ou en équerre, voire en plusieurs sections. et non celle d'une courbe exponentielle.

On appelle « module » (anglais : "scale", allemand : "Mensur") la longueur utile (entre sillet et chevalet) de la corde correspondant au Do au-dessus du milieu du clavier. Le module est considéré comme court autour de 25–28 cm et comme long autour de 32–36 cm ; le module détermine généralement la matière des cordes : cuivre ou bronze pour un module court, fer pour un module long.

Les vibrations des cordes sont transmises à la table d'harmonie, laquelle joue un rôle d'amplificateur, et qui consiste en une lame de bois très fibreux, très mince (entre et , chiffres indicatifs), occupant presque toute la surface de l’instrument. Cette transmission se fait par l’intermédiaire du chevalet, pièce de bois dur qui est collée sur la table d'harmonie et sur laquelle sont tendues les cordes. La cavité de la caisse sert de résonateur.

La table d'harmonie est renforcée (par en dessous, donc de façon invisible) par des barres de bois qui la raidissent partiellement. Le "barrage" joue de façon déterminante sur la qualité du son, et sa disposition exacte était un secret de fabrication jalousement gardé par les facteurs. Il diffère selon les écoles de facture et la disposition de l'instrument.

La table d’harmonie est ordinairement percée d'une ouïe dans son angle droit ; l’orifice généralement circulaire est alors muni d’une rosace ouvragée en parchemin, ou dégagée dans le bois, à motif géométrique (clavecins italiens), ou en étain doré, souvent ornée d'un ange musicien (clavecins flamands et français) : dans ce dernier cas, elle porte la marque du facteur. Cette ouïe n'est pas indispensable mais joue aussi sur le son, permet d'équilibrer l'hygrométrie et d'éviter un couplage avec le fond.

Dans les clavecins de tradition flamande ou française, la table d’harmonie - de même que le plaquage de sommier qui semble la prolonger vers le clavier -, est très généralement décorée de motifs floraux, d’insectes, d’oiseaux, etc., ce alors que les Italiens et les Anglais préfèrent le bois brut. La table n'est pas vernie.

Le clavecin possède un ou deux claviers, voire trois, de manière très exceptionnelle. Leur étendue n'est pas normalisée, elle est inférieure à celle du piano et varie généralement entre 4,5 et 5 octaves, soit de 56 à 61 notes : souvent de Fa0 à Fa5 (anglais/allemand : FF à f<nowiki>"'</nowiki>, américain : F1 à F6). "N.B. le la3 correspond au diapason, 440 Hz (classique) ou 415 Hz voire 392 Hz (baroque) ; le do4 définit le module des cordes".

Le clavier supérieur, s'il existe, est en retrait par rapport au clavier principal et peut, selon la disposition, s'accoupler comme dans l'orgue, au clavier inférieur (ou clavier principal).

L'image classique du clavier de clavecin est celle de couleurs inversées par rapport à celui du piano. Ceci est surtout vrai pour les clavecins de tradition française, et l'est beaucoup moins pour les autres traditions de facture. En revanche, les touches sont moins longues que celles d'un piano et leur partie frontale est généralement ornée d'arcades plus ou moins travaillées (voir photo).

La présence de plusieurs claviers rend le clavecin particulièrement adapté à la musique ancienne où le contrepoint est important : chaque main peut jouer sur un clavier différent sa propre ligne mélodique indépendante. Elle permet aussi de jouer sur les oppositions de timbre entre les différentes sections d'une même pièce.

L'élément principal du mécanisme du clavecin est une lamelle de bois dur appelée sautereau qui se présente verticalement au-dessus de la partie arrière (cachée) de la touche.

Il est maintenu dans cette position par les registres disposés horizontalement et parallèlement au(x) clavier(s). Les registres sont généralement au nombre de deux par rang de sautereaux : celui du bas est fixe ; celui du haut est mobile et peut se déplacer latéralement de quelques millimètres, permettant de mettre en action ou non le rang de sautereaux correspondant. Dans les instruments les plus simples, ne possédant qu'un rang de sautereaux, il n'y a pas de registre mobile (l'unique jeu de sautereaux étant toujours actif). Les registres sont percés d'orifices rectangulaires, éventuellement garnis d'une basane, au travers desquels le sautereau peut coulisser librement, mais avec un jeu très ajusté, de bas en haut.

La touche constitue un levier : lorsque le claveciniste appuie sur son extrémité, l'autre extrémité se soulève et fait monter le sautereau muni d'un bec qui va « pincer » la corde correspondante.

À l’extrémité supérieure du sautereau se trouve une petite languette de bois dur articulée de façon élastique (ressort en soie de sanglier) sur le sautereau et munie du « bec » ou « plectre » (en plume de corbeau, en cuir ou en plastique) qui soulève la corde. Lorsque le sautereau continue à s’élever, le bec se courbe progressivement puis finit par « lâcher » la corde ainsi mise en vibration. Le "chapiteau", barre de bois placée horizontalement au-dessus des rangées de sautereaux, limite leur déplacement vertical.

Lorsqu'on cesse d'appuyer sur la touche, la queue de la touche revient sur le châssis du clavier ; le sautereau retombe et le bec repasse sous la corde, mais sans bruit (ou presque) grâce à la conception de l'articulation de la languette et du sautereau : la languette s’escamote vers l’arrière, et revient ensuite à sa place grâce au ressort. Le sautereau redescend en position basse, l'étouffoir en drap de laine vient reposer sur la corde pour faire cesser le son.

Le son émis par une corde du clavecin ne dépend pratiquement pas de la force appliquée par le claveciniste sur la touche. Néanmoins, l'instrument possède une variété de sonorités obtenue grâce à ses différents jeux (ou rangs de cordes) sélectionnés à l'aide des registres, et par la possibilité de les combiner. La sonorité du clavecin est marquée par une grande richesse en harmoniques.

Aujourd'hui on désigne le plus souvent les jeux de cordes par un nombre de pieds, par analogie avec les registres d'orgue émettant des sons de même hauteur, soit (très rare), (le plus usuel), voire (très rare), en abrégé 16', 8', 4' et 2'. Ce nombre n'indique pas la longueur effective des cordes. En France, les jeux peuvent prendre les noms de jeu principal (premier 8'), unisson (second 8'), octave (4').

On appelle « disposition » l'affectation des différents registres et jeux de cordes au(x) clavier(s).

La disposition d'un clavecin à clavier unique peut être à un seul jeu, à deux jeux (principal et unisson : ' ou principal et octave : , ).

La disposition la plus ordinaire d'un clavecin à deux claviers comporte trois rangs de cordes : 8' et 4' actionnés par le clavier inférieur (« grand clavier »), unisson (second 8') actionné par le clavier supérieur (« petit clavier »), soit , .

En combinant le son du principal et de l'unisson, non seulement on augmente la puissance sonore, mais aussi on joue sur le timbre. On peut aussi combiner les trois jeux, c'est le « plein-jeu ».

Les deux jeux de 8' sont accordés à la hauteur normale et le jeu de 4' (cordes de demi-longueur) à l'octave supérieure. En principe, le jeu de 4' n'est pas utilisé seul, mais combiné à l'un des jeux de 8' pour donner un timbre différent en ajoutant des harmoniques, ainsi qu'une puissance supérieure.

Les deux jeux de 8' eux-mêmes ne rendent pas exactement le même son, le spectre harmonique dépend en effet de la position du « point de pincement » ou distance séparant le bec du sautereau et le sillet. Le schéma ci-dessus montre que si deux registres se partagent le même sillet, les points de pincements sont séparés de quelques dizaines de mm.

Dans certains instruments (spécialement flamands et anglais), un rang de sautereaux traverse le sommier, évidé à cet effet, pour rapprocher au maximum le point de pincement du sillet : ce jeu s'appelle « nasal » (en anglais : "lute stop"). Il peut partager le même rang de cordes qu'un unisson « normal ».

La sonorité peut être affectée par la matière du plectre, et notamment par l'emploi de la « peau de buffle » qui fut introduit par les facteurs parisiens du en jeu complémentaire au principal, sans parler des becs en cuir durci des clavecins du début du .

Certains dispositifs peuvent altérer le son en venant appliquer un dispositif supplémentaire sur la corde, tout près du sillet : feutre ou cuir pour le « jeu de luth » (en anglais : "buff stop", à ne pas confondre avec "lute stop", cf. supra), métal pour l'arpichordum.

Enfin, un dispositif de couvercle à persiennes orientables commandé par des pédales fut employé en Angleterre vers la fin du , permettant des effets de crescendo/decrescendo d'ailleurs accompagnés d'effets d'ouverture/étouffement du son.

Les registres permettent de mettre en action (ou hors action) les différents jeux disponibles.
À chaque registre correspond un jeu de sautereaux. Chaque registre peut être poussé ou tiré, afin de mettre en service ou hors service le jeu de sautereaux correspondants. Les registres se commandent par des manettes ou leviers situés à portée de main au-dessus du clavier. Chez les Flamands, l'extrémité des registres peut traverser la joue.

Le schéma ci-contre montre le fonctionnement d'une paire de registres : le registre inférieur est fixe ; le registre supérieur est mobile ; en se déplaçant longitudinalement (moins de ) il permet aux plectres de la rangée de sautereaux d'être dégagés des cordes ou de se placer au-dessous, afin de les pincer lorsque les touches correspondantes sont enfoncées.

Lorsqu'il n'y a qu'une paire de registres, les deux sont fixes. Les clavecins de facture italienne ont des registres épais (qui ne vont pas par paires) suffisants pour guider et maintenir les sautereaux dans leurs mortaises.

Il n'y a pas autant de jeux que de rangs de cordes :

Toutes ces combinaisons donnent aux instruments les plus complexes une grande variété de timbres. C'est pour les modifier rapidement que furent mis au point, au , des dispositifs particuliers, "genouillères" par les facteurs français et "pédales" par les Anglais.

Lorsque l'instrument possède deux claviers, deux dispositifs différents permettent d’actionner le même sautereau à partir des deux claviers :

Ces deux dispositifs, dont l’utilité musicale est différente, n’apparaissent en principe jamais simultanément sur le même instrument.

Très généralement aujourd'hui, le clavecin est accordé au diapason dit baroque avec un « la » à . Le diapason moderne à n'est adopté que lorsque l'instrument doit s'intégrer à un ensemble moderne, notamment pour l'exécution des œuvres composées au .

L'écart entre ces deux diapasons correspond approximativement à un demi-ton : c'est pourquoi beaucoup d'instruments disposent d'un dispositif transpositeur rudimentaire qui consiste simplement à pouvoir décaler les claviers de la largeur d'une touche : décalée vers la droite, la touche donnant « la » vient se placer sous les sautereaux produisant auparavant « si bémol » ce qui demande d'effectuer un nouvel accord.

De façon générale, de nos jours, le clavecin n'est pas accordé au tempérament égal, sauf lorsqu'il doit s'insérer dans un ensemble moderne. Ce sont donc principalement les tempéraments en usage pendant les qui sont utilisés : tempéraments mésotoniques et tempéraments inégaux ; en effet l'accord est beaucoup plus harmonique pour le clavecin. Ce respect de l'instrument et des styles musicaux est devenu la règle dans l'interprétation authentique du répertoire baroque.

La stabilité de l'accord du clavecin est particulièrement sensible aux variations des conditions atmosphériques auxquelles réagissent sa structure en bois (variations d'hygrométrie principalement) et ses cordes métalliques (variations de température exclusivement). Il doit donc être réaccordé assez fréquemment ; le claveciniste procède lui-même à cette opération, à l'oreille (écoute et interprétation des battements) ou en s'aidant d'un accordeur électronique chromatique. Le réglage de l'accordage est en fait celui de la tension des cordes, obtenu en tournant les chevilles d'accord dans un sens ou dans l'autre à l'aide d'un té ou d'une clef adaptés d'accordeur, jusqu'à obtenir la hauteur désirée.

La table d'harmonie n'est pratiquement pas vernie et offre une surface d'un mètre carré extrêmement poreuse à l'atmosphère ambiante : la fermeture du couvercle, hors utilisation, est indispensable.

Le clavecin craint les climats sujets à de très fortes variations de température et d'hygrométrie. Relativement peu affecté par les climats d'Europe de l'Ouest, il l'est de façon beaucoup plus marquée par ceux que l'on trouve en Amérique du Nord : dans ces régions, les grandes variations d'hygrométrie peuvent provoquer des dommages irrémédiables aux instruments, quels qu'ils soient. Lorsqu'un instrument doit être installé pour un concert dans un lieu étranger (par exemple une église, souvent froide et humide), il est donc nécessaire de l'y placer plusieurs heures à l'avance et de rectifier l'accord, en plusieurs phases et notamment à l'approche du concert. Une légère adaptation peut même intervenir au cours de celui-ci, pendant entractes et pauses.

Avant le retour à la facture traditionnelle opéré à partir des années 1950, les facteurs ont produit des instruments différant sensiblement des clavecins historiques et reprenant de nombreuses caractéristiques des pianos :
Ces instruments étaient beaucoup plus lourds et produisaient un son plus métallique et plus grêle, qu'on leur reproche actuellement. Ils se prêtaient mieux à une production industrielle et furent d'ailleurs construits en grand nombre, notamment par quelques firmes allemandes telles que Neupert, Wittmayer, Sperrhake, Ammer, Sassmann.

Les seules innovations du qui aient été communément conservées aujourd'hui sont les sautereaux en plastique et les plectres en Delrin.

Outre le grand clavecin, plusieurs instruments sont basés sur le mécanisme du sautereau. Ils se distinguent par leur structure, leur forme, leur taille, leur timbre et peuvent continuer de nos jours à être fabriqués par les facteurs.

Ces instruments sont, pour la plupart, plus archaïques, munis d'un seul clavier et d'un seul rang de cordes, avec une étendue moindre (quatre octaves ou un peu plus suffisent pour la musique de la Renaissance et des dispositifs anciens tels que octaves « courtes » ou touches dédoublées (dites feintes brisées).

Leur nom dépend plus ou moins de la direction des cordes relativement au clavier. Néanmoins, la terminologie est souvent imprécise et variable géographiquement, notamment pour la distinction entre épinette, virginal et clavecin : 
Le clavicythérium ou clavecin vertical a ses cordes disposées verticalement. Les sautereaux y sont donc placés à l'horizontale et un système de renvoi est nécessaire pour les relier au clavier. Cet instrument assez rare a pour avantages une place au sol réduite (contrepartie d'une hauteur importante) et un son très proche de l'interprète.

L'épinette peut être de formes variées : rectangulaire, polygonale, en forme d'aile d'oiseau (épinette courbe) ; les cordes sont plus ou moins inclinées par rapport au clavier – les plus graves étant les plus éloignées. Le nom était interchangeable avec celui de clavecin en France au .

Le virginal a une forme rectangulaire ou polygonale, les cordes y sont parallèles au clavier – les plus graves étant les plus proches. Le clavier est situé à gauche du coffre. 

Le muselaar est un virginal dont le clavier est situé au centre de la caisse. Les sautereaux y pincent les cordes en leur milieu, d'où une sonorité particulière.

L'ottavino est une épinette rectangulaire, voire triangulaire, de taille réduite sonnant « à l'octave » grâce à ses cordes à moitié plus courtes.

Le clavecin à pédale est un grand clavecin auquel était adjoint un pédalier à touches. L'instrument permettait aux organistes de s'entraîner à moindres frais.

Le claviorganum était un instrument hybride intégrant un double mécanisme, les cordes d'un clavecin et les tuyaux d'un orgue.

Le luth-clavecin était équipé de cordes en boyau.

L'origine du clavecin remonte au Moyen Âge : il est une évolution du psaltérion, auquel a été adapté un clavier. C'est au que l'on en trouve les plus anciens documents. Un manuscrit en latin d'Arnaut de Zwolle, datant d'environ 1440, inclut des schémas détaillés du "clavicymbalum" (ancien nom du clavecin) ainsi que de quatre dispositifs d'excitation des cordes, soit pincées, soit frappées. Arnault précise que la première mécanique est la meilleure : les premiers sautereaux.

L'instrument est peut-être originaire d'Italie ou de Bourgogne, que ces deux centres de facture aient été en communication ou se soient développés de façon indépendante. L’Italie sera toujours, et de loin, le siège de la plus importante production, avec une facture très typée qui demeure la même pendant trois siècles.

En 2009, le clavecin le plus ancien qui est conservé est de facture italienne ; il est daté de 1521, a été construit par le facteur Jérôme de Bologne ("Hieronymus Bononiensis") et est conservé à Londres au "Victoria and Albert Museum". La "Royal Academy of Music" possède un clavicythérium, qui doit être antérieur, mais le mécanisme est manquant. Notons qu'ils sont précédés par une épinette datant de 1493, exposée au musée de Pérouse. Les autres instruments remontant à cette haute époque, la première moitié du , sont également de facture italienne. Cependant, ils ne nous fournissent aucun renseignement sur la genèse de l'instrument : de manière surprenante, la facture en est quasi accomplie, et l'on ne peut donc qu'émettre des hypothèses quant à son évolution antérieure, d’autant que les documents écrits manquent presque complètement jusqu'à l'Encyclopédie de Diderot.

Les facteurs italiens construisaient des instruments très légers, dont la structure évoque la lutherie, munis d'un seul clavier et des cordes de tension modérée. Cette structure perdura pendant plusieurs siècles sans modification notoire. Les instruments italiens ont un son plaisant, mais qui manque de puissance : ils devinrent l'instrument d'accompagnement par excellence.

Un changement décisif dans la facture eut lieu à Anvers vers les années 1580, surtout sur l'impulsion du facteur Hans Ruckers et de ses héritiers, parmi lesquels Ioannes Couchet. Les facteurs flamands construisaient des instruments beaucoup plus solides que les Italiens. Leurs instruments avaient des cordes plus longues, sous plus forte tension, de diamètre progressivement augmenté vers les basses – d'où une forme plus trapue, avec deux jeux de 8 et (ou de deux fois ) –, une caisse plus épaisse et une table d'harmonie très mince qui rendait un son puissant et noble. Il y avait parfois deux claviers, généralement transpositeurs (à intervalle de quarte) et qui ne pouvaient être accouplés. Plus tard, le second clavier fut aussi utilisé pour produire des modifications de sonorité. Le modèle flamand servit de base au développement de la facture dans les autres pays d'Europe occidentale (essentiellement la France, l'Angleterre, l'Allemagne), même si une tradition antérieure a pu y exister.

C'est la grande époque du clavecin : facteurs et musiciens portent l'instrument et son répertoire à leur apogée. Le clavecin devient un instrument de prestige et un meuble d'apparat qui orne hôtels particuliers, châteaux et palais, chez les bourgeois aisés, les membres de la noblesse et des familles royales. Il participe à la riche vie musicale qui les anime, et les enfants des classes privilégiées apprennent à en jouer auprès des meilleurs professeurs.

En France, la Révolution de 1789 porte un coup fatal à l'un des instruments les plus prestigieux de l'époque baroque : il est considéré comme un symbole de l'Ancien Régime et son sort se lie à celui de la monarchie. Les biens des condamnés et des immigrés sont confisqués, autant les clavecins que les piano-fortes (il est attesté qu'une douzaine de clavecins furent brûlés dans la cour du Conservatoire de Paris). Une anecdote précise qu'à la mort de Louis les instruments furent peints en noir en signe de deuil. Le clavecin est considéré comme un instrument démodé, souvent relégué comme objet décoratif.

Le clavecin pour autant ne disparaît pas totalement et il reste en usage jusqu'au début du en Italie, en Angleterre, en même temps que se développe le piano-forte. Son effacement rapide coïncide avec les changements esthétiques, politiques et philosophiques de la fin du , le passage à la période dite « classique » (celle de Haydn, de Mozart, de Beethoven) et surtout avec la naissance du romantisme. Pendant l'époque romantique, correspondant au développement de l'ère industrielle, les compositeurs n'écrivent plus pour lui. Les derniers clavecins « à l'antique » sont construits par Kirkman à Londres au tout début du .

À la fin du , les facteurs sont tentés de lui appliquer les techniques des manufactures de piano, mais sans réel succès. L'instrument historique retrouve progressivement son usage grâce aux facteurs et aux musicologues dans le courant du .

Il faut attendre 1889 (Exposition universelle de Paris) pour voir réapparaître en public le clavecin, via des instruments fabriqués par Pleyel et Érard et présentant des caractéristiques très différentes du clavecin ancien.

Au début du , la pianiste virtuose Wanda Landowska découvre le clavecin et va œuvrer pour le renouveau de l’instrument en s’y consacrant de façon exclusive : elle interprète sur un instrument spécialement conçu et construit pour elle par Pleyel les œuvres de Bach, Couperin, Rameau, Scarlatti, … et forme de nombreux disciples. Parmi ceux-ci, Ralph Kirkpatrick, Rafael Puyana, Ruggero Gerlin entre autres, deviendront à leur tour des professeurs renommés.

La tradition de la facture classique s'est perdue depuis le : les facteurs de pianos sollicités par les artistes désireux de ressusciter l’instrument croient bon de faire « bénéficier » celui-ci des améliorations qui avaient transformé le piano-forte en piano moderne. Ils le munissent donc d’une caisse massive en contreplaqué, d’un cadre métallique avec des cordes sous forte tension, de becs en cuir durci, de dispositifs de réglage fin sous la forme de multiples vis d’ajustement, de clefs d'accord doublées, de pédales permettant les changements rapides de registre, sans soupçonner que ces nouveautés en font un instrument différent et moins convaincant sur le plan musical : le son en est différent, beaucoup plus métallique, et, chose plus inattendue, d’une faiblesse qui surprend et qui pouvait justifier "a posteriori" l’abandon de l’instrument à la fin de l’époque baroque. Le clavecin ainsi « falsifié » ne supporte guère la comparaison avec le piano ni la confrontation avec l'orchestre.

La puissance sonore du clavecin, en effet, comme celle de la guitare ou de la harpe, le rend incapable de lutter à égalité avec un orchestre moderne, où pour chaque pupitre on multiplie le nombre d'instruments. Sa sonorité délicate le marie admirablement aux formations instrumentales beaucoup plus réduites de l’époque baroque, au cours de laquelle a été constitué l’essentiel de son répertoire.

Les compositeurs redécouvrent pourtant l’instrument et composent pour lui : de Falla (le "Concerto pour clavecin") ou Poulenc ("Concert champêtre pour clavecin et orchestre").

Dès les années 1950, le clavecin suscite à nouveau la curiosité et l'intérêt du monde musical.

En France, la firme Erato sous l'impulsion de son directeur artistique Michel Garcin va lancer dès 1958 une encyclopédie du clavecin. Il engage de jeunes virtuoses tels Robert Veyron-Lacroix, auteur de l'enregistrement de quantités de pages de musique de chambre et de concertos avec orchestre, Zuzana Růžičková, qui va au cours de la décennie 1960 enregistrer la première intégrale des œuvres pour clavecin de J.S. Bach en 21 disques, et Laurence Boulay, à qui l'on doit de nombreuses recherches sur l'instrument en relation avec la maison Mercier-Ythier.

Aux Pays-Bas, Gustav Leonhardt, organiste, claveciniste et pédagogue, va susciter une nouvelle vague dans l’approche, la lecture et l’interprétation de la musique ancienne. Il va, avec ses nombreux élèves, ses disciples, et bien au-delà du seul clavecin, susciter un engouement pour les techniques anciennes d’interprétations et de facture.

Un retour progressif à la facture authentique s’opère en effet après la Seconde Guerre mondiale, sous l’impulsion de quelques facteurs enthousiastes, en particulier Hugh Gough en Angleterre, Frank Hubbard, William Dowd ainsi que Bruce Kennedy aux États-Unis, Martin Skowroneck en Allemagne, et en France, dans les années 1960, David Boinnard, Claude Mercier-Ythier, Jean-Paul Rouaud, Reinhard von Nagel, Marc Ducornet, Émile Jobin, Philippe Humeau, Patrick Chevalier ou encore Laurent Soumagnac. Ils admirent les facteurs de la grande époque qui avaient mis au point un instrument presque parfait, qu’il suffit de reproduire pour retrouver les qualités sonores historiques. Ce retour à une facture qui ne trahit pas l'instrument est parallèle à la redécouverte des techniques de jeu de l'époque baroque, par l’étude des traités des , et manifeste une recherche d’authenticité qui n’est d’ailleurs pas spécifique au clavecin.

Aujourd’hui, de nombreux facteurs produisent des clavecins « à l’ancienne » de grande qualité, tandis que d'autres fabricants proposent des instruments à monter soi-même permettant à l’amateur passionné de le construire à partir d'un kit.
Le clavecin a retrouvé depuis le dernier quart du sa place privilégiée dans l’interprétation de la musique baroque, comme instrument soliste, concertant ou assurant le continuo. Il a pratiquement repris sa place jusqu'alors usurpée par le piano dans ce domaine, car ses caractéristiques sont beaucoup plus appropriées à l’interprétation d’une musique qui a été conçue pour lui. Ainsi, seul le double clavier permet d’exécuter exactement certaines œuvres contrapuntiques, mais surtout, les ornements ne peuvent être exécutés correctement que grâce au mécanisme du clavecin. D'une manière générale, la couleur, le jeu spatial et la répartition des voix sont uniques au clavecin.

Des compositeurs contemporains tels que Francis Poulenc, Bohuslav Martinů, Manuel de Falla, Frank Martin, Peter Mieg, Maurice Ohana, Jean Françaix, György Ligeti, Iannis Xenakis ou Henryk Górecki vont composer pour l'instrument. Il sera également employé dans le domaine de la chanson française (Léo Ferré, Jacques Brel, Jean Ferrat, ...) et dans le jazz (Erroll Garner), mais aussi par des compositeurs de musiques de film (Vangelis dans "" (Ridley Scott, 1992).

L'histoire de la musique composée pour le clavecin s'écrit en deux pages bien distinctes, la pratique de cet instrument ayant cessé pendant tout le : la musique composée pour le piano pendant cette période ne peut pas être interprétée convenablement sur le clavecin car elle mise sur des caractéristiques musicales propres au piano, notamment les possibilités expressives et la puissance sonore. Il y a donc une discontinuité entre la première période (Renaissance et baroque, soit près de trois siècles) et la seconde (période contemporaine), au cours de laquelle le clavecin a retrouvé la faveur de certains compositeurs.

Pendant la Renaissance, les tablatures concernent indifféremment l'orgue et le clavecin, et ce dans toute l'Europe occidentale, malgré les caractères opposés des deux instruments. Ce n'est que progressivement que se développe un répertoire propre à chacun d'eux, à des périodes qui diffèrent selon les pays : le clavecin tend alors à se spécialiser dans la musique profane, et l'orgue dans la musique sacrée.

Pendant la période baroque, le clavecin trouve son utilisation dans trois domaines :

Chacune des nations qui participent à l'essor du répertoire apporte une composante à une synthèse européenne qui aboutira, au , à un apogée précédant de peu la disparition rapide de l'instrument, supplanté par le piano.

À la suite de Frank Hubbard et Raymond Russell on distingue habituellement cinq « écoles » nationales pour la facture ancienne :

Il s’est produit peu de clavecins en dehors de ces régions. Quelques instruments ibériques (portugais, espagnols) subsistent, qui dénotent une forte influence italienne, mais la production a toujours été très réduite. En Scandinavie, c'est la tradition hambourgeoise qui prévaut, et en Europe orientale, Suisse, Autriche celle de l'Allemagne méridionale. L'influence anglaise se ressent quant à elle en Irlande ainsi qu'en Amérique du Nord (ou sont construits quelques instruments vers la fin du ).

Les cinq principales écoles nationales sont bien caractérisées, même si de nombreuses variantes existent et que les facteurs les plus talentueux ont toujours fait preuve d’imagination et d’ingéniosité. Les caractères indiqués ci-après ne décrivent que des tendances générales, qui sont sujettes à de nombreuses exceptions.
Du , l'Italie a été le plus important centre de fabrication de clavecins de toute l'Europe. Les instruments en sont très caractérisés ; leur son typé et leur attaque précise les destinent tout particulièrement à la réalisation de la basse continue. D'ailleurs, relativement peu d'artistes italiens se sont consacrés exclusivement à cet instrument, en proportion du nombre de musiciens italiens pendant toute la période baroque.

Le clavecin italien le plus typique a une caisse extrêmement légère formée de parois minces en cyprès, dont la construction évoque la lutherie. Cet instrument fragile n’est pas décoré ; il est contenu dans une caisse extérieure solide, de caractère utilitaire à l’origine, et qui prendra au cours des temps une fonction décorative de plus en plus importante. Les instruments plus tardifs ont souvent des parois plus épaisses, mais ils s’efforcent de conserver la même apparence de structure. La table d'harmonie est découpée à la forme du fond, que les éclisses entourent, et à partir duquel on construit l’instrument.

Très généralement, il n’y a qu’un seul clavier, actionnant deux jeux à l'unisson, les registres n’étant pas mobiles ; ils sont parfois disposés obliquement (non parallèles au clavier) ce qui nécessite un processus de fabrication très sophistiqué. Ce clavier est proéminent (en console) par rapport à la caisse. La mesure des cordes (en laiton) est courte, et la progression de leurs longueurs vers le grave sans variation de diamètre donne à l’instrument une forme très allongée, avec une éclisse courbe très incurvée. C’est dans la partie la plus grave que les diamètres de cordes sont augmentés, ce qui s’accompagne d’une pointe presque perpendiculaire à l’échine. Dans cette partie, le ou les chevalets ne sont pas courbes, mais anguleux.

En ce qui concerne la décoration :

La facture flamande représente le pôle opposé à la facture italienne. Les autres écoles nationales (France, Allemagne et Angleterre) se rattachent à sa tradition avec des variations locales. Le plus grand nombre d’instruments conservés vient de la famille Ruckers, dont la production représente l’archétype de cette école.

Contrairement au clavecin italien, le clavecin flamand est un instrument solide, aux parois relativement épaisses ; la rigidité de la caisse est assurée, en outre, par des renforts internes disposés en éventail, plus ou moins perpendiculairement à l’éclisse courbe. Les Flamands emploient des bois tels que le tilleul ou le peuplier. Les éclisses sont posées sur le fond (donc la table d’harmonie n’a pas la même surface que ce dernier), mais il est probable qu’en fait, on fixait le fond (par collage et cloutage), une fois l’instrument terminé, sur la tranche inférieure des éclisses.

Le plus souvent, il n’y a qu’un seul clavier, actionnant deux jeux à l'unisson ou un jeu principal et un d'octave, avec registres mobiles disposés parallèlement au clavier. Ce clavier est rentrant dans la caisse. La mesure des cordes (en acier ou laiton) est assez longue, et la progression de leurs longueurs vers le grave est accompagnée d’une variation de diamètre qui permet d’avoir un instrument assez compact, avec une éclisse courbe peu incurvée. Les chevalets sont incurvés en forme de « S ». Les instruments à deux claviers en état d’origine sont transpositeurs (claviers décalés, un en Ut, l'autre en Fa, et surtout sans accouplement).

La table d’harmonie a une épaisseur variable sur les bords, et un barrage assez rigide.

Le piétement est l’extrapolation de simples tréteaux : quatre pieds en chêne tourné et ciré, reliés par des traverses horizontales ou une balustrade. Il était parfois étonnamment haut, car l’on pouvait jouer debout.

En ce qui concerne la décoration :

Il existe une tradition française de la facture antérieure à la période d’extraordinaire engouement pour les clavecins flamands qui la fit évoluer de façon décisive. Cette manière ancienne est d’ailleurs beaucoup plus proche des Flamands que des Italiens. La production française est presque entièrement concentrée à Paris, qui comptait plus de cent facteurs au - on peut citer les familles Denis, Bellot, Jean-Antoine Vaudry. Quelques autres travaillent à Lyon (Gilbert Desruisseaux, plus tard Pierre Donzelague), Toulouse (Vincent Thibaut) — Les instruments datant de cette période sont excessivement rares et aucun ne remonte avant 1648.

Le clavecin français typique du est un grand instrument à deux claviers dont la structure rappelle beaucoup celle des Flamands. Bien souvent, ces instruments sont issus de l’opération de ravalement qui consiste à transformer un ancien instrument pour le mettre au goût du jour. Il s’agit soit d’adjoindre un second clavier ou d’ajouter un accouplement, soit d’augmenter l’étendue du clavier, soit d’augmenter le nombre des registres, et éventuellement d’ajouter des dispositifs de changement rapide.
Les grands facteurs parisiens du se sont fait une spécialité du ravalement de clavecins flamands, particulièrement des Rückers. En effet cette signature était synonyme de qualité sonore exceptionnelle et de prix exorbitant : elle donna lieu aussi à des contrefaçons.

La décoration, qui s'harmonise au reste du mobilier, est somptueuse, avec dorures, sculptures, peintures ; la table d’harmonie possède une décoration florale raffinée. L’intérieur du couvercle est souvent un tableau pour lequel on fait appel aux meilleurs peintres.

La recherche d’expressivité a donné lieu, au , à des innovations telles que les genouillères pour changement rapide des registres, le plectre en peau de buffle, … .

Malgré la production importante, il subsiste relativement peu d’instruments français de la grande époque, à cause des destructions consécutives à la Révolution. Il n'existe plus, pour certains facteurs, qu'un seul instrument, et nombreux sont ceux dont toute la production a disparu.

Les grands noms de la facture française :

Les pays allemands n’ont pas été des centres de production importants. Il subsiste peu d’instruments anciens, et ceux-ci présentent une grande diversité. Les influences flamande et française sont très fortes.

Les instruments fabriqués en Allemagne méridionale sont d’aspect moins élaboré que ceux d’Allemagne du nord, parmi lesquels se distinguent particulièrement ceux du facteur hambourgeois Hieronymus Hass. Celui-ci a réalisé des instruments avec les jeux, rares, de deux pieds et de seize pieds. Son œuvre la plus exceptionnelle de complexité est un clavecin vraiment unique à trois claviers, cinq jeux de cordes et deux tables d’harmonie. Autres facteurs hambourgeois de renom : les Fleischer, Christian Zell. La Saxe possède aussi des facteurs célèbres : les Gräbner, Michael Mietke, Gottfried Silbermann (également fameux facteur d'orgues), ... .

Les clavecins hambourgeois présentent souvent une éclisse doublement courbée dont la queue fait partie intégrante : cette forme est celle des instruments du début du ; elle ne se retrouve pas en Saxe ou en Allemagne méridionale, dont les instruments apparaissent moins massifs.

La facture anglaise est influencée par celle de Flandre. Il y a une production importante de virginals au , et d’épinettes courbes « bentside spinet » dont la taille inférieure à celle du grand clavecin a favorisé la diffusion.

Un des traits distinctifs de cette école est l’utilisation, pour la caisse du chêne, plaqué de noyer puis d’acajou. Les facteurs les plus significatifs sont d’ailleurs des menuisiers et ébénistes d’origine continentale, Kirkman (alsace) et Shudi - Tschudi - (Suisse alémanique). Le jeu de nasal était préféré au jeu d'octave. Dans les deux claviers, c'est le supérieur qui se déplace pour l'accouplement, il vient s'engager dans un cran réalisé dans le sautereau dit en pied-de-biche . La table d’harmonie, dont l’épaisseur croît légèrement de l’aigu vers le grave, comporte une rosace, mais n’est presque jamais décorée. Le piètement est des plus simples, souvent des pieds de section carrée.

Les facteurs anglais autour de la fin du ont multiplié les dispositifs permettant d’agir sur l’expressivité : pédales pour le changement de registres, « machine-stop » permettant de préparer un changement complet et rapide du registre, volets vénitiens (1769) placés au-dessus de la table d’harmonie pour modifier le volume sonore. De 1790 à 1825, la maison Broadwood, successeur de Shudi, passa insensiblement de la production de clavecins à celle de piano-forte.

Le clavecin est un instrument aussi coûteux qu’un piano, une harpe, une contrebasse ou un orgue-coffre. Cependant il offre une large gamme d'instruments de tailles et de prix variés. Étant donné leur poids (30 à ), le clavecin et l'épinette font partie des instruments que le joueur de musique baroque peut éventuellement emporter avec lui lorsqu'il participe à un concert, au contraire des pianos, des orgues, des timbales. Bien qu’il se déplace facilement à deux personnes, le clavecin est souvent loué, ou bien l'institution qui reçoit en possède déjà plusieurs.

À l'heure actuelle, malgré une forte demande, la fabrication de clavecins demeure une activité largement artisanale (à l'instar de la lutherie en violon), qui n'est pas organisée en un véritable secteur économique. Ainsi, il n'existe aucune production industrielle en France depuis le . Cependant des avatars du piano furent manufacturés en Allemagne (1920-1960). On estime toutefois que la facture de l'instrument peut offrir des opportunités de niche prestigieuses et porteuses d'images pour le savoir-faire du pays.

« Le clavecin a dans son espèce un brillant et une netteté qu'on ne trouve guère dans les autres instruments. Il est parfait quant à son étendue par lui-même… . Cet instrument a ses propriétés comme le violon a les siennes. Si le clavecin n'enfle point les sons, si les battements redoublés sur une même note ne lui conviennent pas extrêmement, il a d'autres avantages qui sont : la "précision", la "netteté", le "brillant" et l'"étendue" ». ("Art de toucher le clavecin", François Couperin, 1717).

« Ces couplets sont assez bons… pour un piano-forte qui n'est qu'un instrument de chaudronnier en comparaison du clavecin. » ("Correspondance avec la marquise Mme du Deffand", Voltaire, 8 décembre 1774).

Le grand chef d'orchestre britannique Sir Thomas Beecham n'aimait pas le clavecin. Il en comparait le son à celui de « squelettes copulant sur un toit en tôle ondulée » ("« skeletons copulating on a corrugated iron roof »") ou à celui d'une « cage à oiseaux jouée à l'aide de fourchettes à rôtir » ("« a birdcage played with toasting forks »") !

« Schwermütig denkt die Gambe ihren Traum,Die Flöte singt das Sehnen und das Irren,aber das Cembalo mit zartem Klirrenstreut Sterne in den leeren Raum. »(Manfred Hausmann, extrait du poème "Alte Musik")





</doc>
<doc id="17973" url="https://fr.wikipedia.org/wiki?curid=17973" title="Hautbois">
Hautbois

Le hautbois est un instrument de musique à vent de la famille des bois, de perce conique et dont le son est créé par la vibration d'une anche double au passage du souffle. Son timbre peut être puissant et sonore ou doux et charmeur, clair ou plein de rondeur et de chaleur. Le joueur de cet instrument est un hautboïste.

Connu dès l'antiquité, l'instrument a évolué dans l'espace et dans le temps avec une diversité qui n'a d'égale que la créativité des civilisations et cultures dans lesquelles cet instrument est encore utilisé de nos jours. Les hautbois traditionnels (bombarde, cornemuse, duduk, gaïta, hichiriki et autre zurna) et les hautbois modernes (musette, hautbois, hautbois d'amour, cor anglais et hautbois baryton, hautbois baroque, hautbois classique) forment une grande famille aux multiples facettes.

Utilisé en solo, musique concertante, musique de chambre, orchestre symphonique ou bande de hautbois, le hautbois moderne désigne à l'orchestre l'ensemble de la famille. Selon Hector Berlioz ("Traité d'instrumentation") : 

Les œuvres pour hautbois sont essentiellement issues des répertoires baroque (Johann Sebastian Bach), et classique (Wolfgang Amadeus Mozart), puis du renouveau du (Robert Schumann) à nos jours (Nicolas Bacri).

C'est aussi le nom d'un des jeux d'anche de l'orgue, et également le nom du tuyau sur lequel la mélodie est jouée à la cornemuse.

De nombreux vestiges offrent des représentations des mizmars égyptiens, de l'aulos grec, simple ou double, signalé par Homère dans l"'Iliade" : « Et l'on entend sur Thèbes en flammes le son des auloi », ou des tibiae romaines en roseau.

Les zurnas ou les zurlas se jouent encore aujourd'hui de la Turquie à la Tunisie ou en Macédoine ; les doudouks sont d'origine arménienne, les toroksips et les tárogatós de Hongrie, les surnajs de Russie ; les alghaitas se retrouvent dans toute l'Afrique et même jusqu'en Birmanie ; les shenaïs sont utilisés dans la musique traditionnelle du nord de l'Inde, les nagasvarams plus au sud ; la Thaïlande a ses pinais ; en Chine, ce sont les suonas et au Japon les hichirikis. À Cuba, la trompette chinoise est une sorte de suona chinois jouée durant les carnavals. En Bretagne (France), la bombarde est traditionnellement sonnée en couple avec la cornemuse bretonne, le biniou. Dans les Landes, le hautbois traditionnel est appelé "tchalemine" mais il n'est pratiquement plus joué; il en va de même pour le "clarin" pyrénéen. Le graïle est quant à lui toujours bien vivant en Languedoc et accompagne notamment les joutes nautiques de Sète.

À partir du , l'observation des enluminures des manuscrits monastiques, des tapisseries, des sculptures et des tableaux où les représentations des différents hautbois ne manquent pas, donne une idée assez précise des instruments joués suivant les circonstances et les périodes (les musettes du Cantigas de Santa Maria par exemple).

Déclinée en consort (dessus, haute-contre, taille, basse…) la chalemie, appelée aussi hautbois ancien, tournée d'une seule pièce, de perce large surtout au pavillon, donnera naissance aux discants, aux cromornes, aux ciaramellas ou aux pifferi italiens, aux dulzainas ou aux gralles catalanes, mais aussi aux bombardes, hautbois du Poitou ou autres hautbois du Languedoc… Le hautbois est également la partie de la cornemuse, du biniou, de la veuze ou de la musette de cour jouant la mélodie.

En France, les chalemies et les cromornes font partie de l'univers musical de la cour des rois jusqu'aux fêtes de hameau ; le hautbois du Poitou distrait les soirées du roi Louis XI, les musettes font danser les paysans.

À partir de 1650, les familles Hotteterre et Philidor, facteurs d'instruments, compositeurs, musiciens virtuoses, membres de la "Musique de la Chambre & de la Grande Écurie du Roy", vont faire évoluer l'instrument, le divisant en trois parties (corps du haut, corps du bas et pavillon), affinant la perce, ajustant le trou des notes, ajoutant une clé de do grave en forme de W (permettant l'alternance de la position des mains) et une clé de miformula_1. Abandonnant définitivement les et les , ils imposent le contrôle de l'anche par les lèvres pour exprimer toutes les finesses du son (différence révolutionnaire avec tous les autres instruments de la famille). Ils sont considérés comme les créateurs du hautbois baroque.

En 1664, Jean-Baptiste Lully, surintendant de la Cour, écrit une marche pour ces nouveaux hautbois, les intègre à « La Grande Écurie du Roy » de Louis XIV, institution datant de François, supprimant progressivement les pupitres des instruments plus anciens, comme les cromornes. Déclinés en plusieurs tailles, ils font également leur entrée dans la musique des mousquetaires et dès lors, avec les bassons, prennent leur essor dans l'Europe entière. Si les bandes de hautbois (surtout militaires) sont appréciées, l'instrument s'impose surtout dans l'orchestre symphonique naissant, accompagnant les fêtes, les opéras, les ballets de cour, les oratorios, les cantates… Il triomphe également comme soliste, en sonates, dans les concerti et en musique de chambre.

De nombreux compositeurs de l'époque baroque vont écrire pour ces hautbois, hautbois d'amour, de chasse (), cors anglais, tailles de hautbois et hautbois barytons (plus rares, mais certains ayant déjà vers 1680, la forme du saxophone !). Le sera véritablement l'âge d'or du hautbois (voir le chapitre Répertoire).

Le hautbois de la période classique, du milieu du au début du , ne varie pas beaucoup par rapport à son prédécesseur. Pour simplifier les doigtés, particulièrement les et les trilles, pour augmenter la tessiture (jusqu'au contre-fa) avec des recherches très empiriques, les clés deviennent progressivement plus nombreuses (do# grave, fa, sol#, clé d'octave), mais globalement, la forme et la perce restent relativement les mêmes. Il n'est d'ailleurs pas rare que les clés soient rajoutées longtemps après la fabrication de l'instrument.

Au début du , la facture des instruments de la famille des bois subit une révolution fondamentale : Theobald Boehm invente pour la flûte traversière un système de clés et de plateaux pour boucher les différents trous. Le diamètre des trous ne dépend plus de la largeur des doigts et un plateau peut commander l'ouverture ou la fermeture de plusieurs trous. Un système de tringle pivotante, muni de ressorts plats ou en aiguille, permet d'actionner le bouchages des trous hors d'atteinte.

Pour le hautbois, après quelques tâtonnements, ce sont Guillaume Triébert et ses fils Charles-Louis (professeur de hautbois au Conservatoire de Paris) et Frédéric, qui adaptent, perfectionnent et font évoluer le mécanisme, repensant également la perce. Leurs successeurs, François et Lucien , fabriquent le modèle à plateaux qui sera rapidement adopté par tous les hautboïstes.

Le terme « hautbois » désigne tout instrument à anche double mise directement dans la bouche ou enfermée dans une « capsule » (tube recouvrant l'anche), excitée par le musicien ou par une poche d'air avec ou sans soufflet. Parfois, les lèvres s'appuient sur une pirouette – anneau de cuir, d'os ou de métal fixé au milieu du bocal (tube sur lequel est ligaturée l'anche). La perce est conique et l'instrument octavie quand le son est forcé (les instruments à perce cylindrique, ou clarinettes, à anche simple ou à anche double, quintoient). Le corps de l'instrument est en os, en corne, en roseau, en bois, en matière synthétique, plus rarement en ivoire ou en métal. Le timbre peut être extrêmement sonore ou au contraire très doux : une diversité qui n'a d'égale que la grande variété des instruments issus des différentes civilisations et des nombreuses cultures dans lesquelles il est encore utilisé actuellement.

Les premiers hautbois sont fabriqués en graminée (roseau, bambou…), utilisant le creux naturel du tuyau (voir le hichiriki de la musique gagaku japonaise). Même si certains instruments traditionnels actuels sont encore fabriqués dans ces matières éphémères, très vite la nécessité d'un matériau plus résistant et perdurant est devenue évidente. Les facteurs ont recherché les bois les plus durs, de grande densité, avec des fibres fines et régulières comme essentiellement le buis mais aussi le merisier (cerisier sauvage), le bois de rose (palissandre) ou le poirier. Certains hautbois baroques ont même été tournés en ivoire.

Au , l'ajout des clés et la multiplicité des trous a imposé le bois le plus résistant : l'ébène, plus précisément le bois de grenadille ou "Dalbergia melanoxylon". Actuellement, l'ébène domine encore, mais les bois exotiques comme le cocobolo ou le bois de violette apportent des nouvelles sonorités et sensations aux hautboïstes. Certains facteurs ont même construit des hautbois en métal ou en "altuglas" (polyméthacrylate de méthyle) une sorte de plexiglas (Marigaux). Dernière évolution technologique, Buffet Crampon fabrique ses instruments de la gamme « Green Line » sur la base du matériau composite le plus moderne, breveté, constitué de 95 % de poudre d’ébène, 5 % de fibres de carbone et de résine époxy.

La perce, de à la base du tube de l’anche passe à à la l’extrémité du corps du bas (soit sur ), puis s’évase à à la base du pavillon (sur ).

Quelque 23 trous, masqués par un clétage complexe fabriqué en maillechort (alliage de cuivre, de nickel et de zinc), façonné le plus souvent à la main, ajusté, poncé, limé, soudé, argenté ou même aurifié, des ressorts plats ou en aiguille, des tringles pivotantes fixés sur une cinquantaine de boules vissées dans le bois, 6 plateaux/anneaux et une vingtaine de clés/spatules pour ouvrir et fermer ces trous... Tout ce mécanisme permet la centaine de doigtés de notes, de trilles et de sons multiples possibles sur un hautbois moderne.

L'orchestre philharmonique de Vienne utilise un hautbois conçu au début du par Hermann Zuleger et demeuré sans changement notable jusqu'à présent. Il est caractérisé par une perce, un clétage et une anche particuliers qui lui donnent la couleur propre à cet orchestre. L'Akademiemodel est exclusivement utilisé à Vienne et diffère nettement du hautbois français employé partout ailleurs. Il n'est fabriqué que par de très rares facteurs comme Guntram Wolf ou Yamaha.



Voir la liste dans la page "Catégorie:Hautbois" en bas de page.

Une anche de hautbois est constituée de deux fines lamelles de roseau ligaturées sur un tube. C'est elle qui, sollicitée par le souffle, se met à vibrer et produit donc le son.
Le plus souvent fabriquées par les hautboïstes eux-mêmes, les anches doivent être adaptées au souffle (la vitesse et le volume d’air), à l’embouchure (formes des dents et des lèvres), à la pression de la mâchoire, à la température, à l’hygrométrie et même à la pression atmosphérique.

Le roseau, choisi pour ses fibres très fines et sa souplesse sans mollesse, est séché, coupé, fendu, gougé et taillé, plié pour être ligaturé sur un tube avec un fil de nylon. Commence alors l’opération délicate : le "grattage". Après avoir séparé les deux lamelles, il faut effiler ou raboter finement l’extrémité à l’aide d’un couteau ou d'un rasoir. Pour une bonne vibration, l’épaisseur et la forme de ce grattage doivent être précis. En France elle est communément grattée en U ou en W et montée à environ 

L’œuvre la plus célèbre du répertoire pour hautbois baroque est probablement le "Concerto en ré mineur" d'Alessandro Marcello longtemps attribué à son frère Benedetto, dont Johann Sebastian Bach a réalisé une transcription pour clavier.

Si le hautbois baroque apparaît au milieu du — généralement date de sa première prestation dans un orchestre du ballet "L’Amour malade" de Jean-Baptiste Lully, en 1657 — il faut attendre le début du siècle suivant pour voir réellement apparaître des œuvres composées spécifiquement pour le hautbois. Dans la musique française, beaucoup sont en réalité des œuvres pour un ou plusieurs instrument(s) de “dessus” (instrument aigu : violon, flûte traversière ou à bec, hautbois, dessus de viole, etc.) et la basse continue, comme les "Concerts royaux" et "Goûts réunis" de François Couperin.

Pierre Danican Philidor compose et fait publier trois livres des pièces dont chacun est en deux parties : d’abord des suites à deux flûtes, puis des suites à un instrument de dessus et le basse continue. L’ordre des mots dans le titre invite à penser que les suites pour dessus et basse ont été pensées pour le hautbois. En effet, les premières suites (deux ou trois, selon les livres) sont indiquées « à deux flûtes traversières seules », tandis que les dernières (également deux ou trois, selon les livres) sont « dessus et basse, pour les hautbois, flûtes, violons, etc. » À titre de comparaison, dans les titres de livres de Jacques Hotteterre, la flûte est toujours indiquée en premier. Cette disposition invite à penser que ceux de Pierre Danican Philidor ont été conçus pour le hautbois.

Joseph Bodin de Boismortier composera également spécifiquement pour le hautbois. Signalons enfin qu’une sonate d’Antoine Dornel, en "sol" majeur (dans les "Concerts de symphonies, IIIe livre"), est indiquée « pour le hautbois avec la basse », sans alternative instrumentale.

Le répertoire pour hautbois va beaucoup se développer en Italie — où Antonio Vivaldi composera plusieurs concertos pour un ou deux hautbois, et pour hautbois et basson — mais surtout dans l’aire germanique. Ainsi, les deux ou trois sonates pour hautbois du même Vivaldi — RV 53 en "do" mineur, RV 28 en "sol" mineur et RV 34 en "si" bémol majeur, dont l’attribution au hautbois est moins nette — semblent avoir été composées à Dresde.

Parmi les compositeurs qui ont beaucoup écrit pour le hautbois, il faut signaler Georg Philipp Telemann, compositeur de nombreuses sonates pour hautbois et basse continue, de concertos, d’une sonate pour hautbois et clavecin obligé, de sonates en trio, etc. Johann Sebastian Bach avait écrit un concerto pour hautbois et un concerto pour hautbois d’amour. Tous deux nous sont parvenus dans des versions remaniées ultérieurement par Bach pour clavecin et orchestre. À partir de ces versions, des reconstitutions des deux concertos originels — concerto pour hautbois en "fa" majeur, concerto pour hautbois d’amour en "la" majeur — ont été proposées et enregistrées. De nombreux solos de hautbois se trouvent également dans les passages instrumentaux (introductions ou "sinfonie", par exemple dans la BWV 12) et dans les arias des cantates et des Passions.

L’Angleterre a également prisé le hautbois. Outre les trois sonates et quelques solos dans des airs d’opéras et d’oratorios de Georg Friedrich Händel, plusieurs compositeurs, comme Thomas Vincent, William Babell et Filippo Prover, feront publier des recueils de sonates pour hautbois et basse. L’Italie, nous l’avons évoqué, n’est pas en reste, avec les œuvres de Vivaldi, mais aussi Tomaso Albinoni, Alessandro Marcello, Giuseppe Sammartini, etc.

Si Johann Sebastian Bach est le champion de la musique baroque pour l'instrument, Wolfgang Amadeus Mozart est le phénix de la période classique. Outre le "Concerto en ut" (qu'il réécrira pour la flûte traversière), le "Quatuor en fa" avec trio à cordes, tous les passages remarquables dans les opéras, symphonies et œuvres religieuses, il faut noter sa musique de chambre pour ensemble à vent où le hautbois tient un rôle prépondérant, les six sextuors pour 2 hautbois, 2 cors et 2 bassons ou les 2 sérénades pour 2 hautbois, 2 clarinettes, 2 cors et 2 bassons par exemple.

Le "Concerto en ut" qui lui est attribué n'est pas de Joseph Haydn (probablement d'un de ses élèves), mais ses cinq concerti pour deux lyres ont été adaptés pour flûte et hautbois. Salieri en composa un pour la même formation.

Le "Concerto en ut mineur" de Domenico Cimarosa rendu célèbre par Pierre Pierlot est un arrangement réussi d'œuvres pour clavecin.

Ayant entendu en décembre 1793 un trio pour 2 hautbois et cor anglais du hautboïste-compositeur Jan Vent (ou Wenth) (1745-1801), le jeune Ludwig van Beethoven composa deux œuvres pour cette formation : les "Variations sur "La ci darem la mano" de Mozart" et le "Trio en ut majeur" op. 87. Seules des ébauches de son "Concerto en fa majeur pour hautbois et orchestre" ont été retrouvées, mais il montre à de multiples reprises son attachement à l'instrument comme dans sa symphonie n°3 dans le deuxième mouvement à partir de la mesure ou dans la symphonie n°6 dans le troisième mouvement à partir de la mesure. À citer également, le solo poignant dans le mouvement de la symphonie n°5 à partir de la mesure.

"La Grande Symphonie" de Franz Schubert est un véritable concerto pour hautbois : dans le deuxième mouvement, après le solo des cordes graves, il annonce un des thèmes principaux qui revient à plusieurs reprises.

L'originalité du "Concertino" de Carl Maria von Weber est d'être écrit pour hautbois et petit orchestre d'harmonie, montrant l'intérêt souvent méconnu des compositeurs de cette époque pour cette formation.

L"'Introduction, thème et variations" op. 102 de Johann Nepomuk Hummel, magnifiquement interprété par Jacques Chambon, a été composé vers 1825 à Weimar.

Le guitariste Napoléon Coste, outre une "Fantaisie de concert" pour deux hautbois op. 35, composa quelques œuvres pour hautbois et guitare pour lui et son ami Charles-Louis Triébert dont la "Cavatine" op. 37.




Le pupitre de hautbois est constitué des instruments de la famille moderne, en nombre plus ou moins important selon les œuvres. Les parties de cor anglais ou de hautbois d'amour sont jouées par l'un des hautboïstes. La musette (hautbois piccolo) fait parfois son apparition ("Concerto n° 2" de Bruno Maderna).

Pour comparer le timbre de la flûte, du hautbois, de la clarinette et du basson, écouter les quarante premières secondes de la "Danse des cygnes" (tempo di valse) : à deux reprises le même thème est joué, à quelques variations près, par chaque instrument (du plus aigu au plus grave de la famille des bois). Ce thème est repris plusieurs fois durant le morceau, chaque fois succédant au tout premier thème exposé dès le début de la "Danse".

Traditionnellement, le hautbois donne le la à l'orchestre pour accorder les instruments. Cela s'explique par la richesse du timbre du hautbois en harmoniques de tous rangs dans le spectre audible, qui fournit de nombreux repères facilitant l'accord des autres instruments. Cela s'explique aussi par les possibilités très restreintes d'accord du hautbois.

Tous les types de formation sont autorisés, du duo à l'orchestre de chambre, en passant par le quintette et le nonette, en ensembles exclusivement à vent ou en ensembles mixtes.

Le hautbois et le cor anglais sont fréquemment utilisés dans les musiques de film, pour leur timbre particulièrement évocateur.

Quelques exemples : 

Dans la chanson française des années 1950 à 1970, où l'instrumentarium de l'orchestre symphonique domine, le hautbois est aussi employé pour son timbre évocateur. Par exemple :


Le hautbois est un instrument encore rare en jazz, certains artistes en jouent, développant ainsi son entrée dans les musiques improvisées :


L'enseignement du hautbois est dispensé par des musiciens qui partagent leur temps entre leur activité de concertiste, chambriste ou musicien d'orchestre, et celle de pédagogue au sein des centres d'enseignement spécialisé de la musique, fonction nécessaire à la formation des futures générations.


Le Livre Guinness des records 2007 a classé le hautbois parmi les deux instruments de musique les plus difficiles à jouer (l'autre étant le cor d'harmonie).




</doc>
<doc id="17974" url="https://fr.wikipedia.org/wiki?curid=17974" title="Liste des comtes de Hollande">
Liste des comtes de Hollande

Les premiers seigneurs de Hollande portaient le titre de comte de Frise occidentale et encore avant de comte de Kennemerland. Ce n'est qu'avec Thierry Ier qu’apparaît le titre de comte de Frise occidentale puis avec Thierry V qu'apparaît le titre de "comte de Hollande".


En 1076, Robert le Frison céda à son beau-fils Thierry V de Hollande les îles méridionales de la Zélande, qui désormais constituèrent pour les comtes de Hollande un arrière-fief qu'ils relevaient de la Flandre. En échange, le comte récupéra le pays de Waes.


Après Jean , les comtes de Hainaut héritent du comté de Hollande et de Zélande.


Son fils Guillaume lui disputa les comtés de Hollande et de Zélande qu'elle lui abandonna en 1354.
Elle reconnut en 1428 Philippe le Bon comme héritier de ses domaines, qu'elle lui abandonna formellement en 1433.




</doc>
<doc id="17976" url="https://fr.wikipedia.org/wiki?curid=17976" title="Groupe de Rio">
Groupe de Rio

Le groupe de Rio est une organisation créée le . Cette association vise à améliorer la coopération entre les différents pays d'Amérique latine.

Le groupe est une instance régionale informelle, son siège est à Asuncion au Paraguay. Le secrétariat est tenu à tour de rôle par l'un des membres.

Le , le ministère mexicain des Affaires étrangères annonce l'intégration de l'ile de Jamaïque comme membre du groupe de Rio. Le groupe de Rio compte les 24 pays membres :

Depuis la seconde moitié du , l’Amérique latine a connu de multiples tensions au niveau régional mais aussi international. On peut ici penser notamment aux différents coups d’État comme le Brésil en 1964 ou la Bolivie en 1970, mais aussi à des conflits internationaux tels que la crise de Cuba. Cette instabilité chronique du continent sud-américain est à lier avec l’influence des États-Unis d'Amérique dans cette région du globe durant les années de guerre froide. 

L’envie de proposer une alternative à l’Organisation des États américains, mais surtout la volonté d’enrayer la propagation des tensions en Amérique centrale, vont être à la base d’une coopération entre différents pays de l’Amérique du Sud. La Colombie, Le Mexique, le Venezuela, le Panama vont ainsi créer le « groupe de Contadora », puis ce groupe va être rejoint par le groupe d’appui composé de l’Argentine, le Brésil, le Pérou et l’Uruguay. Ces deux groupes vont finalement fusionner en 1986 pour donner naissance au groupe de Rio.

Au fil des années, le groupe de Rio s’est élargi. En 1990, la Bolivie, le Chili, l’Équateur et le Paraguay intègrent le groupe. Puis en 2000 c’est au tour du Costa Rica, du Salvador, du Guatemala, du Honduras, du Nicaragua et de la République dominicaine. En 2005, le Belize intègre à son tour le groupe de Rio, suivi de près par la Guyana et Haïti en mars 2008 et enfin Cuba en novembre 2008. Le groupe de Rio est un mécanisme permanent de consultation et de concertation politique sur différents thèmes qui affectent et intéressent les pays membres (ex : développement de la démocratie, droit de l’homme, développement économique…).

Le groupe de Rio ne dispose pas de secrétariat ou d’organe permanent. Il s’organise autour d’une présidence tournante chaque année entre les différents pays membres. Le pays présidant accueille alors la réunion annuelle.

Une des missions majeures du groupe de Rio est d’améliorer les relations entre ses pays membres. C’est pourquoi le groupe traite de différents sujets visant à développer ces relations en réglant les conflits mais aussi à développer l’économie de la région. Pour illustrer ce rôle d’interlocuteur régional nous prenons ici comme exemple le sommet de Turkeyen. Celui-ci s’appuyait essentiellement sur six thèmes principaux : la situation en Haïti, lutte contre la faim et la pauvreté, développement de l’éducation, le droit des personnes handicapées et la protection sociale, la situation sur les îles Malouines et la solidarité avec la Bolivie.

Le groupe de Rio joue aussi un rôle au niveau international. Il donne son avis sur différents événements qui se déroulent dans le monde et entretient des relations avec d’autres entités comme l’Union européenne. Nous avons étudié à titre d’exemple la déclaration de Prague du qui a réuni UE et groupe de Rio. Lors de cette déclaration deux sujets principaux ont été traités : les sources d’énergie renouvelables dans un premier temps, et le rétablissement de la stabilité financière et la croissance de l’économie mondiale dans un second temps.

Les relations sont pérennes depuis la création du groupe de Rio. En effet, de nouveaux pays ont intégré au fil du temps le groupe sans heurts. Le meilleur exemple est la dernière intégration ; celle de Cuba qui prouve que le groupe de Rio est aujourd’hui sur la bonne voie pour représenter un jour toute l’Amérique du Sud, l’Amérique centrale et l’ensemble des Caraïbes. Cette intégration prouve aussi que l’influence américaine tend à diminuer. Les relations stables et régulières avec l’UE prouvent aussi la réussite du groupe de Rio.

On peut mettre en avant deux grandes limites au fonctionnement du groupe de Rio. La première est évidemment l’inexistence d’institutionnalisation ; en effet il n’y a pas de secrétariat ni d’organes permanents, pas de budgets communs… L’autre grande limite est le paradoxe qu’il existe entre les résolutions prises par le groupe de Rio et le comportement des différents pays membres. Le groupe prône la démocratie, les droits de l’homme, mais on constate l’exploitation des ouvriers brésiliens dans les champs de cannes à sucre. L’asymétrie des économies des États membres peut aussi créer des divergences d’opinion sur les décisions à prendre pour le groupe. Enfin, un des fondements du groupe de Rio est d’instaurer la paix en Amérique du Sud et on peut se poser la question de son efficacité face au coup d’État au Honduras.

Le groupe de Rio, de par son développement avec ses nouveaux membres et la multiplication de ses relations à l’international avec notamment l’UE, a acquis aujourd’hui un poids plus important sur la scène internationale. L’influence américaine tend à disparaître et on constate que des pays comme l’Argentine, le Brésil et le Mexique font maintenant partie du G20 et ce en partie grâce à la stabilité régionale instaurée par le groupe de Rio.<br>Néanmoins, le groupe de Rio a de nombreux points négatifs. Il a toujours du mal à régler les problèmes récurrents à l’Amérique du Sud que sont la drogue, la corruption, le respect des droits de l’Homme ou encore la coutume du « "golpe de estado" ».



</doc>
<doc id="17982" url="https://fr.wikipedia.org/wiki?curid=17982" title="Hénin-Beaumont">
Hénin-Beaumont

Hénin-Beaumont est une commune française du département du Pas-de-Calais en région Hauts-de-France.

La commune résulte de la fusion d'Hénin-Liétard et de Beaumont-en-Artois en 1971. Peuplée en de , c'est la du département et la de la région. Hénin-Beaumont est le siège de la communauté d'agglomération Hénin-Carvin qui regroupe , soit en 2013, mais est directement sous l'influence des villes de Lens et Douai, entre lesquelles elle est située, appartenant ainsi à l'unité urbaine de Douai-Lens, agglomération de 67 communes formant le pôle urbain de l'aire urbaine de Douai-Lens, duquel elle est l'une des quatre villes-centres.

Avec sept charbonnages sur son territoire, quelques terrils et plusieurs centaines de maisons des mines, la commune conserve d'importantes traces de son passé (cf. époque contemporaine).

Hénin-Beaumont se situe dans la Gohelle dans la région Hauts-de-France et comme toutes les villes de la communauté d'agglomération Hénin-Carvin, elle fait partie du bassin minier du Nord-Pas-de-Calais. La ville fait partie du SCOT de Lens-Liévin et d'Hénin-Carvin. Lille est à , Arras à 20 et Douai à 10.

Oignies étant séparée de Hénin-Beaumont par un bras de la Deûle, il n'existe aucune route reliant les 2 villes.

L'altitude naturelle varie de au nord à au sud. Les terrils culminent cependant à et , et le Haut-Bois du Bois à .

La partie nord de la commune, basse, est traversée par le canal de la Deûle à son extrémité, ainsi que par un bras de ce canal. La station d'épuration se situe au croisement de ces deux cours d'eau. Cette zone touche plusieurs anciens marais : le grand marais (Courrières), le marais de Labiette, le marais de Dourges, le marais de Bourcheuil (Dourges). Autrefois la ville était traversé par un cours d'eau : l'Eurin. Ce cours d'eau prend sa source dans les environs de la mairie pour finir sa course lente dans la Deûle. Canalisé en sous sol, il n'est visible qu'à partir de la route reliant le hameau Bourcheuil à Courrières.

Hénin-Beaumont connaît un climat océanique (Cfb), avec des hivers peu froids et des étés peu chauds.
La moyenne température sur l'année est .

La ville est desservie par l'A1, sortie , sortie construite en 2003 pour l'accès aux zones logistiques et commerciales proches.

Pour les transports en commun, Tadao assure la desserte vers Lens/Liévin par la ligne spéciale buLLe1.

La gare est à une demi-heure de Lille, 1 h 35 de Paris, 2 h de Bruxelles et par l'Eurostar à 2 h 18 de Londres. On peut arriver par avion à l'aéroport de Lesquin qui est à . La Ligne grande vitesse Nord traverse la ville mais il n'y a pas de gare et il faut aller à Lens, Arras, Douai ou Lille pour prendre un TGV.

La ville comporte officiellement quatre quartiers, nommés récemment ainsi par la municipalité :

Cependant, de manière non officielle, les habitants d'Hénin-Beaumont préfèrent nommer leurs quartiers comme autrefois d'après les anciens sites miniers, dont certains sont protégés au niveau architectural, comme la cité Darcy et la cité Foch. Les autres quartiers sont le Ponchelet, les Bouviers, le Champ de l'Abbaye, Kennedy, Beaumont (ancien village), et bien sûr le centre-ville .

Le nombre de ménages dans la ville en 1999 était de et est passé à en 2005, ce qui représente 745 ménages de plus soit 7,7 % d'augmentation. Sur cette masse de population, on compte 28 % de foyers d'une seule personne, 30 % de foyers de 2 personnes ; 12 % des ménages ont cinq personnes ou plus.

Un tiers des habitants de la ville vivent dans des quartiers dits et 23 % des foyers vivent du RSA.

La ville prend son nom d'Hénin-Liétard au mais le nom à beaucoup évolué : dès 950-960 environ, on trouve diverses écritures telles que "Hennium", "Henninium" ou "Heninium". Le nom a diverses variantes, souvent très proches, sauf "Saint Martinus de Henain" (). En 1274, apparaît pour la première fois le nom "Hanin-le-Liétard". On ajouta Liétard en souvenir du seigneur, soit en reconnaissance des services rendus à la ville pour ses fortifications, soit tout simplement pour la distinguer d'Hénin-sur-Cojeul, à de là.

L'origine de Liétard est donc certaine, mais celle d'Hénin l'est moins. Il est généralement admis que Hénin viendrait du latin "hinniens" qui signifie « hénissant », ce qui expliquerait les armoiries de la ville (un cheval passant) et que la plupart des pièces gauloises retrouvées portent un cheval . Certains vont chercher l'origine de Hénin dans les origines des populations (celtes et germano-belges). Ainsi, Hénin proviendrait de "Hen-yn" ou "Hen-oen", mots celtiques signifiant « vieux frêne ». Il y a d'autres théories assez farfelues.

Durant la Révolution, la commune d"Hénin-Liétard" porta le nom de "L'Humanité".

En flamand, la ville s'appelle "Henen-Schoonberg".

Ses habitants sont appelés les "Héninois" ; ceux de l'ancienne commune de "Beaumont" conservent l'habitude de s'appeler "Beaumontois".

Le site a été très tôt occupé : l'existence d'un village gaulois prospère est attestée par les fouilles. Au furent retrouvées de nombreuses monnaies sur les bords du marais, attestant la venue de populations celte et germano-belge.

En 360, une première église est construite à l'emplacement actuel de l'église Saint-Martin. Hénin est définitivement christianisée au .

Au , l'abbaye augustinienne d'Hénin, renommée plus tard Notre-Dame d'Hénin-Liétard, fut fondée par saint Aubert.

Le 28 décembre 880, les Vikings incendièrent Hennium.

La ville, qui dépendait de la seigneurie des comtes de Liétard (qui dominent la ville de 950 jusqu'à 1244), est pillée et incendiée par les troupes impériales en 1053. Isaac Liétard, le d'Hénin fait établir de nouvelles fortifications qui vont tenir trois siècles. En 1169, l'abbaye fut reconstruite et, en 1187, sa nouvelle église fut édifiée. En 1244, Baudouin IV, le d'Hénin vend la ville à Robert, comte d’Artois.

Verrerie Beauvois à Hénin

En 1852, la découverte de la houille dans le bassin minier du Nord-Pas-de-Calais fait entrer Hénin-Liétard dans la modernité. À partir de 1856, de nombreux étrangers viennent à Henin-Liétard travailler dans les mines. La compagnie des chemins de fer du Nord fait construire une gare à Hénin-Liétard en 1859.

Lors de la guerre de 1870, le maire de l'époque s'enfuit. Paul Galland devint maire par la force des choses et organisa la défense de la ville face aux Prussiens qui arrivaient. Il fit d'Hénin-Liétard la seule ville défendue par ses habitants que les Prussiens ne prirent pas. Après la guerre, il démissionna, bien que tout le monde lui demandât de rester maire. En 1888, à sa mort, il y eut une foule importante et un long discours pour saluer ce grand personnage de la ville. Il fut enterré dans le caveau de la famille de son épouse Bruneau-Maréchal. Cette tombe existe toujours derrière le calvaire du cimetière d'Hénin-Beaumont, section B .
Au début du , la ville devient un bastion du Jeune syndicat de Benoît Broutchoux.

En 1914-1918, l'occupation allemande est dure et destructrice. Dans la nuit du 3 au 4 octobre 1914, les occupants procèdent au pillage et au saccage de la maison communale. En avril 1917, cette dernière subit l'acharnement des alliés à vouloir déloger l'occupant, il n'en restera bientôt plus que les murs. L'évacuation totale de la commune est effective le 26 avril 1917. Vers le 18 octobre 1918 (non certifié par l'absence de témoins), les Allemands font exploser l'église Saint-Martin avant de quitter la ville. Après la signature de l'Armistice, les premiers habitants seront notés de retour le 2 décembre 1918.
Durant l'entre-deux-guerres, Hénin-Liétard se reconstruit peu à peu sous l'impulsion du maire Adolphe Charlon (SFIO), qui sera constamment réélu jusqu'à sa démission en 1940. L'hôtel de ville est reconstruit en 1925. La Compagnie des mines de Dourges fait édifier l'église Sainte-Marie en 1928 pour les mineurs de la cité Foch. L'église Saint-Martin est reconstruite en style gréco-byzantin par l'architecte Boutterin en 1932, la nouvelle gare est inaugurée en 1933.

Libérée par les Alliés en septembre 1944, la ville est d'abord dirigée par le communiste Nestor Calonne de mai 1945 à octobre 1947. Le socialiste Fernand Darchicourt est élu maire d'Hénin-Liétard en 1953. C'est un autre socialiste, Jacques Piette, qui lui succède. Après la fermeture du dernier puits de mine en octobre 1970, Jacques Piette doit négocier la difficile transition d'Hénin-Liétard d'une ville minière à une ville tournée vers le commerce et les services. Il mène à bien la fusion d'Hénin-Liétard et de Beaumont-en-Artois, qui permet à la ville de se doter d'une vaste zone commerciale dans le secteur du Bord des Eaux, le centre commercial de Noyelle avec l'enseigne Auchan. Dès lors, le centre-ville ne cesse de se vider de ses commerçants.
La commune avec la percée du Front National depuis le début du ne cesse de focaliser l'attention des médias à chaque période électorale. Elle cumule un taux de chômage élevé, une pression migratoire constante depuis les années 1990, un fort endettement de la commune dans les années 2000 et compte 23 % de ses foyers ne vivant que du RSA.

La dégradation de la situation financière de la commune entre 2002 et 2009 ( de déficit cumulé et de dettes) évoque d'autres villes célèbres pour leurs difficultés, comme la commune d'Angoulême (Charente) ( de déficit et de dettes) ou celle de Pont-Saint-Esprit (Gard) ( de déficit et d'endettement). Les éléments qui suivent sont issus des documents publics de la Cour des comptes cités plus loin (avis budgétaires, rapport d'observation).

La ville est confrontée à une situation financière difficile diagnostiquée depuis 2003 : endettement élevé, accumulation de déficits, manque de recettes de fonctionnement. Après une amélioration surtout due à des ventes de terrains et bâtiments communaux, la situation s'est aggravée depuis 2007 avec des augmentations de dépenses de fonctionnement nouvelles. De ce fait, les impôts locaux ont augmenté de 85 % en 2004 puis de 10 % en 2008.

En 2008, après un déficit de en 2002, le déficit de fonctionnement dépasse les 13 millions d'euros, pour un budget de 38 millions d'euros. Vu la situation financière, sur proposition du préfet du Pas-de-Calais, le maire, Gérard Dalongeville est suspendu de ses fonctions par arrêté du 27 avril 2009 du Ministère de l'intérieur puis finalement révoqué par un décret pris en conseil des ministres.
C'est seulement la seconde fois depuis 1982 qu'un maire est démis de ses fonctions (CE, 27/02/1987). La préfecture du Pas-de-Calais a indiqué que le préfet avait demandé la révocation « après la sortie du dernier rapport de la Chambre régionale des comptes sur la gestion de la ville », sans qu'il soit question de poursuites judiciaires.

La CRC a été saisie par le préfet du Pas-de-Calais du fait d'un déficit excessif pour 2002, en application de l’article L.1612-14 du code général des collectivités territoriales (CGCT). Dans un avis du 7 novembre 2003, la chambre propose à la commune un plan de rétablissement de son équilibre budgétaire par la résorption sur trois exercices (2004 à 2006) du déficit constaté fin 2002, établi à €. Ce plan inclut une augmentation de 85 % de la fiscalité.

Conformément à l’article L. 1612-14 du CGCT, le préfet du Pas-de-Calais transmet à la chambre les budgets primitifs de la ville, chaque année entre 2004 et 2006. Par ses avis successifs (2004-392, 2005-282, 2006-333) des 5 mai 2004, 11 juin 2005 et 23 juin 2006, la chambre constate la mise en œuvre par la commune de mesures pour atteindre le rétablissement de l’équilibre budgétaire puis le retour à l'équilibre.

Soulignant la part importante prise par des circonstances exceptionnelles (régularisation d’écritures comptables, cessions d’immobilisations) dans le rétablissement, la CRC a relevé des insincérités dans le budget 2006 et invité la commune dans son dernier avis à des efforts d'économie, notamment s’agissant des dépenses de personnel et de la politique d'investissement.

Les avis budgétaires sont publics :

Saisie par le préfet du Pas-de-Calais en raison de l'insincérité du budget 2008 en application de l'article L.1612-5 du CGCT, la chambre a rendu deux avis budgétaires en juin et juillet 2008 (2008-149 et 2008-235), constatant « un déficit de » et proposant des mesures de redressement sur 4 ans. Le budget de la commune a été réglé par arrêté préfectoral du août 2008, imposant une augmentation de la fiscalité de 10,8 %.
Fin 2008, le conseil municipal prend une délibération augmentant ses dépenses de , sans recettes en face. La CRC, saisie par le préfet, constate dans son avis du 12 décembre 2008 que l'augmentation de 10,8 % des impôts avait été absorbée intégralement par des dépenses nouvelles.

Les avis budgétaires de 2008 sont publics :

En 2009, le préfet du Pas-de-Calais saisit la CRC pour examiner le compte administratif 2008, en déficit excessif (article L. 1612-14 du CGCT), et le budget 2009 (article L. 1612-5 du code CGCT). À la suite de cela, dans l'avis (2009-163/164) du 8 juin 2009, le déficit de fonctionnement 2008 est chiffré à « (et non 12,8) » soit un déficit représentant 18 % des recettes de fonctionnement. Les augmentations d'impôts de 2008 ont été absorbées par des dépenses nouvelles. Le budget 2009 n'était pas sincère, avec des surévaluations de recettes et des sous-évaluations de dépenses, sans compter le déficit du centre communal d'action sociale de la ville : car . La Chambre régionale des comptes a aussi constaté que le budget 2009 de la caisse des écoles, subventionnée par la ville, avait été faussement présenté en équilibre et qu'il présentait en réalité un déficit de .

La CRC a proposé pour 2009 des économies supplémentaires de (y compris en baissant les indemnités des élus), une augmentation des impôts de 8 % et la prolongation d'un an de la durée du plan de redressement (jusqu’en 2012), avec une réduction annuelle de du déficit. Le budget de la ville a été réglé par arrêté préfectoral du 29 juillet 2009.

Les avis budgétaires de 2009 sont publics :

La cour des comptes a examiné la gestion de la commune à plusieurs reprises ces dernières années, ce qui a donné lieu à plusieurs rapports d'observations définitives (ROD) très critiques, qui sont publics :

Le ROD indiquait en particulier : 

Simultanément aux rapports de la CRC, une enquête préliminaire a été ouverte par le parquet de Béthune, conduisant à la mise en détention le 9 avril 2009 de plusieurs personnes dont le maire, Gérard Dalongeville lui-même, son ancien adjoint aux finances et l'éditeur d'un journal gratuit diffusé dans les communes du secteur "Le Journal du pays", en liquidation judiciaire. Ils ont été mis en examen pour faux en écritures et usage de faux, détournement de fonds publics, favoritisme et recel de favoritisme. Par ailleurs, l'adjoint à l'urbanisme a été mis en examen en avril 2009 pour prise illégale d'intérêts portant sur des achats de terrain ; en première instance, il a été relaxé du fait de la prescription des faits.

Un élu d'une commune voisine, chef d'entreprise fournisseur d'Hénin-Beaumont et vice-président de la CCI, a été mis en examen le 28 mai 2009 dans la même affaire pour recel de favoritisme et recel de détournement de fonds publics.

La ville s'est finalement portée partie civile en septembre 2009, d'une part concernant et d'autre part les .


La population de la commune est relativement jeune. Le taux de personnes d'un âge supérieur à 60 ans (19,8 %) est en effet inférieur au taux national (21,6 %) tout en étant toutefois égale au taux départemental (19,8 %). 
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (52,6 %) est du même ordre de grandeur que le taux national (51,6 %). 
La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante : 
 





Une importante usine agroalimentaire de produits cuisinés à base de volaille et de porc se situe sur la commune (MoyPark).

En 1999, on pouvait compter dans la ville personnes comprenant 47 % d'hommes et 53 % de femmes. La ville compte aussi 40 % d'actifs dont 32 % d'actifs avec un emploi et 8 % d'actifs sans emploi, mais aussi 10 % d'élèves et d'étudiants. Au recensement de 2005, les pourcentages d'hommes et de femmes restent inchangés mais la population gagne personnes, ce qui élève la population à habitants, soit une progression de 4,1 %.

En 2014, la ville compte 60 % de non-imposables et 23 % des foyers vivent du RSA. Dans le bassin d'emploi Lens-Hénin, le taux de chômage atteint 17,9 % de la population active, un des plus élevés de France.

Certains commerces du centre-ville restent présents malgré l'immense centre commercial Auchan de Noyelles-Godault (à 5 minutes) qui ne cesse de s'étendre et qui a vidé le centre-ville de ses habitants. On peut compter dans toute la ville encore près de 400 commerçants.


Pour une ville de l'ancien bassin minier du Nord-Pas-de-Calais, les terrils y sont implantés. On peut voir le terril appelé aussi le terril Sainte-Henriette qui appartient au patrimoine historique et paysager du bassin minier. Tandis que le est seulement recensé par le schéma régional de protection des milieux et des paysages naturels.

Plus au sud, à côté du centre commercial Noyelles de Noyelles-Godault, le lac des Bords-des-Eaux est le seul plan d'eau de la ville.

La ville ne comporte pas de musée mais une salle de théâtre et de concert nommée "L'Escapade", où sont également organisées des activités diverses (cours de guitare, de danse, etc.), et aussi deux cinémas. Le premier, l"'Espace Lumière" dans le centre de la ville, est composé de trois salles ; le second, construit au côté du centre commercial Auchan de Noyelles-Godault, est d'abord un cinéma Gaumont avant de devenir un Cinéville avec douze salles d'exploitation.




En 2003, le documentariste Édouard Mills-Affif tourne à Hénin-Beaumont "Au pays des Gueules noires : la fabrique du Front national ", film visant à témoigner de la percée du Front national dans le bassin minier du Pas-de-Calais. 

En 2014, il filme "Bassin miné" afin de poursuivre le travail de décryptage commencé en 2003 et pour faire un bilan de dix ans de percée de l’extrême droite dans la région.

En 2017, le cinéaste franco-belge Lucas Belvaux présente "Chez nous", une fiction dont l’action se déroule dans la commune fictive d'« Hénard », nom qui fait allusion à une contraction de l'ancien nom de la commune (Hénin-Liétard).






</doc>
<doc id="17983" url="https://fr.wikipedia.org/wiki?curid=17983" title="Carvin">
Carvin

Carvin est une commune française située dans le département du Pas-de-Calais en région Hauts-de-France. Ses habitants sont appelés les "Carvinois".

La commune appartient au bassin minier du Nord-Pas-de-Calais. Elle fait partie de la communauté d'agglomération d'Hénin-Carvin et du schéma de cohérence territoriale (SCOT) de Lens-Liévin et d'Hénin-Carvin.

Située à l'extrémité est du département du Pas-de-Calais, Carvin est, à vol d'oiseau, à de Lens, à de Douai, à de Lille et à d'Arras.

Carvin est souvent incluse à la plaine du Carembault, un des cinq quartiers de la châtellenie de Lille, et appartient à l'Artois. Carvin fait également partie du bassin minier du Nord-Pas-de-Calais. La commune a une superficie de , dont d'espaces naturels.

La superficie de la commune est de ; l'altitude varie entre 17 et .

Le sol de Carvin est composé d'argile, de sable, de craie à de profondeur.

Carvin est dans une zone marécageuse.

Au , la richesse de son sol en houille en a fait une ville de l'ouest du bassin minier du Nord-Pas-de-Calais.

Carvin est sur les bords du canal de la Deûle. 

Carvin se situe en climat océanique tempéré.

Tableau comparatif des données météorologiques de Carvin et de quelques villes françaises

S'étant de tous temps trouvée sur la route des invasions, la ville a été traversée à partir du par un intense trafic routier, celui de Lille à Arras et à Paris, qui provoqua une certaine forme de congestion dans les années 1940 et 1950. Le problème fut résolu par la construction de l'autoroute du Nord, nommée autoroute A1 (Paris-Lille), dont la construction du premier tronçon, reliant Lille à Carvin, commence en septembre 1950 et fut mise en service en 1954. Carvin est accessible par la sortie 18 de l'autoroute A1.

Carvin est également desservi par l'ancienne route nationale 319 (RN319), devenue route départementale 919 (RD919), reliant la commune à celle d'Hénin-Beaumont vers le sud et par l'ancienne RN 25, devenue RD 917 dans le Pas-de-Calais et RD 925 dans le Nord, reliant Lens à Lille du sud-ouest au nord-ouest.

Depuis la fermeture de la gare de Carvin, la commune n'est plus desservie par les transports en commun ferré. Les gares les plus proches se situent à Libercourt et à Bauvin - Provin et sont desservies par les TER Nord-Pas-de-Calais.

L’aéroport de Lille - Lesquin se situe à une vingtaine de kilomètres au nord-est.

Carvin est desservie par le réseau du syndicat mixte des transports Artois-Gohelle Tadao, dont l’un des pôles de correspondance se trouve place Jean-Jaurès.

Le site, sur les bords de la Deûle, est à l'origine un vaste marécage dont l'assèchement progressif permettra une riche exploitation agricole. Le seul relief notable est une butte sur laquelle sera bâti le château d'Épinoy, au sud-est de l'actuelle commune. Cette butte est aujourd'hui indécelable. En revanche, des reliefs artificiels sont apparus aux et avec les terrils de la Compagnie des mines de Carvin, dont le plus élevé est celui de la fosse n°4.

Le territoire de la commune est étendu, mais densément urbanisé uniquement en son centre avec une excroissance à l'est. L'histoire de l'urbanisation est celle de la jonction des deux pôles (Carvin au nord, Épinoy au sud), puis de l'apparition d'un troisième à l'est (appelé par extension « Fosse 4 ») avec l'exploitation minière. Les caractéristiques d'une urbanisation plus lâche, le long des axes de sortie de la ville (notamment vers le nord) ne sont apparues qu'à la fin du .

Selon Hippolyte-Romain-Joseph Duthillœul, l'étymologie de Carvin proviendrait de « "Car" », signifiant charriot et de « "Win" », rappelant le vinage, le droit seigneurial sur les vins. Le nom aurait ainsi des origines gauloises.

Une autre hypothèse, défendue par l'historien local Henri Couvreur, est que Carvin s'orthographiait Carvent, nom d'origine celte qui signifierait « place de marché ».

La commune a porté le nom de Carvin-Épinoy et de Carvin-en-Carembault.

D'importantes fouilles ont révélé en 2008 l'ancienneté du peuplement local qui remonte au néolithique. Ce peuplement est lié à l'existence de la vaste dépression humide située au large de l'ancien château d'Épinoy. Les recherches en cours permettront d'en savoir davantage dès que les archéologues de l'INRAP remettront leurs conclusions définitives qui concerneront aussi le peuplement à l'époque gallo-romaine.

Du plus loin que l'histoire puisse remonter, Carvin fait partie intégrante du Carembault, territoire résultant du démembrement du royaume franc devenu par la suite un des quatre « quartiers » de la châtellenie de Lille.

Au début du , Épinoy est donné comme la terre natale de saint Druon. À l'autre bout de la ville actuelle, l'église du bourg est vouée à saint Martin. La ville actuelle est pour l'essentiel formée à partir de ces deux sous-ensembles, celui d'Épinoy où se trouvait aussi le château du seigneur et celui de Carvin où s'élevaient les halles et la maison des échevins, centre administratif de la seigneurie.

La seigneurie d'Épinoy (De Spineto) fut successivement possession de la famille d'Antoing, puis de la maison de Melun et enfin de celle des Rohan. Les seigneurs d'Épinoy furent faits chevalier de l'ordre de la Toison d'or. Ils devinrent comtes, puis, sans pour autant être de sang royal, furent faits prince. Leur château fut démantelé au début du XVIII. 

À l'époque de Louis XIV, le rattachement de la châtellenie et principauté d'Épinoy à la France s'accompagna de changements profonds. Devenus grands du royaume de France, les princes d'Epinoy confièrent l'administration de Carvin et celle des paroisses environnantes aux Robespierre. Ceux-ci s'y succédèrent aux postes de notaires et procureurs pendant plusieurs générations avant que, devenu avocat, l'un d'entre eux ne s'en aille se fixer à Arras et fonder ainsi la lignée dont est issu le célèbre conventionnel Maximlien de Robespierre. En 1783, celui-ci rendit visite à ses parents de Carvin et relate son voyage et son séjour dans une lettre méconnue enjouée et divertissante.

Au , contre l'avis des habitants, les princes d'Epinoy entreprirent une urbanisation à marche forcée des quartiers nord ainsi que de l'espace situé en rase campagne entre Epinoy et le bourg, qui vit ainsi la création de la grand-place actuelle. Les États d'Artois créant une nouvelle chaussée qui, reliant Arras à Lille, réunit les différents hameaux en pleine prospérité. En vue des États généraux de 1789, la paroisse de Carvin-Epinoy rédigea des cahiers de doléances. Puis l’administration révolutionnaire fit de ce chef-lieu d'ancien régime le chef-lieu d'un canton, regroupant alors dix communes, au sein de l'arrondissement de Béthune. Au plus fort des guerres révolutionnaires qui agitèrent la frontière nord de la France, la tour de l'église fut surmontée d'une cabine de relais du premier télégraphe optique mis au point par les frères Chappe pour la ligne Lille-Paris.

L’histoire de ce bourg agricole et marchand sera ensuite marquée par l'exploitation minière, démarrée au milieu du et achevée en 1975. Une des plus petites compagnies minières privées concessionnaires de l'exploitation du sous-sol avait son siège à Carvin même, non loin de la gare. Les compagnies voisines des mines d'Ostricourt et de Courrières s'étaient vu également concéder l'exploitation d'une partie du sous-sol de la commune. Une magnifique série de photographies prises au fond de la mine à Carvin en 1902 immortalise les différents postes de travail présentés sous forme de scènes de travail. Ces prises de vue de l'ingénieur Corriol ont été éditées en cartes postales et diffusées en nombre par l'imprimerie carvinoise Plouvier, qui édita aussi des poèmes patoisants écrits par Jules Mousseron et les brochures du maire Charles Baggio.

Sur le territoire de la commune (dans ses limites actuelles, en excluant Libercourt), le charbon sera remonté depuis les fosses n° 1 (Sainte-Barbe), n° 2 (ultérieurement fosse n° 12), n° 3 (Saint-Louis) et n° 4 (Boudenoot, ultérieurement fosse n° 14) des mines de Carvin et la fosse n° 4 des mines d'Ostricourt.

La mémoire locale récente est riche d'une culture de guerre propre aux civils des régions envahies. Pendant la Première Guerre mondiale, d'octobre 1914 à octobre 1918, Carvin se trouve en région occupée. Située à l'arrière du front d'Artois, c'est une ville de garnison et de soutien à l'effort de guerre allemand où cohabitent civils et militaires . Les corps d'un peu plus de six mille soldats reposent dans un des plus grands cimetières allemands du Nord-Pas-de-Calais. La commune reçut la croix de guerre 1914-1918 le 25 septembre 1920, en reconnaissance de toutes les souffrances endurées.

En mai 1940, lors de l'invasion du Nord de la France, tout le secteur dont Carvin constitue la ville-centre est le théâtre d'une importante bataille de retardement qui permet au gros des troupes françaises et anglaises encerclées de gagner la poche de Dunkerque. Dès que les alliés se furent dérobés, les troupes de la Wehrmacht se livrèrent à des représailles féroces sur les populations des cités minières de la fosse 4, tout comme dans les villes voisines de Courrières et Oignies. À la suite de la grande grève des mineurs de mai et juin 1941 qui se répandit dans tout le bassin minier du Nord-Pas-de-Calais, de nombreux mineurs communistes pour la plupart furent arrêtés et fournirent un contingent important d'otages exécutés à Arras.

Pendant l'Occupation, l'Allemagne cherche à exploiter au mieux les ressources de l'exploitation minière, une tâche compliquée par la Résistance, dans une ville où la majorité des mineurs sont acquis depuis les années 1920 au Parti communiste. Carvin est libérée le 2 septembre 1944.

En 1946, à la nationalisation, les fosses et installations minières carvinoises sont regroupées au sein du groupe d'Oignies. L'exploitation charbonnière et le développement du chemin de fer ont modifié la géographie de la commune, faisant naître notamment le quartier des cités de la fosse 4, dans l'Est de la ville, en démarquant aussi un peu plus le hameau de Libercourt. La gare de Libercourt se retrouva placée sur la ligne du chemin de fer du Nord qui relie Lens à Lille, tandis que la ville de Carvin, dotée elle aussi d'une gare en centre-ville, se trouvait sur le barreau ferroviaire reliant Hénin-Liétard à Don, avec ramification vers les gares d'eau des mines d'Ostricourt et de Meurchin. Ce barreau servit jusqu'en 1950 au transport de voyageurs, puis jusque dans les années 1990 à l'industrie. Son exploitation s'étant arrêtée, ces voies n'existent plus.

En 1947, Carvin perd près d'un tiers de son territoire avec la séparation de la commune de Libercourt (y compris le hameau de Garguetelle), qui avait réclamé son indépendance.

La ville a perdu la majeure partie de son industrie dans les années 1970 à 1990 : les mines, dont l'exploitation s'arrête en 1975, et les constructions mécaniques (CMC, groupe Poclain). La circulation ferroviaire s'est également arrêtée, les voies ferrées étant démantelées dans les années 1990. C'est aujourd'hui une commune de services et résidentielle, proche de la capitale régionale, Lille.

Carvin est située dans le département du Pas-de-Calais, en région Nord-Pas-de-Calais. Appartenant à l'arrondissement de Béthune puis à celui de Lens dès sa création en 1962, elle est le chef-lieu du canton de Carvin, regroupant la commune et celle de Libercourt. Depuis de la Cinquième République, la commune est toujours restée dans la onzième circonscription du Pas-de-Calais, même si le découpage de cette circonscription a évolué.

La commune fait également partie de la communauté d'agglomération d'Hénin-Carvin, qui regroupe quatorze communes, et du schéma de cohérence territoriale de Lens-Liévin / Hénin-Carvin.

Sous la IIIe République, les électeurs de Carvin optent pour le républicanisme. Par la suite, avec la montée du mouvement ouvrier mineur, les élections locales conduiront les radicaux et les socialistes aux commandes de la ville, tandis qu'au niveau des élections législatives Carvin devient un fief socialiste . Dans l'entre deux guerres, dans les années 30, l'influence grandissante des communistes conduit à l'élection de Cyprien Quinet, premier député communiste du Pas-de-Calais . Sous la IV et la Ve République, Carvin reste une ville de gauche dans une région ancrée à gauche dirigée alternativement par des maires socialistes et communistes. 
Ces dernières années comme dans tout l'ex-bassin minier, le Front national se met à recueillir des suffrages de plus en plus nombreux aux élections politiques, Marine Le Pen, arrivant en tête au premier tour des législatives de 2012, seconde aux élections régionales de 2010, troisième aux élections européennes de 2009. Au élections cantonales de 2011, le candidat du FN termine second, derrière le groupe socialiste.

Le nombre d'habitants à Carvin étant supérieur à et inférieur à , le nombre de conseillers municipaux est de 33. Outre le député-maire, Philippe Kemel, le conseil municipal est composé de 25 élus sympathisants et de sept élus d'opposition.

En 2007, la commune de Carvin est récompensée par le label « Ville Internet @@ ».

Depuis la Libération, six maires se sont succédé à Carvin.

Le tribunal d'instance, situé à côté de l'église Saint-Martin, est une survivance de la justice qu'opéraient les échevins en Épinoy. Installé dans un bâtiment du , il est compétent sur l'ensemble de la communauté d'agglomération d'Hénin-Carvin. Le tribunal de Carvin relevait de la cour d'appel de Douai. Le 12 octobre 2007, la garde des Sceaux Rachida Dati a annoncé sa suppression dans le cadre de la réforme de la carte judiciaire. La fermeture eu lieu le 31 décembre 2009.

Carvin relève du tribunal d'instance de Lens, du tribunal de grande instance de Béthune, de la cour d'appel de Douai, du tribunal pour enfants de Béthune, du conseil de prud'hommes de Lens, du tribunal de commerce d'Arras, du tribunal administratif de Lille et de la cour administrative d'appel de Douai.

Carvin obtient une fleur au Concours des villes et villages fleuris. Eden 62, syndicat mixte s'occupant de la protection des sites naturels du Pas-de-Calais, répertorie deux sites naturels sur la commune dont le département est propriétaire : la Tour d'Horloge et la Gare d'eau. Le premier est un bois, le second un étang, la faune et la flore ne sont pas identiques. Il y a 177 espèces de flore au site de la Gare d'eau parmi lesquels le séneçon qui était déjà cité en 1857

Du fait de son appartenance à la communauté d'agglomération d'Hénin-Carvin, la commune de Carvin pratique le tri sélectif. Elle dispose également une déchèterie sur sa propre commune.

Au 5 juillet 2013, Carvin est jumelée avec :

La population de la commune est relativement jeune. Le taux de personnes d'un âge supérieur à 60 ans (18,4 %) est en effet inférieur au taux national (21,6 %) et au taux départemental (19,8 %). 
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (52,2 %) est du même ordre de grandeur que le taux national (51,6 %). 
La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante : 
 
Carvin est doté d’un centre hospitalier de 150 lits, spécialisé en gériatrie et alcoologie et d'un collège national d'audioprothèse.

Carvin appartient à l'académie de Lille.

Carvin possède plusieurs salles de sport (Deschauwer, Dauchez, Rabelais, Pascal, Copernic), de deux salles et stades (Plantigeons, Cordier), d'un skatepark et d'un mur d'escalade. Le complexe sportif régional comporte deux terrains de tennis.

En 1999, Carvin a accueilli la première coupe du monde de Yoseikan Budo, art martial d'origine japonaise. Seul un petit nombre de nations étaient toutefois représentées.

Le quotidien régional "La Voix du Nord" publie une édition locale pour la communauté d'agglomération d'Hénin-Carvin, ajoutée à celle du communaupole de Lens-Liévin.

Les habitants de Carvin reçoivent, outre certaines stations de radio nationales, les programmes de Nostalgie Lens et de Chérie FM Haut de France. Elle reçoit également des radios régionales comme Fréquence Horizon, Metropolys, Contact et Mona FM.

La ville est couverte par les programmes de France 3 Nord-Pas-de-Calais et les chaînes nationales de la TNT. Elle reçoit également la chaîne régionale Wéo.



L'économie de la ville est traditionnellement tournée vers l'agriculture, dépeinte sur des vitraux de l'hôtel de ville, et le commerce, avec des marchés renommés (sur les actuelles places Jean-Jaurès et Jules-Guesde). L'agriculture (pommes de terre, betteraves, endives, céréales) garde une place importante sur le territoire de la commune, avec plusieurs fermes en activité et une coopérative agricole (UNÉAL) située à Carvin Saint-Paul .

Pendant un siècle et demi, s'y ajoute l'exploitation minière, la ville étant située au centre du territoire Houillères du Nord-Pas-de-Calais. La Compagnie des mines de Carvin est l'une des plus petites de la région. Cette activité minière est également représentée sur un vitrail de l'hôtel de ville, qui a pour atlantes deux mineurs de fond.
Parmi les industries qui se sont implantées à Carvin, on relève l'agro-alimentaire (vinaigrerie), les constructions mécaniques (engins de chantier avec les Constructions mécaniques carvinoises puis Poclain, le sondage du sol, machines agricoles avec Sulky Burel) et les matériaux de construction (Bonna Sabla).

La Vinaigrerie de Carvin, fondée en 1848 par Jules Duquesne, était à l’origine une amidonnerie. Elle est implantée au cœur de la commune sur une emprise de . Elle produit 24 millions de bouteilles chaque année reparties à 55 % de vinaigre cristal, 25 % de vinaigre de vin et 20 % de vinaigre coloré. La vinaigrerie de Carvin représente environ 30 % de la production française en vinaigre courant, avec hectolitres à 10°.

Carvin a vu plus récemment l'implantation d'entreprises de logistique, attirées par sa situation géographique et ses disponibilités foncières.

En revanche, son commerce en centre-ville a eu tendance à décliner à partir des années 1980, concurrencé par les vastes zones commerciales en périphérie.

En mars 2010, l’union des commerçants de la ville (ACTIV) met en ligne sur le réseau Internet, une galerie marchande numérique afin de redynamiser les boutiques urbaines (projet porté par la ville de Carvin).

Bâti en position centrale par rapport au bourg médiéval (quartiers nord aujourd'hui), à ce qui fut le village d'Épinoy et aux cités ouvrières entourant la fosse 4 des mines d'Ostricourt, le château Delehelle a appartenu à une famille de notables locaux, brasseurs et notaires. Il est, quand il sort de terre, le bâtiment d'habitation le plus imposant de la ville. Il est racheté vers 1930 par la Ville, qui se trouvait à l'étroit et excentrée dans ses locaux municipaux près de l'église Saint-Martin. Il devient le siège de la mairie, inaugurée le 22 juillet 1934 par le maire Marcel Paget, et voit une extension de ses locaux sur les deux ailes.

La commission régionale du patrimoine et des sites a proposé en 2009 sa protection au titre des monuments historiques, ce qui fut obtenu en 2010.

Depuis le 30 juin 2012, il fait partie des sites du bassin minier du Nord-Pas-de-Calais inscrits au patrimoine mondial. Il est en effet relié à l'histoire de la mine par ses deux atlantes, des mineurs de fond, et son vitrail, qui dépeint la tradition agricole et minière de la ville.

L'église Saint-Martin et sa tour érigée en 1702, dont le style baroque tardif, d'inspiration espagnole, est rare dans la région et totalement unique dans l'arrondissement de Lens, est le principal monument de la ville. Des pierres tombales des membres de la famille Robespierre sont insérées dans le dallage de l'église.
En octobre 1793, Claude Chappe installa sur la plateforme au sommet de la tour un relais de son télégraphe reliant Paris à Lille.

L’église possède un grand orgue, construit en 1847 par François-Joseph Carlier, restauré et reharmonisé par le facteur d’orgue Michel Garnier en 1989. C'est l'un des instruments les plus renommés de la région.

L’ensemble a été classé monument historique par arrêté du 13 juin 1921.
L'origine de l'église Saint-Druon, du puits, ainsi que celle du château d'Epinoy remontent au Moyen Âge. La chapelle qui surmonte le puits a été bâtie en plein milieu d'un champ, attenant à la ferme et à l'église.

Le monument aux morts de 1914-1918, inauguré en 1922 à l'un des carrefours les plus importants de la ville (rue de la Gare et rue du Centre), porte la mention « La ville de Carvin à ses enfants morts pour la France ».
Y sont gravés les noms de 271 soldats morts pour la France, ce qui ne représente qu'une partie des souffrances endurées par la population civile d'une ville occupée par l'ennemi. À côté des noms des victimes militaires figurent aussi les noms d'une soixantaine de victimes civiles. .Au premier plan, une femme et ses deux enfants sur les ruines de leur maison. 
Au second plan, l'allégorie de la ville de Carvin grave dans la pierre la dédicace à ses enfants morts. 
Un bas-relief représentant la mairie, l’église et un puits de mine. 
En avant un poilu sans armes, offre sa poitrine pour la défense de son foyer et de son pays .
Signé du sculpteur Laouet à Roubaix,
Carré militaire français.

Ce cimetière de la Première Guerre mondiale, situé rue Victor-Hugo et accolé au cimetière civil, fut conçu par l'occupant allemand dès 1914. Y reposent soldats allemands, dont 26 inconnus, et trois Russes prisonniers de guerre.

Au sein du cimetière communal ont été regroupés au sein de carrés militaires des tombes avec stèles commémoratives dédiées aux combattants britanniques et du comonwealth tués lors des deux guerres mondiales

La Brasserie a été ouverte avant 1900 par trois associés : Émile Daubresse, Paul Dupont et Germain Thiry. Elle a obtenu une médaille d'or à l'exposition internationale de Lille de 1902 [cité en-tête lettre du directeur de la brasserie moderne]. Les bâtiments d'origine ont été construits rue Florent-Évrard (appelée rue d'Arras à cette époque). C'est l’un des témoignages de cette activité dans la région. Elle produisait une bière brune.
Dirigée par Germain Thiry [information familiale], l'entreprise a été transformée en société anonyme en 1908, et la direction en a été confiée à M. Éloi Vincent Imbrassé. 
Les bâtiments ont été partiellement détruits sous l'occupation allemande pendant la Première Guerre mondiale, et ont été reconstruits après la guerre. M. Georges Labisse en était le directeur jusqu'à son décès accidentel dans les locaux de la brasserie, le 17 juillet 1929 [document décès de G. Labisse]. La brasserie a ensuite été dirigée par M. Godin, puis par Albert Hennart.
En 1927, l'usine produisait de la bière de fermentation haute ; en 1946, la production s'élevait à hectolitres de bière de fermentation haute et mixte.

La production de bière cesse vers 1961. Dans les années 1980, les locaux de la brasserie ont été transformés en un ensemble de logements et certaines de ses parties ont été détruites.


Les armes carvinoises sont composées de l’écu et de son ornement, qui, réunis, forment le blason. L’écu carvinois, "« d’azur à sept besants d’or et au chef de même »," représente également les armes de la maison de Melun, dont le fief a longtemps été la Terre d’Épinoy. L’ornement de l’écu a, lui, une histoire qui s’est forgée principalement au :

Finalement, l’écu "« d’azur à sept besants d’or et au chef de même »" est donc "« abrité sous un manteau de pourpre doublé d’hermine, brodé et frangé d’or, surmonté de la couronne princière. »"

La croix de guerre 1914-1918 présente sous le blason a été obtenue par Carvin le 28 septembre 1920 pour « citation à l’ordre des armées au cours de la campagne de 1914-18 contre l’Allemagne et ses alliés ».

Le blason d'Illies est quasi identique, et celui de Wingles très proche.

Devise de la ville : Carvin, la ville du bien vivre ensemble.


Le géant Métisségalée, baptisé le 14 juillet 2010 est à la fois homme et femme, blanc et noir, jeune et vieux (comme le montre sa canne). Sa taille est de . Il a été conçu et fabriqué par une trentaine de jeunes de 13 à 17 ans, les mercredi et samedi de décembre 2009 au 7 juillet 2010, au sein du centre d'animation jeunesse.








</doc>
<doc id="17984" url="https://fr.wikipedia.org/wiki?curid=17984" title="Table des isotopes">
Table des isotopes

Cette table répertorie tous les isotopes connus des éléments chimiques, ordonnés selon leur numéro atomique croissant de gauche à droite et avec un nombre croissant de neutrons de haut en bas.

Les demi-vies sont indiquées par la couleur de la cellule de chaque isotope ; les isotopes ayant deux modes de désintégration qui diffèrent par leur demi-vie ont des couleurs de fond et de bordure différentes.

Les éléments colorés en rouge (éléments stables) forment une suite qualifiée de « vallée de stabilité ».


</doc>
<doc id="17987" url="https://fr.wikipedia.org/wiki?curid=17987" title="Cycloalcane">
Cycloalcane

En chimie organique, les cycloalcanes (ou cyclanes) sont une série de molécules contenant deux atomes d'hydrogène par carbone et arrangées en forme d'anneau. Ils font partie des composés alicycliques.

La formule générale de ce type de composé est : CH.

Les trois principaux éléments de la série :

La méthode de nommage de l'UICPA pour ce type de composé est très simple. Il suffit de prendre le nom de l'alcane correspondant et de le faire précéder de cyclo :

Les cycloalcanes (ou cyclanes) sont des hydrocarbures comportant un ou plusieurs cycles d'atomes de carbone unis par des liaisons simples. Les cycloalcanes simples, non substitués, de formule générale (CH) constituent une série homologue particulièrement intéressante, car leurs synthèses et leurs propriétés chimiques varient directement avec la taille du cycle, différant nettement en cela des alcanes correspondants CH - (CH) - CH. La tension de cycle due à l'angle entre liaisons entre atomes de carbone et l'encombrement stérique expliquent en grande partie les différences de comportement entre cyclanes et alcanes.

Quel est l'angle formé entre les liaisons entre atomes de carbone constituant un cycle à n atomes ? L'angle formé doit être au moins égal à l'angle au sommet du polygone régulier de n côtés.

Or, on constate que pour n > 5 tous les cycles présentent une grande stabilité ; on en conclut que pour n > 5 les cycles ne sont plus plans.


La cyclisation est une réaction chimique permettant d'obtenir un cycloalcane à partir d'un alcane.

Mêmes types de réactions que les alcanes.

Notons cependant que l'existence de tensions de cycle (cycle à 3 ou 4 carbones) dans les réactifs ou symétriquement des cycles non tendus dans les produits (cycles à 5 ou mieux à 6 carbones) peut accélérer une réaction.



</doc>
<doc id="17990" url="https://fr.wikipedia.org/wiki?curid=17990" title="Table divisée des isotopes">
Table divisée des isotopes

Ces tables répertorient tous les isotopes connus des éléments chimiques, ordonnés selon leur numéro atomique croissant de gauche à droite et avec un nombre croissant de neutrons de haut en bas. Les demi-vies sont indiquées par la couleur de la cellule de chaque isotope, les isotopes ayant deux modes de désintégration qui diffèrent par leur demi-vie ont des couleurs de fond et de bordure différentes.
Les éléments colorés en rouge (éléments stables) forment une suite qualifiée de « vallée de stabilité ».


</doc>
<doc id="17993" url="https://fr.wikipedia.org/wiki?curid=17993" title="Voyageurs étrangers dans le monde indien">
Voyageurs étrangers dans le monde indien

Les relations des Voyageurs étrangers dans le monde indien sont une source primaire importante pour reconstituer une histoire de l'Asie du sud à laquelle ses habitants ont mis longtemps à s'intéresser.

Cette liste ne concerne essentiellement les voyageurs qui font un compte-rendu de leur séjour avant 1784, date de la création de la société asiatique du Bengale par le linguiste William Jones qui marque le début d'une étude du monde indien proprement scientifique, et avec un souci d'exhaustivité, par la puissance colonisatrice. En ce sens, on acceptera en manière d'exception le cas tardif de l'abbé Dubois.

Ces relations ont quatre origines principales :


Plusieurs pèlerins bouddhistes chinois font le voyage des Indes pour visiter le pays du Bouddha et ramener en Chine des textes sacrés. De retour au pays, ils relatent leur voyage et la relation qu'ils en font nous renseigne sur la prépondérance du bouddhisme en Inde avant la contre-réforme hindouiste et l'irruption de l'Islam qui vont changer profondément le sous-continent indien. C'est souvent grâce à ces écrits que les archéologues retrouvent des sites qui avaient disparu des mémoires, comme Lumbinî.


Du Moyen Âge à la colonisation britannique de nombreux voyageurs européens - Italiens, Portugais, Anglais, Français - commerçants ou soldats arpentent le pays et racontent leurs découvertes.





</doc>
<doc id="17996" url="https://fr.wikipedia.org/wiki?curid=17996" title="A56">
A56

A56 ou A-56 est un sigle qui peut faire référence à plusieurs choses.






</doc>
<doc id="17997" url="https://fr.wikipedia.org/wiki?curid=17997" title="Ahmed Zewail">
Ahmed Zewail

Ahmed Hassan Zewail () né le à Damanhur et mort le à Pasadena, était un chimiste égyptien travaillant aux États-Unis. Il a reçu le prix Nobel de chimie en .

Ahmed Zewail a étudié la chimie à l'université d'Alexandrie où il obtient sa licence puis un master en spectroscopie, puis a poursuivi un doctorat à l'université de Pennsylvanie en 1973 avant d'être chercheur post-doctoral à Berkeley, puis d'être nommé en 1976 professeur de chimie et de physique au "California Institute of Technology" (CalTech), et occupe la chaire Linus Pauling depuis 1990 où il dirige le laboratoire pour les sciences moléculaires et le Centre de recherche multidisciplinaire fondé à CalTech par la "National Science Foundation" (NSF) autour de l'étude des processus moléculaires fondamentaux intervenant dans les systèmes moléculaires complexes.

Ahmed Zewail est le premier à avoir montré comment l’étude des réactions chimiques pouvait être réalisée grâce à des flashs lasers extrêmement brefs (picosecondes puis femtosecondes), à l'aide d'un laser décrit comme . Le chercheur a réussi à montrer comment se faisaient les liaisons chimiques à l'échelle de quelques femtosecondes, soit un millionième de milliardième de seconde. L'appareil mis au point permet de voir les mouvements des électrons formant les liaisons chimiques, ce qui ouvre la possibilité de comprendre leur comportement et potentiellement de contrôler le résultat de leurs réactions.

Le principe qu’il a développé consiste à soumettre un milieu chimique à deux flashs successifs : le premier génère la réaction, le second permet d’analyser par spectroscopie les composés chimiques.

Par ses découvertes, Ahmed Zewail a permis d'ouvrir de nouvelles perspectives en chimie, en biologie et en pharmacologie pour la mise au point de réactions chimiques et biochimiques plus performantes et plus sélectives, avec les conséquences que cela implique tant pour la synthèse chimique que pour la santé humaine.

Ses recherches ont ensuite été consacrées à l'étude aux échelles femto à nanosecondes du couplage électronique et de la dynamique atomique et moléculaire au cours des actes chimiques élémentaires qui sont mis en jeu lors de plusieurs réactions fondamentales en chimie et en biologie, ou qui contrôlent la communication électronique à longue distance dans les supramolécules chimiques et biologiques comme l'ADN.

Ahmed Zewail a reçu plus de 100 prix et récompenses.

Outre le prix Nobel de chimie en 1999, ses travaux décisifs lui ont valu de nombreuses distinctions prestigieuses dont le prix Wolf en 1993, la médaille du Collège de France en 1995, le prix Peter Debye et le "NAS Award in Chemical Sciences" en 1996, le prix Robert A. Welch en 1997, la médaille Benjamin Franklin, le prix Lawrence en 1998, le prix Röntgen en 1999, la médaille de l'Institut du monde arabe en 2000, le prix Albert Einstein en 2006 et la médaille Davy en 2011.

Son pays l'Égypte lui a décerné en 1999 sa plus haute distinction républicaine en lui attribuant le grand collier de l'Ordre du Nil. Il est par ailleurs décoré des insignes de grand cordon de l’ordre du Cèdre du Liban, de commandeur de l'Ordre la République tunisienne, de chevalier de l'ordre national du Mérite (France) et de chevalier de la Légion d'honneur. 

En 2010, il est docteur "honoris causa" de plusieurs universités à travers le monde, aux États-Unis, en Angleterre, en Suisse, en Égypte, en Belgique, en Australie, au Canada, en Inde, en Italie, en Écosse, en Corée, en Suède, en France, en Chine, au Mexique, en Irlande, au Japon, au Liban, et en Argentine et occupe des postes honorifiques en sciences, arts, philosophie, loi, médecine, et littérature.

Il est par ailleurs membre d'académies et de sociétés nationales et internationales, dont la National Academy of Sciences, la société philosophique américaine, l'Académie pontificale des sciences, l'académie européenne des arts, des sciences, et des sciences humaines, membre étranger de la Royal Society (1998), de l’Académie des sciences de Russie, de l'Académie royale des sciences de Suède, de l'Académie des sciences de la Malaisie, de l'Académie des sciences (France) et de l'Académie tunisienne des sciences, des lettres et des arts. Il détient la chaise honorifique à l'université des Nations unies. 

Enfin, des timbres ont été publiés en son honneur.




Académie tunisienne des sciences, des lettres et des arts]]


</doc>
<doc id="18000" url="https://fr.wikipedia.org/wiki?curid=18000" title="Ah ! si j'étais riche">
Ah ! si j'étais riche

Ah ! si j'étais riche est un film français réalisé et écrit par Michel Munz et Gérard Bitton, sorti en salles le 27 novembre 2002.

Modeste V.R.P. en produits capillaires, Aldo Bonnard est cerné de toutes parts : il est endetté, son ménage bat de l'aile et Gérard, son nouveau patron, n'a d'yeux que pour son épouse Alice, qu'il ne tarde pas à séduire. C'est au moment où cette dernière demande le divorce qu'Aldo gagne 10 millions d'euros au Loto. Décidé à ne pas partager le magot, il va cacher sa fortune jusqu'au prononcé du divorce. Entre-temps, il mène clandestinement la vie d'un homme richissime et se fait volontairement mettre à la porte de son entreprise, sans trop de remords. Cependant, lorsque Gérard licencie un à un ses anciens collègues, l'idée lui vient de se venger de cet ex-patron devenu rival ...





</doc>
<doc id="18001" url="https://fr.wikipedia.org/wiki?curid=18001" title="François Morel (acteur)">
François Morel (acteur)

François Morel, né à Flers (Orne) le , est un acteur et chroniqueur radio français.

Né à Flers, François Morel a un père employé SNCF et militant CGT, une mère dactylo, un grand frère et une grande sœur. Il passe son enfance dans la bourgade de Saint-Georges-des-Groseillers, commune attenante à Flers. Élève timide mais potache, il étudie dans un collège catholique Sainte Marie et trouve sa vocation en prenant des cours de théâtre à "l'Albatros", la maison des jeunes et de la culture. Il passe une maîtrise de lettres obtenue à l'université de Caen, puis en 1981 monte sur Paris et intègre l'école de théâtre de la rue Blanche. C'est à cette époque qu'il rencontre une étudiante aux Beaux-Arts qui deviendra sa femme, Christine Patry-Morel.

C'est Jean-Michel Ribes qui fait le premier appel au talent de François Morel en lui confiant en 1988 le rôle du groom dans sa série télévisée Palace. Puis en 1989, il intègre la troupe de Jérôme Deschamps et Macha Makeïeff : Les Deschiens.

Sa notoriété explose lorsque l'univers de Jérôme Deschamps et Macha Makeïeff est transposé au petit écran dans une série appelée "Les Deschiens" qui est diffusée sur Canal+ en 1993 dans l'émission "Nulle part ailleurs".

Fin 2006, François Morel entame une carrière de chanteur avec son spectacle "Collection particulière". À cette occasion, il enregistre un album du même nom dont la sortie est suivie d’une tournée s’étalant jusqu’en 2007. François Morel est l’auteur de tous les textes, sur des musiques principalement composées par Reinhardt Wagner, mais aussi par Juliette Noureddine et Vincent Delerm (parution le ). Il a écrit également des chansons pour Francesca Solleville, Anne Baquet, Norah Krief, Juliette, Maurane, Nathalie Miravette, Juliette Gréco.
Il prête également sa voix au personnage de Rantanplan lors des courts-métrages qui montrent les mésaventures du chien le plus bête de l'Ouest. À cette occasion, il a interprété une musique de rap en rapport avec le film "Tous à l'Ouest".

Pour le film "Le Chat du rabbin", il prête sa voix au personnage principal, le chat du rabbin Sfar.

Depuis septembre 2009, il est chroniqueur sur France Inter, chaque vendredi matin dans l'émission "le 7/9", sur laquelle il met sa verve poétique au service de l'humour et des causes sociales/sociétales qui le touchent. , ses billets d'humeur révèlent un humour tantôt absurde, tantôt mordant selon "Libération", consensuel et « progressiste raisonnable » selon "Télérama".

Il joue le rôle de Monsieur Jourdain dans "Le Bourgeois Gentilhomme", comédie-ballet de Molière et Lully, création du CADO (le centre national de création d'Orléans), d'octobre à décembre 2011 à Orléans, puis à Paris début 2012.

Le peintre illustrateur Martin Jarrie réalise ; avec son éditrice, ils ont ensuite cherché un auteur qui pourrait être inspiré par ces créations, avec l'. Une auteure leur a conseillé François Morel, qui est venu voir les peintures dans son atelier, et a ensuite proposé ses textes. De cette collaboration naît l'ouvrage "Hyacinthe et Rose", publié en 2010. Martin Jarrie indique, en 2012, que François Morel Lors de la publication de l'album, la critique du magazine "Télérama" mentionne : L'ouvrage obtient la « Mention » Prix Fiction de la Foire du livre de jeunesse de Bologne (Italie) en 2011 .

En 2013 est publié l'album-CD avec la lecture du texte par François Morel, mis en musique par Antoine Sahler, et, de 2013 à 2017, François Morel joue les textes de l'ouvrage sur scène, dans un spectacle au même titre, "Hyacinthe et Rose", accompagné par le musicien Antoine Sahler, dans de multiples tournées.

En 2012, Martin Jarrie est invité en résidence à Saint-Gratien (Val-d'Oise) pour un travail d'illustration autour de cette ville. Il s'intéresse alors à ses habitants, en rencontre quinze, leur demandant à . Il en tire des . Il , qui, cette fois encore, écrit des textes inspirés de ces peintures. Un ouvrage en sera publié en 2013 : "La Vie des gens".

















</doc>
<doc id="18003" url="https://fr.wikipedia.org/wiki?curid=18003" title="Le bonheur est dans le pré">
Le bonheur est dans le pré

Le bonheur est dans le pré est un film français réalisé par Étienne Chatiliez sorti en 1995. Le film raconte l'histoire d'un chef d'entreprise qui profite de son incroyable ressemblance avec un homme disparu depuis vingt-six ans pour prendre sa place et fuir ses problèmes.

Chef d'entreprise à Dole, dans le Jura, Francis Bergeade n'a pas une vie réjouissante : tourmenté par les employées de son usine de sièges de toilettes qui menacent de faire grève régulièrement, en butte à un contrôle fiscal, tenu pour quantité négligeable par son épouse, Nicole, et par sa fille, symbole d'une bourgeoisie provinciale, qui ne songe qu'à faire des dépenses inconsidérées, il n'a comme joie dans la vie que les moments passés au restaurant "Le bon laboureur" avec son meilleur ami, Gérard, le concessionnaire automobile de Dole. Ce dernier l'appelle affectueusement « Lapin ».

Les tracas s'accumulent tant qu'il finit par en faire un malaise. Hospitalisé, c'est pendant sa convalescence que se produit l'imprévisible : une fermière, Dolorès Thivart, et ses deux filles, venues de Condom, dans le Gers, participent à une émission de télévision pour retrouver leur mari et père, un certain Michel Thivart, disparu vingt-six ans plus tôt… Or, Michel Thivart est le sosie parfait de Francis ! Alors, est-ce lui ou n'est-ce pas lui ? Francis commence par nier, mais ni sa femme ni les médias ne le croient. Poussé par Gérard et voyant sa vie devenir un enfer, Francis décide de faire croire qu'il est bien le disparu. 

À sa grande surprise et malgré plusieurs évidences, Dolorès le reconnaît et l'accepte. Commence alors pour Francis une vie plaisante, simple, entourée d'une famille aimante. Dans le même temps, sa femme, sa fille et son gendre héritent de tous ses problèmes et, après l'avoir rejeté, finissent par implorer son aide. À cette occasion, Francis découvre la fortune cachée de Michel et s'en sert pour sauver son entreprise. Torturé par sa conscience, il avoue alors à Dolorès son imposture. Cette dernière lui répond qu'elle l'a toujours su mais qu'elle l'a accepté pour faire plaisir à sa famille et qu'elle le préfère au vrai Michel, un gangster violent. Francis et Gérard — qui entretemps a Nicole, ce qui semble convenir aussi bien à Francis qu'à cette dernière — mènent leur enquête et finissent par découvrir ce qui est arrivé au vrai Michel : après un ultime braquage qui a mal tourné, ce dernier s'est noyé dans un puits en tentant de faire disparaître des éléments compromettants. Francis, Dolorès et Nicole décident d'un commun accord de ne rien dire à leurs enfants respectifs et d'accepter chacun leur nouvelle vie.









</doc>

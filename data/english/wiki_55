<doc id="7346" url="https://en.wikipedia.org/wiki?curid=7346" title="Centimetre–gram–second system of units">
Centimetre–gram–second system of units

The centimetre–gram–second system of units (abbreviated CGS or cgs) is a variant of the metric system based on the centimetre as the unit of length, the gram as the unit of mass, and the second as the unit of time. All CGS mechanical units are unambiguously derived from these three base units, but there are several different ways of extending the CGS system to cover electromagnetism.

The CGS system has been largely supplanted by the MKS system based on the metre, kilogram, and second, which was in turn extended and replaced by the International System of Units (SI). In many fields of science and engineering, SI is the only system of units in use but there remain certain subfields where CGS is prevalent.

In measurements of purely mechanical systems (involving units of length, mass, force, energy, pressure, and so on), the differences between CGS and SI are straightforward and rather trivial; the unit-conversion factors are all powers of 10 as and . For example, the CGS unit of force is the dyne which is defined as , so the SI unit of force, the newton (), is equal to 100,000 dynes.

On the other hand, in measurements of electromagnetic phenomena (involving units of charge, electric and magnetic fields, voltage, and so on), converting between CGS and SI is more subtle. Formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on which system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit "sub-systems", including Gaussian units, "ESU", "EMU", and Lorentz–Heaviside units. Among these choices, Gaussian units are the most common today, and "CGS units" often used specifically refers to CGS-Gaussian units.

The CGS system goes back to a proposal in 1832 by the German mathematician Carl Friedrich Gauss to base a system of absolute units on the three fundamental units of length, mass and time. Gauss chose the units of millimetre, milligram and second. In 1873, a committee of the British Association for the Advancement of Science, including British physicists James Clerk Maxwell and William Thomson recommended the general adoption of centimetre, gram and second as fundamental units, and to express all derived electromagnetic units in these fundamental units, using the prefix "C.G.S. unit of ...".

The sizes of many CGS units turned out to be inconvenient for practical purposes. For example, many everyday objects are hundreds or thousands of centimetres long, such as humans, rooms and buildings. Thus the CGS system never gained wide general use outside the field of science. Starting in the 1880s, and more significantly by the mid-20th century, CGS was gradually superseded internationally for scientific purposes by the MKS (metre–kilogram–second) system, which in turn developed into the modern SI standard.

Since the international adoption of the MKS standard in the 1940s and the SI standard in the 1960s, the technical use of CGS units has gradually declined worldwide, in the United States more slowly than elsewhere. CGS units are today no longer accepted by the house styles of most scientific journals, textbook publishers, or standards bodies, although they are commonly used in astronomical journals such as "The Astrophysical Journal". CGS units are still occasionally encountered in technical literature, especially in the United States in the fields of material science, electrodynamics and astronomy. The continued usage of CGS units is most prevalent in magnetism and related fields because the B and H fields have the same units in free space and there is a lot of potential for confusion when converting published measurements from cgs to MKS.

The units gram and centimetre remain useful "as prefixed units" within the SI system, especially for instructional physics and chemistry experiments, where they match the small scale of table-top setups. However, where derived units are needed, the SI ones are generally used and taught instead of the CGS ones today. For example, a physics lab course might ask students to record lengths in centimetres, and masses in grams, but force (a derived unit) in newtons, a usage consistent with the SI system.

In mechanics, the CGS and SI systems of units are built in an identical way. The two systems differ only in the scale of two out of the three base units (centimetre versus metre and gram versus kilogram, respectively), while the third unit (second as the unit of time) is the same in both systems.

There is a one-to-one correspondence between the base units of mechanics in CGS and SI, and the laws of mechanics are not affected by the choice of units. The definitions of all derived units in terms of the three base units are therefore the same in both systems, and there is an unambiguous one-to-one correspondence of derived units:

Thus, for example, the CGS unit of pressure, barye, is related to the CGS base units of length, mass, and time in the same way as the SI unit of pressure, pascal, is related to the SI base units of length, mass, and time:

Expressing a CGS derived unit in terms of the SI base units, or vice versa, requires combining the scale factors that relate the two systems:

The conversion factors relating electromagnetic units in the CGS and SI systems are made more complex by the differences in the formulae expressing physical laws of electromagnetism as assumed by each system of units, specifically in the nature of the constants that appear in these formulae. This illustrates the fundamental difference in the ways the two systems are built: 

Electromagnetic relationships to length, time and mass may be derived by several equally appealing methods. Two of them rely on the forces observed on charges. Two fundamental laws relate (seemingly independently of each other) the electric charge or its rate of change (electric current) to a mechanical quantity such as force. They can be written in system-independent form as follows:


Maxwell's theory of electromagnetism relates these two laws to each other. It states that the ratio of proportionality constants formula_10 and formula_14 must obey formula_17, where "c" is the speed of light in vacuum. Therefore, if one derives the unit of charge from the Coulomb's law by setting formula_18 then Ampère's force law will contain a prefactor formula_19. Alternatively, deriving the unit of current, and therefore the unit of charge, from the Ampère's force law by setting formula_20 or formula_21, will lead to a constant prefactor in the Coulomb's law.

Indeed, both of these mutually exclusive approaches have been practiced by the users of CGS system, leading to the two independent and mutually exclusive branches of CGS, described in the subsections below. However, the freedom of choice in deriving electromagnetic units from the units of length, mass, and time is not limited to the definition of charge. While the electric field can be related to the work performed by it on a moving electric charge, the magnetic force is always perpendicular to the velocity of the moving charge, and thus the work performed by the magnetic field on any charge is always zero. This leads to a choice between two laws of magnetism, each relating magnetic field to mechanical quantities and electric charge:
These two laws can be used to derive Ampère's force law above, resulting in the relationship: formula_25. Therefore, if the unit of charge is based on the Ampère's force law such that formula_26, it is natural to derive the unit of magnetic field by setting formula_27. However, if it is not the case, a choice has to be made as to which of the two laws above is a more convenient basis for deriving the unit of magnetic field.

Furthermore, if we wish to describe the electric displacement field D and the magnetic field H in a medium other than vacuum, we need to also define the constants ε and μ, which are the vacuum permittivity and permeability, respectively. Then we have (generally) formula_28 and formula_29, where P and M are polarization density and magnetization vectors. The units of P and M are usually so chosen that the factors λ and λ′ are equal to the "rationalization constants" formula_30 and formula_31, respectively. If the rationalization constants are equal, then formula_32. If they are equal to one, then the system is said to be "rationalized": the laws for systems of spherical geometry contain factors of 4π (for example, point charges), those of cylindrical geometry – factors of 2π (for example, wires), and those of planar geometry contain no factors of π (for example, parallel-plate capacitors). However, the original CGS system used λ = λ′ = 4π, or, equivalently, formula_33. Therefore, Gaussian, ESU, and EMU subsystems of CGS (described below) are not rationalized.

The table below shows the values of the above constants used in some common CGS subsystems:
The constant "b" in SI system is a unit-based scaling factor defined as: formula_34.

Also, note the following correspondence of the above constants to those in Jackson and Leung:

In system-independent form, Maxwell's equations can be written as:

formula_39

Note that of all these variants, only in Gaussian and Heaviside–Lorentz systems formula_40 equals formula_41 rather than 1. As a result, vectors formula_42 and formula_43 of an electromagnetic wave propagating in vacuum have the same units and are equal in magnitude in these two variants of CGS.

In one variant of the CGS system, Electrostatic units (ESU), charge is defined via the force it exerts on other charges, and current is then defined as charge per time. It is done by setting the Coulomb force constant formula_44, so that Coulomb's law does not contain an explicit prefactor.

The ESU unit of charge, franklin (Fr), also known as statcoulomb or esu charge, is therefore defined as follows: Therefore, in electrostatic CGS units, a franklin is equal to a centimetre times square root of dyne:
The unit of current is defined as:

Dimensionally in the ESU CGS system, charge "q" is therefore equivalent to mLt. Hence, neither charge nor current is an independent physical quantity in ESU CGS. This reduction of units is the consequence of the Buckingham π theorem.

All electromagnetic units in ESU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix "stat" or with a separate abbreviation "esu".

In another variant of the CGS system, electromagnetic units (EMU), current is defined via the force existing between two thin, parallel, infinitely long wires carrying it, and charge is then defined as current multiplied by time. (This approach was eventually used to define the SI unit of ampere as well). In the EMU CGS subsystem, this is done by setting the Ampere force constant formula_47, so that Ampère's force law simply contains 2 as an explicit prefactor (this prefactor 2 is itself a result of integrating a more general formulation of Ampère's law over the length of the infinite wire).

The EMU unit of current, biot (Bi), also known as abampere or emu current, is therefore defined as follows:
The unit of charge in CGS EMU is:

Dimensionally in the EMU CGS system, charge "q" is therefore equivalent to mL. Hence, neither charge nor current is an independent physical quantity in EMU CGS.

All electromagnetic units in EMU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix "ab" or with a separate abbreviation "emu".

The ESU and EMU subsystems of CGS are connected by the fundamental relationship formula_17 (see above), where "c" = 29,979,245,800 ≈ 3⋅10 is the speed of light in vacuum in centimetres per second. Therefore, the ratio of the corresponding "primary" electrical and magnetic units (e.g. current, charge, voltage, etc. – quantities proportional to those that enter directly into Coulomb's law or Ampère's force law) is equal either to "c" or "c":
and
Units derived from these may have ratios equal to higher powers of "c", for example:

The practical cgs system is a hybrid system that uses the volt and the ampere as the unit of voltage and current respectively. Doing this avoids the inconveniently large and small quantities that arise for electromagnetic units in the esu and emu systems. This system was at one time widely used by electrical engineers because the volt and amp had been adopted as international standard units by the International Electrical Congress of 1881. As well as the volt and amp, the farad (capacitance), ohm (resistance), coulomb (electric charge), and henry are consequently also used in the practical system and are the same as the SI units. However, intensive properties (that is, anything that is per unit length, area, or volume) will not be the same as SI since the cgs unit of distance is the centimetre. For instance electric field strength is in units of volts per centimetre, magnetic field strength is in amps per centimetre, and resistivity is in ohm-cm.

Some physicists and electrical engineers in North America still use these hybrid units.

There were at various points in time about half a dozen systems of electromagnetic units in use, most based on the CGS system. These also include the Gaussian units and the Heaviside–Lorentz units.

In this table, "c" = 29,979,245,800 is the numeric value of the speed of light in vacuum when expressed in units of centimetres per second. The symbol "↔" is used instead of "=" as a reminder that the SI and CGS units are "corresponding" but not "equal" because they have incompatible dimensions. For example, according to the next-to-last row of the table, if a capacitor has a capacitance of 1 F in SI, then it has a capacitance of (10 "c") cm in ESU; "but" it is usually incorrect to replace "1 F" with "(10 "c") cm" within an equation or formula. (This warning is a special aspect of electromagnetism units in CGS. By contrast, for example, it is "always" correct to replace "1 m" with "100 cm" within an equation or formula.)

One can think of the SI value of the Coulomb constant "k" as:
This explains why SI to ESU conversions involving factors of "c" lead to significant simplifications of the ESU units, such as 1 statF = 1 cm and 1 statΩ = 1 s/cm: this is the consequence of the fact that in ESU system "k" = 1. For example, a centimetre of capacitance is the capacitance of a sphere of radius 1 cm in vacuum. The capacitance "C" between two concentric spheres of radii "R" and "r" in ESU CGS system is:
By taking the limit as "R" goes to infinity we see "C" equals "r".

While the absence of explicit prefactors in some CGS subsystems simplifies some theoretical calculations, it has the disadvantage that sometimes the units in CGS are hard to define through experiment. Also, lack of unique unit names leads to a great confusion: thus "15 emu" may mean either 15 abvolts, or 15 emu units of electric dipole moment, or 15 emu units of magnetic susceptibility, sometimes (but not always) per gram, or per mole. On the other hand, SI starts with a unit of current, the ampere, that is easier to determine through experiment, but which requires extra multiplicative factors in the electromagnetic equations. With its system of uniquely named units, the SI also removes any confusion in usage: 1.0 ampere is a fixed value of a specified quantity, and so are 1.0 henry, 1.0 ohm, and 1.0 volt.

A key virtue of the Gaussian CGS system is that electric and magnetic fields have the same units, 4"πϵ" is replaced by 1, and the only dimensional constant appearing in the Maxwell equations is "c", the speed of light. The Heaviside–Lorentz system has these desirable properties as well (with "ϵ" equaling 1), but it is a "rationalized" system (as is SI) in which the charges and fields are defined in such a way that there are many fewer factors of 4"π" appearing in the formulas, and it is in Heaviside–Lorentz units that the Maxwell equations take their simplest form.

In SI, and other rationalized systems (for example, Heaviside–Lorentz), the unit of current was chosen such that electromagnetic equations concerning charged spheres contain 4π, those concerning coils of current and straight wires contain 2π and those dealing with charged surfaces lack π entirely, which was the most convenient choice for applications in electrical engineering. However, modern hand calculators and personal computers have eliminated this "advantage". In some fields where formulas concerning spheres are common (for example, in astrophysics), it has been argued that the nonrationalized CGS system can be somewhat more convenient notationally.

Specialized unit systems are used to simplify formulas even further than "either" SI "or" CGS, by eliminating constants through some system of natural units. For example, in particle physics a system is in use where every quantity is expressed by only one unit of energy, the electronvolt, with lengths, times, and so on all converted into electronvolts by inserting factors of speed of light "c" and the Planck constant "ħ". This unit system is very convenient for calculations in particle physics, but it would be considered impractical in other contexts.




</doc>
<doc id="7355" url="https://en.wikipedia.org/wiki?curid=7355" title="Christology">
Christology

Christology (from Greek Χριστός "Khristós" and , "-logia") is the field of study within Christian theology which is primarily concerned with the ontology and person of Jesus as recorded in the canonical Gospels and the epistles of the New Testament.
Primary considerations include the ontology and person of Jesus in conjunction with His relationship with that of God the Father. As such, Christology is concerned with the details of Jesus' ministry, his acts and teachings, to arrive at a clearer understanding of who he is in his person, and his role in salvation. The views of Paul the Apostle provided a major component of the Christology of the Apostolic Age. Paul's central themes included the notion of the pre-existence of Christ and the worship of Christ as "Kyrios" (Greek: "Lord").

The pre-existence of Christ became a central theme of Christology. Proponents of Christ's deity argue the Old Testament has many cases of Christophany: "The pre-existence of Christ is further substantiated by the many recorded Christophanies in the Bible." "Christophany" is often considered a more accurate term than the term "theophany" due to the belief that all the visible manifestations of God are in fact the preincarnate Christ. Many argue that the appearances of "the Angel of the Lord" in the Old Testament were the preincarnate Christ. "Many understand the angel of the Lord as a true theophany. From the time of Justin on, the figure has been regarded as the preincarnate Logos."

Following the Apostolic Age, the early church engaged in fierce and often politicized debate on many interrelated issues. Christology became a major focus of these debates, and every one of the first seven ecumenical councils addressed Christological issues. The second through fourth of these councils are generally entitled "Christological councils", with the latter three mainly elucidating what was taught in them and condemning incorrect interpretations. The Council of Chalcedon in 451 issued a formulation of the being of Christ – that of two natures, one human and one divine, "united with neither confusion nor division". Chalcedonian Christianity – Eastern Orthodox, Roman Catholic, and many Protestant Christians – continue to advocate this doctrine of the hypostatic union. Due to politically-charged differences in the 4th century, schisms developed, and the first denominations (from the Latin, "to take a new name") formed.

In the 13th century, Saint Thomas Aquinas provided the first systematic Christology that consistently resolved a number of the existing issues. In his Christology from above, Aquinas also championed the principle of perfection of Christ's human attributes. The Middle Ages also witnessed the emergence of the "tender image of Jesus" as a friend and a living source of love and comfort, rather than just the "Kyrios" image. Catholic theologian Karl Rahner sees the purpose of modern Christology as to formulate the Christian belief that "God became man and that God-made-man is the individual Jesus Christ" in a manner that this statement can be understood consistently, without the confusions of past debates and mythologies.

Over the centuries, a number of terms and concepts have been developed within the framework of Christology to address the seemingly simple questions: "who was Jesus and what did he do?" A good deal of theological debate has ensued and significant schisms within Christian denominations took place in the process of providing answers to these questions. After the Middle Ages, systematic approaches to Christology were developed.

The term "Christology from above" refers to approaches that begin with the divinity and pre-existence of Christ as the "Logos" (the Word), as expressed in the . These approaches interpret the works of Christ in terms of his divinity. Christology from above was emphasized in the ancient Church, beginning with Ignatius of Antioch in the second century. The term "Christology from below", on the other hand, refers to approaches that begin with the human aspects and the ministry of Jesus (including the miracles, parables, etc.) and move towards his divinity and the mystery of incarnation.

The concept of "Cosmic Christology", first elaborated by Saint Paul, focuses on how the arrival of Jesus as the Son of God forever changed the nature of the cosmos. The terms "functional", "ontological" and "soteriological" have been used to refer to the perspectives that analyze the "works", the "being" and the "salvific" standpoints of Christology. Some essential sub-topics within the field of Christology include the incarnation, the resurrection, and salvation.

Other relevant topics of faith are: Christian messianic prophecies of the Old Testament, Annunciation, the regal Genealogy
and Transfiguration, Miracles (Lord of the creation), the Last Supper and institution of the Eucharist, the Passion and Crucifixion (INRI) , the doubting Thomas (Five Holy Wounds), the Harrowing of Hell, the Ascension and the Pentecost, the Kingship and Kingdom of God, the Rapture (Communion of Saints) and the Great Tribulation, the Second Coming of Christ and Last Judgement, the rising from the dead of all men.

The term "monastic Christology" has been used to describe spiritual approaches developed by Anselm of Canterbury, Peter Abelard and Bernard of Clairvaux. The Franciscan piety of the 12th and 13th centuries led to "popular Christology". Systematic approaches by theologians, such as Thomas Aquinas, are called "scholastic Christology".

Early Christians found themselves confronted with a set of new concepts and ideas relating to the life, death, and resurrection of Jesus, as well the notions of salvation and redemption, and had to use a new set of terms, images, and ideas in order to deal with them. The existing terms and structures which were available to them were often insufficient to express these religious concepts, and taken together, these new forms of discourse led to the beginnings of Christology as an attempt to understand, explain, and discuss their understanding of the nature of Christ.

Furthermore, as early Christians (following the Great Commission) had to explain their concepts to a new audience which
had at times been influenced by Greek philosophy, they had to present arguments that at times
resonated with, and at times confronted, the beliefs of that audience. A key example is the Apostle Paul's Areopagus sermon that appears in Acts 17:16–34. Here, the apostle attempted to convey the underlying concepts about Christ to a Greek audience, and the sermon illustrates some key elements of future Christological discourses that were first brought forward by Paul.

The title Kyrios for Jesus is central to the development of New Testament Christology, for the early Christians placed it at the center of their understanding, and from that center attempted to understand the other issues related to the Christian mysteries. The question of the deity of Christ in the New Testament is inherently related to the "Kyrios" title of Jesus used in the early Christian writings and its implications for the absolute lordship of Jesus. In early Christian belief, the concept of "Kyrios" included the pre-existence of Christ, for they believed if Christ is one with God, he must have been united with God from the very beginning.

In everyday Aramaic, "Mari" was a very respectful form of polite address, which means more than just "Teacher" and was somewhat similar to Rabbi. In Greek, this has at times been translated as "Kyrios". While the term "Mari" expressed the relationship between Jesus and his disciples during his life, the Greek "Kyrios" came to represent his lordship over the world.

No writings were left by Jesus, and the study of the various Christologies of the Apostolic Age is based on early Christian documents. The Gospels provide episodes from the life of Jesus and some of his works, but the authors of the New Testament show little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life, and as in , the Gospels do not claim to be an exhaustive list of his works.

Christologies that can be gleaned from the three Synoptic Gospels generally emphasize the humanity of Jesus, his sayings, his parables, and his miracles. The Gospel of John provides a different perspective that focuses on his divinity. The first 14 verses of the Gospel of John are devoted to the divinity of Jesus as the "Logos", usually translated as "Word", along with his pre-existence, and they emphasize the cosmic significance of Christ, e.g. John 1:3: "All things were made through him, and without him was not any thing made that was made." In the context of these verses, the Word made flesh is identical with the Word who was in the beginning with God, being exegetically equated with Jesus.

A foremost contribution to the Christology of the Apostolic Age is that of Paul. The central Christology of Paul conveys the notion of Christ's pre-existence and the identification of Christ as "Kyrios". The Pauline epistles use "Kyrios" to identify Jesus almost 230 times, and express the theme that the true mark of a Christian is the confession of Jesus as the true Lord. Paul viewed the superiority of the Christian revelation over all other divine manifestations as a consequence of the fact that Christ is the Son of God. Nevertheless, the view that it was apostle Paul who introduced the idea that Jesus was divine and thus distorted the actual Jesus has been widely rejected by historians. As Richard Bauckham observes, Paul was not so influential that he could have invented the central doctrine of Christianity. Before his active missionary work, there were already groups of Christians across the region. For example, a large group already existed in Rome even before Paul visited the place. The earliest centre of Christianity was the twelve apostles in Jerusalem. Paul himself consulted and sought guidance from the Christian leaders in Jerusalem (Galatians 2:1-2; Acts 9:26-28, 15:2). “What was common to the whole Christian movement derived from Jerusalem, not from Paul, and Paul himself derived the central message he preached from the Jerusalem apostles. On the other hand, if Jesus himself did not claim and show himself to be truly divine (i.e. on the Creator side of the Creator–creature divide), the earliest Christian leaders who were devout ancient monotheistic Jews would not have come to a widespread agreement that he was truly divine (which they did), but would have regarded Jesus as merely a teacher or a prophet instead.

The Pauline epistles also advanced the "cosmic Christology" later developed in the fourth gospel, elaborating the cosmic implications of Jesus' existence as the Son of God, as in Corinthians 5:17: "Therefore, if anyone is in Christ, he is a new creation. The old has passed away; behold, the new has come." Also, in Colossians 1:15: "He is the image of the invisible God, the firstborn of all creation."

Following the Apostolic Age, from the second century onwards, a number of controversies developed about how the human and divine are related within the person of Jesus. As of the second century, a number of different and opposing approaches developed among various groups. For example, Arianism did not endorse divinity, Ebionism argued Jesus was an ordinary mortal, while Gnosticism held docetic views which argued Christ was a spiritual being who only appeared to have a physical body. The resulting tensions led to schisms within the church in the second and third centuries, and ecumenical councils were convened in the fourth and fifth centuries to deal with the issues. Eventually, by the Ecumenical Council of Chalcedon in 451, the "Hypostatic union" was decreed—the proposition that Christ has one human nature "<nowiki>[</nowiki>physis<nowiki>]</nowiki>" and one divine nature "[physis]", united with neither confusion nor division—making this part of the creed of orthodox Christianity. Although some of the debates may seem to various modern students to be over a theological iota, they took place in controversial political circumstances, reflecting the relations of temporal powers and divine authority, and certainly resulted in schisms, among others that which separated the Church of the East from the Church of the Roman Empire.

In 325, the First Council of Nicaea defined the persons of the Godhead and their relationship with one another, decisions which were ratified at the First Council of Constantinople in 381. The language used was that the one God exists in three persons (Father, Son, and Holy Spirit); in particular, it was affirmed that the Son was "homoousios" (of the same being) as the Father. The Nicene Creed declared the full divinity and full humanity of Jesus.

In 431, the First Council of Ephesus was initially called to address the views of Nestorius on Mariology, but the problems soon extended to Christology, and schisms followed. The 431 council was called because in defense of his loyal priest Anastasius, Nestorius had denied the "Theotokos" title for Mary and later contradicted Proclus during a sermon in Constantinople. Pope Celestine I (who was already upset with Nestorius due to other matters) wrote about this to Cyril of Alexandria, who orchestrated the council. During the council, Nestorius defended his position by arguing there must be two persons of Christ, one human, the other divine, and Mary had given birth only to a human, hence could not be called the "Theotokos", i.e. "the one who gives birth to God". The debate about the single or dual nature of Christ ensued in Ephesus.

In 431, the Council of Ephesus debated miaphysitism (two natures united as one after the hypostatic union) verses dyophysitism (coexisting natures after the hypostatic union) versus monophysitism (only one nature) versus Nestorianism (two hypostases). From the Christological viewpoint, the council adopted "Mia Physis (But being made one κατὰ φύσιν)" - Council of Ephesus, Epistle of Cyril to Nestorius, i.e. One Nature of the Word of God Incarnate (μία φύσις τοῦ θεοῦ λόγου σεσαρκωμένη mía phýsis toû theoû lógou sesarkōménē). In 451, the Council of Chalcedon affirmed dyophysitism. The Oriental Orthodox rejected this and subsequent councils and continued to consider themselves as "miaphysite" according to the faith put forth at the Councils of Nicaea and Ephesus. The council also confirmed the "Theotokos" title and excommunicated Nestorius.

The 451 Council of Chalcedon was highly influential and marked a key turning point in the Christological debates that broke apart the church of the Eastern Roman Empire in the fifth century. It is the last council which many Anglicans and most Protestants consider ecumenical. It fully promulgated the Western dyophisite understanding put forth by Pope Leo I of Rome of the hypostatic union, stating the human and divine natures of Christ coexist after the union, yet each is distinct and complete. Most importantly, it unquestionably established the primacy of Rome in the East over those who accepted the Council of Chalcedon. This was reaffirmed in 519 when those Eastern Chalcedonians accepted the Formula of Hormisdas anathematizing all of their own Eastern Chalcedonian hierarchy who died out of communion with Rome from 482-519. Although, the Chalcedonian Creed did not put an end to all Christological debate, it did clarify the terms used and became a point of reference for many future Christologies. Most of the major branches of Western Christianity – Roman Catholicism, Eastern Orthodoxy, Anglicanism, Lutheranism, and Reformed – subscribe to the Chalcedonian Christological formulation, while many branches of Eastern Christianity - Syrian Orthodoxy, Assyrian Church, Coptic Orthodoxy, Ethiopian Orthodoxy, and Armenian Apostolicism – reject it.

The term Person of Christ refers to the prosopic (and hypostatic) union of the human and divine natures of Jesus Christ as they coexist within one person (prosopon) and one hypostasis. There are no direct discussions in the New Testament regarding the dual nature of the Person of Christ as both divine and human. Hence, since the early days of Christianity, theologians have debated various approaches to the understanding of these natures, at times resulting in schisms.

Historically in the Alexandrian school of thought (fashioned on the Gospel of John), Jesus Christ is the eternal "Logos" who already possesses unity with the Father before the act of Incarnation. In contrast, the Antiochian school views Christ as a single, unified human person apart from his relationship to the divine.

Some controversial notions of "two persons" (prosopic duality) caused heated debates among Christian theologians during the 5th century, resulting in official condemnation of such theological views. The Fourth Ecumenical Council, held in Chalcedon in 451, reaffirmed the notion of "One Person" of Jesus Christ, and formulated the famous Chalcedonian Definition with its "monoprosopic" (mono-prosopic: having one person) clauses, explicitly denying the validity of "dyoprosopic" (dyo-prosopic: having two persons) views.

John Calvin maintained there was no human element in the Person of Christ which could be separated from the Person of The Word. Calvin also emphasized the importance of the "Work of Christ" in any attempt at understanding the Person of Christ and cautioned against ignoring the Works of Jesus during his ministry.

The study of the Person of Christ continued into the 20th century, with modern theologians such as Karl Rahner and Hans von Balthasar. Rahner pointed out the coincidence between the Person of Christ and the Word of God, referring to and which state whoever is ashamed of the words of Jesus is ashamed of the Lord himself. Balthasar argued the union of the human and divine natures of Christ was achieved not by the "absorption" of human attributes, but by their "assumption". Thus, in his view, the divine nature of Christ was not affected by the human attributes and remained forever divine.

The Nativity of Jesus impacted the Christological issues about his Person from the earliest days of Christianity. Luke's Christology centers on the dialectics of the dual natures of the earthly and heavenly manifestations of existence of the Christ, while Matthew's Christology focuses on the mission of Jesus and his role as the savior. The salvific emphasis of later impacted the theological issues and the devotions to Holy Name of Jesus.

The accounts of the crucifixion and subsequent resurrection of Jesus provides a rich background for Christological analysis, from the canonical Gospels to the Pauline Epistles.

A central element in the Christology presented in the Acts of the Apostles is the affirmation of the belief that the death of Jesus by crucifixion happened "with the foreknowledge of God, according to a definite plan". In this view, as in , the cross is not viewed as a scandal, for the crucifixion of Jesus "at the hands of the lawless" is viewed as the fulfilment of the plan of God.

Paul's Christology has a specific focus on the death and resurrection of Jesus. For Paul, the crucifixion of Jesus is directly related to his resurrection and the term "the cross of Christ" used in Galatians 6:12 may be viewed as his abbreviation of the message of the gospels. For Paul, the crucifixion of Jesus was not an isolated event in history, but a cosmic event with significant eschatological consequences, as in Cor 2:8. In the Pauline view, Jesus, obedient to the point of death (Phil 2:8), died "at the right time" (Rom 4:25) based on the plan of God. For Paul, the "power of the cross" is not separable from the resurrection of Jesus.

The threefold office (Latin "munus triplex") of Jesus Christ is a Christian doctrine based upon the teachings of the Old Testament. It was described by Eusebius and more fully developed by John Calvin. It states that Jesus Christ performed three functions (or "offices") in his earthly ministry – those of prophet (Deuteronomy 18:14–22), priest (Psalm 110:1-4), and king (Psalm 2). In the Old Testament, the appointment of someone to any of these three positions could be indicated by anointing him or her by pouring oil over the head. Thus, the term messiah, meaning "anointed one", is associated with the concept of the threefold office. While the office of king is that most frequently associated with the Messiah, the role of Jesus as priest is also prominent in the New Testament, being most fully explained in chapters 7 to 10 of the Book of Hebrews.

Some Christians, notably Roman Catholics, view Mariology as a key component of Christology. In this view, not only is Mariology a logical and necessary consequence of Christology, but without it, Christology is incomplete, since the figure of Mary contributes to a fuller understanding of who Christ is and what he did.

Protestants have criticized Mariology because many of its assertions lack any biblical foundation. Strong Protestant reaction against Roman Catholic Marian devotion and teaching has been a significant issue for ecumenical dialogue.

Joseph Cardinal Ratzinger (later Pope Benedict XVI) expressed this sentiment about Roman Catholic Mariology when in two separate occasions he stated, "The appearance of a truly Marian awareness serves as the touchstone indicating whether or not the Christological substance is fully present" and "It is necessary to go back to Mary, if we want to return to the truth about Jesus Christ."




</doc>
<doc id="7357" url="https://en.wikipedia.org/wiki?curid=7357" title="Complaint">
Complaint

In legal terminology, a complaint is any formal legal document that sets out the facts and legal reasons (see: cause of action) that the filing party or parties (the plaintiff(s)) believes are sufficient to support a claim against the party or parties against whom the claim is brought (the defendant(s)) that entitles the plaintiff(s) to a remedy (either money damages or injunctive relief). For example, the Federal Rules of Civil Procedure (FRCP) that govern civil litigation in United States courts provide that a civil action is commenced with the filing or service of a pleading called a complaint. Civil court rules in states that have incorporated the Federal Rules of Civil Procedure use the same term for the same pleading. 

In Civil Law, a “complaint” is the very first formal action taken to officially begin a lawsuit. This written document contains the allegations against the defense, the specific laws violated, the facts that led to the dispute, and any demands made by the plaintiff to restore justice.

In some jurisdictions, specific types of criminal cases may also be commenced by the filing of a complaint, also sometimes called a criminal complaint or felony complaint. Most criminal cases are prosecuted in the name of the governmental authority that promulgates criminal statutes and enforces the police power of the state with the goal of seeking criminal sanctions, such as the State (also sometimes called the People) or Crown (in Commonwealth realms). In the United States, the complaint is often associated with misdemeanor criminal charges presented by the prosecutor without the grand jury process. In most U.S. jurisdictions, the charging instrument presented to and authorized by a grand jury is referred to as an indictment.

Virtually every U.S. state has some forms available on the web for most common complaints for lawyers and self-representing litigants; if a petitioner cannot find an appropriate form in their state, they often can modify a form from another state to fit his or her request. Several United States federal courts publish general guidelines for the petitioners and Civil Rights complaint forms.

A complaint generally has the following structural elements:

After the complaint has been filed with the court, it has to be properly served to the opposite parties, but usually petitioners are not allowed to serve the complaint personally. The court also can issue a summons - an official summary document which the plaintiff needs to have served together with the complaint. The defendants have limited time to respond, depending on the State or Federal rules. A defendant's failure to answer a complaint can result in a default judgment in favor of the petitioner.

For example, in United States federal courts, any person who is at least 18 years old and not a party may serve a summons and complaint in a civil case. The defendant must submit an answer within 21 days after being served with the summons and complaint, or request a waiver, according to FRCP Rule 12. After the civil complaint has been served to the defendants, the plaintiff must, as soon as practicable initiate a conference between the parties to plan for the rest of the discovery process and then the parties should submit a proposed discovery plan to the judge within 14 days after the conference.

In many U.S. jurisdictions, a complaint submitted to a court must be accompanied by a Case Information Statement, which sets forth specific key information about the case and the lawyers representing the parties. This allows the judge to make determinations about which deadlines to set for different phases of the case, as it moves through the court system.

There are also freely accessible web search engines to assist parties in finding court decisions that can be cited in the complaint as an example or analogy to resolve similar questions of law. Google Scholar is the biggest database of full text state and federal courts decisions that can be accessed without charge. These web search engines often allow one to select specific state courts to search.

Federal courts created the Public Access to Court Electronic Records (PACER) system to obtain case and docket information from the United States district courts, United States courts of appeals, and United States bankruptcy courts. The system is managed by the Administrative Office of the United States Courts; it allows lawyers and self-represented clients to obtain documents entered in the case much faster than regular mail.

In addition to Federal Rules of Civil Procedure, many of the U.S. district courts have developed their own requirements included in Local Rules for filing with the Court. Local Rules can set up a limit on the number of pages, establish deadlines for motions and responses, explain whether it is acceptable to combine a motion petition with a response, specify if a judge needs an additional copy of the documents (called "judge’s copy"), etc. Local Rules can define page layout elements like: margins, text font/size, distance between lines, mandatory footer text, page numbering, and provide directions on how the pages need to be bound together – i.e. acceptable fasteners, number and location of fastening holes, etc. If the filed motion does not comply with the Local Rules then the judge can choose to strike the motion completely, or order the party to re-file its motion, or grant a special exception to the Local Rules.

According to Federal Rules of Civil Procedure (FRCP) , sensitive text like Social Security number, Taxpayer Identification Number, birthday, bank accounts and children’s names, should be redacted from the filings made with the court and accompanying exhibits, (however, exhibits normally do not need to be attached to the original complaint, but should be presented to Court after the discovery). The redacted text can be erased with black-out or white-out, and the page should have an indication that it was redacted - most often by stamping word "redacted" on the bottom. Alternately, the filing party may ask the court’s permission to file some exhibits completely under seal. A minor's name of the petitions should be replaced with initials.

A person making a redacted filing can file an unredacted copy under seal, or the Court can choose to order later that an additional filing be made under seal without redaction. Copies of both redacted and unredacted documents filed with court should be provided to the other parties in the case. Some courts also require that an additional electronic courtesy copy be emailed to the other parties.

Before filing the complaint, it is important for plaintiffs to remember that Federal courts can impose liability for the prevailing party's attorney fees to the losing party, if the judge considers the case frivolous or for purposes of harassment, even when the case was voluntarily dismissed. In the case of Fox v. Vice, the U.S. Supreme Court held that reasonable attorneys' fees could be awarded to the defendant under 42 U.S.C. Sec. 1988, but only for costs that the defendant would not have incurred "but for the frivolous claims." Even when there is no actual trial or judgment, if there is only pre-trial motion practice such as motions to dismiss, attorney fee shifting still can be awarded under FRCP Rule 11 when the opposing party files a Motion for Sanctions and the court issue an order identifying the sanctioned conduct and the basis for the sanction. The losing party has a right to appeal any order for sanctions in the higher court. In the state courts, however, each party is generally responsible only for its own attorney fees, with certain exceptions.




</doc>
<doc id="7362" url="https://en.wikipedia.org/wiki?curid=7362" title="Casimir III the Great">
Casimir III the Great

Casimir III the Great (; 30 April 1310 – 5 November 1370) reigned as the King of Poland from 1333 to 1370. He was the son of King Władysław I ("the Elbow-high") and Duchess Jadwiga of Kalisz, and the last Polish king from the Piast dynasty.

Kazimierz inherited a kingdom weakened by war and made it prosperous and wealthy. He reformed the Polish army and doubled the size of the kingdom. He reformed the judicial system and introduced a legal code, gaining the title "the Polish Justinian." Kazimierz built extensively and founded the University of Kraków, the oldest Polish university. He also confirmed privileges and protections previously granted to Jews and encouraged them to settle in Poland in great numbers.

Kazimierz left no lawful male heir to his throne, producing only daughters. When Kazimierz died in 1370 from an injury received while hunting, his nephew, King Louis I of Hungary, succeeded him as king of Poland in personal union with Hungary.

When Kazimierz attained the throne in 1333, his position was in danger, as his neighbours did not recognise his title and instead called him "king of Kraków". The kingdom was depopulated and exhausted by war, and the economy was ruined. In 1335, in the Treaty of Trentschin, Casimir was forced to relinquish his claims to Silesia "in perpetuity".

Kazimierz rebuilt and his kingdom became prosperous and wealthy, with great prospects for the future. He waged many victorious wars and doubled the size of the kingdom, mostly through addition of lands in modern-day Ukraine (then called the Duchy of Halych). Kazimierz built extensively during his reign, ordering the construction of over 40 castles, including many castles along the Trail of the Eagle's Nests, and he reformed the Polish army.

At the Sejm in Wiślica, on 11 March 1347, Kazimierz introduced reforms to the Polish judicial system and sanctioned civil and criminal codes for Great and Lesser Poland, earning the title "the Polish Justinian." He founded the University of Kraków, the oldest Polish University, and he organized a meeting of kings in Kraków in 1364 at which he exhibited the wealth of the Polish kingdom. Kazimierz is the only king in Polish history to both receive and retain the title of "Great" (Bolesław I Chrobry is also called "Great", but more commonly "Valiant").

In 1355, in Buda, Kazimierz designated his nephew Louis I of Hungary as his successor should he produce no male heir, as his father had with Charles I of Hungary to gain his help against Bohemia. In exchange Kazimierz gained Hungarian favourable attitude, needed in disputes with the hostile Teutonic Order and Kingdom of Bohemia. Kazimierz at the time was still in his early years and having a son did not seem to be a problem (he already had a few children).

Kazimierz left no legal son, however, begetting five daughters instead. He tried to adopt his grandson, Casimir IV, Duke of Pomerania, in his last will. The child had been born to his second daughter, Elisabeth, Duchess of Pomerania, in 1351. This part of the testament was invalidated by Louis I of Hungary, however, who had traveled to Kraków quickly after Kazimierz died and bribed the nobles with future privileges. Kazimierz III had a son-in-law, Louis VI of Bavaria, Margrave and Prince-elector of Brandenburg, who was considered a possible successor, but he was deemed ineligible as his wife, Kazimierz's daughter Cunigunde, had died in 1357 without issue. 
Thus King Louis I of Hungary became successor in Poland. Louis was proclaimed king upon Kazimierz's death in 1370, though Kazimierz's sister Elisabeth (Louis's mother) held much of the real power until her death in 1380.

Casimir was facetiously named "the Peasants' King". He introduced the codes of law of Greater and Lesser Poland as an attempt to end the overwhelming superiority of the nobility. During his reign all three major classes — the nobility, priesthood, and bourgeoisie — were more or less counterbalanced, allowing Casimir to strengthen his monarchic position. He was known for siding with the weak when the law did not protect them from nobles and clergymen. He reportedly even supported a peasant whose house had been demolished by his own mistress, after she had ordered it to be pulled down because it disturbed her enjoyment of the beautiful landscape.

Due to his deep relationship with the legendary Esterka who played a significant role in the King's life, Casimir was favorably disposed toward Jews living in Poland. On 9 October 1334, he confirmed the privileges granted to Jews in 1264 by Bolesław V the Chaste. Under penalty of death, he prohibited the kidnapping of Jewish children for the purpose of enforced Christian baptism, and he inflicted heavy punishment for the desecration of Jewish cemeteries. While Jews had lived in Poland since before his reign, Casimir allowed them to settle in Poland in great numbers and protected them as "people of the king".

Casimir III was born in Kowal, and he married four times. Casimir first married Anna, or Aldona Ona, the daughter of Grand Duke Gediminas of Lithuania. The marriage produced two daughters, Cunigunde (d. 1357), who was married to Louis VI the Roman, the son of Louis IV, Holy Roman Emperor, and Elisabeth, who was married to Duke Bogislaus V of Pomerania. Aldona died in 1339, and Casimir then married Adelaide of Hesse. He divorced Adelaide in 1356, married Christina, divorced her, and while Adelaide and possibly Christina were still alive (ca. 1365), he married Hedwig of Głogów and Sagan. He had three daughters by his fourth wife, and they were still very young when he died, and regarded as of dubious legitimacy because of Casimir's bigamy.

On 30 April or 16 October 1325, Casimir married Aldona of Lithuania. She was a daughter of Gediminas of Lithuania and Jewna. They had two children:

Aldona died on 26 May 1339. Casimir remained a widower for two years.

On 29 September 1341, Casimir married his second wife, Adelaide of Hesse. She was a daughter of Henry II, Landgrave of Hesse, and Elizabeth of Meissen. They had no children. Casimir started living separately from Adelaide soon thereafter. Their loveless marriage lasted until 1356. 

Casimir effectively divorced Adelaide and married his mistress Christina Rokiczana, the widow of Miklusz Rokiczani, a wealthy merchant. Her own origins are unknown. Following the death of her first husband she had entered the court of Bohemia in Prague as a lady-in-waiting. Casimir brought her with him from Prague and convinced the abbot of the Benedictine abbey of Tyniec to marry them. The marriage was held in a secret ceremony but soon became known. Queen Adelaide renounced it as bigamous and returned to Hesse without permission. Casimir continued living with Christine despite complaints by Pope Innocent VI on behalf of Queen Adelaide. The marriage lasted until 1363–64 when Casimir again declared himself divorced. They had no children. 

In about 1365, Casimir married his fourth wife Hedwig of Żagań. She was a daughter of Henry V of Iron, Duke of Żagań and Anna of Mazovia. They had three children:

With Adelheid still alive and Christina possibly surviving, the marriage to Hedwig was also considered bigamous. The legitimacy of the three last daughters was disputed. Casimir managed to have Anne and Cunigunde legitimated by Pope Urban V on 5 December 1369. Jadwiga the younger was legitimated by Pope Gregory XI on 11 October 1371.

Esterka was the only one who gave him male offspring. She had a significant place in Casimir's life. She was a legendarily beautiful and intelligent woman who even performed as a king's adviser in support of various initiatives: building stone cities, tolerance to representatives of different religious faiths, free trade and support of cultural development.

It was she who laid the foundations of a tolerant attitude towards Jews in Poland and it remained so for centuries, making this country "a paradise for the Jews". Casimir was not only loyal to the Jews, but also encouraged them, as a result of it the country experienced phenomenal economic and cultural growth. Casimir was called The Great King for his wisdom. The sons of King Casimir and Esterka, Pelko and Nemir, were baptized at the request of their father and became the ancestors of several Polish noble families: Rudanovsky and . To develop legal and commercial relations between Jews, Poles and Germans, Pelko was sent to Konitz and his brother Nemir in 1363 to the southwest to Lower Silesia to participate in the foundation of the city of Neurode, which later became the patrimonial nest of the new Nourode's Rudanovsky dynasty.

Casimir's full title was: "Casimir by the grace of God king of Poland and Russia (Ruthenia), lord and heir of the land of Kraków, Sandomierz, Sieradz, Łęczyca, Kuyavia, Pomerania (Pomerelia)". The title in Latin was: "Kazimirus, Dei gratia rex Polonie et Russie, nec non Cracovie, Sandomirie, Siradie, Lancicie, Cuiavie, et Pomeranieque Terrarum et Ducatuum Dominus et Heres."





 


</doc>
<doc id="7363" url="https://en.wikipedia.org/wiki?curid=7363" title="Complexity">
Complexity

Complexity characterises the behaviour of a system or model whose components interact in multiple ways and follow local rules, meaning there is no reasonable higher instruction to define the various possible interactions.

The stem of the word "complexity" - "complex" - combines the Latin roots "com" (meaning "together") and "plex" (meaning "woven"). Contrast "complicated" where "plic" (meaning "folded") refers to many layers. A complex system is thereby characterised by its inter-dependencies, whereas a complicated system is characterised by its layers.

Complexity is generally used to characterize something with many parts where those parts interact with each other in multiple ways, culminating in a higher order of emergence greater than the sum of its parts. Just as there is no absolute definition of "intelligence", there is no absolute definition of "complexity"; the only consensus among researchers is that there is no agreement about the specific definition of complexity. However, "a characterization of what is complex is possible". The study of these complex linkages at various scales is the main goal of complex systems theory.

Science takes a number of approaches to characterizing complexity; Zayed "et al."
reflect many of these. Neil Johnson states that "even among scientists, there is no unique definition of complexity – and the scientific notion has traditionally been conveyed using particular examples..." Ultimately Johnson adopts the definition of "complexity science" as "the study of the phenomena which emerge from a collection of interacting objects".

Definitions of complexity often depend on the concept of a confidential "system" – a set of parts or elements that have relationships among them differentiated from relationships with other elements outside the relational regime. Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system and numerous forms of relationships among the elements. However, what one sees as complex and what one sees as simple is relative and changes with time.

Warren Weaver posited in 1948 two forms of complexity: disorganized complexity, and organized complexity.
Phenomena of 'disorganized complexity' are treated using probability theory and statistical mechanics, while 'organized complexity' deals with phenomena that escape such approaches and confront "dealing simultaneously with a sizable number of factors which are interrelated into an organic whole". Weaver's 1948 paper has influenced subsequent thinking about complexity.

The approaches that embody concepts of systems, multiple elements, multiple relational regimes, and state spaces might be summarized as implying that complexity arises from the number of distinguishable relational regimes (and their associated state spaces) in a defined system.

Some definitions relate to the algorithmic basis for the expression of a complex phenomenon or model or mathematical expression, as later set out herein.

One of the problems in addressing complexity issues has been formalizing the intuitive conceptual distinction between the large number of variances in relationships extant in random collections, and the sometimes large, but smaller, number of relationships between elements in systems where constraints (related to correlation of otherwise independent elements) simultaneously reduce the variations from element independence and create distinguishable regimes of more-uniform, or correlated, relationships, or interactions.

Weaver perceived and addressed this problem, in at least a preliminary way, in drawing a distinction between "disorganized complexity" and "organized complexity".

In Weaver's view, disorganized complexity results from the particular system having a very large number of parts, say millions of parts, or many more. Though the interactions of the parts in a "disorganized complexity" situation can be seen as largely random, the properties of the system as a whole can be understood by using probability and statistical methods.

A prime example of disorganized complexity is a gas in a container, with the gas molecules as the parts. Some would suggest that a system of disorganized complexity may be compared with the (relative) simplicity of planetary orbits – the latter can be predicted by applying Newton's laws of motion. Of course, most real-world systems, including planetary orbits, eventually become theoretically unpredictable even using Newtonian dynamics; as discovered by modern chaos theory.

Organized complexity, in Weaver's view, resides in nothing else than the non-random, or correlated, interaction between the parts. These correlated relationships create a differentiated structure that can, as a system, interact with other systems. The coordinated system manifests properties not carried or dictated by individual parts. The organized aspect of this form of complexity vis-a-vis to other systems than the subject system can be said to "emerge," without any "guiding hand".

The number of parts does not have to be very large for a particular system to have emergent properties. A system of organized complexity may be understood in its properties (behavior among the properties) through modeling and simulation, particularly modeling and simulation with computers. An example of organized complexity is a city neighborhood as a living mechanism, with the neighborhood people among the system's parts.

There are generally rules which can be invoked to explain the origin of complexity in a given system.

The source of disorganized complexity is the large number of parts in the system of interest, and the lack of correlation between elements in the system.

In the case of self-organizing living systems, usefully organized complexity comes from beneficially mutated organisms being selected to survive by their environment for their differential reproductive ability or at least success over inanimate matter or less organized complex organisms. See e.g. Robert Ulanowicz's treatment of ecosystems.

Complexity of an object or system is a relative property. For instance, for many functions (problems), such a computational complexity as time of computation is smaller when multitape Turing machines are used than when Turing machines with one tape are used. Random Access Machines allow one to even more decrease time complexity (Greenlaw and Hoover 1998: 226), while inductive Turing machines can decrease even the complexity class of a function, language or set (Burgin 2005). This shows that tools of activity can be an important factor of complexity.

In several scientific fields, "complexity" has a precise meaning:


Other fields introduce less precisely defined notions of complexity:


Complexity has always been a part of our environment, and therefore many scientific fields have dealt with complex systems and phenomena. From one perspective, that which is somehow complex – displaying variation without being random – is most worthy of interest given the rewards found in the depths of exploration.

The use of the term complex is often confused with the term complicated. In today's systems, this is the difference between myriad connecting "stovepipes" and effective "integrated" solutions. This means that complex is the opposite of independent, while complicated is the opposite of simple.

While this has led some fields to come up with specific definitions of complexity, there is a more recent movement to regroup observations from different fields to study complexity in itself, whether it appears in anthills, human brains, or stock markets, social systems. One such interdisciplinary group of fields is relational order theories.

The behavior of a complex system is often said to be due to emergence and self-organization. Chaos theory has investigated the sensitivity of systems to variations in initial conditions as one cause of complex behaviour.

Recent developments around artificial life, evolutionary computation and genetic algorithms have led to an increasing emphasis on complexity and complex adaptive systems.

In social science, the study on the emergence of macro-properties from the micro-properties, also known as macro-micro view in sociology. The topic is commonly recognized as social complexity that is often related to the use of computer simulation in social science, i.e.: computational sociology.

Systems theory has long been concerned with the study of complex systems (in recent times, "complexity theory" and "complex systems" have also been used as names of the field). These systems are present in the research of a variety disciplines, including biology, economics, social studies and technology. Recently, complexity has become a natural domain of interest of real world socio-cognitive systems and emerging systemics research. Complex systems tend to be high-dimensional, non-linear, and difficult to model. In specific circumstances, they may exhibit low-dimensional behaviour.

In information theory, algorithmic information theory is concerned with the complexity of strings of data.

Complex strings are harder to compress. While intuition tells us that this may depend on the codec used to compress a string (a codec could be theoretically created in any arbitrary language, including one in which the very small command "X" could cause the computer to output a very complicated string like "18995316"), any two Turing-complete languages can be implemented in each other, meaning that the length of two encodings in different languages will vary by at most the length of the "translation" language – which will end up being negligible for sufficiently large data strings.

These algorithmic measures of complexity tend to assign high values to random noise. However, those studying complex systems would not consider randomness as complexity.

Information entropy is also sometimes used in information theory as indicative of complexity.

Recent work in machine learning has examined the complexity of the data as it affects the performance of supervised classification algorithms. Ho and Basu present a set of complexity measures for binary classification problems.

The complexity measures broadly cover:
Instance hardness is a bottom-up approach that first seeks to identify instances that are likely to be misclassified (or, in other words, which instances are the most complex). The characteristics of the instances that are likely to be misclassified are then measured based on the output from a set of hardness measures. The hardness measures are based on several supervised learning techniques such as measuring the number of disagreeing neighbors or the likelihood of the assigned class label given the input features. The information provided by the complexity measures has been examined for use in meta learning to determine for which data sets filtering (or removing suspected noisy instances from the training set) is the most beneficial and could be expanded to other areas.

A recent study based on molecular simulations and compliance constants describes molecular recognition as a phenomenon of organisation.
Even for small molecules like carbohydrates, the recognition process can not be predicted or designed even assuming that each individual hydrogen bond's strength is exactly known.

Computational complexity theory is the study of the complexity of problems – that is, the difficulty of solving them. Problems can be classified by complexity class according to the time it takes for an algorithm – usually a computer program – to solve them as a function of the problem size. Some problems are difficult to solve, while others are easy. For example, some difficult problems need algorithms that take an exponential amount of time in terms of the size of the problem to solve. Take the travelling salesman problem, for example. It can be solved in time formula_1 (where "n" is the size of the network to visit – the number of cities the travelling salesman must visit exactly once). As the size of the network of cities grows, the time needed to find the route grows (more than) exponentially.

Even though a problem may be computationally solvable in principle, in actual practice it may not be that simple. These problems might require large amounts of time or an inordinate amount of space. Computational complexity may be approached from many different aspects. Computational complexity can be investigated on the basis of time, memory or other resources used to solve the problem. Time and space are two of the most important and popular considerations when problems of complexity are analyzed.

There exist a certain class of problems that although they are solvable in principle they require so much time or space that it is not practical to attempt to solve them. These problems are called intractable.

There is another form of complexity called hierarchical complexity. It is orthogonal to the forms of complexity discussed so far, which are called horizontal complexity.



</doc>
<doc id="7366" url="https://en.wikipedia.org/wiki?curid=7366" title="Chastity">
Chastity

Chastity is sexual conduct of a person that is deemed praiseworthy and virtuous according to the moral standards and guidelines of their culture, civilization or religion. The term has become closely associated (and is often used interchangeably) with sexual abstinence, especially before marriage and outside marriage.

The words "chaste" and "chastity" stem from the Latin adjective "castus" meaning "pure". The words entered the English language around the middle of the 13th century; at that time they meant slightly different things. "Chaste" meant "virtuous or pure from unlawful sexual intercourse" (referring to extramarital sex),
while "chastity" meant "virginity". It was not until the late 16th century that the two words came to have the same basic meaning as a related adjective and noun.

For many Muslims and Christians, acts of sexual nature are restricted to marriage. For unmarried persons, chastity is identified with sexual abstinence. Sexual acts outside or apart from marriage, such as adultery, fornication, and prostitution, are considered sinful.

In many Christian traditions, chastity is synonymous with sexual purity. Chastity means not having any sexual relations before marriage. It also means fidelity to husband or wife during marriage. In Catholic morality, chastity is placed opposite the deadly sin of lust, and is classified as one of seven virtues. The moderation of sexual desires is required to be virtuous. Reason, will and desire can harmoniously work together to do what is good.

In marriage, the spouses commit to a lifelong relationship which excludes sexual intimacy with other persons. After marriage, a third form of chastity, often called "vidual chastity", is expected of a woman while she is in mourning for her late husband. For example, Anglican Bishop Jeremy Taylor defined 5 rules in "Holy Living" (1650), including abstaining from marrying "so long as she is with child by her former husband" and "within the year of mourning".

The particular ethical system may not prescribe each of these. For example, Roman Catholics view sex within marriage as chaste, but prohibit the use of artificial contraception as an offense against chastity, seeing contraception as unnatural, contrary to God's will and design of human sexuality. Many Anglican communities allow for artificial contraception, seeing the restriction of family size by artificial contraception as possibly not contrary to God's will. A stricter view is held by the Shakers, who prohibit marriage (and sexual intercourse under any circumstances) as a violation of chastity. The Catholic Church has set up various rules regarding clerical celibacy, while most Protestant communities allow clergy to marry.

Celibacy is required of monastics—monks, nuns and friars—even in a rare system of double cloisters, in which husbands could enter the (men's) monastery while their wives entered a (women's) sister monastery. Required celibacy among the clergy is a relatively recent practice: it became Church policy at the Second Lateran Council in 1139. It was not uniformly enforced among the clergy until 200 years later. Eastern Catholic priests are permitted to be married, provided they are so before ordination and outside the monastic life.

"Vows of chastity" can also be taken by laypersons, either as part of an organised religious life (such as Roman Catholic Beguines and Beghards in the past) or on an individual basis: as a voluntary act of devotion, or as part of an ascetic lifestyle (often devoted to contemplation), or both. Some protestant religious communities, such as the Bruderhof, take vows of chastity as part of the church membership process.

The voluntary aspect has led it to being included among the main counsels of perfection.

Chastity is a central and pivotal concept in Roman Catholic praxis. Chastity's importance in traditional Roman Catholic teaching stems from the fact that it is regarded as essential in maintaining and cultivating the unity of body with spirit and thus the integrity of the human being. It is also regarded as fundamental to the practise of the Catholic life because it involves an "apprenticeship in self-mastery". By attaining mastery over one's passions, reason, will and desire can harmoniously work together to do what is good.

In The Church of Jesus Christ of Latter-day Saints (LDS Church), chastity is very important, quoting:

"Physical intimacy between husband and wife is a beautiful and sacred part of God's plan for His children. It is an expression of love within marriage and allows husband and wife to participate in the creation of life. God has commanded that this sacred power be expressed only between a man and a woman who are legally married. The law of chastity applies to both men and women. It includes strict abstinence from sexual relations before marriage and complete fidelity and loyalty to one's spouse after marriage.

"The law of chastity requires that sexual relations be reserved for marriage between a man and a woman.

"In addition to reserving sexual intimacy for marriage, we obey the law of chastity by controlling our thoughts, words, and actions. Jesus Christ taught,"Ye have heard that it was said by them of old time, Thou shalt not commit adultery: but I say unto you, That whosoever looketh on a woman to lust after her hath committed adultery with her already in his heart" (Matthew 5:27–28)."

LDS teaching also includes that sexual expression within marriage is an important dimension of spousal bonding apart from, but not necessarily avoiding its procreative result.

Chastity is mandatory in Islam. Sex outside legitimacy is prohibited, for both men and women whether married or unmarried.

The most famous personal example of chastity in the Quran is Virgin Mary (Mariam) :

"And [mention] the one who guarded her chastity, so We blew into her [garment] through Our angel [Gabriel], and We made her and her son a sign for the worlds." (21 :91)

"And she took, in seclusion from them, a screen. Then We sent to her Our Angel, and he represented himself to her as a well-proportioned man.She said, "Indeed, I seek refuge in the Most Merciful from you, [so leave me], if you should be fearing of Allah ."He said, "I am only the messenger of your Lord to give you [news of] a pure boy."She said, "How can I have a boy while no man has touched me and I have not been unchaste?"" (19:17-20)

Extramarital sex is forbidden and the Quran says:

"And do not approach unlawful sexual intercourse. Indeed, it is ever an immorality and is evil as a way." (17:32)

"And those who do not invoke with Allah another deity or kill the soul which Allah has forbidden [to be killed], except by right, and do not commit unlawful sexual intercourse. And whoever should do that will meet a penalty.Multiplied for him is the punishment on the Day of Resurrection, and he will abide therein humiliated -Except for those who repent, believe and do righteous work. For them Allah will replace their evil deeds with good. And ever is Allah Forgiving and Merciful." (25:68-70)

The injunctions and forbiddings in Islam apply equally to men and women. The legal punishment for adultery is equal for men and women. Social hypocrisy in many societies over history had led to a double standard when considering sin committed by men versus sin committed by women. Society tended to be more lenient and permissive towards men forgiving men for sins not forgivable when women do them. At the root of the contemporary wave of free sex for both sexes was apparently to establish equality and remove discrimination between man and woman. Instead of calling for equal cleanliness the call was for equal dirt. Not so in Islam. and in a list of commendable deeds the Quran says:

"Indeed, the Muslim men and Muslim women, the believing men and believing women, the obedient men and obedient women, the truthful men and truthful women, the patient men and patient women, the humble men and humble women, the charitable men and charitable women, the fasting men and fasting women, the men who guard their private parts and the women who do so, and the men who remember Allah often and the women who do so - for them Allah has prepared forgiveness and a great reward." (33:35)

Because the sex desire is usually attained before a man is financially capable of marriage, the love to God and mindfulness of Him should be sufficient motive for chastity:

"But let them who find not [the means for] marriage abstain [from sexual relations] until Allah enriches them from His bounty. And those who seek a contract [for eventual emancipation] from among whom your right hands possess - then make a contract with them if you know there is within them goodness and give them from the wealth of Allah which He has given you. And do not compel your slave girls to prostitution, if they desire chastity, to seek [thereby] the temporary interests of worldly life. And if someone should compel them, then indeed, Allah is [to them], after their compulsion, Forgiving and Merciful." (24:33) The prophet's prescription to the youth was:

Those of you who own the means should marry for this should keep their eyes uncraving and their chastity secure. Those who don't, may practise fasting for it curbs desire. " (Ibn Massoud)

Chastity is an attitude and a way of life. In Islam it is both a personal and a social value. A Muslim society should not condone relations entailing or conducive to sexual license. Social patterns and practices bent on flaring up the sexual desire are frowned upon by Islam be the means permissive ideologies, pruritic art or neglect of moral upbringing. Personal freedoms should never be taught as the freedom to challenge God's injunctions or trespass over the limits He drew.

Chastity is highly prized in the Bahá'í Faith. Similar to other Abrahamic religions, Bahá'í teachings call for the restriction of sexual activity to that between a wife and husband in Bahá'í marriage, and discourage members from using pornography or engaging in sexually explicit recreational activities. The concept of chastity is extended to include avoidance of alcohol and mind-altering drugs, profanity, and gaudy or immoderate attire.

Hinduism's view on premarital sex is rooted in its concept of Ashrama (stage) or the stages of life. The first of these stages, known as "Brahmacharya," roughly translates as chastity. Celibacy and chastity are considered the appropriate behavior for both male and female students during this stage, which precedes the stage of the married householder (Grihastha). Sanyasis and Hindu monks or Sadhus are also celibate as part of their ascetic discipline.

In Sikhism, premarital or extra-marital sex is strictly forbidden. However, it is encouraged to marry and live as a family unit to provide and nurture children for the perpetual benefit of creation (as opposed to sannyasa or living as a monk, which was, and remains, a common spiritual practice in India). A Sikh is encouraged not to live as a recluse, beggar, monk, nun, celibate, or in any similar vein.

Celibacy is a must for all Jain monks and nuns. Chastity (Bhramacharya) is one of the five major vows of Jainism. The general Jain code of ethics requires that one do no harm to any living being in thought, action, or word. Adultery is clearly a violation of a moral agreement with one's spouse, and therefore forbidden, and fornication too is seen as a violation of the state of chastity.

The teachings of Buddhism include the Noble Eightfold Path, comprising a division called right action. Under the Five Precepts ethical code, Upāsaka and Upāsikā lay followers should abstain from sexual misconduct, while Bhikkhu and Bhikkhuni monastics should practice strict chastity.

The Five Precepts of the Daoist religion include No Sexual Misconduct, which is interpreted as prohibiting extramarital sex for lay practitioners and marriage or sexual intercourse for monks and nuns.



</doc>
<doc id="7376" url="https://en.wikipedia.org/wiki?curid=7376" title="Cosmic microwave background">
Cosmic microwave background

The cosmic microwave background (CMB) is electromagnetic radiation as a remnant from an early stage of the universe in Big Bang cosmology. In older literature, the CMB is also variously known as cosmic microwave background radiation (CMBR) or "relic radiation". The CMB is a faint cosmic background radiation filling all space that is an important source of data on the early universe because it is the oldest electromagnetic radiation in the universe, dating to the epoch of recombination. With a traditional optical telescope, the space between stars and galaxies (the "background") is completely dark. However, a sufficiently sensitive radio telescope shows a faint background noise, or glow, almost isotropic, that is not associated with any star, galaxy, or other object. This glow is strongest in the microwave region of the radio spectrum. The accidental discovery of the CMB in 1964 by American radio astronomers Arno Penzias and Robert Wilson was the culmination of work initiated in the 1940s, and earned the discoverers the 1978 Nobel Prize in Physics.

The discovery of CMB is landmark evidence of the Big Bang origin of the universe. When the universe was young, before the formation of stars and planets, it was denser, much hotter, and filled with a uniform glow from a white-hot fog of hydrogen plasma. As the universe expanded, both the plasma and the radiation filling it grew cooler. When the universe cooled enough, protons and electrons combined to form neutral hydrogen atoms. Unlike the uncombined protons and electrons, these newly conceived atoms could not absorb the thermal radiation, and so the universe became transparent instead of being an opaque fog. Cosmologists refer to the time period when neutral atoms first formed as the "recombination epoch", and the event shortly afterwards when photons started to travel freely through space rather than constantly being scattered by electrons and protons in plasma is referred to as photon decoupling. The photons that existed at the time of photon decoupling have been propagating ever since, though growing fainter and less energetic, since the expansion of space causes their wavelength to increase over time (and wavelength is inversely proportional to energy according to Planck's relation). This is the source of the alternative term "relic radiation". The "surface of last scattering" refers to the set of points in space at the right distance from us so that we are now receiving photons originally emitted from those points at the time of photon decoupling.

Precise measurements of the CMB are critical to cosmology, since any proposed model of the universe must explain this radiation. The CMB has a thermal black body spectrum at a temperature of . The spectral radiance dE/dν peaks at 160.23 GHz, in the microwave range of frequencies, corresponding to a photon energy of about 6.626 × 10 eV. Alternatively, if spectral radiance is defined as dE/dλ, then the peak wavelength is 1.063 mm (282 GHz, 1.168 x 10 eV photons). The glow is very nearly uniform in all directions, but the tiny residual variations show a very specific pattern, the same as that expected of a fairly uniformly distributed hot gas that has expanded to the current size of the universe. In particular, the spectral radiance at different angles of observation in the sky contains small anisotropies, or irregularities, which vary with the size of the region examined. They have been measured in detail, and match what would be expected if small thermal variations, generated by quantum fluctuations of matter in a very tiny space, had expanded to the size of the observable universe we see today. This is a very active field of study, with scientists seeking both better data (for example, the Planck spacecraft) and better interpretations of the initial conditions of expansion. Although many different processes might produce the general form of a black body spectrum, no model other than the Big Bang has yet explained the fluctuations. As a result, most cosmologists consider the Big Bang model of the universe to be the best explanation for the CMB.

The high degree of uniformity throughout the observable universe and its faint but measured anisotropy lend strong support for the Big Bang model in general and the ΛCDM ("Lambda Cold Dark Matter") model in particular. Moreover, the fluctuations are coherent on angular scales that are larger than the apparent cosmological horizon at recombination. Either such coherence is acausally fine-tuned, or cosmic inflation occurred.<ref name="hep-ph/0309057"></ref>

The cosmic microwave background radiation is an emission of uniform, black body thermal energy coming from all parts of the sky. The radiation is isotropic to roughly one part in 100,000: the root mean square variations are only 18 µK, after subtracting out a dipole anisotropy from the Doppler shift of the background radiation. The latter is caused by the peculiar velocity of the Earth relative to the comoving cosmic rest frame as the planet moves at some 371 km/s towards the constellation Leo. The CMB dipole as well as aberration at higher multipoles have been measured, consistent with galactic motion.

In the Big Bang model for the formation of the universe, inflationary cosmology predicts that after about 10 seconds the nascent universe underwent exponential growth that smoothed out nearly all irregularities. The remaining irregularities were caused by quantum fluctuations in the inflaton field that caused the inflation event. Before the formation of stars and planets (after 10 seconds), the early universe was smaller, much hotter, and filled with a uniform glow from its white-hot fog of interacting plasma of photons, electrons, and baryons.

As the universe expanded, adiabatic cooling caused the energy density of the plasma to decrease until it became favorable for electrons to combine with protons, forming hydrogen atoms. This recombination event happened when the temperature was around 3000 K or when the universe was approximately 379,000 years old. As photons did not interact with these electrically neutral atoms, the former began to travel freely through space, resulting in the decoupling of matter and radiation.

The color temperature of the ensemble of decoupled photons has continued to diminish ever since; now down to , it will continue to drop as the universe expands. The intensity of the radiation also corresponds to black-body radiation at 2.726 K because red-shifted black-body radiation is just like black-body radiation at a lower temperature. According to the Big Bang model, the radiation from the sky we measure today comes from a spherical surface called "the surface of last scattering". This represents the set of locations in space at which the decoupling event is estimated to have occurred and at a point in time such that the photons from that distance have just reached observers. Most of the radiation energy in the universe is in the cosmic microwave background, making up a fraction of roughly of the total density of the universe.

Two of the greatest successes of the Big Bang theory are its prediction of the almost perfect black body spectrum and its detailed prediction of the anisotropies in the cosmic microwave background. The CMB spectrum has become the most precisely measured black body spectrum in nature.

Density of energy for CMB is () or (400–500 photons/cm).

The cosmic microwave background was first predicted in 1948 by Ralph Alpher and Robert Herman. Alpher and Herman were able to estimate the temperature of the cosmic microwave background to be 5 K, though two years later they re-estimated it at 28 K. This high estimate was due to a mis-estimate of the Hubble constant by Alfred Behr, which could not be replicated and was later abandoned for the earlier estimate. Although there were several previous estimates of the temperature of space, these suffered from two flaws. First, they were measurements of the "effective" temperature of space and did not suggest that space was filled with a thermal Planck spectrum. Next, they depend on our being at a special spot at the edge of the Milky Way galaxy and they did not suggest the radiation is isotropic. The estimates would yield very different predictions if Earth happened to be located elsewhere in the universe.
The 1948 results of Alpher and Herman were discussed in many physics settings through about 1955, when both left the Applied Physics Laboratory at Johns Hopkins University. The mainstream astronomical community, however, was not intrigued at the time by cosmology. Alpher and Herman's prediction was rediscovered by Yakov Zel'dovich in the early 1960s, and independently predicted by Robert Dicke at the same time. The first published recognition of the CMB radiation as a detectable phenomenon appeared in a brief paper by Soviet astrophysicists A. G. Doroshkevich and Igor Novikov, in the spring of 1964. In 1964, David Todd Wilkinson and Peter Roll, Dicke's colleagues at Princeton University, began constructing a Dicke radiometer to measure the cosmic microwave background. In 1964, Arno Penzias and Robert Woodrow Wilson at the Crawford Hill location of Bell Telephone Laboratories in nearby Holmdel Township, New Jersey had built a Dicke radiometer that they intended to use for radio astronomy and satellite communication experiments. On 20 May 1964 they made their first measurement clearly showing the presence of the microwave background, with their instrument having an excess 4.2K antenna temperature which they could not account for. After receiving a telephone call from Crawford Hill, Dicke said "Boys, we've been scooped." A meeting between the Princeton and Crawford Hill groups determined that the antenna temperature was indeed due to the microwave background. Penzias and Wilson received the 1978 Nobel Prize in Physics for their discovery.

The interpretation of the cosmic microwave background was a controversial issue in the 1960s with some proponents of the steady state theory arguing that the microwave background was the result of scattered starlight from distant galaxies. Using this model, and based on the study of narrow absorption line features in the spectra of stars, the astronomer Andrew McKellar wrote in 1941: "It can be calculated that the 'rotational temperature' of interstellar space is 2 K." However, during the 1970s the consensus was established that the cosmic microwave background is a remnant of the big bang. This was largely because new measurements at a range of frequencies showed that the spectrum was a thermal, black body spectrum, a result that the steady state model was unable to reproduce.

Harrison, Peebles, Yu and Zel'dovich realized that the early universe would have to have inhomogeneities at the level of 10 or 10. Rashid Sunyaev later calculated the observable imprint that these inhomogeneities would have on the cosmic microwave background. Increasingly stringent limits on the anisotropy of the cosmic microwave background were set by ground based experiments during the 1980s. RELIKT-1, a Soviet cosmic microwave background anisotropy experiment on board the Prognoz 9 satellite (launched 1 July 1983) gave upper limits on the large-scale anisotropy. The NASA COBE mission clearly confirmed the primary anisotropy with the Differential Microwave Radiometer instrument, publishing their findings in 1992. The team received the Nobel Prize in physics for 2006 for this discovery.

Inspired by the COBE results, a series of ground and balloon-based experiments measured cosmic microwave background anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the scale of the first acoustic peak, which COBE did not have sufficient resolution to resolve. This peak corresponds to large scale density variations in the early universe that are created by gravitational instabilities, resulting in acoustical oscillations in the plasma. The first peak in the anisotropy was tentatively detected by the Toco experiment and the result was confirmed by the BOOMERanG and MAXIMA experiments. These measurements demonstrated that the geometry of the universe is approximately flat, rather than curved. They ruled out cosmic strings as a major component of cosmic structure formation and suggested cosmic inflation was the right theory of structure formation.

The second peak was tentatively detected by several experiments before being definitively detected by WMAP, which has also tentatively detected the third peak. As of 2010, several experiments to improve measurements of the polarization and the microwave background on small angular scales are ongoing. These include DASI, WMAP, BOOMERanG, QUaD, Planck spacecraft, Atacama Cosmology Telescope, South Pole Telescope and the QUIET telescope.

The cosmic microwave background radiation and the cosmological redshift-distance relation are together regarded as the best available evidence for the Big Bang theory. Measurements of the CMB have made the inflationary Big Bang theory the Standard Cosmological Model. The discovery of the CMB in the mid-1960s curtailed interest in alternatives such as the steady state theory.

The CMB essentially confirms the Big Bang theory. In the late 1940s Alpher and Herman reasoned that if there was a big bang, the expansion of the universe would have stretched and cooled the high-energy radiation of the very early universe into the microwave region of the electromagnetic spectrum, and down to a temperature of about 5 K. They were slightly off with their estimate, but they had exactly the right idea. They predicted the CMB. It took another 15 years for Penzias and Wilson to stumble into discovering that the microwave background was actually there.

The CMB gives a snapshot of the universe when, according to standard cosmology, the temperature dropped enough to allow electrons and protons to form hydrogen atoms, thereby making the universe nearly transparent to radiation because light was no longer being scattered off free electrons. When it originated some 380,000 years after the Big Bang—this time is generally known as the "time of last scattering" or the period of recombination or decoupling—the temperature of the universe was about 3000 K. This corresponds to an energy of about 0.26 eV, which is much less than the 13.6 eV ionization energy of hydrogen.

Since decoupling, the temperature of the background radiation has dropped by a factor of roughly 1,100 due to the expansion of the universe. As the universe expands, the CMB photons are redshifted, causing them to decrease in energy. The temperature of this radiation stays inversely proportional to a parameter that describes the relative expansion of the universe over time, known as the scale length. The temperature "T" of the CMB as a function of redshift, "z", can be shown to be proportional to the temperature of the CMB as observed in the present day (2.725 K or 0.2348 meV):

For details about the reasoning that the radiation is evidence for the Big Bang, see Cosmic background radiation of the Big Bang.

The anisotropy, or directional dependency, of the cosmic microwave background is divided into two types: primary anisotropy, due to effects that occur at the last scattering surface and before; and secondary anisotropy, due to effects such as interactions of the background radiation with hot gas or gravitational potentials, which occur between the last scattering surface and the observer.

The structure of the cosmic microwave background anisotropies is principally determined by two effects: acoustic oscillations and diffusion damping (also called collisionless damping or Silk damping). The acoustic oscillations arise because of a conflict in the photon–baryon plasma in the early universe. The pressure of the photons tends to erase anisotropies, whereas the gravitational attraction of the baryons, moving at speeds much slower than light, makes them tend to collapse to form overdensities. These two effects compete to create acoustic oscillations, which give the microwave background its characteristic peak structure. The peaks correspond, roughly, to resonances in which the photons decouple when a particular mode is at its peak amplitude.

The peaks contain interesting physical signatures. The angular scale of the first peak determines the curvature of the universe (but not the topology of the universe). The next peak—ratio of the odd peaks to the even peaks—determines the reduced baryon density. The third peak can be used to get information about the dark-matter density.

The locations of the peaks also give important information about the nature of the primordial density perturbations. There are two fundamental types of density perturbations called "adiabatic" and "isocurvature". A general density perturbation is a mixture of both, and different theories that purport to explain the primordial density perturbation spectrum predict different mixtures.

The CMB spectrum can distinguish between these two because these two types of perturbations produce different peak locations. Isocurvature density perturbations produce a series of peaks whose angular scales ("l" values of the peaks) are roughly in the ratio 1:3:5:..., while adiabatic density perturbations produce peaks whose locations are in the ratio 1:2:3:... Observations are consistent with the primordial density perturbations being entirely adiabatic, providing key support for inflation, and ruling out many models of structure formation involving, for example, cosmic strings.

Collisionless damping is caused by two effects, when the treatment of the primordial plasma as fluid begins to break down:
These effects contribute about equally to the suppression of anisotropies at small scales and give rise to the characteristic exponential damping tail seen in the very small angular scale anisotropies.

The depth of the LSS refers to the fact that the decoupling of the photons and baryons does not happen instantaneously, but instead requires an appreciable fraction of the age of the universe up to that era. One method of quantifying how long this process took uses the "photon visibility function" (PVF). This function is defined so that, denoting the PVF by "P"("t"), the probability that a CMB photon last scattered between time "t" and is given by "P"("t") "dt".

The maximum of the PVF (the time when it is most likely that a given CMB photon last scattered) is known quite precisely. The first-year WMAP results put the time at which "P"("t") has a maximum as 372,000 years. This is often taken as the "time" at which the CMB formed. However, to figure out how "long" it took the photons and baryons to decouple, we need a measure of the width of the PVF. The WMAP team finds that the PVF is greater than half of its maximal value (the "full width at half maximum", or FWHM) over an interval of 115,000 years. By this measure, decoupling took place over roughly 115,000 years, and when it was complete, the universe was roughly 487,000 years old.

Since the CMB came into existence, it has apparently been modified by several subsequent physical processes, which are collectively referred to as late-time anisotropy, or secondary anisotropy. When the CMB photons became free to travel unimpeded, ordinary matter in the universe was mostly in the form of neutral hydrogen and helium atoms. However, observations of galaxies today seem to indicate that most of the volume of the intergalactic medium (IGM) consists of ionized material (since there are few absorption lines due to hydrogen atoms). This implies a period of reionization during which some of the material of the universe was broken into hydrogen ions.

The CMB photons are scattered by free charges such as electrons that are not bound in atoms. In an ionized universe, such charged particles have been liberated from neutral atoms by ionizing (ultraviolet) radiation. Today these free charges are at sufficiently low density in most of the volume of the universe that they do not measurably affect the CMB. However, if the IGM was ionized at very early times when the universe was still denser, then there are two main effects on the CMB:

Both of these effects have been observed by the WMAP spacecraft, providing evidence that the universe was ionized at very early times, at a redshift more than 17. The detailed provenance of this early ionizing radiation is still a matter of scientific debate. It may have included starlight from the very first population of stars (population III stars), supernovae when these first stars reached the end of their lives, or the ionizing radiation produced by the accretion disks of massive black holes.

The time following the emission of the cosmic microwave background—and before the observation of the first stars—is semi-humorously referred to by cosmologists as the dark age, and is a period which is under intense study by astronomers (see 21 centimeter radiation).

Two other effects which occurred between reionization and our observations of the cosmic microwave background, and which appear to cause anisotropies, are the Sunyaev–Zel'dovich effect, where a cloud of high-energy electrons scatters the radiation, transferring some of its energy to the CMB photons, and the Sachs–Wolfe effect, which causes photons from the Cosmic Microwave Background to be gravitationally redshifted or blueshifted due to changing gravitational fields.

The cosmic microwave background is polarized at the level of a few microkelvin. There are two types of polarization, called E-modes and B-modes. This is in analogy to electrostatics, in which the electric field ("E"-field) has a vanishing curl and the magnetic field ("B"-field) has a vanishing divergence. The E-modes arise naturally from Thomson scattering in a heterogeneous plasma. The B-modes are not produced by standard scalar type perturbations. Instead they can be created by two mechanisms: the first one is by gravitational lensing of E-modes, which has been measured by the South Pole Telescope in 2013; the second one is from gravitational waves arising from cosmic inflation. Detecting the B-modes is extremely difficult, particularly as the degree of foreground contamination is unknown, and the weak gravitational lensing signal mixes the relatively strong E-mode signal with the B-mode signal.

E-modes were first seen in 2002 by the Degree Angular Scale Interferometer (DASI).

Cosmologists predict two types of B-modes, the first generated during cosmic inflation shortly after the big bang, and the second generated by gravitational lensing at later times.

Primordial gravitational waves are gravitational waves that could be observed in the polarisation of the cosmic microwave background and having their origin in the early universe. Models of cosmic inflation predict that such gravitational waves should appear; thus, their detection supports the theory of inflation, and their strength can confirm and exclude different models of inflation. It is the result of three things: inflationary expansion of space itself, reheating after inflation, and turbulent fluid mixing of matter and radiation.
On 17 March 2014 it was announced that the BICEP2 instrument had detected the first type of B-modes, consistent with inflation and gravitational waves in the early universe at the level of , which is the amount of power present in gravitational waves compared to the amount of power present in other scalar density perturbations in the very early universe. Had this been confirmed it would have provided strong evidence of cosmic inflation and the Big Bang,

and on 19 September 2014 new results of the Planck experiment reported that the results of BICEP2 can be fully attributed to cosmic dust.

The second type of B-modes was discovered in 2013 using the South Pole Telescope with help from the Herschel Space Observatory. This discovery may help test theories on the origin of the universe. Scientists are using data from the Planck mission by the European Space Agency, to gain a better understanding of these waves.

In October 2014, a measurement of the B-mode polarization at 150 GHz was published by the POLARBEAR experiment. Compared to BICEP2, POLARBEAR focuses on a smaller patch of the sky and is less susceptible to dust effects. The team reported that POLARBEAR's measured B-mode polarization was of cosmological origin (and not just due to dust) at a 97.2% confidence level.

Subsequent to the discovery of the CMB, hundreds of cosmic microwave background experiments have been conducted to measure and characterize the signatures of the radiation. The most famous experiment is probably the NASA Cosmic Background Explorer (COBE) satellite that orbited in 1989–1996 and which detected and quantified the large scale anisotropies at the limit of its detection capabilities. Inspired by the initial COBE results of an extremely isotropic and homogeneous background, a series of ground- and balloon-based experiments quantified CMB anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the angular scale of the first acoustic peak, for which COBE did not have sufficient resolution. These measurements were able to rule out cosmic strings as the leading theory of cosmic structure formation, and suggested cosmic inflation was the right theory. During the 1990s, the first peak was measured with increasing sensitivity and by 2000 the BOOMERanG experiment reported that the highest power fluctuations occur at scales of approximately one degree. Together with other cosmological data, these results implied that the geometry of the universe is flat. A number of ground-based interferometers provided measurements of the fluctuations with higher accuracy over the next three years, including the Very Small Array, Degree Angular Scale Interferometer (DASI), and the Cosmic Background Imager (CBI). DASI made the first detection of the polarization of the CMB and the CBI provided the first E-mode polarization spectrum with compelling evidence that it is out of phase with the T-mode spectrum.

In June 2001, NASA launched a second CMB space mission, WMAP, to make much more precise measurements of the large scale anisotropies over the full sky. WMAP used symmetric, rapid-multi-modulated scanning, rapid switching radiometers to minimize non-sky signal noise. The first results from this mission, disclosed in 2003, were detailed measurements of the angular power spectrum at a scale of less than one degree, tightly constraining various cosmological parameters. The results are broadly consistent with those expected from cosmic inflation as well as various other competing theories, and are available in detail at NASA's data bank for Cosmic Microwave Background (CMB) (see links below). Although WMAP provided very accurate measurements of the large scale angular fluctuations in the CMB (structures about as broad in the sky as the moon), it did not have the angular resolution to measure the smaller scale fluctuations which had been observed by former ground-based interferometers.

A third space mission, the ESA (European Space Agency) Planck Surveyor, was launched in May 2009 and performed an even more detailed investigation until it was shut down in October 2013. Planck employed both HEMT radiometers and bolometer technology and measured the CMB at a smaller scale than WMAP. Its detectors were trialled in the Antarctic Viper telescope as ACBAR (Arcminute Cosmology Bolometer Array Receiver) experiment—which has produced the most precise measurements at small angular scales to date—and in the Archeops balloon telescope.

On 21 March 2013, the European-led research team behind the Planck cosmology probe released the mission's all-sky map (565x318 jpeg, 3600x1800 jpeg) of the cosmic microwave background. The map suggests the universe is slightly older than researchers thought. According to the map, subtle fluctuations in temperature were imprinted on the deep sky when the cosmos was about 370,000 years old. The imprint reflects ripples that arose as early, in the existence of the universe, as the first nonillionth of a second. Apparently, these ripples gave rise to the present vast cosmic web of galaxy clusters and dark matter. Based on the 2013 data, the universe contains 4.9% ordinary matter, 26.8% dark matter and 68.3% dark energy. On 5 February 2015, new data was released by the Planck mission, according to which the age of the universe is 13.799 ± 0.021 billion years old and the Hubble constant was measured to be 67.74 ± 0.46 (km/s)/Mpc.

Additional ground-based instruments such as the South Pole Telescope in Antarctica and the proposed Clover Project, Atacama Cosmology Telescope and the QUIET telescope in Chile will provide additional data not available from satellite observations, possibly including the B-mode polarization.

Raw CMBR data, even from space vehicles such as WMAP or Planck, contain foreground effects that completely obscure the fine-scale structure of the cosmic microwave background. The fine-scale structure is superimposed on the raw CMBR data but is too small to be seen at the scale of the raw data. The most prominent of the foreground effects is the dipole anisotropy caused by the Sun's motion relative to the CMBR background. The dipole anisotropy and others due to Earth's annual motion relative to the Sun and numerous microwave sources in the galactic plane and elsewhere must be subtracted out to reveal the extremely tiny variations characterizing the fine-scale structure of the CMBR background.

The detailed analysis of CMBR data to produce maps, an angular power spectrum, and ultimately cosmological parameters is a complicated, computationally difficult problem. Although computing a power spectrum from a map is in principle a simple Fourier transform, decomposing the map of the sky into spherical harmonics, in practice it is hard to take the effects of noise and foreground sources into account. In particular, these foregrounds are dominated by galactic emissions such as Bremsstrahlung, synchrotron, and dust that emit in the microwave band; in practice, the galaxy has to be removed, resulting in a CMB map that is not a full-sky map. In addition, point sources like galaxies and clusters represent another source of foreground which must be removed so as not to distort the short scale structure of the CMB power spectrum.

Constraints on many cosmological parameters can be obtained from their effects on the power spectrum, and results are often calculated using Markov Chain Monte Carlo sampling techniques.

From the CMB data it is seen that the Local Group (the galaxy group that includes the Milky Way galaxy) appears to be moving at relative to the reference frame of the CMB (also called the CMB rest frame, or the frame of reference in which there is no motion through the CMB) in the direction of galactic longitude "l" = , "b" = . This motion results in an anisotropy of the data (CMB appearing slightly warmer in the direction of movement than in the opposite direction). From a theoretical point of view, the existence of a CMB rest frame breaks Lorentz invariance even in empty space far away from any galaxy. The standard interpretation of this temperature variation is a simple velocity red shift and blue shift due to motion relative to the CMB, but alternative cosmological models can explain some fraction of the observed dipole temperature distribution in the CMB.

With the increasingly precise data provided by WMAP, there have been a number of claims that the CMB exhibits anomalies, such as very large scale anisotropies, anomalous alignments, and non-Gaussian distributions.<ref name="arXiv:astro-ph/0511666"></ref><ref name="arXiv:astro-ph/0503213"></ref> The most longstanding of these is the low-"l" multipole controversy. Even in the COBE map, it was observed that the quadrupole ("l" = 2, spherical harmonic) has a low amplitude compared to the predictions of the Big Bang. In particular, the quadrupole and octupole ("l" = 3) modes appear to have an unexplained alignment with each other and with both the ecliptic plane and equinoxes, A number of groups have suggested that this could be the signature of new physics at the greatest observable scales; other groups suspect systematic errors in the data. Ultimately, due to the foregrounds and the cosmic variance problem, the greatest modes will never be as well measured as the small angular scale modes. The analyses were performed on two maps that have had the foregrounds removed as far as possible: the "internal linear combination" map of the WMAP collaboration and a similar map prepared by Max Tegmark and others. Later analyses have pointed out that these are the modes most susceptible to foreground contamination from synchrotron, dust, and Bremsstrahlung emission, and from experimental uncertainty in the monopole and dipole. A full Bayesian analysis of the WMAP power spectrum demonstrates that the quadrupole prediction of Lambda-CDM cosmology is consistent with the data at the 10% level and that the observed octupole is not remarkable. Carefully accounting for the procedure used to remove the foregrounds from the full sky map further reduces the significance of the alignment by ~5%.
Recent observations with the Planck telescope, which is very much more sensitive than WMAP and has a larger angular resolution, record the same anomaly, and so instrumental error (but not foreground contamination) appears to be ruled out. Coincidence is a possible explanation, chief scientist from WMAP, Charles L. Bennett suggested coincidence and human psychology were involved, ""I do think there is a bit of a psychological effect; people want to find unusual things."" 

Assuming the universe keeps expanding and it does not suffer a Big Crunch, a Big Rip, or another similar fate, the cosmic microwave background will continue redshifting until it will no longer be detectable, and will be overtaken first by the one produced by starlight, and later by the background radiation fields of processes that are assumed will take place in the far future of the universe.







</doc>
<doc id="7378" url="https://en.wikipedia.org/wiki?curid=7378" title="Comparative law">
Comparative law

Comparative law is the study of differences and similarities between the law of different countries. More specifically, it involves the study of the different legal "systems" (or "families") in existence in the world, including the common law, the civil law, socialist law, Canon law, Jewish Law, Islamic law, Hindu law, and Chinese law. It includes the description and analysis of foreign legal systems, even where no explicit comparison is undertaken. The importance of comparative law has increased enormously in the present age of internationalism, economic globalization, and democratization.

The origins of modern comparative law can be traced back to 18th century Europe, although, prior to that, legal scholars had always practiced comparative methodologies.

Montesquieu is generally regarded as an early founding figure of comparative law. His comparative approach is obvious in the following excerpt from Chapter III of Book I of his masterpiece, "De l'esprit des lois" (1748; first translated by Thomas Nugent, 1750):

Also, in Chapter XI (entitled 'How to compare two different Systems of Laws') of Book XXIX, discussing the French and English systems for punishment of false witnesses, he advises that "to determine which of those systems is most agreeable to reason, we must take them each as a whole and compare them in their entirety." Yet another place where Montesquieu's comparative approach is evident is the following, from Chapter XIII of Book XXIX:

The modern founding figure of comparative and anthropological jurisprudence was Sir Henry Maine, a British jurist and legal historian. In his 1861 work "Ancient Law: Its Connection with the Early History of Society, and Its Relation to Modern Ideas", he set out his views on the development of legal institutions in primitive societies and engaged in a comparative discussion of Eastern and Western legal traditions. This work placed comparative law in its historical context and was widely read and influential.

The first university course on the subject was established at the University of Oxford in 1869, with Maine taking up the position of professor.

Comparative law in the US was brought by a legal scholar fleeing persecution in Germany, Rudolf Schlesinger. Schlesinger eventually became professor of comparative law at Cornell Law School helping to spread the discipline throughout the US.

Comparative law is an academic discipline that involves the study of legal systems, including their constitutive elements and how they differ, and how their elements combine into a system.

Several disciplines have developed as separate branches of comparative law, including comparative constitutional law, comparative administrative law, comparative civil law (in the sense of the law of torts, delicts, contracts and obligations), comparative commercial law (in the sense of business organisations and trade), and comparative criminal law. Studies of these specific areas may be viewed as micro- or macro-comparative legal analysis, i.e. detailed comparisons of two countries, or broad-ranging studies of several countries. Comparative civil law studies, for instance, show how the law of private relations is organised, interpreted and used in different systems or countries. The purposes of comparative law are:

Comparative law is different from the fields of general jurisprudence (legal theory), international law, including both public international law and private international law (also known as conflict of laws).

Despite the differences between comparative law and these other legal fields, comparative law helps inform all of these areas of normativity. For example, comparative law can help international legal institutions, such as those of the United Nations System, in analyzing the laws of different countries regarding their treaty obligations. Comparative law would be applicable to private international law when developing an approach to interpretation in a conflicts analysis. Comparative law may contribute to legal theory by creating categories and concepts of general application. Comparative law may also provide insights into the question of legal transplants, i.e. the transplanting of law and legal institutions from one system to another. The notion of legal transplants was coined by Alan Watson, one of the world's renowned legal scholars specializing in comparative law.

Also, the usefulness of comparative law for sociology of law and law and economics (and vice versa) is very large. The comparative study of the various legal systems may show how different legal regulations for the same problem function in practice. Conversely, sociology of law and law & economics may help comparative law answer questions, such as: 

Arminjon, Nolde, and Wolff believed that, for purposes of classifying the (then) contemporary legal systems of the world, it was required that those systems "per se" get studied, irrespective of external factors, such as geographical ones. They proposed the classification of legal system into seven groups, or so-called 'families', in particular the

René David proposed the classification of legal systems, according to the different ideology inspiring each one, into five groups or families:
Especially with respect to the aggregating by David of the Romano-Germanic and Anglo-Saxon Laws into a single family, David argued that the antithesis between the Anglo-Saxon Laws and Romano-German Laws, is of a technical rather than of an ideological nature. Of a different kind is, for instance, the antithesis between (say) the Italian and the American Law, and of a different kind that between the Soviet, Muslim, Hindu, or Chinese Law. According to David, the Romano-Germanic legal systems included those countries where legal science was formulated according to Roman Law, whereas common law countries are those where law was created from the judges. The characteristics that he believed uniquely differentiate the Western legal family from the other four are

Konrad Zweigert and Hein Kötz propose a different, multidimensional methodology for categorizing laws, i.e. for ordering families of laws. They maintain that, to determine such families, five criteria should be taken into account, in particular: the historical background, the characteristic way of thought, the different institutions, the recognized sources of law, and the dominant ideology. Using the aforementioned criteria, they classify the legal systems of the world into six families:

Up to the second German edition of their introduction to comparative law, Zweigert and Kötz also used to mention Soviet or socialist law as another family of laws.







</doc>
<doc id="7380" url="https://en.wikipedia.org/wiki?curid=7380" title="CD (disambiguation)">
CD (disambiguation)

A CD or compact disc is a thin plastic silvery disc for audio recordings.

CD or cd may also refer to:










</doc>
<doc id="7381" url="https://en.wikipedia.org/wiki?curid=7381" title="Cyberspace">
Cyberspace

Cyberspace is interconnected technology. The term entered the popular culture from science fiction and the arts but is now used by technology strategists, security professionals, government, military and industry leaders and entrepreneurs to describe the domain of the global technology environment. Others consider cyberspace to be just a notional environment in which communication over computer networks occurs. The word became popular in the 1990s when the uses of the Internet, networking, and digital communication were all growing dramatically and the term "cyberspace" was able to represent the many new ideas and phenomena that were emerging. It has been called the largest unregulated and uncontrolled domain in the history of mankind, and is also unique because it is a domain created by people vice the traditional physical domains.

The parent term of cyberspace is "cybernetics", derived from the Ancient Greek "κυβερνήτης" ("kybernētēs", steersman, governor, pilot, or rudder), a word introduced by Norbert Wiener for his pioneering work in electronic communication and control science. This word first appeared in the short story "Burning Chrome" by William Gibson ("Omni", July 1982).

As a social experience, individuals can interact, exchange ideas, share information, provide social support, conduct business, direct actions, create artistic media, play games, engage in political discussion, and so on, using this global network. They are sometimes referred to as "cybernauts". The term "cyberspace" has become a conventional means to describe anything associated with the Internet and the diverse Internet culture. The United States government recognizes the interconnected information technology and the interdependent network of information technology infrastructures operating across this medium as part of the US national critical infrastructure. Amongst individuals on cyberspace, there is believed to be a code of shared rules and ethics mutually beneficial for all to follow, referred to as cyberethics. Many view the right to privacy as most important to a functional code of cyberethics. Such moral responsibilities go hand in hand when working online with global networks, specifically, when opinions are involved with online social experiences.

According to Chip Morningstar and F. Randall Farmer, cyberspace is defined more by the social interactions involved rather than its technical implementation. In their view, the computational medium in cyberspace is an augmentation of the communication channel between real people; the core characteristic of cyberspace is that it offers an environment that consists of many participants with the ability to affect and influence each other. They derive this concept from the observation that people seek richness, complexity, and depth within a virtual world.

The term “cyberspace” first appeared in the visual arts in the late 1960s, when Danish artist Susanne Ussing (1940-1998) and her partner architect Carsten Hoff (b. 1934) constituted themselves as Atelier Cyberspace. Under this name the two made a series of installations and images entitled “sensory spaces” that were based on the principle of open systems adaptable to various influences, such as human movement and the behaviour of new materials.

Atelier Cyberspace worked at a time when the Internet did not exist and computers were more or less off-limit to artists and creative engagement. In a 2015-interview with Scandinavian art magazine Kunstkritikk, Carsten Hoff recollects, that although Atelier Cyberspace did try to implement computers, they had no interest in the virtual space as such:

And in the same interview Hoff continues:

The works of Atelier Cyberspace were originally shown at a number of Copenhagen venues and have later been exhibited at The National Gallery of Denmark in Copenhagen as part of the exhibition “What’s Happening?”

The term "cyberspace" first appeared in fiction in the 1980s in the work of cyberpunk science fiction author William Gibson, first in his 1982 short story "Burning Chrome" and later in his 1984 novel "Neuromancer". In the next few years, the word became prominently identified with online computer networks. The portion of "Neuromancer" cited in this respect is usually the following:

Now widely used, the term has since been criticized by Gibson, who commented on the origin of the term in the 2000 documentary "No Maps for These Territories":

Don Slater uses a metaphor to define cyberspace, describing the "sense of a social setting that exists purely within a space of representation and communication ... it exists entirely within a computer space, distributed across increasingly complex and fluid networks." The term "Cyberspace" started to become a de facto synonym for the Internet, and later the World Wide Web, during the 1990s, especially in academic circles and activist communities. Author Bruce Sterling, who popularized this meaning, credits John Perry Barlow as the first to use it to refer to "the present-day nexus of computer and telecommunications networks". Barlow describes it thus in his essay to announce the formation of the Electronic Frontier Foundation (note the spatial metaphor) in June 1990:

As Barlow, and the EFF, continued public education efforts to promote the idea of "digital rights", the term was increasingly used during the Internet boom of the late 1990s.

Although the present-day, loose use of the term "cyberspace" no longer implies or suggests immersion in a virtual reality, current technology allows the integration of a number of capabilities (sensors, signals, connections, transmissions, processors, and controllers) sufficient to generate a virtual interactive experience that is accessible regardless of a geographic location. It is for these reasons cyberspace has been described as the ultimate tax haven.

In 1989, Autodesk, an American multinational corporation that focuses on 2D and 3D design software, developed a virtual design system called Cyberspace.

Although several definitions of cyberspace can be found both in scientific literature and in official governmental sources, there is no fully agreed official definition yet. According to F. D. Kramer there are 28 different definitions of the term cyberspace. See in particular the following links: "Cyberpower and National Security: Policy Recommendations for a Strategic Framework," in Cyberpower and National Security, FD Kramer, S. Starr, L.K. Wentz (ed.), National Defense University Press, Washington (DC) 2009; see also Mayer, M., Chiarugi, I., De Scalzi, N., https://www.academia.edu/14336129/International_Politics_in_the_Digital_Age.

The most recent draft definition is the following:

Cyberspace is a global and dynamic domain (subject to constant change) characterized by the combined use of electrons and electromagnetic spectrum, whose purpose is to create, store, modify, exchange, share and extract, use, eliminate information and disrupt physical resources. Cyberspace includes: a) physical infrastructures and telecommunications devices that allow for the connection of technological and communication system networks, understood in the broadest sense (SCADA devices, smartphones/tablets, computers, servers, etc.); b) computer systems (see point a) and the related (sometimes embedded) software that guarantee the domain's basic operational functioning and connectivity; c) networks between computer systems; d) networks of networks that connect computer systems (the distinction between networks and networks of networks is mainly organizational); e) the access nodes of users and intermediaries routing nodes; f) constituent data (or resident data). Often, in common parlance (and sometimes in commercial language), networks of networks are called Internet (with a lowercase i), while networks between computers are called intranet. Internet (with a capital I, in journalistic language sometimes called the Net) can be considered a part of the system a). A distinctive and constitutive feature of cyberspace is that no central entity exercises control over all the networks that make up this new domain.
Just as in the real world there is no world government, cyberspace lacks an institutionally predefined hierarchical center. To cyberspace, a domain without a hierarchical ordering principle, we can therefore extend the definition of international politics coined by Kenneth Waltz: as being "with no system of law enforceable." This does not mean that the dimension of power in cyberspace is absent, nor that power is dispersed and scattered into a thousand invisible streams, nor that it is evenly spread across myriad people and organizations, as some scholars had predicted. On the contrary, cyberspace is characterized by a precise structuring of hierarchies of power.

While cyberspace should not be confused with the Internet, the term is often used to refer to objects and identities that exist largely within the communication network itself, so that a website, for example, might be metaphorically said to "exist in cyberspace". According to this interpretation, events taking place on the Internet are not happening in the locations where participants or servers are physically located, but "in cyberspace". The philosopher Michel Foucault used the term heterotopias, to describe such spaces which are simultaneously physical and mental.

Firstly, cyberspace describes the flow of digital data through the network of interconnected computers: it is at once not "real", since one could not spatially locate it as a tangible object, and clearly "real" in its effects. Secondly, cyberspace is the site of computer-mediated communication (CMC), in which online relationships and alternative forms of online identity were enacted, raising important questions about the social psychology of Internet use, the relationship between "online" and "offline" forms of life and interaction, and the relationship between the "real" and the virtual. Cyberspace draws attention to remediation of culture through new media technologies: it is not just a communication tool but a social destination, and is culturally significant in its own right. Finally, cyberspace can be seen as providing new opportunities to reshape society and culture through "hidden" identities, or it can be seen as borderless communication and culture.
The "space" in cyberspace has more in common with the abstract, mathematical meanings of the term (see space) than physical space. It does not have the duality of positive and negative volume (while in physical space for example a room has the negative volume of usable space delineated by positive volume of walls, Internet users cannot enter the screen and explore the unknown part of the Internet as an extension of the space they are in), but spatial meaning can be attributed to the relationship between different pages (of books as well as web servers), considering the unturned pages to be somewhere "out there." The concept of cyberspace therefore refers not to the content being presented to the surfer, but rather to the possibility of surfing among different sites, with feedback loops between the user and the rest of the system creating the potential to always encounter something unknown or unexpected.

Videogames differ from text-based communication in that on-screen images are meant to be figures that actually occupy a space and the animation shows the movement of those figures. Images are supposed to form the positive volume that delineates the empty space. A game adopts the cyberspace metaphor by engaging more players in the game, and then figuratively representing them on the screen as avatars. Games do not have to stop at the avatar-player level, but current implementations aiming for more immersive playing space (i.e. Laser tag) take the form of augmented reality rather than cyberspace, fully immersive virtual realities remaining impractical.

Although the more radical consequences of the global communication network predicted by some cyberspace proponents (i.e. the diminishing of state influence envisioned by John Perry Barlow) failed to materialize and the word lost some of its novelty appeal, it remains current .

Some virtual communities explicitly refer to the concept of cyberspace, for example Linden Lab calling their customers "Residents" of Second Life, while all such communities can be positioned "in cyberspace" for explanatory and comparative purposes (as did Sterling in "The Hacker Crackdown", followed by many journalists), integrating the metaphor into a wider cyber-culture.

The metaphor has been useful in helping a new generation of thought leaders to reason through new military strategies around the world, led largely by the US Department of Defense (DoD). The use of cyberspace as a metaphor has had its limits, however, especially in areas where the metaphor becomes confused with physical infrastructure. It has also been critiqued as being unhelpful for falsely employing a spatial metaphor to describe what is inherently a network.

A forerunner of the modern ideas of cyberspace is the Cartesian notion that people might be deceived by an evil demon that feeds them a false reality. This argument is the direct predecessor of modern ideas of a brain in a vat and many popular conceptions of cyberspace take Descartes's ideas as their starting point.

Visual arts have a tradition, stretching back to antiquity, of artifacts meant to fool the eye and be mistaken for reality. This questioning of reality occasionally led some philosophers and especially theologians to distrust art as deceiving people into entering a world which was not real (see Aniconism). The artistic challenge was resurrected with increasing ambition as art became more and more realistic with the invention of photography, film (see "Arrival of a Train at La Ciotat"), and immersive computer simulations.

American counterculture exponents like William S. Burroughs (whose literary influence on Gibson and cyberpunk in general is widely acknowledged) and Timothy Leary were among the first to extol the potential of computers and computer networks for individual empowerment.

Some contemporary philosophers and scientists (e.g. David Deutsch in "The Fabric of Reality") employ virtual reality in various thought experiments. For example, Philip Zhai in "Get Real: A Philosophical Adventure in Virtual Reality" connects cyberspace to the platonic tradition:

Note that this brain-in-a-vat argument conflates cyberspace with reality, while the more common descriptions of cyberspace contrast it with the "real world".

The technological convergence of the mass media is the result of a long adaptation process of their communicative resources to the evolutionary changes of each historical moment. Thus, the new media became (plurally) an extension of the traditional media on the cyberspace, allowing to the public access information in a wide range of digital devices. In other words, it is a cultural virtualization of human reality as a result of the migration from physical to virtual space (mediated by the ICTs), ruled by codes, signs and particular social relationships. Forwards, arise instant ways of communication, interaction and possible quick access to information, in which we are no longer mere senders, but also producers, reproducers, co-workers and providers. New technologies also help to “connect” people from different cultures outside the virtual space, what was unthinkable fifty years ago. In this giant relationships web, we mutually absorb each other’s beliefs, customs, values, laws and habits, cultural legacies perpetuated by a physical-virtual dynamics in constant metamorphosis (ibidem). In this sense, Professor Doctor Marcelo Mendonça Teixeira created, in 2013, a new model of communication to the virtual universe, based in Claude Elwood Shannon (1948) article "A Mathematical Theory of Communication".

Having originated among writers, the concept of cyberspace remains most popular in literature and film. Although artists working with other media have expressed interest in the concept, such as Roy Ascott, "cyberspace" in digital art is mostly used as a synonym for immersive virtual reality and remains more discussed than enacted.

Cyberspace also brings together every service and facility imaginable to expedite money laundering. One can purchase anonymous credit cards, bank accounts, encrypted global mobile telephones, and false passports. From there one can pay professional advisors to set up IBCs (International Business Corporations, or corporations with anonymous ownership) or similar structures in OFCs (Offshore Financial Centers). Such advisors are loath to ask any penetrating questions about the wealth and activities of their clients, since the average fees criminals pay them to launder their money can be as much as 20 percent.

In 2010, a five-level model was designed in France. According to this model, cyberspace is composed of five layers based on information discoveries: language, writing, printing, Internet, etc. This original model links the world of information to telecommunication technologies.





</doc>
<doc id="7382" url="https://en.wikipedia.org/wiki?curid=7382" title="The Maritimes">
The Maritimes

The Maritimes, also called the Maritime provinces () or the Canadian Maritimes, is a region of Eastern Canada consisting of three provinces: New Brunswick, Nova Scotia, and Prince Edward Island (PEI). The Maritimes had a population of 1,813,606 in 2016. The Maritimes, along with a fourth province – Canada's easternmost province, Newfoundland and Labrador – make up the region of Atlantic Canada.

Located along the Atlantic coast, various aquatic sub-basins are located in the Maritimes, such as the Gulf of Maine and Gulf of St. Lawrence. The region is located northeast of New England, southeast of Quebec's Gaspé Peninsula, and southwest of the island of Newfoundland. The notion of a Maritime Union has been proposed at various times in Canada's history; the first discussions in 1864 at the Charlottetown Conference contributed to Canadian Confederation which instead formed the larger Dominion of Canada. The Mi'kmaq, Maliseet and Passamaquoddy people are indigenous to the Maritimes, while Acadian and British settlements date to the 17th century.

The word maritime is an adjective that simply means "of the sea", thus any land associated with the sea can be considered a maritime state or province (all provinces of Canada except Alberta and Saskatchewan border the sea). Nonetheless, the term "Maritimes" has historically been collectively applied to New Brunswick, Nova Scotia and Prince Edward Island, all of which border the Atlantic Ocean. In other provinces except Newfoundland & Labrador and British Columbia human settlement along the sea is sparse, since the Hudson Bay area is northerly and has a severe climate, with the majority of the populations of Ontario, Quebec and Manitoba residing far inland.

Following the northerly retreat of glaciers at the end of the Wisconsin glaciation over 10,000 years ago, human settlement by First Nations began in the Maritimes with Paleo-Indians during the "Early Period", ending around 6,000 years ago.

The "Middle Period", starting 6,000 years ago, and ending 3,000 years ago, was dominated by rising sea levels from the melting glaciers in polar regions. This is also when what is called the "Laurentian tradition" started among Archaic Indians, existing First Nations peoples of the time. Evidence of Archaic Indian burial mounds and other ceremonial sites existing in the Saint John River valley has been uncovered.

The "Late Period" extended from 3,000 years ago until first contact with European settlers and was dominated by the organization of First Nations peoples into the Algonquian-influenced Abenaki Nation which existed largely in present-day interior Vermont, New Hampshire, and Maine, and the Mi'kmaq Nation which inhabited all of Nova Scotia, Prince Edward Island, eastern New Brunswick and the southern Gaspé. The primarily agrarian Maliseet Nation settled throughout the Saint John River and Allagash River valleys of present-day New Brunswick and Maine. The Passamaquoddy Nation inhabited the northwestern coastal regions of the present-day Bay of Fundy. The Mi'kmaq Nation is also assumed to have crossed the present-day Cabot Strait at around this time to settle on the south coast of Newfoundland but were in a minority position compared to the Beothuk Nation.

The Maritimes were the second area in Canada to be settled by Europeans, after Newfoundland. There is evidence that Viking explorers discovered and settled in the Vinland region around 1000 AD, which is when the L'Anse aux Meadows settlement in Newfoundland and Labrador has been dated, and it is possible that further exploration was made into the present-day Maritimes and northeastern United States.

Both Giovanni Caboto (John Cabot) and Giovanni da Verrazzano are reported to have sailed in or near Maritime waters during their voyages of discovery for England and France respectively. Several Portuguese explorers/cartographers have also documented various parts of the Maritimes, namely Diogo Homem. However, it was French explorer Jacques Cartier who made the first detailed reconnaissance of the region for a European power, and in so doing, claimed the region for the King of France. Cartier was followed by nobleman Pierre Dugua, Sieur de Monts who was accompanied by explorer/cartographer Samuel de Champlain in a 1604 expedition where they established the second permanent European settlement in what is now the United States and Canada, following Spain's settlement at St. Augustine. Champlain's settlement at Saint Croix Island, later moved to Port-Royal, survived where the ill-fated English settlement at Roanoke did not, and pre-dated the more successful English settlement at Jamestown by three years. Champlain went on to greater fame as the founder of New France's province of Canada which comprises much of the present-day lower St. Lawrence River valley in the province of Quebec.

Champlain's success in the region, which came to be called "Acadie", led to the fertile tidal marshes surrounding the southeastern and northeastern reaches of the Bay of Fundy being populated by French immigrants who called themselves "Acadien". Acadians eventually built small settlements throughout what is today mainland Nova Scotia and New Brunswick, as well as Île-Saint-Jean (Prince Edward Island), Île-Royale (Cape Breton Island), and other shorelines of the Gulf of St. Lawrence in present-day Newfoundland and Labrador, and Quebec. Acadian settlements had primarily agrarian economies, although there were many early examples of Acadian fishing settlements in southwestern Nova Scotia and in Île-Royale, as well as along the south and west coasts of Newfoundland, the Gaspé Peninsula, and the present-day Côte-Nord region of Quebec. Most Acadian fishing activities were overshadowed by the comparatively enormous seasonal European fishing fleets based out of Newfoundland which took advantage of proximity to the Grand Banks.

The growing English colonies along the American seaboard to the south and various European wars between England and France during the 17th and 18th centuries brought Acadia to the centre of world-scale geopolitical forces. In 1613, Virginian raiders captured Port-Royal, and in 1621 Acadia was ceded to Scotland's Sir William Alexander who renamed it "Nova Scotia". By 1632, Acadia was returned from Scotland to France under the "Treaty of Saint-Germain-en-Laye", and the Port Royale settlement was moved to the site of nearby present-day Annapolis Royal. More French settlers, primarily from the Vienne, Normandie, and Brittany regions of France, continued to populate the colony of Acadia during the latter part of the 17th and early part of the 18th centuries. Important settlements also began in the Beaubassin region of the present-day Isthmus of Chignecto, and in the Saint John River valley, and settlers began to establish communities on Île-Saint-Jean and Île-Royale as well.

In 1654, New England raiders attacked Acadian settlements on the Annapolis Basin, starting a period of uncertainty for Acadians throughout the English constitutional crises under Oliver Cromwell, and only being properly resolved under the Treaty of Breda in 1667 when France's claim to the region was reaffirmed. Colonial administration by France throughout the history of Acadia was contemptuous at best. France's priorities were in settling and strengthening its claim on New France and the exploration and settlement of interior North America and the Mississippi River valley.

Over 74 years (1689–1763) there were six colonial wars, which involved continuous warfare between New England and Acadia (see the French and Indian Wars as well as Father Rale's War and Father Le Loutre's War). Throughout these wars, New England was allied with the Iroquois Confederacy and Acadia was allied with the Wabanaki Confederacy. In the first war, King William's War (the North American theater of the Nine Years' War), natives from the Maritime region participated in numerous attacks with the French on the Acadia/ New England border in southern Maine (e.g., Raid on Salmon Falls). New England retaliatory raids on Acadia, such as the Raid on Chignecto (1696), were conducted by Benjamin Church. In the second war, Queen Anne's War (the North American theater of the War of the Spanish Succession), the British conducted the Conquest of Acadia, while the region remained primarily in control of Maliseet militia, Acadia militia and Mi'kmaq militia.

In 1719, to further protect strategic interests in the Gulf of St. Lawrence and St. Lawrence River, France began the 20-year construction of a large fortress at Louisbourg on Île-Royale. Massachusetts was increasingly concerned over reports of the capabilities of this fortress, and of privateers staging out of its harbour to raid New England fishermen on the Grand Banks. In the fourth war, King George's War (the European theater of the War of the Austrian Succession), the British engaged successfully in the Siege of Louisbourg (1745). The British returned control of Île-Royale to France with the fortress virtually intact three years later under the Treaty of Aix-la-Chapelle and the French reestablished their forces there.

In 1749, to counter the rising threat of Louisbourg, Halifax was founded and the Royal Navy established a major naval base and citadel. The founding of Halifax sparked Father Le Loutre's War.
During the sixth and final colonial war, the French and Indian War (the North American theater of the Seven Years' War), the military conflicts in Nova Scotia continued. The British Conquest of Acadia happened in 1710. Over the next forty-five years the Acadians refused to sign an unconditional oath of allegiance to Britain. During this time period Acadians participated in various militia operations against the British and maintained vital supply lines to the French Fortress of Louisbourg and Fort Beausejour. The British sought to neutralize any military threat Acadians posed and to interrupt the vital supply lines Acadians provided to Louisbourg by deporting Acadians from Acadia.

The British began the Expulsion of the Acadians with the Bay of Fundy Campaign (1755). Over the next nine years over 12,000 Acadians were removed from Nova Scotia.

In 1758, the fortress of Louisbourg was laid siege for a second time within 15 years, this time by more than 27,000 British soldiers and sailors with over 150 warships. After the French surrender, Louisbourg was thoroughly destroyed by British engineers to ensure it would never be reclaimed. With the fall of Louisbourg, French and Mi'kmaq resistance in the region crumbled. British forces seized remaining French control over Acadia in the coming months, with Île-Saint-Jean falling in 1759 to British forces on their way to Quebec City for the Siege of Quebec and ensuing Battle of the Plains of Abraham.

The war ended and Britain had gained control over the entire Maritime region and the Indigenous people signed the Halifax Treaties.

Following the Seven Years' War, empty Acadian lands were settled first by New England Planters and then by immigrants brought from Yorkshire. Île-Royale was renamed Cape Breton Island and incorporated into the Colony of Nova Scotia.

Both the colonies of Nova Scotia (present-day Nova Scotia and New Brunswick) and St. John's Island (Prince Edward Island) were affected by the American Revolutionary War, largely by privateering against American shipping, but several coastal communities were also the targets of American raiders. Charlottetown, the capital of the new colony of St. John's Island, was ransacked in 1775 with the provincial secretary kidnapped and the Great Seal stolen. The largest military action in the Maritimes during the revolutionary war was the attack on Fort Cumberland (the renamed Fort Beausejour) in 1776 by a force of American sympathizers led by Jonathan Eddy. The fort was partially overrun after a month-long siege, but the attackers were ultimately repelled after the arrival of British reinforcements from Halifax.

The most significant impact from this war was the settling of large numbers of Loyalist refugees in the region, especially in Shelburne and Parrtown (Saint John). Following the Treaty of Paris in 1783, Loyalist settlers in what would become New Brunswick persuaded British administrators to split the Colony of Nova Scotia to create the new colony of New Brunswick in 1784. At the same time, another part of the Colony of Nova Scotia, Cape Breton Island, was split off to become the Colony of Cape Breton Island.
The Colony of St. John's Island was renamed to Prince Edward Island on November 29, 1798.

The War of 1812 had some effect on the shipping industry in the Maritime colonies of New Brunswick, Nova Scotia, Prince Edward Island, and Cape Breton Island; however, the significant Royal Navy presence in Halifax and other ports in the region prevented any serious attempts by American raiders. Maritime and American privateers targeted unprotected shipping of both the United States and Britain respectively, further reducing trade. New Brunswick's section of the Canada–US border did not have any significant action during this conflict, although British forces did occupy a portion of coastal Maine at one point. The most significant incident from this war which occurred in the Maritimes was the British capture and detention of the American frigate USS "Chesapeake" in Halifax.

In 1820, the Colony of Cape Breton Island was merged back into the Colony of Nova Scotia for the second time by the British government.

British settlement of the Maritimes, as the colonies of Nova Scotia, New Brunswick and Prince Edward Island came to be known, accelerated throughout the late 18th century and into the 19th century with significant immigration to the region as a result of Scottish migrants displaced by the Highland Clearances and Irish escaping the Great Irish Famine (1845–1849). As a result, significant portions of the three provinces are influenced by Celtic heritages, with Scottish Gaelic (and to a lesser degree, Irish Gaelic) having been widely spoken, particularly in Cape Breton, although it is less prevalent today.

During the American Civil War, a significant number of Maritimers volunteered to fight for the armies of the Union, while a small handful joined the Confederate Army. However, the majority of the conflict's impact was felt in the shipping industry. Maritime shipping boomed during the war due to large-scale Northern imports of war supplies which were often carried by Maritime ships as Union ships were vulnerable to Confederate naval raiders. Diplomatic tensions between Britain and the Unionist North had deteriorated after some interests in Britain expressed support for the secessionist Confederate South. The Union navy, although much smaller than the British Royal Navy and no threat to the Maritimes, did posture off Maritime coasts at times chasing Confederate naval ships which sought repairs and reprovisioning in Maritime ports, especially Halifax.
The immense size of the Union army (the largest on the planet toward the end of the Civil War), however, was viewed with increasing concern by Maritimers throughout the early 1860s. Another concern was the rising threat of Fenian raids on border communities in New Brunswick by those seeking to end British rule of Ireland. This combination of events, coupled with an ongoing decline in British military and economic support to the region as the Home Office favoured newer colonial endeavours in Africa and elsewhere, led to a call among Maritime politicians for a conference on Maritime Union, to be held in early September 1864 in Charlottetown – chosen in part because of Prince Edward Island's reluctance to give up its jurisdictional sovereignty in favour of uniting with New Brunswick and Nova Scotia into a single colony. New Brunswick and Nova Scotia felt that if the union conference were held in Charlottetown, they might be able to convince Island politicians to support the proposal.

The Charlottetown Conference, as it came to be called, was also attended by a slew of visiting delegates from the neighbouring colony of Canada, who had largely arrived at their own invitation with their own agenda. This agenda saw the conference dominated by discussions of creating an even larger union of the entire territory of British North America into a united colony. The Charlottetown Conference ended with an agreement to meet the following month in Quebec City, where more formal discussions ensued, culminating with meetings in London and the signing of the British North America Act. Of the Maritime provinces, only Nova Scotia and New Brunswick were initially party to the BNA Act, Prince Edward Island's reluctance, combined with a booming agricultural and fishing export economy having led to that colony opting not to sign on.

The major communities of the region include Halifax and Cape Breton in Nova Scotia, Saint John, Fredericton and Moncton in New Brunswick, and Charlottetown in Prince Edward Island.

In spite of its name, The Maritimes has a humid continental climate of the warm-summer subtype. Especially in coastal Nova Scotia, differences between summers and winters are pretty narrow compared with most of Canada. The inland climate of New Brunswick is in stark contrast during winter, resembling more continental areas. Summers are somewhat tempered by the marine influence throughout the provinces, but due to the southerly parallels still remain similar to more continental areas further west. Yarmouth in Nova Scotia has significant marine influence to have a borderline oceanic microclimate, but winter nights are still cold even in all coastal areas. The northernmost areas of New Brunswick are only just above subarctic with very cold continental winters.

Maritime society is based upon a mixture of traditions and class backgrounds. Predominantly rural until recent decades, the region traces many of its cultural activities to those rural resource-based economies of fishing, agriculture, forestry, and coal mining.

While Maritimers are predominantly of west European heritage (Scottish, Irish, English, and Acadian), immigration to Industrial Cape Breton during the heyday of coal mining and steel manufacturing brought people from eastern Europe as well as from Newfoundland. The Maritimes also have a black population who are mostly descendants of African American loyalists or refugees from the War of 1812, largely concentrated in Nova Scotia but also in various communities throughout southern New Brunswick, Cape Breton (where the black population is largely of West Indian descent), and Prince Edward Island. The Mi'kmaq Nation's reserves throughout Nova Scotia, Prince Edward Island and eastern New Brunswick dominate aboriginal culture in the region, compared to the much smaller population of the Maliseet Nation in western New Brunswick. New Brunswick, in general, differs from the other two Maritime Provinces in that its French population plays a significant role in the everyday cultural experience. Being the only officially bilingual province in Canada, many of New Brunswick's inhabitants speak both French and English, especially in Moncton and the capital region of Fredericton.
Cultural activities are fairly diverse throughout the region, with the music, dance, theatre, and literary art forms tending to follow the particular cultural heritage of specific locales. Notable Nova Scotian folklorist and cultural historian Helen Creighton spent the majority of her lifetime recording the various Celtic musical and folk traditions of rural Nova Scotia during the mid-20th century, prior to this knowledge being wiped out by mass media assimilation with the rest of North America. A fragment of Gaelic culture remains in Nova Scotia, specifically on Cape Breton Island. PEI and New Brunswick share this historical tie to Gaelic culture with Nova Scotia but it plays a far less significant role in their respective public provincial images.
Canada has witnessed a "Celtic revival" in which many Maritime musicians and songs have risen to prominence in recent decades. Some companies, particularly breweries such as Alexander Keith's and Moosehead have played up a connection between folklore with alcohol consumption during their marketing campaigns. The Maritimes were among the strongest supporters of prohibition (Prince Edward Island lasting until 1949), and some predominantly rural communities maintain "dry" status, banning the retail sale of alcohol as a vestige of the original temperance movement in the region.

Given the small population of the region (compared with the Central Canadian provinces or the New England states), the regional economy is a net exporter of natural resources, manufactured goods, and services. The regional economy has long been tied to natural resources such as fishing, logging, farming, and mining activities. Significant industrialization in the second half of the 19th century brought steel to Trenton, Nova Scotia, and subsequent creation of a widespread industrial base to take advantage of the region's large underground coal deposits. After Confederation, however, this industrial base withered with technological change, and trading links to Europe and the U.S. were reduced in favour of those with Ontario and Quebec. In recent years, however, the Maritime regional economy has begun increased contributions from manufacturing again and the steady transition to a service economy.

Important manufacturing centres in the region include Pictou County, Truro, the Annapolis Valley and the South Shore, and the Strait of Canso area in Nova Scotia, as well as Summerside in Prince Edward Island, and the Miramichi area, the North Shore and the upper Saint John River valley of New Brunswick.

Some predominantly coastal areas have become major tourist centres, such as parts of Prince Edward Island, Cape Breton Island, the South Shore of Nova Scotia and the Gulf of St. Lawrence and Bay of Fundy coasts of New Brunswick. Additional service-related industries in information technology, pharmaceuticals, insurance and financial sectors—as well as research-related spin-offs from the region's numerous universities and colleges—are significant economic contributors.

Another important contribution to Nova Scotia's provincial economy is through spin-offs and royalties relating to off-shore petroleum exploration and development. Mostly concentrated on the continental shelf of the province's Atlantic coast in the vicinity of Sable Island, exploration activities began in the 1960s and resulted in the first commercial production field for oil beginning in the 1980s. Natural gas was also discovered in the 1980s during exploration work, and this is being commercially recovered, beginning in the late 1990s. Initial optimism in Nova Scotia about the potential of off-shore resources appears to have diminished with the lack of new discoveries, although exploration work continues and is moving farther off-shore into waters on the continental margin.
Regional transportation networks have also changed significantly in recent decades with port modernizations, with new freeway and ongoing arterial highway construction, the abandonment of various low-capacity railway branchlines (including the entire railway system of Prince Edward Island and southwestern Nova Scotia), and the construction of the Canso Causeway and the Confederation Bridge. There have been airport improvements at various centres providing improved connections to markets and destinations in the rest of North America and overseas.

Improvements in infrastructure and the regional economy notwithstanding, the three provinces remain one of the poorer regions of Canada. While urban areas are growing and thriving, economic adjustments have been harsh in rural and resource-dependent communities, and emigration has been an ongoing phenomenon for some parts of the region. Another problem is seen in the lower average wages and family incomes within the region. Property values are depressed, resulting in a smaller tax base for these three provinces, particularly when compared with the national average which benefits from central and western Canadian economic growth.

This has been particularly problematic with the growth of the welfare state in Canada since the 1950s, resulting in the need to draw upon equalization payments to provide nationally mandated social services. Since the 1990s the region has experienced an exceptionally tumultuous period in its regional economy with the collapse of large portions of the ground fishery throughout Atlantic Canada, the closing of coal mines and a steel mill on Cape Breton Island, and the closure of military bases in all three provinces. That being said, New Brunswick has one of the largest military bases in the British Commonwealth (CFB Gagetown), which plays a significant role in the cultural and economic spheres of Fredericton, the province's capital city.

While the economic underperformance of the Maritime economy has been long lasting, it has not always been present. The mid-19th century, especially the 1850s and 1860s, has long been seen as a "Golden Age" in the Maritimes. Growth was strong, and the region had one of British North America's most extensive manufacturing sectors as well as a large international shipping industry. The question of why the Maritimes fell from being a centre of Canadian manufacturing to being an economic hinterland is thus a central one to the study of the region's pecuniary difficulties. The period in which the decline occurred had a great many potential culprits. In 1867 Nova Scotia and New Brunswick merged with the Canadas in Confederation, with Prince Edward Island joining them six years later in 1873. Canada was formed only a year after free trade with the United States (in the form of the Reciprocity Agreement) had ended. In the 1870s John A. Macdonald's National Policy was implemented, creating a system of protective tariffs around the new nation. Throughout the period there was also significant technological change both in the production and transportation of goods.
Several scholars have explored the so-called "golden age" of the Maritimes in the years just before Confederation. In Nova Scotia, the population grew steadily from 277,000 in 1851 to 388,000 in 1871, mostly from natural increase since immigration was slight. The era has been called a golden age, but that was a myth created in the 1930s to lure tourists to a romantic era of tall ships and antiques. Recent historians using census data have shown that is a fallacy. In 1851–1871 there was an overall increase in per capita wealth holding. However most of the gains went to the urban elite class, especially businessmen and financiers living in Halifax. The wealth held by the top 10% rose considerably over the two decades, but there was little improvement in the wealth levels in rural areas, which comprised the great majority of the population. Likewise Gwyn reports that gentlemen, merchants, bankers, colliery owners, shipowners, shipbuilders, and master mariners flourished. However the great majority of families were headed by farmers, fishermen, craftsmen and laborers. Most of them—and many widows as well—lived in poverty. Out migration became an increasingly necessary option. Thus the era was indeed a golden age but only for a small but powerful and highly visible elite.

The cause of economic malaise in the Maritimes is an issue of great debate and controversy among historians, economists, and geographers. The differing opinions can approximately be divided into the "structuralists," who argue that poor policy decisions are to blame, and the others, who argue that unavoidable technological and geographical factors caused the decline.

The exact date that the Maritimes began to fall behind the rest of Canada is difficult to determine. Historian Kris Inwood places the date very early, at least in Nova Scotia, finding clear signs that the Maritimes "Golden Age" of the mid-19th century was over by 1870, before Confederation or the National Policy could have had any significant impact. Richard Caves places the date closer to 1885. T.W. Acheson takes a similar view and provides considerable evidence that the early 1880s were in fact a booming period in Nova Scotia and this growth was only undermined towards the end of that decade. David Alexander argues that any earlier declines were simply part of the global Long Depression, and that the Maritimes first fell behind the rest of Canada when the great boom period of the early 20th century had little effect on the region. E.R. Forbes, however, emphasizes that the precipitous decline did not occur until after the First World War during the 1920s when new railway policies were implemented. Forbes also contends that significant Canadian defence spending during the Second World War favoured powerful political interests in Central Canada such as C.D. Howe, when major Maritime shipyards and factories, as well as Canada's largest steel mill, located in Cape Breton Island, fared poorly.
One of the most important changes, and one that almost certainly had an effect, was the revolution in transportation that occurred at this time. The Maritimes were connected to central Canada by the Intercolonial Railway in the 1870s, removing a longstanding barrier to trade. For the first time this placed the Maritime manufacturers in direct competition with those of Central Canada. Maritime trading patterns shifted considerably from mainly trading with New England, Britain, and the Caribbean, to being focused on commerce with the Canadian interior, enforced by the federal government's tariff policies.

Coincident with the construction of railways in the region, the age of the wooden sailing ship began to come to an end, being replaced by larger and faster steel steam ships. The Maritimes had long been a centre for shipbuilding, and this industry was hurt by the change. The larger ships were also less likely to call on the smaller population centres such as Saint John and Halifax, preferring to travel to cities like New York and Montreal. Even the Cunard Line, founded by Maritime-born Samuel Cunard, stopped making more than a single ceremonial voyage to Halifax each year.

More controversial than the role of technology is the argument over the role of politics in the origins of the region's decline. Confederation and the tariff and railway freight policies that followed have often been blamed for having a deleterious effect on the Maritime economies. Arguments have been made that the Maritimes' poverty was caused by control over policy by Central Canada which used the national structures for its own enrichment. This was the central view of the Maritime Rights Movement of the 1920s, which advocated greater local control over the region's finances. T.W. Acheson is one of the main proponents of this theory. He notes the growth that was occurring during the early years of the National Policy in Nova Scotia demonstrates how the effects of railway fares and the tariff structure helped undermine this growth. Capitalists from Central Canada purchased the factories and industries of the Maritimes from their bankrupt local owners and proceeded to close down many of them, consolidating the industry in Central Canada.
The policies in the early years of Confederation were designed by Central Canadian interests, and they reflected the needs of that region. The unified Canadian market and the introduction of railroads created a relative weakness in the Maritime economies. Central to this concept, according to Acheson, was the lack of metropolises in the Maritimes.

Montreal and Toronto were well suited to benefit from the development of large-scale manufacturing and extensive railway systems in Quebec and Ontario, these being the goals of the Macdonald and Laurier governments. In the Maritimes the situation was very different. Today New Brunswick has several mid-sized centres in Saint John, Moncton, and Fredericton but no significant population centre. Nova Scotia has a growing metropolitan area surrounding Halifax, but a contracting population in industrial Cape Breton, and several smaller centres in Bridgewater, Kentville, Yarmouth, and Pictou County. Prince Edward Island's only significant population centres are in Charlottetown and Summerside. During the late 19th and early 20th centuries, just the opposite was the case with little to no population concentration in major industrial centres as the predominantly rural resource-dependent Maritime economy continued on the same path as it had since European settlement on the region's shores.

Despite the region's absence of economic growth on the same scale as other parts of the nation, the Maritimes has changed markedly throughout the 20th century, partly as a result of global and national economic trends, and partly as a result of government intervention. Each sub-region within the Maritimes has developed over time to exploit different resources and expertise. Saint John became a centre of the timber trade and shipbuilding and is currently a centre for oil refining and some manufacturing. The northern New Brunswick communities of Edmundston, Campbellton, Dalhousie, Bathurst, and Miramichi are focused on the pulp and paper industry and some mining activity. Moncton was a centre for railways and has changed its focus to becoming a multi-modal transportation centre with associated manufacturing and retail interests. The Halifax metropolitan area has come to dominate peninsular Nova Scotia as a retail and service centre, but that province's industries were spread out from the coal and steel industries of industrial Cape Breton and Pictou counties, the mixed farming of the North Shore and Annapolis Valley, and the fishing industry was primarily focused on the South Shore and Eastern Shore. Prince Edward Island is largely dominated by farming, fishing, and tourism.

Given the geographic diversity of the various sub-regions within the Maritimes, policies to centralize the population and economy were not initially successful, thus Maritime factories closed while those in Ontario and Quebec prospered.

The traditional staples thesis, advocated by scholars such as S.A. Saunders, looks at the resource endowments of the Maritimes and argues that it was the decline of the traditional industries of shipbuilding and fishing that led to Maritime poverty, since these processes were rooted in geography, and thus all but inevitable. Kris Inwood has revived the staples approach and looks at a number of geographic weaknesses relative to Central Canada. He repeats Acheson's argument that the region lacks major urban centres, but adds that the Maritimes were also lacking the great rivers that led to the cheap and abundant hydro-electric power, key to Quebec and Ontario's urban and manufacturing development, that the extraction costs of Maritime resources were higher (particularly in the case of Cape Breton coal), and that the soils of the region were poorer and thus the agricultural sector weaker.

The Maritimes are the only provinces in Canada which entered Confederation in the 19th century and have kept their original colonial boundaries. All three provinces have the smallest land base in the country and have been forced to make do with resources within. By comparison, the former colony of the United Province of Canada (divided into the District of Canada East, and the District of Canada West) and the western provinces were dozens of times larger and in some cases were expanded to take in territory formerly held in British Crown grants to companies such as the Hudson's Bay Company; in particular the November 19, 1869 sale of Rupert's Land to the Government of Canada under the "Rupert's Land Act 1868" was facilitated in part by Maritime taxpayers. The economic riches of energy and natural resources held within this larger land base were only realized by other provinces during the 20th century.

The maritime provinces' main industry is fishing. Fishing can be found in any maritime province. This includes fishing for lobster, mackerel, tuna, salmon and many more kinds of fish. Oysters and salmonid aquiculture is also increasingly important economically.

Nova Scotia is very strong in agriculture, forestry and fishing.
Tourism is important to the economy of PEI. "Anne of Green Gables" was written there, so every year, it attracts families to see where the beloved story was based, and in the summers, to enjoy beaches and sunlight. PEI is also known for its agriculture, mainly the potato, and fishing industries.

Agriculture and forestry are two prominent industries found in New Brunswick. Despite having an extensive coastline, New Brunswick's industrial sector has never been entirely reliant on the success of the fisheries. Likewise, the strong shipbuilding heritage of the province directly relates to its forest resources. Because of this, New Brunswickers tend to attribute their cultural heritage less with the sea and more with their forests and rivers.

Maritime conservatism since the Second World War has been very much part of the Red Tory tradition, key influences being former Premier of Nova Scotia and federal Progressive Conservative Party leader Robert Stanfield and New Brunswick Tory strategist Dalton Camp.

In recent years, the social democratic New Democratic Party (NDP) has made significant inroads both federally and provincially in the region. The NDP has elected Members of Parliament (MPs) from New Brunswick, but most of the focus of the party at the federal and provincial levels is currently in the Halifax area of Nova Scotia. Industrial Cape Breton has historically been a region of labour activism, electing Co-operative Commonwealth Federation (and later NDP) MPs, and even produced many early members of the Communist Party of Canada in the pre-World War II era. In the 2004 federal election, the NDP captured 28.45% of the vote in Nova Scotia, more than any other province. In the 2009 provincial election the NDP formed a majority government, the first in the region.

The Maritimes are generally socially conservative but unlike Alberta, they also have fiscally socialist tendencies. It is because of the lack of support for fiscal conservatism that federal parties such as the Canadian Alliance never had much success in the region, In the 2004 federal election, the Conservatives had one of the worst showings in the region for a right-wing party, going back to Confederation, with the exception of the 1993 election. The Conservative party improved its seat count in the 2008 and elected 13 MPs in the 2011 election. However, in the 2015 election the Liberal Party won every seat in the region, defeating all of the Conservative (and NDP) challengers.

An area within the region where both fiscal and social conservatism coincide and where the federal Reform Party and Canadian Alliance had some success is in central-western part of New Brunswick, in the Saint John River valley north of Saint John and south of Grand Falls. Contributing demographics include a predominantly Anglophone population residing in a largely rural agrarian setting. One influence might be proximity to the Canada–US border and the state of Maine. The valley is also settled by descendants of United Empire Loyalists whose influence continues in the area. There are also a large number of active and retired military personnel located in the Fredericton and Oromocto area as a result of the large military base at CFB Gagetown. Another area in the region with smatterings of coinciding fiscal and social conservatism is the Annapolis Valley of Nova Scotia.

The Liberal Party of Canada has done well in the Maritimes in the past because of its interventionist policies. The Acadian Peninsula region of New Brunswick, long dependent upon seasonal employment in the Gulf of St. Lawrence fishery, tends to vote for the Liberals or NDP for this reason. In the 1997 federal election, Prime Minister Jean Chrétien's Liberals endured a bitter defeat to the PCs and NDP in many ridings as a result of unpopular cuts to unemployment benefits for seasonal workers, as well as closures of several Canadian Forces Bases, the refusal to honour a promise to rescind the Goods and Services Tax, cutbacks to provincial equalization payments, health care, post-secondary education and regional transportation infrastructure such as airports, fishing harbours, seaports, and railways. The Liberals held onto seats in Prince Edward Island and New Brunswick, while being shut out of Nova Scotia entirely, the second time in history (the only other time being the Diefenbaker sweep). In 2015 the Liberals won every seat in The Maritimes, defeating Conservative and NDP incumbents.

The Maritimes is currently represented in the Canadian Parliament by 25 Members of the House of Commons (Nova Scotia – 11, New Brunswick – 10, Prince Edward Island – 4) and 24 Senators (Nova Scotia and New Brunswick – 10 each, Prince Edward Island – 4). This level of representation was established at the time of Confederation when the Maritimes had a much larger proportion of the national population. The comparatively large population growth of western and central Canada during the immigration boom of the 20th century has reduced the Maritimes' proportion of the national population to less than 10%, resulting in an over-representation in Parliament, with some federal ridings having fewer than 35,000 people, compared to central and western Canada where ridings typically contain 100,000–120,000 people.

The Senate of Canada is structured along regional lines, giving an equal number of seats (24) to the Maritimes, Ontario, Quebec, and western Canada, in addition to the later entry of Newfoundland and Labrador, as well as the three territories. Enshrined in the Constitution, this model was developed to ensure that no area of the country is able to exert undue influence in the Senate. The Maritimes, with its much smaller proportion of the national population (compared to the time of Confederation) also have an over-representation in the Senate, particularly compared to the population growth of Ontario and the western provinces. This has led to calls to reform the Senate; however, such a move would entail constitutional changes.

Another factor related to the number of Senate seats is that a constitutional amendment in the early 20th century mandated that no province can have fewer Members of Parliament than it has senators. This court decision resulted from a complaint by the Government of Prince Edward Island after that province's number of MPs was proposed to change from 4 to 3, accounting for its declining proportion of the national population at that time. When PEI entered Confederation in 1873, it was accorded 6 MPs and 4 Senators; however this was reduced to 4 MPs by the early 20th century. Senators being appointed for life at this time, these coveted seats rarely went unfilled for a long period of time anywhere in Canada. As a result, PEI's challenge was accepted by the federal government, and its level of federal representation was secured. In the aftermath of the 1989 budget, which saw a fillibuster by Liberal Senators in attempt to kill legislation creating the Goods and Services Tax, Prime Minister Brian Mulroney "stacked" the Senate by creating additional seats in several provinces across Canada, including New Brunswick; however, there was no attempt by these provinces to increase the number of MPs to reflect this change in Senate representation.




</doc>
<doc id="7383" url="https://en.wikipedia.org/wiki?curid=7383" title="Cyril of Alexandria">
Cyril of Alexandria

Cyril of Alexandria (; ; c. 376 – 444) was the Patriarch of Alexandria from 412 to 444. He was enthroned when the city was at the height of its influence and power within the Roman Empire. Cyril wrote extensively and was a leading protagonist in the Christological controversies of the late-4th and 5th centuries. He was a central figure in the Council of Ephesus in 431, which led to the deposition of Nestorius as Patriarch of Constantinople.

Cyril is counted among the Church Fathers and the Doctors of the Church, and his reputation within the Christian world has resulted in his titles "Pillar of Faith" and "Seal of all the Fathers", but Theodosius II, the Roman Emperor, condemned him for behaving like a "proud pharaoh", and the Nestorian bishops at the Council of Ephesus declared him a heretic, labelling him as a "monster, born and educated for the destruction of the church."

Cyril is well-known due to his dispute with Nestorius and his supporter Patriarch John of Antioch, whom Cyril excluded from the Council of Ephesus for arriving late. He is also known for his expulsion of Novatians and Jews from Alexandria and for inflaming tensions that led to the murder of the Hellenistic philosopher Hypatia by a Christian mob. Historians disagree over the extent of his responsibility in this. 

The Roman Catholic Church did not commemorate Saint Cyril in the Tridentine Calendar: it added his feast only in 1882, assigning to it the date of 9 February. This date is used by the Western Rite Orthodox Church. Yet the 1969 Catholic Calendar revision moved it to 27 June, considered to be the day of the saint's death, as celebrated by the Coptic Orthodox Church. The same date has been chosen for the Lutheran calendar. The Eastern Orthodox and Byzantine Catholic Churches celebrate his feast day on 9 June and also, together with Pope Athanasius I of Alexandria, on 18 January.

Little is known for certain of Cyril's early life. He was born c. 376, in the small town of Theodosios, Egypt, near modern-day El-Mahalla El-Kubra. A few years after his birth, his maternal uncle Theophilus rose to the powerful position of Patriarch of Alexandria. His mother remained close to her brother and under his guidance, Cyril was well educated. His writings show his knowledge of Christian writers of his day, including Eusebius, Origen, Didymus the Blind, and writers of the Church of Alexandria. He received the formal Christian education standard for his day: he studied grammar from age twelve to fourteen (390–392), rhetoric and humanities from fifteen to twenty (393–397) and finally theology and biblical studies (398–402). In 403 he accompanied his uncle to attend a synod in Constantinople.

Theophilus died on 15 October 412, and Cyril was made Pope or Patriarch of Alexandria on 18 October 412, but only after a riot between his supporters and those of his rival Archdeacon Timotheus. According to Socrates Scholasticus, the Alexandrians were always rioting. 

Thus, Cyril followed his uncle in a position that had become powerful and influential, rivalling that of the prefect in a time of turmoil and frequently violent conflict between the cosmopolitan city's Pagan, Jewish, and Christian inhabitants. He began to exert his authority by causing the churches of the Novatianists to be closed and their sacred vessels to be seized.

Orestes, "Praefectus augustalis" of the Diocese of Egypt, steadfastly resisted Cyril's ecclesiastical encroachment onto secular prerogatives. 

Tension between the parties increased when in 415, Orestes published an edict that outlined new regulations regarding mime shows and dancing exhibitions in the city, which attracted large crowds and were commonly prone to civil disorder of varying degrees. Crowds gathered to read the edict shortly after it was posted in the city's theater. Cyril sent the "grammaticus" Hierax to discover the content of the edict. The edict angered Christians as well as Jews. At one such gathering, Hierax, read the edict and applauded the new regulations, prompting a disturbance. Many people felt that Hierax was attempting to incite the crowd into sedition. Orestes had Hierax tortured in public in a theatre. This order had two aims: the first was to quell the riot, the other to mark Orestes' authority over Cyril.

Socrates Scholasticus recounts that upon hearing of Hierex's severe and public punishment, Cyril threatened to retaliate against the Jews of Alexandria with "the utmost severities" if the harassment of Christians did not cease immediately. In response to Cyril's threat, the Jews of Alexandria grew even more furious, eventually resorting to violence against the Christians. They plotted to flush the Christians out at night by running through the streets claiming that the Church of Alexander was on fire. When Christians responded to what they were led to believe was the burning down of their church, "the Jews immediately fell upon and slew them" by using rings to recognize one another in the dark and killing everyone else in sight. When the morning came, the Jews of Alexandria could not hide their guilt, and Cyril, along with many of his followers, took to the city’s synagogues in search of the perpetrators of the massacre.

After Cyril rounded up all the Jews in Alexandria, he ordered them to be stripped of all possessions, banished them from Alexandria, and allowed their goods to be pillaged by the remaining citizens of Alexandria. With Cyril's banishment of the Jews, "Orestes [...] was filled with great indignation at these transactions, and was excessively grieved that a city of such magnitude should have been suddenly bereft of so large a portion of its population." Because of this, the feud between Cyril and Orestes intensified, and both men wrote to the emperor regarding the situation. Eventually, Cyril attempted to reach out to Orestes through several peace overtures, including attempted mediation and, when that failed, showed him the Gospels, which he interpreted to indicate that the religious authority of Cyril would require Orestes' acquiescence in the bishop's policy. Nevertheless, Orestes remained unmoved by such gestures.

This refusal almost cost Orestes his life. Nitrian monks came from the desert and instigated a riot against Orestes among the population of Alexandria. These monks' had resorted to violence 15 years before, during a controversy between Theophilus (Cyril's uncle) and the "Tall Brothers"; The monks assaulted Orestes and accused him of being a pagan. Orestes rejected the accusations, showing that he had been baptised by the Archbishop of Constantinople. A monk named Ammonius, threw a stone hitting Orestes in the head. The prefect had Ammonius tortured to death, whereupon the Patriarch honored him as a martyr. However, according to Scholasticus, the Christian community displayed a general lack of enthusiasm for Ammonius's case for martyrdom. The prefect then wrote to the emperor Theodosius II, as did Cyril.

Prefect Orestes enjoyed the political backing of Hypatia, an astronomer, philosopher and mathematician who had considerable moral authority in the city of Alexandria, and who had extensive influence. At the time of her death, she was likely over sixty years of age. Indeed, many students from wealthy and influential families came to Alexandria purposely to study privately with Hypatia, and many of these later attained high posts in government and the Church. Several Christians thought that Hypatia's influence had caused Orestes to reject all reconciliatory offerings by Cyril. Modern historians think that Orestes had cultivated his relationship with Hypatia to strengthen a bond with the Pagan community of Alexandria, as he had done with the Jewish one, in order to better manage the tumultuous political life of the Egyptian capital. A mob, led by a lector named Peter, took Hypatia from her chariot and murdered her, hacking her body apart and burning the pieces outside the city walls. 

Neoplatonist historian Damascius (c. 458 – c. 538) was "anxious to exploit the scandal of Hypatia's death", and attributed responsibility for her murder to Bishop Cyril and his Christian followers. Damascius's account of the Christian murder of Hypatia is the sole historical source attributing direct responsibility to Bishop Cyril. Some modern studies represent Hypatia's death as the result of a struggle between two Christian factions, the moderate Orestes, supported by Hypatia, and the more rigid Cyril. According to lexicographer William Smith, "She was accused of too much familiarity with Orestes, prefect of Alexandria, and the charge spread among the clergy, who took up the notion that she interrupted the friendship of Orestes with their archbishop, Cyril." Scholasticus writes that Hypatia ultimately fell "victim to the political jealousy which at the time prevailed". News of Hypatia's murder provoked great public denouncement, not only against Cyril but against the whole Alexandrian Christian community.

Another major conflict was between the Alexandrian and Antiochian schools of ecclesiastical reflection, piety, and discourse. This long running conflict widened with the third canon of the First Council of Constantinople which granted the see of Constantinople primacy over the older sees of Alexandria and Antioch. Thus, the struggle between the sees of Alexandria and Antioch now included Constantinople. The conflict came to a head in 428 after Nestorius, who originated in Antioch, was made Archbishop of Constantinople.
Cyril gained an opportunity to restore Alexandria's pre-eminence over both Antioch and Constantinople when an Antiochine priest who was in Constantinople at Nestorius' behest began to preach against calling Mary the "Mother of God". As the term "Mother of God" had long been attached to Mary, the laity in Constantinople complained against the priest. Rather than repudiating the priest, Nestorius intervened on his behalf. Nestorius argued that Mary was neither a "Mother of Man" nor "Mother of God" as these referred to Christ's two natures; rather, Mary was the "Mother of Christ". Christ, according to Nestorius, was the conjunction of the Godhead with his "temple" (which Nestorius was fond of calling his human nature). The controversy seemed to be centered on the issue of the suffering of Christ. Cyril maintained that the Son of God or the divine Word, truly suffered "in the flesh." However, Nestorius claimed that the Son of God was altogether incapable of suffering, even within his union with the flesh. Eusebius of Dorylaeum went so far as to accuse Nestorius of adoptionism. By this time, news of the controversy in the capital had reached Alexandria. At Easter 429 A.D., Cyril wrote a letter to the Egyptian monks warning them of Nestorius' views. A copy of this letter reached Constantinople where Nestorius preached a sermon against it. This began a series of letters between Cyril and Nestorius which gradually became more strident in tone. Finally, Emperor Theodosius II convoked the Council of Ephesus (in 431) to solve the dispute. Cyril selected Ephesus as the venue since it supported the veneration of Mary. The council was convoked before Nestorius's supporters from Antioch and Syria had arrived and thus Nestorius refused to attend when summoned. Predictably, the Council ordered the deposition and exile of Nestorius for heresy.

However, when John of Antioch and the other pro-Nestorius bishops finally reached Ephesus, they assembled their own Council, condemned Cyril for heresy, deposed him from his see, and labelled him as a "monster, born and educated for the destruction of the church". Theodosius, by now old enough to hold power by himself, annulled the verdict of the Council and arrested Cyril, but Cyril eventually escaped. Having fled to Egypt, Cyril bribed Theodosius' courtiers, and sent a mob led by Dalmatius, a hermit, to besiege Theodosius' palace, and shout abuse; the Emperor eventually gave in, sending Nestorius into minor exile (Upper Egypt). 
Cyril died about 444, but the controversies were to continue for decades, from the "Robber Synod" of Ephesus (449) to the Council of Chalcedon (451) and beyond.

Cyril regarded the embodiment of God in the person of Jesus Christ to be so mystically powerful that it spread out from the body of the God-man into the rest of the race, to reconstitute human nature into a graced and deified condition of the saints, one that promised immortality and transfiguration to believers. Nestorius, on the other hand, saw the incarnation as primarily a moral and ethical example to the faithful, to follow in the footsteps of Jesus. Cyril's constant stress was on the simple idea that it was God who walked the streets of Nazareth (hence Mary was "Theotokos", meaning "God bearer", which became in Latin "Mater Dei or Dei Genetrix", or Mother of God), and God who had appeared in a transfigured humanity. Nestorius spoke of the distinct "Jesus the man" and "the divine Logos" in ways that Cyril thought were too dichotomous, widening the ontological gap between man and God in a way that some of his contemporaries believed would annihilate the person of Christ.

The main issue that prompted this dispute between Cyril and Nestorius was the question which arose at the Council of Constantinople: What exactly was the being to which Mary gave birth? Cyril affirmed that the Holy Trinity consists of a singular divine nature, essence, and being ("ousia") in three distinct aspects, instantiations, or subsistencies of being ("hypostases"). These distinct hypostases are the Father or God in Himself, the Son or Word ("Logos"), and the Holy Spirit. Then, when the Son became flesh and entered the world, the pre-Incarnate divine nature and assumed human nature both remained, but became "united" in the person of Jesus. This resulted in the miaphysite slogan "One Nature united out of two" being used to encapsulate the theological position of this Alexandrian bishop. 
According to Cyril's theology, there were two states for the Son of God: the state that existed "prior" to the Son (or Word/Logos) becoming enfleshed in the person of Jesus and the state that actually became enfleshed. The Logos Incarnate suffered and died on the Cross, and therefore the Son was able to suffer without suffering. Cyril passionately argued for the continuity of a single subject, God the Word, from the pre-Incarnate state to the Incarnate state. The divine Logos was really present in the flesh and in the world—not merely bestowed upon, semantically affixed to, or morally associated with the man Jesus, as the adoptionists and, he believed, Nestorius had taught.

Cyril of Alexandria became noted in Church history because of his spirited fight for the title "Theotokos" during the First Council of Ephesus (431).

His writings include the homily given in Ephesus and several other sermons. Some of his alleged homilies are in dispute as to his authorship. In several writings, Cyril focuses on the love of Jesus to his mother. On the Cross, he overcomes his pain and thinks of his mother. At the wedding in Cana, he bows to her wishes. Cyril created the basis for all other mariological developments through his teaching of the blessed Virgin Mary, as the "Mother of God." The conflict with Nestorius was mainly over this issue, and it has often been misunderstood. "[T]he debate was not so much about Mary as about Jesus. The question was not what honors were due to Mary, but how one was to speak of the birth of Jesus."
St. Cyril received an important recognition of his preachings by the Second Council of Constantinople (553 d.C.) which declared;

Cyril was a scholarly archbishop and a prolific writer. In the early years of his active life in the Church he wrote several exegetical documents. Among these were: "Commentaries on the Old Testament", "Thesaurus", "Discourse Against Arians", "Commentary on St. John's Gospel", and "Dialogues on the Trinity". In 429 as the Christological controversies increased, the output of his writings was so extensive that his opponents could not match it. His writings and his theology have remained central to the tradition of the Fathers and to all Orthodox to this day.



Konrad F. Zawadzki, Der Kommentar Cyrills von Alexandrien zum 1. Korintherbrief. Einleitung, kritischer Text, Übersetzung, Einzelanalyse, Traditio Exegetica Graeca 16, Leuven-Paris-Bristol, CT, 2015

Konrad F. Zawadzki, Syrische Fragmente des Kommentars Cyrills von Alexandrien zum 1. Korintherbrief, Zeitschrift für Antikes Christentum 21 (2017), 304-360



</doc>
<doc id="7387" url="https://en.wikipedia.org/wiki?curid=7387" title="Cyril of Jerusalem">
Cyril of Jerusalem

Cyril of Jerusalem (; ) was a distinguished theologian of the early Church ( 313 386 AD). He is venerated as a saint by the Roman Catholic Church, the Eastern Orthodox Church, Oriental Orthodox Church and the Anglican Communion. In 1883, Cyril was declared a Doctor of the Church by Pope Leo XIII. He is highly respected in the Palestinian Christian Community.

About the end of 350 AD he succeeded Maximus as Bishop of Jerusalem, but was exiled on more than one occasion due to the enmity of Acacius of Caesarea, and the policies of various emperors. Cyril left important writings documenting the instruction of catechumens and the order of the Liturgy in his day.

Little is known of his life before he became a bishop; the assignment of his birth to the year 315 rests on conjecture. According to Butler, Cyril was born at or near the city of Jerusalem, and was apparently well-read in both the Church fathers and the pagan philosophers.

Cyril was ordained a deacon by Bishop St. Macarius of Jerusalem in about 335 and a priest some eight years later by Bishop St. Maximus. About the end of 350 he succeeded St. Maximus in the See of Jerusalem.

Relations between Metropolitan Acacius of Caesarea and Cyril became strained. Acacius is presented as a leading Arian by the orthodox historians, and his opposition to Cyril in the 350s is attributed by these writers to this. Sozomen also suggests that the tension may have been increased by Acacius's jealousy of the importance assigned to St. Cyril's See by the Council of Nicaea, as well as by the threat posed to Caesarea by the rising influence of the seat of Jerusalem as it developed into the prime Christian holy place and became a centre of pilgrimage.

Acacius charged Cyril with selling church property. The city of Jerusalem had suffered drastic food shortages at which point church historians Sozomen and Theodoret report “Cyril secretly sold sacramental ornaments of the church and a valuable holy robe, fashioned with gold thread that the emperor Constantine had once donated for the bishop to wear when he performed the rite of Baptism”. It was believed that Cyril sold some plate, ornaments and imperial gifts to keep his people from starving.

For two years, Cyril resisted Acacius' summons to account for his actions in selling off church property, but a council held under Acacius's influence in 357 deposed St. Cyril in his absence (having officially charged him with selling church property to help the poor) and Cyril took refuge with Silvanus, Bishop of Tarsus. The following year, 359, in an atmosphere hostile to Acacius, the Council of Seleucia reinstated Cyril and deposed Acacius. In 360, though, this was reversed by Emperor Constantius, and Cyril suffered another year's exile from Jerusalem until the Emperor Julian's accession allowed him to return.

Cyril was once again banished from Jerusalem by the Arian Emperor Valens in 367. St. Cyril was able to return again at the accession of Emperor Gratian in 378, after which he remained undisturbed until his death in 386. In 380, St. Gregory of Nyssa came to Jerusalem on the recommendation of a council held at Antioch in the preceding year. He found the faith in accord with the truth, but the city a prey to parties and corrupt in morals. Cyril's jurisdiction over Jerusalem was expressly confirmed by the First Council of Constantinople (381), at which he was present. At that council he voted for acceptance of the term "homoousios", having been finally convinced that there was no better alternative. His story is perhaps best representative of those Eastern bishops (perhaps a majority), initially mistrustful of Nicaea, who came to accept the creed of that council, and the doctrine of the "homoousion".

Though his theology was at first somewhat indefinite in phraseology, he undoubtedly gave a thorough adhesion to the Nicene Orthodoxy. Even if he did avoid the debatable term "homoousios", he expressed its sense in many passages, which exclude equally Patripassianism, Sabellianism, and the formula "there was a time when the Son was not" attributed to Arius. In other points he takes the ordinary ground of the Eastern Fathers, as in the emphasis he lays on the freedom of the will, the "autexousion" (αὐτεξούσιον), and in his view of the nature of sin. To him sin is the consequence of freedom, not a natural condition. The body is not the cause, but the instrument of sin. The remedy for it is repentance, on which he insists. Like many of the Eastern Fathers, he focuses on high moral living as essential to true Christianity. His doctrine of the Resurrection is not quite so realistic as that of other Fathers; but his conception of the Church is decidedly empirical: the existing Church form is the true one, intended by Christ, the completion of the Church of the Old Testament. His interpretation of the Eucharist is disputed. If he sometimes seems to approach the symbolic view, at other times he comes very close to a strong realistic doctrine. The bread and wine are not mere elements, but the body and blood of Christ.

Cyril's writings are filled with the loving and forgiving nature of God which was somewhat uncommon during his time period. Cyril fills his writings with great lines of the healing power of forgiveness and the Holy Spirit, like “The Spirit comes gently and makes himself known by his fragrance. He is not felt as a burden for God is light, very light. Rays of light and knowledge stream before him as the Spirit approaches. The Spirit comes with the tenderness of a true friend to save, to heal, to teach, to counsel, to strengthen and to console”. Cyril himself followed God's message of forgiveness many times throughout his life. This is most clearly seen in his two major exiles where Cyril was disgraced and forced to leave his position and his people behind. He never wrote or showed any ill will towards those who wronged him. Cyril stressed the themes of healing and regeneration in his catechesis.

Cyril's famous twenty-three lectures given to catechumens in Jerusalem being prepared for, and after, baptism are best considered in two parts: the first eighteen lectures are common known as the "Catechetical Lectures", "Catechetical Orations" or "Catechetical Homilies", while the final five are often called the "Mystagogic Catecheses" (μυσταγωγικαί), because they deal with the "mysteries" (μυστήρια) i.e. Sacraments of Baptism, Confirmation and the Eucharist.

His catechetical lectures (Greek "Κατηχήσεις") are generally assumed, on the basis of limited evidence, to have been delivered either in Cyril's early years as a bishop, around 350, or perhaps in 348, while Cyril was still a priest, deputising for his bishop, Maximus. The "Catechetical Lectures" were given in the "Martyrion", the basilica erected by Constantine. They contain instructions on the principal topics of Christian faith and practice, in rather a popular than a scientific manner, full of a warm pastoral love and care for the catechumens to whom they were delivered. Each lecture is based upon a text of Scripture, and there is an abundance of Scriptural quotation throughout. In the "Catechetical Lectures", parallel with the exposition of the Creed as it was then received in the Church of Jerusalem are vigorous polemics against pagan, Jewish, and heretical errors. They are of great importance for the light which they throw upon the method of instruction usual of that age, as well as upon the liturgical practises of the period, of which they give the fullest account extant.

It is not only among us, who are marked with the name of Christ, that the dignity of faith is great; all the business of the world, even of those outside the Church, is accomplished by faith. By faith, marriage laws join in union persons who were strangers to one another. By faith, agriculture is sustained; for a man does not endure the toil involved unless he believes he will reap a harvest. By faith, seafaring men, entrusting themselves to a tiny wooden craft, exchange the solid element of the land for the unstable motion of the waves.”

In the 13th lecture, Cyril of Jerusalem discusses the Crucifixion and burial of Jesus Christ. The main themes that Cyril focuses on in these lectures are Original sin and Jesus’ sacrificing himself to save us from our sins. Also, the burial and Resurrection which occurred three days later proving the divinity of Jesus Christ and the loving nature of the Father. Cyril was very adamant about the fact that Jesus went to his death with full knowledge and willingness. Not only did he go willingly but throughout the process he maintained his faith and forgave all those who betrayed him and engaged in his execution. Cyril writes “who did not sin, neither was deceit found in his mouth, who, when he was reviled, did not revile, when he suffered did not threaten”. This line by Cyril shows his belief in the selflessness of Jesus especially in this last final act of Love. The lecture also gives a sort of insight to what Jesus may have been feeling during the execution from the whippings and beatings, to the crown of thorns, to the nailing on the cross. Cyril intertwines the story with the messages Jesus told throughout his life before his execution relating to his final act. For example, Cyril writes “I gave my back to those who beat me and my cheeks to blows; and my face I did not shield from the shame of spitting”. This clearly reflects the teachings of Jesus to turn the other cheeks and not raising your hands against violence because violence just begets violence begets violence. The segment of the Catechesis really reflects the voice Cyril maintained in all of his writing. The writings always have the central message of the Bible; Cyril is not trying to add his own beliefs in reference to religious interpretation and remains grounded in true biblical teachings.

Danielou see the baptism rite are carrying eschatological overtones, in that "to inscribe for baptism is to write one's name in the register of the elect in heaven".

Oded Irshai observed that Cyril lived in a time of intense apocalyptic expectation, when Christians were eager to find apocalyptic meaning in every historical event or natural disaster. Cyril spent a good part of his episcopacy in intermittent exile from Jerusalem. Abraham Malherbe argued that when a leader's control over a community is fragile, directing attention to the imminent arrival of the antichrist effectively diverts attention from that fragility.

Soon after his appointment, Cyril in his "Letter to Constantius" of 351 recorded the appearance of a cross of light in the sky above Golgotha, witnessed by the whole population of Jerusalem. The Greek church commemorates this miracle on the 7th of May. Though in modern times the authenticity of the "Letter" has been questioned, on the grounds that the word "homoousios" occurs in the final blessing, many scholars believe this may be a later interpolation, and accept the letter's authenticity on the grounds of other pieces of internal evidence.

Cyril interpreted this as both a sign of support for Constantius, who was soon to face the usurper Magnentius, and as announcing the Second Coming, which was soon to take place in Jerusalem. Not surprisingly, in Cyril's eschatological analysis, Jerusalem holds a central position.

Matthew 24:6 speaks of "wars and reports of wars", as a sign of the End Times, and it is within this context that Cyril read Julian's war with the Persians. Matthew 24:7 speaks of "earthquakes from place to place", and Jerusalem experienced an earthquake in 363 at a time when Julian was attempting to rebuild the temple in Jerusalem. Embroiled in a rivalry with Acacius of Caesarea over the relative primacy of their respective sees, Cyril saw even ecclesial discord a sign of the Lord's coming. Catechesis 15 would appear to cast Julian as the antichrist, although Irshai views this as a later interpolation.

“In His first coming, He endured the Cross, despising shame; in His second, He comes attended by a host of Angels, receiving glory. We rest not then upon His first advent only, but look also for His second." He looked forward to the Second Advent which would bring an end to the world and then the created world to be re-made anew. At the Second Advent he expected to rise in the resurrection if it came after his time on earth.

There has been considerable controversy over the date and authorship of the "Mystagogic Catecheses", addressed to the newly baptized, in preparation for the reception of Holy Communion, with some scholars having attributed them to Cyril's successor as Bishop of Jerusalem, John. Many scholars would currently view the "Mystagogic Catecheses" as being written by Cyril, but in the 370s or 380s, rather than at the same time as the "Catechetical Lectures".

According to the Spanish pilgrim Egeria, these "mystagogical catecheses" were given to the newly baptised in the Church of the "Anastasis" in the course of Easter Week.







</doc>
<doc id="7388" url="https://en.wikipedia.org/wiki?curid=7388" title="Hanukkah">
Hanukkah

Hanukkah ( ; ', Tiberian: ', usually spelled , pronounced in Modern Hebrew, or in Yiddish; a transliteration also romanized as Chanukah or Ḥanukah) is a Jewish holiday commemorating the rededication of the Holy Temple (the Second Temple) in Jerusalem at the time of the Maccabean Revolt against the Seleucid Empire. Hanukkah is observed for eight nights and days, starting on the 25th day of Kislev according to the Hebrew calendar, which may occur at any time from late November to late December in the Gregorian calendar. It is also known as the Festival of Lights and the Feast of Dedication.

The festival is observed by lighting the candles of a candelabrum with nine branches, called a Hanukkah menorah (or hanukkiah). One branch is typically placed above or below the others and its candle is used to light the other eight candles. This unique candle is called the "shamash" (, "attendant"). Each night, one additional candle is lit by the "shamash" until all eight candles are lit together on the final night of the holiday. Other Hanukkah festivities include playing dreidel and eating oil-based foods such as doughnuts and latkes. Since the 1970s, the worldwide Chabad Hasidic movement has initiated public menorah lightings in open public places in many countries.

The name "Hanukkah" derives from the Hebrew verb "", meaning "to dedicate". On Hanukkah, the Maccabean Jews regained control of Jerusalem and rededicated the Temple.
Many homiletical explanations have been given for the name:

In Hebrew, the word Hanukkah is written or (). It is most commonly transliterated to English as "" or "Hanukkah", the latter because the sound represented by "CH" (, similar to the Scottish pronunciation of "loch") is not native to the English language. Furthermore, the letter "ḥet" (), which is the first letter in the Hebrew spelling, is pronounced differently in modern Hebrew (voiceless uvular fricative) from in classical Hebrew (voiceless pharyngeal fricative ), and neither of those sounds is unambiguously representable in English spelling. Moreover, the 'kaf' consonant is geminate in classical (but not modern) Hebrew. Adapting the classical Hebrew pronunciation with the geminate and pharyngeal can lead to the spelling "Hanukkah"; while adapting the modern Hebrew pronunciation with no gemination and uvular leads to the spelling "".

The story of Hanukkah is preserved in the books of the First and Second Maccabees, which describe in detail the re-dedication of the Temple in Jerusalem and the lighting of the menorah. These books are not part of the Tanakh (Hebrew Bible) which came from the Palestinian canon; however, they were part of the Alexandrian canon which is also called the Septuagint (sometimes abbreviated LXX). Both books are included in the Old Testament used by the Catholic and Orthodox Churches, since those churches consider the books deuterocanonical. They are not included in the Old Testament books in most Protestant Bibles since most Protestants consider the books apocryphal. Multiple references to Hanukkah are also made in the Mishna (Bikkurim 1:6, Rosh HaShanah 1:3, Taanit 2:10, Megillah 3:4 and 3:6, Moed Katan 3:9, and Bava Kama 6:6), though specific laws are not described. The miracle of the one-day supply of oil miraculously lasting eight days is first described in the Talmud, committed to writing about 600 years after the events described in the books of Maccabees.

Rav Nissim Gaon postulates in his "Hakdamah Le'mafteach Hatalmud" that information on the holiday was so commonplace that the Mishna felt no need to explain it. A modern-day scholar Reuvein Margolies suggests that as the Mishnah was redacted after the Bar Kochba revolt, its editors were reluctant to include explicit discussion of a holiday celebrating another relatively recent revolt against a foreign ruler, for fear of antagonizing the Romans.

The Gemara (Talmud), in tractate "Shabbat," page 21b, focuses on Shabbat candles and moves to Hanukkah candles and says that after the forces of Antiochus IV had been driven from the Temple, the Maccabees discovered that almost all of the ritual olive oil had been profaned. They found only a single container that was still sealed by the High Priest, with enough oil to keep the menorah in the Temple lit for a single day. They used this, yet it burned for eight days (the time it took to have new oil pressed and made ready).

The Talmud presents three options:

Except in times of danger, the lights were to be placed outside one's door, on the opposite side of the mezuza, or in the window closest to the street. Rashi, in a note to "Shabbat 21b," says their purpose is to publicize the miracle. The blessings for Hanukkah lights are discussed in tractate "Succah," p. 46a.

The Jewish historian Titus Flavius Josephus narrates in his book, Jewish Antiquities XII, how the victorious Judas Maccabeus ordered lavish yearly eight-day festivities after rededicating the Temple in Jerusalem that had been profaned by Antiochus IV Epiphanes. Josephus does not say the festival was called Hanukkah but rather the "Festival of Lights":

The story of Hanukkah is alluded to in the book of 1 Maccabees and 2 Maccabees. The eight-day rededication of the temple is described in , though the name of the festival and the miracle of the lights do not appear here. A story similar in character, and obviously older in date, is the one alluded to in according to which the relighting of the altar fire by Nehemiah was due to a miracle which occurred on the 25th of Kislev, and which appears to be given as the reason for the selection of the same date for the rededication of the altar by Judah Maccabee. The above account in 1 Maccabees 4, as well as portrays the feast as a delayed observation of the eight-day Feast of Booths (Sukkot)"; similarly explains the length of the feast as "in the manner of the Feast of Booths".

Another source is the Megillat Antiochus. This work (also known as "Megillat Benei Ḥashmonai", "Megillat Hanukkah", or "Megillat Yevanit") is extant in both the Aramaic and Hebrew languages; the Hebrew version is a literal translation from the Aramaic original. Recent scholarship dates it to somewhere between the 2nd and 5th Centuries, probably in the 2nd century, with the Hebrew dating to the 7th century. It was published for the first time in Mantua in 1557. Saadia Gaon, who translated it into Arabic in the 9th century, ascribed it to the elders of the School of Shammai and the School of Hillel. The Hebrew text with an English translation can be found in the Siddur of Philip Birnbaum.

The "Scroll of Antiochus" concludes with the following words:

Original language (Aramaic):
In the Christian Greek Scriptures, John 10:22–23 says Jesus walked in Solomon's Porch at the Jerusalem Temple during "the Feast of Dedication and it was winter." The Greek term that is used is "the renewals" (Greek "ta enkainia" τὰ ἐγκαίνια). The Hebrew word for "dedication" is Hanukkah. The Aramaic New Testament uses the Aramaic word Khawdata (a close synonym), which literally means "renewal" or "to make new." Josephus refers to the festival as "lights."

Judea was part of the Ptolemaic Kingdom of Egypt until 200 BCE when King Antiochus III the Great of Syria defeated King Ptolemy V Epiphanes of Egypt at the Battle of Panium. Judea then became part of the Seleucid Empire of Syria. King Antiochus III the Great wanting to conciliate his new Jewish subjects guaranteed their right to "live according to their ancestral customs" and to continue to practice their religion in the Temple of Jerusalem. However, in 175 BCE, Antiochus IV Epiphanes, the son of Antiochus III, invaded Judea, at the request of the sons of Tobias. The Tobiads, who led the Hellenizing Jewish faction in Jerusalem, were expelled to Syria around 170 BCE when the high priest Onias and his pro-Egyptian faction wrested control from them. The exiled Tobiads lobbied Antiochus IV Epiphanes to recapture Jerusalem. As Flavius Josephus relates:

When the Second Temple in Jerusalem was looted and services stopped, Judaism was outlawed. In 167 BCE, Antiochus ordered an altar to Zeus erected in the Temple. He banned brit milah (circumcision) and ordered pigs to be sacrificed at the altar of the temple.

Antiochus's actions provoked a large-scale revolt. Mattathias (Mattityahu), a Jewish priest, and his five sons Jochanan, Simeon, Eleazar, Jonathan, and Judah led a rebellion against Antiochus. It started with Mattathias killing first, a Jew who wanted to comply with Antiochus's order to sacrifice to Zeus, and then a Greek official who was to enforce the government's behest (1 Mac. 2, 24–25). Judah became known as Yehuda HaMakabi ("Judah the Hammer"). By 166 BCE Mattathias had died, and Judah took his place as leader. By 165 BCE the Jewish revolt against the Seleucid monarchy was successful. The Temple was liberated and rededicated. The festival of Hanukkah was instituted to celebrate this event. Judah ordered the Temple to be cleansed, a new altar to be built in place of the polluted one and new holy vessels to be made. According to the Talmud, unadulterated and undefiled pure olive oil with the seal of the kohen gadol (high priest) was needed for the menorah in the Temple, which was required to burn throughout the night every night. The story goes that one flask was found with only enough oil to burn for one day, yet it burned for eight days, the time needed to prepare a fresh supply of kosher oil for the menorah. An eight-day festival was declared by the Jewish sages to commemorate this miracle.

The version of the story in 1 Maccabees states that an eight-day celebration of songs and sacrifices was proclaimed upon re-dedication of the altar, and makes no specific mention of the miracle of the oil.

Some modern scholars argue that the king was intervening in an internal civil war between the Maccabean Jews and the Hellenized Jews in Jerusalem.

These competed violently over who would be the High Priest, with traditionalists with Hebrew/Aramaic names like Onias contesting with Hellenizing High Priests with Greek names like Jason and Menelaus. In particular, Jason's Hellenistic reforms would prove to be a decisive factor leading to eventual conflict within the ranks of Judaism. Other authors point to possible socioeconomic reasons in addition to the religious reasons behind the civil war.

What began in many respects as a civil war escalated when the Hellenistic kingdom of Syria sided with the Hellenizing Jews in their conflict with the traditionalists. As the conflict escalated, Antiochus took the side of the Hellenizers by prohibiting the religious practices the traditionalists had rallied around. This may explain why the king, in a total departure from Seleucid practice in all other places and times, banned a traditional religion.

The miracle of the oil is widely regarded as a legend and its authenticity has been questioned since the Middle Ages. However, given the famous question Rabbi Yosef Karo posed concerning why Hanukkah is celebrated for eight days when the miracle was only for seven days (since there was enough oil for one day), it was clear that he believed it was a historical event. This belief has been adopted by most of Orthodox Judaism, in as much as Rabbi Karo's "Shulchan Aruch" is a main Code of Jewish Law.


Selected battles between the Maccabees and the Seleucid Syrian-Greeks:


Hanukkah is celebrated with a series of rituals that are performed every day throughout the 8-day holiday, some are family-based and others communal. There are special additions to the daily prayer service, and a section is added to the blessing after meals.

Hanukkah is not a "Sabbath-like" holiday, and there is no obligation to refrain from activities that are forbidden on the Sabbath, as specified in the "Shulkhan Arukh". Adherents go to work as usual but may leave early in order to be home to kindle the lights at nightfall. There is no religious reason for schools to be closed, although in Israel schools close from the second day for the whole week of Hanukkah. Many families exchange gifts each night, such as books or games, and "Hanukkah Gelt" is often given to children. Fried foods (such as latkes (potato pancakes), jelly doughnuts (sufganiyot), and Sephardic bimuelos) are eaten to commemorate the importance of oil during the celebration of Hanukkah. Some also have a custom of eating dairy products to remember Judith and how she overcame Holofernes by feeding him cheese, which made him thirsty, and giving him wine to drink. When Holofernes became very drunk, Judith cut off his head.

Each night throughout the 8 day holiday, a candle or oil-based light is lit. As a universally practiced "beautification" (hiddur mitzvah) of the mitzvah, the number of lights lit is increased by one each night. An extra light called a "shamash", meaning "attendant" or "sexton," is also lit each night, and is given a distinct location, usually higher, lower, or to the side of the others.

Among Ashkenazim the tendency is for every male member of the household (and in many families, girls as well) to light a full set of lights each night, while among Sephardim the prevalent custom is to have one set of lights for the entire household.

The purpose of the "shamash" is to adhere to the prohibition, specified in the Talmud (Tractate Shabbat 21b–23a), against using the Hanukkah lights for anything other than publicizing and meditating on the Hanukkah miracle. This differs from Sabbath candles which are meant to be used for illumination and lighting. Hence, if one were to need extra illumination on Hanukkah, the "shamash" candle would be available, and one would avoid using the prohibited lights. Some, especially Ashkenazim, light the "shamash" candle first and then use it to light the others. So altogether, including the "shamash", two lights are lit on the first night, three on the second and so on, ending with nine on the last night, for a total of 44 (36, excluding the "shamash"). It is Sephardic custom not to light the shamash first and use it to light the rest. Instead, the shamash candle is the last to be lit, and a different candle or a match is used to light all the candles. Some Hasidic Jews follow this Sephardic custom as well.

The lights can be candles or oil lamps. Electric lights are sometimes used and are acceptable in places where open flame is not permitted, such as a hospital room, or for the very elderly and infirm; however, those who permit reciting a blessing over electric lamps only allow it if it is incandescent and battery operated (an incandescent flashlight would be acceptable for this purpose), while a blessing may not be recited over a plug-in menorah or lamp. Most Jewish homes have a special candelabrum referred to as either a "Chanukiah" (the modern Israeli term) or a "menorah" (the traditional name, simply Hebrew for 'lamp'). Many families use an oil lamp (traditionally filled with olive oil) for Hanukkah. Like the candle Chanukiah, it has eight wicks to light plus the additional "shamash" light.

In the United States, Hanukkah became a more visible festival in the public sphere from the 1970s when Rabbi Menachem M. Schneerson called for public awareness and observance of the festival and encouraged the lighting of public menorahs. Diane Ashton attributed the increased visibility and reinvention of Hanukkah by some of the American Jewish community as a way to adapt to American life, re-inventing the festival in "the language of individualism and personal conscience derived from both Protestantism and the Enlightenment".

The reason for the Hanukkah lights is not for the "lighting of the house within", but rather for the "illumination of the house without," so that passersby should see it and be reminded of the holiday's miracle (i.e. that the sole cruse of pure oil found which held enough oil to burn for one night actually burned for eight nights). Accordingly, lamps are set up at a prominent window or near the door leading to the street. It is customary amongst some Ashkenazi Jews to have a separate menorah for each family member (customs vary), whereas most Sephardi Jews light one for the whole household. Only when there was danger of antisemitic persecution were lamps supposed to be hidden from public view, as was the case in Persia under the rule of the Zoroastrians, or in parts of Europe before and during World War II. However, most Hasidic groups light lamps near an inside doorway, not necessarily in public view. According to this tradition, the lamps are placed on the opposite side from the "mezuzah", so that when one passes through the door s/he is surrounded by the holiness of "mitzvot" (the commandments).

Generally, women are exempt in Jewish law from time-bound positive commandments, although the Talmud requires that women engage in the mitzvah of lighting Hanukkah candles “for they too were involved in the miracle.”

Hanukkah lights should usually burn for at least half an hour after it gets dark. The custom of many is to light at sundown, although most Hasidim light later. Many Hasidic Rebbes light much later to fulfill the obligation of publicizing the miracle by the presence of their Hasidim when they kindle the lights.

Inexpensive small wax candles sold for Hanukkah burn for approximately half an hour so should be lit no earlier than nightfall. Friday night presents a problem, however. Since candles may not be lit on Shabbat itself, the candles must be lit before sunset. However, they must remain lit through the lighting of the Shabbat candles. Therefore, the Hanukkah menorah is lit first with larger candles than usual, followed by the Shabbat candles. At the end of the Shabbat, there are those who light the Hanukkah lights before Havdalah and those who make Havdalah before the lighting Hanukkah lights.

If for whatever reason one didn't light at sunset or nightfall, the lights should be kindled later, as long as there are people in the streets. Later than that, the lights should still be kindled, but the blessings should be recited only if there is at least somebody else awake in the house and present at the lighting of the Hannukah lights.

Typically two blessings ("brachot"; singular: "brachah") are recited during this eight-day festival when lighting the candles. On the first night, the shehecheyanu blessing is added, making a total of three blessings.

The blessings are said before or after the candles are lit depending on tradition. On the first night of Hanukkah one light (candle or oil) is lit on the right side of the menorah, on the following night a second light is placed to the left of the first but it is lit first, and so on, proceeding from placing candles right to left but lighting them from left to right over the eight nights.

Transliteration: "Barukh ata Adonai Eloheinu, melekh ha'olam, asher kid'shanu b'mitzvotav v'tzivanu l'hadlik ner shel Hanukkah."

Translation: "Blessed are You, our God, King of the universe, Who has sanctified us with His commandments and commanded us to kindle the Hanukkah light[s]."

Transliteration: "Barukh ata Adonai Eloheinu, melekh ha'olam, she'asa nisim la'avoteinu ba'yamim ha'heim ba'z'man ha'ze."

Translation: "Blessed are You, LORD our God, King of the universe, Who performed miracles for our ancestors in those days at this time..."

After the lights are kindled the hymn "Hanerot Halalu" is recited. There are several different versions; the version presented here is recited in many Ashkenazic communities:

In the Ashkenazi tradition, each night after the lighting of the candles, the hymn Ma'oz Tzur is sung. The song contains six stanzas. The first and last deal with general themes of divine salvation, and the middle four deal with events of persecution in Jewish history, and praises God for survival despite these tragedies (the exodus from Egypt, the Babylonian captivity, the miracle of the holiday of Purim, the Hasmonean victory), and a longing for the days when Judea will finally triumph over Rome.

The song was composed in the thirteenth century by a poet only known through the acrostic found in the first letters of the original five stanzas of the song: Mordechai. The familiar tune is most probably a derivation of a German Protestant church hymn or a popular folk song.

After lighting the candles and Ma'oz Tzur, singing other Hanukkah songs is customary in many Jewish homes. Some Hasidic and Sephardi Jews recite Psalms, such as , , and . In North America and in Israel it is common to exchange presents or give children presents at this time. In addition, many families encourage their children to give tzedakah (charity) in lieu of presents for themselves.

An addition is made to the ""hoda'ah"" (thanksgiving) benediction in the Amidah (thrice-daily prayers), called "Al ha-Nissim" ("On/about the Miracles"). This addition refers to the victory achieved over the Syrians by the Hasmonean Mattathias and his sons.

The same prayer is added to the grace after meals. In addition, the "Hallel" (praise) ( – ) are sung during each morning service and the "Tachanun" penitential prayers are omitted.

The Torah is read every day in the shacharit morning services in synagogue, on the first day beginning from (according to some customs, ), and the last day ending with . Since Hanukkah lasts eight days it includes at least one, and sometimes two, Jewish Sabbaths (Saturdays). The weekly Torah portion for the first Sabbath is almost always "Miketz", telling of Joseph's dream and his enslavement in Egypt. The "Haftarah" reading for the first Sabbath Hanukkah is – . When there is a second Sabbath on Hanukkah, the "Haftarah" reading is from – .

The Hanukkah "menorah" is also kindled daily in the synagogue, at night with the blessings and in the morning without the blessings.

The menorah is not lit during Shabbat, but rather prior to the beginning of Shabbat as described above and not at all during the day.
During the Middle Ages "Megillat Antiochus" was read in the Italian synagogues on Hanukkah just as the Book of Esther is read on Purim. It still forms part of the liturgy of the Yemenite Jews.

The last day of Hanukkah is known by some as "Zot Hanukkah" and by others as "Chanukat HaMizbeach", from the verse read on this day in the synagogue , "Zot Hanukkat Hamizbe'ach": "This was the dedication of the altar". According to the teachings of Kabbalah and Hasidism, this day is the final "seal" of the High Holiday season of Yom Kippur and is considered a time to repent out of love for God. In this spirit, many Hasidic Jews wish each other "Gmar chatimah tovah" ("may you be sealed totally for good"), a traditional greeting for the Yom Kippur season. It is taught in Hasidic and Kabbalistic literature that this day is particularly auspicious for the fulfillment of prayers.

It is customary for women not to work for at least the first half-hour of the candles' burning, and some have the custom not to work for the entire time of burning. It is also forbidden to fast or to eulogize during Hanukkah.

A large number of songs have been written on Hanukkah themes, perhaps more so than for any other Jewish holiday. Some of the best known are ""Ma'oz Tzur"" (Rock of Ages), ""Latke'le Latke'le"" (Yiddish song about cooking Latkes), ""Hanukkiah Li Yesh"" ("I Have a Hanukkah Menorah"), ""Ocho Kandelikas"" ("Eight Little Candles"), ""Kad Katan"" ("A Small Jug"), ""S'vivon Sov Sov Sov"" ("Dreidel, Spin and Spin"), ""Haneirot Halolu"" ("These Candles which we light"), ""Mi Yimalel"" ("Who can Retell") and ""Ner Li, Ner Li"" ("I have a Candle"). Among the most well known songs in English-speaking countries are "Dreidel, Dreidel, Dreidel" and "Oh Chanukah".

Among the Rebbes of the Nadvorna Hasidic dynasty, it is customary for the Rebbes to play violin after the menorah is lit.

Penina Moise's Hannukah Hymn published in the 1842 "Hymns Written for the Use of Hebrew Congregations" was instrumental in the beginning of Americanization of Hanukkah.

There is a custom of eating foods fried or baked in oil (preferably olive oil) to commemorate the miracle of a small flask of oil keeping the Second Temple's Menorah alight for eight days. Traditional foods include potato pancakes, known as "latkes" in Yiddish, especially among Ashkenazi families. Sephardi, Polish, and Israeli families eat jam-filled doughnuts ( "pontshkes"), bimuelos (fritters) and sufganiyot which are deep-fried in oil. Hungarian Jews eat cheese pancakes known as "cheese latkes".
Latkes are not popular in Israel, having been largely replaced by sufganiyot due to local economic factors, convenience and the influence of trade unions. Bakeries in Israel have popularized many new types of fillings for "sufganiyot" besides the traditional strawberry jelly filling, including chocolate cream, vanilla cream, caramel, cappuccino and others. In recent years, downsized, "mini" sufganiyot containing half the calories of the regular, 400-to-600-calorie version, have become popular.

Rabbinic literature also records a tradition of eating cheese and other dairy products during Hanukkah. This custom, as mentioned above, commemorates the heroism of Judith during the Babylonian captivity of the Jews and reminds us that women also played an important role in the events of Hanukkah. The deuterocanonical book of Judith (Yehudit or Yehudis in Hebrew), which is not part of the Tanakh, records that Holofernes, an Assyrian general, had surrounded the village of Bethulia as part of his campaign to conquer Judea. After intense fighting, the water supply of the Jews was cut off and the situation became desperate. Judith, a pious widow, told the city leaders that she had a plan to save the city. Judith went to the Assyrian camps and pretended to surrender. She met Holofernes, who was smitten by her beauty. She went back to his tent with him, where she plied him with cheese and wine. When he fell into a drunken sleep, Judith beheaded him and escaped from the camp, taking the severed head with her (the beheading of Holofernes by Judith has historically been a popular theme in art). When Holofernes' soldiers found his corpse, they were overcome with fear; the Jews, on the other hand, were emboldened and launched a successful counterattack. The town was saved, and the Assyrians defeated.

Roast goose has historically been a traditional Hanukkah food among Eastern European and American Jews, although the custom has declined in recent decades.

After lighting the candles, it is customary to play (or spin) the dreidel. The dreidel, or "sevivon" in Hebrew, is a four-sided spinning top that children play with during Hanukkah. Each side is imprinted with a Hebrew letter which is an abbreviation for the Hebrew words (, "A great miracle happened there"), referring to the miracle of the oil that took place in the Beit Hamikdash. On dreidels sold in Israel, the fourth side is inscribed with the letter "(Pe)", rendering the acronym (, "A great miracle happened here"), referring to the fact that the miracle occurred in the land of Israel, although this is a relatively recent innovation. Stores in Haredi neighborhoods sell the traditional "Shin" dreidels as well, because they understand "there" to refer to the Temple and not the entire Land of Israel, and because the Hasidic Masters ascribe significance to the traditional letters.

Chanukkah gelt (Yiddish for "Chanukkah money") known in Israel by the Hebrew translation "dmei Hanukkah", is often distributed to children during the festival of Hanukkah. The giving of Hanukkah gelt also adds to the holiday excitement. The amount is usually in small coins, although grandparents or relatives may give larger sums. The tradition of giving Chanukah "gelt" dates back to a long-standing East European custom of children presenting their teachers with a small sum of money at this time of year as a token of gratitude. One minhag favors the fifth night of Hanukkah for giving Hanukkah gelt. Unlike the other nights of Hanukkah, the fifth does not ever fall on the Shabbat, hence never conflicting with the Halachic injunction against handling money on the Shabbat.

The United States has a history of recognizing and celebrating Hanukkah in a number of ways. The earliest Hanukkah link with the White House occurred in 1951 when Israeli Prime Minister David Ben-Gurion presented United States President Harry Truman with a Hanukkah Menorah. In 1979 president Jimmy Carter took part in the first public Hanukkah candle-lighting ceremony of the National Menorah held across the White House lawn. In 1989, President George H.W. Bush displayed a menorah in the White House. In 1993, President Bill Clinton invited a group of schoolchildren to the Oval Office for a small ceremony.

The United States Postal Service has released several Hanukkah-themed postage stamps. In 1996 the United States Postal Service (USPS) issued a 32 cent Hanukkah stamp as a joint issue with Israel. In 2004 after 8 years of reissuing the menorah design, the USPS issued a dreidel design for the Hanukkah stamp. The dreidel design was used through 2008. In 2009 a Hanukkah stamp was issued with a design featured a photograph of a menorah with nine lit candles.

In 2001, President George W. Bush held an official Hanukkah reception in the White House in conjunction with the candle-lighting ceremony, and since then this ceremony has become an annual tradition attended by Jewish leaders from around the country. In 2008, George Bush linked the occasion to the 1951 gift by using that menorah for the ceremony, with a grandson of Ben-Gurion and a grandson of Truman lighting the candles.

In December 2014, two Hanukkah celebrations were held at the White House. The White House commissioned a menorah made by students at the Max Rayne school in Israel and invited two of its students to join U.S. President Barack Obama and First Lady Michelle Obama as they welcomed over 500 guests to the celebration. The students' school in Israel had been subjected to arson by extremists. President Obama said these "students teach us an important lesson for this time in our history. The light of hope must outlast the fires of hate. That’s what the Hanukkah story teaches us. It’s what our young people can teach us— that one act of faith can make a miracle, that love is stronger than hate, that peace can triumph over conflict.” Rabbi Angela Warnick Buchdahl, in leading prayers at the ceremony commented on the how special the scene was, asking the President if he believed America's founding fathers could possibly have pictured that a female Asian-American rabbi would one day be at the White House leading Jewish prayers in front of the African-American president.

The dates of Hanukkah are determined by the Hebrew calendar. Hanukkah begins at the 25th day of Kislev and concludes on the 2nd or 3rd day of Tevet (Kislev can have 29 or 30 days). The Jewish day begins at sunset. Hanukkah begins at sunset of the date listed.
In 2013, on 28 November, the American holiday of Thanksgiving fell during Hanukkah for only the third time since Thanksgiving was declared a national holiday by President Abraham Lincoln. The last time was 1899; and due to the Gregorian and Jewish calendars being slightly out of sync with each other, it will not happen again in the foreseeable future. This convergence prompted the creation of the portmanteau neologism Thanksgivukkah.

Major Jewish holidays are those when all forms of work are forbidden, and that feature traditional holiday meals, kiddush, holiday candle-lighting, etc. Only biblical holidays fit these criteria, and Chanukah was instituted some two centuries after the Hebrew Bible was completed. Nevertheless, though Chanukah is of rabbinic origin, it is traditionally celebrated in a major and very public fashion. The requirement to position the menorah, or Chanukiah, at the door or window, symbolizes the desire to give the Chanukah miracle a high-profile.

Some Jewish historians suggest a different explanation for the rabbinic reluctance to laud the militarism. First, the rabbis wrote after Hasmonean leaders had led Judea into Rome’s grip and so may not have wanted to offer the family much praise. Second, they clearly wanted to promote a sense of dependence on God, urging Jews to look toward the divine for protection. They likely feared inciting Jews to another revolt that might end in disaster, like the CE 135 experience.

With the advent of Zionism and the state of Israel, however, these themes were reconsidered. In modern Israel, the national and military aspects of Hanukkah became, once again, more dominant.

In North America especially, Hanukkah gained increased importance with many Jewish families in the latter part of the 20th century, including among large numbers of secular Jews, who wanted a Jewish alternative to the Christmas celebrations that often overlap with Hanukkah. Though it was traditional among Ashkenazi Jews to give "gelt" or money to children during Hanukkah, in many families this has been supplemented with other gifts so that Jewish children can enjoy gifts just as their Christmas-celebrating peers do.

While Hanukkah is a relatively minor Jewish holiday, as indicated by the lack of religious restrictions on work other than a few minutes after lighting the candles, in North America, Hanukkah in the 21st century has taken a place equal to Passover as a symbol of Jewish identity. Both the Israeli and North American versions of Hanukkah emphasize resistance, focusing on some combination of national liberation and religious freedom as the defining meaning of the holiday.

Some Jews in North America and Israel have taken up environmental concerns in relation to Hanukkah's "miracle of the oil", emphasizing reflection on energy conservation and energy independence. An example of this is the Coalition on the Environment and Jewish Life's renewable energy campaign.

An entire room of Paris's Museum of Jewish Art and History is dedicated to Hanukkah, through an exceptional collection of Hanukkiyot, in a variety of shapes and designs, origins and periods. This panorama stands as a metaphor for the great diversity of Jewish customs throughout the world.




</doc>
<doc id="7390" url="https://en.wikipedia.org/wiki?curid=7390" title="Christian views on marriage">
Christian views on marriage

Marriage is the legal union of a couple as spouses—an intimate and complementing union, generally between a man and a woman, in which the two become one physically in the whole of life.

The basic elements of a marriage are: (1) the parties' legal ability to marry each other, (2) mutual consent of the parties, and (3) a marriage contract as required by law.

Christian marriage is a state instituted and ordained by God for the lifelong relationship between one man as husband and one woman as wife. The Apostle Paul gave a similar directive when he wrote, "Let marriage be held in honour among all". Conservative Christians consider marriage as the most intimate of human relationships, a gift from God, and a sacred institution. Protestants consider it to be sacred, holy, and central to the community of faith. Catholics and Eastern Orthodox Christians consider marriage a Sacrament.

Though it is presumed that Jesus never married, he taught the importance and sacredness of lifelong marriage. He quoted from both Genesis 1 and 2 that God had created humanity as male and female, and that in marriage the two become one flesh'. Then he added a divine postscript, saying: "So they are no longer two, but one flesh. Therefore, what God has joined together, let no one separate".

While marriage is honored throughout the Bible and affirmed among Christians, there is no suggestion that it is necessary for everyone. Single people who either have chosen to remain unmarried or who have lost their spouse for some reason are neither incomplete in Christ nor personal failures.

Christians seek to uphold the seriousness of wedding vows. Yet, they respond with compassion to deep hurts by recognizing that divorce, though less than the ideal, is sometimes necessary to relieve one partner of intolerable hardship, unfaithfulness or desertion. While the voice of God had said, "I hate divorce", some authorities believe the divorce rate in the church is nearly comparable to that of the culture at large.

Christians today hold three competing views as to what is the biblically ordained relationship between husband and wife. These views range from Christian egalitarianism that interprets the New Testament as teaching complete equality of authority and responsibility between the man and woman in marriage, all the way to one that calls for a “return to complete patriarchy” in which relationships are based on male-dominant power and authority in marriage:

1. Christian Egalitarians believe in an equal partnership of the wife and husband with no prescribed leader. They propose a fully "equal partnership" between men and women in both the family and in the church. Its proponents teach "the fundamental biblical principle of the equality of all human beings before God". According to this principle, there can be no moral or theological justification for permanently granting or denying status, privilege, or prerogative solely on the basis of a person's race, class, or gender.

2. Christian Complementarians prescribe husband-headship—a male-led hierarchy. This view's core beliefs call for a husband’s “loving, humble headship" and the wife’s “intelligent, willing submission" to his headship. Without necessarily using the term "obey", they believe women have "different but complementary roles and responsibilities in marriage..."

3. Biblical patriarchy, though not popular among mainstream Christians, prescribes a strict male-dominant hierarchy. This view makes the husband the “ruler” over "his" wife and “his” household. Their organization's first tenet is that "God reveals Himself as masculine, not feminine. God is the eternal Father and the eternal Son, the Holy Spirit is also addressed as 'He,' and Jesus Christ is a male." They consider the husband-father to be "sovereign" over "his" household—the family leader, provider, and protector. They call for a wife to be "obedient" to her "head" (husband).

Some Christian authorities used to permit polygamy (specifically polygyny) in the past, but this practice, besides being illegal in Western cultures, is now considered to be out of the Christian mainstream and continues to be practiced only by fringe fundamentalist sects.

Much of the dispute hinges on how one interprets the New Testament Household Code "(Haustafel)" which has as its main focus hierarchical relationships between three pairs of social classes that were controlled by Roman law: husbands/wives, parents/children, and masters/slaves. The Code, with variations, occurs in four epistles (letters) by the Apostle Paul and in 1  Peter. The Roman law of "Manus" gave the husband nearly absolute autocratic power over his wife, including life and death. The law of "Patria Potestas" (Latin for "Rule of the Fathers") gave a husband equally severe power over his children and slaves. Theologian Frank Stagg finds the basic tenets of the Code in Aristotle's discussion of the household in Book 1 of "Politics" and in Philo's "Hypothetica 7.14". Serious study of the New Testament Household Code "(Haustafel)" began with Martin Dilbelius in 1913, with a wide range of studies since then. In a Tübingen dissertation, by James E. Crouch concludes that the early Christians found in Hellenistic Judaism a code which they adapted and Christianized.

The Staggs believe the several occurrences of the New Testament Household Code in the Bible were intended to meet the needs for "order" within the churches and in the society of the day. They maintain that the New Testament Household Codes are attempts by Paul and Peter to Christianize the harsh Codes for Roman citizens who had become followers of Christ. The Staggs write that there is some suggestion in scripture that because Paul had taught that they had newly found freedom "in Christ", wives, children, and slaves were taking improper advantage of the "Haustafel" both in the home and the church. 
"The form of the code stressing reciprocal social duties is traced to Judaism's own Oriental background, with its strong moral/ethical demand but also with a low view of woman... At bottom is probably to be seen the perennial tension between freedom and order... What mattered to (Paul) was 'a new creation' and 'in Christ' there is 'not any Jew not Greek, not any slave nor free, not any male and female .” 

Such codes existed in Greek tradition. Two of these Christianized codes are found in (which contains the phrases "husband is the head of the wife" and "wives, submit to your husband") and in 


</doc>
<doc id="7392" url="https://en.wikipedia.org/wiki?curid=7392" title="Class (computer programming)">
Class (computer programming)

In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated.

When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class.

In some languages, classes are only a compile-time feature (new classes cannot be declared at runtime), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type Class or similar). In these languages, a class that creates classes is called a metaclass.

In casual use, people often refer to the "class" of an object, but narrowly speaking objects have "type": the interface, namely the types of member variables, the signatures of member functions (methods), and properties these satisfy. At the same time, a class has an implementation (specifically the implementation of the methods), and can create objects of a given type, with a given implementation. In the terms of type theory, a class is an implementationa "concrete" data structure and collection of subroutineswhile a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system); for example, the type Stack might be implemented with two classes SmallStack (fast for small stacks, but scales poorly) and ScalableStack (scales well but high overhead for small stacks). Similarly, a given class may have several different constructors.

Types generally represent nouns, such as a person, place or thing, or something nominalized, and a class represents an implementation of these. For example, a Banana type might represent the properties and functionality of bananas in general, while the ABCBanana and XYZBanana classes would represent ways of producing bananas (say, banana suppliers or data structures and functions to represent and draw bananas in a video game). The ABCBanana class could then produce particular bananas: instances of the ABCBanana class would be objects of type Banana. Often only a single implementation of a type is given, in which case the class name is often identical with the type name.

Classes are composed from structural and behavioral constituents. Programming languages that include classes as a programming construct offer support, for various class-related features, and the syntax required to use these features varies greatly from one programming language to another.

A class contains data field descriptions (or "properties", "fields", "data members", or "attributes"). These are usually field types and names that will be associated with state variables at program run time; these state variables either belong to the class or specific instances of the class. In most languages, the structure defined by the class determines the layout of the memory used by its instances. Other implementations are possible: for example, objects in Python use associative key-value containers.

Some programming languages support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.

The behavior of class or its instances is defined using methods. Methods are subroutines with the ability to operate on objects or classes. These operations may alter the state of an object or simply provide ways of accessing it. Many kinds of methods exist, but support for them varies across languages. Some types of methods are created and called by programmer code, while other special methods—such as constructors, destructors, and conversion operators—are created and called by compiler-generated code. A language may also allow the programmer to define and call these special methods.

Every class "implements" (or "realizes") an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented. There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing implementation.

Languages that support class inheritance also allow classes to inherit interfaces from the classes that they are derived from. In languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external code and thus are not part of the interface.

Object-oriented programming methodology dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client has access to the object. 

The buttons on the front of your television set are the interface between you and the electrical wiring on the other side of its plastic casing. You press the "power" button to toggle the television on and off. In this example, your particular television is the instance, each method is represented by a button, and all the buttons together comprise the interface. (Other television sets that are the same model as yours would have the same interface.) In its most common form, an interface is a specification of a group of related methods without any associated implementation of the methods.

A television set also has a myriad of "attributes", such as size and whether it supports color, which together comprise its structure. A class represents the full description of a television, including its attributes (structure) and buttons (interface).

Getting the total number of televisions manufactured could be a "static method" of the television class. This method is clearly associated with the class, yet is outside the domain of each individual instance of the class. Another example would be a static method that finds a particular instance out of the set of all television objects.

The following is a common set of access specifiers:


Although many object-oriented languages support the above access specifiers, their semantics may differ.

Object-oriented design uses the access specifiers in conjunction with careful design of public method implementations to enforce class invariants—constraints on the state of the objects. A common usage of access specifiers is to separate the internal data of a class from its interface: the internal structure is made private, while public accessor methods can be used to inspect or alter such private data.

Access specifiers do not necessarily control "visibility", in that even private members may be visible to client external code. In some languages, an inaccessible but visible member may be referred to at run-time (for example, by a pointer returned from a member function), but an attempt to use it by referring to the name of the member from client code will be prevented by the type checker.

The various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile-time or run-time. For example, the Java language does not allow client code that accesses the private data of a class to compile.

Some languages feature other accessibility schemes:

In addition to the design of standalone classes, programming languages may support more advanced class design based upon relationships between classes. The inter-class relationship design capabilities commonly provided are "compositional" and "hierarchical".

Classes can be composed of other classes, thereby establishing a compositional relationship between the enclosing class and its embedded classes. Compositional relationship between classes is also commonly known as a "has-a" relationship. For example, a class "Car" could be composed of and contain a class "Engine". Therefore, a Car "has an" Engine. One aspect of composition is containment, which is the enclosure of component instances by the instance that has them. If an enclosing object contains component instances by value, the components and their enclosing object have a similar lifetime. If the components are contained by reference, they may not have a similar lifetime. For example, in Objective-C 2.0:

This Car class "has" an instance of NSString (a string object), Engine, and NSArray (an array object).

Classes can be "derived" from one or more existing classes, thereby establishing a hierarchical relationship between the derived-from classes ("base classes", "parent classes" or ') and the derived class ("child class" or "subclass") . The relationship of the derived class to the derived-from classes is commonly known as an is-a relationship. For example, a class 'Button' could be derived from a class 'Control'. Therefore, a Button is a"' Control. Structural and behavioral members of the parent classes are "inherited" by the child class. Derived classes can define additional structural members (data fields) and behavioral members (methods) in addition to those that they "inherit" and are therefore "specializations" of their superclasses. Also, derived classes can override inherited methods if the language allows.

Not all languages support multiple inheritance. For example, Java allows a class to implement multiple interfaces, but only inherit from one class. If multiple inheritance is allowed, the hierarchy is a directed acyclic graph (or DAG for short), otherwise it is a tree. The hierarchy has classes as nodes and inheritance relationships as links. Classes in the same level are more likely to be associated than classes in different levels. The levels of this hierarchy are called layers or levels of abstraction.

Example (Simplified Objective-C 2.0 code, from iPhone SDK):

In this example, a UITableView is a UIScrollView is a UIView is a UIResponder is an NSObject.

Conceptually, a superclass is a superset of its subclasses. For example, a common class hierarchy would involve GraphicObject as a superclass of Rectangle and Elipse, while Square would be a subclass of Rectangle. These are all subset relations in set theory as well, i.e., all squares are rectangles but not all rectangles are squares.

A common conceptual error is to mistake a "part of" relation with a subclass. For example, a car and truck are both kinds of vehicles and it would be appropriate to model them as subclasses of a vehicle class. However, it would be an error to model the component parts of the car as subclass relations. For example, a car is composed of an engine and body, but it would not be appropriate to model engine or body as a subclass of car.

In object-oriented modeling these kinds of relations are typically modeled as object properties. In this example the Car class would have a property called parts. parts would be typed to hold a collection of objects such as instances of Body, Engine, Tires...
Object modeling languages such as UML include capabilities to model various aspects of part of and other kinds of relations. Data such as the cardinality of the objects, constraints on input and output values, etc. This information can be utilized by developer tools to generate additional code beside the basic data definitions for the objects. Things such as error checking on get and set methods.

One important question when modeling and implementing a system of object classes is whether a class can have one or more superclasses. In the real world with actual sets it would be rare to find sets that didn't intersect with more than one other set. However, while some systems such as Flavors and CLOS provide a capability for more than one parent to do so at run time introduces complexity that many in the object-oriented community consider antithetical to the goals of using object classes in the first place. Understanding which class will be responsible for handling a message can get complex when dealing with more than one superclass. If used carelessly this feature can introduce some of the same system complexity and ambiguity classes were designed to avoid.

Most modern object-oriented languages such as Smalltalk and Java require single inheritance at run time. For these languages, multiple inheritance may be useful for modeling but not for an implementation.

However, semantic web application objects do have multiple superclasses. The volatility of the Internet requires this level of flexibility and the technology standards such as the Web Ontology Language (OWL) are designed to support it.

A similar issue is whether or not the class hierarchy can be modified at run time. Languages such as Flavors, CLOS, and Smalltalk all support this feature as part of their meta-object protocols. Since classes are themselves first-class objects, it is possible to have them dynamically alter their structure by sending them the appropriate messages. Other languages that focus more on strong typing such as Java and C++ do not allow the class hierarchy to be modified at run time. Semantic web objects have the capability for run time changes to classes. The rational is similar to the justification for allowing multiple superclasses, that the Internet is so dynamic and flexible that dynamic changes to the hierarchy are required to manage this volatility.

Although class-based languages are commonly assumed to support inheritance, inheritance is not an intrinsic aspect of the concept of classes. Some languages, often referred to as "object-based languages", support classes yet do not support inheritance. Examples of object-based languages include earlier versions of Visual Basic.

In object-oriented analysis and in UML, an association between two classes represents a collaboration between the classes or their corresponding instances. Associations have direction; for example, a bi-directional association between two classes indicates that both of the classes are aware of their relationship. Associations may be labeled according to their name or purpose.

An association role is given end of an association and describes the role of the corresponding class. For example, a "subscriber" role describes the way instances of the class "Person" participate in a "subscribes-to" association with the class "Magazine". Also, a "Magazine" has the "subscribed magazine" role in the same association. Association role multiplicity describes how many instances correspond to each instance of the other class of the association. Common multiplicities are "0..1", "1..1", "1..*" and "0..*", where the "*" specifies any number of instances.

There are many categories of classes, some of which overlap.

In a language that supports inheritance, an abstract class, or "abstract base class" (ABC), is a class that cannot be instantiated because it is either labeled as abstract or it simply specifies abstract methods (or "virtual methods"). An abstract class may provide implementations of some methods, and may also specify virtual methods via signatures that are to be implemented by direct or indirect descendants of the abstract class. Before a class derived from an abstract class can be instantiated, all abstract methods of its parent classes must be implemented by some class in the derivation chain.

Most object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java, C# and PHP, the keyword "abstract" is used. In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).

A class consisting of only virtual methods is called a Pure Abstract Base Class (or "Pure ABC") in C++ and is also known as an "interface" by users of the language. Other languages, notably Java and C#, support a variant of abstract classes called an interface via a keyword in the language. In these languages, multiple inheritance is not allowed, but a class can implement multiple interfaces. Such a class can only contain abstract publicly accessible methods.
A concrete class is a class that can be instantiated, as opposed to abstract classes, which cannot. 

In some languages, classes can be declared in scopes other than the global scope. There are various types of such classes.

An inner class is a class defined within another class. The relationship between an inner class and its containing class can also be treated as another type of class association. An inner class is typically neither associated with instances of the enclosing class nor instantiated along with its enclosing class. Depending on language, it may or may not be possible to refer to the class from outside the enclosing class. A related concept is "inner types", also known as "inner data type" or "nested type", which is a generalization of the concept of inner classes. C++ is an example of a language that supports both inner classes and inner types (via "typedef" declarations).

Another type is a local class, which is a class defined within a procedure or function. This limits references to the class name to within the scope where the class is declared. Depending on the semantic rules of the language, there may be additional restrictions on local classes compared to non-local ones. One common restriction is to disallow local class methods to access local variables of the enclosing function. For example, in C++, a local class may refer to static variables declared within its enclosing function, but may not access the function's automatic variables.

Metaclasses are classes whose instances are classes. A metaclass describes a common structure of a collection of classes and can implement a design pattern or describe particular kinds of classes. Metaclasses are often used to describe frameworks.

In some languages, such as Python, Ruby or Smalltalk, a class is also an object; thus each class is an instance of a unique metaclass that is built into the language.

The Common Lisp Object System (CLOS) provides metaobject protocols (MOPs) to implement those classes and metaclasses.
Non-subclassable classes allow programmers to design classes and hierarchies of classes where at some level in the hierarchy, further derivation is prohibited. (A stand-alone class may be also designated as non-subclassable, preventing the formation of any hierarchy). Contrast this to "abstract" classes, which imply, encourage, and require derivation in order to be used at all. A non-subclassable class is implicitly "concrete".

A non-subclassable class is created by declaring the class as sealed in C# or as final in Java or PHP.

For example, Java's class is designated as "final".

Non-subclassable classes may allow a compiler (in compiled languages) to perform optimizations that are not available for subclassable classes.

Some languages have special support for mixins, though in any language with multiple inheritance a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes; for example, a class UnicodeConversionMixin might provide a method called unicode_to_ascii when included in classes FileReader and WebPageScraper that do not share a common parent.

In languages supporting the feature, a partial class is a class whose definition may be split into multiple pieces, within a single source-code file or across multiple files. The pieces are merged at compile-time, making compiler output the same as for a non-partial class.

The primary motivation for introduction of partial classes is to facilitate the implementation of code generators, such as visual designers. It is otherwise a challenge or compromise to develop code generators that can manage the generated code when it is interleaved within developer-written code. Using partial classes, a code generator can process a separate file or coarse-grained partial class within a file, and is thus alleviated from intricately interjecting generated code via extensive parsing, increasing compiler efficiency and eliminating the potential risk of corrupting developer code. In a simple implementation of partial classes, the compiler can perform a phase of precompilation where it "unifies" all the parts of a partial class. Then, compilation can proceed as usual.

Other benefits and effects of the partial class feature include:


Partial classes have existed in Smalltalk under the name of "Class Extensions" for considerable time. With the arrival of the .NET framework 2, Microsoft introduced partial classes, supported in both C# 2.0 and Visual Basic 2005. WinRT also supports partial classes.

This simple example, written in Visual Basic .NET, shows how parts of the same class are defined in two different files.


When compiled, the result is the same as if the two files were written as one, like this:
In Objective-C, partial classes, also known as categories, may even spread over multiple libraries and executables, like this example:

In Foundation, header file NSData.h:

In user-supplied library, a separate binary from Foundation framework, header file NSData+base64.h:

And in an app, yet another separate binary file, source code file main.m:

The dispatcher will find both methods called over the NSData instance and invoke both of them correctly.

Uninstantiable classes allow programmers to group together per-class fields and methods that are accessible at runtime without an instance of the class. Indeed, instantiation is prohibited for this kind of class.

For example, in C#, a class marked "static" can not be instantiated, can only have static members (fields, methods, other), may not have "instance constructors", and is "sealed".
An unnamed class or anonymous class is a class that is not bound to a name or identifier upon definition. This is analogous to named versus unnamed functions.

The benefits of organizing software into object classes fall into three categories:


Object classes facilitate rapid development because they lessen the semantic gap between the code and the users. System analysts can talk to both developers and users using essentially the same vocabulary, talking about accounts, customers, bills, etc. Object classes often facilitate rapid development because most object-oriented environments come with powerful debugging and testing tools. Instances of classes can be inspected at run time to verify that the system is performing as expected. Also, rather than get dumps of core memory, most object-oriented environments have interpreted debugging capabilities so that the developer can analyze exactly where in the program the error occurred and can see which methods were called to which arguments and with what arguments.

Object classes facilitate ease of maintenance via encapsulation. When developers need to change the behavior of an object they can localize the change to just that object and its component parts. This reduces the potential for unwanted side effects from maintenance enhancements.

Software re-use is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailor some aspect of the behavior or data accordingly. Re-use via interfaces (also known as methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.

These benefits come with a cost of course. One of the most serious obstacles to using object classes has been performance. Interpreted environments that support languages such as Smalltalk and CLOS provided rapid development but the resulting code was not nearly as fast as what could be achieved in some procedural languages such as C. This has been partly addressed by the development of object-oriented languages that are not interpreted such as C++ and Java. Also, due to Moore's law the processing power of computers has increased to the point where efficient code is not as critical for most systems as it was in the past.Still, no matter how well designed the language, there will always be an inevitable bit of required extra overhead to create a class rather than use procedural code and in some circumstances, especially where performance or memory are required to be optimal, that using object classes may not be the best approach.
Also, getting the benefits of object classes requires that they be used appropriately and that requires training. Without the proper training developers may simply code procedural programs in an object-oriented environment and end up with the worst of both worlds.

As a data type, a class is usually considered as a compile-time construct. A language may also support prototype or factory metaobjects that represent run-time information about classes, or even represent metadata that provides access to reflection facilities and ability to manipulate data structure formats at run-time. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at run-time. Some dynamic languages do not make strict distinctions between run-time and compile-time constructs, and therefore may not distinguish between metaobjects and classes.

For example, if Human is a metaobject representing the class Person, then instances of class Person can be created by using the facilities of the Human metaobject.





</doc>
<doc id="7394" url="https://en.wikipedia.org/wiki?curid=7394" title="Canterbury (disambiguation)">
Canterbury (disambiguation)

Canterbury is a city located in the county of Kent in southeast England. It may also refer to:















</doc>
<doc id="7397" url="https://en.wikipedia.org/wiki?curid=7397" title="Color blindness">
Color blindness

Color blindness, also known as color vision deficiency, is the decreased ability to see color or differences in color. Color blindness can make some educational activities difficult. Buying fruit, picking clothing, and reading traffic lights can be more challenging, for example. Problems, however, are generally minor and most people adapt. People with total color blindness, however, may also have decreased visual acuity and be uncomfortable in bright environments.
The most common cause of color blindness is an inherited fault in the development of one or more of the three sets of color sensing cones in the eye. Males are more likely to be color blind than females, as the genes responsible for the most common forms of color blindness are on the X chromosome. As females have two X chromosomes, a defect in one is typically compensated for by the other, while males only have one X chromosome. Color blindness can also result from physical or chemical damage to the eye, optic nerve, or parts of the brain. Diagnosis is typically with the Ishihara color test; however a number of other testing methods also exist.
There is no cure for color blindness. Diagnosis may allow a person's teacher to change their method of teaching to accommodate the decreased ability to recognize colors. Special lenses may help people with red–green color blindness when under bright conditions. There are also mobile apps that can help people identify colors.
Red–green color blindness is the most common form, followed by blue–yellow color blindness and total color blindness. Red–green color blindness affects up to 8% of males and 0.5% of females of Northern European descent. The ability to see color also decreases in old age. Being color blind may make people ineligible for certain jobs in certain countries. This may include pilot, train driver, and armed forces. The effect of color blindness on artistic ability, however, is controversial. The ability to draw appears to be unchanged and a number of famous artists are believed to have been color blind.

In almost all cases, color blind people retain blue–yellow discrimination, and most color-blind individuals are anomalous trichromats rather than complete dichromats. In practice, this means that they often retain a limited discrimination along the red–green axis of color space, although their ability to separate colors in this dimension is reduced. Color blindness very rarely refers to complete monochromatism.

Dichromats often confuse red and green items. For example, they may find it difficult to distinguish a Braeburn apple from a Granny Smith or red from green of traffic lights without other clues—for example, shape or position. Dichromats tend to learn to use texture and shape clues and so may be able to penetrate camouflage that has been designed to deceive individuals with normal color vision.

Colors of traffic lights are confusing to some dichromats as there is insufficient apparent difference between the red/amber traffic lights and sodium street lamps; also, the green can be confused with a grubby white lamp. This is a risk on high-speed undulating roads where angular cues cannot be used. British Rail color lamp signals use more easily identifiable colors: The red is blood red, the amber is yellow and the green is a bluish color. Most British road traffic lights are mounted vertically on a black rectangle with a white border (forming a "sighting board") and so dichromats can more easily look for the position of the light within the rectangle—top, middle or bottom. In the eastern provinces of Canada horizontally mounted traffic lights are generally differentiated by shape to facilitate identification for those with color blindness. In the United States, this is not done by shape but by position, as the red light is always on the left if the light is horizontal, or on top if the light is vertical. However, a lone flashing light (e.g. red for stop, yellow for caution) is still problematic.

Color vision deficiencies can be classified as acquired or inherited.


Color blindness is typically an inherited genetic disorder. It is most commonly inherited from mutations on the X chromosome but the mapping of the human genome has shown there are many causative mutations—mutations capable of causing color blindness originate from at least 19 different chromosomes and 56 different genes (as shown online at the Online Mendelian Inheritance in Man (OMIM)).
Two of the most common inherited forms of color blindness are protanomaly (and, more rarely, protanopia – the two together often known as "protans") and deuteranomaly (or, more rarely, deuteranopia – the two together often referred to as "deutans").
Both "protans" and "deutans" (of which the deutans are by far the most common) are known as "red–green color-blind" which is present in about 8 percent of human males and 0.6 percent of females of Northern European ancestry.

Some of the inherited diseases known to cause color blindness are:

Inherited color blindness can be congenital (from birth), or it can commence in childhood or adulthood. Depending on the mutation, it can be stationary, that is, remain the same throughout a person's lifetime, or progressive. As progressive phenotypes involve deterioration of the retina and other parts of the eye, certain forms of color blindness can progress to legal blindness, i.e., an acuity of 6/60 (20/200) or worse, and often leave a person with complete blindness.

Color blindness always pertains to the cone photoreceptors in retinas, as the cones are capable of detecting the color frequencies of light.

About 8 percent of males, and 0.6 percent of females, are red-green color blind in some way or another, whether it is one color, a color combination, or another mutation. The reason males are at a greater risk of inheriting an X linked mutation is that males only have one X chromosome (XY, with the Y chromosome carrying altogether different genes than the X chromosome), and females have two (XX); if a woman inherits a normal X chromosome in addition to the one that carries the mutation, she will not display the mutation. Men do not have a second X chromosome to override the chromosome that carries the mutation. If 8% of variants of a given gene are defective, the probability of a single copy being defective is 8%, but the probability that two copies are both defective is 0.08 × 0.08 = 0.0064, or just 0.64%.

Other causes of color blindness include brain or retinal damage caused by shaken baby syndrome, accidents and other trauma which produce swelling of the brain in the occipital lobe, and damage to the retina caused by exposure to ultraviolet light (10–300 nm). Damage often presents itself later on in life.

Color blindness may also present itself in the spectrum of degenerative diseases of the eye, such as age-related macular degeneration, and as part of the retinal damage caused by diabetes. Another factor that may affect color blindness includes a deficiency in Vitamin A.

Some subtle forms of colorblindness may be associated with chronic solvent-induced encephalopathy (CSE), caused by longtime exposure to solvent vapors.

Red–green color blindness can be caused by ethambutol, a drug used in the treatment of tuberculosis.

Based on clinical appearance, color blindness may be described as total or partial. Total color blindness is much less common than partial color blindness. There are two major types of color blindness: those who have difficulty distinguishing between red and green, and who have difficulty distinguishing between blue and yellow.

Immunofluorescent imaging is a way to determine red–green color coding. Conventional color coding is difficult for individuals with red–green color blindness (protanopia or deuteranopia) to discriminate. Replacing red with magenta or green with turquoise improves visibility for such individuals.

The different kinds of inherited color blindness result from partial or complete loss of function of one or more of the different cone systems. When one cone system is compromised, dichromacy results. The most frequent forms of human color blindness result from problems with either the middle or long wavelength sensitive cone systems, and involve difficulties in discriminating reds, yellows, and greens from one another. They are collectively referred to as "red–green color blindness", though the term is an over-simplification and is somewhat misleading. Other forms of color blindness are much more rare. They include problems in discriminating blues from greens and yellows from reds/pinks, and the rarest forms of all, complete color blindness or "monochromacy", where one cannot distinguish any color from grey, as in a black-and-white movie or photograph.

Protanopes, deuteranopes, and tritanopes are dichromats; that is, they can match any color they see with some mixture of just two primary colors (whereas normally humans are trichromats and require three primary colors). These individuals normally know they have a color vision problem and it can affect their lives on a daily basis. Two percent of the male population exhibit severe difficulties distinguishing between red, orange, yellow, and green. A certain pair of colors, that seem very different to a normal viewer, appear to be the same color (or different shades of same color) for such a dichromat. The terms protanopia, deuteranopia, and tritanopia come from Greek and literally mean "inability to see ("anopia") with the first ("prot-"), second ("deuter-"), or third ("trit-") [cone]", respectively.

Anomalous trichromacy is the least serious type of color deficiency. People with protanomaly, deuteranomaly, or tritanomaly are trichromats, but the color matches they make differ from the normal. They are called anomalous trichromats. In order to match a given spectral yellow light, protanomalous observers need more red light in a red/green mixture than a normal observer, and deuteranomalous observers need more green. From a practical standpoint though, many protanomalous and deuteranomalous people have very little difficulty carrying out tasks that require normal color vision. Some may not even be aware that their color perception is in any way different from normal.

Protanomaly and deuteranomaly can be diagnosed using an instrument called an anomaloscope, which mixes spectral red and green lights in variable proportions, for comparison with a fixed spectral yellow. If this is done in front of a large audience of males, as the proportion of red is increased from a low value, first a small proportion of the audience will declare a match, while most will see the mixed light as greenish; these are the deuteranomalous observers. Next, as more red is added the majority will say that a match has been achieved. Finally, as yet more red is added, the remaining, protanomalous, observers will declare a match at a point where normal observers will see the mixed light as definitely reddish.

Protanopia, deuteranopia, protanomaly, and deuteranomaly are commonly inherited forms of red–green color blindness which affect a substantial portion of the human population. Those affected have difficulty with discriminating red and green hues due to the absence or mutation of the red or green retinal photoreceptors. It is sex-linked: genetic red–green color blindness affects males much more often than females, because the genes for the red and green color receptors are located on the X chromosome, of which males have only one and females have two. Females (46, XX) are red–green color blind only if "both" their X chromosomes are defective with a similar deficiency, whereas males (46, XY) are color blind if their single X chromosome is defective.

The gene for red–green color blindness is transmitted from a color blind male to all his daughters who are heterozygote carriers and are usually unaffected. In turn, a carrier woman has a fifty percent chance of passing on a mutated X chromosome region to each of her male offspring. The sons of an affected male will not inherit the trait from him, since they receive his Y chromosome and not his (defective) X chromosome. Should an affected male have children with a carrier or colorblind woman, their daughters may be colorblind by inheriting an affected X chromosome from each parent.

Because one X chromosome is inactivated at random in each cell during a woman's development, deuteranomalous heterozygotes (i.e. female carriers of deuteranomaly) are potentially tetrachromats, because they will have the normal long wave (red) receptors, the normal medium wave (green) receptors, the abnormal medium wave (deuteranomalous) receptors and the normal autosomal short wave (blue) receptors in their retinas. The same applies to the carriers of protanomaly (who have two types of short wave receptors, normal medium wave receptors, and normal autosomal short wave receptors in their retinas). If, by chance, a woman is heterozygous for "both" protanomaly and deuteranomaly she could be pentachromatic. This situation could arise if, for instance, she inherited the X chromosome with the abnormal long wave gene (but normal medium wave gene) from her mother who is a carrier of protanomaly, and her other X chromosome from a deuteranomalous father. Such a woman would have a normal and an abnormal long wave receptor, a normal and abnormal medium wave receptor, and a normal autosomal short wave receptor – 5 different types of color receptors in all. The degree to which women who are carriers of either protanomaly or deuteranomaly are demonstrably tetrachromatic and require a mixture of four spectral lights to match an arbitrary light is very variable. In many cases it is almost unnoticeable, but in a minority the tetrachromacy is very pronounced. However, Jameson "et al." have shown that with appropriate and sufficiently sensitive equipment all female carriers of red-green color blindness (i.e. heterozygous protanomaly, or heterozygous deuteranomaly) are tetrachromats to a greater or lesser extent.

Since deuteranomaly is by far the most common form of red-green blindness among men of northwestern European descent (with an incidence of 8%), then the carrier frequency (and of potential deuteranomalous tetrachromacy) among the females of that genetic stock is 14.7% (= [92% × 8%] × 2).





Those with tritanopia and tritanomaly have difficulty discriminating between bluish and greenish hues, as well as yellowish and reddish hues.

Color blindness involving the inactivation of the short-wavelength sensitive cone system (whose absorption spectrum peaks in the bluish-violet) is called tritanopia or, loosely, blue–yellow color blindness. The tritanope's neutral point occurs near a yellowish 570 nm; green is perceived at shorter wavelengths and red at longer wavelengths. Mutation of the short-wavelength sensitive cones is called tritanomaly. Tritanopia is equally distributed among males and females. Jeremy H. Nathans (with the Howard Hughes Medical Institute) demonstrated that the gene coding for the blue receptor lies on chromosome 7, which is shared equally by males and females. Therefore, it is not sex-linked. This gene does not have any neighbor whose DNA sequence is similar. Blue color blindness is caused by a simple mutation in this gene.



Total color blindness is defined as the inability to see color. Although the term may refer to acquired disorders such as cerebral achromatopsia also known as color agnosia, it typically refers to congenital color vision disorders (i.e. more frequently rod monochromacy and less frequently cone monochromacy).

In cerebral achromatopsia, a person cannot perceive colors even though the eyes are capable of distinguishing them. Some sources do not consider these to be true color blindness, because the failure is of perception, not of vision. They are forms of visual agnosia.

Monochromacy is the condition of possessing only a single channel for conveying information about color. Monochromats possess a complete inability to distinguish any colors and perceive only variations in brightness. It occurs in two primary forms:

The typical human retina contains two kinds of light cells: the rod cells (active in low light) and the cone cells (active in normal daylight). Normally, there are three kinds of cone cells, each containing a different pigment, which are activated when the pigments absorb light. The spectral sensitivities of the cones differ; one is most sensitive to short wavelengths, one to medium wavelengths, and the third to medium-to-long wavelengths within the visible spectrum, with their peak sensitivities in the blue, green, and yellow-green regions of the spectrum, respectively. The absorption spectra of the three systems overlap, and combine to cover the visible spectrum. These receptors are known as short (S), medium (M), and long (L) wavelength cones, but are also often referred to as blue, green, and red cones, although this terminology is inaccurate.

The receptors are each responsive to a wide range of wavelengths. For example, the long wavelength "red" receptor has its peak sensitivity in the yellow-green, some way from the red end (longest wavelength) of the visible spectrum. The sensitivity of normal color vision actually depends on the overlap between the absorption ranges of the three systems: different colors are recognized when the different types of cone are stimulated to different degrees. Red light, for example, stimulates the long wavelength cones much more than either of the others, and reducing the wavelength causes the other two cone systems to be increasingly stimulated, causing a gradual change in hue.

Many of the genes involved in color vision are on the X chromosome, making color blindness much more common in males than in females because males only have one X chromosome, while females have two. Because this is an X-linked trait, an estimated 2–3% of women have a 4th color cone and can be considered tetrachromats. One such woman has been reported to be a true or functional tetrachromat, as she can discriminate colors most other people can't.

The Ishihara color test, which consists of a series of pictures of colored spots, is the test most often used to diagnose red–green color deficiencies. A figure (usually one or more Arabic digits) is embedded in the picture as a number of spots in a slightly different color, and can be seen with normal color vision, but not with a particular color defect. The full set of tests has a variety of figure/background color combinations, and enable diagnosis of which particular visual defect is present. The anomaloscope, described above, is also used in diagnosing anomalous trichromacy.
Because the Ishihara color test contains only numerals, it may not be useful in diagnosing young children, who have not yet learned to use numbers. In the interest of identifying these problems early on in life, alternative color vision tests were developed using only symbols (square, circle, car).

Besides the Ishihara color test, the US Navy and US Army also allow testing with the Farnsworth Lantern Test. This test allows 30% of color deficient individuals, whose deficiency is not too severe, to pass.

Another test used by clinicians to measure chromatic discrimination is the Farnsworth-Munsell 100 hue test. The patient is asked to arrange a set of colored caps or chips to form a gradual transition of color between two anchor caps.

The HRR color test (developed by Hardy, Rand, and Rittler) is a red–green color test that, unlike the Ishihara, also has plates for the detection of the tritan defects.

Most clinical tests are designed to be fast, simple, and effective at identifying broad categories of color blindness. In academic studies of color blindness, on the other hand, there is more interest in developing flexible tests to collect thorough datasets, identify copunctal points, and measure just noticeable differences.

There is generally no treatment to cure color deficiencies. ″The American Optometric Association reports a contact lens on one eye can increase the ability to differentiate between colors, though nothing can make you truly see the deficient color.″

Optometrists can supply colored spectacle lenses or a single red-tint contact lens to wear on the non-dominant eye, but although this may improve discrimination of some colors, it can make other colors more difficult to distinguish. A 1981 review of various studies to evaluate the effect of the X-chrom contact lens concluded that, while the lens may allow the wearer to achieve a better score on certain color vision tests, it did not correct color vision in the natural environment. A case history using the X-Chrom lens for a rod monochromat is reported and an X-Chrom manual is online.

Lenses that filter certain wavelengths of light can allow people with a cone anomaly, but not dichromacy, to see better separation of colors, especially those with classic "red/green" color blindness. They work by notching out wavelengths that strongly stimulate both red and green cones in a deuter- or protanomalous person, improving the distinction between the two cones' signals. As of 2013, sunglasses that notch out color wavelengths are available commercially.

Many applications for iPhone and iPad have been developed to help colorblind people to view the colors in a better way. Many applications launch a sort of simulation of colorblind vision to make normal-view people understand how the color-blinds see the world. Others allow a correction of the image grabbed from the camera with a special "daltonizer" algorithm.

The GNOME desktop environment provides colorblind accessibility using the gnome-mag and the libcolorblind software. Using a gnome applet, the user may switch a color filter on and off, choosing from a set of possible color transformations that will displace the colors in order to disambiguate them. The software enables, for instance, a colorblind person to see the numbers in the Ishihara test.

Color blindness affects a large number of individuals, with protanopia and deuteranopia being the most common types. In individuals with Northern European ancestry, as many as 8 percent of men and 0.4 percent of women experience congenital color deficiency.

The number affected varies among groups. Isolated communities with a restricted gene pool sometimes produce high proportions of color blindness, including the less usual types. Examples include rural Finland, Hungary, and some of the Scottish islands. In the United States, about 7 percent of the male population—or about 10.5 million men—and 0.4 percent of the female population either cannot distinguish red from green, or see red and green differently from how others do (Howard Hughes Medical Institute, 2006 ). More than 95 percent of all variations in human color vision involve the red and green receptors in male eyes. It is very rare for males or females to be "blind" to the blue end of the spectrum.

The first scientific paper on the subject of color blindness, "Extraordinary facts relating to the vision of colours", was published by the English chemist John Dalton in 1798 after the realization of his own color blindness. Because of Dalton's work, the general condition has been called "daltonism", although in English this term is now used only for deuteranopia.

Color codes present particular problems for those with color deficiencies as they are often difficult or impossible for them to perceive.

Good graphic design avoids using color coding or using color contrasts alone to express information; this not only helps color blind people, but also aids understanding by normally sighted people by providing them with multiple reinforcing cues.

Designers need to take into account that color-blindness is highly sensitive to differences in material. For example, a red–green colorblind person who is incapable of distinguishing colors on a map printed on paper may have no such difficulty when viewing the map on a computer screen or television. In addition, some color blind people find it easier to distinguish problem colors on artificial materials, such as plastic or in acrylic paints, than on natural materials, such as paper or wood. Third, for some color blind people, color can only be distinguished if there is a sufficient "mass" of color: thin lines might appear black, while a thicker line of the same color can be perceived as having color.

Designers should also note that red–blue and yellow–blue color combinations are generally safe. So instead of the ever-popular "red means bad and green means good" system, using these combinations can lead to a much higher ability to use color coding effectively. This will still cause problems for those with monochromatic color blindness, but it is still something worth considering.

When the need to process visual information as rapidly as possible arises, for example in an emergency situation, the visual system may operate only in shades of gray, with the extra information load in adding color being dropped. This is an important possibility to consider when designing, for example, emergency brake handles or emergency phones.

Color blindness may make it difficult or impossible for a person to engage in certain occupations. Persons with color blindness may be legally or practically barred from occupations in which color perception is an essential part of the job ("e.g.," mixing paint colors), or in which color perception is important for safety ("e.g.," operating vehicles in response to color-coded signals). This occupational safety principle originates from the Lagerlunda train crash of 1875 in Sweden. Following the crash, Professor Alarik Frithiof Holmgren, a physiologist, investigated and concluded that the color blindness of the engineer (who had died) had caused the crash. Professor Holmgren then created the first test using different-colored skeins to exclude people from jobs in the transportation industry on the basis of color blindness. However, there is a claim that there is no firm evidence that color deficiency did cause the collision, or that it might have not been the sole cause.

Color vision is important for occupations using telephone or computer networking cabling, as the individual wires inside the cables are color-coded using green, orange, brown, blue and white colors. Electronic wiring, transformers, resistors, and capacitors are color-coded as well, using black, brown, red, orange, yellow, green, blue, violet, gray, white, silver, gold.

Some countries have refused to grant driving licenses to individuals with color blindness. In Romania, there is an ongoing campaign to remove the legal restrictions that prohibit colorblind citizens from getting drivers' licenses.

The usual justification for such restrictions is that drivers of motor vehicles must be able to recognize color-coded signals, such as traffic lights or warning lights.

While many aspects of aviation depend on color coding, only a few of them are critical enough to be interfered with by some milder types of color blindness. Some examples include color-gun signaling of aircraft that have lost radio communication, color-coded glide-path indications on runways, and the like. Some jurisdictions restrict the issuance of pilot credentials to persons who suffer from color blindness for this reason. Restrictions may be partial, allowing color-blind persons to obtain certification but with restrictions, or total, in which case color-blind persons are not permitted to obtain piloting credentials at all.

In the United States, the Federal Aviation Administration requires that pilots be tested for normal color vision as part of their medical clearance in order to obtain the required medical certificate, a prerequisite to obtaining a pilot's certification. If testing reveals color blindness, the applicant may be issued a license with restrictions, such as no night flying and no flying by color signals—such a restriction effectively prevents a pilot from holding certain flying occupations, such as that of an airline pilot, although commercial pilot certification is still possible, and there are a few flying occupations that do not require night flight and thus are still available to those with restrictions due to color blindness (e.g., agricultural aviation). The government allows several types of tests, including medical standard tests ("e.g.," the Ishihara, Dvorine, and others) and specialized tests oriented specifically to the needs of aviation. If an applicant fails the standard tests, they will receive a restriction on their medical certificate that states: "Not valid for night flying or by color signal control". They may apply to the FAA to take a specialized test, administered by the FAA. Typically, this test is the "color vision light gun test". For this test an FAA inspector will meet the pilot at an airport with an operating control tower. The color signal light gun will be shone at the pilot from the tower, and they must identify the color. If they pass they may be issued a waiver, which states that the color vision test is no longer required during medical examinations. They will then receive a new medical certificate with the restriction removed. This was once a Statement of Demonstrated Ability (SODA), but the SODA was dropped, and converted to a simple waiver (letter) early in the 2000s.

Research published in 2009 carried out by the City University of London's Applied Vision Research Centre, sponsored by the UK's Civil Aviation Authority and the US Federal Aviation Administration, has established a more accurate assessment of color deficiencies in pilot applicants' red–green and yellow–blue color range which could lead to a 35% reduction in the number of prospective pilots who fail to meet the minimum medical threshold.

Inability to distinguish color does not necessarily preclude the ability to become a celebrated artist. The 20th century expressionist painter Clifton Pugh, three-time winner of Australia's Archibald Prize, on biographical, gene inheritance and other grounds has been identified as a protanope. 19th century French artist Charles Méryon became successful by concentrating on etching rather than painting after he was diagnosed as having a red–green deficiency.

A Brazilian court ruled that people with color blindness are protected by the Inter-American Convention on the Elimination of All Forms of Discrimination against Person with Disabilities.

At trial, it was decided that the carriers of color blindness have a right of access to wider knowledge, or the full enjoyment of their human condition.

In the United States, under federal anti-discrimination laws such as the Americans with Disabilities Act, color vision deficiencies have not been found to constitute a disability that triggers protection from workplace discrimination.

A famous traffic light on Tipperary Hill in Syracuse, New York, is upside-down due to the sentiments of its Irish American community, but has been criticized due to the potential hazard it poses for color-blind persons.

Some tentative evidence finds that color blind people are better at penetrating certain color camouflages. Such findings may give an evolutionary reason for the high rate of red–green color blindness. There is also a study suggesting that people with some types of color blindness can distinguish colors that people with normal color vision are not able to distinguish. In World War II, color blind observers were used to penetrate camouflage.

In September 2009, the journal "Nature" reported that researchers at the University of Washington and University of Florida were able to give trichromatic vision to squirrel monkeys, which normally have only dichromatic vision, using gene therapy.

In 2003, a cybernetic device called eyeborg was developed to allow the wearer to hear sounds representing different colors. Achromatopsic artist Neil Harbisson was the first to use such a device in early 2004; the eyeborg allowed him to start painting in color by memorizing the sound corresponding to each color. In 2012, at a TED Conference, Harbisson explained how he could now perceive colors outside the ability of human vision.





</doc>
<doc id="7398" url="https://en.wikipedia.org/wiki?curid=7398" title="Computer security">
Computer security

Cybersecurity, computer security or IT security is the protection of computer systems from the theft and damage to their hardware, software or information, as well as from disruption or misdirection of the services they provide.

Cybersecurity includes controlling physical access to the hardware, as well as protecting against harm that may come via network access, data and code injection. Also, due to malpractice by operators, whether intentional or accidental, IT security is susceptible to being tricked into deviating from secure procedures through various methods.

The field is of growing importance due to the increasing reliance on computer systems and the Internet, wireless networks such as Bluetooth and Wi-Fi, the growth of "smart" devices, including smartphones, televisions and tiny devices as part of the Internet of Things.

A vulnerability is a weakness in design, implementation, operation or internal control. Most of the vulnerabilities that have been discovered are documented in the Common Vulnerabilities and Exposures (CVE) database.

An "exploitable" vulnerability is one for which at least one working attack or "exploit" exists. Vulnerabilities are often hunted or exploited with the aid of automated tools or manually using customized scripts.

To secure a computer system, it is important to understand the attacks that can be made against it, and these threats can typically be classified into one of these categories below:

A backdoor in a computer system, a cryptosystem or an algorithm, is any secret method of bypassing normal authentication or security controls. They may exist for a number of reasons, including by original design or from poor configuration. They may have been added by an authorized party to allow some legitimate access, or by an attacker for malicious reasons; but regardless of the motives for their existence, they create a vulnerability.

Denial of service attacks (DoS) are designed to make a machine or network resource unavailable to its intended users. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victims account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of Distributed denial of service (DDoS) attacks are possible, where the attack comes from a large number of points – and defending is much more difficult. Such attacks can originate from the zombie computers of a botnet, but a range of other techniques are possible including reflection and amplification attacks, where innocent systems are fooled into sending traffic to the victim.

An unauthorized user gaining physical access to a computer is most likely able to directly copy data from it. They may also compromise security by making operating system modifications, installing software worms, keyloggers, covert listening devices or using wireless mice. Even when the system is protected by standard security measures, these may be able to be by-passed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.

Eavesdropping is the act of surreptitiously listening to a private conversation, typically between hosts on a network. For instance, programs such as Carnivore and NarusInSight have been used by the FBI and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact to the outside world) can be eavesdropped upon via monitoring the faint electro-magnetic transmissions generated by the hardware; TEMPEST is a specification by the NSA referring to these attacks.

Spoofing is the act of masquerading as a valid entity through falsification of data (such as an IP address or username), in order to gain access to information or resources that one is otherwise unauthorized to obtain. There are several types of spoofing, including:


Tampering describes a malicious modification of products. So-called "Evil Maid" attacks and security services planting of surveillance capability into routers are examples.

Privilege escalation describes a situation where an attacker with some level of restricted access is able to, without authorization, elevate their privileges or access level. For example, a standard computer user may be able to fool the system into giving them access to restricted data; or even to "become root" and have full unrestricted access to a system.

Phishing is the attempt to acquire sensitive information such as usernames, passwords, and credit card details directly from users. Phishing is typically carried out by email spoofing or instant messaging, and it often directs users to enter details at a fake website whose look and feel are almost identical to the legitimate one. Preying on a victim's trust, phishing can be classified as a form of social engineering.

Clickjacking, also known as "UI redress attack" or "User Interface redress attack", is a malicious technique in which an attacker tricks a user into clicking on a button or link on another webpage while the user intended to click on the top level page. This is done using multiple transparent or opaque layers. The attacker is basically "hijacking" the clicks meant for the top level page and routing them to some other irrelevant page, most likely owned by someone else. A similar technique can be used to hijack keystrokes. Carefully drafting a combination of stylesheets, iframes, buttons and text boxes, a user can be led into believing that they are typing the password or other information on some authentic webpage while it is being channeled into an invisible frame controlled by the attacker.

Social engineering aims to convince a user to disclose secrets such as passwords, card numbers, etc. by, for example, impersonating a bank, a contractor, or a customer.

A common scam involves fake CEO emails sent to accounting and finance departments. In early 2016, the FBI reported that the scam has cost US businesses more than $2bn in about two years.

In May 2016, the Milwaukee Bucks NBA team was the victim of this type of cyber scam with a perpetrator impersonating the team's president Peter Feigin, resulting in the handover of all the team's employees' 2015 W-2 tax forms.

Employee behavior can have a big impact on information security in organizations. Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization.″Exploring the Relationship between Organizational Culture and Information Security Culture″ provides the following definition of information security culture: ″ISC is the totality of patterns of behavior in an organization that contribute to the protection of information of all kinds.″

Andersson and Reimers (2014) found that employees often do not see themselves as part of the organization Information Security "effort" and often take actions that ignore organizational Information Security best interests. Research shows Information security culture needs to be improved continuously. In ″Information Security Culture from Analysis to Change″, authors commented, ″It′s a never ending process, a cycle of evaluation and change or maintenance.″ To manage the information security culture, five steps should be taken: Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.

The growth in the number of computer systems, and the increasing reliance upon them of individuals, businesses, industries and governments means that there are an increasing number of systems at risk.

The computer systems of financial regulators and financial institutions like the U.S. Securities and Exchange Commission, SWIFT, investment banks, and commercial banks are prominent hacking targets for cybercriminals interested in manipulating markets and making illicit gains. Web sites and apps that accept or store credit card numbers, brokerage accounts, and bank account information are also prominent hacking targets, because of the potential for immediate financial gain from transferring money, making purchases, or selling the information on the black market. In-store payment systems and ATMs have also been tampered with in order to gather customer account data and PINs.

Computers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies. Vulnerabilities in smart meters (many of which use local radio or cellular communications) can cause problems with billing fraud.

The aviation industry is very reliant on a series of complex systems which could be attacked. A simple power outage at one airport can cause repercussions worldwide, much of the system relies on radio transmissions which could be disrupted, and controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore. There is also potential for attack from within an aircraft.

In Europe, with the (Pan-European Network Service) and NewPENS, and in the US with the NextGen program, air navigation service providers are moving to create their own dedicated networks.

The consequences of a successful attack range from loss of confidentiality to loss of system integrity, air traffic control outages, loss of aircraft, and even loss of life.

Desktop computers and laptops are commonly targeted to gather passwords or financial account information, or to construct a botnet to attack another target. Smartphones, tablet computers, smart watches, and other mobile devices such as quantified self devices like activity trackers have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. Wifi, Bluetooth, and cell phone networks on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.

The increasing number of home automation devices such as the Nest thermostat are also potential targets.

Large corporations are common targets. In many cases this is aimed at financial gain through identity theft and involves data breaches such as the loss of millions of clients' credit card details by Home Depot, Staples, Target Corporation, and the most recent breach of Equifax. 

Some cyberattacks are ordered by foreign governments, these governments engage in cyberwarfare with the intent to spread their propaganda, sabotage, or spy on their targets. Many people believe the Russian government played a major role in the US presidential election of 2016 by using Twitter and Facebook to affect the results of the election, despite the fact that no evidence has been found. 

Medical records have been targeted for use in general identify theft, health insurance fraud, and impersonating patients to obtain prescription drugs for recreational purposes or resale. Although cyber threats continue to increase, 62% of all organizations did not increase security training for their business in 2015.

Not all attacks are financially motivated however; for example security firm HBGary Federal suffered a serious series of attacks in 2011 from hacktivist group Anonymous in retaliation for the firm's CEO claiming to have infiltrated their group, and in the Sony Pictures attack of 2014 the motive appears to have been to embarrass with data leaks, and cripple the company by wiping workstations and servers.

Vehicles are increasingly computerized, with engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver-assistance systems on many models. Additionally, connected cars may use WiFi and Bluetooth to communicate with onboard consumer devices and the cell phone network. Self-driving cars are expected to be even more complex.

All of these systems carry some security risk, and such issues have gained wide attention. Simple examples of risk include a malicious compact disc being used as an attack vector, and the car's onboard microphones being used for eavesdropping. However, if access is gained to a car's internal controller area network, the danger is much greater – and in a widely publicised 2015 test, hackers remotely carjacked a vehicle from 10 miles away and drove it into a ditch.

Manufacturers are reacting in a number of ways, with Tesla in 2016 pushing out some security fixes "over the air" into its cars' computer systems.

In the area of autonomous vehicles, in September 2016 the United States Department of Transportation announced some initial safety standards, and called for states to come up with uniform policies.

Government and military computer systems are commonly attacked by activists and foreign powers. Local and regional government infrastructure such as traffic light controls, police and intelligence agency communications, personnel records, student records, and financial systems are also potential targets as they are now all largely computerized. Passports and government ID cards that control access to facilities which use RFID can be vulnerable to cloning.

The Internet of things (IoT) is the network of physical objects such as devices, vehicles, and buildings that are embedded with electronics, software, sensors, and network connectivity that enables them to collect and exchange data – and concerns have been raised that this is being developed without appropriate consideration of the security challenges involved.

While the IoT creates opportunities for more direct integration of the physical world into computer-based systems,
it also provides opportunities for misuse. In particular, as the Internet of Things spreads widely, cyber attacks are likely to become an increasingly physical (rather than simply virtual) threat. If a front door's lock is connected to the Internet, and can be locked/unlocked from a phone, then a criminal could enter the home at the press of a button from a stolen or hacked phone. People could stand to lose much more than their credit card numbers in a world controlled by IoT-enabled devices. Thieves have also used electronic means to circumvent non-Internet-connected hotel door locks.

Medical devices have either been successfully attacked or had potentially deadly vulnerabilities demonstrated, including both in-hospital diagnostic equipment and implanted devices including pacemakers and insulin pumps. There are many reports of hospitals and hospital organizations getting hacked, including ransomware attacks, Windows XP exploits, viruses, and data breaches of sensitive data stored on hospital servers. On 28 December 2016 the US Food and Drug Administration released its recommendations for how medical device manufacturers should maintain the security of Internet-connected devices – but no structure for enforcement.

Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. "Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal." Security breaches continue to cost businesses billions of dollars but a survey revealed that 66% of security staffs do not believe senior leadership takes cyber precautions as a strategic priority.

However, reasonable estimates of the financial cost of security breaches can actually help organizations make rational investment decisions. According to the classic Gordon-Loeb Model analyzing the optimal investment level in information security, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).

As with physical security, the motivations for breaches of computer security vary between attackers. Some are thrill-seekers or vandals, some are activists, others are criminals looking for financial gain. State-sponsored attackers are now common and well resourced, but started with amateurs such as Markus Hess who hacked for the KGB, as recounted by Clifford Stoll, in "The Cuckoo's Egg".

A standard part of threat modelling for any particular system is to identify what might motivate an attack on that system, and who might be motivated to breach it. The level and detail of precautions will vary depending on the system to be secured. A home personal computer, bank, and classified military network face very different threats, even when the underlying technologies in use are similar.

In computer security a countermeasure is an action, device, procedure, or technique that reduces a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.

Some common countermeasures are listed in the following sections:

Security by design, or alternately secure by design, means that the software has been designed from the ground up to be secure. In this case, security is considered as a main feature.

Some of the techniques in this approach include:

The Open Security Architecture organization defines IT security architecture as "the design artifacts that describe how the security controls (security countermeasures) are positioned, and how they relate to the overall information technology architecture. These controls serve the purpose to maintain the system's quality attributes: confidentiality, integrity, availability, accountability and assurance services".

Techopedia defines security architecture as "a unified security design that addresses the necessities and potential risks involved in a certain scenario or environment. It also specifies when and where to apply security controls. The design process is generally reproducible." The key attributes of security architecture are:

A state of computer "security" is the conceptual ideal, attained by the use of the three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:


Today, computer security comprises mainly "preventive" measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real time filtering and blocking. Another implementation is a so-called "physical firewall", which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.

Some organizations are turning to big data platforms, such as Apache Hadoop, to extend data accessibility and machine learning to detect advanced persistent threats.

However, relatively few organisations maintain computer systems with effective detection systems, and fewer still have organised response mechanisms in place. As a result, as Reuters points out: "Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets". The primary obstacle to effective eradication of cyber crime could be traced to excessive reliance on firewalls and other automated "detection" systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.

Vulnerability management is the cycle of identifying, and remediating or mitigating vulnerabilities, especially in software and firmware. Vulnerability management is integral to computer security and network security.

Vulnerabilities can be discovered with a vulnerability scanner, which analyzes a computer system in search of known vulnerabilities, such as open ports, insecure software configuration, and susceptibility to malware.

Beyond vulnerability scanning, many organisations contract outside security auditors to run regular penetration tests against their systems to identify vulnerabilities. In some sectors this is a contractual requirement.

While formal verification of the correctness of computer systems is possible, it is not yet common. Operating systems formally verified include seL4, and SYSGO's PikeOS – but these make up a very small percentage of the market.

Cryptography properly implemented is now virtually impossible to directly break. Breaking them requires some non-cryptographic input, such as a stolen key, stolen plaintext (at either end of the transmission), or some other extra cryptanalytic information.

Two factor authentication is a method for mitigating unauthorized access to a system or sensitive information. It requires "something you know"; a password or PIN, and "something you have"; a card, dongle, cellphone, or other piece of hardware. This increases security as an unauthorized person needs both of these to gain access. The more tight we are on security measures, the less unauthorized hacks there will be.

Social engineering and direct computer access (physical) attacks can only be prevented by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Training is often involved to help mitigate this risk, but even in a highly disciplined environments (e.g. military organizations), social engineering attacks can still be difficult to foresee and prevent.

Enoculation, derived from inoculation theory, seeks to prevent social engineering and other fraudulent tricks or traps by instilling a resistance to persuasion attempts through exposure to similar or related attempts.

It is possible to reduce an attacker's chances by keeping systems up to date with security patches and updates, using a security scanner or/and hiring competent people responsible for security. The effects of data loss/damage can be reduced by careful backing up and insurance.

While hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process, hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.


One use of the term "computer security" refers to technology that is used to implement secure operating systems. In the 1980s the United States Department of Defense (DoD) used the "Orange Book" standards, but the current international standard ISO/IEC 15408, "Common Criteria" defines a number of progressively more stringent Evaluation Assurance Levels. Many common operating systems meet the EAL4 standard of being "Methodically Designed, Tested and Reviewed", but the formal verification required for the highest levels means that they are uncommon. An example of an EAL6 ("Semiformally Verified Design and Tested") system is Integrity-178B, which is used in the Airbus A380
and several military jets.

In software engineering, secure coding aims to guard against the accidental introduction of security vulnerabilities. It is also possible to create software designed from the ground up to be secure. Such systems are "secure by design". Beyond this, formal verification aims to prove the correctness of the algorithms underlying a system;
important for cryptographic protocols for example.

Within computer systems, two of many security models capable of enforcing privilege separation are access control lists (ACLs) and capability-based security. Using ACLs to confine programs has been proven to be insecure in many situations, such as if the host computer can be tricked into indirectly allowing restricted file access, an issue known as the confused deputy problem. It has also been shown that the promise of ACLs of giving access to an object to only one person can never be guaranteed in practice. Both of these problems are resolved by capabilities. This does not mean practical flaws exist in all ACL-based systems, but only that the designers of certain utilities must take responsibility to ensure that they do not introduce flaws.

Capabilities have been mostly restricted to research operating systems, while commercial OSs still use ACLs. Capabilities can, however, also be implemented at the language level, leading to a style of programming that is essentially a refinement of standard object-oriented design. An open source project in the area is the E language.

Repeated education/training in security "best practices" can have a marked effect on compliance with good end user network security habits—which particularly protect against phishing, ransomware and other forms of malware which have a social engineering aspect.

Responding forcefully to attempted security breaches (in the manner that one would for attempted physical security breaches) is often very difficult for a variety of reasons:


Some illustrative examples of different types of computer security breaches are given below.

In 1988, only 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On 2 November 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers – the first internet "computer worm". The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris, Jr. who said 'he wanted to count how many machines were connected to the Internet'.

In 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as
a trusted Rome center user.

In early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.

The computer worm known as Stuxnet reportedly ruined almost one-fifth of Iran's nuclear centrifuges by disrupting industrial programmable logic controllers (PLCs) in a targeted attack generally believed to have been launched by Israel and the United States – although neither has publicly admitted this.

In early 2013, documents provided by Edward Snowden were published by "The Washington Post" and "The Guardian" exposing the massive scale of NSA global surveillance. It was also revealed that the NSA had deliberately inserted a backdoor in a NIST standard for encryption and tapped the links between Google's data centres.

In 2013 and 2014, a Russian/Ukrainian hacking ring known as "Rescator" broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards, and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers. Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. "The malware utilized is absolutely unsophisticated and uninteresting," says Jim Walter, director of threat intelligence operations at security technology company McAfee – meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.

In April 2015, the Office of Personnel Management discovered it had been hacked more than a year earlier in a data breach, resulting in the theft of approximately 21.5 million personnel records handled by the office. The Office of Personnel Management hack has been described by federal officials as among the largest breaches of government data in the history of the United States. Data targeted in the breach included personally identifiable information such as Social Security Numbers, names, dates and places of birth, addresses, and fingerprints of current and former government employees as well as anyone who had undergone a government background check. It is believed the hack was perpetrated by Chinese hackers but the motivation remains unclear.

In July 2015, a hacker group known as "The Impact Team" successfully breached the extramarital relationship website Ashley Madison. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company's CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently. With this initial data release, the group stated "Avid Life Media has been instructed to take Ashley Madison and Established Men offline permanently in all forms, or we will release all customer records, including profiles with all the customers' secret sexual fantasies and matching credit card transactions, real names and addresses, and employee documents and emails. The other websites may stay online." When Avid Life Media, the parent company that created the Ashley Madison website, did not take the site offline, The Impact Group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned, but the website remained functional.

Conflict of laws in cyberspace has become a major cause of concern for computer security community. Some of the main challenges and complaints about the antivirus industry are the lack of global web regulations, a global base of common rules to judge, and eventually punish, cyber crimes and cyber criminals. There is no global cyber law and cyber security treaty that can be invoked for enforcing global cyber security issues.

International legal issues of cyber attacks are complicated in nature. Even if an antivirus firm locates the cybercriminal behind the creation of a particular virus or piece of malware or form of cyber attack, often the local authorities cannot take action due to lack of laws under which to prosecute. Authorship attribution for cyber crimes and cyber attacks is a major problem for all law enforcement agencies.

"[Computer viruses] switch from one country to another, from one jurisdiction to another – moving around the world, using the fact that we don't have the capability to globally police operations like this. So the Internet is as if someone [had] given free plane tickets to all the online criminals of the world." Use of dynamic DNS, fast flux and bullet proof servers have added own complexities to this situation.

The role of the government is to make regulations to force companies and organizations to protect their systems, infrastructure and information from any cyberattacks, but also to protect its own national infrastructure such as the national power-grid.

The question of whether the government should intervene or not in the regulation of the cyberspace is a very polemical one. Indeed, for as long as it has existed and by definition, the cyberspace is a virtual space free of any government intervention. Where everyone agrees that an improvement on cyber security is more than vital, is the government the best actor to solve this issue?
Many government officials and experts think that the government should step in and that there is a crucial need for regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the "industry only responds when you threaten regulation. If the industry doesn't respond (to the threat), you have to follow through."
On the other hand, executives from the private sector agree that improvements are necessary, but think that the government intervention would affect their ability to innovate efficiently.

Many different teams and organisations exist, including:


CSIRTs in Europe collaborate in the TERENA task force TF-CSIRT. TERENA's Trusted Introducer service provides an accreditation and certification scheme for CSIRTs in Europe. A full list of known CSIRTs in Europe is available from the Trusted Introducer website.

Most countries have their own computer emergency response team to protect network security.

On 3 October 2010, Public Safety Canada unveiled Canada's Cyber Security Strategy, following a Speech from the Throne commitment to boost the security of Canadian cyberspace. The aim of the strategy is to strengthen Canada's "cyber systems and critical infrastructure sectors, support economic growth and protect Canadians as they connect to each other and to the world." Three main pillars define the strategy: securing government systems, partnering to secure vital cyber systems outside the federal government, and helping Canadians to be secure online. The strategy involves multiple departments and agencies across the Government of Canada. The Cyber Incident Management Framework for Canada outlines these responsibilities, and provides a plan for coordinated response between government and other partners in the event of a cyber incident. The Action Plan 2010–2015 for Canada's Cyber Security Strategy outlines the ongoing implementation of the strategy.

Public Safety Canada's Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada's critical infrastructure and cyber systems. The CCIRC provides support to mitigate cyber threats, technical support to respond and recover from targeted cyber attacks, and provides online tools for members of Canada's critical infrastructure sectors. The CCIRC posts regular cyber security bulletins on the Public Safety Canada website. The CCIRC also operates an online reporting tool where individuals and organizations can report a cyber incident. Canada's Cyber Security Strategy is part of a larger, integrated approach to critical infrastructure protection, and functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure.

On 27 September 2010, Public Safety Canada partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations dedicated to informing the general public on how to protect themselves online. On 4 February 2014, the Government of Canada launched the Cyber Security Cooperation Program. The program is a $1.5 million five-year initiative aimed at improving Canada's cyber systems through grants and contributions to projects in support of this objective. Public Safety Canada aims to begin an evaluation of Canada's Cyber Security Strategy in early 2015. Public Safety Canada administers and routinely updates the GetCyberSafe portal for Canadian citizens, and carries out Cyber Security Awareness Month during October.

China's Central Leading Group for Internet Security and Informatization () was established on 27 February 2014. This Leading Small Group (LSG) of the Communist Party of China is headed by General Secretary Xi Jinping himself and is staffed with relevant Party and state decision-makers. The LSG was created to overcome the incoherent policies and overlapping responsibilities that characterized China's former cyberspace decision-making mechanisms. The LSG oversees policy-making in the economic, political, cultural, social and military fields as they relate to network security and IT strategy. This LSG also coordinates major policy initiatives in the international arena that promote norms and standards favored by the Chinese government and that emphasize the principle of national sovereignty in cyberspace.

Berlin starts National Cyber Defense Initiative:
On 16 June 2011, the German Minister for Home Affairs, officially opened the new German NCAZ (National Center for Cyber Defense) Nationales Cyber-Abwehrzentrum located in Bonn. The NCAZ closely cooperates with BSI (Federal Office for Information Security) Bundesamt für Sicherheit in der Informationstechnik, BKA (Federal Police Organisation) Bundeskriminalamt (Deutschland), BND (Federal Intelligence Service) Bundesnachrichtendienst, MAD (Military Intelligence Service) Amt für den Militärischen Abschirmdienst and other national organisations in Germany taking care of national security aspects. According to the Minister the primary task of the new organization founded on 23 February 2011, is to detect and prevent attacks against the national infrastructure and mentioned incidents like Stuxnet.

Some provisions for cyber security have been incorporated into rules framed under the Information Technology Act 2000.

The National Cyber Security Policy 2013 is a policy framework by Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyber attacks, and safeguard "information, such as personal information (of web users), financial and banking information and sovereign data". CERT- In is the nodal agency which monitors the cyber threats in the country. The post of National Cyber Security Coordinator has also been created in the Prime Minister's Office (PMO).

The Indian Companies Act 2013 has also introduced cyber law and cyber security obligations on the part of Indian directors.
Some provisions for cyber security have been incorporated into rules framed under the Information Technology Act 2000 Update in 2013.

O CNCS em Portugal promove a utilização do ciberespaço de uma forma livre, confiável e segura, através da melhoria contínua da cibersegurança nacional e da cooperação internacional.
— Cyber Security Services, Nano IT Security is a Portuguese company specialized in cyber security, pentesting and vulnerability analyses.

Cyber-crime has risen rapidly in Pakistan. There are about 34 million Internet users with 133.4 million mobile subscribers in Pakistan. According to Cyber Crime Unit (CCU), a branch of Federal Investigation Agency, only 62 cases were reported to the unit in 2007, 287 cases in 2008, ratio dropped in 2009 but in 2010, more than 312 cases were registered. However, there are many unreported incidents of cyber-crime.

"Pakistan's Cyber Crime Bill 2007", the first pertinent law, focuses on electronic crimes, for example cyber-terrorism, criminal access, electronic system fraud, electronic forgery, and misuse of encryption.

National Response Centre for Cyber Crime (NR3C) – FIA is a law enforcement agency dedicated to fighting cyber crime. Inception of this Hi-Tech crime fighting unit transpired in 2007 to identify and curb the phenomenon of technological abuse in society. However, certain private firms are also working in cohesion with the government to improve cyber security and curb cyber attacks.

Following cyber attacks in the first half of 2013, when the government, news media, television station, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart for these attacks, as well as incidents that occurred in 2009, 2011, and 2012, but Pyongyang denies the accusations.

The 1986 , more commonly known as the Computer Fraud and Abuse Act is the key legislation. It prohibits unauthorized access or damage of "protected computers" as defined in .

Although various other measures have been proposed, such as the "Cybersecurity Act of 2010 – S. 773" in 2009, the "International Cybercrime Reporting and Cooperation Act – H.R.4962" and "Protecting Cyberspace as a National Asset Act of 2010 – S.3480" in 2010 – none of these has succeeded.

Executive order "Improving Critical Infrastructure Cybersecurity" was signed 12 February 2013.

The Department of Homeland Security has a dedicated division responsible for the response system, risk management program and requirements for cybersecurity in the United States called the National Cyber Security Division. The division is home to US-CERT operations and the National Cyber Alert System. The National Cybersecurity and Communications Integration Center brings together government organizations responsible for protecting computer networks and networked infrastructure.

The third priority of the Federal Bureau of Investigation (FBI) is to: ""Protect the United States against cyber-based attacks and high-technology crimes"", and they, along with the National White Collar Crime Center (NW3C), and the Bureau of Justice Assistance (BJA) are part of the multi-agency task force, The Internet Crime Complaint Center, also known as IC3.

In addition to its own specific duties, the FBI participates alongside non-profit organizations such as InfraGard.

In the criminal division of the United States Department of Justice operates a section called the Computer Crime and Intellectual Property Section. The CCIPS is in charge of investigating computer crime and intellectual property crime and is specialized in the search and seizure of digital evidence in computers and networks.

The United States Cyber Command, also known as USCYBERCOM, is tasked with the defense of specified Department of Defense information networks and ensures ""the security, integrity, and governance of government and military IT infrastructure and assets"" It has no role in the protection of civilian networks.

The U.S. Federal Communications Commission's role in cybersecurity is to strengthen the protection of critical communications infrastructure, to assist in maintaining the reliability of networks during disasters, to aid in swift recovery after, and to ensure that first responders have access to effective communications services.

The Food and Drug Administration has issued guidance for medical devices, and the National Highway Traffic Safety Administration is concerned with automotive cybersecurity. After being criticized by the Government Accountability Office, and following successful attacks on airports and claimed attacks on airplanes, the Federal Aviation Administration has devoted funding to securing systems on board the planes of private manufacturers, and the Aircraft Communications Addressing and Reporting System. Concerns have also been raised about the future Next Generation Air Transportation System.

"Computer emergency response team" is a name given to expert groups that handle computer security incidents.
In the US, two distinct organization exist, although they do work closely together.

There is growing concern that cyberspace will become the next theater of warfare. As Mark Clayton from the "Christian Science Monitor" described in an article titled "The New Cyber Arms Race":

In the future, wars will not just be fought by soldiers with guns or with planes that drop bombs. They will also be fought with the click of a mouse a half a world away that unleashes carefully weaponized computer programs that disrupt or destroy critical industries like utilities, transportation, communications, and energy. Such attacks could also disable military networks that control the movement of troops, the path of jet fighters, the command and control of warships.
This has led to new terms such as "cyberwarfare" and "cyberterrorism". The United States Cyber Command was created in 2009 and many other countries have similar forces.

Cybersecurity is a fast-growing field of IT concerned with reducing organizations' risk of hack or data breach. According to research from the Enterprise Strategy Group, 46% of organizations say that they have a "problematic shortage" of cybersecurity skills in 2016, up from 28% in 2015. Commercial, government and non-governmental organizations all employ cybersecurity professionals. The fastest increases in demand for cybersecurity workers are in industries managing increasing volumes of consumer data such as finance, health care, and retail. However, the use of the term "cybersecurity" is more prevalent in government job descriptions.

Typical cyber security job titles and descriptions include:
Student programs are also available to people interested in beginning a career in cybersecurity. Meanwhile, a flexible and effective option for information security professionals of all experience levels to keep studying is online security training, including webcasts. A wide range of certified courses are also available.

The following terms used with regards to engineering secure systems are explained below.





</doc>
<doc id="7400" url="https://en.wikipedia.org/wiki?curid=7400" title="Chris Cunningham">
Chris Cunningham

Chris Cunningham (born 15 October 1970) is a British video artist. He has primarily directed music videos for ambient music and electronica acts such as Autechre and Aphex Twin. He has also created art installations and directed short movies. He was approached to direct a movie version of the cyberpunk novel "Neuromancer", but nothing came of early discussions. In the 2000s, Cunningham began doing music production work. He has also designed album artwork for a variety of musicians.

After seeing Cunningham's work on the 1995 film version of "Judge Dredd", Stanley Kubrick head-hunted Cunningham to design and supervise animatronic tests of the central robot child character in his version of the film "A.I. Artificial Intelligence". Cunningham worked for over a year on the film before leaving to pursue a career as a director.

Earlier work in film included model-making, prosthetic make-up and concept illustrations for "Hardware" and "Dust Devil" for director Richard Stanley; work on "Nightbreed" for Clive Barker; and on "Alien" for David Fincher. Between 1990 and 1992, he contributed the occasional cover painting and strip to "Judge Dredd Megazine", working under the pseudonym "Chris Halls"; Halls is his stepfather's surname.

Cunningham has had close ties to Warp Records since his first production for Autechre. Videos for Aphex Twin's "Come to Daddy" and "Windowlicker" are perhaps his best known. His video for Björk's "All Is Full of Love" won multiple awards, including an MTV music video award for Breakthrough Video and was nominated for a Grammy for Best Short Form Music Video. It was also the first ever music video to win a Gold Pencil at the D&AD Awards. It can still be seen at the Museum of Modern Art in New York. His video for Aphex Twin's "Windowlicker" was nominated for the "Best Video" award at the Brit Awards 2000. He also directed Madonna's "Frozen" video which became an international hit and won the award for Best Special Effects at the 1998 MTV Music Video Awards. Cunningham also came out of a seven-year hiatus from making music videos to direct the video for "Sheena Is a Parasite" by The Horrors.

His video installation "Flex" was first shown in 2000 at the Royal Academy of Arts, and subsequently at the Anthony d'Offay Gallery and other art galleries. "Flex" was commissioned by the Anthony d'Offay Gallery for the exhibition curated by Norman Rosenthal and Max Wigram at the Royal Academy of Arts in 2000.

The Anthony d'Offay Gallery also commissioned "Monkey Drummer", a 2½ minute piece intended for exhibition as a companion to "Flex" at the 2000 "Apocalypse" exhibition at the Royal Academy of Arts: however, the piece was not finished in time. In it an automaton with nine appendages and the head of a monkey plays the drums to "Mt Saint Michel + Saint Michaels Mount", the 10th track on Aphex Twin's 2001 album "drukqs". "Monkey Drummer" debuted as part of Cunningham's installation at the 49th International Exhibition of Art at the 2001 Venice Biennale, which consisted of a loop of "Monkey Drummer", "Flex", and his video for Björk's "All Is Full of Love". In 2002 both "Flex" and "Monkey Drummer" were exhibited by 5th Gallery in Dublin, Ireland, in an exhibition curated by Artist/Curator Paul Murnaghan,

In 2007, an excerpt from "Flex" was shown in the Barbican's exhibition Seduced: Art and Sex from Antiquity to Now curated by Martin Kemp, Marina Wallace and Joanne Bernstein. alongside other pieces by Bacon, Klimt, Rembrandt, Rodin and Picasso.

In 2005, Cunningham released the short film "Rubber Johnny" as a DVD accompanied by a book of photographs and drawings. "Rubber Johnny", a six-minute experimental short film cut to a soundtrack by Aphex Twin, remixed by Cunningham was shot between 2001 and 2004. Shot on DV night-vision, it was made in Cunningham's own time as a home movie of sorts, and took three and half years of weekends to complete. The Telegraph called it "like a Looney Tunes short for a generation raised on video nasties and rave music".

During this period Cunningham also made another short film for Warp Films, Spectral Musicians, which remains unreleased. The short film was edited to music by Squarepusher, My Fucking Sound, from the album Go Plastic and a piece called Mutilation Colony, which was written especially for the short and was released on the EP Do You Know Squarepusher.

Cunningham has directed a handful of commercials for companies including Gucci, PlayStation, Levis, Telecom Italia, Nissan and Orange.

In 2004/2005, Cunningham took a sabbatical from filmmaking to learn about music production and recording and to develop his own music projects.

In December 2007 Cunningham produced two tracks, "Three Decades" and "Primary Colours", for "Primary Colours", the second album by The Horrors. In the summer of 2008, due to scheduling conflicts with his feature film script writing he could not work on the rest of the album which was subsequently recorded by Geoff Barrow from Portishead.

In 2008, he produced and arranged a new version of 'I Feel Love' for the Gucci commercial that he also directed. He travelled to Nashville to work with Donna Summer to record a brand new vocal for it.

In 2005, Cunningham played a 45-minute audio visual piece performed live in Tokyo and Osaka in front of 30,000+ fans over the two nights at Japan’s premier electronic music event Electraglide. These performances evolved into "Chris Cunningham Live", a 55 minute long performance piece combining original and remixed music and film. It features remixed, unreleased and brand new videos and music dynamically edited together into a new live piece spread over three screens. The sound accompanying these images includes Cunningham’s first publicly performed compositions interspersed with his remixes of other artist’s work. "Chris Cunningham Live" debuted as one of the headline attractions at Warp 20 in Paris on 8 May 2009 with other performances scheduled at festivals in UK, and a number of European cities later in the year. "Chris Cunningham Live" continued in June 2011, with performances in London, Barcelona, and Sydney, Australia.

Cunningham has created photography and cover artwork for various people including Björk's "All Is Full of Love", Aphex Twin's "Windowlicker" and "Come to Daddy".

In 2008, Cunningham produced a fashion shoot for "Dazed & Confused" using Grace Jones as a model to create "Nubian versions" of Rubber Johnny. In an interview for BBC's "The Culture Show", it was suggested that the collaboration may expand into a video project. In regards to the collaboration, Cunningham stated "For me, Grace has the strongest iconography of any artist in music. She’s definitely the most inspiring person I’ve worked with so far.”

In November 2008, Cunningham followed on with another photoshoot for "Vice Magazine".

In 2000, Cunningham and cyberpunk author William Gibson began work on the script for Gibson's 1984 novel "Neuromancer". However, because "Neuromancer" was due to be a big budget studio film, it is rumoured that Cunningham pulled out due to being a first time director without final cut approval. He also felt that too much of the original book's ideas had been cannibalised by other recent films.

On 18 November 2004, in the FAQ on the William Gibson Board, Gibson was asked:

In an August 1999 "Spike Magazine" interview, Gibson stated "He (Chris) was brought to my attention by someone else. We were told, third-hand, that he was extremely wary of the Hollywood process, and wouldn't return calls. But someone else told us that "Neuromancer" had been his "Wind In The Willows", that he'd read it when he was a kid. I went to London and we met." Gibson is also quoted in the article as saying "Chris is my own 100 per cent personal choice...My only choice. The only person I've met who I thought might have a hope in hell of doing it right. I went back to see him in London just after he'd finished the Bjork video, and I sat on a couch beside this dead sex little Bjork robot, except it was wearing Aphex Twin's head. We talked."

It is rumoured that the character of Damien Pease in Gibson's 2003 novel "Pattern Recognition" was based on Cunningham, with the character's apartment featuring a female robot which had appeared in one of Cunningham's videos.

Development funding was in place for Cunningham to direct and co-write his first feature film for Warp Films, to whom he was at the time committed "for all future full-length film projects." He has since left Warp Films to set up his own production company 'CC Co' to produce his films independently.

Cunningham was married to Warpaint's bassist Jenny Lee Lindberg. They are currently no longer together. 

The video collection "The Work of Director Chris Cunningham" was released in November 2004 as part of the Directors Label set. This DVD includes selected highlights from 1995–2000.



</doc>
<doc id="7401" url="https://en.wikipedia.org/wiki?curid=7401" title="Centaur">
Centaur

A centaur (; , "Kéntauros", ), or occasionally hippocentaur, is a mythological creature with the upper body of a human and the lower body and legs of a horse.

The centaurs were usually said to have been born of Ixion and Nephele (the cloud made in the image of Hera). Another version, however, makes them children of a certain Centaurus, who mated with the Magnesian mares. This Centaurus was either himself the son of Ixion and Nephele (inserting an additional generation) or of Apollo and Stilbe, daughter of the river god Peneus. In the later version of the story his twin brother was Lapithes, ancestor of the Lapiths, thus making the two warring peoples cousins.

Centaurs were said to have inhabited the region of Magnesia and Mount Pelion in Thessaly, the Foloi oak forest in Elis, and the Malean peninsula in southern Laconia.

Another tribe of centaurs was said to have lived on Cyprus. According to Nonnus, they were fathered by Zeus, who, in frustration after Aphrodite had eluded him, spilled his seed on the ground of that land. Unlike those of mainland Greece, the Cyprian centaurs were horned.

There were also the Lamian Pheres, twelve rustic daimones of the Lamos river. They were set by Zeus to guard the infant Dionysos, protecting him from the machinations of Hera but the enraged goddess transformed them into ox-horned Centaurs. The Lamian Pheres later accompanied Dionysos in his campaign against the Indians.

Centaurs subsequently featured in Roman mythology, and were familiar figures in the medieval bestiary. They remain a staple of modern fantastic literature. The centaur's half-human, half-horse composition has led many writers to treat them as liminal beings, caught between the two natures, embodied in contrasted myths, both as the embodiment of untamed nature, as in their battle with the Lapiths (their kin), or conversely as teachers, like Chiron.

The Centaurs are best known for their fight with the Lapiths, which was caused by their attempt to carry off Hippodamia and the rest of the Lapith women on the day of Hippodamia's marriage to Pirithous, king of the Lapithae, himself the son of Ixion. The strife among these cousins is a metaphor for the conflict between the lower appetites and civilized behavior in humankind. Theseus, a hero and founder of cities, who happened to be present, threw the balance in favour of the right order of things, and assisted Pirithous. The Centaurs were driven off or destroyed. Another Lapith hero, Caeneus, who was invulnerable to weapons, was beaten into the earth by Centaurs wielding rocks and the branches of trees. Centaurs are thought of in many Greek myths as wild as untamed horses. Like the Titanomachy, the defeat of the Titans by the Olympian gods, the contests with the Centaurs typify the struggle between civilization and barbarism.

The Centauromachy is most famously portrayed in the Parthenon metopes by Phidias and in a Renaissance-era sculpture by Michelangelo.

The tentative identification of two fragmentary Mycenaean terracotta figures as centaurs, among the extensive Mycenaean pottery found at Ugarit, suggests a Bronze Age origin for these creatures of myth. A painted terracotta centaur was found in the "Hero's tomb" at Lefkandi, and by the Geometric period, centaurs figure among the first representational figures painted on Greek pottery. An often-published Geometric period bronze of a warrior face-to-face with a centaur is at the Metropolitan Museum of Art.

In Greek art of the Archaic period, centaurs are depicted in three different forms. Some centaurs are depicted with a human torso attached to the body of a horse at the withers, where the horse's neck would be; this form, designated "Class A" by Professor Baur, later became standard. "Class B" centaurs are depicted with a human body and legs, joined at the waist with the hindquarters of a horse; in some cases centaurs of both types appear together. A third type, designated "Class C", depicts centaurs with human forelegs terminating in hooves. Baur describes this as an apparent development of Aeolic art, which never became particularly widespread. At a later period, paintings on some amphoras depict winged centaurs.

Centaurs were also frequently depicted in Roman art. A particularly famous example is the pair of centaurs drawing the chariot of Constantine the Great and his family, in the Great Cameo of Constantine ("circa" AD 314–16), which embodies wholly pagan imagery, and contrasts sharply with the popular image of Constantine as the patron of early Christianity.

The most common theory holds that the idea of centaurs came from the first reaction of a non-riding culture, as in the Minoan Aegean world, to nomads who were mounted on horses. The theory suggests that such riders would appear as half-man, half-animal (Bernal Díaz del Castillo reported that the Aztecs had this misapprehension about Spanish cavalrymen). Horse taming and horseback culture arose first in the southern steppe grasslands of Central Asia, perhaps approximately in modern Kazakhstan.

The Lapith tribe of Thessaly, who were the kinsmen of the Centaurs in myth, were described as the inventors of horse-back riding by Greek writers. The Thessalian tribes also claimed their horse breeds were descended from the centaurs.
Of the various Classical Greek authors who mentioned centaurs, Pindar was the first who describes undoubtedly a combined monster. Previous authors (Homer) tend to use words such as "pheres" (cf. "theres", "beasts") that could also mean ordinary savage men riding ordinary horses, though Homer does specifically refer to a centaur ("kentauros") in the Odyssey Contemporaneous representations of hybrid centaurs can be found in archaic Greek art.

Lucretius in his first century BC philosophical poem "On the Nature of Things" denied the existence of centaurs based on their differing rate of growth. He states that at the age of three years horses are in the prime of their life while, at three humans are still little more than babies, making hybrid animals impossible.

Robert Graves (relying on the work of Georges Dumézil, who argued for tracing the centaurs back to the Indian gandharva), speculated that the centaurs were a dimly remembered, pre-Hellenic fraternal earth cult who had the horse as a totem. A similar theory was incorporated into Mary Renault's "The Bull from the Sea." Kinnaras, another half-man half-horse mythical creature from the Indian mythology, appeared in various ancient texts, arts as well as sculptures from all around India. It is shown as a horse with the torso of a man in place of where the horse's head has to be, that is similar to a Greek centaur.

The Greek word "kentauros" is generally regarded as of obscure origin. The etymology from "ken – tauros", "piercing bull-stickers" was a euhemerist suggestion in Palaephatus' rationalizing text on Greek mythology, "On Incredible Tales" (Περὶ ἀπίστων): mounted archers from a village called "Nephele" eliminating a herd of bulls that were the scourge of Ixion's kingdom. Another possible related etymology can be "bull-slayer".

In a popular legend associated with Pazhaya Sreekanteswaram Temple in Thiruvananthapuram, the curse of a saintly Brahmin transformed a handsome Yadava prince into a creature having a horse's body with the prince's head, arms and torso in place of the head and neck of the horse.

Though female centaurs, called centaurides or centauresses, are not mentioned in early Greek literature and art, they do appear occasionally in later antiquity. A Macedonian mosaic of the 4th century BC is one of the earliest examples of the centauress in art. Ovid also mentions a centauress named Hylonome who committed suicide when her husband Cyllarus was killed in the war with the Lapiths.

Centaurs preserved a Dionysian connection in the 12th century Romanesque carved capitals of Mozac Abbey in the Auvergne, where other capitals depict harvesters, boys riding goats (a further Dionysiac theme) and griffins guarding the chalice that held the wine.

Centaurs are shown on a number of Pictish carved stones from north-east Scotland, erected in the 8th–9th centuries AD (e.g., at Meigle, Perthshire). Though outside the limits of the Roman Empire, these depictions appear to be derived from Classical prototypes.

Jerome's version of the "Life" of St Anthony the Great, the hermit monk of Egypt, written by Athanasius of Alexandria, was widely disseminated in the Middle Ages; it relates Anthony's encounter with a centaur, who challenged the saint but was forced to admit that the old gods had been overthrown. The episode was often depicted; in "The Meeting of St Anthony Abbot and St Paul the Hermit" by Stefano di Giovanni called "Sassetta", of two episodic depictions in a single panel of the hermit Anthony's travel to greet the hermit Paul, one is his encounter along the pathway with the demonic figure of a centaur in a wood.

A centaur-like half-human half-equine creature called "Polkan" appeared in Russian folk art, and lubok prints of the 17th–19th centuries. Polkan is originally based on "Pulicane", a half-dog from Andrea da Barberino's poem "I Reali di Francia", which was once popular in the Slavonic world in prosaic translations.

The John C. Hodges library at The University of Tennessee hosts a permanent exhibit of a "Centaur from Volos", in its library. The exhibit, made by sculptor Bill Willers, by combining a study human skeleton with the skeleton of a Shetland pony is entitled "Do you believe in Centaurs?" and was meant to mislead students in order to make them more critically aware, according to the exhibitors.

Another exhibit by Willers is now on long-term display at the International Wildlife Museum in Tucson, Arizona. The full-mount skeleton of a Centaur, built by Skulls Unlimited International, is on display, along with several other fabled creatures, including the Cyclops, Unicorn and Griffin.

C.S. Lewis' "The Chronicles of Narnia" series depicts centaurs as the wisest and noblest of creatures. Narnian Centaurs are gifted at stargazing, prophecy, healing, and warfare, a fierce and valiant race always faithful to the High King Aslan the Lion. Lewis generally used the species to inspire awe in his readers.

In J.K. Rowling's "Harry Potter" series, centaurs live in the Forbidden Forest close to Hogwarts, preferring to avoid contact with humans. They live in societies called herds and are skilled at archery, healing and astrology, but like in the original myths, they are known to have some wild and barbarous tendencies. Although film depictions include very animalistic facial features, the reaction of the Hogwarts girls to Firenze suggests a more classical appearance.

With the exception of Chiron, the centaurs in Rick Riordan's "Percy Jackson & the Olympians" are seen as wild party-goers who use a lot of American slang. Chiron retains his mythological role as a trainer of heroes and is skilled in archery. In Riordan's subsequent series, "Heroes of Olympus", another group of centaurs are depicted with more animalistic features (such as horns) and appear as villains, serving the Gigantes.

Philip Jose Farmer's "World of Tiers" series (1965) includes centaurs, called Half-Horses or Hoi Kentauroi. His creations address several of the metabolic problems of such creatures—how could the human mouth and nose intake sufficient air to sustain both itself and the horse body and, similarly, how could the human ingest sufficient food to sustain both parts.

Brandon Mull's "Fablehaven" series features Centaurs that live in an area called Grunhold. The Centaurs are portrayed as a proud, elitist group of beings that consider themselves superior to all other creatures. The fourth book also has a variation on the species called an Alcetaur, which is part man, part moose.

Centaur appears in the novel by John Updike (The Centaur, 1963). The author depicts a rural Pennsylvanian town as seen through the optics of the myth of Centaur. An unknown and marginalized local school teacher, just like the mythological Chiron did for Prometheus, gave up his life for the future of his son who had chosen to be an independent artist in New York.

Other hybrid creatures appear in Greek mythology, always with some liminal connection that links Hellenic culture with archaic or non-Hellenic cultures:
Also,




</doc>
<doc id="7403" url="https://en.wikipedia.org/wiki?curid=7403" title="Chemotaxis">
Chemotaxis

Chemotaxis (from "chemo-" + "taxis") is the movement of an organism in response to a chemical stimulus. Somatic cells, bacteria, and other single-cell or multicellular organisms direct their movements according to certain chemicals in their environment. This is important for bacteria to find food (e.g., glucose) by swimming toward the highest concentration of food molecules, or to flee from poisons (e.g., phenol). In multicellular organisms, chemotaxis is critical to early development (e.g., movement of sperm towards the egg during fertilization) and subsequent phases of development (e.g., migration of neurons or lymphocytes) as well as in normal function and health (e.g., migration of leukocytes during injury or infection). In addition, it has been recognized that mechanisms that allow chemotaxis in animals can be subverted during cancer metastasis.

"Positive" chemotaxis occurs if the movement is toward a higher concentration of the chemical in question; "negative" chemotaxis if the movement is in the opposite direction. Chemically prompted kinesis (randomly directed or nondirectional) can be called chemokinesis.

Although migration of cells was detected from the early days of the development of microscopy by Leeuwenhoek, a Caltech lecture regarding chemotaxis propounds that 'erudite description of chemotaxis was only first made by T. W. Engelmann (1881) and W. F. Pfeffer (1884) in bacteria, and H. S. Jennings (1906) in ciliates'. The Nobel Prize laureate I. Metchnikoff also contributed to the study of the field during 1882 to 1886, with investigations of the process as an initial step of phagocytosis. The significance of chemotaxis in biology and clinical pathology was widely accepted in the 1930s, and the most fundamental definitions underlying the phenomenon were drafted by this time. The most important aspects in quality control of chemotaxis assays were described by H. Harris in the 1950s. In the 1960s and 1970s, the revolution of modern cell biology and biochemistry provided a series of novel techniques that became available to investigate the migratory responder cells and subcellular fractions responsible for chemotactic activity. The availability of this technology led to the discovery of C5a, a major chemotactic factor involved in acute inflammation. The pioneering works of J. Adler represented a significant turning point in understanding the whole process of intracellular signal transduction of bacteria.

Some bacteria, such as "E. coli", have several flagella per cell (4–10 typically). These can rotate in two ways:
The directions of rotation are given for an observer outside the cell looking down the flagella toward the cell.

The overall movement of a bacterium is the result of alternating tumble and swim phases. If one watches a bacterium swimming in a uniform environment, its movement will look like a random walk with relatively straight swims interrupted by random tumbles that reorient the bacterium. Bacteria such as "E. coli" are unable to choose the direction in which they swim, and are unable to swim in a straight line for more than a few seconds due to rotational diffusion; in other words, bacteria "forget" the direction in which they are going. By repeatedly evaluating their course, and adjusting if they are moving in the wrong direction, bacteria can direct their motion to find favorable locations with high concentrations of attractants (usually food) and avoid repellents (usually poisons).

In the presence of a chemical gradient bacteria will chemotax, or direct their overall motion based on the gradient. If the bacterium senses that it is moving in the correct direction (toward attractant/away from repellent), it will keep swimming in a straight line for a longer time before tumbling; however, if it is moving in the wrong direction, it will tumble sooner and try a new direction at random. In other words, bacteria like "E. coli" use temporal sensing to decide whether their situation is improving or not, and in this way, find the location with the highest concentration of attractant (usually the source) quite well. Even under very high concentrations, it can still distinguish very small differences in concentration, and fleeing from a repellent works with the same efficiency.

This biased random walk is a result of simply choosing between two methods of random movement; namely tumbling and straight swimming. In fact, chemotactic responses such as "forgetting" direction and "choosing" movements resemble the decision-making abilities of higher life-forms with brains that process sensory data.

The helical nature of the individual flagellar filament is critical for this movement to occur, and the protein that makes up the flagellar filament, flagellin, is quite similar among all flagellated bacteria. Vertebrates seem to have taken advantage of this fact by possessing an immune receptor (TLR5) designed to recognize this conserved protein.

As in many instances in biology, there are bacteria that do not follow this rule. Many bacteria, such as "Vibrio", are monoflagellated and have a single flagellum at one pole of the cell. Their method of chemotaxis is different. Others possess a single flagellum that is kept inside the cell wall. These bacteria move by spinning the whole cell, which is shaped like a corkscrew.

Chemical gradients are sensed through multiple transmembrane receptors, called methyl-accepting chemotaxis proteins (MCPs), which vary in the molecules that they detect. These receptors may bind attractants or repellents directly or indirectly through interaction with proteins of periplasmatic space. The signals from these receptors are transmitted across the plasma membrane into the cytosol, where "Che proteins" are activated. The Che proteins alter the tumbling frequency, and alter the receptors.

The proteins CheW and CheA bind to the receptor. The absence of receptor activation results in autophosphorylation in the histidine kinase, CheA, at a single highly conserved histidine residue. CheA, in turn, transfers phosphoryl groups to conserved aspartate residues in the response regulators CheB and CheY; CheA is a histidine kinase and it does not actively transfer the phosphoryl group, rather, the response regulator CheB takes the phosphoryl group from CheA. This mechanism of signal transduction is called a two-component system, and it is a common form of signal transduction in bacteria. CheY induces tumbling by interacting with the flagellar switch protein FliM, inducing a change from counter-clockwise to clockwise rotation of the flagellum. Change in the rotation state of a single flagellum can disrupt the entire flagella bundle and cause a tumble.

CheB, when activated by CheA, acts as a methylesterase, removing methyl groups from glutamate residues on the cytosolic side of the receptor; it works antagonistically with CheR, a methyltransferase, which adds methyl residues to the same glutamate residues. If the level of an attractant remains high, the level of phosphorylation of CheA (and, therefore, CheY and CheB) will remain low, the cell will swim smoothly, and the level of methylation of the MCPs will increase (because CheB-P is not present to demethylate). The MCPs no longer respond to the attractant when they are fully methylated; therefore, even though the level of attractant might remain high, the level of CheA-P (and CheB-P) increases and the cell begins to tumble. The MCPs can be demethylated by CheB-P, and, when this happens, the receptors can once again respond to attractants. The situation is the opposite with regard to repellents: fully methylated MCPs respond best to repellents, while least-methylated MCPs respond worst to repellents. This regulation allows the bacterium to 'remember' chemical concentrations from the recent past, a few seconds, and compare them to those it is currently experiencing, thus 'know' whether it is traveling up or down a gradient. Although the methylation system accounts for the wide range of sensitivity that bacteria have to chemical gradients, other mechanisms are involved in increasing the absolute value of the sensitivity on a given background. Well-established examples are the ultra-sensitive response of the motor to the CheY-P signal, and the clustering of chemoreceptors.

Chemoattractants and chemorepellents are inorganic or organic substances possessing chemotaxis-inducer effect in motile cells. These chemotactic ligands create chemical concentration gradients that organisms, prokaryotic and eukaryotic, move toward or away from, respectively.

Effects of chemoattractants are elicited via chemoreceptors such as methyl-accepting chemotaxis proteins (MCP). MCPs in E.coli include Tar, Tsr, Trg and Tap. Chemoattracttants to Trg include ribose and galactose with phenol as a chemorepellent. Tap and Tsr recognize dipeptides and serine as chemoattractants, respectively.

Chemoattractants or chemorepellents bind MCPs at its extracellular domain; an intracellular signaling domain relays the changes in concentration of these chemotactic ligands to downstream proteins like that of CheA which then relays this signal to flagellar motors via phosphorylated CheY (CheY-P). CheY-P can then control flagellar rotation influencing the direction of cell motility.

For "E.coli", "S. meliloti", and "R. spheroids," the binding of chemoattractants to MCPs inhibit CheA and therefore CheY-P activity, resulting in smooth runs, but for "B. substilis", CheA activity increases. Methylation events in "E.coli" cause MCPs to have lower affinity to chemoattractants which causes increased activity of CheA and CheY-P resulting in tumbles. In this way cells are able to adapt to the immediate chemoattractant concentration and detect further changes to modulate cell motility.

Chemoattractants in eukaryotes are well characterized for immune cells. Formyl peptides, such as N-formylmethioninyl, attract leukocytes such as neutrophils and macrophages, causing movement toward infection sites. Non-acylated methioninyl peptides do not act as chemoattractants to neutrophils and macrophages. Leukocytes also move toward chemoattractants C5a, a complement component, and pathogen-specific ligands on bacteria.

Mechanisms concerning chemorepellents are less known than chemoattractants. Although chemorepellents work to confer an avoidance response in organisms, "Tetrahymena thermophila" adapt to a chemorepellent, Netrin-1 peptide, within 10 minutes of exposure; however, exposure to chemorepellents such as GTP, PACAP-38, and nociceptin show no such adaptations. GTP and ATP are chemorepellents in micro-molar concentrations to both "Tetrahymena" and "Paramecium". These organisms avoid these molecules by producing avoiding reactions to re-orient themselves away from the gradient.

The mechanism of chemotaxis that eukaryotic cells employ is quite different from that in bacteria; however, sensing of chemical gradients is still a crucial step in the process. Due to their small size, prokaryotes cannot directly detect a concentration gradient. Instead, prokaryotes sense their environments temporally, constantly swimming and redirecting themselves each time they sense a change in the gradient.

Eukaryotic cells are much larger than prokaryotes and have receptors embedded uniformly throughout the cell membrane. Eukaryotic chemotaxis involves detecting a concentration gradient spatially by comparing the asymmetric activation of these receptors at the different ends of the cell. Activation of these receptors results in migration towards chemoattractants, or away from chemorepellants.

It has also been shown that both prokaryotic and eukaryotic cells are capable of chemotactic memory. In prokaryotes, this mechanism involves the methylation of receptors called methyl-accepting chemotaxis proteins (MCPs). This results in their desensitization and allows prokaryotes to "remember" and adapt to a chemical gradient. In contrast, chemotactic memory in eukaryotes can be explained by the Local Excitation Global Inhibition (LEGI) model. LEGI involves the balance between a fast excitation and delayed inhibition which controls downstream signaling such as Ras activation and PIP3 production.

Levels of receptors, intracellular signalling pathways and the effector mechanisms all represent diverse, eukaryotic-type components. In eukaryotic unicellular cells, amoeboid movement and cilium or the eukaryotic flagellum are the main effectors (e.g., Amoeba or Tetrahymena). Some eukaryotic cells of higher vertebrate origin, such as immune cells also move to where they need to be. Besides immune competent cells (granulocyte, monocyte, lymphocyte) a large group of cells—considered previously to be fixed into tissues—are also motile in special physiological (e.g., mast cell, fibroblast, endothelial cells) or pathological conditions (e.g., metastases). Chemotaxis has high significance in the early phases of embryogenesis as development of germ layers is guided by gradients of signal molecules.

Unlike motility in bacterial chemotaxis, the mechanism by which eukaryotic cells physically move is unclear. There appear to be mechanisms by which an external chemotactic gradient is sensed and turned into an intracellular PIP3 gradient, which results in a gradient and the activation of a signaling pathway, culminating in the polymerisation of actin filaments. The growing distal end of actin filaments develops connections with the internal surface of the plasma membrane via different sets of peptides and results in the formation of anteriorpseudopods and posterior uropods.
Cilia of eukaryotic cells can also produce chemotaxis; in this case, it is mainly a Ca-dependent induction of the microtubular system of the basal body and the beat of the 9+2 microtubules within cilia. The orchestrated beating of hundreds of cilia is synchronized by a submembranous system built between basal bodies.
The details of the signaling pathways are still not totally clear.

Chemotaxis refers to the directional migration of cells in response to chemical gradients; several variations of chemical-induced migration exist as listed below. 

In general, eukaryotic cells sense the presence of chemotactic stimuli through the use of 7-transmembrane (or serpentine) heterotrimeric G-protein-coupled receptors, a class representing a significant portion of the genome. Some members of this gene superfamily are used in eyesight (rhodopsins) as well as in olfaction (smelling). The main classes of chemotaxis receptors are triggered by:
However, induction of a wide set of membrane receptors (e.g., cyclic nucleotides, amino acids, insulin, vasoactive peptides) also elicit migration of the cell.

While some chemotaxis receptors are expressed in the surface membrane with long-term characteristics, as they are determined genetically, others have short-term dynamics, as they are assembled "ad hoc" in the presence of the ligand. The diverse features of the chemotaxis receptors and ligands allows for the possibility of selecting chemotactic responder cells with a simple chemotaxis assay. By chemotactic selection, we can determine whether a still-uncharacterized molecule acts via the long- or the short-term receptor pathway. The term "chemotactic selection" is also used to designate a technique that separates eukaryotic or prokaryotic cells according to their chemotactic responsiveness to selector ligands.

The number of molecules capable of eliciting chemotactic responses is relatively high, and we can distinguish primary and secondary chemotactic molecules. The main groups of the primary ligands are as follows:

Chemotactic responses elicited by the ligand-receptor interactions are, in general, distinguished upon the optimal effective concentration(s) of the ligand. Nevertheless, correlation of the amplitude elicited and ratio of the responder cells compared to the total number are also characteristic features of the chemotactic signaling. Investigations of ligand families (e.g., amino acids or oligo peptides) proved that there is a fitting of ranges (amplitudes; number of responder cells) and chemotactic activities: Chemoattractant moiety is accompanied by wide ranges, whereas chemorepellent character by narrow ranges.

A changed migratory potential of cells has relatively high importance in the development of several clinical symptoms and syndromes.
Altered chemotactic activity of extracellular (e.g., Escherichia coli) or intracellular (e.g., Listeria monocytogenes) pathogens itself represents a significant clinical target. Modification of endogenous chemotactic ability of these microorganisms by pharmaceutical agents can decrease or inhibit the ratio of infections or spreading of infectious diseases.
Apart from infections, there are some other diseases wherein impaired chemotaxis is the primary etiological factor, as in Chédiak–Higashi syndrome, where giant intracellular vesicles inhibit normal migration of cells.

Several mathematical models of chemotaxis were developed depending on the type of

Although interactions of the factors listed above make the behavior of the solutions of mathematical models of chemotaxis rather complex, it is possible to describe the basic phenomenon of chemotaxis-driven motion in a straightforward way.
Indeed, let us denote with formula_1 the spatially non-uniform concentration of the chemo-attractant and with formula_2 its gradient. Then the chemotactic cellular flow (also called current) formula_3 that is generated by the chemotaxis is linked to the above gradient by the law: formula_4, where formula_5 is the spatial density of the cells and formula_6 is the so-called ’Chemotactic coefficient’. However, note that in many cases formula_6 is not constant: It is, instead, a decreasing function of the concentration of the chemo-attractant formula_8: formula_9.

Spatial ecology of soil microorganisms is a function of their chemotactic sensitivities towards substrate and fellow organisms. The chemotactic behavior of the bacteria was proven to lead to non-trivial population patterns even in the absence of environmental heterogeneities. The presence of structural pore scale heterogeneities has an extra impact on the emerging bacterial patterns.

A wide range of techniques is available to evaluate chemotactic activity of cells or the chemoattractant and chemorepellent character of ligands.
The basic requirements of the measurement are as follows:

Despite the fact that an ideal chemotaxis assay is still not available, there are several protocols and pieces of equipment that offer good correspondence with the conditions described above. The most commonly used are summarised in the table below:
"Chemical robots" that use artificial chemotaxis to navigate autonomously have been designed. Applications include targeted delivery of drugs in the body.



</doc>
<doc id="7406" url="https://en.wikipedia.org/wiki?curid=7406" title="Cheshire">
Cheshire

Cheshire ( , ; archaically the County Palatine of Chester) is a county in North West England, bordering Merseyside and Greater Manchester to the north, Derbyshire to the east, Staffordshire and Shropshire to the south and Flintshire, Wales to the west. Cheshire's county town is Chester; the largest town is Warrington.

Other major towns include Congleton, Crewe, Ellesmere Port, Macclesfield, Northwich, Runcorn, Widnes, Wilmslow, and Winsford. The county covers and has a population of around 1 million. It is mostly rural, with a number of small towns and villages supporting the agricultural and other industries which produce Cheshire cheese, salt, chemicals and silk.

Cheshire's name was originally derived from an early name for Chester, and was first recorded as "Legeceasterscir" in the "Anglo-Saxon Chronicle", meaning "the shire of the city of legions". Although the name first appears in 980, it is thought that the county was created by Edward the Elder around 920. In the Domesday Book, Chester was recorded as having the name "Cestrescir" (Chestershire), derived from the name for Chester at the time. A series of changes that occurred as English itself changed, together with some simplifications and elision, resulted in the name Cheshire, as it occurs today.

Because of the historically close links with the land bordering Cheshire to the west, which became modern Wales, there is a history of interaction between Cheshire and North Wales. The Domesday Book records Cheshire as having two complete Hundreds (Atiscross and Exestan) that later became the principal part of Flintshire. Additionally, another large portion of the Duddestan Hundred later became known as Maelor Saesneg when it was transferred to North Wales. For this and other reasons, the Welsh language name for Cheshire ("Swydd Gaerlleon") is sometimes used.

After the Norman conquest of 1066 by William I, dissent and resistance continued for many years after the invasion. In 1069 local resistance in Cheshire was finally put down using draconian measures as part of the Harrying of the North. The ferocity of the campaign against the English populace was enough to end all future resistance. Examples were made of major landowners such as Earl Edwin of Mercia, their properties confiscated and redistributed amongst Norman barons. William I made Cheshire a county palatine and gave Gerbod the Fleming the new title of Earl of Chester. When Gerbod returned to Normandy in about 1070, the king used his absence to declare the earldom forfeit and gave the title to Hugh d'Avranches (nicknamed Hugh Lupus, or "wolf"). Due to Cheshire's strategic location on Welsh Marches, the Earl had complete autonomous powers to rule on behalf of the king in the county palatine. The earldom was sufficiently independent from the kingdom of England that the 13th century Magna Carta did not apply to the shire of Chester, so the earl wrote up his own Chester Charter at the petition of his barons.

Cheshire in the "Domesday Book" (1086) is recorded as a much larger county than it is today. It included two hundreds, Atiscross and Exestan, that later became part of North Wales. At the time of the "Domesday Book", it also included as part of Duddestan Hundred the area of land later known as English Maelor (which used to be a detached part of Flintshire) in Wales. The area between the Mersey and Ribble (referred to in the Domesday Book as "Inter Ripam et Mersam") formed part of the returns for Cheshire. Although this has been interpreted to mean that at that time south Lancashire was part of Cheshire, more exhaustive research indicates that the boundary between Cheshire and what was to become Lancashire remained the River Mersey. With minor variations in spelling across sources, the complete list of hundreds of Cheshire at this time are: Atiscross, Bochelau, Chester, Dudestan, Exestan, Hamestan, Middlewich, Riseton, Roelau, Tunendune, Warmundestrou and Wilaveston.

Feudal baronies or baronies by tenure were granted by the Earl as forms of feudal land tenure within the palatinate in a similar way to which the king granted English feudal baronies within England proper. An example is the barony of Halton. One of Hugh d'Avranche's barons has been identified as Robert Nicholls, Baron of Halton and Montebourg.

In 1182 the land north of the Mersey became administered as part of the new county of Lancashire, thus resolving any uncertainty about the county in which the land "Inter Ripam et Mersam" was. Over the years, the ten hundreds consolidated and changed names to leave just seven—Broxton, Bucklow, Eddisbury, Macclesfield, Nantwich, Northwich and Wirral.

In 1397 the county had lands in the march of Wales added to its territory, and was promoted to the rank of principality. This was because of the support the men of the county had given to King Richard II, in particular by his standing armed force of about 500 men called the "Cheshire Guard". As a result, the King's title was changed to "King of England and France, Lord of Ireland, and Prince of Chester". No other English county has been honoured in this way, although it lost the distinction on Richard's fall in 1399.

Through the Local Government Act 1972, which came into effect on 1 April 1974, some areas in the north became part of the metropolitan counties of Greater Manchester and Merseyside. Stockport (previously a county borough), Altrincham, Hyde, Dukinfield and Stalybridge in the north-east became part of Greater Manchester. Much of the Wirral Peninsula in the north-west, including the county boroughs of Birkenhead and Wallasey, joined Merseyside as the Metropolitan Borough of Wirral. At the same time the Tintwistle Rural District was transferred to Derbyshire. The area of south Lancashire not included within either the Merseyside or Greater Manchester counties, including Widnes and the county borough of Warrington, was added to the new non-metropolitan county of Cheshire.

Halton and Warrington became unitary authorities independent of Cheshire County Council on 1 April 1998, but remain part of Cheshire for ceremonial purposes and also for fire and policing.

A referendum for a further local government reform connected with an elected regional assembly was planned for 2004, but was abandoned.

As part of the local government restructuring in April 2009, Cheshire County Council and the Cheshire districts were abolished and replaced by two new unitary authorities, Cheshire East and Cheshire West and Chester. The existing unitary authorities of Halton and Warrington were not affected by the change.

Prehistoric burial grounds have been discovered at The Bridestones, near Congleton (Neolithic) and Robin Hood's Tump, near Alpraham (Bronze Age). The remains of Iron Age hill forts are found on sandstone ridges at several locations in Cheshire. Examples include Maiden Castle on Bickerton Hill, Helsby Hillfort and Woodhouse Hillfort at Frodsham. The Roman fortress and walls of Chester, perhaps the earliest building works in Cheshire remaining above ground, are constructed from purple-grey sandstone.

The distinctive local red sandstone has been used for many monumental and ecclesiastical buildings throughout the county: for example, the medieval Beeston Castle, Chester Cathedral and numerous parish churches. Occasional residential and industrial buildings, such as Helsby railway station (1849), are also in this sandstone.

Many surviving buildings from the 15th to 17th centuries are timbered, particularly in the southern part of the county. Notable examples include the moated manor house Little Moreton Hall, dating from around 1450, and many commercial and residential buildings in Chester, Nantwich and surrounding villages.

Early brick buildings include Peover Hall near Macclesfield (1585), Tattenhall Hall (pre-1622), and the Pied Bull Hotel in Chester (17th century). From the 18th century, orange, red or brown brick became the predominant building material used in Cheshire, although earlier buildings are often faced or dressed with stone. Examples from the Victorian period onwards often employ distinctive brick detailing, such as brick patterning and ornate chimney stacks and gables. Notable examples include Arley Hall near Northwich, Willington Hall near Chester (both by Nantwich architect George Latham) and Overleigh Lodge, Chester. From the Victorian era, brick buildings often incorporate timberwork in a mock Tudor style, and this hybrid style has been used in some modern residential developments in the county. Industrial buildings, such as the Macclesfield silk mills (for example, Waters Green New Mill), are also usually in brick.

Cheshire covers a boulder clay plain separating the hills of North Wales and the Peak District (the area is also known as the Cheshire Gap). This was formed following the retreat of ice age glaciers which left the area dotted with kettle holes, locally referred to as meres. The bedrock of this region is almost entirely Triassic sandstone, outcrops of which have long been quarried, notably at Runcorn, providing the distinctive red stone for Liverpool Cathedral and Chester Cathedral.

The eastern half of the county is Upper Triassic Mercia Mudstone laid down with large salt deposits which were mined for hundreds of years around Winsford. Separating this area from Lower Triassic Sherwood Sandstone to the west is a prominent sandstone ridge known as the Mid Cheshire Ridge. A footpath, the Sandstone Trail, follows this ridge from Frodsham to Whitchurch passing Delamere Forest, Beeston Castle and earlier Iron Age forts.

The highest point in Cheshire is Shining Tor on the Derbyshire/Cheshire border between Macclesfield and Buxton, at above sea level. Before county boundary alterations in 1974, the county top was Black Hill () near Crowden in the far east of the historic county on the border with the West Riding of Yorkshire. Black Hill is now the highest point in West Yorkshire.

Cheshire contains portions of two green belt areas surrounding the large conurbations of Merseyside and Greater Manchester (North Cheshire Green Belt, part of the North West Green Belt) and Stoke-on-Trent (South Cheshire Green Belt, part of the Stoke-on-Trent Green Belt), these were first drawn up from the 1950s. Contained primarily within Cheshire East and Chester West & Chester, with small portions along the borders of the Halton and Warrington districts, towns and cities such as Chester, Macclesfield, Alsager, Congleton, Northwich, Ellesmere Port, Knutsford, Warrington, Poynton, Disley, Neston, Wilmslow, Runcorn, and Widnes are either surrounded wholly, partially enveloped by, or on the fringes of the belts. The North Cheshire Green Belt is contiguous with the Peak District Park boundary inside Cheshire.

Based on the Census of 2001, the overall population of Cheshire is 673,781, of which 51.3% of the population were male and 48.7% were female. Of those aged between 0–14 years, 51.5% were male and 48.4% were female; and of those aged over 75 years, 62.9% were female and 37.1% were male. This increased to 699,735 at the 2011 Census.

In 2001, the population density of Cheshire was 32 people per km², lower than the North West average of 42 people/km² and the England and Wales average of 38 people/km². Ellesmere Port and Neston had a greater urban density than the rest of the county with 92 people/km².

The population for 2021 is forecast to be 708,000.

In 2001, ethnic white groups accounted for 98% (662,794) of the population, and 10,994 (2%) in ethnic groups other than white.

Of the 2% in non-white ethnic groups:

Cheshire is a ceremonial county. This means that although there is no county-wide elected local council, Cheshire has a Lord Lieutenant and High Sheriff for ceremonial purposes under the Lieutenancies Act 1997.

Local government functions apart from the Police and Fire/Rescue services are carried out by four smaller unitary authorities: Cheshire East, Cheshire West and Chester, Halton, and Warrington. All four unitary authority areas have borough status.

Policing and fire and rescue services are still provided across the County as a whole. The Cheshire Fire Authority consist of members of the four councils, while governance of Cheshire Constabulary is performer by the elected Cheshire Police and Crime Commissioner.

From 1 April 1974 the area under the control of the county council was divided into eight local government districts; Chester, Congleton, Crewe and Nantwich, Ellesmere Port and Neston, Halton, Macclesfield, Vale Royal and Warrington. Halton (which includes the towns of Runcorn and Widnes) and Warrington became unitary authorities in 1998. The remaining districts and the county were abolished as part of local government restructuring on 1 April 2009. The Halton and Warrington boroughs were not affected by the 2009 restructuring.

On 25 July 2007, the Secretary of State Hazel Blears announced she was 'minded' to split Cheshire into two new unitary authorities, Cheshire West and Chester, and Cheshire East. She confirmed she had not changed her mind on 19 December 2007 and therefore the proposal to split two-tier Cheshire into two would proceed.

Cheshire County Council leader Paul Findlow, who attempted High Court legal action against the proposal, claimed that splitting Cheshire would only disrupt excellent services while increasing living costs for all. A widespread sentiment that this decision was taken by the European Union long ago has often been portrayed via angered letters from Cheshire residents to local papers. On 31 January 2008 "The Standard", Cheshire and district's newspaper, announced that the legal action had been dropped. Members against the proposal were advised that they may be unable to persuade the court that the decision of Hazel Blears was "manifestly absurd".

The Cheshire West and Chester unitary authority covers the area formerly occupied by the City of Chester and the boroughs of Ellesmere Port and Neston and Vale Royal; Cheshire East now covers the area formerly occupied by the boroughs of Congleton, Crewe and Nantwich, and Macclesfield. The changes were implemented on 1 April 2009.

Congleton Borough Council pursued an appeal against the judicial review it lost in October 2007. The appeal was dismissed on 4 March 2008.

The ceremonial county borders Merseyside, Greater Manchester, Derbyshire, Staffordshire and Shropshire in England along with Flintshire and Wrexham in Wales, arranged by compass directions as shown in the table. below. Cheshire also forms part of the North West England region.

In the 2001 Census, 81% of the population (542,413) identified themselves as Christian; 124,677 (19%) did not identify with any religion or did not answer the question; 5,665 (1%) identified themselves as belonging to other major world religions; and 1,033 belonged to other religions.

The boundary of the Church of England Diocese of Chester follows most closely the pre-1974 county boundary of Cheshire, so it includes all of Wirral, Stockport, and the Cheshire panhandle that included Tintwistle Rural District council area. In terms of Roman Catholic church administration, most of Cheshire falls into the Roman Catholic Diocese of Shrewsbury.

Cheshire has a diverse economy with significant sectors including agriculture, automotive, bio-technology, chemical, financial services, food and drink, ICT, and tourism. The county is famous for the production of Cheshire cheese, salt and silk. The county has seen a number of inventions and firsts in its history.

A mainly rural county, Cheshire has a high concentration of villages. Agriculture is generally based on the dairy trade, and cattle are the predominant livestock. Land use given to agriculture has fluctuated somewhat, and in 2005 totalled 1558 km² over 4,609 holdings. Based on holdings by EC farm type in 2005, 8.51 km² was allocated to dairy farming, with another 11.78 km² allocated to cattle and sheep.

The chemical industry in Cheshire was founded in Roman times, with the mining of salt in Middlewich and Northwich. Salt is still mined in the area by British Salt. The salt mining has led to a continued chemical industry around Northwich, with Brunner Mond based in the town. Other chemical companies, including Ineos (formerly ICI), have plants at Runcorn. The Essar Refinery (formerly Shell Stanlow Refinery) is at Ellesmere Port. The oil refinery has operated since 1924 and has a capacity of 12 million tonnes per year. 

Crewe was once the centre of the British railway industry, and remains a major railway junction. The Crewe railway works, built in 1840, employed 20,000 people at its peak, although the workforce is now less than 1,000. Crewe is also the home of Bentley cars. Also within Cheshire are manufacturing plants for Jaguar and Vauxhall Motors in Ellesmere Port. The county also has an aircraft industry, with the BAE Systems facility at Woodford Aerodrome, part of BAE System's Military Air Solutions division. The facility designed and constructed Avro Lancaster and Avro Vulcan bombers and the Hawker-Siddeley Nimrod. On the Cheshire border with Flintshire is the Broughton aircraft factory, more recently associated with Airbus.

Tourism in Cheshire from within the UK and overseas continues to perform strongly. Over 8 million nights of accommodation (both UK and overseas) and over 2.8 million visits to Cheshire were recorded during 2003.

At the start of 2003, there were 22,020 VAT-registered enterprises in Cheshire, an increase of 7% since 1998, many in the business services (31.9%) and wholesale/retail (21.7%) sectors. Between 2002 and 2003 the number of businesses grew in four sectors: public administration and other services (6.0%), hotels and restaurants (5.1%), construction (1.7%), and business services (1.0%). The county saw the largest proportional reduction between 2001 and 2002 in employment in the energy and water sector and there was also a significant reduction in the manufacturing sector. The largest growth during this period was in the other services and distribution, hotels and retail sectors.

Cheshire is considered to be an affluent county. However, towns such as Crewe have significant deprivation. The county's proximity to the cities of Manchester and Liverpool means counter urbanisation is common. Cheshire West has a fairly large proportion of residents who work in Liverpool and Manchester, while the town of Northwich and area of Cheshire East falls more within Manchester's sphere of influence.

All four local education authorities in Cheshire operate only comprehensive state school systems. When Altrincham, Sale and Bebington were moved from Cheshire to Trafford and Merseyside in 1974, they took some former Cheshire selective schools. Today, there are three universities based in the county, the University of Chester, the Crewe campus of Manchester Metropolitan University and the Chester campus of The University of Law.

Cheshire has one Football League team, Crewe Alexandra who play in League Two. Chester, phoenix club formed in 2010 after an ex-Football League club Chester City was dissolved competes in the National League along with other Cheshire side Macclesfield Town, who played in the Football League from 1997 till 2012. Northwich Victoria are also an ex-Football League team who were founder members of the Football League Division Two in 1892/1893 now represent Cheshire in the Northern Premier League along with Nantwich Town, Warrington Town and Witton Albion.

Warrington Wolves and the Widnes Vikings are the premier Rugby league teams in Cheshire and play in the Super League. There are also numerous junior clubs in the county, including Chester Gladiators. Cheshire County Cricket Club is one of the clubs that make up the Minor counties of English and Welsh cricket. Cheshire also is represented in the highest level basketball league in the UK, the BBL, by Cheshire Phoenix (formerly Cheshire Jets). Each May, Europe's largest motorcycle event, the Thundersprint, is held in Northwich.

The county has also been home to many notable sportsmen and athletes. Due to its proximity to both Manchester and Liverpool, many Premier League footballers have lived in Cheshire, including Dean Ashton, Seth Johnson, Michael Owen and Wayne Rooney. Other local athletes have included cricketer Ian Botham, marathon runner Paula Radcliffe, oarsman Matt Langridge, hurdler Shirley Strong, sailor Ben Ainslie, cyclist Sarah Storey and mountaineer George Mallory, who died in 1924 on Mount Everest. Cheshire has also produced a military hero in Norman Cyril Jones, a World War I flying ace who won the Distinguished Flying Cross.
The county has produced several notable popular musicians, including Gary Barlow (Take That, born and raised in Frodsham), Harry Styles (singer with One Direction, raised in Holmes Chapel), John Mayall (John Mayall & the Bluesbreakers), Ian Astbury (The Cult), Tim Burgess (Charlatans), Ian Curtis (Joy Division) and Hooton Tennis Club. Matthew Healy, lead singer of The 1975, met his three bandmates at Wilmslow High School in Wilmslow. Concert pianist Stephen Hough, singer Thea Gilmore and her producer husband Nigel Stonier also reside in Cheshire.

The county has also been home to several writers, including Hall Caine (1853–1931), popular romantic novelist and playwright; Alan Garner; Victorian novelist Elizabeth Gaskell, whose novel "Cranford" features her home town of Knutsford; and most famously Lewis Carroll, born and raised in Daresbury, hence the Cheshire Cat (a fictional cat popularised by Carroll in "Alice's Adventures in Wonderland" and known for its distinctive mischievous grin). Artists from the county include ceramic artist Emma Bossons and sculptor and photographer Andy Goldsworthy. Actors from Cheshire include Tim Curry; Daniel Craig, the 6th James Bond; Dame Wendy Hiller; and Lewis McGibbon, best known for his role in "Millions".

Local radio stations in the county include Dee 106.3, Heart and Gold for Chester and West Cheshire, Silk FM for the east of the county, Signal 1 and The Cat 107.9 for the south, Wire FM for Warrington and Wish FM, which covers Widnes. Cheshire is one of the only counties (along with County Durham, Dorset and Rutland) that does not have its own designated BBC Radio station. The majority of the county (south and east) are covered by BBC Radio Stoke, whilst BBC Radio Merseyside tends to cover the west. The BBC directs readers to Stoke and Staffordshire when Cheshire is selected on their website. The BBC covers the west with BBC Radio Merseyside, the north and east with BBC Radio Manchester and the south with BBC Radio Stoke. There were plans to launch BBC Radio Cheshire, but those were shelved in 2007 after a lower than expected BBC licence fee settlement.

The Royal Cheshire Show, an annual agricultural show, has taken place for the last 175 years and includes exhibitions, games and competitions.

As part of a 2002 marketing campaign, the plant conservation charity Plantlife chose the cuckooflower as the county flower. Previously, a sheaf of golden wheat was the county emblem, a reference to the Earl of Chester's arms in use from the 12th century.

The county is home to some of the most affluent areas of northern England, including Alderley Edge, Wilmslow, Prestbury, Tarporley and Knutsford, named in 2006 as the most expensive place to buy a house in the north of England. The former Cheshire town of Altrincham was in second place. The area is sometimes referred to as The Golden Triangle on account of the area in and around the aforementioned towns and villages.

The cities and towns in Cheshire are:

Some settlements which were historically part of the county now fall under the counties of Derbyshire, Merseyside and Greater Manchester:

Bus transport in Cheshire is provided by various operators. The major bus operator in the Cheshire area is Arriva North West. Other operators in Cheshire include Stagecoach Chester & Wirral, Halton Transport and Network Warrington.

There are also several operators based outside of Cheshire who either run services wholly within the area or services which start from outside the area. Companies include Arriva Buses Wales, BakerBus, High Peak, First Greater Manchester, GHA Coaches and Stagecoach Manchester.

Some services are run under contract to Cheshire West and Chester, Cheshire East, Borough of Halton and Warrington Councils.

The main railway line through the county is the West Coast Main Line. Many trains call at Crewe (in the south of the county) and Warrington Bank Quay (in the north of the county) en route to London and Scotland, as well as Runcorn on the Liverpool branch of the WCML.

The major interchanges are:


In the east of Cheshire, Macclesfield station is served by Virgin Trains and CrossCountry, on the Manchester-London line. Services from Manchester to the south coast frequently stop at Macclesfield.

Cheshire has of roads, including of the M6, M62, M53 and M56 motorways, with 23 interchanges and four service areas. The M6 motorway at the Thelwall Viaduct carries 140,000 vehicles every 24 hours.

The Cheshire canal system includes several canals originally used to transport the county's industrial products (mostly chemicals). Nowadays they are mainly used for tourist traffic. The Cheshire Ring is formed from the Rochdale, Ashton, Peak Forest, Macclesfield, Trent and Mersey and Bridgewater canals.

The Manchester Ship Canal is a wide, stretch of water opened in 1894. It consists of the rivers Irwell and Mersey made navigable to Manchester for seagoing ships leaving the Mersey estuary. The canal passes through the north of the county via Runcorn and Warrington.






</doc>
<doc id="7407" url="https://en.wikipedia.org/wiki?curid=7407" title="County town">
County town

A county town in Great Britain or Ireland is usually, but not always, the location of administrative or judicial functions within the county. The concept of a county town is ill-defined and unoffical. Following the establishment of County Councils in 1889, the administrative headquarters of the new authorities were usually located in the county town of each county. However, this was not always the case and the idea of a "county town" pre-dates the establishment of these councils. For example, Lancaster is the county town of Lancashire but the county council is located at Preston.

The county town was often where the county members of parliament were elected or where certain judicial functions were carried out, leading it to becoming established as the most important town in the county.

Some county towns are no longer situated within the administrative county. For example, Nottingham is administered by a unitary authority entirely separate from the rest of Nottinghamshire. Many county towns are classified as cities, but all are referred to as county towns regardless of whether city status is held or not. The term was also used historically in Jamaica.

This list shows county towns prior to the reforms of 1889.


This list shows county towns prior to the reforms of 1889.

Note – Despite the fact that Belfast is the capital of Northern Ireland, it is not the county town of any county. Greater Belfast straddles two counties ("Antrim" and "Down").

With the creation of elected county councils in 1889 the location of administrative headquarters in some cases moved away from the traditional county town. Furthermore, in 1965 and 1974 there were major boundary changes in England and Wales and administrative counties were replaced with new metropolitan and non-metropolitan counties. The boundaries underwent further alterations between 1995 and 1998 to create unitary authorities and some of the ancient counties and county towns were restored. (Note: not all headquarters are or were called County Halls or Shire Halls e.g.: Cumbria County Council's HQ up until 2016 was called "The Courts" and have since moved to Cumbria House.) Before 1974 many of the county halls were located in towns and cities that had the status of a county borough i.e.: a borough outside of the county council's jurisdiction.


The follow lists the location of the administration of each of the 31 local authorities in Ireland, with the 26 traditional counties.

Jamaica's three counties were established in 1758 to facilitate the holding of courts along the lines of the British county court system, with each county having a county town. The counties have no current administrative relevance.



</doc>
<doc id="7411" url="https://en.wikipedia.org/wiki?curid=7411" title="Constitution of Canada">
Constitution of Canada

The Constitution of Canada is the supreme law in Canada; the country's constitution is an amalgamation of codified acts and uncodified traditions and conventions. Canada is one of the oldest constitutional democracies in the world. The constitution outlines Canada's system of government, as well as the civil rights of all Canadian citizens and those in Canada.

The composition of the Constitution of Canada is defined in subsection 52(2) of the Constitution Act, 1982 as consisting of the Canada Act 1982 (including the Constitution Act, 1982), all acts and orders referred to in the schedule (including the Constitution Act, 1867, formerly the British North America Act, 1867), and any amendments to these documents. The Supreme Court of Canada has held that the list is not exhaustive and includes a number of pre-confederation acts and unwritten components as well. See list of Canadian constitutional documents for details.

The first semblance of a constitution for Canada was the Royal Proclamation of 1763. The act renamed the northeasterly portion of the former French province of New France as Province of Quebec, roughly coextensive with the southern third of contemporary Quebec. The proclamation, which established an appointed colonial government, was the de facto constitution of Quebec until 1774, when the British parliament passed the Quebec Act, which expanded the province's boundaries to the Ohio and Mississippi Rivers, which was one of the grievances listed in the United States Declaration of Independence. Significantly, the Quebec Act also replaced the French criminal law presumption of guilty until proven innocent with the English criminal law presumption of innocent until proven guilty; but the French code or civil law system was retained for non-criminal matters.
The Treaty of Paris of 1783 ended the American War of Independence and sent a wave of British loyalist refugees northward to Quebec and Nova Scotia. In 1784, the two provinces were divided; Nova Scotia was split into Nova Scotia, Cape Breton Island (rejoined to Nova Scotia in 1820), Prince Edward Island, and New Brunswick, while Quebec was split into Lower Canada (southern Quebec) and Upper Canada (southern through lower northern Ontario). The winter of 1837–38 saw rebellion in both of the Canadas, with the result they were rejoined as the Province of Canada in 1841. This was reversed by the British North America Act in 1867 which established the Dominion of Canada.

Initially, on 1 July 1867, there were four provinces in confederation as "One dominion under the name of Canada": Canada West (former Upper Canada, now Ontario), Canada East (former Lower Canada, now Quebec), Nova Scotia, and New Brunswick. Title to the Northwest Territories was transferred by the Hudson’s Bay Company in 1870 and the province of Manitoba (the first to be established by the Parliament of Canada) was in the same year the first created out of it. British Columbia joined confederation in 1871, followed by Prince Edward Island in 1873. The Yukon Territory was created by Parliament in 1898, followed by Alberta and Saskatchewan in 1905. The Dominion of Newfoundland, Britain's oldest colony in the Americas, joined Canada as a province in 1949. Nunavut was created in 1999.

An Imperial Conference in 1926 that included the leaders of all Dominions and representatives from India (which then included Burma, Bangladesh, and Pakistan), led to the eventual enactment of the Statute of Westminster 1931. The statute, an essential transitory step from the British Empire to the Commonwealth of Nations, provided that all existing Dominions became fully sovereign of the United Kingdom and all new Dominions would be fully sovereign upon the grant of Dominion status. Newfoundland never ratified the statute, so it was still subject to imperial authority when its entire system of government and economy collapsed in the mid-1930s. Canada did ratify the statute, but had requested an exception because the Canadian federal and provincial governments could not agree on an amending formula for the Canadian constitution. It would be another 50 years before this was achieved. In the interim, the British parliament periodically passed enabling acts with respect to amendments to Canada's constitution; this was never anything but a rubber stamp.

The patriation of the Canadian constitution was achieved in 1982 when the British parliament, with the assent of the Canadian parliament, passed the Canada Act, 1982, which included in its schedules the Constitution Act, 1982, the United Kingdom thus formally absolving itself of any remaining responsibility for, or jurisdiction over, Canada. In a formal ceremony on Parliament Hill in Ottawa, Queen Elizabeth II proclaimed both acts as law on 17 April 1982. Constitution Act, 1982, included the Canadian Charter of Rights and Freedoms. Prior to the charter, there were various statutes which protected an assortment of civil rights and obligations, but nothing was enshrined in the constitution until 1982. The charter has thus placed a strong focus upon individual and collective rights of the people of Canada.

Enactment of the Charter of Rights and Freedoms has also fundamentally changed much of Canadian constitutional law. The act also codified many previously oral constitutional conventions and made amendment of the constitution significantly more difficult. Previously, the Canadian federal constitution could be amended by solitary act of the Canadian or British parliaments, by formal or informal agreement between the federal and provincial governments, or even simply by adoption as ordinary custom of an oral convention or unwritten tradition that was perceived to be the best way to do something. Since the act, amendments must now conform to certain specified provisions in the written portion of the Canadian constitution.

This was an Act of the British parliament, originally called the British North America Act 1867. It outlined Canada's system of government, which combines Britain's Westminster model of parliamentary government with division of sovereignty (federalism). Although it is the first of 20 British North America Acts, it is still the most famous of these and is understood to be the document of Canadian Confederation. With the patriation of the Constitution in 1982, this Act was renamed "Constitution Act, 1867". In recent years, the 1867 document has mainly served as the basis on which the division of powers between the provinces and federal government have been analyzed.

Endorsed by all provincial governments except that of Quebec (led by René Lévesque), this was the formal Act of Parliament that effected Canada's full political independence from the United Kingdom. Part V of this act established an amending formula for the Canadian constitution, the lack of which (due to more than 50 years of disagreement between the federal and provincial governments) was the only reason Canada's constitutional amendments still required approval by the British parliament after enactment of the Statute of Westminster in 1931.

The Act was enacted as a schedule to the Canada Act 1982, a British Act of Parliament which was introduced at the request of a joint address to the Queen by the Senate and House of Commons of Canada. As a bilingual act of parliament, the Canada Act 1982 has the distinction of being the only legislation in French that has been passed by an English or British parliament since Norman French (Law French) ceased to be the language of government in England. In addition to enacting the Constitution Act, 1982, the Canada Act 1982 provides that no further British Acts of Parliament will apply to Canada as part of its law, finalizing Canada's legislative independence.

As noted above, this is Part I of the Constitution Act, 1982. The Charter is the constitutional guarantee of the civil rights and liberties of every citizen in Canada, such as freedom of expression, of religion, and of mobility. Part II addresses the rights of Aboriginal peoples in Canada.

It is written in plain language in order to ensure accessibility to the average citizen. It only applies to government and government actions with the intention to prevent government from creating laws that are unconstitutional.

Instead of the usual parliamentary procedure, that includes the monarch's formal Royal Assent for enacting legislation, amendments to the Constitution Act, 1982 must be done in accordance with Part V of the Constitution Act, 1982, which provides for five different amending formulae. Amendments can be brought forward under section 46(1) by any province or either level of the federal government. The general formula is set out in section 38(1), known as the "7/50 formula", requires: (a) assent from both the House of Commons and the Senate; (b) the approval of two-thirds of the provincial legislatures (at least seven provinces) representing at least 50% of the population (effectively, this would include at least Quebec or Ontario, as they are the most populous provinces). This formula specifically applies to amendments related to the proportionate representation in Parliament, powers, selection, and composition of the Senate, the Supreme Court and the addition of provinces or territories.

The other amendment formulae are for exceptional cases as provided by in the act. In the case of an amendment related to the Office of the Queen, the use of either official language (subject to section 43), the amending formula itself, or the composition of the Supreme Court, the amendment must be adopted by unanimous consent of all the provinces in accordance with section 41. In the case of an amendment related to provincial boundaries or the use of an official language within a province alone, the amendment must be passed by the legislatures affected by the amendment (section 43). In the case of an amendment that affects the federal government only, the amendment does not need approval of the provinces (section 44). The same applies to amendments affecting the provincial government alone (section 45).

In 1983, Peter Greyson, an art student, entered Ottawa's National Archives (known today as Library and Archives Canada) and poured red paint mixed with glue over a copy of the proclamation of the 1982 constitutional amendment. He said he was displeased with the federal government's decision to allow United States missile testing in Canada and had wanted to "graphically illustrate to Canadians" how wrong he believed the government to be. Greyson was charged with public mischief and sentenced to 89 days in jail, 100 hours of community work, and two years of probation. A grapefruit-sized stain remains on the original document; restoration specialists opted to leave most of the paint intact, fearing that removal attempts would only cause further damage.

Canada's constitution has roots going back to the thirteenth century, and include England's Magna Carta and the first English parliament of 1275. It is one of the oldest working constitutions in the world (others are: UK, USA, Sweden, Norway, Switzerland, Denmark). Canada's constitution is composed of several individual statutes. There are three general methods by which a statute can become entrenched in the Constitution:

The existence of an unwritten constitution was reaffirmed in 1998 by the Supreme Court in "Reference re Secession of Quebec".

"The Constitution is more than a written text. It embraces the entire global system of rules and principles which govern the exercise of constitutional authority. A superficial reading of selected provisions of the written constitutional enactment, without more, may be misleading."
In practice, there have been three sources of unwritten constitutional law:





</doc>
<doc id="7424" url="https://en.wikipedia.org/wiki?curid=7424" title="Crochet">
Crochet

"Not to be confused with Crotchet, the common name for a Quarter note in music."

Crochet (; ) is a process of creating fabric by interlocking loops of yarn, thread, or strands of other materials using a crochet hook. The name is derived from the French term "crochet", meaning 'small hook'. These are made of materials such as metal, wood, or plastic and are manufactured commercially and produced in artisan workshops. The salient difference between crochet and knitting, beyond the implements used for their production, is that each stitch in crochet is completed before proceeding with the next one, while knitting keeps a large number of stitches open at a time. (Variant forms such as Tunisian crochet and broomstick lace keep multiple crochet stitches open at a time.)

The word crochet is derived from the Old French "crochet", a diminutive of "croche", in turn from the Germanic "croc", both meaning "hook". It was used in 17th-century French lace making, "crochetage" designating a stitch used to join separate pieces of lace, and "crochet" subsequently designating both a specific type of fabric and the hooked needle used to produce it. Although the fabric is not known to be crochet in the present sense, a genealogical relationship between the techniques sharing that name appears likely.

Knitted textiles survive from early periods, but the first substantive evidence of crocheted fabric relates to its appearance in Europe during the 19th century. Earlier work identified as crochet was commonly made by nålebinding, a separate looped yarn technique.

The first known published instructions for crochet explicitly using that term to designate the craft in its present sense, appeared in the Dutch magazine "Penélopé" in 1823. This includes a color plate showing five different style purses of which three were intended to be crocheted with silk thread. The first is "simple open crochet" ("crochet simple ajour"); a mesh of chain-stitch arches. The second (illustrated here) starts in a semi-open form ("demi jour"), where chain-stitch arches alternate with equally long segments of slip-stitch crochet, and closes with a star made with "double-crochet stitches" ("dubbelde hekelsteek": double-crochet in British terminology; single-crochet in US). The third purse is made entirely in double-crochet. The instructions prescribe the use of a tambour needle (as illustrated below) and introduce a number of decorative techniques.

The earliest dated English reference to garments made of cloth produced by looping yarn with a hook—"shepherd's knitting"—is in "The Memoirs of a Highland Lady" by Elizabeth Grant (1797–1830). The journal entry, itself, is dated 1812 but was not recorded in its subsequently published form until some time between 1845 and 1867, and the actual date of publication was first in 1898. Nonetheless, the 1833 volume of "Penélopé" describes and illustrates a shepherd's hook, and recommends its use for crochet with coarser yarn.

In 1842, one of the numerous books discussing crochet that began to appear in the 1840s states:

Two years later, the same author, writes:

An instruction book from 1846 describes "Shepherd or Single Crochet" as what in current British usage is either called single crochet or slip-stitch crochet, with U.S. American terminology always using the latter (reserving single crochet for use as noted above). It similarly equates "Double" and "French crochet".
Notwithstanding the categorical assertion of a purely British origin, there is solid evidence of a connection between French tambour embroidery and crochet. The former method of production was illustrated in detail in 1763 in Diderot's Encyclopedia. The tip of the needle shown there is indistinguishable from that of a present-day inline crochet hook and the chain stitch separated from a cloth support is a fundamental element of the latter technique. The 1823 "Penélopé" instructions unequivocally state that the tambour tool was used for crochet and the first of the 1840s instruction books uses the terms "tambour" and "crochet" as synonyms. This equivalence is retained in the 4th edition of that work, 1847.

The strong taper of the shepherd's hook eases the production of slip-stitch crochet but is less amenable to stitches that require multiple loops on the hook at the same time. Early yarn hooks were also continuously tapered but gradually enough to accommodate multiple loops. The design with a cylindrical shaft that is commonplace today was largely reserved for tambour-style steel needles. Both types gradually merged into the modern form that appeared toward the end of the 19th century, including both tapered and cylindrical segments, and the continuously tapered bone hook remained in industrial production until World War II.

The early instruction books make frequent reference to the alternate use of 'ivory, bone, or wooden hooks' and 'steel needles in a handle', as appropriate to the stitch being made. Taken with the synonymous labeling of shepherd's- and single crochet, and the similar equivalence of French- and double crochet, there is a strong suggestion that crochet is rooted both in tambour embroidery and shepherd's knitting, leading to thread and yarn crochet respectively; a distinction that is still made. The locus of the fusion of all these elements—the "invention" noted above—has yet to be determined, as does the origin of shepherd's knitting.

Shepherd's hooks are still being made for local slip-stitch crochet traditions. The form in the accompanying photograph is typical for contemporary production. A longer continuously tapering design intermediate between it and the 19th-century tapered hook was also in earlier production, commonly being made from the handles of forks and spoons.

In the 19th century, as Ireland was facing the Great Irish Famine (1845–1849), crochet lace work was introduced as a form of famine relief (the production of crocheted lace being an alternative way of making money for impoverished Irish workers). Men, women, children joined a cooperative in order to crochet and produce products to help with famine relief during the Great Irish Famine. Schools to teach crocheting were started. Teachers were trained and sent across Ireland to teach this craft. When the Irish immigrated to the Americas, they were able to take with them crocheting. Mademoiselle Riego de la Blanchardiere is generally credited with the invention of Irish Crochet, publishing the first book of patterns in 1846. Irish lace became popular in Europe and America, and was made in quantity until the first World War.

Fashions in crochet changed with the end of the Victorian era in the 1890s. Crocheted laces in the new Edwardian era, peaking between 1910 and 1920, became even more elaborate in texture and complicated stitching.
The strong Victorian colours disappeared, though, and new publications called for white or pale threads, except for fancy purses, which were often crocheted of brightly colored silk and elaborately beaded. After World War I, far fewer crochet patterns were published, and most of them were simplified versions of the early 20th-century patterns. After World War II, from the late 1940s until the early 1960s, there was a resurgence in interest in home crafts, particularly in the United States, with many new and imaginative crochet designs published for colorful doilies, potholders, and other home items, along with updates of earlier publications. These patterns called for thicker threads and yarns than in earlier patterns and included wonderful variegated colors. The craft remained primarily a homemaker's art until the late 1960s and early 1970s, when the new generation picked up on crochet and popularized granny squares, a motif worked in the round and incorporating bright colors.

Although crochet underwent a subsequent decline in popularity, the early 21st century has seen a revival of interest in handcrafts and DIY, as well as great strides in improvement of the quality and varieties of yarn. There are many more new pattern books with modern patterns being printed, and most yarn stores now offer crochet lessons in addition to the traditional knitting lessons. There are many books you can purchase from local book stores to teach yourself how to crochet whether it be as a beginner or intermediate. There are also many books for children and teenagers who are hoping to take up the hobby. 
Filet crochet, Tunisian crochet, tapestry crochet, broomstick lace, hairpin lace, cro-hooking, and Irish crochet are all variants of the basic crochet method.

Crochet has experienced a revival on the catwalk as well. Christopher Kane's Fall 2011 Ready-to-Wear collection makes intensive use of the granny square, one of the most basic of crochet motifs. In addition, crochet has been utilized many times by designers on the popular reality show "Project Runway". Websites such as Etsy and Ravelry have made it easier for individual hobbyists to sell and distribute their patterns or projects across the internet.

Laneya Wiles released a music video titled "Straight Hookin'" which makes a play on the word "hookers," which has a double meaning for both "one who crochets" and "a prostitute."

Basic materials required for crochet are a hook and some type of material that will be crocheted, most commonly yarn or thread. Yarn, one of the most commonly used materials for crocheting has varying weights which need to be taken into consideration when following patterns. Additional tools are convenient for keeping stitches counted, measuring crocheted fabric, or making related accessories. Examples include cardboard cutouts, which can be used to make tassels, fringe, and many other items; a pom-pom circle, used to make pom-poms; a tape measure and a gauge measure, both used for measuring crocheted work and counting stitches; a row counter; and occasionally plastic rings, which are used for special projects.
In recent years, yarn selections have moved beyond synthetic and plant and animal-based fibers to include bamboo, qiviut, hemp, and banana stalks, to name a few.

The crochet hook comes in many sizes and materials, such as bone, bamboo, aluminium, plastic, and steel. Because sizing is categorized by the diameter of the hook's shaft, a crafter aims to create stitches of a certain size in order to reach a particular gauge specified in a given pattern. If gauge is not reached with one hook, another is used until the stitches made are the needed size. Crafters may have a preference for one type of hook material over another due to aesthetic appeal, yarn glide, or hand disorders such as arthritis, where bamboo or wood hooks are favored over metal for the perceived warmth and flexibility during use. Hook grips and ergonomic hook handles are also available to assist crafters.

Steel crochet hooks range in size from 0.4 to 3.5 millimeters, or from 00 to 16 in American sizing. These hooks are used for fine crochet work such as doilies and lace.

Aluminium, bamboo, and plastic crochet hooks are available from 2.5 to 19 millimeters in size, or from B to S in American sizing.

Artisan-made hooks are often made of hand-turned woods, sometimes decorated with semi-precious stones or beads.

Crochet hooks used for Tunisian crochet are elongated and have a stopper at the end of the handle, while double-ended crochet hooks have a hook on both ends of the handle. There is also a double hooked apparatus called a Cro-hook that has become popular.

A hairpin loom is often used to create lacy and long stitches, known as hairpin lace. While this is not in itself a hook, it is a device used in conjunction with a crochet hook to produce stitches.

See : List of United States standard crochet hook and knitting needle sizes

Yarn for crochet is usually sold as balls or skeins (hanks), although it may also be wound on spools or cones. Skeins and balls are generally sold with a "yarn band", a label that describes the yarn's weight, length, dye lot, fiber content, washing instructions, suggested needle size, likely gauge, etc. It is a common practice to save the yarn band for future reference, especially if additional skeins must be purchased. Crocheters generally ensure that the yarn for a project comes from a single dye lot. The dye lot specifies a group of skeins that were dyed together and thus have precisely the same color; skeins from different dye lots, even if very similar in color, are usually slightly different and may produce a visible stripe when added onto existing work. If insufficient yarn of a single dye lot is bought to complete a project, additional skeins of the same dye lot can sometimes be obtained from other yarn stores or online.

The thickness or weight of the yarn is a significant factor in determining how many stitches and rows are required to cover a given area for a given stitch pattern. This is also termed the gauge. Thicker yarns generally require large-diameter crochet hooks, whereas thinner yarns may be crocheted with thick or thin hooks. Hence, thicker yarns generally require fewer stitches, and therefore less time, to work up a given project. The recommended gauge for a given ball of yarn can be found on the label that surrounds the skein when buying in stores. Patterns and motifs are coarser with thicker yarns and produce bold visual effects, whereas thinner yarns are best for refined or delicate pattern-work. Yarns are standardly grouped by thickness into six categories: superfine, fine, light, medium, bulky and superbulky. Quantitatively, thickness is measured by the number of wraps per inch (WPI). The related "weight per unit length" is usually measured in tex or denier.

Before use, hanks are wound into balls in which the yarn emerges from the center, making crocheting easier by preventing the yarn from becoming easily tangled. The winding process may be performed by hand or done with a ballwinder and swift.

A yarn's usefulness is judged by several factors, such as its "loft" (its ability to trap air), its "resilience" (elasticity under tension), its washability and colorfastness, its "hand" (its feel, particularly softness vs. scratchiness), its durability against abrasion, its resistance to pilling, its "hairiness" (fuzziness), its tendency to twist or untwist, its overall weight and drape, its blocking and felting qualities, its comfort (breathability, moisture absorption, wicking properties) and its appearance, which includes its color, sheen, smoothness and ornamental features. Other factors include allergenicity, speed of drying, resistance to chemicals, moths, and mildew, melting point and flammability, retention of static electricity, and the propensity to accept dyes. Desirable properties may vary for different projects, so there is no one "best" yarn.
Although crochet may be done with ribbons, metal wire or more exotic filaments, most yarns are made by spinning fibers. In spinning, the fibers are twisted so that the yarn resists breaking under tension; the twisting may be done in either direction, resulting in a Z-twist or S-twist yarn. If the fibers are first aligned by combing them and the spinner uses a worsted type drafting method such as the short forward draw, the yarn is smoother and called a "worsted"; by contrast, if the fibers are carded but not combed and the spinner uses a woolen drafting method such as the long backward draw, the yarn is fuzzier and called "woolen-spun". The fibers making up a yarn may be continuous "filament" fibers such as silk and many synthetics, or they may be "staples" (fibers of an average length, typically a few inches); naturally filament fibers are sometimes cut up into staples before spinning. The strength of the spun yarn against breaking is determined by the amount of twist, the length of the fibers and the thickness of the yarn. In general, yarns become stronger with more twist (also called "worst"), longer fibers and thicker yarns (more fibers); for example, thinner yarns require more twist than do thicker yarns to resist breaking under tension. The thickness of the yarn may vary along its length; a "slub" is a much thicker section in which a mass of fibers is incorporated into the yarn.

The spun fibers are generally divided into animal fibers, plant and synthetic fibers. These fiber types are chemically different, corresponding to proteins, carbohydrates and synthetic polymers, respectively. Animal fibers include silk, but generally are long hairs of animals such as sheep (wool), goat (angora, or cashmere goat), rabbit (angora), llama, alpaca, dog, cat, camel, yak, and muskox (qiviut). Plants used for fibers include cotton, flax (for linen), bamboo, ramie, hemp, jute, nettle, raffia, yucca, coconut husk, banana trees, soy and corn. Rayon and acetate fibers are also produced from cellulose mainly derived from trees. Common synthetic fibers include acrylics, polyesters such as dacron and ingeo, nylon and other polyamides, and olefins such as polypropylene. Of these types, wool is generally favored for crochet, chiefly owing to its superior elasticity, warmth and (sometimes) felting; however, wool is generally less convenient to clean and some people are allergic to it. It is also common to blend different fibers in the yarn, e.g., 85% alpaca and 15% silk. Even within a type of fiber, there can be great variety in the length and thickness of the fibers; for example, Merino wool and Egyptian cotton are favored because they produce exceptionally long, thin (fine) fibers for their type.

A single spun yarn may be crochet as is, or braided or plied with another. In plying, two or more yarns are spun together, almost always in the opposite sense from which they were spun individually; for example, two Z-twist yarns are usually plied with an S-twist. The opposing twist relieves some of the yarns' tendency to curl up and produces a thicker, "balanced" yarn. Plied yarns may themselves be plied together, producing "cabled yarns" or "multi-stranded yarns". Sometimes, the yarns being plied are fed at different rates, so that one yarn loops around the other, as in bouclé. The single yarns may be dyed separately before plying, or afterwords to give the yarn a uniform look.

The dyeing of yarns is a complex art. Yarns need not be dyed; or they may be dyed one color, or a great variety of colors. Dyeing may be done industrially, by hand or even hand-painted onto the yarn. A great variety of synthetic dyes have been developed since the synthesis of indigo dye in the mid-19th century; however, natural dyes are also possible, although they are generally less brilliant. The color-scheme of a yarn is sometimes called its colorway. Variegated yarns can produce interesting visual effects, such as diagonal stripes.

Crocheted fabric is begun by placing a slip-knot loop on the hook (though other methods, such as a magic ring or simple folding over of the yarn may be used), pulling another loop through the first loop, and repeating this process to create a chain of a suitable length. The chain is either turned and worked in rows, or joined to the beginning of the row with a slip stitch and worked in rounds. Rounds can also be created by working many stitches into a single loop. Stitches are made by pulling one or more loops through each loop of the chain. At any one time at the end of a stitch, there is only one loop left on the hook. Tunisian crochet, however, draws all of the loops for an entire row onto a long hook before working them off one at a time. Like knitting, crochet can be worked either flat (back and forth in rows) or in the round (in spirals, such as when making tubular pieces).

There are five main types of basic stitches (the following description uses US crochet terminology which differs from the terminology used in the UK and Europe). 1. Chain Stitch – the most basic of all stitches and used to begin most projects. 2. Slip Stitch – used to join chain stitch to form a ring. 3. Single Crochet Stitch (called Double Crochet Stitch in the UK) – easiest stitch to master Single Crochet Stitch Tutorial 4. Half Double Crochet Stitch (called Half Treble Stitch in the UK) – the 'in-between' stitch Half-Double Crochet Tutorial 5. Double Crochet Stitch (called Treble Stitch in the UK) – many uses for this unlimited use stitch Double Crochet Stitch Tutorial while the horizontal distance covered by these basic stitches is the same, they differ in height and thickness.

The more advanced stitches are often combinations of these basic stitches, or are made by inserting the hook into the work in unusual locations. More advanced stitches include the Shell Stitch, V Stitch, Spike Stitch, Afghan Stitch, Butterfly Stitch, Popcorn Stitch, Cluster stitch, and Crocodile Stitch.

In the English-speaking crochet world, basic stitches have different names that vary by country. The differences are usually referred to as UK/US or British/American. To help counter confusion when reading patterns, a diagramming system using a standard international notation has come into use (illustration, left).

Another terminological difference is known as "tension" (UK) and "gauge" (US). Individual crocheters work yarn with a loose or a tight hold and, if unmeasured, these differences can lead to significant size changes in finished garments that have the same number of stitches. In order to control for this inconsistency, printed crochet instructions include a standard for the number of stitches across a standard swatch of fabric. An individual crocheter begins work by producing a test swatch and compensating for any discrepancy by changing to a smaller or larger hook. North Americans call this "gauge", referring to the end result of these adjustments; British crocheters speak of "tension", which refers to the crafter's grip on the yarn while producing stitches.

One of the more obvious differences is that crochet uses one hook while much knitting uses two needles. In most crochet, the artisan usually has only one live stitch on the hook (with the exception being Tunisian crochet), while a knitter keeps an entire row of stitches active simultaneously. Dropped stitches, which can unravel a knitted fabric, rarely interfere with crochet work, due to a second structural difference between knitting and crochet. In knitting, each stitch is supported by the corresponding stitch in the row above and it supports the corresponding stitch in the row below, whereas crochet stitches are only supported by and support the stitches on either side of it. If a stitch in a finished crocheted item breaks, the stitches above and below remain intact, and because of the complex looping of each stitch, the stitches on either side are unlikely to come loose unless heavily stressed.

Round or cylindrical patterns are simple to produce with a regular crochet hook, but cylindrical knitting requires either a set of circular needles or three to five special double-ended needles. Many crocheted items are composed of individual motifs which are then joined together, either by sewing or crocheting, whereas knitting is usually composed of one fabric, such as entrelac.

Freeform crochet is a technique that can create interesting shapes in three dimensions because new stitches can be made independently of previous stitches almost anywhere in the crocheted piece. It is generally accomplished by building shapes or structural elements onto existing crocheted fabric at any place the crafter desires.

Knitting can be accomplished by machine, while many crochet stitches can only be crafted by hand. The height of knitted and crocheted stitches is also different: a single crochet stitch is twice the height of a knit stitch in the same yarn size and comparable diameter tools, and a double crochet stitch is about four times the height of a knit stitch.

While most crochet is made with a hook, there is also a method of crocheting with a knitting loom. This is called loomchet. Slip stitch crochet is very similar to knitting. Each stitch in slip stitch crochet is formed the same way as a knit or purl stitch which is then bound off. A person working in slip stitch crochet can follow a knitted pattern with knits, purls, and cables, and get a similar result.

It is a common perception that crochet produces a thicker fabric than knitting, tends to have less "give" than knitted fabric, and uses approximately a third more yarn for a comparable project than knitted items. Though this is true when comparing a single crochet swatch with a stockinette swatch, both made with the same size yarn and needle/hook, it is not necessarily true for crochet in general. Most crochet uses far less than 1/3 more yarn than knitting for comparable pieces, and a crocheter can get similar feel and drape to knitting by using a larger hook or thinner yarn. Tunisian crochet and slip stitch crochet can in some cases use less yarn than knitting for comparable pieces. According to sources claiming to have tested the 1/3 more yarn assertion, a single crochet stitch (sc) uses approximately the same amount of yarn as knit garter stitch, but more yarn than stockinette stitch. Any stitch using yarnovers uses less yarn than single crochet to produce the same amount of fabric. Cluster stitches, which are in fact multiple stitches worked together, will use the most length.

Standard crochet stitches like sc and dc also produce a thicker fabric, more like knit garter stitch. This is part of why they use more yarn. Slip stitch can produce a fabric much like stockinette that is thinner and therefore uses less yarn.

Any yarn can be either knitted or crocheted, provided needles or hooks of the correct size are used, but the cord's properties should be taken into account. For example, lofty, thick woolen yarns tend to function better when knitted, which does not crush their airy structure, while thin and tightly spun yarn helps to achieve the firm texture required for Amigurumi crochet.

It has been very common for people and groups to crochet clothing and other garments and then donate them to soldiers during war. People have also crocheted clothing and then donated it to hospitals, for sick patients and also for newborn babies. Sometimes groups will crochet for a specific charity purpose, such as crocheting for homeless shelters, nursing homes, etc.
It is also becoming increasingly popular to crochet hats (commonly referred to as "chemo caps") and donate them to cancer treatment centers, for those undergoing chemotherapy. During the month of October pink hats and scarves are made and proceeds are donated to breast cancer funds. A few organizations dedicated to using crochet as a way to help others include Knots of Love, Crochet for Cancer, and Soldiers' Angels. These organizations offer warm useful items for people in need.

Crochet has been used to illustrate shapes in hyperbolic space that are difficult to reproduce using other media or are difficult to understand when viewed two-dimensionally. 

Mathematician Daina Taimina first used crochet in 1997 to create strong, durable models of hyperbolic space after finding paper models were delicate and hard to create. These models enable one to turn, fold, and otherwise manipulate space to more fully grasp ideas such as how a line can appear curved in hyperbolic space yet actually be straight. Her work received an exhibition by the Institute For Figuring.
Examples in nature of organisms that show hyperbolic structures include lettuces, sea slugs, flatworms and coral. Margaret Wertheim and Christine Wertheim of the Institute For Figuring created a travelling art installation of coral reef using Taimina's method. Local artists are encouraged to create their own "satellite reefs" to be included alongside the original display. 

As hyperbolic and mathematics-based crochet has continued to become more popular, there have been several events highlighting work from various fiber artists. Two such shows include Sant Ocean Hall at the Smithsonian in Washington D.C. and Sticks, Hooks, and the Mobius: Knit and Crochet Go Cerebral at Lafayette College in Pennsylvania.

In "Style in the technical arts", Gottfried Semper looks at the textile with great promise and historical precedent. In Section 53, he writes of the "loop stitch, or Noeud Coulant: a knot that, if untied, causes the whole system to unravel." In the same section, Semper confesses his ignorance of the subject of crochet but believes strongly that it is a technique of great value as a textile technique and possibly something more.

There are a small number of architects currently interested in the subject of crochet as it relates to architecture. The following publications, explorations and thesis projects can be used as a resource to see how crochet is being used within the capacity of architecture.

In the past few years, a practice called yarn bombing, or the use of knitted or crocheted cloth to modify and beautify one's (usually outdoor) surroundings, emerged in the US and spread worldwide. Yarn bombers sometimes target existing pieces of graffiti for beautification. In 2010, an entity dubbed "the Midnight Knitter" hit West Cape May. Residents awoke to find knit cozies hugging tree branches and sign poles. In September 2015, Grace Brett was named "The World's Oldest Yarn Bomber". She is part of a group of yarn graffiti-artists called the Souter Stormers, who beautify their local town in Scotland. When she is not yarn bombing, she is utilizing her craft by making items for her children and grandchildren.



</doc>
<doc id="7425" url="https://en.wikipedia.org/wiki?curid=7425" title="Electromagnetic coil">
Electromagnetic coil

An electromagnetic coil is an electrical conductor such as a wire in the shape of a coil, spiral or helix. Electromagnetic coils are used in electrical engineering, in applications where electric currents interact with magnetic fields, in devices such as electric motors, generators, inductors, electromagnets, transformers, and sensor coils. Either an electric current is passed through the wire of the coil to generate a magnetic field, or conversely an external "time-varying" magnetic field through the interior of the coil generates an EMF (voltage) in the conductor.

A current through any conductor creates a circular magnetic field around the conductor due to Ampere's law. The advantage of using the coil shape is that it increases the strength of magnetic field produced by a given current. The magnetic fields generated by the separate turns of wire all pass through the center of the coil and add (superpose) to produce a strong field there. The more turns of wire, the stronger the field produced. Conversely, a "changing" external magnetic flux induces a voltage in a conductor such as a wire, due to Faraday's law of induction. The induced voltage can be increased by winding the wire into a coil, because the field lines intersect the circuit multiple times.

The direction of the magnetic field produced by a coil can be determined by the right hand grip rule. If the fingers of the right hand are wrapped around the magnetic core of a coil in the direction of conventional current through the wire, the thumb will point in the direction the magnetic field lines pass through the coil. The end of a magnetic core from which the field lines emerge is defined to be the North pole.

There are many different types of coils used in electric and electronic equipment.

The wire or conductor which constitutes the coil is called the winding. The hole in the center of the coil is called the core area or "magnetic axis". Each loop of wire is called a turn. In windings in which the turns touch, the wire must be insulated with a coating of nonconductive insulation such as plastic or enamel to prevent the current from passing between the wire turns. The winding is often wrapped around a "coil form" made of plastic or other material to hold it in place. The ends of the wire are brought out and attached to an external circuit. Windings may have additional electrical connections along their length; these are called taps. A winding which has a single tap in the center of its length is called center-tapped.

Coils can have more than one winding, insulated electrically from each other. When there are two or more windings around a common magnetic axis, the windings are said to be inductively coupled or magnetically coupled. A time-varying current through one winding will create a time-varying magnetic field which passes through the other winding, which will induce a time-varying voltage in the other windings. This is called a transformer. The winding to which current is applied, which creates the magnetic field, is called the "primary winding". The other windings are called "secondary windings".

Many electromagnetic coils have a magnetic core, a piece of ferromagnetic material like iron in the center to increase the magnetic field. The current through the coil magnetizes the iron, and the field of the magnetized material adds to the field produced by the wire. This is called a ferromagnetic-core or iron-core coil. A ferromagnetic core can increase the magnetic field and inductance of a coil by hundreds or thousands of times over what it would be without the core. A ferrite core coil is a variety of coil with a core made of ferrite, a ferrimagnetic ceramic compound. Ferrite coils have lower core losses at high frequencies.
A coil without a ferromagnetic core is called an air-core coil. This includes coils wound on plastic or other nonmagnetic forms, as well as coils which actually have empty air space inside their windings.

Coils can be classified by the frequency of the current they are designed to operate with:

Coils can be classified by their function:

Electromagnets are coils that generate a magnetic field for some external use, often to exert a mechanical force on something. A few specific types:

Inductors or reactors are coils which generate a magnetic field which interacts with the coil itself, to induce a back EMF which opposes changes in current through the coil. Inductors are used as circuit elements in electrical circuits, to temporarily store energy or resist changes in current. A few types:

A transformer is a device with two or more magnetically coupled windings (or sections of a single winding). A time varying current in one coil (called the primary winding) generates a magnetic field which induces a voltage in the other coil (called the secondary winding). A few types:

Electric machines such as motors and generators have one or more windings which interact with moving magnetic fields to convert electrical energy to mechanical energy. Often a machine will have one winding through which passes most of the power of the machine (the "armature"), and a second winding which provides the magnetic field of the rotating element ( the "field winding") which may be connected by brushes or slip rings to an external source of electric current. In an induction motor, the "field" winding of the rotor is energized by the slow relative motion between the rotating winding and the rotating magnetic field produced by the stator winding, which induces the necessary exciting current in the rotor. 

These are coils used to translate time-varying magnetic fields to electric signals, and vice versa. A few types:

There are also types of coil which don't fit into these categories.





</doc>
<doc id="7426" url="https://en.wikipedia.org/wiki?curid=7426" title="Charles I of England">
Charles I of England

Charles I (19 November 1600 – 30 January 1649) was monarch of the three kingdoms of England, Scotland, and Ireland from 27 March 1625 until his execution in 1649.

Charles was born into the House of Stuart as the second son of King James VI of Scotland, but after his father inherited the English throne in 1603, he moved to England, where he spent much of the rest of his life. He became heir apparent to the thrones of England, Scotland and Ireland on the death of his elder brother, Henry Frederick, Prince of Wales, in 1612. An unsuccessful and unpopular attempt to marry him to the Spanish Habsburg princess Maria Anna culminated in an eight-month visit to Spain in 1623 that demonstrated the futility of the marriage negotiations. Two years later, he married the Bourbon princess Henrietta Maria of France instead.

After his succession, Charles quarrelled with the Parliament of England, which sought to curb his royal prerogative. Charles believed in the divine right of kings and thought he could govern according to his own conscience. Many of his subjects opposed his policies, in particular the levying of taxes without parliamentary consent, and perceived his actions as those of a tyrannical absolute monarch. His religious policies, coupled with his marriage to a Roman Catholic, generated the antipathy and mistrust of Reformed groups such as the English Puritans and Scottish Covenanters, who thought his views were too Catholic. He supported high church Anglican ecclesiastics, such as Richard Montagu and William Laud, and failed to aid Protestant forces successfully during the Thirty Years' War. His attempts to force the Church of Scotland to adopt high Anglican practices led to the Bishops' Wars, strengthened the position of the English and Scottish parliaments and helped precipitate his own downfall.

From 1642, Charles fought the armies of the English and Scottish parliaments in the English Civil War. After his defeat in 1645, he surrendered to a Scottish force that eventually handed him over to the English Parliament. Charles refused to accept his captors' demands for a constitutional monarchy, and temporarily escaped captivity in November 1647. Re-imprisoned on the Isle of Wight, Charles forged an alliance with Scotland, but by the end of 1648 Oliver Cromwell's New Model Army had consolidated its control over England. Charles was tried, convicted, and executed for high treason in January 1649. The monarchy was abolished and a republic called the Commonwealth of England was declared. The monarchy was restored to Charles's son, Charles II, in 1660.

The second son of King James VI of Scotland and Anne of Denmark, Charles was born in Dunfermline Palace, Fife, on 19 November 1600. At a Protestant ceremony in the Chapel Royal at Holyrood Palace in Edinburgh on 23 December 1600, he was baptised by David Lindsay, Bishop of Ross, and created Duke of Albany, the traditional title of the second son of the King of Scotland, with the subsidiary titles of Marquess of Ormond, Earl of Ross and Lord Ardmannoch.

James VI was the first cousin twice removed of Queen Elizabeth I of England, and when she died childless in March 1603, he became King of England as James I. Charles was a weak and sickly infant, and while his parents and older siblings left for England in April and early June that year, due to his fragile health, he remained in Scotland with his father's friend Lord Fyvie, appointed as his guardian.

By 1604, when Charles was three-and-a-half, he was able to walk the length of the great hall at Dunfermline Palace without assistance, and it was decided that he was strong enough to make the journey to England to be reunited with his family. In mid-July 1604, Charles left Dunfermline for England where he was to spend most of the rest of his life. In England, Charles was placed under the charge of Elizabeth, Lady Carey, the wife of courtier Sir Robert Carey, who put him in boots made of Spanish leather and brass to help strengthen his weak ankles. His speech development was also slow, and he retained a stammer, or hesitant speech, for the rest of his life.

In January 1605, Charles was created Duke of York, as is customary in the case of the English sovereign's second son, and made a Knight of the Bath. Thomas Murray, a Presbyterian Scot, was appointed as a tutor. Charles learnt the usual subjects of classics, languages, mathematics and religion. In 1611, he was made a Knight of the Garter.

Eventually, Charles apparently conquered his physical infirmity, which might have been caused by rickets. He became an adept horseman and marksman, and took up fencing. Even so, his public profile remained low in contrast to that of his physically stronger and taller elder brother, Henry Frederick, Prince of Wales, whom Charles adored and attempted to emulate. However, in early November 1612, Henry died at the age of 18 of what is suspected to have been typhoid (or possibly porphyria). Charles, who turned 12 two weeks later, became heir apparent. As the eldest surviving son of the sovereign, Charles automatically gained several titles (including Duke of Cornwall and Duke of Rothesay). Four years later, in November 1616, he was created Prince of Wales and Earl of Chester.

In 1613, his sister Elizabeth married Frederick V, Elector Palatine, and moved to Heidelberg. In 1617, the Habsburg Archduke Ferdinand of Austria, a Catholic, was elected king of Bohemia. The following year, the Bohemians rebelled, defenestrating the Catholic governors. In August 1619, the Bohemian diet chose as their monarch Frederick V, who was leader of the Protestant Union, while Ferdinand was elected Holy Roman Emperor in the imperial election. Frederick's acceptance of the Bohemian crown in defiance of the emperor marked the beginning of the turmoil that would develop into the Thirty Years' War. The conflict, originally confined to Bohemia, spiralled into a wider European war, which the English Parliament and public quickly grew to see as a polarised continental struggle between Catholics and Protestants. In 1620, Charles's brother-in-law, Frederick V, was defeated at the Battle of White Mountain near Prague and his hereditary lands in the Electoral Palatinate were invaded by a Habsburg force from the Spanish Netherlands. James, however, had been seeking marriage between the new Prince of Wales and Ferdinand's niece, Habsburg princess Maria Anna of Spain, and began to see the Spanish match as a possible diplomatic means of achieving peace in Europe.

Unfortunately for James, negotiation with Spain proved generally unpopular, both with the public and with James's court. The English Parliament was actively hostile towards Spain and Catholicism, and thus, when called by James in 1621, the members hoped for an enforcement of recusancy laws, a naval campaign against Spain, and a Protestant marriage for the Prince of Wales. James's Lord Chancellor, Francis Bacon, was impeached before the House of Lords for corruption. The impeachment was the first since 1459 without the king's official sanction in the form of a bill of attainder. The incident set an important precedent as the process of impeachment would later be used against Charles and his supporters: the Duke of Buckingham, Archbishop Laud, and the Earl of Strafford. James insisted that the House of Commons be concerned exclusively with domestic affairs, while the members protested that they had the privilege of free speech within the Commons' walls, demanding war with Spain and a Protestant Princess of Wales. Charles, like his father, considered the discussion of his marriage in the Commons impertinent and an infringement of his father's royal prerogative. In January 1622, James dissolved Parliament, angry at what he perceived as the members' impudence and intransigence.

Charles and the Duke of Buckingham, James's favourite and a man who had great influence over the prince, travelled incognito to Spain in February 1623 to try to reach agreement on the long-pending Spanish match. In the end, however, the trip was an embarrassing failure. The Infanta thought Charles was little more than an infidel, and the Spanish at first demanded that he convert to Roman Catholicism as a condition of the match. The Spanish insisted on toleration of Catholics in England and the repeal of the penal laws, which Charles knew would never be agreed by Parliament, and that the Infanta remain in Spain for a year after any wedding to ensure that England complied with all the terms of the treaty. A personal quarrel erupted between Buckingham and the Count of Olivares, the Spanish chief minister, and so Charles conducted the ultimately futile negotiations personally. When Charles returned to London in October, without a bride and to a rapturous and relieved public welcome, he and Buckingham pushed a reluctant King James to declare war on Spain.

With the encouragement of his Protestant advisers, James summoned the English Parliament in 1624 so that he could request subsidies for a war. Charles and Buckingham supported the impeachment of the Lord Treasurer, Lionel Cranfield, 1st Earl of Middlesex, who opposed war on grounds of cost and who quickly fell in much the same manner as Bacon had. James told Buckingham he was a fool, and presciently warned his son that he would live to regret the revival of impeachment as a parliamentary tool. An under-funded makeshift army under Ernst von Mansfeld set off to recover the Palatinate, but it was so poorly provisioned that it never advanced beyond the Dutch coast.

By 1624, James was growing ill, and as a result was finding it difficult to control Parliament. By the time of his death in March 1625, Charles and the Duke of Buckingham had already assumed "de facto" control of the kingdom.

With the failure of the Spanish match, Charles and Buckingham turned their attention to France. On 1 May 1625 Charles was married by proxy to the fifteen-year-old French princess Henrietta Maria in front of the doors of the Notre Dame de Paris. Charles had seen Henrietta Maria in Paris while en route to Spain. The couple married in person on 13 June 1625 in Canterbury. Charles delayed the opening of his first Parliament until after the second ceremony, to forestall any opposition. Many members of the Commons were opposed to the king's marriage to a Roman Catholic, fearing that Charles would lift restrictions on Catholic recusants and undermine the official establishment of the reformed Church of England. Although he told Parliament that he would not relax religious restrictions, he promised to do exactly that in a secret marriage treaty with his brother-in-law Louis XIII of France. Moreover, the treaty loaned to the French seven English naval ships that would be used to suppress the Protestant Huguenots at La Rochelle in September 1625. Charles was crowned on 2 February 1626 at Westminster Abbey, but without his wife at his side because she refused to participate in a Protestant religious ceremony.

Distrust of Charles's religious policies increased with his support of a controversial anti-Calvinist ecclesiastic, Richard Montagu, who was in disrepute among the Puritans. In his pamphlet "A New Gag for an Old Goose" (1624), a reply to the Catholic pamphlet "A New Gag for the New Gospel", Montagu argued against Calvinist predestination, the doctrine that salvation and damnation were preordained by God. Anti-Calvinists – known as Arminians – believed that human beings could influence their own fate through the exercise of free will. Arminian divines had been one of the few sources of support for Charles's proposed Spanish marriage. With the support of King James, Montagu produced another pamphlet, entitled "Appello Caesarem", in 1625 shortly after the old king's death and Charles's accession. To protect Montagu from the stricture of Puritan members of Parliament, Charles made the cleric one of his royal chaplains, increasing many Puritans' suspicions that Charles favoured Arminianism as a clandestine attempt to aid the resurgence of Catholicism.

Rather than direct involvement in the European land war, the English Parliament preferred a relatively inexpensive naval attack on Spanish colonies in the New World, hoping for the capture of the Spanish treasure fleets. Parliament voted to grant a subsidy of £140,000, which was an insufficient sum for Charles's war plans. Moreover, the House of Commons limited its authorisation for royal collection of tonnage and poundage (two varieties of customs duties) to a period of one year, although previous sovereigns since Henry VI had been granted the right for life. In this manner, Parliament could delay approval of the rates until after a full-scale review of customs revenue. The bill made no progress in the House of Lords past its first reading. Although no Parliamentary Act for the levy of tonnage and poundage was obtained, Charles continued to collect the duties.

A poorly conceived and executed naval expedition against Spain under the leadership of Buckingham went badly, and the House of Commons began proceedings for the impeachment of the duke. In May 1626, Charles nominated Buckingham as Chancellor of Cambridge University in a show of support, and had two members who had spoken against Buckingham – Dudley Digges and Sir John Eliot – arrested at the door of the House. The Commons was outraged by the imprisonment of two of their members, and after about a week in custody, both were released. On 12 June 1626, the Commons launched a direct protestation attacking Buckingham, stating,
"We protest before your Majesty and the whole world that until this great person be removed from intermeddling with the great affairs of state, we are out of hope of any good success; and do fear that any money we shall or can give will, through his misemployment, be turned rather to the hurt and prejudice of this your kingdom than otherwise, as by lamentable experience we have found those large supplies formerly and lately given." Despite Parliament's protests, however, Charles refused to dismiss his friend, dismissing Parliament instead.

Meanwhile, domestic quarrels between Charles and Henrietta Maria were souring the early years of their marriage. Disputes over her jointure, appointments to her household, and the practice of her religion culminated in the king expelling the vast majority of her French attendants in August 1626. Despite Charles's agreement to provide the French with English ships as a condition of marrying Henrietta Maria, in 1627 he launched an attack on the French coast to defend the Huguenots at La Rochelle. The action, led by Buckingham, was ultimately unsuccessful. Buckingham's failure to protect the Huguenots – and his retreat from Saint-Martin-de-Ré – spurred Louis XIII's siege of La Rochelle and furthered the English Parliament's and people's detestation of the duke.

Charles provoked further unrest by trying to raise money for the war through a "forced loan": a tax levied without parliamentary consent. In November 1627, the test case in the King's Bench, the "Five Knights' Case", found that the king had a prerogative right to imprison without trial those who refused to pay the forced loan. Summoned again in March 1628, on 26 May Parliament adopted a Petition of Right, calling upon the king to acknowledge that he could not levy taxes without Parliament's consent, not impose martial law on civilians, not imprison them without due process, and not quarter troops in their homes. Charles assented to the petition on 7 June, but by the end of the month he had prorogued Parliament and re-asserted his right to collect customs duties without authorisation from Parliament.

On 23 August 1628, Buckingham was assassinated. Charles was deeply distressed. According to Edward Hyde, 1st Earl of Clarendon, he "threw himself upon his bed, lamenting with much passion and with abundance of tears". He remained grieving in his room for two days. In contrast, the public rejoiced at Buckingham's death, which accentuated the gulf between the court and the nation, and between the Crown and the Commons. Although the death of Buckingham effectively ended the war with Spain and eliminated his leadership as an issue, it did not end the conflicts between Charles and Parliament. It did, however, coincide with an improvement in Charles's relationship with his wife, and by November 1628 their old quarrels were at an end. Perhaps Charles's emotional ties were transferred from Buckingham to Henrietta Maria. She became pregnant for the first time, and the bond between them grew ever stronger. Together, they embodied an image of virtue and family life, and their court became a model of formality and morality.

Partly inspired by his visit to the Spanish court in 1623, Charles became a passionate and knowledgeable art collector, amassing one of the finest art collections ever assembled. In Spain, he sat for a sketch by Velázquez, and acquired works by Titian and Correggio, among others. In England, his commissions included the ceiling of the Banqueting House, Whitehall, by Rubens and paintings by other artists from the Low Countries such as van Honthorst, Mytens, and van Dyck. His close associates, including the Duke of Buckingham and the Earl of Arundel, shared his interest and have been dubbed the Whitehall Group. In 1627 and 1628, Charles purchased the entire collection of the Duke of Mantua, which included work by Titian, Correggio, Raphael, Caravaggio, del Sarto and Mantegna. His collection grew further to encompass Bernini, Bruegel, da Vinci, Holbein, Hollar, Tintoretto and Veronese, and self-portraits by both Dürer and Rembrandt. By Charles's death, there were an estimated 1,760 paintings, most of which were sold and dispersed by Parliament.

In January 1629, Charles opened the second session of the English Parliament, which had been prorogued in June 1628, with a moderate speech on the tonnage and poundage issue. Members of the House of Commons began to voice opposition to Charles's policies in light of the case of John Rolle, a Member of Parliament whose goods had been confiscated for failing to pay tonnage and poundage. Many MPs viewed the imposition of the tax as a breach of the Petition of Right. When Charles ordered a parliamentary adjournment on 2 March, members held the Speaker, Sir John Finch, down in his chair so that the ending of the session could be delayed long enough for resolutions against Catholicism, Arminianism and tonnage and poundage to be read out and acclaimed by the chamber. The provocation was too much for Charles, who dissolved Parliament and had nine parliamentary leaders, including Sir John Eliot, imprisoned over the matter, thereby turning the men into martyrs, and giving popular cause to their protest.

Shortly after the prorogation, without the means in the foreseeable future to raise funds from Parliament for a European war, or the influence of Buckingham, Charles made peace with France and Spain. The following eleven years, during which Charles ruled England without a Parliament, are referred to as the personal rule or the "eleven years' tyranny". Ruling without Parliament was not exceptional, and was supported by precedent. Only Parliament, however, could legally raise taxes, and without it Charles's capacity to acquire funds for his treasury was limited to his customary rights and prerogatives.

A large fiscal deficit had arisen in the reigns of Elizabeth I and James I. Notwithstanding Buckingham's shortlived campaigns against both Spain and France, there was little financial capacity for Charles to wage wars overseas. Throughout his reign Charles was obliged to rely primarily on volunteer forces for defence and on diplomatic efforts to support his sister, Elizabeth, and his foreign policy objective for the restoration of the Palatinate. England was still the least taxed country in Europe, with no official excise and no regular direct taxation. To raise revenue without reconvening Parliament, Charles resurrected an all-but-forgotten law called the "Distraint of Knighthood", in abeyance for over a century, which required any man who earned £40 or more from land each year to present himself at the king's coronation to be knighted. Relying on this old statute, Charles fined individuals who had failed to attend his coronation in 1626.

The chief tax imposed by Charles was a feudal levy known as ship money, which proved even more unpopular, and lucrative, than poundage and tonnage before it. Previously, collection of ship money had been authorised only during wars, and only on coastal regions. Charles, however, argued that there was no legal bar to collecting the tax for defence during peacetime and throughout the whole of the kingdom. Ship money, paid directly to the Treasury of the Navy, provided between £150,000 to £200,000 annually between 1634 and 1638, after which yields declined. Opposition to ship money steadily grew, but the 12 common law judges of England declared that the tax was within the king's prerogative, though some of them had reservations. The prosecution of John Hampden for non-payment in 1637–38 provided a platform for popular protest, and the judges found against Hampden only by the narrow margin of 7–5.

The king also derived money through the granting of monopolies, despite a statute forbidding such action, which, though inefficient, raised an estimated £100,000 a year in the late 1630s. One such monopoly was for soap, pejoratively referred to as "popish soap" because some of its backers were Catholics. Charles also raised funds from the Scottish nobility, at the price of considerable acrimony, by the Act of Revocation (1625), whereby all gifts of royal or church land made to the nobility since 1540 were revoked, with continued ownership being subject to an annual rent. In addition, the boundaries of the royal forests in England were restored to their ancient limits as part of a scheme to maximise income by exploiting the land and fining land users within the reasserted boundaries for encroachment. The focus of the programme was disafforestation and sale of forest lands for conversion to pasture and arable farming, or in the case of the Forest of Dean, development for the iron industry. Disafforestation frequently caused riots and disturbances including those known as the Western Rising.

Against the background of this unrest, Charles faced bankruptcy in mid-1640. The City of London, preoccupied with its own grievances, refused to make any loans to the king, as did foreign powers. In this extremity, in July Charles seized silver bullion worth £130,000 held in trust at the mint in the Tower of London, promising its later return at 8% interest to its owners. In August, after the East India Company refused to grant a loan, Lord Cottington seized the company's stock of pepper and spices and sold it for £60,000 (far below its market value), promising to refund the money with interest later.

Throughout Charles's reign, the issue of how far the English Reformation should progress was constantly in the forefront of political debate. Arminian theology emphasised clerical authority and the individual's ability to reject or accept salvation, which opponents viewed as heretical and a potential vehicle for the reintroduction of Roman Catholicism. Puritan reformers thought Charles was too sympathetic to the teachings of Arminianism, which they considered irreligious, and opposed his desire to move the Church of England in a more traditional and sacramental direction. In addition, his Protestant subjects followed the European war closely and grew increasingly dismayed by Charles's diplomacy with Spain and his failure to support the Protestant cause abroad effectively.

In 1633, Charles appointed William Laud Archbishop of Canterbury. They initiated a series of reforms aimed at ensuring religious uniformity by restricting non-conformist preachers, insisting the liturgy be celebrated as prescribed by the Book of Common Prayer, organising the internal architecture of English churches to emphasise the sacrament of the altar, and re-issuing King James's Declaration of Sports, which permitted secular activities on the sabbath. The Feoffees for Impropriations, an organisation that bought benefices and advowsons so that Puritans could be appointed to them, was dissolved. Laud prosecuted those who opposed his reforms in the Court of High Commission and the Star Chamber, the two most powerful courts in the land. The courts became feared for their censorship of opposing religious views and unpopular among the propertied classes for inflicting degrading punishments on gentlemen. For example, in 1637 William Prynne, Henry Burton and John Bastwick were pilloried, whipped and mutilated by cropping and imprisoned indefinitely for publishing anti-episcopal pamphlets.

When Charles attempted to impose his religious policies in Scotland he faced numerous difficulties. Although born in Scotland, Charles had become estranged from his northern kingdom; his first visit since early childhood was for his Scottish coronation in 1633. To the dismay of the Scots, who had removed many traditional rituals from their liturgical practice, Charles insisted that the coronation be conducted in the Anglican rite. In 1637, the king ordered the use of a new prayer book in Scotland that was almost identical to the English Book of Common Prayer, without consulting either the Scottish Parliament or the Kirk. Although it had been written, under Charles's direction, by Scottish bishops, many Scots resisted it, seeing the new prayer book as a vehicle for introducing Anglicanism to Scotland. On 23 July, riots erupted in Edinburgh upon the first Sunday of the prayer book's usage, and unrest spread throughout the Kirk. The public began to mobilise around a reaffirmation of the National Covenant, whose signatories pledged to uphold the reformed religion of Scotland and reject any innovations that were not authorised by Kirk and Parliament. When the General Assembly of the Church of Scotland met in November 1638, it condemned the new prayer book, abolished episcopal church government by bishops, and adopted Presbyterian government by elders and deacons.

Charles perceived the unrest in Scotland as a rebellion against his authority, precipitating the First Bishops' War in 1639. Charles did not seek subsidies from the English Parliament to wage war, but instead raised an army without parliamentary aid and marched to Berwick-upon-Tweed, on the border of Scotland. Charles's army did not engage the Covenanters as the king feared the defeat of his forces, whom he believed to be significantly outnumbered by the Scots. In the Treaty of Berwick, Charles regained custody of his Scottish fortresses and secured the dissolution of the Covenanters' interim government, albeit at the decisive concession that both the Scottish Parliament and General Assembly of the Scottish Church were called.

The military failure in the First Bishops' War caused a financial and diplomatic crisis for Charles that deepened when his efforts to raise funds from Spain, while simultaneously continuing his support for his Palatine relatives, led to the public humiliation of the Battle of the Downs, where the Dutch destroyed a Spanish bullion fleet off the coast of Kent in sight of the impotent English navy.

Charles continued peace negotiations with the Scots in a bid to gain time before launching a new military campaign. Because of his financial weakness, he was forced to call Parliament into session in an attempt to raise funds for such a venture. Both English and Irish parliaments were summoned in the early months of 1640. In March 1640, the Irish Parliament duly voted in a subsidy of £180,000 with the promise to raise an army 9,000 strong by the end of May. In the English general election in March, however, court candidates fared badly, and Charles's dealings with the English Parliament in April quickly reached stalemate. The earls of Northumberland and Strafford attempted to broker a compromise whereby the king would agree to forfeit ship money in exchange for £650,000 (although the cost of the coming war was estimated at around £1 million). Nevertheless, this alone was insufficient to produce consensus in the Commons. The Parliamentarians' calls for further reforms were ignored by Charles, who still retained the support of the House of Lords. Despite the protests of Northumberland, the Short Parliament (as it came to be known) was dissolved in May 1640, less than a month after it assembled.

By this stage Strafford, Lord Deputy of Ireland since 1632, had emerged as Charles's right-hand man and together with Laud, pursued a policy of "Thorough" that aimed to make central royal authority more efficient and effective at the expense of local or anti-government interests. Although originally a critic of the king, Strafford defected to royal service in 1628 (due in part to Buckingham's persuasion), and had since emerged, alongside Laud, as the most influential of Charles's ministers.

Bolstered by the failure of the English Short Parliament, the Scottish Parliament declared itself capable of governing without the king's consent, and in August 1640 the Covenanter army moved into the English county of Northumberland. Following the illness of the earl of Northumberland, who was the king's commander-in-chief, Charles and Strafford went north to command the English forces, despite Strafford being ill himself with a combination of gout and dysentery. The Scottish soldiery, many of whom were veterans of the Thirty Years' War, had far greater morale and training compared to their English counterparts. They met virtually no resistance until reaching Newcastle upon Tyne, where they defeated the English forces at the Battle of Newburn and occupied the city, as well as the neighbouring county of Durham.

As demands for a parliament grew, Charles took the unusual step of summoning a great council of peers. By the time it met, on 24 September at York, Charles had resolved to follow the almost universal advice to call a parliament. After informing the peers that a parliament would convene in November, he asked them to consider how he could acquire funds to maintain his army against the Scots in the meantime. They recommended making peace. A cessation of arms, although not a final settlement, was negotiated in the humiliating Treaty of Ripon, signed in October 1640. The treaty stated that the Scots would continue to occupy Northumberland and Durham and be paid £850 per day until peace was restored and the English Parliament recalled, which would be required to raise sufficient funds to pay the Scottish forces. Consequently, Charles summoned what later became known as the Long Parliament. Once again, Charles's supporters fared badly at the polls. Of the 493 members of the Commons returned in November, over 350 were opposed to the king.

The Long Parliament proved just as difficult for Charles as had the Short Parliament. It assembled on 3 November 1640 and quickly began proceedings to impeach the king's leading counsellors of high treason. Strafford was taken into custody on 10 November; Laud was impeached on 18 December; John Finch, now Lord Keeper of the Great Seal, was impeached the following day, and he consequently fled to the Hague with Charles's permission on 21 December. To prevent the king from dissolving it at will, Parliament passed the Triennial Act, which required Parliament to be summoned at least once every three years, and permitted the Lord Keeper and 12 peers to summon Parliament if the king failed to do so. The Act was coupled with a subsidy bill, and so to secure the latter, Charles grudgingly granted royal assent in February 1641.

Strafford had become the principal target of the Parliamentarians, particularly John Pym, and he went on trial for high treason on 22 March 1641. However, the key allegation by Sir Henry Vane that Strafford had threatened to use the Irish army to subdue England was not corroborated and on 10 April Pym's case collapsed. Pym and his allies immediately launched a bill of attainder, which simply declared Strafford guilty and pronounced the sentence of death.

Charles assured Strafford that "upon the word of a king you shall not suffer in life, honour or fortune", and the attainder could not succeed if Charles withheld assent. Furthermore, many members and most peers were opposed to the attainder, not wishing, in the words of one, to "commit murder with the sword of justice". However, increased tensions and an attempted coup by royalist army officers in support of Strafford and in which Charles was involved began to sway the issue. The Commons passed the bill on 20 April by a large margin (204 in favour, 59 opposed, and 230 abstained), and the Lords acquiesced (by 26 votes to 19, with 79 absent) in May. On 3 May, Parliament's Protestation attacked the "wicked counsels" of Charles's "arbitrary and tyrannical government"; while those who signed the petition undertook to defend the king's "person, honour and estate", they also swore to preserve "the true reformed religion", parliament, and the "rights and liberties of the subjects". Charles, fearing for the safety of his family in the face of unrest, assented reluctantly to Strafford's attainder on 9 May after consulting his judges and bishops. Strafford was beheaded three days later.

Additionally in early May, Charles assented to an unprecedented Act that forbade the dissolution of the English Parliament without its consent. In the following months, ship money, fines in distraint of knighthood and excise without parliamentary consent were declared unlawful, and the Courts of Star Chamber and High Commission were abolished. All remaining forms of taxation were legalised and regulated by the Tonnage and Poundage Act. The House of Commons also launched bills attacking bishops and episcopacy, but these failed in the Lords.

Charles had made important concessions in England, and temporarily improved his position in Scotland by securing the favour of the Scots on a visit from August to November 1641 during which he conceded to the official establishment of Presbyterianism. However, following an attempted royalist coup in Scotland, known as "The Incident", Charles's credibility was significantly undermined.

In Ireland, the population was split into three main socio-political groups: the Gaelic Irish, who were Catholic; the Old English, who were descended from medieval Normans and were also predominantly Catholic; and the New English, who were Protestant settlers from England and Scotland aligned with the English Parliament and the Covenanters. Strafford's administration had improved the Irish economy and boosted tax revenue, but had done so by heavy-handedly imposing order. He had trained up a large Catholic army in support of the king and had weakened the authority of the Irish Parliament, while continuing to confiscate land from Catholics for Protestant settlement at the same time as promoting a Laudian Anglicanism that was anathema to Presbyterians. As a result, all three groups had become disaffected. Strafford's impeachment provided a new departure for Irish politics whereby all sides joined together to present evidence against him. In a similar manner to the English Parliament, the Old English members of the Irish Parliament argued that while opposed to Strafford they remained loyal to Charles. They argued that the king had been led astray by malign counsellors, and that, moreover, a viceroy such as Strafford could emerge as a despotic figure instead of ensuring that the king was directly involved in governance.

Strafford's fall from power weakened Charles's influence in Ireland. The dissolution of the Irish army was unsuccessfully demanded three times by the English Commons during Strafford's imprisonment, until Charles was eventually forced through lack of money to disband the army at the end of Strafford's trial. Disputes concerning the transfer of land ownership from native Catholic to settler Protestant, particularly in relation to the plantation of Ulster, coupled with resentment at moves to ensure the Irish Parliament was subordinate to the Parliament of England, sowed the seeds of rebellion. When armed conflict arose between the Gaelic Irish and New English, in late October 1641, the Old English sided with the Gaelic Irish while simultaneously professing their loyalty to the king.

In November 1641, the House of Commons passed the Grand Remonstrance, a long list of grievances against actions by Charles's ministers committed since the beginning of his reign (that were asserted to be part of a grand Catholic conspiracy of which the king was an unwitting member), but it was in many ways a step too far by Pym and passed by only 11 votes – 159 to 148. Furthermore, the Remonstrance had very little support in the House of Lords, which the Remonstrance attacked. The tension was heightened by news of the Irish rebellion, coupled with inaccurate rumours of Charles's complicity. Throughout November, a series of alarmist pamphlets published stories of atrocities in Ireland, which included massacres of New English settlers by the native Irish who could not be controlled by the Old English lords. Rumours of "papist" conspiracies circulated in England, and English anti-Catholic opinion was strengthened, damaging Charles's reputation and authority.
The English Parliament distrusted Charles's motivations when he called for funds to put down the Irish rebellion; many members of the Commons suspected that forces raised by Charles might later be used against Parliament itself. Pym's Militia Bill was intended to wrest control of the army from the king, but it did not have the support of the Lords, let alone Charles. Instead, the Commons passed the bill as an ordinance, which they claimed did not require royal assent. The Militia Ordinance appears to have prompted more members of the Lords to support the king. In an attempt to strengthen his position, Charles generated great antipathy in London, which was already fast falling into lawlessness, when he placed the Tower of London under the command of Colonel Thomas Lunsford, an infamous, albeit efficient, career officer. When rumours reached Charles that Parliament intended to impeach his wife for supposedly conspiring with the Irish rebels, the king decided to take drastic action.

Charles suspected, probably correctly, that some members of the English Parliament had colluded with the invading Scots. On 3 January 1642, Charles directed Parliament to give up five members of the Commons – Pym, John Hampden, Denzil Holles, William Strode and Sir Arthur Haselrig – and one peer – Lord Mandeville – on the grounds of high treason. When Parliament refused, it was possibly Henrietta Maria who persuaded Charles to arrest the five members by force, which Charles intended to carry out personally. However, news of the warrant reached Parliament ahead of him, and the wanted men slipped away by boat shortly before Charles entered the House of Commons with an armed guard on 4 January. Having displaced the Speaker, William Lenthall, from his chair, the king asked him where the MPs had fled. Lenthall, on his knees, famously replied, "May it please your Majesty, I have neither eyes to see nor tongue to speak in this place but as the House is pleased to direct me, whose servant I am here." Charles abjectly declared "all my birds have flown", and was forced to retire, empty-handed.

The botched arrest attempt was politically disastrous for Charles. No English sovereign had ever entered the House of Commons, and his unprecedented invasion of the chamber to arrest its members was considered a grave breach of parliamentary privilege. In one stroke Charles destroyed his supporters' efforts to portray him as a defence against innovation and disorder.

Parliament quickly seized London, and Charles fled the capital for Hampton Court Palace on 10 January, moving two days later to Windsor Castle. After sending his wife and eldest daughter to safety abroad in February, he travelled northwards, hoping to seize the military arsenal at Hull. To his dismay, he was rebuffed by the town's Parliamentary governor, Sir John Hotham, who refused him entry in April, and Charles was forced to withdraw.

In mid-1642, both sides began to arm. Charles raised an army using the medieval method of commission of array, and Parliament called for volunteers for its militia. Following futile negotiations, Charles raised the royal standard in Nottingham on 22 August 1642. At the start of the First English Civil War, Charles's forces controlled roughly the Midlands, Wales, the West Country and northern England. He set up his court at Oxford. Parliament controlled London, the south-east and East Anglia, as well as the English navy.

After a few skirmishes, the opposing forces met in earnest at Edgehill, on 23 October 1642. Charles's nephew Prince Rupert of the Rhine disagreed with the battle strategy of the royalist commander Lord Lindsey, and Charles sided with Rupert. Lindsey resigned, leaving Charles to assume overall command assisted by Lord Forth. Rupert's cavalry successfully charged through the parliamentary ranks, but instead of swiftly returning to the field, rode off to plunder the parliamentary baggage train. Lindsey, acting as a colonel, was wounded and bled to death without medical attention. The battle ended inconclusively as the daylight faded.

In his own words, the experience of battle had left Charles "exceedingly and deeply grieved". He regrouped at Oxford, turning down Rupert's suggestion of an immediate attack on London. After a week, he set out for the capital on 3 November, capturing Brentford on the way while simultaneously continuing to negotiate with civic and parliamentary delegations. At Turnham Green on the outskirts of London, the royalist army met resistance from the city militia, and faced with a numerically superior force, Charles ordered a retreat. He overwintered in Oxford, strengthening the city's defences and preparing for the next season's campaign. Peace talks between the two sides collapsed in April.

The war continued indecisively over the next couple of years, and Henrietta Maria returned to Britain for 17 months from February 1643. After Rupert captured Bristol in July 1643, Charles visited the port city and laid siege to Gloucester, further up the river Severn. His plan to undermine the city walls failed due to heavy rain, and on the approach of a parliamentary relief force, Charles lifted the siege and withdrew to Sudeley Castle. The parliamentary army turned back towards London, and Charles set off in pursuit. The two armies met at Newbury, Berkshire, on 20 September. Just as at Edgehill, the battle stalemated at nightfall, and the armies disengaged. In January 1644, Charles summoned a Parliament at Oxford, which was attended by about 40 peers and 118 members of the Commons; all told, the Oxford Parliament, which sat until March 1645, was supported by the majority of peers and about a third of the Commons. Charles became disillusioned by the assembly's ineffectiveness, calling it a "mongrel" in private letters to his wife.

In 1644, Charles remained in the southern half of England while Rupert rode north to relieve Newark and York, which were under threat from parliamentary and Scottish Covenanter armies. Charles was victorious at the battle of Cropredy Bridge in late June, but the royalists in the north were defeated at the battle of Marston Moor just a few days later. The king continued his campaign in the south, encircling and disarming the parliamentary army of the Earl of Essex. Returning northwards to his base at Oxford, he fought at Newbury for a second time before the winter closed in; the battle ended indecisively. Attempts to negotiate a settlement over the winter, while both sides re-armed and re-organised, were again unsuccessful.

At the battle of Naseby on 14 June 1645, Rupert's horsemen again mounted a successful charge against the flank of Parliament's New Model Army, but Charles's troops elsewhere on the field were pushed back by the opposing forces. Charles, attempting to rally his men, rode forward but as he did so, Lord Carnwath seized his bridle and pulled him back, fearing for the king's safety. Carnwath's action was misinterpreted by the royalist soldiers as a signal to move back, leading to a collapse of their position. The military balance tipped decisively in favour of Parliament. There followed a series of defeats for the royalists, and then the Siege of Oxford, from which Charles escaped (disguised as a servant) in April 1646. He put himself into the hands of the Scottish Presbyterian army besieging Newark, and was taken northwards to Newcastle upon Tyne. After nine months of negotiations, the Scots finally arrived at an agreement with the English Parliament: in exchange for £100,000, and the promise of more money in the future, the Scots withdrew from Newcastle and delivered Charles to the parliamentary commissioners in January 1647.

Parliament held Charles under house arrest at Holdenby House in Northamptonshire until Cornet George Joyce took him by threat of force from Holdenby on 3 June in the name of the New Model Army. By this time, mutual suspicion had developed between Parliament, which favoured army disbandment and Presbyterianism, and the New Model Army, which was primarily officered by Independent non-conformists who sought a greater political role. Charles was eager to exploit the widening divisions, and apparently viewed Joyce's actions as an opportunity rather than a threat. He was taken first to Newmarket, at his own suggestion, and then transferred to Oatlands and subsequently Hampton Court, while more ultimately fruitless negotiations took place. By November, he determined that it would be in his best interests to escape – perhaps to France, Southern England or to Berwick-upon-Tweed, near the Scottish border. He fled Hampton Court on 11 November, and from the shores of Southampton Water made contact with Colonel Robert Hammond, Parliamentary Governor of the Isle of Wight, whom he apparently believed to be sympathetic. Hammond, however, confined Charles in Carisbrooke Castle and informed Parliament that Charles was in his custody.

From Carisbrooke, Charles continued to try to bargain with the various parties. In direct contrast to his previous conflict with the Scottish Kirk, on 26 December 1647 he signed a secret treaty with the Scots. Under the agreement, called the "Engagement", the Scots undertook to invade England on Charles's behalf and restore him to the throne on condition that Presbyterianism be established in England for three years.

The royalists rose in May 1648, igniting the Second Civil War, and as agreed with Charles, the Scots invaded England. Uprisings in Kent, Essex, and Cumberland, and a rebellion in South Wales, were put down by the New Model Army, and with the defeat of the Scots at the Battle of Preston in August 1648, the royalists lost any chance of winning the war.

Charles's only recourse was to return to negotiations, which were held at Newport on the Isle of Wight. On 5 December 1648, Parliament voted by 129 to 83 to continue negotiating with the king, but Oliver Cromwell and the army opposed any further talks with someone they viewed as a bloody tyrant and were already taking action to consolidate their power. Hammond was replaced as Governor of the Isle of Wight on 27 November, and placed in the custody of the army the following day. In Pride's Purge on 6 and 7 December, the members of Parliament out of sympathy with the military were arrested or excluded by Colonel Thomas Pride, while others stayed away voluntarily. The remaining members formed the Rump Parliament. It was effectively a military coup.

Charles was moved to Hurst Castle at the end of 1648, and thereafter to Windsor Castle. In January 1649, the Rump House of Commons indicted him on a charge of treason, which was rejected by the House of Lords. The idea of trying a king was a novel one. The Chief Justices of the three common law courts of England – Henry Rolle, Oliver St John and John Wilde – all opposed the indictment as unlawful. The Rump Commons declared itself capable of legislating alone, passed a bill creating a separate court for Charles's trial, and declared the bill an act without the need for royal assent. The High Court of Justice established by the Act consisted of 135 commissioners, but many either refused to serve or chose to stay away. Only 68 (all firm Parliamentarians) attended Charles's trial on charges of high treason and "other high crimes" that began on 20 January 1649 in Westminster Hall. John Bradshaw acted as President of the Court, and the prosecution was led by the Solicitor General, John Cook.

Charles was accused of treason against England by using his power to pursue his personal interest rather than the good of the country. The charge stated that he, "for accomplishment of such his designs, and for the protecting of himself and his adherents in his and their wicked practices, to the same ends hath traitorously and maliciously levied war against the present Parliament, and the people therein represented", and that the "wicked designs, wars, and evil practices of him, the said Charles Stuart, have been, and are carried on for the advancement and upholding of a personal interest of will, power, and pretended prerogative to himself and his family, against the public interest, common right, liberty, justice, and peace of the people of this nation." Reflecting the modern concept of command responsibility, the indictment held him "guilty of all the treasons, murders, rapines, burnings, spoils, desolations, damages and mischiefs to this nation, acted and committed in the said wars, or occasioned thereby." An estimated 300,000 people, or 6% of the population, died during the war.

Over the first three days of the trial, whenever Charles was asked to plead, he refused, stating his objection with the words: "I would know by what power I am called hither, by what lawful authority...?" He claimed that no court had jurisdiction over a monarch, that his own authority to rule had been given to him by God and by the traditional laws of England, and that the power wielded by those trying him was only that of force of arms. Charles insisted that the trial was illegal, explaining that, The court, by contrast, challenged the doctrine of sovereign immunity, and proposed that "the King of England was not a person, but an office whose every occupant was entrusted with a limited power to govern 'by and according to the laws of the land and not otherwise'."

At the end of the third day, Charles was removed from the court, which then heard over 30 witnesses against the king in his absence over the next two days, and on 26 January condemned him to death. The following day, the king was brought before a public session of the commission, declared guilty and sentenced. Fifty-nine of the commissioners signed Charles's death warrant.

Charles's beheading was scheduled for Tuesday, 30 January 1649. Two of his children remained in England under the control of the Parliamentarians: Elizabeth and Henry. They were permitted to visit him on 29 January, and he bade them a tearful farewell. The following morning, he called for two shirts to prevent the cold weather causing any noticeable shivers that the crowd could have mistaken for fear: "the season is so sharp as probably may make me shake, which some observers may imagine proceeds from fear. I would have no such imputation."

He walked under guard from St James's Palace, where he had been confined, to the Palace of Whitehall, where an execution scaffold was erected in front of the Banqueting House. Charles was separated from spectators by large ranks of soldiers, and his last speech reached only those with him on the scaffold. He blamed his fate on his failure to prevent the execution of his loyal servant Strafford: "An unjust sentence that I suffered to take effect, is punished now by an unjust sentence on me." He declared that he had desired the liberty and freedom of the people as much as any, "but I must tell you that their liberty and freedom consists in having government ... It is not their having a share in the government; that is nothing appertaining unto them. A subject and a sovereign are clean different things." He continued, "I shall go from a corruptible to an incorruptible Crown, where no disturbance can be."

At about 2:00 p.m., Charles put his head on the block after saying a prayer and signalled the executioner when he was ready by stretching out his hands; he was then beheaded with one clean stroke. According to observer Philip Henry, a moan "as I never heard before and desire I may never hear again" rose from the assembled crowd, some of whom then dipped their handkerchiefs in the king's blood as a memento.

The executioner was masked and disguised, and there is debate over his identity. The commissioners approached Richard Brandon, the common hangman of London, but he refused, at least at first, despite being offered £200. It is possible he relented and undertook the commission after being threatened with death, but there are others who have been named as potential candidates, including George Joyce, William Hulet and Hugh Peters. The clean strike, confirmed by an examination of the king's body at Windsor in 1813, suggests that the execution was carried out by an experienced headsman.

It was common practice for the severed head of a traitor to be held up and exhibited to the crowd with the words "Behold the head of a traitor!" Although Charles's head was exhibited, the words were not used, possibly because the executioner did not want his voice recognised. On the day after the execution, the king's head was sewn back onto his body, which was then embalmed and placed in a lead coffin.

The commission refused to allow Charles's burial at Westminster Abbey, so his body was conveyed to Windsor on the night of 7 February. He was buried in private in the Henry VIII vault alongside the coffins of Henry VIII and Henry's third wife, Jane Seymour, in St George's Chapel, Windsor Castle, on 9 February 1649. The king's son, Charles II, later planned for an elaborate royal mausoleum to be erected in Hyde Park, London, but it was never built.

Ten days after Charles's execution, on the day of his interment, a memoir purporting to be written by the king appeared for sale. This book, the "Eikon Basilike" (Greek for the "Royal Portrait"), contained an "apologia" for royal policies, and it proved an effective piece of royalist propaganda. John Milton wrote a Parliamentary rejoinder, the "Eikonoklastes" ("The Iconoclast"), but the response made little headway against the pathos of the royalist book. Anglicans and royalists fashioned an image of martyrdom, and in the Convocations of Canterbury and York of 1660 King Charles the Martyr was added to the Church of England's liturgical calendar. High church Anglicans held special services on the anniversary of his death. Churches, such as those at Falmouth and Tunbridge Wells, and Anglican devotional societies such as the Society of King Charles the Martyr, were founded in his honour.

With the monarchy overthrown, England became a republic or "Commonwealth". The House of Lords was abolished by the Rump Commons, and executive power was assumed by a Council of State. All significant military opposition in Britain and Ireland was extinguished by the forces of Oliver Cromwell in the Third English Civil War and the Cromwellian conquest of Ireland. Cromwell forcibly disbanded the Rump Parliament in 1653, thereby establishing the Protectorate with himself as Lord Protector. Upon his death in 1658, he was briefly succeeded by his ineffective son, Richard. Parliament was reinstated, and the monarchy was restored to Charles I's eldest son, Charles II, in 1660.

In the words of John Philipps Kenyon, "Charles Stuart is a man of contradictions and controversy". Revered by high Tories who considered him a saintly martyr, he was condemned by Whig historians, such as Samuel Rawson Gardiner, who thought him duplicitous and delusional. In recent decades, most historians have criticised him, the main exception being Kevin Sharpe who offered a more sympathetic view of Charles that has not been widely adopted. While Sharpe argued that the king was a dynamic man of conscience, Professor Barry Coward thought Charles "was the most incompetent monarch of England since Henry VI", a view shared by Ronald Hutton, who called him "the worst king we have had since the Middle Ages".

Archbishop William Laud, who was beheaded by Parliament during the war, described Charles as "A mild and gracious prince who knew not how to be, or how to be made, great." Charles was more sober and refined than his father, but he was intransigent and deliberately pursued unpopular policies that ultimately brought ruin on himself. Both Charles and James were advocates of the divine right of kings, but while James's ambitions concerning absolute prerogative were tempered by compromise and consensus with his subjects, Charles believed that he had no need to compromise or even to explain his actions. He thought that he was answerable only to God. "Princes are not bound to give account of their actions," he wrote, "but to God alone".


The official style of Charles I as king in England was "Charles, by the Grace of God, King of England, Scotland, France and Ireland, Defender of the Faith, etc." The style "of France" was only nominal, and was used by every English monarch from Edward III to George III, regardless of the amount of French territory actually controlled. The authors of his death warrant referred to him as "Charles Stuart, King of England".


As Duke of York, Charles bore the royal arms of the kingdom differenced by a label Argent of three points, each bearing three torteaux Gules. The Prince of Wales bore the royal arms differenced by a plain label Argent of three points. As king, Charles bore the royal arms undifferenced: Quarterly, I and IV Grandquarterly, Azure three fleurs-de-lis Or (for France) and Gules three lions passant guardant in pale Or (for England); II Or a lion rampant within a tressure flory-counter-flory Gules (for Scotland); III Azure a harp Or stringed Argent (for Ireland). In Scotland, the Scottish arms were placed in the first and fourth quarters with the English and French arms in the second quarter.

Charles had nine children, two of whom eventually succeeded as king, and two of whom died at or shortly after birth.






</doc>
<doc id="7431" url="https://en.wikipedia.org/wiki?curid=7431" title="Counter-Strike (video game)">
Counter-Strike (video game)

Counter-Strike (also known as Half-Life: Counter-Strike) is a first-person shooter video game developed by Valve Corporation. It was initially developed and released as a "Half-Life" modification by Minh "Gooseman" Le and Jess Cliffe in 1999, before Le and Cliffe were hired and the game's intellectual property acquired. "Counter-Strike" was released by Valve on the Microsoft Windows platform in 2000. The game spawned a franchise, and is the first installment in the "Counter-Strike" series. Several remakes and ports of "Counter-Strike" have been released on the Xbox console, as well as OS X and Linux. It is sometimes referred to as Counter-Strike 1.6 to distinguish it from other titles of the series, 1.6 being the final major software update the game received.

Set in various locations around the globe, players assume the roles of members of combating teams of the governmental counter-terrorist forces and various terrorist militants opposing them. During each round of gameplay, the two teams are tasked with defeating the other by the means of either achieving the map's objectives, or else eliminating all of the enemy combatants. Each player may customize their arsenal of weapons and accessories at the beginning of every match, with currency being earned after the end of each round.

"Counter-Strike" is a first-person shooter game in which players join either the terrorist team, the counter-terrorist team, or become spectators. Each team attempts to complete their mission objective and/or eliminate the opposing team. Each round starts with the two teams spawning simultaneously. All players have only one life by default and start with a pistol as well as a knife.

The objectives vary depending on the type of map, and these are the most usual ones:

A player can choose to play as one of eight different default character models (four for each side, although "" added two extra models, bringing the total to ten). Players are generally given a few seconds before the round begins (known as "freeze time") to prepare and buy equipment, during which they cannot attack or move. They can return to the buy area within a set amount of time to buy more equipment (some custom maps included neutral "buy zones" that could be used by both teams). Once the round has ended, surviving players retain their equipment for use in the next round; players who were killed begin the next round with the basic default starting equipment.

Standard monetary bonuses are awarded for winning a round, losing a round, killing an enemy, being the first to instruct a hostage to follow, rescuing a hostage, planting the bomb (Terrorist) or defusing the bomb (Counter-Terrorist).

The scoreboard displays team scores in addition to statistics for each player: name, kills, deaths, and ping (in milliseconds). The scoreboard also indicates whether a player is dead, carrying the bomb (on bomb maps), or is the VIP (on assassination maps), although information on players on the opposing team is hidden from a player until his/her death, as this information can be important.

Killed players become "spectators" for the duration of the round; they cannot change their names before their next spawn, text chat cannot be sent to or received from live players, and voice chat can only be received from live players and not sent to them. Spectators are generally able to watch the rest of the round from multiple selectable views, although some servers disable some of these views to prevent dead players from relaying information about living players to their teammates through alternative media (most notably voice in the case of Internet cafes and Voice over IP programs such as TeamSpeak or Ventrilo). This form of cheating is known as "ghosting."

"Counter-Strike" itself is a mod, and it has developed its own community of script writers and mod creators. Some mods add bots, while others remove features of the game, and others create different modes of play. Some mods, often called "admin plugins", give server administrators more flexible and efficient control over their servers. There are some mods which affect gameplay heavily, such as Gun Game, where players start with a basic pistol and must score kills to receive better weapons, and Zombie Mod, where one team consists of zombies and must "spread the infection" by killing the other team (using only the knife). There are also the Superhero and mods which mix the first-person gameplay of "Counter-Strike" with an experience system, allowing a player to become more powerful as they continue to play. The game is also highly customizable on the player's end, allowing the user to install or even create their own custom skins, HUDs, spray graphics, sprites, and sound effects, given the proper tools.

"Counter-Strike" has been a target for cheating in online games since its release. In-game, cheating is often referred to as "hacking" in reference to programs or "hacks" executed by the client. Valve has implemented an anti-cheat system called Valve Anti-Cheat (VAC). Players cheating on a VAC-enabled server risk having their account permanently banned from all VAC-secured servers.

With the first version of VAC, a ban took hold almost instantly after being detected and the cheater had to wait two years to have the account unbanned. Since VAC's second version, cheaters are not banned automatically. With the second version, Valve instituted a policy of 'delayed bans,' the theory being that if a new hack is developed which circumvents the VAC system, it will spread amongst the 'cheating' community. By delaying the initial ban, Valve hopes to identify and ban as many cheaters as possible. Like any software detection system, some cheats are not detected by VAC. To remedy this, some servers implement a voting system, in which case players can call for a vote to kick or ban the accused cheater. VAC's success at identifying cheats and banning those who use them has also provided a boost in the purchasing of private cheats. These cheats are updated frequently to minimize the risk of detection, and are generally only available to a trusted list of recipients who collectively promise not to reveal the underlying design. Even with private cheats however, some servers have alternative anticheats to coincide with VAC itself. This can help with detecting some cheaters, but most paid for cheats are designed to bypass these alternative server-based anticheats.

When "Counter-Strike" was published by Sierra Studios, it was bundled with "Team Fortress Classic", "" multiplayer, and the "Wanted", "Half-Life: Absolute Redemption" and "Firearms" mods.

On March 24, 1999, Planet Half-Life opened its "Counter-Strike" section. Within two weeks, the site had received 10,000 hits. On June 19, 1999, the first public beta of "Counter-Strike" was released, followed by numerous further "beta" releases. On April 12, 2000, Valve announced that the "Counter-Strike" developers and Valve had teamed up. In January 2013, Valve began testing a version of "Counter-Strike" for OS X and Linux, eventually releasing the update to all users in April 2013.

Upon its retail release, "Counter-Strike" received highly favorable reviews. "The New York Times" reported that E-Sports Entertainment ESEA League started the first professional fantasy e-sports league in 2004 with the game "Counter-Strike". Some credit the move into professional competitive team play with prizes as a major factor in "Counter-Strike" longevity and success.

Global retail sales of "Counter-Strike" surpassed 250,000 units by July 2001, and 1.5 million by December 2003. In the United States alone, its retail version sold 550,000 copies and earned $15.7 million by August 2006, after its release in November 2000. It was the country's 22nd best-selling computer game between January 2000 and August 2006.

On January 17, 2008, a Brazilian federal court order prohibiting all sales of "Counter-Strike" and "EverQuest" began to be enforced. The federal Brazilian judge Carlos Alberto Simões de Tomaz ordered the ban in October 2007 because, as argued by the judge, the games "bring imminent stimulus to the subversion of the social order, attempting against the democratic state and the law and against public security." As of June 18, 2009, a regional federal court order lifting the prohibition on the sale of "Counter-Strike" was published. The game is now being sold again in Brazil.

Following the success of the first "Counter-Strike", Valve went on to make multiple sequels to the game. "", a game using "Counter-Strike's" GoldSrc engine, was released in 2004. "", a remake of the original "Counter-Strike" game, was the first in the series to use Valve's Source engine and was also released in 2004, only eight months after the release of "Counter-Strike: Condition Zero". The next game in the "Counter-Strike" series to be developed primarily by Valve Corporation was "", released for Windows, OS X, Linux, PlayStation 3, and Xbox 360 in 2012.

The game also spawned multiple spin-offs in the form of arcade games developed by Nexon Corporation and targeted primarily at Asian gaming markets. Four "Counter-Strike" games have been developed and released by Nexon Corporation thus far, "Counter-Strike Neo", "Counter-Strike Online", "", and "Counter-Strike Online 2".



</doc>
<doc id="7434" url="https://en.wikipedia.org/wiki?curid=7434" title="Camille Pissarro">
Camille Pissarro

Camille Pissarro (; 10 July 1830 – 13 November 1903) was a Danish-French Impressionist and Neo-Impressionist painter born on the island of St Thomas (now in the US Virgin Islands, but then in the Danish West Indies). His importance resides in his contributions to both Impressionism and Post-Impressionism. Pissarro studied from great forerunners, including Gustave Courbet and Jean-Baptiste-Camille Corot. He later studied and worked alongside Georges Seurat and Paul Signac when he took on the Neo-Impressionist style at the age of 54.

In 1873 he helped establish a collective society of fifteen aspiring artists, becoming the "pivotal" figure in holding the group together and encouraging the other members. Art historian John Rewald called Pissarro the "dean of the Impressionist painters", not only because he was the oldest of the group, but also "by virtue of his wisdom and his balanced, kind, and warmhearted personality". Cézanne said "he was a father for me. A man to consult and a little like the good Lord," and he was also one of Gauguin's masters. Renoir referred to his work as "revolutionary", through his artistic portrayals of the "common man", as Pissarro insisted on painting individuals in natural settings without "artifice or grandeur".

Pissarro is the only artist to have shown his work at all eight Paris Impressionist exhibitions, from 1874 to 1886. He "acted as a father figure not only to the Impressionists" but to all four of the major Post-Impressionists, including Georges Seurat, Paul Cézanne, Vincent van Gogh and Paul Gauguin.

Jacob Abraham Camille Pissarro was born on 10 July 1830 on the island of St. Thomas to Frederick and Rachel Manzano de Pissarro. His father was of Portuguese Jewish descent and held French nationality. His mother was from a French-Jewish family from the island of St. Thomas. His father was a merchant who came to the island from France to deal with the hardware store of a deceased uncle and married his widow. The marriage caused a stir within St. Thomas' small Jewish community because she was previously married to Frederick's uncle and according to Jewish law a man is forbidden from marrying his aunt. In subsequent years his four children were forced to attend the all-black primary school. Upon his death, his will specified that his estate be split equally between the synagogue and St. Thomas' Protestant church.

When Camille was twelve his father sent him to boarding school in France. He studied at the Savary Academy in Passy near Paris. While a young student, he developed an early appreciation of the French art masters. Monsieur Savary himself gave him a strong grounding in drawing and painting and suggested he draw from nature when he returned to St. Thomas, which he did when he was seventeen. However, his father preferred he work in his business, giving him a job working as a cargo clerk. He took every opportunity during those next five years at the job to practise drawing during breaks and after work.

When Pissarro turned twenty-one, Danish artist Fritz Melbye, then living on St. Thomas, inspired him to take on painting as a full-time profession, becoming his teacher and friend. Pissarro then chose to leave his family and job and live in Venezuela, where he and Melbye spent the next two years working as artists in Caracas and La Guaira. He drew everything he could, including landscapes, village scenes, and numerous sketches, enough to fill up multiple sketchbooks. In 1855 he moved back to Paris where he began working as assistant to Anton Melbye, Fritz Melbye's brother.

In Paris he worked as assistant to Danish painter Anton Melbye. He also studied paintings by other artists whose style impressed him: Courbet, Charles-François Daubigny, Jean-François Millet, and Corot. He also enrolled in various classes taught by masters, at schools such as École des Beaux-Arts and Académie Suisse. But Pissarro eventually found their teaching methods "stifling," states art historian John Rewald. This prompted him to search for alternative instruction, which he requested and received from Corot.

His initial paintings were in accord with the standards at the time to be displayed at the Paris Salon, the official body whose academic traditions dictated the kind of art that was acceptable. The Salon's annual exhibition was essentially the only marketplace for young artists to gain exposure. As a result, Pissarro worked in the traditional and prescribed manner to satisfy the tastes of its official committee.

In 1859 his first painting was accepted and exhibited. His other paintings during that period were influenced by Camille Corot, who tutored him. He and Corot both shared a love of rural scenes painted from nature. It was by Corot that Pissarro was inspired to paint outdoors, also called "plein air" painting. Pissarro found Corot, along with the work of Gustave Courbet, to be "statements of pictorial truth," writes Rewald. He discussed their work often. Jean-François Millet was another whose work he admired, especially his "sentimental renditions of rural life".

During this period Pissarro began to understand and appreciate the importance of expressing on canvas the beauties of nature without adulteration. After a year in Paris, he therefore began to leave the city and paint scenes in the countryside to capture the daily reality of village life. He found the French countryside to be "picturesque," and worthy of being painted. It was still mostly agricultural and sometimes called the "golden age of the peasantry". Pissarro later explained the technique of painting outdoors to a student:

Corot, however, would complete his own scenic paintings back in his studio where they would often be revised to his preconceptions. Pissarro, on the other hand, preferred to finish his paintings outdoors, often at one sitting, which gave his work a more realistic feel. As a result, his art was sometimes criticised as being "vulgar," because he painted what he saw: "rutted and edged hodgepodge of bushes, mounds of earth, and trees in various stages of development." According to one source, details such as those were equivalent to today's art showing garbage cans or beer bottles on the side of a street scene. This difference in style created disagreements between Pissarro and Corot.

In 1859, while attending the free school, the Académie Suisse, Pissarro became friends with a number of younger artists who likewise chose to paint in the more realistic style. Among them were Claude Monet, Armand Guillaumin and Paul Cézanne. What they shared in common was their dissatisfaction with the dictates of the Salon. Cézanne's work had been mocked at the time by the others in the school, and, writes Rewald, in his later years Cézanne "never forgot the sympathy and understanding with which Pissarro encouraged him." As a part of the group, Pissarro was comforted from knowing he was not alone, and that others similarly struggled with their art.

Pissarro agreed with the group about the importance of portraying individuals in natural settings, and expressed his dislike of any artifice or grandeur in his works, despite what the Salon demanded for its exhibits. In 1863 almost all of the group's paintings were rejected by the Salon, and French Emperor Napoleon III instead decided to place their paintings in a separate exhibit hall, the Salon des Refusés. However, only works of Pissarro and Cézanne were included, and the separate exhibit brought a hostile response from both the officials of the Salon and the public.

In subsequent Salon exhibits of 1865 and 1866, Pissarro acknowledged his influences from Melbye and Corot, whom he listed as his masters in the catalogue. But in the exhibition of 1868 he no longer credited other artists as an influence, in effect declaring his independence as a painter. This was noted at the time by art critic and author Émile Zola, who offered his opinion:

Another writer tries to describe elements of Pissarro's style:
And though, on orders from the hanging Committee and the Marquis de Chennevières, Pissarro's paintings of Pontoise for example had been skyed, hung near the ceiling, this did not prevent Jules-Antoine Castagnary from noting that the qualities of his paintings had been observed by art lovers. At the age of thirty-eight, Pissarro had begun to win himself a reputation as a landscapist to rival Corot and Daubigny.

In the late 1860s or early 1870s, Pissarro became fascinated with Japanese prints, which influenced his desire to experiment in new compositions. He described the art to his son Lucien:

In 1871 in Croydon he married his mother's maid, Julie Vellay, a vineyard grower's daughter, with whom he would later have seven children. They lived outside Paris in Pontoise and later in Louveciennes, both of which places inspired many of his paintings including scenes of village life, along with rivers, woods, and people at work. He also kept in touch with the other artists of his earlier group, especially Monet, Renoir, Cézanne, and Frédéric Bazille.

After the outbreak of the Franco-Prussian War of 1870–71, having only Danish nationality and being unable to join the army, he moved his family to Norwood, then a village on the edge of London. However, his style of painting, which was a forerunner of what was later called "Impressionism", did not do well. He wrote to his friend, Theodore Duret, that "my painting doesn't catch on, not at all ..."

Pissarro met the Paris art dealer Paul Durand-Ruel, in London, who became the dealer who helped sell his art for most of his life. Durand-Ruel put him in touch with Monet who was likewise in London during this period. They both viewed the work of British landscape artists John Constable and J. M. W. Turner, which confirmed their belief that their style of open air painting gave the truest depiction of light and atmosphere, an effect that they felt could not be achieved in the studio alone. Pissarro's paintings also began to take on a more spontaneous look, with loosely blended brushstrokes and areas of impasto, giving more depth to the work.

Through the paintings Pissarro completed at this time, he records Sydenham and the Norwoods at a time when they were just recently connected by railways, but prior to the expansion of suburbia. One of the largest of these paintings is a view of "St. Bartholomew's Church" at Lawrie Park Avenue, commonly known as "The Avenue, Sydenham", in the collection of the London National Gallery. Twelve oil paintings date from his stay in Upper Norwood and are listed and illustrated in the catalogue raisonné prepared jointly by his fifth child Ludovic-Rodolphe Pissarro and Lionello Venturi and published in 1939. These paintings include "Norwood Under the Snow", and "Lordship Lane Station", views of The Crystal Palace relocated from Hyde Park, "Dulwich College", "Sydenham Hill", "All Saints Church Upper Norwood", and a lost painting of St. Stephen's Church.

Returning to France, in 1890 Pissarro again visited England and painted some ten scenes of central London. He came back again in 1892, painting in Kew Gardens and Kew Green, and also in 1897, when he produced several oils described as being of Bedford Park, Chiswick, but in fact all being of the nearby Stamford Brook area except for one of Bath Road, which runs from Stamford Brook along the south edge of Bedford Park.

When Pissarro returned to his home in France after the war, he discovered that of the 1,500 paintings he had done over 20 years, which he was forced to leave behind when he moved to London, only 40 remained. The rest had been damaged or destroyed by the soldiers, who often used them as floor mats outside in the mud to keep their boots clean. It is assumed that many of those lost were done in the Impressionist style he was then developing, thereby "documenting the birth of Impressionism." Armand Silvestre, a critic, went so far as to call Pissarro "basically the inventor of this [Impressionist] painting"; however, Pissarro's role in the Impressionist movement was "less that of the great man of ideas than that of the good counselor and appeaser ..." "Monet ... could be seen as the guiding force."

He soon reestablished his friendships with the other Impressionist artists of his earlier group, including Cézanne, Monet, Manet, Renoir, and Degas. Pissarro now expressed his opinion to the group that he wanted an alternative to the Salon so their group could display their own unique styles.

To assist in that endeavour, in 1873 he helped establish a separate collective, called the "Société Anonyme des Artistes, Peintres, Sculpteurs et Graveurs," which included fifteen artists. Pissarro created the group's first charter and became the "pivotal" figure in establishing and holding the group together. One writer noted that with his prematurely grey beard, the forty-three-year-old Pissarro was regarded as a "wise elder and father figure" by the group. Yet he was able to work alongside the other artists on equal terms due to his youthful temperament and creativity. Another writer said of him that "he has unchanging spiritual youth and the look of an ancestor who remained a young man".

The following year, in 1874, the group held their first 'Impressionist' Exhibition, which shocked and "horrified" the critics, who primarily appreciated only scenes portraying religious, historical, or mythological settings. They found fault with the Impressionist paintings on many grounds:

Pissarro showed five of his paintings, all landscapes, at the exhibit, and again Émile Zola praised his art and that of the others. In the Impressionist exhibit of 1876; however, art critic Albert Wolff complained in his review, "Try to make M. Pissarro understand that trees are not violet, that sky is not the color of fresh butter ..." Journalist and art critic Octave Mirbeau on the other hand, writes, "Camille Pissarro has been a revolutionary through the revitalized working methods with which he has endowed painting".
According to Rewald, Pissarro had taken on an attitude more simple and natural than the other artists. He writes:

In later years, Cézanne also recalled this period and referred to Pissarro as "the first Impressionist". In 1906, a few years after Pissarro's death, Cézanne, then 67 and a role model for the new generation of artists, paid Pissarro a debt of gratitude by having himself listed in an exhibition catalogue as "Paul Cézanne, pupil of Pissarro".

Pissarro, Degas, and American impressionist Mary Cassatt planned a journal of their original prints in the late 1870s, a project that nevertheless came to nothing when Degas withdrew. Art historian and the artist's great-grandson Joachim Pissarro notes that they "professed a passionate disdain for the Salons and refused to exhibit at them." Together they shared an "almost militant resolution" against the Salon, and through their later correspondences it is clear that their mutual admiration "was based on a kinship of ethical as well as aesthetic concerns".

Cassatt had befriended Degas and Pissarro years earlier when she joined Pissarro's newly formed French Impressionist group and gave up opportunities to exhibit in the United States. She and Pissarro were often treated as "two outsiders" by the Salon since neither were French or had become French citizens. However, she was "fired up with the cause" of promoting Impressionism and looked forward to exhibiting "out of solidarity with her new friends". Towards the end of the 1890s she began to distance herself from the Impressionists, avoiding Degas at times she did not have the strength to defend herself against his "wicked tongue". Instead, she came to prefer the company of "the gentle Camille Pissarro", with whom she could speak frankly about the changing attitudes toward art. She once described him as a teacher "that could have taught the stones to draw correctly."

By the 1880s, Pissarro began to explore new themes and methods of painting to break out of what he felt was an artistic "mire". As a result, Pissarro went back to his earlier themes by painting the life of country people, which he had done in Venezuela in his youth. Degas described Pissarro's subjects as "peasants working to make a living".

However, this period also marked the end of the Impressionist period due to Pissarro's leaving the movement. As Joachim Pissarro points out, "Once such a die-hard Impressionist as Pissarro had turned his back on Impressionism, it was apparent that Impressionism had no chance of surviving ..."

It was Pissarro's intention during this period to help "educate the public" by painting people at work or at home in realistic settings, without idealising their lives. Pierre-Auguste Renoir, in 1882, referred to Pissarro's work during this period as "revolutionary," in his attempt to portray the "common man." Pissarro himself did not use his art to overtly preach any kind of political message, however, although his preference for painting humble subjects was intended to be seen and purchased by his upper class clientele. He also began painting with a more unified brushwork along with pure strokes of color.

In 1885 he met Georges Seurat and Paul Signac, both of whom relied on a more "scientific" theory of painting by using very small patches of pure colours to create the illusion of blended colours and shading when viewed from a distance. Pissarro then spent the years from 1885 to 1888 practising this more time-consuming and laborious technique, referred to as pointillism. The paintings that resulted were distinctly different from his Impressionist works, and were on display in the 1886 Impressionist Exhibition, but under a separate section, along with works by Seurat, Signac, and his son Lucien.

All four works were considered an "exception" to the eighth exhibition. Joachim Pissarro notes that virtually every reviewer who commented on Pissarro's work noted "his extraordinary capacity to change his art, revise his position and take on new challenges." One critic writes:

Pissarro explained the new art form as a "phase in the logical march of Impressionism", but he was alone among the other Impressionists with this attitude, however. Joachim Pissarro states that Pissarro thereby became the "only artist who went from Impressionism to Neo-Impressionism".

In 1884, art dealer Theo van Gogh asked Pissarro if he would take in his older brother, Vincent, as a boarder in his home. Lucien Pissarro wrote that his father was impressed by Van Gogh's work and had "foreseen the power of this artist", who was 23 years younger. Although Van Gogh never boarded with him, Pissarro did explain to him the various ways of finding and expressing light and color, ideas which he later used in his paintings, notes Lucien.

Pissarro eventually turned away from Neo-Impressionism, claiming its system was too artificial. He explains in a letter to a friend:

However, after reverting to his earlier style, his work became, according to Rewald, "more subtle, his color scheme more refined, his drawing firmer ... So it was that Pissarro approached old age with an increased mastery."

But the change also added to Pissarro's continual financial hardship which he felt until his 60s. His "headstrong courage and a tenacity to undertake and sustain the career of an artist", writes Joachim Pissarro, was due to his "lack of fear of the immediate repercussions" of his stylistic decisions. In addition, his work was strong enough to "bolster his morale and keep him going", he writes. His Impressionist contemporaries, however, continued to view his independence as a "mark of integrity", and they turned to him for advice, referring to him as "Père Pissarro" (father Pissarro).

In his older age Pissarro suffered from a recurring eye infection that prevented him from working outdoors except in warm weather. As a result of this disability, he began painting outdoor scenes while sitting by the window of hotel rooms. He often chose hotel rooms on upper levels to get a broader view. He moved around northern France and painted from hotels in Rouen, Paris, Le Havre and Dieppe. On his visits to London, he would do the same.

Pissarro died in Paris on 13 November 1903 and was buried in Père Lachaise Cemetery.

During the period Pissarro exhibited his works, art critic Armand Silvestre had called Pissarro the "most real and most naive member" of the Impressionist group. His work has also been described by art historian Diane Kelder as expressing "the same quiet dignity, sincerity, and durability that distinguished his person." She adds that "no member of the group did more to mediate the internecine disputes that threatened at times to break it apart, and no one was a more diligent proselytizer of the new painting."

According to Pissarro's son, Lucien, his father painted regularly with Cézanne beginning in 1872. He recalls that Cézanne walked a few miles to join Pissarro at various settings in Pontoise. While they shared ideas during their work, the younger Cézanne wanted to study the countryside through Pissarro's eyes, as he admired Pissarro's landscapes from the 1860s. Cézanne, although only nine years younger than Pissarro, said that "he was a father for me. A man to consult and a little like the good Lord."

Lucien Pissarro was taught painting by his father, and described him as a "splendid teacher, never imposing his personality on his pupil." Gauguin, who also studied under him, referred to Pissarro "as a force with which future artists would have to reckon". Art historian Diane Kelder notes that it was Pissarro who introduced Gauguin, who was then a young stockbroker studying to become an artist, to Degas and Cézanne. Gauguin, near the end of his career, wrote a letter to a friend in 1902, shortly before Pissarro's death:

The American impressionist Mary Cassatt, who at one point lived in Paris to study art, and joined his Impressionist group, noted that he was "such a teacher that he could have taught the stones to draw correctly."

Caribbean author and scholar Derek Walcott based his book-length poem, "Tiepolo's Hound" (2000), on Pissarro's life.

During the early 1930s throughout Europe, Jewish owners of numerous fine art masterpieces found themselves forced to give up or sell off their collections for minimal prices due to anti-Jewish laws created by the new Nazi regime. Many Jews were forced to flee Germany. When those forced into exile owned valuables, including artwork, they were often seized by officials for personal gain. In the decades after World War II, many art masterpieces were found on display in various galleries and museums in Europe and the United States. Some, as a result of legal action, were later returned to the families of the original owners. Many of the recovered paintings were then donated to the same or other museums as a gift.

One such lost piece, Pissarro's 1897 oil painting, "Rue St. Honoré, Apres Midi, Effet de Pluie", was discovered hanging at Madrid's government-owned museum, the Museo Thyssen-Bornemisza. In January 2011 the Spanish government denied a request by the US ambassador to return the painting. At the subsequent trial in Los Angeles, the court ruled that the Thyssen-Bornemisza Collection Foundation was the rightful owner. Pissarro's "Le Quai Malaquais, Printemps" is said to have been similarly stolen, while in 1999, Pissarro's 1897 "Le Boulevard de Montmartre, Matinée de Printemps" appeared in the Israel Museum in Jerusalem, its donor having been unaware of its pre-war provenance. In January 2012, "Le Marché aux Poissons" (The Fish Market), a color monotype, was returned after 30 years.

During his lifetime, Camille Pissarro sold few of his paintings. By the 21st century, however, his paintings were selling for millions. An auction record for the artist was set on 6 November 2007 at Christie's in New York, where a group of four paintings, "Les Quatre Saisons" (the Four Seasons), sold for $14,601,000 (estimate $12,000,000 – $18,000,000). In November 2009 "Le Pont Boieldieu et la Gare d'Orléans, Rouen, Soleil" sold for $7,026,500 at Sotheby's in New York. In February 2014 the 1897 "Le Boulevard de Montmartre, Matinée de Printemps", originally owned by the German industrialist and Holocaust victim Max Silberberg (), sold at Sotheby's in London for £19.9M, nearly five times the previous record.

Camille's son Lucien was an Impressionist and Neo-impressionist painter as were his second and third sons Georges Henri Manzana Pissarro and Félix Pissarro. Lucien's daughter Orovida Pissarro was also a painter. Camille's great-grandson, Joachim Pissarro, became Head Curator of Drawing and Painting at the Museum of Modern Art in New York City and a professor in Hunter College's Art Department. Camille's great-granddaughter, Lélia Pissarro, has had her work exhibited alongside her great-grandfather. From the only daughter of Camille, Jeanne Pissarro, other painters include Henri Bonin-Pissarro (1918–2003) and Claude Bonin-Pissarro (born 1921), who is the father of the Abstract artist Frédéric Bonin-Pissarro (born 1964).



In June 2006 publishers Skira/Wildenstein released "Pissarro: Critical Catalogue of Paintings", compiled by Joachim Pissarro (descendant of the painter) and Claire Durand-Ruel Snollaerts (descendant of the French art dealer Paul Durand-Ruel). The 1,500-page, three-volume work is the most comprehensive collection of Pissarro paintings to date, and contains accompanying images of drawings and studies, as well as photographs of Pissarro and his family that had not previously been published. 



</doc>
<doc id="7435" url="https://en.wikipedia.org/wiki?curid=7435" title="Cardiology diagnostic tests and procedures">
Cardiology diagnostic tests and procedures

The diagnostic tests in cardiology are methods of identifying heart conditions associated with healthy vs. unhealthy, pathologic heart function.

Obtaining a medical history is always the first "test", part of understanding the likelihood of significant disease, as detectable within the current limitations of clinical medicine. Yet heart problems often produce no symptoms until very advanced, and many symptoms, such as palpitations and sensations of extra or missing heart beats correlate poorly with relative heart health "vs" disease. Hence, a history alone is rarely sufficient to diagnose a heart condition.

"Auscultation" employs a stethoscope to more easily hear various normal and abnormal sounds, such as normal heart beat sounds and the usual heart beat sound changes associated with breathing versus heart murmurs.

A variety of "blood tests" are available for analyzing cholesterol transport behavior, HDL, LDL, triglycerides, lipoprotein little a, homocysteine, C-reactive protein, blood sugar control: fasting, after eating or averages using glycosylated albumen or hemoglobin, myoglobin, creatine kinase, troponin, brain-type natriuretic peptide, etc. to assess the evolution of coronary artery disease and evidence of existing damage. A great many more physiologic markers related to atherosclerosis and heart function are used and being developed and evaluated in research.
(*) due to the high cost, LDL is usually calculated instead of being measured directly<br>
source: Beyond Cholesterol, Julius Torelli MD, 2005 

"Electrocardiography" (ECG/EKG in German vernacular. Elektrokardiogram) monitors electrical activity of the heart, primarily as recorded from the skin surface. A 12 lead recording, recording the electrical activity in three planes, anterior, posterior, and lateral is the most commonly used form. The ECG allows observation of the heart electrical activity by visualizing waveform beat origin (typically from the sinoatrial or SA node) following down the bundle of HIS and ultimately stimulating the ventricles to contract forcing blood through the body. Much can be learned by observing the QRS morphology (named for the respective portions of the polarization/repolarization waveform of the wave, P,Q,R,S,T wave). Rhythm abnormalities can also be visualized as in slow heart rate bradycardia, or fast heart rate tachycardia.

A "Holter monitor" records a continuous EKG rhythm pattern (rarely a full EKG) for 24 hours or more. These monitors are used for suspected frequent rhythm abnormalities, especially ones the wearer may not recognize by symptoms. They are more expensive than event monitors.

An "event monitor" records short term EKG rhythm patterns, generally storing the last 2 to 5 minutes, adding in new and discarding old data, for 1 to 2 weeks or more. There are several different types with different capabilities. When the wearer presses a button on the monitor, it quits discarding old and continues recording for a short additional period. The wearer then plays the recording, via a standard phone connection, to a center with compatible receiving and rhythm printing equipment, after which the monitor is ready to record again. These monitors are used for suspected infrequent rhythm abnormalities, especially ones the wearer does recognize by symptoms. They are less expensive than Holter monitors.

"Cardiac stress testing" is used to determine to assess cardiac function and to disclose evidence of exertion-related cardiac hypoxia. Radionuclide testing using thallium or technetium can be used to demonstrate areas of perfusion abnormalities. With a maximal stress test the level of exercise is increased until the patient heart rate will not increase any higher, despite increased exercise. A fairly accurate estimate of the target heart rate, based on extensive clinical research, can be estimated by the formula 220 beats per minute minus patient's age. This linear relation is accurate up to about age 30, after which it mildly underestimates typical maximum attainable heart rates achievable by healthy individuals. Other formulas exist, such as that by Miller (217 - (0.85 × Age)) and others . Achieving a high enough heart rate at the end of exercise is critical to improving the sensitivity of the test to detect high grade heart artery stenosis.

The electrophysiology study or EP study is the end all of electrophysiological tests of the heart. It involves a catheter with electrodes probing the endocardium, the inside of the heart, and testing the conduction pathways and electrical activity of individual areas of the heart.

Cardiac imaging techniques include coronary catheterization, echocardiogram, intravascular ultrasound, and the coronary calcium scan.



</doc>
<doc id="7437" url="https://en.wikipedia.org/wiki?curid=7437" title="Carlo Collodi">
Carlo Collodi

Carlo Lorenzini, better known by the pen name Carlo Collodi (; 24 November 1826 – 26 October 1890), was an Italian author and journalist, widely known for his world-renowned fairy tale novel "The Adventures of Pinocchio".

Collodi was born in Florence on 24 November 1826. He spent most of his childhood in the town of Collodi where his mother was born.
His mother was a farmer's daughter and his father was a cook.
He had 10 siblings but seven died at a young age.

During the Italian wars of Independence in 1848 and 1860 Collodi served as a volunteer with the Tuscan army. His active interest in political matters may be seen in his earliest literary works as well as in the founding of the satirical newspaper "Il Lampione" in 1853. This newspaper was censored by order of the Grand Duke of Tuscany. In 1854 he published his second newspaper, "Lo scaramuccia" ("The Controversy").

Lorenzini's first publications were in his periodicals. A debut came in 1856 with the play "Gli amici di casa" and parodic guidebook "Un romanzo in vapore", both in 1856; he had also begun intense activity on other political newspapers such as "Il Fanfulla"; at the same time he was employed by the Censorship Commission for the Theatre. During this period he composed various satirical sketches and stories (sometimes simply by collating earlier articles), including "Macchiette" (1880), "Occhi e nasi" (1881), "Storie allegre" (1887).

In 1875, he entered the domain of children's literature with "Racconti delle fate", a translation of French fairy tales by Perrault. In 1876 Lorenzini wrote "Giannettino" (inspired by Alessandro Luigi Parravicini's "Giannetto"), the "Minuzzolo", and "Il viaggio per l'Italia di Giannettino", a pedagogic series which explored the unification of Italy through the ironic thoughts and actions of the character Giannettino.

Lorenzini became fascinated by the idea of using an amiable, rascally character as a means of expressing his own convictions through allegory. In 1880 he began writing "Storia di un burattino" ("The story of a marionette"), also called "Le avventure di Pinocchio", which was published weekly in "Il Giornale per i Bambini", the first Italian newspaper for children. Pinocchio was adapted into a 1940 film by Disney that is considered to be one of Disney's greatest.

Lorenzini died in Florence in 1890



</doc>
<doc id="7439" url="https://en.wikipedia.org/wiki?curid=7439" title="Constructible number">
Constructible number

In geometry and algebra, a real number is constructible if and only if, given a line segment of unit length, a line segment of length || can be constructed with compass and straightedge in a finite number of steps. Not all real numbers are constructible and to describe those that are, algebraic techniques are usually employed. However, in order to employ those techniques, it is useful to first associate points with constructible numbers.

A point in the Euclidean plane is a constructible point if it is either endpoint of the given unit segment, or the point of intersection of two lines determined by previously obtained constructible points, or the intersection of such a line and a circle having a previously obtained constructible point as a center passing through another constructible point, or the intersection of two such circles. Now, by introducing cartesian coordinates so that one endpoint of the given unit segment is the origin and the other at , it can be shown that the coordinates of the constructible points are constructible numbers.

In algebraic terms, a number is constructible if and only if it can be obtained using the four basic arithmetic operations and the extraction of square roots, but of no higher-order roots, from constructible numbers, which always include 0 and 1. The set of constructible numbers can be completely characterized in the language of field theory: the constructible numbers form the quadratic closure of the rational numbers: the smallest field extension that is closed under square roots. This has the effect of transforming geometric questions about compass and straightedge constructions into algebra. This transformation leads to the solutions of many famous mathematical problems, which defied centuries of attack.

The traditional approach to the subject of constructible numbers has been geometric in nature, but this is not the only approach. However, the geometric approach does provide the motivation for the algebraic definitions and is historically the way the subject developed. In presenting the material in this manner, the basic ideas are introduced synthetically and then coordinates are introduced to transition to an algebraic setting.

Let and be two given distinct points in the Euclidean plane. The set of points that can be constructed with compass and straightedge starting with and will be denoted by and whose elements will be called constructible points. and are, by definition, elements of . To more precisely describe the elements of , we make the following two definitions:
Then, the points of , besides and are:

As an example, the midpoint of constructed segment is a constructible point. To see this, note that the constructed circle with center and passing through intersects the constructed circle with center and passing through at the constructible points and . The intersection of constructed segment with constructed segment is the desired constructed midpoint.

A Cartesian coordinate system can be introduced where the point is associated to the origin having coordinates and the point is associated with . The points of may now be used to link the geometry and algebra, namely, we define

Due to point , 0 and 1 are constructible numbers. Let be a point in , that is, a constructible point. If lies on the -axis, then is a constructed segment and the first coordinate of is, in absolute value, the length of this constructed segment. If does not lie on the -axis then let the foot of the perpendicular from to the -axis be the point . The point is a constructed point, so and are constructed segments. The absolute values of the coordinates of the point are therefore lengths of constructed segments. This process is reversible, so it is possible to use this device to provide an alternate characterization of constructible numbers, namely:

If and are the non-zero lengths of constructed segments then elementary compass and straightedge constructions can be used to obtain constructed segments of lengths , (if ), and . The latter two can be done with a construction based on the intercept theorem. A slightly less elementary construction using these tools is based on the geometric mean theorem and will construct a segment of length from a constructed segment of length .

If and are constructible numbers with , then , , , and , for non-negative , are constructible. Thus, the set of constructible real numbers form a field. Furthermore, since 1 is a constructible number, all rational numbers are constructible and is a (proper) subfield of the field of constructible numbers. Also, any constructible number is an algebraic number. More precisely, 
Using slightly different terminology, a real number is constructible if and only if it lies in a field at the top of a finite tower of quadratic extensions, starting with the rational field . More precisely, is constructible if and only if there exists a tower of fields

formula_1

where is in and for all , .

For yet another formulation of this result, this time using the geometric definition of a constructible point, let be a non-empty set of points in and the subfield of generated by all the coordinates of points in . If a point is constructible from the points of , then the degrees and are powers of 2.

Using the natural correspondence between points of and complex numbers (namely, ) some authors prefer to phrase results in the complex setting by defining:
It can then be shown, in a manner analogous to the real case, that a complex number is constructible if and only if it lies in a field at the top of a finite tower of complex quadratic extensions, starting with the field . More precisely, is constructible if and only if there exists a tower of complex fields

formula_2

where is in and for all , .

Consequently, if a complex number is constructible, then is a power of two. 

This algebraic characterization of constructible numbers provides an important "necessary" condition for constructibility: if is constructible, then it is algebraic, and its minimal irreducible polynomial has degree a power of 2, which is equivalent to the statement that the field extension has dimension a power of 2. Note however, that the converse is false — this is not a "sufficient" condition for constructibility as there are non-constructible numbers with . 
Trigonometric numbers are irrational cosines or sines of angles that are rational multiples of . Such a number is constructible if and only if the denominator of the fully reduced multiple is a power of or the product of a power of with the product of one or more distinct Fermat primes. Thus, for example, Is constructible because is the product of two Fermat primes, and .

See here a list of trigonometric numbers expressed in terms of square roots.

The ancient Greeks thought that certain construction problems they could not solve were simply obstinate, not unsolvable. However, the non-constructibility of certain numbers proves them to be logically impossible to perform. (The problems themselves, however, are solvable, and the Greeks knew how to solve them, "without" the constraint of working only with straightedge and compass.)

In the following chart, each row represents a specific ancient construction problem. The left column gives the name of the problem. The second column gives an equivalent algebraic formulation of the problem. In other words, the solution to the problem is affirmative if and only if each number in the given set of numbers is constructible. Finally, the last column provides a simple counterexample. In other words, the number in the last column is an element of the set in the same row, but is not constructible.

The birth of the concept of constructible numbers is inextricably linked with the history of the three impossible compass and straightedge constructions: duplicating the cube, trisecting an angle, and squaring the circle. The restriction of using only compass and straightedge in geometric constructions is often credited to Plato due to a passage in Plutarch. According to Plutarch, Plato gave the duplication of the cube (Delian) problem to Eudoxus and Archytas and Menaechmus, who solved the problem using mechanical means, earning a rebuke from Plato for not solving the problem using pure geometry (Plut., "Quaestiones convivales" VIII.ii, 718ef). However, this attribution is challenged, due, in part, to the existence of another version of the story (attributed to Eratosthenes by Eutocius of Ascalon) that says that all three found solutions but they were too abstract to be of practical value. Since Oenopides (circa 450 BCE) is credited with two ruler and compass constructions, by Proclus– citing Eudemus (circa 370 - 300 BCE)–when other methods were available to him, has led some authors to hypothesize that Oenopides originated the restriction. 

The restriction to compass and straightedge is essential in making these constructions impossible. Angle trisection, for instance, can be done in many ways, several known to the ancient Greeks. The Quadratrix of Hippias of Elis, the conics of Menaechmus, or the marked straightedge (neusis) construction of Archimedes have all been used and we can add a more modern approach via paper folding to the list.
Although not one of the classic three construction problems, the problem of constructing regular polygons with straightedge and compass is usually treated alongside them. The Greeks knew how to construct regular -gons with (for any integer ) or the product of any two or three of these numbers, but other regular -gons eluded them. Then, in 1796, an eighteen-year-old student named Carl Friedrich Gauss announced in a newspaper that he had constructed a regular 17-gon with straightedge and compass. Gauss' treatment was algebraic rather than geometric; in fact, he did not actually construct the polygon, but rather showed that the cosine of a central angle was a constructible number. The argument was generalized in his 1801 book "Disquisitiones Arithmeticae" giving the "sufficient" condition for the construction of a regular -gon. Gauss claimed, but did not prove, that the condition was also necessary and several authors, notably Felix Klein, attributed this part of the proof to him as well. 

In a paper from 1837, Pierre Laurent Wantzel proved algebraically that the problems of
are impossible to solve if one uses only compass and straightedge. In the same paper he also solved the problem of determining which regular polygons are constructible:

An attempted proof of the impossibility of squaring the circle was given by James Gregory in "Vera Circuli et Hyperbolae Quadratura" (The True Squaring of the Circle and of the Hyperbola) in 1667. Although his proof was faulty, it was the first paper to attempt to solve the problem using algebraic properties of . It was not until 1882 that Ferdinand von Lindemann rigorously proved its impossibility, by extending the work of Charles Hermite and proving that is a transcendental number.

The study of constructible numbers, per se, was initiated by René Descartes in La Geometrie, an appendix to his book "Discourse on the Method" published in 1637. Descartes associated numbers to geometrical line segments in order to display the power of his philosophical method by solving an ancient straightedge and compass construction problem put forth by Pappus.





</doc>
<doc id="7441" url="https://en.wikipedia.org/wiki?curid=7441" title="Carson City, Nevada">
Carson City, Nevada

Carson City, officially the Consolidated Municipality of Carson City, is an independent city and the capital of the US state of Nevada, named after the mountain man Kit Carson. As of the 2010 census, the population was 55,274. The majority of the town's population lives in Eagle Valley, on the eastern edge of the Carson Range, a branch of the Sierra Nevada, about south of Reno. 

The town began as a stopover for California bound emigrants, but developed into a city with the Comstock Lode, a silver strike in the mountains to the northeast. The city has served as Nevada's capital since statehood in 1864 and for much of its history was a hub for the Virginia and Truckee Railroad, although the tracks were removed in the 1950s. Before 1969, Carson City was the county seat of Ormsby County. In 1969, the county was abolished, and its territory merged with Carson City to form the Consolidated Municipality of Carson City. With the consolidation, the city limits extend west across the Sierra Nevada to the California state line in the middle of Lake Tahoe. Like other independent cities in the United States, it is treated as a county-equivalent for census purposes.

The Washoe people have inhabited the valley and surrounding areas for about 6,000 years.

The first European Americans to arrive in what is now known as Eagle Valley were John C. Frémont and his exploration party in January 1843. Fremont named the river flowing through the valley Carson River in honor of Kit Carson, the mountain man and scout he had hired for his expedition. Later, settlers named the area Washoe in reference to the indigenous people.

By 1851 the Eagle Station ranch along the Carson River was a trading post and stopover for travelers on the California Trail's Carson Branch which ran through Eagle Valley. The valley and trading post received their name from a bald eagle that was hunted and killed by one of the early settlers and was featured on a wall inside the post.

As the area was part of the Utah Territory, it was governed from Salt Lake City, where the territorial government was headquartered. Early settlers bristled at the control by Mormon-influenced officials and desired the creation of the Nevada territory. A vigilante group of influential settlers, headed by Abraham Curry, sought a site for a capital city for the envisioned territory. In 1858, Abraham Curry bought Eagle Station and thereafter renamed the settlement Carson City. As Curry and several other partners had Eagle Valley surveyed for development. Curry decided Carson City would someday serve as the capital city and left a plot in the center of town for a capitol building.

After gold and silver were discovered in 1859 on nearby Comstock Lode, Carson City's population began to grow. Curry built the Warm Springs Hotel a mile to the east of the city center. When territorial governor James W. Nye traveled to Nevada, he chose Carson City as the territorial capital, influenced by Carson City lawyer William Stewart, who escorted him from San Francisco to Nevada. As such, Carson City bested Virginia City and American Flat. Curry loaned the Warm Springs Hotel to the territorial Legislature as a meeting hall. The Legislature named Carson City to be the seat of Ormsby County and selected the hotel as the territorial prison with Curry serving as its first warden. Today the property is still part of the state prison.

When Nevada became a state in 1864 during the American Civil War, Carson City was confirmed as Nevada's permanent capital. Carson City's development was no longer dependent on the mining industry and instead became a thriving commercial center. The Virginia and Truckee Railroad was built between Virginia City and Carson City. A log flume was also built from the Sierra Nevadas into Carson City. The current capitol building was constructed from 1870 to 1871. The United States Mint operated the Carson City Mint between the years 1870 and 1893, which struck gold and silver coins. People came from China during that time, many to work on the railroad. Some of them owned businesses and taught school. By 1880, almost a thousand Chinese people, "one for every five Caucasians", has lived in Carson City.

Carson City's population and transportation traffic decreased when the Central Pacific Railroad built a line through Donner Pass, too far to the north to benefit Carson City. The city was slightly revitalized with the mining booms in Tonopah and Goldfield. The US federal building (now renamed the Paul Laxalt Building) was completed in 1890 as was the Stewart Indian School. Even these developments could not prevent the city's population from dropping to just over 1,500 people by 1930. Carson City resigned itself to small city status, advertising itself as "America's smallest capital". The city slowly grew after World War II; by 1960 it had reached its 1880 boom-time population.

As early as the late 1940s, discussions began about merging Ormsby County and Carson City. By this time, the county was little more than Carson City and a few hamlets to the west. However, the effort did not pay off until 1966, when a statewide referendum approved the merger. The required constitutional amendment was passed in 1968. On April 1, 1969; Ormsby County and Carson City officially merged as the Consolidated Municipality of Carson City. With this consolidation, Carson City absorbed former town sites such as Empire City, which had grown up in the 1860s as a milling center along the Carson River and current U.S. Route 50. Carson City could now advertise itself as one of America's largest state capitals with its of city limits.

In 1991, the city adopted a downtown master plan, specifying no building within of the capitol would surpass it in height. This plan effectively prohibited future high-rise development in the center of downtown. The Ormsby House is the tallest building in downtown Carson City, at a height of . The structure was completed in 1972.

Carson City features a semi-arid climate with cool but not inordinately cold winters and hot summers. The city is in a high desert river valley approximately above sea level. There are four fairly distinct seasons, all of which are relatively mild compared to many parts of the country and to what one may expect given its elevation. Winters see typically light to moderate snowfall, with a median of . Most precipitation occurs in winter and spring, with summer and fall being fairly dry, drier than neighboring California. There are 37 days of + highs annually, with + temperatures occurring in some years.

The average temperature in Carson City increased by between 1984 and 2014, a greater change than in any other city in the United States.

The Carson River flows from Douglas County through the southwestern edge of Carson City.




Carson City is the smallest of the United States' 366 metropolitan statistical areas.

As of the 2010 census there are 55,274 people, 20,171 households, and 13,252 families residing in the city. The population density is 366 people per square mile (141/km). There are 21,283 housing units at an average density of 148/sq mi (57/km). The racial makeup of the city is 81.1% White, 1.9% Black or African American, 2.4% Native American, 2.1% Asian, 0.2% Pacific Islander, 9.4% from other races, and 2.9% from two or more races. 21% of the population are Hispanic or Latino of any race.

As of the 2000 census, there are 20,171 households, out of which 29.8% have children under the age of 18 living with them, 50.0% are married couples living together, 11.0% have a female householder with no husband present, and 34.3% are non-families. 27.8% of all households are made up of individuals and 11.00% have someone living alone who is 65 years of age or older. The average household size is 2.44 and the average family size is 2.97. The city's age distribution is: 23.4% under the age of 18, 7.9% from 18 to 24, 28.9% from 25 to 44, 24.9% from 45 to 64, and 14.9% who are 65 years of age or older. The median age is 39 years. For every 100 females there are 106.90 males. For every 100 females age 18 and over, there are 108.20 males.

Data from the 2000 census indicates the median income for a household in the city is $41,809, and the median income for a family is $49,570. Males have a median income of $35,296 versus $27,418 for females. The per capita income for the city is $20,943. 10.0% of the population and 6.9% of families are below the poverty line. Out of the total population, 13.7% of those under the age of 18 and 5.8% of those 65 and older are living below the poverty line.

As of 2010, 82.31% (42,697) of Carson City residents age 5 and older spoke English at home as a first language, while 14.12% (7,325) spoke Spanish, 0.61% (318) French, and numerous Indo-Aryan languages were spoken as a main language by 0.50% (261) of the population over the age of five. In total, 17.69% (9,174) of Carson City's population age 5 and older spoke a first language other than English.
Ormsby County consolidated with Carson City in 1969, and the county simultaneously dissolved. The city is now governed by a five-member board of supervisors, consisting of a mayor and four supervisors. All members are elected at-large, but each of the four supervisors must reside in respective wards, numbered 1 through 4. The mayor and supervisors serve four year terms. Elections are staggered so the mayor and the supervisors from Wards 2 and Ward 4 are elected in presidential election years, and the supervisors from Ward 1 and 3 are elected in the even-numbered years in between (i.e., the same year as gubernatorial elections).

Nevada's capital is generally considered a Republican stronghold, often voting for Republicans by wide margins. In 2004, George Bush defeated John Kerry 57-40%. In 2008 however Barack Obama became the first Democrat since 1964 to win Ormsby County/Carson City, defeating John McCain 49% to 48%, by 204 votes, a margin of under 1%.

Carson City, being the state capital, is home to many political protests and demonstrations at any given time.

In an attempt to either make proposed spent nuclear fuel storage facility at Yucca Mountain prohibitively expensive (by raising property tax rates to the maximum allowed) or to allow the state to collect the potential federal payments of property taxes on the facility, the state government in 1987 carved Yucca Mountain out of Nye County and created a new county with no residents out of the area surrounding Yucca called Bullfrog County. Carson City became the county seat of Bullfrog County, even though it is not in Bullfrog County and is more than from Yucca Mountain. A state judge found the process unconstitutional in 1989, and Bullfrog County's territory was retroceded to Nye County.

Carson City has never hosted any professional team sports. However, a variety of sports are offered at parks and recreation. Many neighborhood parks offers a wide variety of features, including picnic tables, beaches, restrooms, fishing, softball, basketball hoops, pond, tennis, and volleyball. The largest park is Mills Park, which has a total land area of and includes the narrow-gauge Carson & Mills Park Railroad.
While there are no ski slopes within Carson City, the city is near Heavenly Mountain Resort, Diamond Peak and Mount Rose Ski Tahoe skiing areas.

Carson City has served as one of the state’s centers for politics and business. Every state governor since Denver S. Dickerson has resided in the Governor's Mansion in Carson City. "See also: List of Governors of Nevada." The following personalities took up a residence in Carson City at some point in their lives.

The following is a list of the top employers in Carson City from the fourth quarter of 2012:

1,500 - 1,999 employees
1,000 - 1,499 Employees
500 - 999 employees
200 - 499 employees
100-199 employees

There are two highways in the city U.S. Route 395 and U.S. Route 50. Carson City is home to Interstate 580, its only freeway. Phase 1 of the Carson City Freeway Project from US 395, just north of the city, to US 50 was completed in February 2006 and Phase 2A, extending from Rt. 50 to Fairview Drive, was officially opened on September 24, 2009. Phase 2B, Fairview Drive to Rt. 50, was completed in August 2017. Prior to 2012, Carson City was one of only five state capitals not directly served by an Interstate highway; the city lost this distinction when I-580 was extended into the city limits.

Carson City's first modern bus system, Jump Around Carson, or JAC, opened to the public in October 2005. JAC uses a smaller urban bus ideal for Carson City. However, there is virtually no ground public transportation to other destinations. Passenger trains haven't served Carson City since 1950, when the Virginia and Truckee Railroad was shut down. Greyhound Lines stopped their bus services to the town in 2006 and Amtrak discontinued their connecting thruway bus to Sacramento, California in 2008. There is now only a limited Monday – Friday RTC bus service to Reno which is still served by both Greyhound and Amtrak.

Carson City is also served by the Carson Airport, which is a regional airport in the northern part of the city. Reno–Tahoe International Airport, which is away, handles domestic commercial flights.

The Carson City School District operates ten schools in Carson City. The six elementary schools are Bordewich-Bray Elementary School, Empire Elementary School, Fremont Elementary School, Fritsch Elementary School, Mark Twain Elementary School, and Al Seeliger Elementary School. The two middle schools are Carson Middle School and Eagle Valley Middle School. Carson High School and the alternative Pioneer High School serve high school students. Carson High is on Saliman Road.

Western Nevada College (WNC) is a regionally accredited, two-year and four-year institution which is part of the Nevada System of Higher Education. The college offers many programs including education, arts and science.




</doc>
<doc id="7442" url="https://en.wikipedia.org/wiki?curid=7442" title="Clark Kent">
Clark Kent

Clark Joseph Kent is a fictional character appearing in American comic books published by DC Comics. Created by Jerry Siegel and Joe Shuster, he debuted in "Action Comics" #1 (June 1938) and serves as the civilian and secret identity of the superhero Superman.

Over the decades there has been considerable debate as to which personality the character identifies with most. From his first introduction in 1938 to the mid-1980s, "Clark Kent" was seen mostly as a disguise for Superman, enabling him to mix with ordinary people. This was the view in most comics and other media such as movie serials and TV (e.g., in "Atom Man vs. Superman" starring Kirk Alyn and "The Adventures of Superman" starring George Reeves) and radio. In 1986, during John Byrne's revamping of the character, Clark Kent became more emphasized. Different takes persist in the present, with the character typically depicted as being clumsy and mild-mannered.

As Superman's alter ego, the personality, concept, and name of Clark Kent have become ingrained in popular culture as well, becoming synonymous with secret identities and innocuous fronts for ulterior motives and activities. In 1992, Superman co-creator Joe Shuster told the "Toronto Star" that the name derived from 1930s cinematic leading men Clark Gable and Kent Taylor, but the persona from bespectacled silent film comic Harold Lloyd and himself. Another, perhaps more likely possibility, is that Jerry Siegel pulled from his own love of pulp heroes Doc Clark Savage and The Shadow alias Kent Allard. This idea was notably stated in the book "Men of Tomorrow: Geeks, Gangsters, and the Rise of the American Comic Book".

Clark's middle name is given variously as either Joseph, Jerome, or Jonathan, all being allusions to creators Jerry Siegel and Joe Shuster.

In the earliest "Superman" comics, Clark Kent's primary purpose was to fulfill the perceived dramatic requirement that a costumed superhero cannot remain on full duty all the time. Clark thus acted as little more than a front for Superman's activities. Although his name and history were taken from his early life with his adoptive Earth parents, everything about Clark was staged for the benefit of his alternate identity: as a reporter for the "Daily Planet", he receives late-breaking news before the general public, has a plausible reason to be present at crime scenes, and need not strictly account for his whereabouts as long as he makes his story deadlines. He sees his job as a journalist as an extension of his Superman responsibilities—bringing truth to the forefront and fighting for the little guy. He believes that everybody has the right to know what is going on in the world, regardless of who is involved.

To deflect suspicion that he is Superman, Clark Kent adopted a largely passive and introverted personality with conservative mannerisms, a higher-pitched voice, and a slight slouch. This personality is typically described as "mild-mannered", perhaps most famously by the opening narration of Max Fleischer's "Superman" animated theatrical shorts. These traits extended into Clark's wardrobe, which typically consists of a bland-colored business suit, a red necktie, black-rimmed glasses (which in Pre-Crisis stories had lenses of Kryptonian material that would not be damaged when he fired his heat vision through them), combed-back hair, and occasionally a fedora.

Fellow reporter Lois Lane became the object of Clark's/Superman's romantic affection. Lois' affection for Superman and her rejection of Clark's clumsy advances have been a recurring theme in Superman comics, as well as in movies and on television.

Clark wears his Superman costume underneath his street clothes, allowing easy changes between the two personae and the dramatic gesture of ripping open his shirt to reveal the familiar "S" emblem when called into action. Superman usually stores his Clark Kent clothing compressed in a secret pouch within his cape, though some stories have shown him leaving his clothes in some covert location (such as the "Daily Planet" storeroom) for later retrieval.

In the Pre-Crisis Superman comic book, Clark appears in occasional back-up stories called "The Private Life of Clark Kent", wherein he solves problems subtly as Clark without changing into Superman. The feature was later shown in the "Superman Family" title.

Adopted by Jonathan and Martha Kent from the Kansas town of Smallville, Clark (and thus Superman) was raised with the values of a typical rural American town, including attending the local Methodist Church (though it is debated by comic fans if Superman is a Methodist).

Most continuities state that the Kents never had biological children of their own and were usually depicted as middle-aged or elderly when they found Clark. In the Golden and Silver Age versions of his origin, after the Kents retrieved Clark from his rocket, they brought him to the Smallville Orphanage and returned a few days later to formally adopt the orphan, giving him as a first name Martha's maiden name, "Clark". In John Byrne's 1986 origin version "The Man of Steel," instead of adopting him through an orphanage, the Kents passed Clark off as their own child after their farm was isolated for months by a series of snowstorms that took place shortly after they found his rocket, using their past medical history of various miscarriages to account for their reasons for keeping Martha's pregnancy secret.

In the Silver Age comics continuity, Clark's superpowers manifested upon his landing on Earth and he gradually learned to master them, adopting the superhero identity of Superboy at the age of eight. He subsequently developed Clark's timid demeanor as a means of ensuring that no one would suspect any connection between the two alter-egos.

In the wake of John Byrne's reboot of Superman continuity in "The Man of Steel", many traditional aspects of Clark Kent were dropped in favor of giving him a more aggressive and extroverted personality (although not as strong as Lois's), including such aspects as making Clark a top football player in high school along with being a successful author and Pulitzer Prize-winning writer, which includes at least two original novels, "The Janus Contract", and "Under a Yellow Sun". Furthermore, Clark's motivations for his professional writing were deepened as both a love for the art that "contributes at least as much social good as his Superman activities" and as a matter of personal fulfillment in an intellectual field in which his abilities give no unfair competition to his colleagues beyond typing extraordinarily fast. Following "One Year Later", Clark adopts some tricks to account for his absences, such as feigning illness or offering to call the police. These, as well as his slouching posture, are references to his earlier mild-mannered Pre-Crisis versions, but he still maintains a sense of authority and his assertive self. Feeling that Clark is the real person and that Clark is not afraid to be himself in his civilian identity, John Byrne has stated in interviews that he took inspiration for this portrayal from the George Reeves version of Superman.

Clark's favorite movie is "To Kill a Mockingbird" (in which Gregory Peck wears glasses not unlike Kent's). According to the "DC Comics Official Guide to Superman," Clark enjoys peanut butter and jelly sandwiches, football games, and the smell of Kansas in the springtime. His favorite baseball team is the Metropolis Monarchs and his favorite football team is the Metropolis Sharks. As of "One Year Later", Clark is in his mid-thirties, stands at , and weighs about . Unlike in the Silver Age, his powers developed over several years, only coming to their peak when he was an adult.

Superman's secret identity as Clark Kent is one of the DC Universe's greatest secrets. Only a few trusted people are aware of it, such as Batman and other members of the Justice League, Superman's cousin Supergirl, and Clark's childhood friend Lana Lang. (In pre-Crisis stories, Lana did not know, but their friend Pete Ross did, unbeknownst to anyone, including Clark.) Lex Luthor, other supervillains, and various civilians have learned the secret identity several times, though their knowledge is usually removed through various means (the boxer Muhammad Ali is one of the very few to deduce the identity and retain the knowledge).

Traditionally, Lois Lane (and sometimes others) would often suspect Superman of truly being Clark Kent; this was particularly prominent in Silver Age stories, including those in the series "Superman's Girl Friend Lois Lane". More recent stories (post-Crisis) often feature the general public assuming that Superman has no secret identity owing to the fact that he, unlike most heroes, does not wear a mask. In "The Secret Revealed", a supercomputer constructed by Lex Luthor calculated Superman's true identity from information that had been assembled by his staff, but Lex dismissed the idea because he could not believe that someone so powerful would want another, weaker identity. In post-"Crisis" continuity, Lois Lane, feeling that someone like Clark could not be Superman, never suspected the dual identity beyond one isolated incident before Clark finally revealed it to her. In "Visitor", Lois finds Superman at the Kent farm with Lana Lang and asks him point-blank if he is Clark Kent. Before he can answer, the Kents tell her that they raised Superman alongside Clark like a brother. In the 2009 retcon of the mythos, Lois Lane is fully aware from the beginning, along with Perry White, that the meek, pudgy, and bumbling Clark Kent deliberately holds himself back: however, still far from associating him with Superman, they simply believe he's hiding his qualities as a good reporter. 

In the current continuity established by DC's New 52 relaunch in 2011, Lois Lane remains unaware that Clark is Superman until she discovers his identity as part of an elaborate plan by new villain Hordr-Root—later revealed to be a son of Vandal Savage—to blackmail Superman at the same time as he is experiencing a strange power shortage, with his abilities gradually draining each time he uses them. Shortly before this trouble began, Superman also revealed his identity to Jimmy Olsen. In an attempt to protect Superman from his Hordr-Root's blackmail efforts, Lois reveals his identity herself, forcing Clark to go off-grid to find the source of his power shortage after his "Daily Planet" colleagues abandon him in disgust at his lies. This storyline concludes with the New 52 Superman being forced to subject himself to a form of kryptonite chemotherapy to restore his powers and defeat Savage's latest plot, only to subsequently die of kryptonite poisoning. 

Following the death of the New-52 Superman, he is 'replaced' as Superman by his counterpart from the pre-"Flashpoint" timeline—who operates in secret in a farmhouse with his wife and son after the events of "Convergence" saw them trapped in this new universe—but all are left puzzled when a completely human Clark Kent appears. Superman's tests confirming that this new man genuinely believes that he is Clark Kent, adopted by the Kents after his biological parents died in a gas explosion when he was three months old, who went into hiding while investigating the mysterious Geneticorp and allowed Superman to claim to be him to divert attention from his own actions. However, when Lois—replacing her apparently dead New 52 counterpart—accepts a date with this Clark to find out more about him, she is unnerved when he goes all out on their date, hiring a limo and booking an expensive restaurant to propose to her. When Lois leaves, she is followed by 'Clark' back to the house she lives in with her family, with 'Clark' being so outraged at the sight of Lois with someone else that he triggers an attack that somehow erases the Kents' entire house and their son Jon, subsequently revealing that he is really Mister Mxyzptlk, having subjected himself to an intense spell that made him literally "believe" he was Clark Kent for a time. This confrontation with Mxyzptlk concludes with the essence of pre-"Flashpoint" Clark and Lois merging with the essences of their "New 52" counterparts, allowing them to exist in the new universe with a mixture of their pre-"Flashpoint" and "New 52" histories, although Mxyzptlk's actions have allowed Superman to resume his identity as Clark.

In the 2016 miniseries American Alien, written by Max Landis. Clark's powers and the fact he is Superman's alter ego is an open secret in Smallville. This is due primarily to power flare ups that happened in his childhood, such as spontaneously beginning to float in front of his friends at a drive through movie. The Smallville Police Department even approach a young Clark to ask for his help solving a local murder.

In the future of the Legion of Super-Heroes, his secret identity is historical fact, with exhibits at a Superman Museum depicting the hero and his friends' and family's adventures.

Various explanations over the decades have been offered for why people have never suspected Superman and Clark Kent of being one and the same:

When crises arise, Clark quickly changes into Superman. Originally during his appearances in "Action Comics" and later in his own magazine, the Man of Steel would strip to his costume and stand revealed as Superman, often with the transformation having already been completed. But within a short time, Joe Shuster and his ghost artists began depicting Clark Kent ripping open his shirt to reveal the "S" insignia on his chest—an image that became so iconic that other superheroes, during the Golden Age and later periods, would copy the same type of change during transformations.

In the Fleischer theatrical cartoons released by Paramount, the mild-mannered reporter often ducked into a telephone booth or stockroom to make the transformation. Since the shorts were produced during the rise of film noir in cinema, the change was usually represented as a stylized sequence: Clark Kent's silhouette is clearly seen behind a closed door's pebble glass window (or a shadow thrown across a wall) as he strips to his Superman costume. Then, the superhero emerges having transformed from his meek disguise to his true self. In the comic books and in the George Reeves television series, he favors the "Daily Planet"s storeroom for his changes of identities (the heroic change between identities within the storeroom is almost always seen in the comics, but never viewed in the Reeves series).

The CBS Saturday morning series "The New Adventures of Superman" produced by Filmation Studios—as well as "The Adventures of Superboy" from the same animation house—featured the iconic "shirt rip" to reveal the "S" or Clark Kent removing his unbuttoned white dress shirt in a secluded spot, usually thanks to stock animation which was reused over dozens of episodes, to reveal his costume underneath while uttering his famed line "This is a job for Superman!"

As a dramatic plot device, Clark often has to quickly improvise in order to find a way to change unnoticed. For example, in "Superman" (1978), Clark, unable to use a newer, open-kiosk pay phone (and getting a nice laugh from the theater audience), runs down the street and rips open his shirt to reveal his costume underneath. He quickly enters a revolving door, spinning through it at incredible speed while changing clothes. Thus made invisible, he appears to have entered the building as Clark Kent and exited seconds later as Superman. Later in the film, when the need to change is more urgent (as he believes the city is about to be poisoned by Lex Luthor), he simply jumps out a window of the "Daily Planet" offices, changing at super-speed as he falls (the film merely shows the falling Kent blurring into a falling Superman) and flies off. Further films in the series continued this tradition, with Clark blurring into Superman, changing at super-speed while he runs.

In "Lois & Clark," Clark's usual method of changing was to either "suddenly" remember something urgent that required his immediate attention or leave the room/area under the pretense of contacting a source, summoning the police, heading to a breaking story's location, etc. The change would then frequently occur off-screen, although the shirt-rip reveal was a prominently used move well-associated with the show. Clark also developed a method of rapidly spinning into his costume at super speed which became a trademark change, especially during the third and fourth seasons of the series, and extremely popular with the show's fans. An alternate universe Clark (who lost his adoptive parents as a child and only adopted the Superman persona at the urging of Lois from the main timeline) was shown to enter an empty room and emerge as Superman a moment later, although he did adopt the spinning move eventually at Lois' advice.

In one scene of "", Clark becomes aware of an emergency while talking with Bruce Wayne and, in the next panel, he has flown out of his Kent clothing and glasses so quickly that they have had no time to fall.

In Season 8 of "Smallville," Clark begins to show a bit more of his double identity. He starts slowing down his superspeed enough for surveillance cameras to see his iconic red and blue streak. This reveals to the citizens of Metropolis that a superhero is among them and the name "The Red-Blue Blur" is coined. When Jimmy Olsen becomes suspicious, Clark decides to reserve his usual red-and-blue for saving people. He carries a backpack with him to work every day, containing his change of clothes. He begins to practice his speed change at home and at the "Daily Planet." He changes in a superspeed spin in the "Daily Planet"s phone booth and once even in his office chair. The last minute of the last episode of "Smallville" had Clark responding to an emergency, rushing to the top of the Daily Planet building, and then using the familiar shirt-rip while the camera zoomed in on the familiar S-logo to the original John Williams fanfare.

There is debate as to which of the two identities (Superman or Clark Kent) is the real person and which is the façade. Fans and Superman scholars follow one of three interpretations:

Clark Kent has also been depicted without the Superman alter ego. In the Elseworlds stories starting with "", he is the son of Jonathan Kent, who saves his son from the destruction of the Earth. Clark ends up on Krypton, where he is adopted by Jor-El and becomes the planet's Green Lantern.

In the early "Adventures of Superman" radio episodes, Kal-El landed on Earth as an adult. He saved a man and his son and they gave him the idea of living as a normal person. They gave him the name of Clark Kent, and he later got a job as a newspaper reporter under that name. In that role he adopted a higher voice and a more introverted personality – clearly establishing that Kent is the secret identity and Superman is the true person.

Later episodes shifted to the usual origin story, in which Kal-El landed on Earth as a baby and was raised by the Kent family.

Clayton "Bud" Collyer voiced both Clark Kent and Superman, until Michael Fitzmaurice replaced him in the final episodes.

In the film serials "Superman" (1948) and "Atom Man vs. Superman" (1950), Kirk Alyn portrays Clark as a mild-mannered reporter who comes to Metropolis and secures a job at the "Daily Planet", following the death of his foster parents. While he quickly gains the respect of "Planet" editor Perry White, he is forced to contend with rival reporter Lois Lane, who often uses trickery to prevent Clark from pursuing a lead (giving her the chance to scoop him). Nevertheless, his journalistic skills are useful as he pursues stories on the crime boss known as the Spider Lady, and the criminal scientist Luthor (who had yet to receive his first name, Lex).

In the 1950s George Reeves series, Clark Kent is portrayed as a cerebral character who is the crime reporter for the "Daily Planet" and who as Kent uses his intelligence and powers of deduction to solve crimes (often before Inspector Henderson does) before catching the villain as Superman. Examples include the episodes "Mystery of the Broken Statues", "A Ghost for Scotland Yard", "The Man in the Lead Mask", and "The Golden Vulture". George Reeves' Kent/Superman is also established as a champion of justice for the oppressed in episodes like "The Unknown People" and "The Birthday Letter". Although Kent is described in the show introduction as "mild-mannered", he can be very assertive, often giving orders to people and taking authoritative command of situations, though, as in the Pre-Crisis Superman stories at that time, Clark is still considered the secret identity. He gets people to trust his judgment very easily and has a good, often wisecracking, sense of humor. Reeves, who first appeared as the character in the 1951 film "Superman and the Mole Men", was older than subsequent Superman actors.

In 1978, the first of four Superman films was made in which Clark Kent and Superman were portrayed by Christopher Reeve (with teenage Kent played by Jeff East in the first film). This was followed nearly two decades later by a fifth film called "Superman Returns" with Brandon Routh giving a performance very similar to Reeve's. In contrast to George Reeves' intellectual Clark Kent, Reeve's version is much more of an awkward fumbler and bungler, although Reeve is also an especially athletic, dashing and debonair Superman. Clark Kent's hair is always absolutely flat, while Superman's hair has a slight wave and is parted on the opposite side as Kent's. These films leave the impression that Clark Kent is really a "secret" identity that is used to enable Superman to serve humanity better, rather than just a role to help him assimilate into the human community.

A great deal of emphasis is placed on his origins on the planet Krypton with exotic crystalline sets designed by John Barry, effectively giving Superman a third persona as Kal-El. The first film is in three sections: Kal-El's infancy on Krypton (shot in London on the 007 stage), Clark Kent's teen years in Smallville, and Kent/Superman's adult life in Metropolis (shot in New York City). In earlier sections of the film, Reeve's Kent interacts with both his earthly parents and the spirit of his Kryptonian father through a special crystal, in a way George Reeves never did. The film has a fair amount of quasi-Biblical imagery suggestive of Superman as a sort of Christ-figure sent by Jor-El "to show humans the way". (See also Superman (1978 film)#Themes). In "Superman II" Reeve's Superman has to sacrifice his powers (effectively becoming just Clark Kent) in order to have a love relationship with Lois Lane, a choice he eventually abrogates to protect the world.

The relationship between Superman and Kent came to actual physical blows in "Superman III". Superman is given a piece of manufactured Kryptonite, but instead of weakening or killing him it drives him crazy, depressed, angry, and casually destructive, committing crimes which range from petty acts of vandalism to environmental disasters, like causing an oil spillage in order to bed a lusty woman by the name of Lorelei in league with the villains. Driven alcoholic, Superman, his outfit dirty and neglected, eventually goes to a car wrecking yard where Kent, in a proper business suit and glasses, suddenly emerges from within him. A fight ensues in which the "evil" Superman tries to dispose of the "good" Kent, but the latter fights back, "kills" the evil side to his nature and, reclaiming the Superman mantle, sets off to repair the damage and capture the villains. Les Daniels comments in his book, "DC Comics: A Celebration of the World's Favourite Comic Book Heroes", "The 'good' Superman, ultimately triumphant, (is) dressed as Clark, thus implying that he is the more valid personality (as well as the one Lana loves)" and expresses annoyance that "Something could have been made of this, but sadly nothing was".

The indirect "Christianization" of Superman in the Reeve films (admitted by film producer Pierre Spengler on the DVD commentaries) has provoked comment on the Jewish origins of Superman. Rabbi Simcha Weinstein's book "Up, Up and Oy Vey: How Jewish History, Culture and Values Shaped the Comic Book Superhero" says that Superman is both a pillar of society and one whose cape conceals a "nebbish", saying, "He's a bumbling, nebbish Jewish stereotype. He's Woody Allen."

Clark Kent's character is given one of its heaviest emphases in the 1990s series "". It is made very clear during the series, even discussed directly by the characters, that Clark Kent is who he really is, rather than his superheroic alter-ego.

In "Lois and Clark", Kent (Dean Cain) is a stereotypical wide-eyed farm kid from Kansas with the charm, grace and humor of George Reeves, but without the awkward geekiness of Christopher Reeve. Emphasis is laid on the comic elements of his dual relationship with Lois Lane (Teri Hatcher). The ban on Christopher Reeve's Superman having a relationship with a human while retaining his superpowers is entirely absent in the world of "Lois and Clark." In the final season, Clark Kent marries Lois Lane (a few years after her almost-marriage to his arch-enemy Lex Luthor, whom she refused at the altar), finding love, happiness, and completeness in this relationship which does not jeopardize his Superman persona.

Superman's secret identity was discovered by a number of villains during the series. In some cases, like that of Lex Luthor, the villain died before he could share the discovery. In two cases, the claim was discredited by having Superman and Clark appear together in public, using a hologram in the first case and a Clark Kent from a parallel universe in the second (in the first case, there was also footage filmed of Superman uniforms in Kent's closet, but that was explained by stating Superman simply needs a place to store them). In one case, Superman destroyed the evidence (a time traveler's journal), and stated that the villain's unsupported words will be ignored.

"Smallville" was adapted to television in 2001, by Alfred Gough and Miles Millar. Clark Kent is played by Tom Welling, with others portraying Clark as an infant. Throughout the series, Clark never officially adopted a costume until around the eighth season, but prior to this was seen wearing Superman's traditional colors of red and blue, more often as the series progresses (more commonly a blue shirt underneath a red jacket, reflecting Superman's uniform and cape colors). He is going through a process of character formation, making many mistakes in his youth, over time forming better and better judgment, while always self-consciously aware of his status as an alien from another planet who is different from other people. In season eight, he begins a fight against evil, hoping to be a source of inspiration and hope to others. A modest amount of religious imagery is seen occasionally in the series, but to a lesser degree than in the Christopher Reeve series.

"Smallville"'s Kent is particularly inwardly conflicted as he attempts to live the life of a normal human being, while keeping the secret of his alien heritage from his friends. Throughout the first seven seasons of the series he has a complicated relationship with Lana Lang, as well as his self-perceived guilt over the fact that the meteor shower that killed Lana's parents and created most of the superhumans he fought in the show's first few years was caused by his rocket coming to Earth and dragging pieces of Krypton with it. Clark's powers appear over time. He is not aware of all of his powers at the start of the show; for instance, his heat vision and super breath do not develop until seasons two and six, respectively, and his power of flight did not emerge until the series finale, up until that point the power appeared only in a few rare cases, such as when he was temporarily 're-programmed' to assume a Kryptonian persona or when he was trapped in a virtual reality.

Clark Kent starts out best friends with Lex Luthor, whom he meets after saving the latter's life. (Boyhood friendship with Lex Luthor had been the basis of a "Superboy" adventure published in 1960).

Clark and Lex remain entangled for most of the series. Lex Luthor's father, Lionel Luthor, is an unscrupulous industrialist with whom Lex has a troubled relationship. Lex would like to transcend his family background and be a better person than his father, but after multiple setbacks he slowly slips into evil, becoming convinced that only way he can "protect" the world from the perceived alien threats is by taking control of it, regardless of the cost to others. In turn, Clark Kent has a slightly dark side with which he comes to grips over time, made even worse by his experiences with Red Kryptonite, which causes him to lose his morals and act solely on impulse while under its influence. In different ways to Luthor, at times Clark also does not have a fully ideal relationship either with his adoptive father, Jonathan, nor with an A.I. based on Jor-El that was sent by the original to guide him, Jonathan occasionally having trouble relating to Clark while Jor-El's lack of his template's emotions causes him to treat Clark too harshly at times. The younger Luthor slightly envies Clark's "clean-cut" and wholesome parents (who disapprove of Clark's friendship with Luthor), while Clark is impressed with Luthor's wealth while failing to understand some of the manipulations he carries out in his interactions with others. Even in his better days, Luthor is highly ambitious for power and wealth, at one time noting that he shares his name with Alexander the Great. Clark Kent, on the other hand, has no idea what he is going to do with his life while bewildered by his powers, and his uncertainty as to why he was sent to Earth.

In season eight of "Smallville", Clark Kent begins to work as a reporter at the "Daily Planet". Shortly after he begins to save lives as an anonymous superhero crimefighter, which becomes known as the "Red-Blue Blur" after a photograph is taken of one of his rescues.

In season nine, Clark unintentionally begins to formalize his dual identity to protect his secret and also privately introduces the well-known glasses to Lois Lane. Additionally, during the opening scene of the season nine finale, Clark finds a gift from his mother containing his Superman suit (although the suit is subsequently taken by Jor-El until Clark is ready for it).

In season ten, for the first time in public Clark begins to formulate a bumbling/stuttering Kent with glasses akin to the Christoper Reeve/Brandon Routh portrayal of the character. In the season ten finale of the series he fully adopts the Superman identity, when he takes action to save Earth from Darkseid, who was drawn to Earth by Clark's actions and sought to take the hero as a host.

"Smallville"s Kent has also appeared in various literature (including comics and over a dozen young adult novels) based on the television series.

In the 1940s Superman shorts, Clark is shown to have a wisecracking sense of humor and he and Lois are good friends. At the near end of each short, Clark gives out a smile and a wink to the audience (that was carried over to the 1966 Superman animated series).

In the "" of the mid to late 1990s, Clark Kent is shown as a mild-mannered but competent reporter and is shown exposing various criminals through his reporter identity. In this identity, Clark and Lois are good friends (with Lois frequently calling him "Smallville" in a teasing but good-natured way) but do not share romantic feelings; instead, it is Superman and Lois who have a romantic relationship. Lana Lang on the other hand knows of Clark's identity as Superman but seems more interested in Clark because she knew him as that first.

Clark was mentioned by Cat Grant in "Supergirl" episode "Stronger Together". Clark Kent/Superman later appeared in the first two episodes of Season 2 portrayed by Tyler Hoechlin. Compared to Superman's charismatic and strong personality, Clark purposefully comes off as clumsy around others to help maintain his secret identity, though he admits that some of his ineptitudes are genuine. He maintains a close friendship with James Olsen and his romantic relationship with Lois Lane is strong, to the chagrin of his former coworker Cat Grant. He is very supportive of his cousin Kara, often staying in touch with her via instant messaging to give her advice.

In the Superman reboot film "Man of Steel", Clark Kent is portrayed by Henry Cavill, with Dylan Sprayberry and Cooper Timberline portraying younger versions of the character.

In this film, Kal-El is Krypton's first natural birth in centuries, a birth without using Krypton's genesis chamber. In order to save Krypton's future and stop Zod's coup, his biological father Jor-El steals Krypton's DNA template (Codex), bonds it to Kal-El's cells, and sends him to Earth before Krypton explodes. Kal-El's ship lands in a small Kansas town. He is raised as the adoptive son of Jonathan and Martha Kent, who name him Clark.

As a boy, Clark is a conflicted and lonely person who questions his place and purpose in the world. At a very young age, he learns of his superhuman abilities such as X-ray vision and superhuman strength, and this discovery frightens him. Jonathan Kent impresses on Clark to keep his abilities a secret and not to use them openly for fear of the consequences. This comes to a head when Clark uses his powers to save his schoolmates from drowning after their bus crashes into a lake. Jonathan is dismayed that Clark would risk exposing himself, and even tells Clark that maybe he should have let the kids die. Despite being ridiculed throughout his childhood and adolescence, Clark still wants to use his abilities to help others and make a difference in the world. This causes tension between him and Jonathan, who would rather he remain on the farm, believing that a useful endeavor by itself, and live his life in relative safety. Jonathan carries this conviction even to his death, when he stops Clark from saving him from being killed by a tornado to prevent him revealing his powers to others.

After Jonathan's death, an adult Clark spends several years living a nomadic lifestyle, working different jobs under false identities while saving people in secret, as well as struggling to cope with the loss of his adoptive father. He is now shown to be a confused, slightly angry individual who is forced to show restraint to bring harm to those who try to harm him or others. He eventually infiltrates a U.S. military investigation of a Kryptonian scout spaceship in the Arctic. Clark enters the alien ship and communicates with the preserved consciousness of Jor-El in the form of a hologram. Jor-El reveals Clark's origins and the extinction of his race, and tells Clark that he was sent to Earth to bring hope to mankind. Lois Lane, a journalist from the Daily Planet sent to write a story on the discovery, sneaks inside the ship while following Clark and is rescued by him when she is injured. Lois' editor, Perry White, rejects her story of a "superhuman" rescuer, so she traces Clark back to Kansas with the intention of writing an exposé. After hearing his story, she decides not to reveal his secret. After the discovery of his background and purpose, he is shown to be less confused and a little more joyful, as evidenced by his discussion with his adoptive mother Martha.

When Zod arrives to transform Earth into a new Krypton, Lois helps Clark/Superman stop Zod. By film's end, to create an alias that gives him access to dangerous situations without arousing suspicion, Clark takes a job as a reporter at the Daily Planet and adopts a modernized version of his "mild-mannered" look from the comics.

It is worth noting that, as a nod to many comics, Clark is implied to have an interest in football, as evidenced when he is seen watching a game while drinking beer just before Zod's arrival and ultimatum.

Henry Cavill reprises his role as Clark Kent in the film "". Here, his full name is given as Clark Joseph Kent. Nearly two years after the events of Man of Steel, Clark and Lois are close in their relationship, but Clark finds himself continually questioning his role as Superman. At the beginning, he rescues Lois from African terrorists when a riot ensues that he is blamed for. When he hears of Batman's actions in Gotham, he decides to investigate against Perry White's orders, believing Batman's methods to be unjust. He meets Bruce Wayne at a party hosted by Lex Luthor and grows suspicious when he hears Alfred communicating to Bruce in an earpiece. He later confronts Batman when the vigilante is chasing down Luthor's men who have kryptonite, and orders him to cease his activities. Superman is later summoned by Senator June Finch to the U.S. Capitol to discuss his actions, but the room is bombed by Luthor, framing Superman once more. He goes into a self-exile, feeling guilty for not stopping the bombing.

Later, he dons the costume once more when Lois is endangered by Lex, and confronts the scientist on the roof. However, Lex reveals he knows all about Clark's true identity and blackmails Superman into fighting Batman by holding Martha hostage. Superman tries to reason with Batman, revealing that he knows his secrets, but this leads to a fight in which Batman nearly kills Superman with a kryptonite spear. Superman pleads for Batman to "save Martha", which was also the latter's late mother's name, causing him to come to his senses and realize Superman is not a threat. Upon learning of Luthor's plan, Batman leaves to rescue Martha while Superman confronts Luthor, who unleashes a monstrous artificially-bred creature he dubs as Clark's "Doomsday" made with Kryptonian technology on the crashed ship. Superman is aided by Batman and the mysterious Wonder Woman in confronting the monster, but none of them are able to put the creature down. Knowing it's kryptonian, Clark retrieves the kryptonite spear and impales Doomsday with the object, who in response mortally wounds Clark by stabbing him with one of its claws. Two separate funerals are held. Metropolis holds a funeral for Superman, but the Kent farm holds a private one for Clark Kent (which contains his actual body). Martha gives Lois an engagement ring that Clark planned to give to her. The dirt around Clark's casket briefly levitates indicating he may still be alive.

Batman also had a vision at one point in the movie where he leads a rebellion against the forces of Superman in a dystopian future where Clark kills two hostages with his heat vision before killing Batman while saying "You took her from me." A mysterious time traveler also briefly appears warning him that Lois is the key. Whether or not Batman's vision comes true remains to be seen.
Clark Kent makes a cameo appearance in Marvel Comics' "Thor" #341 (1984; art by Walter Simonson). When Thor asks Nick Fury to set up a new alternate identity for him since he cannot turn into Donald Blake anymore, Fury gives him a pair of glasses as a disguise aid, stating, "It always worked for that other guy." Right afterwards, Thor accidentally bumps into Kent, who is attending a press conference at S.H.I.E.L.D. with Lois Lane. Thanks to the glasses, Kent nearly recognizes Thor, but then (ironically) dismisses the idea.



</doc>
<doc id="7445" url="https://en.wikipedia.org/wiki?curid=7445" title="Classification of finite simple groups">
Classification of finite simple groups

In mathematics, the classification of the finite simple groups is a theorem stating that every finite simple group belongs to one of four broad classes described below. These groups can be seen as the basic building blocks of all finite groups, in a way reminiscent of the way the prime numbers are the basic building blocks of the natural numbers. The Jordan–Hölder theorem is a more precise way of stating this fact about finite groups. However, a significant difference from integer factorization is that such "building blocks" do not necessarily determine a unique group, since there might be many non-isomorphic groups with the same composition series or, put in another way, the extension problem does not have a unique solution.

Group theory is central to many areas of pure and applied mathematics and the classification theorem is one of the great achievements of modern mathematics. The proof consists of tens of thousands of pages in several hundred journal articles written by about 100 authors, published mostly between 1955 and 2004. Gorenstein (d.1992), Lyons, and Solomon are gradually publishing a simplified and revised version of the proof.

The classification theorem has applications in many branches of mathematics, as questions about the structure of finite groups (and their action on other mathematical objects) can sometimes be reduced to questions about finite simple groups. Thanks to the classification theorem, such questions can sometimes be answered by checking each family of simple groups and each sporadic group.

Daniel Gorenstein announced in 1983 that the finite simple groups had all been classified, but this was premature as he had been misinformed about the proof of the classification of quasithin groups. The completed proof of the classification was announced by after Aschbacher and Smith published a 1221-page proof for the missing quasithin case.

 wrote two volumes outlining the low rank and odd characteristic part of the proof, and 
wrote a 3rd volume covering the remaining characteristic 2 case. The proof can be broken up into several major pieces as follows:

The simple groups of low 2-rank are mostly groups of Lie type of small rank over fields of odd characteristic, together with five alternating and seven characteristic 2 type and nine sporadic groups.

The simple groups of small 2-rank include:
The classification of groups of small 2-rank, especially ranks at most 2, makes heavy use of ordinary and modular character theory, which is almost never directly used elsewhere in the classification.

All groups not of small 2 rank can be split into two major classes: groups of component type and groups of characteristic 2 type. This is because if a group has sectional 2-rank at least 5 then MacWilliams showed that its Sylow 2-subgroups are connected, and the balance theorem implies that any simple group with connected Sylow 2-subgroups is either of component type or characteristic 2 type. (For groups of low 2-rank the proof of this breaks down, because theorems such as the signalizer functor theorem only work for groups with elementary abelian subgroups of rank at least 3.)

A group is said to be of component type if for some centralizer "C" of an involution, "C"/"O"("C") has a component (where "O"("C") is the core of "C", the maximal normal subgroup of odd order).
These are more or less the groups of Lie type of odd characteristic of large rank, and alternating groups, together with some sporadic groups.
A major step in this case is to eliminate the obstruction of the core of an involution. This is accomplished by the B-theorem, which states that every component of "C"/"O"("C") is the image of a component of "C".

The idea is that these groups have a centralizer of an involution with a component that is a smaller quasisimple group, which can be assumed to be already known by induction. So to classify these groups one takes every central extension of every known finite simple group, and finds all simple groups with a centralizer of involution with this as a component. This gives a rather large number of different cases to check: there are not only 26 sporadic groups and 16 families of groups of Lie type and the alternating groups, but also many of the groups of small rank or over small fields behave differently from the general case and have to be treated separately, and the groups of Lie type of even and odd characteristic are also quite different.

A group is of characteristic 2 type if the generalized Fitting subgroup "F"*("Y") of every 2-local subgroup "Y" is a 2-group.
As the name suggests these are roughly the groups of Lie type over fields of characteristic 2, plus a handful of others that are alternating or sporadic or of odd characteristic. Their classification is divided into the small and large rank cases, where the rank is the largest rank of an odd abelian subgroup normalizing a nontrivial 2-subgroup, which is often (but not always) the same as the rank of a Cartan subalgebra when the group is a group of Lie type in characteristic 2.

The rank 1 groups are the thin groups, classified by Aschbacher, and the rank 2 ones are the notorious quasithin groups, classified by Aschbacher and Smith. These correspond roughly to groups of Lie type of ranks 1 or 2 over fields of characteristic 2.

Groups of rank at least 3 are further subdivided into 3 classes by the trichotomy theorem, proved by Aschbacher for rank 3 and by Gorenstein and Lyons for rank at least 4.
The three classes are groups of GF(2) type (classified mainly by Timmesfeld), groups of "standard type" for some odd prime (classified by the Gilman–Griess theorem and work by several others), and groups of uniqueness type, where a result of Aschbacher implies that there are no simple groups.
The general higher rank case consists mostly of the groups of Lie type over fields of characteristic 2 of rank at least 3 or 4.

The main part of the classification produces a characterization of each simple group. It is then necessary to check that there exists a simple group for each characterization and that it is unique. This gives a large number of separate problems; for example, the original proofs of existence and uniqueness of the monster group totaled about 200 pages, and the identification of the Ree groups by Thompson and Bombieri was one of the hardest parts of the classification. Many of the existence proofs and some of the uniqueness proofs for the sporadic groups originally used computer calculations, most of which have since been replaced by shorter hand proofs.

In 1972 announced a program for completing the classification of finite simple groups, consisting of the following 16 steps:

Many of the items in the list below are taken from . The date given is usually the publication date of the complete proof of a result, which is sometimes several years later than the proof or first announcement of the result, so some of the items appear in the "wrong" order.
The proof of the theorem, as it stood around 1985 or so, can be called "first generation". Because of the extreme length of the first generation proof, much effort has been devoted to finding a simpler proof, called a second-generation classification proof. This effort, called "revisionism", was originally led by Daniel Gorenstein.

, seven volumes of the second generation proof have been published . In 2012 Solomon estimated that the project would need another 5 volumes, but said that progress on them was slow. It is estimated that the new proof will eventually fill approximately 5,000 pages. (This length stems in part from second generation proof being written in a more relaxed style.) Aschbacher and Smith wrote their two volumes devoted to the quasithin case in such a way that those volumes can be part of the second generation proof.

Gorenstein and his collaborators have given several reasons why a simpler proof is possible.

Gorenstein has discussed some of the reasons why there might not be a short proof of the classification similar to the classification of compact Lie groups.


This section lists some results that have been proved using the classification of finite simple groups.






</doc>
<doc id="7446" url="https://en.wikipedia.org/wiki?curid=7446" title="Chalcolithic">
Chalcolithic

The Chalcolithic (; "khalkós", "copper" and "líthos", "stone") period or Copper Age, in particular for eastern Europe often named Eneolithic or Æneolithic (from Latin "aeneus" "of copper"), was a period in the development of human technology, before it was discovered that adding tin to copper formed the harder bronze, leading to the Bronze Age. The Copper Age was originally defined as a transition between the Neolithic and the Bronze Age, but is now usually considered as belonging to the Neolithic.

The archaeological site of Belovode on the Rudnik mountain in Serbia contains the world's oldest securely dated evidence of copper smelting from 5000 BCE.

The multiple names result from multiple recognitions of the period. Originally, the term Bronze Age meant that either copper or bronze was being used as the chief hard substance for the manufacture of tools and weapons.

In 1881, John Evans recognized that use of copper often preceded the use of bronze, and distinguished between a "transitional Copper Age" and the "Bronze Age proper". He did not include the transitional period in the three-age system of Early, Middle and Late Bronze Age, but placed it outside the tripartite system, at its beginning. He did not, however, present it as a fourth age but chose to retain the traditional tripartite system.

In 1884, Gaetano Chierici, perhaps following the lead of Evans, renamed it in Italian as the "eneo-litica", or "bronze–stone" transition. The phrase was never intended to mean that the period was the only one in which both bronze and stone were used. The Copper Age features the use of copper, excluding bronze; moreover, stone continued to be used throughout both the Bronze Age and the Iron Age. The part "-litica" simply names the Stone Age as the point from which the transition began and is not another "-lithic" age.

Subsequently, British scholars used either Evans's "Copper Age" or the term "Eneolithic" (or Æneolithic), a translation of Chierici's "eneo-litica". After several years, a number of complaints appeared in the literature that "Eneolithic" seemed to the untrained eye to be produced from "e-neolithic", "outside the Neolithic", clearly not a definitive characterization of the Copper Age. Around 1900, many writers began to substitute "Chalcolithic" for Eneolithic, to avoid the false segmentation. It was then that the misunderstanding began among those who did not know Italian. The Chalcolithic was seen as a new "-lithic" age, a part of the Stone Age in which copper was used, which may appear paradoxical. Today, "Copper Age", "Eneolithic" and "Chalcolithic" are used synonymously to mean Evans's original definition of Copper Age. The literature of European archaeology in general avoids the use of "Chalcolithic" (the term "Copper Age" is preferred), whereas Middle Eastern archaeologists regularly use it. "Chalcolithic" is not generally used by British prehistorians, who disagree as to whether it applies in the British context.

Analysis of stone tool assemblages from sites on the Tehran Plain, in Iran, has illustrated the effects of the introduction of copper working technologies on the in-place systems of lithic craft specialists and raw materials. Networks of exchange and specialized processing and production that had evolved during the Neolithic seem to have collapsed by the Middle Chalcolithic ( 4500–3500 BCE) and been replaced by the use of local materials by a primarily household-based production of stone tools.

The emergence of metallurgy may have occurred first in the Fertile Crescent. The earliest use of lead is documented here from the late Neolithic settlement of Yarim Tepe in Iraq,
"The earliest lead (Pb) finds in the ancient Near East are a 6th millennium BC bangle from Yarim Tepe in northern Iraq and a slightly later conical lead piece from Halaf period Arpachiyah, near Mosul. As native lead is extremely rare, such artifacts raise the possibility that lead smelting may have begun even before copper smelting."
Copper smelting is also documented at this site at about the same time period (soon after 6000 BC), although the use of lead seems to precede copper smelting. Early metallurgy is also documented at the nearby site of Tell Maghzaliyah, which seems to be dated even earlier, and completely lacks pottery.

Although traditional view holds that the transition to the Bronze Age has first occurred in the Fertile Crescent in the 4th millennium BCE, finds from the Vinča culture in Europe have now been securely dated to slightly earlier than those of the Fertile Crescent.

There was an independent invention of copper and bronze smelting first by Andean civilizations in South America extended later by sea commerce to the Mesoamerican civilization in West Mexico (see Metallurgy in pre-Columbian America and Metallurgy in pre-Columbian Mesoamerica).

According to Parpola, ceramic similarities between the Indus Civilization, southern Turkmenistan, and northern Iran during 4300–3300 BCE of the Chalcolithic period suggest considerable mobility and trade.

The Copper Age in the Middle East and the Caucasus began in the late 5th millennium BCE and lasted for about a millennium before it gave rise to the Early Bronze Age. The transition from the European Copper Age to Bronze Age Europe occurs about the same time, between the late 5th and the late 3rd millennia BCE.

An archaeological site in Serbia contains the oldest securely dated evidence of coppermaking from 7,500 years ago. The find in June 2010 extends the known record of copper smelting by about 800 years, and suggests that copper smelting may have been invented in separate parts of Asia and Europe at that time rather than spreading from a single source.
In Serbia, a copper axe was found at Prokuplje, which indicates that humans were using metals in Europe by 7,500 years ago (5500 BCE), many years earlier than previously believed. Knowledge of the use of copper was far more widespread than the metal itself. The European Battle Axe culture used stone axes modeled on copper axes, even with imitation "mold marks" carved in the stone. Ötzi the Iceman, who was found in the Ötztal Alps in 1991 and whose remains were dated to about 3300 BCE, was found with a Mondsee copper axe.

Examples of Chalcolithic cultures in Europe include Vila Nova de São Pedro and Los Millares on the Iberian Peninsula. Pottery of the Beaker people has been found at both sites, dating to several centuries after copper-working began there. The Beaker culture appears to have spread copper and bronze technologies in Europe, along with Indo-European languages. In Britain, copper was used between the 25th and 22nd centuries BCE, but some archaeologists do not recognise a British Chalcolithic because production and use was on a small scale.

In Bhirrana, the earliest Indus civilization site, copper bangles and arrowheads were found. The inhabitants of Mehrgarh in present-day Pakistan fashioned tools with local copper ore between 7000–3300 BCE. 
At the Nausharo site dated to 4500 years ago, a pottery workshop in province of Balochistan, Pakistan, were unearthed 12 blades or blade fragments. These blades are long and and relatively thin. Archaeological experiments show that these blades were made with a copper indenter and functioned as a potter's tool to trim and shape unfired pottery. Petrographic analysis indicates local pottery manufacturing, but also reveals that existence of a few exotic black-slipped pottery items from the Indus Valley.

In the 5th millennium BCE copper artifacts start to appear in East Asia, such as in the Jiangzhai and Hongshan cultures, but those metal artifacts were not widely used.

The Timna Valley contains evidence of copper mining in 7000–5000 BCE. The process of transition from Neolithic to Chalcolithic in the Middle East is characterized in archaeological stone tool assemblages by a decline in high quality raw material procurement and use. This dramatic shift is seen throughout the region, including the Tehran Plain, Iran. Here, analysis of six archaeological sites determined a marked downward trend in not only material quality, but also in aesthetic variation in the lithic artefacts. Fazeli et al. use these results as evidence of the loss of craft specialisation caused by increased use of copper tools.

North Africa and the Nile Valley imported their iron technology from the Near East and followed the Near Eastern course of Bronze Age and Iron Age development. However the Iron Age and Bronze Age occurred simultaneously in much of Africa. The earliest dating of iron in Sub-Saharan Africa is 2500 BCE at Egaro, west of Termit, making it contemporary to the Middle East. The Egaro date is debatable with archaeologists, due to the method used to attain it. The Termit date of 1500 BCE is widely accepted.

In the region of the Aïr Mountains in Niger, we have the development of independent copper smelting between 3000 and 2500 BCE. The process was not in a developed state, indicating smelting was not foreign. It became mature about 1500 BCE.

The term is also applied to American civilizations that already used copper and copper alloys thousands of years before the European migration. Besides cultures in the Andes and Mesoamerica, the Old Copper Complex, centered in the Upper Great Lakes region—present-day Michigan and Wisconsin in the United States—mined and fabricated copper as tools, weapons, and personal ornaments. The evidence of smelting or alloying that has been found is subject to some dispute and a common assumption by archaeologists is that objects were cold-worked into shape. Artifacts from some of these sites have been dated to 4000–1000 BCE, making them some of the oldest Chalcolithic sites in the world. Furthermore, some archaeologists find artifactual and structural evidence of casting by Hopewellian and Mississippian peoples to be demonstrated in the archaeological record.





</doc>
<doc id="7447" url="https://en.wikipedia.org/wiki?curid=7447" title="Circumcision and law">
Circumcision and law

Laws restricting, regulating, or banning circumcision, some dating back to ancient times, have been enacted in many countries and communities. In a number of modern states, circumcision is presumed to be legal, but laws pertaining to assault or child custody have been applied in cases involving circumcision. In the case of non-therapeutic circumcision of children, proponents of laws in favor of the procedure often point to the rights of the parents or practitioners, namely the right of freedom to religion. Those against the procedure point to the boy's right of freedom from religion. In several court cases, judges have pointed to the irreversible nature of the act, the grievous harm to the boy's body, and the right to self-determination, and bodily integrity.

There are ancient religious requirements for circumcision. The Hebrew Bible commands Jews to circumcise their male children on the eighth day of life, and to circumcise their male slaves ().

Laws banning circumcision are also ancient. The ancient Greeks prized the foreskin and disapproved of the Jewish custom of circumcision. 1 Maccabees, 1:60–61 states that King Antiochus IV of Syria, the occupying power of Judea in 170 BCE, outlawed circumcision on penalty of death. one of the grievances leading to the Maccabean Revolt.

According to the "Historia Augusta", the Roman emperor Hadrian issued a decree banning circumcision in the empire, and some modern scholars argue that this was a main cause of the Jewish Bar Kokhba revolt of 132 CE. The Roman historian Cassius Dio, however, made no mention of such a law, and blamed the Jewish uprising instead on Hadrian's decision to rebuild Jerusalem as Aelia Capitolina, a city dedicated to Jupiter.

Antoninus Pius permitted Jews to circumcise their own sons. However, he forbade the circumcision of non-Jews that were either foreign-slaves or non-Jewish members of the household, contrary to He also made it illegal for a man to convert to Judaism. Antoninus Pius exempted the Egyptian priesthood from the otherwise universal ban on circumcision.

In 1993, a non-binding research paper of the Queensland Law Reform Commission ("Circumcision of Male Infants") concluded that "On a strict interpretation of the assault provisions of the Queensland Criminal Code, routine circumcision of a male infant could be regarded as a criminal act", and that doctors who perform circumcision on male infants may be liable to civil claims by that child at a later date. No prosecutions have occurred in Queensland, and circumcisions continue to be performed.

In 1999, a Perth man won A$360,000 in damages after a doctor admitted he botched a circumcision operation at birth which left the man with a badly deformed penis.

In 2002, Queensland police charged a father with grievous bodily harm for having his two sons, then aged nine and five, circumcised without the knowledge and against the wishes of the mother. The mother and father were in a family court dispute. The charges were dropped when the police prosecutor revealed that he did not have all family court paperwork in court and the magistrate refused to grant an adjournment.

Cosmetic circumcision for newborn males is currently banned in all Australian public hospitals, South Australia being the last state to adopt the ban in 2007; the procedure was not forbidden from being performed in private hospitals. In the same year, the Tasmanian President of the Australian Medical Association, Haydn Walters, stated that they would support a call to ban circumcision for non-medical, non-religious reasons. In 2009, the Tasmanian Law Reform Institute released its Issues Paper investigating the law relating to male circumcision in Tasmania, it "highlights the uncertainty in relation to whether doctors can legally perform circumcision on infant males".

The Tasmania Law Reform Institute released its recommendations for reform of Tasmanian law relative to male circumcision on 21 August 2012. The report makes fourteen recommendations for reform of Tasmanian law relative to male circumcision.

Male circumcision has traditionally been presumed to be legal under British law, however some authors have argued that there is no solid foundation for this view in English law.

The passage of the Human Rights Act 1998 has led to some speculation that the lawfulness of the circumcision of male children is unclear.

One 1999 case, "Re "J" (child's religious upbringing and circumcision)" said that circumcision in Britain required the consent of all those with parental responsibility (however this comment was not part of the reason for the judgement and therefore is not legally binding), or the permission of the court, acting for the best interests of the child, and issued an order prohibiting the circumcision of a male child of a non-practicing Muslim father and non-practicing Christian mother with custody. The reasoning included evidence that circumcision carried some medical risk; that the operation would be likely to weaken the relationship of the child with his mother, who strongly objected to circumcision without medical necessity; that the child may be subject to ridicule by his peers as the odd one out and that the operation might irreversibly reduce sexual pleasure, by permanently removing some sensory nerves, even though cosmetic foreskin restoration might be possible. The court did not rule out circumcision against the consent of one parent. It cited a hypothetical case of a Jewish mother and an agnostic father with a number of sons, all of whom, by agreement, had been circumcised as infants in accordance with Jewish laws; the parents then have another son who is born after they have separated; the mother wishes him to be circumcised like his brothers; the father for no good reason, refuses his agreement. In such a case, a decision in favor of circumcision was said to be likely.

In 2001 the General Medical Council had found a doctor who had botched circumcision operations guilty of abusing his professional position and that he had acted "inappropriately and irresponsibly", and struck him off the register. A doctor who had referred patients to him, and who had pressured a mother into agreeing to the surgery, was also condemned. He was put on an 18-month period of review and retraining, and was allowed to resume unrestricted practice as a doctor in March 2003, after a committee found that he had complied with conditions it placed on him. According to the "Northern Echo", he "told the committee he has now changed his approach to circumcision referrals, accepting that most cases can be treated without the need for surgery.".

Fox and Thomson (2005) argue that consent cannot be given for non-therapeutic circumcision. They say there is "no compelling legal authority for the common view that circumcision is lawful."

In 2005 a Muslim man had his son circumcised against the wishes of the child's mother who was the custodial parent.

In 2009 it was reported that a 20-year-old man whose father had him ritually circumcised as a baby is preparing to sue the doctor who circumcised him. This is believed to be the first time a person who was circumcised as an infant has made a claim in the UK. The case is expected to be heard in 2010.

In a 2015 case regarding female circumcision, a judge concluded that non-therapeutic circumcision of male children is a "significant harm". 

According to the College of Physicians and Surgeons of British Columbia:

A study commissioned by the European Parliament Committee on Civil Liberties, Justice and Home Affairs published in February 2013 stated that "Male circumcision for non-therapeutic reasons appears to be practiced with relative
regularity and frequency throughout Europe," and said it was "the only scenario, among the topics discussed in the present chapter, in which the outcome of the balancing between the right to physical integrity and religious freedom is in favour of the latter." The study recommended that "the best interests of children should be paramount, while acknowledging the relevance of this practice for Muslims and Jews. Member States should ensure that circumcision of underage children is performed according to the medical profession’s art and under conditions that do not put the health of minors at risk. The introduction of regulations by the Member States in order to set the conditions and the
appropriate medical training for those called to perform it is warranted."

On 1 October 2013, the Parliamentary Assembly of the Council of Europe adopted a non-binding resolution in which they state they are "particularly worried about a category of violation of the physical integrity of children," and included in this category "circumcision of young boys for religious reasons." On 7 October, Israel's president Shimon Peres wrote a personal missive to the Secretary General of the Council of Europe, Thorbjørn Jagland, to stop the ban, arguing: "The Jewish communities across Europe would be greatly afflicted to see their cultural and religious freedom impeded upon by the Council of Europe, an institution devoted to the protection of these very rights." Two days later, Jagland clarified that the resolution was non-binding and that “Nothing in the body of our legally binding standards would lead us to put on equal footing the issue of female genital mutilation and the circumcision of young boys for religious reasons.”

As of February 2018, no European country has a ban on male circumcision, but Iceland was planning to become the first to outlaw the practice for non-medical reasons.

In August 2006, a Finnish court ruled that the circumcision of a four-year-old boy arranged by his mother, who is Muslim, to be an illegal assault. The boy's father, who had not been consulted, reported the incident to the police. A local prosecutor stated that the prohibition of circumcision is not gender-specific in Finnish law. A lawyer for the Ministry of Social Affairs and Health stated that there is neither legislation nor prohibition on male circumcision, and that "the operations have been performed on the basis of common law." The case was appealed and in October 2008 the Finnish Supreme Court ruled that the circumcision, " carried out for religious and social reasons and in a medical manner, did not have the earmarks of a criminal offence. It pointed out in its ruling that the circumcision of Muslim boys is an established tradition and an integral part of the identity of Muslim men". In 2008, the Finnish government was reported to be considering a new law to legalise circumcision if the practitioner is a doctor and if the child consents. In December 2011, Helsinki District Court said that the Supreme Court's decision does not mean that circumcision is legal for any non-medical reasons. The court referred to the Convention on Human rights and Biomedicine of the Council of Europe, which was ratified in Finland in 2010.

In February 2010, a Jewish couple were fined for causing bodily harm to their then infant son who was circumcised in 2008 by a mohel brought in from the UK. Normal procedure for persons of Jewish faith in Finland is to have a locally certified mohel who works in Finnish healthcare perform the operation. In the 2008 case, the infant was not anesthetized and developed complications that required immediate hospital care. The parents were ordered to pay 1500 euros in damages to their child.

In October 2006, a Turkish national who performed ritual circumcisions on seven boys was convicted of causing dangerous bodily harm by the state court in Düsseldorf.

In September 2007, a Frankfurt am Main appeals court found that the circumcision of an 11-year-old boy without his approval was an unlawful personal injury. The boy, whose parents were divorced, was visiting his Muslim father during a vacation when his father forced him to be ritually circumcised. The boy had planned to sue his father for .

In May 2012, the Cologne regional appellate court ruled that religious circumcision of male children amounts to bodily injury, and is a criminal offense in the area under its jurisdiction. The decision based on the article "Criminal Relevance of Circumcising Boys. A Contribution to the Limitation of Consent in Cases of Care for the Person of the Child" published by Holm Putzke, a German law professor at the University of Passau. The court arrived at its judgment by application of the human rights provisions of the Basic Law, a section of the Civil Code, and some sections of the Criminal Code to non-therapeutic circumcision of male children. Some observers said it could set a legal precedent that criminalizes the practice. Jewish and Muslim groups were outraged by the ruling, viewing it as trampling on freedom of religion.

The German ambassador to Israel, Andreas Michaelis, told Israeli lawmakers that Germany was working to resolve the issue and that it doesn't apply at a national level, but instead only to the local jurisdiction of the court in Cologne. The Council of the Coordination of Muslims in Germany condemned the ruling, stating that it is "a serious attack on religious freedom." Ali Kizilkaya, a spokesman of the council, stated that, "The ruling does not take everything into account, religious practice concerning circumcision of young Muslims and Jews has been carried out over the millennia on a global level." The Roman Catholic archbishop of Aachen, Heinrich Mussinghoff, said that the ruling was "very surprising", and the contradiction between "basic rights on freedom of religion and the well-being of the child brought up by the judges is not convincing in this very case." Hans Ulrich Anke, the head of the Protestant Church in Germany, said the ruling should be appealed since it didn't "sufficiently" consider the religious significance of the rite. A spokesman, Steffen Seibert, for German Chancellor Angela Merkel stated that Jewish and Muslim communities will be free to practice circumcision responsibly, and the government would find a way around the local ban in Cologne. The spokesman stated "For everyone in the government it is absolutely clear that we want to have Jewish and Muslim religious life in Germany. Circumcision carried out in a responsible manner must be possible in this country without punishment.".

In July, a group of rabbis, imams, and others said that they view the ruling against circumcision "an affront on our basic religious and human rights." The joint statement was signed by leaders of groups including Germany's Turkish-Islamic Union for Religious Affairs, the Islamic Center Brussels, the Rabbinical Centre of Europe, the European Jewish Parliament and the European Jewish Association, who met with members of European Parliament from Germany, Finland, Belgium, Italy, and Poland. European rabbis, who urged Jews to continue circumcision, planned further talks with Muslim and Christian leaders to determine how they can oppose the ban together. The Jewish Hospital of Berlin suspended the practice of male circumcision. On 19 July 2012, a joint resolution of the CDU/CSU, SPD and FDP factions in the Bundestag requesting the executive branch to draft a law permitting circumcision of boys to be performed without unnecessary pain in accordance with best medical practice carried with a broad majority.

The New York Times reported that the German Medical Association "condemned the ruling for potentially putting children at risk by taking the procedure out of the hands of doctors, but it also warned surgeons not to perform circumcisions for religious reasons until legal clarity was established." The ruling was supported by Deutsche Kinderhilfe, a German child rights organization, which asked for a two-year moratorium to discuss the issue and pointed out that religious circumcision may contravene the Convention on the Rights of the Child (Article 24.3: "States Parties shall take all effective and appropriate measures with a view to abolishing traditional practices prejudicial to the health of children.").

The German Academy for Pediatric and Adolescent Medicine (Deutsche Akademie für Kinder- und Jugendmedizin e.V., DAKJ), the German Association for Pediatric Surgery (Deutsche Gesellschaft für Kinderchirurgie, DGKCH) and the Professional Association of Pediatric and Adolescent Physicians (Berufsverband der Kinder- und Jugendärzte) took a firm stand against non-medical routine infant circumcision.

In July, in Berlin, a criminal complaint was lodged against Rabbi Yitshak Ehrenberg for "causing bodily harm" by performing religious circumcision, and for vocal support of the continuation of the practice. In September, the prosecutors dismissed the complaint, concluding that "there is no proof to establish that the rabbi's conduct met the 'condition of a criminal' violation."

In September, Reuters reported "Berlin's senate said doctors could legally circumcise infant boys for religious reasons in its region, given certain conditions."

On 12 December 2012, following a series of hearings and consultations, the Bundestag adopted the proposed law explicitly permitting non-therapeutic circumcision to be performed under certain conditions; it is now §1631(d) in the German Civil Code. The vote tally was 434 ayes, 100 noes, and 46 abstentions. Following approval by the Bundesrat and signing by the Bundespräsident, the new law became effective on 28 December 2012 a day after its publication in the Federal Gazette.

In February 2018, Iceland was planning to become the first European country to ban male circumcision for non-medical reasons. The bill discussed in the Alþing, the Icelandic parliament, claimed the practice harmed the physical integrity of young boys, was often performed without anaesthesia and in an unhygienic manner by religious leaders instead of medical experts. These facts were deemed incompatible with the United Nations Convention on the Rights of the Child (1990), and the bill proposed a penalty of up to six years imprisonment for any violation of the ban. Critics argued the bill infringed on religious freedom or constituted antisemitism or anti-Muslim bigotry, making it hard for Muslims and Jews to live there. Silja Dögg Gunnarsdóttir of the Progressive Party, who proposed the ban, retorted that Iceland had already prohibited female circumcision in 2005, and “If we have laws banning circumcision for girls, then we should do so for boys.”

In October 2005 a Nigerian man was cleared of a charge of reckless endangerment over the death of a baby from hemorrhage and shock after he had circumcised the child. The judge directed the jury not to "bring what he called their white western values to bear when they were deciding this case" and effectively imposed a not guilty verdict on the jury. After deliberating for an hour and a half they found the defendant not guilty.

In Israel, Jewish circumcision is entirely legal. Though illegal, female circumcision is still practiced among the Negev Bedouin, and tribal secrecy among the Bedouin makes it difficult for authorities to enforce the ban. In 2013, Rabbinical court in Israel ordered a mother, Elinor Daniel, to circumcise her son or pay a fine of 500 Israeli Shekel for every day that the child is not circumcised. She appealed against the Rabbinical court ruling and the High Court ruled in her favour stating, among other considerations, the basic right of freedom from religion.

In May 2008 a father who had his two sons, aged 3 and 6 circumcised against the will of their mother was found not guilty of abuse as the circumcision was performed by a physician and due to the court's restraint in setting a legal precedent; instead he was given a 6-week suspended jail sentence for taking the boys away from their mother against her will.

In 2012, the Senterpartiet proposed a ban on circumcision on males under eighteen.

In September 2013, the [[Children's Ombudsman|Children's ombudsmen]] in all [[Nordic countries]] issued a statement by which they called for a ban on circumcision of minors for non-medical reasons, stating that such circumcisions violate the rights of children after the [[Convention on the Rights of the Child]] to co-determination and protection from harmful traditions.

A bill on ritual circumcision of boys was passed (against two votes) in The Norwegian Parliament, Stortinget, June 2014, with the new law effective from 1.1.2015. This law protects the right of Jews to brit mila and obligates the Norwegian Health Care regions to offer the Muslim minority a safe and affordable procedure.

The [[Children's Act 2005]] makes the circumcision of male children under 16 unlawful except for religious or medical reasons. In the [[Eastern Cape]] province the Application of Health Standards in Traditional Circumcision Act, 2001, regulates traditional circumcision, which causes the death or mutilation of many youths by traditional surgeons each year. Among other provisions, the minimum age for circumcision is age 18.

In 2004, a 22-year-old Rastafarian convert was forcibly circumcised by a group of [[Xhosa people|Xhosa]] tribal elders and relatives. When he first fled, two police returned him to those who had circumcised him. In another case, a medically circumcised Xhosa man was forcibly recircumcised by his father and community leaders. He laid a charge of unfair discrimination on the grounds of his religious beliefs, seeking an apology from his father and the Congress of Traditional Leaders of South Africa. According to South African newspapers, the subsequent trial became "a landmark case around forced circumcision." In October 2009, the [[Eastern Cape High Court, Bhisho|Eastern Cape High Court at Bhisho]] (sitting as an [[Equality Court]]) clarified that circumcision is unlawful unless done with the full consent of the initiate.

In 2001, the [[Parliament of Sweden]] enacted a law allowing only persons certified by the National Board of Health to circumcise infants. It requires a medical doctor or an anesthesia nurse to accompany the circumciser and for anaesthetic to be applied beforehand. After the first two months of life circumcisions can only be performed by a physician. The stated purpose of the law was to increase the safety of the procedure.

Swedish Jews and Muslims objected to the law, and in 2001, the [[World Jewish Congress]] called it "the first legal restriction on Jewish religious practice in Europe since the Nazi era." The requirement for an anaesthetic to be administered by a medical professional is a major issue, and the low degree of availability of certified professionals willing to conduct circumcision has also been subject to criticism. According to a survey, two out of three paediatric surgeons said they refuse to perform non-therapeutic circumcision, and less than half of all [[County Councils of Sweden|county councils]] offer it in their hospitals. However, in 2006, the U.S. State Department stated, in a report on Sweden, that most Jewish [[mohel]]s had been certified under the law and 3000 Muslim and 40–50 Jewish boys were circumcised each
year. An estimated 2000 of these are performed by persons who are neither physicians nor have officially recognised certification.

The Swedish National Board of Health and Welfare reviewed the law in 2005 and recommended that it be maintained, but found that the law had failed with regard to the intended consequence of increasing the safety of circumcisions. A later report by the Board criticised the low level of availability of legal circumcisions, partly due to reluctance among health professionals. To remedy this, the report suggested a new law obliging all county councils to offer non-therapeutic circumcision in their hospitals, but this was later abandoned in favour of a non-binding recommendation.

Circumcision of adults who grant personal informed consent for the surgical operation is legal.

In the United States, non-therapeutic circumcision of male children has long been assumed to be lawful in every jurisdiction provided that one parent grants surrogate informed consent. Adler (2013) has recently challenged the validity of this assumption. As with every country, doctors who circumcise children must take care that all applicable rules regarding informed consent and safety are satisfied.

While anti-circumcision groups have occasionally proposed legislation banning non-therapeutic child circumcision, it has not been supported in any legislature. After a failed attempt to adopt a local ordinance banning circumcision on a San Francisco ballot, the state of California enacted in October 2011 a law protecting circumcision from local attempts to ban the practice.

In 2012, New York City required those performing "[[metzitzah b'peh]]", a part of circumcision required by some [[Hasidim]], to obey stringent consent requirements, including documentation. [[Agudath Israel of America]] and other Jewish groups have planned to sue the city in response.

Disputes between parents

Occasionally the courts are asked to make a ruling when parents cannot agree on whether or not to circumcise a child.

In January 2001 a dispute between divorcing parents in New Jersey was resolved when the mother, who sought to have the boy circumcised withdrew her request. The boy had experienced two instances of foreskin inflammation and she wanted to have him circumcised. The father, who had experienced a traumatic circumcision as a child objected and they turned to the courts for a decision. The Medical Society of New Jersey and the Urological Society of New Jersey both opposed any court ordered medical treatment. As the parties came to an agreement, no precedent was set. In June 2001 a Nevada court settled a dispute over circumcision between two parents but put a strict gag order on the terms of the settlement. In July 2001 a dispute between parents in [[Kansas]] over circumcision was resolved when the mother's request to have the infant circumcised was withdrawn. In this case the father opposed circumcision while the mother asserted that not circumcising the child was against her religious beliefs. (The woman's pastor had stated that circumcision was "important" but was not necessary for salvation.) On 24 July 2001 the parents reached agreement that the infant would not be circumcised.

On 14 July 2004 a mother appealed to the [[Missouri Supreme Court]] to prevent the circumcision of her son after a county court and the Court of Appeals had denied her a writ of prohibition. However, in early August 2004, before the Supreme Court had given its ruling, the father, who had custody of the boy, had him circumcised.

In October 2006 a judge in Chicago granted an injunction blocking the circumcision of a 9-year-old boy. In granting the injunction the judge stated that "the boy could decide for himself whether to be circumcised when he turns 18."

In November 2007, the [[Oregon Supreme Court]] heard arguments from a divorced [[Oregon]] couple over the circumcision of their son. The father wanted his son, who turned 13 on 2 March 2008, to be circumcised in accordance with the father's religious views; the child's mother opposes the procedure. The parents dispute whether the boy is in favor of the procedure. A group opposed to circumcision filed briefs in support of the mother's position, while some Jewish groups filed a brief in support of the father. On 25 January 2008, the Court returned the case to the trial court with instructions to determine whether the child agrees or objects to the proposed circumcision. The father appealed to the US Supreme Court to allow him to have his son circumcised but his appeal was rejected. The case then returned to the trial court. When the trial court interviewed the couple's son, now 14 years old, the boy stated that he did not want to be circumcised. This also provided the necessary circumstances to allow the boy to change residence to live with his mother. The boy was not circumcised.

Other disputes

In September 2004 the [[North Dakota Supreme Court]] rejected a mother's attempt to prosecute her doctor for circumcising her child without fully informing her of the consequences of the procedure. The judge and jury found that the defendants were adequately informed of possible complications, and the jury further found that it is not incumbent on the doctors to describe every "insignificant" risk.

In March 2009 a [[Fulton County, Georgia|Fulton County, Ga.]], State Court jury awarded $2.3 million in damages to a 4-year-old boy and his mother for a botched circumcision in which too much tissue was removed causing permanent disfigurement.

In August 2010 an eight-day-old boy was circumcised in a Florida hospital against the stated wishes of the parents. The hospital admitted that the boy was circumcised by mistake; the mother has sued the hospital and the doctor involved in the case.

Before [[glasnost]], according to an article in [[The Jewish Press]], Jewish ritual circumcision was forbidden in the [[Soviet Union|USSR]]. However, David E. Fishman, professor of Jewish History at the [[Jewish Theological Seminary of America]], states that, whereas the "[[Cheder|heder]]" and "[[yeshiva]]", the organs of Jewish education, "were banned by virtue of the law separating church and school, and subjected to tough police and administrative actions," circumcision was not proscribed by law or suppressed by executive measures.
Jehoshua A. Gilboa writes that while circumcision was not officially or explicitly banned, pressure was exerted to make it difficult. "[[Mohel]]s" in particular were concerned that they could be punished for any health issue that might develop, even if it arose some time after the circumcision.



[[Category:Circumcision debate]]
[[Category:Tort law]]
[[Category:Common law]]

[[fr:Circoncision#Aspects juridiques]]

</doc>
<doc id="7449" url="https://en.wikipedia.org/wiki?curid=7449" title="Called to Common Mission">
Called to Common Mission

Called to Common Mission is an agreement between The Episcopal Church and the Evangelical Lutheran Church in America (ELCA), establishing full communion between them. It was ratified by the ELCA in 1999, the ECUSA in 2000, after the narrow failure of a previous agreement. Its principal author on the Episcopal side was the J. Robert Wright. Under the agreement, they recognize the validity of each other's baptisms and ordinations. The agreement provided that the ELCA would accept the historical episcopate, something which became controversial in the ELCA. In response to concerns about the meaning of CCM, bishops in the ELCA drafted Tucson Resolution, which presented the official ELCA position.

Some within the ELCA argued that requiring the historic episcopate would contradict the traditional Lutheran doctrine that the church exists wherever the Word is preached and Sacraments are practiced. Others objected on the grounds that adopting the Episcopal view on priestly orders and hierarchical structure was contrary to the Lutheran concept of the priesthood of all believers, which holds that all Christians stand on equal footing before God. They argued that the Old Covenant required a priest to mediate between God and humanity, but that New Covenant explicitly abolishes the need for priestly role by making every Christian a priest with direct access to God's grace. Still others objected because of the implied directive that lay presidency would be abolished. This was a particularly issue for rural congregations that periodically "called" a congregation member to conduct communion services in the absence of ordained clergy.




</doc>
<doc id="7450" url="https://en.wikipedia.org/wiki?curid=7450" title="Context menu">
Context menu

A context menu (also called contextual, shortcut, and pop up or pop-up menu) is a menu in a graphical user interface (GUI) that appears upon user interaction, such as a right-click mouse operation. A context menu offers a limited set of choices that are available in the current state, or context, of the operating system or application to which the menu belongs. Usually the available choices are actions related to the selected object. From a technical point of view, such a context menu is a graphical control element.

Context menus first appeared in the Smalltalk environment on the Xerox Alto computer, where they were called "pop-up menus"; they were invented by Dan Ingalls in the mid-1970s.

Microsoft Office v3.0 introduced the context menu for copy and paste functionality in 1990. Lotus 1-2-3/G for OS/2 v1.0 added additional formatting options in 1991. Borland Quattro Pro for Windows v1.0 introduced the Properties context menu option in 1992.

Context menus are opened via various forms of user interaction that target a region of the GUI that supports context menus. The specific form of user interaction and the means by which a region is targeted vary:
Windows mouse click behavior is such that the context menu doesn't open while the mouse button is pressed, but only opens the menu when the button is released, so the user has to click again (this time with the first mouse button) to select a context menu item. This behavior differs from that of macOS and most free software GUIs.

Context menus are sometimes hierarchically organized, allowing navigation through different levels of the menu structure. The implementations differ: Microsoft Word was one of the first applications to only show sub-entries of some menu entries after clicking an arrow icon on the context menu, otherwise executing an action associated with the parent entry. This makes it possible to quickly repeat an action with the parameters of the previous execution, and to better separate options from actions.

The following window managers provide context menu functionality:


Context menus have received some criticism from usability analysts when improperly used, as some applications make certain features "only" available in context menus, which may confuse even experienced users (especially when the context menus can only be activated in a limited area of the application's client window).

Context menus usually open in a fixed position under the pointer, but when the pointer is near a screen edge the menu will be displaced - thus reducing consistency and impeding use of muscle memory. If the context menu is being triggered by keyboard, such as by using Shift + F10, the context menu appears near the focused widget instead of the position of the pointer, to save recognition efforts.

Microsoft's guidelines call for always using the term "context menu", and explicitly deprecate "shortcut menu".



</doc>
<doc id="7451" url="https://en.wikipedia.org/wiki?curid=7451" title="Jews as the chosen people">
Jews as the chosen people

In Judaism, "chosenness" is the belief that the Jews, via descent from the ancient Israelites, are the chosen people, i.e. chosen to be in a covenant with God. The idea of the Israelites being chosen by God is found most directly in the Book of Deuteronomy as the verb "bahar" (), and is alluded to elsewhere in the Hebrew Bible using other terms such as "holy people". Much is written about these topics in rabbinic literature. The three largest Jewish denominations— Orthodox Judaism, Conservative Judaism and Reform Judaism—maintain the belief that the Jews have been chosen by God for a purpose. Sometimes this choice is seen as charging the Jewish people with a specific mission — to be a light unto the nations, and to exemplify the covenant with God as described in the Torah.

This view, however, did not preclude a belief that God has a relationship with other peoples — rather, Judaism held that God had entered into a covenant with all humankind, and that Jews and non-Jews alike have a relationship with God. Biblical references as well as rabbinic literature support this view: Moses refers to the "God of the spirits of all flesh" (), and the Tanakh (Hebrew Bible) also identifies prophets outside the community of Israel. Based on these statements, some rabbis theorized that, in the words of Nethanel ibn Fayyumi, a Yemenite Jewish theologian of the 12th century, "God permitted to every people something he forbade to others...<nowiki>[and]</nowiki> God sends a prophet to every people according to their own language."(Levine, 1907/1966) The Mishnah states that "Humanity was produced from one man, Adam, to show God's greatness. When a man mints a coin in a press, each coin is identical. But when the King of Kings, the Holy One, blessed be He, creates people in the form of Adam not one is similar to any other." (Mishnah Sanhedrin 4:5) The Mishnah continues, and states that anyone who kills or saves a single human, not Jewish, life, has done the same (save or kill) to an entire world. The Tosefta, an important supplement to the Mishnah, also states: "Righteous people of all nations have a share in the world to come" (Sanhedrin 105a).

According to the Israel Democracy Institute, approximately two thirds of Israeli Jews believe that Jews are the "chosen people".

According to the Bible, Israel's character as the chosen people is unconditional as it says in , 
The Torah also says, 
God promises that he will never exchange his people with any other:

Other Torah verses about chosenness, 

The obligation imposed upon the Israelites was emphasized by the prophet Amos (): 

Sometimes this choice is seen as charging the Jewish people with a specific mission — to be a light unto the nations, and to exemplify the covenant with God as described in the Torah. This view, however, did not preclude a belief that God has a relationship with other peoples — rather, Judaism held that God had entered into a covenant with all humankind, and that Jews and non-Jews alike have a relationship with God.
Biblical references as well as rabbinic literature support this view: Moses refers to the "God of the spirits of all flesh" (), and the Tanakh (Hebrew Bible) also identifies prophets outside the community of Israel. Based on these statements, some rabbis theorized that, in the words of Nethanel ibn Fayyumi, a Yemenite Jewish theologian of the 12th century, "God permitted to every people something he forbade to others...<nowiki>[and]</nowiki> God sends a prophet to every people according to their own language."(Levine, 1907/1966) The Mishnah states that "Humanity was produced from one man, Adam, to show God's greatness. When a man mints a coin in a press, each coin is identical. But when the King of Kings, the Holy One, blessed be He, creates people in the form of Adam not one is similar to any other." (Mishnah Sanhedrin 4:5) The Mishnah continues, and states that anyone who kills or saves a single human, not Jewish, life, has done the same (save or kill) to an entire world. The Tosefta, a collection of important post-Talmudic discourses, also states: "Righteous people of all nations have a share in the world to come" (Sanhedrin 105a).

Most Jewish texts do not state that "God chose the Jews" by itself. Rather, this is usually linked with a mission or purpose, such as proclaiming God's message among all the nations, even though Jews cannot become "unchosen" if they shirk their mission. This implies a special duty, which evolves from the belief that Jews have been pledged by the covenant which God concluded with the biblical patriarch Abraham, their ancestor, and again with the entire Jewish nation at Mount Sinai. In this view, Jews are charged with living a holy life as God's priest-people.

In the Jewish prayerbook (the Siddur), chosenness is referred to in a number of ways. The blessing for reading the Torah reads, "Praised are You, Lord our God, King of the Universe, Who has chosen us out of all the nations and bestowed upon us His Torah." In the "Kiddush", a prayer of sanctification, in which the Sabbath is inaugurated over a cup of wine, the text reads, "For you have chosen us and sanctified us out of all the nations, and have given us the Sabbath as an inheritance in love and favour. Praised are you, Lord, who hallows the Sabbath." In the "Kiddush" recited on festivals it reads, "Blessed are You ... who have chosen us from among all nations, raised us above all tongues, and made us holy through His commandments." The Aleinu prayer refers to the concept of Jews as a chosen people: "It is our duty to praise the Master of all, to exalt the Creator of the Universe, who has not made us like the nations of the world and has not placed us like the families of the earth; who has not designed our destiny to be like theirs, nor our lot like that of all their multitude. We bend the knee and bow and acknowledge before the Supreme King of Kings, the Holy One, blessed be he, that it is he who stretched forth the heavens and founded the earth. His seat of glory is in the heavens above; his abode of majesty is in the lofty heights.

According to the Rabbis, "Israel is of all nations the most willful or headstrong one, and the Torah was to give it the right scope and power of resistance, or else the world could not have withstood its fierceness."

"The Lord offered the Law to all nations; but all refused to accept it except Israel."

How do we understand "A Gentile who consecrates his life to the study and observance of the Law ranks as high as the high priest", says R. Meïr, by deduction from Lev. xviii. 5; II Sam. vii. 19; Isa. xxvi. 2; Ps. xxxiii. 1, cxviii. 20, cxxv. 4, where all stress is laid not on Israel, but on man or the righteous one.
Maimonides states: "It is now abundantly clear that the pledges Hashem made to Avraham and his descendants would be fulfilled exclusively first in Yitzchak and then in Yaakov, Yitzchak son. This is confirmed by a passage that states, “He is ever mindful of His covenant . . . that He made with Avraham, swore to Yitzchak, and confirmed in a decree for Yaakov, for Yisrael, as an eternal covenant (Tehillim 105: 8,9).

The Gemara states this regarding a non-Jew who studies Torah [his 7 mitzvot] and regarding this, see Shita Mekubetzes, Bava Kama 38a who says that this is an exaggeration. In any case, this statement was not extolling the non-Jew. The Rishonim explain that it is extolling the Torah.

Tosfos explains that it uses the example of a "kohen gadol" (high priest), because this statement is based on the verse, ""y'kara hi mipnimim"" (it is more precious than pearls). This is explained elsewhere in the Gemara to mean that the Torah is more precious "pnimim" (translated here as "inside" instead of as "pearls"; thus that the Torah is introspectively absorbed into the person), which refers to "lifnai v'lifnim" (translated as "the most inner of places"), that is the Holy of Holies where the "kahon gadol" went.

In any case, in Midrash Rabba (Bamidbar 13:15) this statement is made with an important addition: a non-Jew who converts and studies Torah etc.

The Nation of Israel is likened to the olive. Just as this fruit yields its precious oil only after being much pressed and squeezed, so Israel's destiny is one of great oppression and hardship, in order that it may thereby give forth its illuminating wisdom. Poverty is the quality most befitting Israel as the chosen people (Ḥag. 9b). Only on account of its good works is Israel among the nations "as the lily among thorns", or "as wheat among the chaff."

Rabbi Lord Immanuel Jakobovits, former Chief Rabbi of the United Synagogue of Great Britain (Modern Orthodox Judaism), described chosenness in this way: "Yes, I do believe that the chosen people concept as affirmed by Judaism in its holy writ, its prayers, and its millennial tradition. In fact, I believe that every people—and indeed, in a more limited way, every individual—is "chosen" or destined for some distinct purpose in advancing the designs of Providence. Only, some fulfill their mission and others do not. Maybe the Greeks were chosen for their unique contributions to art and philosophy, the Romans for their pioneering services in law and government, the British for bringing parliamentary rule into the world, and the Americans for piloting democracy in a pluralistic society. The Jews were chosen by God to be 'peculiar unto Me' as the pioneers of religion and morality; that was and is their national purpose."
Modern Orthodox theologian Michael Wyschogrod wrote: "[T]he initial election of Abraham himself was not earned. ... We are simply told that God commanded Abraham to leave his place of birth and go to a land that God would show him. He is also promised that his descendants will become a numerous people. But nowhere does the Bible tell us why Abraham rather than someone else was chosen. The implication is that God chooses whom He wishes and that He owes no accounting to anyone for His choices."

Rabbi Norman Lamm, a leader of Modern Orthodox Judaism wrote: "The chosenness of Israel relates exclusively to its spiritual vocation embodied in the Torah; the doctrine, indeed, was announced at Sinai. Whenever it is mentioned in our liturgy—such as the blessing immediately preceding the Shema...it is always related to Torah or Mitzvot ("commandments"). This spiritual vocation consists of two complementary functions, described as "Goy Kadosh", that of a holy nation, and "Mamlekhet Kohanim", that of a kingdom of priests. The first term denotes the development of communal separateness or differences in order to achieve a collective self-transcendence [...] The second term implies the obligation of this brotherhood of the spiritual elite toward the rest of mankind; priesthood is defined by the prophets as fundamentally a teaching vocation."

Conservative Judaism, views the concept of chosenness in this way: "Few beliefs have been subject to as much misunderstanding as the 'Chosen People' doctrine. The Torah and the Prophets clearly stated that this does not imply any innate Jewish superiority. In the words of Amos (3:2) 'You alone have I singled out of all the families of the earth—that is why I will call you to account for your iniquities.' The Torah tells us that we are to be "a kingdom of priests and a holy nation" with obligations and duties which flowed from our willingness to accept this status. Far from being a license for special privilege, it entailed additional responsibilities not only toward God but to our fellow human beings. As expressed in the blessing at the reading of the Torah, our people have always felt it to be a privilege to be selected for such a purpose. For the modern traditional Jew, the doctrine of the election and the covenant of Israel offers a purpose for Jewish existence which transcends its own self interests. It suggests that because of our special history and unique heritage we are in a position to demonstrate that a people that takes seriously the idea of being covenanted with God can not only thrive in the face of oppression, but can be a source of blessing to its children and its neighbors. It obligates us to build a just and compassionate society throughout the world and especially in the land of Israel where we may teach by example what it means to be a 'covenant people, a light unto the nations.'"

Rabbi Reuven Hammer comments on the excised sentence in the Aleinu prayer mentioned above: "Originally the text read that God has not made us like the nations who "bow down to nothingness and vanity, and pray to an impotent god", [...] In the Middle Ages these words were censored, since the church believed they were an insult to Christianity. Omitting them tends to give the impression that the Aleinu teaches that we are both different and better than others. The actual intent is to say that we are thankful that God has enlightened us so that, unlike the pagans, we worship the true God and not idols. There is no inherent superiority in being Jewish, but we do assert the superiority of monotheistic belief over paganism. Although paganism still exists today, we are no longer the only ones to have a belief in one God."

Reform Judaism views the concept of chosenness in this way: "Throughout the ages it has been Israel's mission to witness to the Divine in the face of every form of paganism and materialism. We regard it as our historic task to cooperate with all men in the establishment of the kingdom of God, of universal brotherhood, Justice, truth and peace on earth. This is our Messianic goal." In 1999 the Reform movement stated, "We affirm that the Jewish people are bound to God by an eternal covenant, as reflected in our varied understandings of Creation, Revelation and Redemption [...] We are Israel, a people aspiring to holiness, singled out through our ancient covenant and our unique history among the nations to be witnesses to God's presence. We are linked by that covenant and that history to all Jews in every age and place."

Many Kabbalistic sources, notably the Tanya, contain statements to the effect that the Jewish soul is qualitatively different from the non-Jewish soul. A number of known Chabad rabbis offered alternative readings of the Tanya, did not take this teaching literally, and even managed to reconcile it with the leftist ideas of internationalism and class struggle. The original text of the Tanya refers to the "idol worshippers" and does not mention the "nations of the world" at all, although such interpretation was endorsed by Menachem Mendel Schneerson and is popular in contemporary Chabad circles. Hillel of Parich, an early Tanya commentator, wrote that the souls of righteous Gentiles are more similar to the Jewish souls, and are generally good and not egoistic. This teaching was accepted by Schneerson and is considered normative in Chabad.

According to the author of the Tanya himself, a righteous non-Jew can achieve a high level of spiritually, similar to an angel, although his soul is still fundamentally different in character, but not value, from a Jewish one. Tzemach Tzedek, the third rebbe of Chabad, wrote that the Muslims are naturally good-hearted people. Rabbi Yosef Jacobson, a popular contemporary Chabad lecturer, teaches that in today's world most non-Jews belong to the category of righteous Gentiles, effectively rendering the Tanya's attitude anachronistic.

Dov Ber Pinson, a contemporary Chabad mystic, denies the idea that there is any essential difference between the Jews and non-Jews. According to his theory, every person has a lower animalistic and higher Godly soul. The Tanya does not talk about Jews and non-Jews as social groups, but describes the internal struggle between the materialistic "Gentile" and spiritual "Jewish" levels of consciousness within every human soul.

An anti-Zionist interpretation of Tanya was offered by Abraham Yehudah Khein, a prominent Ukrainian Chabad rabbi, who supported anarchist communism and considered Peter Kropotkin a great Tzaddik. Khein basically read the Tanya backwards; since the souls of idol worshipers are known to be evil, according to the Tanya, while the Jewish souls are known to be good, he concluded that truly altruistic people are really Jewish, in a spiritual sense, while Jewish nationalists and class oppressors are not. By this logic, he claimed that Vladimir Solovyov and Rabindranath Tagore probably have Jewish souls, while Leon Trotsky and other totalitarians do not, and many Zionists, whom he compared to apes, are merely "Jewish by birth certificate".

Nachman of Breslov also believed that Jewishness is a level of consciousness, and not an intrinsic inborn quality. He wrote that, according to the Book of Malachi, one can find "potential Jews" among all nations, whose souls are illuminated by the leap of "holy faith", which "activated" the Jewishness in their soul. These people would otherwise convert to Judaism, but prefer not to do so. Instead, they recognize the Divine unity within their pagan religions.

Isaac Arama, an influential philosopher and mystic of the 15th century, believed that righteous non-Jews are spiritually identical to the righteous Jews. Rabbi Menachem Meiri, a famous Catalan Talmudic commentator and Maimonidian philosopher, considered all people, who sincerely profess an ethical religion, to be part of a greater "spiritual Israel". He explicitly included Christians and Muslims in this category. Meiri rejected all Talmudic laws that discriminate between the Jews and non-Jews, claiming that they only apply to the ancient idolators, who had no sense of morality. The only exceptions are a few laws related directly or indirectly to intermarriage, which Meiri did recognize.

Meiri applied his idea of "spiritual Israel" to the Talmudic statements about unique qualities of the Jewish people. For example, he believed that the famous saying that Israel is above astrological predestination ("Ein Mazal le-Israel") also applied to the followers of other ethical faiths. He also considered countries, inhabited by decent moral non-Jews, such as Languedoc, as a spiritual part of the Holy Land.

One Jewish critic of chosenness was the philosopher Baruch Spinoza. In the third chapter of his "Theologico-Political Treatise", Spinoza mounts an argument against a naive interpretation of God's choice of the Jews. Bringing evidence from the Bible itself, he argues that God's choice of Israel was not unique (he had chosen other nations before choosing the Hebrew nation) and that the choice of the Jews is neither inclusive (it does not include all of the Jews, but only the 'pious' ones) nor exclusive (it also includes 'true gentile prophets'). Finally, he argues that God's choice is not unconditional. Recalling the numerous times God threatened the complete destruction of the Hebrew nation, he asserts that this choice is neither absolute, nor eternal, nor necessary.

Reconstructionist Judaism rejects the concept of chosenness. Its founder, Rabbi Mordecai Kaplan, said that the idea that God chose the Jewish people leads to racist beliefs among Jews, and thus must be excised from Jewish theology. This rejection of chosenness is made explicit in the movement's siddurim (prayer books). For example, the original blessing recited before reading from the Torah contains the phrase, "asher bahar banu mikol ha’amim"—"Praised are you Lord our God, ruler of the Universe, "who has chosen us from among all peoples" by giving us the Torah." The Reconstructionist version is rewritten as "asher kervanu la’avodato", "Praised are you Lord our God, ruler of the Universe, "who has drawn us to your service" by giving us the Torah." In the mid-1980s, the Reconstructionist movement issued its "Platform on Reconstructionism". It states that the idea of chosenness is "morally untenable", because anyone who has such beliefs "implies the superiority of the elect community and the rejection of others."

Not all Reconstructionists accept this view. The newest siddur of the movement, "Kol Haneshamah", includes the traditional blessings as an option, and some modern Reconstructionist writers have opined that the traditional formulation is not racist, and should be embraced.

An original prayer book, by Reconstructionist feminist poet Marcia Falk, "The Book of Blessings", has been widely accepted by both Reform and Reconstructionist Jews. Falk rejects all concepts relating to hierarchy or distinction; she sees any distinction as leading to the acceptance of other kinds of distinctions, thus leading to prejudice. She writes that as a politically liberal feminist, she must reject distinctions made between men and women, homosexuals and heterosexuals, Jews and non-Jews, and to some extent even distinctions between the Sabbath and the other six days of the week. She thus rejects the idea of chosenness as unethical. She also rejects Jewish theology in general, and instead holds to a form of religious humanism. Falk writes: "The idea of Israel as God's chosen people [...] is a key concept in rabbinic Judaism. Yet it is particularly problematic for many Jews today, in that it seems to fly in the face of monotheistic belief that all humanity is created in the divine image - and hence, all humanity is equally loved and valued by God [...] I find it difficult to conceive of a feminist Judaism that would incorporate it in its teaching: the valuing of one people "over and above" others is all too analogous to the privileging of one sex over another." Reconstructionist author Judith Plaskow also criticises the idea of chosenness, for many of the same reasons as Falk. A politically liberal lesbian, Plaskow rejects most distinctions made between men and women, homosexuals and heterosexuals, and Jews and non-Jews. In contrast to Falk, Plaskow does not reject all concepts of difference as inherently leading to unethical beliefs, and holds to a more classical form of Jewish theism than Falk.

A number of responses to these views have been made by Reform and Conservative Jews; they hold that these criticisms are against teachings that do not exist within liberal forms of Judaism, and which are rare in Orthodox Judaism (outside certain Haredi communities, such as Chabad). A separate criticism stems from the very existence of feminist forms of Judaism in all denominations of Judaism, which do not have a problem with the concepts of chosenness.

The children of Israel enjoy a special status in the Islamic book, the Quran (2:47 and 2:122) However, Muslim scholars point out that this status did not confer upon Israelites any racial superiority, and was only valid so long as the Israelites maintain their covenant with God:

Some Christians believe that the Jews were God's chosen people (), but because of Jewish Rejection of Jesus, the Christians in turn received that special status (). This doctrine is known as Supersessionism.

Augustine criticized Jewish chosenness as "carnal." He reasoned that Israel was chosen "according to the flesh."

Avi Beker, an Israeli scholar and former Secretary General of the World Jewish Congress, regarded the idea of the chosen people as Judaism's defining concept and "the central unspoken psychological, historical, and theological problem at the heart of Jewish-Gentile relations." In his book "The Chosen: The History of an Idea, and the Anatomy of an Obsession", Beker expresses the view that the concept of chosenness is the driving force behind Jewish-Gentile relations, explaining both the admiration and, more pointedly, the envy and hatred the world has felt for the Jews in religious and also secular terms. Beker argues that while Christianity has modified its doctrine on the displacement of the Jews, Islam has neither reversed nor reformed its theology concerning the succession of both the Jews and the Christians. According to Beker, this presents a major barrier to conflict resolution in the Arab-Israeli conflict.

Israeli philosopher Ze’ev Levy writes that chosenness can be "(partially) justified only from the historical angle" with respect to its spiritual and moral contribution to Jewish life through the centuries, "a powerful agent of consolation and hope". He points out however that modern anthropological theories "do not merely proclaim the inherent universal equality of all people [as] human beings; they also stress the "equivalence" of all human cultures." (emphasis in original) He continues that "there are no inferior and superior people or cultures but only different, "other", ones." He concludes that the concept of chosenness entails ethnocentrism, "which does not go hand in hand with otherness, that is, with unconditional respect of otherness".

Some people have claimed that Judaism's chosen people concept is racist because it implies that Jews are superior to non-Jews. The Anti-Defamation League asserts that the concept of a chosen people within Judaism has nothing to do with racial superiority.






</doc>
<doc id="7453" url="https://en.wikipedia.org/wiki?curid=7453" title="Christian persecution">
Christian persecution

Christian persecution may refer to:







</doc>
<doc id="7455" url="https://en.wikipedia.org/wiki?curid=7455" title="Chaparral">
Chaparral

Chaparral is a shrubland or heathland plant community found primarily in the US state of California and in the northern portion of the Baja California Peninsula, Mexico. It is shaped by a Mediterranean climate (mild, wet winters and hot dry summers) and wildfire, featuring summer-drought-tolerant plants with hard sclerophyllous evergreen leaves, as contrasted with the associated soft-leaved, drought-deciduous, scrub community of coastal sage scrub, found below the chaparral biome. Chaparral covers 5% of the state of California, and associated Mediterranean shrubland an additional 3.5%. The name comes from the Spanish word for scrub oak, "chaparro".

In its natural state, chaparral is characterized by infrequent fires, with intervals ranging between 10–15 years and over a hundred years. Mature chaparral (stands that have survived for greater intervals between fires) is characterized by nearly impenetrable, dense thickets (except the more open chaparral of the desert). These plants are highly flammable during the late summer and autumn months when conditions are characteristically hot and dry. They grow as woody shrubs with thick, leathery, and often small leaves, contain green leaves all year (are evergreen), and are typically drought resistant (with some exceptions). After the first rains following a fire, the landscape is dominated by small flowering herbaceous plants, known as fire followers, which die back with the summer dry period.

Similar plant communities are found in the four other Mediterranean climate regions around the world, including the Mediterranean Basin (where it is known as maquis), central Chile (where it is called matorral), the South African Cape Region (known there as fynbos), and in Western and Southern Australia (as kwongan). According to the California Academy of Sciences, Mediterranean shrubland contains more than 20 percent of the world's plant diversity. The word "chaparral" is a loan word from Spanish "chaparro", meaning both "small" and "dwarf" evergreen oak, which itself comes from a Basque word, "txapar", that has the same meaning.

Conservation International and other conservation organizations consider chaparral to be a biodiversity hotspot – a biological community with a large number of different species – that is under threat by human activity.

The California chaparral and woodlands ecoregion, of the Mediterranean forests, woodlands, and scrub biome, has three sub-ecoregions with ecosystem—plant community subdivisions:

For the numerous individual plant and animal species found within the California chaparral and woodlands ecoregion, see:

Some of the indicator plants of the California chaparral and woodlands ecoregion include:
Chaparral soils and nutrient composition

Soils in the California chaparral are made of serpentine rock and are generally low in essential nutrients such as nitrogen. Another characteristic of these soils is that they are ultramafic, meaning they have a high ratio of magnesium and iron to calcium and potassium.

Another phytogeography system uses two California chaparral and woodlands subdivisions: the cismontane chaparral and the transmontane (desert) chaparral.

Cismontane chaparral ("this side of the mountain") refers to the chaparral ecosystem in the Mediterranean forests, woodlands, and scrub biome in California, growing on the western (and coastal) sides of large mountain range systems, such as the western slopes of the Sierra Nevada in the San Joaquin Valley foothills, western slopes of the Peninsular Ranges and California Coast Ranges, and south-southwest slopes of the Transverse Ranges in the Central Coast and Southern California regions.

In Central and Southern California chaparral forms a dominant habitat. Members of the chaparral biota native to California, all of which tend to regrow quickly after fires, include:


The complex ecology of chaparral habitats supports a very large number of animal species. The following is a short list of birds which are an integral part of the cismontane chaparral ecosystems.



Transmontane chaparral or desert chaparral —"transmontane" ("the other side of the mountain") "chaparral"—refers to the desert shrubland habitat and chaparral plant community growing in the rainshadow of these ranges. Transmontane chaparral features xeric desert climate, not Mediterranean climate habitats, and is also referred to as desert chaparral. Desert chaparral is a regional ecosystem subset of the deserts and xeric shrublands biome, with some plant species from the California chaparral and woodlands ecoregion. Unlike cismontane chaparral, which forms dense, impenetrable stands of plants, desert chaparral is open, with only about 50 percent of the ground covered. Individual shrubs can reach up to in height.
Transmontane chaparral or desert chaparral is found on the eastern slopes of major mountain range systems on the western sides of the deserts of California. The mountain systems include the southeastern Transverse Ranges (the San Bernardino and San Gabriel Mountains) in the Mojave Desert north and northeast of the Los Angeles basin and Inland Empire; and the northern Peninsular Ranges (San Jacinto, Santa Rosa, and Laguna Mountains), which separate the Colorado Desert (western Sonoran Desert) from lower coastal Southern California. It is distinguished from the cismontane chaparral found on the coastal side of the mountains, which experiences higher winter rainfall. Naturally, desert chaparral experiences less winter rainfall than cismontane chaparral. Plants in this community are characterized by small, hard (sclerophyllic) evergreen (non-deciduous) leaves. Desert chaparral grows above California's desert cactus scrub plant community and below the pinyon-juniper woodland. It is further distinguished from the deciduous sub-alpine scrub above the pinyon-juniper woodlands on the same side of the Peninsular ranges.

Transmontane (desert) chaparral typically grows on the lower ( elevation) northern slopes of the southern Transverse Ranges (running east to west in San Bernardino and Los Angeles counties) and on the lower () eastern slopes of the Peninsular Ranges (running south to north from lower Baja California to Riverside and Orange counties and the Transverse Ranges). It can also be found in higher-elevation sky islands in the interior of the deserts, such as in the upper New York Mountains within the Mojave National Preserve in the Mojave Desert.

The California transmontane (desert) chaparral is found in the rain shadow deserts of the following:


There is overlap of animals with those of the adjacent desert and pinyon-juniper communities.

Chaparral is a coastal biome with hot, dry summers and mild, rainy winters. The Chaparral area receives about of precipitation a year. This makes the chaparral most vulnerable to fire in the late summer and fall.

The chaparral ecosystem as a whole is adapted to be able to recover from infrequent wildfires (fires occurring a minimum of 15 years apart); indeed, chaparral regions are known culturally and historically for their impressive fires. (This does create a conflict with human development adjacent to and expanding into chaparral systems.) Additionally, Native Americans burned chaparral to promote grasslands for textiles and food. Before a major fire, typical chaparral plant communities are dominated by manzanita, chamise (also called greasewood; "Adenostoma fasciculatum") and "Ceanothus" species, toyon (which can sometimes be interspersed with scrub oaks), and other drought-resistant shrubs with hard (sclerophyllous) leaves; these plants resprout (see resprouter) from underground burls after a fire. The shoots of these plants are, however, not resistant to chaparral crown-fire regimes as the bark is simply not thick enough. Plants that are long-lived in the seed bank or serotenous with induced germination after fire include chamise", Ceanothus," and fiddleneck"." Some chaparral plant communities may grow so dense and tall that it becomes difficult for large animals and humans to penetrate, but may be teeming with smaller fauna in the understory. Many chaparral plant species require some fire cue (heat, smoke, or charred wood, and chemical changes in the soil following fires) for germination. Others, such as annual and herbaceous species like "Phacelia" require fires to allow sunlight to reach them, and are known as fire followers. During the time shortly after a fire, chaparral communities may contain soft-leaved herbaceuous annual plants that dominate the community for the first few years – until the burl resprouts and seedlings of chaparral perennials create an overstory, blocking the sunlight from other plants in the community. When the overstory regrows, seeds of annuals and smaller plants may lie dormant until the next fire creates the conditions required for germination. Mid-sized plants such as "Ceonothus" fix nitrogen, while others cannot, which, together with the need for exposure to the sun, creates a symbiotic relationship of the entire community with infrequent fires.

Because of the hot, dry conditions that exist in the California summer and fall, chaparral is one of the most fire-prone plant communities in North America. Some fires are caused by lightning, but these are usually during periods of high humidity and low winds and are easily controlled. Nearly all of the very large wildfires are caused by human activity during periods of very hot, dry easterly Santa Ana winds. These man-made fires are commonly caused by power line failures, vehicle fires and collisions, sparks from machinery, arson, or campfires. In natural Chaparral communities without human interference, the fires are ignition-prone as there are plenty of ground fuels and the temperatures are fire-permitting during the dry season.

Though adapted to infrequent fires, chaparral plant communities can be exterminated by frequent fires. A moderate frequency of fire (less than ten years) will result in the loss of seeder plants such as This moderate frequency disallows seeder plants to reach their reproductive size before the next fire and the community shifts to a sprouter-dominance. High frequency fires (less than five years) can cause the additional loss of sprouters by exhausting their reserves below-ground. Today, frequent accidental ignitions can convert chaparral from a native shrubland to non-native annual grassland and drastically reduce species diversity, especially under drought brought about by climate change.

There are two assumptions relating to California chaparral fire regimes that appear to have caused considerable debate, and sometimes confusion and controversy, within the fields of wildfire ecology and land management. 

The perspective that older chaparral is unhealthy or unproductive may have originated during the 1940s when studies were conducted measuring the amount of forage available to deer populations in chaparral stands. However, according to recent studies, California chaparral is extraordinarily resilient to very long periods without fire and continues to maintain productive growth throughout pre-fire conditions. Seeds of many chaparral plants actually require 30 years or more worth of accumulated leaf litter before they will successfully germinate (e.g., scrub oak, "Quercus berberidifolia"; toyon, "Heteromeles arbutifolia"; and holly-leafed cherry, "Prunus ilicifolia"). When intervals between fires drop below 10 to 15 years, many chaparral species are eliminated and the system is typically replaced by non-native, invasive, weedy grassland.

The idea that older chaparral is responsible for causing large fires was originally proposed in the 1980s by comparing wildfires in Baja California and southern California . It was suggested that fire suppression activities in southern California allowed more fuel to accumulate, which in turn led to larger fires (in Baja, fires often burn without active suppression efforts ). This is similar to the argument that fire suppression in western United States has allowed ponderosa pine forests to become “overstocked”. In the past, surface fires burned through these forests at intervals of anywhere between 4 and 36 years, clearing out the understory and creating a more ecologically balanced system. However, chaparral has a crown-fire regime, meaning that fires consume the entire system whenever they burn, with a historical frequency of 30 to 50 years. In one study, a detailed analysis of historical fire data concluded that fire suppression activities have been ineffective at excluding fire from southern California chaparral, unlike in ponderosa pine forests. In addition, the number of fires is increasing in step with population growth. Chaparral stand age does not have a significant correlation to its tendency to burn. Low humidity, low fuel moisture, and high winds appear to be the primary factors in determining when and where a chaparral fire occurs and how large it becomes. Fires can be beneficial to plant communities by clearing away canopies of litter, inducing serotenous germination, and sanitizing the soils from pathogens.





</doc>
<doc id="7456" url="https://en.wikipedia.org/wiki?curid=7456" title="CJD">
CJD

CJD can mean:


</doc>
<doc id="7460" url="https://en.wikipedia.org/wiki?curid=7460" title="Clinker">
Clinker

Clinker may refer to:


Clinker may also refer to:



</doc>
<doc id="7461" url="https://en.wikipedia.org/wiki?curid=7461" title="Clipper">
Clipper

A clipper was a very fast sailing ship of the middle third of the 19th century, generally either a schooner or a brigantine. The original Baltimore clippers were schooners. They had multiple types of sail plans but the most common was three masts and a square rig. They were generally narrow for their length, small by later 19th century standards, could carry limited bulk freight, and had a large total sail area. Clipper ships were mostly constructed in British and American shipyards, though France, Brazil, the Netherlands and other nations also produced some. Clippers sailed all over the world, primarily on the trade routes between the United Kingdom and its colonies in the east, in trans-Atlantic trade, and on the New York-to-San Francisco route round Cape Horn during the California Gold Rush. Dutch clippers were built beginning in the 1850s for the tea trade and passenger service to Java.

The boom years of the clipper ship era began in 1843 as a result of a growing demand for a more rapid delivery of tea from China. It continued under the stimulating influence of the discovery of gold in California and Australia in 1848 and 1851, and ended with the opening of the Suez Canal in 1869.

The term "clipper" most likely derives from the verb "clip", which in former times meant, among other things, to run or fly swiftly. Dryden, the English poet, used the word "clip" to describe the swift flight of a falcon in the 17th century when he said "And, with her eagerness the quarry missed, Straight flies at check, and clips it down the wind." The ships appeared to clip along the ocean water. The term "clip" became synonymous with "speed" and was also applied to fast horses and sailing ships. "To clip it," and "going at a good clip," are familiar expressions to this day.
While the first application of the term "clipper" in a nautical sense is by no means certain, it seems to have had an American origin when applied to the Baltimore clippers of the late 18th century. When these vessels of a new model were built, which were intended to "clip" over the waves rather than plough through them, the improved type of craft became known as "clippers" because of their speed.

In England the nautical term "clipper" appeared a little later. The "Oxford English Dictionary" says its earliest quotation for "clipper" is from 1830. This does not mean, however, that little British opium clippers from prior to 1830 were not called "opium clippers" just as they are today. Carl C. Cutler reports the first newspaper appearance was in 1835, and by then the term was apparently familiar. An undated painting of the British "Water Witch" built in 1831 is labeled "OPIUM CLIPPER "WATER WITCH"" so the term had at least passed into common usage during the time that this ship sailed.

There is no single definition of the characteristics of a clipper ship, but mariner and author Alan Villiers describes them as follows:To sailors, three things made a ship a clipper. She must be sharp-lined, built for speed. She must be tall-sparred and carry the utmost spread of canvas. And she must "use" that sail, day and night, fair weather and foul. Optimized for speed, they were too fine-lined to carry much cargo. Clippers typically carried extra sails such as skysails and moonrakers on the masts, and studdingsails on booms extending out from the hull or yards, which required extra sailors to handle them. In conditions where other ships would shorten sail, clippers drove on, heeling so much that their lee rails were in the water.

A clipper is often confused with a windjammer, but they are two completely different types of ship. Clippers were optimized for speed only and carrying highly priced cargo in small quantities, such as tea, spices or opium; a windjammer is a large sailing ship which, although it carries a large sail area, is optimized, by contrast, for cargo capacity, ease of handling and carrying low priced bulk cargo, such as grain, fertilizers or lumber. Whereas clippers had very short lifespans—most were scrapped after only two decades of service—windjammers could have fifty or more years of service life, and several windjammers are still today in use as school ships.

The first ships to which the term "clipper" seems to have been applied were the Baltimore clippers. Baltimore clippers were topsail schooners developed in the Chesapeake Bay before the American Revolution, and which reached their zenith between 1795 and 1815. They were small, rarely exceeding 200 tons OM, and modelled after French luggers. Some were lightly armed in the War of 1812, sailing under Letters of Marque and Reprisal, when the type—exemplified by "Chasseur", launched at Fells Point, Baltimore in 1814—became known for her incredible speed; the deep draft enabled the Baltimore clipper to sail close to the wind. Clippers, running the British blockade of Baltimore, came to be recognized for speed rather than cargo space.

Speed was also required for the Chinese opium trade between England, India and China. Small, sharp-bowed British vessels were the result. An early example, which is today known as an opium clipper, was "Transit" of 1819. She was followed by many more.

Meanwhile, Baltimore Clippers still continued to be built, and were built specifically for the China opium trade running opium between India and China, a trade that only became unprofitable for American shipowners in 1849.

"Ann McKim" is generally known as the original clipper ship. She was built in Baltimore in 1833 and was the first attempt at building a larger swift vessel in the United States. "Ann McKim", 494 tons OM, was built on the enlarged lines of a Baltimore clipper, with sharply raked stem, counter stern and square rig. She was built in Baltimore in 1833 by the Kennard & Williamson shipyard. Although "Ann McKim" was the first large clipper ship ever constructed, it cannot be said that she founded the clipper ship era, or even that she directly influenced shipbuilders, since no other ship was built like her; but she may have suggested the clipper design in vessels of ship rig. She did, however, influence the building of "Rainbow" in 1845, the first extreme clipper ship.

In Aberdeen, Scotland, the shipbuilders Alexander Hall and Sons developed the "Aberdeen" clipper bow in the late 1830s: the first was "Scottish Maid" launched in 1839. "Scottish Maid", 150 tons OM, was the first British clipper ship. ""Scottish Maid" was intended for the Aberdeen-London trade, where speed was crucial to compete with steamships. The Hall brothers tested various hulls in a water tank and found the clipper design most effective. The design was influenced by tonnage regulations. Tonnage measured a ship's cargo capacity and was used to calculate tax and harbour dues. The new 1836 regulations measured depth and breadth with length measured at half midship depth. Extra length above this level was tax-free and became a feature of clippers. "Scottish Maid" proved swift and reliable and the design was widely copied." The earliest British clipper ships were built for trade amongst the British Isles. Then followed the vast clipper trade of tea, opium, spices and other goods from the Far East to Europe, and the ships became known as "tea clippers".

From 1839, larger American clipper ships started to be built beginning with "Akbar", 650 tons OM, in 1839, and including the 1844-built Houqua, 581 tons OM. These larger vessels were built predominantly for use in the China tea trade and known as "tea clippers". Smaller clipper vessels also continued to be built predominantly for the China opium trade and known as "opium clippers" such as the 1842-built "Ariel", 100 tons OM.

Then in 1845 "Rainbow", 757 tons OM, the first extreme clipper was launched in New York. These American clippers were larger vessels designed to sacrifice cargo capacity for speed. They had a bow lengthened above the water, a drawing out and sharpening of the forward body, and the greatest breadth further aft. Extreme clippers were built in the period 1845 to 1855. 

From 1851 or earlier another type of clipper ship was also being built in American shipyards, the medium clipper. 
In the mid-1800s, shipbuilders in Medford, Massachusetts began building the medium clipper ship. They “quietly evolved a new type (of ship) of about 450 tons burden which, handled by eighteen officers and men, would carry half as much freight as a British-Indianman of 1500 tons with a crew of 125, and sail half again as fast.” Most owners wanted ships that could do all kinds of work and the “finest type” then being built was the Medford or Merrimac East Indianman. An example would be the Columbiana built in Medford in 1837, or Jotham Stetson’s ship the Rajah, 531 tons, 140 feet long which was constructed in the previous year. As Admiral Morison points out, ships built in Medford by the firms of J.O. Curtis, Hayden & Cudworth, S. Lapham “have more fast California passages to their credit, considering the number they built, than those of any other place.” That is quite a record.

In 1851, shipbuilders in Medford, Massachusetts built the "Antelope". Often called the "Antelope of Boston" to distinguish it from other ships of the same name, this vessel is sometimes called one of the first medium clipper ships. A ship-design journalist noted that "the design of her model was to combine large stowage capacity with good sailing qualities." The "Antelope" was relatively flat-floored and had only an 8-inch dead rise at half floor.

The medium clipper, though still very fast, had comparatively more allowance for cargo. After 1854 extreme clippers were replaced in American shipbuilding yards by medium clippers.

The "Flying Cloud" was a clipper ship that set the world's sailing record for the fastest passage between New York and San Francisco, 89 days 8 hours. She held this record for over 100 years, from 1854 to 1989.[1] "Flying Cloud" was the most famous of the clippers built by Donald McKay. She was known for her extremely close race with the "Hornet" in 1853; for having a woman navigator, Eleanor Creesy, wife of Josiah Perkins Creesy, who skippered the "Flying Cloud" on two record-setting voyages from New York to San Francisco; and for sailing in the Australia and timber trades.

Clipper ships largely ceased being built in American shipyards in 1859 when, unlike the earlier boom years, only 4 clipper ships were built. That is except for a small number built in the 1860s. The last American clipper ship was "the Pilgrim" launched in 1873 from the shipyards of Medford, Massachusetts, built by Joshua T. Foster. Among ship owners of the day, “Medford-built” came to mean the best.
During the time from 1859 British clipper ships continued to be built. Earlier British clipper ships had become known as extreme clippers, and were considered to be "as sharp as the American" built ships. From 1859 a new design was developed for British clipper ships that was nothing like the American clippers. These ships built from 1859 continued to be called extreme clippers. The new design had a sleek graceful appearance, less sheer, less freeboard, lower bulwarks, and smaller breadth. They were built for the China tea trade and began with "Falcon" in 1859, and finished with the last ships built in 1870. It is estimated that 25 to 30 of these ships were built, and no more than 4–5 per year. The earlier ships were made from wood, though some were made from iron, just as some British clippers had been made from iron prior to 1859. In 1863 the first tea clippers of composite construction were brought out, combining the best of both worlds. Composite clippers had the strength of iron spars with wooden hulls, and copper sheathing could be added to prevent the fouling that occurred on iron hulls.

After 1869 with the opening of the Suez Canal that greatly advantaged steam vessels (see below, "Decline"), the tea trade then collapsed for clippers. From the late 1860s-early 1870s the clipper trade increasingly focused on trade and the carrying of immigrants between England and Australia and New Zealand, a trade that had begun earlier with the Australian Gold Rush in the 1850s. British-built clipper ships were used for this trade, as were many American-built ships which were sold to British owners. Even in the 1880s, sailing ships were still the main carriers of cargoes to and from Australia and New Zealand. Eventually, however, even this trade became unprofitable, and the aging clipper fleet became unseaworthy.

Among the most notable clippers were the China clippers, also called tea clippers or opium clippers, designed to ply the trade routes between Europe and the East Indies. The last example of these still in reasonable condition was "Cutty Sark", preserved in dry dock at Greenwich, United Kingdom. Damaged by fire on 21 May 2007 while undergoing conservation, the ship was permanently elevated three meters above the dry dock floor in 2010 as part of a plan for long-term preservation.

Before the early 18th century, the East India Company paid for its tea mainly in silver. However, when the Chinese Emperor chose to embargo European manufactured commodities and demand payment for all Chinese goods in silver, the price rose, restricting free trade. The East India Company began to manufacture a product that was desired by the Chinese as much as tea was by the British: opium. This had a significant influence on both India and China. Opium was also imported into Britain and was not prohibited because it was thought to be medically beneficial. Laudanum, which was made from opium was also used as a pain killer, to induce sleep and to suppress anxiety. The famous literary opium addicts Thomas De Quincey, Samuel Taylor Coleridge and Wilkie Collins also took it for its pleasurable effects. The Limehouse area in London was notorious for its opium dens, many of which catered for Chinese sailors as well as English addicts.

Clippers were built for seasonal trades such as tea, where an early cargo was more valuable, or for passenger routes. One passenger ship survives, the City of Adelaide designed by William Pile of Sunderland. The fast ships were ideally suited to low-volume, high-profit goods, such as tea, opium, spices, people, and mail. The return could be spectacular. The "Challenger" returned from Shanghai with ""the most valuable cargo of tea and silk ever to be laden in one bottom"". Competition among the clippers was public and fierce, with their times recorded in the newspapers. The ships had short-expected lifetimes and rarely outlasted two decades of use before they were broken up for salvage. Given their speed and maneuverability, clippers frequently mounted cannon or carronades and were used for piracy, privateering, smuggling, or interdiction service.

The last China clippers were acknowledged as the fastest sail vessels. When fully rigged and riding a tradewind, they had peak average speeds over . The Great Tea Race of 1866 showcased their speed. China clippers are also the fastest commercial sailing vessels ever made. Their speeds have been exceeded many times by modern yachts, but never by a commercial sail vessel. Only the fastest windjammers could attain similar speeds.
The 24h record of the "Champion of the Seas" wasn't broken until 1984 (by a multihull), or 2001 (by another monohull).

Decline in the use of clippers started with the economic slump following the Panic of 1857 and continued with the gradual introduction of the steamship. Although clippers could be much faster than early steamships, they depended on the vagaries of the wind, while steamers could keep to a schedule. The "steam clipper" was developed around this time, and had auxiliary steam engines which could be used in the absence of wind. An example was "Royal Charter", built in 1857 and wrecked on the coast of Anglesey in 1859. The final blow was the Suez Canal, opened in 1869, which provided a great shortcut for steamships between Europe and Asia, but was difficult for sailing ships to use. With the absence of the tea trade, some clippers began operating in the wool trade, between Britain and Australia.

Although many clipper ships were built in the mid-19th century, "Cutty Sark" was, perhaps until recently, the only intact survivor. Other surviving examples of clippers of the era are less well preserved, for example the oldest surviving clipper "City of Adelaide"" (a.k.a. S.V. "Carrick").

"Falls of Clyde" is a well-preserved example of a more conservatively designed, slower contemporary of the clippers, which was built for general freight in 1878.

Departures of clipper ships, mostly from New York and Boston to San Francisco, were advertised by clipper ship sailing cards. These cards, slightly larger than today’s postcards, were produced by letterpress and wood engraving on coated card stock. Most clipper cards were printed in the 1850s and 1860s, and represented the first pronounced use of color in American advertising art.

Relatively few (perhaps 3,500) cards survive today. With their stunning appearance, rarity, and importance as artifacts of nautical, Western, and printing history, clipper cards are highly prized by both private collectors and institutions.









</doc>
<doc id="7462" url="https://en.wikipedia.org/wiki?curid=7462" title="Clive Anderson">
Clive Anderson

Clive Stuart Anderson (born 10 December 1952 in Stanmore, Middlesex) is an English television and radio presenter, comedy writer and former barrister. Winner of a British Comedy Award in 1991, Anderson began experimenting with comedy and writing comedic scripts during his 15-year legal career, before starring in "Whose Line Is It Anyway?" on BBC Radio 4, then later Channel 4. He has also hosted a number of radio programmes, and made guest appearances on "Have I Got News for You", "Mock the Week" and "QI".

Anderson was educated at Stanburn Primary School and Harrow County School for Boys where his group of friends included Geoffrey Perkins and Michael Portillo. His Scottish father was manager of the Bradford & Bingley's Wembley branch. Anderson attended Selwyn College, University of Cambridge, where, from 1974 to 1975, he was President of Footlights. He was called to the bar at the Middle Temple in 1976 and became a practising barrister, specialising in criminal law. While still practising law, he continued performing, including taking a show to the Edinburgh Fringe in 1981 with Griff Rhys Jones.

Anderson was involved in the fledgling alternative comedy scene in the early 1980s and was the first act to come on stage at The Comedy Store when it opened in 1979. He made his name as host of the improvised television comedy show "Whose Line Is It Anyway?", which ran for 10 series.

Anderson hosted his own chat-show, "Clive Anderson Talks Back", on Channel 4, which ran for 10 series. Anderson moved to the BBC in 1996. The show's name was changed to "Clive Anderson All Talk" and it was aired on BBC1. In one incident in 1996, Anderson interviewed the Bee Gees, and throughout the interview he repeatedly joked about their life and career, ultimately prompting the band to walk out. Anderson once had a glass of water poured over his head by a perturbed Richard Branson. He also said to Jeffrey Archer, "There's no beginning to your talents." Archer retorted that "The old jokes are always the best," for Anderson to reply "Yes, I've read your books." The last series of "Clive Anderson All Talk" aired in 2001.

He has made ten appearances on "Have I Got News for You". He has also frequently appeared on "QI". In 2007, he featured as a regular panellist on the ITV comedy show "News Knight". One heated exchange on "Have I Got News for You" occurred when he joked to fellow guest Piers Morgan that the "Daily Mirror" was now, thanks to Morgan (then its editor), almost as good as "The Sun". When asked by Morgan, "What do you know about editing newspapers?", he swiftly replied, "About as much as you do."

In 2005 he presented the short-lived quiz "Back in the Day" for Channel 4. On 25 February 2008, he started presenting "Brainbox Challenge", a new game show, for BBC Two. Later that year, he presented a reality TV talent show-themed television series produced by the BBC entitled "Maestro", starring eight celebrities. In 2009, Anderson was the television host of the BBC's "Last Night of the Proms".

Anderson presents legal show "Unreliable Evidence" on Radio 4. He also covered the Sunday morning 11 AM-1 PM show on BBC Radio 2 through the end of January 2008.

It was announced in April 2008 that Anderson, who had previously filled in for host Ned Sherrin from 2006 until Sherrin's death in 2007, would be taking over as permanent host of "Loose Ends". He also hosted six series of "Clive Anderson's Chat Room" on BBC Radio 2 from 2004–2009. Anderson has appeared on BBC Radio 4's "The Unbelievable Truth" hosted by David Mitchell.

Anderson also presents "The Guessing Game (radio)" on BBC Radio Scotland. Anderson has also appeared on BBC Radio 5 Live's "Fighting Talk".

Anderson is a comedy sketch writer who has written for Frankie Howerd, "Not the Nine O'Clock News", and Griff Rhys Jones and Mel Smith. One of his early comedy writing projects was "Black Cinderella Two Goes East" with Rory McGrath for BBC Radio 4 in 1978. As well as writing comedy, Anderson is also a frequent contributor to newspapers, and was a regular columnist in the "Sunday Correspondent".

Anderson lives in Highbury, north London, with his wife, Jane, and three children. He supports Arsenal, Rangers and Albion Rovers and is President of the Woodland Trust and Vice Patron of the Solicitors' Benevolent Association.

He also has a holiday home in Dalmally, Argyll.

The show "Whose Line is it Anyway?" won a BAFTA award in 1990. Later, Clive Anderson won both the "Top Entertainment Presenter" and "Top Radio Comedy Personality" at the British Comedy Awards in 1991.



</doc>
<doc id="7463" url="https://en.wikipedia.org/wiki?curid=7463" title="Cold fusion">
Cold fusion

Cold fusion is a hypothesized type of nuclear reaction that would occur at, or near, room temperature. This is compared with the "hot" fusion which takes place naturally within stars, under immense pressure and at temperatures of millions of degrees, and distinguished from muon-catalyzed fusion. There is currently no accepted theoretical model that would allow cold fusion to occur.

In 1989 Martin Fleischmann (then one of the world's leading electrochemists) and Stanley Pons reported that their apparatus had produced anomalous heat ("excess heat") of a magnitude they asserted would defy explanation except in terms of nuclear processes. They further reported measuring small amounts of nuclear reaction byproducts, including neutrons and tritium. The small tabletop experiment involved electrolysis of heavy water on the surface of a palladium (Pd) electrode. The reported results received wide media attention, and raised hopes of a cheap and abundant source of energy.

Many scientists tried to replicate the experiment with the few details available. Hopes faded due to the large number of negative replications, the withdrawal of many reported positive replications, the discovery of flaws and sources of experimental error in the original experiment, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. In 1989 the United States Department of Energy (DOE) concluded that the reported results of excess heat did not present convincing evidence of a useful source of energy and decided against allocating funding specifically for cold fusion. A second DOE review in 2004, which looked at new research, reached similar conclusions and did not result in DOE funding of cold fusion.

A small community of researchers continues to investigate cold fusion, now often preferring the designation low-energy nuclear reactions (LENR) or condensed matter nuclear science (CMNS). Since cold fusion articles are rarely published in peer-reviewed mainstream scientific journals, they do not attract the level of scrutiny expected for mainstream scientific publications.
Nuclear fusion is normally understood to occur at temperatures in the tens of millions of degrees. Since the 1920s, there has been speculation that nuclear fusion might be possible at much lower temperatures by catalytically fusing hydrogen absorbed in a metal catalyst. In 1989, a claim by Stanley Pons and Martin Fleischmann (then one of the world's leading electrochemists) that such cold fusion had been observed caused a brief media sensation before the majority of scientists criticized their claim as incorrect after many found they could not replicate the excess heat. Since the initial announcement, cold fusion research has continued by a small community of researchers who believe that such reactions happen and hope to gain wider recognition for their experimental evidence.

The ability of palladium to absorb hydrogen was recognized as early as the nineteenth century by Thomas Graham. In the late 1920s, two Austrian born scientists, Friedrich Paneth and Kurt Peters, originally reported the transformation of hydrogen into helium by nuclear catalysis when hydrogen was absorbed by finely divided palladium at room temperature. However, the authors later retracted that report, saying that the helium they measured was due to background from the air.

In 1927 Swedish scientist John Tandberg reported that he had fused hydrogen into helium in an electrolytic cell with palladium electrodes. On the basis of his work, he applied for a Swedish patent for "a method to produce helium and useful reaction energy". Due to Paneth and Peters's retraction and his inability to explain the physical process, his patent application was denied. After deuterium was discovered in 1932, Tandberg continued his experiments with heavy water. The final experiments made by Tandberg with heavy water were similar to the original experiment by Fleischmann and Pons. Fleischmann and Pons were not aware of Tandberg's work.

The term "cold fusion" was used as early as 1956 in a "New York Times" article about Luis Alvarez's work on muon-catalyzed fusion. Paul Palmer and then Steven Jones of Brigham Young University used the term "cold fusion" in 1986 in an investigation of "geo-fusion", the possible existence of fusion involving hydrogen isotopes in a planetary core. In his original paper on this subject with Clinton Van Siclen, submitted in 1985, Jones had coined the term "piezonuclear fusion".

The most famous cold fusion claims were made by Stanley Pons and Martin Fleischmann in 1989. After a brief period of interest by the wider scientific community, their reports were called into question by nuclear physicists. Pons and Fleischmann never retracted their claims, but moved their research program to France after the controversy erupted.

Martin Fleischmann of the University of Southampton and Stanley Pons of the University of Utah hypothesized that the high compression ratio and mobility of deuterium that could be achieved within palladium metal using electrolysis might result in nuclear fusion. To investigate, they conducted electrolysis experiments using a palladium cathode and heavy water within a calorimeter, an insulated vessel designed to measure process heat. Current was applied continuously for many weeks, with the heavy water being renewed at intervals. Some deuterium was thought to be accumulating within the cathode, but most was allowed to bubble out of the cell, joining oxygen produced at the anode. For most of the time, the power input to the cell was equal to the calculated power leaving the cell within measurement accuracy, and the cell temperature was stable at around 30 °C. But then, at some point (in some of the experiments), the temperature rose suddenly to about 50 °C without changes in the input power. These high temperature phases would last for two days or more and would repeat several times in any given experiment once they had occurred. The calculated power leaving the cell was significantly higher than the input power during these high temperature phases. Eventually the high temperature phases would no longer occur within a particular cell.

In 1988 Fleischmann and Pons applied to the United States Department of Energy for funding towards a larger series of experiments. Up to this point they had been funding their experiments using a small device built with $100,000 out-of-pocket. The grant proposal was turned over for peer review, and one of the reviewers was Steven Jones of Brigham Young University. Jones had worked for some time on muon-catalyzed fusion, a known method of inducing nuclear fusion without high temperatures, and had written an article on the topic entitled "Cold nuclear fusion" that had been published in "Scientific American" in July 1987. Fleischmann and Pons and co-workers met with Jones and co-workers on occasion in Utah to share research and techniques. During this time, Fleischmann and Pons described their experiments as generating considerable "excess energy", in the sense that it could not be explained by chemical reactions alone. They felt that such a discovery could bear significant commercial value and would be entitled to patent protection. Jones, however, was measuring neutron flux, which was not of commercial interest. To avoid future problems, the teams appeared to agree to simultaneously publish their results, though their accounts of their 6 March meeting differ.

In mid-March 1989, both research teams were ready to publish their findings, and Fleischmann and Jones had agreed to meet at an airport on 24 March to send their papers to "Nature" via FedEx. Fleischmann and Pons, however, pressured by the University of Utah, which wanted to establish priority on the discovery, broke their apparent agreement, submitting their paper to the "Journal of Electroanalytical Chemistry" on 11 March, and disclosing their work via a press release and press conference on 23 March. Jones, upset, faxed in his paper to "Nature" after the press conference.

Fleischmann and Pons' announcement drew wide media attention. But the 1986 discovery of high-temperature superconductivity had made the scientific community more open to revelations of unexpected scientific results that could have huge economic repercussions and that could be replicated reliably even if they had not been predicted by established theories. Many scientists were also reminded of the Mössbauer effect, a process involving nuclear transitions in a solid. Its discovery 30 years earlier had also been unexpected, though it was quickly replicated and explained within the existing physics framework.

The announcement of a new purported clean source of energy came at a crucial time: adults still remembered the 1973 oil crisis and the problems caused by oil dependence, anthropogenic global warming was starting to become notorious, the anti-nuclear movement was labeling nuclear power plants as dangerous and getting them closed, people had in mind the consequences of strip mining, acid rain, the greenhouse effect and the Exxon Valdez oil spill, which happened the day after the announcement. In the press conference, Chase N. Peterson, Fleischmann and Pons, backed by the solidity of their scientific credentials, repeatedly assured the journalists that cold fusion would solve environmental problems, and would provide a limitless inexhaustible source of clean energy, using only seawater as fuel. They said the results had been confirmed dozens of times and they had no doubts about them. In the accompanying press release Fleischmann was quoted saying: "What we have done is to open the door of a new research area, our indications are that the discovery will be relatively easy to make into a usable technology for generating heat and power, but continued work is needed, first, to further understand the science and secondly, to determine its value to energy economics."

Although the experimental protocol had not been published, physicists in several countries attempted, and failed, to replicate the excess heat phenomenon. The first paper submitted to "Nature" reproducing excess heat, although it passed peer-review, was rejected because most similar experiments were negative and there were no theories that could explain a positive result; this paper was later accepted for publication by the journal "Fusion Technology". Nathan Lewis, professor of chemistry at the California Institute of Technology, led one of the most ambitious validation efforts, trying many variations on the experiment without success, while CERN physicist Douglas R. O. Morrison said that "essentially all" attempts in Western Europe had failed. Even those reporting success had difficulty reproducing Fleischmann and Pons' results. On 10 April 1989, a group at Texas A&M University published results of excess heat and later that day a group at the Georgia Institute of Technology announced neutron production—the strongest replication announced up to that point due to the detection of neutrons and the reputation of the lab. On 12 April Pons was acclaimed at an ACS meeting. But Georgia Tech retracted their announcement on 13 April, explaining that their neutron detectors gave false positives when exposed to heat. Another attempt at independent replication, headed by Robert Huggins at Stanford University, which also reported early success with a light water control, became the only scientific support for cold fusion in 26 April US Congress hearings. But when he finally presented his results he reported an excess heat of only one degree celsius, a result that could be explained by chemical differences between heavy and light water in the presence of lithium. He had not tried to measure any radiation and his research was derided by scientists who saw it later. For the next six weeks, competing claims, counterclaims, and suggested explanations kept what was referred to as "cold fusion" or "fusion confusion" in the news.

In April 1989, Fleischmann and Pons published a "preliminary note" in the "Journal of Electroanalytical Chemistry". This paper notably showed a gamma peak without its corresponding Compton edge, which indicated they had made a mistake in claiming evidence of fusion byproducts. Fleischmann and Pons replied to this critique, but the only thing left clear was that no gamma ray had been registered and that Fleischmann refused to recognize any mistakes in the data. A much longer paper published a year later went into details of calorimetry but did not include any nuclear measurements.

Nevertheless, Fleischmann and Pons and a number of other researchers who found positive results remained convinced of their findings. The University of Utah asked Congress to provide $25 million to pursue the research, and Pons was scheduled to meet with representatives of President Bush in early May.

On 30 April 1989 cold fusion was declared dead by the "New York Times". The "Times" called it a circus the same day, and the "Boston Herald" attacked cold fusion the following day.

On 1 May 1989 the American Physical Society held a session on cold fusion in Baltimore, including many reports of experiments that failed to produce evidence of cold fusion. At the end of the session, eight of the nine leading speakers stated that they considered the initial Fleischmann and Pons claim dead, with the ninth, Johann Rafelski, abstaining. Steven E. Koonin of Caltech called the Utah report a result of ""the incompetence and delusion of Pons and Fleischmann,"" which was met with a standing ovation. Douglas R. O. Morrison, a physicist representing CERN, was the first to call the episode an example of pathological science.

On 4 May, due to all this new criticism, the meetings with various representatives from Washington were cancelled.

From 8 May only the A&M tritium results kept cold fusion afloat.

In July and November 1989, "Nature" published papers critical of cold fusion claims. Negative results were also published in several other scientific journals including "Science", "Physical Review Letters", and "Physical Review C" (nuclear physics).

In August 1989, in spite of this trend, the state of Utah invested $4.5 million to create the National Cold Fusion Institute.

The United States Department of Energy organized a special panel to review cold fusion theory and research. The panel issued its report in November 1989, concluding that results as of that date did not present convincing evidence that useful sources of energy would result from the phenomena attributed to cold fusion. The panel noted the large number of failures to replicate excess heat and the greater inconsistency of reports of nuclear reaction byproducts expected by established conjecture. Nuclear fusion of the type postulated would be inconsistent with current understanding and, if verified, would require established conjecture, perhaps even theory itself, to be extended in an unexpected way. The panel was against special funding for cold fusion research, but supported modest funding of "focused experiments within the general funding system." Cold fusion supporters continued to argue that the evidence for excess heat was strong, and in September 1990 the National Cold Fusion Institute listed 92 groups of researchers from 10 different countries that had reported corroborating evidence of excess heat, but they refused to provide any evidence of their own arguing that it could endanger their patents. However, no further DOE nor NSF funding resulted from the panel's recommendation. By this point, however, academic consensus had moved decidedly toward labeling cold fusion as a kind of "pathological science".

In March 1990 Michael H. Salamon, a physicist from the University of Utah, and nine co-authors reported negative results. University faculty were then "stunned" when a lawyer representing Pons and Fleischmann demanded the Salamon paper be retracted under threat of a lawsuit. The lawyer later apologized; Fleischmann defended the threat as a legitimate reaction to alleged bias displayed by cold-fusion critics.

In early May 1990 one of the two A&M researchers, Kevin Wolf, acknowledged the possibility of spiking, but said that the most likely explanation was tritium contamination in the palladium electrodes or simply contamination due to sloppy work. In June 1990 an article in "Science" by science writer Gary Taubes destroyed the public credibility of the A&M tritium results when it accused its group leader John Bockris and one of his graduate students of spiking the cells with tritium. In October 1990 Wolf finally said that the results were explained by tritium contamination in the rods. An A&M cold fusion review panel found that the tritium evidence was not convincing and that, while they couldn't rule out spiking, contamination and measurements problems were more likely explanations, and Bockris never got support from his faculty to resume his research.

On 30 June 1991 the National Cold Fusion Institute closed after it ran out of funds; it found no excess heat, and its reports of tritium production were met with indifference.

On 1 January 1991 Pons left the University of Utah and went to Europe. In 1992, Pons and Fleischman resumed research with Toyota Motor Corporation's IMRA lab in France. Fleischmann left for England in 1995, and the contract with Pons was not renewed in 1998 after spending $40 million with no tangible results. The IMRA laboratory stopped cold fusion research in 1998 after spending £12 million. Pons has made no public declarations since, and only Fleischmann continued giving talks and publishing papers.

Mostly in the 1990s, several books were published that were critical of cold fusion research methods and the conduct of cold fusion researchers. Over the years, several books have appeared that defended them. Around 1998, the University of Utah had already dropped its research after spending over $1 million, and in the summer of 1997, Japan cut off research and closed its own lab after spending $20 million.

A 1991 review by a cold fusion proponent had calculated "about 600 scientists" were still conducting research. After 1991, cold fusion research only continued in relative obscurity, conducted by groups that had increasing difficulty securing public funding and keeping programs open. These small but committed groups of cold fusion researchers have continued to conduct experiments using Fleischmann and Pons electrolysis set-ups in spite of the rejection by the mainstream community. "The Boston Globe" estimated in 2004 that there were only 100 to 200 researchers working in the field, most suffering damage to their reputation and career. Since the main controversy over Pons and Fleischmann had ended, cold fusion research has been funded by private and small governmental scientific investment funds in the United States, Italy, Japan, and India.

Cold fusion research continues today in a few specific venues, but the wider scientific community has generally marginalized the research being done and researchers have had difficulty publishing in mainstream journals. The remaining researchers often term their field Low Energy Nuclear Reactions (LENR), Chemically Assisted Nuclear Reactions (CANR), Lattice Assisted Nuclear Reactions (LANR), Condensed Matter Nuclear Science (CMNS) or Lattice Enabled Nuclear Reactions; one of the reasons being to avoid the negative connotations associated with "cold fusion". The new names avoid making bold implications, like implying that fusion is actually occurring.

The researchers who continue acknowledge that the flaws in the original announcement are the main cause of the subject's marginalization, and they complain of a chronic lack of funding and no possibilities of getting their work published in the highest impact journals. University researchers are often unwilling to investigate cold fusion because they would be ridiculed by their colleagues and their professional careers would be at risk. In 1994, David Goodstein, a professor of physics at Caltech, advocated for increased attention from mainstream researchers and described cold fusion as:

United States Navy researchers at the Space and Naval Warfare Systems Center (SPAWAR) in San Diego have been studying cold fusion since 1989. In 2002 they released a two-volume report, "Thermal and nuclear aspects of the Pd/DO system," with a plea for funding. This and other published papers prompted a 2004 Department of Energy (DOE) review.
In August 2003, the U.S. Secretary of Energy, Spencer Abraham, ordered the DOE to organize a second review of the field. This was thanks to an April 2003 letter sent by MIT's Peter L. Hagelstein, and the publication of many new papers, including the Italian ENEA and other researchers in the 2003 International Cold Fusion Conference, and a two-volume book by U.S. SPAWAR in 2002. Cold fusion researchers were asked to present a review document of all the evidence since the 1989 review. The report was released in 2004. The reviewers were "split approximately evenly" on whether the experiments had produced energy in the form of heat, but "most reviewers, even those who accepted the evidence for excess power production, 'stated that the effects are not repeatable, the magnitude of the effect has not increased in over a decade of work, and that many of the reported experiments were not well documented.'" In summary, reviewers found that cold fusion evidence was still not convincing 15 years later, and they didn't recommend a federal research program. They only recommended that agencies consider funding individual well-thought studies in specific areas where research "could be helpful in resolving some of the controversies in the field". They summarized its conclusions thus:

Cold fusion researchers placed a "rosier spin" on the report, noting that they were finally being treated like normal scientists, and that the report had increased interest in the field and caused "a huge upswing in interest in funding cold fusion research." However, in a 2009 BBC article on an American Chemical Society's meeting on cold fusion, particle physicist Frank Close was quoted stating that the problems that plagued the original cold fusion announcement were still happening: results from studies are still not being independently verified and inexplicable phenomena encountered are being labelled as "cold fusion" even if they are not, in order to attract the attention of journalists.

In February 2012, millionaire Sidney Kimmel, convinced that cold fusion was worth investing in by a 19 April 2009 interview with physicist Robert Duncan on the US news-show "60 Minutes", made a grant of $5.5 million to the University of Missouri to establish the Sidney Kimmel Institute for Nuclear Renaissance (SKINR). The grant was intended to support research into the interactions of hydrogen with palladium, nickel or platinum under extreme conditions. In March 2013 Graham K. Hubler, a nuclear physicist who worked for the Naval Research Laboratory for 40 years, was named director. One of the SKINR projects is to replicate a 1991 experiment in which a professor associated with the project, Mark Prelas says bursts of millions of neutrons a second were recorded, which was stopped because "his research account had been frozen". He claims that the new experiment has already seen "neutron emissions at similar levels to the 1991 observation".

In May 2016, the United States House Committee on Armed Services, in its report on the 2017 National Defense Authorization Act, directed the Secretary of Defense to "provide a briefing on the military utility of recent U.S. industrial base LENR advancements to the House Committee on Armed Services by September 22, 2016."

Since the Fleischmann and Pons announcement, the Italian National agency for new technologies, energy and sustainable economic development (ENEA) has funded Franco Scaramuzzi's research into whether excess heat can be measured from metals loaded with deuterium gas. Such research is distributed across ENEA departments, CNR laboratories, INFN, universities and industrial laboratories in Italy, where the group continues to try to achieve reliable reproducibility (i.e. getting the phenomenon to happen in every cell, and inside a certain frame of time). In 2006–2007, the ENEA started a research program which claimed to have found excess power of up to 500 percent, and in 2009, ENEA hosted the 15th cold fusion conference.

Between 1992 and 1997, Japan's Ministry of International Trade and Industry sponsored a "New Hydrogen Energy (NHE)" program of US$20 million to research cold fusion. Announcing the end of the program in 1997, the director and one-time proponent of cold fusion research Hideo Ikegami stated "We couldn't achieve what was first claimed in terms of cold fusion. (...) We can't find any reason to propose more money for the coming year or for the future." In 1999 the Japan C-F Research Society was established to promote the independent research into cold fusion that continued in Japan. The society holds annual meetings. Perhaps the most famous Japanese cold fusion researcher is Yoshiaki Arata, from Osaka University, who claimed in a demonstration to produce excess heat when deuterium gas was introduced into a cell containing a mixture of palladium and zirconium oxide, a claim supported by fellow Japanese researcher Akira Kitamura of Kobe University and McKubre at SRI.

In the 1990s India stopped its research in cold fusion at the Bhabha Atomic Research Centre because of the lack of consensus among mainstream scientists and the US denunciation of the research. Yet, in 2008, the National Institute of Advanced Studies recommended that the Indian government revive this research. Projects were commenced at the Chennai's Indian Institute of Technology, the Bhabha Atomic Research Centre and the Indira Gandhi Centre for Atomic Research. However, there is still skepticism among scientists and, for all practical purposes, research has stalled since the 1990s. A special section in the Indian multidisciplinary journal "Current Science" published 33 cold fusion papers in 2015 by major cold fusion researchers including several Indian researchers.

A cold fusion experiment usually includes:

Electrolysis cells can be either open cell or closed cell. In open cell systems, the electrolysis products, which are gaseous, are allowed to leave the cell. In closed cell experiments, the products are captured, for example by catalytically recombining the products in a separate part of the experimental system. These experiments generally strive for a steady state condition, with the electrolyte being replaced periodically. There are also "heat-after-death" experiments, where the evolution of heat is monitored after the electric current is turned off.

The most basic setup of a cold fusion cell consists of two electrodes submerged in a solution containing palladium and heavy water. The electrodes are then connected to a power source to transmit electricity from one electrode to the other through the solution. Even when anomalous heat is reported, it can take weeks for it to begin to appear—this is known as the "loading time," the time required to saturate the palladium electrode with hydrogen (see "Loading ratio" section).

The Fleischmann and Pons early findings regarding helium, neutron radiation and tritium were never replicated satisfactorily, and its levels were too low for the claimed heat production and inconsistent with each other. Neutron radiation has been reported in cold fusion experiments at very low levels using different kinds of detectors, but levels were too low, close to background, and found too infrequently to provide useful information about possible nuclear processes.

An excess heat observation is based on an energy balance. Various sources of energy input and output are continuously measured. Under normal conditions, the energy input can be matched to the energy output to within experimental error. In experiments such as those run by Fleischmann and Pons, an electrolysis cell operating steadily at one temperature transitions to operating at a higher temperature with no increase in applied current. If the higher temperatures were real, and not an experimental artifact, the energy balance would show an unaccounted term. In the Fleischmann and Pons experiments, the rate of inferred excess heat generation was in the range of 10–20% of total input, though this could not be reliably replicated by most researchers. Researcher Nathan Lewis discovered that the excess heat in Fleischmann and Pons's original paper was not measured, but estimated from measurements that didn't have any excess heat.

Unable to produce excess heat or neutrons, and with positive experiments being plagued by errors and giving disparate results, most researchers declared that heat production was not a real effect and ceased working on the experiments. In 1993, after their original report, Fleischmann reported "heat-after-death" experiments—where excess heat was measured after the electric current supplied to the electrolytic cell was turned off. This type of report has also become part of subsequent cold fusion claims.

Known instances of nuclear reactions, aside from producing energy, also produce nucleons and particles on readily observable ballistic trajectories. In support of their claim that nuclear reactions took place in their electrolytic cells, Fleischmann and Pons reported a neutron flux of 4,000 neutrons per second, as well as detection of tritium. The classical branching ratio for previously known fusion reactions that produce tritium would predict, with 1 watt of power, the production of 10 neutrons per second, levels that would have been fatal to the researchers. In 2009, Mosier-Boss et al. reported what they called the first scientific report of highly energetic neutrons, using CR-39 plastic radiation detectors, but the claims cannot be validated without a quantitative analysis of neutrons.

Several medium and heavy elements like calcium, titanium, chromium, manganese, iron, cobalt, copper and zinc have been reported as detected by several researchers, like Tadahiko Mizuno or George Miley. The report presented to the United States Department of Energy (DOE) in 2004 indicated that deuterium-loaded foils could be used to detect fusion reaction products and, although the reviewers found the evidence presented to them as inconclusive, they indicated that those experiments did not use state-of-the-art techniques.

In response to doubts about the lack of nuclear products, cold fusion researchers have tried to capture and measure nuclear products correlated with excess heat. Considerable attention has been given to measuring He production. However, the reported levels are very near to background, so contamination by trace amounts of helium normally present in the air cannot be ruled out. In the report presented to the DOE in 2004, the reviewers' opinion was divided on the evidence for He; with the most negative reviews concluding that although the amounts detected were above background levels, they were very close to them and therefore could be caused by contamination from air.

One of the main criticisms of cold fusion was that deuteron-deuteron fusion into helium was expected to result in the production of gamma rays—which were not observed and were not observed in subsequent cold fusion experiments. Cold fusion researchers have since claimed to find X-rays, helium, neutrons and nuclear transmutations. Some researchers also claim to have found them using only light water and nickel cathodes. The 2004 DOE panel expressed concerns about the poor quality of the theoretical framework cold fusion proponents presented to account for the lack of gamma rays.

Researchers in the field do not agree on a theory for cold fusion. One proposal considers that hydrogen and its isotopes can be absorbed in certain solids, including palladium hydride, at high densities. This creates a high partial pressure, reducing the average separation of hydrogen isotopes. However, the reduction in separation is not enough by a factor of ten to create the fusion rates claimed in the original experiment. It was also proposed that a higher density of hydrogen inside the palladium and a lower potential barrier could raise the possibility of fusion at lower temperatures than expected from a simple application of Coulomb's law. Electron screening of the positive hydrogen nuclei by the negative electrons in the palladium lattice was suggested to the 2004 DOE commission, but the panel found the theoretical explanations not convincing and inconsistent with current physics theories.

Criticism of cold fusion claims generally take one of two forms: either pointing out the theoretical implausibility that fusion reactions have occurred in electrolysis set-ups or criticizing the excess heat measurements as being spurious, erroneous, or due to poor methodology or controls. There are a couple of reasons why known fusion reactions are an unlikely explanation for the excess heat and associated cold fusion claims.

Because nuclei are all positively charged, they strongly repel one another. Normally, in the absence of a catalyst such as a muon, very high kinetic energies are required to overcome this charged repulsion. Extrapolating from known fusion rates, the rate for uncatalyzed fusion at room-temperature energy would be 50 orders of magnitude lower than needed to account for the reported excess heat. In muon-catalyzed fusion there are more fusions because the presence of the muon causes deuterium nuclei to be 207 times closer than in ordinary deuterium gas. But deuterium nuclei inside a palladium lattice are further apart than in deuterium gas, and there should be fewer fusion reactions, not more.

Paneth and Peters in the 1920s already knew that palladium can absorb up to 900 times its own volume of hydrogen gas, storing it at several thousands of times the atmospheric pressure. This led them to believe that they could increase the nuclear fusion rate by simply loading palladium rods with hydrogen gas. Tandberg then tried the same experiment but used electrolysis to make palladium absorb more deuterium and force the deuterium further together inside the rods, thus anticipating the main elements of Fleischmann and Pons' experiment. They all hoped that pairs of hydrogen nuclei would fuse together to form helium, which at the time was needed in Germany to fill zeppelins, but no evidence of helium or of increased fusion rate was ever found.

This was also the belief of geologist Palmer, who convinced Steven Jones that the helium-3 occurring naturally in Earth perhaps came from fusion involving hydrogen isotopes inside catalysts like nickel and palladium. This led their team in 1986 to independently make the same experimental setup as Fleischmann and Pons (a palladium cathode submerged in heavy water, absorbing deuterium via electrolysis). Fleischmann and Pons had much the same belief, but they calculated the pressure to be of 10 atmospheres, when cold fusion experiments only achieve a loading ratio of one to one, which only has between 10,000 and 20,000 atmospheres. John R. Huizenga says they had misinterpreted the Nernst equation, leading them to believe that there was enough pressure to bring deuterons so close to each other that there would be spontaneous fusions.

Conventional deuteron fusion is a two-step process, in which an unstable high energy intermediary is formed:
Experiments have observed only three decay pathways for this excited-state nucleus, with the branching ratio showing the probability that any given intermediate follows a particular pathway. The products formed via these decay pathways are:
Only about one in one million of the intermediaries decay along the third pathway, making its products comparatively rare when compared to the other paths. This result is consistent with the predictions of the Bohr model. If one watt (1 eV = 1.602 x 10 joule) of nuclear power were produced from deuteron fusion consistent with known branching ratios, the resulting neutron and tritium (H) production would be easily measured. Some researchers reported detecting He but without the expected neutron or tritium production; such a result would require branching ratios strongly favouring the third pathway, with the actual rates of the first two pathways lower by at least five orders of magnitude than observations from other experiments, directly contradicting both theoretically predicted and observed branching probabilities. Those reports of He production did not include detection of gamma rays, which would require the third pathway to have been changed somehow so that gamma rays are no longer emitted.

The known rate of the decay process together with the inter-atomic spacing in a metallic crystal makes heat transfer of the 24 MeV excess energy into the host metal lattice prior to the intermediary's decay inexplicable in terms of conventional understandings of momentum and energy transfer, and even then there would be measurable levels of radiation. Also, experiments indicate that the ratios of deuterium fusion remain constant at different energies. In general, pressure and chemical environment only cause small changes to fusion ratios. An early explanation invoked the Oppenheimer–Phillips process at low energies, but its magnitude was too small to explain the altered ratios.

Cold fusion setups utilize an input power source (to ostensibly provide activation energy), a platinum group electrode, a deuterium or hydrogen source, a calorimeter, and, at times, detectors to look for byproducts such as helium or neutrons. Critics have variously taken issue with each of these aspects and have asserted that there has not yet been a consistent reproduction of claimed cold fusion results in either energy output or byproducts. Some cold fusion researchers who claim that they can consistently measure an excess heat effect have argued that the apparent lack of reproducibility might be attributable to a lack of quality control in the electrode metal or the amount of hydrogen or deuterium loaded in the system. Critics have further taken issue with what they describe as mistakes or errors of interpretation that cold fusion researchers have made in calorimetry analyses and energy budgets.

In 1989, after Fleischmann and Pons had made their claims, many research groups tried to reproduce the Fleischmann-Pons experiment, without success. A few other research groups, however, reported successful reproductions of cold fusion during this time. In July 1989, an Indian group from the Bhabha Atomic Research Centre (P. K. Iyengar and M. Srinivasan) and in October 1989, John Bockris' group from Texas A&M University reported on the creation of tritium. In December 1990, professor Richard Oriani of the University of Minnesota reported excess heat.

Groups that did report successes found that some of their cells were producing the effect, while other cells that were built exactly the same and used the same materials were not producing the effect. Researchers that continued to work on the topic have claimed that over the years many successful replications have been made, but still have problems getting reliable replications. Reproducibility is one of the main principles of the scientific method, and its lack led most physicists to believe that the few positive reports could be attributed to experimental error. The DOE 2004 report said among its conclusions and recommendations:

Cold fusion researchers (McKubre since 1994, ENEA in 2011) have speculated that a cell that is loaded with a deuterium/palladium ratio lower than 100% (or 1:1) will not produce excess heat. Since most of the negative replications from 1989–1990 did not report their ratios, this has been proposed as an explanation for failed replications. This loading ratio is hard to obtain, and some batches of palladium never reach it because the pressure causes cracks in the palladium, allowing the deuterium to escape. Fleischmann and Pons never disclosed the deuterium/palladium ratio achieved in their cells, there are no longer any batches of the palladium used by Fleischmann and Pons (because the supplier uses now a different manufacturing process), and researchers still have problems finding batches of palladium that achieve heat production reliably.

Some research groups initially reported that they had replicated the Fleischmann and Pons results but later retracted their reports and offered an alternative explanation for their original positive results. A group at Georgia Tech found problems with their neutron detector, and Texas A&M discovered bad wiring in their thermometers. These retractions, combined with negative results from some famous laboratories, led most scientists to conclude, as early as 1989, that no positive result should be attributed to cold fusion.

The calculation of excess heat in electrochemical cells involves certain assumptions. Errors in these assumptions have been offered as non-nuclear explanations for excess heat.

One assumption made by Fleischmann and Pons is that the efficiency of electrolysis is nearly 100%, meaning nearly all the electricity applied to the cell resulted in electrolysis of water, with negligible resistive heating and substantially all the electrolysis product leaving the cell unchanged. This assumption gives the amount of energy expended converting liquid DO into gaseous D and O. The efficiency of electrolysis is less than one if hydrogen and oxygen recombine to a significant extent within the calorimeter. Several researchers have described potential mechanisms by which this process could occur and thereby account for excess heat in electrolysis experiments.

Another assumption is that heat loss from the calorimeter maintains the same relationship with measured temperature as found when calibrating the calorimeter. This assumption ceases to be accurate if the temperature distribution within the cell becomes significantly altered from the condition under which calibration measurements were made. This can happen, for example, if fluid circulation within the cell becomes significantly altered. Recombination of hydrogen and oxygen within the calorimeter would also alter the heat distribution and invalidate the calibration.

The ISI identified cold fusion as the scientific topic with the largest number of published papers in 1989, of all scientific disciplines. The Nobel Laureate Julian Schwinger declared himself a supporter of cold fusion in the fall of 1989, after much of the response to the initial reports had turned negative. He tried to publish his theoretical paper "Cold Fusion: A Hypothesis" in "Physical Review Letters", but the peer reviewers rejected it so harshly that he felt deeply insulted, and he resigned from the American Physical Society (publisher of "PRL") in protest.

The number of papers sharply declined after 1990 because of two simultaneous phenomena: scientists abandoning the field and journal editors declining to review new papers, and cold fusion fell off the ISI charts. Researchers who got negative results abandoned the field, while others kept publishing. A 1993 paper in "Physics Letters A" was the last paper published by Fleischmann, and "one of the last reports [by Fleischmann] to be formally challenged on technical grounds by a cold fusion skeptic".

The "Journal of Fusion Technology" (FT) established a permanent feature in 1990 for cold fusion papers, publishing over a dozen papers per year and giving a mainstream outlet for cold fusion researchers. When editor-in-chief George H. Miley retired in 2001, the journal stopped accepting new cold fusion papers. This has been cited as an example of the importance of sympathetic influential individuals to the publication of cold fusion papers in certain journals.

The decline of publications in cold fusion has been described as a "failed information epidemic". The sudden surge of supporters until roughly 50% of scientists support the theory, followed by a decline until there is only a very small number of supporters, has been described as a characteristic of pathological science. The lack of a shared set of unifying concepts and techniques has prevented the creation of a dense network of collaboration in the field; researchers perform efforts in their own and in disparate directions, making the transition to "normal" science more difficult.

Cold fusion reports continued to be published in a small cluster of specialized journals like "Journal of Electroanalytical Chemistry" and "Il Nuovo Cimento". Some papers also appeared in "Journal of Physical Chemistry", "Physics Letters A", "International Journal of Hydrogen Energy", and a number of Japanese and Russian journals of physics, chemistry, and engineering. Since 2005, "Naturwissenschaften" has published cold fusion papers; in 2009, the journal named a cold fusion researcher to its editorial board. In 2015 the Indian multidisciplinary journal "Current Science" published a special section devoted entirely to cold fusion related papers.

In the 1990s, the groups that continued to research cold fusion and their supporters established (non-peer-reviewed) periodicals such as "Fusion Facts", "Cold Fusion Magazine", "Infinite Energy Magazine" and "New Energy Times" to cover developments in cold fusion and other fringe claims in energy production that were ignored in other venues. The internet has also become a major means of communication and self-publication for CF researchers.

Cold fusion researchers were for many years unable to get papers accepted at scientific meetings, prompting the creation of their own conferences. The first International Conference on Cold Fusion (ICCF) was held in 1990, and has met every 12 to 18 months since. Attendees at some of the early conferences were described as offering no criticism to papers and presentations for fear of giving ammunition to external critics; thus allowing the proliferation of crackpots and hampering the conduct of serious science. Critics and skeptics stopped attending these conferences, with the notable exception of Douglas Morrison, who died in 2001. With the founding in 2004 of the International Society for Condensed Matter Nuclear Science (ISCMNS), the conference was renamed the International Conference on Condensed Matter Nuclear Science (the reasons are explained in the subsequent research section), but reverted to the old name in 2008. Cold fusion research is often referenced by proponents as "low-energy nuclear reactions", or LENR, but according to sociologist Bart Simon the "cold fusion" label continues to serve a social function in creating a collective identity for the field.

Since 2006, the American Physical Society (APS) has included cold fusion sessions at their semiannual meetings, clarifying that this does not imply a softening of skepticism. Since 2007, the American Chemical Society (ACS) meetings also include "invited symposium(s)" on cold fusion. An ACS program chair said that without a proper forum the matter would never be discussed and, "with the world facing an energy crisis, it is worth exploring all possibilities."

On 22–25 March 2009, the American Chemical Society meeting included a four-day symposium in conjunction with the 20th anniversary of the announcement of cold fusion. Researchers working at the U.S. Navy's Space and Naval Warfare Systems Center (SPAWAR) reported detection of energetic neutrons using a heavy water electrolysis set-up and a CR-39 detector, a result previously published in "Naturwissenschaften". The authors claim that these neutrons are indicative of nuclear reactions; without quantitative analysis of the number, energy, and timing of the neutrons and exclusion of other potential sources, this interpretation is unlikely to find acceptance by the wider scientific community.

Although details have not surfaced, it appears that the University of Utah forced the 23 March 1989 Fleischmann and Pons announcement to establish priority over the discovery and its patents before the joint publication with Jones. The Massachusetts Institute of Technology (MIT) announced on 12 April 1989 that it had applied for its own patents based on theoretical work of one of its researchers, Peter L. Hagelstein, who had been sending papers to journals from the 5 to 12 April. On 2 December 1993 the University of Utah licensed all its cold fusion patents to ENECO, a new company created to profit from cold fusion discoveries, and in March 1998 it said that it would no longer defend its patents.

The U.S. Patent and Trademark Office (USPTO) now rejects patents claiming cold fusion. Esther Kepplinger, the deputy commissioner of patents in 2004, said that this was done using the same argument as with perpetual motion machines: that they do not work. Patent applications are required to show that the invention is "useful", and this utility is dependent on the invention's ability to function. In general USPTO rejections on the sole grounds of the invention's being "inoperative" are rare, since such rejections need to demonstrate "proof of total incapacity", and cases where those rejections are upheld in a Federal Court are even rarer: nevertheless, in 2000, a rejection of a cold fusion patent was appealed in a Federal Court and it was upheld, in part on the grounds that the inventor was unable to establish the utility of the invention.

A U.S. patent might still be granted when given a different name to disassociate it from cold fusion, though this strategy has had little success in the US: the same claims that need to be patented can identify it with cold fusion, and most of these patents cannot avoid mentioning Fleischmann and Pons' research due to legal constraints, thus alerting the patent reviewer that it is a cold-fusion-related patent. David Voss said in 1999 that some patents that closely resemble cold fusion processes, and that use materials used in cold fusion, have been granted by the USPTO. The inventor of three such patents had his applications initially rejected when they were reviewed by experts in nuclear science; but then he rewrote the patents to focus more in the electrochemical parts so they would be reviewed instead by experts in electrochemistry, who approved them. When asked about the resemblance to cold fusion, the patent holder said that it used nuclear processes involving "new nuclear physics" unrelated to cold fusion. Melvin Miles was granted in 2004 a patent for a cold fusion device, and in 2007 he described his efforts to remove all instances of "cold fusion" from the patent description to avoid having it rejected outright.

At least one patent related to cold fusion has been granted by the European Patent Office.

A patent only legally prevents others from using or benefiting from one's invention. However, the general public perceives a patent as a stamp of approval, and a holder of three cold fusion patents said the patents were very valuable and had helped in getting investments.

In "Undead Science", sociologist Bart Simon gives some examples of cold fusion in popular culture, saying that some scientists use cold fusion as a synonym for outrageous claims made with no supporting proof, and courses of ethics in science give it as an example of pathological science. It has appeared as a joke in "Murphy Brown" and "The Simpsons". It was adopted as a software product name Adobe ColdFusion and a brand of protein bars (Cold Fusion Foods). It has also appeared in advertising as a synonym for impossible science, for example a 1995 advertisement for Pepsi Max.

The plot of "The Saint", a 1997 action-adventure film, parallels the story of Fleischmann and Pons, although with a different ending. The film might have affected the public perception of cold fusion, pushing it further into the science fiction realm.

"Final Exam", the 16th episode of season 4 of "The Outer Limits", depicts a student named Todtman who has invented a cold fusion weapon, and attempts to use it as a tool for revenge on people who have wronged him over the years. Despite the secret being lost with his death at the end of the episode, it is implied that another student elsewhere is on a similar track, and may well repeat Todtman's efforts.

In the "DC's Legends of Tomorrow" episode "No Country for Old Dads," Ray Palmer theorizes that cold fusion could repair the shattered Fire Totem, if it wasn't only theoretical. Damien Dahrk reveals that he assassinated a scientist in 1962 East Germany that developed a formula for cold fusion. Ray and Dahrk's daughter Nora time travel from 2018 to 1962 in an attempt to rescue the scientist from the younger version of Dahrk and/or retrieve the formula.



</doc>
<doc id="7466" url="https://en.wikipedia.org/wiki?curid=7466" title="Coal tar">
Coal tar

Coal tar is a thick dark liquid which is a by-product of the production of coke and coal gas from coal. It has both medical and industrial uses. As a medication it is used to treat psoriasis and seborrheic dermatitis (dandruff). It is used by application to the affected area and may be used together with ultraviolet light therapy. Industrial uses include preservation of railway ties and improving the surface of roads.
Side effects include skin irritation, sun sensitivity, allergic reactions, and skin discoloration. It is unclear if use during pregnancy is safe for the baby and use during breastfeeding is not typically recommended. The exact mechanism of action is unknown. It is a complex mixture of phenols, polycyclic aromatic hydrocarbons (PAHs), and heterocyclic compounds. It may have antifungal, anti-inflammatory, anti-itch, and antiparasitic properties.
Coal tar was discovered around 1665 and used for medical purposes as early as the 1800s. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. Coal tar is available as a generic medication and over the counter. In the United Kingdom 125 ml of 5% shampoo costs the NHS about £1.89. In the United States a month of treatment costs less than $25 USD. Coal-tar was one of the key starting materials for the early pharmaceutical industry.

Coal tar may be used in two forms: crude coal tar () or a coal tar solution () also known as liquor carbonis detergens (LCD).

Coal tar is used in medicated shampoo, soap and ointment, as a treatment for dandruff and psoriasis, as well as being used to kill and repel head lice. When used as a medication in the U.S., coal tar preparations are considered over-the-counter drug pharmaceuticals and are subject to regulation by the FDA. Named brands include Denorex, Balnetar, Psoriasin, Tegrin, T/Gel, and Neutar. When used in the extemporaneous preparation of topical medications, it is supplied in the form of coal tar topical solution USP, which consists of a 20% w/v solution of coal tar in alcohol, with an additional 5% w/v of polysorbate 80 USP; this must then be diluted in an ointment base such as petrolatum.

Pine tar has historically also been used for this purpose. Though it is frequently cited online as having been banned as a medical product by the FDA due to a "lack of evidence having been submitted for proof of effectiveness", pine tar is included in the Code of Federal Regulations, subchapter D: Drugs for Human Use, as an OTC treatment for "Dandruff/seborrheic dermatitis/psoriasis".

Various phenolic coal tar derivatives have analgesic (pain-killer) properties. These included acetanilide, phenacetin, and paracetamol (acetaminophen). Paracetamol is the only coal-tar derived analgesic still in use today, but industrial phenol is now usually synthesized from crude oil rather than coal tar.

Coal tar is incorporated into some parking-lot sealcoat products, which are used to protect and beautify the underlying pavement. Sealcoat products that are coal-tar based typically contain 20 to 35 percent coal-tar pitch. Research shows it is used in United States states from Alaska to Florida, but several areas have banned its use in sealcoat products,

Coal tar was a component of the first sealed roads. In its original development by Edgar Purnell Hooley, tarmac was tar covered with granite chips. Later the filler used was industrial slag. Today, petroleum derived binders and sealers are more commonly used. These sealers are used to extend the life and reduce maintenance cost associated with asphalt pavements, primarily in asphalt road paving, car parks and walkways.

Being flammable, coal tar is sometimes used for heating or to fire boilers. Like most heavy oils, it must be heated before it will flow easily.

A large part of the binders used in the graphite industry for making "green blocks" are coke oven volatiles (COV), a considerable portion of which are coal tar. During the baking process of the green blocks as a part of commercial graphite production, most of the coal tar binders are vaporised and are generally burned in an incinerator to prevent release into the atmosphere, as COV and coal tar can be injurious to health.

Coal tar is also used to manufacture paints, synthetic dyes (notably tartrazine/Yellow #5), and photographic materials.

In the coal gas era, there were many companies in Britain whose business was to distill coal tar to separate the higher-value fractions, such as naphtha, creosote and pitch. A great many industrial chemicals were first isolated from coal tar during this time. These companies included:


According to the National Psoriasis Foundation, coal tar is a valuable, safe and inexpensive treatment option for millions of people with psoriasis and other scalp or skin conditions. According to the FDA, coal tar concentrations between 0.5% and 5% are considered safe and effective for psoriasis.

Scientific evidence is inconclusive whether the coal tar in the concentrations seen in non-prescription treatments is carcinogenic, because there are too few studies and insufficient data to make a judgement. While concerns have been found in animals studies, short-term treatment of humans have shown no significant increase in cancer. It's possible that the skin can repair itself after short-term exposure to PAHs, but not after long-term exposure.

Working with coal tar such as during the paving of roads or when working on roofs increases the risk of cancer.

It is believed that their metabolites bind to DNA, damaging it. Long-term skin exposure to these compounds can produce "tar warts", which can progress to squamous cell carcinoma.

The International Agency for Research on Cancer lists coal tars as Group 1 carcinogens, meaning they directly cause cancer. Both the U.S. Department of Health and Human Services and the state of California list coal tars as known human carcinogens.

Coal tar causes increased sensitivity to sunlight, so skin treated with topical coal tar preparations should be protected from sunlight.

The residue from the distillation of high-temperature coal tar, primarily a complex mixture of three or more membered condensed ring aromatic hydrocarbons, was listed on 28 October 2008 as a substance of very high concern by the European Chemicals Agency.

It is a keratolytic agent, which reduces the growth rate of skin cells and softens the skin's keratin.

Coal tar is produced through thermal destruction (pyrolysis) of coal, and the composition of coal tar varies with the process and type of coal (for example,: lignite, bituminous or anthracite) used to make it.

It contains approximately 10,000 chemicals, of which only about 50% have been identified. Components include polycyclic aromatic hydrocarbons (4-rings: chrysene, fluoranthene, pyrene, triphenylene, naphthacene, benzanthracene, 5-rings: picene, benzo[a]pyrene, benzo[e]pyrene, benzofluoranthenes, perylene, 6-rings: dibenzopyrenes, dibenzofluoranthenes, benzoperylenes, 7-rings: coronene), as well as methylated and polymethylated derivatives, mono- and polyhydroxylated derivatives, and heterocyclic compounds. Others include benzene, toluene, xylenes, cumenes, coumarone, indene, benzofuran, naphthalene and methyl-naphthalenes, acenaphthene, fluorene, phenol, cresols, pyridine, picolines, phenanthracene, carbazole, quinolines, fluoranthene. Many of these constituents are known carcinogens.

It is notable as one of the first chemical substances proven to cause cancer from occupational exposure, during research in 1775 on the cause of chimney sweeps' carcinoma.

People can be exposed to coal tar pitch volatiles in the workplace by breathing them in, skin contact, or eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for coal tar pitch volatiles exposure in the workplace as 0.2 mg/m benzene-soluble fraction over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.1 mg/m cyclohexane-extractable fraction over an 8-hour workday. At levels of 80 mg/m, coal tar pitch volatiles are immediately dangerous to life and health.




</doc>
<doc id="7467" url="https://en.wikipedia.org/wiki?curid=7467" title="Cobbler">
Cobbler

Cobbler(s) may refer to:








</doc>
<doc id="7471" url="https://en.wikipedia.org/wiki?curid=7471" title="Catherine of Siena">
Catherine of Siena

Saint Catherine of Siena (25 March 1347 in Siena – 29 April 1380 in Rome), was a tertiary of the Dominican Order and a Scholastic philosopher and theologian who had a great influence on the Catholic Church. She is declared a saint and a doctor of the Church.

Born in Siena, she grew up there and wanted very soon to devote herself to God, against the will of her parents. She joined the Sisters of the Penance of St. Dominic and made her vows. She made herself known very quickly by being marked by mystical phenomena such as stigmata and mystical marriage.

She accompanied the chaplain of the Dominicans to the pope in Avignon, as ambassador of Florence, then at war against the pope. Her influence with Pope Gregory XI played a role in his decision to leave Avignon for Rome. She was then sent by him to negotiate peace with Florence. After Gregory XI's death and peace concluded, she returned to Siena. She dictated to secretaries her set of spiritual treatises The Dialogue.

The Great Schism of the West lead Catherine of Siena to go to Rome with the pope. She sent numerous letters to princes and cardinals to promote obedience to Pope Urban VI and defend what she calls the "vessel of the Church." She died on 29 April 1380, exhausted by her penances. Urban VI celebrated her funeral and burial in the Basilica of Santa Maria sopra Minerva in Rome.

The devotion around Catherine of Siena developed rapidly after her death. She was canonized in 1461, declared patron saint of Rome in 1866, and of Italy in 1939. First woman declared "doctor of the Church" on 3 October 1970 by Paul VI with Teresa of Ávila, she was proclaimed patron saint of Europe in 1999 by John Paul II. She is also the patron saint of journalists, media, and all communication professions, because of her epistolary work for the papacy.

Catherine of Siena is one of the outstanding figures of medieval Catholicism, by the strong influence she has had in the history of the papacy. She is behind the return of the Pope from Avignon to Rome, and then carried out many missions entrusted by the pope, something quite rare for a simple nun in the Middle Ages. 

Her writings—and especially The Dialogue, her major work which includes a set of treatises she would have dictated during ecstasies—mark theological thought. She is one of the most influential writers in Catholicism, to the point that she is one of only four women to be declared a doctor of the Church. This recognition by the Church consecrates the importance of her writings.

Since 18 June 1939, Catherine of Siena has been one of the two patron saints of Italy, together with Francis of Assisi. On 3 October 1970, she was proclaimed a Doctor of the Church by Pope Paul VI, and on 1 October 1999, Pope John Paul II named her as one of the six patron saints of Europe, together with Benedict of Nursia, Saints Cyril and Methodius, Bridget of Sweden and Edith Stein.

Caterina di Giacomo di Benincasa was born on 25 March 1347 in Black Death-ravaged Siena, Italy, to Lapa Piagenti, the daughter of a local poet, and Giacomo di Benincasa, a cloth dyer who ran his enterprise with the help of his sons. The house where Catherine grew up is still in existence. Lapa was about forty years old when she gave premature birth to twin daughters Catherine and Giovanna. She had already borne 22 children, but half of them had died. Giovanna was handed over to a wet-nurse and died soon after. Catherine was nursed by her mother and developed into a healthy child. She was two years old when Lapa had her 25th child, another daughter named Giovanna. As a child Catherine was so merry that the family gave her the pet name of "Euphrosyne", which is Greek for "joy" and the name of an early Christian saint.

Catherine is said by her confessor and biographer Raymond of Capua O.P.'s "Life" to have had her first vision of Christ when she was five or six years old: She and a brother were on the way home from visiting a married sister when she is said to have experienced a vision of Christ seated in glory with the Apostles Peter, Paul, and John. Raymond continues that at age seven, Catherine vowed to give her whole life to God.

When Catherine was sixteen, her older sister Bonaventura died in childbirth; already anguished by this, Catherine soon learned that her parents wanted her to marry Bonaventura's widower. She was absolutely opposed and started a strict fast. She had learned this from Bonaventura, whose husband had been far from considerate but his wife had changed his attitude by refusing to eat until he showed better manners. Besides fasting, Catherine further disappointed her mother by cutting off her long hair as a protest against being overly encouraged to improve her appearance to attract a husband.
Catherine would later advise Raymond of Capua to do during times of trouble what she did now as a teenager: "Build a cell inside your mind, from which you can never flee." In this inner cell she made her father into a representation of Christ, her mother into the Blessed Virgin Mary, and her brothers into the apostles. Serving them humbly became an opportunity for spiritual growth. Catherine resisted the accepted course of marriage and motherhood on the one hand, or a nun's veil on the other. She chose to live an active and prayerful life outside a convent's walls following the model of the Dominicans. Eventually her father gave up and permitted her to live as she pleased.

A vision of Saint Dominic gave strength to Catherine, but her wish to join his Order was no comfort to Lapa, who took her daughter with her to the baths in Bagno Vignoni to improve her health. Catherine fell seriously ill with a violent rash, fever and pain, which conveniently made her mother accept her wish to join the "Mantellate", the local association of Dominican tertiaries. Lapa went to the Sisters of the Order and persuaded them to take in her daughter. Within days, Catherine seemed entirely restored, rose from bed and donned the black and white habit of the Third Order of Saint Dominic. Catherine received the habit of a Dominican tertiary from the friars of the order after vigorous protests from the tertiaries themselves, who up to that point had been only widows. As a tertiary, she lived outside the convent, at home with her family like before. The Mantellate taught Catherine how to read, and she lived in almost total silence and solitude in the family home.

Her custom of giving away clothing and food without asking anyone's permission cost her family significantly, but she requested nothing for herself. By staying in their midst, she could live out her rejection of them more strongly. She did not want their food, referring to the table laid for her in Heaven with her real family. 
According to Raymond of Capua, at the age of twenty-one (c. 1368), Catherine experienced what she described in her letters as a "Mystical Marriage" with Jesus, later a popular subject in art as the "Mystic marriage of Saint Catherine". Caroline Walker Bynum explains one surprising and controversial aspect of this marriage that occurs both in artistic representations of the event and in some early accounts of her life: "Underlining the extent to which the marriage was a fusion with Christ's physicality [...] Catherine received, not the ring of gold and jewels that her biographer reports in his bowdlerized version, but the ring of Christ's foreskin." Catherine herself mentions the foreskin-as-wedding ring motif in one of her letters (#221), equating the wedding ring of a virgin with a foreskin; she typically claimed that her own wedding ring to Christ was simply invisible. Raymond of Capua also records that she was told by Christ to leave her withdrawn life and enter the public life of the world. Catherine rejoined her family and began helping the ill and the poor, where she took care of them in hospitals or homes. Her early pious activities in Siena attracted a group of followers, women and men, who gathered around her.

As social and political tensions mounted in Siena, Catherine found herself drawn to intervene in wider politics. She made her first journey to Florence in 1374, probably to be interviewed by the Dominican authorities at the General Chapter held in Florence in May 1374, though this is controverted (if she was interviewed, then the absence of later evidence suggests she was deemed sufficiently orthodox). It seems that at this time she acquired Raymond of Capua as her confessor and spiritual director.

After this visit, she began travelling with her followers throughout northern and central Italy advocating reform of the clergy and advising people that repentance and renewal could be done through "the total love for God." In Pisa, in 1375, she used what influence she had to sway that city and Lucca away from alliance with the anti-papal league whose force was gaining momentum and strength. She also lent her enthusiasm towards promoting the launch of a new crusade. It was in Pisa in 1375 that, according to Raymond of Capua's biography, she received the stigmata (visible, at Catherine's request, only to herself).

Physical travel was not the only way in which Catherine made her views known. From 1375 onwards, she began dictating letters to scribes. These letters were intended to reach men and women of her circle, increasingly widening her audience to include figures in authority as she begged for peace between the republics and principalities of Italy and for the return of the Papacy from Avignon to Rome. She carried on a long correspondence with Pope Gregory XI, asking him to reform the clergy and the administration of the Papal States.

Towards the end of 1375, she returned to Siena, to assist a young political prisoner, Niccolò di Tuldo, at his execution. In June 1376 Catherine went to Avignon as ambassador of the Republic of Florence to make peace with the Papal States (on 31 March 1376 Gregory XI had placed Florence under interdict). She was unsuccessful and was disowned by the Florentine leaders, who sent ambassadors to negotiate on their own terms as soon as Catherine's work had paved the way for them. Catherine sent an appropriately scorching letter back to Florence in response. While in Avignon, Catherine also tried to convince Pope Gregory XI, the last Avignon Pope, to return to Rome. Gregory did indeed return his administration to Rome in January 1377; to what extent this was due to Catherine's influence is a topic of much modern debate.

Catherine returned to Siena and spent the early months of 1377 founding a women's monastery of strict observance outside the city in the old fortress of Belcaro. She spent the rest of 1377 at Rocca d'Orcia, about twenty miles from Siena, on a local mission of peace-making and preaching. During this period, in autumn 1377, she had the experience which led to the writing of her "Dialogue" and learned to write, although she still seems to have chiefly relied upon her secretaries for her correspondence.

Late in 1377 or early in 1378 Catherine again travelled to Florence, at the order of Gregory XI, to seek peace between Florence and Rome. Following Gregory's death in March 1378 riots, the revolts of the Ciompi, broke out in Florence on 18 June, and in the ensuing violence she was nearly assassinated. Eventually, in July 1378, peace was agreed between Florence and Rome; Catherine returned quietly to Florence.

In late November 1378, with the outbreak of the Western Schism, the new Pope, Urban VI, summoned her to Rome. She stayed at Pope Urban VI's court and tried to convince nobles and cardinals of his legitimacy, both meeting with individuals at court and writing letters to persuade others.

For many years she had accustomed herself to a rigorous abstinence. She received the Holy Eucharist almost daily. This extreme fasting appeared unhealthy in the eyes of the clergy and her own sisterhood. Her confessor, Blessed Raymond, ordered her to eat properly. But Catherine claimed that she was unable to, describing her inability to eat as an "infermità" (illness). From the beginning of 1380, Catherine could neither eat nor swallow water. On February 26 she lost the use of her legs.

Catherine died in Rome, on 29 April 1380, at the age of thirty-three, having eight days earlier suffered a massive stroke which paralyzed her from the waist down. Her last words were, "Father, into Your Hands I commend my soul and my spirit."

There is some internal evidence of Catherine's personality, teaching and work in her nearly four hundred letters, her "Dialogue", and her prayers.

Much detail about her life has also, however, been drawn from the various sources written shortly after her death in order to promote her cult and canonisation. Though much of this material is heavily hagiographic, it has been an important source for historians seeking to reconstruct Catherine's life. Various sources are particularly important, especially the works of Raymond of Capua, who was Catherine's spiritual director and close friend from 1374 until her death, and himself became Master General of the Order in 1380. Raymond began writing what is known as the "Legenda Major", his "Life" of Catherine, in 1384, and completed it in 1395.

Another important work written after Catherine's death was "Libellus de Supplemento" ("Little Supplement Book"), written between 1412 and 1418 by Tommaso d'Antonio Nacci da Siena (commonly called Thomas of Siena, or Tommaso Caffarini): the work is an expansion of Raymond's "Legenda Major" making heavy use of the notes of Catherine's first confessor, Tommaso della Fonte (notes that do not survive anywhere else). Caffarini later published a more compact account of Catherine's life, entitled the "Legenda Minor".

From 1411 onwards, Caffarini also co-ordinated the compiling of the "Processus" of Venice, the set of documents submitted as part of the process of canonisation of Catherine, which provides testimony from nearly all of Catherine's disciples. There is also an anonymous piece entitled "Miracoli della Beata Caterina" ("Miracle of Blessed Catherine"), written by an anonymous Florentine. A few other relevant pieces survive.

Three genres of work by Catherine survive:

Catherine's theology can be described as mystical, and was employed towards practical ends for her own spiritual life or those of others. She used the language of medieval scholastic philosophy to elaborate her experiential mysticism. Interested mainly with achieving an incorporeal union with God, Catherine practiced extreme fasting and asceticism, eventually to the extent of living solely off the Eucharist every day. For Catherine, this practice was the means to realize fully her love of Christ in her mystical experience, with a large proportion of her ecstatic visions relating to the consumption or rejection of food during her life. She viewed Christ as a "bridge" between the soul and God and transmitted that idea, along with her other teachings, in her book The Dialogue. The Dialogue is highly systematic and explanatory in its presentation of her mystical ideas; however, these ideas themselves are not so much based in reason or logic as they are based in her ecstatic mystical experience.

She was buried in the (Roman) cemetery of Santa Maria sopra Minerva which lies near the Pantheon. After miracles were reported to take place at her grave, Raymond moved her inside the Basilica of Santa Maria sopra Minerva, where she lies to this day.

Her head however, was parted from her body and inserted in a gilt bust of bronze. This bust was later taken to Siena, and carried through that city in a procession to the Dominican church. Behind the bust walked Lapa, Catherine's mother, who lived until she was 89 years old. By then she had seen the end of the wealth and the happiness of her family, and followed most of her children and several of her grandchildren to the grave. She helped Raymond of Capua write his biography of her daughter, and said, "I think God has laid my soul athwart in my body, so that it can't get out." The incorrupt head and thumb were entombed in the Basilica of San Domenico at Siena, where they remain.

Pope Pius II, himself from Siena, canonized Catherine on 29 June 1461.

On 3 October 1970, Pope Paul VI named Catherine a Doctor of the Church; this title was almost simultaneously given to Saint Teresa of Ávila (27 September 1970), making them the first women to receive this honour.

Initially however, her feast day was not included in the General Roman Calendar. When it was added in 1597, it was put on the day of her death, April 29; however, because this conflicted with the feast of Saint Peter of Verona which also fell on the 29th of April, Catherine's feast day was moved in 1628 to the new date of April 30. In the 1969 revision of the calendar, it was decided to leave the celebration of the feast of St Peter of Verona to local calendars, because he was not as well known worldwide, and Catherine's feast was restored to April 29.

In his decree of 13 April 1866, Pope Pius IX declared Catherine of Siena to be a co-patroness of Rome. On 18 June 1939 Pope Pius XII named her a joint patron saint of Italy along with Saint Francis of Assisi.

On 1 October 1999, Pope John Paul II made her one of Europe's patron saints, along with Edith Stein and Bridget of Sweden. She is also the patroness of the historically Catholic American woman's fraternity, Theta Phi Alpha.

The people of Siena wished to have Catherine's body. A story is told of a miracle whereby they were partially successful: knowing that they could not smuggle her whole body out of Rome, they decided to take only her head which they placed in a bag. When stopped by the Roman guards, they prayed to Catherine to help them, confident that she would rather have her body (or at least part thereof) in Siena. When they opened the bag to show the guards, it appeared no longer to hold her head but to be full of rose petals. Once back at Siena, as they reopened the bag her head was visible once more. Due to this story, Catherine is often seen holding a lily.

Catherine ranks high among the mystics and spiritual writers of the Church. She remains a greatly respected figure for her spiritual writings, and political boldness to "speak truth to power"—it being exceptional for a woman, in her time period, to have had such influence in politics and on world history.

The main churches in honor of Catherine of Siena are:




English translations of The "Dialogue" include:

The Letters are translated into English as:

The Prayers are translated into English as:

Raymond of Capua's "Life" was translated into English in 1493 and 1609, and in Modern English is translated as:





</doc>
<doc id="7472" url="https://en.wikipedia.org/wiki?curid=7472" title="Charles Lyell">
Charles Lyell

Sir Charles Lyell, 1st Baronet, (14 November 1797 – 22 February 1875) was a Scottish geologist who popularised the revolutionary work of James Hutton. He is best known as the author of "Principles of Geology", which presented uniformitarianism–the idea that the Earth was shaped by the same scientific processes still in operation today–to the broad general public. "Principles of Geology" also challenged theories popularised by Georges Cuvier, which were the most accepted and circulated ideas about geology in Europe at the time.

His scientific contributions included an explanation of earthquakes, the theory of gradual "backed up-building" of volcanoes, and in stratigraphy the division of the Tertiary period into the Pliocene, Miocene, and Eocene. He also coined the currently-used names for geological eras, Palaeozoic, Mesozoic and Cenozoic. He incorrectly conjectured that icebergs may be the emphasis behind the transport of glacial erratics, and that silty loess deposits might have settled out of flood waters.

Lyell, following deistic traditions, favoured an indefinitely long age for the earth, despite geological evidence suggesting an old but finite age. He was a close friend of Charles Darwin, and contributed significantly to Darwin's thinking on the processes involved in evolution. He helped to arrange the simultaneous publication in 1858 of papers by Darwin and Alfred Russel Wallace on natural selection, despite his personal religious qualms about the theory. He later published evidence from geology of the time man had existed on Earth.

Lyell was born into a wealthy family, on 14 November 1797, at the family's estate house, Kinnordy House, near Kirriemuir in Forfarshire. He was the eldest of ten children. Lyell's father, also named Charles Lyell, was noted as a translator and scholar of Dante. Also an accomplished botanist, it was he who first exposed his son to the study of nature. Lyell's grandfather, also Charles Lyell, had made the family fortune supplying the Royal Navy at Montrose, enabling him to buy Kinnordy House.
The family seat is located in Strathmore, near the Highland Boundary Fault. Round the house, in the strath, is good farmland, but within a short distance to the north-west, on the other side of the fault, are the Grampian Mountains in the Highlands. His family's second country home was in a completely different geological and ecological area: he spent much of his childhood at Bartley Lodge in the New Forest, in Hampshire in southern England.

Lyell entered Exeter College, Oxford, in 1816, and attended William Buckland's lectures. He with a graduated BA Hons. second class degree in classics, in December 1819, and gained his M.A. 1821.
After graduation he took up law as a profession, entering Lincoln's Inn in 1820. He completed a circuit through rural England, where he could observe geological phenomena. In 1821 he attended Robert Jameson's lectures in Edinburgh, and visited Gideon Mantell at Lewes, in Sussex. In 1823 he was elected joint secretary of the Geological Society. As his eyesight began to deteriorate, he turned to geology as a full-time profession. His first paper, "On a recent formation of freshwater limestone in Forfarshire", was presented in 1822. By 1827, he had abandoned law and embarked on a geological career that would result in fame and the general acceptance of uniformitarianism, a working out of the ideas proposed by James Hutton a few decades earlier.
In 1832, Lyell married Mary Horner in Bonn, daughter of Leonard Horner (1785–1864), also associated with the Geological Society of London. The new couple spent their honeymoon in Switzerland and Italy on a geological tour of the area.

During the 1840s, Lyell travelled to the United States and Canada, and wrote two popular travel-and-geology books: "Travels in North America" (1845) and "A Second Visit to the United States" (1849). After the Great Chicago Fire, Lyell was one of the first to donate books to help found the Chicago Public Library. In 1866, he was elected a foreign member of the Royal Swedish Academy of Sciences.

Lyell's wife died in 1873, and two years later (in 1875) Lyell himself died as he was revising the twelfth edition of "Principles". He is buried in Westminster Abbey. Lyell was knighted (Kt) in 1848, and later, in 1864, made a baronet (Bt), which is an hereditary honour. He was awarded the Copley Medal of the Royal Society in 1858 and the Wollaston Medal of the Geological Society in 1866. Mount Lyell, the highest peak in Yosemite National Park, is named after him; the crater Lyell on the Moon and a crater on Mars were named in his honour; Mount Lyell in western Tasmania, Australia, located in a profitable mining area, bears Lyell's name; and the Lyell Range in north-west Western Australia is named after him as well. In Southwest Nelson in the South Island of New Zealand, the Lyell Range, Lyell River and the gold mining town of Lyell (now only a camping site) were all named after Lyell. The jawless fish "Cephalaspis lyelli", from the Old Red Sandstone of southern Scotland, was named by Louis Agassiz in honour of Lyell.

Lyell had private means, and earned further income as an author. He came from a prosperous family, worked briefly as a lawyer in the 1820s, and held the post of Professor of Geology at King's College London in the 1830s. From 1830 onward his books provided both income and fame. Each of his three major books was a work continually in progress. All three went through multiple editions during his lifetime, although many of his friends (such as Darwin) thought the first edition of the "Principles" was the best written. Lyell used each edition to incorporate additional material, rearrange existing material, and revisit old conclusions in light of new evidence.

"Principles of Geology", Lyell's first book, was also his most famous, most influential, and most important. First published in three volumes in 1830–33, it established Lyell's credentials as an important geological theorist and propounded the doctrine of uniformitarianism. It was a work of synthesis, backed by his own personal observations on his travels.

The central argument in "Principles" was that "the present is the key to the past" – a concept of the Scottish Enlightenment which David Hume had stated as "all inferences from experience suppose ... that the future will resemble the past", and James Hutton had described when he wrote in 1788 that "from what has actually been, we have data for concluding with regard to that which is to happen thereafter." Geological remains from the distant past can, and should, be explained by reference to geological processes now in operation and thus directly observable. Lyell's interpretation of geological change as the steady accumulation of minute changes over enormously long spans of time was a powerful influence on the young Charles Darwin. Lyell asked Robert FitzRoy, captain of HMS "Beagle", to search for erratic boulders on the survey voyage of the "Beagle", and just before it set out FitzRoy gave Darwin Volume 1 of the first edition of Lyell's "Principles". When the "Beagle" made its first stop ashore at St Jago in the Cape Verde islands, Darwin found rock formations which seen "through Lyell's eyes" gave him a revolutionary insight into the geological history of the island, an insight he applied throughout his travels.

While in South America Darwin received Volume 2 which considered the ideas of Lamarck in some detail. Lyell rejected Lamarck's idea of organic evolution, proposing instead "Centres of Creation" to explain diversity and territory of species. However, as discussed below, many of his letters show he was fairly open to the idea of evolution. In geology Darwin was very much Lyell's disciple, and brought back observations and his own original theorising, including ideas about the formation of atolls, which supported Lyell's uniformitarianism. On the return of the "Beagle" (October 1836) Lyell invited Darwin to dinner and from then on they were close friends. Although Darwin discussed evolutionary ideas with him from 1842, Lyell continued to reject evolution in each of the first nine editions of the "Principles". He encouraged Darwin to publish, and following the 1859 publication of "On the Origin of Species", Lyell finally offered a tepid endorsement of evolution in the tenth edition of "Principles".

"Elements of Geology" began as the fourth volume of the third edition of "Principles": Lyell intended the book to act as a suitable field guide for students of geology. The systematic, factual description of geological formations of different ages contained in "Principles" grew so unwieldy, however, that Lyell split it off as the "Elements" in 1838. The book went through six editions, eventually growing to two volumes and ceasing to be the inexpensive, portable handbook that Lyell had originally envisioned. Late in his career, therefore, Lyell produced a condensed version titled "Student's Elements of Geology" that fulfilled the original purpose.

"Geological Evidences of the Antiquity of Man" brought together Lyell's views on three key themes from the geology of the Quaternary Period of Earth history: glaciers, evolution, and the age of the human race. First published in 1863, it went through three editions that year, with a fourth and final edition appearing in 1873. The book was widely regarded as a disappointment because of Lyell's equivocal treatment of evolution. Lyell, a devout Christian, had great difficulty reconciling his beliefs with natural selection.

Lyell's geological interests ranged from volcanoes and geological dynamics through stratigraphy, palaeontology, and glaciology to topics that would now be classified as prehistoric archaeology and paleoanthropology. He is best known, however, for his role in popularising the doctrine of uniformitarianism. He played a critical role in advancing the study of loess.

From 1830 to 1833 his multi-volume "Principles of Geology" was published. The work's subtitle was "An attempt to explain the former changes of the Earth's surface by reference to causes now in operation", and this explains Lyell's impact on science. He drew his explanations from field studies conducted directly before he went to work on the founding geology text. He was, along with the earlier John Playfair, the major advocate of James Hutton's idea of uniformitarianism, that the earth was shaped entirely by slow-moving forces still in operation today, acting over a very long period of time. This was in contrast to catastrophism, an idea of abrupt geological changes, which had been adapted in England to support belief in Noah's flood. Describing the importance of uniformitarianism on contemporary geology, Lyell wrote,

Never was there a doctrine more calculated to foster indolence, and to blunt the keen edge of curiosity, than this assumption of the discordance between the former and the existing causes of change... The student was taught to despond from the first. Geology, it was affirmed, could never arise to the rank of an exact science... [With catastrophism] we see the ancient spirit of speculation revived, and a desire manifestly shown to cut, rather than patiently untie, the Gordian Knot.

Lyell saw himself as "the spiritual saviour of geology, freeing the science from the old dispensation of Moses." The two terms, "uniformitarianism" and "catastrophism", were both coined by William Whewell; in 1866 R. Grove suggested the simpler term "continuity" for Lyell's view, but the old terms persisted. In various revised editions (12 in all, through 1872), "Principles of Geology" was the most influential geological work in the middle of the 19th century, and did much to put geology on a modern footing. For his efforts he was knighted in 1848, then made a baronet in 1864.

Lyell noted the "economic advantages" that geological surveys could provide, citing their felicity in mineral-rich countries and provinces. Modern surveys, like the British Geological Survey (founded in 1835), and the US Geological Survey (founded in 1879), map and exhibit the natural resources within the country. So, in endorsing surveys, as well as advancing the study of geology, Lyell helped to forward the business of modern extractive industries, such as the coal and oil industry.

Before the work of Lyell, phenomena such as earthquakes were understood by the destruction that they brought. One of the contributions that Lyell made in "Principles" was to explain the cause of earthquakes. Lyell, in contrast focused on recent earthquakes (150 yrs), evidenced by surface irregularities such as faults, fissures, stratigraphic displacements and depressions.

Lyell's work on volcanoes focused largely on Vesuvius and Etna, both of which he had earlier studied. His conclusions supported gradual building of volcanoes, so-called "backed up-building", as opposed to the upheaval argument supported by other geologists.

Lyell's most important specific work was in the field of stratigraphy. From May 1828, until February 1829, he travelled with Roderick Impey Murchison (1792–1871) to the south of France (Auvergne volcanic district) and to Italy. In these areas he concluded that the recent strata (rock layers) could be categorised according to the number and proportion of marine shells encased within. Based on this he proposed dividing the Tertiary period into three parts, which he named the Pliocene, Miocene, and Eocene.

In "Principles of Geology" (first edition, vol. 3, Ch. 2, 1833) Lyell proposed that icebergs could be the means of transport for erratics. During periods of global warming, ice breaks off the poles and floats across submerged continents, carrying debris with it, he conjectured. When the iceberg melts, it rains down sediments upon the land. Because this theory could account for the presence of diluvium, the word "drift" became the preferred term for the loose, unsorted material, today called "till". Furthermore, Lyell believed that the accumulation of fine angular particles covering much of the world (today called loess) was a deposit settled from mountain flood water. Today some of Lyell's mechanisms for geological processes have been disproven, though many have stood the test of time. His observational methods and general analytical framework remain in use today as foundational principles in geology.

Lyell initially accepted the conventional view of other men of science, that the fossil record indicated a directional geohistory in which species went extinct. Around 1826, when he was on circuit, he read Lamarck's "Zoological Philosophy" and on 2 March 1827 wrote to Mantell, expressing admiration, but cautioning that he read it "rather as I hear an advocate on the wrong side, to know what can be made of the case in good hands".:
He struggled with the implications for human dignity, and later in 1827 wrote private notes on Lamarck's ideas. Lyell reconciled transmutation of species with natural theology by suggesting that it would be as much a "remarkable manifestation of creative Power" as creating each species separately. He countered Lamarck's views by rejecting continued cooling of the Earth in favour of "a fluctuating cycle", a long-term steady-state geohistory as proposed by James Hutton. The fragmentary fossil record already showed "a high class of fishes, close to reptiles" in the Carboniferous period which he called "the first Zoological era", and quadrupeds could also have existed then. In November 1827, after William Broderip found a Middle Jurassic fossil of the early mammal "Didelphis", Lyell told his father that "There was everything but man even as far back as the Oolite." Lyell inaccurately portrayed Lamarckism as a response to the fossil record, and said it was falsified by a lack of progress. He said in the second volume of "Principles" that the occurrence of this one fossil of the higher mammallia "in these ancient strata, is as fatal to the theory of successive development, as if several hundreds had been discovered."

In the first edition of "Principles", the first volume briefly set out Lyell's concept of a steady state with no real progression of fossils, in which humanity had appeared recently, with unique intellectual and moral qualities but no great physical distinction from animals. The second volume dismissed Lamarck's claims of animal forms arising from habits, continuous spontaneous generation of new life, and man having evolved from lower forms. Lyell explicitly rejected Lamark's concept of transmutation of species, drawing on Cuvier's arguments, and concluded that species had been created with stable attributes. He discussed the geographical distribution of plants and animals, and proposed that every species of plant or animal was descended from a pair or individual, originated in response to differing external conditions. Species would regularly go extinct, in a "struggle for existence" between hybrids, or a "war one with another" due to population pressure. He was vague about how replacement species formed, portraying this as an infrequent occurrence which could rarely be observed.

The leading naturalist Sir John Herschel wrote from Cape Town on 20 February 1836, thanking Lyell for sending a copy of "Principles" and praising the book as opening a way for bold speculation on "that mystery of mysteries, the replacement of extinct species by others" – by analogy with other intermediate causes, "the origination of fresh species, could it ever come under our cognizance, would be found to be a natural in contradistinction to a miraculous process". Lyell replied: "In regard to the origination of new species, I am very glad to find that you think it probable that it may be carried on through the intervention of intermediate causes. I left this rather to be inferred, not thinking it worth while to offend a certain class of persons by embodying in words what would only be a speculation." 
Whewell subsequently questioned this topic, and in March 1837 Lyell told him:

As a result of his letters and, no doubt, personal conversations, Huxley and Haeckel were convinced that, at the time he wrote "Principles", he believed new species had arisen by natural methods. Sedgwick wrote worried letters to him about this.

By the time Darwin returned from the "Beagle" survey expedition in 1836, he had begun to doubt Lyell's ideas about the permanence of species. He continued to be a close personal friend, and Lyell was one of the first scientists to support "On the Origin of Species", though he did not subscribe to all its contents. Lyell was also a friend of Darwin's closest colleagues, Hooker and Huxley, but unlike them he struggled to square his religious beliefs with evolution. This inner struggle has been much commented on. He had particular difficulty in believing in natural selection as the main motive force in evolution.

Lyell and Hooker were instrumental in arranging the peaceful co-publication of the theory of natural selection by Darwin and Alfred Russel Wallace in 1858: each had arrived at the theory independently. Lyell's data on stratigraphy were important because Darwin thought that populations of an organism changed slowly, requiring "geological time".

Although Lyell did not publicly accept evolution (descent with modification) at the time of writing the "Principles", after the Darwin–Wallace papers and the "Origin" Lyell wrote in his notebook:

Lyell's acceptance of natural selection, Darwin's proposed mechanism for evolution, was equivocal, and came in the tenth edition of "Principles". "The Antiquity of Man" (published in early February 1863, just before Huxley's "Man's place in nature") drew these comments from Darwin to Huxley:
Quite strong remarks: no doubt Darwin resented Lyell's repeated suggestion that he owed a lot to Lamarck, whom he (Darwin) had always specifically rejected. Darwin's daughter Henrietta (Etty) wrote to her father: "Is it fair that Lyell always calls your theory a modification of Lamarck's?" 

In other respects "Antiquity" was a success. It sold well, and it "shattered the tacit agreement that mankind should be the sole preserve of theologians and historians". But when Lyell wrote that it remained a profound mystery how the huge gulf between man and beast could be bridged, Darwin wrote "Oh!" in the margin of his copy.

Places named after Lyell:













</doc>
<doc id="7473" url="https://en.wikipedia.org/wiki?curid=7473" title="Chelsea F.C.">
Chelsea F.C.

Chelsea Football Club is a professional football club in London, England, that competes in the Premier League. Founded in 1905, the club's home ground since then has been Stamford Bridge.

Chelsea won the First Division title in 1955, followed by various cup competitions between 1965 and 1971. The past two decades have seen sustained success, with the club winning 21 trophies since 1997. In total, the club has won 27 major trophies; six titles, seven FA Cups, five League Cups and four FA Community Shields, one UEFA Champions League, two UEFA Cup Winners' Cups, one UEFA Europa League and one UEFA Super Cup.

Chelsea's regular kit colours are royal blue shirts and shorts with white socks. The club's crest has been changed several times in attempts to re-brand the club and modernise its image. The current crest, featuring a ceremonial lion rampant regardant holding a staff, is a modification of the one introduced in the early 1950s. The club have the sixth-highest average all-time attendance in English football, and for the 2016–17 season at 41,507.<ref name="14/15 attendances"></ref> Since 2003, Chelsea have been owned by Russian billionaire Roman Abramovich. In 2017, they were ranked by "Forbes" magazine as the seventh most valuable football club in the world, at £1.40 billion ($1.85 billion) and in the 2016–17 season it was the eighth highest-earning football club in the world, earned €428 million.

In 1904, Gus Mears acquired the Stamford Bridge athletics stadium with the aim of turning it into a football ground. An offer to lease it to nearby Fulham was turned down, so Mears opted to found his own club to use the stadium. As there was already a team named Fulham in the borough, the name of the adjacent borough of Chelsea was chosen for the new club; names like "Kensington FC", "Stamford Bridge FC" and "London FC" were also considered. Chelsea were founded on 10 March 1905 at The Rising Sun pub (now The Butcher's Hook), opposite the present-day main entrance to the ground on Fulham Road, and were elected to the Football League shortly afterwards.

The club won promotion to the First Division in their second season, and yo-yoed between the First and Second Divisions in their early years. They reached the 1915 FA Cup Final, where they lost to Sheffield United at Old Trafford, and finished third in the First Division in 1920, the club's best league campaign to that point. Chelsea attracted large crowds and had a reputation for signing big-name players, but success continued to elude the club in the inter-war years.

Former Arsenal and England centre-forward Ted Drake became manager in 1952 and proceeded to modernise the club. He removed the club's Chelsea pensioner crest, improved the youth set-up and training regime, rebuilt the side with shrewd signings from the lower divisions and amateur leagues, and led Chelsea to their first major trophy success – the League championship – in 1954–55. The following season saw UEFA create the European Champions' Cup, but after objections from The Football League and the FA Chelsea were persuaded to withdraw from the competition before it started. Chelsea failed to build on this success, and spent the remainder of the 1950s in mid-table. Drake was dismissed in 1961 and replaced by player-coach Tommy Docherty.

Docherty built a new team around the group of talented young players emerging from the club's youth set-up and Chelsea challenged for honours throughout the 1960s, enduring several near-misses. They were on course for a treble of League, FA Cup and League Cup going into the final stages of the 1964–65 season, winning the League Cup but faltering late on in the other two. In three seasons the side were beaten in three major semi-finals and were FA Cup runners-up. Under Docherty's successor, Dave Sexton, Chelsea won the FA Cup in 1970, beating Leeds United 2–1 in a final replay. Chelsea took their first European honour, a UEFA Cup Winners' Cup triumph, the following year, with another replayed win, this time over Real Madrid in Athens.

The late 1970s through to the '80s was a turbulent period for Chelsea. An ambitious redevelopment of Stamford Bridge threatened the financial stability of the club, star players were sold and the team were relegated. Further problems were caused by a notorious hooligan element among the support, which was to plague the club throughout the decade. In 1982, Chelsea were, at the nadir of their fortunes, acquired by Ken Bates for the nominal sum of £1, although by now the Stamford Bridge freehold had been sold to property developers, meaning the club faced losing their home. On the pitch, the team had fared little better, coming close to relegation to the Third Division for the first time, but in 1983 manager John Neal put together an impressive new team for minimal outlay. Chelsea won the Second Division title in 1983–84 and established themselves in the top division, before being relegated again in 1988. The club bounced back immediately by winning the Second Division championship in 1988–89.

After a long-running legal battle, Bates reunited the stadium freehold with the club in 1992 by doing a deal with the banks of the property developers, who had been bankrupted by a market crash. Chelsea's form in the new Premier League was unconvincing, although they did reach the 1994 FA Cup Final with Glenn Hoddle. It was not until the appointment of Ruud Gullit as player-manager in 1996 that their fortunes changed. He added several top international players to the side, as the club won the FA Cup in 1997 and established themselves as one of England's top sides again. Gullit was replaced by Gianluca Vialli, who led the team to victory in the League Cup Final, the UEFA Cup Winners' Cup Final and the UEFA Super Cup in 1998, the FA Cup in 2000 and their first appearance in the UEFA Champions League. Vialli was sacked in favour of Claudio Ranieri, who guided Chelsea to the 2002 FA Cup Final and Champions League qualification in 2002–03.

In June 2003, Bates sold Chelsea to Russian billionaire Roman Abramovich for £140 million. Over £100 million was spent on new players, but Ranieri was unable to deliver any trophies, and was replaced by José Mourinho. Under Mourinho, Chelsea became the fifth English team to win back-to-back league championships since the Second World War (2004–05 and 2005–06), in addition to winning an FA Cup (2007) and two League Cups (2005 and 2007). After a poor start to the 2007-2008 season, Mourinho was replaced by Avram Grant, who led the club to their first UEFA Champions League final, which they lost on penalties to Manchester United.

Luiz Felipe Scolari took over from Grant, but was sacked after 7 months following poor results. Guus Hiddink then took over the club on an interim basis while continuing to manage the Russian national football team. Hiddink guided Chelsea to another FA Cup success, after which he left the club to return full time to the Russian managerial position. In 2009–10, his successor Carlo Ancelotti led them to their first Premier League and FA Cup Double", the team becoming the first English top-flight club to score 100 league goals in a season since 1963. In 2012, caretaker manager Roberto Di Matteo led Chelsea to their seventh FA Cup, and their first UEFA Champions League title, beating Bayern Munich 4–3 on penalties, the first London club to win the trophy. In 2013, interim manager Rafael Benítez guided Chelsea to win the UEFA Europa League against Benfica, becoming the first club to hold two major European titles simultaneously and one of five clubs, and the first British club followed by Manchester United, to have won all three of UEFA's major club competitions. In the summer of 2013, Mourinho returned as manager, leading Chelsea to League Cup success in March 2015, and their fifth league title two months later. Mourinho was sacked after four months of the following season, with the club having lost 9 of their first 16 games and sitting only one point above the relegation zone. Two years later, under new coach Antonio Conte, Chelsea won its sixth English title.

Chelsea have only had one home ground, Stamford Bridge, where they have played since the team's foundation. It was officially opened on 28 April 1877 and for the first 28 years of its existence it was used almost exclusively by the London Athletic Club as an arena for athletics meetings and not at all for football. In 1904 the ground was acquired by businessman Gus Mears and his brother Joseph, who had also purchased nearby land (formerly a large market garden) with the aim of staging football matches on the now 12.5 acre (51,000 m²) site. Stamford Bridge was designed for the Mears family by the noted football architect Archibald Leitch, who had also designed Ibrox, Craven Cottage and Hampden Park. Most football clubs were founded first, and then sought grounds in which to play, but Chelsea were founded for Stamford Bridge.

Starting with an open bowl-like design and one covered terrace, Stamford Bridge had an original capacity of around 100,000. The early 1930s saw the construction of a terrace on the southern part of the ground with a roof that covered around one fifth of the stand. It eventually became known as the "Shed End", the home of Chelsea's most loyal and vocal supporters, particularly during the 1960s, 70s and 80s. The exact origins of the name are unclear, but the fact that the roof looked like a corrugated iron shed roof played a part.

In the early 1970s, the club's owners announced a modernisation of Stamford Bridge with plans for a state-of-the-art 50,000 all-seater stadium. Work began on the East Stand in 1972 but the project was beset with problems and was never completed; the cost brought the club close to bankruptcy, culminating in the freehold being sold to property developers. Following a long legal battle, it was not until the mid-1990s that Chelsea's future at the stadium was secured and renovation work resumed. The north, west and southern parts of the ground were converted into all-seater stands and moved closer to the pitch, a process completed by 2001.

When Stamford Bridge was redeveloped in the Bates era many additional features were added to the complex including two hotels, apartments, bars, restaurants, the Chelsea Megastore, and an interactive visitor attraction called Chelsea World of Sport. The intention was that these facilities would provide extra revenue to support the football side of the business, but they were less successful than hoped and before the Abramovich takeover in 2003 the debt taken on to finance them was a major burden on the club. Soon after the takeover a decision was taken to drop the "Chelsea Village" brand and refocus on Chelsea as a football club. However, the stadium is sometimes still referred to as part of ""Chelsea Village"" or ""The Village"".

The Stamford Bridge freehold, the pitch, the turnstiles and Chelsea's naming rights are now owned by Chelsea Pitch Owners, a non-profit organisation in which fans are the shareholders. The CPO was created to ensure the stadium could never again be sold to developers. As a condition for using the Chelsea FC name, the club has to play its first team matches at Stamford Bridge, which means that if the club moves to a new stadium, they may have to change their name.
Chelsea's training ground is located in Cobham, Surrey. Chelsea moved to Cobham in 2004. Their previous training ground in Harlington was taken over by QPR in 2005. The new training facilities in Cobham were completed in 2007.

Stamford Bridge has been used for a variety of other sporting events since 1905. It hosted the FA Cup Final from 1920 to 1922, has held ten FA Cup Semi-finals (most recently in 1978), ten FA Charity Shield matches (the last in 1970), and three England international matches, the last in 1932; it was also the venue for an unofficial "Victory International" in 1946. The 2013 UEFA Women's Champions League Final was played at Stamford Bridge. In October 1905 it hosted a rugby union match between the All Blacks and Middlesex, and in 1914 hosted a baseball match between the touring New York Giants and the Chicago White Sox. It was the venue for a boxing match between world flyweight champion Jimmy Wilde and Joe Conn in 1918. The running track was used for dirt track racing between 1928 and 1932, greyhound racing from 1933 to 1968, and Midget car racing in 1948. In 1980, Stamford Bridge hosted the first international floodlit cricket match in the UK, between Essex and the West Indies. It was also the home stadium of the London Monarchs American Football team for the 1997 season.
The current club ownership have stated that a larger stadium is necessary in order for Chelsea to stay competitive with rival clubs who have significantly larger stadia, such as Arsenal and Manchester United. Owing to its location next to a main road and two railway lines, fans can only enter the ground via the Fulham Road exits, which places constraints on expansion due to health and safety regulations. The club have consistently affirmed their desire to keep Chelsea at their current home, but have nonetheless been linked with a move to various nearby sites, including the Earls Court Exhibition Centre, Battersea Power Station and the Chelsea Barracks. In October 2011, a proposal from the club to buy back the freehold to the land on which Stamford Bridge sits was voted down by Chelsea Pitch Owners shareholders. In May 2012, the club made a formal bid to purchase Battersea Power Station, with a view to developing the site into a new stadium, but lost out to a Malaysian consortium. The club subsequently announced plans to redevelop Stamford Bridge into a 60,000-seater stadium.
On 11 January 2017 it was announced that the stadium was given the go ahead from Hammersmith and Fulham council for the new 60,000 stadium to be built.

Chelsea have had four main crests, which all underwent minor variations. The first, adopted when the club was founded, was the image of a Chelsea pensioner, the army veterans who reside at the nearby Royal Hospital Chelsea. This contributed to the club's original "pensioner" nickname, and remained for the next half-century, though it never appeared on the shirts. When Ted Drake became Chelsea manager in 1952, he began to modernise the club. Believing the Chelsea pensioner crest to be old-fashioned, he insisted that it be replaced. A stop-gap badge which comprised the initials C.F.C. was adopted for a year. In 1953, the club crest was changed to an upright blue lion looking backwards and holding a staff. It was based on elements in the coat of arms of the Metropolitan Borough of Chelsea with the "lion rampant regardant" taken from the arms of then club president Viscount Chelsea and the staff from the Abbots of Westminster, former Lords of the Manor of Chelsea. It also featured three red roses, to represent England, and two footballs. This was the first Chelsea crest to appear on the shirts, in the early 1960s.

In 1986, with Ken Bates now owner of the club, Chelsea's crest was changed again as part of another attempt to modernise and because the old rampant lion badge could not be trademarked. The new badge featured a more naturalistic non-heraldic lion, in white and not blue, standing over the C.F.C. initials. This lasted for the next 19 years, with some modifications such as the use of different colours, including red from 1987 to 1995, and yellow from 1995 until 1999, before the white returned. With the new ownership of Roman Abramovich, and the club's centenary approaching, combined with demands from fans for the popular 1950s badge to be restored, it was decided that the crest should be changed again in 2005. The new crest was officially adopted for the start of the 2005–06 season and marked a return to the older design, used from 1953 to 1986, featuring a blue heraldic lion holding a staff. For the centenary season this was accompanied by the words '100 YEARS' and 'CENTENARY 2005–2006' on the top and bottom of the crest respectively.

Chelsea have always worn blue shirts, although they originally used the paler eton blue, which was taken from the racing colours of then club president, Earl Cadogan, and was worn with white shorts and dark blue or black socks. The light blue shirts were replaced by a royal blue version in around 1912. In the 1960s Chelsea manager Tommy Docherty changed the kit again, switching to blue shorts (which have remained ever since) and white socks, believing it made the club's colours more modern and distinctive, since no other major side used that combination; this kit was first worn during the 1964–65 season. Since then Chelsea have always worn white socks with their home kit apart from a short spell from 1985 to 1992, when blue socks were reintroduced.

Chelsea's away colours are usually all yellow or all white with blue trim. More recently, the club have had a number of black or dark blue away kits which alternate every year. As with most teams, they have also had some more unusual ones. At Docherty's behest, in the 1966 FA Cup semi-final they wore blue and black stripes, based on Inter Milan's kit. In the mid-1970s, the away strip was a red, white and green kit inspired by the Hungarian national side of the 1950s. Other memorable away kits include an all jade strip worn from 1986–89, red and white diamonds from 1990–92, graphite and tangerine from 1994–96, and luminous yellow from 2007–08. The graphite and tangerine strip often appears in lists of the worst football kits ever.

Chelsea are among the most widely supported football clubs in the world. They have the sixth highest average all-time attendance in English football and regularly attract over 40,000 fans to Stamford Bridge; they were the seventh best-supported Premier League team in the 2013–14 season, with an average gate of 41,572. Chelsea's traditional fanbase comes from all over the Greater London area including working-class parts such as Hammersmith and Battersea, wealthier areas like Chelsea and Kensington, and from the home counties. There are also numerous official supporters clubs in the United Kingdom and all over the world. Between 2007 and 2012, Chelsea were ranked fourth worldwide in annual replica kit sales, with an average of 910,000. Chelsea's official Twitter account has 9.8 million followers as of September 2017.

At matches, Chelsea fans sing chants such as "Carefree" (to the tune of "Lord of the Dance", whose lyrics were probably written by supporter Mick Greenaway), "Ten Men Went to Mow", "We All Follow the Chelsea" (to the tune of "Land of Hope and Glory"), "Zigga Zagga", and the celebratory "Celery", with the latter often resulting in fans ritually throwing celery. The vegetable was banned inside Stamford Bridge after an incident involving Arsenal midfielder Cesc Fàbregas at the 2007 League Cup Final.

During the 1970s and 1980s in particular, Chelsea supporters were associated with football hooliganism. The club's "football firm", originally known as the Chelsea Shed Boys, and subsequently as the Chelsea Headhunters, were nationally notorious for football violence, alongside hooligan firms from other clubs such as West Ham United's Inter City Firm and Millwall's Bushwackers, before, during and after matches. The increase of hooligan incidents in the 1980s led chairman Ken Bates to propose erecting an electric fence to deter them from invading the pitch, a proposal that the Greater London Council rejected.

Since the 1990s, there has been a marked decline in crowd trouble at matches, as a result of stricter policing, CCTV in grounds and the advent of all-seater stadia. In 2007, the club launched the 'Back to the Shed' campaign to improve the atmosphere at home matches, with notable success. According to Home Office statistics, 126 Chelsea fans were arrested for football-related offences during the 2009–10 season, the third highest in the division, and 27 banning orders were issued, the fifth-highest in the division.

Chelsea have long-standing rivalries with North London clubs Arsenal and Tottenham Hotspur. A strong rivalry with Leeds United dates back to several heated and controversial matches in the 1960s and 1970s, particularly the 1970 FA Cup Final. More recently a rivalry with Liverpool has grown following repeated clashes in cup competitions. Chelsea's fellow West London sides Brentford, Fulham and Queens Park Rangers are generally not considered major rivals, as matches have only taken place intermittently due to the clubs often being in separate divisions. A 2004 survey by Planetfootball.com found that Chelsea fans consider their main rivalries to be with (in order): Arsenal, Tottenham Hotspur and Manchester United. In the same survey, fans of six clubs (Arsenal, Fulham, Leeds United, QPR, Tottenham and West Ham United) named Chelsea as one of their three main rivals. In a 2008 poll conducted by the Football Fans Census, Chelsea fans named Liverpool, Arsenal and Manchester United as their most disliked clubs. However, a 2012 survey has shown that Chelsea fans consider Tottenham to be their main rival, above Arsenal and Manchester United.

Chelsea's highest appearance-maker is ex-captain Ron Harris, who played in 795 competitive games for the club between 1961 and 1980. The record for a Chelsea goalkeeper is held by Harris's contemporary, Peter Bonetti, who made 729 appearances (1959–79). With 103 caps (101 while at the club), Frank Lampard of England is Chelsea's most capped international player.

Frank Lampard is Chelsea's all-time top goalscorer, with 211 goals in 648 games (2001–2014); he passed Bobby Tambling's longstanding record of 202 in May 2013. Seven other players have also scored over 100 goals for Chelsea: George Hilsdon (1906–12), George Mills (1929–39), Roy Bentley (1948–56), Jimmy Greaves (1957–61), Peter Osgood (1964–74 and 1978–79), Kerry Dixon (1983–92) and Didier Drogba (2004–12 and 2014–2015). Greaves holds the record for the most goals scored in one season (43 in 1960–61).

Chelsea's biggest winning scoreline in a competitive match is 13–0, achieved against Jeunesse Hautcharage in the Cup Winners' Cup in 1971. The club's biggest top-flight win was an 8–0 victory against Wigan Athletic in 2010, which was matched in 2012 against Aston Villa. Chelsea's biggest loss was an 8–1 reverse against Wolverhampton Wanderers in 1953. Officially, Chelsea's highest home attendance is 82,905 for a First Division match against Arsenal on 12 October 1935. However, an estimated crowd of over 100,000 attended a friendly match against Soviet team Dynamo Moscow on 13 November 1945. The modernisation of Stamford Bridge during the 1990s and the introduction of all-seater stands mean that neither record will be broken for the foreseeable future. The current legal capacity of Stamford Bridge is 41,663. Every starting player in Chelsea's 57 games of the 2013–14 season was a full international – a new club record.
Chelsea hold the English record for the highest ever points total for a league season (95), the fewest goals conceded during a league season (15), the highest number of clean sheets overall in a Premier League season (25) (all set during the 2004–05 season), the most consecutive clean sheets from the start of a league season (6, set during the 2005–06 season), and the highest number of Premier League victories in a season (30), set during the 2016–17 season. The club's 21–0 aggregate victory over Jeunesse Hautcharage in the UEFA Cup Winners' Cup in 1971 remains a record in European competition. Chelsea hold the record for the longest streak of unbeaten matches at home in the English top flight, which lasted 86 matches from 20 March 2004 to 26 October 2008. They secured the record on 12 August 2007, beating the previous record of 63 matches unbeaten set by Liverpool between 1978 and 1980. Chelsea's streak of eleven consecutive away league wins, set between 5 April 2008 and 6 December 2008, is also a record for the English top flight. Their £50 million purchase of Fernando Torres from Liverpool in January 2011 was the record transfer fee paid by a British club until Ángel Di María signed for Manchester United in August 2014 for £59.7 million.

Chelsea, along with Arsenal, were the first club to play with shirt numbers, on 25 August 1928 in their match against Swansea Town. They were the first English side to travel by aeroplane to a domestic away match, when they visited Newcastle United on 19 April 1957, and the first First Division side to play a match on a Sunday, when they faced Stoke City on 27 January 1974. On 26 December 1999, Chelsea became the first British side to field an entirely foreign starting line-up (no British or Irish players) in a Premier League match against Southampton.

In May 2007, Chelsea were the first team to win the FA Cup at the new Wembley Stadium, having also been the last to win it at the old Wembley. They were the first English club to be ranked No. 1 under UEFA's five-year coefficient system in the 21st century. They were the first team in Premier League history to score at least 100 goals in a single season, reaching the milestone on the final day of the 2009–10 season. Chelsea are the only London club to win the UEFA Champions League, after beating Bayern Munich in the 2012 final. Upon winning the 2012–13 UEFA Europa League, Chelsea became the first English club to win all four European trophies and the only club to hold the Champions League and the Europa League at the same time.

Chelsea Football Club were founded by Gus Mears in 1905. After his death in 1912, his descendents continued to own the club until 1982, when Ken Bates bought the club from Mears' great-nephew Brian Mears for £1. Bates bought a controlling stake in the club and floated Chelsea on the AIM stock exchange in March 1996. In July 2003, Roman Abramovich purchased just over 50% of Chelsea Village plc's share capital, including Bates' 29.5% stake, for £30 million and over the following weeks bought out most of the remaining 12,000 shareholders at 35 pence per share, completing a £140 million takeover. Other shareholders at the time of the takeover included the Matthew Harding estate (21%), BSkyB (9.9%) and various anonymous offshore trusts. After passing the 90% share threshold, Abramovich took the club back into private hands, delisting it from the AIM on 22 August 2003. He also took on responsibility for the club's debt of £80 million, quickly paying most of it.

Thereafter, Abramovich changed the ownership name to Chelsea FC plc, whose ultimate parent company is Fordstam Limited, which is controlled by him. Chelsea are additionally funded by Abramovich via interest free soft loans channelled through his holding company Fordstam Limited. The loans stood at £709 million in December 2009, when they were all converted to equity by Abramovich, leaving the club themselves debt free, although the debt remains with Fordstam. Since 2008 the club have had no external debt.

Chelsea did not turn a profit in the first nine years of Abramovich's ownership, and made record losses of £140m in June 2005. In November 2012, Chelsea announced a profit of £1.4 million for the year ending 30 June 2012, the first time the club had made a profit under Abramovich's ownership. This was followed by a loss in 2013 and then their highest ever profit of £18.4 million for the year to June 2014.

Chelsea have been described as a global brand; a 2012 report by Brand Finance ranked Chelsea fifth among football brands and valued the club's brand value at US$398 million – an increase of 27% from the previous year, also valuing them at US$10 million more than the sixth best brand, London rivals Arsenal – and gave the brand a strength rating of AA (very strong). In 2016, "Forbes" magazine ranked Chelsea the seventh most valuable football club in the world, at £1.15 billion ($1.66 billion). As of 2016, Chelsea are ranked eighth in the Deloitte Football Money League with an annual commercial revenue of £322.59 million.

Chelsea's kit has been manufactured by Nike since July 2017. Previously, the kit was manufactured by Adidas, which was originally contracted to supply the club's kit from 2006 to 2018. The partnership was extended in October 2010 in a deal worth £160 million over eight years. This deal was again extended in June 2013 in a deal worth £300 million over another ten years. In May 2016, Adidas announced that by mutual agreement, the kit sponsorship would end six years early on 30 June 2017. Chelsea had to pay £40m in compensation to Adidas. In October 2016, Nike was announced as the new kit sponsor, in a deal worth £900m over 15 years, until 2032. Previously, the kit was manufactured by Umbro (1975–81), Le Coq Sportif (1981–86), The Chelsea Collection (1986–87), Umbro (1987–2006), and Adidas (2006–2017).

Chelsea's first shirt sponsor was Gulf Air, agreed during the 1983–84 season. The club were then sponsored by Grange Farms, Bai Lin Tea and Simod before a long-term deal was signed with Commodore International in 1989; Amiga, an offshoot of Commodore, also appeared on the shirts. Chelsea were subsequently sponsored by Coors beer (1994–97), Autoglass (1997–2001), Emirates (2001–05), Samsung Mobile (2005–08) and Samsung (2008–15). Chelsea's current shirt sponsor is the Yokohama Rubber Company. Worth £40 million per year, the deal is second in English football to Chevrolet's £50 million-per-year sponsorship of Manchester United.

The club has a variety of other sponsors, which include Carabao, Delta Air Lines, Beats by Dre, Singha, EA Sports, Rexona, Hublot, Ericsson, William Hill, Levy Restaurants, Wipro, Grand Royal Whisky, Bangkok Bank, Guangzhou R&F, Mobinil, IndusInd Bank, and Ole777.

In 1930, Chelsea featured in one of the earliest football films, "The Great Game". One-time Chelsea centre forward, Jack Cock, who by then was playing for Millwall, was the star of the film and several scenes were shot at Stamford Bridge, including on the pitch, the boardroom, and the dressing rooms. It included guest appearances by then-Chelsea players Andrew Wilson, George Mills, and Sam Millington. Owing to the notoriety of the Chelsea Headhunters, a football firm associated with the club, Chelsea have also featured in films about football hooliganism, including 2004's "The Football Factory". Chelsea also appear in the Hindi film "Jhoom Barabar Jhoom". In April 2011, Montenegrin comedy series "Nijesmo mi od juče" made an episode in which Chelsea play against FK Sutjeska Nikšić for qualification of the UEFA Champions League.

Up until the 1950s, the club had a long-running association with the music halls; their underachievement often provided material for comedians such as George Robey. It culminated in comedian Norman Long's release of a comic song in 1933, ironically titled "On the Day That Chelsea Went and Won the Cup", the lyrics of which describe a series of bizarre and improbable occurrences on the hypothetical day when Chelsea finally won a trophy. In Alfred Hitchcock's 1935 film "The 39 Steps", Mr Memory claims that Chelsea last won the Cup in 63 BC, "in the presence of the Emperor Nero." Scenes in a 1980 episode of "Minder" were filmed during a real match at Stamford Bridge between Chelsea and Preston North End with Terry McCann (played by Dennis Waterman) standing on the terraces.

The song "Blue is the Colour" was released as a single in the build-up to the 1972 League Cup Final, with all members of Chelsea's first team squad singing; it reached number five in the UK Singles Chart. The song has since been adopted as an anthem by a number of other sports teams around the world, including the Vancouver Whitecaps (as "White is the Colour") and the Saskatchewan Roughriders (as "Green is the Colour"). In the build-up to the 1997 FA Cup Final, the song "Blue Day", performed by Suggs and members of the Chelsea squad, reached number 22 in the UK charts. Bryan Adams, a fan of Chelsea, dedicated the song "We're Gonna Win" from the album "18 Til I Die" to the club.
Chelsea also operate a women's football team, Chelsea Ladies. They have been affiliated to the men's team since 2004 and are part of the club's Community Development programme. They play their home games at Wheatsheaf Park, the home ground of Conference South club Staines Town. The club were promoted to the Premier Division for the first time in 2005 as Southern Division champions and won the Surrey County Cup nine times between 2003 and 2013. In 2010 Chelsea Ladies were one of the eight founder members of the FA Women's Super League. In 2015, Chelsea Ladies won the FA Women's Cup for the first time, beating Notts County Ladies at Wembley Stadium, and a month later clinched their first FA WSL title to complete a league and cup double. John Terry, former captain of the Chelsea men's team, is the President of Chelsea Ladies.

"For recent transfers, see 2017–18 Chelsea F.C. season."

"For further information: Chelsea F.C. Under-23s and Academy"

Source: Chelsea F.C.

The following managers won at least one trophy when in charge of Chelsea:

!Position
!Staff

Chelsea FC plc is the company which owns Chelsea Football Club. The ultimate parent company of Chelsea FC plc is Fordstam Limited and the ultimate controlling party of Fordstam Limited is Roman Abramovich.

On 22 October 2014, Chelsea announced that Ron Gourlay, after ten successful years at the club including five as Chief Executive, is leaving Chelsea to pursue new business opportunities. On 27 October 2014, Chelsea announced that Christian Purslow is joining the club to run global commercial activities and the club do not expect to announce any other senior appointments in the near future having chairman Bruce Buck and Director Marina Granovskaia assumed the executive responsibilities. Guy Laurence was appointed as the club's Chief Executive on 11 January 2018, filling the vacancy following the departure of Michael Emenalo.

Chelsea Ltd.

Chelsea F.C. plc Board

Executive Board

Life President

Vice-Presidents

Source: Chelsea F.C.

Upon winning the 2012–13 UEFA Europa League, Chelsea became the fourth club in history to have won the "European Treble" of European Cup/UEFA Champions League, European Cup Winners' Cup/UEFA Cup Winners' Cup, and UEFA Cup/UEFA Europa League after Juventus, Ajax and Bayern Munich. Chelsea are the first English club to have won all three major UEFA trophies.





Source: Chelsea F.C.






</doc>
<doc id="7475" url="https://en.wikipedia.org/wiki?curid=7475" title="CANDU reactor">
CANDU reactor

The CANDU, for Canada Deuterium Uranium, is a Canadian pressurized heavy water reactor design used to generate electric power. The acronym refers to its deuterium oxide (heavy water) moderator and its use of (originally, natural) uranium fuel. CANDU reactors were first developed in the late 1950s and 1960s by a partnership between Atomic Energy of Canada Limited (AECL), the Hydro-Electric Power Commission of Ontario, Canadian General Electric, and other companies.

There have been two major types of CANDU reactors, the original design of around 500 MWe that was intended to be used in multi-reactor installations in large plants, and the rationalized CANDU 6 in the 600 MWe class that is designed to be used in single stand-alone units or in small multi-unit plants. CANDU 6 units were built in Quebec and New Brunswick, as well as Pakistan, Argentina, South Korea, Romania, and China. A single example of a non-CANDU 6 design was sold to India. The multi-unit design was used only in Ontario, Canada, and grew in size and power as more units were installed in the province, reaching ~880 MWe in the units installed at the Darlington Nuclear Generating Station. An effort to rationalize the larger units in a fashion similar to CANDU 6 led to the CANDU 9.

By the early 2000s, sales prospects for the original CANDU designs were dwindling due to the introduction of newer designs from other companies. AECL responded by cancelling CANDU 9 development and moving to the Advanced CANDU reactor (ACR) design. ACR failed to find any buyers; its last potential sale was for an expansion at Darlington, but this was cancelled in 2009. In October 2011, the Canadian Federal Government licensed the CANDU design to Candu Energy (a wholly owned subsidiary of SNC-Lavalin), which also acquired the former reactor development and marketing division of AECL at that time. Candu Energy offers support services for existing sites, and are completing formerly stalled installations in Romania and Argentina through a partnership with China National Nuclear Corporation. Sales effort for new reactors has ended, along with design work on CANDU and ACR.

The basic operation of the CANDU design is similar to other nuclear reactors. Fission reactions in the reactor core heat pressurized water in a "primary cooling loop". A heat exchanger, also known as a steam generator, transfers the heat to a "secondary cooling loop", which powers a steam turbine with an electric generator attached to it (for a typical Rankine thermodynamic cycle). The exhaust steam from the turbines is then cooled, condensed and returned as feedwater to the steam generator. The final cooling often uses cooling water from a nearby source, such as a lake, river, or ocean. Newer CANDU plants, such as the Darlington Nuclear Generating Station near Toronto, Ontario, use a diffuser to spread the warm outlet water over a larger volume and limit the effects on the environment. 

Where the CANDU design differs is details of the fissile core and the primary cooling loop. Natural uranium consists of a mix of mostly uranium-238 with small amounts of uranium-235 and trace amounts of other isotopes. Fission in these elements releases high-energy neutrons, which can cause other U-235 atoms in the fuel to undergo fission as well. This process is much more effective when the neutron energies are much lower than what the reactions release naturally. Most reactors use some form of neutron moderator to lower the energy of the neutrons, or "thermalize" them, which makes the reaction more efficient. The energy lost by the neutrons heats the moderator and is extracted for power.

Most commercial reactor designs use normal water as the moderator. Water will absorb some of the neutrons, enough that it is not possible to keep the reaction going. CANDU replaces this "light" water with heavy water.
Heavy water’s extra neutron decreases its ability to absorb excess neutrons, resulting in a better neutron economy. This allows CANDU to run on unenriched natural uranium, or uranium mixed with a wide variety of other materials such as plutonium and thorium. This was a major goal of the CANDU design; by operating on natural uranium the cost of enrichment is removed. This also presents an advantage in nuclear proliferation terms, as there is no need for enrichment facilities which might also be used for weapons. 

In conventional light-water reactor (LWR) designs, the entire fissile core is placed in a large pressure vessel. The amount of heat that can be removed by a unit of a coolant is a function of the temperature; by pressurizing the core the water can be heated to much greater temperatures before boiling, thereby removing more heat and allowing the core to be smaller and more efficient. Building a pressure vessel of the required size is a challenge. At the time of CANDU's design, Canada lacked the heavy industry to cast and machine the pressure vessels of this size.

In CANDU the fuel bundles are instead contained in much smaller metal tubes about 10 cm diameter. The tubes are then contained in a larger vessel containing additional heavy water acting purely as a moderator. This vessel, known as a "calandria", is not pressurized and remains at much lower temperatures, making it much easier to fabricate. In order to prevent the heat from the pressure tubes leaking into the surrounding moderator, each fuel tube is enclosed in a second tube. Carbon dioxide gas in the gap between the two tubes acts as an insulator. The moderator tank also acts as a large heat sink that provides an additional safety feature.

In a conventional design with a pressurized core, refuelling the system requires the core to shut down and the pressure vessel to be opened. Due to the arrangement used in CANDU, only the single tube being refuelled needs to be depressurized. This allows the CANDU system to be continually refuelled without shutting down, another major design goal. In modern systems, two robotic machines hook up to the reactor faces and open the end caps of a pressure tube. One machine pushes in the new fuel, whereby the depleted fuel is pushed out and collected at the other end. A significant operational advantage of online refuelling is that a failed or leaking fuel bundle can be removed from the core once it has been located, thus reducing the radiation levels in the primary cooling loop.

Each fuel bundle is a cylinder assembled from alloy tubes containing ceramic pellets of fuel. In older designs, the assembly had 28 or 37 half-meter-long fuel tubes with 12 such assemblies lying end to end in a pressure tube. The newer CANFLEX bundle has 43 tubes, with two pellet sizes (so the power rating can be increased without melting the hottest pellets). It is about in diameter, long and weighs about and replaces the 37-tube bundle. To allow the neutrons to flow freely between the bundles, the tubes and bundles are made of neutron-transparent zircaloy (zirconium + 2.5% wt niobium).

Natural uranium is a mix of isotopes—mainly uranium-238, with 0.72% fissile uranium-235 by weight. A reactor aims for a steady rate of fission over time, where the neutrons released by fission cause an equal number of fissions in other fissile atoms. This balance is referred to as "criticality". The neutrons released in these reactions are fairly energetic and don't readily react with (get "captured" by) the surrounding fissile material. In order to improve this rate, they must have their energy "moderated", ideally to the same energy as the fuel atoms themselves. As these neutrons are in thermal equilibrium with the fuel, they are referred to as "thermal neutrons".

During moderation it helps to separate the neutrons and uranium, since U has a large affinity for intermediate-energy neutrons ("resonance" absorption), but is only easily fissioned by the few energetic neutrons above ≈1.5–2 MeV. Since most of the fuel is usually U, most reactor designs are based on thin fuel rods separated by moderator, allowing the neutrons to travel in the moderator before entering the fuel again. More neutrons are released than are needed to maintain the chain reaction; when uranium-238 absorbs just the excess, plutonium is created which helps to make up for the depletion of uranium-235. Eventually the build-up of fission products that are even more neutron-absorbing than U slows the reaction and calls for refuelling.

Light water makes an excellent moderator, the light hydrogen atoms are very close in mass to a neutron and can absorb a lot of energy in a single collision (like a collision of two billiard balls). Light hydrogen is also fairly effective at absorbing neutrons, and there will be too few left over to react with the small amount of U in natural uranium, preventing criticality. In order to allow criticality, the fuel must be "enriched", increasing the amount of U to an usable level. In light water reactors, the fuel is typically enriched to between 2% and 5% U (the leftover fraction with less U is called depleted uranium). Enrichment facilities are expensive to build and operate. They are also a proliferation concern as they can be used to enrich the U much further, up to weapons-grade material (90% or more U). This can be remedied if the fuel is supplied and reprocessed by an internationally approved supplier.

The main advantage of heavy water moderator over light water is the reduced absorption of the neutrons that sustain the chain reaction, allowing a lower concentration of active atoms (to the point of using unenriched natural uranium fuel). Deuterium ("heavy hydrogen") already has the extra neutron that light hydrogen would absorb, reducing the tendency to capture neutrons. Deuterium is twice the mass of a single neutron (vs light hydrogen which is about the same mass); the mismatch means more collisions are needed to moderate the neutrons, requiring a larger thickness of moderator between the fuel rods. This increases the size of the reactor core and the leakage of neutrons. It is also the practical reason for the calandria design, otherwise, a very large pressure vessel would be needed. The low U density in natural uranium also implies that less of the fuel will be consumed before the fission rate drops too low to sustain criticality, because the ratio of U to fission products+U is lower. In CANDU most of the moderator is at lower temperatures than in other designs, reducing the spread of speeds and the overall speed of the moderator particles. This means most of the neutrons will end up at a lower energy and be more likely to cause fission, so CANDU not only "burns" natural uranium, but it does so more effectively as well. Overall, CANDU reactors use 30–40% less mined uranium than light-water reactors per unit of electricity produced. This is a major advantage to the heavy water design; it not only requires less fuel, but as the fuel does not have to be enriched, it is much less expensive as well.

A further unique feature of heavy-water moderation is the greater stability of the chain reaction. This is due to the relatively low binding energy of the deuterium nucleus (2.2 MeV), leading to some energetic neutrons and especially gamma rays breaking the deuterium nuclei apart to produce extra neutrons. Both gammas produced directly by fission and by the decay of fission fragments have enough energy, and the half-lives of the fission fragments range from seconds to hours or even years. The slow response of these gamma-generated neutrons delays the response of the reactor and gives the operators extra time in case of an emergency. Since gamma rays travel for meters through water, an increased rate of chain reaction in one part of the reactor will produce a response from the rest of the reactor, allowing various negative feedbacks to stabilize the reaction.

On the other hand, the fission neutrons are thoroughly slowed down before they reach another fuel rod, meaning that it takes neutrons a longer time to get from one part of the reactor to the other. Thus if the chain reaction accelerates in one section of the reactor, the change will propagate itself only slowly to the rest of the core, giving time to respond in an emergency. The independence of the neutrons' energies from the nuclear fuel used is what allows for such fuel flexibility in a CANDU reactor, since every fuel bundle will experience the same environment and affect its neighbors in the same way, whether the fissile material is uranium-235, uranium-233 or plutonium.

Canada developed the heavy water moderated design in the post-World War II era to explore nuclear energy while lacking access to enrichment facilities. War-era enrichment systems were extremely expensive to build and operate, whereas the heavy water solution allowed the use of natural uranium in the experimental ZEEP reactor. A much less expensive enrichment system was developed, but the United States classified work on the cheaper gas centrifuge process. The CANDU was therefore designed to use natural uranium.

The CANDU includes a number of active and passive safety features in its design. Some of these are a side-effect of the physical layout of the system.

CANDU designs have a positive void coefficient as well as a small power coefficient, normally considered bad in reactor design. This implies that steam generated in the coolant will "increase" the reaction rate, which in turn would generate more steam. This is one of the many reasons for the cooler mass of moderator in the calandria, as even a serious steam incident in the core would not have a major impact on the overall moderation cycle. Only if the moderator itself starts to boil would there be any significant effect, and the large thermal mass ensures this will occur slowly. The deliberately "sluggish" response of the fission process in CANDU allows controllers more time to diagnose and deal with problems.

The fuel channels can only maintain criticality if they are mechanically sound. If the temperature of the fuel bundles increases to the point where they are mechanically unstable, their horizontal layout means they will bend under gravity, shifting the layout of the bundles and reducing the efficiency of the reactions. Because the original fuel arrangement is optimum for a chain reaction and the natural uranium fuel has little excess reactivity, any significant deformation will stop the inter-fuel pellet fission reaction. This will not stop heat production from fission product decay, which would continue to supply a considerable heat output. If this process further weakens the fuel bundles, they will eventually bend far enough to touch the calandria tube, allowing heat to be efficiently transferred into the moderator tank. The moderator vessel has a considerable thermal capability on its own, and is normally kept relatively cool.

Heat generated by fission products would initially be at about 7% of full reactor power, which requires significant cooling. The CANDU designs have several emergency cooling systems, as well as having limited self-pumping capability through thermal means (the steam generator is well above the reactor). Even in the event of a catastrophic accident and core meltdown, it is important to remember that the fuel is not critical in light water. This means that cooling the core with water from nearby sources will not add to the reactivity of the fuel mass.

Normally the rate of fission is controlled by light-water compartments called liquid zone controllers, which absorb excess neutrons, and by adjuster rods which can be raised or lowered in the core to control the neutron flux. These are used for normal operation, allowing the controllers to adjust reactivity across the fuel mass as different portions would normally burn at different rates depending on their position. The adjuster rods can also be used to slow or stop criticality. Because these rods are inserted into the low-pressure calandria, not the high-pressure fuel tubes, they would not be "ejected" by steam, a design issue for many pressurized-water reactors.

There are two independent, fast-acting safety shutdown systems as well. Shutoff rods are held above the reactor by electromagnets, and drop under gravity into the core to quickly end criticality. This system works even in the event of a complete power failure, as the electromagnets only hold the rods out of the reactor when power is available. A secondary system injects a high-pressure gadolinium nitrate neutron absorber solution into the calandria.

A heavy water design can sustain a chain reaction with a lower concentration of fissile atoms than light water reactors, allowing it to use some alternative fuels; for example, "recovered uranium" (RU) from used LWR fuel. CANDU was designed for natural uranium with only 0.7% U-235, so RU with 0.9% U-235 is a rich fuel. This extracts a further 30–40% energy from the uranium. The DUPIC ("Direct Use of spent PWR fuel In CANDU") process under development can recycle it even without reprocessing. The fuel is sintered in air (oxidized), then in hydrogen (reduced) to break it into a powder, which is then formed into CANDU fuel pellets.
CANDU can also breed fuel from the more abundant thorium. This is being investigated by India to take advantage of its natural thorium reserves.

Even better than LWRs, CANDU can utilize a mix of uranium and plutonium oxides (MOX fuel), the plutonium either from dismantled nuclear weapons or reprocessed reactor fuel. The mix of isotopes in reprocessed plutonium is not attractive for weapons, but can be used as fuel (instead of being simply nuclear waste), while burning weapons-grade plutonium eliminates a proliferation hazard. If the aim is explicitly to burn plutonium or other actinides from spent fuel, then special inert-matrix fuels are proposed to do this more efficiently than MOX. Since they contain no uranium, these fuels do not breed any extra plutonium.

The neutron economy of heavy water moderation and precise control of on-line refueling allow CANDU to use a great range of fuels other than enriched uranium, e.g., natural uranium, reprocessed uranium, thorium, plutonium, and used LWR fuel. Given the expense of enrichment, this can make fuel much cheaper. There is an initial investment into the tonnes of 99.75% pure heavy water to fill the core and heat transfer system. In the case of the Darlington plant costs released as part of a freedom of information act request put the overnight cost of the plant (four reactors totalling 3,512 MWe net capacity) at $5.117 billion CAD (about $4.2 billion USD at early 1990s exchange rates). Total capital costs including interest were $14.319 billion CAD (about $11.9 billion USD) with the heavy water accounting for $1.528 billion, or 11%, of this.

Since heavy water is less efficient at slowing neutrons, CANDU needs a larger moderator to fuel ratio and a larger core for the same power output. Although a calandria-based core is cheaper to build, its size increases the cost for standard features like the containment building. Generally nuclear plant construction and operations are ≈65% of overall lifetime cost; for CANDU costs are dominated by construction even more. Fueling CANDU is cheaper than other reactors, costing only ≈10% of the total, so the overall price per kWh electricity is comparable. The next-generation Advanced CANDU Reactor (ACR) mitigates these disadvantages by having light water coolant and using a more compact core with less moderator.

When first introduced, CANDUs offered much better capacity factor (ratio of power generated to what would be generated by running at full power, 100% of the time) than LWRs of a similar generation. The light-water designs spent, on average, about half the time being refueled or maintained. Since the 1980s dramatic improvements in LWR outage management have narrowed the gap, with several units achieving capacity factors ~90% and higher, with an overall fleet performance of 92% in 2010. The latest-generation CANDU 6 reactors have an 88–90% CF, but overall performance is dominated by the older Canadian units with CFs on the order of 80%. Refurbished units have demonstrated poor performance to date, on the order of 65%.

Some CANDU plants suffered from cost overruns during construction, often from external factors such as government action. For instance, a number of imposed construction delays led to roughly a doubling of the cost of the Darlington Nuclear Generating Station near Toronto, Ontario. Technical problems and redesigns added about another billion to the resulting $14.4 billion price. In contrast, in 2002 two CANDU 6 reactors at Qinshan in China were completed on-schedule and on-budget, an achievement attributed to tight control over scope and schedule.

In terms of safeguards against nuclear weapons proliferation, CANDUs meet a similar level of international certification as other reactors. The plutonium for India's first nuclear detonation, Operation Smiling Buddha in 1974, was produced in a CIRUS reactor supplied by Canada and partially paid for by the Canadian government using heavy water supplied by the United States. In addition to its two PHWR reactors, India has some safeguarded pressurised heavy water reactors (PHWRs) based on the CANDU design, and two safeguarded light-water reactors supplied by the US. Plutonium has been extracted from the spent fuel from all of these reactors; 
India mainly relies on an Indian designed and built military reactor called Dhruva. The design is believed to be derived from the CIRUS reactor, with the Dhruva being scaled-up for more efficient plutonium production. It is this reactor which is thought to have produced the plutonium for India's more recent (1998) Operation Shakti nuclear tests.

Although heavy water is relatively immune to neutron capture, a small amount of the deuterium turns into tritium in this way. Tritium+deuterium mix undergoes nuclear fusion more easily than any other substance. Tritium can be used in both the "fusion boost" of a boosted fission weapon and the main fusion process of an H-bomb. In an H-bomb, it is usually created "in situ" by neutron irradiation of lithium-6.

Tritium is extracted from some CANDU plants in Canada, mainly to improve safety in case of heavy-water leakage. The gas is stockpiled and used in a variety of commercial products, notably "powerless" lighting systems and medical devices. In 1985 what was then Ontario Hydro sparked controversy in Ontario due to its plans to sell tritium to the U.S. The plan, by law, involved sales to non-military applications only, but some speculated that the exports could have freed American tritium for the U.S. nuclear weapons program. Future demands appear to outstrip production, in particular the demands of future generations of experimental fusion reactors like ITER. Currently between 1.5 and 2.1 kg of tritium are recovered yearly at the Darlington separation facility, of which a minor fraction is sold.

The 1998 Operation Shakti test series in India included one bomb of about 45 kt yield that India has publicly claimed was a hydrogen bomb. An offhand comment in the BARC publication "Heavy Water — Properties, Production and Analysis" appears to suggest that the tritium was extracted from the heavy water in the CANDU and PHWR reactors in commercial operation. "Janes Intelligence Review" quotes the Chairman of the Indian Atomic Energy Commission as admitting to the tritium extraction plant, but refusing to comment on its use. India is also capable of creating tritium more efficiently by irradiation of lithium-6 in reactors.

Tritium is a radioactive form of hydrogen (H-3), with a half-life of 12.3 years. 
It is produced in small amounts in nature (about 4 kg/year globally), by cosmic ray interactions in the upper atmosphere. 
Tritium is considered a weak radionuclide because of its low-energy radioactive emissions (beta particle energy up to 18.6 keV). 
The beta particles travel 6 mm in air and only penetrate skin up to 6 micrometers. The biological half-life of inhaled, ingested, or absorbed tritium is 10–12 days.

Tritium is generated in the fuel of all reactors; CANDU reactors generate tritium also in their coolant and moderator, due to neutron capture in heavy hydrogen. 
Some of this tritium escapes into containment and is generally recovered; a small percentage (about 1%) escapes containment and is considered a routine radioactive emission (also higher than from an LWR of comparable size). Responsible operation of a CANDU plant therefore includes monitoring tritium in the surrounding environment (and publishing the results).

In some CANDU reactors the tritium is periodically extracted. Typical emissions from CANDU plants in Canada are less than 1% of the national regulatory limit, which is based on International Commission on Radiological Protection (ICRP) guidelines (for example, the maximum permitted drinking water concentration for tritium in Canada, 7,000 Bq/L, corresponds to 1/10 of the ICRP's dose limit for members of the public). Tritium emissions from other CANDU plants are similarly low.

In general there is significant public controversy about radioactive emissions from nuclear power plants, and for CANDU plants one of the main concerns is tritium. In 2007 Greenpeace published a critique of tritium emissions from Canadian nuclear power plants by Ian Fairlie. This report was criticized by Richard Osborne.

The CANDU development effort has gone through four major stages over time. The first systems were experimental and prototype machines of limited power. These were replaced by a second generation of machines of 500 to 600 MWe (the CANDU6), a series of larger machines of 900 MWe, and finally developing into the CANDU9 and current ACR-1000 effort.

The first heavy water moderated design in Canada was the ZEEP, which started operation just after the end of World War II. ZEEP was joined by several other experimental machines, including the NRX in 1947 and NRU in 1957. These efforts led to the first CANDU-type reactor, the Nuclear Power Demonstration (NPD), in Rolphton, Ontario. It was intended as a proof-of-concept and rated for only 22 MWe, a very low power for a commercial power reactor. NPD produced the first nuclear-generated electricity in Canada, and ran successfully from 1962 to 1987.

The second CANDU was the Douglas Point reactor, a more powerful version rated at roughly 200 MWe and located near Kincardine, Ontario. It went into service in 1968, and ran until 1984. Uniquely among CANDU stations, Douglas Point had an oil-filled window with a view of the east reactor face, even when the reactor was operating. Douglas Point was originally planned to be a two-unit station, but the second unit was cancelled because of the success of the larger 515 MWe units at Pickering.

In parallel with the classic CANDU design, experimental variants were being developed. WR-1, located at the AECL's Whiteshell Laboratories in Pinawa, Manitoba, used vertical pressure tubes and organic oil as the primary coolant. The oil used has a higher boiling point than water, allowing the reactor to operate at higher temperatures and lower pressures than a conventional reactor. WR-1's outlet temperature was about 490 C compared to the CANDU 6's nominal 310 C, which means less cooling fluid is needed to remove the same amount of heat, resulting in a smaller and less expensive core. The higher temperatures also result in more efficient conversion to steam, and ultimately, electricity. WR-1 operated successfully for many years, and promised a significantly higher efficiency than water-cooled versions.

The successes at NPD and Douglas Point led to the decision to construct the first multi-unit station in Pickering, Ontario. Pickering A, consisting of Units 1 to 4, went into service in 1971. Pickering B with units 5 to 8 came online in 1983, giving a full-station capacity of 4,120 MWe. The station is very close to the city of Toronto, in order to reduce transmission costs.

A series of improvements to the basic Pickering design led to the CANDU 6 design, which first went into operation in the early 1980s. CANDU 6 was essentially a version of the Pickering power plant that was re-designed to be able to be built in single-reactor units. CANDU 6 was used in several installations outside Ontario, including the Gentilly-2 in Quebec, and Point Lepreau Nuclear Generating Station in New Brunswick. CANDU 6 forms the majority of foreign CANDU systems, including the designs exported to Argentina, Romania, China and South Korea. Only India operates a CANDU system that is not based on the CANDU 6 design.

The economics of nuclear power plants generally scale well with size. This improvement at larger sizes is offset by the sudden appearance of large quantities of power on the grid, which leads to a lowering of electricity prices through supply and demand effects. Predictions in the late 1960s suggested that growth in electricity demand would overwhelm these downward pricing pressures, leading most designers to introduce plants in the 1000 MWe range.

Pickering A was quickly followed by such an upscaling effort for the Bruce Nuclear Generating Station, constructed in stages between 1970 and 1987. It is the largest nuclear facility in North America, and second largest in the world (after Kashiwazaki-Kariwa in Japan), with eight reactors at around 800 MWe each, in total 6,232 MW (net) and 7,276 MW (gross). Another, smaller, upscaling led to the Darlington Nuclear Generating Station design, similar to the Bruce plant, but delivering about 880 MWe per reactor.

As was the case for the development of the Pickering design into the CANDU 6, the Bruce design was also developed into the similar CANDU 9. Like the CANDU 6, the CANDU 9 is essentially a re-packaging of the Bruce design so it can be built as a single-reactor unit. No CANDU 9 reactors have been built.

Through the 1980s and 90s the nuclear power market suffered a major crash, with few new plants being constructed in North America or Europe. Design work continued throughout, and new design concepts were introduced that dramatically improved safety, capital costs, economics and overall performance. These Generation III+ and Generation IV machines became a topic of considerable interest in the early 2000s as it appeared a nuclear renaissance was underway and large numbers of new reactors would be built over the next decade.

AECL had been working on a design known as the ACR-700, using elements of the latest versions of the CANDU 6 and CANDU 9, with a design power of 700 MWe. During the nuclear renaissance, the upscaling seen in the earlier years re-expressed itself, and the ACR-700 was developed into the 1200 MWe ACR-1000. ACR-1000 is the next-generation (officially, "Generation III+") CANDU technology which makes some significant modifications to the existing CANDU design.

The main change, and the most radical among the CANDU generations, is the use of pressurized light water as the coolant. This significantly reduces the cost of implementing the primary cooling loop, which no longer has to be filled with expensive heavy water. The ACR-1000 uses about 1/3rd the heavy water needed in earlier generation designs. It also eliminates tritium production in the coolant loop, the major source of tritium leaks in operational CANDU designs. The redesign also allows for a slightly negative void reactivity, a major design goal of all Gen III+ machines.

The design also requires the use of slightly enriched uranium, enriched by about 1 or 2%. The main reason for this is to increase the burn-up ratio, allowing bundles to remain in the reactor longer, so that only a third as much spent fuel is produced. This also has effects on operational costs and timetables, as the refuelling frequency is reduced. As is the case with earlier CANDU designs, the ACR-1000 also offers online refuelling.

Outside of the reactor, the ACR-1000 has a number of design changes that are expected to dramatically lower capital and operational costs. Primary among these changes is the design lifetime of 60 years, which dramatically lowers the price of the electricity generated over the lifetime of the plant. The design also has an expected capacity factor of 90%. Higher pressure steam generators and turbines improve efficiency downstream of the reactor.

Many of the operational design changes were also applied to the existing CANDU 6 to produce the Enhanced CANDU 6. Also known as CANDU 6e or EC 6, this was an evolutionary upgrade of the CANDU 6 design with a gross output of 740 MWe per unit. The reactors are designed with a lifetime of over fifty years, with a mid-life program to replace some of the key components e.g. the fuel channels. The projected average annual capacity factor is more than ninety percent. Improvements to construction techniques (including modular, open-top assembly) decrease construction costs. The CANDU 6e is designed to operate at power settings as low as 50%, allowing them to adjust to load demand much better than the previous designs.

By most measures, the CANDU is "the Ontario reactor". The system was developed almost entirely in Ontario, and only two experimental designs were built in other provinces. Of the 29 commercial CANDU reactors built, 22 are in Ontario. Of these 22, a number of reactors have been removed from service. Two new CANDU reactors have been proposed for Darlington with Canadian government help with financing, but these plans ended in 2009 due to high costs.

AECL has heavily marketed CANDU within Canada, but has found a limited reception. To date, only two non-experimental reactors have been built in other provinces, one each in Quebec and New Brunswick, other provinces have concentrated on hydro and coal-fired plants. Several Canadian provinces have developed large amounts of hydro power. Alberta and Saskatchewan do not have extensive hydro resources, and use mainly fossil fuels to generate electric power.

Interest has been expressed in Western Canada, where CANDU reactors are being considered as heat and electricity sources for the energy-intensive oil sands extraction process, which currently uses natural gas. Energy Alberta Corporation announced 27 August 2007 that they had applied for a licence to build a new nuclear plant at Lac Cardinal (30 km west of the town of Peace River, Alberta), with two ACR-1000 reactors going online in 2017 producing 2.2 gigawatts (electric). A 2007 parliamentary review suggested placing the development efforts on hold.

The company was later purchased by Bruce Power, who proposed expanding the plant to four units of a total 4.4 gigawatts. These plans were upset and Bruce later withdrew its application for the Lac Cardinal, proposing instead a new site about 60 km away. The plans are currently moribund after a wide consultation with the public demonstrated that while about of the population were open to reactors, were opposed.

During the 1970s, the international nuclear sales market was extremely competitive, with many national nuclear companies being supported by their governments' foreign embassies. In addition, the pace of construction in the United States had meant that cost overruns and delayed completion was generally over, and subsequent reactors would be cheaper. Canada, a relatively new player on the international market, had numerous disadvantages in these efforts. The CANDU was deliberately designed to reduce the need for very large machined parts, making it suitable for construction by countries without a major industrial base. Sales efforts have had their most success in countries that could not locally build designs from other firms.

In the late 1970s, AECL noted that each reactor sale would employ 3,600 Canadians and result in $300 million in balance-of-payments income. These sales efforts were aimed primarily at countries being run by dictatorships or similar, a fact that led to serious concerns in parliament. These efforts also led to a scandal when it was discovered millions of dollars had been given to foreign sales agents, with little or no record of who they were, or what they did to earn the money. This led to a Royal Canadian Mounted Police investigation after questions were raised about sales efforts in Argentina, and new regulations on full disclosure of fees for future sales.

CANDU's first success was the sale of early CANDU designs to India. In 1963, an agreement was signed for export of a 200 MWe power reactor based on the Douglas Point reactor. The success of the deal led to the 1966 sale of a second reactor of the same design. The first reactor, then known as RAPP-1 for "Rajasthan Atomic Power Project", began operation in 1972. A serious problem with cracking of the reactor's end shield led to the reactor being shut down for long periods, and the reactor was finally downrated to 100 MW. Construction of the RAPP-2 reactor was still underway when India detonated its first atomic bomb in 1974, leading to Canada ending nuclear dealings with the country. Part of the sales agreement was a technology transfer process. When Canada withdrew from development, India continued construction of CANDU-like plants across the country. By 2010, CANDU-based reactors were operational at the following sites: Kaiga (3), Kakrapar (2), Madras (2), Narora (2), Rajasthan (6), and Tarapur (2).

In Pakistan, the Karachi Nuclear Power Plant with a gross capacity of 137 MWe was built between 1966 and 1971.

In 1972, AECL submitted a design based on the Pickering plant to Argentina's Comision Nacional de Energia Atomica process, in partnership with the Italian company Italimpianti. High inflation during construction led to massive losses, and efforts to re-negotiate the deal were interrupted by the March 1976 coup led by General Videla. The Embalse Nuclear Power Station began commercial operation in January 1984. There have been ongoing negotiations to open more CANDU 6 reactors in the country, including a 2007 deal between Canada, China and Argentina, but to date no firm plans have been announced.

A licensing agreement with Romania was signed in 1977, selling the CANDU 6 design for $5 million per reactor for the first four reactors, and then $2 million each for the next twelve. In addition, Canadian companies would supply a varying amount of equipment for the reactors, about $100 million of the first reactor's $800 million price tag, and then falling over time. In 1980, Nicolae Ceaușescu asked for a modification to provide goods instead of cash, in exchange the amount of Canadian content was increased and a second reactor would be built with Canadian help. Economic troubles in the country worsened throughout the construction phase. The first reactor of the Cernavodă Nuclear Power Plant only came online in April 1996, a decade after its December 1985 predicted startup. Further loans were arranged for completion of the second reactor, which went online in November 2007.

In January 1975, a deal was announced for a single CANDU 6 reactor to be built in South Korea, now known as the Wolsong-1 Power Reactor. Construction started in 1977 and commercial operation began in April 1983. In December 1990 a further deal was announced for three additional units at the same site, which began operation in the period 1997–1999. South Korea also negotiated development and technology transfer deals with Westinghouse for their advanced System-80 reactor design, and all future development is based on locally built versions of this reactor.

In June 1998, construction started on a CANDU 6 reactor in Qinshan China Qinshan Nuclear Power Plant, as Phase III (units 4 and 5) of the planned 11 unit facility. Commercial operation began in December 2002 and July 2003, respectively. These are the first heavy water reactors in China. Qinshan is the first CANDU-6 project to use open-top reactor building construction, and the first project where commercial operation began earlier than the projected date.

CANDU Energy is continuing marketing efforts in China. In addition, China and Argentina have agreed a contract to build a 700 MWe Candu-6 derived reactor. Construction is planned to start in 2018 at Atucha.

The cost of electricity from any power plant can be calculated by roughly the same selection of factors: capital costs for construction or the payments on loans made to secure that capital, the cost of fuel on a per-watt-hour basis, and fixed and variable maintenance fees. In the case of nuclear power, one normally includes two additional costs, the cost of permanent waste disposal, and the cost of decommissioning the plant when its useful lifetime is over. Generally, the capital costs dominate the price of nuclear power, as the amount of power produced is so large that it overwhelms the cost of fuel and maintenance. The World Nuclear Association calculates that the cost of fuel, including all processing, accounts for less than one cent(USD) per kWh.

Information on economic performance on CANDU is somewhat lopsided; the majority of reactors are in Ontario, which is also the "most public" among the major CANDU operators, so their performance dominates the available information. Based on Ontario's record, the economic performance of the CANDU system is quite poor. Although much attention has been focussed on the problems with the Darlington plant, every CANDU design in Ontario went over budget by at least 25%, and average over 150% higher than estimated. Darlington was the worst, at 350% over budget, but this project was stopped in-progress thereby incurring additional interest charges during a period of high interest rates, which is a special situation that was not expected to repeat itself.

In the 1980s, the pressure tubes in the Pickering A reactors were replaced ahead of design life due to unexpected deterioration caused by hydrogen embrittlement. Extensive inspection and maintenance has avoided this problem in later reactors.

All the Pickering A and Bruce A reactors were shut down in 1999 in order to focus on restoring operational performance in the later generations at Pickering, Bruce, and Darlington. Before restarting the Pickering A reactors, OPG undertook a limited refurbishment program. The original cost and time estimates based on inadequate project scope development were greatly below the actual time and cost and it was determined that Pickering units 2 and 3 would not be restarted for commercial reasons. Despite this refurbishment, the reactors have not performed well since the restart.

These overruns were repeated at Bruce, with Units 3 and 4 running 90% over budget. Similar overruns were experienced at Point Lepreau, and Gentilly-2 plant was shut down on December 28, 2012.

Based on the projected capital costs, and the low cost of fuel and in-service maintenance, in 1994 power from CANDU was predicted to be well under 5 cents/kWh. In 1998, Ontario Hydro calculated that the cost of generation from CANDU was 7.7 cents/kWh, whereas hydropower was only 1.1 cents, and their coal-fired plants were 4.3 cents. As Ontario Hydro received a regulated price averaging 6.3 cents/kWh for power in this period, the revenues from the other forms of generation were being used to fund the operating losses of the nuclear plants. The debt left over from the nuclear construction could not be included in the rate base until the reactors were declared in service, thereby exacerbating the total capital cost of construction with unpaid interest, at that time around $15 billion, and another $3.5 billion in debts throughout the system was held by a separate entity and repaid through a standing charge on electricity bills.

In 1999, Ontario Hydro was broken up and its generation facilities re-formed into Ontario Power Generation (OPG). In order to make the successor companies more attractive for private investors, $19.4 billion in "stranded debt" was placed in the control of the Ontario Electricity Financial Corporation. This debt is slowly paid down through a variety of sources, including a 0.7-cent/kWh tariff on all power, all income taxes paid by all operating companies, and all dividends paid by the OPG and Hydro One. Even with these sources of income, the amount of debt has grown on several occasions, and in 2010 stood at almost $15 billion. This is in spite of total payments on the order of $19 billion, ostensibly enough to have paid off the debt entirely if interest repayment requirements are ignored.

Darlington is currently in the process of considering a major re-build of several units, as it too is reaching its design mid-life time. The budget is currently estimated to be between $8.5 and $14 billion, and produce power at 6 to 8 cents/kWh. This prediction is based on three assumptions that appear to have never been met in operation: that the rebuild will be completed on-budget, that the system will operate at an average capacity utilization of 82%, and that the Ontario taxpayer will pay 100% of any cost overruns. Although Darlington Units 1, 3 and 4 have operated with an average lifetime annual capacity factor of 85% and Unit 2 with a capacity factor of 78%, refurbished units at Pickering and Bruce have lifetime capacity factors between 59 and 69%. This includes periods of several years while the units were shut down for the retubing and refurbishing. In 2009, Bruce A Units 3 and 4 had capacity factors of 80.5% and 76.7%, respectively, in a year when they had a major Vacuum Building outage.

Today there are 29 CANDU reactors in use around the world, and 13 "CANDU-derivatives" in India, developed from the CANDU design. After India detonated a nuclear bomb in 1974, Canada stopped nuclear dealings with India. The breakdown is:




</doc>
<doc id="7477" url="https://en.wikipedia.org/wiki?curid=7477" title="Cuitláhuac">
Cuitláhuac

Cuitláhuac (, ) (c. 1476 – 1520) or Cuitláhuac (in Spanish orthography; , , honorific form Cuitlahuatzin) was the 10th "tlatoani" (ruler) of the Aztec city of Tenochtitlan for 80 days during the year Two Flint (1520).

Cuitláhuac was the eleventh son of the ruler Axayacatl and a younger brother of Moctezuma II, the previous ruler of Tenochtitlan. His mother's father, also called Cuitlahuac, had been ruler of Iztapalapa, and the younger Cuitláhuac also ruled there initially.

Cuitláhuac was made "tlatoani" of Tenochtitlan during the Spanish conquest of Mexico; After Pedro de Alvarado had ordered the Massacre in the Great Temple, the Aztecs were very upset and started to fight and put a siege to the Spaniards. Hernán Cortés ordered Moctezuma to ask his people to stop fighting. Moctezuma told him that they would not listen to him and suggested Cortés free Cuitláhuac so that he could convince them to dispose of their arms and not fight anymore. Cortés then freed Cuitláhuac and once Cuitláhuac was free he led his people against the conquistadors. He succeeded and the Spaniards were driven out of Tenochtitlan on June 30, 1520. Cuitláhuac was ritually married to Moctezuma's eldest daughter, a ten- or eleven-year-old girl who later was called Isabel Moctezuma.

After having ruled for just 80 days, Cuitláhuac died of smallpox that had been introduced to the New World by the Europeans. His elder brother Matlatzincatzin, who had been "cihuacoatl" ("president"), resigned upon Cuitláhuac's death. As soon as Cuitláhuac died, Cuauhtémoc was made the next "tlatoani".

The modern Mexican municipality of Cuitláhuac, Veracruz and the Mexico City Metro station Metro Cuitláhuac are named in honor of Cuitláhuac. The asteroid 2275 Cuitláhuac is also named after this ruler.

There is an Avenue in Mexico City Called Cuitláhuac (Eje 3 Norte) that runs from Avenue Insurgentes to Avenue Mexico-Tacuba and that is part of an inner ring; also many streets in other towns and villages in Mexico are so called.


 


</doc>
<doc id="7478" url="https://en.wikipedia.org/wiki?curid=7478" title="Cuauhtémoc">
Cuauhtémoc

Cuauhtémoc (, also known as Cuauhtemotzin, Guatimozin or Guatemoc; c. 1495) was the Aztec ruler ("tlatoani") of Tenochtitlan from 1520 to 1521, making him the last Aztec Emperor. The name Cuauhtemōc means "one who has descended like an eagle", and is commonly rendered in English as "Descending Eagle", as in the moment when an eagle folds its wings and plummets down to strike its prey. This is a name that implies aggressiveness and determination.

Cuauhtémoc took power in 1520 as successor of Cuitláhuac and was a cousin of the late emperor Moctezuma II. His young wife, who was later known as Isabel Moctezuma, was one of Moctezuma's daughters. He ascended to the throne when he was around 25 years old, while Tenochtitlan was being besieged by the Spanish and devastated by an epidemic of smallpox brought to the New World by the invaders. After the killings in the Great Temple, there were probably few Aztec captains available to take the position.

Cuauhtemoc's date of birth is unknown, as he does not enter the historical record until he became emperor. He was the eldest legitimate son of Emperor Ahuitzotl and may well have attended the last New Fire ceremony, marking the beginning of a new 52-year cycle in the Aztec calendar. Like the rest of Cuauhtemoc's early biography, that is inferred from knowledge of his age, and the likely events and life path of someone of his rank. Following education in the calmecac, the school for elite boys, and then his military service, he was named ruler of Tlatelolco, with the title "cuauhtlatoani" ("eagle ruler") in 1515. To have reached this position of rulership, Cuauhtemoc had to be a male of high birth and a warrior who had captured enemies for sacrifice.

When Cuauhtemoc was elected tlatoani in 1520, Tenochtitlan had already been rocked by the invasion of the Spanish and their indigenous allies, the death of Moctezuma II, and the death of Moctezuma's brother Cuitlahuac, who succeeded him as ruler, but died of smallpox shortly afterwards. In keeping with traditional practice, the most able candidate among the high noblemen was chosen by vote of the highest noblemen, Cuauhtemoc assumed the rulership. Although under Cuitlahuac Tenochtitlan began mounting a defense against the invaders, it was increasingly isolated militarily and largely faced the crisis alone, as the numbers of Spanish allies increased with the desertion of many polities previously under its control.
Cuauhtémoc called for reinforcements from the countryside to aid the defense of Tenochtitlán, after eighty days of warfare against the Spanish. Of all the Nahuas, only Tlatelolcas remained loyal, and the surviving Tenochcas looked for refuge in Tlatelolco, where even women took part in the battle. Cuauhtémoc was captured on August 13, 1521, while fleeing Tenochtitlán by crossing Lake Texcoco with his wife, family, and friends.

He surrendered to Hernán Cortés along with the surviving "pipiltin" (nobles) and, according to Spanish sources, he asked Cortés to take his knife and "strike me dead immediately". According to the same Spanish accounts, Cortés refused the offer and treated his foe magnanimously. "You have defended your capital like a brave warrior," he declared. "A Spaniard knows how to respect valor, even in an enemy."

At Cuauhtémoc's request, Cortés also allowed the defeated Mexica to depart the city unmolested. Subsequently, however, when the booty found did not measure up to the Spaniards' expectations, Cuauhtémoc was subjected to "torture by fire", whereby the soles of his bare feet were slowly broiled over red-hot coals, in an unsuccessful attempt to discover its whereabouts. On the statue to Cuauhtemoc, on the Paseo de la Reforma in Mexico City, there is a bas relief showing the Spaniards' torture of the emperor. Eventually, some gold was recovered but far less than Cortés and his men expected.

Cuauhtémoc continued to hold his position under the Spanish, keeping the title of tlatoani, but he was no longer the sovereign ruler. He ordered the construction of a renaissance-style two-storied stone palace in Tlatelolco, in which he settled after the destruction of Mexico City; the building survived and was known as the Tecpan or palace.

In 1525, Cortés took Cuauhtémoc and several other indigenous nobles on his expedition to Honduras, as he feared that Cuauhtémoc could have led an insurrection in his absence. While the expedition was stopped in the Chontal Maya capital of Itzamkanac, known as Acalan in Nahuatl, Cortés had Cuauhtémoc executed for allegedly conspiring to kill him and the other Spaniards.
There are a number of discrepancies in the various accounts of the event. According to Cortés himself, on 27 February 1525, he learned from a citizen of Tenochtitlan, Mexicalcingo, that Cuauhtémoc, Coanacoch (the ruler of Texcoco), and Tetlepanquetzal, the ruler of Tlacopan, were plotting his death. Cortés interrogated them until each confessed and then had Cuauhtémoc, Tetlepanquetzal, and another lord, Tlacatlec, hanged. Cortés wrote that the other lords would be too frightened to plot against him again, as they believed he had uncovered the plan through magic powers. Cortés's account is supported by the historian Francisco López de Gómara.

According to Bernal Díaz del Castillo, a conquistador serving under Cortés who recorded his experiences in his book "The True History of the Conquest of New Spain", the supposed plot was revealed by two men, named Tapia and Juan Velásquez. Díaz portrays the executions as unjust and based on no evidence, and he admits to having liked Cuauhtémoc personally. He also records Cuauhtémoc giving the following speech to Cortés through his interpreter Malinche:

Díaz wrote that afterwards, Cortés suffered from insomnia because of guilt and badly injured himself while he was wandering at night.

Fernando de Alva Cortés Ixtlilxóchitl, a mestizo historian and descendant of Coanacoch, wrote an account of the executions in the 17th century partly based on Texcocan oral tradition. According to Ixtlilxóchitl, the three lords were joking cheerfully with one another because of a rumor that Cortés had decided to return the expedition to Mexico, when Cortés asked a spy to tell him what they were talking about. The spy reported honestly, but Cortés invented the plot himself. Cuauhtémoc, Coanacoch, and Tetlepanquetzal were hanged as well as eight others. However, Cortés cut down Coanacoch, the last to be hanged, after his brother began rallying his warriors. Coanacoch did not have long to enjoy his reprieve, as Ixtlilxóchitl wrote that he died a few days later.

Tlacotzin, Cuauhtémoc's "cihuacoatl", was appointed his successor as "tlatoani". He died the next year before he could return to Tenochtitlan.

The modern-day town of Ixcateopan in the state of Guerrero is home to an ossuary purportedly containing Cuauhtémoc's remains. Archeologist Eulalia Guzmán, a "passionate indigenista", excavated the bones in 1949, which were discovered shortly after bones of Cortés, found in Mexico City, had been authenticated by the Instituto Nacional de Antropología e Historia (INAH). Initially, Mexican scholars congratulated Guzmán, but after a similar examination by scholars at INAH, their authenticity as Cuauhtemoc's was rejected, as the bones in the ossuary belonged to several different persons, several of them seemingly women. The finding caused a public uproar. A panel assembled by Guzmán gave support to the initial contention. The Secretariat of Public Education (SEP) had another panel examine the bones, which gave support to INAH's original finding, but did not report on the finding publicly. A scholarly study of the controversy was published in 2011 and argued that the available data suggests that the grave is an elaborate hoax prepared by a local of Ichcateopan as a way of generating publicity, and that subsequently supported by Mexican nationalists such as Guzman who wished to use the find for political purposes.

Cuauhtemoc is the embodiment of indigenist nationalism in Mexico, being the only Aztec emperor who survived the conquest by the Spanish Empire (and their native allies). He is honored by a monument on the Paseo de la Reforma, his face has appeared on Mexican banknotes, and he is celebrated in paintings, music, and popular culture.

Many places in Mexico are named in honour of Cuauhtémoc. These include Ciudad Cuauhtémoc in Chihuahua and the Cuauhtémoc borough of the Mexican Federal District, as well as Ciudad Cuauhtémoc, in the state of Veracruz.

There is a Cuauhtémoc station on Line 1 of the Mexico City metro as well as one for Moctezuma, but none for Hernán Cortés. There is also a metro station in Monterrey named after him.

Cuauhtémoc is also one of the few non-Spanish given names for Mexican boys that is perennially popular.
Cuauhtémoc Cárdenas Solórzano, a prominent Mexican politician, is named after him. In the Aztec campaign of the PC game "", the player plays as Cuauhtémoc, despite the name "Montezuma" for the campaign itself, and Cuauhtémoc narrates the openings and closings to each scenario. In the next installment to the series, "", Cuauhtémoc was the leader of Aztecs. The Mexican football player Cuauhtémoc Blanco was also named after him.

In the 1996 Rage Against The Machine single "People of the Sun", lyricist Zack De La Rocha rhymes "When the fifth sun sets get back reclaimed, The spirit of Cuauhtémoc alive and untamed".

Cuauhtémoc, in the name Guatemoc, is portrayed sympathetically in the adventure novel "Montezuma's Daughter", by H. Rider Haggard. First appearing in Chapter XIV, he becomes friends with the protagonist after they save each other's lives. His coronation, torture, and death are described in the novel.


 


</doc>
<doc id="7480" url="https://en.wikipedia.org/wiki?curid=7480" title="Cross section (physics)">
Cross section (physics)

When two particles interact, their mutual cross section is the area transverse to their relative motion within which they must meet in order to scatter from each other. If the particles are hard inelastic spheres that interact only upon contact, their scattering cross section is related to their geometric size. If the particles interact through some action-at-a-distance force, such as electromagnetism or gravity, their scattering cross section is generally larger than their geometric size. When a cross section is specified as a function of some final-state variable, such as particle angle or energy, it is called a differential cross section. When a cross section is integrated over all scattering angles (and possibly other variables), it is called a total cross section. Cross sections are typically denoted (sigma) and measured in units of area.

Scattering cross sections may be defined in nuclear, atomic, and particle physics for collisions of accelerated beams of one type of particle with targets (either stationary or moving) of a second type of particle. The probability for any given reaction to occur is in proportion to its cross section. Thus, specifying the cross section for a given reaction is a proxy for stating the probability that a given scattering process will occur.

The measured reaction rate of a given process depends strongly on experimental variables such as the density of the target material, the intensity of the beam, the detection efficiency of the apparatus, or the angle setting of the detection apparatus. However, these quantities can be factored away, allowing measurement of the underlying two-particle collisional cross section.

Differential and total scattering cross sections are among the most important measurable quantities in nuclear, atomic, and particle physics.

In a gas of finite-sized particles there are collisions among particles that depend on their cross-sectional size. The average distance that a particle travels between collisions depends on the density of gas particles. These quantities are related by

where

If the particles in the gas can be treated as hard spheres of radius that interact by direct contact, as illustrated in Figure 1, then the effective cross section for the collision of a pair is

If the particles in the gas interact by a force with a larger range than their physical size, then the cross section is a larger effective area that may depend on a variety of variables such as the energy of the particles.

Cross sections can be computed for atomic collisions but also are used in the subatomic realm. For example, in nuclear physics a "gas" of low-energy neutrons collides with nuclei in a reactor or other nuclear device, with a cross section that is energy-dependent and hence also with well-defined mean free path between collisions.

If a beam of particles enters a thin layer of material of thickness , the flux of the beam will decrease by according to

where is the total cross section of "all" events, including scattering, absorption, or transformation to another species. The number density of scattering centers is designated by . Solving this equation exhibits the exponential attenuation of the beam intensity:

where is the initial flux, and is the total thickness of the material. For light, this is called the Beer–Lambert law.

Consider a classical measurement where a single particle is scattered off a single stationary target particle. Conventionally, a Spherical coordinate system is used, with the target placed at the origin and the axis of this coordinate system aligned with the incident beam. The angle is the scattering angle, measured between the incident beam and the scattered beam, and the is the azimuthal angle.

The impact parameter is the perpendicular offset of the trajectory of the incoming particle, and the outgoing particle emerges at an angle . For a given interaction (Coulombic, magnetic, gravitational, contact, etc.), the impact parameter and the scattering angle have a definite one-to-one functional dependence on each other. Generally the impact parameter can neither be controlled nor measured from event to event and is assumed to take all possible values when averaging over many scattering events. The differential size of the cross section is the area element in the plane of the impact parameter, i.e. . The differential angular range of the scattered particle at angle is the solid angle element . The differential cross section is the quotient of these quantities, .

It is a function of the scattering angle (and therefore also the impact parameter), plus other observables such as the momentum of the incoming particle. The differential cross section is always taken to be positive, even though larger impact parameters generally produce less deflection. In cylindrically symmetric situations (about the beam axis), the azimuthal angle is not changed by the scattering process, and the differential cross section can be written as

In situations where the scattering process is not azimuthally symmetric, such as when the beam or target particles possess magnetic moments oriented perpendicular to the beam axis, the differential cross section must also be expressed as a function of the azimuthal angle.

For scattering of particles of incident flux off a stationary target consisting of many particles, the differential cross section at an angle is related to the flux of scattered particle detection in particles per unit time by

Here is the finite angular size of the detector (SI unit: sr), is the number density of the target particles (SI units: m), and is the thickness of the stationary target (SI units: m). This formula assumes that the target is thin enough that each beam particle will interact with at most one target particle.

The total cross section may be recovered by integrating the differential cross section over the full solid angle ( steradians):

It is common to omit the “differential” qualifier when the type of cross section can be inferred from context. In this case, may be referred to as the "integral cross section" or "total cross section". The latter term may be confusing in contexts where multiple events are involved, since “total” can also refer to the sum of cross sections over all events.

The differential cross section is extremely useful quantity in many fields of physics, as measuring it can reveal a great amount of information about the internal structure of the target particles. For example, the differential cross section of Rutherford scattering provided strong evidence for the existence of the atomic nucleus.

Instead of the solid angle, the momentum transfer may be used as the independent variable of differential cross sections.

Differential cross sections in inelastic scattering contain resonance peaks that indicate the creation of metastable states and contain information about their energy and lifetime.

In the time-independent formalism of quantum scattering, the initial wave function (before scattering) is taken to be a plane wave with definite momentum :

where and are the "relative" coordinates between the projectile and the target. The arrow indicates that this only describes the "asymptotic behavior" of the wave function when the projectile and target are too far apart for the interaction to have any effect.

After the scattering takes place, it is expected that the wave function takes on the following asymptotic form:

where is some function of the angular coordinates known as the scattering amplitude. This general form is valid for any short-ranged, energy-conserving interaction. It is not true for long-ranged interactions, so there are additional complications when dealing with electromagnetic interactions.

The full wave function of the system behaves asymptotically as the sum

The differential cross section is related to the scattering amplitude:

This has the simple interpretation as the probability density for finding the scattered projectile at a given angle.

A cross section is therefore a measure of the effective surface area seen by the impinging particles, and as such is expressed in units of area. The cross section of two particles (i.e. observed when the two particles are colliding with each other) is a measure of the interaction event between the two particles. The cross section is proportional to the probability that an interaction will occur; for example in a simple scattering experiment the number of particles scattered per unit of time (current of scattered particles ) depends only on the number of incident particles per unit of time (current of incident particles ), the characteristics of target (for example the number of particles per unit of surface ), and the type of interaction. For we have

If the reduced masses and momenta of the colliding system are , and , before and after the collision respectively, the differential cross section is given by

where the on-shell matrix is defined by

in terms of the S-matrix. Here is the Dirac delta function. The computation of the S-matrix is the main goal of the scattering theory.

Although the SI unit of total cross sections is m, smaller units are usually used in practice.

In nuclear and particle physics, the conventional unit is the barn b, where 1 b = 10 m = 100 fm. Smaller prefixed units such as mb and μb are also widely used. Correspondingly, the differential cross section can be measured in units such as mb/sr.

When the scattered radiation is visible light, it is conventional to measure the path length in centimetres. To avoid the need for conversion factors, the scattering cross section is expressed in cm, and the number concentration in cm. The measurement of the scattering of visible light is known as nephelometry, and is effective for particles of 2–50 µm in diameter: as such, it is widely used in meteorology and in the measurement of atmospheric pollution.

The scattering of X-rays can also be described in terms of scattering cross sections, in which case the square ångström is a convenient unit: 1 Å = 10 m = = 10 b.

For light, as in other settings, the scattering cross section is generally different from the geometrical cross section of a particle, and it depends upon the wavelength of light and the permittivity, shape and size of the particle. The total amount of scattering in a sparse medium is proportional to the product of the scattering cross section and the number of particles present.

In terms of area, the "total cross section" () is the sum of the cross sections due to absorption, scattering and luminescence. The sum of the absorption and scattering cross sections is sometimes referred to as the extinction cross section.
The total cross section is related to the absorbance of the light intensity through the Beer–Lambert law, which says that absorbance is proportional to particle concentration:
where is the absorbance at a given wavelength , is the particle concentration as a number density, and is the path length. The absorbance of the radiation is the logarithm (decadic or, more usually, natural) of the reciprocal of the transmittance :

In the context of scattering light on extended bodies, the scattering cross section, , describes the likelihood of light being scattered by a macroscopic particle. In general, the scattering cross section is different from the geometrical cross section of a particle, as it depends upon the wavelength of light and the permittivity in addition to the shape and size of the particle. The total amount of scattering in a sparse medium is determined by the product of the scattering cross section and the number of particles present. In terms of area, the "total cross section" () is the sum of the cross sections due to absorption, scattering and luminescence:

The total cross section is related to the absorbance of the light intensity through the Beer–Lambert law, which says that absorbance is proportional to concentration: , where is the absorbance at a given wavelength , is the concentration as a number density, and is the path length. The extinction or absorbance of the radiation is the logarithm (decadic or, more usually, natural) of the reciprocal of the transmittance :

There is no simple relationship between the scattering cross section and the physical size of the particles, as the scattering cross section depends on the wavelength of radiation used. This can be seen when driving in foggy weather: the droplets of water (which form the fog) scatter red light less than they scatter the shorter wavelengths present in white light, and the red rear fog light can be distinguished more clearly than the white headlights of an approaching vehicle. That is to say that the scattering cross section of the water droplets is smaller for red light than for light of shorter wavelengths, even though the physical size of the particles is the same.

The scattering cross section is related to the meteorological range :

The quantity is sometimes denoted , the scattering coefficient per unit length.

The elastic collision of two hard spheres is an instructive example that demonstrates the sense of calling this quantity a cross section. and are respectively the radii of the scattering center and scattered sphere, the impact parameter, and the polar angle of the exit trajectory as above.
The total cross section is

So in this case the total scattering cross section is equal to the area of the circle (with radius ) within which the center of mass of the incoming sphere has to arrive for it to be deflected, and outside which it passes by the stationary scattering center.

Another example illustrates the details of the calculation of a simple light scattering model obtained by a reduction of the dimension. For simplicity, we will consider the scattering of a beam of light on a plane treated as a uniform density of parallel rays and within the framework of geometrical optics from a circle with radius with a perfectly reflecting boundary. Its three-dimensional equivalent is therefore the more difficult problem of a laser or flashlight light scattering from the mirror sphere, for example, from the mechanical bearing ball. The unit of cross section in one dimension is the unit of length, for example 1 m. Let be the angle between the light ray and the radius joining the reflection point of the light ray with the center point of the circle mirror. Then the increase of the length element perpendicular to the light beam is expressed by this angle as
the reflection angle of this ray with respect to the incoming ray is then , and the scattering angle is
The energy or the number of photons reflected from the light beam with the intensity or density of photons on the length is 
The differential cross section is therefore ()
As it is seen from the behaviour of the sine function, this quantity has the maximum for the backward scattering (; the light is reflected perpendicularly and returns), and the zero minimum for the scattering from the edge of the circle directly forward (). It confirms the intuitive expectations that the mirror circle acts like a diverging lens, and a thin beam is more diluted the closer it is from the edge defined with respect to the incoming direction. The total cross section can be obtained by summing (integrating) the differential section of the entire range of angles:
so it is equal as much as the circular mirror is totally screening the two-dimensional space for the beam of light. In three dimensions for the mirror ball with the radius it is therefore equal .

We can now use the result from the Example 2 to calculate the differential cross section for the light scattering from the perfectly reflecting sphere in three dimensions. Let us denote now the radius of the sphere as . Let us parametrize the plane perpendicular to the incoming light beam by the cylindrical coordinates and . In any plane of the incoming and the reflected ray we can write now from the previous example:
while the impact area element is 
Using the relation for the solid angle in the spherical coordinates:
and the trigonometric identity
we obtain
while the total cross section as we expected is
As one can see, it also agrees with the result from the Example 1 if the photon is assumed to be a rigid sphere of zero radius.

Notes

Sources




</doc>
<doc id="7482" url="https://en.wikipedia.org/wiki?curid=7482" title="Christian mythology">
Christian mythology

Christian mythology is the body of myths associated with Christianity. The term encompasses a broad variety of stories and legends. Various authors have used it to refer to the mythological and allegorical elements found in the Bible, such as the story of the Leviathan. The term has been applied to myths and legends from the Middle Ages, such as the story of Saint George and the Dragon, the stories of King Arthur and his Knights of the Round Table, and the legends of the "Parsival". Multiple commentators have classified John Milton's epic poem "Paradise Lost" as a work of "Christian mythology". The term has also been applied to modern stories revolving around Christian themes and motifs, such as the writings of C. S. Lewis, J. R. R. Tolkien, Madeleine L'Engle, and George MacDonald.

Mythological themes and elements occur throughout Christian literature, including recurring myths such as ascending to a mountain, the "axis mundi", myths of combat, descent into the Underworld, accounts of a dying-and-rising god, flood stories, stories about the founding of a tribe or city, and myths about great heroes (or saints) of the past, paradises, and self-sacrifice.

In ancient Greek, "muthos", from which the English word "myth" derives, meant "story, narrative." Early Christians contrasted their sacred stories with "myths", by which they meant false and pagan stories.

Several modern Christian writers, such as C.S. Lewis, have described elements of Christianity, particularly the story of Christ, as "myth" which is also "true" ("true myth"). Others object to associating Christianity with "myth" for a variety of reasons: the association of the term "myth" with polytheism, the use of the term "myth" to indicate falsehood or non-historicity, and the lack of an agreed-upon definition of "myth".

As examples of Biblical myths, Every cites the creation account in Genesis 1 and 2 and the story of Eve's temptation. Many Christians believe parts of the Bible to be symbolic or metaphorical (such as the Creation in Genesis).

Mythic patterns such as the primordial struggle between good and evil appear in passages throughout the Hebrew Bible, including passages that describe historical events. A distinctive characteristic of the Hebrew Bible is the reinterpretation of myth on the basis of history, as in the Book of Daniel, a record of the experience of the Jews of the Second Temple period under foreign rule, presented as a prophecy of future events and expressed in terms of "mythic structures" with "the Hellenistic kingdom figured as a terrifying monster that cannot but recall [the Near Eastern pagan myth of] the dragon of chaos".

Mircea Eliade argues that the imagery used in some parts of the Hebrew Bible reflects a "transfiguration of history into myth". For example, Eliade says, the portrayal of Nebuchadnezzar as a dragon in Jeremiah 51:34 is a case in which the Hebrews "interpreted contemporary events by means of the very ancient cosmogonico-heroic myth" of a battle between a hero and a dragon.

According to scholars including Neil Forsyth and John L. McKenzie, the Old Testament incorporates stories, or fragments of stories, from extra-biblical mythology. According to the "New American Bible", a Catholic Bible translation produced by the Confraternity of Christian Doctrine, the story of the Nephilim in Genesis 6:1-4 "is apparently a fragment of an old legend that had borrowed much from ancient mythology", and the "sons of God" mentioned in that passage are "celestial beings of mythology". The "New American Bible" also says that Psalm 93 alludes to "an ancient myth" in which God battles a personified Sea. Some scholars have identified the biblical creature Leviathan as a monster from Canaanite mythology. According to Howard Schwartz, "the myth of the fall of Lucifer" existed in fragmentary form in Isaiah 14:12 and other ancient Jewish literature; Schwartz claims that the myth originated from "the ancient Canaanite myth of Athtar, who attempted to rule the throne of Ba'al, but was forced to descend and rule the underworld instead".

Some scholars have argued that the calm, orderly, monotheistic creation story in Genesis 1 can be interpreted as a reaction against the creation myths of other Near Eastern cultures. In connection with this interpretation, David and Margaret Leeming describe Genesis 1 as a "demythologized myth", and John L. McKenzie asserts that the writer of Genesis 1 has "excised the mythical elements" from his creation story.

Perhaps the most famous topic in the Bible that could possibly be connected with mythical origins is the topic of Heaven (or the sky) as the place where God (or angels, or the saints) resides, with stories such as the ascension of Elijah (who disappeared in the sky), war of man with an angel, flying angels. Even in the New Testament Saint Paul is said to "have visited the third heaven", and Jesus was portrayed in several books as going to return from Heaven on a cloud, in the same way he ascended thereto. The official text repeated by the attendees during Roman Catholic mass (the Apostles' Creed) contains the words "He ascended into Heaven, and is Seated at the Right Hand of God, The Father. From thence He will come again to judge the living and the dead". Medieval cosmology adapted its view of the Cosmos to conform with these scriptures, in the concept of celestial spheres (later attacked, amongst others, by Giordano Bruno). Some famous opponents of religion, including John Lennon and Stephen Hawking, mentioned this in their public works.

According to a number of scholars, the Christ story contains mythical themes such as descent to the underworld, the heroic monomyth, and the "dying god" (see section below on "mythical themes and types").

Some scholars have argued that the Book of Revelation incorporates imagery from ancient mythology. According to the "New American Bible", the image in Revelation 12:1-6 of a pregnant woman in the sky, threatened by a dragon, "corresponds to a widespread myth throughout the ancient world that a goddess pregnant with a savior was pursued by a horrible monster; by miraculous intervention, she bore a son who then killed the monster". Bernard McGinn suggests that the image of the two Beasts in Revelation stems from a "mythological background" involving the figures of Leviathan and Behemoth.

The Pastoral Epistles contain denunciations of "myths" ("muthoi"). This may indicate that Rabbinic or Gnostic mythology was popular among the early Christians to whom the epistles were written and that the epistles' author was attempting to resist that mythology.

The Sibylline oracles contain predictions that the dead Roman Emperor Nero, infamous for his persecutions, would return one day as an Antichrist-like figure. According to Bernard McGinn, these parts of the oracles were probably written by a Christian and incorporated "mythological language" in describing Nero's return.

According to Mircea Eliade, the Middle Ages witnessed "an upwelling of mythical thought" in which each social group had its own "mythological traditions". Often a profession had its own "origin myth" which established models for members of the profession to imitate; for example, the knights tried to imitate Lancelot or Parsifal. The medieval trouveres developed a "mythology of woman and Love" which incorporated Christian elements but, in some cases, ran contrary to official church teaching.

George Every includes a discussion of medieval legends in his book "Christian Mythology". Some medieval legends elaborated upon the lives of Christian figures such as Christ, the Virgin Mary, and the saints. For example, a number of legends describe miraculous events surrounding Mary's birth and her marriage to Joseph.

In many cases, medieval mythology appears to have inherited elements from myths of pagan gods and heroes. According to Every, one example may be "the myth of St. George" and other stories about saints battling dragons, which were "modelled no doubt in many cases on older representations of the creator and preserver of the world in combat with chaos". Eliade notes that some "mythological traditions" of medieval knights, namely the Arthurian cycle and the Grail theme, combine a veneer of Christianity with traditions regarding the Celtic Otherworld. According to Lorena Laura Stookey, "many scholars" see a link between stories in "Irish-Celtic mythology" about journeys to the Otherworld in search of a cauldron of rejuvenation and medieval accounts of the quest for the Holy Grail.

According to Eliade, "eschatological myths" became prominent during the Middle Ages during "certain historical movements". These eschatological myths appeared "in the Crusades, in the movements of a Tanchelm and an Eudes de l'Etoile, in the elevation of Fredrick II to the rank of Messiah, and in many other collective messianic, utopian, and prerevolutionary phenomena". One significant eschatological myth, introduced by Gioacchino da Fiore's theology of history, was the "myth of an imminent third age that will renew and complete history" in a "reign of the Holy Spirit"; this "Gioacchinian myth" influenced a number of messianic movements that arose in the late Middle Ages.

During the Renaissance, there arose a critical attitude that sharply distinguished between apostolic tradition and what George Every calls "subsidiary mythology"—popular legends surrounding saints, relics, the cross, etc.—suppressing the latter.

The works of Renaissance writers often included and expanded upon Christian and non-Christian stories such as those of creation and the Fall. Rita Oleyar describes these writers as "on the whole, reverent and faithful to the primal myths, but filled with their own insights into the nature of God, man, and the universe". An example is John Milton's "Paradise Lost", an "epic elaboration of the Judeo-Christian mythology" and also a "veritable encyclopedia of myths from the Greek and Roman tradition".

According to Cynthia Stewart, during the Reformation, the Protestant reformers used "the founding myths of Christianity" to critique the church of their time.

Every argues that "the disparagement of myth in our own civilization" stems partly from objections to perceived idolatry, objections which intensified in the Reformation, both among Protestants and among Catholics reacting against the classical mythology revived during the Renaissance.

The philosophes of the Enlightenment used criticism of myth as a vehicle for veiled criticisms of the Bible and the church. According to Bruce Lincoln, the philosophes "made irrationality the hallmark of myth and constituted philosophy—rather than the Christian "kerygma"—as the antidote for mythic discourse. By implication, Christianity could appear as a more recent, powerful, and dangerous instance of irrational myth".

Some commentators have categorized a number of modern fantasy works as "Christian myth" or "Christian mythopoeia". Examples include the fiction of C.S. Lewis, Madeleine L'Engle, J.R.R. Tolkien, and George MacDonald.

In "The Eternal Adam and the New World Garden", written in 1968, David W. Noble argued that the Adam figure had been "the central myth in the American novel since 1830". As examples, he cites the works of Cooper, Hawthorne, Melville, Twain, Hemingway, and Faulkner.

According to Lorena Laura Stookey, many myths feature sacred mountains as "the sites of revelations": "In myth, the ascent of the holy mountain is a spiritual journey, promising purification, insight, wisdom, or knowledge of the sacred". As examples of this theme, Stookey includes the revelation of the Ten Commandments on Mount Sinai, Christ's ascent of a mountain to deliver his Sermon on the Mount, and Christ's ascension into Heaven from the Mount of Olives.

Many mythologies involve a "world center", which is often the sacred place of creation; this center often takes the form of a tree, mountain, or other upright object, which serves as an "axis mundi" or axle of the world. A number of scholars have connected the Christian story of the crucifixion at Golgotha with this theme of a cosmic center. In his "Creation Myths of the World", David Leeming argues that, in the Christian story of the crucifixion, the cross serves as "the "axis mundi", the center of a new creation".

According to a tradition preserved in Eastern Christian folklore, Golgotha was the summit of the cosmic mountain at the center of the world and the location where Adam had been both created and buried. According to this tradition, when Christ is crucified, his blood falls on Adam's skull, buried at the foot of the cross, and redeems him. George Every discusses the connection between the cosmic center and Golgotha in his book "Christian Mythology", noting that the image of Adam's skull beneath the cross appears in many medieval representations of the crucifixion.

In "Creation Myths of the World", Leeming suggests that the Garden of Eden may also be considered a world center.

Many Near Eastern religions include a story about a battle between a divine being and a dragon or other monster representing chaos—a theme found, for example, in the "Enuma Elish". A number of scholars call this story the "combat myth". A number of scholars have argued that the ancient Israelites incorporated the combat myth into their religious imagery, such as the figures of Leviathan and Rahab, the Song of the Sea, Isaiah 51:9-10's description of God's deliverance of his people from Babylon, and the portrayals of enemies such as Pharaoh and Nebuchadnezzar. The idea of Satan as God's opponent may have developed under the influence of the combat myth. Scholars have also suggested that the Book of Revelation uses combat myth imagery in its descriptions of cosmic conflict.

According to Christian tradition, Christ descended to hell after his death, in order to free the souls there; this event is known as the Harrowing of Hell. This story is narrated in the Gospel of Nicodemus and may be the meaning behind 1 Peter 3:18-22. According to David Leeming, writing in "The Oxford Companion to World Mythology", the harrowing of hell is an example of the motif of the hero's descent to the underworld, which is common in many mythologies.

Many myths, particularly from the Near East, feature a god who dies and is resurrected; this figure is sometimes called the "dying god". An important study of this figure is James George Frazer's "The Golden Bough", which traces the dying god theme through a large number of myths. The dying god is often associated with fertility. A number of scholars, including Frazer, have suggested that the Christ story is an example of the "dying god" theme. In the article "Dying god" in "The Oxford Companion to World Mythology", David Leeming notes that Christ can be seen as bringing fertility, though of a spiritual as opposed to physical kind.

In his 2006 homily for Corpus Christi, Pope Benedict XVI noted the similarity between the Christian story of the resurrection and pagan myths of dead and resurrected gods: "In these myths, the soul of the human person, in a certain way, reached out toward that God made man, who, humiliated unto death on a cross, in this way opened the door of life to all of us."

Many cultures have myths about a flood that cleanses the world in preparation for rebirth. Such stories appear on every inhabited continent on earth. An example is the biblical story of Noah. In "The Oxford Companion to World Mythology", David Leeming notes that, in the Bible story, as in other flood myths, the flood marks a new beginning and a second chance for creation and humanity.

According to Sandra Frankiel, the records of "Jesus' life and death, his acts and words" provide the "founding myths" of Christianity. Frankiel claims that these founding myths are "structurally equivalent" to the creation myths in other religions, because they are "the pivot around which the religion turns to and which it returns", establishing the "meaning" of the religion and the "essential Christian practices and attitudes". Tom Cain uses the expression "founding myths" more broadly, to encompass such stories as those of the War in Heaven and the fall of man; according to Cain, "the disastrous consequences of disobedience" is a pervasive theme in Christian founding myths.

In his influential 1909 work "Der Mythus von der Geburt des Helden" (The Myth of the Birth of the Hero), Otto Rank argued that the births of many mythical heroes follow a common pattern. Rank includes the story of Christ's birth as a representative example of this pattern.

According to Mircea Eliade, one pervasive mythical theme associates heroes with the slaying of dragons, a theme which Eliade traces back to "the very ancient cosmogonico-heroic myth" of a battle between a divine hero and a dragon. He cites the Christian legend of Saint George as an example of this theme. An example from the later Middle Ages comes from Dieudonné de Gozon, third Grand Master of the Knights of Rhodes, famous for slaying the dragon of Malpasso. Eliade writes:
"Legend, as was natural, bestowed upon him the attributes of St. George, famed for his victorious fight with the monster. […] In other words, by the simple fact that he was regarded as a hero, de Gozon was identified with a category, an archetype, which […] equipped him with a mythical biography from which it was "impossible" to omit combat with a reptilian monster."
In the "Oxford Companion to World Mythology" David Leeming lists Moses, Jesus, and King Arthur as examples of the "heroic monomyth", calling the Christ story "a particularly complete example of the heroic monomyth". Leeming regards resurrection as a common part of the heroic monomyth, in which the resurrected heroes often become sources of "material or spiritual food for their people"; in this connection, Leeming notes that Christians regard Jesus as the "bread of life".

In terms of values, Leeming contrasts "the myth of Jesus" with the myths of other "Christian heroes such as St. George, Roland, el Cid, and even King Arthur"; the later hero myths, Leeming argues, reflect the survival of pre-Christian heroic values—"values of military dominance and cultural differentiation and hegemony"—more than the values expressed in the Christ story.

Many religious and mythological systems contain myths about a paradise. Many of these myths involve the loss of a paradise that existed at the beginning of the world. Some scholars have seen in the story of the Garden of Eden an instance of this general motif.

Sacrifice is an element in many religious traditions and often represented in myths. In "The Oxford Companion to World Mythology", David Leeming lists the story of Abraham and Isaac and the story of Christ's death as examples of this theme. Wendy Doniger describes the gospel accounts as a "meta-myth" in which Jesus realizes that he is part of a "new myth [...] of a man who is sacrificed in hate" but "sees the inner myth, the old myth of origins and acceptance, the myth of a god who sacrifices himself in love".

According to Mircea Eliade, many traditional societies have a cyclic sense of time, periodically reenacting mythical events. Through this reenactment, these societies achieve an "eternal return" to the mythical age. According to Eliade, Christianity retains a sense of cyclical time, through the ritual commemoration of Christ's life and the imitation of Christ's actions; Eliade calls this sense of cyclical time a "mythical aspect" of Christianity.

However, Judeo-Christian thought also makes an "innovation of the first importance", Eliade says, because it embraces the notion of linear, historical time; in Christianity, "time is no longer [only] the circular Time of the Eternal Return; it has become linear and irreversible Time". Summarizing Eliade's statements on this subject, Eric Rust writes, "A new religious structure became available. In the Judaeo-Christian religions—Judaism, Christianity, Islam—history is taken seriously, and linear time is accepted. [...] The Christian myth gives such time a beginning in creation, a center in the Christ-event, and an end in the final consummation."

Heinrich Zimmer also notes Christianity's emphasis on linear time; he attributes this emphasis specifically to the influence of Saint Augustine's theory of history. Zimmer does not explicitly describe the cyclical conception of time as itself "mythical" per se, although he notes that this conception "underl[ies] Hindu mythology".

Neil Forsyth writes that "what distinguishes both Jewish and Christian religious systems [...] is that they elevate to the sacred status of myth narratives that are situated in historical time".

According to Carl Mitcham, "the Christian mythology of progress toward transcendent salvation" created the conditions for modern ideas of scientific and technological progress. Hayden White describes "the myth of Progress" as the "secular, Enlightenment counterpart" of "Christian myth". Reinhold Niebuhr described the modern idea of ethical and scientific progress as "really a rationalized version of the Christian myth of salvation".

According to Mircea Eliade, the medieval "Gioacchinian myth [...] of universal renovation in a more or less imminent future" has influenced a number of modern theories of history, such as those of Lessing (who explicitly compares his views to those of medieval "enthusiasts"), Fichte, Hegel, and Schelling; and has also influenced a number of Russian writers.

Calling Marxism "a truly messianic Judaeo-Christian ideology", Eliade writes that Marxism "takes up and carries on one of the great eschatological myths of the Middle Eastern and Mediterranean world, namely: the redemptive part to be played by the Just (the 'elect', the 'anointed', the 'innocent', the 'missioners', in our own days the proletariat), whose sufferings are invoked to change the ontological status of the world".

In his article "The Christian Mythology of Socialism", Will Herberg argues that socialism inherits the structure of its ideology from the influence of Christian mythology upon western thought.

In "The Oxford Companion to World Mythology", David Leeming claims that Judeo-Christian messianic ideas have influenced 20th-century totalitarian systems, citing the state ideology of the Soviet Union as an example.

According to Hugh S. Pyper, the biblical "founding myths of the Exodus and the exile, read as stories in which a nation is forged by maintaining its ideological and racial purity in the face of an oppressive great power", entered "the rhetoric of nationalism throughout European history", especially in Protestant countries and smaller nations.

See Secular Christmas stories, Christmas in the media, and Christmas in literature.



</doc>
<doc id="7484" url="https://en.wikipedia.org/wiki?curid=7484" title="Company (disambiguation)">
Company (disambiguation)

A company is a group of more than one persons to carry out an enterprise and so a form of business organization.

Company may also refer to:





</doc>
<doc id="7485" url="https://en.wikipedia.org/wiki?curid=7485" title="Corporation">
Corporation

A corporation is a company or group of people authorized to act as a single entity (legally a person) and recognized as such in law. Early incorporated entities were established by charter (i.e. by an "ad hoc" act granted by a monarch or passed by a parliament or legislature). Most jurisdictions now allow the creation of new corporations through registration. Corporations enjoy limited liability for their investors, which can lead to losses being externalized from investors to the government or general public. Corporations are usually the most profitable and powerful business entities, such as their public control, influence over government (including political candidates that support them), and ability to protect its interests and make huge profits.

Corporations come in many different types but are usually divided by the law of the jurisdiction where they are chartered into two kinds: by whether they can issue stock or not, or by whether they are formed to make a profit or not.

Where local law distinguishes corporations by the ability to issue stock, corporations allowed to do so are referred to as "stock corporations", ownership of the corporation is through stock, and owners of stock are referred to as "stockholders" or "shareholders". Corporations not allowed to issue stock are referred to as "non-stock" corporations; those who are considered the owners of a non-stock corporation are persons (or other entities) who have obtained membership in the corporation and are referred to as a "member" of the corporation.

Corporations chartered in regions where they are distinguished by whether they are allowed to be for profit or not are referred to as "for profit" and "not-for-profit" corporations, respectively.

There is some overlap between stock/non-stock and for-profit/not-for-profit in that not-for-profit corporations are always non-stock as well. A for-profit corporation is almost always a stock corporation, but some for-profit corporations may choose to be non-stock. To simplify the explanation, whenever "Stockholder" or "shareholder" is used in the rest of this article to refer to a stock corporation, it is presumed to mean the same as "member" for a non-profit corporation or for a profit, non-stock corporation.

Registered corporations have legal personality and are owned by shareholders whose liability is generally limited to their investment. Shareholders do not typically actively manage a corporation; shareholders instead elect or appoint a board of directors to control the corporation in a fiduciary capacity. In most circumstances, a shareholder may also serve as a director or officer of a corporation.

In American English, the word "corporation" is most often used to describe large business corporations. In British English and in the Commonwealth countries, the term "company" is more widely used to describe the same sort of entity while the word "corporation" encompasses all incorporated entities. In American English, the word "company" can include entities such as partnerships that would not be referred to as companies in British English as they are not a separate legal entity.

Despite not being individual human beings, corporations, as far as US law is concerned, are legal persons, and have many of the same rights and responsibilities as natural persons do. For example, a corporation can own property, and can sue or be sued. Corporations can exercise human rights against real individuals and the state, and they can themselves be responsible for human rights violations. Corporations can be "dissolved" either by statutory operation, order of court, or voluntary action on the part of shareholders. Insolvency may result in a form of corporate failure, when creditors force the liquidation and dissolution of the corporation under court order, but it most often results in a restructuring of corporate holdings. Corporations can even be convicted of criminal offenses, such as fraud and manslaughter. However, corporations are not considered living entities in the way that humans are.

Late in the 19th century, a new form of company having the limited liability protections of a corporation, and the more favorable tax treatment of either a sole proprietorship or partnership was developed. While not a corporation, this new type of entity became very attractive as an alternative for corporations not needing to issue stock. In Germany, the organization was referred to as "Gesellschaft mit beschränkter Haftung" or "GmbH". In the last quarter of the 20th Century this new form of non-corporate organization became available in the United States and other countries, and was known as the "limited liability company" or "LLC". Since the GmbH and LLC forms of organization are technically not corporations (even though they have many of the same features), they will not be discussed in this article.

The word "corporation" derives from "corpus", the Latin word for body, or a "body of people". By the time of Justinian (reigned 527–565), Roman law recognized a range of corporate entities under the names "universitas", "corpus" or "collegium". These included the state itself (the "Populus Romanus"), municipalities, and such private associations as sponsors of a religious cult, burial clubs, political groups, and guilds of craftsmen or traders. Such bodies commonly had the right to own property and make contracts, to receive gifts and legacies, to sue and be sued, and, in general, to perform legal acts through representatives. Private associations were granted designated privileges and liberties by the emperor.

Entities which carried on business and were the subjects of legal rights were found in ancient Rome, and the Maurya Empire in ancient India. In medieval Europe, churches became incorporated, as did local governments, such as the Pope and the City of London Corporation. The point was that the incorporation would survive longer than the lives of any particular member, existing in perpetuity. The alleged oldest commercial corporation in the world, the Stora Kopparberg mining community in Falun, Sweden, obtained a charter from King Magnus Eriksson in 1347.

In medieval times, traders would do business through common law constructs, such as partnerships. Whenever people acted together with a view to profit, the law deemed that a partnership arose. Early guilds and livery companies were also often involved in the regulation of competition between traders.

The progenitors of the modern corporation were the chartered companies, such as the Dutch East India Company (VOC) and the Hudson's Bay Company, which were created to lead the colonial ventures of European nations in the 17th century. Acting under a charter sanctioned by the Dutch government, the Dutch East India Company defeated Portuguese forces and established itself in the Moluccan Islands in order to profit from the European demand for spices. Investors in the VOC have issued paper certificates as proof of share ownership, and were able to trade their shares on the original Amsterdam Stock Exchange. Shareholders are also explicitly granted limited liability in the company's royal charter.

In England, the government created corporations under a royal charter or an Act of Parliament with the grant of a monopoly over a specified territory. The best-known example, established in 1600, was the East India Company of London. Queen Elizabeth I granted it the exclusive right to trade with all countries to the east of the Cape of Good Hope. Some corporations at this time would act on the government's behalf, bringing in revenue from its exploits abroad. Subsequently, the Company became increasingly integrated with English and later British military and colonial policy, just as most corporations were essentially dependent on the Royal Navy's ability to control trade routes.

Labeled by both contemporaries and historians as "the grandest society of merchants in the universe", the English East India Company would come to symbolize the dazzlingly rich potential of the corporation, as well as new methods of business that could be both brutal and exploitative. On 31 December 1600, Queen Elizabeth I granted the company a 15-year monopoly on trade to and from the East Indies and Africa. By 1711, shareholders in the East India Company were earning a return on their investment of almost 150 per cent. Subsequent stock offerings demonstrated just how lucrative the Company had become. Its first stock offering in 1713–1716 raised £418,000, its second in 1717–1722 raised £1.6 million.
A similar chartered company, the South Sea Company, was established in 1711 to trade in the Spanish South American colonies, but met with less success. The South Sea Company's monopoly rights were supposedly backed by the Treaty of Utrecht, signed in 1713 as a settlement following the War of the Spanish Succession, which gave Great Britain an "asiento" to trade in the region for thirty years. In fact the Spanish remained hostile and let only one ship a year enter. Unaware of the problems, investors in Britain, enticed by extravagant promises of profit from company promoters bought thousands of shares. By 1717, the South Sea Company was so wealthy (still having done no real business) that it assumed the public debt of the British government. This accelerated the inflation of the share price further, as did the Bubble Act 1720, which (possibly with the motive of protecting the South Sea Company from competition) prohibited the establishment of any companies without a Royal Charter. The share price rose so rapidly that people began buying shares merely in order to sell them at a higher price, which in turn led to higher share prices. This was the first speculative bubble the country had seen, but by the end of 1720, the bubble had "burst", and the share price sank from £1000 to under £100. As bankruptcies and recriminations ricocheted through government and high society, the mood against corporations, and errant directors was bitter.

In the late 18th century, Stewart Kyd, the author of the first treatise on corporate law in English, defined a corporation as:

Due to the late 18th century abandonment of mercantilist economic theory and the rise of classical liberalism and laissez-faire economic theory due to a revolution in economics led by Adam Smith and other economists, corporations transitioned from being government or guild affiliated entities to being public and private economic entities free of governmental directions.

Adam Smith wrote in his 1776 work "The Wealth of Nations" that mass corporate activity could not match private entrepreneurship, because people in charge of others' money would not exercise as much care as they would with their own.

The British Bubble Act 1720's prohibition on establishing companies remained in force until its repeal in 1825. By this point, the Industrial Revolution had gathered pace, pressing for legal change to facilitate business activity. The repeal was the beginning of a gradual lifting on restrictions, though business ventures (such as those chronicled by Charles Dickens in "Martin Chuzzlewit") under primitive companies legislation were often scams. Without cohesive regulation, proverbial operations like the "Anglo-Bengalee Disinterested Loan and Life Assurance Company" were undercapitalised ventures promising no hope of success except for richly paid promoters.

The process of incorporation was possible only through a royal charter or a private act and was limited, owing to Parliament's jealous protection of the privileges and advantages thereby granted. As a result, many businesses came to be operated as unincorporated associations with possibly thousands of members. Any consequent litigation had to be carried out in the joint names of all the members and was almost impossibly cumbersome. Though Parliament would sometimes grant a private act to allow an individual to represent the whole in legal proceedings, this was a narrow and necessarily costly expedient, allowed only to established companies.

Then, in 1843, William Gladstone became the chairman of a Parliamentary Committee on Joint Stock Companies, which led to the Joint Stock Companies Act 1844, regarded as the first modern piece of company law. The Act created the Registrar of Joint Stock Companies, empowered to register companies by a two-stage process. The first, provisional, stage cost £5 and did not confer corporate status, which arose after completing the second stage for another £5. For the first time in history, it was possible for ordinary people through a simple registration procedure to incorporate. The advantage of establishing a company as a separate legal person was mainly administrative, as a unified entity under which the rights and duties of all investors and managers could be channeled.

However, there was still no limited liability and company members could still be held responsible for unlimited losses by the company. The next, crucial development, then, was the Limited Liability Act 1855, passed at the behest of the then Vice President of the Board of Trade, Mr. Robert Lowe. This allowed investors to limit their liability in the event of business failure to the amount they invested in the company – shareholders were still liable directly to creditors, but just for the unpaid portion of their shares. (The principle that shareholders are liable to the corporation had been introduced in the Joint Stock Companies Act 1844).

The 1855 Act allowed limited liability to companies of more than 25 members (shareholders). Insurance companies were excluded from the act, though it was standard practice for insurance contracts to exclude action against individual members. Limited liability for insurance companies was allowed by the Companies Act 1862.

This prompted the English periodical "The Economist" to write in 1855 that "never, perhaps, was a change so vehemently and generally demanded, of which the importance was so much overrated. " The major error of this judgment was recognised by the same magazine more than 70 years later, when it claimed that, "[t]he economic historian of the future. . . may be inclined to assign to the nameless inventor of the principle of limited liability, as applied to trading corporations, a place of honour with Watt and Stephenson, and other pioneers of the Industrial Revolution. "

These two features – a simple registration procedure and limited liability – were subsequently codified into the landmark 1856 Joint Stock Companies Act. This was subsequently consolidated with a number of other statutes in the Companies Act 1862, which remained in force for the rest of the century, up to and including the time of the decision in "Salomon v A Salomon & Co Ltd".

The legislation shortly gave way to a railway boom, and from then, the numbers of companies formed soared. In the later nineteenth century, depression took hold, and just as company numbers had boomed, many began to implode and fall into insolvency. Much strong academic, legislative and judicial opinion was opposed to the notion that businessmen could escape accountability for their role in the failing businesses.

In 1892, Germany introduced the Gesellschaft mit beschränkter Haftung with a separate legal personality and limited liability even if all the shares of the company were held by only one person. This inspired other countries to introduce corporations of this kind.

The last significant development in the history of companies was the 1897 decision of the House of Lords in "Salomon v. Salomon & Co." where the House of Lords confirmed the separate legal personality of the company, and that the liabilities of the company were separate and distinct from those of its owners.

In the United States, forming a corporation usually required an act of legislation until the late 19th century. Many private firms, such as Carnegie's steel company and Rockefeller's Standard Oil, avoided the corporate model for this reason (as a trust). State governments began to adopt more permissive corporate laws from the early 19th century, although these were all restrictive in design, often with the intention of preventing corporations for gaining too much wealth and power.

New Jersey was the first state to adopt an "enabling" corporate law, with the goal of attracting more business to the state, in 1896. In 1899, Delaware followed New Jersey's lead with the enactment of an enabling corporate statute, but Delaware only became the leading corporate state after the enabling provisions of the 1896 New Jersey corporate law were repealed in 1913.

The end of the 19th century saw the emergence of holding companies and corporate mergers creating larger corporations with dispersed shareholders. Countries began enacting anti-trust laws to prevent anti-competitive practices and corporations were granted more legal rights and protections.
The 20th century saw a proliferation of laws allowing for the creation of corporations by registration across the world, which helped to drive economic booms in many countries before and after World War I. Another major post World War I shift was toward the development of conglomerates, in which large corporations purchased smaller corporations to expand their industrial base.

Starting in the 1980s, many countries with large state-owned corporations moved toward privatization, the selling of publicly owned (or 'nationalised') services and enterprises to corporations. Deregulation (reducing the regulation of corporate activity) often accompanied privatization as part of a laissez-faire policy.

A corporation is, at least in theory, owned and controlled by its members. In a joint-stock company the members are known as shareholders and each of their shares in the ownership, control, and profits of the corporation is determined by the portion of shares in the company that they own. Thus a person who owns a quarter of the shares of a joint-stock company owns a quarter of the company, is entitled to a quarter of the profit (or at least a quarter of the profit given to shareholders as dividends) and has a quarter of the votes capable of being cast at general meetings.

In another kind of corporation, the legal document which established the corporation or which contains its current rules will determine who the corporation's members are. Who a member is depends on what kind of corporation is involved. In a worker cooperative, the members are people who work for the cooperative. In a credit union, the members are people who have accounts with the credit union.

The day-to-day activities of a corporation are typically controlled by individuals appointed by the members. In some cases, this will be a single individual but more commonly corporations are controlled by a committee or by committees. Broadly speaking, there are two kinds of committee structure.

Historically, corporations were created by a charter granted by government. Today, corporations are usually registered with the state, province, or national government and regulated by the laws enacted by that government. Registration is the main prerequisite to the corporation's assumption of limited liability. The law sometimes requires the corporation to designate its principal address, as well as a registered agent (a person or company designated to receive legal service of process). It may also be required to designate an agent or other legal representative of the corporation.

Generally, a corporation files articles of incorporation with the government, laying out the general nature of the corporation, the amount of stock it is authorized to issue, and the names and addresses of directors. Once the articles are approved, the corporation's directors meet to create bylaws that govern the internal functions of the corporation, such as meeting procedures and officer positions.

The law of the jurisdiction in which a corporation operates will regulate most of its internal activities, as well as its finances. If a corporation operates outside its home state, it is often required to register with other governments as a foreign corporation, and is almost always subject to laws of its host state pertaining to employment, crimes, contracts, civil actions, and the like.

Corporations generally have a distinct name. Historically, some corporations were named after their membership: for instance, "The President and Fellows of Harvard College". Nowadays, corporations in most jurisdictions have a distinct name that does not need to make reference to their membership. In Canada, this possibility is taken to its logical extreme: many smaller Canadian corporations have no names at all, merely numbers based on a registration number (for example, "12345678 Ontario Limited"), which is assigned by the provincial or territorial government where the corporation incorporates.

In most countries, corporate names include a term or an abbreviation that denotes the corporate status of the entity (for example, "Incorporated" or "Inc." in the United States) or the limited liability of its members (for example, "Limited" or "Ltd."). These terms vary by jurisdiction and language. In some jurisdictions, they are mandatory, and in others they are not. Their use puts everybody on constructive notice that they are dealing with an entity whose liability is limited: one can only collect from whatever assets the entity still controls when one obtains a judgment against it.

Some jurisdictions do not allow the use of the word "company" alone to denote corporate status, since the word "company" may refer to a partnership or some other form of collective ownership (in the United States it can be used by a sole proprietorship but this is not generally the case elsewhere).




</doc>
<doc id="7487" url="https://en.wikipedia.org/wiki?curid=7487" title="Fairchild Channel F">
Fairchild Channel F

The Fairchild Channel F is a home video game console released by Fairchild Semiconductor in November 1976 across North America at the retail price of $169.95. It was also released in Japan in October the following year. It has the distinction of being the first programmable ROM cartridge–based video game console, and the first console to use a microprocessor. It was launched as the Video Entertainment System, or VES, but when Atari released its VCS the next year, Fairchild renamed its machine. By 1977, the Fairchild Channel F had sold 250,000 units, trailing behind sales of the VCS.

The Channel F electronics were designed by Jerry Lawson using the Fairchild F8 CPU, the first public outing of this processor. The F8 was very complex compared to the typical integrated circuits of the day, and had more inputs and outputs than other contemporary chips. Because chip packaging was not available with enough pins, the F8 was instead fabricated as a pair of chips that had to be used together to form a complete CPU.

Lawson worked with Nick Talesfore and Ron Smith. As manager of Industrial Design, Talesfore was responsible for the design of the hand controllers, console, and video game cartridges. Smith was responsible for the mechanical engineering of the video cartridges and controllers. All worked for Wilf Corigan, head of Fairchild Semiconductor, a division of Fairchild Camera & Instrument.

One feature unique to this console is the 'hold' button, which allowed the player to freeze the game, change the time or change the speed of the game. In the original unit, sound is played through an internal speaker, rather than the TV set. However, the System II passed sound to the television through the RF modulator.

The controllers are a joystick without a base; the main body is a large hand grip with a triangular "cap" on top, the top being the portion that actually moved for eight-way directional control. It could be used as both a joystick and paddle (twist), and not only could it be pushed down to operate as a fire button it could be pulled up as well. The model 1 unit contained a small compartment for storing the controllers when moving it. The System II featured detachable controllers and had two holders at the back to wind the cable around and to store the controller in. Zircon later offered a special control which featured an action button on the front of the joystick. It was marketed by Zircon as "Channel F Jet-Stick" in a letter sent out to registered owners before Christmas 1982.

Despite the failure of the Channel F, the joystick's design was so popular—"Creative Computing" called it "outstanding"— that Zircon also released an Atari joystick port-compatible version, the Video Command Joystick, first released without the extra fire button. Before that, only the downwards plunge motion was connected and acted as the fire button; the pull-up and twist actions weren't connected to anything.

Twenty-seven cartridges, termed 'Videocarts', were officially released to consumers in the United States during the ownership of Fairchild and Zircon, the first twenty-one of which were released by Fairchild. Several of these cartridges were capable of playing more than one game and were typically priced at $19.95. The Videocarts were yellow and approximately the size and overall texture of an 8 track cartridge. They usually featured colorful label artwork. The earlier artwork was created by nationally known artist Tom Kamifuji and art directed by Nick Talesfore. The console contained two built-in games, Tennis and Hockey, which were both advanced "Pong" clones. In Hockey the reflecting bar could be changed to diagonals by twisting the controller, and could move all over the playing field. Tennis was much like the original Pong.

A sales brochure from 1978 listed 'Keyboard Videocarts' for sale. The three shown were "K-1 Casino Poker", "K-2 Space Odyssey", and "K-3 Pro-Football". These were intended to use the Keyboard accessory. All further brochures, released after Zircon took over Fairchild, never listed this accessory nor anything called a Keyboard Videocart.

There was one additional cartridge released numbered Videocart-51 and simply titled 'Demo 1'. This Videocart was shown in a single sales brochure released shortly after Zircon acquired the company. It was never listed for sale after this single brochure which was used in the winter of 1979.

Carts listed (as mentioned above) but never released:

Official carts that also exist:

German SABA also released a few compatible carts different from the original carts, translation in Videocart 1 Tic-Tac-Toe to German words, Videocart 3 released with different abbreviations (German), Videocart 18 changed graphics and German word list and the SABA 20, a chess game released only by SABA.

A homebrew clone of "Pac-Man" for the Channel F was released in 2009.

Ken Uston reviewed 32 games in his book "Ken Uston's Guide to Buying and Beating the Home Video Games" in 1982, and rated some of the Channel F's titles highly; of these, "Alien Invasion" and "Video Whizball" were considered by Uston to be "the finest adult cartridges currently available for the Fairchild Channel F System." The games on a whole, however, rated last on his survey of over 200 games for the Atari, Intellivision, Astrocade and Odyssey consoles, and contemporary games were rated "Average" with future Channel F games rated "below average". Uston rated almost one half of the Channel F games as "high in interest" and called that "an impressive proportion" and further noted that "Some of the Channel F cartridges are timeless; no matter what technological developments occur, they will continue to be of interest." His overall conclusion was that the games "serve a limited, but useful, purpose" and that the "strength of the Channel F offering is in its excellent educational line for children."

In 1983, after Zircon announced its discontinuation of the Channel F, "Video Games" reviewed the console. Calling it "the system nobody knows", the magazine described its graphics and sounds as "somewhat primitive by today's standards". It described "Space War" as perhaps "the most antiquated game of its type still on the market", and rated the 25 games for the console with an average grade of three ("not too good") on a scale from one to ten. The magazine stated, however, that Fairchild "managed to create some fascinating games, even by today's standards", calling "Casino Royale" ("Video Blackjack") "the best card game, from blackjack to bridge, made for "any" TV-game system". It also favorably reviewed "Dodge-It" ("simple but great"), "Robot War" ("Berzerk without guns"), and "Whizball" ("thoroughly original ... hockey "with" guns"), but concluded that only those interested in nostalgia, video game collecting, or card games would purchase the Channel F in 1983.

Original Channel F technical specifications:

Some time in 1979, Zircon International bought the rights to the Channel F and released the re-designed console as the Channel F System II to compete with Atari's VCS. This re-designed System II was completed by Nick Talesfore at Fairchild. He was the same industrial designer who designed the original game console. Only six new games were released after the debut of the second system before its demise, several of which were developed at Fairchild before they sold it off.

The major changes were in design, with the controllers removable from the base unit instead of being wired directly into it, the storage compartment was moved to the rear of the unit, and the sound was now mixed into the TV signal so the unit no longer needed a speaker. This version also featured a simpler and more modern-looking case design. However, by this time the market was in the midst of the first video game crash, and Fairchild eventually threw in the towel and left the market. A number of licensed versions were released in Europe, including the Luxor Video Entertainment System in Scandinavia (Sweden), Adman Grandstand in the UK, and the Saba Videoplay, Nordmende Teleplay and ITT Tele-Match Processor, from Germany and also Dumont Videoplay and Barco Challenger from the Barco/Dumont company in Italy and Belgium.




</doc>
<doc id="7489" url="https://en.wikipedia.org/wiki?curid=7489" title="Collation">
Collation

Collation is the assembly of written information into a standard order. Many systems of collation are based on numerical order or alphabetical order, or extensions and combinations thereof. Collation is a fundamental element of most office filing systems, library catalogs, and reference books.

Collation differs from "classification" in that classification is concerned with arranging information into logical categories, while collation is concerned with the ordering of items of information, usually based on the form of their identifiers. Formally speaking, a collation method typically defines a total order on a set of possible identifiers, called sort keys, which consequently produces a total preorder on the set of items of information (items with the same identifier are not placed in any defined order).

A collation algorithm such as the Unicode collation algorithm defines an order through the process of comparing two given character strings and deciding which should come before the other. When an order has been defined in this way, a "sorting algorithm" can be used to put a list of any number of items into that order.

The main advantage of collation is that it makes it fast and easy for a user to find an element in the list, or to confirm that it is absent from the list. In automatic systems this can be done using a binary search algorithm or interpolation search; manual searching may be performed using a roughly similar procedure, though this will often be done unconsciously. Other advantages are that one can easily find the first or last elements on the list (most likely to be useful in the case of numerically sorted data), or elements in a given range (useful again in the case of numerical data, and also with alphabetically ordered data when one may be sure of only the first few letters of the sought item or items).

Strings representing numbers may be sorted based on the values of the numbers that they represent. For example, "−4", "2.5", "10", "89", "30,000". Note that pure application of this method may provide only a partial ordering on the strings, since different strings can represent the same number (as with "2" and "2.0" or, when scientific notation is used, "2e3" and "2000").

A similar approach may be taken with strings representing dates or other items that can be ordered chronologically or in some other natural fashion.

Alphabetical order is the basis for many systems of collation where items of information are identified by strings consisting principally of letters from an alphabet. The ordering of the strings relies on the existence of a standard ordering for the letters of the alphabet in question. (The system is not limited to alphabets in the strict technical sense; languages that use a syllabary or abugida, for example Cherokee, can use the same ordering principle provided there is a set ordering for the symbols used.)

To decide which of two strings comes first in alphabetical order, initially their first letters are compared. The string whose first letter appears earlier in the alphabet comes first in alphabetical order. If the first letters are the same, then the second letters are compared, and so on, until the order is decided. (If one string runs out of letters to compare, then it is deemed to come first; for example, "cart" comes before "carthorse".) The result of arranging a set of strings in alphabetical order is that words with the same first letter are grouped together, and within such a group words with the same first two letters are grouped together, and so on.

Capital letters are typically treated as equivalent to their corresponding lowercase letters. (For alternative treatments in computerized systems, see Automated collation, below.)

Certain limitations, complications, and special conventions may apply when alphabetical order is used:

In several languages the rules have changed over time, and so older dictionaries may use a different order than modern ones. Furthermore, collation may depend on use. For example, German dictionaries and telephone directories use different approaches.

Another form of collation is radical-and-stroke sorting, used for non-alphabetic writing systems such as the hanzi of Chinese and the kanji of Japanese, whose thousands of symbols defy ordering by convention. In this system, common components of characters are identified; these are called radicals in Chinese and logographic systems derived from Chinese. Characters are then grouped by their primary radical, then ordered by number of pen strokes within radicals. When there is no obvious radical or more than one radical, convention governs which is used for collation. For example, the Chinese character 妈 (meaning "mother") is sorted as a six-stroke character under the three-stroke primary radical 女.

The radical-and-stroke system is cumbersome compared to an alphabetical system in which there are a few characters, all unambiguous. The choice of which components of a logograph comprise separate radicals and which radical is primary is not clear-cut. As a result, logographic languages often supplement radical-and-stroke ordering with alphabetic sorting of a phonetic conversion of the logographs. For example, the kanji word "Tōkyō" (東京) can be sorted as if it were spelled out in the Japanese characters of the hiragana syllabary as "to-u-ki--u" (とうきょう), using the conventional sorting order for these characters.

In addition, in Greater China, surname stroke ordering is a convention in some official documents where people's names are listed without hierarchy.

The radical-and-stroke system, or some similar pattern-matching and stroke-counting method, was traditionally the only practical method for constructing dictionaries that someone could use to look up a logograph whose pronunciation was unknown. With the advent of computers, dictionary programs are now available that allow one to handwrite a character using a mouse or stylus.

When information is stored in digital systems, collation may become an automated process. It is then necessary to implement an appropriate collation algorithm that allows the information to be sorted in a satisfactory manner for the application in question. Often the aim will be to achieve an alphabetical or numerical ordering that follows the standard criteria as described in the preceding sections. However, not all of these criteria are easy to automate.

The simplest kind of automated collation is based on the numerical codes of the symbols in a character set, such as ASCII coding (or any of its supersets such as Unicode), with the symbols being ordered in increasing numerical order of their codes, and this ordering being extended to strings in accordance with the basic principles of alphabetical ordering (mathematically speaking, lexicographical ordering). So a computer program might treat the characters "a", "b", "C", "d", and "$" as being ordered "$", "C", "a", "b", "d" (the corresponding ASCII codes are "$" = 36, "a" = 97, "b" = 98, "C" = 67, and "d" = 100). Therefore, strings beginning with "C", "M", or "Z" would be sorted before strings with lower-case "a", "b", etc. This is sometimes called "ASCIIbetical order". This deviates from the standard alphabetical order, particularly due to the ordering of capital letters before all lower-case ones (and possibly the treatment of spaces and other non-letter characters). It is therefore often applied with certain alterations, the most obvious being case conversion (often to uppercase, for historical reasons) before comparison of ASCII values.

In many collation algorithms, the comparison is based not on the numerical codes of the characters, but with reference to the collating sequence – a sequence in which the characters are assumed to come for the purpose of collation – as well as other ordering rules appropriate to the given application. This can serve to apply the correct conventions used for alphabetical ordering in the language in question, dealing properly with differently cased letters, modified letters, digraphs, particular abbreviations, and so on, as mentioned above under Alphabetical order, and in detail in the Alphabetical order article. Such algorithms are potentially quite complex, possibly requiring several passes through the text.

Problems are nonetheless still common when the algorithm has to encompass more than one language. For example, in German dictionaries the word "ökonomisch" comes between "offenbar" and "olfaktorisch", while Turkish dictionaries treat "o" and "ö" as different letters, placing "oyun" before "öbür".

A standard algorithm for collating any collection of strings composed of any standard Unicode symbols is the Unicode Collation Algorithm. This can be adapted to use the appropriate collation sequence for a given language by tailoring its default collation table. Several such tailorings are collected in Common Locale Data Repository.

In some applications, the strings by which items are collated may differ from the identifiers that are displayed. For example, "The Shining" might be sorted as "Shining, The" (see Alphabetical order above), but it may still be desired to display it as "The Shining". In this case two sets of strings can be stored, one for display purposes, and another for collation purposes. Strings used for collation in this way are called "sort keys".

Sometimes, it is desired to order text with embedded numbers using proper numerical order. For example, "Figure 7b" goes before "Figure 11a", even though '7' comes after '1' in Unicode. This can be extended to Roman numerals. This behavior is not particularly difficult to produce as long as only integers are to be sorted, although it can slow down sorting significantly. For example, Microsoft Windows does this when sorting file names.

Sorting decimals properly is a bit more difficult, because different locales use different symbols for a decimal point, and sometimes the same character used as a decimal point is also used as a separator, for example "Section 3.2.5". There is no universal answer for how to sort such strings; any rules are application dependent.

Ascending order of numbers differs from alphabetical order, e.g. 11 comes alphabetically before 2. This can be fixed with leading zeros: 02 comes alphabetically before 11. See e.g. ISO 8601.

Also −13 comes alphabetically after −12 although it is less. With negative numbers, to make ascending order correspond with alphabetical sorting, more drastic measures are needed such as adding a constant to all numbers to make them all positive.

In some contexts, numbers and letters are used not so much as a basis for establishing an ordering, but as a means of labeling items that are already ordered. For example, pages, sections, chapters, and the like, as well as the items of lists, are frequently "numbered" in this way. Labeling series that may be used include ordinary Arabic numerals (1, 2, 3, ...), Roman numerals (I, II, III, ... or i, ii, iii, ...), or letters (A, B, C, ... or a, b, c, ...). (An alternative method for indicating list items, without numbering them, is to use a bulleted list.)

When letters of an alphabet are used for this purpose of enumeration, there are certain language-specific conventions as to which letters are used. For example, the Russian letters Ъ and Ь (which in writing are only used for modifying the preceding consonant), and usually also Ы, Й, and Ё, are usually omitted. Also in many languages that use extended Latin script, the modified letters are often not used in enumeration.




</doc>
<doc id="7490" url="https://en.wikipedia.org/wiki?curid=7490" title="Civil Rights Act">
Civil Rights Act

Civil Rights Act may refer to several acts of the United States Congress, including:




</doc>
<doc id="7491" url="https://en.wikipedia.org/wiki?curid=7491" title="Cola">
Cola

Cola is a sweetened, carbonated soft drink, made from ingredients that contain caffeine from the kola nut and non-cocaine derivatives from coca leaves, flavored with vanilla and other ingredients. Most colas now use other flavoring (and caffeinating) ingredients with a similar taste. Colas became popular worldwide after pharmacist John Pemberton invented Coca-Cola in 1886. His non-alcoholic recipe was inspired by the coca wine of pharmacist Angelo Mariani, created in 1863.

Modern colas usually contain caramel color, caffeine, and sweeteners such as sugar or high-fructose corn syrup. They now come in numerous different brands. Among them, the most popular are Coca-Cola and Pepsi-Cola. These two cola companies have been rivaling each other since the 1980s.

The primary modern flavoring ingredients in a cola drink are sugar, citrus oils (from oranges, limes, or lemon fruit peel), cinnamon, vanilla, and an acidic flavorant. Manufacturers of cola drinks add trace ingredients to create distinctively different tastes for each brand. Trace flavorings may include nutmeg and a wide variety of ingredients, but the base flavorings that most people identify with a cola taste remain vanilla and cinnamon. Acidity is often provided by phosphoric acid, sometimes accompanied by citric or other isolated acids. Coca-Cola's recipe is maintained as a corporate trade secret.

A variety of different sweeteners may be added to cola, often partly dependent on local agricultural policy. High-fructose corn syrup (HFCS) is predominantly used in the United States and Canada due to the lower cost of government-subsidized corn. In Europe, however, HFCS is subject to production quotas designed to encourage the production of sugar; sugar is thus typically used to sweeten sodas. In addition, stevia or an artificial sweetener may be used; "sugar-free" or "diet" colas typically contain artificial sweeteners only.

Cola can be manufactured with sugar as in Mexican Coca-Cola. Kosher for Passover Coca-Cola sold in the U.S. around the Jewish holiday also uses sucrose rather than HFCS and is also highly sought after by people who prefer the original taste. In addition, PepsiCo has recently been marketing versions of its Pepsi and Mountain Dew sodas that are sweetened with sugar instead of HFCS. These are marketed under the name "Throwback" and became permanent products.

In the 1940s, Coca-Cola produced White Coke at the request of Marshal of the Soviet Union Georgy Zhukov.

Clear colas were again produced during the Clear Craze of the early 1990s. Brands included Crystal Pepsi, Tab Clear, and 7 Up Ice Cola. Crystal Pepsi has been repeatedly reintroduced in the 2010s.

In Denmark a popular clear cola was made by the Cooperative FDB in 1976. It was especially known for being the "Hippie Cola" because of the focus of the harmful effects the colour additive could have on children, and the boycott of multinational brands. It was inspired by a campaign on harmful additives in Denmark by the Environmental-Organisation NOAH, an independent Danish division of Friends of the Earth. This was followed up with a variete of sodas without artificial clouring Today many organic colas are available in Denmark, but for nostalgic reasons the Cola still has remained its popularity to a certain degree 

A 2007 study found that consumption of colas, both those with natural sweetening and those with artificial sweetening, was associated with increased risk of chronic kidney disease. The phosphoric acid used in colas was thought to be a possible cause.

Studies indicate "soda and sweetened drinks are the main source of calories in [the] American diet", so most nutritionists advise that Coca-Cola and other soft drinks can be harmful if consumed excessively, particularly to young children whose soft drink consumption competes with, rather than complements, a balanced diet. Studies have shown that regular soft drink users have a lower intake of calcium, magnesium, ascorbic acid, riboflavin, and vitamin A.

The drink has also aroused criticism for its use of caffeine, which can cause physical dependence (caffeine addiction). A link has been shown between long-term regular cola intake and osteoporosis in older women (but not men). This was thought to be due to the presence of phosphoric acid, and the risk was found to be the same for caffeinated and noncaffeinated colas, as well as the same for diet and sugared colas.

Many soft drinks are sweetened mostly or entirely with high-fructose corn syrup, rather than sugar. Some nutritionists caution against consumption of corn syrup because it may aggravate obesity and type-2 diabetes more than cane sugar.









</doc>
<doc id="7492" url="https://en.wikipedia.org/wiki?curid=7492" title="Capability Maturity Model">
Capability Maturity Model

The Capability Maturity Model (CMM) is a development model created after a study of data collected from organizations that contracted with the U.S. Department of Defense, who funded the research. The term "maturity" relates to the degree of formality and optimization of processes, from "ad hoc" practices, to formally defined steps, to managed result metrics, to active optimization of the processes.

The model's aim is to improve existing software development processes, but it can also be applied to other processes.

The Capability Maturity Model was originally developed as a tool for objectively assessing the ability of government contractors' "processes" to implement a contracted software project. The model is based on the process maturity framework first described in "IEEE Software" and, later, in the 1989 book "Managing the Software Process" by Watts Humphrey. It was later published in a report in 1993 and as a book by the same authors in 1995.

Though the model comes from the field of software development, it is also used as a model to aid in business processes generally, and has also been used extensively worldwide in government offices, commerce, and industry.

In the 1960s, the use of computers grew more widespread, more flexible and less costly. Organizations began to adopt computerized information systems, and the demand for software development grew significantly. Many processes for software development were in their infancy, with few standard or "best practice" approaches defined.

As a result, the growth was accompanied by growing pains: project failure was common, the field of computer science was still in its early years, and the ambitions for project scale and complexity exceeded the market capability to deliver adequate products within a planned budget. Individuals such as Edward Yourdon, Larry Constantine, Gerald Weinberg, Tom DeMarco, and David Parnas began to publish articles and books with research results in an attempt to professionalize the software-development processes.

In the 1980s, several US military projects involving software subcontractors ran over-budget and were completed far later than planned, if at all. In an effort to determine why this was occurring, the United States Air Force funded a study at the SEI.

The first application of a staged maturity model to IT was not by CMU/SEI, but rather by Richard L. Nolan, who, in 1973 published the stages of growth model for IT organizations.

Watts Humphrey began developing his process maturity concepts during the later stages of his 27-year career at IBM.

Active development of the model by the US Department of Defense Software Engineering Institute (SEI) began in 1986 when Humphrey joined the Software Engineering Institute located at Carnegie Mellon University in Pittsburgh, Pennsylvania after retiring from IBM. At the request of the U.S. Air Force he began formalizing his Process Maturity Framework to aid the U.S. Department of Defense in evaluating the capability of software contractors as part of awarding contracts.

The result of the Air Force study was a model for the military to use as an objective evaluation of software subcontractors' process capability maturity. Humphrey based this framework on the earlier Quality Management Maturity Grid developed by Philip B. Crosby in his book "Quality is Free". Humphrey's approach differed because of his unique insight that organizations mature their processes in stages based on solving process problems in a specific order. Humphrey based his approach on the staged evolution of a system of software development practices within an organization, rather than measuring the maturity of each separate development process independently. The CMM has thus been used by different organizations as a general and powerful tool for understanding and then improving general business process performance.

Watts Humphrey's Capability Maturity Model (CMM) was published in 1988 and as a book in 1989, in "Managing the Software Process".

Organizations were originally assessed using a process maturity questionnaire and a Software Capability Evaluation method devised by Humphrey and his colleagues at the Software Engineering Institute.

The full representation of the Capability Maturity Model as a set of defined process areas and practices at each of the five maturity levels was initiated in 1991, with Version 1.1 being completed in January 1993. The CMM was published as a book in 1995 by its primary authors, Mark C. Paulk, Charles V. Weber, Bill Curtis, and Mary Beth Chrissis.
United States of America
New York, USA.

The CMM model's application in software development has sometimes been problematic. Applying multiple models that are not integrated within and across an organization could be costly in training, appraisals, and improvement activities. The Capability Maturity Model Integration (CMMI) project was formed to sort out the problem of using multiple models for software development processes, thus the CMMI model has superseded the CMM model, though the CMM model continues to be a general theoretical process capability model used in the public domain.

The CMM was originally intended as a tool to evaluate the ability of government contractors to perform a contracted software project. Though it comes from the area of software development, it can be, has been, and continues to be widely applied as a general model of the maturity of "process" (e.g., IT service management processes) in IS/IT (and other) organizations.

A maturity model can be viewed as a set of structured levels that describe how well the behaviors, practices and processes of an organization can reliably and sustainably produce required outcomes.

A maturity model can be used as a benchmark for comparison and as an aid to understanding - for example, for comparative assessment of different organizations where there is something in common that can be used as a basis for comparison. In the case of the CMM, for example, the basis for comparison would be the organizations' software development processes.

The model involves five aspects:

There are five levels defined along the continuum of the model and, according to the SEI: "Predictability, effectiveness, and control of an organization's software processes are believed to improve as the organization moves up these five levels. While not rigorous, the empirical evidence to date supports this belief".

Within each of these maturity levels are Key Process Areas which characterise that level, and for each such area there are five factors: goals, commitment, ability, measurement, and verification. These are not necessarily unique to CMM, representing — as they do — the stages that organizations must go through on the way to becoming mature.

The model provides a theoretical continuum along which process maturity can be developed incrementally from one level to the next. Skipping levels is not allowed/feasible.






At maturity level 5, processes are concerned with addressing statistical "common causes" of process variation and changing the process (for example, to shift the mean of the process performance) to improve process performance. This would be done at the same time as maintaining the likelihood of achieving the established quantitative process-improvement objectives. There are only a few companies in the world that have attained this level 5..

The model was originally intended to evaluate the ability of government contractors to perform a software project. It has been used for and may be suited to that purpose, but critics pointed out that process maturity according to the CMM was not necessarily mandatory for successful software development.

The software process framework documented is intended to guide those wishing to assess an organization's or project's consistency with the Key Process Areas. For each maturity level there are five checklist types:




</doc>

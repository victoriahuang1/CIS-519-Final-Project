<doc id="2997" url="https://fr.wikipedia.org/wiki?curid=2997" title="Théories Gaïa">
Théories Gaïa

Le terme générique théories Gaïa fait référence à un ensemble d'hypothèses et de théories selon lesquelles :

Bien que certains soutiennent que des aspects de la théorie Gaïa sont déjà parties intégrantes de nombreuses religions et cultures autochtones, celle-ci a tout d'abord été décrite en tant qu'hypothèse (hypothèse Gaïa) par James Lovelock, chimiste britannique, et Lynn Margulis, une microbiologiste américaine en 1974. Un modèle assez simple fréquemment utilisé pour illustrer l'hypothèse originelle est celui de Daisyworld. L'hypothèse originelle repose sur le concept d'homéostasie et soutient que les formes vivantes d'une planète hôte associées avec leur environnement, se sont comportées et se comportent encore comme un système auto-régulateur. Ce système naturel inclut la biomasse, l'atmosphère, la pédosphère et une mince couche de la lithosphère. De multiples formes de ce concept coexistent, bien que controversées, et une partie au moins en est plus ou moins admise par la communauté scientifique.

Ces théories se veulent aussi très significatives pour l'écologie politique. Les théories Gaïa sont développées par des courants du New Age.

James Lovelock s'est officiellement désolidarisé de la façon dont la "mouvance Gaïa" présente sa théorie, qu'il estime caricaturée par eux.

Il existe des précédents d'ordre mystique, scientifique et religieux à cette théorie. De nombreuses mythologies religieuses, comme certaines religions des Amériques, voient la Terre comme un "Tout plus grand que la somme de ses parties" (holisme).
Si les humains agissent comme espèce clé, pour prévenir les changements climatiques, l'extinction des primates, etc., alors ils peuvent générer une homéostasie avec leur seule cognition.

À la suite de Johannes Kepler qui voyait la Terre comme un organisme rond et unique, Vernadsky concevra la notion de biosphère et, en 1924, son ami paléontologue et géologue Teilhard de Chardin forgera, en lien avec lui et le philosophe Edouard Le Roy, le concept de noosphère, que reprendra Vernadsky. Teilhard a par la suite influencé Thomas Berry et de nombreux humanistes catholiques du . Buckminster Fuller est généralement vu comme ayant rendu l'idée respectable dans certains cercles scientifiques au . Pour sa part, Helan Jaworski présente la Terre comme un être vivant doué de conscience dans "Le Géon ou la Terre vivante" (Paris, Librairie Gallimard, 1928). Lewis Thomas (1913-1993), quant à lui, envisage la Terre comme une cellule. Pour Lee Smolin également, les univers féconds pourraient provenir d'implosions de trous noirs. Toutes ces théories sont des « théories Gaïa ».

Aucune de ces idées ne peut être considérée comme hypothèse scientifique : par définition, une hypothèse scientifique doit pouvoir être testée (voir par exemple l'épistémologie de Karl Popper ou la réfutabilité) ; les idées citées ci-dessus ne pouvant être vérifiées, elles sont exclues du champ scientifique classique.

Ces conjectures peuvent être considérées d'un point de vue social et peut-être de philosophie politique, elles peuvent également avoir des implications d'un point de vue théologique.

La théorie Gaïa a tout d'abord été décrite comme une hypothèse (l'Hypothèse Gaïa) par James Lovelock, chimiste britannique, et Lynn Margulis, microbiologiste américaine, en 1979.

Elle est ensuite reprise par Margulis, laquelle définit la Théorie Gaïa comme une revendication purement scientifique reposant sur la reconnaissance de processus homéostatiques et homéorhétiques s'appliquant à l'ensemble de la biosphère terrestre. L'Hypothèse Gaïa, toutefois, a généré beaucoup plus de discussions que la théorie Gaïa. Pour mieux comprendre les différences entre théorie initiale et Théorie, il est préférable de revenir sur l'hypothèse originelle et sur la simulation Daisyworld.

L'hypothèse de fond de Lovelock est que la biomasse modifie les conditions de vie de la planète afin de rendre celle-ci plus hospitalière. L'Hypothèse Gaïa relie cette notion d' « hospitalité » à l'homéostasie. La Théorie Gaïa de Margulis fait plutôt référence à la notion d'homéorhésie. Un système en homéostasie tend vers un équilibre caractérisé par des paramètres constants, tandis qu'un système homéorhétique se caractérisera par un comportement dynamique de type ondulatoire, oscillant autour d'un état stable sans y converger.
Il semble très probable que les plantes soient favorisées par les effets micro-climatiques qu'elles peuvent avoir localement. D'autre part, il y a de bonnes raisons de penser que ces effets existent également à plus grande échelle, avec des relations symbiotiques ayant une influence climatique globale.

La version de la Théorie Gaïa de Margulis modifie l'Hypothèse Gaïa originelle (et en fait de facto une version moins controversée) de la façon suivante : la biosphère terrestre évolue entre plusieurs points de stabilité, à l'image de ce qui existe en théorie économique. La Terre ne serait pas un organisme vivant, qui pourrait vivre ou mourir brutalement, mais plutôt une sorte de communauté de confiance, pouvant exister à différents niveaux d'intégration.

Coauteur de l'Hypothèse Gaïa originelle, Lynn Margulis soutient que la Terre n'est pas homéostatique mais homéorhétique : en d'autres termes, que les composants atmosphérique, hydrosphérique et lithosphérique sont régulés autour de points homéostatiques, mais que ces points changent au cours du temps... Gaïa serait une symbiose vue de l'espace.

Richard Dawkins, tout en soulignant le côté autorégulateur de la planète (la biologie est après tout son métier), met l'accent sur le fait que les auteurs de l'hypothèse Gaïa ont oublié la condition essentielle nécessaire à définir un être vivant et son évolution, qui est "l'opposition permanente à un milieu extérieur - proies et prédateurs -, seule susceptible de le faire évoluer au fil du temps par le mécanisme bien connu de l'évolution naturelle". Cette opposition n'existe pas pour la Terre, qui n'a ni milieu, ni proies, ni prédateurs. Dawkins affirme donc parfaitement abusive l'hypothèse de l'assimiler à un organisme vivant au seul prétexte qu'elle possède une "dynamique" comme en a aussi n'importe quel système inanimé (un incendie de forêt, un ouragan, une avalanche...) et que les autorégulations concernées n'ont pas le moindre rapport avec des manifestations de 'vie'. Voir cependant noosphère.

Toutefois, encore dans le champ des hypothèses, il est possible de conceptualiser la stérilité environnant la Terre comme étant l'opposition à la vie qu'elle abrite. Tel que propose la deuxième loi de la thermodynamique, ou loi de l'entropie, la vie joue contre la dégradation de la complexité de la matière en réalisant la néguentropie, en produisant des molécules nouvelles dans un univers qui se simplifie continuellement. Dans ce milieu, la lutte pour le maintien de la vie en se nourrissant de l'entropie paraît donner un sens à ce qui n'a aucun sens, la compétition pour la survie. En poussant la théorie évolutionniste au-delà du jeu manichéen entre proies et prédateurs, c'est plutôt le jeu de coopération contre l'entropie qui peut mieux expliquer l'évolution et l'interdépendance des organismes jusqu'à l'échelle planétaire. À ce sujet, voir Lynn Margulis.

Un autre aspect à considérer dans la position de Dawkins est l'interprétation anthropocentrique de l'évolution, c'est-à-dire la projection des rapports sociaux humains sur les relations entre les autres espèces, tel que propose Edward O. Wilson dans sa théorie sociobiologiste. Cette analogie scientiste fondée par Wilson et clairement partagée par Richard Dawkins, suscite encore la controverse. À ce sujet, Jacques Ruelland fait une analyse critique, complète et précise.

La théorie Gaïa constitue un ensemble d'hypothèses, allant de l'indéniable au radical :

Quelques biologistes voient généralement cette activité comme une propriété émergente de l'écosystème : alors que chaque espèce poursuit son intérêt propre, la combinaison de leurs actions tend à contrebalancer les effets du changement environnemental.

Les opposants à ce point de vue évoquent plutôt les conséquences de l'activité des êtres vivants ayant résulté en des modifications drastiques par le passé plutôt qu'à un équilibre stable, telle que la conversion de l'atmosphère terrestre d'un milieu réducteur à un milieu riche en oxygène.

Une position plus poussée considère tous les êtres vivants comme parties d'un organisme "unique", qui est nommé par eux Gaïa. De ce point de vue, l'atmosphère, les mers, la croûte terrestre seraient le résultat des interventions dues à Gaïa, à travers la diversité coévolutive des êtres vivants. Une partie importante des scientifiques récuse cette position ; toutefois, le point de vue scientifique ne s'oppose pas - au contraire - à son "examen".

La forme la plus extrême de la théorie Gaïa considère la planète Terre dans son ensemble comme un organisme; de ce point de vue, la biosphère terrestre "manipulerait" de façon « consciente » le climat de façon à rendre les conditions environnementales plus favorables à la vie. Néanmoins la "vie" est une chose, et la "conscience" une autre : où trouver un témoignage de la "conscience" d'une amibe, malgré ses nombreuses autorégulations ?

Aucune preuve ne vient confirmer ni infirmer cette hypothèse ; le problème se complique par le fait que quelques non-scientifiques confondent "homéostasie" et "activité requérant un contrôle conscient", ce que l'examen de n'importe quel thermostat suffit pourtant à invalider.

Il existe des versions encore plus spéculatives de Gaïa, en particulier celles soutenant que la Terre est effectivement consciente ou partie d'une évolution beaucoup plus vaste. Ces hypothèses sont actuellement considérées comme hors du champ scientifique.

De toutes ces hypothèses, Lovelock apparaît se positionner vers le milieu, Margulis étant plus conservatrice.

Malheureusement, la plupart des partisans de cette théorie ne précisent pas exactement où ils se situent sur cette échelle, ce qui rend la discussion et la critique de cette théorie difficile. Ceci est partiellement dû à la confusion existante entre observateur conscient (par exemple le scientifique) et le manipulateur actif (par exemple l'activiste). Beaucoup d'efforts de la part des tenants de cette théorie consistent à éclaircir les différences existant entre les différentes hypothèses. Une vue fréquente voit la Théorie Gaïa de Margulis comme faisant partie du champ de la biologie, et la différencie de l'Hypothèse originale. Les gaïens en particulier, se voient comme faisant partie de l'homéostasie de la biosphère - que leur rôle soit effectivement indispensable ou non.

Un point de vue social de la théorie Gaïa concerne le rôle des humains comme espèce clé. Si les humains agissent pour prévenir le changement climatique global, etc., alors ils agissent eux-mêmes pour assurer une homéostasie.

Les partisans politiques de cette théorie se nomment parfois les gaïens et cherchent à maintenir l'homéostasie de la Terre, lorsqu'ils constatent que celle-ci penche d'un côté de la balance, par exemple pour empêcher les modifications climatiques anthropiques, les extinctions, la destruction des forêts anciennes... Finalement, ils cherchent à coopérer pour « manipuler consciemment le système pour restaurer son équilibre ». Une telle activité « définit » l'homéostasie. Toutefois, pour être efficace, cette activité repose sur la recherche et la compréhension des équilibres homéorhétiques, ne serait-ce que pour trouver les leviers pour intervenir dans un système qui évolue dans une direction défavorable.

Certains gaïens semblent développer une nouvelle théorie, fusionnant les conclusions d'ordre biologique et politique. Ils voient cette idéologie comme une protoscience de l'écologie humaine. Ces idées incluent le fait de voir les humains comme une espèce clé, qui peut agir pour empêcher les modifications climatiques, les extinctions de primates, etc. et maintiennent délibérément l'équilibre dans toute la biosphère.

Les gaïens affirment qu'il est du devoir moral de l'homme d'agir ainsi - action s'inscrivant dans le cadre du principe de précaution. De telles vues influencent les partis verts, Greenpeace et des ailes plus radicales du mouvement environnemental. Certains voient cette activité comme un mouvement écologique indépendant.

Un gaïen ne se demande pas passivement « ce qui se passe » mais plutôt « ce qu'il y a à faire ensuite », par exemple, terraformation, génie climatique ou même à petite échelle le jardinage. Ainsi, des modifications peuvent être délibérément planifiées et conduites, comme en écologie urbaine et en écologie industrielle.

L'activisme politique des gaïens ressemble à celui des néo-païens comme les Wiccans et autres religions basées sur la Terre ("earth-based religions"), ceci découle d'une analogue conception métaphysique de la Planète, et du principe de responsabilité personnelle/devoir d'agir, afin d'assurer une bonne homéostasie, qui est perçu aux yeux des néo-païens comme le flux harmonieux d'énergie-matière-spiritualité entre l'individu et la divinité Gaïa, dans un esprit de communion, réciprocité et égalité.

La question de ce qu'est un organisme et de l'échelle à laquelle il est rationnel de parler d'organisme plutôt que de biosphère, peut donner naissance à un débat sémantique. 

Nous sommes des écosystèmes dans le sens où notre corps abrite des bactéries ou des parasites, et pour ces derniers, il constitue leur milieu. Selon la conception ordinaire du terme "organisme", nous sommes également des organismes. Or, le premier degré d'hypothèse Gaïa revient plus ou moins à affirmer que la Terre est un écosystème, ce qui est une évidence scientifique. Le point de vue qui promeut un être vivant du statut d'écosystème à celui d'organisme peut-il être appliqué à une planète comme la Terre?

L'argument avancé est que ces organismes symbiotiques, incapables de survivre séparément l'un de l'autre, loin de leur climat et de leurs conditions locales, forment un organisme unique et à part entière, selon une conception de l'organisme plus vaste que l'acception traditionnelle. Cette utilisation du terme organisme fait souvent l'objet de débats. Selon cette définition, la théorie avance que la totalité de la biomasse terrestre est un organisme.

Malheureusement, beaucoup de partisans des théories Gaïa ont du mal à se positionner sur l'échelle des théories, ce qui rend la discussion et la critique difficiles. Beaucoup d'efforts ont été faits pour éclaircir ce que sont ces différentes hypothèses . 

La théorie originelle formulée par Lovelock et Margulis est scientifique, en ce sens que des expériences ont pu être menées pour la réfuter ou la confirmer. Le modèle du Daisyworld a été proposé pour la formaliser mathématiquement et les recherches sur les cycles biogéochimiques, notamment sur celui du soufre, tendent à la confirmer.

Le retour moderne de la Terre-mère pourrait signer l'apparition de nouveaux mythes selon certaines personnes.

Par exemple, le mythologue Joseph Campbell pensait que l'hypothèse Gaïa pourrait être un futur mythe, qui parlerait non pas d'une localité ou d'un peuple, mais d'une planète entière, avec tous les êtres vivants qui s'y trouvent. Le nouveau mythe indiquerait comment entrer en rapport avec la nature et le cosmos, et la société concernée par le mythe serait une société planétaire.

Le mythe de Gaïa est aussi exploité par Isaac Asimov sur une partie du cycle de Fondation. Il en fait un élément de réflexion centrale face à la robotisation des mondes.




</doc>
<doc id="2998" url="https://fr.wikipedia.org/wiki?curid=2998" title="Théorie des graphes">
Théorie des graphes

La théorie des graphes est la discipline mathématique et informatique qui étudie les "graphes", lesquels sont des modèles abstraits de dessins de réseaux reliant des objets. Ces modèles sont constitués par la donnée de , appelés "nœuds" ou "sommets" (en référence aux polyèdres), et de entre ces points ; ces liens sont souvent symétriques (les graphes sont alors dits "non orientés") et sont appelés des "arêtes". 

Les algorithmes élaborés pour résoudre des problèmes concernant les objets de cette théorie ont de nombreuses applications dans tous les domaines liés à la notion de réseau (réseau social, réseau informatique, télécommunications, etc.) et dans bien d'autres domaines (par exemple génétique) tant le concept de graphe, à peu près équivalent à celui de relation binaire (à ne pas confondre donc avec "graphe d'une fonction"), est général. De grands théorèmes difficiles, comme le théorème des quatre couleurs, le théorème des graphes parfaits, ou encore le théorème de Robertson-Seymour, ont contribué à asseoir cette matière auprès des mathématiciens, et les questions qu'elle laisse ouvertes, comme la conjecture de Hadwiger, en font une branche vivace des mathématiques discrètes.

Un graphe est un ensemble de points nommés "nœuds" (parfois "sommets" ou "cellules") reliés par des traits (segments) ou flèches nommées "arêtes" (ou "liens" ou "arcs"). L'ensemble des arêtes entre nœuds forme une figure similaire à un réseau. Différents types de réseaux sont étudiés suivant leur genre de forme (ou topologie) et propriétés ; les "arbres" sont une sous-catégorie plus simple de graphes particulièrement importante et qui est très étudiée, notamment en informatique.

Les arêtes peuvent être orientées (flèches) ou non orientées (traits). Si les arêtes sont orientées, la relation va dans un seul sens et est donc asymétrique, et le graphe lui-même est dit "orienté". Sinon, si les arêtes sont non orientées, la relation va dans les deux sens et est symétrique, et le graphe est dit "non orienté".

En mathématiques, l'ensemble des nœuds est le plus souvent noté formula_1 ("vertices" en anglais), tandis que formula_2 désigne l'ensemble des arêtes ("edges" en anglais). Dans le cas général, un graphe peut avoir des , c'est-à-dire que plusieurs arêtes différentes relient la même paire de points. De plus, une arête peut être une "boucle", c'est-à-dire ne relier qu'un point à lui-même. Un graphe est "simple" s'il n'a ni arêtes multiples ni boucles, il peut alors être défini simplement par un couple formula_3, où formula_2 est un ensemble de paires d'éléments de formula_1. Dans le cas d'un graphe simple orienté, formula_2 est un ensemble de "couples" d'éléments de formula_1. Notons qu'un graphe sans arête multiple peut être représenté par une relation binaire, qui est symétrique si le graphe est non orienté.

Pour définir un graphe général, il faut une fonction formula_8 (gamma) qui associe à chaque arête une paire de sommets. Ainsi, un graphe est un triplet formula_9 avec formula_10. Toutefois l'usage veut que l'on note simplement formula_11, sachant que ce n'est parfaitement rigoureux que pour les graphes simples.

Il existe trois grandes familles de graphes et cinq catégories au total

Un article du mathématicien suisse Leonhard Euler, présenté à l'Académie de Saint-Pétersbourg en 1735 puis publié en 1741, traitait du problème des sept ponts de Königsberg, ainsi que schématisé ci-dessous. Le problème consistait à trouver une promenade à partir d'un point donné qui fasse revenir à ce point en passant une fois et une seule par chacun des sept ponts de la ville de Königsberg. Un chemin passant par toute arête exactement une fois fut nommé chemin eulérien, ou circuit eulérien s'il finit là où il a commencé. Par extension, un graphe admettant un circuit eulérien est dit "graphe eulérien", ce qui constitue donc le premier cas de propriété d'un graphe. Euler avait formulé qu'un graphe n'est eulérien que si chaque sommet a un nombre pair d'arêtes. L'usage est de s'y référer comme "théorème d'Euler", bien que la preuve n'en ait été apportée que 130 ans plus tard par le mathématicien allemand Carl Hierholzer. Un problème similaire consiste à passer par chaque sommet exactement une fois, et fut d'abord résolu avec le cas particulier d'un cavalier devant visiter chaque case d'un échiquier par le théoricien d'échecs arabe Al-Adli dans son ouvrage "Kitab ash-shatranj" paru vers 840 et perdu depuis. Ce problème du cavalier fut étudié plus en détail au par les mathématiciens français Alexandre-Théophile Vandermonde, Pierre Rémond de Montmort et Abraham de Moivre; le mathématicien britannique Thomas Kirkman étudia le problème plus général du parcours où on ne peut passer par un sommet qu'une fois, mais un tel parcours prit finalement le nom de chemin hamiltonien d'après le mathématicien irlandais William Rowan Hamilton, et bien que ce dernier n'en ait étudié qu'un cas particulier. On accorde donc à Euler l'origine de la théorie des graphes parce qu'il fut le premier à proposer un traitement mathématique de la question, suivi par Vandermonde.

Au milieu du , le mathématicien britannique Arthur Cayley s'intéressa aux arbres, qui sont un type particulier de graphe n'ayant pas de cycle, "i.e." dans lequel il est impossible de revenir à un point de départ sans faire le chemin inverse. En particulier, il étudia le nombre d'arbres à "n" sommets et montra qu'il en existe formula_12. Ceci constitua « une des plus belles formules en combinatoire énumérative », domaine consistant à compter le nombre d'éléments dans un ensemble fini, et ouvrit aussi la voie à l'énumération de graphes ayant certaines propriétés. Ce champ de recherche fut véritablement initié par le mathématicien hongrois George Pólya, qui publia en 1937 le théorème de dénombrement qui porte son nom, et le mathématicien hollandais Nicolaas Govert de Bruijn. Les travaux de Cayley, tout comme ceux de Polya, présentaient des applications à la chimie et le mathématicien anglais James Joseph Sylvester, coauteur de Cayley, introduisit en 1878 le terme de "graphe" basé sur la chimie :

Un des problèmes les plus connus de théorie des graphes vient de la "coloration de graphe", où le but est de déterminer combien de couleurs différentes suffisent pour colorer entièrement un graphe de telle façon qu'aucun sommet n'ait la même couleur que ses voisins. En 1852, le mathématicien sud-africain Francis Guthrie énonça le problème des quatre couleurs par une discussion à son frère, qui demandera à son professeur Auguste De Morgan si toute carte peut être coloriée avec quatre couleurs de façon que des pays voisins aient des couleurs différentes. De Morgan envoya d'abord une lettre au mathématicien irlandais William Rowan Hamilton, qui n'était pas intéressé, puis le mathématicien anglais Alfred Kempe publia une preuve erronée dans l’"American Journal of Mathematics", qui venait d'être fondé par Sylvester. L'étude de ce problème entraîna de nombreux développements en théorie des graphes, par Peter Guthrie Tait, Percy John Heawood, Frank Ramsey et Hugo Hadwiger. 

Les problèmes de "factorisation de graphe" émergèrent ainsi à la fin du en s'intéressant aux sous-graphes couvrants, c'est-à-dire aux graphes contenant tous les sommets mais seulement une partie des arêtes. Un sous-graphe couvrant est appelé un "k"-facteur si chacun de ses sommets a "k" arêtes et les premiers théorèmes furent donnés par Julius Petersen ; par exemple, il montra qu'un graphe peut être séparé en 2-facteurs si et seulement si tous les sommets ont un nombre pair d'arêtes (mais il fallut attendre 50 ans pour que Bäbler traite le cas impair). Les travaux de Ramsey sur la coloration, et en particulier les résultats du mathématicien hongrois Pal Turan, permirent le développement de la théorie des graphes extrémaux s'intéressant aux graphes atteignant le maximum d'une quantité particulière (par exemple le nombre d'arêtes) avec des contraintes données, telles que l'absence de certains sous-graphes.

Dans la seconde moitié du , le mathématicien français Claude Berge contribue au développement de la théorie des graphes par ses contributions sur les "graphes parfaits" et l'introduction du terme d’"hypergraphe" (suite à la remarque de Jean-Marie Pla l'ayant utilisé dans un séminaire) avec un monographe sur le sujet. Son ouvrage d'introduction à la théorie des graphes proposa également une alternative originale, consistant plus en une promenade personnelle qu'une description complète. Il marquera également la recherche française en ce domaine, par la création conjointe avec Marcel-Paul Schützenberger d'un séminaire hebdomadaire à l'Institut Henri Poincaré, des réunions le lundi à la Maison des Sciences de l'Homme, et la direction de l'équipe Combinatoire de Paris.

Les Allemands Franz Ernst Neumann et Jacobi, respectivement physicien et mathématicien, fondèrent en 1834 une série de séminaires. Le physicien allemand Gustav Kirchhoff était un des étudiants participant au séminaire entre 1843 et 1846, et il étendit le travail de Georg Ohm pour établir en 1845 les lois de Kirchhoff exprimant la conservation de l'énergie et de la charge dans un circuit électrique. En particulier, sa loi des nœuds stipule que la somme des intensités des courants entrant dans un nœud est égale à celle qui en sort. Un circuit électrique peut se voir comme un graphe, dans lequel les sommets sont les nœuds du circuit, et les arêtes correspondent aux connexions physiques entre ces nœuds. Pour modéliser les courants traversant le circuit, on considère que chaque arête peut être traversée par un "flot". Ceci offre de nombreuses analogies, par exemple à l'écoulement d'un liquide comme l'eau à travers un réseau de canaux, ou la circulation dans un réseau routier. Comme stipulé par la loi des nœuds, le flot à un sommet est conservé, ou identique à l'entrée comme à la sortie ; par exemple, l'eau qui entre dans un canal ne disparaît pas et le canal n'en fabrique pas, donc il y a autant d'eau en sortie qu'en entrée. De plus, une arête a une limite de capacité, tout comme un canal peut transporter une certaine quantité maximale d'eau. Si l'on ajoute que le flot démarre à un certain sommet (la "source") et qu'il se termine à un autre (le "puits"), on obtient alors les principes fondamentaux de l'étude des flots dans un graphe.

Si on considère que la source est un champ pétrolifère et que le puits est la raffinerie où on l'écoule, alors on souhaite régler les vannes de façon à avoir le meilleur débit possible de la source vers le puits. En d'autres termes, on cherche à avoir une utilisation aussi efficace que possible de la capacité de chacune des arêtes, ce qui est le problème de flot maximum. Supposons que l'on « coupe » le graphe en deux parties, telles que la source est dans l'une et le puits est dans l'autre. Chaque flot doit passer entre les deux parties, et est donc limité par la capacité maximale qu'une partie peut envoyer à l'autre. Trouver la coupe avec la plus petite capacité indique donc l'endroit où le réseau est le plus limité, ce qui revient à établir le flot maximal qui peut le traverser. Ce théorème est appelé flot-max/coupe-min et fut établi en 1956.

L’étude des flots réseaux se généralise de plusieurs façons. La recherche d'un maximum, ici dans le cas du flot, est un problème d'optimisation, qui est la branche des mathématiques consistant à optimiser ("i.e." trouver un minimum ou maximum) une fonction sous certaines contraintes. Un flot réseau est soumis à trois contraintes : la limite de capacité sur chaque arête, la création d'un flot non nul entre la source et le puits ("i.e." la source crée un flot), et l'égalité du flot en entrée/sortie pour tout sommet autre que la source et les puits ("i.e." ils ne consomment ni ne génèrent une partie du flot). Ces contraintes étant linéaires, le problème d'un flot réseau fait partie de l'optimisation linéaire. Il est également possible de rajouter d'autres variables au problème pour prendre en compte davantage de situations : on peut ainsi avoir , une sur chaque arête, un coût lorsqu'on utilise une arête, ou une passant par une arête.

Jusqu'au milieu du , l'algorithme construisant un graphe n'avait rien d'aléatoire : tant que les paramètres fournis à l'algorithme ne changeaient pas, alors le graphe qu'il construisait était toujours le même. Une certaine dose d'aléatoire fut alors introduite, et les algorithmes devinrent ainsi probabilistes. Le mathématicien d'origine russe Anatol Rapoport eut d'abord cette idée en 1957 mais elle fut proposée indépendamment deux ans après, de façon plus formelle, par les mathématiciens hongrois Paul Erdős et Alfréd Rényi. Ceux-ci se demandèrent à quoi ressemble un graphe « typique » avec "n" sommets et "m" arêtes. Ils souhaitaient ainsi savoir quelles propriétés pouvaient être trouvées avec "n" sommets, et "m" arêtes créées au hasard. Une quantité fixe "m" n'étant pas pratique pour répondre à cette question, il fut décidé que chaque arête existerait avec une probabilité "p". Ceci fut le début de la "théorie des graphes aléatoires", où l'on considère un nombre de sommets "n" assez grand, et l'on s'intéresse à la probabilité "p" suffisante pour que le graphe ait une certaine propriété.

Erdős et Rényi découvrirent que le graphe n'évoluait pas de façon linéaire mais qu'il y avait au contraire une probabilité critique "p" après laquelle il changeait de façon radicale. Ce comportement est bien connu en physique : si l'on observe un verre d'eau que l'on met dans un congélateur, il ne se change pas progressivement en glace mais plutôt brutalement lorsque la température passe en dessous de . L'eau avait deux phases (liquide et glace) et passe de l'une à l'autre par un phénomène nommé "transition de phase", la transition étant rapide autour d'un "point critique" qui est dans ce cas la température de . Pour nombre de propriétés observées, les graphes aléatoires fonctionnent de la même manière : il existe une probabilité critique formula_13 en dessous de laquelle ils se trouvent dans une phase sous-critique, et au-dessus de laquelle ils passent en phase sur-critique. Dans le cas d'un graphe aléatoire, la probabilité que l'on observe la propriété nous intéressant est faible en phase sous-critique mais devient très forte ("i. e." quasi-certitude) en phase sur-critique ; le tracé de la probabilité d'avoir la propriété en fonction de "p" a donc une allure bien particulière, simplifiée dans le schéma à droite.

Au-delà du vocabulaire commun des phases, la théorie des graphes aléatoires se retrouve en physique statistique sous la forme de la théorie de la percolation. Cette dernière visait à l'origine à étudier l'écoulement d'un fluide à travers un matériau poreux. Par exemple, si l'on immerge une pierre ponce dans un seau rempli d'eau, on s'intéresse à la façon dont l'eau va s'écouler dans la pierre. Pour modéliser ce problème, on se concentre sur les paramètres importants : l'âge ou la couleur de la pierre n'importe pas, tandis que les ouvertures ou 'canaux' dans lesquels peut circuler l'eau sont primordiaux. L'abstraction la plus simple est de voir une pierre comme une grille, où chaque canal existe avec une probabilité "p". On retrouve ainsi le modèle du graphe aléatoire, mais avec une contrainte "spatiale" : un arc ne peut exister entre deux sommets que s'ils sont voisins dans la grille. Cependant, cette contrainte peut être levée pour établir une équivalence entre la théorie des graphes et celle de la percolation. Tout d'abord, un graphe de "n" sommets peut être représenté par une grille avec "n" dimensions ; puisqu'on s'intéresse au cas où "n" est assez grand, c'est-à-dire formula_14, ceci établit une équivalence avec la percolation en dimension infinie. De plus, il existe une dimension critique formula_15 telle que le résultat ne dépend plus de la dimension dès que celle-ci atteint formula_15 ; on pense que cette dimension critique est 6, mais elle n'a pu être prouvée que pour 19.

De nombreux modèles ont été proposés depuis le début des années 2000 pour retrouver des phénomènes observés dans des graphes tels que celui représentant les connexions entre des acteurs de Hollywood (obtenu par IMDb) ou des parties du Web. En 1999, Albert-László Barabási et Réka Albert expliquèrent qu'un de ces phénomènes « est une conséquence de deux mécanismes : le réseau grandit continuellement avec l'ajout de nouveaux sommets, et les nouveaux sommets s'attachent avec certaines préférences à d'autres qui sont déjà bien en place ». Une certaine confusion s'installa autour de leur modèle : s'il permet effectivement d'obtenir le phénomène souhaité, il n'est pas le seul modèle arrivant à ce résultat et on ne peut donc pas conclure en voyant le phénomène qu'il résulte d'un processus d'attachement préférentiel. Les phénomènes de "petit monde" et de "liberté d'échelle", pour lesquels de très nombreux modèles ont été proposés, peuvent être réalisés simplement par des graphes aléatoires : la technique de Michael Molloy et Bruce Reed permet d'obtenir l'effet de libre d'échelle, tandis que celle de Li, Leonard et Loguinov conduit au petit-monde.

Formellement un graphe est "étiqueté" : chaque sommet ou arête appartient à un ensemble, donc porte une "étiquette". Typiquement, les graphes sont étiquetés par des nombres entiers, mais une étiquette peut en fait appartenir à n'importe quel ensemble : ensemble de couleurs, ensemble de mots, ensemble des réels. Les exemples ci-contre montrent des graphes étiquetés par des entiers et par des lettres. L'étiquetage d'un graphe peut être conçu de façon à donner des informations utiles pour des problèmes comme le routage : partant d'un sommet formula_17, on veut arriver à un sommet formula_18, c'est-à-dire que l'on souhaite acheminer une information de formula_17 à formula_18. Selon la façon dont les sommets sont étiquetés, les étiquettes que portent formula_17 et formula_18 peuvent nous permettre de trouver facilement un chemin. Par exemple, dans le où la distance maximale entre deux sommets est formula_23, imaginons que l'on soit à un sommet étiqueté formula_24 et que l'on souhaite aller à formula_25 : il suffit de décaler l'étiquette en introduisant la destination, ce qui donne le cheminformula_26Ce chemin se lit de la façon suivante : si on se trouve au sommet étiqueté formula_24 alors on va vers le voisin portant l'étiquette formula_28, et ainsi de suite. 

On se retrouve cependant face à un problème : si on regarde plus haut l'illustration de la liste des arbres à 2, 3 et 4 sommets, beaucoup d'entre eux ont exactement la même "structure" mais un étiquetage différent (donné ici par des couleurs). Pour étudier uniquement la structure, il faut donc un outil permettant d'ignorer l'étiquetage, c'est-à-dire de donner une équivalence structurelle. Pour cela, on introduit la notion de morphisme. Un "morphisme de graphes", ou "homomorphisme de graphes", est une application entre deux graphes qui respecte la structure des graphes. Autrement dit l'image du graphe formula_29 dans formula_30 doit respecter les relations d'adjacences présentes dans formula_29. Plus précisément, si formula_29 et formula_30 sont deux graphes, une application formula_34 est un morphisme de graphes si formula_35 où formula_36 transforme les sommets de G en ceux de H, et formula_37 les arêtes de G en celles de H en respectant la contrainte suivante :
s'il existe une arête formula_38 entre deux sommets de formula_29 alors il doit y avoir une arête formula_40 entre les deux sommets correspondants de formula_30. On dit de l'homomorphisme formula_42 qu'il est une injection (respectivement surjection) si ses deux fonctions formula_43 et formula_44 sont injectives (respectivement surjectives); si elles sont à la fois injectives et surjectives, c'est-à-dire bijectives, alors formula_42 est un "isomorphisme de graphes". Si deux graphes sont isomorphes, alors ils ont la même structure : peu importe la façon dont ils sont dessinés ou étiquetés, il est possible de déplacer les sommets ou de changer les étiquettes pour que l'un soit la copie conforme de l'autre, ainsi qu'illustré ci-dessous. On désigne alors par graphe "non étiqueté" la classe d'équivalence d'un graphe pour la relation d'isomorphisme. Deux graphes isomorphes seront alors considérés comme égaux si on les considère en tant que graphes non étiquetés.

Le mot graphe peut désigner, selon les contextes, un graphe étiqueté ou non étiqueté. Quand on parle du graphe du web, les étiquettes sont des URL et ont un sens. Le mot est utilisé pour désigner un graphe étiqueté. À l'opposé le graphe de Petersen est toujours considéré à isomorphisme près, donc non étiqueté, seules ses propriétés structurelles étant intéressantes.

Tout graphe formula_11 peut être représenté par une matrice.
Les relations entre arêtes et sommets, appelées les relations d'incidence, sont toutes représentées par la matrice d'incidence du graphe.
Les relations d'adjacences (si deux sommets sont reliés par une arête ils sont adjacents) sont représentés par sa matrice d'adjacence. Elle est définie par 
formula_47

De nombreuses informations d'un graphe peuvent être représentées par une matrice. Par exemple, la matrice des degrés formula_23 est une matrice diagonale où les éléments formula_49 correspondent au nombre de connexions du sommet formula_50, c'est-à-dire à son degré. En utilisant cette matrice et la précédente, on peut également définir la matrice laplacienne formula_51 ; on obtient sa forme normalisée formula_52 par formula_53, où formula_54 dénote la matrice identité, ou on peut aussi l'obtenir directement par chacun de ses éléments :
formula_55

Ces représentations dépendent de la façon dont les sommets du graphe sont étiquetés. Imaginons que l'on garde la même structure que dans l'exemple ci-dessus et que l'on inverse les étiquettes "1" et "6" : on inverse alors les colonnes "1" et "6" de la matrice d'adjacence. Il existe en revanche des quantités qui ne dépendent pas de la façon dont on étiquette les sommets, tels que le degré minimal/maximal/moyen du graphe. Ces quantités sont des "invariants" du graphe : elles ne changent pas selon la numérotation. Tandis qu'une matrice d'adjacence ou laplacienne varie, son "spectre", c'est-à-dire l'ensemble de ses valeurs propres formula_56, est un invariant. L'étude du rapport entre les spectres et les propriétés d'un graphe est le sujet de la théorie spectrale des graphes ; parmi les rapports intéressants, le spectre donne des renseignements sur le nombre chromatique, le nombre de composantes connexes et les cycles du graphe.

Les graphes permettant de représenter de nombreuses situations, il existe de nombreux algorithmes ("i.e." programmes) les utilisant. La complexité d'un algorithme consiste essentiellement à savoir, pour un problème donné, combien de temps est nécessaire pour le résoudre et quel est l'espace machine que cela va utiliser. Certaines représentations de graphes permettent d'obtenir de meilleures performances, c'est-à-dire que le problème est résolu plus rapidement ou en occupant moins d'espace. Dans certains cas, un problème NP-complet (classe la plus ardue) sur une représentation d'un graphe peut être résolu en temps polynomial (classe simple) avec une autre représentation; l'idée n'est pas qu'il suffit de regarder le graphe différemment pour résoudre le problème plus vite, mais que l'on « paye » pour le transformer et que l'on « économise » alors pour résoudre le problème. Une telle transformation est la "décomposition arborescente" proposée par les mathématiciens Robertson et Seymour dans leur série "Graph Minors". Intuitivement, une décomposition arborescente représente le graphe d'origine formula_29 par un arbre, où chaque sommet correspond à un sous-ensemble des sommets de G, avec quelques contraintes. Formellement, pour un graphe donné formula_3, sa décomposition arborescente est formula_59 où formula_60 est un arbre et formula_42 une fonction associant à chaque sommet formula_62 un ensemble de sommets formula_63. Trois contraintes doivent être satisfaites :

La "largeur arborescente" formula_72 d'une décomposition formula_73 d'un graphe formula_29 est formula_75, c'est-à-dire la taille du plus grand ensemble représenté par un sommet moins 1 ; on peut la voir comme l'abstraction maximale : pour un sommet de l'arbre, jusqu'à combien de sommets du graphe représente-t-on ? Construire la décomposition arborescente d'un graphe quelconque avec la plus petite largeur arborescente est un problème NP-dur. Cependant, cela peut être fait rapidement pour certains graphes, ou approximée pour d'autres tels les graphes planaires ("i. e." pouvant être dessinés sans croiser deux arêtes).
Robertson et Seymour développèrent également le concept de "décomposition en branches". Pour la comprendre, il faut introduire davantage de vocabulaire sur un arbre. Dans les graphes, un arbre est dessiné "à l'envers" : on démarre de la racine en haut, et on descend jusqu'à atteindre les feuilles en bas ; tout sommet n'étant pas une feuille est appelé un 'nœud interne'. La décomposition en branches résulte en un arbre dans lequel tout nœud interne a exactement trois voisins (comme sur l'exemple ci-contre), et où chaque feuille représente une arête du graphe d'origine. La profondeur minimale de la décomposition d'un graphe formula_29 est notée formula_77, et on a la relation formula_78. De même que pour la décomposition arborescente, il est NP-dur de construire une décomposition en branches avec formula_77 minimal pour un graphe quelconque ; dans ce cas, cette construction est réalisable pour un graphe planaire.

Ces représentations sont utilisées sur des problèmes NP-complets par des techniques de programmation dynamique, qui prennent généralement un temps exponentiel en formula_77 ou formula_72. Un tel problème est par exemple l'ensemble dominant : on veut savoir s'il y a un sous-ensemble formula_23 de sommets de taille au plus formula_83 tel qu'un sommet n'étant pas dans formula_23 y soit relié par une arête. Si le graphe est planaire, cette technique permet de résoudre le problème en temps formula_85.

La façon dont le graphe est représenté en tant qu'objet mathématique a été exposée dans la section précédente. Dans l'aspect algorithmique de la théorie des graphes, on cherche à concevoir un processus efficace pour traiter un problème faisant intervenir un graphe. Les principaux critères d'efficacités d'un processus sont le temps nécessaire avant d'obtenir la réponse, et l'espace que le processus consomme dans son travail. La façon dont on représente le graphe influence la performance en temps et en espace : par exemple, si l'on veut connaître l'existence d'une arête entre deux sommets, la matrice d'adjacence permettra d'obtenir un résultat immédiatement, ce que l'on appelle en formula_86. En revanche, une opération de base telle que trouver le voisin d'un sommet est en formula_87 sur une matrice d'adjacence : dans le pire des cas, il faudra scanner la totalité de la colonne pour s'apercevoir qu'il n'y a pas de voisin. Une autre structure de données est la liste d'adjacence, consistant en un tableau dont l'entrée formula_50 donne la liste des voisins du sommet formula_50 : sur une telle structure, trouver un voisin se fait en formula_86 tandis que l'existence d'une arête est en formula_87. Ainsi, au niveau du temps, le choix de la structure dépend des opérations de base que l'on souhaite optimiser. 

De même, l'espace qu'une structure consomme dépend du type de graphe considéré : un raccourci abusif consiste à dire qu'une liste d'adjacences consomme moins d'espace qu'une matrice car celle-ci sera creuse, mais cela prend par exemple plus d'espace pour stocker un graphe aléatoire avec les listes qu'avec une matrice ; dans le cas général, une matrice utilise un espace formula_92 et les listes utilisent formula_93 donc "si" le graphe est dense alors formula_94 peut être suffisamment grand pour qu'une matrice consomme moins d'espace, et "si" le graphe est peu dense alors les listes consommeront moins d'espace. Des modifications simples d'une structure de données peuvent permettre d'avoir un gain appréciable : par exemple, dans une représentation "partiellement complémentée" d'une liste, un bit spécial indique si la liste est celle des voisins présents "ou" manquants ; cette technique permet d'avoir des algorithmes linéaires sur le complément d'un graphe.

Tandis que ces structures sont locales, il existe aussi des structures de données "distribuées". Le principe de ces structures est de concevoir un "schéma d'étiquetage" tel que, pour deux sommets formula_95 et formula_96, on puisse répondre à une question comme « quelle est la distance entre formula_95 et formula_96 » "uniquement" en utilisant les étiquettes de ces nœuds ; une telle utilisation des étiquettes a été vue en section « Étiquetage et morphismes » avec le graphe de Kautz où l'on peut déduire le chemin entre deux sommets uniquement grâce à leur étiquette, et la longueur de ce chemin nous donne la distance. Un étiquetage est "efficace" s'il permet de répondre à une question donnée uniquement en utilisant deux étiquettes, tout en minimisant le nombre maximum de bits d'une étiquette. Outre la distance, une question type peut être de tester l'adjacence, c'est-à-dire de savoir si deux sommets sont voisins ; notons que cela se ramène également au cas particulier d'une distance 1. Le premier exemple d'étiquetage efficace pour tester l'adjacence fut proposé dans le cas des arbres, et chaque étiquette est constituée de deux parties de formula_99 bits : la première partie identifie le sommet, et un nombre allant jusqu'à formula_100 nécessite formula_99 bits pour être codé, tandis que la seconde partie identifie le parent de ce sommet ; pour tester l'adjacence, on utilise le fait que deux sommets sont voisins dans un arbre si et seulement si l'un est le parent de l'autre.

L'efficacité d'un schéma d'étiquetage est lié à la taille des séparateurs du graphe.

Si un graphe a des séparateurs de taille formula_102, alors on peut par exemple concevoir des étiquettes de formula_103 bits pour la distance ; ceci permet directement d'en déduire l'étiquetage pour des graphes dont on connaît la taille des séparateurs, tels un graphe planaire où le séparateur est de taille formula_104. Enfin, il ne faut pas considérer que la taille de l'étiquetage mais également le temps nécessaire, étant donnés deux étiquettes, pour effectuer le "décodage" répondant à la question ("i.e." quelle est la distance ? sont-ils voisins ?).

De nombreux problèmes sur les graphes sont NP-complets, c'est-à-dire durs à résoudre. Cependant, cette dureté est inégale : "certaines" parties du problème peuvent être particulièrement dures, et en constituent ainsi le cœur, tandis que d'autres sont assez faciles à gérer. Ainsi, avant d'exécuter un algorithme sur un problème qui peut être dur, il est préférable de passer du temps à réduire ce problème pour ne plus avoir à considérer que son cœur.





</doc>
<doc id="3002" url="https://fr.wikipedia.org/wiki?curid=3002" title="Tsunami">
Tsunami

Un tsunami, du japonais , est une série d'ondes de très grande période se propageant à travers un milieu aquatique (océan, mer ou lac), issues du brusque mouvement d'un grand volume d'eau, provoqué généralement par un séisme, un glissement de terrain sous-marin ou une explosion volcanique, et pouvant se transformer, en atteignant les côtes, en vagues destructrices déferlantes de très grande hauteur.

En eau profonde, les vagues du tsunami ont une période (temps séparant chaque crête) se comptant en dizaines de minutes, et peuvent voyager à plus de , tout en ne dépassant pas quelques décimètres de hauteur. Mais à l'approche des côtes, leur période et leur vitesse diminuent, tandis-que leur amplitude augmente, leur hauteur pouvant dépasser . Elles peuvent alors submerger le rivage, inondant les terrains bas, pénétrant profondément dans les terres, en emportant tout sur leur passage, dans une succession de flux et de reflux. 

Les tsunamis font partie des catastrophes les plus destructrices de l'histoire. Sur les quatre derniers millénaires, ils totalisent plus de , à travers au moins répertoriés. Le tsunami de 2004 dans l'Océan Indien est la catastrophe la plus meurtrière des années, avec plus de .

En français, le terme de est couramment employé pour désigner les tsunamis. C'est toutefois un terme imprécis, car il regroupe sous la même appellation les tsunamis et d'autres phénomènes de submersion marine. Les scientifiques ont donc officialisé en 1963 le terme "tsunami", sujet de cet article.

Un tsunami est créé lorsqu'une grande masse d'eau est déplacée. Cela peut être le cas lors d'un séisme important, d'une magnitude de 6,3 (valeur « seuil » d'après les catalogues de tsunamis disponibles : NOA, catalogue de Novossibirsk, etc.) ou plus, lorsque le niveau du plancher océanique le long d'une faille s'abaisse ou s'élève brutalement (voir Fig. 1), lors d'un glissement de terrain côtier ou sous-marin, lors d'un impact par un astéroïde ou une comète ou encore lors d'un retournement d'iceberg. Un fort séisme ne produit pas nécessairement un tsunami : tout dépend de la manière (vitesse, surface, etc.) avec laquelle la topographie sous-marine (bathymétrie) évolue aux alentours de la faille et transmet la déformation à la colonne d'eau au-dessus.

Les mouvements de l'eau provoquent un mouvement de grande longueur d'onde (généralement quelques centaines de kilomètres) et de grande période (quelques minutes dans le cas d'un glissement de terrain à quelques dizaines de minutes dans le cas d'un séisme).

Certains tsunamis sont capables de se propager sur des distances de plusieurs milliers de kilomètres et d'atteindre l'ensemble des côtes d'un océan en moins d'une journée. Ces tsunamis de grande étendue sont généralement d'origine tectonique, car les glissements de terrain et les explosions volcaniques produisent généralement des ondes de plus courte longueur d'onde qui se dissipent rapidement : on parlera de dispersion des ondes.

Ce n'est pas principalement la hauteur du tsunami qui fait sa force destructrice, mais la durée de l'élévation du niveau de l'eau et la quantité d'eau déplacée à son passage : si des vagues de plusieurs mètres de hauteur, voire d'une dizaine de mètres, sont légion sur les côtes de l'océan Pacifique, elles ne transportent pas assez d'énergie pour pénétrer profondément à l'intérieur des terres. On peut voir le phénomène sous un autre angle : une vague classique, d'une période d'au plus une minute, n'élève pas le niveau de l'eau suffisamment longtemps pour qu'elle pénètre profondément, tandis que le niveau des eaux s'élève au-dessus de son niveau normal pendant 5 à 30 minutes lors du passage d'un tsunami.

La force destructrice provient de l'énergie considérable qu'il véhicule : contrairement à la houle ou aux vagues classiques qui sont des phénomènes de surface et de faible longueur, le tsunami touche l'océan sur toute sa profondeur et sur une longueur d'onde bien plus importante. L'énergie dépendant de la vitesse et de la masse, celle-ci est considérable, même pour une faible élévation de surface au large près de l'épicentre. C'est cette énergie qui est révélée par l'élévation de la vague à l'approche des côtes. D'où son impact sur le littoral.

Les victimes emportées par un tsunami peuvent recevoir divers chocs par les objets charriés (morceaux d'habitations détruites, bateaux, voitures, etc.) ou être projetées violemment contre des objets terrestres (mobilier urbain, arbres, etc.) : ces coups peuvent être mortels ou provoquer une perte de conscience et de facultés, pertes menant à la noyade. Certaines victimes peuvent aussi être piégées sous les décombres d'habitations. Enfin, le reflux du tsunami est capable d'emmener des personnes au large, où elles dérivent et, sans secours, meurent de noyade, d'épuisement ou de soif.

Dans les jours et les semaines suivant l'événement, le bilan peut s'alourdir, en particulier dans les pays pauvres. Mais de temps à autre des victimes survivent et restent des jours, des semaines voire des mois sous les décombres. L'après-tsunami peut être plus mortel que la vague elle-même. Les maladies liées à la putréfaction de cadavres, à la contamination de l'eau potable et à la péremption des aliments sont susceptibles de faire leur apparition. La faim peut survenir en cas de destruction des récoltes et des stocks alimentaires.

Les tsunamis sont susceptibles de détruire habitations, infrastructures et flore en raison :

De plus, dans les régions plates, la stagnation d'eaux maritimes saumâtres peut porter un coup fatal à la faune et à la flore côtières, ainsi qu'aux récoltes. Sur les côtes sableuses ou marécageuses, le profil du rivage peut être modifié par la vague et une partie des terres, immergées.

Les récifs coralliens peuvent également être disloqués et mis à mal par le tsunami lui-même et par la turbidité de l'eau qui peut s'ensuivre les semaines suivantes, ainsi que par les polluants (engrais, pesticides…) que l'eau a pu ramener.

Pour mesurer les effets ou la magnitude des tsunamis, différentes échelles, analogues à l'échelle de Richter pour les séismes, sont utilisées.

L'échelle Sieberg-Ambraseys, utilisée par le BRGM, classe les tsunamis par degrés :
L'échelle d'Imamura permet d'attribuer une magnitude aux tsunamis. Introduite par Akitsune Imamura en 1942 et développée par Iida en 1956, elle est l'une des plus simples. La magnitude est calculée à partir de la hauteur maximum de la vague au niveau de la côte, selon la formule :
où formula_2 désigne la magnitude et formula_3 la hauteur maximale de la vague (formula_4 est le logarithme de base 2).

Par exemple, le tsunami de 2004 dans l'océan Indien fut de magnitude 4 à Sumatra et de magnitude 2 en Thaïlande.

La présence d'un système d'alerte permettant d'alerter la population quelques heures avant la survenue d'un tsunami, la sensibilisation des populations côtières aux risques et aux gestes de survie, et la sécurisation de l'habitat permettent de sauver la plupart des vies humaines.

Au Japon, habitué à ce genre de catastrophes, les habitants ont pris des précautions systématiques. Ils ont mis en place un système doté d'ordinateurs très performants, système qui peut détecter la formation d'un tsunami, en déduire la hauteur des vagues ainsi que la vitesse de leur propagation et le moment où les vagues atteindront les côtes grâce à l'épicentre et à la magnitude du séisme. Ils transmettent aussi ces données aux pays du Pacifique, même à leurs concurrents, contrairement à la surveillance de l'océan Indien.

Il suffit généralement de s'éloigner de quelques centaines de mètres à quelques kilomètres des côtes ou d'atteindre un promontoire de quelques mètres à quelques dizaines de mètres de hauteur pour être épargné. La mise à l'abri ne prend donc que quelques minutes à un quart d'heure, aussi un système d'alerte au tsunami permet-il d'éviter la plupart des pertes humaines.

Un système de bouées adaptées à la réception des mouvements (capteurs de pression disposés sur les fonds océaniques) peut être installé le long des côtes et ainsi prévenir du danger.

Un dispositif de surveillance et d'alerte, utilisant une maille de sondes subocéaniques et traquant les séismes potentiellement déclencheurs de tsunamis, permet d'alerter les populations et les plagistes de l'arrivée d'un tsunami dans les pays donnant sur l'océan Pacifique : le Centre d'alerte des tsunamis dans le Pacifique, basé sur la plage d'Ewa à Hawaï, non loin d'Honolulu.

À Hawaï, où le phénomène est fréquent, les règlements d'urbanisme imposent que les constructions proches du rivage soient bâties sur pilotis.

À Malé, la capitale des Maldives, une rangée de tétrapodes en béton dépassant de le niveau de la mer est prévue pour diminuer l'impact des tsunamis.

La sensibilisation au phénomène et à ses dangers est également un facteur déterminant pour sauver des vies humaines, car toutes les côtes ne possèdent pas de système d'alarme - les côtes des Océans Atlantique et Indien en sont notamment dépourvues. De plus, certains tsunamis ne peuvent être détectés à temps (tsunamis locaux).

Deux indices annonçant la survenue possible d'un tsunami sont à reconnaître et impliquent qu'il faut se rendre en lieu sûr :

Si l'on est surpris par le tsunami, grimper sur le toit d'une habitation ou la cime d'un arbre solide, tenter de s'accrocher à un objet flottant que le tsunami charrie sont des solutions de dernier recours. En aucun cas, il n'est sûr de revenir auprès des côtes dans les heures suivant le tsunami car celui-ci peut être composé de plusieurs vagues espacées de quelques dizaines de minutes à plusieurs heures.

"Sources : voir Bibliographie thématique : prévention".

Un rapport publié par le PNUE suggère que le tsunami du 26 décembre 2004 a causé moins de dégâts dans les zones où des barrières naturelles, telles que les mangroves, les récifs coralliens ou la végétation côtière, étaient présentes. Une étude japonaise sur ce tsunami au Sri Lanka, établit à l’aide d’une modélisation sur image satellite, les paramètres de résistance côtière en fonction de différentes classes d’arbres.

En France Métropolitaine, le programme MAREMOTI financièrement soutenu par l'ANR dans le cadre de RiskNat 2008 et ayant débuté le 24 mars 2009. Il associe plusieurs disciplines : la marégraphie, l'observation historique et de traces de paléo-tsunamis d'événements anciens (aux Baléares et sur la côte Nord-Est Atlantique notamment), la modélisation (notamment pour la création d'outils d'alerte) et des études de vulnérabilité. Le CEA coordonne les dix partenaires (CEA/DASE, SHOM, université de La Rochelle, Noveltis, GEOLAB - Université Blaise Pascal, LGP - Université Paris 1, Géosciences Consultants, GESTER - Université Montpellier, Centro de Geofisica da Universidade de Lisboa (Portugal), Laboratoire de Géologie - ENS).

En outre-mer, le programme de recherche PREPARTOI s'intéresse à l'évaluation et la réduction du risque de tsunami à La Réunion et à Mayotte. Également pluridisciplinaire, ce projet se veut intégré et systémique, tout comme le programme MAREMOTI, apportant des solutions opérationnelles aux services de l'État.

Le CENALT, le centre d'alerte aux tsunamis pour l'Atlantique Nord-Est et la Méditerranée occidentale est opérationnel depuis juillet 2012 à Bruyères-le-Chatel.

Glissements de terrain et éruptions volcaniques peuvent déclencher des tsunamis dans des lacs et des fleuves.

On définit comme mégatsunami un tsunami dont la hauteur au niveau des côtes dépasse cent mètres. Un mégatsunami, s'il se propage librement dans l'océan, est capable de provoquer des dégâts majeurs à l'échelle de continents entiers. Les séismes étant incapables "a priori" d'engendrer de telles vagues, seuls des événements cataclysmiques, tels un impact météoritique de grande ampleur ou l'effondrement d'une montagne dans la mer, en sont la cause possible.

Aucun mégatsunami non local n'a été rapporté dans l'histoire de l'humanité. Notamment, l'explosion du Krakatoa en 1883 et l'effondrement de Santorin dans l'Antiquité n'en ont pas produit.

Les causes possibles d'un mégatsunami sont des phénomènes "rares", espacés d'échelles de temps géologiques — au bas mot plusieurs dizaines de milliers d'années, si ce n'est des millions d'années. Certains scientifiques estiment cependant qu'un mégatsunami aurait récemment été provoqué par l'effondrement du Piton de la Fournaise sur lui-même, à La Réunion : l'événement remonterait à environ.

Les glissements de terrain produisent des tsunamis de courte période qui ne peuvent se propager sur plusieurs milliers de kilomètres sans dissiper leur énergie. Par exemple, lors des glissements de terrain à Hawaï en 1868 sur le Mauna Loa et en 1975 sur le Kīlauea, des tsunamis locaux importants furent créés, sans que les côtes américaines ou asiatiques distantes fussent inquiétées.

Le risque de mégatsunami reste cependant médiatisé et surévalué. Des modèles controversés prédisent en effet deux sources possibles de mégatsunami dans les prochains millénaires : sont envisagés un effondrement le long des flancs du Cumbre Vieja aux Canaries (mettant la côte est du continent américain en danger) et un autre au Kīlauea à Hawaï (menaçant la côte ouest de l'Amérique et celles de l'Asie). Des études plus récentes remettent en cause le risque d'effondrement sur les flancs de ces volcans, d'une part, et le caractère non local des tsunamis engendrés, d'autre part.

"Sources : Bibliographie thématique : mégatsunamis".

Dans la mythologie grecque, à l'occasion de son témoignage sur la vengeance des dieux contre le roi de Troie Laomédon (~) et le sacrifice d'Hésione, le poète romain Ovide identifie le monstre marin divin Céto à une inondation. D'autres auteurs, comme Valérius Flaccus y joigne un bruit de tremblement de terre. L'un dans l'autre suggérant un tsunami.

Des tsunamis surviennent quasiment chaque année dans le monde. Les plus violents peuvent changer le cours de l'histoire. Par exemple, des archéologues ont avancé qu'un raz de marée en mer Méditerranée a ravagé la côte nord de la Crète, il y a un peu plus de ans ; ce désastre aurait marqué le début de la décadence de la civilisation minoenne, l'une des plus raffinée de l'Antiquité.

L'historien grec Thucydide fut le premier à établir un lien entre tremblements de terre et tsunamis, au . Il avait noté que le premier indice d'un raz de marée est souvent le soudain retrait des eaux d'un port, tandis que la mer s'éloigne de la côte.

Au , dix tsunamis par an furent enregistrés, dont un et demi par an a provoqué des dégâts ou des pertes humaines. Sur cette période d'un siècle, sept provoquèrent plus d'un millier de morts, soit moins d'un tous les dix ans.

80 % des tsunamis enregistrés le sont dans l'océan Pacifique ; parmi les huit tsunamis ayant causé plus d'un millier de victimes depuis 1900, seul le tsunami du 26 décembre 2004 n'a pas eu lieu dans l'océan Pacifique.

"Sources : voir Bibliographie thématique : statistiques sur les tsunamis".

En pleine mer, le tsunami se comporte comme la houle : c'est une onde à propagation elliptique, c'est-à-dire que les particules d'eau sont animées d'un mouvement elliptique à son passage. Il n'y a (presque) pas de déplacement global de l'eau, une particule retrouve sa position initiale après le passage du tsunami. La figure 2 illustre le déplacement des particules d'eau au passage de la vague.

Mais, contrairement à la houle, le tsunami provoque une oscillation de l'eau aussi bien en surface (un objet flottant est animé d'un mouvement elliptique à son passage, cf. point rouge du haut sur la Fig. 2) qu'en profondeur (l'eau est animée d'une oscillation horizontale dans le sens de la propagation de l'onde, voir le point rouge du bas sur la Fig. 2). Ce fait est lié à la grande longueur d'onde du tsunami, typiquement quelques centaines de kilomètres, qui est très supérieure à la profondeur de l'océan - une dizaine de kilomètres tout au plus. Il en résulte que la quantité d'eau mise en mouvement est bien supérieure à ce que la houle produit ; aussi le tsunami transporte-t-il beaucoup plus d'énergie que la houle.

Les vagues ordinaires de l'océan sont de simples rides formées à sa surface par le vent. Mais un tsunami déplace une colonne d'eau tout entière, depuis le plancher océanique jusqu'en haut. La perturbation initiale se propage dans des directions opposées à partir de la faille, dans de longs fronts de houle parfois séparés les uns des autres par . Ceux-ci se remarquent à peine au large, en eaux profondes. Ils n'atteignent des hauteurs redoutables qu'en eaux peu profondes, quand ils se cumulent à l'approche d'une côte.

Un tsunami possède deux paramètres fondamentaux :

Ces paramètres sont sensiblement constants au cours de la propagation du tsunami, dont la perte d'énergie par friction est faible du fait de sa grande longueur d'onde.

Les tsunamis d'origine tectonique ont des périodes longues, généralement entre une dizaine de minutes et plus d'une heure. Les tsunamis créés par des glissements de terrain ou l'effondrement d'un volcan ont souvent des périodes plus courtes, de quelques minutes à un quart d'heure.

Les autres propriétés du tsunami comme la hauteur de la vague, la longueur d'onde (distance entre les crêtes) ou la vitesse de propagation sont des quantités variables qui dépendent de la bathymétrie et/ou des paramètres fondamentaux formula_5 et formula_6.

La plupart des tsunamis ont une longueur d'onde supérieure à la centaine de kilomètres, bien supérieure à la profondeur des océans qui ne dépasse guère , de sorte que leur propagation est celle d'une vague en milieu « peu profond ». La longueur d'onde formula_9 dépend alors de la période formula_6 et de la profondeur de l'eau formula_11 selon la relation :
où formula_13 est la gravité, ce qui donne numériquement
La période spatiale ou longueur d'onde est le plus souvent comprise entre (période de 10 min et profondeur de ), typique des tsunamis locaux non tectoniques, et (période de 60 min et profondeur de ), typique des tsunamis d'origine tectonique.

Pour les tsunamis de période suffisamment longue, typiquement une dizaine de minutes, soit la plupart des tsunamis d'origine tectonique, la vitesse formula_15 de déplacement d'un tsunami est fonction de la seule profondeur d'eau formula_11 :
Cette formule peut être utilisée pour obtenir une application numérique :
ce qui signifie que la vitesse est de pour une profondeur de et de pour une profondeur d'un kilomètre. La figure 4. illustre la variabilité de la vitesse d'un tsunami, en particulier le ralentissement de la vague en milieu peu profond, notamment à l'approche des côtes.

De la variabilité de cette vitesse de propagation, il résulte une réfraction de la vague dans les zones peu profondes. Ainsi, le tsunami a rarement l'allure d'une onde circulaire centrée sur le point d'origine, comme le montre la Fig. 5. Toutefois, l'heure d'arrivée d'un tsunami sur les différentes côtes est prévisible puisque la bathymétrie des océans est bien connue. Cela permet d'organiser au mieux l'évacuation lorsqu'un système de surveillance et d'alerte est en place.

Il est ainsi possible de calculer et de retracer les temps de parcours de différents tsunamis historiques à travers un océan comme le fait le National Geophysical Data Center

Pour des tsunamis de longue période, qui présentent peu de dissipation d'énergie même sur de grandes distances, l'amplitude formula_19 du tsunami est donnée par la relation :

Pour les tsunamis de faible période (souvent ceux d'origine non sismique) la décroissance avec la distance peut être beaucoup plus rapide.

Lorsque le tsunami s'approche des côtes sa période et sa vitesse diminuent, son amplitude augmente. Lorsque l'amplitude du tsunami devient non négligeable par rapport à la profondeur de l'eau, une partie de la vitesse d'oscillation de l'eau se transforme en un mouvement horizontal global, appelé courant de Stokes. Sur les côtes, c'est davantage ce mouvement horizontal et rapide (typiquement plusieurs dizaines de km/h) qui est la cause des dégâts que l'élévation du niveau de l'eau.

À l'approche des côtes, le courant de Stokes d'un tsunami a pour vitesse théorique
soit

Cependant, contrairement à la propagation en haute mer, les effets d'un tsunami sur les côtes sont difficiles à prévoir, car de nombreux phénomènes peuvent avoir lieu.

Contre une falaise, par exemple, le tsunami peut être fortement réfléchi ; à son passage on observe une onde stationnaire dans laquelle l'eau a essentiellement un mouvement vertical.

Les derniers tsunamis vraiment importants de la période historique ont concerné la mer Méditerranée et datent de l'Antiquité : le premier récit historique d'un tsunami est fait par Hérodote dans son "Enquête" lors de la prise de la ville de Potidée par le général perse Artabaze en -479 lors des guerres médiques. Ils peuvent aussi naître dans la mer du nord située au-dessus de ce qui a été la jonction de trois plaques tectoniques continentales dans la première période de l'ère paléozoïque (des mouvements et failles résiduels peuvent encore provoquer des tremblements de terre et les tsunamis de petite taille). Quelques petits tsunamis semblent avoir eu lieu durant les vingt derniers siècles dans le pas de Calais, notamment lors du tremblement de terre de 1580.

La France Métropolitaine a connu des tsunamis de faible amplitude en 1564 et 1887 sur la côte d'Azur, en 1755 sur la façade ouest de la Corse et en Atlantique à la suite du séisme de Lisbonne, en 1846 sur la région de Marseille et en 1986 aux Saintes-Maries-de-la-Mer, le raz-de-marée des Saintes-Maries-de-la-Mer. En 1979, l'effondrement d'une partie du remblai de l'aéroport de Nice (construction d'un port de commerce) cause un tsunami local inondant les quartiers de la Garoupe et de La Salis à Antibes, atteignant à La Salis une altitude . Plus récemment, le 21 mai 2003, le séisme de Boumerdès-Zemmouri (Algérie) a engendré un tsunami affectant les côtes françaises de la Méditerranée, entraînant la perte de plusieurs embarcations. Il a notamment affecté les ports du Lavandou, de Fréjus, de Saint-Raphaël, de la Figueirette, de Cannes, d'Antibes ou encore de Menton.

L'outre-mer est bien plus exposé à l'aléa tsunami que la France Métropolitaine de par la localisation de ses territoires et départements dans des bassins océaniques plus propices au déclenchement de tsunami par des séismes de forte magnitude, notamment dans les zones de subduction. De nombreux catalogues de ces tsunamis existent dans la littérature scientifique pour la Polynésie française, la Guadeloupe, la Martinique ou encore la Nouvelle-Calédonie. À noter l'événement meurtrier du 28 mars 1875, tuant 25 personnes sur l'île de Lifou en Nouvelle-Calédonie.






</doc>
<doc id="3003" url="https://fr.wikipedia.org/wiki?curid=3003" title="Tinos">
Tinos

Tinos (en grec moderne ) est une île du nord des Cyclades grecques, dans la mer Égée méridionale. Elle se situe entre Andros et Mykonos. Son port principal et sa capitale, Tinos, est dominé par l'imposant rocher de l'Exombourgo (). Ses habitants sont appelés les "Tiniotes".

Tinos est peu fréquentée par les touristes étrangers, faute de monuments antiques connus et de belles plages. Ses côtes sont principalement rocheuses mais elle compte cependant quelques plages de sable fin. Par ailleurs, son exposition aux vents du nord, principalement le "meltem" en été, ne joue pas en sa faveur ; cette caractéristique a même fait d'elle, dans la mythologie antique, la demeure d'Éole.

Pour les Grecs orthodoxes, Tinos est un très important lieu de pèlerinage marial , qui attire une foule importante le 25 mars (fête de l'Annonciation) et surtout le 15 août (fête de la Dormition). Elle abrite également une communauté catholique importante, héritage de son histoire.

L'île est divisée en trois grandes régions :

Depuis les élections d'octobre 2010, les 3 municipalités attachées à ces 3 régions, ne font plus qu'une seule, avec un seul maire qui officie dans le port de Tinos.

La plupart des chrétiens orthodoxes de l’ile dépendent de la métropole de Syros, Tinos, Andros, Kéa, Milo et Myconos et les catholiques de l'archidiocèse de Naxos et Tinos .

L'eau est abondante sur Tinos, contrairement aux autres Cyclades, mais le vent contrarie l'activité agricole, sauf dans les vallées protégées, comme celles de Livadia, Agapi ou Potamia (autour d'un « fleuve » comme son nom l'indique) et sur la plaine de Komi. Ces régions de maraîchage ont fourni Athènes en produits frais jusqu'aux années 1950. L'île exporte aussi encore son marbre (utilisé au palais de Buckingham et au Louvre) et du talc.

Tinos fait pourtant partie des îles des Cyclades qui ne sont pas autosuffisantes en eau. Elle reçoit de l'eau tous les ans (et surtout l'été à cause de la saison touristique) depuis le port du Laurion en Attique, pour un coût moyen de le mètre cube.

Tinos a la forme d'une « poire » tournée vers le nord-ouest. Elle est longue de , son littoral fait et le tour de l'île s'effectue en 38 milles marins. Sa base, au sud-est vers Mykonos, fait . Elle est séparée de cette île par un détroit de . Un bras de mer de moins d'un mille marin la sépare d'Andros. Le plus haut sommet de l'île est le Tsiknia ().

L'île est formée de roches métamorphiques plissées et érodées. Ses côtes sud-ouest et est sont rectilignes et escarpées, sauf au niveau de Tinos-ville. La côte nord-est a les baies les plus profondes : Panormos, Kolymbithra et Livada. L'île peut-être décomposée en trois ensembles géologiques :

La neige et le gel sont rares sur Tinos. Les régions les plus hautes (plateaux de Falatados-Sténi-Messi et d'Ysternia-Panormos) et Xombourgo sont en revanche régulièrement touchées par le brouillard. La principale caractéristique de l'île est le vent qui souffle du nord. Le meltem a pour effet de rafraîchir les chaudes journées d'été.

La réforme "Kallikratis" de 2010 a vu la fusion des 3 anciennes municipalités de l'île : les dèmes de Tinos et Exombourgo, et la communauté de Panormos. L'île forme donc actuellement un dème avec pour chef-lieu la localité de Tinos-ville et pour « capitale historique » la localité de Panormos. Elle forme également un district régional de la périphérie d'Égée-Méridionale.

Tinos aurait été, selon la mythologie, l'île d'Éole, le dieu du vent. Cette légende est justifiée par la puissance avec laquelle souffle le vent, et surtout le meltem, sur l'île.

Une autre légende fait référence aux serpents - encore présents - vipères et couleuvres - qui mangent des petits rongeurs. Appelée "Ophioussa" en raison des nombreux serpents qu'on y trouvait, elle changea de nom lorsque Poséidon envoya des cigognes nettoyer l'île.

L'île aurait été appelée à cette époque "Idroussa" en raison de l'abondance de ses sources. Athénée fait référence à une source miraculeuse ne se mélangeant pas au vin. Pline l'Ancien écrit à propose de l'île : « À pas d'Andros et à de Délos est Ténos, avec sa ville ; elle s'étend dans une longueur de pas ; d'après Aristote, elle fut appelée Hydroussa à cause de l'abondance de ses eaux ; d'après d'autres, Ophioussa. ».

Les premiers habitants de l'île sont peu connus : les hypothèses phrygienne, phénicienne, carienne, pélasge ou lélège sont avancées. Les Ioniens auraient colonisé Tinos vers 1000 avant notre ère.

Les traces d'occupation les plus anciennes sont mycéniennes : on a retrouvé deux tombeaux mycéniens ou géométriques dans la région de Kyra-Xéni.

Une cinquantaine d'habitats de l'époque géométrique ont été recensés, dont celui au sommet de Xombourgo qui était une véritable ville fortfiée.

Au , elle était dominée par Érétrie, cité de l'île d'Eubée. La domination athénienne se fit sentir à partir de Pisistrate à qui on a autrefois attribué un des principaux aqueducs de l'île vers 549-542 avant l'ère commune; depuis la découverte que la cité de la période archaïque se situait à l'Exombourgo et non en bord de mer, cet aqueduc est maintenant daté de la fin du , comme le reste de la ville basse.

En 480 avant l'ère commune, les Perses de Xerxès s'en emparèrent et obligèrent l'île à fournir des navires contre la Grèce. Un de ces navires déserta juste avant Salamine et informa les Grecs des intentions perses. Pour cette raison, Tinos eut le droit d'avoir son nom sur le trépied de Delphes, dont la partie restante, la colonne serpentine, subsiste toujours à Constantinople.

D’après Démosthène et Diodore de Sicile, le tyran thessalien Alexandre de Phères mena des opérations de pirateries dans les Cyclades vers 362-360 avant l’ère commune. Ses navires se seraient emparés de quelques-unes des îles, dont Tinos, et auraient emporté un grand nombre d’esclaves. Ce fut peu de temps après cette expédition pirate que la ville haute ("polis") sur le Xombourgo aurait été abandonnée et que la ville basse ("asty"), à l'emplacement actuel de Tinos-ville, aurait été créée.

Vers 315 avant l'ère commune, Antigonos Monophtalmos organisa les Cyclades dans une Ligue des Nésiotes, un État fédéral au service des Antigonides, sur lequel Démétrios Poliorcète se serait appuyé pour ses campagnes navales.

Les Ptolémées établirent un protectorat sur cette ligue vers - 295. Mais, vaincus à Andros vers le milieu du , ils perdirent leur influence dans les îles. Après Cynocéphales, les îles passèrent aux Rhodiens puis aux Romains. Les Rhodiens réorganisèrent la Ligue des Nésiotes, dont le centre politique était probablement Tinos, choisie vraisemblablement grâce à la renommée et aux privilèges diplomatiques de son sanctuaire consacré à Poséidon et Amphitrite (l'île de Délos et sanctuaire d'Apollon étant à cette époque une cité neutre).

Dans son ouvrage sur Tinos, Roland Étienne évoque une société tiniote dominée par une « aristocratie » agrarienne et patriarchale marquée par une forte endogamie. Ces quelques familles avaient beaucoup d'enfants et tiraient une partie de leurs ressources d'une exploitation financière de la terre (ventes, emprunts, etc.), que R. Étienne qualifie d'« affairisme rural ». Ce « marché de l'immobilier » était dynamique à cause du nombre d'héritiers et du partage du patrimoine au moment des héritages. Il n'y avait pas d'autre solution que l'achat et la vente de terres pour se constituer un patrimoine cohérent. Une partie de ces ressources financières pouvait être aussi investie dans les activités commerciales.

En 1204, la croisade s'empara de Constantinople, et les vainqueurs se partagèrent l'Empire byzantin. Alors que la souveraineté nominale sur la plupart des Cyclades avait échu aux Croisés, Tinos avait été théoriquement attribuée à l'empereur latin de Constantinople, qui ne put cependant en prendre directement possession. Un accord entre ce dernier et Venise fut conclu, autorisant des citoyens vénitiens agissant à titre privé à conquérir les îles, dont ils feraient hommage à l'Empereur. Cette nouvelle suscita des vocations. De nombreux aventuriers armèrent des flottes à leurs frais, dont Marco Sanudo, neveu du doge Enrico Dandolo. En 1207, il contrôlait les Cyclades avec ses compagnons et parents. Les frères Andrea et Geremia Ghisi devinrent maîtres de Tinos, Mykonos et des Sporades et vassaux de l'empereur latin de Constantinople ; ils ne dépendaient pas du duché. Les Latins imposèrent le système féodal occidental sur les îles qu'ils dominaient.

La coutume de la principauté de Morée, les "Assises de Romanie", devint rapidement la base de la législation dans les îles. Le système féodal fut appliqué même pour les plus petites propriétés, ce qui eut pour effet de créer une importante « élite locale ». Les « nobles francs » reproduisirent la vie seigneuriale qu'ils avaient laissée derrière eux : ils se construisirent des « châteaux » où ils entretinrent une cour. Aux liens de vassalité s'ajoutèrent ceux du mariage. Les fiefs circulèrent et se fragmentèrent au fil des dots et des héritages.
Cependant, ce système féodal « franc » (comme on appelait tout ce qui venait d'Occident à l'époque) se surimposa au système administratif byzantin, conservé par les nouveaux seigneurs : les taxes et corvées féodales étaient appliquées aux divisions administratives byzantines et l'exploitation des fiefs continuait selon les techniques byzantines. Il en était de même pour la religion : si la hiérarchie catholique dominait, la hiérarchie orthodoxe subsistait et parfois, lorsque le curé catholique n'était pas disponible, la messe était célébrée par le pope orthodoxe. Les deux cultures se mêlèrent étroitement.

En 1292, Roger de Lauria ravagea Tinos (ainsi qu'Andros, Mykonos et Kythnos), peut-être une conséquence de la guerre qui faisait rage entre Venise et Gênes. En 1390, les anciens fiefs des Ghisi (Tinos et Mykonos) passèrent sous la domination directe de la République de Venise qui conserva l'île jusqu’à la conquête ottomane de 1715, dernier territoire grec à être conquis. Cette longue domination vénitienne explique la forte communauté catholique de l'île.

En 1822, une nonne du couvent Kekrovounio fit un rêve lui indiquant où était cachée une icône miraculeuse de la Vierge à l'Enfant, peinte, selon la légende, par l'Évangéliste Luc lui-même. On attribua rapidement à cette icône des vertus curatives et des milliers de malades vinrent en pèlerinage dans l’île pour s’y faire guérir. En 1915, lorsque le roi Constantin de Grèce fut atteint d’une pleurésie aggravée d’une pneumonie, le gouvernement envoya un navire à Tinos pour y chercher l’image sainte et la placer dans la chambre du souverain. Alors que Constantin avait déjà reçu les derniers sacrements, son état s’améliora progressivement après qu’il eût embrassé l’icône. En guise d’ex voto, la reine Sophie de Grèce fit alors don d’un saphir pour enrichir l’icône.

L'attaque italienne contre la Grèce avait été précédée du torpillage du croiseur Elli, un navire symbolique pour la Grèce, en baie de Tinos, le .
L'attaque allemande d'avril 1941 entraîna la défaite totale et l'occupation de la Grèce dès la fin de ce mois. Cependant, les Cyclades furent occupées tardivement et plus par les troupes italiennes que par les troupes allemandes. Les premières troupes d'occupation firent leur apparition le : Tinos fut occupée par des Italiens. Cela permit aux îles de servir d'étape aux personnalités politiques allant se réfugier en Égypte pour continuer la lutte. Georges Papandréou et Constantin Karamanlís s'arrêtèrent ainsi sur l'île avant de rejoindre Alexandrie.

À la suite de la reddition italienne, l'OKW donna l'ordre le aux commandants des unités du secteur de la Méditerranée de neutraliser, par la force si nécessaire, les unités italiennes. Le , Hitler ordonna d'occuper toutes les îles de l'Égée contrôlées par les Italiens.

Comme le reste du pays, les Cyclades eurent à souffrir de la famine organisée par l'occupant allemand. Ainsi, sur Tinos, on considère que 327 personnes dans la ville de Tinos et autour de 900 dans la région de Panormos moururent de faim lors du conflit.

Au recensement de 2011, l'île compte habitants.





</doc>
<doc id="3004" url="https://fr.wikipedia.org/wiki?curid=3004" title="Tanzanie">
Tanzanie

La Tanzanie, en forme longue la République unie de Tanzanie ou la République-Unie de Tanzanie, en swahili et , en anglais et , est un pays d'Afrique de l'Est situé en bordure de l'océan Indien, dans la partie tropicale de l'Hémisphère sud. Il est entouré au nord par le Kenya et l'Ouganda, à l'ouest par le Rwanda, le Burundi et la République démocratique du Congo, au sud-ouest par la Zambie et le Malawi et au sud par le Mozambique. Le pays couvre et compte plus de 51 millions d'habitants en 2015, essentiellement bantous. Sa capitale est Dodoma, située dans l'intérieur des terres, mais le principal pôle économique est l'ancienne capitale Dar es Salam, située sur la côte. Les langues officielles sont le swahili et l'anglais mais l'arabe est aussi parlé notamment dans les îles de Zanzibar et Pemba.

La Tanzanie actuelle est née de l'union du Tanganyika et de Zanzibar le , peu de temps après leur indépendance respective vis-à-vis du Royaume-Uni. Elle est membre du Commonwealth depuis fin 1961 et des Nations unies depuis le .

En 1953, Julius Nyerere, un enseignant né en 1922, passé par l'université de Makerere en Ouganda puis Édimbourg au début des années 1950 pour terminer ses études, prend à 31 ans la tête de la TAA (Tanganyika African Association), qu’il transforme rapidement en un véritable parti politique – le Tanganyika African National Union (TANU) – qui prône l’indépendance. Celle-ci est accordée par le Royaume-Uni le , sans aucune violence. Julius Nyerere est un court temps Premier ministre, puis à la suite des élections de décembre 1962, devient le premier président de la République du Tanganyika.

L’indépendance de Zanzibar et Pemba est obtenue le 10 décembre 1963. Le nouvel État commence par être contrôlé par les partis initiés par les Britanniques (une coalition du ZNP et de petits partis de Pemba). Mais, à peine un mois plus tard, en janvier 1964, les tensions communautaires qui couvent depuis des années se libèrent et le parti ASP (Afro-Shirazi Party), étant écarté depuis longtemps du pouvoir alors qu’il est majoritaire dans les urnes, déclenche une révolution. Celle-ci fait de nombreuses victimes dans les rangs des communautés arabes et indiennes. On estime à environ le nombre de personnes qui furent massacrées dans la nuit du 11 au 12 janvier 1964 à Zanzibar. À la suite de ce renversement, Sheikh Abeid Karume, chef de l’ASP, devient président de la République de Zanzibar.

Le 26 avril 1964, le Tanganyika et Zanzibar fusionnent pour former la République unie de Tanzanie. Nyerere devient le président de l’État nouvellement créé, tandis qu'Abeid Karume, restant président de Zanzibar, devient le vice-président de la Tanzanie. Dans les faits, même si l’union est bien célébrée avec le reste du pays, Zanzibar a conservé jusqu'à aujourd’hui une large autonomie. En pratique, c’est le gouvernement central tanzanien qui s’occupe des domaines « nationaux » de la politique à Zanzibar : Défense, Intérieur, Affaires étrangères, tandis que le gouvernement local zanzibarite traite des sujets comme l’éducation ou l’économie.

Soucieux d’accélérer l’émancipation des Africains par rapport au monde occidental, inspiré des expériences communistes en Chine, Nyerere s’engage résolument dans une politique socialiste. En février 1967, lors de la déclaration d'Arusha, il définit les principes et doctrines qu’il souhaite voir suivre par le pays. Selon l’idéal de Nyerere, le socialisme africain doit conduire à la création d’une société égalitaire, juste, solidaire, qui trouve dans ses propres ressources les moyens de son autosuffisance. L’éducation est la priorité numéro un. Il faut dire qu’il y a urgence dans ce domaine : la Tanzanie ne produit à cette époque que 120 diplômés par an.

Les premières mesures concrètes d’application de cette politique ne tardent pas à arriver. Les principales industries et sociétés de services sont nationalisées, les impôts augmentés pour une plus grande répartition des richesses et les discriminations raciales abolies. C’est sur le plan de l’agriculture, principal secteur économique du pays, que les changements sont les plus forts. Appelé Ujamaas, c’est-à-dire cofraternité, des communautés villageoises sont organisées sur des principes collectivistes. Des incitations financières encouragent la formation de coopératives. En octobre 1969, Bibi Titi Mohammed et l'ex-ministre Michael Kamaliza sont arrêtés avec quatre officiers de l'armée. Ils sont accusés d'avoir fomenté un coup d'État. À Zanzibar, l’Afro-Shirazi Party mène une politique autoritaire, à tendance ouvertement révolutionnaire. Les propriétés arabes et indiennes sont nationalisées. Quelques désaccords apparaissent même entre Nyerere et Karume, ce dernier voulant se rapprocher davantage du monde communiste que le président tanzanien qui cherche, lui, à ménager au maximum les relations avec les Occidentaux. En 1972, Karume est assassiné par des opposants au régime. Des obsèques nationales lui sont rendues, en présence de Julius Nyerere.

En politique extérieure, la Tanzanie donne son appui à la guérilla lumumbiste au Congo et l'OUA établit son siège à Dar es Salaam et plusieurs mouvements révolutionnaires ont une représentation dans le pays (l'ANC, la ZANU, la SWAPO, le MPLA et le FRELIMO). Parallèlement, les relations se détériorent avec les pays occidentaux ; en 1965 la Tanzanie rompt ses relations avec le Royaume-Uni et expulse hors du pays les troupes britanniques en réaction au soutien de Londres à un régime ségrégationniste en Rhodésie, tandis que l’Allemagne de l'Ouest rompt ses propres relations avec la Tanzanie suite à l'ouverture dans le pays d'une ambassade de l’Allemagne de l'Est. Les aides économiques accordées par certains pays occidentaux sont coupées. D'autre part, les forces coloniales portugaises bombardent le sud du pays pour couper les voies d’approvisionnement du FRELIMO mozambicain, soutenu par le gouvernement de Julius Nyerere.

Pendant ces années, la Tanzanie reçoit l’aide de la Chine, bien qu'étant elle-même en voie de développement. C’est avec un soutien chinois que la ligne de chemin de fer TAZARA de Dar-es-Salaam à la Zambie est construite en 1975. C’est aussi sur le modèle des communes chinoises que sont créés 800 villages collectifs, regroupant des populations d’origine ethniques et tribales différentes, et déplacées de force en camion. On estime qu’en 4 ans, de 1973 à 1976, 9 millions de personnes sont ainsi déplacées. Cette politique, si elle permet un certain brassage entre les différentes ethnies qui composent la population tanzanienne, casse brutalement les repères humains et communautaires des individus.

Ces politiques dirigistes et utopiques apportent de moins en moins les résultats escomptés. Le premier choc pétrolier de 1973 assombrit fortement les perspectives économiques du pays. Les productions manufacturière et agricole régressent, la planification de l’économie par l’administration est inefficace. Sur le plan politique, les partis TANU de Nyerere et l’ASP se rapprochent et fusionnent en 1977 pour former le Chama cha Mapinduzi (CCM), c’est-à-dire le parti de la Révolution. Malgré les difficultés économiques, le pays est en paix et reçoit de nombreux réfugiés venus des pays voisins en guerre ou fuyant le régime d'Amin Dada en Ouganda. Nyerere refuse que la politique d'africanisation de l’administration ne favorise que les seuls Tanzaniens et autorise l'accès aux emplois publics aux étrangers. Beaucoup obtiennent également la nationalité tanzanienne, y compris des réfugiés blancs.

Les relations de la Tanzanie avec ses voisins africains, en particulier ceux du nord, Ouganda et Kenya, se détériorent au fil des années. Les intentions étaient pourtant bonnes puisque ces trois pays ont formé en 1967 l"East Africain Community" (Communauté d'Afrique de l'Est) dans le but de constituer à terme un marché économique commun. Les premières coopérations visent notamment à uniformiser la politique des changes et de contrôle des devises. Mais le Kenya, proche des pays occidentaux, s’éloigne de plus en plus de la Tanzanie soutenue par les communistes chinois, et la frontière entre ces deux pays est même fermée de 1977 à 1983. En Ouganda, le dirigeant Idi Amin Dada, qui nourrit des ambitions d’expansions territoriales, reproche à son voisin tanzanien d’héberger des opposants à son régime. L’Ouganda attaque la Tanzanie à la fin de l’année 1978, et envahit les environs du lac Victoria. Les Tanzaniens, avec l’aide du matériel militaire chinois, parviennent, au bout de plusieurs mois d’efforts et au prix de lourdes pertes humaines, à reprendre les territoires perdus et occupent même l’Ouganda pendant presque deux ans.

La guerre a coûté cher, environ 500 millions de dollars, et au début des années 1980, sans réelle industrie, avec un secteur agricole improductif, la Tanzanie est l’un des pays les plus pauvres de la planète. Le pays s’enfonçant dans l’échec, Nyerere commence à modifier progressivement sa politique dirigiste menée depuis le milieu des années 1960. Avec l’intervention de plus en plus grande de la Banque mondiale et du FMI, les incitations financières à la production collectiviste sont en partie réorientées vers un investissement pour les grandes fermes de l’État et pour les infrastructures routières. En 1984, la possibilité d’une propriété privée des moyens de production apparaît et la société est, très progressivement, libéralisée.

En 1985, Nyerere, le « mwalimu » (l’instituteur), choisit, contrairement à l’habitude prise par la plupart des autres chefs d’État africains, de se retirer de la politique, après avoir tout de même conservé le pouvoir pendant 24 années. C’est Ali Hassan Mwinyi, alors président depuis 1980 de l’archipel de Zanzibar, qui prend sa succession. Malgré les résultats très largement négatifs de sa politique de développement économique, Nyerere conserva jusqu’à sa mort en 1999 l’estime de beaucoup de Tanzaniens et d’une partie de la communauté internationale. On lui reconnaît en effet le mérite d’avoir posé les bases d’un État démocratique pluriethnique.

Ali Hassan Mwinyi accélère l’ouverture et la libéralisation progressive du pays. En 1992, il autorise le multipartisme. En 1995, les premières élections multipartites ont lieu, même si elles sont entachées de sérieux doutes sur leur régularité. Elles voient la victoire de Benjamin William Mkapa, un des disciples de Nyerere, qui est réélu en 2000. Mkapa doit faire face à de nombreuses difficultés qui grèvent le décollage tant espéré du pays : crise économique, épidémie du SIDA ou encore afflux de réfugiés qui fuient les guerres du Burundi.

À Zanzibar, des velléités indépendantistes émergent parfois, mais jusqu’à présent, l’Union tanzanienne est préservée.

En 1998, des attentats visent les ambassades américaines de Dar es-Salaam et de Nairobi au Kenya : on compte plus de 250 victimes et blessés.

Après les élections de décembre 2005, Jakaya Kikwete devient le nouveau président de la République, le quatrième depuis la création de la Tanzanie. John Magufuli lui succède le 5 novembre 2015 et devient ainsi le cinquième président de la République de Tanzanie.

La Tanzanie est un pays de l'hémisphère sud situé en Afrique de l'Est. Ses frontières naturelles sont formées par l'océan Indien à l'est, le Kilimandjaro et le lac Victoria au nord, la rivière Kagera au nord-ouest, le lac Tanganyika à l'ouest, le Malawi au sud-ouest et le fleuve Ruvuma au sud. Il a des frontières terrestres avec le Kenya et l'Ouganda au nord, le Rwanda et le Burundi au nord-ouest, la République démocratique du Congo à l'ouest, la Zambie et le Malawi au sud-ouest et le Mozambique au sud.
Le pays est traversé par la vallée du grand rift qui parcourt la partie occidentale du pays du nord au sud et dans laquelle se logent quelques-uns des grands lacs africains : lac Malawi, lac Rukwa, lac Tanganyika, lac Victoria, lac Eyasi, lac Manyara, lac Natron, etc. Le centre du pays est constitué d'un plateau drainé par des rivières et des fleuves qui se jettent dans l'océan Indien. La façade maritime du pays est formée d'une plaine côtière qui fait face à l'archipel de Zanzibar formé de trois principales îles : Unguja, Pemba et Mafia.

La Tanzanie comporte plusieurs volcans dont un seul, l'Ol Doinyo Lengaï, est encore actif et un autre constitue le point culminant du continent africain, le Kilimandjaro, avec d'altitude.

On y trouve de nombreux parcs naturels tels que l'énorme terrain du Serengeti, le Tarangire, le Lac Manyara ou le magnifique cratère du N'Gorongoro, où on peut avoir la chance d'apercevoir lions, éléphants, rhinocéros, guépards, léopards, hyènes, chacals, girafes, hippopotames, crocodiles Ces aires protégées sont cependant victimes du braconnage, qui vise essentiellement les éléphants.

La Tanzanie est une république fédérale multipartite présidentielle née le de la fusion du Tanganyika et de Zanzibar qui avaient accédé à l'indépendance respectivement le et le après les colonisations allemande puis britannique. Le nom de "Tanzanie" est formé du "Tan" de Tanganyika et du "zan" de Zanzibar.

La constitution actuelle, adoptée le , a été révisée en .

Le Président de la République, qui est aussi le Chef de l'État, est élu au suffrage universel direct pour un mandat de cinq ans. Celui-ci nomme le Premier ministre, qui représente le gouvernement devant le Parlement, et les ministres parmi les membres du Parlement ou parmi dix personnes non élues.

L'Assemblée nationale, le parlement monocaméral, compte 274 sièges, adopte les lois s'appliquant à la totalité de la République ou uniquement aux 21 régions continentales (île de Mafia comprise), les cinq régions formant le Gouvernement révolutionnaire de Zanzibar pouvant adopter certaines lois spécifiques car disposant de son propre Parlement. Les parlementaires sont élus au suffrage universel direct pour un mandat de cinq ans.

Le pouvoir judiciaire est sensiblement plus compliqué car disposant de cinq niveaux combinant les institutions tribales, islamiques et de la Common law : tribunaux de première instance, tribunaux de districts, Cour Magistrale, Haute Cour et enfin Cour d’Appel.

La récente création de la Tanzanie par l'union de deux pays tend constamment à le déstabiliser mais la démocratie est préservée grâce à l'héritage de son premier Président, le charismatique Julius Nyerere, qui est resté trente ans au pouvoir sans installer de régime autoritaire ou dictatorial. Cette stabilité a permis par exemple l'installation du Tribunal pénal international pour le Rwanda à Arusha, chargé de juger les criminels de guerre lors du génocide rwandais de 1994.

La Tanzanie est divisée en 26 régions (21 dites "continentales" et 5 dites "insulaires" formant le Gouvernement révolutionnaire de Zanzibar), elles-mêmes divisées en 127 districts aussi appelés "wilaya".

Fondées en 1964, les forces de défense tanzaniennes ont un total de personnels actifs sous ses drapeaux. Leur budget est de 19,68 millions de dollars, soit 0,2 % du PNB en 2005.

L'économie de la Tanzanie est, à bien des égards, typique d'un pays en voie de développement. Essentiellement axée sur l'agriculture et l'industrie minière, elle dispose d'une base industrielle quasi-inexistante et peu compétitive. En 2009, l'agriculture représentait ainsi
plus de 25 % du PNB, plus de 30 % des exportations et 70 % des emplois. Le tourisme y constitue une source appréciable et croissante de devises.

Mais le pays est également très différent de la plupart des pays africains, avec une présence marchande arabe et perse sur ses côtes, commerce datant des premiers siècles de l'ère commune, et une ville, Zanzibar, qui pendant plusieurs centaines d'années domina l'économie de toute la région. Plaque tournante du commerce d'or, d'ivoire et d'esclaves, à l'interface des mondes africain, arabe et indien, son hinterland s'étend jusqu'à l'Afrique des Grands Lacs, à près de km de distance. L'arrivée des Européens dans le sillage de Vasco de Gama au ne remet pas tout de suite cette domination en question, ceux-ci offrant même de nouveaux marchés pour une denrée locale appréciée, le clou de girofle. Le pays est au palmarès des huit premiers producteurs de coton d'Afrique de l'est, du sud et du nord au milieu des années 2010. C'est surtout le cinquième au palmarès des producteurs africains de thé au début de la décennie 2010, dominé par le Kenya. 
L'installation progressive des empires coloniaux allemand puis britannique relègue cependant la région hors des grands axes de développement. L'accession à l'indépendance, au début des années 1960, voit la jeune République unie du Tanganyka et de Zanzibar (son nom d'origine) se tourner vers un « socialisme africain » d'inspiration maoïste qui se révèle rapidement être un échec : plutôt que de décoller et se moderniser, l'économie du pays s'effondre. La remise en question qui s'ensuit, à partir du milieu des années 1980, conduit progressivement le pays à revenir sur la scène économique régionale. La libéralisation franche opérée dans les premières années du , couplée à des efforts de gouvernance, permet l'arrivée massive d'investisseurs étrangers dans tous les secteurs de l'économie. Pour la première fois de sa jeune histoire, ce pays chroniquement sous-développé semble toucher les premiers dividendes de ses efforts. Le potentiel touristique de la Tanzanie, avec son littoral composé de superbes plages sur l'océan Indien, la région nord qui comprend le Kilimandjaro, le cratère de Ngorongoro et les plus grandes réserves animales d'Afrique, la présence de la plupart des grands lacs du continent et sa stabilité politique, est également très important et continue à se développer.

Les transports tanzaniens s'effectuent principalement par la route, avec un complément par le rail. Le réseau routier est cependant mauvais, et peu nombreuses encore sont les voies goudronnées : la saison des pluies rend nombre de pistes impraticables pendant des jours, voire des semaines, et le seul lien entre la côte et le lac Tanganyka durant cette période est ferroviaire. Le transport aérien est hors de portée pour l'immense majorité de la population. Il sert essentiellement le tourisme pour ce qui est des connexions internationales, et s'appuie sur un plus grand nombre de pistes de terre pour le trafic régional.

La plupart des routes pavées tanzaniennes se situent dans l'Est et le Sud-Ouest du pays. Dans le reste du pays, des pistes (parfois en gravier) constituent l'essentiel d'un réseau où l'on peut occasionnellement trouver des sections goudronnées mais isolées.

L'autoroute Le Caire-Le Cap (autoroute sur le réseau autoroutier panafricain) pénètre en Tanzanie depuis le Kenya par la localité frontalière de Tunduma et traverse Arusha, Dodoma, Iringa et Mbeya avant de poursuivre vers la Zambie. Une importante section entre le Parc national de Tarangire et Iringa n'est pas goudronnée, et peut devenir difficile en cas de pluies abondantes. Il s'agit d'un des axes de développement majeur des transports routiers dans le proche avenir, au risque sinon d'étrangler ou freiner l'importante croissance des dernières années.

Avec km de voies ferrées réparties sur quatre corridors principaux, la Tanzanie est, comparativement à certains autres pays africains, bien desservie. Conséquence indirecte d'un climat qui peut être cruel avec les routes, c'est le rail qui a longtemps été privilégié pour relier la côte à l'intérieur du pays. Seul le sud-est du pays ne dispose pas de réseau ferré, et les liaisons internationales se limitent à la Zambie, l'Ouganda et le Kenya. Plusieurs projets d'extension vers les autres voisins sont à l'étude.
Le pays dispose de deux compagnies indépendantes, la Tanzania Railways Corporation (TRC) et la Tazara. Cette dernière exploite de voies entre Dar es Salaam et Kapiri Mposhi, en Zambie. Les deux réseaux ne sont pas connectés pour cause de différence d'écartement des rails, mais un centre de transbordement est en fonctionnement près de Morogoro, permettant ainsi à un même conteneur de voyager jusqu'en Ouganda ou au Kenya depuis l'Afrique du Sud. Cette section était un élément important du projet britannique de Chemin de fer Le Cap-Le Caire.

Pour pallier une gestion chaotique et plombée par le manque chronique de moyens, les deux compagnies ferroviaires sont en cours de privatisation.

Le pays dispose de plusieurs ports sur sa côte orientale et sur les Grands Lacs, mais ces deux régions ne sont pas connectées, le transport fluvial étant inexistant dans le centre. La Tanzanie dispose d'une tradition maritime bien établie, Zanzibar ayant été pendant des siècles le port le plus important de toute la côte africaine de l'océan Indien; son hinterland s'étendait jusqu'au bassin du Congo. Les marchands swahilis utilisaient des dhows pour commercer le long des côtes, tradition encore bien vivante de nos jours.

Les lignes de ferries sur les Grands Lacs sont gérées par les compagnies ferroviaires nationales des États riverains, le Lac Victoria profitant du trafic le plus dense. L'activité sur le Lac Tanganyika a décliné du fait de la guerre au République démocratique du Congo voisin, mais un solide trafic commercial reste entre les villes de Kigoma, Bujumbura (Burundi) et Mpulungu (Zambie), notamment grâce au MV Liemba, ancien navire de guerre allemand bâti en 1913, coulé en 1916 et renfloué (et en activité) depuis 1927.

La compagnie nationale aérienne, Air Tanzania, relie les principales villes du pays, et de petits opérateurs privés commencent à s'intéresser à certaines de ces lignes intérieures. Les trois aéroports internationaux sont à Dar es Salaam, Kilimandjaro (Arusha) et Zanzibar.

La Tanzanie est dotée d'une large variété de sources d'énergie : biomasse, gaz naturel, hydroélectricité, charbon, énergie solaire ou éolienne. Dans les faits, la plupart de ces ressources ne sont pas exploitées, moins de 10 % de la population a accès à l'électricité et, dans les zones rurales, 20 % du temps de travail est consacré quotidiennement à la collecte de bois, qui représente près de 92 % de l'énergie produite. Le reste se répartit entre produits pétroliers (< 7 %) et hydroélectricité (< 2 %).

La compagnie nationale d'électricité, la "Tanzania Electric Supply Company" (TANESCO), est responsable de 98 % de l'électricité produite dans le pays, essentiellement par le biais de barrages, notamment ceux de Kihansi et Kidatu, qui à eux deux fournissent près de 40 % du total. Ceux-ci sont malheureusement touchés par la sécheresse récurrente depuis quelques années, poussant le gouvernement à investir plus lourdement dans des sources alternatives alors que seuls 400 des MW d'énergie hydroélectrique théoriquement disponibles sont construits : avec 11 à 13 % de progression annuelle, les besoins en électricité sont réels et croissants.

L'énergie thermique, gérée pour l'essentiel par deux société privée (IPTL et Songas), se base sur du pétrole lourd d'importation et, depuis peu, l'exploitation du gaz issu du gisement de Songo Songo directement connecté à une centrale à turbine de Dar es Salaam, ville qui consomme près de la moitié de la production nationale d'électricité à elle seule. Mais les capacités se développent : la station d'Ubungo, à Dar es Salaam, consomme près de 300 millions de m de gaz de Songo Songo, et il est prévu que la demande pour celui-ci grimpe jusqu'à 900 millions de m vers 2010, notamment grâce à l'installation de nouvelles centrales ou la reconversion de centrales au fioul, comme à Mtwara.

De nombreuses villes sont encore dépendantes de générateurs au diesel vieillissants : en 2004, 18 capitales de districts attendaient encore leur raccordement. Une faible quantité d'énergie est importée de la Zambie et de l'Ouganda voisins pour les villes frontalières. Le pétrole, malgré divers projets d'exploration en cours, est presque entièrement importé : la dernière raffinerie du pays, aux installations obsolètes, a fermé en 2000. L'effort se porte sur le raccordement rapide des zones qui ne le sont pas de manière à ralentir la déforestation. Dans cette optique de protection, les énergies solaire et éolienne ne sont pas exploitées, et le charbon n'est pas utilisé à la pleine mesure de son potentiel non plus, notamment dans l'industrie.

Le pays s'est doté en 2003 d'une nouvelle politique nationale concernant les télécommunications principalement axée sur le développement de ses infrastructures et de la formation de la population dans ce domaine. Visant à coordonner des politiques jusqu'ici erratiques, elle montre combien le pays a parcouru un long chemin depuis la promulgation de l'ordonnance d'interdiction des équipements électroniques et télévisions, en 1974. Promulguée par le premier président du pays, Julius Nyerere, celle-ci reflétait sa vision selon laquelle la présence de la télévision accroitrait le fossé entre riches et pauvres. La première chaîne privée fut finalement lancée en 1994, sept années avant la première émission du service public. Les mesures de libéralisation des ondes (radio et télé) de 2001 ne s'appliquent d'ailleurs pas à Zanzibar, qui ne possède pas de médias privés (ceux du continent restant cependant accessibles). Le manque de professionnels disposant de la formation adaptée fait cependant sentir ses effets, et le gouvernement essaie d'investir plus particulièrement dans ce secteur éducatif (notamment via la création du "Mosi Institute of Technology", calqué - modestement - sur le MIT américain).

Les progrès enregistrés depuis le début des années 2000 s'expliquent entre autres par la conjonction de deux facteurs:

Le paysage des télécoms tanzaniens reste quoi qu'il en soit excessivement déséquilibré, avec l'essentiel des infrastructures et investissements toujours concentrés dans la capitale économique. Le manque de moyens est criant, de même que le manque de personnel qualifié : certaines écoles, afin de familiariser leurs étudiants avec le matériel informatique en dépit de la pénurie, donnent leurs cours sur des ordinateurs en bois ou carton. Quand bien même la loi prévoit la mise en place d'une politique d'accès universel pour le monde rural (80 % de la population), celle-ci est laissée faute de moyens aux bons soins du secteur privé, souvent concentré sur les zones urbaines plus denses.

La télédensité du pays est faible, avec moins de 15 lignes mobiles pour 100 habitants en 2006, mais reste en progression constante, notamment à Dar-es-Salaam. Le réseau est désormais dans sa grande majorité numérique et la pénétration dans les zones urbaines s'accélère.

En 2005, la partie continentale du pays a modifié son système d'attribution de licences pour copier celui appliqué avec succès par la Malaisie à la fin des années 1990 : d'un système "vertical" (droit d'opérer un réseau de télécoms OU de radiodiffusion et de fournir des services sur celui-ci), on est passé à une approche "horizontale" (la licence permet de posséder un réseau de télécommunications ET de radiodiffusion, mais une licence distincte est attribuée pour fournir les contenus et services sur ledit réseau). Cette réforme, la première de ce type sur le continent africain, a permis aux investisseurs de se concentrer sur leur métier de base (infrastructures ou services) et sur un maximum de secteurs simultanément. Cette réforme a permis l'augmentation des investissements directs étrangers, et devrait favoriser à terme l'arrivée rapide de services téléphoniques par le biais de la télévision câblée, de la télévision par téléphone, et de l'internet sur tous les médias existants : la Tanzanie est le premier pays africain à s'être adapté au phénomène de convergence des technologies.

La "Tanzania Telecommunications Company" (TTCL) est le seul fournisseur d'accès fixe sur le continent, son "alter ego" Zantel, une compagnie privée opérant depuis Zanzibar, visant à prendre pied dans le reste du pays à court terme. La TTCL était en situation de monopole jusqu'à sa privatisation partielle en 2001 : cinq opérateurs mobiles se partagent désormais les licences pour l'ensemble du pays. Conséquence de cette concurrence accrue, les prix des services ont diminué de plus de la moitié en cinq ans. Comme dans la plupart des pays en voie de développement, l'absence d'infrastructures dans un pays à la densité de population faible favorise grandement le développement de la téléphonie mobile : 97 % de la population peut en théorie avoir accès à un réseau mobile.

La compagnie de chemins de fer a également déposé une demande de licence pour opérer le long de la ligne Dar es Salaam - Mwanza. Le pays a également été sélectionné pour la mise en route d'un projet-pilote de télécentre à Sengerema (près de Mwanza). Ce projet, mené en coopération avec l'UIT et l'UNESCO (et peut-être la FAO et d'autres acteurs potentiellement intéressés), vise à développer un centre multi-services capable de coordonner les activités commerciales, agricoles et gouvernementales.

Le marché est dominé par trois fournisseurs d'accès qui obtinrent leur licence lors du premier appel d'offre en 1996 : les conditions d'obtention se résument à dollars de frais, plus 5 % de redevance sur tous les services à valeur ajoutée. Les trois opérateurs dépendent de capitaux internationaux, avec participation locale dans le cas de Datel (coopération entre Nexus International, entité appartenant à France Télécom, et TTCL). L'université de Dar es Salaam dispose également d'une licence, mais celle-ci (gratuite) est limitée à la communauté universitaire et elle ne peut en faire commerce avec le grand public. Les fournisseurs d'accès devant passer par les bandes satellitaires pour accéder à l'international, la mise en place d'Eassy (pour "East African Submarine Cable System"), un câble sous-marin de en chantier de fin 2008 à mi 2011, relie la côte est-africaine au reste du monde, et devrait permettre de faire baisser les coûts de connexion.

Les cybercafés sont désormais omniprésents dans les villes du pays, mais les efforts du gouvernement pour développer l'accès de la population à internet ont eu, au moins dans les premiers temps, un impact particulièrement limité : l'utilisation voire la connaissance des réseaux reste marginale limitée essentiellement aux zones à forte concentration urbaine.

La Tanzanie est peuplée de habitants en décembre 2012. Le taux d'alphabétisation est de 69,4 % pour les personnes de plus de 15 ans.
Sur le continent, 99 % de la population est d'origine africaine (dont 95 % de bantous répartis en plus de 130 ethnies), le 1 % restant étant représenté par des Asiatiques (), Européens () et Arabes (). À Zanzibar, la population est constituée d'un mélange plus homogène d'Africains et d'Arabes.

La Tanzanie accueille sur son territoire plus de réfugiés provenant principalement du Burundi et de la République démocratique du Congo.

Le taux de fécondité est estimé à 5 enfants par femme tandis que le taux de mortalité infantile est de 66,93 pour mille. L'espérance de vie est de 52,85 ans en 2011, environ 14 ans de moins que l'espérance de vie moyenne dans le monde. L'âge moyen total est de 18,5 ans, pour les femmes de 18,7 ans et pour les hommes de 18,2 ans. La croissance démographique annuelle est de 2,5 %.

On relève la présence de plusieurs maladies infectieuses :

. En effet, la colonisation a entrainé l'apparition de la scolarisation dans le pays, sur un modèle occidental puisque l'Allemagne est ladite colonisatrice. Sa politique d'enseignement est calquée sur ce qu'on appelle l'assimilationnisme.
Vient ensuite le modèle britannique, lorsque l'Allemagne ne possède plus la Tanzanie, qui est basé sur le différentialisme. Ces façons d'enseigner ont profondément marqué le modèle éducatif tanzanien.
Peu avant l'Indépendance, le système scolaire est caractérisé par une forte ségrégation des Européens, des Asiatiques (Asiatiques principalement venus d'Inde) et des Africains, au détriment de ces derniers.
Quoi qu'il en soit, les deux puissances colonisatrices n'ont jamais considéré l'éducation en Tanzanie comme un souci majeur, un point important de leur politique coloniale. Cela a laissé une importante séquelle pour l'avenir (pendant l'Indépendance), avec un système éducatif très peu rodé et très fragile.

Avec l'arrivée de l'Indépendance, et Julius Nyerere au pouvoir, le problème de la discrimination raciale au sein de l'éducation devient une priorité absolue pour le premier président tanzanien. C'est donc une volonté de changement radical qui apparait ici, avec la politique éducative pour l'autosuffisance mise en place à la fin des années 1960.
Avec Nyerere au pouvoir, l'éducation va prendre une nouvelle figure. Sa vision de l'éducation va partager les avis, certains verront en elle quelque chose de novateur, pour d'autres au contraire, elle ne serait que la continuité d'une éducation de colonie. Cette éducation porte le nom de "Education for Self-reliance" notée ESR.
Elle est considérée par les dirigeants (comme dans de nombreux pays africains) comme un moteur de développement mais aussi, et surtout, comme un moyen de faire un certain type de société. Cependant, l'éducation ne permit pas de constituer le type de société souhaité, ceci en partie dû à la crise économique des années 1980. Cette crise fit intervenir le FMI en Tanzanie, sous la demande du président A.H. Mwinyi, FMI qui regarda les institutions éducatives présentes et qui remania le système avec certains plans d'ajustement structurel. Le Ministère de l'éducation nationale vit son budget resserré, ce qui limita encore le champ et la liberté d'action des politiques tanzaniennes en matière d'éducation.

Aujourd'hui, l'alphabétisation est définie en Tanzanie par : tout individu de plus de 15 ans capable de lire et d'écrire le swahili, l'anglais ou encore l'arabe. Ce chiffre s'élève à 69,4 % de la population en 2011, et plus précisément : 77,5 % des hommes, 62,2 % pour les femmes. Ainsi, la Tanzanie se classe au mondial en matière d'alphabétisation.
Les dépenses pour l'éducation étaient de 6,8 % du PIB en 2008.
La scolarité et la vie professionnelle sont souvent réservées aux hommes.


La Tanzanie est largement influencée par la culture swahilie de Zanzibar. D'une manière plus générale, l'ensemble du pays conserve les traces de la présence arabe, qui s'est diffusée le long des routes des caravanes entre la côte et les actuels pays des grands lacs dès la fin du . Cette influence se repère dans différents aspects culturels, comme l'architecture, les vêtements, et surtout la religion (un tiers environ de la population est de confession musulmane, les deux autres étant chrétien et animiste). Depuis l'introduction du libéralisme économique au milieu des années 1980 et de la démocratisation de la vie politique dans les années 1990, les grandes villes sont également soumises à une occidentalisation relative, très visible dans les choix vestimentaires et les goûts musicaux.

Les deux langues officielles sont le kiswahili ou swahili (appelé Kiunguja à Zanzibar) et l'anglais mais il existe d'autres langues véhiculaires comme l'arabe ou le gujarati, cette dernière parlée par des communautés originaires du sous-continent indien. Le pays compte plus de 120 groupes ethniques, chacun ayant conservé sa langue. On note toutefois que l'influence du kiswahili a contribué à un affaiblissement du poids des langues locales. Ce fait est surtout notable en milieu urbain, où l'on assiste à la naissance de la première génération de Tanzaniens ne maitrisant qu'une des langues de leur pays, le kiswahili. .

De nombreuses femmes sont mères au foyer. Beaucoup contribuent toutefois aux revenus du ménage par des travaux informels, comme vendeuses de repas dans la rue (mama ntilie en kiswahili).

Les religions les plus représentées sont l’animisme (35 %), l'islam (35 %, essentiellement sunnite) et le christianisme (30 %) selon "France Diplomatie", tandis que la CIA et le Département d'État des États-Unis font une estimation beaucoup plus faible de l’animisme (religion populaire : 1,8 %) et estiment le christianisme à 61 ou 62 %. La population de Zanzibar est à 98 % ou 99 % musulmane. Le fort développement des Églises pentecôtistes est un fait récent. La présence importante de luthériens dans la communauté protestante est un héritage de la .

Selon des propos de l’Église catholique tanzanienne rapportés par le Vatican, les catholiques de Zanzibar seraient persécutés par un groupe de musulmans, sur fond de tensions indépendantistes.

La Tanzanie a pour codes :





</doc>
<doc id="3006" url="https://fr.wikipedia.org/wiki?curid=3006" title="Tunisie">
Tunisie

, en forme longue , est un État d’Afrique du Nord.

Elle est bordée au nord et à l’est par la mer Méditerranée ( kilomètres de côtes), à l’ouest par l’Algérie avec de frontière commune et au sud-est par la Libye avec de frontière. Sa capitale Tunis est située dans le nord-est du pays, au fond du golfe de Tunis. Plus de 30 % de la superficie du territoire est occupée par le désert du Sahara, le reste étant constitué de régions montagneuses et de plaines fertiles, berceau de la civilisation carthaginoise qui atteignit son apogée au , avant de devenir une province de l’Empire romain.

Longtemps appelée , notamment sous la domination ottomane, la Tunisie passe sous protectorat français le avec la signature du traité du Bardo. Avec l’avènement de l’indépendance, le , le pays s’achemine, au début, vers le statut d’une monarchie constitutionnelle ayant pour souverain Lamine Bey, dix-neuvième et dernier bey régnant de la dynastie des Husseinites. Avec la proclamation de la république, le , c’est le leader nationaliste Habib Bourguiba qui devient le premier président de la République tunisienne et modernise le pays. Toutefois, en 1987, au terme de trente ans à la tête du pays dont la fin est marquée par le clientélisme et la montée de l’islamisme, le Premier ministre Zine el-Abidine Ben Ali finit par le déposer, mais poursuit dès lors les principaux objectifs du tout en libéralisant l’économie. Après vingt-trois ans d’une présidence autoritaire et policière, caractérisée par l’importance de la corruption, Ben Ali est chassé le par une révolution populaire. Trouvant refuge en Arabie saoudite, plus précisément à Djeddah, il fait l’objet, avec son épouse Leïla Ben Ali, d’un mandat d'arrêt international.

Intégrée aux principales instances de la communauté internationale telles que l’ONU ou la Cour pénale internationale, la Tunisie fait également partie de l’Union du Maghreb arabe, de la Ligue arabe, de la Grande zone arabe de libre-échange, de l’Organisation de la coopération islamique, de l’Union pour la Méditerranée, de l’Union africaine, de l’Organisation internationale de la francophonie, du Groupe des 77, de la Communauté des États sahélo-sahariens et du mouvement des non-alignés. La Tunisie a également conclu un accord d'association avec l'Union européenne et obtenu le statut d’allié majeur hors OTAN.

Le nom actuel de la , qui vient du français, est dérivé du nom de la capitale, Tunis, suivi du suffixe latin -ie. Le dérivé latin est par la suite adopté dans plusieurs autres langues européennes, à quelques modifications près, pour différencier le pays de la ville de Tunis.

Toutefois, dans certaines langues, il n’y a pas eu une telle modification, comme en russe ('), en espagnol (') et en arabe ( ou ""). Dans ces cas, seul le contexte permet de déterminer si l’on parle de la ville ou du pays.

Avant cela, le territoire de la Tunisie est appelé Ifriqiya ou encore Africa dans l’Antiquité, ce qui donne par la suite son nom à l’Afrique.

La Tunisie, le plus petit des États du Maghreb, se situe au nord du continent africain. Il est séparé de l’Europe par une distance de au niveau du canal de Sicile.

Disposant d’une superficie de , le pays est limité à l’ouest par l’Algérie avec de frontière commune, au sud-est par la Libye avec de frontière et au nord et à l’est par la mer Méditerranée avec de côtes.

Le désert du Sahara occupe une superficie comprise entre 33 % et 40 % du territoire selon qu’on le définisse d’après son aridité ou selon des caractéristiques paysagères. La superficie des terres à vocation agricole est estimée à dix millions d’hectares, réparties en cinq millions de terres labourables, quatre millions de parcours naturels et un million de forêts et garrigues.

La Tunisie possède un relief contrasté avec une partie septentrionale et occidentale montagneuse, la dorsale tunisienne, située dans l’extension du massif montagneux de l’Atlas ; elle est coupée par la plaine de la Medjerda, le seul cours d’eau du pays qui soit alimenté de façon continue.

Le point culminant du territoire est le Djebel Chambi culminant à mètres. À l’est, une plaine s’étend entre Hammamet et Ben Gardane, via le Sahel tunisien et la Djeffara.

La partie méridionale du pays, principalement désertique, est divisée entre une succession de chotts (Chott el-Gharsa, Chott el-Jérid et Chott el-Fejaj), des plateaux rocheux et les dunes du Grand Erg oriental. Le littoral parsemé de tombolos et de lagunes s’étend sur kilomètres dont 575 de plages sablonneuses. Quelques îles dont les Kerkennah et Djerba parsèment le littoral.

Le climat de la Tunisie se divise en sept zones bioclimatiques, la grande différence entre le nord et le reste du pays étant due à la chaîne de la dorsale tunisienne qui sépare les zones soumises au climat méditerranéen (classification de Köppen "Csa") de celles soumises au climat désertique chaud (classification de Köppen "BWh") typique du Sahara, le plus grand désert chaud du globe. Entre les deux, on y trouve le climat semi-aride chaud (classification de Köppen "BSh") avec des caractéristiques communes aux deux principaux régimes climatiques du pays.

En raison de sa situation géographique, le climat tunisien est influencé par divers types de vents : la côte nord est exposée aux vents marins doux et humides soufflant depuis le sud de la France, ce qui provoque une baisse significative des températures et une hausse des précipitations, et le sud du pays aux vents continentaux chauds et secs, tels le sirocco soufflant sur les grandes étendues désertiques et les plaines, provoquant alors une brutale hausse des températures et un net assèchement de l’atmosphère.

Le pays bénéficie également d’un taux d’ensoleillement important dépassant par an et qui atteint des sommets dans le sud désertique, aux abords des frontières algérienne et libyenne.

Les températures varient en fonction de la latitude, de l’altitude et de la proximité ou de l’éloignement de la mer Méditerranée. S’il peut faire quelques degrés au-dessous de dans les montagnes de Kroumirie en hiver, la température maximale grimpe souvent aux environs de dans les régions désertiques en été. La pluviométrie annuelle moyenne varie également selon les régions : d’environ au nord à environ au centre et jusqu’à moins de à l’extrême sud.

La flore varie beaucoup en fonction des régions : celle des régions côtières est semblable à celle de l’Europe méridionale et comprend prairies, garrigue, maquis et forêts de chênes-liège. Plus au sud, la végétation est de type steppique avec une dominance de l’alfa. Dans les régions arides de l’extrême sud, les oasis sont plantées de palmiers-dattiers.

Quinze aires naturelles ont été érigées en parcs nationaux. Le parc national de l'Ichkeul, qui s’étend sur , est inscrit sur la liste du patrimoine mondial de l’Unesco. Il existe également seize réserves naturelles qui ont pour but d’être un habitat pour des espèces ayant une valeur écologique, économique et en tant qu’écosystèmes vulnérables.

Selon une étude du programme méditerranéen du WWF, la région côtière du nord-ouest figure parmi les treize sites de la Méditerranée qui se distinguent par leur richesse naturelle, leur biodiversité et leurs espèces végétales et animales uniques.

L’espace tunisien apparaît inégalement peuplé et développé sur le plan socioéconomique selon un gradient intérieur-littoral (ouest-est) : les treize gouvernorats côtiers totalisent ainsi 65,3 % de la population totale avec une forte densité de population ( par km contre 65,6 pour l’ensemble du pays).

La Tunisie est urbanisée à 65,6 % en 2007 et connaît un taux d’urbanisation annuelle de 3,6 %. Le réseau urbain se situe sur la bande littorale orientale, entre les régions de Bizerte et Gabès en passant par Tunis, le cap Bon, le Sahel et Sfax (centre-est du pays), qui dispose des plus grandes infrastructures économiques et concentre plus de 80 % de la population urbaine. Au terme du recensement de 2014, les principales municipalités sont :

La Tunisie est divisée en 24 gouvernorats qui portent le nom de leurs chefs-lieux :

À leur tête se trouvent des gouverneurs, nommés par le président de la République, qui sont les de l’autorité de l’État. Trois institutions les aident à accomplir leurs missions : le conseil local de développement, le conseil rural et le comité de quartier. Aux côtés des gouverneurs se trouvent les Conseils régionaux qui sont chargés d’examiner .

Ils donnent ainsi leur avis sur les programmes et projets que l’État envisage de réaliser dans leur gouvernorat respectif, arrêtent le budget des gouvernorats et les impôts perçus au profit de la collectivité publique et établissent des relations de coopération avec des instances étrangères de niveau régional (après approbation du ministre de l’Intérieur).

Les gouvernorats sont subdivisés en 264 délégations puis en 281 municipalités. La plus petite division administrative est le secteur ou imada, dont le nombre se monte à .

Au travers des siècles, le territoire de l’actuelle Tunisie a successivement été sous influence carthaginoise, numide, romaine, byzantine, vandale, omeyyade, aghlabide, fatimide, normande, almohade, hafside, ottomane et française.

Ces circonstances, ainsi que la position de la Tunisie à l’intersection entre le bassin méditerranéen, l’Europe et l’Afrique, ont influencé la diversité culturelle du pays.

Les premières traces de présence humaine en Tunisie datent du Paléolithique. C’est à vingt kilomètres à l’est de Gafsa, dans l’oasis d’El Guettar, que se rassemble une petite population nomade de chasseurs-cueilleurs moustériens. Michel Gruet, l’archéologue qui découvre le site, relève qu’ils consomment des dattes dont il retrouve le pollen aux alentours de la source aujourd’hui asséchée.

À une culture ibéromaurusienne, répartie sur le littoral et relativement minime en Tunisie, succède la période du Capsien, nom créé par Jacques de Morgan et issu du latin "Capsa", qui a lui-même donné le nom de l’actuelle Gafsa. Morgan définit le Capsien comme étant une culture allant du Paléolithique supérieur au Néolithique, couvrant ainsi une période qui s’étend du . D’un point de vue ethnologique et archéologique, le Capsien prend une importance plus grande puisque des ossements et des traces d’activité humaine remontant à plus de sont découverts dans la région. Outre la fabrication d’outils en pierre et en silex, les Capsiens produisaient, à partir d’ossements, divers outils dont des aiguilles pour coudre des vêtements à partir de peaux d’animaux.

Au Néolithique (4500 à 2500 av. J.-C. environ), arrivé tardivement dans cette région, la présence humaine est conditionnée par la formation du désert saharien, qui acquiert son climat actuel. De même, c’est à cette époque que le peuplement de la Tunisie s’enrichit par l’apport des Berbères, issus semble-t-il de la migration vers le nord de populations libyques (ancien terme grec désignant les populations africaines en général). Le Néolithique voit également le contact s’établir entre les Phéniciens de Tyr, les futurs Carthaginois qui fondent la civilisation punique, et les peuples autochtones de l’actuelle Tunisie, dont les Berbères sont désormais devenus la composante essentielle.

On observe le passage de la Préhistoire à l’Histoire principalement dans l’apport des populations phéniciennes, même si le mode de vie néolithique continue un temps à exister aux côtés de celui des nouveaux arrivants. Cet apport est nuancé, notamment à Carthage (centre de la civilisation punique en Occident), par la coexistence de différentes populations minoritaires mais dynamiques comme les Berbères, les Grecs, les Italiens ou les Ibères d’Espagne. Les nombreux mariages mixtes contribuent à l’établissement de la civilisation punique.

L’entrée de la Tunisie dans l’histoire se fait par l’expansion d’une cité issue d’une colonisation proche-orientale. La Tunisie accueille progressivement une série de comptoirs phéniciens comme bien d’autres régions méditerranéennes. Le premier comptoir selon la tradition est celui d’Utique, qui date de 1101 av. J.-C. En 814 av. J.-C., des colons phéniciens venus de Tyr fondent la ville de Carthage. D’après la légende, c’est la reine Élyssa (Didon pour les Romains), sœur du roi de Tyr Pygmalion, qui est à l’origine de la cité. Ouverte sur la mer, Carthage est également ouverte structurellement sur l’extérieur. Un siècle et demi après la fondation de la ville, les Carthaginois ou Puniques étendent leur emprise sur le bassin occidental de la mer Méditerranée.

Cette présence prend diverses formes, y compris celle de la colonisation, mais reste d’abord commerciale (comptoirs de commerce, signature de traités, etc.). La mutation vers un empire plus terrestre se heurte aux Grecs de Sicile puis à la puissance montante de Rome et de ses alliés massaliotes, campaniens ou italiotes. Le cœur carthaginois qu’est la Tunisie, à la veille des guerres puniques, possède une capacité de production agricole supérieure à celle de Rome et de ses alliés réunis, et son exploitation fait l’admiration des Romains. La lutte entre Rome et Carthage prend de l’ampleur avec l’essor des deux cités : ce sont les trois guerres puniques, qui faillirent voir la prise de Rome mais se conclurent par la destruction de Carthage, en 146 av. J.-C., après un siège de trois ans. À l’issue de la Troisième Guerre punique, Rome s’installe sur les décombres de la ville. La fin des guerres puniques marque l’établissement de la province romaine d’Afrique dont Utique devient la première capitale, même si le site de Carthage s’impose à nouveau par ses avantages et redevient capitale en 14.

En 44 av. J.-C., Jules César décide d’y fonder une colonie romaine, la "Colonia Julia Carthago", mais il faudra attendre quelques décennies pour qu’Auguste lance les travaux de la cité. La région connaît alors une période de prospérité où l’Afrique devient pour Rome un fournisseur essentiel de productions agricoles, comme le blé et l’huile d’olive, grâce aux plantations d’oliviers chères aux Carthaginois. La province se couvre d’un réseau dense de cités romanisées dont les vestiges encore visibles à l’heure actuelle demeurent impressionnants : il suffit de mentionner les sites de Dougga (antique "Thugga"), Sbeïtla ("Sufetula"), Bulla Regia, El Jem ("Thysdrus") ou Thuburbo Majus. Partie intégrante de la République puis de l’Empire avec la Numidie, la Tunisie devient pendant six siècles le siège d’une civilisation romano-africaine d’une exceptionnelle richesse, fidèle à sa vocation de . La Tunisie est alors le creuset de l’art de la mosaïque, qui s’y distingue par son originalité et ses innovations.

Concurrents des dieux romains, des dieux indigènes apparaissent sur des frises d’époque impériale, et le culte de certaines divinités, Saturne et Caelestis, s’inscrit dans la continuité du culte voué par les Puniques à Ba'al Hammon et à Tanit, sa parèdre. Le voit aussi l’installation précoce de communautés juives et, dans le sillage de celles-ci, des premières communautés chrétiennes. L’apogée du et du début du s ne va toutefois pas sans heurts, la province connaissant quelques crises au : elle est frappée par la répression de la révolte de Gordien en 238 ; elle subit de même les affrontements entre usurpateurs au début du .

La province est l’une des moins touchées par les difficultés que connaît l’Empire romain entre 235 et le début du . Avec la Tétrarchie, la province recouvre une prospérité que révèlent les vestiges archéologiques, provenant tant de constructions publiques que d’habitations privées. Cette époque est aussi le premier siècle du christianisme officiel, devenu religion licite en 313 et religion personnelle de l’empereur Constantin.

Dans un espace ouvert sur l’extérieur comme l’est alors la province d’Afrique, le christianisme se développe de façon précoce grâce aux colons, commerçants et soldats, et la région devient l’un des foyers essentiels de la diffusion de la nouvelle foi, même si les affrontements religieux y sont violents avec les païens. Dès le , la province applique aussi les sanctions impériales, les premiers martyrs étant attestés dès le 17 juillet 180 : ceux qui refusent de se rallier au culte officiel peuvent être torturés, relégués sur des îles, décapités, livrés aux bêtes féroces, brûlés voire crucifiés.

À la fin du , la nouvelle religion progresse dans la province car, malgré une situation difficile, la nouvelle foi s’implante plus vite qu’en Europe, notamment en raison du rôle social joué par l’Église d’Afrique qui apparaît dans la seconde moitié du , aidé en cela par une très forte densité urbaine. De plus, une fois l'Édit de Thessalonique publié par l’empereur Théodose en 381, la christianisation devient automatique, puisqu’aucun autre culte n’est permis dans l’empire. Ainsi, au cours du et sous l’action dynamique d’Augustin d'Hippone et l’impulsion de quelques évêques, les grands propriétaires terriens et l’aristocratie citadine se rallient au christianisme, où ils voient leur intérêt, l’Église intégrant alors les diverses couches sociales. Rapidement, la province d’Afrique est considérée comme un phare du christianisme latin occidental.

Cette expansion rencontre toutefois des obstacles, en particulier lors du schisme donatiste qui est condamné de façon définitive au concile de Carthage. Ce dernier accuse les schismatiques d’avoir coupé les liens entre l’Église africaine et les Églises orientales originelles.

En dépit de cette lutte religieuse, la conjoncture économique, sociale et culturelle est relativement favorable au moment du triomphe du christianisme, comme en témoignent les nombreux vestiges, notamment de basiliques à Carthage et de nombreuses églises aménagées dans d’anciens temples païens (comme à Sbeïtla) ou même certaines églises rurales découvertes récemment. Le 19 octobre 439, après s’être rendus maîtres d’Hippone, les Vandales et les Alains entrent dans Carthage, où ils installent leur royaume pour près d’un siècle. Les Vandales sont adeptes de l’arianisme, déclarée hérésie au concile de Nicée, ce qui ne facilite pas les relations entre eux et les notables locaux majoritairement chalcédoniens. Or les Vandales exigent de la population une totale allégeance à leur pouvoir et à leur foi. En conséquence, ceux qui tentent de s’opposer aux Vandales ou à l’arianisme sont persécutés : de nombreux hommes d’Église sont martyrisés, emprisonnés ou exilés dans des camps au sud de Gafsa. Dans le domaine économique, les Vandales appliquent à l’Église la politique de confiscation dont doivent pâtir les grands propriétaires. Cependant, la culture latine reste largement préservée et le christianisme prospère tant qu’il ne s’oppose pas au souverain en place.

Dans ce contexte, le territoire, enserré par des principautés berbères, est attaqué par les tribus de nomades chameliers : la défaite, en décembre 533 à la bataille de Tricaméron, confirme l’anéantissement de la puissance militaire vandale. Carthage est prise facilement par les Byzantins dirigés par le général Bélisaire, envoyé par Justinien, le roi vandale Gélimer se rendant en 534. Malgré la résistance des Berbères, les Byzantins rétablissent l’esclavage et instituent de lourds impôts. Par ailleurs, l’administration romaine est restaurée. L’Église d’Afrique est mise au pas et Justinien fait alors de Carthage le siège de son diocèse d’Afrique. À la fin du , la région est placée sous l’autorité d’un exarque cumulant les pouvoirs civil et militaire, et disposant d’une large autonomie vis-à-vis de l’empereur. Prétendant imposer le christianisme d’État, les Byzantins pourchassent le paganisme, le judaïsme et les hérésies chrétiennes. Pourtant, à la suite de la crise monothéliste, les empereurs byzantins, opposés à l’Église locale, se détournent de la cité. Or, avec une Afrique byzantine entraînée dans le marasme, un état d’esprit insurrectionnel secoue des confédérations de tribus sédentarisées et constituées en principautés. Ces tribus berbères sont d’autant plus hostiles à l’Empire byzantin qu’elles ont conscience de leur propre force. Avant même sa prise par les Arabes en 698, la capitale et, dans une certaine mesure, la province d’Afrique se sont vidées de leurs habitants byzantins. Dès le début du , l’archéologie témoigne en effet d’un repli, ceci étant particulièrement évident à Carthage.

La première expédition arabe sur la Tunisie est lancée en 650, à l’époque du calife Othmân ibn Affân. Commandée par Abd Allâh ibn Saad, l’armée arabe écrase l’armée byzantine du patrice Grégoire près de Sbeïtla. En 666, une deuxième offensive menée par Mu’awiya ibn Hudayj à l’époque du calife omeyyade Muʿawiya se termine par la prise de plusieurs villes dont Sousse et Bizerte. L’île de Djerba est prise en 667. La troisième expédition, menée en 670 par Oqba Ibn Nafi Al Fihri, est décisive : ce dernier fonde la ville de Kairouan au cours de la même année et cette ville devient la base des expéditions contre le nord et l’ouest du Maghreb. L’invasion complète manque d’échouer avec la mort d’Ibn Nafi en 683, à la suite d’une embuscade tendue par le chef berbère Koceila au sud de l’Aurès. Après la mort d’Ibn Nafi, les Arabes évacuent Kairouan, où s’installe Koceila qui devient le maître de l’Ifriqiya : les Byzantins ne sont plus, selon les historiens arabes, que ses simples auxiliaires. Envoyé en 693 avec une puissante armée arabe, le général ghassanide Hassan Ibn Numan réussit à vaincre l’exarque et à prendre Carthage en 695. Seuls résistent certains Berbères dirigés par la Kahena.

Les Byzantins, profitant de leur supériorité navale, débarquent une armée qui s’empare de Carthage en 696 pendant que la Kahena remporte une bataille contre les Arabes en 697. Ces derniers, au prix d’un nouvel effort, finissent cependant par reprendre définitivement Carthage en 698 et par vaincre et tuer la Kahena. Contrairement aux Phéniciens, les Arabes ne se contentent pas d’occuper la côte et entreprennent de conquérir l’intérieur du pays.

Après avoir résisté, les Berbères se convertissent à la religion de leurs vainqueurs, principalement à travers leur recrutement dans les rangs de l’armée victorieuse. Des centres de formation religieuse s’organisent alors, comme à Kairouan, au sein des nouveaux ribats. On ne saurait toutefois estimer l’ampleur de ce mouvement d’adhésion à l’islam. D’ailleurs, refusant l’assimilation, nombreux sont ceux qui rejettent la religion dominante et adhèrent au kharidjisme, courant religieux musulman né en Orient et proclamant notamment l’égalité de tous les musulmans sans distinction de race ni de classe. La région reste une province omeyyade jusqu’en 750, quand la lutte entre Omeyyades et Abbassides voit ces derniers l’emporter. De 767 à 776, les kharidjites berbères sous le commandement d’Abou Qurra s’emparent de tout le territoire, mais ils se retirent finalement dans le royaume de Tlemcen, après avoir tué Omar ibn Hafs, surnommé Hezarmerd, dirigeant de la Tunisie à cette époque.

En 800, le calife abbasside Hâroun ar-Rachîd délègue son pouvoir en Ifriqiya à l’émir Ibrahim ibn al-Aghlab et lui donne le droit de transmettre ses fonctions par voie héréditaire. Al-Aghlab établit la dynastie des Aghlabides, qui règne durant un siècle sur le Maghreb central et oriental. Le territoire bénéficie d’une indépendance formelle tout en reconnaissant la souveraineté abbasside. La Tunisie devient un foyer culturel important avec le rayonnement de Kairouan et de sa Grande Mosquée, un centre intellectuel de haute renommée. À la fin du règne de Ziadet Allah (817-838), Tunis devient la capitale de l’émirat jusqu’en 909. Appuyée par les tribus Kutama qui forment une armée fanatisée, l’action du prosélyte ismaélien Abu Abd Allah ach-Chi'i entraîne la disparition de l’émirat en une quinzaine d’années (893-909). En décembre 909, Ubayd Allah al-Mahdi se proclame calife et fonde la dynastie des Fatimides, qui déclare usurpateurs les califes omeyyades et abbassides ralliés au sunnisme. L’État fatimide s’impose progressivement sur toute l’Afrique du Nord en contrôlant les routes caravanières et le commerce avec l’Afrique subsaharienne. En 945, Abu Yazid, de la grande tribu des Ifrenides, organise sans succès une grande révolte berbère pour chasser les Fatimides. Le troisième calife, Ismâ`îl al-Mansûr, transfère alors la capitale à Kairouan et s’empare de la Sicile en 948. Lorsque la dynastie fatimide déplace sa base vers l’est en 972, trois ans après la conquête finale de la région, et sans abandonner pour autant sa suzeraineté sur l’Ifriqiya, le calife Al-Muizz li-Dîn Allah confie à Bologhine ibn Ziri — fondateur de la dynastie des Zirides — le soin de gouverner la province en son nom. Les Zirides prennent peu à peu leur indépendance vis-à-vis du calife fatimide, ce qui culmine lors de la rupture avec ce suzerain devenu lointain et inaugure l’ère de l’émancipation berbère.

L’envoi depuis l’Égypte de tribus arabes nomades sur l’Ifriqiya marque la réplique des Fatimides à cette trahison. Les Hilaliens suivis des Banu Sulaym — dont le nombre total est estimé à et — se mettent en route après que de véritables titres de propriété leur ont été distribués au nom du calife fatimide. Kairouan résiste pendant cinq ans avant d’être occupée et pillée. Le souverain se réfugie alors à Mahdia en 1057 tandis que les nomades continuent de se répandre en direction de l’Algérie, la vallée de la Medjerda restant la seule route fréquentée par les marchands. Ayant échoué dans sa tentative pour s’établir dans la Sicile reprise par les Normands, la dynastie ziride s’efforce sans succès pendant 90 ans de récupérer une partie de son territoire pour organiser des expéditions de piraterie et s’enrichir grâce au commerce maritime.

Les historiens arabes sont unanimes à considérer cette migration comme l’événement le plus décisif du Moyen Âge maghrébin, caractérisé par une progression diffuse de familles entières qui a rompu l’équilibre traditionnel entre nomades et sédentaires berbères. Les conséquences sociales et ethniques marquent ainsi définitivement l’histoire du Maghreb avec un métissage de la population. Depuis la seconde moitié du , la langue arabe demeurait l’apanage des élites citadines et des gens de cour. Avec l’invasion hilalienne, les dialectes berbères sont plus ou moins influencés par l’arabisation, à commencer par ceux de l’Ifriqiya orientale.

À partir du premier tiers du , la Tunisie est régulièrement attaquée par les Normands de Sicile et du sud de l’Italie, basés dans le royaume normano-sicilien, qui finissent par conquérir l’ensemble du littoral tunisien et y fonde le Royaume d'Afrique. Celui-ci est une extension de la frontière siculo-normande dans l’ancienne province romaine d'Afrique (alors appelée "Ifriqiya"), qui correspond aujourd’hui à la Tunisie ainsi qu’à une partie de l’Algérie et de la Libye. Les sources primaires ayant trait au royaume sont en arabe alors que les sources latines (chrétiennes) sont plus rares. Selon Hubert Houben, étant donné qu’ n’a jamais été officiellement ajouté aux titres royaux des rois de Sicile . L’ est plutôt une constellation de villes gouvernées par les Normands sur la côte ifriqiyenne.

La conquête sicilienne de l’Ifriqiya commence sous le règne de en 1146-1148. Le règne sicilien consiste en des garnisons militaires dans les principales villes, des exactions sur les populations musulmanes, la protection des chrétiens et le monnayage de pièces de monnaies. L’aristocratie locale est largement gardée en place et des princes musulmans se chargent des affaires civiles sous surveillance normande. Les relations économiques entre la Sicile et l’Ifriqiya, qui étaient déjà fortes avant la conquête, sont renforcées, tandis que les échanges entre l’Ifriqiya et le nord de l’Italie sont étendus. Sous le règne de , le Royaume d’Afrique tombe aux mains des Almohades (1158-1160). Son héritage le plus durable est le réalignement des puissances méditerranéennes provoqué par sa disparition et la paix siculo-almohade finalisée en 1180. L’ensemble du territoire de l’Ifriqiya finit par être occupé par l’armée du sultan almohade Abd al-Mumin lors de son expédition depuis le nord du Maroc en 1159. L’économie devient florissante et des relations commerciales s’établissent avec les principales villes du pourtour méditerranéen (Pise, Gênes, Marseille, Venise et certaines villes d’Espagne).

L’essor touche également le domaine culturel avec les œuvres du grand historien et père de la sociologie Ibn Khaldoun ; le siècle almohade est considéré comme l’ du Maghreb. De grandes villes se développent et les plus belles mosquées sont érigées à cette époque. Les Almohades confient la Tunisie à Abû Muhammad `Abd al-Wâhid ben Abî Hafs mais son fils Abû Zakariyâ Yahyâ se sépare d’eux en 1228 et fonde la nouvelle dynastie berbère des Hafsides. Elle acquiert son indépendance dès 1236 et dirige la Tunisie jusqu’en 1574, ce qui en fait la première dynastie tunisienne par sa durée. Elle établit la capitale du pays à Tunis, et la ville se développe grâce au commerce avec les Vénitiens, les Génois, les Aragonais et les Siciliens.

Les Hafsides de Tunis s’essoufflent et perdent peu à peu, après la bataille de Kairouan en 1348, le contrôle de leurs territoires au profit des Mérinides d’Abu Inan Faris, alors que, frappée de plein fouet par la peste de 1384, l’Ifriqiya continue de subir une désertification démographique amorcée par les invasions hilaliennes. C’est alors que commencent à arriver les Maures musulmans et juifs andalous fuyant la déchéance du royaume de Grenade en 1492 et occasionnant des problèmes d’assimilation. En une dizaine d’années, les souverains espagnols Ferdinand d’Aragon et Isabelle de Castille prennent les cités de Mers el-Kébir, Oran, Bougie, Tripoli et l’îlot situé en face d’Alger. Pour s’en libérer, les autorités de la cité sollicitent l’aide de deux corsaires renommés, d’origine grecque : les frères Arudj et Khayr ad-Din Barberousse.

La Tunisie offrant un environnement favorable, les frères Barberousse s’y illustrent : Arudj reçoit en effet du souverain hafside aux abois l’autorisation d’utiliser le port de La Goulette puis l’île de Djerba comme base. Après la mort d’Arudj, son frère Khayr ad-Din se place dans la vassalité du sultan d’Istanbul. Nommé grand amiral de l’Empire ottoman, il s’empare de Tunis en 1534 mais doit se retirer après la prise de la ville par l’armada que Charles Quint mène en 1535. En 1560, Dragut parvient à Djerba et, en 1574, Tunis est reprise par les Ottomans, qui font de la Tunisie une province de leur empire en 1575. Pourtant, malgré leurs victoires, les Ottomans ne s’implantent guère en Tunisie.

Au cours du , leur rôle ne cesse de décroître au profit des dirigeants locaux qui s’émancipent progressivement de la tutelle du sultan d’Istanbul alors que seuls sont en poste à Tunis. Au bout de quelques années d’administration turque, plus précisément en 1590, ces janissaires s’insurgent, plaçant à la tête de l’État un dey et, sous ses ordres, un bey chargé du contrôle du territoire et de la collecte des impôts. Ce dernier ne tarde pas à devenir le personnage essentiel de la régence aux côtés du pacha, qui reste confiné dans le rôle honorifique de représentant du sultan ottoman, au point qu’une dynastie beylicale finit par être fondée par Mourad Bey en 1613.

Le , Hussein Bey fonde la dynastie des Husseinites. Quoique toujours officiellement province de l’Empire ottoman, la Tunisie acquiert une grande autonomie au , notamment avec Ahmed Bey, régnant de 1837 à 1855, qui enclenche un processus de modernisation.

À cette époque, le pays vit de profondes réformes, comme l’abolition de l’esclavage et l’adoption en 1861 d’une Constitution, et manque même de devenir une république indépendante.

Il est difficile de mesurer l’importance des influences turques qui demeurent en Tunisie. Quelques monuments affichent leur filiation ottomane à l’instar de la mosquée Sidi Mahrez à Tunis, édifiée entre 1692 et 1697. Dans un autre domaine, l’art des tapis, qui existait pour certains avant l’arrivée des Ottomans, voit les productions de Kairouan présenter au des motifs purement anatoliens.

Malgré ces influences perceptibles dans l’aspect des objets manufacturés, l’empreinte de l’Italie voisine se fait de plus en plus manifeste au cours du , tant dans l’architecture que dans la décoration, marquant ainsi une ouverture du pays à l’Europe.

Le pays connaît toutefois peu à peu de graves difficultés financières, en raison de la politique ruineuse des beys, de la hausse des impôts et d’interférences étrangères dans l’économie. Tous ces facteurs contraignent le gouvernement à déclarer la banqueroute en 1869 et à créer une commission financière internationale anglo-franco-italienne. La régence apparaît vite comme un enjeu stratégique de première importance de par la situation géographique du pays, à la charnière des bassins occidental et oriental de la Méditerranée. La Tunisie fait donc l’objet des convoitises rivales de la France et de l’Italie. Les consuls français et italien tentent de profiter des difficultés financières du bey, la France comptant sur la neutralité de l’Angleterre (peu désireuse de voir l’Italie prendre le contrôle de la route du canal de Suez) et bénéficiant des calculs de Bismarck, qui souhaite la détourner de la question de l’Alsace-Lorraine.

Les combats entre tribus algériennes et tribus khroumirs en territoire algérien fournissent un prétexte à Jules Ferry pour souligner la nécessité de s’emparer de la Tunisie. En avril 1881, les troupes françaises y pénètrent sans résistance majeure et parviennent aux abords de Tunis en trois semaines, sans combattre. Le , le protectorat est officialisé lorsque Sadok Bey, menacé d’être destitué et remplacé par son frère Taïeb Bey, signe le traité du Bardo au palais de Ksar Saïd. Ceci n’empêche pas les troupes françaises de faire face, quelques mois plus tard, à des révoltes rapidement étouffées dans les régions de Kairouan et Sfax. Le régime du protectorat est renforcé par les conventions de La Marsa du qui accordent à la France le droit d’intervenir dans les affaires internes de la Tunisie. La France représente dès lors la Tunisie sur la scène internationale, et ne tarde pas à abuser de ses droits et prérogatives de protecteur pour exploiter le pays comme une colonie, en contraignant le bey à abandonner la quasi-totalité de ses pouvoirs au résident général. Néanmoins, des progrès économiques ont lieu, notamment via les banques et les compagnies, ainsi que le développement de nombreuses infrastructures (routes, ports, chemins de fer, barrages, écoles, etc.).

La colonisation permet l’expansion des cultures de céréales et de la production d’huile d’olive ainsi que l’exploitation des mines de phosphates par la Compagnie des phosphates et des chemins de fer de Gafsa, ainsi que de fer par la Société du Djebel Djerissa, première entreprise tunisienne et quinzième française. Un important port militaire est aménagé à Bizerte. De plus, les Français établissent un système bilingue arabe et français qui donne l’opportunité à l’élite tunisienne de se former dans les deux langues. La lutte contre l’occupation française commence dès le début du avec le mouvement réformiste et intellectuel des Jeunes Tunisiens fondé en 1907 par Béchir Sfar, Ali Bach Hamba et Abdeljelil Zaouche. Ce courant nationaliste se manifeste par l’affaire du Djellaz en 1911 et le boycott des tramways tunisois en 1912. De 1914 à 1921, le pays vit en état d’urgence et la presse anticolonialiste est interdite. Malgré tout, le mouvement national ne cesse pas d’exister. Dès la fin de la Première Guerre mondiale, une nouvelle génération organisée autour d’Abdelaziz Thâalbi prépare la naissance du parti du Destour.

Entré en conflit avec le régime du protectorat, le parti expose, dès la proclamation officielle de sa création le , un programme en huit points. Après avoir fustigé le régime du protectorat dans des journaux comme "La Voix du Tunisien" et "L’Étendard tunisien", l’avocat Habib Bourguiba fonde en 1932, avec Tahar Sfar, Mahmoud El Materi et Bahri Guiga, le journal "L'Action tunisienne", qui, outre l’indépendance, prône la laïcité. Cette position originale conduit le , lors du congrès de Ksar Hellal, à la scission du parti en deux branches, l’une islamisante qui conserve le nom "Destour", et l’autre moderniste et laïque, le Néo-Destour, une formation politique moderne, structurée sur les modèles des partis socialistes et communistes européens, et déterminée à conquérir le pouvoir pour transformer la société.

Après l’échec des négociations engagées par le gouvernement Blum, des incidents sanglants éclatent en 1937 et les émeutes d’avril 1938 sont sévèrement réprimées. Cette répression conduit à la clandestinité du Néo-Destour, qui incite les nouveaux dirigeants à ne pas exclure l’éventualité d’une lutte plus active. En 1942, le régime de Vichy livre Bourguiba à l’Italie, à la demande de Benito Mussolini, qui espère l’utiliser pour affaiblir la Résistance française en Afrique du Nord. Cependant Bourguiba ne désire pas cautionner les régimes fascistes et lance le un appel pour le soutien aux troupes alliées. Pendant ce temps, la Tunisie est le théâtre d’importantes opérations militaires connues sous le nom de campagne de Tunisie Après plusieurs mois de combats et une contre-offensive blindée allemande dans la région de Kasserine et Sidi Bouzid au début de l’année 1943, les troupes du Troisième Reich sont contraintes de capituler le 11 mai dans le cap Bon, quatre jours après l’arrivée des forces alliées à Tunis. Après la Seconde Guerre mondiale, les dirigeants nationalistes inscrivent la résistance armée dans la stratégie de libération nationale. Des pourparlers sont menés après la guerre avec le gouvernement français, si bien que Robert Schuman évoque en 1950 la possibilité de l’indépendance de la Tunisie en plusieurs étapes. Mais le gouvernement français met fin aux négociations avec le gouvernement tunisien par la affirmant le .

Avec l’arrivée du nouveau résident général, Jean de Hauteclocque, le , et l’arrestation, le 18 janvier, de dont Bourguiba, débutent la révolte armée, la répression militaire française et un durcissement des positions de chaque camp. De plus, avec l’assassinat du syndicaliste Farhat Hached par l’organisation colonialiste extrémiste de la Main rouge, le 5 décembre, se déclenchent manifestations, émeutes, grèves, tentatives de sabotage et jets de bombes artisanales.

Le développement de la répression, accompagnée de l’apparition du contre-terrorisme, incite les nationalistes à prendre plus spécifiquement pour cibles les colons, les fermes, les entreprises françaises et les structures gouvernementales. C’est pourquoi les années 1953 et 1954 sont marquées par la multiplication des attaques contre le système colonial.

En réponse, près de soldats français sont mobilisés pour arrêter les guérillas des groupes tunisiens dans les campagnes. Cette situation difficile est apaisée par la reconnaissance de l’autonomie interne de la Tunisie, concédée par Pierre Mendès France le . C’est finalement le que les conventions franco-tunisiennes sont signées entre le Premier ministre tunisien Tahar Ben Ammar et son homologue français Edgar Faure. En dépit de l’opposition de Salah Ben Youssef, qui sera exclu du parti, les conventions sont approuvées par le congrès du Néo-Destour tenu à Sfax le 15 novembre de la même année. Après de nouvelles négociations, la France finit par reconnaître le , tout en conservant la base militaire de Bizerte.

Le , l’Assemblée constituante est élue : le Néo-Destour en remporte tous les sièges et Bourguiba est porté à sa tête le 8 avril de la même année. Le , il devient le Premier ministre de Lamine Bey. Le Code du statut personnel, à tendance progressiste, est proclamé le 13 août.

Finalement, le , la monarchie est abolie ; la Tunisie devient une république dont Bourguiba est élu président le .

Le , en pleine guerre d'Algérie, des avions de l’armée française franchissent la frontière algéro-tunisienne et bombardent le village tunisien de Sakiet Sidi Youssef. En 1961, dans un contexte d’achèvement prévisible de la guerre, la Tunisie revendique la rétrocession de la base de Bizerte.

La crise qui suit fait près d’un millier de morts, essentiellement tunisiens, et la France finit, le , par rétrocéder la base à l’État tunisien. Avec l’assassinat de Salah Ben Youssef, principal opposant de Bourguiba depuis 1955, à Francfort et l’interdiction du Parti communiste (PCT) le , la République tunisienne devient un régime de parti unique dirigé par le Néo-Destour. En mars 1963, Ahmed Ben Salah entame une politique d’étatisation pratiquement totale de l’économie. Des émeutes contre la collectivisation des terres dans le Sahel tunisien le poussent au limogeage de Ben Salah le 8 septembre avec la fin de l’expérience socialiste. Avec une économie affaiblie par cet épisode et un panarabisme défendu par Mouammar Kadhafi, un projet politique qui unifierait la Tunisie et la République arabe libyenne sous le nom de République arabe islamique est lancé en 1974 mais échoue très rapidement en raison des tensions tant nationales qu’internationales.

Après la condamnation à une lourde peine de prison de Ben Salah, rendu responsable de l’échec de la politique des coopératives, viennent l’épuration de l’aile libérale du PSD animée par Ahmed Mestiri puis la proclamation de Bourguiba comme président à vie en 1975. C’est dans ces conditions, marquées par un léger desserrement de l’étau du PSD sous le gouvernement d’Hédi Nouira, que l’Union générale tunisienne du travail (UGTT) gagne en autonomie tandis que naît en 1976 la Ligue tunisienne des droits de l'homme, première organisation nationale des droits de l’homme en Afrique et dans le monde arabe. Le coup de force du contre l’UGTT en janvier 1978 puis l’attaque contre la ville minière de Gafsa, en janvier 1980, ne suffisent pas à museler la société civile émergente.

Dès le début des années 1980, le pays traverse une crise politique et sociale où se conjuguent le développement du clientélisme et de la corruption, la paralysie de l’État devant la dégradation de la santé de Bourguiba, les luttes de succession et le durcissement du régime. En 1981, la restauration partielle du pluralisme politique, avec la levée de l’interdiction frappant le Parti communiste, suscite des espoirs qui seront déçus par la falsification des résultats aux élections législatives de novembre. Par la suite, la répression sanglante des de décembre 1983, la nouvelle déstabilisation de l’UGTT et l’arrestation de son dirigeant Habib Achour contribuent à accélérer la chute du président vieillissant. La situation favorise la montée de l’islamisme et le long règne de Bourguiba s’achève dans une lutte contre cette mouvance politique, lutte menée par Zine el-Abidine Ben Ali, nommé ministre de l’Intérieur puis Premier ministre en octobre 1987.

Le , Ben Ali dépose le président pour sénilité, un coup d’État médical accueilli favorablement par une large fraction du monde politique. Élu le avec 99,27 % des voix, le nouveau président réussit à relancer l’économie alors que, sur le plan de la sécurité, le régime s’enorgueillit d’avoir épargné au pays les convulsions islamistes qui ensanglantent l’Algérie voisine, grâce à la neutralisation du parti Ennahdha au prix de l’arrestation de dizaines de milliers de militants et de multiples procès au début des années 1990. Les opposants laïcs signent quant à eux le Pacte national en 1988, plate-forme destinée à la démocratisation du régime. Pourtant, l’opposition et de nombreuses ONG de défense des droits de l'homme accusent peu à peu le régime d’attenter aux libertés publiques en étendant la répression au-delà du mouvement islamiste. En 1994, le président Ben Ali est réélu avec 99,91 % des voix.

L’année suivante, un accord de libre-échange est signé avec l’Union européenne. Les élections du , bien qu’elles soient les premières présidentielles à être pluralistes avec trois candidats, voient le président Ben Ali réélu avec un score comparable aux scrutins précédents. La réforme de la Constitution approuvée par le référendum du accroît encore les pouvoirs du président, repousse l’âge limite des candidats, supprime la limite des trois mandats réintroduite en 1988 et permet au président de briguer de nouveaux mandats au-delà de l’échéance de 2004 tout en bénéficiant d’une immunité judiciaire à vie.

Le , un attentat au camion piégé vise la synagogue de la Ghriba et provoque la mort de 19 personnes dont quatorze touristes allemands. Durant le premier semestre 2008, de graves troubles secouent la région minière de Gafsa durement frappée par le chômage et la pauvreté. Le , le président Ben Ali est réélu pour un cinquième mandat consécutif avec 89,62 % des voix, passant pour la première fois sous la barre des 90 %. La campagne est marquée par une visibilité accrue de son épouse Leïla. L’un des gendres du couple, Mohamed Sakhr El Materi, est élu député à cette occasion.

Le , un climat insurrectionnel éclate à la suite de l’immolation d’un jeune vendeur de fruits et légumes ambulant, Mohamed Bouazizi, dans la région de Sidi Bouzid ; celle-ci devient le théâtre d’émeutes et d’affrontements meurtriers entre habitants et forces de l’ordre.

Ces événements, qui s’étendent ensuite à d’autres régions du pays, s’inscrivent dans un contexte où le taux de chômage des jeunes diplômés est particulièrement élevé alors que le poids démographique relatif des jeunes générations d’actifs atteint son maximum historique. Les causes sont également politiques : le président Ben Ali et sa famille, notamment celle de sa seconde épouse Leïla, les Trabelsi, qualifiés selon les observateurs de , sont directement mis en cause dans des affaires de corruption, de détournement ou de vol, fléaux qui ont particulièrement pris de l’ampleur sous sa présidence. Le , Ben Ali annonce la prise de mesures extraordinaires lors d’une intervention télévisée : la promesse d’une pleine liberté de la presse et d’expression politique ainsi que son refus de se représenter aux élections prévues en 2014. Cependant, cette allocution ne contribue pas à calmer la colère de la population, contraignant le président à céder finalement le pouvoir à son Premier ministre Mohamed Ghannouchi le lendemain et à quitter le pays le soir même. Conformément à la Constitution de 1959, le président de la Chambre des députés, Fouad Mebazaa, est finalement proclamé président par intérim par le Conseil constitutionnel le 15 janvier.

Il est chargé d’organiser des élections présidentielles dans les soixante jours. Le 17 janvier, un de 24 membres incluant des opposants au régime déchu (dont trois chefs de l’opposition légale) est constitué. Le jour même, la libération de tous les prisonniers d’opinion, la levée de l’interdiction d’activité de la Ligue tunisienne des droits de l'homme, ainsi que la légalisation de tous les partis politiques et associations qui le demanderaient, est annoncée. Cependant, la présence de membres du Rassemblement constitutionnel démocratique (RCD) à des postes clés provoque de nouveau, en moins de 24 heures, la colère de la population et la démission de plusieurs ministres d’opposition, fragilisant d’autant plus ce gouvernement. Le départ ou la radiation du RCD de plusieurs personnalités éminentes n’ont aucun effet sur la suspicion que l’opinion publique entretient à l’égard de l’ancien parti présidentiel, dont plusieurs manifestants réclament la dissolution. Cependant, le 20 janvier, les ministres encore affiliés à cette formation annoncent l’avoir quitté eux aussi. Face à la pression de la rue exigeant leur départ, un remaniement ministériel a lieu le 27 janvier, écartant définitivement (hormis Mohamed Ghannouchi) les anciens membres du RCD de toutes responsabilités gouvernementales. Le 6 février, le ministre de l’Intérieur Farhat Rajhi gèle les activités du RCD en attendant sa dissolution juridique, tandis que le Parlement confère au président par intérim des pouvoirs supplémentaires, comme celui de dissoudre le Parlement.

Ghannouchi est cependant contraint de démissionner à son tour le 27 février à la suite de plusieurs jours de manifestations marquées par des violences ; il est remplacé le jour même par l’ancien ministre de Bourguiba, Béji Caïd Essebsi. L’état d'urgence, en vigueur à partir de janvier 2011, est maintenu.

Le 15 septembre 2012, de violentes émeutes éclatent à Tunis à la suite de la diffusion du film "L'Innocence des musulmans". Alors que les forces de l’ordre restent passives, certains groupes salafistes prennent d’assaut l’ambassade des États-Unis et l’incendient, détruisant plusieurs véhicules et bâtiments. Mis sous pression par les États-Unis, le gouvernement décide de réagir et envoie l’armée et la garde présidentielle pour repousser les manifestants. Les affrontements font deux morts et plusieurs blessés. Dans les mois qui suivent, l’armée et la garde nationale prennent la relève pour combattre les groupuscules salafistes et djihadistes qui sont actifs sur le territoire. L’état d’urgence est prolongé de trois mois en novembre 2012, pour n’être finalement levé qu’en mars 2014.

Après les élections législatives du 26 octobre 2014, qui voit le parti Nidaa Tounes arriver en tête, l’Assemblée des représentants du peuple remplace l’Assemblée constituante. Le premier tour de l’élection présidentielle a lieu le 23 novembre et voit s’affronter 27 candidats dont deux, en la personne de Béji Caïd Essebsi (Nidaa Tounes) avec 39,46 % des voix et Moncef Marzouki avec 33,43 % des voix, sont qualifiés pour le second tour organisé le 21 décembre et qui permet à Caïd Essebsi de remporter le scrutin avec 55,68 % des voix contre 44,32 % des voix pour Marzouki et de devenir ainsi le premier président issu d’une élection démocratique et transparente. Le quartet du dialogue national, association de quatre organisations s’étant donné pour but d’organiser des négociations entre les partis politiques tunisiens pour assurer la transition vers un régime démocratique permanent, obtient le prix Nobel de la paix 2015. Ce prix est le premier Nobel attribué à un ressortissant ou organisation de la Tunisie après son indépendance. Ban Ki-moon, secrétaire général des Nations unies, exprime sa joie et félicite le quartet tout en affirmant que ce prix est dédié à tous les Tunisiens qui ont commencé le Printemps arabe. François Hollande, président de la République française, affirme dans un communiqué que le prix prouve le succès de la transition démocratique en Tunisie, que ce pays est sur la bonne voie et qu’il est le seul parmi les pays du Printemps arabe à réussir son évolution transitoire vers la démocratie.

En janvier 2018, le pays est touché par une vague de contestation de la jeunesse tunisienne qui manifeste dans plusieurs villes du pays. En effet, à partir du début du mois, à Tunis, Gabès, Thala, Jilma, Kasserine, Sidi Bouzid, ou encore Gafsa, des Tunisiens expriment leur ras le bol face à la cherté de la vie, l’inflation (6,4 % en 2017) et un chômage omniprésent (15 % de la population active et 30 % des jeunes diplômés de l’enseignement supérieur). Cette vague de contestation contre une politique d’austérité économique serait organisée par le Front populaire. Les heurts avec les policiers et forces de l'ordre font une victime et plusieurs blessés, et des centaines de manifestants sont arrêtés.

Une Assemblée constituante rédige une Constitution proclamée le 1959, trois ans après l’indépendance. Elle subit plusieurs amendements dont celui du pour limiter le nombre de mandats présidentiels à trois et celui du 2002 à la suite du référendum constitutionnel tenu le 26 mai de la même année, permettant notamment la suppression de la limite du nombre de mandats présidentiels, l’allongement de l’âge limite pour déposer une candidature à la présidence, l’instauration d’une immunité judiciaire pour le président durant et après l’exercice de ses fonctions et l’instauration d’un Parlement bicaméral.

Le manque de transparence politique, la faible liberté d'expression et la censure, notamment de la presse et de nombreux sites web, ont longtemps fait qu’une situation politique précise de la Tunisie a été difficile à déterminer. De nombreuses ONG internationales ont toutefois pointé du doigt les atteintes aux droits de l’homme, notamment en ce qui concerne les atteintes à la liberté d’expression, les prisonniers politiques et d’opinion, l’instrumentalisation de la justice par le pouvoir exécutif, la torture et la situation dans les prisons, ainsi que le harcèlement de toute dissidence politique. De leur côté, les autorités de l’époque ont fait valoir que leurs efforts en matière de droits de l’homme ont été officiellement reconnus par des instances internationales comme le Conseil des droits de l'homme des Nations unies dont les membres ont souligné, avec quelques réserves pour certains, les progrès accomplis par le pays en la matière.

La Tunisie ne connaît que deux présidents de la République en cinq décennies : Bourguiba du au puis Zine el-Abidine Ben Ali du au . Au niveau des partis, le Néo-Destour puis le Parti socialiste destourien et le Rassemblement constitutionnel démocratique dominent la vie politique après l’indépendance, dont une vingtaine d’années en tant que seul parti politique légal, avec plus de deux millions d’adhérents revendiqués.

La révolution du et la chute du régime Ben Ali changent la donne. Le Rassemblement constitutionnel démocratique est dissous et la scène politique compte rapidement une centaine de partis politiques. Fouad Mebazaa assure à titre intérimaire la présidence de la République du 15 janvier au , avant d’être remplacé par Moncef Marzouki à partir du . Mohamed Ghannouchi, ayant assuré l’intérim du pouvoir durant 24 heures après la fuite de Ben Ali, est placé à la tête du gouvernement de transition avant d’être remplacé par Béji Caïd Essebsi. La Chambre des députés et la Chambre des conseillers sont dissoutes et leurs pouvoirs assumés de fait par la Haute instance pour la réalisation des objectifs de la révolution puis, à la suite de l’élection du 23 octobre 2011, premier scrutin pluraliste et transparent organisé par l’Instance supérieure indépendante pour les élections au détriment du ministère de l’Intérieur, par l’Assemblée constituante. La Constitution est suspendue et remplacée par le décret-loi du 23 mars 2011 puis la loi constitutive du 16 décembre 2011. Hamadi Jebali forme alors un gouvernement de coalition dominé par Ennahdha, reconduit par Ali Larayedh à partir du .

En 2014, une nouvelle Constitution est votée par l’Assemblée constituante qui établit un régime semi-présidentiel où le président de la République conserve des pouvoirs en matière de politique étrangère, de défense et de sécurité intérieure. Il est élu tous les cinq ans au suffrage universel et ne peut prétendre qu’à deux mandats présidentiels. Responsable de l’action gouvernementale, le chef du gouvernement est le candidat du parti ou de la coalition qui obtient la majorité de siège à l’Assemblée des représentants du peuple. Il est nommé par le président de la République et définit la politique générale de l’État. Le pouvoir législatif, monocaméral, est exercé par l’assemblée composée de 217 députés.

Mehdi Jomaa forme un gouvernement de technocrates le après l’adoption de la nouvelle Constitution. Après les élections législatives du 26 octobre 2014, qui voit Nidaa Tounes arrivé en tête, l’élection présidentielle, organisée en deux tours, voit Béji Caïd Essebsi, leader de Nidaa Tounes, être élu avec 55,68 % des voix contre 44,32 % des voix pour Marzouki. Habib Essid forme dans la foulée un nouveau gouvernement, remplacé à l’été 2016 par celui de Youssef Chahed.

Le premier président, Habib Bourguiba, choisit le non-alignement durant la guerre froide tout en ayant des relations étroites avec l’Europe et les États-Unis. Son successeur, Zine el-Abidine Ben Ali, maintient la tradition tunisienne de bonnes relations avec l’Occident tout en jouant un rôle actif dans les instances régionales arabes et africaines : le pays accueille, en mai 2004, la ordinaire du sommet de la Ligue arabe (dont la Tunisie est membre depuis 1958) au cours de laquelle est adoptée la Charte arabe des droits de l’homme et envoie régulièrement de l’aide humanitaire aux Palestiniens et aux États en crise. Le pays est également un membre fondateur de l’OUA, dont elle assure la présidence en 1994-1995, avant de participer à la fondation de l’Union africaine en juillet 2002.

La Tunisie a également soutenu le développement de l’Union du Maghreb arabe qui inclut l’Algérie, le Maroc, la Mauritanie et la Libye. Toutefois, ses progrès restent limités en raison de tensions entre l’Algérie et le Maroc à propos du Sahara occidental. En février 2001, la Tunisie adhère à la Communauté des États sahélo-sahariens et accueille le siège de la Banque africaine de développement en 2003. Le pays est depuis longtemps une voix modératrice sur la question du Proche-Orient : Bourguiba est ainsi le premier dirigeant arabe à appeler à la reconnaissance d’Israël par les pays arabes dans un discours prononcé à Jéricho le .

Le pays abrite le quartier général de la Ligue arabe de 1979 à 1990 ainsi que celui de l’OLP de 1982 à 1993, jusqu’à ce que son comité exécutif s’installe dans les Territoires occupés, bien que son département politique reste à Tunis. Le pays joue également un rôle modérateur dans les négociations de paix au Proche-Orient : la Tunisie est le premier pays arabe à recevoir une délégation israélienne en 1993, dans le cadre du processus de paix, et maintient une représentation en Israël jusqu’au début de la seconde Intifada en 2000.

Coincée entre l’Algérie et la Libye, la Tunisie a toujours cherché à maintenir de bonnes relations avec ses voisins malgré des tensions occasionnelles. La Tunisie et l’Algérie ont résolu un long litige frontalier en 1993 et ont coopéré dans la construction du gazoduc transméditerranéen menant vers l’Italie. La Tunisie a par ailleurs récemment signé un accord avec l’Algérie pour démarquer la frontière maritime entre les deux pays.

Vis-à-vis de son autre voisin, les relations sont plus difficiles à partir de l’annulation par la Tunisie d’un accord visant à la formation d’une union tuniso-libyenne en 1974.

Les relations diplomatiques sont rompues entre 1976 et 1977 puis se détériorent à nouveau en 1980 lorsque des rebelles appuyés par la Libye tentent de prendre la ville de Gafsa. En 1982, la Cour internationale de justice tranche le différend relatif à la partition du plateau continental frontalier (riche en pétrole) en faveur de la Libye. L’expulsion par la Libye de nombreux travailleurs tunisiens en 1985 et les menaces militaires américaines conduisent la Tunisie à restreindre leurs relations qui sont à nouveau normalisées dès 1987. Tout en soutenant les sanctions de l’ONU imposées à la Libye, à la suite de bombardements aériens américains, la Tunisie prend soin de maintenir de bonnes relations avec son voisin. Elle soutient ainsi la levée de ces sanctions en 2003, la Libye redevenant ainsi l’un de ses partenaires commerciaux majeurs. Néanmoins, les deux pays ont encore un contentieux maritime sur leur frontière commune.

La Tunisie revendique également sa dimension méditerranéenne. Elle participe ainsi au Forum méditerranéen, dont elle organise l’édition 2005, et devient le premier pays du bassin méditerranéen à signer, le , un accord d’association avec l’Union européenne dans le but de renforcer son ancrage à l’Europe. Avec son plus proche voisin européen, Malte, la Tunisie discute actuellement de l’exploitation pétrolière du plateau continental qui se trouve entre les deux pays.

L’action politique de la Tunisie dépasse pourtant les frontières régionales. Lors d’un discours prononcé devant l’Assemblée générale des Nations unies en 1999, le président Ben Ali appelle à la création d’un Fonds mondial de solidarité (en s’inspirant du Fonds de solidarité nationale) visant à contribuer à la lutte contre la pauvreté dans les zones les plus déshéritées dans le monde. L’Assemblée générale adopte à l’unanimité, le , une résolution portant création de ce fonds et instaurant les modalités pratiques requises pour sa mise en place.

La défense extérieure de la Tunisie est exercée par l’armée. Fondée le , elle compte un personnel régulier de dont dans l’armée de terre. Elle participe surtout à des activités civiles de développement et de lutte contre les catastrophes naturelles et à des opérations militaires de maintien de la paix sous couvert des Nations unies.

Le droit tunisien reste largement inspiré par le droit français, tant dans son contenu que dans ses grandes divisions (public et privé) et ses structures.

La Constitution garantit les principes fondamentaux suivants :

Sous les régimes de Habib Bourguiba et Zine el-Abidine Ben Ali, la justice tunisienne demeure influencée par le pouvoir exécutif. Ainsi, en tant que chef du Conseil supérieur de la magistrature, le président nomme par décret les magistrats, les révoque ou les transfère sur proposition dudit conseil. Certains principes fondamentaux du droit, comme le principe de la présomption d’innocence (art. 12 de la Constitution de 1959) et la non-rétroactivité de la loi (art. 13 de la Constitution de 1959), sont garantis, tout comme l’inviolabilité de domicile, la liberté de mouvement et les libertés d’opinion, d’expression, de publication, de réunion et d’association, mais ces droits peuvent être toutefois limités par des dispositions légales ou la sécurité d’État (art. 8-10). Le système juridictionnel est alors précisé par la loi portant sur l’organisation judiciaire de 1967, les règles de compétence (attribution, compétence territoriale et compétence d’exception) étant établies dans d’autres textes dont le Code de procédure pénale du .

Le système judiciaire est composé de trois grands ensembles :

Les tribunaux militaires sont compétents en matière de crimes militaires. Quant au Conseil supérieur de la magistrature, il se compose pour deux tiers de magistrats en majorité élus et qui élisent un président parmi leurs membres. Une Cour constitutionnelle contrôle sur demande la constitutionnalité des lois, des traités internationaux et du règlement intérieur de l’Assemblée des représentants du peuple.

En février 2011, le nombre de condamnés à mort est de 130 dont quatre femmes. La dernière exécution d’une sentence de peine de mort remonte à octobre 1991. Le , le parlement adopte une loi rétablissant la peine de mort contre des actes terroristes.

En 2010, le produit intérieur brut (PIB) de la Tunisie atteint 57,17 milliards de dinars (39,58 milliards de dollars) soit une hausse de 7 % par rapport à 2009. En 1960, celui-ci ne se montait qu’à de dollars, passant à en 1970, en 1980, en 1990 et en 1999. Quant à la population active, elle atteint de personnes en 2010 mais la population active occupée totalise de personnes, dont près de 30 % de femmes, ce qui représente tout de même plus du double du niveau de 1980.

Le Printemps arabe a eu des conséquences désastreuses sur l’économie du pays. Les attentats islamistes ont touché le tourisme qui représentait près de 7 % du PIB national. Avec une croissance économique quasi nulle, le pays est proche de la récession et connaît un spectaculaire envol de sa dette qui atteint 60 % du PIB.

À la proclamation de l’indépendance en 1956, le pays ne dispose pas des atouts de ses voisins maghrébins : terres agricoles moins productives, infrastructure portuaire moins développée, marché intérieur étriqué, épargne faible et écornée par l’émigration des populations d’origine européenne et relations avec les milieux d’affaires français réduits, chômage élevé et équipement industriel embryonnaire. La priorité établie par le nouveau président Habib Bourguiba est alors de libérer l’économie nationale du contrôle français qui avait favorisé l’agriculture et l’extraction minérale, mais avait, en grande partie, négligé l’industrie, la Tunisie étant alors le pays le moins industrialisé du Maghreb. Dans ce contexte, l’importance croissante de l’Union générale tunisienne du travail (UGTT) dans les choix économiques par l’action de son secrétaire général, Ahmed Ben Salah, mène le pays vers l’adoption de mesures collectivistes dans l’économie. L’expérience coopérative dure jusqu’en septembre 1969 lorsque Bourguiba suspend Ben Salah de ses fonctions à la suite de la parution d’un rapport confidentiel de la Banque mondiale sur le déficit des entreprises publiques et aux pressions de l’aile pragmatique du parti. Avec l’arrivée d’Hédi Nouira, pragmatique gouverneur de la Banque centrale hostile au collectivisme, au ministère de l’Économie puis au Premier ministère, la Tunisie se réoriente vers l’économie de marché et la propriété privée. Durant la décennie des années 1970, la Tunisie connaît une expansion du secteur privé et un développement rapide de l’emploi manufacturier. Cette timide ouverture permet la création de nouveaux emplois et, par conséquent, le développement d’une meilleure mobilité sociale de la jeunesse nouvellement instruite et la croissance d’une classe moyenne.

En 1986, la Tunisie connaît toutefois sa première année de croissance négative depuis son indépendance. Les agitations sociales augmentent de façon dramatique pendant cette période et l’UGTT, qui critique ouvertement la politique économique adoptée par le gouvernement, organise des grèves et des manifestations contre l’augmentation du chômage et la politique salariale. Le gouvernement se met alors d’accord avec le Fonds monétaire international sur la mise en place d’un programme de reprise économique sur 18 mois. L’objectif principal du plan est d’accroître l’efficacité et de promouvoir les mécanismes du marché. En même temps, il est conçu pour surmonter les conséquences sociales et politiques de ses mesures. Les dépenses publiques sont concentrées aux secteurs de la santé, de l’éducation, du logement et des services. Le gouvernement ne lance cependant pas de véritables programmes avant 1987.

La privatisation se traduit dans un premier temps par la vente de petites et moyennes entreprises avec un bon historique bancaire à des acheteurs tunisiens présélectionnés. Depuis le lancement du nouveau programme de privatisation en 1987, le gouvernement a totalement ou partiellement privatisé , dont de grands établissements publics tel Tunisie Télécom, pour une recette globale de de dinars. De plus, la non préparation de plusieurs secteurs à l’ouverture a conduit au maintien d’un niveau de chômage élevé et variant selon les sources de 13 % à 20 %. Pourtant, le chômage ne touche pas que les populations les plus vulnérables : le taux de chômage des diplômés de l’enseignement supérieur est ainsi en augmentation depuis plusieurs années. Alors qu’il était de 4 % en 1997 et de 0,7 % en 1984, il atteint 20 % contre une moyenne nationale de 14 %, voire près de 60 % dans certaines filières selon une enquête de la Banque mondiale. En 1959, le pays prend ses premiers contacts avec la Communauté économique européenne. En juillet 1966, le président Bourguiba effectue une tournée en Europe et aboutit au lancement de négociations qui conduisent à la signature d’un premier accord commercial le à Tunis.

Un accord d’association est finalement signé avec l’Union européenne le et entre en vigueur le 1998 pour engendrer dès 1996 le démantèlement progressif des barrières douanières jusqu’au 2008.

Depuis l’indépendance de la Tunisie, l’agriculture a enregistré des taux de croissance importants et a permis au pays d’atteindre un niveau de sécurité alimentaire suffisant. En dépit du développement des autres secteurs de l’économie nationale, l’agriculture conserve une importance sociale et économique : elle assure environ 12,3 % du PIB et emploie 16,3 % de la main-d’œuvre en 2006. Les principales productions agricoles sont les céréales (blé et orge), les olives ( et exportateur mondial d’huile d’olive en 2007-2008), les dattes, les agrumes et les produits de la mer.

Si la gestion de l’agriculture appartient encore à des établissements publics, tels l’Office des céréales ou l’Office national de l’huile, le secteur agricole est de plus en plus pris en charge par des groupes privés souvent présents dans l’industrie agroalimentaire tel le groupe Poulina, le premier groupe privé du pays.

En matière industrielle, la Tunisie est le premier exportateur d’Afrique en valeur absolue : elle est ainsi passée devant l’Afrique du Sud en 1999. Les secteurs du textile et de l’agroalimentaire représentent 50 % de la production et 60 % de l’emploi de l’industrie manufacturière. Mais, après avoir cru à un rythme annuel de 2,1 % (entre 2000 et 2005), l’industrie tunisienne fait aujourd’hui face à la concurrence étrangère. Toutefois, les exportations de produits mécaniques et électriques se sont multipliées par cinq entre 1995 et 2005. Quatrième fournisseur de l’Union européenne en produits textiles, elle était jusqu’en 2002 le premier fournisseur de la France avant d’être surclassée par la Chine en 2003.

Dans le secteur des services, le développement du tourisme remonte aux années 1960 grâce à l’action conjuguée de l’État et de groupes privés. Le secteur touristique représente 6,5 % du PIB et fournit dont directs, soit 11,5 % de la population active occupée avec une forte part d’emploi saisonnier. Outre le tourisme balnéaire majoritaire, le tourisme saharien (Douz et Tozeur attirant chaque année plus de durant toute l’année) est en fort développement. Plus récemment, le tourisme vert, la thalassothérapie et le tourisme médical sont apparus et croissent très rapidement.

Le secteur du commerce et de la distribution, qui emploie plus de et participe à 10,7 % du PIB national, se divise en deux catégories. Ainsi, le secteur se caractérise encore par la prédominance du commerce traditionnel avec 88 % (2006) du chiffre d’affaires, l’essentiel des transactions commerciales étant réalisé par de petits commerçants. La distribution moderne, qui compte pour 12 % du chiffre d’affaires global et regroupe des enseignes nationales et internationales, n’est apparue que lorsque le marché s’est libéralisé en 1999.

En matière de transport, la Tunisie compte aujourd’hui sept ports de commerce (Radès, Sfax, Bizerte, Gabès, Sousse, Zarzis et La Goulette) tandis qu’un port en eaux profondes va être réalisé à Enfida. Placés sous la gestion de l’Office de la marine marchande et des ports, ils assurent à eux seuls 96 % du commerce extérieur tunisien. Avec ses et ses enregistrés en 2004, le port de La Goulette est l’une des destinations les plus appréciées dans l’ouest du bassin méditerranéen. La Compagnie tunisienne de navigation, société publique, est le principal armateur du pays et assure des lignes régulières reliant les deux rives de la mer Méditerranée (vers Marseille, Gênes, Livourne et Barcelone).

Le pays compte également 32 aéroports dont huit aéroports internationaux (Tunis-Carthage, Monastir-Habib Bourguiba, Djerba-Zarzis, Enfidha-Hammamet, Tozeur-Nefta, Sfax-Thyna, Tabarka-Aïn Draham et Gafsa-Ksar). En 2005, 39,2 % du trafic s’effectue par l’aéroport international de Tunis-Carthage.

Le transport ferroviaire assure plus du tiers des déplacements nationaux à travers un réseau national de de voies ferrées. Le réseau est exploité par la Société nationale des chemins de fer tunisiens (SNCFT) ainsi que par la Société des transports de Tunis spécialisée dans le transport urbain dans la région de Tunis.

Le réseau routier s’étend pour sa part sur dont de routes goudronnées ainsi que de trois autoroutes reliant Tunis à Sfax au sud, Bizerte au nord et Oued Zarga à l’ouest. Le secteur du transport routier domine les transports terrestres de voyageurs et de marchandises. Il est néanmoins contrôlé par les sociétés étrangères à cause du petit nombre d’entreprises tunisiennes.

Les infrastructures de télécommunications sont largement développées : le réseau téléphonique compte environ sept millions d’abonnés en 2006 dont six millions d’abonnés mobiles et environ 12,5 % de la population a accès à Internet en février 2007. L’opérateur public, Tunisie Télécom, a longtemps été le seul fournisseur de la téléphonie fixe alors que trois opérateurs se partagent à ce jour le marché de la téléphonie mobile : Tunisie Télécom, Ooredoo et Orange Tunisie.

L’Agence tunisienne d'Internet gère le réseau Web au plan national qui compte douze fournisseurs d’accès (sept publics et cinq privés). Par ailleurs, 281 "publinets" (accès publics à Internet) sont répartis sur l’ensemble du territoire.

Les ressources naturelles de la Tunisie sont modestes si on les compare à celles de ses voisines : l’Algérie et la Libye. Le secteur de l’industrie est le premier consommateur d’énergie, avec une part de 36 % de la consommation globale, suivi par le secteur du transport avec 30 % de la consommation totale.

Le phosphate est extrait par la Compagnie des phosphates de Gafsa dans plusieurs gisements situés dans le centre du pays et en particulier dans la région de Gafsa. 15 % du phosphate produit sont vendus à l’état brut et 85 % sont transformés par le Groupe chimique tunisien. En 1999, la Tunisie était le cinquième producteur mondial de phosphate avec 5,5 % du total mondial.

Selon les estimations, la Tunisie possède des réserves prouvées de pétrole brut estimées à 425 millions de barils en janvier 2015. La majorité est située dans le golfe de Gabès et le bassin de Ghadamès dans le sud du pays. Le pays produit près de barils de pétrole brut par jour en 2015, dont la majorité provient de seulement six concessions (Adam, Ashtart, Didon, El Borma, Miskar et Oued Zar) ne parvient pas à couvrir la demande locale, qui se monte à barils par jour en 2013. Le secteur est dominé par une société publique, l’Entreprise tunisienne d'activités pétrolières dont la mission est de gérer les activités d’exploration et de production de pétrole mais aussi de gaz naturel pour le compte du gouvernement.

Face aux limites de sa production pétrolière, le pays se tourne de plus en plus vers le gaz naturel pour couvrir sa demande en énergie. Le pays dispose de réserves prouvées de 65,13 milliards de pieds cubes en 2014 dont deux-tiers sont offshore. En 2013, le pays produit 1,879 milliard de pieds cubes tout en consommant 4,079 milliards de pieds cubes durant la même année. 60 % de la production provient des gisements exploités par British Gas, le plus important investisseur énergétique en Tunisie, à Miskar et Hasdrubal. Les entreprises tunisiennes constituent 19 % du marché de l’exploration et de la production du pays. L’ETAP gère les réserves nationales et agit en tant que partenaire principal dans presque toutes les activités d’exploration et de production car elle détient 51 % de toutes les concessions. Mais ce sont les entreprises américaines qui dominent avec 38 % du marché, suivi par les entreprises européennes avec 19 %, canadiennes avec 12 % et asiatiques avec 10 %.

La grande majorité de l’électricité du pays, gérée par la Société tunisienne de l'électricité et du gaz, est produite à base d’énergies fossiles (95,9 % de la capacité totale), le résidu étant produit à partir d’énergie hydroélectrique et éolienne. Le pays dispose en 2012 d’une capacité totale de 16,9 milliards de kWh alors que la consommation atteint 13,31 milliards de kWh. Dans le même temps, le gouvernement cherche à développer les énergies renouvelables.

Alors que la vaste majorité des Tunisiens tend à s’identifier culturellement aux Arabes, certaines études tendent à indiquer qu’ils seraient ethniquement plus proches des Berbères mais aussi de certains Européens : Comparés avec d’autres communautés, notre résultat indique que les Tunisiens sont très liés aux Nord-Africains et aux Européens de l’Ouest, en particulier aux Ibériques, et que les Tunisiens, les Algériens et les Marocains sont proches des Berbères, suggérant une petite contribution génétique des Arabes qui ont peuplé la région au ou . Toutefois, de nombreuses civilisations ont envahi le pays puis ont été assimilées à des degrés divers : Phéniciens, Romains, Vandales venant d’Allemagne, Ottomans et enfin Français. De plus, beaucoup de musulmans et de juifs arrivèrent d’Andalousie à la fin du . Les premiers Arabes orientaux, venus à partir du avec les conquêtes musulmanes, ont contribué à l’islamisation de la majeure partie de l’Ifriqiya. À cette occasion se créent quelques villes nouvelles dont Kairouan et Mahdia. C’est à partir du , avec l’arrivée des tribus hilaliennes chassées d’Égypte, que l’arabisation linguistique et culturelle devient déterminante. Certains groupes, descendants des Berbères, ont cependant su conserver leur langue et leurs coutumes, souvent en raison de leur enclavement géographique. En effet, de nos jours, ils habitent souvent les régions de montagnes (Matmata, Tataouine, Gafsa ou Sbeïtla). Toutefois, les berbérophones, qui représentent un important pourcentage au Maroc et en Algérie, restent peu nombreux en Tunisie.

On considère que la quasi-totalité des Tunisiens est de confession musulmane sunnite, principalement de rite malikite, bien qu’il n’existe aucun recensement couvrant l’intégralité du territoire. De la forte population juive qui a existé durant deux mille ans, il n’en reste plus qu’une infime partie, vivant principalement dans la région de Tunis et à Djerba, car la majorité des Juifs tunisiens ont émigré vers Israël ou la France. Il existe également une petite population chrétienne. Les quelques tribus nomades, minoritaires, sont pour la plupart intégrées et sédentarisées.

La Tunisie a dépassé le cap des dix millions d’habitants en 2005, ce qui correspond à un triplement de sa population depuis 1956 () et à un doublement depuis le début des années 1970. Néanmoins, la croissance démographique ralentit, le pays accélérant sa transition démographique dans les années 1990. En 2012, l’indice de fécondité est estimé à par femme. La Tunisie est aussi un pays qui connaît un taux important d’émigration : le nombre de Tunisiens résidant à l’étranger est évalué en 2012 à , dont 84,5 % résident en Europe.

La culture de la Tunisie se diversifie par un héritage de quelque ans d’histoire et une position géographique en plein bassin méditerranéen, berceau des civilisations les plus prestigieuses et des principales religions monothéistes. La Tunisie a en effet été un carrefour de civilisations et sa culture reflète les traces des cultures punique, arabe, turque, africaine, européenne et musulmane ainsi que l’influence des dynasties successives qui ont régné sur le pays.

L’islam est la religion principale et officielle de la Tunisie. La grande majorité des musulmans tunisiens sont sunnites de rite malikite, le reste étant hanafite ou ibadites. Les Tunisiens conservent paradoxalement quelques croyances comme le mauvais œil. Par ailleurs, le soufisme tient une grande place dans le pays qui est parsemé de constructions blanches que sont les zaouïas. Ce sont les tombeaux de saints qui sont censés posséder un privilège dans l’au-delà qui leur permet d’être un lien entre l’homme et Dieu. De nos jours, certains Tunisiens continuent à les fréquenter et à leur demander des faveurs.

Les fêtes religieuses musulmanes (Aïd al-Adha, Aïd el-Fitr, Mawlid, etc.) sont considérées comme des jours fériés mais le vendredi n’est pas chômé comme en Algérie.

Le christianisme et le judaïsme sont très minoritaires en Tunisie mais le pays se caractérise par sa tolérance et son ouverture aux autres cultures qui ont fait son identité, notamment sur l’île de Djerba. La synagogue de la Ghriba est l’une des plus anciennes synagogues au monde et la plus ancienne utilisée sans interruption. Beaucoup de Juifs d’origine tunisienne la considèrent comme un lieu de pèlerinage. Chaque année, des célébrations sont organisées à raison de son ancienneté et la légende selon laquelle elle aurait été construite en utilisant des pierres du temple de Salomon.

La Constitution de 1959 prévoit ainsi l’exercice libre de la foi tant qu’elle ne porte pas atteinte à l’ordre public. Les gouvernements des présidents Bourguiba et Ben Ali respectent généralement ce droit mais ne permettent pas l’établissement de partis politiques basés sur la religion, interdisent le prosélytisme non-musulman, la polygamie et limitent le port du hijab, notamment dans les administrations et les écoles publiques. Les minorités religieuses connaissent néanmoins deux discriminations : le président de la République tunisienne ne peut être d’une autre confession que celle musulmane (article 40 de la Constitution) et le prosélytisme non-musulman est interdit car il est considéré comme une atteinte à l’ordre public.

La Constitution de 2014 exige de l’État qu’il garantisse la liberté de croyance et de conscience et le libre exercice du culte, protège le sacré, garantisse la neutralité des lieux de culte, diffuse les valeurs de modération et de tolérance, proscrive l’accusation d’apostasie et s’oppose à l’incitation à la haine et à la violence.

La Tunisie est l’État du Maghreb le plus homogène sur le plan linguistique car la quasi-totalité de la population parle l’arabe tunisien, ou darija, et maîtrise l’arabe littéral, qui est la langue officielle du pays, ainsi que le français. La darija tunisienne est considérée comme un dialecte dérivé de l’arabe classique — ou plus exactement un ensemble de dialectes — pour lesquels il n’existe aucun organisme officiel de normalisation et qui est surtout parlé dans le cadre d’un dialogue quotidien au sein de la famille. Selon des études linguistiques, il serait proche du maltais, qui n’est toutefois pas considéré comme un dialecte arabe pour des raisons sociolinguistiques. Le berbère est parlé par une minorité berbérophone, surtout dans le Sud du pays.

Durant le protectorat français de Tunisie, le français s’impose à travers les institutions nationales, particulièrement l’éducation, qui deviennent un fort vecteur de diffusion. À partir de l’indépendance, le pays s’arabise peu à peu, même si l’administration, la justice et l’enseignement restent longtemps bilingues, alors que la connaissance des langues européennes est renforcée par l’exposition de la Tunisie à ce continent par l’intermédiaire de la télévision et du tourisme. Le pays est membre de l’Organisation internationale de la francophonie depuis 1970. De plus, les gouvernorats de Béja, Gafsa, Médenine, Monastir, Sfax, Sousse et Tunis sont membres de l’Association internationale des régions francophones.

Les années 1990 marquent un tournant avec l’arabisation des cours de sciences jusqu’à la fin du collège, avec toutes les difficultés occasionnées par ce type de processus, afin de faciliter l’accès aux études supérieures et ce dans un contexte de réhabilitation du référent arabo-islamique dans l’espace public. En octobre 1999, les établissements commerciaux se voient contraints d’accorder deux fois plus de place aux caractères arabes qu’aux caractères latins. Dans le même temps, l’administration se voit contrainte de communiquer exclusivement en arabe mais seuls les ministères de la Défense et de la Justice et le Parlement sont totalement arabisés. Dans ce contexte, l’usage du français semble régresser malgré le nombre accru de diplômés du système d’enseignement, ce qui conduit au fait qu’une bonne pratique du français demeure un marqueur social important. Puisqu’elle reste largement pratiquée dans les milieux d’affaires, l’univers médical et le monde culturel, on peut même considérer qu’elle s’est embourgeoisée.

D’après les dernières estimations fournies par le gouvernement tunisien à l’Organisation internationale de la francophonie, le nombre de personnes ayant une certaine maîtrise du français est chiffré à de personnes, soit 63,6 % de la population.

Le paysage audiovisuel tunisien se compose de deux chaînes de télévision publiques (Télévision tunisienne 1 et Télévision tunisienne 2) ainsi que de chaînes de télévision privées nées du processus d’ouverture au secteur privé initié en 2003 et dont le nombre est renforcé après la révolution de 2011 : Zitouna TV, Al Mutawasit TV, Al Janoubiya TV, Al Qalam TV, Hannibal TV, El Hiwar El Tounsi, Nessma, Tunisna TV, Attessia TV ou encore TWT.

Il existe également de nombreuses stations de radio publiques, qu’elles soient nationales (Radio Tunis, Radio Tunisie Culture, Radio Jeunes et RTCI) ou régionales (Radio Gafsa, Radio Le Kef, Radio Monastir, Radio Sfax et Radio Tataouine), de même que privées (Radio 6, Cap FM, Chaambi FM, Express FM, IFM, Jawhara FM, Radio Kalima, Radio Al Karama, Kif FM, Mosaïque FM, Oasis FM, Oxygène FM, Sabra FM, Shems FM, Sawt Al Manajem, Ulysse FM).

La presse écrite connaît, sous les régimes autoritaires des présidents Habib Bourguiba puis Zine el-Abidine Ben Ali, des périodes de libéralisation puis de censure. La révolution constitue un tournant, avec l’autorisation donnée à près de 200 nouveaux journaux et revues de paraître. Les partis politiques tunisiens ont le droit de publier leurs propres journaux mais ceux des partis d’opposition n’ont longtemps eu qu’un tirage très limité.

L’éducation préscolaire non obligatoire, qui s’adresse aux enfants de trois à six ans, est dispensée dans les jardins d’enfants.

L’enseignement de base est obligatoire et gratuit, de six à seize ans, et se répartit sur deux cycles : le premier cycle, d’une durée de six ans, est dispensé à l’école primaire alors que le deuxième cycle, d’une durée de trois ans, se déroule au collège.

Ce parcours est sanctionné par le diplôme de fin d’études de l’enseignement de base permettant aux diplômés d’accéder à l’enseignement secondaire (toujours gratuit) dispensé au lycée durant quatre ans à partir de la réforme de 1995.

Il comprend un tronc commun d’une année (trois jusqu’en 1991) au terme duquel les élèves sont orientés vers un deuxième cycle de trois ans comprenant sept filières (lettres, mathématiques, sciences expérimentales, sciences techniques, sciences de l’informatique, économie-gestion et sport) et sanctionné par le baccalauréat permettant l’accès à l’enseignement supérieur. Celui-ci compte notamment 179 établissements rattachés aux treize universités — dont cinq à Tunis, une à Sousse, une à Sfax, une à Kairouan, une à Gabès, une à Gafsa, une à Monastir et une à Jendouba — mais aussi 24 instituts supérieurs des études technologiques (ISET).

La formation professionnelle est assurée par un ensemble d’opérateurs publics parmi lesquels figure l’Agence tunisienne de la formation professionnelle qui assure une tutelle pédagogique de l’ensemble des opérateurs publics et privés. Les diplômes délivrés après une formation initiale sont de trois niveaux : le certificat d’aptitude professionnelle (CAP) qui sanctionne un cycle de formation d’une durée minimale d’une année après l’enseignement de base, le brevet de technicien professionnel (BTP) qui sanctionne un cycle de formation d’une durée minimale d’une année après la fin du premier cycle de l’enseignement secondaire ou après l’obtention du CAP et le brevet de technicien supérieur qui sanctionne un cycle de formation d’une durée minimale de deux années après le baccalauréat ou après l’obtention du BTP.

Alors que 21 % du budget national est consacré en 2008 à l’éducation nationale, le nombre d’élèves inscrits dans les niveaux primaire et secondaire se monte à 2,1 millions en 2008 contre 2,4 millions en 2000 et 1,7 million en 1987 ; sont inscrits dans le même temps dans l’enseignement supérieur, soit 27 % de la classe d’âge concernée. En 2005, le taux d’alphabétisation est de 76,2 % et le taux de scolarisation des enfants de 12 à 17 ans, égal pour les garçons et les filles, est de 66 %.

En 2013, les dépenses de santé représentent 7,1 % du PIB du pays. En 2010, on compte 1,22 médecins pour habitants.

L’espérance de vie à la naissance est de 75,89 ans en 2015, soit respectivement 73,79 ans pour les hommes et 78,14 ans pour les femmes. La mortalité infantile est de 22,35 morts pour naissances en 2015.

Les productions du cinéma tunisien restent rares et confidentielles même si certaines rencontrent un certain succès hors de Tunisie. Parmi les plus connues, on peut citer "Un été à La Goulette" (1996) et "Halfaouine, l’enfant des terrasses" (1990) de Férid Boughedir. Ce dernier, sans doute le plus grand succès du cinéma tunisien, met en scène un enfant dans le Tunis des années 1960. Nouri Bouzid porte quant à lui sur la réalité tunisienne un regard sans complaisance. Dans "L’Homme de cendres" (1986), il traite de la pédophilie, de la prostitution et des relations entre les communautés musulmane et juive. Dans "Bezness" (1991), c’est le tourisme sexuel qui se trouve dans sa ligne de mire.

Dans "Les Ambassadeurs" (1975), Naceur Ktari met en scène des émigrés maghrébins en France qui y sont confrontés au racisme. "Les Silences du palais" (1994) de Moufida Tlatli a quant à lui été primé par plusieurs jurys internationaux. Premier film arabe réalisé par une femme, on y découvre la vie dans une maison aristocratique de Tunis à travers les yeux d’une jeune fille. En 2007, le paysage cinématographique tunisien voit la sortie de plusieurs films recevant un certain succès auprès du public tels que "Making of" de Bouzid ou "VHS Kahloucha" de Nejib Belkadhi.

La musique tunisienne est, quant à elle, relativement diversifiée. Principalement influencée par les cultures arabo-andalouse, arabe et occidentale, elle est le résultat d’un métissage culturel. Son courant musical classique et le plus réputé est le malouf. Toutefois, les chants traditionnels continuent de rencontrer un certain succès. Côté instruments, les régions urbaines et rurales se différencient quelque peu.

En milieu urbain, ce sont les instruments à cordes (rebec, oud et qanûn) et les percussions (darbouka) qui dominent alors que, en milieu rural, le chant bédouin, en plus des percussions, est accompagné d’instruments à vent comme le mezoued et la gasba. Parmi les grands chanteurs et chanteuses tunisiens, on peut citer Saliha, Khemaïs Tarnane, Ali Riahi, Hédi Jouini, Latifa Arfaoui, Mohamed Jamoussi, Cheikh El Afrit, Lotfi Bouchnak ou encore Dhikra Mohamed. Chez les musiciens, on peut également citer Salah El Mahdi, Ridha Kalaï, Ali Sriti, Anouar Brahem, Jasser Haj Youssef ou encore Youssef Slama.

Dans le même temps, une majorité de la population est attirée par des musiques d’origine arabe (égyptienne, libanaise ou encore syrienne). La musique occidentale actuelle remporte également un succès important avec l’émergence de nombreux groupes et de festivals de rock, de hip-hop, de reggae et de jazz.

Le théâtre tunisien s’est surtout développé entre la fin du et le début du durant le protectorat français. Fondé à cette époque, le Théâtre municipal de Tunis a accueilli en plus d’un siècle d’existence de grands noms de la scène tunisienne et internationale. Le , Habib Bourguiba consacre son discours au théâtre qu’il considère comme . Toutefois, le théâtre tunisien n’a jamais connu un réel développement. En 1970, sous l’impulsion de l’acteur Aly Ben Ayed, "Caligula" d’Albert Camus est traduit en arabe et les œuvres "Mourad III" ou "Le Temps du Bouraq" de Habib Boularès maintiennent le ton de la violence sanglante. Même si, de plus en plus, les spectacles dits de boulevard sont restreints au profit d’un genre de spectacle plus sophistiqué, Moncef Souissi et Ezzedine Madani ont créé un théâtre d’expression populaire et moqueur en tunisien. Le courant dit du Nouveau Théâtre de Tunis a également repris le fil de la dérision. Nommé en 1988 à la tête du Théâtre national tunisien (TNT), Mohamed Driss lui offre une nouvelle salle, "Quatrième art", en 1996 et l’ouvre aux spectacles de ballet, de cirque et de chant. Quant à El Teatro, le premier théâtre privé de Tunisie, il offre des représentations théâtrales, des spectacles de danse, des concerts de jazz, des galas de musique arabe, des expositions d’art et des récitals de poésie.

La naissance d’une peinture tunisienne contemporaine est fortement liée à l’École de Tunis mise en place par un groupe d’artistes de Tunisie unis par la volonté d’incorporer des thèmes proprement tunisiens et rejetant l’influence orientaliste de la peinture coloniale. Après la peinture expressionniste d’Amara Debbache, Jellal Ben Abdallah et Aly Ben Salem se font reconnaître, l’un pour ses miniatures de style byzantin, l’autre pour son rattachement à l’impressionnisme. La vie quotidienne devient par ailleurs l’inspiration de Zoubeir Turki et d’Abdelaziz Gorgi. L’abstraction saisit également l’imagination des peintres comme Edgard Naccache, Nello Lévy et Hédi Turki. Après l’indépendance en 1956, le mouvement pictural tunisien entre dans une dynamique d’édification nationale, des artistes se mettant au service de l’État. Des artistes ont ainsi pu accéder à une reconnaissance internationale tels que Hatem El Mekki, peintre abstrait, dont la facture rappelle celle d’Alberto Giacometti. La jeune peinture emboîte davantage le pas à ce qui se passe ailleurs dans le monde : Sadok Gmech puise son inspiration dans le patrimoine national alors que Moncef Ben Amor se tourne vers le fantastique. Dans un autre registre, Youssef Rekik réutilise la technique de la peinture sur verre et Nja Mahdaoui retrouve la calligraphie dans sa dimension mystique.

Enfin, la littérature tunisienne existe sous deux formes : en langue arabe et en langue française. La littérature arabophone remonte au avec l’arrivée de la civilisation arabe dans la région. Elle est plus importante en volume comme en valeur que la littérature en langue française qui suit l’implantation du protectorat en 1881. Malgré la longue histoire de la littérature tunisienne, la production nationale reste pourtant maigre : la bibliographie nationale a recensé non scolaires publiés en 2002 dont en arabe. Parmi les grands auteurs tunisiens, on peut citer Abou el Kacem Chebbi, Moncef Ghachem et Mahmoud Messadi.

La Tunisie est réputée pour ses nombreux produits artisanaux dont les diverses régions du pays font leur spécificité. La poterie tunisienne est principalement issue de Guellala, ville à l’origine de la création d’autres centres potiers sur le littoral tunisien, notamment à Tunis, Nabeul, Moknine, etc. Mais si la poterie poreuse s’identifie à Guellala, celle émaillée (jaune, verte ou brune) est la marque de fabrique de Nabeul.

La ferronnerie remonte pour sa part à l’époque andalouse lorsque l’on décorait les portes cloutées, ornement devenu caractéristique du fer forgé tunisien. Bleues par tradition, destinées à embellir les maisons et à préserver l’intimité des habitants, ces grilles rappellent les moucharabiehs de la tradition arabo-andalouse, panneaux de bois sculpté qui permettaient aux femmes de regarder dans la rue sans être vues.

La ville de Kairouan constitue quant à elle le centre national de production de tapis. La Tunisie possède enfin une riche tradition de mosaïques remontant à la période antique.

Le costume traditionnel est la tenue par excellence des mariages et autres cérémonies. Au niveau national, c’est la jebba qui s’est imposée comme habit traditionnel. Les babouches masculines sont en général de la couleur naturelle du cuir, celles des femmes étant dans leur majorité brodées de fils de soie, de coton, d’or et d’argent avec des motifs floraux ou des croissants. Dans des régions du Nord et du Sud, les femmes portent traditionnellement la melia ou le houli.

Importé par les Andalous au , le jasmin ("Jasminum grandiflorum") est devenu la fleur emblématique de la Tunisie. Dès la tombée de la nuit, les vendeurs confectionnent de petits bouquets et les vendent aux passants dans la rue ou aux automobilistes arrêtés aux carrefours. Par ailleurs, le jasmin fait l’objet d’un langage spécifique. Ainsi, un homme qui en porte à l’oreille gauche indique qu’il est célibataire. De plus, offrir du jasmin blanc est une preuve d’amour alors qu’offrir du jasmin d'hiver, sans odeur, est signe d’insolence.

La cuisine tunisienne est essentiellement basée sur les légumes, le poisson et les fruits de mer (surtout le long des côtes), la viande de mouton, de bœuf et dans certaines régions de dromadaire, le couscous et les pâtes.

Ces dernières sont sans doute le plat le plus consommé, la Tunisie se plaçant au troisième rang mondial après l’Italie et le Venezuela avec par habitant et par an, en particulier les spaghettis et macaronis servis généralement avec de la sauce tomate et de l’harissa, même si le plat traditionnel reste le couscous. Le pain, notamment le traditionnel pain tabouna, est également un aliment apprécié de beaucoup de Tunisiens. Le fricassé est une sorte de sandwich au thon constitué d’une demi-baguette remplie de miettes de thon, d’harissa avec parfois des olives vertes, des câpres et des rondelles d’œufs durs, est vendu dans toutes les échoppes d’alimentation. Un autre plat fort apprécié est la brik à l’œuf, traditionnellement avec des garnitures à base de viande et de fromage.

La cuisine tunisienne se différencie quelque peu de ses voisines nord-africaines. Le tajine tunisien, contrairement à la version marocaine, consiste en une sorte de quiche à base d’œuf, de viande, de pommes de terre et de persil. Le couscous, lui, se caractérise par une combinaison entre les légumes (pommes de terre, tomate, carottes, courge, etc.), la viande (surtout celle de l’agneau) ou le poisson et la semoule. La mloukhiya, contrairement à la version égyptienne, préfère les viandes rouges (comme l’agneau) aux viandes blanches (comme le lapin). Elle est préparée à l’occasion du nouvel an musulman. Le Mouled est, quant à lui, l’occasion de préparer une crème pâtissière à base de pignons de pin, l’assidat zgougou.

La pâtisserie tunisienne est diversifiée : parmi les pâtisseries traditionnelles, qualifiées de dans les pays occidentaux, les plus connues sont le makroud de Kairouan, la zlabia surtout préparée lors du ramadan et les gâteaux à base d’amandes et de fruits secs, notamment le baklawa servi lors des fêtes et des mariages.

Le sport en Tunisie est marqué par la domination du football, tant sur le plan de la couverture médiatique que du succès populaire avec contre pour le taekwondo, second sport le plus pratiqué dans le pays. Toutefois, des sports comme le volley-ball ou le handball figurent également parmi les sports les plus représentés même si des sports moins connus sont plus pratiqués par les Tunisiens, notamment les arts martiaux (taekwondo, judo et karaté), l’athlétisme voire le tennis. D’autres grands sports comme le cyclisme sont en revanche moins représentés, faute d’infrastructures, d’équipements et d’intérêt médiatique suffisants.

L’Espérance sportive de Tunis est le club de football le plus titré du championnat national, avec 27 titres à son actif, et le plus titré de la coupe de Tunisie avec quinze titres à son actif ; c’est le premier club à participer à une compétition continentale en 1971 : la coupe des clubs champions africains. Le Club athlétique bizertin devient en 1988 le premier club tunisien à avoir remporter un trophée continental : la coupe d’Afrique des vainqueurs de coupe. Le Club africain est le premier club tunisien à avoir remporté la coupe d’Afrique des clubs champions en 1992. L’Étoile sportive du Sahel est le premier club tunisien à remporter la Ligue des champions de la CAF dans sa nouvelle édition le . Le Club sportif sfaxien a aussi remporté des manifestations continentales et régionales ; il est le premier club à remporter deux fois de suite la coupe de la CAF en 2007 et 2008. Le derby de la capitale entre le Club africain et l’Espérance sportive de Tunis reste l’événement footballistique phare de l’année en rassemblant à deux reprises par saison plus de et donnant lieu à un show ("dakhla" en tunisien) de la part des supporters des deux équipes. Il existe d’autres classiques entre les quatre grandes équipes, l’Espérance sportive de Tunis, l’Étoile sportive du Sahel, le Club sportif sfaxien et le Club africain.

L’année sportive tunisienne est rythmée par les grandes compétitions que sont les championnats (football, handball, volley-ball et basket-ball) et les coupes (football, handball, volley-ball et basket-ball) des sports les plus populaires. En cyclisme, discipline moins suivie, sont organisés les championnats de Tunisie de cyclisme et, de façon irrégulière, le Tour de Tunisie.

Mais le pays organise également des compétitions internationales. Ainsi, la première édition de la Coupe du monde de football des moins de 20 ans s’y tient en 1977 tout comme les phases finales des coupes d’Afrique des nations de football en 1965, 1994 et 2004, dernière édition remportée par la sélection nationale. Plus récemment, le championnat du monde de handball masculin 2005 s’est également tenu en Tunisie. Le championnat d'Afrique des nations de handball masculin 2020 se déroulera en Tunisie.

En mai 2007, le pays compte clubs sportifs dont les principaux sont actifs dans le football (250) et le taekwondo (206). Viennent ensuite le karaté et ses dérivés (166), le handisport (140), le handball (85), l’athlétisme (80), le judo (66), le kung fu (60), le kick boxing (59), le basket-ball (48), la pétanque (47), le tennis de table (45), le volley-ball (40), la boxe (37), la natation (31) et le tennis (30).

Parmi les sportifs les plus connus, Mohammed Gammoudi s’illustre en athlétisme, ce qui lui permet de remporter quatre médailles aux Jeux olympiques, ce qui en fait le sportif tunisien le plus médaillé de l’histoire du pays. La Tunisie a également vu émerger des champions dans des sports individuels tels que Anis Lounifi (champion du monde de judo) ou encore Oussama Mellouli (champion du monde et olympique de natation). En ce qui concerne les sports collectifs, les équipes nationales ont remporté une coupe d'Afrique des nations de football, neuf championnats d’Afrique de handball masculin, huit championnats d’Afrique de volley-ball masculin ou encore la compétition de basket-ball des Jeux africains de 1973.

La Tunisie a pour codes : 


</doc>
<doc id="3007" url="https://fr.wikipedia.org/wiki?curid=3007" title="Tricosane">
Tricosane

Le tricosane est un hydrocarbure linéaire de la famille des alcanes (paraffine) de formule brute CH . Il a une légère odeur de cire.


</doc>
<doc id="3008" url="https://fr.wikipedia.org/wiki?curid=3008" title="Théorie de la musique occidentale">
Théorie de la musique occidentale

La théorie de la musique occidentale est une branche de la musicologie qui a pour objet la description du fonctionnement de la musique occidentale, ainsi que du système de notation qui lui est associé. Elle étudie en particulier la musique tonale et le système tonal, qui correspond à une période allant de la fin de la Renaissance au début du , mais elle envisage aussi les origines au Moyen Âge, ainsi que les développements qui accompagnent la dissolution de la tonalité au et au : à ce titre, elle traite par exemple de musique sérielle, de jazz, de musiques populaires

Comparé à d'autres systèmes musicaux, le système rythmique de la musique occidentale peut paraître singulièrement pauvre : durées proportionnelles des sons et des silences, égalité des temps, des parties de temps et des mesures. Ce principe de proportionnalité rythmique a été rendu nécessaire, dès le Moyen Âge, par la polyphonie : il était indispensable que les voix puissent se coordonner rythmiquement entre elles. Dans d'autres musiques, cette nécessité est moindre : dans l'hétérophonie en particulier, où les voix peuvent être légèrement décalées, la proportion exacte des rythmes n'est pas nécessaire.

Le système mélodique n'a cessé d'aller vers une simplification toujours plus prononcée : au , l'échelle chromatique au tempérament égal, composée de 12 demi-tons égaux et de 12 degrés, a fini par se généraliser et supplanter les échelles et gammes héritées des modes médiévaux, et au-delà, du système musical de la Grèce antique. 

Le système harmonique constitue l'une des principales particularités de la musique occidentale. Le mot « harmonie » ici, doit être entendu au sens large, incluant aussi bien le contrepoint — système exclusivement employé du XII au — que l'harmonie classique — système qui lui a progressivement succédé à partir du . Ce système harmonique régit la "simultanéité délibérée" des sons, et met en œuvre les notions de consonance et de dissonance. Contrepoint aussi bien qu'harmonie tonale exigent une "planification" : s'il est toujours possible d'improviser "rythmiquement" ou "mélodiquement", en revanche, aussitôt qu'émerge une "pensée harmonique", on ne peut éviter de recourir à la partition. Par ailleurs, ce système harmonique n'est viable qu'au prix de la simplification des systèmes rythmique et mélodique. 

Le "solfège" est le système de notation né des exigences de la polyphonie médiévale aux alentours du . Il affirme la notion de « musique composée » par opposition à la « musique improvisée ». Le plus souvent, la musique occidentale met en jeu un compositeur, une partition et un interprète. Les moments d'improvisation sont rigoureusement délimités et règlementés par la tradition — cadences, préludes, récitatifs.



</doc>
<doc id="3009" url="https://fr.wikipedia.org/wiki?curid=3009" title="Tableau périodique des éléments">
Tableau périodique des éléments

Le tableau périodique des éléments, également appelé tableau ou table de Mendeleïev, classification périodique des éléments ou simplement tableau périodique, représente tous les éléments chimiques, ordonnés par numéro atomique croissant et organisés en fonction de leur configuration électronique, laquelle sous-tend leurs propriétés chimiques.

La conception de ce tableau est généralement attribuée au chimiste russe Dmitri Mendeleïev, qui, en 1869, construisit une table, différente de celle qu'on utilise aujourd'hui mais semblable dans son principe, dont le grand intérêt était de proposer une classification systématique des éléments connus à l'époque en vue de souligner la périodicité de leurs propriétés chimiques, d'identifier les éléments qui restaient à découvrir, voire de prédire certaines propriétés d'éléments chimiques alors inconnus.

Le tableau périodique a connu de nombreux réajustements depuis lors jusqu'à prendre la forme que nous lui connaissons aujourd'hui. Il est devenu un référentiel universel auquel peuvent être rapportés tous les types de comportements physique et chimique des éléments. Depuis la mise à jour de l'UICPA du , sa forme standard comporte , allant de l'hydrogène H à l'oganesson Og.

Ce tableau est la représentation la plus usuelle de la classification des éléments chimiques. Certains chimistes ont proposé d'autres façons de classer les éléments, mais celles-ci restent bornées au domaine scientifique.

Parmi les chimiques connus, 83 sont dits primordiaux parce qu'ils possèdent au moins un isotope stable ou suffisamment stable pour être plus ancien que la Terre. Parmi eux, trois sont radioactifs : l'uranium U, le thorium Th et le bismuth Bi ; la radioactivité de ce dernier est cependant si faible qu'elle n'a été mise en évidence qu'en 2003<ref name="10.1038/nature01541">
</ref>.

Les éléments sont dits synthétiques car ils n'existent pas naturellement dans l'environnement terrestre et sont produits artificiellement dans les réacteurs nucléaires ou expérimentalement en laboratoire. On peut cependant trouver certains d'entre eux dans la nature à la suite d'essais nucléaires atmosphériques ou d'accidents nucléaires, comme c'est le cas, dans certaines zones contaminées, pour l'américium Am, le curium Cm, le berkélium Bk et le californium Cf. Hors de notre planète, ces éléments, ainsi que l'einsteinium Es, sont peut-être produits naturellement par <ref name="10.1007/s11443-008-3006-1">
</ref> lors d'explosions de supernovae, comme on l'a longtemps pensé de l'isotope Cf<ref name="10.1103/PhysRev.103.1145">
</ref><ref name="10.1086/126941">
</ref><ref name="10.1007/BF00630928">
</ref>, hypothèse cependant réfutée depuis lors<ref name="10.1364/JOSA.52.000222">
</ref> ; ils auraient également été détectés dans le spectre de l'étoile de Przybylski<ref name="10.3103/S0884591308020049">
</ref>.

Parmi les dont l'état standard est connu aux conditions normales de température et de pression ( et ), 90 sont solides, 11 sont gazeux, et seulement deux sont liquides : le brome Br, fondant à , et le mercure Hg, fondant à ; plusieurs éléments solides ont cependant un point de fusion voisin de la température ambiante, par exemple le francium Fr, à , le césium Cs, à , le gallium Ga, à , le rubidium Rb, à , ou encore le phosphore blanc P, à .

Dans la mesure où les propriétés physico-chimiques des éléments reposent sur leur configuration électronique, cette dernière est sous-jacente à l'agencement du tableau périodique. Ainsi, chaque ligne du tableau (appelée période) correspond à une couche électronique, identifiée par son nombre quantique principal, noté : il existe sept couches électroniques connues à l'état fondamental, donc sept périodes dans le tableau périodique standard, numérotées de 1 à 7. Chaque période est elle-même scindée en un à quatre blocs, qui correspondent aux sous-couches électroniques, identifiées par leur nombre quantique secondaire, noté : il existe quatre types de sous-couches électroniques connues à l'état fondamental, notées "s", "p", "d" et "f" (ces lettres viennent d'abréviations utilisées initialement en spectroscopie). Chacune de ces sous-couches contient respectivement 1, 3, 5 et 7 orbitales atomiques, identifiées par leur nombre quantique magnétique, noté . Enfin, chaque orbitale est occupée par au plus deux électrons, identifiés chacun par leur nombre quantique magnétique de spin, noté .

Chaque électron d'un atome est donc décrit par quatre nombres quantiques, qui vérifient les propriétés suivantes :

En vertu du principe d'exclusion de Pauli, selon lequel deux fermions (ici, deux électrons) d'un même système (ici, un même atome) ne peuvent partager le même état quantique, les sous-couches électroniques "s", "p", "d" et "f" ne peuvent contenir chacune qu'au plus 2, 6, 10 et 14 électrons respectivement ; dans le tableau périodique, elles matérialisent ainsi le bloc s, le bloc p, le bloc d et le bloc f, contenant respectivement 2, 6, 10 et 14 éléments par période.

Si l'on respecte la construction du tableau par blocs en fonction des configurations électroniques, l'hélium devrait se trouver au-dessus du béryllium dans la , celle dont les atomes ont une sous-couche externe "n"s, et non au-dessus du néon dans la , dont les atomes ont une sous-couche externe "n"p ; l'hélium est positionné usuellement dans la car c'est celle des gaz nobles, dont il fait chimiquement partie.

Toutes les sous-couches d'une période n'appartiennent pas nécessairement à la même couche électronique : à partir de la , des sous-couches appartenant à des couches différentes se remplissent sur une même période. En effet, la distribution des électrons sur les différents niveaux d'énergie quantiques autour de l'atome obéit au principe d"'Aufbau" (« "édification" » en allemand), selon lequel l'ordre précis des sous-couches électroniques est donné par la règle de Klechkowski : les sous-couches sont remplies afin que les valeurs puis soient croissantes, avec le nombre quantique principal et le nombre quantique azimutal.

C'est la succession des sous-couches électroniques de chaque période qui détermine la structure du tableau périodique en blocs, chaque période étant définie par le retour d'une sous-couche "s" suivant une sous-couche "p" de la période précédente, avec un nombre quantique principal incrémenté d'une unité.

La règle de Klechkowski est observée pour plus de 80 % des 103 éléments dont la configuration électronique à l'état fondamental est connue avec précision, mais une vingtaine d'éléments y font exception. L'état fondamental est en effet par définition celui dont l'énergie est la plus faible, et le nombre quantique magnétique de spin des électrons entre en jeu pour déterminer cette énergie : plus le spin résultant des électrons d'une orbitale atomique est élevé, plus la configuration de ces électrons sur cette orbitale est stable (règle de Hund). Il s'ensuit que, pour les éléments du bloc "d" et du bloc "f" (métaux de transition, lanthanides et actinides), il est énergétiquement moins favorable de suivre la règle de Klechkowski que de favoriser l'occupation impaire des sous-couches les plus externes lorsque la couche "d" ou "f" est vide, à moitié remplie ou entièrement remplie, car l'écart d'énergie entre ces sous-couches est inférieur au gain d'énergie induit par la redistribution des électrons maximisant leur nombre quantique magnétique de spin résultant (dans le tableau qui suit, les distributions d'électrons irrégulières sont indiquées en gras) :

Le grand intérêt de la classification périodique est d'organiser les éléments chimiques de telle sorte que leurs propriétés physico-chimiques puissent être largement prédites par leur position dans la table. Ces propriétés évoluent différemment selon qu'on se déplace verticalement ou horizontalement dans le tableau.

Une période désigne une ligne du tableau périodique. Elle se définit par le remplissage progressif des sous-couches électroniques jusqu'à atteindre la sous-couche "s" de la couche électronique suivante. Les propriétés des éléments varient généralement beaucoup le long d'une période, mais peuvent être localement assez similaires et constituer des familles d'éléments chimiques complètes, notamment dans le bloc d (métaux dits « de transition ») et surtout dans le bloc f (lanthanides sur la et actinides sur la ).

Un groupe désigne une colonne du tableau périodique. Chacun des 18 groupes du tableau périodique standard constitue souvent un ensemble d'éléments aux propriétés distinctes des groupes voisins, notamment aux extrémités gauche et droite du tableau périodique (c'est-à-dire dans les blocs s et p), où ils se sont vu attribuer des noms d'usage au fil du temps :

Si les termes "pnictogène" et "chalcogène" sont aujourd'hui assez désuets, les quatre autres en revanche sont encore très employés car ils se confondent usuellement avec des familles de même nom :





Le groupe 3 est un cas particulier dans la mesure où sa composition ne fait pas l'objet d'un consensus parmi les chimistes : si les éléments des périodes 4 et 5 qui le constituent sont toujours le scandium et l'yttrium, ceux des périodes 6 et 7 sont en revanche ou bien le lanthane et l'actinium, ou bien le lutécium et le lawrencium. Cela signifie que la composition des blocs d et f est également variable selon les sources, car le groupe 3 fait partie du bloc d. La première option, plaçant le lanthane et l'actinium dans le groupe 3, et donc dans le bloc d, était prépondérante jusqu'au début du siècle, avec semble-t-il un renversement de tendance depuis lors ; ce choix relève essentiellement d'une convention : les propriétés chimiques du scandium, de l'yttrium et des (lanthane et lutécium compris) sont ainsi suffisamment semblables pour que ces éléments soient collectivement appelés "terres rares".

La description quantique de la configuration électronique des atomes permet d'expliquer la similitude des propriétés chimiques au sein d'un groupe par une configuration identique des électrons dans la couche de valence. Le rayon atomique augmente rapidement de haut en bas d'un groupe, car à chaque période s'ajoute une couche électronique. En corollaire, l'énergie d'ionisation et l'électronégativité diminuent car les électrons périphériques sont moins fortement liés au noyau dans le bas du tableau.

Outre les analyses par lignes et par colonnes, le tableau périodique permet également d'établir des relations diagonales entre certains éléments chimiques des deuxième et troisième périodes qui se trouvent en diagonale les uns par rapport aux autres dans le tableau. Il s'agit toujours de la direction diagonale allant du haut à gauche vers le bas à droite, car parcourir une période vers la droite et descendre le long d'une colonne se traduisent de façon opposée sur la couche de valence des atomes (respectivement, diminution et augmentation du rayon atomique, d'où augmentation et diminution de l'électronégativité). Il s'ensuit certaines similitudes entre éléments diagonaux, qui pourtant ne partagent ni la même période ni le même groupe : la distribution des métalloïdes dans le tableau illustre cet effet.

D'une manière générale, le rayon atomique tend à décroître lorsqu'on parcourt une période de gauche à droite, depuis les métaux alcalins jusqu'aux gaz nobles, et à croître lorsqu'on parcourt un groupe de haut en bas. Il croît brutalement lorsqu'on passe d'une période à la suivante, entre le gaz noble d'une période "P" et le métal alcalin de la période . Ceci s'explique très bien par les couches électroniques constituant les atomes, et ces observations fournissent des preuves importantes pour l'élaboration et la confirmation des théories de la mécanique quantique.

La décroissance du rayon atomique le long des périodes résulte notamment du fait que la charge électrique du noyau atomique augmente tout au long de chaque période, ce qui accroît l'attraction du noyau sur les électrons et réduit par conséquent le volume des orbitales atomiques. La contraction des lanthanides, observée au cours du remplissage de la sous-couche 4f, illustre très bien ce phénomène : le rayon atomique de l'osmium () est quasiment identique à celui du ruthénium (), qui lui est juste au-dessus dans le tableau. Cette particularité s'observe le long de la à partir du hafnium () jusqu'au platine (), après lequel elle est masquée par un effet relativiste appelé effet de paire inerte. Un phénomène semblable s'observe également avec le remplissage des sous-couches "n"d du bloc d, mais est moins marqué que celui observé avec les lanthanides, bien qu'il ait la même origine. 

L'énergie d'ionisation, qui correspond implicitement à l'énergie de première ionisation, est l'énergie minimum nécessaire pour retirer un électron à un atome et former un cation. L'électron retiré est le moins lié au noyau atomique et se trouve dans la couche de valence. L'énergie de deuxième ionisation est par conséquent l'énergie nécessaire pour retirer un deuxième électron à l'ion précédemment formé, etc. Pour un atome donné, les énergies d'ionisation successives augmentent avec le degré d'ionisation. Pour le magnésium, par exemple, l'énergie de première ionisation est de pour former le cation Mg, tandis que la deuxième est de pour former le cation Mg. Cela s'explique par le fait que les électrons sont d'autant plus liés au noyau qu'ils sont dans des sous-couches intérieures, ce qui explique également que l'énergie de première ionisation croisse quand on se rapproche du haut et de la droite du tableau.

L'énergie d'ionisation fait un bond lorsqu'on tente d'arracher un électron à une configuration électronique de gaz noble, ce qui est par exemple le cas du magnésium ionisé deux fois, dont la configuration électronique est très semblable à celle du néon : l'énergie de troisième ionisation passe à pour former le cation Mg et correspond à l'arrachement d'un électron de la sous-couche 2p après que les deux électrons de la sous-couche 3s ont été retirés lors des première et deuxième ionisations.

L'électronégativité est une indication de la tendance d'un atome à attirer les électrons. Elle dépend à la fois du numéro atomique et de l'éloignement des électrons de valence par rapport au noyau atomique. Plus l'électronégativité est élevée, plus l'élément attire les électrons. Cette grandeur, déterminée par exemple par l'échelle de Pauling, suit globalement la même tendance que l'énergie d'ionisation : elle croît quand on va vers le haut et vers la droite du tableau, avec un maximum pour le fluor et un minimum pour le francium. Il existe cependant des exception à cette règle générale, qui suivent les exceptions à l'évolution du rayon atomique : le gallium et le germanium ont une électronégativité supérieure à celle de l'aluminium et du silicium respectivement en raison de la contraction du bloc d. Les éléments de la qui viennent immédiatement après les métaux de transition ont des rayons atomiques particulièrement petits, d'où une électronégativité plus élevée. On observe également que les métaux du groupe du platine et les métaux nobles ont une électronégativité particulièrement élevée et croissante vers le bas du tableau, phénomène qu'on observe également le long du groupe. 

L'affinité électronique d'un atome est la quantité d'énergie libérée lorsqu'un électron est ajouté à un atome neutre pour former un anion. Cette grandeur varie beaucoup d'un élément à un autre, mais des tendances sont perceptibles à travers le tableau périodique, présentant certaines similitudes avec l'électronégativité. Les halogènes présentent la plus forte affinité électronique, très supérieure à celle des tous les autres éléments ; elle est maximum pour le chlore, et non le fluor, à la différence de l'électronégativité.

D'une manière générale, les non métaux ont une affinité électronique plus positive que celle des métaux, tandis que celle des gaz nobles n'a pas été mesurée. L'affinité électronique croît généralement le long d'une période, mais il est plus difficile de dégager une tendance le long des groupes : elle devrait décroître en descendant le long d'un groupe puisque les couches de valence sont de moins en moins liées au noyau, mais on observe expérimentalement qu'environ un tiers des éléments échappent à cette tendance, et présentent une affinité électronique supérieure à celle de l'élément situé au-dessus d'eux dans le tableau périodique ; seul le , celui des métaux alcalins, est caractérisé par une décroissance régulière de l'affinité électronique.

En fonction de leurs propriétés physiques et chimiques générales, les éléments peuvent être classés en métaux, métalloïdes et non-métaux :

Plus l'énergie d'ionisation, l'électronégativité et l'affinité électronique sont faibles, plus l'élément a un caractère métallique prononcé. Réciproquement, les éléments pour lesquels ces grandeurs sont élevées sont non métalliques. Les non-métaux se regroupent par conséquent autour de l'angle supérieur droit du tableau (typiquement le fluor et le chlore), tandis que la grande majorité des éléments ont un caractère métallique plus ou moins prononcé, les plus métalliques se regroupant autour de l'angle inférieur gauche (typiquement le francium et le césium). Entre ces deux extrêmes, on a coutume de distinguer parmi les métaux :
Parmi les non-métaux, on peut distinguer, outre les familles conventionnelles :

Au-delà des lignes, des colonnes et des diagonales, les éléments sont traditionnellement regroupés en familles aux propriétés physico-chimiques homogènes :


Aux extrémités gauche et droite du tableau, ces familles se confondent à peu près avec les groupes, tandis qu'au centre du tableau elles ont plutôt tendance à se confondre avec les blocs, voire avec les périodes. Ces regroupements d'éléments fondés sur leurs propriétés physiques et chimiques sont par essence imparfaits, car ces propriétés varient souvent de manière assez continue à travers le tableau périodique, de sorte qu'il est fréquent d'observer des recouvrements aux limites entre ces regroupements. Ainsi, le béryllium est toujours classé parmi les métaux alcalins bien qu'il soit amphotère et présente une tendance marquée à former des composés covalents, deux caractéristiques des métaux pauvres comme l'aluminium. De même, le radon est toujours classé comme gaz noble bien qu'il ne soit pas chimiquement inerte et tende à former des composés ioniques, ce qui le rapproche des métaux.

D'autres regroupements sont également en usage, par exemple :


La configuration électronique des éléments est décrite de façon satisfaisante par le modèle des orbitales atomiques jusqu'au milieu de la . Pour , des effets relativistes deviennent significatifs sur des électrons en interaction avec un noyau très fortement chargé, certaines corrections induites par l'électrodynamique quantique ne peuvent plus être négligées, les approximations considérant les électrons de façon individuelle pour déterminer les orbitales — approximation du champ central — ne sont plus valides, et des effets de couplage spin-orbite redistribuent les niveaux d'énergie, et donc les sous-couches électroniques. Il s'ensuit que la distribution des électrons autour du noyau devient délicate à modéliser pour ces éléments, et qu'on peut s'attendre à ce que leurs propriétés chimiques soient plus difficiles à prévoir.

Si tous les éléments jusqu'au hassium Hs sont bien connus du point de vue de leurs propriétés physiques et chimiques, seuls deux éléments de numéro atomique supérieur à 108 ont fait l'objet d'études expérimentales : le copernicium Cn et le flérovium Fl ; on n'a par conséquent que très peu d'informations sur les propriétés physiques et chimiques des autres éléments de numéro atomique supérieur à 108.

Le copernicium Cn, dont les propriétés chimiques ont été particulièrement étudiées, s'est révélé être un homologue plus volatil du mercure et a donc été caractérisé comme un métal de transition ; il présente cependant certaines propriétés le rapprochant des gaz nobles et pourrait d'ailleurs être gazeux. Le flérovium, quant à lui, présente des propriétés ambiguës : davantage métal que gaz noble, contrairement à ce que laissaient penser les premiers résultats obtenus en 2008, il serait lui aussi volatil mais plus réactif que le copernicium, et pourrait appartenir, tout comme lui, à une nouvelle famille correspondant à des « métaux volatils », intermédiaires entre métaux et gaz nobles du point de vue de leurs propriétés d'adsorption sur l'or ; dans la mesure où il s'avère chimiquement semblable au plomb, il peut être vu comme un métal pauvre, mais ne peut en toute rigueur être rangé dans une famille d'éléments en l'état actuel de nos connaissances.

Les propriétés de l'oganesson Og, qui devrait être un gaz noble en vertu de son positionnement en bas de la du tableau, n'ont pas été étudiées expérimentalement ; des modélisations suggèrent qu'il pourrait peut-être s'agir d'un solide semiconducteur avec des propriétés évoquant les métalloïdes<ref name="10.1007/978-3-642-37466-1_8">
</ref>.

Les éléments chimiques sont identifiés dans le tableau périodique par leur numéro atomique, qui représente le nombre de protons que contient leur noyau, mais il peut exister plusieurs atomes différents pour un même élément chimique, différant les uns des autres par le nombre de neutrons dans leur noyau. Dans la mesure où ces atomes occupent la même case dans le tableau périodique, ils sont dits "isotopes" — avec une étymologie issue du grec ancien ἴσος τόπος signifiant « "au même endroit" ».

Les isotopes d'un élément ont généralement exactement les mêmes propriétés chimiques, car leur configuration électronique est identique. Mais la masse du noyau étant différente, on observe un effet isotopique d'autant plus prononcé que l'atome est léger. C'est notamment le cas pour le lithium Li, l'hélium He (du point de vue de ses propriétés physiques) et surtout l'hydrogène H.

L'isotope H (deutérium) est suffisamment différent de l'isotope H (protium) pour que l'UICPA admette — mais sans le recommander — l'usage d'un symbole chimique spécifique au deutérium (D) distinct de celui de l'hydrogène (H).

80 des 118 éléments du tableau périodique standard possèdent au moins un isotope stable : ce sont tous les éléments de numéro atomique compris entre 1 (hydrogène) et 82 (plomb) hormis le technétium Tc et le prométhium Pm, qui sont radioactifs.

Dès le bismuth Bi, tous les isotopes des éléments connus sont radioactifs. L'isotope Bi a ainsi une période radioactive valant un milliard de fois l'âge de l'univers. Lorsque la période dépasse quatre millions d'années, la radioactivité produite par ces isotopes devient négligeable et présente à court terme un risque sanitaire très faible : c'est par exemple le cas de l'uranium 238, dont la période est de près de 4,5 milliards d'années et dont la toxicité est avant tout chimique<ref name="10.1016/S0162-0134(02)00391-4">
</ref>, à travers notamment des composés solubles tels que , , , , , , , certains composés peu solubles tels que et étant quant à eux radiotoxiques.

Au-delà de Z = 110 (darmstadtium Ds), tous les isotopes des éléments ont une période radioactive de moins de 30 secondes, et de moins d'un dixième de seconde à partir de l'élément 115 (moscovium Mc).

Le modèle en couches de la structure nucléaire permet de rendre compte de la plus ou moins grande stabilité des noyaux atomiques en fonction de leur composition en nucléons (protons et neutrons). En particulier, des « nombres magiques » de nucléons, conférant une stabilité particulière aux atomes qui en sont composés, ont été observés expérimentalement, et expliqués par ce modèle. Le plomb 208, qui est le plus lourd des noyaux stables existants, est ainsi composé du nombre magique de et du nombre magique de .

Certaines théories extrapolent ces résultats en prédisant l'existence d'un îlot de stabilité parmi les nucléides superlourds, pour un « nombre magique » de et — selon les théories et les modèles — 114, 120, 122 ou 126 protons ; une approche plus moderne montre toutefois, par des calculs fondés sur l'effet tunnel, que, si de tels noyaux doublement magiques sont probablement stables du point de vue de la fission spontanée, ils devraient cependant subir des désintégrations α avec une période radioactive de quelques microsecondes, tandis qu'un îlot de relative stabilité pourrait exister autour du darmstadtium 293, correspondant aux nucléides définis par Z compris entre 104 et 116 et N compris entre 176 et 186 : ces éléments pourraient avoir des isotopes présentant des périodes radioactives de l'ordre de la minute.

On ignore jusqu'à combien de protons et d'électrons un même atome peut contenir. La limite d'observabilité pratique est généralement estimée à au plus , dans la mesure où l'existence des atomes superlourds se heurte à la limite de stabilité des noyaux<ref name="10.1038/nature03336">
</ref>. Cela place la fin du tableau périodique peu après l'une des valeurs proposées pour le dernier îlot de stabilité, centré dans ce cas autour de .

Richard Feynman releva en 1948 qu'une interprétation simple de l'équation de Dirac semi-relativiste aboutit à une impossibilité pour représenter les orbitales atomiques lorsque le numéro atomique vaut , où α est la constante de structure fine : de tels atomes ne pourraient avoir d'orbitale électronique stable pour plus de , ce qui rendrait impossible l'existence d'atomes électriquement neutres au-delà de ; l' est depuis lors parfois surnommé « feynmanium »<ref name="10.1007/978-3-642-28506-6_1">
</ref>. Le modèle de Bohr donne par ailleurs une vitesse supérieure à celle de la lumière pour les électrons de la sous-couche 1s dans le cas où . Une étude plus poussée, prenant notamment en compte la taille non nulle du noyau, montre cependant que le nombre critique de protons pour lequel l'énergie de liaison électron-noyau devient supérieure à 2"m""c", où "m" représente la masse au repos d'un électron ou d'un positron, vaut : dans ce cas, si la sous-couche 1s n'est pas pleine, le champ électrostatique du noyau y crée une paire électron-positron<ref name="10.1119/1.2820395">
</ref>, d'où l'émission d'un positron<ref name="10.1126/science.1234320">
</ref> ; si ce résultat n'écarte pas complètement la possibilité d'observer un jour des atomes comprenant plus de , il met en lumière un facteur supplémentaire d'instabilité les concernant.

Au-delà des sept périodes standard, une huitième période est envisagée pour classer les atomes — à ce jour inobservés — ayant plus de . Cette huitième période serait la première à posséder des éléments du bloc g, caractérisés à l'état fondamental par des électrons sur une orbitale g. Néanmoins, compte tenu des limites à la périodicité aux confins du tableau — effets relativistes sur les électrons des très gros atomes — qui deviennent significatifs dès le dernier tiers de la , il est peu probable que la configuration électronique de tels atomes obéisse aux règles observées tout au long des six premières périodes. Il est en particulier délicat d'établir le nombre d'éléments contenus dans ce : la règle de Klechkowski en prédit 18, mais la méthode de Hartree-Fock en prédit 22.

Le tableau périodique étendu à la huitième période avec dans le pourrait ainsi présenter l'aspect suivant :

Une neuvième période est parfois évoquée, mais, compte tenu de l'incertitude réelle quant à la possibilité d'observer à terme plus d'une dizaine d'éléments nouveaux sur la huitième période, tous les éléments de numéro atomique supérieur à 130 relèvent "a priori" de la pure extrapolation mathématique. À noter qu'une variante de la table ci-dessus, proposée par Fricke en 1971<ref name="10.1007/BF01172015">
</ref> et revue par Pekka Pyykkö en 2011<ref name="10.1039/C0CP01575J">
</ref>, répartit les éléments sur , et non 8, en les distribuant de manière non périodique : les éléments 139 et 140 sont ainsi placés entre les éléments 164 et 169, dans le et non plus dans le , tandis que les éléments 165 à 168 sont placés sur une dans les et p.

De la toute première tentative de classification des éléments chimiques par Antoine Lavoisier en 1789 au tableau périodique de Glenn Seaborg que nous utilisons aujourd'hui, de nombreux hommes de sciences, issus d'horizons — et parfois de disciplines — différents, ont apporté chacun leur contribution, sur une période de près de deux siècles.

C'est en 1789 que le chimiste français Antoine Lavoisier a publié à Paris son "Traité élémentaire de chimie, présenté dans un ordre nouveau et d'après les découvertes modernes". Cet ouvrage en deux volumes a jeté les bases de la chimie moderne, en faisant le point sur les connaissances de la fin du dans cette discipline. Il y précise notamment le concept d'élément chimique comme une "substance simple" qui ne peut être décomposée en d'autres substances, avec en corollaire la loi fondamentale de conservation de la masse de chacune de ces "substances simples" au cours des réactions chimiques. Il mentionna également le fait que de nombreuses substances considérées comme simples par le passé se sont révélées être en réalité des composés chimiques (par exemple l'huile et le sel marin), et il précisa s'attendre à ce qu'on considère sous peu les "terres" (c'est-à-dire certains minerais) comme des substances composées de nouveaux éléments.

Il publia dans cet ouvrage un tableau récapitulatif des « substances » considérées à son époque comme des éléments chimiques, en prenant soin d'établir une équivalence avec le vocabulaire hérité des alchimistes afin d'éliminer toute ambiguïté. Ce tableau, qui se voulait exhaustif et outil de référence, mentionnait ainsi, parmi les éléments chimiques, la lumière et le feu, encore considérés à cette époque comme des principes « chimiques » bien que Lavoisier lui-même ait invalidé la théorie du phlogistique :

Les éléments chimiques y sont classés en quatre familles :


Le chlore est désigné comme « "radical muriatique" », car Lavoisier considérait que tous les acides étaient des oxoacides — le nom "oxygène" signifie étymologiquement « formant des acides » — et cherchait donc le « radical » que l'oxygène aurait rendu acide — l"'acide muriatique" désignait l'acide chlorhydrique, qui ne contient cependant pas d'oxygène.

Cette classification a surtout le mérite de clarifier certaines notions fondamentales, mais ne révèle encore aucune périodicité des propriétés des éléments classés : les métaux sont ainsi recensés tout simplement par ordre alphabétique en français.

La première tentative de classification moderne des éléments chimiques revient au chimiste allemand Johann Wolfgang Döbereiner qui, en 1817, nota que la masse atomique du strontium (88) était égale à la moyenne arithmétique des masses atomiques du calcium (40) et du baryum (137), qui ont des propriétés chimiques semblables (aujourd'hui, ils sont classés parmi les métaux alcalino-terreux). En 1829, il avait découvert deux autres « triades » de ce type : celle des halogènes (la masse atomique du brome (80) étant égale à la moyenne arithmétique (81) de celles du chlore (35,5) et de l'iode (127)) et celle des métaux alcalins (la masse atomique du sodium (23) étant égale à la moyenne arithmétique de celles du lithium (7) et du potassium (39)).

D'autres chimistes identifièrent d'autres séries d'éléments, et Leopold Gmelin publia en 1843 la première édition de son "Handbuch der Chemie", qui mentionnait des triades, ainsi que trois « tétrades » et une « pentade » — azote, phosphore, arsenic, antimoine et bismuth, que nous connaissons aujourd'hui comme les éléments du groupe 15 du tableau périodique.

En 1859, le chimiste français Jean-Baptiste Dumas généralisa les triades de Döbereiner en les étendant en tétrades incluant les éléments les plus légers, définies non plus par les moyennes arithmétiques, mais par une progression similaire d'une tétrade à l'autre, par exemple :


Bien qu'en apparence similaire à celle de Döbereiner, l'approche de Dumas était potentiellement bien plus féconde car applicable de façon pertinente à un bien plus grand nombre d'éléments : alors que les progressions arithmétiques sont restreintes à quelques groupes d'éléments, l'incrément constaté par Dumas entre éléments successifs aux propriétés similaires mesure précisément la longueur de la période qui sépare ces deux éléments — incrément d'environ 16 entre les deux premiers éléments d'une tétrade, puis incrément d'environ 48 entre deuxième et troisième éléments, puis entre troisième et quatrième éléments.

Le premier à remarquer la périodicité des propriétés chimiques des éléments fut le géologue français Alexandre-Émile Béguyer de Chancourtois lorsqu'il classa en 1862 les éléments chimiques alors connus en fonction de leur masse atomique déterminée en 1858 par le chimiste italien Stanislao Cannizzaro. Il normalisa la masse atomique de tous les éléments en prenant celle de l'oxygène égale à 16, et, considérant que « les propriétés des éléments sont les propriétés des nombres, » organisa les éléments chimiques en spirale sur un cylindre divisé en seize parties, de telle sorte que les éléments aux propriétés similaires apparaissent l'un au-dessus de l'autre.

Chancourtois remarqua alors que certaines « triades » se retrouvaient précisément alignées dans cette représentation, ainsi que la tétrade oxygène – soufre – sélénium – tellure, qui se trouvait également avoir des masses atomiques à peu près multiples de seize (respectivement 16, 32, 79 et 128). C'est la raison pour laquelle il appela cette représentation « vis tellurique, » en référence au tellure. C'était la première ébauche de classification périodique des éléments. Celle-ci ne retint cependant pas l'attention de la communauté scientifique, car Chancourtois n'était pas chimiste et avait employé des termes appartenant plutôt au domaine de la géochimie dans la publication qu'il avait adressée à l'Académie des sciences, laquelle fut éditée de surcroît sans ses schémas explicatifs, ce qui rendit le texte abscons.

D'un point de vue conceptuel, c'était une grande avancée, mais, d'un point de vue pratique, Chancourtois n'avait pas identifié la période correcte pour les éléments les plus lourds, de sorte que, dans sa représentation, une même colonne regroupait le bore, l'aluminium et le nickel, ce qui est correct pour les deux premiers mais totalement erroné d'un point de vue chimique pour le troisième.

Dans la foulée, le chimiste anglais John Alexander Reina Newlands publia en 1863 une classification périodique qui eut, elle, un plus fort retentissement (quoique tardif, et "a posteriori"), car il avait organisé les premiers éléments alors connus par masse atomique croissante — plus précisément, par masse équivalente croissante — dans un tableau à sept lignes en les arrangeant de telle sorte que leurs propriétés chimiques soient similaires par lignes, sans hésiter à placer deux éléments dans une même case si nécessaire pour éviter de laisser des cases vides par ailleurs.

Ce faisant, il avait identifié une nouvelle triade, dont les extrémités étaient le silicium et l'étain, et dont l'élément médian restait à découvrir : il prédit ainsi l'existence du germanium, en lui assignant une masse atomique d'environ 73. Mais la grande faiblesse de son travail était qu'il n'avait pas laissé de case vide dans son tableau pour accueillir notamment le futur germanium : il avait en fait cherché avant tout à classer les éléments connus dans un tableau complet sans chercher de classification plus large tenant compte de possibles éléments à découvrir, qu'il avait pourtant pressentis. De plus, comme Chancourtois, il avait un problème de périodicité, car si les éléments légers connus à l'époque avaient bien une périodicité chimique tous les sept éléments, cela cessait d'être valable au-delà du calcium, et le tableau de Newlands s'avère alors inopérant :

La mise en évidence d'une périodicité globale jusqu'au calcium était néanmoins une grande avancée, et Newlands présenta cette classification en l'appelant « loi des octaves » par analogie avec les sept notes de musique, mais ce travail fut assez mal accueilli par ses pairs de la Société de chimie de Londres, qui le tournèrent souvent en ridicule et firent obstacle à sa publication ; ce n'est qu'après la publication des travaux de Dmitri Mendeleïev que la qualité de cette analyse a été reconnue.

Le chimiste anglais — secrétaire de la Société de chimie de Londres, et donc rival de Newlands — travaillait également, dans les années 1860, à une table périodique des éléments chimiques remarquablement proche de celle que publierait Mendeleïev en 1869. Elle était organisée en périodes verticales avec des cases vides pour les éléments manquants et plaçait — à la différence du premier tableau de Mendeleïev — le platine, le mercure, le thallium et le plomb dans les bons groupes. Son action négative à l'encontre de Newlands entacha néanmoins définitivement la renommée d'Odling, et sa contribution à l'élaboration du tableau périodique des éléments est aujourd'hui largement méconnue.

La contribution du chimiste allemand Lothar Meyer est à peine mieux reconnue que celle d'Odling, car ses travaux décisifs ont été publiés après ceux de Mendeleïev alors qu'ils étaient pour la plupart antérieurs. Il publia ainsi une première version de sa classification des éléments en 1864, puis finalisa en 1868 une seconde version plus aboutie qui ne fut intégralement publiée qu'à sa mort, en 1895.

Le premier tableau de Meyer comprenait vingt-huit éléments classés en six familles définies par leur valence : c'était un grand pas en direction de la forme moderne du tableau périodique, organisé en groupes dépendant de la configuration électronique des éléments, elle-même directement en relation avec leur valence ; ce n'était néanmoins pas encore le même tableau qu'aujourd'hui, car les éléments étaient toujours rangés par masse atomique croissante. Le second tableau de Meyer, qui élargissait et corrigeait le premier, fut publié en 1870, quelques mois après celui de Mendeleïev, dont il renforça l'impact sur la communauté scientifique en apportant aux thèses du chimiste russe, encore très contestées, le soutien de travaux indépendants. La grande force de ce travail résidait dans les périodes de longueur variable, avec une disposition des éléments qui permettait d'éviter les regroupements fâcheux de Newlands, tels que le fer, l'or et certains éléments du groupe du platine parmi l'oxygène, le soufre, et les autres éléments du groupe 16 :

Meyer avait également remarqué que si l'on trace une courbe représentant en abscisse la masse atomique et en ordonnée le volume atomique de chaque élément, cette courbe présente une série de maxima et de minima périodiques, les maxima correspondant aux éléments les plus électropositifs.

Malgré la qualité réelle des travaux de ses contemporains, c'est bien au chimiste russe Dmitri Mendeleïev qu'on doit le premier tableau périodique des éléments s'approchant de celui que nous utilisons aujourd'hui, non seulement dans sa forme mais surtout par la vision qui l'accompagne. À la différence de ses prédécesseurs, Mendeleïev a en effet formulé explicitement en quoi son tableau constituait un outil d'analyse théorique des propriétés de la matière :


L'avancée était significative :


Les travaux de Mendeleïev ont été accueillis avec scepticisme par ses pairs, mais la publication subséquente de plusieurs résultats similaires (ceux de John Newlands et de Lothar Meyer en particulier) obtenus de façon indépendante ont fait basculer le consensus en faveur de cette nouvelle vision des éléments chimiques.

C'est en voulant mesurer avec précision la masse atomique de l'oxygène et de l'azote par rapport à celle de l'hydrogène que John William Strutt Rayleigh nota une divergence entre la masse atomique de l'azote produit à partir d'ammoniac et celle de l'azote séparé de l'air atmosphérique, légèrement plus lourd. Employant une méthodologie rigoureuse, William Ramsay parvint en 1894 à isoler l'argon à partir de « l'azote » atmosphérique, et expliqua l'anomalie apparente de la masse atomique de l'azote atmosphérique en déterminant la masse atomique de ce nouvel élément, pour lequel rien n'était prévu dans le tableau de Mendeleïev. Sa nature gazeuse et son inertie chimique l'avaient rendu jusqu'alors invisible aux chimistes.

La masse atomique de l'argon (un peu moins de 40) est très voisine de celle du calcium (un peu plus de 40) et donc supérieure à celle du potassium (39,1), ce qui posa quelques problèmes de classification car il semblait y avoir « plus de place » dans le tableau périodique entre le chlore et le potassium qu'entre le potassium et le calcium. Les choses se compliquèrent encore lorsque Ramsay et Morris Travers découvrirent le néon en 1898, matérialisant, avec l'hélium (découvert en 1868 par l'astronome français Jules Janssen et l'Anglais Joseph Norman Lockyer), le groupe nouveau des gaz rares (ou gaz nobles), appelé « groupe 0 » : la masse atomique du néon (20,2) était exactement intermédiaire entre celles du fluor (19) et du sodium (23). Ainsi, les gaz rares semblaient se positionner tantôt entre un métal alcalin et un métal alcalino-terreux, tantôt entre un halogène et un métal alcalin.

À la suite de la découverte de l'électron et de celle des isotopes par l'Anglais Joseph John Thomson — qui ont accompagné les débuts de la physique de l'atome avec les travaux de l'Allemand Max Planck, du Néo-Zélandais Ernest Rutherford et du Danois Niels Bohr — les recherches du physicien anglais Henry Moseley sur la corrélation entre la charge du noyau atomique et le spectre aux rayons X des atomes ont abouti en 1913 au classement des éléments chimiques non plus par masse atomique croissante, mais par numéro atomique croissant. C'était une évolution majeure, qui résolvait toutes les incohérences issues du classement en fonction de la masse atomique, lesquelles devenaient gênantes depuis les travaux de systématisation de Dmitri Mendeleïev.

L'argon était ainsi placé entre le chlore et le potassium, et non plus entre le potassium et le calcium, tandis que le cobalt était clairement positionné avant le nickel bien qu'il soit un peu plus lourd. Il confirma que le tellure devait être placé avant l'iode sans nécessiter de revoir sa masse atomique, contrairement à ce qu'avait suggéré Mendeleïev. Il releva également que les éléments de numéro atomique 43 et 61 manquaient à l'appel : l'élément 43 avait déjà été prédit par Mendeleïev comme eka-manganèse (il s'agit du technétium, radioactif, synthétisé en 1937) mais l'élément 61 était nouveau — il s'agit du prométhium, radioactif également, isolé en 1947 :

Ce tableau, directement inspiré de celui de John Newlands, constituait l'étape intermédiaire conduisant à la disposition contemporaine. En particulier, la numérotation des groupes avec des chiffres romains de à , qui remontent à Newlands, et les lettres A et B, introduites par Moseley, étaient encore largement utilisées à la fin du :

Il était identique au tableau actuel, hormis pour ce qui avait trait à la septième période.

Le physicien américain Glenn Theodore Seaborg contribua dès 1942 au projet Manhattan dans l'équipe du physicien italien Enrico Fermi. Il était chargé d'isoler le plutonium — que lui-même avait synthétisé et caractérisé en février 1941 — de la matrice d'uranium au sein de laquelle il se formait. C'est au cours de ce travail qu'il développa une connaissance approfondie de la chimie particulière de ces éléments. Il établit ainsi que leur position dans le tableau périodique (l'uranium était alors placé sous le tungstène et le plutonium sous l'osmium) ne rendait pas compte de leurs propriétés.

En 1944, il parvint à synthétiser et à caractériser l'américium et le curium (éléments 95 et 96), ce qui lui permit de formaliser le concept des actinides, c'est-à-dire d'une nouvelle famille aux propriétés spécifiques et formée des éléments 89 à 103, située sous les lanthanides dans le tableau périodique, qui prit ainsi sa configuration actuelle. Seaborg conjectura également l'existence des superactinides, regroupant les éléments 121 à 153 et situés sous les actinides.

Le tableau périodique utilisé de nos jours est celui remanié en 1944 par Seaborg.

De très nombreuses présentations alternatives du tableau périodique ont été proposées tout au long du , et des présentations graphiques innovantes sont encore régulièrement proposées. L'une des plus anciennes et des plus simples est celle d'un autodidacte français par ailleurs inconnu, Charles Janet, qui a donné son nom à une disposition du tableau élaborée au début du et récemment redécouverte par les Anglo-saxons, chez lesquels elle est assez bien connue des spécialistes du sujet (sous les noms de "Janet Form" ou de "Left-Step Periodic Table") car elle range les éléments chimiques sur des périodes définies chacune par une valeur de donnée (où est le nombre quantique principal et le nombre quantique azimutal) tout en ayant le double mérite de rester familière et de disposer les éléments dans l'ordre naturel des blocs (de droite à gauche), à la différence du tableau usuel :

Une autre représentation est celle de Theodor Benfey, datée de 1960, dont l'objectif était de remédier aux discontinuités du tableau standard à l'aide d'une représentation en spirale :

De nombreux modèles en trois dimensions ont également été proposés afin d'enrichir la représentation des éléments par diverses informations spécifiques.

Une autre représentation a été proposée par Timmothy Stowe, en losanges par niveaux de remplissage: voir Tableau radial des éléments chimiques)

Le tableau de Mendeleïev a été adapté pour représenter d'autres données physiques des éléments, et été appliqué pour visualiser des éléments totalement différents 

Concernant plus particulièrement les métaux, jusque dans les années 1970, moins de 20 métaux étaient utilisés dans l'industrie. Depuis les années 2000, par suite du développement exponentiel des produits électroniques, des technologies de l'information et de la communication, de l'aéronautique, allié à l'innovation technique dans la recherche de performances et de rendements, la demande en nouveaux métaux « high tech » a explosé, et concerne maintenant environ 60 métaux. Pratiquement tous les éléments de la table sont utilisés jusqu'au (uranium). Les réserves de la plupart des métaux au niveau de production 2008 varient de 20 ans à 100 ans.




</doc>
<doc id="3010" url="https://fr.wikipedia.org/wiki?curid=3010" title="Théâtre">
Théâtre

Le théâtre () est à la fois l'art de la représentation d'un drame ou d'une comédie, un genre littéraire particulier, et l'édifice dans lequel se déroulent les spectacles de théâtre. On parle aussi de genre dramatique.

Jadis, le mot en , désignait également la scène ou le plateau, c'est-à-dire toute la partie cachée au public par le rideau.

Au sens figuré, « théâtre » désigne un lieu où se déroule une action importante (par exemple, un théâtre d'opérations militaires).

Aujourd'hui, à l'heure des arts dits pluridisciplinaires, la définition de l'art du théâtre est de plus en plus large (jusqu'à se confondre avec l'expression spectacle vivant), si bien que certains grands metteurs en scène n'hésitent pas à dire que pour qu'il y ait théâtre, il suffit d'avoir un lieu, un temps, un acte et un public.

Il s'agit de spectacles dans lesquels des comédiens, mis dans les circonstances et les situations créées par un texte et la vision d'un metteur en scène/réalisateur, incarnent des personnages pour un regard extérieur (le public), dans un temps et un espace limité. Les dialogues écrits sont appelés pièces de théâtre, mais il peut y avoir également du théâtre sans texte écrit ou même sans aucune parole. Il existe aussi des œuvres de théâtre musical, le genre étant particulièrement représenté dans les célèbres quartiers de Broadway aux États-Unis ou du West End à Londres, mais aussi de plus en plus autour des Grands boulevards à Paris.

Dans la création contemporaine, les frontières entre les différents arts de la scène (théâtre, mime, cirque, danse...) sont de plus en plus ténues, si bien que certains professionnels n'hésitent pas à remplacer le mot "théâtre" par les mots "spectacle pluridisciplinaire" ou "spectacle vivant", mettant ainsi l'accent sur le métissage des disciplines.

Dès les débuts de l'humanité, le "théâtre" désignait l'acteur qui racontait, qui revivait une expérience de chasse, de conflit, pour la partager avec son groupe.
Dans la civilisation occidentale on considère les cortèges en l'honneur du dieu grec Dionysos comme les premières représentations théâtrales, bien avant le . C'est en effet d'abord à l'époque grecque antique qu'apparaît le "Theatron" (, qui vient de : regarder, contempler). Le terme désigne alors l'hémicycle destiné aux spectateurs. Un théâtre est donc à l'origine un lieu d'où le public observe un spectacle. À la Renaissance, la signification s'étend non seulement à l'ensemble de l'édifice de spectacle, scène comprise, mais également à l'art dramatique. Ce n'est qu'après la période du théâtre classique que le terme devient par antonomase le texte qu'il soit lu ou joué.

Le théâtre est né en Grèce, où des concours tragiques existent depuis le . Il est apparu à Rome à la fin du Les représentations font partie des « jeux » ("ludi"), fêtes officielles de la cité. À Rome, on édifie d'abord des théâtres en bois, où seuls les spectateurs des premiers rangs sont assis, puis des théâtres en pierre : théâtre de Pompée en 55 av. J.-C., de Balbus en 13 av. J.-C., de Marcellus en 12 ou 11 av. J.-C. En Campanie, par exemple à Pompéi, on construit des théâtres en pierre dès le . À l'époque impériale, chaque ville romaine a son théâtre, comme Ostie en Italie, Orange en Gaule ou Sabratha en Afrique.

Dans le théâtre romain, plus anciennement dans le théâtre grec, les acteurs portaient un masque : cet accessoire leur permettait d'être mieux vus des spectateurs assis sur les gradins parfois éloignés et d'en être mieux entendus, leur voix étant amplifiée comme par des porte-voix. Il y avait des masques tragiques (un visage triste) ou comiques (un visage fendu d'un large rire) ainsi que des masques doubles (un côté tragique, un côté comique) ; les acteurs qui se servaient de ces derniers devaient jouer de profil.
L'acteur, exclusivement masculin, porte aussi des vêtements aux rembourrages voyants et cloturaux ainsi qu'une coiffure très haute, censés évoquer le gigantisme des dieux et des héros qu'il incarne.
Au Moyen Âge, des troupes itinérantes jouent des pièces de genre dit des « Miracles », des "Mystères" et des « drames liturgiques », d'abord dans les églises puis dans leurs porches, sur leurs parvis et sur les places publiques. Elles ont pour vocation de raconter la vie des Saints mais sont très longues, alors pour maintenir le spectateur éveillé on y glissait en intermède quelques petites farces.

Un genre théâtral est le résultat d'une création comique correspondant à une forme particulière : le spectateur, connaissant un genre donné, sait à quoi s'attendre, et selon la présentation de l'œuvre (tragédie, comédie…), il a une vision stéréotypique de l'œuvre.

Le genre est donc, avant tout, une convention qui donne un cadre, une forme précise. C'est un premier échange implicite entre l'artiste et le spectateur. Il inclut diverses formes théâtrales dont la farce, la comédie, la pantomime, la tragédie, le drame romantique, le drame bourgeois, la tragédie lyrique, le vaudeville, le mélodrame, les mystères médiévaux, le théâtre de marionnettes, le théâtre forum, le théâtre d'improvisation, le théâtre en plein air, le théâtre de rue, le théâtre expérimental, le théâtre installation performance, la danse-théâtre (ou théâtre-danse), le web-théâtre avec les expérimentations d'e-toile, le café-théâtre d'improvisation, le théâtre de l'absurde, le conte, la revue.

Le théâtre de société, théâtre amateur joué dans les demeures privées de riches propriétaires, par, et pour, des proches de ces derniers, est une forme théâtrale qui s'est développée plus particulièrement à partir du . Notamment en Suisse romande sous l'influence de Voltaire, installé près de Genève, et de Germaine de Staël, au château de Coppet. Des témoignages exceptionnels (costumes et décors) ont été conservés au château d'Hauteville.

Molière disait, traduisant ainsi une devise de Santeul : le but de la comédie est de corriger les mœurs ("castigat ridendo mores"), ce qui vaut aussi pour la tragédie. Ces deux formes théâtrales ont en effet une "portée édifiante".
Depuis quelques années est apparu un genre nouveau : le théâtre témoignage. Les premiers spectacles abordaient la question des drames vécus par les personnes ayant subi des licenciements économiques ("Les yeux rouges" pour les employés de Lip ; "501 blues" pour ceux de Levis). Puis sont apparus des spectacles témoignant des horreurs des génocides de la fin du : Olivier Py et son "Requiem pour Srebrenica", ou encore Jacques Delcuvellerie avec "Rwanda 94".

Le metteur en scène au théâtre prend une réelle dimension à la fin du . Il acquiert la place de « maître du plateau ». Ce bouleversement est notamment provoqué par Constantin Stanislavski, auteur et metteur en scène russe né en 1863 à Moscou, qui va, à 35 ans, créer avec Vladimir Nemirovitch-Dantchenko le Théâtre d'Art de Moscou. Il y crée des spectacles de Tchekhov notamment ("Les Trois Sœurs", 1900) et y enseigne une nouvelle pratique du théâtre basée sur le travail corporel, le travail physique et le refus du jeu conventionnel. Ce « système » (nom donné, par les contemporains, à sa façon de travailler) qu'il décrit dans son livre, "La formation de l'acteur", influence ses successeurs, dont Valère Novarina, Claude Régy ou encore Jean Vilar qui, dans la préface du roman, expose qu'« "il n'est pas de comédien authentique qui n'ait, un jour ou l'autre, emprunté, sciemment ou non, quelques-uns des sentiers" » du livre de Stanislavski.

Pour la préparation d’une production et les représentations, le metteur en scène peut faire appel à plusieurs autres personnes, notamment :

L'acteur de théâtre est difficile à définir car dans la vie quotidienne, tous sont acteurs à des degrés divers. Les humains, vivant en société, deviennent nécessairement des acteurs sociaux, qui changent de rôle constamment (au travail, en famille, entre amis, etc.). Ces rôles sont constituants, puisqu'ils font partie de l'identité ; et indispensables, puisqu'ils donnent de la cohérence à la société et une place à chacun. La scène 1 de l'acte 1 est nommée l'intersigne.

Le réel acteur de théâtre ne joue en général qu'un seul rôle à la fois, clairement défini et cohérent. L'acteur sait qu'il n'est pas réellement le personnage, même s'il doit s'identifier à lui. Les rôles de théâtre ne sont donc pas constituants. Cependant, afin de rendre celui-ci fort et cohérent, un acteur s'investit souvent dans son rôle avec sa personnalité et son vécu. Il n'empêche que certains sont accusés de jouer tous leurs personnages de la même manière, de cabotiner. Ce problème du paradoxe sur le comédien est exposé par Diderot.



Pourquoi, s'ils jouent déjà naturellement des rôles, les humains se sont-ils mis à jouer du théâtre ? De façon générale, comme le rappelle Aristote dans "La Poétique", les gens réagissent différemment dans la vie, et face à une œuvre d'art. Un cadavre en décomposition horrifie, mais une nature morte ravit. Il y a donc un pouvoir propre à la représentation ("mimésis"), au jeu, qui permet d'appréhender avec plaisir ce qui autrement pose problème.

Le théâtre est donc joué pour faire face aux mystères et conflits qui inquiètent.

Les gens de théâtre cherchent ainsi à créer un "miroir social", un reflet plus ou moins caricatural de la société, qui permet de mieux la comprendre, et de mieux dénoncer ses failles : ce rôle politique était particulièrement évident dans la Grèce antique, avec la comédie ancienne. Mais cette citation du "Hamlet" de Shakespeare peut aussi être mentionnée : « "for any thing so overdone is from the purpose of playing, whose end, both at the first and now, was and is, to hold, as 'twere, the mirror up to nature" ». Le théâtre est aussi un miroir tendu à la nature : le spectateur, comme l'acteur, vient chercher une réponse, se construire une identité.

Le théâtre peut avoir un effet cathartique, servant d'exutoire aux passions qui ne sont pas autorisées par la société. Le théâtre peut aussi être un divertissement, sans autre objectif que de changer les idées à ses spectateurs, par l'utilisation du comique notamment.

Augusto Boal, qui aborda une manière de faire du théâtre résolument politique, c'est-à-dire qu'il faisait jouer à des gens des situations conflictuelles en changeant la position des personnages : par exemple, le directeur qui avait licencié tel salarié jouait le rôle du salarié. Cela permettait selon lui de régler certains conflits. C'est l'origine de ce qu'on a appelé le théâtre forum, et en Belgique le théâtre-action.

Le festival le plus renommé en France est le "Festival d'Avignon". Il existe de nombreux festivals, notamment en période estivale. Certains se concentrent sur un genre particulier (Aurillac pour le théâtre de rue, ou celui de Charleville-Mézières pour le théâtre de marionnettes, par exemple) ou bien restent "généralistes" en tentant la plupart du temps de programmer un spectacle avec une tête d'affiche pour attirer le public.

Pour leur formation, les jeunes apprentis acteurs sont envoyés en tournée dans toute leur région.

Une majorité des comédiens en activité a suivi une formation, que ce soit par le conservatoire national supérieur d'art dramatique, un Conservatoire de musique, danse et art dramatique ou un cours privé.

Internet est considéré par certains comme un concurrent du théâtre voire un adversaire qui encourage le goût de la dématérialisation de relations, contraire à la proximité humaine propre au théâtre. Certains ont considéré les sites de théâtre sur Internet comme « des officines responsables de la régression du théâtre ».

D'autres personnes estiment qu'Internet apporte beaucoup au théâtre : une popularisation par la diffusion d'opinions sur les pièces, une démocratisation par la pression à la baisse des tarifs initiée par les billetteries en ligne et un accès facilité à l’information (historique, programmation, réservation).

Ainsi, en septembre 2006, la Comédie-Française a ouvert ses portes aux acteurs du web, que ce soit pour publier sur le Web des avis sur les pièces, ou pour proposer des tarifs réduits aux spectateurs.

Le Théâtre des Osses, compagnie suisse fondée en 1978 et devenue Centre dramatique fribourgeois en 2003 ouvre un site d'archives.



</doc>
<doc id="3011" url="https://fr.wikipedia.org/wiki?curid=3011" title="Tricky">
Tricky

Tricky, de son vrai nom Adrian Nicholas Matthews Thaws, est un musicien britannique, né le . Il est considéré comme un des piliers du trip hop, mouvement musical des années 1990 et il mélange notamment rock, hip-hop, musique électronique et musique soul. Son premier album, "Maxinquaye", a été nommé pour le Mercury Prize et fut élu album de l’année par la revue musicale NME. Il fut membre de Massive Attack mais il quitte le groupe en 1994 après la sortie de l'album "Protection".

Tricky est né dans le quartier de Knowle West, à Bristol, au sud-ouest de l'Angleterre. Son père abandonna sa famille avant même sa naissance et sa mère, Maxine Quaye se suicida alors qu’Adrian n’avait que quatre ans. Tricky nomma son premier album "Maxinquaye" en hommage à sa mère, et affirma un jour que, bien qu’il ne l’ait connue que très peu, il avait le sentiment qu’elle parlait encore à travers lui.

Adrian passa une grande partie de sa jeunesse avec sa grand-mère, qui le laissait souvent regarder de vieux films d’horreur plutôt que d’aller à l’école. À 15 ans, Tricky commença à écrire ses premiers textes, qui reflétaient plus les préoccupations sexuelles d’un adolescent que les thèmes sombres et ambigus qu’on trouverait plus tard sur ses albums. À 17 ans, il fit un séjour en prison pour avoir acheté des faux billets de 50 livres sterling à un ami, qui le dénonça par la suite. Tricky déclara à ce sujet dans une interview : (NME, 1995).

Après avoir rencontré DJ Milo, Tricky commença à fréquenter les membres d’un "sound system" nommé The Wild Bunch (des rangs duquel naquit entre autres le groupe Massive Attack). Adrian fut alors surnommé « "Tricky Kid" », et à 18 ans il intégra le groupe de rap Fresh 4, qui gravitait autour du Wild Bunch. Tricky collabora sur le premier album de Massive Attack, "Blue Lines", sorti en 1991. Mais dès qu’il devint clair que faire de la musique était aussi un business, Tricky fut passablement échaudé. Bien qu’il apparaisse à nouveau sur l’album suivant de Massive Attack, "Protection" (1994), Tricky n’eut jamais le sentiment de faire partie du groupe.

En 1991, avant la sortie de "Blue Lines", il rencontra Martina Topley-Bird. Quelque temps plus tard, de passage chez Thaws, elle lui dit, ainsi qu’à Mark Stewart, qu’elle savait chanter. Martina n’avait que 15 ans à l’époque, mais sa voix impressionna grandement les deux hommes, et ensemble ils enregistrèrent "Aftermath" (bien que le magazine musical anglais "The Face" prétendît en 1995 que la première chanson que Tricky et Martina enregistrèrent ensemble était "Shoebox"). Tricky fit écouter "Aftermath" aux membres de Massive Attack, qui ne furent pas intéressés. En conséquence de quoi, en 1993, Tricky fit presser la chanson sur une centaine de vinyles. Il la copia directement depuis la cassette, pour un rendu plus brut. Finalement, le single "Aftermath", édité en white label, lui permit de décrocher un contrat avec la maison de disques Island Records, de sorte qu’il put commencer l’enregistrement de son premier album.

Après avoir quitté Massive Attack Tricky sort son premier album solo "Maxinquaye".
Cet album sera un succès massif et lui vaudra une célébrité internationale avec laquelle il sera notoirement mal à l'aise. La richesse de cet album réside dans le mélange d'influences musicales variées qui parviennent toutefois à produire un son unique et original. À propos de cet album le magazine Rolling Stone écrit : "Tricky a tout absorbé depuis le hip-hop et la soul américaine jusqu'au reggae et aux racines les plus mélancoliques du rock anglais des années 80. Il dira notamment que Kate Bush l'a beaucoup inspiré pour ce premier album. Sorti quelques mois après le premier album de Portishead, "Maxinquaye" contribue à faire de Bristol une capitale (éphémère) de la pop.

Tricky apparaît ensuite à l’écran dans "Le Cinquième Elément" de Luc Besson et livre six albums, dont le chef-d’œuvre "Nearly God" sorti en 1996 où il reprend "Tattoo", un morceau pré-trip-hop de Siouxsie and the Banshees. À ses débuts en solo, la chanteuse qui l'accompagnait était Martina Topley-Bird, son ex-femme. Puis, il a rencontré Costanza Francavilla qui lui a passé une maquette en Italie à la fin d'un de ses concerts. Elle l'a accompagné sur le disque Vulnerable.

Tricky a collaboré également avec Björk et Neneh Cherry, les Red Hot Chili Peppers, Cyndi Lauper, repris Nirvana, Eurythmics, XTC, Kylie Minogue, le thème de "Wonder Woman" signé Charles Fox ("Barbarella", "La croisière s'amuse")... Il est apparu sur la compilation "Mr Gainsbourg Revisited" en hommage à Serge Gainsbourg, au côté de Placebo, Franz Ferdinand, Marianne Faithfull, Marc Almond et Carla Bruni entre autres.

Il a donné un concert le au Zénith de Paris. Il est aussi passé régulièrement par le festival des Eurockéennes de Belfort en 1999, 2001, 2003 et 2009.

Tricky s'est forgé une image forte et à part à travers ses clips, notamment ceux réalisés par Stéphane Sednaoui pour les morceaux "Hell Is Round The Corner" et "For Real". Il s'est aussi essayé avec succès à la réalisation.

En 2008, il réside à Paris au Cent Quatre (établissement culturel) du décembre à mi-février.

En 2009, il est accompagné sur scène de la chanteuse Francesca Belmont avec qui il a commencé à écrire un nouvel album, entre Paris et Londres.









La carrière de Tricky est marquée par un nombre important de collaborations et de duos. L'album "Nearly God" en particulier est composé uniquement de duos.

Certaines de ces collaborations se sont achevées de manière brutale. Il a notamment exprimé son animosité envers le groupe Massive Attack et utilise parfois le terme «vampire» quand il parle de Björk.

Voici une liste non-exhaustive des artistes et des groupes ayant travaillé avec Tricky :



</doc>
<doc id="3012" url="https://fr.wikipedia.org/wiki?curid=3012" title="Tercet">
Tercet

En poésie, un tercet est une strophe de trois vers. Par exemple, dans un sonnet, on trouve deux tercets précédés de deux quatrains.

Le nom "tercet" est un emprunt à l'italien "terzetto", lui-même dérivé de nombre "trois", et son existence est attestée en 1606 sous la forme "tiercet", mot qui persiste jusqu’à la fin du comme le prouve le vers 802 des "Femmes Savantes" de Molière en 1672 : 

La forme actuelle « tercet » est repérée à partir de 1658 et le mot désigne une strophe de trois vers, essentiellement utilisée dans le sonnet.

Dante invente le tercet dans sa "Divine Comédie" en 1307 en instituant la "terza rima" qui prolonge le système de rime de tercet en tercet sur le modèle aba/bcb/cdc… en achevant la cascade par un vers isolé final. Cette structure est également connue en Espagne sous le terme de "tercet enchaîné" ("terceto encadenato").

Cette strophe impaire sera exploitée en tant que telle par Pétrarque qui en fera également une des composantes du sonnet.
Une combinaison des rimes concernant les deux tercets s'intitule et ouvre la voie à des dispositions diverses structurant de fait un distique avec une rime suivie et un quatrain aux rimes croisées ou embrassées (ccd/ede ou ccd/eed).

Cette technique est utilisée en France aux et siècles (dans la villanelle par exemple : 19 vers sur 2 rimes avec 5 tercets et un quatrain final, système aba/aba/…abaa) mais tombe assez vite en désuétude et les poètes majeurs comme Du Bellay et Ronsard ne l'utiliseront qu'en association avec des quatrains dans le sonnet.
Le tercet est repris par exception par la suite : on repère son emploi au par Vigny (dans le poème d'ouverture des "Destinées"), par Gautier ou Leconte de Lisle avant Paul Valéry au siècle suivant dans son poème "La fileuse".

Aujourd'hui le tercet réapparaît avec l'adaptation du haïku japonais qui est un poème réduit à un seul tercet codifié (5/7/5 syllabes).

Son statut de strophe lui est parfois contesté en raison de son défaut d'autonomie quant aux rimes puisqu'il est soit monorime, soit déséquilibré par une rime orpheline. Certains théoriciens préfèrent alors parler de « groupement de vers », typographiquement repérable, en signalant que la mise en page qui isole le tercet n'est pas toujours respectée même si, dans l'habitude française, elle est commune.



</doc>
<doc id="3013" url="https://fr.wikipedia.org/wiki?curid=3013" title="Terminatif">
Terminatif

En linguistique, le terminatif est un cas grammatical présent dans certaines langues, exprimant la limite spatiale ou temporelle, le point ultime d'un déplacement ou d'une période de temps. Il correspond à la préposition française "jusqu'à".

Exemple :



</doc>
<doc id="3014" url="https://fr.wikipedia.org/wiki?curid=3014" title="Translatif">
Translatif

En linguistique, le translatif est un cas grammatical présent dans certaines langues qui exprime le résultat d'un processus de transformation.

Ainsi, le translatif existe en finnois (langue agglutinante) où il se construit avec le suffixe "-ksi". Du fait de son sens, ce cas s'utilise fréquemment avec le verbe "tulla", « devenir », comme dans l'exemple suivant :
On le trouve dans d'autres emplois, comme les demandes de termes étrangers :
soit plus correctement « Comment dit-on « kissa » en français ? » (la réponse étant « chat »). Précisons aussi que le translatif a d'autres emplois spécifiques plus éloignés de cette valeur première dans les grammaires des langues qui en possèdent un.

Autre exemple, en estonien :
En espéranto, le suffixe correspondant est -iĝ :


</doc>
<doc id="3016" url="https://fr.wikipedia.org/wiki?curid=3016" title="Tsui Hark">
Tsui Hark

Tsui Hark (徐克 en chinois, Xú Kè en hànyǔ pīnyīn, Cheui4 Hak1 en cantonais) né Tsui Man-kong, est un réalisateur hong-kongais né au Viêt Nam le (alors en Cochinchine française), à qui l'on doit notamment la série des "Il était une fois en Chine" avec Jet Li. Il est également producteur, acteur et scénariste.

Né au Viêt Nam, il étudie à Hong Kong, puis part aux États-Unis, au Texas dans les années 1970 pour étudier le cinéma. Il revient à Hong Kong en 1977. Il tourne d'abord des séries pour la télévision avant de passer au cinéma en 1979 avec "The Butterfly Murders" (un film de sabre, le "Wu Xia Pian", très populaire dans le monde chinois), puis deux autres films : "We're Going to Eat You" et "L'Enfer des armes". Ceux-ci, extrêmement agressifs et provocateurs, sont rejetés par le public. En 1981, il entre à la "Cinema City" du producteur Karl Maka pour tourner des films plus conventionnels mais qui marchent.

En 1983, "Zu, les guerriers de la montagne magique" est le film qui marque le tournant de sa carrière. Il marque également le renouveau du film de combat hong-kongais avec des combats spectaculaires, des experts en arts martiaux qui volent... Il essaie d'apporter des effets spéciaux qui tiennent la route comme dans les films occidentaux, mais le film, sorti trop tôt à son goût, ne va pas avoir un succès public. Il retourne donc au cinéma commercial. 

Pour avoir son indépendance, il fonde sa propre maison de production, Film Workshop, en 1984. Il tourne de nouvelles œuvres importantes : "Shanghai Blues" et "Peking Opera Blues". Il déborde d'idées et cherche notamment à remettre au goût du jour la culture chinoise. En tant que producteur, il s'attaque au polar avec "Le Syndicat du crime" en 1986, au film en costumes avec "Histoire de fantômes chinois" en 1987, au film de sabre avec "Swordsman" en 1990. Comme réalisateur, il renouvelle le film de kung-fu avec "Il était une fois en Chine" en 1991.

Il crée sa propre société d'effets spéciaux, Cinefex. Il s'entoure, malgré son caractère un peu despotique, de gens talentueux : les réalisateurs John Woo (avec qui il se brouille en 1990), Ching Siu-tung, Yuen Woo-ping, Kirk Wong, Ringo Lam, les acteurs Chow Yun-fat, Leslie Cheung ou Zhao Wen-zhou, sans oublier Jet Li (qui se brouillera aussi momentanément avec lui après "") et Brigitte Lin.

Dans le mitan des années 1990, la crise du cinéma n'a pas épargné Hong Kong et la Film Workshop. Tsui Hark s'est recentré sur ses propres réalisations. Il signe alors quelques-unes de ses œuvres majeures : "Green Snake" (1993) d'après la Légende du serpent blanc, "The Lovers" (1994) d'après la légende des amants papillons et "The Blade" (1995), remake plus ou moins avoué du classique de Chang Cheh pour la Shaw Brothers : "La Rage du tigre". Puis il tente sa chance à Hollywood pour deux films mineurs : "Double Team" et "Piège à Hong Kong" avec Jean-Claude Van Damme. Il ne s'est pas fait aux méthodes américaines et est donc revenu à Hong Kong où il réalise une nouvelle œuvre majeure, "Time and Tide". 

Par la suite, il poursuit ses films liés à la culture chinoise en retournant une nouvelle version de "Zu" en 2001, "La Légende de Zu", un film contenant une débauche d'effets spéciaux. En 2005, il adapte la nouvelle "Seven Swordsmen from Mountain Tian", de Liang Yusheng avec "Seven Swords".

En 2007, il réalise la première partie de "Triangle"; les deux autres parties étant réalisés par Ringo Lam et Johnnie To.

Tsui Hark a également collaboré à la réalisation d'un film d'animation : "", dirigé par Andrew Cheng.







</doc>
<doc id="3017" url="https://fr.wikipedia.org/wiki?curid=3017" title="TGV">
TGV

En France, un TGV, sigle de train à grande vitesse, est un train alimenté électriquement et propulsé par des moteurs électriques (à l’exception du prototype , dont la puissance était fournie par un groupe électrogène à turbines à gaz) et atteignant régulièrement la vitesse de sur des lignes à grande vitesse (LGV). Il a atteint lors d’un record du monde de vitesse sur rail en 2007.

Depuis sa mise en service en 1981, un réseau de lignes nouvelles à grande vitesse a été construit, qui atteint en décembre 2011, le quatrième au monde après ceux de la Chine, du Japon et de l’Espagne.

Les trains ont été conçus et, au départ, construits par des entreprises françaises. Ils ont été initialement conçus par la SNCF et Alstom (anciennement Alsthom) et largement construits par la société Alstom en France, permettant des améliorations de la vie de service, et la modernisation des intérieurs (construits par Bombardier Transport). Les TGV sont notamment fabriqués à Belfort (Alstom) pour les motrices et à Aytré (Alstom) pour les remorques intermédiaires, d'autres composants venant d'autres sites comme Tarbes (Alstom) pour la traction, Le Creusot (Alstom) pour les bogies, etc. D'autres composants essentiels, comme les essieux, étaient initialement fournis par d'autres entreprises françaises.

Un TGV est composé de deux locomotives, ou motrices, indépendantes encadrant une rame articulée de huit ou dix voitures, sauf le TGV TMST d’Eurostar qui présente des particularités liées aux règles de sécurité pour le franchissement du tunnel sous la Manche.

Depuis le retrait début 2015 des sept demi-rames du TGV postal utilisées par La Poste entre Paris et Cavaillon (Vaucluse), les TGV assurent exclusivement le transport de voyageurs. Ils circulent en France (ils sont alors exploités par la SNCF), entre la France et un autre pays (ils sont exploités par une de ces entreprises ferroviaires : Eurostar, Thalys, Lyria ou Alleo), en Espagne (Alta Velocidad Española AVE) ou en Corée du Sud (Korea Train Express).

L’idée de créer un train à grande vitesse pour relier les principales villes françaises a émergé au cours des années 1960, après que le Japon a commencé en 1959 la construction du premier train à grande vitesse au monde, le Shinkansen, qui fut mis en service en 1964 avec l'inauguration de la première ligne à grande vitesse reliant Tokyo à Osaka.

À l’époque, la SNCF cherchait un moyen de redresser la fréquentation de ses trains, qui baissait inexorablement. Une augmentation substantielle de la vitesse apparut comme la solution qui lui permettrait de concurrencer efficacement l’automobile et l’avion. Elle était stimulée par les expérimentations du projet d’aérotrain qui faisait appel à la technologie du coussin d’air radicalement différente du contact roue/rail du chemin de fer classique. Elle expérimentait également la voie des turbotrains légers testant dès 1967 le prototype TGS puis les ETG en service commercial dès mars 1971.

Le , un service de recherche naît à la SNCF, avec le lancement de l’étude « possibilités ferroviaires à très grande vitesse sur infrastructures nouvelles », le projet « C03 ». Ce projet innove à la fois par l’idée de la création de lignes nouvelles et par l’attention portée à la qualité de service et à la tarification, alors que la politique de la SNCF visait à cette époque prioritairement la réduction des coûts. Après avoir été présenté aux pouvoirs publics en 1969, le projet C03 est adopté en comité interministériel le 25 mars 1971. Toutefois, la mise en concurrence de ce projet avec celui de l’aérotrain laisse planer un doute sur l’issue de cette opération. . La décision est prise en 1974 lors d’un conseil interministériel restreint sur les économies d’énergie. Jules Antonini rédige une note qu’il dépose en bas de la pile de dossiers. Le président Georges Pompidou, déjà malade, abrège les débats au bout de trois heures et le projet TGV est lancé.

Dans sa première version, le TGV devait être mû par des turbines à gaz. Ce choix était motivé par la taille relativement petite des turbines, leur puissance massique élevée et leur capacité à délivrer une puissance élevée pendant un temps important. Le premier prototype TGV 001 sortit des ateliers Alsthom de Belfort le 25 octobre 1971 et fut la première rame de ce type construite. La seconde ne vit jamais le jour pour cause de budget serré.

Les essais du TGV 001, qui débutèrent le 4 avril 1972 sur la ligne de la plaine d'Alsace, apportèrent cependant beaucoup d’enseignements utiles à la suite du projet, notamment dans le domaine du freinage à haute vitesse, qui nécessitait de dissiper une importante quantité d’énergie cinétique, de l’aérodynamique et de la signalisation. La rame était articulée, deux caisses adjacentes s’appuyant sur un bogie commun (comme sur les rames inox Z 3700) tout en conservant une possibilité de mouvement relatif.

Ce prototype atteignit la vitesse de , pulvérisant ainsi le record du monde de vitesse ferroviaire en traction thermique. Le style du TGV, tant intérieur qu’extérieur avec le nez caractéristique des motrices, est dû au "designer" français Jacques Cooper et a marqué les générations suivantes de matériel.

Depuis 2003, une des motrices du TGV 001 est exposée aux abords de l’autoroute A36, près de Belfort, et l’autre à Bischheim, près de Strasbourg le long de l’autoroute A4, où se situe également un centre industriel de rénovation et d’entretien des rames et des motrices.

À la suite de la crise pétrolière de 1973, le choix fut fait de passer à la traction électrique, avec acheminement du courant par caténaires et captage par pantographe. Les raisons de ce choix furent autant politiques que techniques ou économiques : en effet, le coût de l’énergie ne représentait alors que 5 % environ du coût de traction, soit de l’époque par rame/km ( équivalent 2007), et le coût d’une rame électrique était d’environ 10 % plus élevé que celui d’une rame à turbines, pour une capacité inférieure, sans compter le coût des installations fixes.

Le passage à la traction électrique imposa de reprendre le programme de recherches et d’essais dans nombre de domaines.

La SNCF transforma en 1974 une automotrice pour construire le prototype surnommé Zébulon, qui permit de tester plusieurs innovations :
Zébulon parcourut environ un million de kilomètres en marches d’essais.

En 1974, le président Pompidou décide du lancement de ce projet plutôt que de celui de l’aérotrain (moteur thermique), et le premier ministre Pierre Messmer décide le 5 mars d’engager la construction d’une première ligne entre Paris et Lyon, la LGV Sud-Est (LN1).

Le projet est entièrement financé par la SNCF, essentiellement par emprunts sur le marché international en yens et en dollars US, très fluctuants par rapport au franc. Cela entraîne une explosion de l’endettement de la société nationale. Cet endettement conduira avec d’autres motifs (directives européennes notamment) à la réforme de 1997 qui verra la création de RFF qui devient, dès lors, le maître d’ouvrage de toute nouvelle ligne intérieure (la SNCF n’étant impliquée que dans les prévisions de trafic et de dessertes).

Une campagne d’essais est menée avec deux rames de présérie, appelées en interne Patrick et Sophie (initiales de PSE : Paris Sud Est), livrées en 1978. La première commande fut livrée à partir du 25 avril 1980. Le service TGV ouvrit au public entre Paris et Lyon le 27 septembre 1981. La France entre ainsi dans l'Histoire ferroviaire à grande vitesse après le Japon avec son Shinkansen en service depuis 1964. Entre septembre 1981 et septembre 1983, seule la partie Sud de la LGV Sud-Est (entre Saint-Florentin et Sathonay) est utilisée pour la grande vitesse. La partie Nord du trajet s’effectue encore sur les anciennes voies PLM. C’est seulement à partir du service d’hiver 1983 que la LGV est ouverte en intégralité entre Lieusaint et Sathonay.

Le temps de parcours considérablement réduit par rapport à la situation antérieure (grâce aussi au tracé plus direct de la ligne, qui ramenait la distance entre les deux villes de ) permit au rail d’acquérir de nouvelles parts de marché au détriment de l’automobile et surtout de l’avion.

L’innovation était non seulement technique, mais aussi commerciale avec la réservation obligatoire, .

C’est Jacques Cooper qui a dessiné les prototypes, la livrée et les matériels de série du TGV Sud-Est. Il est également à l’origine de la silhouette du TGV Atlantique, dont le design (principalement intérieur) sera finalisé par Roger Tallon. Ce dernier a ensuite dessiné le Duplex entre 1988 et 1998.

Le , la SNCF dévoile à la gare de Lyon la nouvelle livrée du TGV. Il arbore désormais les couleurs violettes et grises, rappelant les uniformes du personnel dessinés par Christian Lacroix. Tous les TGV seront restaurés dans cette nouvelle livrée parallèlement à l’acquisition de nouvelles rames.

Le 26 février 1981, la rame TGV obtient un premier record de vitesse sur la LGV Sud-Est à . Ce record de vitesse, pour lequel des dizaines de journalistes avaient été conviés, n’avait pas pour but d’ajouter un nouveau record au palmarès de la SNCF, mais plutôt de rassurer les futurs voyageurs, en montrant que les auxquels ils seraient bientôt transportés pouvaient être atteints en toute sécurité. Le deuxième but était de démontrer l’élargissement possible du transport ferroviaire par rapport à l’aérien, tout en conservant confort, sécurité et rapidité, ainsi que sa faible consommation d'énergie.

Après qu’une rame d’essais allemande Intercity-Express (ICE-V) de la Deutsche Bahn (DB) eut roulé à sur la LGV Hanovre – Wurtzbourg le mai 1988, la SNCF battit officieusement ce record le 12 décembre 1988 en faisant rouler la rame TGV PSE 88 à sur la LGV Sud-Est au cours d’essais de la chaîne de traction synchrone des futurs TGV Atlantique.

Le 18 mai 1990, le TGV a obtenu le record du monde de vitesse sur rail, à sur la LGV Atlantique, avec une rame d’essai Atlantique numérotée 325 raccourcie à trois caisses intermédiaires au lieu de dix.

Le 3 avril 2007 sur la LGV Est européenne, la SNCF bat son propre record en faisant rouler la rame d’essais 4402 à la vitesse de (soit ). Cette vitesse correspond à Mach 0,47 (pour une température de ). Ce record s’inscrivait dans le cadre du programme « V150 » visant à dépasser la vitesse de (soit ). Une rame Duplex composée de trois remorques uniquement avait été spécialement préparée et sa puissance (, soit environ ) avait été doublée par rapport au TGV classique (, soit environ ). Elle possédait des roues d’un diamètre plus important et la voiture au centre de la rame était équipée de bogies motorisés de la future AGV. Pour le reste, ce train était similaire à un train de série SNCF. Durant la phase d’essais, la vitesse de avait déjà été atteinte. Il s’agissait le 3 avril d’obtenir un record de vitesse homologué.

Le TGV a également établi un record de vitesse sur une longue distance, le 26 mai 2001 avec le trajet Calais - Marseille () en ( de moyenne), lors de l’inauguration de la LGV Méditerranée, avec la rame Réseau 531 (Opération sardine).

Il a également établi le 17 mai 2006 le « record du monde de la plus longue distance parcourue sans arrêt par un train de voyageurs », en transportant dans une rame TGV TMST d’Eurostar l’équipe du film "Da Vinci Code" de Londres à Cannes, soit en (soit une moyenne de ).

Le sigle "TGV" signifie à l'origine « "Turbotrain à Grande Vitesse" », dans la dénomination du premier turbotrain expérimental . Il s'agit de marquer une nouvelle étape, en dépassant les pratiqués occasionnellement sur le réseau classique français depuis 1967. L'usage courant de ce sigle pour désigner cette rame a fait évoluer sa signification pour « Train à Grande Vitesse ».

« TGV » est désormais une marque déposée de la SNCF. Le troisième logo de la marque TGV a un aspect métallisé, qui évoque la fluidité, la vitesse et la puissance du train à grande vitesse. Par détournement, le logo inversé peut ironiquement évoquer un escargot, la vitesse du TGV étant ainsi mise en opposition à ce symbole de lenteur.

À partir de , la SNCF choisit de ne plus utiliser de logo spécifique pour la marque TGV (ainsi que ), comme pour les trains de types Transilien et Intercités, pour identifier ses services ferroviaires. Le troisième logo, métallisé, est encore utilisé sur les trains , et , en .

À partir de , certains TGV sont désignés par le nom commercial « », à commencer par la liaison Paris – Bordeaux.

Le TGV ne suffit pas à voyager à grande vitesse, il faut également construire un réseau de voies ferrées, ainsi que des nouvelles gares, pour l'exploiter. Le TGV ne peut en effet atteindre ses vitesses maximales que sur une ligne à grande vitesse. Il peut cependant emprunter les autres voies, à la vitesse maximale autorisée par ces lignes, jusqu’à selon le tracé et le type de signalisation.

La particularité des rames TGV, par exemple par rapport aux matériels ICE de Siemens et ETR de Fiat Ferroviaria, est qu’elles sont constituées de deux motrices à deux bogies encadrant un tronçon articulé composé de remorques dont les bogies intermédiaires sont communs à deux caisses adjacentes (principe du bogie Jacobs). L’articulation entre deux caisses fait appel à un dispositif original qui permet de solidariser avec amortissement les deux caisses. Cette disposition présente plusieurs avantages :

Elle présente cependant l’inconvénient de faire reposer le poids de chaque caisse sur seulement deux essieux. La limite de de charge à l’essieu a donc été l’une des difficultés à résoudre lors de la conception du TGV Duplex. L’impossibilité de séparer facilement les remorques du tronçon central nécessite aussi des installations de levage capables de soulever une rame entière d'environ dans les ateliers d’entretien.

Deux rames peuvent être couplées en unité multiple ce qui permet de doubler la capacité offerte par un train qui atteint alors la longueur maximale de normalisée par UIC ou pour le TGV Atlantique.

Roulant sur des voies ferrées classiques, le TGV est parfaitement compatible, à vitesse adaptée, avec les installations existantes, dès lors qu’elles sont électrifiées. De 2000 à 2004 le « TGV Vendée » était tracté par une locomotive diesel CC 72000 entre Nantes et Les Sables-d'Olonne.

Les premiers TGV fonctionnaient à l’aide de moteurs à courant continu alimentés par des redresseurs.

À la fin des années 1980, le développement de l’électronique de puissance a permis de substituer le moteur synchrone au moteur à courant continu. Ces moteurs sont d’abord alimentés à l’aide d’onduleurs de courant à thyristor (1988). À cette époque l’électronique de puissance nécessaire à leur alimentation est beaucoup plus simple que celle requise par les moteurs asynchrones.

L’utilisation de moteurs synchrones présentait plusieurs avantages :

Néanmoins, le moteur synchrone est plus coûteux et nécessite plus d’entretien que le moteur asynchrone. Avec les progrès de l’électronique de puissance (onduleur de tension à IGBT), ce dernier va supplanter le moteur synchrone dès le milieu des années 1990 dans quasiment tous les domaines de la traction. Ce type de motorisation est installé sur les motrices de l’Eurostar, avec des thyristors GTO, et maintenant sur le TGV POS, avec des IGBT.


L'introduction du moteur synchrone à aimant permanent alimenté par IGBT sur l'AGV marque une nouvelle étape, avec pour (soit /kg).

Le TGV Atlantique a inauguré la gestion de la rame par ordinateurs reliés en réseau. Baptisé TORNAD (), le système est composé de (ordinateurs).

Pour les générations suivantes de TGV : TGV Réseau, TGV Duplex et TGV POS, les ordinateurs sont reliés entre eux par le réseau TORNAD. Il s’agit d’un réseau de type (802.4).

Environ (août 2010) de type TGV sont en service dans le monde, la grande majorité étant exploitée par la SNCF ou ses filiales internationales qui gèrent un parc de TGV (septembre 2010) dont pour la SNCF en propre (septembre 2010).

La rame prototype TGV 001 était motorisée par quatre turbines à gaz pour hélicoptère. Le sigle TGV signifiait alors turbotrain à grande vitesse. Les quatre turbines à gaz entrainaient des alternateurs fournissant la puissance électrique aux moteurs de traction accouplés aux roues. Ce prototype était d'une certaine manière l'héritier des rames ETG (élément à turbine à gaz) qui circulèrent en Normandie dès 1970 et aussi une réponse au projet d'aérotrain qui utilisait aussi des turbines (actionnant une hélice). Parallèlement la SNCF menait des essais de rame automotrice électrique avec le « Zébulon », surnom de la rame Z 7001. Le choix de l'électrification fut fait en mars 1974 lors d'un conseil ministériel consacré aux économies d'énergie. Il fallut aussi, entre autres, mettre au point des pantographes aptes à la grande vitesse.

Construits entre 1978 et 1985, les TGV Sud-Est sont destinés à l'origine à la LGV Sud-Est à son ouverture. En 1990, ces trains sont numérotés de 01 à 118 dans la série TGV, et sont limités à à l'origine, rénovés pour atteindre dans les années 2000. Les TGV SE sont les anciennes rames orange qui ont été repeintes en bleu et en argent. Les TGV SE sont les TGV Lyria (toujours limités à ), les TGV nord (Lille-Paris, Valencienne-Paris...) et les TGV SE (Metz-Nice) (les deux derniers étant limités à ).

Les TGV SE se composent de deux motrices encadrant huit voitures. Les deux voitures d'extrémité possèdent chacune un bogie moteur.

Toutes les rames sont bicourant ( continu français et alternatif français), cependant, quelques rames (lignes de cœur ou maintenant "Lyria") sont tritension et peuvent circuler en Suisse ( alternatif en ).

Les TGV SE ne peuvent être couplés qu'avec d'autres TGV SE (de 01 à 102)

Certaines rames sont retirées du service depuis 2012 (Rames dites "Lyria" tritension numérotées de 110 à 118 ).

Depuis le 14 décembre 2014, les TGV SE n'assurent plus les relations comme Paris – Marseille – Toulon – Nice et Paris - Nîmes – Montpellier, en raison d'une demande de passagers supérieure à leur capacité. Ainsi, les TGV Duplex assurent entièrement ces relations.

Il s'agit de TGV SE décorés en jaune La Poste et aménagés pour le transport du courrier. Ils circulent sur le réseau ferré à . Chaque nuit, ce train postal à grande vitesse transporte environ de courrier, colis et presse en liasse (journaux...) entre l’Île-de-France, la Bourgogne, Rhône-Alpes et Provence-Alpes-Côte d’Azur.

Tout comme les TGV SE, ces TGV postaux sont bicourant et couplables uniquement avec d'autres TGV SE (bien que cela soit rare, mais leurs motrices compatibles peuvent parfois remplacer une motrice sur une rame classique TGV SE).

Ces rames ont toutes été retirées du service en 2015.

Construits entre 1988 et 1992, les TGV Atlantique sont destinés à l'origine à la LGV Atlantique à son ouverture. En 2010, ces trains sont numérotés de 301 à 405 dans la série TGV, et sont aptes à . Entre 2005 et 2009 l'intérieur a été rénové selon le design de Christian Lacroix.

Ils sont bicourant et se composent de dix remorques non motorisées encadrées par deux motrices.
Ils ne sont compatibles qu'avec d'autres TGV A. Ils peuvent circuler en dehors du réseau Atlantique sous réserve de compatibilité technique, en raison de la longueur des rames supérieure aux autres TGV d'une part ( contre 200), et selon leur système de signalisation embarqué d'autre part (TVM300 ou TVM430).

La mise à la retraite de ces rames a commencé en 2014.

Construits entre 1992 et 1996, les TGV Réseau sont destinés à l'origine à la LGV Nord à son ouverture. Il existe une version tricourant capable de rouler sous tension de pour assurer les relations entre Bruxelles et des destinations françaises hors Paris, et Paris-Milan, sur lesquelles ils sont toujours en service. En 2007 les TGV Réseau ont été rénovés selon le design de Christian Lacroix en vue de leur changement de service (TGV Est).

C'est la première génération de TGV à utiliser la TVM 430.

Les trains bicourant ont beaucoup assuré les relations intersecteurs et Nord avant de tous être regroupés au technicentre est-européen pour assurer les services nationaux sur la LGV Est.

En 2011, ces trains sont numérotés de 501 à 553 (bicourant) et de 4501 à 4540 (tricourant) dans la série TGV, et sont tous aptes à .

Depuis 2013, les TGV Réseau perdent leur livrée extérieure d'origine bleu et gris métallisé, au profit de la nouvelle livrée Carmillon.

La mise à la retraite de ces rames a également commencé à partir de 2015.

Construits entre 1993 et 1996, les TGV TMST sont destinés à l'origine à emprunter le tunnel sous la Manche à son ouverture. On les connaît plus sous leur nom commercial d"'Eurostar". Le TGV TMST peut circuler en service commercial à la vitesse de . Son alimentation électrique était à l'origine compatible avec trois types de courants : en courant continu délivré par (supprimé en 2008), en courant continu délivré par caténaire, et en courant alternatif à délivré par caténaire. Il est moins large que les autres types de TGV, pour se conformer au gabarit britannique.
Quelques TGV TMST ont été achetés par la SNCF pour être utilisés uniquement en TGV Nord.

Construits dans les années 1990, les TGV PBKA sont destinés à l'origine à circuler entre Paris et Cologne. Ce sont les Thalys. Ils circulent maintenant en Belgique, en Allemagne et aux Pays-Bas en passant par la LGV Nord.

Le TGV PBKA peut circuler en service commercial à la vitesse de . Son alimentation électrique est compatible avec quatre types de courants délivrés par caténaire : ou en courant continu, en courant alternatif à , et en courant alternatif à .

Le TGV PBKA est compatible avec les éléments de sécurité ferroviaire suivants : KVB et TVM (utilisés en France), ATB (utilisé aux Pays-Bas), TBL (utilisé en Belgique), PZB et LZB (utilisés en Allemagne), et enfin ERTMS de niveau 2 (système européen de contrôle des trains) depuis la rénovation du matériel effectuée en 2009.

Ce TGV a été commandé par la SNCF pour répondre à l'augmentation du trafic sur le réseau Sud-Est où on le voit le plus souvent. Il y a eu trois générations de TGV Duplex (Duplex, Dasye et Euroduplex).

Les TGV duplex sont couplables avec les TGV réseau, dont ils reprennent la chaîne de traction (pour la première génération), et les TGV POS (même chaîne de traction que les Dasye).

Cette première génération a été construite entre 1995 et 2006.

Ces rames ont la particularité d'être constituées de motrices issues des TGV Réseau et de remorques à deux niveaux identiques à celles des premiers TGV Duplex.

Seconde génération de TGV Duplex, il a été construit entre 2008 et 2012.

Les rames Ouigo () sont composées de voitures Duplex (de première génération, rénovées et modifiées) et de motrices de type Dasye.

En fabrication depuis 2011, ce TGV peut être considéré comme la troisième génération de Duplex. Cette série a été commandée par la SNCF pour des liaisons vers l'Allemagne et la Suisse (rames tricourant) et pour faire face à l'augmentation du trafic due à l'ouverture de la LGV Rhin-Rhône. Depuis décembre 2011, les premiers sont en exploitation, principalement sur la nouvelle ligne Rhin-Rhône. Les rames bicourant, construites en 2012, sont numérotées à partir de 800 dans la série TGV. Elles sont limitées à . Leur système ERTMS leur permet d'assurer les missions vers l'Espagne, en support des rames Dasye. Les rames numérotées de 4702 à 4730 () RGV 2N2-3UA (3 tensions - Allemagne) dans la série circulent sur la ligne TGV Est. Les rames numérotées de 801 à 810 () RGV 2N2-3UH ( - Hispanisées). Les rames numérotées de 811 à 825 () RGV 2N2-3UF (3 tensions - France).

Les rames numérotées de 836 à 865 et 867 à 891, commercialement appelées "L'Océane" () RGV 2N2-3UFC, ( - France) circulent progressivement sur la LGV SEA, vers la Nouvelle-Aquitaine (Bordeaux et Hendaye) et l'Occitanie (Tarbes et Toulouse) depuis l'ouverture de la ligne le 2 juillet 2017. Les premières rames de cette sous-série ont été mises en service le 11 décembre 2016 entre Paris et Toulouse.

Construits dans les années 2000, les TGV POS sont destinés à l'origine à la LGV Est européenne à son ouverture. En 2010, ces trains sont numérotés de 4401 à 4419 dans la série TGV. En 2012, la SNCF a cédé la totalité du parc au réseau TGV Lyria, compagnie ferroviaire privée suisse. Lors de cette cession les TGV POS sont passés de la livrée Lacroix à Lyria.

Le TGV POS peut circuler en service commercial à la vitesse de . Son alimentation électrique est compatible avec trois types de courants : en courant continu délivré par caténaire, en courant alternatif à délivré par caténaire, et en courant alternatif à délivré par caténaire.

Le TGV POS est compatible avec les éléments de sécurité ferroviaire suivants : KVB et TVM (utilisés en France), PZB et LZB en Allemagne, ZUB en Suisse, et enfin ERTMS de niveau 2 (système européen de contrôle des trains).

Les TGV POS sont l'association de motrices extérieurement similaires à celles des TGV Duplex et de voitures de TGV Réseau.

Construit par le canadien Bombardier Transport et motorisé par Alstom sur le même principe que le TGV, l'Acela est un train pendulaire mis en œuvre par Amtrak et circulant sur le « Corridor Nord-Est » ("Northeast Corridor") entre les villes de Boston dans le Massachusetts et Washington (district de Columbia). Contrairement aux TGV qui circulent sur une ligne à grande vitesse, l'Acela circule sur une ligne classique modernisée, ce qui ne lui permet d'atteindre que des vitesses commerciales de l'ordre de .

Le S-100 (pour Série 100, nom d'exploitation) est un train à grande vitesse construit dans les années 1990 par Alstom (qui s'appelait à l'époque Alsthom) et qui circule en Espagne pour la Renfe.

Ce matériel est dérivé du TGV Atlantique, avec un transfert partiel de technologie à la Corée du Sud.


Le premier service TGV entre Paris et Lyon a été lancé en 1981, utilisant une partie de la « LGV Sud-Est » encore partiellement en construction. Dès l’origine le service TGV continuait par voie classique vers Marseille et vers Montpellier en desservant les gares intermédiaires. Depuis, le réseau TGV centré sur Paris s’est étendu jusqu’à relier désormais de nombreuses villes françaises, en partie grâce à la construction de lignes nouvelles à grande vitesse vers le sud, l’ouest, le nord et l’est de la France depuis le 10 juin 2007.

Le kilométrage relativement faible de lignes nouvelles ( environ en décembre 2011) fait que beaucoup de dessertes en TGV empruntent les lignes nouvelles sur une partie de leurs parcours et continuent sur le réseau classique sur des distances parfois assez longues, y compris dans les pays voisins de la France. Le TGV dessert ainsi l’Allemagne, la Belgique et les Pays-Bas (sous la marque Thalys ou TGV pour la province française au départ de Bruxelles), le Royaume-Uni sous la marque Eurostar, l’Italie sous la marque Artesia, l'Espagne, mais aussi la Suisse sous la marque Lyria, qui dessert Genève depuis 1981, Lausanne depuis 1984, Berne depuis 1987, Zurich et Bâle depuis 2007. Ces dessertes sont assurées par la SNCF dans le cadre de partenariats internationaux pour lesquels des structures "ad hoc" ont été créées sous formes de sociétés ou GEIE.

La plupart des liaisons TGV partent de Paris ou y arrivent, mais d'autres évitent Paris intra-muros en empruntant la ligne d’interconnexion à l’est de la capitale ou la Grande Ceinture au sud, ou ne desservent pas l’Île-de-France. Les TGV desservent plus de en France, dont huit en Île-de-France et plus d’une trentaine dans les pays limitrophes.

Comme les TER de nombreuses régions, les TGV Paris – Bruxelles, Paris – Lille, Paris – Nantes et Paris – Lyon ont des horaires cadencés (départs et arrivées aux mêmes minutes de chaque heure pendant toute la journée).

Comme le Minitel, la fusée Ariane, le Concorde, le nucléaire civil ou Airbus par exemple, le TGV fait partie des « grands projets » qui associent fortement l’État français à un « "champion national "». Ces entreprises, domaine d'excellence en France des grand corps d’ingénieur, sont caractérisées par leur intégration verticale ; la R&D y est notamment fortement internalisée et les partenariats avec des universités ou des bureaux d’étude indépendants sont rarissimes.

Ces technologies sont aussi des vitrines de démonstration de savoir-faire exportables. Par exemple pour Jacques Chirac en 2007, alors président de la République française : 

Les TGV ne s'arrêtent plus aux frontières. Il existe une association "« Villes et Régions européennes de la Grande Vitesse »" (VREGV), présidée (en 2010) par Bernard Soulage (vice-président de la région Rhône-Alpes et député européen).

Le TGV constitue un facteur de "déséquilibre" partiel parce qu'il dessert moins de gares et ne relie que de grands pôles, ce qui désavantage les territoires ruraux. Il entraîne par ailleurs une hausse de prix de 30 % en moyenne par rapport aux liaisons classiques qu’il remplace.

Après la mise en service des premières LGV (vers Lyon et vers Rennes/Bordeaux) une desserte par la ligne classique avait été maintenue. Elle a été supprimée par la suite par manque de fréquentation. Contrairement au système allemand, il n’existe généralement pas d'alternative ferroviaire commode au TGV sur les liaisons qu'il dessert. Comme la plupart des moyens de transports à grande distance, le TGV est surtout utilisé par les professionnels (30 % des voyages sont professionnels) et les ménages aisés, pour lesquels le TGV remplace bien souvent l’avion. Les cadres et leurs familles représentent 56 % des voyageurs alors qu’ils ne forment que 28 % de la population française.

La SNCF estime cependant que ces trains à grande vitesse sont en moyenne 30 % moins chers que leurs équivalents en Espagne ou en Allemagne ; et ils seraient approximativement 50 % moins chers que le Shinkansen japonais néanmoins, le Shinkansen offre un haut niveau de services et de confort à ses passagers.

La SNCF développe néanmoins à partir du printemps 2013 une offre à bas coût, sous l'appellation "Ouigo".

L’aménagement intérieur, en classe, est d’un certain niveau . Cependant, dès l’origine, il n'y a pas de voiture-restaurant, puis la restauration à la place est supprimée progressivement et les oreillers repose-tête disparaissent, tandis que les services et le confort proposés aux voyageurs sont jugés modestes par rapport aux défunts Trans-Europ-Express ou à certains trains à grande vitesse étrangers, notamment l’Acela Express, le Shinkansen, l’ICE ou le KTX coréen. Sous la pression de la concurrence en cours, cette situation est en train d’évoluer avec l’adoption d’un nouveau décor luxueux conçu par le grand couturier Christian Lacroix et l’instauration d’un véritable service à bord sur les grandes liaisons : Eurostar, Lyria, TGV "Pro". Enfin, le retour d’une restauration à la place généralisée est de nouveau envisagé.
Des voyageurs trouvent le TGV moins confortable en classe que les rames Corail qu’il a remplacées. Les principaux problèmes évoqués sont la diminution du « pas » (distance entre deux sièges, cette dernière est résolue sur l’AGV par un plancher un peu plus bas et donc plus large), un rembourrage des sièges moins épais et l’obligation de réservation, jugée contraignante, notamment pour les trajets très courts. Le placement de certains sièges à hauteur des trumeaux (montants de fenêtre) produit des places aveugles où les passagers n’ont presque aucune vue sur l’extérieur.

Le confort acoustique est généralement très bon. Le niveau de bruit moyen, à , peut être aussi bas que aux meilleures places (salle hautes, duplex, classe) notamment du fait de l'éloignement des sources de bruit solidien (équipements techniques, essieux, bogies et suspension), contre plus de à bord de certains "vieux" Shinkansen (anciennes générations avant la mise en service depuis des années des rames N700, N700A, "Hayabusa" E5 et rames E7 pour ne citer que celles-ci), et près de à bord d'un avion de ligne.

En cas de panne interdisant la traction, les rames sont secourues par des locomotives diesel BB 67200 en unités multiples munies d'un attelage spécial. Ce sont d’anciennes BB 67000 spécialement modifiées pour pouvoir circuler et remorquer les rames TGV sur LGV. Elles assurent également la traction des trains de travaux pour l’entretien des LGV.

L’entretien du parc TGV, pour les opérations de maintenance régulière, a nécessité l’aménagement d’ateliers situés à proximité des gares têtes de lignes. C’est ainsi que les rames TGV Sud-Est étaient entretenues aux ateliers de Villeneuve-Saint-Georges et Paris-Conflans, celles du TGV Atlantique aux ateliers de Châtillon et celles du TGV Nord aux ateliers du Landy à Saint-Denis. Depuis, avec l’apparition du TGV Duplex (TGV à deux niveaux), le parc a été redéployé, et les ateliers du Landy entretiennent des rames TGV Sud-Est, tandis que les ateliers de Paris Conflans s'occupent d'autres rames TGV Sud-Est, mais également de rames Réseau et Duplex.

La répartition du parc entre les ateliers était au la suivante :

Le Technicentre de Lyon-Gerland inauguré le 31 mars 2009 est le seul centre TGV en province. Au départ il était prévu Duplex d’ici fin 2009 (60 horizon 2011 ; à l’ouverture de la LGV Rhin-Rhône), pour l’entretien de TGV Duplex de la ligne Paris-Lyon et du futur TGV Rhin-Rhône.

Depuis début 2010, la maintenance des rames Duplex est mutualisée entre les Technicentre Sud Est Européen, Technicentre Atlantique et Technicentre de Lyon Gerland.

De ce fait, l’autocollant apposé sur le devant des motrices, indiquant l’appartenance au Technicentre (par exemple : « Paris Sud-Est » pour le TSEE et « Châtillon » pour le TA) est maintenant remplacé par la mention « rame à grande vitesse entretenue par les technicentres SNCF ».

Grandes révisions périodiques :


Les TGV s’y rendent en circulation non commerciale (ou « en W », pour vide voyageurs, dans le jargon cheminot). À noter que Romilly est située sur une ligne non électrifiée et que les TGV s'y rendent remorqués par des locomotives diesel.

Entretien courant en Europe :


Le TGV a fêté son premier milliard de voyageurs transportés depuis l’inauguration du premier service en septembre 1981, le 28 novembre 2003. Son deuxième milliard est atteint le 25 janvier 2013.

En 2004, le TGV a transporté de voyageurs en France (cette valeur ne concerne que le trafic national, les dessertes internationales étant gérées par des sociétés spécifiques).

Ce trafic est en croissance de 4 %, soit de voyageurs supplémentaires par rapport à l’année précédente. Le chiffre d’affaires correspondant s’élève à d’euros, en progression de 7,3 %.

En fin d’année 2004, sa part de marché (par rapport à l’avion) s’établit à 68 % sur la ligne Paris-Marseille, et à 66 % sur Paris-Bordeaux.

Il y a environ de TGV parmi les de la SNCF.

Après d’exploitation, la SNCF annonce le 9 juillet 2007 avoir transporté un million de personnes sur le TGV Est, soit en moyenne par jour avec quotidiennes.

Les taux d’occupation des TGV Est sont très bons (en moyenne de 88 % en seconde classe et 75 % en classe) mais le taux de régularité des TGV Est n’est en moyenne que de 86 %. À titre de comparaison la régularité 2006 moyenne de l’Eurostar était de 91,5 % et de 92 % au premier semestre 2007 (communiqué de presse Eurostar). Ces comparaisons brutes sont toutefois peu significatives en elles-mêmes, car elles éludent 1) le fait que les TGV circulent pour une bonne partie sur ligne classique, avec les aléas que cela comporte (contrairement aux Eurostar ou Shinkansen) ; 2) que les retards ont fréquemment des causes liées à l'infrastructure (signalisation, alimentation électrique) et à la circulation d'autres trains.

En 2013, le nombre de passager diminue pour la deuxième fois consécutive. La marge opérationnelle est en retrait et le modèle est confronté à la crise économique et à la concurrence Ouigo.

En trente ans, le TGV a connu plusieurs accidents dont quatre déraillements à grande vitesse. Le TGV a causé la mort de onze personnes (neuf employés et deux passagères) présentes à bord d'une rame d'essai (non commerciale) qui effectuait des essais de vitesse sur le nouveau tronçon de la LGV Est européenne au nord de Strasbourg, en raison de son déraillement le . Deux minutes avant de dérailler, le TGV allait à , c'est-à-dire au dessus de la vitesse de pour laquelle il était homologué.

Cependant, l’exploitation sur LGV ne représente que 25 % environ des trains-kilomètres réalisés par les TGV. Sur le réseau classique, le TGV est confronté aux mêmes aléas que les autres trains et plusieurs accidents, impliquant notamment des collisions avec des véhicules routiers à des passages à niveaux, ont causé la mort de personnes extérieures au train. De plus, deux attentats mortels se sont produits sur une ligne classique, en 1983 et en 1986.










</doc>
<doc id="3018" url="https://fr.wikipedia.org/wiki?curid=3018" title="Train">
Train

Le train est un matériel roulant ferroviaire assurant le transport de personnes ou de marchandises sur une ligne de chemin de fer. Par extension, on appelle train le service que constitue chacun de ces transports, réguliers ou non. Le train est un mode de transport, s'effectuant sur voie ferrée. 

Étymologiquement parlant, le mot train désigne une rame de wagons de marchandises ou de voitures de passagers tractée par au moins une locomotive, par opposition aux rames automotrices (catégorie dont fait partie le TGV) ou autorails qui assurent leur propre propulsion. Cependant, dans l'usage courant, le mot train désigne n'importe quelle circulation ferroviaire, quelle que soit sa composition, depuis le plus simple autorail local jusqu'aux longs trains de grandes lignes ou de transports industriels.

Un train se compose de plusieurs éléments dont au moins un véhicule moteur (locomotive, locotracteur, rame automotrice) assurant la traction de la rame, accompagné de n'importe quelle combinaison, inclusive et exclusive, de voitures pour le transport de personnes, de fourgons assurant différents services comme le transport de colis ou de bagages, et de wagons pour le transport de marchandises. Il peut s'agir également d'engins spécialisés pour l'entretien des voies (trains de travaux).

Pour la traction, la locomotive à vapeur, omniprésente au , laisse une place à la locomotive électrique dès le début du , puis à l'autorail à partir des années 1930 pour s'effacer finalement avant la fin du devant la locomotive électrique ou Diesel sur les lignes non encore électrifiées. D'autres modes de traction marginaux ont été (et sont parfois encore) utilisés : animaux (chevaux, bœufs), câbles, cordes et cabestans, gravité, pneumatique, turbines à gaz, etc. Aujourd'hui, pour le transport de passagers, les rames remorquées cèdent régulièrement du terrain devant les rames automotrices qui composent désormais aussi bien des trains de banlieue que des trains à grande vitesse.

Le terme « train », écrit "traïn" ou "trahin" en ancien français est le déverbal du verbe « traïner », à l'origine de notre verbe traîner ; ce premier emploi correspond en particulier à toute la gamme des divers systèmes de traînes sur surface de sol (traîneau), dans l'eau (filet) ou sur l'eau (flottage, halage, touage). Bien avant l'invention de la voie ferrée, on appelait « train » une file de chevaux de bât, une suite ordonnée d'hommes et de bêtes de charge accompagnant une personnalité éminente en déplacement, un convoi de bateaux rendus solidaires, pour mettre en commun les équipages et parfois l'énergie du vent. Dans ce dernier registre, le plus grand bateau, portant la plus haute voile, en tête, servait de « locomotive ». Cette pratique fut énormément utilisée sur la Loire, pour la remontée de Nantes à Orléans, voire plus en amont si les conditions le permettaient. On appelait « train » également les longs radeaux formés par de planches, de troncs attachés entre eux, dans le but d'en faire une embarcation suffisamment solide pour descendre ainsi les bois des montagnes jusqu'aux grandes villes par flottage, comme cela s'est pratiqué sur l'Yonne et ses affluents (comme la Cure), ou même ses sous-affluents, du au .

Pourtant le mot « train », dans le vocabulaire de la langue française et spécifique du chemin de fer et de la traction à vapeur, provient bien du mot anglais ', à la prononciation différente et concernant l'industrie britannique naissante des transports par rails et locomotive à vapeur. Les adaptations-traductions, en particulier celles de l'inspecteur divisionnaire des Ponts et chaussées Joseph Cordier dans ses "Considérations sur les chemins de fer" parues en 1826 ou du livre ' de Nicolas Wood traduit en 1835 par l'ingénieur des Ponts et Chaussées Franquet de Franqueville, ne comportaient pas de détail phonétique. 

Lorsque la technique du chemin de fer fut importée, par exemple après 1827 pour les rares trains transportant les produits pondéreux des industries minières ou charbonnières ou après 1836 pour les voyageurs et/ou les marchandises, d'abord sur quelques lignes à Saint-Étienne, au Pecq près de Saint-Germain ou à Mulhouse, le mot français le plus semblable se calque et se surimpose sur celui-ci, avec un certain nombre d'autres expressions communes du monde du transport terrestre, maritime et fluvial. Par exemple, les premières gares étaient nommées « embarcadères ».

Plus tard, linguistes et érudits, à l'instar de Pierre Larousse, avalisent tacitement ce choix, arguant que le mot ancien français avait bien franchi la Manche avant de s'intégrer au lexique technique anglo-saxon. Le train, terminologie des chemins de fer, provient du verbe traîner, comme l'indique le Grand Larousse de 1923. Notons que quelques siècles se sont écoulés avant l'apparition de ce terme technique dans les mines et les ports britanniques au début des Temps modernes: l'ancien français, langue migratrice aux , était alors représenté soit par une langue véhiculaire, celle parlée ou écrite par les élites françaises seigneuriales, soit par les multiples dialectes de l'Ouest, en particulier de Normandie, des hommes de métiers ou de services, appelés par les premières mais se mêlant plus facilement aux populations locales. C'est plutôt par ce dernier biais que les Anglo-saxons ont emprunté le terme. 

Trois arguments simples peuvent justifier l'évolution moderne en France par emprunt et calque-superposition : la prononciation française "a posteriori", la préservation intégrale du sens anglais, le mot anglais "" lui-même. D'autre part, selon François Crouzet, il n'existe en France avant 1838 que des locomotives importées de Grande-Bretagne. Ce n'est qu'en 1840 que débute une production locale concurrentielle.

Il faut signaler que, avec le temps, certains usages bien antérieurs du terme français, train, ont pâti de l'irruption de ce sens spécifique. Ainsi le train de charronnage, le train d'animaux de trait, le train d'attelage, le train d'artillerie, le "train des équipages", le train de flottage, le train d'engrenages ne sont bien souvent plus compris ou pire, parfois assimilé fautivement et univoquement au dernier vocable du train sur chemin de fer. Certains érudits nostalgiques de la création du régiment du train à l'époque napoléonienne citent comme origine un spirituel acronyme TRAIN signifiant « Transport et Ravitaillement de l'Armée Impériale de Napoléon ». Cette étymologie est fantaisiste au niveau historique : aucun service de l'armée impériale n'était nommé ainsi.

C'est le , près de Merthyr Tydfil, au pays de Galles, qu'a lieu la première circulation sur des rails d'une locomotive à vapeur, construite par Richard Trevithick, en Angleterre. 

Mais des convois formant un train ont été signalés bien auparavant. La première utilisation attestée de chariots sur rails (non motorisés) remonte à 1550, sous la forme de gravures montrant des wagonnets sur rail dans les mines de Leberthal en Alsace. On suppose que les Romains ont pu utiliser un système similaire à des voies ferrées, certaines de leurs routes étant dotées de deux ornières à écartement fixe, très proche de celui de notre voie actuelle.

La généralisation du système ferroviaire a été permise par la mise au point de la machine à vapeur, mais de nombreux systèmes alternatifs ont été utilisés au début, pour faire face au manque de puissance de celle-ci, ou pour s'adapter à des situations particulières, notamment la traction par chevaux, ou par câble, ainsi que l'utilisation de la simple gravité quand la pente le permettait. Ces méthodes à la fois lentes et hasardeuses ont rapidement pris fin avec la généralisation de la traction par locomotive et les progrès rapides de ces machines.

À partir de 1900 environ, l'apparition de moteurs électriques puissants et suffisamment compacts a permis l'apparition de la traction électrique, toujours utilisée à l'heure actuelle. Ce mode de traction nécessite cependant que la ligne sur laquelle le train circule soit équipée, soit d'une caténaire, soit d'un troisième rail, alimenté en électricité.

L'entre-deux-guerres verra l'apparition de locotracteurs diesel puis progressivement de locomotives diesel dans l'après guerre. Les années 1950 sont la charnière entre disparition de la traction à vapeur et développement des moteurs thermiques. C'est également à cette époque que l'on observe l'apparition de locomotives capables de fonctionner sous une tension alternative.

Les trains nécessitent une voie ferrée pour circuler. Elle se compose de rails posés sur des traverses à un écartement précis, elles-mêmes posées sur du ballast. La source d'énergie est, soit portée par le train lui-même comme dans le cas de la traction vapeur ou de la traction diesel, soit apportée par l'infrastructure sous forme de caténaire ou de troisième rail pour l'électricité. En général, les locomotives diesel sont diesel-électrique : un moteur diesel entraîne un alternateur qui produit de l'électricité pour faire tourner un moteur électrique qui entraîne les roues de la motrice

Le mode de roulement, qui est un contact roue/rail (acier sur acier) à adhérence réduite, donne un rapport entre puissance et charge tractée favorable mais réduit considérablement les déclivités admissibles pour la voie : 4 % est un maximum. Certains métros ont des roues munies de pneumatiques, à la suite des essais de Michelin dès les années 1930. Les premiers véhicules équipés ont été les fameuses michelines, sortes de petits autocars sur rail (le mot a été appliqué improprement aux autorails en général par le grand public). Par la suite, le train Paris-Strasbourg a disposé pendant plusieurs années de véhicules à pneus.

Le rayon de courbure des voies ne doit pas descendre en dessous d'une centaine de mètres. Ces deux contraintes fortes ont donc obligé les constructeurs à des prouesses, notamment en zone montagneuse, en réalisant de nombreux ouvrages d'art de génie civil comme des ponts, tunnels, viaducs, remblais, tranchées. Pour les pentes fortes, on a parfois recours au système de crémaillère.

À l'inverse d'un véhicule routier, un train ne peut pas changer d'itinéraire par lui-même. Il doit emprunter des appareils de voie, dont les plus connus sont les aiguillages, afin de passer d'une voie à une autre. Une contrainte forte d'exploitation est qu'un train ne peut en dépasser un autre qu'à des endroits particuliers d'une ligne, d'où une moindre souplesse dans l'organisation des circulations et la nécessité d'un suivi rigoureux des plans de marche.

Le terme « train » désigne plusieurs types de convois. Le plus connu consiste en une (éventuellement plusieurs) locomotive(s) et des véhicules ferroviaires, voitures ou wagons. Il peut aussi s'agir de plusieurs éléments autonomes constituant un train d'automoteurs. Il a aussi existé des trains simplement poussés à la main ou tirés par des chevaux.

Des types de trains tout à fait spéciaux nécessitent une voie adaptée, par exemple les chemins de fer atmosphériques, les monorails, les Maglevs et autres funiculaires.

Les trains de voyageurs sont constitués par des automotrices (ou autorails s'il s'agit de traction diesel) ou de rames tractées composées d'une ou plusieurs locomotives et une ou plusieurs voitures. Dans certains pays (France, Espagne, Allemagne, Corée, Japon…) il existe des trains à grande vitesse, composés de matériel spécifique et roulant principalement sur des lignes spécialement construites ou adaptées.

Les trains de voyageurs sont souvent adaptés aux distances à parcourir et à la période de transport. Ils peuvent intégrer des voitures destinées à la restauration, à la détente ou au sommeil des passagers. Ainsi, pour les voyages de nuit on peut emprunter un train de nuit, ou un service auto-train qui permet de faire transporter son véhicule avec soi. Ce service auto-train est également utilisé pour franchir des obstacles naturel (navette Eurotunnel pour franchir la Manche, transport d'automobiles accompagnées en Suisse pour la traversée des Alpes).

Pour les trajets autour d'une métropole, la compagnie exploitante fait circuler des trains de banlieue. Ceux-ci sont équipés pour pouvoir faire face au trafic des heures de pointe : nombreuses portes et places debout. Pour assurer les transports au cœur même des villes, on a recours au métro ou au tramway.

Sur des lignes à très faible trafic, on trouve aussi des trains mixtes voyageurs/marchandises, alors que sur les lignes à très fort trafic sont utilisées des voitures à deux niveaux, comme sur le réseau Transilien et certains TGV.

De nombreuses compagnies ont pour usage de classer leurs trains selon la distance parcourue et la desserte. On trouve ainsi souvent :

Un regain d'intérêt pour les trains d'autrefois fait se développer des trains touristiques, comme le Chemin de fer de la baie de Somme. Ces trains ont la particularité de servir à la promenade et non au réel transport de voyageurs.
Certains trains continuent à offrir un service quotidien, mais y est adjoint un service hebdomadaire, le samedi et/ou le dimanche, lors de certaines périodes (l'été) avec l'utilisation des anciennes machines à vapeur et wagons, avec en plus des animations. C'est le cas du Train des Pignes qui circule entre Nice et Digne, le train à vapeur, quant à lui, circulant entre Puget-Théniers et Annot.

Les trains de marchandises, appelés trains de fret en France, comprennent des wagons ou du matériel à voyageurs devant être acheminé dans la même direction. Il existe des types de wagons spécialisés en fonction de la marchandise à transporter, comme le wagon-citerne, le tombereau, le wagon couvert, plat, etc. De plus en en plus fréquemment, un train de marchandises est constitué de wagons de même type (train d'hydrocarbure, train de céréales, etc.). Si tous les wagons ont la même origine et la même destinations, on parle (en France) de train direct ou train-bloc, si la rame est composée de wagons variés ayant des destinations diverses, on parle de trafic « diffus » (en France). Un train postal, dans lequel éventuellement du personnel travaille au tri du courrier en cours de route (situation devenue rare de nos jours), appartient en France à la catégorie « train de fret ».

Le train de marchandises doit de plus en plus s'adapter aux contraintes de l'intermodalité des transports. Des trains transportant des conteneurs ou des remorques peuvent participer à une chaîne globale, combinée avec le transport maritime et le transport routier. 

En France, on appelle train de marchandises un train ayant une vitesse comprise entre 80 et , et train de messagerie un train circulant de 120 à (les trains dits MVGV étaient des trains de messageries de nuit utilisé sur les lignes à grande vitesse qui pouvaient circuler à 200 km/h ce qui en fait les trains de fret les plus rapides au monde).

La traction d'un train peut être assurée par plusieurs locomotives. On dira qu'elles sont en « unité multiple » si la commande est assurée depuis un seul poste de conduite et en « double traction » si un conducteur est nécessaire par machine. Lorsque d'autres machines sont attelées au convoi mais ne sont pas en marche, il s'agit de locomotives en "marchandise roulante" (France) ou "comme véhicule" (Belgique), ou si le train ne comporte que des locomotives : d'un train de machines. La longueur totale peut atteindre des valeurs importantes (par exemple 850 mètres pour , sur parcourus en 15 heures), afin d'obtenir un meilleur taux de rentabilité.

Dans d'autres pays, aux États-Unis par exemple, il est fréquent de trouver des trains en triple, quadruple, voire quintuple traction. Les locomotives complémentaires peuvent être ajoutées en queue de train ou même au milieu de la rame : cela permet d'accélérer le freinage des trains très longs et de diminuer les efforts sur les attelages.

Dans de nombreux pays, l'emploi de l'expression « unité multiple » (UM) est normalement réservé aux couplages de locomotives dirigés par un seul conducteur, les commandes étant transmises de la machine de tête aux suivantes par un câblage spécifique.

La double traction repose sur le même principe d'utilisation simultanée de plusieurs machines, mais dans ce cas-ci, il faut un conducteur par engin. En France, ces derniers se transmettent les indications par radio. En Belgique, des voyants sont installés à l'arrière de la locomotive de tête afin d'indiquer les différentes actions du conducteur de tête. On utilise ce système lorsque les machines ne sont pas compatibles.

En cas d'accident, on dispose de train de secours, constitué d'équipements de relevage et de voitures d'hébergement du personnel.
Dans certains pays, notamment en Suisse où il y en aurait une vingtaine, il existe également des trains de lutte contre le feu, qui ont pour mission d'intervenir en cas d'incendie ou d'accident sur tout le réseau et en particulier dans les tunnels ferroviaires.

Plus fréquemment, on peut être amené à rencontrer un train de travaux constitué soit d'un ou plusieurs engins moteurs et de wagons, soit du matériel automoteur spécifique aux différentes opérations de voie (bourreuse, régaleuse, dégarnisseuse, train-caténaires…) ; ils permettent l'entretien des voies et des ouvrages d'art, et aussi la construction des voies nouvelles.

Un train-laveur n'est pas considéré comme un train de travaux ; il circule sous le régime des marchandises avec une vitesse spécifique sur son parcours de travail.

Les draisines (automotrices légères servant à l'acheminement du personnel chargé de l'entretien des voies sur les chantiers) tirant ou non une ou plusieurs allèges (remorques légères plates servant au transport de l'outillage et du matériel léger) sont considérées comme un train si elles sont capables de fermer les circuits de voie.

Un véhicule ferroviaire isolé n'est pas considéré techniquement comme un train (mais peut l'être d'après la règlementation).

Elle a beaucoup varié selon les époques et les pays. 
En Europe à la fin du statistiquement, c'est dans le train qu'un voyageur risque le moins un accident, et notamment un accident mortel (Le risque en termes de 

Le thème du train est fortement présent dans la littérature, les arts plastiques (affiches, dessins, gravures, photographies…), le cinéma, les jeux vidéo, etc. C'est à la fois un monde clos propice aux intrigues (policier, espionnage, rencontres et drame sentimental) et un lieu qui voyage, propice à l'aventure et au suspense (passage de frontière, arrivée qui se rapproche). Un imaginaire fort est notamment lié à certains types de trains (trains de luxe internationaux de style Orient Express ou Transsibérien, train de nuit, etc.).

Le monde ferroviaire est particulièrement présent dans la littérature, et ce quasiment depuis son apparition: Victor Hugo, en 1837 en fait mention alors que les premières lignes ouvrent aux voyageurs. Honoré de Balzac, qui avait en 1838, au bout de sa propriété des Jardies, l'embarcadère du chemin de fer de Paris à Versailles, cite ce moyen de transport dans "La Cousine Bette", "Le Cousin Pons", "Les Comédiens sans le savoir". L'ambiance du voyage et des gares, l'univers tantôt feutré tantôt sombre en fait un cadre de choix pour le roman autour de thématiques particulièrement variées. La technologie et l'imaginaire associé lui ouvre les portes de la science fiction, tandis que l'espace clos des voitures donne bien des idées aux auteurs de policiers. La bande dessinée : "Des rails sur la prairie", ou la poésie : "Crains qu'un jour un train ne t'émeuve plus" de Guillaume Apollinaire, ne dérogent pas à la règle et il n'est pas rare d'y trouver un sujet ferroviaire. Aussi, dans Harry Potter, J.K. Rowling introduit le fameux Poudlard Express, un train qui conduit les sorciers jusqu'à l'école de Poudlard. Plusieurs études ont été réalisées sur le train dans la littérature. Pour la francophonie, notamment "Le train dans la littérature française" de Marc Baroli. Il existe également de nombreuses études sur des thèmes précis : la métaphore du tunnel, le voyage en train au . Une revue en ligne, des rails, est spécialisée dans l'imaginaire ferroviaire et la littérature en particulier.

Quelques exemples d'auteurs, en langue française, et d'ouvrages dont le thème principal est ferroviaire : 


Le thème du train a largement inspiré les artistes dès sa création, leurs œuvres, notamment gravures et photographies utilisées pour l'illustration d'ouvrages, nous permettent de visualiser les machines et chemins de fer disparus. 

De grands peintres ont produit des toiles inspirées par ce sujet, entre autres : William Turner : "Pluie, Vapeur et Vitesse : la grande voie ferrée de l’Ouest", 1844 ; Édouard Manet : "Le chemin de fer", 1873, Claude Monet, La Gare St Lazare 1877, Paul Cézanne : "La Montagne Sainte-Victoire et le Viaduc de la vallée de l'Arc", 1882-1885 ; Vincent van Gogh : "Wagons de chemin de fer", 1888 ; André Derain : "Charing Cross Bridge", 1915 ; Fernand Léger : "The Railway Crossing", 1919 ; Edward Hopper : "The House by the Railroad", 1925.

Depuis que le train existe, il est indissociable des histoires des hommes qu'ils soient cheminots ou voyageurs. Les cinéastes auteurs, toujours avides d'alimenter leur imaginaire, y trouvent un sujet inépuisable d'inspiration . Quelques grands moments du cinéma depuis le film des frères Lumière "L'Arrivée d'un train en gare de La Ciotat" en 1896 : "La Bête humaine" de Jean Renoir, "La Bataille du rail" de René Clément, "Le Crime de l'Orient-Express" de Sidney Lumet et "Les Vacances de Mr. Bean", de Steve Bendelack… (voir liste dans la catégorie Film ferroviaire ci-dessus).


Hedy West avec "Five Hundred Miles" reprend le thème éternel de la séparation par un long voyage en train. La chanson sera interprétée en version française par Richard Anthony "Et j'entend siffler le train" dans les années 60.




</doc>
<doc id="3020" url="https://fr.wikipedia.org/wiki?curid=3020" title="TeX">
TeX

TeX ou TeX est un système logiciel libre de composition de documents, indépendant du matériel utilisé pour la visualisation ou l'impression. Il fut créé à partir de 1977 par le mathématicien et informaticien Donald Knuth, excédé par la piètre qualité de la typographie des logiciels d'édition de l'époque. Il est principalement conçu pour l'édition de documents techniques et est largement utilisé par les scientifiques, particulièrement en mathématiques, physique, bio-informatique, astronomie et informatique. Il est également extensible et permet notamment l'édition de documents plus complexes (affiches, plaquettes publicitaires, partitions musicales...). "TeX" vient de , début du mot (« art, science », en grec ancien), et se prononce // ou //, au choix.

Lorsque le premier volume de "The Art of Computer Programming" fut publié en 1968, le premier livre de Donald Knuth, celui-ci fut d'abord imprimé par composition par métal chaud, une technique datant du qui donnait un caractère « ancien » apprécié par Knuth. Cependant lors de la seconde édition du second volume en 1976, le livre entier a dû être réimprimé parce que les éditeurs utilisaient une nouvelle technique appelée photocomposition qui ne fonctionnait plus avec les anciennes polices. En effet, Les anciennes imprimeries avaient été remplacées par des imprimeries à impression photographique. Lorsque Knuth reçut les premiers essais de son deuxième livre, il les trouva horribles. Tant et si bien qu'il décida de résoudre le problème et de réaliser lui-même son outil d'édition typographique qui ne serait d'aucun ressort photographique mais bien informatique.

Knuth prévoyait initialement de le terminer en 6 mois pour la fin de son année sabbatique. Il prendra presque 10 ans au final, soit de 1977 à 1989. Bien qu'il fut seul à le développer, il reçut cependant de l'aide de nombreux collaborateurs dont Hermann Zapf, Chuck Bigelow, Kris Holmes, Matthew Carter ou encore Richard Southall.

Décidant d'aller travailler dans les locaux de Stanford au lieu des laboratoires Xerox de Palo Alto, Knuth se mit au travail sur "TeX" lors du printemps 1977. En tenant comme référence un grand nombre de livres de typographie, notamment ceux de l'ancienne collection Aldine, il se mit en tête de retranscrire les anciens caractères dans son programme en utilisant des formules mathématiques. Il tentait alors de comprendre la logique derrière les lettres. C'est ainsi qu'il se mit à travailler sur son propre logiciel de polices de caractères Metafont.

Le premier prototype de "TeX" fut implémenté pendant l'été 1977 par 2 étudiants de Knuth, MIchael F. Plass et Frank M. Liang, d'après les spécifications qu'il leur avait donné. Il ne contenait initialement que quelques définitions de macro et d'opérations élémentaires sur les boites.

La première version de "TeX" fut achevée en 1978 par Knuth sur base du prototype. Il était alors implémenté en langage SAIL, un langage de compilation semblable à Algol. Il écrivit le premier manuel utilisateur de "TeX" pendant l'été 78. 

Au début de 1979, Trabb Pardo et Knuth commencèrent à planifier une conversion du SAIL au WEB. Cela se traduira par une conversion complète entre 1979 et 1980 faite par . Parallèlement, Knuth sortit la première version de Metafont en 1979.

En 1982 est publiée une nouvelle version nommée "TeX82", pratiquement réécrite de A à Z. Cette version inclut notamment des idées de l'implémentation de 1979 en MESA écrite par Leonidas Guibas, Robert Sedgewick et Douglas Wyatt du centre de recherche Xerox de Palo Alto. L' original fut remplacé par un nouvel algorithme écrit par Frank Liang en 1983. "TeX82" utilisa une représentation à virgule fixe à la place de virgule flottante afin de renforcer la portabilité sur plusieurs hardwares. Knuth inclut également un véritable langage de programmation certifié Turing-complet sous la pression de programmeurs notamment Guy Steele.

En août 1989, Donald Knuth entame de nouvelles modifications dans le code de "TeX" et Metafont. En effet jusque-là, les caractères étaient codés sur 7 bits, ce qui est suffisant pour afficher 128 lettres, soit l'alphabet anglais plus quelques caractères spéciaux, mais nettement insuffisant pour des alphabets étrangers (notamment en Europe et en Asie). Il dut donc réadapter entièrement "TeX" et Metafont pour fonctionner en 8 bits, soit 256 caractères différents, ce qui le rendit utilisable pour une bonne partie des langues d'Europe occidentale. Il introduisit également dans "TeX" la capacité à faire la césure dans d'autres langages que l'anglais, basée sur plusieurs idées de Michael J. Ferguson. "TeX" passa alors dans sa version stable 3.0.

À partir de la version 3, "TeX" utilisa un système idiosyncratique de numérotation de version, les mises à jour étant indiquées en ajoutant un chiffre supplémentaire après le point décimal, le numéro de version approchant ainsi asymptotiquement le nombre formula_1. La version actuelle de "TeX" est la 3.14159265, la dernière mise à jour datant du 12 janvier 2014. Les caractéristiques essentielles de "TeX" ont été figées après la , et toutes les versions suivantes ne contiennent que des corrections de "bugs". Donald Knuth, bien qu'ayant signalé des améliorations possibles, a insisté sur le fait qu'avoir un système fixe qui produira toujours la même sortie est plus important que d'introduire des caractéristiques nouvelles.

Dès le départ, le système "TeX" fut conçu afin d'être :
Durant le développement, le logiciel devint indépendant par rapport à son support. Pour un même fichier d'entrée, il délivre le même fichier de sortie quel que soit l'ordinateur ou système d'exploitation sur lequel il est installé.

Le langage "TeX" est un langage balisé. Il utilise à la fois du texte brut pour le corps du texte ainsi que des commandes commençant par une contre-oblique « \ ».

Les commandes reconnues par le compilateur "TeX" sont divisées en « primitives » (on en compte environ 300), et « macros », créées à partir des primitives. Les macros rendent possibles des structures assez complexes (boucles, conditions, etc) ainsi que la réalisation de calculs.

Afin de simplifier la préparation d'un document, D. Knuth a défini un ensemble d'environ 600 macros appelé plain TeX qui est décrit dans son livre "The TeXbook" en même temps que les primitives et les mécanismes internes. Cela fait qu'il y a souvent confusion entre plain TeX et le compilateur "TeX". Afin d'accélérer la création d'un document, cet ensemble de macros, appelé "format", est précompilé.

L'utilisation directe du format plain "TeX" étant assez ardue, il a été étendu en LaTeX (là encore, pas de distinction avec le nom du langage dans lequel est décrit le document que l'on appelle également LaTeX), écrit à l'origine par Leslie Lamport, qui constitue en fait un jeu de macro-commandes basées sur TeX. D'autres formats sont aussi disponibles, comme ConTeXt, …

Les commandes "TeX" ont de nombreuses utilisations. Elles servent par exemple à définir quelles polices à utiliser, ajouter des espaces, etc.

Voici un exemple de code "TeX" pour afficher "Hello World!".
Hello World!
\bye
%\Bye désigne la fin du fichier

Voici un exemple de code de formule mathématique. Celui-ci montre la formule quadratique.

"TeX" interprète un langage de description formel de document défini par D. Knuth (couramment, on ne fait pas la distinction entre le logiciel "TeX" et ce langage que l'on appelle également "TeX") autrement dit, ce n'est pas un formateur de texte à rendu immédiat où le document en préparation apparaît à l'écran comme il est censé être une fois imprimé, mais un logiciel traduisant une description textuelle en un document graphique.

"TeX" prend en entrée un document écrit en langage "TeX" et crée un fichier au format dvi contenant une description des pages, c'est-à-dire les positions des caractères, images, figures, etc sur la page. Afin d'être imprimé ou exporté, le dvi doit être traduit dans un autre format qui soit reconnu par l'appareil. PostScript (pour les imprimantes notamment) et PDF sont les formats les plus populaires pour "TeX".

"TeX" lit l'entrée octet par octet. Dans un premier temps, il va assembler ces octets en unités lexicales. Pour cela "TeX" utilise un système très souple de catégorisation des caractères. Dès qu'il rencontre un caractère d'échappement, il lit tout ce qui suit jusqu'à rencontrer un caractère qui n'est pas catégorisé comme une lettre. Cet ensemble de caractères est stocké comme un nom de commande. Celle-ci peut ou bien servir d'abréviation, et elle sera simplement développée lors de la composition, ou bien être exécutée, un peu comme une fonction ou une procédure d'un langage de programmation.

Certains des algorithmes utilisés par "TeX" sont assez complexes (Par exemple, il lit un paragraphe en entier pour décider où faire un retour à la ligne). Celui de la césure est notamment décrit peu après.

Il a été écrit par Frank Liang lors de sa thèse de doctorat en 1983 au département informatique de Stanford. C'est lui qui détermine où correctement couper dans un mot entre deux lignes. L'implémentation utilisée dans "TeX" utilise peu de place et son taux de réussite se trouve vers 90-95 % de chances de trouver une césure correcte. Son principe repose sur l'utilisation de dictionnaires et sur la reconnaissance de patterns (ou motifs) dans les mots.

Il procède en deux étapes :
Avant toute chose, "TeX" entoure le mot concerné de marqueurs spéciaux. Prenons le mot codice_1. Avec ces marqueurs, cela donne : codice_2. Ensuite il découpe le mot en sous-mots de longueur "k", le découpage précédent correspondant à une longueur 1. Voici à titre d'illustration le découpage en longueur 2 et puis 3 :

codice_3

codice_4

et ainsi de suite.

Chaque sous-mot (ou motif) de longueur "k" possède "k+1" coefficients entier de poids de césure. Ce poids permet de savoir où il est préférable de couper le mot. Le coefficient est un chiffre compris entre 0 à 9 et est placé entre chaque lettre. Un poids de 0 signifie qu'il est absolument interdit de couper entre ces deux lettres, un poids pair désigne une interdiction de couper (comme 0) et un poids impair une préférence pour couper. Si deux motifs s'affrontent, celui ayant le plus de poids l'emporte.

Par exemple dans les mots codice_5 et codice_6, nous avons le pattern codice_7. Cependant ils se coupent différemment, codice_8 et codice_9. Les patterns seront alors scop, iscope et discop. Par défaut le 1 schéma s'applique. S'il y a concurrence avec un autre pattern, on prend les coefficients plus grands.

Comme ces motifs sont dépendants des mots, il est donc impératif d'avoir un dictionnaire de mots (et d'exceptions) propre à chaque langue.

Metafont est un logiciel également conçu par Knuth qui permet de créer des caractères et par extension, des polices de caractères. Bien que Metafont ne fasse pas partie de "TeX" à proprement dit, il reste très lié à celui-ci. En effet, la police de caractère utilisée par "TeX", Computer Modern, a été conçue à l'aide de Metafont.

Metafont utilise de nombreuses techniques afin de tracer les lettres, comme les courbes de Bézier. Lors de sa conception, Knuth essayait de trouver une certaine logique entre lettres. Par exemple la lettre "n" prend exactement la largeur de la lettre "i", la lettre "m" prend la largeur de la lettre "i" et la lettre "u" prend la même largeur que la lettre "n".

Pour créer des lettres avec Metafont, il faut décrire les lettres à l'aide d'équations mathématiques.

"TeX" est souvent utilisé dans les milieux académiques et scientifiques afin de produire des documents techniques, cela principalement dû à son rendu de formules et autres symboles.

Cependant, il a d'autres usages notamment dans le milieu professionnel. Il peut par exemple produire des cartes de visites, des lettres, des programmes de concert ou encore des livres (la motivation de départ de Knuth).

Il est possible d'utiliser "TeX" pour générer de manière complètement automatique la mise en page délicate de données XML. Ceci permet la conciliation des différences syntaxiques entre les deux langages descriptifs à l'aide de TeXML. On peut donc considérer "TeX" comme une alternative à XSL-FO dans le cadre de publication en XML.

Donald Knuth a répété plusieurs fois que le code source de "TeX" était dans le domaine public et qu'il encourageait fortement les modifications de celui-ci. D'autre part bien qu'il soit dans le domaine public, Knuth demande que lorsqu'une modification de "TeX" est publiée, celle-ci porte une autre dénomination que "TeX" pour permettre de la distinguer (exemple avec LaTeX ou ConTeXt).

Le compilateur TeX a fait l'objet de quelques extensions (à ne pas confondre avec des formats comme LaTeX, ConTeXt), parmi lesquelles :


Il existe de nombreux groupes d'utilisateurs de "TeX"/"LaTeX" de par le monde, qui sont généralement liés à une langue :
GUTenberg (francophone), TUG (anglophone, signifie "TeX Users Group"), DANTE (germanophone), CervanTeX (hispanophone), GUIT (italophone), NTG (néerlandophone)…

Certains de ces groupes publient des journaux (les "Cahiers GUTenberg", la "Lettre GUTenberg", "PracTeX", "TUGboat"…) et organisent aussi des conférences, parfois de façon conjointe ("EuroTeX"…).

Donald Knuth encourage fortement quiconque à trouver des bugs ou des erreurs de typographies dans le code source de "TeX" ainsi que dans le reste de son travail, incluant ses livres. Jusqu'en 2008, il offrait même une qui variait selon l'âge de l'erreur. La première année de l'introduction du bug, elle était de 2,56 dollars US et augmentait ensuite progressivement chaque année par puissance de 2 en partant de 1,28$ jusqu'à un maximum de 327,68$. Bien qu'énormément de chèques aient été envoyés, en réalité peu d'entre eux ont été réellement encaissés, la valeur du mérite étant plus importante que celle de la récompense elle-même.

À partir de 2008, Knuth décida d'arrêter d'envoyer des chèques. Il préfère dorénavant envoyer des certificats personnels avec un montant symbolique noté en hexadécimal de la banque de la nation imaginaire de San Seriffe.





</doc>
<doc id="3021" url="https://fr.wikipedia.org/wiki?curid=3021" title="Tom-Tom et Nana">
Tom-Tom et Nana

Tom-Tom et Nana est une bande dessinée qui paraît dans la revue mensuelle "J'aime lire" depuis 1977. Les auteures initiales de la série sont Jacqueline Cohen pour les textes et Bernadette Després pour les dessins. Elles ont ensuite été rejointes, pour les scénarios, par Évelyne Reberg. C'est Catherine Viansson-Ponté qui s'occupe de la couleur.

Elle met en scène la famille Dubouchon, tenancière du restaurant "À la bonne fourchette", dont la tranquillité est troublée par les fantaisies des deux plus jeunes enfants, Tom-Tom et Nana.

Extrêmement peu médiatisée, sans doute par la faute de son statut de bande dessinée en fin de magazine donc créée uniquement pour fidéliser les jeunes lecteurs, la BD connut pourtant un grand succès qui atteint son apogée vers les années 1995-2000 (pendant lesquelles un dessin animé est créé). Mais, pendant les années 2000, elle perd en régularité : les histoires, qui avaient jusque-là toujours fait 10 pages, changent souvent de nombre de pages. Maintenant, les histoires (qui ne font plus qu'une page) paraissent irrégulièrement dans "J'aime lire". Coïncidant avec cette perte de régularité, de nouveaux héros sont créés et commencent à cohabiter avec eux, puis peu à peu à les remplacer. Ce sont eux qui tiennent maintenant le devant de la scène, notamment Anatole Latuile et Ariol, mais Tom-Tom et Nana conservent une grande popularité.




De nombreux gags récurrents pimentent la série :

Les histoires sont régulièrement regroupées dans des albums dédiés. La première collection débute en 1985 et regroupe les aventures de Tom-Tom Dubouchon. Chaque album est composé de neuf histoires de dix planches chacune.


Cette série présente des compilations d'aventures précédemment publiées.

La bande-dessinée fut adaptée en 1997 en une série télévisée d'animation, diffusée à partir du sur France 3 dans l'émission "Les Minikeums", Canal J en 2005, France 5 dans "Zouzous" depuis 2011, ou encore Tiji en 2011.



</doc>
<doc id="3022" url="https://fr.wikipedia.org/wiki?curid=3022" title="Terraformation">
Terraformation

La terraformation est un thème classique de la science-fiction, popularisé par l'auteur américain Jack Williamson. Il s'agit d'une science qui étudie la transformation de l'environnement naturel d'une planète, d'un satellite naturel ou d'un autre corps céleste, afin de la rendre habitable en réunissant les conditions d'une vie de type terrestre.

Le terme officiel en France a été dans un premier temps « écogenèse », préconisé en 1995 puis en 2000. En 2008, il a été remplacé par le terme « biosphérisation », défini comme la .

L'action primordiale pour y arriver est la modification ou la création d'une atmosphère de composition proche de celle de la Terre, composante essentielle au développement de la vie.
On parle aussi d'ingénierie planétaire si l'objectif n'est pas de faire ressembler la planète en question à la Terre.

Chaque candidat à la terraformation présente des conditions qui lui sont propres, rendant le processus spécifique pour chacun d'eux. Les principales études menées concernent la planète Mars. D'autres concernent Vénus, Europe (satellite de Jupiter) et Titan (satellite de Saturne), voire d'autres corps, mais les conditions semblent beaucoup plus difficiles à modifier.

Progressivement, les scientifiques se sont intéressés à la terraformation, à commencer par l'américain Carl Sagan qui proposa de terraformer Vénus en 1961, à l'aide d'algues injectées dans son atmosphère. L'environnement vénusien est cependant assez infernal, avec une température de l'ordre de . Ces conditions sont liées à la présence de dioxyde de carbone () et de vapeur d'eau, deux gaz à effet de serre.

Les algues devaient générer du dioxygène par photosynthèse et du carbone minéral en se décomposant. La baisse du taux de entraînait alors le refroidissement et la condensation de la vapeur d'eau.
Néanmoins, le carbone a tendance à reformer du sous forte température ; l'objectif ne semble donc pas accessible.

À la suite de ce premier développement, la terraformation s'est petit à petit imposée comme une réelle possibilité et aujourd'hui la terraformation de Mars est un sujet sérieusement envisagé par de nombreux scientifiques.

Les images évoquées par Mars sont celles d'une planète rouge, sèche, rocailleuse… et (du moins actuellement) surtout sans vie. Cependant, on y distingue parfois des vallées d'apparence érodée et les recherches "in situ" semblent indiquer la présence d'anciens fleuves et d'anciennes mers. Or si l'eau, élément essentiel à la vie telle que nous la connaissons, a coulé sur Mars, où se trouve-t-elle aujourd'hui et peut-on la faire resurgir ? Ce sont les principales questions qui animent les débats autour de la terraformation de la planète. L'objectif est donc de redonner à Mars cet environnement qu'elle semble avoir perdu et y ajouter le nécessaire pour l'Homme.

Mars possède plusieurs points communs avec la Terre. Sa vitesse de rotation, l'inclinaison de son orbite ou l'aspect de sa surface laissent entrevoir des paysages modelés par des saisons proches de celles que connait la Terre. Le sol (régolithe) martien est composé de nombreux oxydes ( à 45 % et à 15 % environ). Cependant le climat actuel n'est pas tout à fait favorable : du fait de son éloignement du Soleil, la température moyenne avoisine les et la pression atmosphérique est 160 fois inférieure à celle que l'on trouve sur Terre. 
De plus, Mars a une gravité équivalente à environ un tiers seulement de celle de la Terre : une si faible gravité provoquerait à moyen terme un relâchement et une perte de contrôle musculaires. Seule une activité physique importante permettrait d'éviter cette atrophie.
En l'absence de champ magnétique et d'une atmosphère consistante, la surface de Mars est constamment bombardée par des rayons cosmiques et ultraviolets qui ont un effet létal sur l'homme à court/moyen terme. Le régolite martien très fin et abrasif aurait des effets particulièrement néfastes sur la physiologie humaine.

Le principal obstacle à la Terraformation de Vénus est sa température de surface pouvant dépasser les . La pression y est 90 fois celle de la Terre, et elle est recouverte de nuages d'acide sulfurique. Vénus fait cependant à peu près le même diamètre que la Terre.

Deux pistes sont explorées pour diminuer la température à la surface de Vénus:

Expulser du gaz hors de la planète semble très difficile. La meilleure solution semble être de transformer le gaz en composés solides ou liquides. Plusieurs méthodes sont envisageables : soit en envoyant de la poussière de magnésium ou de calcium (que l'on pourrait prélever sur Mercure), ce qui conduirait à la formation de carbonates, soit en injectant de l'hydrogène qui conduirait à la production de graphite et d'eau via la réaction de Bosch. Une autre solution serait d'introduire des organismes vivants, comme des bactéries extrêmophiles, mais si la température au sol reste la même, la matière organique redeviendrait immédiatement du gaz carbonique.
Transformer une quantité importante de gaz carbonique en carbone et eau comme expliqué plus haut permettrait à ces organismes de survivre en haute atmosphère et de faire croître le taux d'oxygène à une vitesse honorable.

Une idée originale pour permettre une colonisation rapide de Vénus, proposée par Geoffrey A. Landis, est de faire flotter d'immenses sacs de gaz à environ d'altitude, à condition de veiller à ce que lesdits sacs résistent aux pluies d'acide sulfurique courantes à cette altitude, là où la pression atmosphérique et la température sont proches des conditions terrestres. On pourrait construire des villes à l'intérieur des sacs, qui flotteraient sur l'atmosphère dense de Vénus comme des montgolfières dont l'habitacle serait à l'intérieur du ballon. Le gaz à l'intérieur serait un mélange respirable.

De telles cités pourraient être comme une tête de pont d'où assurer les interventions lourdes pour une terraformation complète de Vénus.

La rotation de Vénus est très lente, ce qui fait qu'un jour vénusien dure quasiment une année. Accélérer la rotation demanderait trop d'énergie.





</doc>
<doc id="3023" url="https://fr.wikipedia.org/wiki?curid=3023" title="Typhlodromus">
Typhlodromus

Le genre Typhlodromus regroupe des acariens prédateurs de la famille des Phytoseiidae, dont les formes mobiles ont pour proies principalement les acariens et les thrips sur les arbres fruitiers, et la vigne. Plus de 300 espèces sont connues.

Typhlodromus Scheuten, 1857 synonymes Amblydromella Muma, 1961, Aphanoseius Wainstein, 1972, Berethria Tuttle & Muma, 1973, Clavidromus Muma, 1961, Colchodromus Wainstein, 1962, Indodromus Ghai & Menon, 1969, Litoseius Kolodochka, 1992, Mumaseius De Leon, 1965, Orientiseius Muma & Denmark, 1968, Seiodromus Wainstein, 1962, Taxodromus Wainstein, 1962, Typhlodromella Muma, 1961 & Vittoseius Kolodochka, 1988

Le typhlodrome est un acarien carnivore, efficace prédateur naturel de l'araignée rouge (dans les vergers). Il est utilisable en tant que prédateur naturel pour les acariens tels que l'araignée rouge.



</doc>
<doc id="3027" url="https://fr.wikipedia.org/wiki?curid=3027" title="Terre">
Terre

La Terre est une planète du Système solaire, la troisième plus proche du Soleil et la cinquième plus grande, tant en taille qu'en masse, de ce système planétaire dont elle est aussi la plus massive des planètes telluriques.

La Terre s'est formée il y a d'années environ et la vie y est apparue moins d'un milliard d'années plus tard. La planète abrite des millions d'espèces vivantes, dont les humains. La biosphère de la Terre a fortement modifié l'atmosphère et les autres caractéristiques abiotiques de la planète, permettant la prolifération d'organismes aérobies de même que la formation d'une couche d'ozone, qui associée au champ magnétique terrestre, bloque une partie des rayonnements solaires, permettant ainsi "la vie" sur Terre. Les propriétés physiques de la Terre, de même que son histoire géologique et son orbite, ont permis à la vie de subsister durant cette période et la Terre devrait pouvoir maintenir la vie (telle que nous la connaissons actuellement) durant encore au moins d'années.

La croûte terrestre est divisée en plusieurs segments rigides appelés plaques tectoniques qui se déplacent sur des millions d'années. Environ 71 % de la surface terrestre est couverte par des océans d'eau salée qui forment l'hydrosphère avec les autres sources d'eau comme les lacs, les fleuves ou les nappes phréatiques. Les pôles géographiques de la Terre sont principalement recouverts de glace (inlandsis de l'Antarctique) ou de banquises. L'intérieur de la planète reste actif avec un épais manteau composé de roches silicatées (généralement solides, mais localement fondues), un noyau externe de fer liquide qui génère un champ magnétique, et un noyau interne de fer solide.

La Terre interagit avec les autres objets spatiaux, principalement le Soleil et la Lune. Actuellement, la Terre orbite autour du Soleil en ou une année sidérale. L'axe de rotation de la Terre est incliné de 23,437° par rapport à la perpendiculaire du plan de l'écliptique, ce qui produit des variations saisonnières sur la surface de la planète avec une période d'une année tropique (365,24219 jours solaires). Le seul satellite naturel connu de la Terre est la Lune qui commença à orbiter il y a d'années. Celle-ci provoque les marées, stabilise l'inclinaison axiale et ralentit lentement la rotation terrestre. Il y a environ d'années, lors du grand bombardement tardif, de nombreux impacts d'astéroïdes causèrent alors d'importantes modifications de sa surface.

La Terre a pour particularité, du point de vue de l'être humain, d'être le seul endroit connu de l'univers à abriter la vie telle que "nous la connaissons", comme la faune (dont entre autres l'espèce humaine) et la flore. Les cultures humaines ont développé de nombreuses représentations de la planète, dont une personnification en tant que déité, la croyance en une terre plate, la Terre en tant que centre de l'univers et la perspective moderne d'un monde en tant que système global nécessitant une gestion raisonnable.

La science qui étudie la Terre est la géologie. Compte tenu de l'influence de la vie sur la composition de l'atmosphère, des océans et des roches sédimentaires, la géologie emprunte à la biologie une partie de sa chronologie et de son vocabulaire.

L'âge de la Terre est aujourd'hui estimé à d'années. L'histoire de la Terre est divisée en quatre grands intervalles de temps, dits éons :

La formation de la Terre par accrétion était presque terminée en moins de d'années. Initialement en fusion, la couche externe de la Terre s'est refroidie pour former une croûte solide lorsque l'eau commença à s'accumuler dans l'atmosphère, aboutissant aux premières pluies et aux premiers océans. La Lune s'est formée peu de temps après, il y a d'années.
Le consensus actuel pour la formation de la Lune est l'hypothèse de l'impact géant, selon laquelle un objet (quelquefois appelé Théia), de la taille de Mars et de masse environ égale au dixième de la masse terrestre, est entré en collision avec la Terre. Dans ce modèle, une partie de cet objet se serait agglomérée avec la Terre tandis qu'une autre partie, mêlée avec peut-être 10 % de la masse totale de la Terre, aurait été éjectée dans l'espace, où elle aurait formé la Lune.

L'activité volcanique a produit une atmosphère primitive. De la vapeur d'eau condensée ayant plusieurs origines possibles, mêlée à de la glace apportée par des comètes, a produit les océans. Une combinaison de gaz à effet de serre et d'importants niveaux d'activité solaire permirent d'augmenter la température à la surface de la Terre et empêchèrent les océans de geler. Vers d'années, le champ magnétique se forma et il permit d'éviter à l'atmosphère d'être balayée par le vent solaire.

Deux principaux modèles ont été proposés pour expliquer la vitesse de croissance continentale : une croissance constante jusqu'à nos jours et une croissance rapide au début de l'histoire de la Terre. Les recherches actuelles montrent que la deuxième hypothèse est la plus probable avec une formation rapide de la croûte continentale suivie par de faibles variations de la surface globale des continents. Sur une échelle de temps de plusieurs centaines de millions d'années, les continents ou supercontinents se forment puis se divisent. C'est ainsi qu'il y a environ d'années, le plus vieux des supercontinents connus, Rodinia, commença à se disloquer. Les continents entre lesquels il s'était divisé se recombinèrent plus tard pour former Pannotia, il y a 650- d'années, puis finalement Pangée, au Permien, qui se fragmenta il y a d'années.

On suppose qu'une activité chimique intense dans un milieu hautement énergétique a produit une molécule capable de se reproduire, dans un système particulier, il y a environ d'années. On pense que la vie elle-même serait apparue entre 200 et d'années plus tard.

Le développement de la photosynthèse, active depuis bien avant 3 à d'années avant le présent, permit à la vie d'exploiter directement l'énergie du Soleil. Celle-ci produisit de l'oxygène qui s'accumula dans l'atmosphère, à partir d'environ d'années avant le présent, et forma la couche d'ozone (une forme d'oxygène [O]) dans la haute atmosphère, lorsque les niveaux d'oxygène dépassèrent quelques pourcents. Le regroupement de petites cellules entraina le développement de cellules complexes appelées eucaryotes. Les premiers organismes multicellulaires formés de cellules au sein de colonies devinrent de plus en plus spécialisés. Aidées par l'absorption des dangereux rayons ultraviolets par la couche d'ozone, des colonies bactériennes pourraient avoir colonisé la surface de la Terre, dès ces époques lointaines. Les plantes et les animaux pluricellulaires ne colonisèrent la terre ferme qu'à partir de la fin du Cambrien (pour mousses, lichens et champignons) et pendant l'Ordovicien (pour les premiers végétaux vasculaires et les arthropodes), le Silurien (pour les gastéropodes ?) et le Dévonien (pour les vertébrés).

Depuis les années 1960, il a été proposé une hypothèse selon laquelle une ou plusieurs séries de glaciations globales eurent lieu il y a 750 à d'années, pendant le Néoprotérozoïque, et qui couvrirent la planète d'une couche de glace. Cette hypothèse a été nommée "Snowball Earth" (« Terre boule de neige »), et est d'un intérêt particulier parce qu'elle précède l'explosion cambrienne, quand des formes de vies multicellulaires commencèrent à proliférer.

À la suite de l'explosion cambrienne, il y a environ 535 millions d'années, cinq extinctions massives se produisirent. La dernière extinction majeure date de d'années, quand une météorite est entrée en collision avec la Terre, exterminant les dinosaures et d'autres grands reptiles, épargnant de plus petits animaux comme les mammifères, les oiseaux, ou encore les lézards.

Dans les d'années qui se sont écoulées depuis, les mammifères se sont diversifiés, le genre humain (Homo) s'étant développé depuis deux millions d'années. Des changements périodiques à long terme de l'orbite de la Terre, causés par l'influence gravitationnelle des autres astres, sont probablement une des causes des glaciations qui ont plus que doublé les zones polaires de la planète, périodiquement dans les derniers millions d'années.

À l'issue de la dernière glaciation, le développement de l'agriculture et, ensuite, des civilisations, permit aux humains de modifier la surface de la Terre dans une courte période de temps, comme aucune autre espèce avant eux, affectant la nature tout comme les autres formes de vie.

Le futur de la Terre est très lié à celui du Soleil. Du fait de l'accumulation d'hélium dans le cœur du Soleil, la luminosité de l'étoile augmente lentement à l'échelle des temps géologiques. La luminosité va croître de 10 % au cours du 1,1 milliard d'années à venir et de 40 % sur les prochains d'années. Les modèles climatiques indiquent que l'accroissement des radiations atteignant la Terre aura probablement des conséquences dramatiques sur la pérennité de son climat « terrestre », notamment la disparition des océans.

La Terre devrait cependant rester habitable durant encore plus de d'années, cette durée pouvant passer à 2,3 milliards d'années si la pression atmosphérique diminue en retirant une partie de l'azote de l'atmosphère. L'augmentation de la température terrestre va accélérer le cycle du carbone inorganique, réduisant sa concentration à des niveaux qui pourraient devenir trop faibles pour les plantes (10 ppm pour la photosynthèse du) dans environ 500 ou d'années. La réduction de la végétation entrainera la diminution de la quantité d'oxygène dans l'atmosphère, ce qui provoquera la disparition progressive de la plupart des formes de vies animales. Ensuite, la température moyenne (de la Terre) augmentera plus vite en raison de l'emballement de l'effet de serre par la vapeur d'eau, vers 40 à . Dans 1 milliard à 1,7 milliard d'années, la température sera si élevée que les océans s'évaporeront, précipitant le climat de la Terre dans celui de type vénusien, et faisant disparaître toute forme simple de vie à la surface de la Terre.

Même si le Soleil était éternel et stable, le refroidissement interne de la Terre entrainerait la baisse du niveau de CO du fait d'une réduction du volcanisme, et 35 % de l'eau des océans descendrait dans le manteau du fait de la baisse des échanges au niveau des dorsales océaniques.

Dans le cadre de son évolution, le Soleil deviendra une géante rouge dans plus de d'années. Les modèles prédisent qu'il gonflera jusqu'à atteindre environ son rayon actuel.

Le destin de la Terre est moins clair. En tant que géante rouge, le Soleil va perdre environ 30 % de sa masse, donc sans prendre en compte les effets de marée, la Terre se déplacerait sur une orbite à du Soleil lorsque celui-ci atteindra sa taille maximale. La planète ne devrait donc pas être engloutie par les couches externes du Soleil même si l'atmosphère restante finira par être « soufflée » dans l'espace, et la croûte terrestre finira par fondre pour se transformer en un océan de lave, lorsque la luminosité solaire atteindra environ son niveau actuel. Cependant, une simulation de 2008 indique que l'orbite terrestre va se modifier du fait des effets de marées et poussera la Terre à entrer dans l'atmosphère du Soleil où elle sera absorbée et vaporisée.

La forme de la Terre est approchée par un ellipsoïde, une sphère aplatie aux pôles. La rotation de la Terre entraîne l'apparition d'un léger bourrelet de sorte que le diamètre à l’équateur est plus long que le diamètre polaire (du pôle Nord au pôle Sud). Le diamètre moyen du sphéroïde de référence (appelé géoïde) est d'environ , ce qui est approximativement /π, car le mètre était initialement défini comme 1/e (dix-millionième) de la distance de l'équateur au pôle Nord en passant par Paris.

La topographie locale dévie de ce sphéroïde idéalisé même si à grande échelle, ces variations sont faibles : la Terre a une tolérance d'environ 0,17 % par rapport au sphéroïde parfait. Proportionnellement, c'est un peu moins lisse qu'une boule de billard neuve, alors qu'une boule de billard usée peut présenter des aspérités légèrement plus marquées. Les plus grandes variations dans la surface rocheuse de la Terre sont l'Everest ( au-dessus du niveau de la mer) et la fosse des Mariannes ( sous le niveau de la mer). Du fait du bourrelet équatorial, les lieux les plus éloignés du centre de la Terre sont les sommets du Chimborazo en Équateur et du Huascarán au Pérou.

Le rayon de la Terre est d'environ , selon divers modèles sphériques. La Terre n'étant pas parfaitement sphérique, la distance entre son centre et la surface varie de (fond de l'océan Arctique) à (sommet du Chimborazo). Le rayon équatorial est de , alors que le rayon polaire est de (modèle ellipsoïde de sphère aplatie aux pôles).

Une vieille théorie, nouvellement remise sur le devant de la scène, explique que la Terre n'aurait pas toujours eu la même taille, et qu'elle serait en expansion. Une conséquence en serait un rallongement des journées, à l'échelle de plusieurs millions d'années. Cette théorie est peu reconnue par la communauté scientifique mondiale, voire considérée comme de la pseudo-science.

La masse de la Terre est estimée à . On la détermine en divisant la constante géocentrique "GM" par la constante de gravitation "G". Sa précision est limitée par celle de "G", le produit "GM" pouvant être déduit des mesures de géodésie spatiale avec une précision bien supérieure.

La Terre est une planète tellurique, c'est-à-dire une planète essentiellement rocheuse à noyau métallique, contrairement aux géantes gazeuses, telles que Jupiter, essentiellement constituées de gaz légers (hydrogène et hélium). Il s'agit de la plus grande des quatre planètes telluriques du Système solaire, que ce soit par la taille ou la masse. De ces quatre planètes, la Terre a aussi la masse volumique globale la plus élevée, la plus forte gravité de surface, le plus puissant champ magnétique global, la vitesse de rotation la plus élevée et est probablement la seule avec une tectonique des plaques active.

La surface externe de la Terre est divisée en plusieurs segments rigides, ou plaques tectoniques, qui se déplacent lentement sur la surface sur des durées de plusieurs millions d'années. Environ 71 % de la surface est couverte d'océans d'eau salée, les 29 % restants étant des continents et des îles. L'eau liquide, nécessaire à la vie telle que nous la connaissons, est très abondante sur Terre, et aucune autre planète n'a encore été découverte avec des étendues d'eau liquide (lacs, mers, océans) à sa surface.

La Terre est principalement composée de fer (32,1 %), d'oxygène (30,1 %), de silicium (15,1 %), de magnésium (13,9 %), de soufre (2,9 %), de nickel (1,8 %), de calcium (1,5 %) et d'aluminium (1,4 %), le reste (1,2 %) consistant en de légères traces d'autres éléments. Les éléments les plus denses ayant tendance à se concentrer au centre de la Terre (phénomène de différenciation planétaire), on pense que le cœur de la Terre est composé majoritairement de fer (88,8 %), avec une plus petite quantité de nickel (5,8 %), de soufre (4,5 %) et moins de 1 % d'autres éléments.

Le géochimiste F. W. Clarke a calculé que 47 % (en poids, soit 94 % en volume) de la croûte terrestre était faite d'oxygène, présent principalement sous forme d'oxydes, dont les principaux sont les oxydes de silicium (sous forme de silicates), d'aluminium (aluminosilicates), de fer, de calcium, de magnésium, de potassium et de sodium. La silice est le constituant majeur de la croûte, sous forme de pyroxénoïdes, les minéraux les plus communs des roches magmatiques et métamorphiques. Après une synthèse basée sur l'analyse de de roches, Clarke a obtenu les pourcentages présentés dans le tableau ci-contre.

L'intérieur de la Terre, comme celui des autres planètes telluriques, est stratifié, c'est-à-dire organisé en couches concentriques superposées, ayant des densités croissantes avec la profondeur. Ces diverses couches se distinguent par leur nature pétrologique (contrastes chimiques et minéralogiques) et leurs propriétés physiques (changements d'état physique, propriétés rhéologiques). La couche extérieure de la Terre solide, fine à très fine relativement au rayon terrestre, s'appelle la croûte ; elle est solide, et chimiquement distincte du manteau, solide, sur lequel elle repose ; sous l'effet combiné de la pression et de la température, avec la profondeur, le manteau passe d'un état solide fragile (cassant, sismogène, « lithosphérique ») à un état solide ductile (plastique, « asthénosphérique », et donc caractérisé par une viscosité plus faible, quoiqu'encore extrêmement élevée). La surface de contact entre la croûte et le manteau est appelée le Moho ; il se visualise très bien par les méthodes sismiques du fait du fort contraste de vitesse des ondes sismiques, entre les deux côtés. L'épaisseur de la croûte varie de sous les océans jusqu'à plus de en moyenne sous les continents. La croûte et la partie supérieure froide et rigide du manteau supérieur sont appelés lithosphère ; leur comportement horizontalement rigide à l'échelle du million à la dizaine de millions d'années est à l'origine de la tectonique des plaques. L'asthénosphère se trouve sous la lithosphère et est une couche convective, relativement moins visqueuse sur laquelle la lithosphère se déplace en « plaques minces ». Des changements importants dans la structure cristallographique des divers minéraux du manteau, qui sont des changements de phase au sens thermodynamique, vers respectivement les profondeurs de et de sous la surface, encadrent une zone dite de transition, définie initialement sur la base des premières images sismologiques. Actuellement, on appelle manteau supérieur la couche qui va du Moho à la transition de phase vers de profondeur, la transition à de profondeur étant reconnue pour ne pas avoir une importance majeure sur le processus de convection mantellique, au contraire de l'autre. Et l'on appelle donc manteau inférieur la zone comprise entre cette transition de phase à de profondeur, et la limite noyau-manteau. Sous le manteau inférieur, le noyau terrestre, composé à presque 90 % de fer métal, constitue une entité chimiquement originale de tout ce qui est au-dessus, à savoir la Terre silicatée. Ce noyau est lui-même stratifié en un noyau externe liquide et très peu visqueux (viscosité de l'ordre de celle d'une huile moteur à ), qui entoure un noyau interne solide encore appelé graine. Cette graine résulte de la cristallisation du noyau du fait du refroidissement séculaire de la Terre. Cette cristallisation, par la chaleur latente qu'elle libère, est source d'une convection du noyau externe, laquelle est la source du champ magnétique terrestre. L'absence d'un tel champ magnétique sur les autres planètes telluriques laisse penser que leurs noyaux métalliques, dont les présences sont nécessaires pour expliquer les données astronomiques de densité et de moment d'inertie, sont totalement cristallisés. Selon une interprétation encore débattue de données sismologiques, le noyau interne terrestre semblerait tourner à une vitesse angulaire légèrement supérieure à celle du reste de la planète, avançant relativement de à par an.

La chaleur interne de la Terre est issue d'une combinaison de l'énergie résiduelle issue de l'accrétion planétaire (environ 20 %) et de la chaleur produite par les éléments radioactifs (80 %). Les principaux isotopes producteurs de chaleur de la Terre sont le potassium 40, l'uranium 238, l'uranium 235 et le thorium 232. Au centre de la planète, la température pourrait atteindre et la pression serait de . Comme la plus grande partie de la chaleur est issue de la désintégration des éléments radioactifs, les scientifiques considèrent qu'au début de l'histoire de la Terre, avant que les isotopes à courte durée de vie ne se soient désintégrés, la production de chaleur de la Terre aurait été bien plus importante. Cette production supplémentaire, deux fois plus importante qu'aujourd'hui il y a d'années, aurait accru les gradients de températures dans la Terre et donc le rythme de la convection mantellique et de la tectonique des plaques, ce qui aurait permis la formation de roches ignées comme les komatiites qui ne sont plus formées aujourd'hui.

La perte moyenne de chaleur par la Terre est de pour une perte globale de . Une portion de l'énergie thermique du noyau est transportée vers la croûte par des panaches, une forme de convection où des roches semi-fondues remontent vers la croûte. Ces panaches peuvent produire des points chauds et des trapps. La plus grande partie de la chaleur de la Terre est perdue à travers la tectonique des plaques au niveau des dorsales océaniques. La dernière source importante de perte de chaleur est la conduction à travers la lithosphère, la plus grande partie ayant lieu dans les océans, car la croûte y est plus mince que celle des continents, surtout au niveau des dorsales.

Les plaques tectoniques sont des segments rigides de lithosphère qui se déplacent les uns par rapport aux autres. Les relations cinématiques qui existent aux frontières des plaques peuvent être regroupées en trois domaines : des domaines de convergence où deux plaques se rencontrent, de divergence où deux plaques se séparent et des domaines de transcurrence où les plaques se déplacent latéralement les unes par rapport aux autres. Les tremblements de terre, l'activité volcanique, la formation des montagnes et des fosses océaniques sont plus fréquents le long de ces frontières. Le mouvement des plaques tectoniques est lié aux mouvements de convection ayant lieu dans le manteau terrestre.

Du fait du mouvement des plaques tectoniques, le plancher océanique plonge sous les bords des autres plaques. Au même moment, la remontée du magma au niveau des frontières divergentes crée des dorsales. La combinaison de ces processus permet un recyclage continuel de la lithosphère océanique qui retourne dans le manteau. Par conséquent, la plus grande partie du plancher océanique est âgée de moins de d'années. La plus ancienne croûte océanique est localisée dans l'ouest du Pacifique et a un âge estimé de d'années. Par comparaison, les éléments les plus anciens de la croûte continentale sont âgés de d'années.

Il existe sept principales plaques, Pacifique, Nord-Américaine, Eurasienne, Africaine, Antarctique, Australienne et Sud-Américaine. Parmi les plaques importantes, on peut également citer les plaques Arabique, Caraïbe, Nazca à l'ouest de la côte occidentale de l'Amérique du Sud et la plaque Scotia dans le sud de l'océan Atlantique. La plaque australienne fusionna avec la plaque indienne il y a d'années. Les plaques océaniques sont les plus rapides : la plaque de Cocos avance à un rythme de et la plaque pacifique à . À l'autre extrême, la plus lente est la plaque eurasienne progressant à une vitesse de .

Le relief de la Terre diffère énormément suivant le lieu. Environ 70,8 % de la surface du globe est recouverte par de l'eau et une grande partie du plateau continental se trouve sous le niveau de la mer. Les zones submergées ont un relief aussi varié que les autres dont une dorsale océanique faisant le tour de la Terre ainsi que des volcans sous-marins, des fosses océaniques, des canyons sous-marins, des plateaux et des plaines abyssales. Les 29,2 % non recouvertes d'eau sont composés de montagnes, de déserts, de plaines, de plateaux et d'autres géomorphologies.

La surface planétaire subit de nombreuses modifications du fait de la tectonique et de l'érosion. Les éléments de surface construits ou déformés par la tectonique des plaques sont sujets à une météorisation constante du fait des précipitations, des cycles thermiques et des effets chimiques. Les glaciations, l'érosion du littoral, la construction des récifs coralliens et les impacts météoriques contribuent également aux modifications du paysage.
La lithosphère continentale est composée de matériaux de faible densité comme les roches ignées : granite et andésite. Le basalte est moins fréquent et cette roche volcanique dense est le principal constituant du plancher océanique. Les roches sédimentaires se forment par l'accumulation de sédiments qui se compactent. Environ 75 % des surfaces continentales sont recouvertes de roches sédimentaires même si elles ne représentent que 5 % de la croûte. Le troisième type de roche rencontré sur Terre est la roche métamorphique, créée par la transformation d'autres types de roche en présence de hautes pressions, de hautes températures ou les deux. Parmi les silicates les plus abondants de la surface terrestre, on peut citer le quartz, le feldspath, l'amphibole, le mica, le pyroxène et l'olivine. Les carbonates courants sont la calcite (composant du calcaire) et la dolomite.

La pédosphère est la couche la plus externe de la Terre. Elle est composée de sol et est sujette au processus de formation du sol. Elle se trouve à la rencontre de la lithosphère, de l'atmosphère, de l'hydrosphère et de la biosphère. Actuellement, les zones arables représentent 13,31 % de la surface terrestre et seulement 4,71 % supportent des cultures permanentes. Près de 40 % de la surface terrestre est utilisée pour l'agriculture et l'élevage soit environ de cultures et de pâturage.

L'altitude de la surface terrestre de la Terre varie de dans la mer Morte à au sommet de l'Everest. L'altitude moyenne des terres émergées est de au-dessus du niveau de la mer.

L'abondance de l'eau sur la surface de la Terre est une caractéristique unique qui distingue la « planète bleue » des autres planètes du Système solaire. L'hydrosphère terrestre est principalement composée par les océans, mais techniquement elle inclut également les mers, les lacs, les rivières et les eaux souterraines jusqu'à une profondeur de . La Challenger Deep de la fosse des Mariannes dans l'océan Pacifique est le lieu immergé le plus profond avec une profondeur de .

La masse des océans est d'environ , soit environ 1/e de la masse totale de la Terre. Les océans couvrent une superficie de avec une profondeur moyenne de , soit un volume estimé à . Environ 97,5 % de l'eau terrestre est salée. Les 2,5 % restants sont composés d'eau douce, mais environ 68,7 % de celle-ci est immobilisée sous forme de glace.

La salinité moyenne des océans est d'environ de sel par kilogramme d'eau de mer (35 ‰). La plupart de ce sel fut libéré par l'activité volcanique ou par l'érosion des roches ignées. Les océans sont également un important réservoir de gaz atmosphériques dissous qui sont essentiels à la survie de nombreuses formes de vie aquatiques. L'eau de mer a une grande influence sur le climat mondial du fait de l'énorme réservoir de chaleur que constituent les océans. Des changements dans les températures océaniques peuvent entraîner des phénomènes météorologiques très importants comme El Niño.

La Terre est entourée d'une enveloppe gazeuse qu'elle retient par attraction gravitationnelle : l'atmosphère. L'atmosphère de la Terre est intermédiaire entre celle, très épaisse, de Vénus, et celle, très ténue, de Mars. La pression atmosphérique au niveau de la mer est en moyenne de , soit par définition. L'atmosphère est constituée de 78,09 % d'azote, de 20,95 % d'oxygène, de 0,93 % d'argon et de 0,039 % de dioxyde de carbone, ainsi que de divers autres gaz dont de la vapeur d'eau. La hauteur de la troposphère varie avec la latitude entre aux pôles et à l'équateur, avec quelques variations résultant de facteurs météorologiques et saisonniers.

La biosphère de la Terre a fortement altéré son atmosphère. La photosynthèse à base d'oxygène apparut il y a 2,7 milliards d'années et forma l'atmosphère actuelle, principalement composée d'azote et d'oxygène. Ce changement permit la prolifération d'organismes aérobies de même que la formation de la couche d'ozone bloquant les rayons ultraviolets émis par le Soleil. L'atmosphère favorise également la vie en transportant la vapeur d'eau, en fournissant des gaz utiles, en faisant brûler les petites météorites avant qu'elles ne frappent la surface et en modérant les températures. Ce dernier phénomène est connu sous le nom d'effet de serre : des molécules présentes en faible quantité dans l'atmosphère bloquent la déperdition de chaleur dans l'espace et font ainsi augmenter la température globale. La vapeur d'eau, le dioxyde de carbone, le méthane et l'ozone sont les principaux gaz à effet de serre de l'atmosphère terrestre. Sans cette conservation de la chaleur, la température moyenne sur Terre serait de par rapport aux actuels.

L'atmosphère terrestre n'a pas de limite clairement définie, elle disparaît lentement dans l'espace. Les trois-quarts de la masse de l'air entourant la Terre sont concentrés dans les premiers de l'atmosphère. Cette couche la plus inférieure est appelée la troposphère. L'énergie du Soleil chauffe cette couche et la surface en dessous, ce qui entraîne une expansion du volume atmosphérique par dilatation de l'air, ce qui a pour effet de réduire sa densité et ce qui l’amène à s'élever et a etre remplacé par de l'air plus dense, car plus froid. La circulation atmosphérique qui en résulte est un acteur déterminant dans le climat et la météorologie du fait de la redistribution de la chaleur, entre les différentes couches d'air qu'elle implique.

Les principales bandes de circulations sont les alizés dans la région équatoriale à moins de 30° et les vents d'ouest dans les latitudes intermédiaires entre 30° et 60°. Les courants océaniques sont également importants dans la détermination du climat, en particulier la circulation thermohaline qui distribue l'énergie thermique des régions équatoriales vers les régions polaires.

La vapeur d'eau générée par l'évaporation de surface est transportée par les mouvements atmosphériques. Lorsque les conditions atmosphériques permettent une élévation de l'air chaud et humide, cette eau se condense et retombe sur la surface sous forme de précipitations. La plupart de l'eau est ensuite transportée vers les altitudes inférieures par les réseaux fluviaux et retourne dans les océans ou dans les lacs. Ce cycle de l'eau est un mécanisme vital au soutien de la vie sur Terre et joue un rôle primordial dans l'érosion des reliefs terrestres. La distribution des précipitations est très variée, de plusieurs mètres à moins d'un millimètre par an. La circulation atmosphérique, les caractéristiques topologiques et les gradients de températures déterminent les précipitations moyenne sur une région donnée.

La quantité d'énergie solaire atteignant la Terre diminue avec la hausse de la latitude. Aux latitudes les plus élevées, les rayons solaires atteignent la surface suivant un angle plus faible et doivent traverser une plus grande colonne d'atmosphère. Par conséquent, la température moyenne au niveau de la mer diminue d'environ à chaque degré de latitude en s'éloignant de l'équateur. La Terre peut être divisée en ceintures latitudinaires de climat similaires. En partant de l'équateur, celles-ci sont les zones tropicales (ou équatoriales), subtropicales, tempérées et polaires. Le climat peut également être basé sur les températures et les précipitations. La classification de Köppen (modifiée par Rudolph Geiger, étudiant de Wladimir Peter Köppen) est la plus utilisée et définit cinq grands groupes (tropical humide, aride, tempéré, continental et polaire) qui peuvent être divisés en sous-groupes plus précis.

Au-dessus de la troposphère, l'atmosphère est habituellement divisée en trois couches, la stratosphère, la mésosphère et la thermosphère. Chaque couche possède un gradient thermique adiabatique différent définissant l'évolution de la température avec l'altitude. Au-delà, l'exosphère se transforme en magnétosphère, où le champ magnétique terrestre interagit avec le vent solaire. La couche d'ozone se trouve dans la stratosphère et bloque une partie des rayons ultraviolets, ce qui est important pour la vie sur Terre. La ligne de Kármán, définie comme se trouvant à au-dessus de la surface terrestre, est la limite habituelle entre l'atmosphère et l'espace.

L'énergie thermique peut accroître la vitesse de certaines particules de la zone supérieure de l'atmosphère qui peuvent ainsi échapper à la gravité terrestre. Cela entraîne une lente, mais constante « fuite » de l'atmosphère dans l'espace. Comme l'hydrogène non lié a une faible masse moléculaire, il peut atteindre la vitesse de libération plus facilement et disparaît dans l'espace à un rythme plus élevé que celui des autres gaz. La fuite de l'hydrogène dans l'espace déplace la Terre d'un état initialement réducteur à un état actuellement oxydant. La photosynthèse fournit une source d'oxygène non lié, mais la perte d'agents réducteurs comme l'hydrogène est considéré comme une condition nécessaire à l'accumulation massive d'oxygène dans l'atmosphère. Ainsi la capacité de l'hydrogène à quitter l'atmosphère terrestre aurait pu influencer la nature de la vie qui s'est développée sur la planète. Actuellement, la plus grande partie de l'hydrogène est convertie en eau avant qu'il ne s'échappe du fait de l'atmosphère riche en oxygène. La plupart de l'hydrogène s'échappant provient de la destruction des molécules de méthane dans la haute atmosphère.

Le champ magnétique terrestre a pour l'essentiel la forme d'un dipôle magnétique avec les pôles actuellement situés près des pôles géographiques de la planète. À l'équateur du champ magnétique, son intensité à la surface terrestre est de , avec un moment magnétique global de . Selon la théorie de la dynamo, le champ est généré par le cœur externe fondu où la chaleur crée des mouvements de convection au sein de matériaux conducteurs, ce qui génère des courants électriques. Ceux-ci produisent le champ magnétique terrestre. Les mouvements de convection dans le noyau externe sont organisés spatialement selon un mode spécifique de cette géométrie (colonnes de Busse), mais présentent néanmoins une composante temporelle relativement chaotique (au sens de la dynamique non-linéaire) ; bien que le plus souvent plus ou moins alignés avec l'axe de rotation de la Terre, les pôles magnétiques se déplacent et changent irrégulièrement d'alignement. Cela entraîne des inversions du champ magnétique terrestre à intervalles irréguliers, approximativement plusieurs fois par million d'années pour la période actuelle, le Cénozoïque. L'inversion la plus récente eut lieu il y a environ .

Le champ magnétique forme la magnétosphère qui dévie les particules du vent solaire et s'étend jusqu'à environ treize fois le rayon terrestre en direction du Soleil. La collision entre le champ magnétique et le vent solaire forme les ceintures de Van Allen, une paire de régions toroïdales contenant un grand nombre de particules énergétiques ionisées. Lorsque, à l'occasion d'arrivées de plasma solaire plus intenses que le vent solaire moyen, par exemple lors d'événements d'éjections de masse coronale vers la Terre, la déformation de la géométrie de la magnétosphère sous l'impact de ce flux solaire permet le processus de reconnexion magnétique, et une partie des électrons de ce plasma solaire entre dans l'atmosphère terrestre en une ceinture autour aux pôles magnétiques ; il se forme alors des aurores polaires, qui sont l'émission d'une lumière de fluorescence résultant de la désexcitation des atomes et molécules, essentiellement d'oxygène de la haute et moyenne atmosphère, excités par les chocs des électrons solaires.

La période de rotation relative de la Terre par rapport au Soleil est d'environ soit un jour solaire. La période de rotation relative de la Terre par rapport aux étoiles fixes, appelé son "jour stellaire" par l'International Earth Rotation and Reference Systems Service (IERS), est de de temps solaire moyen (UT1) ou . Du fait de la précession des équinoxes, la période de rotation relative de la Terre, son jour sidéral est de . Ainsi le jour sidéral est plus court que le jour stellaire d'environ 8,4 ms.

À part des météorites dans l'atmosphère et les satellites en orbite basse, le principal mouvement apparent des corps célestes dans le ciel terrestre est vers l'ouest à un rythme de 15°/h ou 15'/min. Pour les corps proches de l'équateur céleste, cela est équivalent à un diamètre apparent de la Lune ou du Soleil toutes les deux minutes.

Avant la création de la Lune, on suppose que l'axe de rotation de la Terre oscillait en permanence, ce qui rendait difficile l'apparition de la vie à sa surface pour causes de dérèglement climatique. Puis, une petite planète de la taille de Mars nommée Théia serait entrée en collision avec la Terre et aurait créé la Lune. L'apparition de cette dernière aurait stabilisé l'axe de rotation de la Terre.

La Terre orbite autour du Soleil à une distance moyenne d'environ de kilomètres suivant une période de solaires ou une année sidérale. De la Terre, cela donne un mouvement apparent du Soleil vers l'est par rapport aux étoiles à un rythme d'environ 1°/jour ou un diamètre solaire toutes les 12 heures. Du fait de ce mouvement, il faut en moyenne 24 heures, un jour solaire, à la Terre pour réaliser une rotation complète autour de son axe et que le Soleil revienne au plan méridien. La vitesse orbitale de la Terre est d'environ ().

La Lune tourne avec la Terre autour d'un barycentre commun tous les 27,32 jours par rapport aux étoiles lointaines. Lorsqu'il est associé au mouvement du couple Terre-Lune autour du Soleil, la période du mois synodique, d'une nouvelle lune à une nouvelle lune, est de 29,53 jours. Vu depuis le pôle céleste nord, le mouvement de la Terre, de la Lune et de leurs rotations axiales sont toutes dans le sens inverse de rotation. Depuis un point situé au-dessus du pôle nord de la Terre et du Soleil, la Terre semble tourner dans le sens trigonométrique autour du Soleil. Les plans orbitaux et axiaux ne sont pas précisément alignés, l'axe de la Terre est incliné de 23,4° par rapport à la perpendiculaire au plan Terre-Soleil et le plan Terre-Lune est incliné de 5° par rapport au plan Terre-Soleil. Sans cette inclinaison, il y aurait une éclipse toutes les deux semaines, avec une alternance entre éclipses lunaires et solaires.

La sphère de Hill ou la sphère d'influence gravitationnelle de la Terre a un rayon d'environ . C'est la distance maximale à laquelle l'influence gravitationnelle de la Terre est supérieure à celle du Soleil et des autres planètes. Pour orbiter autour de la Terre, les objets doivent se trouver dans cette zone où ils peuvent être perturbés par l'attraction gravitationnelle du Soleil.

La Terre, au sein du Système solaire, est située dans la Voie lactée et se trouve à du centre galactique. Elle est actuellement à environ 20 années-lumière du plan équatorial de la galaxie dans le bras d'Orion.

Du fait de l'inclinaison axiale de la Terre, la quantité de rayonnement solaire atteignant tout point de la surface varie au cours de l'année. Cela a pour conséquence des changements saisonniers dans le climat avec un été dans l'hémisphère nord lorsque le pôle nord pointe vers le Soleil et l'hiver lorsque le pôle pointe dans l'autre direction. Durant l'été, les jours durent plus longtemps et le Soleil monte plus haut dans le ciel. En hiver, le climat devient généralement plus froid et les jours raccourcissent. Au-delà du cercle Arctique, il n'y a aucun jour durant une partie de l'année, ce qui est appelé une nuit polaire. Dans l'hémisphère sud, la situation est exactement l'inverse.

Par convention astronomique, les quatre saisons sont déterminées par les solstices, lorsque le point de l'orbite où l'inclinaison vers ou dans la direction opposée du Soleil est maximale et les équinoxes lorsque la direction de l'inclinaison de l'axe et la direction au Soleil sont perpendiculaires. Dans l'hémisphère nord, le solstice d'hiver a lieu le 21 décembre, le solstice d'été est proche du 21 juin, l'équinoxe de printemps a lieu autour du 20 mars et l'équinoxe d'automne vers le 21 septembre. Dans l'hémisphère sud, la situation est inversée et les dates des solstices d'hiver et d'été et celles des équinoxes de printemps et d'automne sont inversées.

L'angle d'inclinaison de la Terre est relativement stable au cours du temps. L'inclinaison entraine la nutation, un balancement périodique ayant une période de 18,6 années. L'orientation (et non l'angle) de l'axe de la Terre évolue et réalise un cycle complet en . Cette précession des équinoxes est la cause de la différence de durée entre une année sidérale et une année tropique. Ces deux mouvements sont causés par le couple qu'exercent les forces de marées de la Lune et du Soleil sur le renflement équatorial de la Terre. De plus, les pôles se déplacent périodiquement par rapport à la surface de la Terre selon un mouvement connu sous le nom d'oscillation de Chandler.

À l'époque moderne, le périhélie de la Terre a lieu vers le 3 janvier et l'aphélie vers le 4 juillet. Ces dates évoluent au cours du temps du fait de la précession et d'autres facteurs orbitaux qui suivent un schéma cyclique connu sous le nom de paramètres de Milanković.

La Terre a un seul satellite naturel « permanent » connu, la Lune, située à environ de la Terre. Relativement grand, son diamètre est environ le quart de celui de la Terre. Au sein du Système solaire, c'est l'un des plus grands satellites naturels (après Ganymède, Titan, Callisto et Io) et le plus grand d'une planète non gazeuse. De plus, c'est la plus grande lune du Système solaire par rapport à la taille de sa planète (même si Charon est relativement plus grand par rapport à la planète naine Pluton). Elle est relativement proche de la taille de la planète Mercure (environ les trois quarts du diamètre de cette dernière). Les satellites naturels orbitant autour des autres planètes sont communément appelés « lunes » en référence à la Lune de la Terre.

L'attraction gravitationnelle entre la Terre et la Lune cause les marées sur Terre. Le même effet a eu lieu sur la Lune, de sorte que sa période de rotation est identique au temps qu'il lui faut pour orbiter autour de la Terre, et qu'elle présente ainsi toujours la même face vers la Terre. En orbitant autour de la Terre, différentes parties du côté visible de la Lune sont illuminées par le Soleil, causant les phases lunaires.

À cause du couple des marées, la Lune s'éloigne de la Terre à un rythme d'environ par an, produisant aussi l'allongement du jour terrestre de 23 microsecondes par an. Sur plusieurs millions d'années, l'effet cumulé de ces petites modifications produit d'importants changements. Durant la période du Dévonien, il y a approximativement d'années, il y avait ainsi dans une année, chaque jour durant 21,8 heures.

La Lune aurait eu une influence importante dans le développement de la vie en régulant le climat de la Terre. Les preuves paléontologiques et les simulations informatiques montrent que l'inclinaison de l'axe de la Terre est stabilisée par les effets de marées avec la Lune. Certains scientifiques considèrent que sans cette stabilisation contre les couples appliqués par le Soleil et les planètes sur le renflement équatorial, l'axe de rotation aurait pu être très instable, ce qui aurait provoqué des changements chaotiques au cours des millions d'années, comme cela semble avoir été le cas pour Mars.

La Lune est aujourd'hui à une distance de la Terre telle que, vue de la Terre, notre satellite a la même taille apparente (taille angulaire) que le Soleil. Le diamètre angulaire (ou l'angle solide) des deux corps est quasiment identique, car même si le diamètre du Soleil est 400 fois plus important que celui de la Lune, celle-ci est 400 fois plus rapprochée de la Terre que ce dernier. Ceci permet des éclipses solaires totales et annulaires sur Terre.

Le consensus actuel sur les origines de la Lune, l'hypothèse de l'impact géant, est celle d'un impact géant entre un planétoïde de la taille de Mars, appelé Théia, et la Terre (ou proto-Terre) nouvellement formée. Cette hypothèse explique en partie le fait que la composition de la Lune ressemble particulièrement à celle de la croûte terrestre.

Les modèles informatiques des astrophysiciens Mikael Granvik, Jérémie Vaubaillon et Robert Jedicke suggèrent que des « satellites temporaires » devraient être tout à fait communs et que . Ces objets resteraient en orbite durant en moyenne dix mois avant de revenir dans une orbite solaire.

L'une des premières mentions dans la littérature scientifique d'un satellite temporaire est celle de Clarence Chant lors de la grande procession météorique de 1913 : 

Dans les faits, un tel objet est connu. En effet, entre 2006 et 2007, était effectivement temporairement en orbite autour de notre planète plutôt qu'autour du Soleil.

Le , des chutes de météorites ont lieu sur les îles Britanniques et l'Amérique du Nord. L'origine de ces météorites pourrait être un petit corps en orbite autour de la Terre.

En janvier 2014, on compte satellites artificiels en orbite autour de la Terre (contre 931 en 2011).

2010 TK est le premier astéroïde troyen connu de la Terre, autour du point de Lagrange L du couple Terre-Soleil, 60° en avance par rapport à la Terre sur son orbite autour du Soleil.

Les nuages de Kordylewski graviteraient aux points L et L du système Terre-Lune, mais leur existence reste incertaine à ce jour.

La Terre a au moins sept satellites co-orbitaux :

Une planète qui peut abriter la vie est dite habitable même si la vie n'en est pas originaire. La Terre fournit de l'eau liquide, un environnement où les molécules organiques complexes peuvent s'assembler et interagir et suffisamment d'énergie pour maintenir un métabolisme. La distance de la Terre au Soleil, de même que son excentricité orbitale, sa vitesse de rotation, l'inclinaison de son axe, son histoire géologique, son atmosphère accueillante et un champ magnétique protecteur contribuent également aux conditions climatiques actuelles à sa surface.

Les formes de vie de la planète sont parfois désignées comme formant une « biosphère ». On considère généralement que cette biosphère a commencé à évoluer il y a environ 3,5 milliards d'années. La biosphère est divisée en plusieurs biomes, habités par des groupes similaires de plantes et d'animaux. Sur terre, les biomes sont principalement séparés par des différences de latitudes, l'altitude et l'humidité. Les biomes terrestres se trouvant au-delà des cercles Arctique et Antarctique, en haute altitude ou dans les zones très arides sont relativement dépourvus de vie animale et végétale alors que la biodiversité est maximale dans les forêts tropicales humides.

La Terre fournit des ressources qui sont exploitables par les humains pour diverses utilisations. Certaines ne sont pas renouvelables, comme les combustibles fossiles, qui sont difficiles à reconstituer sur une courte échelle de temps. D'importantes quantités de combustibles fossiles peuvent être obtenues de la croûte terrestre, comme le charbon, le pétrole, le gaz naturel ou les hydrates de méthane. Ces dépôts sont utilisés pour la production d'énergie, et en tant que matière première pour l'industrie chimique. Les minerais se sont formés dans la croûte terrestre et sont constitués de divers éléments chimiques utiles comme les métaux.

La biosphère terrestre produit de nombreuses ressources biologiques pour les humains, comme de la nourriture, du bois, des médicaments, de l'oxygène et assure également le recyclage de nombreux déchets organiques. Les écosystèmes terrestres dépendent de la couche arable et de l'eau douce, tandis que les écosystèmes marins sont basés sur les nutriments dissous dans l'eau. Les humains vivent également sur terre en utilisant des matériaux de construction pour fabriquer des abris. En 1993, l'utilisation humaine des terres était approximativement répartie ainsi :

La superficie irriguée estimée en 1993 était de .

D'importantes zones de la surface terrestre sont sujettes à des phénomènes météorologiques extrêmes comme des cyclones, des ouragans ou des typhons qui dominent la vie dans ces régions. De 1980 à 2000, ces événements ont causé environ par an. De même, de nombreuses régions sont exposées aux séismes, aux glissements de terrain, aux éruptions volcaniques, aux tsunamis, aux tornades, aux dolines, aux blizzards, aux inondations, aux sécheresses, aux incendies de forêt et autres calamités et catastrophes naturelles.

De nombreuses régions sont sujettes à la pollution de l'air et de l'eau créée par l'homme, aux pluies acides, aux substances toxiques, à la perte de végétation (surpâturage, déforestation, désertification), à la perte de biodiversité, à la dégradation des sols, à l'érosion et à l'introduction d'espèces invasives.

Selon les Nations unies, un consensus scientifique existe qui lie les activités humaines au réchauffement climatique du fait des émissions industrielles de dioxyde de carbone, et plus généralement des gaz à effet de serre. Cette modification du climat risque de provoquer la fonte des glaciers et des calottes glaciaires, des amplitudes de température plus extrêmes, d'importants changements de la météorologie et une élévation du niveau de la mer.

La Terre compte approximativement milliards d'habitants en 2015. Les projections indiquent que la population mondiale atteindra d'habitants en 2050. La plupart de cette croissance devrait se faire dans les pays en développement. La densité de population humaine varie considérablement autour du monde, mais une majorité vit en Asie. En 2020, 60 % de la population devrait vivre dans des zones urbaines plutôt que rurales.

On estime que seul un-huitième de la surface de la Terre convient pour les humains ; trois-quarts de la Terre sont recouverts par les océans et la moitié des terres émergées sont des déserts (14 %), des hautes montagnes (27 %) ou d'autres milieux peu accueillants. L'implantation humaine permanente la plus au nord est Alert sur l'île d'Ellesmere au Canada (82°28′N). La plus au sud est la station d'Amundsen-Scott en Antarctique située près du pôle sud (90°S).

La totalité des terres émergées, à l'exception de certaines zones de l'Antarctique et du Bir Tawil non revendiqué que ce soit par l'Égypte ou le Soudan, sont revendiquées par des nations indépendantes. En 2011, on compte 204 États souverains dont 193 sont membres des Nations unies. De plus, il existe 59 territoires à souveraineté limitée et de nombreuses entités autonomes ou contestées. Historiquement la Terre n'a jamais connu une souveraineté s'étendant sur l'ensemble de la planète même si de nombreuses nations ont tenté d'obtenir une domination mondiale et ont échoué.

L'Organisation des Nations unies est une organisation internationale qui fut créée dans le but de régler pacifiquement les conflits entre nations. Les Nations unies servent principalement de lieu d'échange pour la diplomatie et le droit international public. Lorsque le consensus est obtenu entre les différents membres, une opération armée peut être envisagée.

Le premier astronaute humain à avoir orbité autour de la Terre fut Youri Gagarine le 12 avril 1961. Au total, en 2015, environ 550 personnes se sont rendues dans l'espace et douze d'entre elles ont marché sur la Lune. En temps normal, au début du , les seuls humains dans l'espace sont ceux se trouvant dans la station spatiale internationale qui est habitée en permanence, les stations spatiales chinoises n'ayant eu dans les années 2010 que des séjours de courtes durées. Les astronautes de la mission Apollo 13 sont les humains qui se sont le plus éloignés de la Terre avec en 1970.

Dans le passé, la croyance en une terre plate fut contredite par les observations et par les circumnavigations et le modèle d'une Terre sphérique s'imposa.

À la différence des autres planètes du Système solaire, l'humanité n'a pas considéré la Terre comme un objet mobile en rotation autour du Soleil avant le . La Terre a souvent été personnifiée en tant que déité, en particulier sous la forme d'une déesse. Les mythes de la création de nombreuses religions relatent la création de la Terre par une ou plusieurs divinités.

Quelques groupes religieux souvent affiliés aux branches fondamentalistes du protestantisme et de l'islam avancent que leur interprétation des mythes de la création dans les textes sacrés est la vérité et que celle-ci devrait être considérée comme l'égale des hypothèses scientifiques conventionnelles concernant la formation de la Terre et le développement de la vie voire devrait les remplacer. De telles affirmations sont rejetées par la communauté scientifique et par les autres groupes religieux.

La vision humaine concernant la Terre a évolué depuis les débuts de l'aérospatiale et la biosphère est maintenant vue selon une perspective globale. Cela est reflété dans le développement de l'écologie qui s'inquiète de l'impact de l'humanité sur la planète.

Dès 1945, Paul Valéry, dans son ouvrage "Regards sur le monde actuel", estimait que « le temps du monde fini commence ». Par « monde », il n'entendait pas le monde-univers des Anciens, mais "notre monde" actuel, c'est-à-dire, la Terre et l'ensemble de ses habitants.

Bertrand de Jouvenel a évoqué la finitude de la Terre dès 1968.

Le philosophe Dominique Bourg, spécialiste de l'éthique du développement durable, évoque la découverte de la finitude écologique de la Terre dans "la nature en politique ou l'enjeu philosophique de l'écologie" (2000). Estimant que cette finitude est suffisamment connue et prouvée pour qu'il soit inutile de l'illustrer, il souligne qu'elle a entraîné dans nos représentations un changement radical de la relation entre l'universel et le singulier. Alors que le paradigme moderne classique postulait que l'universel commandait le singulier, et le général le particulier, on ne peut pas y réduire la relation entre le planétaire et le local. Dans l'univers systémique de l'écologie, la biosphère (le planétaire) et les biotopes (le local) sont interdépendants. Cette interdépendance du local et du planétaire fait voler en éclats le principe moteur de la modernité, qui tendait à abolir toute particularité locale au profit de principes généraux, ce en quoi le projet moderne fut proprement utopique. La preuve expérimentale du raccordement symbolique de l'écologie à la culture a été fournie par les réactions des premiers astronautes qui, en 1969, ont pu observer notre planète à partir de la Lune. Ils dirent que la Terre était belle, précieuse, et fragile. C'est-à-dire que l'Homme a le devoir de la protéger.

La finitude écologique de la Terre est une question devenue tellement prégnante que certains philosophes (Heidegger, Grondin, Schürch) ont pu parler d'une éthique de la finitude.

Les concepts d'empreinte écologique et de biocapacité permettent d'appréhender les problèmes liés à la finitude écologique de la Terre.



</doc>
<doc id="3029" url="https://fr.wikipedia.org/wiki?curid=3029" title="Taekwondo">
Taekwondo

Le taekwondo est un art martial d'origine sud-coréenne, dont le nom peut se traduire par "La voie du pied et du poing".

Le taekwondo, dont le nom a été proposé en 1955 par le général Choi Hong Hi, est le fruit de la fédération progressive, à partir des années 1950, après l'occupation japonaise de la Corée, de différentes écoles d'arts martiaux coréennes qui enseignaient le karaté. Sa création et son développement sont intimement liés à la promotion du nationalisme étatique coréen. L'unification n'est cependant pas complète car deux grandes fédérations cohabitent encore, l'International Taekwon-Do Federation (ITF), qui revendique actuellement 50 millions d'adhérents, et la Fédération mondiale de taekwondo (FMT), qui a popularisé auprès de plus de 80 millions de personnes une pratique du taekwondo moins axée sur la self-défense et plus sur le sport de combat, interdisant notamment les coups de poing au visage.

Le taekwondo se distingue des autres arts martiaux, surtout dans sa forme, par le haut degré de spécialisation de ses pratiquants en techniques de coups de pieds bien plus que dans d'autres techniques, par les nombreuses protections utilisées lors des compétitions de combat ainsi que par le fait que, depuis qu'il a été inclus au programme des Jeux olympiques d'été en 2000, c’est le seul sport olympique de combat autorisant les frappes avec le pied.

Le taekwondo est un art martial d'origine sud-coréenne, qui se pratique, en général, sans armes. Son nom, taekwondo, selon la romanisation révisée du coréen, qui s'écrit à l'origine 태권도 en hangeul et 跆拳道 en hanja, et dont la prononciation api est [tʰɛk͈wʌndo], peut se traduire par "La voie du coup de pied et du coup de poing" ("tae" (Hangul:태 hanja: 跆), frapper du pied - "kwon" (Hangul: 권, hanja: 拳), frapper du poing - "do" (Hangul: 도, hanja: 道), méthode, art de vivre, voie spirituelle). De cette définition découle l'idée que le taekwondo est non seulement un art martial mais aussi une manière d'entraîner son esprit et d'atteindre la maîtrise de soi par des mouvements de combat.

Le pratiquant de taekwondo est appelé un taekwondoïste (et ce, même si le terme d'origine est "taekwondoin"). La salle d'entraînement est appelée un "dojang". Il est possible de pratiquer le taekwondo quel que soit son âge. En tant que sport, il fait travailler l'endurance et la souplesse et augmente la force physique.

Cet art martial est basé sur des techniques d'attaque où le pratiquant concentre son énergie sur telles que le bol du pied ou la tête des phalanges avec lesquelles il vise les points faibles de son adversaire et des techniques de blocage des attaques adverses. Les taekwondoïstes utilisent des techniques de coups de pied spectaculaires, notamment retournés et sautés, dont la fréquence d'usage est caractéristique de la discipline.

Il existe en 2010 de pratiquants du taekwondo dans 180 pays. par le souhait de maintenir leur forme physique permettant notamment de lutter contre le stress, par les techniques d'autodéfense enseignées par ce sport, l'attrait pour la compétition ainsi que par l'enrichissement spirituel qui peut être amené dans l'enseignement des valeurs du taekwondo telles que la modestie, le respect ou le goût de l'effort.

Les origines du taekwondo sont à la fois culturelles et politiques. D'un point de vue culturel, le taekwondo est une unification des pratiques de plusieurs écoles sud-coréennes des années 1950 inspirées par le karaté Shotokan (développé au Japon) et certains éléments caractéristiques des arts martiaux coréens (en particulier le taekkyon). D'un point de vue politique, le taekwondo a été un outil de propagande nationaliste dont le but a été d'exalter le patriotisme de la jeune nation sud-coréenne à la suite de l'occupation japonaise et du conflit avec la Corée du Nord.

Le taekwondo a été nommé et codifié en Corée du Sud entre la fin des années 1950 et le début des années 1960.

À la suite de la diffusion de cet art martial en Corée du Nord par les responsables de la première organisation internationale, l'ITF, en 1972, une nouvelle organisation concurrente, la WTF, a tourné le dos aux membres de l'ITF.

Cette scission de nature politique, qui perdure depuis, ainsi que les motivations idéologiques à l'origine de la création du taekwondo, ont induit une forte propagande et une déformation volontaire des faits historiques dans chacune de ces organisations, notamment au sujet des origines et de la création du taekwondo.

Le taekwondo est généralement présenté comme l'héritier des arts martiaux coréens antiques, comme le "taekkyon" et le "" ; on peut en effet retrouver certains points communs entre les techniques utilisées dans ces arts martiaux et celles du taekwondo, surtout du point de vue des techniques de jambes, prépondérantes. Toutefois, ces points communs sont considérés par certains historiens de la discipline comme insuffisants et surtout postérieurs à la volonté politique de création d'un art martial spécifiquement national, après l'occupation japonaise de la Corée, d'autant plus que la pratique du taekkyon avait quasiment disparu au début du , à tel point que ces historiens qualifient la filiation entre le taekwondo et ces arts anciens de mensonger, d'argument de propagande.

Les origines établies et incontestées du taekwondo remontent à la fédération progressive, entre la fin des années 1950 et le début des années 1960, des principales écoles d'arts martiaux coréennes ("kwans"), qui enseignaient alors le , appelé "Tang Soo Do" () ou "Kong Soo Do" (), une pratique martiale issue du karaté Shotokan développé au Japon, notamment parce que la pratique des arts martiaux coréens avait été interdite par l'occupant, tout comme beaucoup d'autres éléments de la culture coréenne. À partir de 1955, sous l'impulsion du gouvernement coréen, et en particulier du président Syngman Rhee, dans le but politique d'exalter le patriotisme de la jeune nation sud-coréenne, le développement d'un art martial national et sa diffusion à l'ensemble des écoles d'arts martiaux coréennes ont été confiés principalement au général Choi Hong Hi, qui dirigeait le plus ancien des kwans, l’école Chung Do Kwan, et la principale école militaire d'arts martiaux, nommée Oh Do Kwan. Choi Hong Hi proposa, avec son instructeur , le nom de Tae Kwon Do, le , et utilisa son influence pour imposer progressivement ce terme, qui commença à être popularisé en Corée principalement à partir de 1959, lorsque plusieurs écoles et organisations se réunirent sous l’égide de la (KTA).

Une des premières importantes étapes du processus d'unification est la promotion internationale du Taekwon-Do par des équipes de démonstration, composées de ses représentants les plus techniques et spectaculaires. En mars 1959, une première tournée de démonstration fait découvrir à Taïwan et au Sud-Viêt Nam ce nouvel art martial.

Nommé ambassadeur en Malaisie, le général Choi abandonne la présidence de la KTA pour se consacrer à la diffusion du Taekwondo dans ce pays, après avoir effectué une démonstration dans un stade à la demande du premier ministre Malais. Ce travail de promotion aboutira à la création de l’Association malaise de Taekwondo en 1963. 

Dès le départ en Malaisie du général Choi (1961), le président Park Chung Hee (박정희) décida d'ordonner une réunification des différentes écoles. En effet, hormis les élèves des écoles Chundokwan et Ohdokwan, seule une faible minorité pratiquaient le taekwondo tel que défini par le général Choi, et de nombreux maîtres, insatisfaits du nom « taekwondo » continuèrent à enseigner sous les noms « Gongsoodo », « Dangsoodo » et « Soobahkdo ». Hwang Kee (황기), le principal rival de la KTA, avait créé sa propre fédération : Korea Dangsoodo association, qui changera plus tard en Korea Soobahkdo Association. Le 16 septembre 1961, une nouvelle réunion a lieu, et les différents représentants tombent d'accord sur le terme « taesoodo » qui combine les termes de taekkyon, gongsoodo, soobahkdo et dangsoodo. La KTA est donc renommée Korea Taesoodo Association.

En 1961-62, le taekwondo est pratiqué par les militaires coréens autant que par la population civile de ce pays, mais aussi par les forces armées américaines stationnées en Corée. Ces mêmes années, le taekwondo est introduit à l’académie militaire de West Point aux États-Unis.

En juin 1963, une démonstration a lieu dans le bâtiment des Nations unies, à New York, et le Tae Kwon Do est choisi pour l’entraînement des militaires du Sud Viêt Nam.Toujours en 1963, les associations nationales de Singapour et de Brunei sont créées.

En 1965, à la retraite, Choi dirige une nouvelle tournée de démonstration internationale pour assurer la promotion du Taekwon-Do en Allemagne de l’Ouest, Italie, Égypte, Turquie, Émirats arabes unis, Malaisie et Singapour. Les membres de son équipe de démonstration étaient Han Cha Kyo ( dan à l’époque), Kim Jun Kun ( dan), Kwon Jai Hwa ( dan) et Park Jong Soo ( dan). À l'issue de cette tournée, des associations nationales sont créées dans ces pays.

La même année, à son retour en Corée, le général Choi redevient président de la KTA. Le 5 août 1965, il organise un vote pour évincer du nom de l'organisation le terme de taesoodo et restaurer celui de taekwondo. Les conditions de ce vote sont restées douteuses, car le nom taekwondo, qui ne contentait que l'Ohdokwan et le Chungdokwan, ne fut choisi qu'avec une voix d'écart. La KTA fut donc renommée Korea Taekwondo Association et l'usage du terme taekwondo s'imposa alors définitivement.

Peu après, en 1966, Choi quitte la KTA, sous la pression des leaders d'autres écoles d'arts martiaux comme Lee Chong-woo et Uhm Woon-kyu qui le considéraient comme un "fauteur de troubles permanent", en ayant négocié la possibilité de fonder sa propre fédération, d'envergure internationale, l'International Taekwon-Do Federation (ITF), qu'il crée effectivement le , à l'hôtel Chosun de Séoul.

Les pays fondateurs de l’ITF sont donc la Corée, le Viêt Nam, la Malaisie, Singapour, l’Allemagne de l’Ouest, les États-Unis, la Turquie, l’Italie et les Émirats arabes unis. Choi commence à cette époque à clamer que le taekwondo est le « sport national » de la Corée .

Afin que l’ITF bénéficie d'une influence politique suffisante pour rivaliser avec la KTA, Choi offre le titre de directeur honoraire de l’ITF à Kim Jong-pil, créateur des services de renseignement de la KCIA.

En 1968, Choi visite la France à l’occasion du symposium sur le sport militaire et y organisa une démonstration devant les représentants de 32 pays. La même année, le Royaume-Uni forme une association nationale de Taekwondo, et le général se rend en Espagne, au Canada, aux Pays-Bas, en Belgique et en Inde. En 1969, Choi effectue une tournée dans 29 pays afin d'y rencontrer des instructeurs de ces différents pays et d'effectuer les prises de vues qui illustrent la première édition de (1972).

De son côté, la KTA commença à fonder un programme technique commun et nomma un comité de création de formes, composé de Kwak Kun Sik (Chung Do Kwan), Lee Yong Sup (Song Moo Kwan), Park Hae Man (Chung Do Kwan), Hyun Jong Myung (Oh Do Kwan) et Kim Soon Bae (Chang Moo Kwan). Ils créèrent les poumsés (품새) Palgwae et Yudanja (Koryeo (고려) à Ilyeo (일여)), mais ces poumsés furent créés sans la participation de deux Kwan originaux, Ji Do Kwan et Moo Duk Kwan, fusionnés au sein de la Korea Soo Bahk Do association. Quelques années plus tard, sous l'impulsion de Chong Hong Soo, Im Young Taek (Moo Duk Kwan) et Lee Chong Woo (Ji Do Kwan), une partie de ces Kwan rejoignit la KTA (les Jidokwan Lee Chong Woo et Bae Young Ki, et le Moo Duk Kwan Han Yong Tae), qui décida de refaire les poumsés, en créant de nouveaux : les taegeuk (태극).

En 1971, le gouvernement décida de construire le Kukkiwon (국기원), siège mondial de la KTA, qui fut fondé en 1972 à Séoul.

En 1972, Choi introduit le Taekwon-Do en Bolivie, République dominicaine, Haïti et Guatemala. Mais confronté à une situation politique particulièrement difficile dans son pays, il est contraint à l’exil : en effet, le gouvernement sud-coréen désapprouve une initiative de Choi de faire une démonstration de Taekwondo en Corée du Nord, où il se rend en 1966. Le développement du Taekwon-Do en Corée du Nord est dû notamment à Yoon Byung-in.

Afin de ne pas perdre la direction de l'ITF, et avec l’accord des pays membres de l’organisation, Choi déplace le siège de l’ITF à Toronto, au Canada, d’où il espère pouvoir diffuser plus aisément le Taekwondo dans les pays de l’Est.

Le gouvernement de Corée du Sud, qui ne souhaite pas, lui non plus, perdre le contrôle de cet art martial, devenu un extraordinaire outil de propagande nationaliste, crée la WTF (World Taekwondo Federation ou Fédération Mondiale de Taekwondo) en 1973. Les premiers championnats du monde de taekwondo WTF sont organisés en 1973 et seront par la suite organisés tous les deux ans.

Bénéficiant de l’appui du gouvernement, cette fédération se développe très rapidement, surtout dans les pays de l’Ouest, qui sont alors engagés dans cette période de guerre froide auprès des États-Unis contre le bloc soviétique auquel appartient la Corée du Nord pendant la guerre froide. Elle popularise une pratique du taekwondo moins axée sur la self-défense et plus sur le sport de combat, interdisant notamment les coups de poing au visage, dans plus de 200 pays. Elle revendique désormais plus de 80 millions d'adhérents.

Dès lors, l'ITF et la WTF rivaliseront de manière plus ou moins conflictuelle, proportionnellement à l'intensité de la guerre froide et des conflits entre la Corée du Sud et du Nord.

Après la création de la WTF, le général Choi continue son travail de développement du Taekwon-Do originel par le biais de l'ITF. Une nouvelle équipe de démonstration part en tournée dès novembre 1973. Maître Park Jong Soo, désormais dan, en fait toujours partie. Il est accompagné par Maître Rhee Ki Ha, Maître Park Sun Tae et Maître Choi Chang Keun. Ils visitent 23 pays d’Europe, d’Asie, du Moyen-Orient et d’Afrique, et établissent des organisations nationales dans 5 d’entre eux.

Sans s’arrêter de voyager, le général Choi assiste à l’organisation du championnat du Monde en 1974 à Montréal (Canada) et du championnat d’Europe à Amsterdam (Pays-Bas) en 1976.

En 1978, une nouvelle équipe de démonstration est constituée. Elle comprend Maître Choi Chank Keun, Park Jung Tae, Rhee Ki Ha et Leong Wei Meng. Ces deux derniers, ainsi que Park Jong Soo, ont aujourd’hui le grade de Grand Maître.

En 1979, l’AETF (All Europe Taekwon-Do Federation) est fondée à Oslo (Norvège).

Les équipes de démonstrations se succèdent, voyageant dans le monde entier pour introduire le Taekwon-Do.

L’année 1980 est une année historique pour le Taekwon-Do et pour le général Choi, puisqu’une équipe de 15 membres (comprenant son fils maître Choi Jung Hwa) effectue une tournée en Corée du Nord, pays natal du Général Choi.

C’est la première fois que le Taekwon-Do est introduit dans ce pays.

En 1981, une équipe de démonstration composée de nord-coréens et de sud-coréens est présentée par le Général Choi.

En 1985, le siège de l’ITF s'installe à Vienne.

Le décès du président de l'ITF, le général Choi Hong Hi, survient le 15 juin 2002. Sa succession est disputée et occasionne la scission de l'ITF en 3 organisations concurrentes : une ITF dont le siège se situe à Londres et présidée par , fils unique du général Choi Hong Hi, une autre ITF dont le siège se situe à Pyongyang et dirigée par Jang Ung à partir du congrès de Pyongyang du 22 septembre 2002 et enfin une dernière ITF dont le siège se situe à Benidorm et dirigée par à partir du Congrès de Varsovie du 13 juin 2003 puis par Pablo Trajtenberg depuis 2011.

Le taekwondo ITF revendique aujourd'hui 50 millions d'adhérents dans 127 pays.

Le taekwondo WTF est présenté comme sport de démonstration aux Jeux olympiques de Séoul en 1988 et à ceux de Barcelone en 1992 avant de devenir sport olympique à partir des jeux olympiques d'été de 2000, ce qui intensifie sa popularisation par rapport au Taekwon-Do ITF.

En préparation du second sommet intercoréen de 2007, des rencontres ont été organisées entre les dirigeants de l'ITF (à laquelle sont affiliés les athlètes nord-coréens), alors présidée par Jang Ung, et de la WTF (proche des instances officielles sud-coréennes), dirigée alors par Choe Chung-won, afin d'unifier les deux fédérations mondiales de taekwondo.

Pendant les Jeux olympiques de la jeunesse d'été de 2014, un accord (Memorandum of Understanding) est signé entre la WTF et l'ITF pour que les membres de chacune de ces deux organisations puissent participer aux compétitions organisées par l'autre ; en particulier, cet accord permet aux membres de l'ITF de participer aux compétitions olympiques suivantes, selon les règles définies par la WTF.

En 2017, à l'occasion du championnat du monde organisé par la WTF à Muju, l'ITF a déclaré qu'elle enverrait une délégation de 36 membres, dont Jang Ung et une équipe de démonstration.

Le 23 juin 2017, la World Taekwondo Federation annonce son renommage en World Taekwondo (WT) pour éviter les "connotations négatives" de l'acronyme WTF.

La tenue de base du taekwondo est un dobok, généralement blanc. L'ampleur du pantalon permet de ne pas gêner les coups de pieds, même avec un écartement maximal des jambes. Il est fermé par une ceinture nouée par un nœud plat. Lors des entraînements au combat en contact partiel ou plein, des protections sont rajoutées à ce dobok.

Le taekwondo est surtout célèbre pour ses techniques de pied spectaculaires.

Contrairement à de nombreuses idées reçues, en particulier à cause des règles de compétition, qui n'autorisent les coups qu'au-dessus de la ceinture, les différents coups de pied peuvent être exécutés à tous les niveaux : bas (jambes ou éventuellement organes génitaux), moyen (plexus solaire ou côtes flottantes), ou haut (visage ou gorge).

L'entraînement aux techniques de coups de pied se réalise souvent à l'aide de raquettes de frappe, pour améliorer la précision des coups et la réactivité. Les paos sont utilisés pour s'entraîner à taper plus en puissance et à enchaîner des séries de coups identiques rapides.

Quasiment pas utilisées en compétition, elles portent des noms plus techniques, composées :

En plus des techniques traditionnelles, il y a aussi :

Les formes de taekwondo, qui sont, comme dans d'autres arts martiaux, des enchaînements de techniques exécutées sans adversaire, s'appellent des poumsés (terme officiel WTF), (terme officiel ITF) ou hyeong (terme originel).

Chaque forme possède son propre rythme qui doit être en rapport avec les différents enchaînements, ainsi chaque blocage devra être instinctivement suivi de la contre-attaque.

En taekwondo WTF, le poumsé débute et se termine par la position "Tchaliot Seugui" suivi du salut "Kyongnye". Il s’annonce à haute voix. Il se déroule suivant un diagramme différent, selon sa complexité. Le point de départ doit être également celui du retour après la prestation. Un poumsé doit être réalisé avec intensité, de manière à faire sentir une réelle impression de combat dans l’exécution des différents mouvements avec la plus grande efficacité contrôlée. Le poumsé s’exécute dignement, avec un dobok propre et une ceinture correctement nouée.

La réalisation des formes a une importance capitale pour monter en grade. En taekwondo ITF comme en WTF, il existe 8 formes normales et 9 supérieures.

Les différents styles de taekwondo reposent sur différentes approches philosophiques. Cependant, la plupart se réfèrent aux cinq principes du taekwondo définis par Choi Hong Hi en s'inspirant des "cinq préceptes de la vie séculaire" du Hwarang-do, et auxquels les élèves du taekwondo ITF doivent prêter serment : courtoisie, intégrité, persévérance, contrôle de soi et "esprit indomptable".

En taekwondo, les capacités individuelles de développement personnel, d'engagement et de technique sont évaluées par deux échelles de promotion : les grades ("keu"p), d'abord, puis les degrés ("dan"). La progression du débutant commence par un grade élevé (le pour les adultes) et se termine avec le , tandis que les degrés commencent au contraire par le et finissent avec le (il est possible d'obtenir un en taekwondo, mais il n'est décerné qu'à titre posthume).

Un âge minimal est requis pour l'obtention des différents degrés (en France, 16 ans pour le , 16 ans pour le , 18 ans pour le , 21 ans pour le , etc.). Si le candidat n'a pas l'âge requis, il peut obtenir un grade poom (du au poom). Ce grade n'est jamais converti en degré mais permet le passage du degré supérieur, dont l'obtention nécessite le passage d'un examen une fois que le pratiquant a atteint l'âge minimal requis : par exemple, un jeune sportif ayant le poom, s'il participe avec succès à l'examen de grade supérieur, obtiendra directement le s'il a 16 ans ou, s'il n'a pas l'âge minimal requis, obtiendra le poom (qui lui permettra de prétendre au à partir de ses 18 ans).

D'une fédération à l'autre, la correspondance entre grades et ceintures peut varier. Ainsi, tandis que la fédération française FFTDA utilise sept couleurs différentes pour les enfants et quatre pour les adultes, la fédération belge ABFT en compte cinq pour les adultes.
Les degrés sont en revanche, pour toutes les fédérations, exprimés par la ceinture noire. Pour les enfants, qui ne peuvent pas passer de degré mais seulement les 3 grades de poom, elle est remplacée, à partir de l'âge de 14 ans minimum, par une ceinture bicolore rouge et noire (grade "Il Poom").

Les examens pour le et se déroulent au niveau régional. Les candidats s’inscrivent auprès des Ligues (Comités régionaux) de la FFTDA. Les examens sont indiqués, sur le calendrier sportif régional. Une date limite d'inscription est fixée pour chaque examen, par la Ligue.

Pour les grades supérieurs au les examens se dérouleront au niveau national (au minimum un par saison sportive) par décision de la CSDGE. Les examens sont indiqués, sur le calendrier sportif national. Les dossiers d'inscription des candidats doivent parvenir au siège de la Fédération par l'intermédiaire du président de la Ligue. Les dates d'examens nationaux et les stages préparatoires sont mentionnés au calendrier national. Une date limite d'inscription est fixée pour chaque examen par la C.S.D.G.E.

Chaque couleur de ceinture a une signification particulière : le blanc représente la pureté, l’innocence de l’initié et son ignorance vis-à-vis de la pratique ; le jaune, couleur du soleil levant, représente l'éveil ; le bleu, couleur de l'eau, représente la clarté ; le rouge, couleur du feu, représente la puissance, le noir, couleur de la plénitude, représente le savoir et la sagesse.

Lors des combats, le combattant doit obligatoirement porter l'ensemble de l'équipement de protection, vérifié par les inspecteurs du bureau de contrôle.

Les compétitions de la Fédération mondiale de taekwondo imposent notamment à tous les combattants le port d'un casque en pvc, d'un protège-dents, d'un plastron, de protège-avant-bras, de gants, de protège-tibias et d'une coquille génitale ; selon les règles de la fédération française (FFTDA), pour les catégories des benjamins, minimes, cadets et vétérans, des protège-pieds sont également obligatoires.



Ceci n'est qu'un résumé de l'arbitrage, il regroupe juste les règles de base à savoir pour comprendre et apprécier un combat lors d'une compétition combat.

Les principales compétitions de taekwondo organisées par la WTF sont celles des Jeux olympiques d'été (grade 20) et des championnats du monde (grade 12). Depuis 2013, une à quatre compétitions par an sont classées ; la finale annuelle est de grade 8 et les autres compétitions GP ("GP series") sont de grade 4.

Parmi les compétiteurs qui ont le plus marqué l'histoire de la discipline par l'ampleur de leur palmarès, on peut notamment citer :
Les compétiteurs classés actuellement numéro un mondial par la WTF sont, au 9 octobre 2017 :


Les compétiteurs s'affrontent face à un jury qui juge la qualité de l'exécution des formes effectuées par chaque compétiteur.

Chaque technique doit être armée avec souplesse selon les "Kibon" et se terminer avec fermeté et précision selon la hauteur définie. La trajectoire doit être nette, ample et rapide. Les techniques doivent dégager une grande impression d’efficacité.

Les techniques devront être enchaînées de sorte que chaque blocage soit instinctivement suivi de l’attaque. D’une manière générale les enchaînements seront rythmés et sans temps mort selon les différentes trajectoires du diagramme.

La maîtrise des déplacements et du corps lors des différents enchaînements est essentielle. Chaque position devra être bien marquée et verrouillée. Les appuis au sol devront être solides et réalisés conformément au "Kibon". La position et le mouvement du corps devront être contrôlés.

Le poumsé doit être exécuté dans sa forme originale en respectant l’ordre des techniques, des positions et des directions, mais aussi le diagramme défini par l'exécution du poumsé.

La respiration doit être synchronisée avec les techniques et les déplacements, elle doit être inaudible. D’une manière générale, l’inspiration se fait en début de mouvement et l’expiration à la fin des différentes phases du poumsé.

La concentration est extrême dans l’exécution du poumsé afin de pouvoir "Vivre son poumsé". La concentration permet de dégager une unité entre le corps et l’esprit. Le regard doit être porteur de toute la détermination à l’exécution du poumsé, il doit suivre la direction des enchaînements techniques et des déplacements.

En taekwondo ITF, il existe des compétitions de casse, une épreuve de puissance qui consiste à briser des planches de bois. Chaque compétiteur se présente pour effectuer cinq casses. L'une s'effectue avec un coup de poing, une avec le tranchant de la main, une avec un coup de pied de côté, une avec un coup de pied circulaire, et une avec un coup de pied retourné. Le compétiteur qui brise le plus de planches remporte la compétition

Comme dans le cassage, chaque compétiteur a cinq techniques à effectuer. Il ne s'agit cependant pas de briser des planches en puissance, mais d'atteindre avec chaque technique une cible placée le plus haut possible.




</doc>
<doc id="3031" url="https://fr.wikipedia.org/wiki?curid=3031" title="Tokugawa Ieyasu">
Tokugawa Ieyasu

Il est le dernier des trois unificateurs du Japon de l'époque Sengoku, après Oda Nobunaga et Toyotomi Hideyoshi (aussi nommé "Hashiba").

Tokugawa Ieyasu naît le sous le nom de "Matsudaira Takechiyo". Il est l'héritier du Clan Matsudaira, petit clan de la province de Mikawa (dans l'actuelle préfecture d'Aichi), déchiré entre les puissants clans Oda et Imagawa.

En 1548, les Oda envahissent le Mikawa. Le père de Ieyasu, Matsudaira Hirotada demande l'aide d'Imagawa Yoshimoto, daimyo du clan Imagawa, qui accepte à la condition qu'Hirotada lui envoie son fils en tant qu'otage. Hirotada s'exécute, Ieyasu, part donc pour Sunpu (l'actuelle Shizuoka). Mais Oda Nobuhide, le daimyo du clan Oda, a vent de la tractation et il intercepte Ieyasu et sa suite. Il menacera ensuite Hirotada de tuer son fils s'il ne récuse pas son pacte avec Imagawa Yoshimoto pour s'allier aux Oda, Hirotada refuse, arguant que laisser son fils mourir ne ferait que sceller plus encore le pacte le liant aux Imagawa. Nobuhide ne fera finalement aucun mal à Ieyasu.

L'année suivante, en 1549, Hirotada meurt et Nobuhide succombe à une épidémie, laissant le clan Oda dans une grave désorganisation. Imagawa Yoshitomo saute sur l'occasion et envoie Imagawa Sessai assiéger le château où se trouve Oda Nobuhiro, fils aîné et successeur de Nobuhide. Le siège tourne vite à l'avantage des Imagawa, mais Sessai rencontre Oda Nobunaga, fils cadet de Nobuhide, afin de lui proposer un marché : il lèvera le siège à condition que les Oda lui livrent Ieyasu. Les Oda ne peuvent qu'accepter le marché et Ieyasu arrive à Sunpu le lendemain. Il y sera bien traité. Ainsi, de ses six ans jusqu'à ses quatorze ans, Ieyasu restera l'otage des Oda puis des Imagawa.

En 1556, Ieyasu obtient le droit de retourner sur ses terres du Mikawa. Puis il se marie pour la première fois avec Tsukiyama-dono, la nièce de Imagawa Yoshitomo, et change son nom en Matsudaira Motoyasu. Ieyasu fera ensuite ses premiers pas en tant que tacticien dans une campagne contre les Oda qu'il mène sur ordre de Yoshimoto. Il y remportera quelques victoires de relative importance, mais qui lui permettront de commencer à se faire un nom.

En 1560, Imagawa Yoshimoto assemble une armée d'environ hommes et marche vers Kyoto, la capitale, afin d'obtenir de l'empereur le titre de shogun. Ieyasu, à la tête de son armée, fait partie des troupes mais est détaché du gros de l'armée pour attaquer un fort frontalier. Il y restera ensuite pour le défendre, ce qui lui permettra d'éviter la bataille surprise d'Okehazama qui sera un vrai désastre pour les Imagawa et de regagner sa liberté. En effet, alors que Yoshimoto fait avancer son armée sur les terres d'Oda Nobunaga (qui a pris la succession de son père à la tête du clan) ce dernier lance une attaque surprise contre Yoshimoto (malgré une nette infériorité numérique et l'avis contraire de ses généraux) et remporte une victoire éclair (la bataille dura quelques minutes seulement), tuant Imagawa Yoshimoto au passage.

Apprenant la défaite d'Okehazama, Ieyasu bat en retraite, puis contacte Nobunaga en vue d'organiser une alliance. Cependant, les tractations se font dans la plus grande discrétion, la femme et le second fils d'Ieyasu se trouvant à ce moment à Sunpu.

En 1561, Ieyasu s'empare du château de Kaminogō, possession Imagawa, et y capture des membres de la famille d'un proche d'Imagawa Ujizane, successeur de Yoshimoto. Cette capture lui permet de récupérer sa femme et son fils (en échange de ses prisonniers), ainsi que de prouver sa bonne volonté à Nobunaga. Ieyasu devient donc vassal de Nobunaga, et il le restera jusqu'à la mort de ce dernier.

Ieyasu ayant maintenant les mains libres, il se consacre à la réorganisation de son clan et de son domaine : il récompense ses vassaux en leur accordant des terres à Mikawa. Ces hommes auront une grande importance dans les nombreuses batailles qu'Ieyasu livrera tout au long de sa vie, et parmi eux on compte notamment Honda Tadakatsu, Ishikawa Kazumasa, Kōriki Kiyonaga, Hattori Hanzō, Sakai Tadatsugu et Sakakibara Yasumasa. Puis, en 1564, il combat les Mikawa Monto au cours de la bataille d'Azukizaka, armée de moines guerriers qui avaient refusé de se soumettre à son autorité. Il les vainc et rase leurs temples.

En 1566, il demande à l'empereur l'autorisation de changer son nom en Tokugawa Ieyasu, ce qui lui est accordé. À ce moment-là, il déclare descendre des Minamoto, ce qui lui apporterait en cas de position de force une grande crédibilité pour être nommé shogun. Cependant, il semble peu probable que cette revendication se fonde sur quoi que ce soit d'avéré. Mais, pour le moment, Ieyasu demeure vassal de Nobunaga, et il participe à toutes les batailles importantes aux côtés des Oda : ainsi, il est présent lors de la prise de Kyoto en 1568.

En 1570, il agrandit son territoire en prenant le reste des terres Imagawa à l'issue d'un pacte avec Shingen Takeda, pacte qui lui coûta cher puisque Shingen prit Sunpu avant lui, l'empêchant ainsi d'annexer le Suruga. En réaction, Ieyasu accueille Imagawa Ujizane en lui promettant de lui rendre ses terres. Les relations entre les Tokugawa et les Takeda deviennent alors tendues, situation qui empire lorsque Ieyasu s'allie avec Uesugi Kenshin, ennemi avéré de Takeda Shingen. Puis Tokugawa déplace son quartier général pour se rapprocher du territoire de Shingen et la guerre devient inévitable.

Fin 1571, Shingen qui s'est allié au clan Go-Hōjō envahit le Totomi, qui appartient aux Tokugawa. Ieyasu rallie ses hommes et les deux armées se rencontrent au début de l'année 1572, à la bataille de Mikata-Ga-Hara, où les Tokugawa essuient une cuisante défaite : Ieyasu échappe de peu à la mort. À la suite de cela, Ieyasu passera un an à refuser le combat contre Shingen, vivant dans un état de siège permanent. Fort heureusement pour lui, Takeda Shingen meurt au printemps 1573. L'homme était un brillant général, souvent considéré comme le meilleur de la période et Takeda Katsuyori, son fils et successeur, s'avère incapable de capitaliser les écrasantes victoires de son père.

En 1575, Katsuyori attaque le château de Nagashino à Mikawa et Ieyasu appelle Nobunaga à la rescousse. Celui-ci vient personnellement à la tête d'une grande armée. L'armée Tokugawa-Oda, forte de soldats affronte l'armée Takeda à la célèbre bataille de Nagashino. Katsuyori est vaincu, mais il parvient à s'enfuir et se retire sur ses terres de Kai. Il ne laissera jamais Ieyasu tranquille et des affrontements sporadiques entre les deux clans eurent encore lieu, mais Katsuyori ne parvint jamais à reprendre le contrôle de la province de Suruga.

En 1579, la femme de Ieyasu et son fils aîné Nobuyasu furent accusés de conspiration avec les Takeda. Ieyasu ordonna à son fils de se faire seppuku, sa femme fut, quant à elle, exécutée.

Au printemps 1582, Nobunaga est assassiné par l'un de ses vassaux, Akechi Mitsuhide. Ieyasu est, à ce moment-là, dans les environs d'Osaka et, n'étant pas en mesure d'affronter Mitsuhide, il se voit contraint de rentrer chez lui en évitant les troupes de son ennemi qui le cherche pour l'exécuter. De retour sur ses terres, Ieyasu envisage d'aller venger Nobunaga mais il est devancé par Toyotomi Hideyoshi qui a écrasé Mitsuhide à la bataille de Yamazaki. Ieyasu met ensuite à profit la mort de Nobunaga pour envahir les provinces de Kai et Shinano, les deux anciennes provinces du clan Takeda, que Nobunaga avait écrasé juste avant de mourir. Mais les Hōjō réagissent et envoient une grande armée l'en empêcher. Les deux clans ne s'affronteront pas, et passeront un accord stipulant qu'Ieyasu garde le contrôle de Kai et Shinano, tandis que les Hojo prendront le contrôle de la province de Kazusa.

En 1583, une guerre éclate entre Hideyoshi et Shibata Katsuie, un autre ancien vassal de Nobunaga. Ieyasu restera neutre dans cet affrontement, préférant éviter le conflit avec Hideyoshi, qui anéantira Katsuie à la bataille de Shizugatake et deviendra ainsi le daimyo le plus puissant du Japon.

En 1584, Ieyasu soutient Oda Nobukatsu contre Hideyoshi. Nobukatsu, fils de Nobunaga, voulait succéder à son père et contestait ainsi le pouvoir d'Hideyoshi, le fait qu'Ieyasu le soutienne n'était qu'une manière de provoquer les Toyotomi, de générer un affrontement avant que la puissance d'Hideyoshi ne devienne trop grande pour qu'il soit vaincu. Ieyasu envoie donc une armée au château d'Owari, Hideyoshi répond en conduisant une armée dans la province du même nom. Les deux armées se rencontrent une première fois à la bataille de Komaki (en vérité une simple escarmouche) puis à la bataille de Nagakute, seul véritable affrontement de ce qu'on appelle aujourd'hui la campagne de Komaki. Un an plus tard, les deux daimyos décrètent une trêve sous l'impulsion de Nobukatsu, puis Ieyasu se rend en 1586 à Osaka pour y rencontrer Hideyoshi et lui faire allégeance. La paix est conclue, mais il va de soi qu'Hideyoshi n'a plus confiance en Ieyasu, de fait celui-ci ne participera plus à aucune campagne militaire (exception faite de la campagne d'Odawara). Pendant les deux invasion de la Corée (1592 et 1597) Ieyasu sera présent au quartier général mais n'enverra aucun homme sur place.

En 1590, après avoir soumis Shikoku et les Shimazu de Kyushu, Hideyoshi attaque Hōjō Ujimasa, grand daimyo de la région de Kantō. Ieyasu envoie hommes sur place, qui se joignent à l'armée Toyotomi qui atteint alors un total de hommes. Après la prise de plusieurs châteaux frontaliers, l'armée met le siège devant le château d'Odawara où les Hōjō se sont enfermés. Le siège durera six mois aux termes desquels il sera pris. Les chefs Hōjō se suicident et Hideyoshi offre à Ieyasu de prendre le contrôle de leurs provinces en échange des cinq qu'il possède (Mikawa, Totomi, Suruga, Shinano, Kai) Ieyasu accepte et emménage sur ses nouvelles terres. Après leur disparition, Date Masamune, daimyo de la province septentrionale de Sendai, devient le dernier daimyo indépendant du Japon ; ses terres sont éloignées de celles d'Hideyoshi. Il se soumettra quelque temps après.

Ieyasu hérite donc des provinces Hōjō, plus riches que celles qu'il possédait auparavant. Il établit sa capitale à Edo (actuelle Tokyo) mais ce faisant il s'éloigne d'Hideyoshi et donc du centre politique du pays, ce qui était probablement l'objectif de Toyotomi. Cet éloignement lui permet également d'éviter de participer à l'invasion de la Corée entre 1592 et 1597 et ainsi d'économiser son armée ce qui contribue à lui donner un avantage à Sekigahara quelques années plus tard contre les autres généraux d'Hideyoshi dont le réservoir de troupes avait été entamé pendant cette campagne qui fut coûteuse en hommes.

Après la mort de Hideyoshi Toyotomi en 1598, le combat commence presque immédiatement entre les 5 régents qu'il avait mis en place pour gérer la minorité de son fils Hideyori Toyotomi (né en 1593). Ieyasu Tokugawa, membre éminent de ce conseil et ancien lieutenant important de Nobunaga Oda, prend rapidement l'avantage. Il obtint, en 1600, le soutien de la moitié des daimyos en écrasant une coalition de rivaux dans l'ouest du Japon au cours de la bataille de Sekigahara et devint, "de facto", le dirigeant du pays. Il fait épouser à Hideyori sa petite-fille, Senhime, âgée de sept ans, puis s'en débarrassera définitivement en 1615, prétextant un différend au sujet d'une inscription dans un temple.

En 1603, après s'être fait attribuer le titre de shogun, il fit du village de , où il avait établi ses quartiers généraux, la nouvelle capitale. Edo deviendra Tokyo (« capitale de l'Est ») à partir de l'Ère Meiji. Ieyasu était donc le premier shogun de la dynastie des Tokugawa, qui règnera sur le Japon jusqu'en 1868 (révolution Meiji).

En 1614, le clan Toyotomi reconstruit le château d'Osaka et un sanctuaire proche, incluant une cloche, sur laquelle se trouve une inscription disant : « Puisse l'État être pacifique et prospère ; à l'Est il salue la pâle lune, et dans l'Ouest fait ses adieux au soleil couchant. » Ieyasu, installé à Edo, qui est situé à l'est, interprète ceci comme une insulte, et la tension commence à grimper entre les deux clans. Cela empire lorsque Hideyori commence à rassembler une force composée de "rōnin" et d'ennemis des Tokugawa à Osaka. Ieyasu décide alors d'empêcher cette force grandissante, et y envoie hommes. Ainsi débute la campagne d'Osaka, série de batailles livrées par le shogunat Tokugawa afin de détruire le clan Toyotomi. C'est en 1615 que le siège aboutira avec la défaite de Hideyori, qui fait seppuku dans son château, à l'issue de la bataille de Tennōji, mettant fin au clan Toyotomi et ouvrant la voie à 250 ans de shogunat Tokugawa.

À sa mort, Ieyasu a été enterré à Sumpu (maintenant Shizuoka) dans le sanctuaire appelé Kunōzan Tōshō-gū, puis son corps a été déplacé à Nikkō. Le mausolée de Tokugawa Ieyasu se situe dans le sanctuaire Tōshō-gū, sis à Nikkō (日光) (à plus ou moins au nord de Tokyo).

Le seigneur Ieyasu avait de nombreux atouts qui lui ont permis d'accéder au pouvoir. Il n'était pas très apprécié du peuple mais était craint et respecté pour son charisme et sa ruse. Il était calculateur et subtil et a souvent modifié ses alliances au moment où cela l'arrangeait. Il s'est d'abord allié à Shingen Takeda puis changea d'avis et fut responsable de la mort de Shingen et de son fils. Il s'est allié au clan Hōjō puis rejoint l'armée d'Hideyoshi qui le détruisit et c'est Ieyasu qui récupéra leur territoire. Ce genre de comportement était courant dans une période de violence, de mort soudaine et de trahison.

Il était capable d'une grande loyauté. Une fois allié à Oda Nobunaga, il ne se dressa jamais contre lui et les deux chefs profitèrent de leur longue alliance. Il était connu pour son dévouement envers ses amis et ses vassaux qu'il récompensait. Toutefois, il était rancunier. On dit que, devenu puissant, il a exécuté un homme qui l'avait insulté pendant sa jeunesse.

Ieyasu protège de nombreux anciens obligés des Takeda de la colère d'Oda Nobunaga, qui était connu pour nourrir une rancune amère envers les Takeda. Il réussit à transformer avec succès un grand nombre de vassaux des clans Takeda, Hōjō et Imagawa — qu'il a battu lui-même ou aidé à vaincre — en partisans.

Ieyasu est connu pour être impitoyable. Il ordonne également personnellement à ses hommes d'exécuter le jeune fils encore enfant de Hideyori, Kunimatsu. Il ordonne l'exécution de chaque soldat trouvé qui a participé à la défense du château d'Osaka. Des dizaines de milliers de samouraïs auraient été tués, leurs têtes fichées sur des planches de bois qui bordent la route de Kyoto jusqu'à Fushimi. Son manque de compassion n'est pas particulièrement rare pour son temps et peut être attribué à son éducation au milieu des guerres, des assassinats et de la violence continue. Une citation de Ieyasu tel que représenté dans le film "Rikyu" résume à peu près sa vision de la vie : « La vie signifie que je peux vivre pour voir demain. »

Dans ses relations personnelles, Ieyasu manifeste le même tempérament que celui qu'il a avec des étrangers. Il eut 19 femmes et concubines, qui lui donnèrent 11 fils et 5 filles.




Il prit grand soin de ses enfants et petits-enfants, et fit de 3 d'entre eux, Yorinobu, Yoshinao, et Yorifusa les daimyos respectifs des provinces de Kii, Owari et Mito. Il pouvait aussi être extrêmement dur : il ordonna l'exécution de sa première femme et de son fils aîné.

Le passe-temps favori de Ieyasu est le colportage. Il le considère comme une excellente formation pour un guerrier. « Quand vous allez colporter dans le pays, vous apprenez à comprendre l'esprit militaire et aussi la vie difficile des classes inférieures. Vous exercez vos muscles et entraînez vos membres. Vous avez une quantité de marche et de course et devenez indifférent à la chaleur et froid et ainsi vous êtes peu susceptibles de souffrir d'une maladie. » (A. L. Sadler, ). Ieyasu a souvent nagé ; même à la fin de sa vie, il aurait nagé dans les douves du château d'Edo.

Plus tard, il se tourne vers l'érudition et la religion et se fait mécène de savants célèbres comme Hayashi Razan.

Deux de ses citations célèbres :

Il affirme s'être battu comme guerrier ou général en 90 combats.

Selon certaines sources, Ieyasu était connu pour avoir l'habitude de se ronger les ongles quand il était nerveux, en particulier avant et pendant la bataille.

Il s'est intéressé à divers compétences "kenjutsu" et était un défendeur de l'école Yagyu-ryū shinkage et employait aussi certains d'entre eux comme ses instructeurs personnels d'épée.





</doc>
<doc id="3032" url="https://fr.wikipedia.org/wiki?curid=3032" title="Théodore Agrippa d'Aubigné">
Théodore Agrippa d'Aubigné

Théodore Agrippa d’Aubigné, né le au château de Saint-Maury près de Pons, en Saintonge, et mort le à Genève, est un homme de guerre, un écrivain controversiste et poète baroque français. Il est notamment connu pour "Les Tragiques", poème héroïque racontant les persécutions subies par les protestants.

Calviniste intransigeant, il soutient sans relâche le parti protestant, se mettant souvent en froid avec le roi Henri de Navarre, dont il fut l'un des principaux compagnons d'armes. Après la conversion de celui-ci, il rédigea des textes qui avaient pour but d'accuser Henri IV de trahison envers l'Église. Chef de guerre, il s'illustra par ses exploits militaires et son caractère emporté et belliqueux. Ennemi acharné de l'Église romaine, ennemi de la cour de France et souvent indisposé à l'égard des "princes", il s'illustra par sa violence, ses excès et ses provocations verbales.

À sa grande horreur, son fils Constant d'Aubigné abjure le protestantisme en 1618 pour mener une vie de débauche dans le château paternel de Maillezais, avant de tuer sa première femme, qu'il surprend en flagrant délit d’adultère dans une auberge, puis de se remarier en prison à Jeanne de Cardilhac qui donnera naissance à Françoise d'Aubigné, qui devient marquise de Maintenon et maîtresse puis épouse du roi de France Louis XIV.

Théodore Agrippa est né à Saint Maury près de Pons, en Saintonge. Il est le fils du juge Jean d’Aubigné, d'origine roturière, et Catherine de L’Estang, de petite noblesse, qui meurt en lui donnant la vie. On l’appelle ainsi "Agrippa" (aegre partus, accouchement difficile), parce qu’il a été enfanté avec peine. Agrippa est baptisé dans la religion catholique mais est élevé dans la religion calviniste. 

Son père Jean, converti au calvinisme, prend part au soulèvement protestant, il participe aux opérations de la conjuration d'Amboise sous les ordres de Tanneguy du Bouchet, dit Saint-Cyr, chef militaire protestant du Poitou. En avril 1560, alors qu'il passe par Amboise avec son fils, il lui aurait fait jurer de venger la mort de ses compagnons. 

Sous la férule de précepteurs calvinistes, Agrippa apprend entre autres disciplines, le latin, le grec et l'hébreu. En avril 1562, pour ses études, Jean installe Agrippa à Paris chez Mathieu Béroalde. Deux mois plus tard, la guerre est déclenchée et un arrêt ordonne l'expulsion des protestants ; Agrippa quitte la ville avec son professeur. Sur le chemin, ils sont arrêtés et emprisonnés par des pillards catholiques, mais parviennent à s’échapper grâce à un complice, et gagnent Montargis, où les accueille Renée de France. 

Ils séjournent ensuite à Orléans, où Agrippa est atteint de la peste mais en guérit. Il se rompt aux armes, et assiste au siège d’Orléans au cours duquel meurt son père.

Envoyé à Genève en 1565, Agrippa y poursuit ses études sous la protection de Théodore de Bèze. Un an plus tard, il est contraint de fuir la ville du fait de son implication dans une histoire de mœurs : il accuse son condisciple Bartholomé Tecia de tentative de « bougrerie » sur sa personne ; Tecia est condamné et exécuté par noyade. Lorsque éclate la deuxième guerre de religion (1567), il s’engage sans hésiter dans l’armée protestante. 

Il était, à la suite d’un duel, absent de Paris durant les massacres de 1572 mais en garda une rancune tenace contre la monarchie. "Les Tragiques" conservent la trace des visions d’horreur dont il fut le témoin.

Quelque temps après la Saint-Barthélemy, Aubigné retourne à la cour de France où il se lie avec le roi de Navarre (futur Henri IV) et devient son écuyer (août 1573). Il a vingt-et-un ans. À cette époque, Henri de Navarre est assigné à résidence à la cour et placé sous une étroite surveillance. On ignore si, comme lui, Aubigné a feint d'être catholique. Toujours est-il qu'il participe à la tentative d'évasion de son maître lors des évènements de la conjuration des Malcontents. L'affaire échoue, et Henri de Navarre doit donner des gages de sa soumission en écartant ses serviteurs les plus suspects et en envoyant ses hommes combattre les troupes protestantes. Aubigné se retrouve alors enrôlé à plusieurs reprises dans l'armée catholique. Guidon du seigneur de Fervaques, il combat les protestants en Normandie puis à la bataille de Dormans où il se lie d'amitié avec le duc de Guise.

À la cour où il côtoie les plus grands, Aubigné fait figure de courtisan accompli. Apprécié pour son intelligence et son esprit mordant, il aurait fait partie de l'Académie de musique et de poésie qui siège au Palais du Louvre. Amateur des mascarades et des joutes, il invente des divertissements de cour et se fait connaître comme expert en magie. C'est aussi un querelleur courant sans cesse après les duels.
Il fait partie des compagnons du roi de Navarre lorsque ce dernier fuit la cour le .

Cette amitié entre le futur roi et son écuyer dure plusieurs années, Henri de Navarre lui confiant de nombreuses missions. Mais de caractère emporté et intransigeant, Aubigné se brouille à de nombreuses reprises avec son maître. Aubigné lui reproche de ne pas être suffisamment attaché à la cause protestante, l'accusant de trop favoriser les catholiques de son entourage. Henri de Navarre est porté à la conciliation et ménageait la cour de France, alors qu'Aubigné appelle à la poursuite de la lutte. Après la signature de la paix de Poitiers qu'il condamne, il quitte une première fois son maître (1577). Grièvement blessé à Casteljaloux, il se retire pendant deux ans sur ses terres aux Landes-Guinemer dans le Blaisois où il se met à écrire. Selon la légende qu’il a lui-même forgée bien plus tard, c’est à Casteljaloux que, alors qu'il était entre la vie et la mort, lui seraient venues les premières « clauses » de son grand poème épique sur les guerres de religion, "Les Tragiques".

Aubigné retourne à la cour de Navarre en 1579. En 1582, il se met au plus mal avec la reine Marguerite de Valois qui demande à son époux de l'éloigner. Ses relations avec Diane d'Andoins, maîtresse du roi ne sont pas meilleures. En 1588, il déconseille au roi de se séparer de son épouse légitime pour épouser sa maîtresse. Entre temps, Aubigné a épousé Suzanne de Lusignan de Lezay (1583).

Pendant les guerres de la Ligue, Aubigné s'illustre de nouveau au combat. Il participe à la bataille de Coutras que remporte Henri sur l'armée royale en 1587. Henri de Navarre le nomme maréchal de camp en 1586, puis gouverneur d’Oléron et de Maillezais, que d'Aubigné avait conquis par les armes en 1589, puis vice-amiral de Guyenne et de Bretagne.

Après l’assassinat du duc de Guise en 1588, Aubigné reprend part aux combats politiques et militaires de son temps. Il est alors le représentant de la tendance dure du parti protestant (« les Fermes ») et voit d’un mauvais œil les concessions faites par le chef de son parti pour accéder au trône. Comme de nombreux protestants, d’Aubigné ressent l’abjuration d’Henri IV, en 1593, comme une trahison. Les divergences politiques et religieuses finissent par le séparer du roi. Aubigné est peu à peu écarté de la cour, dont il se retira définitivement après l’assassinat d’Henri IV en 1610. Aubigné et Henri IV ne se doutaient pas que leurs petits-enfants respectifs, Louis XIV et Françoise d’Aubigné, s'uniraient en 1683.

En 1611, à l’Assemblée des églises protestantes de Saumur, D’Aubigné, élu pour le Poitou, ridiculise le parti des « Prudents » dans "Le Caducée ou l’Ange de la paix".

Il semblerait que c’est à cette période qu’il se tourna vers l’écriture de ses œuvres, et en particulier des "Tragiques". Mais ce n’est pour lui qu’un autre moyen de prendre les armes, en multipliant les pamphlets anti-catholiques et les attaques polémiques contre les protestants convertis. 

De son premier mariage avec Suzanne de Lusignan de Lezay, d'une branche cadette de l'illustre maison de Lusignan, il a un fils, Constant, père de Françoise d’Aubigné, la future marquise de Maintenon, et deux filles, Louise Arthémise de Villette (1584-1663) et Marie de Caumont d’Adde (1586-1624). Son fils Constant lui causa les plus grandes déceptions de sa vie en se convertissant au catholicisme puis en menant une vie scandaleuse de débauche et de malversation ; il le déshérita, plongeant du même coup sa belle-fille et ses petits-enfants dans la misère. Après la mort de son épouse en 1596, d'Aubigné eut un fils naturel avec Jacqueline Chayer, Nathan d'Aubigné, ancêtre de la famille suisse des Merle d'Aubigné.

Refusant tout compromis, d’Aubigné est contraint de quitter la France en 1620, après la condamnation de son "Histoire universelle depuis 1550 jusqu’en 1601" par le Parlement; il se retire alors à Genève, où est publié l’essentiel de ses œuvres. Il y épouse en 1623 Renée Burlamacchi, petite-fille du Lucquois Francesco Burlamacchi, et y meurt le .

 * "Agrippa d’Aubigné" (1552-1630)

Méconnu de ses contemporains, il fut redécouvert à l’époque romantique, notamment par Victor Hugo, puis par le critique Sainte-Beuve. 

En 1976, dans une chanson polémique et anticolonialiste, "Un air de liberté", Jean Ferrat le mentionne : Son œuvre la plus connue est son recueil "Les Tragiques".

Mais d'Aubigné n’est pas l’auteur d’une seule œuvre. "Le Printemps" est un recueil de sonnets, de stances et d’odes qui reprend la lyrique pétrarquiste sur les tons opposés de la rage du désespoir et d'une fantaisie plus légère. Le premier recueil de sonnets du "Printemps", "L'Hécatombe à Diane", est dédié à Diane Salviati, jeune fille qu'il aimait et qu'il n'a pas pu épouser à cause de la différence de religion. À la fin de sa vie, les "Petites œuvres meslees" associent des "Méditations sur les Psaumes" et des poésies religieuses.

L’essentiel de son œuvre est polémique. D'Aubigné, engagé dans les combats de son époque, cherche ainsi à discréditer les vanités de la cour royale et la religion catholique dans la "Confession du Sieur de Sancy" et "Les Aventures du baron de Faeneste". Son "Histoire universelle" est aussi, malgré son titre, une œuvre engagée, destinée à justifier l'autonomie politique et militaire des protestants français. Il publie aussi de nombreux opuscules politiques.

À la fin de son existence, il écrit ses mémoires sous le titre "Sa vie à ses enfants" (Constant, Marie et Louise), pour leur montrer « sa gloire » et « ses fautes » et leur être par là-même un exemple profitable.







</doc>
<doc id="3033" url="https://fr.wikipedia.org/wiki?curid=3033" title="Tantō">
Tantō

Le est un couteau japonais légèrement courbe à un seul tranchant dont la taille de la lame est inférieure à 30 cm (un "shaku" en vérité, unité de mesure des longueurs japonaise). 

La structure du "tantō" est généralement semblable à celle du katana, à la différence près qu'il est bien plus court et souvent moins courbé. En général, la section est "hira-zukuri" (les flancs de la lame sont plats), "unokubi-zukuri" (la partie supérieure des flancs de la lame subit un rétreint, environ du premier tiers de la lame jusqu'au "yokote", arête définissant le commencement de la pointe) ou "kanmuri-otoshi-zukuri" (même conception que sur la section "unokubi-zukuri" mais avec un rétreint se prolongeant jusqu'à la pointe de la lame, formant dans certains cas un contre-tranchant affûté).

Il est l'équivalent d'un poignard ou d'une dague japonaise. 
Selon sa monture, il peut être appelé "tantō" (s'il est glissé dans la ceinture) ou "kaiken" (s'il est caché dans les vêtements).

Il se différencie du "aiguchi" par la présence d'une garde ("tsuba"). En vérité, "aiguchi" n'est pas le nom d'un couteau mais le nom d'une monture. La monture standard est "buke-zukuri" : une garde est présente, une tresse de soie ou de coton ("sageo") sert à attacher le fourreau à la ceinture. La monture "aiguchi" est une monture épurée, où le couteau n'a pas de garde (ou, du moins, son diamètre est égal à celui de la poignée, si bien qu'elle n'est pas proéminente) et où le "sageo" est inexistant dans la majorité des cas.

Le Tantō était principalement porté par les samouraïs, et il était rare que les gens ordinaires en possédassent un. Les femmes portaient souvent sur elles un "kaiken" dans leur "obi", dans un but d'auto-défense.



</doc>
<doc id="3034" url="https://fr.wikipedia.org/wiki?curid=3034" title="Tachi">
Tachi

Le tachi (太刀) est un sabre possédant une lame courbe d'environ , précurseur du sabre japonais classique. C'est principalement une arme de cavalerie.

Il diffère du katana par plusieurs caractéristiques :


Le terme "tachi" provient probablement du verbe "tachikiru" signifiant « couper en deux ». Il apparaît pour la première fois dans le "Tôdai-ji kemmotsuchô" (registre des objets possédés par l'Empereur). La combinaison de "kanji" peut se traduire par :


D'un point de vue purement métallurgique, les anciens Tachi ne peuvent rivaliser de qualité avec celle des lames futures.

Cela est dû au fait que les sabres japonais soient fais en Tamahagane, à base de Satetsu (sables de fer), moins pur, avec une teneur en carbone inférieure à celle de l'acier eurasiatique et de l'acier moderne. 

Il ne reste que peu de lames de cette période, et la plupart ayant été entreposées dans des conditions relativement mauvaises, elles sont aujourd'hui dans un état qui ne nous permet pas leur juste appréciation. Cependant, ces lames restent d'une importance historique non négligeable et sont exposées dans de nombreux musées à cet effet.

Les Tachi sont généralement recoupés en deux groupes par les japonais, les Kazaritachi (litt. Tachi de Décoration) et les Jintachi (litt. Tachi de Guerre). 

Etymologiquement, le terme "Tachi" / Tachikiru est proche de "Mapputatsu", il signifie ouvrir en deux, trancher quelque chose. 

En réalité, les Kazaritachi sont, un peu comme en Occident, des épées de cérémonie, principalement utilisée à la Cour Impériale par les Kuge et plus rarement par les Buke de haut rang lors de visite au Palais Impérial de Kyoto. Le terme désigne avant tout la monture, souvent d'un raffinement extrême, avec de l'or et des perles, le "Samehada" recouvrant le bois de la Tsuka est généralement non-recouvert de soie, et le Tsuba est typiquement un Kara-Tsuba (interprétation japonaise d'une garde d'épée chinoise) luxueux. La lame en elle-même n'a rien à voir avec ça. Bien que certains Kazari-Tachi n'étaient pas aiguisés ou nullement remarquables, tout comme les épées de Cour en Europe avaient cessées d'êtres fonctionnelles. 

Les Jintachi à l'inverse avaient une monture plus sobre et plus pratique, orientée pour le champ de bataille, ils sont plus proche des katana, et sont favorisés par les cavaliers samouraïs, et donc, souvent portés par les samouraïs de haut rang jusqu'à l'époque Sengoku (le Daisho est standardisé au tout début de l'époque Edo). C'est donc le type de sabre que tendent à porter les Daimyo et les généraux samouraïs. 

En d'autres termes, les Jindachi se basent esthétiquement sur une beauté formelle, sobre et fonctionnelle, tandis que les Kazaridachi visent à montrer le rang nobiliaire, donc sa dignité, et la puissance de son porteur. 

Il existe en Occident une grave mésinterprétation du mot Jintachi, et l'on trouve sur internet parfois des lames avec une poignée à la courbure étrange, serpentine. Il s'agit là de pure divagation fantaisiste. Les Jintachi sont des Tachi on ne peut plus normaux. En général, la courbure d'un Tachi est de type "Koshizori" (courbure accentuée sur le premier tiers de la lame, près de la garde) tandis que les katana ont généralement une courbure plus égale dite "Chūzori" ou parfois une courbure de type "Sakizori" typique des Uchigatana. Ce sont les trois principaux types de courbures. 

S'il existe aussi des Tachi dont la poignée se recourbe légèrement vers le pommeau - une forme particulièrement associée aux Jintachi et encore utilisée par les guerriers japonais jusqu'à des époques aussi avancées que "Sengoku-jidai" - la courbure en S marquée associée sur le net aux Jintachi est purement fantaisiste sans aucune base historique quelconque ! 

Le jeu de cartes américain Anachronism est également connu pour avoir confondu Jintachi et Odachi (litt. Grand Tachi, Grand Sabre). 

Avant la standardisation des dimensions du katana opérées lors de l'époque Edo, les Tachi pouvait avoir une Nagasa allant de à , au-delà on parlait de Odachi ou de Nodachi. Ainsi, certaines écoles Koryu Bujutsu prévoient dans leurs cursus le maniement du Odachi, tels que l'antique Kage-ryu ou le Koden Enshin-ryu, célèbre pour le maître Tanaka Fumon, qui manie des sabres mesurant ou supérieur. 

Les Nodachi les plus longs sont des titans avec une Nagasa supérieure à , souvent utilisées comme offrandes pour les dieux japonais. Ainsi, le Tarōtachi de Makara Naotaka, avec une Nagasa de (pour un poids de ). Nenekirimaru, forgé pendant la période Nanbokucho, mesure Nagasa (longueur de lame) et Zenchō (全長 longueur totale). Un sabre signé Ho Norimitsu mesure de Nagasa et Zenchō , il peut aussi bien être utilisé comme Nodachi que monté en Naginata. Ces trois exemples sont des Odachi (bien que l'on puisse aussi parler de Ō-ōdachi...) mais ce ne sont pas des Jintachi.



</doc>
<doc id="3035" url="https://fr.wikipedia.org/wiki?curid=3035" title="The Clash">
The Clash

The Clash est un groupe de punk rock britannique, originaire de Londres, en Angleterre. Il s'agit d'un des quatuors majeurs de l'histoire du rock et du punk rock britannique. Le groupe commence sa carrière en 1976 et se dissout en 1986. En 2003, la formation entre au "Rock and Roll Hall of Fame".

Ses personnalités les plus importantes sont Joe Strummer et Mick Jones, tous deux à la guitare et au chant, ainsi que le bassiste Paul Simonon et le batteur Topper Headon. Son style, rattaché au mouvement punk, se caractérise par un rock contestataire où les textes, « habités d'un élan anarchiste », jouent un rôle primordial.

C'est un groupe qui se caractérise également par sa capacité à intégrer à sa musique des sonorités différentes en puisant à la source des racines musicales de ses membres, parmi lesquelles le punk rock, le rock, le rockabilly, le reggae, le ska ou encore le dub.

À l'origine composé de Joe Strummer, Mick Jones, Paul Simonon, Keith Levene et Terry Chimes (crédité sous le nom de Tory Crimes, un jeu de mots, sur leur premier LP), The Clash se forme à Ladbroke Grove, dans la banlieue ouest de Londres, en 1976, pendant la première vague du punk britannique. Après l'échec de leur groupe punk au nom provocateur London SS, avec Tony James (futur Generation X), Jones et Simonon recrutent Joe Strummer, sur les conseils de leur manager Bernie Rhodes, lui expliquant qu'il . Strummer, de son vrai nom John Graham Mellor, alors légèrement plus âgé qu'eux, est le chanteur des The 101'ers, un groupe de pub rock à la renommée naissante. Après deux jours de réflexion, il accepte, séduit par l'énergie et le potentiel offert par ce nouveau mouvement musical grâce à un concert des Sex Pistols. De son côté, Keith Levene (qui intégrera plus tard Public Image Limited), un ami de Mick Jones, les rejoint en tant que guitariste et compositeur au sein du groupe.

. Cette phrase insérée au verso de la pochette de "The Clash" par Sebastian Conran résume le sens profond du nom du groupe trouvé par Paul Simonon, peu après l'arrivée de Terry Chimes. Le mot étant régulièrement cité dans le journal "Evening Standard", Paul Simonon décide de le proposer aux autres membres, qui approuvent l'idée.

Le , les Clash jouent en première partie des Sex Pistols à Sheffield. Ils interprètent "Janie Jones", "London's Burning" et "1977", montrant un goût certain pour le pur style punk au détriment de la diversité des mélodies. Présent sur place, Charles Shaar Murray, journaliste du "New Musical Express", écrit plus tard dans sa critique : . Le groupe répond malicieusement à cette attaque en composant "Garageland". À l'automne, ils participent au festival punk du 100 Club, puis signent chez CBS Records. Keith Levene est renvoyé en début septembre par manque de motivation et ne participe pas au premier album du groupe, bien qu'il y soit crédité. Terry Chimes quitte le groupe en fin novembre de la même année, et est brièvement remplacé par Rob Harper pour la tournée "Anarchy Tour" de décembre 1976. Finalement, il revient participer au premier album.

Ils publient le single "White Riot" / "1977", et leur premier album, l'éponyme "The Clash", en 1977. Le succès est rapidement au rendez-vous au Royaume-Uni. À cette période, cependant, CBS ne distribue pas l'album aux États-Unis, et attend l'année 1979 pour y sortir une version modifiée (sous le titre "I'm So Bored with the USA") qui deviendra l'album importé par un groupe britannique le mieux vendu dans le pays. Après la sortie de ce premier album, Chimes quitte définitivement le groupe d'un commun accord, en raison de divergences personnelles avec les autres membres. L'album est très bien accueilli par la presse spécialisée, et se classe des meilleures ventes à sa sortie. Dans le neuvième numéro de "Sniffin' Glue", Mark Perry, qui a pourtant critiqué le groupe pour sa signature chez CBS, est enthousiaste. Il écrit dans sa chronique : 

Après une période de tests avec différents batteurs, le choix s'arrête finalement sur Nicholas Bowen Headon, surnommé « Topper » Headon par le groupe en raison de sa ressemblance avec un singe de dessin-animé. Doué, Topper sera même surnommé The Human Drum Machine par le producteur de "Give 'Em Enough Rope", Sandy Pearlman, grâce à sa synchronisation impeccable. Ce musicien doté d'un don pour la batterie a en réalité prévu de ne rester que brièvement dans le groupe, le temps pour lui de se faire une réputation avant de rejoindre un meilleur groupe. Mais, face au potentiel de son groupe actuel, il change ses plans et décide de rester. Le , ils jouent au festival punk de Mont-de-Marsan.

Au départ, les membres de The Clash se font connaître pour leur vision politique révolutionnaire et véhémente, accompagné d'un look novateur. Leurs vêtements, caractéristiques reconstitués et peints par eux-mêmes dans le style « éclaboussure » de Jackson Pollock, arborent des slogans révolutionnaires tels que , , . Pendant l'année 1977, Strummer et Jones rencontrent des ennuis avec la police pour une série de délits mineurs allant du petit vandalisme au vol, tandis que Simonon et Headon sont brièvement arrêtés pour avoir tiré sur des pigeons voyageurs avec des pistolets à air comprimé du toit de leur studio d'enregistrement. Cette dernière histoire est d'ailleurs la source d'inspiration de la chanson "Guns on the Roof" (1978).

Le deuxième album du groupe, intitulé "Give 'Em Enough Rope", est produit par Sandy Pearlman. Le batteur Topper Headon y est crédité sur tous les titres. L'album sort en 1978 et atteint la deuxième place des classements britanniques, mais il échoue à entrer dans le Top 100 américain. Au Royaume-Uni, l'accueil des critiques est mitigé. La production est jugée trop lisse en comparaison de l'excitation brute du premier album. Cependant, le public britannique lui réserve un accueil favorable. Les Clash obtiennent leur premier titre à succès, "Tommy Gun".

"Give 'Em Enough Rope" est le premier album des Clash qui sort officiellement aux États-Unis. D'ailleurs, le groupe effectue sa première tournée américaine, appelée "Pearl Harbour Tour", en son soutien au début de l'année 1979. Peu après, en juillet 1979, leur premier album sort à son tour officiellement aux États-Unis, mais sans les titres "48 Hours", "Cheat", "Protex Blue" et "Deny", qui seront remplacés par quelques singles sortis entre l'album original de 1977 et "Give 'Em Enough Rope". "The Clash" contient donc en plus une version de "I Fought the Law" de Sonny Curtis (qui sortira plus tard sur leur EP "The Cost of Living"), "Clash City Rockers", "Complete Control" et "(White Man) In Hammersmith Palais.

Le troisième album, "London Calling", un double album vendu au prix d'un simple sur l'insistance du groupe, sort en 1979 et reste le sommet de leur succès commercial. Au départ, il est accueilli au Royaume-Uni avec suspicion par leurs premiers fans, du fait que les doubles albums étaient généralement associés aux groupes de rock progressif. Ouvrant la musique punk sur d'autres univers, il offre une plus large palette de styles et d'influences musicaux que les albums précédents, parmi lesquels le rockabilly à la sauce américaine et le reggae jamaïcain qui faisait écho aux styles dub et ska populaires au Royaume-Uni. Accompagné à l'époque d'un autocollant annonçant the Clash comme , l'album est considéré depuis comme l'un des meilleurs qu'un groupe de rock ait jamais produit, cité de la liste des 500 plus grands albums de tous les temps établie par le magazine "Rolling Stone". Il atteint également la première place du classement des 25 albums des 25 dernières années de "Entertainment Weekly". Les titres qui le composent, tels que "Train in Vain", "Clampdown" et "London Calling", sont encore régulièrement diffusés sur les ondes des chaînes de radio. Lors de sa sortie, "Train in Vain" devient le premier tube du groupe à se classer dans le Top 40 américain, bien qu'il soit au départ un titre caché sur le vinyle original car ajouté trop tard en fin de sessions d'enregistrement.

La police de caractères utilisée sur la pochette est un hommage au premier RCA LP éponyme d'Elvis Presley, tandis que la photo prise par Pennie Smith montre un Paul Simonon frustré fracassant sa guitare basse contre le sol lors du show au Palladium de New York en 1979. Selon Simonon, qui au départ était contre l'utilisation de cette photo sur la pochette, il s'agit de la seule fois où il cassa une guitare sur scène. Cette basse est aujourd'hui au musée "Rock and Roll Hall of Fame", avec la légende .

À la fin de l'année 1980, The Clash, après la sortie du double album "London Calling", sort le triple album intitulé "Sandinista!" (avec le numéro de catalogue FSLN1 pour les initiales espagnoles du mouvement politique nicaraguayen Sandinista : ). Une nouvelle fois, le groupe insiste pour que l'album se vende au même prix qu'un album simple, payant la différence en baissant ses propres royalties. Explorant divers styles musicaux, "Sandinista!" reçoit, de la part des critiques et des fans, des réactions très opposées. Si certains trouvent l'album confus, dispersé et très complaisant, il atteint tout de même le sommet de la liste "Pazz and Jop" des meilleurs albums de l'année selon "The Village Voice". Enregistrant chaque idée qu'ils ont, les membres du groupe deviennent moins intéressés par la conception traditionnelle du punk que par leurs expérimentations dans le reggae et le dub ("One More Time") et élargissent leur spectre musical avec le jazz ("Look Here"), le hip-hop ("The Magnificent Seven"), la musique de chambre ("Rebel Waltz"), le gospel ("Hitsville UK" et "The Sound of the Sinners"), et le chant du bébé de Mickey Gallagher, le joueur de synthétiseur.

Bien que les fans soient troublés et que les ventes chutent en Angleterre, le groupe fonctionne plus aux États-Unis que précédemment, principalement en surfant sur le précédent succès de "London Calling". À la suite de la sortie de "Sandinista!", The Clash fait sa première tournée mondiale, avec des dates en Asie de l'Est et en Australie.
C'est à ce moment que la combinaison du calendrier démentiel de la tournée et l'enregistrement d'un nouvel album laisse apparaître de plus en plus de frictions au sein du groupe.

Les tensions et conflits au sein du groupe mènent à une séparation, spécialement depuis que le batteur du groupe, Topper Headon, est devenu instable à cause de son addiction à l'héroïne. Cependant, en pleine tournée, le groupe réussit à enregistrer un album supplémentaire, "Combat Rock", qui devient même sa meilleure vente mondiale. Avec des titres tels que "Rock the Casbah" et la double face A "Should I Stay or Should I Go"/"Straight to Hell", le disque entre en force dans les hit-parades américains et britanniques. À cette période, l'unité de The Clash commence à s'effriter. Il est demandé à Topper Headon de quitter le groupe juste avant la sortie de ce cinquième album. Le batteur du groupe est alors incapable de faire face à sa toxicomanie continue, qui a un impact négatif à la fois sur sa santé, sur sa technique de batterie, et sur un absentéisme devenant problématique. La véritable raison du départ de Headon est cachée par Bernie Rhodes qui parlera d'une divergence de point de vue politique. Le batteur des débuts, Terry Chimes, est recruté pour les mois qui suivent.

La tournée "Combat Rock" est paradoxalement un énorme succès. En partie avec la première partie de la tournée d'adieu de The Who, le groupe joue dans les plus grands stades américains (JFK Stadium de Philadelphie, Pontiac Silverdome de Détroit, Shea Stadium de New York, Coliseum d'Oakland…). À cette époque, après une période de recherche d'identité vestimentaire et capillaire, Joe Strummer arbore une crête iroquoise, symbole alors quelque peu démodé des punks, et relance la mode chez la « génération MTV ». La perte de Headon, membre fédérateur et apprécié de tous, apporte beaucoup de frictions au sein du groupe. Jones et Strummer commencent à se quereller sans cesse, bien qu'il ait parfois été dit que cette animosité venait du fait que Bernie Rhodes n'aimait pas Jones, le trouvant arrogant, et qu'il aurait monté Strummer contre celui-ci. Les membres du groupe communiquent à peine, s'évitant même du regard, aussi bien lors des concerts qu'en coulisses. À la veille de la tournée au Royaume-Uni, Joe Strummer disparaît, obligeant le groupe à annuler les premières dates. The Clash continue néanmoins à tourner, mais en 1983, après des années de tournées et d'enregistrements continus, en paie le prix. Bien qu'ayant mûri en tant que musiciens et individus, les membres sont encore assez jeunes (Paul Simonon et Mick Jones ont seulement 26 et 27 ans, Strummer 30) et ne savent pas faire face à de telles situations difficiles et tendues. Simonon, un ami de longue date de Jones, se rapproche plus de Strummer parce qu'il est frustré par les expérimentations musicales de Mick Jones.

Chimes claque la porte après le "Combat Rock Tour" de 1982-1983, convaincu qu'il ne peut continuer à supporter les incessantes querelles internes e ntre membres. En 1983, après une recherche intensive, le nouveau batteur Pete Howard est recruté et accompagne le trio sur scène pendant plusieurs dates américaines au style dépouillé et finalement au festival de San Bernardino, Californie. Tête d'affiche du festival aux côtés de David Bowie et Van Halen, The Clash donne là le plus gros concert de sa carrière devant près d'un demi-million de spectateurs. Cette date sera aussi la dernière apparition de Mick Jones avec le groupe.
En septembre 1983, poussés par Rhodes, Strummer et Simonon virent Jones du groupe, prétextant son attitude problématique et le fait qu'il se soit éloigné de l'idée originelle du groupe.

Après une série d'auditions, le groupe annonce l'arrivée des guitaristes Nick Sheppard (ex-membre des Cortinas, une formation de Bristol), et Vince White. Howard continue en tant que batteur, malgré les rumeurs selon lesquelles Headon ou Chimes pourraient revenir le remplacer. La nouvelle formation joue son premier concert en janvier 1984 avec un ensemble de nouveau matériel et se lance dans une tournée autoproduite, appelée le "Out of Control tour". Cette formation restera tout de même un échec artistique. Joe Strummer préfère d'ailleurs l'appeler par la suite The Clash Mark Two. À ce propos, répondant à la question de savoir si cette version du groupe était une erreur, il déclare : .

En 1988, dans une autre interview, Joe Strummer avoue qu'il pense encore de temps en temps à ces musiciens : . The Clash tourne de l'hiver jusqu'au début de l'été. À l'occasion de "Scargill's Christmas Party", un spectacle caritatif donné en décembre 1984 à l'attention des mineurs, le groupe annonce la sortie de son prochain album dans le courant de l'année.

Les sessions d'enregistrement de "Cut the Crap" sont chaotiques, surtout avec Bernie Rhodes et Strummer travaillant à Munich. La plupart des morceaux musicaux sont joués par des musiciens de studio, que Sheppard et plus tard White survolent en enrichissant de bouts de guitare. Luttant contre Rhodes pour avoir le contrôle du groupe, Strummer, concernant le projet, décide de s’en laver les mains et retourne chez lui. À la même période, le groupe part en tournée. Appliquant des règles strictes qui leur permettent de transporter uniquement 10 livres sterling et des sous-vêtements de rechange, le groupe voyage séparément ou par deux. Ils se retrouvent uniquement pour des concerts dans des espaces publics à travers le Royaume-Uni où ils jouent des versions acoustiques de leurs tubes ainsi que des reprises telles que "Twist and Shout" et "Stepping Stone".

Après un concert à Athènes, Strummer s'exile en Espagne pour faire le point. Début 1986, lui et Paul Simonon dissolvent officiellement le groupe. Tandis que Strummer est parti, le premier single "This Is England" issu de "Cut the Crap" sort avec un accueil principalement négatif. La chanson, comme la plupart du reste de l'album qui suit plus tard la même année, a été en grande partie remixée par Rhodes, ajoutant des synthétiseurs, des boites à rythme et des chants aux enregistrements inachevés de Strummer. D'autres chansons jouées lors de la tournée restent inédites aujourd'hui, parmi lesquelles figurent "Jericho", "Glue Zombie" et "In the Pouring Rain". Bien qu'Howard fût un batteur émérite, paradoxalement toutes les pistes à la percussion ont été produites à l'aide de boîtes à rythme.

Pour Joe Strummer, le son live des Clash est comme celui d'. Sur scène, le style du groupe est extrêmement rythmé. Au départ, Paul Simonon n'est qu'un novice et c'est Mick Jones qui s'occupe de lui accorder sa basse. Le son n'est pas parfait mais le public apprécie l'énergie qu'ils dégagent. Joe Strummer, quant à lui, hurle au point que peu de personnes le comprennent. Au milieu des autres membres, la rage qu'il emploie pour chanter attire la majorité des regards vers lui.

Leurs différentes prestations leur valent d'acquérir assez rapidement une solide réputation. Les critiques les comparent alors aux Who et Rolling Stones des débuts ou encore à Bruce Springsteen. La rage qu'ils dégagent sur scène devient leur marque de fabrique. Et la totale implication des membres dans leurs performances scéniques impressionne le public lors de leurs tournées.

En plus des concerts, les membres sont proches de leurs fans. Cette attitude altruiste les détache du reste de la scène rock de l'époque. Dans une interview, Lester Bangs, chroniqueur de "Rolling Stone" et de "Creem", explique sa fascination pour cette approche marginale : .

Comme le déclare Mick Jones, les textes de leurs chansons sont plus des slogans que des paroles traditionnelles. Aux débuts du groupe, les membres les réutilisent d'ailleurs pour confectionner leurs propres tee-shirts. Sortie en face B du single de "White Riot" mais non présente sur le premier album, la chanson "1977" est caractéristique de cette époque avec son passage accrocheur . Il résume l'esprit qui règne en 1977 lors de l'explosion du mouvement punk. Le groupe respecte néanmoins ces artistes qui influencent leur musique. La critique vise la production musicale de cette époque, qui est devenue une simple histoire de profits.

Dans les années 1970, le vide laissé au Royaume-Uni par le parti travailliste pousse l'émergence du punk et de son engagement politique. Idéalistes, avec une sensibilité poussée de gauche, les Clash sont l'un des groupes incarnant le plus cet aspect du punk, principalement porté par son leader Joe Strummer. dit d'ailleurs Strummer.

La majeure partie des groupes punks britanniques contemporains des Clash ne font pas preuve d'un aussi grand sens politique, comme The Sex Pistols ou The Damned. Même la scène américaine des Ramones, Talking Heads et Blondie est dénuée de sens politique. Pour beaucoup, dont Billy Bragg, l'apport des Clash et en particulier de Joe Strummer dans ce domaine est donc déterminant : . Issu de la tradition des musiciens engagés, l'un des combats politiques du groupe est de pousser la jeunesse à se rebeller contre le conservatisme oppressif du thatcherisme. .

Pour Lester Bangs, ce qui crédibilise le discours du groupe, c'est son authenticité plus que les origines sociales de ses membres. Ainsi, dans son article "The Clash" long d'une trentaine de pages, il écrit à leur sujet : 

Inspiré par le précepte du do it yourself, le groupe choisit pourtant à ses débuts de signer dans une major plutôt que chez un label indépendant. Lorsque le groupe signe avec la firme CBS Records pour £ , certains fans sont donc décontenancés. Mark Perry déclare même à ce sujet : . Conscient de ces critiques, Joe Strummer répond dans une interview pour "Melody Maker" : . Mais au même moment le mouvement punk commence à se transformer en un vaste business, la communauté éprouve donc de la défiance pour cette annonce et le groupe Crass compose la chanson "Punk Is Dead" dans laquelle il hurle : .

Le groupe pense pourtant avoir la main sur le côté artistique. Mais lorsque CBS sort le single sans demander l'avis des membres, ceux-ci sont irrités. En réponse, ils écrivent la chanson "Complete Control" qui attaque leur maison de disques.

L'argent a toujours été le problème récurrent des Clash. Ainsi, lors de la tournée "Anarchy Tour", après l'annulation de la plupart des concerts, d'autres salles plus petites sont trouvées, diminuant ainsi les recettes. Par provocation, sur la scène de Leeds, Joe Strummer arbore un Tee-shirt où l'on peut lire : (« sécurité sociale £9,70 »), soit la modique somme qu'il touche par semaine. Le déchainement du public pendant les concerts du groupe aboutit à des dégradations lourdes lors de nombreux concerts, que le groupe s'attache à payer… D'ailleurs, Bernie Rhodes couvre les dépenses de sa poche durant cette période. De la même manière, lorsqu'en 1980 le groupe sort son quatrième album, "Sandinista !", il décide d'abandonner ses royalties sur les 200 000 premiers exemplaires. En sortant ce triple album au prix habituel d'un double (voire moins), les membres de The Clash souhaitent fournir au public le maximum de morceaux possible pour une somme abordable. Paul Simonon résume alors la pensée qui pousse le groupe à agir de la sorte : . Ainsi, malgré l'argent amassé par les tournées du groupe, dont celle aux États-Unis avec les Who, le groupe est régulièrement proche de la banqueroute. En 1981, malgré les succès de leurs quatre premiers albums, les membres se battent avec leur maison de disques pour toucher un salaire hebdomadaire d'environ par personne.

 Opposition à l'impérialisme, dénonciation de l'injustice raciale, critique du capitalisme, les Clash avec à leur tête Joe Strummer abordent de nombreux thèmes qui leur sont chers pour ouvrir les yeux de la jeunesse. Dans leur répertoire de chansons, les Clash ont avec "Remote Control" un titre qui dénote une certaine paranoïa. Inspiré par "1984" de George Orwell, la chanson est illustrée dans "The Clash Song Book" (1978) par le message . Empreinte de colère envers les oppresseurs (gouvernement local, politiciens, monde des affaires, police), cette chanson remet en cause l'ordre établi, si critique envers le mouvement punk.

Les Clash sont perçus comme les pionniers du punk politique et se voient affublés du nom de () dès 1977 dans un article de Tony Parsons de "NME". Leur vision politique s'exprime explicitement dans leurs paroles, dès leurs premiers enregistrements tels que "White Riot". Inspiré à Joe Strummer à la suite des émeutes du carnaval de Notting Hill de 1976, le texte de cette chanson encourage la jeunesse blanche désabusée à s'engager politiquement de façon active à l'instar de la minorité noire. Alors que l'Angleterre est en pleine récession, la jeunesse punk anglaise se reconnait dans ces paroles qui expriment la frustration de l'époque.

"Career Opportunities" en est un autre exemple, dénonçant les emplois sous-payés, le style d'emplois en usine et le manque d'alternatives. La chanson "London's Burning", quant à elle, traite de la complaisance politique. L'aspect politique de leurs textes est un des traits caractéristique du groupe ("Spanish Bombs", "The Guns of Brixton", "Something About England", "Straight to Hell"...).

Dès 1976, dans une interview pour "Sniffin' Glue", Strummer explique que la musique se doit d'être un moyen d'expression plus qu'un simple loisir : . "Washington Bullets" en est un exemple. Dans ce titre de "Sandinista!", l'armée américaine est décriée pour son implication dans divers renversements politiques à travers le monde et en particulier l'Amérique du Sud. Sur un rythme disco et avec Headon au chant, ils s'amusent même à comparer les deux blocs américain et soviétique en 1980 dans "Ivan Meets GI Joe", morceau évocateur de l'époque.

Au fil des albums et de divers entretiens, le groupe claironne son scepticisme envers les médias, en particulier ceux de masse. Dans une interview, Joe Strummer explique sa perception des médias : . Alors que CBS refuse de sortir leur premier album aux États-Unis et leur impose le producteur Sandy Pearlman pour le suivant, les Clash attaquent frontalement l'industrie musicale avec leur deuxième disque de 1978. Intitulé , il signifie littéralement . La pochette qui l'accompagne montre un cadavre dévoré par des vautours, une métaphore des maisons de disques.

Les années 1970 voient les États-Unis dominer culturellement l'Angleterre. The Clash dénonce cette Amérique toute puissante et son impérialisme. Ainsi, Joe Strummer explique : Antimilitaristes, les musiciens de The Clash chantent des thèmes en rapport avec la guerre à plusieurs reprises. "The Call-Up" appelle à la désertion du service militaire avec ses paroles . C'est à la fois les jeunes soviétiques avec la guerre d'Afghanistan et les Américains avec la crise iranienne des otages qui sont alors concernés par cette chanson. Dans la même veine, "Charlie Don't Surf", inspirée par "Apocalypse Now" de Francis Ford Coppola, a une approche ironique de la guerre. Ceci renforce l'attaque dirigée contre ceux qui veulent imposer leur vision du monde. Dans la chanson "Career Opportunities", sortie sur l'album "The Clash" en 1977, un passage exprime le rejet de l'autorité militaire et le refus d'aller se battre lors de son service national ().

À l'image de nombreux groupes de la première vague punk, les Clash protestent contre la monarchie et l'aristocratie au Royaume-Uni et à travers le monde. Néanmoins, les Clash ne partagent pas la même vision nihiliste de la politique qu'ont la plupart de ces groupes. Ceci leur attire d'ailleurs de nombreuses critiques de la part d'autres groupes influents tels que Crass et Angelic Upstarts. En 1978, à l'occasion du spectacle "Rock Against Racism" organisé par l'Anti-Nazi League, Joe Strummer porte un tee-shirt controversé portant la mention « "Brigate-Rosse" » accompagnée de l'insigne de la Fraction armée rouge (Andreas Baader-Ulrike Meinhof). Il affirmera plus tard que ce geste n'avait pas pour but d'apporter son soutien aux brigades terroristes d'extrême gauche d'Allemagne et d'Italie, mais seulement faire parler de lui. Dans le film Rude Boy, Strummer, interrogé sur la signification de son T-shirt, affirme que c'est le nom d'une pizzeria. De même, dans la chanson "Tommy Gun", son attitude est ambiguë. Caroline Coon apporte un éclairage sur ce que les Clash faisaient en réalité à cette époque : (« ces chansons, dures et militaristes, étaient ce dont nous avions besoin alors que nous entrions dans le thatcherisme. »)

Le groupe apportera aussi son soutien à d'autres concerts caritatifs, dont les plus célèbres sont ceux de décembre 1979 pour la population du Cambodge, organisé par Paul McCartney. L'album sorti à la suite de ces concerts contient d'ailleurs une chanson de The Clash, "Armagideon Time". Les Clash offrent également leur soutien aux sandinistes au Nicaragua et à d'autres mouvements marxistes d'Amérique latine (comme en témoigne le titre de leur album de 1980, "Sandinista!"). À l'époque de leur album "London Calling", sorti en décembre 1979, le groupe essaie de garder l'énergie punk tout en développant de plus en plus d'expérimentations musicales. Bien que particulièrement méfiants vis-à-vis de leur célébrité naissante, ils réservent toujours un bon accueil à leurs fans en coulisse après leurs concerts, faisant preuve d'ouverture d'esprit, d'intérêt intellectuel et de compassion dans leurs relations avec eux. Le titre de "London Calling" évoque la devise du journaliste radio américain Edward R. Murrow durant la Seconde Guerre mondiale. Ses paroles annoncent d'ailleurs : . Elle met en garde ceux qui espèrent qu'ils soient les sauveurs : , dresse un sombre portrait de l'époque : mais appelle ceux qui écoutent à sortir de leur hébétude droguée et à reprendre le combat sans constamment se tourner vers les Clash eux-mêmes pour des réponses — — demandant finalement, .

Pendant l'existence de The Clash, les musiciens ne sont jamais guidés par l'argent. Même contre leurs intérêts, les tickets pour leurs concerts sont vendus à des prix raisonnables. Le groupe insiste également auprès de CBS pour que leurs double et triple albums, "London Calling" et "Sandinista!", soient vendus au prix d'un simple (environ £5). Pour cela, ils cèdent leurs royalties jusqu’à atteindre les 200 000 ventes. Cette logique du « en avoir pour son argent » a pour effet de les laisser toujours sous l'emprise de leur label. Ce n'est qu'à partir de 1982 qu'ils peuvent maitriser complètement leur propre carrière musicale.

La sortie de "White Riot" est accompagnée d'un malentendu : certains critiques et journalistes relaient l'idée que The Clash est un groupe nationaliste. Pourtant, la chanson crie l'inverse. Les Clash participent d'ailleurs à un concert pour la Ligue antinazie, puis au Rock Against Racism. Lancé par Red Saunders et Roger Huddle en 1976, le mouvement "Rock Against Racism" recueille le soutien de nombreux musiciens et intellectuels et prend rapidement de l'ampleur. En avril 1978, le Rock Against Racism Carnival rassemble 100 000 personnes de Trafalgar Square jusqu'à Victoria Park. Le concert en plein air qui est donné pour l'occasion voit se succéder The Clash, The Buzzcocks, Steel Pulse, X-Ray Spex, The Ruts, Sham 69, Generation X et le Tom Robinson Band.
L'importance des Clash est telle dans ce mouvement que Red Saunders expliquera : .

Durant sa tournée promotionnelle internationale "White Riot Tour" (avec les Buzzcocks et The Jam) du premier album, le groupe commence à se créer une image conflictuelle, en partie à cause des arrestations de ses membres pour vandalisme. Le 21 mai 1977, après un concert à St Albans, la police anglaise stoppe le car transportant les membres et l'entourage du groupe. Strummer et Headon sont arrêtés pour avoir volé des taies d'oreiller et une clé d'un hôtel Holiday Inn de Seaton Burn près de Newcastle. Ils seront condamnés chacun à une amende de .

Tout au long de leur carrière, les membres du groupe connaissent régulièrement des démêlés avec la justice. Le 10 juin 1977, Joe Strummer et Topper Headon sont arrêtés à Londres après avoir écrit le nom du groupe sur un mur. Le 6 juin 1978, Joe Strummer et Paul Simonon sont arrêtés cette fois-ci à Glasgow pour ivresse et désordre sur la voie publique.
Pendant les trois premières années de leur carrière, les membres du groupe sont basés dans ce qui s'appelle maintenant « The Stables » Market à Camden Town, à Londres. Le 30 mars 1978, lors d'un enregistrement, Topper Headon et Paul Simonon sont arrêtés pour avoir abattu des pigeons depuis le toit de leur studio avec un fusil à air comprimé. Ces volatiles se sont révélés être en réalité des pigeons voyageurs d'une grande valeur.

Mais si la police s'est déplacée ce jour-là pour arrêter les deux musiciens, c'est parce que la police britannique des transports ferroviaires () possèdent des bureaux non loin du studio. En se basant sur la réputation « anti-establishment » du groupe punk, elle pense que ceux-ci souhaitent tirer sur les trains. Un hélicoptère et des officiers du département d'enquête criminelle () sont déployés pour l'arrestation. À la suite de cet incident, le groupe compose la chanson "Guns on the Roof" sur l'album "Give Em Enough Rope".

Issus du même mouvement, les Sex Pistols et les Clash ont une histoire commune. Au départ, malgré la concurrence entre Malcolm McLaren et Bernie Rhodes pour installer leur groupe respectif comme chef de file du mouvement punk, les deux groupes tournent en même temps. Les Pistols sont les tête d'affiche, les Clash un des groupes de support. Pourtant, les deux groupes n'ont pas la même philosophie, ni les mêmes buts. Comme l'explique Mick Jones, .

D'ailleurs, Mark Perry, éditeur et créateur du fanzine "Sniffin' Glue", considère que les Clash délivraient un véritable message. Selon lui, les Pistols n'avaient pas de véritable sens et se concentraient principalement sur la haine et la dérision. En revanche, The Clash abordaient des thèmes inspirés des problèmes quotidiens, comme le chômage ou la misère, la base même du mouvement punk. Musicalement, Joe Strummer affirmait, en interview, que même dans un style basique et primaire comme le punk, le batteur devait obligatoirement être doué et expérimenté sous peine de plomber les concerts, tandis que les Pistols revendiquaient, tout comme The Ramones, la volonté, quel que soit l'instrument, de jouer même sans savoir.

Fan du groupe et de son leader, Sharleen Spiteri, la chanteuse du groupe Texas, utilise la même guitare que ce dernier, une Fender Telecaster noire. En effet, à ses débuts, Strummer s'achète une Telecaster de 1966 pour qu'il personnalise en la peignant lui-même en noir. D'ailleurs, en 2007, en écho au fait que Strummer ait gardé la sienne toute sa carrière, le fabricant réalise la « Telecaster Joe Strummer ». En hommage au groupe, la compilation "Burning London: The Clash Tribute" est sortie en 1999 avec des reprises par No Doubt, Third Eye Blind, 311 et Silverchair entre autres. Babyshambles, The Paddingtons, Dirty Pretty Things, Guillemots, The Kooks et environ 20 autres artistes ont également sorti une reprise de "Janie Jones" pour le Strummerville Music Charity.

À ses débuts avec Hot Pants, Manu Chao a The Clash pour groupe de référence. Même si au départ il n'est pas spécialement attaché au mouvement punk, il est emballé par un concert que donne le groupe britannique au Palais des Sports.

Noir Désir a également été influencé par The Clash. Lors d'une interview donnée à "Vibrations Magazine", Bertrand Cantat déclare que l'album "London Calling" fait partie de sa discothèque privilégiée : 
Les membres du groupe Téléphone ont de nombreuses fois évoqué la parenté entre leur démarche et celles des Clash, citant Strummer et Jones comme leurs auteurs et compositeurs favoris. En 1984, les Têtes Raides, alors qu'ils se font encore appeler "Red Ted", commencent par jouer du punk, également inspiré de The Clash.
En 2009, les éditions Buchet Chastel publient un recueil de nouvelles intitulé « London Calling - 19 histoires rock et noires » sous la direction de Jean-Noël Levavasseur. Dix-neuf auteurs principalement issus du roman noir français s'y approprient les 19 chansons de l'album pour y puiser 19 nouvelles.

L'une des particularités de ce groupe punk est son éclectisme musical. Dès son premier album, The Clash impose à CBS la reprise de "Police and Thieves", un titre reggae de Junior Murvin. Cette démarche d'ouvrir le répertoire punk à d'autres morceaux les démarque du reste des groupes de l'époque. Au cours de son existence, le groupe explore un maximum de courants musicaux, s'essayant par exemple au groove avec "Magnificent Seven " et "Lightning strikes" et continue quasi systématiquement de s'inspirer de la musicalité du reggae ("(White Man) In Hammersmith's Palais" entre autres).

Le groupe considère néanmoins que puiser dans les rythmes reggae et les racines de la musique noire reste une démarche typiquement punk. Ainsi, à l'occasion de la sortie de "Sandinista!", Paul Simonon déclare dans "Rolling Stone" : . L'enregistrement de cet album (qui devient un triple-album en raison de la liste des morceaux enregistrés qui s'allongeait), est réalisé dans une ambiance créative qui ne se donne pas de limite. Ainsi, le « son » du groupe, travaillé au fil des albums par l'ingénieur du son Bill Price, a du mal à garder son identité sur certains morceaux, par l'apport d'instruments inhabituels chez les punks (synthétiseurs, boîtes à rythme, violons…). Strummer se souviendra que, dans l'enthousiasme débridé qui a régné pendant les enregistrements, il aurait joué de la balalaika s'il en avait trouvé une dans le studio. C'est pourtant l'envie de s'inspirer de nouvelles sonorités qui pousse le groupe à se séparer.

En 1983, Mick Jones souhaite continuer à insérer des références hip-hop dans les chansons qu'ils composent, à l'instar de ce qui a été fait pour l'album "Sandinista!". Mais après avoir exploré le rap ("The Magnificient seven"), le dub avec entre autres "Robber Dub", le ska, le rockabilly ("Brand new Cadillac" de Vince Taylor) ou même la soul ("Stagger Lee"), Joe Strummer et Paul Simonon cherchent à retourner aux sources du punk pour le prochain album. Ce point de divergence participera à l'éviction de Mick Jones.

En 1986, Strummer collabore avec son ancien compère Mick Jones sur le second album de BAD, "No. 10 Upping St.". Il le coproduit et coécrit sept chansons. Strummer joue la comédie dans quelques films, notamment dans "Walker" d'Alex Cox et "Mystery Train" de Jim Jarmusch. Il effectue également une apparition caméo pour Aki Kaurismäki dans "I hired a Contract Killer" où il chante "Burning Lights/Afro-Cuban Be-Bop". Il se fait remarquer à cette période pour sa participation à des bandes originales, dont "Love Kills" pour le film "Sid and Nancy". Il coproduit plus tard celle de "Grosse Pointe Blank" avec John Cusack qui rencontre du succès. Après quelques expérimentations avec des groupes de soutien au succès limité, il refait surface en 1989 en réalisant son premier album solo. "Earthquake Weather" n'est ni un succès critique, ni un succès commercial. Strummer part néanmoins en tournée avec une nouvelle troupe de musiciens, le Latino Rockabilly War, avant de sortir le single "Trash City". En 1991/1992, Strummer rejoint les Pogues après l'éviction de l'ancien chanteur Shane MacGowan pour une série de concerts à travers l'Europe.

Finalement, à la fin des années 1990, Joe Strummer rassemble des musiciens de haut vol sous le nom de The Mescaleros. Le , Joe Strummer and The Mescaleros donne un concert caritatif pour les pompiers londoniens (FBU) à l'Acton Town Hall de Londres. À cette occasion, Mick Jones rejoint le groupe sur scène sur "Bankrobber", "White Riot" et "London's Burning".

La dernière fois que Strummer monte sur scène est le 22 novembre 2002 à la Liverpool Academy. Il meurt subitement le mois suivant d'une crise cardiaque à l'âge de 50 ans. "Streetcore", l'album des Mescaleros sur lequel il travaillait, sort à titre posthume en 2003. Sa réception critique est élogieuse. Mick Jones avouera plus tard à la presse que cette mort est intervenue alors que les ex-membres de The Clash songeaient sérieusement à se remettre ensemble pour une tournée mondiale. Leurs retrouvailles pour le documentaire "Westway to the World" de Don Letts (2001) leur avait redonné l'envie.

Après son expulsion de The Clash, Jones forme Big Audio Dynamite (ou BAD) en 1984 accompagné de Don Letts, réalisateur de plusieurs vidéos sur les Clash. Leur premier album, "This is Big Audio Dynamite", sort l'année suivante. "E=MC²", qui en est tiré, s'écoute alors dans les dance clubs. Le disque suivant, "No. 10 Upping St.", réunit Jones et Strummer. 3 albums plus tard, Jones décide de modifier complètement sa formation et la renomme Big Audio Dynamite II. Et au milieu des années 1990, le groupe s'appelle Big Audio. Producteur, Mick Jones travaille avec les Libertines pour leurs deux albums studio et Babyshambles pour leur premier. Depuis, il joue et a enregistré un nouvel album, The Last Post, avec son nouveau groupe Carbon/Silicon.

À la suite de l'éclatement des Clash, Simonon forme un groupe appelé Havana 3a.m., lequel enregistre seulement un album au Japon avant de rapidement abandonner. Il retourne alors à ses racines en devenant artiste peintre, exposant dans plusieurs galeries d'art et contribuant à la couverture du troisième album de Mick Jones et BAD, "Tighten Up Vol. 88". Le refus de Simonon de rejouer de la musique est l'une des principales explications données pour laquelle The Clash est l'un des quelques groupes punks des années 1970 à ne pas s'être reformé lors de la période nostalgique punk de la fin des années 1990.

Simonon collaborera avec Damon Albarn de Blur et de la formation virtuelle Gorillaz, Simon Tong de The Verve et Tony Allen, principal fondateur de l'afrobeat et batteur de Fela Kuti. Ensemble, ils créent The Good, the Bad and the Queen dont le premier concert a été donné le au Roundhouse de Camden Town.

La contribution de Headon à The Clash n'est pas uniquement limitée à son jeu à la batterie. Il compose et arrange la musique pour " Ivan Meets G.I. Joe" (où il chante aussi) et "Rock The Casbah" pratiquement tout seul. Ce dernier titre est d'ailleurs leur plus gros hit aux États-Unis où il atteint la 8 place du Billboard en 1982. À cette époque cependant, Headon est viré du groupe pour son addiction à l'héroïne.

À l'exception d'un petit groupe de RnB avec qui il enregistre un LP intitulé "Waking Up" et le 12" E.P. "Drumming Man" en 1986, Headon disparaît du milieu musical jusqu'au documentaire rétrospectif de Don Letts "Westway to the World". Il y fait son "mea culpea" au sujet de sa toxicomanie, vivant des royalties provenant des Clash, atteint d’hypercyphose, et après plusieurs années d'échec dans sa quête d'une réhabilitation, il est maintenant sain et monte sur scène à nouveau.









</doc>
<doc id="3036" url="https://fr.wikipedia.org/wiki?curid=3036" title="Tension">
Tension

La tension en physique est une variable d'extension.










</doc>
<doc id="3038" url="https://fr.wikipedia.org/wiki?curid=3038" title="Tamoul">
Tamoul

Le tamoul ou le tamil ( est une langue dravidienne et la langue des Tamouls.

Elle est l'une des plus anciennes langues du monde, apparue il y a plus de 2000 ans et encore aujourd'hui parlée. Elle est parlée dans l'État du Tamil Nadu, le territoire de Pondichéry et l'Inde du sud. Il est également langue officielle à Singapour et au Sri Lanka est parlé à travers la diaspora tamoule aux Fidji, en Malaisie, en Birmanie, en Afrique du Sud, à l'île Maurice, à l'Ile de La Réunion, mais aussi en Europe, en Amérique du Nord et en Océanie (notamment en Australie). Le nombre total de locuteurs est évalué à 74 millions, d'après l'édition de 1999 du , dont 61 millions en Inde.

Le tamil appartient à la famille des langues dravidiennes. Il s'écrit au moyen d'un alphasyllabaire dérivé du grantha, provenant lui-même de la brahmi. La langue comprend cependant de nombreux dialectes assez éloignés les uns des autres.

Étant donné le faible taux d'alphabétisation qui régna en Inde durant plusieurs siècles, il existe une grande divergence entre la langue tamoule écrite ("centamil") et la langue parlée ("koduntamil"). De même, l'apprentissage se faisant plus de bouche à oreille qu'à partir des livres scolaires, il n'est pas rare que la prononciation d'un mot varie d'un village à l'autre, voire qu'elle n'ait plus rien à voir avec le mot écrit…

D'une façon plus générale, il existe un tamoul littéraire, utilisé dans les journaux, les livres, etc., qui respecte des règles strictes d'orthographe, de grammaire et de syntaxe ; et un tamoul populaire, utilisé à l'oral ou dans les médias ; une telle situation peut être qualifiée de diglossie. Le tamoul populaire est extrêmement variable d'un pays à l'autre, d'une région à l'autre, voire d'un village à l'autre : même un locuteur maîtrisant le tamoul littéraire, c'est-à-dire officiel, peut ne rien comprendre à cette langue, qui utilise en abondance des noms étrangers ("lugéj" : bagage, de l'anglais "luggage", "zanti" : gentil, du français…) souvent adaptés à la prononciation de la langue. Ceci s'explique par le fait qu'une communauté tamil importante réside dans des pays étrangers, la population vivant dans ces pays mélangeant le tamoul avec la langue parlée dans le pays. 
Le tamil parlé en Inde est un tamil plus éloigné du tamil littéraire, car davantage confronté à d'autres langues (autres langues indiennes, anglais...) ; le tamil parlé au Sri Lanka est plus littéraire et plus préservé par ses locuteurs, car vecteur de l'identité même des Tamils de l'île, face à la langue cinghalaise notamment, majoritaire.

Depuis le début du , les grammairiens tamouls tentent de rapprocher la langue écrite de la langue parlée, tout en conservant les particularités de cette écriture très ancienne.

Le tamil est noté à l’aide d’un alphasyllabaire – tout comme les autres langues indiennes et sud-asiatiques d’origine indienne – composé de douze voyelles (5 courtes : a, i, u, e, o – 5 longues : ā, ī, ū, ē, ō et 2 diphtongues : ai, au) et de dix-huit consonnes, classées en trois classes (6 consonnes rudes, 6 moyennes et 6 douces). Cinq autres consonnes, dites "grantha" sont ajoutées à ce système pour retranscrire les sons des mots étrangers, historiquement sanskrites, mais de plus en plus anglais.
Le graphème d’une consonne est doté par défaut d’une voyelle, le plus souvent un /a/, pour former une syllabe. D’autres signes, ajoutés en linéaire à la consonne, viennent modifier la voyelle par défaut pour former de nouvelles syllabes finissant par /i/, /u/, etc. Le tableau ci-dessous recense les différentes combinaisons possibles.

Ainsi, le mot "uyir" (« vie ») s’écrit உயிர், cest-à-dire u + yi + r



Le tamoul est une langue post-positionelle principalement SOV, et possède une syntaxe très stricte : 
Le tamoul compte trois temps : le passé, le présent et le futur. Le contexte de la phrase permet de situer avec plus de précision. Point important : il n'existe pas de verbe en tamoul pour exprimer la possession ; on a recours à une formule employant un datif (complément indirect).

En grammaire tamoule, on compte une seule déclinaison, puisque tous les noms prennent à leurs divers cas les mêmes terminaisons. Les grammariens ne sont pas unanimes sur le nombre de cas, même si probablement sous l'influence du sanskrit on retient sept cas plus le vocatif comme le huitième cas. Ces cas portent le nom de leur numéro d'ordre (ex. ", ...") ou le nom de leur terminaison : 

Comme toutes les langues dravidiennes, le Tamoul possède deux pronoms à la première personne du pluriel, le premier exclusif nāṅkaḷ/நாங்கள் qui n'intègre pas l'interlocuteur dans le groupe (eux, moi mais pas toi ou vous), le second inclusif nām/நாம் qui inclut la personne qui écoute (eux, moi mais également toi). À la troisième personne du singulier et du pluriel, on distingue deux séries de pronoms personnels, la première commençant par i/இ exprimant la proximité du locuteur avec la ou les personnes désignées ("ivaN"/இவன், il la personne proche) et la seconde par a/அ exprimant l'éloignement ("avaN"/அவன், il la personne éloignée). Les pronoms personnels de la troisième personne servent également d'adjectifs démonstratifs. Le Tamoul se distingue aussi par l’existence des pronoms honorifiques ou de politesse spécifiques aux deuxième et troisième personnes du singulier.
Exemple : « partir » 

La syntaxe est marquée par la position du verbe conjugué, toujours à la fin de la phrase. Les verbes sont divisés en 2 groupes :


Généralement, seul le verbe principal est conjugué en fonction du sujet ; les autres verbes prennent une forme infinitive ou dépendant de leur rôle et ne portent pas la marque de la personne.

Le tamoul possède dans son paradigme verbal deux flexions distinctes, positive et négative. Voir : Négation (linguistique).

Les termes français dérivés du vocabulaire tamoul sont rares. Parmi eux, le terme marin de "catamaran". D'origine tamoul et plus largement dravidienne aussi, les termes "" et "curry".

L'écriture tamoule, à l'origine dérivée du grantha, utilisait un système de numération décimal propre :

Chiffres :

Ce système de notation n'est plus utilisé depuis plusieurs siècles, mais on retrouve de telles inscriptions sur les différents temples hindouistes. On utilise aujourd'hui les chiffres dits « arabes » ou "hindou-arabe" originaires de l'Inde du Nord et transmis au reste du monde par les arabes.




</doc>
<doc id="3039" url="https://fr.wikipedia.org/wiki?curid=3039" title="Télougou">
Télougou

Le télougou () est une langue de l'Inde. Il est parlé dans les États d'Andhra Pradesh et du Telangana, où il a le statut de langue officielle, ainsi qu'à Yanaon (territoire de Pondichéry), dans l'arrière-pays tamoul, à l'est du Karnataka, au Maharashtra, en Orissa. Il existe une diaspora télougoue en Birmanie, en Malaisie, à Maurice, en Arabie saoudite, dans les émirats du Golfe, en Afrique du Sud, aux îles Fidji, en Amérique du Nord, au Royaume-Uni et en France ; 70 millions de personnes le parlent comme première langue.

Le télougou est noté au moyen d'un semi-syllabaire, l’alphasyllabaire télougou, sans doute dérivé de l’écriture brahmi. Il fait partie, avec le tamoul, de la famille des langues dravidiennes.

Le télougou s'est différencié des autres langues dravidiennes probablement lors de la période s'étendant de 1500 à 1000 av. J.-C. Cette période correspond également à la naissance de la langue tamoule en prenant comme référence l'activité littéraire. Le télougou appartient à la sous-famille des langues dravidiennes du centre. Sont identifiées comme telles les langues originaires du « proto-dravidien » parlées dans la partie centrale du plateau du Deccan.
D'autres langues rustiques très proches du télougou telles que le "gondi", le "konda", le "koui" et le "kouvi" font également partie de cette sous-famille.

La première grammaire du télougou, l’"Andhra Shabda Chintamani", composée en sanskrit, est l’œuvre du poète Nannayya (). Ce traité reprend le plan d'exposition des grammaires plus anciennes de la langue sanskrite, l’"Aṣṭādhyāyī" et le "Vālmīkivyākaranam", mais contrairement au grammairien Pāṇini, Nannayya a divisé son ouvrage en cinq chapitres, consacrés respectivement aux adjectifs ("samjnā"), à la prononciation ("sandhi"), aux déclinaisons ("ajanta"), à l'élision ("virama") et aux verbes ("kriya").

L'étymologie du mot "télougou" est sujette à débat.

En télougou, on distingue le "karta" (le cas nominatif ou le sujet), le "karma" (l'objet du verbe) et le "kriya" (l'action ou le verbe), qui suivent une séquence. Il existe aussi le "vibhakthi" (la préposition). 

Le télougou est souvent considéré comme une langue agglutinante, où certaines syllabes sont ajoutées à la fin d'un substantif afin d'indiquer son cas grammatical.

Ces agglutinations s'appliquent généralement à tous les substantifs, au singulier et au pluriel.

En télougou, la conjugaison prend en compte dans la flexion non seulement le temps mais aussi la négation et l'interrogation. Par exemple :


 


</doc>
<doc id="3045" url="https://fr.wikipedia.org/wiki?curid=3045" title="Transcription des hiéroglyphes">
Transcription des hiéroglyphes

En égyptologie, la transcription des hiéroglyphes est le processus qui tente de reproduire la "prononciation" des mots à partir des hiéroglyphes de l'égyptien ancien, langue morte depuis longtemps. On a l'habitude de dire qu'il y a autant de règles de transcription des hiéroglyphes qu'il y a d'égyptologues. En d'autres termes, il ne semble pas exister de transcription universelle des hiéroglyphes. Cela s'explique simplement par l'existence d'obstacles difficiles à franchir pour transcrire fidèlement les hiéroglyphes, obstacles qui obligent à des extrapolations qui rendent illégitime toute suprématie d'un système sur un autre.

Il faut cependant différencier les transcriptions courantes, celles que l'on peut rencontrer pour désigner des personnages ou des notions célèbres de l'Antiquité égyptienne ─ comme "pharaon", "oudjat", "Néfertiti" ou "ka" ─, transcriptions figées, ambiguës (un même phonème de l'égyptien sera rendu par un grand nombre de signes différents) et propres à une langue ("Néfertiti" se dit "Nofretete" en allemand) des transcriptions scientifiques, utilisées en linguistique et en grammatologie (étude de l'écriture) égyptiennes. Cette seconde, plus rigoureuse, utilise un symbole unique pour un même phonème, même si la valeur de ce phonème n'est pas sûre. 

Le système de notation scientifique peut varier d'une langue à l'autre : il garde cependant une base constante. Son principal défaut est qu'il reste imprononçable : c'est plus une translittération abstraite qu'une transcription indiquant une lecture réelle. La transcription scientifique ne peut être utilisée par le profane, qui ne peut la lire et donc l'apprendre aisément. De plus, elle s'éloigne grandement des usages déjà implantés dans les langues modernes.

On peut ainsi comparer les deux versions des mots cités : "pr ʿ3" est la transcription scientifique du mot que l'on écrit "pharaon", "oudjat" se transcrit, entre autres possibilités, "wḏ3.t", "Néfertiti" vaut "nfr.t jy(j).tj" et "ka" "k3".

Le système d'écriture des hiéroglyphes est maintenant bien connu. Cependant, même si cela peut paraître contradictoire, la manière dont était prononcée la langue transcrite par les hiéroglyphes (l'égyptien) est difficile à reconstituer, cette langue étant morte depuis des siècles. Les seuls indices dont nous disposons à son sujet sont :

Pour avoir une idée de la faible fiabilité des citations grecques, il suffit de comparer la façon dont nous transcrivons le nom de la capitale de la Chine ("Pékin") avec la romanisation qu'utilisent les Chinois eux-mêmes ("Běijīng"). Rien ne prouve que les mots égyptiens employés par les Grecs étaient prononcés à la manière égyptienne. De plus, ils ne pouvaient que les adapter à leur propre système phonologique, très éloigné de celui de l'égyptien. Or, nombre de termes égyptiens courants — comme "pharaon", "oudjat" ou les noms de dieux — nous viennent du latin "via" le grec. On imagine aisément comment ils ont pu être déformés : de l'égyptien au grec, du grec au latin puis du latin au français.

Malgré tout, par regroupement des différentes sources et application des connaissances que l'on a des autres langues afro-asiatiques (comme le copte, l'arabe, l'hébreu, l'akkadien ou encore le haoussa), les égyptologues arrivent à se mettre plus ou moins d'accord sur un certain nombre de sons, et à créer des conventions arbitraires qui ont pour but de permettre aux contemporains de prononcer les mots égyptiens. Car en plus de ces difficultés de transcription des caractères écrits, il faut savoir que, comme dans les systèmes d'écriture sémites, seules les consonnes sont écrites : l'écriture égyptienne est en effet un abjad. Les voyelles, elles, étaient déduites intuitivement (tâche impossible sans une connaissance précise de la langue). Les voyelles du copte peuvent, d'une certaine manière, donner une idée de la vocalisation possible. Il ne faut cependant pas perdre de vue que les sons d'une langue sont en constante évolution : le copte ne peut donner qu'un indice.

Il faut savoir également qu'il n'existait pas de règles orthographiques rigides et donc que certains mots pouvaient, à l'époque, être écrits d'une multitude de façons. Enfin, l'ordre des hiéroglyphes dans un mot n'était pas forcément linéaire. En effet leur ordre pouvait être bouleversé par pur souci esthétique ou pour marquer le respect envers une divinité dont le nom, par exemple, entre dans la composition d'un mot et qui se trouve alors placé en tête. Par exemple, le nom du pharaon Toutankhamon s'écrivait en fait "Amon-toutankh". Ceci explique que pour certains pharaons peu connus, nous ne soyons pas certains de l'ordre dans lequel il convient de lire les hiéroglyphes.

Le nom du dieu Amon (voir ci-contre) est composé de trois hiéroglyphes dont on s'accorde à dire qu'il s'agit des phonèmes, tous trois des consonnes, "j", "m" et "n" (à noter qu'ici "j" n'est pas un "j" de Jean" ou un "j" de Jan" en néerlandais, mais une « consonne faible », vraisemblablement un « coup de glotte » comparable à la "hamza" arabe).

Il est évident que ce squelette de consonnes, comparable à celui des radicaux sémitiques, était prononcé avec des voyelles. Le placement de ces voyelles dans le radical consonantique se nomme la "vocalisation". L'écriture hiéroglyphique n'ayant pas noté cette vocalisation, ce n'est qu'indirectement qu'on peut la reconstituer, restitution d'autant plus hasardeuse que la valeur des consonnes elle-même n'est pas toujours claire. Par exemple, le "j" initial est ─ c'est établi ─ une consonne faible dont la prononciation n'a eu que peu d'incidence sur le mot et qui a pu servir à écrire la voyelle /a/ (de la même façon que le "aleph" hébreu ou le "alif" arabe, qui notent la même consonne et qui ont, du reste, donné le "alpha" grec prononcé /a/). Le seul squelette réel qui reste se limite donc à "mn".

Actuellement, ce dieu est dénommé "Amon", prononcé /amɔ̃/ par les francophones, /amon/ ailleurs. Cela suppose donc que la vocalisation soit "Jamon". Cette vocalisation nous vient des Romains "via" les Grecs. Les Grecs, en effet, entendaient Ἄμμων "ámmôn", que les Romains ont transcrit "Ammōn" (voire "Hammōn"). Dans les deux cas, le /o/ est long (et ouvert en grec, c'est-à-dire, en API [ɔː]). Les Coptes, cependant, ont conservé ce mot sous la forme αμουν, qui se lit /amun/ (/u/ = "ou" de "loup"). Ces premiers témoignages nous montrent combien la vocalisation est variable d'une langue à l'autre, d'autant plus qu'il existait sans doute des accents différents selon les régions et que les phonèmes évoluent avec le temps. D'autres témoignages rendent l'interprétation du mot encore plus difficile : pour les Babyloniens du au avant l'ère chrétienne, le mot se lisait, d'après leurs textes, "Amāna" (où "ā" est un /aː/) ou "Amānu", "Aman" dans un mot composé. Quant aux Assyriens des et siècles, ils entendaient "Amūnu"

De ces témoignages divergents ressortent cependant deux constantes :

Ainsi, vouloir connaître la prononciation réelle d'un mot aussi fréquent que le nom du dieu Amon s'avère impossible. Cela explique pourquoi il n'est pas impossible de trouver, entre autres possibilités, des transcriptions "Imen", "Imon", "Amen", "Amon" ou "Amun" pour le même mot. Cette ambiguïté se retrouve en français puisqu'on parle couramment du dieu Amon (avec un "o") mais du pharaon Aménophis (avec un "é") alors que le radical "jmn" est commun aux deux mots. 

Le problème est donc double : 

La seule transcription qui puisse faire l'unanimité doit se passer des voyelles : c'est "jmn". Elle est scientifique et très abstraite, comme on va le constater.

Cette transcription est utilisée en linguistique, dans l'étude de la langue égyptienne. Elle vise à la plus grande rigueur et s'apparente presque à une translittération. C'est celle que l'on utilise dans les grammaires, les ouvrages scientifiques, des études grammatologiques consacrées au système d'écriture égyptien. À part quelques détails, ce système est international.

Les symboles de la colonne « prononciation restituée » suivent les conventions de l'API ; ce ne sont que des suppositions, parmi celles que les égyptologues retiennent majoritairement. La colonne « prononciation courante » respecte les usages français (la lettre "j" note donc le phonème initial de "jeu", etc.).

" Note : le premier symbole n'est pas disponible en Unicode ; de fait, il a été remplacé ici par le chiffre trois, de forme similaire. Le symbole employé est normalement composé de deux sortes d'apostrophes courbes l'une sur l'autre."

Le phonème noté "j" (ou "ἰ" dans certains systèmes de transcription) représente un coup de glotte quand il est écrit "j" / "ἰ", un yod (son initial de "yourte ") quand c'est "y" / "j" (selon le système adopté). En fait, ce coup de glotte ayant évolué en yod avec le temps, il n'est pas aisé de déterminer quelle valeur il adopte. Quand la consonne "j" (ou, plus rarement, "w") est muette et ne s'écrit pas (dans les radicaux "tertiæ" et "quartæ infirmæ", par exemple, ou en tant qu'augment ; consulter l'article sur la langue égyptienne), elle est écrite entre parenthèses : "ms(j)", « mettre au monde ». En sorte, il existe deux sons d'origine différente notés de manière déconcertante :

Dans nombre d'ouvrages ─ et dans cette encyclopédie ─, on écrit les deux facettes du coup de glotte, "ἰ" et "j", de la même manière : "ἰr(j)" (« faire ») sera donc écrit "jr(j)".

La consonne "ȝ" a longtemps été considérée comme un coup de glotte. Or des recherches plus récentes tendent à prouver que c'est plutôt une liquide, un /l/ ou un /r/ roulé. 

Les signes "d" et "ḏ", enfin, sont vraisemblablement des consonnes emphatiques (consulter Phonologie de l'arabe).

Les symboles "s" / "z" et "s" / "ś" s'utilisent par paire. Un texte qui se servira de "z" n'utilisera pas "ś", et vice versa. Ce qu'un égyptologue écrira "zȝ" correspondra donc à ce qu'un autre écrira "sȝ".

Il existe quelques conventions à connaître. En premier lieu, les mots égyptiens ne sont jamais vocalisés : c'est un des problèmes majeurs de la transcription. Il est cependant courant qu'on force la vocalisation, même s'il n'est pas possible de la déterminer en détail (la comparaison avec le copte permet cependant quelques avancées) en prononçant "ȝ", "j", "ʿ" et "w", respectivement, /a/, /i/, /ā/ (long), et /u/ (de "loup") après des consonnes. Cette convention n'est probablement pas dénuée de sens car, dans nombre de langues afro-asiatiques, ces consonnes servent aussi à noter ces voyelles (ce sont alors des "mater lectionis"). Quand les phonèmes "j" et "w" sont au contact des autres « semi-voyelles », on les prononce comme des consonnes en position initiale ou médiane, comme des voyelles dans les autres cas, de façon à alterner les consonnes et les voyelles. Par exemple, Le mot "jȝw" sera donc lu /jau/ mais "jwȝ" sera rendu /iwa/. Quand la succession de consonnes n'utilise pas ces signes, ou en nombre insuffisant, on introduit un /e/ bref pour faciliter la prononciation : "nb.t", « dame, maîtresse », pourra être lu /nebet/, "nfr", « beau », /nefer/, etc.

Enfin, il existe trois signes auxiliaires que l'on utilise pour représenter des notions grammaticales et morphologiques :

Finalement, la transcription scientifique de l'égyptien est plus que déroutante. Elle s'avère surtout utile aux spécialistes et ne peut en aucun cas se substituer aux usages courants pour les mots connus. On peut citer, à titre d'exemple, la phrase suivante, tirée de la "Mittelägyptische Grammatik für Anfänger" d'Erhart Graefe (, chez Harrassowitz, Wiesbaden, 2004) pour montrer la complexité du système : 

On pourrait lire cette phrase /reʃwi seʤed depetenef/, ce qui reste une approximation sans doute très éloignée de la réalité : la transcription scientifique possède donc cet avantage certain que le problème de la lecture est évacué pour permettre une notation précise de phonèmes dont la réalisation, somme toute, importe peu à l'étude d'une langue si ancienne.


</doc>
<doc id="3046" url="https://fr.wikipedia.org/wiki?curid=3046" title="Chaîne audio">
Chaîne audio

Une chaîne audio, composée de plusieurs appareils, utilise diverses technologies qui relèvent du domaine de la physique, telles les mouvements ondulatoires (physique vibratoire), le traitement du signal et l'acoustique.

Trois types d'appareils sont considérés dans cette page : les capteurs, les transformateurs et les restitueurs. Dans les musiques modernes, qui utilisent facilement les musiques amplifiées, on retrouve des configurations de base.

Les capteurs sont appelés microphones (en abrégé « micros »). Il existe des microphones pour la voix et des microphones adaptés pour des instruments acoustiques. Les instruments appelés « électriques » ont des microphones intégrés, et on peut brancher un câble directement sur l'instrument. Actuellement, les technologies de transmission sans fil par ondes radio permettent de limiter l'utilisation des câbles.

Le transformateur principal est l'amplificateur. Celui-ci restitue le son, avec plus ou moins de puissance, selon les réglages. Souvent couplée à l'amplificateur, la table de mixage permet de gérer plusieurs entrées et sorties de son. En effet, c'est sur cette table que l'on effectue la « balance » d'un groupe : chaque niveau est réglé, et le son perfectionné (aigus, graves, effets).

D'autres appareils, intégrés à la table de mixage ou externes, permettent d'ajouter un effet spécial sur un son. En effet, on ajoutera facilement de la réverbération ou de la compression dynamique sur une voie. Certains se trouvent sous forme de pédales d'effet, qui peuvent être actionnées par le pied tout en jouant.
Parmi ces effets, on peut citer :

Les haut-parleurs transforment un signal électrique en onde sonore.


La manipulation électrique du son est possible grâce à l'électromagnétisme. En effet, une variation du champ magnétique à proximité d'une bobine produit un "courant électrique induit".

Pour vanter les qualités de reproduction de leurs appareils et fournir des indicateurs de comparaison, les fabricants de matériels électroniques de traitement de reproduction sonore, ont, dans les années 1960, introduit la notion de « Haute Fidélité » ou "Hi-Fi" pour "".

Un amplificateur prend en entrée un signal de faible amplitude et l'amplifie à un niveau utile pour l'équipement qui y sera relié en sortie.

Lorsque le signal d'entrée est très faible (quelques millivolts, voire moins), on utilise un préamplificateur dont le rôle est d'amener cette tension à un niveau relativement immune au bruit (quelques volts). La sortie du préamplificateur sera reliée à l'entrée de l'amplificateur de puissance, qui à son tour pourra générer le courant nécessaire à l'alimentation du (ou des) haut-parleur(s).

Les premiers amplificateurs électroniques utilisaient la technologie des tubes, avant d'évoluer vers le transistor bipolaire. Aujourd'hui, on trouve toujours des amplificateurs à tubes, mais la majorité des unités produites utilisent le transistor, bipolaire ou à effet de champ (FET). L'un des avantages des transistors est leur faible encombrement qui rend possible la réalisation de circuits intégrés de puissance, où tout le circuit d'amplification est regroupé dans un seul composant.

Une des différences entre les technologies tube/transistor est la production possible d'harmoniques impaires par les transistors, rendant le signal reproduit moins agréable à l'oreille que celui reproduit par les tubes, qui génèrent des harmoniques paires. Néanmoins, les transistors à effet de champ partagent cet avantage avec les tubes.

Par opposition à analogique, on parle de traitement numérique du son, lorsque les traitements sont accomplis sur le résultat de la numérisation du signal d'entrée.


Parmi les activités sonores les plus étonnantes du : la phonographie.




</doc>
<doc id="3048" url="https://fr.wikipedia.org/wiki?curid=3048" title="Tri par insertion">
Tri par insertion

En informatique, le tri par insertion est un algorithme de tri classique. La plupart des personnes l'utilisent naturellement pour trier des cartes à jouer.

En général, le tri par insertion est beaucoup plus lent que d'autres algorithmes comme le tri rapide (ou "quicksort") et le tri fusion pour traiter de grandes séquences, car sa complexité asymptotique est quadratique.

Le tri par insertion est cependant considéré comme le tri le plus efficace sur des entrées de petite taille. Il est aussi très rapide lorsque les données sont déjà presque triées. Pour ces raisons, il est utilisé en pratique en combinaison avec d'autres méthodes comme le tri rapide.

En programmation informatique, on applique le plus souvent ce tri à des tableaux. La description et l'étude de l'algorithme qui suivent se restreignent à cette version, tandis que l'adaptation à des listes est considérée plus loin.

Dans l'algorithme, on parcourt le tableau à trier du début à la fin.
Au moment où on considère le "i"-ème élément, les éléments qui le précèdent sont déjà triés.
Pour faire l'analogie avec l'exemple du jeu de cartes, lorsqu'on est à la "i"-ème étape du parcours, le "i"-ème élément est la carte saisie, les éléments précédents sont la main triée et les éléments suivants correspondent aux cartes encore mélangées sur la table.

L'objectif d'une étape est d'insérer le "i"-ème élément à sa place parmi ceux qui précèdent. Il faut pour cela trouver où l'élément doit être inséré en le comparant aux autres, puis décaler les éléments afin de pouvoir effectuer l'insertion. En pratique, ces deux actions sont fréquemment effectuées en une passe, qui consiste à faire « remonter » l'élément au fur et à mesure jusqu'à rencontrer un élément plus petit.

Voici une description en pseudo-code de l'algorithme présenté. Les éléments du tableau "T" (de taille "n") sont numérotés de 0 à "n"-1.

Le tri par insertion est un tri stable (conservant l'ordre d'apparition des éléments égaux) et un tri en place (il n'utilise pas de tableau auxiliaire).

L'algorithme a la particularité d'être online, c'est-à-dire qu'il peut recevoir la liste à trier élément par élément sans perdre en efficacité.

Voici les étapes de l'exécution du tri par insertion sur le tableau formula_1. Le tableau est représenté au début et à la fin de chaque itération.

La complexité du tri par insertion est Θ("n") dans le pire cas et en moyenne, et linéaire dans le meilleur cas. Plus précisément :

La complexité du tri par insertion reste linéaire si le tableau est "presque" trié (par exemple, chaque élément est à une distance bornée de la position où il devrait être, ou bien tous les éléments sauf un nombre borné sont à leur place). Dans cette situation particulière, le tri par insertion surpasse d'autres méthodes de tri : par exemple, le tri fusion et le tri rapide (avec choix aléatoire du pivot) sont tous les deux en formula_2 même sur une liste triée.

Plusieurs modifications de l'algorithme permettent de diminuer le temps d'exécution, bien que la complexité reste quadratique.

Le tri de Shell est une variante du tri par insertion qui améliore sa complexité asymptotique, mais n'est pas stable.

Le principe du tri par insertion peut être adapté à des listes chaînées. Dans ce cas, le déplacement de chaque élément peut se faire en temps constant (une suppression et un ajout dans la liste). Par contre, le nombre de comparaisons nécessaires pour trouver l'emplacement où insérer reste de l'ordre de "n²/4", la méthode de recherche par dichotomie ne pouvant pas être appliquée à des listes.

En pratique, les algorithmes de tri en formula_3 basés sur la méthode « diviser pour régner » (tri fusion, tri rapide) sont moins efficaces que le tri par insertion sur les petites entrées, en dessous d'une taille critique "K" (qui dépend de l'implémentation et de la machine utilisée). Dans ce type d'algorithmes, plutôt que de diviser récursivement l'entrée jusqu'à avoir des sous-problèmes élémentaires de taille 1 ou 2, on peut s'arrêter dès que les sous-problèmes ont une taille inférieure à "K" et les traiter avec le tri par insertion.

Pour le cas particulier du tri rapide, une variante plus efficace existe :



</doc>
<doc id="3049" url="https://fr.wikipedia.org/wiki?curid=3049" title="Tri à bulles">
Tri à bulles

Le tri à bulles ou tri par propagation est un algorithme de tri. Il consiste à comparer répétitivement les éléments consécutifs d'un tableau, et à les permuter lorsqu'ils sont mal triés. Il doit son nom au fait qu'il déplace rapidement les plus grands éléments en fin de tableau, comme des bulles d'air qui remonteraient rapidement à la surface d'un liquide.

Le tri à bulles est souvent enseigné en tant qu'exemple algorithmique, car son principe est simple. Mais c'est le plus lent des algorithmes de tri communément enseignés, et il n'est donc guère utilisé en pratique.

L'algorithme parcourt le tableau et compare les éléments consécutifs. Lorsque deux éléments consécutifs ne sont pas dans l'ordre, ils sont échangés.

Après un premier parcours complet du tableau, le plus grand élément est forcément en fin de tableau, à sa position définitive. En effet, aussitôt que le plus grand élément est rencontré durant le parcours, il est mal trié par rapport à tous les éléments suivants, donc échangé à chaque fois jusqu'à la fin du parcours.

Après le premier parcours, le plus grand élément étant à sa position définitive, il n'a plus à être traité. Le reste du tableau est en revanche encore en désordre. Il faut donc le parcourir à nouveau, en s'arrêtant à l'avant-dernier élément. Après ce deuxième parcours, les deux plus grands éléments sont à leur position définitive. Il faut donc répéter les parcours du tableau, jusqu'à ce que les deux plus petits éléments soient placés à leur position définitive.

Le pseudo-code suivant est repris de Knuth.

Une optimisation courante de ce tri consiste à l'interrompre dès qu'un parcours des éléments possiblement encore en désordre (boucle interne) est effectué sans échange. En effet, cela signifie que tout le tableau est trié. Cette optimisation nécessite une variable supplémentaire.

Pour le tri non optimisé, la complexité en temps est de Θ("n"), avec "n" la taille du tableau.

Pour le tri optimisé, le nombre d'itérations de la boucle externe est compris entre 1 et "n". En effet, on peut démontrer qu'après la "i"-ème étape, les "i" derniers éléments du tableau sont à leur place. À chaque itération, il y a exactement "n"-1 comparaisons et au plus "n"-1 échanges.

Application du tri à bulles au tableau de nombres « 5 1 4 2 8 » ; pour chaque étape, les éléments comparés sont en gras.


Un dérivé du tri à bulles est le "tri cocktail" ou tri shaker. Cette méthode de tri est basée sur l'observation suivante : dans le tri à bulles, les éléments peuvent avancer rapidement vers la fin du tableau, mais ne sont déplacés vers le début du tableau que d'une position à la fois.

L'idée du tri cocktail consiste à alterner le sens du parcours. On obtient un tri un peu plus rapide, d'une part parce qu'il nécessite moins de comparaisons, d'autre part parce qu'il relit les données les plus récentes lors du changement de sens (elles sont donc encore dans la mémoire cache).
Cependant, le nombre d'échanges à effectuer est identique (voir ci-dessus). Ainsi, le temps d'exécution est toujours proportionnel à "n" donc médiocre.

Le code de ce tri ressemble énormément au tri à bulles. Tout comme le tri à bulles, ce tri fait remonter en premier les plus grands éléments. Toutefois, il ne travaille pas sur des éléments adjacents ; il compare chaque élément du tableau avec celui qui est à la place du plus grand, et échange lorsqu'il trouve un nouveau plus grand.

Une variante du tri à bulles, nommée tri à peigne ('), fut développée en 1980 par Włodzimierz Dobosiewicz et réapparut en avril 1991 dans '. Elle corrige le défaut majeur du tri à bulles que sont les « tortues » et rend l'algorithme aussi efficace que le tri rapide.



</doc>
<doc id="3055" url="https://fr.wikipedia.org/wiki?curid=3055" title="2 Tone Records">
2 Tone Records

2 Tone Records est un label de musique britannique créé en 1979 par le fondateur des Specials Jerry Dammers, avec comme têtes de file The Specials, Madness, The Beat, The Selecter, Bad Manners.

Les deux tons peuvent être compris de plusieurs façons. Visuellement, le logo du label 2 Tone représente un échiquier ou juste une bande de carreaux noirs et blancs. Ce qui donne une explication de la base idéologique du mouvement 2 tone : noirs et blancs réunis. Des slogans tels que « "" » (musique contre le racisme) sont très représentatifs du mouvement. Les deux tons sont aussi une allusion au tempo du ska (temps-contre-temps, rythme syncopé), car le two-tone est, musicalement, une tendance du ska. « Two-Tone » peut désigner tant bien le drapeau au damier noir et blanc que la période de 1979 à 1981, ou encore qualifier un groupe ska de cette période (ex : Madness est un groupe 2 Tone). La mascotte du label est Walt Jabsco, dont le design est inspiré d'une vieille photo de Peter Tosh, sur la pochette d'un album des Wailing Wailers de 1964, "Studio One".

Ces groupes adoptent un look caractéristique reconnaissable au premier coup d'œil ; ils reviennent aux origines du mouvement skinhead en prenant l'apparence des rude boys ; costume noir et chemise blanche, un chapeau nommé "pork pie hat", souvent décoré à sa base de damiers, les vêtements Fred Perry (habits de sports de luxe récupérés par les skinheads) Le damier two-tone est d'ailleurs devenu un des symboles du ska, il vient du fameux drapeau à damier utilisé lors des courses de scooter affectionnées par les Mods (années 1960 jusqu'à 69), puis par les rude boys et skinheads des origines.





</doc>
<doc id="3057" url="https://fr.wikipedia.org/wiki?curid=3057" title="Teddy Boys">
Teddy Boys

Le mouvement des , issu de la sous-culture britannique des années 1950, était constitué de jeunes anglais (les Teddy Boys) portant des vêtements d'inspiration édouardienne et souvent considérés comme violents et durs. La ville de Londres en fut l'épicentre. La mode se répandit très vite à travers le Royaume-Uni et fut dès ses débuts associée au rock 'n' roll. Bien qu'il y eût déjà, au dix-neuvième siècle, à Manchester et à Liverpool, des bandes de jeunes, appelés "scuttlers", ayant leurs propres codes vestimentaires, les "Teddy Boys" furent les premiers à s'imposer comme , contribuant ainsi à l'émergence d'une culture de consommation propre à la jeunesse.

Le mouvement tira son nom du gros titre d'un journal de 1953 qui utilisait le diminutif "Teddy" pour "Edward", auquel fut adjoint le terme "boy" (garçon en anglais).

Certains formèrent des gangs qui firent leur notoriété en s'affrontant lors de violentes échauffourées souvent amplifiées par la presse populaire. Les affrontements les plus mémorables furent notamment ceux de Notting Hill, en 1958, au cours desquels les "Teddy boys" furent impliqués en grande part dans des agressions envers des personnes de la communauté des Antilles britanniques.

Ces mœurs violentes sont décrites :


</doc>
<doc id="3059" url="https://fr.wikipedia.org/wiki?curid=3059" title="Evenks">
Evenks

Les Evenks ou Ewenkis (russe :  ; ) constituent l'un des peuples Toungouses de Sibérie (Russie et Nord-Est de la Chine). Leur langue est une langue toungouse, l'evenki. Les Evenks forment au total une population d'environ individus. Les Evenks ne pratiquent pas tous la même religion ; certains pratiquent le lamaïsme, d'autres sont orthodoxes et d'autres encore restent dans un système animiste articulé autour du chamanisme. Ils constituent également l'une des nationalités de Chine où, selon le recensement de 2000, ils seraient autour de . 

Le recensement de 2010 a dénombré Evenks en Russie contre en Chine.





</doc>
<doc id="3060" url="https://fr.wikipedia.org/wiki?curid=3060" title="Totémisme">
Totémisme

Le totémisme est un concept anthropologique qui désigne un mode d'organisation social et religieux, clanique ou tribal, fondé sur le principe du totem. On peut dire, par exemple, qu'« un «totem» est un animal, un végétal, voire un objet fabriqué qui est considéré non seulement comme le parrain du groupe ou de l'individu mais comme son père, son patron ou son frère: un clan se dit parent de l'ours, de l'araignée ou de l'aigle » (Anne Stamm, ). Le lien entre le groupe social, ou l'individu, et son totem n'est pas seulement fondé sur une analogie de nom ou sur une ressemblance quelconque (la ruse du renard et la ruse d'un individu), mais est un rapport spirituel qu'on a pu qualifier de mystique . Bien que le nom du totémisme provienne du mot « totem », lui-même emprunté à l'ojibwa (un groupe amérindien) "ototeman", le totémisme se retrouve dans d'autres cultures qu'en Amérique du Nord, par exemple en Amazonie, chez les Aborigènes d'Australie, en Papouasie Nouvelle-Guinée, en Afrique (chez les Dinka), etc .

La conception traditionnelle du totémisme par les anthropologues associe plusieurs éléments :






Cela dit, ces éléments ne font pas bloc, en particulier on peut détacher l'exogamie : il existe des clans totémiques endogames. Le point le plus important est la définition du totem comme apparentement ou amitié entre une espèce naturelle et un groupe humain, en même temps que la définition du totémisme comme organisation sociale du clan (groupe exogame dont les membres se réclament d'un ancêtre commun, en vertu d'un mode de filiation exclusif). 

Il faut distinguer le totémisme de la possession d'un esprit tutélaire, phénomène répandu chez les Nord-Amérindiens.

C'est à partir d’un terme ojibwé, langue algonquine parlée autour des Grands Lacs de l'Amérique du Nord, que se constitue le "totémisme". Le mot revient à un anglais, John Long, qui l'utilisa en 1791 pour désigner un esprit bienveillant qui protège les hommes. Un groupe d'hommes est ainsi sous la protection d'un totem. C'est James George Frazer qui introduit le débat sur le totémisme en 1887 et qui propose la définition suivante : « un totem est une classe d'objets matériels que le sauvage considère avec un respect superstitieux et environnemental, croyant qu'il existe entre lui et chacun des membres de la classe une relation intime et tout à fait spéciale ».

Pour les Américains, qui révisent les théories britanniques (théories britanniques constituées à partir de l'ethnographie australienne) à la lumière des faits amérindiens, le totem désigne le nom d'un groupe. On privilégie la relation entre le groupe ou l'individu avec l'objet naturel duquel il porte le nom. Boas précise qu'il ne cherche pas à étendre cette analyse, car il n'entend pas faire du totémisme une théorie générale, au contraire de Frazer, qui cherche une unité dans le totémisme.

Frazer (et l'école évolutionniste britannique) fait de l'Australie le terrain privilégié du totémisme, qu'il veut constituer en théorie générale. Selon Claude Lévi-Strauss, Frazer confond trois phénomènes distincts : l'organisation clanique, l'attribution aux clans de noms ou emblèmes animaux ou végétaux, la croyance en une parenté entre un clan et un totem.

Dans "The History of Melanesian Society" (1914), William H. Rivers distingue trois traits dans le totémisme. 1) Un élément social : il y a connexion entre un groupe exogamique du clan et une espèce animale ou végétale ou une classe d'objets. 2) Un élément psychologique : le "Primitif" croit qu'il existe une parenté entre les membres du groupe et un animal, une plante ou un objet. 3) Un élément rituel : le totem mérite respect, on ne peut manger l'animal ou la plante, on ne peut utiliser l'objet.

Pour Freud, le repas totémique est absorption de la vie sacrée, mais l'animal sacrifié est, en réalité, le substitut du père, il permet aux « frères chassés » de s'identifier à leur père.

Boas (et les anthropologues américains) propose une autre définition du totémisme à partir des faits amérindiens, mais il précise que ce qu'il avance ne concerne que les Indiens Kwakiutl. Il considère davantage le totémisme comme un outil analytique servant à décrire des situations ethnographiques particulières. Pour les Américains , le totémisme « en général » n'existe pas. "Le totémisme se caractérise fondamentalement par l'association entre différents types d'activités ethniques et l'exogamie ou l'endogamie", il vaut donc comme classification.

A. P. Elkin définit le totémisme comme la croyance qu'il y a partage d'"une même forme de vie", communauté d'essence entre une personne ou un groupe de personnes, d'un côté, et, de l'autre côté, des espèces naturelles (animale, végétale, atmosphérique) ou un objet. Il distingue quatre formes du totémisme en Australie. 1) Totémisme individuel. Une personne et une seule, médecin, sorcier, est impliquée dans une relation avec une espèce naturelle ou un membre de cette espèce. Plusieurs cas se présentent. Chez les Wuradjeri, un jeune reçoit un totem lors de son initiation, ou un reptile sert d'auxiliaire ou de second moi, "alter ego" à un médecin indigène... 2) Totémisme sexuel. Chaque sexe peut avoir son emblème, par exemple un oiseau. Les Kurnai prennent deux oiseaux différents comme emblèmes, un pour chaque sexe. Un peu à part, le totémisme conceptuel des Aranda et des Aluridja dépend du lieu où la mère a pris conscience de sa grossesse. 3) Le totémisme collectif : de moitiés, de sections, de sous-sections. Il y a moitié lorsque la société, en régime de filiation unilinéaire, se divise en deux groupes mutuellement exclusifs. Chaque moitié a son totem. 4) Totémisme clanique. Un clan est un groupe qui déclare descendre d'un même ancêtre. Le totem sert alors de symbole d'appartenance commune.

Claude Lévi-Strauss, dans "Le totémisme aujourd'hui" (1962) tient le totémisme pour une illusion des anthropologues du XIX° siècle : "le totémisme est une unité artificielle, qui existe seulement dans la pensée de l'ethnologue, et à quoi rien de spécifique ne correspond au dehors" (). Il ne s'agit que d'une logique de classification. Le "totémisme" ne se contente pas de mettre en correspondance, terme à terme, un individu ou un groupe avec un animal ou une plante qui est considéré comme son totem, il pose un système de différence entre une série naturelle et une série culturelle. "Le terme totémisme recouvre des relations idéalement posées, entre deux séries, l'une naturelle, l'autre culturelle. La série naturelle comprend d'une part des catégories, d'autre part des individus ; la série culturelle comprend des groupes et des personnes. Tous ces termes sont arbitrairement choisis pour distinguer, dans chaque série, deux modes d'existence, collectif et individuel, et pour éviter de confondre les séries" ().

La même année, dans "La pensée sauvage", Lévi-Strauss examine une autre forme de totémisme : non plus le totémisme classificatoire mais le totémisme ontologique. Cette fois, il y a identification entre groupes humains et espèces animales, l'un et l'autre ont des qualités en commun. On trouve ce totémisme chez les Chicasaw, Indiens du sud-est des États-Unis, qui postulent une "analogie entre groupes humains et espèces naturelles" : ils comparent un clan ou hameau au raton laveur (qui se nourrit de poisson et de fruits sauvages), au puma (qui vit dans les montagnes, évite l'eau, consomme beaucoup de gibier), etc.

Parmi les anthropologues contemporains, Philippe Descola a redéfini le totémisme dans un ouvrage remarqué, "Par-delà nature et culture" (2005). Il se place pour cela dans la situation de l'Homme s'identifiant au monde suivant deux perspectives complémentaires : celle de son « intériorité » et celle de sa « physicalité » vis-à-vis des autres, humains et non humains.

Le totémisme poserait une identité commune des intériorités des existants, humains et non humains, mais aussi une identité commune entre leurs physicalités.
L'anthropologue décrit les trois autres « ontologies » qui suivent la perception d'une fusion ou d'une rupture entre intériorité et physicalité, et qui se nomment animisme, analogisme et naturalisme ; les quatre modes (identité/rupture) * (intériorité/physicalité) réunis auraient une vocation universelle, tout en revêtant diverses formes de cohabitation ou de dominance suivant les cultures (qu'elles soient archaïques, traditionnelles ou modernes).

Pour Descola, le totémisme se caractérise ainsi. a) Ontologie. Le totémisme repose sur cette affirmation : "ressemblance des intériorités' et "ressemblance des physicalités" () entre humains et non-humains (animaux, végétaux, esprits, objets). D'une part, les animaux, les plantes ont la même âme, intériorité (émotions, conscience, désirs, mémoire, aptitude à communiquer...) que les humains, d'autre part tous partagent "une même substance (la chair, le sang, la peau)" et "une même forme de vie" (). Il y a, comme dans l'animisme, classification par prototype, à partir du modèle le plus représentatif, qui est, dans le totémisme l'unité d'origine, un ensemble d'attributs identifiant l'espèce emblématique (, 234, 334), par exemple le kangourou, rapide, à sang chaud. "De même que l'animisme est anthropogénique parce qu'il emprunte aux humains le minimum indispensable pour que des non-humains puissent être traités comme des humains, le totémisme est cosmogénique car il fait procéder de groupes d'attributs cosmiques préexistants à la nature et à la culture tout ce qui est nécessaire pour que l'on ne puisse jamais démêler les parts respectives de ces deux hypostases dans la vie des collectifs" (). b) Géographie. Sont totémistes les aborigènes australiens, les Algonquins (). c) Notions. "Hybridation entre humains et non-humains" : le totémisme admet un croisement d'animaux, plantes, humains, d'espèces ou variétés différentes ; "chez les Mangarrayi et les Yangman, les êtres du Rêve sont des hybrides d'humains et d'animaux" (). d) Religion. Dans le "Dreamtime" (Rêve) des êtres originaires surgirent des profondeurs de la terre en des sites identifiés, certains laissant des traces sous forme de rochers, points d'eau, bosquets, gisements d'ocre ; ils ont laissé derrière eux une partie des existants actuels, les hommes, les plantes et les animaux avec leurs affiliations totémiques respectives et les noms qui les désignent, les rites et les objets cultuels (p.. 206). e) Sociabilité. "Échanges de femmes, échanges de services, échanges de nourriture, échanges de ressources" caractérisent la vie sociale, en même temps que l'exogamie (). f) Problème. "Problème du totémisme : comment singulariser des individus (humains et non humains) au sein d'un collectif hybride ? Solution : distinguer les attributs de l'individu de ceux de l'espèce" ().






</doc>
<doc id="3061" url="https://fr.wikipedia.org/wiki?curid=3061" title="Totem">
Totem

Le nom totem peut revêtir des significations très différentes selon le contexte.

Le mot « totem » est emprunté à l’ojibwa, langue algonquine parlée sur le pourtour des Grands Lacs nord-américains. Il a été introduit en occident par J. Long en 1791 et c’est à J.-F. Mac Lennan (1869-1870) que l’on doit le concept anthropologique correspondant. Un Ojibwa entend « totem » dans le sens de relation d’ordre purement sociologique (apparemment d’amitié) entre deux personnes. Ototeman est l’une des premières versions du terme, dans lequel otem, possessif vient du morphème grammatical ote. Le mot, pour les Ojibwas, précise à la fois une relation collective (parenté entre germains, plus généralement dans un groupe exogame), et traditionnelle. La forme aoutem relevée en Arcadie en 1609 ne s’est pas répandue en France. Certains groupes ojibwa sont organisés en clans patrilinéaires exogames, lesquels ont pour éponymes le nom d’espèces animales. Le terme sert parfois à énoncer son appartenance clanique : makwa nitotem « l’ours est mon clan ». Il s’agit en fait d’une formule abréviatique qui recouvre la signification suivante « je suis apparenté avec celui qui appartient au clan dont l’éponyme est l’ours, donc j’appartiens à ce clan » (Bonte et Izard, 1995). 
Il a été introduit dans la littérature, à ce qu’il paraît, par J. Long, interprète indien du siècle dernier, qui l’orthographiait totam. Le R.P. Peter Jones, indien Ojibeway, écrit Toodaim ; Warren dodaim ; Morgan adopte cette dernière orthographe en la considérant comme une variante de la précédente. Francis Assikinak, indien Ottawa écrit Ottotam. Selon l’abbé Thavent, le mot est proprement « ote » : « famille, tribu », dont la forme passive est otem, avec l’adjectif passif on a nind otem, ma famille, kit otem, ta famille (Frazer, 1898 : 3). 

Le terme renvoie à une appartenance symbolique revendiquée.

Durkheim, dans les Formes élémentaires de la vie religieuse (1912), analyse le fait religieux totémique en Australie : « c’est seulement à la fin du XVIIIème siècle que le mot totem apparaît dans la littérature ethnographique. On le rencontre pour la première fois, dans le livre d’un interprète indien, J. Long, qui fut publié à Londres en 1791 (source : Voyages and travels of an Indian interpreter). Pendant près d’un demi-siècle, [et à partir de ce même texte], le totémisme ne fut connu que comme une institution exclusivement américaine. C’est seulement en 1841 que Grey, dans un passage resté célèbre, signala l’existence de pratiques tout à fait similaires en Australie. On commença dès lors à se rendre compte qu’on se trouvait en présence d’un système d’une certaine généralité. (…). Mac Lennan fut le premier ayant entrepris de rattacher le totémisme à l’histoire générale de l’humanité. Dans une série d’articles parus dans la Fortnightly review, il s’efforça de montrer, non seulement que le totémisme était une religion, mais que de cette religion étaient dérivées une multitude de croyances et de pratiques que l’on retrouve dans des systèmes religieux beaucoup plus avancés ». 
Mauss tend vers la définition généralisante lorsqu’il écrit un peu plus tard : « [Totem] désigne brièvement tout ce que nous devons écrire longuement si nous devons donner une définition claire de ce qu’il désigne : un culte thériomorphique de clan ». Le terme chez Mauss provient de l’algonquin (Mauss, 1905). 

Nous considérons alors à travers ces définitions combien Freud se réapproprie un terme de nature sociologique. Les lectures de la part du psychanalyste des textes contemporains aux deux années 1910-1912, de l’anthropologie anglo-saxonne, sont multiples. Il lit La religion des Sémites de W.R. Smith (1889), les quatre volumes du Rameau d’Or de J. Frazer paraissant jusqu’en 1915, mais aussi les Formes élémentaires de la vie religieuse d’E. Durkheim. Freud y puise des concepts et trouve une inspiration déterminante à l’écriture du texte qui insistera aussi sur l’association au tabou, comme interdiction rituelle. 











</doc>
<doc id="3069" url="https://fr.wikipedia.org/wiki?curid=3069" title="Théisme">
Théisme

Un théisme (du grec "theos", dieu) est une croyance ou doctrine qui affirme l'existence personnelle et unique d'un Dieu, cause du monde. Le théisme est nécessairement religieux, car la relation de l'Homme avec Dieu passe par des intermédiaires (la religion). Selon le théisme philosophique, Dieu régit l'univers directement.
Le théisme est opposé à l'athéisme. Parmi les formes de théisme, on peut notamment citer le panenthéisme, le monothéisme et le polythéisme. Et à côté de lui (car opposés à lui sur la question de la transcendance) le déisme et le panthéisme (parfois assimilé à un athéisme).

Le théisme affirme que :

Le théisme religieux ajoute les affirmations suivantes :

Le théisme tend ainsi vers un œcuménisme universel, qui se traduit concrètement dans des institutions telles que le Parlement des religions et les progrès du dialogue interreligieux.

Dans la religion catholique, la déclaration "Nostra Ætate" publiée lors du concile Vatican II constitue une véritable révolution dans son rapport aux autres religions, qu'elle considère désormais comme dignes de respect, même en dehors du monothéisme.

L'islam prône une sorte d'"œcuménisme" temporel. C'est-à-dire qu'il ne prétend pas avoir été fondé en 622 par Mahomet, mais aurait toujours existé depuis la Genèse, sous des formes différentes s'abrogeant successivement. Ainsi, la "vraie" religion avant l'Hégire était le Christianisme, puis avant la prédication de Jésus de Nazareth autour de l'an 30, le Judaïsme, jusqu'à Abraham. Pour les polythéistes n'ayant jamais connu ces religions, la "vraie" était la variante locale qui se rapprochait le plus du monothéisme, tel que l'hénothéisme (le culte d'Odin chez les Scandinaves, celui du Grand-Esprit chez les Animistes), du moins jusqu'à ce que ses adeptes rencontrent la forme valide de monothéisme pour leur époque. Cependant, l'Islam abroge ces versions précédentes après son apparition et pour tous les peuples rencontrés depuis chronologiquement.

Les satanistes prétendent que leur "religion" est universelle, indépendamment des variations entre les rituels pour invoquer le Diable, tels qu'ils existent à travers le monde.

Voltaire a ajouté les affirmations suivantes dans le "Dictionnaire philosophique" :

Le théisme philosophique promeut donc le principe de religion naturelle.

On remarquera que le théisme philosophique se rapproche fortement du Déisme dit de l'ingérence, affirmant la vanité des préceptes religieux face aux lois immuables de la Justice et du Bien définies et appliquées par Dieu. Cependant, cette forme est susceptible d'être établie comme une véritable religion, à caractère universel, qui comporterait une liste de règles morales ; alors que le déisme refuse toute possibilité d'établir une interprétation objective, qui soit susceptible d'être institutionnalisée en religion (inconstance de l'interprétation de la parole divine).

On peut citer comme exemples de religions théistes l'hindouisme, le judaïsme, le christianisme et l'islam. En revanche, certaines philosophies, qui ne sont pas fondées sur une ou plusieurs divinités, ne sont donc pas explicitement théistes, comme le bouddhisme, le taoïsme et le confucianisme.

Le théisme religieux est différent du déisme qui, lui, est représenté par la théophilanthropie, pendant la Révolution française, ou par la religion de l’humanité établie au par Armand Bazard, puis Auguste Comte, à partir de la doctrine de Saint-Simon.



</doc>
<doc id="3073" url="https://fr.wikipedia.org/wiki?curid=3073" title="Tchétchène">
Tchétchène

Le tchétchène (' / ' en tchétchène) est une langue appartenant au groupe nakh (tchétchène, ingouche et bats), langues caucasiennes du nord-est. Il est parlé principalement par les Tchétchènes en Tchétchénie, par les Ingouches en Ingouchie et par la diaspora tchétchène à travers le monde.

La langue écrite est fondée sur le dialecte de Grozny. D'abord écrit avec l'alphabet arabe, le tchétchène a été noté en caractères latins en 1925, avant de passer au cyrillique en 1938. En 1992, après la proclamation de l'indépendance de la Tchétchénie, l'alphabet latin (enrichi de diacritiques) a commencé à être de nouveau utilisé.
La langue tchétchène a emprunté certains mots aux langues avec lesquelles elle s'est trouvée en contact (turc, arabe, russe…).




</doc>
<doc id="3075" url="https://fr.wikipedia.org/wiki?curid=3075" title="Traité de San Francisco">
Traité de San Francisco

La « Conférence de la paix » concernant les campagnes du Pacifique s'est ouverte le dans la ville de San Francisco et s'est conclue par la signature par 48 des 51 pays participants du traité de San Francisco ou "traité de paix avec le Japon" le , en conformité avec la Charte des Nations unies et la Déclaration universelle des droits de l'homme. Le traité est entré en vigueur le 28 avril 1952.

La République de Chine ne participe pas à ce traité, elle signe un traité avec le Japon à Taipei, le .

Le Japon reconnaît l’indépendance de la Corée, renonce à Taïwan, aux Pescadores, aux Kouriles, au sud de Sakhaline, à tout droit sur la zone antarctique et aux îles Spratley et Paracel et s'engage à offrir des compensations aux pays et aux victimes de son expansionnisme militaire. Seule la période entre le (attaque de Pearl Harbor ) et le (capitulation du Japon) est concernée par le traité, excluant ainsi les dix premières années de l'expansionnisme du Japon Shōwa. Selon l'historienne Linda Goetz Holmes, cette obligation n'a jamais vraiment été exécutée puisque l'argent utilisé par le gouvernement japonais provenait de fonds d'aide aux victimes mis sur pied par trois États alliés et confisqué en 1945 par le régime shōwa (à commencer par le royaume de Ryūkyū conquis en 1879 et occupé depuis par les États-Unis via les bases d'Okinawa).

Quarante-huit pays alliés de la Seconde Guerre mondiale signèrent ce document. Toutefois, d'importants pays ne furent pas invités à la conférence (la République populaire de Chine, ), refusèrent d'y participer (Birmanie, Inde, Yougoslavie) ou refusèrent de signer le traité (URSS, Tchécoslovaquie, Pologne).

Ce traité entra en application le et donna l'indépendance au Japon, mettant ainsi fin à la période d'occupation (1945-1952).




</doc>
<doc id="3078" url="https://fr.wikipedia.org/wiki?curid=3078" title="Organisme thermophile">
Organisme thermophile

Les organismes thermophiles (du grec "thermê", chaleur et "philein", aimer) ou hyperthermophiles sont des organismes qui ont besoin d'une température élevée pour vivre. Ils font partie des organismes extrémophiles. Les premiers ont été découverts à la fin des années 1960 par Thomas D. Brock dans le parc national de Yellowstone.
Les organismes thermophiles peuvent vivre et se multiplier entre 50 et . Ils peuvent croître entre 25 et mais faiblement. Il existe des organismes thermophiles parmi les différents groupes d'organismes eucaryotes comme des protozoaires, des champignons, des algues, et des procaryotes comme des streptomycètes, des cyanobactéries, des "Clostridium", des "Bacillus". Les eucaryotes connus ne peuvent pas vivre à des températures supérieures à . La bactérie "Thermus aquaticus" est un exemple d'organisme thermophile ; la haute résistance thermique de son ADN polymérase est utilisée pour la réaction de polymérisation en chaîne.

Les organismes hyperthermophiles sont ceux qui peuvent optimalement vivre et se multiplier à des températures supérieures à (de 80 et pour ceux que l'on connaît). Ils sont incapables de croître à des températures inférieures à . 

Ils ne sont à ce jour représentés que par des procaryotes, quelques bactéries et surtout Archaea. 

Les organismes thermophiles et hyperthermophiles peuvent être isolés de biotopes comme des systèmes hydrothermaux volcaniques et géothermiques, comme des sources chaudes, cheminées hydrothermales sous-marines…

Les températures élevées augmentent la fluidité des membranes et détruisent de nombreuses macromolécules organiques. Pour maintenir la fluidité et la cohérence optimale des membranes et de leur milieu interne, ces cellules doivent ajuster leur composition en lipide (ratio acide gras saturé et insaturé, liaisons tétra-éther plus solides). 
La température affecte aussi la structure et la fonction des protéines et enzymes. 

Le fonctionnement au niveau moléculaire des protéines et enzymes thermophiles est très étudié afin d'une part, de mieux comprendre l'adaptation aux fortes températures et d'autre part, pour des applications biotechnologiques (biologie moléculaire).

Certains biologistes font l'hypothèse que les micro-organismes thermophiles et barophiles ressembleraient plus que tout autre être vivant actuel à l'ancêtre commun de toutes les cellules modernes, "le Last universal common ancestor" (Dernier ancêtre commun universel ou LUCA), et que la structure du code génétique aurait été formée chez ces organismes, en milieu hyperthermique et à haute pression hydrostatique. Cette hypothèse ne fait cependant pas l'unanimité parmi les scientifiques.




</doc>
<doc id="3079" url="https://fr.wikipedia.org/wiki?curid=3079" title="Terry Pratchett">
Terry Pratchett

Terry Pratchett est un écrivain britannique né le à Beaconsfield (Buckinghamshire) et mort le à Broad Chalke (Wiltshire). Il est principalement connu pour ses romans de fantasy humoristique prenant place dans l'univers du Disque-monde, dans lequel il détourne les canons du genre pour se livrer à une satire de divers aspects de la société contemporaine.

Pratchett publie son premier roman en 1971, mais ce n'est qu'en 1983 qu'il rencontre vraiment le succès avec le premier volume des "Annales du Disque-monde". Il devient par la suite l'un des auteurs de fantasy les plus prolifiques (les "Annales" comptent plus de trente tomes) et les plus appréciés (ses livres se sont vendus à plus de d'exemplaires). Pratchett est ainsi l'auteur britannique le plus vendu des années 1990. Selon un sondage publié en 2006 dans le magazine littéraire britannique , Terry Pratchett est alors le second auteur vivant le plus apprécié de ses compatriotes, derrière J. K. Rowling.

Il est anobli par la reine en 2008, et reçoit de nombreuses récompenses pour son œuvre. Atteint d'une forme rare de la maladie d'Alzheimer, il milite pendant ses dernières années en faveur du droit au suicide assisté, notamment dans son documentaire ".

Terence David John Pratchett est né le 28 avril 1948 à Beaconsfield dans le Buckinghamshire, en Angleterre. Il est le fils unique de David et Eileen Pratchett, originaires de Hay-on-Wye. La famille déménage à Bridgwater dans le Somerset en 1957, où Terry intègre la ". Il se décrit comme un et affirme devoir son éducation à la bibliothèque publique de Beaconsfield.

Il s'intéresse à l'astronomie : il collectionne les cartes concernant l'espace offertes dans les paquets de thé Brooke Bond, possède un télescope et rêve de devenir astronaute, mais il n'est pas assez doué en mathématiques. Son intérêt se porte ensuite sur les romans de science-fiction anglais et américains, ce qui l'amène à assister à des conventions de science-fiction à partir de 1963-1964, jusqu'à ce qu'il commence à travailler. Ses premières lectures incluent les œuvres de H. G. Wells et d'Arthur Conan Doyle, ainsi que , ce qu'il considère comme .

À 13 ans, Terry Pratchett publie sa première nouvelle, "", dans le magazine de l'école. Elle est publiée commercialement alors qu'il a 15 ans.

Il suit des cours d'arts, d'anglais et d'histoire. Il choisit d'embrasser la carrière de journaliste avant la fin de ses études et est embauché à 17 ans, en 1965, au journal local '. Il y écrit, entre autres, plusieurs histoires pour la section ' sous le pseudonyme d'. Un des épisodes fait apparaître les personnages de son futur roman "Le Peuple du Tapis". Pendant ses jours de repos, il parvient à décrocher des diplômes avancés en anglais (A-level) tout en prenant des cours de journalisme.

En 1968, Pratchett, alors journaliste, réalise l'interview de Peter Bander Van Duren, le codirecteur de , une petite maison d'édition de Gerrards Cross. Lors de cette rencontre, Pratchett mentionne le livre qu'il a écrit, "Le Peuple du Tapis". Bander Van Duren et Colin Smythe acceptent d'éditer le livre en 1971, avec des illustrations de Pratchett lui-même. Il reçoit des critiques favorables, bien que peu nombreuses. Pratchett renouvelle l'expérience avec deux autres romans de science-fiction : "La Face obscure du Soleil" en 1976 et "Strate-à-gemmes" en 1981.

En 1980, après avoir changé plusieurs fois d'employeur, il devient chargé des relations publiques pour le (bureau pour l’énergie) dans une zone qui recouvrait plusieurs centrales nucléaires. Par la suite, il affirme en plaisantant avoir choisi le moment idéal : quelques mois plus tôt s'est produit l’accident nucléaire de Three Mile Island aux États-Unis.

Son premier roman de la série "Disque-monde" est publié en 1983 par . Les droits pour la version poche sont rapidement acquis par Corgi, une filiale de l'éditeur . L'auteur gagne en popularité lorsqu'il est mis en avant dans l'émission radio de la BBC « "" » ; il signe alors chez l'éditeur Victor Gollancz, dont il est le premier auteur de fantasy. Colin Smythe devient son agent. À cette époque, Pratchett prend l'habitude d'écrire tous les soirs après le travail, environ 400 mots.

Terry Pratchett rencontre un tel succès qu'il peut abandonner son poste au CEGB en 1987, après la publication du quatrième volume du "Disque-monde", "Mortimer". Il se consacre ensuite exclusivement à l'écriture, et voit ses ventes décoller : ses livres figurent fréquemment dans le haut des classements de . Selon le journal ', Pratchett est l'auteur britannique le plus lu en 1996 et également le plus vendu des années 1990. Selon le ' de 2005, les ventes britanniques de Pratchett en 2003 représentent 3,4 % du marché de la fiction en format relié, ce qui le classe deuxième derrière J. K. Rowling (6 %). Les versions poche de ses ouvrages représentent quant à elles 1,2 % du marché. Il se vend en effet plus de de livres de Pratchett par an, rien qu'au Royaume-Uni.

Terry Pratchett se marie en 1968 avec Lyn Purves. Ils déménagent à Rowberrow, près de Shipham dans le Somerset en 1970. Ils ont une fille, Rhianna, en 1976 ; elle devient journaliste puis scénariste de jeux vidéo. La famille déménage en 1993 dans un village au nord-ouest de Salisbury, dans le Wiltshire. Terry Pratchett dit aimer . Il se décrit comme un humaniste, soutient activement la et est membre de la .

Très proche de ses lecteurs, il a souvent participé aux forums de discussion qui lui sont consacrés sur Usenet : "alt.books.pratchett" et "alt.fan.pratchett". Selon un sondage publié en 2006 dans le magazine littéraire britannique , Terry Pratchett est le second auteur vivant le plus apprécié de ses compatriotes, derrière J. K. Rowling.

Parmi ses autres passions, l'histoire naturelle a une place importante. Il possède notamment une serre avec des plantes carnivores.Le 31 décembre 2008, il est anobli par la reine et devient , ce qui lui confère le droit d'être appelé « ». Il commente alors : Fin 2009, il entreprend de se fabriquer une épée avec l'aide d'amis. Il explique plus tard au "" avoir récolté du minerai de fer, construit un four et fondu une lame ; son vieil ami et agent Colin Smythe lui offre même quelques morceaux de météorite de fer qu'il ajoute à l'alliage.

Le 28 avril 2010, son blason, créé par , fut accordé par et du College of Arms. La chouette ("Ninoxe boubouk", ou en anglais) reposant sur de l'eau et la croix Ânkh sont des références à la cité d'Ankh-Morpork (la ville jumelle composée des vieilles villes d'Ankh et de Morpork séparées par la rivière). Elles figurent sur le blason de la cité comme décrit dans le Roman Pieds d'argile (1996) où l'auteur critique à sa manière ironique l'héraldique et Collège Royal Héraldique d'Ankh-Morpork (l'équivalent Morporkien du College of Arms). Les livres rouges (de gueules selon le terme héraldique ) représentent l’œuvre de l'auteur qui lui valu son anoblissement. 

La devise NOLI TIMERE MESSOREM signifie en latin "Ne craignez pas le Faucheur". La Mort (personnage présent dans presque tous ses livres), ainsi que sont acceptation étant des sujets clé de l’œuvre de Terry Pratchett. L'utilisation de majuscules, bien qu'habituelle dans ce contexte, ne peut que rappeler l'"élocution" de la Mort tout au long de l’œuvre. En effet, lorsque la Mort (ou celui ou celle qui le remplace) parle, ses propos sont systématiquement écrits en majuscules.

Le 12 Mars 2015, la mort de Terry Pratchett fut communiquée (entre autres) par le biais de Tweeter. Ses trois derniers Tweets (postés par son assistant Rob Wilkins) sont devenus légendaires auprès de ses très nombreux admirateurs :

"AT LAST, SIR TERRY, WE MUST WALK TOGETHER."

"Terry took Death’s arm and followed him through the doors and on to the black desert under the endless night."

"The End."

(Fin).

C'est ainsi que ses proches ont choisi de lui rendre hommage, en lui faisant quitter ce monde tel un de ses personnages (en effet, nombreux sont ceux qui se retrouvent dans ce désert sans étoiles tout au long de ses livres). 

Malgré ses plaidoyers pour le suicide assisté, il semblerait que sa mort fut naturelle.

En août 2007, on lui diagnostique à tort un léger accident vasculaire cérébral qui lui aurait endommagé la partie droite du cerveau. Il constate que ses facultés motrices sont réduites, mais pas son écriture. En décembre 2007, on lui annonce le diagnostic réel : une forme rare et précoce de la maladie d'Alzheimer avec atrophie corticale postérieure. L'écrivain ne tarde pas à annoncer sa maladie, qu'il compare à . Il dit prendre les choses . Pratchett affirme qu'il sent qu'il pourra faire , et explique qu'aux personnes qui lui diront : , il ne répondra positivement . Interviewé lors du festival de littérature de Bath, il confie qu'il lui est devenu trop difficile d'écrire des dédicaces.

En mars 2008, Pratchett annonce qu'il fait une donation d'un million de dollars américains à l’. Il se dit choqué d'avoir appris que le budget de la recherche dans la maladie d'Alzheimer ne représente que 3 % de celui pour le cancer.

En avril 2008, il crée, en collaboration avec la BBC, un documentaire en deux parties basé sur sa maladie : « " » (« Vivre avec Alzheimer »). Les deux parties, diffusées le 4 et le 11 février 2009, rassemblent respectivement (10,4 % d'audience) et de téléspectateurs (6,8 %). Le documentaire remporte un BAFTA du meilleur documentaire.

Dans un article de 2009, Terry Pratchett explique qu'il souhaite un « suicide assisté », bien qu'il n'aime pas ce terme, avant que sa maladie ne parvienne à un stade critique. L'écrivain est choisi par la BBC pour donner la « conférence Richard Dimbleby » en 2010, qu'il intitule « " ». La conférence, diffusée le février, est lue par son ami Tony Robinson : Pratchett n'en assure que l'introduction à cause des problèmes de lecture que sa maladie lui occasionne. Il prend notamment position pour l'euthanasie qui fait débat au Royaume-Uni.

Le 11 juin 2011, il annonce avoir entamé la procédure visant à obtenir un suicide assisté auprès de l'organisation suisse Dignitas. Deux jours plus tard, la chaîne de télévision diffuse le documentaire "" (« Choisir de mourir »), à propos du suicide assisté ; on y voit la mort d'une personne atteinte d'une maladie neurodégénérative. Terry Pratchett remporte un nouveau BAFTA pour cette réalisation en novembre 2011. Après avoir terminé un dernier roman du Disque-monde au cours de l'été 2014, il cesse toute activité à cause de la maladie et décède le 12 mars 2015 à son domicile.

Selon Terry Pratchett, un écrivain doit lire énormément, que ce soit dans son genre littéraire ou en dehors, jusqu'à saturation. Pour lui, l'écriture est un travail difficile, pour lequel on doit . Cependant, il aime écrire et considère la rétribution financière comme plutôt que comme la raison de son travail. En 2010, ses ventes totales franchissent la barre des d'exemplaires.

Après quelques histoires de science-fiction ou d'horreur, Terry Pratchett se consacre rapidement à la fantasy, expliquant qu' dans ce genre littéraire. Dans un discours, il affirme que , en référence à la série "Harry Potter" de J. K. Rowling et à celle du "Seigneur des anneaux" de . Dans le même discours, il reconnaît les bénéfices amenés au genre par ces œuvres.

Il dit avoir une dette envers le genre science-fiction et la fantasy qui l'a vu grandir, mais déteste l'appellation « réalisme magique », qui est selon lui . Il trouve dommage que la fantasy soit déconsidérée en tant que genre littéraire alors qu'il s'agit de la plus ancienne des formes de fiction, et se dit lorsque des romans contenant des éléments de science-fiction ou de fantasy ne sont pas inclus dans ces genres.

En juillet 2005, J. K. Rowling donne une interview au "Time Magazine" dans laquelle elle explique que lorsqu'elle écrivait "Harry Potter", elle ne se rendait pas compte qu'elle écrivait de la fantasy, qui d'ailleurs était un genre qu'elle n'appréciait pas plus que ça. Terry Pratchett réagit en ironisant sur le fait que , ce qui est pris pour une attaque, et même une accusation de plagiat. Il explique qu'il cherchait à dénoncer l'attitude des journalistes, selon lesquels la fantasy, restée au point mort depuis Tolkien, aurait été réinventée par Rowling. Pour lui, cela revient à ignorer de nombreux auteurs de fantasy innovants.

Le succès de Terry Pratchett repose entre autres sur les parodies et les allusions présentes dans ses œuvres. Il amène le lecteur dans un univers de fantasy tout en faisant référence à d'autres univers, ainsi qu'à certains aspects de la réalité. Si ces allusions ont uniquement pour but, dans ses premiers ouvrages, de faire rire, elles évoluent ensuite pour s'intégrer à un niveau plus profond de l'intrigue, faisant appel de manière plus subtile aux connaissances du lecteur. Ces allusions permettent à l'auteur de présenter deux visions parodiques. D'un côté, il parvient à montrer au lecteur les incohérences des univers de fantasy en faisant ressortir à l'extrême ses conventions irréalistes et le manque de bon sens dans certaines situations. Il reprend des clichés du genre et les présente sous un point de vue logique et presque scientifique, accédant ainsi à l'imaginaire d'un large public. À d'autres moments, ce sont certains éléments de la réalité qui sont imités dans son univers de fantasy : le cinéma dans "Les Zinzins d'Olive-Oued", le journalisme dans "La Vérité", etc. Le Disque-monde, allusion directe au monde réel, en permet une interprétation critique.

Malgré leur univers de fiction, les romans de Pratchett sont empreints de détails technologiques et de méthodologie scientifique, dans la lignée de l'écrivain Lyon Sprague de Camp. Il développera certaines de ses idées en allant jusqu'à la vulgarisation scientifique dans "La Science du Disque-monde".

Terry Pratchett est aussi connu pour son style d'écriture original qui inclut un certain nombre d'éléments caractéristiques, comme l'utilisation de notes de bas de page qui impliquent souvent une digression comique ou un commentaire sur la narration.

Il évite d'utiliser des chapitres, ce qu'il explique dans une interview pour "" : . Il ajoute : . Certains de ses ouvrages font néanmoins exception, comme "Timbré" ou "Monnayé", ainsi que quelques-uns de ses livres pour enfants. Il explique que dans les œuvres pour jeunes adultes, son éditeur tant qu'il n'y a pas de chapitres, mais le reste du temps, il estime que cela constitue un obstacle inutile sur le chemin de la narration. La ponctuation entre deux de ces « séquences » sert parfois la narration suivant les éditions. Ainsi, dans "Procrastination", les pendules ponctuant ces « chapitres » sont positionnées afin de figurer un mouvement de balancier pour « s'immobiliser » durant une partie du roman.

Les noms des personnages, des lieux et les titres des œuvres de Terry Pratchett contiennent souvent des jeux de mots et des références culturelles. Quelques-uns de ses personnages sont des parodies de personnages connus : par exemple, Cohen le Barbare, aussi appelé Gengis Cohen, est une parodie évidente de Conan le Barbare et de Gengis Khan, tandis que Léonard de Quirm se réfère à Léonard de Vinci.

Une autre de ses marques de fabrique est le non-respect des conventions typographiques lors des dialogues de certains personnages. La Mort, par exemple, n'utilise pas les guillemets et ses paroles sont écrites entièrement en capitales, ce qui met en relief le fait qu'il parle par télépathie. Les contrôleurs de la réalité n'utilisent pas non plus de guillemets, et les épiciers d'Ankh-Morpork n'utilisent pas la ponctuation correctement ; de même, les golems mettent une capitale à chaque mot.

Dans les premiers romans du "Disque-monde", l'histoire s'articule autour des situations humoristiques, échafaudées à l'avance ; les personnages ne font que subir et réagir à ces évènements. Au fur et à mesure de la série, l'univers gagne en profondeur et les scénarios s'étoffent : d'une simple parodie de l'univers fantasy, Pratchett glisse vers la critique de nombreux concepts bien réels. Les personnages mènent l'histoire, et ce sont leur pérégrinations et leur caractère qui amènent les situations humoristiques. L'auteur le reconnait lui-même : 

L'univers gagne en profondeur, mais devient aussi plus sombre. Le personnage de Samuel Vimaire, commissaire désabusé de la ville d'Ankh-Morpork, a progressivement rendu l'univers plus noir et complexe : les comportements sont plus cruels dans ce que les uns peuvent faire aux autres, mais plus nobles dans ce que les uns peuvent faire pour les autres. L'auteur reconnait que son écriture s'assombrit, mais que c'est parce qu'elle gagne en réalisme : l'humour reste certes un élément principal, mais n'est plus l'unique moteur. Cela se ressent sur les couvertures des livres, devenues moins cartoonesques.

Terry Pratchett pioche sans s'en cacher dans la littérature classique, la culture populaire et l'histoire ancienne, mais toujours en ajoutant une altération inattendue ; les personnages qui en ressortent sont une des principales sources de l'humour qu'on trouve dans son œuvre. Sa passion pour les romans policiers se ressent dans les apparitions fréquentes du guet municipal d'Ankh-Morpork dans les "Annales du Disque-monde". La plupart de ses personnages sont des enfants uniques, tout comme lui ; il explique que .

Ses premières inspirations sont "Le Vent dans les saules" de Kenneth Grahame et les œuvres d'Isaac Asimov et d'Arthur C. Clarke. Il découvre par la suite P. G. Wodehouse, Tom Sharpe, Jerome K. Jerome, Roy Lewis, G. K. Chesterton et Mark Twain.

Les critiques sont très favorables quant à l'ensemble de la carrière de Terry Pratchett. Selon Peter Ingham du ', . Pour Mat Coward du ', . Le journal ' écrit que . Alex Hamilton du ', lui, compare Pratchett à Roald Dahl. Sa série du "Disque-monde", malgré sa longueur, est qualifiée de très inventive.

Il est fréquemment considéré comme le meilleur écrivain humoristique actuel.

"Les Annales du Disque-monde", série la plus connue de Pratchett, présentent un univers de fantasy humoristique et parodique dans lequel l'action se déroule sur un monde en forme de disque (le Disque-monde), supporté par quatre éléphants reposant eux-mêmes sur le dos de la tortue géante A'Tuin qui voyage sans fin à travers le cosmos. Les romans suivent différents personnages, et les "Annales" peuvent être subdivisées en « sous-séries ». Entamée en 1983, la série compte fin 2011 près d'une quarantaine de volumes. Pratchett a également écrit plusieurs nouvelles prenant place sur le Disque-monde.

"Les Annales du Disque-monde" parodient de nombreux domaines : la science-fiction, la fantasy (notamment celle de Tolkien), la littérature (les pièces de Shakespeare ou les romans policiers par exemple), les films d’Ingmar Bergman, des pays existants (l’Australie, la Chine, l’Égypte antique, les Caraïbes), des inventions du (le rock 'n' roll, le cinéma), la religion, la philosophie, la monarchie, et bien d’autres choses encore. Mais plus qu’une parodie, le Disque-monde est , comme le dit Terry Pratchett lui-même.

Les romans des "Annales du Disque-monde" sont traduits en français par Patrick Couton, qui a reçu en 1998 le prix de traduction du grand prix de l'Imaginaire pour son travail.

En plus des romans, Terry Pratchett a écrit ou collaboré à un certain nombre de livres liés au "Disque-monde". "" (1994), coécrit avec Stephen Briggs, est un guide encyclopédique de l'univers de la série. La troisième édition, rebaptisée "Le Nouveau Vade-Mecum", paraît en 2003. La collaboration entre Pratchett et Briggs continue en 1995 avec "La Carte du Disque-monde" qui comprend une grande carte du "Disque-monde", ainsi qu'un livret contenant les biographies des principaux explorateurs du monde. Plus tard, trois nouvelles cartes sont publiées, représentant Ankh-Morpork, Lancre et le domaine de la Mort. Avec Tina Hannan, Terry Pratchett écrit en 1999 "Les Recettes de Nounou Ogg" (""), un livre de cuisine humoristique.

En 1999, il s'entoure du mathématicien Ian Stewart et du biologiste Jack Cohen pour écrire le premier livre de la série "La Science du Disque-monde", dont les trois autres volets sortent en 2002, 2005 et 2013. Dans ces livres, la fiction côtoie la réalité : les personnages du "Disque-monde" y étudient le « Globe-monde », une métaphore de la Terre, et tentent d'analyser leurs découvertes. En 2004, Pratchett est récompensé pour ces ouvrages de vulgarisation scientifique par un diplôme honorifique de l'université de Warwick, dont sont issus les deux scientifiques coauteurs.

Le premier livre pour enfants de Terry Pratchett est aussi son premier roman publié : "Le Peuple du Tapis" ('). Il crée ensuite la trilogie "Le Grand Livre des gnomes" ('). Les trois volumes, "Les Camionneurs" (1988), "Les Terrassiers" (1990) et "Les Aéronautes" (1990), ont pour thème de petites créatures ressemblant à des gnomes. Pratchett se lance ensuite dans l'écriture d'une nouvelle trilogie, les "Aventures de Johnny Maxwell", qui suit les péripéties d'un garçon et de ses amis. Les trois opus : "Le Sauveur de l'humanité", "Johnny et les Morts" et "Johnny et la Bombe" sont respectivement sortis en 1992, 1993 et 1996.

En 2012, Terry Pratchett revient au livre pour enfant avec "Roublard" (""), un roman dont l'action se situe dans les bas-quartiers les plus pauvres de Londres dans les années 1850, et dans lequel le personnage principal éponyme croise la route de personnages réels tels que Charles Dickens ou Robert Peel.

En , un recueil de nouvelles pour enfants de Terry Pratchett est publié sous le titre "Les Dragons de Château-Croulant" ('), illustré par Mark Beech. Un autre recueil a suivi, "L'Aspirateur de la sorcière" ('), illustré également par Mark Beech, en 2016. Un troisième recueil, "", est sorti en 2017.

Après "Le Peuple du Tapis", Terry Pratchett écrit deux livres de science-fiction : "La Face obscure du Soleil" et "Strate-à-gemmes", avant d'entamer la série du "Disque-monde". Il sort à plusieurs reprises de cet univers et publie en 1989 "Sacrés Chats", un livre humoristique sur les chats illustré par Gray Jolliffe.

Il entre ensuite en collaboration avec l'écrivain Neil Gaiman, qu'il avait rencontré en 1985 : Gaiman, alors journaliste, effectuait une des premières interviews de Pratchett. Les deux hommes se lient d'amitié et décident d'écrire un roman, qu'ils pensent au premier abord comme une parodie de la série de romans "William" de Richmal Crompton. Le scénario évolue, et se transforme en une parodie de l'Apocalypse ; le roman est publié sous le nom "De bons présages" ("") en 1990. Les années suivantes, il préface plusieurs ouvrages.

L'auteur se focalise ensuite sur le "Disque-monde", avant de ressortir de cet univers en 2008 avec "Nation", une uchronie qui se déroule dans le milieu du . Il collabore ensuite avec l'écrivain britannique Stephen Baxter sur une série basée sur des mondes parallèles à celui de notre Terre : en 2012 sort le premier opus, titré "La Longue Terre", suivi en 2013 par "La Longue Guerre" puis en 2014 par "La Longue Mars", en 2015 par "La Longue Utopie" et enfin en 2016 par "Le Long Cosmos".

En 2012 paraît le premier volume de l'intégrale des nouvelles de Terry Pratchett, intitulé "Fond d’écran" (""). En 2014 est publié un recueil similaire d'essais, titré "A Slip of the Keyboard" et dont la parution française, sous le titre "Lapsus clavis" est annoncée pour octobre 2017.

Les romans du "Disque-monde" ont remporté plusieurs prix : le "" en 1989 pour "Pyramides", la Carnegie Medal dans la catégorie enfants en 2001 pour "Le Fabuleux Maurice et ses rongeurs savants". En 2003, "Ronde de nuit" remporte le prix Prometheus du roman libertarien. Trois de ses romans, qui mettent en scène Tiphaine Patraque, obtiennent le prix Locus du meilleur roman pour jeunes adultes en 2004, 2005 et 2007. En 2010, "Je m'habillerai de nuit" remporte un prix Andre-Norton.

L'auteur s'est vu attribuer au total huit diplômes honorifiques : par l'université de Warwick en 1999, celle de Portsmouth en 2001, celle de Bath en 2003, celle de Bristol en 2004, celles du Buckinghamshire et du Trinity Collegeen 2008, celles de Bradford et de Winchester en 2009.

En 2003, la BBC établit la liste des 200 romans les plus appréciés au Royaume-Uni, la liste « "" ». Terry Pratchett fait partie des deux seuls auteurs, avec Charles Dickens, à voir cinq de ses romans dans le top 100, et est l'écrivain avec le plus de romans dans le classement total des 200 ouvrages, avec quinze apparitions, dont quatorze font partie de la série "Disque-monde".

Terry Pratchett décroche en 1994 le prix de l'« auteur de fantasy et science-fiction de l'année » aux "". En 1998, il est nommé officier de l'Ordre de l'Empire britannique (OBE) pour « services à la littérature », puis pour la même raison, il est anobli en 2008. Il commente plus tard : , suggérant que le titre était une reconnaissance de son succès plutôt qu'une réelle approbation du genre fantasy. Il ajoute : .

En 2009, la « » lui remet le prix Edward E. Smith pour sa contribution à la science-fiction. En 2010, il reçoit un prix World Fantasy pour l'ensemble de sa carrière, puis, en 2011, le prix , par l', pour sa contribution significative et durable à la littérature pour jeunes adultes. Enfin, la "Science Fiction and Fantasy Writers of America" lui décerne à titre posthume le prix Solstice 2015 pour son impact significatif dans le domaine des littératures de l'imaginaire.

Plusieurs romans du Disque-monde ont été adaptés en téléfilms, notamment par Sky1, une chaîne britannique. Terry Pratchett suit de près la réalisation de ces adaptations : très présent dans les coulisses, il joue le rôle d', en plus d'apparaître en caméo. Il affirme : Les altérations apportées par la réécriture au format télévisé, bien que peu nombreuses sur les adaptations de Sky1, ont agacé l'auteur. Il confie : 

Trois jeux vidéo inspirés directement de l'univers de Terry Pratchett ont été développés et distribués : "Discworld" en 1995, "" en 1996, et "Discworld Noir" en 1999. L'auteur s'est énormément impliqué sur le premier opus, où il avoue que son rôle consistait à . Les deux volets suivants ont nécessité moins d'investissement de sa part, tant les développeurs se sont imprégnés de l'univers de l'auteur.





</doc>
<doc id="3081" url="https://fr.wikipedia.org/wiki?curid=3081" title="Unix">
Unix

Unix, officiellement UNIX (parfois écrit « Unix », avec des petites capitales), est un système d'exploitation multitâche et multi-utilisateur créé en 1969 par Kenneth Thompson. Il repose sur un interpréteur ou superviseur (le "shell") et de nombreux petits utilitaires, accomplissant chacun une action spécifique, commutables entre eux (mécanisme de « redirection ») et appelés depuis la ligne de commande.

Particulièrement répandu dans les milieux universitaires au début des années 1980, il a été utilisé par beaucoup de start-ups fondées par des jeunes entrepreneurs à cette époque et a donné naissance à une famille de systèmes, dont les plus populaires à ce jour sont les variantes de BSD (notamment FreeBSD, NetBSD et OpenBSD), GNU/Linux, iOS et macOS. D'une manière générale, la quasi-totalité des systèmes PC ou mobile les plus courants (à l'exception des Windows NT) sont basés sur le noyau de Unix, y compris ceux commercialisés par Apple. On nomme « famille Unix », « systèmes de type Unix » ou simplement « systèmes Unix » l'ensemble de ces systèmes. Il existe un ensemble de standards réunis sous les normes POSIX et single UNIX specification qui visent à unifier certains aspects de leur fonctionnement. 

Le nom « UNIX » est une marque déposée de l'Open Group, qui autorise son utilisation pour tous les systèmes certifiés conformes à la single UNIX specification ; cependant, il est courant d'appeler ainsi les systèmes de type Unix de façon générale. Il dérive de « Unics » (acronyme de « Uniplexed Information and Computing Service »), et est un jeu de mot avec « Multics », car contrairement à ce dernier qui visait à offrir simultanément plusieurs services à un ensemble d'utilisateurs, le système initial de Kenneth Thompson se voulait moins ambitieux et utilisable par une seule personne à la fois avec des outils réalisant une seule tâche.

En 1969, Ken Thompson qui travaillait alors pour les laboratoires Bell développa la première version d'un système d'exploitation mono-utilisateur sous le nom de "New Ken's System". Il réalisa ce travail sur un mini-ordinateur PDP-7 ("Programmed Data Processor") de marque DEC animé par General Comprehensive Operating System et rédigea le nouveau logiciel en langage d'assemblage. Le nom Unics fut suggéré par Brian Kernighan à la suite d'un jeu de mots « latin » avec Multics; « Multi- car Multics faisait la même chose de plusieurs façons alors qu'Unics faisait chaque chose d'une seule façon ». Ce nom fut par la suite contracté en Unix (pour être déposé finalement sous le nom UNIX par AT&T), à l'initiative de Brian Kernighan.

Un jugement d'expédient datant de 1956 interdisait à l'entreprise AT&T, dont dépendait Bell Labs, de commercialiser autre chose que des équipements téléphoniques ou télégraphiques. C'est la raison pour laquelle la décision fut prise en 1975 de distribuer le système Unix complet avec son code source dans les universités à des fins éducatives, moyennant l'acquisition d'une licence au prix très faible.

En 1971, conscient de la difficulté que représente la maintenance d'un système écrit en langage d'assemblage, Ken Thompson songea à réécrire Unix en TMG, mais il trouva que le TMG n'offrait pas ce dont il avait besoin. Pendant une courte période il songea à réécrire Unix en Fortran, mais finalement conçut le B avec l'aide de Dennis Ritchie dans les années 1969 et 1970, en s'inspirant du langage BCPL. Cependant Unix ne fut jamais réécrit en B ; le B ne supportait pas les types, toutes les variables étaient de la même taille que les mots de l'architecture, l'arithmétique sur les flottants n'était pas implémentée ; de plus, le compilateur B utilisait la technique du . C'est pourquoi Dennis Ritchie entreprit en 1971 d'écrire le "New B", qui fut renommé en C. Le langage C est toujours l'un des langages les plus utilisés aujourd'hui.

Ken Thompson et Dennis Ritchie présentent le premier article sur Unix au Symposium on Operating Systems Principles à l'université de Purdue en 1973. Le professeur Bob Fabry de l'université de Californie Berkeley (UCB), alors dans l'auditoire, est immédiatement intéressé et en Keith Standiford, étudiant de , installe la "Version 4" à l'UCB, distribuée sur bande magnétique.

Début 1975, Ken Thompson passe une année comme professeur invité à son "alma mater", l'UCB. Avec Jeff Schriebman et Bob Kridle, ils mettent sur pied la "Version 6". C'est à ce moment qu'Unix commença à être diffusé hors des laboratoires Bell.

À l'automne 1975, Bill Joy et Chuck Haley, alors en second cycle, s'intéressent au nouveau système et implémentent l'éditeur en ligne "ex" en Pascal, et finissent par explorer le fonctionnement du noyau au moment du départ de Ken Thompson. Le développement fut également rejoint par Alan Snyder, Steven C. Johnson, Michael Lesk dans cette période allant de 1973 à 1977. Au début de cette dernière année, Bill Joy réalise la première distribution dite "Berkeley Software Distribution" (BSD). Plus tard, avec l'arrivée de nouveaux terminaux, il écrit vi (l'éditeur visuel), une surcouche de "ex". L'été 1978, la "Second Berkeley Software Distribution" ou 2BSD voit le jour.

Parallèlement, les concepts de datagramme et d'informatique distribuée émergent, avec Arpanet, le réseau Cyclades et la Distributed System Architecture, devenue en 1978 le modèle OSI-DSA. Plus de communications entre les machines des différents centres de recherche démontre l'utilité de systèmes d'exploitation ouverts et convergents, ce qui deviendra une nécessité avec les premières stations de travail incluant TCP/IP de Sun Microsystems, créée par Andy Bechtolsheim, Bill Joy, Vinod Khosla et Scott McNealy.

Puis en , Bill joy distribue 3BSD, la première distribution à supporter les ordinateurs VAX de DEC. C'est également cette année que sort la version 7, qui s'accompagne de nombreuses modifications notables telles que l'extension à de la taille maximale d'un fichier, l'ajout de plusieurs utilitaires, et surtout une plus grande "portabilité" du système, c'est-à-dire qu’il devient plus facile de le modifier afin qu'il fonctionne sur d'autres plates-formes matérielles. C'est à cette époque que le premier grand portage d'Unix, la version 32/V, fut réalisé, sur un VAX 11/780.

Dès la fin de l'année 1977, des chercheurs de l'université de Californie apportèrent de nombreuses améliorations au système Unix fourni par AT&T et le distribuèrent sous le nom de Berkeley Software Distribution (ou BSD). Ainsi BSD fut par exemple le premier système Unix à exploiter pleinement le mécanisme de mémoire virtuelle paginée du VAX 11/780.

Trois branches principales de développement des sources virent le jour :

Ces branches se sont mutuellement empruntées du code et/ou des concepts. Ainsi :

Lors de la publication de 3BSD à la fin des années 1970, la Defense Advanced Research Projects Agency (DARPA) prend connaissance des avancées réalisées à l'UCB. Ils ont l'intention d'utiliser Unix pour leurs projets. De nombreux ingénieurs espèrent alors la création de standards innovants face au monopole IBM.

En automne de cette même année, Bob Fabry propose à la DARPA une version augmentée de 3BSD pour répondre à leurs besoins.
Un contrat de 18 mois est signé en , et Bob Fabry rassemble une équipe. Bill Joy, qui vient juste de soutenir sa thèse de doctorat, se propose d'y participer. Les versions se succèdent jusqu'à 4.1BSD.

Satisfaite, la DARPA signe pour deux ans supplémentaires et le budget est presque multiplié par cinq.
Le nombre de personnes impliquées croît vite. Le "steering committee" est formé pour aider à définir l'évolution du système.

Ce groupe se réunit deux fois par an entre et , et comprend en particulier Bob Fabry, Bill Joy et Sam Leffler de l'UCB, Dennis Ritchie des Bell Laboratories, Duane Adams et Bob Baker de la DARPA, ainsi que du personnel et des élèves de plusieurs autres universités, en particulier Stanford, Carnegie-Mellon et l’université de Californie à Los Angeles. À partir de , ce sont des ateliers réunissant bien plus de personnes qui prennent le relais.

C'est Rob Gurwitz qui publie la première implémentation des protocoles TCP/IP, les protocoles de l'Internet d'aujourd'hui. Bill Joy l'intègre au système et ajuste les performances. Cette implémentation est considérée par beaucoup comme l'implémentation de référence. Elle est reprise plus tard par Microsoft pour le système d'exploitation Windows, ce qui est possible grâce à la licence BSD très permissive sous laquelle BSD est publié. Vers la fin de l'été 1982, Bill Joy annonce qu'il rejoint Sun Microsystems, et Sam Leffler prend la suite du projet.

En , 4.2BSD est publié ; c'est la première version qui inclut la nouvelle pile TCP/IP. Sam Leffler quitte l'UCB pour Lucasfilm et Mike Karels le remplace. 4.2BSD est alors très populaire et est plus vendue que toutes les autres distributions réunies, et que le System V d'AT&T, en particulier parce que ce dernier n'avait ni la communication par réseau ni le système de fichiers FFS (Berkeley Fast Filesystem).

À la conférence Usenix de 1985, 4.3BSD est annoncé. De nouvelles architectures matérielles deviennent supportées, et, avec la version 4.3-Tahoe, le noyau est scindé en parties dépendantes et indépendantes du matériel.

Début 1992, Unix System Laboratories (USL), composante d'AT&T chargée de développer et vendre Unix, commence à poursuivre Berkeley Software Design, Incorporated (BSDI), mise en place pour développer et vendre une version commerciale. Le procès n'aboutit pas comme le souhaitait USL qui lance alors un autre procès contre BSDI et l'UCB.
USL est vendu par AT&T à Novell. En , un accord est trouvé :
En , FreeBSD 2.0 sort avec les nouveaux fichiers de Net/2, alors appelée 4.4BSD-Lite, et des éléments de 386BSD.

Jusqu'à 4.3BSD-Tahoe, la licence AT&T s'applique toujours aux sources, qui sont toujours distribuées. Les utilisateurs ne sont pas des utilisateurs passifs mais participent activement au développement et améliorent progressivement le code original d'AT&T. La licence d'AT&T sur les sources étant devenue excessivement chère, les dernières sources originales ont été nettoyées du code d'AT&T, et en , la première BSD libre, la "Networking Release 1" ou Net/1 est publiée.

La licence est volontairement très libérale : le logiciel peut être redistribué ou vendu, avec ou sans modification des sources, sous forme binaire (compilée) ou non. Les notices de copyright dans les sources doivent être laissées intactes, et la documentation doit mentionner l'origine du code (l'université de Californie à Berkeley, UCB).

Net/1 alors coûte à l'UCB pour la bande magnétique qui le transporte, et est mis à disposition par connexion FTP (file transfert protocol) anonyme (pas de mot de passe requis).

Le système de mémoire virtuelle du système d'exploitation MACH de l'université Carnegie-Mellon est importé, et 4.3BSD-Reno sort début 1990.

L'incompatibilité grandissante entre les nombreuses variantes d'Unix proposées par les différents éditeurs pour les différentes machines porte peu à peu atteinte à la popularité et à la diversité des systèmes Unix. De nos jours, les systèmes Unix propriétaires, longtemps majoritaires dans l'industrie et l'éducation, sont de moins en moins utilisés. En revanche, trois systèmes de type Unix basés sur BSD (FreeBSD, NetBSD et OpenBSD) d'une part, et le système GNU/Linux, compatible Unix, d'autre part, ainsi que macOS (anciennement OS X, basé sur Darwin), occupent une part de marché de plus en plus importante.

Bill Jolitz à partir de "Networking Release 2" publie 386/BSD, destiné à une architecture PC (386), mais est vite débordé quant à sa maintenance. Quelques mois après sa publication, des utilisateurs de 386BSD forment le groupe NetBSD, et rassemblent leurs ressources pour maintenir et améliorer ce système. Leurs objectifs sont alors de faire en sorte que NetBSD fonctionne sous n'importe quel matériel. Le public cible de NetBSD est des développeurs-administrateurs de haute technicité.

Encore quelques mois plus tard, le groupe FreeBSD se forme et décide lui de se focaliser sur l'architecture PC. En , grâce au soutien de Walnut Creek CDROM, FreeBSD 1.0 est publié.

Le projet OpenBSD est créé en 1995 à la suite d'un désaccord entre l'un des développeurs de NetBSD, Theo de Raadt, et les autres membres du projet. Il se focalise sur la sécurité informatique.

GNU est un système d'exploitation lancé en 1983 par Richard Stallman dans le but de fournir un équivalent d'Unix composé uniquement de logiciel libre. Bien que compatible avec Unix, GNU s'en démarque notamment par sa grande utilisation du Lisp.

En 1991, alors que le noyau de GNU, le Hurd traînait à être opérationnel, fut créé le noyau Linux "(voir ci-dessous)" qui fut libéré en 1992. Cela permit d'utiliser pour la première fois un système d'exploitation entièrement libre, une variante de GNU utilisant le noyau Linux connue sous le nom de GNU/Linux, ou plus courammment, simplement Linux.

GNU et GNU/Linux sont utilisés sous la forme de distributions qui les accompagnent de logiciels supplémentaires. Parmi les distributions les plus populaires, on compte notamment Debian, Ubuntu, Linux Mint, Red Hat, Fedora et Arch. Parmi ces six distributions, seuls deux proposent, à la place de Linux, l'utilisation du Hurd : Debian et Arch. De plus, Debian propose d'utiliser deux noyaux issu de la famille des BSD avec les distributions Debian GNU/kFreeBSD et Debian GNU/NetBSD.

En 1991 un étudiant finlandais, Linus Torvalds, décida de concevoir, sur le modèle de Minix, un système d'exploitation capable de fonctionner sur les architectures à base de processeur Intel 80386. Le noyau, qui était alors au stade expérimental, devait être généré sur un système Minix.

Le nom de Linux vient de la personne qui hébergeait le projet pour sa diffusion (version 0.0.1) et non d'un choix de Linus. Il voulut un temps rebaptiser son système "Freax", mais il était trop tard, Linux s'était déjà imposé auprès des aficionados. Linux ne contient aucun code provenant de UNIX, il en est juste inspiré, et complètement réécrit. D'autre part, Linux est un logiciel libre.

Linux lui-même n'étant qu'un noyau, il nécessite d'être accompagné d'autres logiciels pour former un système d'exploitation. Une des possibilités les plus populaires est l'utilisation de Linux en tant que noyau du système d'exploitation GNU pour constituer un système désigné sous le nom "GNU/Linux" ou simplement "Linux". Plusieurs entreprises ou associations distribuent Linux et GNU accompagné d'un ensemble cohérent de logiciels ; on appelle distribution Linux un tel système.

Android est un système d'exploitation pour terminaux mobiles développé par une startup du même nom et racheté par Google. Il est basé sur le noyau Linux et la machine virtuelle Dalvik ou à partir de Android KitKat: Android Runtime (ART).

Unix est à l'origine de macOS (précédemment Mac OS X), l'actuelle version du système d'exploitation d'Apple. macOS, comme Darwin est basé sur le noyau XNU, un dérivé du micro-noyau Mach.

En , la version 10.5 ("Leopard") de Mac OS X sur Intel a reçu la certification UNIX 03 du Open Group.

Dès 1977, AT&T mit les sources d'Unix version 6 à la disposition d'autres entreprises. Ainsi, tandis que l'opérateur téléphonique poursuivait ses développements avec la version 7 puis le System V, un grand nombre de dérivés d'Unix furent développés :

Au milieu des années 1980, un professeur américain installé aux Pays-Bas, Andrew Tanenbaum, développa un système d'exploitation minimal, baptisé Minix, afin d'enseigner les concepts des systèmes d'exploitation à ses étudiants ; la première version fut publiée en 1987, et était accompagnée d'un livre détaillant la conception du système. Un projet similaire nommé XINU (pour "Xinu Is Not Unix") fit aussi son apparition dans les années 1980 sous la direction de Douglas Comer.

Le grand nombre de variantes d'Unix, chacune ayant ses spécificités, permet aux systèmes Unix d'être utilisés dans un grand nombre d'environnements différents.

Plusieurs systèmes d'exploitation pour appareils mobiles (smartphones, tablettes, PDA…) sont des systèmes Unix. On peut citer en particulier iOS et Android, qui se partagent plus de 85 % du marché des smartphones.

Depuis novembre 2017, Linux est le seul système d'exploitation utilisé par les 500 supercalculateurs les plus puissants du monde. Les autres systèmes Unix équipaient encore quelques-uns de ces ordinateurs en 2016. Entre 1995 et 2000, les systèmes Unix autres que Linux (notamment Berkeley Software Distribution, Solaris, AIX, UNICOS et HP-UX) équipaient plus de 90 % de ces ordinateurs.

Seules quelques versions d'Unix produites par de grands constructeurs de stations de travail et de serveurs subsistent aujourd'hui :

La philosophie des constructeurs de stations et serveurs Unix a été au départ de développer un système d'exploitation pour pouvoir vendre leurs machines, en y ajoutant si possible un petit « plus » pour se démarquer de la concurrence. C'était oublier que les parcs Unix sont le plus souvent hétérogènes et que toute différence d'une machine à l'autre, même créée avec la meilleure intention du monde, menace l'interopérabilité donc constitue un risque réel de contre-productivité car contraignent les informaticiens à de nombreuses manipulations fastidieuses afin d'interconnecter les systèmes.

C'est une des raisons pour lesquelles nombre de ces constructeurs proposent désormais le système GNU/Linux avec leurs serveurs. Toutefois, les différences entre les différentes distributions Linux posent souvent les mêmes problèmes, quoiqu'à un niveau moindre.

Certains logiciels de conception assistée par ordinateur ont longtemps été disponibles pour des stations de travail Unix uniquement, mais, ce marché se réduisant, sont également devenus disponibles pour d'autres systèmes. C'est par exemple le cas de CATIA, utilisé notamment par des grands constructeurs industriels comme Dassault Aviation, PSA Peugeot Citroën ou Boeing, qui fonctionne sous Microsoft Windows depuis la version 5 dont la version Unix a été abandonnée depuis la version 6.

Les systèmes Unix ont en commun plusieurs concepts développés dès les premières versions d'Unix aux laboratoires Bell.

Unix a initialement été conçu pour disposer de nombreux petits programmes, chacun effectuant un nombre limité de tâches, le plus souvent une seule, agissant le plus souvent sur des flux de texte et pouvant être interconnectés par le biais de pipes. Cette idée était relativement novatrice au début des années 1970, et a contribué au succès rapide d'Unix chez les programmeurs.

Les systèmes Unix disposent d'un grand nombre d'interpréteurs de commandes, appelés shells Unix. On peut notamment citer sh, bash et tcsh.

Une autre particularité d'Unix est de considérer un grand nombre d'objets comme des fichiers : dès les premières versions d'Unix, les périphériques d'entrée-sortie sont gérés comme des fichiers d'un type spécial. Cela permet par exemple, au niveau applicatif, d'écrire sur une bande magnétique de la même façon que sur un fichier standard qui serait stocké sur le disque.

Unix dispose d'un système de fichiers hiérarchique, qui supporte certaines fonctionnalités comme les liens symboliques, permettant de rediriger un fichier sur un autre, et un système de permissions permettant de donner des autorisations différentes au propriétaire du fichier, aux utilisateurs de son groupe, et aux autres utilisateurs.

Au contraire de nombreux autres systèmes (comme Microsoft Windows ou Mac OS Classic par exemple), qui disposent d'une racine de système de fichiers indépendante par périphérique de stockage ou par partition, le système de fichiers d'Unix a une unique racine, et les autres périphériques de stockage sont accessibles par des points de montage dans le système de fichiers. Par exemple, le dossier /home, qui contient les fichiers personnels des utilisateurs, est fréquemment stocké sur un périphérique ou une partition différente de la racine ; une fois ce périphérique montée sur le dossier /home, les demandes de fichiers situés dans /home seront redirigés vers ce périphérique.
L'organisation de l'arborescence du système de fichiers est définie par certaines conventions qui existent depuis la version 7 d'Unix, où est apparue la page de manuel qui la décrit. Le Filesystem Hierarchy Standard tente d'harmoniser les différences qui ont pu se développer, en particulier entre les différentes versions de Linux.

Le grand nombre de systèmes Unix développés sur la base du System V de AT&T ou bien de BSD conduisit des membres du groupe d'utilisateurs "/usr/group", qui a pris depuis le nom de UniForum, à forger un standard UNIX dès 1980 afin d'assurer une portabilité maximale entre les différents systèmes :

Aujourd'hui, la marque déposée UNIX est détenue par l'Open Group. Pour obtenir l'autorisation d'utiliser officiellement cette marque pour un système d'exploitation, il faut que celui-ci soit conforme à la Single UNIX Specification.




</doc>
<doc id="3083" url="https://fr.wikipedia.org/wiki?curid=3083" title="Ukraine">
Ukraine

LUkraine est un État d'Europe de l'Est, le deuxième d'Europe par sa superficie. Elle est bordée par la mer Noire et la mer d'Azov au sud, frontalière avec la Russie au nord et à l'est, avec la Biélorussie au nord, avec la Pologne, la Slovaquie et la Hongrie à l'ouest et avec la Roumanie et la Moldavie au sud-ouest.

Sa capitale est Kiev, sa langue officielle est l'ukrainien et sa monnaie est la hryvnia.

L'Ukraine est le foyer du premier État slave oriental, fondé par des Scandinaves : la Rous' de Kiev (appelée aussi dans les écrits occidentaux Ruthénie), qui durant les est l'État le plus vaste et aussi, après l'Empire byzantin, le plus puissant d'Europe.

Au , Kiev est prise aux Khazars par les Varègues (Vikings orientaux en russe venant de Suède) d’Oleh le Sage (de Novgorod). Située sur des routes marchandes lucratives, Kiev devient rapidement le centre d'un puissant État slave, appelé « Rus » ou Ruthénie.
Selon la tradition, en 988 eut lieu, sous le règne de Vladimir le "Beau Soleil", le baptême de ce que seront les peuples russe, ukrainien et biélorusse.
Sous le règne de Iaroslav le Sage (1016-1054), le prestige de l'État kiévien atteint son apogée : il s'étend alors de la mer Baltique à la mer Noire et du confluent de l'Oka avec la Volga jusqu'aux Carpates septentrionales. Iaroslav est un grand bâtisseur et un grand législateur. Le droit, l'éducation, l'architecture et l'art kiévien connaissent un renouveau impressionnant sous son règne. En 1051, il marie sa fille Anne de Kiev au roi Henri de France.

Cependant, au , des conflits éclatent entre différents seigneurs locaux. Ces conflits mènent l'État kiévien au déclin, fractionné en plusieurs principautés rivales. Kiev est saccagée par la principauté de Vladimir (1169) durant la lutte pour le pouvoir entre les princes, et plus tard par les Coumans et les Tatars Mongols aux . Ces derniers finissent par imposer leur souveraineté dans toutes les principautés ruthènes. La cruauté de l'autorité mongole, notamment en matière pénale, pousse les populations autochtones à fuir vers d'autres pays comme la Pologne, la Hongrie ou la Moldavie.

Durant le , les Polonais et les Lituaniens combattirent les Mongols et finalement toute l’Ukraine du nord-ouest passa sous l’autorité de la Pologne-Lituanie, qui annexe Kiev en 1362. Les Tatars se maintiennent dans la steppe pontique au nord de la mer Noire et en Crimée ; toutefois, de 1382 à 1484, le Grand Duché de Lituanie atteignit la mer Noire du côté d’Oçaq (ou Otchakiv, vers l’actuelle Odessa). La Lituanie prit le contrôle de la Volhynie au nord-ouest de l’Ukraine (y compris les régions autour de Kiev). Quant à la Pologne, elle prit le contrôle de la Galicie ; plus au sud la principauté de Moldavie était sa vassale (plusieurs citadelles et régions alors moldaves sont aujourd’hui ukrainiennes). Dans ces régions du nord-ouest, outre les Ukrainiens que l’on nommait à l’époque "Russyns", "Ruthènes", le pays comptait des Polonais, des Moldaves, des Allemands, des Arméniens, des Juifs et des Russes. À mesure que les Tatars perdaient du terrain, nombre de villes et villages furent fondés. La noblesse d’Ukraine occidentale fut souvent « polonisée ». La législation polonaise est introduite en Ukraine occidentale en 1434. Si la Pologne mène une politique relativement tolérante vis-à-vis de l’orthodoxie, elle favorise cependant le catholicisme qui progresse dans les territoires occidentaux de l'actuelle Ukraine.

L’influence polonaise pénètre plus lentement dans les territoires relevant du grand-duché de Lituanie. L’orthodoxie y garde sa prédominance. Pourtant, les rapports de force au sein de l’État polono-lituanien tournent à l’avantage des Polonais. L’Union de Lublin (janvier 1569) consacre le triomphe de la Pologne. La Lituanie perd la plus grande partie de ses possessions ukrainiennes (Podlachie, Volhynie, Podolie, région de Bratslav et de Kiev). La noblesse de ces régions se polonise et se convertit au catholicisme. Une partie du haut-clergé orthodoxe est tentée par le rapprochement avec Rome. Le métropolite de Kiev et une partie du haut-clergé, en réaction contre les interventions réformatrices du patriarche de Constantinople, se rallie à Rome lors du concile de Brešč (Brest-Litovsk) en 1596. L'Union de l'Église de la Rus' de Kiev avec Rome forma l'Église grecque-catholique ukrainienne faisant partie des "uniates.

C’est durant cette domination lituano-polonaise, à partir du , que se formèrent les Cosaques, des paysans ruthènes orthodoxes qui refusaient la servitude et l’assimilation aux Polonais catholiques. Le royaume de Pologne les tolère et les utilise contre les Tatars, puis, à partir du , contre les Turcs ottomans, devenus suzerains des Tatars de Crimée.

Le clivage entre le nord-ouest, orthodoxe mais d'influence polonaise et lituanienne, c'est-à-dire occidentale, et le sud-est soumis aux Tatars et aux Ottomans, puis conquis et colonisé par l'Empire russe, se retrouve jusqu'à aujourd'hui dans la structure politique du pays : le nord-ouest vote plutôt pour les pro-européens et se méfie de l'influence russe, tandis que le sud-est vote plutôt pour les pro-russes, se méfie de l'influence occidentale (souvent assimilée au fascisme depuis la Seconde Guerre mondiale) et peut même se soulever contre le pouvoir de Kiev lorsque ce dernier se rapproche de l'Ouest.

À la suite de la révolution paysanne anti-féodale (1648-1654), connue dans l'histoire comme "Hmelnichina", la partie orientale de l'Ukraine s’émancipe du pouvoir lituanien et se constitue en État autonome de caste cosaque : le Hetmanat cosaque, administré par les chefs cosaques et dirigé par un Hetman élu, est établi et perdure pendant plus d'un siècle malgré la pression des envahisseurs moscovites attirés par les terres riches et fertiles. À la suite du traité d'Androussovo, il est partagé en deux : une partie est placée sous le protectorat de la République des Deux Nations, l'autre sous un protectorat moscovite qui perdure pendant plus d'un siècle. Le territoire des Cosaques Zaporogues de la Sitch est tout d'abord cogéré par les deux souverains.

Catherine la Grande, impératrice de Russie, supprime le Hetmanat au milieu du et détruit la Sitch dans le dernier quart du . Le partage de la Pologne lui permet de récupérer pratiquement toute la rive droite à l'exception de la Galicie, passée sous administration de l'Autriche, laquelle deviendra en 1867 l'Empire austro-hongrois. Les grandes steppes incultes du sud sont colonisées par des paysans venus de tout l'Empire, mais aussi d'Allemagne ou de Hollande, appelés par l'impératrice en échange de privilèges fiscaux. Le port d'Odessa (dont le nom a été choisi d’après celui d’Ulysse), gouverné au début par le duc de Richelieu, est fondé à cette époque teintée de retour aux sources grecques (Tauride, Chersonèse).
La culture ukrainienne connaît une renaissance au milieu du , en parallèle avec le mouvement régionaliste à la même époque en Europe. Ce mouvement est concentré dans les régions de la Ruthénie, de la Volynie ou de la Podolie et autour de Zaporojié. C'est alors qu'apparaît de plus en plus le terme d'Ukraine relancé par les intellectuels à la fin du . Le pouvoir impérial russe officiellement ne connaît pas ce terme d'Ukraine. Il ne forme dans les territoires de l'actuelle Ukraine, comme partout ailleurs dans l'Empire que différents gouvernements ou provinces au sein de plusieurs entités : Petite Russie, Nouvelle Russie (correspondant en partie aux territoires enlevés à l'Empire ottoman), parties de la Bessarabie, etc. En 1876, l'Empire interdit la langue ukrainienne dans les écoles, et la limite dans les journaux et la littérature. Cette limitation provoque en retour une revendication idéologique qui permet de comprendre l'opposition linguistique actuelle. Les différentes formes d'ukrainien ne sont plus parlées que par une frange de la paysannerie et certains cercles cultivés de régionalistes : instituteurs, universitaires, ecclésiastiques.

De grandes villes sont fondées sous l'Empire russe, comme Odessa mentionné plus haut et Ekaterinoslav, Sébastopol, etc. qui accueillent des migrants de tout l'Empire, et même d'Europe centrale : de la Pologne autrichienne ou d’Allemagne. En 1892, Kiev compte près d'un demi-million d'habitants. En effet, après l'abolition du servage en 1861, l'industrialisation provoque un exode rural de paysans russes, ukrainiens, ruthènes, etc. dans les nouveaux centres industriels. Le négoce se développe parallèlement avec l'extension du chemin de fer et cette et l'ouest.

Après la révolution de Février, qui met fin à l’Empire en 1917, l'Ukraine est brièvement indépendante jusqu'en 1920, mais la Rada ne parvient pas à contrôler efficacement le territoire, envahi d'abord par les Allemands puis, à leur retrait, devenu champ de bataille entre le Parti bolchevique, les Russes blancs et les forces de la Triple-Entente.

Le , la plupart des partis politiques s’accordent pour former la Rada ukrainienne centrale. Le , alors qu'il est toujours à Moscou, Mykhaïlo Hrouchevsky est élu président de la Rada centrale. Sous son impulsion, l'Ukraine proclame son autonomie le . En tant que chef de l'USDRP, Volodymyr Vynnytchenko est choisi comme un des deux vice-présidents de la Rada centrale puis comme le premier président du secrétariat général de la Rada centrale du gouvernement autonome de l'Ukraine.

Le , soit treize jours après que le Parti bolchevique russe a renversé le gouvernement social-démocrate de Saint-Pétersbourg , la Rada ukrainienne centrale proclame la république populaire d’Ukraine et sa séparation de la Russie. L'indépendance totale de l'Ukraine est confirmée le et Mykhaïlo Hrouchevsky est élu officiellement « président de la République populaire ukrainienne » le .

Le traité de Brest-Litovsk est signé le entre les Bolcheviks russes, les gouvernements des empires centraux menés par l'Empire allemand et la jeune république populaire d’Ukraine, issue de la révolution de Février, dans la ville du même nom, aujourd’hui Brest en Biélorussie. Les 17-, la république socialiste soviétique d'Ukraine est fondée à l'Est du pays avec pour capitale Kharkov.

Pour combattre l'Armée rouge qui contrôle alors une partie de l’Ukraine, la Rada centrale cherche le soutien des Allemands qui organisent un coup d’État et renversent le gouvernement de Vynnytchenko, mettant à sa place Pavlo Skoropadsky qui, le , est proclamé hetman de l’« État ukrainien » : Ukrayinska Derjava. Mais l’Allemagne perd la Première Guerre mondiale et Skoropadsky, resté sans soutien, est renversé par le mouvement populaire, guidé par Simon Petlioura. Finalement, le , la république populaire d’Ukraine est rétablie avec Vynnytchenko à sa tête.

À la fin de 1918, les Alliés interviennent dans le sud de l'Ukraine pour soutenir les Blancs de Dénikine dans la guerre civile russe. Odessa, Sébastopol et d'autres localités côtières sont occupées par les Français, mais l'intervention tourne court en raison du manque de moyens engagés et de l'hostilité de la population (mars-). L'Ukraine est envahie par l'Armée rouge et ramenée dans le giron soviétique. L'ancien « grenier » de l'Empire russe, devenu une république socialiste soviétique, ravitaille les centres urbains soviétiques. Le , l'Union des républiques socialistes soviétiques (URSS) naît du traité qui réunit la RSFSR, la Biélorussie, l'Ukraine et la Transcaucasie. Dans le conflit qui oppose les communistes du centre (Moscou) et les partis communistes nationaux, c'est le centre qui l'emporte et impose une fédération.

Quand Staline déclenche sa révolution industrielle vers la fin des , l'Ukraine devient l'une des sources indispensables de son financement. Les années d'industrialisation sont marquées par la construction de ce qui est à l'époque la plus grande centrale hydraulique d'Europe sur le Dniepr (le DnieproGuES), ce qui contribue à l'électrification de la République, ainsi qu'une importante mise en valeur du grand bassin minier et métallurgique, le Donbass, déjà exploité depuis la fin du .

Après une brève période d'ukrainisation (campagne dite de "korenizatsiya") dans les , se traduisant par le retour à l'ukrainien dans les publications, la réouverture des écoles et des universités avec un enseignement en ukrainien et la promotion des cadres nationaux, Staline ne ménage pas les efforts pour réprimer le moindre signe d'un réveil nationaliste ukrainien, interprété comme un rejet du pouvoir bolchevik et une menace à l'intégrité de l'URSS. De plus des oblasts russes, comme celle de Kharkov, sont intégrées à la RSS d'Ukraine pour renforcer le poids des russophones.

Entre 1931 et 1933, une série de famines et l'intensification de la dékoulakisation frappent l'Union soviétique et ravagent particulièrement l'Ukraine, alors que cette région était la plus fertile de toute l'URSS. Entre 2,6 et de personnes meurent des suites de cette famine. De nombreux historiens soutiennent que Staline a utilisé cette famine, voire l'a sciemment provoquée, pour briser la paysannerie et le nationalisme ukrainiens, même si la part de responsabilité du régime et ses intentions restent très débattues. Les Ukrainiens l'appellent « Holodomor » ou « l'extermination par la faim ». Le Parlement européen a reconnu dans une résolution de 2008 l'Holodomor comme un « crime effroyable perpétré contre le peuple ukrainien et contre l'humanité ». 

Des exécutions et des déportations de nationalistes ukrainiens sont orchestrées durant les purges staliniennes de 1937-1939 : plusieurs millions d'Ukrainiens sont exécutés ou envoyés vers des camps de travail soviétiques, comme le sont aussi tous les suspects de nationalisme dit , les Russes en premier. En outre, le marxisme-léninisme appliqué par le Kremlin prône l'athéisme d'État et s'attaque aux symboles religieux, détruisant les églises et les cathédrales de toute l'URSS et des millions de croyants en majorité orthodoxes, mais aussi d'autres obédiences chrétiennes, sont envoyés au Goulag. De même l'islam est étouffé.

En septembre et , après l'invasion de la moitié occidentale de la Pologne par les troupes allemandes puis, de sa partie orientale, par les troupes soviétiques, certaines régions polonaises à forte minorité ukrainienne (comme la Galicie et Lwow, aujourd'hui Lviv) sont annexées par l'Union soviétique et incorporées au sein de l'Ukraine occidentale.
À l'été 1941, l'Ukraine est envahie par les armées allemandes. À leur arrivée, les Allemands sont reçus en libérateurs par une partie de la population ukrainienne, surtout par la population de la partie de la Pologne envahie par Staline en 1939 puis intégrée à l'Ukraine. Mais, au fur et à mesure de leur progression vers l'est du pays, et notamment en raison des mauvais traitements infligés à la population, les nazis rencontrent une forte résistance de la part de la population locale, laquelle perdure jusqu'au retour des Soviétiques en 1944. En représailles, les Allemands traquent les partisans, et brûlent des centaines de villages et des milliers de maisons avec leurs habitants. La population juive d'Ukraine est anéantie par l'application de la solution finale.

Le , le haut commandement de la Wehrmacht annonce la création de la division SS Galicie constituée de volontaires ukrainiens ; les historiens estiment que plus de se sont engagés aux côtés des forces allemandes durant la Seconde Guerre mondiale pour combattre le régime soviétique (Polizei, U.V.V., Hiwis ou Waffen-SS).

En 1944, l'Armée rouge libère la plus grande partie de l'Ukraine.

À la fin du conflit, le bilan des pertes ukrainiennes est de de morts dont étaient des militaires.

Quant aux indépendantistes , ils continuent une résistance locale armée contre l'URSS jusqu'en 1954.

Le , l’Ukraine devient l’un des membres fondateurs de l'ONU, en y obtenant, en soulignement de son rôle dans la victoire sur le nazisme, avec la Biélorussie, une place distincte de l'URSS. Cette disposition particulière permet à l'Union soviétique de bénéficier de voix supplémentaires dans les votes de l'assemblée générale de l'ONU.

En 1954, le du Parti communiste d'Union soviétique, Nikita Khrouchtchev qui a passé sa jeunesse en Ukraine, transfère la péninsule de Crimée à la République soviétique socialiste d'Ukraine pour marquer le du traité de Pereïaslav marquant l'union entre la Russie et les provinces formant l'Ukraine d'alors. L'Ukraine est considérée comme un modèle des républiques soviétiques. Notamment, Léonid Brejnev, le principal dirigeant de l'URSS pendant entre 1964 et 1982, est d'origine ukrainienne.

C'est seulement vers 1989 que la libéralisation du régime soviétique et la libération des détenus politiques permettent aux Ukrainiens de s'organiser pour défendre leurs droits à la souveraineté. En 1989, le Mouvement national ukrainien, Roukh, est créé. Lors des élections de , les partis ukrainiens du bloc démocratique obtiennent alors environ 25 % des sièges au Parlement. Sous l'influence des députés démocrates, le Parlement adopte, le , la Déclaration sur la souveraineté politique de la République d'Ukraine. C'est le premier pas vers l'indépendance complète de l'Ukraine. Celle-ci est proclamée le et confirmée par le référendum du : 90,5 % des électeurs votent en faveur de l'indépendance.

Le , la dislocation de l'URSS est actée par l'Accord de Minsk, signé par les dirigeants russe, ukrainien et biélorusse.

L'Ukraine devient l'un des membres fondateurs de la Communauté des États indépendants.

Par le Mémorandum de Budapest sur les garanties de sécurité, signé le , l'Ukraine abandonne son arsenal nucléaire en échange de la garantie par les États-Unis, le Royaume-Uni et la Russie de son intégrité territoriale.

À la suite du refus du gouvernement Ianoukovytch de signer des accords de rapprochement avec l'Union européenne, le renforcement du mouvement Euromaïdan provoque un renversement du pouvoir. Très rapidement, une crise éclate entre les territoires majoritairement russophones du sud-est du pays et le nouveau pouvoir central de Kiev.

Le 11 mars 2014, la Crimée proclame son indépendance, puis à la suite d'un référendum est rattachée à la Russie le 18 mars. Ce référendum et le rattachement qui a suivi ont été condamnés par l'Ukraine et une large part de la communauté internationale. Ainsi, le , l'Assemblée générale de l'ONU a voté la sur « l'intégrité territoriale de l'Ukraine », la majorité des pays condamnant le rattachement de la Crimée à la Russie : dont les États-Unis et l'UE.

Une guerre civile, dite Guerre du Donbass, éclate ensuite dans l'Est de l'Ukraine majoritairement russophone, qui entraîne plus de dix mille morts.

L'Ukraine est la cible de cyberattaques dont le but est de réduire la légitimité du pouvoir ukrainien et tester de nouvelles cyberarmes, perturbant également l'économie. Les cyberattaques ont pu notamment arrêter des centrales nucléaires et empêcher les distributeurs de billet de distribuer de l'argent aux citoyens. Parmi les attaques, NotPetya (un logiciel malveillant) aurait affecté entre 70 à 80 % des ordinateurs des grandes entreprises. Bien que NotPetya ait été utilisé par la suite pour créer des attaques mondiales, d'après Microsoft, la première infection a eu lieu en Ukraine. Lors de l'annonce des résultats de l'élection présidentielle en 2014, la principale chaine de télévision, victime d'un piratage, a annoncé des résultats erronés.

En 2016, l'OSCE, une organisation chargée notamment d’observer le cessez-le-feu en Ukraine a été la cible d’une attaque de grande ampleur attribuée à Moscou. L’OSCE est le seul acteur indépendant capable de documenter des exactions ou de vérifier si les promesses faites par Kiev, les prorusses ou le Kremlin sont mises en application.

L'Ukraine est un pays d'Europe de l'Est. Elle partage ses frontières terrestres avec sept pays limitrophes : à l'ouest la Pologne, la Slovaquie et la Hongrie ; au sud-ouest la Roumanie et la Moldavie ; à l'est et au nord-est la Russie ; au nord la Biélorussie. Le pays mesure d'est en ouest et du nord au sud, pour une superficie totale de , ou sans compter la superficie de la Crimée.

À l’exception du vaste plateau de Podolie (altitude ) qui occupe l'ouest du pays, c'est un pays relativement plat, avec les terres fertiles du bassin du Dniepr en son centre, ce qui lui permet d'avoir une agriculture productive. Les montagnes ukrainiennes sont principalement constituées des contreforts des reliefs d'Europe centrale et méditerranéenne :

L’Ukraine bénéficie également d'un réseau fluvial étendu, composé principalement par le Dniepr (Dnipro), le Dniester (Dnister), le Boug occidental, le Boug méridional et le Donets à l'est. Le Danube (Dounay) marque la frontière à l'extrême sud-ouest entre l'Ukraine et la Roumanie.

Au sud, l'Ukraine s'ouvre sur la mer Noire, bordée de nombreux « limans », et où s'avance la presqu'île de Crimée.

Le climat de la majeure partie de l'Ukraine est continental avec des hivers froids et des étés chauds ; le climat n'est méditerranéen que sur la côte sud de la Crimée. Les températures moyennes à Kharkov en Ukraine orientale sont d'environ en janvier et en juillet. Les précipitations vont d'environ par an dans le nord à environ dans le sud.

L'Ukraine comporte diverses régions historiques, dont certaines, comme l'oblast de Ruthénie subcarpathique ou la république autonome de Crimée, peuvent correspondre à une subdivision administrative actuelle. Certaines de ces régions historiques, comme la Volhynie et la Galicie , la Bukovine ou la Méotide se prolongent également dans les pays voisins. D'autres sont intégralement ukrainiennes : la Podolie le Boudjak et le Yedisan , la Tauride et la Crimée, auparavant tatares sous suzeraineté turque. La plus vaste des régions historiques est la Zaporogue, pays des Cosaques du même nom, héritée des rapides du Dniepr.

L'Ukraine est divisée en régions administratives et une municipalité ("misto") avec un statut juridique particulier, Kiev . Par ailleurs, l'Ukraine revendique l'intégralité de la Crimée, autrement dit la ville à statut particulier de Sébastopol et la République autonome de Crimée, qui ont été rattachées en 2014 à la Russie et constituent actuellement le District fédéral de Crimée de la Fédération de Russie.


Les données liées à l'évolution du nombre d'habitants sont connues pour la période 1950-2012.

D'après le recensement de 2001, la répartition ethnique est la suivante : ; ; . La classification « Autres » comprend aussi des minorités religieuses et non ethniques comme les Juifs. Les Roumains / Moldaves sont le troisième groupe ethnique après les Russes. Les Ruthènes sont en Ukraine considérés comme Ukrainiens, et ne sont par conséquent pas répertoriés comme une nationalité séparée. Il y aurait donc 75,8 % d'Ukrainiens et environ 1 % de Ruthènes, selon les estimations.

Le pays compte une importante communauté musulmane estimée à de personnes : pour l'essentiel des Tatars de Crimée.

Plus d'un million d'Ukrainiens ont quitté leur pays depuis 1991 avec pour destinations privilégiées : l'Australie, les États-Unis, le Canada, Israël, l’Union européenne…

L'ukrainien est la langue officielle mais treize autres langues minoritaires sont reconnues, dont le russe qui domine dans l'Est du pays et à Odessa, et qui est compris par la plupart des Ukrainiens. Le fait que la plupart des Ukrainiens parlent le russe couramment est principalement la conséquence de l'intégration jusqu'en 1991 de l’Ukraine dans l'Union soviétique. Cependant, l'ukrainien est largement dominant dans l'Ouest de l'Ukraine tandis que le russe prédomine dans l'Est et le Sud du pays, ainsi que dans la capitale Kiev.

Chez les plus jeunes, surtout dans les grandes villes, le choix de l'anglais en seconde langue devient de plus en plus important. Dans l'Ouest du pays, on trouve des minorités qui parlent le polonais, le hongrois, le biélorusse, le roumain, le grec, le yiddish, ainsi que le tchèque et le slovaque. Déportés sous Staline après 1945, les Tatars de Crimée qui sont rentrés au pays, essentiellement après 1961, parlent surtout le russe. L’allemand qui jadis était une langue minoritaire a disparu presque complètement après la Seconde Guerre mondiale. Aujourd'hui, la langue allemande est surtout enseignée à l'université ; elle est considérée comme une langue à usage commercial, sans doute la troisième langue étrangère enseignée après le russe et l'anglais.

La Constitution a été adoptée par le Parlement le , après qu'un accord fut conclu entre le Parlement et le président en 1995.

L'Ukraine est une démocratie parlementaire où les pouvoirs présidentiels sont étendus (quoique réduits au profit du parlement). Le président d'Ukraine est élu au suffrage universel direct pour un mandat de cinq ans, renouvelable une fois. Il nomme le Premier ministre avec l'accord du Conseil suprême.

Le Parlement monocaméral ("Verkhovna Rada" ou plus simplement "Rada") est composé de 450 députés élus au suffrage universel direct pour un mandat de cinq ans (quatre ans avant 2006). Le mode de scrutin est mixte (car il combine à la fois scrutin proportionnel et scrutin majoritaire) jusqu'en 2006 puis uniquement proportionnel.

La Cour constitutionnelle contrôle la constitutionnalité des lois et peut être saisie entre autres par la Cour suprême d'Ukraine, le président ou le Parlement.

Alors que la présidence était assurée par Leonid Koutchma, un ancien apparatchik du Parti communiste, considéré comme corrompu et lié aux groupes mafieux, la dernière élection présidentielle a eu lieu le 31 octobre et . À la suite de soupçons de fraude et de la pression populaire, plus ou moins spontanée, de la Révolution orange, la a annulé le résultat du second tour qui donnait vainqueur l'ancien Premier ministre Viktor Ianoukovytch sur Viktor Iouchtchenko. Finalement, c'est ce dernier, jouant la carte de l'Europe et du libéralisme, qui l'a emporté bien que son adversaire ait maintenu ses solides positions dans l'Est et le Sud du pays, russophones et russophiles. Viktor Iouchtchenko prête serment en janvier 2005.

Il désigne alors comme premier ministre Ioulia Tymochenko, femme d'affaires entrée en politique du temps du président Koutchma. Sur fond d'accusations réciproques de corruption, le le président Viktor Iouchtchenko limogea le gouvernement du Premier ministre Ioulia Tymochenko, nommant à sa place Iouriï Iekhanourov, gouverneur de l'oblast de Dnipropetrovsk.

Des commentateurs comme Jean-Baptiste Naudet, reporter au "Nouvel Observateur" ont estimé que l'on peut observer, à travers plusieurs élections, une préférence pour les candidats pro-européens en Ukraine du Nord-Ouest jadis soumise à l'influence polono-lituanienne, et pour les candidats pro-russes en Ukraine du Sud-Est jadis soumise à la domination turco-tatare et délivrée de celle-ci par les cosaques et la Russie.

Les élections législatives qui suivirent ont eu lieu le . En voici les résultats :

Les partis qui n'ont pu obtenir un minimum de 3 % des voix ne sont pas représentés à la Rada.

La coalition parlementaire « orange » (Notre Ukraine — Bloc de Ioulia Tymochenko (BIT) — Parti socialiste) difficilement constituée après plus de deux mois de débats, a éclaté le , à la suite de la défection surprise du socialiste Oleksandr Moroz élu président du Parlement avec le soutien de l'opposition pro-russe. Cette défection a entraîné le ralliement des socialistes à la formation Parti des Régions – Communistes et à la création d'une nouvelle alliance majoritaire ( sur 450), cette fois dirigée par l'ex-premier ministre Viktor Ianoukovytch.

À la suite des pourparlers entre Iouchtchenko et Ianoukovytch entamés le , les deux anciens rivaux se sont mis d'accord sur la signature du pacte de l'unité nationale ("Universal"), qui marque les concessions politiques des deux côtés (entre autres, la soumission au référendum de la question de l'entrée du pays dans l'OTAN). Le groupe du Bloc Ioulia Tymochenko, jadis un allié de « Notre Ukraine », qui a quitté le siège de la Rada le 20 juillet en exigeant la tenue des législatives anticipées, n'a pas signé l'Universal. Il devient donc l'opposition officielle.

Le la Rada a nommé le chef du Parti des régions Viktor Ianoukovytch au poste de Premier ministre ukrainien. La candidature de Ianoukovytch a été appuyée par , pour 226 requises.

Le , le président Viktor Iouchtchenko dissout le parlement et provoque de nouvelles élections législatives. Elles eurent lieu le , les résultats étaient les suivants :

Lors des élections législatives anticipées du 30 septembre 2007, le bloc dirigé par Ioulia Tymochenko arrive en deuxième position avec 30,7 % des voix, gagnant presque huit points par rapport aux précédentes législatives de mars 2006 (22,9 %). Le parti des Régions de Viktor Ianoukovytch remporte les élections avec 34,4 % des voix. Après les premiers dépouillements, le « Bloc Ioulia Tymochenko » arrivait en tête et l'OSCE avait déclaré que les élections s'étaient déroulées de manière libre et équitable

Nommé Premier ministre par le président Iouchtchenko, elle ne parvient pas, cependant, à obtenir la majorité le 11 décembre, obtenant seulement sur les 226 requises.

De nouveau proposée au poste de Premier ministre, la Rada entérine sa nomination à la tête du gouvernement le 18 décembre lors d'un deuxième vote par sur les 450.

La "Rada" est dissoute par le président Iouchtchenko le à la suite de la crise parlementaire de septembre 2008 en Ukraine, une élection anticipée d'abord prévue pour le puis le , a été reportée pour début 2009, à une date indéterminée, en raison de la crise financière. Une nouvelle coalition se forme alors entre le parti de Volodymyr Lytvyn, le bloc Ioulia Tymochenko et Notre Ukraine. Volodymyr Lytvyn est élu président du Parlement, et celui-ci annonce que la "Rada" poursuivra son travail jusqu'en 2012.

Le premier tour de l'élection présidentielle s'est déroulé le . Le chef de l'opposition Viktor Ianoukovytch obtient 35 % des voix, et la première ministre Ioulia Tymochenko 25 %. Le président sortant Viktor Iouchtchenko réunit environ 5,5 % des voix.

Viktor Ianoukovytch emporte le second tour du 7 février avec 48,95 % des voix contre 45,47 % pour Ioulia Tymochenko. L'OSCE a annoncé que le scrutin avait été « transparent et honnête ».

Mykola Azarov, fidèle du président Ianoukovytch, accède au poste de Premier ministre le , à la suite d'une motion de censure votée le 3 mars contre Ioulia Tymochenko.

En novembre 2013, l'Ukraine renonce à signer un accord d'association avec l'Union européenne et . Ce revirement entraîne d'importantes manifestations pro-européennes à Kiev rassemblant des centaines de milliers de personnes, l'occupation du Maïdan Nézalejnosti et de la mairie, avec comme mot d'ordre la démission du président Viktor Ianoukovytch.

Au fil des jours, la capitale ukrainienne (Kiev) se transforme en champ de bataille. Les deux premiers décès ont lieu le mardi . La légitimité de Viktor Ianoukovytch est d'autant plus remise en cause après la mort de tués par balle le jeudi .

Le 22 février 2014, Viktor Ianoukovytch quitte Kiev pour Kharkiv et le régime politique est renversé. Alors que des rumeurs évoquent sa démission, le président dément, refuse de démissionner, parle d'un « coup d'État » qu'il compare à l'arrivée des Nazis en Allemagne. Quelques heures plus tard, le Parlement vote sa destitution et fixe au suivant la prochaine élection présidentielle par sur 450. Dans le même temps la libération de l'ancienne Première ministre Ioulia Tymochenko est votée et Oleksandr Tourtchynov est choisi pour diriger pour quelques mois l'Ukraine par intérim.

Les manifestants et la presse ont pu entrer facilement dans la Mejyhiria, la résidence de l'ancien président située dans la banlieue de Kiev. Ceux-ci ont été choqués par le train de vie que menait Viktor Ianoukovytch dans celle-ci.

En fin de compte, les affrontements ont fait au moins 82 morts chez les manifestants et 16 morts chez les forces de l'ordre (bercoutes) .

Après un bref passage par l'Est de l'Ukraine, le président déchu Viktor Ianoukovytch s'est réfugié en Russie. Un mandat d'arrêt est lancé contre lui pour « meurtres de masse ».

Le 11 mars, le Conseil suprême de Crimée proclame l'indépendance de la République autonome de Crimée, indépendance qui sera entérinée à la suite d'un référendum qui s'est tenu le 16 mars, et lors duquel la population a voté à une écrasante majorité pour un rattachement à la Russie. Les conditions de ce rattachement ont été critiquées par la communauté internationale.

Les élections sont marqués par une forte abstention, d’électeurs, soit des votants, mais ce résultat inclut la Crimée et les régions sous contrôle séparatiste, où la participation a été très faible.

Lors de cette élection, les électeurs donnent la victoire à Petro Porochenko, à la majorité dès le premier tour avec environ 54,7 % des voix, soit sur , alors que Ioulia Timochenko arrive deuxième avec 13 %.

Le 27 juin 2014, le nouveau président Petro Porochenko, signe un accord de libre échange avec l'Union européenne à Bruxelles

L'Ukraine a une économie diversifiée, mais encore tributaire des industries établies à l'époque soviétique. C'est un libre marché émergent, où la croissance fut à deux chiffres durant ces dernières années, jusqu'à la Révolution Orange. Ses ressources naturelles tournent beaucoup autour de l'agriculture (tournesol, noix, betteraves sucrières). Autre point fort de son agriculture, le pays était aussi septième au palmarès des producteurs mondiaux de céréales au milieu des années 2010, dominé par les Etats-Unis, grâce à une forte progression. Le secteur agricole est cependant en repli dans d'autres domaines: sur les six premières années de la décennie 2010, le pays n'a jamais regagné place au palmarès des huit plus grands producteurs mondiaux de sucre.

Les ressources minières (fer, acier, uranium, potasse, etc.) jouent un rôle également important. L'économie est caractérisée par une forte inflation et des rendements économiques encore un peu faibles.

Du point de vue commercial, son principal partenaire économique reste la Russie, même si l'Ukraine s'efforce de se tourner vers les pays de l'Union européenne géographiquement proches d'elle. Le pays joue un rôle important dans la distribution gazière européenne.

La monnaie nationale, la hryvnia, a été introduite en 1996 et a contribué à réduire l'hyperinflation qui régnait alors.

De 1989 à 1999, le PIB s'est effondré de 60 %, passant de de dollars internationaux à , puis est remonté à jusqu'à la crise de 2008, depuis laquelle le PIB a tendance à diminuer irrégulièrement (PIB en 2012 : de dollars internationaux).

En 2001, le gouvernement prit la décision d'accélérer le processus d'adhésion à l'Organisation mondiale du commerce (OMC), cependant les résultats ne furent pas aussi bons que prévu. L'objectif était d'entrer dans l'OMC en (le seul pays s'y opposant étant le Kirghizistan), entrée qui fut le .

La crise politique de 2006 aurait pu affecter l'économie ukrainienne en raison de la longueur de la désignation du Premier ministre. Les investisseurs ne furent pas vraiment effrayés et l'économie résista bien. La croissance du PIB en était de 9 % comparé à , la production industrielle a augmenté, le secteur bancaire s'est étendu, grâce à l'arrivée de banques européennes. En 2009, à la suite de la crise financière, le PIB ukrainien a chuté de 15 %, l'une des pires performances économiques enregistrées pendant cette période. Grâce aux exportations, la croissance a repris en 2010, mais les conditions extérieures sont susceptibles d'entraver les efforts pour la reprise économique en 2011.

L'Ukraine possède un réseau de transport assez développé, avec de routes majoritairement en mauvais état, de voies ferrées, et la longueur des voies fluviales ouvertes à la navigation est de .

Le transport ferroviaire international est très développé : le volume d'échanges est, en 2008, le deuxième d'Europe, s'élevant à (au sens géographique, avec les États de l'Union européenne + la Suisse + la Norvège + les États de la CEI), après la Russie.

Les principaux ports se trouvent sur la mer Noire et la mer d'Azov, ses seules côtes. En 2008, ils ont transporté de marchandises. Le plus important est celui d'Odessa, sur la mer Noire.

La plupart des compagnies aériennes ukrainiennes, agréées par l'IATA, secteur en plein développement, figurent dans la liste des compagnies ukrainiennes.

Les routes sont relativement mal entretenues, autour de 51,1 % des routes ne répondent pas aux normes minimales et 39,2 % ont besoin de reconstructions majeures. La vitesse moyenne sur les routes en Ukraine est 2 à 3 fois plus faible que dans les pays occidentaux.


Autoroutes privées en Ukraine, (2010) :

Kiev - Boryspil | Kharkiv - Dnipropetrovsk

Autoroutes détenues par l'État, (2009) :

M01 | M02 | M03 | M04 |
M05 | M06 | M07 | M08 | M09 | M10 | M11 | M12 | M13 | M14 | M15 | M16 | M17 | M18 | M19 | M20 | M21 | M22 | M23

"Voir : "

Total : 412 (en 2012)

Principaux aéroports : Kiev Boryspil, Dnipropetrovsk, Lviv, Donetsk, Odessa, Simferopol.

L’Ukraine occupait en 2012 la neuvième place en Europe par le nombre de visiteurs. Les principales villes visités sont Kiev, Lviv, Odessa, Kamianets-Podilskï et Yalta sur la mer Noire. Les Sept merveilles d'Ukraine ainsi que les « sept merveilles naturelles d’Ukraine » sont des endroits principalement fréquentés par les touristes étrangers. Depuis 2005, les citoyens de l'Union européenne, de l'EFTA, des États-Unis, du Canada, du Japon et de la Corée du Sud n'ont plus besoin de visa pour visiter l'Ukraine. Les Russes avaient ce droit avant 2005.

L'industrie touristique du pays a besoin d'investissement pour se moderniser, mais elle continue de contribuer stratégiquement à l'économie de l'Ukraine. En 2012, la part du tourisme dans le PIB s'est montée à , soit 2,2 % du PIB, tout en procurant directement (1,7 % des emplois totaux). En 2012, plus de de visiteurs étrangers ont visité l'Ukraine.

L'Ukraine possède de très nombreux sites touristiques dans tout le pays, un littoral sur la mer Noire avec des plages nombreuses et très populaires, des châteaux historiques, des parcs, des sites viticoles et un nombre important de musées répartis dans l'ensemble du pays, et notamment dans les grandes villes de Kiev, Odessa, Donetsk et Lviv. L'un des symboles les plus connus reste la cathédrale Sainte-Sophie et le monastère Saint-Michel avec ses toits dorés à Kiev, ainsi que le site antique de Chersonèse à Sebastopol (Crimée). Le massif des Carpates, dans l'Ouest, offre des stations de ski ainsi que des sentiers pédestres pour faire de la randonnée.

Le pays a été marqué par la catastrophe de Tchernobyl, même si les retombées ont essentiellement concerné la Biélorussie.

En tant que centre nodal énergétique pour l'Europe de l'Est, le risque d'accident lié à une infrastructure énergétique reste élevé.

Bien que les problèmes d'environnement en Ukraine provoquent une baisse de l'espérance de vie, l'environnement du pays est très bien préservé depuis des années grâce à un réseau de très nombreuses réserves naturelles et le programme de Kiev pour préserver l'environnement et la mer Noire.

L'Ukraine a connu dans le courant de l’été 2010 de nombreux feux de forêts, tout comme sa voisine russe.

L'université d'État Tarass-Chevtchenko, l'Institut Polytechnique de Kiev et Université nationale de commerce et d'économie de Kiev sont les principales universités ukrainiennes.

Jusqu’au début du , la langue écrite diffère significativement de la langue parlée. La littérature ukrainienne moderne nait au . Des écrivains de la nouvelle génération commencent alors à écrire en langue du peuple, cherchant la codification qui refléterait la prononciation.

Cette vague commence avec Ivan Kotliarevsky qui en 1798 publie le poème "Eneyida" (), qui est considéré comme la première œuvre en ukrainien moderne. Sa pièce de théâtre "Natalka Poltavka" est devenue un classique de la littérature ukrainienne. Elle est jouée encore aujourd’hui dans de nombreux théâtres de l’Ukraine.

Mais c’est l’œuvre de Taras Chevtchenko, fils de paysans serfs qui a eu la chance d’être libéré et de recevoir de l’éducation, qui marque véritablement la renaissance littéraire ukrainienne. Parmi d'autres écrivains ukrainiens : Ivan Franko et Lessia Oukraïnka.

D'autres écrivains ukrainiens ont par ailleurs influencé la littérature russophone dont le plus célèbre Nikolaï Gogol, est sujet de dispute entre Ukrainiens et Russes.

Sujet des interdictions et de la censure tsariste, la littérature de la langue ukrainienne vécut une brève période de renaissance dans les années 1920 que l’on appelle la « renaissance fusillée » car beaucoup de ses représentants furent exécutés lors des purges staliniennes et d’autres emprisonnés dans des camps du Goulag. 

Après la fin de l’URSS et l’indépendance de l’Ukraine, la littérature ukrainienne connaît une nouvelle renaissance, limitée par la crise économique et par la russification des grandes villes de l’Est de l’Ukraine.

Parmi les écrivains modernes les plus connus, on trouve Iouri Androukhovitch, Serhiy Jadan, Andreï Kourkov, Oksana Zaboujko, Ihor Pavliouk.



Une enquête nationale du centre Razoumkov menée en 2006 a conduit à produire des estimations de l'appartenance aux différentes confessions du territoire ukrainien fort différent des revendications des Églises les plus importantes, issues d'un schisme dans l'Orthodoxie nationale à la suite de l'indépendance du pays. En outre, plus de 60 % des Ukrainiens interrogés déclarent qu'ils n'ont pas de religion ou ne savent identifier la confession à laquelle ils pourraient s'identifier, particulièrement pour les citoyens de culture religieuse orthodoxe.

Les principales confessions du pays sont chrétiennes, orthodoxes et catholiques. Les différentes églises orthodoxes - essentiellement l'Église orthodoxe d'Ukraine du Patriarcat de Kiev (issue du schisme de 1992),
celle du même nom rattachée canoniquement au Patriarcat de Moscou et l'Église orthodoxe autocéphale ukrainienne (issue du schisme de 1921) - regroupent 26,8 % de la population. Les églises catholiques - essentiellement l'Église grecque-catholique ukrainienne et l'Église catholique en Ukraine - en réunissent 5,9 %.

D'autres confessions chrétiennes issues du protestantisme ou encore l'Église apostolique arménienne sont aussi représentées mais en très petit nombre, environ 1 %. L'islam, qui est principalement la religion des Tatars de Crimée, réunit moins d'1 % des croyants et le judaïsme moins de 0,5 %.

La confession la plus représentée chez les croyants, d'après cette étude, est l'orthodoxie du Patriarcat de Kiev (39,8 %), suivie de celle du Patriarcat de Moscou (29,4 %) et de l'Église grecque-catholique ukrainienne (14,1 %).

L’Ukraine dispose d'un patrimoine religieux considérable, parfois très ancien (jusqu'au ) et présentant, dans les Carpates, une architecture en bois. Un projet national de recensement et de préservation existe depuis 2005.

En 2017, dans un contexte de relations conflictuelles avec la Russie, le gouvernement ukrainien instaure de restrictions sévères sur les activités de l’Église orthodoxe d'Ukraine qui est rattachée au Patriarcat de Moscou. Cette décision suscite de vives critiques du patriarche Cyrille de Moscou selon lequel celle loi « menace les droits constitutionnels de millions de croyants ukrainiens ».

La cuisine ukrainienne est une part importante de la culture nationale. Des plats spéciaux sont préparés à Pâques ou à Noël. Les Ukrainiens utilisent diverses sortes de sauces, de poissons et de fromages. Le pain est un élément essentiel à tout repas. Le bortsch est une soupe traditionnelle servie en entrée. Elle est à base de betteraves et de légumes (chou, carottes, pommes de terre, oignons ou tomates) et de viande (poulet, porc ou bœuf).

Le varenyky (Вареники) est un plat ukrainien traditionnel populaire et très ancré dans la cuisine ukrainienne. Ressemblant à des raviolis, ils sont cependant plus volumineux et très similaires aux "pelmeni" russes, aux "pierogi" polonais voire aux "buuz" mongols. Leur farce est constituée généralement de pommes de terre, mais il y a de nombreuses déclinaisons : fromages, fraises, cerises, champignons, choux, voire plusieurs combinaisons entre elles.

Le gâteau de Kiev (en ukrainien : торт « Київський ») est à base de noisettes et de meringue.

On y boit de l'horilka, une sorte de vodka.

L'Ukraine a pour codes :





</doc>
<doc id="3084" url="https://fr.wikipedia.org/wiki?curid=3084" title="Uruguay">
Uruguay

L’Uruguay, en forme longue la République orientale de l'Uruguay, en espagnol et , est un pays d’Amérique du Sud situé au sud du Brésil et à l’est de l’Argentine, dont il est séparé par le fleuve Uruguay qui lui a donné son nom. L'Uruguay a une superficie totale de pour une population de .

La langue nationale est l’espagnol latino-américain. Le nord du pays est fortement influencé par le portugais. D'ailleurs à la frontière avec le Brésil, les locaux parlent le portunhol (ou portuñol) frontalier, mélange de portugais et de castillan. L'Uruguay a donné au portugais un statut égal à l'espagnol/castillan dans son système éducatif le long de la frontière nord avec le Brésil. Dans le reste du pays, il est enseigné comme une matière obligatoire à compter de la collège/ année primaire.

Sa capitale est Montevideo, qui est également la plus grande ville du pays avec près de . 

Le mode de vie y est européen teinté de cultures guarani et africaine et le niveau de vie est comparable à celui du Chili, si l'on prend en compte l'IDH.

L'Uruguay était considéré dans les années 1950 comme la « Suisse de l'Amérique » par les Européens. La monnaie nationale est le peso uruguayen.

Le nom Uruguay vient du guarani. Bien que sa signification ne soit pas très claire, Félix de Azara affirma que ce nom désigne un petit oiseau nommé "el urú" qui vit sur les rives du fleuve Uruguay (qui signifie lui-même-alors « rivière du pays de l'urú » ("río del país del urú"). Néanmoins, l'un des accompagnateurs d'Azara donna une autre version en disant que le mot Uruguay se divise en deux parties : "uruguá" signifiant « escargot », et le "ï" signifiant rivière, la traduction serait donc « rivière des escargots » ("río de los caracoles"). Enfin, le poète Juan Zorrilla de San Martín a interprété le mot d'une troisième façon, comme le « fleuve des oiseaux peints » ("río de los pájaros pintados").

L'histoire de ce pays commence réellement avec celle des Guaranis et des Charrúas. Ces derniers étaient le groupement le plus nombreux et le plus organisé. Jugés inassimilables, leur annihilation fut décidée peu après la déclaration d'indépendance du pays vers 1830.
En 1516, les Espagnols découvrent le territoire mais le délaissent au départ du fait de la faiblesse de ses ressources naturelles.
La menace causée par l'expansion des Portugais conduit les "Conquistadores" à édifier la ville fortifiée de Montevideo en 1726 et à coloniser le pays.

Le début du vit l'émergence de mouvements indépendantistes un peu partout en Amérique du Sud, y compris en Uruguay (désigné alors sous le nom de "Banda Oriental", c'est-à-dire « Région orientale »). Entre 1811 et 1817, le héros national de l'indépendance, José Gervasio Artigas, organisa les "Orientaux" dans le but d'obtenir l'indépendance des "Provincias Unidas del Rio de la Plata" (actuellement, une bonne partie de l'Argentine et l'Uruguay).

À la suite de trahisons et de multiples disputes entre les dirigeants locaux, les victoires initiales se transformèrent en défaites, et Artigas — suivi de dizaines de milliers de personnes — dut se réfugier en dehors de la Banda Oriental, puis s'exiler au Paraguay, d'où il ne revint jamais.

Le contrôle du territoire uruguayen fit l'objet d'un conflit entre les deux États naissants de l'Argentine et du Brésil : ce dernier finit par annexer la région en 1821 et la baptisa "« Provincia Cisplatina »". Mais le , le groupe nationaliste Trente-trois Orientaux (les "Treinta y Tres Orientales" en espagnol) conduit par Juan Antonio Lavalleja débarqua sur la plage de "La Agraciada" et commença la guerre d'indépendance contre le Brésil. Cette guerre se termina le par le Traité de Montevideo (1828). La première constitution de l'Uruguay fut signée le .

Entre 1839 et 1851, l'Uruguay connut une guerre civile nommée « Grande Guerre » durant laquelle les "Colorados", partisans de Fructuoso Rivera, et les "Blancos", partisans de Manuel Oribe, appuyé par l'Argentine s'affrontèrent, avec l'appui de volontaires étrangers dont la Ligue italienne commandée par Garibaldi. Les Colorados finirent par l'emporter. À la fin du siècle, le pays participa à la guerre de la Triple-Alliance contre le Paraguay.

De 1903 à 1920, l'Uruguay connut une période de prospérité sous la présidence de José Batlle y Ordóñez. Celui-ci nationalise les raffineries, les grandes industries et les banques, favorise le développement industriel, et proclame la séparation de l’Église et de l’État. L'ère Batlle donna son nom au « battlisme ». L'Uruguay fut ensuite touché par la crise de 1929, ce qui provoqua le coup d'État, en 1933, de Gabriel Terra, et ne sortit vraiment de cette crise qu'à partir de 1950. Le pays renoua alors partiellement avec une prospérité qui rappela l'ère Batlle, tandis qu'en 1952, un Conseil national du gouvernement (direction collégiale de l'exécutif) fut mis en place.

En 1958, le Parti national remporte les élections et soumet l’économie du pays aux requêtes des États-Unis et du Fonds monétaire international. La fin de la politique protectionniste est suivie d'une sévère crise économique qui ruine une grande partie des classes moyennes et réduit drastiquement le salaire réel des travailleurs. Le Conseil national du gouvernement fut renversé, et en décembre 1967, le vice-président Jorge Pacheco Areco accéda à la présidence, son prédécesseur étant mort quelques mois après avoir pris ses fonctions. L'inflation, qui dépasse les 100 % annuels, est ramenée par Pacheco à 20 %, qui établit un contrôle strict et pointilleux des salaires et des prix. Par ailleurs, pour faire face aux mouvements social et syndical, Pacheco interdit plusieurs partis de gauche et promulgue des mesures de sécurité, les "" à partir de juin 1968, l'Uruguay étant alors influencé par le mai 68 parisien. Sans cesse renouvelées avec l'accord du Parlement, ces mesures se transforment en état d'exception durable, avec l'application de la censure et des détentions sans inculpation, tandis qu'une guérilla urbaine, les Tupamaros, commence à se faire connaître avec la prise de Pando d'octobre 1969. 

La gauche met en place un Front large en vue des élections générales de 1971, afin de défier les deux partis traditionnels, "blancos" et "colorados". Présidé par le général Líber Seregni, démissionnaire du gouvernement Pacheco, celui-ci rassemble du Parti démocrate chrétien au Parti communiste, en passant par des dissidents "blancos" et "colorados", dont Zelmar Michelini. Pour réprimer la gauche, des communistes aux socialistes, le gouvernement Pacheco sponsorise des escadrons de la mort, lesquels tentent d'assassiner le général Seregni, tandis que la police commence à faire un large usage de la torture.

En novembre 1971, les élections sont remportées de justesse, dans un contexte de fraudes importantes, par le dauphin de Pacheco, Juan María Bordaberry. Celui-ci démantèle l'appareil de contrôle de l'économie mis en place par Pacheco, au risque de faire remonter l'inflation à un taux annuel de 100 %. La montée en puissance de l'armée se poursuit, tandis que l'« état de guerre interne » est voté après l'assassinat, par les Tupamaros, du sous-secrétaire d'État à l'Intérieur, Armando Costa y Lara, qui dirigeait les escadrons de la mort. En février 1973, après l'échec d'une tentative de reprise en main de l'armée par Bordaberry, celle-ci lui impose le Pacte de Boiso Lanza, qui établit un Conseil de sécurité nationale, l'armée partageant, de fait, le pouvoir avec lui. Le processus débouche finalement sur le coup d'Etat du 27 juin 1973, Bordaberry restant en place mais sous étroite surveillance de l'armée.

La dictature militaire dissout les partis politiques et suspend la Constitution, et emprisonne environ un habitant sur 450. Participant à l'opération Condor dès avant sa création officielle en 1975, les escadrons de la mort pourchassent les opposants, y compris hors des frontières (notamment en Argentine, où sont assassinés, en mai 1976, les parlementaires Michelini et Héctor Gutiérrez Ruiz, ainsi qu'un couple d'ex-Tupamaros et un communiste). L’économie est fortement libéralisée par le régime militaire. Les médias sont censurés ou interdits, le mouvement syndical est détruit et des tonnes de livres brulées après l'interdiction d'ouvrages de certains écrivains. Les personnes fichées comme opposées au régime sont exclues de la fonction publique et de l'enseignement.

L'échec de la dictature, consacré par le refus massif de la population lors du plébiscite de 1980 sur la réforme constitutionnelle visant à entériner la dictature, conduit à une transition démocratique qui n'aboutit qu'avec les élections de 1984 et la libération des prisonniers politiques en 1985. L'armée continua toutefois à surveiller étroitement la scène politique jusqu'aux années 2000, tandis que les gouvernements civils élus, "blanco" (Luis Alberto Lacalle, 1990-1995) et "colorado" (Julio María Sanguinetti, 1985-1990 et 1995-2000), mettaient en place une politique libérale, bientôt inspirée du « consensus de Washington ». L'une des principales réalisations de la période qui suivit fut le rapprochement de l'Uruguay avec ses voisins pour former le Mercosur. Ces échanges ont amené l'espoir pour le pays d'un retour à la prospérité dans un futur proche, déçus par la crise bancaire de 2002 provoquée par la crise argentine.

Sur le plan économique, le gouvernement libéral de Batlle (2000-2005) engage des négociations avec les États-Unis concernant la création de la « Zone de libre-échange des Amériques » (ZLEA). La période a marquée le point culminant d'un processus qui visait à une réorientation néolibérale de l’économie du pays : désindustrialisation, pression sur les salaires, essor du travail informel, etc. La situation sociale se détériore considérablement sous sa présidence et près du tiers de la population plonge dans la pauvreté entre 1999 et 2005.
Les élections de 2004 marquèrent, pour la première fois, la victoire de la gauche, le Front large remportant massivement celles-ci, conduisant son candidat présidentiel, le socialiste Tabaré Vázquez, à assumer la présidence (2005-2010). Formant un gouvernement avec une majorité de socialistes, mais incluant d'ex-Tupamaros, réunis au sein du Mouvement de participation populaire (MPP), dont José Mujica et Eduardo Bonomi, Vázquez parvient à faire baisser de façon importante la dette, tout en augmentant les salaires minimums et en faisant baisser le chômage et la pauvreté. L'Uruguay connaît alors des taux de croissance à 10 %, qui baissent subitement en 2009, sous l'influence de la crise mondiale. Les élections générales de 2009 sont à nouveau gagnées par le Front large, qui remporte une majorité absolue dans les deux chambres. Le MPP se confirme comme la force politique la plus importante, L'ex-Tupamara Lucía Topolansky étant la sénatrice élue avec le plus de voix. Son mari, José Mujica, est élu président.

La Constitution, qui s'inspire de celle des États-Unis, a été adoptée le . La "ley de lemas", modifiée par la réforme constitutionnelle de 1997, régit le système électoral, en mélangeant vote majoritaire pour la présidentielle, représentation proportionnelle pour l'élection des parlementaires, et « double vote simultané », ces différentes élections ayant lieu le même jour. Toutefois, depuis 1997, des élections primaires sont obligatoires au sein des partis, qui ne peuvent plus présenter plusieurs candidats à la présidence. 


Le 31 octobre 2004, Tabaré Vázquez, candidat de la coalition de gauche est élu président, le premier à n'être ni un "Blanco", ni un "Colorado" depuis plus de 150 ans. En novembre 2009, José Mujica, lui aussi du Front large, est élu président de l'Uruguay.

L'Uruguay est l'un des pays constituant le Mercosur, avec l'Argentine, le Brésil, le Paraguay et le Venezuela. Sur le plan intérieur, le gouvernement tente d'enrayer la montée du taux d'inflation, de réduire le chômage et le taux de pauvreté, de stabiliser la dette extérieure et de combattre la délinquance croissante liée au trafic de drogue.

L'Uruguay est un des trois pays sud-américains à avoir disposé d'un parti politique pour la population "afrodescendiente" (d'ascendance africaine).

Le 30 novembre 2014, Tabaré Vázquez remporte l'élection présidentielle avec 56,6 % des suffrages exprimés face à Luis Lacalle Pou et revient ainsi au pouvoir le .

L'Uruguay est le premier pays à reconnaître officiellement le génocide arménien, le 20 avril 1965. 

En décembre 2013, l'Uruguay devient le premier pays au monde à légaliser la production et la vente de cannabis. La même année, le mariage et l'adoption sont ouverts aux couples de même sexe. Il s'agit également du seul pays d'Amérique du Sud permettant l'avortement sur demande.

L'Uruguay est divisé en 19 départements (castillan : "departamentos", singulier - "departamento") dirigés par un intendant ("intendente" municipal) qui est élu pour 5 ans au suffrage universel direct. Les édiles de l'Assemblée départementale "(Junta Departamental)" ont un pouvoir législatif au niveau du département.

Les premiers départements sont formés dès 1816 et le plus jeune date de 1885, c'est celui de Flores.

L'Uruguay se situe dans la continuité géographique de la "Pampa" argentine, c’est-à-dire que le pays est principalement constitué de grandes plaines. On trouve aussi des montagnes de faible altitude mais très escarpées, comme la Cuchilla de Haedo et la Cuchilla Grande. Le point culminant du pays est le Cerro Catedral (Colline Cathédrale) avec ses .
Les précipitations sont en général constantes au cours de l'année, ce qui n'empêche pas des périodes de sécheresse ou, au contraire, des pluies très abondantes (selon les années).

Le relief, souvent considéré comme un prolongement de la Pampa, s'apparente plutôt à un vaste écotone, oscillant entre prairies tempérées et forêts subtropicales. Il est constitué par de vastes plaines ondulées et sillonnées par des collines de faible élévation appelées "cuchillas". Les plus importantes sont la "Cuchilla Grande" et la "Cuchilla de Haedo".

Avec ses étés chauds et ses hivers doux, le climat en Uruguay est subtropical (moyenne ) et les précipitations sont assez copieuses et plus ou moins homogènes pendant toute l'année. Le littoral connaît un climat maritime, avec une certaine amplitude, du fait du courant chaud du Brésil, qui augmente la température des côtes de l'Atlantique à partir de janvier jusqu'au début mai ; et du courant froid des îles Malouines refroidissant leurs eaux de juin à septembre. L'effet des deux courants détermine une température moyenne de la mer au niveau superficiel (Punta del Este) entre et selon l'époque de l'année. La neige est rare, et les hivers sont rarement rigoureux. En été, le thermomètre dépasse rarement les 30°, mais monte plutôt autour des 25°.

L'Uruguay est le seul pays sud-américain qui se trouve complètement dans la zone tempérée. L'absence d'importants systèmes orographiques contribue à ce que les variations de températures, de précipitations et d'autres paramètres soient faibles.


L'économie de l'Uruguay est très marquée par l'agriculture et notamment par l'élevage, puisque le pays se trouve dans le prolongement des prairies de la pampa argentine. Suivent l'industrie, principalement agroalimentaire, et le tourisme, qui se développe de plus en plus. Les groupes de papier Stora Enso et Arauco ont annoncé, en mai 2009, qu'ils achèteraient la firme espagnole Ensō, ce qui ferait de cette coentreprise la plus grande propriétaire terrienne d'Uruguay, avec hectares de terres, soit près de la moitié du total des propriétés terriennes de Stora Enso. 

Le pays est plongé dans une crise économique depuis les années 1960, et a beaucoup de difficulté à s'en relever. La principale tâche du nouveau gouvernement est donc logiquement d'effacer peu à peu la dette extérieure de 12,75 milliards de dollars et de rétablir l'équilibre de certains indicateurs économiques tels que la balance commerciale. 

La crise a été accentuée par l'effondrement de l'économie argentine dès 1999, l'Argentine étant son principal partenaire économique.

L'Uruguay accueille chaque année de nombreux touristes argentins, américains, et brésiliens.

En 1994, 6 % seulement des foyers uruguayens vivaient au-dessous du seuil de pauvreté national, et 94 % de la population uruguayenne appartenait à la classe moyenne ou aisée. Bien qu'il n'existe aucune source d'information fiable ou précise concernant le niveau de pauvreté actuel en Uruguay, celui-ci pourrait avoir doublé ou même triplé depuis la fin des années 1990, ramenant à 15 ou 30 % les foyers vivant dans la pauvreté, alors que le chômage touche actuellement 12 % des actifs. Cependant, l'Uruguay attire chaque année de nombreux Argentins fuyant la crise économique dans leur pays, ainsi que des Brésiliens en quête d'une vie meilleure. Le pays possède, entre autres, l'un des niveaux de vie les plus élevés d'Amérique latine ainsi que des niveaux d'éducation parmi les plus élevés du monde. Le PIB par habitant en Uruguay est de . (PPA, 2010).

Entre 1999 et 2002, une crise économique menace de provoquer l’effondrement du système financier et oblige l’État à intervenir pour sauver les banques. Plus de 40 % de la population vit dans la pauvreté.

Depuis l'arrivée au pouvoir du Front large en 2005, l'Uruguay connait une croissance économique ininterrompue depuis près de 15 ans, ce qui constitue un record pour ce pays. Ce développement économique serait notamment dû à la stabilité politique de l'Uruguay, au faible niveau de la corruption, et au développement des relations commerciales avec la Chine. La part des dépenses sociales dans le total des dépenses publiques est passée de 60,9 % à 75,5 % entre 2005 et 2015.

Le nombre de syndiqués a quadruplé depuis 2003, passant de à plus de en 2015 pour une population active de 1,5 million de personnes. D'après la Confédération syndicale international, l'Uruguay est devenu le pays le plus avancé d’Amérique en matière de respect « des droits fondamentaux du travail, en particulier la liberté syndicale, le droit à la négociation collective et le droit de grève ».

La population de l'Uruguay est essentiellement concentrée sur le littoral. Elle était estimée en 2004 à habitants, dont et . La population est essentiellement urbaine (90,7 %) et vit dans les 20 plus grandes villes du pays, principalement à Montevideo (1,4 million d'habitants). À cause d'un faible taux de natalité n'atteignant que , de l'espérance de vie élevée ("75,92 ans") et de l'émigration (0,32 émigrant pour ), la population du pays vieillit assez vite. Par ailleurs, la croissance de la population n'est que de 0,51 %.


Les descendants d'Européens représentent 88 % de la population. Environ 8 % des Uruguayens sont métis, d'une ascendance à la fois européenne et amérindienne. Les personnes d'origine africaine représentent 4 % des habitants et les Amérindiens représentent moins de 1 % de la population nationale (principalement des Guaranis car il n'y a plus aucun descendant des Charrúas, habitants originels de l'Uruguay exterminés au ). Ces deux dernières décennies, environ ont émigré vers le Brésil, l'Argentine, l'Espagne ou encore les États-Unis d'Amérique.


En ce qui concerne les langues, l'espagnol est quasiment l'unique langue parlée. Le portugais est beaucoup parlé en seconde langue, surtout le long de la frontière avec le Brésil. L'anglais est enseigné à l'université, et est la langue utilisée pour le tourisme. L'italien est parlé par de nombreuses personnes, en majorité des descendants d'Italiens.

La vie culturelle de l'Uruguay s'est épanouie dans plusieurs grands domaines, dont la peinture (avec Juan Manuel Blanes et Pedro Figari), la sculpture (avec José Belloni) et la musique avec le candombé et le tango (avec Jaime Roos et Jorge Drexler). Les écrivains sont José Enrique Rodó, Horacio Quiroga, Juan Carlos Onetti, Mario Benedetti, Eduardo Galeano, Jorge Majfud et Ricardo Paseyro.

Le football occupe une place très importante dans la vie sportive, surtout depuis les coupes du monde de 1930 (première coupe du monde, elle a eu lieu en Uruguay) et 1950, toutes deux remportées par l'équipe nationale des « Célestes ». Les sports équestres hérités des "gauchos" (« bouviers ») jouent aussi un rôle essentiel.

L'Uruguay a pour codes :






</doc>
<doc id="3085" url="https://fr.wikipedia.org/wiki?curid=3085" title="Undernet">
Undernet

Depuis 2011, Undernet est le cinquième plus gros réseau IRC surveillé publiquement. Une trentaine de serveurs accueillent en permanence utilisateurs.

Il fut mis en place en octobre 1992, au départ expérimentalement pour tester une version modifiée du logiciel de serveur IRC irc2.7, alors créé dans une tentative de consommer moins de bande passante et d'être moins chaotique, car des coupures réseau et des prises de contrôle commençaient à infester EFnet.

UnderNet fut créé à une période où de nombreux petits réseaux IRC se créaient et disparaissaient subitement ; cependant, il géra sa croissance pour devenir un des plus grands réseaux pionniers malgré des querelles internes et des échecs. Il est remarquable par la première utilisation du marquage horaire dans le protocole du serveur IRC comme moyen de lutte contre l'abus.

UnderNet réunit approximativement 25 serveurs connectant plus de 35 pays et servant généralement plus de 20 000 personnes. Le panel des utilisateurs couvre tous les âges, nationalités et intérêts fournissant une grande variété de canaux, ainsi que des volontaires pour aider les nouveaux utilisateurs.

On peut connecter son client IRC à UnderNet via un alias continent, tel que codice_1 pour les Américains ou codice_2 pour les Européens.

Afin de pouvoir enregistrer un canal auprès des services d’Undernet, votre nom d’utilisateur de CService ("username") doit être enregistré depuis plus de dix jours et vous devez bénéficier l’appui de dix utilisateurs étant aussi enregistrés.

Parmi les canaux francophones notoires, on peut citer #france, #quebec, #montreal, #maroc, #sexe

X ou W était les superbots qui régissaient tout Undernet. @Copine @MaryBlue aidaient à maintenir certains canaux en paix.

 4. Référence à Undernet dans Undertale




</doc>
<doc id="3087" url="https://fr.wikipedia.org/wiki?curid=3087" title="Universal Serial Bus">
Universal Serial Bus

Le terme anglais ou USB (en français "bus universel en série") est une norme relative à un bus informatique série qui sert à connecter des périphériques informatiques à un ordinateur ou à tout type d'appareil prévu à cet effet (tablette, smartphone). Le bus USB permet de connecter des périphériques "à chaud" (quand l'ordinateur est en marche) et en bénéficiant du "Plug and Play" qui reconnaît automatiquement le périphérique. Il peut alimenter les périphériques peu gourmands en énergie (clé USB, disques SSD). Apparu en 1996, ce connecteur s'est généralisé dans les années 2000 pour connecter souris, clavier d'ordinateur, imprimantes, clés USB et autres périphériques sur les ordinateurs personnels.

Les performances de l'USB, notamment concernant les débits, se sont grandement améliorées au fil des versions : de pour la version 1.0 à théoriques pour la version 3.1.

L’USB a été conçu au milieu des années 1990 afin de remplacer les nombreux ports externes d’ordinateurs, lents, encombrants (port parallèle, port série, port SCSI...), spécialisés (ports clavier PC DIN, puis PS/2 mini-DIN, port souris) et incompatibles les uns avec les autres. Des versions successives de la norme ont été développées au fur et à mesure des avancées technologiques, chacune étant vouée à remplacer les précédentes car plus performante. Une clé de cette généralisation tient au fait que de simples puces de faible coût gèrent en temps réel toute la logique de sérialisation et de partage — de complexité, croissante au fil des versions — de l'USB.

En 1996, la première version de la norme, l'USB 1.0, est spécifiée par sept partenaires industriels (Compaq, DEC, IBM, Intel, Microsoft, NEC et Northern Telecom). Mais elle reste théorique et n'a pas vraiment été appliquée par manque de composants.

Il faut attendre la seconde version de la norme 1998, intitulée USB 1.1, pour que l'USB commence à être effectivement utilisé. Ce que nous appelons couramment USB 1 est donc en réalité de l'USB 1.1.

L'USB 1.1 apporte des corrections à la norme 1.0 et définit également deux vitesses de communication :

En août 1998, avec la sortie de l'iMac G3, Apple est le premier constructeur à proposer un appareil disposant uniquement de ports USB en remplacement des ports d'ancienne génération, ce qui a fait décoller le marché des périphériques USB.

En avril 2000 est publiée la norme USB 2.0, qui optimise l'utilisation de la bande passante, qui débite théoriquement à , baptisé "Haute vitesse (en anglais )". Il est utilisé par les périphériques rapides : disques durs, graveurs Au moment de sa sortie, la plupart des périphériques ont d'ailleurs une vitesse inférieure à celle permise par l'USB 2.0.

En 2005, le , une version sans-fil de l'USB, est spécifiée par le "". Elle promet à une distance de et à .

En 2007, l'extension (OTG), ajoutée à la norme USB 2.0 permet d'effectuer des échanges de données point à point entre deux périphériques sans avoir à passer par un hôte (généralement un ordinateur personnel). La norme OTG s'impose désormais comme un standard.

En 2008, l'USB 3.0 introduit le mode "vitesse supérieure" ('), qui débite théoriquement à '. Mais ce nouveau mode utilisant un codage des données de type 8b/10b, la vitesse de transfert réelle est de seulement . L'USB 3 délivre une puissance électrique maximum de soit (à ).

Les nouveaux périphériques disposent de connexions à au lieu de 4, mais la compatibilité ascendante des prises et câbles avec les versions précédentes est assurée. En revanche, la compatibilité descendante est impossible, les câbles USB 3.0 de type B n'étant pas compatibles avec les prises USB 1.1/2.0, . 

Début 2010, l'USB 3.0 est introduit dans des produits grand public. Les prises femelles correspondantes sont signalées par une couleur bleue. Apparition aussi des prises femelles USB "rouges", signalant une puissance électrique disponible supérieure, et appropriée au chargement rapide de petits appareils y compris (à condition de le paramétrer dans le BIOS ou l'UEFI) lorsque l'ordinateur est éteint.

D'autres couleurs, non normalisées et donc propres à chaque constructeur (bleu ciel, gris...), signalent quels ports USB sont reliés à des adaptateurs distincts, ce qui est important pour les questions de performance (débits parallèles) ou de fiabilité. Le jaune est souvent utilisé pour indiquer quels ports sont à alimentation rémanente quand la machine est hors tension.

Un standard 3.1, qui débite théoriquement à est annoncé en août 2013 ; ses spécifications techniques sont finalement publiées par le consortium "USB Implementers Forum" en août 2014.

L'USB 3.1 permet des débits doubles de ceux de l'USB 3.0, soit . Le nouveau standard (câbles, interface) est rétro compatible avec l'USB 3.0 et l'USB 2.0. Toutefois, il marque la sortie d'une nouvelle connectique, celle-ci est plus fine et n'impose pas de sens de branchement (valable pour les connecteurs 3.1 de type C uniquement). Pour tout de même permettre la connexion vers des connecteurs USB 2.0 et 3.0, le standard permet d'avoir des adaptateurs passifs (à l'inverse des adaptateurs Lightning, le connecteur réversible qu'Apple a lancé avec l'iPhone 5 en 2012), pour garder une taille réduite et un coût de fabrication mesuré. Cette nouvelle connectique se nomme "Type-C".

Le , Apple présente le MacBook, le premier ordinateur équipé d'un seul port USB 3.1 de type C, mais ne bénéficiant que du débit de l'USB 3.0 () au lieu de celui de l'USB 3.1 (), bien que le connecteur utilisé soit un port USB 3.1 Gen 1.

Les protocoles utilisés pour la communication sur le bus USB sont :

Lorsque l’on parle d’un équipement USB, il est nécessaire de préciser la version de la norme (1.1, 2.0 ou 3.0) mais également la vitesse ("). Une clef USB spécifiée en n’est pas forcément « haute vitesse » si cela n’est pas précisé par un logo « ».

Jusqu'à la version 3.1, le bus USB était plus lent que certaines interfaces internes comme le PCI, l'AGP ou le SATA/e-Sata. Ainsi, si l' () était dix fois plus lent que le SATA III (), sa version 3.0 fait presque jeu égal () et la la surclasse ().

Ces débits ne sont atteints cependant en copie de fichiers qu'avec un utilitaire ou un système d'exploitation recourant au double buffering. "Dans le cas contraire, les disques ne seront sollicités qu'à tour de rôle au lieu de débiter en même temps", divisant donc par deux le débit théorique possible. C'est par exemple le cas sous Windows 7 avec la fonction de copie de base du système.

La restriction de débit s'observera aussi avec des prises USB reliées à un contrôleur unique par un hub, que celui-ci soit externe ou interne.

La division de débit ne pose pas de problème lorsqu'il s'agit de périphériques ne fonctionnant pas simultanément (le hub est en ce cas utilisé simplement pour éviter des branchements et débranchements manuels fréquents, et donc aussi une usure prématurée des prises USB), l'allocation de bande dans la norme USB étant dynamique; en d'autre termes, quand un périphérique fonctionne seul il dispose de presque toute la bande passante.

Les hubs possédant une alimentation externe divisent uniquement le débit. Les hubs alimentés par le câble USB répartissent l'intensité disponible pour les péripheriques en ayant besoin mais pas sur les SSD, ni les périphériques USB munis de leur propre alimentation.

Le bus USB ne permet pas de relier entre eux deux périphériques ou deux hôtes : le seul schéma de connexion autorisé est un périphérique sur un hôte. Pour éviter des branchements incorrects, la norme spécifie deux types de connecteurs : le type A, destiné à être situé sur l'hôte, et le type B, destiné à être situé sur le périphérique.

Un hub USB peut comporter à la fois un connecteur B, qui permet de le relier à l'hôte, et des connecteurs A, qui permettent d'y relier des périphériques. Les appareils (hôte et périphériques) sont équipés de connecteurs femelles. Les câbles de connexion ont "toujours" une extrémité de type A mâle, et une extrémité de type B mâle, ce qui garantit le respect de la topologie du bus. Il peut aussi exister des câbles de prolongation équipés de connecteurs de même type mais de genres différents.

Chaque type (A ou B) existait dans les deux genres (mâle ou femelle), ce qui fait qu'il existait au départ quatre connecteurs. En octobre 2000, devant le développement des appareils compacts (téléphones portables, appareils photo numériques), une mise à jour de la norme USB 2 introduit une version miniature du connecteur B : le "mini-B". Elle est fonctionnellement équivalente au connecteur B, mais de dimensions nettement plus réduites.

En décembre 2001, l'USB 2 introduit le connecteur mini-AB, utilisé dans le cadre de l'extension « ». Il permet aux appareils compatibles de jouer indifféremment le rôle d'hôte ou celui de périphérique, contrairement à l'USB classique où l'hôte se branche obligatoirement sur un connecteur de type A et le périphérique sur un connecteur de type B.

La taille des appareils mobiles s'étant encore réduite, les connecteurs mini-B sont devenus à leur tour trop gros. En janvier 2007, le nouveau connecteur micro-B est annoncé. Il est non seulement plus fin que le mini-B, mais également prévu pour supporter un grand nombre de cycles de connexion/déconnexion (jusqu'à ), ce qui le rend particulièrement bien adapté aux appareils mobiles souvent branchés/débranchés (tablettes tactiles, smartphones).

Pour les mêmes raisons, en avril 2007, une nouvelle norme micro-AB vient remplacer la norme mini-AB, qui est officiellement déconseillée le mois suivant.

Avec l'arrivée de l'USB 3, un connecteur micro-B USB 3 est apparu.

Un nouveau connecteur est introduit dans la norme en août 2014 : le type C, destiné à remplacer tous les connecteurs précédents. Il a la particularité d'être réversible, c'est-à-dire qu'il n'a plus de sens haut/bas. Outre l'aspect pratique, il est compatible à la fois avec le standard USB 3.1 (qui porte le débit maximal théorique à 10 Gbit/s) et l'USB Power Delivery. La technologie DisplayPort lui permet également de transmettre des signaux audio et vidéo.

USB a supplanté divers bus et interfaces qui équipaient auparavant les ordinateurs : port série RS-232, port parallèle, port PS/2, port joystick (ou port MIDI), port SCSI, et même des bus internes comme PCI pour la connexion de certains dispositifs (par exemple cartes son ou cartes de réception TV).

La gamme des périphériques utilisant le bus USB est extrêmement vaste :

Le bus USB est également utilisé en interne dans certains ordinateurs pour connecter des périphériques tels que webcams, récepteurs infrarouges (c'est le cas par exemple sur les MacBook Pro) ou lecteurs de cartes mémoire.

Le bus USB peut alimenter en énergie les périphériques, dans une certaine limite de courant consommé ( pour une application haute puissance, pour une application normale). Cela permet au passage la recharge d'appareils portables, pour lesquels on voit apparaître des adaptateurs secteur disposant d'une connectique USB limitée à l'alimentation électrique.

La connectique USB devient ainsi une norme de fait pour alimenter des appareils de faible puissance (au début sous continus soit ), au-delà des périphériques informatiques "stricto sensu". Plusieurs gadgets alimentés par port USB qui ne sont pas des périphériques informatiques sont apparus sur le marché : lampes d'appoint, petits ventilateurs

Cependant, le courant délivré par l'USB est resté longtemps trop faible pour certains périphériques, par exemple des disques durs externes de , ou même quelques uns de pouvant demander jusqu'à . Une solution possible consistait à compléter l'alimentation par un branchement sur un second port USB (parfois aussi une dérivation sur un port clavier PS/2), mais cette pratique était contraignante, le périphérique mobilisant alors deux ports et deux câbles. 
Ce problème pourrait être résolu avec la nouvelle norme USB. En effet, un câble standard avec prises de type C (norme USB 3.1) autorise une puissance électrique de . Des câbles ayant des fils avec une section suffisante peuvent faire faire transiter jusqu'à avec . On tendrait alors vers l'utilisation d'un câble unique pour les périphériques qui assure à la fois l'alimentation et le transfert des données. Par exemple, on peut connecter un écran à un hub USB intégré avec un seul câble USB sans se soucier du sens du câble ou de la prise. L'épaisseur du conducteur nécessaire pour acheminer l'intensité de correspondante risque de poser des problèmes de coût et de raideur du câble, au risque d'endommager la prise comme ce fut déjà le cas au temps du SCSI.

L'USB est aussi devenu un moyen d'alimenter un ordinateur et pas seulement ses périphériques. En 2015, Google sort un Chromebook Pixel incluant une prise USB de type C qui permet de le recharger. En 2017, c'est le tour du " GPD Pocket", PC de poche à base Intel fonctionnant sous Windows 10 et Linux.

Ce système est en concurrence avec le chargement sans fil.

L'Union européenne annonce son intention d'imposer les chargeurs aux normes USB d'ici 2017, afin d'éviter de déchets électroniques par an dans ses . Les tensions et les connectiques correspondantes deviennent donc le standard de fait de la très basse tension.

Depuis 2015, certains abribus parisiens, mis en place par la société Decaux, sont équipés de prises USB permettant un rechargement ponctuel de téléphone mobile.

Bénéficiant du volume de l'écosystème créé par ce nouveau standard interconstructeurs de basse tension et de connectique, de nouveaux produits apparaissent, comme les "batteries externes" normalisées de 4 à , qui présentent l'avantage d'être utilisables avec les téléphones comme les tablettes, y compris de constructeurs différents, et de rester utilisables si on vient à changer l'une, l'autre, ou les deux.

Les constructeurs d'objets connectés utilisent cette normalisation pour fournir les objets en question sans chargeurs, dès lors beaucoup moins utiles. Ils combinent ainsi baisse des coûts de plusieurs euros (ou dollars) et meilleure écologie, deux facteurs favorables à leur acceptation par le marché. Montres connectées et enceintes bluetooth peuvent alors rester aussi éloignées de l'ordinateur principal qu'on le voudra, celui-ci pouvant même parfois disparaître du foyer sans inconvénient.

Depuis la fin de l'année 2016 sont en vente des lampes de bureau à LED munies en standard d'une prise de chargement pour appareil USB externe : leur transformateur fournissant déjà la tension appropriée, ajouter cette prise coûte peu à la fabrication, et celle-ci ne débite que lorsqu'elle charge ou alimente un appareil. De même, des alimentations comme celles de la Microsoft Surface Pro 4 sont munies d'un tel port additionnel pour la même raison. Il est ainsi possible de recharger un téléphone mobile ou d'alimenter une enceinte Bluetooth.

L'usage mixte de l'USB en données et en alimentation alors que tous les appareils n'ont pas besoin des deux usages entraîne la création sur le marché de câbles dégradés pour diminuer les coûts et donc les prix :



Aucun code de couleurs n'ayant été établi pour les distinguer des câbles "complets", leur confusion est courante et source de contrariétés - surtout en déplacement - que l'économie de quelques dizaines de centimes d'euro ne justifie pas toujours. Les câbles complets "alimentation et données" (parfois "power and data") sont en 2017 spécifiés comme tels.

L’ est une connexion à haute vitesse qui permet de connecter des périphériques externes à un ordinateur (hôte dans la terminologie USB). Il permet le branchement simultané de par contrôleur (hôte). Le bus autorise les branchements et débranchements à chaud (« », sans avoir besoin de redémarrer l’ordinateur) et fournit l’alimentation électrique des périphériques sous , dans la limite de , soit . Le consortium USB prévoit des sections de type (diamètre de ) à (soit ). Pour des câbles utilisant des conducteurs de type 24 AWG pour l'alimentation, on a une résistance d'environ pour qui peut fournir au moins 2 ampères. 

D'un point de vue logiciel, le bus possède une topologie arborescente (dite également en étoile) : les feuilles de cet arbre sont les périphériques ; les nœuds internes sont des "hubs" qui permettent de greffer des sous-arborescences dans l'arborescence principale. On trouve dans le commerce ces "hubs" sous forme de petits boîtiers alimentés soit sur le bus, soit sur le secteur, et qui s'utilisent comme des multiprises. Certains périphériques intègrent également un "hub" (moniteurs, claviers…). Cependant, tout bus USB possède au moins un "hub" situé sur le contrôleur : le "hub racine", qui peut gérer les prises USB de l'ordinateur. Le nombre de "hubs" connectés en cascade est limité : "hub racine" compris, il ne doit pas exister plus de dans l'arborescence.

À plus bas niveau, il s'agit d'un anneau à jeton (en anglais ) : chaque nœud dispose successivement de l’accès au bus afin d'éviter la collision de paquets comme sur un réseau Ethernet, mais le nombre maximal de nœuds est prédéfini et l’interrogation de chacun des nœuds génère une perte de temps inutile.

La bande passante est partagée temporellement entre tous les périphériques connectés. Le temps est subdivisé en trames () ou microtrames () ce qui permet de faire du multiplexage. Permettant ainsi le transfert de données différentes (souris, clavier, son) de manière simultanée.

La communication entre l’hôte (l’ordinateur) et les périphériques se fait selon un protocole basé sur l'interrogation successive de chaque périphérique par l'ordinateur ("polling"). Lorsque l’hôte désire communiquer avec un périphérique, il émet pour ce faire un jeton (paquet de données contenant l’adresse du périphérique codée sur sept bits).
Si le périphérique reconnait sa propre adresse dans le jeton, il envoie un paquet de données (de 8 à 255 octets) en réponse. Les données ainsi échangées sont codées selon le codage NRZI. Puisque l’adresse est codée sur 7 bits, 128 périphériques (2) peuvent être connectés simultanément à un port de ce type. Il convient en réalité de ramener ce chiffre à 127 car l’adresse 0 est une adresse réservée. 

USB définit quatre types de transferts :

Il est possible de structurer la communication entre un hôte et un périphérique en plusieurs canaux logiques (en anglais ' et ') pour simplifier la commande du périphérique du port USB.


Les ports USB supportent la connexion à chaud et la reconnaissance automatique des dispositifs ("). Ainsi, les périphériques peuvent être branchés sans éteindre l’ordinateur.

Lors de la connexion du périphérique à l’hôte, ce dernier détecte l’ajout du nouvel élément grâce au changement de la tension entre les fils D+ et D-. À ce moment, l’ordinateur envoie un signal d’initialisation au périphérique pendant , puis lui fournit du courant grâce aux fils GND et VBUS (jusqu’à ); le périphérique étant alors alimenté électriquement peut utiliser temporairement l’adresse par défaut (l’adresse 0); l’étape suivante consiste à lui fournir son adresse définitive et à obtenir sa description : c’est la procédure d’énumération; après avoir reçu son adresse, le périphérique transmet à l'hôte une liste de caractéristiques qui permettent à ce dernier de l'identifier (type, constructeur, nom, version). L’hôte, disposant de toutes les caractéristiques nécessaires est alors en mesure de charger le pilote approprié.

Les périphériques sont regroupés en types ou "classes" dans la terminologie USB. Tous les dispositifs d'une classe donnée reconnaissent le même protocole normalisé. Il existe par exemple une classe pour les périphériques de stockage de masse ("", MSC), implémentée par la quasi-totalité des clés USB, disques durs externes, appareils photo et par certains baladeurs. La plupart des systèmes d’exploitation possèdent des pilotes génériques, pour chaque type de périphérique. Ces pilotes génériques donnent accès aux fonctions de base, mais des fonctions avancées peuvent manquer.

L’architecture USB a pour caractéristique de fournir aussi l’alimentation électrique aux périphériques. Il utilise pour cela un câble composé de quatre fils pour les USB 1 et 2 (la masse GND, l’alimentation VBUS et deux fils de données appelés D- et D+) et de six fils pour l'USB 3 (séparation des données IN/OUT). Les fils D+ et D- forment une paire torsadée et utilisent le principe de la transmission différentielle afin de garantir une certaine immunité aux bruits parasites de l’environnement physique du périphérique ou de son câble.

Par ailleurs, il est remarqué que dans toutes les fiches mâles, les broches de données sont plus courtes que celles de l'alimentation, du fait que le périphérique doit être alimenté avant d'émettre ou reçevoir des données.

Cela donne donc au branchement: Alimentation puis Données 

Au débranchement: Données puis Alimentation

En USB 1.1 le courant disponible est de pendant les transferts mais peut atteindre sans transfert simultané.

Certain chargeurs externes peuvent fournir jusqu'à "ce qui ne veut pas dire que le périphérique branché dessus pourra utiliser cette possibilité" entre autres si la batterie à charger n'est pas complétement vide.

«  » permet de délivrer jusqu'à de puissance sur une tension maximale de au travers du câble USB, tout en maintenant la communication. L'alimentation électrique est désormais bidirectionnelle, elle peut se faire dans les deux sens. Un ordinateur portable peut recharger une batterie USB quand il est lui-même sur le secteur. Et quand il n'est pas sur le secteur, cette même batterie peut le recharger. Ou encore on peut charger un ordinateur et que celui-ci charge par la suite un téléphone avec la même prise et le même câble. De quoi exploiter pleinement les batteries de tous ses appareils. 

Lors de la connexion, les deux périphériques négocient la puissance à délivrer par l'intermédiaire de contrôleurs spécifiques et chaque port USB pourra ainsi indiquer les tensions et intensités qu'il supporte.

La norme prévoit cinq profils :

Le brochage des connecteurs de type A et B est le suivant :
Le brochage de la prise de type C, vue de face, est le suivant :

La broche CC indique l'orientation du connecteur, la broche VConn pour l'alimentation.

Le brochage du connecteur de réception de type C, vue de face, est le suivant :

L'alimentation passe par les broches VBus et GND. Les signaux de configurations par CC1 et CC2 et il y a 2 broches SBU (SideBand Use).




</doc>
<doc id="3088" url="https://fr.wikipedia.org/wiki?curid=3088" title="Utah">
Utah

L'Utah (en anglais ou et écrit en navajo  et prononcé ) est un État de l'Ouest des États-Unis. La capitale est Salt Lake City, centre d'une zone urbaine où vivent 88 % des de l'État. L'Utah est connu pour sa grande diversité géologique, avec des montagnes enneigées, des vallées aux fortes rivières et des déserts arides aux formes géologiques spectaculaires. Un des emblèmes de cet État est ainsi une arche naturelle (Delicate Arch, située dans le parc national des Arches).

L'État est aussi connu pour sa forte communauté mormone, qui en fait un des États américains les plus homogènes religieusement, avec environ 62 % des habitants se déclarant mormons. L'Église de Jésus-Christ des saints des derniers jours influence grandement la culture de l'État et la vie quotidienne. Les pionniers mormons furent parmi les premiers colons à s'installer dans la région en 1847.

L'économie de l'Utah repose sur les technologies de l'information, le transport et les mines. C'est également une destination touristique de premier ordre.

Plusieurs interprétations existent quant au nom de cet État. Il serait dérivé de la langue amérindienne ute et signifie « peuple des montagnes », ou proviendrait du mot apache "yuttahih", qui signifie « celui qui est plus haut ».

Les Amérindiens sont présents dans la région depuis la préhistoire. Ils ont laissé des pétroglyphes et des pictogrammes, témoins de leur culture passée. Francisco Vásquez de Coronado a sans doute traversé le Sud de l’Utah actuel en 1540, alors qu’il cherchait les légendaires Cités d'or. Un groupe d’Espagnols, conduit par deux prêtres, quitte Santa Fe en 1776 à la recherche d'une route menant à la côte californienne. L’expédition parvient au nord du lac Utah et rencontre les Amérindiens. Puis des trappeurs explorent la région au début du . La ville de Provo est nommée ainsi en l’honneur d’Étienne Provost qui voyagea dans la région en 1825.

Les premiers colons mormons arrivèrent à Salt Lake City le , alors que l’Utah était encore un territoire mexicain. Après le Traité de Guadeloupe Hidalgo, l’Utah est cédé aux États-Unis en 1848 (voir : "Cession mexicaine"). Mais il est l'un des derniers États continentaux à entrer dans l’Union en tant qu’État fédéré (1896). Un des obstacles principaux à cette accession a été l’attachement des mormons du à la polygamie.

L'Utah est l'un des "Four Corners states". Il est bordé par l'Idaho au Nord, le Wyoming au nord et à l'est, le Colorado à l'est, le Nouveau-Mexique en un seul point au sud-est (au "Four Corners Monument"), par l'Arizona au sud et le Nevada à l'ouest. Il couvre une superficie de . L'Utah est l'un des trois seuls États américains (avec le Colorado et le Wyoming) à n'avoir que des lignes de latitude et de longitudes comme frontières.

L'Utah est globalement rocheux avec trois régions géologiques distinctes : les montagnes Rocheuses, le Grand Bassin et le plateau du Colorado. L'Utah est connu pour sa diversité naturelle et abrite des formations géologiques très diverses, des déserts arides avec des dunes de sables aux prospères forêts de pin dans des vallées de montagne.

L’Utah est caractérisé par une grande diversité géologique : au centre se trouve la chaîne Wasatch, qui s'élève à environ au-dessus du niveau de la mer. Certaines parties de ces montagnes reçoivent plus de de neige par an en faisant un lieu renommé pour la pratique du ski avec sa poudreuse et sa lumière. Au nord-est les monts Uinta (d’orientation est-ouest) comprennent le plus haut sommet de l’État (Pic Kings, ). À l’ouest des montagnes Wasatch, se trouve le Grand Lac Salé qui appartient au Grand Bassin ("Great Basin"). Les paysages du sud de l’Utah se caractérisent par des formes érodées par le Colorado et ses affluents. L’ouest est principalement aride ; le nord-est est plutôt montagneux avec de grandes forêts.

Les principales curiosités naturelles sont des lacs :

Malgré l'urbanisation, l'Utah demeure l'une des régions les plus surprenantes, les plus belles et les plus folles d'Amérique, l'endroit où la légende d'un certain Far West est encore vivante.

Quand la nature bâtit ses cathédrales : à plus de d'altitude, des milliers d'aiguilles et de flèches de calcaire orange, roses, blanches se dressent vers le ciel au Parc national de Bryce Canyon, l'un des plus célèbres sites de l'Utah. La nature a mis soixante millions d'années à les façonner. Et son œuvre continue encore aujourd'hui.

Près de Kanab, des siècles d'érosion ont modelé ces blocs de grès mettant à nu le cœur et les veines de la roche. Dans ces paysages qui ont maintes fois servi de cadre à des westerns, on prélève certaines de ces pierres polies pour les transformer en objets d'art.

L'Utah est subdivisé en 29 comtés.

L'Utah a pour codes ;

L'État de l'Utah est divisé en 29 comtés.

Le Bureau de la gestion et du budget a défini cinq aires métropolitaines et cinq aires micropolitaines dans l'État de l'Utah.

En 2010, 94,9 % des Utahains résidaient dans une zone à caractère urbain, dont 89,1 % dans une aire métropolitaine et 5,8 % dans une aire micropolitaine. L'aire métropolitaine de Salt Lake City regroupait à elle seule 39,4 % de la population de l'État.

Le Bureau de la gestion et du budget a également défini une aire métropolitaine combinée dans l'État de l'Utah.
En 2010, l'aire métropolitaine combinée de Salt Lake City-Provo-Orem regroupait 82,2 % de la population de l'État.

L'État de l'Utah compte 245 municipalités, dont 25 de plus de .

Le Bureau du recensement des États-Unis estime la population de l'Utah à au juillet 2013, soit une hausse de 5,0 % depuis le recensement des États-Unis de 2010 qui tablait la population à . Depuis 2010, l'État connaît la croissance démographique la plus soutenue des États-Unis après le Dakota du Nord (7,6 %) et le Texas (5,2 %).

Selon des projections démographiques publiées par l’AARP, l'Utah devrait atteindre une population de en 2060 si les tendances démographiques actuelles se poursuivent, soit une hausse de 41,3 % par rapport à 2010.

Avec en 2010, l'Utah était le 34 État le plus peuplé des États-Unis. Sa population comptait pour 0,90 % de la population du pays. Le centre démographique de l'État était localisé dans le nord du comté d'Utah dans la ville de Saratoga Springs.

Avec 12,98 /km en 2010, l'Utah était le État le moins dense des États-Unis.

Le taux d'urbains était de 90,6 % et celui de ruraux de 9,4 %. L'État comptait le fort taux d'urbains du pays.

En 2010, le taux de natalité s'élevait à ( en 2012) et le taux de mortalité à ( en 2012). L'indice de fécondité était de 2,45 enfants par femme (2,37 en 2012). Le taux de mortalité infantile s'élevait à ( en 2012). La population était composée de 31,51 % de personnes de moins de 18 ans, 11,51 % de personnes entre 18 et 24 ans, 28,15 % de personnes entre 25 et 44 ans, 19,80 % de personnes entre 45 et 64 ans et 9,03 % de personnes de 65 ans et plus. L'âge médian était de 29,2 ans.

Entre 2010 et 2013, l'accroissement de la population (+ ) était le résultat d'une part d'un solde naturel positif (+ ) avec un excédent des naissances () sur les décès (), et d'autre part d'un solde migratoire positif (+ ) avec un excédent des flux migratoires internationaux (+ ) et un excédent des flux migratoires intérieurs (+ ).

Selon des estimations de 2013, 90,8 % des Utahains étaient nés dans un État fédéré, dont 62,7 % dans l'État de l'Utah et 28,1 % dans un autre État (17,1 % dans l'Ouest, 4,3 % dans le Sud, 4,2 % dans le Midwest, 2,4 % dans le Nord-Est), 1,0 % étaient nés dans un territoire non incorporé ou à l'étranger avec au moins un parent américain et 8,2 % étaient nés à l'étranger de parents étrangers (57,9 % en Amérique latine, 19,5 % en Asie, 12,1 % en Europe, 4,1 % en Amérique du Nord, 3,3 % en Océanie, 3,0 % en Afrique). Parmi ces derniers, 37,2 % étaient naturalisés américain et 62,8 % étaient étrangers.

Selon des estimations de 2012 effectuées par le Pew Hispanic Center, l'État comptait immigrés illégaux, soit 3,6 % de la population.

Selon le recensement des États-Unis de 2010, la population était composée de 86,09 % — personnes— de Blancs, 2,73 % — personnes— de Métis, 2,00 % — personnes— d'Asiatiques, 1,19 % — personnes— d'Amérindiens, 1,06 % — personnes— de Noirs, 0,89 % — personnes— d'Océaniens et 6,03 % — personnes— de personnes ne rentrant dans aucune de ces catégories.

Les Métis se décomposaient entre ceux revendiquant deux races (2,54 %), principalement blanche et autre (0,65 %), blanche et asiatique (0,58 %) et blanche et amérindienne (0,46 %), et ceux revendiquant trois races ou plus (0,20 %).

Les non hispaniques représentaient 87,03 % — personnes— de la population avec 80,38 % — personnes— de Blancs, 1,96 % — personnes— d'Asiatiques, 1,77 % — personnes— de Métis, 0,98 % — personnes— d'Amérindiens, 0,94 % — personnes— de Noirs, 0,87 % — personnes— d'Océaniens et 0,13 % — personnes— de personnes ne rentrant dans aucune de ces catégories, tandis que les Hispaniques comptaient pour 12,97 % — personnes— de la population, principalement des personnes originaires du Mexique (9,37 %).

En 2010, l'État de l'Utah avait la forte proportion d'Océaniens après Hawaï (9,96 %) et l'Alaska (1,04 %). A contrario, l'État avait la faible proportion de Noirs après le Montana (0,41 %), l'Idaho (0,63 %), le Wyoming (0,84 %) et le Vermont (1,00 %).

L'État comptait également le grand nombre d'Océaniens après la Californie (), Hawaï () et l'État de Washington ().

En 2013, le Bureau du recensement des États-Unis estime la part des non hispaniques à 86,6 %, dont 79,5 % de Blancs, 2,1 % d'Asiatiques, 1,7 % de Métis, 1,1 % d'Amérindiens et 1,0 % de Noirs, et celle des Hispaniques à 13,4 %.

L'Utah connaît depuis la fin de la Seconde Guerre mondiale une baisse continue de la part de la population blanche non hispanique au sein de la population totale, marquée fortement depuis le début des années 1990 en raison notamment d'une immigration importante en provenance du Mexique, d’un âge médian plus élevé () que les Hispaniques (), d'une natalité plus faible ( en 2010) que les Hispaniques () et d'une augmentation substantielle des unions mixtes.

En 2010, les Blancs non hispaniques ne représentaient plus que 74,9 % des enfants de moins de 5 ans (17,2 % pour les Hispaniques, 3,5 % pour les Métis, 1,2 % pour les Asiatiques, 1,1 % pour les Noirs et 1,0 % pour les Océaniens) et 74,6 % des enfants de moins de 1 an (17,6 % pour les Hispaniques, 3,7 % pour les Métis, 1,2 % pour les Asiatiques, 1,0 % pour les Océaniens et 1,0 % pour les Noirs).

Selon des projections démographiques publiées par l’AARP, les Blancs non hispaniques constitueront 66,2 % de la population de l’État en 2060 si les tendances démographiques actuelles se poursuivent.

En 2000, les Utahains s'identifiaient principalement comme étant d'origine anglaise (29,0 %), allemande (11,6 %), américaine (6,8 %), danoise (6,5 %), mexicaine (6,1 %), irlandaise (5,9 %), écossaise (4,4 %) et suédoise (4,3 %).

L'État avait les plus fortes proportions de personnes d'origine anglaise et danoise, la forte proportion de personnes d'origine écossaise, la forte proportion de personnes d'origine suédoise, la forte proportion de personnes d'origine basque ainsi que la forte proportion de personnes d'origine néerlandaise.

L'État abrite la juive des États-Unis. Selon le North American Jewish Data Bank, l'État comptait Juifs en 2013 ( en 1971), soit 0,2 % de la population. Ils se concentraient principalement dans les agglomérations de Salt Lake City () et Summit Park (600).

L'État abrite également la arabe des États-Unis. Selon des estimations du Bureau du recensement des États-Unis, l’État comptait Arabes en 2013, soit 0,2 % de la population, principalement des Libanais ().

L’État abritait en 2013 une population noire assez bigarrée, composée principalement de descendants d’esclaves déportés sur le sol américain entre le début du et le début du (49,5 %) mais aussi d’Africains subsahariens (37,5 %), d’Hispaniques (7,3 %) et de Caribéens non hispaniques (5,7 %).

Le Bureau du recensement des États-Unis estimait le nombre d’Africains subsahariens à , soit 0,4 % de la population, principalement des Sud-Africains () et des Somaliens ().

Le nombre de Caribéens non hispaniques était quant à lui estimé à , soit 0,1 % de la population.

Les Hispaniques étaient essentiellement originaires du Mexique (72,3 %). Composée à 44,0 % de Blancs, 7,4 % de Métis, 1,6 % d'Amérindiens, 0,9 % de Noirs, 0,3 % d'Asiatiques, 0,2 % d'Océaniens et 45,5 % de personnes ne rentrant dans aucune de ces catégories, la population hispanique représentait 35,1 % des Métis, 17,8 % des Amérindiens, 11,4 % des Noirs, 6,6 % des Blancs, 2,6 % des Océaniens, 2,0 % des Asiatiques et 97,8 % des personnes ne rentrant dans aucune de ces catégories.

L'État avait les fortes proportions de personnes originaires d'Argentine (0,17 %), du Chili (0,12 %) et du Venezuela (0,10 %), la forte proportion de personnes originaires du Pérou (0,27 %), la forte proportion de personnes originaires d'Espagne (0,30 %) ainsi que la forte proportion de personnes originaires du Mexique (9,37 %).

L'État comptait également le grand nombre de personnes originaires d'Argentine ().

Les Asiatiques s'identifiaient principalement comme étant Chinois (20,2 %), Viêts (14,5 %), Indiens (11,2 %), Japonais (11,0 %), Philippins (10,1 %), Coréens (9,7 %), Laotiens (4,5 %) et Cambodgiens (3,4 %).

L'État avait la forte proportion de Japonais (0,22 %).

Les Amérindiens s'identifiaient principalement comme étant Navajos (44,2 %), Utes (9,1 %) et Amérindiens du Mexique (3,2 %).

Les Océaniens s'identifiaient principalement comme étant Tongiens (38,3 %), Samoans (33,6 %), Hawaïens (7,8 %) et Marshallais (3,0 %).

Les Métis se décomposaient entre ceux revendiquant deux races (92,9 %), principalement blanche et autre (23,6 %), blanche et asiatique (21,1 %), blanche et amérindienne (16,7 %), blanche et noire (12,7 %) et blanche et océanienne (9,1 %), et ceux revendiquant trois races ou plus (7,1 %).

Selon l'institut de sondage "The Gallup Organization", en 2015, 55 % des habitants de l'Utah se considèrent comme « très religieux » (40 % au niveau national), 15 % comme « modérément religieux » (29 % au niveau national) et 31 % comme « non religieux » (31 % au niveau national).

L'anglais est la langue officielle de l'État depuis 2000. Selon des estimations de 2013, 85,9 % des Utahains âgés de plus de 5 ans parlaient anglais à la maison contre 14,1 % une autre langue, dont 9,3 % espagnol ou un créole espagnol, 1,9 % une autre langue indo-européenne (0,3 % allemand, 0,3 % portugais, 0,2 % français ou un créole français, 0,2 % russe), 2,0 % une langue asiatique ou océanienne (0,5 % chinois, 0,3 % vietnamien, 0,2 % coréen) et 0,8 % une autre langue (0,3 % navajo).

Les mormons, pour vivre en accord avec leur foi, décidèrent de s'exiler dans le désert de l'Utah, faute de pouvoir s'installer plus à l'est. Les pionniers mormons s'installèrent en masse à partir de 1847, menés par Brigham Young, dirigeant mormon et futur premier gouverneur du territoire de l'Utah. Les mormons, habiles hommes d'affaires, épousèrent l'ère industrielle. Ils vont largement contribuer au développement économique de l'État.

Seule une petite partie des 15 millions de mormons vivent en Utah. Salt Lake City, capitale de l'État, est aussi le siège mondial du mormonisme, mais dans la ville même, les mormons sont légèrement minoritaires. L'Église de Jésus-Christ des saints des derniers jours, dont les mormons sont membres, a été organisée aux États-Unis en 1830 par Joseph Smith. Ayant été créé par les pionniers mormons au , l'État d'Utah a une vie politique, sociale et culturelle encore largement dominée par cette Église, qui impose une morale stricte à ses membres.

En 2013, les mormons représentent environ 62 % de la population de l'Utah. Dans certains comtés, comme le comté d'Utah, la part des mormons dépasse 80 %.

Le Gouvernement fédéral a défini sept réserves indiennes dans ou en partie dans l'État de l'Utah.

En 2010, résidaient dans une réserve indienne, soit 1,1 % de la population de l'État.

La réserve indienne de Uintah and Ouray est la la plus vaste () des États-Unis après celle de la Nation Navajo (). Elle était également la la plus peuplée () des États-Unis en 2010.

L'Utah est un État plutôt riche. Dans les zones irriguées, les récoltes sont abondantes. Il dispose en outre d'une industrie relativement importante et de ressources minières de valeur : or, cuivre, plomb et aussi dans la région de Moab, de l'uranium.

L'Utah est l'un des États les plus conservateurs des États-Unis et un bastion du Parti républicain depuis les années 1950. Cela s'explique principalement par l'importance de l'Église de Jésus-Christ des saints des derniers jours, à laquelle appartiennent environ 75 % des électeurs (en 2008). Bien qu'ultraconservateurs sur les questions de société (avortement et mariage homosexuel), les électeurs de l'État sont plus modérés sur les questions d'immigration. En 2004, par référendum, les électeurs de l'Utah ont approuvé par 66 % des voix () un amendement constitutionnel définissant le mariage comme une union civile entre un homme et une femme, bannissant ainsi toute forme d'union homosexuelle. Les rares bastions démocrates sont la capitale Salt Lake City et la station de sports d'hiver de Park City.

La constitution de l'Utah date de 1895. Malgré la présence d'une communauté polygame importante, la constitution de l'Utah n'a jamais légalisé la polygamie, interdite par le Congrès fédéral. L'Église de Jésus-Christ des saints des derniers jours adopte officiellement une position neutre vis-à-vis des partis politiques et des candidats mais ses dirigeants ne cachent pas leur hostilités aux candidats dits libéraux ou progressistes, notamment sur les questions sociales. Les dirigeants et les militants du Parti républicain local sont néanmoins tous très proches de l'église de Jésus-Christ des saints des derniers jours au point d'être parfois perçu comme une de ses émanations.

Jusqu'à l'élection présidentielle de 1952, l'Utah était un état qui penchait alternativement vers les Républicains ou les Démocrates. Ainsi, lors des premières élections organisées en 1896, les électeurs votèrent pour le démocrate William Jennings Bryan (82 % des suffrages) contre le républicain William McKinley, élu au niveau national. Quatre ans plus tard, ils votaient McKinley (50,58 %) contre le même Bryan (45 %). Après avoir été l'un des rares états à voter pour le républicain William Howard Taft en 1912, ils votèrent quatre ans plus tard pour son adversaire démocrate, le président Woodrow Wilson. L'Utah vota encore à quatre reprises pour le démocrate Franklin Delano Roosevelt puis pour Harry S. Truman.

Depuis l'élection présidentielle américaine de 1952, l'Utah est incontestablement devenu l'un des bastions républicains les plus marqués du pays. Depuis cette date, un seul candidat démocrate, Lyndon B. Johnson en 1964, a remporté l'Utah (s'imposant alors face à Barry Goldwater).

Les candidats républicains y ont également réalisés leurs meilleurs scores nationaux en 1976, 1980, 1984, 1988, 1996, 2000, 2004 et 2012. En 1992, l'Utah fut le seul état où le démocrate Bill Clinton, pourtant élu au niveau national, était arrivé en troisième position avec 24,65 % des voix derrière le républicain George Bush (43,36 %) et l'indépendant Ross Perot (27,34 %).

À l'élection présidentielle de 2004, le président républicain George W. Bush y a remporté la totalité des comtés, avec 71,54 % des voix contre 26 % au candidat démocrate John Kerry, parvenant à un pic record de 88,91 % des suffrages dans le comté de Rich.

Lors de l'élection présidentielle de 2008, le candidat républicain John McCain y a obtenu 62,25 % des voix contre 34,22 % des voix au candidat démocrate Barack Obama.

En 2016, le républicain Donald Trump obtient en Utah 45,1 % des voix face à 27,2 % pour son adversaire démocrate Hillary Clinton.

Lors de la 115 législature du Congrès (2017-2019), l'Utah est représenté à la Chambre des représentants par quatre républicains, ainsi que par Mike Lee et Orrin Hatch, tous les deux républicains, au Sénat.

La politique locale de l'Utah est largement dominée par les républicains. Près de 80 % des parlementaires sont membres de l'Église de Jésus-Christ des saints des derniers jours. Depuis 1896, l'Utah n'a ainsi eu que deux gouverneurs qui ne soient pas membres de cette Église.

Depuis le , le gouverneur de l'Utah est le républicain Gary Herbert qui avait repris en cours de mandat le poste du républicain Jon Huntsman, nommé ambassadeur des États-Unis en Chine. Le lieutenant-gouverneur de l'Utah est le républicain Spencer Cox. Les autres principaux postes élus de l'exécutif sont également détenus par des républicains.

La Législature de l'Utah est largement dominée par les républicains. Lors de la législature 2017-2019, les républicains détiennent ainsi 62 sièges de la Chambre des représentants de l'État contre 13 aux démocrates tandis qu'au Sénat, 24 républicains font face à cinq démocrates.

Le pouvoir judiciaire de l'Utah est composé des tribunaux suivants:

La SkyWest Airlines et Delta Air Lines relient les grands aéroports des États-Unis à ceux de Salt Lake City (aéroport international) et Cedar City. La voiture de location constitue ensuite le moyen idéal pour découvrir les parcs nationaux. Un circuit automobile est balisé dans chacun d'eux. La visite se poursuit à pied dans les Jardins du diable à Arches et le long de l'East Rim Trail dans Zion, en jeep sur la piste d'Eléphant Hill dans le Parc national de Canyonlands, à cheval dans Bryce, en avion au-dessus de Glen Canyon ou en bateau sur le lac Powell. Le soir, on s'arrête dans un "lodge", un motel ou un hôtel ou encore dans un terrain de camping.

Les déserts du sud de l’Utah occupent 5 parcs nationaux :

De nombreux autres parcs existent au niveau de l'État.

Les monuments nationaux de l’Utah sont :
Les Bears Ears sont également proposées au classement de monuments nationaux.

Enfin, l’Utah dispose de plusieurs parcs fédéraux et monuments :

Les universités en Utah sont:



L'Utah a accueilli les Jeux olympiques d'hiver de 2002.




</doc>
<doc id="3089" url="https://fr.wikipedia.org/wiki?curid=3089" title="UDF">
UDF

UDF est un sigle de trois lettres, qui signifie :<br>

Partis politiques

Autres

</doc>
<doc id="3090" url="https://fr.wikipedia.org/wiki?curid=3090" title="Union pour la démocratie française">
Union pour la démocratie française

L’Union pour la démocratie française (UDF) est un parti politique français composé de différents partis de centre droit et de droite non gaullistes, d'inspiration démocrate-chrétienne, libérale et laïque. Fondée en 1978 en soutien au président Valéry Giscard d'Estaing, l'UDF s'est dissoute "de facto" dans le Mouvement démocrate le 30 novembre 2007.




L'UDF est fondée le par Michel Poniatowski sur l'idée de Jean Lecanuet et de Jean-Jacques Servan-Schreiber pour aider le président Valéry Giscard d'Estaing à disposer d'un parti le soutenant en vue des élections législatives de 1978, qui s'annoncent difficiles pour la droite. Pour le journaliste Laurent de Boissieu, l'objectif de l'UDF à sa création est de . Une liste de candidats pouvant se revendiquer de l'appartenance à l'Union pour la démocratie française est publiée le , sans toutefois qu'un nouveau parti soit créé. Le nom de l'UDF provient du titre du livre de VGE, "Démocratie française", vendu à plusieurs centaines de milliers d'exemplaires.

Elle réunit six composantes, du centre et de la droite non-gaulliste, qui demeurent chacune des partis politiques indépendants :


Le , l'UDF se dote d'une structure provisoire : Michel Pinton (Parti républicain) en devient délégué général et un comité exécutif et un conseil national, composés de personnalités issues des différentes mouvances, sont créés.

Selon les termes de Laurent de Boissieu, l'UDF regroupe .

L'UDF connaît le succès dès les législatives de mars 1978, avec 21,37 % des voix et 123 sièges. Elle permet au Raymond Barre d'avoir une majorité à l'Assemblée nationale, le RPR étant le partenaire obligé de cette coalition. Les élections européennes de juin 1979 sont un nouveau succès pour l'UDF qui arrive en tête avec 27,61 % des voix devant le PS, le PC et le RPR.

L'UDF passe dans l'opposition en , à la suite de la défaite de Valéry Giscard d'Estaing, battu par François Mitterrand, lors de l'élection présidentielle, et de la défaite aux législatives de juin 1981, où elle obtient 19,17 % des voix. Dirigée par Jean Lecanuet (jusqu'en 1988), elle apparaît surtout comme un « syndicat » d'élus locaux et de notables, dès les cantonales de mars 1982 où la coalition UDF-RPR-divers droite dépasse de nouveau la coalition PS-PC-MRG.

En , Olivier Stirn, cadre de l'UDF, estime que celle-ci .

L'engagement européen reste une valeur fédératrice forte. Simone Veil dirige ainsi aux européennes de juin 1984 une liste UDF-RPR qui obtient 43,01 % des voix et 41 sièges sur 81. Sur le plan national, et faute sans doute d'un dirigeant d'envergure malgré le retour progressif à la vie politique de Valéry Giscard d'Estaing qui en prend la présidence en 1986 (jusqu'en 1996), l'UDF laisse au RPR et à Jacques Chirac la direction de la coalition de droite et de centre droit aux législatives de mars 1986, où le scrutin proportionnel départemental impose un certain nombre de listes d'union. Avec à l'Assemblée nationale 114 sièges et 17 apparentés, l'UDF est alors le partenaire minoritaire du Chirac auquel participent notamment André Giraud, François Léotard, Alain Madelin, Pierre Méhaignerie ou René Monory.

Lors de l'élection présidentielle de 1988, Raymond Barre, candidat officiel de l'UDF obtient un score de 16,54 % des voix, derrière François Mitterrand et Jacques Chirac. Réélu au second tour, François Mitterrand, privé d'une majorité socialiste absolue lors des législatives de juin 1988, mène une politique d'« ouverture » qui permet au gouvernement Rocard, puis aux gouvernements Édith Cresson et Pierre Bérégovoy d'obtenir le soutien ponctuel d'un groupe centriste spécifique à l'Assemblée nationale (Union du centre) et la participation de plusieurs membres historiques de l'UDF, notamment Jean-Pierre Soisson. PS et centristes de la « majorité présidentielle » sont également alliés lors des élections cantonales et régionales de , le plus souvent face à des listes d'union RPR-UDF qui globalement remportent le scrutin.

L'UDF est l'alliée du RPR aux élections législatives de 1993, au sein de l'Union pour la France, et participe aux gouvernements Édouard Balladur puis Alain Juppé. En 1995, l'ancien Premier ministre Raymond Barre devient maire et président de la communauté urbaine de la deuxième agglomération de France, Lyon.

L'UDF commence à se désagréger avec la perte d'influence de son fondateur qui ne réussit plus à surmonter les divergences des courants de pensée au sein du mouvement. Il n'y eut d'ailleurs pas de candidat UDF aux présidentielles de 1988 et 1995 : Raymond Barre, candidat en 1988, n'est qu'apparenté, et l'UDF -comme d'ailleurs le RPR- se déchire en 1995 entre partisans d'Édouard Balladur (CDS et une partie du PR) et de Jacques Chirac (comme Alain Madelin, Charles Millon, et l'ensemble du PPDF derrière Hervé de Charette).

Les élections régionales de 1998 entraînent une scission. Alors que plusieurs présidents de région avaient été élus avec les voix de l'extrême droite, François Bayrou, alors président de Force Démocrate et Alain Madelin, président de Démocratie libérale, se divisent sur la condamnation de ces accords. La position de François Bayrou, qui s'oppose aux alliances avec le Front national (FN), l'emporte finalement, et Démocratie libérale choisit de quitter l'UDF quelques semaines plus tard. Quelques dissidents de DL comme Gilles de Robien et François Léotard décident de rester à l'UDF, au sein d'un Pôle républicain indépendant et libéral (PRIL).

Élu président de l'UDF la même année, François Bayrou décida d'unifier les différentes composantes de l'UDF pour créer un nouveau parti, marqué au centre. En novembre 1998, au Congrès fondateur de Lille, les adhérents approuvèrent le principe de la fusion entre FD, le PRIL et l'UDF-AD, tandis que le Parti radical (PR) et le PPDF conservaient leur autonomie : la "Nouvelle UDF" était née.

Rapidement, François Bayrou affirma sa volonté de prendre ses distances vis-à-vis du RPR, menant lui-même une liste UDF aux élections européennes, face à la liste RPR-DL. Mais cette stratégie suscita des oppositions au sein des élus UDF qui furent nombreux à choisir, lors de l'élection présidentielle de 2002, de soutenir Jacques Chirac plutôt que François Bayrou. Malgré son relatif succès, François Bayrou, arrivé en quatrième position avec 7 % des voix, ne put s'opposer à la création de l'UMP, lancée par Jacques Chirac et Alain Juppé dès le lendemain du dimanche pour unifier les partis de la droite et du centre.

Une grande partie des élus UDF quittèrent le mouvement pour rejoindre l'UMP. Toutefois, aux élections législatives de 2002, l'UDF parvint, avec 29 députés, à conserver un groupe politique à l'Assemblée Nationale. Cette tribune lui permit d'affirmer sa différence en s'opposant à plusieurs reprises au gouvernement de Jean-Pierre Raffarin.

En 2003, après une campagne électorale dirigée par Benjamin Ferniot, directeur de cabinet de François Bayrou, Christian Blanc, apparenté radical de gauche, ancien PDG d'Air France puis de la RATP, étant présenté par Anne-Marie Idrac, députée UDF sortante, remporta au premier tour le siège de député de la troisième circonscription des Yvelines.

L'UDF retrouva un niveau de 12 % des suffrages exprimés au premier tour des élections régionales de 2004 et aux élections européennes de juin 2004. Ce succès est en partie dû à l'alternative qu'elle propose aux électeurs de droite et du centre mécontents de la politique du gouvernement, notamment sur les questions sociales ; en partie à son engagement européen, qui lui permet de séduire au-delà de sa sphère d'influence traditionnelle. Lors des élections régionales 2004 le président de l'UDF François Bayrou, tête de liste UDF dans la région Aquitaine, a décidé, au second tour des élections régionales de 2004, de se désister, en faveur du candidat UMP Xavier Darcos. Au cours de ce scrutin, aucun candidat de l'UDF ne s'est désisté au second tour au profit d'un candidat du Parti socialiste.

Après les élections européennes, l'UDF a quitté le Parti populaire européen (historiquement le parti des Démocrates-chrétiens, mais désormais élargi à toutes les formations de droite), prenant pour prétexte l'euroscepticisme croissant du parti, pour fonder avec le centre gauche italien de La Marguerite un nouveau parti pro-européen, le Parti démocrate européen qui forme avec les libéraux le groupe de l'Alliance des démocrates et des libéraux pour l'Europe (ADLE-ALDE) au Parlement européen.

Le 8 juin 2005, pour la première fois depuis 2002, l'UDF a refusé de voter la confiance au premier gouvernement Dominique de Villepin, à la suite du discours de politique générale que ce dernier a prononcé devant l'Assemblée nationale. La moitié du groupe des députés UDF a ensuite voté contre le projet de budget 2006 présenté par ce gouvernement ; les sénateurs Union centriste se sont abstenus.

Les et , lors du Congrès extraordinaire de Lyon, les adhérents de l'UDF (91,1 % des votants) ont apporté leur soutien à la motion unique de François Bayrou définissant l'UDF comme un « parti libre et indépendant », au centre, séparé des majorités et opposition de droite comme de gauche et garant d'une démocratie pluraliste. Les partisans d'une alliance pré-électorale avec l'UMP, menés par le ministre Gilles de Robien sont marginalisés au sein du parti. Gilles de Robien avait été suspendu de ses fonctions exécutives au sein du parti à la suite de son entrée au gouvernement Villepin, contraire à la décision de non-participation de l'UDF.

Tout en soutenant la ligne défendue par François Bayrou, un certain nombre d'élus locaux de l'UDF et de parlementaires avaient exprimé leurs réserves quant à son application trop radicale. Ils mettaient en avant que l'UDF participait à de nombreux exécutifs de collectivités territoriales (communes, départements, région) aux côtés de l'UMP, ce qui la place toujours comme un allié naturel de la droite. Il est d'ailleurs à noter qu'il existe une collectivité territoriale de taille importante qui est cogérée par l'UDF et un parti de gauche, le Grand Lyon, géré par une coalition PS-Verts-PCF-UDF. Mais il en existe d'autres, à des échelons plus locaux, à Nice notamment.

Les députés UDF n'ont voté ni la confiance au gouvernement Villepin, ni la motion de censure déposée contre ce Gouvernement Villepin le par les députés socialistes.

Le , dans le cadre de l'affaire Clearstream 2, François Bayrou et dix autres députés UDF votent la motion de censure déposée par l'opposition (Parti socialiste) contre le gouvernement Villepin. C'est la première fois depuis la création du parti en 1978 qu'un dirigeant centriste se rallie à un texte de cette nature. Toutefois, la position de François Bayrou est restée relativement minoritaire au sein du groupe parlementaire de l'UDF qui comptait alors 29 membres (dont les apparentés).

En réaction, Gilles de Robien, seul ministre UDF du gouvernement Villepin, a lancé, le 21 mai, le cercle Société en mouvement, favorable à l'appartenance de l'UDF à la majorité, et qui est restée sans lendemain, à défaut d'être soutenue par la dominante sarkozyste de l'UMP.

Le 10 juin, depuis Issy-les-Moulineaux, fief de l'UDF historique André Santini, Bayrou lance son appel à une « révolution civique », en présence de Gilles de Robien, venu reconnaître « la légitimité de François Bayrou en tant que président de l'UDF candidat à l'élection présidentielle ». Celui-ci est néanmoins chahuté par une très grande partie des conseillers nationaux UDF présents et quittera l'assemblée au motif d'un agenda chargé.

Le , François Bayrou arrive en troisième position à l'élection présidentielle avec un score de 18,57 % , derrière Nicolas Sarkozy (UMP avec 31,18 % des voix) et Ségolène Royal (PS avec 25,87 % des voix).

Sévèrement attaqué par l'UMP pour avoir accepté de participer à un débat télévisuel proposé par la candidate au second tour Ségolène Royal et relevant que de son côté le candidat Nicolas Sarkozy refuse comme « non légitime » tout débat officiel et démocratique avec lui, François Bayrou ne donne pas de consigne et indique à titre personnel « ne pas vouloir voter pour Nicolas Sarkozy ». En décembre 2010, il confiera avoir voté blanc.

En opposition à cette position, 24 députés UDF ayant pour la plupart appelé à voter pour Nicolas Sarkozy au second tour – et également soucieux d'être réélus aux législatives de juin 2007 sans désorienter leur électorat habituel de centre droit et avec le soutien de l'UMP – publient le dans "Le Figaro" une tribune intitulée « Pour un centre libre dans la majorité présidentielle ». Ils y déclarent entrer en dissidence avec les positions de François Bayrou et appellent pour la plupart (MM. Albertini et de Courson reviendront sur cet appel) à la constitution d'un nouveau parti politique de centre droit, partenaire de l'UMP, ce qui sera fait avec la fondation du Nouveau Centre.

En réponse, lors du conseil national UDF du suivant, les nationaux UDF présents décident à l'unanimité moins 4 voix d'appeler la constitution d'un nouveau parti politique dénommé Mouvement démocrate et que l'UDF présentera des candidats aux législatives sous la bannière .

La liste des candidats du MoDem, arrêtée le , comporte 535 candidats à la députation sur 577 circonscriptions, plus que le Parti socialiste et le record dans l'histoire de l'UDF.

Le Parti démocrate européen, de tendance réformiste et centriste, est un parti politique européen créé le par le Français François Bayrou et l'Italien Francesco Rutelli qui sont ses deux premiers coprésidents, Romano Prodi en étant le président d'honneur. Le chef de l'UDF a créé dans la foulée, fin avec la "New Democrat Coalition" américaine, composante parlementaire du Parti démocrate, une « internationale », ou plutôt une Alliance mondiale des démocrates. En Italie, un important Parti démocrate, réunissant la gauche ex-communiste et le centre gauche chrétien, a vu le jour dans un congrès convoqué en octobre 2006.

Selon le journaliste Laurent de Boissieu, la marque UDF aurait été déposée plusieurs fois auprès de Institut national de la propriété industrielle (INPI). En effet, lors du dépôt d'une marque, l'INPI rappelle les exigences de droit antérieur mais ne les contrôle pas lui-même. Hervé de Charette, ancien membre de l'UDF ayant rejoint l'UMP en 2002, aurait déposé le la marque « Union pour la démocratie française UDF » au nom de la Fédération nationale des Clubs Perspectives et Réalités, elle-même ayant été renommée le Parti populaire pour la démocratie française, puis en 2002 « Convention démocrate » lors de son adhésion à l'UMP. La personne morale UDF aurait ensuite déposé, entre 2006 et 2008, les marques suivantes : « Nouvelle UDF », « UDF », « Union pour la Démocratie française », « Parti démocrate - UDF », « UDF LE PARTI LIBRE ».

En , Hervé de Charette revendique la propriété de la marque UDF au titre de l'antériorité. La direction de l'UDF - intégrée au MoDem - annonce de son côté vouloir « engager des poursuites judiciaires contre ceux qui avaient frauduleusement déposé le nom de l'UDF et sans en avoir ni droit ni qualité pour procéder à un tel dépôt », invoquant l'intention de nuire du déposant. En décembre 2009, Hervé Morin annonce pour le Nouveau Centre son intention de vouloir utiliser le sigle UDF. L'association UDF menace alors le Nouveau Centre de poursuites judiciaires. En décembre 2010, Hervé Morin indique sa décision de « ne pas se lancer dans un combat inutile », « compte tenu de la complexité juridique du droit des marques ».

Le , Le président de l'UDF François Bayrou annonce la création d'un nouveau parti, le Mouvement démocrate (MoDem) regroupant les formations politiques et les militants souhaitant la constitution d'une force politique centriste indépendante de la droite et de la gauche. Ce parti est créé le . Les membres fondateurs du Mouvement démocrate sont : François Bayrou, Marielle de Sarnez, Michel Mercier et Jacqueline Gourault. Le Mouvement démocrate est dans ses statuts un mouvement politique unitaire.

François Bayrou propose au vote, lors du conseil national le , le principe de la création du Mouvement démocrate. L'UDF est devenue, lors de son dernier congrès, le à Villepinte, membre fondateur du Mouvement démocrate, dont les statuts, déposés en préfecture le , sont très proches de ceux de l'UDF et ne permettent pas l'appartenance à deux partis politiques. Le siège de l'UDF (133"bis", rue de l'Université, à Paris) est devenu le siège du MoDem.

Formellement et selon le droit des associations, l'existence juridique de l'UDF subsiste pour une période transitoire de trois ans. L'UDF délègue aux instances du Mouvement démocrate la « responsabilité de l'action et de l'expression communes », les autres instances nationales et locales de l'UDF devant « s'intégrer aux instances correspondantes du Mouvement démocrate ». Pendant cette période transitoire, « les intérêts juridiques, matériels et moraux » de l'UDF doivent être « garantis et administrés par un bureau de vingt à trente membres désignés par le Congrès, sur proposition du président dont le mandat est prorogé ».

Selon certains médias et observateurs situés plus à droite, ce montage juridique marque la « mort de l'UDF ».

Afin de marquer immédiatement son existence politique, le Mouvement démocrate présente des candidats aux élections législatives de juin 2007 sous l'étiquette « UDF-Mouvement démocrate », le financement de ces candidats étant assuré par l'UDF. Cette dénomination « UDF-Mouvement démocrate » n'est pas reprise lors des élections suivantes: municipales de mars 2008 (sauf en de rares exceptions), européennes de juin 2009 (« Démocrates pour l'Europe ») et régionales de mars 2010 (« Centre-MoDem »).

Le , la majorité du parti refuse définitivement un retour à l'UDF.

Cependant, l'UDF n'a jamais été formellement dissout et il reste dirigé par un bureau politique, dont Bayrou est le président.

En , une large majorité (23 sur 29 en prenant en compte les apparentés) des députés de l'UDF n'ont pas suivi François Bayrou dans son intention de création du Mouvement démocrate (MoDem), reprenant la ligne défendue par Gilles de Robien d'une alliance des centristes dans la majorité présidentielle, qui maintiendrait l'UDF au centre droit. Cette position a été notamment défendue par des personnalités comme Hervé Morin, président sortant du groupe UDF à l'Assemblée nationale, Maurice Leroy, vice-président de l'UDF. D'autre part, Jean-Louis Bourlanges, député européen exprimait des réserves sur la stratégie.

La plus grande partie du groupe des députés UDF sortants, en désaccord avec la ligne politique de François Bayrou, a pris l'initiative de fonder le Nouveau Centre, en présentant des candidats sous cette étiquette aux élections législatives de 2007.

Une autre partie des cadres et militants ont créé l'Alliance centriste ou encore Avenir démocrate, tandis que d'autres ont rejoint le Forum des républicains sociaux, parti associé à l'UMP, ou sont devenus « centristes sans étiquette ».


dont le groupe Union du centre : 34 membres et 7 apparentés (sur 577).


30 membres ont fondé le groupe Démocratie libérale et indépendants en 1998.



Élus sous l'étiquette « UDF » et/ou « MoDem » en juin 2007 :





Les sénateurs membres du Mouvement démocrate, du Nouveau Centre, de l'Alliance centriste, ainsi que certains qui ne sont affiliés à aucune de ces formations siègent au groupe Union centriste. En juin 2009, ce groupe comporte 29 membres dont 28 anciens UDF.


La délégation UDF au Parlement européen, présidée par Marielle de Sarnez, comprenait 11 députés européens au sein de l'Alliance des démocrates et des libéraux pour l'Europe. 3 députés l'ont quittée en pour former la délégation de l'Alliance citoyenne pour la démocratie en Europe de Jean-Marie Cavada (ELDR) d' à 2009.

Avant la création du MoDem, huit conseils généraux avaient des présidents UDF (entre parenthèses leur éventuelle nouvelle étiquette à la suite de la création du MoDem) : Calvados, Loir-et-Cher (Nouveau Centre), Mayenne (Union centriste-UDF), Morbihan (MoDem), Pyrénées-Atlantiques (MoDem), Haut-Rhin (UMP), Rhône (MoDem), Somme (Nouveau Centre).

Avant la création du MoDem, plusieurs maires étaient UDF (entre parenthèses leur éventuelle nouvelle étiquette à la suite de la création du MoDem) : Annecy (Nouveau Centre), Amiens (Nouveau Centre), Arras (MoDem), Biarritz (MoDem), Blois (Nouveau Centre), Deauville (centriste indépendant), Drancy (Nouveau Centre), Issy-les-Moulineaux (Nouveau Centre), Le Plessis-Trévise (MoDem), Meudon(Nouveau Centre), Montrouge (Nouveau Centre), Noisy-le-Sec (MoDem), Redon (divers droite), Reims (divers droite), Rouen (centriste indépendant), Saint-Brieuc (MoDem), Talence (MoDem), Ville-d'Avray (MoDem).



Vingt-six membres désignés par le congrès du , sur proposition du président :





Parmi les conseillers de François Bayrou


Fondés en 1998, lors de l'unification de l'UDF, les Jeunes UDF regroupent les adhérents de l'UDF de 16 à 34 ans inclus. Présents dans toutes les instances de l'UDF, ils prennent activement part au débat interne au parti et font valoir leurs positions. Chaque année, ils organisent l'Université d'été de l'UDF, moment fort de la vie du mouvement.

Contrairement à de nombreux mouvements politiques jeunes, les Jeunes UDF disposent de leurs propres structures et choisissent eux-mêmes leurs responsables :

Au niveau de chaque département existe une fédération des Jeunes UDF organisée de manière similaire :

Si la fédération départementale compte moins de 10 adhérents, un délégué fédéral nommé par le président national tient le rôle du président et du bureau.


Alors que depuis sa fondation, sa couleur était le bleu, le parti passa à l'orange à partir de 2006.



</doc>
<doc id="3091" url="https://fr.wikipedia.org/wiki?curid=3091" title="Planeur ultra-léger motorisé">
Planeur ultra-léger motorisé

Un planeur ultra-léger motorisé, plus couramment appelé ULM, est un aéronef muni d'un moteur, répondant à des critères de masse ou de puissance maximales définis par les juridictions nationales et autorisant son utilisation dans des conditions de navigabilité simplifiées par rapport à celles d'un avion léger et avec une licence de pilotage spécifique. La plupart des ULM peuvent décoller et atterrir sur des distances réduites hors des aérodromes, mais les plus performants s'apparentent aux avions légers certifiés.

L'homme tente depuis des siècles d'imiter le vol des oiseaux en cherchant un système reproduisant le mouvement de leurs ailes, et les débuts de l'ultra-léger motorisé remontent aux débuts de l'aviation, à l'exception du travail précurseur d'Étienne Dormoy à Dayton, Ohio, entre 1919 et 1924 et à son de 1924. Son histoire est donc relativement récente.

Le mouvement ULM est issu du vol libre dont le deltaplane était le premier représentant. L'aile Rogallo a été inventée par l'Américain Francis Rogallo en 1936, mais ce n'est qu'en 1972 que le premier deltaplane a été construit sous sa forme actuelle.

Très rapidement, l'idée de s'affranchir du relief pour décoller par leurs propres moyens, poussa une poignée de pilotes de vol libre à adjoindre un moteur de tronçonneuse sur de simples ailes delta. Des pionniers comme les frères Pérès et leur bimoteur ou comme Roland Magallon et son ou encore comme Jean-Marc Geiser avec son Motodelta, mariage entre une aile Danis et un tricycle motorisé dont le premier vol eu lieu en 1974 et présenté au Salon du Bourget en 1975, ont ouvert la voie à d'autres machines construites sur le même principe. Les premiers ULM étaient des pendulaires. Le monde de l'ULM s'est énormément développé au cours des années 1980, conquérant chaque année de nouveaux adeptes.

La Fédération française de planeur ultra-léger motorisé a été créée en 1989, soit une dizaine d'années après l'apparition des ULM dans l'espace aérien français. Association française de loi 1901, la FFPLUM gère le mouvement ULM en France et regroupe la majorité des adeptes de cette discipline. Elle est agréée par le ministère de la Ville, de la Jeunesse et des Sports et bénéficie d'une délégation de l'état pour la gestion des activités ULM dans la pratique sportive. Elle est également inscrite au Code de l'aviation civile.

La Fédération représente le mouvement ULM auprès des pouvoirs publics. Elle est représentée au sein de la Fédération aéronautique internationale (FAI) et participe aux travaux de la CIMA (Commission Internationale de Micro-Aviation), organisme qui regroupe les différentes fédérations internationales d'ULM et gère les compétitions au niveau international (championnats d'Europe et du Monde).

Elle est membre du Conseil National des Fédérations Aéronautiques et sportives (CNFAS). Elle est déclarée d'utilité publique.

L'ULM est régi en France par l'arrêté du 8 modifié relatif aux aéronefs ultra-légers motorisés.

Un paramoteur est un aéronef monomoteur sustenté par une voilure souple ou de type parachute. Il répond aux conditions techniques suivantes :
Il existe une sous-classe 1A dite « à motorisation auxiliaire » qui répond aux conditions techniques suivantes :

Un pendulaire est un aéronef monomoteur sustenté par une voilure souple sous laquelle est généralement accroché un chariot motorisé. Il répond aux conditions techniques suivantes :
Il existe une sous-classe 2A dite « à motorisation auxiliaire » qui répond aux conditions techniques suivantes :

Un ULM multi-axes est un aéronef sustenté par une voilure fixe. Il répond aux conditions technique suivantes :
Il existe une sous-classe 3A dite « à motorisation auxiliaire » qui répond aux conditions techniques suivantes :

Un autogire ultraléger répond aux conditions techniques suivantes :

Un aérostat dirigeable ultraléger répond aux conditions techniques suivantes :

Un hélicoptère ultraléger répond aux conditions techniques suivantes :

La pratique de l'ULM nécessite l'obtention du brevet de pilote ULM. Le pilotage est autorisé dès l'âge de . Après la réussite à une épreuve théorique consistant en un questionnaire à choix multiples composé de , l'instructeur apprécie souverainement le moment où l'élève-pilote est prêt à être "lâché" : il réalise son premier vol en solo, avec l'instructeur en bord de piste. Quelques dizaines d'heures de pratique sont généralement nécessaires en fonction de la capacité à apprendre de l'élève-pilote. Le brevet est délivré par la direction générale de l'Aviation civile (DGAC) dès lors que l'élève-pilote est en possession de l’attestation de réussite aux épreuves théoriques (QCM) et que l’instructeur juge qu’il est capable de voler en sécurité et de façon autonome. Le brevet délivré n'est valable que pour la classe d'ULM considérée lors de la formation initiale.

Aucune visite médicale spécifique aéronautique n'est demandée pour pratiquer l'ULM. Un simple certificat médical délivré par tout médecin suffit en application de l'article 5 de la loi du : . La réglementation n'impose aucune pratique régulière aux pilotes pour le maintien de la licence d'ULM quand bien même le bon sens et la sécurité générale des vols le préconisent. Le carnet de vol n'est pas obligatoire.

Il existe des qualifications particulières pour deux activités spécifiques :

Une déclaration de niveau de compétence (DNC) est exigée pour :

Les ULM nécessitent en France une identification mais leur suivi technique est de la responsabilité du propriétaire (Identification = système déclaratif, Immatriculation = certification).

Comme tous les engins volants, un ULM est soumis aux règles de circulation aérienne. Ces règles sont appelées règles de l'air. Ils sont de plus limités au mode dit de vol à vue : vol durant le jour aéronautique, en dehors des nuages, avec une visibilité minimale et la vue du sol.

Renseignements en France :

On désigne par avion les aéronefs certifiés par l'OACI, par opposition aux ULM tels que définis ici. Ni mieux ni moins bien, la philosophie des deux catégories est différente et correspond à un usage différent :

Le nombre de pratiquants du vol sur ULM en France était de en 2002, à comparer aux d'aviation légère.
Le nombre d'adhérents à la Fédération française d'ULM était de en 2006 puis d'environ en 2013. Mais il est difficile de chiffrer exactement le nombre d'amateurs d'ULM, car l'adhésion à la FFPLUM n'est pas obligatoire.

Le développement de la pratique de l'ULM a contribué à celui d'activités annexes, que ce soit l'instruction, la vente et l'entretien des machines, le tourisme (découverte d'une région…) ou autres (emport de banderoles publicitaires, travail photographique). Des sociétés de construction ont également été créées. 

Dans le domaine du pendulaire sont à signaler : la société Delta Trikes Aviation, à Montélimar ; La Mouette, à Fontaine-lès-Dijon fondée par les frères Thévenot à la fin des années 1970 ; Air Création, à Aubenas (7)

Dans celui du multi-axes, l'une des plus importantes a été Aviasud Engineering, à Fréjus, aujourd'hui disparue. Mais on a également : USM-Aéro, à Senonches (28), avec ses ULM à structure en aluminium ; Aéro services Guépard à Toulonjac (12) ; ESPACE à Saint-André-des-Eaux (44), fabricant du Sensation ; Best Off Aircraft, fondée en 1999 à Montauban (82), constructeur du Skyranger, etc. Il existe également des entreprises semi-industrielles, comme la société Ptitavion Lille Métropole, fabriquant du Ptitavion.

Dans le matériel, on peut citer la société Duc Hélices (Lentilly, 69), qui fabrique des hélices en fibres de verre pour l'aviation légère et Hélices E-Props, les hélices en carbone d'Electravia (aérodrome de Sisteron 4).

Définition de l'arrêté royal du modifiant l'arrêté royal du fixant les conditions particulières imposées pour l'admission à la circulation aérienne des aéronefs ultra-légers motorisés.

On désigne un ULM ou aéronef ultra-léger motorisé, un avion ou amphibie de type monoplace ou biplace dont la vitesse de décrochage Vso (configuration d'atterrissage, moteur au ralenti) n'excède pas CAS, et dont la masse maximale autorisée au décollage (MTOW) ne dépasse pas :

Ne sont pas compris dans cette définition les aéronefs à voilure rotative et les aéronefs à décollage à pieds.

Sous-classe ULM/DPM :

Selon Transports Canada, un avion ultraléger de base est défini comme étant :

Un avion ultraléger de type évolué s'entend d'un avion dont la définition de type est conforme aux normes précisées dans le manuel intitulé « Normes de conception pour avions ultralégers de type évolué ». Un avion ultraléger de type évolué ne comprend ni les aéronefs pour parachute entraînés par moteur, ni les ultralégers pendulaires à train tricycle, ni les planeurs propulsés.

Au Canada, les ultralégers sont répartis en plusieurs catégories. Les parachutes motorisés, les paramoteurs, les pendulaires et les 3 axes. Dans la catégorie , les aéronefs peuvent être classés en deux autres catégories soit les ultralégers de base et les ultralégers évolués. 

Un ultraléger de base se définit comme un appareil monomoteur pour lequel aucun certificat de navigabilité n'est émis. De plus, on ne peut emmener de passager à son bord. L'ultraléger évolué se définit comme étant un appareil monomoteur de 1 ou dont le poids brut maximum n'excède pas (soit ).

La firme canadienne Fisher commercialise de nombreux kits, comme le Fisher FP-404.

Les États-Unis ont deux catégories d'aéronefs légers : l' et le LSA ().

La catégorie comprend les appareils répondant aux conditions suivantes et autorisés à voler au-dessus des zones non peuplées durant le jour aéronautique :

La loi n'impose aucune licence ou formation. Une formation est cependant fortement recommandée pour la sécurité.

La catégorie LSA comprend les appareils répondant aux conditions suivantes :

L'Office fédéral de l'aviation civile (OFAC) réglemente les vols. Toutes les catégories d'ULM sauf les " sont interdites de vol depuis le pour des raisons « écologiques » et de pollution sonore, .

Toutefois, des autorisations peuvent être accordées pour des essais d'aéronefs dans un rayon de autour de la zone de décollage.

Depuis le , une catégorie d'ULM, les ", peuvent être immatriculés et voler. Ceci ne concerne pour l'instant que les multi-axes, pour autant qu'ils correspondent à la certification anglaise, allemande, ou JAR, depuis le .

Contrairement à l'aviation classique pour laquelle la sécurité s'appuie en grande partie sur la certification des matériels et la surveillance des pilotes, la sécurité de l'aviation ULM repose d'une part sur la responsabilité des pilotes (comme pour toutes les actés de circulation aérienne) et sur la responsabilité du fabricant qui est seul garant du dossier technique déclaratif déposé auprès de l'administration. L'avantage de cette absence de contrôle a priori par l'administration, permet d'effectuer des modifications rapidement et à moindre coût. Mais le pilote ULM garantit seul le respect des consignes d'utilisation et d'entretien de l'appareil et, s'il effectue des modifications ou changements de pièces considérées comme essentielles par le fabricant, il est responsable de celles-ci.

Dans la plupart des pays européens y compris en France la compétence du pilote est attestée par un brevet de pilote. Pour pouvoir emporter un passager le pilote devra passer une qualification « Emport d'un passager ». Dans certains pays, comme les États-Unis, aucun brevet n'est nécessaire, mais la pratique de l'ultra-léger y est extrêmement restrictive (monoplace de à vide et très faibles performances uniquement). Cette liberté particulière est un des principaux avantages de l'aviation ULM qui a dépassé à elle seule le nombre de pilotes brevetés de toutes les autres catégories de pilotes de l’aviation légère. Pour autant, d'un point de vue statistique, la pratique de l'ULM ne s'avère pas plus dangereuse que l'aviation légère certifiée.

Enfin, le principal intérêt de l’ULM en France réside dans le fait qu’il n’est pas obligé d’utiliser uniquement les aérodromes, mais peut aussi se « poser en campagne » et utiliser des plates-formes permanentes (arrêté préfectoral) ou occasionnelles (accord du propriétaire et information préalable du maire).

Un inconvénient : beaucoup d'aérodromes sont interdits d'accès aux ULM pour des raisons de bruit et sécurité, ce qui complique considérablement la pratique de l'ULM dans les régions où les pistes ULM spécifiques ou avion avec autorisation d'accès sont rares.


Le niveau de sécurité des ULM est aujourd'hui comparable à celui des autres catégories d'aviation de loisir, avec pour de vol.




</doc>
<doc id="3092" url="https://fr.wikipedia.org/wiki?curid=3092" title="Unité de mesure en informatique">
Unité de mesure en informatique

Les unités de mesure suivantes sont utilisées en informatique pour quantifier la taille de la mémoire d'un dispositif numérique (ordinateur, Baladeur numérique), l'espace utilisable sur un disque dur, une clé USB, la taille d'un fichier, d'un répertoire ou autre.

On peut utiliser avec la plupart de ces unités :

Les unités suivantes ont des tailles variables suivant l'architecture, voire suivant le programme sur une même architecture :


"Ces unités sont très souvent utilisées dans les jeux-vidéo, afin de connaître précisément la fluidité de l'animation."


Le pixel est la plus petite unité adressable sur l'écran (enfin, presque, compte tenu de l'astuce de la technique ClearType).

La définition d'écran est le produit du nombre de points selon l’horizontale par le nombre de points selon la verticale de l’affichage. Par exemple, signifie de large sur de haut.

La résolution est un rapport entre la définition d’écran et sa taille physique, est exprimé en général en pixels par pouce.

Le Larousse inverse la définition des termes définitions et résolution, la du dictionnaire de l’Académie française indique que définition est le nombre de ligne d’un format de télévision.

Note : certains constructeurs font varier les caractéristiques de leurs écrans, notamment la dimension verticale des écrans 16/9 est souvent légèrement agrandie au format 16/10 afin de permettre l'affichage d'une barre d'outil ou de navigation sous l'image. Ils omettent souvent de préciser les extensions après les lettres "GA", et confondent certaines nomenclatures (en ne précisant que le sigle sur leurs spécifications, mais pas la définition d'affichage effective...).

Ces unités de mesure servent à mesurer un débit (maximal) d'information entre deux points d'un réseau informatique.
Ces unités sont utilisées pour les modems, l'ADSL, les ports série, les , et les cartes réseaux.

Lors de l'accès à un disque (CD-ROM, DVD, CD-RW, disque dur, disquette, clé USB), des unités différentes sont peut-être utilisés.

La vitesse de traitement de la partie virgule flottante, dite FPU (Floating Point Unit), d'un processeur est exprimée en opérations par seconde, les FLOPS (Floating Point Operations Per Second).

En raison de la taille du marché anglo-saxon et du dynamisme de la recherche et développement en informatique aux États-Unis, les dimensions sont fréquemment exprimées en unités de mesure anglo-saxonnes, et notamment en pouces, bien que ceci soit contraire à la norme internationale. La loi française (article R.643-2 du Code pénal) oblige les vendeurs à indiquer l'équivalent en mètre (ou par exemple en centimètre), afin de garantir une information juste du consommateur, client (possibilité de comparaison).

Le tableau ci-contre donne quelques exemples d'équivalences. Curieusement les disquettes de « trois pouces et demi » sont en fait spécifiées comme faisant (la norme est d'origine japonaise) et mesurent en réalité de plus que 3 ½.

En revanche, le millimètre est universellement adopté pour les épaisseurs de disques durs (, )


</doc>
<doc id="3093" url="https://fr.wikipedia.org/wiki?curid=3093" title="USA (homonymie)">
USA (homonymie)

Cette page recense les différentes significations (mots, codes, sigles, acronymes, abréviations, etc.) formés avec la combinaison de lettres USA.

USA est l'acronyme de :

codice_1 est un code signifiant :

Usa est un toponyme ou une entité géographique ou administrative pouvant désigner :



</doc>
<doc id="3094" url="https://fr.wikipedia.org/wiki?curid=3094" title="Utopie">
Utopie

L’utopie (mot forgé par l'écrivain anglais Thomas More, du grec « en aucun lieu ») est une représentation d'une société idéale sans défaut contrairement à la réalité. C'est un genre d'apologue qui se traduit, dans les écrits, par un régime politique idéal (qui gouvernerait parfaitement les Hommes), une société parfaite (sans injustice par exemple, comme la Callipolis de Platon ou la découverte de l'Eldorado dans "Candide") ou encore une communauté d'individus vivant heureux et en harmonie (l'abbaye de Thélème dans "Gargantua" de Rabelais en 1534), souvent écrites pour dénoncer les injustices et dérives de leurs temps.

Les utopistes situent généralement leurs écrits dans des lieux imaginaires pour éviter la censure politique ou religieuse : un pays lointain et mythique ("Les Aventures de Télémaque, Livre 7", Fénelon, 1699), île inconnue par exemple ("L'Île des esclaves", Marivaux, 1725).

Une utopie peut désigner également une réalité difficilement admissible : en ce sens, qualifier quelque chose d'utopique consiste à le disqualifier et à le considérer comme irrationnel. Cette polysémie, qui fait varier la définition du terme entre texte littéraire à vocation politique et rêve irréalisable, atteste de la lutte entre deux croyances, l'une en la possibilité de réfléchir sur le réel par la représentation fictionnelle, l'autre sur la dissociation radicale du rêve et de l'acte, de l'idéal et du réel.

Genre opposé, la dystopie — ou contre-utopie — présente non pas « le meilleur des mondes » mais « une utopie en sens contraire », selon F. Rouvillois.

Le terme "utopia" est un néologisme grec forgé par Thomas More en 1516 pour désigner la société idéale qu'il décrit dans son œuvre (en latin) "Utopia". Il est traduit en français par « utopie ».

Ce terme est composé de la préposition négative grecque "ou" et du mot "topos" qui signifie « lieu ». Le sens d'« utopie » est donc, approximativement, « sans lieu », « qui ne se trouve nulle part ». Dans l'en-tête de l'édition de Bâle de 1518 d’"Utopia", Thomas More utilise, exceptionnellement, le terme d’"Eutopia" pour désigner le lieu imaginaire qu'il a conçu. Ce second néologisme ne repose plus sur la négation "ou" mais sur le préfixe "eu", que l'on retrouve dans "euphorie" et qui signifie « bon ». "Eutopie" signifie donc « le lieu du Bon ».

Seul le premier de ces deux termes est passé à la postérité, mais ils n'en sont pas moins complémentaires pour décrire l'originalité de l’"Utopia" de More. En effet, cette œuvre est d'une part un récit de voyage et la description d'un lieu fictif ("utopia") et d'autre part un projet d'établissement rationnel d'une société idéale ("eutopia"). Ces deux aspects du texte de Thomas More ont amené à qualifier d'utopie des œuvres très différentes.

L'utopie ("utopia") est la description d'une société idéale. Elle procède d'une tradition que l'on fait remonter à "La République" de Platon. Plus spécifiquement l'utopie ("utopia") est un genre littéraire s'apparentant au récit de voyage mais ayant pour cadre des sociétés imaginaires.

Ces deux définitions ne s'excluent pas : l’"Utopie" de Thomas More, "La Cité du Soleil" de Tommaso Campanella ou "La Nouvelle Atlantide" de Francis Bacon remplissent ces deux conditions et sont à la fois des récits et des descriptions de sociétés originales.

Cependant, dès le , de nombreux auteurs s'emparent de ce nouveau genre littéraire et en développeront l'aspect romanesque et satirique au détriment du projet politique. C'est ainsi que des œuvres telles que "Les voyages de Gulliver" (1721) de Jonathan Swift furent qualifiées en leur temps d'utopies.

Thomas More inventa le genre littéraire de l'utopie, il avait l'ambition d'élargir le champ du possible et non de l'impossible comme ce mot est synonyme aujourd'hui.

Dans son essai consacré aux premières utopies, celles d’avant les récits de More, de Tommaso Campanella ou de Cabet ("Les Premières Utopies", rééditions ex nihilo, 2009, paru d'abord en 1938), Régis Messac donne une définition restrictive du terme Utopie : .

Régis Messac considère l’utopie comme une œuvre purement romanesque, nécessairement progressiste, constituée de deux éléments : . Cependant, l’un ne va pas sans l’autre, mais . Pour Messac, il va sans dire que ne peuvent être considérées comme de véritables utopies les œuvres où domine le second élément, le contenu, c'est-à-dire la représentation d’une société parfaite ou du moins perfectionnée.

C'est pourquoi Messac ne reconnaît ni "la République" de Platon ni la "Cyropédie" de Xénophon comme appartenant exactement au genre utopique ; il considère ces œuvres comme relevant de la catégorie des traités de politique pareils à ceux de Félix Bodin, Nicolas Machiavel et Montesquieu. Il dit : .

Selon Régis Messac, les récits utopiques répondent à un besoin social. Il écrit : .

Le philosophe français Michel Foucault définit l'utopie comme un qui entretient un rapport analogique avec la réalité et qui tend soit vers l'envers de la société, soit vers le perfectionnement de cette dernière. Elle s'oppose, dans sa théorie, à l'hétérotopie.

Platon est le premier grand idéaliste de la pensée occidentale. On peut en effet rapprocher l'utopie (au sens moderne que prit ce mot) du concept d'idée de Platon.

La pensée de Platon est exposée dans l’ouvrage classique "La République", dont le titre même est un programme. Par République, Platon entend Politeia, c’est-à-dire État, Constitution. Platon voulut donc tracer les grandes lignes de ce que devait être une cité organisée de manière idéale par castes. C’est cette volonté de constituer une cité idéale, faisant de Platon le grand fondateur du concept d'idée, qui fut reprise ultérieurement par les utopistes du (notamment Fourier, Saint-Simon et Étienne Cabet).

Même si Platon a réfléchi aux questions économiques, sa pensée ne fut pas aussi aboutie sur ce thème que celle de l'école de son successeur, Aristote, auquel on attribue un ouvrage consacré à l'économie : "Économiques".

L’"Histoire vraie" de Lucien de Samosate est, comme la "Batrachomyomachie", avant tout rattachée au genre de la fantaisie imaginaire. Cependant, elle présente aussi, dans son voyage aux confins de l'univers fantasmé de l'époque (), bien des caractéristiques littéraires et thématiques de l'utopie: ainsi, l'utopie, en littérature, n'offre pas seulement la lecture d'un archétype parfait d'organisation, elle propose aussi, parfois au travers du prisme de la fantaisie, une grille interprétative critique des structures institutionnelles, politiques et sociales du modèle culturel de régime dans lequel évolue l'auteur de l'utopie ; celle-ci, loin de se présenter systématiquement comme une œuvre indépendante, peut advenir selon une mise en abîme discursive (cas d'une narration, enchâssée dans une autre trame fictionnelle dont les ressorts seraient historiques, politiques ou philosophiques) introduisant à un dialogue complexe entre les textes ainsi imbriqués l'un dans l'autre. Ainsi en est-il du voyage de Lucien dans le royaume d'Endymion sur la lune ou encore dans l'Île des Bienheureux où Rhadamante l'accueille durant des mois.

L'avocat et homme de lettres Thomas More s'inscrit, à l'instar de son ami Érasme, dans le cadre du mouvement humaniste qui redécouvre la littérature antique grecque et latine et s'en inspire. More connaissait les œuvres d'Aristote et de Platon et le projet de cité idéale qui occupe une partie de La République peut être considéré comme l'une des sources d'inspiration de l’"Utopie".

Le texte de More, paru en 1516, emprunte en partie sa forme aux récits de voyage de Vasco de Gama ou de Magellan. La découverte du Nouveau monde en 1492 a mis les Européens en contact avec d'autres peuples, et permet à More d'imaginer une civilisation originale située aux confins du monde connu. De manière plus générale, son projet de société s'inscrit dans le courant philosophique de la Renaissance.

Le premier livre de "L'Utopie" rapporte une conversation entre le narrateur et plusieurs autres personnages, dont Raphaël Hythlodée un navigateur qui a découvert l'île d'Utopie. La discussion porte principalement sur les injustices et les défauts de la société, injustices auxquelles Raphaël Hythlodée oppose les sages coutumes du pays dont il a fait la découverte. Le second livre rapporte la description par Hythlodée de l'Utopie. Cette description, assez détaillée, porte sur les lois, les coutumes, l'histoire, l'architecture et le fonctionnement économique de l'île.

La société utopienne est foncièrement égalitaire et ignore toute propriété privée. Elle décrit une société qu'on a souvent qualifiée de communiste, ou plus précisément d'« isonome », cherchant l'égalité parfaite de tous devant la loi. Elle repose en outre sur un ensemble de lois et sur une organisation très rationnelle et précise. Elle est présentée comme la plus aboutie des civilisations.

Cette œuvre s'entend avant tout comme une critique de la société anglaise (et européenne) du . Les vertus de l'Utopie sont en quelque sorte des réponses aux injustices du monde réel : elles les soulignent par contraste (l'égalité de tous les citoyens utopiens met en lumière l'extrême misère, à cette époque, de nombre de paysans anglais sans terres) et montrent que les maux de l'Angleterre ne sont peut-être pas des fatalités puisque les Utopiens les ont résolus. L'Utopie, qui se présente comme une œuvre de fiction, affirme néanmoins que l'homme a la possibilité d'influer sur son destin et est donc porteuse du concept d'histoire. More s'abstient pourtant de présenter son utopie comme un programme politique. Il considère la réalisation d'une telle société comme souhaitable mais affirme ne pas même l'espérer.

Ainsi, le genre littéraire créé par Thomas More repose sur un paradoxe. Il se présente en effet comme une œuvre de fiction sans lien avec la réalité : le nom de l'île (« nulle part ») mais aussi du fleuve qui la traverse ("Anhydre", c'est-à-dire sans eau) ou du navigateur Hythlodee (qui signifie : habile à raconter des histoires) sont là pour le rappeler. Cependant, l'utopiste se refuse à tout recours au merveilleux ou à la fantaisie et le bonheur qui est censé régner en Utopie repose seulement sur la cohérence du projet. Nul climat paradisiaque, nulle bénédiction divine, nul pouvoir magique n'a contribué à la réalisation de la société parfaite. Il s'agit donc d'une fiction dont la valeur repose sur la cohérence du discours.











L'utopie se caractérise par un recours à la fiction, par un artifice cinématographique qui consiste à décrire une société idéale dans une géographie imaginaire, souvent dans le cadre d'un récit de voyage. L'utopie a pour condition première la mise à distance du monde réel à partir de laquelle la réflexion critique peut se déployer et le sujet se constituer. Le recours à la fiction est un procédé qui permet de prendre ses distances par rapport au présent pour mieux le relativiser et de décrire ce qui pourrait être. Le genre utopique permet de percevoir, au lieu d'attendre un monde meilleur dans un au-delà providentiel, que les hommes devraient construire autrement leurs formes d'organisation politique et sociale pour venir à bout des vices, des guerres et des misères. En ce sens, les descriptions qu'ils proposent, dans lesquelles ils font voir des cités heureuses bien gouvernées, visent à convaincre leurs spectateurs que d'autres modes de vie sont possibles.




L'interprétation de la portée politique de l'utopie pour la pensée et l'action politique n'est pas consensuelle, notamment parce que l'utopie intervient directement ou bien comme contestation ou bien comme justification idéologique dans la sphère politique. On peut cependant dégager certaines thèses à son sujet qui font autorité pour certaines traditions philosophique.

Selon Miguel Abensour, l'utopie, particulièrement dans sa forme classique, est une simple stratégie d'écriture politique permettant au philosophe de critiquer la société de son époque. En parlant uniquement d'une société fictive, déclarée comme telle, l'auteur de l'utopie évite de critiquer les pouvoirs en place sans le faire directement. Dans "L'Utopie de Thomas More à Walter Benjamin," Abensour montre comment More utilise cette stratégie pour critiquer le pouvoir absolu d'Henri.

Les marxistes orthodoxes, et Karl Marx le premier, ont utilisé le terme « utopie » de manière péjorative. Cela venait en partie d'une volonté de Marx de distinguer sa propre théorie de celles des autres socialistes de son époque (notamment Pierre-Joseph Proudhon), avec qui il avait entretenu des controverses. Il nommait d'ailleurs Proudhon, Charles Fourier, Saint Simon et d'autres « socialistes utopiques ». En ce sens, « utopie » signifie la représentation imaginaire d'un régime politique idéal et idéaliste, c'est-à-dire détaché de toute considération et compréhension des circonstances matérielles qui pourrait présider à son avènement. Bref, pour Marx, le terme « utopique » est une accusation de frivolité. Ici, le sens du terme « utopie » est très proche « d'idéologie ».

Karl Mannheim, un marxiste non-orthodoxe et Paul Ricœur proposent quant à eux de comprendre l'utopie en opposition à l'idéologie. Selon eux, l'utopie est une force de changement : « elle propose une rupture radicale avec un système existant » et ainsi tente de briser la suprématie de l'actuel sur le possible. À cela s'oppose l'idéologie, une force conservatrice ramenant plutôt la société vers l'existant.

L'utopie est souvent entendue, dans un sens large, comme une réponse à la question du meilleur régime, une question de philosophique politique particulièrement favorisée par les anciens. C'est dans ce sens que nous pouvons dire que la République de Platon est une utopie avant la lettre (le terme ne datant que de 1516 apr. J.-C.).

Ces interprétations ne sont ni entièrement contraires, ni parfaitement réconciliables. Par ailleurs, elles n'épuisent pas entièrement le sujet. D'autre théoriciens du politique, plus contemporains, ont donné une extension beaucoup plus large au concept d'utopie, lui donnant un rôle dans la dialectique historique. Notamment, Walter Benjamin et Ernst Bloch.

Notons également que l'utopie a été largement critiquée en philosophie politique dans la seconde moitié du : pour son attachement à l'idée de progrès héritée des Lumières (par les Catastrophistes et Hans Jonas par exemple) et pour les dangers d'exploitation totalitaire qu'elle représente (par l'École de Francfort notamment).

L'expression d'utopie concrète n'est qu'en apparence un oxymore. Dans "L’Age du faire" (Seuil, 2015) le sociologue Michel Lallement l'utilise pour qualifier les espaces où les hackeurs imaginent des formes de travail qui bousculent les règles de l’économie de marché. Elle a été inventée par le philosophe allemand Ernst Bloch (1885-1977) : dans son livre "Le Principe espérance" (Gallimard, 1976) publié en République démocratique allemande dans les années 1950, il affirme que les utopies concrètes permettent de déceler dans le réel, "« l'anticipation réaliste de ce qui est bien »".







</doc>
<doc id="3095" url="https://fr.wikipedia.org/wiki?curid=3095" title="Humbert Ier (roi d'Italie)">
Humbert Ier (roi d'Italie)

Humbert (en italien "Umberto I"), né le 14 mars 1844, mort le 29 juillet 1900, de la maison de Savoie, est roi d'Italie du 9 janvier 1878 jusqu'à sa mort. Il est le fils de Victor-Emmanuel II et d'Adélaïde de Habsbourg-Lorraine.

Humbert d'Italie perd sa mère à l'âge de onze ans. Peu après, son père devient roi d'Italie (1861). Les cours d'Europe sont réticentes à donner leurs filles en mariage à une maison qu'elles considèrent comme traîtresse et parvenue depuis qu'elle a chassé les autres souverains de leurs domaines italiens.

Victor-Emmanuel II essaye de se réconcilier avec l'Autriche et il est vaguement question de marier Humbert à la fille de l'Archiduc Albert Frédéric, Mathilde, mais celle-ci meurt accidentellement en 1867.

Humbert épouse alors à Turin le 22 avril 1868 sa cousine germaine Marguerite de Savoie (1851-1926), fille de Ferdinand de Savoie, , et d'Élisabeth de Saxe.

Ils n'ont qu'un fils, Victor-Emmanuel (1869-1947).

Humbert décide de se faire appeler Humbert d'Italie et non Humbert de Savoie, marquant par là l'unification de toute l'Italie sous son nom. Le 17 novembre 1878, le roi, en visite à Naples avec sa femme et le Premier ministre Benedetto Cairoli, est blessé d’un coup de poignard porté par l'anarchiste Giovanni Passannante. Dans l'attaque, Cairoli est aussi blessé.

Il joue son rôle de souverain constitutionnel alors que la gauche italienne occupe le pouvoir, avec Agostino Depretis et Francesco Crispi. Il récompense le général , responsable du massacre de Milan. Il meurt assassiné de trois coups de revolver, à Monza par l'anarchiste Gaetano Bresci qui voulait ainsi venger la sanglante répression menée contre les ouvriers à Milan en 1898.

Sa dépouille est enterrée au Panthéon.



</doc>
<doc id="3096" url="https://fr.wikipedia.org/wiki?curid=3096" title="Uranus (planète)">
Uranus (planète)

Uranus est une planète géante de glaces de type Neptune froid. Il s'agit de la planète du Système solaire par sa distance au Soleil, de la par la taille et de la par la masse. Elle doit son nom à la divinité romaine du ciel, Uranus, père de Saturne et grand-père de Jupiter. Uranus est la première planète découverte à l’époque moderne. Bien qu'elle soit visible à l’œil nu comme les cinq planètes déjà connues, son caractère planétaire ne fut pas identifié en raison de son très faible éclat (à la limite de la visibilité) et de son déplacement apparent très lent. William Herschel annonce sa découverte le , élargissant les frontières connues du Système solaire pour la première fois à l’époque moderne. Uranus est la première planète découverte à l’aide d’un télescope.

Uranus et Neptune ont des compositions internes et atmosphériques différentes de celles des deux plus grandes géantes gazeuses, Jupiter et Saturne. Les astronomes les placent donc de nos jours généralement dans une catégorie différente, celle des géantes glacées ou des sous-géantes. L’atmosphère d’Uranus, bien que composée principalement d’hydrogène et d’hélium, contient une proportion plus importante de glaces d’eau, d’ammoniac et de méthane, ainsi que les traces habituelles d’hydrocarbures. Uranus est la planète du Système solaire dont l’atmosphère est la plus froide, sa température minimale étant de (), à la tropopause (vers d'altitude et , le niveau zéro étant défini à une pression d'un bar).

À l’instar des autres géantes gazeuses, Uranus a un système d’anneaux, une magnétosphère et de nombreux satellites naturels. Il y a 27 satellites et 13 anneaux étroits. Le système uranien est unique dans le Système solaire car son axe de rotation est pratiquement dans son plan de révolution autour du Soleil ; les pôles nord et sud sont situés où les autres planètes ont leur équateur. En 1986, les images de ont montré Uranus comme une planète sans caractéristique particulière en lumière visible. Cette visite de la sonde se produisit près du solstice, l'hémisphère éclairé était alors principalement son hémisphère austral. En 2017, au printemps boréal d'Uranus, le télescope Keck II montre des bandes nuageuses en infra-rouge. On y remarque des mouvements de nuages, des vents à 900 km/h, d'énormes ouragans et des ondulations étranges en forme de tresse cerclant la planète.

Cependant, les observateurs terrestres ainsi que le télescope spatial "Hubble" ont depuis constaté des signes de changements saisonniers et une augmentation de l’activité météorologique lorsqu'Uranus a approché de son équinoxe, atteint le .

Contrairement à Mercure, Vénus, Mars, Jupiter et Saturne, Uranus ne fut pas découverte dans l'Antiquité. Étant loin du Soleil et circulant lentement sur son orbite, Uranus fut observée à de nombreuses occasions et apparaissait comme une simple étoile jusqu'au en raison de son très faible éclat, à la limite de la visibilité et de son déplacement apparent très lent. La plus ancienne mention prouvée date de 1690 lorsque John Flamsteed l’observe au moins six fois et la catalogue en tant qu'étoile sous le nom de . L’astronome français Pierre Charles Le Monnier observe Uranus au moins douze fois entre 1750 et 1769.

Uranus a peut-être été observée par Hipparque en 128 En effet, un astérisme cité dans l’"Almageste" de Claude Ptolémée, reprenant les travaux d'Hipparque, ne peut être résolu que par la présence d'Uranus à cette époque. Uranus à mi-avril 128 était dans des conditions d'observation très favorables : proche de son périhélie, magnitude de 5,4, à 33° du zénith.

John Bevis a peut-être également observé Uranus en 1738, des indices concordent avec une observation possible d'Uranus mais sans preuve définitive.

William Herschel découvre la planète le lors d’une recherche systématique d’étoiles doubles à l’aide d’un télescope dans le jardin de sa maison du 19 à Bath dans le Somerset en Angleterre (désormais le musée d'astronomie Herschel) mais n’annonce la découverte que le , en tant que comète. Herschel avait entrepris une série de mesures de la parallaxe des étoiles fixes en utilisant un télescope de sa conception. À la frontière des constellations des Gémeaux et du Taureau, Herschel remarque au milieu des points-étoiles une petite tache semblant sortir de derrière la planète Saturne. Il change alors successivement d’oculaire, passant du grossissement 227 à 460. Il note alors que la petite tache double de taille. Il change à nouveau d’oculaire pour un grossissement de 932, et , et là encore, l’objet augmente de taille à chaque fois, tandis que les étoiles tout autour, très éloignées, ne varient pas en taille et restent de simples points brillants. Cela ne peut être une étoile ; il écrit donc dans son journal l’observation d’un curieux objet, une nébuleuse ou une comète. Il note la position de l’astre, puis quelques jours après reprend son observation. La petite tache avait bougé, ça ne pouvait être une nébuleuse, donc c’était une comète. Il décide alors de prévenir la communauté scientifique de sa découverte et envoie un courrier avec les détails de sa comète au directeur de l’observatoire d’Oxford, Thomas Hornsby. Il informe également l’astronome royal Nevil Maskelyne de l’observatoire de Greenwich. Celui-ci, après avoir observé la comète et constaté qu’elle se comportait différemment des autres, conseille à Herschel d’écrire à la " mais n’annonce la découverte que le , en tant que comète.

Herschel avertit l’astronome royal, Nevil Maskelyne, de sa découverte. Ce dernier ne peut trancher entre l'hypothèse d'une planète et celle d'une comète, il répand la nouvelle à travers les milieux scientifiques.

Tandis qu’Herschel continue par précaution à appeler ce nouvel objet une comète, d’autres astronomes soupçonnent sa véritable nature. L’astronome russe Anders Lexell estime sa distance à dix-huit fois la distance Terre-Soleil. Aucune comète ayant un périhélie supérieur à quatre fois la distance Terre-Soleil n’a alors jamais été observée. L’astronome berlinois Johann Elert Bode pense que la découverte d’Herschel est un type d’objet planétaire, jusqu’alors inconnu, orbitant au-delà de l’orbite de Saturne. Bode conclut que son orbite presque circulaire ressemble davantage à celle d’une planète que d’une comète.

Les astronomes commencent alors le calcul de la trajectoire de la « comète », en prenant le modèle classique des orbites de comètes : une parabole, mais celle-ci ne semblait pas vouloir se conformer au modèle prévu. L'astronome français Charles Messier remarque alors qu’avec son aspect de disque, elle ressemblait plus à Jupiter qu’aux dix-huit autres comètes qu’il avait observées. Anders Lexell tente, lui, de calculer l’orbite en appliquant le modèle d’une planète. À sa grande surprise, cette trajectoire semble correspondre, et prouve aux autres astronomes la nature de l’objet : une planète et non une comète.

L’objet est bientôt unanimement accepté en tant que planète. En 1783, Herschel lui-même le reconnaît auprès du président de la ", Joseph Banks. Le roi récompense Herschel de sa découverte en lui attribuant une rente annuelle de , à condition qu’il s’installe à Windsor, afin que la famille royale puisse regarder à travers ses télescopes.

Maskelyne demande à Herschel de donner un nom à cette nouvelle planète, étant le découvreur de cette dernière. En réponse, Herschel décide de nommer l'objet « » (étoile de George), ou « » (planète Géorgienne) en l'honneur de son nouveau mécène, le roi . Il explique cette décision dans une lettre à Joseph Banks en déclarant que dans l'Antiquité, les planètes ont été nommées d'après les noms des divinités principales. Dans l'ère actuelle, il ne serait guère admissible d'avoir recours à la même méthode pour nommer le nouveau corps céleste. Pour lui, l'important pour le désigner est de savoir quand il a été découvert, et sa réponse fut .

Cependant, le nom proposé par Herschel n'est pas populaire en dehors de la Grande-Bretagne, et des solutions alternatives pour un nom sont rapidement proposées. L'astronome français Lalande propose de nommer la planète Herschel en l'honneur de son découvreur. L'astronome suédois Erik Prosperin propose le nom de Neptune, qui sera soutenu par d'autres astronomes qui ont aimé l'idée de commémorer les victoires de la flotte britannique ' dans le cadre de la guerre d'indépendance des États-Unis en appelant même la nouvelle planète « Neptune » ou « ». Johann Elert Bode a opté pour Uranus, la version latinisée du dieu grec du ciel, Ouranos. Bode a fait valoir que, tout comme Saturne était le père de Jupiter, la nouvelle planète devrait être nommée d'après le père de Saturne. En 1789, Martin Klaproth, qui sera plus tard le collègue de Bode à la ', a nommé son élément nouvellement découvert « uranium » avec l'appui du choix de Bode. En fin de compte, la suggestion d'Uranus devient la plus largement utilisée, et est devenue universelle en 1850, lorsque le "", le dernier obstacle, délaisse pour Uranus.

Au et , il est très difficile d'observer correctement la surface d'Uranus. La seule découverte de l'époque est celle de Giovanni Schiaparelli , qui distingue malaisément quelques taches. Par spectroscopie et photométrie, les scientifiques, avant 1937, ont pu fixer à la rotation de la planète, qui était vue comme rétrograde.

En 1948, Gerard Kuiper découvrit Miranda, le plus petit et le dernier des cinq grands satellites sphériques d'Uranus, à l'observatoire McDonald.

Le , les anneaux d’Uranus sont découverts, par hasard, par les astronomes James L. Elliot, Edward W. Dunham et Douglas J. Mink, embarqués à bord de l'observatoire aéroporté Kuiper. Les astronomes veulent utiliser l’occultation de l’étoile par Uranus pour étudier l’atmosphère de cette planète. Or l’analyse de leurs observations met en évidence que l'étoile a été brièvement masquée à cinq reprises, avant et après l’occultation par Uranus ; les trois astronomes concluent à la présence d’un système d’anneaux étroits. Dans leurs articles, ils désignent les cinq occultations observées par les cinq premières lettres de l'alphabet grec : α, β, γ, δ et ε ; ces désignations sont réutilisées par la suite pour nommer les anneaux. Peu de temps après, Elliot, Dunham et Mink découvrent quatre autres anneaux : l'un d'eux est situé entre les anneaux β et γ et les trois autres à l’intérieur de l’anneau α. Le premier est nommé η et les autres 4, 5 et 6, selon le système de numérotation des occultations adopté lors de la rédaction d'un autre article. Le système d’anneaux d’Uranus est le second découvert dans le Système solaire, après celui de Saturne.

La planète géante de glace Uranus a un axe de rotation fortement incliné pratiquement situé dans son plan de révolution autour du Soleil. La recherche d’indices pouvant expliquer cette particularité unique dans le Système solaire est un des objectifs assignés à la sonde , qui est la première sonde à effectuer un survol de la planète. met en évidence la présence d’un champ magnétique, dont l’intensité est proche de celui de la Terre, et qui est incliné de 60° par rapport à l’axe de rotation de la planète.

Les neuf anneaux d’Uranus, découverts en 1977 et 1978 depuis la Terre, sont analysés par la sonde, et montrent des caractéristiques différentes de ceux de Saturne et Jupiter. Ces analyses permirent de mettre en évidence qu'ils ne se sont pas formés en même temps qu’Uranus, et que leur apparition est relativement récente. Les composants qui les forment sont peut-être les restes d’une lune qui aurait été fragmentée, soit par un impact avec un autre objet céleste se déplaçant à très grande vitesse, soit par les forces gravitationnelles de la planète mère.

Depuis 1997, neuf satellites irréguliers extérieurs ont été identifiés à l'aide de télescopes au sol. Deux lunes intérieures supplémentaires, et Mab, ont été découvertes grâce au télescope spatial "Hubble" en 2003. Le satellite Margaret est le dernier découvert ; sa découverte fut publiée en .
Le télescope spatial "Hubble" permit de prendre des photos correctes d'Uranus depuis la Terre. Entre 2003 et 2005, grâce aux observations ainsi effectuées, une nouvelle paire d’anneaux a été découverte, baptisée par la suite système d’anneaux externe, ce qui porte le nombre d’anneaux d’Uranus à treize. Ils ont été nommés anneaux μ et ν ("mu" et "nu"). L’anneau μ, le plus externe, se trouve deux fois plus éloigné de la planète que l’anneau brillant η. Ces anneaux externes diffèrent des anneaux internes étroits par de nombreuses caractéristiques : μ et ν sont larges ( et ) et très ténus ; leurs épaisseurs optiques normales maximales sont de et ; leurs épaisseurs optiques équivalentes sont respectivement de et . Leur profil radial de brillance est triangulaire.

Avec une masse de , Uranus est un corps intermédiaire entre la Terre et les géantes gazeuses de grande taille comme Jupiter ou Saturne. Sa masse est environ quatorze fois plus importante que celle de la Terre. Le rayon équatorial de la planète est de , soit environ quatre fois celui de la Terre. Uranus a la particularité d'être plus grande que Neptune ( pour Neptune contre pour Uranus) mais moins massive qu'elle (Neptune possède une masse de ).

Neptune et Uranus sont souvent considérées comme une sous-classe de géantes, appelée « géants de glace », en raison de leur taille plus petite et d'une plus forte concentration de substances volatiles par rapport à Jupiter et Saturne.

Avant le passage de "", aucune étude de la magnétosphère uranienne n'avait pu être effectuée, sa nature restant un mystère. Avant 1986, les astronomes s'attendaient à ce que le champ magnétique d'Uranus soit aligné sur le vent solaire, puisqu'il serait alors aligné avec les pôles, qui sont sur le plan de l'écliptique.

Les relevés de "" révélèrent un champ magnétique très particulier, d'une part parce qu'il n'a pas pour origine le centre géométrique de planète, et d'autre part parce qu'il penche de 59° par rapport à l'axe de rotation. En fait, le dipôle magnétique présente même un déséquilibre vers le pôle géographique sud par rapport au centre, équivalent à un tiers du rayon planétaire. Cette géométrie inhabituelle a pour conséquence une magnétosphère fortement asymétrique, la force du champ magnétique à la surface du pôle sud pouvant être aussi basse que (), alors qu'au pôle nord, elle peut atteindre (). Le champ magnétique moyen en surface a une force de (). À titre indicatif, le champ magnétique terrestre est à peu près de force équivalente aux deux pôles et l'« équateur magnétique » est actuellement à peu près parallèle à l'équateur géographique. Le moment magnétique bipolaire d'Uranus est celui de la Terre. Un tel champ magnétique penché et déséquilibré se retrouve également sur Neptune, laissant à penser qu'il s'agit d'une caractéristique commune des géantes glacées. Une des hypothèses à ce sujet est qu'alors que le champ magnétique des planètes telluriques et des géantes gazeuses est engendré par leur noyau, celui des géantes glacées serait provoqué par des mouvements à des profondeurs relativement faibles, ici dans l'océan d'eau et d'ammoniaque.

Malgré son étrange alignement, la magnétosphère uranienne est, par bien des aspects, semblable à celle des autres planètes : on y trouve une surface de choc située à le rayon planétaire devant elle, une magnétopause à uraniens, une magnétoqueue bien développée et des ceintures de radiation. En somme, la structure de la magnétosphère ne ressemble pas tant à celle de Jupiter qu'à celle de Saturne. La magnétoqueue d'Uranus s'étend dans l'espace sur des millions de kilomètres et est courbée en un long tire-bouchon par la rotation de travers de la planète.

La magnétosphère d’Uranus contient des particules chargées : des protons et des électrons avec une petite quantité d’ions. Aucun ion plus lourd n’a été détecté. Bon nombre de ces particules proviennent probablement de la couronne chaude de l’atmosphère. L’ion et les énergies d’électrons peuvent être aussi élevés que , respectivement. La densité des ions de basse énergie (inférieure à ) dans la magnétosphère intérieure est d’environ 2 par cm. La population de particules est fortement affectée par les lunes d’Uranus qui balayent la magnétosphère, laissant des lacunes importantes. Le flux de particules est suffisamment élevé pour provoquer un noircissement ou une altération des surfaces des satellites sur une échelle de temps de . Cela peut être la cause de la coloration uniformément sombre des lunes et des anneaux.

Uranus a des aurores polaires relativement bien développées, qui apparaissent comme des arcs lumineux autour des deux pôles magnétiques. Contrairement à Jupiter, les aurores d’Uranus semblent être insignifiantes pour le bilan énergétique de la thermosphère planétaire. Au contraire des aurores de la Terre ou de Jupiter, elles ne se situent pas à proximité des pôles géographiques de la planète, du fait du champ magnétique penché. Des observations réalisées avec le télescope spatial en 2011, 2012 et 201X ont révélé les aurores les plus importantes observées à ce jour sur la planète et ont permis de retrouver les pôles magnétiques de la planète, perdus peu après l'observation de la planète par "".

La période de révolution d’Uranus autour du Soleil est de terrestres. Sa distance moyenne au Soleil est d’environ de kilomètres. L’intensité du flux solaire sur Uranus est d’environ de celui reçu par la Terre.

Les paramètres orbitaux d’Uranus furent calculés pour la première fois par Pierre-Simon de Laplace en 1783. Avec le temps, des différences apparurent entre l'orbite prédite et l'orbite calculée. En 1841, John Couch Adams émit l’hypothèse qu’une planète inconnue serait la cause des perturbations constatées. En 1845, Urbain Le Verrier commença indépendamment ses travaux afin d’expliquer l’orbite d’Uranus. Le , Johann Gottfried Galle identifia une nouvelle planète (qui sera plus tard nommée Neptune) à une position très proche de celle prédite par Le Verrier.

La période de rotation des couches intérieures d’Uranus est de et . Cependant, la haute atmosphère d’Uranus est le théâtre de vents très violents dans la direction de rotation, comme pour toutes les géantes gazeuses. Le vent à la surface d’Uranus peut atteindre une vitesse de , soit . Par conséquent, à certaines latitudes, par exemple vers 60° de latitude, des parties visibles de son atmosphère se déplacent beaucoup plus vite et effectuent une rotation complète en un peu moins de .

À la différence de toutes les autres planètes du Système solaire, Uranus présente une très forte inclinaison de son axe par rapport à la normale (la perpendiculaire) à son plan orbital : cet axe est quasiment parallèle au plan orbital. La planète « roule » pour ainsi dire sur son orbite, et présente alternativement au Soleil son pôle nord, puis son pôle sud. Chacun des pôles est caché du Soleil durant .

Cette configuration particulière de l'axe de rotation de la planète pourrait s'expliquer par la présence d'un satellite ayant provoqué le basculement par un phénomène de résonance avant d'être éjecté de son orbite . Une autre thèse avance le fait que le basculement serait dû à au moins deux impacts distincts par des corps de tailles importante qui auraient eu lieu avant que les satellites d'Uranus ne se soient formés.

Au moment du survol de la planète par en 1986, le pôle sud d’Uranus était orienté presque directement vers le Soleil. On peut dire qu’Uranus a une inclinaison légèrement supérieure à 90° ou encore que son axe a une inclinaison légèrement inférieure à 90° et qu’elle tourne alors sur elle-même dans le sens rétrograde. Ces deux descriptions sont équivalentes d’un point de vue physique, mais il en résulte une définition différente du pôle nord et du pôle sud.

La composition interne d'Uranus serait similaire à celle de Neptune. Elle possède très probablement un noyau solide de silicates et de fer d'à peu près la masse de la Terre. Au-dessus de ce noyau, là encore à l'instar de Neptune, Uranus présenterait une composition assez uniforme (roches en fusion, glaces, 15 % d'hydrogène et un peu d'hélium) et non pas une structure « en couches » comme Jupiter et Saturne.

Cependant, plusieurs modèles actuels de la structure d'Uranus et Neptune proposent l'existence de : un cœur de type tellurique, une couche médiane allant de glacée à liquide et formée d'eau, de méthane et d'ammoniac, et une atmosphère d'hydrogène et d'hélium dans les proportions solaires.

La pression maximum de la couche médiane est estimée à ( d'atmosphères) et sa température maximum à . En 1981 les études théoriques et les expériences réalisées par compression laser conduisent Marvin Ross, du Laboratoire national de Lawrence Livermore, à proposer que cette couche soit totalement ionisée et que le méthane y soit pyrolysé en carbone sous forme de métal ou de diamant. Le méthane se décompose en carbone et en hydrocarbures. La précipitation du carbone libère de la chaleur (énergie potentielle gravitationnelle convertie en chaleur) qui entraîne des courants de convection qui libèrent les hydrocarbures dans l'atmosphère. Ce modèle expliquerait la présence d'hydrocarbures divers dans l'atmosphère de cette planète. Sous l'action de la pression, le carbone adopte un état plus stable, le diamant solide qui flotte sur un océan de carbone métallique liquide. En 2017 de nouvelles expériences simulant les conditions présumées régner sous la surface d'Uranus et de Neptune viennent conforter ce modèle en produisant des diamants de taille nanométrique. Ces conditions de hautes température et pression ne peuvent pas être maintenues plus d'une nanoseconde, mais dans l'atmosphère de Neptune ou d'Uranus les nanodiamants auraient le temps de croître pour donner des pluies (ou, plus exactement, des averses de neige ou de grêle) de diamants.

L’atmosphère d'Uranus, comme celle de Neptune, est différente des deux géantes gazeuses, Jupiter et Saturne. Bien que principalement composée comme elles d'hydrogène et d'hélium, elle possède une plus grande proportion de gaz volatils tels que l'eau, l'ammoniac et le méthane. Contrairement à Jupiter et Saturne, Uranus ne posséderait pas de manteau d'hydrogène métallique ou d'enveloppe en dessous de sa haute atmosphère. À la place se trouverait une région consistant en un océan composé d'ammoniac, d'eau et de méthane, dont la transition est graduelle sans limite claire avec l'atmosphère dominée par de l'hydrogène et de l'hélium. À cause de ces différences, certains astronomes regroupent Uranus et Neptune dans leur propre catégorie, celle des géantes glacées, pour les distinguer de Jupiter et Saturne.

Bien qu'il n'y ait pas de surface clairement définie sur Uranus, la partie la plus extérieure de l'enveloppe gazeuse d'Uranus est considérée comme son atmosphère, là où la pression est inférieure à . Les effets de l'atmosphère sont ressentis jusqu'à environ en dessous du niveau de , où la pression est de et la température de . La couronne ténue de l'atmosphère s'étend jusqu'à deux fois le rayon de la planète à partir de la surface nominale située au niveau où la pression est de . L'atmosphère uranienne peut être divisée en trois couches : la troposphère, d'une altitude de et d'une pression de ; la stratosphère, d'une altitude de et d'une pression allant de et la thermosphère/couronne commençant vers d'altitude et allant jusqu'à près de de la surface. Il n'y a pas de mésosphère.

Le climat d'Uranus est fortement influencé par son manque de chaleur interne, ce qui limite l'activité atmosphérique, et son inclinaison axiale qui induit des variations saisonnières extrêmes. L'atmosphère d'Uranus paraît remarquablement homogène aux longueurs d'onde visibles en comparaison de celle des autres géantes gazeuses, même par rapport à celle de Neptune qui lui ressemble toutefois beaucoup. En revanche des observations récentes effectuées depuis l'un des observatoires d'Hawai dans le domaine infrarouge ont révélé une météorologie complexe (dont des vents soufflant à 900 km/h) et encore largement inexpliquée.

Quand passa près d'Uranus en 1986, il observa seulement dix formations nuageuses autour de la planète.

Une explication proposée pour ce manque de formations nuageuses est que la chaleur interne d'Uranus se trouve plus en profondeur que celle des autres planètes géantes ; en termes astronomiques, elle a un faible flux de chaleur. Les raisons de la température interne d'Uranus si basse ne sont pas comprises. Neptune, qui est presque la jumelle d'Uranus en ce qui concerne la taille et la composition, émet plus d'énergie dans l'espace qu'elle n'en reçoit du Soleil. Uranus, en opposition, émet à peine de la chaleur. La puissance totale des émissions d'Uranus dans les infrarouges (de la chaleur) est de l'énergie solaire absorbée par l'atmosphère.

En fait, le flux de chaleur d'Uranus est seulement de , ce qui est plus bas que le flux de chaleur interne de la Terre qui est d'environ . La température la plus basse enregistrée dans la tropopause d'Uranus est de (−), faisant d'Uranus la planète la plus froide du Système solaire, plus que Neptune.

Une étude publiée en août 2017 rapporte à la suite d'expérimentations en laboratoire qu'il est possible qu'il pleuve des diamants sur Neptune et Uranus.

Avant l'arrivée de , aucune mesure de la magnétosphère d'Uranus n'avait été prise, et sa nature restait un mystère. Avant 1986, les astronomes avaient espéré que le champ magnétique d'Uranus pourrait être en ligne avec le vent solaire, car il serait alors aligné aux pôles d'Uranus, qui se trouvent dans l'écliptique.

Les observations de ont révélé que le champ magnétique d'Uranus est unique, à la fois parce qu'il ne provient pas de son centre géométrique, et parce qu'il est incliné à 59° par rapport à l'axe de rotation. En fait, le dipôle magnétique est décalé du centre d'Uranus vers le pôle de rotation au sud d'un tiers du rayon planétaire. Cette géométrie inhabituelle a pour conséquence une magnétosphère très asymétrique, où l'intensité du champ magnétique sur la surface dans l'hémisphère sud peut n'être que de (), tandis que dans l'hémisphère nord, il peut atteindre (). Le champ moyen à la surface est de (). À titre de comparaison, le champ magnétique de la Terre est à peu près aussi fort à chaque pôle, et son « équateur magnétique » est à peu près parallèle à l'équateur géographique.

Malgré son curieux alignement, à d'autres égards la magnétosphère d'Uranus est comme celle des autres planètes : elle a une onde de choc située à environ uraniens devant elle, une magnétopause à uraniens, et a une magnétoqueue et une ceinture de radiations développées. Dans l'ensemble, la structure de la magnétosphère d'Uranus est différente de Jupiter et plus semblable à Saturne.

La magnétosphère d'Uranus contient des particules chargées : les protons et les électrons avec petite quantité d'ions .

Les anneaux d'Uranus sont moins complexes que les anneaux de Saturne, mais plus élaborés que ceux de Jupiter ou de Neptune.

Uranus possède un système de treize anneaux connus. Cinq sont découverts le par James L. Elliot, Edward W. Dunham et Douglas J. Mink, grâce à une observation d’occultation d’étoile par Uranus, puis quatre autres sont découverts par la même méthode le . Deux autres sont découverts par entre 1985 et 1986 par observation directe. En 2003-2005, deux nouveaux anneaux externes sont photographiés par le télescope spatial "Hubble". Près de deux siècles auparavant, l'astronome William Herschel avait déjà rapporté l’observation d’anneaux, mais les astronomes modernes doutent que les anneaux sombres et ténus aient pu être vus à cette époque.

Les treize anneaux composant le système d’anneaux d’Uranus sont appelés, par ordre de distance croissante de la planète : 1986U2R/ζ, 6, 5, 4, α, β, η, γ, δ, λ, ε, ν et μ. Leurs distances au centre d'Uranus vont de pour l’anneau 1986U2R/ζ à environ pour l’anneau µ. Si les dix premiers anneaux d’Uranus sont fins et circulaires, le onzième, l’anneau ε, est plus brillant, excentrique et plus large, de au point le plus proche de la planète à au point le plus éloigné. Il est encadré par deux satellites « bergers », Cordélia et Desdémone. Les deux derniers anneaux sont très nettement plus éloignés, l’anneau μ se situant deux fois plus loin que l’anneau ε. Il existe probablement de faibles bandes de poussière et des arcs incomplets entre les anneaux principaux. Ces anneaux sont très sombres : l’albédo de Bond des particules les composant ne dépasse pas 2 %, ce qui les rend très peu visibles. Ils sont probablement composés de glace et d'éléments organiques noircis par le rayonnement de la magnétosphère.

La plupart des anneaux d’Uranus sont opaques et larges de quelques kilomètres seulement. L’ensemble du système ne contient que peu de poussières : il se compose essentiellement de rochers de de diamètre. Cependant, certains des anneaux sont translucides : les anneaux larges et peu visibles 1986U2R/ζ, μ et ν sont faits de petites particules de poussières, tandis que l’anneau λ, peu visible également mais étroit, contient aussi des corps plus importants.

Au regard de l'âge du système solaire, les anneaux d’Uranus seraient assez jeunes : leur âge ne dépasserait pas d’années. Le système d'anneaux provient probablement de la collision et de la fragmentation d'anciennes lunes orbitant autour de la planète. Après la collision, les lunes se sont probablement brisées en de nombreuses particules, qui n’ont survécu sous la forme d'anneaux étroits et optiquement denses que dans certaines zones de stabilité maximale.

Au début du , le mécanisme qui confine les anneaux étroits n’est pas bien compris. À l'origine, les scientifiques supposaient que chaque anneau étroit était encadré par des lunes « bergères », assurant sa stabilité. Mais, en 1986, la sonde "" ne découvrit qu’un seul exemple de tels bergers : Cordélia et Ophélie autour de l’anneau ε.

Uranus, la septième planète du Système solaire, possède vingt-sept satellites naturels connus. Ces satellites tirent leurs noms des personnages des œuvres de William Shakespeare et Alexander Pope. William Herschel découvrit les deux premières lunes, Titania et Obéron en 1787 tandis que les autres lunes en équilibre hydrostatique furent découvertes par William Lassell en 1851 (Ariel et Umbriel) et Gerard Kuiper en 1948 (Miranda). Les autres lunes furent découvertes après 1985, pour certaines durant le survol de , et pour les autres par des télescopes au sol.

Les satellites d'Uranus sont divisés en trois groupes : treize satellites intérieurs, cinq satellites majeurs et neuf satellites irréguliers. Les satellites intérieurs sont de petits corps sombres, qui ont des caractéristiques et une origine communes avec les anneaux de la planète. Les cinq satellites majeurs ont une masse suffisante pour être en équilibre hydrostatique, et quatre présentent à la surface des signes d'activité interne, tels que la formation de canyons ou du volcanisme. Le plus grand satellite d'Uranus, Titania, est le huitième plus grand du Système solaire, avec un diamètre de , mais est vingt fois moins massif que la Lune. Les satellites irréguliers d'Uranus ont des orbites elliptiques et fortement inclinées (en majorité rétrogrades), et orbitent à de grandes distances de la planète.

Des études ont montré qu'il serait possible à un quasi-satellite théorique d'Uranus ou de Neptune de le rester pour la durée de vie du Système solaire, moyennant certaines conditions d'excentricité et d'inclinaison. De tels objets n'ont cependant pas encore été découverts.

La magnitude apparente d’Uranus évolue entre +5,3 et +6,0. Ainsi, avec un ciel parfaitement sombre et dégagé, il est possible de la voir à l’œil nu, comme une étoile très peu lumineuse. Il est possible d'observer à l'œil nu des objets astronomiques dont la magnitude apparente est inférieure à +6. C'est d'ailleurs en cataloguant des étoiles allant jusqu'à la limite de visibilité à l'œil nu que John Flamsteed l'inventoria plusieurs fois, chaque fois sous des appellations différentes, dont la plus connue est . Depuis la Terre, la planète possède un diamètre angulaire compris entre 3,3 et 4,1 secondes d’arc selon que sa distance à la Terre varie de 3,16 à 2,58 milliards de kilomètres, et est facilement distinguable avec des jumelles. Avec un télescope de plus de de diamètre, Uranus apparaît comme un disque bleu pâle dont l’obscurcissement du limbe est visible. Les plus grands satellites, Titania et Obéron peuvent être perçus.

Jusqu'en 2007, Uranus s'était approché de son équinoxe et une activité nuageuse s’y développa. La majeure partie de cette activité ne peut pas être perçue autrement qu’avec le télescope spatial "Hubble" ou de grands télescopes munis d’optique adaptative. Cependant, certains phénomènes pourraient être suffisamment brillants pour être vus à l’aide de télescopes amateurs suffisamment grands. En 2006, une tache sombre a été détectée dans les longueurs d’onde visibles par "Hubble".

L'exploration d'Uranus n'a été accomplie que par la sonde spatiale et aucune autre expédition n'est prévue avant l'an 2030 au plus tôt. C'est le que la sonde atteint sa position la plus proche d'Uranus. découvre dix nouveaux satellites naturels d'Uranus. Elle étudie l'atmosphère d'Uranus, unique en raison de son inclinaison de l'axe de rotation sur le plan de l'orbite de et examine le système d'anneaux.

La possibilité d'envoyer la sonde Cassini-Huygens jusqu'à Uranus a été évaluée au cours d'une phase de planification de la mission d'extension en 2009. Il faudrait une vingtaine d'années pour arriver près du système uranien après le départ de Saturne. La mission « "" » a été recommandée pour la période 2013-2022.

La formation des géantes glacées, Neptune et Uranus, s'est avérée difficile à modéliser avec précision. Les modèles actuels suggèrent que la densité de matière dans les régions extérieures du système solaire était trop faible pour permettre la formation de ces grands corps avec la méthode traditionnellement acceptée d'accrétion de base. Différentes hypothèses ont été avancées pour expliquer leur création.

La première est que les géantes de glace n'ont pas été créés par accrétion de base, mais que des instabilités dans le disque protoplanétaire originel ont plus tard fait partir au loin leurs atmosphères par la radiation d'une étoile massive proche de type OB.

Un autre concept est qu'ils se sont formés plus près du Soleil, où la densité de matière est plus élevée, et qu'ils ont ensuite migré vers leurs orbites actuelles, après le retrait du disque protoplanétaire gazeux. Cette hypothèse de la migration après la formation est actuellement favorisée, en raison de sa capacité à mieux expliquer l'occupation des populations de petits objets observés dans la région trans-neptunienne. Le courant le plus largement accepté des explications sur les détails de cette hypothèse est connue sous le nom de modèle de Nice, qui explore l'effet d'une migration d'Uranus et des autres planètes géantes sur la structure de la ceinture de Kuiper.

En astrologie, la planète Uranus () est l'astre associé au Verseau. Depuis, Uranus est associée à la couleur cyan et à l'électricité, la couleur bleu électrique étant associée au signe du Verseau.

« Uranus, le magicien » est le et avant-dernier mouvement de l'œuvre pour grand orchestre "Les Planètes", composée et écrite par Gustav Holst entre 1914 et 1916.

L'opération Uranus fut une opération militaire soviétique couronnée de succès durant la Seconde Guerre mondiale, qui a consisté à reprendre la ville assiégée de Stalingrad.

Dans le poème de John Keats « ' », les deux vers ' (), sont une référence à la découverte d'Uranus par Herschel.





</doc>
<doc id="3097" url="https://fr.wikipedia.org/wiki?curid=3097" title="Une part du ciel">
Une part du ciel

Une part du ciel est un film franco-belge réalisé par Bénédicte Liénard, sorti en 2002.

Le quotidien de Claudine se résume par la répétition dû à son travail dans une usine et l'ennui. Tandis que Joanna, de son côté, purge une peine de prison. Pour elle aussi, le quotidien est une succession de gestes qui se ressemblent. Suite à une faute, Joanna s'est retrouvée incarcérée parce que Claudine n'a pas su, ou voulu, la soutenir. Elle mène un combat contre la justice durant son séjour. Pour cela, Claudine devra témoigner en sa faveur.





</doc>
<doc id="3100" url="https://fr.wikipedia.org/wiki?curid=3100" title="Uniform Resource Locator">
Uniform Resource Locator

Le sigle URL (de l’, littéralement « localisateur uniforme de ressource »), auquel se substitue informellement l'expression adresse web, désigne une chaîne de caractères utilisée pour identifier les ressources d'internet : document HTML, image, son, forum Usenet, boîte aux lettres électronique, entre autres. Les URL constituent un sous-ensemble des identifiants uniformisés de ressource (URI). La syntaxe d'une URL est décrite dans la .

Les URL sont une invention du et sont utilisées pour identifier les pages et les sites web. Elles sont aussi appelées adresses web. L'article sur les adresses web porte sur l'identité des sites web et les aspects techniques, économiques et juridiques qui s'y rattachent, ainsi que des différentes traductions en français de l'acronyme URL.

Cet article décrit les URL en tant que standard technique : toutes les formes qu'elles peuvent prendre, notamment pour pointer des ressources hors du Web, ainsi que les principaux usages techniques.

En France, d'après le Journal officiel de la République française du , « URL » peut être traduit par adresse réticulaire ou adresse universelle."

Les URL ont été inventées pour pouvoir indiquer avec une notation (d'où l'adjectif « uniforme ») aux navigateurs web comment accéder à toutes les ressources d'Internet.

Chaque hyperlien du web est construit avec l'URL de la ressource pointée, insérée avec une certaine syntaxe dans un document source (ou dans un programme ou dans une interface utilisateur) qui le contient pour indiquer la localisation d'une autre ressource (un document) ou fragment de ressource (une ancre cible dans cet autre document). Lorsqu'on active un hyperlien, le navigateur web peut présenter son URL dans une barre d'état (voir ci-dessous pour la « barre d'adresse »).

Un hyperlien peut aussi être construit de façon externe au document lui-même, dans une base de données référençant toutes les paires (URL source, URL cible) entre une zone activable d'un document source et une ancre cible dans un document (qui peut être le même que le document source contenant la zone activable).

L'hyperlien peut aussi être construit de façon inversée, en insérant dans le document cible l'URL de la zone source.

Chaque navigateur web dispose d'une « barre d'adresse » affichant l'URL de la ressource consultée. Il est en outre possible de saisir une URL dans cette barre d'adresse pour consulter une ressource dont on connaît l'URL.

Si le support le permet, on peut aussi trouver l'URL correspondant à un lien en positionnant la souris sur l'image ou le texte approprié. L'URL peut alors être présentée dans une barre d'état ou une bulle d'information.

Les navigateurs web conservent un historique des URL consultées. Cela leur permet de reconnaître et présenter de manière distinctive les hyperliens vers des ressources déjà consultées.

Il suffit à un navigateur web de conserver l'URL d'une ressource pour constituer une liste de favoris (ou marque-pages). Lorsqu'un titre de ressource existe, les navigateurs le conservent aussi, ce qui permet d'afficher le titre d'une page plutôt que son URL.

Une URL est une chaîne de caractères combinant les informations nécessaires pour indiquer à un logiciel comment accéder à une ressource Internet. Ces informations peuvent notamment comprendre le protocole de communication, un nom d'utilisateur, un mot de passe, une adresse IP ou un nom de domaine, un numéro de port TCP/IP, un chemin d'accès, une requête.

Les informations nécessaires varient selon la ressource et le contexte d'utilisation de l'URL. En outre un identificateur de fragment peut être ajouté à la fin d'une URL pour identifier un élément à l'intérieur de la ressource. Bien que l'identificateur de fragment ne fasse pas formellement partie de l'URL, il est également décrit dans cet article et dans les standards techniques.

Une URL absolue permet d'indiquer comment accéder à une ressource indépendamment de tout contexte où elle peut être précisée ou transmise. Elle commence par l'indication d'un schéme de représentation (spécifique au protocole de communication utilisé pour accéder à cette ressource), suivi de l'ensemble des paramètres permettant de localiser sur le réseau le service hébergeant la ressource, puis permet de préciser à ce service le nom d'une ressource à traiter, transmettre des données de traitement, acheminer et récupérer les résultats, puis de préciser éventuellement quelle partie de ce résultat sera utilisée.

Exemple : codice_1

Quelques exemples pratiques :

Les protocoles utilisant un chemin hiérarchique permettent l'utilisation d'URL relatives. Une URL relative ne contient ni protocole ni nom de domaine. Ceux-ci sont déduits à partir de l'URL de la ressource contenant l'URL relative.

Les URL relatives sont souvent utilisées pour les hyperliens à l'intérieur d'un même site web. Si le document d'URL codice_35 contient l'URL relative codice_36, cela correspond à codice_37. Les URL relatives sont directement inspirées de la syntaxe des systèmes de fichiers Unix. L'usage d'URL relatives permet de ne pas avoir à reprendre l'ensemble des liens lors du changement d'adresse d'un site.

L'URL :

Le est publié en juin 1994 par Tim Berners-Lee. C'est un mémo, publié en attendant que l'Internet Engineering Task Force (IETF) termine son travail sur les URI. Ce RFC documente la pratique contemporaine sur le Web, et n'est explicitement pas destiné à devenir une norme.


Documents définissants les recommandations et normalisations liées aux URL :


</doc>
<doc id="3101" url="https://fr.wikipedia.org/wiki?curid=3101" title="Uniform Resource Identifier">
Uniform Resource Identifier

Un URI, de l'anglais , soit littéralement "identifiant uniforme de ressource", est une courte chaîne de caractères identifiant une ressource sur un réseau (par exemple une ressource Web) physique ou abstraite, et dont la syntaxe respecte une norme d'Internet mise en place pour le (voir ). La norme était précédemment connue sous le terme "UDI".

L'IETF l'a d'abord défini dans la en se basant sur des propositions de Tim Berners-Lee (). Mise à jour par la puis révisée de nombreuses fois sous le titre "rfc2396bis", la définit les URI en .

Le sigle URI est généralement utilisé pour désigner une telle chaîne de caractères. Par exemple codice_1 est un URI identifiant la .

Les URI sont la technologie de base du car tous les hyperliens du Web sont exprimés sous forme d'URI.

Une traduction de rfc3986.

Un URI doit permettre d'identifier une ressource de manière permanente, même si la ressource est déplacée ou supprimée.

Bien que les URI soient très largement utilisés dans le monde informatique, avec surtout les URL sur Internet, on en retrouve d'autres applications dans le monde réel. Ainsi le code ISBN, qui est l'identifiant unique d'un livre, et permet de retrouver celui-ci depuis n'importe quelle librairie ou bibliothèque, dans le monde entier. On peut considérer également les codes-barres comme une métaphore d'URI, dans le monde physique : un code-barres ne localise pas un produit mais l'identifie (bien qu'il identifie "l'ensemble des exemplaires" d'un produit, pas chaque exemplaire individuellement, ce qui est le travail du numéro de série, lequel n'est pas systématique mais réservé aux produits onéreux).

Un URI peut être de type « » ou « » ou les deux.

Un (URL) est un URI qui, outre le fait qu'il identifie une ressource sur un réseau, fournit les moyens d'agir sur une ressource ou d'obtenir une représentation de la ressource en décrivant son mode d'accès primaire ou « emplacement » réseau. Par exemple, l'URL "<nowiki>http://www.wikipedia.org/</nowiki>" est un URI qui identifie une ressource (page d'accueil Wikipédia) et implique qu'une représentation de cette ressource (une page HTML en caractères encodés) peut être obtenue via le protocole HTTP depuis un réseau hôte appelé www.wikipedia.org.

Un Uniform Resource Name (URN) est un URI qui identifie une ressource par son nom dans un espace de noms. Un URN peut être employé pour parler d'une ressource sans que cela préjuge de son emplacement ou de la manière de la référencer. Par exemple, l'URN "<nowiki>urn:isbn:0-395-36341-1</nowiki>" est un URI qui, étant un numéro de l' (ISBN), permet de faire référence à un livre, mais ne suggère ni où, ni comment en obtenir une copie réelle.

Le point de vue actuel du groupe de travail qui supervise les URI est que les termes "URL" et "URN" sont des aspects dépendant du contexte des URI, et que l'on a rarement besoin de faire la distinction entre les deux. Dans les publications techniques, spécialement les normes érigées par l'IETF et le W3C, le terme "URL" n'a pas été reconnu pendant longtemps, parce qu'il était rarement nécessaire de faire une distinction entre les URL et les URI. Cependant, dans des contextes non techniques et dans les logiciels du , le terme "URL" reste omniprésent. De plus, le terme "adresse web", qui n'a pas de définition formelle, est souvent employé dans des publications non techniques comme synonyme d'URL ou URI, bien qu'il ne se réfère généralement qu'aux protocoles 'HTTP' et 'HTTPS'.

Les exemples suivants illustrent les URI d'usage courant :






</doc>
<doc id="3102" url="https://fr.wikipedia.org/wiki?curid=3102" title="Uniform Resource Name">
Uniform Resource Name

Uniform Resource Name (URN), traduit littéralement de l'anglais par « nom uniforme de ressource », est le nom d'un standard informatique dans le domaine de l'Internet qui concerne principalement le World Wide Web.

Le document de base est la " URN Syntax" publiée en 1997 par l'Internet Engineering Task Force. Elle donne une syntaxe de chaîne de caractères utilisable pour identifier une ressource (un document, une image, un enregistrement sonore, etc.) globalement, durant toute son existence, indépendamment de sa localisation ou de son accessibilité par Internet. Les "Uniform Resource Names" sont des "Uniform Resource Identifiers" dont la méthode est codice_1.

Le sigle URN est généralement utilisé, en français comme en anglais, pour désigner une telle chaîne de caractères. Par exemple codice_2 est un URN identifiant le .

Les URN sont des "Uniform Resource Identifiers" (URI) et en respectent donc les règles syntaxiques. Les URN ont la syntaxe suivante : 


L'usage de minuscules ou de majuscules ne fait pas de différence pour l'écriture de la méthode codice_1 ni pour le NID. Il peut en revanche faire une différence pour le NSS.

Le NID définit un espace de noms. L'Internet Assigned Numbers Authority (IANA) tient un registre des NID officiellement enregistrés. Le donne la marche à suivre pour procéder à un tel enregistrement.


Lors de la conception du World Wide Web, les "Uniform Resource Locators" (URL) ont été inventées et utilisées pour l'identification des ressources. Mais une URL identifie en fait l'emplacement d'une ressource, plutôt que la ressource elle-même. Ainsi, lorsqu'une ressource est déplacée, par exemple mise sur un autre serveur Web, toutes les URL l'identifiant sont rendues obsolètes. Ce problème est à la base de la plupart des hyperliens « cassés » du Web.

Pour remédier à ce problème, le concept d'URN a été avancé. Par opposition aux URL, les URN identifient les ressources elles-mêmes, indépendamment de leur emplacement. Ce concept nécessite toutefois un mécanisme capable de trouver l'emplacement d'une ressource – par exemple son URL, du moins si elle est accessible sur le réseau – à partir de son URN. Un tel mécanisme repose typiquement sur un répertoire de correspondances.

Dans la pratique, les URN ne sont guère utilisés. Les problèmes de localisation de ressource sont généralement résolus avec un moteur de recherche. On peut noter la fonctionnalité de « document en cache » qui conserve un certain temps une version du document référencé, indépendamment de son accessibilité à son URL originale.

[ URI | URL | RFC | IETF | IANA | World Wide Web ]



</doc>
<doc id="3103" url="https://fr.wikipedia.org/wiki?curid=3103" title="UDP">
UDP

UDP peut faire référence, par ordre alphabétique à :

</doc>
<doc id="3104" url="https://fr.wikipedia.org/wiki?curid=3104" title="Un piège nommé Krytos">
Un piège nommé Krytos

Un piège nommé Krytos (titre original : "The Krytos Trap") est un roman de science-fiction écrit par Michael A. Stackpole. Publié aux États-Unis par Bantam Spectra en 1996, il a été traduit en français et publié par les éditions Fleuve noir en 1999. Ce roman, se déroulant dans l'univers étendu de "Star Wars", est le troisième livre de la série "Les X-Wings". Il se déroule sept ans après la bataille de Yavin.

Sur Corusçant, la Nouvelle République se félicite de sa nouvelle conquête. De plus, le procès de Tycho Celchu, espion de l'Empire, se prépare, pour haute trahison et le meurtre de Corran Horn lors de la conquête de la planète-ville.
Mais Horn n'est pas mort. Emprisonné dans la prison secrète d'Ysanne Isard, Lusenkya, avec entre autres le général Dodonna, il tente de trouver une échappatoire de ce lieu dont personne n'est jamais repartit vivant.




</doc>
<doc id="3105" url="https://fr.wikipedia.org/wiki?curid=3105" title="Urbanisme">
Urbanisme

L’urbanisme désigne l'ensemble des sciences, des techniques et des arts relatifs à l'organisation et à l'aménagement des espaces urbains. Ce projet peut être sous tendu par une volonté d'assurer le bien-être de l'homme et d'améliorer les rapports sociaux en préservant l'environnement. Les professionnels qui exercent ce métier sont des urbanistes. 

Selon les traditions académiques, cette discipline est associée tantôt à l'architecture, tantôt à la géographie, selon l'aspect mis en avant, l'intervention urbaine ou l'étude théorique. En France, l'enseignement et la recherche universitaire dans ce champ relèvent d'une section spécifique du Conseil national des universités (24, "Aménagement de l'espace, Urbanisme").

L'urbaniste Pierre Merlin précise que « "les géographes ont souvent eu tendance à considérer, en France notamment, l'aménagement (et en particulier l'aménagement urbain, voire l'urbanisme) comme un prolongement naturel de leur discipline. Il s'agit en fait de champs d'action pluridisciplinaires par nature qui ne sauraient être l'apanage d'une seule discipline quelle qu'elle soit. Mais la géographie, discipline de l'espace à différentes échelles, est concernée au premier chef" ».

En tant que champ disciplinaire (ou scientifique), les théories de l'urbanisme sont en étroite filiation avec les sciences sociales (économie, géographie, droit, écologie, anthropologie, science politique, sociologie, linguistique, sémiologie) et aussi naturelles : environnement, écologie, géologie, hydrologie, climatologie... Ce champ intègre aussi bon nombre de disciplines dites techniques comme l'ingénierie des transports, le génie civil pour les infrastructures, le paysage, la sauvegarde et la mise en valeur des sites et du patrimoine...

En tant que champ professionnel, les pratiques et techniques de l'urbanisme découlent de la mise en œuvre des politiques urbaines à différentes échelles (projets urbains, rénovation et réhabilitation de quartier, logement, transport, environnement, zones d'activités économiques et équipements commerciaux). Cette deuxième dimension recoupe la planification urbaine et la gestion de la cité (au sens antique du terme), en maximisant le potentiel géographique en vue d'une meilleure harmonie des usages et du bien-être des utilisateurs (résidents, actifs, touristes).

Qu'est-ce que l'urbanisme ? Si l'on entend par urbanisme la traduction volontaire dans l'espace d'un mode d'organisation et de gestion des hommes et de leurs activités, l'urbanisme apparaît très tôt dans l'Antiquité et avant même les premières formalisations grecques puis romaines. Le hiéroglyphe égyptien qui signifie la ville est déjà l'expression d'associer dans un même espace délimité (par un cercle) des hommes et des femmes avec des métiers et des activités différentes, reliés ensemble par des voies que les romains appelleront "cardo" et un "decumanus" greffant ainsi la cité sur le cosmos. Les Mésopotamiens ont aussi une iconographie similaire. La ville est donc directement le produit du politique, le terme étant pris au sens étymologique et le terme "politique" a pour origine le terme grec de "polis" qui désigne la ville comme institution. L'homme pour les philosophes grecs, Platon et Aristote notamment, un animal politique et l'organisation de la ville doit lui permettre de devenir un acteur du destin collectif en devenant citoyen, là encore le terme renvoie à la cité,"civis" chez les Romains. Par là il devient civi-lisé, il devient poli et policé, deux termes qui se souchent là encore sur la ville, c'est-à-dire capable de vivre d'autres qui sont égaux et différents pour reprendre le terme du sociologue Alain Touraine (Comment vivre ensemble égaux et différents). Cette ville là qui fait l'objet d'une réflexion philosophie et politique préalable se fonde sur la "démocratie" même si elle est à l'époque de Platon toute relative. Elle s'organise autour d'un espace vide, l'agora (le forum pour les Romains) qui est par excellence le lieux des échanges. Cet espace est entouré des institutions qui structurent la vie de la Cité : la salle de l'assemblée politique et les temples notamment. Ce n'est pas d'abord le marché qui fonde la cité mais bien la vie sociale et politique de ses habitants même si elle prend en compte les autres fonctions : sanitaires (thermes), culturelles (théâtre), économiques(marché, port...), sportives (gymnase), récréative (stade), formation (académies)... Saint Augustin met en parallèle la cité de Dieu et celle des hommes. 

Cette réflexion sur la traduction dans l'espace d'une société pensée comme idéale va réapparaître à la Renaissance avec Rabelais et son abbaye de "Télème", Thomas More avec son "Utopia" ou Johann Valentin Andreae avec sa "Christianopolis". De nombreux dessins de villes idéales apparaissent à la Renaissance dans toute l'Europe avec Francesco Giorgio di Martini, Pietro Cataneo, Francesco de Marchi... 

Les besoins politiques comme défensifs et le développement des colonisations vont générer de nombreuses villes nouvelles de part le monde et certaines relevant d'un plan régulé à l'instar des villes romaine set gallo-romaines. Les rois font réaliser des places royales avec des plans d'embellissement de la ville. 

Le développement de l'industrialisation va entrainer un afflux de population vers les villes pour faire face aux besoins de main d'œuvre. Face au développement anarchique et insalubre des faubourgs, de développe une analyse critique et aussi de nombreuses théories sur la ville pour améliorer le "vivre ensemble" avec Fourrier par exemple, Ebenezer Howard... C'est à aprtir de là que va se développer aussi le premier mouvement "urbanistique" avec la création en 1899 par Howard de la Town and Country Planning Association, l'association pour la planification des ville et des campagnes. C'est aussi à la fin du XIXème siècle que l'Allemagne met en place les premières obligations de planifier le développement des villes et l'aménagement du territoires. 

Le terme "urbanisme" apparait avec l’ingénieur catalan, Ildefons Cerdà et son ouvrage "Théorie générale de l'urbanisation" paru en 1867. Il fit son apparition en France en 1910 à la suite d'une parution dans le "Bulletin de la Société neuchâteloise de géographie" sous la plume de Pierre Clerget. En 1911, la Société française des urbanistes (SFU) est fondée à partir des membres du Musée social; Ce "musée" qui avant out un conservatoire des expériences en matière sociale, est issu des courants humanistes et hygiénistes de la fin du . La SFU institution réunit depuis ses origines les urbanistes de tous modes d’exercice (public, para public et privé) sur la base de critères professionnels. Ses actions constituent une véritable force de propositions, qui se manifeste notamment dans la vision des urbanistes pour la ville du et la . La SFU représente les urbanistes de France au Conseil européen des urbanistes. 

L'urbanisme en France se développe en même temps que le métier d'urbaniste à partir des réflexions menées à la fin du XIXème siècles, les travaux des membres du Musée social avant la première guerre mondiale et des CIAM après la seconde guerre mondiale. 

En dehors des approches fonctionnelles du , à titre d'exemple contraire, nous pouvons évoquer la Cité-jardin développée en théorie comme en pratique par Ebenezer Howard à la fin du avec la construction de deux villes en Angleterre : Letchworth et Welwyn. C'est là un modèle qui fut largement utilisé dans le contexte de la reconstruction de l'après-guerre par Henri Sellier dans le département de la Seine, à Reims, à Ternier, à Laon… 

C’est à partir de 1953 que l’école des beaux-arts de Paris enseigne l’urbanisme à ses étudiants. Un ouvrage de référence de Françoise Choay permet de mieux saisir les principaux enjeux de l’urbanisme, sous une forme pédagogique : "Urbanisme, Utopies et réalité" (1965), qui est une anthologie des différents concepts urbanistiques développés depuis plusieurs siècles. 

La reconstruction après la seconde guerre mondiale et le boom démographique (baby boom) qui s'ensuivit nécessité construire des logements en grands nombres. Ce fut l'époque des ZUP (zones à urbaniser en priorité) de plusieurs centaines de logements voire milliers. Cette production en logements en série à la périphérie dans grandes agglomérations dans des quartiers souvent mal équipés et mal desservis par les transports en commun posa rapidement de nombreux problèmes notamment avec la crise économique après le début des années 70. De nombreux plans se succèdent pour faire face à la dégradation rapide de ces quartiers et de leurs habitats : HVS (habitat et vie social) 1977-1981, DSQ (développement social des quartiers) 1981-1984, Banlieues 89 avec Roland Castro et Michel Cantal-Dupard... Un Comité Interministériel pour la Ville est créé en 1984 avec la volonté de créer une véritable politique de la ville avec en 1990 un ministère dédié dont Michel Delebarre est le Ministre. En juillet 1991, la loi d’orientation pour la ville (LOV). Cette loi a pour objectif de mettre en œuvre le droit à la ville et de créer les conditions de vie et d’habitat favorisant la cohésion sociale et de nature à éviter ou faire disparaître les phénomènes de ségrégation. Sont alors mis en place les GPU (grands projets urbains). La loi SRU (Solidarité et renouvelle urbain) vient en 2000 complèter le dispositif.

La politique des villes nouvelles menées par la DATAR permet une réflexion plus globale sur la nature même de la ville. Ces villes sont : 
L'échec ressenti des grandes opérations d'aménagement de l'après-guerre - grandes opérations inspirées de la vision moderniste des CIAM - remet en cause une vision par trop fonctionnaliste voire strictement économique ou technique (urbanisme de tuyaux). Par ailleurs, dans le cadre d'une approche plus compétitive au niveau européen voire mondial se développe une politique d'aménagement et d'urbanisme au niveau à une échelle qui dépasse le stade ancien de l'agglomération : Grand Paris, Grand Lyon, Grand Reims... Cette politique est confortée par la loi ALUR qui contraint au regroupement communal retirant aux maires proches de leur population la compétence en matière d'urbanisme hormis la délivrance des permis de construire.

Le plan hippodamien a été largement utilisé par les Romains dans leur expansion coloniale en Europe et a été repris à partir du pour la construction des villes sur le continent américain notamment comme La Nouvelle-Orléans puis en Afrique du Nord au avec Lyautey. Il faut aussi noter l'expérience spécifique et originale des bastides dans le Sud-Ouest de la France avec 300 à 500 établissements nouveaux créés en 200 ans sur un même et unique modèle, fait unique dans l'histoire de l'urbanisation. Il existe enfin des villes qui relève d'un projet utopique à la recherche de la cité idéale comme Chandigarh en Inde avec Le Corbusier ou Auroville non loin de Pondichéry.

L'urbanisme est marqué par la pluridisciplinarité des savoirs qui le composent mais il ne peut se limiter à eux seuls ni à leur somme. On doit envisager l'urbanisme non pas sous l'angle des outils qu'ils soient théoriques ou techniques, mais de l'objet même qu'est la ville, sa conception, son développement et se gestion.

On retrouve les sciences et politiques suivantes :

L'urbanisme n'est pas une branche de la géographie même si cette dernière concerne une part de certaines activités des professionnels de l'urbanisme. Au , des chercheurs comme Pierre Merlin mettent en évidence le manque d'indépendance des formations en urbanisme qui sont encore trop orientées vers la géographie, l'économie, l'architecture, etc. On demande aussi à l'urbaniste à contribuer à l'aménagement des villes où il fait bon vivre en bonne santé (hygiénisme), où la violence et la « criminalité urbaine » trouveraient aussi peu que possible à s'exprimer ou à se développer ; sans tomber dans des modèles relevant de ce que certains auteurs ont appelé un « "urbanisme de la peur" ».

Dans le champ professionnel, on peut classer en plusieurs catégories l'urbanisme : "l'urbanisme théorique, l'urbanisme pratique", l’"urbanisme réglementaire" administratif restrictif-incitatif et l’"urbanisme opérationnel" d’action sur le terrain par des opérations concrètes.

L'urbanisme n'est pas, comme le montre Françoise Choay qu'une discipline qui aurait pour champ d'application la ville. C'est d'abord un champ de réflexion sur l'organisation des hommes et de leurs activités dans le temps et dans l'espace et ce depuis Platon et Aristote qui considèrent l'homme comme un "animal politique". C'est le fondement même de la Cité. On peut considérer que l'urbanisme apparaît de facto avec Hippodamos et le plan de la ville de Millet en Turquie et aussi ceux que les Cités adoptent pour leurs colonies dans le pourtour du Bassin Méditerranéen. Ce modèle sera repris par les Romains lors de la constitution de leur Empire et dont on retrouve la trace dans la plupart des villes d'Europe. Ces derniers s'appuient sur la technique de leurs arpenteurs ("agrimensor").

La réflexion philosophique sur la ville réapparaît dès le Moyen Age avec la notion de régularité. Les bastides du Sud-ouest en sont sans doute la traduction la plus intéressante. Nous trouvons chez François Rabelais (1483-1553) et Thomas More (1478-1535) les premiers écrits qui évoquent la ville idéale, utopique. C'est à la Renaissance que vont apparaître les premiers plans de villes idéales avec notamment Francesco di Giorgio Martini (1439-1502), Pietro Cataneo (1510-1569), Francesco de Marchi (1504-1576)...

L'urbanisme du XVIIIème siècle est avant marqué par les plans d'embellissement des villes la plupart du temps afin de célèbre la grandeur du roi avec des places royales comme à Reims par exemple. Mais c'est à partir du développement industriel qui va engendrer une émigration importante vers les villes que va se développer la réflexion sur la ville avec un certain nombre d'utopies dont certaines verront le jour comme Letchworth Garden City et dues à Ebenezer Howard, , les Salines d'Arc et Senans avec Claude Nicolas Ledoux... On peut aussi évoquer le Phalanstère de Charles Fourrier et le Familistère de Guise dans l'Aisne avec Jean Baptiste Godin... On peut encore évoquer la baron Haussmann qui refaçonne Paris en partie à des fins politiques.

Les conquêtes coloniales sont aussi l'occasion de créer de nouvelles villes qui s'appuient sur une réflexion urbanistique comme Washington avec l'urbaniste Charles André Lenfant ou les villes d'Afrique du Nord avec Lyautey.

C'est à la fin du XIXème siècles qu'apparaissent les premières lois visant à mettre à une organisation de l'espace anarchique et spéculative et notamment pour lutter contre les lotissements insalubres. Se développe en parallèle une réflexion sur l'habitat ouvrier notamment dans le cadre des Expositions Universelles et sa section d'économie sociale dirigée par Frédéric Le Play. Cela va induire la naissance du mouvement HBM et aussi une réflexion sur une approche qui dépasse la seule problématique du logement notamment avec les Cités-Jardins. C'est à partir du mouvement international des cités-jardins que va, en France, se développer l'urbanisme autour du Musée social. Les premières lois d'urbanisme de 1919 et 1924 dites lois Cornudet son directement inspirées par les membres du Musée sociale de cette époque comme le montrent Jean-Pierre Gaudin et François-Xavier Tassel.

Une histoire de la ville commence à s'écrire au XXème siècle avec Léonardo Benevolo, Pierre Lavedan, Michel Ragon... Les sociologues contribuent à une approche plus sociologique avec des centres universitaire de recherche comme le Centre de Sociologie Urbaine de Grenoble ou autour d'Henri Lefbvre à l'Université de Vincennes et qui écrit un ouvrage qui fera date : "Le droit à la ville". La réflexion théorique se poursuit avec notamment Françoise Choay à partir de son ouvrage : "L'urbanisme, réalités et utopies". La réflexion philosophique se poursuit avec notamment les travaux d'Olivier Mongin, Thierry Paquot, Agustin Berque...

A partir des années soixante dix, va se développer en France et en Belgique un urbanisme pratique pour reprendre le terme de "praxis" employé par Henri Lefebvre. Ce champ est celui, dans une approche autogestionnaires porté par les syndicats et les associations locales, de la volonté des habitants non seulement à participer aux décisions urbanistiques les concertant mais aussi contribuer à l'élaboration des projets. Il s'agit pour eux de faire contrepoids à la technostructure, les professionnels comme les élus et aménageurs, notamment aux travers d'Ateliers Publics d'urbanisme. Cela deviendra ce que l'on appelle l'urbanisme participatifs. Parmi les opérations exemplaires on peut citer l'Alma Gare à Roubaix, La ZAC du Mont Hermé à Saint Brice Courcelles près de Reims ou encore le quartier des Marolles à Bruxelles avec Lucien Kroll. Ces expériences ont été à l'origine de l'introduction des lois obligeant à une plus grande information voire concertation

La gouvernance des projets en matière d'urbanisme et d'aménagement revient à l'ordre du jour au delà des luttes symboliques comme Notre Dames des Landes. Le philosophe Thierry Paquot invite à nouveau les habitants à se saisir de la ville avec son ouvrage "L'urbanisme, c'est notre affaire".

Les urbanistes participent aussi à l'approche globale du développement des villes ou de leurs quartiers notamment en élaborant avec les responsables locaux et parfois les populations concernées à des projets urbains qui prennent en compte en amont de la mise en forme spatiale des objectifs plus globaux qui sont ensuite déclinés par secteurs et activités.

Les notions traditionnelles de plan et de planification sont progressivement remplacées par celles de développement urbain durable et de projet urbain. Le projet urbain présente plusieurs dimensions, et peut être défini comme suit : « Le projet urbain est à la fois un processus concerté et un projet territorial : il consiste à définir et mettre en œuvre des mesures d’aménagement sur un territoire urbain donné, en partenariat avec tous les partenaires civils et institutionnels concernés, intégrant les différentes échelles territoriales et le long terme, en vue d’un développement urbain durable.

Il s'agit de créer un document d'urbanisme respectant le droit de l'urbanisme local pour l'action à entreprendre. Ce travail peut dans certains cas concerner des conurbations, plusieurs communes, afin d'obtenir une cohérence sur l'ensemble d'une agglomération (tel qu'en France pour le Schéma de cohérence territoriale SCOT ou le Plan local d'urbanisme communal ou intercommunal). 

Par exemple :

L’urbanisme opérationnel consiste à mettre en place les actions et procédures nécessaires à la réalisation d’un projet urbain. Il regroupe ainsi « l’ensemble des actions conduites ayant pour objet la fourniture de terrains à bâtir, la construction de bâtiments ou le traitement de quartiers et d'immeubles existants (recomposition urbaine, réhabilitation, résorption de l’habitat insalubre) » (Réf. ?). Par cela, il se différencie de l’urbanisme réglementaire qui regroupe l’ensemble des documents thématiques et réglementaires de planification stratégique et de programmation.

Une organisation théorique du territoire fondée sur trois facteurs et ça qu'il appelés « rayon facile de communication ». Ceux-ci donnent une hiérarchie aux centres urbains rayonnant sur des hexagones vers des unités urbaines plus petites. Les trois facteurs sont : la sociabilité, l’économie et l’administration.

Une hiérarchie de la répartition optimale des unités urbaines en 6 niveaux pour maximiser son utilité et profit selon trois caractéristiques :


Son modèle dits que les services de même niveau ont tendance à se regrouper (système de centralité) selon la taille de l’unité urbaine et le degré de trois caractéristiques. (Voir Modèle christallérien) 

Le métier d'urbaniste s'exerce sous différente formes, seul ou en équipe plus ou moins pluridisciplinaires, dans des institutions publiques, des sociétés privés ou des associations. Le métier s'exerce aussi à différents niveaux de territorialité. Au-delà des formations de niveaux inégaux, il existe une reconnaissance professionnelle de qualification (OPQU) en sus des associations professionnelles.




Elle présente un plan pentagonal organisé autour d'une vaste esplanade d'où part un tracé rectiligne qui sillonne la ville et le département de la Vendée.








</doc>
<doc id="3106" url="https://fr.wikipedia.org/wiki?curid=3106" title="Unicode">
Unicode

Unicode est un standard informatique qui permet des échanges de textes dans différentes langues, à un niveau mondial. Il est développé par le Consortium Unicode, qui vise au codage de texte écrit en donnant à tout caractère de n'importe quel système d'écriture un nom et un identifiant numérique, et ce de manière unifiée, quelle que soit la plate-forme informatique ou le logiciel utilisés.

Ce standard est lié à la norme ISO/CEI 10646 qui décrit une table de caractères équivalente. La dernière version, Unicode 10.0, a été publiée en juin 2017.

Totalement compatible avec le jeu universel de caractères (JUC) de l'ISO/CEI 10646, le standard Unicode l'étend en lui ajoutant un modèle complet de représentation et de traitement de textes, , en décrivant avec précision les , et en normalisant des algorithmes de traitement qui préservent au maximum la sémantique des textes transformés. Unicode a pour objet de rendre un même texte utilisable à l'identique sur des systèmes informatiques totalement différents.

Le standard Unicode est constitué d'un répertoire de , couvrant une centaine d’écritures, d'un ensemble de tableaux de codes pour référence visuelle, d'une méthode de codage et de plusieurs codages de caractères standard, d'une énumération des propriétés de caractère (lettres majuscules, minuscules, symboles, ponctuation, etc.) d'un ensemble de fichiers de référence des données informatiques, et d'un certain nombre d'éléments liés, tels que des règles de normalisation, de décomposition, de tri, de rendu et d'ordre d'affichage bidirectionnel (pour l'affichage correct de texte contenant à la fois des caractères d'écritures droite à gauche, comme l'arabe et l'hébreu, et de gauche à droite).

En pratique, Unicode reprend intégralement la norme ISO/CEI 10646, puisque cette dernière ne normalise que les caractères individuels en leur assignant un nom et un numéro normatif (appelé "point de code") et une description informative très limitée, mais aucun traitement ni aucune spécification ou recommandation pour leur emploi dans l'écriture de langues réelles, ce que seul le standard Unicode définit précisément. L'ISO/CEI 10646 fait normativement référence à certaines parties du standard Unicode (notamment ) ; Unicode est également une norme "de facto" pour le traitement du texte et sert de base à de nombreuses autres normes.

Unicode, dont la première publication remonte à octobre 1991, a été développé dans le but de remplacer l'utilisation de pages de code nationales.

Ces pages de code avaient dans le passé quelques problèmes. Par exemple sur les terminaux 3270 fonctionnant en EBCDIC : lorsqu'une note de service électronique comportait un caractère « signe monétaire », le même texte plafonnant une dépense en dollars pour le lecteur américain autorisait le lecteur britannique à la même dépense en livres sterling, sans que quoi que ce soit ait été modifié. 

Dans la pratique, tous les systèmes d'écriture ne sont pas encore présents, car un travail de recherche documentaire auprès de spécialistes peut encore s'avérer nécessaire pour des caractères rares ou des systèmes d'écriture peu connus (parce que disparus, par exemple).

Cependant, les écritures les plus utilisées dans le monde sont représentées, ainsi que des règles sur la sémantique des caractères, leurs compositions et la manière de combiner ces différents systèmes. — Par exemple, comment insérer un système d'écriture de droite à gauche dans un système d'écriture de gauche à droite (texte bidirectionnel).

Sous sa forme UTF-8, l'Unicode offre une certaine interopérabilité avec le code ASCII.

Le standard Unicode définit des exigences permettant d'évaluer la conformité de l'implémentation d'un process (ou d'un logiciel) à Unicode. Ces exigences concernent notamment (dans la version 4.0) :

Ces exigences permettent le support d'un sous-ensemble d'Unicode.

Alors que l'ISO/CEI 10646 définit le même jeu de caractères qu'Unicode, la différence entre ISO/CEI 10646 et Unicode tient essentiellement dans le surplus d'exigence de conformité fourni par Unicode.

Unicode est en 2016 le standard leader pour le codage informatique des caractères. Il sert à l'interopérabilité de logiciels, et permet par exemple de copier des textes utilisant des caractères de différents alphabets entre des logiciels différents, même n'ayant pas été spécifiquement conçus pour eux (par exemple un programme en caractères APL dans un texte LibreOffice ou dans un courriel sous Gmail). Tous les textes Unicode ne sont cependant pas codés de la même manière. Suivant la normalisation Unicode adoptée, un même signe graphique peut parfois être codé de différentes manières. Certains textes utilisent la convention NFC, d'autres la convention NFD, etc. Et le standard n'interdit pas de mélanger plusieurs conventions dans un même texte. Il en va de même des logiciels.

Cette coexistence de plusieurs façons d'écrire la même chose a été exploitée par les pirates dans les années 2000 en leur permettant de déjouer des filtres : les pirates contournaient les interdictions de certaines chaînes de caractères jugées dangereuses tout simplement en les codant sous une autre forme, plus inhabituelle et de ce fait parfois non filtrée.

Unicode répond à ces limitations en apportant la notion d'équivalence canonique.

Le travail sur Unicode est parallèle et synchronisé avec celui sur la norme ISO/CEI 10646 dont les buts sont les mêmes. L'ISO/CEI 10646, une norme internationale publiée en français et en anglais, qui ne précise ni les règles de composition de caractères, ni les propriétés sémantiques des caractères.

Unicode aborde cependant la problématique de la casse, du classement alphabétique, et de la combinaison d'accents et de caractères. Depuis la version 1.1 d'Unicode et dans toutes les versions suivantes, les caractères ont les mêmes identifiants que ceux de la norme ISO/CEI 10646 : les répertoires sont maintenus parallèlement, à l'identique lors de leur normalisation définitive, les deux normes étant mises à jour presque simultanément. Les deux normes Unicode (depuis la version 1.1) et ISO/CEI 10646 assurent une compatibilité ascendante totale : tout texte conforme à une version antérieure doit rester conforme dans les versions ultérieures.

Ainsi les caractères de la version 3.0 d'Unicode sont ceux de la norme ISO/CEI 10646:2000. La version 3.2 d'Unicode classait caractères, symboles et directives.

La version 4.1 d'Unicode, mise à jour en , contient :
soit un total de près de de codes assignés dans un espace pouvant contenir codes différents.

Quelques problèmes semblent cependant exister, pour le codage des caractères chinois, à cause de l'unification des jeux idéographiques utilisés dans différentes langues, avec une calligraphie légèrement différente et parfois signifiante, mais ils sont en cours de résolution par Unicode qui a défini des sélecteurs de variantes et ouvert un registre de séquences normalisées qui les utilise.

La version 5.0 a été publiée en , la version 5.1 a été publiée en , la version 5.2 en , la version 6.0 en , la version 6.1 le .

La version 7.0 du apporte 2834 nouveaux caractères dont les emojis.

La version 8.0, qui ajoute 7716 caractères dont de nombreux emojis, est publiée le .

La version 9.0, qui ajoute 7500 nouveaux caractères (dont 72 emojis), est publiée le .

Unicode est défini suivant un modèle en couches ("Note technique Unicode "). Les autres normes ne faisaient typiquement pas de distinction entre le jeu de caractères et la représentation physique. Les couches sont ici présentées en partant de la plus haute (la plus éloignée de la machine).

La couche la plus élevée est la définition du jeu de caractères. Par exemple, Latin-1 a un jeu de 256 caractères et Unicode normalise actuellement près de caractères. En outre, Unicode leur donne des noms. Dresser la liste des caractères et leur donner des noms est donc la première couche d'Unicode.

Par exemple, le caractère Ç est nommé « lettre majuscule latine c cédille ».

Cette définition est totalement identique à celle de l'ISO/CEI 10646, qui approuve toute extension du répertoire. Unicode ne reprend dans le texte de sa norme que les noms normatifs en anglais, mais la norme ISO/CEI 10646 est publiée en deux langues également normatives. Ainsi les noms en anglais et en français sont tous deux normalisés.

Dans les faits, toute extension du répertoire se fait aujourd'hui conjointement entre le groupe de travail responsable de l'ISO/CEI 10646 ("JTC1/SC2/WG2", dont les membres votants sont uniquement des autorités de normalisation nationales des pays participants, ou leur représentant officiel), et le Comité technique Unicode "UTC" (dont les membres votants peuvent être n'importe quelle organisation privée ou d'intérêt public, ou même un gouvernement, qui a adhéré et paye une redevance annuelle leur permettant de participer à ces décisions).

Ici, on ajoute à la table précédente un index numérique associé à chaque caractère. Notons bien qu'il ne s'agit pas d'une représentation en mémoire, juste d'un nombre entier, appelé "point de code". L'espace de codage de ces nombres est divisé en 17 zones de points de codes. Ces zones sont appelées "plans".

Le point de code est noté U+xxxx où xxxx est en hexadécimal, et comporte 4 à 6 chiffres :

Ainsi, le caractère nommé « Lettre majuscule latine c cédille » (Ç) a un index de U+00C7. Il appartient au premier plan.

En principe toutes les points de code entre U+0000 et U+10FFFF sont disponibles, mais certains intervalles sont perpétuellement réservés à des usages particuliers, notamment une zone d'indirection exclue pour permettre le codage UTF-16 (cf. infra), les zones à usage privé et quelques régions (par exemple U+FFFE ou U+FFFF) contenant des "non-caractères" dont l'usage est interdit dans un échange de données conforme. Les autres points de code sont soit déjà assignées à des caractères, soit réservées pour normalisation future.

Zone à usage privé : Unicode a assigné de nombreux points de code à des caractères valides mais dont la sémantique est inconnue car d'usage privé (par exemple les deux derniers plans entre U+F0000 et U+10FFFF sont entièrement dédiés à cet usage, hormis les deux points de code à la fin de chaque plan qui sont des non-caractères interdits dans un texte conforme).

Là encore, la normalisation du codage, c'est-à-dire l'assignation des points de codes aux caractères du répertoire commun est une décision conjointe partagée entre les normes Unicode et ISO/CEI 10646. Tous les caractères du répertoire disposent d'un point de code unique (même si pour certaines langues ou pour Unicode certains caractères sont considérés comme équivalents).

On peut noter que si le répertoire des caractères est extensible, il est limité par la borne supérieure de l'espace de codage : U+10FFFF. Une grande majorité des points de code possibles n'est pas assignée à un caractère particulier, mais peut le devenir à tout moment.

Aussi ces points de code encore libres ne sont pas considérés comme invalides mais représentent bien des caractères abstraits (non encore spécifiés, et temporairement réservés). Ces caractères abstraits (de même que les caractères à usage privé) complètent le jeu de caractères codés du répertoire normalisé pour former un jeu unique dit « jeu de caractères codés universel » (, souvent abrégé en UCS) qui contient tous les jeux de caractères codés des répertoires de chacune des versions passées, présentes et futures de l'ISO/CEI 10646 et d'Unicode (depuis la version 1.1 uniquement).

Cette fois, nous arrivons à une représentation physique (en mémoire, sur disque, etc.) : cette couche spécifie quelle unité de codage (), ou codet, va représenter un caractère ou plus exactement un point de code : octet, (mot de 16 bits) ou trente-deuzet (mot de 32 bits).

Il peut exister (et il existe) plusieurs de ces formalismes. Un formalisme particulier doit préciser la taille de l'unité de codage et indiquer de quelle façon le nombre entier représentant un point de code est représenté en une suite d'unités de codage − et inversement, c'est-à-dire comment retrouver le point de code étant donnée une suite d'unités de codage.

Cette couche s'occupe de sérialiser les suites d'unités de codage définies par la couche précédente en suites d'octets. C'est ici que se choisit l'ordre des octets entre les ordres gros-boutien (octet le plus significatif d'abord) et petit-boutien (octet le moins significatif d'abord).

C'est également à cette étape qu'il est possible d'ajouter un indicateur d'ordre des octets (ou BOM, pour ), qui permet d'indiquer en début de fichier ou de flot de données s'il est en gros-boutien ou en petit-boutien. Dans le monde Internet, on l'utilise rarement, en préférant un marquage explicite (« "charset=UTF-16BE" » en MIME, par exemple, pour indiquer un flot de données gros-boutien, où "BE" signifie ).

Ici, interviennent optionnellement les mécanismes de compression ou de chiffrement.

Il peut aussi y avoir un surcodage comme pour le LDAP qui spécifie que les chaînes Unicode doivent être codées en UTF-8 et surcodées en Base64.

Pour s'affranchir des contraintes rigides des normes précédentes (une suite de bits, une représentation), Unicode sépare dorénavant d'une part la "définition du jeu de caractères" (liste des caractères par leur nom) et leur index, le "point de code", de celle du codage. Ainsi, on ne peut donc pas parler de la taille d'un caractère Unicode, car elle dépend du codage choisi, et celui-ci peut donc varier à volonté. En pratique, UTF-8 est très utilisé dans les pays occidentaux.

Là où l'ASCII utilisait jadis 7 bits et ISO 8859-1 8 bits (comme la plupart des pages de codes nationales), Unicode, qui rassemble les caractères de chaque page de code, avait besoin d'utiliser plus que les 8 bits d'un octet. La limite fut dans un premier temps fixée à 16 bits pour les premières versions d'Unicode, et à 32 bits pour les premières versions de la norme ISO/CEI 10646.

La limite actuelle est désormais placée entre 20 et 21 bits par point de code assigné aux caractères normalisés dans les deux normes, désormais mutuellement compatibles :

Unicode et ISO/CEI 10646 acceptent plusieurs formes de transformation universelle pour représenter un point de code valide. Citons :
Le nombre après UTF représente le nombre minimal de bits des codets avec lesquels un point de code valide est représenté.

Ces transformations ont été initialement créées pour la représentation interne et les schémas de codage des points de code de la norme ISO/CEI 10646, qui au départ pouvait définir des points de code sur 31 bits. Depuis, la norme ISO/CEI 10646 a été amendée, afin que les trois formes soient totalement compatibles entre elles et permettent de coder tous les points de code (car UTF-16 ne permet de représenter que les points de code des 17 premiers plans).

Unicode a normalisé également de façon très stricte ces trois formes de transformation de tous les points de code valides (U+0000 à U+D7FF et U+E000 à U+10FFFF) et uniquement eux, que ce soit pour représenter du texte sous forme de suites de points de code, ou des points de code assignés aux caractères valides, ou réservés, ou assignés à des non-caractères. Les points de code assignés aux demi-zones (U+D800 à U+DFFF), utilisés uniquement en UTF-16, sont invalides isolément puisqu'il servent à la représentation, par un couple de 2 codets de 16 bits, des points de code des 16 plans supplémentaires.

L'UTF-8, spécifié dans le , est le plus commun pour les applications Unix et Internet. Son codage de taille variable lui permet d'être en moyenne moins coûteux en occupation mémoire (pour les langues à alphabet latin). Mais cela ralentit nettement les opérations où interviennent des extractions de sous-chaînes, car il faut compter les caractères depuis le début de la chaîne pour savoir où se trouve le premier caractère à extraire.

L'UTF-8 assure aussi, et c'est son principal avantage, une compatibilité avec les manipulations simples de chaînes en ASCII dans les langages de programmation. Ainsi, les programmes écrits en C peuvent souvent fonctionner sans modification.

Initialement, l'UTF-8 pouvait coder n'importe quel point de code entre U+0000 et U+7FFFFFFF (donc jusqu'à 31 bits). Cet usage est obsolète et la norme ISO/CEI 10646 a été amendée pour ne plus supporter que les points de code valides des 17 premiers plans, sauf ceux de la "demi-zone" correspondant aux codets utilisés en UTF-16 pour la représentation sur deux codets des points de code des 16 plans supplémentaires. Aussi les séquences les plus longues en UTF-8 nécessitent au maximum , au lieu de 6 précédemment. De plus, UTF-8 a été amendé d'abord par Unicode puis par l'ISO/CEI 10646 pour ne plus accepter que la représentation la plus courte de chaque point de code ("unicité" du codage).

Son avantage sur l'UTF-16 (et l'UTF-32) est que les différences d'ordonnancement des octets composant un mot ("endianness") ne posent pas de problème dans un réseau de systèmes hétérogènes ; ainsi, cette transformation est utilisée aujourd'hui par la plupart des protocoles d'échange normalisés.

D'autre part, l'UTF-8 est totalement compatible pour la transmission de textes par des protocoles basés sur le jeu de caractères ASCII, ou peut être rendu compatible (au prix d'une transformation sur plusieurs octets des caractères non-ASCII) avec les protocoles d'échange supportant les jeux de caractères codés sur 8 bits (qu'ils soient basés sur ISO/CEI 8859 ou de nombreux autres jeux de caractères codés sur 8 bits définis par des normes nationales ou des systèmes propriétaires particuliers).

Son principal défaut est le codage de longueur très variable ( pour les points de code assignés aux caractères ASCII–ISO/CEI 646, 2 à pour les autres points de code), même si l'auto-synchronisation propre à l'encodage UTF-8 permet de déterminer le début d'une séquence à partir d'une position aléatoire (en effectuant au plus 3 lectures supplémentaires des codets qui précèdent). Cependant, cet encodage n'est pas conçu pour faciliter le traitement des chaînes de caractères : on lui préfère alors souvent l'UTF-16, parfois l'UTF-32 (gourmand en mémoire).


L'UTF-16 est un bon compromis lorsque la place mémoire n'est pas trop restreinte, car la grande majorité des caractères Unicode assignés pour les écritures des langues modernes (dont les caractères les plus fréquemment utilisés) le sont dans le plan multilingue de base et peuvent donc être représentés sur . La version française de l'ISO/CEI 10646 nomme ces mots de des « seizets », mais la version internationale les décrit cependant bien comme de classiques mots de composés de deux octets, et soumis aux règles usuelles de boutisme.

Les points de code des seize plans supplémentaires nécessitent une transformation sur deux mots de :

Comme la plupart des caractères couramment usités résident dans le plan de base, l'encodage des plans supplémentaires est souvent peu testé dans les logiciels, conduisant à des bugs ou des problèmes de sécurité même dans des logiciels largement diffusés. Certains cadres légaux, tels le GB 18030, peuvent demander le support des plans supplémentaires, ceux-ci contenant notamment des caractères présents dans les noms propres.

Il est possible de déterminer le début de la séquence de codage à partir d'un point quelconque d'un texte représenté en UTF-16 en effectuant au maximum une lecture supplémentaire, uniquement si ce codet est dans la demi-zone basse. Cette forme est plus économique et plus facile à traiter rapidement que l'UTF-8 pour la représentation de textes contenant peu de caractères ASCII (U+0000 à U+007F).

Toutefois, cette transformation possède deux schémas de codage incompatibles qui dépendent de l'ordonnancement des octets dans la représentation d'entiers sur . Pour résoudre cette ambiguïté et permettre la transmission entre systèmes hétérogènes, il est nécessaire d'adjoindre une information indiquant le schéma de codage utilisé (UTF-16BE ou UTF-16LE), ou bien de préfixer le texte codé avec la représentation du point de code valide U+FEFF (assigné au caractère « espace insécable de largeur nulle », un caractère aujourd'hui réservé à ce seul usage en tant que marqueur d'ordonnancement des octets), puisque le point de code « renversé » U+FFFE valide est un non-caractère, interdit dans les textes conformes à Unicode et ISO/CEI 10646.

L'autre défaut d'UTF-16 est qu'un texte transformé avec lui et transmis avec l'un ou l'autre des deux schémas de codage contient un grand nombre d'octets nuls ou ayant une valeur en conflit avec les valeurs d'octets réservées par certains protocoles d'échange.

C'est notamment le codage qu'utilise la plate-forme Java en interne, ainsi que Windows pour ses APIs compatibles Unicode (avec le type codice_1).

L'UTF-32 est utilisé lorsque la place mémoire n'est pas un problème et que l'on a besoin d'avoir accès à des caractères de manière directe et sans changement de taille (hiéroglyphes égyptiens).

L'avantage de cette transformation normalisée est que tous les codets ont la même taille. Il n'est donc pas nécessaire de lire des codets supplémentaires pour déterminer le début de la représentation d'un point de code.

Toutefois, ce format est particulièrement peu économique (y compris en mémoire) puisqu'il « gaspille » inutilement au moins un octet (toujours nul) par caractère . La taille en mémoire d'un texte joue négativement sur les performances puisque cela nécessite plus de lectures et écritures sur disque en cas de saturation de la mémoire vive, et que cela diminue aussi les performances du cache mémoire des processeurs.

Pour les textes écrits dans les langues modernes actuelles (hormis certains caractères rares du plan idéographique supplémentaire) et n'utilisant donc que les points de code du plan multilingue de base, cette transformation double la quantité mémoire nécessaire par rapport à l'UTF-16.

Comme l'UTF-16, l'UTF-32 possède plusieurs schémas de codage dépendant de l'ordonnancement des octets composant un entier de plus de 8 bits (deux schémas de codage de l'UTF-32 sont normalisés, UTF-32BE et UTF-32LE). Il est donc aussi nécessaire de préciser ce schéma de codage, ou de le déterminer en préfixant le texte par la représentation en UTF-32 du point de code U+FEFF. Comme l'UTF-16, la présence d'octets nuls dans les schémas de codage normalisés de l'UTF-32 le rend incompatible avec de nombreux protocoles d'échange entre systèmes hétérogènes.

Aussi ce format n'est utilisé le plus souvent que très localement pour certains traitements en tant que forme intermédiaire plus facile à manipuler, et on lui préfère souvent la transformation UTF-16 souvent plus performante pour traiter et stocker des quantités importantes de textes, la conversion entre les deux étant très simple à réaliser, et très peu coûteuse en termes de complexité de traitement.

En fait, de très nombreuses bibliothèques de traitement de textes sont écrites uniquement avec l'UTF-16 et sont plus performantes qu'en UTF-32, même lorsque les textes contiennent des caractères des plans supplémentaires (car ce cas de figure reste rare dans la très grande majorité des cas).

On notera toutefois que la transformation en UTF-32 utilise des codets sur 32 bits, dont de très nombreuses valeurs peuvent ne représenter aucun point de code valide (valeurs hors des deux intervalles représentant les points de code valides U+0000 à U+D7FF et U+E000 à U+10FFFF), donc aucun caractère valide ou réservé (toute information qui y serait contenue ne peut donc pas être du texte au sens d'Unicode). La transmission de textes utilisant ces valeurs invalides de codets dans un des schémas de codage normalisés de l'UTF-32 est interdite pour tout système conforme à Unicode (il faut utiliser plutôt les points de code à usage privé), puisqu'il sera impossible de les représenter dans une autre transformation UTF avec lesquelles les trois UTF normalisées sont bijectivement compatibles.

Il s'agit d'une transformation de l'Unicode qui n'est pas définie par le Consortium Unicode, mais par l'administration de normalisation en Chine, où son support est obligatoire dans les applications. Historiquement c'était un jeu de caractères codé, qui a été étendu pour supporter l'intégralité du répertoire UCS par une transformation algorithmique complétant une large table de correspondance d'un code à l'autre.

Affirmer qu'Unicode code des caractères revient à affirmer qu'il attribue un numéro à des symboles abstraits, selon un principe de codage logique. Unicode ne code en revanche pas les représentations graphiques des caractères, les glyphes. Il n'y a donc pas une bijection entre la représentation du caractère et son numéro, puisque toutes les variantes graphiques de style sont unifiées.

De plus, contrairement à une police ASCII ou latin-1 classique, la sélection d'un glyphe par un code n'est pas unique et est souvent contextuelle, et peut aussi afficher le même glyphe pour des codes différents. Ainsi, le caractère français « é » peut-il être décrit de deux manières : soit en utilisant directement le numéro correspondant au « é », soit en faisant suivre le numéro du « e » par celui de l'accent aigu sans chasse. Quelle que soit l'option choisie, le même glyphe sera affiché. On dira du premier caractère qu'il est précomposé, du second que c'est une composition (deux caractères forment un seul glyphe composé des deux). Ceci est autorisé et même hautement recommandé car les différentes formes de codage sont classées par Unicode comme « canoniquement équivalentes », ce qui signifie que deux formes de codage équivalentes devraient être traitées de façon identique.

De nombreux caractères composites sont dans ce cas et peuvent être codés de ces deux manières (ou plus, certains caractères composés pouvant être décomposés de plusieurs façons, notamment quand ils comportent plusieurs signes diacritiques). Le plus souvent, le caractère précomposé est préférable pour le codage du texte, si celui-ci existe (c'est le cas pour le grec polytonique, par exemple, lequel, codé en décomposition, peut ne pas être satisfaisant graphiquement : selon les polices de caractères, les différents constituants du glyphe étant parfois mal disposés et peu lisibles). Toutefois, tous les caractères composites ne disposent pas d'un point de code unique pour leur forme précomposée.

De même, certains systèmes d'écriture, comme la devânagarî ou les caractères arabes, nécessitent un traitement complexe des ligatures : les graphèmes changent en effet de forme en fonction de leur position et/ou par rapport à leurs voisines (voir Variante contextuelle et Lettre conjointe). La sélection du glyphe correct à utiliser nécessite un traitement permettant de déterminer la forme contextuelle à sélectionner dans la police, alors même que toutes les formes contextuelles sont codées de façon identique en Unicode.

Pour ces raisons, la police Unicode doit être utilisée très prudemment. Avoir une police qui représente un certain nombre ou toutes les représentations graphiques que l'on peut obtenir avec Unicode n'est pas suffisant, il faut en plus que le système d'affichage possède les mécanismes de représentation idoines (le moteur de rendu) capable de gérer les ligatures, variantes contextuelles et formes conjointes de certaines écritures. Au contraire, une police qui ne représente que certains caractères mais qui sait comment les afficher mérite mieux le terme de « police Unicode ». Enfin, il existe des contraintes techniques dans les formats de polices de caractère, qui les empêchent de supporter la totalité du répertoire et, en pratique, il est en 2009 impossible de trouver une police de caractères unique supportant tout le répertoire.

Une police de caractères Unicode est donc seulement une police permettant d'afficher directement un texte codé selon toutes les formes autorisées par Unicode, et permettant de supporter un sous-ensemble cohérent adapté à une ou plusieurs langues pour supporter une ou plusieurs écritures. Aucune police de caractère Unicode ne peut « fonctionner » seule, et le support complet de l'écriture nécessite un support de celles-ci dans un moteur de rendu, capable de détecter les formes de codage équivalentes, rechercher les formes contextuelles dans le texte et sélectionner les différents glyphes d'une police codée avec Unicode, en s'aidant au besoin de tables de correspondances incluses dans la police elle-même.

La bibliothèque logicielle multi-plate-forme ICU permet de manipuler des données unicodées. Un support d'Unicode spécifique à certaines plates-formes (non compatible quant au code-source) est également fourni par les systèmes modernes (Java, MFC, GNU/Linux).

Les types à utiliser pour stocker des variables Unicode, sont les suivants :
Unicode souffre toutefois encore d'un faible support des expressions rationnelles par certains logiciels, même si des bibliothèques comme ICU et Java peuvent les supporter. Un tel support n'a pas encore été standardisé pour ECMAScript et n'est fourni qu'avec l'aide de bibliothèques créées avec le langage ou des interfaces d'interopérabilité avec d'autres systèmes (notamment avec CORBA, COM) ou langages (notamment C++ et Java).

Le partitionnement à jour peut être trouvé sur le site officiel d'Unicode. Cependant, étant donné le rôle important d'Unicode actuellement (ISO/CEI 10646), on décrira ici les principaux blocs de caractères. Les noms français sont les noms officiels d'ISO/CEI 10646, la norme internationale bilingue qui reprend les mêmes caractères qu'Unicode. Ils sont aussi officiels que les noms anglais.

L'ancienne norme Unicode 1.0 est obsolète et incompatible avec la norme ISO/CEI 10646 et la norme Unicode 1.1 et toutes ses versions ultérieures ; la principale incompatibilité est celle des blocs de caractères Hangul utilisés pour l'écriture de la langue coréenne qui ont changé de position et dont les anciens points de code ont depuis été assignés à d'autres blocs. La table ci-dessous est compatible avec ISO/CEI 10646 (toutes versions) et Unicode 1.1 (ou ultérieur).
N.B. La casse des noms de bloc n'est pas normative. « Latin de base » est donc équivalent à « LATIN DE BASE ».

Dans les tableaux suivants, tout nom de bloc ayant une note ramenant à un PDF officiel Unicode signifie que la page Wikipédia associée à ce bloc est inexistante ou erronée.

Les zones à usage privé ne contiennent pas les mêmes œils d'une police à l'autre et doivent donc être évités pour le codage de textes destinés aux échanges entre systèmes hétérogènes. Toutefois ces points de codes à usage privé sont valides et peuvent être utilisés dans tout traitement automatisé conforme aux normes Unicode et ISO/CEI 10646, y compris entre systèmes différents s'il existe un accord mutuel privé concernant leur usage.

En l'absence d'accord entre les deux parties, des systèmes utilisant ces caractères peuvent rejeter les textes les contenant, car les traitements qu'ils leur font subir pourraient ne pas fonctionner correctement ou causer des problèmes de sécurité ; les autres systèmes qui n'attribuent aucune fonction spéciale à ces caractères doivent en revanche les accepter comme valides et les conserver comme partie intégrante des textes, comme s'il s'agissait de symboles graphiques, même s'ils ne savent pas les afficher correctement.

Les non-caractères sont des points de code valides, mais ils ne sont pas (et ne seront jamais) assignés à des caractères normalisés. Leur usage dans le codage de textes transmis entre systèmes (même si identiques) est interdit, car il est impossible de les rendre compatibles avec les formes de transformation universelles normalisées (dont UTF-8, UTF-16, UTF-32) les schémas de codage correspondants, et les autres codages normalisés compatibles avec Unicode et ISO/CEI 10646 (BOCU-1, SCSU, différentes versions de la norme chinoise GB 18030, etc.). Toutefois certains systèmes les génèrent et les utilisent localement, mais pour un traitement strictement interne destiné à faciliter l'implémentation des algorithmes de traitement de textes utilisant les autres caractères normalisés.

Parmi ces derniers non-caractères figurent les points de code valides mais réservés aux demi-zones (privées ou non). Ces points de code ne peuvent pas être utilisés individuellement pour coder un caractère. Ils servent uniquement pour la forme de transformation universelle UTF-16 (et les schémas de codage correspondants) pour représenter sur deux codets (à 16 bits chacun) des points de code valides dans un des 16 plans complémentaires (certaines combinaisons de codets correspondent à des caractères valides de ces plans, standards ou privés, d'autres combinaisons peuvent ne représenter aucun caractère valide car elles correspondraient à des non-caractères de ces plans complémentaires, et sont donc interdites dans les textes conformes à la norme).

Les autres zones libres (non assignées à un bloc nommé normalisé, ou les points de code laissés libres et réservés dans les blocs nommés existants) sont réservés pour un usage ultérieur dans des versions futures d'Unicode et ISO/CEI 10646, mais sont valides. Tout système traitant des textes contenant ces points de code réservés doivent les accepter sans les filtrer. Unicode définit des propriétés par défaut pour les hypothétiques caractères correspondants, afin de préserver la compatibilité des systèmes (conformes à la norme Unicode) avec les futurs textes conformes qui les contiendraient. Aucune application conforme ne doit leur assigner un caractère ou une sémantique spéciale (les zones privées sont destinées à cet usage).









</doc>
<doc id="3107" url="https://fr.wikipedia.org/wiki?curid=3107" title="UTF-32">
UTF-32

UTF-32 est un codage des caractères défini par Unicode où chaque caractère est codé sur un mot de 32 bits.

Le codage était défini dans l’annexe 19 à la norme Unicode. Depuis, l’annexe est devenue obsolète, car UTF-32 fait partie intégrante de la norme Unicode, dans son chapitre 3 "Conformance" où elle est définie de façon très stricte.


</doc>
<doc id="3108" url="https://fr.wikipedia.org/wiki?curid=3108" title="UTF-8">
UTF-8

UTF-8 (abréviation de l’anglais "Universal Character Set Transformation Format" - ) est un codage de caractères informatiques conçu pour coder l’ensemble des caractères du « répertoire universel de caractères codés », initialement développé par l’ISO dans la norme internationale ISO/CEI 10646, aujourd’hui totalement compatible avec le standard Unicode, en restant compatible avec la norme ASCII limitée à l’anglais de base, mais très largement répandue depuis des décennies.

L’UTF-8 est utilisé par 82,2 % des sites web en décembre 2014, puis 87,6 % en 2016 et près de 90,5 % en 2017. Par sa nature, UTF-8 est d’un usage de plus en plus courant sur Internet, et dans les systèmes devant échanger de l'information. Il s’agit également du codage le plus utilisé dans les systèmes GNU, Linux et compatibles pour gérer le plus simplement possible des textes et leurs traductions dans tous les systèmes d’écritures et tous les alphabets du monde.

UTF-8 est un « format de transformation » issu à l’origine des travaux pour la norme ISO/CEI 10646, c’est-à-dire que UTF-8 définit un codage pour tout point de code scalaire (caractère abstrait ou « non-caractère ») du répertoire du jeu universel de caractères codés ("Universal Character Set", ou "UCS"). Ce répertoire est aujourd’hui commun à la norme ISO/CEI 10646 (depuis sa révision 1) et au standard Unicode (depuis sa version 1.1).

UTF-8 est officiellement défini dans la norme ISO/CEI 10646 depuis son adoption dans un amendement publié en 1996. Il fut aussi décrit dans le standard Unicode et fait partie de ce standard depuis la version 3.0 publiée en 2000. En 1996 fut publiée la (« "UTF-8, a transformation format of ISO 10646" ») dans le but de fournir une spécification accessible d’UTF-8 et d’entamer sa standardisation au sein de l’"Internet Engineering Task Force" (IETF). Cette RFC fut révisée en 1998 () puis finalement en 2003 (), cette dernière version faisant d’UTF-8 un des standards de l'internet (STD 63).

Techniquement, il s’agit de coder les caractères Unicode sous forme de séquences de un à quatre codets d’un octet chacun. La norme Unicode définit entre autres un ensemble (ou répertoire) de caractères. Chaque caractère est repéré dans cet ensemble par un index entier aussi appelé « point de code ». Par exemple le caractère « € » (euro) est le 8365 caractère du répertoire Unicode, son index, ou point de code, est donc 8364 (on commence à compter à partir de 0).

Le répertoire Unicode peut contenir plus d’un million de caractères, ce qui est bien trop grand pour être codé par un seul octet (limité à des valeurs entre 0 et 255). La norme Unicode définit donc des méthodes standardisées pour coder et stocker cet index sous forme de séquence d’octets : UTF-8 est l'une d’entre elles, avec UTF-16, UTF-32 et leurs différentes variantes.

La principale caractéristique d’UTF-8 est qu’elle est rétro-compatible avec la norme ASCII, c’est-à-dire que tout caractère ASCII se code en UTF-8 sous forme d’un unique octet, identique au code ASCII. Par exemple « A » (A majuscule) a pour code ASCII 65 et se code en UTF-8 par l'octet 65. Chaque caractère dont le point de code est supérieur à 127 (caractère non ASCII) se code sur 2 à . Le caractère « € » (euro) se code par exemple sur : 226, 130, et 172.

Le numéro (valeur scalaire) de chaque point de code dans le jeu universel de caractères (UCS) est donné par la norme ISO/CEI 10646 qui assigne un point de code à chaque caractère valide, puis permet leur codage en leur attribuant une valeur scalaire identique au point de code ; cette norme est reprise dans le standard Unicode (qui utilise depuis la version 1.1 le même répertoire).

Tous les « points de code » ("code points" en anglais) de U+0000 à U+D7FF et de U+E000 à U+10FFFF sont représentables en UTF-8 et uniquement ceux-là. Les seuls points de codes valides dans l’espace de l’UCS et qui ne doivent pas être représentés dans UTF-8 sont ceux qui sont attribués aux « demi-codets » ("surrogates" en anglais), car ils ne sont pas représentables de façon bijective dans le codage UTF-16 et ne sont pas non plus par eux-mêmes des caractères : contrairement aux autres points de codes, les demi-codets n’ont donc "pas" de « valeur scalaire » ("scalar value" en anglais) définie.

Les points de code ayant une valeur scalaire de 0 à 127 (points de codes U+0000 à U+007F, attribués aux caractères du jeu codé sur 7 bits dans l’ASCII) sont codés sur un seul octet dont le bit de poids fort est nul.

Les autres points de code (attribués ou non à des caractères) ayant une valeur scalaire supérieure à 127 (sauf ceux auxquels sont attribués des « demi-codets » qui ne sont pas eux-mêmes des caractères) sont codés sur plusieurs octets ayant chacun leur bit de poids fort non nul : les bits de poids fort du premier octet de la séquence codée forment une suite de 1 de longueur égale au nombre total d’octets (au moins 2) utilisés pour la séquence entière suivie d'un 0 et les octets suivants nécessaires ont leurs deux bits de poids fort positionnés à 10.

Ce principe pourrait être étendu jusqu’à huit octets pour un seul point de code (pour représenter des points de code comprenant jusqu’à 42 bits), mais la version normalisée actuelle d’UTF-8 pose la limite à quatre.

Le codage interdit la représentation des points de code réservés aux "demi-codets" (qui n’ont pas de valeur scalaire définie, afin de préserver la compatibilité avec UTF-16 qui ne permet pas non plus de les représenter). Il autorise cependant la représentation des points de code assignés à des "non-caractères" (alors même que leur présence est interdite dans un texte conforme).

Dans toute chaîne de caractères codée en UTF-8, on remarque que :

Le plus grand point de code valide assignable à un caractère valide "non privé" est U+EFFFD dans le (il n’est pas encore assigné mais peut le devenir dans l’avenir), mais le codage UTF-8 peut être utilisé aussi, de façon conforme aux normes, pour représenter n’importe caractère valide à usage privé (dans une des trois plages U+E000 à U+F8FF, U+F0000 à U+FFFFD, et U+100000 à U+10FFFD).

L’acceptation ou non des "non-caractères" ou des "caractères d’usage privé" est laissée aux applications ou protocoles de transport de texte. Cependant les "non-caractères" ne sont normalement pas acceptés dans des textes strictement conformes au standard Unicode où à la norme ISO/CEI 10646.

Certaines applications imposent des restrictions supplémentaires sur les points de code utilisables (par exemple, les standards HTML et XML interdisent, dans tout document conforme à ces spécifications, la présence de la plupart des caractères de contrôle entre U+0000 et U+001F et entre U+0080 et U+009F, en dehors du contrôle de la tabulation U+0009 considéré comme un caractère blanc, et interdisent aussi les "non-caractères").

Tout point de code est toujours représenté par exactement la même séquence binaire, quelle que soit sa position relative dans le texte, et ces séquences sont autosynchronisées sur la position indivise des codets significatifs (ici les octets : on peut toujours savoir si un octet débute ou non une séquence binaire effective) ; ce codage autorise donc les algorithmes rapides de recherche de texte, tels que l’algorithme de Boyer-Moore.

Ce n’est pas toujours le cas des codages contextuels (qui utilisent généralement la compression de données, par exemple SCSU défini dans la note technique standard UTS#6 optionnelle complétant le standard Unicode) et qui peuvent nécessiter de lire le texte complètement depuis le début, ni des codages basés sur plus d’une seule variable d’état (ou qui incorporent des codes supplémentaires de redondance) ; au mieux certains de ces codages peuvent demander d’utiliser des algorithmes complexes de resynchronisation, basés souvent sur des heuristiques qui peuvent échouer ou conduire à de fausses interprétations si on ne lit pas le texte depuis le début (par exemple BOCU-1).

Dans le tableau ci-dessus, on voit que le caractère « € » se trouve au point de code U+20AC, soit en décimal 8364, ou en binaire : 100000 10101100.

Ce dernier nombre comporteformula_1 chiffres binaires significatifs, donc au moins sont nécessaires pour encoder le caractère « € ». La norme présentée ci-dessus impose en réalité trois octets pour représenter ces caractères.

Avec quatre octets à disposition, il serait possible de placer selon cette norme jusqu’à , donc en particulier de représenter le caractère « € » par "00000 00"100000 10101100, en lui ajoutant en tête non significatifs. Toutefois, la norme impose qu’un programme décodant l’UTF-8 ne doit pas accepter de chaînes d’octets inutilement longues comme dans cet exemple, ce pour des raisons de sécurité (éviter l’exploitation de tests de sous-chaînes trop tolérants). Ainsi « € » se codera : 11100010 10000010 10101100, mais le codage 11110000 10000010 10000010 10101100, déduit de la représentation de « € » sur , bien qu’univoque, ne doit pas être utilisé.

Une telle forme plus longue que nécessaire s’appelle en anglais "". De telles formes (initialement autorisées dans des spécifications anciennes avant qu’elles soient normalisées successivement par la RFC initiale publiée par le Consortium X/Open, puis parallèlement par la norme ISO 10646 et le standard Unicode) sont interdites et doivent être traitées comme invalides.

Le codage est prédictif et permet toujours de retrouver la position du premier octet d’une séquence représentant un point de code, à partir de la valeur d’un octet quelconque et de la lecture d’un nombre limité d’octets voisins, dans les deux directions de lecture (ce sera toujours l’octet lui-même, ou le premier éligible dans un des 1 à voisins).



De telles séquences sont dites mal formées ("ill-formed"). (Voir la référence ci-dessus, notamment la seconde table dans la clause de conformité "D36" du standard ou l’article Unicode).

En revanche, les points de code réservés (pas encore alloués à des caractères) sont autorisés (même si l’interprétation des caractères peut rester ambigüe) : il appartient aux applications de décider si ces caractères sont acceptables ou non, sachant que les mêmes applications continueront probablement à être utilisées alors que ces positions auront été assignées dans les normes Unicode et ISO 10646 à de nouveaux caractères parfaitement valides.

De même les autres points de code assignés de façon permanente aux autres « "non-caractères" » sont interdits dans les textes conformes à la norme ISO/CEI 10646 ou au standard Unicode : par exemple U+"x"FFFE à U+"x"FFFF (où x indique un numéro de plan hexadécimal de 0 à 10). Mais ils restent encodables et décodables en tant que tels en UTF-8 (les "non-caractères" sont à disposition des applications qui peuvent en faire un usage au sein d’API internes, par exemple comme codes intermédiaires nécessaires à l’implémentation de certains traitements).

La restriction de l’espace de représentation aux seuls points de code inférieurs ou égaux à U+10FFFF (non compris les points de codes assignés aux "demi-codets") n’a pas toujours été appliquée :

Un texte en US-ASCII est codé identiquement en UTF-8 (lorsque le BOM n’est pas utilisé).

Du fait qu’un caractère est découpé en une suite d’octets (et non en mots de plusieurs octets), il n’y a pas de problème d’"endianness" (ou « boutisme »).

Pour la plupart des langues à écriture latine, les fichiers de données numériques ou les codes sources de programmes, ou de nombreux protocoles textuels de communication (comme FTP, HTTP ou MIME), qui utilisent abondamment (voire parfois exclusivement dans certaines parties) les caractères US-ASCII, UTF-8 nécessite moins d’octets que l’UTF-16 ou l’UTF-32.

De nombreuses techniques de programmation informatique valables avec les caractères uniformément codés sur un octet le restent avec UTF-8, notamment :

Il s’agit d’un codage auto-synchronisant (en lisant un seul octet on sait si c’est le premier d’un caractère ou non).

Les points de code sont représentés en UTF-8 par des séquences d’octets de taille variable (de même qu’en UTF-16), ce qui rend certaines opérations sur les chaînes de points de code plus compliquées : le calcul du nombre de points de code ; le positionnement à une distance donnée (exprimée en nombre de points de code) dans un fichier texte et en règle générale toute opération nécessitant l’accès au point de code de position "N" dans une chaîne.

Une taille variable des caractères d’une chaine empêche l'exploitation d’algorithme efficaces en matière de comparaisons de chaines, telles que l'algorithme de et pénalise donc fortement les traitements de données en masse comme dans l’exploitation des bases de données. Ce problème est toutefois davantage lié aux aspects de normalisation que d’encodage.

Pour les langues utilisant beaucoup de caractères extérieurs à US-ASCII, UTF-8 occupe sensiblement plus d’espace. Par exemple, les idéogrammes courants employés dans les textes de langues asiatiques comme le chinois ou le japonais (kanji, par exemple) utilisent en UTF-8 contre en UTF-16.

De manière générale, les écritures employant beaucoup de points de code de valeur égale ou supérieure à U+0800 occupent plus de mémoire que s’ils étaient codés avec UTF-16 (UTF-32 sera plus efficace uniquement pour les textes utilisant majoritairement des écritures anciennes ou rares codées hors du plan multilingue de base, c’est-à-dire à partir de U+10000, mais il peut aussi s’avérer utile localement dans certains traitements pour simplifier les algorithmes, car les caractères y ont toujours une taille fixe, la conversion des données d’entrée ou de sortie depuis ou vers UTF-8 ou UTF-16 étant triviale).

De fait, un logiciel détectant certaines chaînes de caractères (pour prévenir les injections SQL, par exemple) pouvait échouer dans sa tâche (ce n’est plus le cas si la conformité du codage avec la définition stricte et normalisée d’UTF-8 est vérifiée avant toute chose).

Prenons un exemple tiré d’un cas réel de virus attaquant des serveurs HTTP du Web en 2001 (Crypto-Gram: July 15, 2000 Microsoft IIS and PWS Extended Unicode Directory Traversal Vulnerability Microsoft IIS 4.0/5.0 Web Directory Traversal Vulnerability). Une séquence à détecter pourrait être « /../ » représentée en ASCII ("a fortiori" en UTF-8) par les octets « codice_1 » en notation hexadécimale. Cependant, une manière malformée de coder cette chaîne en UTF-8 serait « codice_2 », appelée aussi en anglais "overlong form (forme superlongue)". Si le logiciel n’est pas soigneusement écrit pour rejeter cette chaîne, en la mettant par exemple sous forme canonique, une brèche potentielle de sécurité est ouverte. Cette attaque est appelée "directory traversal".

Les logiciels acceptant du texte codé en UTF-8 ont été blindés pour rejeter systématiquement ces formes longues car non conformes à la norme : soit le texte entier est rejeté ; mais parfois les séquences invalides sont remplacées par un caractère de substitution (généralement U+FFFD si l’application accepte et traite ce caractère normalement ; parfois un point d’interrogation ou le caractère de contrôle de substitution "SUB" U+001A de l’ASCII, qui peuvent poser d’autres problèmes de compatibilité) ; moins souvent, ces séquences interdites sont éliminées silencieusement (ce qui est très peu recommandé).

UTF-8 ne peut représenter le caractère de contrôle nul (U+0000) qu’avec un seul octet nul, ce qui pose des problèmes de compatibilité avec le traitement de chaînes qui ne codifient pas séparément leur longueur effective car cet octet nul ne représente alors aucun caractère mais la fin de chaîne (cas très courant en langage C ou C++ et dans les API des systèmes d’exploitation). Si un caractère nul doit être stocké dans un texte sur de tels systèmes, il sera nécessaire de recourir à un système d’échappement, spécifique de ce langage ou système avant de coder en UTF-8 le texte ainsi transformé. En pratique, aucun texte valide ne devrait contenir ce caractère. Une autre solution est d’utiliser une des séquences interdites dans le codage UTF-8 standard afin de coder le caractère par cette séquence ; mais le texte ainsi codé ne sera pas conforme au codage UTF-8 standard, même si le codage ainsi modifié reste un format de transformation universelle conforme (qui ne doit cependant pas être désigné comme « UTF-8 »). Voir la section ci-dessous relative aux variantes non standards basées sur UTF-8.

UTF-8 a été inventé par Kenneth Thompson lors d’un dîner avec Rob Pike aux alentours de septembre 1992. Appelé alors "FSS-UTF", il a été immédiatement utilisé dans le système d’exploitation Plan 9 sur lequel ils travaillaient. Une contrainte à résoudre était de coder les caractères nul et '/' comme en ASCII et qu’aucun octet codant un autre caractère n’ait le même code. Ainsi les systèmes d’exploitation UNIX pouvaient continuer à rechercher ces deux caractères dans une chaîne sans adaptation logicielle.

FSS-UTF a fait l'objet d’un standard préliminaire X/Open de 1993 qui fut proposé à l'ISO. Cette dernière l'adopta dans le cadre de la norme ISO/CEI 10646 sous le nom d’abord d’UTF-2, puis finalement UTF-8.

Le codage original FSS-UTF était destiné à remplacer le codage multi-octets UTF-1 initialement proposé par l'ISO 10646. Ce codage initialement permissif, permettait plusieurs représentations binaires pour le même caractère (cela a été interdit dans la version normalisée dans la RFC publiée par le Consortium X/Open, et approuvé par Kenneth Thompson).

De plus il pouvait (dans une version préliminaire non retenue) coder tous les caractères dont la valeur de point de code comprenait jusqu’à en définissant un huitième type d’octet (dans des séquences comprenant jusqu’à ), au lieu des d’octets finalement retenus pour ne coder (dans des séquences comprenant aussi jusqu’à ) que les points de code jusqu’à dans la version initiale d’UTF-8 (publiée par le Consortium X/Open sous le nom FSS-UTF, puis proposé par le comité technique d’ISO 10646 comme la proposition « UTF-2 » alors encore en concurrence avec la proposition « UTF-1 », jusqu’à ce que la proposition UTF-2 soit retenue et adopte le nom UTF-8 déjà retenu et utilisé dans X/Open et Plan 9).

Ce codage UTF-8 a été restreint encore lorsque Unicode et ISO 10646 sont convenus de n’allouer des caractères que dans les plans afin de maintenir indéfiniment la compatibilité avec UTF-16 (sans devoir le modifier), en restreignant les séquences jusqu’à seulement et en n’utilisant que les des d’octets (ce qui a nécessité de définir comme invalides de nouvelles valeurs d’octet et certaines séquences d’octets pourtant valides individuellement).

L’IETF exige maintenant qu’UTF-8 soit pris en charge par défaut (et non pas simplement supporté en tant qu’extension) par tous les nouveaux protocoles de communication d’Internet (publiés dans ses RFC numérotées) qui échangent du texte (les plus anciens protocoles n’ont toutefois pas été modifiés pour rendre ce support obligatoire, mais seulement étendus si possible, pour le supporter de façon optionnelle, si cela produit des incompatibilités ou introduit de nouveaux risques de sécurité : c’est le cas de protocoles Internet très utilisés comme DNS, HTTP, FTP, Telnet et de HTML dans ses versions initiales alors pas encore standardisés par le W3C et l’ISO).

Il est devenu incontournable, notamment dans les principaux logiciels de communication du web et aujourd’hui les systèmes d’exploitation :



Toutefois, des variantes d’UTF-8 (basées sur les possibilités de codage de la version initiale non restreinte) ont continué à être utilisées (notamment dans l’implémentation de la sérialisation des chaînes Java) pour permettre de coder sous forme d’un échappement multioctets certains caractères ASCII réservés normalement codés sur un seul octet (par exemple le caractère nul).

De plus, certains systèmes utilisent des chaînes de caractères non restreints : par exemple, Java (et d’autres langages y compris des bibliothèques de manipulation de chaînes en C, PHP, Perl, etc.) représentent les caractères avec des unités de codage sur (ce qui permet de stocker les chaînes en utilisant le codage UTF-16, mais sans les contraintes de validité imposées par UTF-16 concernant les valeurs interdites et l'appariement dans l’ordre des « demi-codets » ou "surrogates") ; dans ce cas, les unités de codage sont traitées comme des valeurs binaires et il est nécessaire de les sérialiser de façon individuelle (indépendamment de leur interprétation possible comme caractères ou comme demi-points de code). Dans ce cas, chaque unité de codage qui représente un « caractère » (non-contraint) est sérialisé sous forme de séquences comprenant jusqu’à chacune, et certains octets interdits par l’implémentation (par exemple les caractères nuls ou la barre de fraction « / » dans un système de fichiers ou d’autres caractères codés sur un octet dans d’autres protocoles) sont codés sous forme de séquences d’échappement à deux octets dont aucun n’est nul, en utilisant simplement le principe de codage de la première spécification de FSS-UTF (avant celle qui a été retenue par le Consortium X/Open dans sa RFC initiale où ces échappements étaient spécifiquement interdits et le sont restés).

Avant l’adoption de la proposition UTF-2 retenue pour UTF-8, il a également existé une variante UTF-1, où les codages multiples étaient impossibles, mais nécessitait un codage/décodage plus difficile devant prendre en compte la position de chaque octet et utilisant un certain nombre de valeurs « magiques ».

Ces variantes ne doivent pas être appelées « UTF-8 ».

Une de ces variantes non standards a fait cependant l’objet d’une standardisation ultérieure (en tant qu’alternative à UTF-16 et utilisant des paires de « demi-codets » codés chacun sur 3 octets; soit en tout au lieu de UTF-8) : voir CESU-8.

Par exemple, les API d’intégration des machines virtuelles Java (pour JNI, "Java Native Interface" ou pour la sérialisation des classes précompilées), qui permettent d’échanger les chaînes Java non contraintes sous forme de séquences d’octets (afin de les manipuler, utiliser ou produire par du code natif, ou pour le stockage sous forme de fichier natif codés en suites d’octets), sont suffixées par « UTFChars » ou « UTF », mais ce codage propre à Java n’est pas UTF-8 (La documentation de Sun la désigne comme "modified UTF", mais certains documents plus anciens relatifs à JNI désignent encore ce codage incorrectement sous le nom "UTF-8", ce qui a produit des anomalies de comportement de certaines bibliothèques natives JNI, notamment avec les API systèmes d’anciennes plateformes natives qui ne supportent pas nativement les codages de caractères sur plus de ), car :


En conséquence :

Ces traitements peuvent être inefficaces pour l’interfaçage de grosses quantités de texte car ils demandent l'allocation de tampons mémoire supplémentaires pour s'interfacer ensuite dans le code natif avec des interfaces système ou réseau qui n'acceptent que l’UTF-8 standard.

Cependant JNI fournit aussi une API binaire plus efficace permettant d’utiliser UTF-16 directement, capable de s'interfacer directement avec les protocoles réseau et les interfaces système (par exemple les API Windows) qui supportent l’UTF-16, sans nécessiter aucune allocation mémoire supplémentaire pour le transcodage (seule la vérification de conformité peut être nécessaire, principalement pour vérifier dans le texte codé l'appariement correct des demi-codets ou "surrogate", que Java (comme aussi d’autres langages de programmation) permet de manipuler sans restriction de validité dans ses propres chaînes de caractères non dédiées au stockage des seuls textes conformes avec l’UCS). Cette API binaire est supportée sur tous les systèmes où Java a été porté, même ceux dont le système d’exploitation n’offre pas d’API de texte Unicode (la prise en charge pouvant se faire dans l’application native hôte ou en utilisant les bibliothèques standard fournies avec la machine virtuelle Java ou d’autres bibliothèques natives indépendantes.



</doc>
<doc id="3109" url="https://fr.wikipedia.org/wiki?curid=3109" title="UTF-16">
UTF-16

UTF-16 est un codage des caractères définis par Unicode où chaque caractère est codé sur une suite de un ou deux mots de 16 bits.

Le codage était défini dans le rapport technique 17 à la norme Unicode. Depuis, cette annexe est devenue obsolète car UTF-16 fait partie intégrante de la norme Unicode, dans son chapitre 3 "Conformance" qui la définit de façon très stricte.

L'UTF-16 n'est pas l'UCS-2 qui est le codage, plus simple, de chaque caractère sur deux octets. Ces deux normes sont pourtant appelées toutes les deux Unicode, car le codage est le même tant que l'on n'utilise pas les plages U+D800 à U+DFFF (en principe réservées) et les plages après U+FFFF (peu utilisées en occident).

L'UTF-16 est en particulier utilisé dans les environnements Windows.
Dans ce système, les API dites Unicode utilisent ce standard. Il en va de même du système NTFS.

UTF-16 est le standard de chaînes de caractères utilisé par l'UEFI.

Le numéro de chaque caractère (son point de code) est donné par la norme ISO/CEI 10646, et repris à l'identique par le standard Unicode. Les points de code qui peuvent être représentés doivent être dans l’intervalle de validité U+0000 à U+10FFFF, et ne doivent pas être affectés à un non-caractère. Tous les caractères possibles dans Unicode possèdent de tels points de codes.

Tout point de code qui n’est pas un non-caractère, et dont la valeur peut être codée sur un seul codet de deux octets (16 bits), c’est-à-dire tout point de code U+0000 à U+D7FF et U+E000 à U+FFFD, est stocké sur un seul mot de 16 bits (la plage de non-caractères U+D800 à U+DFFF est donc exclue, c'est-à-dire les points de code dont les 5 bits de poids fort sont 11011).

Dans les autres cas, le caractère est un point de code d’un plan supplémentaire (donc entre U+10000 et U+10FFFD et dont les 16 bits de poids faible ne doivent pas égaler 0xFFFE ou 0xFFFF) ; il est alors stocké sur 2 mots (codets) successifs de 16 bits chacun, dont les valeurs correspondent aux points de codes réservés dans les "demi-zones d’indirection" allouées dans le plan multilingue de base des normes Unicode et ISO/CEI 10646 :

Puis suivant le format de stockage des mots de 16 bits dans un flux ordonné d’octets, deux systèmes sont possibles pour le codage final :

L’indication du type de codage utilisé (ordre des octets) peut être implicite pour le protocole utilisé, ou précisé explicitement par ce protocole (en indiquant par exemple les noms réservés "UTF-16BE" ou "UTF-16LE" dans un entête de MIME). Si le protocole ne permet pas de spécifier l’ordre des octets, et s’il permet l’une ou l’autre des alternatives, on pourra utiliser le codage UTF-16 du point de code valide U+FEFF comme indicateur en tête du flux de données (car un changement d’ordre de ses octets à la lecture du flux conduira à un point de code U+FFFE, valide dans Unicode mais affecté à un non-caractère et donc interdit dans ce cas dans tout flux UTF-16. Ce point de code ainsi représenté (appelé marque d’ordonnancement des octets, en anglais, abrégé "BOM") ne sera codé qu’au début du flux de données, et permet de savoir comment a été codé le flux :

Si l’une des deux séquences de deux octets chacune est présente en tête de flux, le type de codage en est déduit et la séquence est retirée du flux : elle ne représente aucun caractère du texte stocké dans ce flux de données. Si aucune des deux séquences ne figure en tête du flux de données, la norme Unicode spécifie que le flux doit être décodé en big endian (UTF-16BE).

Ailleurs qu’au début du flux (y compris après un "BOM" initial), ces séquences ne sont pas reconnues comme codant un "BOM" et le décodage se poursuit avec un type de codage unique ; donc si ces séquences apparaissent après le début, alors :

De même le flux doit être considéré comme invalide et ne contenant pas de texte conforme à Unicode s’il contient un mot de 16 bits compris entre 0xD800 et 0xDBFF non immédiatement suivi d’un mot compris entre 0xDC00 et 0xDFFF, ou s’il contient un mot de 16 bits entre 0xDC00 et 0xDFFF non immédiatement précédé d’un mot entre 0xD800 et 0xDBFF, ou si le décodage fait apparaître le point de code de tout autre non-caractère.


</doc>
<doc id="3111" url="https://fr.wikipedia.org/wiki?curid=3111" title="Union des républiques socialistes soviétiques">
Union des républiques socialistes soviétiques

L’Union des républiques socialistes soviétiques, abrégé en URSS, ou en Union soviétique (en russe : , abrégé en : ; transcription : "Soïouz Sovietskikh Sotsialistitcheskikh Riespoublik", "SSSR" ; littéralement « Union des républiques socialistes des conseils »), est un État fédéral à régime communiste, formé de quinze Républiques socialistes soviétiques dites « unionales » en URSS : , qui a existé du jusqu'à sa dissolution le .

Plus vaste État du monde, l'URSS occupait un sixième des terres émergées et s'étendait sur onze fuseaux horaires, de la mer Baltique et de la mer Noire à l'océan Pacifique, c'est-à-dire toute la partie nord-est de l'Eurasie. Elle reprenait à peu près le territoire de l'ancien Empire russe (à l'exception notable de la majeure partie de la Pologne et de la Finlande, indépendantes depuis la guerre civile russe de 1918 à 1921) et s'était augmentée des gains territoriaux de la période stalinienne en Europe orientale et en Asie de l'Est entre 1939 et 1945.

Le territoire de l'URSS varia donc dans le temps, surtout avant et à l'issue de la Seconde Guerre mondiale. Le pays était composé, avant sa dissolution, de quinze républiques fédérées, ainsi que d'un certain nombre de républiques et régions autonomes.

La formation de l'URSS fut l'une des conséquences de la révolution russe de 1917. Après la révolution de Février (1917), qui avait mis fin au règne de l'empereur Nicolas II, le coup d'État de Lénine qui renversa la République russe le , permit la prise du pouvoir par les bolcheviks, qui étaient fédéralistes. L'un des moteurs de la création de l'URSS fut la volonté de Lénine d'appliquer sa doctrine fédéraliste en transformant la Russie unitaire en une union de républiques formées selon le principe de la répartition ethnique et jouissant d'un certain degré d'autonomie culturelle locale. Sa conception s'opposait initialement à celle unitariste de Joseph Staline, qui voulait créer une seule "République socialiste fédérative soviétique de Russie". Toutefois, Staline revint ultérieurement sur ses positions et, dans les , procéda lui-même à la création de plusieurs républiques fédérées (dans le Caucase, en Carélie et en Asie centrale).

L'organisation politique de l'URSS était définie par un parti unique, le Parti communiste de l'Union soviétique (PCUS) et tout particulièrement, par son bureau exécutif, le Politburo. Tout autre pouvoir (législatif, exécutif ou judiciaire), ainsi que la presse et la société civile dans son ensemble, étaient directement soumis aux oukases de l'appareil du PCUS. Il s'agissait d'un État totalitaire.

L'Union soviétique se fragmenta dans le courant de sous l'effet conjugué de plusieurs facteurs, qui avaient été analysés dès 1970 par Andreï Amalrik :

Pour enrayer ce processus, un programme de réformes fut engagé en par le secrétaire général du Parti communiste d'Union soviétique puis premier (et dernier) président de l'URSS Mikhaïl Gorbatchev, sur le double thème de la "perestroïka" (« restructuration ») et de la "glasnost" (« transparence »), mais en fait, au lieu d'enrayer le délitement, ce programme eut l'effet d'un catalyseur pour toutes les forces centrifuges, car la majorité des citoyens, et même des dirigeants, ne croyait déjà plus en la capacité de régénération du régime.

L'URSS était parfois, dans le langage courant, désignée sous le nom de "Russie" ou de "Russie soviétique". Cette appellation, impropre mais fréquente, l'assimilait à la République socialiste fédérative soviétique de Russie, qui était, de loin, la plus importante des républiques soviétiques, tant du point de vue de sa surface, de sa population, que de sa puissance politique et culturelle (le russe étant la langue de communication de toute l'Union), ainsi que la composante d'origine de la fédération sur le plan chronologique et de la diffusion de la population russe dans toute l'Union. L'ex-RSFS de Russie, devenue fédération de Russie le 26 décembre 1991, est actuellement considérée comme l'héritière de l'URSS du point de vue diplomatique, et a notamment hérité de son siège de membre permanent au Conseil de sécurité des Nations unies et de ses dettes (qu'elle a fini de payer en 2017).

Le mot " Soviet " est dérivé d'un mot russe signifiant conseil, assemblée, conseil, harmonie, concorde, et tous dérivent finalement de la racine verbale proto-slave de * vět-iti "informer", liée au slave "věst" ("news"), anglais "wise", la racine dans "ad-vis-or" (qui est venu à l'anglais par le français), ou le néerlandais "weten" (savoir, voir "wetenschap" = science ). Le mot "sovietnik" signifie conseiller. 

Un certain nombre d'organisations dans l'histoire russe ont été appelés "conseil" ( russe : совет ). Par exemple, dans l' Empire russe , le Conseil d'État , qui fonctionna de 1810 à 1917, fut appelé Conseil des ministres après la révolte de 1905. 

Pendant l'Affaire géorgienne, Lénine a envisagé une expression du chauvinisme ethnique russe par Joseph Staline et ses partisans, appelant ces Etats-nations à rejoindre la Russie en tant que parties semi-indépendantes d'une union plus grande, qu'il a d'abord nommée l' Union des Républiques Soviétiques d'Europe et d'Asie ( Russe : Союз Советских Республик Европы и Азии , "Soyúz Sovétskikh Respúblik Evrópy i Ázii" ). Staline a initialement résisté à la proposition, mais finalement l'a acceptée, bien que - avec l'accord de Lénine - il ait changé le nom de l'état nouvellement proposé à l' Union des Républiques Socialistes Soviétiques , bien que toutes les républiques aient commencé comme "Soviétique Socialiste" l'autre ordre jusqu'en 1936 . En outre, dans les langues nationales de plusieurs républiques, le mot «conciliaire / conciliaire» dans la langue respective ne fut modifié que tardivement en une adaptation du «Soviet» russe - et jamais dans d'autres, par exemple l' Ukraine .

Les noms de l'Union Soviétique sont les suivants dans plusieurs langues de ses 15 républiques constitutives:
Dans certains cas, en raison de la longueur de son nom, l'État était appelé «Union soviétique» ou «URSS», surtout lorsqu'il était utilisé dans les médias occidentaux . Il était également appelé de manière informelle "Russie" (et ses citoyens "Russes" ), bien que techniquement incorrect puisque la Russie n'était qu'une des républiques. 

Durant son existence, l'URSS était le pays le plus étendu du monde (). C'était également l'un des pays les plus variés, avec plus de cent « nationalités » (ethnies) recensées sur son territoire, une soixantaine de langues et cinq religions. La population totale était estimée à en 1990 (dite peuple soviétique). Aujourd'hui la Russie demeure toujours le pays le plus étendu du monde et reste un pays très divers, administrant des centaines de minorités, y compris musulmanes telles que les Tatars, et bien d'autres ethnies non russes. Elle a conservé, à une seule exception près, les frontières "de jure" de la République socialiste fédérative soviétique de Russie telles qu'elles étaient en 1945. Toutefois, des territoires contrôlés "de facto" s'y sont ajoutés depuis 1991.

Entre 1917 et 1940, plusieurs républiques de conseils (ou de soviets) se sont constituées, certaines avant la fondation de jure de l'URSS, d'autre après sa fondation:

Entre 1940 et 1954, le territoire de l'Union soviétique varie entre perte, dût aux conquêtes allemandes, et gains:

Entre 1954 et 1991, l'Union soviétique était composée de quinze républiques socialistes soviétiques (RSS) :

Chaque république fédérée était, à son tour, divisée en régions ("oblast"), à l'exception des RSS de Lettonie, de Lituanie, d'Estonie, de Moldavie et d'Arménie qui avaient une structure unitaire. La RSFS de Russie disposait, en plus, de « pays » ("kraï") qui étaient divisés en régions autonomes, ainsi que d'arrondissements autonomes faisant partie des oblasts et de kraïs. Certaines républiques fédérées (Russie, Géorgie, Azerbaïdjan, Ouzbékistan et Tadjikistan) avaient aussi dans leur structure des républiques autonomes, à certains degrés d'auto-gouvernance.

(Selon les chiffres officiels).


Dès le , la Russie tsariste connaît une agitation révolutionnaire qui va en s'aggravant après une révolution réprimée en 1905 et la défaite russe lors de la guerre russo-japonaise. Le mécontentement populaire culmine à la suite des pénuries causées par la Première Guerre mondiale et aboutit à la chute du gouvernement impérial et à l'abdication de en lors de la révolution de Février.

Le nouveau gouvernement de coalition démocrate prolonge l'engagement russe dans la guerre et peine à engager des réformes, entravé par des différends internes. Aussi à , un vaste soulèvement paysan spontané procède de lui-même au partage des terres, tandis que le gouvernement Kerenski perd ses appuis dans la population et la classe ouvrière, et que les forces de réaction, autour du général Kornilov, tentent vainement un coup d'État (« affaire Kornilov »). L'État perd progressivement son autorité sur le pays et l'armée se décompose.

Le Parti bolchevique, parti révolutionnaire marxiste mené par Lénine, devient progressivement majoritaire dans les conseils politiques ouvriers et paysans dits « Soviets ». Le (selon l'ancien calendrier julien) ou le , il renverse le gouvernement provisoire lors d'une révolution dite « révolution d'Octobre ». Le slogan de la révolution qui emporte l'adhésion des masses populaires est simple et percutant : , ce qui signifie nationalisations et armistice.

Ainsi, la jeune république bolchevique décide de se sortir de la Première Guerre mondiale en concluant une paix séparée avec l'Empire allemand. Un armistice signé en aboutit au traité de Brest-Litovsk en qui consacre, en pratique, la défaite de la Russie qui cède au vainqueur la majeure partie de l'Ukraine, la Biélorussie, les pays baltes et la Pologne . La Russie y perd 3,6 % de son territoire et 26 % de sa population. Elle perd aussi 32 % de sa production agricole, 23 % de sa production industrielle et 75 % de ses réserves de charbon.

Par ailleurs, la propriété privée industrielle est supprimée, les usines et les banques nationalisées. À la place, une propriété d'État est instaurée sur la quasi-totalité des moyens de production, sauf agricoles. Le marché libre disparaît et l'État acquiert le monopole du commerce intérieur et extérieur. Cette tendance au capitalisme d'État est néanmoins critiquée par des communistes comme Nikolaï Ossinski.

Lénine annule également les engagements russes sur les emprunts obligataires qui avaient été contractés par le gouvernement tsariste.

La jeune RSFS de Russie créée par la Constitution de 1918 fonctionne selon un principe fédéral, dont le principe de gouvernance est le centralisme démocratique. Le pouvoir législatif est théoriquement exercé par le « congrès panrusse des Soviets », lequel mandate le « Comité exécutif central panrusse », tant en matière législative qu'exécutive. Il appartient ainsi au Comité exécutif de contrôler le « Conseil des commissaires du peuple », lequel, avec Lénine à sa tête, a la charge de gouverner la RSFS de Russie. Cette apparence de démocratie ne survit pas à une analyse plus poussée : noyauté et contrôlé totalement par les bolcheviks, le congrès des Soviets, son Comité exécutif et donc le Conseil des commissaires du peuple, sont aux mains de Lénine et de ses camarades, et en particulier du Politburo du PCUS.

Ensuite, le pouvoir d'État devient bien plus strict en raison de la guerre civile, combinée à l'intervention ouverte des États occidentaux, qui fait rage jusqu'en 1921.

Pour faire face aux problèmes posés par la guerre civile russe et l'offensive militaire de pays étrangers (Allemagne, Angleterre, France, Japon, États-Unis), et afin d'assurer l'approvisionnement des villes et de l'armée, Lénine décrète le , dont les mesures essentielles sont :

Les éléments fondateurs du régime, sous l'appellation de , se mettent aussi en place à cette époque :

Grâce au « communisme de guerre », Lénine et le Parti bolchevik parviennent à se maintenir au pouvoir. Ils sortent vainqueurs de la guerre civile, et le danger d'une restauration monarchique est écarté dès 1919-1920 à la suite de la défaite des « armées blanches ». Mais ils doivent ensuite faire face à l'armée anarchiste de Makhno (Makhnovchtchina) qui tient le Sud de l'Ukraine, et se confronter en 1921-1922 aux « armées vertes » créées par les paysans en révolte à la fois contre les Blancs et les bolcheviks.

Le , l'Armée rouge réprime dans le sang la révolte de Kronstadt, dont les marins avaient exigé le retour au « pouvoir des soviets » et la fin du monopole bolchevique.

Sur le plan territorial, la Russie bolchevique perd les pays baltes, la Finlande et la Pologne, devenus indépendants, et doit concéder un important recul de ses frontières après sa défaite dans la guerre russo-polonaise. Mais elle conserve l'Ukraine après des luttes confuses, et entre 1920 et 1922, elle envahit la Géorgie, l'Arménie et l'Asie centrale, réintégrées de force dans l'ancien Empire russe.

La guerre civile, l'embargo total décrété par les puissances occidentales sur la Russie soviétique et la politique d’expropriation de biens des paysans afin de nourrir les soldats de l'Armée rouge conduisent à une grande famine provoquant la mort de millions de Russes, surtout le long de la Volga en 1922.

L'Union des républiques socialistes soviétiques naquit le , date de la signature du entre la RSFS de Russie, la RSFS de Transcaucasie, la RSS d'Ukraine et la RSS de Biélorussie. Ce traité est ratifié le par le premier congrès des Soviets d'URSS.

Cette nouvelle entité n'est pas aussi grande que celle de la guerre froide, elle a ainsi perdu de nombreux territoires, tels que l'Ouest de l'Ukraine actuelle, les pays baltes ou bien la Carélie à la suite des guerres qui l'ont secouée. C'est néanmoins le plus grand état du monde et il devra attendre avant d'être reconnu internationalement.

Une constitution fut rédigée en 1923 ; l'union regroupa plusieurs républiques fédérés dont les frontières furent constituées selon une répartition démographique correspondant à un "peuple" dans sa définition soviétique. L'URSS fut donc un État fédéral dans lequel chaque république fut égale en droits. Dans les faits, le PCUS (et au début le RSDRP) et la Tchéka surveillent étroitement ces républiques dont les premiers secrétaires du Parti furent désignés par Moscou.

Le PCUS devint rapidement le seul parti légal. Le pays fut théoriquement gouverné par des « Soviets » élus démocratiquement au niveau régional et local. Néanmoins, en pratique, chaque niveau de gouvernement était dirigé par la branche correspondante du Parti.

Après la guerre civile (1921), le pays se trouve dans une situation humanitaire et économique désastreuse. La famine sévit (cinq millions de morts), notamment sur la Volga, et les paysans se soulèvent sporadiquement contre les réquisitions. Ce mécontentement prend une ampleur inquiétante en avec la révolte de Kronstadt, ville pionnière de la révolution, abritant l'amirauté et les forces navales de la mer Baltique défendant Saint-Pétersbourg. Conscient que la répression, aussi dure soit-elle, ne suffit pas à enrayer le mouvement, Lénine décida alors d'assouplir la politique du régime, et met en œuvre la « Nouvelle politique économique » (NEP), libéralisation économique donnant droit à une propriété privée limitée, notamment aux agriculteurs. Les réquisitions sont ainsi remplacées par un impôt en nature peu élevé.

Pour expliquer le passage à la NEP, Lénine déclare que , se référant au fait que la Russie était encore une société essentiellement agraire avec une base industrielle encore faible et ne correspondait donc pas aux critères permettant le socialisme tel que défini par Karl Marx. La NEP devait également rassurer les pays occidentaux capitalistes.

La NEP atteint les résultats escomptés en permettant à l'économie de se relever des conséquences désastreuses de la guerre. La famine rampante disparaît virtuellement et la classe paysanne s'enrichît. Les paysans aisés sont appelés koulaks ; dans les agglomérations, les "nepmen" constituent une bourgeoisie riche.

Bien que présentée comme une mesure provisoire, la NEP fut extrêmement critiquée par une frange importante du Parti bolchevique. De nombreux membres voyaient la NEP comme une trahison aux principes socialistes et voulaient un retour au plus vite à une économie intégralement planifiée. Il semble qu'à sa mort Lénine considérait que la NEP devrait être maintenue, tout du moins n'a-t-il jamais fixé, ni même évoqué, la date de son arrêt. Ainsi, à l'approche de sa succession, les oppositions au sein du Politburo se cristallisèrent autour de la NEP.

Dès 1922, la santé de Lénine décline à la suite d'attaques cérébrales, conséquences d'un attentat dont il fut victime en 1918. La lutte pour sa succession aboutira à l'accession au pouvoir suprême de Joseph Staline, ayant appartenu au premier cercle d'adhérents au Parti (entrée en 1904), bien que Lénine ne l'appréciait plus beaucoup, déclarant même dans son testament () qu'il fallait démettre de ses fonctions cet homme .

L'ascension de Staline débute avec sa nomination au poste-tremplin de secrétaire général du Parti le , fonction conciliatrice obtenue grâce à son effacement (peu de prises de position), ses relations de longue date, son dévouement, et sa loyauté à l'appareil du Parti. Face à lui, il rencontre rapidement l'opposition de Léon Trotski, fondateur de l'Armée rouge, ayant acquis dès 1902 l'estime de Lénine mais aussi adhérent tardif au Parti bolchevique (1917) ayant été proche des mencheviks. Alors que Trotski n'avait parfois pas hésité à s'opposer à Lénine sur certains points dans le cadre des congrès du parti, Staline se présente comme un loyal serviteur du grand révolutionnaire ne l'ayant jamais contredit.

Pour évincer Trotski du gouvernement, Staline s'associe dès 1923, du vivant de Lénine, à Lev Kamenev, ayant lui aussi adhéré en 1905, et à Grigori Zinoviev, haut dirigeant du Komintern, ami intime de Lénine convaincu d'être son légitime successeur et ayant lui aussi proposé un temps l'alliance avec les mencheviks.

En 1926, deux ans après la mort de Lénine, Zinoviev et Kamenev décident de rompre avec Staline pour se rapprocher de Trotski avec lequel ils partagent une doctrine commune : exportation de la révolution d'essence mondiale et abandon de la NEP. Cette "troïka des purs" forme l'Opposition de gauche à Staline, qui réagit tactiquement en se rapprochant de l'opposition de droite favorable à la NEP et à une réalisation du socialisme d'abord sur le sol russe puis à l'extérieur (Nikolaï Boukharine, Alexeï Rykov et Mikhaïl Tomski).

Il s'appuie sur cette aile droite pour exclure du Parti en 1927 ses trois grands opposants de l'aile gauche. Le , une fois assuré que les partisans de l'Opposition de gauche ont été réduits au silence (par l'exclusion, la force, l'emprisonnement, l'exil), il se retourne contre Boukharine, Rykov, et Tomski qu'il exclut du Politburo et démet de leurs fonctions respectives de président du Komintern, chef du gouvernement, et dirigeant du Profintern.

Staline, seul maître à bord, n'hésite pas dès lors à adopter la mesure-phare prônée par l'ancienne opposition de gauche devenue impuissante : l'abandon de la NEP. Cette réorientation s'accompagne d'une "relégitimation" de façade. Ainsi, en 1928, Kamenev est rétabli, il en va de même pour Zinoviev en 1929, mais Trotski, toujours populaire, est expulsé la même année. Kamenev et Zinoviev furent finalement jugés et exécutés le , Boukharine et Rykov en , et Trotski assassiné le dans son exil au Mexique.

Après avoir réussi à éliminer politiquement, puis physiquement, toute opposition au sein du parti, Staline devint le dirigeant suprême de l'Union soviétique de 1927 à sa mort, en . Du point de vue politique, ce fut une période de dictature totalitaire, bien que ce qualificatif de ait pu être contesté, par exemple par l'historien Eric Hobsbawm dans son étude du .

Il s'agissait de prévoir les activités économiques selon des plans quinquennaux et qui fixaient les objectifs obligatoires de production. Ces plans quinquennaux donnaient la priorité aux industries lourdes en laissant de côté les industries de consommation. En URSS, il y eut au total dix plans quinquennaux allant du Plan (1928-1932) jusqu'au Plan (1976-1980).

Il s'agit d'un plan typique de l'Union soviétique mais certains plans ressemblent à celui-ci comme le Commissariat général du Plan (en France) ou même le « Grand Bond en avant » (mis en place par la République populaire de Chine).

Staline ne forgea pas immédiatement sa doctrine au sujet de la NEP. Sans doute est-il exact de dire que ses changements d'opinion tenaient plus de la tactique politique que de la doctrine, ce qui lui permit de se débarrasser des uns et des autres. La « richesse » des "nepmen" et des koulaks l'amena à les considérer comme une nouvelle classe capitaliste rendue responsable de l'augmentation du chômage et de l'inflation.

Staline finit par se forger une doctrine qui excluait l'économie de marché tout en se concentrant sur le développement économique et industriel du pays. Ce qui conduit à l'autarcie par rapport à l'économie capitaliste externe et au recours massif au travail extensif (stakhanovisme) et même gratuit (des prisonniers dans les camps correctionnels de travail) pour réaliser les investissements colossaux qui sont nécessaires (plans quinquennaux).

En 1929, Staline décide de supprimer la propriété privée dans les campagnes : le bétail, les outils, les terres doivent être mis en commun. Les moyens de production agricoles sont regroupés dans les kolkhozes ou dans des sovkhozes.

Cette collectivisation forcée provoque des résistances : plutôt que donner leurs troupeaux, les paysans les abattent pour les consommer immédiatement. Face à ces émeutes, Staline accorde à chaque kolkhozien un lopin de terre.

Les koulaks doivent être éliminés en tant que classe. Entre 1929 et 1935, plus de deux millions de paysans sont déportés et plusieurs millions meurent de faim, surtout en Ukraine et dans le Sud de la Russie ("voir : Holodomor"). Leurs biens sont confisqués. Cette famine organisée, lors de laquelle non seulement les récoltes, mais tout produit alimentaire étaient volés aux paysans, est considéré par de nombreux pays dans le monde, dont le Canada, comme un génocide ou comme un ethnocide. Le système du passeport intérieur, destiné à contrôler les déplacements et qui n'était pas accordé aux paysans, a été mis en place en Ukraine avant 1929. Après 1935, le premier recensement en Ukraine a montré une baisse démographique si importante qu'aucun recensement n'a plus été mené pendant 30 ans.

La Russie du début du était une puissance économique nouvelle et en essor, mais encore très rurale et agricole. Staline voulait développer l'industrie lourde et faire de l'URSS une puissance économique majeure : lire "Histoire de l'URSS sous Staline#Planification et industrialisation".

Les moyens utilisés sont ceux d'une économie planifiée et centralisée et d'une organisation politique totalitaire :
Selon certaines estimations, payèrent de leur vie la mise en place du premier plan quinquennal (de 1928 à 1932). Par ailleurs, l'allocation prioritaire des ressources à l'industrie, les exportations forcées de céréales pour financer des importations de biens d'équipement, combinées à la diminution de la productivité agricole provoquèrent de nouvelles famines : la famine de 1931-1933 cause près de six millions de morts. Le plan quinquennal fut cependant "bouclé" officiellement en quatre ans. De 1928 à 1932, la production de charbon avait doublé, celle de l'acier avait triplé.

En dix ans, l'URSS a accompli un bond remarquable du point de vue industrialisation au détriment de la production de biens de consommation et au prix d'une forte baisse du niveau de vie de la population. À la suite du second plan quinquennal, la production d'acier a grimpé à de tonnes, celle de charbon à de tonnes. Avant son interruption par la guerre, le troisième plan avait permis d'atteindre de tonnes d'acier et de tonnes de charbon. Les structures de production de masse étaient ainsi bel et bien établies, le complexe militaro-industriel allait être durement mis à l'épreuve par l'invasion allemande.

La pire répression jamais connue par un pays en temps de paix, les « Grandes Purges » (appelées aussi la « Grande Terreur ») aboutissent entre 1936 et 1939 à l’exécution de et à la déportation de centaines de milliers d’autres. En , Staline autorise personnellement le recours à la torture dans les prisons, et ne l’interdit à nouveau que fin 1938.

Le pays traverse donc une intense période de terreur, de délation et de suspicion généralisée, qui met bien des nerfs à rude épreuve (la pression subie en conduit plus d’un au suicide), et qui brise les solidarités amicales, familiales et professionnelles. Après le premier procès de Moscou en , c’est qui marque le vrai lancement de la « Grande Terreur », dont elle deviendra synonyme.

À court terme, Staline veut fournir à la population des boucs émissaires (souvent des communistes mêmes) aux difficultés du quotidien, en rejetant tout le mal sur une pléthore de . Au-delà, il renforce son pouvoir absolu en liquidant la vieille garde bolchevique, qui sait son faible rôle dans la révolution, et en brisant les réseaux clientélistes et les fiefs personnels que se sont taillés les ministres, les membres du Politburo, ou bien, à tous les échelons, les responsables locaux du Parti et les directeurs du Goulag qui, de ce fait, se trouvent abondamment pourvus de main d'œuvre à bas coût. Quand le « clan des voleurs de poules » est épuisé on fixe des quotas que les autorités locales sont chargées de fournir aux camps de travail. Les cadres compétents et les techniciens, qui osent souvent contredire ses objectifs politiques irréalistes, sont aussi particulièrement visés. Enfin, Staline entend éliminer radicalement tous les éléments socialement suspects, et tous les mécontents suscités par sa politique. Alors que les tensions diplomatiques s’accumulent en Europe depuis l’avènement d'Adolf Hitler, et que le déclenchement de la guerre d'Espagne en fait craindre un conflit général, il s’agit d’éliminer tout ce qui pourrait constituer une en cas d'invasion.

Pour lancer et développer cette terreur de masse, Staline bénéficie du soutien indispensable de ses fidèles, mais aussi du zèle indéniable de nombreux responsables locaux, de bien des policiers et bureaucrates enthousiastes, ou de bien des simples citoyens délateurs.

En 1939, à l’arrêt des « Grandes Purges », Staline a éliminé les dernières sphères d’autonomie dans le Parti et la société, conforté par les élections du et imposé définitivement son « culte » et son pouvoir absolu. Il a, ce faisant, désorganisé gravement le pays et décimé les cadres supérieurs de l'armée, alors même que la guerre approche.

Staline mit en place un système totalitaire sur lequel il régnait en despote absolu et reposant sur deux piliers : la propagande, mettant en œuvre un véritable culte de la personnalité et la répression, s'appuyant notamment sur le NKVD, police politique toute puissante.

Si les estimations des victimes entre 1921 et 1954 varient beaucoup, celui de de morts a été avancé. Parmi les personnes condamnées pour des crimes contre-révolutionnaires, furent condamnés à mort, emprisonnés ou envoyés dans des camps de travail du Goulag, et à l'expatriation. Le haut encadrement de l'Armée rouge ne fut pas plus épargné (« affaire Toukhatchevsky ») et subit une épuration qui devait affaiblir l'URSS au début de la Seconde Guerre mondiale.

Tirant des accords de Munich la conclusion que les puissance de l’Ouest, France et Grande-Bretagne, veulent laisser à Hitler les mains libres à l’est, Staline conclut, le , le Pacte germano-soviétique avec l’Allemagne nazie. Il s’agissait d’un « pacte de non-agression » qui contenait une annexe secrète attribuant l’Est de la Pologne, la Lettonie, l’Estonie, l'Est de la Roumanie et la Finlande à l’Union soviétique, tandis que l’Ouest de la Pologne et de la Roumanie ainsi que la Lituanie étaient attribués au Troisième Reich.

La Wehrmacht envahit la Pologne le .

L’Allemagne ayant rejeté les prétentions territoriales de l’URSS, celle-ci tente d’envahir la Finlande le : c’est le début de la guerre d’Hiver. La campagne fut difficile, mais par une paix signée à Moscou le , l’URSS obtenait l'annexion de la Carélie, lui permettant d’éloigner la frontière de Léningrad.

À la suite du déclenchement de la guerre, l’URSS avait été expulsée de la SDN le . Un avenant au pacte cède alors également la Lituanie à l'URSS. Au , l’Estonie, la Lettonie, la Lituanie et l’Est de la Roumanie, qui n’ont pas de forces militaires ni d’unité civile pour résister à la pression de Staline, sont annexés par un jeu de manipulations politiques, et quatre nouvelles républiques soviétiques sont créées (celles d'Estonie, Lettonie, Lituanie et Moldavie) tandis que la Biélorussie et l’Ukraine sont agrandies vers l’Ouest des territoires pris à la Pologne.

L'expression de « Grande Guerre patriotique » désigne la seconde partie de la Seconde Guerre mondiale en Europe, où l'URSS répond à l'attaque allemande du (« opération "Barbarossa" »), tandis que les pays que l'URSS avait agressés (Finlande et Roumanie, jusque-là aidés par les Alliés) se retrouvent du côté de l'Axe. Par contre, elle ne désigne pas la guerre soviéto-japonaise déclarée le pour laquelle le traité de paix n'est toujours pas signé entre la Russie et le Japon, puisque le contentieux relatif aux îles Kouriles bloque la signature d'un tel accord.

Le , l'Allemagne rompit le « pacte de non-agression » et attaqua l'Union soviétique, Staline ayant refusé de réagir aux mises en garde de ses agents et de Churchill qui était renseigné grâce au décryptage du code de la machine Enigma qui chiffrait les communications militaires allemandes.

L'invasion nazie prit l'URSS dans un état de totale impréparation. D'abord débordée et surprise par le choc de l'attaque allemande du , l'Armée rouge perd hommes, matériels et laisse la Wehrmacht occuper d'immenses territoires en quelques mois (Pays baltes, Biélorussie, Ukraine). Pour beaucoup la guerre semble gagnée par l'Allemagne au début de . Certains historiens estiment que les Grandes Purges des , au cours desquelles auraient été emprisonnés ou liquidés, ne sont pas étrangères aux premières difficultés de l'Armée rouge. Les troupes du Reich atteignirent les environs de Moscou en , mais avaient atteint leur extension maximale, des troupes devant aller consolider le flanc sud de l'attaque.

Pourtant plusieurs facteurs vont stopper net l'offensive allemande et permettre la première contre-offensive soviétique. D'abord, et malgré les apparences, l'attaque allemande coûte cher à la Wehrmacht. Début décembre, elle a déjà perdu autant d'hommes que lors de toutes les campagnes précédentes. De plus, son matériel (chars, etc.) n'est pas remplacé facilement de sorte qu'au fur et à mesure ses meilleures divisions s'affaiblissent. Son équipement n'est pas adapté à la guerre en Russie : ses camions s'enlisent dans la boue dès octobre, ses moteurs sont sensibles au froid, les hommes ne sont pas habillés pour affronter l'hiver De plus, l'apparition du char T-34 inconnu jusque-là des Allemands, constitue un adversaire redoutable : son puissant canon de 76 mm perce tous les blindages des panzers, ce qui n'était pas le cas des chars français ou anglais (à l'exception de rares Renault B1 bis). De plus, ses larges chenilles lui évite de s'embourber dans la boue ou la neige. 
Par ailleurs de nombreuses divisions stationnés en Sibérie, face à la Mandchourie occupée par les Japonais, sont rapatriées pour protéger Moscou.

Enfin, le traitement que les nazis réservent aux prisonniers soviétiques et aux Slaves en général considérés comme des « sous-hommes » ("Untermenschen") et privés du respect de la Convention de Genève galvanise les populations contre l'occupant. Dès , des groupes de partisans apparaissent et l'Armée rouge se reprend très rapidement malgré les premiers mois éprouvants de sorte qu'en , les Allemands sont incapables de prendre Moscou et subissent une contre-offensive qui les oblige à reculer de plusieurs centaines de kilomètres, contre-offensive qui cependant s'épuise et s'arrête au . Moscou est alors sauvée.

Certains historiens estiment même que le vrai tournant de la guerre à l'est date de . Cependant, l'armée allemande reste relativement forte, l'Armée rouge n'a pas encore déployé toute sa puissance. L'enjeu pour Hitler va être alors de terminer au plus vite la guerre à l'est, avant que l'Armée rouge ne puisse définitivement inverser le rapport de force (et que la puissante Amérique, en guerre depuis le 7 décembre 1941, ne vienne en aide matériellement à l'URSS). C'est l'enjeu de la campagne avec deux objectifs : conquérir le Caucase et rejoindre Rommel, à la tête de l'Afrika Korps, au Moyen-Orient ; repousser les Soviétiques au-delà de la Volga et prendre Moscou à revers. Les premiers mois de l'offensive semblent favorables au Führer. Pourtant, le plan aboutit à une situation stratégiquement mauvaise pour les Allemands : ils divisent leurs forces en deux groupes (un groupe pour le Caucase et un pour Stalingrad sur la Volga) et, de fait, créent deux groupes militaires incapables, à terme, de remporter leurs objectifs.

L'aide des Alliés par Mourmansk dans le cadre du prêt-bail et l'industrialisation à marche forcée contribua à la victoire finale de l'URSS sur le Reich. Quoique l'Union soviétique ait reçu des fournitures en armes et matériel des États-Unis et de l'Empire britannique, sa production de matériel de guerre était plus importante que celle de l'Allemagne du fait de l'importante augmentation de la production industrielle entre les deux guerres. Durant l'invasion allemande, de nombreuses industries ont été transférées à l'est de l'Oural, ainsi que de travailleurs civils. L'Armée rouge réussit à arrêter l'avance à l'est des armées du Reich, notamment grâce à la victoire de Stalingrad (-).

Les Allemands et leurs alliés italiens, espagnols (division Bleue), français vichystes (division Charlemagne), hongrois et roumains, s'enlisent dans le Caucase, et sont stoppés à Stalingrad où s'engage une bataille de rues. La armée allemande est encerclée dans la ville par une attaque soviétique fin 1942. Un long siège commence pour cette armée qui, coupée du reste de la Wehrmacht, s'effondre peu à peu affamée, frigorifiée, soumise à une pression de plus en plus forte des Soviétiques. Notons que d'autres Français (escadrille Normandie-Niémen) et Roumains (division Vladimlirescu) ainsi que des Polonais (armée "LWP" ou Ludowe Wojsko) combattaient du côté soviétique. Le , le général Paulus se rend, marquant le début d'une contre-offensive soviétique qui, malgré l'intermède de la bataille de Koursk (), ne s'arrêtera qu'à Berlin en . Le tournant de la campagne fut donc la bataille de Stalingrad en 1942 et 1943 : l'Armée rouge remportait la victoire après avoir perdu un million d'hommes. L'URSS reprit ensuite progressivement l'initiative, surtout après la bataille de Koursk en , et commença à regagner du terrain sur l'armée allemande. En , l'Armée rouge pénètre dans Berlin ; le , Hitler se suicide ; le , le drapeau rouge flotte sur le Reichstag et la capitulation sans condition est signée le . Le , conformément aux accords de Yalta, l'URSS déclare la guerre à l'empire du Japon et réalise l'invasion de la Mandchourie.

L'URSS supporta l'essentiel de l'effort de guerre sur le théâtre d'opérations européen jusqu'à ce que les Alliés ouvrent un second front en Europe (deux ans après la demande de Staline), mais également en Afrique du Nord française (opération "Torch", débarquement du 8 novembre 1942 à Casablanca et à Alger puis guerre en Tunisie où l'Armée française d'Afrique s'illustra aux côtés des Alliés) avec le débarquement en Sicile, en 1943, et en Normandie en 1944. À la fin de la guerre, on estime qu'environ et demi de Soviétiques y avaient perdu la vie, parmi lesquels de civils, mais pas nécessairement au front : ce chiffre comprend les nombreux prisonniers de l'opération "Barbarossa" qui périrent soit dans les camps allemands de malnutrition et maladie, soit au camp du Goulag après leur délivrance (car la reddition étant interdite au soldat soviétique, ils étaient considérés comme coupables de haute trahison). S'ajoutent à cela des destructions matérielles importantes, ayant provoqué une diminution de 25 % du PIB.

Plusieurs millions d'Estoniens, Lettons, Lituaniens, Polonais, Roumains, Ukrainiens occidentaux, Géorgiens, Tchétchènes et autres minorités ethniques furent déportés dans les camps de Sibérie, ou dans des zones reculées pour limiter leurs contacts avec l'Ouest.

Pendant et après la guerre, les négociations entre les Alliés aboutirent à la mise en place de deux zones d'influence, suivant les accords de Yalta et de Potsdam.

L'Union soviétique mit en place des régimes dits de « démocraties populaires » dans les pays d'Europe centrale et orientale (y compris dans la partie de l'Allemagne sous son contrôle), dans lesquels elle implanta des gouvernements qui lui étaient dévoués. La ligne frontière séparant cet ensemble de pays de l'Europe occidentale alliés aux États-Unis, fut nommée « rideau de fer », qui constitue un des éléments à l'origine de la guerre froide.

Depuis 1945 et quasiment jusqu'à sa dislocation, l'Union soviétique est opposée aux États-Unis dans la « guerre froide », chacun des protagonistes essayant d'augmenter sa sphère d'influence au détriment de l'autre, et souvent des pays concernés.

L'URSS avait réuni, dans tout l'Est de l'Europe, un ensemble de pays satellites (République socialiste tchécoslovaque, République démocratique allemande, République populaire de Hongrie, République populaire de Pologne, République populaire de Roumanie, République populaire de Bulgarie, République populaire d'Albanie). Ces pays étaient regroupés au sein du pacte de Varsovie à partir de 1955. Les États-Unis avaient formé, avec l'Europe de l’Ouest et le Canada, l'OTAN en 1949.

Dès 1943, Staline fonde l'Institut Kourtchatov de recherches nucléaires, suivi de la création entre 1945 et 1948 du complexe nucléaire Maïak, puis de la création en 1946 de l'Institut panrusse de recherche scientifique en physique expérimentale. L'essor de l'industrie nucléaire soviétique permet ainsi à l'URSS de faire son premier essai nucléaire en 1949.

Hors d'Europe, l'Union soviétique et les États-Unis s'opposaient, souvent par « mouvements de libération » interposés, dans diverses parties du monde, notamment en Amérique du Sud et en Afrique.

Après la mort de Staline en , Nikita Khrouchtchev devint premier secrétaire du Comité central du Parti tandis que Gueorgui Malenkov devient Premier ministre. Lavrenti Beria, le chef du NKVD, qui pouvait prétendre à la succession est arrêté en et exécuté peu de temps après, en . La nouvelle direction du pays déclara une amnistie pour certaines catégories de prisonniers et relâcha quelque peu le carcan qui enserrait les libertés publiques. Khrouchtchev consolida peu à peu son pouvoir personnel et pendant le du Parti communiste, il prononça, le , un discours sur « le culte de la personnalité et ses conséquences » au cours duquel il dénonça le culte de la personnalité entretenu par Staline ainsi que la dictature qu'il avait fait subir à l'URSS et les crimes de cette période. L'impact de ce discours fut immense et détruisit la légitimité des staliniens qui lui étaient encore opposés. S'ensuivirent de nouvelles mesures de démocratisation de la vie publique, la libération de dissidents, et la mise en place d'une économie plus favorable aux biens de consommation par rapport aux plans quinquennaux précédents.

La même année, les troupes soviétiques réprimèrent dans le sang la révolution hongroise : de à et de l'Armée soviétique perdirent la vie, tandis que près de quittaient le pays. Cet événement fut, pour la part de l'opinion occidentale favorable à l'Union soviétique, un premier choc sérieux.
Khrouchtchev dut encore se défendre en contre les menées de staliniens. Ainsi, la vieille garde stalinienne, constituée de Lazare Kaganovitch, Viatcheslav Molotov, Gueorgui Malenkov et Dmitri Chepilov, tente de démettre de ses fonctions Nikita Khrouchtchev. Avec l'aide du « héros de la Grande Guerre patriotique » et ministre de la défense Gueorgui Joukov, Krouchtchev parvient à déjouer leur plan en les présentant comme un « groupe anti-parti ». Ils seront tous trois mis au ban de l'URSS, mais, signe des temps, ils ne seront pas éliminés à la suite de procès aux preuves fabriquées, comme il était de mise du temps de Staline. Khrouchtchev devint enfin Premier ministre le . Il s'agit là d'un grand tournant dans l'histoire de l'Union soviétique.

La période de dix ans qui suivit confirma cette nouvelle tendance : le pouvoir politique avait pris le pas sur la coercition pure et simple, le parti reprenant le rôle premier par rapport à la police secrète et à l'armée. Au cours de cette période, également, l'URSS confirma sa place de super-puissance et défiait les États-Unis, souvent sur leur propre terrain. Cuba, pays soutenu par l'URSS, devint le centre de cette opposition lors de la « crise des missiles de Cuba » en .

En 1957, les Soviétiques envoyèrent dans l'espace le premier satellite artificiel, Spoutnik et le premier être vivant dans l'espace, Laïka. En 1961, Youri Gagarine fut le premier homme dans l'espace, et en 1963, Valentina Terechkova la première femme. C'est également durant son mandat que, le , explosa la plus puissante arme jamais développée par l'Homme, la tsar bomba. Sans doute partiellement à cause de l'affaire des missiles et d'une politique trop défavorable à la nomenklatura, Khrouchtchev fut déposé lors d'une réunion du Comité central du Parti le .

À la suite de la chute de Khrouchtchev en 1964, Léonid Brejnev devient premier secrétaire du Parti, Alexis Kossyguine Premier ministre et Anastase Mikoyan chef de l’État, rapidement remplacé par Nikolaï Podgorny (on parle alors de "troïka" pour désigner ces trois personnages détenteurs du pouvoir d'État ; mais Brejnev ne tardera pas à concentrer l'essentiel de la réalité du pouvoir pour lui-même).

Sous Brejnev, le régime soviétique se durcit à nouveau. La police politique (le KGB), dirigée par Iouri Andropov, retrouve une grande partie du pouvoir dont elle avait joui sous Staline. Cependant, Andropov n'imitera pas les excès répressifs de cette époque.

Une des crises les plus graves de l'époque de Brejnev fut celle du Printemps de Prague en 1968, lorsque les tentatives de la Tchécoslovaquie de construire un « socialisme à visage humain » sont finalement réprimées par les forces du pacte de Varsovie, sans toutefois tomber dans les excès de la répression de la révolution hongroise. Au niveau économique, le niveau de vie de la population commença à descendre et le manque de productivité dans de nombreux secteurs dont l'agriculture se fit sentir. L'URSS dut entre autres, pour faire face à la faiblesse de la production d'aliments, acheter des millions de tonnes de céréales en Occident en général et aux États-Unis en particulier. Sur le plan international, l'ère Brejnev fut marquée par un certain relâchement de la tension avec les États-Unis, avec notamment la signature de traités de limitation des armes nucléaires (accords sur la démilitarisation de l'espace en 1967, traités SALT I en 1972, SALT II en 1979) et le traité d'Helsinki.

En , Brejnev intervint en Afghanistan pour soutenir le régime communiste en place. Cet événement mit un coup de frein à la détente, provoquant un embargo par les États-Unis, la fourniture d’armements aux moudjahidines et le boycott des Jeux olympiques de Moscou en 1980. En , Brejnev fit une crise cardiaque qui le diminua considérablement. À partir de ce moment, il ne remplit que partiellement ses fonctions jusqu'à sa mort en novembre de la même année. Deux chefs d'État en mauvaise santé se succédèrent entre et : Iouri Andropov et Konstantin Tchernenko. Chacun continua d'appliquer la ligne politique de Brejnev, malgré de réels efforts d'Andropov pour combattre le népotisme que son prédécesseur avait organisé ou laissé s'organiser. Toutefois en politique extérieure, les deux successeurs de Brejnev marquèrent quelques points. Andropov mit en échec les États-Unis au Liban qui occupaient le pays du cèdre depuis . De ce fait une aide massive de l'URSS à la Syrie à partir de , entraîna la multiplication des attentats, contre les marines américains et obligea le président Reagan à faire retirer ses marines du Liban en . Puis sous Tchernenko, l'URSS rendit aux États-Unis la monnaie de leur pièce à leur offense sportive. Ce fut l'annonce en d'une non-participation soviétique aux Jeux olympiques de Los Angeles, faisant ainsi pendant au boycott des JO de Moscou par les États-Unis. À cette initiative soviétique s'ajoutèrent des « contre-jeux » à dans une dizaine de capitales de pays socialistes qui s'associaient au boycott. Cependant ils subirent un échec retentissant avec l'installation des Pershing en Europe occidentale en novembre 1983 et durent faire face devant la communauté internationale deux mois plus tôt à l'annonce de la destruction par l'un de leurs chasseurs, d'un Boeing sud-coréen qui avait fait mystérieusement intrusion pendant plusieurs heures au-dessus de l'espace aérien de l'URSS. Après Andropov (-) et Konstantin Tchernenko (-), Mikhaïl Gorbatchev, un jeune et énergique dirigeant de , devint premier secrétaire du Parti.

Constatant la déliquescence du pays et de son économie, Gorbatchev tenta tout d'abord de sortir son pays de l'impasse que devenait la guerre froide. En effet, Ronald Reagan avait lancé un réarmement massif des États-Unis en orientant sa recherche et ses investissements vers des types d'armement à très haute valeur technologique, entraînant ainsi l'URSS, sous peine d'obsolescence, dans une course rapide qu'elle ne pouvait que perdre vu son retard technologique et son économie en grave crise.

Gorbatchev entama donc une série d'initiatives qui aboutirent à une détente certaine et à la signature d'accords de désarmement. Gorbatchev obtint le prix Nobel de la paix pour ces efforts en 1990. Cette politique aboutit à la chute du mur de Berlin en 1989.

Se débarrasser de cette contrainte externe n'était cependant pas suffisant, et sans abandonner le dogme central du « socialisme », Gorbatchev lança la "glasnost" (« publicité des débats », politique d'informations libres) et la "perestroïka" (« restructuration », nouvelle politique économique et sociale), avec trois principaux objectifs :

Alors que tous les prisonniers politiques détenus par le gouvernement sont libérés, la "glasnost" est également marquée par le retour de la liberté d’expression : on voit des humoristes caricaturer Gorbatchev. Il cherche par là une voie intermédiaire entre les « traditionalistes » attachés au régime (la nomenklatura) et les « réformistes », tels Boris Eltsine qui lui reprochent la lenteur des réformes. Pourtant il était trop tard, et Gorbatchev ne réussit pas à corriger les failles qui minaient l’État depuis des décennies. Les problèmes économiques furent mal résolus. La privatisation des grandes entreprises se fit au bénéfice des privilégiés de la nomenklatura et l’inflation se développa : la "perestroïka" fut un échec.

Le , Gorbatchev créa une nouvelle assemblée législative : le congrès des députés du peuple dont les deux tiers étaient des membres élus au suffrage universel, à bulletin secret, sur candidatures multiples. Les premières élections législatives révélèrent l’échec des candidats de Gorbatchev et l’émergence des réformateurs et des nationalistes. Son gouvernement apparut trop modéré pour des réformateurs, partisans d’une économie libérale, et trop réformateur pour ceux qui souhaitaient un retour au communisme.

En , Boris Eltsine, président du Soviet suprême de la RSFS de Russie, déclara la souveraineté de la Russie et démissionna du Parti. En , un putsch mené par des membres du gouvernement opposés aux réformes montra à quel point la position de Gorbatchev s'était fragilisée. Le complot échoua en partie grâce à l'intervention de Eltsine, qui confirma de ce fait sa position de chef de file des réformistes. La date du putsch ne fut pas choisie au hasard, car c'est le que Gorbatchev devait signer un traité instaurant une nouvelle Union, appelée "Union des républiques souveraines soviétiques" (puis "Union des républiques souveraines"), réduisant notamment le rôle du KGB et de l’État centralisé, qui avaient tout à y perdre, au profit des républiques.

Au cours de , tandis que les républiques constituantes de l'URSS proclamaient, l'une après l'autre, leur indépendance sans que Gorbatchev ait la possibilité de s'y opposer par la force, le gouvernement russe prit peu à peu l'ascendant, reprenant les fonctions auparavant assurées par l'Union. Ainsi, Gorbatchev tout en étant président de l'Union soviétique perdait rapidement prise. On disait à l'époque que l'Union soviétique se limitait aux murs du Kremlin.

En , le président russe Eltsine publia un décret qui interdisait les activités du Parti communiste de l'Union soviétique sur le territoire de la fédération de Russie. Le , lors des accords de Minsk, les chefs de la Russie, de l'Ukraine et de la Biélorussie publièrent une déclaration selon laquelle l'Union soviétique était dissoute et remplacée par la Communauté des États indépendants (CEI), une organisation sans entité juridique forte, qui ne fonctionna pas réellement, malgré un renouveau récent avec de nouvelles organisations partenaires telles que l'OTSC ou la Communauté économique eurasiatique ("Eurasec").

Gorbatchev était encore président, mais sans pays, son pouvoir ne signifiait plus rien. Le , Gorbatchev remit sa démission en tant que président de l'Union soviétique. Le jour suivant, l'Union soviétique était officiellement dissoute. La fédération de Russie, elle-même constituée de républiques, allait désormais la remplacer, avec quatorze autres républiques indépendantes, mais d'une importance moindre. La Russie hérita du siège permanent au Conseil de sécurité des Nations unies dont jouissait l'URSS.

Les pays composants l'ex-URSS, très affaiblis, avaient prévu de la reformer. Sur l'initiative de l'Ukraine ou du Kazakhstan, certains projets sont nées entre 1994 et 1995 pour refaire l'Union. En effet, une proposition faite en 1994 par le président kazakh Nazarbayev, envisageant la création d'une union eurasiatique, n'a jamais été envisagée sérieusement ou adoptée, jusqu'en 2010. À cette date, une réelle union économique et politique est annoncée pour 2015.

La CEI est une entité intergouvernementale composée de onze anciennes républiques soviétiques. Conformément à ses instruments constitutifs, les accords de Minsk et d'Alma-Ata, la CEI est dépourvue de personnalité juridique internationale. Pour cette raison, la collectivité des anciennes républiques soviétiques n'est pas une organisation internationale. La CEI, créée en , a comme membres les républiques suivantes : Arménie, Azerbaïdjan, Biélorussie, Kazakhstan, Kirghizistan, Moldavie, Ouzbékistan, Russie, Tadjikistan, Turkménistan et l'Ukraine. La Mongolie est un observateur au sein de certaines institutions communautaires. La Géorgie, elle, quitta la Communauté à la suite des événements en Ossétie du Sud (2008). Il devait s'agir d'une union dont le projet ressemble à celui de l'Union européenne, mais qui a peu avancé.

Dans le début des , les réformes de la CEI contribuent à créer une nouvelle organisation (l'Organisation du traité de sécurité collective/OTSC) et une nouvelle communauté au sein de la CEI (la Communauté économique eurasienne/"Eurasec").



L'Union eurasiatique (ou Union eurasienne) est une organisation supranationale fondée sur le modèle de l'Union européenne et du traité de Maastricht de 1992. Elle est effective depuis le janvier 2015. Englobant une union douanière et économique, elle intègre (en 2016) la Russie, la Biélorussie, le Kazakhstan, l'Arménie, le Kirghizistan et pourrait s'étendre au Tadjikistan. Proche du projet de l'Union des républiques souveraines imaginé par Gorbatchev en 1991, bon nombre d'observateurs, en particulier les États-Unis, mettent en garde la Russie face à une refondation de l'Union soviétique, sous une nouvelle forme.

L’Organisation de coopération centre-asiatique (OCCA) est une organisation internationale (nommée ainsi depuis 2002), fondée initialement en 1991 regroupant le Kazakhstan, l'Ouzbékistan, le Tadjikistan, le Kirghizistan et la Russie. Cette organisation est en cours de fusion au sein de l"'Eurasec".

L'Union de la Russie et de la Biélorussie est une union politico-économique de type confédéral entre les deux pays slaves (Russie et Biélorussie). La Serbie, l'Abkhazie, et l'Ossétie du Sud y ont un rôle d'observateur. C'est une des unions post-soviétiques les plus avancées.

La Transnistrie est un pays non reconnu, dont le territoire est moldave et limitrophe à l'Ukraine, montée en république non reconnue mais soutenue par Moscou.

L'URSS fut officiellement un État fédéral, basé sur le « centralisme démocratique » regroupant quinze républiques soviétiques. Le système politique, très hiérarchisé, reposait en droit sur le « Conseil des ministres » ("Sovet ministrov"), censé détenir le pouvoir exécutif, et le Parlement (« Soviet suprême », "Verkhovny Sovet") censé détenir le pouvoir législatif.

En pratique, la séparation des pouvoirs n'était pas respectée, car un seul parti politique fut autorisé, le Parti communiste de l'URSS (PCUS), dont le Politburo concentrait tous les pouvoirs et contrôlait l'État, tous les hauts fonctionnaires étant choisis parmi les « activistes » (« permanents ») supérieurs du Parti. L'organisation qui maintenait la cohésion du Parti et son pouvoir absolu sur la société soviétique était la police politique, successivement nommée Tchéka, Guépéou, NKVD et KGB : cette organisation fit la singularité du modèle soviétique, imité dans l'ensemble du pacte de Varsovie, en République populaire de Chine, au Viêt Nam et à Cuba. Le Parti était censé exercer la « dictature du prolétariat » telle que le « marxisme-léninisme » l'avait conçue. En principe, le Parti était ouvert à tout citoyen , cependant le processus d'adhésion au parti était long, accompagné de multiples enquêtes, et finalement élitiste, mais exclusivement sur des critères de soumission à la hiérarchie.

Ainsi, dans les , 6 % des d'habitants étaient membres du PCUS, ce qui était loin de conférer la représentativité du peuple tant affichée. Par contre, celui-ci compta quelque à plein temps, les "apparatchiki", les « hommes de l'appareil ». Ce que Voslenski a désigné par le terme populaire soviétique de nomenklatura était composée de ces "apparatchiki", des membres de la police politique, des hauts gradés de l'armée, des chefs du Parti et de leur parentèle. L'ensemble de cette nouvelle classe sociale que Jean-François Revel a qualifiée de , mais que les trotskistes préfèrent appeler bureaucratie.

La structure du Parti doublait la structure de l'État : si à chaque niveau il y avait des organes étatiques qui semblaient exercer le pouvoir, ces organes étaient contrôlés par le Parti, et donc par son responsable à chaque niveau, lequel prenait ses ordres de l'échelon supérieur, jusqu'à arriver au secrétaire général du Parti, poste rendu par Staline le plus important de toute l'Union soviétique.

Au sommet de l'État se situaient donc le « Soviet suprême », avec son organe exécutif, le Præsidium, ainsi que la Cour suprême et le Procureur de l'Union soviétique. Ces trois magistratures étant en principe sous le contrôle des deux chambres législatives. Le Conseil des ministres supervise une quantité de commissions et de services, dont le nombre et les attributions changent à intervalles, mais qui sont des organes plus importants que les ministères des Républiques.

Au sommet du Parti, le Secrétaire général, dont le titre est modeste mais le pouvoir beaucoup plus grand que celui du Président du Præsidium du Soviet suprême de l'Union soviétique dont le titre est purement honorifique, et plus grand que celui du Président du Conseil des ministres (Premier ministre) de l'URSS. Au-dessous de lui, par ordre d'autorité décroissante viennent le Politburo, le Secrétariat et le Comité central. Au-dessous encore le congrès du PCUS, puis les Comités centraux, les Secrétariats et les Conférences provinciales représentent l'échelon suivant. Un degré plus bas viennent les Comités, Secrétariats et Conférences de district. Enfin, constituant la base de la pyramide, les secrétariats, bureaux et cellules locales.

Le Parti déterminait la politique à suivre que l'État devait exécuter. La tâche des fonctionnaires du gouvernement consistait à mettre en application les décisions du Parti, c'est-à-dire du Politburo et du Comité central. Cette méthode avait un avantage : contrairement à ce qui se passa en Occident, ceux qui font la politique sont ainsi déchargés des besognes de routine. Staline a été le premier chef soviétique à cumuler les titres du Premier secrétaire du Parti et celui du président du Conseil des ministres de l'URSS. Khrouchtchev, qui lui a succédé a lui aussi cumulé les deux fonctions pendant une partie de son mandat de Secrétaire général. Quant à Brejnev, il fut en même temps Premier secrétaire (depuis 1966, secrétaire général) du Parti et président du « Soviet suprême » de l'URSS (de 1960 à 1964 et de 1977 à 1982). En 1990, Gorbatchev sera le premier et dernier dirigeant soviétique à prendre le poste de président de l'Union soviétique.

À la veille de la révolution russe, l'économie de l'Empire russe était . La valeur de la production industrielle en 1913 représentait moins de la moitié de celle de la France, un sixième de celle de l'Allemagne, ou un quatorzième de celle des États-Unis. Le rendement agricole était médiocre, la pénurie de transport paralysait toute tentative de modernisation économique. Le PIB par habitant était inférieur à celui de la Hongrie ou de l'Espagne de l'époque, et environ un quart de celui des États-Unis. Surtout, le pays était dominé par les capitaux étrangers qui possédaient un tiers des actions en Russie.

Au , l'URSS devient une puissance économique majeure. De 1928 à 1991, le développement économique est guidé par une série de plans quinquennaux. L’URSS devient une des trois premières productrices d'un grand nombre de produits industriels, mais reste en retard dans l'industrie légère, les biens de consommation et l'agriculture.

Le transport en URSS, confronté au double défi de la distance et du climat extrême, est marqué par le choix de privilégier le transport collectif (, métro de Moscou, etc.) plutôt que la voiture particulière. Il comporte quelques points forts comme les avions-cargos Antonov.

L'économie soviétique est gérée par le "Gosplan" (« Commission de Planification d'État »), la "Gosbank" (« Banque d'État ») et le "Gossnab" (« Commission d'État pour la fourniture en matériaux et équipements »), au moyen d'indicateurs comme le produit matériel net.

L'économie soviétique est basée sur la propriété d’État, mais il existe quelques autres formes juridiques de propriété dites « collectives » telles que le kolkhoze (« ferme collective ») et la coopérative.

L'entre-deux-guerres et l'après guerre sont des périodes de croissance économique importante que certains attribuent, pour une bonne part, au mariage de la planification et du travail forcé.

Entre 1913 et 1989, le revenu par habitant est multiplié en Russie par , contre en Grande-Bretagne, aux États-Unis, en France ou en Allemagne.

Lorsque la croissance se ralentit vers les , cela est considéré comme un phénomène provisoire. Les responsables de la planification sont incapables de prévoir certains problèmes économiques, et le concept même d'économie planifiée semble difficile à mettre en œuvre dans le cadre d'une économie mondiale capitaliste et changeante, surtout que sur le plan interne, l'administration de la planification étant paralysée par la bureaucratie, et que la nomenklatura semble parfois être plus attachée à ses privilèges qu'au service de l’État.

De plus, la production militaire d'armement représente une part très importante de l'industrie, freinant la production de biens de consommation. Le maréchal Nikolaï Ogarkov publie, à partir de 1979, une série d'articles, dans la presse officielle, expliquant de façon alarmiste que les Américains avaient une et même deux générations d'avance en électronique et en informatique, et sans possibilité de les rattraper. Dans les , l'URSS commence pourtant à développer le secteur de la micro-informatique et des technologies (ordinateurs de la série et Élektronika-60).

Le taux d’activité des femmes s’élève à 84 % en 1989, soit l’un des plus élevés au monde.

Le bilan économique en 1992 (un an après l'éclatement de l'URSS) fait état d'une inflation de à la suite de la déréglementation de la plupart des prix alors fixés par l'administration. D'après la Banque mondiale, les inégalités telles que mesurées par le coefficient de Gini double après l’éclatement de l'URSS : situé à 0,24 en 1988, il monte à 0,48 en 1993.

Le gouvernement de l'URSS a entravé la formation d'une conscience écologique en interdisant les partis et les associations jusque dans les . Dans les dernières années du régime stalinien, le nombre de réserves naturelles et parcs nationaux fut fortement réduit. Le productivisme entraîna l'érosion et l'épuisement de nombreuses terres arables. Le développement d'une industrie lourde et l'exploitation intensive et extensive des ressources naturelles ont laissé derrière eux une situation préoccupante, dont souffre encore l'actuelle Russie et les anciennes Républiques soviétiques : déforestation, régions affectées par des pluies acides, dégradation des sols, accumulation de déchets industriels, désertification, contamination radioactive (à la suite des essais nucléaires et de la catastrophe de Tchernobyl survenue en 1986), pollution des lacs (le lac Baïkal a été notamment fragilisé par la construction des chemins de fer Baïkal Amour Magistral dans les ).

L'irrigation intensive (pour supporter l'agriculture intensive, notamment du coton) et la construction de barrages hydro-électriques est notamment responsable de l'assèchement de la mer d'Aral en Asie centrale soviétique.

La culture de l'Union soviétique, est passée, au cours des d'existence de l'Union soviétique, par plusieurs étapes. Des personnes de diverses nationalités en provenance des quinze républiques y ont contribué, bien que la majorité d'entre eux soient des Russes. L'État soviétique a aidé les institutions culturelles, mais a effectué également une censure stricte.

Le bilan militaire était florissant :

Le complexe militaro-industriel soviétique représentait entre 1985 et 1990 :

L'industrie de défense proprement dite absorbait durant les 20 % du revenu national, 8 % du PIB et 47 % des dépenses publiques pour les besoins de l'Armée soviétique.

La production d’armes soviétiques était la plus importante du monde. En 1981 : , , de combat, , , balistiques (IRBM, ICBM).

Après la chute de l'URSS en 1991, c'est l'armée de la fédération de Russie qui hérita de la quasi-totalité de l'équipement de l'Armée soviétique en particulier l'arsenal nucléaire et les différentes flottes.

"Voir aussi : Forces armées de la fédération de Russie, VVS, Flotte maritime militaire de Russie"

La révolution de Février avait permis l'obtention de nouveaux droits par les femmes. Le 20 juillet 1917, le droit de vote des femmes était officiellement garanti.

Les bolcheviks maintiennent ensuite cette volonté d'égalité entre hommes et femmes, que l'on peut retrouver dans la Constitution de 1918 (puis, en théorie, celle de 1936 et celle de 1977 : (art. 35 de la Constitution de 1977)).

L'URSS se présentait donc initialement comme un État particulièrement en avance en matière d'égalité homme-femme, notamment grâce aux actions de la Commissaire du peuple Alexandra Kollontaï ou aux initiatives d'Inès Armand. Les femmes obtiennent en 1917 droit de vote et d'être élues, le droit au divorce par consentement mutuel, l'accès à l'éducation, un salaire égal à celui des hommes, des congés de maternité et l'égalité de reconnaissance entre enfants légitimes et naturels. Le droit à l'avortement est obtenu en 1920 — il sera limité en 1936 par Staline, puis rétabli après la mort de ce dernier. Par ailleurs dans la vie professionnelle, très majoritairement actives les femmes bénéficiaient avec les hommes du principe à travail égal-salaire égal.

L'URSS, par sa grandeur et donc par la variété ses régions, était un État largement multi-ethnique. Le groupe ethnique (en , souvent traduit par "nationalité") était indiqué sur certains documents, à certaines époques. Quinze grands groupes ethniques (dont le Russe) étaient représentés chacun par une république. Quatorze disposaient du droit à l'apprentissage d'une première langue locale mais devaient, comme seconde langue, apprendre le russe.

La population soviétique a d'abord baissé aux débuts de son existence à la suite de la Première Guerre mondiale (front de l'Est), à la révolution russe et à la guerre civile russe qui a suivi, stagnant autour de d'habitants.

Les furent également difficiles. Malgré les famines soviétiques de 1931-1933 ayant causé la mort de six millions de personnes, les Grandes Purges dirigées par Staline, ainsi que les victimes des goulags (chiffrées à 963 866 selon les archives soviétiques), la population était de plus de à la veille de la Seconde Guerre mondiale (front de l'Est).

Lors de l'après-guerre, la population a connu une diminution importante de la mortalité, qui s'est toutefois interrompue dès les . Cette diminution a permis de rattraper rapidement les déficits de naissances à la suite de la guerre, faisant passer la population de en 1950 à en 1960 et à plus de en 1970.

Son augmentation continua, surtout dans les républiques musulmanes d'Asie centrale où le taux de natalité était plus élevé que dans la vieille Europe, pour atteindre, en 1989, d'habitants. Vers la fin de la période, il existe en outre une différence notable entre une population russe et ukrainienne à croissance faible, et des peuples « allogènes » (principalement turcophones) à forte natalité.
L’Union soviétique a pour codes :




</doc>

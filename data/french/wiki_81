<doc id="14896" url="https://fr.wikipedia.org/wiki?curid=14896" title="Libéralisme">
Libéralisme

Le libéralisme est une doctrine de philosophie politique qui affirme .

Le mot libéralisme fait son apparition au début du . Les racines du libéralisme sont plus anciennes. L'opposition à l'absolutisme du souverain s'est développée dans l’Europe des Lumières (), mais aussi auparavant par la scolastique de l'École de Salamanque () faisant obligation morale au souverain de respecter les droits fondamentaux de chaque être humain au motif de sa nature de créature de Dieu, douée de raison, ou plus anciennement par les chartes médiévales (telles la "Magna Carta") introduisant des droits fondamentaux dont le respect est exigé du souverain, ou encore par certains pans de la philosophie thomiste, eux-mêmes précédés par le principe de justice naturelle d'Aristote. La date des débuts formels du libéralisme ou de ses composantes politiques, économiques ou religieuses diffère selon les auteurs. De nombreux auteurs font commencer le libéralisme avec la "Lettre sur la tolérance" de John Locke (1689) qui complète les racines préexistantes.

Le libéralisme repose sur l’idée que chaque être humain possède des droits fondamentaux naturels précédant toute association et qu'aucun pouvoir n'a le droit de violer. En conséquence, les libéraux veulent limiter les obligations sociales imposées par le pouvoir et plus généralement le système social, telles que la morale, au profit du libre choix et de l'intérêt de chaque individu indépendamment des autres. La question de l'articulation entre « libéralisme économique » et « libéralisme politique » reçoit des réponses variées.

Le libéralisme prône la liberté d'expression des individus. Dans le domaine économique, il défend notamment l'initiative privée, la libre concurrence et son corollaire l'économie de marché ; dans le domaine politique, il accepte des pouvoirs politiques encadrés par une loi librement débattue, défend un État de droit et des contre-pouvoirs.

Au libéralisme classique, fondé davantage sur la liberté en tant que droit négatif, s'oppose entre autres le libéralisme social fondé sur la liberté en tant que droit positif (protection exigée du souverain contre la misère matérielle ou la pression morale communautaire, quitte à accorder au souverain un droit de coercition sociale à cette fin). Ainsi le libéralisme peut se manifester de façons fort diverses, voire opposées. Le « libéral » peut être suivant le cas celui qui exige de l'État qu'il brise une tradition qui contraint la liberté de l'individu (caste, statuts, discriminations et privilèges…), celui qui défend la liberté de pratiquer ou non une religion ou une tradition, celui qui demande que l'État intervienne pour redonner une véritable capacité d'action économique (bridée par un monopole, la pauvreté, le manque d'éducation, de crédit ou autre), ou encore celui qui s'oppose à l'intervention du pouvoir (dans le respect de l'initiative privée, de la libre concurrence, de l’égalité de traitement…).

Les limites à fixer à l'action de l'État, ainsi que les modalités de l'action publique, notamment aux rôles respectifs de l'action administrative et de la loi, sont donc sujets à débat au sein même du libéralisme. La plupart des libéraux considèrent que l'action de l'État est nécessaire à la protection des libertés individuelles, dans le cadre de ses fonctions régaliennes, et nombre d'entre eux (comme Adam Smith, Raymond Aron, Karl Popper ou Benedetto Croce) acceptent et même recommandent certaines interventions de l'État dans l'économie, notamment en matière de contrôle et de régulation. À l'opposé, les libertariens de tendance anarcho-capitaliste refusent à l'État toute légitimité dans quelque domaine que ce soit.

L'individu est au centre du libéralisme. La plus haute tâche de l'État est d'assurer et de défendre la liberté individuelle qui est considérée comme imprescriptible. La liberté individuelle étant aux yeux des libéraux la norme fondamentale et le fondement de la société humaine autour de laquelle l'État, l'ordre politique et économique doivent être structurés.

Mais, alors que pour les libéraux classiques, la primauté de la liberté individuelle est un principe absolu qui s'applique à tous les domaines de la vie en société, il est devenu courant, pour les opposants à cette philosophie, de subordonner l’application de ce principe aux circonstances, de considérer les volets philosophique, politique, social et économique du libéralisme comme indépendants les uns des autres, voire de réduire le libéralisme à ses aspects économiques comme le fait l'usage moderne.

Il existe plusieurs courants de pensée libéraux qui se différencient notamment par leurs fondements philosophiques, par les limites et les fonctions qu’ils assignent à l'État, et par le domaine auquel ils appliquent le principe de liberté (économie, institutions politiques, domaine social). Le libéralisme a connu une fracture assez profonde à la fin du , date à laquelle on a commencé à distinguer notamment en Angleterre, le libéralisme classique du nouveau libéralisme appelé aussi parfois social-libéralisme. John Maynard Keynes, dans la "Théorie générale", a assimilé l'économie classique avec la loi de Say ou, dit de façon plus schématique, avec ceux qui prônent l'autorégulation des marchés. S'il a contribué par là à donner une signification forte à ce que pouvait désigner le libéralisme classique sous l'angle économique, cela ne va pas sans prêter à confusion. En effet, les grands économistes classiques anglais tels que David Ricardo ou John Stuart Mill auxquels l'expression peut faire penser sont considérés par Élie Halévy comme le versant économique de ce qu'il appelle le radicalisme philosophique et peuvent donc de ce point de vue être vus comme plus proches du social-libéralisme que du libéralisme classique qui à ce niveau a des racines plus continentales. En plus de ces deux courants, on peut citer le libertarianisme (minarchisme, agorisme et anarcho-capitalisme). Du point de vue de la théorie économique, il est possible de distinguer l'école néoclassique qui accepte en général une certaine régulation autoritaire des marchés et les écoles faisant une plus large confiance au marché telle que l'École autrichienne moderne, ou telle que l'école des choix publics étudiant l'instrumentalisation massive de la puissance publique par les lobbys de financiers, de medias, ou d'électeurs.

Sur le plan théorique, le libéralisme s’oppose aux doctrines selon lesquelles les comportements et les choix des individus sont ou doivent être subordonnés à une entité supérieure ou à ses représentants, par exemple la Nation, l'État, la communauté, la classe sociale ou la religion. En prônant l'égale liberté pour tous, il s'oppose aussi aux doctrines selon lesquelles les individus ont des droits différents selon certaines de leurs caractéristiques (racisme) ou selon leur appartenance à des communautés (communautarisme).

Sur le plan individuel, il reconnaît en particulier à chacun le droit d'adhérer aux convictions de son choix, tant qu'il ne cherche pas par ses actions à imposer ses convictions à d'autres.

En prônant la liberté individuelle, notamment par rapport aux traditions, le libéralisme peut s'opposer au conservatisme. Il s'oppose également à toutes les formes d'abus de pouvoir qui limitent voire détruisent la liberté des individus et/ou imposent une économie dirigiste et planificatrice, et "a fortiori" aux régimes politiques interventionnistes, autoritaires ou totalitaires (totalitarisme).

Le mot « libéralisme » naît sous la plume de Maine de Biran en 1818, il le définit comme 

De nos jours, le terme de "libéralisme" est utilisé dans des sens différents, plus ou moins larges, et quelquefois contradictoires. En partie à la faveur de ce flou sémantique, le libéralisme est l’objet de controverses nombreuses et souvent violentes qui résultent souvent d’un désaccord sur le sens même du terme. Si la reconnaissance et l'acceptation des aspects du libéralisme social et politique dépassent le seul courant libéral, le versant économique du libéralisme, lui, est beaucoup moins accepté (nombreuses prestations étatiques obligatoires, tentatives de contrôle de l'économie par le collectif). Certains opposants le considèrent comme une idéologie responsable de la plupart des maux du monde actuel, ou comme un courant de pensée étant aujourd'hui détourné de sa vocation d'origine, considérant que la liberté économique n'est pas automatiquement factrice de libertés individuelles pour tous les acteurs économiques.

Dans la tradition la plus ancienne, la liberté individuelle est un principe général qui s'applique à tous les domaines de la vie en société. Selon ce point de vue, c'est une erreur de séparer différentes formes de libéralisme, car toutes sont des conséquences indissociables d'un seul et même principe philosophique de liberté. Ce courant est souvent appelé « libéralisme classique » pour le distinguer des autres usages modernes du mot libéralisme. Pour ses partisans, il n'y a pas grand sens à séparer les dimensions du libéralisme qui, historiquement autant que théoriquement, sont profondément liées parce qu'elles appartiennent fondamentalement au même mouvement de pensée et à la même vision d’ensemble de l’ordre social.

D'autres auteurs ne voient pas le principe de liberté comme absolu et le prônent selon le domaine. Ils sont rejoints par des spécialistes qui ne s'intéressent qu'à un domaine bien précis, et qui pour cette raison évitent de parler du libéralisme en général. On distingue alors trois domaines principaux :

Les historiens des idées politiques s'intéressent aux courants qui se sont réclamés du libéralisme à différentes époques et en différents lieux. Ils distinguent ainsi plus finement un grand nombre de variétés dans les courants libéraux. C'est pourquoi le terme « libéralisme » recouvre aussi des réalités diverses selon les pays et leur histoire politique. Aux États-Unis, on appelle "Liberals" les sympathisants de l'aile gauche du Parti démocrate. Ce sont des progressistes à peu près équivalents aux sociaux-démocrates européens mais souvent en moins étatistes, ce qui les place à la gauche, voire à l'extrême gauche de l'échiquier politique américain : l'accent est mis sur la liberté de mœurs et les droits civils. L'usage est plus nuancé en Europe, les Britanniques désignant par exemple par "classical liberals" les libéraux au sens propre et les Allemands par "liberal" les mêmes personnes. En France et en Belgique notamment, le qualificatif « libéral » sert soit à désigner une personne favorable au libéralisme économique ou au capitalisme, sans nécessairement faire référence à la philosophie libérale, soit à une personne favorable au corpus libéral.

Le terme « libéralisme » pour désigner ce courant de pensée est apparu en France dès 1818 chez Maine de Biran et entre en 1823 dans le "Dictionnaire universel de la langue française" ou dans le lexique de Pierre-Claude-Victor Boiste. Le néologisme libéralisme est en réalité forgé par un parti des "liberales aux Cortes" en Espagne en 1812. Mais les origines de ce mouvement sont lointaines, l'idée a donc précédé le mot. De plus, l'adjectif libéral existait avant le néologisme et désignait l'esprit de tolérance pendant l'Antiquité et au Moyen Âge ("Art libéraux"), la revendication de droit individuels et la lutte contre l'absolutisme lors du siècle des Lumières.

Pour certains historiens des idées comme Philippe Nemo ou Murray Rothbard, ce que l'on définit généralement comme la tradition libérale prolongerait un mouvement d’idées perceptible depuis la liberté politique qui fut parfois accordée dans les Cités grecques, et la méfiance que l'on pouvait y trouver à l'encontre d'un gouvernement arbitraire. Ainsi, la représentation libérale du monde n'aurait pas surgi aux environs du mais aurait déjà été en germe par exemple sur l’"agora", où l'exposition et la critique de théories furent parfois possibles.

Dans le même mouvement, depuis Platon qui en donne une idée d’ dans le "Théétète", Aristote dans l’"Éthique à Nicomaque", Épictète et les Stoïciens, Diogène de Sinope et les Cyniques, un courant peut être identifié, affirmant la primauté de l’individu. Par certains côtés, le monothéisme abrahamique (judaïsme, christianisme et islam) affirme la responsabilité de l'individu.

L’humanisme de la Renaissance modifie profondément la définition du rapport de l'homme à la création, au pouvoir, à l'éducation ou au religieux, en particulier par l'affirmation du rôle central du libre arbitre. La fidélité à l'ordre de la tradition est remise en cause au profit de l'arbitrage de la raison de l'individu éclairé. Dès le , les philosophes de l'école de Salamanque reformulent la notion de droit naturel héritée d’Aristote, des stoïciens, de Cicéron et de Thomas d’Aquin, et en déduisent les principes de souveraineté du peuple et de séparation des pouvoirs. Dans le domaine économique, ils justifient la propriété privée, la libre circulation des personnes et des biens et défendent le libre marché. Au , le mouvement libéral s’incarne en particulier dans les "levellers" de la Révolution anglaise de 1642.

La pensée libérale se construit entre le milieu du et le milieu du , sous l’impulsion des philosophes des Lumières, en opposition à l’absolutisme politique légitimé par des conceptions religieuses. Les théoriciens du libéralisme sont nombreux et divers. Pour n’en citer que quelques-uns parmi ceux reconnus comme « grands penseurs libéraux » à la naissance du libéralisme, on évoquera Locke au , Turgot ou Smith au . La diversité de leurs écrits ne peut se comprendre qu'en fonction du contexte historique avec lequel ils étaient en interaction.

Ainsi, John Locke pose ce qui deviendra les fondements de la philosophie libérale moderne, avec l’« état de droit », en organisant et en développant ses thèmes principaux : théorie des droits naturels, limitation et séparation des pouvoirs, justification de la désobéissance civile, affirmation de la liberté de conscience, séparation de l’Église et de l’État, avec sa "Lettre sur la tolérance" de 1699, où il combattit les doctrines religieuses intolérantes.

Hume, Condillac et Montesquieu, quant à eux, développent les conséquences de leurs positions philosophiques libérales dans les domaines politique et économique. Montesquieu (1689-1755), faisant face au pouvoir absolu de la monarchie française se soucie alors principalement d’instaurer une séparation des pouvoirs afin de limiter les abus du pouvoir exécutif du roi et garantir ainsi les libertés du Parlement et de la Justice avec "L’Esprit des lois" (1748). Il articulera une pensée républicaine et libérale, et défendit ainsi « la vertu civique, l’amour de la patrie et la liberté ».

Parallèlement, avec le développement de la circulation des échanges en Europe, des penseurs plutôt connus en tant qu'économistes, comme Turgot et Adam Smith, prennent soin de rattacher leurs revendications pour les libertés économiques aux racines philosophiques du libéralisme, face à l'administration étatique alors très contrôlante. Adam Smith reste ainsi l'un des principaux théoriciens du libéralisme économique en fondant une théorie économique à partir d'une intuition de Bernard Mandeville. L’école libérale dite « classique » se constitue alors comme une pensée cohérente englobant tous les domaines de l’action humaine étudiés à cette époque.

Le libéralisme a exercé une profonde influence sur plusieurs grandes révolutions et traditions politiques — anglaise, américaine, française — qui ont permis l'émergence des « démocraties libérales ».

Depuis la « glorieuse révolution » anglaise de 1688, par laquelle les libéraux anglais chassèrent le roi Jacques II, le parlement anglais a institué une république et un régime représentatif, qui s’inscrit dans la continuité de la tradition libérale anglaise qui a poursuivi graduellement les améliorations des libertés politiques (« Magna Carta », « Bill of Rights », « Habeas corpus ») qui fit de l'Angleterre de l'époque le pays le plus libéral du monde. La voie libérale en Grande-Bretagne est donc née des particularités du droit anglais et de l'histoire propre au pays.

La mise en place des nouvelles libertés à la suite des glorieuses révolutions s'est introduite très rapidement dans le domaine économique et a contribué ainsi au développement économique selon David Hume, important penseur des Lumières écossaises. Voltaire, autre philosophe libéral de la même époque, louait ainsi le gouvernement anglais : « le commerce, qui a enrichi les citoyens en Angleterre, a contribué à les rendre libres, et cette liberté a étendu le commerce à son tour ».

Selon Tocqueville, le modèle américain tient à sa coupure radicale avec l’aristocratie européenne. La révolution américaine manifesterait ainsi une prédominance de l’esprit « démocratique » sur l’esprit « révolutionnaire ». Elle fut riche d'auteurs libéraux, de Thomas Jefferson à Benjamin Franklin en passant par Thomas Paine. Certains des principes fondateurs du libéralisme sont contenus dans le préambule de la Constitution américaine de 1787, ainsi que dans la Déclaration des droits de l'homme et du citoyen de 1789. La Convention de Philadelphie qu'adopta la Constitution des États-Unis, qui parachevait la conquête de l'Indépendance, fit l'admiration des démocrates et révolutionnaires français.

Quelques décennies avant la Révolution française, la France se fondait sur plusieurs principes du libéralisme, avec le ministère Turgot, influencé sans doute par le mouvement physiocrate. C'est pourquoi une partie des élites, notamment bourgeoises, ayant soutenu la Révolution française de 1789 et dirigé le pays après la chute de la monarchie constitutionnelle, était partisane du libéralisme qui se traduisait en France par une pensée subversive à l'encontre de la monarchie absolue de droit divin. La relation entre le libéralisme et la Révolution française est complexe puisqu’il est permis de les concevoir à la fois selon la continuité et comme deux termes opposés. Car avant d’être celle de la Terreur, la Révolution française est celle des droits de l’homme et aussi l'héritière de l'Ancien Régime. La Révolution française s'inscrit initialement dans le texte de la Déclaration des droits de l'homme et du citoyen qui est interprétée comme un rappel du droit naturel et des libertés économiques. La prédominance de l’esprit « révolutionnaire » sur l’esprit « démocrate » est née de la radicalité des révolutionnaires à vouloir recommencer à neuf leur histoire, à la différence des Américains, qui n'avaient pas d'Ancien Régime à détruire.

La Révolution française a montré que la tradition libérale pouvait se séparer et s'alimenter en plusieurs courants : un courant plus conservateur (Edmund Burke) considérant que les principes individualistes sont incapables de fonder le lien social, le deuxième plus radical (Thomas Paine) défend une réforme permanente de la société. Un autre plus classique a conduit à s'interroger sur la première révolution, la révolution de 1793 ou les résultats de la Terreur et le consulat puis la Restauration. Les écrits ou débats de la période qui suit la Terreur (Germaine de Staël, Tocqueville et Benjamin Constant) font apparaitre l'hétérogénéité de « l'esprit de 1789 » avec « l'esprit de 1793 » mais aussi une défense de la liberté politique qui repose sur la condition égalitaire de tous les citoyens au pouvoir politique. C'est le but du célèbre discours, "De la liberté des Anciens comparée à celle des Modernes", prononcé à l'Athénée royal de Paris par Benjamin Constant en 1819. Il faut aussi préciser la relation entre le libéralisme et le rousseauisme, ce qui permet de nouer les deux histoires, puisque Benjamin Constant et Immanuel Kant affirment la revendication inévitable d’égalité et la norme de l’intérêt général, évoquant le pluralisme dans les démocraties libérales.

Le début du voit l’approfondissement des idées libérales, dans la littérature, avec Victor Hugo, dans les aspects politiques, avec Benjamin Constant, Tocqueville et le groupe de Coppet qui rassemble des opposants libéraux à Napoléon III, avec Jean-Baptiste Say ou même John Stuart Mill dans les aspects économiques. Les libéraux s’efforcent ainsi de diffuser largement leurs idées, qui s’opposent aux idées étatistes prédominantes dans les cercles du pouvoir, comme a pu le faire Tocqueville en traquant l'origine du goût des Français pour la toute-puissance de l'État. Au milieu du siècle sont publiés aussi les célèbres pamphlets de Frédéric Bastiat.

Les idées libérales se diffusent dans la vie politique occidentale, au point de devenir la « base continue » des systèmes politiques à partir du selon Pierre Manent. Pour Raymond Aron, parlant de l'exemple britannique dans la première partie de "L'Opium des intellectuels" (1955), les idées libérales s'imposent au point d'être présentes dans les programmes de tous les partis et de ne plus nécessiter de parti spécifique.

À partir de la fin du , des divergences apparaissent au sein du courant libéral qui portent sur le rôle et la nature des interventions de l'État. Un courant progressiste apparait avec L. T. Hobhouse qui tente de prendre davantage en considération les conditions sociales qui permettent la liberté de chacun. Aux droits sociaux arrachés se sont ajoutés les droits politiques des citoyens (droit de vote et suffrage universel), inspirés par les libéraux républicains du (voir Tocqueville). Au confluent d’une double tradition, la tradition libérale anglaise, soucieuse de protéger en priorité les libertés fondamentales et la diversité des intérêts sociaux, va léguer l’économie de marché.

Au début du , la philosophie libérale va ensuite être radicalement contestée, d’abord par la révolution russe de 1917 puis pendant l'entre-deux-guerres avec la crise économique de 1929, les socialismes de gouvernement (notamment la Seconde République espagnole et le Front populaire français), l'émergence du fascisme et du national-socialisme. L'influence des doctrines opposées aux sociétés libérales entraîne une redéfinition du rôle et des contours de l'État dans le sens d’une intervention croissante (économie étatisée pour le communisme, et État fort et dirigiste pour le nazisme).

Face à l'adversaire communiste ou national-socialiste, la tradition dite autrichienne (avec Ludwig von Mises, Friedrich Hayek, Murray Rothbard) opposera dès les années 1940 une théorie libérale capable d’éviter selon Hayek « la route de la servitude ». Mais bien avant, Dans "Économie et société" et dans sa conférence sur le socialisme, le sociologue libéral Max Weber avait prophétisé la pénétration de l'État dans l'économie et l'apparition de fonctionnaires bureaucrates, échappant à tout contre-pouvoir. Max Weber en avait tiré la conclusion que « c’est la dictature du bureaucrate, et non pas celle de l’ouvrier, qui est en marche, du moins pour le moment ».

Après la Seconde Guerre mondiale, la théorie libérale est aussi renouvelée (Alain, Bertrand de Jouvenel, Raymond Aron ou Karl Popper et Benedetto Croce). Ce sera, en Europe continentale, la mise en place de « l’économie sociale de marché », telle que théorisée par l'Allemand Wilhelm Röpke. Le libéral britannique William Beveridge dans "Social Insurance and Allied Services" fournira également les bases de réflexion à l’instauration du "Welfare State" et du système de sécurité sociale en Europe occidentale. Dans les sociétés anglo-saxonnes, des divergences autour du libéralisme classique portent surtout sur le degré interventionniste et les idées keynesiennes depuis la création du FMI. Des débats qui opposeront l’école de "Welfare" de Pigou avec l’école du "Public Choice" (James M. Buchanan) ou l'école de Chicago (Milton Friedman, Ronald Coase).

Aujourd'hui, la philosophie libérale est portée notamment par des économistes comme Amartya Sen, des sociologues comme Raymond Boudon, et des romanciers comme Mario Vargas Llosa et Gabriel García Márquez.

Max Weber a souligné le fondement commun et le même socle anthropologique de tous les libéralismes : l'individu. Ainsi la domination de l’État par la société n’est pas libérale, mais l’absorption de l’État dans la société ne l’est pas non plus. L'État, après avoir été maitre de l'individu, doit se mettre à son service. Dans le champ politique, le libéralisme s'inscrit dans l'héritage des doctrines du droit naturel, autrement dit et en résumé, le passage de l'État nature à l'État civil s'établit sur la base d'un volontariat émanant d'individus libres. Dans le champ économique, il s'inscrit dans l'héritage des .

Le fondement de la pensée libérale est une théorie du droit selon laquelle chaque être humain est seul maître de lui-même et possède des droits fondamentaux et inaliénables qui découlent de sa simple existence et sont inhérents à la nature humaine, indépendamment des structures sociales dans lesquelles il est (ou n'est pas) inséré. Ces droits sont le droit à la liberté et le droit à la propriété.

Du droit à la vie découlent le droit de légitime défense contre toute agression, le droit à la sûreté et le droit de résistance à l’oppression.

La définition de la liberté individuelle est celle de l'article 4 de la Déclaration des droits de l'homme et du citoyen de 1789 :

Certains philosophes des Lumières lui préfèrent la définition suivante :

La liberté se traduit par le droit pour chacun d'agir comme il le décide afin de poursuivre ses objectifs propres par ses moyens propres, d'échanger, de s’associer et de contracter librement, de s'exprimer librement et de choisir librement ses sources d’information.

Le droit de propriété est le droit pour chaque individu de disposer à sa guise du fruit de son activité et des richesses qu'il a créées ou acquises de façon légitime, ainsi que de s’approprier toute chose (par exemple l’espace qu’il occupe ou l’air qu’il respire) qui n'est pas déjà la propriété d'un autre individu. Ces droits ont un caractère universel. Ils sont applicables à tous les êtres humains, à tout moment et en tout lieu, ce qui fonde l’égalité en droit.

Un droit naturel se distingue d'un droit positif en ce que son exercice ne suppose rien quant à l’action d’autres personnes et qu'il ne découle pas d'une définition législative. « Personnalité, Liberté, Propriété [...] sont antérieures et supérieures à toute législation humaine. » (Bastiat)

La thèse des droits naturels est largement développée par John Locke. De cette théorie est issue la conception moderne des droits de l'homme qui a fourni historiquement une partie de la justification idéologique de la Révolution américaine et de la Révolution française.

Cependant, la théorie des droits naturels a été vigoureusement contestée par Jeremy Bentham et John Stuart Mill. Selon ces deux auteurs, dont les idées sont déjà présentes "in nucleo" chez David Hume ("Enquête sur les principes de la morale", Section V, "Pourquoi plaît l'utilité", Deuxième partie), les principes du libéralisme ne ressortissent pas au respect de droits naturels dont Bentham et Mill nient par ailleurs l'existence, mais à la contribution essentielle de la liberté à notre bonheur. Pour les utilitaristes, une société heureuse est une société libre où chacun vit comme il l'entend tant que cela ne nuit pas à autrui. C'est le "principe de non-nuisance" développé par J.S. Mill dans son "De la liberté". L'utilitarisme pense donc que les sociétés libérales sont celles qui maximisent notre bonheur.

On voit dès lors où se situe la différence entre l'école libérale des droits naturels dont Kant est un des représentants les plus marquants, et le libéralisme utilitariste. L'utilitarisme admettra par exemple le sacrifice de certains au bonheur du plus grand nombre tandis que le libéralisme d'obédience kantienne tiendra la vie humaine pour sacrée et inaliénable puisque le respect absolu de la vie d'autrui est imposée par le droit naturel. Se pose alors la question de savoir si une démocratie libérale a le droit d'enrôler ses citoyens lorsqu'elle est en danger. Faute d'envisager des cas tels que la guerre, la théorie libérale des droits naturels s'interdit de penser le rôle de l'État (dont la version extrême libertarienne conteste du reste la légitimité) dans les relations internationales.

Inversement, l'utilitarisme libéral peut courir le danger grave de justifier les raisonnements du type "la fin justifie les moyens". Jusqu'où a-t-on le droit de sacrifier le bonheur de certains au bonheur du plus grand nombre ? Ou bien encore : y a-t-il place pour l'eugénisme dans une société libérale ?
La morale libérale peut se résumer par un seul précepte : "Tu ne violeras pas les droits naturels d’un autre être humain". Elle laisse chacun libre de choisir ses propres fins, ses propres moyens et sa propre morale, dans la mesure où il n’empêche pas les autres d’en faire autant.

Réciproquement, ces droits impliquent des obligations qui forment le noyau d'une morale personnelle. Ils impliquent l’interdiction de toute agression contre l’intégrité de la personne, du meurtre, du vol et de l’esclavage sous toutes leurs formes, et de toute forme de dictature. Ils commandent la tolérance à l'égard des idées, des croyances et des actes d'autrui.

À part cela, le libéralisme ne prescrit aucun comportement particulier au niveau individuel. Il considère que la morale et les religions sont hors de son domaine et se borne à interdire l’usage de la contrainte en matière religieuse ou morale, comme dans toutes les autres matières. La responsabilité, inséparable de la liberté et de la propriété, dit que chaque individu doit supporter les conséquences de ses actions, bonnes ou mauvaises. C'est une condition de la liberté : si autrui devenait responsable de nos actions, il devrait acquérir l'autorité pour nous imposer ses vues et donc restreindre notre liberté (un peu comme le ferait un parent envers son enfant). C'est aussi une composante de la sûreté d'autrui.

La notion de liberté est liée à celle d'égalité en droit : la liberté des autres implique de leur reconnaître les mêmes droits que ceux qu'on s'accorde. Pour les libéraux, tous les êtres humains doivent être traités comme des égaux quelles que soient leurs différences.

Le libéralisme n'est pas l'anomie comme absence de règles de droits. Le droit est formé d’une part par le droit naturel, et d’autre part par le droit positif qui est le produit des contrats passés entre les individus.

Le libéralisme exige de la société le respect des droits naturels et la limitation des compétences de l’État comme le dit le philosophe allemand Wilhelm von Humboldt dans son "Essai sur les limites de l'action de l'État" (1792). Une société libérale est le résultat des choix et des actions effectués librement par l'ensemble de ses membres, ce qui lui permet théoriquement de prendre des formes très diverses.

Cependant, la plupart des auteurs libéraux forment un pronostic explicite ou implicite sur la forme que doit prendre une société libérale. Ils partent du constat que l’être humain est un animal profondément social, qui est attentif aux sentiments et au bien-être de ses semblables et sait que l'association avec eux est le moyen de sa propre survie et de sa propre satisfaction. Tout en reconnaissant l'extrême diversité des êtres humains, les penseurs libéraux ont "a priori" confiance dans leur action spontanée, et pensent que les individus sont conduits par leurs instincts et leur raison à coopérer et à mettre en place des solutions efficaces d'un point de vue individuel et social (principe revendiqué par les libéraux depuis Mandeville dans "La Fable des abeilles", Montesquieu ou Adam Smith avec sa « main invisible » souvent mal comprise).

Rien dans le libéralisme ne s'oppose aux actions collectives, à condition que les associations qui les entreprennent soient entièrement volontaires et n'exercent aucune contrainte ni sur leurs membres, qui doivent pouvoir les quitter librement, ni sur les autres individus.

Selon les projets auxquels il veut participer, chaque être humain peut appartenir à un nombre quelconque de communautés, chacune ne pouvant lui demander que ce qui est nécessaire à la réalisation de son objet particulier. La société libérale idéale n’est ni une juxtaposition d’individus égoïstes étrangers les uns aux autres, ni une juxtaposition de communautés séparées, mais plutôt un enchevêtrement d’associations volontaires de toutes natures à travers lesquelles chacun peut travailler aux fins qu’il se donne, en coopérant librement avec ceux qui partagent tel ou tel de ses idéaux.

Par l’exemple et l’imitation, les objectifs, les règles et les méthodes adoptés par certains groupes peuvent se diffuser à l’ensemble de la société, faisant émerger un ordre spontané que les libéraux considèrent comme le seul légitime, à condition qu'il ne viole pas les droits naturels des individus.

Juliet Rhys-Williams et Milton Friedman sont à l'origine du concept d'impôt négatif dans lequel l'État joue un rôle prépondérant de protection sociale par le versement d'une allocation universelle couplée à l'impôt sur le revenu. Il s'agit cependant de la part de ses concepteurs non pas de préconiser une intervention sociale de l'État mais de préconiser la meilleure forme d'intervention sociale que l'État puisse proposer.

Le libéralisme classique admet que l’institution de l’État est nécessaire pour faire respecter l’interdiction de la violence. Chacun doit renoncer à utiliser la violence, selon le principe fondamental de responsabilité individuelle, et en confier à l’État le monopole, au service de la protection de chacun contre tous les autres.

L’État étant une organisation humaine, les libéraux pensent que le risque que les hommes qui le composent abusent de ce monopole de la violence est permanent. En même temps qu’il est le garant des libertés, l’État est donc perçu comme la plus grave menace pour ces mêmes libertés. Lui accorder « le monopole de la violence légitime » (Max Weber) a pour contrepartie nécessaire de limiter son domaine d’action de façon rigoureuse.

Pour les libéraux classiques, les seules fonctions légitimes de l’État sont celles qui assurent la protection du citoyen : police, justice, diplomatie et défense nationale. Ces fonctions forment l’État minimal limité à ses fonctions dites régaliennes. Dans l’exercice de ces fonctions, l’État doit être soumis aux mêmes lois que les citoyens, et ne pas faire de lois qu’il ne s’appliquerait pas à lui-même.

Le libéralisme classique ne se prononce pas sur la forme institutionnelle de l’État, mais seulement sur l’étendue de ses pouvoirs. Il préfère néanmoins les dispositions qui permettent de limiter effectivement ces pouvoirs, comme la démocratie et la séparation des pouvoirs.

Le libéralisme classique ne reconnaît pas de droits particuliers aux majorités, même démocratiquement élues. De la même façon qu’il interdit à un plus fort d’imposer sa volonté à un plus faible, il interdit à un plus grand nombre d’individus d’imposer leur volonté à un plus petit nombre. Le rôle de l’État libéral n’est pas de faire régner la loi de la majorité, mais au contraire de protéger la liberté des individus et des minorités contre les plus forts et les plus nombreux. En particulier, le libéralisme classique refuse qu’une majorité, même démocratique, puisse étendre le domaine d’action exclusif de l’État au-delà de l’État minimal.

Cette philosophie politique pourrait se résumer en trois citations :


Ces positions ont été développées au par l’École des choix publics, qui analyse les actions de l’État comme celles d'une organisation comme les autres (qui défend les intérêts particuliers de ceux qui la composent ou qui la soutiennent) et constate la non-existence de l’« intérêt général » (dans la mesure où il est impossible d'en donner la moindre définition ou caractéristique).

Les libéraux les plus radicaux, les libertariens ou anarcho-capitalistes, affirment que la sphère des attributions légitimes du pouvoir politique est vide, et que le risque pris en confiant à l’État le monopole de la violence est trop grand pour valoir d’être couru : ils considèrent donc l’État comme un ennemi et prônent sa disparition totale et la fin du politique ; l'économiste anarcho-capitaliste Hans-Hermann Hoppe, quant à lui, estime dans son ouvrage "" que la monarchie est un moindre mal par rapport à la « démocratie » pour contenir l'État, même s'il souhaiterait ce qu'il appelle une « société de droit privé ». Par opposition, les tenants des positions classiques sur l’État minimal sont souvent appelés minarchistes.

Les démocraties modernes sont qualifiées de "libérales" car y sont institués l’État de droit, la séparation et la limitation des pouvoirs ainsi que la liberté de la presse. Elles prennent soit la forme d’une République (exemple : Allemagne, Inde, France) soit d'une Monarchie constitutionnelle (exemple : Espagne, Norvège, Pays-Bas, le Royaume-Uni et son Commonwealth, Suède).

Deux positions coexistent dans la tradition classique. À la suite d’Adam Smith, l’école classique anglaise (Smith, Malthus, Ricardo, Stuart Mill) légitime une certaine intervention de l'État dans la sphère économique en lui assignant d'abord trois devoirs :

Au fil de la "Richesse des nations", Adam Smith ajoute d'autres prérogatives à l'État. Il prévient que la « main invisible » n'intervient que dans des situations de concurrence, comme dans le petit artisanat, et avertit que, pour leur part, les industriels conspirent toujours ensemble afin de faire monter les prix. L'État a donc le devoir de sauvegarder les conditions de la concurrence contre les capitalistes. Enfin, certaines activités de l'industrie ont des effets non souhaitables (principe des externalités) : la division du travail abrutit les hommes ; et il faut souhaiter que l'État prenne en charge ces désagréments, en assurant l'éducation de la population par exemple.

Pour les classiques français (Turgot, Condillac, Say), le libéralisme économique est essentiellement l’application de la philosophie libérale aux actes économiques : l'économie n'est qu'un des domaines de l'activité humaine où l'État n'a pas de légitimité à intervenir autrement que comme un acteur économique sans privilèges particuliers, et dans le plus petit nombre de domaines possible : la protection des citoyens, l'exécution de la justice et la défense contre d'éventuels agresseurs. Ils jugent inutile et dangereuse toute intervention supplémentaire, considérant d'une part que l'initiative privée, informée par le marché, est à même de suppléer avantageusement la plupart des fonctions de l'État, et, d'autre part, que l'extension de la sphère d'intervention de l'État conduit à une croissance non maîtrisée de la sphère publique au détriment de l'initiative privée, à des inefficacités chroniques, et même à des dérives totalitaires.

À cette forme du libéralisme classique, l’École autrichienne ajoute l'idée que tout accord librement consenti ou ensemble d'échanges librement consentis augmente la satisfaction des participants telle que perçue par chacun d'entre eux, car s’il en était autrement, celui qui se sentirait lésé refuserait cet accord qui n’aurait donc pas lieu. La liberté d’échanger et d’entreprendre est vue par ces auteurs à la fois comme un cas particulier du principe philosophique de liberté, donc un impératif moral qui s’impose indépendamment de ses conséquences, et comme un moyen qui conduit le plus probablement à la plus grande satisfaction générale.

La vision conséquentialiste du rôle de l'État est devenue prédominante de nos jours avec la conception néoclassique, qui voit dans la liberté des échanges un moyen d’arriver à un optimum économique. Pour certains néoclassiques, l’État doit alors faciliter l'enrichissement des citoyens, jouer un rôle primordial en tant qu'arbitre des échanges économiques, assurer le respect de l'exécution des contrats, encadrer les échanges marchands par une législation adaptée afin de corriger les défaillances du marché, gérer les biens publics, ouvrir des voies commerciales, etc. D'autres néoclassiques arrivent à la conclusion d'une nuisance générale des ingérences de l'État.

De même, le keynésianisme ou les diverses formes du « libéralisme de gauche », tout en se réclamant du libéralisme, recommandent une intervention « raisonnable » et limitée de l’État dans l’économie pour assurer le plein emploi, la stabilité économique et la croissance ; mais aussi pour mettre en place un « plancher » sous la société libérale afin d'aider les plus démunis, tout en gardant à l'esprit qu'il importe d'interférer le moins possible avec les libertés économiques et politiques fondamentales. Pour Noam Chomsky, au-delà de la vision traditionnelle du libéralisme comme volonté de limiter les fonctions de l'État, « à un niveau plus profond, la vision libérale classique est issue d'une conception précise de la nature humaine, qui met l'accent sur l'importance de la diversité et de la libre création. Cette conception s'oppose donc fondamentalement au capitalisme industriel, qui se caractérise par son esclavage salarial, son travail aliénant et ses principes hiérarchiques et autoritaires d'organisation sociale et économique ».

Compte tenu d'un risque naturel de constitution de cartels (ou trusts), toutes les grandes démocraties occidentales se sont dotées de lois antitrusts comme le "Sherman Act", qui visent à rétablir la fluidité des rapports économiques et protéger voire institutionnaliser la libre concurrence. Cette protection de la libre concurrence est considérée comme une gageure par certains économistes, à l'instar de ce qu'à pu écrire Alan Greenspan en 1962. D’autres ajoutent que les lois qui régissent le comportement des entreprises doivent être les mêmes pour toutes indépendamment de leur taille, et que toute discrimination reposant sur la taille des entreprises est illégitime et contre-productive. D'autres encore considèrent à l'inverse que le libéralisme suppose l'existence de lois antitrust garantissant la pérennité de la concurrence sans obstruction des entreprises géantes, tout en demandant à l'État de garder son rôle d'arbitre, et non de joueur. Faute d'un État fort, il serait par exemple difficile d'interdire les pratiques de vente liée, qui entravent par définition la libre concurrence. C'est pour la même raison – interdiction d'entente entre des producteurs économiques, mais cette fois-ci des ouvriers – que la grève fut quelque temps au considérée comme activité illégale.

La naissance du libéralisme correspond à l'avènement de la Révolution industrielle, et à l'apparition de modèles économiques qui ont fortement modelé la structure de l'activité économique, engendrant une hausse significative de la production et une diminution relative de la part de l'agriculture par rapport à celle de l'industrie.

L'économie post-industrielle, malgré le développement des technologies de l'information et de la communication en apparence peu consommatrices de ressources naturelles (mais il ne s'agit que d'apparences car elles consomment également des ressources) continue d'être fortement dépendante en ressources naturelles (énergies fossiles, matières premières).

Dès le début du , Arthur Cecil Pigou, dans ses travaux sur l'économie du bien-être, va prendre en considération les effets non désirés qui peuvent découler d'une relation marchande en inventant la notion d'. Pigou proposera par la suite la création d'une taxe qui portera son nom pour corriger les externalités négatives, ce qui donnera plus tard naissance au principe du pollueur-payeur en matière d'environnement.

Aujourd'hui encore, les modèles économiques continuent d'évoluer pour tenter de mieux intégrer les effets des croissances démographique et économique sur l'environnement, notamment l'épuisement des ressources naturelles non renouvelables.

Pour Jean-Claude Michéa, libéralisme culturel et libéralisme économique sont les deux faces d’une même médaille : un système qui n’accepte plus de limites. Contre ce qu’il appelle la « métaphysique du Progrès », responsable, selon lui, de l’atomisation du monde contemporain, le philosophe mise sur la « décence ordinaire » des classes populaires.

Une objection, transversale à plusieurs courants de pensée, est que le « libéralisme philosophique » fait la promotion d'une liberté purement formelle. Des critiques, de nature marxiste ou psychosociologique, opposent les libertés formelles (droit de circuler, par exemple) aux libertés réelles (capacité économique de réellement circuler, par exemple). Ces critiques reprochent aux libéraux de favoriser les droits de l'individu sans se préoccuper des conditions d'existence de ces mêmes individus au sein de la société. Le conservateur Michel Villey rejoint sur ce point la pensée marxiste quand elle soutient que si les droits formels libéraux sont supposés profiter à tous, ils ne profitent en réalité qu'à ceux qui peuvent matériellement les exercer : les riches, les propriétaires.

Des auteurs, comme Charles Taylor, avancent que les présupposés individualistes du libéralisme ne trouvent pas de traduction concrète : l’unité sociale est essentiellement le groupe selon leurs observations, et l’individu ne peut être appréhendé dans sa totalité sur des bases uniquement et strictement individuelles. Selon le groupe qui est considéré, on trouve différentes variétés d'holisme prenant en compte des réalités collectives telles que l’entreprise, l’association, la famille. Selon ces critiques, l'individu ne peut pas être une force agissante ou se considérer de prime abord comme libre au sein d'une société de masse.

Ouvrages fondateurs (par ordre chronologique)

Ouvrages d'analyse (par ordre chronologique)

Autres ouvrages (par ordre chronologique)

Articles



</doc>
<doc id="14898" url="https://fr.wikipedia.org/wiki?curid=14898" title="Panarchisme">
Panarchisme

Le panarchisme est une conception prônant la coexistence de tous les systèmes politiques, où chacun s'affilie au gouvernement de son choix (ou ne s'affilie à aucun gouvernement), et 

Le terme a été inventé en 1860 par le botaniste, économiste et romancier Belge Paul-Émile De Puydt.

Les panarchistes affirment que la panarchie est apolitique, puisque tous les pouvoirs politiques (impôts obligatoires, règlements imposés, autorité gouvernementale et administrative) disparaissent pour laisser place aux seuls rapports volontaires entre les hommes.

La panarchie a beaucoup en commun avec plusieurs formes d'anarchisme.

L’inventeur de la panarchie, Paul Emile de Puydt, la présente comme une forme de tolérance politique, succédant à la tolérance religieuse.

Une objection courante est que la panarchie existe déjà aujourd'hui d'une certaine façon : si l'on n'est pas satisfait d'une politique, on peut toujours quitter le pays et s'installer ailleurs. Cependant, la panarchie préconise l’a-territorialisme intégral (plutôt que l’extra-territorialisme qui est admis actuellement par le droit positif en diplomatie), c'est-à-dire « la fin du monopole de tout pouvoir territorial et de toute prétention à ce pouvoir, partout et pour tous » (selon Gian Piero de Bellis). 

La gouvernance n'est plus liée à un territoire donné, plusieurs gouvernements peuvent régir un même territoire, ce qui n'empêche pas des confédérations ou des alliances pour des questions territoriales qui relèvent des fonctions régaliennes, comme la défense. Les gens seraient libres de choisir à quel gouvernement adhérer tandis que les gouvernements auraient à rivaliser pour avoir des citoyens. 

Paul-Émile De Puydt a certainement été influencé par son compatriote Gustave de Molinari qui avait décrit en 1849 la notion de gouvernement concurrentiel, limitée à la défense, dans son article « De la production de la sécurité », dans le "Journal des économistes" du 15 février 1849, le terme sécurité n'étant pas ici restreint aux domaines de la sécurité physique, mais englobant au contraire l'intégralité des apports quotidiens que peut fournir un gouvernement (éducation, travail..).




</doc>
<doc id="14899" url="https://fr.wikipedia.org/wiki?curid=14899" title="Classe suivant un sous-groupe">
Classe suivant un sous-groupe

En théorie des groupes, les classes à gauche d'un groupe "G" suivant un sous-groupe "H" sont les parties de "G" de la forme "gH" avec "g" élément de "G", où "gH" désigne l'ensemble des éléments "gh" quand "h" parcourt "H". Elles constituent les classes d'une relation d'équivalence sur "G", donc forment une partition de "G". On peut les voir aussi comme les orbites de l'action à droite de "H" sur "G", par translations par les symétriques des éléments de "H".

L'ensemble des classes à gauche d'un groupe "G" suivant un sous-groupe "H" est noté "G"/"H". Il est naturellement muni d'une action à gauche de "G", qui est transitive.

L'ensemble des classes à droite d'un groupe "G" suivant un sous-groupe "H" est noté "H"\"G". Il est défini de façon analogue et vérifie des propriétés semblables.

Si le sous-groupe "H" est normal, alors "G"/"H" et "H"\"G" coïncident et forment le groupe quotient de "G" par "H".

Ces deux ensembles servent de modèles pour les espaces homogènes, car toute orbite d'une action de "G" s'identifie naturellement à un tel ensemble.

L'utilisation des classes intervient notamment dans l'étude des groupes finis, par exemple à travers le théorème de Lagrange et les théorèmes de Sylow.

Soient "H" un sous-groupe d'un groupe "G" et "g" un élément de "G".

On appelle "classe à gauche de g suivant H" l'ensemble "gH" défini par :formula_1

On appelle "classe à droite de g suivant H" l'ensemble "Hg" défini par :formula_2

L'ensemble des classes à gauche suivant "H" de tous les éléments de "G" se note "G"/"H", et celui des classes à droite, "H"\"G".

Étant donné un entier "n" fixé, l'ensemble "n"ℤ des entiers relatifs multiples de "n" forme un sous-groupe du groupe (ℤ,+).

La loi étant ici notée additivement, la classe à droite d'un entier "r" quelconque, suivant ce sous-groupe, est l'ensemble :formula_3C'est donc l'ensemble des entiers congrus à "r" modulo "n", i.e. des entiers "k" tels que "k - r" appartient au sous-groupe "n"ℤ.

La classe à gauche de "r" est égale à sa classe à droite, puisque l'addition est commutative. L'ensemble de toutes ces classes est le groupe ℤ/"n"ℤ.



D'après les trois premières propriétés précédentes, les classes à gauche suivant "H" forment une partition de "G" et la relation d'équivalence associée formula_15 (dont les classes d'équivalence sont les classes à gauche suivant "H") possède les descriptions équivalentes suivantes :formula_16

De même, les classes à droite suivant "H" sont les classes de la relation d'équivalence formula_17 décrite par :formula_18



</doc>
<doc id="14901" url="https://fr.wikipedia.org/wiki?curid=14901" title="Bataille de la Marne">
Bataille de la Marne

Il y eut deux batailles de la Marne, toutes deux au cours de la Première Guerre mondiale. 
Cependant, le nom se réfère en général à la première, qui eut lieu en 1914 ; la seconde se déroula quant à elle en 1918.


</doc>
<doc id="14902" url="https://fr.wikipedia.org/wiki?curid=14902" title="Cuisine indienne">
Cuisine indienne

La cuisine indienne recouvre une grande variété de cuisines régionales d'Inde. Elles sont influencées par les épices, herbes, fruits et légumes que l'on trouve dans chaque région du pays, mais également par la religion et l'histoire.

Ainsi, le végétarisme est très répandu dans la société indienne, majoritairement d'obédience hindoue, souvent résultat d'éthiques religieuses brahmaniques, jaïnes ou sikhes. La consommation de bovin ou de porc, de rapaces, etc., est limitée par les interdits de l'hindouisme et de l'islam. La cuisine indienne est influencée par les interactions avec la Perse et la présence européenne dans le sous-continent.

La cuisine indienne s'est répandue dans le monde avec les migrations des Indiens, notamment dans l'océan Indien, l'Europe, l'Amérique du Nord et les Antilles où elle a été enrichie d'apports nouveaux.

Les épices sont d'une grande importance dans la cuisine indienne. Beaucoup de plats en sauce sont faits à base de masala, un mélange d'épices qui caractérise chaque recette et qu'on nomme souvent curry ou cari (en fait, le mot « curry » découle d'un mot tamoul signifiant « sauce », « accompagnement pour le riz) ».

Parmi les épices utilisées :

Ces épices sont souvent utilisées sous forme de mélange (masala), par exemple :

Parmi les autres ingrédients : les noix de cajou, la noix de coco, le lait de coco, les pistaches, l'ail, l'oignon, les amandes, la grenade en graines séchées, la menthe, l'eau de rose, le sésame en graines.

La cuisine non-végétarienne comprend surtout du poulet, du mouton, du poisson. En raison des interdits religieux, le bœuf et le porc sont peu cuisinés.

D'une manière générale, la cuisine de Goa est moins piquante et l'influence occidentale est assez présente. Grâce aux Portugais, il y a eu le porc "vindaloo" (voir infra).

Beaucoup des plats indiens les plus connus en dehors de l'Inde, comme le poulet "tandoori", les "paranthas" ou le "dhal", sont originaires du Pendjab. La cuisine penjabie est influencée par les Moghols. Elle utilise beaucoup de "ghee" et de crème ainsi que du riz et du blé complet. Le masala comprend en général de l'oignon, de l'ail et du gingembre.

Les repas se prennent généralement accompagnés de toutes sortes de pains : "naan, paratha, roti" ou chapati.

Les cuisines du sud sont très épicées, et seule la présence de riz cuisiné à la vapeur ou le yaourt et le lait dans leurs différentes préparations (babeurre, etc.) permettent d'en tempérer le feu.

La cuisine tamoule, moins influencée par les Moghols que celle du nord de l'Inde, recourt beaucoup moins aux laitages. Elle est le plus souvent végétarienne, et fait appel, outre le riz, à divers légumes et lentilles. Parmi les plus appréciés des assaisonnements, on trouve les feuilles de curry, le curcuma, le tamarin, la coriandre, le gingembre, l'ail, les piments, la cannelle, le clou de girofle, la cardamome, le cumin, la noix muscade, la noix de coco, et bien d'autres encore, que fournit en particulier le Kerala voisin.

À la différence de certains états du nord, c'est le riz qui constitue à la fois la base du repas et l'accompagnement des plats qui sont traditionnellement servis sur une feuille de bananier, ou dans les "thalis", plateau ou assiette compartimenté en acier inoxydable.

Les musulmans moghols, en envahissant le nord de l'Inde, y ont apporté leurs recettes et modes de cuisson, en particulier les kebabs et la cuisson au "tandoor" ou "tandoori" : les "naans", le poulet "tandoori", etc.

De même, les ingrédients venus des Amériques (piments, pommes de terre, tomates, maïs) ont été intégrés dans les habitudes culinaires.

Certaines cuisines régionales ont également été influencées par les pays colonisateurs. Ainsi, les villes « comptoirs » de pays comme la France et le Portugal ont cuisiné le porc et le bœuf, peu fréquentes dans les autres régions. Par exemple, le porc qui arrivait du Portugal à Goa dans des tonneaux de vinaigre a donné le porc "vindaloo".

Sauf dans certaines régions du pays, la cuisine indienne n'a pas d'entrées ni de plats servis comme en France.





Il y a une très grande variété de pains, de galettes et du riz selon les régions ou les communautés en Inde. En voici quelques-uns, parmi les plus connus en France :

Le choix des desserts est très varié et change en fonction de la région. Les plus connus en France sont :

Le choix de boissons est très varié et change en fonction de la région et des communautés. Généralement, les Indiens ne boivent que de l'eau pendant les repas et les boissons sont prises soit lors d'un apéritif ou comme « coupe-soif ». Parmi les boissons les plus connues en France, il y a :

Et moins connus, étant donné que la plupart des Indiens ne consomment pas d'alcool, l"'asha", liqueur du Rajasthan et le "chang", bière du Sikkim et du Ladakh.





</doc>
<doc id="14911" url="https://fr.wikipedia.org/wiki?curid=14911" title="Sous-groupe">
Sous-groupe

Un sous-groupe est un objet mathématique décrit par la théorie des groupes. 

Dans cet article, ("G", ∗) désigne un groupe d'élément neutre "e".

Dans la pratique, on note la loi interne du sous-groupe avec le même symbole que celui de la loi interne du groupe, c'est-à-dire ∗.


L'élément neutre de "H" est idempotent donc égal à "e" (le neutre de "G"), et le symétrique (dans "H") d'un élément "h" de "H" est aussi (l'unique) symétrique de "h" dans "G". Pour cette raison, leur notation est la même dans "H" que dans "G".

D'après la propriété précédente, tout sous-groupe de "G" est non seulement stable par produits mais aussi par inverses, et il contient "e". La réciproque est vraie :

Une partie "H" de "G" est un sous-groupe de "G" si et seulement si :

De plus, dans cette équivalence, on peut (compte tenu de la condition 2.) remplacer la condition 1. par : "H" est non vide.

Dans le cas particulier des groupes finis, "H "est un sous-groupe de "G "si et seulement s'il est non vide et stable pour les produits. La condition de stabilité par les inverses n'est pas nécessaire, car elle découle de la stabilité pour les produits ; en effet, si l'on note "n "l'ordre d'un élément "a "de "H", l'inverse de "a "est "a", qui appartient à "H".

Soit "G" un groupe cyclique fini d'ordre "pq", où "p" et "q" sont deux entiers strictement positifs. Alors "G" a un unique sous-groupe d'ordre "p". Ce sous-groupe est cyclique, engendré par "g" où "g" est n'importe quel générateur de "G".
Les sous-groupes du groupe additif ℤ des entiers relatifs sont les parties de la forme "n"ℤ, pour n'importe quel entier "n".

Plus généralement, les sous-groupes non denses du groupe additif ℝ des réels sont les parties de la forme "r"ℤ, pour n'importe quel réel "r".
On en déduit le "théorème de Jacobi-Kronecker" : dans le cercle unité (le groupe multiplicatif des complexes de module 1), le sous-groupe des puissances d'un élément (qui est évidemment fini si est rationnel) est dense si est irrationnel.

Soit "S" une partie de "G". Il existe un plus petit sous-groupe de "G" contenant "S", appelé « sous-groupe engendré par "S" », et noté 〈"S"〉.

Si "G "est d'ordre fini, et "H "un sous-groupe de "G", alors le théorème de Lagrange affirme que ["G":"H"] |"H"| = |"G"|, où |"G"| et |"H"| désignent les ordres respectifs de "G "et "H". En particulier, si "G "est fini, alors l'ordre de tout sous-groupe de "G "(et l'ordre de tout élément de "G") doit être un diviseur de |"G"|.

Tout groupe d'ordre premier "p "est cyclique et isomorphe à ℤ/"p"ℤ.

La notion de sous-groupe est « stable » pour les morphismes de groupes. Plus précisément :

Soit "f ": "G "→ "G' "un morphisme de groupes.

Si "K "est un sous-groupe de "H "et "H "un sous-groupe de "G "alors "K "est un sous-groupe de "G", et de même en remplaçant « est un sous-groupe » par « est isomorphe à un sous-groupe ». Mais l'analogue du théorème de Cantor-Bernstein est faux pour les groupes, c'est-à-dire qu'il existe (parmi les groupes libres par exemple) deux groupes non isomorphes tels que chacun se plonge dans l'autre.

Les sous-groupes d'un groupe quelconque donné, forment un treillis complet pour l'inclusion. Il y a un sous-groupe minimal, le groupe {"e"} ("e "étant l'élément neutre de "G"), et un sous-groupe maximal, le groupe "G "lui-même. 
La borne inférieure de deux sous-groupes "A "et "B "est leur intersection "A"⋂"B". La borne supérieure est le sous-groupe
engendré par la réunion des sous-groupes, soit 〈"A"⋃"B"〉.

Les sous-groupes distingués d'un groupe "G "quelconque forment également un treillis pour l'inclusion. Les éléments minimal et maximal sont respectivement {"e"} et "G". 


</doc>
<doc id="14917" url="https://fr.wikipedia.org/wiki?curid=14917" title="Produit direct">
Produit direct

La plupart des structures algébriques permettent de construire de façon très simple une structure produit sur le produit cartésien des ensembles sous-jacents. Plus généralement, . C'est le cas de la topologie produit dans la catégorie des espaces topologiques.

Soient "E" un ensemble muni d'une loi de composition interne "T" et "F" un ensemble muni d'une loi de composition interne formula_1. On peut définir une loi de composition interne formula_2 sur le produit cartésien "E"×"F "de la façon suivante :

formula_3


Soit ("E") une famille d'ensembles, chaque "E" étant muni d'une loi de composition interne formula_12. On peut définir une loi de composition interne formula_2 sur le produit cartésien ∏ "E" de la façon suivante :

formula_14

Cette construction est valable que "I" soit un ensemble fini ou infini.


En particulier, le produit direct d'une famille de groupes est un groupe.

Soit ("E") une famille d'ensembles, chaque "E" étant muni de deux lois formula_22 et formula_23. On peut comme précédemment définir une loi formula_24, produit direct des
formula_22 et une loi formula_2, produit direct des lois formula_23.

Si chaque loi formula_23 est distributive par rapport à la loi formula_22, alors la loi formula_2 est distributive par rapport à la loi formula_24.

En particulier, si chaque "E" est muni d'une structure d'anneau, on construit ainsi un anneau produit direct.

Soit une famille ("E") d'espaces vectoriels sur un même corps "K". Les lois suivantes font du produit cartésien ∏ "E" un "K"-espace vectoriel, appelé produit de la famille ("E") :
formula_32
Le vecteur nul est la famille (0) formée par les vecteurs nuls des espaces "E".

Lorsque tous les "E" sont égaux à un même "K"-espace vectoriel "E "(par exemple à "K", vu comme "K"-droite vectorielle), ∏ "E" est l'espace vectoriel "E" des applications de "I" dans "E".

Somme directe


</doc>
<doc id="14921" url="https://fr.wikipedia.org/wiki?curid=14921" title="Sonora">
Sonora

Le Sonora ou l’État de Sonora est un État du nord du Mexique, délimité à l'est par l'État de Chihuahua, à l'ouest par l'État de Basse-Californie et par le Golfe de Californie, au nord par la frontière avec les États-Unis (Arizona), et au sud par l´État de Sinaloa. Les Yaquis, les Mayos et les Seris, qui peuplaient Sonora avant l'arrivée des Espagnols, furent à plusieurs reprises combattus et déportés avant et après l'indépendance du Mexique.

Le désert de Sonora s'étend sur le Mexique et les États-Unis. Certaines plantes et animaux n'existent que là-bas et sont protégés dans des parcs de conservation. Sonora attire de nombreux touristes sur ses plages. Cet État mexicain est composé de 72 municipalités et sa capitale est Hermosillo. Des aéroports desservent les villes d'Hermosillo, de Ciudad Obregón et de Guaymas.



</doc>
<doc id="14922" url="https://fr.wikipedia.org/wiki?curid=14922" title="Felix Klein">
Felix Klein

Felix Christian Klein ( à Düsseldorf – à Göttingen) est un mathématicien allemand, connu pour ses travaux en théorie des groupes, en géométrie non euclidienne, et en analyse. Il a aussi énoncé le très influent programme d'Erlangen, qui ramène l'étude des différentes géométries à celle de leurs groupes de symétrie respectifs.

Les parents de Klein sont prussiens ; son père était fonctionnaire en Rhénanie prussienne. Felix est élève du lycée de Düsseldorf, puis, en 1865, étudie les mathématiques et la physique à l'université de Bonn dans l'intention de devenir physicien. À ce moment, Julius Plücker est nommé à la chaire de mathématiques et de physique expérimentale à Bonn. Felix devient son assistant en 1866, à un moment où Plücker s'intéresse de près à la géométrie. Sous la direction de Plücker, il passe son doctorat en 1868, sous la direction de Plücker et Lipschitz.

Plücker meurt la même année, laissant derrière lui un livre inachevé, "Neue Geometrie des Raumes", sur la géométrie des droites projectives (voir coordonnées plückeriennes). Klein est la personne la mieux placée pour compléter la seconde partie. Il quitte donc Bonn pour Göttingen afin de travailler avec Alfred Clebsch. En juillet 1870, il se trouve à Paris. La guerre franco-allemande l'oblige à retourner en Allemagne ; il sert un temps dans l'armée prussienne avant d'être nommé lecteur à Göttingen en 1871.

En 1872, à l'âge de 23 ans, Klein est nommé professeur à Erlangen grâce à l'aide providentielle de Clebsch, qui voit en lui l'un des futurs plus grands mathématiciens de son temps. Trop peu d'étudiants se trouvant à Erlangen, il est heureux de se voir offrir une chaire à la "Technische Hochschule" de Munich en 1875. Il y enseigne les mathématiques à Adolf Hurwitz, Carl Runge, Max Planck, Luigi Bianchi et Gregorio Ricci-Curbastro. C'est aussi en 1875 que Klein épouse Anne Hegel, la petite-fille du philosophe Hegel.

Cinq ans plus tard, en 1880, Klein obtient une chaire de géométrie à l'université de Leipzig. Ces années à Leipzig sont marquées par une dégradation de sa santé. De 1883 à 1884, il souffre de dépression.

Sa carrière de mathématicien étant derrière lui, il accepte une chaire à l'université de Göttingen en 1886 ; il y restera jusqu'à sa retraite en 1913. Il dispense divers cours, la plupart à la frontière des mathématiques et de la physique, comme des cours de mécanique ou de théorie du potentiel. Il souhaitait rétablir Göttingen comme premier centre mondial de recherche mathématique. S'il n'y parvint pas, la formule qu'il initia, avec réunions de discussion hebdomadaires, salle de lecture et bibliothèque de mathématiques, a servi de modèle aux meilleurs centres de recherches.

Sous la direction de Klein, les Mathematische Annalen deviennent un des journaux de mathématiques les plus connus du monde. Fondé par Clebsch, ce journal rivalise puis surpasse le "Journal de Crelle". Klein a mis en place une petite équipe de rédacteurs qui se réunissent régulièrement pour prendre des décisions démocratiques. Le journal se spécialise dans l'analyse complexe et la géométrie algébrique. Il publie aussi sur l'analyse réelle et la théorie naissante des groupes.

Grâce en partie aux efforts de Klein, les femmes sont admises comme étudiantes à Göttingen à partir de 1893. Il supervise lui-même la première thèse soutenue en Allemagne par une femme, l'anglaise Grace Chisholm Young, .

À partir de 1900, Klein s'intéresse à l'apprentissage des mathématiques dans les écoles. En 1905, il recommande d'enseigner la représentation dans l'espace et les rudiments du calcul intégral et différentiel dès le secondaire. Cette recommandation est progressivement appliquée dans de nombreux pays à travers le monde. Il s'est engagé également dans la didactique des mathématiques.

Klein est élu membre de la Royal Society en 1885. La Société mathématique de Londres lui décerne la médaille De Morgan en 1893. Il est lauréat de la médaille Copley en 1912. Il prend sa retraite l'année suivante en raison d'une mauvaise santé, mais continue à donner des cours particuliers de mathématiques pendant quelques années.

Klein a eu de nombreux étudiants thèse : Ludwig Bieberbach, Maxime Bôcher, , Frank Nelson Cole, , , Robert Fricke, Philipp Furtwängler, Axel Harnack, Adolf Hurwitz, Edward Kasner, Ferdinand von Lindemann, Alexander Ostrowski, , , Hermann Rothe, , , , , , , , Grace Chisholm Young, Walther von Dyck…

Sophus Lie présente à Klein le concept de groupes, qu'il a aussi étudié aux côtés de Camille Jordan. Les premières découvertes importantes de Klein datent de 1870. En collaboration avec Lie, il étudie les propriétés fondamentales des lignes asymptotiques sur la . Ils en viennent à s'intéresser à des courbes invariantes sous un groupe de transformations projectives.

En 1871, alors à Göttingen, Klein fait d'importantes découvertes en géométrie. Il publie deux articles, dont "On the So-called Non-Euclidian Geometry", plaçant les géométries euclidiennes et non euclidennes sur un même plan, et mettant un terme à la controverse autour de la géométrie non euclidienne.

La synthèse de Klein de la géométrie comme étude des invariants sous un groupe de transformations donné, connue sous le nom de programme d'Erlangen (1872), influença profondément l'évolution de la géométrie et des mathématiques dans leur ensemble. Ce programme était le cours inaugural de Klein comme professeur à Erlangen. Il propose une vision unifiée de la géométrie. Klein décrit en détail comment les propriétés centrales d'une géométrie donnée se traduisent par l'action d'un groupe de transformations.

Aujourd'hui, cette vision est devenue tellement banale dans l'esprit des mathématiciens qu'il est difficile de juger de son importance, d'apprécier sa nouveauté et de comprendre l'opposition à laquelle elle a dû faire face.




</doc>
<doc id="14923" url="https://fr.wikipedia.org/wiki?curid=14923" title="Libertarianisme">
Libertarianisme

Le libertarianisme, aussi appelé libertarisme (à ne pas confondre avec libertarisme de gauche et libertaire) est une philosophie politique pour laquelle une société juste est une société dont les institutions respectent et protègent la liberté de chaque individu d’exercer son plein droit de propriété sur lui-même ainsi que les droits de propriété qu’il a légitimement acquis sur des objets extérieurs. Issue du libéralisme, elle prône donc, au sein d'un système de propriété et de marché universel, la liberté individuelle en tant que "droit naturel". La liberté est conçue par le libertarianisme comme une valeur fondamentale des rapports sociaux, des échanges économiques et du système politique.

Les libertariens se fondent sur le "principe de non-agression" qui affirme que nul ne peut prendre l'initiative de la "force physique" contre un individu, sa personne, sa liberté ou sa propriété. De fait, ses partisans, les libertariens, sont favorables à une réduction voire à la disparition de l'État (antiétatisme) en tant que système fondé sur la coercition, au profit d'une coopération libre et volontaire entre les individus, avec un État limité à des fonctions régaliennes. Robert Nozick (1938-2002), Murray Rothbard (1926-1995) et Charles Murray (1943-) font partie des principaux auteurs nourrissant cette doctrine.

Les idées défendues par les libertariens américains trouveraient leur origine en Europe : leur paternité est attribuée, notamment par Murray Rothbard, au belge Gustave de Molinari, lequel ne fit que pousser le raisonnement de l'État régalien de son maître à penser, le député libéral français Frédéric Bastiat, jusqu'à sa limite logique et cohérente concluant à une concurrence entre micro-communautés.

Le mot « libertarien » est l'adaptation en français de l'anglais « », lui-même traduction anglaise du français « libertaire ». Ce néologisme a été inventé afin de distinguer les libertariens des libéraux des États-Unis (lesquels sont estimés à gauche de l'échiquier politique des États-Unis, voir libéralisme contemporain aux États-Unis), le "libertarianism" se faisant le promoteur d'un marché sans entrave (voir libre marché) au nom de la liberté individuelle.

Le Parti libertarien, se revendiquant de ce courant de pensée, est né en 1971 aux États-Unis, avec la publication du livre de Robert Nozick, "Anarchie, État et utopie", qui critiquait la "Théorie de la justice" de John Rawls et notamment son « principe de différence » () . 

Au début du , le parti libéral britannique, au pouvoir, pratiquait des politiques de plus en plus étatistes. L'évolution se poursuivit dans les années 1920, au cours desquelles l'économiste John Maynard Keynes redéfinit un nouveau libéralisme. Dans les années 1950, à la suite de la répression politique opérée par le maccarthysme, les socialistes américains, dans la tradition de la social-démocratie, se sont massivement affirmés "« liberals »", reprenant la tradition keynésienne.

Le mot "liberal", aux États-Unis en étant venu à désigner les progressistes favorables à l'intervention de l'État dans l'économie, des libéraux américains (au sens original du terme) ont repris à leur compte le mot "libertarian", qui aux États-Unis n'avait pas la même connotation que "libertaire" en France (originellement, ce terme a été forgé par opposition au terme " libéral " par Joseph Déjacque). Le mot "libertarian" s'est depuis implanté en Grande-Bretagne (où il avait des connotations d'anarchisme socialiste), fort de toute la littérature "libertarian" déjà existante.

Dans les années 1970, Henri Lepage, en traduisant le terme "libertarian", et en l'absence de littérature "libertarian" francophone, n'a pas voulu risquer l'amalgame avec les libertaires, et a donc préféré utiliser « libertarien ». Les "libertarian" francophones du Québec ont repris le terme « libertarien », phonétiquement proche de l'américain "libertarian".

Certains libéraux/libertariens considèrent l'usage du terme comme un anglicisme et une erreur, puisqu'en France, le terme " libéral " ne prête pas à confusion (même s'il a pris un sens plus large) puisque ceux qui s'en réclament défendent bien le libre-échange et ceux qui s'y opposent le font sur cette base. Ils relèvent notamment la tradition de libéraux comme Frédéric Bastiat dont ils se réclament. Ils préfèrent donc se dire tout simplement "libéraux".

Dans le domaine politique, les idées libertariennes ont été exprimées dès le avec les œuvres de Wilhelm von Humboldt ("Essai sur les limites de l'action de l'État"), Herbert Spencer, Lysander Spooner et Gustave de Molinari.

Dans le domaine économique, elles ont été exprimées dès le par les Physiocrates, notamment Vincent de Gournay et Turgot, et développées entre autres par Condillac ("Le commerce et le gouvernement considérés relativement l'un l'autre") et Jean-Baptiste Say dans son "Traité d’Économie Politique".

Au , elles ont été reprises et développées par l'école autrichienne d’économie, dont les auteurs principaux sont Carl Menger, Ludwig von Mises, Friedrich Hayek, et Murray Rothbard.

Le libéralisme libertarien semble échapper à la dichotomie politique classique gauche/droite de par ses thèses qui le situent à la fois à gauche au plan des libertés individuelles (dépénalisation des drogues, liberté d'expression, liberté d'immigration, liberté sexuelle, refus de la conscription...) et à droite au plan des libertés économiques (respect de la propriété privée, liberté d'entreprendre, libre-échange, réduction drastique de la fiscalité, rejet des politiques étatiques de redistribution...).

Certains libertariens – notamment les anarcho-capitalistes – refusent tout État. D'autres veulent, en vertu des théories minarchistes, le restreindre à un État minimal – à savoir Police, Justice et armée – permettant de garantir le droit à la propriété.

Le libertarianisme pose la liberté individuelle comme valeur suprême et fin en soi plus encore qu'il n'est l'anti-Keynes.

David Nolan, fondateur du parti libertarien américain, a créé un diagramme pour démontrer sa doctrine, diagramme largement parce qu'il ne montre, selon eux, que les thèmes que défendent les libertariens (libéralisme économique et libertés individuelles au sens libéral), sans prendre en compte les idées défendues par les autres courants politiques.

Le libertarisme réunit quatre grands principes dans le but de préciser les droits de propriété légitimes qui circonscrivent la liberté de chacun :

Du point de vue économique, la doctrine du libertarianisme se rapproche de celle de l'anarcho-capitalisme par le fait qu'aucun impôt n'est envisageable – excepté pour subvenir aux respects des droits de police et de justice – ni aucune loi relative au droit de la concurrence qui sanctionnerait les pratiques anticoncurrentielles ou anti-monopole. 

Du point de vue sociétal, la doctrine libertarienne est en faveur d'une société permissive : sans service étatique (ni service militaire obligatoire ni fonction publique), sans prohibition concernant le blasphème, la discrimination, la consommation de drogue, les pratiques sexuelles entre adultes consentants, la pornographie et sans aucune réglementation étatique liée à la sécurité personnelle tel que le port du casque ou de la ceinture.

Le libertarianisme est l'objet de nombreuses critiques, tant de la part des conservateurs que des socialistes et des anarchistes anticapitalistes.

Une des critiques fréquentes accuse le libertarianisme d'être une liberté faussée en particulier par l'argent. Pour Philippe Van Parijs, l'argumentation libertarienne poussée à ses limites conduit à adopter une position « réal-libertarienne », interventionniste (voir aussi Gerald Cohen, du courant du marxisme analytique et qui défend une position libertarienne de gauche), qui remplace la liberté formelle des auteurs libertariens classiques par le principe d'une liberté réelle maximale pour tous, ce qui le conduit à défendre le concept d'une allocation universelle et à autoriser les interférences de l'État dans des cas exceptionnels (par exemple lorsque des actes rationnellement motivés au niveau individuel conduisent à des irrationalités collectives, limitant la liberté réelle de chacun : l'État pourrait ainsi interdire, par exemple, aux agriculteurs d'utiliser des engrais dont le rejet dans la mer, par la prolifération d'algues, restreindrait la liberté des pêcheurs).

Les libertariens rejettent cette critique en s'appuyant sur les importants fonds privés des associations caritatives qui financent des œuvres de bienfaisance comme l'éducation et la santé des démunis partout dans le monde, avec comme exemples courants le Fonds mondial pour la nature, la Fondation Rockefeller ou la fondation Bill-et-Melinda-Gates. Les libertariens estiment que le bénévolat privé est réduit d'autant plus qu'augmente la redistribution publique, et réciproquement.

Pour le linguiste Noam Chomsky, « la version américaine du “libertarisme” est une aberration – personne ne la prend vraiment au sérieux. Tout le monde sait qu'une société qui fonctionnerait selon les principes libertariens américains s'autodétruirait en quelques secondes. La seule raison pour laquelle certains font mine de la prendre au sérieux, c'est qu'ils peuvent s'en servir comme d'une arme. [...] C'est une aberration exclusivement américaine qui n'a rien de très sérieux ».

Il existe au sein de la mouvance libertarienne plusieurs tendances, s'opposant parfois entre pro-propriétés et anti-propriétés. 

Cependant, toutes s'accordent sur le principe fondamental de souveraineté individuelle qu'elles partagent également avec le courant de l'anarchisme individualiste.

En France, il existe depuis 1991 une association du nom d'ADEL (Association des Étudiants Libéraux, puis Association des Libertariens), qui représente la tendance anarcho-capitaliste.

Les associations politiques Liberté Chérie et Alternative libérale, ainsi que le parti politique Parti Libéral Démocrate diffusent des analyses libertariennes ou proches du libertarianisme (minarchisme). Ils se distinguent du libéralisme économique traditionnel par leur promotion d'un " libéralisme grand angle " ou " libéralisme authentique ". Ils restent néanmoins des libéraux classiques, au sens où ils considèrent l'État comme un mal nécessaire : disant , le parti ne prétend pas supprimer l'État mais le réformer. Certaines propositions d'intervention dans le domaine de l'éducation (le chèque éducation) font des partisans d'alternative libérale des libéraux classiques dans l'ensemble, les libertariens constituant une minorité. L'association SOS Éducation fait aussi partie de cette mouvance et a donné lieu à des critiques sévères, y compris de la part de personnalités de droite jacobine.

Le février 2013 a été constitué le premier mouvement politique français à vocation électorale se réclamant officiellement des thèses libertariennes : le Mouvement des Libertariens. Il participe à sa première élection lors de l'élection législative partielle du dans la du Lot et Garonne où Stéphane Geyres (président du mouvement à cette époque) rassembla 56 voix, soit 0,17 % des suffrages exprimés. Ce mouvement est relancé en avril 2017 sous le nom "Parti libertarien".

Le libertarianisme a une existence politique dans plusieurs pays, principalement de tradition anglo-saxonne : 

Il y a eu au cours de l'histoire plusieurs projets de mise en pratique des principes libertariens pour organiser une cité ou une nation.

Notons par exemple le projet « La République de Minerve » de fondation d'une micro-nation anti-interventionniste dans les îles Tonga.

L'animateur de radio américain Glenn Beck a créé le projet « Independance » visant à réaliser une ville autonome fonctionnant selon les principes libertariens.

Au même moment, le projet « The Citadel » vise à construite une citadelle libertarienne dans les montagnes de l'Idaho.

Créé à l'initiative de Patri Friedman, petit-fils de l'économiste américain Milton Friedman, l'institut Seasteading ambitionne de créer des îles artificielles dans les eaux internationales pour y vivre selon les principes libertariens. L'institut est notamment financé par Peter Thiel, fondateur de PayPal.





</doc>
<doc id="14927" url="https://fr.wikipedia.org/wiki?curid=14927" title="Lynx">
Lynx

Les Lynx (genre Lynx) sont des félins de la sous-famille des félinés. Parmi les félins, les lynx sont aisément reconnaissables à leur face ornée de favoris, à leurs oreilles triangulaires surmontées d'une touffe de poils noirs, et à leur corps doté d'une courte queue et de longues pattes. Parmi les caractéristiques moins visibles, les lynx ne possèdent que 28 dents, au lieu des habituelles chez les félins.

Descendants du Lynx d'Issoire, les lynx ont connu de nombreuses classifications taxinomiques différentes et les diverses espèces ont tour à tour été sous-espèces puis espèces à part entière. Depuis la fin du , seules quatre espèces sont reconnues : le Lynx du Canada ("Lynx canadensis"), le Lynx boréal ("Lynx lynx"), le Lynx pardelle ("Lynx pardinus") et le Lynx roux ("Lynx rufus"). Le Caracal, qui morphologiquement ressemble aux lynx, a longtemps fait partie du genre "Lynx" et est encore appelé « Lynx du désert ».

Prédateurs de l'hémisphère nord, les lynx ont pour habitat préféré la forêt boréale. Considérés comme très largement répandus, exception faite du Lynx pardelle gravement menacé, ils font partie des rares félins dont on estime les populations stables. Alors qu'ils tenaient une place importante dans la mythologie amérindienne, les lynx étaient fort méconnus en Europe et y ont souffert d'une réputation de bête féroce.

Les lynx ont un physique très reconnaissable, et peuvent difficilement être confondus avec les membres d'un autre genre de félins, hormis peut-être le Caracal. Le corps est caractérisé par une démarche chaloupée du fait de leurs membres postérieurs très développés, ce qui est une particularité du genre, les félins ayant généralement la partie antérieure du corps plus puissante. Les jambes sont longues et les pattes volumineuses comparées au reste du corps ; il s'agit d'une adaptation au déplacement dans la neige : les longues pattes permettent de se dégager plus facilement d'un épais manteau neigeux et les pieds très larges agissent comme des raquettes afin de ne pas s’enfoncer dans la neige. De plus, la largeur des coussinets étouffe le bruit des pas et assure une démarche totalement silencieuse. Les lynx exercent une pression très faible sur le sol, même en comparaison avec d'autres carnivores : ainsi le Lynx boréal exerce une pression sur le sol trois fois plus faible que celle du Chat sauvage ("Felis silvestris") et on estime ce ratio entre 4,1 et 8,8 pour le Lynx du Canada et le Coyote ("Canis latrans"). L'empreinte des lynx, aussi longue que large, ressemble à celle du chat domestique. La piste est quasiment rectiligne, surtout lorsqu'ils avancent au pas.

La queue est courte, comme tronquée et se termine en manchon ; elle mesure à peine 20 à de long. La taille totale varie selon les espèces, mais reste dans les mêmes proportions : seul le Lynx boréal se différencie par son gabarit pouvant être deux fois plus élevé que celui des autres espèces. Le dimorphisme sexuel est important : les mâles sont en moyenne un quart plus gros que les femelles.

La quantité de taches et la couleur de la robe des lynx varient selon les espèces et la latitude. Quatre types de robes sont reconnus : tacheté, rayé, uni et à rosettes. Chaque individu a une disposition particulière des marques. Parmi les quatre espèces de lynx, le Lynx pardelle a une fourrure très tachetée, tandis que le Lynx du Canada a peu ou pas de taches, notamment parce que sa longue fourrure a tendance à atténuer les marques. Au nord, les robes des lynx sont plutôt de couleur grise tandis qu’au sud elles tendent vers le roux. En règle générale, les joues, le ventre, l'intérieur des pattes, le menton et le tour des yeux sont de couleur crème. Le Lynx du Canada et le Lynx boréal ont une fourrure particulièrement dense, notamment sur le dos où la concentration de poils atteint contre sur le ventre ; on compte également de douze à treize poils de bourre pour un poil de jarre.

La tête des lynx, de forme arrondie et portée par un cou court, est également assez caractéristique. Les oreilles sont triangulaires, longues et ornées d'une touffe de poils noirs appelée « pinceau ». Ces pinceaux auriculaires ne se trouvent que chez les espèces du genre "Lynx" et également chez le Caracal, le Chat des marais et certaines races de chat domestique. Il se pourrait que cela permette de capter la direction du vent. De longs poils le long des joues, appelés « favoris », forment une collerette qui leur donne un air un peu joufflu. L’utilité de la forme du visage des lynx, notamment des pinceaux auriculaires, a été discutée. Matjuschkin a proposé une analogie avec la face du hibou, très ronde, avec des petites plumes dressées sur la tête : les favoris autour des joues du lynx formeraient un miroir parabolique permettant de mieux capter les sons, tandis que les pinceaux amélioreraient la localisation sonore.

Les lynx ont pour caractéristique de n'avoir que 28 dents au lieu des 30 habituelles chez les félins : ils ne possèdent que deux prémolaires sur la mâchoire supérieure, ce qui est une caractéristique du genre "Lynx". Le raccourcissement des mâchoires conduit à l’augmentation de la puissance de la morsure. Au sein du genre "Lynx", seul le Lynx boréal a la possibilité d'avoir une dent surnuméraire. La dentition lactéale des lynx ne comprend pas de molaires, l'ordre d'apparition des dents est canine - incisive - prémolaire, puis, pour la dentition finale incisive - canine - prémolaire - molaire.

Comme tous les félins, les lynx ont une vision très sensible en faible luminosité et très précise pour détecter le mouvement. L’odorat est puissant, mais il ne sert qu’à la communication intraspécifique (marquage du territoire par exemple), et jamais pour la chasse comme pour les canidés. Les vibrisses, souvent appelées « moustaches », se trouvent sur le museau, au-dessus des yeux, sur les joues et au niveau des pattes : comme pour tous les félins, elles sont un organe du toucher très sensible. Les lynx ne réagissent pas à la cataire (herbe à chat) en captivité, mais seraient attirés par son odeur en liberté.

Ils sont capables de nager quand il le faut, et d’excellents sauteurs et grimpeurs, grâce à leurs membres postérieurs particulièrement adaptés au bond. Des lynx captifs se sont par exemple évadés en sautant par-dessus leur clôture de trois à quatre mètres. Comme tous les félins, les lynx sont de très mauvais coureurs de fond. Cette faible endurance peut être corrélée à la petite taille du cœur : le poids du cœur d'un lynx ne représente que 3,4 à de sa masse totale. Les lynx connaissent trois allures : le pas, qui est l'allure la plus utilisée, le trot et le bond.

Comme tous les félins, les lynx sont territoriaux. Le territoire du mâle recouvre celui d'une ou plusieurs femelles. Les territoires, tous sexes confondus, comportent cependant des « zones neutres » où il est possible de circuler sans qu’il y ait affrontement : les limites du territoire sont fréquemment des zones neutres chez les lynx. La taille du territoire dépend de la densité en proies et de l’espèce de Lynx considérée. Le territoire du mâle peut atteindre en Amérique du Nord. Le lynx mâle est intolérant envers les autres mâles traversant son territoire, même si ce sont les femelles qui restent les plus vindicatives entre elles.

Les marquages olfactifs, qui permettent de signaler sa présence sur le territoire, sont le plus souvent effectués sur un support facilement repérable. Il s'agit le plus souvent de jets d'urine et de marques de griffures. Les marquages sont plus fréquents au centre du territoire que sur sa périphérie.

Les lynx sont généralement solitaires, excepté les femelles avec leurs petits. Les seules rencontres entre mâle et femelle se déroulent durant la période de reproduction, pendant laquelle le mâle suit la femelle dans tous ses déplacements.

Extrêmement discrets, les lynx sont rarement visibles. Dans le parc national de Bavière, où le Lynx boréal a été réintroduit, annuels empruntent un sentier à du lieu de reproduction du lynx ; l’ensemble du parc de , contenant six lynx résidents, était visité par de personnes en 1976. Pourtant, seules six à huit observations annuelles ont été rapportées.

Les lynx sont surtout actifs au crépuscule et au lever du soleil. Ils chassent principalement à l'affût. Comme la plupart des félins, les lynx asphyxient généralement leurs proies par une morsure à la gorge, sans utiliser leurs pattes pour les assommer. Ils peuvent parcourir leur territoire à la recherche de proies sur plusieurs kilomètres. La fréquence de chasse est d’une proie tous les deux à trois jours. Le taux de réussite de la chasse varie énormément selon les individus. Pour le Lynx boréal, on estime que les femelles accompagnées de leurs petits réussissent leur chasse dans 60 à 70 % des cas, les mâles dans 40 à 60 % des cas et les subadultes dans 10 à 20 % des cas. La distance entre l’attaque et la mise à mort est généralement de moins de vingt mètres. Les lynx ne poursuivent leur proie sur plus de deux cents mètres que dans 1 à 5 % des attaques.

Les proies capturées sont différentes selon les espèces. La plupart du temps, les lynx se nourrissent de petites proies comme les lagomorphes ou les oiseaux. Le Lynx boréal est le seul à s’attaquer de préférence aux petits ongulés comme le chevreuil ou le chamois, bien qu'il arrive que le Lynx roux s’attaque aux Cerfs de Virginie et que le Lynx du Canada chasse le Caribou. Le lynx n’est pas un charognard et refuse toute nourriture en état de décomposition trop avancé. Les lynx peuvent s'attaquer au bétail : la pression de prédation sur les animaux domestiques est très variable selon les régions. Des cas de lynx spécialisés dans la chasse au mouton ont été rapportés. Lors de réintroductions de lynx, on constate une augmentation brusque des attaques sur le bétail suivie d'une période de stabilisation. En Europe, l'action des lynx sur le bétail est considérée comme mineure comparée à celles du loup et de l'ours. Les lynx n'attaquent pas l'être humain, pas même lorsque celui-ci s'approche de leur progéniture.

Les lynx mangent en position accroupie en commençant par les parties charnues de leur proie, comme les cuisses ou les épaules et n'attaquent jamais l'estomac ni les intestins. La peau et les poils sont repoussés peu à peu durant le repas et la peau retroussée finit souvent par « empaqueter » les parties du corps non mangées. Les oiseaux sont plumés. Les lynx peuvent également tirer leur proie sous le couvert des arbres afin de manger au calme.

Les attaques du lynx sur les troupeaux de cervidés favoriseraient la dispersion des hardes, permettant ainsi une meilleure répartition de l'espèce sur l'ensemble du territoire . Cela pourrait avoir un impact sur les jeunes pousses mangées par les chevreuils et garantir un meilleur équilibre des écosystèmes.

Le cycle de reproduction des lynx est soumis à de grandes variations. Ainsi, le cycle du Lynx du Canada est en étroite connexion avec celui du Lièvre à raquettes ("Lepus americanus") et sa population fluctue environ tous les dix ans. De même, les observations menées sur le Lynx boréal montrent que selon les années, seuls 43 à 64 % des femelles donnent naissance à des jeunes.

La saison des amours se situe majoritairement à la fin de l'hiver. Après une parade amoureuse de plusieurs jours, le mâle retourne à ses occupations tandis que la femelle part en quête d'un gîte pour mettre bas après une gestation d'environ deux mois. Elle élève seule ses petits et leur apprend à chasser. Ils quitteront leur mère quelques semaines avant la naissance de la génération future. Ces subadultes chercheront un nouveau territoire : la dispersion est assez faible puisque les jeunes s'installent sur des territoires proches de ceux déjà occupés.

Les lynx sont très peu vecteurs de la rage. Sur mille lynx de Slovaquie capturés ou tués sur dix ans, seuls 0,6 % étaient infectés par le virus rabique. De plus, les lynx ne développent pas la forme agressive de la maladie et ont tendance à faire diminuer les populations de renards (très sensibles à la rage) par pression de prédation. Les décès par maladie ne représentent qu'un quart des décès totaux. Les trois-quarts des décès des adultes sont dus à l'activité humaine, soit par une pression de chasse et/ou de braconnage, soit par le trafic routier. Pour les jeunes, c'est avant tout la famine et les maladies parasitaires qui déciment les populations (80 % des jeunes n'atteignent pas l'âge de procréer chez le Lynx boréal). Les lynx ont assez peu de prédateurs naturels en dehors de l'Homme. Selon les espèces, ours, loups, pumas et gloutons peuvent attaquer et tuer un lynx. La longévité est d'une quinzaine d'années dans la nature et d'environ trente ans en captivité.

La classification des lynx a fait l'objet d'un débat : les lynx devaient-ils être classés dans leur propre genre "Lynx" ou être un sous-genre de "Felis" ? En effet, jusque dans les années 1980, presque tous les félins étaient inclus dans le genre "Felis", excepté les grands félins du genre "Panthera" et le guépard du genre "Acinonyx" : c’est la classification de Simpson. La taxonomie actuelle admet à présent que les lynx appartiennent à leur propre genre, mais les synonymes "Felis lynx", "Felis rufus" ou encore "Felis pardinus" subsistent dans la littérature.

Le nombre d'espèces de lynx a beaucoup varié, du fait de la grande fluctuation morphologique, tant au niveau de la taille que de la couleur, des différents individus : jusqu’à sept espèces de lynx ont été proposées par Pocock et Balestri. Dans les années 1980, on comptait uniquement deux espèces : le Lynx boréal et le Lynx roux. Un autre modèle à deux espèces a existé qui admettait uniquement le Lynx boréal et le Lynx pardelle. Au sein de l’ensemble des différents modèles à deux espèces, la seule variation était l’aire de distribution du Lynx boréal qui tour à tour englobait celle du Lynx du Canada ou du Lynx pardelle : les espèces actuelles, surtout le Lynx du Canada et le Lynx pardelle devenaient alors des sous-espèces du Lynx boréal "Lynx lynx canadensis" ou "Felis lynx canadensis" et "Lynx (Felis) lynx pardinus".

Le Caracal, du fait de similitude morphologique (tête, denture, queue) a longtemps été classé dans le genre "Lynx". L’absence totale de taches, puis, plus tard, les analyses génétiques, l’ont écarté du genre "Lynx" vers son propre genre "Caracal". Le Manul ("Otocolobus manul") a lui aussi temporairement fait partie du genre "Lynx".

La phylogénie s'est longtemps basée sur l'étude des fossiles d'un animal afin de préciser l'apparition et l'évolution d'une espèce. La phylogénie moderne s'appuie essentiellement sur les analyses génétiques en raison du nombre peu important de fossiles de félins. Le premier félin daterait d'il y a 11 millions d'années. L’ancêtre commun des lignées "Leopardus", "Lynx", "Puma", "Prionailurus" et "Felis" aurait traversé la Béringie et colonisé l’Amérique du Nord il y a environ 8 à 8,5 millions d’années. Il y a 7,2 millions d’années, la lignée des lynx diverge de celle des pumas. Le dernier ancêtre commun à tous les lynx date d’il y a 3,2 millions d’années au Pliocène.

Bien que les fossiles soient rares chez les félins, les lynx font exception. Le Lynx d'Issoire ("Lynx issodoriensis") est généralement considéré comme l'ancêtre commun du genre "Lynx". Possédant une aire de répartition très large, "Lynx issiodorensis" présentait une morphologie proche des félinés tout en ayant les caractéristiques des lynx : une queue courte et la denture à 28 dents. Plusieurs hypothèses d'« apparitions » des lynx modernes au travers de la forme intermédiaire du Lynx d'Issoire ont été proposées. Une première hypothèse suggère une divergence en trois lignées distinctes : "L. pardinus", "L. lynx", et "L. rufus" ; dans cette première hypothèse, "L. canadensis" descend de "L. lynx".

Le Lynx d’Issoire aurait migré en Amérique du Nord par le détroit de Béring durant la glaciation du Pléistocène il y a cinq à deux millions d’années : des preuves de sa présence il y a 2,5 à 2,4 millions d’années ont été découvertes au Texas. Le Lynx d’Issoire aurait ensuite évolué en une forme intermédiaire "Lynx issiodorensis kurteni" puis vers l'actuel Lynx roux ("Lynx rufus").

Les premières formes de "Lynx pardinus" pourraient dater de fossiles attribués à "Lynx issiodorensis" du Pléistocène moyen selon Argant (1996). Le Lynx des cavernes "Lynx pardinus speleus" ou "Lynx spelaea", dont des traces ont été retrouvées dans les grottes de l’Observatoire à Monaco et de Grimaldi en Italie, possède des caractéristiques intermédiaires entre "Lynx lynx" et "Lynx pardinus". Il est possible que le Lynx d’Issoire ait évolué vers le Lynx des cavernes qui par la suite a évolué vers le Lynx pardelle. Des études menées tant sur la morphologie que sur le squelette du Lynx pardelle ont mis en évidence la sympatrie entre le Lynx pardelle et le Lynx boréal au sud-ouest de l’Europe durant le Pléistocène. Les deux espèces sont à présent considérées comme allopatriques.

Le Lynx d'Eurasie "Lynx lynx" est plus éloigné de "Lynx issiodorensis" que le Lynx pardelle. La dentition de cette espèce est différente de celle des autres lynx et il est également plus grand que les autres espèces de lynx ; une hypothèse proposée est que le Lynx boréal, originaire d'Asie, aurait repoussé le Lynx pardelle sur la péninsule espagnole. Le Lynx du Canada et le Lynx boréal sont en fait issus du même ancêtre commun asiatique. Bien après la première colonisation ayant abouti au Lynx roux, une forme de Lynx d'Issoire a effectué une nouvelle colonisation des Amériques depuis l’Asie qui serait à l’origine du Lynx du Canada moderne ("Lynx canadensis").

La classification classique range le genre "Lynx" dans la sous-famille des "Felinae", qui contient historiquement tous les félins qui ne rugissent pas.

La classification phylogénétique divise les félins en huit lignées distinctes ; les lynx constituent la cinquième lignée. Les quatre espèces auraient évolué dans cet ordre : Lynx roux, Lynx du Canada, Lynx boréal et Lynx pardelle.
Le genre "Lynx" est subdivisé en quatre espèces distinctes : le Lynx du Canada ("Lynx canadensis"), le Lynx boréal ("Lynx lynx"), le Lynx pardelle ("Lynx pardinus") et le Lynx roux ("Lynx rufus"). La validité des sous-espèces est fortement débattue, notamment celle du Lynx roux : il n'existe pas moins de douze sous-espèces divisées selon des critères géographiques et morphologiques (taille et couleur). Selon Mammal Species of the World, le Lynx du Canada n'admet que trois sous-espèces, le Lynx boréal cinq et le Lynx pardelle aucune. Pour le Lynx boréal, il est possible que le Lynx de Sardaigne ("Lynx lynx sardiniae") ne soit en fait qu'une sous-espèce de Chat sauvage ("Felis silvestris").

L’hybridation naturelle entre le Lynx roux et le Lynx du Canada existe : aux États-Unis, on appelle le résultat d’un tel croisement un « "Blynx" » ou un « "Lynxcat" », contraction du terme « Bobcat » désignant le Lynx roux et « Lynx » désignant le Lynx du Canada. En 2004, des études génétiques menées sur ces deux espèces ont confirmé que trois spécimens sauvages du Minnesota à l’origine ambiguë étaient issus de l’hybridation. L’ensemble des hybrides étudiés avait un Lynx du Canada pour mère. Les signalements d’hybrides sauvages sont, pour l’instant, confinés au sud de l’aire de répartition du Lynx du Canada. Ces hybrides naturels partagent les caractéristiques morphologiques des deux espèces dont ils sont issus.

Des hybridations en captivité avec l’Ocelot, le Caracal et le Serval ont été signalées. Une légende, probablement colportée par la créatrice de la race dans les années 1980 aux États-Unis, veut également que le pixie-bob soit une race de chat issue du croisement naturel entre un Lynx roux et un chat domestique. C'est également le cas pour le bobtail américain et le lynx domestique ; bien que des observations d'accouplement entre le chat domestique et un lynx aient été rapportées, aucun test génétique n'a jamais confirmé ce genre d'hybridation.

Les lynx vivent préférentiellement dans les forêts boréales et mixtes à feuillage caduc ; le Lynx roux accepte un plus large panel d'habitats qui vont des aires semi-désertiques aux marécages humides de Floride bien qu'il préfère les forêts, mais contrairement aux autres espèces de lynx, il n’en dépend pas exclusivement. Le Lynx pardelle préfère les forêts de pins et la garrigue.

L'ensemble des espèces de lynx est situé dans l'hémisphère nord. Le Lynx roux et le Lynx du Canada vivent en Amérique du Nord, le Lynx pardelle se trouve exclusivement sur de petites portions de la péninsule ibérique et le Lynx boréal possède la plus large distribution qui s'étend sur toute l'Europe et l'Asie.

L'aire de répartition des lynx s'est réduite plus ou moins fortement selon les espèces, mais c'est en Europe que la réduction a été la plus importante. Le Lynx boréal était présent partout en Europe, sauf en Grande-Bretagne puis il a disparu de l’ouest de l’Europe et des Alpes avant l’ours et le loup, bien qu’il ait été persécuté moins intensivement. Les populations de lynx régressèrent partout en Europe, puis eurent tendance à s’accroître au milieu du , du fait de sa protection légale. Le lynx pardelle, extrêmement menacé, a vu ses populations chuter drastiquement durant la fin du en raison des épidémies de myxomatose qui a décimé sa proie principale, le lapin, et d'importants réseaux routiers qui ont fragmenté son habitat et augmenté le nombre de collisions avec des véhicules : les populations de lynx pardelle ont diminué de 80 % en l'espace de vingt ans. En Amérique, les populations ont moins régressé ; toutefois, en raison de changements d’habitat dus aux pratiques agricoles modernes, le Lynx roux n’est plus présent dans le Midwest des États-Unis et dans le sud du Minnesota, l’est du Dakota du Sud, l’Iowa et une grande partie du Missouri. Le Lynx du Canada est encore présent sur 95 % de son aire de répartition historique au Canada mais a régressé aux États-Unis.

Excepté pour le Lynx pardelle, l'Union internationale pour la conservation de la nature (UICN) considère que les populations de lynx sont stables et abondantes ; par conséquent, elles sont classées en « Préoccupation mineure » (LC). Le Lynx pardelle est en « danger critique d'extinction » (CR).

Le commerce des animaux sauvages est régi par la Convention sur le commerce international des espèces de faune et de flore sauvages menacées d'extinction (CITES). À part le Lynx pardelle qui est classé en Annexe I (toutes formes de commerce interdites) depuis 1990, l'ensemble des espèces de lynx sont en Annexe II de la CITES depuis 1977. Les États-Unis ont lancé une demande de retrait du Lynx roux de l’appendice II à la CITES en raison de l'accroissement des populations, mais celle-ci fut refusée.

La chasse au Lynx boréal est réglementée en Russie, en Norvège, en Finlande, en Pologne, en Roumanie, en Turquie, en Estonie, en Lettonie, en Slovaquie jusqu'en 2001 date à laquelle il fut totalement protégé, en Croatie et en Slovénie. En France et en Suisse, les lynx à problème sont déplacés. Le Lynx pardelle est protégé sur l'ensemble de son aire de répartition. La chasse au Lynx roux et au Lynx du Canada est réglementée au Canada, aux États-Unis et au Mexique, mais la législation peut varier selon les États.

Seul le Lynx boréal fait l'objet d'un studbook européen (ESB) visant à créer un arbre généalogique fiable des individus détenus par les zoos. Selon l'Association mondiale des zoos et des aquariums, les lynx sont gardés dans les parcs zoologiques à des fins d'éducation et pour la sympathie nouvelle du public. Selon l""' (ISIS), 667 lynx sont détenus par des zoos le octobre 2009, les espèces les plus représentées étant le Lynx boréal et le Lynx roux.

Un programme d'élevage du Lynx pardelle a été décidé en urgence en juin 2003. Le parc national de Doñana met en place plusieurs systèmes permettant de fournir aux lynx sauvages de quoi se nourrir sans émousser leur instinct de chasseur : des lapins sont contenus dans des enclos spéciaux, difficiles d'accès et proposant de nombreuses cachettes. En parallèle, le centre de reproduction permet d'accroître rapidement la population : toutes les naissances devraient, à terme, être réintroduites.

Quatorze projets de réintroduction du lynx ont été mis en œuvre en Europe de 1970 à 2006, qui ont donné les meilleurs résultats en Slovénie, dans les Alpes suisses et dans le Jura. Le Lynx boréal a été réintroduit en Slovénie, en Croatie, dans le parc national de Bavière en Allemagne, en Suisse dans le Jura, le canton de Vaud (Alpes et Jura), le canton d'Obwald et le parc national des Grisons et le parc national du Grand-Paradis en Italie. Des tentatives de réintroduction du Lynx du Canada ont été menées dans l’État de New York et dans le Colorado ; pour ce dernier la réintroduction semble être un succès. Le Lynx roux a été réintroduit sur l’île de Cumberland et dans le New Jersey.

Pour capturer des lynx à des fins de réintroduction, les scientifiques utilisent la tendance des félins à emprunter toujours les mêmes passages. Une cage à deux portes coulissantes est placée de telle manière que le félin puisse voir sa piste au-delà du piège, sur un chemin fréquemment utilisé. Le lynx est souvent capturé au début ou à la fin de l’hiver, il subit ensuite une période de quarantaine avant d’être relâché, de préférence en couple, à la belle saison. Les individus capturés sont souvent des jeunes, généralement des mâles.

Le terme « lynx » est directement issu du latin « ' », lui-même tiré du grec ancien « ' » qui désigne tout simplement l'animal. Il existe quelques variations orthographiques telles que « linz » durant le ou « lins » au . Au sens figuré, un lynx est une personne très rusée. « Avoir des yeux de lynx » signifie avoir une très bonne vue ; cette expression est issue d'une confusion avec « avoir des yeux de Lyncée », en référence à l’argonaute Lyncée qui possédait une vision perçante, et a été à l'origine de la légende sur les bons yeux du lynx. Ainsi, la constellation du Lynx aurait été appelée ainsi par Hevelius au car il faut avoir les yeux de lynx pour l'apercevoir. Le terme « Lynx du désert » ou « Lynx désertique » fait référence au Caracal ("Caracal caracal"), qui était autrefois placé dans le genre "Lynx".

Le Lynx boréal est anciennement nommé « loup-cervier » ou « loup cervier » , du latin "" qui signifie littéralement « loup qui attire les cerfs ». Au départ, ce terme ne désignait que la femelle du lynx et le féminin « louve-cervière » est antérieur au masculin. Une forme féminine « loup-cerve » est proposée dans certains dictionnaires. Le Lynx du Canada est encore appelé « loup-cervier » en français du Canada. Outre la désignation de l'animal, le terme loup-cervier peut symboliser un homme sans scrupule, travaillant dans le secteur de l'économie (banquier par exemple).

Dans la mythologie amérindienne, la figure du lynx est souvent associée à celle du Coyote, dans un thème de gémellité. Le lynx et le coyote sont respectivement associés au vent et au brouillard, deux éléments opposés dans le folklore amérindien. Les légendes varient légèrement entre les peuples nord-américains, et des mythes équivalents existent en Amérique du Sud, comme au Brésil par exemple. Les figures du Lynx et du Coyote dans les mythes des Indiens d’Amérique ont été étudiées par Claude Lévi-Strauss, dans son livre "Histoire de Lynx". Selon lui, ces jumeaux opposés et de force inégale représentent un monde en perpétuel déséquilibre. Cette analyse lui permet d’interpréter les comportements amicaux des Amérindiens lors de leurs premiers contacts avec des Européens : pour les Amérindiens, l’existence de leur peuple impliquait l’existence d’autres peuples dont ils attendaient la venue. Toujours selon Lévi-Strauss, les versions plus tardives sont le résultat du contact régulier avec les Européens.

Dans une légende Shawnee, le lynx, un des quatre protecteurs de l’étoile du matin, se fait entourlouper par un lapin : alors que ce dernier est acculé dans un arbre, prêt à être attrapé par le lynx, il suggère à son prédateur de faire un feu pour le rôtir ; le lapin saute alors de l’arbre, et les braises s’éparpillent sur la fourrure du lynx et dessinent des taches marron foncé sur sa robe. Les Mojaves croient que rêver souvent d’un objet ou d’un être vivant leur donne leurs caractéristiques. S’ils rêvent des deux divinités que représentent le lynx et le puma, ils pensent que cela va leur donner des compétences à la chasse supérieures à celles des autres tribus. Les colons européens ont aussi admiré ce félin, et aux États-Unis, il reste prééminent dans les anthologies du folklore national.

Le Lynx boréal est quasiment absent des mythologies européennes ; toutefois, il a fait l’objet de nombreuses superstitions colportées dans les bestiaires. Le lynx apparaît comme un loup aux taches de panthère, dont la femelle ne peut enfanter qu'une seule fois.

Une autre superstition veut que le lynx aient de bons yeux. Cette croyance est née d’une confusion avec l’argonaute Lyncée qui possédait une vision perçante. On pensait également que les yeux brillants du lynx éclairaient la route et pouvaient rendre aveugle tant la lumière était intense. Ses yeux étincelants avaient prétendument la faculté de voir à travers les murs. La légende du loup-cervier raconte que le lynx peut se transformer en loup pour se nourrir de cervelle humaine.

L’urine de lynx avait la propriété de se solidifier pour former une pierre précieuse rouge, le lyncurium, lyncurius ou "". Afin de cacher cette pierre et par jalousie, le lynx recouvre son urine de terre. La pierre fabuleuse est capable de soigner l’ictère et de faire disparaître les calculs de la vessie. Selon Theophrastus (), la pierre attire à elle la paille, les copeaux de bois, le cuivre et le fer ; elle est de meilleure qualité si elle provient d'individus sauvages et masculins. Bien que personne n'ait jamais vu cette pierre fabuleuse, les écrits de Theophrastus seront repris par plusieurs auteurs classiques comme Ovide (), Pline l'Ancien () et Isidore de Séville () jusqu'au où il disparaît progressivement des lapidaires, sans que les croyances de Theophrastus ne soient jamais remises en doute.

La première description du Lynx boréal nous vient de Pline l'Ancien, qui n'hésita pas à le comparer au loup : « "" », c'est-à-dire « Ressemblant au loup, tacheté comme une panthère ». De plus, selon Pline l'ancien, il existe deux formes de lynx, le « loup-cervier » utilisé à Rome lors des jeux du cirque, et le « lynx », créature fabuleuse venue d'Éthiopie. Ces descriptions, pourtant très peu précises, servirent de base à l'ensemble des travaux et écrits sur le lynx. Combiné à l'extrême discrétion de ce félin que personne ou presque ne rencontrait, il devint un animal fantasmagorique, réputé féroce. Ainsi, au Moyen Âge, le loup-cervier est toujours assimilé au loup. On appelait ainsi le lynx « loup à robe zébrée ou mouchetée », et tout le monde était terrifié par cet animal. Le lynx était très méconnu, absent de la Bible. Il apparaît dans le livre de Marco Polo, le "Devisement du monde" et pour la première fois illustré dans le "Livre de chasse" de Gaston Phoebus. Au Moyen Âge, les griffes et les dents du Lynx boréal servaient d’amulettes et il était également chassé pour sa fourrure.

Pendant longtemps, le lynx et le loup-cervier sont considérés comme deux espèces différentes. Bien que les premières superstitions soient écartées, les connaissances sur cet animal sont erronées ; par exemple, au , Pierre Boitard écrit que . L'animal est considéré comme féroce et sanguinaire. Ainsi, la bête de la Gargaille, sorte de bête du Gévaudan jurassienne, aurait terrorisé la population durant l'année 1819. Les descriptions très contradictoires pointent l'action d'un lynx. Cependant, l'histoire aurait été gonflée en véritable massacre par le préfet tandis que le louvetier décrivait de simples vêtements déchirés. Le félin est si méconnu que les véritables dépouilles de lynx capturés en Europe de l'Ouest sont prises pour quelques « animaux exotiques » et cela jusqu'au . Les écrits à propos du lynx restent empreints de légendes jusqu'au où des recherches sérieuses ne sont entreprises qu'à partir des années 1980.

En héraldique, lynx et loup-cervier sont deux figures différentes. Le lynx est passant dans l'écu et tout comme le loup-cervier symboliserait la perspicacité. Le loup-cervier, représenté comme une panthère tachetée avec la queue d'un chat et la face d'un lynx, est très peu présent. Le lynx par défaut est passant, la tête de front, et peut être confondu avec le loup bien qu'il ait le plus souvent la queue entre les jambes.

Le lynx est considéré comme un symbole de la Macédoine et est présent sur le côté pile de la pièce de 5 denars. Le lynx est choisi comme emblème par de nombreuses universités et équipes sportives d’Amérique du Nord, comme les Bobcats de Charlotte, ou les Lynx de Toronto.

Les lynx sont assez peu présents dans les œuvres de fiction. À la télévision, Bonkers D. Bobcat est un Lynx roux anthropomorphique créé par les studios Disney. Le Lynx roux Bubsy est un personnage de jeux vidéo ; une série de dessins animés consacrée au personnage a été produite, mais seul un épisode existe.

Au Moyen Âge, les griffes et les dents du Lynx boréal servaient d’amulettes et il était également chassé pour sa fourrure. La fourrure du Lynx du Canada est recherchée depuis le début de la colonisation du Canada par les Européens. Les trappeurs de la côte nord du Canada et les peuples autochtones mangent sa chair.

La peau de Lynx roux est la plus vendue parmi celles des félins. La fourrure de Lynx roux sert à faire des manteaux, des tapis ou des décorations murales ; c’est la fourrure du ventre qui est la plus recherchée. La plupart des exportations viennent des États-Unis, dont les exportations annuelles moyennes sont passées de plus de dans les années 1990 à un peu moins de dans les années 2000.

Le changement de mentalité de l'humain envers la nature et plus particulièrement envers les carnivores a été profitable au lynx. 70 à 80 % des habitants des pays d'Europe de l'Ouest sont favorables au retour des lynx, les citadins étant bien plus favorables au retour du lynx que les habitants des milieux ruraux. Les principaux détracteurs des lynx sont les chasseurs, qui l'accusent de faire diminuer la population de gibier, et les éleveurs, préoccupés par les prélèvements sur leurs troupeaux. Pourtant, l'impact du lynx est considéré comme bénéfique au gibier. Par exemple en Suisse, la population de lynx, qui compte environ , tue par an contre par la chasse et par la circulation automobile sur un effectif d'environ 130'000 chevreuils. De nombreux moyens ont été testés pour minimiser l'impact du lynx sur le bétail : les plus efficaces restent l'emploi du chien patou, le gardiennage et l'utilisation de clôtures. De plus, si la présence des lynx est parfois mal vécue lors de leur réintroduction, on constate que dans les pays où les lynx n'ont jamais disparu aucune accusation ni demande d'extermination n'est effectuée.

Selon une étude menée au "" où le Lynx roux a été réintroduit, l’évaluation des connaissances a une note moyenne de 3,8/10, les chasseurs ayant obtenu les meilleurs scores (5,1/10). Selon les auteurs, ce score si faible peut être corrélé avec la nature discrète du Lynx roux : les opportunités d’apprentissage par contact direct sont faibles. De plus, peu de reportages animaliers lui sont dédiés, à l’inverse de ce qu’on peut voir pour le lion, le tigre, ou encore le puma.








</doc>
<doc id="14928" url="https://fr.wikipedia.org/wiki?curid=14928" title="Lynx (navigateur)">
Lynx (navigateur)

Lynx est un navigateur web en mode texte, et utilisant donc le clavier comme interface principale. Les protocoles supportés sont : Gopher, HTTP, HTTPS, FTP, WAIS, et NNTP.

La navigation dans Lynx consiste à sélectionner le lien choisi à l'aide des touches fléchées, ou bien à faire afficher à Lynx un numéro correspondant à chaque lien et à taper le numéro du lien choisi. Les versions actuelles prennent en charge SSL et un grand nombre de caractéristiques du HTML, mais pas le JavaScript. Les cadres sont identifiés par leur nom et peuvent être explorés comme s'il s'agissait de pages distinctes.

Lynx est un produit de l'Academic Computing Services de l'Université du Kansas et a été développé à l'origine en 1992 par Lou Montulli, Michael Grobe et Charles Rezac. Garret Blythe a créé DosLynk avant de travailler lui aussi sur Lynx. Foteos Macrides a porté une grande partie de Lynx sur VMS et l'a maintenu pendant un temps. Depuis 1995, Lynx est distribué sous la licence GNU GPL. Il est maintenant maintenu par un groupe de bénévoles.

Lynx a été conçu pour les systèmes Unix et VMS, et tourne en particulier sur GNU/Linux. Des versions pour MS-DOS et pour Windows sont aussi disponibles. Il existe également une version appelée MacLynx pour Macintosh « Système 7 ou supérieur » qui n'est pas mise à jour régulièrement.

Grâce à son interface facile à intégrer à un synthétiseur vocal, Lynx était très populaire auprès des déficients visuels, mais l'amélioration des synthèses vocales a réduit son intérêt pour les non-voyants.

Lynx est toujours utilisé comme outil de développement web dans la mesure où il permet de consulter un document de la même manière que le ferait un robot d'indexation.

Ses principaux homologues sont Links et w3m.





</doc>
<doc id="14929" url="https://fr.wikipedia.org/wiki?curid=14929" title="Atari Lynx">
Atari Lynx

L'Atari Lynx fut la seule console portable d'Atari et la première portable avec un écran LCD couleur. Elle sortit en 1989, la même année que le Game Boy (monochrome) de Nintendo.

La machine fut développée par Epyx sous le nom « Handy » (c'est aujourd'hui le nom de l'émulateur le plus avancé sur PC de cette console) et terminée en 1987, date à laquelle Atari racheta les droits. La compagnie modifia le haut parleur interne et supprima le stick qui se trouvait alors sur le pad. Atari commercialisa la console deux ans plus tard au prix initial de américains. Deux des créateurs de la console, Dave Needle et R.J. Mical ont aussi fait partie de l'équipe Amiga (et plus tard, de l'équipe 3DO) : c'est cette machine qui fut essentiellement utilisée pour le développement des jeux Lynx. On pouvait inverser l'écran pour jouer en tant que gaucher et jouer jusqu'à 8 en réseau. Les capacités techniques étaient largement plus performantes que celles de la Game Boy (couleur et 3D obligent). Cette dernière s'imposa tout de même sur le marché grâce à une meilleure autonomie et une gamme de jeux très variée. La Lynx, elle, était plus imposante, trop gourmande en piles et manquait de titres porteurs.

En 1991, Atari sortit une seconde version de sa console sous une nouvelle forme, avec des cartouches relookées. La nouvelle console (nommée par Atari « Lynx II ») possédait des grips de prise en main, un écran de meilleure qualité avec une option d'économie d'énergie qui permettait de mettre la console en mode veille.

Bien que cette console fût technologiquement supérieure à la Game Boy, les erreurs marketing d'Atari et la faible quantité de jeux disponibles firent que la console fut un échec commercial. Finalement, au milieu des années 1990, Atari abandonne la console… pour un temps. Atari tente une dernière fois de relancer la 8 bits portable en parallèle du lancement de l'Atari Jaguar en 1995. Quelques jeux commerciaux sont lancés, mais très vite Atari abandonne, cette fois définitivement.

Depuis, la console n'est pas tombée dans l'oubli puisque régulièrement, de nouvelles productions développées par des fans sortent sur cette plate-forme pour le plus grand bonheur des joueurs et des collectionneurs.

La Lynx possédait de nombreuses caractéristiques innovantes en plus de son écran couleur :






</doc>
<doc id="14933" url="https://fr.wikipedia.org/wiki?curid=14933" title="Henry Hazlitt">
Henry Hazlitt

Henry Hazlitt ( - ) est un philosophe, essayiste, économiste et journaliste libertarien américain.

Journaliste au "Wall Street Journal", à "Newsweek" et au "New York Times", il s'est fait connaître grâce à son livre "L’Économie Politique en une Leçon", un ouvrage de vulgarisation sur les principes de l'économie de marché, basé sur "Ce qu'on voit et ce qu'on ne voit pas" de Frédéric Bastiat. Auteur prolifique, il est aussi l'auteur d'une œuvre majeure sur l'éthique, "The Foundations of Morality".

Dans son ouvrage de 1959, "The Failure of the New Economics: An Analysis of the Keynesian Fallacies", il établit une critique méthodique et systématique de la "Théorie générale de l'emploi, de l'intérêt et de la monnaie" de John Maynard Keynes. Il en dira même qu'il « n'a pas pu y trouver une seule doctrine qui soit vraie et originale. Ce qui est original dans son livre est faux et ce qui est juste n'est pas nouveau ».

Très proche de la romancière et philosophe Ayn Rand dans les années 1940-1950, sa pensée fut influencée par cette dernière.






</doc>
<doc id="14937" url="https://fr.wikipedia.org/wiki?curid=14937" title="Probabilité">
Probabilité

Le terme probabilité possède plusieurs sens : venu historiquement du latin "probabilitas", il désigne l'opposé du concept de certitude ; il est également une évaluation du caractère probable d'un événement, c'est-à-dire qu'une valeur permet de représenter son degré de certitude ; récemment, la probabilité est devenue une science mathématique et est appelée théorie des probabilités ou plus simplement "probabilités"; enfin une doctrine porte également le nom de probabilisme.

La probabilité d'un événement est un nombre réel compris entre 0 et 1. Plus ce nombre est grand, plus le "risque", ou la "chance", que l'événement se produise est grand. L'étude scientifique des probabilités est relativement récente dans l'histoire des mathématiques. L'étude des probabilités a connu de nombreux développements depuis le grâce à l'étude de l'aspect aléatoire et en partie imprévisible de certains phénomènes, en particulier les jeux de hasard. Ceux-ci ont conduit les mathématiciens à développer une théorie qui a ensuite eu des implications dans des domaines aussi variés que la météorologie, la finance ou la chimie.

À l'origine, dans les traductions d'Aristote, le mot « probabilité » ne désigne pas une quantification du caractère aléatoire d'un fait, mais la perception qu'une idée est communément admise par tous. Ce n'est qu'au cours du Moyen Âge, puis de la Renaissance, autour des commentaires successifs et des imprécisions de traduction de l'œuvre d'Aristote, que ce terme connaîtra un glissement sémantique pour finir par désigner la vraisemblance d'une idée. 

L'apparition de la notion de « risque », préalable à l'étude des probabilités, n'est apparue qu'au , pour l'évaluation de contrats commerciaux avec le Traité des contrats de Pierre de Jean Olivi, et s'est développée au , avec la généralisation des contrats d'assurance maritime. À part quelques considérations élémentaires par Girolamo Cardano au début du , et par Galilée au début du , le véritable début de la théorie des probabilités date de la correspondance entre Pierre de Fermat et Blaise Pascal, en 1654.

C'est dans la deuxième moitié du , à la suite des travaux de Blaise Pascal, Pierre de Fermat et Christian Huygens sur le problème des partis, que le terme « probabilité » prend peu à peu son sens actuel, avec les développements du traitement mathématique du sujet par Jakob Bernoulli. 

Au , Gabriel Cramer donne un cours sur la "logique probabiliste" qui deviendra une base à l'article "probabilité" de l'encyclopédie de Diderot, écrite à la fin de ce même siècle. Ce n'est alors qu'au qu'apparaît ce qui peut être considéré comme la théorie moderne des probabilités en mathématiques. 

Le calcul des probabilités prend un nouvel essor au début du , avec l'axiomatique de Kolmogorov; commence alors la théorie des probabilités. Les probabilités deviennent une science et une théorie, comme branche des mathématiques.

Ainsi, il existe plusieurs notions que nous détaillerons dans les sections suivantes : 

Le premier usage du mot "probabilité" apparaît en 1370 avec la traduction de "l'éthique à Nicomaque" d'Aristote par Oresme, et désigne alors « le caractère de ce qui est probable ». Le concept de probable chez Aristote (ενδοξον, en grec) est ainsi défini dans les "Topiques" :

Ce qui rend une opinion probable chez Aristote est son caractère généralement admis; ce n'est qu'avec la traduction de Cicéron des "Topiques" d'Aristote, qui traduit par "probabilis" ou par "verisimilis", que la notion de vraisemblance est associée à celle de « probabilité », ce qui aura un impact au cours du Moyen Âge puis de la Renaissance, avec les commentaires successifs de l'œuvre d'Aristote.

Une phrase, situation ou proposition est vraie ou fausse. Sa probabilité est la . La notion d'incertitude est quant à elle le défaut de cette connaissance. Pour une proposition, il existe alors trois cas :

Cette représentation développée par Cramer permet de faire apparaître une manière de mesurer la notion d'incertitude ou de probabilité. Il donne alors la définition suivante de la probabilité :

Comme précisé précédemment, la notion de probabilité permet de quantifier le hasard. La formalisation du début du est aujourd'hui unanimement utilisée. (par exemple, voir l'ouvrage de Jacod et Protter pour cette section)

La probabilité d'un certain événement , notée formula_1, associe une valeur entre 0 et 1 que l'événement se réalise. Lorsque formula_2, l'événement est dit presque sûr (ou quasi certain), c'est-à-dire qu'il a « toutes les chances » de se réaliser. À l'inverse si formula_3, est dit négligeable (ou quasi impossible), c'est-à-dire qu'il a une chance nulle de se réaliser.

La probabilité d'un événement peut s'obtenir de manière fréquentiste, notamment lorsqu'il est possible de faire une expérience plusieurs fois et de compter le nombre de succès de l'expérience. En effet, si on effectue fois une expérience et que dans formula_4 fois des cas, l'événement est réalisé, alors, la probabilité de est donnée par : 
De manière plus probabiliste, lorsque le nombre de résultats possibles de l'expérience est fini et que ces résultats sont équiprobables, la probabilité de est obtenue par :

Mathématiquement, l'événement est un sous-ensemble d'un ensemble formula_7 qui représente toutes les éventualités possibles. Pour obtenir une théorie, des axiomes ont été proposés par Kolmogorov : la probabilité formula_8 doit vérifier :

Grâce à cette description, plusieurs notions peuvent s'écrire de manière mathématique.
Deux événements sont dits "indépendants" si le fait de connaître la probabilité du premier événement ne nous aide pas pour prévoir la probabilité du second et inversement. Mathématiquement, cela s'écrit : formula_14. Par exemple, la probabilité d'obtenir un as à un premier jeté de dé et d'obtenir un as au deuxième jeté de dé est la multiplication des deux probabilités et vaut 1/36.
Il est possible de considérer la probabilité d'un événement (notons le ) "conditionnellement" à un autre (noté ). Lorsque les deux événements ne sont pas indépendants, le fait de connaître la probabilité de l'un influence la probabilité de l'autre par la formule : formula_15. Par exemple, la probabilité d'obtenir la somme des deux dés égale à 12 lorsque le premier dé a donné 6 vaut 1/6.
Des formules existent pour pouvoir calculer tout type de probabilité. C'est le cas de la "formule de Poincaré", de la "formule des probabilités totales" ou du "théorème de Bayes".

Encouragé par Pascal, Christian Huygens publie "De ratiociniis in ludo aleae" (raisonnements sur les jeux de dés) en 1657. Ce livre est le premier ouvrage important sur les probabilités. Il y définit la notion d'espérance et y développe plusieurs problèmes de partages de gains lors de jeux ou de tirages dans des urnes. Deux ouvrages fondateurs sont également à noter : "Ars Conjectandi" de Jacques Bernoulli (posthume, 1713) qui définit la notion de variable aléatoire et donne la première version de la loi des grands nombres, et "Théorie de la probabilité" d' Abraham de Moivre (1718) qui généralise l'usage de la combinatoire.

La théorie de la probabilité classique ne prend réellement son essor qu'avec les notions de mesure et d'ensembles mesurables qu'Émile Borel introduit en 1897. Cette notion de mesure est complétée par Henri Léon Lebesgue et sa théorie de l'intégration. La première version moderne du théorème central limite est donnée par Alexandre Liapounov en 1901 et la première preuve du théorème moderne est donnée par Paul Lévy en 1910. En 1902, Andrei Markov introduit les chaînes de Markov pour entreprendre une généralisation de la loi des grands nombres pour une suite d'expériences dépendant les unes des autres. Ces chaînes de Markov connaîtront de nombreuses applications, entre autres pour modéliser la diffusion ou pour l'indexation de sites internet sur Google.

Il faudra attendre 1933 pour que la théorie des probabilités sorte d'un ensemble de méthodes et d'exemples divers et devienne une véritable théorie, axiomatisée par Kolmogorov.

Kiyoshi Itô met en place une théorie et un lemme qui porte son nom dans les années 1940. Ceux-ci permettent de relier le calcul stochastique et les équations aux dérivées partielles, faisant ainsi le lien entre analyse et probabilités. Le mathématicien Wolfgang Doeblin avait de son côté ébauché une théorie similaire avant de se suicider à la défaite de son bataillon en juin 1940. Ses travaux furent envoyés à l'Académie des sciences dans un pli cacheté qui ne fut ouvert qu'en 2000.

Au début du , Kolmogorov définit des axiomes mathématiques afin de pouvoir étudier le hasard. Ainsi il construit l'espace des possibles, appelé univers, qui contient tous les hasards possibles, il le munit d'un ensemble qui contient les sous-ensembles de l'univers, appelés tribu, et d'une mesure de probabilité qui permet de calculer les probabilités correspondantes. L'espace formula_16 ainsi construit vérifie les trois axiomes des probabilités :

Afin de pouvoir mieux manipuler le hasard, il est commode d'utiliser une variable aléatoire. Elle peut être réelle, mais peut aussi être multidimensionnelle, ou même plus générale. Cette variable réelle est, en théorie, une application : formula_24 qui à chaque aléa formula_25, associe le résultat de l'expérience : formula_26.
Cette variable possède une répartition de ses valeurs donnée par sa loi de probabilité, qui est une mesure. Cette dernière peut être représentée de nombreuses manières, les plus communes étant par l'utilisation de la fonction de répartition, la densité de probabilité (si elle existe) ou la fonction de masse, le cas échéant. De nombreuses propriétés des lois de probabilité, et donc des variables aléatoires, peuvent être étudiées : espérance, moments, indépendance entre plusieurs variables, etc.

Il est possible de considérer une infinité de variables aléatoires : formula_27. Dans ce cas, y a-t-il une limite possible? La question de notion de convergence aléatoire se pose alors. Il existe plusieurs types de convergences : la "convergence en loi" qui est la convergence de la loi de la variable (en tant que mesure), la "convergence en probabilité", la "convergence presque sûre" ou encore la "convergence en moyenne".
De nombreux théorèmes limites existent alors. Les plus connus sont : la loi des grands nombres qui annonce que la moyenne des premières variables aléatoires converge vers la moyenne théorique de la loi commune des variables aléatoires ; le théorème central limite, qui donne la bonne renormalisation de la somme des variables aléatoires pour avoir une limite non triviale.

Le calcul stochastique est l'étude des phénomènes qui évoluent au cours du temps de manière aléatoire. Le temps peut être modélisé de manière discrète, c'est-à-dire par les valeurs entières : formula_28, dans ce cas le phénomène est représenté par une suite (infinie) de variables aléatoires : formula_29, c'est une marche aléatoire. Le temps peut également être modélisé de manière continue, c'est-à-dire par des valeurs réelles formula_30 ou formula_31, il s'agit alors d'un processus stochastique formula_32.

Plusieurs propriétés sont alors liées au calcul stochastique : la propriété de Markov annonce que le mouvement futur du phénomène ne dépend que de l'état présent et non pas du mouvement passé ; la récurrence et la transience d'une chaîne de Markov assurent le retour ou le passage unique en un état donné ; une martingale est un processus tel que l'état futur est déterminé en moyenne par l'état présent, etc.

La doctrine de la probabilité, autrement appelée probabilisme, est une théologie morale catholique qui s'est développée au cours du , sous l'influence, entre autres, de Bartolomé de Medina et des jésuites. Avec l'apparition de la doctrine de la probabilité, ce terme connaîtra un glissement sémantique pour finir par désigner, au milieu du , le caractère vraisemblable d'une idée.

La probabilité d'une opinion désigne alors, au milieu du , la probabilité qu'une opinion soit vraie. Ce n'est qu'à partir de la fin du , avec l'émergence de la probabilité mathématique, que la notion de probabilité ne concernera plus seulement les opinions et les idées, mais aussi les faits, et se rapprochera de la notion de hasard que l'on connaît aujourd'hui.

Lors de l'étude d'un phénomène aléatoire, il existe plusieurs façons d'aborder la notion de probabilité liée à ce phénomène.


Une notion philosophique apparaît alors : puisque nous ne connaissons la nature et le monde autour de nous que par notre expérience et notre point de vue, nous ne le connaissons que de manière subjective et ne pouvons estimer précisément les lois objectives qui les dirigent.

Les jeux de hasard sont l'application la plus naturelle des probabilités mais de nombreux autres domaines s'appuient ou se servent des probabilités. Citons entre autres :

Il existe plusieurs façons d'aborder les probabilités : le calcul "a priori" et le calcul "a posteriori". (voir la section "interprétation des probabilités" ci-dessus). Le calcul des probabilités "a posteriori" correspond à une attribution des valeurs des probabilités inconnues par une manière statistique. 

Pour estimer les probabilités, les estimateurs statistiques sont utilisés afin de mieux approcher la variable recherchée. Un estimateur est une valeur calculée à partir d'un échantillon de la population totale étudiée. Un estimateur est bien choisi, c'est-à-dire qu'il donnera une bonne estimation des valeurs recherchées, si c'est un estimateur sans biais et convergent ; autrement dit la moyenne empirique approche la moyenne théorique et l'estimateur converge vers la bonne variable aléatoire lorsque la taille de l'échantillon augmente. La méthode du maximum de vraisemblance permet de choisir un bon estimateur.

Par ces méthodes, il est possible de retrouver les paramètres inconnus d'une loi de probabilité associée au phénomène étudié.

La révision bayésienne est une autre méthode pour le calcul des probabilités a priori. Celle-ci se fait grâce au théorème de Bayes :
Dans cette formule, l"'hypothèse" représente ce que l'on suppose a priori sur le phénomène aléatoire, la "preuve" est une partie du phénomène que l'on connaît et que l'on peut mesurer. Ainsi formula_34 permet de mesurer la vraisemblance de l'hypothèse que l'on fixe.

La fréquence empirique permet d'estimer les probabilités. Dans un échantillon de individus, il suffit de compter le nombre de fois où l'individu appartient à la catégorie recherchée. En notant formula_4 ce nombre parmi les tirages, la fréquence formula_36 est proche de la probabilité formula_37 recherchée. Lors de 400 lancers de pièces, s'il apparaît 198 fois le côté "face", alors on en déduit que la probabilité d'obtenir "face" est approximativement formula_38. C'est un cas particulier de la loi des grands nombres. 0,495 est la valeur estimée de formula_39.

Une liste de valeurs formula_40 est connue, elle est supposée de loi normale dont la moyenne est connue. La question est de trouver l'écart type formula_41 de la loi normale. La statistique T définie par formula_42 est un estimateur de formula_41, c'est-à-dire qu'il tend vers formula_41 lorsque tend vers l'infini.

On se demande quel temps il fera demain, la météo permet d'obtenir des informations supplémentaires. Certaines données sont alors connues : la probabilité que la météo annonce un beau temps sachant qu'il fera effectivement beau : formula_45, la probabilité que la météo annonce un beau temps sachant qu'il pleuvra : formula_46.

Une hypothèse est choisie : par exemple formula_47, c'est-à-dire que l'on considère, "a priori", qu'il y a une chance sur deux qu'il fera beau demain.

Il est alors possible de calculer la probabilité que la météo annonce un beau temps : formula_48, c'est-à-dire que la météo annonce un beau temps dans 55 % des cas. La probabilité qu'il fera beau demain sachant que la météo a annoncé beau temps est alors donnée par :

Il est alors possible de réviser une deuxième fois l'hypothèse qu'il fera beau en regardant un deuxième bulletin météo d'une source différente. On prendrait alors comme nouvelle hypothèse la probabilité d'avoir un beau temps nouvellement calculée.




</doc>
<doc id="14939" url="https://fr.wikipedia.org/wiki?curid=14939" title="Jean Calvin">
Jean Calvin

Jean Calvin (forme re-francisée de la forme latinisée, "Calvinus", du nom Jehan Cauvin), né le 10 juillet 1509 à Noyon (Picardie) et mort le 27 mai 1564 à Genève, est un théologien, un important réformateur, et un pasteur emblématique de la Réforme protestante du , notamment pour son apport à la doctrine dite du calvinisme.

Après des études de droit, Calvin rompt avec l'Église catholique romaine vers 1530. Du fait des persécutions contre ceux qu'on appellera plus tard les "protestants" en France, Calvin se réfugie à Bâle, en Suisse, où il publie la première édition de son œuvre maîtresse, l"Institution de la religion chrétienne" en 1536. La même année, il est recruté par Guillaume Farel pour aider à la réforme de l'église à Genève. À la suite d'un différend entre les pasteurs et le Conseil municipal, Calvin et Farel sont expulsés de Genève. À l'invitation de Martin Bucer, Calvin se rend à Strasbourg, où il séjourne entre 1538 et 1541, devenant pasteur d'une église de réfugiés français et wallons. De Strasbourg, il continue à soutenir le mouvement réformateur à Genève. En particulier lorsque les catholiques tentent d'y reprendre pied grâce à l’évêque Sadolet, Calvin rédige une réponse définitive. Il est finalement invité à revenir dans la cité genevoise en 1541.

Après son retour, Calvin introduit une nouvelle liturgie et des idées politiques novatrices malgré l'opposition de plusieurs puissantes familles de la ville qui tentent de s'opposer à son autorité, notamment lors du procès de Michel Servet. De nouvelles élections et l'arrivée de réfugiés favorables à Calvin lui permettent d'évincer ses opposants au Conseil municipal. Calvin passe les dernières années de sa vie à promouvoir la Réforme à Genève et dans toute l'Europe.

Calvin est un écrivain apologétique et un polémiste provoquant de nombreuses controverses. Il échange également une riche correspondance avec de nombreux réformés, comme Philippe Melanchthon et Heinrich Bullinger. Outre l’"Institution", il rédige des essais sur la plupart des livres de la Bible, de même que des traités de théologie et des confessions de foi. Il prêche régulièrement à Genève et écrit pour soutenir les martyrs protestants qui attendent leur exécution. Calvin est influencé par la tradition augustinienne qui le pousse à disserter sur les concepts de prédestination et de la souveraineté absolue de Dieu en ce qui concerne la rédemption et donc aussi la damnation. Les écrits et les prédications de Calvin fondent la théologie réformée. Les églises réformées et presbytériennes ont depuis lors adopté la pensée calvinienne et l'ont largement répandue.

Calvin, né Jehan Cauvin, est né le 10 juillet 1509 à Noyon en Picardie, province du Royaume de France. Il est l'aîné de quatre fils parvenus à l'âge adulte. Le père, Gérard Cauvin, exerçait la fonction de notaire de la cathédrale et de responsable du tribunal ecclésiastique. La mère, Jeanne le Franc, était fille d'un aubergiste de Cambrai. Gérard destinait ses fils à la prêtrise.

Jean Calvin se révèle précoce. À l'âge de douze ans, il est employé comme greffier par l'évêque et adopte la tonsure, devenant le 10 mai 1521 chapelain de l'autel Notre-Dame-de-la-Gésine de la cathédrale de Noyon. Il bénéficie également de la protection d'une famille influente, les Montmors. Grâce à leur aide, Calvin entre au collège de la Marche à Paris où il perfectionne son latin avec Mathurin Cordier. Puis il y intègre le collège de Montaigu en tant qu'élève en philosophie, ayant pour condisciple Ignace de Loyola.

En 1525 ou 1526, le père, Gérard, retire son fils du collège de Montaigu et l'inscrit à l'université d'Orléans afin qu'il y étudie le droit. Selon Théodore de Bèze et Nicolas Colladon, ses biographes contemporains, Gérard aurait estimé que son fils gagnerait mieux sa vie comme avocat que comme prêtre. Après quelques années d'études, Calvin entre à l'université de Bourges en 1529 pour y suivre les enseignements de l'avocat humaniste André Alciat et apprend le grec, indispensable à l'étude du Nouveau Testament.

À l'automne 1533, Calvin adopte les nouvelles idées de la Réforme protestante. Il rapporte cette conversion à deux reprises, de façon différente. Dans son premier récit, qui figure dans ses "Commentaires sur le livre des Psaumes" il décrit sa conversion comme un changement soudain, provoqué par Dieu :

Dans un second rapport, il évoque un long et difficile processus intérieur, accompagné par une anxiété spirituelle et psychologique :
Il est admis que cette conversion correspond à une rupture avec l'Église catholique romaine. Le biographe de Calvin, Bruce Gordon, estime que .

En 1532, Calvin obtient sa licence en droit et publie son premier livre, un commentaire de l'ouvrage "De Clementia" de Sénèque. Après des visites à Orléans et dans sa ville natale de Noyon, Calvin retourne à Paris en octobre 1533. Les tensions étaient alors fortes au Collège Royal (futur collège de France) entre les humanistes réformés et la direction conservatrice de la faculté. L'un des réformés, Nicolas Cop, est élu recteur de l'université. Le novembre 1533, il consacre son discours d'investiture à la nécessité d'une réforme religieuse et appelle à un renouveau au sein de l'Église catholique.

Ce discours provoque un grand émoi et la faculté dénonce Nicolas Cop comme hérétique, obligeant celui-ci à prendre la fuite et à se réfugier à Bâle, en Suisse. Calvin, proche ami de Cop, est impliqué lui aussi dans le scandale et doit se cacher durant un an. Il trouve refuge chez son ami Louis du Tillet à Angoulême. Puis on le retrouve à Noyon (où, le 4 mai 1534, il résilie les bénéfices ecclésiastiques qu'il perçoit depuis sa tonsure, cet acte suggérant qu'il est alors converti) ou encore à Orléans. Il est cependant obligé de quitter la France après l'affaire des Placards au mois d'octobre 1534, déclenchée par affiches posées dans diverses villes. Ces attaques contre la messe catholique entraînent une violente réaction politique à l'encontre des protestants. En janvier 1535, Calvin rejoint Cop à Bâle, ville ouverte aux idées de l'influent réformateur Œcolampade.

En mars 1536, Calvin publie la première édition de son "Institutio Christianae Religionis" ou "Institution de la religion chrétienne". L'ouvrage est une "apologie", soit défense de la foi, et un exposé de la position doctrinale des réformés. Il cherche également à offrir une instruction de base pour toute personne intéressée par la religion chrétienne. L'ouvrage est la première expression de la théologie de Calvin. Par la suite, ce dernier amende son écrit et en propose plusieurs nouvelles éditions. Peu après la première publication de l'ouvrage, il quitte Bâle pour Ferrare en Italie, où il devient brièvement secrétaire de Renée de France. Il retourne à Paris en juin avec son frère Antoine pour régler les affaires de leur père. À la suite de l'édit de Coucy, qui donne six mois aux hérétiques pour se réconcilier avec la foi catholique, Calvin quitte définitivement la France. En août, il part pour Strasbourg, une ville libre du Saint-Empire romain germanique, qui devient donc une ville-refuge pour les protestants. Mais les affrontements entre troupes françaises et impériales l'obligent à se détourner de son chemin et il arrive à Genève.

Calvin n'envisage pas de rester à Genève, mais Guillaume Farel, un réformé français qui y réside, lui demande avec insistance de l'aider dans son travail de réforme. Calvin se souvient de cette rencontre particulièrement intense, telle que la narre William Ramsay en 2006 :

Calvin accepte sa tâche sans conditions préalables. Ses premières fonctions sont mal connues : il reçoit finalement le titre de « lecteur », signifiant probablement qu'il peut procéder à des lectures explicatives de la Bible. En 1537, il est choisi pour devenir « pasteur ». Pour la première fois de sa vie, l'avocat-théologien assume des fonctions pastorales comme les baptêmes, les mariages et les services religieux.
Tout au long de l'automne 1536, Farel rédige une confession de foi tandis que Calvin écrit des articles séparés sur la réorganisation de l'église à Genève. Le 16 janvier 1537, Farel et Calvin présentent leurs "Articles concernant l'organisation de l'église et du culte à Genève" devant le Conseil municipal. Le document décrit la manière et la fréquence des célébrations de l'eucharistie, la raison et la méthode de l'excommunication, l'importance de souscrire à la confession de foi, la pratique du chant dans la liturgie et la révision des lois sur le mariage. Le Conseil adopte le document dans la même journée. Calvin rédige aussi un catéchisme, largement basé sur le "Grand Catéchisme" de Martin Luther

Auprès du Conseil, l'influence des deux hommes diminue cependant durant l'année, cette autorité étant réticente à faire appliquer les dispositions des articles de cette cette confession de foi, à laquelle peu de citoyens avaient encore souscrit. Le 26 novembre, Calvin et Farel débattent avec passion devant le Conseil à ce sujet. En outre, la France cherche alors à former une alliance avec Genève et, comme les deux pasteurs sont Français, les membres du Conseil se mettent à douter de leur loyauté. Enfin une importante querelle politico-religieuse éclate lorsque Berne, l'alliée de Genève dans la réforme des églises suisses, propose d'uniformiser les cérémonies religieuses. Sa proposition impose l'emploi de pain azyme dans l'eucharistie. Calvin et Farel refusent de suivre cette recommandation et retardent l'emploi d'un tel pain jusqu'à ce qu'un synode soit organisé à Zurich pour trancher la question. Le Conseil ordonne cependant aux deux hommes d'utiliser du pain azyme pour le culte de Pâques. En protestation, ils refusent de présider la cène, provoquant une émeute durant le service. Le lendemain, le Conseil expulse les deux pasteurs.

Farel et Calvin se rendent à Berne et Zurich pour défendre leur cause. Le synode de Zurich attribue une grande part de responsabilité de ce conflit à Calvin, qui n'aurait pas été suffisamment conciliant avec les habitants de Genève. Le synode demande cependant à Berne de plaider en faveur de la réintégration des pasteurs. Le Conseil de Genève refuse néanmoins d'accueillir à nouveau les deux hommes, qui trouvent refuge à Bâle. Par la suite, Farel est invité à diriger l'église de Neuchâtel, tandis que les réformateurs les plus influents de Strasbourg, Martin Bucer et Wolfgang Capiton, sollicitent Calvin pour qu'il se charge d'une communauté de réfugiés français dans cette ville d'Alsace. Calvin commence par refuser, Farel n'étant pas invité également, mais finit par accepter. En septembre 1538, Calvin prend ses fonctions à Strasbourg et, quelques mois plus tard, obtient la citoyenneté de la ville.

Durant son séjour à Strasbourg, Calvin ne reste pas attaché à une église particulière mais dirige successivement l'église Saint-Nicolas, l'église Sainte-Madeleine et l'ancienne église dominicaine renommée Temple Neuf (ces églises existent toujours, mais toutes ont été transformées). Calvin accueille généralement entre 400 et 500 personnes au culte. Il enseigne chaque jour, et prêche deux sermons le dimanche. La communion est célébrée chaque mois et le chant des psaumes est encouragé. Il travaille également à la seconde édition de ses "Institutions", étant notamment mécontent de la structure en forme de catéchisme de la première version.

Pour la seconde édition, publiée en 1539, Calvin abandonne cette forme en faveur d'une présentation systématique des principales doctrines bibliques. Le livre passe ainsi de six à dix-sept chapitres. Il rédige parallèlement un autre livre, les "Commentaires de l'épître aux Romains", qui est publié en mars 1540. L'ouvrage sert de modèle pour ses futurs commentaires : il y inclut sa propre traduction latine du grec, plutôt que de reprendre la Vulgate, une exégèse et une . Dans son introduction, Calvin loue le travail de ses prédécesseurs Philipp Melanchthon, Heinrich Bullinger et Martin Bucer mais s'en démarque et critique certaines de leurs positions.

Durant son séjour à Strasbourg, Calvin souscrit également à la Concorde de Wittenberg et est chargé de défendre la Confession d'Augsbourg lors du colloque de Ratisbonne en 1540.

Les amis de Calvin le pressant de se marier, ce dernier écrit à l'un de ses correspondants :

Plusieurs jeunes femmes lui sont cependant présentées, dont l'une issue d'une famille noble. Calvin accepterait à contre-cœur ce mariage, à condition que la fiancée apprenne le français. Toutefois, la cérémonie, prévue pour mars 1540, n'a jamais été eu lieu. Il écrit plus tard qu'il n'a d'ailleurs jamais pensé à épouser cette jeune fille, . Finalement, il épouse en août 1540 Idelette de Bure, veuve d'un anabaptiste converti par lui, ayant deux enfants de son premier mariage. Le couple a un fils, Jacques, mort jeune.

Genève, à la longue, regrette l'expulsion de Calvin, car le climat politique a changé et l'on constate que la fréquentation des cultes diminue. L'alliance entre Berne et Genève vacille en raison de querelles territoriales. Lorsque le cardinal Jacopo Sadoleto écrit au Conseil municipal, invitant Genève à rentrer dans le giron catholique, le Conseil cherche une autorité ecclésiastique pour lui répondre. Pierre Viret est consulté, mais refuse; le Conseil s'adresse alors à Calvin. Sa "Responsio ad Sadoletum" (Réponse à Sadoleto) défend fermement la réforme protestante à Genève. Le 21 septembre, le Conseil charge l'un de ses membres, Ami Perrin, de solliciter le retour de Calvin. Un émissaire rencontre le réformateur à Worms, lors d'une conférence destinée à résoudre des disputes religieuses. Sa première réaction est négative, puisqu'il écrit .

Après réflexion, cependant, Calvin se déclare néanmoins prêt à suivre l'appel du Seigneur. Il est prévu que Viret prendra temporairement en charge Genève, tandis que Bucer et Calvin visiteront la ville pour organiser les étapes suivantes. Le Conseil municipal insiste toutefois sur la nomination immédiate de Calvin. À l'été 1541, Strasbourg délègue donc Calvin pour six mois à la ville de Genève; ce dernier et sa famille prennent la route le 13 septembre en direction du Léman, accompagnés d'une escorte officielle .

Soutenant les propositions de réforme de Calvin, le Conseil de Genève vote les "Ordonnances ecclésiastiques" le 20 novembre 1541. Ces ordonnances définissent quatre types de fonctions ministérielles : les pasteurs pour prêcher et administrer les sacrements, les docteurs pour instruire les croyants dans la foi, les Anciens pour assurer la discipline et les diacres pour prendre soin des pauvres et des nécessiteux. Ces ordonnances appellent également à la création d'un "Consistoire", tribunal ecclésiastique composé d'«Anciens» laïcs et de pasteurs. Le gouvernement municipal conserve le pouvoir de convoquer des accusés devant le tribunal. Le Consistoire ne peut juger que des affaires religieuses qui n'ont pas d'implications devant la justice civile. Initialement, le tribunal peut infliger des peines, dont la plus sévère est l'excommunication. Le gouvernement civil conteste cependant ce pouvoir et le 19 mars 1543, le Conseil décide que toutes les condamnations seront infligées par les autorités civiles.

En 1542, Calvin, adaptant un livre liturgique utilisé à Strasbourg, publie "La Forme des Prières et Chants Ecclésiastiques", étant persuadé que la musique soutient la lecture de la Bible. Le psautier originel de Strasbourg renferme douze psaumes de Clément Marot; Calvin ajoute dans la version genevoise plusieurs hymnes de sa propre composition. À la fin de l'année 1542, Marot se réfugie lui-aussi à Genève et compose dix-neuf autres psaumes. Loys Bourgeois, également réfugié, enseigne la musique à Genève depuis seize ans et Calvin en profite pour intégrer les hymnes de ce dernier. La même année, il publie le "Catéchisme de l'Église de Genève", inspiré de la "Kurze Schrifftliche Erklärung" de Bucer (1534).

Durant son ministère à Genève, Calvin rédige plus de prédications, données initialement deux fois le dimanche, et trois fois durant la semaine. Ses sermons durent plus d'une heure et l'orateur parle sans notes . Un greffier tente parfois d'enregistrer ses messages, mais peu de sermons sont préservés avant 1549. Cette année-là, le scribe Denis Raguenier, qui a appris ou développé un système de sténographie, est chargé d'enregistrer tous les sermons de Calvin. Une analyse de ces textes, réalisée par T. H. L. Parker, suggère que le prédicateur était constant dans ses thèses et que son style a peu évolué au cours des années.

On ne sait que très peu de choses sur la vie privée de Calvin à Genève. Sa maison et son mobilier appartiennent à la Ville. La demeure est assez grande et accueille sa famille, ainsi que celle de son frère Antoine, avec quelques serviteurs. Le 28 juillet 1542, Idelette donne naissance à un fils, Jacques, mais celui-ci meurt en bas-âge. Idelette tombe malade en 1545 et meurt le 29 mars 1549. Calvin ne se remarie jamais et exprime sa tristesse dans une lettre à Viret : 
Tout au long de sa vie à Genève, le réformateur reste en contact étroit avec ses anciens amis, dont Montmor, Cordier, Cop, Farel, Melanchthon et Bullinger.

Calvin rencontre bientôt une forte opposition à Genève. Vers 1546, ses adversaires se constituent en un groupe qu'il appelle les libertins. Selon Calvin, ces personnes pensent qu'après avoir été affranchies par la grâce irrésistible de Dieu, elles sont exemptées des lois civiles et ecclésiastiques. Le groupe rassemble des familles riches et politiquement puissantes à Genève. À la fin du mois de janvier 1546, Pierre Ameaux, un fabricant de cartes à jouer qui a déjà eu maille à partir avec le Consistoire, attaque Calvin en le traitant de « Picard », expression dénotant un sentiment anti-français, et l'accuse de promouvoir de fausses doctrines. Ameaux est condamné par le Conseil et forcé d'expier son crime en se voyant exposé publiquement, suppliant Dieu de lui pardonner. Quelques mois plus tard, Ami Perrin, l'homme qui avait convaincu Calvin de venir à Genève, se montre aussi ouvertement hostile. Il a épousé Françoise Favre, fille d'un marchand allemand bien établi qui, ayant enfreint les lois contre la danse, a été puni par le Consistoire.

En 1547, l'opposition à Calvin et aux autres pasteurs français réfugiés grandit et gagne la majorité des magistrats civils de Genève. Le 27 juin, une lettre de menaces anonymes en patois genevois, est découverte sur la chaire de la cathédrale Saint-Pierre de Genève, où prêche Calvin. Suspectant un complot contre l'Église et l'État, le Conseil nomme une commission d'enquête. Jacques Gruet, un soutien de Favre, est arrêté et des preuves contre lui sont découvertes dans sa maison. Sous la torture, il avoue plusieurs crimes dont la rédaction de la lettre anonyme, qui menace Dieu, ses ministres et tout l'ordre religieux. Le tribunal civil le condamne à mort et, avec l'approbation de Calvin, il est décapité le 26 juillet 1547, à Champel.

Les libertins cependant poursuivent leur opposition en attisant le mécontentement populaire, en insultant les pasteurs et en défiant l'autorité du Consistoire. Le Conseil encourage les deux camps en admonestant ou en défendant alternativement Calvin et les libertins. Lorsque Perrin est élu premier syndic en février 1552, l'autorité de Calvin semble tomber à son plus bas niveau. Après quelques défaites devant le Conseil, Calvin demande au Conseil, le 24 juillet 1553, l'autorisation de démissionner. Sa requête est toutefois refusée, l'opposition réalisant qu'elle peut assurément affaiblir l'autorité de Calvin, mais qu'elle n'a pas assez de pouvoir pour le bannir.

Un retournement de situation a lieu lorsque Michel Servet, fugitif condamné par toutes les autorités ecclésiastiques, arrive à Genève le 13 août 1553. Servet est un médecin espagnol et un théologien protestant qui critique fermement les doctrines de la Trinité et le pédobaptisme, c'est à dire le baptême des enfants. En juin 1530, il affronte Œcolampade à Bâle et est expulsé. Il se rend à Strasbourg, où il publie un pamphlet contre la Trinité. Bucer le réfute publiquement et demande à Servet de partir. Revenu à Bâle, Servet publie les "Dialogues sur la Trinité" en deux livres () qui scandalisent à la fois les réformés et les catholiques. L'Inquisition espagnole ordonne son arrestation.

Calvin et Servet (ce dernier alors encore à Bâle) entrent en contact en 1546 par l'intermédiaire d'une connaissance commune, l'imprimeur lyonnais Jean Frellon. Leurs lettres débattant de questions théologiques sont signées respectivement "Michael Servetus" et "Charles d'Espeville", pseudonyme de Calvin. Ce dernier perd finalement patience et refuse de répondre plus longtemps. Il est particulièrement outré lorsque Servet lui renvoie une copie de l"Institution de la religion chrétienne" sévèrement annotée avec des arguments soulignant les erreurs du livre. Calvin écrit à Farel le 13 février 1546 en précisant que si Servet devait venir à Genève, il ne pourrait lui garantir un sauf-conduit, .

En 1553, Guillaume de Trie, un ami de Calvin, écrit à l'Inquisition française relativement à Servet, le qualifiant d'« hispano-portugais » et le critiquant pour ses origines juives,récemment découvertes ; il écrit encore que . Lorsque l'inquisiteur-général de France apprend que Servet se cache à Vienne sous un faux nom, il contacte le cardinal François de Tournon, secrétaire de l'archevêque de Lyon, pour qu'il enquête. Servet est arrêté et interrogé. Ses lettres à Calvin sont présentées comme preuve d'hérésie mais il nie les avoir écrites. Il déclare, après avoir juré sur la Bible qu'il . Le lendemain, il déclare que . Il parvient à s'évader de prison et les autorités catholiques le condamnent à mort.

En route pour l'Italie, Servet s'arrête à Genève pour des raisons inconnues. Il y est reconnu et arrêté. Le secrétaire de Calvin, Nicolas de la Fontaine, compose un acte d'accusation qui est soumis au tribunal. Philibert Berthelier, procureur, appartient au groupe des libertins et est fils d'un patriote genevois. Les séances du tribunal sont dirigées par Pierre Tissot, beau-frère de Perrin. Les libertins font traîner le procès pour affaiblir Calvin. La réputation d'hérétique de Servet leur crée un dilemme et, le 21 août, le Conseil décide prendre l'avis de villes confédérées. En attendant les réponses, le Conseil donne le choix à Servet d'être jugé à Vienne ou à Genève. Celui-ci opte pour Genève. Le 20 octobre, ayant reçu les réponses de Zurich, Berne, Bâle et Schaffhouse, le Conseil déclare Servet hérétique. Il est condamné au bûcher. Calvin et les autres pasteurs demandent, mais en vain, que par charité il soit décapité. Servet est brûlé vif le 27 octobre 1553 sur le plateau de Champel, aux portes de Genève.

Après la mort de Servet, Calvin passe pour un défenseur de la Chrétienté, mais son triomphe contre les libertins est encore à venir. Calvin a toujours exigé que le Consistoire ait le pouvoir d'excommunication, malgré la décision contraire du Conseil. Durant le procès de Servet, Philibert Berthelier demande au Conseil la permission de pouvoir prendre la communion, car il avait été excommunié l'année précédente pour avoir insulté un pasteur. Calvin proteste, en avançant que le Conseil n'a pas autorité pour annuler l'excommunication de Berthelier. Avant de connaître l'issue de la dispute, il signale, dans un sermon du 3 septembre 1553, que la demande de Berthelier pourrait être rejetée par les autorités. Le Conseil décida de réexaminer les "Ordonnances" et, le 18 septembre, admet en effet que l'excommunication relève de l'autorité du Consistoire. En novembre, cependant, Berthelier en appelle à une autre assemblée administrative de Genève, le Conseil des Deux-Cents. Ce corps s'oppose au jugement précédent et décide que le Conseil doit être l'arbitre final d'une décision consistoriale. Une fois de plus, l'avis des villes confédérées est requis et, finalement, le 22 janvier 1555, le Conseil se rend à l'arbitrage helvétique : les "Ordonnances" originales doivent être respectées et le Consistoire recouvre la totalité de son autorité.

La chute des libertins commence avec les élections de février 1555. De nombreux réfugiés français ont alors reçu la citoyenneté genevoise et, avec leur appui, les partisans de Calvin regagnent une majorité des voix auprès des syndics et conseillers. Les libertins complotent cependant et, le 16 mai, se préparent à incendier une maison qu'ils pensaient occupée par des Français. Le syndic Henri Aulbert tente de s'interposer en affichant le sceptre symbolisant son pouvoir. Perrin, étourdiment, s'empare de ce bâton de commandement, signifiant ainsi qu'il prend le pouvoir. L'insurrection est stoppée dès qu'un autre syndic arrive et l'on emmène Perrin à l'hôtel de ville. Ce dernier, tout comme certains autres meneurs, est expulsé de la ville. D'autres conspirateurs sont arrêtés et exécutés. L'opposition à l'autorité ecclésiastique de Calvin est ainsi décapitée.

L'autorité de Calvin est dès lors incontestée durant les dernières années de sa vie. Il jouit d'une réputation internationale en tant que réformateur distinct de Martin Luther. Les deux hommes, initialement, s'apprécient, mais un conflit doctrinal se développe entre Luther et le réformateur Ulrich Zwingli, de Zurich, au sujet de l'eucharistie. Calvin se place dans le camp de Zwingli et participe activement aux polémiques entre les branches luthériennes et réformées du protestantisme, tout en déplorant le manque d'unité parmi les réformateurs. Il se rapproche par conséquent de Bullinger en signant le "Consensus Tigurinus", un concordat entre les églises de Zurich et de Genève. Il entre également en contact avec l'archevêque de Cantorbéry, Thomas Cranmer, lorsque ce dernier appelle à un synode œcuménique de toutes les églises protestantes. Calvin soutient l'idée, mais Cranmer ne parvient pas à la réaliser.

La plus grande contribution de Calvin à la communauté anglophone est l'accueil à Genève des exilés protestants chassés d'Angleterre par les persécutions de la reineMarie à partir de 1555. Ils constituent ainsi leur propre église réformée, menée par John Knox et William Whittingham, et importent finalement les idées de Calvin en Angleterre et en Écosse. Calvin est toutefois plus intéressé par l'introduction de la Réforme en France, son pays natal. Il y soutient la formation d'églises en fournissant des livres et en envoyant des pasteurs. Entre 1555 et 1562, plus de 100 ministres sont ainsi envoyés en France. Cet engagement est entièrement financé par l'église genevoise, le Conseil de la ville refusant de s'impliquer dans des activités de prosélytisme. Les protestants de France étaient alors persécutés en raison de l'Édit de Chateaubriant promulgué par le roi Henri II. Lorsque les autorités françaises se plaignent de ces actions missionnaires, la Ville de Genève peut en toute bonne foi décliner toute responsabilité.
À Genève, Calvin se soucie de la création d'un collège. Le site de l'école est choisi le 25 mars 1558 et l'établissement ouvre ses portes le 5 juin 1559. L'école est divisée en deux parties : un Collège, ou "schola privata", et un lycée, appelé Académie ou "schola publica". Calvin tente de recruter deux professeurs, Mathurin Cordier, son ancien ami latiniste basé à Lausanne, et Emmanuel Tremellius, "" à Cambridge. Aucun des deux n'étant disponible, il parvient à convaincre Théodore de Bèze de se charger de la fonction de recteur. Cinq ans après son ouverture, l'établissement accueille étudiants dont 300 à l'Académie. Le Collège devient finalement le Collège Calvin, l'une des écoles de maturité de Genève, tandis que l'Académie sera l'ancêtre de l'université de Genève.
À l'automne 1558, Calvin est atteint d'une fièvre et, craignant de mourir avant d'achever sa dernière révision de l"Institution", il accélère son rythme de travail. Il récrit en grande partie cette nouvelle édition, qu'il considère comme une nouvelle œuvre. Le passage de 21 à 80 chapitres résulte du développement des textes existants, plutôt qu'en raison de l'adjonction de nouveaux thèmes. A l'occasion d'un culte, toutefois, un violent accès de toux pendant la prédication provoque une hémorragie pulmonaire. Sa santé décline dès lors et il donne son dernier sermon à la cathédrale Saint-Pierre le 6 février 1564. Il rédige son testament le 25 avril, prévoyant des legs à sa famille et au Collège. Quelques jours plus tard, les pasteurs genevois lui rendent une dernière visite et ses adieux sont consignés dans son "Discours d'adieu aux ministres". Il y relate sa vie à Genève, et rappelle les difficultés qu'il a parfois rencontrées.

Calvin meurt le 27 mai 1564 à l'âge de 54 ans. Son corps est d'abord exposé mais, devant l'affluence de visiteurs, les réformateurs craignent d'être accusés de promouvoir le culte d'un saint. Il est inhumé le lendemain dans une fosse anonyme, au cimetière des Rois. L'emplacement exact de la tombe est inconnu, mais une pierre funéraire est posée au pour marquer l'emplacement traditionnellement considéré comme son lieu de repos.

Calvin expose sa théologie dans ses commentaires de la Bible, ainsi que dans ses sermons et ses essais. Mais l'expression la plus concise de sa pensée se trouve dans son œuvre maîtresse, l"Institution de la religion chrétienne". Ce livre offre un résumé de ses vues sur la théologie chrétienne et Calvin tient à ce qu'il soit lu parallèlement à ses commentaires. S'il retouche cet ouvrage tout au long de sa vie, les versions successives montrent cependant que sa pensée, en fait, a peu évolué. La première édition de 1536 ne compte que six chapitres. La seconde, publiée en 1539, est trois fois plus longue, car l'auteur complète son texte par des thèmes apparaissant dans les "Loci Communes" de Melanchthon. En 1543, il ajoute de nouveaux passages et approfondit le chapitre consacré au symbole des apôtres. La dernière édition de l"Institution" est publiée en 1559. L'ouvrage comprend alors quatre livres pour un total de 80 chapitres, et chaque livre porte le nom d'une confession de foi : 1) Dieu le créateur; 2) la rédemption par Jésus-Christ; 3) la réception de la Grâce de Dieu par le Saint-Esprit; 4) l'Église.
La première confession, dans l"Institution," en constitue le thème central. Elle avance que la sagesse humaine comprend deux parties: la connaissance de Dieu, et la connaissance que nous avons de nous-mêmes. Selon Calvin, la connaissance de Dieu n'est pas inhérente à l'humanité et ne peut être découverte en observant la nature. La seule manière d'y parvenir est d'étudier les Écritures saintes. Calvin écrit, . Il n'essaye pas de prouver l'autorité des Écritures mais les décrit plutôt comme "autopiston" ou « certaines en elles-mêmes ». Il défend l'idée de la Trinité et, dans une virulente polémique avec l'Église catholique, affirme que les images religieuses mènent à l'idolâtrie. À la fin du premier livre, il offre sa vision de la providence en écrivant, . Les hommes sont incapables de comprendre pourquoi Dieu veut une situation particulière mais, quelles que soient leurs actions, bonnes ou mauvaises, celles-ci entraînent toujours l'exécution de la volonté divine.

Le second livre comporte plusieurs essais sur le péché originel et la chute de l'homme; il fait directement référence à Augustin d'Hippone, qui développa ces doctrines. Il cite fréquemment les Pères de l'Église pour défendre la cause de la Réforme et pour démentir l'accusation de créer une nouvelle théologie. Dans l'esprit de Calvin, le péché, initié par la chute d'Adam, s'est transmis à toute l'humanité. Par conséquent, la domination du péché est si complète que les hommes sont poussés à commettre le mal. Cette humanité déchue a donc un besoin de rédemption qui ne peut être trouvé que dans le Christ. Cependant, avant d'exposer cette doctrine, Calvin décrit la situation particulière des juifs vivant à l'époque de l'Ancien Testament. Dieu ayant fait une alliance avec Abraham, le sens profond de cette promesse est la venue de Jésus. Par conséquent, l'ancienne Alliance ne s'oppose pas au Christ mais en est au contraire la promesse. Calvin décrit ensuite la nouvelle Alliance en utilisant le symbole des apôtres, relatant la souffrance de Jésus sur la croix et son retour pour juger les vivants et les morts. Pour Calvin, l'obéissance du Christ au Père efface la discorde qui régna jusque là entre l'humanité et Dieu.

Dans le troisième livre, Calvin décrit comment l'union spirituelle du Christ et de l'humanité est achevée. Il définit d'abord la foi comme la connaissance ferme et certaine de Dieu en Christ. Les effets immédiats de la foi sont la repentance et la rémission du péché. Cela est suivi par une régénération spirituelle qui ramène le croyant au même état de sainteté que celui d'Adam avant sa transgression. La perfection complète est cependant inaccessible dans cette vie et le croyant doit s'attendre à une lutte continuelle contre le péché. Plusieurs chapitres sont ensuite consacrés au thème de la justification par la foi seule. Calvin définit la justification comme . Dans cette définition, il est clair que c'est Dieu qui possède l'initiative et l'autorité, et que les hommes n'y jouent aucun rôle : Dieu est souverain dans le salut. Il en découle que les réformateurs honnissent les indulgences, qui font croire que « l’achat de Paradis [serait] taxé à certains deniers » pour qu’ensuite « les oblations [soient] vilainement despendues en paillardises et gourmandises ». Au chapitre XIV, Calvin décrit et défend la doctrine de prédestination, un concept développé par saint Augustin par opposition aux enseignements de Pélage. D'autres théologiens, comme Thomas d'Aquin et Martin Luther, ont également suivi la tradition augustinienne sur ce point. Ce principe, dans les mots de Calvin, est que .

Le dernier livre décrit ce qu'il considère être la véritable Église et ses ministres, son autorité et ses sacrements. Calvin refuse l'idée de primauté pontificale, tout comme l'accusation de schisme portée contre les réformateurs. Pour Calvin, l'Église est définie comme le corps des fidèles qui placent Christ à sa tête. Par définition, il n'y a qu'une Église « catholique » ou « universelle ». Les ministres de l'Église sont décrits par un passage de l'Épître aux Éphésiens et ce groupe comprend les apôtres, les prophètes, les évangélistes, les pasteurs et les docteurs. Calvin considère que les trois premières charges sont limitées à l'époque du Nouveau Testament. Les deux dernières fonctions ont été créées dans l'église à Genève. Même si Calvin respecte le travail des conciles œcuméniques, il les considère comme soumis à la parole de Dieu, c'est-à-dire à l'enseignement des Écritures. Il pense également que les autorités civiles et religieuses doivent être séparées, sans interférences entre elles.

Calvin définit un sacrement comme un signe terrestre associé à une promesse à Dieu. Selon lui, deux sacrements seulement sont valides sous la nouvelle Alliance : le baptême et l'eucharistie, par opposition aux sept sacrements de l'église catholique. Il rejette la doctrine catholique de la transsubstantiation et le traitement de l'eucharistie comme un sacrifice. Il refuse également la doctrine luthérienne de l'union sacramentale, dans laquelle Christ est du vin et du pain. Sa pensée, sur ce point, rejoint celle de Zwingli. Plutôt que d'avoir une vision purement symbolique, Calvin note qu'avec la participation du Saint-Esprit, la foi est nourrie et renforcée par ce sacrement. Selon lui, l'eucharistie est .

La théologie de Calvin a été critiquée par d'autres théologiens. En 1536, Pierre Caroli, un pasteur protestant de Lausanne, accuse Calvin, ainsi que Viret et Farel, d'arianisme. Calvin défend ses positions sur la Trinité dans la "Confessio de Trinitate propter calumnias P. Caroli". En 1551, Jérome-Hermès Bolsec, un médecin genevois, attaque la doctrine de la prédestination et accuse Calvin de faire de Dieu l'auteur du péché. Bolsec est banni de la ville et, après la mort de Calvin, il rédige une biographie très critique de ce réformateur . L'année suivante, Joachim Westphal, un pasteur de Hambourg, condamne pour hérésie Calvin et Zwingli dans un pamphlet en latin, leur reprochant leur refus d'approuver la doctrine luthérienne de l'eucharistie. Calvin lui répond dans sa "Defensio sanae et orthodoxae doctrinae de sacramentis" en 1555. En 1556 Justus Velsius, un dissident hollandais, organise une disputatio avec Calvin durant la visite de ce dernier à Francfort, au cours de laquelle Velsius défend la notion de libre-arbitre contre celle de la prédestination. Après l'exécution de Servet, un proche de Calvin, Sébastien Castellion, rompt avec lui sur la question du traitement des hérétiques. Dans le "Traité des Hérétiques", Castellion défend les enseignements charitables du Christ, contre la raideur vaniteuse d'une d'une institution ecclésiale. Il développe par la suite une théologie de la tolérance basée sur les principes bibliques.

Les historiens ont débattu de l'opinion de Calvin sur les juifs et le judaïsme. Certains ont avancé que Calvin, de tous les réformateurs contemporains, était le moins antisémite, tout particulièrement en comparaison avec Luther. D'autres ont affirmé que Calvin était fermement dans le camp des antisémites. Les spécialistes s'accordent cependant sur la distinction à faire entre les idées de Calvin sur les juifs à l'époque biblique, et sur son attitude envers ses contemporains. Dans sa théologie, Calvin ne fait aucune différence entre l'alliance de Dieu avec Israël et la nouvelle Alliance. Il écrit: . Calvin est néanmoins un partisan de la théologie de la substitution et avance que les juifs sont un peuple rejeté, qui doit embrasser Jésus pour rentrer dans l'Alliance.

La plupart des déclarations de Calvin sur les juifs contemporains sont polémiques. Il écrit par exemple: . À cet égard, il diffère peu des autres théologiens protestants et catholiques de son époque. Il considère les juifs comme un peuple déicide et des « chiens profanes », des scélérats qui .

Dans ses écrits connus, Calvin a consacré un seul traité au judaïsme contemporain, "Réponse aux questions et objections d'un certain juif". Il y affirme que les juifs interprètent mal leurs propres écritures, car il leur manque l'unité de l'Ancien et du Nouveau Testament. Calvin écrit également que leur .

La première publication de Calvin est un commentaire du "De Clementia" de Sénèque. Publié à compte d'auteur en 1532, il s'y montre comme un humaniste dans la tradition d'Érasme, possédant une connaissance approfondie des auteurs classiques. Son premier ouvrage de théologie, "Psychopannychia", tente de réfuter la doctrine du sommeil de l'âme proposée par les anabaptistes. Calvin le rédige probablement à la suite du discours d'investiture de Nicolas Cop, en 1533, mais l'ouvrage n'est publié qu'en 1542 à Strasbourg.
Calvin rédige des commentaires de la plupart des livres de la Bible. Son premier commentaire, sur l'épître aux Romains, est publié en 1540 et il envisage d'écrire des commentaires sur l'ensemble du Nouveau Testament. Il écrit son second traité sur la première épître aux Corinthiens six ans plus tard, mais consacre ensuite toute son attention à l'objectif qu'il s'est fixé. En moins de quatre ans, il publie des commentaires sur toutes les épîtres de Paul et discute également ses lettres aux Romains. Il s'intéresse ensuite aux épîtres catholiques, dédiant son texte au roi Édouard VI d'Angleterre. En 1555, il achève son étude du Nouveau Testament en terminant par les Actes des Apôtres; il omet la troisième épître de Jean et l"Apocalypse".

Pour l'Ancien Testament, il rédige des commentaires sur le Livre d'Isaïe, les livres du Pentateuque, les Psaumes et le livre de Josué. Calvin fonde ses publications sur les conférences données aux étudiants et aux ministres, textes qu'il retravaille ensuite. Cependant, à partir de 1557, faute de temps, il autorise la publication de ses discours directement à partir de notes sténographiées. Ces "Praelectiones" couvrent les petits prophètes, les livres de Daniel, de Jérémie, des Lamentations et une partie de celui d'Ézéchiel.

Calvin écrit également de nombreuses lettres et traités. Son "Traité des reliques", rédigé en français en 1543, connaît un grand succès, et est traduit dans plusieurs langues ; il y ridiculise le culte des reliques. Après sa "Responsio ad Sadoletum", Calvin rédige en 1543, à la demande de Bucer, une lettre ouverte à l'empereur Charles Quint, "Supplex exhortatio ad Caesarem", qui défend la foi réformée. Suit une lettre ouverte au pape, "Admonitio paterna Pauli III", en 1544, dans laquelle Calvin critique Paul III pour son opposition à un rapprochement avec les réformés. Le concile de Trente entraîne l'application de nouveaux décrets contre les protestants et Calvin réfute ces textes avec ses "Acta synodi Tridentinae cum Antidoto" de 1547. Lorsque Charles-Quint tente de trouver un compromis avec l'intérim d'Augsbourg, Bucer et Bullinger pressent Calvin de répondre. Il rédige le traité "Vera Christianae pacificationis et Ecclesiae reformandae ratio" en 1549, dans lequel il décrit les doctrines qui doivent être défendues, dont la justification par la foi seule.

Calvin fournit de nombreux documents de base pour les églises réformées: notamment des traités sur le catéchisme, la liturgie et l'organisation de l'église. Il rédige également plusieurs confessions de foi pour essayer d'unifier les églises. En 1559, il ébauche la confession de foi française, dite confession de La Rochelle. Le synode de Paris l'accepte avec quelques modifications. La Confessio Belgica de 1561, une confession de foi néerlandaise, est en partie basée sur la confession de La Rochelle.

Calvin est aussi l'auteur de très nombreux sermons qui occupaient 43 volumes. Par erreur, ces volumes ont été vendus au poids en 1805 par la Bibliothèque de Genève où ils étaient conservés ! L'ouverture des archives de l’Église protestante française de Londres à un chercheur par la pasteure Leila Hamrat en 1995 a permis de redécouvrir 3 volumes, soit 243 sermons sur les chapitres 22 à 66 d’Ésaïe prêchés du 22 mai 1557 au 26 août 1559.

Après la mort de Calvin et de Théodore de Bèze, son successeur, le Conseil municipal de Genève reprend progressivement le contrôle de fonctions relevant précédemment du domaine ecclésiastique. La sécularisation est accompagnée d'un déclin de l'église. Même l'Académie de Genève est éclipsée par les universités de Leyde et d'Heidelberg qui deviennent les nouveaux bastions des idées de Calvin, qualifiées de calvinisme pour la première fois par Joachim Westphal en 1552. En 1585, Genève, auparavant la source du mouvement réformé, n'est plus que son symbole. Calvin avait refusé d'être considéré lui-même comme une « idole » et Genève comme la « nouvelle Jérusalem ». Il encourage au contraire ses disciples à s'adapter à leur environnement. Même durant son échange polémique avec Westphal, il conseille à un groupe de réfugiés francophones installés à Wesel, en Allemagne, l'intégration aux églises luthériennes locales. Malgré ses différends avec les luthériens, il concède qu'il appartiennent à la véritable Église. La nécessité de s'adapter aux conditions locales devient grâce à Calvin une caractéristique importante du mouvement réformateur qui s'étend alors en Europe.

Grâce aux travaux missionnaires de Calvin en France, son programme de réforme arrive finalement jusque dans les provinces francophones des Pays-Bas. Par ailleurs, le calvinisme est adopté dans l'électorat du Palatinat sous Frédéric III, ce qui entraîne la formulation du catéchisme de Heidelberg en 1563. Ce dernier, et la Confessio Belgica, sont adoptés comme standards confessionnels lors du premier synode de l'église réformée néerlandaise en 1571. Des dirigeants religieux, calvinistes ou sympathisants, s'implantent en Angleterre (Martin Bucer, Pierre Martyr et Jean de Lasco), et en Écosse (John Knox). Durant la Première Révolution anglaise, les puritains calvinistes rédigent la confession de foi de Westminster qui devient un standard des presbytériens dans le monde anglophone. Le mouvement s'étend ensuite à d'autres parties du monde, dont l'Amérique du Nord, l'Afrique du Sud et la Corée.


Les Archives de l'Etat de Neuchâtel conservent la correspondance autographe que Jean Calvin a envoyée à d'autres réformateurs, notamment Guillaume Farel





</doc>
<doc id="14941" url="https://fr.wikipedia.org/wiki?curid=14941" title="Gluon">
Gluon

En physique, les gluons sont les bosons de jauge responsables de l'interaction forte. Les gluons confinent les quarks ensemble en les liant très fortement. Ils permettent ainsi l'existence des protons et des neutrons, ainsi que des autres hadrons et donc de l'univers que nous connaissons.


Dans la théorie de la chromodynamique quantique (en anglais : quantum chromodynamics, ou QCD) utilisée aujourd'hui pour décrire l'interaction forte, les gluons sont échangés lorsque des particules possédant une charge de couleur interagissent. Lorsque deux quarks échangent un gluon, leur charge de couleur change ; le gluon se chargeant d'une anti-couleur compensant la perte du quark, de même que la nouvelle charge de couleur du quark. Étant donné que les gluons portent eux-mêmes une charge (et une anti-charge) de couleur, ils peuvent aussi interagir avec d'autres gluons, ce qui rend l'analyse mathématique de l'interaction forte très compliquée.

" A priori" il pourrait y avoir neuf types de gluons, un pour chaque combinaison de charge et d'anti-charge de couleur (rouge, vert, bleue, et anti-rouge, anti-vert, anti-bleue), ce qui donnerait les gluons suivants :

formula_1.

En fait, du point de vue mathématique il existe un nombre infini de gluons, chacun pouvant être représenté par une combinaison linéaire des neuf états fondamentaux (aussi appelés états propres) listés ci-dessus. Par exemple, un gluon pourrait être représenté par l'état combiné formula_2. Ce genre de combinaisons d'états est assez courant en mécanique quantique.

Cependant, la chromodynamique quantique nous enseigne que la relation linéaire suivante lie trois des états fondamentaux, du fait que les états complètement neutres du point de vue de la couleur n'interagissent pas par interaction forte :

formula_3

Cela implique alors que les neuf états fondamentaux cités plus haut ne sont plus tous indépendants. Cette relation réduit de un le nombre de degrés de liberté correspondants. Il n'y a plus que huit degrés de liberté disponibles, donc huit états fondamentaux linéairement indépendants, donc huit gluons.

La première trace expérimentale des gluons a été découverte en 1979 dans l'accélérateur de particules PETRA (collisions électron-positron) du laboratoire DESY à Hambourg, lorsque fut réalisée la preuve d'une collision à trois jets : le troisième jet fut ainsi attribué à l'émission d'un gluon par un des quarks produits.

Selon la théorie du Big Bang, l'Univers primordial était à une température et une pression telles que les quarks et les gluons devaient être totalement libres ("déconfinés"). Cet état est dit "plasma de quarks et de gluons" (PQG), puis alors que ce plasma se refroidissait, les gluons ont confiné les quarks ensemble, ce qui permet l'existence des protons et des neutrons, ainsi que des autres hadrons. Une expérience de physique nucléaire et hadronique nommée "ALICE" vise à étudier ce plasma, pour mieux comprendre la chromodynamique quantique. Ce plasma sera produit au LHC (Large Hadron Collider) du CERN, par collisions d’ions lourds (de plomb) à très haute énergie. Ces collisions devraient produire une température plus de fois supérieures à celle qui règne au cœur du Soleil, ce qui devrait en quelque sorte faire « fondre » les protons et les neutrons de la matière, libérant les quarks de l’emprise des gluons et créant un état de la matière encore jamais observé : le "plasma de quarks et de gluons".




</doc>
<doc id="14942" url="https://fr.wikipedia.org/wiki?curid=14942" title="Muon">
Muon

Le muon est, selon le modèle standard de la physique des particules, une particule élémentaire de charge électrique négative. Le muon a les mêmes propriétés physiques que l'électron, mis à part sa masse, 207 fois plus grande (105,66 MeV.c, c'est pour cela qu'on l'appelle parfois « électron lourd ») et possède un spin d'1/2. Les muons sont des fermions de la famille des leptons aux côtés des électrons et des taus. Les muons sont notés μ. L'antimuon, l'antiparticule associée au muon, est notée μ et est chargée positivement.

Sur Terre, les muons sont produits par la désintégration de pions chargés. Les pions sont créés dans la haute atmosphère par des rayons cosmiques. Les muons ont une durée de vie faible (environ deux microsecondes). Cependant, les muons ont une grande énergie, ainsi l'effet de dilatation temporelle décrite par la relativité restreinte les rend observables à la surface de la Terre.

Tout comme il existe un neutrino électronique associé à l'électron, il existe un neutrino muonique qui est associé au muon. Les neutrinos muoniques sont notés ν.

Les muons positifs peuvent former une particule appelée le muonium, ou μe. À cause de la différence de masse entre le muon et l'électron, le muonium ressemble plus à un atome d'hydrogène que le positronium.

La masse du muon est voisine de celle du pion.

Le muon trouve sa place dans le tableau récapitulatif suivant.

Les muons furent découverts par Carl David Anderson et son assistant Seth Neddermeyer, au Caltech, en 1936, alors qu'ils travaillaient sur les rayons cosmiques. Ils remarquèrent des particules dont la trajectoire s'incurvait de manière distincte de celle des électrons et des autres particules connues, lorsqu'elles étaient soumises à un champ magnétique. Ces nouvelles particules portaient une charge électrique négative mais leur trajectoire était moins incurvée que celle des électrons mais plus incurvée que celle des protons à vitesse égale. On supposait que leur charge électrique négative était égale à celle de l'électron et qu'étant donné la différence de courbure de la trajectoire, on devait en déduire qu'elles avaient une masse intermédiaire à celle de l'électron et du proton.

C'est pour cela qu'Anderson nomma d'abord cette particule "mesotron", dont le préfixe "meso-" venant du grec signifie "intermédiaire". Comme peu après d'autres particules de masses intermédiaires furent découvertes, le terme générique de "meson" fut adopté pour nommer de telles particules. Face au besoin de les différencier, le "mesotron" fut renommé "mu meson" (avec la lettre grecque "μ" ("mu") utilisée pour ressembler au son de la lettre latine "m").

Cependant on découvrit bientôt que le "mu meson" différait de manière significative des autres mésons; par exemple ses produits de désintégration comprenaient un neutrino et un antineutrino, en lieu et place de l'un ou de l'autre, comme on l'observait pour les autres mésons, ceux-ci étant des hadrons, particules formées de quarks et donc sujettes à des interactions fortes. Dans le modèle de quark, un "meson" est composé d'exactement deux quarks (un quark et un anti-quark), à la différence des baryons qui sont composés de trois quarks. On découvrit, cependant, que les "mu mesons" étaient des particules fondamentales (leptons) comme les électrons, sans structure de quark. Ainsi les "mu mesons" n'étant pas du tout des mésons (au sens nouvellement défini du terme "méson"), le terme "mu meson" fut abandonné et remplacé par la nouvelle appellation de "muon".



</doc>
<doc id="14943" url="https://fr.wikipedia.org/wiki?curid=14943" title="Sinaloa">
Sinaloa

Le Sinaloa est un État du Mexique situé sur la côte du golfe de Californie (océan Pacifique). Entouré par les États de Sonora, de Chihuahua, de Durango et de Nayarit, il occupe une superficie de km². Avec habitants, le Sinaloa est l'État le plus peuplé du nord-ouest du Mexique.

Il est divisé en 18 municipalités (du Nord au Sud) : Choix, , Ahome, , Guasave, Mocorito, Salvador Alvarado, Angostura, Navolato, Culiacán, Cosalá, Elota, San Ignacio, Mazatlán, Concordia et Escuinapa. Sa capitale est Culiacán de Rosales ( habitants). 

Villes principales (ordonnées par taille) : Culiacán, Mazatlán, Los Mochis, Sinaloa, et Angostura.

En outre, l'état de Sinaloa a une population majoritairement blanche et métisse. 

En février 2013 la dépouille de Julia Pastrana est rendue au Mexique par l'Université d'Oslo et est enterrée dans l'état de Sinaloa.

L'homme politique mexicain Mario López Valdez en fut le gouverneur.


Le climat est chaud et semi aride dans les plaines, avec des températures moyennes allant de 24 à 25 degrés. Les précipitations annuelles sont de 600 mm, allant jusqu'à mm au sud de l'état. Sinaloa est traversé par plusieurs fleuves côtiers qui forment de larges vallées fertiles au pied des montagnes. Les principaux cours d'eau sont les ríos Culiacán, Fuerte et Sinaloa.



</doc>
<doc id="14946" url="https://fr.wikipedia.org/wiki?curid=14946" title="Nombres dans le monde">
Nombres dans le monde

Tableau présentant au travers des nombres un échantillon des langues et des écritures du monde.

"Note : les mots indiqués en caractères italiques sont des translittérations approchantes."








</doc>
<doc id="14948" url="https://fr.wikipedia.org/wiki?curid=14948" title="Rajasthan">
Rajasthan

Le Rajasthan (, littéralement ) est un État du nord-ouest de l'Inde. Sa capitale est Jaipur.

Il est bordé à l'ouest par le Pakistan, au nord par le Pendjab, au nord-est par l'Haryana et l'Uttar Pradesh, au sud-est par le Madhya Pradesh et au sud-ouest par le Gujarat.

Le Rajasthan a été formé le , quand les anciens États princiers du Rajputana se sont fondus pour la création de l'Inde.

Le Rajasthan, est le fruit de multiples traditions historiques dont principalement celle des Rajputs mais aussi des Naths, des Jats, des Bhils, des Ahirs, des Gujars et des Meenas. Les premiers faits historiques au Rajasthan datent d'il y a plus de 5000 ans.

Le Rajasthan actuel n’existait pas en tant qu’entité géopolitique et était gouverné par les divers empires et grands royaumes d’Inde tels que l’Empire Maurya, les royaumes des Malavas, Arjunyas, Yaudhyas, l’Empire Kouchan, les royaumes satrapes (Scythes), l’Empire Gupta et les Huns.

Le Rajasthan n’émerge qu’à partir de 700 lorsque les clans Rajputs commencent à monter en puissance et dominer la région. C’est entre le et le que les Rajputs connaissant leur apogée et que les rivalités entre les divers clans sont les plus vives.

La dynastie des Pratihâra est la première à former entre le et l’an 1000 un royaume vaste et puissant allant du Gujarat jusqu’à la plaine gangétique. Malgré leurs victoires sur leurs grands voisins que sont l’Empire Pala (Bengale et Bihar) et l’Empire Rashtrakuta (Deccan), une sorte de rivalité se crée entre les Pratihâra et les deux autres dynasties qui tentent également de contrôler le Nord de l’Inde. À la fin du , les Pratihâra s’affaiblissent face à l’invasion islamique à l’Ouest (Sindh et Penjab) et les Pala à l’Est. La première grande dynastie rajput fini par perdre petit à petit les divers territoires gouvernés (dont le Rajasthan) et n’est plus qu’à la tête de la région de Kannauj (Uttar Pradesh), qui finira par être attaquée et envahie par les hordes arabes.

Le Rajasthan devient rapidement après le départ des Pratihâra le fruit de dispute et de rivalité entre d’autres clans rajput tels que les Chalukyas, les Parmars et les Chauhans, qui luttent pour la prise du pouvoir sur l’ensemble de la région jusqu’au .

Vers l'an 1200, les envahisseurs sarrasins réussirent à pénétrer dans le Rajasthan et à affaiblir et dominer la plupart des clans rajputs. Seul le royaume de Mewar (région d'Udaipur), le plus puissant et le plus important des États rajputs ne fut pas vaincu et continua à résister aux nombreux assauts des troupes musulmanes. Les envahisseurs occupèrent la région durant un certain temps depuis leurs bastions de Nagaur et de Ajmer. Au fil du temps, le Rajasthan redevient divisé entre de nombreux royaumes.

L'arrivée des Afghans et la formation de l'Empire Moghol raniment la violence au Rajasthan (particulièrement dans la région de Chittorgarh) et dans le Nord de l'Inde en général, mais s'estompent rapidement du fait des nombreuses alliances entre les principautés rajputs et le nouvel empire musulman.

Le Rajasthan n'a jamais pu être unifié en une entité, mais l'empereur moghol Akbar, par souci d'administration décide de créer une réunion des différents vassaux rajputs en une province divisée en plusieurs États appelée Rajputana. À partir de 1707, le pouvoir moghol déclinant, le Rajputana doit faire face aux invasions des Marathes, ennemis du pouvoir musulman. Les Marathes gagnent rapidement avec grand succès leurs batailles contre les roitelets et les princes rajputs, ils occupent Ajmer en 1755 et réussissent à atteindre les portes du Penjab. Les invasions marathes sont suivies au par les incursions des Pindarîs, des cavaliers mercenaires affiliés aux nobles marathes et célèbres pour leur mode de vie basé sur les ressources du pillage.

C’est en 1818 que la Compagnie des Indes Orientales pose pied dans la région et s’allie avec les souverains locaux. Les nombreuses principautés rajput deviennent vassales de la Couronne Britannique et tel qu’à l’époque des Moghols sont réunis sous la province collective du Rajputana. En dépit des bonnes relations qu’ont les rajputs avec les Anglais, le rapport entre les Jats du Nord-Est du Rajasthan (Bharatpur et Dholpur) et les colonisateurs est très violent. En effet, les Jats luttent pour leur indépendance et ne veulent pas être subordonnés, ce qui va créer de nombreux conflits dans la région.

Le Rajasthan couvre une surface de , c'est le plus grand État indien.

Les villes importantes sont : Jaipur, Kota, Udaipur, Ajmer, Jodhpur, Bikaner, Jaisalmer.

Le Rajasthan est formé de deux parties très distinctes :

Le Rajasthan est un État plutôt sec qui peut connaître plusieurs années sans précipitation. L'eau est issue en grande partie des pluies de la mousson d'été (de juin à septembre), ainsi que des nombreux cours d'eau prenant source dans les Ârâvalli.

Au sud d'Udaipur, dans la pointe comprise entre le Gujarat et le Madhya Pradesh, se trouve une région ancienne nommée Vagad, ancien fief aujourd'hui gravement déforesté de la tribu des Bhīls.

Le Rajasthan est principalement une zone de climat désertique (BWh) d’après la classification de Köppen, tandis que les parties orientales de l’État subissent un climat semi-aride (BSh). La température moyenne se situe entre et dans la plupart de la région, seuls les lieux les plus septentrionaux ont une température moyenne différente ( à ). L’année est divisée en 4 saisons influencées par le régime des moussons.

L’hiver débute vers le mois de décembre et prend fin vers le mois de février, cette saison est la plus fraîche de l’année et est marquée par un temps ensoleillé toute la journée et brumeux à l’aurore. Les températures en hiver sont entre à en journée tandis qu’il fait entre et la nuit.

Le printemps commence au mois de mars et s’achève au mois de mai, cette saison est remarquable par la montée fulgurante des températures en quelques mois. En effet, il fait entre à au début du printemps, alors que la température en fin de saison est de l’ordre de à g.

L’été est une saison totalement différente de la période estivale connue dans les zones tempérées, les nuages de moussons se créent suite à l’étouffant mois de mai et arrosent le Rajasthan durant 3 mois, soit de juin à août. La température saisonnière est de à .

L'automne démarre en septembre et se termine au mois de novembre. Cette saison est marquée par une baisse des températures, par une diminution des heures d’ensoleillement et par le retour d'un temps ensoleillé permanent. La température de saison est de l'ordre de à .

L'État du Rajasthan n'est que peu marqué par son relief, en effet seuls la chaîne montagneuse de l'Aravalli et quelques massifs isolés diffèrent du paysage de la région. Cependant, l'influence qu'ont ces chaînes montagneuses sur le climat de l'État est considérable. Le point culminant du Rajasthan est le Guru Shikhar, qui culmine à plus de mètres au-dessus de Mont Abu et de la région d'Udaipur.

L'assemblée législative du Rajasthan, Vindhan Sabha, compte 200 membres élus au suffrage universel pour un mandat de 5 ans. Le Ministre en chef (Chief Minister), issu du parti ou de la coalition majoritaire, est nommé par le gouverneur de l'État. 
Le Rajasthan est divisé en 33 districts regroupés en 7 divisions territoriales qui sont :

L'économie du Rajasthan est principalement liée à l'agriculture. L'orge et le blé y sont cultivés sur de larges surfaces, de même que la canne à sucre, et les graines oléagineuses. Le coton et le tabac y sont récoltés, c'est aussi le premier État en termes de production de laine.

La région située à l’ouest des monts des Ârâvalli, au climat beaucoup plus sec, est consacrée à l’élevage. Il existe dans cet État des filatures de coton et des usines de ciment, mais l’artisanat reste le principal secteur industriel.

L'industrialisation du Rajasthan a commencé dans les années 1960. Les principales industries se trouvent dans le textile, les mines (ciment, zinc, marbre etc.), l'agriculture et la saliculture. Il est aussi un important producteur de fibres polyester.

L'industrie pétrolière, malgré un développement lent, est un secteur économique prometteur. Le Rajasthan est doté de trois grands bassins pétroliers localisé dans l'ouest de l'État. Actuellement, l'extraction se fait principalement dans le district de Barmer. Plus de ont été produits en 2013.

D'après le recensement de 2011, le Rajasthan compte environ 90,5 % d'hindous, 9,05 % de musulmans, 1,27 % de sikhs et 0,90 % de jaïns.

Sur les quelque villages que compte le Rajasthan, ont été frappés par la sécheresse en 1988. L'armée organisait la survie des populations dans les zones sinistrées en dirigeant notamment le déblayage des routes ensablées par les vents.

L’hindi est la langue officielle de l’État, cependant cette langue n’est pas native de la région. Les langues régionales sont très couramment parlées par les Rajasthanis, les principales langues sont le rajasthani et le marwari, ainsi que d'autres langues moins connues tel que le bagri, le dhundari, le mewari et le mewati.

Récemment, le gouvernement fédéral a exprimé son souhait de vouloir faire du rajasthani la langue officielle de l’État. Malgré le soutien des habitants du Rajasthan, l’officialisation du rajasthani en tant que langue officielle est empêchée par les autorités qui voient en cette officialisation une porte ouverte aux autres langues régionales et au déclin de l'hindi, langue censée unifier linguistiquement les Indiens.

Le Rajasthan est un État culturellement très riche dont les traditions reflètent le mode de vie de l'Inde ancienne. En particulier, il est très connu pour ses forts majestueux, son art très coloré dont les Kathputli.
La peinture Mandana ("mandan" = décoration) s’exerce au Rajasthan et dans le Nord de Madhya Pradesh. Les femmes peignent les murs (le Bhitti Chitra) et les sols (le Bhumi Chitra) de leur maisons avec des images géométriques pour protéger la maison, pour les fêtes rituelles et surtout pour des raisons décoratives. 

Si la peinture Mandana a un vocabulaire ornemental géométrique et stylisé, les peintures Thapa peints sur des murs extérieurs sont en revanche des compositions très libres mettant en scène le monde animal et floral et l’univers villageois.

Le Rajasthan est la principale région touristique de renommée internationale du pays. D’après le Ministère du Tourisme, plus de 1,437,162 visiteurs se sont rendus au Rajasthan en 2013, faisant de cet État le le plus visité du pays. Ce secteur économique forme une part importante du gagne-pain des habitants et de la région en soi. La notoriété de cette partie de l’Inde est due à son riche héritage culturel et historique, mais surtout à la beauté de son patrimoine comparable aux Mille et Une Nuits.

Malgré les récents événements en Inde du Nord (viols de touristes où de ressortissantes étrangères), le Rajasthan est une destination sûre et peu affectée par ce phénomène épidémique. En effet, seuls quelques très rares cas ont été recensés dans la région (une touriste allemande violée à Alwar et une étudiante japonaise violée à Jaipur) et le gouvernement du Rajasthan prononce rapidement les sentences contre les violeurs et punit sévèrement les coupables.

Cependant, il est recommandé aux touristes féminins de se vêtir décemment, voire en respectant les contraintes vestimentaires locales. Pour tout cas d’attouchement ou autres dans l’espace public, il est vivement conseillé de donner l’alerte aux passants qui n’hésiteront pas à utiliser la violence pour venir en aide. Les agresseurs quittent souvent les lieux lorsqu’ils se sentent menacés par la population.

Principaux sites touristiques : Jaipur, Jaisalmer, Jodhpur, Udaipur, Pushkar, Mont Âbû, Ajmer, Ranakpur, Shekhawati, Bikaner, Sawai Madhopur, Bharatpur, Bundi, Chittorgarh, Kumbhalgarh, Deeg.


</doc>
<doc id="14951" url="https://fr.wikipedia.org/wiki?curid=14951" title="Transcription et translittération">
Transcription et translittération

La translittération est l'opération qui consiste à substituer à chaque graphème d'un système d'écriture un graphème ou un groupe de graphèmes d'un autre système, indépendamment de la prononciation. Elle dépend donc du système d'écriture cible, mais pas de la langue.

La transcription est l'opération qui consiste à substituer à chaque phonème (on parle alors de transcription phonologique) ou à chaque son (transcription phonétique) d'une langue un graphème ou un groupe de graphèmes d'un système d'écriture. Elle dépend donc de la langue cible, un unique phonème pouvant correspondre à différents graphèmes suivant la langue considérée.

Par exemple, la lettre russe «  » se translittère « č » (ce qui n'éclaire pas forcément le francophone sur sa prononciation), mais se transcrit « tch » en français et « ch » en anglais (ce qui correspond bien au même son, malgré deux écritures différentes).

La translittération est l'opération qui consiste à substituer à chaque graphème d'un système d'écriture un graphème ou un groupe de graphèmes d'un autre système, indépendamment de la prononciation. Autrement dit, c'est l'écriture d'un mot ou d'un texte écrit avec un système, dans un autre système d'écriture. La translittération vise à être sans perte, de sorte qu'il devrait idéalement toujours être possible, en connaissant les règles de translittération, de reconstituer le texte original à partir de la translittération. Les deux systèmes d'écriture devraient donc être équipotents : une translittération ne peut être ambiguë et devrait être bijective. Pour atteindre cet objectif, les systèmes de translittération définissent souvent des conventions complexes pour traiter les graphèmes de l'écriture d'origine qui n'ont pas de correspondance évidente dans l'écriture d'arrivée.

La translittération s'oppose en cela à la transcription, qui substitue à chaque phonème (on parle alors de transcription phonologique) ou à chaque son (transcription phonétique) d'une langue un graphème ou un groupe de graphèmes d'un système d'écriture. Plus simplement, c'est l'écriture dans un système donné de mots ou phrases prononcés. La transcription vise également à être sans perte, de sorte qu'il devrait idéalement toujours être possible, en connaissant les règles de transcription, de reconstituer la prononciation originale à partir de la transcription. Pour les langues dont l'orthographe est phonétique ou quasi phonétique (par exemple l'espagnol ou le vietnamien), on peut donc considérer que l'écriture habituelle est une transcription.

La frontière entre translittération et transcription s'efface cependant quand un système de translittération utilise comme point de départ un système d'écriture purement phonétique. Par exemple, le "hanyu pinyin" peut être simultanément considéré comme un système de transcription de la langue chinoise et comme un système de translittération du "bopomofo". Dans la pratique, il existe aussi des systèmes qui combinent transcription et translittération, c'est-à-dire qu'ils translittèrent une partie de l'écriture d'origine et qu'ils transcrivent le reste selon la prononciation.

La romanisation est soit un système de translittération d'une écriture non latine (comme le cyrillique ou l'arabe) vers une écriture latine, soit la transcription dans une écriture latine d'une langue utilisant une écriture non latine. Certains systèmes de romanisation ont un statut de norme officielle nationale ou internationale (normes ISO).

Les transcriptions et les translittérations peuvent être utilisées dans un texte pour noter les termes issus de langues étrangères n'utilisant pas le même système d'écriture. Ainsi, les langues utilisant une écriture latine utilisent diverses méthodes de romanisation pour noter les mots russes, chinois, japonais, etc.

Dans l'apprentissage d'une langue étrangère, même quand la langue enseignée utilise le même système d'écriture que la langue d'instruction, la transcription est utile pour fixer par écrit la prononciation correcte. Par exemple, pour un francophone apprenant l'allemand, on pourrait écrire "Chteufleur" pour lui expliquer approximativement la prononciation du nom du savant "Stöffler".

Pour les langues utilisant des systèmes d'écriture logographiques (ce qui implique un nombre de graphèmes très élevé, comme en chinois), l'utilisation d'une transcription est très utile dans l'apprentissage de l'écriture par les enfants (que cette transcription soit une romanisation, comme le "hanyu pinyin" utilisé en Chine, ou non, comme le "bopomofo" utilisé à Taïwan).

La stabilité des graphies obtenues (via les standards de normalisation, par exemple ISO et GOST cités sur cette page) ainsi que la possibilité de rétroconversion (retrouver l'alphabet original) destinent la translittération aux usages nécessitant un traitement rigoureux de l'écrit.

Elle est donc privilégiée par les bibliothèques ou encore pour le traitement informatique des données textuelles. Les impératifs de saisie informatique et d'internationalisation expliquent en partie pourquoi la translittération est le plus souvent une romanisation. Par exemple, le système Palladi de notation du chinois en cyrillique (en admettant de le considérer comme une translittération alternative du bopomofo) est encore en vigueur en Russie, mais son usage reste en retrait du pinyin. Inversement, il n'existe pas de système de translittération codifié du français en arabe, en russe ou en géorgien.

Elle est également en usage dans les recherches linguistiques et philologiques visant un public plus large que les spécialistes de la langue concernée : typologie, descriptions destinées aux revues généralistes, recherches étymologiques, etc.

La transcription dépend souvent des usages de la langue du transcripteur. Pour une même langue, un francophone pourra transcrire le son [] (dans "chat") par le digramme "ch", tandis que quelqu'un de langue anglaise choisira "sh", allemande "sch" et polonaise "sz". 

Une transcription "phonétique" peut se faire au moyen de l’alphabet phonétique international. Elle vise à représenter les sons tels qu'ils sont émis.

Une autre transcription est celle dite "phonologique", qui ne tente pas de représenter pas les sons émis exactement, mais seulement les phonèmes pertinents d’une langue donnée.

Les transcriptions phonologiques les plus simplifiées (et les plus approximatives et pas toujours assez pertinentes pour différencier correctement les sons nécessaires à la langue transcrite) utilisent l’orthographe habituelle utilisée par les locuteurs dans leur langue native (pas forcément la langue transcrite elle-même). On parle plutôt alors de transcription pseudo orthographiques, ou de fausses translittérations (quand la transcription ne traduit même pas la phonologie dans des sons de la langue native du transcripteur, mais résulte souvent d’une mauvaise lecture par lui de l’orthographe de la langue transcrite, une orthographe qui peut par ailleurs être assez éloignée aussi de la phonologie réelle de cette langue que le transcripteur n’a même pas entendue prononcée ! cf. la section ci-dessous au sujet des fausses translittérations parfois considérées comme des traductions).

La norme internationale la plus connue de translittération du russe porte le numéro ISO 9. Dans sa dernière version (1995), ce système fait correspondre à chaque caractère cyrillique un caractère latin unique, ce qui rend les translittérations parfaitement réversibles sans la moindre ambiguïté.

On peut donner un exemple simple de la différence entre translittération et transcription : soit le patronyme Горбачёв ; celui-ci devra être translittéré "Gorbačëv" selon la norme ISO 9 (équivalence "un caractère unique" ≡ "un caractère unique" : à tout "č" doit correspondre un "ч" et inversement), mais sera transcrit "Gorbatchof", "Gorbachof" ou encore "Gorbatschow", selon la langue du transcripteur (équivalence phonétique approximative en tenant compte des usages de la langue cible, ici respectivement le français
, l'anglais et l'allemand). 

Le standard ISO 9-95 (et les standards associés, notamment le GOST 7.79 - 2001 russe) est appliqué dans les cas où il faut retranscrire sans ambiguïté le mot ou le texte cyrillique en caractères latins, indépendamment de la prononciation, afin de pouvoir ensuite les reconstituer sans perte lors des traitements informatiques ou dans les bibliothèques. Le standard ne vise pas à se substituer à la transcription phonétique ni à être utilisé pour translittérer les toponymes. Il prévoit notamment le respect des normes traditionnelles et esthétiques basées sur la transcription phonétique et ne s'impose pas dans les champs d'applications autres que le traitement algorithmique de texte (GOST 7.79 - 2001. Paragraphe 4).

Le standard russe GOST 7.79 - 2001 - Système « B » s'éloigne sensiblement du standard ISO 9 en adoptant les phonèmes à deux lettres au lieu des signes diacritiques.

Il existe également d'autres normes de translittération du russe vers l'alphabet latin, dont l'ALA-LC et le BGN/PCGN.

La recommandation ISO R 9 d'octobre 1955 est synthétisée dans le « Fascicule de documentation FD Z 46-00I » de janvier 1956 et publié par l'Afnor.

Cette recommandation ne s'est pas imposée dans tous les pays. Les États-Unis ne l'avaient pas approuvée. Ils ont continué à utiliser la translittération de la Bibliothèque du Congrès. 

En 1958 le Royaume-Uni a publié la norme BS 2979 avec à la fois le système britannique traditionnel pour le seul cyrillique moderne et le système international (recommandation ISO R 9).

La transcription du grec ancien ne soulève pas de grands problèmes : en effet, l'alphabet grec de l'époque est relativement peu ambigu (à un graphème correspond le plus souvent une seule interprétation phonétique) et une transcription sera très proche d'une translittération. Par exemple, « γνῶθι σεαυτόν » pourra être transcrit "gnothi seauton" et translittéré "gnỗthi seautón". La translittération fera juste intervenir les accents et les quantités vocaliques. Il est possible de retrouver l'original, même à partir d'une transcription floue.

Le grec moderne est toutefois plus difficile à traiter. En effet, sa prononciation s'est modifiée en donnant naissance à nombre de phonèmes écrits de manières différentes ainsi qu'à des valeurs phonétiques de certaines lettres très éloignées de nos habitudes. L'une des modifications les plus « gênantes » est l'iotacisme, qui a fait se prononcer [i] six graphèmes différents qui en grec ancien n'étaient pas confondus. De même, "ε" et "αι" se prononcent [ɛ] ; "ο" et "ω" valent tous deux [ɔ]. Ainsi, la translittération et la transcription seront parfois très éloignées de l'original (ce qui est l'indice d'une orthographe complexe : en effet, il n'est pas possible de noter un mot grec moderne directement à l'écoute sans en connaître la graphie).

Voici comme exemple concret le vers suivant du poète grec Odysséas Elýtis :

Une romanisation (phonétique et avec les accents) possible serait "Stin arkhéa ekíni thálassa pu egnóriza". On compte quatre [i], écrits "η", "ει" et "ι", et deux [ɛ], écrits "αι" et "ε". Si l'on veut proposer une romanisation qui permettrait de reconnaître le texte original, il est nécessaire de distinguer ces différentes graphies. On pourrait, par exemple, employer la translittération du grec ancien : "Stên arkhaía ekeínê thálassa pou egnốriza" sera très éloignée de la transcription et demandera au lecteur de connaître des règles de lecture moins intuitives.

Le problème se pose donc pour les noms propres actuels : faut-il choisir la transcription ou la translittération ? Par exemple, le nom du politicien grec "Ιωάννης Αλευράς" se translittère "Ioánnês Aleurás" mais se transcrit "Ioánnis Alevrás", voire "Ioannis Alévras" si l'on utilise l'accent aigu en suivant les conventions françaises. De même, le nom du politicien grec est "Βασίλης Κοντογιαννόπουλος" s'il est translittéré le plus prochement possible des habitudes françaises car son prénom vaudra "Basílês", qui permet de reconnaître "Basile", alors que la transcription "Vasílis" (voire "Vassilis" pour noter le premier son [s]) masque le lien avec "Basile". Quant au patronyme, il peut être surprenant de constater qu'il se translittère "Kontogiannópoulos" et se transcrit "Kondoyannópoulos" (voire "Kondoyannopoulos").

Toutes ces questions sont résolues par la normalisation : la norme internationale ISO 843:1997 "Information and Documentation. Conversion of Greek characters into Latin characters" est adaptée à toutes les écritures grecques « indépendamment de la période dans laquelle elle fut utilisée. Elle s'applique aux écritures monotonique et polytonique de toutes les périodes du grec classique ou moderne (archaïque, alexandrine, hellénistique, byzantine, katharévousa, démotique, etc.) ». Elle propose des règles de transcription et des règles de translittération.

Quant aux noms propres, la constitution d'autorités au niveau international propose, là aussi, des solutions adaptées en particulier par les bibliothèques du monde entier. En France, l'accès aux formes normalisées des noms propres est donné par la Bibliothèque nationale de France : http://catalogue.bnf.fr rubrique Autorités> Autorités BnF. La règle est de proposer la forme translittérée.

Un code officiel de translittération de l'hébreu en caractères latins a été édité en 1956 par l'académie israélienne de la langue hébraïque.

À titre d'exemple, on peut citer le nom « Josué » (en hébreu : יהושע), translittéré en "yhôšua`".

La norme ISO 259 a depuis normalisé la translittération de l'alphabet hébreu.

Dans les médias écrits francophones, il est courant de pratiquer une translittération qui consiste à représenter grossièrement un mot étranger dans le respect flou de sa graphie d'origine. Quand l'alphabet de départ est déjà latin, cette fausse translittération se fait le plus souvent en abandonnant les diacritiques et autres signes n'existant pas dans la langue d'arrivée. Le résultat est cependant lu comme une transcription mais celle-ci n’a parfois plus rien à voir avec la prononciation de départ. 

Par exemple, le patronyme du politicien Горбачёв est rendu par "Gorbatchev". Le "ё" russe, écrit normalement sans tréma (qui s'utilise surtout dans les ouvrages pédagogiques ; le patronyme Горбачёв est donc couramment écrit en cyrillique "Горбачев"), se prononce ici [o] et non [e], et le "в" final, [f]. Le rendre, en alphabet latin, par "Gorbatchev", amène à le prononcer d'une manière très différente de la prononciation originale, avec un [e] et un [v]. 

Ce procédé s'est dorénavant imposé comme un véritable procédé de traduction de tous les noms propres russes comportant -е- ou -ё- dans l'original, indépendamment de la prononciation effective : "Gorbatchev" rejoint "Khrouchtchev", mais aussi "Brejnev", le nom de ville "Орёл" [ʌr'oɫ] se traduit officiellement "Orel".

Faute de connaître la translittération spécifique du moldave, certains bibliothécaires, cartographes et traducteurs d’avant 1989 utilisaient la translittération de la langue russe, rendant incompréhensibles les textes, toponymes et anthroponymes moldaves ; après 1989 cette difficulté a en grande partie disparu, car le Soviet suprême de la Moldavie soviétique a adopté pour le « moldave » l’alphabet latin, mais perdure encore lorsque les sources utilisées sont antérieures à 1989.

Le cas est aussi très fréquent avec des alphabets latins modifiés, comme celui du polonais. Les caractères étendus absents des claviers courants sont simplement omis, sans que l'on adapte cependant l'orthographe pour qu'elle représente mieux la prononciation réelle. Ainsi, le nom du pape Jean-Paul II, "Wojtyła" (avec un "ł", prononcé [w] en polonais) est simplement écrit "Wojtyla", avec un "l" normal. Le locuteur français devrait donc prononcer cette fausse translittération [wɔʒti'la, v-] mais la prononciation la plus fréquente [vɔjti'la] est un compromis finalement assez proche de la prononciation polonaise [vɔj'tɪwa] auquel ne manque que la valeur réelle du "ł" (abstraction faite de la place de l'accent d'intensité, sur l'avant-dernière syllabe en polonais, et la valeur de [ɪ], plus proche du [e] que du [i] français).

De même "Lech Walesa", politicien polonais, écrit "Lech Wałęsa "en polonais, se prononce dans cette langue" [ˈlɛx vaˈwɛ̃ŋsa] "qu'une transcription empirique française pourrait approximativement rendre par «" Lekh Vawensa "». Inversement, l'usage polonais exige la transcription des noms étrangers traditionnels : l'écrivain anglais "Shakespeare" s'écrit donc en polonais "Szekspir", mais on garde la graphie française pour le politicien français "Chirac" au lieu de "Szirak".

Dans le milieu de l'interprétation auprès des personnes sourdes ou malentendantes, la translittération est l'opération qui consiste à reproduire le message sonore en message visible sur les lèvres pour le bénéfice des malentendants qui pratiquent la lecture labiale. L'expression « interprétation orale » est également utilisée pour désigner la translittération.




</doc>
<doc id="14956" url="https://fr.wikipedia.org/wiki?curid=14956" title="Haryana">
Haryana

L’Haryana (hindi : ) est un État du nord de l’Inde.

L’Haryana a été constitué le novembre 1966 avec une partie du Pendjab.

L'Haryana est un état enclavé du nord de l'Inde. Il est situé entre les latitudes 27°39' et 30°35' N, et les longitudes 74°28' et 77°36' E. L'altitude moyenne de l'Haryana varie entre 200 et au-dessus du niveau de la mer. La forêt couvre une surface de . La géographie physique de l'Haryana peut être divisée en quatre entités :


L’Haryana est bordé au nord-ouest par le Pendjab, au nord-est par l’Himachal Pradesh, à l’est par l’Uttar Pradesh, au sud par le Rajasthan.

La rivière Yamunâ coule le long de la frontière orientale. La rivière Sarasvatî est supposée avoir coulé à Yamunâ Nagar, mais se serait asséchée depuis longtemps.

La rivière Ghaggar est principalement une rivière saisonnière.

Le climat de l'Haryana est similaire à d'autres états de l'Inde des plaines du nord. Les étés sont chauds (jusqu'à ) et les hivers sont froids (parfois sous les ). Les mois les plus chauds sont mai et juin, et les plus froids sont décembre et janvier. Les précipitations ne sont pas uniformes, allant de la cordillère de Shivalik, la région la plus arrosée, à la chaîne de l'Aravali, la plus sèche. Environ 80 % des précipitations ont lieu pendant la mousson, de juillet à septembre, et ce qui cause parfois des inondations.

Des forêts sèches d'arbres et de buissons caducs et épineux couvrent l'ensemble du territoire. Pendant la mousson, des prairies d'herbe verte recouvrent les collines. On trouve des mûriers, des eucalyptus, des résineux, des gommiers rouges, des Seshams. Parmi la faune endémique on trouve des antilopes cervicapres, des antilopes nilgauts, des léopards, des renards, des mangoustes, des chacals et des chiens sauvages. Plus de 300 espèces d'oiseaux sont répertoriées sur le territoire.

L'Haryana est découpé en 4 divisions administratives qui sont subdivisées en 21 districts.

29 % de la population habite en zones urbaines.

Le Haryana est l'un des États où le déficit en femmes est le plus élevé (voir démographie de l'Inde).

L'Haryana a un riche héritage culturel qui remonte aux temps védiques. La colline Dhosi, l'ashram du révéré Rishi Chyawyan est un site important où le Chyawanprash a été formulé pour la première fois.

Samrat Hem Chandra Vikramaditya, le dernier empereur Hindou des Indes, aussi appelé Hemu, était originaire de Rewari dans l'Haryana. Après avoir défait les forces d'Akbar à Delhi en 1556, il se fit appeler roi « Vikramaditya », faisant référence aux Védas.

La race bovine Hariana provient de cette région. Elle est aussi élevée au Bangladesh.



</doc>
<doc id="14957" url="https://fr.wikipedia.org/wiki?curid=14957" title="Tamil Nadu">
Tamil Nadu

Le Tamil Nadu () est un État d'Inde du Sud. Il compte environ 72 millions d'habitants pour un peu plus de . La densité moyenne est forte, mais la croissance démographique est inférieure à la moyenne indienne. Le Tamil Nadu est plus riche et plus urbanisé que la moyenne nationale. La capitale de l'État est Chennai (autrefois appelée Madras).

Le Tamil Nadu a été créé selon des critères linguistiques en 1956 : il correspond à peu près aux régions d'Inde où l'on parle tamoul. D'abord appelé « État de Madras », il a pris son nom actuel, qui signifie , en 1960

Le territoire semble occupé par l'homme depuis environ trois cent quatre vingt cinq mille ans. Les populations dravidiennes sont peut-être arrivées vers 1500 , poussées par l'avancée des peuples Indo-Européens (cependant rien n'est certain à ce sujet).

Lors de l'indépendance de l'Inde, en 1947, la présidence de Madras devint l'État de Madras. Il comprenait le Tamil Nadu, ainsi qu'une partie du Karnataka et de l'Andhra Pradesh. Les frontières actuelles datent de 1956.

La population du Tamil Nadu a été sévèrement touchée par le tsunami de décembre 2004 avec plus de trois mille morts.

Le Tamil Nadu est bordé au nord par l'Andhra Pradesh, au nord-ouest par le Karnataka, à l'est par le golfe du Bengale, l'ouest par le Kérala et au sud il a une frontière maritime avec le Sri Lanka.

On peut distinguer deux régions au Tamil Nadu :

Les villes principales sont :

Le Tamil Nadu est divisé en trente-deux districts.

Depuis les élections de 1967 et la défaite du Congrès face au Dravida Munnetra Kazhagam, le paysage politique du Tamil Nadu est dominé par les partis politiques locaux, dits dravidiens. Le Dravida Munnetra Kazhagam (DMK) et le All India Anna Dravida Munnetra Kazhagam (AIADMK) sont les deux partis les plus importants et dirigent chacun une coalition.

En 1986, le Tamil Nadu a supprimé son Conseil législatif au profit d'une législature monocamérale, comme la plupart des États indiens. Une tentative de recréation du Conseil a été stoppée en 2011 par la Cour suprême.

Lors des élections de 2011, la coalition dirigée par le DMK (DMK+, allié au Congrès) a été défaite par l'alliance autour de l'AIADMK sous la houlette de Jayalalithaa.

Avec habitants en 2011, le Tamil Nadu est plus peuplé que la France et se classe au septième rang des États indiens les plus peuplés.


Le Tamil Nadu est le premier État indien producteur de jasmin, avec une production annuelle de sur une surface cultivée de . Les fleurs produites sont exportées dans les pays voisins, au Sri Lanka, à Singapour, en Malaisie et dans les pays du Moyen-Orient.

Principaux sites touristiques : Chennai, Kanchipuram, Mahabalipuram, Tiruvannamalai, Yercaud, Coimbatore, Yelagiri, Vellore, Gingee, Kumbakonam, Thanjavur, Gangaikondacholapuram, Tiruchirappalli, Shrirangam, Chettinad, Karaikudi, Rameshwaram, Kanyakumari, Tirunelveli, Tenkasi, Madurai, Dindigul, Kodaikanal, Ooty.

Un dense réseau de routes dessert les villes, ports de l’État. 25 autoroutes nationales (), d'autoroutes d’État et est desservie par le Quadrilatère d'or. Le réseau est entretenu par le Département des routes et autoroutes (Tamil Nadu).

L’État est desservi par la Zone 7 de l'Indian Railways.

L'Aéroport international de Chennai, l'Aéroport international de Tiruchirappalli, l'Aéroport international de Coimbatore et l'Aéroport de Madurai sont les portes d'entrée pour le Tamil Nadu.

Les ports principaux se trouvent à Chennai, Ennore et Tuticorin.

55 % de l’énergie d'origine éolienne produite en Inde l'est au Tamil Nadu.





</doc>
<doc id="14958" url="https://fr.wikipedia.org/wiki?curid=14958" title="Kerala">
Kerala

Le Kerala ou Kérala (malayalam ) est un État indien. La langue principale est le malayalam qui fait partie des langues dravidiennes, famille linguistique dominante en Inde du Sud.

Le Kerala, densément peuplé, s'étend sur près de dans le sud-sud-ouest de la péninsule indienne. Il est couvert de denses forêts sur les contreforts des Ghats occidentaux et traversé d'un réseau de lagunes et canaux le long de la côte de la mer des Laquedives. Il est parfois surnommé « le pays de Dieu » ("God's own land").

Le Kerala possède un indicateur de développement humain élevé par rapport à son niveau de développement économique. L'espérance de vie et le taux d'alphabétisation sont très au-dessus de la moyenne nationale. De nombreuses personnes originaires du Kerala ont émigré à l'étranger, en grande partie dans les pays du Golfe. Les fonds envoyés par ces personnes équivalent à plus de 20 % du produit intérieur brut.

Dès le , grecs et romains faisaient déjà du commerce avec le Kerala qui se trouvait alors à la convergence des mondes occidental et extrême-oriental. Ses productions comme la noix de coco, la noix d'arec, et les épices s’échangeaient contre du riz, du sucre, des perles, des pierres précieuses ou des cotonnades. 

Des juifs issus de la première diaspora fondent la première communauté juive d'Inde.

Selon la tradition l'apôtre Thomas diffuse le "nazaréisme" (la forme juive du mouvement créé par Jésus) à partir du milieu du de notre ère.

Les premiers chrétiens nestoriens, issus des missions venues des communautés chrétiennes d'Orient (Byzance, Assyriens) s'y installent ou diffusent le christianisme dès le , en particulier sur la côte.

Au début du Moyen Âge, les relations commerciales sont alors dominées par les marchands perses et arabes qui se sont substitués aux gréco-romains. À ce moment-là, le Kerala est divisé entre de nombreux royaumes, mais c'est au que l'identité régionale se forme, lorsque le malayalam se différencie nettement du tamoul.

L'une des raisons du voyage de Vasco de Gama depuis le Portugal vers le Kerala, en 1498, est de briser le contrôle musulman sur le commerce des épices entre les producteurs locaux et le Moyen-Orient. Il fait construire la première forteresse portugaise, Fort Emmanuel, en terre indienne à Cochin ("Kochi") en 1503, puis, prenant avantage de la rivalité entre les rajahs de Calicut et de Cochin, entreprend de détruire ce monopole.

Cette lutte entre Calicut et Cochin, permet finalement aux Hollandais d'intervenir, puis d'expulser les Portugais de leurs forts. Les Anglais s'implantent dans la zone par l'intermédiaire de la Compagnie anglaise des Indes orientales et s'installent fermement au Kerala au début du .

En 1792, Tipû Sâhib essaie de regagner du territoire sur celui tenu par les Britanniques, mais sans succès.

L'État moderne du Kerala est créé officiellement, le , à partir du Malabar, une partie de la Présidence de Madras, du Travancore et de "Cochin". Les maharajahs de ces deux derniers états princiers ont eu la particularité de se préoccuper de l'éducation et du bien-être de leurs sujets.

Le Kerala consiste en une étroite bande de terre le long de la côte sud ouest de l'Inde dont la largeur varie de 35 à .
Il est bordé par la mer des Laquedives à l'ouest et par les Ghats occidentaux à l'est, culminant à l'Anamudi avec .
Situé entre 18' et 48' de latitude nord et 52' et 22' de longitude est, sa modeste superficie, , représente 1,18 % du territoire indien.

Les États du Karnataka au nord et du Tamil Nadu à l'est sont les voisins immédiats du Kerala.
Le district de Mahé qui fait partie du territoire de Pondichéry, est enclavé dans le Kerala.

Le Kerala est divisé en trois zones distinctes :

Le climat du Kerala est tropical avec un régime de mousson du sud-ouest de juin à septembre. Les précipitations sont abondantes, en moyenne (Inde : ) et il pleut entre 120 et 140 jours par an. En été, d'avril à juin, les températures atteignent , ce qui est relativement raisonnable en Inde.

Certaines zones du Kerala sont exposées à une radioactivité naturelle intense due à la présence de monazite dans le sol.

La variété géographique et le climat expliquent l'importance de la biodiversité.

La forêt couvre 26 % de l’État, on y rencontre des essences recherchées telles que le teck, le bois de rose ou le santal, ainsi que de nombreuses herbes aromatiques.

Ces forêts et plantes donnent lieu à une activité économique appréciable : exportation de bois, artisanat, encens, huiles essentielles.

La faune est également très riche : léopards, tigres, éléphants, écureuils, singes, cerfs, ours et de nombreuses variétés d'oiseaux. Pour protéger cette richesse, nombre de ces plantes et animaux étant endémiques au Kerala, l’État a créé plusieurs parcs nationaux et réserves naturelles : Chinnar Wildlife Santuary, Eravikulan National Park, Jumarlom Bird Santuary, Periyar National Park, Silent Valley National Park.

L'économie du Kerala est principalement agricole, ce secteur emploie 17 % de la population active. La culture de la noix de coco est très développée sur la côte et dans les backwaters, les fibres permettent aussi de construire de très nombreuses embarcations de transport, commerce et destinées aux touristes.

Le thé et le café sont les principales cultures des gaths occidentaux, notamment autour de Munnar. D'autres produits agricoles sont cultivés de manière intensive comme le caoutchouc (91 % de la production nationale), l'anacardier et les épices, de tous temps la richesse de la région.

Les épices généralement cultivées au Kerala sont le poivre (96 % de la production nationale), la cardamome, la vanille, la cannelle et la noix de muscade.

Le poivre a longtemps été la principale ressource du Kerala, exporté vers le Proche-Orient et l'Europe par bateau

La pêche en mer ou dans les Backwaters, et ses industries de transformation, sont aussi des activités importantes (crevettes, palourdes, homards et huîtres).

L'élevage familial et les cultures potagères contribuent de façon substantielle aux revenus des foyers modestes.
Les ressources minières kéralaises, bien que n'employant que 0,1 % de la population active, représentent plus de 10 % du PIB du Kérala. Les minerais extraits sont le monazite pour l'industrie nucléaire, l'ilménite, le rutile, le zircon, la bauxite et le kaolin.

Les nombreuses rivières descendant des Ghats ont permis la construction de barrages pour l'irrigation et la production d'énergie hydraulique. Cependant, les nuisances écologiques et humaines causées par l'inondation des vallées ont mobilisé l'opinion et les autorités sont en butte à une forte opposition comme celle qu'a suscité la construction du barrage Athirapally sur la Chalakkudy.

Le commerce, le bâtiment, les transports, l'industrie textile et le tourisme sont les autres secteurs importants de l'économie kéralaise.

Le niveau de vie au Kerala est relativement élevé. Le revenu par habitant est passé de Rs en 1992 à Rs en 2000 et à plus de Rs en 2012.

Aujourd'hui, 80 % des foyers sont reliés au réseau électrique et 78 % ont l'eau courante. Les Kéralais en recherche d'emploi sont moins nombreux que dans le reste de l'Inde, et bénéficient d'une économie agricole de proximité, avec beaucoup de jardins familiaux. Par ailleurs, profitant des ports et aéroports tournés vers la mer d'Arabie, de nombreux Kéralais travaillent à l'étranger, surtout dans les états du Golfe (plus d'un million dans les pays du Golfe) et en Angleterre. 
Ces émigrés contribuent largement à l'économie kéralaise, leurs transferts d'argent envoyés aux familles dépassent 20 % des revenus du Kérala.

Trois aéroports internationaux desservent le Kerala :

Le Kerala, politiquement l'un des États les plus stables de l'Inde, a la particularité d'avoir élu démocratiquement, en 1957, un gouvernement communiste. Les communistes sont à nouveau élus en mai 2016 : le Kerala est alors, . Le parti communiste .

Les citoyens sont très impliqués dans la vie politique et leur participation est bien plus importante que dans le reste du pays.

La vie politique est actuellement dominée par deux coalitions : le "Left Democratic Front" (LDF) conduit par le Parti communiste d'Inde (marxiste) (PCI(M)) et l"United Democratic Front" (UDF) conduit par le Congrès. L'actuel ministre en chef est Ooman Chandy de l'UDF, élu en 2011.

Le Kerala est composé de 14 districts.

Avec 33,4 millions d'habitants pour en 2011, le Kerala a une densité de population de 860 habitants au km.
C'est l'une des plus élevées du pays (324 hab/km).
La concentration de population la plus forte se rencontre dans la plaine côtière où la densité diffuse de l'habitat crée une continuité ville-campagne.
Les villes principales sont : Thiruvananthapuram ( habitants), capitale de l'État (son nom est souvent utilisé sous sa forme coloniale de Trivandrum), Kozhikode ( habitants) et Kochi ( habitants), abritant la capitale économique et le principal aéroport.

Le taux de natalité, , est un des plus bas de l'Inde (). Le taux de mortalité global, ainsi que la mortalité infantile, , sont également faibles. Ces chiffres ont permis au Kerala de limiter sa croissance démographique à 9,4 % sur la période 1991-2001, alors qu'elle est de 21,3 % pour l'ensemble du pays.

Le Kerala est le seul État indien dans lequel le sex ratio, , est favorable aux femmes. 

L'espérance de vie, 74 ans en moyenne, y est également l'une des plus élevées de l'Inde et de 12 années supérieure à la moyenne (62 ans).

Le Kerala bénéficie du meilleur indice de développement humain des États indiens.

Il est le seul État indien où les femmes sont plus nombreuses que les hommes (104 femmes pour 100 hommes).

Cet État a le plus fort taux d'alphabétisation en Inde avec 94 % en 2011 contre 64 % en moyenne pour l'ensemble du pays selon le recensement de 2011. Ce taux d'alphabétisation est de 92 % les femmes et 96 % chez les hommes.

D'après le recensement de 2001, le Kerala compte 56 % d'hindous, 24 % de musulmans et 19 % de chrétiens.

D'après le recensement de 2011, 54,73 % des résidents du Kerala étaient hindous, 26,56 % musulmans, 18,38 % chrétiens, et 0,32 % soit n'ont pas d'affiliation religieuse soit est d'une autre religion.

Cependant, la création en 1990 de l"Islamic Sevak Sangh", dissout en 1992, mais qui réapparaît sous la forme d'un parti politique fondamentaliste, le "People's Democratic Party", bien qu'ayant une audience très limitée, traduit des tensions communautaires.

Cette communauté chrétienne date de l'arrivée en 52 de saint Thomas qui y fonde l'une des premières Églises de la chrétienté ; puis elle est augmentée de chrétiens syriaques venus de Bagdad en 192. Cette communauté chrétienne parle le malayalam et ses membres s'appellent eux-mêmes les Nazaréens. Ils sont encore très influencés par le judaïsme et ont des relations avec les Nestoriens de l'île de Socotra.
Cependant ce n'est qu'avec l'arrivée des colons portugais et de leurs missionnaires que la communauté chrétienne a pris de l'importance grâce au nombreuses conversions vers le catholicisme. Aujourd'hui, les trois quarts des Kéralais chrétiens sont catholiques.

Contrairement à ce qui s'est passé en Inde du Nord, l'arrivée au Kerala de populations musulmanes ne s'est pas faite par des conquêtes militaires, mais par l'apport progressif de commerçants. En effet, dès le des marchands musulmans installent des comptoirs sur la côte kéralaise, s'y établissent et se marient à des femmes dravidiennes.

Le Kerala se distinguait par la présence d'une communauté importante d'israélites (Juifs de Cochin) la plus nombreuse qui soit recensée en Inde (voir : "Histoire des Juifs en Inde"). Celle-ci, qui comptait membres en 1945, se réduit désormais à une vingtaine d'individus à la suite d'une émigration massive vers Israël.

La culture Kéralaise, création riche et originale, tire ses principales influences du Tamil Nadu et du Karnataka voisins ainsi que d'apports extérieurs (musulman et occidental).

La musique du Kerala est très riche et variée, allant de la musique savante, à savoir la musique carnatique, en passant par divers genres, tel que le style folklorique local et les musiques aux influences étrangères amenées par les communautés extérieures (musulmanes et chrétiennes) et les divers nations qui commercèrent avec la région.

Le kutiyattam, l'un des théâtres sacrés les plus anciens de l'Inde, est né au Kerala. Le kathakali, théâtre dansé, a pris sa forme actuelle au . Les représentations peuvent durer toute la nuit et sont très populaires. C'est un art qui requiert une grande concentration et une extrême précision et auquel les artistes se préparent en pratiquant le kalarippayatt, art martial datant du . Le Krishnanattam, semblable au kathakali, est, quant à lui, totalement dédié à l'avatar Krishna et au temple de Guruvayur, il naquît au sous le patronage des Samuthiris (Seigneurs) de Calicut.

Le Mohiniyattam, signifiant "Danse de l'Enchantresse", est une danse classique emblématique du Kerala dont la pratique est réservée aux femmes. La performance de cet art est directement lié aux récits mythologiques et en particulier au Seigneur Vishnou, connu pour prendre sa forme féminine "Mohini" afin de combattre les forces du mal.

Certaines communautés sont particulièrement liées aux arts, notamment les Chakyars, les Nambiars et les Iyers venus du Tamil Nadu. 

Le théâtre d'ombres, ou "Tolpava Koothu", demeure au Kerala dans la plus pure tradition. Inconnu au nord de l'Inde, cet art a subi dans le sud de multiples avatars : la taille des figurines varie de au Maharashtra à près de en Andhra Pradesh. Seuls le Kerala et l'Orissa ont conservé des silhouettes opaques : ailleurs, elles laissent passer la lumière et la colorent comme des vitraux. Le lien avec le Tamil Nadu est très visible, car les montreurs du Kerala émaillent leurs récits de propos en langue tamoule tirés du "Iramavataram", une traduction du Ramayana faite par le célèbre auteur Kamban et qui constitue un des chefs-d’œuvre de la littérature antique tamoule. S'inscrivant à l'origine dans une pratique spirituelle liée à la mythologie, le spectacle est organisé par les temples de la déesse Bhagavati, principalement dans le district de Palakkad qui est le centre majeur de cet art, mais également dans les régions voisines de Thrissur et Malappuram.

De nombreuses fêtes rythment l'année. Onam, la fête des moissons, est célébrée dans tout le Kerala en l'honneur de Mahabali, souverain que l'on honore en organisant des festins, en portant des vêtements neufs et en décorant le seuil des maisons de fresques florales colorées et de tapis de fleurs.

Le "pooram" est une fête hindoue dont la plus renommée est celle de Thrissur. La parade d'éléphants caparaçonnés et l'orchestre de plus de cent percussionnistes attirent de nombreux dévots et curieux.

La Peinture murale est un art dans lequel le Kerala excelle, c'est une des dernières régions d'Inde à préserver des styles picturales remontants à l'Inde antique et n'ayant pas subis d'influences persanes. Illustrant des scènes mythologiques, on retrouve ces peintures principalement dans les temples et les palais, mais également dans certaines anciennes églises. Aujourd'hui, les artistes proposent leurs œuvres sur de nouveaux supports afin d'en vivre. 

Si les premiers textes en malayalam datent du , c'est au que , le père de la littérature malayalam moderne, écrivit son œuvre. Aux XVIIe et XVIIIe siècles, le répertoire versifié des pièces du théâtre Kathakali, écrit en malayalam, sanskrit et manipravalam (malayalam littéraire encore mâtiné de tamoul), constitue un style dans lequel se distingue le "Nala Caritam" d'Unnayi Variyar ("Jours d'Amour et d'épreuve, l'histoire de Nala", Gallimard "Connaissance de l'Orient", 1995 (tr. du malayalam par Dominique Vitalyos). Le est une période de renaissance avec les écrivains , auteur du "premier roman malayalam" "Indulekha" et .

Actuellement la littérature kéralaise s'écrit en malayalam et en anglais. Parmi les nombreux auteurs, les œuvres de ceux-ci sont accessibles en français :
Journaliste, diplomate à l'ONU (1978-2007) et secrétaire d’État aux Affaires étrangères (mai 2009-avril 2010), écrit en anglais :
Après avoir travaillé dans la publicité, elle se consacre entièrement à l'écriture en anglais :
Militante pacifiste et écologiste dont le seul roman, écrit en anglais, a reçu le Booker Prize en Grande-Bretagne, l'équivalent du Prix Goncourt.
Romancier et nouvelliste, écrivait en malayalam. Un des écrivains les plus lus et les plus aimés de sa génération.
Poète et romancière, écrivait en anglais et en malayalam.
Romancier, nouvelliste et caricaturiste de presse, écrivait en malayalam. Il a parfois traduit ses propres œuvres en anglais. Son premier roman, écrit en 1969, a ouvert une voie nouvelle aux écrivains malayali(s) et connu un immense succès au Kerala :
Né à Mahé, écrit en malayalam :

Le premier film kéralais est réalisé en 1928. De même que le cinéma bengali, la production kéralaise se distingue par ses préoccupations sociales et son engagement politique. Elle peut être rapprochée du néoréalisme. L'âge d'or de la "Nouvelle vague kéralaise" commence dans les années 1970 avec Adoor Gopalakrishnan dont le premier film, "Swayamvaram" (1972), permet au cinéma kéralais d'émerger sur la scène internationale. Suivent "Elipathayam" (1981), "Mukhamukham" (1984), "Anantharam" (1987) "Mathilukal" (1989)... Les films réalisés à la fin des années 1980 et au début des années 1990 sont remarquables par la qualité de la réalisation, l'attention portée au scénario, la finesse de la narration de la vie de tous les jours, la recherche dans la musique, sans exclure l'humour. "Perumthachan" (1990), d'Ajayan marque également cette période. Pendant les années 1990 on voit se développer une production à mi-chemin entre le cinéma d'auteur et le cinéma commercial avec des films tels que "Oru Vadakkan Veeragatha" (1989) et "Sargame" (1992) de T. Hariharan, "Kireedam" (1989) et "His Highness Abdullah" (1990) de Sibi Malayil et "Amaram" (1991) de Bharathan. De la fin des années 1990 à maintenant, la qualité du cinéma kéralais a décliné et la production est majoritairement destinée au divertissement. Il y a quelques notables exceptions telle "Swaham" (1994) de Shaji N. Karun, premier film kéralais à concourir au Festival de Cannes. Le cinéma kéralais a été, et reste dans une moindre mesure, un cinéma intellectuel et novateur qui forme d'excellents professionnels qui, reconnus, partent travailler pour les studios de Bombay, de Madras ou de Hollywood tels Priyadarshan (réalisateur), Santosh Sivan (directeur de la photographie et réalisateur), Sabu Cyril (décorateur) ou encore Resul Pookutty, lauréat en 2009 de l'Oscar du meilleur mixage de son pour "Slumdog Millionaire".

Depuis 1996, se déroule à Thiruvananthapuram, le "Festival international du film du Kerala". L'exigence de qualité des différentes sections, fiction, documentaire, long métrage, film court, lui ont permis d'intégrer dans son jury des personnalités du cinéma mondial telles qu'Agnieszka Holland (réalisatrice d'origine polonaise), Bertrand Tavernier (réalisateur français), Mohsen Makhmalbaf (réalisateur iranien) ou Alain Jalladeau (directeur du Festival des trois continents de Nantes).

La variété des paysages, le climat, la richesse du patrimoine et la beauté de certains sites font du Kerala une destination touristique recherchée.

La longue côte kéralaise abrite de nombreuses et très belles plages parmi lesquelles on peut citer Kovalam à au sud de Thiruvananthapuram ou Varkala. Les stations climatiques, telles Munnar ou Nelliampathi, développées par les colons britanniques qui venaient s'y réfugier pour échapper à la fournaise de l'été, sont toujours appréciées. L'abondance et la qualité de la flore et de la faune retiennent également l'attention.

Les "backwaters", la « Venise du Kerala », attirent un nombre important de touristes, auxquels sont proposés des croisières sur des bateaux aménagés, les "kettuvallams" ou "house boats". Des courses de "snakes boats" (bateaux-serpents) y sont organisées lors des fêtes d'Onam en août ou septembre : des bateaux en bois de de long dans lesquels une centaine de rameurs prennent place, s'affrontent dans un spectacle impressionnant qui attire les foules.

Le patrimoine architectural kéralais se caractérise par une utilisation importante du bois dans les bâtiments traditionnels. Le palais de Padmanabhapuram, ancienne résidence du raja de Travancore, bien que situé au Tamil Nadu, est administré par l'État du Kerala. On peut citer aussi le palais de Krishnapuram pour ses peintures murales ou le temple de Peruntirukoilappan représentatif de l'architecture religieuse.

Kochi, avec sa vieille ville pleine de charme et ses alignements de filets de pêche au carrelet d'origine chinoise, recèle de nombreux édifices remarquables dont le palais Mattancherry, la synagogue Paradesi et l'église Saint-François, la plus ancienne d'Inde.

Le tourisme médical se développe depuis quelques années.

Le tourisme a pris son essor au Kerala dans les années 1980. Actuellement c'est un secteur important de l'économie qui représente 6,3 % du PIB, a un taux de croissance de 13 % et emploie personnes. En 2005, le Kerala a accueilli près de 7 millions de touristes dont étrangers, essentiellement de novembre à mars.




</doc>
<doc id="14961" url="https://fr.wikipedia.org/wiki?curid=14961" title="Uttar Pradesh">
Uttar Pradesh

LUttar Pradesh ( littéralement  ) est un État de l'Union indienne situé dans la partie nord du pays. Il en est l'État le plus peuplé du pays, avec environ 200 millions d'habitants.

Des vestiges archéologiques, attestant de la présence de chasseurs-cueilleurs Homo sapiens à l'âge de la pierre en Uttar Pradesh,
sont datés entre et ans.
D'autres découvertes préhistoriques comprennent des artefacts paléolithiques vieux de à ans
et des campements de chasseurs-cueilleurs du Mésolithique/Microlithique à proximité de Pratapgarh, datant entre et 
Des villages abritant du bétail, des chèvres et des moutons attestent qu'une activité agricole a débuté dès - et s'est graduellement développée entre - et - , entre la Civilisation de la vallée de l'Indus et la Période védique en s’étendant jusqu'à l'âge du fer.

La royaume de Kosala, à l'époque des Mahâ-Janapadas, était situé à l'intérieur des frontières actuelles de l'Uttar Pradesh.
Selon la légende hindoue, le roi divin Rāma de l'épopée du Ramayana régnait à Ayodhya, la capitale du Kosala.
Krishna, un autre roi hindou qui joua un rôle clé dans l'épopée du Mahabharata, et qui est révéré comme la huitième réincarnation du dieu Vishnu, est réputé être né dans la ville de Mathura dans l'Uttar Pradesh.
La fin de la guerre de Kurukshetra s'est déroulée dans la zone située entre le Doāb supérieur et Delhi, dans ce qui était le des Mahâ-Janapadas, durant le règne du roi Yudhisthira du Pândava.
Le royaume correspond à la culture de la céramique noire et rouge et à la culture de la céramique peinte grise et au début de l'âge du fer dans le nord-ouest de l'Inde autour de l'an 1000 

La plupart des envahisseurs du nord de l'Inde sont passés par les plaines du Gange de ce qui est maintenant appelé l'Uttar Pradesh.
Le contrôle de cette région était vital pour le pouvoir et la stabilité de tous les empires majeurs de l'Inde, dont l'empire Maurya, l'empire Kouchan, l'Empire Gupta et l'empire Gurjâra-Pratihâra.
À la suite des invasions des Huns qui brisent l'empire Gupta, le Doāb du Gange-Yamuna voit l'essor de la dynastie Kannauj.
Pendant le règne de l'empereur Harshavardhana, l'empire Kannauj atteint son apogée.
Il s'étend de la région du Pendjab au Nord, du Gujarat à l'Ouest, au Bengale à l'Est et l'Odisha au Sud.
Il inclut alors des parties du centre de l'Inde, le nord du fleuve Narmadâ et il comprend toute la Plaine indo-gangétique.
De nombreuses communautés de diverses régions de l'Inde se disent descendre de migrants de Kannauj.
Juste après la mort de Harshavardhana, l'empire se scinde en de nombreux royaumes qui seront envahis et subiront la loi de l'empire Gurjara-Pratihara qui est en concurrence avec la Dynastie Pala pour la domination de la région.

Plus tard, à l'époque de l'Empire moghol, l'Uttar Pradesh devient le cœur du vaste empire de l'Hindoustan, nom que l'on utilise encore de nos jours pour désigner l'Inde.
Les empereurs moghols Bâbur et Humâyûn gouvernaient de Delhi.
En 1540 un Afghan, Sher Shah Suri, gouverne l'Uttar Pradesh après avoir vaincu Humâyûn.
Sher Shah et son fils Islam Shah Suri dirigent l'Uttar Pradesh de leur capitale Gwalior.
Après la mort de Islam Shah Suri, son premier ministre Hemu devient le dirigeant de l'Uttar Pradesh, du Bihar, du Madhya Pradesh, et de l'ouest du Bengale.
On lui décerne le titre de Vikramaditya lors de son couronnement" au fort de Purana Quila à Delhi et il est appelé Samrat Hem Chandra Vikramaditya.
Hemu meurt durant la Deuxième bataille de Pânipat, et l'empereur Akbar impose sa loi à l'Uttar Pradesh.
Akbar gouverne d'Āgrā et Fatehpur-Sikrī.
À son apogée, l'empire Moghol, qui couvre presque tout le sous continent indien (y compris les territoires actuels de l'Afghanistan, du Pakistan et du Bangladesh), est gouverné de Delhi, Āgrā, ou Allahabad selon les périodes.

Lors de la seconde moitié du une série de batailles donna le contrôle de l’État à la Compagnie britannique des Indes orientales. Après la victoire britannique lors de la seconde guerre anglo-marathe, Daulat Rao Sindhia signa le traité de Surij-Anjangaon cédant ainsi à l’Empire les terres situées entre le Gange et la Yamuna, la région de Delhi, ainsi que des parties du Bundelkhand et du Braj. Les royaumes d’Ajmer et de Jaipur furent également rattachés à ce que les Britanniques appelèrent les provinces du Nord-Ouest.

Le mécontentement dû à la domination britannique engendra une grande rébellion à travers la province et qui deviendra le point de départ de la révolte des Cipayes. Après que les Britanniques eurent maté la rébellion, ceux-ci tentèrent de diviser les régions révolutionnaires en redécoupant les territoires.

L'Uttar Pradesh a une superficie totale de , ce qui le classe cinquième État de l'Inde pour sa superficie. Il est bordé au nord par le Népal et l'Uttarakhand, à l’ouest par l’Himachal Pradesh, l’Haryana, le Rajasthan et Delhi, à l’est par le Bihar, au sud par le Madhya Pradesh et le Chhattisgarh.

Ses villes principales : Vārānasī, Lakhnau (la capitale), Allāhābād, Jhansi, Kanpur, Mathura, Meerut et Āgrā.

L’Uttar Pradesh est traversé par plus de 32 rivières, citons le Gange, la Yamunā, le Saraswatī, la Sarayu, la Gomti, la Ramganga, la Karnali, la Betwa qui sont les plus grandes et ont une grande importance religieuse dans l'Hindouisme.

L'Himalaya borde l'Uttar Pradesh au nord, mais les plaines qui couvrent la plus grande partie de l’État diffèrent fortement de cette région montagneuse.
La région la plus étendue, la Plaine indo-gangétique au nord comprend le Doāb entre le Gange et la Yamuna, les plaines de la Ghaghra, les plaines du Gange et la Terraï.
Au sud, se trouvent les Vindhya et la région des plateaux.
Elle est caractérisée par des strates de roches dures et une topographie variée de collines, de plaines, de vallées et de plateaux.
Le Bhabhar laisse la place à la région de Terraï couverte d'herbe des éléphants et d'épaisses forêts entrecoupées de zones marécageuses.
Les rivières paresseuses du Bhabhar s'y approfondissent, elles s'écoulent sous enchevêtrement de végétation épaisse.
Le Terraï est parallèle à la Bhabhar, le long d'une mince bande terrestre appelée Doāb. La plaine alluviale est partagée en trois sous-régions.

En 2001, trois nouveaux États ont été créés en Inde, dont l'Uttarakhand, petit État sur la bordure himalayenne, qui faisait jusque-là partie de l'Uttar Pradesh.

L'Uttar Pradesh a un climat tempéré et humide et présente trois saisons. L'hiver d'octobre à février est suivi de l'été de mars à mi-juin et par la saison de la mousson de mars à septembre.

Les climats des plaines du Gange vont du semi-aride au sub-humide.
Les valeurs moyennes des précipitations annuelles varient de dans le coin sud-ouest de l'État à dans les parties orientales et méridionales de l'État.
Phénomène principalement estival, la branche de la baie du Bengale de la est le principal pourvoyeur de pluie de la plupart des parties de l’État.
C'est la mousson du Sud-Ouest qui apporte la majorité des pluies. Même si les pluies dues à la et à la mousson du Nord-Est contribuent faiblement aux précipitations qui affectent l’État.

Les précipitations dans les plaines sont les plus fortes dans l'est et décroissent vers le nord-est. Les inondations sont un problème récurrent de l'U.P., endommageant les récoltes et causant des morts. Les districts de l'Est sont les plus vulnérables en particulier à cause du manque de drainage dû à l'obstruction faite par les routes, les voies ferrées, les canaux et les nouvelles zones résidentielles. Les cours d'eau les plus sujets à débordement sont le Gange, la Yamuna, la Gomti, la Karnali, la Rapti, la Sarda et la Ramganga. La faible capacité de drainage des rivières occidentales Sirsa, Kali et Aligarh est aussi la raison des inondations.

Les ressources naturelles de l'Utta Pradesh sont très abondantes
En 2011, les forêts s’étendent sur soit environ 6,88 % de la surface totale de l'État.
En dépit de la déforestation rapide et du braconnage des animaux sauvages, il reste une faune et une flore diversifiées dans l'Uttar Pradesh.

On trouve de nombreuses espèces d'arbres, des petits et grands mammifères, des reptiles et des insectes dans la ceinture forestière des montagnes tempérées.

On trouve des plantes médicinales naturelles et on les cultive dans des plantations.

Le bétail se nourrit dans la savane et les prairies du Terraï et des Douars.

Des arbres des zones humides à feuilles caduques poussent dans la plaine du Gange supérieur, en particulier le long de ses berges. Cette plaine accueille une grande variété de plantes et d'animaux. Le Gange et ses affluents sont l'habitat de grands et petits reptiles, d'amphibiens, de poissons d'eau douce et de crabes.
Les Fruticées comme le Babool et des animaux tels que le chinkara vivent dans les Vindhyas arides.
Dans toutes les parties des plaines, on trouve des forêts de feuillus des zones tropicales sèches.
Comme une grande partie de la lumière solaire atteint le sol, les arbustes et les graminées y sont aussi abondants
De larges bandes de ces forets ont été défrichées pour la culture.

Au sud de l’État, on trouve des forêts tropicales d'épineux, avec des arbres clairsemés principalement des babool
Ces forets sont confinées dans les zones de faible pluviosité annuelle (50–), est une température moyenne de .

L'Uttar Pradesh est connu pour la richesse de ses espèces aviaires.
Les espèces aviaires les plus communes sont : les columbidés, le paon, le gallus, la perdrix noire, le moineau domestique, les passeri, le geai bleu, les perruches, les cailles, les pycnonotidés, le canard à bosse, les martins-pêcheurs, les picidéss, les bécassines, et les psittaciformess.
Les réserves aviaires de l'État sont entre autres : le Bakhira Sanctuary, le National Chambal Sanctuary, le Chandra Prabha Sanctuary, le Hastinapur Sanctuary, le Kaimoor Sanctuary et le Okhla Sanctuary.

Parmi les autres espèces animales, notons les lézards, les cobras, le Bungarus, et le gavial.

Parmi les nombreuses variétés de poisson, les plus courantes sont le tor et la truite.

Quelques espèces animales ont disparu de l'Uttar Pradesh ces dernières années. D'autres, comme le lion des plaines du Gange et le rhinoceros de la Teraï, sont maintenant devenues espèces menacées.

De nombreuses espèces sont vulnérables au braconnage en dépit des règlements du gouvernement de l'Uttar Pradesh.

L’Uttar Pradesh est un État politiquement extrêmement sensible. Le 3 mars 2012, Akhilesh Yadav du Samajwadi Party est devenu chef de l’exécutif, remportant 224 des 403 sièges de l'État. Auparavant depuis le 13 mai 2007, son prédécesseur était Mayawati, leader du Bahujan Samaj Party (BSP). Parti des sans caste (intouchables), le BSP est principalement implanté en Uttar Pradesh. Aux élections législatives locales de 2007, ce parti avait obtenu, avec 206 élus, la majorité absolue des 403 sièges que compte l'Assemblée de l'État.
La dynastie politique des Jawaharlal Nehru-Indira Gandhi, représentée aujourd'hui par Sonia Gandhi et son fils Rahul Gandhi est implantée électoralement en Uttar Pradesh. L'ancien Premier ministre Atal Behari Vajpayee en est également issu.

Aux élections de mars 2017, le Bharatiya Janata Party obtient une large victoire en remportant plus de 300 sièges. Il retourne au pouvoir après 14 ans. Il s'agit du plus grand nombre de sièges que le parti a jamais remporté dans cet État jusqu'à présent.

L'Uttar Pradesh est divisés en 75 districts regroupés en 18 divisions territoriales:

Il existe un projet de découpage de l'Uttar Pradesh en 4 états distincts.

En décembre 1992, la destruction de la Mosquée de Bâbur à Ayodhya, provoquée par les nationalistes hindous (BJP, RSS), a causé d'importantes vagues de violences entre hindous et musulmans, faisant plus de , en majorité musulmanes, dans de nombreuses grandes villes de l'Inde, notamment à Bombay et à Delhi. S'ensuit la promulgation de l'état d'urgence en Uttar Pradesh et la destitution du gouvernement local (BJP).

Depuis 2006, il y a eu de nombreuses attaques terroristes, dont des explosions dans un lieu historique sacré pour les Hindous, une cour et un temple. Le 7 mars 2006, des bombes explosent dans la ville sainte hindoue de Varanasi faisant 20 morts et 101 blessés. Les bombes explosent simultanément juste après 18 heures.
La première bombe explose dans le temple bondé Sankat Mochan Hanuman à côté de l'Université hindoue de Bénarès.
D'autres explosions ont lieu dans la gare de Bénarès, à côté de la salle d’attente. Une autre explosion a eu lieu dans le train "Shivganga Express" à destination de Delhi.

Le 23 novembre 2007, en l'espace de 25 minutes, six séries d'explosions ont lieu dans les palais de justice de Lucknow, Varanasi, et Faizabad, tuant 28 personnes et en blessant d'autres.
Ces explosions ont lieu une semaine après que la police de l'Uttar Pradesh et les agences centrales de sécurité ont arrêté des terroristes du groupe Jaish-e-Mohammed qui avaient planifié l'enlèvement de Rahul Gandhi. Les moudjahidin indiens ont revendiqué la responsabilité de ces explosions dans un courriel envoyé aux chaines de télévision cinq minutes avant les explosions.
La première explosion s'est produite à 13h05-13h07 dans les locaux du tribunal de Varanasi. Deux explosions successives ont eu lieu autour de 13:12 et 13:15 dans au tribunal du district de Faizabad, suivies de près par celui de Lucknow à 13:32.
Les bombes ciblaient les avocats qui travaillaient au tribunal.

Le 7 décembre 2010, une bombe explose au Ghât de Sheetla Ghat, proche du Dashashwamedh Ghat, tuant 38 personnes et en blessant de nombreuses autres. L'explosion a lieu le lendemain de l’anniversaire de la démolition de Babri Masjid.

L’Uttar Pradesh, avec une population proche de 224 millions de personnes (estimations de 2017), est l’État le plus peuplé de l’Inde.
Si l'Uttar Pradesh était un État indépendant, il serait le le plus peuplé du monde.
Seuls la Chine, l'Inde elle-même, les États-Unis et l'Indonésie ont une population plus importante que l'Uttar Pradesh.

Comme peut être déduit de ce tableau, le taux d'accroissement naturel est, depuis 1981, d'environ 2,3 %.
Le taux de fécondité en 2010 est de 3,5 enfants par femme
En 2011, la population est composée de hommes et de femmes.

D'après le recensement de 2001, 80 % de la population est hindouiste alors que les musulmans représentent 18 % des habitants de l'Uttar Pradesh.
Le reste est composé de sikhs, de bouddhistes, de chrétiens et de jaïns.

Le Produit National Net de l'Uttar Pradesh en fait le troisième état de l'Inde (2011–2012), avec un PNN de milliards de .

L'évolution du Produit National Net de l'Utta Pradesh est la suivante:

L'agriculture est l'activité principale de l'Uttar Pradesh.
Le blé est la principale culture vivrière de l'État et la canne à sucre est la principale culture commerciale.
L'Uttar Pradesh fournit 70 % du sucre de l'Inde.

Les industries de l'U.P sont situées dans la région de Kanpur, les terres fertiles de la région du Purvanchal et la région de Noida.
La ville de Mughalsarai a les plus importantes usine de locomotives.
Les principaux produits manufacturés sont les produits électroniques, les équipements électriques, les câbles, l'acier, le cuir, les textiles, la joaillerie, les frégates, les automobiles, les wagons ferroviaires.
L'état a plus de PME que les autres états, soient 12 pour-cent des 2.3 millions d’entreprises.

En 2009–10, le Secteur tertiaire est le plus gros contributeur au produit intérieur brut de l'état, avec 44,8 % du PNB comparé aux 44 % du secteur primaire (agriculture, sylviculture et tourisme) et 11,2 % pour le secteur secondaire (industries).
Durant le quinquénal (2007–2012), le taux de croissance moyen du Produit intérieur brut (PIB) est de 7,28 %, donc inférieur aux 15,5 % qui caractérisent la moyenne pour l'ensemble des états de l'Inde.
Le PIB par tête de , est plus faible que le PIB national par tête de .
La dette financière globale de l'U.P. de est identique à celle de 2011.
La productivité est avec un index de 26 supérieure à la moyenne nationale de 25.
L'économie bénéficie de l'industrie touristique.

L'état attire les investissements étrangers principalement dans les domaines du logiciel et de l'électronique.
Noida et Lucknow deviennent des centres importants pour l'industrie informatique.
Le district de Sonbhadra a de grandes industries.
Sa partie méridionale est connue comme étant la "Capitale énergétique de l'Inde" pour sa production de charbon.

L'aviation civile de l'Uttar Pradesh dispose de 6 aéroports situés à Lucknow et Varanasi, Āgrā, Allahabad, Gorakhpur et Kanpur.
Parmi ceux-ci, les aéroports de Lucknow et de Varanasi sont des aéroports internationaux.

Le tronçon du Gange allant de Allahabad à Haldia a été déclaré voie navigable (NW)-I.
Sa partie située dans l'Uttar Pradesh mesure de longueur.

L'Uttar Pradesh a le réseau ferroviaire le plus long du pays.
En 2011, ce réseau de l'état mesure .
Allahabad est le siège de la North Central Railway Zone
et Gorakhpur est le siège de la North Eastern Railway Zone.
Le train Lucknow Swarna Shatabdi Express relie New Delhi à Lucknow.
Les gares de Lucknow NR, Kanpur Central, Varanasi Cantt, Agra Cantt, Gorakhpur Jn et Mathura JN sont dans la liste, établie par les Chemins de fer indiens, des 50 gares ferroviaires devant être de classe mondiale.

L'Uttar Pradesh dispose du plus long réseau routier du pays.
L'état est bien connecté à ses états voisins et à presque toutes les autres parties de l'Inde par le réseau des routes nationales (NH) dont sur son territoire.
Il entretient aussi de 83 routes de l'état d'une longueur totale de .

La Uttar Pradesh State Road Transport Corporation est établie en 1972 pour fournir des transports économiques, fiable et confortable dans l'état et avec des services d'interconnexion avec les états voisins.
Toutes les villes sont reliées au réseau des routes nationales. D'autres routes locales permettent aux districts et aux villages d'accéder au réseau et de transporter les produits agricoles jusqu'aux marchés voisins. Les routes des districts assurent aussi l'interconnexion entre les routes locales et les routes nationales.
L'Uttar Pradesh est le État d'Inde pour sa densité routière ( pour en 2002) et le plus long réseau bitumé du pays ( en 2002).
La Golden Quadrilateral nationale d'Inde traverse Jhansi, Āgrā, Kanpur, Allahabad et Varanasi.
Une route express relie Lucknow à Kanpur et deux nouvelles liaisons express sont en cours de réalisation, la Yamuna Expressway entre Āgrā et Noida et la Ganga Expressway entre Noida et Ballia.
L'état projette de développer sept nouvelles routes express .
Ces projets comprennent la Upper Ganga Canal Expressway, la liaison express Bijnor–Moradabad–Fatehgarh, la liaison express Jhansi–Kanpur–Lucknow–Gorakhpur–Kushinagar, la liaison express Lucknow–Barabanki–Nanpara, la liaison express Āgrā–Kanpur.

L'Uttar Pradesh a plus de 30 universités, dont 4 universités centrales, 20 universités d'État, 8 établissements reconnus comme université, 2 Instituts indiens de technologie, 1 Institut indien de management à Lucknow, 1 NIT à Allahabad et plusieurs polytechniques, collèges d'ingénierie et Instituts de formation industrielle.

Des instituts prestigieux comme l'Institut supérieur de médecine Sanjay Gandhi, ou l'Institut indien de technologie de Kanpur, l'Institut de technologie de Bénarès, l'Institut indien de management de Lucknow, l'Institut indien d'informatique d'Allahabad, et le Institut national de technologie Motilal Nehru sont connus mondialement pour la qualité de leur éducation et de leur recherche dans leurs domaines respectifs.
La présence de ces institutions offre, aux étudiants de l'Uttar Pradesh, un large choix de filières d’enseignement supérieur.

Parmi les autres universités de l'état, notons la Gautam Buddha University, l'université hindoue de Bénarès, l'université Sampurnanand de Sanskrit, la Aligarh Muslim University, l'Université d'Allahabad, l'Institut indien de recherche vétérinaire de Bareli, l'IMT Ghaziabad, la Gautam Buddha Technical University, la M.J.P. Rohilkhand University, l'université Narendra Dev d'agriculture et de technologie, la Babasaheb Bhimrao Ambedkar University, et la King George's Medical University.
L'Université intégrale de Lucknow, a été fondée par le Gouvernement de l'Uttar Pradesh pour offrir la formation dans différentes disciplines techniques, sciences appliquées et autres.
L'université centrale des études tibétaines a été fondée comme organisation autonome par le Ministère de la culture de l'Inde.
La Jagadguru Rambhadracharya Handicapped University est la seule université dans le monde créée exclusivement pour les personnes handicapées.

Plusieurs textes et hymnes de la littérature védique ont été composés en Uttar Pradesh.
Ces textes constituent la couche la plus ancienne de la littérature sanskrite et les textes les plus anciens de l'hindouisme.
C'est en Uttar Pradesh que Vyasa écrit le Mahabharata, le récit épique de la guerre de Kurukshetra et le destin des princes Kauravas et Pandavas.
Le festival du "Guru Purnima", lui est dédié, et il est aussi appelé le "Vyasa Purnima" car c'est le jour présumé de sa naissance et le jour où il divisa les Vedas
L'état a une longue tradition littéraire et populaire de la langue hindi.
Aux s, la littérature Hindi est modernisée par des auteurs comme Jaishankar Prasad, Maithili Sharan Gupt, Munshi Premchand, Suryakant Tripathi Nirala, Babu Gulabrai, Sachchidananda Vatsyayan, Rahul Sankrityayan, Harivansh Rai Bachchan, Dharamvir Bharati, Subhadra Kumari Chauhan, Mahavir Prasad Dwivedi, Swami Sahajanand Saraswati, Dushyant Kumar, Hazari Prasad Dwivedi, Acharya Kuber Nath Rai, Bharatendu Harishchandra, Kamleshwar Prasad Saxena, Shivmangal Singh Suman, Mahadevi Varma, et Vibhuti Narain Rai.

L'Uttar Pradesh est souvent appelé le « cœur hindi de l'Inde ».
L'Hindi est devenu la langue officielle de l'administration de l'état par le "Uttar Pradesh Official Language Act" de 1951.
En 1989, un amendement de cet acte ajoutera le Ourdou comme autre langue native de l'état.
Linguistiquement, l'état s'étend sur plusieurs zones des langues indo-aryennes, les langues natives majoritaires de l'état sont le Awadhi, le Bundeli, le Braj Bhasha, le Kanauji et une forme vernaculaire du Khariboli.

Diwali et Rama Navami sont des festivals populaires de l'Uttar Pradesh.
Kumbh Mela, organisé durant le mois de Maagha, est un festival trisannuel qui a lieu alternativement Allahabad, Haridwar, Ujjain, et Nasik au bord du Gange.
Lath mar Holi est la célébration locale de la fête Hindoue de Holî.
Le festival a lieu avant Holî dans la ville de Barsana voisine de Mathura.
Taj Mahotsav, qui a lieu chaque année à Āgrā, est une manifestation très colorée de la culture de la région de Braj.
Buddha Purnima, qui célèbre la naissance du Bouddha Gautama, est un festival Hindou et Bouddhiste majeur.
Parmi les autres festivals notons Vijayadashami, Makar Sankranti, Vasant Panchami, Ayudha Puja, Ganga Mahotsava, Janmashtami, la foire chrétienne de Sardhana, Maha Shivaratri, Mahavir Jayanti, Mouharram, Bārah Wafāṭ, Aïd el-Fitr, Bakreed, Chhath puja, Lucknow Mahotsav et Hanuman Jayanti.

Le menu végétarien quotidien en Uttar Pradesh, comme tout thali de l'Inde du Nord, est constitué de chapati, chawal, dal, sabji, raïta et de papadum.

La boisson traditionnelle chaas est très communément consommée pendant les repas.

Pour les fêtes, l'utilisation de tava est considérée comme de mauvais augure et l'on préfère consommer des mets frits.

Un thali festif typique est constitué de puri, de kachori, de sabji, de pulav, de papadum, de raïta, de salade et de desserts (comme le sewai ou kheer).

De nombreuses communautés, comme les Jains, les Kayasths et les musulmans, ont leur propre type de cuisine.

Il y a aussi des délices sub-régionaux.

La cuisine Awadhi est mondialement célèbre pour ses plats comme le kebab, le biryani, le keema et le nihari.

Les sucreries ont une place importante dans l'alimentation hindoue et on les consomme lors de cérémonies.
On confectionne des confiseries à base de produits laitiers, comme le khurchan, le peda, le gulab Jamun, le petha, le makhan malai, et le chamcham.

Le chaat de Lucknow et le Banarasi paan sont connus à travers toute l'Inde.

L’Uttar Pradesh est l’État le plus touristique. Il comprend notamment les villes d’Āgrā, la ville du Taj Mahal, du mausolée d'Itimâd-ud-Daulâ, Vârânasî, Sârnâth.



</doc>
<doc id="14979" url="https://fr.wikipedia.org/wiki?curid=14979" title="Youri Bandajevsky">
Youri Bandajevsky

Youri Ivanovitch Bandajevsky (en biélorusse : Юрый Бандажэўскі / Iouri Bandajewski, en russe : Юрий Иванович Бандажевский / Iouri Ivanovitch Bandajevski ; (aussi orthographié Yuri Bandazhevsky suivant la graphie anglaise), né le en Biélorussie et , est à la fois professeur de médecine et anatomo-pathologiste travaillant sur les conséquences sanitaires de la catastrophe de Tchernobyl de 1986.

Fils unique, son père était un officiel du Parti et sa mère enseignante. À l'âge de 16 ans, il entre à l'Université médicale d'État de Grodno. 

En 1987, après l'obtention de son deuxième doctorat de spécialisation, il devient directeur du "Laboratoire central de recherches scientifiques" de Biélorussie. Profondément marqué par l'accident de Tchernobyl, il s'engage dès les premières heures en proposant à l'Académie de sciences et au ministère de la santé une série de mesures et des projets de recherche. À l'âge de 33 ans, il devient le plus jeune professeur de médecine de l'ex Union Soviétique et s'installe, avec sa femme Galina, pédiatre et cardiologue, à Gomel, à 130 kilomètres au nord de Tchernobyl, où, en 1990, il est nommé recteur à l'université de médecine. Pendant neuf ans, il publie quelque 240 travaux sur les effets délétères de la radiation à faibles doses et est lauréat de cinq médailles internationales. 

Par différentes approches (expérimentations, examens cliniques, autopsies), Bandajevsky met en évidence les processus pathologiques induits par la contamination chronique des enfants. Ses recherches portent notamment sur la corrélation entre le taux de césium 137 mesuré dans leur organisme et les altérations cardiaques (arythmie cardiaque…) révélées par l'électrocardiogramme. Il arrive à la conclusion ".

En 1999, le professeur Bandajevsky est un scientifique de référence. C'est en cette qualité qu'il est . Son rapport, dresse un bilan soulignant le gaspillage de 95 % des fonds publics consacrés à la recherche sur les conséquences de Tchernobyl, s'en prenant à un institut dépendant du ministère de la Santé. Car, comme il dira plus tard dans une lettre adressée aux scientifiques, ".
, pour demander une réorientation de la recherche médicale afin de la rendre plus efficace. En 1999, la télévision biélorusse diffuse "Le cœur de Tchernobyl" basé sur ses travaux sur des données concernant les effets délétères de l'incorporation chronique de produits radioactifs, et des conséquences d'une exposition chronique aux radiation.

Dans la nuit du , le professeur Bandajevsky est arrêté, , accusé d'avoir reçu des pots-de-vin pour un montant total de . Il est condamné par le collège militaire de la cour suprême, le , à 8 années de prison - malgré la rétractation publique de son accusateur -, au terme d'un procès où les observateurs de l'OSCE et Amnesty International ont dénoncé au moins 8 infractions au code en vigueur en Biélorussie. Les personnes accusées de l'avoir corrompu ont été jugées innocentes.

Son témoignage sur sa maltraitance physique et psychologique dans les prisons biélorusses et sa longue lutte de résistance peut être consulté dans les "" recueillies et traduites par Wladimir Tchertkoff.

C'est grâce à ce dernier, qui prend contact avec Solange et Michel Fernex, puis avec le GSIEN et d’autres que la nouvelle de son incarcération est connue à l'ouest à partir d'octobre 1999.

Amnesty International l'adopte comme prisonnier d'opinion, et d'autres organisations interviennent en sa faveur. En 2001 le parlement européen lui discerne le Passeport pour la liberté.

La CRIIRAD s’engage dans le soutien en février 2001. Elle organise, avec d’autres associations, une manifestation internationale le 25 mai 2002, à Genève, devant le Palais des Nations et devant l'OMS, afin d’interpeller l'ONU, l'OMS et les autorités biélorusses.

En juin 2002 le Comité Bandajevsky pour "le droit à la vérité et à la justice" se constitue. Il crée un site et fédère le soutien des différentes ONG – Amnesty International, France Libertés, la FIDH, Réseau sortir du nucléaire et d'autres associations de défense de l'environnement ou de la santé… autour de multiples actions. Un « manifeste » publié dans "Le Monde", incite les municipalités et les assemblées territoriales à faire de Bandajevsky leur citoyen d’honneur : il fera l’objet de 24 citations. De plus, il a été fait docteur "honoris causa" de l’université de la Méditerranée. Il fut aussi l’un des sept nominés pour le prix Sakharov en 2003 et fit l’objet d’une action de soutien des Académies des sciences au niveau international en mars 2005. Il est aussi membre d’honneur de l’association des cyber-journalistes.

Même de virulents adversaires de ses théories demandent sa libération, tel André Aurengo qui écrit dans une brève critique de ses travaux : "

La libération de Bandajevsky survenue le 6 janvier 2006 a été intégrée dans les négociations entre la Biélorussie de Loukachenko et l'Union européenne. En avril 2006, Youri Bandajevsky, indésirable en Biélorussie, a reçu une bourse de recherches d'un an financée par le Conseil régional d'Auvergne et il s'est installé en France à Clermont-Ferrand. Dans le même temps, un projet de création d'un laboratoire international indépendant à Minsk, en partenariat avec la CRIIRAD a été lancé.

Il y a controverse au sujet des recherches du professeur Bandajevsky. Certains scientifiques ont dès le départ refusé l’idée du caractère pathogène de faibles doses de Césium 137 incorporé en particulier par l’alimentation.

Selon A. Gonzales, représentant l'AIEA en 2001 à la conférence de Kiev, il n'y a aucun impact sur la santé attribuable à Tchernobyl par exposition à la radiation au-delà de 31 morts à la suite des lésions causées par la radiation et cancers évitables de la thyroïde chez l'enfant. Ce bilan a été révisé à la hausse par le (environ 56 décès et potentiels par cancers), puis en 2006 ().

Le professeur Aurengo, chef du service de médecine nucléaire de la Pitié-Salpêtrière, considère que les travaux de Bandajevsky ' et que seules ' ont accepté de les publier.

L'IRSN signale avoir démarré un programme de recherches sur l'existence de pathologies non-cancéreuses liées à Tchernobyl, visant à confirmer ou infirmer l'existence des arythmies cardiaques reportées par Youri Bandajevsky et leurs liens avec une éventuelle contamination au césium 137.




</doc>
<doc id="14980" url="https://fr.wikipedia.org/wiki?curid=14980" title="Bihar">
Bihar

Le Bihar () est un État du nord-est de l'Inde. Situé dans l'est de la plaine indo-gangétique, le Bihar est un État très densément peuplé (104 millions d'habitants sur ) et encore peu développé sur le plan économique. C'est la région d'origine du bouddhisme et du jaïnisme.

Le cœur de l'ancien royaume de Magadha se trouvait dans le Bihar actuel et sa capitale, Patna, appelé alors Pataliputra, est le centre de l'Empire maurya qui gouverne le sous-continent indien entre -325 et -185. L'empereur Ashoka est le dirigeant le plus connu de cette dynastie. Le Bihar reste un lieu important de pouvoir, de culture et d'éducation durant mille ans jusqu'à la période des invasions musulmanes qui ravagent le pays. Les universités de Nâlandâ et de Vikramaśīla sont alors des centres d'éducation renommés dans le monde asiatique.

Le Bihar est aussi le lieu de naissance de plusieurs religions, dont le bouddhisme et le jaïnisme. Bouddha atteint l'illumination à Bodh-Gaya, une ville de l'État, dans le district de Gaya. Mahavira, le fondateur du jaïnisme, est originaire de Vaishali dans le Bihar.

Avec l'apparition des musulmans, le Bihar va connaître des fortunes diverses. Muhammad Khilji, un général de Muhammad Ghûrî s'empare du Bihar au . Le pays connaît cependant une période de prospérité avec le règne de l'afghan Sher Shâh Sûrî, originaire de Sasaram, qui établit la Grand Trunk Road, la plus grande voie du sous-continent, qu'il traverse d'est en ouest, de Calcutta à Peshawar, dans l'actuel Pakistan. De 1557 à 1576, Akbar, l'empereur moghol, annexe le Bihar et le Bengale à son empire et fait du Bihar une partie du Bengale. Avec le déclin des Moghols, le Bihar passe sous le contrôle des nawabs du Bengale.

Après la bataille de Buxar (1765), la Compagnie anglaise des Indes orientales obtient les droits "diwani", c'est-à-dire ceux d'administrer et de collecter les taxes pour le Bihar, Bengale et l'Orissa. À partir de là, le Bihar devient une partie de la Présidence du Bengale, structure administrative de l'Inde britannique, et ce jusqu'en 1912, année où le Bihar est érigé en province distincte. En 1935, certaines parties du Bihar sont incorporées dans la province de l'Orissa.

En 1951, la province est frappée par une terrible famine qui fait plus de 10 millions de morts. À nouveau, en 2000, 18 districts administratifs du Bihar lui sont retranchés au sud pour former l'État du Jharkhand, avec pour capitale Ranchi.

Après son retour d'Afrique du Sud, Gandhi commence son mouvement pour la liberté par son satyagraha dans le district de Champaran au Bihar, en soutenant des paysans forcés à la culture de l'indigo, une culture très appauvrissante pour leur sol.

Le Bihar est bordé à l'est par le Bengale occidental, au nord par le Népal, au sud par le Jharkhand (créé en 2000), et à l'ouest par l'Uttar Pradesh.

Le Bihar est divisé en 38 districts regroupés en 9 divisions territoriales qui sont :

Le Ministre en chef, chef de l’exécutif, est Nitish Kumar depuis le 22 février 2015. Après une crise politique au sein du JD(U), il remplace Jitan Ram Manjhi qui lui avait lui-même succédé le 20 mai 2014.

Les élections à l'assemblée législative de l'état d'octobre-novembre 2015 voient la nette victoire du Mahagathbandhan (Grande Alliance) rassemblant le JD(U), le Rashtriya Janata Dal et le Parti du Congrès qui remporte 178 sièges (+ 45) sur 243 s'assurant ainsi la majorité absolue. À l'inverse, la NDA, coalition menée par le BJP au pouvoir à New Delhi, perd 36 sièges malgré l'implication du Premier Ministre Narendra Modi.

Le Bihar est le troisième État le plus peuplé d'Inde derrière l'Uttar Pradesh et le Maharashtra. C'est également l'État le plus densément peuplé. Avec un taux de fécondité estimé à 3,7 enfants par femme en 2010, le Bihar connaît une importante croissance démographique.

Les religions les plus pratiquées au Bihar sont l'hindouisme (83,2 %) et l'islam (16,5 %).

Hormis le bihari, on parle au Bihar le maithili proche du bengalî et le magahi, un dialecte hindi oriental qui a donné le pali, la langue religieuse du bouddhisme du Sud.

Sites touristiques : Bodh-Gaya, la ville où le Bouddha a connu l'illumination, est un centre de pèlerinage pour les bouddhistes du monde entier.

Les "Bihari" ou habitants du Bihar sont généralement très pauvres, ils s'exilent très souvent dans les autres États de la fédération indienne - Bengale occidental, Assam, en particulier - pour y faire les travaux les plus durs (construction des routes dans les hauteurs himalayennes, par exemple). Ils émigrent aussi hors de l'Inde et on en trouve aux îles Fidji, à l'île Maurice, à Trinité-et-Tobago. D'autres choisissent un exil social en rejoignant les groupes maoïstes naxalistes présents aussi au Bengale occidental et politiquement proches des népalais du Parti communiste du Népal (maoïste) ou en intégrant les bandes de dacoïts, bandits de grands chemins.



</doc>
<doc id="14986" url="https://fr.wikipedia.org/wiki?curid=14986" title="Concorde (avion)">
Concorde (avion)

Le Concorde était un avion de ligne supersonique construit par l'association de Sud-Aviation (devenue par la suite Aérospatiale) et de la BAC (devenue ensuite British Aerospace).

Sa vitesse de croisière était de à une altitude variant de . Il était doté d'une aile delta, dite « gothique », et de turboréacteurs à postcombustion développés d'abord pour le bombardier britannique Avro Vulcan. Il fut aussi le premier avion civil à être équipé de commandes de vol électriques analogiques.

Les vols commerciaux commencèrent en 1976 et prirent fin plus tard, en 2003. La forte consommation de carburant de l'appareil avait rendu son exploitation déficitaire. Son déclin fut précipité par l'accident du vol 4590 d'Air France en , unique accident majeur d'un Concorde, qui entraîna la mort de .

Confiné à des liaisons transatlantiques et exploité par deux compagnies seulement, l'appareil ne fut produit qu'à vingt exemplaires, dont six non commerciaux. Cependant, moteur d'importants développements technologiques et stratégiques, il eut un fort impact culturel. Avec le Tupolev Tu-144, il fut le seul avion supersonique de transport de voyageurs à avoir été mis en service.

À la fin des années 1950, des entreprises aéronautiques britannique, française, américaine et soviétique souhaitèrent construire le premier avion civil supersonique.

L'entreprise française Sud-Aviation et l'entreprise britannique ' développèrent respectivement leurs supersoniques Super-Caravelle et Bristol 223. Ils étaient financés par leurs gouvernements respectifs, ceux-ci tenant à contrer la domination aérienne américaine. Dans les années 1960, les deux projets étaient déjà bien avancés, mais les énormes coûts de développement des appareils amenèrent les États à faire collaborer les deux entreprises. Le développement du Concorde fut donc plus un accord international franco-britannique qu'un accord commercial entre les constructeurs. Le traité de coopération, dont les discussions durèrent environ un an, fut signé le . ' (BAC) et Sud Aviation se partagèrent les coûts de l'appareil, "" (racheté par Rolls-Royce en 1966) et Snecma firent de même pour développer le turboréacteur dérivé du Bristol Olympus référence 593. Les Britanniques voulaient un modèle long-courrier (transatlantique) alors que les Français voulaient un moyen-courrier. Le , à la suite des élections générales britanniques du qui conduisent à la victoire du parti travailliste, le Royaume-Uni se retire du projet mais fait volte-face deux mois plus tard.

Le , le président français Charles de Gaulle suggéra que l'avion soit baptisé « Concorde » et, le , une première maquette grandeur nature du « Concord » sans « e » fut présentée ; une polémique s'ensuivit sur le nom de l'avion. Le ministre britannique de la Technologie Tony Benn mit fin à la polémique en annonçant : .

L'assemblage d'un premier prototype, ', débuta à Toulouse en et l'avion sortit des hangars le sous l'immatriculation ', « TSS » signifiant « transport supersonique ». Au moins une section fut construite en Grande-Bretagne, puis acheminée à Toulouse via Cherbourg dans un des ferries de la compagnie Townsend Thoresen : le jeu entre le colis et la porte du ferry n'excédait pas de chaque côté. Un second prototype, immatriculé ', sort des chaînes le suivant. L'avion est présenté officiellement, le . Il est ensuite présenté à la population toulousaine le . Le premier vol d'essai de ' eut lieu au-dessus de Toulouse, le . L'équipage était composé d'André Turcat aux commandes, secondé par Jacques Guignard, Henri Perrier et Michel Retif. Ce vol dura . .

Concorde effectua sa première entrée dans le domaine supersonique le au cours du , piloté par Jean Pinet. Le , au cours de son , il atteignit , vitesse qu'il maintint pendant une durée de . Le programme d'essais en vol se déroulant sans incidents, cette version de développement commença les démonstrations destinées au grand public le . Deux appareils de préproduction furent également construits pour les essais, en plus des prototypes. Le premier () fut construit à Filton ; il intègrait plusieurs modifications par rapport aux prototypes, dont une nouvelle voilure plus grande, de d'envergure, un fuselage rallongé et une verrière sur le nez à la place des hublots. Le second appareil (), de construction française, fut le premier à avoir l'aspect et les dimensions des futurs avions de série ; le cône de queue fut allongé, portant la longueur totale à et il fut le premier Concorde à être équipé des tuyères 28 à coquilles. Les deux premiers avions de série furent également engagés dans le programme d'essais, le premier d'entre eux vola le .

Au cours des essais, Concorde établit des records de vitesse et d'altitude. Le , ' atteignit une altitude de , soit plus de . Le record de vitesse fut établi le à par '. En , peu avant d'être retiré des vols, le prototype 001 fut équipé d'appareils de mesure afin de suivre une éclipse de soleil totale. Le vol eut lieu le , entre les îles Canaries et Fort-Lamy, capitale du Tchad, avec André Turcat aux commandes. L'avion vola à et resta dans l'ombre en suivant l'éclipse pendant .

Les essais des Concorde ont enregistré plus de de vol sans trop de problèmes, les appareils de présérie et les deux premiers avions de série servant à terminer la mise au point, notamment des entrées d'air. Au total, plus de de tests furent réalisées à vitesse supersonique. Avec autant d'heures d'essais, le Concorde avait été testé environ quatre fois plus longtemps qu'un avion commercial subsonique moyen ou long-courrier. « Malgré sa construction relativement simple, le Concorde est de loin l'avion civil le plus cher qui ait jamais été construit ».

Cependant, à partir de 1973, une combinaison de facteurs causa l'annulation de la presque totalité des commandes en option. Parmi ceux-ci, il est possible de citer principalement le premier choc pétrolier, les difficultés financières des compagnies aériennes, l'absence de soutien du projet en Amérique du Nord, l'accident au salon du Bourget du concurrent direct soviétique Tupolev Tu-144 et les problèmes environnementaux, comme le bruit généré par le passage d'un aéronef en régime supersonique (bang sonore caractéristique). Finalement, Air France et British Airways restèrent les seuls acquéreurs de l'avion.

Les États-Unis lancèrent leur propre projet de transporteur supersonique en 1963. Deux conceptions s'affrontèrent à l'origine : le Lockheed L-2000, qui ressemblait au Concorde, et le Boeing 2707, projet techniquement plus audacieux avec une cellule en titane et une voilure à géométrie variable. C'est Boeing qui fut retenu en 1966 par le Congrès américain. Plus rapide que le Concorde, le « 2707 » devait transporter à une vitesse proche de . Cependant, face à de grandes difficultés techniques et de fortes oppositions politiques et environnementales, le projet fut annulé cinq ans plus tard. À la suite de cette décision, l'Administration Fédérale Aéronautique (FAA) interdit le survol du territoire américain à vitesse supersonique pour les avions civils, ce qui contribua à l'annulation des commandes de Concorde par les compagnies nord-américaines.

Les deux compagnies aériennes européennes commencèrent les vols de démonstration et d'essais vers diverses destinations à partir de 1974. Le Concorde reçut son certificat de navigabilité le de l'année suivante. Toulouse, en France, et Filton, au Royaume-Uni, furent les deux seuls centres de production des appareils.

Les premiers associés, BAC (qui devint BAE Systems) et "aerospatiale" (qui devint EADS), étaient les copropriétaires de Concorde. La responsabilité se vit ensuite transférée à Airbus, lorsque l'entreprise qui regroupa BAE Systems et EADS fut fondée.

Beaucoup d'améliorations technologiques très communes dans les avions de ligne actuels furent utilisées pour la première fois avec le Concorde.

Un pilote automatique permettait une gestion automatique de la puissance , autorisant un contrôle « mains libres » (ou ') de l'avion de la montée initiale à l'atterrissage. L'électricité à bord était produite par des IDG ('), prédécesseurs et de même technologie que ceux montés sur les avions actuels (Airbus et Boeing). Le Concorde disposait de trois circuits hydrauliques à haute pression de (soit ) pour les composants légers à circuits hydrauliques utilisant un liquide hydraulique à huile synthétique (M) résistant à la température.

Des pièces étaient usinées à partir d'une ébauche unique (et non issues d'un assemblage), ce qui permettait de réduire la masse et la nomenclature des composants. Les gouvernes de direction et élevons étaient constitués de matériaux composites. Toutefois, il s'est révélé que le vieillissement du matériau entraînait des pertes partielles de gouvernes, particulièrement de direction.

Certaines de ces nouveautés technologiques avaient vingt ans d'avance. Si les coûts de conception étaient élevés, cela a toutefois permis aux constructeurs aéronautiques français et anglais de rester dans la course avec les États-Unis, puis de créer Airbus. Nombre de ces améliorations sont maintenant des standards dans les avions de ligne actuels. Par ailleurs, la Snecma a commencé à construire des moteurs pour l'aviation civile avec le Concorde, et l'expérience qu'elle en a tiré lui a donné l'expertise technique nécessaire à l'établissement du consortium CFM International avec General Electric, qui produit avec succès le moteur CFM56.

Dès les premiers vols commerciaux du Concorde en 1976, Aérospatiale proposa de développer une version B, pour réduire le bruit de l'avion et porter sa distance franchissable de (le projet initial français, dénommé "Super Caravelle", avait un rayon d'action de ). Cela entraînait diverses modifications :

Le programme ne fut jamais lancé, en raison de l'absence de commandes.

Les premiers vols commerciaux commencèrent le , sur les trajets Paris–Rio de Janeiro "via" Dakar et Londres-Bahreïn. Les deux appareils décollaient quasi simultanément. La première liaison commerciale Air France Paris–Caracas "via" les Açores fut effectuée le (avec vol retour le lendemain). Le premier vol Paris–Genève (non régulier) eut lieu le et dura . L'interdiction au Concorde d'atterrir sur le territoire des États-Unis, les autorités portuaires de ce pays invoquant des raisons environnementales et des nuisances sonores, gêna les compagnies qui voulaient faire des trajets transatlantiques.

Le , le secrétaire américain aux transports William Coleman leva l'interdiction pour les vols supersoniques au-dessus des eaux territoriales et accorda les atterrissages à Washington et à New York, mais le , les autorités portuaires new-yorkaises opposèrent pour six mois le survol local au Concorde. Avec le peu de choix qu'elles avaient en destinations, Air France et British Airways commencèrent les transatlantiques avec Washington (District de Columbia) le . Finalement, en 1977, les nuisances sonores que les New-Yorkais devaient subir avaient été éclipsées par les avantages que procurait le Concorde, et la liaison Paris et Londres vers l'aéroport new-yorkais John-F.-Kennedy commença le .

Jusqu'en 1982, les destinations pour Air France étaient : Rio de Janeiro, Caracas, Dakar, Mexico, Washington, Dallas et New York. À partir de 1983, pour rentabiliser au maximum son supersonique, la compagnie réduisit ses vols à la seule destination de New York, assurant cependant en plus des vols spéciaux appelés charters et des tours du monde.

Le temps de vol moyen sur l'un ou l'autre des itinéraires était d'environ trois heures et demie. Jusqu'en 2003, Air France et British Airways ont continué à avoir des liaisons quotidiennes avec New York. En plus, Concorde a volé vers la Barbade pendant la saison de vacances d'hiver et, de temps en temps, aux destinations de Rovaniemi en Finlande. Le , un Concorde a effectué le tour du monde en .

Pendant une période brève en 1977, puis de 1979 à 1980, British Airways et Singapore Airlines partagèrent un Concorde pour les vols entre Bahreïn et l'aéroport international de Changi. L'appareil, immatriculé « G-BOAD », était peint aux couleurs de la compagnie singapourienne sur le flanc gauche et aux couleurs de la compagnie britannique du côté droit. Le trajet fut cependant arrêté au bout de trois mois. En effet, le gouvernement malaisien s'était plaint des nuisances sonores. Le trajet fut réutilisé lorsqu'une nouvelle ligne, qui cette fois ne passait pas dans l'espace aérien malaisien, ouvrit. Cependant, l'Inde refusant que le Concorde atteigne la vitesse supersonique dans son espace aérien, l'itinéraire fut lui-aussi plus tard déclaré inutilisable.

De 1978 à 1980, la compagnie américaine Braniff International loua deux Concorde, l'un appartenant à British Airways et l'autre à Air France. Ils furent utilisés pour effectuer des vols réguliers à vitesse subsonique entre l'aéroport Fort Worth de Dallas à l'aéroport international Dulles de Washington D.C., vols qui continuaient ensuite vers l'Europe. Pour des raisons de légalité, les avions utilisés par Braniff étaient enregistrés aux États-Unis mais aussi dans les deux États d'origine (France, Royaume-Uni). Les vols Dallas-Washington étaient assurés par des équipages de la Braniff, puis des équipages français et britanniques prenaient le relais pour le vol transatlantique vers Paris ou Londres. Cependant, les vols ne furent pas bénéficiaires, ce qui força Braniff à arrêter les opérations en .

Les compagnies Air France et British Airways tentent, à partir de 1983, après l'arrêt des vols commerciaux autres que vers JFK, de rentabiliser les avions (maintenance, équipage).

Les équipes commerciales développent des vols à la demande pour les entreprises, mais aussi pour les agences de voyages, pour des tours du monde ou comme complément à des croisières en paquebot. Des vols sont effectués pour marquer certains événements médiatiques comme la Coupe du monde de football, les Jeux olympiques (transport de la flamme olympique en 1992 pour les jeux d'Albertville), les Grands Prix de Formule 1, le Carnaval de Rio. Dans le monde de l'aéronautique, la présence de l'avion est appréciée, comme lors de l'inauguration de l'aéroport de Kansai ou, jusqu'en juin 1989, dans les meetings d'aviation.

Ces tours du monde durent environ un mois.

Les passagers des tours du monde sont principalement des passagers américains. Les principales agences sont Kuoni, Intrav Missouri et TMR France (Marseille). Certaines années, chez Air France, jusqu'à 6 tours du monde sont effectués.

En 1995, plusieurs événements politiques et contentieux diplomatiques déroutent deux tours du monde. L'un de ces évènements est une vague d'attentats en France et l'autre, la reprise des essais nucléaires français en Polynésie. Les escales de remplacement sont Nouméa avec un transfert des passagers par vols subsoniques vers Christchurch et Sydney ainsi que Londres au lieu de Paris.

Une pause a été faite en 1991 pendant la première guerre du Golfe.

En septembre 1995, la Chine donne l'autorisation d'atterrir à Pékin pour British Airways et Air France. Mais le bruit au décollage amène les chinois à interdire Pékin au Concorde. Les escales en Chine se font à Tianjin à au sud de Pékin, en bord de mer.

Le 7 mai 1971, le Concorde emporte le président de la République française Georges Pompidou. C'est la première fois qu'un chef d'État utilise un prototype pour effectuer un voyage officiel. Durant ce vol, le président Pompidou donne une interview en direct au micro de l'ORTF, dans laquelle il a dit : « "Je suis frappé par la stabilité de l'appareil à plus de deux mille kilomètres à l'heure. Je ne m'en apercevrais même pas, tant le vol est calme, doux et silencieux, si je ne voyais pas les côtes de France au loin, qui défilent devant nous à une vitesse extraordinaire. À tout le personnel d’aérospatiale, des ingénieurs aux techniciens et à tous les travailleurs, je voudrais dire, pour la joie qu'ils me donnent aujourd'hui, de tout cœur merci" ».
De 1981 à 1995, après un voyage du président de la République française en Chine avec un avion supersonique, tous les voyages présidentiels lointains sont effectués en Concorde. Celui-ci était aménagé en bureau et chambres à coucher dans la cabine avant, la cabine arrière étant réservée aux invités. Une photocopieuse était installée en cabine arrière.

De même, un système de chiffrement des communications dites « sensibles » était installé avec un téléphone vers le bureau du président. Un pilote spécialiste radio était embarqué pour s'occuper des communications présidentielles.

La visite du 12 septembre 1985 sur le site de Kourou pour le lancement de la fusée Ariane 3 laisse un souvenir désastreux au président François Mitterrand : après deux demi-tours sol pour des problèmes de train avant, il doit changer d'appareil (de plus, la fusée a dû être détruite en vol, à la suite d'un défaut d'allumage du étage).

D'autres présidents ou rois ont affrété le Concorde pour leurs déplacements soit par les vols réguliers vers New York (assemblée générale des Nations unies) soit des transports vers l'Afrique comme le président Mobutu (Zaïre) ou le président Houphouët-Boigny (Côte d'Ivoire).

Lors des voyages du pape, la règle est que le pays recevant le pape organise le voyage de départ vers sa destination suivante.

Lors du passage du pape sur l'île de La Réunion le , un Concorde d'Air France (F-BTSC) est affrété pour le transporter entre Saint-Denis de la Réunion et Lusaka (via Gillot).

L'entretien du Concorde avec les contraintes exigées, sécurité des vols, ponctualité, régularité vol en supersonique, peut être assimilé à l'entretien d'une Formule 1 donc gourmand en heures de main-d'œuvre et en pièces.

À titre de comparaison, la maintenance d'un Concorde est de 18 à par heure de vol alors que celle d'un avion classique d'aujourd'hui est en moyenne de .

D'autre part, le nombre réduit de vols entraîne des stationnements prolongés au sol.

L'arrivée du Concorde entraîne une petite révolution en maintenance puisque les circuits étaient commandés en électrique et en hydraulique, avec pour certains des tests embarqués pour faciliter le dépannage. Il a fallu repenser les métiers des mécaniciens et électriciens pour entretenir les Concorde : l'électronique faisait son entrée dans tous les circuits en commande et en surveillance.

Comme les autres avions, le programme d'entretien est déposé par la compagnie aérienne. Cependant, les deux compagnies avaient deux philosophies différentes en matière d'entretien, particulièrement dans l'utilisation et l'occupation des mécaniciens.

Le choix de British Airways est de créer un département entretien spécialement réservé au Concorde.

Dès les débuts de l'exploitation de Concorde, le choix est également de créer un département "Concorde", mais la fréquence des vols, la sous-utilisation des mécaniciens et les coûts de maintenance entraînent la création d'un département avion européens. Dans un premier temps en 1979 avec l'A300, en 1984 avec l'A310, puis en 1989 avec l'A320. À partir de 1990, la maintenance des Concorde est partagée avec seulement les A300 et A310. En 2001, après l'accident de Gonesse, un département "Concorde" seul est recréé jusqu'en 2003, fin de l'exploitation.

Cette organisation permet d'occuper les mécaniciens en permanence, mais aussi de maintenir les compétences dans les technologies nouvelles.

Dans les escales régulières, comme JFK, une équipe spécialisée est en permanence sur place. À partir de 1995, la maintenance à JFK est sous-traitée à une entreprise créée par d'anciens mécaniciens Air France, "Mach 2".

Dans les autres escales, deux mécaniciens sont envoyés sur place pour assurer les pleins et la maintenance.

Pour les tours du monde, un technicien superviseur est en permanence à bord en vol, en plus de l'officier mécanicien navigant, et deux mécaniciens envoyés sur place assurent la maintenance dans chaque escale. Un lot de bord permet d'assurer un dépannage de qualité permettant la poursuite du vol.

Avant l'accident de Gonesse, le Concorde n'a jamais connu d'avaries entraînant des pertes humaines.

L'enquête judiciaire qui a suivi l'accident met en cause le talon d'Achille du Concorde, la fragilité des pneumatiques. Des dizaines de cas d'éclatement de pneumatiques sont survenus depuis sa mise en service, avec dans plusieurs cas des perforations d'un réservoir ou d'une aile notamment à Washington et à Dakar en 1979.

Le , le F-BTSC du vol 4590 Air France, charter à destination de New York, avec des passagers de nationalité allemande, décolle de l'aéroport Paris-Charles-de-Gaulle puis s'écrase deux minutes après le décollage sur un hôtel à la Patte d'Oie de Gonesse, provoquant la mort de : cent passagers, neuf membres d'équipage et quatre personnes au sol.

L'accident du serait dû, notamment, à une cause extérieure, une lame métallique tombée sur la piste d'un avion précédent : un DC-10 de la Continental Airlines. L'éclatement d'un pneu aurait provoqué une fuite de carburant plus importante que lors des incidents précédents ; l'inflammation du carburant aurait entraîné des « pompages » (décrochage aérodynamique des pales des compresseurs) et des pertes massives de puissance sur un moteur (le 2), puis sur l'autre situé juste à côté (le 1). La principale cause retenue par la version officielle est celle de « la lame métallique présente sur la piste », .

L'accident est à l'origine de nouvelles modifications sur le Concorde. Les contrôles électriques ont été améliorés : protection anti-perforation en kevlar des réservoirs de carburant (au nombre de 13 sur Concorde), montage de pneus plus résistants, fournis par Michelin qui a développé des pneus d'une nouvelle technologie Radial NZG (Near Zero Growth) qui pèsent de moins que ceux précédemment utilisés. Cependant, le nombre de places à bord est réduit d'une dizaine, rendant l'exploitation de l'appareil d'autant moins rentable. Les deux itinéraires sont rouverts le .

Le procès relatif à cet accident s'est ouvert le au palais de justice de Pontoise. Le , l'accusation a requis :

Le la justice rend son verdict et condamne Continental Airlines à une amende de deux cent mille euros et à verser un million d'euros de dédommagement en faveur d'Air France (cinq cent mille euros pour « préjudice moral » et la même somme pour « atteinte à l'image »). Continental Airlines, par la voix de son avocat , a décidé de faire appel de cette décision.
Le chaudronnier John Taylor est condamné à de prison avec sursis, son chef d'équipe, Stanley Ford, ayant été relaxé.
Les trois autres prévenus (Henri Perrier, Jacques Hérubel et Claude Frantzen) ont été relaxés.

Le 16 décembre 2010 Air France décide de faire appel en raison des propos tenus après l'annonce du verdict par Continental Airlines (, ). Le procès en appel aboutit à la relaxe de Continental Airlines et de John Taylor sur le plan pénal, tandis que la condamnation de Continental Airlines à verser un million d'euros à Air France est maintenue sur le plan civil.

Avant le désastre, Air France et British Airways voulaient maintenir le Concorde jusqu'en 2007 voire 2017, l'appareil n'étant néanmoins rentable qu'avec un faible coût du carburant, et on argue que les pertes sont insignifiantes, Concorde représente une part d'activité chez Air France inférieure à 1 %, même si les rumeurs d'abandons sont récurrentes.

Après l'accident de Gonesse, le Concorde est remis en service le 7 novembre 2001 mais il connaît plusieurs incidents (problèmes moteur le 15 juillet 2002, le 3 et le 5 novembre 2002). Un Concorde de la British Airways connaît un souci majeur le 27 novembre 2002 : il perd une de ses gouvernes alors qu'il amorce sa descente vers l'aéroport JFK de New York. Le , British Airways et Air France annoncent simultanément le retrait de leurs Concorde pour l'année suivante. Les raisons invoquées sont la baisse du nombre de passagers depuis l'accident de Gonesse le et le coût élevé de maintenance. De plus, le trafic aérien connaît une grave crise après les attentats du 11 septembre 2001, les nouvelles normes contre la pollution, le bruit et la hausse du prix du kérosène. Mais la raison essentielle vient de la décision d'EADS de ne plus assurer l'entretien du supersonique à partir d'octobre 2003.

Dans le même temps Sir Richard Branson offre la somme d'une livre sterling pour acheter un appareil à British Airways qui aurait servi dans la Virgin Atlantic, mais cette offre est refusée. Plus tard, il écrit dans "The Economist" (du ) que l'offre finale était de cinq millions de livres sterling et qu'il voulait utiliser le Concorde pendant encore de nombreuses années. Cette offre était probablement destinée à faire de la publicité pour Virgin, Airbus ayant de toute façon refusé de continuer à livrer des pièces de rechange pour Concorde.

Les derniers vols commerciaux de Concorde avec Air France décollent de l'aéroport JFK de New York (dernier vol régulier New York vers Paris) et de Roissy (dernière boucle supersonique) et atterrissent à Roissy le . Le dernier Concorde à atterrir en service commercial devait être le "Sierra Delta" en provenance de New York, mais un problème sur le moteur 4 retarde de le décollage du "Fox Bravo", chargé d'effectuer la dernière boucle supersonique au-dessus de l'Atlantique, et le "FB" atterrit donc finalement le dernier vers alors que le "Sierra Delta" se pose à (les arrivées étant initialement prévues à une minute d'intervalle). Les camions de pompiers arrosent l'avion comme de coutume sur la piste de l'aéroport John F. Kennedy alors que, à Roissy, attendaient les deux derniers Concorde.

La fin de l'aventure Concorde avec Air France est marquée, pour le "Fox Bravo", par un vol au-dessus du golfe de Gascogne à vitesse supersonique. De retour de sa boucle au-dessus de l'Atlantique, le "Fox Bravo" survole Orly, l'aérodrome de Lognes, puis passe à la verticale de Roissy avant de s'y poser. De nombreux véhicules (véhicules de piste, voitures de gendarmerie et de pompiers) escortent les deux derniers Concorde après leurs atterrissages respectifs. Les deux avions font une longue promenade sur les taxiways de Roissy, s'arrêtant entre autres devant le siège d'Air France et devant les milliers de personnes venues assister aux deux derniers atterrissages de Concorde en service commercial.
Mais c'est le 3 juin 2003 que Concorde effectua pour sa toute dernière fois la liaison New York - Paris Charles de Gaulle, à l'issue d'un vol VIP non commercial, à bord du F-BVFB.

Les derniers vols de convoyage vers les musées des Concorde se sont effectués de cette manière : le 12 juin 2003, le Concorde F-BVFA est le premier à rejoindre son musée, il quitte Paris pour rejoindre la collection du "Smithsonian Museum" de Washington, où il est exposé officiellement le 20 décembre de la même année. Le 14 juin 2003, c'est le F-BTSD qui rejoint la collection du Musée de l'Air et de l'Espace du Bourget, il effectue un court vol entre Roissy et Le Bourget, et arrive en vol durant le salon du Bourget 2003 devant le Président Jacques Chirac, présent pour l'occasion. Le 24 juin, le Concorde F-BVFB quitte Paris pour rejoindre Karlsruhe-Baden en Allemagne, où il se pose pour la dernière fois pour rejoindre par voie fluviale et terrestre la collection du musée automobile et technologique de Sinsheim aux côtés de son rival Tupolev-144. Le 27 juin 2003, le F-BVFC effectue l'ultime vol Concorde français entre Paris et Toulouse, où il est depuis exposé au sein du musée Aeroscopia, avec à son bord André Turcat, et autres acteurs du projet Concorde.

Une enchère a par ailleurs lieu chez Christie's à Paris le . sont présentes pour acheter des objets et des photos des moments importants de la vie du Concorde. Parmi ces objets, certains voient leur valeur multipliée par dix (voire plus) par rapport à la mise à prix.

Le dernier Concorde de British Airways décolle de la Barbade le .

La dernière semaine de vols de démonstration du Concorde se fait au-dessus de Birmingham le 20 octobre, à Belfast le 21, Manchester le 22, Cardiff le 23, et Édimbourg le 24. Chaque jour, l'avion part de la ville de Heathrow et va jusqu'aux villes concernées en volant à basse altitude en vol subsonique. Il y a eu environ ayant gagné à un concours et invitées qui ont volé dans ce Concorde.

Élisabeth II consent à éclairer le château de Windsor pour la soirée du , pour le passage de Concorde au-dessus du château après un décollage de Londres. C'est, pour le Concorde, un honneur suprême, car seuls quelques avions des principaux chefs d'État ont droit à ce privilège.

British Airways retire officiellement l'avion le jour suivant, le 24 octobre. Cette sortie définitive se fait avec l'un des Concorde qui quitte New York avec une fanfare similaire à celle qu'a connu son homologue d'Air France, tandis que, simultanément, deux autres avions paradent, l'un au-dessus du golfe de Gascogne pour Air France, et l'autre au-dessus d'Édimbourg pour British Airways. Les trois avions ont obtenu la permission spéciale de voler à basse altitude. Les deux Concorde (qui faisaient des tours) atterrissent respectivement à et à l'heure britannique et celui venant de New York à . Chacun des trois avions passe alors en roulant au sol autour de l'aéroport avant de débarquer les derniers passagers d'un vol supersonique commercial. Le pilote du vol New York/Londres est , qui est aussi le pilote du premier vol commercial d'un Concorde aux couleurs de British Airways, qui a eu lieu en 1976.

Parmi les passagers de ce dernier vol transatlantique il y a, comme souvent, de nombreuses célébrités du monde du spectacle et des affaires, des cadres ou des dirigeants de grandes compagnies internationales .

Il y a eu par la suite une vente aux enchères des pièces d'un Concorde de British Airways qui s'est déroulée le au centre d'exposition d'Olympia, dans le quartier Kensington de Londres. Les articles vendus sont hétéroclites et comprenaient un compteur de mach, le cône du nez, le siège du pilote, des fauteuils de passagers et même des couverts, des cendriers et des couvertures utilisés à bord de l'appareil. Environ sont récoltés, dont sont donnés à l'association 'Get Kids Going!' qui offre aux enfants handicapés et aux jeunes l'occasion de faire du sport.

L'entrée du poste de pilotage se fait par un couloir bas ( de haut) d'une longueur de . Dans les armoires électroniques de chaque côté, sont disposés des calculateurs servant au pilotage automatique, navigation, communications VHF, batterie, conditionnement d'air, conduite moteur. La partie supérieure est réservée aux panneaux disjoncteurs.

Trois sièges à manœuvre électrique sont disposés dans le cockpit, les deux sièges des pilotes (CDB et OPL) avec des planches de bord similaires à droite et à gauche (navigation).

La partie centrale, conduite moteur, commande du pilote automatique et pylône, (radionavigation et communications) est commune. En partie supérieure, au-dessus des pare-brises, un panneau de centrale d'alarme avec en fonction des niveaux d'alarme des voyants de couleurs différentes.
Au panneau supérieur, les commandes de vol, les poignées coupe-feu, les éclairages extérieurs (feux de navigation et phares).

Le poste de l'OMN, siège orientable soit vers le panneau ou vers l'avant (position décollage), derrière l'OPL, est équipé de nombreux indicateurs et interrupteurs : conditionnement d'air, électricité, carburant, indicateurs complémentaires moteurs, panneau de démarrage, commandes des entrées d'air et hydrauliques. Le panneau, du plafond au plancher, est équipé d'indications et commandes. Sur la cloison gauche du cockpit, encore des panneaux disjoncteurs.

Deux sièges observateurs peuvent être utilisés en fonction des besoins, l'un derrière le CDB, l'autre dans le couloir central derrière l'OMN.

En raison de sa forme élancée nécessaire afin d'avoir de bonnes performances pour le vol supersonique, les pilotes ont généralement une très mauvaise visibilité vers l'avant. Cela ne cause pas de problème en vol de croisière, mais un peu plus dans les phases de décollage et d'atterrissage. C'est ce qui explique pourquoi les pilotes doivent abaisser le nez du Concorde pour ces phases.

L'appareil est séparé en deux cabines, pour la cabine avant et pour la cabine arrière, les toilettes, les vestiaires et les portes centrales servant de séparation entre les deux cabines. Les sièges sont installés par rangées de quatre, séparés en deux par une travée centrale.

À l'entrée de la cabine avant, un office avec four est installé pour le service. La conservation des aliments est faite avec de la carboglace. Le même type d'équipement est installé en cabine arrière.

Il n'y a ni vidéo ni projection de film pendant les vols, mais un choix de musiques est disponible à chaque siège.

Trois toilettes sont installées, une à l'avant pour les passagers cabine avant et l'équipage et deux entre les deux cabines.

Chaque siège dispose d'un porte-bagages en partie supérieure et des vestiaires à porte-manteaux sont installés en extrémité de chaque cabine.

Dans le galley arrière, des calculateurs, entrées d'air, communications longue portée (HF) sont disposés de chaque côté avec accès par le galley. Au fond, un accès mène vers la soute arrière mais ne peut être ouvert qu'au sol.

Deux soutes peuvent accueillir les bagages des passagers, l'une sous la cabine avant, l'autre derrière le galley arrière. Chaque soute dispose d'une entrée indépendante. Les soutes à bagages ont un volume de et ne sont pas ventilées. De ce fait, le transport d'animaux vivants est exclu.

Toutes les parties disponibles restantes sont utilisées pour les équipements : centrale à inertie et radar à l'avant, soute hydraulique, soute de conditionnement d'air.

L'aile delta n'est que très peu portante à faible vitesse, ce qui oblige l'avion à avoir un angle d'incidence élevé durant les phases de décollage et d'atterrissage. La visibilité vers l'avant s'en trouve fortement réduite lors du décollage et en phase d'approche.

En réponse à ce problème, le Concorde (comme le Tupolev Tu-144), est équipé d'un nez et d'une visière mobiles inclinables, pour une meilleure visibilité à basse vitesse et meilleure pénétration dans l'air à haute vitesse. L'ensemble nez-visière peut prendre :


Le fuselage et la voilure de Concorde sont construits en alliage d'aluminium, connu sous la référence RR58 en Grande-Bretagne et AU2GN en France. Cet alliage a été mis au point afin d'offrir le meilleur compromis entre masse, résistance aux déformations et résistance à la température. L'échauffement cinétique en vol supersonique est important : sur le nez de l'appareil, environ sur le bord d'attaque des ailes. Cet échauffement provoque une dilatation du fuselage et un allongement de celui-ci pouvant aller jusqu'à .

Partie essentielle et spécifique de cet avion : l'aile adaptée au vol supersonique. Le concept d'aile delta (triangulaire) est modifié afin d'avoir de meilleures performances aux basses vitesses. Cette modification de l'aile du Concorde porte un nom spécifique : l'aile gothique. En effet, si on regarde le plan de l'aile, on s'aperçoit que la forme en plan est en ogive, d'où le nom "gothique".

Les travaux de l'Onera, dans les années 1950 ont démontré de nombreuses hypothèses. L'augmentation de la flèche à l'emplanture (apex) permet une augmentation de la portance, notamment grâce à la portance tourbillonnaire. Une corde à emplanture plus longue offre plus de volume pour les réservoirs (un point-clef du projet). Les ailes du Concorde disposent de bords d'attaque à double courbure, il y a ainsi une augmentation de la surface en bout d'aile. Les commandes de vol sont multifonctions, les élevons sont à la fois les ailerons (roulis) et les gouvernes de profondeur (tangage). Il n'y a pas d'aérofreins (inutiles sur une aile delta à forte traînée), ni de volet déporteurs, ni encore de volets de bord d'attaque et de bord de fuite. Du fait de son faible allongement (ici 1,55), une aile delta est peu portante (Cz ≈ 1), l'avion doit avoir un angle d'incidence élevé au décollage et à l'atterrissage, ce qui gêne fortement la visibilité depuis le cockpit.

En l'absence de volets, les vitesses minimales (portance maximale) sont obtenues de la façon suivante :
Au total un coefficient de portance d'environ 0,65 pour une masse de permet de décoller aux environs de (soit ), soit une vitesse supérieure de 50 à 60 % à celle d'un avion de ligne subsonique (entre ).


Le Concorde est un quadriréacteur. Les réacteurs sont disposés deux par deux. Les nacelles, dans lesquelles ils sont logés, sont réalisées en acier et matériaux résistants aux hautes températures. Des panneaux de protection thermique sont installés au plafond. Les détecteurs d'incendie y sont installés.

La grande difficulté de conception et de mise au point des réacteurs vient du fait que l'avion vole en subsonique et en supersonique, alors que la vitesse de l'air à l'intérieur du moteur doit être inférieure à la vitesse du son même en supersonique. Pour cela, les constructeurs ont partagé le moteur en trois parties :

Ces trois parties disposent de leurs commandes et contrôles particuliers.

Le but des entrées d'air est d'amener la vitesse de l'air à une vitesse compatible avec le fonctionnement du moteur (environ Mach 0,5). Des panneaux articulés, appelés « rampes » assurent cette fonction. Ces rampes sont manœuvrées par des tubes de torsions, eux-mêmes entraînés par un moteur hydraulique. Ces moteurs sont au nombre de deux : un normal et un autre de secours.

On distingue trois phases de fonctionnement :

Le Concorde est propulsé par des turboréacteurs composés de trois ensembles : l'entrée d'air conçue par British Aircraft Corporation, le réacteur Bristol Siddeley (puis Rolls-Royce) type Olympus 593 à postcombustion monté sur l'Avro Vulcan et le canal d'éjection étudié et réalisé par la SNECMA. Des modifications importantes ont permis d'accroître la poussée et de diminuer la consommation en régime subsonique. La version définitive est la Mk IV.

Des entrées d'air moteurs à section variable à régulation électronique servent à réduire la vitesse de l'air entrant dans le réacteur. Une sortie des gaz à section variable augmente la vitesse de l'air sortant. Le dégivrage de la voilure et des entrées d'air moteurs est entièrement électrique soit en continu soit par cycle, limitant les tuyauteries d'air. Cette spécificité n'est pas reprise sur les avions actuels.

La conception (difficile), la réalisation et la mise au point de l'entrée d'air du réacteur ont été prises en charge en partie par la Snecma.

Cependant, il n'y a pas d'APU, ce qui impose la présence d'un groupe électrique et à air indépendant dans chaque escale. Un projet a été étudié mais abandonné (prototype APU au MAE, don de M. Chevalier). Le Boeing 727 est le premier avion civil à disposer d'APU intégré pour la mise en route des réacteurs.

Les Concorde français sont équipés de réacteurs identiques à ceux équipant les Concorde britanniques, mais assemblés par la Snecma.

Simple flux, double corps (compresseurs basse pression (N1) et haute pression (N2)), chambres de combustion annulaire, turbines haute et basse pression. Un système de postcombustion (ou réchauffe) est ajouté. Une tuyère à section variable (AJ : Area Jet) vient se positionner à l'arrière.

Un relais accessoire, entraîné par le corps haute pression N2, permet d'entraîner les vario-alternateurs, les pompes hydrauliques, les pompes d'alimentation en carburant haute et basse pression.

La régulation de la poussée est effectuée par le biais du corps haute pression N2 (Contrairement aux moteurs d'aujourd'hui qui se régulent au N1). Ce dernier (N2) réagit aux variations de débit carburant piloté par la manette des gaz associée au moteur. L'attelage basse pression N1 est régulé par la tuyère primaire (AJ), montée en sortie de canal de réchauffe (postcombustion). Le N1 est ajusté au N2. Le rapport de vitesses des deux compresseurs doit rester dans une plage de fonctionnement compatible. La régulation du N1 n'interfère par sur celle du N2 car un phénomène de saturation (ou bouchon) permet de dissocier les deux. Concrètement, un col sonique est présent dans le distributeur de la turbine BP. Les paramètres variants en amont n'affectent pas ceux situés en aval et inversement. C'est une particularité de ce moteur. Ce système a permis de se passer de clapet de décharge.

L'équipage ajuste et contrôle la poussée par la vitesse de rotation du corps haute pression (N2) au moyen de deux calculateurs de poussée (TCU) par moteurs, l'un suppléant l'autre en cas de panne. Au poste de pilotage, des indicateurs à aiguilles et à tambours permettent de contrôler les paramètres de vitesse de rotation moteur, de consommation de carburant, de pressions et de températures.

La postcombustion (appelée aussi réchauffe) est utilisée pour le décollage et pour passer le mur du son, à partir de Mach 0,97 et jusqu'à Mach 1,7. Elle permet d'obtenir une poussée supplémentaire d'environ 18 % pendant ces deux phases, mais au prix d'une consommation très élevée ( par heure au décollage au lieu de 20 en croisière). La postcombustion est réalisée par une pompe et un régulateur de carburant haute pression envoyant du carburant dans les gaz d'échappement du moteur. Elle est commandée par le pilote au moyen d'un interrupteur situé derrière les manettes de poussée moteur au travers d'un calculateur électronique.

La postcombustion n'est pas allumée sur les quatre moteurs en même temps mais par paire symétrique, d'abord les moteurs 1 et 4 (moteurs extérieurs, les plus éloignés du fuselage) puis les moteurs 2 et 3.

Une couronne de sondes mesurant les températures des gaz de turbine (TGT) est disposée dans le cône de queue du moteur.

Cette partie située en arrière du moteur est faite d'un tube d'acier haute température d'environ de diamètre et de longueur.

La partie tube est une cheminée pour les gaz d'échappement en sortie de turbine. Elle est terminée par deux équipements : 

Le train d'atterrissage est un train dit « tricycle » : un train principal sous chaque aile plus un train avant sous la cabine avant. La commande est électrique, elle pilote des électrovannes qui envoient un fluide dans des vérins hydrauliques. La sortie, comme la rentrée, est normalement hydraulique, mais en cas d'urgence, après déverrouillage manuel, chaque train est sorti par gravité.

Le train avant se replie vers l'avant ; les deux trains principaux, après raccourcissement se replient latéralement, dans leur logement situé en partie dans le fuselage. Une fois le train rentré, des portes ferment les logements.

Une roulette dite « de queue » rétractable est installée au niveau du cône de queue pour protéger le fuselage en cas d'incidence trop élevée pendant le décollage.

Les disques de freins principaux, au nombre de 8, un par roue, sont en carbone pour réduire la masse de l'avion. Ce point clef de la conception n'est adopté qu'à partir de l'avion 102.

Le Concorde dispose de trois possibilités de freinage : un freinage normal avec antipatinage, un freinage « alternate » et un freinage de secours.

Les roues avant sont freinées par un frein à disque pour le freinage à la rentrée du train uniquement.

Un transmetteur de position pédale électrique commande la puissance hydraulique pour les freinages normal et "alternate". Le freinage de secours est entièrement hydraulique, des pédales de freins aux freins. Des ventilateurs permettent le refroidissement accéléré des freins.

Une sonde de température est installée sur chaque frein dont la température est transmise au cockpit.

Il y a quatre roues sur chaque train principal. Les pneus sont gonflés à l'azote pour limiter l'échauffement des roues. Il n'y a pas de transmetteurs de pression des pneus comme sur les avions actuels, mais, à la suite d'un incident à Washington en 1979, un système de détection de sous-gonflage a été installé sur chaque train principal. Il s'agit de mesurer les contraintes du bogie dû, par exemple, à une roue dégonflée ou crevée par des détecteurs d'effort collés sur le bogie. Le signal est envoyé au cockpit sur des voyants au panneau avant et au panneau OMN.

Le test du système est quotidien et l'alarme de sous-gonflage pendant le roulage nécessitait un retour au parking pour vérification. De plus, la vérification des pressions des roues est effectuée avant chaque vol. L'orientation des roues avant est faite à l'aide d'un volant pour chaque pilote. Le signal généré par le volant est envoyé vers un calculateur. Un vérin hydraulique commandé électriquement oriente le train avant en fonction de la consigne reçue.

La génération électrique est de même principe que sur les autres avions modernes contemporains (Boeing 747) (triphasé et avec mise en parallèle des 4 alternateurs). Ceux-ci sont entraînés par les moteurs par l'intermédiaire du boîtier accessoires. Il y avait un IDG ("Integrated Driving Generator") par moteur.

La nouveauté du Concorde sont les générateurs électriques mis en place pour gagner du poids, réuni les deux fonctions, régulation de fréquence et générateur électrique en un seul équipement appelé IDG. Le gain de poids est d'environ par alternateur. Cette technologie fut reprise par les constructeurs d'équipement pour les avions modernes à partir de l'Airbus A310. Tous les avions en sont maintenant équipés.

Les commandes et contrôles des tension et fréquence de chaque IDG sont gérés par un moteur et un calculateur, appelé "Generator Control Unit" (GCU). Les paramètres (tension, fréquence et températures de l'huile de refroidissement) peuvent être vérifiés par l'officier mécanicien navigant (OMN). Un bouton-poussoir et un voyant de synchronisme permettant de faciliter la mise en parallèles des alternateurs, qui est normalement automatique (même tension, même fréquence et même rotation de phase).

En cas de panne, le mécanicien navigant peut déconnecter mécaniquement l'IDG à partir du poste de pilotage. Le vol se poursuit avec trois générateurs. De plus, pour respecter la réglementation, un alternateur de secours entraîné par un circuit hydraulique est également installé. En dernier recours, un convertisseur statique courant continu/courant alternatif assure le courant alternatif à partir des batteries de bord. Ces deux batteries cadmium/nickel assurent le dernier secours en . La recharge de ces batteries et l'alimentation électrique continue sont assurées par des transfo-redresseurs 115/28 "via" des contrôleurs de charge.

Au sol, moteurs arrêtés, l'avion est alimenté par un groupe de parc de minimum de puissance.

Le Concorde dispose de nombreux éclairages. Les commandes des éclairages se situent dans le cockpit, juste au-dessus du pare-brise afin d'être accessible aux deux pilotes. Deux phares d'atterrissage rétractables d'une puissance de sont situés à l'intrados, près du bord d'attaque, à proximité de la jonction entre l'aile et le fuselage. Deux phares de roulage et de décollage également rétractables sont situés sous le fuselage. En avant du cockpit, en partie inférieure du fuselage, de chaque côté, se trouvent deux phares de virage. Trois feux de navigation sont inclus soit dans les ailes, soit dans le cône de queue, afin d'éviter les trainées supplémentaires. Trois feux anticollisions à flash rouge sont situés de part et d'autre du fuselage au début de la jonction entre l'aile et le fuselage et un à l'arrière en extrémité de fuselage.

À l'arrière, le boîtier de feu de navigation est commun avec le feu anticollision. La fixation de ce feu est renforcée afin de parer à la dégradation due aux vibrations dans cette partie de l'avion. Les logements de trains d'atterrissage sont éclairés au sol à des fins d'inspection.

Comme la "Caravelle" et les Airbus actuels, le Concorde est doté de trois circuits hydrauliques. Circuits normaux appelés vert et bleu et circuit secours appelé jaune. Le liquide est de l'Oronite, un liquide synthétique résistant à la température de fonctionnement en vol soit . Ces circuits sont alimentés par des réservoirs situés dans la soute hydraulique placée sous la soute arrière.

Au sol, moteurs arrêtés, la pression est générée par deux pompes électriques, une pour le circuit vert et une pour le circuit bleu, alimentées en triphasé. Le circuit jaune peut être utilisé par une ou les deux électro-pompes sous réserve qu'on ait orienté le sélecteur sur jaune. Ces pompes sont commandées par des interrupteurs situé au panneau mécanicien navigant. Tous les équipements hydrauliques peuvent être commandés par la pression délivrée par ces pompes.

En situation de maintenance, des groupes de parcs hydrauliques sont utilisés pour les essais prolongés notamment les essais de rentrée de train alors que quand les moteurs sont en route, la pression hydraulique est délivrée par les pompes entraînées par les moteurs.

Les circuits hydrauliques commandent les trains d'atterrissage (rétraction/extension, freins), les commandes de vol et le nez basculant.

En dernier recours, en cas de perte des trois circuits hydrauliques, une hélice (RAT, ram air turbine) située sous l'aile gauche peut être sortie à partir du poste de pilotage. Cette hélice, mue par le vent relatif lié au déplacement de l'avion, entraîne une pompe hydraulique permettant de conserver un minimum de commandes de vol et les freins en freinage secours (pas d'antipatinage) ainsi que l'alternateur de secours. Pendant la vie de l'avion, cet équipement de secours n'a jamais servi. Seuls les essais en maintenance garantissent le bon fonctionnement en cas de besoin en vol.

Treize réservoirs contenant au total de kérosène, soit environ (densité 0,8) permettent d'alimenter les réacteurs. Ces réservoirs sont répartis dans les ailes, dans le cône de queue derrière la soute à bagage et dans le fuselage en partie basse en avant des trains d'atterrissage principaux. Les réacteurs sont alimentés à partir des quatre réservoirs dits « nourrices ». Ceux-ci se remplissent pendant le vol par transfert de carburant à partir des autres réservoirs.
La consommation de carburant pouvant varier en fonction des vents, de la charge (passagers et bagages), du temps estimé d'attente à l'arrivée (notamment de CDG vers JFK), une quantité de carburant supplémentaire (environ ) peut être ajoutée dans les parties hautes des réservoirs (surplein).

La quantité de carburant vers les États-Unis est le plein complet à pleine charge, soit avec environ restant à l'arrivée (le tableau de caractéristiques indique ). Le retour vers l'Europe ne nécessite pas le plein complet (vents favorables). La quantité pour le retour est d'environ avec également restant. Cette quantité restante pouvant être utilisé en cas de panne du conditionnement d'air ou du moteur, et dégagement en cas d'indisponibilité de l'escale d'arrivée.

En plus de l'alimentation des réacteurs, le carburant remplit deux autres fonctions. Il est utilisé pour le centrage. Après le passage du mur du son, l'équilibre aérodynamique est modifié, le centre de portance recule. Pour compenser cet effet, les ingénieurs auraient pu utiliser le braquage des gouvernes de profondeur, mais ce système n'était pas acceptable, car il aurait produit une augmentation significative de la traînée, ce qui aurait entraîné une surconsommation de carburant, réduisant considérablement l'autonomie de l'avion. La solution trouvée pour parer à ce phénomène consiste à déplacer vers l'arrière le centre de gravité de l'appareil. Sur Concorde, la seule masse déplaçable est le carburant. Le transfert du carburant se fait de l'avant vers l'arrière pour le vol supersonique et le contraire pour le retour en subsonique comme sur le Dassault Mirage IV. Trois réservoirs situés dans le fuselage, deux à l'avant et un à l'arrière servaient principalement à cette fonction. Le transfert s'effectue par deux conduits dits « main gallery » entre les trois réservoirs. Pendant ces transferts, le déplacement du carburant est entendu en cabine. À , transfert vers l'arrière du carburant, aux environs de , début du transfert vers l'avant. Pendant l'avitaillement, la séquence de chargement du carburant permet de ne pas « poser » l'avion sur la roulette de queue. Une table des volumes des réservoirs permet de connaître la répartition du carburant. Enfin, le carburant est également utilisé pour le refroidissement de l'air de conditionnement de la cabine.

Selon la vitesse, le maintien de la température en cabine peut se faire de deux manières. En vol subsonique, la cabine est réchauffée par le prélèvement d'air sur les étages compresseur haute pression. Pour des vitesses supersoniques, la climatisation est rendue difficile par l'échauffement de la cellule en raison des frottements de l'air. Le refroidissement se fait par échange avec le carburant, prélèvement des frigories. Une surconsommation de carburant peut obliger à revenir en subsonique plus tôt que prévu afin de conserver une température acceptable en cabine.

Quatre groupes de conditionnement d'air sont utilisés, mais une surveillance accrue de la température par l'officier mécanicien navigant est nécessaire pour éviter une augmentation de la température cabine non compatible avec le confort des passagers. La pressurisation de la cabine est réalisée par quatre vannes ("ouflow valves") commandées par un contrôleur de pressurisation. L'OMN programme le système manuellement. Quatre indicateurs permettent la surveillance de la pressurisation. Il y a un variomètre cabine, un altimètre cabine, un indicateur d'écart de pression externe/interne (delta P) et un indicateur de position de vanne de régulation de pression de la cabine.

Le Concorde dispose de 2 circuits de secours à oxygène.

Le circuit pilotes comprend une bouteille oxygène gazeux qui alimente cinq masques à oxygène au poste de pilotage. Le circuit passagers est constitué de trois bouteilles installées en soute arrière qui alimentent les masques pour cent passagers et six personnels commerciaux.

Des bouteilles portatives sont installées à bord afin de permettre aux personnels commerciaux de circuler en cabine avec un masque à oxygène si besoin.

Comme les autres avions de la même époque (747, A300, DC-10), le Concorde est équipé de deux centrales aérodynamiques et d'un circuit de secours. Les centrales, situées dans l'entrée du cockpit, récupèrent les informations de nombreux instruments. La vitesse est obtenue grâce à des tubes de Pitot situés de chaque côté. L'altitude est mesurée à l'aide des prises statiques situées de part et d'autre du fuselage en arrière des portes avant. La température est relevée en utilisant des sondes sous le nez. La température est très importante pour le calcul du nombre de Mach. Les informations sont distribuées par des tuyauteries souples et rigides situées sous les planchers cabine et poste de pilotage sauf pour la température (informations électriques).

On retrouve les instruments classiques mais doubles, puisque servant en mode électrique (normal) et secours (pneumatique) sur chaque planche de bord. Ces indicateurs sont les altimètres pour l'altitude et les anémomètres pour la vitesse par rapport à l'air. Les machmètres indiquent la vitesse en nombre de Mach. Pour le calcul de celle-ci, il est nécessaire de connaître la température de l'air ambiant et la vitesse. Les variomètres sont utilisés pour connaître la variation d'altitude. Il y a également les indicateurs de température. Les informations reçues par ces instruments sont des informations calculées par les centrales aérodynamiques ayant pour origine les pressions prises par les Pitot et les prises statiques. Des sondes d'incidences et de dérapages, au nombre de deux chacune, complètent le dispositif aérodynamique. Deux sondes de dégivrage sont également installées. Toutes les sondes sont dégivrées en subsonique.

Le circuit de secours est entièrement pneumatique, des sondes aux indicateurs. Le Pitot est constitué par la pointe de perche de nez et la prise statique est placée sur la partie externe de cette même perche.

Deux recopies machmètres installés à l'avant des cabines avant et arrière permettaient aux passagers de suivre l'évolution du Mach en croisière.

Un test embarqué commandé par deux interrupteurs situés en arrière du pylône permet de simuler les vitesses et altitudes au sol.

Trois centrales à inertie permettent d'obtenir les informations de cap et horizon de manière indépendante de systèmes terrestres. Ces centrales, situées en soute électronique, sous le cockpit avec accès par une porte indépendante, sont chacune couplées à une batterie de petite capacité pour permettre d'assurer l'alimentation des centrales en cas de perte de réseau électrique.

Afin de lire et d'utiliser un cap magnétique, les centrales sont couplées à un coupleur compas, qui permet de corriger le cap géographique donné par les centrales à inertie pour obtenir un cap magnétique. Deux vannes de flux situées sur le toit de l'avion permettent de récupérer les informations magnétiques. Ces informations peuvent être lues sur les instruments de bord de chaque côté, mais les informations de cap et d'altitudes distribuées sur chaque planche sont d'origine différente pour faciliter la détection de pannes ou d'erreurs d'indications.

Le temps d'alignement et chauffe des centrales à inertie est d'environ . Ces centrales sont utilisées pour effectuer de la navigation par waypoints. Ces points de repère sont insérés un par un par les équipages. Les informations des centrales sont utilisées pour le cap, l'altitude (horizon artificiel), les corrections de vitesse et d'altitude, le calcul de la vitesse par rapport au sol et de la vitesse ascensionnelle, ainsi que pour le pilote automatique.

Couplé au pilote automatique, l'avion peut rejoindre son point de destination automatiquement sans autre surveillance que la vérification du passage du way-point.

Des systèmes d'aide à la navigation par radio étaient installés sur Concorde. Il y avait deux VOR, radio navigation en VHF, constitués de deux antennes, deux récepteurs et boîtes de commandes, et des indicateurs RMI VOR pour la chaîne automatique et les HSI pour les chaînes manuelles. Les VOR sont couplés aux centrales à inertie pour le recalage des positions. Deux DME permettaient de calculer les distances de l'avion par rapport aux stations au sol. Deux systèmes ILS étaient utilisés pour le guidage des approches de précision. Ces systèmes utilisent les mêmes instruments de vol que les VOR. Le Concorde était équipé de deux ADF dont les antennes sont fixées sur le toit du fuselage et dont les récepteurs sont installés dans les armoires électroniques situées dans le galley arrière. Deux RMI ADF permettent la visualisation des indications de directions des stations. Deux radio altimètres permettaient de lire les altitudes d'approche (inférieur à ) avec précision (au pied près). Les antennes sont situées sous le fuselage à hauteur de la soute avant. Les émetteurs-récepteurs sont installés au fond de la soute avant. Il y avait deux systèmes radar météo qui permettaient la détection des zones nuageuses en vol. L'antenne double, installée dans le radôme de nez, envoyait les informations à l'aide d'un guide d'ondes vers les émetteurs-récepteurs situés en soute électronique avant. Les zones nuageuses seront visibles sur deux écrans mono-couleurs, situé à l'avant droit et gauche des pilotes. Deux systèmes ATC permettaient d'envoyer les informations de situation et altitude vers les Centres de Contrôle en vol. Deux systèmes anticollisions en vol ont été installés en 1998 à la suite de l'obligation d'installation pour les vols, vers les États-Unis dans un premier temps.

Le Concorde est équipé de deux pilotes automatiques/directeur de vol, permettant de faciliter la conduite du vol aux pilotes pendant le vol. Le panneau de commande ("AFCS") est situé, comme pour les autres avions, sur le panneau situé au-dessus des indications moteurs. Il permet d'engager les différents modes PA/DV.

Les calculateurs PA, sont situés dans les meubles avioniques situé de chaque côté du couloir d'entrée du cockpit. Un test embarqué permet la détection et le dépannage des PA. La liaison PA/Commande de vol s'effectue par les relay-jack situés sous le plancher du poste de pilotage. À l'avant des manettes de poussée, un panneau avec des boutons de commande permet de faire évoluer en PA dit manuel. De plus, en PA, des bielles d'effort permettent de piloter l'avion en mode PA dit « pilotage transparent » à partir des manches sur simple effort du pilote. Les signaux d'effort transmis par les bielles sont traités par les calculateurs PA avant d'être envoyés sur les commandes de vol.

Le Concorde est certifié atterrissage tout temps dit CAT , hauteur de décision .

Le Concorde est équipé des systèmes traditionnels de communications radio.

Il dispose de deux radios VHF de de portée. Les émetteurs récepteurs VHF sont situés dans l'armoire électronique situé dans l'entrée du poste de pilotage. Les antennes sont situées, une sur le toit, l'autre sous le fuselage. Celle sous le fuselage a pour particularité d'être double (VHF et VOR).

Il y a également deux radios longue portée (HF) pour les routes empruntées au-dessus des océans et parties désertiques qui rendent obligatoire l'utilisation permanente de la HF. La nouveauté du Concorde est l'utilisation d'une antenne HF structurale située dans la partie basse du bord d'attaque de l'empennage vertical (tous les avions modernes sont maintenant équipés de cette façon). Les deux boîtes d'accord HF sont situées dans l'épaisseur de l'empennage vertical (portes ovales situées à gauche). La garantie du fonctionnement du système nécessite un essai par la maintenance avant chaque vol.

Aucun avion n'a été équipé de système téléphone satellite ni ACARS (telex).

À la mise en service, la détection incendie moteur est réalisée avec des détecteurs dits de « flamme ». Des cellules disposées dans les nacelles moteurs, trois doubles par moteur, sont chargées de détecter les flammes et la fumée. Trop sensibles et non fiables, d'une maintenance difficile (accès très difficile) ces détecteurs sont remplacés ensuite par des détecteurs classiques de l'époque dit « capacitifs ».

La détection incendie et fumée soutes est des plus classiques. Il y a deux types de détecteurs, détecteurs « ambiance » et détecteurs « prélèvement ». Les détecteurs « ambiance » analysent l'air ambiant grâce à des cellules photoélectriques. Les détecteurs « prélèvement » analysent l'air des conduits d'évacuation de l'air de ventilation des équipements.

Comme sur tous les autres avions, deux enregistreurs de paramètres équipent le Concorde. Il y a un enregistreur de paramètres dit DFDR, qui est réglementaire, situé dans la partie basse des meubles avionique du galley arrière. Un autre enregistreur de paramètres dit QAR se situe en partie avionique du cockpit. Cet enregistreur dispose dans un premier temps d'une cassette, puis d'un disque optique facilement remplaçable, le but étant un accès rapide aux paramètres par la compagnie à des fins de contrôle de trajectoires et de maintenance dans des conditions définies par la compagnie.

Comme pour les avions de ligne, le Concorde était équipé d'un enregistreur de conversation. Celui-ci, situé en partie avionique du galley arrière, permet l'enregistrement des conversations cockpit dès la prévol de l'équipage et jusqu'à la fin du vol. Il est équipé également d'une balise émettrice sous-marine.

L'altitude de vol étant élevée, un détecteur de rayonnement cosmique est installé à bord. Un indicateur permet à l'équipage de contrôler en permanence le niveau de rayons cosmiques.

Seuls vingt Concorde ont été construits (plus 2 cellules pour essais statiques), six pour les essais et quatorze pour les vols commerciaux.

Il y a ainsi eu :

Tous sauf deux sont préservés, ce qui représente 90 % des appareils produits qui ne sont, pour l'instant, pas détruits. Cela est très rare en aéronautique.

Cet avion, s'il ne fut pas un succès commercial, se révéla, en revanche, une très grande réussite technologique. Il reste un symbole fort de technologie ultra moderne malgré ses , et nombreux sont ceux qui aiment ses formes sculpturales. Il est de plus un symbole de fierté nationale pour beaucoup de gens au Royaume-Uni et en France .

Le Concorde détient toujours le record des liaisons commerciales les plus rapides :

La vitesse et les horaires de Concorde (départ à de l'aéroport Paris-Charles-de-Gaulle et arrivée à à l'aéroport John-F. Kennedy) ont facilité certaines négociations diplomatiques. Dans des moments critiques pour la paix dans le monde (Yougoslavie et guerre du Golfe), les diplomates et Kofi Annan, ex-secrétaire général de l'ONU, ont utilisé le Concorde dans les deux sens.

De plus, le passage des chefs d'État et diplomates à Paris pour prendre le Concorde était l'occasion d'une visite au locataire de l'Élysée avant de s'envoler vers leur destination finale.
En 1991, François Mitterrand a utilisé le Concorde pour se rendre en Arabie saoudite quelques jours avant la guerre du Golfe afin de rencontrer le roi et de soutenir les troupes françaises stationnées dans le Royaume.

La réaction des riverains contre la perspective d'importantes nuisances sonores dues aux vols a aussi représenté un changement social important. Avant les premiers essais en vol du Concorde, les nouveautés de l'industrie civile aéronautique étaient largement acceptées par les gouvernements démocratiques et leurs électeurs. Les protestations populaires (particulièrement sur la côte est des États-Unis) contre le bruit du Concorde ont marqué un point de rupture politique. Par la suite, les scientifiques et ingénieurs de domaines variés ont commencé à prendre en compte plus sérieusement les impacts environnementaux et sociaux de leurs innovations.

De ce point de vue, le grand bond en avant technique incarné par le Concorde a aussi été un bond en avant pour la sensibilisation du public (et des médias) aux conflits entre la technologie et les écosystèmes naturels qui sont toujours d'actualité. Beaucoup d'avions actuels produisent moins de particules polluantes et de nuisances sonores, et cela est peut être une partie de l'héritage du Concorde. L'usage de murs antibruit le long des lignes de TGV n'aurait peut-être pas été si développé sans les protestations des années 1970 au sujet de la pollution sonore des avions.
Un billet sur une ligne régulière Concorde était un privilège pour les plus aisés. Cependant, certains vols charter circulaires (les boucles supersoniques) ou aller simple (avec retour en voiture, train ou bateau) étaient organisés et accessibles à des passionnés moins fortunés.

Le Concorde est aussi apparu lors d'événements royaux au Royaume-Uni, volant parfois en formation avec la patrouille des Red Arrows. Il a aussi participé à de nombreux salons aéronautiques, et a été accompagné par la Patrouille de France.

Un timbre a été édité en France à son effigie, l'exemplaire en photo étant F-BTSC. Un cachet "premier courrier postal supersonique" a également existé pour la première desserte de Rio de Janeiro par courrier postal en Concorde (qui n'eût cependant pas de suite).

La Britannique Barbara Harmer et la Française Béatrice Vialle sont les deux seules pilotes professionnelles du Concorde. Jacqueline Auriol a piloté l'appareil en tant que pilote d'essai.

En 1992, Aérospatiale Avions a présenté un avant-projet de supersonique « Alliance » dit ATSF pour Avion de Transport Supersonique du Futur. Ce projet devait transporter sur à une vitesse similaire (Mach 2). Grâce à un allongement plus important ( au lieu de ) la finesse aérodynamique aurait été de 10 au lieu de pour le Concorde ; la consommation spécifique (rapportée à la poussée) restant du même ordre, au lieu de , la consommation par passager pour serait descendue de 18 à . Les valeurs actuelles (en 2010) pour les avions subsoniques les plus économes sont de l'ordre de 2,5 à par passager.

En novembre 2003, la compagnie EADS qui codétient Airbus (avec BAe Systems) annonça qu'elle travaillait avec des compagnies aériennes japonaises pour développer un avion plus grand et deux fois plus rapide (hypersonique) que Concorde. Le projet ZEHST a été présenté au salon international du Bourget en juin 2011.

Concernant les aides d'État à la recherche, le réseau « Recherche aéronautique sur le supersonique » créé en 2000 par la direction de la Technologie (ministère de la Recherche) a été clos en 2004.

Un accord de coopération entre le Groupement des industriels français de l'aéronautique et de l'espace (Gifas) et son homologue japonais SJAC a été signé lors du Salon du Bourget 2005. Le supersonique franco-japonais qui pourrait succéder au Concorde devrait transporter entre Mach 1,6 et 1,8 à d'altitude, sur . Son premier vol pourrait s'effectuer en 2017.

Les projets « hypersoniques » plus futuristes que réalistes étant mis de côté, il semble qu'un avion supersonique « possible » aurait une vitesse d'environ Mach 1,6, et des moteurs à double flux sans réchauffe présentant le meilleur rendement possible en subsonique. La question de la finesse aérodynamique, très inférieure à celle d'un avion subsonique, reste entière.

La compagnie britannique Reaction Engines Limited est engagée dans un programme de recherche appelé LAPCAT, financé à 50 % par l'Union européenne via l'Agence Spatiale Européenne. Ce projet a pour but d'étudier la possibilité d'un avion fonctionnant à l'hydrogène et transportant 300 passagers, l'A2. Cet avion hypersonique serait capable de voler entre Bruxelles et Sydney à Mach 5 en . Il s'agit seulement d'une application prospective dérivée de leur projet d'avion spatial Skylon comprenant les technologies clefs, notamment le très innovant moteur hybride Sabre dont la variante appliquée à l'avion civil A2 s'appelle Scimitar ; celui-ci autorise l'entrée d'air à haute vitesse dans les moteurs mais l'air est néanmoins refroidi et ralenti avant d'entrer dans le compresseur du moteur au moyen de l'hydrogène, embarqué dans d'imposants réservoirs, qui sert à la fois de refroidisseur et de carburant.

Un autre projet encore plus ambitieux est étudié par l'agence spatiale allemande (Deutsches Zentrum für Luft- und Raumfahrt) et soutenu également par l'Union européenne et l'ESA. Cet avion suborbital, le SpaceLiner, serait capable de relier l'Australie à l'Europe en seulement en atteignant une vitesse maximale supérieure à Mach 20.

La prochaine génération d'avion supersonique pourrait être l'avion "Boom" construit par l'entreprise Boom Technology





</doc>
<doc id="14987" url="https://fr.wikipedia.org/wiki?curid=14987" title="Axiomes des probabilités">
Axiomes des probabilités

Dans la théorie des probabilités, une mesure de probabilité (ou plus brièvement probabilité) formula_1 est une application qui à un événement "A" quelconque associe un nombre réel (noté formula_2). Une mesure de probabilité doit satisfaire les axiomes des probabilités ou axiomes de Kolmogorov, du nom d'Andreï Nikolaievitch Kolmogorov, mathématicien russe qui les a développés. 

Une mesure de probabilité formula_1 est toujours définie sur un espace probabilisable formula_4 c'est-à-dire sur un couple constitué d'un ensemble d'éventualités, l'univers "Ω", et d'une tribu formula_5 de parties de l'univers "Ω". Les éléments de la tribu formula_6 sont appelés les événements. Ainsi la mesure de probabilité formula_1 est une application de formula_6 dans formula_9 

Pour tout événement formula_10 : 
C'est-à-dire que la probabilité d'un événement est représentée par un nombre réel compris entre 0 et 1.

formula_12 désignant l'univers associé à l'expérience aléatoire considérée,

C'est-à-dire que la probabilité de l'événement certain, ou d'obtenir un quelconque résultat de l'univers, est égale à 1. Autrement dit, la probabilité de réaliser l'un ou l'autre des événements élémentaires est égale à 1.

Toute famille dénombrable d'événements deux à deux disjoints (on dit aussi : deux à deux incompatibles), formula_14 satisfait :

C'est-à-dire que la probabilité d'un événement qui est la réunion (dénombrable) disjointe d'événements est égale à la somme des probabilités de ces événements. Ceci s'appelle la σ-additivité, ou additivité dénombrable (si les événements ne sont pas deux à deux disjoints, cette relation n'est plus vraie en général).

À partir des axiomes, se démontrent un certain nombre de propriétés utiles pour le calcul des probabilités, par exemple :

Remarque : en particulier, cela interdit à l'univers d'être vide, le deuxième axiome exigeant que sa mesure vaille 1 (et ne soit donc pas nulle "a fortiori").

Cette relation signifie que la probabilité que B se réalise, mais pas A, est égale à la différence formula_23. Cette relation découle de ce que B est réunion disjointe de formula_24 et de formula_25

C'est la propriété de croissance de la probabilité. En effet, dans le cas particulier où formula_26, la propriété précédente s'écrit
Ceci signifie que la probabilité pour qu'un événement ne se produise pas est égale à 1 moins la probabilité pour qu'il se réalise ; cette propriété s'utilise lorsqu'il est plus simple de déterminer la probabilité de l'événement contraire que celle de l'événement lui-même.
Ceci signifie que la probabilité pour que l'un au moins des événements formula_36 "ou" formula_37 se réalise est égale à la somme des probabilités pour que formula_10 se réalise, et pour que formula_18 se réalise, moins la probabilité pour que formula_10 "et" formula_18 se réalisent simultanément. De même,
qui donne la probabilité de la réunion de n ensembles "non nécessairement disjoints".


C'est-à-dire que la probabilité de la limite d'une suite croissante d'événements (qui est dans ce cas la réunion - dénombrable - de tous les événements de cette suite) est égale à la limite de la suite numérique des probabilités de ces événements.


C'est-à-dire que la probabilité de la limite d'une suite décroissante d'événements (qui est dans ce cas l'intersection - dénombrable - de tous les événements de cette suite) est égale à la limite de la suite numérique des probabilités de ces événements.



De manière équivalente, on définit plus simplement le triplet formula_51 représentant un espace probabilisé, comme un espace mesuré dont la mesure, formula_52, a la particularité d'avoir une masse totale égale à 1 :
formula_53 
En théorie de la mesure, les événements sont appelés « ensembles mesurables ».
Ce mini-lexique permet de traduire les résultats de la théorie de la mesure et de l'intégration de Lebesgue en termes probabilistes.


</doc>
<doc id="14991" url="https://fr.wikipedia.org/wiki?curid=14991" title="Mongols">
Mongols

Les Mongols sont un peuple nomade vivant actuellement en Mongolie, en Russie et en Chine. Les quatre ethnies principales sont les Khalkhas, les Oïrates, les Bouriates et les Kalmouks. Ils sont actuellement environ 10 millions, dont près de 5,8 millions en Chine. Leurs langues forment un groupe spécifique de la famille altaïque comportant plusieurs langues. Les principales sont le khalkha, dialecte devenu langue vernaculaire de Mongolie, le tchakhar, dialecte vernaculaire de Mongolie-Intérieure, le bouriate, parlé en Bouriatie, dans les deux Mongolies et dans d'autres républiques de Sibérie et le kalmouk, parlé en Kalmoukie. Les Mongols pratiquent traditionnellement un bouddhisme lamaïste (dit tibétain) teinté d'animisme voire de chamanisme.

À l'origine d'un des plus grands empires de tous les temps, qui s'étendit de la mer de Chine méridionale jusqu'au-delà de la Volga au et au , ils conservent encore leur culture, malgré leur éclatement en quatre entités politiques distinctes ; outre la langue et l'histoire, cette culture profondément originale couvre des domaines tels la musique, la religion, les fêtes, les sports, le mode de vie, et enfin l'organisation sociale.

Les Mongols sont répartis principalement dans quatre territoires, dont un seul est souverain : la Mongolie. En Russie, ils disposent de deux républiques disposant d'une autonomie relative : la Bouriatie et la Kalmoukie. En Bouriatie, du fait du processus de colonisation russe, les Bouriates ne représentent que 25 % de la population. Ils se démarquent des autres ethnies mongoles par une certaine conservation des anciennes croyances chamanistes, malgré leur conversion au bouddhisme. En Chine, la majeure partie des Mongols se trouve dans la province autonome de Mongolie-Intérieure, où sont présents plus de 5 millions de Mongols, mais où ils restent une minorité vis-à-vis des Chinois Han. Il existe également des minorités mongoles dans le Xinjiang, le Qinghai et le Gansu.

On compte environ 25 ethnies mongoles au total. Les Khalkhas, principalement urbains, constituent la majorité en Mongolie. Diverses petites ethnies cohabitent dans les monts Altaï, dont les Oïrates, voisins des Kalmouks. Les Oïrates sont à l'origine de l'Empire de Dzoungarie, aux et , vaincu et annexé par la Chine. Les Oïrates émigrèrent alors sur les bords de la Volga, puis une partie d'entre eux repartit en Mongolie, mais ils furent décimés en cours de route par les Kazakhs : ce sont les Oïrates actuels. Les Kalmouks sont littéralement « ceux qui sont restés ». Ces derniers faisaient partie des « peuples punis » par Staline, accusés d'avoir coopéré avec les armées blanches. En Mongolie-Intérieure, il existe diverses ethnies regroupées sous le nom de Mongols Occidentaux. Au nord, ce sont les Bouriates qui dominent.

Les Mongols sont restés suffisamment isolés pour que subsiste un type (facial notamment) bien défini et caractéristique. On appelle leur phénotype le type "Mongoloïde" : Peau "jaune foncé", massif, tête large, yeux bridés. Par leur couleur de peau et la forme de leurs yeux, ils se rapprochent des Japonais, des Coréens, des peuples turcs, mais aussi des peuples dits « ouraliens », ce qui tend à renforcer les théories linguistiques rapprochant les langues altaïques du coréen, du japonais, du turc et des langues ouraliennes (cf. langues).

Au premier millénaire de notre ère, les Turco-mongols vivaient probablement en chasseurs-cueilleurs en Sibérie orientale, sur le cours supérieur du fleuve Amour. Les Mongols, les Toungouses et les Turcs ne faisaient qu'un. Tandis que les Toungouses restaient, les Mongols et les Turcs s'établirent dans les régions steppiques voire désertiques au Nord de la Chine, où ils devinrent donc nomades.

Pendant toute l'Antiquité, les Turco-Mongols vécurent en guerriers nomades, faisant régulièrement des raids en Chine, et établissant de temps en temps des empires éphémères, dirigés par des hommes de guerre forts mais s'effondrant dès la mort de ceux-ci. Les hommes se divisaient alors en multiples tribus, ce qui les rendait beaucoup plus vulnérables. À l'instar des Romains, les Chinois se servaient parfois d'une de ces tribus pour se protéger d'une autre.

Les Turcs comme les Mongols ont conquis de nombreuses terres en occident. Certains peuples de ces deux cultures se retrouvent en Europe (Huns, Horde d'or…), où à l'Ouest de l'Asie, d'autres sont restés attachés à leur terres comme les Kazakhs et Kirghizes (Turcs) ou certains Mongols.

Au , les Mongols se rassemblèrent sous Gengis Khan (« le roi universel » en mongol, de son vrai nom Temüdjin). Celui-ci soumit la Sibérie méridionale, le nord de la Chine, le royaume Xia (dirigé par les Tanguts), puis le Khara Kitaï et le Khwarezm en Asie Centrale. Ses successeurs bâtirent l'un des plus grands empires ayant jamais existé, en conquérant le reste de l'Asie Centrale et de la Chine, le Moyen-Orient, la Russie. Ils poussèrent à l'ouest jusqu'en Autriche, à l'est ils attaquèrent (en vain) le Japon, la Birmanie au Sud, et allèrent même jusqu'en Indonésie. À son apogée, l'Empire était parvenu à établir une « Pax Mongolica », développant les routes de la soie, les rencontres entre les grandes confessions et les relations entre l'Orient et l'Occident.
Mais, après quelques décennies, l'empire ne tarda pas à se fracturer en plusieurs khanats, qui devinrent rivaux. Ainsi affaiblis, les khanats succombèrent l'un après l'autre aux poussées locales. L'élite mongole, de plus en plus réduite, se fondit rapidement dans les cultures locales. Ce fut la Horde d'or, à qui la Russie payait tribut, qui disparut en dernier en 1502, plus de trois siècles après l'apparition de Gengis Khan.
À l'époque moderne, il existait encore des États se réclamant de l'héritage mongol : le khanat de Crimée, le khanat d'Astrakhan, les khanats ouzbeks et khazakh... 

De manière progressive, les Mongols retournèrent dans leurs terres natales ou se fondirent soit aux Turcs avec qui ils partageaient la même origine et langue, soit aux Iraniens de par la proximité du territoire perse. 
Une bonne part des Mongols se convertirent au bouddhisme réformé au . 
. Cette dernière contient la plus grande partie du peuple mongol, mais la Russie domina la Mongolie pendant l'ère soviétique (sous le nom de "République populaire de Mongolie"). La Mongolie, encore préservée il y a quelques décennies, doit maintenant faire face à la modernisation : alors que le chômage et la pauvreté sévissent en ville, la plupart des foyers possèdent une télévision, même à la campagne.

Les Turco-Mongols sont traditionnellement des nomades. En effet, les pâtures des prairies sont trop faibles, en dehors de la saison des hautes herbes (de juin à août) pour soutenir en permanence les grands troupeaux de chevaux, de vaches, de moutons ou de chèvres. Cette pauvreté s'agrandit dans les déserts où les Mongols sont obligés de se déplacer fréquemment, parfois chaque semaine. 
En général, les Mongols se déplacent par groupes de 2 ou 3 familles dans les steppes herbeuses, au plus une famille dans les déserts. Il y a occasionnellement de grands rassemblements, lors de tournois et de fêtes, notamment pendant Tsagaan Sar, la fête mongole du nouvel an.
Le nomadisme décline rapidement en Mongolie et Mongolie-Intérieure du fait d'épisodes climatiques qui ont décimé les cheptels et conduit de nombreux éleveurs à la ruine. Depuis les années 2000, la population urbaine est montée à 50 % en Mongolie ; ce chiffre atteint a 58 % en 2007. La désertification induite, et la disparition concomitante de services aggrave le phénomène. L'attrait des études et de la civilisation occidentale pourrait être un second facteur d'explication.
En Chine, où les éleveurs mongols ont vu une partie de leurs terres accaparée par les colons hans, et plus encore en Russie, la sédentarisation est de règle, parfois accompagnée de transhumance. Dans les trois États, le passage d'une économie centralisée à une économie de marché a bouleversé les relations socio-économiques. 
Toutefois, une bonne part de la population (environ un quart aujourd'hui) demeure très attachée au mode de vie ancestral dans la steppe, qui procure un grand sentiment de liberté. Les Mongols aiment leur pays, et cela s'est vu lors des conquêtes gengiskhanides : la majorité des troupes souhaitait revenir dans leurs terres natales plutôt que de s'installer en terre conquise.

En Kalmoukie, le nomadisme se maintient avec plus de succès.

Les anciennes tribus turco-mongoles se répartissaient en clans exogames, dirigés par des aristocraties guerrières. L'enlèvement de femmes était courant et facilement accepté, mais pouvait néanmoins susciter des conflits entre clans. D'une manière générale, la Mongolie était caractérisée par une désunion dont savaient tirer profit les Chinois. 

Gengis Khan, en unifiant les Turco-Mongols, apaise ces conflits, au prix d'une suite de plusieurs guerres contre des concurrents. Il introduit, pour des raisons militaires, un premier niveau de centralisation et de rationalisation : sous l'Empire, des unités militaires (les "tümen") d'environ guerriers apparaissent, dirigées de manière féodale par des princes (les "noyan"). Elles sont à l'origine des quatre grandes ethnies mongoles orientales : Khalkhas, Tchakhars, Ordosses, Toumètes.

Le centre névralgique de l'Empire Mongol se déplaçant vers Pékin sous Kubilaï Khan, la Mongolie se retrouve de fait sous la domination chinoise. Elle subit au cours des siècles les évolutions de celle-ci. Pendant la majeure partie du , l'ensemble de la région est gouvernée par des États communistes, ce qui la marque profondément.

De nos jours, la Mongolie, bien qu'encore largement structurée par la campagne, tend à se rapprocher du modèle de société occidental, fondé sur la famille, à l'instar des autres pays ex-communistes d'Asie Centrale.

La principale activité des Mongols est l'élevage, qui est véritablement au centre de leur survie. Ils pratiquent un élevage pastoral ancestral, en nomadisant afin de ne pas trop appauvrir la terre. Ils élèvent des moutons surtout, mais aussi des vaches, parfois des chèvres, et bien sûr des chevaux : on compte à peu près autant de chevaux que d'hommes en Mongolie. 

Les moutons sont surtout utilisés pour leur viande, leur laine et leur graisse, et les vaches pour leur lait. Les outils, les aliments et les vêtements traditionnels sont tous issus de ces quatre éléments (cf. sections correspondantes). Les chevaux servent pour les déplacements, pour leur peau et le lait des juments. Lorsqu'ils sont en pâture, ils sont laissés dans un état semi-sauvage, étant seulement montés de temps en temps pour éviter qu'ils ne retournent tout à fait à l'état sauvage : les chevaux étant alors récalcitrants, on utilise l"'uurga", c'est-à-dire une perche terminée par un lasso afin de les attraper. Cette activité est devenue un véritable sport occasionnant des défis et des compétitions.

Le principal adversaire pour les troupeaux est le loup, qui peut faire des ravages en hiver lors des famines, si des précautions ne sont pas prises. Le jour, la simple présence d'un homme suffit normalement à décourager une meute, mais la nuit, elle peut se montrer plus hardie, allant jusqu'à attaquer les camps.

Les Mongols sont présents dans trois types d'habitats : la steppe herbeuse (en Mongolie centrale et orientale, en Kalmoukie, et en Mongolie-Intérieure orientale), le désert chaud (désert de Gobi aux paysages divers, à cheval sur la Mongolie et la Mongolie-Intérieure), et les forêts de type boréal dans les montagnes (monts Altaï en Mongolie occidentale, Sibérie Méridionale). 
Pour les deux premiers habitats, la yourte, habitation typiquement turco-mongole, est utilisée. Dans les monts Altaï et en Sibérie, on trouvera plutôt des tentes coniques semblables à celles des autres peuples de Sibérie. Certaines ethnies de ces régions se sont tournées vers l'élevage des rennes, comme les Tsaatans ou les Evenks.
La yourte (en mongol : , translittération : "ger", signifiant "maison" au sens large) est une habitation familiale facilement démontable, de forme ronde et à taille variable. Le squelette en bois est recouvert d'une toile en peau ou en feutre. Sa forme ne donne aucune prise au vent omniprésent dans la steppe, et le feutre protège très efficacement du froid. Il suffit de 30 minutes pour la (dé)monter et elle pourra alors facilement être transportée dans une charrette. Il existe également des yourtes posées sur des chariots de grande taille, généralement les yourtes des khans. Le chariot est alors directement déplacé, le paysage de steppe permettant de laisser passer de larges véhicules.

À l'intérieur, la disposition des meubles correspond à des règles précises. L'unique porte est toujours placée vers le sud. Le poêle est placé au centre, où un trou a été percé sur le toit pour laisser s'échapper la fumée. Le feu qui y couve est généralement un feu de bouse séchée ("argal"), le bois étant rare, voir complètement absent dans les steppes. Les outils et ustensiles divers sont placés près de la porte, à droite pour ceux des femmes et à gauche pour ceux des hommes. Au fond (au nord), on trouve les lits et les coffres ou armoires qui servent de rangements personnels. Autrefois, quelques tapis et ustensiles constituaient le maigre équipement des yourtes. Aujourd'hui, des meubles et différents objets comme une machine à coudre, par exemple, s'y entassent.

Lors des réunions ou des repas, les femmes sont traditionnellement placées à droite et les hommes à gauche. La place au fond est normalement réservée à l'hôte ou au chef de famille, en signe de respect. L'espace dans la yourte est très hiérarchisé, et sacralisé. Il fait l'objet de nombreuses croyances héritées du chamanisme.

Tous les aliments traditionnels sont issus de l'élevage. Les deux principaux ingrédients utilisés sont la viande de mouton et le lait de vache. Cependant, la marmotte est chassée de temps en temps pour sa viande et sa peau. Les mongols ne mangent traditionnellement que des animaux broutant de l'herbe. Ils ne mangent pas de poisson car ceux-ci peuvent consommer des êtres vivants dans l'eau.

Les produits laitiers sont nombreux. Les femmes mongoles consacrent une grande partie de leur temps à fabriquer crèmes, yaourts, alcools de lait, fromage... Ce dernier est sec et souvent acide. Le lait de vache peut être aussi chauffé et mélangé à de la farine, donnant ainsi une mousse qui se transforme en crème jaune, puis en beurre rance. Le beurre fermenté sert à graisser des plats, il peut servir aussi de cire pour les bougies. La boisson la plus courante est le thé au lait salé, appelé süütei tsai, que l'on boit à toute occasion, mais la boisson nationale est le fameux alcool de lait de jument fermenté ("airag"), difficile à aborder pour certains européens. Il existe aussi en Mongolie-Intérieure un alcool de lait (jument, chamelle, brebis) distillé aux alentours de 50 % d'alcool.

Pour les Mongols, le sang des animaux est sacré et porteur d'énergie, et il serait tabou de le laisser s'écouler sur le sol extérieur. Ils abattent le bétail sans l'égorger. Une fois le ventre de l'animal ouvert, le sacrificateur se saisit du cœur qui bat encore. C'est alors seulement qu'il coupe l'aorte, et que les femmes viennent récupérer le sang en le puisant du ventre, pour le préparer en boudins qui seront consommés.

Le vêtement traditionnel des mongols est le "deel". Il est fabriqué avec des peaux d'agneaux, ou en soie pour les grandes occasions. Les "deel" d'hiver sont rembourrés avec de la laine. Le "deel" est fréquemment accompagné de bottes et d'une ceinture en cuir. Les hivers étant très rudes, on chausse pendant cette période des bottes de laine voire de feutre, et l'on se couvre la tête avec des chapeaux en fourrure de renard ou de zibeline. Dans le désert de Gobi, les Mongols portent des turbans et des écharpes à la manière des touaregs. Enfin, les Mongols produisent du cachemire en duvet de chèvre, vendu à prix d'or en Europe.

La principale fête des Mongols est le "Tsagaan Sar" (littéralement « mois blanc » ou « lune blanche » : fête du Nouvel An). Elle est l'occasion de « festins » où l'on se permet de manger des denrées plus rares, plus sucrées.
Il existe trois jeux traditionnels mongols : le tir à l'arc, les courses de chevaux, et la lutte mongole. Ces trois sports sont pratiqués notamment lors de tournois et festivals, le plus célèbre étant le festival de Naadam. Les Mongols sont en effet experts dans l'équitation, car les éleveurs passent la moitié de leur vie sur un cheval, d'où les jambes arquées caractéristiques. Les soldats mongols étaient réputés les meilleurs du monde outre leurs capacités de résistance face à la chaleur comme au froid, leur capacité à se priver de nourriture pendant plusieurs jours, les archers mongols étaient considérés comme étant les meilleurs archers montés du monde, capables de tirer en arrière en se repliant au galop (voir aussi armée mongole). Les courses de chevaux se font en rase campagne, sur des distances comprises entre et suivant l'âge des chevaux. Pour plus de rapidité, les chevaux sont parfois montés par des jeunes de moins de 12 ans... Les mongols mesurent également leur adresse lors de la capture des chevaux à l'aide de lassos. La lutte mongole se fait dans des habits traditionnels, et n'a aucun classement de poids.
Le soir venu, les Mongols ont pour habitude de faire des jeux intellectuels dans la yourte, comme les échecs mongols, appelés "Shatar", mais aussi d'autres jeux moins complexes comme l"'ail ger" ou l'"alag malkhii", le plus populaire étant le "shagai", un jeu d'osselets.

La musique est un élément important de la société traditionnelle mongole, et les Mongols pensent qu'elle appelle la bonne fortune. On chante n'importe où, pour passer le temps, bercer les bébés, porter chance ou encourager quelqu'un. Le "khöömei" est un chant diphonique lyrique et puissant, accompagné du luth et de la vièle, hérité des tibétains.
Il existe plusieurs instruments traditionnels, dont les principaux sont le "dombra", le "khuuchir", le "limbe", le "morin khuur", le "shanz" et le "yotga".

La population mongolophone n'est pas unie par la langue, bien que certaines langues jouissent d'une position prépondérante. Les mongols peuvent être divisés en quatre groupes, sur des critères à la fois géographiques, linguistiques, historiques et ethniques. Quatre langues mongoles se démarquent des autres par leur nombre de locuteurs et leur littérature : le kalmouk bien vivace, le khalkha dominant en Mongolie, le tchakhar dominant en Chine, et le bouriate au nord, quoique menacé par le russe.

Le groupe septentrional, largement dominé par le bouriate, s'étend dans les régions du lac Baïkal (comprenant la Bouriatie, le kraï de Transbaïkalie et l'oblast d'Irkoutsk), le nord-est de la Mongolie et le nord de la Mongolie-Intérieure. Il ne comprend que deux langues : le bouriate et le barga, ayant chacune deux dialectes. Les Bouriates sont divisés en deux groupes sur des bases géographiques et socio-économiques, ayant chacun leur dialecte : les Ekhirit-Bulagat, en Cisbaïkalie (à l'ouest et au nord du lac Baïkal), et les Khori en Transbaïkalie et en Mongolie. Il existe une littérature bouriate (écrite en cyrillique), et la langue est officielle dans la République de Bouriatie (elle l'était également dans les anciens okrugs autonomes d'Oust-Orda et d'Aga-Bouriatie).
Les Bargas, de même, se divisent entre Nouveaux-Bargas et Vieux-Bargas, sur des bases historiques. Ils sont présents en Mongolie-Intérieure du Nord.

Le groupe occidental, qui est le groupe des Oïrates, rassemble le plus grand nombre d'ethnies. Il s'étendent en Mongolie Occidentale, en Dzoungarie, dans le Xinjiang (Turkestan), et dans le Qinghai, ainsi qu'en Kalmoukie. Mais, à part le kalmouk isolé, les langues oïrates ne sont plus vivaces et sont de plus en plus imprégnées par le khalkha, le chinois, et le turc. Le kalmouk (ou "kalmyk"), officiel en Kalmoukie, reste bien vivant et bénéficie d'une littérature suffisamment riche. Il s'écrit en alphabet cyrillique et est un peu influencé par le russe. Les autres langues occidentales, en revanche, ont leur écriture propre, appelée « écriture claire » ("todo üseg"). Elle a été créée par le moine bouddhiste Zaya Pandita.

Le groupe oriental se compose de cinq langues : le khalkha, le tchakhar, l'ordosse, le toumète et le dariganga. Le khalkha, langue officielle de la Mongolie, correspond normalement à ce que nous désignons comme la langue mongole, c'est-à-dire celle apprise par les étrangers. En effet, les Khalkhas dominant la Mongolie par leur urbanisation, ils se sont servis de leur langue comme base d'homogénéisation des langues mongoles. Ils sont majoritaires en Mongolie, et de ce fait leur langue n'est pas menacée et bénéficie d'une littérature riche. Le tchakhar, quant à lui, jouit d'une situation similaire en Mongolie-Intérieure, bien que concurrencé par le chinois. Il a servi de base, avec le toumète, pour la formation de la langue officielle de la région autonome.

Enfin, le groupe méridional, situé entre le Qinghai et le Gansu, est un amas de langues mineures aux influences diverses. Leurs situations sont variées. Le groupe se compose du baoan, du dongxiang, du monguor, du youghour et du kangjia.

Les langues mongoles constituent l'une des trois branches de la famille altaïque : les deux autres sont le turc et le toungouse. Les ressemblances entre ces trois branches sont surtout structurelles plutôt que lexicales. Des ressemblances ont également été constatées avec le coréen et le japonais, ce qui a amené certains linguistes à les faire rejoindre la famille altaïque. Des analogies structurelles ont été remarquées aussi pour les langues ouraliennes, la principale étant la construction du lexique par agglutination (c'est-à-dire en ajoutant des suffixes et préfixes sans modifier le radical, contrairement aux langues indo-européennes qui sont flexionnelles). Enfin, comme les autres langues altaïques, le mongol a une harmonie vocalique et a sept cas: nominatif, génitif, accusatif, datif, ablatif, instrumental et comitatif.

Il existe plusieurs alphabets associé au mongol. L'écriture Phagspa (ou écriture carrée), créée par le tibétain du même nom, était utilisée sous l'Empire Yuan en Chine (dynastie fondée par Kubilaï Khan, descendant de Gengis Khan). Les oïrates, quant à eux, utilisaient un autre alphabet venu du Tibet, l'« écriture claire » ("todo üseg"), toujours en vigueur (sauf pour les kalmouks). À l'époque soviétique, les langues mongoles utilisaient un alphabet cyrillique modifié. Enfin, à l'effondrement de l'URSS, les khalkhas se sont inscrits dans une logique d'occidentalisation accélérée tout en revenant sur une identité plus ancienne, basée sur l'Empire Mongol et les croyances traditionnelles. Ainsi, l'enseignement de l'alphabet traditionnel mongol, qui s'écrit verticalement, a été rétabli en 1990. Apparu au , il est dérivé de l'ancien alphabet ouïghour. Celui-ci venait de l'alphabet sogdien, en Asie Centrale, lui-même tirant ses origines dans l'alphabet syriaque, voisin de l'alphabet arabe. Cette origine sémitique explique notamment le caractère changeant des lettres suivant leur place dans le mot. Par ailleurs, remis à l'horizontale, l'écriture mongole rappelle un peu par son apparence l'alphabet arabe. En 2012, l'alphabet officiel reste l'alphabet cyrillique modifié.


Les premiers temples bouddhistes en Mongolie ont été construits durant la période des Huns, qui débuta lors du avant Jésus-Christ. Bien que le bouddhisme ait été plusieurs fois officialisé en Mongolie à travers l'Histoire, il ne fut réellement pratiqué par le peuple qu'à partir du . Le sommet du développement du Bouddhisme en Mongolie fut atteint à la fin du - début du . Les monastères se sont multipliés, accueillant de nombreux lamas.
À la suite de la révolution populaire de 1921, le Bouddhisme commença toutefois à décliner, et dans les années 1930 et 1940, la quasi-totalité des monastères mongols furent détruits par le gouvernement dans un effort pour abolir la religion de l'état.

La forme du bouddhisme pratiquée en Mongolie a extrêmement été influencée par les pratiques rituelles et mystiques du tantrisme, reflété concrètement par le pouvoir surnaturel des mots sacrés. Cette croyance prend la forme de livres ou drapeaux imprimés et de mantras récités. Dans l'école gelugpa du bouddhisme tibétain, le livre est l'objet le plus sacré. Il est la source de sagesse aidant l'Homme à échapper à sa souffrance. En cas de maladie ou de malheur, ainsi qu'aux périodes indiquées par les tables astrologiques, les Mongols considèrent opportun de faire lire des livres par des lamas.

Les Mongols croient également que placer un drapeau de prière ("khiimoriin dartsag") sur un poteau à l'arrière de leur yourte leur apportera soutien dans leur vie quotidienne. Les mots inscrits sur leur drapeau sont supposés être « activés » par le souffle continu du vent. De la même manière, les prières peuvent être activés en portant un livre à la tête, ou en faisant tourner un « moulin à prières » contenant des centaines de prières.





</doc>
<doc id="14992" url="https://fr.wikipedia.org/wiki?curid=14992" title="Xi'an">
Xi'an

Xi'an ( ; EFEO Si-ngan-fou) est la capitale de la province du Shaanxi en Chine. Elle a le statut de ville sous-provinciale. Cette ville, qui a une histoire de plus de ans, a été la capitale de la Chine et se nommait alors Chang'an. L'actuelle Xi'an se classe dans les dix plus grandes villes chinoises. Elle compte plus de huit millions d'habitants enregistrés. 

Xi'an s'élève à au-dessus du niveau de la mer, et la ville elle-même couvre .

Le climat est de type subtropical humide (Köppen: Cwa ou Cfa ), qui se caractérise par des étés chauds et humides et des hivers frais. Les températures moyennes vont d'environ - pour le mois le plus froid à + pour le mois le plus chaud, avec une moyenne annuelle de +; la pluviométrie y est de par an. 

La région est donc relativement sèche: depuis 2001, un canal de 86 kilomètres lui apporte annuellement 428 millions de m³ d'eau en provenance du Fleuve Jaune.

La population résidente de la préfecture était, en 2010, d'après le recensement officiel de habitants, et celle de la ville de Xi'an de habitants.

On y parle le dialecte de Xi'an du mandarin zhongyuan.

Autrefois nommé Hao () ou Zōngzhōu (), pendant la dynastie Zhou, elle fut la capitale de la Chine pour la période des Zhou occidentaux. À la suite de la folie du roi Zhou Youwang, la ville fut incendiée et pillée par les barbares Rong.

Xi'an est l'extrémité est de la route de la soie considérée comme ayant été « ouverte » par le général chinois Zhang Qian au C'était l'une des Quatre Grandes Capitales Anciennes car ce fut la capitale de la Dynastie Qin (221 — 210 av. J.-C.), des Han (202 av. J.-C. — 220), alors connue sous le nom de Chang'an, et des Tang (618-907). Sous cette dernière dynastie, c'est l'une des plus grandes villes du monde (deux millions d'habitants, soit dix fois plus que Constantinople ou Cordoue, mille fois plus qu'Aix-la-Chapelle au temps de Charlemagne). En 763, les cavaliers de Trisong Detsen (740 — 797), empereur de l'Empire du Tibet, envahissent Xi'an. L'empereur chinois Daizong de la dynastie Tang s'étant enfui, les Tibétains nommèrent un nouvel empereur. 

À Xi'an se trouve une stèle de pierre qui prouve la présence de chrétiens nestoriens en Chine dès le , probablement venus de Perse par la route de la soie.

À la suite du traité de paix sino-tibétain de 822, une stèle connue sous le nom de « Tablette de pierre de l’Unité du long Terme » fut érigée devant la porte principale du Temple de Jokhang à Lhassa et dont il existerait deux autres copies, l'une à Xi'an (Chang'an) à la porte de l'empereur, et l'autre à la frontière tibéto-chinoise d'alors (Songpan ?). Y sont inscrits les termes du traité d'alliance.
Cette ville possède une communauté musulmane dont la présence remonte aux commerçants arabes ou persans venus par la Route de la Soie au Moyen Âge. Elle possède une étonnante mosquée de style chinois très ancienne.

Le célèbre moine Xuanzang y traduisit de 645 à sa mort en 664 les textes sacrés bouddhiques qu'il avait rapporté de son voyage en Inde, commencé en 629.

La cité a plus de 3000 ans d'histoire et elle possède encore son enceinte fortifiée très visible par photo satellite.

C'est l'endroit où se déroule en 1936 l'incident de Xi'an, puis est signé l'Accord du même nom : il met en place une trêve entre le Parti communiste chinois et le Kuomintang et permet aux deux camps de se consacrer à la guerre contre le Japon.

Dans la ville elle-même, plusieurs bâtiments datent de la dynastie Tang : la grande pagode de l'oie sauvage, la petite pagode de l'oie sauvage, la forêt de stèles (un musée de calligraphie) et la grande mosquée. Les remparts de Xi'an datent de la dynastie Ming, tout comme la tour de la cloche et la tour du tambour. Le musée de l'histoire du Shaanxi rassemble des pièces de collection de toutes les dynasties chinoises. 

La stèle nestorienne de Xi'an exposée dans le musée de la Forêt de stèles est un témoin exceptionnel de la présence de chrétiens nestoriens en Chine dès le .

Dans les environs, le site le plus extraordinaire à visiter est le mausolée de l'empereur Qin, célèbre pour son armée enterrée composée de guerriers et chevaux en terre cuite de grandeur nature ; vieille de ans, elle ne fut découverte qu'en 1974 par des paysans alors qu'ils construisaient un puits. En revanche, sa tombe, située à environ à l'ouest, et recouverte d'un tumulus, n'a pas encore été fouillée par les archéologues, et n'est pas ouverte aux visiteurs.

Les mausolées des empereurs des Han occidentaux et des Tang se trouvent également dans les environs, mais peu d'entre eux sont fouillés jusqu'à nos jours faute de techniques de protection de ces immenses richesses culturelles enterrées. Les sites sont néanmoins devenus des curiosités touristiques telles que le mausolée de Qianling (qui contient la tombe de Wu Zetian) et ceux de Jingdi et Wudi de la dynastie Han.

À environ à l'ouest de Xi'an se trouve le temple Famen qui comporte deux parties. L'une date de la dynastie Zhou du Nord et l'autre, très moderne, est un nouveau complexe achevé en mai 2009, surmonté d'une pagode de de haut.

Le temple Guangren (广仁寺) est un temple lamaïque (bouddhisme du Tibet et Mongolie).

Xi'an possède une desserte aérienne internationale via l'Aéroport international Xi'an-Xianyang, situé à environ au Nord-Ouest du centre ville. L'aéroport est desservi par les principales compagnies aériennes chinoises et quelques compagnies asiatiques.

Au niveau des liaisons ferroviaires, la ville comporte deux gares :

Des liaisons en TAGV existent vers différentes villes, comme notamment Beijing, Wuhan, Changsha, Guangzhou, Zhengzhou. La plus éloignée est Shenzhen, à 9 heures de TAGV. La ville la plus proche est Weinan à 20 minutes de TAGV.
Dans la Province du Shaanxi, d'autres relations TAGV sont prévues: Xi'an-Baotou, Xi'an-Ankang et Xi'an-Chengdu, Xi'an-Yinchuan, Xi'an-Wuhan, Xi'an-Chongqing et Xi'an-Nanjing.

Il est possible de se déplacer dans la ville via les nombreuses lignes de bus, des taxis ou de tuk-tuk.

La plupart des scooters sont électriques.

Comme dans toutes les grandes villes chinoises un grand nombre de gratte-ciel ont été construits à Xi'an depuis les années 1980.
Le plus haut gratte-ciel de la ville est le Greenland Center (Xi'an) haut de 270 mètres et achevé en 2016
Xi'an est jumelée avec

La ville sous-provinciale de Xi'an exerce sa juridiction sur treize subdivisions - neuf districts et quatre "xian" :



</doc>
<doc id="14993" url="https://fr.wikipedia.org/wiki?curid=14993" title="Théorie des probabilités">
Théorie des probabilités

La théorie des probabilités en mathématiques est l'étude des phénomènes caractérisés par le hasard et l'incertitude. Elle forme avec la statistique les deux "sciences du hasard" qui sont partie intégrante des mathématiques. Les débuts de l'étude des probabilités correspondent aux premières observations du hasard dans les jeux ou dans les phénomènes climatiques par exemple.

Bien que le calcul de probabilités sur des questions liées au hasard existe depuis longtemps, la formalisation mathématique n'est que récente. Elle date du début du avec l'axiomatique de Kolmogorov. Des objets tels que les événements, les mesures de probabilité, les espaces probabilisés ou les variables aléatoires sont centraux dans la théorie. Ils permettent de traduire de manière abstraite les comportements ou des quantités mesurées qui peuvent être supposés aléatoires. En fonction du nombre de valeurs possibles pour le phénomène aléatoire étudié, la théorie des probabilités est dite "discrète" ou "continue". Dans le cas discret, c'est-à-dire pour un nombre au plus dénombrable d'états possibles, la théorie des probabilités se rapproche de la théorie du dénombrement ; alors que dans le cas continu, la théorie de l'intégration et la théorie de la mesure donnent les outils nécessaires.

Les objets et résultats probabilistes sont un support nécessaire à la statistique, c'est le cas par exemple du théorème de Bayes, de l'évaluation des quantiles ou du théorème central limite et de la loi normale. Cette modélisation du hasard permet également de résoudre plusieurs paradoxes probabilistes.

Qu'il soit discret ou continu, le calcul stochastique est l'étude des phénomènes aléatoires qui dépendent du temps. La notion d'intégrale stochastique et d'équation différentielle stochastique font partie de cette branche de la théorie des probabilités. Ces processus aléatoires permettent de faire des liens avec plusieurs domaines plus appliqués tels que les mathématiques financières, la mécanique statistique, le traitement d'images, etc.

Avant que l'étude des probabilités soit considérée comme une science, l'observation du hasard dans les événements naturels a amené les philosophes et les scientifiques à réfléchir sur la notion de liens entre événements, causes et conséquences, et lois de la nature. Les jeux de hasard, les situations météorologiques ou les trajectoires des astres ont fait partie des domaines étudiés. Les explications données sont alors liées au destin, à une colère céleste ou à une présence divine.

Il est communément admis que le début de la science des probabilités se situe au avec l'analyse de jeux de hasard par Jérôme Cardan et au avec les discussions entre Pierre de Fermat et Blaise Pascal au sujet du problème des partis posé par Antoine Gombaud, chevalier de Méré : comment partager les gains quand la partie est interrompue à un moment quelconque. Cette nouvelle théorie est nommée "géométrie aléatoire" par le chevalier de Méré en 1654, elle est appelée par la suite "calcul conjectural", "arithmétique politique" et plus communément aujourd'hui "théorie des probabilités". Cette théorie, dite des probabilités modernes, est alors étudiée par de nombreux penseurs jusqu'au : Kepler, Galilée, Leibniz, Huygens, Halley, Buffon, les frères Bernoulli, Moivre, Euler, D'Alembert, Condorcet, Laplace, Fourier. Elle est principalement basée sur les événements discrets et la combinatoire.

Au début du , Kolmogorov fit la connexion entre la théorie de la mesure de Borel, la théorie de l'intégration de Lebesgue et les probabilités.

Des considérations analytiques ont forcé l'introduction de variables aléatoires continues dans la théorie. Cette idée prend tout son essor dans la théorie moderne des probabilités, dont les fondations ont été posées par Andreï Nikolaevich Kolmogorov. Kolmogorov combina la notion d'univers, introduite par Richard von Mises et la théorie de la mesure pour présenter son système d'axiomes pour la théorie des probabilités en 1933. Très vite, son approche devint la base incontestée des probabilités modernes.

Le voit également le développement de l'application de la théorie des probabilités dans plusieurs sciences.

Avec la mécanique newtonienne, la théorie du champ électromagnétique ou la thermodynamique, la physique classique est la théorie utilisée jusqu'à la fin du . En 1929, Erwin Schrödinger étudie l'équation qui détermine l'évolution d'une onde au cours du temps : l'équation de Schrödinger. Max Born utilise cette équation pour décrire une collision entre des particules telles que des électrons ou des atomes. Les observations de ses expériences l'amène à supposer que la fonction d'onde est la probabilité que la particule soit détectée en un point de l'espace. C'est le début d'une nouvelle approche de la physique quantique.

En 1900, Louis Bachelier fut un des premiers mathématiciens à modéliser les variations de prix boursiers grâce à des variables aléatoires. « le marché n'obéit qu'à une seule loi : la loi du hasard ». Bachelier utilise alors le calcul stochastique pour étudier les variations boursières au cours du temps. En 1970, Fischer Black et Myron Scholes reprennent les idées de Bachelier pour modéliser les rendements d'une action grâce à leur modèle Black-Scholes.

L'utilisation des probabilités en biologie a pris un essor dans les années 1970, notamment dans l'étude de l'évolution des espèces. La reproduction des individus est modélisée par un choix aléatoire des gènes transmis ainsi que des mutations apparaissant de manière aléatoire sur les individus. L'extinction des espèces ou des gènes est alors étudiée en fonction des effets stochastiques.

Suivant les époques ou les domaines d'application, la théorie des probabilités peut prendre des noms différents : la théorie de la probabilité mathématique, le calcul des probabilités, ou plus simplement les probabilités bien qu'il ne faille pas confondre avec "une probabilité" qui est une loi (ou mesure) de probabilité ou "la probabilité" d'un événement qui est l'évaluation de son caractère probable. Pour ce dernier terme, voir les différentes approches d'une probabilité.

La théorie des probabilités a évolué au cours de son existence. Dans son cours vers 1893, Henri Poincaré s'exprime ainsi : « On ne peut guère donner de définition satisfaisante de la Probabilité. On dit généralement ... etc. ». Cependant, il est toujours fait mention de l'étude de notions comme le hasard, l'aléa, la chance ou encore le caractère d'un événement. Une définition peut être donnée sous la forme : 
C'est-à-dire que la théorie des probabilités est un domaine des mathématiques. Ce n'a pas toujours été le cas, cette théorie a été rattachée à la théorie des jeux de hasard, à la philosophie, les géomètres ont été parmi les premiers scientifiques à utiliser le calcul des probabilités. Il est à noter que le groupe mathématique Bourbaki, créé en 1930 et dont le but est de proposer une présentation cohérente des mathématiques, a été critiqué pour ne pas avoir pris suffisamment en considération la théorie des probabilités : « Bourbaki s'est écarté des probabilités, les a rejetées, les a considérées comme non rigoureuses et, par son influence considérable, a dirigé la jeunesse hors du sentier des probabilités. » soulignait Laurent Schwartz dans son autobiographie.

Pour pleinement appartenir aux mathématiques, la théorie des probabilités a eu besoin d'une axiomatique. Plusieurs constructions sont proposées au début du comme la "théorie des collectifs" de Richard von Mises ou l'axiomatique de Andreï Kolmogorov. Cette dernière étant la plus pratique des axiomatiques disponibles à l'époque a été adoptée définitivement par les scientifiques à partir de 1950. Elle a permis de pouvoir étudier le calcul des probabilités au-delà des probabilités finies, dites "théorie discrète des probabilités" et de considérer un cadre plus général pour la théorie des probabilités. Dans cette axiomatique, la théorie des probabilités est basée sur un espace probabilisé et ainsi beaucoup de notions correspondent à des notions de la théorie de l'intégration. Cependant dans la théorie des probabilités, le but est de proposer un modèle prédictif pour une expérience aléatoire.

L'intérêt de cette construction plutôt abstraite est qu'elle permet une explication globale des calculs de probabilités notamment de pouvoir considérer à la fois les théories des probabilités discrète et continue (voir sections suivantes). Elle permet également de résoudre des problèmes probabilistes en cours au début du comme les paradoxes qui occupaient tant les scientifiques tels que Joseph Bertrand (voir le paradoxe de Bertrand), Émile Borel (voir le paradoxe du singe savant), etc.

Donnons une modélisation sous forme d'espace probabilisé d'un exemple simple : le lancer d'un dé usuel cubique parfaitement équilibré.

Cependant cette axiomatique n'est pas nécessaire pour calculer des probabilités dans des cas simples notamment dans le cas discret : il est facile de calculer que la probabilité d'obtenir un numéro pair dans le lancer de dé précédent est de 1/2.

Il est également possible de décrire une situation plus complexe. On considère au déplacement d'une particule soumise à des perturbations aléatoires. Si on se limite à l'espace de temps formula_19, l'espace de probabilité naturel est l'ensemble des fonctions continues à valeurs dans formula_20. Autrement dit, un élément de formula_1 est une fonction continue formula_22. La tribu associée à cet espace est la plus petite tribu qui rende toutes les applications formula_23 mesurables. Reste à définir la mesure de probabilité, un exemple est de prendre la mesure de Wiener, c'est-à-dire la loi d'un mouvement brownien. Le choix de cet espace n'est généralement pas donné, on s'intéresse directement aux propriétés des fonctions définies sur cet espace.

L'espace probabilisé formula_24 construit dans la section précédente est un espace abstrait. Il n'est pas forcément adapté pour effectuer des calculs. Lorsque les résultats possibles de l'expérience aléatoire ne sont pas des nombres, c'est le cas des résultats "pile" et "face" dans un lancer de pièce, il est utile de pouvoir associer une valeur numérique à chaque résultat. Une variable aléatoire remplit ce rôle.

Une variable aléatoire est une application mesurable formula_25 où formula_26 est un espace mesurable. C'est-à-dire qu'à chaque éventualité formula_27 est associée une valeur formula_28. Si cette valeur est réelle, la variable aléatoire est dite réelle.

Comme précisé précédemment, il n'est pas toujours utile de définir l'espace probabilisé formula_24, mais il est possible de donner directement les variables aléatoires sur l'espace formula_26. La variable aléatoire s'écrit simplement formula_31 au lieu de formula_28.

De la même manière qu'il existe des cas continus et discrets pour la théorie des probabilités, il existe des variables aléatoires discrètes et continues. Il est possible de considérer un vecteur aléatoire comme une variable aléatoire multidimensionnelle : formula_33. Lorsque la dimension "n" du vecteur n'est plus finie mais infinie, on parle de marche aléatoire ; lorsque la dimension est infinie non dénombrable, on parle de processus stochastique (voir la section "Calcul stochastique" ci-dessous).

Donnons un exemple simple du lancer de deux dés, ce qui est équivalent à lancer deux fois un dé. Une première variable formula_34 aléatoire donne le résultat du premier lancer, une deuxième formula_35 donne le résultat du deuxième lancer, c'est-à-dire formula_36 et formula_37 que l'on note plus simplement formula_38 et formula_39.

Il est possible de s'intéresser à la somme des deux résultats, qui peut être notée par une variable aléatoire : formula_40.

La théorie des probabilités est dite discrète lorsque l'ensemble formula_1 de l'espace probabilisé est fini ou dénombrable. Le plus simple exemple d'étude en théorie des probabilités discrète est le jeu de pile ou face, dans ce cas l'univers formula_1 ne contient que deux éléments : "pile" et "face". Les études d'un lancer de dé, d'un tirage d'une carte dans un jeu de cartes ou par exemple du loto font également parties de la théorie des probabilités discrète.

Avant que la théorie de la mesure soit introduite, la probabilité d'un événement a été définie comme le nombre de cas favorables divisé par le nombre de cas possibles. De manière plus pratique, une expérience aléatoire était répétée un nombre "N" de fois, le nombre de fois où l'événement "A" est réalisé est noté formula_43. Lorsque "N" tend vers l'infini, la proportion formula_44 converge vers une valeur dite probabilité de "A".

Cependant ce raisonnement n'est pas si simple pour toute question relative à une expérience aléatoire. Les différentes manières de compter ont amené des paradoxes probabilistes. L'axiomatique de Kolmogorov (voir la section ci-dessus) a permis de résoudre ces problèmes. Dans le cas de la théorie discrète, pour une expérience non répétée, l'axiomatique s'écrit :
où le choix formula_46 peut être effectué pour représenter les différents résultats équiprobables de l'expérience. Plusieurs choix de tribu sont possibles, cependant il est raisonnable pour une étude discrète de choisir la tribu de l'ensemble des parties puisqu'elle contient tous les événements possibles :
Dans le cas de la théorie discrète, la mesure de probabilité possède la particularité de pouvoir être définie uniquement sur les singletons : formula_48. Les probabilités des autres événements s'obtiennent grâce aux axiomes des probabilités (voir la section ci-dessus).

Lorsque l'univers formula_1 est fini, contenant "n" éléments, il est possible de choisir la mesure uniforme : formula_50 et ainsi obtenir la formule utile et cohérente à l'intuition des scientifiques plus anciens :
Grâce à l'utilisation de ces formules, la théorie des probabilités discrète repose sur la théorie des combinaisons, aujourd'hui appelée la combinatoire et le dénombrement.

Reprenons l'exemple du lancer de deux dés. L'ensemble de tous les possibles est :
C'est-à-dire que formula_1 contient tous les couples de deux chiffres, le premier correspondant au résultat du premier dé, le deuxième au résultat du deuxième. Un choix possible pour la tribu formula_2 est l'ensemble des parties :
Le choix de l'espace formula_6 est fait de telle sorte que les singletons de formula_1 aient tous la même probabilité : ils sont dits équiprobables. Il est alors possible de calculer les probabilités de plusieurs événements comme 

La théorie des probabilités est dite continue lorsque l'univers formula_1 n'est plus dénombrable mais quelconque, possiblement non topologique. C'est-à-dire lorsque la théorie des probabilités n'est plus discrète.

Il est possible de choisir plusieurs tribus, cependant lorsque l'univers est l'ensemble des réels, il est classique de lui munir la tribu borélienne qui possède de bonnes propriétés. Si ce n'est pas le cas, l'utilisation des variables aléatoires permet de représenter l'univers par l'ensemble des réels formula_66. Le terme théorie des probabilités continue est également utilisé pour désigner le cas où la variable aléatoire, ou la loi de probabilité, associée est absolument continue, c'est-à-dire qu'elle possède une densité.

La mesure de probabilité se définit plus facilement sur formula_66, c'est-à-dire qu'il est plus facile de définir la loi de probabilité de la variable aléatoire :

Dans certains cas de la théorie des probabilités continue, la variable aléatoire réelle est dite absolument continue par rapport à la mesure de Lebesgue, c'est-à-dire qu'il existe une fonction formula_74 telle que :
où le terme formula_76 dans l'intégrale est une indicatrice. La fonction formula_77 est appelée la densité de probabilité de formula_31.

Grâce à l'utilisation de ces formules, la théorie des probabilités continue repose sur la théorie de l'intégration.

Des algorithmes de calcul utilisent des valeurs choisies de manière uniforme entre 0 et 1. C'est-à-dire que l'on choisit (aléatoirement) une valeur réelle entre 0 et 1 telle qu'aucune des valeurs n'ait plus de chance d'apparaître qu'une autre. Pour formaliser cette expérience, il y a un espace probabilisé formula_79 non détaillé ici, cependant on se donne une variable aléatoire formula_31 à valeurs dans formula_81 muni de sa tribu borélienne formula_82 ainsi que les probabilités :

Plusieurs formules dites élémentaires se déduisent des axiomes des probabilités (voir la section ci-dessus). Certaines sont intuitives, d'autres le sont moins.

Il est à noter que toute tribu contenant l'ensemble vide, le deuxième axiome des probabilités permet d'obtenir sa probabilité : formula_86. Un événement de probabilité nulle est appelé ensemble négligeable, ensemble formula_8-négligeable, ou ensemble impossible. Il existe des ensembles négligeables autres que l'ensemble vide. Par exemple la probabilité d'obtenir le résultat pile à chaque lancer lors d'une infinité de lancers de pile ou face est nulle.

Il est possible de calculer la probabilité de la négation d'une proposition ; mathématiquement, c'est la probabilité du complémentaire d'un ensemble. Il est également possible d'obtenir la probabilité de se trouver dans une configuration ou dans une autre, cela correspond à une union de deux ensembles. Quant à la probabilité de se retrouver dans deux situations simultanément, c'est la probabilité de l'intersection des deux ensembles. Elle est nulle si et seulement si les deux ensembles sont disjoints. En notant formula_88 le complémentaire d'un événement formula_71 et respectivement formula_90 et formula_91 la réunion et l'intersection de deux événements formula_71 et formula_93, on a :

Reprenons l'exemple du lancer de deux dés.

La probabilité d'obtenir au moins une fois un 6 se calcule à partir de la probabilité de ne pas obtenir de 6 lors des deux lancers :
Cet événement est le même qu'obtenir un 6 au premier lancer ou un 6 au deuxième lancer. Sa probabilité s'obtient également par le calcul de la probabilité de l'union :

La notion d'indépendance est une hypothèse utilisée depuis longtemps en théorie des probabilités. On dit que deux événements sont indépendants lorsque le fait de connaître le résultat du premier événement ne nous aide pas pour prévoir le second et inversement. Plusieurs lancers de dés successifs sont considérés indépendants. Dans ce cas l'hypothèse est raisonnable, cependant d'autres situations d'indépendance peuvent paraître indépendantes alors qu'elles ne le sont pas. C'est le cas par exemple du problème de Monty Hall. L'indépendance n'est pas toujours intuitive et demande alors d'être étudiée.

L'indépendance peut se définir sur les ensembles, deux événements "A" et "B" sont dits indépendants si la probabilité que "A" apparaissent ne dépend pas de la connaissance de l'obtention de "B". Mathématiquement, les événements sont indépendants si et seulement si la probabilité de leur intersection est égale au produit de leur probabilité :
L'indépendance se définit également pour les variables aléatoires en utilisant la formule précédente. Les variables aléatoires "X" et "Y" sont indépendantes si :
en reprenant les notation de la section "variable aléatoire" et formula_103 pour les variables aléatoires réelles.

De même, des tribus formula_104 et formula_105 sont dites indépendantes si :

Lorsque l'on considère plusieurs événements, variables aléatoires ou tribus, il existe plusieurs notions d'indépendance. Les événements "A", "B" et "C" sont dits
Ces définitions se généralisent pour plus de trois événements, variables aléatoires ou tribus, possiblement un nombre infini.

À partir des probabilités élémentaires, il est possible de définir la probabilité conditionnelle d'un événement "A" sachant qu'un autre événement "B" est réalisé, notée formula_111 ou formula_112. Si formula_113 alors la probabilité de "A" sachant "B" est définie par :
Plus mathématiquement, formula_115 est une nouvelle mesure de probabilité, elle permet de définir des espérances conditionnelles ou des lois conditionnelles. De manière plus générale, il est possible de définir la probabilité conditionnelle sachant une variable aléatoire, une probabilité conditionnelle sachant une tribu, une densité conditionnelle, etc.

Cette formule simple permet de faire le lien entre formula_111 et formula_117 par le très utile théorème de Bayes :
De même que la remarque précédente, il est possible de donner d'autres versions du théorème de Bayes par un conditionnement utilisant des variables aléatoires, des tribus ou par l'intermédiaire de lois de probabilité.

Il est possible de décomposer la probabilité d'un événement en probabilités conditionnelles sachant toutes les situations possibles. C'est le rôle de la formule des probabilités totales : pour une partition d'événements formula_119, possiblement infinie, 
Une manière de représenter cette formule est un arbre de probabilité, chaque branche représente un cas possible.

Reprenons l'exemple des deux dés. Considérons les deux événements formula_121 : « le résultat du premier lancer est i », "B" : « le résultat de la somme des deux lancers est 7 » et "C" : « le résultat du premier lancer est pair ». Il est facile de calculer les probabilités : formula_122, formula_123 et formula_124. La formule des probabilités totales permet d'obtenir :
Le théorème de Bayes permet d'obtenir la probabilité d'avoir eu un résultat pair au premier lancer sachant que la somme des deux résultats est de 7 :

Comme précisé dans les sections ci-dessus, le choix de la mesure de probabilité pour l'espace probabilisé peut se faire en donnant directement les probabilités formula_128 d'une variable aléatoire "X". Ainsi la mesure de probabilité donnée par :

est appelée la loi de probabilité de la variable "X". Elle décrit complètement le comportement de la variable "X". De manière plus générale, une loi de probabilité est une mesure décrivant le comportement aléatoire d'un phénomène dépendant du hasard, c'est-à-dire qu'elle n'est pas toujours définie à partir d'une variable aléatoire. Cependant pour une loi de probabilité donnée, il existe une variable aléatoire dont la loi est la loi de probabilité précédente. La représentation d'une loi par une variable aléatoire n'est pas unique, c'est-à-dire que deux variables aléatoires différentes peuvent avoir la même loi. Comme mentionné dans les sections précédentes, il existe des lois discrètes, des lois absolument continues, mais il existe également des lois plus générales. Les lois discrètes et les lois absolument continues peuvent s'écrire respectivement sous la forme :

Certaines lois de probabilité sont fréquemment rencontrées en théorie des probabilités car on les retrouve dans de nombreux processus naturels. Les lois discrètes les plus fréquentes sont la loi uniforme discrète, la loi de Bernoulli, ainsi que les lois binomiale, de Poisson et géométrique. Les lois uniforme continue, normale, exponentielle et gamma sont parmi les plus importantes lois continues.

Plusieurs outils permettent de définir et étudier ces lois. La fonction de répartition, la fonction caractéristique, la fonction génératrice, la fonction quantile, la densité de probabilité (pour les lois continues), la fonction de masse (pour les lois discrètes) en sont les exemples principaux.

L'espérance est une propriété des lois de probabilités mais elle s'écrit plus simplement en utilisant une variable aléatoire. Elle donne la moyenne de la variable aléatoire "X". L'espérance de la variable aléatoire "X" de loi formula_132 est donnée par :
Cette expression s'écrit de manière plus simple dans le cas des variables discrètes et des variables continues (en reprenant les notation de la section "Lois de probabilité") : formula_134 pour le cas discret et formula_135 pour le cas continu, si les séries et intégrales convergent.

Il est possible de calculer l'espérance d'une fonction de la variable aléatoire par la formule : pour toute fonction formula_136 mesurable
Lorsque la fonction formula_138 est suffisamment générale, alors formula_139 permet de récupérer la loi de "X". Pour la fonction indicatrice formula_140, l'espérance redonne la probabilité : formula_141. Pour les fonctions formula_142, les valeurs formula_143 sont les moments de la loi de "X".

Ces définitions sont valides pour tout espace de valeurs de la variable aléatoire. Dans le cas multidimensionnel, c'est-à-dire de vecteurs aléatoires réels, la notion de espérance se généralise en vecteur des moyennes et la variance en matrice de variance-covariance qui donne les variances des coordonnées sur la diagonale et les covariances entre coordonnées dans le reste de la matrice.

L'espérance et les moments permettent d'obtenir des inégalités : sans préciser les conditions d'existence,
Ces inégalités sont très utiles pour estimer la queue de la loi d'une variable aléatoire, c'est-à-dire le comportement de la variable aléatoire lorsqu'elle prend des valeurs éloignées de sa moyenne.

Lorsque l'on considère un nombre infini de données aléatoires, elles sont modélisées par une suite (infinie) de variables aléatoires. Il peut être utile d'étudier le comportement limite de cette suite. Plusieurs notions de convergences de variables aléatoires ont été définies et des théorèmes limites renseignent sur les résultats asymptotiques.

Une suite de variables aléatoires formula_146 :

Donnons quelques théorèmes limites importants :
Pour pouvoir utiliser ces théorèmes de convergence dans les applications, notamment informatiques, il est utile de connaître leur vitesse de convergence : c'est l'étude du principe de grandes déviations.

Le calcul stochastique est l'étude des phénomènes qui évoluent au cours du temps de manière aléatoire. Le temps peut être modélisé de manière discrète, c'est-à-dire par les valeurs entières : formula_172, dans ce cas le phénomène est représenté par une suite (infinie) de variables aléatoires : formula_173, c'est une marche aléatoire. Le temps peut également être modélisé de manière continue c'est-à-dire par des valeurs réelles formula_174 ou formula_175, il s'agit alors de processus stochastique formula_176.

Parmi les modélisations de phénomènes aléatoires dépendant du temps, certaines l'ont été par un temps discret, c'est-à-dire à valeurs entière : formula_177. Un processus formula_159 est appelé marche aléatoire partant d'un point formula_179 lorsque la variable formula_180 s'écrit sous la forme d'une somme de "pas" aléatoires donné par des variables :
L'espace de probabilité et la tribu sur lequel le processus est défini n'est pas trivial, la notion de filtration a donc été introduite. C'est une suite de tribu prévue pour que la marche aléatoire puisse être définie sur chaque tribu de la suite, le processus est dit "adaptée".

Une propriété particulière des marches aléatoires est régulièrement utilisée. Une marche aléatoire est appelée chaîne de Markov si elle possède la propriété de Markov, c'est-à-dire que le "n"-ième pas ne dépend pas du comportement du processus avant. Autrement dit, le comportement à venir ne dépend que du temps présent et non du temps passé. Plusieurs expressions mathématiques traduisent cette propriété, en voici une courante grâce aux probabilités conditionnelles :
La probabilité formula_185 est appelée la "probabilité de transition" de l'état formula_179 à l'état formula_187. Lorsque le nombre d'états possibles est fini. Toutes ces probabilités sont résumées dans une matrice stochastique, ou matrice de transition. Elle représente à elle seule la chaîne de Markov. La chaîne de Markov dont les états possibles sont les valeurs entières et telle que les probabilités d'aller vers les plus proches voisins sont identiques est appelée la "chaîne de Markov simple sur formula_188".

Les récurrence et transience d'une chaîne de Markov sont également étudiées. Si une marche aléatoire revient indéfiniment au point de départ elle est dite récurrente, sinon elle est transiente. Les temps d'arrêt représentent le temps en lequel la marche possède pour la première fois une certaine propriété.

Ces notions se généralisent de plusieurs manières : les pas peuvent être des vecteurs aléatoires multidimensionnels ; les états possibles peuvent être les points d'un graphe plus général, ceci introduit, entre autres, la théorie des graphes aléatoires et la théorie de la percolation qui font partie de la théorie des systèmes dynamiques ; le "n"-ième pas peut être la somme d'un nombre aléatoire de variables, c'est le cas des processus de branchement.

L'étude du comportement de la marche aléatoire lorsque le temps devient grand amène à considérer des théorème limites sur les processus tels que le théorème de Donsker ou le théorème de Glivenko-Cantelli très utilisés en statistique. Apparaissent alors des processus aléatoires dont le temps n'est plus discret mais continu.

L'introduction des processus aléatoires à temps continu a été possible notamment grâce à l'axiomatique de Kolmogorov. Les processus stochastiques sont des familles de variables aléatoires indexées par un indice réel : formula_189. De même que dans le cas du temps discret, les notions de filtration et de processus adapté, permettent de définir mathématiquement le processus. Les théorèmes d'extension de Kolmogorov et d'extension de Carathéodory permettent de donner l'existence via les lois finies dimensionnelles, c'est-à-dire que le processus est défini par la donnée d'un nombre fini de ses accroissements :

Des probabilités de transitions sont données par des fonctions du type : formula_191 qui donnent la probabilité que le processus soit dans un des états de l'ensemble "A" au temps formula_192 sachant qu'au temps formula_193 le processus était en formula_179, elle doit vérifiée l'équation de Chapman-Kolmogorov :

Un exemple important de processus stochastique est le mouvement brownien, il apparait comme limite (en loi) d'une suite de marches aléatoires via le théorème de Donsker, il est également un objet central puisque ses lois finies dimensionnelles sont des lois normales, c'est-à-dire que ses accroissements sont gaussiens. La loi du processus est appelée "mesure de Wiener". Le mouvement brownien a été beaucoup étudié et nombreux objets mathématiques lui sont liés : bruit blanc, mouvement brownien fractionnaire, processus de Lévy, pont brownien, arbre brownien, processus stationnaire, etc. Le processus de Poisson est un processus de Markov dont les accroissements sont de loi de Poisson, ce processus de comptage est un processus de sauts.

Différentes méthodes de définition existent : le processus de Feller est un processus de Markov dont les probabilités de transition possède une propriété dite de Feller, le processus d'Ornstein-Uhlenbeck est défini à partir d'une équation différentielle stochastique (voir la section ci-dessous), les processus ponctuels sont définis sur des espaces plus généraux, l'espace des excursions par exemple. Une autre manière est l'utilisation de générateurs infinitésimaux, c'est une fonctionnelle sur les fonctions continues qui décrit comment le processus se déplace de points en points. Le générateur infinitésimal d'un processus de Markov "X" est l'opérateur "A" tel que :

Les processus stochastiques sont utilisés dans de nombreux domaines : historiquement le mouvement brownien a été utilisé pour modéliser des trajectoires de particules ou pour calculer le nombre d'Avogadro, il est également utilisé pour modéliser des phénomènes tels que les marchés financiers dont les premiers travaux sont dus à Louis Bachelier ou les travaux en physique par les travaux de Sydney Chapman.

Parmi les processus stochastiques à temps discret et à temps continu, certains possèdent une propriété liée à la filtration formula_198 sur laquelle ils sont définis. Un processus formula_159 est appelé une "martingale" si :
Cette définition se généralise pour un processus stochastique en temps continu. Le processus est une "sur-martingale" si formula_201 et une "sous-martingale" si formula_202. Intuitivement la valeur moyenne du processus à un temps futur "n" + 1 connaissant le passé est égale à la valeur présente du processus. C'est une représentation du bénéfice dans un jeu équitable, c'est de cette correspondance que provient le nom martingale. Une sous-martingale correspond à un jeu favorable et une sur-martingale à un jeu défavorable.

Les martingales ont donc une moyenne constante en tout temps ainsi qu'en certains temps aléatoires : les temps d'arrêt, c'est ce qu'annonce le théorème d'arrêt de Doob.

Les bonnes propriétés des martingales permettent d'obtenir des inégalités ainsi que des résultats de convergence.

Une intégrale stochastique est soit l'intégration d'un processus aléatoire par rapport à une mesure (non aléatoire), soit l'intégration d'une fonction (localement bornée) par rapport à un processus stochastique (semi-martingale continue). Dans le cas où la fonction est étagée, l'intégrale se définit simplement par une formule du type : 
De manière plus générale, l'intégrale se définit à partir d'un objet appelé crochet de martingale. L'intégrale formula_204 s'écrit alors de manière plus simple avec la notation : formula_205. En particulier on obtient formula_206.

La formule d'Itô dans sa formule générale la plus courante s'écrit sous la forme : pour une fonction formula_207 de classe C1 en formula_192 et de classe C2 en formula_179 :
où formula_211 est un mouvement brownien et "X" est un processus stochastique solution de l'équation différentielle stochastique :

Cette formule permet d'obtenir des formules utiles dans le cas du mouvement brownien formula_213 par exemple : 

Le Calcul de Malliavin, également connu sous le nom calcul stochastique des variations, est une méthode de calculer grâce à des opérateurs différentiels de dimension infinie sur l'espace de Wiener. Il a été initié par Paul Malliavin pour étudier les propriétés des fonctionnelles appliquées au processus de Wiener. Le calcul de Malliavin comprend l'utilisation d'un opérateur différentiel, une formule d'intégration par parties liées aux intégrales stochastiques mais également la notion de matrice de covariance.

Les domaines liés avec la théories des probabilités sont nombreux, donnons ici quelques exemples.

La statistique et la théorie des probabilités forment les "sciences de l'aléatoire". Ces deux sciences utilisent les mêmes outils aléatoires (loi de probabilité, espérance, écart-type, etc), les frontières entre ces deux domaines sont assez floues. Une des différences entre statistique et probabilité vient de l'interprétation faite du hasard : c'est la différence entre probabilité "a priori" et probabilité "a posteriori". Intuitivement, les probabilités permettent d'évaluer les chances de réalisation d'un évènement dans un cadre prédéfini alors que la statistique est l'étude et l'analyse de caractères à partir de données récoltées.

Les notions de la théorie des probabilités sont les outils à la base de la théorie statistique. Certaines lois de probabilités ont même une utilisation principalement en statistique plutôt que dans la modélisation directe d'un phénomène.

Les jeux de hasard ont été une des premières motivations pour développer le calcul des probabilités, avec notamment le problème des partis. Les raisonnements au début faits de manière peu rigoureuse sont devenus plus élaborés avec l'arrivée de l'axiomatique de Kolmogorov. Cette écriture scientifique permet dorénavant de pouvoir faire des calculs plus précis sur des questions autour des jeux de hasard : calcul de probabilité dans les jeux de dés, dans les jeux de pile ou face, pour les paris à la loterie, etc. Ces études ont alors donné naissance à une théorie : la théorie des jeux.

De nombreux paradoxes probabilistes sont devenus des jeux mathématiques : c'est le cas du problème de Monty Hall, du paradoxe des prisonniers ou encore du problème de la Belle au bois dormant.

De nombreuses méthodes statistiques sont utilisées en biologie pour réaliser des échantillonnages d'individus et pouvoir ainsi mieux connaître la population entière. Ces données statistiques sont alors étudiées grâce au calcul des probabilités.

Le calcul des probabilités permet de pouvoir mieux apprécier les risques économiques qui sont considérés comme aléatoires. C'est le cas dans les calculs d'assurances, dans la gestion des stocks ou dans la finance moderne telle que la création d'un portefeuille d'actifs ou dans les activités de spéculation.




</doc>
<doc id="14996" url="https://fr.wikipedia.org/wiki?curid=14996" title="Arcadi et Boris Strougatski">
Arcadi et Boris Strougatski

Les frères Arcadi et Boris Strougatski (Arcadi 1925-1991 et Boris 1933-2012) sont des écrivains soviétiques de science-fiction.

Leur père a échappé aux purges staliniennes, mais n'a pas survécu au siège de Leningrad pendant la Grande guerre patriotique.

Arcadi devient traducteur pour l'armée, spécialisé notamment dans le japonais. Son frère cadet Boris devient pour sa part astrophysicien. Leurs métiers se complétant, ils collaborent à partir de 1958 pour écrire de la science-fiction.

Dans leurs romans, ils cherchent un idéal, mais n'épargnent pas le régime soviétique qui commence à les censurer en 1969. Ils écrivent alors clandestinement une partie de leur œuvre, partiellement diffusée "sous le manteau", puis publient de nouveau au grand jour dans les années 1980 avec la glasnost ; leurs œuvres sont traduites et publiées par les éditions officielles soviétiques (Éditions en langues étrangères puis Radouga), ce qui les fait connaître au-delà des frontières de leur pays.








</doc>
<doc id="14997" url="https://fr.wikipedia.org/wiki?curid=14997" title="Brown v. Board of Education">
Brown v. Board of Education

Un arrêt complémentaire est rendu dans la même affaire le (349 U.S. 294), et les deux arrêts sont aussi dits Brown I et Brown . L'arrêt est sans doute la plus importante des décisions de la cour Warren. Si techniquement, la décision "Brown" s'applique seulement au système d'éducation publique des États, l'arrêt Bolling v. Sharpe 349 U.S. 497 (1954), moins connu, est rendu le jour suivant et étend l'obligation au gouvernement fédéral.

À l'issue de la Guerre de Sécession (1861-1865) commence la période de la "Reconstruction". L'armée de l'Union occupe les États du Sud et impose l'émancipation des esclaves noirs. Trois amendements à la Constitution sont adoptés :

En 1875, le Congrès adopte une loi des droits civiques ("Civil Rights Act"). Il s'agit d'un texte très bref qui interdit la ségrégation, dans de nombreux domaines. Pendant quelques années, la jurisprudence de la Cour suprême va dans le même sens. Le ' ne cite pas le domaine de l'éducation. En 1877, l'élection présidentielle est indécise. Les partis politiques négocient le nom du président, et un des éléments de l'accord est la fin de la Reconstruction. En 1883, la Cour suprême déclare le ' de 1875 non constitutionnel. 

En 1896, avec l'arrêt "Plessy v. Ferguson", la Cour suprême valide une loi de l'État de Louisiane qui impose la ségrégation dans les chemins de fer. La doctrine est habituellement résumée par l'expression "" (séparés mais égaux). Dès lors que les deux races se voient offrir des conditions égales, la ségrégation est constitutionnelle, le Congrès ne peut l'interdire aux États. L'égalité est imposée par le amendement, mais la Cour suprême en donne une interprétation restrictive. L'arrêt mentionne explicitement l'éducation comme un des exemples de ségrégation les plus largement acceptés « y compris dans les États où les droits de la race colorée ont été le plus anciennement et le plus scrupuleusement respectés ». Seul le juge Harlan se prononce contre la décision dans une opinion dissidente restée célèbre. Bien que la décision "Plessy" réaffirme en revanche la stricte égalité des droits politiques, les Noirs du Sud se verront dans la pratique interdits de vote, par des astuces légales, par la mauvaise volonté des autorités, ou par l'intimidation et la violence.

Après la Guerre de Sécession, les Noirs sont nombreux à émigrer vers le Nord industriel, où ils étaient peu présents auparavant. La ségrégation se développe dans l'ensemble du pays, quoiqu'à des degrés divers. Souvent, l'égalité des conditions est très relative.

Après la Première Guerre mondiale, sans remettre en cause la jurisprudence de "Plessy", la Cour l'interprète plus strictement. La ségrégation n'est acceptable que si les conditions sont égales, et la Cour devient plus exigeante sur cette égalité. Elle impose l'admission d'un étudiant noir dans une université blanche, une première fois avant la guerre (arrêt "" en 1938), rejetant la proposition du Missouri de financer ses études dans un État voisin.

À l'issue de la Seconde Guerre mondiale, la question de la ségrégation devient pressante ; la guerre a été gagnée, avec la participation de citoyens noirs, contre un régime criminel fondé sur le racisme, une classe moyenne noire a commencé à se développer et les mouvements de décolonisation qui s'amplifient, notamment en Afrique, gagnent la sympathie du pays. La ségrégation est de plus en plus souvent ressentie comme une anomalie, mais garde des défenseurs, et pas seulement dans le Sud profond. En 1948, le président Harry S. Truman met fin, par décret, à la ségrégation raciale dans les forces armées. En 1950, la Cour suprême rend encore deux arrêts sur la ségrégation dans les universités : ' rejette les conditions imposées à un étudiant noir pour son admission dans une université antérieurement réservée aux seuls Blancs : s'asseoir dans des emplacements séparés en classe, à la cafétéria, ou à la bibliothèque ; ' impose l'admission d'un étudiant noir dans une université blanche, la nouvelle université du Texas réservée aux Noirs n'ayant, entre autres, pas le prestige de celle réservée aux Blancs.

En 1951, Linda Brown est une élève noire résidant à Topeka au Kansas qui se voit refuser l'inscription dans une école blanche toute proche de son domicile et doit s'inscrire dans l'école noire distante de plus d'un kilomètre. La loi du Kansas autorise, sans l'imposer, les villes de plus de habitants à établir des écoles séparées. À Topeka, c'était le cas pour les seules écoles élémentaires. Le père de Linda Brown conteste la décision en justice. Il s'agit d'une plainte en nom collectif (') : plusieurs plaintes portant sur les mêmes faits regroupées ; le nom de Brown est simplement le premier dans l'ordre alphabétique. La plainte est soutenue et en fait organisée par la NAACP (' : association nationale pour le progrès des gens de couleur, qui est alors la principale organisation de défense des droits civiques). Thurgood Marshall, le principal avocat de la NAACP, et qui sera en 1967 le premier juge noir nommé à la Cour suprême plaide l'affaire. D'autres plaintes sont déposées dans d'autres États (Caroline du Sud, Delaware et Virginie) qui seront jointes à Brown devant la Cour suprême, ainsi qu'une à Washington qui donne lieu à l'arrêt distinct "Bolling v. Sharpe" (voir "infra"). Les plaignants fondent en partie leur argumentation sur la psychologie, l'impact sur les élèves de la ségrégation, ressentie comme une déclaration d'infériorité. Dans chacun des procès, ils font témoigner plusieurs experts.

La cour fédérale qui juge en première instance reconnaît que la ségrégation se fait au détriment des élèves noirs, mais se plie à la jurisprudence "Plessy v. Ferguson", constatant que les deux écoles sont matériellement ("") égales en termes de bâtiments, de services, d'enseignement. En Virginie et en Caroline du Sud, les cours fédérales avaient constaté l'inégalité, d'ailleurs criante, des conditions d'enseignement, ordonné d'y remédier immédiatement, mais refusé l'admission des plaignants dans les écoles blanches. En Caroline, (affaire "Briggs v. Elliott"), l'affaire est entendue par trois juges, et l'un d'eux, Warding, rédige contre la décision de ses deux collègues une opinion dissidente très dure (« si c'est là la justice rendue par cette cour, je ne veux y avoir aucune part »). Il s'y indigne qu'on laisse les autorités se sortir d'affaire en reconnaissant qu'il faudra, quand les fonds seront disponibles, « changer quelques ampoules, tableaux, et remettre les sanitaires en état » et que par ce moyen, on refuse aux plaignants l'examen au fond de leurs droits constitutionnels. Il qualifie la solution "séparés mais égaux" ("Jim Crow laws", lois à l'origine de la ségrégation en Amérique) de « fausse doctrine et boniment ». Dans le Delaware, la cour suprême de l'État avait ordonné l'admission immédiate des plaignants dans les écoles blanches. Toutefois, elle avait laissé ouverte la possibilité de réexaminer l'affaire après que les conditions respectives auront été égalisées. La cour suprême accepte de revoir tous ces jugements en appel. Le gouvernement fédéral, sous la présidence de Truman, intervient comme "amicus curiae", c’est-à-dire une partie intéressée à l'issue du procès, notamment à la jurisprudence qui s'en dégagera, et qui fait connaître son opinion. Il plaide vivement pour l'inconstitutionnalité de la ségrégation, arguant de plus qu'elle nuit à l'image des États-Unis dans le monde et à leur politique étrangère. En 1952, les juges sont partagés : quatre pensent que la ségrégation est illégale, deux, dont le président Fred Vinson veulent réaffirmer la jurisprudence Plessy, les trois autres sont indécis. Ils reportent la décision à l'année suivante. Eisenhower succède à Truman, sans que le Département de la Justice ne change sa position sur l'affaire. À l'audience de 1953, la cour demande aux parties de lui présenter, pour sa session de 1954, des arguments sur les conditions de l'adoption du amendement, afin de déterminer quelles étaient les intentions des législateurs : elle constitue aux États-Unis le principal critère qui guide le juge dans l'interprétation de la loi. Entre-temps, Fred Vinson meurt et Eisenhower nomme Earl Warren pour le remplacer. Il souhaite la déségrégation, portant le nombre de ses partisans convaincus à cinq, la majorité de la Cour. Démontrant immédiatement son influence sur la cour, il parvient à convaincre tous ses collègues de s'y rallier.

L'arrêt est adopté à l'unanimité des neuf juges. Il déclare que la ségrégation dans l'éducation est inconstitutionnelle et qu'il doit y être mis fin : « la doctrine "" adoptée "dans Plessy v. Ferguson" n'a pas sa place dans le domaine de l'éducation ».

L'opinion de la Cour est rédigée par Warren. Il constate d'abord que le contexte historique de l'adoption du amendement ne permet guère de trancher, tant les opinions des législateurs étaient diverses sur la question et rappelle que la cour avait d'abord interprété la clause d'égale protection strictement et interdit toute discrimination, avant d'en venir vers la doctrine "séparés mais égaux". Il constate que l'éducation publique était quasi inexistante en 1896 et est devenue « peut être la plus importante des fonctions de l'État ». Vu l'importance qu'elle revêt pour l'avenir de l'enfant, il est clair que « si l'État choisit de l'offrir, il doit la proposer à tous dans des conditions égales », c'est le principe même de l""'. Reste alors à déterminer si cette égalité est compatible avec la ségrégation. Warren rappelle des arrêts récents (voir ci-dessus) et affirme que ce qui vaut pour l'université vaut plus encore pour des enfants plus jeunes et plus vulnérables. Il reprend les termes de l'arrêt de la cour de district : « la politique de séparation des races est généralement interprétée comme dénotant l'infériorité des Noirs. Ce sentiment d'infériorité affecte la motivation des enfants à apprendre. [La ségrégation] prive [les Noirs] de certains avantages qu'ils tireraient d'un système scolaire racialement intégré ». Il tranche enfin : « quelles qu'aient pu être les connaissances en matière de psychologie à l'époque de "Plessy v. Ferguson", les connaissances modernes valident largement [l'opinion de la cour de district sur l'infériorité]. Toute considération contraire dans "Plessy v. Ferguson" est rejetée » (dans "Plessy v. Ferguson", la cour avait au contraire déclaré que si la ségrégation implique une infériorité, c'est « uniquement parce que la race colorée choisit de le percevoir ainsi »). Vient la décision : « des systèmes d'éducation séparés sont par essence inégaux. [Les requérants], en raison de la ségrégation contestée ici, ont été privés de l'égale protection de la loi ». Reconnaissant les difficultés pratiques de l'abolition de la ségrégation, la cour demande aux parties à l'affaire et aux autres parties concernées (le gouvernement fédéral et les 17 États qui pratiquent alors la ségrégation dans l'enseignement) de présenter pour la session de 1955 leurs conclusions sur les moyens d'y parvenir.

La jurisprudence "Plessy v. Ferguson" n'est pas explicitement rejetée : il n'est pas contesté que la ségrégation soit légale dès lors que les possibilités offertes aux deux races sont égales. Mais elle est vidée de sa substance, puisque, au moins dans le domaine de l'éducation pour ce premier arrêt, des systèmes séparés ne peuvent être égaux.

La Cour avait tranché la question constitutionnelle en 1954. Dans son second arrêt, elle règle les questions d'applications. Entre-temps, le gouvernement, dans un nouveau mémoire, plaide pour le pragmatisme et la modération. Exceptionnellement, le président Eisenhower rédige lui-même une partie de la demande. Dans son arrêt, la cour casse les jugements des cours inférieures. Elle rend les autorités locales chargées de l'éducation responsables du processus de déségrégation et charge les tribunaux fédéraux d'en surveiller l'application. La cour reconnaît de probables difficultés d'applications, accepte d'éventuel délais, et choisit, pour préciser les délais acceptables, l'expression '. L'expression, ambiguë, voire contradictoire, est difficilement traduisible, "speed" étant la vitesse, ' signifiant volontaire et assuré, mais aussi réfléchi voire lent, on peut la rapprocher de l'expression « se hâter lentement » en français. Elle laisse ainsi beaucoup de temps ; la NAACP avait souhaité le terme ', « immédiatement ». Mais les autorités doivent démontrer que les délais sont nécessaires et qu'elles s'appliquent de bonne foi à mettre en œuvre la décision aussitôt que possible. La Cour exige d'elles au moins un commencement rapide et crédible (') des mesures de transitions.

La décision "Bolling v. Sharpe", oubliée dans l'ombre de "Brown", concerne une affaire traitée par la Cour dans le même groupe et qui porte sur des faits similaires, à la seule différence qu'ils ont lieu à Washington, DC, la capitale fédérale, où l'éducation comme le reste dépend du seul gouvernement fédéral.

La Cour rend un arrêt distinct pour "Bolling" alors que les autres affaires cas sont joints à "Brown", parce que le amendement s'applique aux États, et non au gouvernement fédéral, qu'il n'est donc pas possible de recourir ici à la clause d" et donc que le raisonnement de "Brown" ne peut s'appliquer tel quel. En revanche, le amendement impose la clause de ' au gouvernement fédéral, comme le l'impose aux États.

Dans Brown I, Warren, après avoir tranché sur la base de l", note « il n'est donc pas nécessaire de décider si la ségrégation viole aussi le ' » (les cours des pays de common law, dont la jurisprudence est contraignante doivent veiller à ne pas se prononcer sur des questions dont la réponse n'est pas nécessaire à la solution de l'affaire à juger).

Dans Bolling, il explique que la clause d" est plus précise que celle de ', et qu'elle ne sont pas interchangeables. Cependant, « la ségrégation ne remplit aucun objectif gouvernemental légitime, et donc, impose aux enfants noirs de Washington une restriction qui constitue une privation arbitraire de liberté en violation de la clause de "" ». Et il termine : « au vu de notre décision interdisant aux États de conserver la ségrégation dans leur système éducatif, il serait impensable que la constitution en demande moins au gouvernement fédéral. »

Les deux arrêts sont inhabituels dans leur forme. Celui de 1954, le plus important se distingue des habitudes de la cour par sa concision, alors qu'il s'agit d'un arrêt d'une importance extrême. Plus inhabituel encore le peu de référence tant à la constitution, qu'à des précédents : si Warren souligne que "Plessy" est une interprétation tardive du amendement, et rappelle que la jurisprudence récente de la cour va dans le sens de la décision "Brown" et que la cour n'a jamais formellement décidé de l'applicabilité de la doctrine "" au domaine de l'enseignement, il se garde de remettre en cause formellement "Plessy". La décision privilégie sans le détailler un argument de psychologie, qui pourrait sembler relever plus de l'appréciation du législateur que de celle du juge. Il prête ainsi le flanc à la critique d'une décision mal fondée et relevant plus des préférences politiques des juges que d'une analyse juridique. Non que les arguments juridiques manquassent forcément : ils figuraient déjà dans l'opinion dissidente jointe à "Plessy" par le juge Harlan, voire dans celle de Warding en Caroline du Sud et ont été amplement développés par la cour depuis. Mais la cour choisit de limiter strictement sa décision au domaine scolaire. Plus encore, elle décide, contre tout usage, de ne remédier que progressivement à une violation des droits constitutionnels des personnes, dans une concession aux nécessités pratiques, mais aussi à l'opinion. Personne, après que l'arrêt a été rendu, ne se trompe sur l'intention de la cour de mettre fin à la ségrégation. En rendant une décision courte, que la presse pourra reprendre intégralement, et en éliminant autant que possible toute complexité juridique de son arrêt, la Cour y donne, à dessein comme Warren le racontera par la suite, le plus grand retentissement. Mais cette décision partielle, sans qu'un principe constitutionnel soit clairement énoncé traduit aussi la prise en compte des limites politiques du pouvoir de la cour.

Lors de son annonce, la décision est saluée par les grands journaux de la côte Est, tel le "New-York Times" ou le "Washington Post". L'accueil dans le pays est en général favorable, la décision est même bien acceptée dans des États qui pratiquent une ségrégation limitée, notamment le Kansas. Dans le Sud au contraire, la décision provoque la colère. L'éditorial du "Daily News" de Jackson, dans le Mississippi est resté célèbre : « Il se pourrait bien que le sang coule dans bien des endroits dans le Sud à cause de cette décision, mais ce sont les marches de marbre blanc du bâtiment de la Cour suprême qui seront souillées par ce sang. Mettre des enfants blancs et noirs dans les mêmes écoles mènera au métissage, le métissage mènera aux mariages mixtes, et les mariages mixtes mèneront à l'abâtardissement de la race humaine ». Des gouverneurs s'engagent à s'opposer à la décision par tous les moyens légaux, dans certains États les législatures reprennent les mots de très anciennes querelles, parlant d"'interposition" ou de "nullification", des termes utilisés dans les premières décennies du quand les États disaient disposer du pouvoir d'invalider la législation fédérale. On appelle à la destitution des juges, voire à la suppression de la Cour elle-même. En 1956, des représentants et des sénateurs du Sud signent le manifeste du Sud ("The Southern Manifesto") condamnant la décision de la Cour, « arbitraire ». Clairement minoritaire au Congrès, ne rassemblant même pas tous les parlementaires du Sud, ils parviennent souvent à ralentir le travail législatif vers la déségrégation, mais pas à l'infléchir durablement.

L'application de la décision Brown commence lentement, comme l'arrêt de 1955 le prescrivait. Les juges fédéraux siégeant dans le Sud font effectivement appliquer la décision, ce qui ne paraissait pas aller de soi à l'époque, les autorités locales doivent leur présenter des plans de déségrégation. Souvent ces plans renvoient la fin de la déségrégation à un terme lointain. Mais même le « commencement rapide et crédible » exigé provoque des troubles.

En 1957, alors que neuf élèves noirs doivent rejoindre le lycée central de Little Rock dans l'Arkansas, des émeutes éclatent, encouragées par le gouverneur de l'État Orval Faubus. Le président Eisenhower doit envoyer sur place mille hommes de la prestigieuse aéroportée et faire passer la garde nationale de l'Arkansas sous contrôle fédéral. Constamment harcelés, parfois violemment, les neuf élèves (« les 9 de Little Rock ») se voient affecter chacun un militaire de la comme garde du corps. Le gouverneur choisit même un moment de faire fermer les écoles plutôt que d'accepter qu'elles soient multiraciales. Les tribunaux fédéraux ordonnent leur réouverture, la Cour suprême confirme. Des évènements similaires se produisent ailleurs, jusque tard dans les années 1960 : les autorités locales modifient leur législation, mais pas son esprit, ce qui permet des procédures dilatoires devant les tribunaux, et parfois précipitent la crise en fermant les écoles. Brown dit en effet que lorsqu'un État établit un système d'instruction publique, il doit le faire sans discrimination, mais rien ne dit qu'un tel système doit exister. Les États subventionnent alors des écoles privées réservées aux blancs, ce que les tribunaux rejettent. Encore en 1964, la Cour suprême (arrêt "") ordonne la réouverture des écoles dans le comté du Prince-Édouard, en Virginie, alors que ce comté était partie à une des quatre affaires regroupées dans l'arrêt Brown.

Le mouvement des droits civiques, emmené par des personnalités comme Martin Luther King, débute peu après l'affaire, avec le boycott des bus de Montgomery à partir du . Il est difficile de dégager les contributions respectives des mouvements militants et des juges dans la déségrégation. Sans doute le mouvement aurait-il eu lieu sans l'arrêt Brown et au contraire, Brown et les autres arrêts sont eux le produit d'actes militants et de la patiente stratégie de la NAACP : avant l'arrêt Brown et plusieurs années après, il faut du courage pour un étudiant ou un élève Noir pour réclamer aux tribunaux son admission dans une école réservée aux Blancs et ensuite pour s'y rendre. Il est certain que le soutien des tribunaux fédéraux aide au mouvement des droits civiques, même si les tribunaux fédéraux ne sont accessibles qu'après une longue procédure, qui passe d'abord par des tribunaux locaux souvent hostiles. L'action des tribunaux comme le mouvement des droits civiques, de grande ampleur, non violent mais souvent confronté à la violence, ont poussé le Congrès à réagir, et à modifier la législation.

Dès 1957, après la crise de Little Rock, le département de la justice prend l'initiative d'une nouvelle loi sur les droits civiques ("Civil Rights Act"), la première depuis 1875. Destinée à favoriser le vote des Noirs, mais peu ambitieuse, elle est réduite par le Sénat à une portée seulement symbolique. Ce n'est que sous la présidence et l'impulsion de Johnson qu'est voté le Civil Rights Act de 1964 qui rend illégale pratiquement tout forme de discrimination raciale, non seulement de la part des organismes publics, fédéraux ou locaux, mais aussi dans les relations commerciales et de travail. Les pouvoirs constitutionnels du Congrès sont utilisés à leurs limites pour cette loi, non seulement le pouvoir d'application du amendement mais aussi celui de régulation du commerce inter-État interprétée de la façon la plus large. La Cour suprême en valide le principe dans l'arrêt ', dès décembre 1964. Le gouvernement fédéral, en conditionnant le versement de ses subventions, se substitue partiellement aux juges fédéraux par nature lents pour poursuivre la déségrégation dans les écoles. Le dispositif des droits civiques est complété en 1965 par le ' (loi sur le droit de vote) qui donne aux Noirs du Sud la possibilité réelle de voter, comme le amendement l'imposait normalement.

La question de la mixité raciale dans les écoles demeure jusqu'au milieu des années 1970. Le principe a été posé non seulement que les lois imposant cette ségrégation devaient disparaître, de même que toute politique destinée à la favoriser, mais qu'une fois ces lois abolies et ces politiques disparues, il fallait encore remédier à leurs effets. Or la ségrégation de fait dans l'habitat, , favorise la persistance d'écoles monoraciales. Là où la ségrégation a été imposée dans le passé, les autorités doivent maintenant assurer la mixité. C'est ce que rappelle fermement la Cour en 1969 dans ' : le plan adopté par les autorités, qui laisse les parents choisir l'école de leurs enfants, est « intolérable ». La décision Brown confie non pas aux parents, mais aux autorités, la responsabilité de démanteler la ségrégation. Reprenant les mots de Brown , « Il est trop tard pour ne se hâter que lentement » "(Time for mere deliberate speed has run out)", le juge Black conclut pour la Cour « Aujourd'hui, la responsabilité de la commission est de proposer un plan qui ait des chances raisonnable de succès, et des chances raisonnables de succès maintenant ». En 1971, avec l'arrêt ', elle accepte, là où la ségrégation a existé, le principe de quotas raciaux et du « busing », l'organisation du transport des élèves vers l'école à laquelle ils ont été inscrits pour atteindre ces quotas. La question perd cependant peu à peu de son importance, même si encore aujourd'hui, des tribunaux jugent d'affaires liées à la déségrégation.

Une autre question est venue au premier plan, celle de la discrimination positive ("") politique qui conduit beaucoup d'universités à favoriser de diverses manières l'admission d'étudiants appartenant aux minorités (Africains-américains, Femmes, ...).

D'autres étudiants, lésés, ont contesté ces politiques et la Cour a admis qu'il s'agissait aussi de discrimination. Elle invalide tout système qui établirait des quotas ou qui accorderait de trop fortes bonifications (', 1978). Mais dans ce même arrêt, elle reconnaît que la diversité de la population étudiante est un objectif légitime, y compris parce qu'elle contribue à dispenser une meilleure formation. Elle laisse la porte ouverte, sous la stricte surveillance des tribunaux, à un système qui pourrait prendre en compte l'appartenance à une minorité, parmi d'autres critères destinés à assurer la diversité, celle-ci ne pouvant s'entendre seulement comme diversité raciale. En 2003 un tel système, celui de la faculté de droit du Michigan, est pour la première fois validé par la Cour dans '.





</doc>
<doc id="14998" url="https://fr.wikipedia.org/wiki?curid=14998" title="Graviton">
Graviton

Le graviton est une particule élémentaire hypothétique qui transmettrait la gravité, prévue dans la plupart des systèmes de gravité quantique. Il serait donc le quantum de la force gravitationnelle. En langage courant, on peut dire que les gravitons sont les messagers de la gravité, ou les supports de la force. Pour matérialiser cette force, on pourrait prendre l'exemple d'une fronde avec la ficelle (graviton) qui tient la pierre. Plus il y en a dans un champ gravitationnel, plus ce champ est puissant.

Les gravitons ont été postulés suite aux succès de la représentation des interactions dans le cadre de la théorie quantique des champs dans d'autres domaines. Par exemple, l'électrodynamique quantique explique très précisément l'ensemble de l'électromagnétisme, du domaine macroscopique au domaine microscopique, par l'échange de photons entre les particules dotées de charges électriques. Ainsi, les photons échangés sont donc responsables des forces électriques et magnétiques.

Étant donné le large succès de la mécanique quantique pour la description des autres interactions représentant les forces fondamentales de l'univers, il a semblé naturel que les mêmes méthodes pouvaient fonctionner pour la description de la gravitation.

Afin de répondre aux caractéristiques de l'interaction gravitationnelle, les gravitons doivent toujours mener à une interaction attractive, avoir une portée infinie, et être en nombre illimité. Quantiquement, cela signifie que c'est un boson de masse nulle et de spin égal à 2. Ce qui implique qu'ils sont des luxons, des particules se déplaçant à la vitesse de la lumière.

Les théoriciens pensent que la gravité et la mécanique quantique doivent converger, ou fusionner, à une échelle de taille de 10m, pour observer la brisure de symétrie de Lorentz, mais les meilleurs instruments actuels n'informent pas en dessous de 10m.

Dans le cadre de la relativité générale (non quantique), l'interaction gravitationnelle n'a pas une représentation vectorielle, comme les trois autres forces. En effet, elle se fond alors avec la membrane de l'espace-temps : dans le paradigme de la relativité générale, les masses ne s'attirent plus : elles suivent simplement les géodésiques d'un espace-temps ordonné par le tenseur énergie-impulsion réparti dans l'univers. Dans ce cadre, il n'y a nul besoin d'une particule pour transmettre la gravitation, celle-ci étant inhérente à la 'forme' même de l'univers, ou plus exactement ses déformations locales. Ceci justifie que, en un endroit précis de l'espace, des corps de masses différentes suivront strictement la même trajectoire (en l'absence de l'intervention de forces extérieures : électromagnétiques par exemple, ou chocs).

Malgré de nombreuses tentatives, le graviton n'a été ni observé, ni même théoriquement bien cerné. À ce jour, toutes les tentatives de créer une théorie simple de la gravité quantique ont échoué. La recherche du Higgs (ou boson BEH du nom de ses découvreurs : "Brout", "Englert", "Higgs"), autre boson pressenti quant à lui comme le fondement de la masse de tout fermion – alors que le graviton constituerait le vecteur de la force gravitationnelle –, focalisait au début du les efforts de la communauté des spécialistes en recherche fondamentale. Le boson de Higgs a été découvert au CERN, l'annonce en fut faite le 4 juillet 2012.

Mais certains chercheurs ne veulent pas considérer le graviton comme une véritable particule.

Une difficulté fondamentale pour sa mise en évidence réside dans le fait que les masses sont toutes positives, que les effets se font sentir à distance infinie, et sans effet d'écran ferme : l'interaction d'un hypothétique graviton avec un appareillage destiné à le mettre en évidence risque d'être noyé dans un bruit de fond énorme et universel. La seule façon de détecter ce boson serait de chercher les événements où le mouvement ou l'énergie d'un objet-test change différemment de ce qui est établi par la relativité générale, mais un des principes de base de la gravité quantique est pour l'heure qu'elle permette elle-même de retrouver l'ensemble des connaissances expérimentales cohérentes avec la relativité générale.

En théorie des cordes et en cosmologie branaire, le graviton a une place importante. Comme celui-ci est engendré par une corde fermée, il ne peut pas être emprisonné dans une D-brane. Cela implique qu'à travers la force gravitationnelle, la mise en évidence de l'existence d'autres D-branes deviendrait envisageable.

Le graviton pourrait aussi être considéré comme un composant des ondes gravitationnelles, telle que celles qui ont été détectées par l'interféromètre VIRGO, ou celles de bien plus grande longueur d'onde, dont la détection est le but du projet spatial LISA de l'ESA.

Le graviton ne doit pas être confondu avec le boson de Higgs : le premier a été postulé par la théorie quantique de Bluck pour expliquer la propagation spatiale de la gravitation, tandis que le second apparait dans le modèle standard (lequel s'appuie notamment sur la théorie quantique, mais aussi sur la relativité restreinte) pour expliquer les fondements des effets gravitationnels, à travers la masse des particules. 




</doc>
<doc id="15000" url="https://fr.wikipedia.org/wiki?curid=15000" title="Interaction faible">
Interaction faible

L'interaction faible (aussi appelée force faible et parfois force nucléaire faible) est l'une des quatre interactions fondamentales de la nature, les trois autres étant les interactions électromagnétique, nucléaire forte et gravitationnelle. Elle est responsable de la désintégration radioactive de particules subatomiques et est à l'origine de la fusion nucléaire dans les étoiles. Elle affecte toutes les catégories de fermions connues, à commencer par les électrons, les quarks et les neutrinos. 

Dans le modèle standard de la physique des particules, l'interaction faible est causée par l'échange de bosons W, W et Z. L'effet le plus connu en est la radioactivité β. La plupart des particules sont sujettes à la désintégration causée par l'interaction faible. Les bosons W et Z ont une masse très élevée, ce qui explique qu'elle a une portée très courte. Par ailleurs, son intensité (constante de couplage) est généralement plus faible de plusieurs ordres de grandeur que celles des interactions électromagnétique et forte, ce qui explique son nom. L'interaction faible a plusieurs propriétés uniques, parmi lesquelles sa capacité à changer la saveur des quarks et à briser la symétrie de parité et la symétrie CP.

L'interaction faible a été décrite pour la première fois dans les années 1930 par Enrico Fermi, qui en faisait une interaction de contact à quatre fermions. Nommée interaction de Fermi, Fermi s'en est servi pour expliquer la désintégration β du neutron. Elle fut aussi utilisée en 1947 lors de la découverte de la désintégration du muon. Par la suite, une description sous forme de champ à très faible portée a été préférée. En 1968, les interactions électromagnétique et faible ont été unifiées, et présentées comme deux aspects de l'interaction électrofaible.

La radioactivité β est à l'origine de la nucléosynthèse dans les étoiles. C'est elle qui rend possible la datation par le carbone 14, en transformant le carbone 14 en azote 14. Elle est aussi à l'origine de la radioluminescence, utilisée dans l'illumination au tritium et dans les générateurs bêtavoltaïques.

L'interaction faible est unique à plusieurs points de vue :

L'interaction faible permet à tous les leptons et tous les quarks d'échanger de l'énergie, de la masse et de la charge électrique, leur permettant de changer de famille et de saveur.

L'interaction faible a une portée très courte, et son influence est limitée au noyau atomique. On peut l'expliquer par la masse des bosons W et Z, qui est d'environ , ce qui leur donne une durée de vie inférieure à 10 s et confère à l'interaction faible une portée théorique d'environ 10 m, soit cent fois moins que l'interaction forte (les autres interactions fondamentales, électromagnétique et gravitationnelle, ont une portée infinie).

Cette force fondamentale est la plus faible des interactions non gravitationnelles. Aux énergies habituellement considérées en physique nucléaire, on la modélise par une interaction effective simplifiée (force de Fermi) dont la constante de couplage est environ moindre que celle de l'interaction électromagnétique et moindre que celle de l'interaction nucléaire forte. Cela s'explique entre autres par le fait que son champ d'action est très limité. Cependant, son intensité croît rapidement avec l'énergie des particules en présence, ce qui fait qu'elle rattrape l'interaction électromagnétique vers quelques dizaines de GeV. C'est à ce niveau qu'elle se mélange avec elle pour donner l'interaction électrofaible. Seule la force gravitationnelle est encore plus faible mais elle croît encore plus vite avec l'énergie que l'interaction faible, ce qui laisse ouverte la possibilité d'une unification de toutes les interactions élémentaires.

La charge associée à l'interaction faible est l'isospin faible (T ou T). C'est l'équivalent de la masse pour la gravitation, la charge électrique pour l'interaction électromagnétique et la charge de couleur pour l'interaction forte. Elle gouverne la manière dont deux particules interagissent. Les fermions élémentaires ont un isospin faible de ±1/2. Par exemple, les quarks de type up (u, c et t) ont T = +1/2. Ils se transforment en quarks de type down (d, s ou b) qui ont T = −1/2, et vice-versa. Les bosons ont un isospin faible de 0 ou ±1. En particulier, le W a T = 1 et le W a T = -1, ce qui permet des auto-interactions du champ d'interaction faible appelées couplages trilinéaires et quadratiques.

L'isospin faible est conservé lors des désintégrations : la somme des isospins faibles est identique avant et après la réaction. Par exemple un pion π, qui a un isospin faible de +1, se désintègre en un muon μ d'isospin faible +1/2 et un neutrino muonique ν d'isospin faible +1/2.

Depuis l'introduction de la théorie électrofaible, une nouvelle charge nommée hypercharge faible a été proposée. C'est une combinaison de la charge électrique et de l'isospin faible : formula_1. L'hypercharge faible est le générateur de la composante U(1) du groupe de jauge électrofaible SU(2)xU(1).

L'interaction de Fermi a été mise en évidence la première. Au début du , elle est considérée comme une interaction effective à basse énergie représentant l'échange du W, qui, étant électriquement chargé, se couple à un courant lui-même électriquement chargé : la partie la plus couramment utilisée de ce courant est constituée d'une partie qui annihile un neutrino électronique et crée un électron, ou l'inverse, ou encore crée/annihile une paire neutrino-positron, ou les mêmes processus avec les antiparticules. Elle joue le même rôle avec la paire neutron-proton. C'est cette interaction qui est à la base de la désintégration β du neutron, que l'on peut schématiser par la réaction :
Comme le W est très lourd, la durée de la réaction pendant laquelle le W est virtuel est très brève, et l'interaction se produit pratiquement sur place, se résumant à l'interaction ponctuelle de Fermi :
Comme les particules présentes avant la réaction et celles présentes après sont différentes, il a été très facile de mettre en évidence les désintégrations β, et ainsi indirectement les courants électriquement chargés.

Le Z donne lieu au même genre de réactions que les W, mais il se fait durement concurrencer par des interactions électromagnétique et forte. On n'a donc réussi à mettre en évidence le courant électriquement neutre qui se couple au Z qu'en mettant en évidence des réactions où le neutrino présent au début se retrouve à la fin. Ceci exigeait évidemment de faire une expérience avec un faisceau de neutrinos suffisamment intense, et à une énergie suffisante pour avoir une probabilité d'observer des événements. Ce tour de force ne fut réalisé, après la formulation de la théorie, qu'en 1973 (voir découverte des courants neutres).

La radioactivité est connue depuis 1896, la distinction entre radioactivité α et β faite depuis 1899, la transmutation entre atomes établie depuis 1901, et les indices d'une perte d'énergie dans le processus s'accumulèrent entre 1911 et 1927. C'est en 1930 que Wolfgang Pauli a suggéré qu'une particule neutre très légère était émise mais pas encore observée, et en 1934 que Fermi proposa une théorie de la radioactivité β dans laquelle des neutrinos sont émis. Cette théorie prévoit des interactions à 4 fermions (neutron, proton, électron et neutrino), qui constituent la première version de l'interaction faible.

On a longtemps cru que les lois de la nature étaient identiques entre deux situations qui sont le reflet l'une de l'autre dans un miroir. Cette loi de parité était respectée par la gravitation classique, l'électromagnétisme et l'interaction forte, et on la supposait universelle. Mais dans les années 1950, Chen Ning Yang et Tsung-Dao Lee suggérèrent que l'interaction faible violait cette symétrie. Chien Shiung Wu et ses collaborateurs démontrèrent la violation de symétrie en 1957, et Yang et Lee reçurent le Prix Nobel de physique la même année.

La découverte de la violation de symétrie et l'apparition des théories de renormalisation suggérèrent à Robert Marshak et George Sudarshan en 1957 puis à Richard Feynman et Murray Gell-Mann de modifier la théorie de Fermi en introduisant une caractéristique des particules nommée chiralité. Dans cette théorie, l'interaction faible agit seulement sur les particules de chiralité gauche, celles de chiralité droite n'y étant pas sensibles. Dans la situation miroir, la chiralité change et donc l'interaction n'agit pas sur les mêmes particules. À cette époque, le boson Z n'était pas connu et la théorie n'incluait pas les champs de chiralité droite qui interviennent dans les courants neutres.

La nouvelle théorie introduisait une nouvelle symétrie nommée CP qui combine la parité (permutation gauche droite) et la conjugaison (permutation entre particules et antiparticules). Mais en 1964, James Christenson, James Cronin, Val Fitch et René Turlay montrèrent expérimentalement que cette symétrie était violée elle aussi dans la désintégration des kaons ; Cronin et Fitch obtinrent le Prix Nobel de physique en 1980 pour ce résultat. En 1973, Makoto Kobayashi et Toshihide Maskawa montrèrent que la violation de symétrie CP requiert une troisième génération de particules dans les modèles théoriques.

En 1979, Abdus Salam, Sheldon Glashow et Steven Weinberg reçurent le prix Nobel de physique pour leurs contributions à l'unification théorique entre l'interaction faible et l'interaction électromagnétique, créant ainsi le modèle standard de l'interaction électrofaible. Son existence fut prouvée expérimentalement en deux étapes. Tout d'abord, la collaboration Gargamelle en 1973 permit de découvrir les courants neutres. Puis, en 1983 les collaborations UA1 et UA2 démontrèrent l'existence des bosons W et Z ; en 1984, Carlo Rubbia et Simon van der Meer obtinrent le prix Nobel de physique pour leur contribution à ces expériences.




</doc>
<doc id="15001" url="https://fr.wikipedia.org/wiki?curid=15001" title="Interaction électrofaible">
Interaction électrofaible

L'interaction électrofaible, aussi appelée force électrofaible, est la description unifiée de deux des quatre interactions fondamentales de l'univers, à savoir l'électromagnétisme (appelé électrodynamique quantique dans sa version quantique) et l'interaction faible. Ces deux forces paraissent pourtant très différentes aux échelles d'énergie atomique, et même nucléaire : la force électromagnétique est dite de "portée infinie" car on peut l'observer aisément à l'échelle macroscopique tandis que la force faible a une influence uniquement à l'échelle microscopique, au niveau du noyau atomique. Cependant, la force faible, telle qu'elle a été décrite par Enrico Fermi pour rendre compte de la désintégration β a une intensité croissant rapidement avec les énergies auxquelles on la considère, ce qui la rend incohérente à très grande énergie. La force électromagnétique, elle, croît aussi avec l'énergie considérée, mais bien plus lentement. Ces deux forces deviennent donc du même ordre de grandeur, vers une énergie d'une centaine de GeV. La théorie électrofaible s'appuie sur ce phénomène pour prédire une unification des deux théories, qui se confondent à cette échelle d'énergies.

Plus en détail, la théorie électrofaible est une théorie quantique des champs basée sur un groupe de jauge formula_1 où formula_2 est le groupe de jauge correspondant à l'interaction faible dont les bosons de jauge sont les bosons W et le boson Z tandis que le groupe de jauge formula_3 de l'électrodynamique, dont le boson de jauge est le photon, est une combinaison de formula_4 avec un groupe formula_5 appelé groupe d'isospin. La théorie est capable de prédire les masses des bosons à 80 GeV pour le W" et à 90 GeV pour le Z", les bosons vecteurs de la force faible, tandis que le photon, vecteur de l'interaction électromagnétique a une masse nulle. Ces différences de masse expliquent la différence considérable de comportement de ces interactions à basse énergie.

Lorsque l'échelle d'énergie des observations est plus faible que l'échelle électrofaible une asymétrie apparaît en effet entre électromagnétisme et interaction faible via une brisure spontanée de la symétrie électrofaible engendrée par le mécanisme de Higgs. Au cours de ce processus un champ scalaire appelé champ de Higgs, initialement invariant par le groupe de jauge, acquiert dynamiquement une valeur moyenne non nulle dans le vide (de la même manière qu'un corps ferromagnétique acquiert spontanément une aimantation non nulle lorsque la température est suffisamment faible). C'est ce "gel", qui n'est pas symétrique de jauge, du champ de Higgs qui explique les masses des bosons de jauge, de façon asymétrique par rapport à la symétrie de jauge. Seul le photon garde une masse nulle, ce qui rend l'électromagnétisme facilement détectable, en raison de sa portée infinie.

C'est pour leurs contributions majeures, et largement indépendantes, au cours des années 1960 à l'unification des interactions faibles et électromagnétiques entre les particules élémentaires, que Sheldon Glashow, Abdus Salam et Steven Weinberg obtinrent le prix Nobel de physique en 1979.

La théorie initiale de la désintégration β de Fermi, achevée en 1934, ne concernait que la partie des interactions faibles prenant place sous l'influence des bosons "W". Elle a longuement été vérifiée, sur les noyaux d'abord, puis sur les diverses particules que l'on a su créer avec les accélérateurs après la Seconde Guerre mondiale, et notamment les particules étranges, pour lesquelles on a pu noter que l'interaction faible violait la conservation de l'étrangeté, permettant ainsi aux particules étranges chargées les plus légères de se désintégrer, avec un temps de vie comparable à une désintégration β.

Cependant, la désintégration des particules étranges neutres n'entrait pas dans le cadre de cette théorie, car les neutrinos – pierre angulaire de la théorie de Fermi – n'intervenaient pas, ou très rarement.

Ce n'est qu'en 1973 que des expériences réalisées au CERN sur la chambre à bulles Gargamelle avec un faisceau de neutrinos ont permis d'établir l'existence d'une interaction faible électriquement neutre. En effet, selon la théorie de Fermi, dans une interaction faible, un neutrino est toujours produit avec un positron, ou un antineutrino avec un électron, ce qui donne un ensemble électriquement chargé.

On s'attendait donc à ce que dans les faibles – et donc rares – interactions entre les neutrinos et la matière, un neutrino donne un électron. On a observé ce phénomène, mais on a aussi observé le phénomène inattendu d'un neutrino interagissant, en gardant sa charge nulle. Il fallait conclure à l'existence d'une interaction faible neutre, faisant intervenir un "courant neutre". Ceci donnait déjà un corps indirect aux prévisions faites sur la théorie électrofaible.

Quand la théorie unifiée de l'interaction faible a été proposée, il n'existait aucun moyen réaliste de tester directement l'existence des bosons "W" et "Z". Le CERN était alors en train de construire un nouvel accélérateur à protons de 400 GeV, le SPS, dont les premiers essais ont lieu en 1976, et la mise en place des halls d'expérience terminée en 1978. Cependant, sur la suggestion de Carlo Rubbia, il est presque immédiatement décidé de le transformer en collisionneur à protons et antiprotons.

Dans un accélérateur, les particules sont accélérées, en un faisceau suivant un chemin à sens unique, tout autour de l'anneau, guidées par des (électro)aimants, et sont éjectées à la fin de l'accélération vers des halls d'expérience où elles rencontrent des cibles fixes.

Dans un collisionneur, deux faisceaux de particules circulent, un dans chaque sens. Leurs particules se rencontrent aux points où leurs trajectoires se recoupent.

Le gros inconvénient de l'accélérateur est que dans la collision sur la cible du hall d'expérience, une majeure partie de l'énergie de la particule accélérée est utilisée pour emporter tous les fragments de la collision dans l'élan communiqué par la particule énergique. C'est ainsi que les protons de 400 GeV émis par le SPS rencontrant un proton-cible fixe consacrent 372 GeV à emporter l'ensemble des produits de la collision, et il ne reste que 28 GeV disponibles pour les réactions qui peuvent survenir entre le proton accéléré et le proton cible. C'est en particulier notablement insuffisant pour produire des "W" ou "Z".

Dans un collisionneur, au contraire, on peut disposer de la somme des énergies des particules qui se choquent pour faire des réactions. Ainsi, dans un collisionneur entre protons et antiprotons de 400 GeV chacun, il y a 800 GeV pour l'ensemble des produits de la réaction.

Il est difficile de transformer un accélérateur en collisionneur. L'accélérateur ne disposant que d'une seule "piste" pour faisceau, si l'on veut faire tourner un faisceau en sens inverse, ce doit être un faisceau de particules de même masse et de charge électrique opposée. Dans le cas d'un accélérateur à protons, ce doivent donc être des antiprotons.

Ceci nécessite donc la construction d'une usine à antiprotons, avec un accélérateur proche, qui n'a pas besoin d'être très haut en énergie, mais qui doit avoir une très forte intensité. Il faut ensuite récolter la plupart des antiprotons produits, les canaliser, et les regrouper dans un anneau de stockage intermédiaire, où on va pouvoir uniformiser leurs angles et leurs vitesses (processus dit de refroidissement par analogie avec le concept correspondant en physique statistique), pour pouvoir les injecter de façon cohérente et efficace dans le collisionneur.

Il faut aussi réaliser de nouveaux tunnels d'injection, allant en sens inverse de ceux des protons.

Enfin, il faut changer le régime supportable par les aimants de trajectoire. En effet, dans un accélérateur, le champ magnétique créé par les aimants qui définissent la trajectoire dépend à chaque instant de l'énergie atteinte par le faisceau, afin de le maintenir sur la trajectoire prévue. En fin de cycle d'accélération, quand le faisceau est éjecté, les aimants sont ramenés au champ convenable pour l'injection. Donc en moyenne, les aimants doivent supporter un courant bien inférieur à celui correspondant à l'énergie maximale d'éjection.

Par contre, dans un collisionneur, quand on a réussi à fabriquer des antiprotons, à les injecter et à les accélérer à l'énergie souhaitée, on ne peut pas se permettre de les gaspiller en les éjectant. Les deux faisceaux doivent être maintenus à énergie constante, voisine du maximum. Ceci suppose donc une amélioration du système de refroidissement des aimants, qui doit alors fonctionner en régime pratiquement continu.

Carlo Rubbia avait fait sa proposition en 1976. Personne à l'époque n'avait jamais construit d'usine à antiprotons avec les contraintes mentionnées ci-dessus. Simon van der Meer releva le défi, si bien que le collisionneur – désormais désigné par formula_6 – fut mis en fonction en 1981.

Et dès 1983, les deux expériences installées autour des points de collision entre les deux faisceaux, dénommées UA1 et UA2 (pour "Underground Area 1 & 2", soit « Zone souterraine 1 & 2 », puisque l'accélérateur SPS était construit sous terre), donnèrent en première mondiale des résultats concordants sur la détection directe des bosons "W", et quelques mois plus tard, sur celle du boson "Z", plus difficile à mettre en évidence. En 1984, Carlo Rubbia et Simon van der Meer partagèrent le prix Nobel de physique, après un des délais les plus courts entre une découverte et cette récompense.

Ultérieurement, le CERN rendit à sa destination initiale le SPS, et se mit en devoir de construire un collisionneur électron-positon, le LEP.

Construit dans un tunnel de 27 km de périphérie, le LEP a atteint une énergie de 104 GeV par faisceau. Mais les premières expériences ont porté sur la formation de "Z" par annihilation directe électron-positon. La masse du "Z" ainsi produit étant directement reliée à l'énergie bien connue des faisceaux, il a été possible de déterminer avec une extrême précision cette masse, ainsi que la "largeur" d'indétermination de cette masse, reliée au temps de vie fini du "Z" par le principe d'incertitude.

On a ainsi abouti à une masse "m" = 91,187 GeV, avec une erreur expérimentale de ± 0,002 GeV, et à un temps de vie de 2,64×10 s. En calculant les contributions à l'instabilité provenant des neutrinos, on déduit que 3 types de neutrinos existent, tout au moins des neutrinos suffisamment légers pour que le "Z" puisse se désintégrer dans une paire correspondante, c'est-à-dire des neutrinos de moins de 45 GeV. Ce sont les 3 types de neutrinos connus.

Les collisions à énergie supérieure ont permis de confirmer ces résultats, quoiqu'avec une moindre précision. Elles ont aussi permis de déterminer les caractéristiques du "W" : masse de "m" = 80,4 GeV avec une erreur expérimentale de ± 0,03 GeV, et temps de vie de 3,07×10 s.

Comme toutes les théories des champs formelles, la théorie électrofaible est fondée sur l'étude d'un lagrangien, qui est une densité dans l'espace-temps formée par un polynôme des champs des particules impliquées dans la théorie, ainsi que de leurs dérivées.

Le lagrangien de l'interaction électrofaible est composé de quatre parties, avant la brisure de la symétrie électrofaible.

Le terme "g" décrit l'interaction entre les 3 particules W et la particule B.

Le terme "f" donne le terme cinétique des fermions du modèle standard. L'interaction entre bosons de jauge et fermions a lieu par le biais des dérivées covariantes (au sens des théories de jauge).

Le terme "h" décrit le champ de Higgs complexe "h".

Ce terme a un signe inhabituel pour le terme quadratique correspondant usuellement à la masse. Il en résulte que la valeur "h = 0" est instable. Le champ prend donc dans le vide une valeur correspondant au minimum de "|h| - v/2", soit "|h| = v/4". Ceci fixe le module de la valeur de "h" dans le vide, mais en laisse la phase arbitraire. Le choix de la phase déterminera une orientation spécifique dans le groupe de jauge global, et en brisera donc la symétrie.

Le terme "y" donne l'interaction de Yukawa qui engendrera les masses de fermion quand le Higgs aura acquis une valeur moyenne dans le vide.

Le lagrangien se réorganise quand le boson de Higgs acquiert une valeur moyenne dans le vide. En raison de sa complexité, ce lagrangien se décrit au mieux en séparant explicitement la valeur constante du champ de Higgs dans le vide, ce qui amène à le décomposer en différentes parties comme suit :

Le terme cinématique formula_13 contient tous les termes quadratiques du Lagrangien, y compris les termes dynamiques (les dérivées partielles) et les termes de masse (remarquablement absents du lagrangien avant la brisure de symétrie), issus du couplage résiduel entre les champs et la valeur moyenne dans le vide du champ de Higgs :

où la somme parcourt tous les fermions de la théorie (quarks et leptons), et les champs formula_15, formula_16, formula_17, et formula_18 sont donnés par :

(remplacer "X" par le champ concerné, et "f" par les constantes de structure du groupe de jauge). Il apparaît bien que le champ "A" n'acquiert pas de masse, tandis que "Z", "W" en acquièrent une, qui est élevée.

Les composantes des courants neutre formula_20 et chargé formula_21 du lagrangien contiennent les interactions entre fermions et bosons de jauge :

où le courant électromagnétique formula_23 et le courant neutre faible formula_24 sont :

et :

"q" et "I" sont les charges électrique et isospin faible des fermions.

L'interaction par l'intermédiaire des courants neutres se décompose donc en une interaction propagée par le champ "A" de masse nulle, et en une interaction propagée par le champ "Z", qui ne peut être à l'état virtuel que sur une distance infime proportionnelle à "1/m".

La partie courant chargé du lagrangien est :

L'interaction propagée par le champ "W" est également ponctuelle à basse énergie. C'est l'interaction ponctuelle de Fermi.

formula_28 contient les termes d'auto-interaction du Higgs de degrés 3 et 4 :

formula_29

formula_30 contient les termes d'interaction du Higgs avec les bosons vecteurs de jauge :

formula_32 contient les termes d'auto-interaction des bosons vecteurs de jauge de degré 3 :

formula_33

formula_34 contient les termes d'auto-interaction des bosons vecteurs de jauge de degré 4 :

formula_35
Les 4 interactions précédentes ne sont encore pas sujettes à l'expérience, compte tenu du fait que l'on n'a même pas encore observé le boson de Higgs, et l'interaction de bosons "W"" ou "Z"" réels entre eux est peut-être plus difficile encore à observer.

et finalement formula_36 contient les termes d'interaction de Yukawa entre fermions et champ de Higgs :

Cette dernière interaction, ou celle entre des bosons vecteurs lourds virtuels, permettrait de produire et d'observer des bosons de Higgs, peut-être auprès du LHC.





</doc>
<doc id="15004" url="https://fr.wikipedia.org/wiki?curid=15004" title="Mahabalipuram">
Mahabalipuram

Mahabalipuram ou Mamallapuram (en tamoul , « la ville de Mahabali ») est une ville indienne dans le district de Kanchipuram (Tamil Nadu). Elle est située à au sud de Chennai (Madras) sur la côte de Coromandel; elle servait de port à Madras au Moyen Âge. Elle abrite un site archéologique et des temples de première importance en Inde du sud.

Le port de Mahabalipuram était connu déjà au temps de la Grèce antique. Au , du temps du règne de la dynastie Pallava, Mahabalipuram était un port important sans doute en communication avec le Srivijaya en Indonésie et le Royaume du Champa sur la péninsule indochinoise. Cependant, si aucune installation portuaire n'a été retrouvée à ce jour, le tsunami, conséquence du tremblement de terre du 26 décembre 2004, a mis au jour des structures qui pourraient être reliées à cette activité.

Le site comporte un grand nombre de monuments hindouistes dédiés à Shiva, à Vishnou, mais aussi à Krishna et aux héros du "Mahabaratha". Les trois principaux monuments ou groupes de monuments sont :

La "Descente du Gange" est un bas-relief datant du , probablement le plus grand au monde. Les sculptures qui couvrent la totalité de la surface de deux énormes rochers, soit de long sur de haut, dépeignent le cours du Gange depuis les Cieux et l'Himalaya tel que décrit dans le "Panchatantra".

Selon le "Ramayana", le roi d'Ayodhya Bhagiratha, de la lignée d'Iksvaku, lui-même ancêtre de Rama, se livra à une très dure ascèse durant mille ans afin d'accomplir les rites funéraires et purifier les cendres de ses soixante mille ancêtres. À force de courage, il obtint de Brahma la descente sur terre de la Ganga. Cependant le flot impétueux du fleuve aurait anéanti toute vie, tant sa force était grande si, à force de nouvelles austérités, le roi n'avait obtenu de Shiva la faveur de recueillir le Gange dans sa Jata (chignon d'ascète) pendant encore cent ans. Au terme de ces années, son cours avait été ralenti et Shiva put le laisser couler librement. Cependant, alors que le Gange dévalait son lit, il aspergea l'autel de l'ascète Jahnu, qui, contrarié, l'avala. Bhagiratha le pria de l'excuser, et l'ascète permit au Gange de sortir par son oreille afin de terminer son œuvre de purification, d'où le nom de "Jahnavi", fille de Jahnu, que l'on donne parfois à la Ganga.

La fissure centrale représentant le cours du Gange est peuplée de créatures aquatiques tels des nâgas et naginis. De part et d'autre de cette représentation du fleuve, se trouve l'image de Shiva. Au-dessus de celle-ci, on trouve les ruines d'un réservoir, ce qui laisse penser qu'autrefois de l'eau s'écoulait pour matérialiser le Gange à l'occasion de rituel et d'offrandes. Hormis les nombreuses représentations divines, le bas-relief dépeint la vie de village en Inde au , notamment des scènes de la vie quotidienne. Dans la partie supérieure, à droite de la fissure, le donateur et mécène, le roi Pallava Mahendravarman (580–630) est représenté en compagnie de ses trois épouses. Dans le bas, à droite de la fissure, on reconnaît un chat yogi en posture de méditation, des souris insouciantes dansant autour de lui. La scène figure un dicton de la sagesse populaire indienne qui conseille de se méfier des faux sâdhus. De part et d'autre de ce relief sont figurés de grands éléphants dont l'interprétation reste incertaine : ils figurent peut être les piliers du monde, placés dans le monde souterrain : c'est là que les ancêtres de Bhagiratha avaient été réduits en cendres par Brahma. Celui-ci, sous la forme du sage Kapila, avait dérobé le cheval de l'ashvameda pour faire mourir les fils de Sagara, les soixante mille ancêtres de Bhagiratha. Ces événements avaient pour but de préparer cette descente du Gange.

Le bas-relief est aussi appelé parfois la "Pénitence d'Arjuna", du nom du principal héros des frères Pandava dans le "Mahabharata", répondant de fait à l'attribution des cinq Ratha du même site. À gauche du bas-relief, se trouve un petit temple excavé appelé Pancha Pandava Mandapa.

L'attribution de ce relief à un épisode du "Ramayana" ou du Mahabaratha fait encore débat et est très souvent sujet à polémique dans les milieux universitaires.

Le "Temple du Rivage" est un temple construit de 700 à 728 par le roi Pallava au bord de la côte du golfe du Bengale. C'est un des premiers temples construits par opposition aux temples creusés dans des grottes ou excavés dans des falaises.

D'après la tradition, c'est le seul temple restant de l'ensemble mythique des . Le temple, qui a souffert depuis douze siècles de sa situation sur le rivage, est maintenant protégé de l'érosion éolienne par une haie et de celle des vagues par des blocs de rocher mis en place par le gouvernement d'Indira Gandhi, blocs qui lui ont permis de résister à la vague du tsunami du . Cependant, cette vague qui a déplacé de grandes quantités de sable sera peut-être à l'origine de futures découvertes concernant le site.

Les "cinq Ratha" (Pancha Ratha) — Yudhishthira (ou Dharmaraja), Bhima, Arjuna, Draupadi et Nakula-Sahadeva — sont des monuments monolithiques de tailles et de formes différentes excavés d'une petite colline, descendant en pente douce vers le sud, au sud du village.

Le terme "ratha" est incorrectement utilisé ici car il signifie « chariot » (voir Puri), comme ceux utilisés dans les processions. Les Ratha de Mahâbalipuram n'ont pas de roue, contrairement au temple de Surya de Konarak qui est en forme de chariot avec des roues, tiré par des chevaux sculptés.

Le Dharmaraja a été dégagé à partir de la partie la plus haute de la colline, puis suivent par ordre décroissant de taille, le Bhima, l'Arjuna et le Draupadi.

Le Sahadeva a été excavé d'une roche un peu plus grande placée à l'ouest de Draupadi. Juste devant le Draupadi, deux autres roches plus petites ont été sculptées en forme d'éléphant (G) et de lion (H), le véhicule de Durga. Derrière le Draupadi et l'Arjuna, qui se tiennent sur une plate-forme commune, se trouve un Nandin (B), un taureau, véhicule de Shiva.

Ces Ratha représentent les formes de temples en service à l'époque de leur excavation et qui étaient faits de matériaux périssables.


Il y a quatre autres ratha ailleurs dans Mahabalipuram. Un grand nombre de temples, souvent excavés, sont aussi éparpillés sur le territoire du village. Enfin, on trouve également dans le village un énorme rocher vaguement sphérique appelé la boule de beurre de Krishna.

Le site de Mahabalipuram est inscrit au patrimoine mondial de l'Unesco depuis 1985.

Mahâballipuram accueille l'un des plus importants festivals de danse indienne en janvier et février. Les danses de style Bharat Natyam, Kuchipudi, Kathak, Mohiniattam, Odissi et Kathakali sont interprétées avec la magnifique "Descente du Gange" comme toile de fond. Les figures les plus éminentes dans leur art se rassemblent pour cet événement culturel qui est également promu par le bureau du tourisme du Tamil Nadu.




</doc>
<doc id="15007" url="https://fr.wikipedia.org/wiki?curid=15007" title="Macroéconomie">
Macroéconomie

La macroéconomie est l'approche théorique qui étudie l'économie à travers les relations existantes entre les grands agrégats économiques, le revenu, l'investissement, la consommation, le taux de chômage, l'inflation, etc. 

La macroéconomie constitue l'outil essentiel d'analyse des politiques économiques des États ou des organisations internationales. Il s'agit d'expliquer les mécanismes par lesquels sont produites les richesses à travers le cycle de la production, de la consommation, et de la répartition des revenus au niveau national.

Selon Frédéric Poulon, la macroéconomie est avant-tout une représentation hiérarchisée de l'économie, articulée entre les agents via des flux.
En considérant d'emblée les relations entre les grands agrégats de l'économie, la macroéconomie cherche à expliciter ces relations et à prédire leur évolution face à une modification des conditions, qu'il s'agisse d'un choc pétrolier (augmentation de prix du pétrole) ou d'une politique économique délibérée. Contrairement à la microéconomie, qui favorise les raisonnements en équilibre partiel, la macroéconomie se place toujours dans une perspective d'équilibre général, ce qui l'amène à accorder plus d'attention au bouclage des modèles et à la dynamique de création et de maintien d'institutions essentielles, comme les marchés, la monnaie.

Partie de relations très simples, à l'image du modèle IS/LM reliant le marché des capitaux et celui de la monnaie ou de la courbe de Phillips reliant inflation et chômage, la macroéconomie a évolué vers la construction de modèles économiques complexes incluant à la fois des relations supposées entre variables et des relations comptables servant à définir les agrégats. Très utilisés pour analyser et prévoir les résultats des politiques économiques, ces vastes modèles (les plus frustes comportent une dizaine d'équations, les plus complexes dépassent les ) sont à l'heure actuelle employés par la plupart des gouvernements, institutions statistiques (comme l'INSEE), organisations internationales (OCDE) et certains acteurs privés voulant disposer de leurs propres prévisions quant à la conjoncture.

Selon les penseurs grecs, l'économie désigne l'art de bien administrer sa maison. La microéconomie est donc historiquement la première forme qu'a pris l'économie. Il faut attendre le , et surtout, avec le courant physiocrate pour avoir une première vision macroéconomique, c'est-à-dire, une représentation hiérarchisée de l'économie via des flux entre agents. Cette représentation se trouve dans l'ouvrage de François Quesnay, "Tableau économique". Quesnay, médecin de la famille royale, avait pour ambition de représenter l'économie sur les bases de la circulation du sang. Toutefois, les considérations philosophiques des physiocrates et les évènements historiques ont rapidement rendu son "Tableau économique" obsolète.

Karl Marx, un siècle plus tard, proposera une représentation schématique de l'économie industrielle de son époque. Parallèlement, les fondateurs de l'école néoclassique ont utilisé la théorie marginaliste, pour agréger les comportements des agents économiques, c'est-à-dire les consommateurs et les producteurs. Cette microéconomie agrégée, approche souvent à la base de certaines théories macroéconomiques, est à la base de la théorie de l'équilibre général de Léon Walras, et complété par Kenneth Arrow et Gérard Debreu. Cette vision de l'économie ne peut toutefois pas se confondre avec la macroéconomie, étant donné qu'elle ne se base que sur des comportements individuels, et n'analyse pas l'économie dans son ensemble.

La distinction systématique, pour autant qu'elle puisse vraiment se faire, entre microéconomie et macroéconomie n'émerge cependant vraiment qu'au cours des années Trente autour des travaux de John Maynard Keynes. Ce fut surtout le retentissement de sa "Théorie générale de l'emploi, de l'intérêt et de la monnaie" (1936) après-guerre qui conduisit à une séparation nette, d'abord dans le milieu académique, des deux domaines. La microéconomie se spécialisait alors sur les problèmes d'allocation des ressources par le moyen des prix relatifs, alors que la macroéconomie étudiait la production globale et le niveau des prix. Soulignons que J.M. Keynes est vu comme le père fondateur de la macroéconomie, du moins à la suite du succès de son ouvrage, essentiel aux réflexions sur la crise de 1936.

Écornée par l'échec des keynésiens à prévoir et à enrayer la stagflation consécutives aux chocs pétroliers, la macroéconomie de la fin du présente un double visage: 

D'une part, on assiste à la construction de modèles de plus en plus complexes et élaborés, construction rendue possible par l'augmentation des capacités de calcul des ordinateurs ainsi que la généralisation des techniques d'optimisation dynamique. Cette voie est également soutenue par l'amélioration considérable des données dont disposaient les macroéconomistes pour tester leurs modèles (voir macroéconométrie). Il apparait cependant que la complexification des modèles n'apporte pas grand-chose en matière de pouvoir explicatif, et que les problèmes de cohérence deviennent difficilement surmontables avec un aussi grand nombre d'équations. L'ensemble de l'approche est également remise en cause par la critique de Lucas, l'économiste Robert Lucas Jr faisant remarquer que les relations macroéconomiques échouent à prendre en compte les réactions d'agents informés aux politiques économiques (voir Courbe de Phillips et Critique de Lucas)

D'autre part, des économistes formés à la microéconomie néoclassique cherchent à donner des fondements microéconomiques aux agrégats observés, en dérivant des grandeurs comme l'offre de travail ou l'investissement des offres des modèles microéconomiques. Ces tentatives, connues sous le nom de synthèse néoclassique, échouent cependant sur le problème de l'agrégation, avec des résultats démontrant que ce passage du niveau micro au niveau macro n'est possible qu'en imposant des hypothèses absurdement restrictives sur le comportement des agents (voir équilibre général). 

L'approche néoclassique utilise alors au concept d'agent représentatif, supposant que les agrégats économiques se comportent comme s'ils répondent aux décisions d'un agent économique unique similaire à l'agent rationnel du niveau microéconomique. La capacité de ces modèles à prédire des résultats opposés en fonction des hypothèses faites sur l'agent représentatif et sur les paramètres de base jettent un doute profond sur la pertinence de cette approche.

Au début du , des économistes cherchent à dépasser la distinction entre microéconomie et macroéconomie. La plupart des modèles macroéconomiques actuels font l'hypothèse qu'ils ne constituent qu'une simplification de la réalité, dont ils étudient un aspect particulier, comme l'effet de l'innovation sur la croissance, ou des structures monétaires sur l'investissement. De ce fait, ils mélangent relations macroéconomiques et extensions au niveau macroéconomique de relations microéconomiques pour autant que ces extensions soient compatibles avec les faits stylisés qu'on cherche à analyser. 

Il existe cependant de nombreuses écoles et courants de pensée touchant à leur conception méthodologique et à leurs préconisations en matière de politiques économiques.

Plusieurs écoles utilisent en pratique des raisonnements macroéconomiques, avec des principes et des recommandations souvent très différents :




</doc>
<doc id="15013" url="https://fr.wikipedia.org/wiki?curid=15013" title="Événement (probabilités)">
Événement (probabilités)

En théorie des probabilités, un événement lié à une expérience aléatoire est un ensemble dont les éléments sont des résultats possibles pour cette expérience (c'est-à-dire un certain sous-ensemble de l'univers lié à l'expérience). Un événement étant souvent défini par une proposition, nous devons pouvoir dire, connaissant le résultat de l'expérience aléatoire, si l'événement a été réalisé ou non au cours de cette expérience.

Soient formula_1 l'univers d'une expérience aléatoire, formula_2 une tribu sur formula_1, et formula_4 l'espace probabilisable ainsi constitué. On appelle événement toute partie de formula_1 qui appartient à la tribu formula_2. 

Si l'événement est constitué d'un seul élément, on parle alors d'un événement élémentaire.

L'univers formula_1 est un événement, regroupant toutes les issues possibles, appelé événement certain.

L'ensemble vide formula_8 est un événement, appelé événement impossible.

Pour tout formula_9 appartenant à formula_1, représentant une issue possible, le singleton formula_11 est un événement, appelé événement élémentaire.

Supposons que l’expérience aléatoire considérée soit le tirage à pile ou face d’une piece de monnaie. L’univers de l’expérience formula_1 comporte alors deux issues possibles, "pile" et "face", et on peut définir pour cette expérience une tribu de quatre événements :

Supposons qu'on dispose de 52 cartes et de deux jokers sur une table et qu'on tire une seule carte. Le tirage d'une carte particulière dans l'univers des 54 cartes, représente alors un événement élémentaire. Les sous-ensembles (y compris les événements élémentaires) sont simplement appelés des « événements ». Des événements de cet univers peuvent être : 

Supposons qu’un assureur automobile considère un échantillon d'automobiliste présentant certains risques. Les événements qui seront considérés pourront être de dépasser ou non un montant total de sinistres supérieur à la franchise. La notion d'événement en probabilités n'est donc pas identique à la notion d'issue. La définition des évènements pourra dépendre par exemple de la conception que l'on a du risque (ou vice versa de la chance).

Les événements étant des ensembles d'issues, on peut leur appliquer toutes les opérations ensemblistes usuelles.

Soient formula_15 deux événements.

Soient formula_15 deux événements. Alors: 


Soient A, B, C trois événements.

Crescendo, six cas probables s'offrent à l'expérience :








</doc>
<doc id="15017" url="https://fr.wikipedia.org/wiki?curid=15017" title="Espace probabilisé">
Espace probabilisé

Un espace de probabilité(s) ou espace probabilisé est construit à partir d'un espace probabilisable en le complétant par une mesure de probabilité : il permet la modélisation quantitative de l'expérience aléatoire étudiée en associant une probabilité numérique à tout événement lié à l'expérience. Formellement, c'est un triplet formula_1 formé d'un ensemble formula_2, d'une tribu formula_3 sur formula_2 et d'une mesure formula_5 sur cette tribu tel que formula_6.

L'ensemble formula_2 est appelé l'univers et les éléments de formula_3 sont appelés les événements.
La mesure formula_5 est appelée "probabilité" ou, mieux, "mesure de probabilité", et pour un événement formula_10 de formula_3, le nombre réel formula_12 s'appelle la probabilité de l’événement formula_10.

Ce qui précède est une formulation extrêmement condensée des axiomes des probabilités.

Remarquons que lorsque formula_2 est infini non dénombrable, n'importe lequel de ses sous-ensembles n'est plus nécessairement un événement : en effet, dans ce cas précis, la tribu des événements est choisie strictement incluse dans l'ensemble des parties de l'univers.



</doc>
<doc id="15019" url="https://fr.wikipedia.org/wiki?curid=15019" title="Liste des municipalités de Hidalgo">
Liste des municipalités de Hidalgo

L'État d'Hidalgo est un État du Mexique composé de 84 municipalités. Pachuca de Soto en est la capitale.

Le code INEGI complet de la municipalité comprend le code de l'État - 13 - suivi du code de la municipalité. Exemple : Pachuca de Soto = 13048. Chaque municipalité comprend plusieurs localités portant leur propre code. Ainsi, pour le chef-lieu de la municipalité, Pachuca de Soto = 130480001.




</doc>
<doc id="15022" url="https://fr.wikipedia.org/wiki?curid=15022" title="Torah">
Torah

La Torah ou Thora (en hébreu , « instruction » ; en grec ancien – "Nomos" –, « Loi ») est, selon la tradition du judaïsme, l'enseignement divin transmis par Moïse ( – "Tōraṯ Mōshe") au travers de ses cinq livres (hébreu : – "Ḥamishā Ḥoumshē Tōrā") ainsi que l'ensemble des enseignements qui en découlent.

Elle est composée de cinq livres désignés en hébreu par un des premiers mots du texte et traditionnellement en français : la Genèse ("Berēshīṯ" : Commencement), l'Exode ("Shemōṯ" : Noms), le Lévitique ("Wayyiqrā"' : Et il appela), les Nombres ("Bamiḏbar" : Dans le désert), le Deutéronome ("Devarim"/ "Deḇārīm" : Paroles).

La Torah sert de charte historique et doctrinale au judaïsme orthodoxe. Elle contient, selon la tradition juive, 613 commandements et comporte, outre la composante écrite (hébreu : תורה שבכתב, "Tōrā sheBikhtāḇ" : « Torah écrite »), une dimension orale (hébreu : תורה שבעל פה, "Tōrā sheBeʿal Pe" : « Torah orale »), ultérieurement compilée dans le Talmud et la littérature midrashique.

Le christianisme appelle Pentateuque les livres traditionnellement attribués à Moïse, terme d'origine grecque "Πεντάτευχος" qui signifie . Il les reconnaît comme faisant intégralement partie des Écritures canoniques ("Ancien Testament"), bien qu'il en ait partiellement abandonné les préceptes rituels et qu'il ne reconnaisse pas d'autorité aux enseignements rabbiniques. Le christianisme soutient en effet que le message du Christ diffusé par le Nouveau Testament conduit à l'accomplissement de la Torah (Matthieu 5, 17-20), désormais objet d'une observance intériorisée et d'une interprétation allégorique, comme l'attestent les écrits de Paul de Tharse dès le milieu du (Première épître aux Corinthiens).

La Torah est aussi reconnue par l'islam, selon lequel elle aurait cependant été falsifiée.

La Torah désigne "stricto sensu" la première section du Tanakh – les cinq premiers livres de la Bible hébraïque – mais le terme est également employé pour désigner tant la loi écrite ("Tōrā sheBikhtāv") que la loi orale ("Tōrā sheBeʿal Pe"), qui contient un ensemble d'enseignements religieux juifs, incluant le Talmud (étude), lui-même formé de la Mishnah (répétition), de la Guémara, du Midrash (récit), et d'autres.

La Torah fut, selon la tradition, dictée à Moïse par Dieu sur le Mont Sinaï. Pour les juifs, elle a traditionnellement été acceptée comme telle : la parole littérale de Dieu au peuple juif tout entier au mont Sinaï.

Toutefois, cette affirmation est remise en cause dès le , notamment par certains érudits et philosophes comme Isaac ibn Yashush, Maïmonide et Abraham ibn Ezra, qui dressent la liste des « post-mosaica » - textes ou éléments rédigés après l'époque mosaïque - sans remettre pour autant en cause la tradition reçue. Cependant le premier à rejeter l'idée que Moïse a écrit les cinq livres est Andreas Bodenstein (1486-1541), un théologien protestant qui examine aussi dans son ouvrage la possibilité qu'Esdras soit le véritable auteur du Pentateuque pour finalement la repousser. Le pas est franchi par Baruch Spinoza, dans son "Tractatus theologico-politicus" où il souligne l'unité organique entre la Torah et les livres « historiques » (de "Josué" aux "Rois"), et en attribue la rédaction à Esdras.

Aujourd'hui, après avoir connu un consensus dans les années 1970 autour de l'hypothèse documentaire, diverses autres théories ont refait surface pour expliquer l'origine de la Torah, dont la théorie des fragments et la théorie des compléments. Malgré leurs divergences, ces théories s'accordent toutefois sur le fait que la Torah est une collection de textes mis en commun par des scribes autour de la période de l'exil et après. La publication de cette littérature de compromis, qui ne cherche pas à gommer les divergences des options théologiques, peut se comprendre comme la mise en place d'une matrice identitaire du judaïsme naissant, une réponse aux changements politiques, économiques et religieux auxquels celui-ci se trouve confronté.

La critique radicale biblique reçoit peu de soutien chez les Juifs orthodoxes. La critique des livres bibliques hors la Torah (Neviim et Ketouvim) est tolérée, quoique d'un mauvais œil, mais l'appliquer à la Torah elle-même est considéré comme erroné, voire hérétique. L'hypothèse documentaire, combattue par l'érudit Umberto Cassuto, a cependant fait l'objet de commentaires du Malbim et du Rabbin Samson Raphael Hirsch.

L'étymon du mot « Torah » est le même que celui de "Mōrē", מורה, « l'enseignant » : "Līrōṯ", לירות, « tirer », au sens de « viser à un objectif ».

Parmi les enseignements relatés dans le Tanakh, on peut trouver :


Les descendants d'Israël n'en jouiront cependant qu'en le servant, en respectant ses prescriptions, sans quoi, ils en seront chassés comme Adam fut chassé du Jardin d'Éden. On peut (artificiellement) subdiviser le service en :

Le peuple croyant que Moise est mort, une petite partie du peuple se fabrique un nouvel intermédiaire par un veau d'or. Surtout, les habitudes contractées en Égypte ont la vie dure : tandis que Moïse se trouve sur le Sinaï, une partie du peuple souhaite se construire un veau d'or pour l'honorer comme son dieu. Il faudra errer dans le désert durant 40 ans, le temps que meure la génération qui a connu l'Égypte, jusqu'à Moïse lui-même, le temps qu'Israël apprenne à vivre selon la Torah. Moïse préfère le lui rappeler au seuil de Canaan, avant de mourir en un lieu indéterminé.

Les cinq livres contiennent donc un système de lois et d'éthique, à la fois complet et ordonné (selon la tradition rabbinique, la Torah comporte 613 « commandements » distincts, positifs — « fais » — ou négatifs — « ne fais pas », chacun appelé "miṣwā", « prescription »), ainsi qu'une description historique des débuts de ce qui deviendrait le Judaïsme.

Les cinq livres (en particulier "Bereshit"/Genèse, la première partie de "Shemot"/Exode, et une grande partie de "Bamidbar"/Nombres) apparaissent à première vue plutôt comme un ensemble de narrations apparemment historiques que comme une énumération de lois ; pourtant, beaucoup de concepts, d'idées et de commandements toraïques sont contenus dans ces « histoires », au point que certains disputent leur historicité (cf. infra).

Le Deutéronome est différent des livres précédents : il est écrit à la première personne. Il s'agit en fait, comme indiqué plus haut du dernier discours et des dernières recommandations de Moïse aux "enfants d'Israël" avant de mourir.

Beaucoup de lois ne sont cependant pas directement mentionnées dans la Torah : elles en ont été déduites par exégèse et traditions orales, avant d'être compilées dans la Mishna, le Talmud, la Mekhilta de Rabbi Ishmaël et autres traités moins souvent étudiés. Les Karaïtes ne reconnaissant pas l'autorité des rabbanim (maîtres), ils ne suivent tout simplement pas ces lois.

D'autre part, selon la tradition rabbinique du moins, les histoires dans la Torah ne se déroulent pas nécessairement dans l'ordre chronologique, mais parfois par ordre de concept (« le futur expliquant le passé », par exemple). Cette vue est résumée par la maxime talmudique (traité Pessa'him 7a) : « [Il n'y a] pas de « [plus] tôt » et « [plus] tard » dans [la] Torah ».

Le livre de la Torah existe sous deux formes différentes selon son usage :

L'écriture des "Sifrē Tōrā", ou "Sefārīm", se fait selon des règles extrêmement contraignantes et précises, et ne sont confiées en conséquence qu'à des scribes professionnels hautement qualifiés. C'est en vertu de ces règles que ce texte plurimillénaire nous est arrivé inchangé, et que des copies datant de plusieurs siècles, voire de millénaires, sont virtuellement identiques entre elles. L'accent a été mis sur ce souci de précision au point de dire que chaque mot, chaque lettre, chaque signe même est d'origine divine, et que s'il en manquait un seul, le monde s'écroulerait.

Il est vrai qu'en hébreu, certaines lettres se ressemblent fortement, et que la vocalisation peut changer le sens d'un mot. Dans un système basé sur l'analyse jusqu'aux plus subtiles nuances de ces mots, une erreur de lecture peut conduire à une erreur de compréhension et une perversion du message. L'analogie avec la récente notion de code génétique a maintes fois été évoquée.

Les "Sefārīm" sont considérés comme l'un des plus grands trésors d'une communauté, et l'acquisition d'un nouveau est prétexte à des célébrations festives. Tous les "Sifrē Tōrā" sont rangés dans l'endroit le plus saint de la synagogue, l'Arche sainte (אֲרוֹן הקֹדשׁ "aron hakodesh" en Hébreu) appelé "Hēkhāl".

Les versions imprimées de la Torah sont traitées avec grand respect, mais leur sainteté est considérée comme inférieure à celle des "Sefārīm" : par exemple, une lettre effacée rend un "Sefēr Tōrā" impropre à l'usage ("passoul"), ce qui n'est pas le cas des "Ḥoummashīm".

La Torah est le document autour duquel le judaïsme s'articule : elle est la source de tous les commandements bibliques dans un cadre éthique. Elle est au centre du culte hebdomadaire : chaque Chabbat, une section est lue publiquement à la synagogue et les fidèles se disputent l'honneur d'en lire un paragraphe. La cérémonie de Bar-Mitzvah est de même centrée sur la lecture de la Parasha.

D'après la tradition juive, ces livres furent révélés à Moïse par Dieu, dont une partie sur le mont Sinaï.

Diverses opinions circulent dans la littérature rabbinique sur le moment où elle fut révélée entière :
D'une manière générale, les tenants du judaïsme orthodoxe s'accordent sur l'origine entièrement (ou quasi-entièrement) mosaïque et tout à fait divine de la Torah. En revanche, le Judaisme massorti (ou conservative) acceptent la critique biblique en soulignant que si la Torah n'a pas été écrite dans sa totalité par Moïse elle est néanmoins d'origine divine, les scribes ayant été inspirés par Dieu.

D'après cette même tradition, le message de la Torah est infini, ne s'arrêtant pas aux mots. La moindre lettre, la plus petite préposition, voire la cédille de la lettre youd ("koutzo shel youd" קוצו של יוד, le youd étant la lettre י), les marques décoratives, les répétitions de mots, furent placées là par Dieu afin d'y celer un enseignement. Ceci est valable quel que soit l'endroit où cela apparaît.

Exemples :

Contre-exemples :
Exauçant la prière de Moïse de comprendre cela, Dieu l'expédie au huitième rang de la Yeshiva de Rabbi Akiva, où précisément, celui-ci enseigne ces lois. Devant l'exposé ardu, Moïse se sent épuisé, lorsqu'un élève se risque à demander d'où Rabbi Akiva tire ces enseignements. Et celui-ci de répondre : « C'est une loi donnée à Moïse sur le Sinaï » !

Une interprétation kabbalistique de ce principe enseigne que la Torah ne constituait qu'un seul long Nom de Dieu, qui fut brisé en mots afin que les esprits humains puissent le comprendre. Par ailleurs, bien que cette façon de décomposer le Nom soit efficace, puisque nous parvenons à l'appréhender, ce n'est pas la seule.

Selon les juifs rabbanites, descendants des Pharisiens, et dont les juifs orthodoxes maintiennent fidèlement l'idéologie, une loi orale ("Torah SheBe'al Pe") fut donnée au peuple en même temps que la Loi écrite ("Torah SheBeKtav"), ainsi que le suggèrent de nombreux versets, notamment . Il s'agissait probablement à la base, outre d'explications quant aux prescriptions, de paraphrases orales du texte, explications d'un tel mot, discussion autour de telle idée dans tel verset, mais en tout cas intimement liée à la loi écrite, et la complétant : de nombreuses notions ne sont pas clairement définies dans le texte. Ce souci de se remémorer les paroles des maîtres alla de pair avec une scrupuleuse exactitude dans le respect et l'application des lois.

Ce matériel parallèle fut originellement transmis à Moïse depuis le Sinaï, et de Moïse à Israël oralement. Dans le souci de maintenir le judaïsme dynamique et d'éviter les mésinterprétations, il était interdit de consigner les traditions orales. Cependant, devant l'accumulation de matériel, les divergences d'interprétations, qui tenaient parfois à des nuances infimes d'une part, et d'autre part la destruction de la Judée par les Babyloniens, le haut taux d'assimilation, etc., l'interdit fut levé, lorsqu'il devint évident que l'écriture devenait le seul moyen de préserver l'héritage oral des Anciens.

Le premier à systématiser les lois en catégories, fut Rabbi Akiva. Son disciple Rabbi Meïr y contribua grandement. Toutefois, le gros du travail est le fait de Rabbi Juda Hanassi, qui acheva cette compilation, et la nomma Mishna (« Répétition »). Les traditions non incluses dans la Mishna furent consignées comme "Baraïtot" ([enseignements] « extérieurs ») ou dans la Tosefta (« Supplément »). Des traditions plus tardives furent également codifiées comme Midrashim.

Au cours des quatre siècles qui suivirent, ce petit corpus de lois et enseignements éthiques suffit à fournir les signes et codes nécessaires pour permettre la continuité de l'enseignement des traditions mosaïques, tout en maintenant leur dynamisme, et leur transmission aux communautés principalement dispersées entre Babylone et la terre d'Israël (devenue la province romaine de "Syria Palestina").

Toutefois, les circonstances historiques contraignirent les communautés galiléennes d'abord, babyloniennes ensuite à compiler le corpus de commentaires de la Mishna, dont les allusions, leçons, traditions, etc. synthétisées en quelques centaines de pages furent développées en milliers de pages, appelées "Guemara". Important changement, alors que la Torah et la Mishna sont rédigées en hébreu (bien que l'hébreu mishnaïque ne soit plus pareil à l'hébreu biblique), la Guemara l'est en araméen, ayant été compilée à Babylone. La notion de "Guemara" est à peu près équivalente à celle de Talmud en hébreu, terme bien plus connu.

Deux « versions » du Talmud existent, le Talmud de Babylone et celui de Jérusalem, en réalité le résultat des compilations des discussions tenues dans les académies babyloniennes d'une part et galiléennes de l'autre. Le Talmud de Jérusalem ayant été terminé à la hâte, sous la pression des circonstances historiques, deux siècles avant celui de Babylone, c'est ce dernier qui fait autorité lorsque les deux se contredisent (y compris deux versions différentes de l'enseignement d'un Rabbi).

Les juifs pratiquants (rabbanites) suivent les explications traditionnelles de ces textes. Les Karaïtes, eux, ne suivent que la Miqra, c'est-à-dire la Torah.

Le christianisme affirme que les lois toraniques sont d'origine divine, mais il les réinterprète selon les principes attribués au Christ, lui-même héritier du courant prophétique du judaïsme, privilégiant l'application spirituelle, morale et intérieure des préceptes au légalisme.

Les positions chrétiennes peuvent être résumées notamment comme suit :
Cependant, le Nouveau Testament prescrit aux chrétiens des lois provenant de la Torah, notamment (Lévitique 19:18; comparer avec la Règle d'Or), (inspiré du Deutéronome 6:4, c'est-à-dire le Shema Israël) et tous les commandements du Décalogue (Exode 20:1-17). Et Matthieu (5:17) stipule bien qu'il n'est pas venu abolir la loi, mais l'accomplir (« la vivre en plénitude »).

Dans l'Eglise anglicane, la confession de foi de Westminster (1646), par exemple, divise les lois mosaïques en catégories civile, morale et cérémoniale, les seules obligatoires étant les morales. Si le reconstructionnisme chrétien voulut les rétablir toutes en vue de construire une théocratie moderne, d'autres estiment qu'aucune loi civile ne s'applique à eux, celles-ci ayant été rédigées en des temps et circonstances révolus, ce qui n'est pas le cas des obligations morales ni des principes religieux.
Depuis la fin du , certains groupes chrétiens, inspirés par le judaïsme messianique, ont affirmé que les lois de la Torah devaient être suivies par les chrétiens, dans une optique et une perspective chrétiennes. Les lois alimentaires, le septième jour, et les jours de fête bibliques sont observés (voir Quartodécimanisme), avec toutefois des variations par rapport aux rites juifs, mais pour la raison que Jésus fut crucifié ce jour).

Ces chrétiens ne voient pas la Torah comme un moyen d'accomplir la rédemption, mais comme un moyen d'obéir plus complètement à Dieu.

La "Tawrat" (Torah) est, avec l'Injil (Évangile) et le Zabur (Psaumes de David) l'un des trois Livres qui furent révélés par Dieu avant le Coran, lequel se veut un « rappel » de ces trois livres. Le mot "Tawrat" est cité en de nombreux endroits du Coran et 

La Torah est la seule partie de la Bible hébraïque que les Samaritains considèrent comme d'autorité divine, à l'exception peut-être du Livre de Josué. Tous les autres livres de la Bible juive sont refusés. Les Samaritains refusent aussi la tradition orale juive (telle qu'exprimée dans la Mishna, puis la Gémara et le Talmud). Le Pentateuque samaritain comporte environ 2 000 versets différents de la version massorétique.

La Bible samaritaine est rédigée en abjad samaritain, la forme primitive de l'alphabet hébreu, dite proto-cananéenne, que les Judéens ont abandonnée pour l'écriture "carrée" assyrienne. On considère cet alphabet comme fidèle à celui utilisé avant la captivité babylonienne.






</doc>
<doc id="15025" url="https://fr.wikipedia.org/wiki?curid=15025" title="Lévitique">
Lévitique

Le Lévitique (en grec ancien , relatif aux Juifs, en hébreu "Vayikra", et Il appela) est le troisième des cinq livres de la "Torah" ("Pentateuque"). Il doit son nom au terme « lévite », prêtre hébreu issu de la tribu de Lévi. Il parle des devoirs sacerdotaux en Israël. Il met l'accent sur la sainteté de Dieu et le code selon lequel son peuple pouvait vivre pour devenir saint. Son but est d'enseigner les préceptes moraux et les rituels religieux de la loi de Moïse.

L'étude de l'hébreu dans le texte montre plusieurs styles d'écriture différents, et l'étude du contenu montre plusieurs préoccupations théologiques qui ne sont pas toujours conciliables. Il est donc raisonnable de penser que le Lévitique, comme l'ensemble du Pentateuque, a été écrit par plusieurs auteurs, et il est aujourd'hui admis que le livre a pris forme durant le , soit de nombreux siècles après Moïse.

Le livre comprend 27 chapitres, qui relatent l'exposé à Moïse de lois et de rites.

Les chapitres 1 à 7 expliquent les ordonnances du sacrifice. Le chapitre 3 est consacré à l'offrande de paix.

Les chapitres 8 à 10 décrivent le rituel observé lors de la consécration des sacrificateurs. 

Le chapitre 11 explique ce que l'on peut ou ne peut pas manger et ce qui est pur ou impur. 

Le chapitre 12 parle de l'état des femmes après l'enfantement. 

Les chapitres 13 à 15 sont des lois relatives à l'impureté cérémonielle :

Le chapitre 16 contient le rituel à respecter pour la fête de "Yom Kippour". Il mentionne le démon Azazel.

Les chapitres 17 à 26 contiennent un code de lois traitant des observances religieuses et sociales. 

Le chapitre 27 explique que le Seigneur a commandé à Israël de consacrer au Seigneur ses récoltes et ses troupeaux de gros et de petit bétail.

Le livre a vraisemblablement pris forme au , durant l'époque perse.

Le commentaire sur le Lévitique dans la tradition du judaïsme rabbinique est le Sifra, aussi appelé Torat Cohanim.

Rabbi Ishmaël a exposé treize principes d'herméneutique dans la Baraïta, qui se trouve en introduction au Sifra. 

Le livre sert à la compréhension de la liturgie chrétienne : dans la théologie de la messe, le sacrifice est interprété comme étant l'Eucharistie, c'est-à-dire l'offrande de Jésus sur la croix.





</doc>
<doc id="15027" url="https://fr.wikipedia.org/wiki?curid=15027" title="Événement élémentaire">
Événement élémentaire

En théorie des probabilités, on appelle événement élémentaire un ensemble de l'univers (un évènement) constitué d'un seul élément. Pour tout formula_1 de formula_2, l'événement élémentaire formula_3 se réalise si et seulement si l'on obtient le résultat formula_1.

Supposons qu'une tribu contienne tous les événements élémentaires ; elle contient alors toutes les parties formula_5 finies ou dénombrables de formula_2, et chacune de ces parties peut s'écrire sous la forme :
La réunion étant disjointe, cette relation permet de déterminer la probabilité de tout événement formula_5 à partir des probabilités des événements élémentaires constituant formula_5.

Pour n'importe quel univers discret (fini ou dénombrable), une probabilité est donc entièrement déterminée par les valeurs qu'elle prend en les événements élémentaires.

Si l'univers est fini et si l'hypothèse d'équiprobabilité des événements élémentaires est applicable, on peut écrire pour tout événement formula_5,



</doc>
<doc id="15028" url="https://fr.wikipedia.org/wiki?curid=15028" title="Univers (probabilités)">
Univers (probabilités)

En théorie des probabilités, un univers, souvent noté formula_1 ou formula_2, est l'ensemble de toutes les issues (résultats) qui peuvent être obtenues au cours d'une expérience aléatoire.

À chaque élément formula_3 de l'univers, c'est-à-dire à chacun des résultats possibles de l'expérience considérée, nous pouvons associer le sous-ensemble formula_4 constitué de cet élément, appelé événement élémentaire. De manière plus générale, toute partie de l'univers est appelée simplement un événement.

On parle également d'espace des événements élémentaires ou d'espace des observables, ou encore d'espace échantillon.

Par exemple, si nous lançons une pièce, nous avons deux résultats possibles : "pile" ou "face". l'expérience aléatoire considérée est alors : « 1 lancer de pièce ». Nous pouvons définir l'univers associé à cette expérience, qui regroupe tous les résultats possibles : formula_5 ≡ {"pile", "face"}. Pour une expérience de lancer de dé, nous choisirions l'univers formula_5 ≡ {1, 2, 3, 4, 5, 6}.

À n'importe quel univers discret (fini et/ou dénombrable), on peut associer une probabilité, qui est entièrement déterminée par les valeurs qu'elle prend sur les événements élémentaires. Ainsi, à chaque événement est associable une probabilité de réalisation. Par exemple, pour le lancer de dé, on va associer à chaque événement de l'univers {1, 2, 3, 4, 5, 6} une probabilité égale à 1/6.

Toute définition de probabilité commence par la recherche d'un univers de tous les événements réalisables et par la définition précise de tous les événements utiles à sa résolution. La recherche de l'univers consiste à représenter de manière unique les issues possibles de l'expérience par des objets mathématiques (nombres, p-listes, p-listes d'éléments distincts, parties d'un ensemble, permutations, suites, ...) pour former un ensemble.

Lorsque l'univers n'est pas discret, on va restreindre les événements à un ensemble de parties dont on peut définir la probabilité, et satisfaisant à certains critères de cohérence : une "tribu", ou σ-algèbre. Le couple obtenu en combinant un univers et une tribu d'événements sur cet univers constitue alors un espace probabilisable. Le triplet obtenu en combinant un univers, une tribu et une probabilité sur cette tribu est appelé un espace probabilisé.

En probabilités, on va souvent étudier non pas les résultats d'une expérience aléatoire mais plutôt des valeurs associées à ces résultats. Pour des lancers de dés, ce pourrait être par exemple la valeur de la face visible, la valeur de la face cachée, la somme des valeurs des faces visibles de deux dés, le gain du joueur, le gain du casino, etc.

Les applications utilisées pour associer une valeur aux éléments de l'univers, sous réserve qu'elles satisfassent à certaines contraintes dites de mesurabilité, sont appelées des variables aléatoires. L'univers d'une expérience aléatoire constitue donc l'ensemble de définition de toutes les variables aléatoires dérivées de cette expérience.

Pour certains types d'expériences, nous pouvons définir plusieurs univers différents. Par exemple, quand nous tirons une carte d'un jeu de 52 cartes, nous pouvons nous intéresser au rang de la carte dans le jeu et définir l'univers comme l'ensemble des entiers de 1 à 52 ; d'autre part, nous pouvons nous intéresser à la couleur de la carte obtenue et définir l'univers comme étant l'ensemble {pique, cœur, carreau, trèfle}. Pour avoir une description complète d'une issue, nous serions amenés à préciser la couleur et le rang de la carte, et à définir dans ce cas l'univers comme le produit cartésien de ces deux ensembles : formula_5 ≡ {pique, cœur, carreau, trèfle} × {2, 3, 4, 5, 6, 7, 8, 9, 10, valet, dame, roi, as}.

Pour choisir l'univers, nous devons aussi tenir compte des probabilités qui entrent dans la définition de l'expérience aléatoire.
Par exemple, dans le cas où le nombre de valeurs possibles de l'expérience aléatoire est fini, il est possible de considérer un univers sur lequel il y a équiprobabilité, c'est-à-dire sur lequel la probabilité est uniforme (par exemple, pour le lancer de dé, si le dé est non truqué, il y a équiprobabilité pour chacun des événements dans {1, 2, 3, 4, 5, 6}.



</doc>
<doc id="15031" url="https://fr.wikipedia.org/wiki?curid=15031" title="Probabilité conditionnelle">
Probabilité conditionnelle

En théorie des probabilités, une probabilité conditionnelle est la probabilité d'un événement sachant qu'un autre événement a eu lieu. Par exemple, si je tire au hasard une carte d'un jeu, j'estime à une chance sur quatre la probabilité d'obtenir un cœur ; mais si j'aperçois un reflet rouge sur la table, je corrige mon estimation à une chance sur deux.
Cette seconde estimation correspond à la probabilité conditionnelle d'obtenir un cœur sachant que la carte est rouge.

Les probabilités conditionnelles font l'objet de paradoxes tels que le paradoxe des deux enfants, le paradoxe des deux enveloppes, le paradoxe des trois pièces de monnaie et le paradoxe des prisonniers.

En théorie des probabilités, la probabilité conditionnelle d'un événement "A", sachant qu'un autre événement B de probabilité non nulle s'est réalisé (ou probabilité de "A", sachant "B") est le nombre noté formula_1 défini par :

Le réel formula_1 se lit « probabilité de "A", sachant "B" ». formula_1 se note aussi parfois formula_5.

Si A et B sont indépendants alors:

D'autre part, on peut noter que le théorème de Bayes permet d'écrire ceci:
Cela se vérifie en remarquant que: formula_8.

Dans un univers d'une classe de lycée, soit A l'événement "un élève est une fille" et B "un élève pratique l'allemand".
Dans cet univers, on peut calculer un certain nombre de probabilités conditionnelles, par exemple, si on interroge au hasard une fille de la classe (A) quelle est la probabilité qu'elle pratique l'allemand (P(B|A))) ?

formula_9

formula_10

formula_11

D'où

formula_12

Mathématiquement, soient formula_13 un espace probabilisé, et "B" un événement de probabilité non nulle. À tout événement "A" de formula_14, nous associons le nombre noté formula_1 ou formula_5 défini par :

Nous pourrions vérifier que l'application formula_18 définie par formula_19 est une probabilité.

Soit formula_13 un espace probabilisé, X une variable aléatoire réelle intégrable et B un évènement. On appelle espérance conditionnelle :
formula_21

formula_22 est l'espérance de X sachant que B s'est réalisé. C'est une variable aléatoire.

Soit formula_13, et soient formula_24 et formula_25 deux variables aléatoires définies sur cet espace. Si l'on suppose que leur loi jointe peut être définie par une densité bivariable formula_26 , et si de plus un formula_27 vérifie formula_28 , alors il existe une loi absolument continue dont la densité est donnée par l'expression

formula_29

Cette fonction formula_30 est appelée : densité conditionnelle de formula_24 sachant formula_32. Intuitivement, cette expression peut être interprétée comme une formulation continue du théorème de Bayes.



</doc>
<doc id="15038" url="https://fr.wikipedia.org/wiki?curid=15038" title="Organisation mondiale du commerce">
Organisation mondiale du commerce

L'Organisation mondiale du commerce (OMC ; en , "WTO", en , "OMC") est une organisation internationale qui s'occupe des règles régissant le commerce international entre les pays. Au cœur de l'organisation se trouvent les accords de l'OMC, négociés et signés en avril 1994 à Marrakech par la majeure partie des puissances commerciales du monde et ratifiés par leurs assemblées parlementaires. L'OMC a pour but principal de favoriser l'ouverture commerciale. Pour cela, elle tâche de réduire les obstacles au libre-échange, d'aider les gouvernements à régler leurs différends commerciaux et d'assister les exportateurs, les importateurs et les producteurs de marchandises et de services dans leurs activités.

Depuis 2001, le cycle de négociation mené par l'OMC est le Cycle de Doha. Bien que l'OMC ne soit pas une agence spécialisée de l'ONU, elle entretient des liens avec cette dernière. Le siège de l'OMC est au Centre William-Rappard, à Genève. Depuis le , l'organisation est présidée par le Brésilien Roberto Azevêdo qui a été élu directeur général.

L'OMC est née le , mais le système commercial qu'elle représente a presque un demi-siècle de plus. En 1947, l'Accord général sur les tarifs douaniers et le commerce (GATT : "General Agreement on Tariffs and Trade") établissait les règles du système. L'Accord général a rapidement donné naissance à une organisation internationale officieuse, existant de fait et aussi dénommée officieusement GATT, qui a évolué au fil des ans à travers plusieurs cycles (ou "rounds") de négociation.

Un accord général débouche sur la création d'une organisation internationale. Débuté en septembre 1986 à Punta del Este (Uruguay), l'acte final du cycle d'Uruguay est adopté le 15 décembre 1993 à Genève et signé à Marrakech le 15 avril 1994. L'OMC couvre les accords passés dans le cadre du GATT depuis 1947 et les résultats des négociations commerciales multilatérales de l'Uruguay Round (outre l'accord instituant l'Organisation mondiale du commerce, l'acte final comporte vingt-huit accords).

En 1996, la première conférence ministérielle se tient à Singapour. Lors de cette première rencontre, il est décidé de créer trois nouveaux groupes de travail. Un sur le commerce et l'investissement, un sur l'interaction du commerce et de la politique de la concurrence et un sur la transparence des marchés publics. Ces sujets sont généralement désignés sous le nom de . En 1998, la ministérielle se tient à Genève. Le commerce électronique est ajouté au programme de travail de l'OMC. En 1999, la troisième conférence ministérielle, à Seattle aux États-Unis, s'est conclue sur un échec, les délégations des cent-trente-cinq pays membres se séparant sans lancer le « cycle du millénaire ». Les pays du Sud forment pour la première fois un bloc de négociation.

En 2001, la quatrième conférence ministérielle, à Doha, au Qatar, marque le début du cycle de Doha, du programme de Doha pour le développement et du lancement d'un programme de négociations sur trois ans, comprenant notamment les services. La question de l'accès des pays les plus pauvres aux médicaments s'est trouvée au centre des discussions, ce qui permet leur ralliement au principe de l'ouverture d'un nouveau cycle. En 2003, la cinquième conférence ministérielle de l'OMC, à Cancún, au Mexique, marque le second échec en quatre ans, principalement à cause de l'opposition entre grandes puissances et G22 sur le dossier agricole. Il a été marqué par une alliance entre certains pays du tiers-monde contre les projets de libéralisation des services qui étaient sur la table des négociations. Cette alliance visait à obtenir de la part des pays riches une modification de leurs politiques agricoles et a abouti, face au refus de ceux-ci, à l'échec des négociations. En 2005, la sixième conférence ministérielle de l'OMC, à Hong Kong, débouche sur un accord sur la suppression, d'ici à 2013, des subventions aux exportations agricoles.

Au second semestre 2017, dans le cadre de la préparation du Brexit, le RU et l'UE doivent présenter une réforme de leurs statuts à l'OMC.
Sont notamment en jeu les litiges en cours, comme celui opposant Airbus à Boeing au sujet de subventions.

L'OMC remplit principalement cinq fonctions : 

L'OMC s'occupe du commerce des marchandises (GATT 1947/ 1995/ dumping/ subventions/ mesures sanitaires/ etc), et des services (AGCS selon quatre modes, télécommunication/ offerts sur place/ grâce à l'investissement/ grâce au déplacement mais sans investissement), des biens agricoles (ASA) et industriels, et de la propriété intellectuelle (les aspects des droits de propriété intellectuelle qui touchent au commerce (ADPIC)).

Il existe des accords dit « plurilatéraux » dans des domaines plus spécifiques et qui ne concernent qu'un nombre limité de pays. Il s'agit : des aéronefs civils (Boeing, Airbus, Embraer, Bombardier, etc.) et les marchés publics. Les produits laitiers et la viande bovine sont deux domaines politiquement sensibles et qui n'ont pas pu encore être réglés par l'OMC.

L’OMC est avant tout un cadre de négociation, un lieu où les gouvernements membres se rendent pour essayer de résoudre les problèmes commerciaux qui existent entre eux. La première étape consiste à discuter. Ces négociations demandent des moyens importants pour pouvoir être suivies efficacement par les membres de l'organisation (juristes, experts, etc.). L'OMC fonctionne sur un mode démocratique au sens où chaque État représente une voix, quel que soit son poids politique ou économique.

Il existe plus de cent accords définissant les règles de fonctionnement de l'OMC. Le principal accord est l'Accord cadre instituant l'OMC.

Trois accords importants définissent les règles du commerce dans le domaine des marchandises, des services et de la propriété intellectuelle :

Deux autres accords définissent la procédure de règlement des différends et l'examen de la politique commerciale des gouvernements. De nombreux accords complémentaires et annexes contiennent des prescriptions plus précises pour certains secteurs ou pour certaines questions comme l'accord sur l'agriculture, l'Accord sur l'application des mesures sanitaires et phytosanitaires (SPS), l'accord sur les mesures concernant l'investissement et liées au commerce (en Anglais TRIMs) ou l'accord sur les obstacles techniques liés au commerce (en anglais TBT).

Les travaux menés actuellement par l'OMC découlent en majeure partie des négociations qui se sont tenues de 1986 à 1994, dénommées le Cycle d'Uruguay, et de négociations antérieures qui ont eu lieu dans le cadre de l'Accord général sur les tarifs douaniers et le commerce (GATT). L'OMC accueille actuellement de nouvelles négociations, dans le cadre du Programme de Doha pour le développement lancé en 2001. Lorsque les pays se sont heurtés à des obstacles au commerce et ont voulu les réduire, les négociations ont contribué à libéraliser le commerce. Mais l'OMC ne s'emploie pas seulement à libéraliser le commerce, et dans certaines circonstances, ses règles peuvent favoriser le maintien d'obstacles au commerce – par exemple pour protéger les consommateurs ou empêcher la propagation d'une maladie. Cela n'a cependant pas empêché l'organe de règlement des différends de l'OMC de pénaliser l'Union européenne pour avoir refusé d'importer du bœuf aux hormones américain.

Depuis 1993, le poste de directeur général de l'OMC a été successivement occupé par :

L'OMC s'est dotée d'un « pouvoir judiciaire », l'Organe de règlement des différends (ORD), auprès duquel les pays qui s'estiment lésés peuvent porter plainte. Une procédure permet de régler les conflits entre les États membres. Elle est avant tout fondée sur la négociation, mais l'Organe d'appel présente la particularité d'avoir un fonctionnement proche de celui d'une juridiction, statuant sur une conciliation par nature non-juridictionnelle. L'institution s'est particulièrement illustrée dans le long contentieux fiscal des subventions à l'exportation par deux affaires commerciales qui ont défrayé la chronique jurisprudentielle de l'OMC (Airbus contre Boeing et Boeing contre Airbus). 

En cas de différend entre deux États membres, la partie plaignante peut demander à entamer des consultations avec l'autre partie, dans le but de trouver un règlement amiable au conflit. Cette demande doit être notifiée à l'ORD (Organe de règlement des différends). Les autres États membres, qui témoignent d'un intérêt commercial substantiel à suivre ces consultations, peuvent obtenir l'autorisation d'y participer en qualité de tierce partie (près d'un quart des conflits sont réglés par le mécanisme des consultations).

En l'absence de solution amiable, la partie plaignante peut demander à l'ORD d'établir un ("panel"). Le groupe spécial est en général constitué de trois personnes, proposées par le secrétariat de l'OMC. Il a pour mission d'examiner, à la lumière des dispositions pertinentes des accords de l'OMC, la question portée devant l'ORD et de faire des constatations propres à aider l'ORD à formuler des recommandations. Les autres États membres qui démontrent l'existence d'un intérêt commercial substantiel peuvent se porter tierce partie et présenter des communications écrites au panel. Le groupe spécial établit lui-même le calendrier de ses travaux et choisit de faire ou non appel à des experts. Il doit rendre, en principe, son rapport dans un délai de six mois à compter de la date de formation du panel. Ce délai peut être prolongé mais ne doit pas dépasser neuf mois. Un accord à l'amiable est encore possible pendant les travaux du groupe spécial. L'ORD peut se réunir pour adopter le rapport du groupe spécial au plus tôt vingt jours et au plus tard soixante jours après sa distribution aux États membres dans les trois langues officielles de l'OMC (anglais, français et espagnol), à moins qu'un État membre, partie du différend, ne notifie à l'ORD sa volonté de faire appel ou que l'ORD décide par consensus de ne pas adopter le rapport (décision au ).

L'Organe d'appel doit statuer sur le rapport du groupe spécial dans les soixante jours de la notification de la décision de faire appel, et au plus tard dans les quatre-vingt-dix jours de cette date en cas de difficultés. L'appel est limité aux questions de droit et aux interprétations du droit données par le rapport du panel. L'ORD doit adopter le rapport de l'Organe d'appel dans les trente jours de sa distribution aux États membres. Il assure la surveillance de la mise en œuvre des décisions et recommandations qu'il a exprimées à la lumière des deux rapports susvisés. La partie concernée doit, en principe, se conformer immédiatement à ces décisions et à ces recommandations. Elle pourra néanmoins disposer d'un délai raisonnable fixé par accord amiable entre les parties ou par un arbitrage. Dans ce dernier cas, ce délai ne doit pas normalement dépasser quinze mois à compter de la date d'adoption du rapport du groupe spécial ou de l'Organe d'appel. En cas de désaccord entre les parties sur la question de savoir si la partie concernée s'est bien conformée aux recommandations de l'ORD, la question peut être portée devant un groupe spécial qui dispose alors de 90 jours pour trancher ce différend. Les parties peuvent de commun accord fixer une compensation volontaire qui vise à l'allongement du délai dans lequel la partie défaillante doit en principe retirer la mesure illicite.

Par ailleurs, dans les vingt jours suivant l'expiration du délai raisonnable visé ci-dessus, la partie plaignante, qui estime que les mesures de conformité mises en œuvre par l'autre partie sont incompatibles avec les recommandations de l'ORD, peut demander à l'ORD de suspendre les concessions et autres droits dont bénéficie l'autre partie dans le cadre des accords de l'OMC. Si l'État membre concerné conteste le niveau de suspension de concession autorisé par l'ORD, il peut demander un arbitrage pour vérifier l'adéquation du niveau de suspension des concessions au niveau d'annulation ou de réduction des avantages. Les sociétés concurrentes d'aéronefs Boeing et Airbus ne se sont pas privées du recours à un tel arbitrage en amont de leur contentieux commercial et fiscal.

La loi américaine sur les "foreign sales corporations" est une loi qui permet aux entreprises des États-Unis d'utiliser des paradis fiscaux lorsque celles-ci réalisent des ventes à l'étranger pour diminuer leur imposition aux États-Unis. Elle est prévue à la section 26 USC § 367 de l"'Internal Revenue Code" (IRC). Le code fiscal américain autorise, en pratique, les sociétés à transférer une partie ou l'ensemble de leurs actifs à des sociétés étrangères en franchise d'impôt. L'impôt dont il est ici question concerne exclusivement les plus-values réalisées sur la cession et fait l'objet d'un report dans le temps. Cette nouvelle législation succède à la loi fiscale américaine de même nature sur les sociétés domestiques de vente internationale ("Domestic International Sales Corporation") qui avait été reconnue, en 1976, incompatibles avec les règles du GATT. Après une plainte de la part de l'Union européenne, en 1998, auprès de l'OMC, l'ORD a estimé qu'il s'agissait de subventions déguisées à l'exportation et a condamné les États-Unis à annuler cette législation avant le . Ce jugement, confirmé à plusieurs reprises, n'ayant pas été respecté par les États-Unis, l'OMC a autorisé, le , l'Union européenne à appliquer des sanctions vis-à-vis de ceux-ci à hauteur d'un montant de de dollars. Ces sanctions prennent la forme d'une augmentation progressive des taxes sur agricoles, textiles et industriels, à partir du . La surtaxe est au départ de 5 % et progresse automatiquement de 1 % par mois jusqu'à un plafond provisoire de 20 % le .

L’OMC regroupe 164 pays membres et des observateurs. Les observateurs peuvent être des États en cours d'adhésion ou des organisations internationales comme le FMI et la Banque mondiale.

L'OMC compte 164 membres. Ceux-ci peuvent être des États, des territoires douaniers pleinement autonomes (3 membres le sont) et l'Union européenne. Les membres sont les suivants (entre parenthèses, la date d'entrée dans l'OMC) :
Territoires couverts par l'adhésion d'un pays

Les pays suivants sont candidats et possèdent le statut d'observateur :
 : Le Saint-Siège est observateur sans être candidat.

Les pays suivants ne sont ni candidats, ni observateurs :

Les entités suivantes sont rattachées à des pays et exclues de l'adhésion :


Les entités suivantes sont contestées et n'ont pas soumis leur candidature :

Depuis la fin des années 1990, l'OMC a été l'objet de critiques de la part des mouvements alter-mondialistes qui lui reprochent de promouvoir la mondialisation de l'économie et la libéralisation du commerce. Les traités signés sont accusés de plus favoriser les entrepreneurs des pays riches que les salariés ou les pays pauvres. Comme l’a reconnu lui-même Pascal Lamy, directeur général de l’OMC depuis 2005, au sujet de l'AGCS (accord général sur la commercialisation des services) que promeut l'OMC : « l’AGCS est avant tout un instrument au bénéfice des milieux d’affaires ». C’est ce qu’avait déjà observé dès 1985 la CNUCED (Conférence des Nations unies sur le commerce et le développement, organe de l’ONU) affirmant dans un rapport que .

Les représentants des grandes puissances, des firmes transnationales, de la finance mondiale, imposent à l’OMC leurs conceptions néolibérales. Il s’agit d’assimiler à des marchandises des secteurs comme les produits agricoles, l’eau, l’éducation, la santé, les services sociaux et notamment les services publics. L’OMC impose inexorablement aux États de modifier leurs lois, règlements, procédures administratives pour les mettre en conformité avec les règles qu’elle édicte. Mais ces règles édictées par l’OMC, loin de résulter d’un processus démocratique, sont prises dans l’opacité par une minorité de (représentants des États les plus riches, des grandes entreprises, des grandes banques), alors que la majorité des États et des populations du monde ne sont même pas consultés ni même réellement informés.

L'OMC semble donc être devenue peu à peu, à l'insu de la majorité des populations, l'organisation internationale la plus puissante du monde. Son pouvoir réside en particulier dans l'Organe de Règlement des Différends (ORD). En effet, par cet instrument, l'OMC est la seule organisation internationale offrant à ses membres la capacité de sanctionner d'autres États qui ne respectent pas les engagements qu'ils ont pris. L'État qui obtient gain de cause peut pratiquer à l'égard de celui qui perd un sous forme de sanctions commerciales frappant des secteurs variés. En outre, les experts appelés à juger en première instance ne sont pas des magistrats, et ils sont désignés au cas par cas, à l'encontre du principe d'inamovibilité des magistrats du siège. De plus, les débats de l'ORD se déroulent à huis clos.

Enfin l'ORD est un mécanisme qui serait pour certains auteurs réservé de fait aux pays industrialisés : appliquer des mesures de rétorsion est inenvisageable pour un État du Sud, dépendant d'un État du Nord. La perte de souveraineté des États par rapport à l'OMC apparaît très préoccupante.

Actuellement, les règles de l'OMC s'imposent "de facto" sur celles de toutes les autres organisations internationales. Ainsi, dans le domaine du travail et des droits sociaux, l'Organisation internationale du travail (OIT), organisme des Nations unies, ne dispose d'aucun moyen pour faire respecter ses recommandations et décisions : les États et les firmes transnationales qui ne respectent pas les principes fixés par l'OIT ne se voient imposer aucune sanction. L'OMC elle-même n'est pas tenue de respecter les principes de base de l'OIT, car le fonctionnement de l'OMC est indépendant de l'OIT. En cas de conflit entre un droit fondamental des travailleurs reconnu par l'OIT et un intérêt commercial garanti par l'OMC, c'est "de facto" le principe garanti par un accord conclu dans le cadre de l'OMC qui l'emporte.

Cette organisation internationale est une de celles qui ont mis en place le plus d'accords pour supprimer les droits de douane entre les pays, mais son action économique se limite à la lutte contre le protectionnisme douanier, l'OMC ne pouvant rien en revanche contre le protectionnisme monétaire et les manipulations de change de certains pays.

Certains considèrent que l'adhésion à l'OMC peut s'assimiler à une récompense pour économiques. Le Vietnam a ainsi rejoint l'organisation le 11 janvier 2007, tout comme la Russie, Samoa, Vanuatu et le Monténégro les 16 et 17 décembre 2011.
Beaucoup critiquent aussi la différence de traitement entre sa capacité à faire appliquer les réformes en matière de commerce (notamment suppression des droits de douanes) en comparaison du peu d'intérêt qu'elle manifeste à faire respecter les droits fondamentaux sociaux et éthiques (pas de règle sur les salaires, sur l'environnement, sur les droits syndicaux). Certains contestent le caractère démocratique de l'OMC en avançant que son mode de fonctionnement favorise les pays riches capables de mener de front des dizaines de dossiers simultanés. Les décisions se prenant en suivant le principe du , les petits pays qui ne disposent que d'un seul représentant pour gérer tous les dossiers seraient donc la plupart du temps consentants malgré eux.

L'OMC ferait du commerce une valeur suprême qui serait la source d'un conflit de droits avec des normes internationales en matière de droits de l'homme, de protection sociale et environnementale, de protection de la santé, de protection sanitaire, bien que les accords du GATT précisent explicitement des exceptions à ces fins. Les altermondialistes se fondent sur ces aspects pour accuser l'OMC de promouvoir le néolibéralisme et une mondialisation discriminatoire. Ils mettent en débat la nécessité de remettre le commerce à ce qu'ils considèrent sa juste place en obligeant l'OMC à mieux coordonner ses décisions à d'autres aspects du droit international via son rattachement à l'ONU. Au contraire, certains économistes, comme Joseph E. Stiglitz, voient dans l'OMC une organisation développant les principes du mercantilisme commercial et dénaturant profondément ceux du libre-échange.
L'OMC est critiquée par les libéraux qui lui reprochent d'organiser non pas le libre-échange, mais la régulation des échanges, et d'être ainsi le reflet des points de vue mercantilistes des hommes politiques.

Le système de règlement des différends de l'Organisation mondiale du commerce (OMC) est devenu le pilier du système commercial multilatéral et un outil privilégié pour mettre en place des « règles » de libéralisation des échanges. Dans le cas des relations commerciales entre États régies par l'OMC, ce sont les intérêts de grands opérateurs privés qui sont directement en cause. De grandes entreprises nationales qui s'estiment lésées par la législation d'un autre État peuvent ainsi entreprendre des pressions pour que des actions soient intentées. Le système en devient donc pervers et ressuscite une « loi du plus fort » en favorisant les lobbies les plus puissants, seuls capables d'initier cette protection. Selon Virgile Pace :






</doc>
<doc id="15042" url="https://fr.wikipedia.org/wiki?curid=15042" title="Friedrich Nietzsche">
Friedrich Nietzsche

Friedrich Wilhelm Nietzsche (prononcé , souvent francisé en ) est un philologue, philosophe, poète, pianiste et compositeur allemand né le à Röcken, en Prusse, et mort le à Weimar, en Allemagne.

L'œuvre de Nietzsche est essentiellement une généalogie critique de la culture occidentale moderne et de l'ensemble de ses valeurs morales (issues de l'interprétation chrétienne du monde), politiques (la démocratie, l'égalitarisme), philosophiques (le platonisme, mais surtout le socratisme, et toutes les formes de dualisme métaphysique) et religieuses (le christianisme et le bouddhisme). Cette critique procède d'un projet de dévaluer ces valeurs et d'en instituer de nouvelles dépassant le ressentiment et la volonté de néant qui ont dominé l'histoire de l'Europe sous l'influence du christianisme ; ceci notamment par l'affirmation d'un Éternel Retour du même et par le dépassement de l'humanité et l'avènement du surhomme. L'exposé de ses idées prend dans l'ensemble une forme aphoristique ou poétique.

Peu reconnu de son vivant, son influence a été et demeure importante sur la philosophie contemporaine de tendance continentale, notamment l'existentialisme et la philosophie postmoderne ; mais Nietzsche a également suscité ces dernières années l'intérêt de philosophes analytiques, ou de langue anglaise, qui en soutiennent une lecture naturaliste remettant en cause une appropriation par la philosophie continentale jugée problématique.

Né en 1844, Nietzsche devient professeur de philologie à l'université de Bâle dès l'âge de 24 ans. Il obtient un congé en 1879 pour raison de santé. Les dix années suivantes, il publie à un rythme rapide ses œuvres majeures. En 1889, il sombre progressivement dans la démence et passe les dix dernières années de sa vie dans un état mental quasi végétatif. Après sa mort, l'interprétation de son œuvre est défigurée par l'image de la folie et par la propagande nazie.

La pensée de Nietzsche présente deux aspects majeurs : c’est une enquête naturaliste sur l’ensemble des valeurs humaines (morales, intellectuelles, religieuses, esthétiques) que Nietzsche explique en termes d'instincts, d'affects et de pulsions (en allemand : "") ; c'est également une critique de ces mêmes valeurs et une tentative pour les réévaluer.

Dans ses recherches sur la nature des phénomènes humains, qui occupent ses œuvres de maturité à partir de "Humain, trop humain" (1878), Nietzsche adopte une forme de naturalisme qualifiée de "méthodologique" par certains commentateurs : par naturalisme, on entend l’idée que l’enquête philosophique doit se développer en continuité avec les sciences naturelles. Cette interprétation s’appuie sur l'utilisation que Nietzsche fait d'auteurs tel que Wilhelm Roux, et sur des passages tel que :
Mais ce qui caractérise particulièrement ce naturalisme, c'est le rejet de toutes les formes de « surnaturalisme » (moral ou religieux) qui placent l’esprit au-dessus de la nature et qui font de lui un principe explicatif des phénomènes humains par une causalité spirituelle (comme l’âme ou la volonté qui serait au principe de nos actions). Or, pour Nietzsche, l’esprit n’explique rien, et ce n’est qu’à partir des sciences empiriques que la philosophie peut spéculer sur la nature humaine et fournir des explications de tout ce qui est humain :
Partageant avec le matérialisme allemand qui lui est contemporain l’idée que l’homme est un produit de la nature, Nietzsche s’efforce de rendre compte du phénomène humain en termes psycho-physiologiques, ce qui se traduit chez lui par une théorie des types. Brian Leiter a ainsi formulé et résumé cette théorie :
Par exemple, l’un des traits typiques les plus célèbres est la Volonté de puissance qui joue un rôle explicatif fondamental, puisque, selon Nietzsche, 
Selon cette méthodologie, toute personne adopte alors nécessairement les valeurs qui forment la philosophie du type de personne qu'elle est. Les traits psychologiques qui caractérisent ces personnes sont donc comme des faits naturels, et ces faits expliquent les idées et les valeurs qui apparaissent. Les explications des idées et des valeurs humaines se présenteront alors sous la forme suivante :
Ce naturalisme ne doit cependant pas être réduit à une conception matérialiste, cette dernière étant explicitement rejetée par Nietzsche. Les faits psychologiques, soutient Nietzsche, peuvent être "expliqués" en termes physiologiques ; mais cela ne conduit pas nécessairement à soutenir que les faits psychologiques "ne sont rien d’autre" que des faits physiologiques. Dans l'expression « naturalisme méthodologique », l’adjectif « méthodologique » signifie donc que Nietzsche n’adopte pas la forme substantielle de naturalisme qu'est le matérialisme, mais qu'il explique néanmoins les phénomènes humains d'après les sciences de la nature. Ce rejet du substantialisme laisse ouverte la possibilité de spéculer sur la nature humaine en ne la fixant pas définitivement dans les termes des sciences de la nature, ce qui laisse également ouverte la possibilité d'une réévaluation des valeurs, l’autre aspect majeur de la pensée nietzschéenne :
Ce que des commentateurs récents nomment le naturalisme de Nietzsche est donc son rejet de toutes les formes de transcendances qui ne peuvent que falsifier la compréhension historique et psychologique de l'homme ; Nietzsche les remplace par le projet, qu'il nomme "généalogie", d'une explication de l'homme comme être entièrement corporel et animal dirigé par des pulsions et des affects qui expliquent ses croyances. Nietzsche est ainsi en ce sens un philosophe de la nature humaine et a pu de ce fait être rapproché de David Hume et de Freud.

Le second aspect principal de la pensée de Nietzsche est la réévaluation des valeurs, au premier rang desquelles les valeurs morales et métaphysiques (le bien et le vrai, par exemple), qu'il soumet à la méthode « généalogique ». Ce projet se manifeste, de "La Naissance de la tragédie" à ses dernières œuvres, par la recherche des conditions et des moyens de l'ennoblissement et de l'élévation de l'homme. Aussi nombre de commentateurs ont-ils souligné que le thème fondamental et constant de la pensée de Nietzsche, à travers les nombreuses variations de ses écrits, est le problème de la culture — ou « élevage », problème qui comprend la question de la hiérarchie et de la détermination des valeurs propres à favoriser cette élévation.

L’enquête naturaliste sur l’origine des valeurs est utilisée dans ce projet afin de montrer que les valeurs qui règnent en Occident depuis la naissance du christianisme, et dont on trouve selon Nietzsche les prémisses chez Platon influencé par Socrate, sont néfastes et ont été des instruments de domination qui ont rendu l’humanité malade. Le projet nietzschéen de réévaluation embrasse donc une partie critique, omniprésente dans son œuvre, qui doit conduire à la destruction des valeurs de l'idéalisme platonicien et chrétien qui font obstacle à l'épanouissement créateur de l'homme et qui, selon Nietzsche, menacent de conduire l'humanité au dernier homme.

Nietzsche pense que tous les idéaux, qu'ils soient religieux, philosophiques ou politiques, ont la même finalité, celle d'inventer un au-delà meilleur que l'ici-bas et d'imaginer des valeurs « transcendantes ». Nier le vrai réel au nom de fausses réalités au lieu de l'assumer et de le vivre tel qu'il est. C'est cela que Nietzsche nomme le « nihilisme ».

Au cours de sa vie, Nietzsche a exprimé cette volonté d'une élévation de l'homme de diverses manières. Elle se rencontre soit sous la forme d’une métaphysique d'artiste, soit d’une étude historique des sentiments et des représentations moraux humains, soit enfin sous la forme d’une affirmation de l'existence tragique, au travers des notions de « Volonté de puissance », « d'Éternel Retour » et de « Surhomme ». Ces thèmes, sans s'exclure, se succèdent, parfois en s'approfondissant et s'entremêlant les uns aux autres, comme lorsque la philosophie de l'affirmation se présente sous la forme d'une exaltation de la puissance créatrice humaine.

L'œuvre de Nietzsche a parfois été divisée en trois périodes, en mettant en avant la prééminence de l'un ou l'autre de ces thèmes. On distingue ainsi une période comprenant "La Naissance de la Tragédie" et les "Considérations Inactuelles", période pendant laquelle Nietzsche s'engage, sous l'influence de Schopenhauer et de Wagner, en faveur d'une renaissance culturelle de la civilisation allemande. La deuxième période est la période positiviste (de "Humain, trop humain" au "Gai Savoir") ; Nietzsche rompt avec le wagnérisme, et développe une pensée historique et psychologique influencée par les moralistes français. La troisième période va de "Ainsi parlait Zarathoustra" à ses derniers textes ; c'est la période de maturité teintée d'un mysticisme symbolisé par l'Éternel Retour.

Cette périodisation a été contestée notamment par Mazzino Montinari et Paolo D'Iorio d'après une analyse basée sur les cahiers manuscrits de Nietzsche et elle est progressivement abandonnée. Cette discussion souligne une difficulté pour l'interprétation des textes de Nietzsche : le devenir de la pensée de Nietzsche demeure un fait difficile à appréhender et à restituer pour tous les commentateurs, difficulté qui fut accrue par les premières éditions des fragments posthumes.

Nietzsche a laissé de nombreux cahiers de notes, représentant quelques milliers de pages qui ont maintenant toutes été publiées et traduites en français. Le problème que posent ces textes est de savoir quelle place leur donner dans l’interprétation de sa pensée. Certains commentateurs en ont fait une expression de sa philosophie, au même titre que les œuvres publiées. Dans cette idée, des notions peu présentes dans ces dernières peuvent se retrouver mises en avant, comme ces notions jugées fondamentales que sont la Volonté de puissance, l’Éternel Retour et le Surhomme. De nombreux commentateurs ont ainsi écrit des études reposant très largement sur ces textes posthumes (par exemple Heidegger, Pierre Montebello, Barbara Stiegler).

D’autres, en revanche, comme , tenant compte du fait que les fragments de Nietzsche ne sont souvent que des ébauches de ses œuvres publiées, et qu’il a en outre manifesté le souhait de voir ses carnets détruits après sa mort, estiment que ces textes ne peuvent pas être légitimement utilisés pour déterminer exactement la pensée de Nietzsche. Ces textes qu'il a laissés de côté seraient obsolètes, et ils ne pourraient tout au plus qu’éclairer la genèse des livres de Nietzsche qui, seuls, expriment la pensée de ce dernier.

Ces difficultés de lecture des œuvres de Nietzsche sont encore accentuées par la forme stylistique qu’il a choisie à partir de "Humain, trop humain". Il décide en effet d'exposer sa pensée sous la forme d'aphorismes qui se suivent plus ou moins thématiquement, ou qu'il regroupe par chapitre. Nietzsche a donné plusieurs explications à ce choix. Ces explications touchent autant le travail de l'exposition de la pensée que celui de la réception de cette pensée par un lecteur.

Dans le premier cas, il s'agit d'éviter d'écrire des traités systématiques, alors que toute pensée est, pour Nietzsche, toujours en devenir. La forme rigide du traité détruit la vie de la pensée, tandis que l'aphorisme conserve quelque chose de la spontanéité philosophique. Dans le second cas, il s'agit d'interdire l'accès aux textes à un lecteur pressé qui ne voudrait pas se donner la peine de repenser ce qu'il lit. Ainsi explique-t-il dans "Ainsi parlait Zarathoustra" au discours "Lire et écrire" : « Celui qui écrit en aphorismes et avec du sang, celui-là ne veut pas être lu, mais appris par cœur ». Nietzsche décrit ainsi ses textes comme un labyrinthe dont on doit trouver le fil qui mènera à travers tous les aphorismes. On peut toutefois remarquer que Nietzsche a au contraire écrit ses dernières œuvres avec le souci d'être compris.

À la suite de ces difficultés de lecture des œuvres de Nietzsche, plusieurs méthodes d'exposition de sa pensée sont utilisées. Certains, comme Eugen Fink, retracent le développement intellectuel de Nietzsche, en soulignant la relative autonomie de chaque période ; d'autres, comme Heidegger, privilégient l'étude des notions de la dernière période de Nietzsche, notions considérées comme l'expression de la maturité de son activité philosophique. L'étude du devenir de la pensée de Nietzsche étant loin d'être achevée, cet article exposera les thèmes qui ont été constamment considérés comme les plus importants dans l'ensemble de l'histoire de la réception de ses œuvres, tout en évoquant la genèse de certains d'entre eux.

Le concept de Volonté de puissance est, pour de nombreux commentateurs (Heidegger, M. Haar par exemple), l'un des concepts centraux de la pensée de Nietzsche, dans la mesure où il est pour lui un instrument de description du monde, d'interprétation de phénomènes humains comme la morale et l'art (interprétation connue sous le nom de généalogie), et d'une réévaluation de l'existence visant un état futur de l'humanité (le surhomme). C'est pourquoi il est souvent utilisé pour exposer l'ensemble de sa philosophie.

Par la notion de Volonté de puissance, Nietzsche entend proposer une interprétation de la réalité dans son ensemble.

"Volonté de puissance" est la traduction devenue usuelle de l'expression allemande "Wille zur Macht". Cette expression forgée par Nietzsche signifie littéralement « volonté "vers" la puissance », ce que met en évidence l'utilisation du datif allemand pour exprimer une tension interne dans l'idée même de volonté. En effet, il ne s'agit pas de vouloir la puissance comme si, dans une conception psychologisante, la puissance était un objet posé à l'extérieur de la volonté. Nietzsche écarte ce sens traditionnel de la notion de volonté, et lui substitue l'idée qu'il y a quelque chose dans la volonté qui affirme sa puissance. Dans cette idée, la volonté de puissance désigne un impératif interne d'accroissement de puissance, une loi intime de la volonté exprimée par l'expression « être plus » : cet impératif pose alors une alternative pour la Volonté de puissance, devenir plus ou dépérir.

Cette conception de la volonté et de la puissance conduit à exclure le recours à des notions comme l'« unité » et l'« identité » pour décrire ce qui existe et en déterminer l’essence : si tout ce qui est Volonté de puissance doit devenir plus, il n'est en effet pas possible pour un être de demeurer dans ses propres limites. La notion de Volonté de puissance ne désigne donc ni ne constitue l’unité ou l’identité d’une chose. Au contraire, pour toute réalité, être « volonté de puissance », c'est ne jamais pouvoir être identique à soi et être toujours porté au-delà de « soi ».

Ce devenir plus, cette manière de devoir toujours aller au-delà de soi, n'est cependant pas arbitraire, mais se produit selon une orientation, que Nietzsche nomme structure, et qui est donc une structure de croissance qui définit et fait comprendre comment une réalité devient ; c'est cette structure qui est sa réalité agissante, individuelle, qui est sa volonté de puissance :

Ce mouvement se conçoit en outre pour Nietzsche comme une exigence d'assimilation, de victoires contre des résistances : cette idée introduit l'idée de « force ». La volonté de puissance est ainsi constituée de forces dont elle est la structure. La Volonté de puissance s'accroît ainsi par l'adversité des forces dont elle est constituée, ou décroît en cherchant cependant toujours d'autres moyens de s'affirmer.

Cette idée de structure d'une Volonté de puissance, qui en fait une ontologie de la relation, possède également une dimension pathologique associée au sentiment de puissance que Nietzsche avait commencé à thématiser dès "Aurore". 

Cette dimension affective est présente en tout vivant, mais Nietzsche l’étend également à l’inorganique, conçu comme une forme plus rudimentaire de Volonté de puissance. Cette affectivité introduit dans l’idée de volonté de puissance (organique ou inorganique) une dimension affective fondamentale (désignée par le terme de "pathos"), qui ne relève pas de l'expression d'un jeu de forces structurées, mais d’une disposition inhérente à toute Volonté de puissance à se déployer d'une certaine manière : 
Ainsi se trouvent liées en une même notion les idées d'être plus (extériorisation ou manifestation de la volonté de puissance), de structure (relations entre des forces) et d'affectivité.

Pour Luc Ferry, la volonté de puissance de Nietzsche n'est pas le goût du pouvoir ou le désir d'occuper une place importante mais le désir profond d'une intensité maximum de vie. Pour cela, cette volonté cherche à éviter les déchirements internes qui, tel le sentiment de culpabilité, diminuent notre puissance psychique et physiologique.

Devenir plus, structure et pathos sont les principales qualités que Nietzsche attribue à une Volonté de puissance. Ces qualités permettent de décrire ce qui est. La Volonté de puissance décrit donc de cette manière toute la réalité. Elle n'est pourtant pas un principe ; structure et être plus de ce qui devient, elle n'en est pas en effet l'origine radicale. En tant que description du monde, elle reste cependant un concept métaphysique, puisqu'elle qualifie l'étant en sa totalité (selon Heidegger et Müller-Lauter), ce que Nietzsche formule ainsi :

Tout étant est donc pour Nietzsche Volonté de puissance, et il n'y a d'être qu'en tant que Volonté de puissance. Dans cette perspective, le monde est un ensemble de volontés de puissance, une multitude. Cette description générale du devenir pose cependant une difficulté jugée fondamentale pour la compréhension de la volonté de puissance : la volonté de puissance est-elle le devenir ou son essence ? La difficulté soulevée par cette question est que, dans la mesure où Nietzsche paraît décrire une structure interne, la volonté de puissance semble devoir être comprise de manière essentialiste ; or, un tel essentialisme reconduirait la division entre un monde phénoménal et un arrière-monde à laquelle Nietzsche s'oppose explicitement.

Mais une telle compréhension exclut toute recherche d'un inconditionné derrière le monde, et de cause derrière les êtres (« fondement », « substance ») : car c'est en tant que nous interprétons que nous concevons le monde comme Volonté de puissance. L'énoncé sur l'essence doit être rapporté à une forme de perspectivisme pour éviter de faire de la Volonté de puissance une substance ou un être. Ceci suppose que d'autres interprétations sont possibles. Mais, tout en refusant un dogmatisme de l'être, Nietzsche refuse également le relativisme qui pourrait découler de sa thèse du perspectivisme de la Volonté de puissance : celle-ci est en effet également un critère de la valeur, de la hiérarchie même des valeurs

Voir aussi : Vocabulaire nietzschéen.

Pour Nietzsche, la volonté de puissance possède donc un double aspect : elle est un "pathos" fondamental et une structure.

Aussi une volonté de puissance peut-elle s'analyser comme une relation interne d'un conflit, comme structure intime d'un devenir, et non seulement comme le déploiement d'une puissance : "Le nom précis pour cette réalité serait la volonté de puissance ainsi désigné d'après sa structure interne et non à partir de sa nature protéiforme, insaisissable, fluide." La volonté de puissance est ainsi la relation interne qui structure un jeu de forces (une force ne pouvant être conçue en dehors d'une relation). De ce fait, elle n'est ni un être, ni un devenir, mais ce que Nietzsche nomme un "pathos" fondamental, pathos qui n'est jamais fixe (ce n'est pas une essence), et qui par ce caractère fluide peut être défini par une direction de la puissance, soit dans le sens de la croissance soit dans le sens de la décroissance. Ce "pathos", dans le monde organique, s'exprime par une hiérarchie d'instincts, de pulsions et d'affects, qui forment une perspective interprétative d'où se déploie la puissance et qui se traduit par exemple par des pensées et des jugements de valeur correspondants.

Pensée par Nietzsche comme la qualité fondamentale d'un devenir, la Volonté de puissance permet d'en saisir la structure (ou "type"), et, partant, d'en décrire la perspective. En ce sens, la Volonté de puissance n'est pas un concept métaphysique mais un instrument interprétatif (selon Jean Granier, contre l'interprétation de Heidegger). Dès lors, pour Nietzsche, il s'agit de déterminer ce qui est interprété, qui interprète et comment.

Nietzsche prend pour point de départ de son interprétation le monde qu'il considère comme nous étant donné et le mieux connu, à savoir le corps. Il prend ainsi, jusqu'à un certain point, le contre-pied de Descartes, pour qui notre esprit (notre réalité pensante) nous est le mieux connu. Toutefois, l'idée de Nietzsche n'est pas totalement opposée à la pensée cartésienne, puisque selon lui nous ne connaissons rien d'autre que le monde de nos sentiments et de nos représentations, ce qui peut se comparer à l'intuition de notre subjectivité chez Descartes. Ainsi le corps n'est-il pas pour Nietzsche en premier lieu le corps objet de la connaissance scientifique, mais le corps vécu : notre conception de l'être est une abstraction de notre rythme physiologique.

Toute connaissance, comme Kant l'avait déjà établi avant Nietzsche, doit prendre pour point de départ la sensibilité. Mais, au contraire de Kant, Nietzsche tient, comme Arthur Schopenhauer, que les formes de notre appréhension de l'existence relèvent en premier lieu de notre organisation physiologique (et de ses fonctions : nutrition, reproduction), tandis que les fonctions jugées traditionnellement plus élevées (la pensée) n'en sont que des formes dérivées.

Aussi, pour Nietzsche, nous ne pouvons rien connaître autrement que par analogie avec ce qui nous est donné, i.e. que toute connaissance est une reconnaissance, une classification, qui retrouve dans les choses ce que nous y avons mis, et qui reflète notre vie la plus intime (nos pulsions, la manière dont nous sommes affectés par les choses et comment, de là, nous les jugeons). Le monde dans son ensemble, lorsque nous tentons une synthèse de nos connaissances pour le caractériser, n'est jamais que le monde de notre perspective, qui est une perspective vivante, affective. C'est pourquoi Nietzsche peut dire du monde qu'il est Volonté de puissance, dès lors qu'il a justifié que l'homme, en tant qu'organisme, est Volonté de puissance. Pour Nietzsche, nous ne pouvons faire autrement que de projeter cette conception de l'être qui nous appartient du fait que nous vivons, et cela entraîne également pour conséquence que la connaissance est interprétation, puisqu'une connaissance objective signifierait concevoir une connaissance sans un sujet vivant. En conséquence, l'être n'est pas d'abord l'objet d'une quête de vérité, l'être est, pour l'homme, de la manière la plus intime et immédiate, vie ou existence.

À partir de ce perspectivisme, Nietzsche estime que toute science (en tant que schématisation quantitative) est dérivée nécessairement de notre rapport qualitatif au monde, elle en est une simplification, et répond à des besoins vitaux :
Dans un premier temps, à l'époque des "Considérations inactuelles", Nietzsche avait déduit de ce point de départ que nous ne pouvons comprendre la matière autrement que comme douée de qualités spirituelles, essentiellement la mémoire et la sensibilité, ce qui signifie que nous anthropomorphisons spontanément la nature. Il avait ainsi tenté de dépasser d'un seul coup le matérialisme et le spiritualisme qui opposent tous deux la matière et la conscience d'une manière qui demeure inexpliquée. Or, Nietzsche supprimait ici le problème, en posant « l'esprit » comme matière. Avec le développement de la notion de Volonté de puissance, Nietzsche ne rompt pas avec cette première thèse de sa jeunesse, puisque les qualités attribuées à cette puissance sont généralisables à l'ensemble de ce qui existe ; de ce fait, Nietzsche suppose que l'inorganique pourrait posséder, comme toute vie, sensibilité et conscience, du moins dans un état plus primitif. Cette thèse peut faire penser à la conception antique (aristotélicienne et stoïcienne) de la nature, qui fait naître un être plus complexe d'un état antérieur (par exemple, l'âme-"psychè" naît de la "physis" en en conservant les qualités).

Cette méthode interprétative implique une réflexion de fond à propos des concepts traditionnels de réalité et d'apparence. En effet, puisque Nietzsche s'en tient à un strict sensualisme (qui nécessite toutefois une interprétation), la réalité devient l'apparence, l'apparence est la réalité :
Mais de ce fait, les concepts métaphysiques de réalité et d'apparence, et leur opposition, se trouvent abolis :
En quoi consiste alors la réalité ? Pour Nietzsche : 
Autrement dit, la réalité qui nous est « donnée » est déjà un résultat qui n'apparaît que par une perspective, structure de la volonté de puissance que nous sommes. La pensée de Nietzsche est donc une pensée de la réalité comme interprétation, reposant sur une thèse sensualiste, tout ceci supposant que toute interprétation n'existe qu'en tant que perspective. À partir de cette thèse perspectiviste, la question qui se pose à Nietzsche (comme elle s'était posée à Protagoras, cf. le dialogue de Platon) est de savoir si toutes les perspectives (ou interprétations) se valent. La généalogie vient répondre à cette question.

Si la Volonté de puissance est appliquée par Nietzsche à l'ensemble de la réalité, elle n'est pas utilisée de manière univoque. Müller-Lauter, qui a étudié l'ensemble des textes qui se rapportent à cette notion, a proposé de regrouper l'ensemble de ces usages d'après l'article qui précède l'expression (« une », « la », « les »). On peut distinguer, en suivant ce commentateur, un usage général et un usage particulier.

Dans un usage général, la « Volonté de puissance » est une expression qui désigne la qualité générale de tout devenir. Elle décrit une manière d'être qui se rencontre en tout étant.

Dans un usage particulier, une volonté de puissance, c'est tel devenir, un être (tel homme par exemple).

La Volonté de puissance est un instrument d'interprétation de ce qui est, mais elle doit permettre également de déterminer une échelle de valeurs. Elle est donc aussi le point de départ du projet de Nietzsche de réévaluer les valeurs traditionnelles de la métaphysique par l'adoption d'une perspective nouvelle sur les valeurs humaines produites jusqu'ici. Ceci doit, d'une part, entraîner l'abolition des valeurs idéalistes platonico-chrétiennes, et, d'autre part, entraîner un mouvement antagoniste au développement de l'histoire sous l'influence de Platon, mouvement qui conduirait alors à une réévaluation de la vie.

L'aspect polémique de la Volonté de puissance peut en particulier être spécifiée par l'idée de naturalisation de l'homme et des valeurs morales, c'est-à-dire par l'interprétation du vivant homme comme volonté de puissance porteuse de certaines valeurs opposées aux anciennes valeurs qui supposent que l'homme possède une dimension métaphysique.

Par la volonté de puissance, Nietzsche s'oppose à la tradition philosophique depuis Platon, tradition dans laquelle on trouve deux manières de saisir l'essence du vivant : le "Conatus", chez Spinoza (le fait de « persévérer dans l'être ») et le "vouloir-vivre" chez Schopenhauer (Nietzsche fut conquis par la philosophie de Schopenhauer avant de la critiquer). Mais chez Nietzsche, vivre n'est en aucune façon une conservation (« Les physiologistes devraient réfléchir avant de poser que, chez tout être organique, l’instinct de conservation constitue l’instinct cardinal. Un être vivant veut avant tout déployer sa force. La vie même est volonté de puissance, et l’instinct de conservation n’en est qu’une conséquence indirecte et des plus fréquentes » (Nietzsche, Par delà bien et mal, 13)), au contraire, pour lui, se conserver c'est s'affaiblir dans le nihilisme, seul le dépassement de soi ("Selbst-Überwindung") de la puissance par la volonté et de la volonté par la puissance est essentiel à la vie et donne son sens à la volonté de puissance.

Nietzsche s'oppose également, par cette notion de Volonté de puissance, aux philosophies faisant du bonheur le Bien Suprême, et de sa recherche le but de toute vie, et notamment aux philosophies eudémonistes antiques comme l'épicurisme - qui ne parvenaient pas à expliquer la persistance du mal - en tête. Cette position se retrouve notamment dans cette déclaration :
Finalement, Nietzsche se propose de modifier par la Volonté de puissance les fondements de toutes les philosophies passées, dont le caractère dogmatique est contraire à son perspectivisme, et de renouveler la question des valeurs que nous attribuons à certaines notions (comme la vérité, le bien) et à notre existence, en posant la question de savoir ce qui fait la valeur propre d'une perspective : quelle est par exemple la valeur de la volonté de vérité?

La question qui découle pour Nietzsche de cette mise en question est de savoir si l'on peut établir, à la suite de cette critique, une nouvelle hiérarchie des interprétations et sur quelles bases. Nietzsche n'est ainsi pas tant un prophète ou un visionnaire, dont une notion comme la Volonté de puissance serait le message, mais il se comprend lui-même comme le précurseur de philosophes plus libres, tant à l'égard des valeurs morales que des valeurs métaphysiques.
Au-delà de ses aspects critiques, la Volonté de puissance, en tant qu'interprétation de la réalité, a donc des aspects positifs et créateurs, qui se traduiront dans la pensée de l'éternel retour et dans l'aspiration à un état futur de l'homme, le Surhomme.

La notion de Volonté de puissance, qui est la qualité générale de tout devenir, doit permettre une interprétation de toutes les réalités en tant que telles. Elle synthétise un ensemble de règles méthodologiques qui sont le résultat de réflexions qui s'étendent des années 1860 à la fin de 1888. Mais cette notion ne prétend pas à la systématisation (Nietzsche a d'ailleurs abandonné pour cette raison l'idée d'un exposé de sa philosophie de la Volonté de puissance ; cf. Volonté de puissance), car elle a beaucoup évolué, mais on peut néanmoins dégager des lignes directrices permettant d'exposer la pensée de Nietzsche dans son ensemble.

Un des aspects les plus connus est son application au problème de l'origine de la morale, sous le nom de généalogie. Cette application de la méthode à la morale permet de comprendre comment Nietzsche analyse les hiérarchies pulsionnelles en jeu dans toute perspective morale, ce qui est proprement la méthode généalogique. Les questions qui se posent sont alors du type : quel type d'hommes a besoin de telles évaluations morales ? À quelle morale tel philosophe ou métaphysicien veut-il en venir, et à quel besoin cela répond-il ? 
Ces analyses des structures pulsionnelles et affectives forment ainsi un projet de reformulation, à la lumière de la Volonté de puissance, de la psychologie traditionnelle qui était fondée sur le statut privilégié accordé à la conscience.
En réfutant le primat de la conscience, Nietzsche est amené à développer une psychologie des profondeurs (dont tout le premier chapitre de "Par-delà bien et mal" est un exemple) qui met au premier plan la lutte ou l'association des instincts, des pulsions et des affects, la conscience n'étant qu'une perception tardive des effets de ces jeux de forces "infra" conscients. Ce que Nietzsche nomme « généalogie » sera alors la recherche régressive partant d'une interprétation (par exemple, l'interprétation morale du monde) pour remonter à sa source de production, i.e. au "pathos" fondamental qui la rend nécessaire.

Les jugements métaphysiques, moraux, esthétiques, deviennent ainsi des symptômes de besoins, d'instincts, d'affects le plus souvent refoulés par la conscience morale, pour lesquels la morale est un masque, une déformation de l'appréciation de soi et de l'existence. "In fine", cela revient à faire reposer l'analyse sur la détermination de la Volonté de puissance d'un type. À ce titre, l'individu n'est pas examiné par Nietzsche pour lui-même, mais en tant qu'expression d'un système hiérarchisé de valeurs.

Cette méthode amène donc à poser des questions du genre : quelle structure pulsionnelle, incarnée par tel ou tel homme, conduit à tel type de jugements ? À quel besoin cela répond-il, à quelle "Volonté de puissance" ? Veut-on, par la morale, discipliner des instincts, et dans ce cas, dans quel but ? Ou veut-on les anéantir, et dans ce cas, est-ce parce qu'ils sont jugés néfastes, dangereux, est-ce parce qu'ils sont, en tant que phénomènes naturels, l'objet de haine et de ressentiment ? Le premier cas peut être l'expression d'un besoin de croissance, le second d'une logique d'auto-destruction.

Dans "Par-delà bien et mal", Nietzsche expose cette généalogie, conception approfondie et renouvelée par la thèse de la Volonté de puissance (exposé au § 36) de la philosophie historique, et il considère la psychologie comme "reine des sciences", tout en soulignant ce qui distingue sa conception de la psychologie traditionnelle :
Si cette nouvelle psychologie repose, en 1886, sur l'hypothèse de la Volonté de puissance, l'idée du conflit des instincts n'est pas née de celle-ci. Dès 1880, des fragments vont dans ce sens, et la Volonté de puissance en tant qu'idée apparaît bien avant d'être nommée. L'expression Volonté de puissance permet de synthétiser cet ensemble.

Comme cela a été signalé, la Volonté de puissance est une notion qui n'est pas d'emblée présente dans l'œuvre de Nietzsche. Pour rendre compte de l'évolution de la pensée de Nietzsche, il faut partir des hypothèses qu'il pose et des notions qu'il utilise avant la période dite de "maturité". Il en va de même pour la psychologie, puisque le développement de cette dernière apparaît significatif surtout à partir de "Humain, trop humain", c'est-à-dire en 1878, quand il rompt de manière consciente avec son milieu culturel. Influencé par Paul Rée, Nietzsche lit alors avec intérêt les moralistes français (La Rochefoucauld, Chamfort, etc.) ; il lit également des ouvrages contemporains de psychologie, à quoi il faut ajouter des études de sociologie, d'anthropologie, et des travaux sur la théorie de la connaissance, tel que celui de Lange ("Histoire du matérialisme"), où l'on trouve une discussion du statut scientifique de la psychologie. La pensée de Nietzsche, en ce qui concerne la psychologie, se développe donc d'une part d'après l'observation des hommes (les maximes de La Rochefoucauld par exemple, ou ses observations personnelles dont il souligne le caractère particulier, relatif, et souvent provisoire), et dialogue d'autre part avec des réflexions épistémologiques contemporaines.

L'observation psychologique est ainsi particulièrement présente dans "Humain, trop humain" et "Aurore" ; Nietzsche souhaite alors jeter les bases d'une philosophie historique, en procédant à un genre d'analyse chimique de nos représentations et sentiments moraux, préfigurant ce qui deviendra la généalogie. Il analyse les comportements humains, sous l'influence de La Rochefoucauld ou de Voltaire (à qui "Humain, trop humain" est dédié) et peut-être aussi de Hobbes, et ramène souvent les mobiles de l'action et de la pensée humaine à la vanité et au sentiment de puissance. Si certaines de ses peintures sont de cette manière des tableaux de moraliste de l'existence humaine, certains thèmes, comme ce sentiment de puissance, mais aussi les différentes sortes de morales, sont des premières formulations des théories majeures qu'il développera plus tard. Cette étape de son œuvre peut être considérée comme une série d'essais plus ou moins aboutis pour décrire l'homme, ses motivations et la nature de ses relations sociales (aphorismes sur l'amitié, sur l'État, les femmes, etc.).

C'est à partir de 1886 que Nietzsche exposera de manière plus ordonnée le résultat de ses recherches, en tant que méthode généalogique, en particulier dans "Par-delà bien et mal", et sous forme de dissertations dans la "Généalogie de la morale". Des éléments de cette généalogie sont toutefois déjà présents dans "Humain, trop humain" (par exemple, les différentes origines de la morale, ou le caractère de palliatif, et non de remède véritable, de la religion) et dans "Aurore" (la moralité des mœurs comme source de la civilisation, ou encore le sentiment de puissance qui guide l'homme jusque dans la morale).

Ces résultats peuvent être résumés grâce aux expositions schématiques que Nietzsche lui-même en a faites. Ainsi, à la question sur l'origine de la morale, il répond que toutes les valeurs morales se ramènent à deux systèmes d'origine différente : la morale des "faibles" et la morale des "forts". Le terme "origine" ne désigne pas ici l'apparition "historique" de ces systèmes, mais le type de création dont ils sont le résultat, si bien que l'origine, au sens de Nietzsche, est ce à partir de quoi l'histoire se détermine, et non un événement quelconque de l'histoire universelle.

Pour parvenir à ce résultat, Nietzsche a procédé à une généalogie comportant plusieurs moments, exposés dans la première dissertation de la "Généalogie de la morale" : il a recherché dans le langage les premières expressions de ce qui a été jugé bon ; puis, suivant l'évolution du sens des mots "bon" et "mauvais", il a montré le processus d'intériorisation de ces valeurs dont la signification était tout d'abord principalement matérielle ; enfin, remontant d'une évaluation morale donnée à ses conditions d'expression, il a distingué deux manières fondamentales de créer des valeurs morales.

Le point de départ de la méthode généalogique est linguistique : se posant la question de l'origine de la morale, Nietzsche demande : où trouve-t-on les premières notions de "bon" et "mauvais", et que signifient-elles ? Écartant l'interprétation utilitariste, Nietzsche met en avant que ce sont les aristocrates de toutes sociétés qui se sont désignés en premier lieu eux-mêmes comme "bons", et que ce terme, d'une manière simple et spontanée, désigne la richesse, la beauté, les plaisirs de l'activité physique, la santé, en un mot, l'excellence. Le mot "bon" désigne ainsi les hommes de la caste la plus élevée, celle des guerriers. De ce fait, il ne désigne pas ce que nous entendons par là aujourd'hui, en particulier, un "bon" n'est pas un homme altruiste, charitable, accessible à la pitié.

L'analyse historique et linguistique débouche ainsi sur une recherche d'ordre sociologique : les premières évaluations morales dépendent et sont l'expression d'un rang. Néanmoins, Nietzsche ne reprend pas à son compte les théories contemporaines, telles que celle de l'influence du milieu de Taine, car s'il faut tenir compte des déterminations sociales, la société ne peut servir de principe explicatif intégral. Il renomme d'ailleurs cette science d'après son interprétation généalogique ("théorie des formes de domination") qu'il juge première relativement à la sociologie et à la psychologie de son temps.

La question est ainsi pour Nietzsche la suivante : dans quelle mesure les castes d'une société permettent-elles le développement d'une espèce particulière de jugements moraux ? Nietzsche distingue typologiquement plusieurs types de jugements moraux en fonction des situations sociales possibles (guerriers, prêtres, esclaves, etc.) :
La situation sociale permet à un sentiment de puissance de se distinguer par des formes qui lui sont propres, et qui, primitivement, possèdent des expressions spontanées et entières peu intériorisées. De cet examen des castes, Nietzsche dégage alors une première grande opposition :
Nietzsche ramène par la suite toute morale à deux types fondamentaux qui correspondent originellement à l'opposition dominant/dominé. Il faut écarter l'idée que les dominants, ceux qui créent en premier lieu les valeurs, seraient uniquement des guerriers : la genèse des valeurs dégagée par Nietzsche énonce clairement un conflit entre le monde de l'activité physique et celui de l'activité intellectuelle (c'est-à-dire de la volonté de puissance intériorisée). Aussi Nietzsche voit-il d'abord une dispute sur la question du "rang" des valeurs entre les guerriers "et" les prêtres.

Du fait que cette compréhension de la morale permet la constitution de types, elle ne doit pas être réduite à la réalité des hiérarchies sociales : une hiérarchie sociale est "une condition première de la création d'une évaluation", mais, selon Nietzsche, les évaluations peuvent devenir indépendantes de leur terrain de naissance. L'origine fait comprendre "comment" une valeur est née, elle ne fait pas encore comprendre "pourquoi" elle s'est perpétuée. En conséquence, un esclave, au sens de Nietzsche (un faible), peut très bien être un maître, dans un sens plus prosaïque, c'est-à-dire posséder du pouvoir et des richesses. Les hiérarchies sociales permettent seulement de comprendre comment des types moraux ont été rendus possibles, et la question reste de savoir quel type d'hommes l'ont ensuite transmise (et par quels nouveaux moyens).

Quant aux « types », ce sont des interprétations généalogiques que l'on ne rencontre pas telles quelles dans la réalité (des traits typiques opposés peuvent par exemple se trouver liés).

Il y a donc, selon Nietzsche, une dualité fondamentale en morale, dualité qu'il avait déjà formulée clairement dans "Humain, trop humain" et "Aurore" : la morale des forts et la morale des faibles, cette dernière trouvant son origine dans son opposition à la première.

La morale des faibles se caractérise par son ressentiment ; Nietzsche en décrit ainsi le mécanisme psychologique :

La morale des faibles est donc l'expression de ce ressentiment : le ressentiment est l'affect d'une volonté vaincue qui cherche à se venger, c'est-à-dire qu'il est le symptôme d'une vie décroissante, qui ne s'est pas épanouie. Cette vengeance s'exprimera par des valeurs créées pour lutter contre les forts, en dévalorisant leur puissance (le fort devient le méchant par opposition au bon). Ainsi, selon Nietzsche, la pitié, l'altruisme, toutes les valeurs humanitaires, sont en fait des valeurs par lesquelles on se nie soi-même pour se donner l'apparence de la bonté morale et se persuader de sa supériorité ; mais sous ces valeurs illusoires fermente une haine impuissante qui se cherche un moyen de vengeance et de domination. Le christianisme, l'anarchisme, le socialisme, etc. sont des exemples de morales du ressentiment.

En sens contraire, la morale des "forts" exalte la puissance, c'est-à-dire l'égoïsme, ou plaisir d'être soi, la fierté, l'activité libre et heureuse. Ces valeurs sont essentiellement le résultat d'une spiritualisation de l'animalité qui peut alors s'épanouir heureusement. Ainsi en Grèce la sexualité est-elle exprimée dans les cultes de "Dionysos" et dans l'art ; chez Platon, le désir de savoir est la conséquence d'une spiritualisation de l'instinct de reproduction. La morale des "faibles" agit en sens contraire, en cherchant à détruire à la racine tous les instincts, par haine de la vie, c'est-à-dire par suite d'une violence intériorisée qui ne peut s'exprimer que sous la forme négative de la destruction de soi (c'est le mauvais de la morale aristocratique). Par contraste, ce qui caractérisera le mieux une morale de "forts", ce sera sa capacité d'élever des hommes cultivés, inventifs, actifs, doués d'une volonté forte et constructive.

On ne doit pas cependant ignorer que les forts, dans l'histoire, sont "tout d'abord" (terme souligné par Nietzsche dans le premier aphorisme de la neuvième partie de "Par-delà bien et mal") des hommes violents, mais cette violence n'est pas d'une même sorte que la violence du faible, qui lui aussi veut la puissance, mais par d'autres moyens. La violence du fort est spontanée et sans arrière-pensées, elle n'est pas vindicative, tandis que la violence du faible est calculée, et c'est une violence au service du ressentiment, i.e. de la haine. Bien que la force ne soit pas chez Nietzsche nécessairement exprimée par la violence, et, qu'en outre, la spiritualisation des instincts les plus agressifs soit la forme la plus haute de la culture, il reste que la « spontanéité » du fort est en premier lieu particulièrement cruelle, quelle que soit la civilisation considérée :
Cette violence n'est pas une fin en soi, mais est le socle de l'élévation humaine, sans lequel l'homme se renie et se mutile en tant qu'animal. L'ensemble des instincts qui font voir la proximité de l'homme avec la bête doit être, pour Nietzsche, spiritualisé, car cette spiritualisation est une augmentation de la volonté de puissance, par exemple dans la création artistique. Ainsi, lorsqu'il examine le processus d'élévation du fort, Nietzsche, qui a souligné la barbarie première de ce fort, ne met pas en avant la force physique, mais bien l'âme. Et, dans "Ainsi parlait Zarathoustra", il s'adresse ainsi aux hommes violents :

La violence du faible est en revanche pour Nietzsche problématique, si elle domine : c'est une violence cruelle, une violence pour la vengeance, et elle ne se laisse pas facilement convertir en activités créatrices, mais se transforme plus aisément en systèmes de cruauté, i.e. en religions ou en morales visant à abattre l'existence même de ce qui est différent.

Il faut alors souligner l'importance de cette opposition des deux morales qui structurent l'histoire de l'Occident : tout ce qui est fort a créé ce qui est bon, la philosophie et l'art grecs, ce qui est faible a créé la religion monothéiste et son système de répression de la force qui est encore le nôtre aujourd'hui. La question qui se pose à Nietzsche est donc de savoir comment un tel système a pu se développer à partir du ressentiment et de l'intériorisation de la volonté de puissance.

L'impossibilité pour les castes soumises à une discipline sévère et pour les peuples soumis d'extérioriser librement leurs forces ne fait pas disparaître ces forces. Nous trouvons dans le second cas l'origine du ressentiment des valeurs morales. Nietzsche met ici au jour un phénomène « prémoral » qui consiste au retournement des forces vers l'intérieur : intériorisation qui va permettre le développement de l'âme et l'approfondissement de la psyché humaine en une variété de types inconnus jusqu'alors.

Les pulsions naturelles de conquête, opprimées par des facteurs extérieurs (État, éducation…) se retournent contre l'individu opprimé, en lui-même, créant un malaise, dont l'origine lui reste inconnue, qu'il va rationaliser en termes de faute, mauvaise conscience et culpabilité.

Ce phénomène d'intériorisation est diversement interprété. Il reçoit en particulier une interprétation religieuse, et, dans le cas du ressentiment des faibles, l'intériorisation, qui est une cause de souffrances morales et physiques, va trouver dans le christianisme une interprétation en tant que péché.

Selon Nietzsche, en effet, l'inversion morale des valeurs par les faibles, ne suffit pas à expliquer la puissance avec laquelle elle s'est imposée dans l'histoire. Il y faut encore l'intervention du prêtre, dont nous avons vu qu'il s'oppose, dans une rivalité de castes, au guerrier (et au politique). L'invention du prêtre chrétien est la réinterprétation de la souffrance en tant que culpabilité de celui qui souffre : alors que la faute était rejetée sur le "méchant", c'est maintenant pour ses propres fautes que le faible souffre.


L'interprétation religieuse de l'existence permet à Nietzsche de dégager deux attitudes fondamentales face à la souffrance, qu'il résume par la formule : "Dionysos contre le Crucifié."

La première attitude consiste à percevoir la souffrance comme un stimulant pour la vie ; la tragédie grecque en est un exemple. La seconde attitude consiste à se replier sur soi, à réagir, en sorte que l'on ne puisse plus agir. De ce fait, l'interprétation de la souffrance est ainsi en même temps une évaluation de la réalité.

Selon Nietzsche, le rapport de l'homme au monde, tant en ce qui concerne la volonté (désirs, aspirations, espoirs) que l'entendement et la raison (métaphysique, connaissance) fut jusqu'ici essentiellement le résultat de jugements moraux nés du ressentiment d'impuissants qui disent « non » à la réalité et la vie, tout en se parant des plus hautes vertus de la morale. C'est cette idée qu'il exprime, dans le "Crépuscule des idoles" : Et, plus loin : La théologie assura la pérennité de cette détermination morale de l'existence, et la philosophie s'en fit l'auxiliaire. .

Que peuvent alors signifier de tels jugements ? Dans la mesure où ils se construisent en opposition à l'apparence, ils ne peuvent signifier que le néant : Dieu, l'être, le bien et tout pensée de l'en soi, de l'absolu, sont les symptômes d'une même volonté de vaincre le devenir, associés au néant, d'une volonté d'en finir qui, paradoxalement en apparence, se mettent à créer des valeurs. Ces valeurs, cependant, expriment la grande lassitude, l'épuisement de l'homme face au monde. 

C'est pourquoi, le nihilisme est selon Nietzsche l'événement majeur de l'Europe, il en est même le destin depuis Platon. Mais ce nihilisme éclate aujourd'hui : il exprimerait alors un tournant historique dans la hiérarchie des valeurs reçues jusqu'ici. Cet éclatement du nihilisme pourrait être résumé par la formule célèbre : , car si Dieu est mort, la morale n'a plus de fondement, bien que l'ombre du dieu mort (son influence axiologique) agisse encore fortement sur des hommes même athées : 

La critique de la métaphysique, en réfutant l'idée de la pensée d'un en soi, d'un être absolu, contribue à précipiter la crise nihiliste, en l'amenant à son point extrême où l'on ne peut esquiver de penser le problème hiérarchique des valeurs qui, privées de leur fondement, entrent en contradiction avec le monde dans lequel nous vivons : nos valeurs sont devenues insoutenables, et sources de contradictions psychiques.

Le nihilisme signifie alors que les anciennes valeurs sont dépréciées. Ainsi, la critique de la métaphysique révèle-t-elle le nihilisme des valeurs humaines. Mais Nietzsche distingue plusieurs types de nihilisme, selon la force ou la faiblesse qui l'inspire.

Tout d'abord, Nietzsche distingue deux types de nihilisme :
Lorsque le nihilisme consiste à dévaluer le monde naturel au nom d'un monde suprasensible, Nietzsche parle d'un nihilisme des faibles : le monde ne devrait pas exister pour le faible qui n'est pas capable de maîtriser les choses, de mettre un sens dans le monde. Le monde est pour lui une souffrance : il se sent supérieur à lui, et, partant, étranger au devenir. Ce nihilisme s'exprime par exemple dans le pessimisme, mais, essentiellement, il est d'origine morale, car les valeurs morales entrent en conflit avec le monde que nous vivons. C'est un nihilisme inconséquent, car il devrait logiquement aboutir à la suppression de soi : si la morale et le monde se contredisent, il faut en effet soit détruire la morale ancienne (mais pas toute morale : Nietzsche est immoraliste et non a-moraliste), soit se détruire soi-même :
En sens contraire, le nihilisme des forts est une sorte de mue : des valeurs sont abandonnées et d'autres sont adoptées. La volonté du fort n'est pas abattue par l'absurde, mais invente de nouvelles valeurs à sa mesure. Ainsi, le dépassement du nihilisme, à travers la pensée de l'éternel retour, est-il nommé transvaluation des valeurs. Ce nihilisme conduit alors au surhomme, qui est celui qui approuve entièrement le monde du devenir, son caractère changeant et incertain : on peut dire que le surhomme "est" ce monde, il le vit.

De ce second sens, il est possible d'extraire encore un autre sens, réservé à l'élite des esprits libres : le nihilisme de la pensée, la négation absolue de l'être, négation qui devient selon Nietzsche la manière la plus divine de penser. Selon cette pensée, il n'y a pas du tout de vérité ; nos pensées sont alors nécessairement fausses.

La définition la plus simple de la décadence donnée par Nietzsche est que l'on peut qualifier de décadent un être qui choisit ce qui le détruit en croyant choisir quelque chose qui accroîtrait sa puissance. Mais la décadence est loin d'être un état définitif ; au contraire, selon Nietzsche, tout être, fort ou faible, a des périodes de décadences. La décadence est ainsi un phénomène naturel et n'est pas utilisé comme condamnation morale.

L'avènement du nihilisme, et la possible décadence des sociétés modernes, mettent en jeu l'avenir de l'Europe (et non des nations, encore moins des « races »), et impliquent de ce fait une réflexion approfondie sur la civilisation moderne, en particulier dans le domaine de la politique et de la législation, le but de Nietzsche étant de comprendre les moyens de rendre possible une nouvelle civilisation qui rompe avec les anciennes valeurs de l'Occident, ainsi qu'avec ses valeurs les plus douteuses, telles que les particularismes nationaux de l'époque.

L'examen des évaluations morales va permettre à Nietzsche de soutenir que ces valeurs sont non seulement des évaluations d'ordre éthique, mais qu'elles s'étendent aussi à la métaphysique et en explique l'origine. La question fondamentale posée par Nietzsche est ici : "que signifie la volonté de vérité ?" Ou bien : nous voulons la vérité, "mais pourquoi pas l'erreur ?"

Puisque toute connaissance est une interprétation, tous les concepts qui lui sont relatifs doivent être eux aussi réinterprétés généalogiquement. La généalogie montre l'origine des valeurs morales du ressentiment qui se sert de certaines catégories métaphysiques, telles que la Vérité, le Bien, etc. Ainsi les facultés cognitives humaines semblent-elles déterminées par une évaluation de l'existence née de la haine, c'est-à-dire d'affects réactifs dont la motivation principale est la vengeance. Connaissance et métaphysique, domaines de la spiritualité humaine en apparence d'une grande pureté, seraient donc en réalité dépendantes d'une forte affectivité sans laquelle elles n'existeraient pas :

La critique nietzschéenne de la métaphysique, en tant que psychologie des profondeurs ou généalogie (dévoilant l’origine de concepts tels que vérité, être), se présente comme un aboutissement, exposé en 1886. Nietzsche critique les contradictions internes de la métaphysique par un examen que l'on pourrait qualifier de positiviste, et qui s'appuie souvent sur des arguments sceptiques.

Dans le premier chapitre du premier tome de "Humain, trop humain" (en 1878), il rend compte de l'impossibilité de la métaphysique, dont on prend conscience pourvu que l'on veuille bien raisonner de manière rigoureuse, c'est-à-dire de manière "sceptique" : méthode qui est celle d'Ænésidème, de Hume et de Kant (malgré les violentes critiques de Nietzsche, la critique kantienne de la métaphysique est vue comme un problème de premier ordre).

En ce qui concerne les sceptiques, Nietzsche dira, à la fin de sa vie consciente (cf. "Antéchrist") :
Cette critique montre que nous n'avons aucune connaissance de quoi que ce soit en dehors de ce que nous percevons, que ce que nous percevons n'est rien d'autre que devenir, et que cette perception est une perspective. Il résulte de cette thèse qu'il ne peut y avoir de vérité absolue pour nous :
Cependant, dans "Humain, trop humain", Nietzsche n'exclut pas qu'un monde métaphysique puisse exister ; conformément à la méthode sceptique, il admet également qu'un tel monde pourrait être prouvé :
Néanmoins, il précisera plus tard cette dernière affirmation en la considérant sous l'angle de la preuve, en s'écartant cette fois de la pensée sceptique :
Cela signifie notamment qu'il n'y a pas du tout de connaissance, mais seulement tentative d'interprétation du monde dans lequel nous vivons. Ce point est exprimé déjà dans "Humain, trop humain" et avec plus de force encore et de manière répétée dans "Le Crépuscule des idoles" :
Nietzsche va passer à un autre plan, en affirmant non seulement que la preuve de l'existence ou la non-existence de ce monde nous est parfaitement indifférente (ce que les sceptiques avaient déjà reconnu), mais qu'il faut encore expliquer pourquoi, malgré cette démonstration rigoureuse connue depuis des millénaires, un autre monde a pu être pensé comme autre chose qu'une simple hypothèse hasardeuse et pourquoi on a voulu le voir vrai en tentant de le prouver.

Pour Nietzsche, il n'y a donc pas de vérité absolue ; or, dès lors qu'aucune vérité absolue n'est possible, on rejette du même coup le monolithisme de la métaphysique (cf. "Le Crépuscule des idoles"). Mais cette négation de la vérité ne signifie pas que Nietzsche n'admet aucun sens à ce concept ; au contraire, le rejet de l'absolu fait apparaître un grand nombre de significations qui se prête à l'analyse et révèle les différentes volontés qui s'investissent dans ce concept. Deux textes des années 1870, "La passion de la vérité" et "Vérité et mensonge au sens extra-moral", montrent à quel point ces volontés sont diverses et le concept riche de sens.

La connaissance n'existant pas, il faut expliquer pourquoi il y a néanmoins une volonté de vérité. Selon Nietzsche, la vérité a en premier lieu un caractère social et pragmatique, qui se comprend à plusieurs niveaux :
Comme ce sont certaines vérités qui sont retenues ; au bénéfice de la communauté.


La règle générale est qu'une institution ou une société génèrent un champ de croyances qui leur sont spécifiques (cf. "Le Crépuscule des idoles"). Plus l'autorité est forte, et moins elle tolère les démonstrations. Les mœurs, les lois, la police, assurent ainsi la pérennité d'une évaluation de la réalité. Toute connaissance qui sort de ce cadre est fausse, dangereuse, mauvaise. Mais il ne s'agit pas pour Nietzsche de condamner unilatéralement cette obstruction arbitraire de l'autorité et de la coutume à la raison car c'est l'arbitraire qui a permis à l'humanité de survivre.

Ce conformisme grégaire n'explique pas dans l'immédiat l'idéalisme métaphysique (que Nietzsche nomme le « désirable », ce que l'homme veut que le monde soit, en contradiction avec ce qui est) et la croyance en une connaissance en soi. Le problème de la métaphysique demande donc tout d'abord à être analysé en plusieurs éléments. Nietzsche propose ici une interprétation de la métaphysique comme division de la totalité de l'étant en deux sphères distinctes.

Nietzsche part en effet d'une conception de la métaphysique dans laquelle les opposés ont une valeur fondamentale :

Ces opposés ont un statut ontologique radicalement différent et ne peuvent être expliqués les uns par les autres. Ces oppositions suscitent de graves difficultés logiques et morales :
Selon Nietzsche, l'opposition métaphysique fondamentale serait alors que "ce qui est ne devient pas, ce qui devient n’est pas".

Pourquoi ce qui est de l’ordre du devenir doit-il être rejeté ? Il faut répondre que le devenir nous trompe car nous ne pouvons jamais l'appréhender.

Mais, si nous n'avons rigoureusement aucun accès cognitif à un monde métaphysique, il nous faut expliquer pourquoi on en vient à penser que le désir nous trompe. Sans l'existence de l'être, le monde du devenir ne pourrait avoir toute notre confiance. Les hommes croient toujours à des entités dont pratiquement personne n'a jamais eu l'expérience. Les croyances religieuses et les certitudes métaphysiques doivent donc faire l'objet d'un examen particulier.

Pour Nietzsche, la croyance en un monde métaphysique est le symptôme d'une volonté de déprécier celui-ci. On retrouve ainsi les évaluations des faibles :
Les philosophes se vengent donc de la vie en "momifiant" tout ce qui à leurs yeux a de la valeur :

En conclusion, selon Nietzsche, la connaissance a une origine morale ; elle est une évaluation du monde selon des valeurs humaines, selon ce que l'homme désire trouver dans le monde.

Le sentiment, le plaisir que cause une croyance serait la preuve de sa vérité. L'idéalisme se confond ainsi avec le "désirable" : l'homme veut que le monde ou une partie du monde satisfasse ses désirs. L'interprétation de Nietzsche réduit de cette manière tout idéalisme, toute métaphysique et toute morale à une forme d'eudémonisme. Par là, il leur dénie le droit de dire ce qui est vrai.

En effet, tout ce qui est prouvé dans ce cas, c'est la force du sentiment, la force du désir en contradiction avec la réalité. Mais une vérité peut être ennuyeuse, désespérante, ne pas se conformer avec nos souhaits moraux ; il faut envisager sérieusement l'idée que la vérité peut être horrible, inhumaine, que l'on peut périr de la vérité. De cette manière Nietzsche supprime tout lien nécessaire entre Vérité et Bien, lien qui existe dans la métaphysique de Platon et d'Aristote mais aussi dans la théologie chrétienne.

De ce fait, l'idéalisme, c'est-à-dire le déni de la réalité que nous avons sous nos yeux au profit d'une réalité "différente" et plus agréable, cet idéalisme, poussé à ses extrêmes, est comparable aux sentiments morbides que ressent un malade qui ne supporte pas le contact physique. L'idéaliste, le chrétien, le démocrate, le socialiste, l'anarchiste, la féministe, etc., sont tous plus ou moins dans une situation fausse relativement à la réalité : ils adoptent un comportement infantile de refus qui découle inévitablement de leurs faiblesses.

Les convictions morales (telles que l'égalité entre les hommes) qui supposent des catégories métaphysiques comme l'idée qu'il y aurait une essence une et universelle de l'homme (qui supposent donc un "autre" monde, le monde vrai, réel, de la morale), ne se distinguent alors pas d'une sorte de mensonge irrépressible déterminé par un profond malaise physiologique et psychologique face à notre existence foncièrement immorale, face au caractère tragique de la vie.

À l'opposé de l'eudémonisme de la vérité, la capacité de regarder froidement la réalité, sans y projeter ses désirs et ses insatisfactions, est pour Nietzsche une vertu philosophique nommée "probité."

Dès lors que la métaphysique est réfutée, apparaît l'idée que nous puissions faire une histoire de la connaissance, ce qui conduit Nietzsche à considérer les catégories de nos facultés cognitives comme les résultats d'habitudes grammaticales devenues instinctives. Mais le langage a une origine lointaine et véhicule des préjugés "rudimentaires" :
Cette métaphysique du langage exprime essentiellement la croyance en la causalité de la volonté, croyance dont découlent des principes de la raison :

Cette métaphysique du langage entraîne à l'erreur de l'Être : 
Le langage a donc une place importante dans le développement des facultés cognitives humaines. La théorie du langage développée par Nietzsche évoque la philosophie d'Épicure : le langage est une convention "naturelle" qui découle des affects. Le langage est un système de signes qui "transpose" dans un autre domaine les impulsions nerveuses. C'est en cet autre sens que le langage est métaphorique.

Mais l'usage qui est fait du langage occulte ce rapport métaphorique au monde, et les images qu’il véhicule s'objectivent en concepts. Nietzsche suggère alors, comme Épicure, que l'on doit pouvoir retrouver l'expérience originelle du langage. Cependant, contrairement à Épicure, ce qui est retrouvé n'est pas un rapport de connaissance, mais un rapport esthétique ; c'est pourquoi, le chant est particulièrement propre à nous le faire revivre :
Il faut enfin découvrir l'origine de la possibilité de toute métaphysique, au-delà ou en deçà des interprétations que l'on peut en faire : le point de départ de toutes les erreurs de la métaphysique est une croyance :
Cette croyance implique deux choses : 

Dès lors, nous projetons les catégories de l'action dans le monde des phénomènes, et croyons que tout événement suppose une substance qui ne se peut réduire aux qualités phénoménales. C'est là l'idée d'une chose en soi.

Cette erreur n'est donc pas seulement induite par le langage, comme les autres erreurs, mais elle a un caractère originellement psychologique dont il faut expliquer pourquoi elle a eu un si grand succès.

Ce succès s'explique si l'on considère que cette erreur dans la connaissance de soi comme cause a été interprétée comme libre arbitre (ce point est analysé par Nietzsche dans le chapitre du "Crépuscule des idoles" intitulé "Les quatre grandes erreurs"). Elle fait référence à la thèse de Nietzsche selon laquelle la liberté a été inventée pour rendre les hommes responsables de leurs actes.

Si nous suivons le raisonnement de Nietzsche, l'ensemble des erreurs de la métaphysique a ainsi une origine théologique et morale : l'homme est la cause de ses actes ; son moi est sa substance, son être, d'après lequel il va interpréter le monde des phénomènes en y projetant cette causalité psychologique qui sépare ce qui agit (un sujet, un substrat de ce qui devient) de ses effets. Cette croyance entraîne l'invention de l'unité, de l'identité, de la causalité, etc. toutes ces catégories qui prendront une forme systématique dans la métaphysique.

La crise nihiliste appelle une réflexion sur les fondements des valeurs qui forment une culture. Cette reflexion embrasse d'une part une critique de la modernité, en tant qu'héritière des valeurs platonico-chrétiennes, et, d'autre part, une nouvelle donne grâce à la possibilité d'établir de nouvelles hiérarchies par le philosophe, en tant que médecin de la culture et législateur. Cette crise des valeurs pose le problème du pourquoi de l'existence humaine (« À quoi bon ? »). L'humanité peut-elle se donner à elle-même des buts ? Le philosophe a pour responsabilité de créer une échelle de valeurs permettant de substituer à la volonté de néant une volonté de vie, d'avenir, de dépassement.

Un aspect important pour comprendre la pensée de Nietzsche est son anti-modernisme relatif. Cette opposition se manifeste avec virulence dans sa critique de la démocratie, de Rousseau, de l'héritage chrétien et de l'éducation moderne. Pour autant, Nietzsche n'est pas traditionaliste, dans la mesure où il souhaiterait voir la politique, l'État et toute autorité subordonnés à une éducation élitiste tournée vers l'art et la pensée. Bien plus, la culture s'oppose à tout ce qui est politique, et tout ce qui est politique est dangereux pour la culture ("Le Crépuscule des idoles", "§ Ce qui manque aux Allemands"). Il n'est donc ni un conservateur, ni un apôtre d'une société de traditions qui figeraient le devenir culturel de l'humanité. Nietzsche s'oppose également au militarisme, et critique très sévèrement la bêtise militaire et culturelle et les vaniteuses prétentions du Reich :
La pensée de Nietzsche a pu sembler foncièrement apolitique, pourtant le problème de l'éducation et de la réussite d'une grande culture l’a hanté toute sa vie.

Ce problème est au cœur des "Considérations Inactuelles" : dans sa troisième "Considérations inactuelles", il reprend les critiques de Schopenhauer contre la philosophie universitaire. On ne peut à la fois servir l'État et la vérité. Quand l'État nomme des « philosophes », il le fait pour sa puissance. Nietzsche soupçonne d'ailleurs que le véritable but de l'université est de dégoûter les jeunes gens de la puissance que constitue l'authentique philosophie en les abêtissant :
Ainsi, la philosophie universitaire est-elle ennuyeuse, approximative, arbitraire, et est une fumisterie de la culture moderne. À ce propos, Nietzsche cite l'anecdote du philosophe qui demandait à une personne en deuil la cause de son malheur ; quand on lui eut appris qu'un grand philosophe venait de mourir, il s'étonna : un philosophe ? Mais… il n'a jamais affligé personne !

Comme ce philosophe, il faut dire, selon Nietzsche, que la philosophie universitaire n'afflige personne, et que cela même est affligeant ! La solution pour remédier à cette situation serait alors d'expulser les « philosophes » de l'université, de leur retirer leur traitement pour faire le tri, voire de les persécuter. On verrait ainsi où sont les véritables penseurs, comme l'était Schopenhauer.

Cette critique de la philosophie universitaire est un aspect capital de la critique qu'il adresse à ceux qu'il appelle les "philistins de la culture" et qui révèle l'état misérable de la civilisation allemande, notamment depuis sa victoire militaire sur la France, victoire qui a marqué, selon lui, la fin lamentable de l'histoire de l'abêtissement millénaire de l'Allemagne.

Nietzsche critique particulièrement l'illusion qu'avaient les Allemands, après leur victoire contre la France en 1870, que cette victoire militaire signifiait également une victoire culturelle, une supériorité de la culture allemande sur la culture française.
Au contraire, il affirme que malgré sa défaite, la France a conservé sa domination culturelle.

Nietzsche décrit le type d'homme qu'il nomme démocratique ("demokratisch") comme le type représentatif des idées modernes ; il décrit également la place de la démocratie dans l'histoire, son mouvement, et l'importance qu'elle peut avoir pour l'avenir ("le mouvement démocratique"). Outre cette distinction, il faut remarquer que Nietzsche emploie le mot « Democratie » dans les années 1876 - 1879 pour désigner l'État démocratique, tandis que la qualité démocratique possède, à partir des années 1882 - 84, un sens général qui désigne un type et peut donc s'appliquer à des réalités non politiques (comme l'art et la science).

Le type démocratique est analysé par Nietzsche de la même manière qu'il analyse, selon la méthode généalogique, tous les autres types : en cherchant la structure des instincts de ce type, et les jugements de valeur, ou "goût", qui en découlent. Le trait typique du goût démocratique est l'égalitarisme, qui peut être aussi appelé "ressentiment contre la grandeur", qui lutte contre tout ce qui veut s'élever, et considère que personne n'est mieux qu'un autre. L'égalitarisme moderne ne peut ainsi, selon Nietzsche, permettre une haute culture de l'esprit et entretient la solidarité du ressentiment des incultes. La démocratie, telle que Nietzsche la conçoit, est cette idéologie du troupeau qui cherche la sécurité et le bien-être, aux dépens de la supériorité intellectuelle, en lui faisant la guerre, en se faisant l'ennemi de tout génie : d'où la critique de l'éducation démocratique moderne qui entrave le développement intellectuel et ne produit que des individus à demi cultivés, grossiers voire barbares.

L'esprit démocratique, tel que le perçoit Nietzsche, est complaisant, curieux et futile, bariolé et sans goût, sans grande ambition avec ses « petits plaisirs pour le jour et ses petits plaisirs pour la nuit », satisfait de sa médiocrité tranquille et de son bonheur bovin :
Nietzsche refuse cette conception d'une égalité entre les hommes (héritée du christianisme selon lui). Cette critique s'accompagne d'une nuance importante, qui soustrait Nietzsche à la qualité d’un opposant absolu à la démocratisation de l'Europe ; il souligne lui-même la duplicité dont on peut faire preuve en simulant une haine féroce contre la démocratie, alors que l'avancée de celle-ci sert des visées entièrement opposées.

En effet, jugeant que le nivellement de l'humanité par l'égalitarisme est inévitable, Nietzsche conçoit l'idée que l'Europe devra nécessairement se fédérer en détruisant les nationalismes et s'unifier économiquement, et que l'humanité sera un jour gérée au niveau mondial (ce qu'il appelle la domination à venir de la Terre) :
Tout cela va dans le sens d'une homogénéisation des sociétés humaines, d'une « médiocrisation » sociale et culturelle généralisée. Ceci équivaut pour lui à la création d'un citoyen moyen, sans qualité, formant un troupeau suivant des vertus d'obéissance à l'ordre social, quasi-esclaves, satisfaits toutefois de leur condition (qu'ils ont voulue). Cette socialisation de l'homme (le grégarisme planétaire) revient à bâtir une infrastructure d'où pourront surgir de nouvelles classes dominantes, et ce nivellement recèle donc une nouvelle possibilité de hiérarchie. Cette pensée est une partie importante de sa grande politique.

Ces critiques de la culture moderne s'accompagnent d'une tentative de repenser les conditions précises de toute civilisation. Comment éduque-t-on les hommes ? Comment l'homme est-il parvenu au génie artistique et philosophique ? Cela ne nous étonne pas, car nous sommes trop habitués par les valeurs humanistes de l'Occident à considérer l'homme comme une nature donnée une bonne fois pour toutes. La réflexion sur ce thème de la culture apparaît alors comme un questionnement sur l'animalité de l'être humain et sur l'éducation (discipline, contraintes) qui lui est donnée. Cette animalité avait été refoulée par la religion, la morale et la philosophie, si bien que la question de l'élevage de l'homme est demeurée inconsciente, comme dans le cas de la volonté morale d'améliorer l'humanité - qui est selon Nietzsche un dressage qui ne se considère pas comme tel, et qui refuse de se considérer comme tel.

Le processus qui conduit l'homme à la civilisation commence par la moralité des mœurs : Nietzsche considère en effet l'homme comme un animal auquel on a dû apprendre à promettre en le soumettant aux mœurs et à la loi par un dressage violent et arbitraire (d'où la torture, la dette à payer en livre de chair). Le résultat est un animal qui peut tenir sa parole, dont la volonté se maintient dans le temps, et qui a conscience que cette faculté est une distinction : la capacité de promettre est en effet l'expression de la puissance que l'on possède du fait de la maîtrise de soi que l'on a acquise. La violence des moyens employées par l'humanité est alors abolie par la création de l'individu autonome, d'un « sur-animal » capable de répondre de lui-même, de se déterminer et de se créer ses propres valeurs.

Dans ce processus, le rôle de la justice est alors de contenir les débordements violents du ressentiment et de la vengeance, et d'imprimer en l'homme, si besoin par la force, un point de vue juridique qui le sépare de ses réactions immédiates (préjudices contre préjudices, violence contre violence) et l'amène à se concevoir comme un être responsable devant la loi.

Le droit dépend de l'équilibre des forces, c'est-à-dire qu'il n'y a pas de contrat naturel. Nietzsche reprend sur ce point les thèses de Spinoza sur l'équivalence du droit et de la puissance.

La généalogie montre que les instincts ne sont jamais éradiqués. La conséquence que Nietzsche en tire est qu'une action bonne n'est qu'une action mauvaise spiritualisée, une action mauvaise n'est qu'une action bonne restée à l'état de la grossièreté et de la bêtise de l'instinct. La spiritualisation consiste donc à ne pas lutter contre les passions, comme le fait la morale en Occident, mais à leur fixer un point d'application différent.

La sexualité est pour Nietzsche un aspect majeur de la culture. Aussi a-t-il considéré la relation entre les sexes comme l'un des fondements de la spiritualisation des instincts et de la force d'une civilisation. Il pose comme principe que les hommes doivent avoir pour les femmes un sentiment déterminé de possession :
L'homme doit ainsi assujettir la femme pour assumer et posséder pleinement son identité sexuelle propre, en sorte qu'une éducation des instincts, et notamment de la sexualité, devienne possible et créatrice d'une haute culture. Car si la femme est dans l'esprit de l'homme qui la désire un être réactif (l'homme étant l'animal fécond, actif, créateur), c'est-à-dire un être faible et servile qui ne peut s'accomplir que dans la servitude, elle participe indirectement à la culture, en suscitant chez l'homme le plaisir de dominer, d'affirmer son désir : les femmes, remarque Nietzsche, parviennent à être le centre d'intérêt de toute une civilisation quand l'amour devient un motif essentiel des arts, ce qui signifie que l'excitation sexuelle y a pris une forme raffinée, esthétique et augmentant le plaisir de la vie. Ainsi, si Nietzsche rappelle que le rôle des femmes est de mettre des enfants au monde et d'être un divertissement pour les hommes, leur force est précisément dans la faiblesse de leur nature, dans la séduction qu'elles exercent, et qui leur permet à leur tour de dominer et de former la sensibilité morale et instinctive masculine. Non seulement les femmes mettent des hommes au monde, au sens propre, mais le désir qu'elles suscitent met un homme au monde, au sens figuré. Nietzsche ne nie pas que certaines femmes puissent être exceptionnelles (de même qu'il y a des hommes exceptionnels, mais rares).

L'égalité entre hommes et femmes est alors pour Nietzsche une injustice démocratique, un préjugé chrétien, une idée qui a des racines théologico-morales, et qui n'a de ce fait, aucun rapport avec la réalité naturelle. Homme et femme possèdent l'un sur l'autre un pouvoir de domination spécifique qui les oppose et les réunit tour à tour, et que l'on ne peut égaliser sans affaiblir à la fois l'homme et la femme, car on abolirait ainsi la lutte féconde entre les sexes. Ce pouvoir des deux sexes possède sa racine commune dans l'attirance sexuelle, cette forme la plus primitive de la Volonté de puissance et, partant, l'expression la plus innocente et la plus dionysiaque de l'affirmation de la vie. C'est pourquoi Nietzsche estime que l'émancipation de la femme s'accompagne de son enlaidissement moral et intellectuel : la femme moderne est sotte et sans intérêt, parce qu'elle se dépouille de la force de sa faiblesse, et tente d'acquérir des vertus masculines, ce qui lui fait perdre toute influence bénéfique sur l'homme. À l'inverse, bien qu'il critique cette volonté d'émancipation, il estime que l'homme occidental, en imposant une morale répressive en matière de sexualité, a produit une situation d'insatisfaction dans les rapports entre sexes, dont la femme, et notamment les femmes d'exceptions, souffre d'autant plus que les conventions ne lui permettent pas d'assouvir ses besoins intellectuels et physiques aussi librement que les hommes.

Sa pensée politique est centrée autour des conditions de possibilité de la culture ("Cultur"). L'inversion des valeurs en est l'une de ces conditions. Mais Nietzsche veut d'abord faire œuvre de législateur, et c'est pourquoi il examine les conditions matérielles de l'éducation, du corps et de l'esprit. Il s'inspire sur tous ces points de la culture grecque (seule véritable culture) et de la civilisation de l'Inde (dont le système de caste peut être considéré comme un type sociologique). Cette partie de sa politique suscite généralement l'indignation, car elle suppose que l'on procède à un "élevage" conscient de l'homme. Ainsi certains commentateurs (par exemple Barbara Stiegler) estiment qu'à la fin de sa vie consciente, Nietzsche hésita entre un eugénisme actif passant par l'éducation (une sélection sociale et religieuse supposée en toute société), et l'idée contraire que toutes les formes de vie sont nécessaires à l'évolution humaine. B. Stiegler note toutefois que « la sélection nietzschéenne s'est (…) construite dans la critique systématique » de la « sélection naturelle darwinienne » . La conception nietzschéenne de la maladie et de la santé s'oppose en effet au concept darwinien d'une sélection par l'« adaptation », puisque la maladie elle-même peut être bénéfique. D'autres commentateurs ont un avis différent : Gregory Moore écrit qu'il n'y a aucun texte de Nietzsche où il préciserait des mesures de sélection positive. Nietzsche étant avant tout lamarckien, il pouvait en effet penser qu'il suffirait d'imposer une nouvelle moralité qui entraînerait un changement du corps, de la physiologie, et qui deviendrait ainsi héréditaire.

Nietzsche n'a pas voulu lutter contre la "dégénérescence" en tant que telle car elle permet le renouvellement, le foisonnement du vivant. La décadence ne peut jamais être éliminée en tant que telle ; tout être vivant, même fort, possède ses moments de décadence, d'affaiblissement.

En revanche, Nietzsche désigne par le nom de « ratés » les individus qui ne supportent pas de vivre dans ce monde, et qui y diffusent des idées pessimistes, assombrissant ainsi la vie tout entière. Il estime que l'on peut procéder à une sélection par l'établissement de pensées telles que l'Éternel Retour en opposition au "nihilisme" chrétien. Le nihilisme du faible, de celui qui veut se venger de la vie sur les autres, est pour Nietzsche d'autant plus insupportable qu'il est inconséquent parce qu'il porte en lui une logique de destruction totale que l'on peut révéler par la critique violente, notamment de la religion et des idéaux modernes.

Ainsi, en instaurant une nouvelle hiérarchie de valeurs (par l'Éternel Retour), on favorise la disparition des faibles et des ratés (par une réévaluation des instincts, supprimant ainsi la mauvaise conscience et le ressentiment, ou par la revalorisation du suicide auquel on avait donné mauvaise conscience), de la même manière que le christianisme a contribué activement à l'extermination des forts.

Selon Nietzsche, l'individu lui-même est un processus de sélection : « Un homme réussi (…) est un principe de sélection (…). Bien loin d'aller au-devant d'elle, il examine attentivement l'excitation qui lui vient à lui ». Le vivant, tout comme chez Spinoza, doit distinguer ce qui lui est bon et ce qui lui est nuisible.

Nietzsche refuse les institutions du type État, mais sa pensée politique n'en est pas moins, dans certaines limites, hiérarchique et inégalitaire. Selon lui, la préservation des inégalités sociales engendre une mentalité de caste d'où seule peut surgir une culture féconde et élitiste, délivrée des besoins et des nécessités de la vie. Il juge en conséquence qu'une classe d'hommes vivants par l'esprit et pour l'esprit devrait être protégée de la foule des hommes médiocres. Dans les années 1870, le jugement de Nietzsche, influencé notamment par les idées d'une renaissance de l'Allemagne, avait un sens matériel sans équivoque :
L'esclavage fait partie de la civilisation. Toutefois, par la suite, il définira l'esclavage en un sens que l'on trouve chez de nombreux moralistes : l'esclave est celui qui ne dispose pas de temps libre pour cultiver ses facultés. Le mot "esclavage" désigne une intériorisation d'un ancien état de fait plus brutal et concerne l'immense majorité des hommes, les travailleurs utilisant leurs forces physiques comme les professeurs d'université soumis à leur programme.

Or, Nietzsche évoque à partir de là une nouvelle possibilité quand il décrit le fonctionnement naturel des sociétés, et qu'il met en avant la brutalité de leur fonctionnement, la lutte pour la domination et l'exploitation cruelle :
Il estime en effet possible de spiritualiser ces conflits (en leur donnant une forme plus subtile susceptible d'être largement acceptée), de la même manière que la moralité des mœurs avait produit une nouvelle forme d'humanité par des moyens violents, pour se trouver ensuite abolie dans son résultat intériorisé. La démocratisation de l'Europe assure aux yeux de Nietzsche cette possibilité :
La médiocrité est ainsi inévitable et indispensable aux fondements des nouvelles sociétés. Lutter contre elle (par exemple en voulant écraser les faibles au profit des forts, ou en exacerbant les sentiments nationaux) serait une absurdité qui conduirait à la destruction des sociétés :
Sur cette base, la hiérarchie que Nietzsche va concevoir sera une hiérarchie spirituelle, et elle vise à établir des conditions institutionnelles favorables à un type d'hommes que Nietzsche conçoit comme bons, aimables et suprêmement cultivés :

Chacun a des droits suivant la puissance qu'il possède, suivant son rang, mais cette idée ne prend sa forme la plus élaborée que lorsqu'elle atteint un degré de spiritualisation qui porte l'esprit au sommet de la hiérarchie des valeurs. La pensée nietzschéenne de la hiérarchie ne s'oppose donc pas à une protection juridique des personnes, tant que la prééminence intellectuelle n'est pas entravée ou niée. L'égalité des droits, principalement dans le domaine de la culture, serait une négation de tout droit et la source véritable de l'injustice qui conduirait à la dépréciation de la culture.

Nietzsche reprend à son compte la vieille opposition entre l’"otium" et le "negotium", rendant compte du statut différent du travail et du loisir (l’école pour les anciens). Seule le travail de la masse des « médiocres » permet le loisir des « élites » qui peuvent alors se consacrer à la direction de la société. La question étant de savoir si cette distribution des rôles est mutuellement consentie ou au contraire, imposée…

Il ressort de sa politique que les plus forts, qui sont ceux qui vivent par l'esprit et qui ont besoin pour cela d'une société hiérarchisée, ont intérêt à trouver des protections contre le ressentiment qu'ils suscitent, mais qu'ils ont aussi intérêt à "protéger" les plus faibles (qui sont faibles du point de vue de leur esprit), et ceci afin de conserver et de développer les valeurs liées à l'esprit et à l'art, en écartant les causes possibles de ressentiment et de vengeance contre la culture. Mais la difficulté est telle, que Nietzsche a songé plus d'une fois à fonder une société savante à l'écart du monde ; ainsi la hiérarchie dont parle Nietzsche se conçoit tout aussi bien comme une différence de rang dans le domaine de l'esprit qui n'a pas d'influence directe sur le cours des sociétés. Il sera également conduit à concevoir le Surhomme comme un homme vivant à l'écart, et à qui il importe peu de posséder un pouvoir politique effectif (voir plus loin, la section Surhomme).

Cet élitisme, dont Nietzsche voit la forme la plus haute dans une classe d'hommes vivant pour l'esprit, le conduit à placer le philosophe au rang le plus élevé dans le développement de la culture. Cette place fait très tôt l'objet des réflexions de Nietzsche : il avait ainsi eu le projet d'écrire un livre sur le philosophe, alors qu'il était encore professeur, et il nous en reste de nombreux fragments. À cela s'ajoutent des œuvres non publiées (comme "Die Philosophie im tragischen Zeitalter der Griechen" et "Das Verhältnis der Schopenhauerischen Philosophie zu einer deutschen Cultur"), et de nombreux passages des "Considérations inactuelles".

Pour comprendre la place accordée au philosophe par Nietzsche, il faut tout d'abord penser les rapports de la philosophie, de l'art et de la science.

Le philosophe est un type d'homme dont l'instinct dominant est, selon Nietzsche, "un instinct de connaissance sélectif". Il s'oppose en cela, dans certaines limites, à l'intempérance de la science , qui est pour lui une forme de barbarie liée à la démocratie. Sont opposés ainsi, en tant que types, le savant et le philosophe : le premier ne fait pas de distinction dans ce qu'il a à connaître, son activité n'a rien de personnelle ; la caricature extrême de la science est l'érudition, forme de « savoir » qui n'instruit pas mais, au contraire, déforme l'esprit et lui est un fardeau. La masse de ce qui est à connaître est en effet infinie et conduit au désespoir de la connaissance.

Cette opposition se manifeste d'abord dans l'œuvre de Nietzsche par une critique de l'histoire et de la philologie (rappelons qu'il était lui-même professeur de philologie) :
De ce fait, le philosophe est plus proche de l'artiste, dans la mesure où il synthétise ce qu'il connaît, c'est-à-dire produit une simplification de la réalité qui a un caractère esthétique au service de la vie et de la culture.

Nietzsche propose de distinguer deux types de forces, les forces « réactives » et les forces « actives ». Pour lui les forces « réactives » sont la négation du monde sensible et sont représentées par la philosophie classique et la science. Elles opposent le monde intelligible au monde sensible qui en sort dévalorisé. Les forces « actives » ou l'affirmation du sensible s'expriment dans l'art. Les forces « actives » peuvent se déployer dans la vie sans opposer un monde à un autre. L'artiste est celui qui pose des valeurs sans discuter, celui qui invente des mondes nouveaux sans avoir besoin de se justifier.

Le philosophe est selon Nietzsche l'expression de la plus haute volonté de puissance humaine. C'est en tant que tel qu'il est également un législateur. Mais Nietzsche n'a pas une conception volontariste de la politique ; il ne s'agit pas d'imposer par la force un ordre auxquels les hommes devraient se conformer. Ce volontarisme relève généralement du fanatisme moral. Nietzsche soutient tout au contraire que l'influence des idées est telle, qu'elle peut s'étendre et se développer sur des siècles voire sur des millénaires. C'est le cas, par exemple, de la pensée de l'Éternel Retour : ce n'est pas un programme politique que devrait appliquer un parti ou un régime ; il ne s'agit pas non plus d'en faire un critère pour se débarrasser activement des décadents et établir un ordre des forts.

L'idée de Nietzsche est que l'introduction de nouvelles pensées dans le cours de l'histoire est susceptible de le transformer. Ainsi en est-il de la pensée de l'Éternel Retour.

Dans le cadre de cette grande politique, le législateur est un artiste de l'humanité qui sélectionne son matériau en forgeant des valeurs : Nietzsche pense ainsi l'Éternel retour comme un outil d'élevage et de sélection. Il y a donc bien, en ce sens, une forme d'eugénisme, qui doit permettre l'avènement du surhomme. Si Nietzsche évoque incontestablement la perspective d'une destruction des ratés, cette destruction est en réalité une "autodestruction".

Nietzsche écrit ainsi :
— "Antéchrist"

et fait l'éloge d'un suicide activement suggeré :

— "Crépuscule des idoles"

Qui sont en effet, pour Nietzsche, ceux qu'il appelle les « ratés » ? Ce sont ceux qui interprètent moralement le monde, et qui ne peuvent en conséquence supporter d'y vivre (car, nous l'avons vu, le monde et son interprétation morale se contredisent), bref ce sont ceux que ronge le nihilisme. Il n'est donc nul besoin d'une politique agressive (qui serait une forme de décadence), mais seulement d'une sélection des interprétations. Pour le comprendre, prenons un exemple : un homme, dans l'Antiquité, ayant appris la doctrine de Platon, se suicida car il estima qu'il ne fallait pas attendre la mort naturelle pour connaître ce monde "meilleur" décrit par le philosophe. Voilà un nihilisme pratique et conséquent, qui est aussi une forme d'eugénisme par l'influence des idées. C'est pourquoi on trouvera chez Nietzsche un éloge du suicide, et d'une mise en scène consolatrice de la mort "librement" choisie, qu'il oppose à l'horrible mise en scène de la mort et de ses tourments moraux dans le christianisme : il faut se sentir libre de se tuer, car c'est ainsi un service que l'on se rend à soi-même, et que l'on rend également aux autres quand la vie est devenue insupportable. "Ainsi parlait Zarathoustra" mentionne qu'il faut savoir « mourir au plus haut point de son ascension », lorsqu'il est impossible de se surpasser, et en faisant cela, notre image et notre puissance ne seront pas altérées par les années qu'il nous serait resté à vivre (et à se dégrader).

Dès lors, Nietzsche se pose les questions suivantes :

Une réponse possible à la seconde question consiste à se demander s'il est possible de naturaliser l'homme, en extirpant les habitudes idéalistes qui détournèrent les hommes du monde sensible, de la "Terre".

Puisque la morale des faibles a vaincu, faut-il comprendre que par la critique de cette morale et les réflexions sur la culture, Nietzsche aspire à un retour à des formes anciennes de civilisation, avec tous les aspects violents et cruels ? Il faut répondre non ; cependant les temps anciens, pour Nietzsche, sont toujours possibles parce qu'il appartient à la Volonté de puissance humaine de s'exercer avec violence dès lors qu'elle n'est pas l'objet d'une autre force qui la contient et la détourne vers des expressions culturelles plus raffinées. Loin de souhaiter la violence pour elle-même, Nietzsche constate qu'elle est naturelle, et qu'il nous appartient de la cultiver dans un sens ou dans un autre.

Dès lors que la morale du ressentiment s'est imposée, il serait absurde de faire comme si elle n'avait eu aucune influence. "Aurore", où il propose de substituer une morale naturelle à la morale chrétienne, qui conserverait par exemple certaines pratiques de la vertu, mais en leur donnant des buts et des moyens différents. Nietzsche s'attache à définir une naturalisation de l'homme qui passerait par une spiritualisation des pulsions : par exemple, l'abstinence absolue, valorisée par la morale, devient une abstinence relative qui permet de concentrer et d'augmenter les forces intellectuelles. La haine peut être transformée en amour de ses ennemis, si l'on comprend la nécessité naturelle de l'adversité.

D'une manière générale, les anciennes vertus peuvent ainsi être réinterprétées, tout en supprimant les éléments réactifs qu'elles contenaient ou dont elles étaient issues. La perspective de cette réévaluation est celle de la grande santé.

La sélection et l'objectif de la naturalisation de l'homme posent deux problèmes : quelles sont les idées qui auront la plus forte valeur sélective ? Peut-on poser une fin à cette sélection des interprétations ? L'Éternel Retour vient répondre à la première question, le Surhomme à la seconde.

La politique de Nietzsche contient deux notions parmi les plus importantes de sa pensée : l'Éternel Retour comme moyen de sélection, et le Surhomme comme fin idéale.

Nietzsche établit une hiérarchie entre les pensées : les pensées sont plus ou moins sélectives. S'il juge l'Éternel Retour la pensée la plus lourde, c'est parce qu'elle possède la portée éthique discriminante la plus extrême. C'est à ce titre qu'elle fait partie de sa philosophie politique et morale. Mais toute pensée possède une valeur discriminante à des degrés variés, comme le mécanisme qui est plus sélectif que le finalisme car il supprime l'idéalisme.

Il faut commencer par remarquer que l'Éternel Retour se distingue de toutes les anciennes conceptions cycliques (comme exposée dans les textes brahmaniques) : si la loi du karma lie l'existence future d'un être à son existence passée (la réincarnation sert à réparer les erreurs d'une existence passée), Nietzsche nie toute dette et toute faute, et conçoit le devenir cyclique par delà bien et mal.

Cette hypothèse éthique et cosmologique que l'on trouve déjà chez Héraclite et les Stoïciens, peut être déduite du concept de volonté de puissance en admettant certains axiomes ; Nietzsche s'est en effet efforcé de montrer le caractère plausible de son hypothèse :

Pour Nietzsche la validité scientifique de cette hypothèse cosmologique n'a aucune importance, toute pensée - métaphysique comme scientifique - est interprétation du monde : il n'existe pas de fait objectif, de vérité ou de sens absolus, indépendamment du sujet. La valeur d'une représentation ne se mesure donc pas à son adéquation au prétendu réel mais à sa capacité à favoriser le développement de la puissance en tant que vie, à sa sélectivité, à son intérêt en tant que "réalité éthique", interprétation normative, supérieure ou inférieure. Nietzsche sait que sa cosmologie est probablement erronée, cela ne va pas en contradiction avec sa pensée vantant les mérites de « l'erreur » ; « si la doctrine de l'éternel retour est valorisée en tant querreur", cela signifie que sa valeur ne dépend pas de sa scientificité ».

La valeur de la doctrine de l'Éternel Retour vient non de ses fondements mais de ses implications : « Si le devenir est un vaste cycle, tout est également précieux, éternel, nécessaire. » L'aspect scientifique de cette doctrine est une « plus-value », non une garantie supplémentaire de sa validité, mais une raison supplémentaire pour y croire. Nietzsche la fournit pour favoriser l'adhésion à cette doctrine dans une époque qu'il sait positiviste.

Le nihilisme, dans cette pensée, est un état normal, et non seulement un symptôme de faiblesse face à l'absurdité de l'existence. Face à L'Éternel Retour, pensée sélective par ce nihilisme extrême, deux attitudes peuvent être adoptées, comme l'indique le "Gai Savoir". Lorsque celui ayant dit non à la vie pense l'Éternel Retour, sa résignation est renforcée, il est effaré à la perspective que ce qu'il fuit dans les consolations métaphysiques et autres arrières mondes l'affligera éternellement ; lorsque l'Éternel Retour est pensé par celui ayant dit oui à la vie, son acceptation de la vie est renforcée, sa volonté de puissance est alors maximale. Advient ainsi le Surhomme, qui accepte et aime la réalité telle qu'elle est, là où l'idéaliste la fuit en l'aimant telle qu'elle devrait ou aurait pu être.

L'Éternel Retour doit conduire à « ne plus "désirer autre chose" » : c'est l"'Amor fati" qui nous délivre du ressentiment. La philosophie de Nietzsche se fonde sur cette métamorphose du désir qui induit une transformation des valeurs nécessitée par la constatation que « l'essence la plus intime de l'être est la volonté de puissance ». Cette pensée doit permettre de dépasser l'homme, non pas de l'éliminer, et d'abandonner les anciennes idoles par lesquelles l'homme "espérait" un autre monde et "désirait autre chose". La transvaluation consiste à penser par-delà bien et mal, alors que tous les philosophes antérieurs pensaient dans les limites de la morale idéaliste.

La notion de "Surhomme" (qui apparaît peu dans les textes à part dans "Ainsi parlait Zarathoustra") esquisse ce que deviendrait l'homme, en étant délivré du ressentiment de la morale et en incarnant l'affirmation la plus intense de la vie, l'Éternel Retour. Le préfixe « sur- », abondamment utilisé par Nietzsche pour désigner un processus de transfiguration, de modification de la structure des instincts (l'homme est ainsi un "sur-animal"), signifie cette transformation de l'être humain ; il s'agit moins d'un accroissement ontologique que d'une manière de percevoir et de juger le monde. Il n’est toutefois pas un au-delà de l’homme et reste humain, trop humain, n’étant pas un nouvel « en soi » idéalisé servant de modèle.

Contrairement à ce que l'on croit souvent, le "Surhomme" n'est pas un homme "surpuissant", physiquement ou intellectuellement :

C'est une évolution possible et souhaitée de l'homme : . L'action de l'homme n'est plus détournée par une pensée et une morale théologique ou métaphysique (…) mais par le consentement de son "éternel retour".

Il faut tout d'abord noter qu'il y a une difficulté dans la traduction de l'expression allemande qui a été rendue de plusieurs manières en français :

On trouve dans l'expression allemande deux fois le radical "Wert-" ; le préfixe "Um-" signifie un retour, un contournement. L'expression pourrait alors être traduite par "réévaluation de toutes les valeurs."

En quoi consiste l'inversion des valeurs ? Nietzsche n'en fait aucun exposé complet, toujours resté à l'état de projet :

Fondamentalement, Nietzsche énonce trois réquisits essentiels qui permettent de déterminer l'expression de réévaluation des valeurs (cf. "Le Crépuscule des idoles", « Les quatre grandes erreurs ») :

Nietzsche est aussi celui qui entend historiquement maudire le christianisme. Et, s'il situe ses « sentences antichrétiennes » en fin de son ouvrage Antéchrist, c'est que, logiquement, tel était le but de cet écrit. Certes, on pourrait, pour sa défense, dire que ces « lois » auraient pu être prononcées par l'Antéchrist censé paraître un jour, et qu'il les utiliserait, comme « avant le temps », pour donner plus de poids à son argumentation. Mais il suffit de se reporter à ce qu'il écrit à Malvida von Meysenburg pour se convaincre que ces déclarations sont de son propre chef et qu'il les assume : (lettre à Malvida von Meysenburg, 4 avril 1883). Il prend ses responsabilités et se renvendique donc lui-même comme l'Antéchrist, en donnant son programme de destruction : susciter les « mauvais instincts ». Nietzsche n'est pas réellement devenu fou, au sens psychiatrique du terme ; mais il va seulement jusqu'au bout de son raisonnement et de son programme de destruction de l'âme. Si nous suivons le fil de sa pensée, la philosophie moderne serait alors une philosophie de destruction des repères sociaux par l'inversion des valeurs les plus fondamentales, ce qu'il appelle, selon sa terminologie personnelle : le règne de l'Antéchrist.

L'art est à la fois premier (interpréter, connaître, c'est faire œuvre d'artiste) et dernier (le surhomme est un embellissement des pulsions humaines). L'art est l'expression d'une pulsion humaine primitive, celle de créer des formes.
Il n'est donc pas surprenant qu'il soit pour Nietzsche le seul facteur justifiant la vie.

La première publication de Nietzsche concernant sa pensée de l'art est "La Naissance de la tragédie". Dans cette œuvre, il oppose et associe les figures opposées de l'ivresse : dionysiaque et apollinienne. Dionysos est une figure qui sera reprise par Nietzsche tout au long de son œuvre, qu'il n'abandonnera jamais, et même plus : peu à peu l'on voit que Dionysos représente l'ensemble des thèmes importants chez Nietzsche, comme la fête, le rire, l'ivresse, la volonté de puissance, et l'acquiescement, l'affirmation de tout ce qu'est la vie. Il peut se substituer à Zarathoustra ou à Nietzsche lui-même : (Dans cet extrait Nietzsche se cite lui-même en reprenant des paroles de Zarathoustra, dont il dit qu'il est l'idée même de Dionysos :"Ecce Homo", Ainsi parlait Zarathoustra, 6) Dans "La naissance de la tragédie" Dionysos est du côté de la musique, et Apollon du côté des arts plastiques (mouvement contre rigidité).

Dionysos est donc l'ivresse de l’instinct, la jouissance primitive de l’absence de raison contrôlant les actes, l’innocence de la liberté et de l'émotion :
La seconde figure, Apollon, est l’œuvre de la raison qui tente de masquer la nature par la culture, en inventant des normes, des symétries, afin de célébrer l’idée du beau par une transformation esthétique des actes et du monde, plaisante à la vision.

Ces deux premières figures ont des expressions esthétiques qui leur sont propres :

L'une des premières formes d'art à laquelle Nietzsche se soit intéressé (dans "La Naissance de la tragédie") est la tragédie, qui réunit l'apollinien et le dionysiaque.

La tragédie grecque est pour Nietzsche l'expression d'un aspect essentiel de la culture grecque : le pessimisme de la force. À ce titre, elle témoigne d'une culture réussie jusqu'à un certain point, ce dont témoignent en particulier les philosophes Présocratiques.

La tragédie naît selon Nietzsche de l'orgiasme dionysiaque : "extériorisations incompréhensibles des pulsions populaires". Les hommes sont en extase ; ils se sentent ensorcelés par le dieu.

La tragédie antique est l'accouplement de deux impulsions symbolisées par des dieux (Apollon et Dionysos) qui se combattent sans cesse.
Ces deux dieux s'expriment primitivement comme des forces de la nature qui se passent du travail de l'artiste. Elles jaillissent au sein du rêve et du délire. L'opposition de ces forces ne doit pas être exagérée : elles produisent des effets bien différents, mais possèdent quelques points communs. Dans les dernières œuvres de Nietzsche, ces forces semblent même être absorbées dans le seul élément dionysiaque, au point que certains commentateurs ont pu soutenir que le dionysiaque était l'élément originel dont l'apollinien est seulement dérivé.

Apollon est le dieu brillant, prophète, qui représente les arts plastiques, le rêve, la belle apparence, le plaisir des formes. Cette beauté de l'apparence n'exclut pas la représentation de sentiments déplaisants. Mais le caractère esthétique qui s'en dégage embellit la vie, et encourage les hommes à vivre. C'est là pour Nietzsche son aspect nécessaire : sans Apollon, la vie ne serait pas digne d'être vécue.

L'esthétique d'Apollon est la mesure, le calme de la sagesse, la grâce. Au milieu des tempêtes de l'existence, l'aspect solaire et paisible d'Apollon est "sublime".

Dionysos est l'ivresse, ivresse des narcotiques, du printemps qui abolit la subjectivité des "fous de Dionysos". Dionysos est la volupté de la nature spontanément surabondante. Le principe dionysiaque dissout l'individualité et permet à l'homme de renouer avec la nature et l'humanité : c'est le "mystère de l'Un originaire" qui ensorcelle tous les êtres et les font danser tous ensemble. L'homme devient l'œuvre d'art d'un dieu.

La tragédie est morte tragiquement ; "son agonie a nom Euripide". Celui-ci a en commun avec les poètes de la nouvelle comédie, de faire entrer le spectacle de la vie quotidienne sur la scène. Alors que les anciennes tragédies représentaient les héros dont l'idéalisation élève l'âme du spectateur, la tragédie d'Euripide représente le commun, le bas, elle est un miroir rhétorique de la vie des spectateurs qui s'y contemplent. Ainsi Euripide a-t-il popularisé la tragédie, en faisant parler le peuple : 
Il croyait ainsi lutter contre la décadence de la tragédie, qui, selon Nietzsche, était en réalité déjà morte. Fort de cette croyance, il crût que l'effet de l'art n'était pas adapté au public athénien. Il conçut alors une forme d'art, comme la loi d'une esthétique rationaliste : "Tout doit être de l'ordre de l'entendement pour que tout puisse être entendu." Euripide envisage ainsi de manière critique toutes les parties de l'art : le mythe, la structure dramatique, la musique, la langue, etc.

Par exemple, Euripide dévoile toute l'intrigue dans le prologue de ses pièces, contrairement à Eschyle et Sophocle, qui, dans les premières scènes, font subtilement comprendre aux spectateurs ce qui doit se produire.

Ainsi Euripide est-il le premier dramaturge à concevoir une esthétique consciente : , principe qui le fait proche de Socrate. La décadence de la tragédie s'exprime dans les pièces d'Euripide, ami de Socrate, dont on rapporte qu'il aida le dramaturge pour la composition de ses œuvres. Or "Socrate fut, dans la tragédie, et dans le drame musical en général, l'élément de sa dissolution." Socrate est selon Nietzsche un personnage anti-tragique.

Nietzsche discerne plusieurs traits de l'évolution de la tragédie qui en montrent la décadence : 


Bien sûr tous ces traits n'en sont en fait qu'un seul : la tragédie n'est plus dionysiaque car la dialectique monte sur scène à sa place, la musique (qui est dionysiaque) n'est donc plus le moteur de la tragédie (qui n'est en fait plus qu'une comédie bourgeoise).

Richard Wagner est étudié par Nietzsche (dans "Le cas Wagner" en particulier) comme un cas typique de la modernité ; la compréhension du compositeur permet de faire la lumière sur la psychologie de l'homme moderne. Wagner représente ainsi, un cas typique de romantisme qui finit dans l'adulation de la croix ("Parsifal"). Nietzsche s'oppose à Wagner avec son romantisme. Il propose une alternative à ce romantisme, que l'on peut nommer classicisme, ce qui ne signifie pas classique dans le temps mais classique dans le mode et la manière. Cette conception nietzschéenne, on la trouve dans la naissance de la tragédie où il oppose deux façons : dionysiaque et apollinienne. La première est réincarnation de la vie et de la force, c'est la guerre et les tragédies dramatiques. La deuxième, c'est l'incarnation des dieux olympiens et un monde de perfection et de paix. La première est classique et la deuxième romantique. Wagner pour Nietzsche fait partie de la deuxième catégorie qui refuse la vie telle quelle, en cherchant des valeurs inexistantes.

Nietzsche n'a pas, à proprement parler, d'esthétique ; ses thèses sur l'origine de l'art sont essentiellement d'ordre physiologique :

Également selon Nietzsche, l'art apparaît comme « un produit vital d'illusions ».

L'art naît d'un sentiment d'ivresse, d'une excitation communicative. Ces états physiologiques et psychiques n'ont pas de liens nécessaires avec le beau. Cela n'empêche pas de faire une physiologie de la beauté et de la laideur :

D'une manière générale, Nietzsche prône l'affirmation de la vie, une affirmation totale et joyeuse de la vie (c'est-à-dire une affirmation du plaisir et de la souffrance), même dans tout ce qu'elle a de problématique et d'inquiétant, jusque dans ses recoins les plus dangereux.

Par art, il ne faut pas entendre seulement les œuvres d'art, mais, d'une manière générale, ce qui, en l'homme, tend à créer des formes, et à préférer la jouissance de la superficie et de l'illusion. En ce sens, l'art s'oppose à la science, et, dans une moindre mesure, à la philosophie, bien que ces deux dernières activités possèdent également une dimension esthétique. Pour comprendre la force affirmative de l'art, il faut comprendre que notre vie, dans les moindres de ses aspects, tient plus de l'illusion, du rêve et du mensonge, que de la « vérité » :

Nietzsche était un excellent pianiste et improvisateur, auprès de son entourage d'époque. Il écrivit une quarantaine de pièces musicales, pour l'essentiel entre treize et vingt et un ans, de 1857 à 1865, dans lesquelles l'apôtre de la philosophie à coups de marteau semble exprimer une indécision, un romantisme timoré. Son "Miserere" (juillet 1860), sans être grandiose, est efficace, avec une belle ferveur vocale. "Eine Silvesternacht" (décembre 1863 - janvier 1864) pour piano et violon s'avère intéressant, mais souffre d'une structure trop lâche. "Nachklang einer Silvesternacht" (novembre 1871) pour piano paraît proche d'un certain romantisme allemand, mais avec une faible détermination, loin du déchaînement d'un Franz Liszt, recelant un climat nocturne sans l'étrangeté d'un Robert Schumann. Si "Das zerbrochene Ringlein", un saisissant "lied" de janvier 1863, expressionniste avant l'heure, reste sans doute le sommet de l'œuvre musicale de Nietzsche : il est sans doute transcendé par la version en "Sprechgesang" (le chanté parlé) de Dietrich Fischer-Dieskau son meilleur interprète, accompagné au piano par le compositeur Aribert Reimann.

En 1882, à trente-huit ans, Nietzsche composa un "lied" sur un poème de Lou Andreas-Salomé, "Gebet an das Leben" (Hymne à la vie), et écrivit à Felix Mottl :
Malheureusement, même le duo de haut niveau Fischer-Dieskau / Reimann peine à transfigurer cette plainte. Sa transcription pour chant et orchestre, arrangé par son ami Heinrich Köselitz en 1887, "Hymnus an das Leben", fait penser à une musique de film américain moraliste des années 1950, loin de l'essence du dionysiaque nietzschéen. "Manfred Meditation" (avril 1872) pour piano à quatre mains souffre encore une fois de l'absence de nécessité interne, d'une indétermination si étrangère à la philosophie de Nietzsche. Il l'envoya imprudemment à Hans Guido von Bülow, alors célèbre chef d'orchestre, qui lui répondit : "« Votre méditation, du point de vue musical, n'a d'autre valeur que celle d'un crime dans l'ordre moral. »" Elle lui évoque "« un lendemain de bacchanale, plutôt que la bacchanale elle-même »".

L'échec de Nietzsche compositeur, trop erratique, a alors ouvert la voie au Nietzsche philosophe, qui est sorti du romantisme pour forger son concept de l'éternel retour.
Écrit-il, tandis qu'il s'affranchit de Richard Wagner qui fut son grand modèle avec Arthur Schopenhauer avant de les condamner tous les deux comme forces extinctives de la vie. Nietzsche libéré de son surmoi de compositeur fit alors l'apologie de Gioachino Rossini, Vincenzo Bellini et, ironiquement, de Georges Bizet, "« Carmen me délivre »", pour se moquer de Wagner. Désormais, "« il faut méditerraniser la musique »", entreprise qui prit fin dans une rue de Turin en 1889, qui vit Nietzsche s’effondrer, aphasique devant un cheval sévèrement battu par son cocher.

Nietzsche eut une très grande influence au , influence qui toucha principalement des artistes et la philosophie continentale. À la fin de sa vie, et au début du , sa pensée fut diffusée en Scandinavie (Brandes, Strindberg), en France (traduction de Henri Albert), Italie, Pologne, Russie et en Angleterre. Sa conception de l'homme animal déterminé par l'économie de ses instincts influença également Freud.

Dans les années 1930, les œuvres de Nietzsche furent récupérées par les nazis et les fascistes italiens. C'est ensuite à partir des années 1960 que Nietzsche devint une référence pour de nombreux intellectuels français, en réaction notamment à l'hégélianisme dominant.

À partir de l'édition Colli-Montinari, tous les commentateurs purent accéder aux carnets de Nietzsche, au lieu de recourir à des éditions de fragments posthumes qui ne respectaient pas l'ordre chronologique, et qui se présentaient parfois comme l'œuvre inachevée de Nietzsche qu'il n'aurait pas eu le temps de terminer. Ces éditions, fautives et non scientifiques par leur caractère sélectif, se sont révélées être des mystifications, puisqu'il est établi depuis les années 1930 que Nietzsche avait abandonné l'idée d'écrire une somme de son « système » (voir "La Volonté de puissance").

À la fin du , Nietzsche est relu comme un naturaliste par des commentateurs de tradition analytique.

Les textes de Nietzsche ont subi de nombreuses manipulations, et ont été utilisés de manières fort diverses avant d'être édités de façon plus exacte et complète par Giorgio Colli et Mazzino Montinari.

Nietzsche cite peu les auteurs qui l'inspirent ou auxquels il s'oppose, et la recherche des lectures qui ont pu avoir une influence sur sa pensée est un domaine à part entière des études nietzschéennes. Pour certains commentateurs (comme Mazzino Montinari ou Barbara Stiegler, dans "Nietzsche et la biologie"), il est difficile de comprendre toute l'importance des thèses de Nietzsche, si l'on ignore de quoi s'est nourrie sa philosophie et dans quel contexte intellectuel elle prend place.

Nietzsche avait une intense activité de lecture et connaissait, directement ou indirectement, les auteurs, penseurs, scientifiques et artistes majeurs de son temps. Ses lectures sont très étendues et il faisait lui-même remarquer dans une lettre à Jacob Burckhardt, à l'occasion de la parution de "Par-delà bien et mal", qu'une vaste culture était nécessaire pour saisir et juger la valeur de cette œuvre.

La bibliothèque de Nietzsche, dont un premier catalogue a été établi dès 1896 par Rudolf Steiner, reflète cet appétit de lectures. On peut citer pour exemples quelques-uns des auteurs qu’il lut dans sa jeunesse : Goethe, Adalbert Stifter, Ludwig Feuerbach, David Strauss, Ralph Waldo Emerson (les "Essais", dont "La Confiance en soi" dont on retrouve des influences dans "Schopenhauer éducateur"), Lord Byron ("Manfred"), Hölderlin, Schopenhauer ("Le Monde comme Volonté et comme Représentation").



L'édition qui fait actuellement référence (et qui contient les fragments posthumes, notamment ceux destinés à la "Volonté de puissance") :

Friedrich Nietzsche, "Correspondance", textes établis par Giorgio Colli et Mazzino Montinari, traduction d'Henri-Alexis Baatsch, Jean Bréjoux et Maurice de Gandillac, placée sous la responsabilité de Maurice de Gandillac, Gallimard, en cinq tomes, dont trois parus :


La bibliothèque de Weimar, où se trouvent les archives Nietzsche, fournit un index bibliographique général. Cet index permet de consulter des listes bibliographiques par thèmes, par exemple : art, éternel retour, métaphysique, volonté de puissance.



Les liens sont réduits à l'essentiel (textes et articles de commentateurs).



</doc>
<doc id="15043" url="https://fr.wikipedia.org/wiki?curid=15043" title="Protectionnisme">
Protectionnisme

Le protectionnisme est une politique économique interventionniste menée par un État ou un groupe d'États, consistant à protéger ses producteurs contre la concurrence des producteurs étrangers. Les buts peuvent être le maintien de l'emploi dans certains secteurs d'activité, la diminution du déficit commercial, ou la défense du niveau de vie. Les mesures protectionnistes consistent essentiellement à freiner les importations (barrières douanières, normes contraignantes, freins administratifs...), encourager les exportations (subventions diverses, incitations fiscales, dévaluation, dumping comme le « protectionnisme offensif »), privilégier les entreprises nationales dans les appels d'offres de marchés publics, ou empêcher les investisseurs étrangers de prendre le contrôle d'entreprises nationales.

Des Institutions internationales comme le GATT puis l'OMC ont été créées pour abaisser les barrières protectionnistes et en limiter autant que possible l'usage.

Les activités économiques protégées peuvent s'abstraire au moins en partie des pressions et contraintes de toutes natures en provenance du contexte concurrentiel. Elles bénéficient de ce fait d'une plus grande liberté de manœuvre et d'une plus grande certitude concernant leur rentabilité et développement futur . 
D'après l’économiste coréen Ha-joon Chang, le protectionnisme a fait la démonstration de son efficacité dans l'histoire. Pendant les années 1960 et 1970, quand il existait bien davantage de protections et autres régulations, la croissance du revenu par tête dans les pays développés croissait d’environ 3 % par an, contre 2,3 % au cours des décennies 1980 et 1990. En Amérique latine, cette croissance est devenue pratiquement nulle : 0,6 %, contre 3,1 % de 1960 à 1980. Au Proche-Orient et en Afrique du Nord, elle est tombée de 2,5 % à - 0,2 %, et en Afrique subsaharienne de 2 % à - 0,7 %.

Selon le journaliste économique français Éric Le Boucher, « le protectionnisme n'est pas une solution contre la chute des emplois industriels », et notant qu'il a fait perdre 2 millions d'emplois en France dans les trente dernières années. Contrairement au régime de libre-échange, Il fait baisser la variété des produits offerts à la consommation et empêche le pays d'acquérir son importance commerciale selon le principe d'avantage comparatif, car les agents économiques ne sont pas incités -en l'absence de pression concurrentielle- à développer au maximum leurs points forts de production.

Selon l'économiste Daniel Cohen, « le protectionnisme aggraverait la crise » et constitue un danger dans la mesure ou si un pays A met en place un protectionnisme à l'import sur un bien d'un pays B, ce pays B peut en retour mettre en place un protectionnisme sur un bien provenant du pays A. Le protectionnisme induit alors un cercle vicieux. L'exemple du bœuf aux hormones américain et du fromage européen illustre ce mécanisme. En effet, depuis 1989, l'Union européenne a mis en place un protectionnisme sur le bœuf aux hormones d'origine américaine, en retour les États-Unis ont mis en place un protectionnisme sur les fromages européens.

Lorsqu'un pays considère qu'une de ses productions est gravement menacée, la mise en œuvre de la « clause de sauvegarde » vise à limiter les importations (soit en les interdisant, soit en les taxant fortement) durant une période donnée. Exemple : Fin 2008, l'Inde a décidé de taxer fortement les importations d'acier

Dans l'histoire de la pensée économique, le libre-échange a longtemps été la règle, et le protectionnisme était perçu comme étant une anomalie nuisible au bon développement de l'économie. Mais à partir du , les pensées économiques vont justifier la légitimité du protectionnisme.

Selon les auteurs mercantilistes, la richesse d'un pays dépend positivement du stock de métaux précieux qu'il possède. La quête de ces ressources, provenant des nouveaux territoires découverts, est comparable à une véritable guerre. Le commerce est considéré comme un jeu à somme nulle : celui qui importe gagne, et celui qui exporte perd. Les États vont donc mettre en place des mesures afin à la fois de capter le maximum de ressources minières provenant du nouveau monde, afin d'en exporter le minimum, car si l'État exporte ses richesses, il les perd même contre toute autre compensation. L'État interdit l'exportation de monnaie du pays ainsi que les métaux précieux (or, argent...), et essaye de faciliter au maximum ses importations (par l'intégration des marchés nationaux par exemple). En France par exemple, l'État va même organiser la production nationale (avec les manufactures de Colbert par exemple).

Dans son ouvrage maître, Adam Smith justifie le libre-échange, en développant l'idée que, contrairement à ce qu'affirmaient les mercantilistes, le commerce est synonyme de paix et d'enrichissement mutuel. Toutefois, Smith n'est pas contre l'idée d'instaurer des droits de douane, pour deux cas bien spécifiques : en cas de présence d'industries stratégiques pour la défense nationale et en réaction à des taxations opérées par des pays sur les exportations nationales. 
Le protectionnisme est donc selon Smith une mesure exceptionnelle, mais qui, en règle générale, nuit au bon fonctionnement de l'économie.

Selon l'historien économiste Paul Bairoch, avant les années 1840, : le décollage industriel de la Grande-Bretagne et de la France au début du bénéficie de fortes barrières douanières, le Zollverein de la Prusse est une union douanière allemande. 

D'après l'historien de l’économie Charles Kindleberger, l’abrogation en Grande-Bretagne des lois sur le blé dans les années 1940 était motivée par un « impérialisme libre-échangiste » destiné à « stopper les progrès de l’industrialisation du continent en y élargissant le marché des produits agricoles et des matières premières. C’est aussi l’argumentation soutenue à l’époque les principaux porte-parole de l’Anti-Corn Law League. Pour l’économiste allemand Friedrich List, « les prêches britanniques en faveur du libre-échange faisaient penser à celui qui, parvenu au sommet d’un édifice, renvoie l’échelle à terre d’un coup de pied afin d’empêcher les autres de le rejoindre ».

Depuis la fin des années 1940, l'extension du libre-échange n'a pas totalement éliminé les pratiques protectionnistes.

Le protectionnisme peut recourir à plusieurs mesures ; on distingue les mesures tarifaires des mesures non tarifaires.

Imposer des droits de douane consiste à taxer les produits importés afin d'augmenter leur prix, et ainsi de diminuer la quantité achetée par les consommateurs.Exemples :

Il s'agit d'alourdir les procédures administratives pour les importations (obligation de remplir des documents administratifs compliqués, longue période de blocage en douane, etc.).Exemple : En 1982, la France a mis en place ce système pour réduire les importations de magnétoscopes en provenance du Japon. Ces derniers devaient être dédouanés à Poitiers.

Ces normes correspondent à un cahier des charges (types de traitements autorisés -ou obligatoires- pour les produits agricoles, etc.) qu'un produit doit remplir pour pouvoir être vendu dans un pays. Exemple : L'Union européenne interdit les importations de bœuf aux hormones.

Les statuts des professions "protégées" et autres réglementations / normes faisant barrière à l'accès à ces activités, présentées comme apportant des garanties de compétence et rigueur aux utilisateurs, sont des domaines où le corporatisme tend à rejoindre le protectionnisme.

Dans le même ordre d'idées se situe la protection des monopoles de certaines entreprises et institutions publiques.

Ils visent à limiter la quantité de produits importés.Exemples :


Les autorités d'un pays peuvent chercher à protéger certaines activités (considérées comme stratégiques) contre les prises de participation par des investisseurs étrangers.Exemples :

Une monnaie se dévalue, ou subit une dévaluation, lorsque son taux de change se déprécie par rapport à une monnaie de référence, ou un panier de monnaies. Un gouvernement peut intervenir sur le marché des changes en « vendant de la monnaie » pour abaisser la valeur de sa devise. Cela rend les produits moins chers à l'exportation, mais diminue le pouvoir d'achat en augmentant le prix des produits importés (Condition de Marshall-Lerner).

Un pays peut instituer dans les politiques de passation des marchés publics une préférence pour les produits fabriqués localement (ou pour les services des entreprises locales).

Exemple : Les États-Unis ont instauré une clause « acheter américain » (Buy American Act) pour leurs marchés publics.

Elles consistent à accorder des facilités financières aux clients (crédits bonifiés, déductions d'impôts, etc.).Exemple : Début 2009, les autorités françaises ont décidé de débloquer 5 milliards d'euros pour les futurs acheteurs d'Airbus.

Exemple : Le gouvernement fédéral du Canada a instauré en janvier 2009 un plan qui permet aux acheteurs d’une première résidence de déduire jusqu’à de coûts (tant pour 
les frais juridiques que de mutation) de leurs revenus imposables pour l’année d’achat.

Elles visent à donner un avantage (soit sous forme de prêts bonifiés, soit sous forme de dons) aux producteurs nationaux.Exemple : En 2008 et 2009, les États-Unis et la France ont accordé des aides à leurs constructeurs automobiles.

Exemple : Depuis 2007, les agriculteurs québécois ont reçu environ 1,1 milliard de
dollars par année des différents programmes gouvernementaux fédéraux et
provinciaux.

Selon l'ancien député Bernard Carayon, qui a lancé la politique publique d'intelligence économique en France en 2005, l'Union européenne ne pratique presque pas le protectionnisme en matière de marchés publics : pour ce type de marchés (1000 milliards d'euros par an), le taux d'ouverture européen est de 90 %, alors qu'il n'est que de 32 % aux États-Unis, de 28 % au Japon, et de 0 % dans les « pays émergés ».

Depuis les années 1930, les États-Unis adoptent une politique systématique consistant à interdire dans les marchés publics les produits qui ne sont pas fabriqués aux États-Unis. Il existe aussi des subventions. Les lois sont :

Voir : Passation des marchés de l'administration américaine : aide-mémoire des liens relatifs aux obstacles les plus fréquents

En 1989, le groupe Bull a acheté le constructeur de micro-ordinateurs Zenith Data Systems, dans l'espoir d'acquérir le marché des micro-ordinateurs de l'administration américaine, méconnaissant totalement la législation américaine sur les achats publics. Le gouvernement fédéral américain a évidemment répliqué en faisant appel à un autre fournisseur. Cette erreur stratégique a entraîné de lourdes pertes financières pour Bull, qui ont dû être négociées par Bernard Pache auprès de l'Union européenne. Les subventions sont aujourd'hui interdites par l'Union européenne.

Les États-Unis se sont opposés au développement du supersonique Concorde en appliquant des normes sur le bruit (il est vrai que le Concorde était un avion très bruyant).

Depuis la fin des années 1980, les États-Unis ont élargi cette politique à des actions plus offensives de soutien cohérent des entreprises américaines à l'exportation. Cette politique est appelée « "advocacy policy" ». Elle s'appuie sur une organisation spécialisée, l'« "advocacy center" », et sur l'utilisation de techniques informatiques sophistiquées, en "Network Centric" (réseau centré) .

Dans la vision des stratèges américains, le monde se répartit en trois zones : les États-Unis conçoivent, l'Asie produit, et l'Europe consomme.

Aujourd'hui les États-Unis cherchent à imposer des normes internationales dans le domaine des technologies de l'information.

Par exemple, la spécification technique ebXML tend à s'imposer dans le monde comme un standard de commerce électronique.

La section 301 de la loi américaine générale de 1988 sur le commerce et la compétitivité permet à l'Administration américaine de prendre dans des délais très brefs toute mesure de rétorsion à l'égard des partenaires commerciaux dont les pratiques seraient jugées déloyales.

En 2000, le président George W. Bush a mis en place des mesures protectionnistes sur les importations d'acier pour satisfaire les demandes des grandes entreprises du secteur dont la productivité était insuffisante. Les effets "a posteriori" semblent avoir été négatifs puisque, si les mesures ont sauvé , elles en ont détruit entre et chez les entreprises qui consomment de l'acier.

L'un des tarifs douaniers les plus élevés du monde est celui que pratique le Japon sur le riz étranger, taxé à 800 %.

La politique agricole commune a longtemps consisté en versement de subventions agricoles. Cette politique a favorisé l'agriculture intensive, ce qui a eu des conséquences dommageables sur le plan du développement durable.

En France, on invoque quelquefois l'exception culturelle.

Les relations économiques entre l'Union européenne et les États-Unis ont fait l'objet d'un rapport d'information à l'Assemblée nationale en France en 1999.

Voir : Rapport d'information déposé par la délégation de l'Assemblée nationale pour l'Union européenne sur les relations économiques entre l'Union européenne et les États-Unis.

La France n'est pas un pays protectionniste à part entière. Les douanes sont présentes et contrôlent les importations par voie d'eau et d'air, mais ce n'est pas un système extrémiste.

Mais, selon un sondage Ifop, 65 % des français sont pour un système plus avancé du protectionnisme national.

Chez les mercantilistes le rôle du commerce extérieur est de permettre le gain monétaire, c'est-à-dire l'afflux d'or. Dans cette optique, les mercantilistes préconisent une politique volontariste de soutien aux exportations "via" la création de grandes compagnies de commerce ou de grandes manufactures. Au contraire l'État doit tenter de freiner les importations qui sont synonymes de sorties d'or.

Pour Jean-Baptiste Colbert, « "les compagnies de commerce sont les armées du roi, et les manufactures sont ses réserves" ». L’objectif de ses « armées » est de repousser les « armées » étrangères. Ainsi pour souligner cette haine du commerce étranger, Antoine de Montchrestien déclare :

La logique mercantiliste repose sur l'idée que la richesse n'est fondée que sur le volume détenu de métaux précieux, et que dès lors, ce volume étant défini, le commerce est un jeu à somme nulle. L'enrichissement d'un État par ses exportations ne peut se faire que par l'appauvrissement d'un autre par ses importations.

D'autres tel Friedrich List, considèrent le protectionnisme comme nécessaire à court terme pour amorcer le développement d'une économie. Le libre-échange ne serait alors juste qu'entre pays de puissance économique comparable. Un pays, ayant une fois rattrapé le niveau des autres, pourra passer à un système de libre-échange qui reste l'objectif de long terme. Il explique :
Ce point de vue fonde son analyse sur plusieurs arguments en faveur de l'insertion internationale dont la protection est bénéfique pour les industries naissantes.

En effet, les industries dans l'enfance (industries naissantes) ne sont pas adaptées au marché international (accoutumance de la main-d’œuvre, niveau de production optimal, tarification optimale…). Pour cela, elles bénéficient d'un « temps d'adaptation » qui vont leur permettre de développer leur compétitivité c’est-à-dire de passer d'un avantage comparatif potentiel à un avantage comparatif réel (au sens de David Ricardo). Les industries naissantes vont donc se protéger de la concurrence internationale afin de développer un système productif en corrélation avec le marché mondial compte tenu de la contrainte de prix et de production extérieure. Pour que la transition soit efficace plusieurs conditions doivent être réunies : le passage d'un avantage comparatif potentiel à un avantage comparatif réel doit être réalisé, la protection doit être temporaire et l'ouverture à la concurrence doit être réalisée au moment opportun c’est-à-dire quand la firme devient compétitive (quand le prix des biens qu’elle fournit sont supérieurs à ses coûts de production =bénéfices). Cette position est critiquée par les libéraux .

Ceci constitue un des arguments au niveau national. D'autres arguments comme celui de l'industrie déclinante, du revenu, de l'emploi ou encore des distorsions internes expliquent la pensée mercantiliste en matière de protectionnisme.

Le protectionnisme a pour effet de diminuer le déplacement des biens dans son ensemble ce qui réduit l'empreinte écologique du transport.

Le concept "libéral" doit être analysé en deux branches :

Le premier a été le thème d'une déclaration du Parlement britannique en 1820 qui conduisit, en 1846, alors que les conservateurs ("Tories") étaient majoritaires, à l'abrogation du "Corn Law" (la loi sur le blé) qui protégeait les agriculteurs britanniques. Cette abrogation entraina la ruine de l'agriculture britannique, la baisse violente des prix des denrées alimentaires, la baisse des salaires et l'enrichissement de la "City", mais aussi un sous-investissements dans la recherche et la modernisation de l'industrie (F. William Engdahl, "Pétrole une guerre d'un siècle", Jean-Cyrille Godefroy, France, 2007, 17).

À la même époque, le "protectionnisme éducatif" favorisa la création de la Zollverein promue par la Prusse (elle disparaît lorsque l'Allemagne est unifiée en 1870), la modernisation très rapide de ses flottes commerciale et militaire, de ses industries. Le Royaume uni ne récupéra la première place qu'au prix de la défaite militaire allemande.

Les libéraux, depuis "Recherches sur la nature et les causes de la richesse des nations" d'Adam Smith (1776), ont beaucoup critiqué les théories protectionnistes des mercantilistes. Selon eux, le protectionnisme est une imposture intellectuelle qui ne sert qu'à favoriser des groupes d'intérêt aux dépens du plus grand nombre et du bien public. Lire par exemple les "Sophismes économiques" de Frédéric Bastiat (1845), dont le septième, la Pétition des fabricants de chandelles.

Pourquoi les libéraux sont-ils opposés au protectionnisme ?

1. Pour des raisons morales : le protectionnisme est l'expression de la loi du plus fort, celle de l'État, qui favorise arbitrairement certains producteurs aux dépens d'autres (étrangers ou non) ;

2. Pour des raisons économiques : contrairement à ce que beaucoup pensent naïvement, le protectionnisme ne profite pas aux pays qui le pratiquent. Son seul effet, résultant de la fermeture du marché, est d'augmenter les coûts des produits dans le pays protectionniste, et ceci au profit de quelques producteurs qui s'enrichissent indûment.
Le commerce international pouvant être vu dans la majorité des situations comme un accord gagnant-gagnant, la mise en place de mesures protectionnistes diminuera le bien-être global. Par exemple, la majorité des historiens économiques considèrent que la Grande Dépression a été aggravée par les mesures protectionnistes mises en place dans les années 1930, comme la loi Hawley-Smoot.

Dans les années 1980, début 1990, des économistes tels que Jagdish Bhagwati ont insisté sur les activités de recherche de rente qu'induisaient les politiques protectionnistes. En effet pour un groupe, il est très tentant d'obtenir de l'État une protection de sorte qu'il puisse soit obtenir des profits plus élevés soit éviter de se mettre au niveau de ses concurrents internationaux. D'une manière générale le protectionnisme est vu, depuis Adam Smith comme favorisant les offreurs au détriment des consommateurs. Enfin, l'alliance entre des groupes de pression forts et l'État a tendance à déplacer les conflits commerciaux du champ économique vers le champ de la souveraineté étatique ce qui peut être potentiellement plus dangereux.

Pour les libéraux, alors que le libre marché est une démocratie de consommateurs, le protectionnisme consiste à faire peser sur le contribuable la protection politique de secteurs économiques défaillants. Défendre le protectionnisme, c'est cautionner la raréfaction autoritaire des marchandises et la hausse artificielle des prix. Cela revient donc à gruger le consommateur, obligé de se contenter de biens et services moins bons ou plus coûteux quand l'accès aux biens ou services qui l'intéressent lui est interdit. Selon eux, les patrons de sociétés qui demandent des aides à l'État ne sont pas de vrais entrepreneurs, mais des confiscateurs de richesses.
Comme toute intervention étatique, le protectionnisme a des effets positifs, mais qui ne concernent qu'une minorité et sont financés par l'impôt ou par les consommateurs, c'est-à-dire par des effets négatifs pesant sur les autres. La propagande étatique montrera les effets positifs (« on protège l'emploi » en achetant « national », on « sauvegarde nos industries », etc.) et dissimulera soigneusement les effets négatifs (renchérissement des biens et services, perte de qualité et de compétitivité, alourdissement de la charge fiscale).
Selon les libéraux, d'un point de vue économique, l'erreur du protectionnisme est de croire qu'il n'y a pas de relation entre importations et exportations, et qu'on peut agir sur les unes, supposées néfastes (les importations), sans conséquence sur les autres, supposées favorables (les exportations). Il n’y a pas d’exemple dans l'histoire d’un pays qui ait été ruiné par le libre échange, alors que le protectionnisme appauvrit tout le monde, tant le pays qui l'instaure que les pays émergents auxquels on refuse ainsi le droit de se développer.
Un des arguments parfois avancés par les partisans du protectionnisme est celui de la symétrie : « notre pays doit se protéger, parce que les autres pays en font autant et protègent leur marché intérieur ». C'est un "non sequitur" pour les libéraux : si les autres pays décident de renchérir le prix des produits importés par des mesures protectionnistes, ils sont les premiers perdants, et il n'y a aucune raison pour les imiter dans leurs erreurs. Le protectionnisme motivé par des raisons exclusivement politiques (comme le fut le Blocus continental napoléonien de 1806 à 1814) relève d'un masochisme absurde, un jeu perdant-perdant : on est prêt à s'appauvrir en espérant qu'en contrepartie cela appauvrisse également l'ennemi.
Certains libéraux assimilent le protectionnisme à une discrimination : faire du commerce avec les nationaux serait bien, tandis que faire du commerce avec les étrangers serait mal, car cela détruit des emplois nationaux.
Pour les partisans du libre-échange, l'erreur centrale du protectionnisme consiste à faire croire qu'il désavantage les étrangers et profite aux seuls nationaux, et notamment aux entreprises nationales. Or, selon eux, le protectionnisme viole non seulement les droits des consommateurs, mais aussi ceux de nombreux producteurs. Il nuit automatiquement à toutes les entreprises autres que celles qu'il privilégie en amputant le pouvoir d'achat général, et plus directement à celles qui dépendent d'approvisionnements étrangers pour maintenir leur compétitivité, ainsi qu'aux exportateurs qui ont besoin que l'étranger vende dans le pays pour avoir les moyens d'acheter des produits du pays.

Le mouvement altermondialiste s'oppose à la concurrence internationale entre les travailleurs qu'induit la baisse des tarifs douaniers entre les États (« libre échange »). C'est en ce sens que le mouvement altermondialiste a organisé la protestation contre la conférence ministérielle de l'Organisation Mondiale du Commerce à Seattle le 30 novembre 1999. À la suite de manifestations parfois violentes contre les forces de police, le sommet n'a pu se dérouler normalement. Reste que le mouvement altermondialiste est aujourd'hui traversé d'un débat entre partisans de ce que l'ancien président d'Attac Bernard Cassen a nommé des formes de « protectionnisme altruiste » et des économistes critiques du protectionnisme.

D'autres altermondialistes préconisent le commerce équitable. Le commerce international n'est alors justifié qu'à la condition de satisfaire les travailleurs des pays en développement et les consommateurs des pays riches. Le commerce équitable préconise l'organisation de la production et du commerce en coopératives.

D'autres enfin préconisent la relocalisation des activités économiques.

Plusieurs altermondialistes considèrent que le protectionnisme des pays riches empêche les pays pauvres de rattraper rapidement leur retard. 

Ce raisonnement est d'autant plus infondé que les pays "pauvres" (telle la Côte d'Ivoire) s'entourent de hautes barrières douanières derrière lesquelles végètent quelques maigres industries de substitution. Dans ces pays où règne la démagogie ("Mes chers paysans" d'Houphouët-Boigny), les recettes publiques propres proviennent principalement de la douane, ce qui évite tout débat sur la nature et la pertinence des dépenses publiques. Allant plus loin encore, la Côte d'Ivoire perpétue les "droits de sortie" institués par le colonisateur (arrêté du 22 décembre 1897) avec l'approbation de la Banque mondiale (C. Garrier, "Forêt et institutions ivoiriennes", L'Harmattan, Paris, 2007, ; "L'exploitation coloniale des forêts de Côte d'Ivoire - Une spoliation institutionnalisée", L'Harmattan, Paris, 2006, ).

L'Europe économique issue des traités successifs depuis la création de la CEE à Rome en 1957 a permis de faire aujourd'hui de la zone des 28 un espace économiquement unifié, très intégré, reposant sur une libre circulation des capitaux, des biens, des services et des personnes. La question est maintenant de savoir si un protectionnisme « extérieur » est possible, afin de protéger un marché intérieur de 500 millions de consommateurs.

Les défenseurs de ce projet mettent en avant le taux d'ouverture de 12 % de la zone régionale (88 % du commerce européen se fait avec un membre de l’Union), taux assez faible pour permettre des politiques économiques communes ainsi que des tarifs extérieurs plus protecteurs pour les secteurs en difficultés (délocalisations). Les États-Unis sont paradoxalement l'un des pays le plus protecteur du monde.

Les adversaires d'un tel projet mettent en avant les méfaits du protectionnisme, la remise en cause de la concurrence, le risque de repli des États sur eux-mêmes. Ainsi les États-Unis ont accusé l'Union européenne de renier la signature qu'elle a donnée au "General Agreement on Tariffs and Trade" (GATT) en créant un marché commun entre les États membres.

Selon eux, le marché auto-élimine les entreprises les moins rentables (cas du textile) qui se délocalisent vers des pays où la main-d'œuvre est moins chère ; les pays dits « développés » sont quant à eux voués à se spécialiser dans des secteurs innovants, à forte « matière grise », et non concurrençables - pour le moment - par les pays émergents.

Une critique plus profonde a été faite par Maurice Allais : pour réussir, une union douanière doit être cohérente, ce qui n'est pas le cas : la crise grecque et la réaction allemande montrent l'écart considérable entre ces économies. Les PECO (pays d'Europe centre et orientale), ex-démocraties populaires, ont importé dans ce qui est devenu l'Union européenne, des économies peu différentes de celles du Tiers-monde, des pratiques gouvernementales incompatibles avec la rigueur morale (un ministre radié pour avoir présentée une thèse faite de copier/coller pris sur Internet, un autre pour n'avoir pas sanctionné les militaires responsable du bombardement de civils en Afghanistan) et financière allemande. Le laisser-emprunter français est déjà hérétique face à ces rigueurs.
Le libre échange que condamnent les auteurs français, est l'un des piliers du prétendu "miracle allemand", un autre étant l'implication des syndicats dans la marche des entreprises (à partir de 2000 salariés, la moitié des administrateurs sont élus par les salariés, disposant des mêmes pouvoirs que ceux qui sont élus par les actionnaires ; on est loin de la monarchie absolue française). L'ordolibéralisme issu de la pensée de Walter Eucken, mis en œuvre depuis plus de quarante ans (initialement, par Franz Josef Strauss), n'est pas même enseigné en France.
Le protectionnisme de Friedrich List est lui aussi envisageable : le financement des projets de recherche ou la mise en place de branches considérées comme naissantes et donc fragiles pourrait être étudiés. Les secteurs clés (énergie, sécurité, agriculture, écologie) ne peuvent pas être considérés comme de simples secteurs vendables au plus offrant. Peut-être faudrait-il réinsérer l'idée de protection dans certains cas précis. Aujourd'hui, le mot protectionnisme n'est plus en vogue. Il est opposé au libre-échange et par extension au terme libéralisme.

Selon les défenseurs du protectionnisme européen, ce dernier serait une voie pour créer un espace fermé où les entreprises auraient la possibilité de prendre conscience que

Il en résulterait une réindustrialisation, et une baisse du chômage. Cette dernière serait alors à l'origine d'une hausse des salaires par le jeu de l'offre et de la demande. Ce type de théorie est défendue par plusieurs auteurs, dont Emmanuel Todd (qui a depuis abandonné l'idée et penche désormais pour un protectionnisme au niveau national).
Ce raisonnement s'applique à la France, non à l'Allemagne qui ne subit que peu les délocalisations. cette dernière bénéficie à l'exportation d'une image qu'elle s'est créée depuis un siècle et demi, la «qualité allemande». C'est ainsi que pour étendre la pénétration de ses appareils photos et caméscope, Sony les dote d'optiques Zeiss ; lorsque Toyota veut mesurer l'image de fiabilité de ses voitures, il les compare avec celles de Mercedes et de BMW ; lorsque Ssangyong lance sa Musso (4x4 haut de gamme), elle la dote d'un moteur Mercedes ; le modèle de luxe Chairman est décliné en trois motorisations Mercedes.

D'autres, proche du nationalisme anti-mondialiste, avancent que les pays développés seraient menacés par les pays émergents et devraient s'en protéger vu que ceux-ci auraient de meilleurs coûts de production dans certains types d'activités.
De fait, ces mouvements sont l'expression d'un même mal généré par un libre-échangisme dérégulé, ou loi de la jungle, qui met en concurrence frontale, sans protection, les riches avec les pauvres, les pays développés (qui ont capitalisé une avance technologique et financière) avec les pays émergents (qui profitent de l'ouverture des frontières pour envahir les marchés et plus discrètement les pays en situation de dépendance qui malgré un déficit de leurs échanges peuvent causer du mal à certaines branches agricoles notamment des premiers).

Pour les partisans de la relocalisation des activités humaines, le protectionnisme doit se penser de la famille à l'unité civilisationnelle : l'Europe. Le protectionnisme n'est pas conçu comme un moyen de protéger un niveau d'organisation humaine en particulier (nation, région, etc.) mais comme un moyen de recentrer l'activité économique sur son objectif premier . Les Localistes proposent d'appliquer le principe de façon souple et progressive en organisant la taxation concentrique des embauches et des ventes de biens et services. Le parti localiste Maison Commune de Laurent Ozon, seule offre politique clairement positionnée comme Localiste et Protectionniste, se réclame d'un protectionnisme localiste et pragmatique. Notons que le localisme se différencie du protectionnisme par la volonté de ne pas définir de frontière mais plutôt des échelles géographiques.

Pour discréditer les mesures protectionnistes, des partisans du libre-échange proclament que les mesures protectionnistes instaurées après la Grande Dépression de 1929 auraient aggravé la crise économique. Par conséquent, certains prétendent même que ces mesures protectionnistes auraient conduit à la Seconde Guerre mondiale, ainsi qu'à la montée du nazisme et du fascisme. Ce parallèle fallacieux entre protectionnisme et xénophobie étant très souvent repris dans les médias.

Jacques Sapir réfute ces hypothèses en expliquant que « "la chute du commerce international a d'autres causes que le protectionnisme" ». Il fait remarquer que « "la production intérieure des grands pays industrialisés régresse [...] plus vite que le commerce international ne se contracte. Si cette baisse avait été la cause de la dépression que les pays ont connue, on aurait dû voir l'inverse. "» De plus, « "si la part des exportations de marchandises dans le produit intérieur brut (PIB) passe de 9,8 % à 6,2 % pour les grands pays industrialisés occidentaux de 1929 à 1938, elle était loin, à la veille de la crise, de se trouver à son plus haut niveau, soit les 12,9 % de 1913". »

« "Enfin, la chronologie des faits ne correspond pas à la thèse des libres-échangistes […] L'essentiel de la contraction du commerce se joue entre janvier 1930 et juillet 1932, soit avant la mise en place des mesures protectionnistes, voire autarciques, dans certains pays, à l'exception de celles appliquées aux États-Unis dès l'été 1930, mais aux effets très limités. En fait, ce sont les liquidités internationales qui sont la cause de la contraction du commerce. Ces liquidités s'effondrent en 1930 (-35,7 %) et 1931 (-26,7 %). Or, on voit la proportion du tonnage maritime inemployé augmenter rapidement jusqu'à la fin du premier trimestre 1932, puis baisser et se stabiliser". »

Jacques Sapir relève que « "la contraction des crédits est une cause majeure de la contraction du commerce "». Une étude du "National Bureau of Economic Research" met en évidence l'influence prédominante de l'instabilité monétaire (qui entraîna la crise des liquidités internationales) et de la hausse soudaine des coûts de transport dans la diminution du commerce durant les années 1930.

Les interdépendances croissantes entre pays apparaissent comme un obstacle aux politiques protectionnistes. En effet, un pays qui protège son marché domestique s'expose à des répliques douanières de la part de ses partenaires. Ainsi, en cas d'escalade protectionniste, les emplois liés à l'export peuvent être menacés. Cet argument est fréquemment employé par les partisans du libre échange, tels Christian Jacob ou Laurence Parisot. Ceux-ci déclarent que 25 % des emplois français sont directement liés à l'exportation.

Néanmoins, ce chiffre semble volontairement exagéré. En effet, le "taux d'ouverture", qui sert de base à l'argumentaire libre-échangiste n'est pas représentatif de la dépendance réelle d'un pays vis-à-vis de l'export. En outre, les conséquences en termes d'emplois sont très difficiles à estimer.



</doc>
<doc id="15046" url="https://fr.wikipedia.org/wiki?curid=15046" title="Socialisme libertaire">
Socialisme libertaire

Le socialisme libertaire est une idéologie et un mouvement politique visant d'une part à l'abolition de l'État et du capitalisme, considérés comme deux formes d'oppression indissociables, et d'autre part à l'instauration d'une société égalitaire, délestée des principes antisociaux de la propriété privée et des institutions de type étatique, et fondée sur l'autogestion et la responsabilité individuelle. 

Le socialisme libertaire est principalement théorisé par Mikhaïl Bakounine et se sépare à sa mort entre communistes libertaires et collectivistes libertaires.

Le socialisme libertaire regroupe un ensemble de philosophies politiques qui visent à établir une société libre de toute hiérarchie politique, sociale et économique – une société d'où toute institution coercitive, répressive, autoritaire ou violente soit exclue, et dans laquelle toute personne aurait un accès libre et égal à toutes les ressources d'information et de production – ou encore une société dans laquelle de telles institutions seraient réduites au minimum. 

Dwight Macdonald définit ainsi le socialisme libertaire : , une société qui .

Cette égalité et cette liberté seraient réalisées principalement à travers l'abolition des institutions d'autorité d'une part, et de la propriété privée d'autre part, afin que le contrôle direct des moyens de production soit détenu par l'ensemble de la classe laborieuse. 

Le socialisme libertaire prône en cela l'identification, la critique et le démantèlement pratique de toute autorité, conçue comme illégitime dans tous les aspects de la vie sociale. Aussi les socialistes libertaires considèrent-ils que « l'exercice du pouvoir sous quelque forme institutionnelle, qu'elle soit économique, politique, religieuse ou sexuelle – fait autant violence à celui qui l'exerce et celui qui le subit ».

Le socialisme libertaire est aussi appelé mouvement libertaire ou anarchisme socialiste. Par sa volonté de transformation sociale, le socialisme libertaire fait partie intégrante du socialisme dont il incarne la tendance libertaire.

. Certains chercheurs utilisent le terme d'anarchisme socialiste comme synonyme d'anarchisme.

Les philosophies politiques communément décrites comme proches du socialisme libertaire incluent plusieurs types d'anarchisme (dont l'anarchisme collectiviste, l'anarcho-syndicalisme et certaines formes d'anarchisme individualiste), de mutualisme, d'écologie sociale et de communisme de conseils du socialisme).

L'anarchisme espagnol a donné de nombreux exemples d'organisations fédératives abouties et opératoires. Des exemples contemporains de socialisme libertaire organisationnel et des modèles décisionnaires pratiques incluent un certain nombre de mouvements anti-capitalistes et anti-globalisation incluant l'Armée zapatiste de libération nationale (EZLN) et ses « Conseils de bon gouvernement », et le Réseau Global Indymedia - qui couvre 45 pays sur 6 continents. On peut citer aussi les nombreux exemples de sociétés indigènes dont le système politique et économique peuvent être opportunément décrits comme anarchiste ou socialistes-libertaires, chacun d'eux étant par ailleurs unique et relatif à la culture qui l'engendra . Pour les libertaires, la diversité des pratiques à l'intérieur d'un cadre de principes communs est une preuve de la vitalité de ces principes, de leur flexibilité et de leur force.

Dans un chapitre portant sur l'anarchisme socialiste, l'économiste radical Robin Hahnel présente ainsi la plus grande période d'influence du socialisme libertaire comme s'étendant de la fin du jusqu'aux quatre premières décennies du : « Au début du , le socialisme libertaire était une force aussi puissante que la sociale-démocratie ou le communisme. L'Internationale Anarchiste de St-Imier fondée au congrès de St-Imier quelques jours après la séparation entre « marxistes » et « anarchistes » au congrès de La Haye de 1872 perdura comme force active, avec les communistes et les sociaux-démocrates (à l'époque anticapitalistes), en fédérant durablement les énergies diverses d'activistes anti-capitalistes, de révolutionnaires socialistes, de syndicats de travailleurs et de partis politiques pendant plus d'un demi-siècle. Les socialistes libertaires auront ainsi joué un rôle majeur dans les révolutions russes de 1905 et 1917. Les socialistes libertaires ont également joué un rôle prépondérant dans la révolution mexicaine de 1911, et dans la révolution sociale qui se déroula en Espagne en 1936 et 1937 ».

Selon le politologue Francis Dupuis-Déri : « Les principes du socialisme libertaire trouveront à s’incarner au fil des années 1970 et 1980 dans des mouvements sociaux de sensibilité antiautoritaire et antihiérarchique, qui pensent l’organisation militante elle-même comme un espace libre, autonome et autogéré par ses membres, et dans lequel se développe par la délibération un sens du bien commun, de l'égalité et de la liberté. Cette sensibilité continue de s’affirmer dans le mouvement altermondialiste, qui émerge vers la fin des années 1990, à travers ses manifestations de rue spectaculaires, de la Bataille de Seattle en 1999 aux mobilisations contre le G8 en Allemagne pendant l’été 2007, ainsi que dans sa structure globale, ses médias alternatifs, sa production artistique et ses camps radicaux en marge des Forums sociaux ».

Le socialisme libertaire est une idéologie sujette à différentes interprétations, mais dont le fond commun vise à une distribution des ressources entre les travailleurs dans une perspective foncièrement différente de celle du capitalisme. La théorie économique anarchiste procède de la conjonction optimale des libertés individuelles et la concentration minimale du pouvoir ou de l'autorité. Les socialistes libertaires ayant une forte aversion pour la coercition sous toutes ses formes, ils prônent l'anarchisme comme seule forme constitutionnelle envisageable dans un cadre démocratique véritable. Les options politiques sont conçues pour décentraliser au mieux le pouvoir économique et politique, en général en impliquant la collectivisation à grande échelle des moyens de production. Les socialistes libertaires récusent la légitimité de la plupart des formes de propriété privée économiquement signifiante autant qu'ils considèrent les relations de propriétés et de capitaux comme des formes de domination contraires au principe d'une liberté individuelle conçue comme souveraine. 

Les anarchistes socialistes posent que lorsque le pouvoir s'exerce, en termes d'une influence économique, sociale ou physique d'un individu sur un autre, sa légitimation échoit à l'autorité détentrice du pouvoir qui devra dès lors justifier de ses actions en tant qu'elles contreviennent à la liberté individuelle.

Les socialistes anarchistes considèrent que toute structure sociale devrait être développée par des individus ayant une part égale de pouvoir décisionnaire, l'accumulation du pouvoir économique ou politique dans les mains de quelques-uns nuisant nécessairement au libre exercice de la liberté individuelle de la majorité des individus d'un groupe donné.

Pour le dire autrement, quand les principes capitalistes (tout comme ceux du libertarianisme nord-américain) concentrent le pouvoir économique dans les mains de qui possède le plus grand capital, l'anarchisme socialiste vise "a contrario" à redistribuer équitablement le pouvoir, et partant la liberté, entre les différents membres du corps social. Une différence clef, à ce titre, entre libertariens et anarchistes de gauche est que ceux-ci considèrent que le degré de liberté de chacun est affecté par le pouvoir économique associé, quand les premiers estiment que la liberté n'existe qu'en terme entrepreneurial et réduisent la créativité politique à la liberté d'entreprendre au sens purement économique.

Les anarchistes socialistes estiment que si la liberté doit être valorisée, alors la société doit travailler à un système dans lequel les individus ont un pouvoir plénier autant sur le plan politique qu'économique. Les socialistes anarchistes cherchent à substituer à l'arbitraire de l'autorité la démocratie directe, la liberté d'association et la liberté d'expression, l'autonomie des populations dans tous les domaines de la vie, l'autogestion étant promue enfin comme système universel d'organisation, sur le plan communal (politique) comme sur le plan syndical ou fédératif (économie). La plupart des mouvements anarchistes arguent que les syndicats et fédérations de travailleurs devraient gérer les infrastructures industrielles, et que les travailleurs devraient pouvoir directement jouir du produit individuel de leur travail. À ce titre, on distingue entre le concept de "propriété privée" et celui de "possession individuelle". Là où la "propriété privée" autorise le contrôle individuel et exclusif d'un bien, "que celui-ci soit en usage ou non", et ce indépendamment de son potentiel productif, la "possession individuelle" ne garantit quant à elle aucun droit concernant les biens "non utilisés". Un titre de propriété autorise son détenteur à soustraire son bien à l'usage collectif, ou, s'il le désire, de requérir paiement contre son usage : la notion de possession individuelle n'est pas compatible, quant à elle, avec ce type d'« extorsion » ou d'« exploitation ».

Le socialisme libertaire considère toute concentration de pouvoir comme une source d'oppression, au point d'aboutir à une contestation radicale des principes fondateurs de l'État.

Pratiquement, les anarchistes cherchent à s'organiser en associations volontaires, souvent appelées collectifs ou syndicats, fondés en démocratie directe dans les processus de décision. Certains anarchistes socialistes plaident pour la combinaison de ces institutions à travers l'usage de , récusables et non renouvelables, aux différents niveaux fédératifs.

Alors que la plupart des mouvements socialistes insistent sur le rôle de l'État démocratique dans la défense de la liberté et de la justice sociale, les socialistes libertaires misent sur les unions syndicales, les assemblées citoyennes, les conseils communaux, les collectivités locales, et autres types de fédérations non-étatiques et décentralisées par nature.

Le socialisme libertaire n'a pas été un mouvement utopique, ayant plutôt tendu à éviter la théorisation excessive et les spéculations sur ce que devrait être une société future et idéale, hormis quelques exemples plutôt atypiques tels les spéculations hautes en couleurs d'un Charles Fourier (qui n'était pas socialiste libertaire !). 

Les penseurs socialistes-libertaires considèrent que les changements majeurs ne peuvent être réalisés dans l'immédiat historique, il s'agissait d'abord de conduire un combat permanent pour la mise en place de structures sociales nouvelles, pour qu'une solution soit atteinte à terme selon des voies démocratiques et organiques. Les socialistes libertaires suggèrent que cette focalisation sur l'exploration plutôt que la prédétermination fait leur plus grande force. Ils soulignent par exemple que le succès de la science dans son explication du monde naturel procède de la méthode scientifique et de son insistance structurelle sur l'exploration et l'expérimentation rationnelles, et non sur ses conclusions ni ses prévisions ; ceci quand la plupart des explications dogmatiques des phénomènes naturels s'est presque toujours soldée par l'échec. . Les socialistes libertaires contemporains considèrent qu'une approche méthodologique de l'expérimentation constitue le meilleur moyen de réaliser à terme leur idéal social. Pour eux, les approches dogmatiques de l'organisation sociale sont ainsi autant vouées à l'échec que le sont les explications dogmatiques du fait naturel. L'anarchiste américain Rudolf Rocker déclare ainsi en 1956 .

Comme le note Michel Onfray, sur la problématique violence / non-violence, les positions des socialistes libertaires sont contradictoires voire antinomiques. La publication, en 2000, d'un numéro de la revue "Réfractions" tout entier consacré à ce sujet démontre la prégnance du débat.

Certains anarchistes révolutionnaires prônent la violence comme nécessaire face à la violence capitaliste et étatique souvent corrélées. Ils l'estiment nécessaire au dépassement et à l'abolition de la société capitaliste.

D'autres, tel Pierre-Joseph Proudhon, souvent qualifié de « Père de l'Anarchisme », plaide en faveur d'une révolution non-violente. Pour n'en citer que quelques-uns, William Godwin, Gaston Leval, Étienne de La Boétie, Henry David Thoreau, Anselme Bellegarrigue, Benjamin Tucker, Han Ryner, Émile Armand, Barthélemy De Ligt s'inscrivent dans ce courant non-violent (voir la "Petite Anthologie de la révolution non-violente chez les principaux précurseurs et théoriciens de l'anarchisme").

En 1979, dans son étude sur "L'anarchisme au XXe siècle", le professeur Henri Arvon fait la synthèse suivante :

Dans le contexte du mouvement socialiste européen, les anarchistes sont tenus comme les opposants au socialisme d'État, position incarnée spécialement par un Michel Bakounine devant l'« autoritarisme » de communistes se revendiquant du marxisme. 

Le socialisme libertaire prôné par la gauche libertaire européenne se distingue radicalement de l'autoritarisme en appelant fondamentalement à la mise en commun des moyens de production, ceci sans l'intermédiaire d'une institution étatique, mais dans une perspective résolument collectiviste de la société, diamétralement opposée en cela au paradigme individualiste et productiviste propre à la tradition philosophique de la pensée de droite, qu'elle soit américaine ou européenne. Comme Noam Chomsky le souligne, « un libertaire cohérent doit s'opposer à la propriété privée des moyens de production et à l'esclavage salarié, inhérent au système qu'elle implique ».

En rejetant autant le capitalisme que l'État, les socialistes libertaires se posèrent en opposition avec la démocratie représentative associée au capitalisme autant qu'avec certaines formes autoritaires issues du marxisme, qui apparurent après la division entre anarchistes et marxistes. Même si l’anarchisme et le marxisme partagent le but d'une société sans État, les anarchistes critiquent des marxistes pour leur défense d'une phase transitoire au cours de laquelle les structures d'État sont utilisées aux fins de leur propre dépassement. Néanmoins, des tendances marxistes comme le communisme autonomiste ou le communisme de conseils ont été historiquement proches du mouvement anarchiste. L’anarchiste Daniel Guérin a proposé un « marxisme libertaire ».

Les mouvances anarchistes sont entrées en conflit avec les forces capitalistes et celles marxistes, parfois au même moment, comme à l'occasion de la révolte ukrainienne dite de la Makhnovchtchina menée par le révolutionnaire anarchiste Nestor Makhno de 1917 à 1923, ou pendant la Guerre civile espagnole, même si comme dans cette dernière guerre les marxistes se divisaient entre alliés (POUM) et ennemis (staliniens) des anarchistes. D'autres persécutions politiques sous des régimes bureaucratiques ont résulté dans un antagonisme historiquement fort opposant anarchistes et marxistes non-léninistes d'un côté, et léninistes de l'autre avec leurs dérivatifs staliniens tels le Maoïsme. Dans l'histoire récente, pourtant, les anarchistes socialistes ont formé des alliances répétées avec des groupes marxistes, léninistes et néo-trotskistes en des coalitions souvent intempestives.

Une part de cet antagonisme peut être rapporté à l'Association internationale des travailleurs ou Première Internationale, congrès des travailleurs radicaux, où Michel Bakounine, alors représentant les « anarchistes » socialistes, et Karl Marx représentant des collectivistes que les « anarchistes » accusent d'autoritarisme, entrent en conflit ouvert sur plusieurs questions. 

Lors du IVe congrès de la Première Internationale de Bâle (6-12 septembre 1869), on peut apprécier le poids respectif de chacune des sensibilités. À partir de votes sur des motions ou amendements présentés par ces divers « courants », on peut établir le « rapport de force » comme suit[réf. nécessaire] :

L'expulsion de Bakounine et ses suiveurs marque le début d'une longue divergence entre les anarchistes socialistes et les marxistes, ceux-ci étant qualifiés par ceux-là d'« autoritaires ».










</doc>
<doc id="15047" url="https://fr.wikipedia.org/wiki?curid=15047" title="Puma">
Puma

Le puma (Puma concolor), également appelé lion de montagne ou cougar est un mammifère carnivore qui appartient à la famille des félidés. C'est un animal solitaire qui vit en Amérique du Nord et du Sud. Difficile à observer, il ressemble à un léopard sans taches, ce qui explique que, par abus de langage, on le désigne parfois également sous le terme de « panthère ».

Le pelage du puma est uniforme ("concolor" signifie ), même si l'on devine parfois des rayures sur ses membres antérieurs. Sa couleur reste dans les tons fauves et varie du brun roux dans les régions tropicales au gris jaune dans les régions arides. Le dessous du corps est plus clair, allant de la couleur crème au blanc. La longueur des poils dépend du milieu naturel dans lequel l'animal vit : ils sont rudes et courts dans les régions chaudes et longs en régions froides. Les cas d'albinisme sont rares mais les cas de mélanisme sont fréquents.

En moyenne, le mâle mesure entre et de longueur, le record étant de , queue comprise. Celle-ci représente un tiers de la taille de l'animal. La masse du puma est comprise en moyenne entre 53 à 72 kg pour les mâles ; le plus gros individu connu faisait . Sa taille varie de 60 à à hauteur d'épaule. La femelle est moins grosse (environ 35 à ). En outre, les sous-espèces de pumas se différencient par leur taille : les plus grands spécimens vivent dans les montagnes Rocheuses et en Patagonie tandis que les plus petits évoluent dans les régions proches de l'équateur. La taille augmente au fur et à mesure que l'on s'éloigne de cette ligne.

La silhouette du puma est fine et musclée et son postérieur est plus haut que sa tête ce qui lui permet de sauter facilement. Sa longue queue (entre 53 et ), plus foncée à son extrémité, est l'une des caractéristiques du puma. Enfin, il possède quatre doigts munis de griffes longues, pointues et rétractiles. Ses pattes arrière sont plus larges et puissantes que celles de devant, ce qui lui permet de bondir efficacement et d'avancer aisément dans la neige ou sur terrain escarpé.

Le puma possède une petite tête de forme arrondie munie d'oreilles courtes et écartées qui lui donnent une acuité auditive exceptionnelle. La puissance de ses mâchoires est plus grande que celle de n'importe quel chien. La fourrure du menton est blanchâtre comme celle du museau. La truffe est rose et son odorat est très développé. La couleur des yeux varie du vert au jaune ambré et son champ de vision est très large. Le puma est capable de bien voir dans l'obscurité.

Le puma est un animal solitaire. Les mâles et les femelles ne se rencontrent qu'en période d'accouplement (environ deux semaines). Le puma peut se reproduire à partir de trois ans. Après une gestation d’environ trois mois (entre 88 et 96 jours) la femelle met au monde jusqu'à six petits, généralement deux ou trois. Les naissances ont lieu surtout à la saison chaude. La femelle met bas dans une tanière qui peut être des fourrés, une cavité rocheuse ou encore un arbre creux. Les petits restent avec leur mère jusqu'à leur deuxième année.

À la naissance, les jeunes pèsent de 600 à et ont un pelage brun jaunâtre avec des points noirs ou marron qui disparaissent vers l'âge de 16 mois. Les chatons ouvrent les yeux à dix jours et mangent de la viande à six semaines, mais l'allaitement dure plus de trois mois. À un mois et demi, leur poids avoisine cinq kilogrammes. Il arrive qu'un mâle attaque et tue les chatons. Les femelles peuvent également mourir en tentant de protéger leur progéniture. Dans son environnement naturel, un puma vit environ huit à dix ans ; en captivité, sa longévité peut dépasser 25 ans. Il atteint sa maturité sexuelle dès l'âge de deux ans.

Le puma peut courir très vite, jusqu'à , mais seulement sur de courtes distances. En outre, il peut franchir jusqu'à en longueur, d'un bond à partir d'une position fixe. Enfin, il est capable de faire des bonds atteignant 4 à de haut, sans élan. Il se déplace en silence. C'est un animal qui nage bien mais il ne le fait qu'en cas de menace. Pour les besoins de la chasse ou en cas de menace, il est capable de grimper aux arbres et de faire preuve d'une grande agilité. Le Puma a peu de prédateurs mais en Amérique centrale et Amérique du Sud, il peut être attaqué par le Jaguar et l'Anaconda. En Amérique du Nord, il peut se trouver confronté à un Grizzly ou à une meute de loups.

Les cris du puma diffèrent selon les circonstances : très aigus ou ressembler à un sifflement en période de rut ; ils peuvent faire aussi penser à un fort ronronnement. Pendant la saison de l'accouplement, les pumas émettent des sortes de miaulements (ou feulements) puissants. Le puma ne rugit pas en raison de l'ossification totale de son appareil hyoïde. Il émet un gémissement aigu pour menacer les intrus osant s'aventurer sur son territoire.

Les pumas sont carnivores (voir tableau ci-contre), ils attaquent en général les grands mammifères comme les cerfs ou les élans mais aussi des animaux plus petits si nécessaire, jusqu'à pêcher ou se nourrir d'insectes ou de lézards. En moyenne, un puma d'Amérique du Nord consomme un cerf tous les sept à dix jours, parfois plus pour une femelle avec des petits. En Amérique latine, les pumas subissent la concurrence des jaguars qui ne leur laissent que des proies de taille moyenne. Enfin, le puma peut tuer des animaux d'élevage (chevaux, moutons, vaches, chèvres, etc.).

Les pumas chassent seuls, à l'aube ou au crépuscule, le jour en montagne. Ils traquent leur proie et l'approchent par derrière. Les pumas peuvent fondre sur un animal du haut d'une branche : c'est la chasse à l'affût. Ils tuent leur proie en mordant la base du crâne, brisant le cou de leur victime. Ils peuvent ainsi s'attaquer à des animaux beaucoup plus gros qu'eux. Ils enterrent ensuite la carcasse ou la recouvrent partiellement afin de la protéger quelques jours des charognards avant de revenir pour s'en nourrir. Comme tous les prédateurs, ils changent de proies selon l'abondance de ces dernières. Ainsi sur une zone où l'on avait réintroduit une espèce de mouflon dit mouflon canadien ("Ovis canadensis"), on a constaté que les pumas ont augmenté leur prédation sur cette espèce alors que les population de cervidés (leur nourriture préférée) avait diminué.

Les mâles adultes sont territoriaux. Ils occupent un territoire moyen de environ (de 100 à ), qui est marqué par leur urine, leurs déjections ou des traces de leurs griffes sur les troncs, accompagnées d'un marquage odorant ; comme les autres félins, le puma possède des glandes sudoripares au niveau des pelotes digitales et plantaires. Le territoire des femelles est plus restreint (moins de en général), ce qui implique que le territoire d'un mâle recouvre plusieurs territoires de femelles.

Des suivis de jeunes couguars par dans un habitat relativement fragmenté, en Californie, ont montré qu'ils trouvent assez facilement les corridors biologiques qui leur conviennent et les écoducs leur permettant de traverser une autoroute. La dispersion se fait au moment de l'abandon des petits par la mère en bordure de son domaine vital. Le jeune restait dans un rayon de à proximité durant 13 à 19 jours et explorait ensuite son nouvel environnement dans la direction opposée à celle prise par la mère. L'âge moyen à la dispersion était de 18 mois (extrêmes : 13-21 mois). Les animaux fréquentent facilement les lisières ville-forêt et les corridors biologiques et écoducs, et semblent apprécier l'absence d'éclairage artificiel direct ou indirect, si ce n'est l'absence de pollution lumineuse.

Après y avoir été exterminé par la chasse et la destruction de vastes superficies d'habitats naturels, le cougar a été confiné aux États de l'ouest des Etats-Unis depuis presque un siècle. Il semble lentement recoloniser des zones situées plus à l'Est du pays. Une modélisation écologique publiée en novembre 2015, basée sur plus de 40 années de statistiques populationnelles croisées à des informations sur l'éthologie et l'habitat de l'espèce laisse attendre une réapparition de populations de cougars dans les États du Midwest entre 2015 et 2040, à condition qu'il soit suffisamment accepté, ce qui implique selon les chercheurs une approche intégrée de la présence potentielle d'un grand carnivore dans la région.

Le puma est en voie de disparition en raison de la fragmentation ou de la disparition de ses habitats et de la pression de la chasse et de dérangement. Les pumas sont très discrets, n'attaquent que très rarement l'homme et dépensent beaucoup d'énergie pour le fuir. Ceci peut arriver quand celui-ci s'aventure dans des zones sauvages ou que l'animal se sent menacé. De 1890 à janvier 2004, on a recensé environ 100 attaques de pumas sur des humains en Amérique du Nord (dont 16 mortelles). Sans doute en raison d'une pression plus forte sur leurs territoires, le phénomène semble s'intensifier avec 53 attaques dans les années 1980 dont neuf mortelles en Amérique du Nord.

Le puma peut être apprivoisé. Inscrit à l'annexe ll de la CITES, il peut donc être commercialisé avec un permis. Des fermiers en adoptent en Argentine et les laissent en liberté sur l'exploitation, où l'animal se révèle joueur et convivial.

Les efforts que le Cougar fait pour ne pas être aperçu par l'homme ont un coût important en termes d’énergie dépensée, et secondairement aussi en termes de chances de survie ; c'est la conclusion d'une étude récente (2015) basée sur le suivi à distance (grâce à des balises GPS) de 30 couguars dans les montagnes de Californie (de 2008 à 2013). Un logiciel spécial a identifié 208 sites vers lesquels ces couguars retournaient plusieurs fois de suite durant plusieurs jours (ce qui est un indice fort qu'ils aient attaqué une proie dans ce secteur). L'étude a montré que dans les zones de ce territoire un peu plus urbanisées (2 à 9 maisons par hectare), les femelles couguars ont tué 36 % de chevreuils en plus que celles qui vivaient dans des zones pas ou peu habitées, et qu'elles passaient moins de temps à se nourrir sur chaque carcasse, alors qu'une telle différence n'a pas été constaté chez les mâles). Les auteurs supposent que les femelles doivent dépenser beaucoup plus d'énergie pour à la fois rester discrètes et chasser pour leur progéniture quand elles vivent à proximité de l'homme, et que ceci se paye en termes de (Ainsi, l'une des femelles suivies a perdu trois de ses portées en 3 ans, et c'est celle qui vivait dans l'habitat le plus anthropisé).

Le puma fait partie des félins pouvant attaquer l'être humain. Entre 1890 et 1990, de pumas, dont ont été répertoriées aux États-Unis et au Canada. Les deux tiers des attaques portaient sur des enfants jusqu'à neuf ans et tous les décès sont survenus sur des jeunes de moins de treize ans. Plus du tiers des incidents ont eu lieu sur l'île de Vancouver, ce qui est peut-être un cas d'apprentissage de prédation. 40 % des attaques ont lieu en été, ce qui est probablement dû aux sorties en nature plus fréquentes de l'Homme en cette période. La majorité des attaques avaient lieu dans le dos de la victime. Bien qu'en forte augmentation sur la période étudiée, les tentatives de prédation reçoivent une couverture médiatique importante en comparaison d'autres prédateurs statistiquement plus dangereux pour l'Homme, comme les chiens.

Avant la colonisation et l'explosion démographique du territoire, le lion des montagnes occupait tout le continent américain : de la Colombie-Britannique au Sud de l'Argentine. Aujourd'hui encore, il est l'animal terrestre qui occupe l'aire la plus étendue du Nouveau Monde, couvrant près de 110 degrés de latitude. Le puma est également le félin le plus répandu après le chat domestique sur le continent américain. En raison de sa grande répartition géographique, on a voulu dénombrer une trentaine de sous-espèces de "puma concolor" déterminées par quelques différences de taille, de pelage ou de comportement (voir phylogénie).

Le puma est absent des îles (Caraïbes, Antilles), de l'Uruguay ainsi que du Centre et de l'Est de l'Amérique du Nord. Il était autrefois présent dans les forêts du Grand Nord mais il a disparu à la suite de l'extinction des grands ongulés dans cette région. Il a été beaucoup chassé aux : on recensait en moyenne 350 pumas tués par an en Colombie-Britannique entre 1910 et 1957. Le puma peut occuper une grande variété d'habitats mais l'extension humaine les a repoussés en montagne, dans une forêt morcelée et considérablement réduite depuis la colonisation européenne, dans les prairies, les déserts et les étendues "sauvages" du continent américain. On le trouve jusqu'à dans la cordillère des Andes.

Le puma est classé en annexe II de la CITES, c'est-à-dire en espèce vulnérable. Les pumas de Floride et d'Amérique centrale appartiennent à l'annexe I et sont menacés d'extinction. La chasse du puma est en général interdite ou réglementée, sauf au Guyana, en Équateur et au Salvador. Les réserves et les parcs naturels tentent de préserver leur habitat (Yosemite, Yellowstone, Río Plátano, Iguazú). Cependant, certains éleveurs, dont les troupeaux sont menacés, les abattent ou les empoisonnent.

L'espèce se trouvait autrefois dans presque toute l'Amérique du Nord, sur le territoire des cerfs, sa source de nourriture principale. Il a cependant été victime de la chasse pendant près de deux siècles, sa fourrure étant prisée et sa présence n'étant pas la bienvenue près du bétail. La sous-espèce de l'Est, "Puma concolor couguar", qui occuperait actuellement le Sud-Est du Canada (Ontario, Québec, Nouveau-Brunswick et Nouvelle-Écosse), avait apparemment disparu dès la seconde moitié du mais une faible population semble encore subsister dans une partie de son aire de répartition historique.

Au Québec, sa population n'a probablement jamais été abondante. Depuis 1955, quelques centaines d'observations ont été rapportées. La majorité d'entre elles sont postérieures à 1991, période à partir de laquelle les mentions de couguar pour la province ont été systématiquement recueillies par les gestionnaires de la faune. Les mentions proviennent toutes de la partie méridionale de la province au sud du , principalement des régions de l'Abitibi-Témiscamingue, de l'Estrie et du Bas-Saint-Laurent. Un seul de ces signalements a été confirmé formellement (preuves vérifiables) en 1992 lorsqu'un individu présentant un danger a été abattu en Abitibi-Témiscamingue. Cependant, une analyse de l' a démontré que l'individu provenait d'une sous-espèce présente en Amérique du Sud. L'hypothèse d'un animal échappé d'un jardin zoologique ou gardé en captivité semble la plus plausible. Les principaux facteurs limitatifs de la présence du cougar au Québec seraient sans doute liés aux diverses activités humaines de même qu'à la dispersion des individus, qui auraient de la difficulté à se rencontrer lors de la période d'accouplement.

La présence du cougar fait actuellement l'objet d'un suivi au Québec. Un système de collecte des observations (rapport d'observation) et d'analyse de leur qualité est en place dans chaque région (bureaux de Protection de la Faune du Québec). À ce jour, la présence d'environ huit individus répartis à travers la province est confirmée par les scientifiques. En 2005, le ministère de la faune et des parcs du Québec a officiellement confirmé la présence du couguar dans trois régions du Québec : la Capitale-Nationale (Québec), la Gaspésie et le Saguenay–Lac-Saint-Jean. D'autres observations auraient été faites dans plusieurs autres régions dont le Centre-du-Québec et l'Estrie. Un couguar a d'ailleurs été filmé dans un champ de Fortierville en mai 2007, alors qu'un autre a été aperçu et clairement identifié le octobre 2007 à la forêt Montmorency située à environ au nord de la ville de Québec, près du parc national de la Jacques-Cartier. Un autre a également été observé au printemps 2007 dans le Parc de la Gatineau, dans l'Outaouais.

D'abord chassé jusqu'à sa quasi-extinction aux États-Unis, le puma fait un grand retour, avec une population estimée entre et individus dans l'Ouest du pays, principalement dans les montagnes Rocheuses. L'animal est présent dans quatorze États de l'Ouest et en Floride. On estime entre et le nombre de "lions des montagnes" en Californie où il est protégé par la loi, entre et au Colorado ; les couguars de Floride sont estimés à une cinquantaine et constituent la sous-espèce la plus menacée du continent américain. Dans les autres États, sa chasse est légalisée mais soumise à l'autorisation de l"'United States Fish and Wildlife Service". Le Texas est le seul État où le puma peut être chassé librement.

Les pumas tentent de reconquérir l'Est du pays, suivant les criques et les cours d'eau, ils ont à présent atteint les États du Missouri et du Michigan. Cette évolution pourrait permettre d'en trouver sur la quasi-totalité du territoire des États-Unis mais, la réintroduction du loup dans les montagnes Rocheuses est une menace pour le puma qui était jusque-là le seul grand prédateur carnivore avec l'ours dans ce territoire. Il y a par exemple environ 25 pumas dans le parc du Yellowstone contre 118 loups.

À cause de l'urbanisation, les pumas se retrouvent de plus en plus fréquemment en contact avec les humains, surtout dans les zones riches en cerfs, leur proie naturelle. Beaucoup de ces félins meurent percutés par des automobiles ou des camions (voir roadkill). Si on a compté des attaques d'animaux domestiques (chats, chiens), ils ne se tournent que très rarement vers le domaine des humains comme source de nourriture. Le 2 mars 2011, le puma de l'est américain est officiellement annoncé par l'USFWS comme étant éteint aux États-Unis, mais peut-être était-ce déjà le cas depuis les années 1930. En fait, le statut de cette population en tant que sous-espèce est incertain, et des migrations d'individus de l'ouest de la répartition sont possibles.

Les services américains de la pêche et de la faune (U.S. Fish and Wildlife Service) ont passé en revue toutes les informations disponibles et donné leur conclusion le 12 mars 2014 : le puma de l'est américain est éteint.

La phylogenèse est l'étude des fossiles d'un animal afin d'en préciser son apparition et son évolution. Cependant, il existe assez peu de fossiles de félins, et la phylogénie moderne s'appuie essentiellement sur les analyses génétiques (Cf. ADN). Le premier félin daterait d'il y a 11 millions d'années. L’ancêtre commun des lignées "Leopardus", "Lynx", "Puma", "Prionailurus" et "Felis" aurait traversé la Béringie et colonisé l’Amérique du Nord il y a environ 8 à 8,5 millions d’années. Des analyses génétiques effectuées en 2006 ont montré que ces lignées ont divergé dans l’ordre de leur citation : le genre "Puma" est donc la troisième lignée à se différencier. Les félins nord-américains ont ensuite envahi l’Amérique du Sud par l’isthme de Panama il y a 3 millions d’années durant le grand échange interaméricain (Cf. les grandes modifications climatiques du Quaternaire et notamment du niveau des mers qui déterminent la géographie des continents et surtout les possibilités d'échanges, de passages).

Le puma est le plus grand des félins de la sous-famille des "Felinae" et possède des caractéristiques similaires aux grands félins de la sous-famille des "Pantherinae". Le Puma fut d’abord considéré comme un membre du genre "Felis" ("Felis concolor"). Dès 1834, Jardine propose de classer le Puma dans un genre à part : "Puma". Le Puma a alternativement fait partie du genre "Felis" puis "Puma". Les différentes références taxinomiques s’accordent à présent pour le rattacher au genre "Puma", qui ne contient qu’une seule autre espèce : le jaguarondi ("Puma yagouaroundi"). Des études ont montré que le puma et le jaguarondi sont étroitement proches du Guépard. La nature de cette relation est cependant mal définie : une première hypothèse serait que les lignées du Guépard et du Puma aient divergé en Amérique (guépard américain) puis que le Guépard soit retourné vers l’Ancien Monde ; une autre suggère que le Guépard a évolué indépendamment sur l'Afro-Eurasie.

Le Puma d’Amérique du Nord présente un haut niveau de similarité génétique, ce qui suggère que l’espèce actuelle descend d’un petit groupe d’individus. Culver "et al." pense que les populations nord-américaines de "Puma concolor" ont été extirpée durant les extinctions du Pléistocène il y a environ ans (Holocène) puis que les populations sud-américaines ont par la suite repeuplé le Nord de l’Amérique.

Jusqu’à la fin des années 1990, de 30 à 32 sous-espèces différentes ont été validées. Certains auteurs ont même avancé jusqu’à 35 sous-espèces différentes. Les différences majeures entre ses différentes subdivisions de l'espèce étaient la localisation et la taille du corps : la plupart de ces formes ne prenaient pas en compte la variabilité naturelle entre les individus. Une étude génétique effectué en 2000 sur l’ADN mitochondrial a diminué drastiquement le nombre de sous-espèces, passant d’une trentaine à six :

La Panthère de Floride est une des sous-espèces de pumas selon l'ancienne classification ("Puma concolor coryi"). Autrefois présente dans tout le Sud-Est des États-Unis, elle survit dans le Sud de la Floride (marais de Big Cypress). Il ne subsisterait qu’une cinquantaine d'individus. Elle est menacée d'extinction malgré les efforts du groupe de sauvegarde de la Panthère de Floride ("The Florida Panther Recovery Team"), fondé en 1976. Il y a actuellement un grand effort de la part de l’État de Floride pour sauver ces panthères locales, leur nombre étant en effet en inquiétante diminution : élevage en captivité, préservation du gibier, reproduction artificielle Néanmoins, la nouvelle classification permet d’envisager une reproduction de préservation par croisement avec d’autres anciennes sous-espèces moins menacées de couguars d’Amérique du Nord, qui sont dans la même lignée phylogénétique, et de parvenir, par sélection, à retrouver les caractères de la Panthère de Floride, avec l’aide d’élevages ou parcs naturels d’autres États.

Des lignes directrices ont été proposées pour sa protection et gestion, mais dans la nature, comme la plupart des grands carnivores, cet animal est souvent victime de collision avec des véhicules, empoisonnement, ou est mal accepté par les propriétaires de terrain, de gibier ou d'animaux d'élevage.

Le mot est dérivé d’un mot quechua introduit en français par l'intermédiaire de l'espagnol, il est attesté en espagnol depuis 1602. Les Incas les tuaient lorsqu'ils s'attaquaient aux guanacos et aux vigognes. Le terme est orthographié de diverses manières (, et parfois ) au cours du . Au Brésil, les Amérindiens Tupi appelaient l'animal "susuarana", déformé ensuite par les Portugais en "suçuarana" puis "cuguacuarana" et qui devint au le du naturaliste français Buffon. Le mot se serait peu à peu altéré : la cédille est perdue, puis les sons et sont confondus.

Les différents noms et expressions utilisés pour désigner le Puma reflètent la diversité des langues et des cultures du continent américain. Il est inscrit au livre Guinness des records en tant qu’animal ayant le plus grand nombre de dénominations, plus de quarante noms différents juste pour l’anglais, probablement grâce à sa large distribution en Amérique. En français, il existe également de nombreux termes synonymes tels que , , , , .

Les peuples amérindiens le baptisèrent de façons diverses : il était par exemple pour les Mayas. Les peuples qui occupaient les rives des Grands Lacs pensaient que sa queue attisait les tempêtes et l'appelaient "Erielhonan", ce qui signifie . Le nom du lac Érié dérive de cette appellation. Le félin est discret, il ne chasse qu'à la tombée de la nuit ou au lever du jour : c'est pourquoi il a été aussi surnommé le (' en anglais). Lorsque Christophe Colomb découvrit le puma, il crut que c'était un lion : les Américains l'appellent encore ', . En anglais, le Puma est également appelé , , et . Le président américain Theodore Roosevelt le surnommait le 

Les civilisations précolombiennes vénéraient le puma comme un dieu ou un être surnaturel, à l'instar du jaguar. Dans les Andes, le dieu Viracocha est représenté par le motif du puma sur la porte du Soleil de Tiahuanaco. Pour les Incas, lors des éclipses de Soleil, Int, dieu du soleil était dévoré par un monstre céleste assimilé à un puma. Les pumas étaient vus comme les représentants des dieux de la montagne. Lorsque ce phénomène céleste se produisait, les paysans des Andes faisaient là encore un maximum de bruit mais cette fois, pour effrayer le félin. Le nom du lac Titicaca signifie le « lac des pumas de pierre ». Les plans de la ville de Cuzco au Pérou auraient été conçus en reprenant la silhouette du félin.

Les Anasazis lui vouaient un culte. Au Nouveau-Mexique, les ont sculpté deux pumas en pierre grandeur nature pour un autel et les Zuñis emportaient avec eux des amulettes en pierre représentant le félin. D'autres peuples le chassaient pour s'en nourrir ou pour sa peau. Dans les croyances animistes des peuples d'Amérique du Nord, l'esprit du puma est celui du chef qui s'impose sans utiliser la violence ou la contrainte. Il est un modèle de persévérance et de détermination, car il attend patiemment le passage d'une proie du haut d'un arbre ou d'un rocher. Le puma est un animal très vénéré de la mythologie andine. Il y occupe une place similaire à celle du lion dans le bestiaire occidental.








</doc>
<doc id="15048" url="https://fr.wikipedia.org/wiki?curid=15048" title="Anarchisme individualiste">
Anarchisme individualiste

L’anarchisme individualiste, parfois anarchisme égoïste, ou individualisme libertaire est un courant de l'anarchisme qui prône la liberté des choix de l'individu face à ceux, généralement imposés, d'un groupe social.

On emploie généralement le terme d'anarchisme individualiste dans un souci de distinction avec l'acception courante d'égoïsme.

Dans ce cadre, l'égoïsme est la doctrine de l’Ego, de la primauté donc de l'individu et de son expérience sur tout autres concepts. Cette philosophie est à rapprocher du solipsisme, du nihilisme de Gorgias et du scepticisme antique, entre autres.

Comme l’indique Errico Malatesta : « Tous les anarchistes, à quelque tendance qu’ils appartiennent, sont d’une certaine façon individualistes. Mais la réciproque est loin d’être vraie : tous les individualistes ne sont pas, tant s’en faut, des anarchistes ».

Selon E. Armand, l’anarchisme individualiste se définit comme « La négation, le rejet, la haine de la domination et de l’exploitation ; l’absence de l’obligation, de la sanction et de l’empiétement dans tous les domaines ; l’abolition de la contrainte grégaire sur l’initiative et l’impulsion individuelles ».

Cette philosophie politique voit dans toute forme de pouvoir telle que l'État, la Religion, mais aussi dans toutes sortes d'organismes ou organisations collectives hiérarchiques, une autorité illégitime et oppressive et donc l'ennemi par excellence de la liberté individuelle, donc de l'individu.

Les anarchistes individualistes considèrent la libre association entre individus comme étant la seule forme légitime d'organisation collective dans la mesure où chacun de ses membres reste libre au sein de l'association, ne subissant pas en particulier l'autorité d'une minorité au sein de celle-ci suivant le principe de l'autogestion. Cette conception de l'organisation oppose singulièrement les anarchistes individualistes aux différentes formes d'anarchisme de type collectiviste.

Le concept de propriété est, selon Max Stirner, défini de différentes manières suivant le type de droit dont il est issu. Considérant que les droits étatiques, tant les droits humains que le droit naturel, sont des droits imposés, et donc étrangers à l'individu, Max Stirner prend le droit au sens étymologique, qui désigne la volonté de l'individu. Ainsi la propriété doit correspondre à la volonté individuelle, volonté qui est matérialisée par la force physique individuelle : De ce point de vue, les anarchistes s'opposent radicalement au communisme, car ces derniers pensent au contraire que la terre doit appartenir à ceux qui la cultivent. Pour résumer la philosophie anarchiste individualiste, elle consiste donc à accomplir sa volonté égoïste/individuelle.

Tenter de définir l'anarchisme individualiste est malaisé car, comme l'a justement écrit E. Armand, « On ne trouve guère deux anarchistes individualistes défendant les mêmes théories ». Au sein de l'anarchisme, l'individualisme s'oppose franchement aux courants liés à la gauche politique et sociale, principalement le socialisme libertaire, le communisme libertaire et l'anarcho-syndicalisme.

Cela ne signifie pas que l'individualisme anarchiste soit associé à la droite ou au conservatisme (si on exclut évidemment l'anarcho-capitalisme qui est en réalité davantage une forme de libéralisme radical qu'un courant anarchiste ). Les individualistes s'opposent pour la plupart radicalement au capitalisme et se placent par delà le binôme « "gauche/droite" » hérité de la Révolution française.

Les anarchistes individualistes considèrent l'individu comme seule réalité et comme principe de toute évaluation. Mais contrairement aux individualistes libéraux, les anarchistes individualistes comprennent l'individu comme l'Unique, l'individu réel, existant, effectif, différent de tous les autres par son existence, et non comme un concept, une idée générale. Cet individu est son propre principe directeur et ne demande pas à être reconnu comme « Homme ». L'individu, l'Unique existe en-soi et par soi, et ne saurait être réduit à aucun concept. Toute tentative de réduction de l'individu à un concept, aussi séduisant soit-il, constitue pour l'individu une coercition inacceptable, une tentative de négation de ce qu'il est. Par exemple, la pression à « être quelqu'un de bien » n'est qu'une tentative de restreindre la richesse de l'Unique à un cadre moralisateur.

Ainsi, l'individualisme anarchiste est foncièrement anti-humaniste. Ne voir en l'individu que l'Homme ou la Personne, ne respecter que l'Homme en lui, c'est ne voir que ce qu'il a de commun avec les autres. C'est ne voir que ses ressemblances avec les autres et tenter de fabriquer une identité fictive à partir de la description de ces ressemblances. Bref, « l'homme » est un concept froid, abstrait, un « fantôme » dans le vocabulaire stirnérien, alors que l'individu, lui, est ce qui existe réellement.

La plupart des individualistes anarchistes font une nette distinction entre la Société et l'association entre individus. Pour eux, la libre association est un instrument de l'individu, alors que la Société est un de ses oppresseurs. La Société veut passer pour sacrée, elle se sert des individus. L'association, au contraire, est à leur service. Une association d'individus est donc pensable si elle reste un moyen pour eux de satisfaire leurs intérêts communs en unissant leurs forces. Mais elle ne doit jamais rester une instance autonome, obligatoire, permanente, supérieure à l'individu au sens où elle poursuit ses propres fins au détriment de l'individu. L'association doit donc être petite, limitée, informelle, ouverte et temporaire.

L'individualisme anarchiste s'oppose généralement à l'idée révolutionnaire, les rêves de Grand Soir étant jugés potentiellement répressifs. Les anarchistes individualistes croient généralement que les mouvements d'insurrection sombrent fatalement dans une organisation militarisée aux antipodes de l'intérêt de l'individu. C'est donc à l’individu lui-même de se libérer en rejetant la société dominatrice. Pour beaucoup d’individualistes, être anarchiste signifie être un « en dehors » et vivre selon ses propres principes, en refusant de collaborer aux institutions oppressives et en refusant toute forme d'embrigadement qui pervertit l'idéal libertaire sous prétexte de le servir. 

Concrètement, les anarchistes individualistes proposent deux grands types de moyens d'action : d'une part, l'objection de conscience généralisée et la mise en pratique de modes de vie en rupture avec les principes autoritaires, et de l'autre la pédagogie libertaire. La conjonction de ces deux stratégies a été qualifiée par Gaetano Manfredonia de « modèle éducationniste-réalisateur ».

La première des stratégies proposées par les individualistes anarchistes est basée sur l'insoumission, l'objection de conscience et la mise en pratique immédiate de modes de vie anti-autoritaires, notamment dans les communautés libertaires. Ainsi, l'individualiste n'obéit que par nécessité, que lorsque sa propre préservation est en cause. Mais lorsque l'État présente comme des devoirs civiques certaines actions (comme le vote), il refuse de répondre à son appel. L'individualiste anarchiste refuse de participer à ce qu'il désapprouve et remet fortement en cause, par ce refus et par ces gestes quotidiens, la légitimité de l'État.

De plus, les anarchistes individualistes préconisent la mise en application immédiate des principes libertaires de la libre association. Selon eux, il est non seulement utopique de croire, à l'instar de Bakounine, que nous ne pouvons être libres tant que tous les individus ne le seront pas, mais une telle croyance condamne également l'individu au sacrifice de soi à une cause extérieure à lui-même, ce qui est inacceptable. Il importe donc de créer immédiatement des zones de liberté expérimentale dans les espaces négligés par l'État, des expériences anarchistes dont le caractère temporaire et insaisissable garantit l'authenticité.

L'expérience immédiate de la liberté passe, pour les anarchistes individualistes, par l'exploration de modes de vie et de valeurs anti-autoritaires, que ce soit par le végétarisme, la création de milieux libres, ou de Zone autonome temporaire, et par des pratiques transgressives, l'amour libre, le naturisme, etc.

D'autres anarchistes individualistes pensent que le préalable à la libération sociale est le changement, non imposé, des individus. Selon eux, on ne peut concevoir une société libre sans la formation d'individus nouveaux, ayant bénéficié d'une éducation spécifique. Ils proposent donc l'éducation intégrale des enfants au moyen d’institutions indépendantes de l’École, de l’Église et de l’État.

Max Stirner est considéré comme le fondateur et le premier théoricien de l'individualisme anarchiste, même s'il se défend dans "L'Unique et sa propriété" d'être anarchiste. En fait, l'individualisme stirnérien a eu peu d'impact sur le développement de l'anarchisme au . Ce n'est qu'avec la parution des ouvrages de John Henry Mackay que l'individualisme stirnérien est redécouvert, vulgarisé et adapté aux revendications anarchistes. Mackay peut ainsi être considéré comme l'un des principaux initiateurs de la tendance anarcho-individualiste.

Les théories unicistes de Stirner ont été lues, commentées et assimilées principalement en France et aux États-Unis, où elles ont donné naissance à deux types distincts d'individualisme anarchiste.

N'ayant été que très peu exposée aux théories holistes radicales portées par le mouvement ouvrier européen, la pensée individualiste américaine évolue, au cours du , d'un libéralisme influencé par John Stuart Mill et Spencer vers une position ultra-libérale, anti-étatiste et anti-autoritaire. S'appuyant sur des expériences pratiques de libre entreprise privée (comme c'est le cas pour Josiah Warren), de vie en autarcie (comme l'a expérimenté Thoreau à Walden Pond) ou d'actions juridiques antiétatistes (comme celles de Lysander Spooner), ce courant libéral, exposé à l'individualisme stirnérien et au mutualisme proudhonien, se mue en une forme d'anarchisme original et spécifiquement américain.

Deux penseurs font figures d'inspirateurs, Josiah Warren et James L. Walker, qui posent dès les années 1860 les bases d'une philosophie faisant de l'égoïsme l'unique base de toutes les actions humaines. Les thèses anarchistes de Warren et de Walker sont ensuite reprises par Lysander Spooner et surtout Benjamin Tucker qui, bien avant Mackay, a redécouvert et vulgarisé l'égoïsme stirnérien. 

De façon générale, les anarchistes individualistes américains préconisent la libre association et rejettent les révolutions violentes. Ils optent plutôt pour la résistance passive et le refus d'obéissance comme moyen de faire advenir l'anarchie. Par exemple, Tucker préconise le refus de payer l'impôt ainsi que la création de coopératives indépendantes, pratiquant le libre-échange commercial et même la fondation d'un système bancaire dégagé de l'emprise de l'État. Les anarchistes individualistes américains ne sont donc pas opposés par principe à la propriété privée mais en critiquent l'utilisation qu'en font les institutions de domination sociale que sont la grande entreprise et l'État. Reconnaissant pour l'essentiel la notion de possession telle que définie par Proudhon, ils ne s'opposent en réalité qu'à la nue-propriété, et donc à tout revenu de prêt - tels que les bénéfices ou les loyers et le salariat - tout en reconnaissent à chacun le droit de posséder son logement ou de travailler sa terre. C'est, selon les anarchistes individualistes, en ce sens l'usage seul qui fonde et légitime la propriété individuelle.

Après la Seconde Guerre mondiale, les principaux thèmes de la pensée individualiste anarchiste américaine ont été repris par Ayn Rand qu'on peut considérer comme la fondatrice du libertarianisme et de l'anarcho-capitalisme. Ses disciples, dont Murray Rothbard est le plus brillant représentant, proposent à partir des années soixante une forme radicale de libéralisme économique préconisant le remplacement du gouvernement par une simple agence rétribuée, chargée de protéger les individus, et un capitalisme libéré de toute ingérence étatique. Les héritiers de l'individualisme anarchiste américain sont actuellement divisés entre les minarchistes du parti libertarien et anarcho-capitalistes qui souhaitent la dissolution de l'État dans le marché par la prise en main de ses pouvoirs par l'entreprise privée.

En France, la philosophie de Stirner se développe dans un terreau riche d'une longue tradition de luttes sociales. Alors que l'individualisme stirnérien se greffe aux États-Unis sur un support libéral et capitaliste, ce même individualisme se greffe en France sur un support plus révolutionnaire et résolument anticapitaliste. L'individualisme anarchiste français conserve donc des préoccupations sociales et égalitaires qui n'apparaissent pas chez les individualistes anarcho-capitalistes américains. Certains anarchistes comme Charles-Auguste Bontemps vont jusqu'à parler d'individualisme social, en considérant le Marché et la Propriété comme des fantômes stirnériens, des idées oppressives qui exigent le sacrifice de l'individu.

Ce double héritage fait que certains anarchistes américains, collaborateurs de la revue "Anarchy, a Journal of Desire Armed" (comme Jason McQuinn, Hakim Bey ou Bob Black) refusent l'étiquette individualiste même si leur pensée rejoint pour l'essentiel celle des individualistes anarchistes français, principalement par souci de se démarquer des libertariens et des anarcho-capitalistes qui ont usurpé l'étiquette anarchiste individualiste aux États-Unis dans l'après-guerre. Ils se disent alors partisans de l"'anarchie post-gauchiste" ("Post-Left Anarchy") ou de l'anarchie, tout simplement.









</doc>
<doc id="15049" url="https://fr.wikipedia.org/wiki?curid=15049" title="Karel Čapek">
Karel Čapek

Karel Čapek (), né le à Malé Svatoňovice dans la région de Hradec Králové en Bohême et mort le à Prague, est l'un des plus importants écrivains tchéques du . Le mot "robot", qui apparaît pour la première fois dans sa pièce de théâtre de science-fiction "R. U. R. en 1920", sous-titre en anglais du titre tchèque "Rossumovi univerzální roboti", a été inventé par son frère Josef à partir du mot tchèque "robota", qui signifie « travail » ou « servage ».

Dans une autre de ses œuvres, "La Guerre des salamandres", Čapek peint avec un humour noir et joyeux la géopolitique de son temps, et tourne notamment en dérision le national-socialisme.

Karel Čapek naît en Bohême et fait ses études secondaires à Hradec Králové, qu'il doit quitter pour Brno à la suite de la découverte du cercle anti-autrichien dont il faisait partie. Il étudie à la faculté de philosophie de l'Université Charles et à l'Université Friedrich Wilhelm à Berlin puis à la faculté des lettres de l'université de Paris. Sa thèse, soutenue en 1915, porte sur "Les méthodes esthétiques objectives en référence aux arts appliqués".

Il est réformé en raison de problèmes de dos (qu'il aura durant le reste de sa vie), et dispensé de participer aux combats lors de la Première Guerre mondiale qui néanmoins l'influencent et l'inspirent. En 1917, il est tuteur du fils du comte Lazansky puis journaliste pour les journaux "Národní listy" (1917–1921), "Nebojsa" (1918–1920), "Lidové noviny" (à partir de 1921).

Il publie en 1922 le roman T"ovarna na Absolutno" ("La fabrique d'Absolu").

Dans ce roman entre science-fiction et fantaisie satirique dirigée contre l'intégrisme religieux, il imagine qu'en tentant de désintégrer les atomes pour réaliser des générateurs d'énergie , l'homme sépare accidentellement l'Esprit de la Matière. Les « carburateurs », source d'énergie simple, bon marché et d'emploi universel, sont produits en très grande quantité, générant des profits colossaux. L'Absolu se répand alors comme un gaz toxique et contamine la population mondiale, en commençant par les classes intellectuelles et urbaines. Seuls les paysans tchèques, obstinément matérialistes et attachés à vendre à meilleur profit leurs pommes de terre, semblent échapper à la folie qui bientôt embrase le monde. Une véritable fièvre de la spiritualité religieuse, toutes religions confondues, s'empare de la planète, les sectes se développent, l'irrationnel prévaut et déclenche une série de guerres de religion sanglantes et de révoltes menées par des illuminés mystiques, avant que la destruction systématique des « carburateurs » atomiques ne ramène la paix dans un monde dévasté. 

De 1925 à 1933 il est président du PEN club tchécoslovaque.

Le , il se marie avec son amie, l'actrice Olga Scheinpflugová, rencontrée pendant l'été 1920.

En 1936 il publie "La Guerre des salamandres" qui met en scène une guerre entre l'homme et l'animal ; c'est une satire du contexte politique de l'époque, le nazisme, l'antisémitisme, la croyance dans le progrès. L'œuvre anticipe également les problèmes écologiques.

En 1938, à la suite des accords de Munich, l'annexion des Sudètes par les troupes nazies touche profondément le démocrate nationaliste qu'il est.

Après s’être remis du premier choc, il essaie d’excuser les actes du gouvernement et du président dans la situation, qui selon Čapek, n’offrait pas d’autres solutions excusables. Il considére comme déplacé, dans la situation de l’époque, de chercher les coupables. Il s’efforçe, par ses activités, d’empêcher la division du peuple, et tâche de maintenir son union. Après l’abdication du président Beneš, il reste le seul symbole visible de la Première République et joue souvent le rôle de « bouc émissaire ». Il reçoit de nombreuses lettres et appels téléphoniques anonymes d’insultes. Les vitres de sa maison sont régulièrement cassées. Le il publie, à la suite des attaques contre sa personne, son essai "Comment ça s’est passé" dans "Lidové noviny" (Le Quotidien du peuple). 

Il passe les trois dernières années de sa vie à Stará Huť u Dobříše, où l'on trouve aujourd’hui un monument à son nom. Il meurt d'un œdème pulmonaire quelques mois avant son arrestation planifiée par la Gestapo. Il est enterré au cimetière de Vyšehrad à Prague. 
Il était le troisième sur la liste de la Gestapo des personnes à arrêter et seule sa mort précoce le délivre du destin tragique qui l'attendait. Son frère Josef est arrêté pour activités anti-fascistes et envoyé en camp de concentration en 1939, peu après l'invasion de la Tchécoslovaquie qui fait suite aux accords de Munich. Josef meurt au camp de Bergen-Belsen en avril 1945.

Ses œuvres sont mises à l'index durant les années d'après-guerre par le régime communiste qui considère d'un mauvais œil cet auteur anti-totalitaire, qui avait publié un article, "Pourquoi je ne suis pas communiste", en 1924.

En 1995, il avait reçu, "in memoriam", l’Ordre de Tomáš Garrigue Masaryk.

Čapek était un très bon photographe amateur dont témoignent, mis à part de photos connues publiées dans "Dášenka" (recueil de proses sur la vie d’un chiot Dášenka), des photos de personnalités (du président Masaryk et d’autres "pátečníci" (Les hommes de vendredi, le cercle littéraire et politique qui se réunissait dans le jardin de maison de Karel Čapek tous les vendredis après midi de 1921 jusqu’à sa mort). 
Paradoxalement, Karel, photographe amateur, fut l'auteur d’une des publications photographiques les plus vendues de l’époque de la Première République "Dášeňka čili Život štěněte" publiée en 1933.

Moins connue est sa passion pour la musique ethnique en lien avec son intérêt pour les cultures étrangères. C'était un collectionneur important ; toute sa collection fut offerte par ses héritiers en 1981 à Náprstkovo muzeum (en tout, 462 vinyles 78 tours, et 115 catalogues de maison de disques mondiales). Après 1990, les enregistrements furent digitalisés avec le soutien de l‘UNESCO et les meilleurs ont été édités sur cinq CD.

La paternité du mot « robot », terme qui s’est répandu dans le monde avec la pièce de théâtre "R.U.R." appartient à son frère Josef. Karel Čapek pensait plutôt au mot « laboř ». Le mot robot est d’origine slave et provient du verbe "robotovat" (travailler).

En 1989, un film biographique sur Karel Čapek est sorti au cinéma, "Člověk proti zkáze" ("L’Homme contre la destruction"). Les réalisateurs étaient Štěpán Skalský et Jaromír Pleskot. Le héros principal était interprété par Josef Abrhám, son frère Josef par František Řehák, Olga Scheinpflugova interprétée par Hana Maciuchová et le personnage de T. G. Masaryk, par Svatopluk Beneš.

Ce n'est qu'en 2009, donc 70 ans après la mort de Čapek, qu'a été éditée la riche correspondance entre l’auteur et l’avocat Jindřich Groag sur le thème du pacifisme et du refus du service militaire.

Karel Čapek fut proposé sept fois pour le prix Nobel de littérature, entre 1932 et 1938. 

Avec originalité et malice, ses livres révèlent sa fine connaissance de l'homme et de ses profondeurs troubles.

Karel Čapek écrivit avec humour et intelligence sur une grande variété de sujets. Son œuvre n'est pas uniquement connue pour sa description exacte de la réalité mais pour ses études sur la langue tchèque, et deviendra immortelle pour avoir participé à la naissance de la science-fiction avant qu'elle ne devienne un genre littéraire à part entière.





Comme toute l'intelligentsia tchécoslovaque de son temps, Čapek est francophile et participe à la diffusion de la culture française dans son pays.





</doc>
<doc id="15058" url="https://fr.wikipedia.org/wiki?curid=15058" title="Gulab jamun">
Gulab jamun

Le gulab jamun ( ; ) est un dessert originaire d'Inde. Il est présenté sous la forme de boulettes de pâte, cuites dans l'huile et servies avec un sirop épais. Il est parfumé de cardamome. On l'appelle "lal mohan" au Népal et on le sert parfois avec du yaourt.

Le mot "gulab" se réfère au sirop parfumé à l'eau de rose, tandis que le "jamun" est un fruit indien d'apparence semblable à ce dessert.



</doc>
<doc id="15061" url="https://fr.wikipedia.org/wiki?curid=15061" title="Tribu borélienne">
Tribu borélienne

En mathématiques, la tribu borélienne sur un espace topologique est la plus petite -algèbre sur contenant tous les ensembles ouverts. Les éléments de la tribu borélienne sont appelés des boréliens. Un borélien est donc une partie de , dont le complémentaire est également un borélien, ainsi que union d'une quantité dénombrable de boréliens.

Le concept doit son nom à Émile Borel, qui a publié en 1898 une première exposition de la tribu borélienne de la droite réelle.

La tribu de Borel peut, de manière équivalente, se définir comme la plus petite -algèbre qui contient tous les sous-ensembles fermés de .

Si la topologie de admet une prébase dénombrable , alors la tribu borélienne associée à est aussi engendrée par .

Étant donné un sous-ensemble de , la tribu borélienne de pour la topologie induite est identique à la trace sur de la tribu borélienne de . Cela se prouve en une ligne si on applique le lemme de transport à l'injection canonique de dans .

Sur un produit de deux espaces topologiques et , la tribu produit des tribus boréliennes de et est toujours incluse dans la tribu borélienne du produit. Quand et sont à base dénombrable, il y a même égalité. On trouvera plus de détails à l'article « tribu produit ».

Un exemple particulièrement important est la tribu borélienne de l’ensemble des nombres réels. La tribu des boréliens sur l'ensemble des nombres réels est la plus petite -algèbre sur ℝ contenant tous les intervalles.

La tribu borélienne est aussi engendrée par les intervalles ouverts de la forme , où parcourt ℝ ; il suffit même de considérer dans une partie dense de ℝ comme ℚ l’ensemble des rationnels.

De la même façon, en dimension quelconque, la tribu des boréliens sur ℝ est engendrée par les pavés. De nombreuses variantes sont possibles, ainsi la tribu borélienne de ℝ est également engendrée par :


(dans chacun des exemples, on peut se borner à utiliser des nombres rationnels : toutes ces familles génératrices sont donc dénombrables).

On appelle tribu borélienne (ou tribu de Borel) la tribu engendrée par les ouverts. Elle permet de définir la mesure borélienne, qui correspond à la notion intuitive de longueur, surface, volume, etc. (la dénomination "mesure borélienne" peut varier suivant les auteurs, voir Mesure de Borel (homonymie) ).

La mesure borélienne n'est pas complète puisque la tribu borélienne n'inclut pas certains éléments négligeables. Lorsqu'on complète la mesure borélienne, on obtient la mesure de Lebesgue.

La mesure de Lebesgue formula_1 et la mesure borélienne formula_2 coïncident sur la tribu des boréliens. Et, si on a formula_3 et formula_4 où formula_5 , on définit formula_6, et on obtient que formula_7.

La tribu de Lebesgue formula_8 est la tribu sur laquelle est définie la mesure de Lebesgue. C'est donc la tribu borélienne formula_9 à laquelle on ajoute tous les sous-ensembles de formula_10 inclus dans un sous ensemble de mesure nulle (pour la mesure borélienne formula_2).

formula_12Par conséquent, formula_13.

Un sous-ensemble de est un borélien s’il peut être obtenu à partir d'ensembles ouverts en effectuant une suite dénombrable d’opérations d’unions, d’intersections et de passage au complémentaire, mais, contrairement à l’intuition première, on n'obtient pas ainsi, loin de là, tous les boréliens (quoiqu'on obtienne tous les boréliens usuels) ; en effet, la classe obtenue selon ce schéma de construction n'est pas stable pour les réunions et intersections dénombrables, et il faut, pour obtenir tous les boréliens, itérer transfiniment ce schéma ; pour plus de détails, voir les articles « tribu engendrée » et « hiérarchie de Borel ».

Cette construction permet de prouver que la tribu borélienne de ℝ a la puissance du continu.

Un espace mesurable est dit lusinien ou standard s’il est isomorphe à une partie borélienne d'un espace polonais muni de la tribu induite par la tribu borélienne. Un théorème de Kuratowski assure que

Ainsi, du point de vue de la structure borélienne, tous les espaces non dénombrables usuels sont indistinguables : ℝ est isomorphe à tous les ℝ, à l’espace de Baire ℕ, au cube de Hilbert , à l’espace de Cantor , à l’espace de Banach séparable (espace vectoriel des fonctions continues de dans ℝ, muni de la norme de la convergence uniforme), etc. — quoique ces espaces soient très différents du point de vue topologique ou algébrique.


</doc>
<doc id="15067" url="https://fr.wikipedia.org/wiki?curid=15067" title="La Liberté éternelle">
La Liberté éternelle

La Liberté éternelle (titre original : "Forever Free") est, vingt-cinq ans plus tard, la suite de "La Guerre éternelle". Ce roman est une odyssée à travers les arcanes de la relativité, de la conscience humaine et de la métaphysique où Joe Haldeman prolonge les enjeux du roman d'origine et de sa nouvelle "Une guerre à part". Il est paru en langue originale en puis en France en .

Ce roman est adapté en une série de trois bandes dessinées par Marvano sous le titre "Libre à jamais".

Vingt ans après leur retour à la vie civile, William Mandella et Marygay Potter vivent parqués sur une planète perdue. Ils ont refusé l'uniformisation proposée par « Homme », entité collective à la conscience unique mais formée de milliards d'individus, ayant remplacé l'ancienne humanité à l'issue de la Guerre Éternelle.

Afin d'échapper à l'insidieuse dictature d'« Homme » et de ses alliés taurans, un groupe de vétérans mené par William et Marygay conçoit le projet de s'emparer d'une navette temporelle.




</doc>
<doc id="15069" url="https://fr.wikipedia.org/wiki?curid=15069" title="Libre à jamais">
Libre à jamais

Libre à jamais est une série de bande dessinée publiée par les éditions Dargaud, inspirée du roman "La Liberté éternelle" de Joe Haldeman et illustrée par Marvano. C'est la suite de "la Guerre éternelle" publiée aux éditions Dupuis.

Cette histoire, basée sur la nouvelle "Une guerre à part" de Haldeman, revient sur le moment où Marygay et William sont affectés à deux postes éloignés : elle vers le collapsar Aleph-10 alors que lui partira en campagne sur Sade-138. années-lumière les sépareront.

Marygay ignore que Homme, la race des clones qui gouverne désormais la Terre a conclu un compromis contre nature avec ses ennemis, les Taurans. Une seconde guerre éternelle se prépare-t-elle ? Le pire ennemi des humains ne serait-il pas Homme ?



</doc>
<doc id="15070" url="https://fr.wikipedia.org/wiki?curid=15070" title="Dallas Barr">
Dallas Barr

Dallas Barr est l'adaptation en bande dessinée par Marvano du roman de Joe Haldeman, "Immortalité à vendre". Cependant les auteurs n'ont repris que les personnages principaux et les idées clés du roman et en ont nettement modifié l'intrigue.

L'aventure débute en 2075. À cette époque, un homme dénommé Julius Stileman a mis au point une cure permettant de bénéficier d'une nouvelle jeunesse.

Mais cette cure n'est efficace que pendant 10 ans et pour pouvoir bénéficier de cette tranche d'immortalité, il faut débourser au minimum un million de livres. Au minimum, car en fait, il faut donner tout ce que l'on possède, après quoi on a dix ans pour essayer d'accumuler 1 million de livres pour s'offrir une nouvelle cure. Tous les coups sont alors permis mais pas de prêt et pas de dons, telle est la règle. Si un patient ne suit pas son traitement décennal, les années qu'il a « volées » le rattrapent très rapidement et il retrouve son âge véritable.

Dallas Barr, cent trente-deux ans, est après Lord Julius Stileman le deuxième homme le plus vieux de la Terre. Malgré son amitié très ancienne avec le patron de la puissante "Stileman Enterprise", Dallas ne bénéficie d'aucune faveur pour payer sa cure. De ce fait, il est régulièrement obligé de servir d'homme de mains à son ami afin de pouvoir rester jeune et en vie. Le premier album de la série commence justement dix jours avant la fin de sa sixième cure de rajeunissement. Et il vient de perdre son dernier sou au poker.





</doc>
<doc id="15076" url="https://fr.wikipedia.org/wiki?curid=15076" title="Mise en œuvre">
Mise en œuvre

La mise en œuvre est le fait de mettre en place.

En ingénierie et plus particulièrement en informatique, la mise en œuvre désigne la création d’un produit fini à partir d’un document de conception, d’un document de spécification, voire directement depuis une version originelle ou un cahier des charges.

L’utilisation de l’anglicisme « implémentation », de l'anglais "to implement", est courante (et acceptée) et reflète la volonté de ne pas tomber dans la traduction ambiguë que serait l'utilisation du verbe "implanter". Cependant, selon certaines personnes, cet anglicisme est impropre car souvent utilisé à tort pour qualifier des actions très différentes.

La mise en œuvre doit répondre à des contraintes qui lui sont propres et qui ne sont généralement pas toutes explicites dans les documents précités :
Plus techniquement, on peut citer :

Comme ces contraintes sont difficilement conciliables, une expression classique dit : « Prix, performances, fiabilité, en choisir deux ».

Certains concepts sont tellement particuliers, par leur complexité ou leurs exigences matérielles, qu’il n’en existe pas de mise en œuvre satisfaisante pendant une longue période. Des exemples classiques sont les compilateurs Ada, le microprocesseur Intel iAPX-432 ou le système d'exploitation Multics. Des exemples plus contemporains sont le support du standard C++ par les logiciels de développement ou des langages et par les navigateurs web.



</doc>
<doc id="15077" url="https://fr.wikipedia.org/wiki?curid=15077" title="God Save the Queen">
God Save the Queen

Il était par le passé l'hymne national de la majeure partie des pays du Commonwealth ; bien que la plupart d'entre eux aient maintenant leur propre hymne national, plusieurs (dont l'Australie et le Canada) le reconnaissent en tant qu'hymne royal.

Le souverain régnant ne chante pas cet hymne puisqu'il s'agit de prier pour lui, mais le consort le chante.

Le Royaume-Uni n'a pas d'hymne national officiel, mais le ", possédant une longue histoire d'usage dans cette fonction, est utilisé par le gouvernement comme hymne national.

Dans ", Percy Scholes cite un morceau de clavier de John Bull (1619) qui a de fortes similitudes avec l'air moderne, selon le placement des altérations qui, à cette époque et dans certains cas, étaient non écrites et laissées à la discrétion de l'interprète. Il indique également que plusieurs morceaux de Henry Purcell, dont un comportant les notes d'ouverture de l'air moderne, contiennent les mots : .

Dans "Les Grotesques de la musique," Hector Berlioz confirme la version selon laquelle la mélodie a été écrite par Jean-Baptiste Lully.

Au Royaume-Uni, la première édition définitive de l'air actuel est apparue en 1744 dans "Thesaurus Musicus". La chanson serait devenue populaire l'année suivante, après le débarquement de Charles Édouard Stuart, qui marqua la fin des espoirs des Stuart de remonter sur le trône anglais. La Maison de Hanovre, victorieuse, adoptera cet air comme hymne royal britannique.

C'est Haendel qui effectua un arrangement de la mélodie composée à l'origine par Jean-Baptiste Lully, lors de l'arrivée de George en Angleterre. Cependant, cet arrangement n'est plus en usage aujourd'hui.

Dans les "Souvenirs" de la marquise de Créquy, la femme de lettres évoque une origine française de l’hymne composé en 1686 pour fêter le rétablissement de Louis XIV après son opération de la fistule anale. L'ancêtre du "" serait la chanson "Grand Dieu sauve le Roi", écrite par Madame de Brinon, supérieure de la Maison royale de Saint-Louis (future école de Saint-Cyr en faveur des orphelines de nobles) et mise en musique par Jean-Baptiste Lully (Haendel de séjour à Versailles en 1714 aurait noté la musique et l'aurait fait traduire par le pasteur Carrey). Cette chanson n'est pas sans lien avec un chant latin, que la chapelle royale exécutait depuis le règne de Louis XIII et qui avait pour titres « Domine, salvum fac regem », dont les paroles étaient exactement tirées du dernier verset du psaume XIX de David, « Domine, salvum fac Regem et exaudi nos in die qua invocaverimus te ». Toutefois, c'est le "Te Deum" qui fonctionnait en tant qu'hymne royal officiel. 

Si la première exécution est officiellement attribuée à l'année 1745, une étude récente trouva une exécution plus ancienne par les royalistes de la maison Stuart en 1688 : "« God Save Great James our King. »" Le chercheur considère que l'origine peut remonter au règne de Charles II († 1660).
Lors du débarquement, en août 1745, au nom de Jacques III Stuart, les partisans de celui-ci l'entonnent à nouveau et dès le mois suivant il est repris par leurs adversaires hanovriens en revendication de la couronne.

Après la mort d'Henry Carey, son fils demanda qu'on reconnaisse la paternité de son père sur cet hymne.

Traduit en allemand en 1790, en pleine période révolutionnaire, perçu alors comme un hymne royal célébrant la royauté, il a été pour cette raison choisi pour être l'hymne du Royaume de Prusse. Le succès de ce chant s'étendit à l'Autriche-Hongrie, pays où il était chanté quotidiennement par tous les écoliers jusqu'en 1918 sous ce titre : « Gott, schütze Unser Kaiser ! ».

Il n'y a pas de version officielle de l'hymne. La monarchie reconnaît aujourd'hui par tradition les premier et troisième couplets du texte ci-dessous comme constituant l'hymne national, et précise que d'autres couplets, « ajoutées au fil des ans, […] sont rarement utilisées ».

Un autre couplet, qui en appelle à l'aide de Dieu pour « écraser les Écossais rebelles », est ajoutée officieusement vers 1745 en réaction à la rébellion jacobite en Écosse. Comme les autres couplets agrégés au texte au fil du temps, il a été populaire sans jamais avoir été officiel, et n'est « plus chanté aujourd'hui ».

"" n'a pas de statut légal au Canada, même s'il est considéré comme l'hymne royal, c'est-à-dire devant être joué en présence d'un membre de la famille royale ou comme partie du salut accordé au gouverneur général et aux lieutenants-gouverneurs. La traduction française de l'hymne est due au journaliste et historien Benjamin Sulte.
En effet, le premier couplet est chanté en français :
En Australie ' était l'hymne national "de jure" jusqu'en 1974, quand "Advance Australia Fair" fut adopté. En 1984, ' fut proclamé l'hymne royal. En Nouvelle-Zélande, "" a statut égal avec "God Defend New Zealand" comme hymne national, mais il est moins souvent joué. Dans les deux pays, il devant être joué en présence d'un membre de la famille royale ou comme partie du salut accordé au gouverneur général et aux lieutenants-gouverneurs.



De nombreux hymnes se chantent sur l'air du ", parmi lesquels :

Le "God Save The Queen" est utilisé comme hymne national par différentes équipes en sport :
Les autres pays constituants le Royaume-Uni, utilisent un hymne différent ("Flower of Scotland" pour les écossais et "Land of My Fathers" pour les gallois). Certaines équipes irlandaises de sports collectifs représentent l'île de l'Irlande (soit l'Irlande du Nord apparentant au Royaume-Uni plus la République d'Irlande), c'est le cas du rugby, du hockey ou encore du cricket. Ils utilisent ainsi un hymne spécialement créé pour cette union : l"'Ireland's Call". Mais toutes les équipes ne sont pas réunies, comme en football par exemple, les nord-irlandais chantent ainsi le "God Save The Queen""."





</doc>
<doc id="15080" url="https://fr.wikipedia.org/wiki?curid=15080" title="DirectX">
DirectX

Microsoft DirectX est une collection de bibliothèques destinées à la programmation d’applications multimédia, plus particulièrement de jeux ou de programmes faisant intervenir de la vidéo, sur les plates-formes Microsoft (Xbox, systèmes d’exploitation Windows).

À l’origine le nom de chacune de ces bibliothèques commençait par Direct, par exemple : Direct3D, DirectDraw, DirectMusic, DirectPlay, DirectSound et ainsi de suite. DirectX étant le nom générique faisant référence à l’ensemble de ces technologies. Avec le temps cette convention de nommage est un peu tombée en désuétude, le X prenant l’ascendant des technologies intitulées Xact, Xinput et qui ont rejoint la grande famille des technologies DirectX. Ainsi lorsque Microsoft s’est lancé dans le développement d’une console faisant un usage intensif de ces technologies, le X était prédominant ce qui a pu conduire au nom Xbox (et par la suite ainsi que l'Xbox One).

Direct3D (la bibliothèque permettant de faire de la 3D temps réel) est largement utilisée dans le développement de jeux pour la plate-forme Microsoft Windows, pour Xbox et . Direct3D est aussi utilisé pour d’autres type d’applications s’appuyant sur des graphismes en 3D de haute qualité pour visualiser des données complexes, par exemple dans le secteur de la CAO/DAO bien que son concurrent OpenGL soit dans ce domaine mieux implanté car existante sur bien plus de plates-formes. Direct3D étant peut-être une des technologies DirectX les plus reconnues, il n’est pas rare de voir le nom DirectX utilisé en lieu et place de Direct3D.

La multitude des composants DirectX existe sous deux formes. L’une uniquement destinée à faire tourner les applications développées en utilisant ces technologies (les ), l’autre étant un kit de développement à l’usage des développeurs. Initialement les composants destinés à l’exécution des applications étaient redistribués avec les jeux qui en faisaient usage, sur les versions récentes, ils se trouvent aussi pré-installés avec Windows. Le SDK est quant à lui proposé en téléchargement gratuit à partir du site de développement de Microsoft (MSDN). La partie runtime est propriétaire et les sources ne sont pas accessibles. Les sources des exemples du SDK sont disponibles avec celui-ci.

Les versions de Direct3D 9Ex et 10 étaient utilisés par Windows Vista. Ces deux versions faisaient usage de fonctionnalités propres au nouveau modèle de pilote apparu avec Windows Vista. La nouvelle infrastructure graphique de Windows supporte la virtualisation du matériel graphique vis-à-vis de multiples applications et services par exemple le (le gestionnaire de desktop de Vista lorsque Aero est activé). Précédemment avec Windows XP, les applications avaient un accès exclusif à la carte graphique et pouvaient accaparer les ressources de celle-ci.

Sur PC, DirectX est actuellement en sur Windows 10, pour , pour Windows Vista et Windows Server 2008 et 9.0c pour toutes les versions antérieures de Windows.

Les composants constituant DirectX sont : 

En 1994, Microsoft était sur le point de lancer son système d’exploitation, . Un des facteurs déterminants du succès de tout système d’exploitation (OS) reste la gamme des logiciels qu’il permet d’exécuter. Trois employés de Microsoft — Craig Eisler, Alex St. John, et Eric Engstrom — étaient alors de fait assez préoccupés qu’un certain nombre de développeurs semblaient penser que l’OS précédent de Microsoft, MS-DOS, constituait une meilleure plate forme pour les jeux, ce qui pouvait signifier que moins de jeux seraient développés pour avec l’impact négatif sur le succès de ce système d’exploitation que cela pouvait impliquer.

MS-DOS permettait un accès direct à la carte vidéo, au clavier, à la souris, aux cartes sons, et à d’autres parties du système contrairement à qui introduisait des notions d’espace d’adressage propre à chaque processus. Microsoft se devait donc de fournir aux développeurs ce qu’ils voulaient ; par ailleurs il était nécessaire de le fournir rapidement, la date de sortie du nouveau système d’exploitation n’étant plus distante que de quelques mois. Eisler, St. John, et Engstrom se mirent alors à travailler sur la solution à ce problème qu’ils appelèrent DirectX.

La première version de DirectX fut livrée au public en sous l’appellation de Windows Games SDK. Il s’agissait du remplaçant pour Win32 de DCI et de l’API WinG disponible pour . Une équipe de développement d’ATI Technologies soumit à l’attention de Microsoft un certain nombre de technologies graphiques spécifiques au monde du jeu. Le développement de DirectX fut conduit par l’équipe de Eisler (chef développeur), St. John, et Engstrom (gestion de projet). Pour résumer, il permit à toutes les versions de Windows depuis de bénéficier de capacités multimédia performantes. Eisler a écrit sur son blog sur la frénésie avec laquelle les versions de à 5 furent réalisés.

Avant l’existence de DirectX, Microsoft avait déjà inclus le support d’OpenGL sur Windows NT. À cette époque, OpenGL nécessitait du matériel haut de gamme qui restait réservé aux utilisateurs disposant de moyen financier important comme dans le monde de l’industrie et plus généralement les utilisateurs de système CAO. Direct3D (introduit par Eisler, Engstrom, et St. John comme une alternative à l’OpenGL de SGI) était destiné à être une technologie compagnon plus légère et spécialisée pour les jeux. La puissance des cartes graphiques et des ordinateurs évoluant rapidement, OpenGL devint de son côté un standard de fait et une technologie accessible à tous. À ce moment-là, l’affrontement a pu faire rage entre les supporters de chacune des deux API, l’initiative de Microsoft étant perçue comme une volonté de marginaliser l’utilisation d’OpenGL (voir Fahrenheit ou Direct3D vs. OpenGL). Quoi qu’il en soit, OpenGL est parfois utilisé conjointement à certaines API DirectX : en effet, OpenGL est le pendant de Direct3D et n’inclut pas nécessairement des fonctionnalités permettant de gérer les entrées clavier ou souris ou le son. Ce même si aujourd’hui des bibliothèques comme SDL remplissent aussi ces besoins et sont aussi largement utilisées.

Dans sa version console, DirectX a été utilisé comme une épine dorsale des API proposés pour développer sur Xbox et . L’API fut développé conjointement par Microsoft et Nvidia, qui fournissait le matériel graphique présent sur la Xbox originale. L’API disponible pour cette version de la Xbox était peu ou prou équivalent à , elle portait le nom DirectXbox, ce qui fut raccourci en Xbox pour obtenir le nom commercial.

En 2002, Microsoft livrait qui bénéficiait d’un support pour des shaders plus long et la des vertex shaders. Depuis Microsoft n’a cessé de faire évoluer DirectX 9 en ajoute par exemple le support du shader avec la version , livrée en .

En , DirectShow a été déplacé du SDK DirectX vers Microsoft Platform SDK. En revanche, DirectX est toujours nécessaire pour compiler les exemples DirectShow.

 est une mise à jour majeure de l’API DirectX. Disponible uniquement à partir de Windows Vista, les versions antérieures de Windows ne peuvent exécuter des applications s’appuyant exclusivement sur cette version de l’API. Les changements introduits par sont profonds, mais seule la partie Direct3D est réellement concernée par ceux-ci.
De nombreuses parties de DirectX API sont considérées comme désuètes dans la dernière version du SDK et ne sont plus présentes que pour des questions de compatibilité : DirectInput s’efface au profit de XInput, DirectSound en faveur de XACT et Xaudio2 et perd son accès direct au matériel sur Windows Vista. En effet la nouvelle pile audio de Vista introduit une nouvelle API intitulé WASAPI sur laquelle les fonctionnalités de DirectSound ont été redirigées. La bibliothèque DirectPlay DPLAY.dll a aussi disparu et a été remplacée par une bibliothèque intitulée dplayx.dll. 

Pour des raisons de compatibilité, plusieurs versions de Direct3D sont installées sur Windows Vista :<br>

Direct3D 9Ex (aussi connu comme 9.0L ou 9.L, le L faisant référence à Longhorn, nom de code de Windows Vista utilisé avant sa sortie) : Cette version permet de bénéficier de certaines fonctionnalités introduites par l’utilisation des pilotes WDDM tout en maintenant la compatibilité avec les applications écrites pour . L’interface Windows Aero s’appuie sur et utilise certaines des fonctionnalités non présentes dans comme la possibilité de partager des surfaces DirectX entre plusieurs processus. 

Microsoft a dévoilé DirectX 11 à la 'Gamefest 08' de Seattle, avec les caractéristiques prévues incluant le support GPGPU (DirectCompute), le Direct3D11 avec tessellation ainsi que l'amélioration du multi-threading, pour aider les développeurs de jeux vidéo à réaliser des logiciels multi-core. Direct3D 11 tourne sur Windows Vista et Windows 7. Il fonctionnera également sur les versions suivantes. Des éléments des nouvelles API comme le 'multi-threaded resource handling' peut être supporté par les matériels Direct3D 9/10/10.1. La tessellation matérielle ainsi que le 'Shader Model 5.0' nécessite Direct3D 11. Microsoft a depuis mis à jour Direct3D 11. Direct3D 11 est un sur-ensemble de Direct3D 10.1 — toutes les caractéristiques de la version 10.1 y sont présentes, les nouvelles fonctionnalités ne sont disponibles que lorsque c'est nécessaire. Ceci pour une meilleure compatibilité.

DirectX 11.1 est présent dans Windows 8. Il supporte WDDM 1.2 pour de meilleures performances, propose une intégration améliorée de Direct2D, Direct3D, DirectCompute, et inclut DirectXMath, XAudio2, ainsi que des bibliothèques XInput framework XNA. Il comprend également le support 3D stéréoscopique.

DirectX 11.2 est une exclusivité Windows 8.1. Cette évolution de l'API apporte le support de WDDM 1.3, un partage dynamique des ressources entre les différentes mémoires de l'ordinateur (nommé Tiled resources) ainsi que d'autres évolutions mineures.

DirectX 11.3 est une nouvelle évolution de l'API qui disposera de certaines fonctions de DirectX 12, surtout au niveau de Direct3D.

Le , Microsoft a officialisé l'annonce de DirectX 12 lors de la Game Developers Conference du 17 au 21 mars à San Francisco. Cette version de DirectX est disponible sur Windows 10, Xbox One et Windows Phone et propose un accès plus bas niveau aux ressources du circuit graphique dans le but d'offrir de meilleures performances et une consommation d'énergie réduite. Le moyen utilisé étant de faciliter la répartition des ressources utilisées, pour mieux exploiter les multi-processus et les multi-cartes graphiques.

Bien que DirectX soit une API conçue pour le système d'exploitation Windows, il existe des pilotes et des bibliothèques logicielles qui fournissent une partie de son API sur d'autres systèmes d'exploitation comme :

 mise à jour Microsoft Vista SP2 et Download details: Update for Windows Vista (KB971512) 7.00.6002.18107 version .

. La mise à jour de Windows 8.0 vers Windows 8.1 apporte DirectX 11.2

Des API comme Direct3D et DirectSound interagissent directement avec le matériel par le biais de pilotes. Les fabricants de matériel doivent écrire ces pilotes pour une version spécifique du DirectX ‘Device Driver Interface’ (ou DDI). Des versions anciennes de DirectX incluaient un certain nombre de mises à jour de pilote DirectX mais cette pratique fut abandonnée au profit de Windows Update qui peut permettre aux utilisateurs de télécharger uniquement les pilotes propres à leur matériel.
Les versions antérieures à étaient vouées à être compatible avec des pilotes anciens, il était possible d’utiliser des versions plus récentes de DirectX avec des pilotes écrits pour supporter une version plus ancienne du DDI. Par exemple, un jeu s’appuyant sur pouvait fonctionner sur une machine équipée d’une carte n’ayant qu’un pilote développé à l’époque de . Par contre avec sur Vista, compte tenu de l’ampleur des changements et du fait que certaines fonctionnalités exposées par l’API ne sont disponibles que grâce au nouveau modèle de pilote, il est impossible d’utiliser une application développée pour cette version de DirectX sans pilote correspondant au nouveau modèle de pilote WDDM. 
Plusieurs versions de DirectX sont préinstallées avec de nombreuses versions de Windows de façon à supporter des applications écrites avec des versions anciennes de cette API tant que celles-ci n’ont pas été réécrites pour la version la plus récente et continuent d’être utilisées.

En 2002, Microsoft mettait à disposition des développeurs une version du SDK DirectX amenant des assembly .Net permettant d’utiliser DirectX à partir de code managé, que cela soit en managed C++, C# ou tout autre langage supporté par le Framework .Net. Ces "assembly" étaient regroupés sous l’appellation « Managed DirectX » (ou MDX), et permettait d’obtenir des performances qui n’avait pas nécessairement à rougir de la comparaison avec ce qu’il était possible d’obtenir en C++ natif. En décembre 2005, février 2006, avril 2006 et août 2006, Microsoft mis à disposition des développeurs des mises à jour successives de cette technologie pour aboutir à la qui n’a jamais existé que sous la forme d’une version bêta ayant expiré le 5 octobre 2006.

Au cours du GDC de 2006, Microsoft présenta le Framework XNA GSE, voué à être la nouvelle bibliothèque permettant aux développeurs souhaitant réaliser des jeux en utilisant un langage .Net d’accéder aux fonctionnalités DirectX. Ce Framework a par ailleurs pour objectifs de faciliter leur travail en regroupant des fonctionnalités qui pouvaient se trouver au préalable éparpillées dans des "assembly" différents. Ce Framework supporte aussi l’exécution des applications produites sur . La version RTM fut livrée le 11 décembre 2006, sous la forme d’un paquet téléchargeable gratuitement. Contrairement à DirectX ou à Managed DirectX, les composants du Framework XNA GSE ou les API issues de la (XInput, XACT) ne sont pas préinstallés avec aucune version de Windows, aussi il est recommandé de les installer en même temps que tout jeu en faisant usage.

Au début de 2013, Microsoft annonce la fin du développement de XNA et son retrait du programme de certification MVP (Most Valuable Professionals) le avril 2014. La communautés open source prend alors le relai avec des framework tel que (étant une implémentation open source de XNA) et des API comme SharpDX faisant des appels à du code natif DirectX (comme le faisait MDX).

Le système DirectX est compatible avec tous les systèmes d'exploitations Microsoft, 32 bits (x86), et 64 bits(x64).

Il existe plusieurs bibliothèques qui couvrent souvent une partie des fonctionnalités de DirectX. Utiliser une combinaison de celles-ci peut permettre d’obtenir le même ensemble de fonctionnalités que DirectX, on peut citer SDL, Allegro, OpenMAX, OpenML, OpenGL, OpenAL, Vulkan, FMOD Ces bibliothèques peuvent présenter l’avantage de permettre de développer des applications portables sur d’autres systèmes d’exploitation que Windows. Ces bibliothèques sont pour la plupart issues de projets .

D’autres projets, comme une partie de Wine, visent à fournir une implémentation alternative du même ensemble de fonctionnalités.



</doc>
<doc id="15083" url="https://fr.wikipedia.org/wiki?curid=15083" title="Kiswahili">
Kiswahili

Le kiswahili ("langue vernaculaire") est une langue bantoue, originaire de la Tanzanie puis qui s'est métissée à d'autres langues africaines et à l’arabe. Elle joue de nos jours un rôle important comme langue véhiculaire dans une grande partie de l’Afrique subsaharienne. Le préfixe "ki" signifie ici « langue », "swahili" désigne la côte, le kiswahili est donc la « langue de la côte ». 

L'Afrique de l'Est et plus particulièrement la Tanzanie fourmillait de langues voisines proches. À partir de 1930, l'administration coloniale, par l'intermédiaire du "Comité sur la langue territoriale", a décidé de normaliser une langue à partir du dialecte de Zanzibar, l’un des plus anciens, le kiunguja. Les langues swahilies étant des langues traditionnellement utilisées comme langue seconde sur le continent et comme langue première sur la frange côtière et les îles.

Au sortir de l'indépendance, pour dépasser les clivages ethniques, Julius Nyerere a choisi cette langue pour être la langue officielle de Tanzanie. L'unification et la standardisation se poursuivent avec l'Institute of Swahili Research de l'University College de Dar es Salaam, et enfin par le Baraza la Kiswahili la Taifa, organe officiel tanzanien. Le swahili, langue première minoritaire mais langue seconde assez répandue, allait constituer un point important de la politique du pays. L'objectif à terme étant de supplanter la langue coloniale, l'anglais, qui s'imposait déjà au parlement. Cette langue ne favorisant aucune ethnie, elle fut donc bien acceptée par la société. À partir de 1965, les campagnes politiques, l'éducation, les séances de tribunaux en première instance, et de nombreux autres secteurs se font en kiswahili, c'est la « swahilisation » de la société. À partir de 1970, le parlement siège dans cette langue. En 1980, l'enseignement secondaire ne se fait plus qu'en kiswahili. Peu à peu, les enfants ne parlent plus la langue de leurs parents puisque le kiswahili est en train de devenir langue première, la langue nationale.

Le kiswahili une langue agglutinante appartenant au groupe des langues bantoues.

Le kiswahili est aussi langue nationale au Kenya, en République démocratique du Congo et langue véhiculaire en Ouganda, au Rwanda, au Burundi, au Mozambique, au Malawi et en Zambie. Il reste compréhensible aux Comores, les langues comoriennes étant des langues très proches.

De nombreux auteurs contemporains écrivent en kiswahili, notamment en Tanzanie et au Kenya. Comme les autres langues swahilies, le kiswahili dispose d’une littérature écrite depuis plusieurs siècles (à l’origine en caractères arabes mais s'écrivant depuis la fin du en caractères latins). Aussi dans les prisons, les "Crips" ont utilisé cette langue aux États-Unis pour être incompréhensibles par les autres gens.




</doc>
<doc id="15085" url="https://fr.wikipedia.org/wiki?curid=15085" title="Tribu (mathématiques)">
Tribu (mathématiques)

En mathématiques, une tribu ou σ-algèbre (lire "sigma-algèbre") ou plus rarement corps de Borel sur un ensemble "X" est un ensemble non vide de parties de "X", stable par passage au complémentaire et par union dénombrable (donc aussi par intersection dénombrable). Les tribus permettent de définir rigoureusement la notion d'ensemble mesurable.

Progressivement formalisées pendant le premier tiers du , les tribus constituent le cadre dans lequel s'est développée la théorie de la mesure. Les exemples les plus fameux en sont les tribus boréliennes, du nom d'Émile Borel, qui construisit la tribu borélienne de la droite réelle en 1898, et la tribu de Lebesgue, formée des ensembles mesurables définis par Henri Lebesgue en 1901. En conséquence, les tribus sont aussi fondamentales en théorie des probabilités, dont l'axiomatisation moderne s'appuie sur la théorie de la mesure. Dans ce domaine, les tribus ne sont pas seulement le support du formalisme, mais aussi un outil puissant, qui est à la base de la définition de concepts parmi les plus importants : espérance conditionnelle, martingales, etc.

Une minorité de sources exigent également que formula_1 ne soit pas vide ; cette hypothèse supplémentaire n'est utilisée à aucun endroit de cet article.

Formellement :

La définition qui précède a l'intérêt d'être lisible sans connaître le langage des algèbres de Boole ; si on le connaît, on peut l'exprimer sous forme plus resserrée :

Le couple formula_10 est appelé espace mesurable ou espace probabilisable en fonction du contexte. Sur les espaces mesurables on définit des mesures ; sur les espaces probabilisables on s'intéresse spécifiquement aux probabilités.

Les parties de formula_1 qui appartiennent à la tribu formula_12 sont appelées ensembles mesurables. Dans un contexte probabiliste, on les appelle événements.






En analyse, l'importance des tribus s'est progressivement affirmée au long des trente premières années du . Le siècle s'ouvre par l'élaboration par Henri Lebesgue de sa théorie de l'intégration. Dans la décennie suivante on commence à exploiter la notion géométrique de mesure en probabilités, Johann Radon construit en 1913 une théorie de l'intégration sur formula_32 qui généralise à la fois celle de Lebesgue et celle de Stieltjes, Felix Hausdorff définit en 1918 la mesure qui porte aujourd'hui son nom en dimensions non entières. Simultanément, on s'efforce de bâtir une axiomatisation abstraite de l'intégration dans laquelle s'intègreraient toutes ces nouvelles théories. Cette unification, réalisée dans le début des années 1930, s'appuie sur la définition moderne d'une mesure. La notion de tribu en est un élément constitutif.
Depuis la publication en 1933 des "Fondements de la théorie des probabilités" d'Andreï Kolmogorov, les probabilités sont solidement ancrées sur la théorie de la mesure. Les σ-algèbres y jouent un rôle incontournable, peut-être plus central qu'en analyse : ici elles ne sont pas seulement un cadre de travail, mais aussi un outil puissant. La preuve de la loi du zéro un de Kolmogorov fournit un exemple relativement élémentaire de leur efficacité.

La théorie des processus stochastiques (l'étude probabiliste de phénomènes variant avec le temps) permet de donner une interprétation intuitive de certaines tribus. Par exemple, supposons qu'on s'intéresse à l'évolution du prix d'un actif financier en fonction du temps. L'espace des événements formula_1 est l'ensemble des évolutions possibles de cet actif, c'est-à-dire des fonctions associant à chaque instant un prix. Pour chaque valeur formula_34 du temps, on définit ainsi une tribu formula_35 : étant donné un ensemble formula_36 d'événements, on décidera que formula_36 est dans formula_35 si on peut le décrire par une formulation qui, lue par un observateur vivant à la date formula_34, ne se réfère qu'au passé. Pour fixer les idées, si formula_36 est l'événement « le cours de l'actif a constamment augmenté pendant l'année 2006 », il appartient à formula_41 puisqu'un observateur vivant en 2010 peut en décider en consultant des archives, mais n'est pas dans formula_42 (sauf à être extralucide, un observateur vivant en 2005 n'en peut rien savoir). On dispose finalement d'une tribu évoluant en fonction du temps, dont la valeur formula_35 représente le niveau d'information disponible à la date formula_34. Sa croissance exprime l'expansion constante de l'information disponible. Cette famille croissante de tribus (on parle de filtration) permet alors de formaliser diverses hypothèses sur le phénomène modélisé (via les concepts d'espérance conditionnelle, de martingale, etc.) puis d'en tirer mathématiquement des conclusions.








On le prouve facilement en remarquant que pour toute suite d'éléments de formula_59 ("a priori" non disjoints) on peut écrire :

formula_60

D'autres sources fournissent une variante de cette proposition, en posant comme troisième condition la stabilité par réunion dénombrable croissante. Si on est familier du vocabulaire défini à l'article « lemme de classe monotone », cet énoncé peut se dire ainsi : tout λ-système qui est aussi un π-système est une σ-algèbre.

Si formula_61 est un ensemble arbitraire de parties de formula_1, il existe alors une plus petite tribu (au sens de l'inclusion) contenant formula_61, notée formula_64 et appelée la "tribu engendrée" par formula_61.

On prouve l'existence de formula_64 en la définissant comme l'intersection de toutes les tribus sur formula_1 qui contiennent formula_61 (cette intersection a un sens, puisqu'au moins une telle tribu existe, à savoir la tribu discrète).

Exemples :



On dispose d'un procédé un peu plus « constructif » de production de formula_64, par application itérée à partir des éléments de formula_61 des opérations d'intersection, de réunion dénombrable et de passage au complémentaire. La « construction » est toutefois techniquement un peu subtile, car il ne suffit pas de répéter cette itération pendant une suite dénombrable d'étapes indexée par formula_82 : on doit faire appel à une technique de récurrence transfinie.

On appelle tribu de Borel ou tribu borélienne sur un espace topologique donné la tribu engendrée par les ensembles ouverts. Dans le cas simple et fondamental de l'espace usuel à formula_83 dimensions, la tribu borélienne de formula_32 est engendrée par une famille dénombrable de parties, les pavés dont les sommets sont à coordonnées rationnelles. Par un résultat mentionné plus loin, elle a donc la puissance du continu — ce qui prouve incidemment qu'elle n'est pas égale à l'ensemble de toutes les parties de formula_32, qui est de cardinal strictement supérieur.

En probabilités, ou dans les théories de l'intégration dérivant de celle de la mesure, la tribu de Borel de formula_86 (ou de la droite achevée formula_87) joue un rôle prééminent : c'est en effet relativement à elle qu'on définit les fonctions mesurables à valeurs réelles ou les variables aléatoires réelles.

Les tribus boréliennes sont le cadre naturel où se rencontrent les théories de l'intégration et la théorie de la mesure, notamment par le théorème de représentation de Riesz qui associe une mesure définie sur la tribu de Borel à certaines fonctionnelles sur un espace de fonctions continues.

Bien que les espaces métriques non dénombrables usuels aient des propriétés topologiques extrêmement dissemblables, toutes leurs tribus boréliennes sont indiscernables. Un théorème de Kuratowski affirme en effet que tous ceux appartenant à une très large classe, les espaces de Lusin, ont des tribus boréliennes isomorphes entre elles et en particulier isomorphes à la tribu de Borel sur la droite réelle. Les espaces de Lusin en tant qu'espaces mesurables sont donc classifiés par leur cardinal.

Sur l'espace formula_88, une autre tribu mérite d'être signalée : la tribu de Lebesgue, dont les éléments sont les
ensembles mesurables au sens de Lebesgue. Cette tribu contient strictement la tribu de Borel, dont elle est la complétée pour la mesure de Lebesgue. Si on accepte d'utiliser l'axiome du choix, elle ne coïncide pas non plus avec l'ensemble de toutes les parties de formula_32.

Comme indiqué un peu plus bas, ceci permet notamment de restreindre une tribu à un sous-ensemble de son univers formula_1. Le lemme de transport est un résultat simple mais utile pour manipuler une image réciproque de tribu définie par une partie génératrice, par exemple une tribu borélienne.

Lorsque plusieurs fonctions partent de formula_91 — typiquement en probabilités, où plusieurs variables aléatoires sont simultanément considérées au départ d'un même espace — il est facile de généraliser la tribu image réciproque : on parle de "tribu engendrée par une famille d'applications" (qui sont souvent des variables aléatoires). On trouvera cette définition à l'article « tribu engendrée ».

La vérification directe est immédiate, mais on peut aussi s'apercevoir que c'est un cas particulier de tribu image réciproque, en l'espèce sous l'injection canonique de formula_92 dans formula_1.

La définition de la tribu produit est le préalable à celle de la mesure produit dont l'usage permet de généraliser à des espaces abstraits les intégrales multiples.

Le concept se généralise à un produit d'une famille infinie d'espaces mesurables.

Comme dans le cas d’un produit fini, on peut alors définir le produit de mesures grâce au théorème d'extension de Carathéodory, mais il faut pour cela des hypothèses sur les espaces mesurables.

Cet outil est plus faible que le précédent mais peut suffire dans certains cas simples, en probabilités notamment. Ainsi, des problèmes tels que le problème du collectionneur de vignettes ou le jeu de pile ou face infini s’étudient sur des espaces probabilisés de la forme (Ω, σ("C"), P) où l’on pose Ω l’univers des possibles au tirage ({Pile, Face} pour le pile ou face infini ; [1,"k" ] pour le problème du collectionneur si les vignettes sont étiquetées de 1 à "k"), P une probabilité (par exemple la loi uniforme) sur l’espace probabilisable formula_94, formula_95 l’univers des possibles pour l’ensemble du tirage aléatoire, σ("C") et P la tribu cylindrique et la probabilité obtenues par le procédé décrit précédemment.

_\mu</math> défini par :

formula_96

Le résultat de la complétion dépend de formula_98, puisque la notion de partie négligeable n'a de sens que vis-à-vis d'une mesure bien précisée.

La construction généralise dans un cadre abstrait la situation de la tribu de Lebesgue relativement à la tribu borélienne de ℝ (sous la mesure de Lebesgue).

Pour élément de , on définit l"'atome" de relativement à la tribu par :

formula_99

En utilisant seulement la stabilité de formula_59 par passage au complémentaire, on vérifie que les atomes constituent une partition de (les atomes sont donc les classes d'équivalence d'éléments de , pour la relation d'équivalence : « appartenir exactement aux mêmes éléments de formula_59 »). On voit également que tout élément de formula_59 est réunion d'atomes mais qu'un atome n'est pas forcément un élément de formula_103.

Ce concept permet notamment de prouver la proposition suivante :

Démonstration :
Supposons infinie la tribu formula_59 sur l'ensemble . Comme tout élément de formula_59 est réunion d'atomes, les atomes sont eux aussi en nombre infini. Considérons alors formula_106 une suite d'atomes distincts (donc deux à deux disjoints).

Pour tous indices formula_107, la définition de formula_108 et le fait que formula_109, entraînent l'existence d'un formula_110 tel que :

formula_111 mais formula_112.

On définit alors une application formula_113 en posant, pour formula_114 :

formula_115.

En utilisant formula_116, on vérifie que formula_117, on conclut que formula_118 est injective.

CQFD

La conjonction de ce résultat et de la construction d'une tribu engendrée par récurrence transfinie permet de prouver un résultat plus précis lorsqu'on suppose la tribu dénombrablement engendrée :

La notion de tribu est étroitement liée à celle de mesure, qui est elle-même une généralisation des notions de longueur (sur une droite), d'aire (dans le plan) et de volume (dans l'espace à trois dimensions). Dans la deuxième moitié du , la question de savoir quels ensembles peuvent être mesurés se pose. La longueur d'un intervalle de bornes formula_119 et formula_120 est formula_121. Bernhard Riemann, avec l'intégrale qui porte son nom, est le premier à permettre de mesurer des parties de la droite réelle qui ne sont pas des intervalles.

À sa suite, d'autres mathématiciens cherchent la meilleure façon de définir les ensembles mesurables : Stolz et Harnack considèrent les réunions "finies" d'intervalles, dans ℝ. Cependant, Harnack, en 1884, est le premier à évoquer une union dénombrable d'intervalles, il prouve ainsi que tout ensemble dénombrable (dont l'ensemble des nombres rationnels) inclus dans ℝ est de mesure nulle.

Cela n'est pas admis par les mathématiciens de l'époque, car paraît contradictoire avec le fait que l'ensemble des nombres rationnels est dense dans celui des réels. En effet, un ensemble de mesure nulle est perçu « très petit » alors qu'un ensemble dense est « très grand ».

Ce paradoxe apparent conduit les mathématiciens (dont Camille Jordan en 1892) à ne considérer comme mesurables que les sous-ensembles de ℝ égaux à une union finie d'intervalles.

En 1898, Émile Borel s'appuie sur les réunions dénombrables d'intervalles ouverts disjoints deux à deux et construit, par récurrence transfinie, l'ensemble de parties qu'on appelle aujourd'hui la tribu borélienne de la droite réelle. Les boréliens ont la propriété suivante : la mesure d'une réunion d'ensembles boréliens deux à deux disjoints est égale à la somme des mesures de chacun de ces ensembles.

Les travaux contemporains de René Baire méritent aussi d'être mentionnés. Ils ont en effet nourri l'inspiration de ses contemporains en prouvant l'efficacité des techniques ensemblistes en analyse, même si c'est ailleurs que dans les fondements de l'intégration qu'ils ont révélé leur fécondité.

Les années 1901 à 1904 voient la publication par Henri Lebesgue de la théorie de la mesure des parties de l'espace euclidien et de la théorie de l'intégration qui portent son nom. Les ensembles mesurables qu'il définit forment un deuxième exemple de tribu, qui est l'ensemble de définition de la mesure de Lebesgue. On sait rapidement qu'en présence de l'axiome du choix il existe des ensembles non mesurables : il n'est plus question d'espérer mesurer toute partie de l'espace.

Les années 1910 voient se développer des recherches où l'accent est mis sur les fondements ensemblistes de la théorie de l'intégration et désormais aussi des probabilités. Felix Hausdorff et surtout Constantin Carathéodory, dont l'axiomatique des mesures extérieures étend à un cadre abstrait les travaux de Lebesgue, ont fait progresser ces recherches. En 1915, Maurice Fréchet publie un article qui propose déjà une définition des mesures très voisine de celle admise de nos jours. Il les définit sur ce qu'on appelle aujourd'hui des sigma-anneaux et est le premier à considérer des « ensembles abstraits » sans relation avec les nombres réels. Dans un article de 1927, Wacław Sierpiński introduit ce qu'on nomme aujourd'hui la tribu engendrée.

Dans les années 1930, la maturation du formalisme moderne est terminée. Pour la première fois semble-t-il, un article de 1930 d'Otton Nikodým énonce explicitement les définitions de sigma-algèbre et de mesure utilisées aujourd'hui. Deux traités influents parus pendant cette décennie popularisent définitivement la notion : "Théorie de l'intégrale" de Stanisław Saks pour l'analyse et "Fondements de la théorie des probabilités" d'Andreï Kolmogorov. Quant au terme de « tribu » utilisé en français pour dénommer les σ-algèbres, il a été introduit dans un article publié en 1936 par René de Possel, membre du groupe Bourbaki.


</doc>
<doc id="15105" url="https://fr.wikipedia.org/wiki?curid=15105" title="Parti libéral">
Parti libéral

Dans de nombreux pays, il existe un parti libéral ou apparenté. Mais le nom du parti ne fait pas forcément référence à une tradition politique fondée sur le libéralisme.

Selon les pays, le terme libéral peut recouvrir des idéologies diversement placées sur l'échiquier politique. Ainsi aux États-Unis le mot "liberal" (ou "left-liberal") désigne les progressistes, depuis le maccarthisme qui poussa de nombreux socialistes à se faire discret, le meilleur équivalent à l'adjectif libéral serait donc "classical liberal" ou "libertarian" dans ce pays. Au Royaume-Uni et au Canada, la situation est intermédiaire, le mot "liberal" fait à la fois référence à la gauche réformiste, qui est social-démocrate, mais aussi à la philosophie libérale selon le contexte ; dans ces pays la notion classique de libéralisme est plus souvent prônée par les partis dits conservateurs (qui peuvent aussi prôner ou ne pas prôner différents types de conservatisme de façon variable, selon le cas). En France, il existe de nouveau un parti revendiqué libéral. Il existe également un Parti libéral au Canada. Enfin, au Japon, le parti dit libéral est un parti conservateur et en Autriche nationaliste.


Le FDP est le plus grand parti libéral en Allemagne et a co-gouverné le pays avec les conservateurs (CDU) entre 2009 et 2013.



Il a longtemps existé en Belgique un Parti libéral. Dès l'indépendance du pays en 1830, il est l'un des deux partis qui dominent la vie politique du pays, avec le parti catholique. Cependant, ce n'est qu'en 1846 qu'il s'organisera et établira son programme, lors d'un congrès fondateur à l'hôtel de ville de Bruxelles. En 1961, à la suite de la crise suivant l'indépendance du Congo belge, il se réforme et devient le Parti de la liberté et du progrès (PLP). En 1972, il se scinde en une aile francophone, le "Parti réformateur libéral" (PRL) (qui fait partie aujourd'hui d'une coalition de centre-droite appelée Mouvement réformateur) et une aile flamande, le "Partij voor Vrijheid en Vooruitgang" (PVV) qui devient en 1992 le "Vlaamse Liberalen en Democraten" (VLD)


Niveau fédéral :

Niveau provincial et territorial :





Le Parti libéral est un ancien parti politique espagnol. Ce fut un des deux partis qui alternèrent au pouvoir pendant la Restauration bourbonienne, dirigé par Práxedes Mateo Sagasta.

Le Parti républicain, composante de l'UDF, est rebaptisé en 1997 Démocratie libérale, sous la présidence d'Alain Madelin. En 1998, Démocratie libérale se détache de l'UDF. Lors du premier tour de l'élection présidentielle de 2002, Alain Madelin obtient 3,91 % des voix. Démocratie libérale se fonde ensuite dans l'Union pour un mouvement populaire. Jusqu'en 2006, il n'y a plus de parti libéral sur la scène politique française.

Des militants de l'association Liberté chérie créent, le , Alternative libérale, qui se revendique d'un libéralisme classique, c'est-à-dire favorable au libéralisme politique comme au libéralisme économique. En 2008, Aurélien Véron, ancien président d'Alternative libérale, s'en détache pour fonder le Parti libéral démocrate. En 2011, Alternative libérale devient un parti associé au Nouveau Centre, puis abandonne son statut de parti politique au profit de celui d'« œuvre auxiliaire ».

Aujourd'hui le seul parti libéral existant en France est le Parti libéral démocrate. Le PLD est lancé en septembre 2008 par Aurélien Véron, ancien président de Liberté chérie puis d'Alternative libérale, actuellement membre du Bureau de Réforme et Modernité. Il regroupe des cadres issus d'Alternative libérale, de l'UMP, de l'ex-UDF ainsi que des libéraux non encartés. Ce parti politique entend reprendre le flambeau de Démocratie libérale, formation disparue en 2002 avec le retrait progressif d'Alain Madelin de la vie politique.





Le Parti libéral italien, fondé en 1946, a disparu en 1994 à la suite de l'opération Mains propres. Un nouveau Parti libéral a été refondé en 1997.

Plusieurs partis politiques japonais se sont appelés, de la fin du au début du , Parti libéral. L'un d'eux, fondé en 1945, fut le parti dominant du direct après-guerre et l'une des deux composantes à l'origine de la création du Parti libéral démocrate (PLD), mouvement de centre-droit qui a dominé tous les gouvernements depuis sa création en 1955, à l'exception de quelques mois de 1993 à 1994. 

Le dernier parti en date à avoir porté cette appellation s'est fondu en 2003 dans le Parti démocrate du Japon (PDJ), principale force d'opposition de tendance sociale-libérale et centriste.



Depuis 2005, le Parti libéral représente une aile européenne du libéralisme, favorable au rapprochement avec l'OTAN, l'Union européenne et la Roumanie voisine.

Le Parti libéral, apparu en 1890, est le premier parti politique dans l'histoire du pays. De tendance social-libérale, et au pouvoir de 1891 à 1912, il introduit d'importantes réformes sociales pour améliorer le niveau de vie des travailleurs et faciliter l'égalité des chances. Il disparaît en 1928, dépassé par les conservateurs sur sa droite et les travaillistes (socialistes) sur sa gauche.

Le parti ACT (Association des consommateurs et des contribuables), petit parti libertarien, revendique depuis 1994 l'étiquette libérale.

Le parti libéral démocrate aux Pays-Bas a été fondé en août 2006.


Le parti whig, qui s'est formé à la fin du , et est devenu "Liberal party" au cours du , a longtemps fait l'alternance du pouvoir avec le parti conservateur Tory, jusqu'à la Première Guerre mondiale, où le parti socialiste "Labour" le supplante comme opposition progressiste aux conservateurs. Le "Liberal party" a depuis assumé ouvertement l'idéologie social-démocrate et a fusionné en 1988 avec le parti social-démocrate, lui-même composé de modérés ayant quitté le Labour. Cette fusion produit le parti libéral-démocrate.

Au cours de sa longue histoire, le parti libéral britannique a beaucoup évolué, parfois dans un sens libéral (autrefois mercantiliste, maintenant libre-échangiste), parfois dans un sens socialiste (autrefois contre les privilèges, maintenant pour l'État-providence). Il se veut populiste, réformiste, et progressiste.







</doc>
<doc id="15116" url="https://fr.wikipedia.org/wiki?curid=15116" title="Anacyclique">
Anacyclique

Un anacyclique est un mot ou phrase que l’on peut lire dans le sens normal de lecture ou dans le sens inverse. Le palindrome est un cas particulier d’anacyclique où l'ordre des lettres est le même quel que soit le sens de lecture ; dans le cas général, l’anacyclique a l'ordre inversé selon le sens de lecture. Par définition, un anacyclique est une anagramme.

On peut parler de lorsque l’on obtient une phrase grammaticalement correcte mais avec un sens différent en inversant l’ordre des mots.

Parmi les plus longs de la langue française, « regagner » et « rengager » (8 lettres), ou encore « retartiner » et « renitrater » (10 lettres) qui figurent dans le Wiktionnaire mais pas toujours dans un dictionnaire classique.

Avec plusieurs mots :
Phrases anacycliques :

Les anacycliques sont parfois utilisés par des romanciers pour donner des noms à leurs personnages. C’est le cas par exemple du prénom « Enola » anacyclique du mot anglais « alone ».

Le roman "Gig", de James Lovegrove, qui est constitué de deux parties complémentaires, utilise des anacycliques pour les titres de ses chapitres. D’autres anacycliques sont également disséminées au fil de l’ouvrage.

Dans la série de jeux vidéo "Castlevania "ainsi que dans le manga "Hellsing", le nom Alucard est l’anacyclique de Dracula.

Dans "Shining" de Stephen King, « Tromal » donne « la mort » (dans le film il s’agit de « Redrum » (murder) autrement dit, en français « meurtre »).

Nujabes, producteur de hip-hop, de trip hop et DJ japonais, a pris pour pseudonyme l'anacyclique de ses nom (Seba) et prénom (Jun) accolés.

Un certain nombre de thèmes de jazz sont des anacycliques: Ecaroh inverse le prénom de son compositeur Horace Silver, Nardis, célèbre morceau de de Miles Davis, inverse le nom du chanteur Ben Sidran. Le thème Airegin de Sonny Rollins est l'anacyclique de Nigeria.

La société Ankama utilise de nombreux anacycliques pour nommer les peuples, objets et lieux de l'univers de Dofus et Wakfu, y compris son propre nom ("Ankama" devient "Amakna", nom du monde où évolue les peuples de Dofus et Wakfu).

Le nom du groupe musical "Starflam" est l'anacyclique de "Malfrats", tiré du nom d'un projet musical précédent de ses membres, "Malfrats linguistiques".

Des marques comme Duarig et Norev sont l'anacyclique du nom du fondateur.




</doc>
<doc id="15117" url="https://fr.wikipedia.org/wiki?curid=15117" title="Triangulation">
Triangulation

En géométrie et trigonométrie, la triangulation est une technique permettant de déterminer la position d'un point en mesurant les angles entre ce point et d'autres points de référence dont la position est connue, et ceci plutôt que de mesurer directement la distance entre les points. Ce point peut être considéré comme étant le troisième sommet d'un triangle dont on connaît deux angles et la longueur d'un côté.

Par analogie, la triangulation fait également référence à l'usage croisé de techniques de recueil de données en étude qualitative, notamment en sciences sociales.

En topologie, une triangulation d'un espace topologique X est un complexe simplicial K homéomorphe à X, et un homéomorphisme "h" : K→X. La triangulation est utile pour déterminer les propriétés d'un espace topologique.

En géométrie, une triangulation est une façon de découper une forme géométrique (un plan, un polygone) en une collection de triangles. Un exemple classique est la triangulation de Delaunay. Une des applications de cette démarche est le maillage d'une pièce permettant l'analyse par éléments finis.

La triangulation est aussi le processus qui permet de déterminer une distance en calculant la longueur de l'un des côtés d'un triangle, et en mesurant deux angles de ce triangle. Cette méthode utilise des identités trigonométriques.

Six cents ans avant l'ère chrétienne, Thalès mit au point une méthode pour évaluer la distance d'un bateau en mer à la côte. Pour avoir une mesure approximative de cette distance, il plaça deux observateurs A et C sur le rivage, éloignés d'une distance "b" connue. Il demanda à chacun d'entre eux de mesurer l'angle que faisait la droite le reliant au bateau B avec celle le reliant à l'autre observateur. Ce principe de télémétrie optique est utilisé en génie optique, ainsi que dans le domaine militaire lorsque l'on ne dispose pas de radar.

La méthode a un intérêt si nous voulons déterminer de grandes distances ; mais dans ce cas nous devons placer les deux observateurs suffisamment éloignés l'un de l'autre, pour que les mesures d'angle soient plus précises.

Les propriétés souvent utilisées pour la triangulation sont :

Jusque dans les années 1980, on utilisait essentiellement la triangulation pour mesurer les distances. La triangulation consiste à obtenir par des visées les angles d’un triangle dont les sommets sont choisis pour leur visibilité (tour, sommet, clocher…). On enchaîne ensuite ce premier triangle à un autre qui a un côté en commun avec lui, en poursuivant la chaîne le long du méridien à mesurer. Il suffit de déterminer une base au départ, c’est-à-dire de mesurer au sol un côté du premier triangle, pour obtenir la longueur des côtés de tous les triangles.

Ce procédé, répété de proche en proche, a été utilisé par Delambre et Méchain de 1792 à 1798 pour mesurer la distance entre Dunkerque et Barcelone (environ km) sur le méridien de Paris, ce qui permettra la première définition pratique et officielle du mètre en 1799 (bien que la conception du mètre lui-même en tant qu'unité universelle et décimale soit bien antérieure, cf. les travaux de John Wilkins et de Tito Livio Burattini).

À partir d'un point de référence, on peut ainsi déterminer la position des différents points d'un territoire et réaliser un maillage. Ce maillage permet ensuite d'avoir une cartographie précise et . La première carte de France ainsi tracée fut publiée en 1745, à partir des relevés de Jacques et César Cassini.

La triangulation est utilisée dans divers secteurs, comme la survie, la navigation, l'astronomie, dans l'armement (fusées).

Un navire peut ainsi connaître sa position en relevant la direction d'observation (angle par rapport au Nord) de deux points distants (par exemple un clocher d'église, un phare) ; il lui suffit alors, sur une carte, de tracer les droites passant par les points observés et ayant la direction relevée, l'intersection de ces droites étant la position du navire. Pour lever les imprécisions de mesure, on utilise généralement trois points de repère, appelés amers. C'est la navigation par relèvement.

Dans le cas d'ondes électromagnétiques (par exemple des ondes radio), la position peut se déterminer avec une antenne directionnelle (c'est-à-dire une antenne ne captant que les ondes venant d'une direction donnée) ; l'orientation pour laquelle le signal est le plus fort donne la direction de l'émetteur, il suffit alors de faire plusieurs relevés pour avoir la position de l'émetteur (radiogoniométrie). Cette méthode était par exemple utilisée durant l'occupation allemande de la France pour détecter les émetteurs radio clandestins.

On vise deux points, et on relève les directions de visée. Il suffit ensuite de tracer, sur une carte, une droite passant par le point visé et ayant la direction relevée. L'intersection des droites donne la position.

On a deux sommets du triangle (les amers) et la direction des deux côtés ne joignant pas ces sommets (relèvements), ce qui permet de déterminer complètement le triangle.

Si l'on effectue trois relevés, on devrait obtenir un point de concurrence unique des trois droites. Dans la pratique, les imprécisions — sur la visée, sur la lecture de l'angle, sur le tracé de la droite — font que l'on obtiendra un triangle, la dimension du triangle donnant une estimation de la précision de la mesure. On peut raisonnablement (si les angles sont à 120° l'un de l'autre) prendre pour position le barycentre de ce triangle, et pour erreur la distance entre ce centre et le point le plus éloigné.

Dans le cas d'un véhicule en mouvement, il faut prendre en compte le déplacement du véhicule. Il faut pour cela connaître la direction et la vitesse du véhicule. La direction du mouvement est donnée par le compas ; dans le cas d'un voilier, la vitesse peut être estimée à partir de la vitesse du vent et du courant.

Si la vitesse est lente et que les relevés sont faits de manière proche (cas de la navigation maritime), on peut négliger ce phénomène, par contre, il faut noter l'heure du relevé.

La connaissance de ce mouvement permet de faire un relevé avec seulement un amer, par exemple dans le cas d'une navigation par temps de brouillard où seul un lieu caractéristique serait visible par intermittence. On relève alors les directions et les heures du relevé.

On a ainsi un sommet du triangle (l'amer), les directions de deux côtés (les deux relevés), et la direction et la longueur du troisième côté (trajectoire du bateau), ce qui permet de déterminer complètement le triangle.

On peut relever une position en estimant la distance par rapport à trois points. Si l'on prend deux points de référence, on a deux sommets et les longueurs des côtés ne joignant pas ces sommets, ce qui définit deux triangles ; le point du relevé se trouve au troisième sommet de l'un de ces triangles. L'adjonction d'un troisième point de référence permet de déterminer lequel des deux sommets est le bon.

Avec une onde électromagnétique, on peut utiliser l'intensité du signal collectée par une antenne non directionnelle. Si le milieu de propagation est homogène et isotrope, l'intensité est inversement proportionnelle au carré de la distance (l'énergie se répartit sur une sphère grandissante) et donc diminue avec l'augmentation de la distance source-récepteur. L'intensité permet donc d'estimer la distance, et donc de situer l'émetteur sur un cercle centré sur le récepteur. Un deuxième récepteur permet de tracer un second cercle, l'émetteur se trouve donc à l'intersection des deux cercles ; un troisième récepteur permet de déterminer lequel des deux points d'intersection est le bon (ou bien la logique, un navire ne peut pas se trouver sur les terres). Cette méthode est aussi utilisée en sismologie pour connaître la position de l'épicentre d'un séisme ; on considère alors l'intersection de sphères (on n'a plus la contrainte d'un émetteur situé en surface), et il faut corriger les calculs de l'hétérogénéité du milieu (variation de l'indice de réfraction en fonction de la profondeur, réflexion et réfraction sur le manteau...).

Si l'on ne connaît pas l'intensité de l'émetteur ni le rendement des récepteurs, il faut se trouver les positions pour lesquelles le signal est reçu avec la même intensité ; l'émetteur se trouve alors sur la médiatrice du segment constitué par les deux récepteurs. On peut localiser la source avec un deuxième relevé, à l'intersection des deux médiatrices. La méthode des médiatrices, légèrement modifiée, est utilisée en sauvetage-déblaiement pour localiser des victimes ensevelies avec un géostéréophone.

Si l'on a des événements émettant un signal, alors en constatant le décalage dans l'arrivée des signaux, on peut déterminer la différence de distance entre les événements et les récepteurs, à condition de connaître la vitesse de propagation du signal.

Si le récepteur est lui-même synchronisé avec les émetteurs, on peut alors déterminer directement le temps de trajet et donc la distance entre émetteurs et récepteur.

Par exemple, pour localiser un séisme, on a un événement unique générant le signal, le séisme, et plusieurs stations réceptrices synchronisées, les sismographes. L'onde sismique arrive à des moments différents aux sismographes. Le foyer du séisme (hypocentre) est à une distance "d" du sismographe "i".

Le foyer et deux sismographes forment un triangle ; on connaît deux sommets S et S du triangle (les sismographes), de coordonnées ("x","y","z") et ("x","y","z"), et la différence de longueur D entre les deux côtés ne joignant pas ces sommets. Le troisième sommet du triangle est donc sur une surface vérifiantsoit

qui est l'équation d'un hyperboloïde.
Il faut suffisamment de sismographes pour définir trois surfaces, le point de concurrence de ces surfaces donnant le foyer du séisme.

On a plusieurs types d'ondes sismiques voyageant à des vitesses différentes, ce qui permet de faire plusieurs déterminations. Toutefois, le problème est rendu complexe car :

On peut également localiser d'autres événements comme une forte explosion : essai nucléaire, catastrophe industrielle de grande ampleur comme la catastrophe de l'usine AZF, …

Pour la navigation par satellite (système type GPS), on a plusieurs émetteurs synchronisés et un récepteur unique à l'endroit à localiser.

C'est également un calcul sur la mesure des temps de réceptions de parasites atmosphériques qui permet de localiser les coups de foudre.




</doc>
<doc id="15122" url="https://fr.wikipedia.org/wiki?curid=15122" title="Dalmate">
Dalmate

Le dalmate est une langue morte ou une famille de langues mortes faisant partie des langues romanes, anciennement parlée en Illyrie, actuellement pour l'essentiel région côtière de la Croatie et du Monténégro. On désigne sous le nom d'illyro-roman la branche des langues romanes à laquelle appartient cette langue.

Par ce terme on désigne les parlers romans employés naguère le long de l'Adriatique, depuis la ville de Fiume jusqu'au golfe de Cattaro. Ils étaient, selon les débris ramassés à la fin du , différents de l'italien et du roumain, mais constituaient cependant une sorte de pont entre les deux langues néo-latines. 

Le terme « dalmate » a aussi été utilisé pour désigner une variante locale du croate qui s'est maintenue comme langue littéraire jusqu'au début du .

Avant l'occupation romaine, l'Illyrie, région où s'est développée la langue dalmate, était habitée entre autres par la tribu des Dalmates, dont le centre était Tomislavgrad ("Dalmium" ou "Delminium").

Les Romains occupèrent le territoire illyrien entre 229 av. J.-C. et 155 ap. J.-C.. Plusieurs empereurs romains étaient d'origine illyrienne : Aurélien, Dioclétien et Constantin.

Après la chute de l'Empire romain d'Occident, l'Illyrie continua à parler une langue apparentée au latin. La langue évolua relativement indépendamment des autres langues romanes, progressant vers une variante régionale, puis finalement une langue distincte. D'autres langues vinrent influencer le dalmate, sans toutefois supplanter la langue dans son origine commune au latin (superstrats) : le slave puis le vénitien (dialecte italo-roman de Venise). Un certain nombre de villes de la région portent d'ailleurs des noms vénitiens.

Le pape Jean IV le Dalmate (640-642) parlait vraisemblablement cette langue.

Le dalmate a connu au moins deux variantes dialectales :

Il faut peut-être leur ajouter l'albano-roman. Mal attesté, celui-ci se serait éteint au Moyen Âge.

C'est à Bernardino Biondelli que revient le mérite d'avoir découvert la langue dalmate. En effet, en 1840, lorsqu'il travaillait à son ouvrage "Atlante linguistico d'Europa", il s'adressa au médecin de Veglia, Gian Battista Cubich pour qu'il lui fournisse un spécimen du parler roman de cette île de l'Adriatique. Cubich envoya le spécimen en 1842, mais Biondelli ne s'en est pas servi. En 1849 la revue de Trieste, Istria, publie des spécimens de parlers de l'Istrie et, en 1861, Cubich lui-même fait paraître, dans la revue "L'Istriano", une partie de sa récolte linguistique.

Ce fut A. Ive, professeur de langue italienne à l'Université de Graz, qui, le premier, dans son étude "L'antico dialetto di Veglia", réunit les matériaux de ses devanciers (Cubich, Pétris, A. Adelmann et M. Celeberini), en les complétant par des matériaux recueillis personnellement. Il avait parmi ses informateurs « le dernier Dalmate », Tuone Udaina, âgé de 59 ans.

Matteo Bartoli, à son tour, consacre à cette langue romane, dans son travail "Das Dalmatische", une étude de la plus grande envergure.

La contribution de Bartoli forme une sorte d'encyclopédie du dalmate, car elle renferme non seulement les matériaux linguistiques recueillis par l'auteur sur place, mais aussi ceux qu'il a pu découvrir dans les archives. L'auteur indique en même temps tous les matériaux publiés auparavant, montrant leur valeur scientifique.

Après avoir indiqué les documents concernant le dalmate de Raguse, l'auteur mentionne les travaux faits à ce sujet par des spécialistes : G. I. Ascoli, W. Meyer-Lübke, A. Mussafia et H. Schuchardt.

Le premier tome se termine par un aperçu étendu sur l'ethnographie de l'Illyrie. Le second tome contient les textes recueillis par l'auteur et ceux découverts dans les archives, de même que les matériaux enregistrés par ses devanciers.

À l'aide de tous les matériaux concernant le dalmate, Bartoli fait ensuite une description linguistique du développement de cette langue romane disparue.

Longtemps considérée comme la principale source et l'étude la plus remarquable sur la langue dalmate, la contribution scientifique de Bartoli a été contestée et revue dans la seconde moitié du . Les romanistes contemporains définissent désormais une famille dalmatoromane composée d'un nombre impossible à déterminer de langues apparentées.




</doc>
<doc id="15123" url="https://fr.wikipedia.org/wiki?curid=15123" title="Sarde">
Sarde

Le sarde est une langue appartenant à la branche romane méridionale de la famille des langues indo-européennes. Il est parlé en Sardaigne et chez nombre de travailleurs émigrés sardes répartis dans tous les continents (la Région Sarde a recensé quelque 145 Circoli Sardi (Cercles sardes) qui sont soutenus et aidés financièrement afin que la culture et la langue soient préservées). C’est la langue romane qui est restée la plus proche du latin vulgaire, qui est à l'origine des langues romanes. En effet, l’isolement insulaire précoce a coupé l'île du centre linguistique moteur qu'était Rome. Pendant un certain temps, ceci lui a évité un grand nombre de contacts avec d’autres langues (interférence linguistique), qui auraient pu être facteur d’évolution linguistique; à cet égard on peut comparer sa situation à celle de l’islandais, langue scandinave restée la plus proche du vieux norrois. Le sarde est donc resté assez archaïque et conservateur (). 

On compte actuellement environ locuteurs de cette langue, pratiquée uniquement en Sardaigne et parmi les émigrés d’origine sarde en Italie et dans le monde. Presque tous les locuteurs sont bilingues sarde-italien. Le sarde ne couvre pas toute l'île, d'autres variétés romanes y étant présentes : sassarese, gallurais (dialecte corse sartenais), tabarquin (dialecte ligure) et l'alguérois (dialecte catalan).

Voyelles : les ĭ et les ŭ (brefs) du latin ont conservé leurs timbres originels ( et ) : "siccus" devient "sikku" (et non comme en français, "sec" ou en italien, "secco"). Une autre caractéristique est l’absence de diphtongaison romane. Exemple : "potet" devient "podet" (prononcé parfois "poðet"), et non comme en italien "può", en espagnol "puede" ou en français "peut", où apparait une diphtongue.

Très archaïsant est également le maintien de et de devant et : "kentu" pour "cent" en français, ou encore "cento" en italien.

Un caractère original du sarde est l’évolution de [-ll-] en [ɖ]. C'est un phonème cacuminal, souvent transcrit avec un "ḍ" ("d" pointé) : "coraḍḍu" pour "corallo" (corail, en italien). Ces traits seraient dus au substrat d'une hypothétique langue paléosarde, mal connue et parfois désignée comme langue nouragique (des nuraghe). ou romaines et se retrouve notamment en Corse-du-Sud et en Sicile. D'autres particularités existent dans certaines régions de la Sardaigne comme la prothèse vocalique devant /r/ (arrana « grenouille » pour rana) ou la lénition du /f/ initial, traits phonétiques qui rappellent fortement le basque, le castillan ou le gascon.

L’article défini sarde est original car il est issu de "ipse" (alors que dans les langues romanes, l'origine est le plus souvent "ille", "illu"), d’où "su", "sa" au singulier et "sos", "sas" au pluriel. On retrouve cette caractéristique en catalan des Îles Baléares et de façon résiduelle en occitan, notamment dans le dialecte provençal des Alpes-Maritimes (hors le niçois).

La marque du pluriel est "-s", comme dans toute l'Europe latine occidentale (français, occitan, catalan, espagnol, portugais) : "sardu, sardus" - "pudda, puddas" (poule) - "margiani, margianis" (renard).

Le futur est construit avec la forme latine "habeo ad" : "app’a istàre" (je resterai).

L'interdiction se construit avec une négation ("no") suivie du verbe. Ce verbe peut être conjugué ou à l'infinitif. Par contre, il ne peut pas être à l'impératif. Pour donner un ordre négatif, on utilise 'non' suivi de la deuxième personne singulier ou pluriel du subjonctif présent, comme dans les langues romanes de la péninsule Ibérique : "no bengias !" (ne viens pas !). Ainsi, lorsque le marqueur négatif 'no' accompagne un autre mot négatif, les deux négations ne s'annulent pas pour donner un sens positif à la phrase, contrairement à d'autres langages. En sarde, 'Je n'ai pas acheté rien' signifie 'Je n'ai rien acheté' et non 'J'ai acheté quelque chose' : la double négation n'existe pas.

Le lexique présente également des traits conservateurs : par exemple, le latin "albus" « blanc » a été conservé, alors que presque toutes les autres langues romanes l'ont remplacé par un mot d'origine germanique.

Le latin reste néanmoins la langue mère du sarde. . 

La Sardaigne, proche de l’Italie, a été en effet conquise par Rome dès 238 av. J.-C. et a constitué, avec la Corse, un des greniers à blé de Rome. Au , les Vandales, peuple germanique ayant migré pendant les grands mouvements de population de l'époque, s’installent en Sardaigne. Ils seront suivis par les Byzantins et les musulmans. Néanmoins, ces peuples ont très peu influencé la langue.

La Sardaigne appartiendra pendant un temps à la ville-État de Pise d’où une forte toscanisation, surtout constatée dans le nord de l’île comme aussi en Corse. Puis, de 1326 à 1713, elle est sous domination espagnole (catalane, puis castillane), ce qui a produit un superstrat catalan, puis castillan, qui a fortement influencé la langue, surtout dans le domaine de l’administration, le sarde étant interdit. De 1713 à 1718, l'île fut administrée par les Habsbourg d'Autriche (à la suite du traité d'Utrecht), puis donnée à l'ancien duc de Savoie, devenu, brièvement, roi de Sicile. L'ensemble Savoie-Piémont-Nice, plus la Sardaigne, devint ainsi, en 1720 le royaume de Sardaigne (la capitale étamt Turin). L'italien (toscan) devient la langue officielle de la Sardaigne en 1764. 

En 1861, le royaume de Sardaigne est naturellement englobé dans ce qui sera le nouveau royaume d’Italie. En 1948, l’île acquiert une autonomie relative (elle reste une région d’Italie) mais toutefois importante : elle constitue une région à statut spécial, avec un parlement régional et un conseil régional, avec à sa tête un président élu.

Dans leurs formes écrites majeures, on distingue un "logudorese illustre" et un "campidanese illustre", formes standardisées qui se disputent la suprématie littéraire. Un effort d’unification, notamment par une norme écrite unifiée, c'est la , créée par l'administration régionale et compréhensible par tous les locuteurs du sarde, ainsi que par les locuteurs d'autres langues locales (sassarese, gallurese, alguerès, tabarquin) en tant que "variété médian".

Toutes les variétés de sarde, l'une plus que l'autre, conservent une forme nettement archaïque, mais en même temps des traces des substrats pré-romains (surtout au centre-est avec des liens possibles avec le basque) et des superstrats catalans (de ), espagnols (de la fin du ) et italiens de 1720 et après.

Le sarde possède deux grandes variétés. 

Ni le gallurais, ni le sassarese, proches entre eux, ne correspondent à la définition d'une macro-langue sarde. Leurs traits sont nettement corse/toscan, avec un pluriel en "-i", un article italien mais avec du vocabulaire et certains traits sardes comme le son cacuminal. Certains linguistes les rattachent dès lors plutôt au corse/toscan qu’au sarde à cause leur origine du au , et ils en font donc deux subdivisions du corse. D’autres, pour des raisons de regroupement régional, les mettent sur un pied d’égalité avec les deux variantes évoquées ci-dessus (logudorais et campidanais) ou les reconnaissent en tant que langues individuelles. Le sarde a servi de superstrat ou d'adstrat pour ces deux variantes, qui correspondent à une bande dans la nord de l’île:

À noter aussi la présence d'autres variétés:

Le premier à avoir évoqué le sarde et son caractère archaïsant est Dante, qui écrivit notamment dans "De vulgari eloquentia" que les Sardes n'étaient pas Italiens et étaient les seuls à ne pas posséder leur propre langue vulgaire « parce qu'ils imitent le latin comme les singes imitent les hommes ».
Pourtant, la langue sarde a une tradition écrite vivace même si elle ne s'est pas forgée une norme durable. Pendant la domination aragonaise, le catalan était la langue officielle, bientôt supplanté par le castillan et par l’italien (en 1764, dans le cadre du royaume savoyard). La présence d’îlots allophones (voir ci-dessus), notamment à Alghero (depuis le ) et dans les îles de San Pietro et de Sant’Antioco (depuis le ), constitue une trace de ces faits historiques.

Un rôle important pour la conservation de la langue sarde a été la tradition poétique et les joutes poétiques ("gare poetiche"), où l'improvisation et la verve des chanteurs attiraient les foules.
Le premier texte littéraire semble être celui d’Antonio Cano sur des martyrs locaux, au , dans une langue assez normalisée (mais qui se délitera un siècle après) :
<poem>
« Tando su rey barbaru su cane renegadu
de custa resposta multu restayt iradu
& issu martiriu fetit apparigiare
itu su quale fesit fortemente ligare
sos sanctos martires cum bonas catenas
qui li segaant sos ossos cum sas veinas
& totu sas carnes cum petenes de linu. »
</poem>

Le sarde, comme les autres dialectes "non sardes" de la région de Sassari ou de la Gallura, ou comme le catalan ou le génois tabarquin, est désormais protégé par la loi régionale du qui lui reconnaît le statut de langue régionale protégée et qui est entrée en vigueur le 1998 (intitulée : "Promozione e valorizzazione della cultura e della lingua della Sardegna"). Le sarde est utilisé dans la signalisation routière bilingue de certaines municipalités.





</doc>
<doc id="15130" url="https://fr.wikipedia.org/wiki?curid=15130" title="Jugement de l'âme (Égypte antique)">
Jugement de l'âme (Égypte antique)

En Égypte antique, le jugement de l'âme est un procès où le défunt doit comparaître pour faire reconnaître ses droits à la vie éternelle. On distingue cependant trois conceptions différentes de cet affrontement judiciaire. La première conception est un modèle mythique où Horus, le successeur d'Osiris, se confronte à Seth pour obtenir la succession au trône d'Égypte. La deuxième conception est plus générale ; le défunt confronte ses ennemis, morts ou vivants, qui l'ont dépouillé de sa vie terrestre. La troisième conception est celle que le "Livre des Morts" a popularisée à travers la scène de la pesée du cœur (formules 30B et 125). Dans ce dernier modèle, le défunt est confronté à un accusateur divin. Jugé à l'aune de Maât, la déesse de la vérité et de la justice, le mort doit rendre compte à Osiris (ou à Rê) de ses actions et de sa manière de vivre sur Terre.
Les "Textes des Pyramides" sont une compilation de liturgies d'époques et de provenances diverses. Cette documentation religieuse apparaît durant l'Ancien Empire égyptien gravée sur les parois intérieures des chambres souterraines d'une dizaine de pyramides.
Bien que cette compilation soit bien antérieure à la et au règne du roi Ounas ; il est le premier souverain à la faire figurer dans sa tombe. Ces textes ne font pas apparaître un récit mythologique structuré. Cependant les allusions à la lutte entre les dieux Horus et Seth sont nombreuses. Lorsque les dieux accusent Seth d'avoir assassiné Osiris, il doit comparaître devant un tribunal divin. Cet épisode judiciaire figure sur le mur ouest de l'antichambre de la pyramide de Pépi:
Le texte se poursuit par une évocation de Thot qui est encouragé à trancher les têtes de ceux qui s'opposent au voyage du défunt Pépi vers Osiris et à la mise en place d'un culte sacerdotal durable au profit de cette dernière divinité.

Dans cet épisode du mythe osirien, la mort n'est pas vue comme la fin de la vie d'Osiris mais comme une personne hostile qu'il faut combattre. Seth est la personnification de la mort. Il est un meurtrier que l'on doit affronter et traîner devant un tribunal. La mort du roi Osiris est plus vue comme une maladie. La guérison est assurée par la magie d'Isis, de Nephtys et d'Anubis lors de la momification. Osiris, le roi mort, ne peut se défendre seul contre Seth. Pour lui conserver son statut social, Horus son fils et héritier doit lui assurer sa défense. Tout défunt égyptien dispose ainsi d'un cadre mythologique où la mort peut être appréhendée rituellement. Le défunt assimilé à Osiris peut combattre sa mort personnifiée en Seth. Pour tout défunt, la mort est un chamboulement de l'ordre et de la justice personnifiée par Maât. Par le procès, le mal est combattu et la justice rétablie.

Au Moyen Empire égyptien, le procès de Seth évoqué dans le chapitre 477 des "'Textes des Pyramides" apparaît inscrit sur quelques sarcophages de grands notables de la Moyenne-Égypte (chapitre 837 des "Textes des Sarcophages"). Il s'intègre dans un long rituel funéraire qui se termine par l'exécution de Seth représenté par l'abattage d'un bœuf :

Le chapitre 149 des « textes des Sarcophages » permet à l'âme-bâ du défunt de se transformer en homme-faucon.
Le défunt ne voit pas son trépas comme un phénomène naturel et biologique. Sa mort est causée par l'action malveillante d'un ennemi évoluant parmi les humains. Le défunt est en colère et cherche à se venger. Mais il doit d'abord traduire son ennemi devant le tribunal d'Osiris. Sa juste vengeance ne peut en effet s'exercer qu'après un avis favorable du tribunal :

La vengeance du mort ne se limite pas à son ennemi. Toute sa famille et ses alliés doivent subir la juste colère de la victime. Cette punition collective est approuvée par Osiris : « Comme il est glorieux ce dieu ! (...) Ils auront à pâtir, ceux présents ou à venir qui viendraient à combattre contre toi et qui tenteraient de (t'enlever ton pouvoir sur ton ennemi) ; te voici un homme-faucon. »

À partir de la , les chapitres 7 à 17 des "Textes des Sarcophages" forment la liturgie du "Khebes-to" ou « Piochage de la terre ». Une autre version, peut-être plus tardive, intègre les chapitres 1 à 6 et 27. La récitation de cet ensemble de textes par le prêtre-ritualiste a pour but d'aider le défunt à obtenir sa justification devant le tribunal divin et à repousser ses ennemis qui pourraient lui voler sa momie.

Dans cet ensemble liturgique, les trois conceptions égyptiennes du jugement de l'âme sont mentionnées. Le chapitre 3, invite le défunt à aller dans le tribunal pour se défendre contre tous ses ennemis potentiels:

Cette conception du tribunal est ancienne. Dès l'Ancien Empire, des inscriptions funéraires dans des tombes de notables menacent les pilleurs potentiels d'être trainés devant cette juridiction divine. Cette conception perdure jusqu'à Diodore de Sicile. Cet historien grec du rapporte qu'entre la momification et l'enterrement, la momie (le défunt) peut être confrontée à des accusateurs. Si les 42 juges réunis autour du corps estiment une accusation crédible, la momie n'est pas ensevelie et le défunt est privé de sa vie éternelle. S'il n'y a pas d'accusation, le défunt est enterré avec tous les honneurs.

Le chapitre 7 qui est le véritable commencement de la liturgie du Khebes-to, mentionne la lutte de Horus contre Seth et le procès où leur querelle s'est réglée judiciairement :

Le prêtre ritualiste évoque ce précédent mythique comme une jurisprudence. Le défunt est assimilé à Horus. Tout comme Horus a obtenu le trône d'Égypte à l'issue de son procès contre Seth, le défunt à l'issue de sa propre procédure doit obtenir la vie éternelle à travers sa justification.

La suite du rituel Khebes-to tente de faire oublier aux dieux les fautes commises par le défunt ainsi que tous ses manquements à la Maât. Il apparaît que le mort doit se présenter devant un tribunal présidé par Geb où un accusateur divin, probablement Thot, n'ignore rien des péchés du défunt d'autant plus qu'ils peuvent être rapportés par des témoins à charge (morts ou vivants, hommes ou femmes):

Pour se défendre le défunt met en avant l'argument de la jeunesse ; l'ignorance de l'enfance étant ses circonstances atténuantes. Toutes les fautes et les péchés de la durée de vie sont rejetés vers le temps de l'enfance, une époque d'ignorance où l'on ne sait pas distinguer le bien du mal.
Le prêtre ritualiste exhorte ensuite Thot et les juges divins de ne retenir que les bonnes paroles et donc les bonnes actions du défunt (Chapitre 9).

La fin du chapitre 9 évoque l'acquittement du défunt et la punition des accusateurs calomniateurs:

Par les chapitres du rituel "Khebes-to" (ou piochage de la terre) il apparaît qu'au Moyen Empire, le défunt doit se battre juridiquement dans un tribunal présidé par Geb, le père d'Osiris. Le mort s'y présente, en victime bafouée, pour affirmer ses droits et pour se les voir confirmer par les dieux. Au Nouvel Empire, le tribunal est présidé par Osiris. Il y acquiert un caractère moral plus affirmé. Ici ce n'est plus la mort, à travers Seth ou ses acolytes, qui est jugée mais le défunt lui-même. Il doit affirmer et justifier son innocence. Cette dernière conception du tribunal divin apparaît déjà dans l'« Enseignement pour Mérikarê ». Cette œuvre littéraire est une "sagesse" (un recueil de conseils) enseignée par le roi Khéty à son fils, le futur souverain Mérikarê. Ces deux rois de la d'Hérakléopolis ont vécu lors de la Première période intermédiaire. Les plus anciens exemplaires de cet enseignement ne sont cependant datés que de la .

Les égyptologues désignent par recension thébaine, les exemplaires du "Livre des Morts" rédigés au Nouvel Empire ; de la à la . Une des plus belles réalisations de cette époque est le "Papyrus d'Ani" daté de la ou de la . Dans cet exemplaire, le jugement de l'âme est évoqué deux fois, vers le début et vers la fin.

L'illustration de la première évocation du jugement de l'âme montre Ani et son épouse respectueusement courbés devant une balance à un fléau où sont suspendus deux plateaux. Le cœur d'Ani est posé sur le plateau de gauche en équilibre parfait avec le plateau de droite qui contient une plume d'autruche symbole de la rectitude (Maât). Le bon déroulement de la pesée est assuré par Anubis et Thot enregistre le résultat. Derrière lui se tient le monstre Ammit, la dévoreuse des âmes impures. Douze dieux constituent le tribunal divin ; Harmakhis, Atoum, Shou, Tefnout, Geb, Nout, Isis, Nephtys, Horus, Hathor, Hou et Sia. Le texte de cette scène est celui du chapitre 30B du « Livre des morts ». Ani demande à son cœur de ne pas le trahir. Comme Thot ne constate aucun péché, les juges déclarent le défunt juste de voix. Le chapitre 30B se poursuit dans la scène suivante. Ani est présenté par Horus à Osiris assis sur son trône et protégé par Isis et Nephtys. Assis devant Osiris, Ani lui demande de lui accorder le statut de bienheureux (Akh).

L'illustration de la deuxième évocation du jugement divin représente le tribunal de la "salle des deux Maât". À l'intérieur siègent quarante-deux juges. À chacun de ces juges, Ani déclare ne pas avoir commis tel ou tel péché. À droite, sont figurés quatre niveaux superposés. Au niveau supérieur sont assises, chacune sur un trône, deux Maât coiffées d'une plume d'autruche. En dessous, Ani est en adoration devant Osiris qui siège sur son trône. Entre les deux personnages est placée une fleur de lotus, symbole du renouveau. Plus bas, Anubis contrôle le bon fonctionnement d'une balance, sous laquelle apparaît le monstre Ammut, que l'on nomme aussi "la grande dévoreuse". Le cœur du défunt est au même niveau que la plume de Maât. Tout en bas, Thot à tête d'ibis est assis dans la position du scribe devant une autre figuration de la plume de Maât.

Les versions tardives du "Livre des Morts" (recension saïte) ont considérablement étoffé la confession négative ; cette déclaration étant doublée. En arrivant devant la "salle des deux Maât" (nom du tribunal d'Osiris), le défunt est accueilli par Anubis, le dieu de la momification. Après avoir été flairé et reconnu par ce dieu, le défunt lui affirme qu'avant de venir ici il a visité tous les lieux saints d'Égypte. Sur ce, Anubis teste les connaissances du défunt en lui demandant le nom de la porte, de son linteau et du seuil. Ayant bien répondu, le défunt est autorisé à entrer. Arrivé devant Osiris, le défunt salue le dieu. Le mort énumère ensuite une quarantaine de péchés qu'il n'a pas commis de son vivant. Ceci fait, il réaffirme devant 42 juges, dont il connaît les noms, qu'il n'a pas commis 42 fautes et qu'il est pur et innocent.




</doc>
<doc id="15171" url="https://fr.wikipedia.org/wiki?curid=15171" title="Bolsa Mexicana de Valores">
Bolsa Mexicana de Valores

La Bolsa Mexicana de Valores (BMV) est la principale bourse du Mexique : 71 entreprises y sont cotées. Le principal indice boursier est l'IPC ou Indice de Precios y Cotizaciones. Cet indicateur est basé sur 35 actions. La bourse est située dans un gratte-ciel appelé Centro Bursatil




</doc>
<doc id="15173" url="https://fr.wikipedia.org/wiki?curid=15173" title="Catalogne">
Catalogne

La Catalogne (en , en , en ) est une communauté autonome et une région historique d'Espagne, régie par un statut d'autonomie. Depuis le , elle est définie comme « réalité nationale » par son statut d'autonomie, mais le préambule de cette loi définit la Catalogne comme nation.

Elle est située dans le nord-est de la péninsule Ibérique en Europe du Sud et, selon les définitions, de l'Ouest. Sa capitale et métropole est la ville de Barcelone. Elle est entourée par la Communauté valencienne au sud, l'Aragon à l'ouest, la France au nord, l'Andorre au nord-ouest, et la mer Méditerranée à l'est. Elle couvre une superficie de (6 % de la superficie de l'Espagne). Ses langues officielles sont le catalan, l'occitan (dialecte aranais en val d'Aran) et l'espagnol ou castillan. En 2015, elle comptait (17 % de la population espagnole), ce qui en faisait la deuxième communauté d'Espagne après l'Andalousie et la dixième subdivision territoriale de premier niveau administratif d'Europe en termes de population. Elle est également la plus peuplée parmi les Pays catalans, ensemble culturel et linguistique qui la lie à la Communauté valencienne, aux îles Baléares et à l'essentiel du département français des Pyrénées-Orientales.

Administrativement, la communauté autonome de Catalogne actuelle est divisée en 42 comarques, regroupées en quatre provinces : Barcelone ("Barcelona"), Gérone ("Girona"), Lérida ("Lleida") et Tarragone ("Tarragona"). Les agglomérations les plus importantes sont celles de Barcelone, par ailleurs deuxième aire urbaine d'Espagne en talonnant de peu Madrid, et de Tarragone.

La Catalogne est née en tant que réalité nationale par la réunion politique de plusieurs comtés de l'ancienne marche d'Espagne carolingienne entre le et le sous l'autorité de la maison de Barcelone. La principauté de Catalogne ainsi constituée devient progressivement un État à la fin du Moyen Âge, avec ses institutions comme les "Corts", son droit hérité du droit romain, wisigothique et féodal et compilé dans les "Usatges", ou encore sa langue, le catalan, qui se constitue en langue administrative, juridique et littéraire à partir du . Par le système politique de monarchie pactiste, la Catalogne conserve ses spécificités et privilèges institutionnels, coutumiers et juridictionnels, appelés constitutions et autres droits, au sein de la couronne d'Aragon puis du royaume d'Espagne, jusqu'aux décrets de Nueva Planta de 1715 et 1716. Après le mouvement de renouveau de la langue et de la culture catalanes de la "Renaixença" dans la deuxième moitié du , le nationalisme catalan ou « catalanisme » se structure idéologiquement à la fin du , tandis que la Catalogne est l'une des rares régions d'Espagne à connaître alors une importante révolution industrielle. De même, le mouvement artistique du modernisme témoigne de l'ouverture sur l'Europe de la région ainsi que du nouveau rayonnement culturel que connaît ce territoire.

Industrialisée depuis le , avec les secteurs historiquement dominants du textile, de la construction navale ou de la mécanique auxquels se sont ajoutés à la fin du ceux du tourisme, de l'automobile, de la chimie, de la pharmacie, de l'agroalimentaire ou de l'informatique, la Catalogne est aujourd'hui la communauté autonome la plus riche d'Espagne et la onzième des subdivisions territoriales de l'Union européenne, avec un produit intérieur brut (PIB) de de dollars en 2012. La communauté fait partie depuis 1988 des Quatre moteurs pour l'Europe avec le Land allemand du Bade-Wurtemberg, la région italienne de Lombardie et celle française de Rhône-Alpes (devenue l'Auvergne-Rhône-Alpes en 2016), et depuis 2004 de l'Eurorégion (devenue en 2009 un Groupement européen de coopération territoriale ou GECT) Pyrénées-Méditerranée avec la région française d'Occitanie ainsi que la communauté espagnole des Îles Baléares (auxquelles s'ajoutait jusqu'en 2006 l'Aragon).

À la suite de lHistoire générale de Languedoc" de Claude Devic et Joseph Vaissète puis des ' de Josep Balari i Jovany, les auteurs s'accordent pour considérer que le toponyme est attesté, pour la première fois, dans le ', un poème latin de relatant l'expédition de Pisans, alliés du comte , contre les Sarrasins des îles Baléares : le substantif ' y figure une fois ; et les adjectifs ' et ', respectivement neuf et six fois.

Le nom de la Catalogne a commencé à être utilisé au en référence au groupe de territoires qui composaient la marche d'Espagne, qui sont progressivement devenus indépendants des autorités franques.

Nombre d'hypothèses ont été émises sur l'étymologie du toponyme mais toutes ont été repoussées, notamment pour des motifs d'ordre phonétique :

Une autre hypothèse pointe les "" (), une tribu ibère établie sur les contreforts méridionaux des Pyrénées orientales, à l'ouest du Llobregat, à l'est du Sègre, au sud de la Noya et de Cervera ; et dont le nom pourrait avoir évolué en "Katelans" par métathèse, puis en "Catalans".
Deux autres hypothèses méritent d'être citées. D'après la première, "catalani" serait dérivé de "*", lui-même dérivé de ' (aujourd'hui, Montcada i Reixac, près de Barcelone). D'après la seconde, il serait dérivé de "catlanus", issu de ' ().

Du fait de son emplacement, le territoire catalan a été l'objet de nombreuses influences externes, souvent simultanément, depuis les temps préhistoriques jusqu'à la naissance de l'Espagne en tant qu'État, ainsi que de nombreuses circulations d'hommes, d'idées, de savoirs et de techniques. Inversement, il a été lui-même une importante source d'inspiration pour d'autres territoires, principalement durant le Moyen Âge, lorsqu'il est devenu le cœur politique et culturel de la couronne d'Aragon. Cette dernière a établi une thalassocratie en Méditerranée occidentale entre le et le , laissant un héritage d'environ de catalanophones à ce jour.

Les plus anciennes traces de peuplement du territoire catalan actuel remontent à la fin du Paléolithique inférieur et au début du Paléolithique moyen, il y a à ans. Si les plus anciens vestiges d'une industrie lithique ont été découverts au nord des Pyrénées, dans la Caune de l'Arago (homme de Tautavel), le plus ancien reste humain au sud est une mandibule de prénéandertalien d'environ ans qui a été mise au jour à Banyoles en 1887.

Plusieurs gisements importants peuvent être cités pour le Paléolithique moyen et le Paléolithique supérieur. En ce qui concerne les sites de l'Épipaléolithique et du Mésolithique, les plus importants gisements conservés sont pour leur part datés entre et ans av. J.-C., dont de nombreux vestiges d'art rupestre levantin.

Le Néolithique débute en terres catalanes environ à ans av. J.-C., même si le degré de sédentarisation était nettement inférieur à celui d'autres régions ; en effet, l'abondance de zones boisées permit de maintenir un rôle majeur à la chasse et à la cueillette au sein des activités de subsistance. Quoi qu'il en soit, plusieurs cultures néolithiques vont se développer dans la région : le Cardial (site de "La Draga" près de Banyoles), la culture des sépultures en fosse montrant des influences du Chasséen (mines de Gavà). Suivent ensuite le Campaniforme au Chalcolithique d'environ av. J.-C. à environ av. J.-C., puis la civilisation des champs d'urnes à l'âge du bronze entre et 750 av. J.-C.

À l'époque pré-romaine, le territoire de Catalogne, comme le reste de la partie méditerranéenne de la péninsule, a été peuplé par les Ibères. D'après les éléments livrés par l'archéologie et les recherches les plus récentes, il semble falloir abandonner l'idée, longtemps défendue par l'historiographie, que les Ibères soient un peuple migrateur venu d'Afrique, mais qu'ils soient le fruit d'apports de population différentes (notamment indo-européennes) ayant fini par développer une culture commune. Les Ibères connaissent un développement qui prend sa source au début du et se termine avec la conquête romaine dans le courant du 

En réalité, ce n'est qu'à partir du que les régions du Nord de l'Ibérie, et surtout la région de la basse vallée de l'Èbre, commencent à prendre plus de visibilité sur le Sud, jusque-là favorisé par ses activités minières et ses relations commerciales avec les peuples puniques. Cette région septentrionale, comprenant l'actuel territoire de la Catalogne, jusque-là d'un caractère plutôt agricole en regard des territoires du Sud, miniers, connaîtra un développement singulier, avec le développement d'une civilisation proto-urbaine, l'entrée dans l'âge du fer et l'invention d'une écriture. Certaines agglomérations deviennent des cités importantes, notamment Ilerda (Lérida) à l'intérieur des terres, Biscargis (à l'emplacement inconnu mais situé au sud de la Catalogne actuelle ou au nord de la Communauté valencienne), Hibera (peut-être Tortosa) ou Indika (Ullastret). Les Ibères de cette région (dont les principaux peuples sont les Ilergetes, les Indigetes, les Lacetani ou les Cerretains) entretiennent également des relations avec les peuples du Nord de la Méditerranée : Gaulois, Grecs, et plus tard Romains. Des colonies marchandes côtières ont été établies par les anciens grecs qui se sont installés à Emporion (Empúries) et à Roses.

Après la défaite des Carthaginois face à la République romaine en -202, ce territoire est devenu la première région ibérique à passer sous domination romaine en intégrant l'Hispanie, la partie occidentale de l'Empire romain. Tarraco (Tarragone) était l'une des plus importantes cités romaines en Hispanie, et la capitale de la province de Tarraconaise (ou Hispanie citérieure). Les autres cités importantes de la période romaine dans cette région sont Ilerda (Lérida), Dertosa (Tortosa), Gerunda (Gérone) ainsi que les ports d'Empuriæ (ancienne Emporion) et Barcino (Barcelone). Il s'agit de l'une des premières parties de l'Empire à se romaniser, entre le et le . Comme pour le reste de l'Hispanie, le droit latin est accordé à l'ensemble des cités sous le règne de Vespasien (69-79), tandis que la citoyenneté romaine est accordée à tous les hommes libres de l'Empire par l'édit de Caracalla en 212 (Tarraco, la métropole, était déjà une colonie de droit romain depuis -45). C'est une province riche, agricole (huile d'olive, vigne, blé), tandis que la période de la République puis du Haut-Empire romain correspond à la construction de routes (dont la plus importante reste la "Via Augusta", parallèle au littoral méditerranéen) et d'infrastructures pour l'irrigation.
Au Bas-Empire, les provinces sont réorganisées en 293 et la Tarraconaise réduite au nord-est de la péninsule, correspondant à un territoire légèrement plus vaste que l'actuelle Catalogne. La christianisation, commencée de manière attestée au , est achevée au . De plus, il s'agit du seul territoire hispanique à rester sous contrôle romain et à ne pas passer sous la domination des Vandales, des Suèves et des Alains au , bien que les principales cités y ait subi des pillages fréquents qui provoquent une certaine désurbanisation (à commencer par le déclin de Tarraco ou la destruction d'Empuriæ). Un repli défensif s'opère vers les grands domaines latinfundiaires de la vallée de l'Èbre ou vers les montagnes. C'est l'installation dans cette région d'un peuple barbare fédéré à Rome, les Wisigoths, qui contribue au maintien d'une certaine stabilité.

Après la chute de l'Empire romain d'Occident, la région est l'un des principaux foyers d'installation des Wisigoths et l'un des points de départ de leur conquête d'une grande partie du reste de la péninsule Ibérique. Ils contribuent à ramener une stabilité et un essor économique, ainsi qu'au développement de nouveaux centres urbains qui existaient déjà à l'époque romaine mais qui étaient jusque-là d'importance secondaire, à commencer par Barcino ou Barcelone. Celle-ci sert d'ailleurs de capitale au Royaume wisigoth en 415, de 507 à 510 et de 531 à 548. En 718, elle est passée sous contrôle musulman et est devenue une partie d'al-Andalus, une province du Califat omeyyade.

Ensemble de comtés qui forment la marche d'Espagne de l'Empire carolingien depuis la conquête par Charlemagne (785-801), la Catalogne naît au . De là, en contact direct avec les territoires restés sous domination musulmane, elle devient l'un des foyers de la "Reconquista" ainsi que l'une des interfaces des échanges commerciaux, culturels, scientifiques et techniques qui s'établissent entre les mondes arabo-musulmans et chrétiens. De nombreuses abbayes sont fondées entre le et le tandis que dans les cités les sièges épiscopaux sont restaurés, formant de riches seigneuries ecclésiastiques ainsi que d'importants foyers artistiques et intellectuels. Ces centres religieux contribuent à une importante diffusion de l'Art roman en Catalogne (abbayes de Ripoll ou de Montserrat, collégiale de Cardona, cathédrale de Gérone) ainsi qu'à l'entretien de riches bibliothèques nourries d'ouvrages antiques, wisigothiques et arabes. C'est là que le philosophe et mathématicien Gerbert d'Aurillac (futur pape sous le nom de Sylvestre II) est formé et apprend, entre autres, le maniement du système de numération décimal (sans le zéro) indo-arabe. Sa langue, le catalan, très proche au Moyen Âge de l'occitan, se développe à partir du .

Le « père fondateur » de la Catalogne serait Guifred le Velu, nommé comte de Barcelone en 878 au concile de Troyes. Guifred le Velu est l'ancêtre de la dynastie de Barcelone, qui construit peu à peu l'État catalan autour du comté de Barcelone, en ignorant la suzeraineté des rois francs considérés de plus en plus comme incapables d'assurer la protection (comme en témoigne la prise de Barcelone en 985 par les troupes maures d'Almanzor sans que le roi Lothaire, pourtant appelé à l'aide par le comte Borrell II, n'intervienne). Ces comtés sont également parmi les lieux de naissance de la paix de Dieu à la fin du , et surtout de la trêve de Dieu qui en découle au , ainsi que de leur institutionnalisation sous le contrôle des Églises locales et de leurs prélats (évêques et abbés réformateurs). La pratique de ces « assemblées de paix » (qui préfigurent les "corts") comme le maintien d'une forte culture juridique de l'écrit (attestée par la rédaction de conventions ou "convenientiæ") vont constituer les bases de cet État catalan en construction, dans une société profondément féodalisée depuis la crise du siècle.

En 1137, le comte de Barcelone épouse l'héritière du royaume d'Aragon. À ce moment naît la couronne d'Aragon qui développe un mode d'administration original, très décentralisé pour répondre aux fortes différences tant politiques qu'économiques et linguistiques des deux parties de la couronne, le royaume d'Aragon et la principauté de Catalogne.
La couronne d'Aragon atteint son apogée avec la conquête du royaume de Valence et le développement de son influence en Méditerranée : les souverains d'Aragon prennent possession de la Sicile, du royaume de Naples et temporairement de la Sardaigne et de la Corse dont ils sont à l'origine du drapeau à tête de maure. Les almogavres, mercenaires catalans, vont créer un éphémère duché en Grèce. Cette expansion explique l'usage de la langue catalane de nos jours au Pays valencien, aux Baléares et dans un bourg de Sardaigne, Alghero. Devenue une véritable thalassocratie, cet ensemble catalano-aragonais joue un rôle de premier plan dans l'essor économique et commercial connu par l'Occident chrétien aux et , porté par le commerce maritime et les activités textiles, contribuant au développement des villes (dont surtout Barcelone mais aussi Gérone, Tarragone, Lérida ou Tortosa) et à l'affirmation d'une nouvelle élite urbaine faite de marchands, négociants ou tisserands, la bourgeoisie.

La frontière avec la France est fixée par le traité de Corbeil de 1258, après l'échec de l'intervention aragonaise lors de la croisade des albigeois. Le Roussillon et le Nord de la Cerdagne sont alors inclus dans la Catalogne. En 1283, la principauté de Catalogne a célébré l'officialisation d'un "Cort General" (le parlement) régulier, qui a approuvé les constitutions catalanes et, en 1359, a créé la Députation du General (ou "Generalitat"), consolidant ainsi le système de gouvernement de la monarchie pactiste ou contractuelle, qui caractérise la Catalogne jusqu'à l'établissement de l'absolutisme au . Par ailleurs, les comtes et les "Corts" ont lancé la compilation, à partir du , de l'ensemble des us et coutumes qui forment les Usages de Barcelone, base du droit catalan.

La principauté de Catalogne amorce son déclin avec la peste noire de 1348 qui touche durement les principales cités de la principauté (à commencer par Barcelone), ainsi qu'à la disparition du roi Martin d'Aragon, le Vieux, dernier souverain de la maison de Barcelone, mort sans héritier en 1410.

À l'extinction de la dynastie catalane d'Aragon en 1410, l'élection du Castillan Ferdinand le Juste en 1412 est suivie par les règnes de ses fils Alphonse V et Jean II. 

En 1462, une rébellion se produit contre Jean II d'Aragon. À cette occasion, le Roussillon et la Cerdagne sont donnés en gage au roi de France Louis XI qui les occupe militairement.

Sous le règne du fils de Jean II, Ferdinand II d'Aragon dit le Catholique, eut lieu l'union dynastique avec la Castille en 1472. Après l'invasion du royaume de Navarre en 1512, les monarchies furent formellement réunies en une seule monarchie d'Espagne en 1516. Chaque royaume de la monarchie conservait ses institutions politiques et maintenait ses propres parlements, lois, administration et monnaies séparées.

Lorsque Christophe Colomb fit sa découverte dans l'Amérique lors d'une expédition sponsorisée par l'Espagne, il commença à déplacer le centre de gravité économique et commercial de l'Europe (et les ambitions de l'Espagne) de la Méditerranée à l'océan Atlantique. Castille et Aragon étaient des entités séparés jusqu'en 1716 malgré une couronne partagée et les colonies nouvellement établies dans les Amériques et le Pacifique étaient castillan. Pendant trois siècles, les Catalans se rebellent à de nombreuses reprises pour défendre leurs droits face à un pouvoir castillan de plus en plus expansionniste et cherchent à échapper à l'effort militaire de l'Empire espagnol.

En 1640 éclate la révolte des faucheurs ("Segadors"). Les Catalans s'opposent au centralisateur ministre Olivares qui veut supprimer leurs privilèges locaux pour les faire participer à l'effort de guerre, jusque-là supporté seul par la Castille. Les Catalans révoltés proclament dans un premier temps une république catalane, puis font appel à Louis XIII, proclamé comte de Barcelone. Par le traité des Pyrénées son fils Louis XIV conclut avec le roi d'Espagne une partition de la Catalogne. Le Roussillon, le Vallespir, le Conflent et le Nord de la Cerdagne rejoignent le royaume de France, tandis que le reste de la principauté, reconquis progressivement par le roi d'Espagne entre 1644 et 1652, se voit reconnaître le respect des lois et institutions catalanes. "Els Segadors" ("Le chant des Faucheurs") est l'hymne national officiel catalan.

La guerre de Succession d'Espagne s'achève le par la prise de Barcelone par les troupes franco-espagnoles. La Catalogne avait choisi le camp de la maison des Habsbourg contre celle des Bourbons. Cette défaite est à l'origine de la fête nationale de la Catalogne ("Diada Nacional de Catalunya"). La nouvelle dynastie affirme l'instauration d'une monarchie absolue et centralisée, le décret de Nueva Planta du abolissant les usages, la "Cort General" et les autres institutions ou fors de la principauté, mettant fin de fait à son indépendance. La Catalogne sort brisée et soumise de cette épreuve et il faut attendre plus d'un siècle pour assister à sa renaissance.

L'union dynastique avec la couronne de Castille en 1479 mais surtout les conséquences de la guerre des faucheurs de 1640-1659, de la prise de Barcelone le par les forces franco-castillanes de Philippe V de Bourbon, des guerres carlistes au ou de la dictature nationaliste et centralisatrice de Francisco Franco entre 1939 et 1975 ont fortement diminué le rôle politique et culturel joué par la Catalogne en Espagne et en Europe.

Elle est annexée à l'Empire français par Napoléon du 26 janvier 1812 au 10 mars 1814 et divisée en quatre départements. Elle s'industrialise rapidement au , et entre ensuite dans l'ère industrielle avec beaucoup plus de dynamisme que la plupart des autres territoires espagnols. Le développement économique entraîne une assez forte urbanisation comme l'atteste l'extension planifiée de Barcelone ("Eixample"), mais aussi le renouveau culturel de la Catalogne (la "Renaixença") et un retour des revendications linguistiques et nationalistes catalanes (le catalanisme). Au tournant du , la Catalogne est l'un des pôles de développement de l'Art nouveau, qui y prend le nom de modernisme catalan, marqué par les productions d'architectes (Antoni Gaudí, Lluís Domènech i Montaner, Josep Puig i Cadafalch), de peintres (Ramon Casas, Santiago Rusiñol), de sculpteurs (Eusebi Arnau, Josep Llimona) et de revues proches du milieu catalaniste ("L'Avenç"). Cette effervescence culturelle culmine avec les expositions universelles qui se tiennent à Barcelone en 1888 puis en 1929-1930. Par la suite, d'autres acteurs majeurs de la scène artistique internationale au se sont formés ou se sont implantés fortement en Catalogne, comme Pablo Picasso, Salvador Dalí, Joan Miró ou Antoni Tàpies. Sur le plan musical, peuvent être cités Pau Casals, Jordi Savall ou les artistes lyriques José Carreras et Montserrat Caballé.

En 1914, les partis catalanistes ont gagné la création de la Mancommunauté de Catalogne, sans autonomie spécifique, mais avec un ambitieux programme de modernisation. Elle est abolie en 1925 par la dictature espagnole de Miguel Primo de Rivera. En 1931 est proclamée la République catalane confédérée à l'Espagne à la suite de la victoire électorale des partis catalanistes de gauche et obtient en échange, après négociation avec le nouveau gouvernement de la République espagnole, un statut d'autonomie en 1932 qui ressuscite l'institution de la Généralité de Catalogne (en catalan : "Generalitat de Catalunya"), présidée par l'indépendantiste de gauche Francesc Macià. Sous la présidence de Francesc Macià (1931-1933) et Lluís Companys (1933-1940), tous deux membres de la Gauche républicaine de Catalogne (en catalan : "Esquerra Republicana de Catalunya", ERC), la Généralité développe un programme social et culturel avancé. Ce statut est suspendu en 1939 lorsque la Catalogne, fidèle à la République, se soumet aux troupes nationalistes de Franco à la fin de la guerre d'Espagne. En 1940, le président catalan, Lluís Companys, est arrêté en France par les Allemands, livré au régime franquiste qui le fait condamner et fusiller au château de Montjuïc.
Durant la dictature franquiste, la répression politique concerne aussi les usages publics de la langue et des symboles de l'identité catalane. Les drapeaux, les hymnes comme "Els Segadors" ou le "Cant de la Senyera", la célébration des fêtes comme la "Diada Nacional", voire la sardane, sont considérés comme des signes de subversion et sont déclarés illégaux. Ainsi, le , l'interprétation du "Cant de la Senyera" par le public au palais de la musique catalane en présence de plusieurs ministres franquistes est l'élément central des événements du Palais de la Musique qui se soldent par la condamnation à sept ans de prison du jeune militant nationaliste Jordi Pujol. Concernant l'enseignement, seules les leçons en langue « chrétienne » (castillane) sont autorisées. Les toponymes sont hispanisés, l'usage du catalan est interdit dans les administrations et en public, par l'introduction du slogan : « Si tu es Espagnol, parle espagnol ! ». Ceci va si loin que le chanteur Joan Manuel Serrat n'a pas le droit de participer au Concours Eurovision de la chanson 1968, parce qu'il veut y chanter la chanson « "La la la" » en catalan. La Catalogne, comme les autres régions aux identités spécifiques (comme le Pays basque, par exemple), commence par réagir en cultivant sa culture dans le domaine privé, puis en s'abstenant massivement aux votes populaires de toutes sortes. Cette résistance passive persiste de façon majoritaire jusque dans les années 1970, ayant trouvé au début des années 1960 son expression dans la ' (la nouvelle chanson). Les compositeurs de chansons tout d'abord anonymes trouvent leurs modèles dans le Folk anglo-saxon, dans la chanson ou dans leur patrimoine de chansons populaires. L'usage s'est tout particulièrement développé de chanter dans les arrière-salles de cafés des chansons en catalan, interdites dans l'espace public. Les compositeurs écrivent eux-mêmes leurs œuvres, et en raison de la répression toujours menaçante, ne se produisent que dans des cadres modestes. Les chants ont souvent pour sujet le sentiment d'allégeance à un groupe. Parmi les représentants connus de la ', on compte Lluís Llach (notamment avec sa chanson ', le pieu, avec laquelle il faisait allusion au régime dictatorial et qui devient l'hymne de la résistance au franquisme), Francesc Pi de la Serra, Maria del Mar Bonet et Raimon. En Catalogne, l'entrée en scène de Raimon le (connue comme le ') est devenu un des événements forts de cette lutte, avec les centaines de milliers de spectateurs affluant malgré les policiers distribuant des coups de matraque autour d'eux. L'abbaye de Montserrat, où sont dites des messes dans la langue catalane interdite, devient aussi connue dans ce contexte. Le chant de louanges à la Vierge ' a remplacé pendant l'ère franquiste l'hymne catalan interdit '.
Après la mort de Franco, la Généralité de Catalogne est rétablie en 1977 avec le retour d'exil de son président Josep Tarradellas. Celui-ci occupe le poste par intérim jusqu'aux élections de 1980, qui voient Jordi Pujol, souverainiste catalan de centre-droit, plusieurs fois emprisonné sous la dictature franquiste, être élu président de la Généralité. Il occupe ce poste pendant six mandats consécutifs. La transition démocratique permet l'expression libre des idées indépendantistes et la restauration d'institutions autonomes. Commençant à se faire sentir durant les deux dernières décennies de la période franquiste, une forte effervescence économique et sociale s'ensuit, portée par le tourisme de masse, l'industrie (automobile par exemple, avec les usines Seat installées dans la zone franche de Barcelone depuis 1953), l'urbanisme innovant de Barcelone qui devient le lieu d'expression d'architectes à la renommée internationale ou les Jeux olympiques d'été de 1992. Par ailleurs, la Catalogne devient un territoire bien intégré aux grands réseaux de la mondialisation et de l'Union européenne, Barcelone étant classée dans la catégorie des villes mondiales « alpha » par le laboratoire d'idées GaWC tandis que l'Eurorégion Pyrénées-Méditerranée est créée avec l'Aragon, les îles Baléares et les régions françaises de Midi-Pyrénées et du Languedoc-Roussillon en 2004. Par ailleurs, un homme politique catalan, Josep Borrell, occupe la présidence du Parlement européen de 2004 à 2007.

La crise économique et du logement de la fin des années 2000 et du début des années 2010 ainsi que la décision du Tribunal constitutionnel espagnol d'invalider plusieurs dispositions du statut d'autonomie entraînent d'importantes tensions sociales et politiques en Catalogne de même qu'entre la communauté et le gouvernement central. Cela aboutit à la montée du mouvement des Indignés et de l'indépendantisme catalan, caractérisé par la victoire de la liste "Barcelona en comú" de la gauche radicale et écologiste aux élections municipales de 2015 à Barcelone, permettant l'accession au poste de maire de l'activiste Ada Colau, et par celle de l'alliance indépendantiste Junts pel Sí aux élections au Parlement de Catalogne de la même année. Une déclaration sur le lancement du processus d'indépendance de la Catalogne est adoptée par le Parlement de Catalogne le .

Le , un référendum pour l'indépendance de la Catalogne est annoncé. Ce dernier se tient le dans un contexte de vives tensions entre l'État espagnol, qui a déclaré cette consultation illégale, et la Généralité de Catalogne. En effet, le Tribunal constitutionnel espagnol a suspendu le la loi définissant les modalités légales de création d'un État catalan votée par le Parlement de Catalogne. Le parquet ordonne aux forces de police d'empêcher la tenue de tout référendum. Bien qu'étant soutenu par l'essentiel de ses homologues européens, le gouvernement espagnol de Mariano Rajoy est vivement critiqué dans la presse internationale pour l'ampleur de la répression.

Lors d'une intervention devant le Parlement réuni le 2017, Carles Puigdemont proclame que . Peu après la fin de la séance, les députés indépendantistes signent une déclaration qui reconnaît . Bien que le texte n'y fasse pas référence, son application est suspendue en conséquence du discours de Carles Puigdemont devant les parlementaires, . La déclaration ne sera pas publiée au "Journal officiel" de la communauté autonome et n'a pas été formellement approuvée par un vote des députés, ce qui la laisse sans valeur ni effet juridiques aux termes de la loi de transition juridique, qui imposait une proclamation par le Parlement réuni en session.

Le 27 octobre 2017, la Catalogne engage un pour se séparer de l'Espagne suivi quelques minutes plus tard par un vote du Sénat espagnol autorisant la mise sous tutelle de la Catalogne en vertu de l'article 155 de la Constitution.

Avec une superficie de , la Catalogne est la sixième communauté la plus étendue d'Espagne. Offrant une certaine diversité de biotopes et de paysages qui ont été forgés par des conditions géologiques, hydrographiques, climatiques et anthropiques particulières, le territoire catalan est constitué en 2009 à 40 % de forêts, à 29,1 % de sols cultivés, à 16,3 % de "matollars" (garrigues), à 6,2 % de zones urbanisées, à 5,3 % de prairies, à 2,5 % de nu naturel et à 0,6 % d'eaux continentales.

La Catalogne est bordée par la mer Méditerranée au sud (mer des Baléares) et à l'est (golfe du Lion), bordant les littoraux touristiques de la Costa Brava, de la Costa del Maresme et de la Costa Daurada. Au nord, les Pyrénées constituent une frontière naturelle avec la France (région Occitanie) et l'Andorre. Les autres communautés autonomes espagnoles d'Aragon et de la Communauté valencienne la bordent respectivement à l'ouest et au sud, tandis que les îles Baléares sont situées au large de ses côtes en mer Méditerranée.

La Catalogne a une diversité géographique remarquable, compte tenu de la taille relativement petite de son territoire. Elle est conditionnée par la côte méditerranéenne, à l'est, avec de côtes, les grandes unités de relief issues des Pyrénées, au nord, et le bassin hydrographique de l'Èbre, au sud.

Le relief catalan présente ainsi environ trois unités structurantes :
Les Pyrénées catalanes représentent près de la moitié orientale de la longueur des Pyrénées car elles s'étendent sur plus de , du massif du Besiberri à celui des Albères au cap de Creus. Le point culminant de la Catalogne, qui se trouve au nord de la région de Pallars Sobirà, sur la frontière franco-espagnole dans les Pyrénées centrales, est la Pique d'Estats (), suivi du Comaloforno (, parfois considéré par les Catalans comme le véritable point culminant car n'étant pas frontalier), du Puig Pedrós (), du pic de Médécourbe () et du Puigmal (). Les paysages pyrénéens sont marqués par l'absence de grands lacs, comparativement au massif des Alpes par exemple, la rareté et l'altitude élevée des cols ainsi que des vallées orientées du nord vers le sud toutefois moins escarpées que leurs homologues françaises. Se différenciant des Pyrénées axiales, les pré-Pyrénées sont des formations de montagnes parallèles, mais avec des altitudes plus basses, moins raides et dont la formation géologique est différente. Parmi ces chaînes piémontaises catalanes figurent les massifs de Montsec, Boumort, Port del Comte, du Cadi, de Moixeró, Pedraforca ou Catllaràs.
Plus au sud, le système méditerranéen catalan repose sur deux cordillères sensiblement parallèles à la côte, orientées du nord-est, où elles s'appuient sur les contreforts pyrénéens par le biais d'une cordillère transversale (sommets volcaniques de la Garrotxa), vers le sud-ouest, où elles font le lien avec le système ibérique de l'autre côté de la basse-vallée de l'Èbre par le massif de transition que sont les ports de Tortosa-Beseit. Ce système comprend alors la cordillère littorale à l'est, d’une part, et la cordillère pré-littorale, davantage à l'intérieur des terres, d’autre part. La cordillère littorale, qui va de la plaine de l'Empordà au nord-est jusqu'à Tarragone au sud-ouest, est de moindre longueur et d’altitude plus basse que celle pré-littorale, qui s'étend de la cordillère transversale de la Garrotxa jusqu'à Montsià. Leurs points culminants respectifs sont le Montnegre () et le mont de l'Homme dans le massif du Montseny (), tandis que Montserrat est l'un des massifs les plus représentatifs de ce système. Au sein de cet ensemble, se trouve une série de faibles reliefs comprenant les plaines littorales dominées par les versants orientaux de la cordillère littorale, relativement étroites mais s'élargissant au nord dans la plaine de l'Empordà, et une dépression pré-littorale, fosse tectonique séparant les deux cordillères (vallées du Vallès Oriental et Occidental, du Haut et Bas Penedès). La plaine de la Selva sépare pour sa part la côte des cordillères littorales, pré-littorales et transversales. Le système est coupé en deux approximativement en son milieu par la vallée du Llobregat. Fortement artificialisée du fait de l'étalement urbain des agglomérations barcelonaises, tarragonaises et gironines, mais aussi en raison du tourisme de masse sur les plaines littorales (Costa Daurada, Costa del Maresme et Costa Brava), cette partie du territoire catalan est également occupée par plusieurs régions viticoles.

Enfin, occupant une grande partie de l'arrière-pays catalan, la dépression centrale catalane est une plaine située entre les Pyrénées au nord et la cordillère pré-littorale à l'est et au sud. Le Sud de la province de Lérida (le Ponent) et le Centre de celle de Barcelone (les Comarques centrales) occupent ce territoire. Il s'agit d'une vaste plaine dont l'altitude varie entre 200 et . Le bassin sédimentaire et les eaux qui descendent des Pyrénées, qui forment la partie nord-orientale du réseau hydrographique de l'Èbre, ont créé un terreau fertile pour les terres agricoles, renforcé par la construction de nombreux canaux d'irrigation, tout en servant à la production hydroélectrique.

Presque toute la Catalogne appartient au bassin méditerranéen. Le réseau hydrographique catalan comprend deux grands bassins versants qui descendent tous deux vers la mer Méditerranée : celui occidental ou de l'Èbre, alimenté par de nombreux affluents (le Sègre, long de en Catalogne) ou sous-affluents (la Noguera Pallaresa, la Noguera Ribagorzana) à régime nival descendant suivant un axe nord-sud des Pyrénées jusqu'au fleuve sur son versant septentrional, le tout s'etendant sur en soit 48,72 % de la superficie catalane (et représentant 18,27 % de la totalité du bassin de l'Èbre) ; et celui oriental dit des bassins internes de Catalogne constitués de plusieurs petits fleuves côtiers (le Ter long de , le Llobregat de ), couvrant une superficie de soit près de 52 % du territoire de la communauté autonome. Par ailleurs, le bassin de la Garonne, qui pour sa part se jette dans l'océan Atlantique, ne couvre que 1,73 % du territoire catalan dans les Pyrénées au nord.
Le bassin occidental est le plus important, apportant une moyenne de par an, tandis que le bassin oriental ne fournit qu'une moyenne de par an. Ce déséquilibre est dû aux forts débits fournis par l'Èbre, fleuve le plus long et le plus puissant de la péninsule Ibérique, et l'un de ses principaux affluents, le Sègre. Irriguant l'essentiel de la dépression centrale catalane ainsi que le delta de l'Èbre au sud de la communauté, ce bassin occidental a grandement contribué au développement de l'agriculture depuis le Néolithique, tandis que c'est là qu'ont été construits l'essentiel des barrages hydroélectriques de Catalogne, dans les contreforts ou basses vallées du piémont pyrénéen. La Catalogne est, en outre, relativement riche en eaux souterraines, mais là encore les inégalités sont importantes entre les comarques, étant donné la structure géologique complexe du pays. Dans les Pyrénées catalanes et leurs piémonts persistent plusieurs lacs glaciaires. Le plus important lac naturel de Catalogne est toutefois d'origine karstique, à savoir celui de Banyoles dans le Pla de l'Estany au nord-est ().

La côte catalane est presque rectiligne, d'une longueur de plus de et avec peu d'accidents géographiques, les plus importants restant le cap de Creus et le golfe de Roses au nord ; le delta de l'Èbre au sud ; les deux segments de la cordillère littorale où celle-ci plonge directement dans la mer, l'un entre L'Estartit et la ville de Blanes sur la Costa Brava, et l’autre au sud de la comarque de Garraf.

La Catalogne est essentiellement soumise à un climat méditerranéen tempéré par sa latitude au sein de l'hémisphère nord. Les zones les plus densément peuplées sur le littoral des provinces de Tarragone, Barcelone et Gérone sont les plus représentatives de ce domaine climatique (étés très chauds et secs, printemps et automnes plus humides et pluvieux, hivers doux). Toutefois, en raison de la topographie variée, ce climat développe de nombreuses caractéristiques particulières selon les endroits, en étant notamment soumis à une influence montagnarde (avec plus de précipitations et des températures moyennes qui baissent avec l'altitude, pouvant se diviser entre une influence de moyenne montagne dans l'essentiel de la Cordillère pré-littorale et du piémont pyrénéen, avec moins de neige, et de haute montagne dans la plupart des massifs pyrénéens, la Cordillère transversale et quelques ilots dans la Cordillère pré-littorale) et continentale (sec la moitié de l'année, forte amplitude thermique entre les saisons) dans la partie occidentale de la dépression centrale. Par ailleurs, deux autres domaines climatiques s'étendent dans des parties plus réduites du territoire de la Communauté : un climat alpin (basses températures et neige en hiver, fortes précipitations) sur les plus hauts sommets des Pyrénées le long de la frontière avec la France au nord, un climat océanique (très pluvieux, froid et humide) dans le versant supérieur du bassin de la Garonne correspondant au val d'Aran.
De ce fait, les températures moyennes annuelles peuvent varier assez fortement selon les endroits, allant de dans les Pyrénées à dans le delta de l'Èbre. Les températures les plus extrêmes jamais enregistrées sont montées jusqu'à (à Lérida et Igualada en ou encore Montblanc le ), et descendues jusqu'à (au lac Gento à d'altitude dans les Pyrénées en ). En règle générale, l'intérieur de la Catalogne est très chaud et sec en été, subissant par ailleurs un effet de foehn causé par les Pyrénées. La température peut atteindre , voire . En revanche, les nuits sont plus froides que sur la côte, avec des températures de l'ordre de à . Dans les régions littorales, la température maximale moyenne est d'environ 25 à .
De même, pour les précipitations, la Catalogne peut être divisée en deux régions :
Le vent dominant, comme dans une grande partie de l'Europe, est le ponant ou vents d'ouest, venant de l'océan Atlantique et de l'anticyclone des Açores. Par ailleurs, tant au nord qu'au sud peuvent dominer des vents de couloir plus puissants, froids et secs venant du nord générés par l'effet Venturi et le passage de massifs montagneux, que ce soit la Tramontane soufflant du nord surtout dans la plaine de l'Empordà, ou le Mistral (d'une autre origine que le Mistral français, causé par des perturbations des vents dominants traversant les Pyrénées et la basse vallée de l'Èbre entre les Cordillères ibériques et pré-littorales catalanes) venant du nord-ouest dans le delta de l'Èbre. S'y ajoutent des vents plus irréguliers comme les brises de mer ("Marinada") ou de montagne ainsi que le foehn. Les vitesses moyennes annuelles du vent (à du sol) vont de à Vielha dans le val d'Aran (protégée par les montagnes environnantes) jusqu'à à Portbou (où l'observatoire se situe au sommet d'une montagne).

Enfin, l'ensoleillement est étroitement lié à la nébulosité et non à la pluviométrie à proprement parler. Cette insolation se situe en Catalogne entre et annuelles.

La variété des substrats géologiques, des sols, des conditions climatiques, d'altitudes ou de distance par rapport à la mer a permis le développement d'une biocénose assez diversifiée, offrant un échantillon représentatif à petite échelle des paysages ouest et sud européens. Il y a plus de 600 types d'habitats naturels et semi-naturels. La faune et la flore présentent un relativement faible taux d'endémisme, avec une majorité d'espèces se trouvant en d'autres endroits d'Europe, tout particulièrement dans le domaine méditerranéen. Toutefois, si 65 % de la Communauté autonome sont peu ou pas artificialisés, la Catalogne est également très vulnérable aux pressions anthropiques auxquelles elle est soumise.

En effet, plus de 7 millions de personnes, soit la quasi-totalité de la population catalane, sont concentrées dans 30 % du territoire principalement dans les plaines littorales. À l'agriculture intensive, à l'élevage et aux activités industrielles se sont ajoutés un afflux touristique massif — plus de 20 millions de visiteurs annuels —, un taux d'urbanisation voire de métropolisation important qui a entraîné un fort étalement urbain — les deux tiers des Catalans habitent dans l'aire urbaine de Barcelone, tandis que la proportion de sols urbanisés est passée de 4,2 % en 1993 à 6,2 % de la totalité du territoire catalan en 2009, soit une croissance de 48,6 % en seize ans — et un réseau dense d'infrastructures de transports. Ceci s'accompagne d'une certaine déprise agricole (baisse de -15 % de l'ensemble des espaces cultivés en Catalogne entre 1993 et 2009) et d'une menace pour les milieux naturels, surtout pour les "matollars" -1,3 % sur la même période). Les activités humaines ont également mis certaines espèces animales en péril, voire ont entraîné leur disparition du territoire, comme pour le loup gris et probablement l'ours brun des Pyrénées. La pression créée par ce modèle de vie fait que l'empreinte écologique du pays dépasse, et de plus en plus, la superficie administrative de la Communauté.
Face aux problématiques ainsi posées, les autorités ont initié plusieurs mesures ou institutions ayant pour protéger les écosystèmes naturels. Ainsi, en 1990, le gouvernement catalan a créé le Conseil de rotection de la nature ("Consell de Protecció de la Natura"), un organe consultatif ayant pour but d'étudier, de protéger et de gérer les milieux naturels et paysages de Catalogne. Par ailleurs, la Généralité a également lancé un Plan d'espaces d'intérêt naturel ("Pla d'Espais d'Interès Natural" ou PEIN) en 1992 tandis que dix-huit espaces naturels de protection spéciale ("Espais naturals de protecció especial" ou ENPE) ont été institués, avec un seul parc national, celui d'Aigüestortes et lac Saint-Maurice, quatorze parcs naturels de Catalogne – des Hautes-Pyrénées ("Alt Pirineu"), des marais de l'Empordà, de Cadí-Moixeró, du cap de Creus, des sources du Ter et du Freser, de Collserola, du delta de l'Èbre, des Ports, du Montgrí, îles Medes et bas-Ter, du Montseny, de Montserrat, de Sant Llorenç del Munt et de l'Obac, de la serra de Montsant et de la zone volcanique de la Garrotxa — ainsi que trois endroits naturels d'intérêt national ("Paratge natural d'interès nacional" ou PNIN) — le Pedraforca, le bois de Poblet et les Albères.

Le territoire catalan offre donc une forte diversité de milieux et de paysages, particulièrement impactés par les activités humaines, liées à un réseau dense d'infrastructures de transports et de communication.

De par sa situation, sa topographie et son histoire, la Catalogne est une des régions de la péninsule Ibérique les plus ouvertes sur le reste de l'Europe et du monde, s'appuyant sur des réseaux de communication terrestres, maritimes et aériens relativement denses. Les réseaux sont essentiellement organisés en étoile autour de Barcelone, tandis que l'axe tracé par l'antique "Via Augusta" romaine le long de la côte méditerranéenne reste d'une importance majeure dans l'organisation des transports terrestres (routiers et ferroviaires) catalans mais aussi à l'échelle européenne (route européenne 15 ou E15, lignes à grande vitesse ou LGV de la Renfe-SNCF en Coopération).


Le réseau routier, centralisé vers Barcelone, s'étend sur environ , dont d'autoroutes ou voies express à grande capacité (689 d’"autopistes", presque entièrement à péage, 769,5 d’"autovies" majoritairement gratuites, 104 de routes dédoublées totalement gratuites et 86 de voies préférentielles payantes à une seule chaussée).

Les acteurs de son aménagement sont la Généralité de Catalogne (qui a autorité sur de routes, soit un peu moins de la moitié du réseau), les Députations provinciales (compétentes pour de voies, soit plus du tiers du réseau), l'État (à travers le ministère de l'Équipement du gouvernement central espagnol, qui est titulaire de de routes en Catalogne, soit environ 1/6 du réseau).

La principale autoroute, l'AP-7, est aussi connue sous le nom d’"Autopista de la Mediterrània". Elle traverse toute la Communauté en suivant la côte à l'est, depuis la frontière française (col du Perthus), où elle est reliée à l'autoroute A9 "La Catalane" et au reste de la route européenne 15 (E15, Inverness-Algésiras), et s'étend au-delà vers le sud (Communauté valencienne, région de Murcie et Andalousie). L'AP-2 et l'A-2, surnommées respectivement "Autopista" et "Autovia del Nord-est", relient Barcelone et l'intérieur de la Catalogne (Lérida) à Saragosse et, au-delà, à Madrid.


L'organisation du réseau ferroviaire catalan suit principalement celle du réseau routier. En effet, là encore le poids du carrefour barcelonais s'impose, les principales lignes convergeant vers la capitale tandis que son agglomération est elle-même parcourue par de nombreux trains de banlieue appelés "Rodalies" ainsi que par l'unique métro de Catalogne (le deuxième plus étendu d'Espagne). De même, les liaisons entre la frontière française au nord-est et celle avec la communauté valencienne au sud-ouest, en suivant la ligne de côte, forment un axe particulièrement important et fréquenté. Le réseau s'étend sur .
Il s'agit d'un réseau ancien, puisque la Catalogne, compte tenu de la précocité de son industrialisation, a vu la première construction de chemins de fer dans la péninsule Ibérique, en 1848, avec la liaison entre Barcelone et Mataró. Environ 80 % des axes de chemin de fer actuels ont été tracés au . Pour sa part, le développement de la grande vitesse est légèrement plus tardif que dans les communautés castillanes ou andalouses. Ainsi, si la première ligne à grande vitesse (LGV) espagnole a été inaugurée entre Madrid et Séville en 1992, les trains express appelés "AVE" desservent Lleida depuis 2002, Tarragone à partir de 2006 et Barcelone depuis le , réduisant alors le trajet entre la capitale espagnole et la métropole catalane à deux heures trente environ. Pour autant, il s'agit du premier territoire ibérique à être relié au reste de l'Europe par le train à grande vitesse, depuis la première liaison rapide réalisée entre Perpignan et Figueras en 2010, traversant les Pyrénées par un tunnel de de long sous le col du Perthus, et le raccordement de cette ligne au reste du réseau à grande vitesse espagnol par l'inauguration de la LGV Barcelone-Figueras en 2013. Cette dernière ligne doit être prolongée en une nouvelle LGV littorale, appelée « corridor méditerranéen », pour relier vers le sud-ouest Valence et Alicante dans la Communauté valencienne, Murcie dans la région de Murcie et Almería, Grenade, Malaga et Algésiras en Andalousie. Ce projet, initié dès les années 1980 et qui créerait ainsi le premier axe ferroviaire à grande vitesse espagnol ne passant pas par Madrid, a connu de nombreux retards dans sa réalisation.

Les compagnies de chemins de fer actives en Catalogne sont les "Ferrocarrils de la Generalitat de Catalunya" (FGC), pour les chemins de fer appartenant à la Généralité de Catalogne, et la Renfe, la compagnie nationale espagnole, qui opère des trains seule ou en coopération avec la Société nationale des chemins de fer français (SNCF) pour les LGV transfrontalières.


La plate-forme de correspondance du réseau aérien catalan est l'aéroport international de Barcelone-El Prat (BCN), situé dans la commune d'El Prat de Llobregat et la banlieue sud-est de Barcelone. Propriété de l'État espagnol et géré par ENAIRE, il sert de "hub" pour les compagnies à bas prix Vueling et Level, ainsi que de base majeure pour Iberia, Air Europa, Air Nostrum, EasyJet, Norwegian Air International et Ryanair. Avec en 2016, il est le deuxième aéroport d'Espagne et de péninsule Ibérique derrière l'aéroport Adolfo-Suárez de Madrid-Barajas, le septième d'Europe et le au monde. En se limitant aux seuls passagers internationaux ( en 2016), il reste toujours le deuxième aéroport d'Espagne mais en réduisant son écart avec Madrid-Barajas, est le neuvième d'Europe et le mondial. Un aérodrome servant uniquement à l'aviation générale se trouve également dans l'aire métropolitaine barcelonaise, à Sabadell (QSA).

Les trois autres capitales provinciales et grandes agglomérations catalanes disposent également de leurs aéroports internationaux : l'aéroport de Gérone-Costa Brava (GRO) au nord-est qui, grâce à l'implantation de Ryanair qui en a fait l'une de ses bases majeures, a vu sa fréquentation fortement augmenter augmenter au cours des années 2000 au point de devenir, avec un pic à de passagers en 2008, retombé à en 2016, le deuxième aéroport de Catalogne ; l'aéroport de Reus (REU) au sud-ouest, près de Tarragone, qui dessert la Costa Daurada et l'important complexe de loisirs PortAventura World avec des vols internationaux essentiellement saisonniers ; l'aéroport de Lleida-Alguaire (ILD) à Alguaire près de Lérida à l'ouest, qui est le seul de ces aéroports internationaux à ne pas être une propriété étatique mais à appartenir à la Généralité de Catalogne. Enfin, un aéroport essentiellement domestique, celui d'Andorre–La Seu d'Urgell (LEU), également propriété de la Généralité de Catalogne, dessert autant les Pyrénées catalanes que la principauté d'Andorre, avec quelques liaisons internationales avec la France ou le Portugal.


La Catalogne est, depuis le Moyen Âge, bien intégrée dans les réseaux maritimes internationaux. Le port de Barcelone est un port industriel, commercial et touristique d'importance mondiale. Avec en 2015, il s'agit du premier port à conteneurs de Catalogne, le troisième d'Espagne après ceux de Valence dans la Communauté valencienne et d'Algésiras en Andalousie, le de mer Méditerranée, le européen et le mondial. Mais il s'agit surtout du sixième plus grand port de croisière au monde et du premier en Europe ainsi qu'en Méditerranée, avec accueillis en 2014. Les ports de Tarragone au sud-ouest et de Palamós près de Gérone au nord-est sont beaucoup plus modestes.

L'aménagement de ces infrastructures, fruit de la topographie et de l'histoire du territoire catalan, répond ainsi fortement à l'organisation administrative et politique de cette communauté autonome.

Les questions de son statut, de son degré d'autonomie (voire de son indépendance) et de la reconnaissance de son identité historique, culturelle et linguistique particulière jouent historiquement un rôle de premier plan en Catalogne, structurant sa vie institutionnelle et politique. Ancien État féodal puis moderne formé aux et et ayant existé jusqu'aux décrets de Nueva Planta de 1716 sous le nom de principauté de Catalogne ("Principat de Catalunya" en catalan), elle a été liée par une union personnelle au royaume d'Aragon à partir de 1137 au sein de la couronne d'Aragon, puis à la couronne de Castille à partir de 1479 au sein de la monarchie catholique espagnole. Après 1716, elle est désormais une province du nouvel État unitaire et absolutiste puis constitutionnel qu'est l'Espagne. Elle dispose ensuite d'une autonomie politique, avec la création d'une institution reprenant le nom de l'ancien gouvernement de l'État catalan (la Généralité), de 1932 à 1939 et depuis 1980 sous la forme d'une communauté autonome espagnole. De plus, il est à noter que la Catalogne a été rattachée au Premier Empire français sous la forme de quatre puis deux départements de 1812 à 1814, et s'est constitué à plusieurs reprises en républiques aux existences éphémères ou contestées (en 1641, en 1873, en 1931, en 1934 et en 2017).

La Catalogne est une des dix-sept communautés autonomes d'Espagne. Elle a accédé à l'autonomie le , avec l'entrée en vigueur de la loi organique du portant statut d'autonomie de la Catalogne.

La Constitution espagnole de 1978 déclare que l'Espagne est une nation indissoluble qui reconnaît et garantit le droit à l'autonomie des régions qui la constituent. On reconnaît à la Catalogne, comme au Pays basque et à la Galice, un statut particulier de « communauté historique ». Compte tenu de la capacité d'accéder à l'autonomie, cela a entraîné en 1979 le statut d'autonomie de la Catalogne. Dans un processus initié par l'Andalousie et achevé en 1985, les quatorze autres communautés autonomes ont obtenu leurs propres statuts d'autonomie. À partir de 2003, on enregistre une série d'amendements concernant les divers statuts d'autonomie (notamment, aux côtés de la Catalogne, ceux de l'Aragon, la Communauté valencienne, les îles Baléares et les îles Canaries).

D'après le statut d'autonomie de 1979 et l'actuel, approuvé en 2006, « la Catalogne, en tant que nationalité, exerce son gouvernement autonome en se constituant en communauté autonome, conformément à la Constitution et au présent statut, qui est sa norme institutionnelle fondamentale ».

Le préambule de 2006 sur le statut d'autonomie affirme que le Parlement a défini la Catalogne comme une nation, mais que la Constitution espagnole reconnaît la Catalogne comme une réalité nationale. Le préambule n'a pas de valeur juridique, donc le statut est le même que ce qu'il était en 1979, c'est-à-dire une communauté autonome. Bien que ce statut ait été approuvé à la fois par le Parlement catalan et par le Parlement espagnol et, plus tard, par un référendum en Catalogne, il a été juridiquement contesté par la communauté autonome d'Aragon, la communauté autonome des îles Baléares et la Communauté valencienne, ainsi que par le Parti populaire. Les objections sont fondées sur divers aspects tels que le patrimoine culturel et le principe de « solidarité entre les régions ». En novembre 2008, le Tribunal constitutionnel est chargé d'évaluer la constitutionnalité des articles en cause. Le 10 juillet 2010, elle récuse les nouveaux statuts comme non conformes à la constitution sur plusieurs points tels que les notions de nation, de justice autonome et la fiscalité. Cette décision entraîne une manifestation rassemblant plus d'un million de personnes le lendemain.

Le 21 octobre 2017, le chef du gouvernement espagnol Mariano Rajoy annonce qu'il va utiliser l'article 155 de la Constitution pour revenir à la légalité et respecter le statut d'autonomie, après le référendum sur l'indépendance de la Catalogne du octobre 2017 (déclaré illégal par Madrid).

Le 27 octobre, le Parlement valide la résolution de déclaration d'indépendance. Après l'approbation par le Sénat espagnol de l'application de l'article 155 de la Constitution, Mariano Rajoy, président du gouvernement espagnol, dissout la chambre catalane et annonce la tenue d'élections régionales pour le 21 décembre 2017.

La Catalogne dispose de sa propre autonomie et possède des compétences dans quelques domaines. Le , le Parlement catalan a adopté le projet de loi de réforme du statut de la Catalogne, qui a ensuite été débattu devant l'Assemblée parlementaire espagnole à Madrid. Après des discussions ayant montré des divisions, et une révision à la baisse négociée par le président du gouvernement espagnol et le chef du premier parti catalan, le projet a été adopté par l'Assemblée et proposé aux Catalans par référendum. Malgré certains indépendantistes ayant appelé à voter non (car le projet ne reconnaissait pas la Catalogne comme nation, ne lui laissait pas la totale maîtrise des impôts, des ports et des aéroports), presque 75 % des votants l'ont accepté le . 
Cependant le taux de participation était légèrement inférieur à 50 %. Le nouveau statut a été en partie annulé par le Tribunal constitutionnel le 10 juillet 2008 (6 % des articles furent annulés ou amendés).

La Généralité de Catalogne (en catalan : "Generalitat de Catalunya") est l'institution dans laquelle l'autonomie de la Catalogne est organisé. Il se compose par le parlement, la présidence, le gouvernement et d'autres institutions créées par le pouvoir législatif.

Le Parlement de Catalogne ("Parlament de Catalunya", en catalan) est l'organe législatif. Il représente le peuple de Catalogne, vote les lois de son compétence, le budget, contrôle l'action du gouvernement et établit d'autres institutions catalanes. Lors des élections d', la coalition Ensemble pour le oui (JxSí, alliance l'indépendantiste entre CDC et la Gauche républicaine de Catalogne, ERC) et la Candidature d'unité populaire (CUP, indépendantiste de gauche anticapitaliste), défendant un programme indépendantiste, ont remporté 73 députés sur 135 au parlement, avec 47,8 % des suffrages exprimés. Il est présidé par Carme Forcadell après cette date.

Le président de la Généralité de Catalogne ("", en catalan) est le plus haut représentant de la Catalogne, et est chargé de diriger l'action du gouvernement. Depuis 2016, Carles Puigdemont, membre du Parti démocrate européen catalan (PDeCat), soutenu par la coalition Ensemble pour le oui (JpS) et la Candidature d'unité populaire (CUP), est président de la Généralité. Il prend la suite d'Artur Mas, au pouvoir à partir de 2010.

Le gouvernement de Catalogne ("Govern de Catalunya", en catalan), est l'organe collégial chargé de la direction de la politique et de l'administration publique de la Généralité, il détient le pouvoir exécutif et réglementaire. Il est composé du président de la Généralité, du premier conseiller (ou du vice-président) et des conseillers.

 La Catalogne est divisée aujourd'hui en trois divisions administratives : les municipalités (en catalan "municipis"), les "comarques", niveau administratif comparable aux communautés de communes françaises, et les provinces (en catalan "províncies"), division générale de l'Espagne, mais les provinces sont actuellement en cours de remplacement par une nouvelle division régionale catalane, les vigueries (en catalan "vegueries").


La communauté autonome de Catalogne couvre une superficie de avec une population estimée à habitants en janvier 2016, les immigrants en représentant 15,73 %. La Catalogne est la communauté autonome d'Espagne qui reçoit le plus grand nombre d’immigrants : l’arrivée d'immigrés entre 1998 et 2009 a représente 77 % de la croissance de la population de cette région durant cette période.

La région urbaine de Barcelone comprend personnes et couvre une superficie de . L'aire métropolitaine de la région urbaine comprend des villes comme L'Hospitalet de Llobregat, Badalona, Santa Coloma de Gramenet et Cornellà de Llobregat.

En dehors de Barcelone, il y a d'autres villes importantes, comme Tarragona, Lleida, Girona.

La région métropolitaine de Tarragone comprend personnes et est la deuxième région métropolitaine de Catalogne.

Entre 1900 et 2001, la population de la Catalogne a été multipliée par 3. Cette augmentation est due à l'expansion démographique en Espagne au cours des années 1960 et au début des années 1970 et aussi à l'exode rural. Cette vague de migration est arrivée dans plusieurs régions d'Espagne, en particulier l'Andalousie, l'Estrémadure et Murcie.

Originaire du territoire historique de la Catalogne, le catalan en est une des trois langues officielles et jouit d'un statut particulier depuis l'approbation du statut d'autonomie de la Catalogne de 1979, qui déclare qu'il est le langage « propre à la Catalogne ». Les autres langues qui ont un statut officiel sont l'espagnol ou castillan, officiel dans toute l'Espagne, et l'occitan (l'occitan gascon, parlé dans le val d'Aran, appelé localement aranais).

Sous la dictature franquiste, le catalan est, de 1939 jusque dans les années 1970, exclu du système d'éducation public et de toutes les autres institutions officielles et publiques. Il est même interdit de donner aux enfants des prénoms catalans. L'exode rural en provenance d'autres zones de l'Espagne réduit l'usage social de la langue dans les zones urbaines. Dans une tentative visant à inverser cette tendance, le rétablissement de l'autonomie des institutions de la Catalogne a entrepris une politique linguistique à long terme visant à accroître l'utilisation du catalan et a, depuis 1983, promulgué des lois qui visent à protéger et à étendre l'usage du catalan. Certains groupes considèrent ces efforts comme une manière de décourager l'utilisation de l'espagnol.

Aujourd'hui, le catalan est la langue principale du gouvernement autonome de Catalogne et des autres institutions publiques qui relèvent de sa juridiction, coofficielle sur le territoire à côté de l'espagnol. L'éducation publique de base est dispensée en catalan, à l'exception de trois heures par semaine consacrées au castillan.

Selon l’enquête linguistique réalisée en 2008 par le gouvernement de la Catalogne, qui diffère sensiblement de celle de 2003, une majorité revendique l'espagnol comme la langue à laquelle elle s’identifie (46,5 % pour l'espagnol contre 37,2 % pour le catalan ; en 2003 les chiffres étaient de 47,5 % pour l'espagnol et 44,3 % pour le catalan ; entre-temps la part de ceux qui s’identifient autant à l’une qu’à l’autre langue a progressé, passant de 5,0 % à 8,8 %). Dans la vie quotidienne, l’usage habituel du catalan est passé de 46,0 % à 35,6 % (de 47,2 % à 45,9 % pour l’espagnol ; et de 4,7 % à 12,0 % pour l’emploi indistinct de l’une comme de l’autre). 55,0 % des citoyens ont déclaré l'espagnol comme langue maternelle, pour 31,6 % le catalan (en 2003, respectivement 56,1 % et 36,2 %), et 3,8 % déclarent deux langues maternelles (contre 2,5 % en 2003). Enfin, 94,6 % des personnes interrogées déclarent comprendre le catalan ; 78,3 %, le parler ; 81,7 %, le lire ; 61,8 % l’écrire (les chiffres pour l’espagnol sont respectivement de 99,9 %, 99,7 %, 97,4 % et 95,6 %).

De même, grâce au statut d’autonomie de 1979, l’aranais (la variété d’occitan parlée en val d'Aran) est devenu officiel et a été soumis à une sauvegarde spéciale dans le val d’Aran. Ce petit espace de habitants est le seul endroit où un dialecte de l’occitan a reçu un statut officiel. Depuis le 9 août 2006, avec l’entrée en vigueur du nouveau statut, l’occitan est devenu officiel dans toute la Catalogne.

En Catalogne, l’instruction est obligatoire de six à seize ans, et l’école publique est gratuite. Tout l'enseignement en Catalogne se fait en catalan, avec trois heures par semaine d'espagnol et trois d'anglais.

L'enseignement maternel est encouragé, même s'il n'est pas obligatoire. Appelé "Educació Infantil" ou populairement connu sous le nom "pàrvuls", il est composé par trois ans (P-3, P-4, P-5). L'enseignement primaire catalan se déroule en six années dans le CEIP ("Centre d'Educació Infantil i Primària" = Centre d'Éducation Infantile et Primaire, ou "Escola" = École). L'Enseignement secondaire obligatoire (ESO) dure quatre années et le "Batxillerat" deux ans. Les deux sont réalisés à "l'Institut d'Educació Secundària" (ou "Institut" ~ lycée). Par ailleurs, trois établissements scolaires français à l'étranger appartenant au réseau de l'Agence pour l'enseignement français à l'étranger (AEFE) existent à Barcelone : l'école française Ferdinand-de-Lesseps (maternelle et élémentaire), le plus ancien établissement français de péninsule Ibérique (fondée par Ferdinand de Lesseps en 1859) ; le lycée français de Barcelone (de la maternelle au lycée) est le plus important de Catalogne et a été fondé en 1924 ; le lycée français de Gavà (LFG) Bon Soleil (ancien collège Bon Soleil, de la maternelle au lycée), créé en 1969. L'Institut français de Barcelone, créé en 1919, assure également des missions d'enseignement du Français langue étrangère ainsi que des actions d'échanges culturels entre la France et la Catalogne.

L'éducation universitaire est divisée en quatre ans de "Grau", une année de "Màster" et le doctorat. Il y a 15 établissements supérieurs en Catalogne (10 à Barcelone), dont 12 universités (sept publiques, cinq privées dont trois catholiques), une université à distance par internet (l'université ouverte de Catalogne ou "Universitat Oberta de Catalunya" UOC) et deux grandes écoles privées (un des campus de l'EU Business School et la "Barcelona Technology School"). La plus ancienne université catalane est l'université de Lérida, fondée en 1297 mais qui a été fermée entre 1717 et 1991. Si toutes les universités de Catalogne ont été supprimées en 1717 des suites des décrets de Nueva Planta, la première à s'être reconstituée est l'université de Barcelone, créée pour la première fois en 1450, déplacée de force à Cervera en 1717 puis rétablie de nouveau dans la capitale catalane en 1837. Le statut d'autonomie reconnaît la liberté de choix en matière linguistique aux enseignants et étudiants du supérieur, ce qui explique que l'importance des enseignements dispensés en catalan varie d'une université à une autre (de 32 % à l'université privée catholique "Abat Oliba CEU" située à Barcelone jusqu'à 84 % à l'université de Vic, elle aussi privée mais laïque, en passant par 44 % à l'université internationale de Catalogne liée à l'Opus Dei à Barcelone, 53,05 % à l'université de Lérida, 59,4 % à l'université polytechnique de Catalogne, 61,8 % à l'université autonome de Barcelone, 64,9 % à l'université Rovira i Virgili de Tarragone, 66,3 % à l'université Pompeu Fabra de Barcelone, 66,4 % à l'université de Barcelone, 71,4 % à l'université ouverte de Catalogne, 75,8 % à l'université Raymond-Lulle de Barcelone et 80 % à l'université de Gérone) mais aussi en fonction du degré d'étude (plus diffusé durant les années de "Grau", son usage est davantage concurrencé par l'espagnol voire l'anglais en "Màster" et en Doctorat, en raison essentiellement d'un pourcentage d'étudiants étrangers plus importants).

La santé en Catalogne est responsabilité de la Généralité. Il est organisé par le Service catalan de Santé ("Servei Català de Salut").

Le sport occupe une place toute particulière dans le cœur des Catalans, notamment le football. On peut citer le FC Barcelone. Une véritable institution depuis 1899 qui occupe le stade du Camp Nou dans le quartier de Les Corts au nord de la ville. Le club de football est une des sections du club omnisports FC Barcelone. Celui-ci se distingue aussi en basket-ball, handball et hockey sur patins ; l'élection du président du "Barça" est pour les Catalans aussi importante que les élections municipales à l"'Ajuntament". Elle est traitée avec une campagne médiatique locale invoquant les voix des "socis", les adhérents du club, nombreux parmi les habitants de Barcelone. Il existe également un deuxième club de football de haut niveau dans la ville. Il s'agit du RCD Espanyol Barcelone qui a déménagé en août 2009 à Cornellà.

La Catalogne a accueilli de nombreuses manifestations sportives internationales, comme les Jeux Olympiques d'été de 1992 à Barcelone, ainsi que les Jeux méditerranéens de 1955 ou les Championnats du monde de natation 2013. Il a tenu annuellement la quatrième plus ancienne course cycliste sur route à étapes encore existante dans le monde, la "Volta a Catalunya" (Tour de Catalogne).

En 2004, la sélection de rink hockey de la communauté participe au championnat du monde B à Macao qu'elle remporte. C'est la première fois, tous sports confondus, qu'une sélection régionale remporte une compétition officielle internationale de ce niveau. L'équipe d'Espagne menaçant de boycotter le championnat du monde A 2005, le n'autorise pas la Catalogne à participer à cette compétition, ne la considérant pas comme membre à part entière de la compétition.

Televisió de Catalunya est l'organisme chargé de la diffusion des chaînes de télévision publiques en Catalogne. Elle est sous la régulation de l'organisation politique de la Généralité, via l'institution du Conseil de l'audiovisuel, élu par le Parlement de Catalogne. Elle dispose actuellement de quatre chaînes, toutes en catalan : TV3 et sa déclinaison satellitaire TV3 Cat, Super3 / 33, 3/24 et Esport3. Il existe aussi une importante chaîne régionale privée, 8tv.

Le groupe national public Radiotelevisión Española dispose d'un centre de production en catalan à Sant Cugat del Vallès (TVE Catalunya) et cinq chaînes : La 1, La 2, 24 horas, Clan et Teledeporte. Les chaînes privées nationales sont également disponibles sur la TNT, comme les principaux généralistes Antena 3, Cuatro, Telecinco et laSexta.

La Corporació Catalana de Mitjans Audiovisuals, dépendant de la Généralité de Catalogne, dispose actuellement de quatre stations de radio, toutes en catalan : Catalunya Ràdio, Catalunya Informació, Catalunya Música et iCat. Il existe aussi autres stations régionales privées de radio comme RAC 1, RAC 105, Ràdio Flaixbac ou Flaix FM.

Appartenant au groupe nationale Radio Nacional de España, est la station de radio publique en catalan, Ràdio 4. Les autres stations du groupe comme Radio Nacional ou Radio 5 et les principales stations privées nationales (Cadena SER, Cadena COPE, Onda Cero ou la musicale Los 40 Principales) sont également disponibles, avec une partie de sa programmation en catalan.


La Catalogne a connu une période d'industrialisation importante au cours du et du , précédé d'une longue tradition commerciale et de fabrication. Aujourd'hui, l'économie catalane se distingue dans le contexte espagnol par un profil industriel très marqué. Elle représente environ un cinquième de l'économie espagnole. La répartition des secteurs est la suivante :

En 2007, le PIB de la Catalogne atteint millions d'euros et le PIB par habitant . Cette même année, la croissance du PIB s'élève à 3,7 %. Dans le contexte de la crise financière de 2008, la Catalogne a subi une récession de près de 2 % de son PIB en 2009.

La Catalogne est la première destination touristique de l'Espagne. Les principales destinations touristiques de la Catalogne sont la ville de Barcelone, les plages de la Costa Brava à Gérone et la Costa Daurada de Tarragone. Dans les Pyrénées, il existe plusieurs stations de ski. Les touristes viennent essentiellement d'Espagne et du Portugal, et dans une moindre mesure du Benelux et de la France.

Les caisses d'épargne ont une grande implantation en Catalogne. Dix des 46 caisses d'épargne espagnoles sont catalanes. La Caixa est la première caisse d'épargne d'Europe. La première banque privée d'origine catalane est Banc Sabadell, qui occupe le quatrième rang des banques privées en Espagne.

La qui représente, en 2004, près de 205 milliards d'euros d'échange, est la deuxième bourse d'Espagne après la Bourse de Madrid. "Fira de Barcelona" organise des congrès à caractère international sur les différents secteurs de l'économie.

La principale dépense économique pour les familles catalanes est l'achat d'une maison. Selon les données de la Société d'estimation du 31 décembre 2005, la Catalogne est, après Madrid, la deuxième communauté d'Espagne où le prix du logement est le plus cher : euros pour un mètre carré sont payés en moyenne. Par villes, cependant, Barcelone est la ville la plus chère d'Espagne, avec un prix moyen de euros au mètre carré. (Voir Bulle immobilière espagnole)

L'endettement du pays, avec 41 milliards d'euros de dette en 2012, est particulièrement élevé, la plaçant dans une situation de quasi banqueroute. En mai 2012, le président de la Catalogne, Artur Mas, envisage un possible défaut de paiement.

La Catalogne est l'une des Communautés autonomes les mieux dotées d'Espagne au titre des sites inscrits au patrimoine mondial de l'Organisation des Nations unies pour l'éducation, la science et la culture (UNESCO). Ainsi, avec six biens inscrits en 2017, tous culturels, elle vient après la Castille-et-León (huit dont sept culturels et un naturel) et l'Andalousie (sept dont six culturels et un naturel).

Ces biens sont, par ordre d'ancienneté d'inscription :

La Catalogne est également la Communauté autonome à compter le plus de sites inscrits sur la liste indicative de l'Espagne, avec neuf biens depuis 2016 dont sept culturels, un naturel et un mixte :


Les plus hauts bâtiments de Catalogne sont :


Ce patrimoine est essentiellement le fruit de l'histoire artistique et architecturale de la Catalogne.

La Catalogne a donné de nombreuses figures importantes au monde dans le domaine de l'art. Les peintres catalans de renommée internationale sont Salvador Dalí, Joan Miró (qui a aussi des origines familiales à Majorque) et Antoni Tàpies, tous appartenant au . Également lié à l'environnement pictural de la Catalogne, Pablo Picasso a vécu son enfance à Barcelone et s'est formé dans les milieux artistiques et intellectuels catalans du début du .

D'autres artistes importants sont Claudi Lorenzale pour le mouvement nazaréen et le romantisme médiévaliste qui a marqué sur le plan artistique la "Renaixença", Marià Fortuny pour le romantisme et l'orientalisme catalan du , Ramon Casas ou Santiago Rusiñol, principaux représentants du courant pictural du modernisme catalan de la fin du et du début du , José Maria Sert pour le Noucentisme du début du , ou encore Josep Maria Subirachs pour la sculpture et la peinture expressionnistes ou abstraites de la fin du .
Les plus importants musées de peinture de la Catalogne sont, en termes de fréquentation en 2014, le théâtre-musée Dalí de Figueras (troisième d'Espagne et le du monde), le musée Picasso de Barcelone (sixième d'Espagne et du monde), le CaixaForum également à Barcelone (septième d'Espagne et mondial), le musée national d'art de Catalogne (MNAC) toujours à Barcelone (huitième d'Espagne et le au monde). D'autres institutions barcelonaises importantes sont la Fondation Antoni-Tàpies, la Fondation Joan-Miró, le musée d'art contemporain de Barcelone (MACBA) et le Centre de culture contemporaine de Barcelone (CCCB).

Dans le domaine de l'architecture, différents styles artistiques qui prévalent en Europe ont été développés ou adaptés en Catalogne, laissant leurs empreintes dans de nombreuses églises, monastères et cathédrales, d'art roman (les meilleurs exemples sont situés dans la moitié nord du territoire) et gothique. Il y a quelques exemples d'architecture de la Renaissance, baroque et néoclassique. Le modernisme (Art nouveau) à la fin du apparaît comme l'art national catalan. Les architectes catalans de renommée mondiale de ce style sont Antoni Gaudí, Lluís Domènech i Montaner et Josep Puig i Cadafalch. En ce qui concerne le rationalisme architectural qui a dominé pendant une grande partie du , peuvent être soulignés les œuvres de Josep Lluís Sert ou de . Enfin, Barcelone est également devenue depuis la fin du l'un des principaux centres de réflexion et d'innovation pour le courant postmoderniste, une effervescence incarnée, entre autres, par le Ricardo Bofill Taller de Arquitectura fondé en 1963 par Ricardo Bofill.

Il y a deux moments historiques de splendeur de la littérature catalane. La première commence par les chroniques historiographiques des et l'âge d'or du , faisant naître le catalan littéraire notamment au travers des œuvres des troubadours ("trobador" en catalan comme en occitan), qu'ils soient des princes comme Alphonse le Chaste ou de véritables professionnels de la poésie et grammairiens comme Cerverí de Girona, Raimon Vidal de Besalú ou Jofre de Foixà. Toutefois, le premier écrivain d'importance en langue catalane reste le philosophe, théologien, poète et romancier Raymond Lulle ("Ramon Llull" en catalan), originaire de Majorque, à la fin du et au début du , tout particulièrement avec ses romans "Blaquerne" et "Felix de les maravelles del mon".

Après cette période, entre les , l'historiographie romantique définit cette époque comme la "", considérée comme la période « décadente » de la littérature catalane en raison d'une baisse de l'usage de la langue vernaculaire dans les productions culturelles (liée à l'influence de la cour royale, désormais castillane, qui va largement contribuer au dynamisme du Siècle d'or espagnol), ainsi que du fait d'un manque de mécénat de la part de l'aristocratie catalane. En revanche, le catalan reste largement la langue écrite et orale des classes populaires. De plus, à partir de la fin du et au début du , de nombreux auteurs se sont montré critiques à l'égard de ce concept de « Décadence », lui reprochant de ne pas prendre en compte les œuvres scientifiques, savantes ou morales, à l'image de celles de Joan Pere Fontanella dans le domaine juridique ou de en arithmétique (auteur du deuxième livre de mathématiques connu à avoir été imprimé au monde, en 1482), ni la littérature écrite par des auteurs catalans dans d'autres langues (castillan, italien, latin, etc.), comme Juan Boscán ou Gaspard Gil Polo (d'origine valencienne).

Quoi qu'il en soit, le catalan littéraire revient en force au avec la "Renaixença" (« renaissance ») culturelle et politique, inspirée par le mouvement romantique européen et représentée par des écrivains et des poètes tels que Jacint Verdaguer pour la poésie, Narcís Oller pour ses romans et Àngel Guimerà pour le théâtre. Ce renouvellement littéraire, intellectuel mais aussi linguistique se maintient dans la durée, grâce aux Jeux floraux de Barcelone créés en 1859, au travail du grammairien Pompeu Fabra, qui a produit les normes du catalan moderne, ainsi qu'à l'Institut d'études catalanes (IEC, "Institut d'Estudis Catalans") fondé en 1907. Au tournant du , le modernisme catalan s'exprime également sur le plan littéraire, avec par exemple Joan Maragall ou Santiago Rusiñol, contribuant à la transformation « d’une culture régionale traditionnaliste en une culture nationale moderne » selon le valencien Joan Fuster et portant la construction intellectuelle et politique du catalanisme.
Au cours du , ont été développés les mouvements d'avant-garde initiés par la Génération de 14 appelée Noucentisme en Catalogne, représentés par Eugeni d'Ors, Joan Salvat-Papasseit, Josep Carner, Carles Riba, J.V. Foix et d'autres. Pendant la dictature de Primo de Rivera, la guerre civile (« Génération de 36 ») et l'époque franquiste, la littérature catalane se maintient malgré la répression qui s'abat sur le catalan et le catalanisme, en étant souvent produite en exil. Les auteurs les plus éminents de cette période restent Salvador Espriu, Josep Pla, Josep Maria de Sagarra (ces trois derniers étant considérés comme les principaux artisans du renouvellement de la prose catalane), Mercè Rodoreda, Joan Oliver Sallarès dit « Pere Quart », Pere Calders, Gabriel Ferrater, Manuel de Pedrolo, ou Miquel Martí i Pol. Par ailleurs, plusieurs écrivains étrangers ayant combattu dans le cadre des Brigades internationales racontent ensuite leurs expériences des combats dans leurs œuvres, historiques ou fictionnelles, avec par exemple "Hommage à la Catalogne" ("Homage to Catalonia") du britannique George Orwell en 1938 ou "Le Palace" en 1962 et "Les Géorgiques" en 1981 du français Claude Simon.

Après la Transition démocratique (1975-1978) et la restauration de la Généralité (1979), la vie littéraire et le marché éditorial sont revenus à la normale et la production littéraire en catalan est soutenue par un certain nombre de politiques linguistiques visant à protéger la culture catalane. Outre les auteurs mentionnés ci-dessus, d'autres auteurs pertinents depuis le retour de la démocratie comprennent Joan Brossa, Jesús Moncada, Núria Perpinyà ou Quim Monzó.

La sardane est considérée comme la danse catalane populaire la plus caractéristique, interprétée au rythme du tamborí (tambourin), du tible et de la tenora (de la famille des hautbois), de la "trompeta" (trompette), du "trombó" (trombone), du fiscorn (de la famille des bugles) et de la "contrabaix" (contrebasse) à trois cordes joués par une cobla. D'autres airs et danses de la musique traditionnelle sont le "contrapàs" (plus désuet aujourd'hui), "" (le « bal de bâtons »), la moixiganga, les "goigs" (cantiques populaires) ou encore la "jota" dans la partie sud. Les "havaneres" sont caractéristiques dans certaines localités maritimes de la Costa Brava, comme à Palafrugell dont elles sont originaires et en particulier pendant les mois d'été lorsque ces chansons sont chantées en plein air accompagnées d'un "cremat" (rhum brûlé).

La musique savante pour sa part s'est d'abord développé, jusqu'au et comme dans une grande partie de l'Europe, dans un cadre liturgique, marqué tout particulièrement par l'Escolania de Montserrat, ou curial. Comme pour les arts, l'architecture ou la littérature, les principaux courants musicaux occidentaux ont marqué ces productions, des monodies ou polyphonies médiévales, avec le travail de l'abbé Oliva de Ripoll au ou le "Livre vermeil de Montserrat" ("Llibre Vermell de Montserrat") du par exemple, jusqu'aux compositeurs romantiques comme Fernando Sor ("Ferran Sor i Muntades"), Josep Anselm Clavé i Camps (père du mouvement des orphéons en Catalogne puis dans toute l'Espagne) ou encore Felip Pedrell (un des précurseurs du catalanisme musical). Ceci en passant par les auteurs de la Renaissance tels que Pere Albert Vila, Joan Brudieu ou les deux Mateu Fletxa ("El Vell" ou le vieux, et "El Jove" ou le jeune) puis le baroque notamment représenté en Catalogne par Joan Cererols.
Le modernisme catalan s'exprime ensuite aussi sur le plan musical, à partir de la fin du , en mélangeant influences folkloriques et internationales post-romantiques, au travers des œuvres d'Isaac Albéniz ou Enric Granados. L'esprit d'avant-garde ainsi initié par les modernistes se prolonge tout au long du , grâce aux activités tout particulièrement de l'Orfeó Català, association chorale fondée en 1891, de sa salle de concert monumentale, le palais de la musique catalane ("Palau de la Música Catalana" en catalan, édifié selon les plans de Lluís Domènech i Montaner de 1905 à 1908), de l'Orchestre symphonique de Barcelone créé en 1944 et d'auteurs, chefs d'orchestre et musiciens engagés contre le franquisme comme Robert Gerhard, Eduard Toldrà et surtout Pau Casals.
Les représentations de l'opéra, principalement importées d'Italie, ont commencé au , avec quelques opéras catalans rivalisant avec les productions italiennes comme ceux de Domènec Terradellas, Carles Baguer et surtout Ramon Carles, Isaac Albéniz et Enric Granados. Le principal théâtre d'opéra de Barcelone, le grand théâtre du Liceu ("Gran Teatre del Liceu" en catalan, ouvert en 1847), demeure l'un des plus importants en Espagne, accueillant de plus l'une des plus prestigieuses écoles de musique de Catalogne, le Conservatoire supérieur de musique du Liceu ("Conservatori Superior de Música del Liceu" en catalan). Plusieurs artistes lyriques formés par cette institution ont acquis une renommée internationale au cours du , à commencer par Victoria de los Ángeles, Montserrat Caballé, Giacomo Aragall ou José Carreras.

D'autres styles musicaux populaires sont nés dans la deuxième moitié du comme la "Nova Cançó" (la « Nouvelle chanson » en français) à partir des années 1950 avec Lluís Llach et le groupe Els Setze Jutges, la rumba catalane dans les années 1960 avec Peret, le à partir de la fin des années 1970 avec La Banda Trapera del Río et Decibelios pour le punk rock, SAU, Els Pets, Sopa de Cabra ou Lax'n'Busto pour le pop rock ou encore Sangtraït pour le hard rock, l'électropop depuis les années 1990 avec OBK et le indie pop à partir des années 1990 représenté, entre autres, par Astrud. L'artiste colombienne de renommée internationale de pop latino, Shakira, est installée depuis les années 2010 à Barcelone, où elle enregistre une partie de ses albums, tourne certains de ses clips (comme celui de "Loca"), tout en chantant parfois en catalan (reprenant notamment une des balades de SAU, "Boig per tu", en catalan et en espagnol dans son album "Shakira" en 2014).

Les premières exhibitions cinématographiques en Catalogne ont lieu à Barcelone, d'abord par la présentation du kinétoscope de l'américain Thomas Edison à la place de Catalogne le puis du cinématographe des frères français Auguste et Louis Lumière au studio des "Fotògrafs Napoleon" sur La Rambla à Noël 1896. Les premières œuvres espagnoles sont produites à Barcelone à partir de 1897, avec les travaux de (natif du quartier de Sants à Barcelone, dont le film de fiction "Riña en un café" est considéré historiquement comme le premier d'Espagne) et de Segundo de Chomón (aragonais d'origine mais actif à Barcelone). La capitale catalane devient alors le centre de la première industrie cinématographique ibérique, en reprenant et diffusant d'abord les productions étrangères (surtout françaises, italiennes et danoises) puis en développant ses propres films en reproduisant les grandes tendances mondiales du cinéma muet de l'époque (vidéos comiques, historiques, mélodrames, reprenant des classiques de la littérature ou du théâtre) tout en exagérant leur caractère espagnol, d'où l'appellation d’"españoladas" qui leur est souvent accolée à partir de cette époque. Cet « âge d'or » du cinéma barcelonais culmine au début des années 1920.
Après la parenthèse causée par la dictature de Primo de Rivera, la création des premières institutions publiques de promotion du cinéma accompagne le statut d'autonomie de 1932. Ceci ainsi que le passage au parlant contribuent à relancer le cinéma et à promouvoir l'usage du catalan dans les films, un élan vite stoppé par les effets de la Grande Dépression, de la guerre civile espagnole et de la victoire franquiste en 1939.

C'est durant la dictature que va naître l'École de Barcelone ("Escuela de Barcelona" en espagnol, "Escola de Barcelona" en catalan), mouvement créé dans les années 1960 par un groupe de cinéastes issus pour l'essentiel de la bourgeoisie et militants de la gauche antifranquiste, en lien avec la Gauche divine et le PSUC, inspirés par la Nouvelle Vague française et en concurrence avec le Nouveau cinéma espagnol (NCE) madrilène. Créant un cinéma d'art et d'essai, généralement en espagnol, ils privilégient l'expérimentation formelle au fond (répétition des flash-back, « désolidarisation » des séquences entre elles, utilisation d’une symbolique cryptique), en réaction à la narrative classique de la NCE, tout en livrant en filigrane une critique de la société espagnole. Parmi les réalisateurs les plus représentatifs de cette période figurent Vicente Aranda, Jorge Grau, Pere Portabella ou Joaquim Jordà i Català.
Après la transition démocratique et l'adoption du nouveau statut d'autonomie en 1979, la Généralité de Catalogne restaurée devient un acteur majeur de la production, faisant désormais de la défense d'un cinéma spécifiquement catalan et de la défense de la culture et de la langue catalanes des enjeux majeurs de cette industrie en Catalogne. Trois styles dominent depuis lors. Tout d'abord, le cinéma d'auteur, dans la continuité de l'École de Barcelone, met en avant l'expérimentation et la forme, tout en s'attachant à développer des thèmes sociaux et politiques (en promouvant le catalanisme, la redécouverte d'une histoire nationale, la libération des mœurs en lien avec la "Movida", les inégalités sociales, les quartiers défavorisés et populaires de Barcelone). Porté d'abord par Josep Maria Forn ou Bigas Luna, puis par Marc Recha, Jaime Rosales et Albert Serra, ce cinéma a obtenu une certaine reconnaissance internationale. Ensuite, le documentaire est devenu un autre genre particulièrement représentatif du cinéma catalan contemporain, initié véritablement par Joaquim Jordà i Català et José Luis Guerín, puis entretenu par le master de documentaire de l'université Pompeu Fabra. Enfin, et surtout, le cinéma d'horreur et le thriller se sont aussi imposés comme une spécialité de l'industrie cinématographique catalane, s'exportant particulièrement bien, grâce notamment à la vitalité du Festival international du film fantastique de Catalogne de Sitges, mieux connu comme le Festival de Sitges, créé en 1968. Plusieurs réalisateurs ont acquis une renommée mondiale grâce à ce genre, à commencer par Jaume Balagueró et sa série "[•REC]" (coréalisée avec le valencien Paco Plaza), Juan Antonio Bayona et "L'Orphelinat" ("El Orfanato" en version originale espagnole) ou encore Jaume Collet-Serra et "Esther", "Sans identité" ("Unknown" en version originale anglaise) puis "Non-Stop".
Par ailleurs, plusieurs acteurs catalans ont tourné pour des productions espagnoles, européennes voire internationales depuis la transition démocratique, comme Sergi López, qui a tourné, entre autres, pour le français Manuel Poirier, le britannique Stephen Frears, le catalan Marc Recha ou le mexicain Guillermo del Toro.

En retour, la Catalogne a attiré plusieurs réalisateurs célèbres mondialement pour y tourner ou y inscrire l'intrigues de certaines de leurs productions, à commencer par Barcelone, comme l'espagnol Pedro Almodóvar avec "Tout sur ma mère" en 1999, le français Cédric Klapisch pour "L'Auberge espagnole" en 2002, l'allemand Tom Tykwer pour "Le Parfum" en 2006, l'américain Woody Allen pour "Vicky Cristina Barcelona" en 2007 et le mexicain Alejandro González Iñárritu pour "Biutiful" en 2010.

Le musée du Cinéma - Collection Tomàs Mallol ("Museu del Cinema - Col·lecció Tomàs Mallol" en catalan) de Gérone abrite l'une des plus importantes expositions permanentes d'objets de cinéma et de pré-cinéma au monde. D'autres institutions importantes pour la promotion du septième art sont les prix Gaudí ("Premis Gaudí" en catalan, qui ont remplacé à partir de 2009 les prix Barcelone du cinéma eux-mêmes créés en 2002), une récompense servant d'équivalent pour la Catalogne des Goyas espagnols ou des Césars français. Les prix Sant Jordi du cinéma ("Premi Sant Jordi de Cinematografia" en catalan), plus anciens car remontant à 1957 et décernés par la délégation de la Radio Nacional de España à Barcelone, s'adressent plus largement au cinéma espagnol.

Les "festes majors" sont les fêtes patronales des villes et villages de Catalogne. Dans les plus grandes célébrations les éléments de la culture populaire catalane sont habituellement présents : les processions ou défilés de "gegants" (géants) et "correfocs" de démons et des pétards, accompagnés de danses et chants traditionnels et de "castells". Parmi les fêtes traditionnelles et populaires les plus représentatives figurent le Patum de Berga ayant lieu dans la ville de Berga (située au nord de Barcelone), la Sant Jordi, la nuit de la Saint-Jean ("Nit de Sant Joan" en catalan) ou la fête de la châtaigne ("Castanyada" en catalan) dans toute la Communauté autonome, La Mercé de Barcelone, le rassemblement de l'escargot ("Aplec del Caragol" en catalan) de Lleida et les fêtes du feu du solstice d'été dans les Pyrénées ("Falles del Pirineu" en catalan), entre autres.

Les "castells" sont l'une des principales manifestations de la culture populaire catalane. L'activité consiste en la construction de tours humaines mettant en concurrence des "colles castelleres" (équipes). Cette pratique a pour origine la partie sud de la Catalogne au cours du . Un autre marqueur des manifestations populaires catalanes restent la sardane, une danse née pour sa part au nord du territoire, dans les comarques gironines.

Plusieurs traditions locales entourent enfin les festivités de Noël. L'une d'entre elles est la figure populaire du Tió de Nadal (« Bûche de Noël »), un personnage ayant la forme d'un tronc de bois creux qui est censé « déféquer » de petits présents aux enfants qui le frappent en chantant une comptine. Une autre coutume est de faire un "Pessebre" (crèche de Noël), qui comprend habituellement le Caganer, une figurine représentée dans l'acte de défécation. Comme dans le reste de l'Espagne, l'apport traditionnel de cadeaux ne se fait pas à Noël mais durant la nuit des rois ("Nit de Reis") par les Rois mages ("Reis d'Orient" ou simplement "els Reis" en catalan, "Reyes Magos" en espagnol).

Trois de ces éléments ont été inscrits dans la liste représentative espagnole du patrimoine culturel immatériel de l’humanité : le Patum de Berga en 2008, les "castells" en 2010 et les "Falles Pirineus" (partagées avec l'Aragon, l'Andorre et les départements français de Haute-Garonne et des Hautes-Pyrénées) en 2015. La Catalogne est également concernée par l'inscription à cette liste, aux côtés d'autres régions du monde, du régime méditerranéen en 2010 et de la fauconnerie en 2012.

Les couleurs du blason, le « Sang et or », d'origines médiévales, revêtent une importance identitaire forte pour les Catalans. Il existe trois drapeaux de la Catalogne utilisant ces couleurs, chacun ayant plus ou moins un sens différent :


D'après la légende, inventée très probablement en 1551 par l'historien valencien Pere Antoni Beuter et largement diffusée dans la culture populaire catalane, l'empereur Charles le Chauve demande au comte Guifred le Velu de lui prêter main-forte contre les Normands. Au cours de la bataille, Guifred est atteint par une flèche. Le soir, l'empereur franc se rend dans la tente du comte catalan, allongé sur sa couche près de laquelle se trouve son bouclier, un champ d'or vierge de tout décor. Il trempe quatre doigts dans la blessure ouverte de Guifred et trace, d'un geste, les quatre barres rouge donnant ainsi à la Catalogne, ses armes "d’or à quatre pals de gueules".

Sur le plan historique, les plus anciens témoignages de ce blason sont des sceaux du comte Raimond-Bérenger IV de Barcelone, époux de la reine Pétronille d'Aragon, puis de leur fils et successeur Alphonse II le Chaste, datant du milieu ou de la fin du . Il existe un débat entre historiens et héraldistes sur les origines de cet emblème, la thèse la plus défendue étant celle le liant à la lignée des comtes de Barcelone, ce que font la plupart des documents datant de la fin du Moyen Âge. L'union des territoires constituant la couronne d'Aragon s'étant faite autour de la maison de Barcelone, ses armoiries seraient alors devenues celle de la maison royale d'Aragon et de la plupart de leurs possessions. D'autres spécialistes ont au contraire émis l'idée que ces couleurs viendraient des souverains d'Aragon avant le mariage de Pétronille avec Raimond-Bérenger IV de Barcelone, y voyant leurs origines dans les couleurs pontificales, les rois aragonais ayant entretenu des liens précoces avec le Saint-Siège. Enfin, une dernière théorie suggère que l'origine des armes proviendrait du mariage en 1112 du comte Raimond-Bérenger III de Barcelone avec la comtesse Douce de Provence. Douce, descendante de Guillaume le Libérateur et du comte Boson II d'Arles, aurait ainsi apporté à son époux et à sa descendance les armes de sa Provence. C’est en effet dans la vallée du Rhône que les écus avec décor comportant des "pals" sont originaires.

Quelles que soient leurs origines, les armes aux quatre "pals" gueules et or ont donné à partir du les couleurs de la principauté de Catalogne et de la couronne d'Aragon. Elles sont communément appelées les « Quatre Barres » ou « Sang et or ». Ces couleurs forment aujourd'hui le drapeau catalan, dans lequel les "pals" sont devenues des bandes horizontales ("fasces" en termes héraldiques). Afin de ne pas les confondre avec celles, verticales, de Provence, les quatre "pals" ont été disposées plus tard horizontalement. En revanche, sur le sceau officiel de la Généralité de Catalogne comme sur les armes de la Communauté autonome, les "pals" restent verticales.

Trois autres communautés autonomes d'Espagne utilisent ces mêmes symboles, ou les mêmes bases, que ce soit les « Quatre Barres » ou les couleurs « Sang et or », dans leurs drapeaux et leurs armoiries, en raison de leur proximité historique : l'Aragon (drapeau et armoiries), la Communauté valencienne et les îles Baléares, par exemple. Les quatre "pals" se retrouvent également, en tant que représentantes de l'ancienne couronne d'Aragon, en partie 3 des armoiries de l'Espagne, elles-mêmes présentes sur le drapeau espagnol (tandis que les deux bandes rouges et la bande or de ce drapeau font référence autant aux couleurs castillanes qu'à celles de l'ensemble catalano-aragonais). De même, elles sont présentes dans les armoiries de l'Andorre, en 3 mais aussi en 2 sous la forme de trois barres qui étaient celles de l'ancien comté de Foix. En France, cette référence est reprise, officiellement ou non, par des collectivités ayant eu un lien passé avec la maison de Barcelone : les départements des Pyrénées-Orientales ou de Lozère, ainsi que les régions Occitanie (et avant elle celle du Languedoc-Roussillon) et Provence-Alpes-Côte d'Azur. En Italie, ces couleurs se retrouvent dans plusieurs provinces ou municipalités, notamment pour la ville métropolitaine de Reggio de Calabre ou encore à Naples.

La Catalogne a ses propres symboles nationaux, définis au nombre de trois à savoir, outre la "Senyera" :







La gastronomie catalane, comme toutes les cuisines du bassin méditerranéen, fait un usage abondant de poissons, fruits de mer, huile d'olive et légumes frais. Les spécialités sont nombreuses et incluent "pa amb tomàquet" (pain à la tomate), "Calçotada", "escudella i carn d'olla", "suquet de peix", (soupe de poisson) et bien sûr la crème catalane (similaire a la crème brûlée).

Région vinicole, le vignoble catalan possède plusieurs dénominations d'origine telles que le Priorat, Montsant, Penedes et Empordà, et on y trouve également un mousseux, le "cava".

La Catalogne est également reconnue au niveau international pour sa haute cuisine, avec notamment des restaurants comme "El Bulli" ou "El Celler de Can Roca" qui dominent régulièrement les classements internationaux.





</doc>
<doc id="15175" url="https://fr.wikipedia.org/wiki?curid=15175" title="Frise">
Frise





</doc>
<doc id="15177" url="https://fr.wikipedia.org/wiki?curid=15177" title="Arche de Noé">
Arche de Noé

L’arche de Noé est, d'après la Bible, un navire construit sur l'ordre de Dieu afin de sauver Noé, sa famille (ses trois fils ainsi que leurs épouses) et ainsi qu'un couple (ou sept couples) de toutes les espèces animales pour les sauver du Déluge sur le point d'advenir. 

L'histoire figure dans le livre de la Genèse, du chapitre 6 au chapitre 9, correspondant à la Parasha Noah. Noé est souvent mentionné dans le Coran, particulièrement dans la sourate 11, intitulée « Houd », des versets 27 à 51.

Selon l'hypothèse documentaire, cette partie de la Genèse se fonde sur deux sources anciennes quasiment indépendantes l'une de l'autre, et n'a atteint sa forme définitive que vers le . Ce processus de consolidation graduelle permettrait d'expliquer les confusions et les répétitions du texte. Certains fondamentalistes bibliques, qui rejettent cette analyse, tiennent l'histoire de l'arche de Noé pour véritable, affirmant qu'elle n'a qu'un seul et unique auteur et que toute incohérence apparente peut s'expliquer rationnellement. Les plus convaincus ont tiré de cette posture une série de déductions très variées sur la taille du bateau, son matériau de construction ou encore la date précise du Déluge.

Les récits bibliques de l'arche de Noé présentent des similitudes avec un mythe mésopotamien décrit dans le Poème du Supersage datant du , dans la légende de Ziusudra qui pourrait elle aussi dater de la fin du , puis repris au au plus tard dans la version assyro-babylonienne « standard » de l'Épopée de Gilgamesh, mythe qui raconte comment un Sage appelé Atrahasis, Ziusudra ou Uta-Napishtim selon les différentes versions du mythe, fut invité par le dieu Enki/Ea à construire un navire, dans lequel il pourrait échapper au déluge envoyé par l'assemblée des grands dieux. D'autres versions, d'une ressemblance plus approximative, peuvent se retrouver dans de nombreuses cultures à travers le monde. L'histoire de l'arche a fait l'objet par les religions abrahamiques d'interprétations abondantes, mêlant raisonnements théoriques, problèmes pratiques et considérations allégoriques : les commentateurs, ainsi, pouvaient aussi bien se poser la question de la gestion du fumier que celle de l'arche comme première incarnation d'une Église offrant le salut à l'humanité.

Dès le début du , le développement de la biogéographie en tant que science naturelle réduisit progressivement le nombre de personnes prêtes à soutenir une interprétation littérale de l'aventure de Noé. Les fondamentalistes bibliques, cependant, continuent à parcourir la région du mont Ararat au nord-est de la Turquie (auparavant d'Arménie occidentale), là où la Bible dit que l'arche de Noé se serait échouée à la fin de son périple. D'après le Coran, elle se serait plutôt établie sur le mont Djoudi.

L'histoire de l'arche de Noé, d'après les chapitres 6 à 9 du livre de la Genèse, commence lorsque Dieu observe la méchanceté et la perversité des hommes, et décide de faire tomber un déluge sur la Terre pour y détruire toute vie, « depuis l'homme, jusqu'aux bestiaux, aux bestioles et aux oiseaux du ciel ». Un homme, Noé, trouve toutefois grâce aux yeux de Dieu, car il apparaît « juste, intègre parmi ses contemporains et il marchait avec Dieu ». Il est choisi, dans ces conditions, pour survivre et perpétuer sa lignée. Dieu, pour cette raison, dit à Noé de construire une arche et rentre dans des spécifications très précises :

Ces mesures correspondent à une grande barge sans mât, d'environ de long, de large et de haut. Dans le passage suivant, Dieu dit à Noé d'engranger des vivres dans l'embarcation, puis d'emmener avec lui sa femme, ses fils Sem, Cham et Japhet ainsi que les épouses de ces derniers, sans oublier des spécimens de toutes les espèces animales existantes :

Une fois l'arche terminée, Noé monta à bord avec toute sa famille et les animaux, et « ce jour-là jaillirent toutes les sources du grand abîme et les écluses du ciel s'ouvrirent ». La pluie tomba ensuite sans discontinuer sur la terre pendant quarante jours et quarante nuits. Les eaux finirent par couvrir même les plus hautes montagnes, qu'elles dépassèrent de plus de quinze coudées. Toutes les créatures vivantes s'éteignirent, et seuls Noé et les siens purent survivre.

Finalement, au bout d'environ 220 jours de navigation, l'arche vint s'échouer sur les monts d'Ararat, et les eaux refluèrent encore quarante autres journées avant qu'apparaissent les sommets des montagnes. Noé décida alors d'envoyer en éclaireur un corbeau, « qui alla et vint en attendant que les eaux aient séché sur la terre ». Noé fit ensuite sortir la colombe, laquelle ne trouva aucun endroit dégagé des eaux où poser ses pattes et revint auprès de lui. La tentative fut renouvelée après sept autres jours, et cette fois la colombe revint avec « dans le bec un rameau tout frais d'olivier », ce qui apprit à Noé que le niveau des eaux avait enfin diminué. Il lâcha la colombe une nouvelle fois après une semaine, et l'oiseau ne revint cette fois plus du tout. Ce signal annonçait la fin de l'épreuve :

Noé fit ensuite de nombreux sacrifices à Dieu sur un autel qu'il édifia pour l'occasion. Satisfait de ce comportement, Dieu se résolut pour sa part à ne plus jamais maudire la terre à cause de l'homme, et à ne plus jamais détruire toute vie de cette manière. En signe de cette promesse, Dieu mit un arc-en-ciel dans les nuages et déclara que « lorsque j'assemblerai les nuées sur la terre et que l'arc apparaîtra dans la nuée, je me souviendrai de l'alliance qu'il y a entre moi et vous et tous les êtres vivants ».

Les 87 versets de l'histoire de l'arche laissent parfois une impression de confusion : pourquoi le récit précise-t-il à deux reprises que l'humanité s'était corrompue mais que Noé devait être sauvé ? Noé reçut-il l'ordre d'emmener un couple de chaque animal pur dans l'arche ou bien sept ? La crue dura-t-elle quarante ou cent cinquante jours ? Qu'arriva-t-il précisément au corbeau qui quitta l'arche en même temps que la colombe et « alla et vint en attendant que les eaux aient séché sur la terre », près de deux à trois semaines plus tard ? Le récit, de plus, semble comporter deux dénouements logiques distincts. Ce type de questions, dans la Bible, n'est pas exclusif de l'histoire de l'arche ou même du livre de la Genèse dans son ensemble, et les tentatives pour y répondre ont mené à l'émergence d'une école de pensée dominante sur l'analyse textuelle des cinq premiers livres de la Bible, celle de l'hypothèse documentaire.

Selon cette hypothèse, ce récit de l'arche trouve son origine dans deux sources, le document sacerdotal (P) et le document non-P (auparavant appelé document jahviste J). La source non-P est la plus ancienne des deux : elle est vraisemblablement rédigée au royaume de Juda, à partir de textes et de traditions encore plus lointaines, et aurait vu le jour peu après la séparation des deux royaumes de Juda et d'Israël, vers l'an 920 av. J.-C. Le récit non-P est de facture plus simple que la version sacerdotale : Dieu envoie les eaux pendant quarante jours (150 dans le document P). Noé, sa famille et les animaux sont sauvés (sept couples de chaque animal pur, ou peut-être simplement sept animaux purs, le texte hébreu étant ambigu sur ce point). Noé construit ensuite un autel et procède à des sacrifices, puis Dieu s'engage à ne plus tuer ainsi tout être vivant. Le document jahviste ne fait cependant aucune mention d'une alliance passée entre Dieu et Noé.

Le texte sacerdotal semble avoir été élaboré à une époque comprise entre la chute du royaume d'Israël au nord, en -722, et celle du royaume de Juda au sud en -586. Les éléments du document sacerdotal sont beaucoup plus détaillés que ceux de la version jahviste, avec par exemple les instructions pour la construction de l'arche et une chronologie précise. Surtout, il donne au récit sa véritable dimension théologique en ajoutant le passage sur l'alliance entre Dieu et Noé au chapitre 9 et en faisant la toute première mention dans la Bible du rituel sacrificiel juif, ces deux éléments constituant la contrepartie logique du serment de Dieu de ne plus détruire la terre. C'est également à la source sacerdotale que l'on doit le corbeau (le texte jahviste contenant pour sa part la colombe), l'arc-en-ciel, ainsi que l'évocation des « "sources du grand abîme et [des] écluses du ciel" », le document jahviste se contentant de dire qu'il a plu. Tout comme la source jahviste, l'auteur du texte sacerdotal a dû avoir accès à des textes et à des traditions plus anciennes, aujourd’hui perdues.

La colère de Dieu face à la corruption des hommes, sa décision de se livrer à une terrible vengeance et ses regrets ultérieurs sont autant de thèmes typiques de l'auteur ou des auteurs jahvistes, qui traitent Dieu comme une entité humaine apparaissant en personne dans le récit biblique. Le document sacerdotal, à l'inverse, tend à présenter un Dieu distant et inaccessible, si ce n'est par l'entremise des prêtres aaronites. Ainsi, par exemple, le texte jahviste annonce le sacrifice de sept de chacun des animaux purs (conformément à la tradition biblique), tandis que le texte sacerdotal réduit ce nombre à un seul couple, étant donné qu'aucun sacrifice ne saurait être effectué selon les règles sacerdotales avant que n'advienne le premier prêtre, Aaron, du temps de l'Exode.

Certains juifs orthodoxes et chrétiens fondamentalistes croient que la Bible, en tant que parole de Dieu, est parfaitement exacte. Le cas échéant, elle doit être interprétée selon la méthode historico-grammaticale, qui consiste à replacer le texte dans son contexte lorsque le sens d'un passage pose problème. Les fondamentalistes tendent aussi à rester fidèle aux hypothèses anciennes entourant la création de la Bible. En vertu de cette herméneutique, ils acceptent donc généralement la tradition juive selon laquelle le récit de l'arche, dans la Genèse, aurait été écrit par Moïse lui-même. Mais il leur est plus difficile de s'accorder sur l'époque précise où ce dernier aurait vécu, et donc sur la date de composition du texte : diverses possibilités ont été avancées, allant du .

En ce qui concerne la date du Déluge, les fondamentalistes se fondent sur une interprétation des généalogies figurant aux chapitres 5 et 11 de la Genèse. L'archevêque James Ussher, en utilisant cette méthode au , est arrivé à l'année -2349, et cette date continue à faire autorité pour beaucoup. Un chercheur fondamentaliste plus contemporain, Gerhard F. Hasel, en résumant les éléments du débat à la lumière de plusieurs manuscrits bibliques (le texte massorétique en hébreu, plusieurs manuscrits de la Septante…) et des différentes interprétations dont ils font l'objet, est cependant arrivé à la conclusion que cette méthode ne pouvait situer le Déluge que dans une fourchette située entre -3402 et -2462. Des thèses concurrentes, fondées sur d'autres sources ou d'autres méthodologies, peuvent même aboutir à des dates situées hors de cette vaste période : le Livre des Jubilés, par exemple, fournit une date équivalant à l'an -2309.

Les fondamentalistes attribuent les contradictions apparentes du récit de l'arche aux conventions stylistiques en vigueur dans les textes anciens. La confusion relative au nombre de couples d'animaux purs que Noé devait emporter (un couple d'espèce impure et sept couples d'espèces pures) viendrait ainsi de ce que l'auteur, Moïse, aurait d'abord introduit le sujet en termes généraux, évoquant les sept couples, avant de répéter à de multiples reprises que ces animaux entrèrent dans l'arche par couples de deux, d'où la méprise. Toutefois, comme l'ont souligné depuis bien longtemps les philologues la question suivante se pose : comment Noé a pu distinguer les espèces d'animaux "purs" des espèces "impures" (Gn 7,2), alors que cette distinction n'a été expliquée à personne avant Moïse (suivant la tradition), soit 1000 ans plus tard ? Les fondamentalistes, de même, ne voient rien de troublant dans le passage relatif au corbeau (pourquoi Noé n'aurait-il pas lâché ce corbeau ?) et contestent l'existence de deux dénouements différents.

Au-delà de ces questions relatives à la date, à l'auteur et à l'intégrité du texte, le littéralisme a attaché beaucoup d'attention à des détails techniques tels que la nature exacte du « bois résineux » ou l'armature de l'embarcation. Les paragraphes suivants abordent les principaux sujets faisant débat.

 
Le quatorzième verset du chapitre 6 de la Genèse énonce que l'arche a été réalisée en « bois résineux », ou « bois גפר » en hébreu (littéralement "gofer" ou "gopher"). La "Jewish Encyclopedia" avance que cette expression est très probablement une traduction du babylonien "gushure iş erini" (« poutres de cèdre ») ou de l'assyrien "giparu" (« roseau »). La Vulgate latine, au , l'a transcrit en "lignis levigatis" (« bois poli »). La Septante grecque, quant à elle, ne mentionne aucune variété de bois en particulier mais évoque la construction d'une grande embarcation carrée, goudronnée à l'intérieur et à l'extérieur de la coque.

De vieilles traductions anglaises, dont la Bible du roi Jacques du , choisissent tout simplement de ne pas traduire l'expression. Plusieurs traductions modernes font le choix du cyprès sur la base d'un faux raisonnement étymologique induit par des rapprochements phonétiques, et ce bien que le mot hébreu employé dans la Bible pour désigner le cyprès soit « erez ». D'autres versions contemporaines proposent le pin ou reprennent l'idée du cèdre. Les suggestions les plus récentes, entre autres, ont émis l'hypothèse que le texte ait perdu son sens par altération au fil des siècles, qu'il fasse référence à un type de bois aujourd’hui disparu ou qu'il s'agisse simplement d'une mauvaise transcription du mot "kopher" (« résine »). Aucune de ces diverses possibilités ne fait cependant l'unanimité à l'heure actuelle.

Selon l'assyriologue Irving Finkel, une tablette mésopotamienne d’argile présentée au British Museum décrit le héros Atrahasis façonnant une arche ronde comme le guffa, une sorte de coracle de l’antique Mésopotamie. Ce type d'embarcation fluviale toujours en usage en Irak dans la première moitié du , était non pas en bois résineux mais en vannerie calfatée avec du bitume pour assurer l'étanchéité. Dans cette tablette, le dieu Enki précise à Atrahasis que le coracle doit être fabriqué en cordes de fibres de palme et consolidé avec des membrures de bois placées dans l’espace intérieur, tout en mettant en place des étais destinés à supporter un pont supérieur. Selon Finkel, le guffa qui devait faire un diamètre de et une superficie de a pu servir de modèle à l'arche de Noé.

L'arche, selon les instructions de Dieu, devait faire trois cents coudées de long, la coudée étant une unité de mesure désignant la distance depuis le coude jusqu’au bout des doigts. De nombreuses coudées différentes ont été utilisées sous l’Antiquité, mais toutes partageaient de grandes similarités, et la plupart des études fondamentalistes s’accordent à donner à l’embarcation une longueur approximative de 137 mètres. En tout état de cause, c’est beaucoup plus que n’importe quel navire en bois construit au cours de l’histoire. D’après certaines sources, l’amiral chinois Zheng He, au début du , aurait disposé de jonques d’une longueur atteignant , mais ce chiffre pourrait être le fruit d’une exagération.

Le six-mâts goélette "Wyoming", mise à la mer en 1909, était longue de « seulement » et constitue le plus grand navire en coque de bois jamais construit et dont on peut attester l’existence. Ce navire, d’ailleurs, avait besoin de supports en fer pour empêcher toute déformation et d’une pompe marchant à la vapeur pour remédier à de sérieux problèmes de voies d’eau : la construction et les défauts inhérents à ces grands bateaux en bois, dans l’Europe de la fin du , indiquaient suffisamment que leur taille avait franchi les limites pratiques de ce type de matériau. Les chercheurs fondamentalistes qui acceptent ces objections — car ce n’est pas toujours le cas — estiment que Noé a dû construire l’arche à l’aide de techniques apparues postérieurement au .

L’arche devait avoir un volume total d’environ m³ et un déplacement égal à un peu moins de la moitié de celui du Titanic, soit environ tonnes. Son espace habitable total devait avoisiner m². 

La question de savoir si l’embarcation, dans ces conditions, pouvait avoir contenu deux spécimens ou plus de chaque espèce animale, en plus de la nourriture et de l’eau douce, fait l’objet de débats nourris, parfois même houleux, entre les fondamentalistes et leurs contradicteurs. Tandis que certains fondamentalistes maintiennent que l’arche pourrait avoir renfermé toutes les espèces connues, une position plus consensuelle aujourd’hui chez la majorité des fondamentalistes voudrait que l’embarcation ait contenu des grands « genres » d’animaux plutôt que toutes les espèces : par exemple, un seul mâle et une seule femelle du « genre » félin plutôt que des spécimens de tigres, de lions, de couguars, etc. Le jésuite Athanase Kircher (1601-1680) expliquait ainsi que seules les espèces principales auraient été contenues dans l'arche, les autres ayant été engendrées des premières par l'influence des astres et du climat ainsi que l'imagination des mères.

Un autre problème se pose : comment en quelques jours Noé et les siens auraient-ils pu récupérer des animaux sur toute la surface de la Terre et même sur des continents inconnus à l'époque : coati en Amérique centrale, marsupiaux en Australie, manchot en Antarctique, etc. ? Parmi les sujets annexes, certains se sont demandé si huit êtres humains pouvaient suffire à assurer à la fois la navigation du bateau et l’entretien des animaux, ou comment les besoins nutritifs de quelques animaux particulièrement exotiques auraient pu être satisfaits. D’autres mettent en avant le problème de l’éclairage, de la ventilation, du contrôle de la température, de l’hibernation de certains animaux, etc.

Le dénouement de l’aventure est tout aussi riche en interrogations : qu’auraient pu manger les animaux juste après la sortie de l’arche, et comment auraient-ils migré jusqu’à leurs habitats actuels ?

La plupart des chercheurs contemporains acceptent la thèse que le Déluge biblique est intimement lié à un cycle de la mythologie assyro-babylonienne, avec laquelle il partage de nombreux points communs. La plus ancienne version de l’épopée d’Atrahasis a pu être datée du règne de l’arrière-petit-fils d’Hammurabi, Ammi-ṣaduqa (de -1646 à -1626), et a continué à être reproduite jusqu’au premier millénaire avant l'ère chrétienne. À en juger par son écriture, la légende de Ziusudra pourrait quant à elle remonter à la fin du , tandis que l’histoire d’Uta-Napishtim, qui nous est connue grâce à des manuscrits du premier millénaire avant notre ère, est probablement une variation sur l’épopée d’Atrahasis. Les diverses légendes mésopotamiennes du Déluge ont connu une remarquable longévité, certaines ayant été transmises jusqu’au Les archéologues ont retrouvé un nombre substantiel de textes originaux en sumérien, en akkadien et en assyrien, rédigés en écriture cunéiforme. La recherche de nouvelles tablettes se poursuit, de même que la traduction des tablettes déjà découvertes. L'évidente parenté entre les deux traditions mésopotamienne et biblique, selon une hypothèse scientifique, pourrait avoir pour origine commune la rapide montée des eaux dans le bassin de la mer Noire, il y a plus de sept millénaires, en raison d'une rupture de la digue naturelle anciennement formée par le détroit du Bosphore. 

L’épopée d’Atrahasis, écrite en akkadien (la langue de l’ancienne Babylone), raconte comment le dieu Enki enjoint au héros Atrahasis (le « très sage ») de Shuruppak de démanteler sa maison, faite en roseaux, et de construire un bateau afin d’échapper au déluge que le dieu Enlil, irrité par le bruit des villes, entend lancer pour éradiquer l’humanité. Le bateau est censé disposer d’un toit « pareil à celui d’Apsû » (l’océan souterrain d’eau douce dont Enki est le seigneur), de ponts inférieur et supérieur, et doit être rendu étanche par du bitume. Atrahasis monte à bord avec sa famille et ses animaux, puis en scelle l’entrée. La tempête et le déluge commencent, « les cadavres encombrent la rivière comme des libellules », et même les dieux prennent peur. Au bout de sept jours, le déluge cesse et Atrahasis procède à des sacrifices. Enlil, pour sa part, est furieux, mais Enki le défie ouvertement, en déclarant s’être « assuré que la vie soit préservée ». Les deux divinités s’accordent finalement sur d’autres mesures pour réguler la population humaine. L’histoire existe également dans une version assyrienne plus tardive.

La légende de Ziusudra, écrite en sumérien, a été retrouvée dans les fragments d'une tablette d’Eridu. Elle raconte comment le même dieu Enki avertit Ziusudra (« il a vu la vie », en référence au don d’immortalité qui lui fut conféré par les dieux), roi de Shuruppak, de la décision des dieux de détruire l’humanité par un déluge, le passage expliquant les raisons de ce choix ayant été perdu. Enki charge Ziusudra de construire un grand bateau, mais les instructions précises ont été perdues elles aussi. Après un déluge de sept jours, Ziusudra procède aux sacrifices requis, puis se prosterne devant An, le dieu du ciel, et Enlil, le chef des dieux. Il reçoit en échange la vie éternelle à Dilmun, l’Éden sumérien.

L’épopée babylonienne de Gilgamesh raconte les aventures d’Uta-Napishtim (en réalité une traduction de « Ziusudra » en akkadien), originaire de Shuruppak. Ellil (équivalent d’Enlil), chef des dieux, souhaite détruire l’humanité par un déluge. Uta-Napishtim reçoit du dieu Ea (équivalent d’Enki) le conseil de détruire sa maison en roseaux et d’utiliser ces derniers pour construire une arche, qu’il doit charger d’or, d’argent, de la semence, de toutes les créatures vivantes ainsi que de tous ses artisans. Après une tempête de sept jours et douze jours supplémentaires passés à dériver sur les eaux, le navire s’échoue sur le mont Nizir. Sept autres jours plus tard, Uta-Napishtim envoie une colombe, qui revient, puis une hirondelle, qui revient également. Le corbeau, finalement, ne revient pas. Uta-Napishtim fait alors des sacrifices (par groupes de sept) aux dieux. Ces derniers sentent l’odeur de la viande rôtie et affluent « comme des mouches ». Ellil est fâché de ce que quelques humains aient survécu, mais Ea le sermonne : « Comment as-tu pu lancer ainsi un déluge sans réfléchir ? Sur le pécheur laisse reposer son péché, sur le malfaiteur son méfait. Abstiens-toi, ne laisse pas faire, et aie pitié, [que les hommes ne périssent point] ». Uta-Napishtim et sa femme reçoivent alors le don d’immortalité, et partent habiter « au loin, à l’embouchure des rivières ».

Au , Bérose, un grand prêtre du temple de Marduk à Babylone, rédigea en grec une histoire de la Mésopotamie pour Antiochos, qui régna de -323 à -261. Cette "Babyloniaka" de Bérose a été perdue, mais l’historien chrétien Eusèbe de Césarée, au début du , en retient la légende de Xisuthrus, une version grecque de Ziusudra largement semblable au texte d’origine. Eusèbe estimait que le navire pouvait toujours être aperçu « sur les monts corcyréens [sic] d’Arménie ; et les gens grattent le bitume avec lequel il avait été revêtu extérieurement pour l’utiliser en tant qu’antidote ou amulette ».

Les histoires rapportant des déluges et la survie d'une poignée d'élus sont très répandues dans toutes les mythologies du monde, avec des exemples dans presque chaque société.


Des légendes de déluges ont aussi pu être mises en évidence dans les mythologies de nombreuses peuplades sans système d’écriture, parfois très loin de la Mésopotamie et du continent eurasiatique : ainsi des légendes de la tribu amérindienne des Ojibwés. Les fondamentalistes bibliques en tirent la conclusion que l’arche de Noé a constitué un épisode historique réel. Mais les ethnologues et les mythologistes conseillent de prendre avec précaution les légendes telles que celles des Ojibwés, qui ont pu naître ou être fortement adaptées au contact du christianisme, dans un désir de conjuguer harmonieusement anciennes et nouvelles croyances. De plus, toutes ces légendes ont pour source le besoin commun d’expliquer les catastrophes naturelles, face auxquelles les sociétés anciennes étaient toutes impuissantes.

L’histoire de Noé et de l’arche fit l’objet de nombreux embellissements dans la littérature rabbinique juive tardive.

En premier lieu, le fait que Noé n’ait pas jugé utile de prévenir ses contemporains du danger qui les guettait a été largement interprété comme une limite de sa supposée rectitude – peut-être cet homme ne semblait-il juste que par contraste avec une génération particulièrement corrompue ? D’après une autre tradition, il aurait en réalité répandu l’avertissement divin, et planté des cèdres près de cent vingt ans avant le Déluge pour que les pécheurs aient le temps de prendre conscience de leurs fautes et de s’amender. Afin de protéger Noé et sa famille des malfaisants qui les raillaient et les malmenaient, Dieu aurait également placé des lions et d’autres animaux féroces à l’entrée de l’arche. Selon un midrash, c’est à Dieu ou aux anges que l’on doit d’avoir réuni les animaux autour de l’arche, avec leur nourriture. Étant donné que jamais encore ne s’était fait sentir le besoin de distinguer les animaux impurs des animaux purs, ces derniers se firent connaître en s’agenouillant devant Noé lorsqu’ils entraient dans l’arche. Une autre source affirme que c’est l’arche elle-même qui a distingué le pur de l’impur, en admettant en son sein sept couples des premiers et seulement deux des seconds.

Noé, pendant le Déluge, se dévoua jour et nuit à l’alimentation et aux soins des animaux, et ne dormit pas une seule fois de toute l’année qu’il passa dans l’arche. Les animaux étaient tous les meilleurs spécimens de leurs espèces, et se comportèrent ainsi admirablement. Ils s’abstinrent de toute procréation, de manière à ce que le nombre de créatures au sortir de l’arche soit exactement le même qu’à l’entrée. Noé fut cependant blessé par le lion, le rendant inapte à accomplir ses obligations cultuelles : le sacrifice réalisé après le voyage fut donc en réalité accompli par son fils Sem. Le corbeau, pour sa part, posa quelques problèmes lorsqu’il refusa de quitter l’arche, au motif qu’il soupçonnait Noé d’avoir de mauvais desseins envers sa femelle. Néanmoins, et comme le soulignent les commentateurs, Dieu souhaitait sauver le corbeau, car ses descendants étaient destinés à nourrir le prophète Élie.

Les déchets et les eaux usées étaient stockés sur le plus bas des trois ponts de l’arche. Les humains et les animaux purs occupaient le second, tandis que les animaux impurs et les oiseaux étaient relégués au niveau le plus élevé. Une tradition divergente situe les déchets au pont supérieur, d’où ils étaient rejetés à la mer par une trappe spécialement aménagée. Des pierres précieuses, brillantes comme en plein jour, fournissaient de la lumière, et Dieu s’assura que la nourriture restât saine. Le géant Og, roi de Bachân, faisait nécessairement partie des heureux passagers, puisque ses descendants sont mentionnés dans les livres suivants de la Torah : en raison de sa taille, il lui fallut cependant rester dehors, ce qui nécessita de lui fournir sa nourriture à travers un trou pratiqué dans la paroi de l’arche.

Les écrivains du début de l’ère chrétienne s’essayèrent à des interprétations allégoriques assez élaborées de l’histoire de l’arche. Augustin d'Hippone (354-430), dans "La Cité de Dieu", démontre que les proportions de l’arche correspondent à celles du corps humain, qui est aussi le corps du Christ, qui est aussi l’Église. L’identification de l’arche à l’Église peut se retrouver dans le rite anglican du baptême, lequel consiste à demander à Dieu, « qui dans [Sa] grande pitié a sauvé Noé », de recevoir au sein de l’Église l’enfant qu’on lui présente. Jérôme de Stridon (347-420), s’intéressant à la figure du corbeau qui partit et ne revint pas, surnomma ce volatile « l’infect oiseau de la corruption », qu’il convient d’expulser de soi par le baptême. De façon plus durable, la colombe et la branche d’olivier en vinrent à symboliser le Saint-Esprit, puis l’espoir du salut et, finalement, la paix. 

Sur un plan plus pratique, Origène (182-251), répliquant à un adversaire qui doutait que l’arche ait pu contenir tous les animaux du monde, développa une argumentation érudite au sujet des coudées. Le théologien fait d’abord valoir que Moïse, l’auteur traditionnel du livre de la Genèse, avait été élevé en Égypte antique, ce qui l’a amené fort naturellement à s’exprimer en coudées égyptiennes, plus grandes que la moyenne. L’arche avait par ailleurs la forme d’une pyramide tronquée, rectangulaire plutôt que carrée à sa base, et s'effilait jusqu'à un sommet carré d'une coudée de côté. Ce n'est pas avant le que l'arche en vint à être envisagée comme une boîte rectangulaire dotée d'un toit incliné.

Selon le récit coranique et les hadiths attribués à Muhammad, il n'existe pas de détails d'un déluge ou de l'embarcation de Noé, mais la tradition musulmane a amplifié le récit coranique au contact des populations conquises (principalement judéo-chrétiennes), et au gré de l'imaginaire populaire. Contrairement à la tradition juive, qui utilise pour décrire une arche des termes vagues pouvant se traduire par « boîte » ou « caisse », la sourate 29 verset 15 parle d'une "safina" (), et on trouve huit fois le mot "fulk" (), autrement dit une embarcation ordinaire, et la sourate 54 verset 13 évoque quant à elle « un objet de planches et d'étoupe ». La notion de Déluge est étrangère au Coran, qui décrit plutôt une inondation, en ("Tûfân"), un mot d'origine araméenne, et non un Déluge, en ("heTûl"). Une inondation que le Coran inscrit parmi d'autres cataclysmes destructeurs de peuplades comme les Ad, le Thamud, le peuple de Loth, celui de Chu'ayb, et l'évincement du règne du Pharaon de l'Exode. Le récit de Noé dans la version coranique est simple et décrit une inondation dont se sauvent quelques hommes et vaguement "de chaque couple une paire" (de bêtes), en . L'inondation est provoquée par Allah en réponse aux prières de Noé contre son peuple adorant des rochers dressés, sa génération corrompue doit être détruite. Le Coran se ferait néanmoins l'écho d'un récit sumérien ou apparenté selon plusieurs spécialistes, comme Mümin Köksoy ou Bayraktar Bayraklı, quand il affirme «… Il demeura parmi eux mille ans moins cinquante années… » (Coran, XXIX:14). Les deux universitaires soulignent ainsi que des écrits en cunéiforme sumériens montrent des dates de naissance et de décès proches de mille ans voire plus. Ils précisent qu'il pourrait s'agir d'un très vieux calendrier fondé sur un comput en nombre de mois lunaires, comme chez les chasseurs cueilleurs d'Amazonie, d'avant l'élaboration de calendriers solaires et luni-solaires. L'expression "moins cinquante années" semble témoigner d'un comput quinquagésimal ou d'un ajustement du calendrier sumérien (1000-50). Ainsi l'âge de Noé selon le nouveau calendrier serait d'environ 75 ans. Tabari note que quelques exégètes prétendent que l'inondation eut été locale, tandis que d'autres disent que l'inondation s'étendit sur toute la terre. Noé est considéré souvent comme un ancêtre commun de tous les hommes actuels. Mais selon d'autres Noé ne serait pas l'ancêtre de toute l'humanité. Les tenants de la thèse que Noé serait un ancêtre commun universel soulignent que de nombreuses lignées auraient été détruites après Noé, ne subsisterait ainsi que sa lignée à lui. Le Coran fait du récit un souvenir ancestral conservé depuis les origines. (Coran, LXIX:12-13) : "C'est Nous qui, quand l'eau déborda, vous avons chargés sur l'embarcation. Afin d'en faire pour vous un rappel que toute oreille fidèle conserve".

Dans la littérature islamique post coranique - il n'existe pas de parole extra coranique attribuée à Muhammad au sujet d'un déluge -, il existe une vaste littérature inspirée d'"isrâiliyyât". En islam, Noé (Nuh) est l'un des cinq principaux prophètes de l'islam, et son histoire sert généralement à illustrer le sort de ceux qui refusent d'écouter la parole divine. Les références de Noé sont omniprésentes dans le Coran, particulièrement dans la sourate 11, intitulée « Houd », des versets 27 à 51. Selon certains mystiques et exégètes musulmans, le point de départ de l'embarcation de Noé sera situé dans les environs de la Mecque considérée comme le centre du monde, du côté de Djebel Nour, et elle se serait arrêtée sur le « Jûdi », de localisation mystérieuse, interprété par la tradition comme une colline située sur la rive est du Tigre, près de la ville de Mossoul au nord de l'Irak. Al-Mas'ûdî (mort en 956) affirmera ainsi que l'endroit où l'embarcation se serait échouée pouvait encore être aperçu à son époque. Le Coran met ces paroles dans la bouche de Noé, s'adressant à ses contemporains : « Montez dedans. Que sa course et son mouillage soient au nom d'Allah ». Al Baidawi, écrira à partir de ce verset, au , que Noé proclamait le nom d'Allah pour mettre l'arche en mouvement, et qu'il faisait de même pour l'arrêter.

Noé est décrit par Al-Baidawi, comme étant un homme intègre, qui continue entretemps de prêcher, et fait si bien que soixante-dix idolâtres se convertissent et le rejoignirent sur l'embarcation, portant ainsi le nombre total de passagers humains à soixante-dix-huit (puisque la propre famille de Noé compte huit membres). Ce dernier a cependant un quatrième fils, Kan'an, qui refuse de se convertir et meurt noyé. La femme de Noé meurt également. Al-Baidawi affirme en se fondant sur des isrâiliyyât que les dimensions de l'arche seraient de trois cents coudées de long, cinquante de large et trente de haut. Il explique ensuite que le premier des trois étages serait destiné aux animaux sauvages et domestiques, tandis que le second accueillerait les êtres humains et que le troisième abritait les oiseaux. Ce qui correspond point par point à la version biblique du récit de Noé. Sur chaque planche figurera le nom d'un prophète. Trois planches manquantes, symbolisant donc trois prophètes, seront apportées d'Égypte par Og, fils d'Anak, le seul des géants à avoir pu survivre à l'inondation. Le corps d'Adam sera placé au milieu de l'embarcation, pour séparer les hommes des femmes. Toujours à travers la tradition musulmane héritée de la littérature biblique et parabiblique, Noé et ses compagnons passeront cinq à six mois à bord de l'arche, au bout desquels il enverra un corbeau. Mais ce dernier s'arrêtera pour se repaître d'une charogne, et sera maudit par Noé, qui enverra alors la colombe, rappelée depuis lors comme l'amie de l'humanité. 

Ibn Battûta (1304-1377), serait passé un jour par le mont Jûdi, près de Mossoul, là où la tradition musulmane situe désormais l’arrivée de l'embarcation de Noé. Al Masudi écrira même qu'Allah ordonna à la terre d'absorber l'eau, et que certains territoires peu prompts à obéir reçurent de l'eau salée en punition, devenant ainsi secs et arides. L'eau qui ne sera pas absorbée formant les mers et les océans, si bien que certaines eaux du Déluge existent encore aujourd’hui. Noé quittera l'arche le dixième jour de mouharram, c'est-à-dire à l'Achoura. Les autres rescapés et lui édifieront une ville au pied du mont Jûdi, qu'ils baptiseront "Thamanin" (« quatre-vingts ») en raison de leur nombre. Noé fermera alors l'arche et en confiera la clé à Sem. Yaqout al-Rumi (1179-1229) mentionnera également une mosquée construite par Noé et visible à son époque. Ainsi, la tradition sur le récit de Noé a été fortement amplifiée dans l'imaginaire populaire musulman après Muhammad, au contact du monde persan et byzantin.

La Renaissance fut le théâtre de spéculations hasardeuses qui auraient paru familières à Origène ou Augustin d'Hippone : parmi les animaux, qu’en est-il par exemple du Phénix ? Cette créature étant unique, on concevait mal comment l’arche aurait pu en accueillir un couple : une solution populaire voulait que le Phénix contienne en lui les deux principes du masculin et du féminin. Quant aux sirènes, qui par nature poussent les marins à leur perte, ont-elles été autorisées à bord ? La réponse est cette fois négative, les tentatrices ayant préféré nager dans le sillage de l’arche. Et l’oiseau de paradis, qui n’a pas de pattes (), a-t-il dû pour cette raison voler sans interruption à l’intérieur de l’arche pendant le voyage ?

Mais à la même époque émergea une nouvelle école de pensée qui, sans jamais remettre en cause la vérité littérale de l’histoire de l’arche, voulut déterminer les spécifications techniques de l’embarcation de Noé avec une rigueur scientifique et naturaliste entièrement nouvelle. Ainsi, au , un inconnu du nom d'Alfonso Tostada fit un exposé détaillé de la logistique de l’arche, allant du traitement du fumier à la bonne circulation de l’air frais. Un grand géomètre du , Johannes Buteo, calcula les dimensions internes du bateau, en réservant de la place pour des installations aussi variées que des meules ou des fours. Ce modèle fut ensuite largement adopté par d’autres commentateurs.

À partir du , l’exploration du Nouveau Monde fit progressivement prendre conscience de la répartition mondiale des espèces : il devint nécessaire de concilier ce nouveau savoir avec l’ancienne croyance que toute forme de vie postérieure au Déluge venait du mont Ararat. L’une des premières réponses fut que l’Homme s’était dispersé à travers les continents à la suite de la destruction de la tour de Babel, et avait emmené les animaux avec lui. Cette hypothèse comportait cependant des incohérences : pourquoi, se demande Sir Thomas Browne en 1646, les Amérindiens ont-ils emporté les serpents à sonnettes et pas les chevaux ? « Que l’Amérique abondait de bêtes de proie et d’animaux nuisibles, mais ne contenait pas cette indispensable créature, le cheval, est très étrange ».

Browne, qui fut parmi les premiers à remettre en cause la notion de génération spontanée, était un médecin et un scientifique amateur, et n’a pas cherché à approfondir cette boutade. Mais quelques commentateurs bibliques de l’époque, au premier rang desquels Juste Lipse (1547-1606) et Athanasius Kircher (1601-1680) se mirent eux aussi à soumettre l’histoire de l’arche à un examen plus rigoureux, leur objectif restant cependant de concilier le récit biblique avec les avancées en sciences naturelles. Les hypothèses de travail ainsi obtenues stimulèrent l’étude de la répartition géographique des plantes et des animaux, et eurent pour conséquence indirecte la naissance de la biogéographie au . Les scientifiques commencèrent à établir des liens entre les différents climats et les animaux ou plantes qui s’y étaient adaptés. Une théorie influente de l’époque voulait que le mont Ararat des temps bibliques ait été divisé en plusieurs zones climatiques, et que les climats venant ensuite à se déplacer, les animaux correspondants suivent le mouvement et repeuplent finalement le globe. Un autre problème était celui du nombre sans cesse croissant des espèces connues : pour Kircher et d’autres naturalistes, il y avait encore peu de difficultés à loger tous les animaux dans l’arche. Mais dès l’époque de John Ray (1627-1705), c’est-à-dire à peine quelques décennies après Kircher, le nombre d’animaux connus avait augmenté bien au-delà des proportions bibliques. Incorporer dans l’arche toute la diversité animale devenait une gageure, et dès 1700, peu de scientifiques étaient encore prêts à défendre une interprétation littérale de l’aventure de Noé.

Selon Flavius Josèphe, c'est dans le pays dit de Carrhes (Haran) "que se trouvent les restes de l'arche où, dit-on, Noé échappa au déluge, restes qui, jusqu'à nos jours, sont montrés à ceux qui veulent les voir"

Depuis l'époque d'Eusèbe de Césarée jusqu'à nos jours, la recherche des restes matériels de l'arche de Noé a constitué une véritable obsession pour certains chrétiens. Au , on doit apparemment à un commentateur arménien dénommé Fauste de Byzance d'avoir appliqué pour la première fois le nom d'« Ararat » à une montagne précise, plutôt qu'à une région (Urartu). L'auteur affirme que l'arche est encore visible au sommet de ce relief, et raconte comment un ange apporta une sainte relique tirée du navire à un évêque, lequel fut ensuite incapable de réaliser l'ascension. La tradition veut que l'empereur byzantin Héraclius ait fait le voyage au . Quant aux pèlerins moins fortunés, ils devaient affronter les zones désertiques, les terrains accidentés, les étendues enneigées, les glaciers et les blizzards, sans compter les brigands, les guerres et, plus tard, la méfiance des autorités ottomanes. 

La région ne fut aménagée et rendue un peu plus hospitalière qu'au , ce qui permit à des Occidentaux aisés de partir à la recherche de l'arche. En 1829, le docteur Friedrich Parrott, après une ascension du mont Ararat, écrivait dans son "Voyage à Ararat" que « tous les Arméniens sont fermement convaincus que l'arche de Noé reste à ce jour au sommet d'Ararat et que, à des fins de préservation, aucun être humain n'est autorisé à s'en approcher ». En 1876, James Bryce, historien, homme politique, diplomate, explorateur et professeur de droit civil à l'université d'Oxford, grimpa au-delà de l'altitude où peuvent pousser les arbres et annonça avoir trouvé une poutre en bois travaillée à la main, d'une longueur de et d'une épaisseur de . Il l'identifia comme une pièce de l'arche. En 1883, le "British Prophetic Messenger" et d'autres journaux indiquèrent qu'une expédition turque enquêtant sur les avalanches avait pu apercevoir l'arche.

La question de l'arche se fit plus discrète au . Au cours de la guerre froide, le mont Ararat se retrouva en effet sur la frontière hautement sensible entre la Turquie et l'Union soviétique, ainsi qu'au beau milieu de la zone d'activité des séparatistes kurdes, si bien que les explorateurs s'exposaient à des risques particulièrement élevés. 

En dépit de ces difficultés, et du commentaire peu encourageant du grand orientaliste Louis Massignon, le Français Jean de Riquer (membre des Expéditions polaires françaises - Missions Paul-Emile Victor) conduisit en 1952 une expédition au mont Ararat : accompagnée d’un détachement militaire turc, celle-ci atteignit le sommet le 18 août 1952, mais les recherches de vestiges de l’arche demeurèrent sans résultat, y compris dans la zone de l’anomalie d’Ararat repérée par observation aérienne (voir à ce sujet les articles parus dans le quotidien Le Monde, l’ouvrage de J. David Pleins et l’émission télévisée d’Alain Decaux).

L'ancien astronaute James Irwin mena deux expéditions à Ararat dans les années 1980, fut même enlevé une fois, mais comme beaucoup ne découvrit aucune preuve tangible de l'existence de l'arche. « J'ai fait tout ce qui m'était possible" », a-t-il déclaré, « "mais l'arche continue à nous échapper ».

Au début du subsistent deux principaux sujets d'exploration : des prises de vue aériennes ou par satellite ont mis d'une part en évidence ce qu'il est convenu d'appeler l'« anomalie d'Ararat », qui montre non loin du sommet de la montagne une tache noire et floue sur la neige et la glace. Mais il faut surtout mentionner ici le site de Durupınar (baptisé ainsi en l'honneur de son découvreur, l'officier turc de renseignement Ilhan Durupinar), près de Doğubeyazıt et à au sud du mont Ararat. Durupinar, qui consiste en une grande formation rocailleuse ayant l'apparence d'un bateau sortant de la terre, a reçu une large publicité grâce à l'aventurier David Fasold dans les années 1990. Le site, par rapport au mont Ararat, présente l'avantage d'être aisément accessible. Sans être une attraction touristique majeure, il reçoit un flot continu de visiteurs. Bien que Durupinar ait depuis été identifié comme une formation naturelle, le grand bateau de pierre a toujours ses avocats.

En 2004, un homme d'affaires originaire de Honolulu, Daniel McGivern, annonça qu'il allait financer une expédition de dollars sur le sommet du mont Ararat au mois de juillet de la même année, afin d'établir la vérité sur l'anomalie d'Ararat. Après des préparatifs très médiatisés, qui inclurent l'achat d'images satellitaires commerciales spécialement réalisées, les autorités turques lui refusèrent toutefois l'accès au sommet, au motif que ce dernier est situé dans une zone militaire. L'expédition fut ensuite accusée par la National Geographic Society de n'être qu'un coup médiatique habilement monté, étant donné que son chef, un professeur turc du nom d'Ahmet Ali Arslan, avait déjà été accusé d'avoir falsifié des photographies de l'arche auparavant. La CIA, qui a examiné les images satellitaires de McGivern, a par ailleurs estimé que l'anomalie était constituée de « couches linéaires de glace recouvertes par de la glace et de la neige plus récemment accumulées ». Des allégations variées et contradictoires ont donc circulé à toutes les époques concernant la découverte de l'arche.

Le , une équipe d'explorateurs évangéliques chinois et turcs incluant des membres de la « Noah's Ark Ministries International » (NAMI) annonce avoir très vraisemblablement découvert l'arche. Cependant, au vu de certaines incohérences et au vu du témoignage de Randall Price, partenaire de la NAMI en 2008, ceci a tout l'air d'être une supercherie.

Le récit de l'arche de Noé a été plusieurs fois adapté au cinéma. Darren Aronofsky en a fait un "" en 2014, "Noé". Une version humoristique et animée, "Oups ! J'ai raté l'arche...", est sortie en 2015.







</doc>

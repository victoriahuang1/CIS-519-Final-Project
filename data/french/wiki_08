<doc id="968" url="https://fr.wikipedia.org/wiki?curid=968" title="Développement durable">
Développement durable

Le développement durable (, parfois traduit par développement soutenable) est une nouvelle conception de l'intérêt général, appliquée à la croissance économique et reconsidérée à l'échelle mondiale afin de prendre en compte les aspects environnementaux et sociaux d'une planète globalisée. Selon la définition donnée dans le rapport de la "Commission mondiale sur l'environnement et le développement" de l'Organisation des Nations unies, dit rapport Brundtland, où cette expression est apparue pour la première fois en 1987, le développement durable est un développement qui répond aux besoins du présent sans compromettre la capacité des générations futures à répondre à leurs propres besoins.

Cette notion s'est imposée à la suite de la prise de conscience progressive, depuis les années 1970, de la finitude écologique de la Terre, liée aux limites planétaires sur le long terme.

La première définition du développement durable apparaît en 1987 dans le rapport Brundtland publié par la "Commission mondiale sur l'environnement et le développement" :
Le développement durable est un développement qui répond aux besoins du présent sans compromettre la capacité des générations futures de répondre aux leurs. Deux concepts sont inhérents à cette notion :

En 1991, Ignacy Sachs propose une définition proche de ce qu'il nomme l'écodéveloppement : .

En France, l'AFNOR définit le développement durable comme un état où . Dans cette définition, .

Parmi les besoins essentiels, on retiendra les besoins indispensables à l'être humain en tant qu’élément de base vivant dans un environnement défini, que l'on appelle les besoins primaires ou physiologiques. Parmi ceux-ci, on notera le besoin de se reproduire qui établit pour l'homme et la femme une filiation, et assure de la sorte le renouvellement des générations.

Face à la crise écologique et sociale qui se manifeste désormais de manière mondialisée (réchauffement climatique, raréfaction des ressources naturelles, pénuries d'eau douce, rapprochement du pic pétrolier, écarts entre pays développés et pays en développement, sécurité alimentaire, déforestation et perte drastique de biodiversité, croissance de la population mondiale, catastrophes naturelles et industrielles), le développement durable est une réponse de tous les acteurs (États, acteurs économiques, société civile), culturels et sociaux du développement.

Il s'agit aussi, en s'appuyant sur de nouvelles valeurs universelles (responsabilité, participation écologique et partage, principe de précaution, débat) d'affirmer une approche double :

Tous les secteurs d'activité sont concernés par le développement durable : l'agriculture, l'industrie, l'habitat, l'organisation familiale, mais aussi les services (finance, tourisme…) matériels ou immatériels. Plus simplement, le développement durable est un mode de développement qui a pour but de répondre aux besoins de tous sans dégrader l'environnement.

L'expression "sustainable development", traduite par développement durable, apparaît dans la littérature scientifique au début des années 1980 (voir par exemple, les articles par Vinogradov ou Clausen de 1981), et pour la première fois dans une publication destinée au grand public en 1987 dans le rapport intitulé "Our Common Future" (Notre avenir à tous) de la Commission mondiale pour le développement et l'environnement de l'Organisation des Nations Unies rédigé par la Norvégienne Gro Harlem Brundtland. 

Une controverse sémantique portant sur la question de savoir s'il fallait parler de développement durable ou soutenable a existé depuis la deuxième traduction en français où l'éditeur canadien a traduit "sustainable" par le mot français soutenable.

Les tenants du terme « durable » plutôt que du mot « soutenable » insistent sur la notion de durabilité définie comme cohérence entre les besoins et les ressources globales de la Terre à long terme, plutôt que sur l'idée d'une recherche de la limite jusqu'à laquelle la Terre sera capable de nourrir l'humanité.
Cependant, la traduction du terme par soutenable, plutôt que durable, peut s'expliquer aussi par de vieilles traces du mot en langue française. En effet, on trouve le mot soutenir employé dans une optique environnementale dès 1346, dans l'ordonnance de Brunoy, prise par Philippe VI de Valois, sur l'administration des forêts, recommandant de les « soutenir en bon état ». Ainsi en matière forestière la notion de forêt cultivée soumise à une exigence de soutenabilité, un renouvellement perpétuel de la ressource, capable d'approvisionner une flotte navale, existe en France depuis plus de six siècles.

L'émergence du concept de développement durable remonte au début du . L'idée d'un développement pouvant à la fois réduire les inégalités sociales et réduire la pression sur l'environnement a fait son chemin. Nous pouvons en retracer quelques jalons majeurs :


La révolution industrielle du introduit des critères de croissance essentiellement économiques, principal critère aisément mesurable : ainsi le produit intérieur brut dont l'origine remonte aux années 1930 est souvent vu comme l'indicateur de la richesse d'un pays. Des corrections ont été apportées dans la deuxième moitié du sur le plan social, avec d'importantes avancées sociales. L'expression « économique et social » fait depuis partie du vocabulaire courant.

Mais les pays développés ont pris conscience depuis les chocs pétroliers de 1973 et de 1979 que leur prospérité matérielle était basée sur l'utilisation intensive de ressources naturelles finies, et que par conséquent, outre l'économique et le social, un troisième aspect avait été négligé : l'environnement (comme dans l'exemple de l'impact environnemental du transport routier). Pour certains analystes, le modèle de développement industriel n'est pas viable ou soutenable sur le plan environnemental, car il ne permet pas un « développement » qui puisse durer. Les points cruciaux en faveur de cette affirmation sont l'épuisement des ressources naturelles (matières premières, énergies fossiles pour les humains), la pénurie des ressources en eaux douces susceptible d'affecter l'agriculture, la destruction et la fragmentation des écosystèmes, notamment la déforestation qui se manifeste par la destruction des forêts tropicales (forêt amazonienne, forêt du bassin du Congo, forêt indonésienne), ainsi que la diminution de la biodiversité qui diminuent la résilience de la planète ou encore le changement climatique dû aux émissions de gaz à effet de serre et de manière générale la pollution due aux activités humaines. Les catastrophes industrielles de ces trente dernières années (Seveso (1976), Bhopal (1984), Tchernobyl (1986), Exxon Valdez (1989), etc.) ont interpellé l'opinion publique et les associations telles que le WWF, les Amis de la Terre ou encore Greenpeace (Voir aussi Chronologie de l'écologisme). En faisant le pari du « tout technologique » dans l'optimisation de la consommation énergétique et la lutte contre le changement climatique, notre civilisation recourt de façon accrue aux métaux que nous ne savons pas bien recycler. La déplétion de ces ressources pourrait devenir un enjeu mondial au même titre que la déplétion du pétrole. Depuis les accords ADPIC (Aspects des droits de propriété intellectuelle qui touchent au commerce) qui autorisent la brevetabilité du vivant, et malgré la convention sur la diversité biologique de 1992, les pays du Nord utilisent les brevets et la propriété intellectuelle pour accaparer les ressources biologiques des pays du Sud, phénomène appelé « biopiratage ».

Au problème de viabilité subsiste une pensée humaine à adapter. Ce qui s'ajoute à un problème d'équité : les pauvres subissent le plus la crise écologique et climatique, et il est à craindre que le souhait de croissance des pays sous-développés ou pays du Sud vers un état de prospérité similaire, édifié sur des principes équivalents, n'implique une dégradation encore plus importante et accélérée de l'habitat humain et peut-être de la biosphère. Ainsi, si tous les États de la planète adoptaient l"American Way Of Life" (qui consomme près de 25 % des ressources de la Terre pour 5 % de la population), il faudrait cinq planètes pour subvenir aux besoins de tous selon l'association écologiste WWF.

Le développement actuel étant consommateur de ressources non renouvelables et considéré par ces critiques comme très gourmand en ressources compte tenu de la priorité donnée aux objectifs patrimoniaux à courte vue, tels que la rentabilité des capitaux propres, voire inéquitable, une réflexion a été menée autour d'un nouveau mode de développement, appelé « développement durable ».

C'est le philosophe allemand Hans Jonas qui a le premier théorisé la notion de développement durable dans "Le Principe responsabilité" (1979). Selon lui, il y a une obligation d'existence des générations futures, qui pourrait être remise en cause par la forme qu'a prise le progrès technique à l'époque contemporaine. Il s'agit donc pour les générations présentes de veiller, non aux droits des générations futures, mais à leur "obligation" d'existence. « Veiller à l'obligation des générations futures d'être une humanité véritable est "notre" obligation fondamentale à l'égard de l'avenir de l'humanité, dont dérivent seulement toutes les autres obligations à l'égard des hommes à venir ». Le problème du développement durable ne se pose donc pas sous l'angle des droits, mais des obligations et des devoirs.

Les aspects essentiels du développement durable, sur les capacités de la planète et les inégalités d'accès aux ressources posent des questions philosophiques et éthiques.

Hans Jonas avança l'idée selon laquelle le modèle économique de l'Occident pourrait ne pas être viable sur le long terme s'il ne devenait pas plus respectueux de l'environnement. En effet, Jonas posa l'idée d'un devoir vis-à-vis des êtres à venir, des vies potentielles et « vulnérables » que nous menaçons et il donne à l'homme une responsabilité. Depuis, l'un des thèmes de la philosophie qui interpelle le plus nos contemporains est celui de la philosophie de la nature, qui interroge sur la place de l'homme dans la nature. Ainsi, en 1987, Michel Serres décrit l'homme comme signataire d'un contrat avec la nature, reconnaissant les devoirs de l'humanité envers celle-ci. À l'inverse, le philosophe Luc Ferry souligne, dans "Le Nouvel Ordre écologique", que l'homme ne peut pas passer de contrat avec la nature et estime que cette vision qui consiste à donner des droits à la nature participe d'une opposition radicale à l'Occident, de nature révolutionnaire et non réformiste, doublée d'un anti-humanisme prononcé.

Jean Bastaire voit l'origine de la crise écologique chez Descartes selon qui l'homme devait se « rendre comme maître et possesseur de la nature ». Au contraire, la géographe Sylvie Brunel critique le développement durable, car elle y voit une conception de l'homme comme un parasite, et la nature comme un idéal. Or, pour elle, l'homme est souvent celui qui protège la biodiversité, là où la nature est le règne de la loi du plus fort, dans lequel « tout milieu naturel livré à lui-même est colonisé par des espèces invasives ».

Sans en aborder tous les aspects philosophiques, le développement durable comporte également des enjeux très importants en matière d'éthique des affaires. André Comte-Sponville entre autres, aborde les questions d'éthique dans "Le capitalisme est-il moral ?". Paul Ricœur et Emmanuel Lévinas le firent aussi sous l'angle de l'altérité et Patrick Viveret et Jean-Baptiste de Foucauld sur celui de la justice sociale.

Le philosophe français Michel Foucault aborde ces questions sur le plan épistémologique. Il parle de changements de conception du monde, qui se produisent à différentes époques de l'Histoire. Il appelle ces conceptions du monde, avec les représentations qui les accompagnent, des épistémès. Selon certains experts, le développement durable correspondrait à un nouveau paradigme scientifique, au sens que Thomas Kuhn donne à ce terme.

La formule « penser global, agir local », employée par René Dubos au sommet sur l'environnement de 1972, est souvent invoquée dans les problématiques de développement durable. Elle montre que la prise en compte des enjeux environnementaux et sociaux nécessite de nouvelles heuristiques, qui intègrent le caractère global du développement durable. Elle fait penser à la philosophie de Pascal, plutôt qu'à celle de Descartes, celle-ci étant davantage analytique. En pratique, elle devrait se traduire par des approches systémiques. Elle est très bien illustrée par le concept de réserve de biosphère créé par l'Unesco en 1971.

L'expert américain Lester Brown affirme que nous avons besoin d'un bouleversement analogue à celui de la révolution copernicienne dans notre conception du monde, dans la manière dont nous envisageons la relation entre la planète et l'économie : « cette fois-ci, la question n'est pas de savoir quelle sphère céleste tourne autour de l'autre, mais de décider si l'environnement est une partie de l'économie ou l'économie une partie de l'environnement ».

Le philosophe français Dominique Bourg estime que la prise de conscience de la finitude écologique de la Terre a entraîné dans nos représentations un changement radical de la relation entre l'universel et le singulier, et remet en cause le paradigme moderne classique du fait que dans l'univers systémique de l'écologie, la biosphère (le planétaire) et les biotopes (le local) sont interdépendants.

Depuis quelques décennies, les ONG environnementales et des leadeurs d'opinion comme Nicolas Hulot ont sensibilisé l'opinion publique sur les enjeux de l'environnement et du développement durable. La démarche d'action locale pour un impact global est également la thèse du film de Coline Serreau : "Solutions locales pour un désordre global" (voir filmographie).

L'objectif du développement durable est de définir des schémas viables qui concilient les trois aspects écologique, social et économique des activités humaines : « trois piliers » à prendre en compte par les collectivités comme par les entreprises et les individus. La finalité du développement durable est de trouver un équilibre cohérent et viable à long terme entre ces trois enjeux. À ces trois piliers s'ajoute un enjeu transversal, indispensable à la définition et à la mise en œuvre de politiques et d'actions relatives au développement durable : la gouvernance.

La gouvernance consiste en la participation de tous les acteurs (citoyens, entreprises, associations, élus…) au processus de décision ; elle est de ce fait une forme de démocratie participative. Le développement durable n'est pas un état statique d'harmonie, mais un processus de transformation dans lequel l'exploitation des ressources naturelles, le choix des investissements, l'orientation des changements techniques et institutionnels sont rendus cohérents avec l'avenir comme avec les besoins du présent.

Intégrer les enjeux environnementaux et les besoins des générations futures implique d'adopter une approche écosystémique, qui repose sur 12 principes de gestion adoptés à Malawi en 2000. Il conviendrait notamment, selon le huitième principe, de se fixer des objectifs à long terme :
Pour Michel Rocard, qui a été ambassadeur de France chargé de la négociation internationale pour les pôles arctique et antarctique, « le court-termisme nous conduit dans le mur ».

La prise en compte des enjeux de développement durable nécessite un système impliquant trois types d'acteurs : le marché, l’État et la société civile :

La société civile est le cadre le plus approprié pour une économie de la gratuité et de la fraternité. Elle est indissociable des deux autres types d'acteurs.

« Le développement durable est un mode de développement qui répond aux besoins du présent sans compromettre la capacité des générations futures de répondre aux leurs ». Rapport Brundtland

La définition classique du développement durable provient du rapport Brundtland de la Commission mondiale sur l’environnement et le développement. Ce rapport rappelle une citation célèbre, mais à l'attribution incertaine et très débattue (entre autres, sont fréquemment donnés comme son auteur, soit le chef amérindien Seattle dont il existe pourtant seulement des transcriptions apocryphes et très douteuses de son célèbre et mythique discours, soit Antoine de Saint-Exupéry, à moins qu'il s'agisse de la traduction d'un proverbe traditionnel indien ou africain) : « Nous n’héritons pas de la Terre de nos ancêtres, nous l’empruntons à nos enfants ». Ce rapport insiste sur la nécessité de protéger la diversité des gènes, des espèces et de l'ensemble des écosystèmes naturels terrestres et aquatiques, et ce, notamment, par des mesures de protection de la qualité de l'environnement, par la restauration, l'aménagement et le maintien des habitats essentiels aux espèces, ainsi que par une gestion durable de l'utilisation des populations animales et végétales exploitées.

Cette préservation de l'environnement doit être accompagnée de la « satisfaction des besoins essentiels en ce qui concerne l’emploi, l’alimentation, l’énergie, l’eau, la salubrité ». Cela étant, on se heurte à une difficulté, qui est de définir ce que sont les besoins des générations présentes, et ce que seront les besoins des générations futures. On pourrait retenir par exemple les besoins élémentaires pour se nourrir, se loger, et se déplacer.

Dans ce contexte, le développement durable a été inséré parmi les Objectifs du millénaire pour le développement fixés par l’ensemble des États membres de l’ONU.

Afin de subvenir aux besoins actuels sans pour autant recourir à une utilisation non durable de ressources non renouvelables, un scénario en trois points a été proposé, notamment par des associations comme négawatt dans le domaine de l'énergie :

Le patrimoine culturel ne doit pas être oublié : transmis de génération en génération et faisant preuve d'une grande diversité, l'UNESCO en souhaite la préservation. La culture au sens large (ou l'environnement culturel) s'impose d'ailleurs peu à peu comme un quatrième pilier du développement durable.

La consommation de ressources et la production de déchets sont très inégalement réparties sur la planète, comme le montre une carte de l'empreinte écologique par habitant des pays du monde. L'empreinte écologique est la plus élevée dans certains pays du Moyen-Orient (pouvant dépasser 10 hag, hectares globaux), en Amérique du Nord (environ 8 hag aux États-Unis), et en Europe, alors qu'elle peut être inférieure à 1 hag dans certains pays d'Afrique, avec une moyenne mondiale de 2,6 hag. Néanmoins, la détérioration de l’environnement et celle de la société affectent d’une manière particulière les pays les moins avancés de la planète : « Tant l’expérience commune de la vie ordinaire que l’investigation scientifique démontrent que ce sont les pauvres qui souffrent davantage des plus graves effets de toutes les agressions environnementales ». Cela engendre de graves problèmes de justice environnementale. Ainsi, l'inégalité affecte des pays entiers, ce qui oblige à penser à une éthique des relations internationales. On parle de dette écologique entre pays du Nord et pays du Sud, liée à des déséquilibres commerciaux, avec des conséquences dans le domaine écologique, et liée aussi à des modes de vie utilisant de manière disproportionnée des ressources naturelles. Dans son encyclique "Laudato si’" « sur la sauvegarde de la maison commune », le pape François insiste sur la nécessité d'« avoir aussi recours aux diverses richesses culturelles des peuples, à l'art, à la vie intérieure et à la spiritualité » pour s'attaquer aux problèmes d'inégalités.

Lorsque Harry S. Truman s'est adressé à ses concitoyens lors de son discours d'investiture en 1949, pour évoquer l'aide aux pays « sous-développés », le peuple américain était loin de penser que l'humanité serait un jour confrontée à une limitation des ressources naturelles. Depuis les années 1970 et les deux chocs pétroliers de 1973 et 1979, l'Occident prend peu à peu conscience de cette limite naturelle. Depuis les années 2000, les ONG environnementales, avec à leur tête le WWF, ont conceptualisé ces questions avec la notion d'empreinte écologique. Elles ont mis en évidence que l'impact écologique des activités des pays les plus développés (États-Unis, Europe occidentale…) dépassait largement la capacité biologique de la Terre à renouveler les ressources. Il est dès lors évident que le modèle occidental de développement, hérité de la révolution industrielle, n'est pas généralisable tel quel à l'ensemble de la planète.

Cet état de fait amènera certainement une révision nécessaire des modèles utilisés jusqu'à présent en Occident dans un certain nombre de domaines. Il serait présomptueux d'affirmer que le développement durable fournit un modèle de développement. Il s'agit plutôt d'un ensemble de principes, qui fixent des objectifs à atteindre. D'autre part, cette notion fait l'objet, dans les pays développés, d'une communication importante, qui n'est pas, tant s'en faut, toujours suivie d'actions concrètes. Il n'est donc pas possible d'affirmer que l'Occident dispose d'un modèle facilement exportable. D'autre part, comme le soulignait l'Unesco lors du sommet de la Terre de Johannesburg en 2002, dans l'aide au développement, il est nécessaire de tenir compte des spécificités culturelles des pays aidés.

Le codéveloppement est apparu comme une évolution du concept d'aide au développement économique, prenant en compte dans une approche globale et coordonnée, non seulement les aspects économiques, mais aussi les évolutions sociales, l'environnement et le fonctionnement démocratique des institutions, tout en contrôlant mieux les flux migratoires. La coopération au service du développement durable et de la solidarité étant l'une des missions que s'est fixé l'Organisation internationale de la francophonie en 2004, la francophonie peut être considérée comme un cadre intéressant pour promouvoir le développement durable. Selon les mots de Léopold Sédar Senghor, « La création d’une communauté de langue française […] exprime le besoin de notre époque où l’homme, menacé par le progrès scientifique dont il est l’auteur, veut construire un nouvel humanisme qui soit, en même temps, à sa propre mesure et à celle du cosmos ». Par exemple, la création de l'université Senghor, l'un des quatre opérateurs directs de la Francophonie, répond au besoin de définir un modèle de développement dans un esprit de diversité culturelle.

Le site francophone Médiaterre sur le développement durable permet d'animer un réseau de compétences réparti entre les pays du Nord et les pays du Sud.

Il existe une relation équivoque entre l'économie et l'environnement. Les économistes voient l'environnement comme une partie de l'économie, alors que les écologues voient plutôt l'économie comme une partie de l'environnement. Selon Lester R. Brown, il s'agit d'un signe qu'un changement de paradigme est à l'œuvre. L'hypothèse de Michael Porter, selon laquelle les investissements des entreprises pour la protection de l'environnement, loin d'être une contrainte et un coût, peuvent apporter des bénéfices par un changement des modes de production et une meilleure productivité, est encore discutée par les experts.

Ce qui est en question, c'est le rôle du progrès technique dans le développement économique par rapport aux problèmes environnementaux (mais aussi sociaux), comme le soulignait le philosophe Hans Jonas dès 1979 dans "Le Principe Responsabilité". Depuis les chocs pétroliers de 1973 et 1979, ainsi que dans la succession des crises économiques et le tassement de la croissance économique observés depuis les années 1970, le modèle du capitalisme productiviste dans lequel les pays occidentaux se sont lancés au cours du semble être en crise. L'économiste Bernard Perret s'interroge sur la question de savoir si le capitalisme est durable.

Les modèles qui décrivaient l'accroissement de la productivité des facteurs de production atteignent leurs limites. Alors que les physiocrates considéraient la terre comme le principal facteur créateur de valeur, l'école classique et l'école néoclassique n'ont retenu que les deux facteurs de production capital et travail, négligeant le facteur terre (l'environnement). Certes, dans certains courants néoclassiques, comme le modèle de Solow, la productivité globale des facteurs correspond à une augmentation de la productivité qui n'est pas due aux facteurs de production capital et travail, mais au progrès technique. Encore faut-il que celui-ci respecte les contraintes environnementales.

Il faut encore souligner qu'à mesure que les améliorations techniques augmentent l'efficacité avec laquelle une ressource est employée, la consommation totale de cette ressource peut augmenter au lieu de diminuer. Ce paradoxe, connu sous le nom d'effet rebond, ou paradoxe de Jevons, a été vérifié pour la consommation de carburant des véhicules automobiles.

Il semble que les problèmes environnementaux que nous rencontrons soient dus au fait que le facteur de production terre n'a pas été suffisamment pris en compte dans les approches économiques récentes, notamment classique et néoclassique. Un modèle de développement qui permet de concilier progrès technique, productivité, et respect de l'environnement est donc à repenser.

Selon l'économiste belge Christian Gollier, le taux d'actualisation est une variable cruciale de la dynamique économique, en ce qu'il détermine les décisions d'investissement de tous les agents économiques : ménages, entreprises, État. Une valeur du taux d'actualisation d'environ 1 %, beaucoup plus faible que celle qui est actuellement pratiquée, serait nécessaire pour tenir compte des intérêts des générations futures à des horizons relativement éloignés.

Une révision des modèles économiques est en train de s'amorcer, comme le montrent par exemple les travaux du cercle de réflexion Les Ateliers de la Terre.

Si les objectifs du développement durable font l'objet d'un relatif consensus, c'est son application qui demeure source d'oppositions. L'une des questions posées par le terme de « développement durable » est de savoir ce que l'on entend par « durable ». Or, la nature peut être vue de deux manières, complémentaires : il existe d'une part un « capital naturel », non renouvelable à l'échelle humaine (la biodiversité par exemple), et d'autre part des « ressources renouvelables » (comme le bois, l'eau…). Cette distinction étant faite, deux conceptions sur la durabilité vont s'opposer.

La première réponse à la question du développement durable est de type technico-économiste : à chaque problème environnemental correspondrait une solution technique, solution disponible uniquement dans un monde économiquement prospère. Dans cette approche, aussi appelée « durabilité faible », le pilier économique occupe une place centrale et reste prépondérant, à tel point que le développement durable est parfois rebaptisé « croissance durable ». C'est ainsi que dans la revue de l'École polytechnique, Jacques Bourdillon exhorte les jeunes ingénieurs à : « ne pas renoncer à la croissance […] dont l'humanité a le plus grand besoin, même sous prétexte de soutenabilité ». L'une des réponses apportées du point de vue technologique consiste à rechercher la meilleure technique disponible (MTD, en anglais "best available technology", BAT) pour un besoin identifié, ou des attentes exprimées par un marché, qui concile les trois piliers du développement durable d'une façon transversale.

Ce discours est légitimé par la théorie économique néoclassique. En effet, Robert Solow et John Hartwick supposent le caractère substituable total du capital naturel en capital artificiel : si l'utilisation de ressources non renouvelables conduit à la création d'un capital artificiel transmissible de génération en génération, elle peut être considérée comme légitime.

Certains acteurs, et notamment de nombreuses organisations non gouvernementales ou associations environnementales, ont un point de vue tout à fait opposé à l'approche technico-économiste : pour eux, « la sphère des activités économiques est incluse dans la sphère des activités humaines, elle-même incluse dans la biosphère » : le « capital naturel » n'est dès lors pas substituable. Afin d'insister sur les contraintes de la biosphère, les tenants de cette approche préfèrent utiliser le terme de « développement soutenable » (traduction littérale de "sustainable development").

Les économistes systémiques légitiment cette approche : plutôt que de se concentrer sur l'aspect purement économique des choses, ceux-ci souhaitent avoir une vision « systémique [qui] englobe la totalité des éléments du système étudié, ainsi que leurs interactions et leurs interdépendances ». On peut citer Joël de Rosnay, E.F. Schumacher ou encore Nicholas Georgescu-Roegen.

Ces deux approches opposées ne sont bien entendu pas les seules : de nombreuses autres approches intermédiaires tentent de concilier vision technico-économiste et environnementaliste, à commencer par les acteurs publics. On pourra voir à ce sujet la typologie dressée par Aurélien Boutaud.

Toutefois, une approche nouvelle, alternative, est reconnue par le monde académique : celle de la valorisation du social (). On parle de "développement socialement durable" (DSD). Une telle approche demande à ce qu'un principe de précaution social (voir un principe de responsabilité) soit admis. Les priorités du DSD se focalisent sur la réduction des vulnérabilités des personnes en raison de modifications dans la structure des capacités (cf. les "Capabilities Approach" d'Amartya Sen). De façon plus globale, le DSD donne la priorité à l'équité intergénérationnelle (niveaux, conditions, qualité de vie...) par rapport à l'équité intragénérationnelle. Il n'y a pas d'antinomie entre les deux versions de la durabilité (écologique "versus" sociale). La prise en compte de la dimension sociale du développement correspond à l'idée que la protection de la nature ne doit pas se faire au détriment du bien-être des populations vivant au contact direct de celle-ci.

La stratégie de l'Union européenne en faveur du développement durable demande de promouvoir des modes de production et de consommation plus durables. Il convient pour cela de briser le lien entre la croissance économique et la dégradation de l'environnement, et de tenir compte de ce que les écosystèmes peuvent supporter, notamment en ce qui a trait aux ressources naturelles par rapport au capital naturel disponible, et aux déchets.

L'Union européenne doit pour cela promouvoir les marchés publics écologiques, définir avec les parties concernées des objectifs de performance environnementale et sociale des produits, accroître la diffusion des innovations environnementales et des techniques écologiques, et développer l'information et l'étiquetage approprié des produits et services.

Le développement durable peut se décliner de manières complémentaires : au niveau politique, sur les territoires, dans les entreprises, voire dans sa vie personnelle. Le développement durable a d'abord été mis en application sur les territoires (lors du sommet de la Terre de Rio de Janeiro en 1992), puis au sein de l'entreprise et de leurs parties prenantes (lors du sommet de la Terre de Johannesburg).

Historiquement, le développement durable a émergé après une longue période de négociations à l'échelle mondiale.

La première conférence mondiale concernant le développement durable, "a posteriori" rebaptisée « Sommet de la Terre », a eu lieu à Stockholm en 1972.

En 1992, au cours du sommet de la Terre de Rio de Janeiro, sont proclamés les 27 principes de la déclaration de Rio sur le développement durable. Les trois piliers du développement durable sont énoncés pour la première fois au niveau international, et l'agenda 21 pour les collectivités territoriales est élaboré.

En 2002, lors du sommet de la Terre de Johannesburg, les grandes entreprises sont pour la première fois représentées.

Lors de ces rencontres, des représentants des parties prenantes (ONG, États, puis entreprises) discutent des grands enjeux mondiaux, mais aussi des modes de pilotage à mettre en place dans les collectivités et les entreprises pour décliner concrètement le concept de développement durable.

En plus de ces sommets « généralistes » ont lieu des sommets sur des sujets plus ciblés, comme les sommets mondiaux de l'eau, ou la Conférence des parties, qui ont lieu à des échéances plus rapprochées.

Toutefois, les ONG et les associations écologistes, appuyées par plusieurs personnalités, estiment que ces sommets ne sont pas suffisants, et que, pour mettre en œuvre les plus de 300 conventions et traités de droit de l'environnement et faire contrepoids à l'OMC, il faudrait se doter d'un gendarme international aux pouvoirs contraignants, qui pourrait s'appeler « Organisation mondiale de l'environnement ».

Dans l'Union européenne, une partie du droit de l'environnement s'est progressivement déplacé des États membres vers le niveau européen qui est apparu subsidiairement plus adapté pour traiter certaines de ces questions, et ceci en plusieurs étapes :

L'impact de l'environnement sur des domaines aussi vitaux que l'eau, l'énergie, les services, l'agriculture, la chimie… est connu depuis très longtemps : ainsi, on trouve en France dès le l'obligation de faire des enquêtes publiques d'impact préalables à l'implantation d'industries polluantes (enquêtes "de comodo incomodo" pour les tanneries), ainsi qu'une administration des eaux et forêts beaucoup plus ancienne, dotée d'un pouvoir règlementaire et coercitif autonome. L'Union européenne a capté certaines compétences des États nationaux, afin d'établir une nouvelle réglementation européenne qu'elle veut uniforme (directives cadres, directives, règlements) et que les États membres doivent transposer dans leurs règlements et leurs normes.

L'Union européenne a demandé à chacun des États-membres de définir et de mettre en œuvre une stratégie nationale de développement durable.

C'est vers les années 2001-2002 que le développement durable apparaît en France comme la nécessité pour les entreprises de rendre compte des conséquences sociales et environnementales de leurs activités, par rapport aux exigences de la société civile. Cela s'est traduit par une disposition législative sur la communication dans la loi relative aux nouvelles régulations économiques (NRE), poussant à l'élaboration de rapports de développement durable.

L'ancien président Jacques Chirac a poussé à la rédaction d'une charte de l'environnement en 2004, soulignant dans un discours que la France était le premier pays au monde à inclure l'environnement dans sa Constitution.

Dans le même temps, les entreprises anglo-saxonnes tissent des réseaux d'influence autour des institutions internationales, en s'appuyant sur les réseaux des organisations non gouvernementales. Ceci permet de collecter une quantité importante d'informations, qui sont structurées puis gérées dans les réseaux internationaux d'entreprises, d'universités, de centres de recherche (voir par exemple le World Business Council on Sustainable Development).

La stratégie américaine consiste aussi à tisser des liens avec les enceintes normatives privées comme la chambre de commerce internationale, située à Paris. La CCI rédige des « rules », règles types dans tous les domaines de la vie des affaires, reprises comme modèles dans les contrats financés par les organismes internationaux. La CCI a joué un rôle important au sommet de la Terre de Johannesburg à l'été 2002 en créant, conjointement avec le WBCSD, le Business Action for Sustainable Development.

Plusieurs États consacrent explicitement un ministère à la question du développement durable. Les États en question et leurs ministres respectifs, en , sont les suivants :

Depuis le sommet de la Terre de Rio de Janeiro (1992) et la signature de la charte d'Aalborg (1994), les territoires sont au cœur du développement durable. À l'aide de l'Agenda 21 - véritable plan d'action de la politique de développement durable des collectivités - les réseaux de villes et les communautés urbaines sont à même d'exprimer les besoins et de mettre en œuvre des solutions. Pour cela, les collectivités territoriales peuvent coopérer avec les entreprises, les universités, les grandes écoles en France, ainsi qu'avec les centres de recherche, pour imaginer des solutions innovantes pour l'avenir.

Les Agendas 21 locaux, déclinaisons de l'Agenda 21 localement, sont réalisables à l'échelle d'une commune, d'un département, d'une région, d'une communauté de communes ou d'une communauté d'agglomération. Ils sont définis en concertation avec les acteurs locaux, dans un cadre de démocratie participative et se déroulent en plusieurs phases :

Les initiatives locales se multiplient en France et, en juin 2011, le label écologique EcoJardin pour la gestion des espaces verts des grandes villes a été lancé officiellement. Ce label consiste à bannir l'utilisation de produits phytosanitaires dans les jardins publics, en vue de préserver la qualité de l'eau et la biodiversité. Un « référentiel écologique » a vu le jour ; il définit le cahier des charges à respecter pour l'obtention du label « jardin écologique ». Ce label s'ajoute à un autre label européen EVE attribué par Ecocert et déjà opérationnel.

Dans les villes de plus de , un rapport annuel de développement durable doit être produit (en quatre parties) et publié chaque année, jouant un rôle d'accompagnement et d'autoévaluation pour l'amélioration continue. C'est aussi une pièce justificative nécessaire à la demande de labellisation.

Puissantes au niveau international, créatrices de richesses et consommatrices de ressources, les entreprises ont une capacité d’intervention qui peut se révéler particulièrement efficace en faveur du développement durable :

Pour le respect d'objectifs de développement durable par les entreprises, spécifiquement on parle de responsabilité sociale des entreprises ("corporate social responsability") ou parfois plus précisément de "responsabilité sociétale des entreprises" puisque le volet de responsabilité ne correspond pas uniquement au volet « social ».

La responsabilité sociétale des entreprises est un concept par lequel les entreprises intègrent les préoccupations sociales, environnementales, voire de bonne gouvernance dans leurs activités et dans leur interaction avec leurs parties prenantes sur une base volontaire. En effet, à côté des obligations réglementaires et législatives, existe tout un champ d'actions possibles sur la base du volontariat et qui peut s'appuyer notamment sur des normes : à citer cependant en France, une loi relative aux nouvelles régulations économiques (NRE) qui incite les entreprises cotées en bourse à inclure dans leur rapport annuel une série d'informations relatives aux conséquences sociales et environnementales de leurs activités.

La notion de développement durable humain en entreprise devient actuelle à la suite des nombreux problèmes d'absentéisme, de stress et de burn-out. Elle est en lien direct avec le comportement managérial responsable en interne et en externe.

Depuis le début des années 2000, bon nombre d'entreprises se sont dotées de directions du développement durable. Elles ont engagé des politiques souvent ambitieuses pour faire évoluer les comportements internes et incarner de manière tangible leurs responsabilités sociale et environnementale.

En mars 2005, lors d'une réunion de haut niveau des ministères de l'environnement et de l'éducation à Vilnius (Lituanie), a été adoptée une stratégie européenne pour l'éducation en vue du développement durable. L’éducation a été présentée non seulement comme un droit de l’homme, mais également comme une condition "sine qua non" du développement durable et comme un outil indispensable à une bonne gouvernance, à des décisions éclairées et à la promotion de la démocratie. L'éducation au développement durable (EDD) conduit à une prise de conscience plus grande et une autonomie accrue permettant l’exploration de nouveaux horizons et concepts et l’élaboration de méthodes nouvelles. En août 2004 avait déjà été défini un cadre de mise en œuvre de cette stratégie pour l'Europe. Des cadres de mise en œuvre ont également été définis pour l'Afrique, les États arabes, l'Asie/Pacifique, l'Amérique latine et les Caraïbes.

En septembre 2005 a été approuvé le plan international de mise en œuvre de la Décennie des Nations unies pour l’éducation en vue du développement durable, lors d'une session de l’Unesco. Ce plan a défini un cadre pour la décennie 2005-2014.

Dans les différents États-membres de l'Union européenne, des actions sur l'éducation ont été intégrées dans les stratégies nationales de développement durable. En France, l'éducation au développement durable a été intégrée dans les enseignements, particulièrement en histoire-géographie, en éducation civique, et dans les sciences de la vie et de la Terre. À la différence des disciplines scientifiques qui privilégient une éducation à l'environnement et de l'éducation civique qui aborde dans le programme de la classe de sixième l'environnement et sa protection par les citoyens dans un thème consacré aux habitants dans leur commune, l'accent est mis en géographie sur les trois aspects du développement durable (social, économie et environnement). Les programmes des classes de cinquième et de seconde y sont pleinement consacrés.
Le ministère de l'Éducation nationale français a également développé des méthodes d'éducation utilisant les techniques de l'information et de la communication pour l'éducation (TICE). En France aussi, il a été créé en 2011 pour la session 2013 une filière préparant au Baccalauréat sciences et technologies de l'industrie et du développement durable ou cette dernière notion y est intégrée totalement aux programmes.

En France, une dimension de développement durable est généralement intégrée dans l'enseignement supérieur. Dans les écoles d'ingénieurs par exemple, les élèves sont informés de leurs obligations futures à travers la diffusion de la charte d'éthique de l'ingénieur, selon laquelle : « L'ingénieur inscrit ses actes dans une démarche de « développement durable ».

Des formations en ligne ouvertes à tous (MOOC, "massive open online course" en anglais) sur le thème du développement durable ont été développées sur les plateformes FUN, Coursera et l'université des Colibris.

Les entreprises ont en général adopté dans leur stratégie des chartes de développement durable. La communication en interne sur ce sujet a cependant souvent laissé sceptiques les employés, en raison de distorsions avec les pratiques sociales observées sur le terrain.

En France, un certain nombre de dirigeants sont formés régulièrement dans différents organismes, comme le Collège des hautes études de l'environnement et du développement durable, l'institut Cap Gemini sur les aspects informatiques, ou échangent des informations dans le cadre de groupes d'anciens élèves d'écoles ("X-environnement" pour l'École polytechnique, "ISIGE Alumni" pour l'ISIGE-MINES ParisTech, etc.).

En France toujours, les ingénieurs sont tenus, au moins théoriquement, de respecter la charte d'éthique de l'ingénieur élaborée par IESF.

Dans la société civile, ce sont les associations et les organisations non gouvernementales qui contribuent le plus à la sensibilisation du grand public. Les grandes ONG (WWF, Les Amis de la Terre, Secours catholique, Action contre la faim, Amnesty International…) mettent en œuvre des démarches de responsabilité sociétale et organisent régulièrement des campagnes de sensibilisation sur des aspects particuliers du développement durable. Les sites internet de ces associations sont par ailleurs des outils de mobilisation remarquables. Les outils de calcul de l'empreinte écologique, librement accessibles sur la Toile, permettent de faire prendre conscience du problème environnemental.

Enfin, les Nations unies organisent chaque année des Journées mondiales de sensibilisation et consacrent chaque année à un thème lié à la protection de l’environnement. En 2010, elles mettaient l’accent sur la biodiversité. 2011 est instituée année internationale des forêts.

Le produit intérieur brut est un indice très employé dans les comptabilités nationales pour mesurer la croissance économique, au point de conditionner une grande part des raisonnements et stratégies économiques. On dit que l'on est en croissance ou en récession selon que le PIB est en augmentation ou en diminution. Le PIB est censé mesurer la croissance économique sur le long terme, mais il prend mal en compte la variation du capital naturel (éventuellement fossiles) qui est un effet de long terme. C'est notamment la raison pour laquelle le PIB est critiqué par certains auteurs, qui en soulignent les limites pour la mesure effective de la richesse d'un pays.

Le PIB est calculé par agrégation de la valeur ajoutée des entreprises, elle-même calculée en comptabilité nationale en fonction de la production et des consommations intermédiaires. Les indicateurs de développement durable tels que ceux qui figurent dans le Global Reporting Initiative ou les indicateurs demandés par la loi sur les nouvelles régulations économiques en France, ne sont pas intégrés dans ces calculs.

La question se pose donc de savoir si le PIB est vraiment une mesure fiable de développement durable. Les insuffisances du PIB comme mesure de la croissance sur le long terme seraient à l'origine du fait que l'on parle maintenant de PIB vert et de croissance verte.

En France, l'Insee fait néanmoins figurer le PIB comme l'un des onze indicateurs de la stratégie nationale de développement durable. La France a une réflexion sur l'utilisation de nouveaux indicateurs dont l'empreinte écologique.

L'Europe a annoncé qu'elle publierait dès 2010 un indice présentant la pression exercée sur l'environnement (émissions de gaz à effet de serre, réduction des espaces naturels, pollution atmosphérique, production de déchets, utilisation des ressources, consommation d'eau et pollution de l'eau), qui accompagnera la publication du PIB.

Les instruments macroéconomiques classiques (PIB par exemple) s'avèrent insuffisants, voire dans certains cas déficients pour mesurer le développement durable : la croissance économique apparaît ainsi dans certains cas comme déconnectée, voire opposée aux objectifs du développement durable.

Il s'agit donc de construire un indice agrégé qui permet de rendre compte au mieux de l'efficacité d'une politique de développement durable. Plusieurs indices ont été établis, qui concernent chacun un ou plusieurs « piliers » du développement durable :

Tout indice est néanmoins sujet à caution : la manière d'agréger les données exprime un parti-pris. Qu'est-ce qu'un pays « avancé en développement durable » ? Est-ce un pays qui consomme peu de ressources (comme le Bangladesh), ou est-ce un pays avec de nombreux parcs nationaux protégés (comme les États-Unis) ?

L’OQADD, outil de questionnement et d’aide au développement durable, est une grille de questionnement permettant de susciter des débats sur les problématiques relatives au développement durable, en mettant en avant les points-clefs d'un projet. Ils se réclament à la fois de l’évaluation des politiques et de l’analyse multicritère, mais sont plutôt utilisés pour questionner des politiques ou des projets au regard des critères de développement durable. Ce sont des grilles de critères en arborescence, déclinants les principales dimensions du développement durable (économie, écologie, social, gouvernance…).

Cet outil peut être soumis aux différents acteurs intervenant dans la mise en place d’un nouveau projet : des élus, des industriels, des associations de défense de l’environnement, des syndicats…

La mesure microéconomique du développement durable pour les entreprises peut se faire par l'intermédiaire des critères du Global Reporting Initiative, comportant 79 indicateurs économiques. Par ailleurs l'OCDE a effectué des travaux importants sur les indicateurs environnementaux, et a développé pour cela le modèle Pression État Réponse.

Les principales normes et certifications qui peuvent être appliquées par les entreprises sont la norme environnementale ISO 14001, la norme sur le management de l'énergie ISO 50001, la norme sur la qualité ISO 9001, la certification OHSAS 18001 sur la santé et la sécurité au travail, et le standard SA 8000 sur l'éthique et le social. Il existe également un guide SD 21000 (en France) pour la prise en compte des enjeux du développement durable dans les entreprises.

Une nouvelle norme sur la responsabilité sociétale des entreprises, l'ISO 26000, a été mise en application en 2010. Cette norme intègre la responsabilité sociétale, la gouvernance et l'éthique d'une manière plus élargie.

Par ailleurs, les entreprises peuvent être notées par des agences de notation sociétale, qui prennent en compte dans leur notation des critères extra-financiers (environnementaux et sociaux). Les entreprises sont jugées par ces agences sur la base de leurs rapports de développement durable, ou de tout document permettant d'apprécier les performances économiques, environnementales et sociales. La notation sociétale est ensuite utilisée par les investisseurs pour constituer des portefeuilles de valeurs appelés investissements socialement responsables (ISR).

La mise en œuvre d'une démarche de développement durable dans une organisation (collectivité ou entreprise) est un processus complexe, qui engage toutes les fonctions de l'entreprise. Il s'agit de mettre en place une véritable gestion de programme transverse, avec des correspondants dans les principales entités de l'organisation, en impliquant les parties prenantes dans un modèle économique durable. Nous donnons ci-dessous quelques exemples de domaines d'application particulièrement concernés par la mise en œuvre d'une démarche de développement durable ou de responsabilité sociétale.

Les ventes et la logistique sont particulièrement impactées par les questions de développement durable. La fonction administration des ventes des entreprises est en effet responsable de la livraison au client final, qui fait appel le plus souvent au transport routier, fortement consommateur de produits pétroliers.

Il s'agit d'identifier les opportunités et les menaces dans le contexte d'une sensibilité accrue des consommateurs et du marché aux enjeux du développement durable, en accord avec les parties prenantes. Le marketing doit aussi véhiculer vers les autres domaines de l'entreprise les valeurs demandées par le marché. Certaines sociétés se contentent parfois d'opérations de communication plutôt que de vraiment changer le fonctionnement de l'entreprise ; on parle alors d'écoblanchiment (en anglais : "greenwashing").

Élizabeth Reiss montre que les entreprises ont intérêt à créer des produits et des services responsables, parce que les clients le demandent, et parce ce que c'est rentable. Elle donne des pistes pour revoir les modes de production et de communication. L'entreprise peut dans certains cas y gagner en productivité et fidéliser ses équipes de salariés et ses clients.

Christophe Sempels et Marc Vandercammen analysent le comportement du consommateur responsable, et soulignent le rôle du marketing dans la mise en œuvre d'innovations durables et dans leur acceptation par les marchés. Ils cherchent à créer le lien entre une demande et une offre plus responsables, en passant d'une logique « produit » à une logique « service ».

Plusieurs programmes de fidélisation ayant pour but la modification des comportements de consommations au travers d'outils marketing ont vu le jour ces dernières années. C'est par exemple le cas de aux États-Unis ou encore du programme Green Points en France. Ces types de programme utilisent le principe de prime pour motiver le consommateur à changer ses habitudes de consommation.

Les caractéristiques du développement durable que sont les échelles temporelles et spatiales multiples, et l'interconnexion des problèmes, conduisent à des problématiques nouvelles de recherche et développement, à la recomposition de certains champs de recherche, et à l'apparition de nouvelles disciplines. La réponse aux demandes du développement durable passe par un accroissement des travaux de nature interdisciplinaire, entre sciences de la nature et sciences humaines et sociales. Il est nécessaire de structurer la recherche scientifique de manière plus fédérative, en organisant des institutions transversales et internationales. La demande d'expertise nécessite souvent la coopération de disciplines différentes. La recherche pour le développement durable nécessite de meilleures données, plus abondantes, et des outils plus performants dans le domaine de la modélisation et de la prospective. La recherche doit imaginer de nouvelles formes de coopération avec les autres acteurs, responsables politiques, entreprises, associations, syndicats, et autres composantes de la société civile.

Le marketing doit répondre à la question de savoir s'il faut investir dans le recyclage ou investir dans de nouveaux produits propres, ce qui impose des choix dans la recherche et développement. La recherche peut se faire dans des laboratoire de recherche internes aux entreprises, ou en partenariat avec des laboratoires publics, par exemple dans le cadre de pôles de compétitivité.

La recherche et développement peut avoir besoin d'outils de gestion des connaissances pour améliorer l'efficacité de ses recherches. Elle doit procéder à une veille technologique orientée vers des objectifs de développement durable.

Sur le plan règlementaire, le développement durable se traduit par un ensemble de textes juridiques, qui peuvent être établis soit au niveau européen (directives européennes), soit au niveau des États. Quelques exemples de règlements européens sont le règlement REACH sur les substances chimiques, ou la directive sur les déchets d'équipements électriques et électroniques (DEEE), pour ce qui concerne le pilier environnemental.

Au niveau des États, le droit environnemental et droit social s'appliquent sur chacun de ces piliers environnemental et social (en France le code de l'environnement et le code du travail).

En France :

Les services juridiques des entreprises doivent procéder à une veille juridique, éventuellement pour les petites et moyennes entreprises (PME) avec l'aide des chambres de commerce et d'industrie.

Outre cette veille, les services juridiques sont amenés à vérifier la conformité des actions de développement durable de l'organisation dans ses déclinaisons économiques, sociales et environnementales par rapport aux normes applicables et la communication extra-financière qui l'accompagne.

Le respect de critères environnementaux, sociaux, et économiques dans l'élaboration des produits d'une entreprise dépend non seulement de ses processus internes, mais aussi de la qualité des produits achetés auprès des fournisseurs de l'entreprise, des services inhérents à ces achats, en particulier le transport, ainsi qu'en amont de ceux-ci. La performance en matière de développement durable dépend donc de l'intégration progressive de la chaîne d'approvisionnement dans le référentiel de responsabilité sociétale des entreprises concernées. Il est nécessaire de revoir la stratégie achats (réduction des coûts, élimination des déchets, augmentation de l'efficacité énergétique, conservation des ressources), en faisant participer les partenaires fournisseurs de l'entreprise.

Gérer le développement durable dans les achats des entreprises, des organismes publics ou encore des collectivités locales peut se faire en tenant compte du coût global d'acquisition qui, outre le prix d'achat, intègre le transport des produits achetés, le dédouanement, les garanties, les coûts de stockage, l'obsolescence, les déchets générés lors de la production et en fin de vie.

L'engagement d'un plan d'action développement durable aux achats répond généralement à des arguments de quatre natures différentes :

La mise en œuvre d'une politique de développement durable dans les entreprises dépend largement de l'utilisation des ressources de l'entreprise. Ces ressources peuvent être des actifs physiques (immobilisations au sens classique du terme), mais aussi des actifs immatériels (immobilisations incorporelles) ou tout simplement des ressources humaines, c'est-à-dire des salariés et des partenaires de l'entreprise.

L'atteinte des objectifs de développement durable dépend en grande partie de la façon dont les entreprises vont orienter l'action de l'ensemble de ces ressources (employés, parties prenantes, organisation…). Des réflexions apparaissent sur de nouvelles méthodes d'estimation de la valeur financière des entreprises à travers la notion de capital immatériel.

Les actifs financiers que sont les investissements socialement responsables (ISR) permettent d'orienter les portefeuilles de valeurs financières vers des actifs qui respectent des critères à la fois environnementaux, sociaux et économiques. L’ISR a une vision à long terme de nature à donner des résultats meilleurs que ceux des sociétés qui agissent dans la perspective d'objectifs financiers à court terme. Selon une définition officielle donnée en juillet 2013 par le Forum pour l'investissement responsable (FIR), association réunissant les acteurs de l'ISR en France, et l'Association française de la gestion financière (AFG), association des acteur du métier de la gestion, « L'ISR (Investissement Socialement Responsable) est un placement qui vise à concilier performance économique et impact social et environnemental en finançant les entreprises et les entités publiques qui contribuent au développement durable quel que soit leur secteur d'activité. En influençant la gouvernant et le comportement des acteurs, l'ISR favorise une économie responsable ».

L’ISR est encore trop récent et le recul insuffisant pour le vérifier de façon tangible et assez large, mais l’observation des fonds ISR les plus anciens laisse penser que leur rentabilité est comparable, voire parfois meilleure que celle des autres fonds.

Il faut également signaler le développement de toute une branche de la finance, la finance du carbone, liée aux enjeux des gaz à effet de serre. Le projet BlueNext s'inscrit dans ce type d'activités.

Il existe une croyance selon laquelle l'informatique serait « virtuelle » ou « immatérielle ». La dématérialisation, qui consiste à faire passer les flux de documents entre organisations d'un support d'information papier à un support d'information électronique (ce terme est peu approprié, car en réalité la dématérialisation ne supprime pas le fait qu'on utilise de la matière avec un support d'information électronique) est souvent présentée, y compris par les spécialistes du développement durable, comme un avantage du point de vue environnemental, car elle supprimerait la consommation de papier. En fait, on se rend compte que le « zéro papier » est un mythe. Une analyse qualitative des avantages et des inconvénients de la dématérialisation du point de vue du développement durable montre en effet que les choses ne sont pas si simples. En particulier, ce processus n'améliore pas la qualité environnementale des produits.

L'informatisation massive de l'économie depuis une cinquantaine d'années nous a fait passer dans une économie de l'immatériel, dans laquelle l'augmentation des flux de gestion pilotés par l'informatique s'est accompagnée d'une augmentation parallèle des flux de biens marchands, donc des quantités de ressources naturelles consommées, comme le montre Jean-Marc Jancovici.

Concilier développement durable et systèmes d'information n'est pas aisé, car les systèmes d'information ne sont généralement pas pensés pour le long terme. Tant les matériels que les logiciels sont généralement conçus pour une durée d'utilisation de quelques années. D'autre part, les systèmes d'information d'entreprise ont été conçus selon une logique essentiellement comptable et financière. Ils se sont structurés autour de la comptabilité générale, avec des progiciels de gestion intégrés, et ils ont longtemps ignoré les critères extra-financiers du développement durable. Les éditeurs de progiciels de gestion intégrés proposent des offres de mise en conformité réglementaire.

Les initiatives actuelles sur l'application des principes de développement durable en informatique concernent le plus souvent le matériel informatique proprement dit (recyclage et consommation électrique). Il existe une certification internationale pour les équipements, la certification TCO, ainsi qu'une directive européenne sur les substances dangereuses, la directive RoHS. L'informatique verte ("green IT" en anglais) se concentre essentiellement sur les bonnes pratiques portant sur le matériel informatique.

Plus fondamentalement, le développement durable pose de nouveaux défis : faire face à l'augmentation des connaissances, gérer une nouvelle relation avec les clients, respecter des réglementations de plus en plus complexes. Pour cela, il est nécessaire de restructurer les systèmes d'information selon une nouvelle architecture : celle du système d'information durable, combinant gestion des données de référence (MDM), système de gestion de règles métier (BRMS), et gestion des processus métier (BPM).

L'application aux processus métier vertueuse sur le plan du développement durable pose le problème du partage de l'information environnementale et sociale entre les entreprises et administrations publiques, ainsi qu'avec leurs parties prenantes. Concernant l'application au volet environnemental proprement dit, on parlera d'écoinformatique (les Américains emploient l'expression "Green IT 2.0").

Les systèmes d'information actuels sont très hétérogènes et n'ont le plus souvent pas été conçus pour gérer une information à caractère sociétal. Ainsi, les exigences de développement durable nécessitent de structurer les informations utiles pour la gestion des programmes concernés, et plus particulièrement pour la gestion des données et la structuration de réseaux de compétence. Le Royaume-Uni a mis en place une régulation publique de l'information environnementale. La France mise sur l'effet de la loi sur les nouvelles régulations économiques pour réguler l'économie. D'une façon générale, le développement durable pose le défi de gérer une grande quantité d'informations non structurées ; pour cela plusieurs méthodes sont apparues : les techniques du web sémantique s'appuyant sur des ontologies et des métadonnées ; les projets d'ingénierie des connaissances ; les systèmes wiki comme l'encyclopédie Ekopedia, ou Wikia Green.

Un autre problème crucial qui se pose est de savoir quels sont les impacts de la course à la puissance informatique en matière environnementale, et si la fameuse loi de Moore est véritablement pertinente à long terme. On constate que les ordinateurs et les logiciels sont généralement surdimensionnés par rapport aux besoins, et que l'arrivée incessante de nouvelles versions de matériels et de logiciels a pour effet de diminuer la durée d'amortissement des équipements, donc de générer des déchets.

La convergence entre l'internet et le développement durable fait l'objet des réflexions du forum TIC21. L'association ADOME (Association pour le développement d'outils multimédia appliqués à l'environnement) a développé un moteur de recherche du développement durable, Ecobase 21, composé de liens.

Avec la mise en place de programmes de développement durable dans les entreprises et d’agendas 21 dans les collectivités territoriales, s’est posée, à partir de 2002, la question de la « communication sur le développement durable ». Autrement dit, comment sensibiliser l’opinion au développement durable, impliquer les professionnels, et parfois convaincre les décideurs ?

Cette question a en partie trouvé sa réponse dans la création d'une direction du développement durable, qui est désormais perçue comme un poste stratégique dans l'entreprise. Une association loi de 1901, le Collège des Directeurs du développement durable (C3D), participe à faire évoluer la fonction du directeur de développement durable.

Plusieurs autres pistes et éléments de réponse sont donnés par des professionnels :


La mise en œuvre d'une démarche de développement durable dans le domaine du service après-vente se traduit le plus souvent par une politique de réparabilité des produits, qui peut permettre à l'entreprise de fidéliser ses clients et éviter l'obsolescence programmée, source de coûts économiques et environnementaux élevés.

Le développement durable reste un concept pouvant être décliné selon de nombreux axes : ses fondements peuvent être vus comme étant philosophiques et/ou scientifiques, ses applications touchent tout autant le droit que les techniques de pointe ou la gouvernance. Le tableau ci-dessous présente les domaines dans lesquels le développement durable est appliqué, ainsi qu'une liste des articles associés.

Le terme de « développement durable » a été critiqué pour le flou qui l'entoure. Luc Ferry écrit ainsi : « Je sais que l'expression est de rigueur, mais je la trouve si absurde, ou plutôt si floue qu'elle ne dit rien de déterminé. (…) qui voudrait plaider pour un « développement intenable » ! Évidemment personne ! […] L'expression chante plus qu'elle ne parle ».

Le concept rencontre des critiques à plusieurs niveaux. Ainsi, considère que la notion de développement durable est dangereuse, car débouchant sur des mesures aux effets inconnus et potentiellement néfastes. Il écrit ainsi : . À l'opposé de cette notion, il défend l'efficacité de la propriété privée pour inciter les producteurs et les consommateurs à économiser les ressources. Selon Baden, « l'amélioration de la qualité de l'environnement dépend de l'économie de marché et de la présence de droits de propriété légitimes et garantis ». Elle permet de maintenir l'exercice effectif de la responsabilité individuelle et de développer les mécanismes d'incitation à la protection de l'environnement. L'État peut dans ce contexte « créer un cadre qui encourage les individus à mieux préserver l'environnement », en facilitant la création de fondations vouées à la protection de l'environnement.

Certains auteurs craignent une dérive vers des modèles de substitution à durabilité faible, qui admettent que le capital naturel est remplaçable par un capital de connaissances humaines. Pearce et Turner, par exemple, maintiennent que la dégradation du capital naturel est irréversible, en soulignant que la capacité de l'environnement à assimiler les pollutions est limitée ; d'autres auteurs appartenant au courant de l'économie écologique mettent en avant le caractère irremplaçable de certaines ressources naturelles, qui rend le capital naturel non substituable. Le développement durable est également critiqué en ce qu'il peut n'être qu'un outil des pays du Nord contre les pays en développement : la géographe spécialiste du Tiers-Monde Sylvie Brunel estime que les idées de développement durable peuvent servir comme paravent aux idées protectionnistes des pays du Nord pour empêcher le développement par le commerce des pays du Sud. Pour Sylvie Brunel, le développement durable « légitime un certain nombre de barrières à l'entrée ». En offrant ainsi un prétexte au protectionnisme des pays développés, « le sentiment que donne le développement durable, c'est qu'il sert parfaitement le capitalisme ».

Certains auteurs dénoncent une dimension religieuse ou irrationnelle du développement durable. Sylvie Brunel parle ainsi de « technique de marketing digne des grands prédicateurs » et souligne ainsi dans une conférence "Naissance d’une religion : le développement durable", que « le développement durable est le produit de la dernière mondialisation et de toutes les peurs qu’elle peut entraîner ». Pour Claude Allègre, il s'agit d'une religion de la nature, qui a oublié que la préoccupation essentielle devait être l'homme : « La moulinette écologique a, hélas, amplifié le mot « durable » et effacé le mot « développement » au fil des années. Nous revendiquons ici le respect de cette exigence dans son intégralité. Ce n’est pas parce qu’on défend la nature qu’on peut laisser de côté la culture ».

D'autres penseurs soulignent encore les menaces potentielles que les idées au fondement du développement durable peuvent représenter pour les libertés individuelles. Le philosophe Luc Ferry voit par exemple dans les idées de Hans Jonas des idées potentiellement totalitaires et souligne les risques du développement durable à cet égard. Cette crainte est également partagée par nombre de libéraux : « L’environnement peut être le prétexte à une nouvelle augmentation du pouvoir et à des dérives dangereuses de la part des personnes les plus assoiffées de puissance. Même les personnes les mieux intentionnées ne sauraient très probablement pas gérer les pouvoirs immenses dont certains écologistes voudraient voir dotés les gardiens de l’écologiquement correct ».

Les tenants de la décroissance considèrent que le terme de développement durable est un oxymore car les ressources naturelles sont finies alors que le mot « développement » présuppose selon eux une exploitation toujours plus importante de ces ressources. Ainsi, Serge Latouche, sous un angle économique, ou Jean-Christophe Mathias, sous un angle philosophico-juridique, critiquent ce concept. Dans l'essai "Politique de Cassandre", Jean-Christophe Mathias estime que le concept de développement durable est car il propose de régler des problèmes environnementaux par ce qui en est selon lui l'origine, à savoir la croissance économique continue. Il considère que le développement durable, de même que le principe de précaution, n'est pas adapté à une politique volontariste de protection de la nature car il donne à ses yeux la primauté à l'économie sur les questions sociale et environnementale. Serge Latouche de son côté interroge les différentes dénominations du concept, à savoir développement durable, soutenable ou supportable et conclut que le développement serait problématique du fait de la finitude de la planète. Il propose de sortir de l'« économicisme » et d'organiser la décroissance. 

D’autres critiques estiment que les trois dimensions ne suffisent pas à refléter la complexité de la société contemporaine. C'est ainsi que l'organisation Cités et Gouvernements locaux unis (CGLU) a approuvé en 2010 la déclaration « La culture : quatrième pilier du développement », fruit du travail réalisé dans le cadre de l'Agenda 21 de la culture.

Enfin, la définition classique du développement durable issue de la commission Brundtland (1987) peut apparaître à certains dépassée. En effet, il ne s'agit aujourd'hui plus de viser, comme dans les années 1980, la satisfaction des besoins lointains de générations futures. C'est la satisfaction actuelle des besoins qui est maintenant compromise par les crises environnementales et sociales que connaît le . Il ne s'agit plus, selon cette critique, d'anticiper les problèmes, mais de les résoudre. Le développement durable pourrait alors laisser place à la notion de « développement désirable » qui regroupe l'ensemble des solutions économiquement viables aux problèmes environnementaux et sociaux que connaît la planète. Ce nouveau mode de développement, facteur de croissance économique et d'emplois, serait une véritable « économie verte », fondée sur l'économie sociale et solidaire, l'écoconception, le biodégradable, le bio, la dématérialisation, le réemploi-réparation-recyclage, les énergies renouvelables, le commerce équitable ou la relocalisation.


Voir ici : .









</doc>
<doc id="969" url="https://fr.wikipedia.org/wiki?curid=969" title="Direction régionale de l'environnement">
Direction régionale de l'environnement

En France, les directions régionales de l'environnement (DIREN) étaient des services déconcentrés de l'État français qui, sous l'autorité du préfet de région et des préfets de département, exerçaient certaines des attributions relevant du Ministère de l'Écologie, de l'Énergie, du Développement durable et de la Mer (MEEDDM).

Les 26 DIREN, une par régions françaises métropolitaines et d'outre-mer, ont été créées en 1991 à la suite de la fusion des délégations régionales à l'architecture et à l'environnement (DRAE), des services régionaux d'aménagement des eaux, des délégations de bassin et des services hydrologiques centralisateurs.

Entre 2009 et 2011, la réorganisation du MEEDDM a conduit à la création de nouvelles directions régionales : les directions régionales de l'environnement, de l'aménagement et du logement (DREAL) par fusion des directions régionales de l'équipement (DRE), des directions régionales de l'industrie, de la recherche et de l'environnement (DRIRE) et des DIREN dans chaque région.

Les missions des DIREN étaient les suivantes :




</doc>
<doc id="971" url="https://fr.wikipedia.org/wiki?curid=971" title="Deraeocoris">
Deraeocoris

Le genre Deraeocoris comprend des insectes prédateurs du sous-ordre des hétéroptères (punaises), de la famille des Miridae, sous-famille des Deraeocorinae, de la tribu des Deraeocorini.
Les larves et les adultes ont pour proies principalement les acariens, les psylles, les pucerons et les thrips sur les arbres fruitiers, la vigne et les cultures légumières.

Le genre "Deraeocoris" a été décrit par l'entomologiste allemand Carl Ludwig Kirschbaum en 1856.




</doc>
<doc id="975" url="https://fr.wikipedia.org/wiki?curid=975" title="Déclaration universelle des droits de l'homme">
Déclaration universelle des droits de l'homme

La Déclaration universelle des droits de l'homme (DUDH) est adoptée par l’Assemblée générale des Nations unies le à Paris au palais de Chaillot par la résolution 217 (III) A. Elle précise les droits fondamentaux de l'homme. Sans véritable portée juridique en tant que tel, ce texte n'a qu'une valeur d'une proclamation de droits.

50 États sur les 58 participants devaient adopter cette charte universelle. Aucun État ne s'est prononcé contre et seuls huit se sont abstenus. Parmi eux, l'Afrique du Sud de l’apartheid refuse l'affirmation au droit à l'égalité devant la loi sans distinction de naissance ou de race ; l’Arabie saoudite conteste l’égalité homme-femme. La Pologne, la Tchécoslovaquie, la Yougoslavie et l'Union soviétique (Russie, Ukraine, Biélorussie), s'abstiennent, quant à eux, en raison d'un différend concernant la définition du principe fondamental d’universalité tel qu'il est énoncé dans l’article 2 alinéa 1. Enfin, les deux derniers États n'ayant pas pris part au vote sont le Yémen et le Honduras.

Le texte énonce les droits fondamentaux de l’individu, leur reconnaissance, et leur respect par la loi. Il comprend aussi un préambule avec huit considérations reconnaissant la nécessité du respect inaliénable de droits fondamentaux de l'homme par tous les pays, nations et régimes politiques, et qui se conclut par l’annonce de son approbation et sa proclamation par l’Assemblée générale des Nations unies.

Le texte du préambule et de la déclaration est inamovible. Sa version en français, composée de 30 articles, est un original officiel, signé et approuvé par les membres fondateurs de l'Organisation des Nations unies, et non une traduction approuvée.

De 1946 à 1948, les délégués des Nations unies se sont consacrés à l'élaboration de la Déclaration. Créée en 1946 par le Conseil économique et social, la Commission nucléaire des droits de l’homme a fixé comme principal mandat de la nouvelle Commission des droits de l’homme l’élaboration d’une charte internationale.

Au début de l’année 1947, lors de sa première session, la Commission des droits de l’homme a établi un Comité de rédaction. Initialement composé de la présidente, Eleanor Roosevelt, du vice-président, P.C. Chang, et du rapporteur, Charles Malik, le Comité de rédaction sera élargi dans un second temps. Il se compose des membres suivants : 

Le Comité de rédaction se réunit pour sa première session du 9 au 25 juin 1947, puis pour une deuxième session du 3 au 21 mai 1948. Le projet de Déclaration rédigé par le Comité et transmis pour discussion à la Commission des droits de l’homme, puis au Conseil économique et social, et enfin à l’Assemblée générale. De nombreux amendements et propositions seront encore proposés par les États membres de l’ONU au sein de ces différents organes.

Les idées et valeurs des droits de l'homme peuvent être retracées au travers de l'histoire jusqu'aux périodes les plus reculées et dans les croyances religieuses et les cultures du monde entier . Certains affirment que la première déclaration des droits de l'homme connue serait celle transcrite sur le cylindre de Cyrus, rédigé par Cyrus le Grand, fondateur de l'Empire perse en l'année -539.

Certains auteurs, tels Norberto Bobbio, affirment que la Déclaration de 1948 trouve ses sources dans l'émergence du droit naturel, des théories du contrat social (en particulier celle de Locke) et dans l'individualisme qui aurait remplacé l'holisme des communautés antérieures. Il y aurait ainsi une filiation directe entre le jusnaturalisme de certaines philosophies du siècle des Lumières, et l'adoption de documents comme la Déclaration des droits anglaise, la Déclaration des Droits américaine et la Déclaration des droits de l'homme et du citoyen française. D'autres soulignent toutefois des divergences considérables entre les « théories contractualistes » (Hobbes, Locke et Rousseau — théories qui d'ailleurs divergent entre elles, Hobbes et Rousseau pouvant être assimilés au positivisme juridique) et la formulation de la Déclaration de 1789.

Lors de la Seconde Guerre mondiale, les alliés adoptèrent les « quatre libertés » : la liberté d'expression, la liberté de religion, la liberté de vivre à l'abri du besoin et la liberté de vivre à l'abri de la peur, comme leurs buts fondamentaux dans ce conflit. La Charte des Nations unies réaffirme la , et engage tous les États membres à promouvoir 

Lorsque les atrocités commises par l'Allemagne nazie furent connues, après la Seconde Guerre mondiale, le consensus au sein de la communauté internationale était que la Charte ne définissait pas suffisamment les droits auxquels elle faisait référence. Une déclaration précisant les droits des individus était nécessaire afin de renforcer les dispositions de la Charte sur les droits de l'homme.

Après avoir voté la Déclaration universelle des droits de l'homme, qui n'a, en tant que telle, qu'une valeur déclarative, et ne crée donc pas d'obligations juridiques, l'Assemblée générale a souhaité une Charte des droits de l'homme qui aurait force obligatoire. La Commission des droits de l'homme de l'ONU a été chargée de la rédiger. Après de longues négociations, le projet a abouti, dans le contexte de la guerre froide avec deux textes complémentaires : le Pacte international relatif aux droits économiques, sociaux et culturels et le Pacte international relatif aux droits civils et politiques.

Le Conseil constitutionnel français n'accorde pas de statut juridique positif à la Déclaration de 1948, bien que celle de 1789 soit intégrée au bloc de constitutionnalité depuis 1971.

Deux critiques principales s'adressent à cette Déclaration :

D'une part, celle qui concerne l'effectivité des droits de l'homme, et qui s'intéresse aux garanties juridiques positives, ou à l'absence de celles-ci. Cette critique a par exemple été formulée par Jeane Kirkpatrick, représentante permanente des États-Unis auprès des Nations unies de 1981 à 1985, qui mettait sur le même plan la Déclaration et la lettre au père Noël.

D'autre part, une autre critique porte sur l'universalité supposée de ces droits de l'homme. Celle-ci rejoint parfois celle-là, ainsi lorsque les pays du Sud dénoncent une application et un intérêt à géométrie variable pour les droits de l'homme, en fonction des pays, des puissances et des conflits. Dans ce dernier cas, ce n'est pas le principe de l'universalité des droits de l'homme qui est contesté en tant que tel, comme peuvent le faire les tenants d'un relativisme culturel radical, mais plutôt l'application différenciée supposée de ceux-ci.

Par ailleurs, les droits liés à la liberté de la presse et la protection des sources d'information des journalistes y sont moins développés que dans d'autres textes, comme la Convention européenne des droits de l'homme et son célèbre article 10.








</doc>
<doc id="976" url="https://fr.wikipedia.org/wiki?curid=976" title="Darius Milhaud">
Darius Milhaud

Darius Milhaud, né le à Marseille et mort à Genève le , est un compositeur français de musique classique.

Darius Milhaud est issu de l’une des plus vieilles familles juives de Provence, originaire du Comtat Venaissin. Cette région de Vaucluse abrite depuis des siècles de nombreuses familles juives surnommées les « Juifs du Pape », dont la famille Milhaud, particulièrement reconnue pour avoir engendré Joseph Milhaud, fondateur en 1840 de la synagogue d’Aix-en-Provence. Parmi les membres de sa famille, on compte également José de Bérys, Francine Bloch (qui l'invitera, en 1961, à devenir le premier président de la Société des amis de la Phonothèque nationale de France et établira sa phonographie), Marcel Dassault et Pierre Vidal-Naquet.
Darius Milhaud est l’unique fils d’un négociant en amandes et d’une mère née à Marseille. Ses parents sont musiciens amateurs. Son père fonde la Société Musicale d’Aix-en-Provence, et sa mère connaît bien les chants religieux. Darius montre des dons précoces, tout d’abord pour le violon et la composition. À 17 ans, en 1909, il va à Paris pour étudier au Conservatoire de Paris, jusqu’en 1915. Ses professeurs sont Gustave Leroux en harmonie, André Gedalge pour le contrepoint, Charles-Marie Widor pour la composition et surtout Paul Dukas pour l'orchestration. 

Ces années sont l’occasion de multiples rencontres sur le plan musical et littéraire : il se lie d’amitié avec les musiciens Georges Auric et Arthur Honegger, et avec le poète Léo Latil, tué en 1915 lors de la Première Guerre mondiale. Il fait également la connaissance de Francis Jammes et de Paul Claudel en 1912, auteurs dont il mettra les textes en musique. Sa rencontre avec André Gide exerce aussi une influence importante.

Atteint de rhumatismes, Darius Milhaud est réformé. Il compose dans ces années des musiques de scène, notamment sur la trilogie "Orestie" d’Eschyle, traduite par Claudel. Il recourt alors à la polytonalité, ce qui devra rester comme l’une des caractéristiques principales de sa musique. Cette amitié entre les deux artistes évolue dans le sens d’une collaboration : Claudel, nommé ministre plénipotentiaire à Rio de Janeiro, propose à Milhaud de devenir son secrétaire. Milhaud accepte. Il s’enthousiasme alors pour les musiques sud-américaines, qu’il insère dans les ballets "L'Homme et son désir" (1918-1921) et "Le Bœuf sur le toit" (1919-1920), ainsi que dans la suite de danses "Saudades do Brasil" (1920-1921).

De retour à Paris, il est associé par le critique Henri Collet au Groupe des Six, constitué de Georges Auric, Louis Durey, Arthur Honegger, Francis Poulenc et Germaine Tailleferre. Le mentor de toute cette équipe est l'écrivain et graphiste Jean Cocteau. Fort de cette association, avec laquelle il écrit notamment la musique des "Mariés de la Tour Eiffel" (1921), unique œuvre collective du Groupe des Six, sur un argument de Cocteau, Milhaud est également reconnu dans le milieu parisien pour ses œuvres de jeunesse imprégnées d’influences sud-américaines. 

Il officie en tant que chef d’orchestre, critique musical, ou même conférencier, et voyage abondamment, notamment à Londres en 1920, et aux États-Unis en 1922, où il découvre les rythmes du jazz qui vont profondément l’influencer pour son ballet "La Création du monde" (1923). Il continue à écrire plusieurs opéras sur des livrets de ses amis : "Le Pauvre Matelot" en 1926 sur un texte de Cocteau, et "Christophe Colomb" en 1930 sur un texte de Claudel. Il s’intéresse également au cinéma et compose pour le cinéma. Toutefois, ses compositions jouissent d’un succès mitigé, et son opéra "Maximilien" (1932) est accueilli fraîchement à l’Opéra Garnier. 

Parallèlement, sa vie sentimentale est comblée par son mariage avec Madeleine Milhaud, une cousine actrice, qui lui donna en 1930 un fils, Daniel, qui devint artiste-peintre (décédé à Pietrasanta en octobre 2014).

Sa production reste prolifique jusqu’au début de la Seconde Guerre mondiale, date à laquelle il doit fuir la France occupée, cumulant l'« inscription sur deux listes de proscription : comme juif et comme compositeur d'art dégénéré ». En 1940, il part pour les États-Unis, où le chef d'orchestre Pierre Monteux l'aide à trouver un poste de professeur de composition au Mills College d’Oakland (Californie). Milhaud y aura notamment comme élèves le pianiste de jazz Dave Brubeck, le compositeur de variétés Burt Bacharach, et les fondateurs du minimalisme américain Steve Reich et Philip Glass. 

Après la guerre, il retourne en France en 1947, et se voit offrir un poste de professeur de composition au Conservatoire de Paris, qui comptera parmi ses élèves de futurs talents tels Georges Delerue. Il alterne alors son activité de professeur entre Paris et les États-Unis, continuant à enseigner à Oakland jusqu'en 1971, ainsi qu'à l’Académie musicale d’été d’Aspen au Colorado et dans divers établissements américains. Malgré une santé de plus en plus fragile (des rhumatismes le font beaucoup souffrir), le compositeur reste donc un infatigable voyageur, même si son activité créatrice est ralentie. 

Sa carrière est couronnée en 1971 par un fauteuil à l’Académie des Beaux-Arts. Il s’éteint le à Genève, à l’âge de 81 ans. Selon ses souhaits, il est enterré au cimetière Saint-Pierre à Aix-en-Provence, sous une modeste pierre du carré juif. Sa femme, Madeleine Milhaud, lui survivra plus de trente ans. Elle est décédée le , dans sa , et est enterrée aux côtés de son mari, à Aix-en-Provence.

Il avait été membre du Comité de direction de l'Association du Foyer de l’Abbaye de Royaumont.

Darius Milhaud s’est intéressé à tous les genres musicaux : opéra, musique de chambre, musique symphonique, concertos, ballets, musique vocale. Il est l’un des compositeurs les plus prolifiques non seulement du , mais aussi de toute l’histoire de la musique. Son style, mélange de lyrisme et de gaieté, emprunte beaucoup aux musiques folkloriques, et au jazz, qu’il affectionne particulièrement pour ses rythmes syncopés. Milhaud explore toutes les possibilités de l’écriture : à la fois fin contrapuntiste, il utilise fréquemment la polyrythmie et la polytonalité, qui rendent son œuvre extrêmement riche et diverse.
Quant au Groupe des Six, il s’agit tout autant d’un canular de journaliste que d’un courant musical. Cette pseudo-école rassemblait des musiciens aux styles divers. Parrainée par Jean Cocteau et Erik Satie, elle prôna un retour à la musique légère, simple ou même comique. Parfois, le cirque n’est pas bien loin. D'ailleurs, la création du Bœuf sur le toit en 1920 s'est fait avec les frères Grimm sur scène. Georges Maurice expliquait ainsi ces choix esthétiques : « Ayant grandi au milieu de la débâcle wagnérienne et commencé d'écrire parmi les ruines du Debussy, imiter Debussy ne me paraît plus aujourd'hui que la pire forme de la nécrophagie. » (revue "le Coq et l’Arlequin"). Le bœuf sur le toit est son œuvre la plus populaire.

Plus généralement, cet après-guerre est l'époque du rejet, dans l'art et la littérature, de certains styles incroyablement "cuisinés" et/ou luxuriants, en usage jusque-là. Dans ces Années folles, la simplicité, parfois proche de l'art populaire ou du cabaret, s'impose facilement, parallèlement à l'apparition du surréalisme. Le Bœuf sur le toit en est une manifestation.

Les opéras sont au nombre de seize, dont trois opéras minute (environ 15 minutes chacun) :
Deux opéras d'une durée courte (~ 30 minutes)
Autres opéras :


Au nombre de 14, dont : 

Milhaud attend 1939 pour entamer l’écriture de symphonies. Elles seront au nombre de douze entre 1939 et 1960. Il écrit également des suites de danses, et une variété de concertos, pour piano, violon, violoncelle, alto, etc.

Concertos :

La production de musique de chambre de Milhaud est tout aussi prolifique : pas moins de dix-huit quatuors à cordes, des quintettes et des suites pour vents, des sonates, des duos, et bien d’autres pièces encore figurent au catalogue de l’artiste. 

Milhaud a grandement contribué à élargir le répertoire vocal, autant pour voix solo que pour chœur. Les textes mis en musique sont extrêmement divers, provenant aussi bien d’écrivains comme André Gide que du Pape Jean XXIII, dont l'encyclique « Pacem in Terris » de 1963 sera mise en musique par le compositeur. C’est en effet dans la musique vocale que la religion prend une place importante chez Milhaud. C’est là qu’il renoue avec la religion qui est la sienne, le judaïsme. La toute dernière œuvre de Milhaud, qu’il compose l’année de sa mort, est en effet une cantate « Ani Maamin », fondée sur un texte d’Élie Wiesel, déporté à l’âge de quinze ans à Auschwitz. Les questions religieuses deviennent alors existentielles, et confinent à la philosophie.


Darius Milhaud n'a - apparemment - jamais joué d'orgue, mais trouvait l'instrument en soi intéressant, pas seulement pour la multiplicité de ses plans sonores mais surtout pour la grande variété de ses timbres/sonorités.







</doc>
<doc id="979" url="https://fr.wikipedia.org/wiki?curid=979" title="Daishō">
Daishō

Le est un terme japonais désignant la paire d'épées traditionnelles portée par les samouraïs de l'ère féodale.

Les deux armes composant le "daishō" sont le et le , le premier étant le plus long, il correspond à l'arme d'attaque, tandis que l'autre s'apparente plutôt à une arme de parade. Ainsi c'est ce qui explique l'étymologie même du mot qui provient des termes et : "daitō" associé à "shōtō" donne "daishō".
Cependant, à l'origine, le "daishō" désignait le port de n'importe quels "uchigatana" longs et courts réunis, et non spécifiquement celui d'un "katana" et d'un "wakizashi". 
Il arrive aussi que l'on considère une paire de "tantō" comme un "daishō", et finalement le terme "daishō" désigne parfois tout simplement un attirail de deux épées à peu près identiques.

Pendant la période des samouraïs, les lames du "daishō" pouvaient être similaires car réalisées par le même artisan, mais cette pratique était relativement rare car elle était plus chère, et en outre la concordance des deux épées n'avait à l'époque pas une grande importance. 

Le concept du "daishō" est né au cours de la période Muromachi (1336-1573), durant laquelle il arrivait de croiser l'association d'une épée courte avec une autre de n'importe quelle longueur. À une certaine époque, le "tachi" et le "tantō" auraient été maniés ensemble, tout comme plus tard deux "uchigatana" de tailles différentes. Avec l'apparition du "katana", le "wakizashi" a finalement été choisi par les samouraïs pour remplacer le "tantō". Dans son livre intitulé "The Japanese Sword", Kanzan Satō note qu'il ne semblait pas y avoir de besoin particulier en ce qui concerne le "wakazaishi", et suggère que ce dernier est sans doute devenu plus populaire que le "tantō" parce qu'il était plus adapté aux combats en intérieur. 

Selon la plupart des écoles traditionnelles de "kenjutsu", qui font partie des "koryū", seule une seule des deux lames du "daishō" était utilisée pour combattre. Cependant, durant la première moitié du , le célèbre escrimeur Miyamoto Musashi favorisa l'utilisation de la prise à une main qui permettait de manier deux épées simultanément. Cette technique, appelée , constitue un des éléments de base du style de Hyoho Niten Ichi Ryu, école enseignant l'art de manier l'épée fondée par Musashi. 

Suite à la "chasse aux épées" ordonnée par Toyotomi Hideyoshi en 1588, le port du "daishō" est exclusivement réservé à la classe des samouraïs et devient ainsi un symbole de leur rang. Le "daishō" est sans doute devenu populaire durant les dernières années de la période Muromachi puisque les premiers exemples de son utilisation datent de la fin du . Par la suite en 1629, un édit définissant les droits des samouraïs est instauré et exige le port du "daishō". Durant l'ère Meiji un second édit rédigé en 1871 annule le premier, et en 1876 le port de l'épée en public est banni pour la plupart de la population japonaise, ce qui en fera définitivement le symbole des samouraïs. La fin de l'époque d'Edo voit arriver l'interdiction des épées et par conséquent la disparition de la classe des samouraïs.

Lorsque le "katana" est sur son présentoir, il est placé :
Le plus souvent, seule la « monture » du sabre est exposée ainsi ("tsuka", "tsuba" et "saya", maintenus ensemble par une lame en bois). En effet la lame est souvent rangée dans une monture de protection hermétique en bois blanc dite de "shirasaya" (qui ne sont pas destinées au combat).

En temps de paix, le "katana" se pose sur le présentoir, la "tsuka" côté gauche, alors qu'en temps de guerre, la "tsuka" est à droite, ceci afin de permettre une sortie plus rapide du "katana" en cas de danger.




</doc>
<doc id="980" url="https://fr.wikipedia.org/wiki?curid=980" title="Dolmen">
Dolmen

Un dolmen est une construction mégalithique préhistorique constituée d'une ou plusieurs grosses dalles de couverture (tables) posées sur des pierres verticales qui lui servent de pieds (les orthostates). Le tout était originellement recouvert, maintenu et protégé par un amas de pierres et de terre nommé tumulus. Les dolmens sont généralement interprétés comme des tombes à chambre, des monuments funéraires ayant abrité des sépultures collectives .

Les dolmens européens ont été construits entre la fin du et la fin du , ceux d'Extrême-Orient au .

Pour certains chercheurs, à côté de ces mégalithes, leurs équivalents en bois appelés, faute de terme créé pour les désigner, "dolmens en bois", pourraient avoir existé.

Dans leur état actuel de dégradation, les dolmens se présentent souvent sous l'apparence de simples tables. Ils ont longtemps pu faire penser à des autels païens, mais il s'agit bien de chambres sépulcrales et de galeries de tumulus (buttes artificielles), dont la partie meuble (remblai) a été érodée au cours des siècles. Leur architecture comporte parfois un couloir d'accès qui peut être construit en dalles ou en pierres sèches. La chambre sépulcrale, aux formes variables (rectangulaire, polygonale, ovale, circulaire...), peut aussi être précédée d'une antichambre. Dans certains dolmens, l'entrée présente une porte taillée dans une ou plusieurs dalles verticales.

La morphologie des dolmens peut varier en fonction des régions ; ainsi observe-t-on, par exemple en Loire-Atlantique, des dolmens dont le couloir central dessert plusieurs chambres, de part et d'autre, formant ainsi un ou deux transepts et compliquant notablement le plan de la sépulture.

En Bretagne, en région parisienne et dans d'autres pays, dans certains dolmens démesurément longs, la chambre et le couloir ont la même largeur et se confondent. Ils sont recouverts de plusieurs tables et sont appelés « allées couvertes ». La complexité et l'importance des monuments peuvent être telles que certains tumuli recouvrent plusieurs dolmens, comme le grand cairn de Barnenez (Finistère, France) qui couvre onze sépultures à couloir, les unes mégalithiques, et d'autres avec voûtes de pierres sèches, en encorbellement…

À l'opposé, la région des Cévennes est riche en tombes du genre coffre, souvent en dalles de schiste et pierres sèches, sans couloir, et sous un cairn assez bas, parfois réunis en nombre dans une nécropole de crête.

Les dolmens de plan simple (sans couloir) abondent dans tout le sud de la France avec plusieurs milliers d'unités.

Il semble que Théophile Malo Corret de La Tour d'Auvergne soit le premier à avoir utilisé le terme « dolmen », dans son ouvrage "Origines gauloises. Celles des plus anciens peuples de l’Europe, puisées dans leur vraie source ou recherche sur la langue, l’origine et les antiquités des Celto-Bretons de l’Armorique, pour servir à l’histoire ancienne et moderne de ce peuple et à celle des Français", publié entre 1792 et 1796. Le terme « dolmen » est repris par Pierre Jean-Baptiste Legrand d'Aussy (1737-1800) qui propose une interprétation différente de la fonction du dolmen, en y voyant, non plus une table de sacrifice ou un autel comme le pensait Malo Corret, mais bien une sépulture.

Le , Legrand d’Aussy fait, à l’Institut National des Sciences et Arts, une lecture de son ouvrage, "Des sépultures nationales", publié par la suite en 1824 :

Le terme semblerait forgé à partir des mots bretons : "t(d)aol" (apparenté au latin "tabula"), « table », et "men", « pierre ». Cependant, on dit généralement "« liac’h ven" », « "liaven" », « "lieven" » ou « "leven" » dans les composés.

Certains dictionnaires étymologiques avancent que ce terme aurait été forgé outre-manche, à partir du cornique "tolmen", qui aurait désigné à l’origine un cercle de pierres ou une pierre trouée.

Les "dolmens simples" sont composés d'une table, de deux à trois orthostates et d'une dalle de chevet. La chambre ainsi définie est de forme rectangulaire (dolmen dit de «type A») ou polygonale (dolmen dit de «type B»). Ce type de dolmens est très répandu dans le sud-ouest (Aveyron, Lot) et le centre (Puy-de-Dôme) de la France. 

Les "dolmens à couloir" sont des dolmens où l'entrée de la chambre est précédée d'un couloir composé de grandes dalles. Ce type de construction se rencontre souvent en Bretagne sud (Morbihan, Loire-Atlantique). Les dolmens à couloir sont parfois réunis sous un même tumulus (Barnenez). Les dalles sont souvent gravées comme à Gavrinis ou Locmariaquer).

Les "dolmens à chambre compartimentée" comme à la Nécropole mégalithique de la Pointe du Souc'h.

Les "dolmens en V" se caractérisent par une chambre trapézoïdale raccordée sans rupture au couloir d'accès.

Les "dolmens à cabinets latéraux" comportent de petites cellules adossées à la chambre.

Les "dolmens transeptés" se caractérisent par un couloir conduisant à une chambre terminale précédées d'un double jeu de chambres latérales dessinant une croix de Lorraine. les plux beaux exemples sont visibles à Pornic (Tumulus des Mousseaux, Dolmen de la Joselière).

Les "dolmens coudés", où la chambre et le couloir dessinent une équerre, se rencontrent fréquemment dans le Morbihan.

Le "dolmen angevin", ou dolmen à portique, est une construction de taille monumentale composée d'une grande chambre précédée d'une antichambre surbaissée (La Roche-aux-Fées).

Le "dolmen angoumoisin", à chambre carrée, ou rectangulaire.

Le "dolmen languedocien", orienté à l'ouest, ou au sud-ouest.

Les allées couvertes très fréquentes dans les Côtes-d'Armor, emblématiques de la culture Seine-Oise-Marne et que l'on retrouve jusqu'en Provence.

Les dolmens étaient des sépultures collectives à caractère réutilisable. Cela explique que, dans certains dolmens, on ait pu découvrir les restes humains de plusieurs centaines d'individus et du mobilier de périodes différentes (Néolithique, âge du cuivre, du bronze, du fer, ou même périodes plus tardives). Un peu à l'image de nos caveaux familiaux, les dolmens pouvaient servir bien plus longtemps qu'aujourd'hui, et il est sûr que certaines tombes ont dû servir durant des siècles. Les ossements pouvaient être superposés en plusieurs couches et, pour faire de la place, subir une réduction ou une évacuation dans les couloirs.

L'expression « sépulture collective » n'implique pas forcément qu'il s'agisse d'un tombeau pour tous : au vu de la quantité d'ossements parfois assez faible découverte dans des sépultures de grande taille , on se demande si certaines n'étaient pas réservées à un groupe de privilégiés de la communauté.

L'interprétation, comme tombeau, ne doit peut-être pas être généralisée. Certains dolmens n'ont pas livré de restes humains de type sépulcral, mais cela peut être une conséquence de phénomènes taphonomiques, de l'érosion, de pillages, de fouilles anciennes peu méthodiques, ou de fouilles clandestines. Lors de son ouverture, le dolmen sous tumulus de Mané-er-Hroeh, à Locmariaquer ne contenait pas de restes humains.

Quant au tumulus, il n'avait pas qu'une utilité protectrice de la chambre funéraire, mais sans doute aussi une fonction de signalisation, voire d'ostentation : un grand tumulus, parementé, imposait sa masse au visiteur, devait inspirer le respect du lieu et conférer un prestige certain à la communauté qui l'avait érigé.

Par ailleurs, plusieurs trouvailles archéologiques (offrandes, autel, allées, etc.) font penser que ces monuments funéraires ont pu avoir une fonction religieuse. Même bien après la grande période d'érection des mégalithes en Europe, les peuples celtes les ont, semble-t-il, parfois utilisés à des fins religieuses, mais n'en sont pas pour autant les constructeurs, comme l'affirmèrent les premiers chercheurs celtomanes des et , qui rattachaient systématiquement les mégalithes aux Gaulois et aux Bretons.

Cinquante mille dolmens auraient été recensés dans le monde, dont vingt mille en Europe.
Ils étaient très nombreux dans certaines régions de France et, si certains ont disparu, il en reste plus de , disséminés dans une soixantaine de départements. Pour schématiser l'implantation des dolmens en France, on peut partir de l'ouest du pays, avec la Bretagne, puis en descendant par le Poitou, pour ensuite rejoindre, plus au sud, les causses du Quercy et de l'Aveyron et, enfin, arriver en bord de mer Méditerranée, au Languedoc (voir carte), et en Roussillon ( Campoussy - Arboussols, etc.). Ils sont nombreux en Aveyron (, Bretagne, Quercy (), Ardèche ( dans ce seul département), Poitou-Charentes et le Languedoc-Roussillon (au moins ). La Provence en compte une centaine.

On en trouve aussi en Irlande, au Pays de Galles avec, notamment, les ' ou ' (Tombe à couloir), dans les comtés anglais du Devon et de Cornouailles. Au Portugal, on recense les sites spectaculaires du Haut-Alentejo, près de la ville d'Evora. Dans le sud de l'Espagne, les sites remarquables d'Antequera, qui comptent parmi les dolmens les plus imposants et les plus anciens au monde.

En Belgique, où l'on recense 120 vestiges de dolmens et de menhirs, tels ceux d'Oppagne et de Barvaux-sur-Ourthe, les mégalithes du domaine de Wéris, près de Durbuy (dont le dolmen de Weris et les menhirs et cromlechs qui ont subsisté dans la même région), ainsi que des sépultures en grotte sous rocher, les Blancs Cailloux de Mousny, les pierres levées de Neerwinden et de Manderfeld, la tombelle de Tourinnes-Saint-Lambert, et jusque dans une commune bruxelloise, avec le Tomberg, tumulus détruit au , mais dont il existe des traces de l'inventaire des objets qu'il contenait.

En Scandinavie, en Allemagne du Nord et aux Pays-Bas, ces vestiges sont appelés "hunebed" ou "hunegraf". L'Afrique du Nord et l'Inde contiennent de tels vestiges et, plus modestement, la Syrie, l'Éthiopie et la Crimée (Ukraine). En Tunisie, la nécropole à dolmens du Djebel Gorra, située près de la petite ville de Thibar, sur la route qui mène à Téboursouk, présente deux à trois cents sépultures mégalithiques bien reconnaissables.

Sur le continent américain, le livre-référence très détaillé, ""Les découvertes des Amériques avant Colomb"" de Hans-Joachim Zillmer, détaille les structures mégalithiques telles que les dolmens, menhirs, cromlechs, cairns, et autres tumulus. Celles-ci sont nombreuses sur le territoire nord-américain, principalement dans le Nord-Est, Massachusetts, New Jersey, New-York, Connecticut, et les états côtiers. Des photos d'archives, souvent en noir et blanc, figurent dans l'ouvrage. En Amérique du Sud, ces structures sont aussi présentes, par exemple en Argentine.





</doc>
<doc id="982" url="https://fr.wikipedia.org/wiki?curid=982" title="Dominique (pays)">
Dominique (pays)

La Dominique, en anglais "", est un pays et une île de l'archipel des Caraïbes, située entre les îles françaises des Saintes et de Marie-Galante (deux dépendances de la Guadeloupe) au nord, et de la Martinique, au sud. Son nom précolombien est "Wai'tu kubuli" qui signifie « Son corps est grand ». 

Le premier Européen à l'avoir abordée est Christophe Colomb lors de son deuxième voyage en 1493. Avant son indépendance en 1978, la Dominique était un État associé de la couronne britannique () et, avant 1967, une colonie britannique membre de l'éphémère fédération des Indes occidentales (1958-1962). Après avoir renoncé à ce territoire par le traité de Paris (1763), la France occupera, passagèrement, deux fois l'île par la suite.

L’île avait été initialement peuplée par des Indiens arawaks, puis par des Caraïbes.
Le dimanche , lors de son deuxième voyage aux Amériques, Christophe Colomb longe les rivages de l’île qu'il appelle ainsi "Domingo" , d’où proviennent ses noms actuels, Dominique, en français et "Dominica", en anglais. 

Les Indiens caraïbes doivent leur survie aux reliefs escarpés de la Dominique, ses forêts denses et sauvages. Venus du nord du Venezuela, ils s'étaient installés sur l'île bien avant que Christophe Colomb ne la découvre. Mais c'est ici seulement, cachés dans la nature, qu'ils ont échappé à l'extermination. En 1903, la Couronne britannique leur concéda quelques terres en propriété.

En 1625, lors de la guerre de Trente Ans, les Espagnols laissent la place aux Français puis au cours du , Français et Anglais s’affrontent pour gouverner l'île. Deux fois leurs canonnades détruiront totalement Roseau. En 1660, Français et Anglais abandonnent l’île aux Caraïbes et la déclarent zone neutre ; pour mettre fin aux conflits, un traité de paix est signé entre les Français, les Anglais et les Caraïbes.

Déjà installés à la Martinique et à la Guadeloupe, les Français s'implantent petit à petit à la Dominique en y introduisant la culture du café. Ils importent des esclaves africains pour combler la main-d'œuvre dont ils ont besoin. Mais les Britanniques s'approprient de nouveau l'île en 1759. À l'issue de la guerre de Sept Ans, par le traité de Paris (1763), la France cède la Dominique à l’Angleterre ; mais entre 1772 et 1814, les Français rompent le traité et s’emparent par deux fois de la Dominique. En 1814, après une dernière tentative de reconquête en incendiant de nouveau Roseau, les Français abandonnent définitivement l’île en échange d’une indemnité et cette dernière redevient britannique.

L'esclavage est aboli à la Dominique en 1833. Comme il ne le fut qu'en 1848 dans les îles voisines de la Martinique et de la Guadeloupe, de nombreux esclaves s'enfuirent de ces îles pendant cette période, à l'aide de moyens de fortune, pour essayer de trouver refuge à la Dominique. 

En 1898, l'île reçoit le statut de Colonie de la Couronne britannique. En 1956, elle acquiert son indépendance au sein de l'éphémère Fédération des Antilles britanniques et, en 1967, elle devient État associé au Commonwealth et entame l’instauration d’un régime démocratique. L’indépendance de la Dominique est déclarée le 3 novembre 1978, lors du de sa découverte par Christophe Colomb.

Aujourd'hui, les descendants des Indiens Caraïbes, derniers héritiers de ces peuples précolombiens, vivent pour la plupart dans l'Indian Carib Reserve, de hectares, autour de la petite ville de Salybia, au nord-est de l'île. Malgré les métissages, ils revendiquent leur identité.

Le 19 septembre 2017, suite au passage de l'ouragan Maria, le premier ministre Roosevelt Skerrit déclare : .

La Dominique est une république démocratique qui combine des aspects du modèle républicain et du « système de Westminster ». Le Président est élu par le parlement pour un mandat de cinq ans (art.18 de la Constitution). En accord avec l'article 59 de la Constitution, il choisit comme Premier ministre un député qui a l'appui d'une majorité au sein du Parlement.

Le Commonwealth de la Dominique est membre du Caricom, de l'AEC, de l'OECO, du Commonwealth, de l'OEA, de l'ALBA, de l'ONU, et de la Francophonie.

Le Président actuel de l'île est Son excellence l'Honorable Charles Angelo Savarin. Le Premier ministre pour sa part est actuellement Roosevelt Skerrit.

L’île de la Dominique est située en plein cœur des petites Antilles, à au nord-nord-ouest de la Martinique, à au sud-est des Îles des Saintes et autant au sud-sud-ouest de Marie-Galante, ces dernières constituant deux des dépendances de la Guadeloupe. Elle mesure de longueur, sur de largeur, soit une superficie de .
L'île est composée d'une chaîne de hauts pitons depuis son extrémité septentrionale à sa pointe méridionale ; le plus élevé, le Morne Diablotins, culmine à . 

La Dominique jouit d’un climat tropical avec des pluies abondantes qui alimentent les chutes d’eau. Il y a environ trente chutes d’eau formant des piscines naturelles, des sources d’eaux chaudes, 365 rivières et dont la "Rain Forest" (forêt tropicale humide). Le parc national de Morne Trois Pitons est classé au patrimoine mondial naturel par l’UNESCO.

L'île témoigne d'un volcanisme de type récent, d'intense activité, comme l'attestent les sites du "", lac en ébullition, et de la « Vallée de la Désolation ». Cette dernière est constituée de , contrastant ainsi avec les forêts tropicales environnantes.

Les habitants de l’île, au nombre de , sont concentrés essentiellement sur la côte ouest, à Roseau, la capitale, forte de habitants, et à Portsmouth ( en 2006), au nord. Il demeure encore indiens, préservant leurs traditions, sur la côte est.


Les richesses écologiques de l'île ont été affectées par le développement de l'agriculture et des bananeraies, ainsi que par l'introduction de nombreuses espèces exogènes, devenant parfois invasives.

Après une économie basée sur l'agriculture et l'exportation de bananes, qui a rendu l'île vulnérable aux catastrophes climatiques et aux crises du marché, la Dominique a souhaité développer un programme d'écotourisme, récompensé par la certification "Green Globe 21" validant la qualité écotouristique de cette destination, attribuée à une île des Caraïbes. La Dominique veut aller plus loin avec, depuis 2007, un programme de dix ans visant à transformer l'île en une « île biologique » par la conjugaison de l’écotourisme, de l’agrotourisme et d'un tourisme de santé, avec la conversion de l'agriculture à la production biologique, un commerce éthique et équitable ne nécessitant pas de consommation excessive des ressources naturelles.

L'« île nature » a ouvert en janvier 2011 un sentier de randonnée, inédit dans la Caraïbe, le "Waitukubuli National Trail" (WNT). Long de , partagé en quatorze segments, il traverse le territoire du sud au nord en reprenant les chemins tracés par les anciens habitants, explique Edison Henry, le chef du projet. La flore typique comprend manguiers, corossols et gommiers. La faune typique est représentée par le « sisserou » (Amazone impériale), un grand perroquet au ventre pourpre et aux ailes vertes, unique au monde, emblème national de la Dominique. 

Certaines rivières débouchent sur des chutes d'eau (Victoria, Sari Sari, Middelham…). La Dominique abrite le deuxième plus grand lac bouillonnant de la planète, au cœur du parc national Morne Trois Pitons, classé au patrimoine mondial.

La Dominique est divisée en 10 paroisses.

L'économie dominiquaise dépend surtout du tourisme et de l'agriculture. En effet, l'agriculture, principalement la banane, représente 18 % du PIB et emploie 28 % de la main-d'œuvre. Les services (dont le tourisme) représentaient 58 % du PIB et 40 % de la main-d'œuvre en 2002. Des réformes ont été entreprises afin de développer les services financiers off-shore à l'instar d'autres îles de la région. C'est également un pavillon de complaisance.

À partir de 2003, le Gouvernement de la Dominique, les Régions Guadeloupe et Martinique, l'Agence française de développement (AFD), l'Agence de l'environnement et de la maîtrise de l'énergie (ADEME) et le Bureau de recherches géologiques et minières (BRGM) ont envisagé de conduire en coopération un projet de développement des ressources géothermales de la Dominique. Il s'agirait d’exporter l'essentiel de la production électrique via des câbles sous-marins vers les deux îles françaises voisines (Guadeloupe et Martinique) qui constituent deux pôles de consommation électrique en forte croissance dans la Caraïbe.

En 2005, une étude préliminaire de cadrage technique et économique a eu lieu entre la Dominique et EDF pour la France, mais aussi plusieurs intervenants économiques. À partir de 2013, une nouvelle phase s'est ouverte avec le forage des premiers puits. Cette phase de préfiguration de la production doit aboutir à l'évaluation de la production et, par la suite, la mise en place d'une centrale de production.

Les prévisions économiques de 2018 menées par The Economist font de la Dominique le pays à plus forte croissance du PIB par rapport à l'année précédente, avec une progression de 8,8%.

La population de la Dominique était de personnes en 2009. À plus de 85 % elle est composée de Noirs pour un faible pourcentage de 1 % de Blancs, surtout des Britanniques et des Américains. La démographie de l'île croît peu, du fait de l'exode de la population vers d'autres pays. La population est très majoritairement catholique (70 %) et on retrouve environ 1 % d'anglicans. On note également la présence de la dernière population indigène des Antilles, les Caraïbes, comptant personnes. Ces derniers vivent aujourd'hui sur une réserve créée spécialement pour eux en 1903, le Territoire Kalinago.

Bien que la langue officielle du pays soit l'anglais, 80 % des citoyens s'expriment en Créole dominiquais, créole à base lexicale française. D'après les derniers recensements de 2014 (OIF) 10 % de la population parle en tant que langue principale le français ( locuteurs). La Dominique est devenue membre de l'Organisation internationale de la Francophonie en décembre 1979.

Selon le Pew Research Center, en 2010, 94,4 % des habitants de la Dominique sont chrétiens, principalement catholiques (58,1 %) et dans une bien moindre mesure protestants (35,5 %). De plus, 3,5 % de la population pratiquent une religion populaire.



La Dominique a pour codes :



</doc>
<doc id="983" url="https://fr.wikipedia.org/wiki?curid=983" title="Département">
Département

Un département peut être une circonscription administrative, ou une division d'un ensemble administratif plus grand.

Le terme de « département » apparaît pour la première fois au en France dans un projet de découpage territorial du Royaume soumis au roi Louis XIV en 1665 par Marc-René d'Argenson. Ce terme est alors entendu en tant que répartition fiscale ou circonscription territoriale pour les Ponts et Chaussées. En 1787, les assemblées régionales d'Ile-de-France sont convoquées par « département » afin de clarifier les échanges.

Un découpage en entités similaires du territoire semble un atout pratique pour l'administration. Ainsi l'on retrouve cette demande dans les cahiers de doléances de 1788 qui souhaitent la formation de circonscriptions uniformes avec un chef-lieu accessible. Le carnet du Puy-en-Velay parle explicitement de "département"

Le décret du 22 décembre 1789 pris par l'Assemblée constituante de 1789 consacrera la création des départements français. Leur nombre exact et leurs limites seront fixés le , et leur mise en place prendra effet le .

Des pays ont utilisé anciennement cette division administrative :
Aujourd'hui, un certain nombre de pays utilisent ce terme pour nommer une division administrative de leur territoire. Il s'agit, soit d'anciennes colonies françaises, soit de pays d'Amérique Latine :
Le terme de « département » désigne également dans certains pays les secteurs ministériels d'un gouvernement. Ainsi : 



</doc>
<doc id="984" url="https://fr.wikipedia.org/wiki?curid=984" title="Dictionnaire">
Dictionnaire

Un dictionnaire est un ouvrage de référence contenant un ensemble des mots d’une langue ou d’un domaine d’activité généralement présentés par ordre alphabétique et fournissant pour chacun une définition, une explication ou une correspondance (synonyme, antonyme, cooccurrence, traduction, étymologie).

Le présent article concerne les dictionnaires unilingues qui décrivent ou normalisent une langue. Ceux-ci sont à distinguer d'autres types d'ouvrages de référence : les encyclopédies ou dictionnaire de choses; les dictionnaires de traduction bilingues; les dictionnaires des synonymes; les dictionnaires thématiques spécialisés (dictionnaire du droit, du commerce, dictionnaire de géographie, dictionnaire humoristique, etc.)

Le substantif masculin "dictionnaire" est un emprunt au latin médiéval ', dérivé du latin '. D'abord écrit avec un seul "n", il est dérivé du latin "dictio" : « action de dire, propos, mode d'expression ».

Sa première utilisation remonte à Jean de Garlande dont le "Dictionarius cum commento" paraît en 1220.

"Dictionnaire" est attesté au : d'après le Trésor de la langue française informatisé, sa plus ancienne occurrence connue se trouve dans "le Jardin de plaisance et fleur de rhétorique".

Les auteurs d'un dictionnaire doivent déterminer au départ les catégories de mots à retenir, en fonction des limites imposées par l'éditeur et du public visé. Il faut décider de la place à faire aux néologismes, aux termes rares ou archaïques, au vocabulaire scientifique et technique, aux mots d'un emploi purement régional, au vocabulaire d'origine étrangère, aux mots grossiers et au vocabulaire populaire et argotique.

Une entrée comprend normalement : (a) la lexie, ou plus petite unité porteuse de signification, ses dérivés affixaux et ses composés (pomme, pommier, pomme de terre); (b) les morphèmes grammaticaux, c’est-à-dire les mots vides qui indiquent les rapports entre les mots pleins, porteurs de signification ou sémantèmes; (c) la prononciation; (d) les marques d'usage; (e) des exemples.

Les renseignements linguistiques sont de trois ordres :

Un dictionnaire doit d'abord donner la définition du mot. Cette opération, bien plus complexe qu'elle n'en a l'air, est . Elle occupe les logiciens depuis des siècles et est également étudiée par la linguistique, la sémiotique et la psycho-sociologie. Selon la méthode fondée par Aristote, définir consiste à découvrir les attributs essentiels, en identifiant les différences et en remontant, par paliers successifs, à la catégorie supérieure. Ainsi, on définirait l’acception principale du mot "chien" comme un animal de la classe des mammifères, ordre des carnivores et famille des canidés. En procédant ainsi, il faut évidemment veiller à ne pas empiéter sur le sens d'autres mots. En théorie, selon cette méthode, les divers objets du monde pourraient s'emboîter dans un arbre binaire, mais cela n'est valide que pour les objets mathématiques, le langage humain comportant un espace de « jeu » essentiel à la compréhension.

Dans la pratique, les définitions incorporent aussi des propriétés non essentielles, mais qui aident le lecteur à identifier ce dont il est question. Ainsi, une définition de "chien" va inclure que l’animal peut servir comme chien de garde, de chasse, de trait, etc. Ces notations sont de nature encyclopédique, tout comme le fait qu'il aime ronger un os. De nombreux dictionnaires intègrent ces données encyclopédiques au moyen d'exemples.

Il est rare qu'une seule définition épuise tous les sens d'un mot. Le plus souvent, un mot va avoir plusieurs acceptions, c'est-à-dire plusieurs significations, phénomène que l'on désigne par le terme de polysémie. Dans certains cas, un mot peut même désigner deux réalités opposées, comme le mot « hôte » qui peut signifier, selon le contexte, la personne qui accueille ou celle qui est accueillie. Souvent, la différence de sens provient d'un emploi figuré plutôt que littéral ou des déplacements de sens d'un domaine d'activité à un autre. Ainsi, le sens du mot « fuite » varie selon qu'il est utilisé en droit, en peinture, en aéronautique, en économie, en plomberie ou en politique. Un dictionnaire doit non seulement identifier les divers sens du mot, mais encore les classer d'une façon aussi cohérente et significative que possible. Il peut également comporter un répertoire indexé pour en faciliter l'utilisation.

Les exemples sont apparus en français avec le dictionnaire de Richelet, en 1680 (voir ci-dessous). Ils ont une triple utilité :


L'étymologie est apparue dans les dictionnaires français avec "Origines de la langue française" (1650), de Ménage, qui « découvrit seul, et de manière intuitive, l'origine d'un grand nombre de mots français ». Les bases d'une étymologie scientifique ont été posées par le philologue allemand Friedrich Christian Diez (1794-1876). Le domaine est maintenant couvert par l'ouvrage monumental de Walther von Wartburg (1888-1971), grâce auquel « nous disposons d'informations indiscutables, dans presque tous les cas, sur l'étymologie des mots français ».

L'histoire du mot est souvent plus instructive que l'étymologie, car elle permet de voir l'évolution des significations au fil des siècles, mais ces données sont souvent très fragmentaires dans les dictionnaires courants.

La datation est également une donnée intéressante, qui indique la date à laquelle un mot a été employé en français pour la première fois dans un texte.

Des indications sur la prononciation des mots sont devenues courantes avec le Dictionnaire de la langue française (1863) de Littré. Divers procédés de transcription phonétique ont été utilisés, avec plus ou moins de bonheur, par divers dictionnaires. En 1964, le Petit Robert a adopté l'A.P.I. ou alphabet phonétique international, qui, en plus d'être standardisé à travers les dictionnaires de différentes langues, présente trois avantages :

La prononciation n'est pas homogène, mais varie selon les régions et les groupes sociaux. Des mots comme "sculpteur" et "oignon" possèdent des lettres que les locuteurs cultivés ne prononcent pas, mais le cas de "dompteur" est moins clair, les deux formes étant en usage : et . L'auteur d'un dictionnaire doit donc déterminer la forme recommandée en se basant sur la prononciation la plus acceptée, qui n'est pas nécessairement la plus répandue. Ces questions complexes, qui touchent à la norme dans ce qu'elle a de plus intime et de moins conscient, ont justifié la rédaction d'ouvrages spécialisés, tel le "Dictionnaire de la prononciation française dans sa norme actuelle" (1964) de Léon Warnant.

Les dictionnaires électroniques modernes proposent un modèle de prononciation sonore, comme dans le wiktionnaire.

Il ne serait pas efficace, pour un dictionnaire de langue, de retenir toutes les formes fléchies des mots, car cela amènerait un fort taux de répétition. Si certains mots ont une forme unique, tels les adverbes, beaucoup d'autres, en effet, existent sous diverses formes, selon qu'ils sont au singulier ou au pluriel, au masculin ou au féminin, ou s'ils sont des verbes aux formes conjuguées. Pour résoudre ce problème, on recourt à une opération de lemmatisation, qui consiste à regrouper les formes occurrentes d’un mot sous une même adresse lexicale. Si cette opération peut paraître à première vue assez simple, elle se trouve rapidement compliquée par les variations orthographiques survenues au fil du temps, voire par la présence, au sein d’une langue évoluée, de divers homographies. On peut s’en faire une idée en consultant un dictionnaire historique de langue ou un dictionnaire étymologique.

Le classement alphabétique, qui nous paraît aujourd'hui normal et caractéristique des dictionnaires, n'a pas toujours été considéré comme la solution idéale. Le Dictionnaire de l'Académie française de 1694 avait plutôt adopté un classement par famille de mots : "malaise" est classé sous l'article "aise", "aîné" sous "naître", "ennemi" et "inimitié" sous "amour", etc. Abandonné par la majorité des dictionnaires, un système similaire a cependant encore été retenu par von Wartburg pour son grand dictionnaire étymologique. Une solution mitoyenne est celle du "Lexis" des éditions Larousse (1979), qui limite les familles aux termes les plus proches, l'objectif, parfaitement défendable au plan pédagogique, étant de faire découvrir à un usager les mots apparentés à celui qu'il consulte. Ce genre de préoccupation devient sans objet avec les dictionnaires électroniques.

Le tri alphabétique, qui apparaît comme une évidence pour un utilisateur francophone contemporain, n'est pas universel.

Les dictionnaires de langue peuvent se classer en deux catégories, selon qu'ils sont de type descriptif ou normatif, ce dernier cas étant le plus fréquent. Un dictionnaire descriptif s'attache autant que possible à décrire une langue telle qu'elle est écrite et parlée dans toute sa diversité; un dictionnaire normatif tente au contraire d'établir la norme et d'orienter l'usage, en utilisant des expressions comme « à éviter » ou « locution vicieuse » :

L'Organisation internationale de normalisation travaille afin de définir un cadre commun normalisé pour l'élaboration des lexiques du traitement automatique des langues.

L'Antiquité n'a pas eu de dictionnaire de langue au sens propre, mais elle a mis au point des listes de mots organisées en fonction de la première syllabe. Progressivement sont apparus des "protodictionnaires" ou formes intermédiaires du dictionnaire tel que nous le connaissons depuis la fin du . Les dictionnaires bilingues sont également apparus à une date très ancienne, mais il n'est pas évident qu'ils aient précédé les protodictionnaires.

Les premières listes de mots apparaissent à Sumer, vers la fin du millénaire av. J.-C. Elles sont utilisées à des fins pédagogiques, afin de former des scribes, profession très valorisée. On a ainsi trouvé une série de 42 tablettes comportant noms classés en fonction de leur premier élément.

Après l'arrivée des Akkadiens, des lexiques bilingues sumérien-akkadien se multiplient. On a aussi trouvé un ensemble de 24 tablettes datant des environs de l'an av. J.-C, comportant environ entrées où sont mis en correspondance mots sumériens et akkadiens et qui ressemble à une sorte d'encyclopédie du monde de la culture et de la nature, organisée thématiquement.

On a également trouvé en Égypte ancienne des listes de mots organisées de façon thématique, telles l' "Onomastique du Ramesseum", rédigé vers 1750 av. J.-C., et l' "Onomastique d'Aménopé", rédigé vers -1100. Ce "proto-dictionnaire" (lointain ancêtre du dictionnaire) avait pour vocation « non pas d'apprendre à écrire aux enfants, mais de proposer un programme d'instruction de l'humanité fondé sur l'organisation du monde ».

En matière de dictionnaires bilingues, on n'a retrouvé que des fragments d'un dictionnaire akkado-égyptien rédigé vers 1400 av. J.-C. Il faut attendre la période alexandrine (de -323 à -30) pour voir se répandre les glossaires thématiques grecs-coptes.. En 580 de notre ère, le "Glossaire de Dioscore" semble avoir remanié une onomastique grecque ancienne.

Divers recueils de gloses ou de scholies (commentaires linguistiques sur des textes), désignés sous le nom de "scala", paraissent durant cette période et dans les siècles qui suivent, servant d'étapes intermédiaires dans la mise au point du dictionnaire.

Le courant sophiste porté sur l'art de convaincre, développe le besoin de préciser le sens des mots et l’utilisation d'un vocabulaire précis et adapté.

On trouve des recueils de gloses destinés aux élèves, enseignants et au public lettré ; ce sont de petits lexiques attachés aux œuvres de grands écrivains fournissant des explications sur les mots rares ou difficiles. Au , Protagoras d’Abdère compile une liste des mots rares chez Homère. D'autres glossaires sont dus à Démocrite, Timée de Locres, Philémon d'Athènes (361-262), Philétas de Cos, Zénodote (320-240). Callimaque de Cyrène (310-240) a laissé une œuvre considérable, comportant notamment des glossaires thématiques. Ératosthène (276-194) se définit comme un philologue et développe cette discipline selon des principes rigoureux. Aristophane de Byzance (257-180) est un savant astronome et mathématicien qui s'intéresse aussi à la comédie et à la critique des textes; un de ses ouvrages s'intitulait "Peri Lexeon" ("Sur les mots"), un autre était un dictionnaire des noms propres donnés à des courtisanes dans la comédie. 
Aristarque de Samothrace (220-143) rédige un lexique homérique.
Cratès de Mallos rédige des glossaires. À l'ère chrétienne, on note les noms d'Apollonius le Sophiste, de Pamphile d'Alexandrie, d'Héliodore, et d'Aelius Herodianus qui jouent un rôle important dans l’évolution du dictionnaire par leurs études lexicographiques.

Les Romains ont montré un intérêt très vif pour la langue. On connaît de cette période différents ouvrages de description de la langue ressemblant de près ou de loin à des dictionnaires. 

Le début de l'ère chrétienne est marqué par le fort développement des gloses des auteurs latins et de la jurisprudence. La tendance est renforcée par l'apparition du codex qui favorise l'étude des textes. Ces recueils de grande dimension continuent donc de mêler les mots et les choses, les noms propres et les extraits. Parmi les gloses les plus célèbres :

Malgré les lacunes et erreurs, ces ouvrages restent essentiels pour la lexicographie et la lexicologie latine.

Khalil ibn Ahmad (718-791) rédige le premier dictionnaire de la langue arabe, le "Kitab al-Ayn" ("Le livre source"). À la suite de celui-ci, une douzaine de dictionnaires arabes sont rédigés jusqu'au . Ces ouvrages sont particulièrement intéressants pour leurs importantes rubriques de citations, qui renvoient à des grammaires, des textes religieux, des ouvrages poétiques, ou encore des proverbes.

Le chinois s'est formé très tôt et son écriture a peu évolué. Le premier dictionnaire connu, l"Erya" date probablement du avant l'ère commune. Le premier dictionnaire largement répandu, le "Shuowen Jiezi" a été publié au début du . 9353 idéogrammes, dont 1163 à doubles significations, avaient leur prononciation et étaient réunis dans l’ancêtre du Shingi, ouvrage en 44 volumes. Voir Dictionnaire de sinogrammes.

Le "Amarakosha" fut le premier lexique sanskrit, rédigé par Amarasimha, probablement au à la cour des empereurs Gupta.


On ne trouve toujours pas à la Renaissance de dictionnaire au sens où nous l'entendons aujourd'hui.

Le premier dictionnaire européen entièrement consacré à une langue vivante et proposant pour chaque entrée une définition est le "Tesoro de la lengua castellana o española" de Covarrubias paru en 1611. La langue italienne est la première à se donner un dictionnaire monolingue rédigé par une académie linguistique : le "Vocabolario dell' Accademia della Crusca", dont la première édition paraît à Florence en 1612. En français, il faudra attendre Richelet pour que paraisse le premier dictionnaire monolingue de langue française (1680). La langue anglaise, bien que pourvue de divers dictionnaire, devra attendre 1755 pour se voir dotée d'un dictionnaire compréhensif de la langue anglaise avec le "Dictionary of the English Language".

La langue française se fixe sous l'influence de plusieurs théoriciens qui travaillent à son épuration et à sa modernisation: François de Malherbe, Vaugelas, Ménage et Dominique Bouhours,
Au siècle des Lumières, la réflexion sur le langage se complexifie. Si les tendances puristes amorcées au siècle précédent s'exacerbent, notamment chez Voltaire, un courant important tend à régler la langue non plus sur l'usage de la Cour, mais sur celui des écrivains classiques. Les néologismes sont assez facilement acceptés. Les dictionnaires gagnent en prestige dès le milieu du siècle et leur nombre s'accroît considérablement. Le classement alphabétique est devenu tellement populaire que Voltaire y a recours pour son "Dictionnaire philosophique". Le prestige des dictionnaires s'accroît.

En Angleterre, Samuel Johnson publie "A dictionary of the English Language" (1755), premier grand dictionnaire de l'anglais. Avec ses entrées appuyées par citations balisant l'évolution des sens et des emplois, cet ouvrage remarquable sera sans rival durant plus d'un siècle et fera l'envie de Voltaire, qui souhaitait voir un jour paraître un dictionnaire du français construit sur ce modèle.
Au cours de ce siècle que Pierre Larousse a qualifié de « siècle des dictionnaires », de nombreux dictionnaires au lexique étendu et faisant une large place aux mots scientifiques voient le jour. 

Dans les autres langues européennes aussi paraissent d'importants dictionnaires, tels le Deutsches Wörterbuch des Frères Grimm en allemand, le "Dizionario della lingua italiana" de Niccolò Tommaseo en italien, le monumental Oxford English Dictionary qui avec son demi-million d'entrées et ses nombreuses citations établit un nouveau standard en matière de dictionnaire, "An American Dictionary of the English Language" par Noah Webster en anglais américain, le "Woordenboek der Nederlandsche Taal" en néerlandais.
Les dictionnaires spécialisés se multiplient : 

Suite à la réforme de l'orthographe paraît celle-ci est prise en compte dans le Dictionnaire de l'Académie française, édition (en cours, depuis 1992).
Avec le développement du web, les dictionnaires se mettent en ligne et les innovations sur papier se font plus rares. 
Les ressources informatiques sont d'une utilité évidente et tendent à tuer le marché du dictionnaire papier, . 

On peut noter les ressources suivantes : 

Par ailleurs, certaines années ont conduit à la mise à jour des dictionnaires papier pour considérer la nouvelle orthographe:




</doc>
<doc id="985" url="https://fr.wikipedia.org/wiki?curid=985" title="Douamoutef">
Douamoutef

Douamoutef est un génie à tête de chacal de la mythologie égyptienne, souvent associé à la ville de Hiérakonpolis.

Divinité protectrice de l’estomac des morts, elle est représentée avec une tête de chacal. Il est l’un des quatre génies funéraires anthropomorphes, appelés « Les fils d'Horus ». Ils avaient pour mission de garder les viscères du corps du défunt. À partir de la fin de la , les bouchons des vases canopes sont modelés à l’image des divinités qui les protègent. 

Le vase canope qui renferme l’estomac protégé par Douamoutef, à un couvercle qui représente une tête de chacal. Pour que le pouvoir s’accomplisse et qu’il protège les organes momifiés, ce génie doit être associé à une déesse et à un point cardinal. Pour Douamoutef c’est la déesse Neith et l’Est.

"Les quatre fils d'Horus représentés sur les vases canopes" :


</doc>
<doc id="986" url="https://fr.wikipedia.org/wiki?curid=986" title="Liste de divinités égyptiennes par ordre alphabétique">
Liste de divinités égyptiennes par ordre alphabétique

Liste des dieux et déesses égyptiens par ordre alphabétique (en italique, autre nom d'un dieu avec redirection vers le nom principal) :

"Aah" - Âbâset - Ach - Ageb - Aha - Aheqet - "Aken" - Akephalos - Aker - Akhet la prairie - "Amam" - Amaounet - Amemet - Amenhotep - "Amenothes" - Ament - "Amentet" - Am-Heh - "Ammit" - Amon-Min - "Ammon-Zeus" - Ammout - Amon - "Amonet" - Amon-Rê - Amset - Anat - Andj - Andjéty - "Andjty" - "Anhor" - Anhour - "Anouket" - Anoukis - "Anoupou" - "Anqet" - Anty - Antywy - Anubis - Anupet - Anzti - Apedemak - Âperet-Isis - "Apet" - Api - Apis - Apophis - Apopis - Âqen - Arensnouphis " - Arensnouphis-Dedoun" - "Arsaphes" - "Aset" - "Ash" - Astarté - Aton - Atoum - "Ausar"

Baal - "Baâl" - Baalat - Baba - Babaï - Babi - Bakha - Ba-Pef - Banebdjedet - Baq - Bastet - Basty - Bat - Bata - Behedetite - "Bélier seigneur de Mendès" - Benben - Benu - Benwen - Bès - Boukhis

Chai - "Ched" - Chemmis - Chentaït - Chentyt - Chepes - Chesmet - Chesmou - "Chou"

Dedoun - Dedwen - Denwen - "Djéhuti" - Djen - Douaour - Douamoutef - Douat - Dounanoui - Doun-anouy - Dounâouy

Edjo - Ermouthis

Fetket

Geb - Gengen Wer

Ha - Hâpi - Hâpy - "Harachte" - Harakhtes - "Harakhti" - Harmakhis - Haroeris - Harpocrate - Harparê - Harsaphes - Harsiesis - Harsomtous - Harsontoum - Hathehyt - Hathor - Hatmehyt - Haurun - Ha-hotep - Heddet - Hededèt - Hedjour - Heh - Hehet - Hehou - Hemen - Henou - Heqa l'enfant - Heqat - Heqet - Heret-Kau - Hermanubis - "Heryshef" - Heryyshaf - Hesat - Hetepes-Sekhous - Hity - Hor - Horakhty - Horemakhet - Hormerty - Hornoufi - Horus - Horus aux deux yeux - Horsaïsis - Hor-khentakhtaï - Hotep - Hou - Houroun - Hyt-Hestia

Iâh - Iahès - Iat - Ibou-ouret - Igaï - Ihet - Ihi - "Ihy" - Ikhesef - Imenhy - Imentèt - Imhotep - Imouthes - Iouf - Iounmoutef - Iounyt - Iousaas - "Ipy" - Iry - Irta - Ir-renef-djesef - Irto - Isdès - Ishtar - Isis - Izi

Kamoutef - Kébehsénouf - Kek - "Keket" - Kekou - Kekout - Kemour - Khefethernebes - Khensit - Khentamentiou - Khentekthai - Khentétkhas - Khenty - Khentykhety - Khepri - Kherty - Khérybaqef - Khnoum - Khonsou - Kolanthes

Maa - Maanitef - Maât - Mafdet - Mahaef - Matit - Mandulis - Mehen - Mehenyt - Mehet-Weret - Mehyt - Mekhenti-En-Irty - Mekhenty-Irty - Menhyt - Mennéfer - Menqèt - Mentit - Meret - Mertseger - Meryt - "Merour" - Meskhenet - Methyer - Methyour - Mihos - Min - Miysis - Mnévis - Monthou - Montou - Mout

Naounet - Naunet - Nebethetepet - Nebetouou - Nebou - Nefertem - Nefertoum - Nehebkaou - Nehebu-Kaou - Nehet-kaou - Nehemetaouay - Neit - Neith - Nekhbet - Nékhbet - Nekheb-Kaou - Nemty - Nephtys - Nepri - Noun - Nounet - Nout - Nya - Nyat

Ogdoade d'Hermopolis - Onnophris - Onouris - "Opet" - Ophois - Osiris - Ouadjet - Ouadjit - Ouadjour - Ouasèt - Ouazet - Oudjat - Oukh - Ounennéfer - Ounout - Ounshepsef - Oupaout - Oupesèt - Oupouaout - Oupouat - Our - Ourèthékaou - Outo

Pachet (Pakhet) - Panebtaouy - Peteese - Pihor - Ptah - Ptah-Sokar - Ptah-Sokar-Osiris-Peret

Qadesh - Qébéhout - Qebehsenouf - Qefedenou - Qerehèt

"Ra" - Rattaoui - Râttaouy - Rê - Rê-Horakhty - Renenout - Renenoutet - Renoutet - Répit - Reshep - Résoudja - Rosetaou

Samaty - Sarapis - Satis - Sbomeker - Sebhmet - Sebioumeker - "Séchat" - Sedjem - Sefegiru - Séfekhètâbouy - Sefkhet-Abwy - Seker - Sekhathor - Sekhmet - "Selket" - "Selkis" - Selkit - Sématourèt - Senmenty - Sepa - Sépédèt - Sept Hathor - Sérapis - Serket - "Serqet" - Seshat - Seth - "Shai" - Shed - Shedit - Shentaït - Shepès - Shesmetet - Shesmou - Shetat - Shezmou - Shou - Sia - Singe-Qefdenou - Sobek - Sokar - Sokaris - Somtous - Sopdou - Soped - Sopdet - Sôpdit - Sothis

Taït - Taa - Tapsaïs - "Taueret" - Taouret - Tasenètnéferèt - Tasenetnofret - Taténen - Tatjenen - "Taurt" - "Tawaret" - "Taweret" - Tayet - Taÿt - Tefnet - Tefnout - Témèt - Tenemèt - "Thoéris" - Thot - "Thouéris" - Tjaïsepef - Tjenenet - Tjnenet - Toum - Toutou - Typhon

Uræus

"Wadj Wer" - Wadjet - Weneg - Wosret

"Yah" - Yam


</doc>
<doc id="990" url="https://fr.wikipedia.org/wiki?curid=990" title="Dreamcast">
Dreamcast

La est une console de jeux vidéo développée par Sega, et est le successeur de la Saturn. Commercialisée dès novembre 1998 au Japon, elle est la première console de sixième génération présente sur le marché, avant ses concurrentes - la PlayStation 2 de Sony, la Xbox de Microsoft et la GameCube de Nintendo. Son nom est composé des mots ' (rêve) et ' (de "" : diffuser). Elle a été connue pendant son développement sous les noms "Blackbelt", "Dural", et "Katana".

Elle est la première console livrée en standard avec un modem lui permettant un support de jeu en ligne, de se connecter à Internet et ainsi de consulter des pages web ou bien lire des courriels. Sega cesse la commercialisation de la console en pour l'Amérique du Nord, en pour l'Europe, et en pour le Japon. La firme ne produira pas de nouvelle console et se retire ainsi complètement du milieu "" des jeux vidéo de salon.

Dès 1996, Sega déclare travailler sur un nouveau projet de console, connu alors sous le nom de "Dural" (emprunté à l'un des personnages de Virtua Fighter). Microsoft est choisi pour fournir le système d'exploitation et NEC pour le processeur graphique. Deux nouveaux projets dérivés de "Dural" voient le jour : "Blackbelt" et "Katana". Le deuxième est un projet japonais. En , Sega annule "Blackbelt", et "Katana" devient l'unique projet. Après une série d'essais et de prototypes, le processeur graphique retenu sera le Power VR II de Nec. Les kits de développement Dreamcast sont livrés aux développeurs dès fin 1996.

La Dreamcast est commercialisée le au Japon au prix de yens, le aux États-Unis (dont les 1500 premiers acheteurs de la console recevaient une cassette vidéo promotionnelle contenant le line-up de la machine) et au Canada, et le en Europe. Son prix de lancement en France est de francs, soit environ . Quatre jours après son lancement aux États-Unis, Sega explique que de la console ont été vendus pour un total de 312 millions de dollars. Des jeux de lancement tels que "SoulCalibur", "Sonic Adventure", "Power Stone", "Hydro Thunder", "Marvel vs. Capcom", "The House of the Dead 2" et "NFL 2K" aident la console à se former une réputation durant les premières années. Les jeux Sega Sports ont comblé le vide que laissent les jeux de sport Electronic Arts sur la console. Les ventes de la Dreamcast augmentent de 156,5 % du 23 juillet jusqu'au 30 septembre 2000 et bat même les ventes de la Nintendo 64 durant cette même période. Cependant, le lancement de la PlayStation 2 par Sony marquera le commencement de la fin pour la Dreamcast.

Le logo de la Dreamcast est une spirale de couleur orange sur le marché américain et asiatique, ou bleue sur le marché européen. L'objectif de Sega était de proposer le premier système de nouvelle génération afin de succéder à la Saturn tout en arrivant en magasin avant la PlayStation 2 (commercialisée par la suite le au Japon). La Dreamcast innovait avec les gâchettes analogiques, un lecteur de GD-ROM six fois plus rapide que le lecteur CD-ROM de la PlayStation, ainsi qu'une unité secondaire appelée VMU () logeable dans la manette et qui servait à la fois de carte mémoire et de mini console de jeux grâce à son écran LCD. Cette console puissante fait figure de « bombe » face à la PlayStation et à la Nintendo 64. Côté , Sony multiplie les publicités télévisées tandis que Sega la considère comme secondaire.

Le , Sega annonce l'arrêt de production de la Dreamcast à partir du 30 mars la même année. Le dernier jeu sur système en Amérique du Nord est "NHL 2K2", commercialisé en février 2002. Sega n'annonce aucune intention de développer le successeur de la Dreamcast, qui restera donc la toute dernière console développée par Sega. Dotée d'une ludothèque relativement peu fournie mais de qualité, elle n'a pas rencontré le succès escompté. L'absence de jeux d'éditeurs tiers renommés comme Electronic Arts ("Fifa") ou Squaresoft ("Final Fantasy"), qui n'ont pas souhaité développer de jeu sur Dreamcast, a eu un impact négatif sur les ventes de consoles.

Sega Europe cependant, continue les ventes de jeux vidéo sur système Dreamcast jusqu'en 2002. Ces derniers jeux vidéo ont été distribués par Bigben Interactive, ce dernier proposant des titres tels que "", "Cannon Spike", "", "Razor Freestyle Scooter" et "Conflict Zone". L'atout majeur de la Dreamcast, selon Sega, résidait dans le fait de pouvoir proposer des jeux jouables en ligne. Cet argument a été utilisé par Sega dans sa campagne marketing de lancement pour damer le pion à sa principale rivale, la PlayStation 2, qui devait sortir presque un an plus tard, le au Japon. Cette stratégie n'a pas fonctionné car trop peu de jeux implémentaient le jeu en ligne. De plus, le public attendait la sortie de la PlayStation 2, réputée plus puissante. Le , la console passe de yens à yens au Japon.

Alors que plus aucun jeu ne devait être édité, la Dreamcast compte sur une communauté de joueurs et de fans qui font perdurer cette machine et tentent de pousser certains éditeurs (notamment japonais) à sortir encore de nouveaux jeux. Ainsi, deux titres sortent en 2006 au Japon : "Radirgy" le 16 février puis "Under Defeat" le 23 mars. pour une console qui n'est plus produite depuis 6 ans. La console a même été redistribuée pour l'occasion sous forme de "" contenant un modèle d'occasion remis à neuf et un des deux jeux. Un jeu de tir à scrolling vertical nommé "Fast Striker" développé par NG:Dev.Team est d'ailleurs prévu pour la fin d'année 2010, soit 9 ans après l'abandon de la console, un record dans le monde du jeu vidéo pour une console de salon (record actuellement battu avec la sortie de "Pier Solar" sur Mega Drive 20 ans après l'abandon de la console). Le studio allemand Redspotgames a annoncé en la sortie de "Sturmwind", shoot 'em up horizontal, courant de l'année 2012. Par ailleurs, pour l'année 2013, le studio NG:Dev.Team a déjà prévu de sortir un nouveau jeu du nom de XYX sur Dreamcast.

Depuis les années 2000, la Dreamcast est une plate-forme de développement indépendant dynamique, et beaucoup de jeux ou programmes « "" » (faits maison) font leur apparition. Ainsi, des développeurs indépendants ont créé des émulateurs et lecteurs de médias (MPEG, DivX, MP3, JPEG).

Les raisons de cette reconversion particulière sont les suivantes :

En 2009, l'entreprise américaine ThinkGeek recommercialise la console de façon limitée aux États-Unis. Au vu de l'engouement de la communauté de joueurs face à ce support, le marché de l'occasion bat son plein. Seulement, la Dreamcast rencontre quelques lacunes en termes de longévité, notamment par sa lentille fragile et ses problèmes de surchauffe. À noter que la console est en version NTSC-US. En conséquence, un disque de boot est requis pour lire les jeux import. Commercialisée à , l'offre limitée est momentanément vidée de son stock. Étant donné que Sega ne fournit plus de support pour cette console, la garantie n'est que de 30 jours via le site du revendeur.

Le , Sega annonce la sortie de certains titres Dreamcast disponible sur le Xbox Live Arcade et le PlayStation Network. Les deux premiers titres à paraître sont "Sonic Adventure" et "Crazy Taxi". De plus, une compilation appelée "Dreamcast Collection" est commercialisée en , sur Xbox 360 et PC (Windows). Elle comporte "Crazy Taxi", "Sonic Adventure", "Sega Bass Fishing" et "Space Channel Five 2".

Par ailleurs, "Pier Solar", le dernier jeu sorti sur Mega Drive, subira une adaptation sur Dreamcast en 2014.

Enfin des émulateurs de cette console ont été développés pour différents systèmes d'exploitation, permettant de continuer à utiliser les jeux et logiciel de la console sur d'autres architectures :

Le format utilisé pour les jeux est le GD-ROM fabriqué par Yamaha. En effet, pour empêcher la copie de jeux, Sega a fait développer un nouveau support pour les jeux Dreamcast. Il s'agit d'un hybride entre le CD et le DVD d'une capacité de , qui ne peut pas être lu par un PC. Une de ses principales caractéristiques est la gravure des données dans une densité élevée et une deuxième TOC commençant après la section de type CD du GD-ROM. La Dreamcast lit les GD-ROM en 12X, d'où des temps de chargement relativement courts.


La Dreamcast possède deux systèmes d'exploitation : SegaOS et Windows CE. Le premier, présent dans la console, est plus difficile à utiliser pour les programmeurs mais permet d'utiliser les capacités maximales du matériel. Le deuxième est, quant à lui, chargé directement depuis le GD-ROM par les jeux qui en ont besoin. Le fait que la console reconnaisse Windows CE permet d'adapter assez rapidement des jeux pour PC sur la Dreamcast.

Divers accessoires sont disponibles : carte mémoire, volant, maracas, tapis de danse, pistolet, kit vibration, stick arcade, canne à pêche, etc

La Dreamcast est la seconde console de jeu permettant l'accès à Internet, après la Sega Saturn et son Sega NetLink, via un modem enfiché sur son côté droit (modem 56k pour la console américaine et 33.6k ailleurs). De plus elle possédait jusqu'en 2003 son propre portail d'accès à Internet : la Dreamarena. La plupart des jeux Dreamcast possédaient une page web dédiée directement accessible depuis le jeu sur ce portail. Sega a mis fin au service en mars 2003.




</doc>
<doc id="991" url="https://fr.wikipedia.org/wiki?curid=991" title="Disc jockey">
Disc jockey

Un () ou (), est un animateur qui sélectionne, diffuse et mixe de la musique à destination d'un public, que ce soit pour une émission radiophonique, dans une discothèque ou à l'occasion d'un événement spécifique.

À l'origine, un "Disc Jockey" ou "DJ" est un animateur qui produit des effets sonores, avec sa voix et surtout, comme le terme anglais l'indique, avec une platine portant un disque 78 tours ou 33 tours. La dénomination s'est ensuite généralisée pour qualifier les musiciens des multiples courants de musique électronique, qu'ils soient créateurs originaux, joueurs interprètes, chanteurs ou conteurs accompagnateurs, arrangeurs pratiquant l'art du mixage et de l'enregistrement sur de multiples supports à codage analogique ou numérique.

Dans des bars et dans les fêtes techno ou les "rave parties".

Il peut simplement enchaîner les morceaux de musique les uns après les autres en fonction des envies des auditeurs. Il peut aussi modifier ou superposer deux musiques, ou une musique et une version a cappella, et faire preuve de créativité et d'ingéniosité, voire utiliser des équipements spéciaux ou des ordinateurs pour refondre entièrement le morceau utilisé. Dans le milieu du « DJing », cette technique est connue sous le nom de « ' » ou encore de « ' » ou « "" ».

L'animateur DJ de soirée privée comme le mariage ne peut pas être considéré comme un artiste du spectacle. Par contre, depuis fin 2015, le DJ ayant une activité en discothèque est considéré comme un grâce à une loi votée par les députés français. Il peut à ce titre prétendre au statut d'intermittent du spectacle pour autant que le lieu qui les reçoit cotise en conséquence.

Depuis, le rôle du disc-jockey a pris de l'ampleur dans les musiques populaires récentes. Il est parfois reconnu comme un musicien à part entière. Le DJ peut parfois produire une œuvre originale à partir de matériaux musicaux existants, soit qu'il joue un rôle de découvreur de titres passés inaperçus ou tombés dans l'oubli, soit qu'il combine avec talent des œuvres mineures. Il se réapproprie alors le travail d'autres musiciens, exploitant un matériau sonore qu'il n'a pas lui-même créé. Cependant, le juste mélange des musiques diffusées, leur arrangement en live demandent une certaine créativité, et peut donc être considéré comme un art, qui diffère un peu de celui des musiciens.

Certaines têtes d'affiches, particulièrement en EDM, sont devenues de véritables vedettes, à l'instar de David Guetta, Tiësto ou Avicii par exemple. annonce Alesso.

Les salaires se mettent alors en adéquation avec leur statut. D'après les études du magazine "Forbes", une douzaine de DJ gagnent plus de quinze millions de dollars dans l'année : les quinze premiers DJ mondiaux représentent à eux seuls de dollars de chiffre d'affaires en 2014 puis plus de 300 millions l'année suivante, sachant que le chiffre global estimé du domaine de l'EDM atteint six à sept milliards de dollars dont plus de 400 millions d'euros rien qu'en France d'après la Sacem. Selon ce même magazine économique, les gains de Calvin Harris se montent alors à plusieurs dizaines de millions de dollars, comprenant, outre ces prestations scéniques, son travail de production ou les revenus de ses labels et droits d'auteur, et ce, quatre ans de suite. précise Martin Garrix ; mais sans que cela profite aux majors du disque, reléguées en fin de peloton en terme d'influence ou de gains financiers.

Internet a changé la donne par la diffusion globale de vidéos ; les événements se multiplient sur un modèle unique : l'Ultra, Tomorrowland ou l'Electric Daisy Carnival se déclinent à travers la planète, avec globalement une programmation identique. Résultat, cette uniformisation savamment marketée laisse les disc-jockeys acquérir un succès plus seulement national, mais bien mondial. Une réciprocité s’établit alors entre la réputation des grands festivals et la renommée de l'artiste, chacun ayant besoin de l'autre pour obtenir revenus et reconnaissance du public.

Mais cette starisation couteuse, combinée à l'industrialisation de cette culture musicale, font disparaitre ces même disc-jockeys des clubs dont ils sont pourtant issus. De plus, David Guetta précise que la fonction de DJ reste également de faire connaitre des nouveautés, mais En définitive, cet avènement d'une frange de disc-jockey entraine comme conséquence de renforcer une scène underground, plus accessible, dans le domaine de la dance : , commente l'artiste français.

Le terme original de DJ désigne la personne qui tient le microphone et intervient en direct sur la version instrumentale d'un disque (souvent en face B), dans les « "" » jamaïcains. Le deejay reggae est un artiste vocal au même titre que le chanteur. Son style vocal est un mélange de voix parlée, scandée et chantée et préfigure en cela celui du rappeur.

Après avoir émergé dans les années 1950 (les deejays se contentaient alors de glisser de courtes interjections dans la musique), ils ont été reconnus à partir de la fin des années 1960 comme des artistes vocaux à part entière (avec des couplets et refrains complets chantés sur une version instrumentale) à égalité avec les chanteurs et sont devenus hégémoniques dans le reggae (aujourd'hui, à peu près 70 % des artistes vocaux du reggae sont des deejays), et le public a également vu l'apparition du "singjay" (mélange de style "deejay" et de chant pur) au cours des années 1970.

Le DJ travaillant derrière les platines est quant à lui nommé « " ». Le lien entre reggae et hip-hop s'est fait par l'intermédiaire de DJ Kool Herc, un Jamaïcain ayant émigré aux États-Unis et pionnier du hip-hop.

À la fin des années 1970, un ensemble de facteurs tant musicaux que sociaux et techniques (évolution du rock vers un style moins dansant, développement d'une musique soul plus dansante, amélioration des sound systems, libération des mœurs, besoin de reconnaissance de certaines minorités) aboutissent au développement aux États-Unis d'un mode de sortie (discothèque) et d'un style de musique qui sera finalement nommé disco.

Le métier de DJ évolue alors dans les lieux de sorties avec musique (bars, clubs) où progressivement les DJ's évincent les vrais musiciens d'abord parce que moins chers, jugés moins capricieux et parce qu'ils réussissent à faire danser le public de manière plus intense que la musique orchestrée. Progressivement, les simples enchaînements de titres deviennent un ensemble cohérent qui accompagne les danseurs.

Le DJ sera au centre de mouvement jusqu'au moment où, à la fin des années 1970, les maisons de disques et l'évolution de la société se chargent soit d'exploiter le filon disco puis de le rejeter une fois épuisé, soit de juger ce genre vide et décadent.

Cependant, dans des clubs des quartiers noirs de New York où se joue et évolue le disco, des DJ's observent les danseurs et constatent que certains d'entre eux se déchaînent en solo quand le titre marque un break. De ce constat naît la breakdance qui donnera naissance au hip-hop, dont l'idée vient de personnalités comme DJ Kool Herc qui, en enchaînant les breaks de plusieurs morceaux, parvenait à prolonger la durée des coupures rythmiques.

Avant que puisse être dupliquée par enregistrement cette répétition, la technicité demandée au DJ pour effectuer cet exercice s'est accrue. Progressivement, avec des artistes comme Grandmaster Flash, le deejaying prend alors un nouveau sens qui exprime l'idée que le DJ produit du son à partir d'un instrument, le disque, grâce à tout un ensemble de techniques nouvelles qui nécessitent une dextérité extrême. Le DJ passe ainsi du rôle de sélectionneur à celui d'artiste.

Dans la musique hip-hop, le Disc Jockey peut être parfois accompagné d'un MC (", le rappeur). Le DJ scratche, c'est-à-dire qu'il pose ses doigts sur le vinyle et en modifie la vitesse et le sens de lecture afin de déformer et de rythmer les sons existants. Cette déformation de sons est associée à l'utilisation, sur la table de mixage (élément central), d'un "fader" ou "crossfader". Cet élément permet de passer du son d'une platine à l'autre et de couper le son d'une des deux platines. Il existe diverses techniques de scratch, comme le "cutting", le "transforming", ou encore le "flare", qui peuvent être cumulées et alternées.

Kool Herc fut le premier DJ qui mixa deux disques réglés sur le même BPM, faisant ainsi une transition appelée de nos jours calage tempo.
Par la suite, cette pratique se développa dans le Bronx notamment grâce à la culture "Zulu Nation" du milieu des années 1970.

Au début des années 1980 vint ensuite le scratch, inventé par DJ Grandwizard Theodore. Cette manipulation révolutionnaire du disque fut largement popularisée en 1983 par Grand Mixer DXT et Herbie Hancock dans le titre "Rockit".

Après 10 ans d'amélioration des techniques de scratch, le terme de "Turntablism" fut finalement proposé par DJ Babu en 1995 pour décrire cet art.


Les disques sont enchaînés de plusieurs façons :

Certains musiciens se disent également DJ du fait qu'ils utilisent les mêmes outils, bien que ce ne soit pas dans le but d'enchaîner des morceaux, mais bien d'en créer de nouveaux à partir d'éléments de plusieurs supports musicaux selon le principe des boucles et du "sampling".

Le "DJ-ing" ne se fait pas toujours en direct, ni face à un public. Par exemple, certains DJ utilisent des logiciels comme Cubase, FL Studio, ou Reason pour créer leur propres tracks.

Certaines maisons de disques ne publient rien d'autre que les réalisations en studio de DJ. Il existe aussi un championnat du monde des DJ, qui se rencontrent dans différents types de catégories.

Le concept du DJ-ing s'applique également à la vidéo. Le « VJ » ("visual jockey" ou "vidéo jockey") enchaîne et superpose des images fixes et animées qui peuvent être projetées sur écran à l'occasion de soirées ou de concerts, mais également, sur les chaînes télévisées musicales. Le terme a d'ailleurs été élargi au simple présentateur d'émissions de telles chaînes du fait qu'il est censé choisir les clips vidéo qui passent. De la même manière, il est question de KJ ("karajockey") pour les animateurs de karaoké.

Si, historiquement, le vinyle a été le premier support des DJ (par le scratch), .

La raison de cette résistance est la longue absence de platine CD à vitesse réglable, condition nécessaire pour mixer en discothèque. De ce fait durant plusieurs décennies la Technics 1200 MKII est devenue puis restée la platine vinyle la plus répandue dans les clubs.

Dans les années 2010, certains labels sortent les nouveautés sous le format vinyle, à destination des DJ qui diffusent ainsi ces morceaux et permettent d'évaluer leur potentiel. Le pressage en plus grand nombre peut alors suivre, et selon le succès du morceau, le public pourra ensuite le trouver dans des compilations CD ou sur les albums des artistes correspondants.

Cependant, à l'exception de la scène underground, de moins en moins de disc-jockeys mixent sur vinyles, ce format étant supplanté par le format numérique pour des raisons de gain de place (ainsi, Laurent Garnier a pu dire ), d'ergonomie et de possibilités de création.

L'utilisation combinée du format MP3 et des ordinateurs a ainsi révolutionné le monde du DJing :

Le contrôle d'un logiciel prend maintenant un intérêt de plus en plus important pour des djs pros, grâce à l'apparition de contrôleurs de qualité ou de systèmes comme le Vinyl timecodé. Cela dit, il est généralement plus aisé d'effectuer des transitions de type Calage tempo sur ces programmes, qui disposent souvent d'une fonction de synchronisation automatique des morceaux à caler, ainsi que l'affichage du BPM


En , la commission générale de terminologie et de néologie française a proposé de traduire en français le terme « Disc jockey » sous l'appellation "platiniste".

Le vocabulaire du DJ comprend un certain nombre de termes techniques :






</doc>
<doc id="992" url="https://fr.wikipedia.org/wiki?curid=992" title="Do it yourself">
Do it yourself


Le "Whole Earth Catalog", lancé en 1968 aux États-Unis, est une des premières formes contemporaines du mouvement.

En 2007, la croissance des ressources de "DIY" en ligne est en forte progression, et le nombre de propriétaires ayant un blog à propos de leur propre expérience ne cesse de croître de même que les sites web "DIY" d'organisations.

On peut associer la formule au bricolage ou à la débrouillardise mais cela ne s'arrête pas là. Différentes choses s'apparentent à la philosophie « Faites-le vous-même » :

Dans la culture punk, l'éthique DIY est liée à la vision punk anti-consumériste ; c'est un rejet de la nécessité d'acheter des objets ou d'utiliser des systèmes ou des procédés existants. 

Le DIY comme sous-culture a sans doute commencé avec le mouvement punk des années 1970. On peut noter cependant que la débrouille, le bricolage, les activités pour enfants, etc., existaient avant le mouvement punk DIY.

En musique les groupes punk émergents effectuent souvent des spectacles dans les sous-sol des habitations, plutôt que sur des scènes traditionnelles, pour éviter le mécénat d'entreprise ou pour assurer la liberté de la performance. Depuis, alors que de nombreuses salles ont tendance à fuir la musique expérimentale, les maisons (ou les caves) sont souvent les seuls endroits où ces groupes peuvent jouer. L'underground est alors réellement "underground", et pourtant les salles de spectacle dans les caves gagnent en renommée dans les grandes villes.

Les adhérents de l'éthique punk DIY peuvent également travailler collectivement. Par exemple, le CD Present (une compagnie musicale de promotion de concert) de l'imprésario punk David Ferguson permettant une production de concerts DIY, et octroyant un studio d'enregistrement, et un réseau de maisons de disques.
L'éthique punk DIY s'applique également à la vie quotidienne, tels que l'apprentissage de réparation de vélos plutôt que les conduire à l'atelier, la couture (réparation/modification des vêtements plutôt que d'acheter de nouveaux), la culture de jardins potagers, la récupération de produits réutilisables dans les poubelles. Certains enseignants se livrent aussi à des techniques d'enseignement de bricolage, parfois appelé Edupunk.

De ce fait le mouvement DIY est une approche concrète et une mise en pratique de l'écologie et de l'anticapitalisme, par l'anti-consumérisme.

Face à la grande différence entre la valeur marchande des matériaux utilisés et le prix de vente du bijou final, de nombreuses boutiques notamment sur Internet vendent aujourd'hui les éléments nécessaires à la fabrication maison de bijoux fantaisie. Leurs produits se résument de manière générale aux apprêts (fermoirs, estampes), fils (en métal ou en laine) et différentes sortes de perles (plastique, pâte Fimo, verre, pierres semi-précieuses). Un grand nombre de blogs et tutoriels vidéos sont disponibles sur Internet, permettant ainsi aux novices de fabriquer eux-mêmes leur bijoux. 
Les consommateurs des boutiques DIY sont en général motivés par la possibilité de création d'un bijou unique, mais aussi par la possibilité de concevoir des bijoux à partir d'éléments recyclés. L'intérêt économique entre également en compte. Contrairement au mouvement général, les personnes fabriquant des bijoux elles-mêmes ne sont pas forcément impliquées dans des mouvances anarchistes ou anticapitalistes.

Les groupes de musique DIY tentent de faire tout eux-mêmes, depuis la production de l'album jusqu'aux concerts, en passant par les actions de communication. Ce choix de production reflète avant tout une volonté de marquer son indépendance face aux grandes maisons et à l'industrie du disque en général : rendu possible par le développement de l'informatique grand public, ce type de production connaît un véritable essor ces dernières années, particulièrement dans la musique électronique. 

En contrôlant l'intégralité de la chaîne de production et de distribution, ces groupes tentent d'inventer une nouvelle conception de la relation entre les artistes et le public, sans aucune forme d'intermédiaire. Plus qu'une simple forme de communication, le DIY permet un contrôle total sur la production finale qui n'est influencée par personne d'autre que les artistes eux-mêmes. Ceci peut aussi bien être analysé d'un point de vue positif, la production finale étant plus personnelle qu'une production industrielle, que négatif, cette production étant par nature même une production non-professionnelle, terme ayant tendance à avoir une connotation négative. 

Cette opposition « DIY "vs" industrie » suscite d'ailleurs de vifs débats entre les deux camps : les majors ne cessent d'assurer que les maisons de disques ont un rôle déterminant à jouer dans la production de musique enregistrée là où certains artistes très en vue défendent le modèle d'auto-production (Radiohead, Trent Reznor et Bérurier Noir par exemple). De nombreux termes sont utilisés pour qualifier les groupes de musique DIY (autoproduction, musique indépendante, direct-to-fan) et l'on retrouve cette opposition avec le mode de production industriel dans les termes de « musique indépendante », « musique libre », ou encore dans celui d'« artisanat musical » développé par le groupe DIY Breakfast at Your Place.

Au-delà d'une simple volonté de récupération, le mouvement "Do It Yourself" (il ne s'agit pas d'un mouvement constitué) se voit comme une autre voie politique en opposition au monde d'ultra-consommation dans lequel il baigne. Ses membres sont ainsi souvent liés à l'anarchisme, l'autogestion et aux mouvements squat et punk. Le besoin de créer, d'avoir une certaine indépendance par rapport à l'industrie et aux grands groupes commerciaux, de retrouver un savoir-faire abandonné les pousse à trouver des solutions pour faire le maximum de choses par eux-mêmes, en opposition à la marchandisation dominante, tout en recherchant la gratuité ou les prix faibles.

Les survivalistes ainsi que les populations défavorisées du monde entier sont aussi adeptes du DIY, dont certains sont engagés politiquement et d'autres non.



</doc>
<doc id="993" url="https://fr.wikipedia.org/wiki?curid=993" title="Dubplate">
Dubplate

Un dubplate désigne un disque microsillon en acétate très fragile gravé en un seul exemplaire, créé à l'origine pour faire les 2 moules matricielles du futur disque, ceux des Faces "A" et "B", qui permettront de presser des disques vinyles par la suite.Le format du dubplate ( 33t - 45t ) dépend de la production prévue.

Pourtant il ne fut pas juste utilisé pour créer des disques vinyles, différents label en produisirent pour les distribuer aux radios et aux sound system afin d'évaluer les réactions du public en vue d'une production commerciale ultérieure. Ce procédé promotionnel a encore lieu de nos jours bien que les formats numériques aient remplacé les formats analogiques.

Le dubplate est également utilisé par les sounds systems et les D.J pour créer des productions musicales ou des remixes qui leur sont propres afin de développer leurs identités musicales et d'accroitre leurs notoriétés via une promotion musicale de qualité. Comme les radios, les sounds systems et D.J, de nous jours pressent rarement leurs dubplates sur acétate, préférant le format numérique plus pratique d'utilisation.

On assimile aujourd'hui le dubplate au special à une production musicale exclusive pour un sound system ou un D.J et sur lequel un artiste (chanteur ou deejay) modifie les paroles d'une de ses compositions pour en faire un hommage au sound-system qui le lui a commandé (on parle aussi de VIP mix, spécialement dans la musique électronique actuelle).

À l'origine, à la fin des années 1950, le "dubplate" désigne un disque promotionnel que les producteurs envoient aux sound-systems pour mesurer leur popularité avant un éventuel pressage en 45 tours. Si la réaction du public est bonne, le morceau est ensuite normalement pressé. Alors que l'industrie musicale jamaïcaine est balbutiante, presser un 45 tours qui ne rencontrerait aucun succès est un risque énorme que les producteurs cherchent à minimiser en ayant recours aux dubplates.

Mais l'engouement immédiat des Jamaïcains pour leur musique locale induit un développement rapide de l'industrie musicale jamaïcaine et le risque d'échec étant nettement réduit, on presse désormais directement les 45 tours sans passer par le stade des "dubplates".

Les "dubplates" ne disparaissent pas pour autant et deviennent même un instrument majeur dans l'évolution de la musique jamaïcaine. Pour les producteurs, ils permettent de prendre la température des sounds et d'observer presque instantanément la réaction du public aux dernières innovations qu'ils apportent. Si cette réaction est bonne, les producteurs sont confirmés dans l'orientation musicale qu'ils ont prise.
Pour les sound-systems et leurs propriétaires, le "dubplate" est un moyen de se distinguer de leurs concurrents de par son caractère exclusif : on se rend chez "Downbeat" (sound-system de Studio One) plutôt que chez un autre car lui seul peut passer avant tout le monde les derniers morceaux en version "dubplate" des Heptones ou des Wailers.

Un accord implicite se noue entre labels et producteurs : les "dubplates" servent à faire la promotion de morceaux avant leur sortie et de tester des innovations avant que le processus ne soit tout à fait enclenché, et à attirer le plus de monde possible dans un sound-system.

C'est d'ailleurs au cours de la gravure d'un "dubplate" que le dub est créé en 1967 : l'opérateur oublie de connecter la piste vocale et seule la rythmique nue est gravée. Mais la réaction du public face à ce qui n'est finalement qu'une version accidentellement instrumentale est telle qu'elle convainc les producteurs de généraliser les versions instrumentales, qui servent ensuite à combler les faces B de 45 tours, puis, avec l'inventivité de Lee Perry ou King Tubby (dont les premières expérimentations surviennent en gravant des "dubplates" à la fin des années 1960), se retrouvent bardées d'effets sonores pour devenir le dub que l'on connaît depuis les années 1970.

Au début des années 1980, apparaît la mode non-démentie jusqu'à aujourd'hui des "specials" (voir définition). Le special n'a plus le caractère de disque promotionnel du dubplate initial. En revanche, il va encore plus loin dans le principe de l'exclusivité, puisque désormais, le morceau est enregistré directement et exclusivement pour le sound-system, sans intervention du producteur. Un artiste y parodie voire y plagie donc un de ses tubes en forme d'hommage plus ou moins glorificateur et guerrier à un sound-system (les dubplates peuvent également consister en un pot-pourri par l'artiste de ses tubes).

Dans les "dubplates" qu'il signe, le chanteur Johnny Osbourne transforme ainsi généralement son "I don't want no ice-cream love 'cause it's too cold for me" en "I don't want no ice-cream sound 'cause it's too soft for me".

Ce qui compte, ce n'est évidemment pas la qualité du son ni la performance artistique : les sound-systems n'ont pas les moyens d'enregistrement des grands labels et les "specials" sont enregistrés à la va-vite. Ce n'est même pas la nouveauté puisque l'artiste ne crée pas des paroles totalement nouvelles ni une nouvelle mélodie et les "dubplates" sont d'ailleurs plus souvent réalisés sur des riddims classiques déjà éprouvés et connus de tous, que sur la rythmique originale du morceau (sauf si celle-ci est un classique).

Par ailleurs, il faut rendre immédiatement identifiable le "dubplate" : par l'utilisation d'un riddim classique et la parodie d'un morceau déjà existant, le public saisit instantanément le caractère exclusif du "dubplate". D'autant plus que celui-ci comprend généralement le nom du sound voire de ses membres. Cette dernière pratique s'est estompée depuis que les résidents des sound-systems ont pris l'habitude de quitter leur sound pour être "transféré" dans un autre ou pour fonder le leur : après leur départ, le sound ne peut plus passer un "dubplate" dans lequel est cité le nom d'un ex-membre désormais concurrent !

Plus l'artiste est coté, plus le "dubplate" a de la valeur (valeur décuplée si l'artiste est décédé ou "retraité"). Des rumeurs de "specials" de Bob Marley ont fréquemment circulé dans le milieu reggae, mais leur existence paraît assez improbable vu que la mode des specials est postérieure à la mort du chanteur.

Les "specials" sont principalement utilisés lors de clashs (joutes par vinyles interposés) entre différents sound systems. Les grands sound-systems réalisent d'ailleurs parfois des "specials" exclusivement destinés à un clash précis dans lesquels le nom du sound concurrent est mentionné et discrédité (on parle de "nominatifs").


</doc>
<doc id="994" url="https://fr.wikipedia.org/wiki?curid=994" title="Duke Reid">
Duke Reid

Arthur "Duke" Reid, surnommé Trojan, né en 1915 dans la Paroisse de Portland (Jamaïque) et mort en 1975, est un producteur de musique jamaïcain.

D'abord propriétaire du plus populaire sound system de Kingston, le "Trojan" (en 1956, 1957 et 1958, il est même sacré roi du sound & blues par le public), il monte un studio d'enregistrement à l'étage de son magasin de vins et spiritueux en 1965, puis fonde les labels Treasure Isle et Trojan. Il était aussi le rival historique de Coxsone.

Duke Reid va participer activement à l’émancipation et au développement de la musique yardie. Tout d’abord amateur de Calypso, rhythm and blues, et de jump blues, il se tourne ensuite vers le ska et le reggae. Au début des années 1950, il devient DJ pour une radio locale, et anime un programme qu’il nomme « Treasure Island Time » du nom d’un magasin d’alcool acheté avec sa femme : « Treasure Isle Liquor store ». Il lance aussi une affaire de "sound system" mobile, en déplaçant son matériel et ses disques à l'aide d'un pick-up « Trojan », d'où lui vient son surnom.

Il est aussi et surtout, un des premiers opérateurs de "sound system" de l’île, avec Tom the great Sebastian et Coxsone Dodd. Les "sound systems", ou « maison de la joie » comme on les surnomme en Jamaïque, sont des espèces de discothèques mobiles, composées d’une quarantaine d’amplis environ, et qui représentent pour le peuple jamaïcain le principal moyen d’accès à la musique. Bien que peu nombreux, la compétition entre les "sounds systems" fait rage, ils s’affrontent sur les « longs » (pelouses) et c’est alors l’originalité du son et le talent des DJ qui font la différence.
Le Duke se démarque notamment par ses performances scéniques souvent spectaculaires, la qualité de ses riddims et de ses Dj : Cuttins et Cliffie.
Ce qui ne l’empêche pas à l’occasion d’utiliser la violence pour briser les oppositions.

Ancien policier, il inspire le respect de tous, d'autant plus qu'il ne sort jamais sans une arme et une ceinture de munitions, voire une grenade ou une machette en tant qu'accessoires. De plus, il emploie des vieilles connaissances et des voyous (« dancehall crashers ») pour saboter le matériel des "sounds systems" concurrents et provoquer des bagarres chez eux, dans le but d'attirer des danseurs par la bonne ambiance de son sound. Duke Reid est en quelque sorte le précurseur d'un comportement « gangsta » dans la musique jamaïcaine.

Sa rivalité avec Coxsone augmentant, il multiplia ses voyages aux États-Unis pour dénicher d'obscurs vinyles R&B ou des pistes instrumentales de saxophone. L'exclusivité devenant un critère de plus en plus important pour garantir le succès d'un sound system, Reid et Coxsone rayaient les noms des artistes et des labels sur leurs disques, et les renommaient pour protéger leurs vraies identités.

Une anecdote célèbre est que Reid s'est risqué à passer une chanson portant la signature de Coxsone, ce qui entama un « battle » entre Duke Reid et un Coxsone choqué et consterné.
Reid était souvent accusé de tactiques peu scrupuleuses à cause, entre autres, de son recours aux « dancehall crashers », des voyous dont le rôle consistait à saboter le matériel hi-fi des sound-systems rivaux et de provoquer des bagarres.

Il remporta le "Jamaica's top sound-system battle" trois années consécutives, de 1956 à 1958.

Fin des années 1950, il produit du Calypso sur son label « Trojan » ; en 1959, il construit son propre studio d'enregistrement, qu'il baptise logiquement Treasure Isle et forme un groupe de studio, le "Duke Reid Band", auquel participent ponctuellement Rico Rodriguez, Don Drummond, Roland Alphonso, Johnny Moore et Ernest Ranglin. Il enregistre un grand nombre de chansons destinées à déchainer les foules lors des soirées organisées à l'aide de son Sound System.
Après quelques mois et devant la demande du public Duke Reid commence à commercialiser certains morceaux en les pressant au format 45 tours en petite quantité (500 exemplaires environs). Devant le succès populaire, Duke Reid se lance dans le commerce de l'édition de disques. De 1962 à 1965, les labels de Reid - Treasure Isle en premier - sortent de nombreux hits ska des Skatalites, Stranger Cole, the Techniques, Justin Hinds & the Dominoes...
Cependant, c'est avec l'arrivée en 1966 d'Alton Ellis et du rocksteady, plus lent que le ska, que Duke Reid devance Studio One et Coxsone dans la course à la renommée. L'apogée du rocksteady (1966-1968) fut très fertile pour les productions de Reid, avec Alton Ellis, Phyllis Dillon, the Melodians, the Paragons, the Ethiopians, the Jamaicans et bien d'autres. La plupart d'entre eux étaient entourés du nouveau "house band" de Reid, Tommy McCook & the Supersonics, menés par l'ancien saxophoniste des Skatalites.

Dans une interview pour Kool 97 FM, Jackie Jackson avec Paul Douglas et Radcliffe "Dougie" Bryan ont été interrogés sur les nombreux enregistrements qu'ils ont fait ensemble comme la section rythmique pour Treasure Isle Records. Ils ont été interviewés sur leur travail avec Sonia Pottinger et Duke Reid.

Avec la fin du "rocksteady" et l'avènement du "roots reggae" à tendance rasta, Reid se trouva face à un dilemme, cette nouvelle musique n'étant pas de son goût, particulièrement les paroles relatant des revendications sociales. Cette position fit petit à petit de lui un personnage de la « vieille garde », passé de mode. Bien qu'ayant été un des plus grands producteurs jamaïcains, il n'hésitait pas à refuser d'enregistrer des chansons rastas, en répliquant « je suis moi-même Babylone, j'ai été flic, c'est de moi que tu parles, pas de rasta-song ici ».

Heureusement, il y avait une autre mode à l'époque qui fleurissait dans les "dancehalls" : les DJ commençaient à insérer leurs propres rythmes, jouant avec les rimes, et commençaient véritablement à « chatter » et à « toaster » sur des mélodies populaires.
Le leader des Paragons, John Holt, présenta le pionnier du genre, U Roy à Duke Reid en 1970, qui décida très vite de l'enregistrer, et insiste sur l'idée de rajouter simplement des voix sur les enregistrements "Treasure Isle" existants.

Les résultats ont été époustouflants de popularité. Quatre singles de U Roy sont apparus dans le Top5 jamaicain d'un seul coup. Reid continue d'enregistrer U Roy jusqu'au début des années 1970, et sortit également des disques d'autres jeunes DJs, comme Dennis Alcapone.

Il tombe gravement malade en 1974, et finit par s’éteindre en 1975 à l’âge de 60 ans.

Les collections, productions estampillées Trojan, Treasure Isle, etc. forment une œuvre tout simplement faramineuse.



</doc>
<doc id="995" url="https://fr.wikipedia.org/wiki?curid=995" title="Don Drummond">
Don Drummond

Don Drummond est né le à Kingston, Jamaïque, dans le quartier de Trenchtown. Il est mort le 6 mai 1969 à Kingston. C'est un tromboniste et compositeur jamaïcain issu du jazz, il restera une des plus grandes légendes du ska et de la musique jamaïcaine pour son œuvre individuelle ainsi que pour sa carrière au sein des Skatalites, groupe mythique formé en 1963.

Don Drummond était parmi les figures les plus représentatives du ska et un des membres fondateurs des Skatalites. Il fut le compositeur le plus prolifique dans son genre, avec près de 300 titres sous son nom. 
Même avant la naissance du ska, Drummond était déjà considéré comme une légende jamaïcaine pour ses prouesses en jazz et en tant que professeur des jeunes orphelins de l'Alpha Boys School de Kingston. C'est d'abord en tant qu'élève qu'il fréquente cet établissement sous la direction de Rupert Anderson. La tradition veut que les anciens pensionnaires deviennent par la suite professeurs. Il enseigne alors à Rico Rodriguez, Vernon Muller, Joe Harriott et Vincent Gordon. 

Don Drummond intègre par la suite les orchestres locaux tel que le Colony Club Orchestra d'Eric Dean ou le Tony Brown Orchestra. Aimant le jazz, il forme un groupe, le Don Drummond Four et est élu meilleur tromboniste de l'année 1954. Clement Dodd le repère lors d'un de ses shows et l'embauche dans ses groupes de studio (les Blues Blasters, The City Slickers ou le Studio One Orchestra), où officient plusieurs futurs Skatalites. Sa carrière de studio commence en 1956, il a principalement enregistré des "specials" (enregistrements originaux et exclusivement pour sound system). En 1959, cependant, ces specials ont commencé à être commercialisées pour la Jamaïque puis l'Angleterre. Drummond enregistre également pour les producteurs Leslie Kong ("Spitfire"), Vincent Chin ("Don't Bury Me", "Dandy Don D."), Prince Buster ("That Man Is Back", "Dewdrops", "Corner Stone"), Justin Yap ("Ringo", "Confucious", "Ska-Ra-Van") et Duke Reid ("Eastern Standart Time", "Occupation", "Let Georges Do It").

Ses premières influences viennent de grands jazzmen américains tels Jay Jay Johnson ou Kai Winding. Le génie de Don Drummond n'est pas venu sans prix, il fut un homme notoirement excentrique qui a souffert de schizophrénie, son comportement erratique lui ayant valu le surnom de "Don Cosmic" de la part de Dodd. Il était fréquent que lors de concerts, il s'arrête de jouer et rester immobile sur la scène devant un public remuant. Il est l'un des premiers musiciens rasta et fréquente les groundations de Count Ossie. Ses titres "King Solomon", "African Beat", "Mesopotamia" ou "Addis Ababa" rendent hommage à l'Afrique et quant à "Marcus Junior" et "Garvey Burial", ils témoignent de sa foi.

Drummond est devenu par la suite un des chefs créateurs et spirituels des Skatalites. En 1964, le Ska inonde les ondes hertziennes jamaïcaines tout au long de l'année, et il participe à des dizaines de séances d'enregistrement. Les Skatalites accompagneront les stars jamaïcaines de l'époque tel que Joe Higgs ou Jackie Opel mais également une nouvelle génération, où l'on retrouve, Delroy Wilson, The Wailers, Lee 'Scratch' Perry et Ken Boothe. 

"Man in the Street", composition de Drummond, rentre dans le Top 10 au Royaume-Uni en 1964, et un an après, son adaptation du thème du film "Guns of Navarone" réussit le même exploit en Angleterre.

Le nouvel an 1965, il se rend au commissariat pour le meurtre de sa femme, une danseuse nommée Anita 'Margarita' Mahfood (que l'on peut entendre sur le titre "Woman A Come"), poignardée. Son corps a été trouvé dans sa maison, la victime avait de multiples blessures causées par des coups violents. Drummond a été considéré comme non-responsable de ses actes par le tribunal et interné au Bellevue Hospital. 

Il meurt le à l'âge de 37 ans. Sa mort a suscité d'étranges rumeurs (notamment une vengeance de la famille de sa concubine ou un suicide); la version officielle était mort naturelle. Heather Augustyn, auteur d'une biographie exhaustive de Don Drummond parue à l'été 2013, affirme avoir la preuve que la mort de Don Drummond fut causée par les médicaments administrés à l'hôpital de Bellevue et qu'il était impossible que la famille d'Anita Mahfood ait été derrière un meurtre de vengeance.



</doc>
<doc id="997" url="https://fr.wikipedia.org/wiki?curid=997" title="Doctrine">
Doctrine

Une doctrine (mot attesté en 1160, du latin "doctrina", « enseignement », « théorie », « méthode », « doctrine ») est un ensemble global de conceptions d'ordre théorique enseignées comme vraies par un auteur ou un groupe d'auteurs. 

Les doctrines peuvent être considérées quelquefois comme fallacieuses, sophistiques, et ou dogmatiques, de par leur origine religieuse ou mythologique.

Elle a une dimension idéologique et elle peut être d'ordre politique, juridique, économique, religieuse, philosophique, scientifique, sociale, militaire

Dans le domaine militaire, politique, diplomatique, et du management, on appelle par extension doctrine les principes de base sur lesquels s'appuient une stratégie et des plans d'actions.


En matière économique, les doctrines peuvent se manifester dans différentes théories économiques, ou en intelligence économique. 







</doc>
<doc id="998" url="https://fr.wikipedia.org/wiki?curid=998" title="Dennis Tito">
Dennis Tito

Dennis Tito est un homme d'affaires californien d'origine italienne et un millionnaire américain, né le à New York, connu pour avoir été le premier « touriste de l'espace ». C'est à l'âge de 17 ans que sa passion pour le cosmos est née, avec l'envoi du satellite Spoutnik dans l'espace, en 1957. Il a ensuite travaillé pour la NASA au calcul des trajectoires. Il s'est finalement tourné vers les affaires : il a fondé son entreprise de conseil en technologies en 1972, et est aujourd'hui à la tête d'une fortune s'élevant à 200 millions de dollars. Pour un montant de 20 millions de dollars octroyé à l'agence spatiale fédérale russe, l'Américain a réalisé son rêve, après un rude entraînement à la Cité des étoiles de Moscou.

Le , il devint le premier « touriste de l'espace » à bord de la mission Soyouz TM-32. Le vol dura 7 jours, 22 heures et 4 minutes et comporta un arrimage à la station spatiale internationale.

Dennis Tito a annoncé en 2013 le projet Inspiration Mars.



</doc>
<doc id="1004" url="https://fr.wikipedia.org/wiki?curid=1004" title="Espéranto">
Espéranto

L’espéranto est une langue construite internationale utilisée comme langue véhiculaire par des personnes provenant d’au moins 120 pays à travers le monde, y compris comme langue maternelle. N’étant la langue officielle d’aucun État, l'espéranto permet d'établir un pont neutre entre cultures ; certains locuteurs nomment « Espérantie » la zone linguistique formée des lieux géographiques où ils se trouvent. Nécessitant un court apprentissage pour être utilisable, l'espéranto est ainsi présenté comme solution efficace et économiquement équitable au problème de communication entre personnes de langues maternelles différentes.

Fondée sur une grammaire régulière sans exception, l'espéranto est une langue globalement agglutinante où les mots se combinent pour former un vocabulaire riche et précis à partir d'un nombre limité de racines lexicales et d’affixes. Ces particularités la rendent flexible et facilitent son apprentissage à tout âge. L’Académie d'espéranto contrôle entre autres l'introduction de mots découlant d'inventions ou de notions nouvelles et l’Association mondiale anationale publie le "Plena Ilustrita Vortaro de Esperanto", dictionnaire tout en espéranto le plus vaste et reconnu internationalement.

C’est en 1887 que Ludwik Zamenhof, sous le pseudonyme (Docteur qui espère) qui donnera par la suite son nom à la langue, publie le projet "Langue Internationale". La langue a connu un rapide développement dès les premières années, donnant lieu à des publications et des rencontres internationales. L'apparition des premières méthodes d'apprentissage en ligne au début des années 2000, puis de cours d'espéranto sur des sites d'apprentissage de masse comme sur Duolingo en 2015 suscitent un regain d’intérêt pour l'espéranto.

L’Association mondiale d’espéranto, fondée en 1908, est en relation officielle avec l’Organisation des Nations unies et l’UNESCO, qui a publié des recommandations en faveur de l’Espéranto en 1954 et 1985. L’espéranto est l’une des langues officielles de l’Académie internationale des sciences de Saint-Marin. L’université Loránd Eötvös en Hongrie sanctionne son cursus universitaire d’enseignement de l’espéranto par un diplôme reconnu par le cadre européen commun de référence pour les langues. Plusieurs universités proposent des cycles d’études espérantophones au Brésil, en Bulgarie, Pologne, Roumanie et Slovaquie.

Espéranto-France a lancé une préparation à une future épreuve écrite d'espéranto comme langue facultative au baccalauréat français et propose aux lycéens intéressés de passer un bac blanc d'espéranto. Le premier examen blanc de ce type a eu lieu le samedi 4 juin 2016 ; cependant la date d’introduction de l'espéranto dans la liste des langues facultatives au baccalauréat dépend d’une décision du ministère de l’Éducation nationale. Le 12 avril 2017, la directrice générale de l’enseignement scolaire Florence Robine précise par une lettre qu’« "il est tout à fait possible d’entreprendre, dans les établissements où l’enseignement de l’espéranto pourrait se développer, une démarche expérimentale à l’échelle locale" ».

L’espéranto est l'une des langues officielles de l'Académie internationale des sciences de Saint-Marin (AIS) dont le but est de favoriser l'utilisation de l'espéranto dans toutes les sciences. Parmi les universités disposant de cycles d'études espérantophones, les plus réputées sont :
Concernant l’Université Adam-Mickiewicz de Poznań, des cours d’interlinguistique sont dispensés depuis 1997 dans le cadre de la faculté de philologie de, et un cursus de trois ans en espéranto est proposé. Il valide des crédits ECTS. C’est Ilona Koutny, membre de l’Académie d'espéranto, qui guide ce cursus.

À l'instar des autres langues, l’espéranto dispose de diplômes validant les acquis, mais seul l'institut des langues étrangères (ITK) de l'université Loránd Eötvös (ELTE) délivre des diplômes d'État sur la base du cadre européen commun de référence pour les langues (CECRL) : niveaux B1, B2 et C1. Il est à noter que parmi les trente langues proposées par ITK, l'espéranto se classe en par le nombre de candidats, après l'anglais et l'allemand. De son côté, la Ligue internationale des enseignants d'espéranto (ILEI) agit pour la promotion de l’apprentissage et propose des ressources pédagogiques aux enseignants d’espéranto.

L'UNESCO a adopté plusieurs recommandations en faveur de l'espéranto. La première a eu lieu le lors de la générale à Montevideo (Uruguay). De plus, la revue "Le Courrier de l'Unesco" est disponible en espéranto depuis 2017. En 1980, l’organisation mondiale du tourisme a souligné à Manille « "l’importance de connaitre des langues, notamment celles à vocation internationale comme l’espéranto" ».

L'espéranto n'est la langue officielle d'aucun pays, mais il est la langue de travail de plusieurs associations à but non lucratif, principalement des associations d'espéranto. La plus grande organisation d'espéranto est l'association mondiale d’espéranto, qui est en relation officielle avec les Nations unies et l'UNESCO dans un rôle consultatif.

Le nombre d'espérantophones est difficile à évaluer. Les estimations varient entre cent mille et dix millions. Deux millions est le nombre le plus couramment repris, voire jusqu'à trois millions. Toutefois, on peut affirmer en 2015 qu'il y a 120 pays dans lesquels se trouvent des espérantophones.

Étant une langue construite, l'espéranto est généralement appris comme langue seconde, et très souvent en autodidacte par une méthode ou un cours en ligne. Il existe cependant un certain nombre d'espérantophones natifs. Le linguiste finlandais estime leur nombre à .

Jouko Lindstedt évalue par l'échelle suivante la capacité à parler l'espéranto dans la communauté espérantophone :

Sidney S. Culbert, ancien professeur de psychologie de l'université de Washington, espérantophone lui-même, est arrivé, en comptabilisant pendant vingt ans dans de nombreux pays les espérantophones à l'aide d'une méthode par échantillonnage, à une estimation de 1,6 million de personnes parlant l'espéranto avec un niveau professionnel. Ses travaux ne concernaient pas que l'espéranto et faisaient partie de sa liste d'estimation des langues parlées par plus d'un million de personnes, liste publiée annuellement dans le "". Comme dans "l'Almanach", toutes ses estimations étaient arrondies au million le plus proche, c'est le nombre de deux millions d'espérantophones qui a été retenu et fréquemment repris depuis. Culbert n'a jamais publié de résultats intermédiaires détaillés pour une région ou un pays particulier, ce qui rend difficile l'analyse de la pertinence de ses résultats.

En tant que langue construite, l'espéranto n'est généalogiquement rattaché à aucune famille de langues vivantes. Cependant, une part de sa grammaire et l'essentiel de son vocabulaire portent à le rattacher aux langues indo-européennes. Ce groupe linguistique a constitué le répertoire de base à partir duquel Ludwik Lejzer Zamenhof a puisé les racines de la langue internationale.

Cependant, la typologie morphologique de l'espéranto l'écarte significativement des langues indo-européennes, qui sont largement à dominante flexionnelle. En effet, il consiste en monèmes invariables qui se combinent sans restriction, ce qui l'apparente aux langues isolantes. En espéranto, comme en chinois, on dérive « mon » "(mia)", de « je » "(mi)" et « premier » "(unua)" de « un » "(unu)". Sa tendance à accumuler, sans en brouiller les limites, des morphèmes porteurs d'un trait grammatical distinct le rapproche aussi des langues agglutinantes.

L'espéranto possède vingt-huit phonèmes : cinq voyelles et vingt-trois consonnes. Ils sont transcrits au moyen d'un alphabet de vingt-huit lettres : vingt-deux lettres de l'alphabet latin ("q", "w", "x" et "y" ne sont pas utilisés, sauf dans les expressions mathématiques), et six lettres utilisant deux diacritiques (accent circonflexe et brève), propres à l'espéranto : "ĉ", "ĝ", "ĥ", "ĵ", "ŝ", "ŭ". L'orthographe est parfaitement phonologique : chaque lettre représente invariablement et exclusivement un seul phonème.

En plus de leur rôle premier de transcription, les lettres diacritées rappellent en espéranto l’orthographe ou la prononciation d’autres langues. Par exemple, "poŝto" « poste », rappelle graphiquement et phonétiquement le mot "pošta" du tchèque, du slovaque, du slovène, du serbo-croate, mais aussi par la graphie les mots français, anglais, néerlandais, allemand "poste, post, post, Post", et par le son le bulgare "поща" (prononcé ). L'espéranto aboutit souvent ainsi à un juste milieu rappelant plusieurs langues sources : ainsi "ĝardeno" rappelle le français "jardin", l'allemand "Garten", le néerlandais "gaarden", l'italien "giardino" et l'anglais "garden".

La langue comporte un accent tonique toujours situé sur l'avant-dernière syllabe des mots. Le système vocalique comporte cinq timbres : "a e i o u", correspondant aux valeurs du français "â é i ô ou", comme dans de nombreuses langues, sans distinction de quantité.
Le cadre sonore ci-après, permet d'écouter un court extrait du . Cet extrait lu par Claude Piron a été enregistré lors de la rencontre commémorative de 2005 à Boulogne-sur-Mer. Ces extraits sont reproduits et traduits dans la "".

Les lettres diacritées peuvent poser quelques problèmes typographiques à l'imprimerie ou l'informatique (plus particulièrement avec les systèmes informatiques anciens). Pour les francophones, le clavier BÉPO et la variante Xorg du clavier AZERTY permettent d’accéder de façon native aux caractères accentués de l'espéranto. Des logiciels peuvent également être installés pour faciliter la frappe des six lettres diacritées.

La grammaire de l'espéranto se fonde sur seize principes énoncés dans le "Fundamento de Esperanto", adopté comme référence intangible au premier congrès mondial d’espéranto de Boulogne-sur-Mer en 1905. Ils ne constituent cependant qu'un cadre dans lequel ont été progressivement dégagées des règles plus détaillées.

Chaque radical peut recevoir des morphèmes invariables signalant chacun un trait grammatical précis : —o pour les substantifs, —a pour les adjectifs, —e pour les adverbes dérivés, —j pour le pluriel et —n pour le cas accusatif.

La régularité de la langue permet d’en expliquer sa grammaire de façon aisée sans avoir recours à la terminologie technique habituelle, parfois difficile pour certains néophytes. L’ouvrage "Plena Manlibro de Esperanta Gramatiko" propose ainsi un panorama complet de la grammaire espérantophone sans vocabulaire complexe.

Les verbes se caractérisent par une série de marques qui forment une conjugaison mêlant des valeurs temporelles et modales : —i pour l'infinitif, —is pour le passé, —as pour le présent, —os pour le futur, —us pour le conditionnel, —u pour le volitif.

L'espéranto utilise également comme déterminants un ensemble de pronoms-adjectifs assemblés systématiquement à partir d'une initiale et d'une finale caractéristiques :

D'autres finales produisent des adverbes circonstanciels : -e (lieu), -am (temps), -el (manière), -al (cause), -om (quantité). Les mots formés sur ces bases sont désignés collectivement comme corrélatifs ou (en espéranto même) "tabel-vortoj".

Par exemple :

L'espéranto recourt également à diverses particules invariables dans l'organisation de la phrase : il s'agit de conjonctions de coordination (kaj « et », aŭ « ou », do « donc », sed « mais »…) ou de subordination (ke « que », ĉar « parce que », dum « pendant que », se « si »…) qui précisent les rapports entre propositions, et des adverbes simples à valeur spatiale, temporelle, logique ou modale. Par exemple, ne marque la négation, et ĉu marque l'interrogation globale.

L'ordre des mots est relativement libre en espéranto : grâce à la marque -n du complément d'objet (accusatif), toutes les constructions (SOV, VSO, OSV, etc.) sont acceptées ; l’ordre le plus fréquent est toutefois "sujet-verbe-objet" suivi du complément circonstanciel. L'usage d'autres dispositions est courant notamment en cas de mise en relief afin de placer l'élément le plus important en début de phrase. Il existe cependant certaines règles et tendances bien établies :

D'une manière générale, on peut dire que l'ordre des syntagmes est libre mais que la disposition des morphèmes à l'intérieur d'un syntagme est fixée par l'usage.

Certaines tendances expressives peuvent sembler peu communes par rapport à l'usage du français :

Du fait de l'absence de restriction sur la combinaison des monèmes, une même phrase peut se formuler de multiples façons :

L'espéranto peut ainsi alternativement se montrer synthétique ou analytique.

Au même titre que le français tire ses racines en partie du latin et du grec et qu’il emprunte à l’anglais ou à d’autres langues, l’espéranto est une langue construite "a posteriori" : elle tire ses bases lexicales de langues existantes. Les principales sources sont, par importance décroissante :

Les mots provenant d'autres langues désignent surtout des réalités culturelles spécifiques: "boaco" « renne » (du same), "jogo" « yoga » (du sanskrit), "haŝioj" « baguettes (pour manger) » (du japonais), etc.

Les morphèmes grammaticaux doivent beaucoup au latin et dans une moindre mesure au grec ancien. Une partie est construite "a priori" sans référence évidente à des langues existantes, ou profondément remaniée à partir d'éléments rappelant ceux de langues préexistantes, comme la série des corrélatifs.

Zamenhof a suivi diverses méthodes pour adapter ses sources lexicales à l'espéranto : adaptation phonétique orthographique, à partir de la prononciation (ex. "trotuaro" du français "trottoir") ou à partir de la forme écrite (ex. "birdo" de l’anglais "bird" « oiseau »). Lorsque plusieurs de ses sources comportaient des mots proches par la forme et le sens, Zamenhof a souvent créé un moyen terme (ex. "ĉefo" « chef »).

Le vocabulaire de l'espéranto comprenait quelques centaines de radicaux dans le "Fundamento de Esperanto" de 1905. En 2002, après un siècle d'usage, le plus grand dictionnaire monolingue en espéranto ("Plena Ilustrita Vortaro de Esperanto"), en comprend correspondant à éléments lexicaux.

La formation des mots espéranto est traditionnellement décrite en termes de dérivation lexicale par affixes et de composition. Cette distinction est cependant relative, dans la mesure où les « affixes » sont susceptibles de s’employer aussi comme radicaux indépendants : ainsi le diminutif forme l’adjectif « petit (avec idée de faiblesse) », le collectif forme le nom « groupe », le causatif forme le verbe « faire, rendre »

Les deux principes essentiels de formation des mots sont :

En théorie, il n’existe pas d’autre limite que sémantique à la combinatoire des radicaux. Il en résulte un certain schématisme qui aboutit à la formation systématique de longues séries sur le même modèle, parfois sans équivalent direct dans d’autres langues. Par exemple :

Ce schématisme a pour effet de diminuer le nombre de radicaux nécessaires à l’expression au profit de dérivés, réduisant ainsi la composante immotivée du lexique. Le procédé pouvant parfois paraître lourd, la langue littéraire a cependant introduit quelques radicaux alternatifs à titre de variantes stylistiques : par exemple « vieux » peut doubler (formé sur « jeune ») ou (formé sur « neuf, nouveau »). L’usage courant tend cependant à préférer les dérivés.

Le système de dérivation s’adapte aisément aux besoins en mots nouveaux. Ainsi, du mot (« réseau, filet »), on a extrait le radical pour former tout un ensemble de mots liés à Internet : (« adresse de courriel »), (« pirate informatique »)

Le tableau ci-dessous présente quelques mots et phrases ainsi que leurs transcriptions en alphabet phonétique international :

La akcent' estas sur la antaŭlasta silab'. La kern' de la silab' formas vokal'. Vokalj" ludas granda rol' en la ritm' de la parol'. Substantivj" finas per ', adjektivj" per -a. La sign' de la plural' estas "-j". La plural' de « lasta vort' » estas « lastaj" vort"'j" ».

« -o » = substantifs« -a » = adjectifs« -j » = pluriel« -n » = accusatif

Traduction :" L'accent est sur l'avant-dernière syllabe. Le cœur de la syllabe est formé par une voyelle. Les voyelles jouent un grand rôle dans le rythme de la parole. Les substantifs finissent par "-o", les adjectifs par "-a". La marque du pluriel est "-j". Le pluriel de « "lasta vorto" » (« dernier mot ») est « "lastaj vortoj" »".

L'idée d'une langue équitable pour la communication internationale germa à Białystok au cours des années 1870, dans la tête d'un enfant polonais nommé Ludwik Lejzer Zamenhof. Quelques années plus tard, à l'âge de 19 ans, il ébaucha son premier projet qu'il présenta à ses camarades de lycée. Ce n'est qu'après ses études en ophtalmologie qu'il publia en langue russe, à Varsovie, le 26 juillet 1887, l'ouvrage "Langue Internationale", premier manuel d'apprentissage. Il fut suivi au cours des deux années suivantes de versions dans plusieurs autres langues. Dans ce manuel, Zamenhof avait défini ainsi le but de la Langue Internationale : « "Qu’on puisse l’apprendre, comme qui dirait, en passant [et] aussitôt en profiter pour se faire comprendre des personnes de différentes nations, soit qu’elle trouve l’approbation universelle, soit qu’elle ne la trouve pas [et que l’on trouve] les moyens de surmonter l’indifférence de la plupart des hommes, et de forcer les masses à faire usage de la langue présentée, comme d’une langue vivante, mais non pas uniquement à l’aide du dictionnaire." ».

Très vite, l'espéranto rencontra un vif succès, dépassant même les espérances de son initiateur. Le nombre de personnes qui apprirent la langue augmenta rapidement, au départ principalement dans la Russie impériale et en Europe de l'Est, ensuite en Europe occidentale et aux Amériques. L'espéranto pénétra au Japon à la suite de la guerre russo-japonaise de 1904-1905. En Chine, les premiers cours furent donnés à Shanghai dès 1906 et à Canton dès 1908. Durant ces premières années, l'espéranto fut essentiellement une langue écrite, les échanges se faisant essentiellement par correspondance et par l'intermédiaire de périodiques spécialisés.

Le premier congrès mondial d’espéranto se déroula en 1905 à Boulogne-sur-Mer. Ce premier congrès marqua un tournant important pour l'espéranto. La langue qui était jusqu'alors essentiellement écrite fut dès lors de plus en plus utilisée pour des échanges directs, notamment lors de rencontres internationales et des congrès qui se déroulent depuis chaque année, mis à part les interruptions dues aux deux guerres mondiales. C'est au cours du premier congrès de 1905 que fut publié le "Fundamento de Esperanto" fixant les bases de la langue.

La Première Guerre mondiale mit un frein au développement de l'espéranto, qui reprit cependant au cours des années 1920 dans l’enthousiasme généré par les espoirs de paix issus de la création de la Société des Nations. Mais les années 1930 avec la montée en puissance des régimes totalitaires, puis la Seconde Guerre mondiale marquèrent un nouveau coup d'arrêt au développement de l'espéranto.

Malgré des conditions difficiles liées aux bouleversements politiques de l'après-guerre, l'apprentissage de l'espéranto a redémarré à partir des années 1950 essentiellement grâce à l'apparition de nombreuses associations et clubs d'espéranto. Au cours de cette deuxième moitié du , les publications en espéranto connaissent un certain succès et les rencontres espérantophones se multiplient.

C'est surtout avec la généralisation d'Internet et à l'initiative de jeunes espérantophones que les années 2000 sont le début d'un renouveau de l'espéranto. Des méthodes d'apprentissage en ligne souvent gratuites sont apparues et de nouveaux usages se sont développés au travers des réseaux sociaux et des échanges directs.

L'apprentissage de l'espéranto repose en grande partie sur l'utilisation de méthodes autodidactes ou de cours traditionnels via des associations ou des clubs locaux. Toutefois quelques établissements d'enseignement ont introduit des cours d'espéranto à leur programme.

Au début des années 2000, l'apparition de méthodes d'apprentissage en ligne de l'espéranto, souvent gratuites, les plus connues étant "lernu!" et " (kurso.com.br)", a permis de toucher un public nouveau, en particulier parmi les jeunes. Le 28 mai 2015, le site d’apprentissage de langues en ligne, Duolingo, met en ligne la version bêta d'apprentissage de l'espéranto. En 2017, la méthode compte plus d’un million d’apprenants.

Les tests de niveaux en espéranto sont organisés suivant deux filières :

Actuellement seul l'institut des langues de l'université Eötvös Loránd (Budapest, Hongrie) délivre des diplômes officiels de connaissance de l'espéranto. Depuis 2009, ces diplômes sont fondés sur le cadre européen commun de référence pour les langues (CECR) et disponibles dans les niveaux B1, B2 et C1. Près de personnes possèdent un tel diplôme à travers le monde : en 2017, environ 570 au niveau B1, 590 au B2 et 820 au C1.

La Ligue internationale des enseignants d'espéranto (ILEI) propose quant à elle des examens qui testent non seulement la maîtrise de la langue, mais également la connaissance de la culture véhiculée par l’espéranto : associations, principaux acteurs, Espérantie, etc.

L’Institut de pédagogie cybernétique de Paderborn (Allemagne) a comparé les durées d’apprentissage de plusieurs groupes d’élèves francophones, de niveau baccalauréat, pour atteindre un niveau comparable dans quatre langues différentes : l’espéranto, l’anglais, l’allemand et l’italien. Les résultats sont les suivants : pour atteindre ce niveau, heures d’études de l’allemand produisaient un niveau linguistique équivalent à heures d’étude de l’anglais, heures d’étude de l’italien et 150 heures d’étude de l’espéranto.

Ces études furent reprises et confirmées par d'autres études dans le rapport remis au ministère italien de l'enseignement public (ministère de l'instruction), ainsi que dans le Rapport Grin.

Cette facilité de l'espéranto fut constatée par Inazō Nitobe, membre de l’Académie Impériale du Japon, homme de science, Secrétaire général adjoint de la Société des Nations, qui avait participé au congrès mondial d’espéranto de Prague en 1921 pour se rendre compte par lui-même de l’efficacité de cette langue. Dans un rapport intitulé "Esperanto as an International Auxiliary Language" (L’espéranto comme langue auxiliaire internationale), publié en 1922, il avait écrit : « On peut affirmer avec une certitude absolue que l’espéranto est de huit à dix fois plus facile que n’importe quelle langue étrangère et qu’il est possible d’acquérir une parfaite élocution sans quitter son propre pays. Ceci est en soi un résultat très appréciable. ».

Lorsque l'on a déjà appris une langue étrangère, l'apprentissage d'une nouvelle langue étrangère est plus facile, d'où l'intérêt de commencer par une langue étrangère facile. Des études menées sur des échantillons comparatifs d'élèves ont montré que les élèves qui avaient d'abord étudié l'espéranto avant de passer à l'étude d'une langue étrangère atteignaient un meilleur niveau, dans cette langue, que le groupe témoin qui pendant la même durée n'avait étudié que cette langue étrangère.

Du point de vue de la graphie, l’espéranto fait partie des langues dites « transparentes » : comme pour le croate, le serbe, l'espagnol, l'italien, le slovène ou le tchèque, la correspondance entre graphèmes et phonèmes est simple, stable et régulière. Une langue complètement transparente suit deux principes : à un phonème correspond une seule graphie ; à une seule graphie correspond un seul phonème. À l’opposé, les langues dites « opaques » comme l'anglais ou « semi-opaques » comme le français ont des règles de correspondance grapho-phonémique complexes et irrégulières.

Un dyslexique utilisant une langue « opaque » devient souvent dysorthographique. Il est préférable de choisir l'apprentissage d’une langue transparente pour faciliter l'apprentissage des langues chez les enfants dyslexiques.

D'autre part, l'espéranto peut aider grâce à sa construction signalant pour chaque mot un trait grammatical précis, à faire comprendre les liens entre la « fonction dans la phrase » et l'« orthographe grammaticale » de chaque mot.

L'espéranto est soutenu par un réseau de militants regroupés dans de nombreuses associations. Au niveau international, ce réseau d'associations nationales et d'associations thématiques est fédéré par l'association mondiale d’espéranto.

L'ensemble des militants favorables à l'espéranto est souvent désigné comme "Le mouvement espérantophone" ou même tout simplement "Le mouvement espérantiste". Toutefois cette appellation est trompeuse dans la mesure où les espérantophones ne constituent pas un ensemble homogène. Dans les faits, les motivations, les aspirations et les idées des espérantophones reflètent la diversité des opinions présentes dans le monde. Il est également à noter que seule une minorité d’espérantophones sont membres d'associations d'espéranto.

De façon générale, l'essentiel du militantisme consiste à promouvoir l'apprentissage de l'espéranto et son usage dans la communication internationale. La défense de cet objectif s'appuie sur différentes études et rapports montrant les avantages de l’espéranto pour cet usage :

Un exemple de cet objectif militant est l'apparition récente du mouvement Europe Démocratie Espéranto qui promeut l'usage de l'espéranto comme langue commune équitable en Europe en complément des langues officielles.

En septembre 2017, le ministère français de l’Éducation nationale accepte que l’espéranto puisse être enseigné à titre expérimental. Cette décision fait suite à une demande de militants encouragés par le résultat considéré par ces mêmes militants comme un succès d'une pétition lancée par des associations pro-espéranto pour son ajout comme langue optionnelle au bac, qui avait recueilli signatures, mais qui n'est de la part du ministère, que l'application de dispositions générales concernant n'importe quelle langue.

L’espéranto a longtemps été une langue plus écrite que parlée. Dès le début, toutefois, son usage oral a été assuré par les clubs d'espéranto, disséminés un peu partout en Europe, en Asie orientale et dans quelques pays d'Amérique. Les personnes intéressées s'y retrouvaient une fois par semaine ou par mois pour pratiquer la langue et accueillir des voyageurs étrangers qui l'avaient apprise. Au début du sont apparus de nombreux écrivains, hommes et femmes, poètes…, qui, ayant adopté l'espéranto comme langue de leurs écrits, lui ont donné sa littérature. Dans la résistance à l'occupation japonaise, des artistes coréens, notamment des réalisateurs qui seront à l'origine du cinéma nord-coréen, choisissent ainsi de se regrouper en 1925 dans une association ayant choisi un nom en espéranto : la Korea Artista Proletaria Federacio (KAPF), ou Association coréenne des artistes prolétariens.

En fait, l'usage oral de la langue, de la simple conversation à la musique, s'est surtout développé lorsque les voyages sont devenus plus accessibles et que les rencontres internationales espérantophones se sont multipliées. La mise en place de services d'hébergement chez l'habitant, comme le Pasporta Servo, et l'apparition de l'enregistrement sonore sur cassette, de même que les programmes de conversation téléphonique par ordinateur (voix sur IP), ont contribué à faire progresser l'utilisation orale de la langue. Avec l'Internet, l'espéranto a trouvé un nouveau vecteur de communication, tant pour la langue écrite que pour la langue parlée. Il est à noter que la version de Wikipédia en espéranto a dépassé le cap de articles le 13 août 2014. , une application mobile gratuite destinée à faciliter les contacts entre espérantophones est lancée le 22 avril 2017.

Il faut également noter qu'avec l'accroissement du nombre de locuteurs, l'espéranto est devenu la langue maternelle d'enfants issus de couples espérantophones.

En défendant son idée à travers l’Europe, le Docteur Zamenhof s'est attiré la sympathie de nombreuses personnalités politiques, telles que Gandhi ainsi que la communauté internationale du Bahaïsme.

La littérature en espéranto se compose à la fois d'œuvres originales et d'œuvres traduites. Quelques ouvrages originaux :


Parmi les œuvres traduites, on trouve des ouvrages aussi divers que "Le Petit Prince", la Bible, le Coran, le Manifeste du parti communiste, une biographie du peintre Camille Pissarro par son fils Ludovic-Rodo.

La majorité des ventes d'ouvrages en espéranto est réalisée par les associations espérantophones. L'une des plus importantes librairies d'ouvrages en espéranto est le "libro-servo" de l’association mondiale d’espéranto, qui compte plus de références.

Il existe de nombreuses publications originales en espéranto. Parmi les plus connues, on trouve :

À côté de la presse papier, on trouve un certain nombre de sites de presse uniquement disponibles en version électronique. Les plus connus sont :

Deux revues paraissent en France et en français : "Le Monde de l'Espéranto" et "Espéranto info".

La musique espérantophone est presque aussi ancienne que l'espéranto. "La Espero", qui deviendra l'hymne du mouvement espérantophone, a été écrit par Zamenhof, peu après la publication du premier manuel, Langue Internationale, paru en 1887.

La musique espérantophone a suivi les évolutions technologiques, avec l’apparition des premiers vinyles dans les années 1960, puis l'apparition des musiques rock dans les années 1980, puis des disques compacts dans les années 1990 et enfin des formats électroniques téléchargeables via Internet depuis les années 2000.

Actuellement les musiciens espérantistes les plus connus sont JoMo (Jean-Marc Leclercq) de France, Martin Wiese de Suède, Jonny M d’Allemagne, Kim J. Henriksen du Danemark, Ĵomart et Nataŝa du Kazakhstan et de Russie, Georgo Handzlik de Pologne, les groupes Kajto dont les membres principaux sont néerlandais, Dolchamar de Finlande, de France, La Perdita Generacio de Suède.

La musique espérantophone est naturellement mise en scène lors des différentes rencontres internationales.

Les premières émissions de radio en Espéranto datent de 1922 et furent émises à Newark () et Londres (Royaume-Uni). En 2012, les émissions sont principalement des podcasts, mais certaines radios diffusent une émission hebdomadaire comme Radio Havana Cuba, ou Radio libertaire à Paris.

La première radio diffusant entièrement en espéranto, Muzaiko, est apparue le juillet 2011. Elle émet sur Internet grâce à la technologie de lecture en continu. Son programme se compose de musique espérantophone, d'interviews, et d'informations généralistes.

Sur les bandes radioamateurs l'espéranto est utilisé aux fréquences :

L'essentiel des films tournés originellement en espéranto sont des courts métrages.

Seuls trois longs métrages ont été tournés directement en espéranto :

Plusieurs films ont par ailleurs été doublés ou sous-titrés en espéranto.

Dans le film de Charlie Chaplin, "Le Dictateur", les plaques des magasins du ghetto juif sont en espéranto, catalogué comme « langue juive internationale » par Hitler dans "Mein Kampf".

Il est aussi possible d'entendre de l'espéranto dans la version originale du film "Bienvenue à Gattaca". En effet, les haut-parleurs de l'entreprise dans laquelle travaille le protagoniste de l'histoire font les annonces d'abord en espéranto puis en anglais.

Bien qu'il soit couramment utilisé dans un contexte associatif, l'usage de l'espéranto dans un contexte professionnel est jusqu'à présent resté relativement limité. On peut citer l'exemple de l'association Réinsertion et Espéranto qui de 1997 à 2008 forma et embaucha en CDI à Montpellier des jeunes chômeurs avec l'espéranto dans le cadre du dispositif Emplois-jeunes.

Afin d'encourager l'usage de l'espéranto, dans un contexte professionnel, quelques chefs d'entreprises se sont regroupés au sein d'une association, "Entreprise-Esperanto" dont l'objectif est d'accompagner les entreprises ayant des besoins de communication internationale et qui souhaitent utiliser l'espéranto.

Dès l'origine de l'espéranto, des propositions de réformes de la langue sont proposées, y compris par Zamenhof lui-même. Cependant la communauté espérantophone fut toujours très réticente à de telles réformes et tous les projets échouèrent.

De fait, le projet de réforme le plus connu est celui qui fut présenté par Louis de Beaufront et Louis Couturat en 1908. À l'époque, il provoqua une crise au sein du mouvement espérantophone. Les partisans de ce projet quittèrent le mouvement pour créer une nouvelle langue construite : l'ido. Au , l’ido ne compte que quelques centaines de locuteurs, même si on trouve quelques sites en ido, dont Wikipédia.

D'autres propositions de réforme d'ampleur plus limitée virent le jour ultérieurement notamment le riisme, mais n'obtinrent que des soutiens limités.

De fait l'espéranto parlé aujourd'hui est très proche de ce qu'il était à l'origine.

Le nom "espéranto" fonctionne comme un nom propre quand il désigne la langue même, mais est parfois utilisé comme nom commun (dans une sorte d'antonomase) pour représenter une langue commune ou un moyen commun dans un domaine donné où cette mise en commun ne va pas de soi. Cette utilisation du mot "espéranto" peut aussi bien être prise dans un sens positif que dans un sens négatif.

Dans le domaine de l'informatique, Java fut qualifié d'« espéranto des langages de programmation », en particulier à cause de sa simplicité et de son universalité (indépendance par rapport au système d'exploitation), métaphore reprise pour XML, qualifié à son tour "d'espéranto du système d'information".

En Allemagne et en Autriche, les opposants à l'euro le décrivirent comme "Esperantogeld" ou "Esperantowährung" ("Geld" = « argent » ; "Währung" = « Monnaie »), voulant dire par là qu'un tel projet international était intrinsèquement voué à l'échec.









</doc>
<doc id="1005" url="https://fr.wikipedia.org/wiki?curid=1005" title="Eure">
Eure


</doc>
<doc id="1006" url="https://fr.wikipedia.org/wiki?curid=1006" title="Eure-et-Loir">
Eure-et-Loir

L'Eure-et-Loir est un département français situé dans la région Centre-Val de Loire qui en compte six.

Il tire son nom des deux principales rivières qui le traversent, l'Eure, affluent de la Seine, et le Loir, affluent de la Sarthe. L'Insee et la Poste lui attribuent le code 28. Sa préfecture est Chartres.

Le département a été créé à la Révolution française, le en application de la loi du , à partir, principalement, de parties des anciennes provinces de l'Orléanais (Beauce) et du Maine (Perche), mais aussi de l'Île-de-France (Drouais, Thymerais, Vallée de l'Avre, Hurepoix).

Au Moyen Âge, le territoire actuel du département est dominé par la ville de Chartres. La ville se développe grâce à la culture des riches terres de Beauce (marché au blé) et à sa vocation religieuse due notamment à la présence de la relique du Voile de la Vierge (don de Charles-le-Chauve en 876). Sur l'impulsion de Fulbert de Chartres, elle sera le berceau d'une renaissance intellectuelle avec la fondation de l'École de Chartres. Au nord, Dreux, la vallée de l'Avre et le Thimerais, de même que le comté du Perche à l'ouest, constituent des postes avancés des rois de France face aux ducs de Normandie. Les terres d'Eure-et-Loir, par leur intérêt stratégique, sont donc très tôt ancrées dans la mouvance capétienne et progressivement rattachées aux anciennes provinces de l'Orléanais et de l'Île-de-France.

Durant la guerre de Cent Ans, le territoire du département est au centre de plusieurs conflits (dont la Journée des Harengs à Rouvray-Saint-Denis), en raison de sa proximité avec Paris et Orléans. Le traité de Brétigny, qui met fin provisoirement à la guerre, y sera signé près de Chartres.

À la Renaissance, l'Eure-et-Loir devient également une région prisée par les rois François et Henri II avec la présence du château d'Anet, appartenant à une grande dame de la cour : Diane de Poitiers.

Le département est également marqué par la présence de Madame de Maintenon, la marquise de Pompadour (Crécy-Couvé), Maximilien de Béthune, duc de Sully, décédé en son château de Villebon et inhumé à Nogent-le-Rotrou.
À la Révolution, il est dans un premier temps envisagé de créer un département beauceron. La Beauce a en effet l'avantage de n'avoir jamais été une province sous l'Ancien Régime. Sa dimension essentiellement géographique et non politique s'inscrivait donc parfaitement dans l'idéologie révolutionnaire. Ce projet est mis en échec principalement par la volonté du roi de ne pas voir dispersées en un nombre trop important de départements ses possessions franciliennes. La création du département dans sa configuration actuelle tient également aux résistances des terres du Thimerais et du Drouais à se voir agrégées à celles de l'ancienne Normandie, et à l'impossibilité de conserver au Perche son unité, en partie pour des raisons politiques (ancien comté), et surtout à cause de l'absence d'une ville suffisamment importante pour se prévaloir du rang de chef-lieu de département. Chartres, avec entre autres le général Marceau, l'abbé Sieyès ou encore Brissot de Warville, chef de file des Girondins, donne plusieurs grands hommes à la Révolution.

Au , le nord du département connaît une forte industrialisation, avec notamment l’imprimerie de Firmin Didot et les manufactures textiles des Waddington. Chartres conserve essentiellement sa vocation commerciale grâce à son important marché au blé et au commerce de la laine des nombreux élevages de moutons, dont la foire de Châteaudun est aussi un haut lieu. Le Perche qui s'est peu développé, et a connu une notable émigration vers le Québec les siècles précédents, devient une terre de nourrices réputées pour leur qualités maternelles auprès des familles aisées de Paris. Dreux devient également une ville industrielle, en particulier après la crise du phylloxéra qui met définitivement à bas les vignes normandes. Après le coup d'État du 2 décembre 1851 de Napoléon III, l'Eure-et-Loir fait partie des départements placés en état de siège afin de parer à tout soulèvement massif. Moins d'une centaine d'opposants sont arrêtés. Le département est durement touché par la guerre de 1870, avec la bataille de Loigny et l'incendie de Châteaudun par l'armée prussienne.

Au , le département devient de plus en plus économiquement lié au développement de la région parisienne. Chartres et Dreux profitent pleinement de cette proximité avec l'implantation de plusieurs grandes entreprises (dont les futurs établissements Philips), pendant que la vallée de l'Eure qui les relie devient un lieu de villégiature avec la construction de nombreuses résidences secondaires. Des bases aériennes importantes s'installent à Chartres de 1909 à 1997 (base aérienne 122 Chartres-Champhol), ainsi qu'à Châteaudun de 1936 à 2014 (base aérienne 279 Châteaudun).

Durant la Seconde Guerre mondiale, le département est marqué par son préfet Jean Moulin qui y fait son premier acte de Résistance face à l'occupant. La ville de Chartres est partiellement détruite par un bombardement en 1944 qui incendie sa bibliothèque. La ville de La Loupe est quant à elle presque totalement sinistrée. Après-guerre, l'est du département intègre progressivement l'aire urbaine de Paris, les cantons d'Anet, de Maintenon et de Nogent-le-Roi, voire d'Auneau, qui sont dès lors intimement liés à ceux du département des Yvelines limitrophes.

En politique, le département est la terre d'élection et le berceau de plusieurs grandes figures des et républiques : William Henry Waddington (ministre de l’Instruction publique en 1873 et 1877), Maurice Viollette (ministre d'État sous le Front populaire), Paul Deschanel (président de la République en 1920), Maurice Bourgès-Maunoury (président du Conseil en 1957).

Au tournant des années 1980, Dreux devient une ville politiquement singulière en élisant comme maire en 1977, puis députée en 1981, Françoise Gaspard, l'une des premières femmes politiques ayant assumé publiquement son homosexualité. En 1983, Dreux est le "théâtre" d'une alliance entre la droite locale et le Front national mené par Jean-Pierre Stirbois, dont la veuve Marie-France Stirbois est élue députée en 1989.

Le blason (non officiel) du département d'Eure-et-Loir a été créé à partir de ceux du comté du Perche (partie occidentale du département) et de l'Orléanais.

Ce département fut dirigé entre 1907 et 1960 par le parti républicain, radical et radical socialiste (PRRRS), avec les présidences de Gustave Lhopiteau (1907-1920) et de Maurice Viollette (1920-1960) qui furent tous les deux ministres sous la République, le second le fut même sous le Front populaire. Le PRRRS est un parti d'idéologie républicaine et laïque qui se situait initialement à gauche de l'échiquier politique, mais qui a ensuite glissé vers le centre lorsque se développa le socialisme.

D'ailleurs c'est le parti socialiste qui prendra les rênes du Conseil général d'Eure-et-Loir en 1960, avec la présidence d'Émile Vivier qui durera jusqu'en 1976. 1976 est l'année du retour des radicaux à la tête du Conseil général, cependant ils apparaissent sous une forme différente : le parti radical de gauche, parti issu de la scission du PRRRS en 1972. Edmond Desouches puis Robert Huwart, occuperont successivement la présidence du Conseil général sous cette étiquette, et ce jusqu'en 1985.

Les élections cantonales de 1985 sont marquées par le basculement à droite du Conseil général d'Eure-et-Loir, Martial Taugourdeau membre du RPR en devient le président, il occupera ce poste jusqu'à son décès en 2001. Après un cours intérim assuré par le sénateur, Gérard Cornu, Albéric de Montgolfier alors conseiller général du canton d'Orgères-en-Beauce et membre du RPR (qui deviendra en 2002 l'UMP, puis Les Républicains en 2015), devient président du Conseil général en 2001. Il sera continuellement réélu en 2004, 2008 et 2015. Au terme des élections départementales de 2015, il bénéficie d'une très large majorité comprenant 28 conseillers généraux sur 30, parmi lesquelles se trouvent 22 élus "Les Républicains", 3 élus UDI et 3 élus DVD.

Le département d'Eure-et-Loir fut également connu pour la forte implantation qu'eut le Front National à partir des années 1980. 
Lors d'une élection municipale partielle à Dreux en 1983, le RPR et l'UDF s'allient avec le Front National pour faire basculer la mairie qui était à gauche depuis les élections municipales de 1977. Cela amène à l'élection du RPR Jean Hieaux qui administrera la ville en compagnie du Front National. 
Le Front National poursuivra sa progression avec l'élection de Marie-France Stirbois au poste de députée de la deuxième circonscription d'Eure-et-Loir, à l'occasion d'une élection législative partielle en 1989. Enfin Marie-France Stirbois obtiendra un autre mandat en devenant conseillère générale du canton de Dreux-Ouest entre 1995 et 2001. 
Cette même année marque la fin de l'implantation du parti dans le département, lorsque Marie-France Stirbois quitte le département pour les Alpes-Maritimes.

Trois des quatre principales villes du département sont dirigées par des maires de droite :

D'autres villes de moindre importance sont dirigées par des maires de droite :

Trois députés, Laure de La Raudière, Olivier Marleix et Jean-Pierre Gorges sont membres de l'UMP.

Les trois sénateurs d'Eure-et-Loir, Gérard Cornu, Albéric de Montgolfier et Chantal Deseyne, sont également membres de l'UMP.

Le Mouvement pour la France compte de son côté deux conseillers municipaux à Chartres et plusieurs élus dans des communes rurales.

Le Nouveau Centre est surtout présent dans le sud du département marqué par l'ancien président de la région Centre et député, Maurice Dousset, dont l'un des héritiers est Philippe Vigier, député-maire de Cloyes-sur-le-Loir, conseiller régional et président du Pays Dunois. Il compte également quatre conseillers généraux (Michel Boisard, Laurent Leclerc, Marc Guerrini et Dominique Leblond) et quatre présidents d'intercommunalités (communauté de communes de la Beauce de Janville, communauté de communes de la Beauce vovéenne, communauté de communes des Trois Rivières, communauté de communes de la Beauce alnéloise).

Le Parti radical, ancien parti dirigeant le département entre 1885 et 1979, compte encore plusieurs élus municipaux : Philippe Masson, maire de Brou, des conseillers municipaux à Châteaudun et un maire honoraire à Gasville-Oisème.

Le Mouvement démocrate compte un conseiller général, Jean-Pierre Gaboriau, également maire de Châteauneuf-en-Thymerais.


Elle compte 2 conseillers départementaux sur 30 : Xavier Roux et Marie-Pierre Lemaître-Lezin, élus du canton de Lucé sous l'étiquette divers gauche.

À Chartres, un des leaders de l'opposition municipale est David Lebon, ancien directeur de cabinet adjoint de Martine Aubry au Parti socialiste.

La gauche a longtemps détenu (1978-2002) la première circonscription du département dont Georges Lemoine était le député. Ancien maire de Chartres et ministre socialiste, il est membre depuis 2008 du Parti ouvrier indépendant au nom duquel il s'est présenté à l'élection législative partielle de septembre 2008 (14,51 % des suffrages) et aux élections cantonales de 2011 sur le canton de Mainvilliers.

Elle compte enfin 7 conseillers régionaux sur 12 dont trois élus socialistes (Marie-Madeleine Miallot-Muller, vice-présidente, ancien adjoint au maire d'Orléans, Annie Dubourg, Jean-Jacques Chatel), deux élus Europe Écologie-Les Vertes (Karim Laanaya, Sandra Renda), une élue communiste (Gisèle Quérité) et un élu divers-gauche (Patrick Riehl, vice-président et maire de Saint-Rémy-sur-Avre), ancien membre du Parti radical de gauche.

Le Front national compte un conseiller régional (Philippe Loiseau), depuis les élections municipales de 2014, le Front national fait partie de l'opposition aux conseils municipaux de Dreux et Chartres.

Élections présidentielles, résultats des deuxièmes tours

Élections législatives, résultats des deuxièmes tours

Élections référendaires

Élections régionales, résultats des deux meilleurs scores

Élections européennes, résultats des deux meilleurs scores


Le département d'Eure-et-Loir s’étend au sud-ouest de l’agglomération parisienne et comprend plusieurs plateaux : le Thimerais qui appartient au Perche au nord-ouest et le Drouais au nord-est, la Beauce à l'est ; le Dunois au sud qui appartiennent au Bassin parisien. Dans l’ouest du département, le relief s’élève et forme les collines du Perche, attenantes à la fois à la Normandie et aux Pays de la Loire. Le Faux Perche marque la transition entre la Beauce et le Perche. Le département a par ailleurs bénéficié de la création du parc naturel régional du Perche.

Les principales rivières du département alimentent deux bassins versants. Celui de la Seine au nord avec son affluent l'Eure et ses sous-affluents Avre et la Blaise et celui de la Loire au sud avec son affluent le Loir et ses sous-affluents Ozanne, Conie et Yerre.

La forêt, avec près (appartenant à 80 % au domaine privé et de forêts domaniales) est également présente, notamment dans le Nord-Ouest du département. Les forêts de Senonches () Dreux (), Châteauneuf () et de Montécot () abritent les massifs les plus importants. La vallée de l'Eure constitue également une trame verte et boisée qui contraste avec le plateau beauceron attenant.

L'Eure-et-Loir est limitrophe des départements de Loir-et-Cher et du Loiret, qui font également partie de la région Centre-Val de Loire. Il jouxte la région Île-de-France avec les départements de l'Essonne et des Yvelines, la région Normandie avec les départements de l'Eure et de l'Orne et enfin la région des Pays de la Loire avec le département de la Sarthe.

Schématiquement, quatre « régions » partagent le département : le Perche, au sud-ouest ; le « Faux Perche », à sa marge ; à l'est et au sud, la Beauce, avec la « Beauce chartraine » autour de Chartres ; et au nord-ouest, on trouve le Drouais-Thimerais (autour de Dreux et de Châteauneuf-en-Thymerais). Le relief et la disposition des cours d’eau dans le Perche et en Beauce peuvent se déduire à partir d’une carte.

La répartition des cours d'eau est différente : il y en a plus au sud-ouest, dans le Perche qu'en Beauce. S'il y a une petite différence de la pluviométrie, elle ne peut expliquer à elle seule cette disposition. Quoi qu'il en soit, on constate donc, que dans le Perche, l'eau ruisselle en surface et qu'en Beauce, ce n'est pas le cas. Elle s'infiltre : cela est dû à une différence géologique au niveau des roches du sous-sol. Le sous-sol du Perche est plutôt du sable et du grès, avec de l'argile à silex, et le sous-sol de Beauce, est lui, plutôt constitué de calcaire.

Concernant la géologie du Perche, la structure actuelle serait en lien avec la formation des Alpes. Lors de cet événement, les roches sédimentaires se sont plissées et cela a formé un bourrelet (anticlinal) dans la région du Perche. En effet, comme les roches du Massif armoricain sont « rigides » (il s'agit surtout de granite), elles se sont comportées comme un « mur » contre lequel se sont bloquées les roches sédimentaires du Bassin parisien, donc du Perche. Le Perche se retrouve donc plus haut que la Beauce. D’autre part, les anciennes failles hercyniennes (ici du Massif armoricain) ont rejoué et permis l’affaissement du sommet du bourrelet. Ainsi, actuellement, la zone centrale est de l’argile à silex et autour, on trouve du sable ou du grès.
La présence d'argile, de grès ou de sable permet somme toute, à l'eau de ruisseler et de se rassembler en cours d’eau. La pluviosité est relativement importante ().

Dans le calcaire de Beauce, on trouve des fossiles, notamment, des planorbes et des limnées (des espèces d’eau douce – qui existent toujours -) : il y avait donc un lac en Beauce, c’est lui qui a permis la formation de ce calcaire. On trouve aussi de l’argile à silex, par décalcification du calcaire (qui n’était donc pas pur). Le calcaire s’est dissout, l’argile et les morceaux de silex sont restés.
Ainsi, en Beauce, le calcaire se dissout facilement et permet à l’eau de ruissellement de s’infiltrer et il n'y a pas (ou peu) de rivières. De plus, la pluviosité est relativement faible ().

Le relief est différent au sud-ouest, et au nord et nord-est : vallonné dans le sud-ouest, relativement plat au nord et au nord-est. 

Les constructions anciennes montrent en général le contenu du sous-sol : en Beauce, les anciennes maisons sont en calcaire, dans le Perche, elles sont en torchis, en conglomérat appelé « grison » (morceaux de silex cimenté par du calcaire et de l'argile), en brique et en grès « roussard » (il est de couleur rousse, car assez riche en fer oxydé : la rouille).

Ces constructions sont aussi en lien avec l'activité agricole.

Les fermes sont allongées (maison d’habitation que l’on agrandit avec le temps, lorsque les besoins s'en font sentir ou lorsqu'il y a suffisamment d'argent…) : on les appelle « longère » ; les champs sont délimités par des haies. Les animaux sont « parqués » par ces haies.

Les cours des fermes sont fermées, mais les champs sont ouverts ("open field"). Cela permet de maintenir les animaux d’élevage dans la cour.

Il y a eu une diminution du nombre de parcelles (donc une modification du paysage), en Beauce et dans le Perche à la suite du remembrement entre le et 1980, pour accroître la surface et permettre l’utilisation du matériel agricole.

Le département présente un contraste climatique entre sa partie ouest et sud-ouest, humide et bocagère (qui fait partie du Perche) et sa partie sud et est, beauceronne, qui fait partie des régions les moins arrosées de France, avec le Haut-Languedoc.

L'Eure-et-Loir est un département de tradition agricole (Beauce) mais aussi en pointe dans trois filières économiques :

Le département est un acteur économique majeur dans la production de céréales et d'oléo-protéagineux en France. Son économie agricole est néanmoins très fortement dépendante de la conjoncture et de l'environnement réglementaire des marchés des grandes cultures. L'Eure-et-Loir est le premier céréalier français. Il occupe également le premier rang national pour la production de colza et de pois protéagineux. Le blé tendre est de loin la production emblématique du département. Ainsi, près de 40 % des surfaces agricoles du département sont consacrées à la culture du blé tendre, qui a généré en moyenne 29 % de la production agricole marchande du département au cours des cinq dernières années.

L'agriculture du département est également promue par le Pôle AgroDynamic, une filière de valorisation des ressources agricoles du département mis en place dans différents secteurs : agroénergie, agroalimentaire, agromatériaux, agrosanté.


Le département est également en pointe en matière d'énergies renouvelables. Déjà classé au deuxième rang national en termes de production électrique grâce à ses parcs éoliens notamment situés dans la Beauce, l'Eure-et-Loir aurait été, dès 2012, le premier producteur d'électricité français d'origine photovoltaïque avec la création sur la base aérienne de l'OTAN désaffectée de Crucey-Villages, près de Brezolles dans la région naturelle du Thymerais, du plus grand parc photovoltaïque de France. Confié en février 2011 par le conseil général à l'opérateur EDF énergies nouvelles, le parc couvre 245 des de l'ancienne base militaire et a une puissance nominale de (équivalent à la consommation électrique de 28 000 habitants ou la production de 15 éoliennes). Le nombre de panneaux installés est de .

Les habitants d'Eure-et-Loir sont les "Eurélien(ne)s".

Au niveau national le département occupe le .

L'évolution démographique du département, bien que positive, cache des contrastes importants selon les secteurs

L'est du département, dans la zone d'influence directe de Paris, bénéficie depuis quelques années, de l'arrivée conséquente d'une nouvelle population poussée hors de l'Île-de-France par la pression immobilière et à la recherche d'une meilleure qualité de vie. Les agglomérations de Chartres et de Dreux, bien desservies en transports et en infrastructures, sont au cœur des bassins de vie les mieux pourvus.

À l'ouest : le Perche et son cadre bucolique s'est revitalisé depuis la création du parc naturel régional en 1998 grâce à une population plus touristique et aisée, en quête d'une résidence secondaire

Le centre et le sud du département connaissent une évolution moins favorable. Si la région de Courville-sur-Eure, reliée à Paris et Chartres par le rail et proche de l'autoroute A11 se maintient bien, les secteurs de Châteaudun et d'Illiers-Combray, mal desservis, subissent un solde démographique nul, voire négatif.
Histogramme 

Lien externe : Revue Population et Avenir - Numéro hors série consacré à l'Eure-et-Loir (2002)














Selon le recensement général de la population du janvier 2008, 6,9 % des logements disponibles dans le département étaient des résidences secondaires.

Ce tableau indique les principales communes de l'Eure-et-Loir dont les résidences secondaires et occasionnelles dépassent 10 % des logements totaux.


Principaux clubs euréliens :




</doc>
<doc id="1007" url="https://fr.wikipedia.org/wiki?curid=1007" title="Espace de Hilbert">
Espace de Hilbert

Le concept mathématique d'espace de Hilbert réel ou complexe, nommé d'après David Hilbert, étend les méthodes de l'algèbre linéaire en généralisant les notions d'espace euclidien (comme le plan euclidien ou l'espace usuel de dimension 3) et d'espace hermitien à des espaces de dimension quelconque (finie ou infinie).

Un espace de Hilbert est un espace vectoriel muni d'un produit scalaire euclidien ou hermitien, qui permet de mesurer des longueurs et des angles et de définir une orthogonalité. De plus, un espace de Hilbert est complet, ce qui permet d'y appliquer les techniques de l'analyse mathématique.

Des espaces de Hilbert apparaissent fréquemment en mathématiques et en physique, essentiellement en tant qu'espaces fonctionnels de dimension infinie. Les premiers espaces de Hilbert ont été étudiés sous cet aspect pendant la première décennie du par David Hilbert, Erhard Schmidt et Frigyes Riesz. Ils sont des outils indispensables dans les théories des équations aux dérivées partielles, mécanique quantique, analyse de Fourier (ce qui inclut des applications au traitement du signal et le transfert thermique) et la théorie ergodique qui forme le fondement mathématique de la thermodynamique. John von Neumann forgea l'expression espace de Hilbert pour désigner le concept abstrait qui sous-tend nombre de ces applications. Les succès des méthodes apportées par les espaces de Hilbert menèrent à une époque très prolifique pour l'analyse fonctionnelle. En plus des espaces euclidiens classiques, les exemples les plus courants d'espaces de Hilbert sont les espaces de fonctions de carré intégrable, les espaces de Sobolev qui sont constitués de fonctions généralisées, et les espaces de Hardy de fonctions holomorphes.

L'intuition géométrique intervient dans de nombreux aspects de la théorie des espaces de Hilbert. Ces espaces possèdent des théorèmes analogues au théorème de Pythagore et à la règle du parallélogramme. En mathématiques appliquées, les projections orthogonales sur un sous-espace (ce qui correspond à aplatir l'espace de quelques dimensions) jouent un rôle important dans des problèmes d'optimisation entre autres aspects de la théorie. Un élément d'un espace de Hilbert peut être défini de manière unique par ses coordonnées relativement à une base de Hilbert, de façon analogue aux coordonnées cartésiennes dans une base orthonormale du plan. Quand cet ensemble d'axes est dénombrable, l'espace de Hilbert peut être vu comme un ensemble de suites de carré sommable. Les opérateurs linéaires sur un espace de Hilbert sont semblables à des objets concrets : dans les « bons » cas, ce sont simplement des transformations qui étirent l'espace suivant différents coefficients dans des directions deux à deux perpendiculaires, en un sens qui est précisé par l'étude de leur spectre.

Un des exemples les plus courants d'espace de Hilbert est l'espace euclidien de dimension 3, noté ℝ, muni du produit scalaire usuel. Le produit scalaire associe, à deux vecteurs formula_1 et formula_2 un nombre réel noté formula_3. Si formula_1 et formula_2 ont pour coordonnées cartésiennes respectives formula_6 et formula_7, alors leur produit scalaire est :
Le produit scalaire satisfait aux propriétés suivantes :

Le produit scalaire est intimement relié avec la géométrie euclidienne par la formule suivante, qui relie le produit scalaire de deux vecteurs formula_1 et formula_2 avec leurs longueurs (notées respectivement formula_21 et formula_22) et l'angle formula_23 qu'ils forment :

Toute opération sur les vecteurs qui vérifie les trois propriétés ci-dessus est également appelée produit scalaire. Un espace vectoriel muni d'un produit scalaire est dit espace préhilbertien réel.

Un espace de Hilbert est un espace préhilbertien qui possède de plus une propriété d'analyse mathématique : il est complet, argument reposant sur les limites de suites de vecteurs dans cet espace.

Un espace de Hilbert est un espace préhilbertien complet, c'est-à-dire un espace de Banach dont la norme ║·║ découle d'un produit scalaire ou hermitien 〈·, ·〉 par la formule
formula_25
C'est la généralisation en dimension quelconque (finie ou infinie) d'un espace euclidien ou hermitien.

Un espace de Banach (respectivement espace vectoriel normé) est un espace de Hilbert (respectivement espace préhilbertien) si et seulement si sa norme vérifie l'égalité

formula_26, 

qui signifie que la somme des carrés des quatre côtés d'un parallélogramme est égale à la somme des carrés de ses diagonales (règle du parallélogramme).

Ce théorème est dû à Maurice René Fréchet, John von Neumann et Pascual Jordan.

Identité de polarisation :


Dans un espace de Hilbert de dimension infinie, le concept habituel de base est remplacé par celui de base hilbertienne (ou base de Hilbert) qui permet, non plus de décrire un vecteur par ses coordonnées, mais de l'approcher par une suite infinie de vecteurs ayant chacun des coordonnées finies. On est donc au confluent de l'algèbre linéaire et de la topologie.



Cours d'analyse — Jacques Harthong


</doc>
<doc id="1008" url="https://fr.wikipedia.org/wiki?curid=1008" title="Essonne (département)">
Essonne (département)

Le département de l’Essonne ( ) est un département français situé au sud de Paris, dans la région Île-de-France, pour partie intégré à l’agglomération parisienne, qui tire son nom de la rivière Essonne dont le cours traverse le territoire selon un axe sud-nord jusqu’à la confluence avec la Seine.

Officiellement créé le par démembrement de l’ancienne Seine-et-Oise, il porte le code Insee et , couvre un territoire de , occupé en par habitants, son chef-lieu est installé à Évry, l’État y est représenté par la préfète Josiane Chevalier et son conseil départemental est présidé par François Durovray (Les Républicains).

Ses habitants sont appelés les "Essonniens".

Le département de l’Essonne est situé dans la région Île-de-France, il est pour 40 % de son territoire intégré à l’agglomération parisienne et géologiquement implanté dans le bassin parisien. Quatre régions naturelles occupent le territoire, délimitées par les cours d’eau. Sur les deux tiers nord-ouest du département, à l’ouest de la rivière l’Essonne et au nord de la Louette se trouve le Hurepoix, au sud-ouest, délimité par la Louette et l’Essonne s’étendent les larges plaines de la Beauce, à l’extrême sud-est, approximativement dans la vallée de l’École se trouve le Gâtinais français et au nord-est, sur la rive droite de la Seine commence le plateau de la Brie. Ce territoire fertile est aujourd’hui encore fortement disparate, avec une urbanisation relativement dense au centre-nord dans la pénéplaine de l’Orge, la Seine et l’Essonne, la présence conjointe de bourgs, de culture maraîchère et d’espaces boisés protégés dans les vallées, de vastes espaces de grande culture céréalières sur les plateaux de l’ouest et du sud où se trouvent des villages ruraux. En 2003, ce sont ainsi près de 78 % du territoire qui étaient encore considérés comme ruraux.

Plusieurs cours d’eau arrosent le département, le fleuve la Seine forme une boucle de l’est au nord, complétée par ses affluents l’Yerres sur la rive droite, sur la rive gauche du sud au nord, l’École, l’Essonne, l’Orge et la Bièvre et les sous-affluents le Réveillon qui se jette dans l’Yerres, la Juine, grossie par la Louette et la Chalouette, qui se jettent dans l’Essonne, la Renarde, la Rémarde, alimentée par la Prédecelle, la Salmouille et l’Yvette, alimentée par la Mérantaise, le Vaularon et le Rouillon se jettent toutes dans l’Orge.

Ces rivières et ruisseaux ont creusé le sous-sol sédimentaire du territoire, caractéristique du bassin parisien et composé principalement de sable, de marne et de calcaire, par endroit complété par de l’argile, du gypse et de la craie ou compacté pour former de la meulière. Le terrain s’étage ainsi en un vaste plateau sur la moitié ouest, descendant en pente douce vers la pénéplaine de la Seine au nord-est et entrecoupé de vallées plus ou moins encaissées. Le point culminant du département est ainsi situé à cent soixante-dix-huit mètres à Pecqueuse tandis que le point le plus bas, en bord de Seine est positionné à trente-et-un mètres à Vigneux-sur-Seine.

Le point le plus bas est d’ailleurs situé à la frontière avec le département limitrophe nord et nord-est du Val-de-Marne, tandis que le plateau ouest s’étend des Hauts-de-Seine au nord-ouest à l’Eure-et-Loir au sud-ouest en passant par les Yvelines à l’ouest. Au Sud, la Beauce occupe une large part du Loiret, complétée par le Gâtinais qui s’étend aussi au Sud-Est en Seine-et-Marne. À l’Est, la Brie constitue le paysage typique de cette même Seine-et-Marne. Aujourd’hui, les communes extrême-cardinales sont Bièvres au Nord, Varennes-Jarcy à l’Est, Angerville au Sud et Chatignonville à l’Ouest. Le centre géographique du département est situé dans la commune de Lardy en lisière du bois des Célestins (). Outre la préfecture Évry et les chefs-lieux Corbeil-Essonnes, Étampes et Palaiseau, plusieurs cités apparaissent comme caractéristiques du département par leurs importance historique, culturelle, économique et administrative, soit Arpajon, Dourdan, La Ferté-Alais, Milly-la-Forêt et Montlhéry.

Elles sont, encore aujourd’hui, reliées au maillage d’infrastructures de transport qui parcourt le département, constitué par les routes nationales 118 au nord-ouest, 20 au centre, 7 à l’est, 6 au nord-est et 104 d’ouest en est, l’autoroute A10 du nord au sud-ouest et A6 du nord au sud-est, le Réseau express régional d'Île-de-France avec les lignes B au nord-ouest, C au centre et D à l’est, les lignes d’autobus et l’aéroport de Paris-Orly implanté au nord du département.
Le département de l’Essonne est traversé par de nombreux cours d'eau d’importances et de tailles diverses. En premier, le fleuve la Seine passe au nord-est du territoire, elle entre par l’est au Coudray-Montceaux et parcourt vingt-quatre kilomètres jusqu’à Vigneux-sur-Seine. Plusieurs de ses affluents parcourent le département, sur la rive droite, l’Yerres parcourt dix-sept kilomètres en Essonne entre Quincy-sous-Sénart et Crosne avant la confluence située dans le département du Val-de-Marne à Villeneuve-Saint-Georges. Sur la rive gauche, l’École fait un court passage au sud-est entre Oncy-sur-École et Soisy-sur-École, la confluence étant située en Seine-et-Marne à Saint-Fargeau-Ponthierry, l’Essonne entre par le sud du département à Boigneville et rejoint le cours du fleuve à Corbeil-Essonnes après un parcours de quarante-quatre kilomètres, l’Orge démarre son tracé essonnien à Dourdan jusqu’à se jeter dans la Seine à Viry-Châtillon et Athis-Mons sur quarante-et-un kilomètres, la Bièvre fait une petite incursion à l’extrême nord-ouest entre Bièvres et Verrières-le-Buisson.

Plusieurs sous-affluents du fleuve sont ensuite répartis sur le territoire. Rejoignant la Bièvre à l’extrême nord, la Sygrie coule depuis le plateau de Vélizy, grossissant la Bièvre par sa rive gauche, au nord-ouest l’Yvette alimentée par la Mérantaise, le Vaularon et le Rouillon trace la vallée de Chevreuse, à l’ouest la Salmouille et la Rémarde alimentée par la Prédecelle descendent du plateau de Limours, sur la rive droite la Renarde descend de la Beauce. Au sud, rejoignant l’Essonne par sa rive gauche coule la Juine, alimentée par la Louette et la Chalouette. À l’extrême nord-est coule enfin le Réveillon qui se jette dans l’Yerres à Yerres.

Plusieurs lacs et étendues d’eau, naturels ou artificiels, sont aussi répartis sur le territoire. En bord de Seine se trouvent le lac de Viry-Châtillon, le lac de Draveil et le lac de Vigneux-sur-Seine. Assurant la régulation et la retenue des eaux, le lac de Saulx-les-Chartreux et les étangs de Saclay constituent aujourd’hui des espaces protégés. Le lac de Vert-le-Petit, le lac de Tigery, l’étang de la Veyssière, l’étang de Trévoix et la fosse Montalbot caractérisent eux aussi le patrimoine hydrique du département.

Enfin, le département est traversé du Sud au Nord par l’aqueduc de la Vanne et du Loing qui est chargé, par son prolongement formé par les aqueducs d'Arcueil et de Cachan et le réservoir de Montsouris, d’alimenter Paris en eau potable. Dans l’Essonne, il démarre son parcours entièrement souterrain à Soisy-sur-École (), traverse les communes de Champcueil, Chevannes, Mennecy, Ormoy, Villabé, Lisses, Courcouronnes, Ris-Orangis, Grigny, Viry-Châtillon, Savigny-sur-Orge et Paray-Vieille-Poste où il quitte le territoire départemental en passant sous l’aéroport de Paris-Orly ().

Le département de l’Essonne occupe un territoire orienté en pente relativement douce de la Beauce au sud-ouest vers la vallée de la Seine au nord-est. Le point culminant du département se trouve à l’ouest sur le territoire de Pecqueuse à cent soixante-dix-huit mètres d’altitude au lieu-dit Chaumusson à proximité de l’ancienne ligne Paris - Chartres par Gallardon (). Le point le plus bas est lui situé à trente-et-un mètres d’altitude à Vigneux-sur-Seine à proximité de la station d’épuration en bord de Seine (). La moitié sud du département est occupée par le large plateau de la Beauce, creusé par les vallées de l’Essonne et l’École à l’est, de la Juine au centre et de l’Orge au nord-ouest. Le nord-ouest du département alterne plateaux et vallées encaissées avec du sud au nord, le plateau de Limours, la petite vallée de la Salmouille, le plateau de Courtabœuf, la vallée encaissée de l’Yvette, le plateau de Saclay, la vallée encaissée de la Bièvre (affluent de la Seine) et enfin les contreforts du plateau de Villacoublay. Le nord-est du département est occupé par la pénéplaine de la Seine, de l’Essonne et de l’Orge, au relief relativement peu marqué jusqu’au lit du fleuve. Sur la rive droite commence le plateau de la Brie, traversé par la vallée peu profonde de l’Yerres.

Géologiquement intégré au bassin parisien, le sous-sol est relativement homogène sur l’ensemble du territoire avec quelques variations entre le sud et le nord-est. Au sud-est le sous-sol est constitué de couches successives de sable de Fontainebleau et de calcaire. Au nord-ouest et à l’est, le calcaire est remplacé par de la marne et dans les vallées de l’Yvette et de la Bièvre, le sable compacté forme des blocs de meulière. Au centre du territoire, le gypse se mêle au calcaire et la marne. Dans les vallées de l’Essonne et de l’Orge s’ajoutent une couche d’argile à silex, et à l’extrême est du territoire, dans le Gâtinais, en profondeur une couche de craie. Au nord-est de la Seine, le plateau briard est composé de couches successives de marne, de sable et de calcaire.

Le département de l’Essonne est une portion de l’ancienne Seine-et-Oise, comme ses départements limitrophes que sont les Yvelines à l’ouest, les Hauts-de-Seine au nord-ouest et le Val-de-Marne au nord et nord-est. À l’est et au sud-est se trouve le plus vaste département de la région Île-de-France, la Seine-et-Marne. Au sud et au sud-ouest, se trouvent le Loiret et l’Eure-et-Loir qui appartiennent tous deux à la région Centre-Val de Loire.

Le climat de l’Essonne est mesuré à partir de la station météorologique départementale de Brétigny-sur-Orge (), approximativement centrale sur le territoire et implantée à soixante-dix huit mètres d’altitude dans un secteur représentatif géographiquement. Cependant, des variations relativement importantes sont mesurables à partir de stations situées à proximité dans les départements limitrophes, comme à Orly et Vélizy-Villacoublay au nord, Melun à l’est, Trappes et Chartres à l’ouest, Orléans au sud.

Le département de l’Essonne, situé en Île-de-France et dans le bassin parisien, se caractérise par un climat océanique dégradé, principalement sous l’influence des régimes d’ouest - sud-ouest, aux hivers frais et aux étés doux. Cela se traduit par une fréquence élevée des pluies, environ cent soixante jours par an. Mais paradoxalement l’Essonne figure parmi les départements les plus « secs » de France, le mot sec étant relatif aux quantités de pluie reçue avec seulement par an à la station météorologique départementale de Brétigny-sur-Orge contre à Nice ou 660 millimètres à Toulouse. Les précipitations sont cependant bien réparties tout au long de l’année, toutefois l’été connaît des précipitations surtout sous forme d’averses orageuses brèves mais intenses. L’ensoleillement est pour sa part conforme aux moyennes relevées au nord de la Loire avec un cumul de par an, un ensoleillement maximum en juillet et minimum en décembre.

Les températures sont typiques des plaines du bassin parisien avec des moyennes en janvier entre et , en juillet entre et à Brétigny-sur-Orge. La température moyenne annuelle est fixée à avec une moyenne haute à et une moyenne basse à , le mois le plus chaud est juillet avec une moyenne haute mensuelle fixée à et le mois le plus froid, janvier avec une moyenne basse à . L’influence du climat continental entraîne cependant des écarts parfois importants et des records de température élevés avec relevés le et très bas à le . À noter que les températures minimales sont systématiquement plus élevées d’un à deux degrés celsius dans le nord du département du fait de la densité urbaine plus forte.

Le département dispose d’un maillage important d’axes de transport d’envergure nationale, des routes en partie héritées des voies romaines et de l’ancien régime, des voies ferrées majeures et un aéroport international.

Implanté pour 60 % de sa superficie à l’extrême nord du département, l'aéroport de Paris-Orly constitue une plaque tournante importante du transport aérien. Deuxième aéroport de France avec un trafic d'environ et en 2015.

Il est complété en Essonne par l'aérodrome d'Étampes - Mondésir, l'aérodrome de La Ferté-Alais, l'aérodrome de Buno-Bonnevaux et l'héliport d'Évry.
La base aérienne 217 de Brétigny-sur-Orge a été dissoute le 26 juin 2012.

Malgré la présence de la Seine à l’est du département, le transport fluvial est relativement peu développé avec la seule présence du port d'Évry, installation de quatre hectares gérée par le port autonome de Paris, qui a permis le traitement en 2001 de tonnes de marchandises et qui devrait voir sa capacité augmenter par l’adjonction d’un terminal à conteneurs. Il est complété un peu en amont par les installations des grands moulins de Corbeil dont le nouveau terminal inauguré en 1995 est adapté aux convois fluviaux et par des installations d’entreprises privées à Viry-Châtillon, Grigny et Athis-Mons. En 2008, le trafic fluvial dans les ports du département s’est élevé à tonnes.

L’Essonne est traversée du nord au sud par six axes routiers majeurs. Implantés d’ouest en est, on trouve la route nationale 118 de Bièvres aux Ulis qui mènent à la porte de Saint-Cloud, l’autoroute A86 qui fait une courte incursion à Verrières-le-Buisson, l’autoroute A10 de Wissous à Dourdan vers l’ouest, la route nationale 20 de Massy à Angerville vers le sud-ouest, l’autoroute A6 de Wissous à Soisy-sur-École vers le sud-est, l’ancienne route nationale 7 de Paray-Vieille-Poste au Coudray-Montceaux vers le sud, la route nationale 6 de Crosne à Tigery, la route nationale 337 au Coudray-Montceaux et la route nationale 449 à Ris-Orangis. Assurant la liaison entre ces axes et la ceinture de la région, la route nationale 104 traverse le département d’ouest en est entre Marcoussis et Tigery. Le département totalise ainsi en 2009 cinquante-huit kilomètres d’autoroute, deux cent vingt-cinq kilomètres de route nationale et mille cent quatre-vingt-onze kilomètres de route départementale.

Complétant ce maillage, plusieurs réseaux d’autobus de la RATP et du réseau Optile relient les agglomérations.

Suivant pour la plupart ces axes routiers, des voies ferrées furent ajoutées. Le département est ainsi parcouru, selon leur implantation d’ouest en est, par l’ancienne ligne de Sceaux utilisée par la ligne B du RER d'Île-de-France de Verrières-le-Buisson à Gif-sur-Yvette, la ligne de Paris-Montparnasse à Monts (LGV) de Verrières-le-Buisson à Saint-Cyr-sous-Dourdan, la ligne de la grande ceinture de Paris entre Bièvres et Athis-Mons, la ligne de Paris-Austerlitz à Bordeaux-Saint-Jean d’Athis-Mons à Angerville et la ligne de Brétigny à La Membrolle-sur-Choisille de Brétigny-sur-Orge à Dourdan aujourd’hui toutes trois utilisées par la ligne C du RER d'Île-de-France, la ligne de Villeneuve-Saint-Georges à Montargis entre Vigneux-sur-Seine et Boigneville, la ligne de Grigny à Corbeil-Essonnes entre Grigny et Corbeil-Essonnes, la ligne de Corbeil-Essonnes à Montereau entre Corbeil-Essonnes et Le Coudray-Montceaux et la ligne de Paris-Lyon à Marseille-Saint-Charles entre Montgeron et Boussy-Saint-Antoine empruntées par la ligne D du RER d'Île-de-France. Au total, ce sont soixante-treize gares qui sont réparties sur les trois lignes de transports en commun. Les TGV desservent les gares de Massy TGV, Massy - Palaiseau et Juvisy.

Quatre réseaux d’oléoducs traversent le département totalisant cent soixante-cinq kilomètres, dont le Donges-Grandpuits-Metz exploité par la Société française Donges-Metz avec quatre dépôts à Guigneville-sur-Essonne, D'Huison-Longueville, Cerny et Orveau et Le Havre-Grandpuits et son jumeau PLIF exploités par la Trapil et l’oléoduc reliant la station de pompage d’Itteville à la raffinerie de Grandpuits. Plusieurs gazoducs du réseau haute pression de GRTgaz sillonnent le territoire et un spécial géré par Air liquide relie l’usine Altis Semiconductor de Corbeil-Essonnes au centre de production de Moissy-Cramayel.

À Villejust se trouve le centre de transformation électrique le plus important de France chargé de transformer le volts provenant des centrales électriques de la Loire en volts à destination des répartiteurs d’Issy-les-Moulineaux et Chevilly-Larue et en volts pour la consommation locale. Il est en outre relié à la ligne à très haute tension « boucle » de RTE qui entoure l’Île-de-France et traverse le département d’ouest en est.

L’appellation de la rivière l’Essonne tire ses origines du nom de la déesse gauloise des rivières, "Acionna", vénérée dans l’Orléanais où le cours d’eau prend sa source. À la création du département en 1964, il fut décidé qu’il prendrait le nom de la rivière qui parcourt son territoire du sud au nord jusqu’à la confluence avec la Seine à Corbeil-Essonnes. Ce même nom se retrouvait aussi dans le toponyme de l’ancienne commune d’Essonnes et comme extension de plusieurs communes du département : Ballancourt-sur-Essonne, Boutigny-sur-Essonne, Courdimanche-sur-Essonne, Gironville-sur-Essonne, Guigneville-sur-Essonne, Prunay-sur-Essonne et Vayres-sur-Essonne.

Le territoire de l’actuel département de l’Essonne fut occupé de façon certaine dès le Néolithique, comme en témoignent les découvertes en divers points du département de silex taillés et l’élévation de menhirs comme dans la forêt de Sénart à Brunoy. À l’époque gauloise, le territoire était à la frontière entre les domaines des "Parisii" au nord, des "Carnutes" au sud-ouest et des "Sénons" au sud-est. Des premières villes commencèrent alors à se démarquer, dont Dourdan, réputée pour son activité de poterie. L’invasion romaine permit l’édification d’une multitude de "villa rustica" sur les plateaux dominant les riches vallées, comme en témoignent les résultats de fouilles archéologiques à Orsay. D’autres villages se transformèrent en "oppidum" à la croisée des routes, tel Arpajon.

Milly-la-Forêt devint par la volonté de Dryus un centre druidique au , la région de Dourdan subissant l’autorité du roi païen "Dordanus" au . Au s’acheva l’évangélisation du territoire, avec l’édification en 600 d’une première église à Corbeil-Essonnes et le développement à Palaiseau d’un abbaye sous l’impulsion de sainte Bathilde et saint Wandrille. En l’an 604 se déroula la première bataille d’Étampes entre Clotaire II, roi de Neustrie et Thierry II, roi de Bourgogne allié de Thibert II, roi d’Austrasie. À partir du , la plupart du territoire était intégré au domaine royal français, les rois disposant des lieux et distribuant les terres à leurs vassaux. Commença alors au l’édification de châteaux forts contrôlant les routes commerciales, comme à Montlhéry, ou pour arrêter les "raids" Vikings à Corbeil-Essonnes et La Ferté-Alais. À partir du , la basilique Notre-Dame-de-Bonne-Garde à Longpont-sur-Orge devint la première étape du pèlerinage de Saint-Jacques-de-Compostelle depuis Paris.

Les révoltes des nobles locaux entraînèrent l’intégration complète du territoire dans le domaine royal, Robert II de France construisant le château d'Étampes, Louis VI de France démantelant le château de Montlhéry au et réduisant à néant la puissante famille de Montlhéry, Philippe II de France construisant le château de Dourdan au . En 1131 se déroula le concile d’Étampes qui se prononça en faveur du futur pape Innocent II. En 1258 fut scellé le traité de Corbeil, fixant les limites territoriales entre le royaume de France et le royaume d'Aragon. Le domaine d’Étampes fut érigé en comté en 1298, créant alors la lignée des comtes puis ducs d’Étampes. De cette époque se fixèrent les deux composantes principales de l’économie locale, l’agriculture pour alimenter la capitale et l’industrie utilisant la force motrice des nombreux cours d’eau. Au , les Grands moulins de Corbeil devinrent « moulins du Roi ».

À partir du s’installèrent dans la région d’importantes commanderies templières à Étampes, Longjumeau, Chalou-Moulineux, Auvernaux, organisant de vaste domaines agricoles prospères. En 1305 fut signé le traité d'Athis-sur-Orge qui intégrait au royaume de France les villes de Lille, Douai et Béthune. En 1326 intervint le second traité de Corbeil, renouvelant l’"Auld Alliance". En 1346, Philippe VI de France signa l’ordonnance de Brunoy, considérée comme le premier acte juridique en français ayant trait au développement durable. Survint alors la guerre de Cent Ans, causant des ravages et des massacres sur le territoire, comme l’incendie en 1360 de l’église Saint-Clément d’Arpajon par les troupes d’Édouard III d'Angleterre, brûlant vifs huit cent habitants. En 1371, ce fut au tour de Milly-la-Forêt d’être ravagée par le « prince noir » Édouard de Woodstock. Entre 1353 et 1355, Paris frappé par la peste noire utilisa les coches des bateliers de Corbeil, les "corbeillards" pour évacuer les cadavres, donnant ainsi naissance au mot corbillard. En 1465 eu lieu la bataille de Montlhéry entre Louis XI de France et Charles le Téméraire.

Le début de la Renaissance vit le développement commercial de la région, avec l’édification de halles à Milly-la-Forêt au puis Dourdan, Arpajon et Méréville au . Dans le même temps, la fixation du pouvoir royal à Paris puis Versailles, deux villes proches et les apanages que constituaient les villes de Dourdan et Étampes, offertes en présents aux favorites Anne de Pisseleu et Gabrielle d'Estrées, entraînèrent l’établissement de châteaux, construits par les courtisans et les magistrats parisiens. En 1568 fut signée la Paix de Longjumeau, concluant la deuxième guerre de religion. En 1590, lors du siège de Paris, ce fut encore Corbeil, prise par Alexandre Farnèse qui permit le ravitaillement de Paris, bloqué par les troupes d’Henri IV de France. En 1628, la ville d’Essonnes fut ravagée par un incendie, provoqué par une nouvelle explosion du moulin à poudre. En 1652, en pleine Fronde, la seconde bataille d’Étampes mena à la victoire de Turenne qui avait stationné ses troupes à Arpajon.

Le vit la région s’équiper de plusieurs hôtels-Dieu à Milly-la-Forêt, Dourdan et Arpajon, de relais de poste sur les routes de Fontainebleau et Orléans. Il s’acheva par la Révolution française, modifiant relativement peu le quotidien des habitants. Un fait-divers marqua cependant cette période troublée, l’assassinat à Étampes du maire Jacques Guillaume Simoneau, entraînant la création par l’Assemblée législative d’une « Fête de la Loi » sur tout le territoire. Relativement peu touché par les conflits en dehors de l’occupation prussienne en 1870, le territoire profita du pour bénéficier d’une modernisation importante, avec la création de plusieurs lignes de chemin de fer, la ligne de Sceaux en 1854, la ligne Brétigny - Tours en 1867, la ligne de la grande ceinture en 1882, plusieurs villages devenant alors un lieu de villégiature pour les riches bourgeois parisiens et les artistes. L’ouverture de la ligne de tramway de l’Arpajonnais en 1894 permit une nouvelle montée en puissance des débouchés agricoles de la région, offrant un accès direct et rapide aux halles de Paris. L’industrie lourde fit aussi son apparition avec l’ouverture des usines Decauville à Évry-Petit-Bourg et des papeteries Darblay à Essonnes, faisant entrer cette riche famille essonnienne dans le cercle des « deux cents familles ».

Le début du vit une nouvelle révolution pour le département, qui devint un des berceau de l’aviation, Viry-Châtillon accueillant en 1909 le premier aéroport organisé du monde à Port-Aviation, en 1910 Louis Blériot et Maurice Farman ouvraient des écoles sur l’aérodrome d'Étampes - Mondésir, Brétigny-sur-Orge disposait dès 1938 de la base aérienne 217, lieu de nombreux records. Autre site emblématique de la course à la vitesse, l’autodrome de Linas-Montlhéry créé en 1924 resta le lieu des plus célèbres courses automobiles jusque dans les années 1960. Relativement épargné par les deux conflits mondiaux, malgré la présence du camp de concentration de Linas-Montlhéry construit en 1940 par les nazis pour l’enfermement des Tsiganes, le futur département connu dès les années 1950 une forte poussée démographique, touché comme ces voisins par l’édification de bidonvilles aux portes de Paris, situation plus tard aggravée par la nécessité d’héberger les nombreux rapatriés d’Algérie venus s’installer dans la région. Ces bouleversements démographiques allaient entraîner le redécoupage administratif.

Au milieu des années 1960, la région Île-de-France était le théâtre de tractations politiques importantes. Le pouvoir central de la jeune Cinquième République, représenté par le président le général de Gaulle et le premier ministre Michel Debré, décida de la réorganisation de la région capitale. Ainsi, la loi du 10 juillet 1964 portant réorganisation de la région parisienne prévoyait de supprimer les départements de Seine-et-Oise et de la Seine pour en créer six nouveaux, dont le département de l’Essonne comprenant la presque totalité de l’arrondissement de Corbeil, l’arrondissement de Palaiseau et une partie de l’ancien arrondissement de Rambouillet. Le , le décret fixait le chef-lieu du département à Évry-Petit-Bourg, précédemment officieusement installé à Corbeil-Essonnes. Le , un nouveau décret 66-339 prévoyait le découpage administratif du département avec la création de l’arrondissement d'Évry en remplacement de celui de Corbeil-Essonnes (cette commune conservant cependant son statut de sous-préfecture) et modifiait les limites de l’arrondissement de Palaiseau. Le même jour, le décret créait l’arrondissement d'Étampes. Le , le décret portait création officielle des vingt-sept cantons du département : Arpajon, Athis-Mons, Bièvres, Brétigny-sur-Orge, Brunoy, Corbeil-Essonnes, Dourdan, Étampes, Étréchy, Évry, Juvisy-sur-Orge, La Ferté-Alais, Limours, Longjumeau, Massy, Mennecy, Méréville, Milly-la-Forêt, Montgeron, Montlhéry, Orsay, Palaiseau, Ris-Orangis, Saint-Chéron, Sainte-Geneviève-des-Bois, Savigny-sur-Orge et Viry-Châtillon.

Le , le décret fixait à la date du l’entrée en vigueur de la loi du 10 juillet 1964, prévoyant effectivement la création du nouveau département. Ainsi, le département de l’Essonne fut officiellement créé le janvier 1968, les élus du nouveau conseil général désignés lors des élections de 1967 entraient en fonction à cette date. Il fallut cependant attendre 1969 pour l’entrée en fonction du préfet Michel Aurillac. Le un décret détacha les communes de Châteaufort et Toussus-le-Noble qui sont rattachées au département voisin des Yvelines. Le , un décret actait la fusion des communes d’Angerville et Dommerville, cette dernière quitte alors le département d’Eure-et-Loir pour intégrer l’Essonne. Le intervient un redécoupage administratif, le décret portant le nombre de cantons à trente-cinq en ajoutant les cantons de Chilly-Mazarin, Draveil, Gif-sur-Yvette, Morsang-sur-Orge, Saint-Germain-lès-Corbeil, Saint-Michel-sur-Orge, Vigneux-sur-Seine, Villebon-sur-Yvette et Yerres et en supprimant le canton de Juvisy-sur-Orge. Le , le préfet de l’Essonne Paul Cousserand signait l’arrêté portant création d’une nouvelle commune, Les Ulis. Le , un nouveau décret modifiait encore les limites administratives en portant le nombre de cantons du département à quarante-deux par l’ajout des cantons de Corbeil-Essonnes-Est, Épinay-sous-Sénart, Évry-Nord, Grigny, Juvisy-sur-Orge, Massy-Est et Les Ulis.

En parallèle de cette mise en place administrative, les autorités religieuses décidèrent de suivre le mouvement en créant le , à partir du vaste diocèse de Versailles, le nouveau diocèse de Corbeil-Essonnes. La collégiale Saint-Spire de Corbeil-Essonnes fut alors élevée au rang de cathédrale, avant l’édification à partir de 1991 de la nouvelle cathédrale de la Résurrection à Évry, conformément au changement de nom intervenu en 1988.

Depuis la création du département, sa morphologie s’est considérablement modifiée, créant deux paysages radicalement différents entre le Nord urbanisé et le Sud rural du territoire. L’explosion démographique entraînant une forte demande de logements, a entraîné comme ailleurs dans la région de grands travaux et la construction de grands ensembles, certains villages devenant en dix ans des grandes villes. Cas typique, Grigny qui ne comptait que habitants en 1962 en comptait plus de en 1975, en grande partie logés dans la nouvelle cité d’habitat social de La Grande Borne. Le chef-lieu du département, Évry connut la même évolution sur une période plus longue, passant de habitants en 1962 à plus de en 2006. Cette dernière fut, en même temps que se forgeait le nouveau territoire, intégrée dès 1965 au grand programme de Paul Delouvrier qui ambitionnait d’y construire une ville nouvelle, menant à la création en 1969 de l’établissement public d’aménagement de la ville d’Évry. Cette ville nouvelle dépassait largement les limites de la petite commune d’Évry-Petit-Bourg puisqu’elle englobait aussi Bondoufle, Courcouronnes et Lisses avec l’objectif d’accroître le pôle urbain que constituait déjà Corbeil-Essonnes, où s’était élevée la vaste cité des Tarterêts. De fait, presque toutes les villes moyennes, quasiment chaque chef-lieu de canton ou leurs périphéries, virent leurs territoires lotis, par des barres d’immeubles, des tours ou des pavillons individuels. Palaiseau, Étampes, Massy, Longjumeau, Sainte-Geneviève-des-Bois, Vigneux-sur-Seine, Épinay-sous-Sénart, Athis-Mons, Saint-Michel-sur-Orge, Brétigny-sur-Orge devinrent des villes de banlieue typiques. Autre exemple de cette course à la construction, Les Ulis, commune créée en 1977 sur les champs de blé du plateau de Courtabœuf, atteint plus de habitants en 1982.

Cette nouvelle concentration de résidents entraîna de nouveaux besoins, permettant le développement d’une nouvelle société de consommation avec l’ouverture en 1963 du premier hypermarché de France sous l’enseigne Carrefour à Sainte-Geneviève-des-Bois, puis la création des centres commerciaux, Ulis 2 en 1973, Évry 2 en 1975, La Croix-Blanche dans les années 1980, Villebon 2 en 1988, Villabé A6 en 1992. Les besoins en transports en commun entraînèrent la création entre 1962 et 1979 du réseau express régional d'Île-de-France avec l’ouverture dans le département de la ligne B en 1977, la ligne C en 1979 et la ligne D en 1987, le percement de nouvelles voies rapides, dont l’autoroute A6 ouverte en 1960 depuis Paris jusqu’au Coudray-Montceaux, l’autoroute A10 ouverte progressivement entre 1960 et 1973 depuis Wissous et la route nationale 104 aménagée dans les années 1980. L’aéroport de Paris-Orly, dont l’aérogare Sud fut inaugurée en 1961 marquait l’avènement de la modernité, mais il fut cependant vite à l’étroit, à cause de l’urbanisation rapide de ses abords, entraînant dès 1968 la première décision d’instaurer un couvre-feu de 23h00 à 6h00.

De façon concomitante, de nombreuses institutions et entreprises s’implantèrent dans le département. La création en 1960 du parc d'activités de Courtabœuf permit l’arrivée d’entreprises de renom comme Hewlett-Packard qui y ouvrit son centre de recherche européen en 1968, imitée en 1983 par Microsoft. Sur le plateau de Saclay voisin, l’implantation en 1975 de l’école supérieure d'électricité et en 1976 de l’École polytechnique renforçait l’importance prise par le secteur depuis l’ouverture en 1971 de l’université Paris-Sud 11 à Orsay et complétait le commissariat à l'énergie atomique ouvert en 1952. L’est du département suivit plus tard le même développement, porté par la ville nouvelle, avec l’implantation en 1972 du centre national d'études spatiales, en 1979 de l’école Télécom SudParis (ex-Télécom INT) et Télécom École de Management (ex-INT Management), en 1980 du groupement Arianespace, l’ouverture en 1991 de l’université d'Évry-Val d'Essonne et en 1998 la création du Génopole. En 2006, l’inauguration à Saint-Aubin du Synchrotron soleil marquait la poursuite du programme de développement économique et scientifique du département.

Des équipements structurants de niveau départemental furent bientôt développés, le théâtre de l’Agora d’Évry ouvert en 1975, la base de plein air et de loisirs d'Étampes en 1977, l’opéra de Massy en 1993, Le Grand Dôme en 1994 à l’occasion des jeux de la francophonie. La décision de créer en 2006 l’opération d'intérêt national de Massy Palaiseau Saclay Versailles Saint-Quentin-en-Yvelines, couvrant presque un quart du nord-ouest du département sur vingt-sept communes relance aujourd’hui les perspectives de développement économique et d’aménagement du territoire, et débouche sur le projet de "cluster" technologique Paris-Saclay. En 2009, la réorganisation des services de l'État entraîna la suppression de la sous-préfecture de Corbeil-Essonnes.

Depuis la création du département le , sa population a crû rapidement, passant de habitants lors du premier recensement de 1968 à résidents en 1975, puis a connu une croissance moins rapide, pour atteindre personnes en 1982 et pour ne dépasser le million qu’en 1990. Lors du recensement en 2011, le nombre d’Essonniens est fixé à personnes. Le ralentissement de la croissance démographique depuis 1990 est dû au solde apparent des entrées-sorties, devenu négatif (- 0,4 % par an de 1990 à 1999, - 0,1 % par an de 1999 à 2006 et - 0,5 % par an de 2006 à 2011). En d'autres termes, il y plus d'habitants qui quittent le département pour habiter ailleurs que de personnes qui viennent y résider. L'augmentation de la population est donc à mettre au compte d'un solde naturel (différence entre les naissances et les décès) encore très largement positif. La proportion d'immigrés dans la population essonnienne (13,8 % en 2011) est supérieure à la moyenne nationale (8,7 %) : en 2011, parmi les immigrés habitant dans l'Essonne, 31,2 % proviennent d'un pays européen (17,9 % du Portugal), 25,4 % d'un pays du Maghreb (11,3 % d'Algérie, 9,5 % du Maroc et 4,6 % de Tunisie), 24,8 % d'autres pays d'Afrique, 4,6 % de Turquie, 10,2 % d'autres pays d'Asie.

Histogramme de l’évolution démographique.

Département jeune par sa date de création, l’Essonne l’est aussi relativement par sa population avec une nette différence de répartition des tranches d’âges comparativement à l’ensemble du territoire national. Ainsi, pour les deux catégories les plus jeunes, entre un et deux points supplémentaires caractérisent la pyramide des âges essonnienne et à l’inverse pour les trois dernières tranches, deux points de moins approximativement séparent les taux de l’Essonne de ceux de la France.

L’Essonne est un département qui vieillit, dans une proportion cependant moindre qu’à l’échelle nationale. La population des 60 à 74 ans en Essonne ne représente que 12,3 % en 2010 ( personnes), tandis qu’au niveau national cette tranche d’âge représente près de 15 % de la population. Par ailleurs, le département compte quelque personnes âgées de 75 ans et plus.

En 2006, la densité moyenne de population s’établissait à 664 habitants par kilomètre carré, taux très supérieur à celui de la moyenne nationale fixée à 113 /km² mais inférieur à la moyenne de la région Île-de-France établie à 960 /km². La densité de peuplement est cependant très inégale sur le territoire départemental, avec une concentration forte au Nord-Est, autour du chef-lieu départemental et des axes majeurs, une densité légèrement moindre au nord-ouest, exception faite des pôles urbains de Massy, Longjumeau et Les Ulis et une densité faible dans une large moitié sud, où les communes conjuguent vaste territoire et faible population, Étampes jouant là le rôle de pôle urbain. La commune la plus densément peuplée est Évry avec et la moins dense est Roinvilliers avec .

Le département de l’Essonne est dirigé par le conseil départemental de l'Essonne, assemblée délibérante départementale composée de quarante-deux conseillers généraux dont dix-huit socialistes, deux divers gauche, deux écologistes, quatre Front de gauche composant la majorité, douze UMP et quatre divers droite dans l’opposition, le tout présidé par Jérôme Guedj (PS). Le président est assisté de douze vice-présidents, quatre présidents délégués et quatre conseillers généraux délégués. L’Insee attribue au département le code 91, Eurostat le code NUTS3 FR104 et l’organisation internationale de normalisation le code ISO 3166-2 FR-91.

En 2007, le département disposait d’un budget de euros dont euros de fonctionnement et euros d’investissement, la dette départementale s’élevait cette même année à euros. En 2008, les taux d’imposition servant à financer pour partie ce budget s’élevaient à 6,68 % pour la taxe d'habitation, 8,97 % et 26,63 % pour la taxe foncière sur le bâti et le non bâti et 8,58 % pour la taxe professionnelle. En 2007, le département employait plus de agents répartis entre cinq directions générales opérationnelles, les charges de personnel représentant 16,8 % du budget de fonctionnement. En 2007 toujours, logements sociaux étaient répartis sur le territoire sur un total de logements dans le département, soit 21 % du parc.

Au Sénat, le département de l’Essonne est représenté par cinq sénateurs. Les grands électeurs essonniens ont choisi pour les représenter durant la mandature 2011-2014 l'UMP Serge Dassault, les socialistes Claire-Lise Campion et Michel Berson, le centriste Vincent Delahaye et le Vert Jean-Vincent Placé.

À l’Assemblée nationale, le département de l’Essonne est représenté par dix députés. Les électeurs essonniens ont choisi pour les représenter au cours de la législature les socialistes Malek Boutih, Carlos Da Silva, Jérôme Guedj, Thierry Mandon, Maud Olivier et Michel Pouzol, les UMP Nathalie Kosciusko-Morizet et Franck Marlin, l’écologiste Éva Sas et Nicolas Dupont-Aignan (DLR).

Au conseil régional d'Île-de-France, le département de l’Essonne est représenté par vingt-quatre conseillers régionaux. Les électeurs essonniens ont choisi pour les représenter durant la mandature 2010-2015 les socialistes Hicham Affane, Marie-Christine Carvalho, Hatouma Doucouré, Julien Dray, Hella Kribi-Romdhane, Marianne Louis et Olivier Thomas, les écologistes Laurence Bonzani, Ghyslaine Degrave, Serge Guérin, Jean-Vincent Placé, Jacques Picard et Jean-Luc Touly, les UMP Stéphane Beaudet, Véronique Carantois, François Durovray, Marie-Claude Girardeau, Hervé Hocquard et Nathalie Kosciusko-Morizet, les Parti de Gauche François Delapierre et Audrey Galland, la radicale Faten Hidri, le républicain et citoyen Jean-Marc Nicolle, la communiste Charlotte Blandiot-Faride.

Le département de l’Essonne compte cent quatre-vingt-seize communes dont quarante-neuf sont dirigées par un maire divers droite, vingt-six sont dirigés par un maire UMP, vingt-sept par un maire socialiste, quatorze par un maire divers gauche, treize par un maire radical, quatre par un maire communiste, trois par un maire affilié au Parti de gauche, deux par un maire centriste, , une par un maire affilié au Mouvement républicain et citoyen, une par un maire Vert, une par un maire Cap21, une par un maire MoDem, et une par un maire affilié au mouvement Debout la République.

Sept présidents se sont succédé à la tête du conseil général puis départemental depuis l’élection du premier en 1967 :

En 2009, le département de l’Essonne est subdivisé en trois arrondissements :

Cette même année, les services de l’État dans le département était dirigés par le préfet Jacques Reiller et les sous-préfets Pascal Sanjuan, secrétaire général et sous-préfet de l’arrondissement d’Évry, Daniel Barnier, sous-préfet de Palaiseau, et Thierry Somma, sous-préfet d’Étampes. Le préfet délégué pour l'égalité des chances est Pierre Lambert.

Les communes du département ont choisi de se regrouper au sein de 5 communautés d’agglomération (Cœur d'Essonne Agglomération, Communauté Paris-Saclay, Grand Paris Sud Seine Essonne Sénart, Val d'Yerres Val de Seine et Versailles Grand Parc) et de 7 communautés de communes (2 Vallées, Entre Juine et Renarde, l'Étampois Sud-Essonne, Le Dourdannais en Hurepoix, l'Orée de la Brie, le pays de Limours et le Val d'Essonne). 6 communes sont par ailleurs rattachées à la métropole du Grand Paris.

Le département de l’Essonne est divisé en trois niveaux de circonscriptions électorales, le dernier découpage électoral étant intervenu en 1986. En 2009, un nouveau redécoupage a entraîné le transfert des communes de Bruyères-le-Châtel et Ollainville (canton d'Arpajon) de la troisième à la quatrième circonscription.

En 2010, il est ainsi divisé en dix circonscriptions législatives :
Il est aussi divisé en vingt-et-un cantons :
Enfin, le département est divisé en cent quatre-vingt-seize communes dont la plus récente, Les Ulis a été créée le . La plus petite commune est Villiers-sur-Orge avec seulement carré, la plus étendue est Étampes avec carrés. D’après les données du recensement intervenu en 2008, la moins peuplée était Chatignonville avec seulement , la plus peuplée était le chef-lieu Évry avec habitants.

Outre le découpage électoral départemental, l’Essonne est intégré dans la circonscription Île-de-France dans le cadre des élections du Parlement européen et de cette même circonscription d’Île-de-France pour les élections régionales.

L’analyse des derniers résultats électoraux d’envergure supra-départementale montre que le département de l’Essonne suit les tendances nationales avec une propension légère au vote à gauche comme en témoignent les nettes avancent des candidats du socialistes lors des scrutins de 2004 et le score légèrement supérieur au national de la candidate Ségolène Royal lors de l’élection présidentielle de 2007. Cette tendance se traduit aussi lors des échéances départementales, le conseil général de l'Essonne est ainsi dirigé par l’alliance de gauche depuis 1998, même si l’on assiste depuis la dernière élection de 2008 à un rééquilibrage et une concentration des forces, légèrement en faveur de la droite. Cette propension au vote de gauche toutefois absente lors des élections législatives puisque les députés de gauche ne sont plus majoritaires dans le département depuis 1988 en n’étant plus que trois sur dix depuis 2002. De la même façon, les élections municipales amènent traditionnellement une majorité d’élus de droite au pouvoir, constat confirmé en 2008 avec cent deux communes à droite et seulement cinquante-cinq à gauche, avec une très nette différence entre le sud du département, presque exclusivement à droite et le nord, plus diversifié.

Élections présidentielles, résultats des deuxièmes tours :
Élections européennes, résultats des deux meilleurs scores :
Élections sénatoriales, résultats des deux meilleurs scores :
Élections régionales, résultats des deux meilleurs scores :
Élections référendaires :

Les établissements scolaires du département de l’Essonne dépendent tous de l’académie de Versailles, ils sont sous la direction de l’inspection académique de l’Essonne. En 2009, huit cent trente-trois écoles maternelles et élémentaires publiques sont réparties sur le territoire, découpé en cinq bassins d’éducation, complétées par vingt-neuf écoles privées. Les collèges sont gérés et entretenus pas le conseil départemental, eux aussi répartis entre cinq bassins, le département en compte cent sur l’ensemble du territoire dont vingt-trois disposant d’une section d'enseignement général et professionnel adapté. Les lycées, sous la responsabilité de la région, sont au nombre de quarante-quatre répartis dans le département. La direction diocésaine de l’enseignement catholique gère en plus seize collèges et treize lycées privés. Deux Greta sont installés à Massy et Corbeil-Essonnes. Onze centres d’information et d’orientation sont répartis sur le territoire.

Plusieurs établissements d’enseignement supérieur d’envergure régionale ou nationale sont implantés dans le département de l’Essonne, constituant deux pôles étudiants d’importance. Au total, en 2006, le département comptait ainsi élèves et étudiants de cycle supérieurs, soit 16 % du total francilien, Paris intra-muros absorbant à lui seul plus de 50 %.

À l’est, autour d’Évry se trouvent l’université d'Évry-Val d'Essonne, l’École nationale supérieure d'informatique pour l'industrie et l'entreprise, Télécom SudParis, Télécom École de Management et une antenne du Conservatoire national des arts et métiers.

Au nord-ouest, le plateau de Saclay et ses alentours concentrent un important centre d’enseignement de pointe avec la présence à Orsay-Bures-sur-Yvette de l’université Paris-Sud 11 complétée par l’Institut de formation d'ingénieurs de l'université Paris-Sud 11 à Orsay et l’Institut des hautes études scientifiques à Bures-sur-Yvette, à Villebon-sur-Yvette de l’École supérieure d'administration de l'armement, à Gif-sur-Yvette de l’École supérieure d'électricité, à Palaiseau de l’École polytechnique, de l’École supérieure d'optique et de l'ENSTA ParisTech et à Massy de l’École nationale supérieure des industries agricoles et alimentaires. Avec les Yvelines, l'Essonne est l'un de deux départements concernés par le projet de "cluster" technologique Paris-Saclay.

Enfin, à Brétigny-sur-Orge se trouve l’Institut de médecine aérospatiale du service de santé des armées et à Étiolles un Institut universitaire de formation des maîtres.

La santé et le social sont deux thèmes qui entrent dans les compétences du conseil départemental. À ce titre, il est chargé de coordonner les actions en faveur des personnes âgées, des handicapés, des enfants, des familles et des personnes en difficulté.

Au janvier 2012, l'Essonne comptait près de 142 lits pour personnes âgées de 75 ans et plus - taux d'hébergement largement supérieur à la moyenne nationale. Ce taux prend en compte toutes les structures d'hébergement pour séniors: maisons de retraite, foyers-logement, unités de soins de longue durée (USLD) et hébergement temporaire. Près de 101 lits pour 1000 sont disponibles en EHPAD et USLD uniquement. À noter que le taux national de structures médicalisées pour seniors s'élève à 103 ‰.

Il dispose aussi d’un droit de regard dans la gestion des établissements publics de santé présents sur le territoire départemental. En Essonne, quatorze centres hospitaliers et hôpitaux sont installés à Arpajon, Ballainvilliers, Briis-sous-Forges, Champcueil, Dourdan, Draveil, Épinay-sur-Orge, Étampes, Évry, Fleury-Mérogis, Juvisy-sur-Orge, Longjumeau, Orsay, Quincy-sous-Sénart et Yerres. Certains hôpitaux spécialisés d’importance régionale sont implantés en Essonne dont l’hôpital Joffre-Dupuytren et l’hôpital Georges-Clemenceau qui dépendent tous deux de l’Assistance publique - Hôpitaux de Paris. À terme, le centre hospitalier sud francilien actuellement répartis sur vingt-sept sites avant son transfert à Évry deviendra le principal centre de santé du département. Ils sont complétés par quatorze cliniques à Arpajon, Athis-Mons, Brunoy, Crosne, Étampes, Évry, Juvisy-sur-Orge, Longjumeau, Massy, Morangis, Ris-Orangis, Saclas, Villiers-sur-Orge et Viry-Châtillon. Au total, quatre-vingt-cinq maisons de retraite plus ou moins médicalisées accueillent les personnes âgées dépendantes, complétant ainsi l’offre de soin au même titre que les quarante-six établissements d’accueil des handicapés. Soixante-deux centres de protection maternelle et infantile relaient sur le terrain les actions du conseil général.

L’organisation juridictionnelle permet au département de l’Essonne de disposer de plusieurs tribunaux et maisons de justice. Le chef-lieu d’Évry accueille ainsi une cour d’assises, un tribunal d’instance, de commerce, de grande instance et un conseil de prud’hommes ainsi que le barreau départemental qui regroupe deux cent cinquante et un avocats. Il est complété par les tribunaux d’instance d’Étampes, Juvisy-sur-Orge, Longjumeau et Palaiseau et par les conseils de prud’hommes d’Étampes et Longjumeau. L’ensemble de ces palais de justices dépendent de la cour d'appel de Paris et du tribunal administratif de Versailles. Le département est doté d’un centre départemental d’accès au droit qui a mis en place trois maisons de justice et du droit à Athis-Mons, Les Ulis et Villemoisson-sur-Orge. Douze permanences du représentant du médiateur de la République sont réparties sur le territoire, vingt conciliateurs de justice exercent dans le département pour traiter les conflits mineurs.

Le département accueille depuis 1968 le plus grand centre pénitentiaire d’Europe avec la maison d'arrêt de Fleury-Mérogis d’une capacité de places, complétée par le centre de semi-liberté de Corbeil-Essonnes.

La sécurité départementale relève de la préfecture de l’Essonne qui coordonne les services de police et de gendarmerie. Les services de police sont organisés autour des trois districts d’Évry, Palaiseau et Juvisy-sur-Orge et de quatorze circonscriptions. La gendarmerie nationale compte trois compagnies à Étampes, Évry et Palaiseau et trente-et-une brigades territoriales. Deux compagnies républicaines de sécurité sont stationnées dans le département à Massy et Bièvres, commune qui accueille aussi le centre de formation et de commandement du Raid. L’organisation des secours dépend du service départemental d'incendie et de secours qui regroupe agents dont 56 % de volontaires répartis dans cinquante-et-un centres de secours.

Le département de l’Essonne relève de la région terre Île-de-France dont le siège est basé à l’hôtel des Invalides de Paris et l’état-major au camp des Loges à Saint-Germain-en-Laye et de la Zone de défense et de sécurité de Paris. Le département dispose d’un délégué militaire départemental basé à Montlhéry et sur son territoire de plusieurs corps militaires dont l’école polytechnique à Palaiseau, le du train et la logistique implantés à Montlhéry, le logistique du commissariat à l’armée de Terre, le bureau d'enquête accident Défense-air et la structure intégrée de maintien en condition opérationnelle des matériels aéronautiques de la Défense basés à Brétigny-sur-Orge sur la base aérienne 217.

En 2008, tonnes de déchets ont été collectés dans le département soit une moyenne de cinq cent trente-trois kilogrammes par habitant dont 47 % étaient incinérés et 13 % stockés, vingt-quatre déchèteries étaient opérationnelles sur le territoire gérées par neuf syndicats intercommunaux.

La Politique culturelle est une des compétences du conseil départemental, à ce titre, il dispose d’un service spécialisé, chargé de soutenir les initiatives locales et les lieux d’expression culturelle.

Le département de l’Essonne dispose d’une multitude de lieux répartis sur le territoire, presque chaque commune disposant d’une salle polyvalente, d’une médiathèque, d’un centre culturel ou d’une maison des jeunes et de la culture. Un maillage important de salles de cinéma complète cette offre. Trois lieux se distinguent cependant par leur importance, le théâtre de l’Agora à Évry, labellisé scène nationale, l’opéra de Massy, labellisée scène conventionnée lyrique et le centre d'art contemporain du château de Chamarande. Plusieurs musées sont répartis aux quatre coins du département, dont certains d’envergure nationale tel le musée français de la photographie à Bièvres. Des festivals réputés sont organisés par les communes du département comme le festival international du cirque de Massy, le festival de cinéma "emergence" à Marcoussis ou par des personnes privées comme la fête des plantes vivaces au domaine de Saint-Jean-de-Beauregard. Héritages des siècles passés, les foires d’Arpajon, Montlhéry et Dourdan marquent encore le calendrier départemental.

Les communes du département ont par ailleurs toujours attiré les artistes, devenant des centres d’expression et de création, comme Étampes où naquit une École de peinture, Milly-la-Forêt qui accueillit Jean Cocteau, Christian Dior, Jean Marais et Jean Tinguely, auteur du fameux Cyclop. D’autres artistes sont originaires du département, parmi lesquels Dany Brillant, Alain Chabat, Marc Lavoine ou Jean-Luc Lemoine, d’autres comme Claude François ont choisi de s’y installer.

Le Sport est aussi une compétence acquise par le conseil départemental, il participe ainsi au financement des installations et aux subventions des clubs. Le département a ainsi accueilli certaines épreuves des jeux de la francophonie 1994, notamment au Grand Dôme de Villebon-sur-Yvette, construit pour l’occasion, il accueille aussi depuis 2002 le Centre national du rugby à Marcoussis. Presque chaque commune disposent d’infrastructures à caractère sportif, un réseau de piscines parsème le territoire mais deux sites se démarquent, le stade omnisports Robert-Bobin à Bondoufle, quatrième francilien par la taille avec places (derrière le Stade de France, le Parc des Princes et le Stade Charléty) et le stade nautique Maurice Herzog de Mennecy, dimensionné pour les compétitions internationales.

Plusieurs clubs sportifs évoluent au niveau national, l’AS Corbeil-Essonnes, l’AS Évry, le Juvisy FCF, le Sainte-Geneviève Sports, l’Entente sportive Viry-Châtillon en football, le RC Massy Essonne en rugby à XV, le Viry-Châtillon Essonne Hockey et le SCA 2000 Évry en hockey sur glace, les Gothics de Gif-sur-Yvette et les Lions de Savigny-sur-Orge en baseball, les Corsaires d'Évry et les Quarks de Villebon en football américain, le Massy Essonne HB en handball, le RC Villebon 91 en volley-ball, l’ASCE en canoë-kayak et le Viry Évry Nord Sud Essonne en athlétisme.

Autrefois, le département était aussi connu pour les courses automobiles organisées à l’autodrome de Linas-Montlhéry et pour l’organisation du tour cycliste de l'Essonne. Aujourd’hui, l’évènement sportif marquant est l’Open international Stade français Paris au golf de Courson-Monteloup. Outre Montgeron qui fut la ville de départ du premier Tour de France 1903, plusieurs communes du département ont été villes-étapes du Tour de France : Épinay-sous-Sénart en 1987, Brétigny-sur-Orge en 1990 et 1993, Montlhéry et Viry-Châtillon en 1993, Sainte-Geneviève-des-Bois en 1995, Palaiseau en 1996, Arpajon en 1999, Évry en 2001, Montgeron en 2003, Corbeil-Essonnes en 2001 et 2005, Marcoussis en 2007, Étampes en 2008, Longjumeau en 2010.

Plusieurs personnalités du sport sont originaires du département, parmi lesquels Thierry Henry et Patrice Évra des Ulis, Ladji Doucouré d’Évry et Mathieu Bastareaud de Massy.

Le culte catholique est organisé en Essonne autour du diocèse d'Évry-Corbeil-Essonnes, qui couvre le département et deux villes voisines des Yvelines. Il est divisé en deux zones, cinq vicariats, vingt-trois secteurs paroissiaux et cent huit paroisses. Son siège est installé à Évry, près de la cathédrale de la Résurrection, et il dispose du siège associé de Corbeil-Essonnes de la cathédrale Saint-Spire. Le culte musulman dispose à Courcouronnes de la plus grande mosquée de France, la mosquée d'Évry-Courcouronnes, et d’une multitude de centres de prières répartis sur le territoire. La religion juive dispose de synagogues dans certaines communes du département, dont la plus importante à Massy. Les protestants disposent de temples répartis dans plusieurs communes. Les chrétiens orthodoxes se retrouvent dans plusieurs lieux, principalement l’église Notre-Dame-de-la-Dormition de Sainte-Geneviève-des-Bois et le siège de la métropole orthodoxe roumaine d'Europe occidentale et méridionale à Limours. Les bouddhistes disposent de la Pagode Khánh-Anh à Évry. L’Église de Jésus-Christ des saints des derniers jours dispose d’un lieu de culte à Évry.

Plusieurs groupes de médias diffusent les informations locales. La presse écrite est représentée par l'hebdomadaire le Républicain de l’Essonne et les éditions locales du quotidien Parisien. Sur la toile, le web-journal Essonne Info publie une édition quotidienne consacrée à l'actualité politique, économique, sociétale, sportive et culturelle.

Après 25 années d'activités, la chaîne de télévision locale Téléssonne a cessé d'émettre le 30 septembre 2014.

Intégré à la région Île-de-France, plus importante région européenne par son produit intérieur brut (PIB), le département de l’Essonne bénéficie de son attrait économique et y participe pleinement, avec un PIB départemental fixé en 2008 à euros, soit euros par habitant, cependant en régression puisqu’il était fixé à euros par habitants en 2005. En 2008, l’Essonne participait à hauteur de millions d’euros aux exportations nationales avec en tête des productions exportées les produits pharmaceutiques et les produits d’entretien ou de parfumerie, et pour millions d’euros aux importations nationales avec en tête des produits importés les machines de bureau, matériels informatiques et appareils d’émission ou réception de son et d’image. Avec un total en 2006 de emplois, dont 81,5 % relevant du secteur tertiaire, elle suit l’évolution économique et sociologique régionale avec cependant une certaine propension à conserver des activités industrielles (11,5 % des emplois) et dans une moindre mesure agricoles (0,8 % en Essonne pour 0,3 % en Île-de-France). Avec une population active évaluée à personnes, le département apparaît cependant comme déficitaire en nombre d’emplois, entraînant un taux de chômage de 8,9 % en 2006 ( demandeurs d’emploi) et des déplacements pendulaires de résidents allant travailler hors du département (42,5 % des Essonniens travaillaient en 2006 hors de l’Essonne). Cette situation implique le développement d’un réseau de transports en commun fortement orienté vers Paris et sa proche banlieue, trois lignes du RER d'Île-de-France, deux lignes à grande vitesse, deux autoroutes et trois routes nationales traversant le territoire pour converger vers la capitale.

Cette présence dans la « région capitale », ces infrastructures et l’histoire récente du département lui permettent aujourd’hui de concentrer sur son territoire une diversité et une richesse économique relativement importante. L’agriculture occupe ainsi une place toujours importante, avec près de trois mille cinq cents hectares cultivés aux portes de l’agglomération parisienne, sur près de 50 % du territoire consacrés pour, 80 % des exploitations se consacrant à la grande culture céréalière au sud et 16 % au maraîchage. Le commerce occupe lui aussi une place importante et historique, occupant 15,4 % des employés, pour la plupart dans de vastes centres commerciaux, dont le plus grand de la région, La Croix-Blanche sur plus de mètres carrés. L’industrie constitue le troisième pilier économique historique du département, elle y est aujourd’hui prioritairement tournée vers la haute technologie, notamment grâce à la présence concentrée de plusieurs universités et grandes écoles. Ainsi, la recherche scientifique a peu à peu pris une importance majeure dans l’économie départementale, au point d’employer plus de personnes en 2005. Le tourisme enfin, d’agrément ou d’affaires occupe une place non négligeable dans l’économie locale avec la présence de deux bases régionales, de châteaux, d’édifices religieux classés aux monuments historiques, de parcs et jardins remarquables, du parc naturel régional du Gâtinais français, certains secteurs du département ayant conservé un caractère rural, vingt-sept communes comptaient en 2006 plus de 10 % de résidences secondaires.

Statistiquement, l’Insee découpe le territoire en huit zones d’emploi dépassant les limites administratives. Le nord-ouest appartient ainsi à la zone d’emploi de Boulogne-Billancourt, le centre-nord à la zone d'emploi d'Orly, le nord-est à la zone d’emploi de Créteil, l’est à la zone d’emploi d’Évry, le sud-ouest à la zone d’emploi d’Étampes, l’ouest aux zones d’emploi de Dourdan et Orsay, une petite enclave étant rattachée à la zone d’emploi de Versailles.

Le département est au cœur du projet Paris-Saclay, inspiré de la Silicon Valley. Géographiquement, deux pôles majeurs de développement économique se distinguent, disposant chacun d’une implantation de la chambre de commerce et d'industrie de l'Essonne et de la chambre de métiers et de l'artisanat de l'Essonne. Au nord-ouest dans la « vallée de la Science », le parc d'activités de Courtabœuf, le plateau de Saclay et la vaste zone industrielle de Massy se concentrent l’université Paris-Sud 11, le pôle ParisTech regroupant plusieurs grandes écoles et de nombreux centres de recherches (Danone, Motorola, Thales, Alcatel-Lucent, Hewlett-Packard, etc.).

L’ensemble est aujourd’hui intégré aux pôles de compétitivité System@tic Paris-Région, Opticsvalley et à l’opération d'intérêt national de Massy Palaiseau Saclay Versailles Saint-Quentin-en-Yvelines. Au nord-est, de Corbeil-Essonnes à Orly en suivant la vallée de la Seine, s’organisent autour de l’université d'Évry-Val d'Essonne et du Génopole le pôle Médicen et autour du centre national d'études spatiales et d’Arianespace le pôle ASTech.

Plusieurs pépinières d’entreprises sont réparties sur le territoire : X Technologies à Palaiseau, Incuballiance à Orsay, Scientipole à Palaiseau, Orsay et Gif-sur-Yvette, Apis Développement à Courtabœuf, Innov'Valley à Marcoussis, Génopole, INT et Magellan à Évry.

Certains lieux sensibles du département bénéficient en outre du statut de zone franche urbaine, dont les quartiers de La Grande Borne et du Village à Grigny et Viry-Châtillon, des Cinéastes et de la Plaine à Épinay-sous-Sénart, des Tarterêts à Corbeil-Essonnes, des Pyramides à Évry,Les Ardrets, Rosières à Brétigny-sur-Orge.

En 2006 les catégories socioprofessionnelles les plus représentées dans les ménages essonniens étaient les cadres (20,7 %) suivis des professions intermédiaires (20,5 %), puis les retraités (18,6 %) et les ouvriers (18,2 %). Cette même année, 92,6 % des actifs ayant un emploi étaient salariés, dont 80,8 % titulaires d’un emploi fixe. Le revenu net imposable moyen du département était alors fixé à euros, mais seul 66,1 % des foyers étaient effectivement assujettis à l’impôt sur le revenu avec un revenu net imposable moyen à euros.

Bien que le département soit situé en Île-de-France et dans l’agglomération parisienne, l’agriculture occupe encore une place importante dans l’économie locale, au point de modeler le paysage départemental. Si elle n’occupait en 2006 que agriculteurs exploitant, soit seulement 0,2 % des actifs, au total ce sont personnes qui occupait un emploi dans l’agriculture, soit 0,8 % de la population active. En 2000, exploitations étaient réparties sur le territoire, d’une superficie moyenne de soixante-dix-neuf hectares et pour un total de hectares soit 49,45 % de la superficie totale du département. Cette agriculture, organisée sur les plaines de Beauce et les vallées du Hurepoix et du Gâtinais, est tournée pour 805 exploitations vers la grande culture céréalière, pour 164 d’entre elles vers le maraîchage et pour trente-huit vers l’élevage, le cheptel départemental étant constitué cette année-là de trente-trois bovins et cent dix-sept volailles. Alimentant la région en produit frais, de nombreuses exploitations ont fait le choix de l’agriculture raisonnée comme pour celles adhérentes à l’association du Triangle vert du Hurepoix ou biologique, malgré leur quasi intégration au centre d’espaces urbains, d’autres comme la ferme de Viltain sur le plateau de Saclay ont choisi de proposer leurs productions directement aux consommateurs, ajoutant une fonction pédagogique à leurs activités. Trente communes du sud-est du territoire sont intégrées à la région d’indication géographique protégée de la « Volaille du Gâtinais ». Quarante pour cent de la production nationale française de cresson est originaire d’Essonne, à tel point que cette plante est surnommée « l’or vert » du département.

Très tôt, le territoire fut situé aux carrefours de routes commerciales où se développèrent des foires, comme à Dourdan qui disposait d’une halle dès le ou Milly-la-Forêt dès le . C’est aussi dans l’Essonne que s’ouvrit en 1963 le premier hypermarché de France à l’enseigne Carrefour. Le commerce dans le département aujourd’hui, outre les centres-villes actifs, s’organise autour de grands centres commerciaux avec par ordre de tailles, La Croix-Blanche qui rassemble 164 enseignes sur sept cent mille mètres carrés de surface, Évry 2 qui accueille 235 magasins sur cent mille mètres carrés de surface, Villebon 2 qui propose soixante enseignes sur environ soixante mille mètres carrés, Ulis 2 qui offre 120 enseigne, le Centre commercial Brétigny-Maison Neuve,Villabé A6 avec 68 magasins, Exona et VdB, de moindre importance. S’ajoute à Corbeil-Essonnes un centre de magasins d’usine géré par le groupe Marques Avenue. En 2006, 15,4 % des emplois relevaient du secteur du commerce.

L’industrie en Essonne a une histoire ancienne, bien avant la création du département, Corbeil et Essonnes étaient réputées pour leurs usines de minoterie dont subsiste encore aujourd’hui les Grands moulins de Corbeil, de tannerie, de poudrerie. Plus tard, dans le même secteur géographique, la famille Darblay qui possédait les papeteries fit fortune dans l’industrie locale, Paul Decauville faisant lui fortune avec sa sucrerie à Évry puis dans le matériel ferroviaire avec l’invention de la Decauville.

Le département occupe encore 11,5 % de sa main-d’œuvre dans l’Industrie, mais il s’agit maintenant principalement d’industrie de pointe, implantée à proximité des grands centres de recherche, notamment dans le parc d'activités de Courtabœuf. En , la répartition des établissements donnait 0,1 % pour l’industrie extractive, 6,8 % pour l’industrie manufacturière, 0,1 % pour la production d’énergie et 0,3 % pour la production et le traitement des eaux. Les centres de production et de recherche d’Alcatel-Lucent à Marcoussis, Arianespace à Évry, Altis Semiconductor à Corbeil-Essonnes, Faurecia à Étampes bénéficient de la présence de personnels bien formé (45 % de la population dispose d’un diplôme supérieur ou égale au baccalauréat), d’infrastructures de transports de qualité et variées.

Grâce à la présence de deux campus sur son territoire à Orsay (université Paris-Sud 11) et Évry (université d'Évry-Val d'Essonne) et de nombreuses grandes écoles (Polytechnique, Supélec, Télécom SudParis, Télécom École de Management…), d’importants laboratoires et centres de recherches ont choisi l’Essonne pour s’implanter. Ainsi, le Commissariat à l'énergie atomique dispose de deux centres importants à Saclay et Bruyères-le-Châtel, le Centre national de la recherche scientifique est à Gif-sur-Yvette, l’Office national d'études et de recherches aérospatiales et Danone à Palaiseau, Thales à Orsay et Limours, Alcatel-Lucent à Nozay, Hewlett-Packard aux Ulis, Microsoft à Villebon-sur-Yvette, l’Institut national de la recherche agronomique à Leudeville, l’Institut national de la santé et de la recherche médicale, le Génopole, Arianespace et le Centre national d'études spatiales à Évry, la Snecma à Corbeil-Essonnes, le Synchrotron soleil à Saint-Aubin, Sagem Défense Sécurité à Massy. En outre, l'Université Paris-Saclay effectuera sa première rentrée en septembre 2015.

Afin d’aider ces acteurs, le conseil départemental a développé le dispositif d’action de soutien à la technologie et à la recherche en Essonne. Plusieurs pôles de compétitivités ont en outre été développés par l’État et sont actifs sur le territoire départemental dont System@tic Paris-Région, Medicen, ASTech et Opticsvalley. Enfin, la chambre de commerce et d'industrie de l'Essonne a développé un site consacré à la recherche dans le département, centralisant les laboratoires et les organismes et la promotion du transfert de compétences. Cette même chambre de commerce a recensé entre 2004 et 2006 le dépôt de brevets, le secteur employant en 2005 personnes.

Le département de l’Essonne n’est pas intrinsèquement réputé pour être un lieu touristique, cependant le patrimoine dont il dispose sur son territoire et les infrastructures développées permettent au département de recevoir un nombre relativement important de visiteurs. Ainsi, les services du conseil départemental dénombre plus de huit cent mille visiteurs chaque année, plus de deux cent mille pour le seul site du château de Chamarande et la création en 2003 de cent vingt-sept entreprises directement liées au tourisme. Selon le comité départemental du tourisme, le chiffre d’affaires du tourisme s’élève globalement à trois cent soixante-et-un millions d’euros, répartis en soixante-trois millions consacrés aux loisirs dans le département et deux parts approximativement égales (151 millions et 146 millions d’euros) pour le tourisme d’agrément et le tourisme d'affaires. En 2008, divers sites ont ainsi accueilli plus de vingt mille visiteurs, le trio de tête étant la verrerie d’art de Soisy-sur-École avec visiteurs, la base de plein air et de loisirs d'Étampes avec visiteurs et Koony Parc à Bondoufle avec visiteurs.

Le tourisme en Essonne tourne autour de six grands axes, les châteaux et leurs jardins, les édifices religieux, les musées, les maisons d’artistes dont celles de Victor Hugo à Bièvres, d’Alphonse Daudet à Draveil, de Claude François à Dannemois et de Tsugouharu Foujita à Villiers-le-Bâcle, le tourisme d’affaires à destination des grandes entreprises implantées et les activités sportives. Ces dernières se pratiquent dans l’une des deux bases de loisirs d’Étampes et du Port-aux-Cerises, la randonnée pédestre ou le cyclotourisme sur les circuits balisés, dont le GR 1, le GR 2 et le GR 11, les quinze golfs dont celui du Stade français Paris rugby à Courson-Monteloup, les parcours d’accrobranche, les piscines ou les clubs équestres.

En 2009, l’Insee dénombrait quatre-vingt-dix-huit hôtels dans le département totalisant chambres et vingt-et-un campings totalisant emplacements, auxquels s’ajoutaient les gîtes ruraux et les chambres d’hôtes.

Selon le recensement de la population de 2006, logements soit 1,43 % des logements disponibles dans le département étaient des résidences secondaires ou occasionnelles.

Le département de l’Essonne, pour moitié intégré à l’agglomération parisienne dispose néanmoins d’un environnement préservé sur une large part de son territoire. Ainsi, près de cent trente-neuf mille hectares, soit près de 78 % du territoire sont encore classés par l’Iaurif comme des espaces ruraux. Les quatre régions naturelles qui composent le département, le Hurepoix, la Brie, le Gâtinais et la Beauce présentent chacune des paysages typiques et bien distincts. Au nord-est, la Brie en Essonne est couverte par la vaste forêt de Sénart aux essences de chêne, de châtaignier, de charme et de bouleau, le Hurepoix mélange vallées boisées et plateaux agricoles, le Gâtinais est pour sa plus grande partie recouvert par l’importante forêt de Fontainebleau et sa forêt annexe de Milly, dont le sol sablonneux et rocailleux est couvert de chêne, de pin sylvestre et de hêtre, la plaine de Beauce est elle presque entièrement recouverte de grandes cultures céréalières.

D’ouest en est, le département est traversé par la ceinture verte d’Île-de-France, avec depuis la forêt de Rambouillet dans les Yvelines, un bandeau formé par la forêt de Dourdan et celle d’Angervilliers, puis la forêt de la Roche Turpin, la forêt du Belvédère, la forêt des Grands Avaux et la forêt de Milly-la-Forêt qui rejoint la forêt de Fontainebleau à l’est. Au nord du département, la forêt de Verrières et la forêt de Sénart forment deux espaces préservés en bordure de la première couronne parisienne. Les forêts de Palaiseau, du Rocher de Saulx et de Bellejame complètent ces massifs.

Plusieurs parcs d’envergures départementales parsèment le territoire et permettent une approche plus ou moins naturelle de l’environnement. Les deux plus importants sont la base de plein air et de loisirs d'Étampes et la base de plein air et de loisirs du Port-aux-Cerises au nord. Elles sont complétées dans leur rôle pédagogique par l’arboretum Vilmorin et l’arboretum municipal de Verrières-le-Buisson, l’arboretum de Segrez à Saint-Sulpice-de-Favières et le Conservatoire national des plantes à parfum, médicinales, aromatiques et industrielles à Milly-la-Forêt. Le parc de Jeurre à Morigny-Champigny, le parc de Chamarande, le parc du château de Courances, le parc de Courson et le parc du château à Saint-Jean-de-Beauregard attirent eux aussi les visiteurs. S’ajoutent deux initiatives environnementale récentes, la coulée verte du sud parisien qui traverse le nord-ouest du département de Verrières-le-Buisson à Gometz-le-Châtel avec une continuation prévue jusque Rambouillet par Limours et la Méridienne verte qui traverse le département en son centre du nord au sud.

Dans ces parcs et forêts plusieurs arbres remarquables ont été recensés dont un tilleul à Boutigny-sur-Essonne, un chêne à Bures-sur-Yvette, des platanes à Chamarande et Morsang-sur-Orge, des séquoias à Courson-Monteloup et Mennecy, un sophora du Japon à Juvisy-sur-Orge et des hêtres communs à Saint-Sulpice-de-Favières.

Deux espaces renommés occupent aussi une part importante du territoire. Au nord-ouest, la Vallée de Chevreuse suit le cours de l’Yvette jusqu’à Palaiseau, avec l’éventualité en 2010 d’une extension du parc naturel régional de la Haute Vallée de Chevreuse, seize communes du département ayant déjà approuvé le principe de l’intégration. Au sud-est, le parc naturel régional du Gâtinais français englobe vingt-huit communes du département entre les vallées de l’Essonne et de l’École.

Dépassant pour certains les limites administratives, dix sites ont été recensés par le réseau Natura 2000 dont trois sont classées « Zone de protection spéciale » : les marais d'Itteville et de Fontenay-le-Vicomte sur cinq cent vingt-deux hectares, le Massif de Fontainebleau sur les communes de Courances et Milly-la-Forêt et le Massif de Rambouillet dont 4 % du territoire se trouve en Essonne. S’ajoutent des sites d’importances communautaires comme les champignonnières d’Étampes, les buttes gréseuses de l’Essonne, les marais des basses vallées de la Juine et de l’Essonne, les pelouses calcaires du Gâtinais et de la haute vallée de la Juine et la haute vallée de l’Essonne. Réparties dans plusieurs communes, la Réserve naturelle des sites géologiques de l'Essonne qui occupe près de cinq hectares est classée Réserve naturelle nationale, elle est complétée par plusieurs réserves naturelles régionales dont le bassin de Saulx-les-Chartreux, le parc d’Itteville, les Grands Réages à Varennes-Jarcy et l’arboretum Roger de Vilmorin à Verrières-le-Buisson. Le ministère de l'Écologie, de l'Énergie, du Développement durable et de la Mer a lui aussi classé un certain nombre de sites dont la vallée de la Juine et ses abords, la vallée de l’Yerres et ses abords. Enfin, le conseil départemental de l’Essonne a acquis des terrains pour les classer en « Espace naturel sensible ».

Principalement résidentielles, les communes du département font des efforts de politique environnementale et d’embellissement, récompensées pour certaines par des fleurs au concours des villes et villages fleuris, Sainte-Geneviève-des-Bois est classée quatre fleurs ; Boutigny-sur-Essonne, Chilly-Mazarin, Corbeil-Essonnes, Dourdan, Étampes, Évry, Les Ulis, Massy, Paray-Vieille-Poste, Ris-Orangis, Villebon-sur-Yvette et Viry-Châtillon sont classées trois fleurs ; Arpajon, Briis-sous-Forges, Épinay-sur-Orge, Étréchy, Grigny, Longjumeau, Morangis, Ollainville, Orsay, Palaiseau, Saint-Germain-lès-Arpajon, Saint-Michel-sur-Orge, Savigny-sur-Orge, Villejust, Wissous et Yerres sont classées deux fleurs ; Baulne, Boussy-Saint-Antoine, Brétigny-sur-Orge, Courcouronnes, Le Coudray-Montceaux, Linas, Mauchamps, Milly-la-Forêt, Morigny-Champigny, Richarville, Saclay, Soisy-sur-Seine et Vert-le-Grand sont classées une fleur.

Pour permettre de visiter ces espaces naturels, le département est équipé de plusieurs circuits de grande randonnée dont le et le qui ceinturent l’Île-de-France, le qui suit le cours de la Seine, le , le qui parcourt l’ensemble du département et le qui correspond à l’ancien chemin de pèlerinage de Saint-Jacques-de-Compostelle depuis Paris.

Le département de l’Essonne est situé dans le bassin parisien, territoire très tôt occupé par l’Homme, comme en témoigne la découverte de silex taillés et l’élévation au néolithique de menhirs, dont certains subsistent et sont aujourd’hui classés aux monuments historiques : la Pierre droite à Milly-la-Forêt, la Pierre Fritte à Étampes, la Fille de Loth à Brunoy. L’occupation gauloise puis gallo-romaine laissa des vestiges de villages comme celui découvert à Gif-sur-Yvette, de "villa rustica" comme à Orsay et d’"oppidum" comme à Champlan.

Du Moyen Âge subsistent à Longjumeau l’un des plus vieux ponts d’Île-de-France, daté du , le Pont des Templiers, des châteaux forts comme à Montlhéry (), Dourdan () ou Étampes (), les remparts de Corbeil-Essonnes ou des lieux de culte catholiques importants, tel la basilique Notre-Dame-de-Bonne-Garde de Longpont-sur-Orge du ou la collégiale Notre-Dame-du-Fort à Étampes du .

Placé dans une région agricole et à proximité des capitales de Versailles et Paris, le territoire fut à la Renaissance et durant l’Époque moderne équipé d’importantes halles à Dourdan, Arpajon (), Milly-la-Forêt () et Méréville (), enrichi de châteaux, dont les plus importants sont le château de Chamarande à Chamarande (), le château du Marais au Val-Saint-Germain (), le château de Courson à Courson-Monteloup () ou le château de Courances à Courances (, d’églises dont la cathédrale Saint-Exupère de Corbeil-Essonnes () ou l’église Saint-Germain-l’Auxerrois de Dourdan (), de lavoirs et de demeures bourgeoises.

Du Premier Empire à la Belle Époque, le département évolua vers l’industrie avec la construction d’importantes usines comme les Grands moulins de Corbeil du , de nombreuses communes devinrent des lieux de villégiatures pour les parisiens, qui se faisait construire des demeures et des folies, comme le temple de la Gloire à Orsay () et la propriété Caillebotte, à Yerres et de lieux de culte d’autres confessions comme l’église orthodoxe Notre-Dame-de-la-Dormition de Sainte-Geneviève-des-Bois ().

Le a lui aussi laissé un patrimoine contemporain avec l’édification de la vaste cité d’habitat social de La Grande Borne à Grigny, l’édification de la sculpture monumentale du Cyclop à Milly-la-Forêt, de l’unique cathédrale française du à Évry accompagnée par la plus grande mosquée de France à Courcouronnes et la plus grande pagode d’Europe toujours à Évry.

Au sud du département, Étampes, ancienne ville royale, concentre sur son territoire un patrimoine remarquable, bénéficiant ainsi du label « Villes et pays d'art et d'histoire ». Au total, ce sont cinquante-et-un châteaux et quatre-vingt-quatorze monuments religieux répartis sur le territoire qui bénéficient d’un classement ou d’une inscription aux monuments historiques.

Le département de l’Essonne a donné plusieurs personnalités politiques d’envergure nationale, parmi lesquelles des membres de gouvernement. Par ordre chronologique : Léo Hamon, secrétaire d'État de la Participation et de l’Intéressement dans le gouvernement Chaban-Delmas en 1972, Jacques Guyard, secrétaire d’État chargé de l’Enseignement technique dans le gouvernement Cresson de 1991 à 1992, Marie-Noëlle Lienemann, ministre délégué au Logement et au Cadre de vie dans le gouvernement Bérégovoy de 1992 à 1993, Jean de Boishue, secrétaire d’État chargé de l’Enseignement supérieur dans le gouvernement Juppé I en 1995, Jean-Luc Mélenchon, ministre délégué à l’Enseignement professionnel dans le gouvernement Jospin de 2000 à 2002, Pierre-André Wiltzer, ministre délégué à la Coopération et à la Francophonie dans le gouvernement Raffarin II de 2002 à 2004, Nathalie Kosciusko-Morizet, secrétaire d’État chargée de l’Écologie puis chargée de la Prospective et du Développement de l’Économie numérique dans le gouvernement Fillon II de 2007 à 2010 puis ministre de l’Écologie, du Développement durable, des Transports et Logement dans le gouvernement Fillon III de 2010 à 2012, Georges Tron, secrétaire d’État chargé de la Fonction publique dans le gouvernement Fillon II de 2010 à 2011, François Lamy, ministre délégué à la Ville dans les gouvernements Ayrault I et II depuis 2012, Manuel Valls, ministre de l’Intérieur dans les gouvernements Ayrault I et II entre 2012 et 2014 puis Premier ministre.

Le département est encore en grande partie couvert par les espaces de culture, dont on distingue le maraîchage au nord et les grandes cultures céréalières dans le sud, il était jusqu’au milieu du l’un des principaux fournisseurs en produits frais des halles de Paris et directement relié à elles par l’Arpajonnais. Certains produits sont ainsi réputés dans le département, comme la fraise dans la vallée de la Bièvre et sur le plateau de Saclay, la tomate dans la région de Montlhéry, le haricot et notamment l’espèce Chevrier autour d’Arpajon, le potiron rouge vif d'Étampes, la mâche verte d'Étampes et le Cresson de fontaine dans le sud-est. Le Gâtinais et la région de Milly sont aussi réputés pour leur plantes aromatiques et médicinales comme le Safran, la Menthe poivrée.

Il en découle quelques spécialités culinaires comme la Quiche au cresson de Milly-la-Forêt, le Vin de cresson de Méréville, le Pâté d'alouette de Chalo-Saint-Mars et la confiserie appelée Buchette d'Étampes.









</doc>
<doc id="1009" url="https://fr.wikipedia.org/wiki?curid=1009" title="Europe">
Europe

L’Europe est considérée politiquement comme un continent ou géographiquement comme une partie des supercontinents de l'Eurasie et de l'Afro-Eurasie. Elle est parfois appelée le « Vieux Continent », par opposition au « Nouveau Monde » (l'Amérique). Sur le plan culturel, l'Europe a reçu de multiples influences au cours des âges, et comprend de nombreux pays qui possèdent à la fois un héritage commun, des différences linguistiques, religieuses et historiques, et des apports récents venus depuis la mondialisation. À ce titre, l'Europe est un espace de civilisation forgé par une histoire millénaire. Une communauté de peuples, de différents États, tend à se constituer politiquement avec l'Union européenne.

L'Europe, et plus particulièrement la civilisation gréco-romaine, est le berceau de la civilisation occidentale. Entre le et le , les nations européennes ont contrôlé et colonisé à plusieurs reprises l'ensemble du continent américain, la quasi-totalité de l'Afrique, l'Océanie et de grandes parties de l'Asie. L'Europe est également à l'origine de plusieurs bouleversements historiques majeurs, comme la Renaissance, les grandes découvertes, le siècle des Lumières, la révolution industrielle ou des guerres mondiales.

Deux étymologies concurrentes sont le plus souvent proposées.

La première étymologie provient de l'usage par les marins phéniciens des deux mots "Ereb", le couchant, et "Assou", le levant pour désigner les deux rives opposées de la mer Égée : d'une part la Grèce actuelle et d'autre part l'Anatolie (Ἀνατολή signifiant pareillement, en grec, le "levant"). La première mention connue de ces mots sémitiques se trouve sur une stèle assyrienne qui distingue "Ereb", la nuit, le [pays du soleil] couchant, et "Assou", le [pays du soleil] levant. Ces deux mots sont probablement à l'origine des deux noms grecs "Eurôpè" et "Asia" dans leur acception géographique antique. En grec, dans un hymne à Apollon datant d’environ 700 avant notre ère, "Eurôpè" représente encore, comme "Ereb", le simple littoral occidental de l’Égée. La mythologie grecque perpétue l’origine sémitique du mot en en faisant le nom d’une princesse phénicienne. et c'est justement le nom de la princesse de Tyr enlevée par Zeus. Néanmoins, cette étymologie sémitique est à peine encore défendue et cette proposition est généralement considérée comme improbable ou indéfendable.

La seconde étymologie est purement grecque. Dans la mythologie grecque, plusieurs « Europe » sont connues, Europe, fille du géant Tityos ; la mère de Niobé ; la fille de Nil, une épouse de Danaé ; selon Hésiode, Europe l'Océanide est l'une des trois mille nymphes d'Océan et de Téthys ; dans l'Iliade, Europe est la fille de Phœnix, ascendant du peuple phénicien. "Europè" ("εὐρώπη") provient de deux mots grecs : et . Le premier, , signifie soit large, qui s'étend en largeur, soit vaste, qui s'étend au loin ; le second, en grec ancien , signifie soit regarder en face, regard, soit œil. Le terme signifie « [celle qui a] de grands yeux » et devient un prénom féminin, donné à plusieurs personnages mythologiques grecs, et notamment à la fameuse princesse Europe enlevée par Zeus déguisé en taureau. Selon Jean Haudry, ce doublet du nom féminin "Eurôpè" désigne la « terre ». Hérodote fait remarquer que la jeune princesse ne pose jamais le pied sur le continent du côté grec désigné par le terme géographique "Eurôpè" puisque Zeus la dépose en Crète.

En 1961, des archéologues spécialistes de l'Empire hittite, avaient émis l'hypothèse que les noms des deux continents "Europe" et "Asie" viendraient de deux royaumes voisins de l'Empire hittite situés de part et d'autre de l'actuel Bosphore. "Avrupa", correspondant approximativement à la Thrace, aurait donné le nom « Europe » tandis que "Assuwa", correspondant au quart nord-ouest de l'actuelle Turquie anatolienne, aurait donné le nom « Asie ». La langue turque actuelle utilise toujours le vocable "Avrupa" pour désigner l'Europe.

De nos jours, les institutions européennes retiennent et propagent l'affirmation selon laquelle le nom du continent viendrait de la mythique Europe enlevée par Zeus.

L'usage fait de l'Europe un continent mais il s'agit, si l'on considère la plaque eurasiatique, de la partie occidentale d'un super-continent. Cela entraîne que les limites terrestres de l'Europe ont donc toujours été imprécises à l'est car il n'existe pas de relief ou de mer venant clairement scinder l'Eurasie. Les frontières géographiques de l'Europe sont donc plus politiques que physiques.

Pour les grecs, l'Europe ne s'étendait pas . Jusqu'au règne du tsar Pierre le Grand (1682 – 1725), la limite orientale de l'Europe est fixée au fleuve Tanaïs (aujourd'hui appelé le Don). Pierre le Grand mène une politique de réorientation de l'Empire russe vers l'Europe, en fondant Saint-Pétersbourg capitale ouverte sur la mer Baltique et en chargeant Vassili Tatichtchev de déplacer vers l'est la frontière de l'Europe. Ce dernier choisit le massif de l'Oural et le fleuve Oural. Au sud-est, la mer Caspienne, le massif du Caucase, la mer Noire et le détroit du Bosphore séparent l'Europe du Proche-Orient. Au sud et au sud-ouest, la Méditerranée et le détroit de Gibraltar séparent l'Europe de l'Afrique. Le continent est bordé à l'ouest par l'océan Atlantique et au nord par l'Arctique. Sont considérées comme européennes l'Islande (située géologiquement sur la séparation Eurasie-Amérique) et les principales îles de la Méditerranée ; le cas de Chypre est toutefois particulièrement sujet à débat, à la fois sur les plans géographique, culturel, politique et historique.

Les cas de la Russie, de la Géorgie et de la Turquie sont emblématiques du hiatus politico-géographique. Ces nations ayant la plus grande partie de leur territoire en Asie (Russie) et au Moyen-Orient (Turquie), le plan politique ne recoupe pas le « plan » géographique premier. Ainsi, si la Russie est occidentale par sa culture, son histoire et une part de son territoire, son centre de gravité fait d'elle un quasi-continent, s'étendant du Pacifique jusque dans l'Europe. Ensuite la Géorgie conserve un territoire de part et d'autre du Caucase qui atteint la mer Noire. Le cas est plus complexe pour la Turquie, celle-ci possédant la majeure partie de son territoire au Moyen-Orient, et possédant par l'histoire une culture mixte entre la culture occidentale et moyen-orientale.

Le Groenland, qui appartient au Danemark est rattaché à l'Europe. 

Certains territoires, les régions ultrapériphériques, font partie de l'Union européenne quoique étant situés en dehors du continent (la communauté autonome espagnole des îles Canaries, les cinq départements et régions d'outre-mer français, la collectivité d'outre-mer française de Saint-Martin et les deux régions autonomes portugaises de Madère et des Açores).

L'Europe a une superficie d'un peu plus de de kilomètres carrés (). Cela représente un tiers de l'Afrique, un quart de l'Asie et de l'Amérique.

On peut distinguer cinq grandes régions géographiques : l'Europe de l'Ouest, l'Europe centrale, l'Europe du Sud, l'Europe de l'Est et l'Europe du Nord.

L'organisation de l'espace montre un cœur économique, la « banane bleue » ou mégalopole européenne, qui comprend notamment l'Europe rhénane ainsi que les périphéries européennes.

Les frontières orientales de l'Europe sont avant tout politiques : la limite de l'Oural est due aux cartographes du tsar Pierre le Grand au . De même, la frontière fut déplacée des hautes crêtes du Caucase vers la mer Caspienne au début du pour justifier l'annexion de la Géorgie et de l'Arménie dans l'Empire russe. D'un point de vue géologique, si l'on se réfère à la tectonique des plaques, l'Europe et la partie continentale de l'Asie ne sont qu'un seul et même continent, dénommé Eurasie. Aussi, quelques géographes éminents, tels qu'Alexander von Humboldt, considéraient-ils l'Europe comme une simple péninsule de l'Asie.

Le climat européen est conditionné notamment par son étalement en latitude du au parallèle nord, soit plus de entre les espaces scandinave et méditerranéen. De ce fait, le contraste de température est considérable entre l'extrême nord, moyenne annuelle environ comme dans l'archipel de Nouvelle-Zemble, et l'extrême sud, moyenne annuelle environ pour la Crète.

L'Europe dispose d'une vaste zone côtière, et l'influence océanique atlantique et méditerranéenne contribuent à modérer les températures sur une bonne partie de l'Europe. Elle est située à l'est et au sud de l'Atlantique nord-est dont la température est notablement attiédie par la dérive nord-atlantique. Du fait de sa latitude, la majeure partie du continent est soumise au flux d'ouest dont la température a été auparavant adoucie par son passage sur cette partie de l'océan. Ce flux d'ouest n'est pas contrarié dans sa progression vers l'est en raison des grandes plaines largement ouvertes vers l'ouest dans la partie moyenne de l'Europe.

En toutes saisons, ce flux est tempéré et porteur de perturbations assurant des pluies régulières. Au fur et à mesure de sa progression à l'intérieur des terres, ce flux subit les influences continentales : il devient moins tempéré et s'assèche progressivement, les précipitations devenant moins régulières. Vers l'est, les hautes pressions hivernales prennent de l'importance, font barrage au flux océanique et sont la source d'épisodes très froids et secs. Au nord, les montagnes scandinaves font obstacle aux vents d'ouest et entrainent un climat continental froid sur la partie orientale de la Scandinavie. Le flux océanique voit également son importance climatique diminuer au sud de l'Europe, à cause de la latitude, des hautes pressions estivales, et des barrières montagneuses conséquentes qui s'interposent la plupart du temps en direction de la Méditerranée.

Tous ces facteurs expliquent la répartition des climats européens.

La bordure de l'océan Arctique connait un climat polaire sans véritable été (température de juillet inférieure à , ET dans la classification de Köppen) avec des précipitations faibles. L'hiver est froid ou très froid avec une température moyenne de janvier qui s'abaisse à vers l'est, il est assez perturbé du fait du voisinage de la mer.

Les littoraux du Nord-Ouest, la bordure côtière de la Norvège, les îles au nord de l'archipel britannique, l'Islande connaissent un climat océanique frais avec une température moyenne dépassant pendant moins de quatre mois (Cfc dans la classification de Köppen). Les précipitations sont abondantes, généralement plus de par an et souvent beaucoup plus dès qu'il y a des reliefs un peu importants. Les pluies sont réparties en toutes saisons avec un maximum d'automne ou d'hiver. Les tempêtes d'automne et d'hiver sont très fréquentes. Bien qu'agité, l'hiver reste « tempéré » par rapport à la latitude, entre et pour le mois le plus froid. L'été est frais et la température moyenne de juillet est comprise entre et .

Sur le domaine littoral plus bas en latitude, depuis les Îles britanniques jusqu'au nord-ouest de l'Espagne, en passant par la bordure côtière des Pays-Bas, de la Belgique, de la France s'étend un climat océanique bien caractérisé, avec une faible amplitude entre l'hiver et l'été et une température moyenne qui augmente du nord vers le sud mais assez homogène par rapport à l'étalement en latitude. Dans cette zone, le flux océanique modère les températures, les pluies sont fréquentes et régulières en toutes saisons avec cependant un maximum d'automne au nord et d'hiver au sud. Le total des précipitations annuelles, plus modéré que dans le type précédent, est compris entre et sauf sur les massifs côtiers (Écosse, Pays de Galles, Cordillère Cantabrique) où ce total peut largement dépasser . Les tempêtes automnales et hivernales sont fréquentes mais un peu moins que dans la zone précédente. En hiver, par rapport à la latitude, le gel et la neige sont relativement rares ainsi que les fortes chaleurs en été. Les étés sont tempérés avec une température moyenne qui dépasse pendant plus de quatre mois, Cfb dans la classification de Köppen. Pour le mois le plus chaud la température est comprise entre et du nord au sud, celle du mois le plus froid de à du nord-est au sud-ouest.

À l'est de cette zone, le climat, encore modéré par l'influence de l'océan, connaît une altération de ses caractéristiques quand on s'éloigne du littoral. La limite avec le domaine précédent est assez floue, cependant on peut considérer qu'à partir de quelques dizaines de kilomètres du littoral, dans la vaste zone de plaines ou de moyennes montagnes qui va du Bassin parisien au sud de la Scandinavie, à l'ouest de la Pologne et limitée par les contreforts des Alpes suisses et autrichiennes au sud, le climat est assez homogène sur une grande étendue. Il se continentalise peu à peu tout en conservant des caractéristiques modérées par rapport à la latitude (comme précédemment Cfb selon Köppen), les pluies deviennent un peu moins régulières, leur volume diminue progressivement, entre en plaine, sur les reliefs. Les pluies sont réparties très uniformément tout au long de l'année avec un maximum pluviométrique qui tend à devenir plutôt estival. Les tempêtes automnales et hivernales voient leur importance diminuer au fur et à mesure que l'on s'éloigne de l'océan, mais ne sont pas exclues. L'amplitude entre l'hiver et l'été ainsi que la fréquence des épisodes de température extrêmes augmentent progressivement mais les moyennes restent modérées par rapport à la latitude. La température du mois le plus chaud est comprise entre et du nord au sud, celle du mois le plus froid de à de l'ouest vers l'est. En France, cette zone correspond aux appellations traditionnelles de climat « parisien », « semi-océanique d'abri ».

Un peu plus au sud, du Bassin aquitain jusqu'à une partie des Balkans hormis la plaine du Pô, le climat est encore océanique ou semi-océanique (Cfb dans la classification de Köppen), mais se distingue par ses températures d'été plus élevées (moyenne de juillet de à ) et par une multiplication des climats locaux du fait du relief beaucoup plus compartimenté. Les précipitations peuvent être importantes à proximité des reliefs exposés aux flux humides ou bien réduites dans les bassins abrités. Les étés sont plus orageux que dans le type précédent avec des précipitations plus irrégulières. Mais la chaleur moyenne de juillet reste en dessous de et l'été connaît encore des périodes de rafraîchissement épisodiques, ce qui est un trait des climats océaniques. Les hivers restent doux à proximité de l'océan mais nettement plus froids vers l'Europe centrale. La température du mois le plus froid (janvier le plus souvent) est comprise entre et de l'ouest vers l'est. En France, cette zone correspond aux appellations traditionnelles de climat « aquitain », « semi-océanique d'abri ».

À l'est des deux domaines précédents, à partir de la Pologne orientale, la façade orientale de la chaine scandinave et les confins de l'océan Arctique au nord jusqu'à l'Oural vers l'est, jusqu'à la mer Noire, le Caucase et la Caspienne au sud apparaît le climat continental. L'hiver est froid avec blocage fréquent du flux océanique par l'anticyclone continental générateur d'épisodes très froids et secs. La moyenne de janvier va de de l'ouest à vers le nord-est. L'été, l'anticyclone continental disparaît et le flux atlantique pénètre plus librement à l'intérieur du continent, l'été est encore frais au nord mais il est de plus en plus chaud vers le sud, en juillet à la frontière du domaine polaire, jusqu'à près de la mer Caspienne (au nord, où les étés sont frais, nous sommes dans le domaine Dfc de Köppen, Dfb plus au sud, là où la moyenne dépasse durant au moins quatre mois). Les saisons intermédiaires sont courtes. Les pluies sont plus irrégulières avec un maximum de printemps ou d'été. Au nord du domaine, les étés sont assez pluvieux et restent frais avec une évaporation modérée, la sécheresse d'été est modérée. Vers le sud, la chaleur augmente ainsi que l'irrégularité des pluies, la sécheresse relative d'été s'intensifie et les abords de la Caspienne connaissent un climat steppique (BSk selon Köppen).

Les montagnes (Alpes, Pyrénées, Carpates, chaines balkaniques, Caucase, Alpes scandinaves) connaissent le climat montagnard qui correspondent à peu près à celui des plaines environnantes mais modifiés par l'altitude. Celle-ci provoque un abaissement de la température, en toutes saisons mais davantage en été qu'en hiver et une augmentation des pluies pour les versants exposés aux vents pluvieux. Les reliefs multiplient les climats locaux du fait des différences d'expositions au soleil et du fait de la modification du régime des vents qu'ils induisent.

Dans la plaine du Pô et dans les Balkans bordant la mer Noire, les chaines de montagnes font barrage au flux océanique, la chaleur estivale s'accentue avec une température moyenne de juillet supérieure à , les précipitations deviennent plus importantes en été. Selon la classification de Köppen, ce climat est appelé tempéré à étés chauds (Cfa). Les hivers sont assez variables, de assez doux comme sur les côtes occidentales de l'Adriatique, à assez froid (Bulgarie, Roumanie), mais toujours avec une température moyenne de janvier supérieure à . La température du mois le plus froid est comprise entre et de l'ouest vers l'est. Les influences océaniques concernent peu cette zone. Le cumul annuel des précipitations s'assèche progressivement vers l'est. Les pluies, encore réparties sur toute l'année, prennent cependant une importance estivale marquée, notamment sous forme d'orages. 

Les régions bordant la Méditerranée (majeure partie de l'Espagne, Sud-Est de la France, Italie hors les Alpes et la plaine du Pô, la Croatie, la Slovénie, l'Albanie, la Grèce et les îles méditerranéennes) connaissent un climat méditerranéen, Csa et Csb d'après Köppen. À l'écart du flux océanique humide du fait des montagnes et de la latitude, ce climat est caractérisé par une sécheresse estivale et un ensoleillement nettement plus importants que dans les domaines précédents. Les pluies ne sont pas souvent apportées par le flux atlantique mais la plupart du temps par des perturbations qui se développent sur place, alimentées par l'air méditerranéen, ces perturbations sont moins nombreuses que les perturbations océaniques mais les pluies qu'elles apportent sont copieuses et parfois excessives. Le total pluviométrique annuel des régions méditerranéennes est à peu près le même que pour les domaines précédents mais la répartition des précipitations est beaucoup plus irrégulière. L'été est à peu près sec surtout près des cotes et dans le sud, les pluies de printemps et d'automne sont prédominantes au nord du domaine méditerranéen et celles d'hiver au sud. Suivant les effets d'abris ou au contraire suivant les effets de couloir induits par les reliefs environnants, ce domaine est calme ou au contraire très venté (mistral, tramontane, bora, etc.). Les températures hivernales sont douces sauf en moyenne montagne, en janvier, de l'intérieur vers la côte et du nord vers le sud. L'été est chaud à en juillet du nord vers le sud. Ce type de climat est généralement limité par les versants sud ou est des massifs montagneux: chaîne Cantabrique, Pyrénées, Alpes et Balkans. Sur le littoral atlantique, la limite se trouve à peu près au nord du Portugal. C'est à partir de cette zone que l'on observe des caractéristiques méditerranéennes marquées (chaleur et sécheresse d'été entraînant des feux de forêt réguliers, un ensoleillement élevé comparé aux régions océaniques…).

L'Europe est assez bien arrosée par des fleuves et rivières, et pratiquement aucune zone n'est en stress hydrique.

Trois fleuves d'Europe, le Rhin, le Rhône, et le Pô, prennent leur source dans les Alpes, quelquefois appelées pour cette raison le « château d'eau de l'Europe » (au moins de sa partie occidentale). Le Rhin se jette dans la mer du Nord, le Rhône dans la mer Méditerranée et le Pô dans la mer Adriatique. Le Danube prend sa source dans la Forêt-Noire et se jette dans la mer Noire. L'Elbe se jette dans la mer du Nord. La Vistule et l'Oder se jettent dans la mer Baltique. Le Dniepr, fleuve de plaine, se jette dans la mer Noire. La Volga et l'Oural se jettent dans la mer Caspienne.

L'Europe regroupe plusieurs zones biogéographiques et une grande variété d'écosystèmes terrestres et marins, qui ont souvent été intensivement exploités, fragmentés et pollués.

L'Europe a été motrice pour de nombreux états-membres en matière de Droit de l'environnement avec notamment les directives Habitats et Oiseaux, bien que certains états membres (dont la France) les aient tardivement et incomplètement appliquées. Une directive-cadre sur l'eau est en cours d'application, des directives sur le sol et la mer sont en projet, et le est entrée en vigueur la nouvelle norme européenne pour limiter la pollution atmosphérique : les agglomérations de plus de de l'Union européenne ne doivent pas dépasser certaines valeurs limites : 50 microgrammes () de particules par mètre cube d'air ambiant doit être le seuil maximum pour 35 jours par an, et la valeur moyenne annuelle ne doit pas aller au-delà de 40 microgrammes. Cependant, les normes anti-pollution déjà en vigueur n'étaient déjà pas respectées : en 2002, 11 pays sur 15 ont dépassé la marge autorisée.

Depuis 1996, le conseil de l'Europe invite les états à construire ensemble un Réseau écologique paneuropéen et ils doivent appliquer, comme toutes les collectivités la directive 2003/4 concernant l'accès du public à l'information en matière d'environnement, la directive INSPIRE (Infrastructure d’information spatiale en Europe). Un futur Réseau européen de données d'observation et de surveillance (EMODNET / "European Monitoring Observation. Data Network") est en construction.

Pour mesurer l'état de l'environnement, les pressions et les réponses, l'UE s'est dotée d'une Agence européenne pour l’Environnement (AEE) qui applique maintenant la méthodologie LEAC (Land and Ecosystem Accounting - Comptabilité des écosystèmes et du territoire). Le système Corine Landcover et d'autres permettent d'harmoniser les cartes européennes de données environnementales.

Bilan : malgré des efforts importants, comme dans la plupart des autres régions du monde, la biodiversité qui y fait l'objet d'évaluations périodiques, est globalement en recul (sauf pour certaines les espèces plutôt généralistes et banales). Les espèces invasives continuent à gagner du terrain. À ce titre, la commission européenne a publié le , une liste des trente-sept espèces à combattre pour éviter qu'elles ne portent préjudice aux espèces indigènes. Cette liste prévoit d’interdire l’importation, la vente, la reproduction, la culture ou l’élevage de ces animaux et végétaux qui menacent la biodiversité.

Les objectifs européens en matière de lutte contre le changement climatique, et limitation des émissions de gaz à effet de serre, dont celui de -25 % pour 2020 semble difficile à tenir (pour les transports et l'agriculture notamment), la Pologne s'y opposant même avant que le 21 juin 2011, les ministres de l'environnement européens (en Conseil environnement) examinent un nouveau projet de feuille de route pour 2050 (économie européenne bas carbone) présentée par la Commission européenne le 8 mars 2011, confirmant l'objectif du Conseil d'octobre 2009 de réduire les émissions de gaz à effet de serre de 80 à 95 % en 2050 (par rapport à 1990), avec un calendrier de -40 % par rapport à 1990 en 2030, -60 % en 2040 et -80 % en 2050 (un pays s'est encore opposé à ces objectifs).

Le peuplement de l'Europe est conditionné par les cycles glaciaires et interglaciaires qui se succèdent, notamment au Pléistocène moyen (), et qui affectent la démographie des populations, créant notamment des périodes d'isolement géographique qui sont une des raisons de la différenciation des formes anciennes du genre "Homo" sur le continent. "Homo" naît et évolue d'abord en Afrique, où il s'affranchit d'abord du milieu forestier, puis connait une expansion constante vers les latitudes moyennes d'Eurasie, puis les hautes latitudes. Cela est rendu possible par sa capacité à s'adapter aux changements d'environnement et par ses caractéristiques de prédateur, lesquelles culminent chez Néandertal l'Européen, devenu principalement un chasseur carnivore occupant le haut de la chaîne alimentaire, un superprédateur chasseur de gros gibier. Arrive ensuite "Sapiens", venu d'Afrique, qui remplace l'espèce d'origine européenne qu'est Néandertal, et « toutes les autres humanités ».

Le genre "Homo" apparaît en Afrique, probablement vers dans la basse vallée de l’Omo, en Éthiopie, et il est attesté de manière certaine vers . Les premiers "Homo erectus" quittent l'Afrique et atteignent l'Eurasie il y a sans doute 1,8 ou 2 millions d'années, mais les dates et les chemins empruntés ainsi que certaines différenciations en espèces ("H. erectus", "H. ergaster", "H. antecessor", "H. heidelbergensis") sont encore discutés.

"Homo georgicus", parfois considéré comme un "Homo ergaster" européen, dont les restes ont été découverts en 2002 à Dmanissi, en Géorgie (Caucase), est le premier représentant du genre "Homo" attesté en Europe (et aussi l'un des plus anciens hors d'Afrique) ; il est daté d'environ . D'autres lui succèdent ; on a trouvé, à Kozarnika (Bulgarie), une industrie lithique, datant de et, en actuelle Espagne, des restes humains à Sima del Elefante (appartenant au site d'Atapuerca), datant de et à Orce, les restes de l'Homme d'Orce et de l'Enfant d'Orce, datant de . Le site d'Atapuerca a livré aussi des restes d'industrie lithique d'environ , et des restes humains ayant abouti à la description d"Homo antecessor", daté d'env. , possible ancêtre de "", attestant d'un peuplement continu de l'Europe occidentale depuis ainsi que de l'existence possible d'une migration à partir de l'Europe centrale, depuis Dmanissi où a été découvert "H. georgicus", et non pas via le détroit de Gibraltar. Jusqu'à "H. antecessor", l'industrie lithique associée à ces peuplements est l'Oldowayen, technique des galets aménagés. Vient ensuite "Homo heidelbergensis" (l"Homo erectus" européen, , espèce à laquelle appartient, par exemple, l'homme de Tautavel), décrit à partir de plusieurs fossiles, retrouvés en Allemagne actuelle, à Heidelberg, d'où provient son holotype et en Espagne, sur le site d'Atapuerca ; "H. heidelbergensis" pourrait être l'ancêtre d"H. neanderthalensis". L'industrie lithique associée aux heidelbergiens est l'Acheuléen, caractérisé par la technique des bifaces, attesté il y a en Europe mais né en Afrique il y a . À la même époque, vers , les Européens commencent à maîtriser le feu, étape importante de l'évolution, qui permet la cuisson des aliments et donc facilite l'assimilation des nutriments, et qui permet aussi de se chauffer, dans un environnement climatique globalement plus froid qu'actuellement, ce qui concourt à un processus de socialisation.

"Homo neanderthalensis", l'« Homme de Néandertal », naît il y a environ en Europe occidentale, issu sans doute d'une forme de spéciation (« spéciation par distance ») d"", ou d'une dérive génétique (modèle d'accrétion), dans un contexte où l'Europe est isolée par les glaces du reste de l'ancien monde. Neandertal est très adaptable, il s'accommode des périodes glaciaires et inter-glaciaires et des environnements correspondants et s'étend massivement en Europe et au-delà, vers l'Asie Centrale et le Proche-Orient entre . Physiquement robuste et adapté au froid, Néandertal possède des capacités cognitives proches de celles de l'« Homme moderne » ("Homo sapiens"), il pratique des rituels, il enterre ses morts, le premier en Europe à le faire, et il pratique une forme d'art. Il est associé au Moustérien. Le nombre de néandertaliens (métapopulation) est compris dans une fourchette allant de quelques milliers à , donnant en tout état de cause une très faible densité de population.

"Homo sapiens", quant à lui, naît en Afrique, il y a . Sa présence hors d'Afrique, sous une forme archaïque, est attestée par des fossiles âgés de environ, en Israël (Grotte de Misliya). L'expansion de Sapiens se fait en plusieurs vagues, lesquelles empruntent probablement un chemin passant par Bab-el-Mandeb, détroit entre la péninsule arabique et l'Afrique, outre celui par le Nil et le Proche-Orient, mais les premières n'atteignent pas l'Europe. Les vagues de Sapiens évolués qui atteignent l'Europe partent d'Afrique vers et leurs plus anciennes traces en Europe datent de (Grotta del Cavallo). À partir de , "H. sapiens" commence sa colonisation de l'Europe, dans un mouvement d'est en ouest. À ce moment, il a déjà eu l'occasion de se métisser avec Néandertal, leurs chemins s'étant croisés au Proche-Orient et à l'est de l'Eurasie. Ce métissage entre Néandertal et Sapiens est sans doute favorable à ce dernier ; venu beaucoup plus récemment d'Afrique, il est plus adapté aux basses latitudes et il acquiert par ce métissage des avantages évolutifs, notamment de résistance au froid. 
Sapiens progresse en Europe et, concomitamment, les néandertaliens régressent, se retrouvant confinés dans des zones refuges avant de disparaitre vers avec des populations relictuelles perdurant jusqu'à , non sans laisser leurs traces génétiques dans l'actuelle population humaine. Les causes de la régression puis de la disparition de Néandertal sont plurifactorielles et toujours discutées. La peau de ces hommes était sombre, adaptée à leur origine africaine et aux régions très ensoleillées. Ce n'est que récemment, il y a , que les chasseurs-cueilleurs européens ont disposé des gènes responsables de la peau pâle.

Avec l'arrivée de Sapiens, des industries diverses (Uluzzien, Bohunicien, Châtelperronien, attribué à Néandertal, Lincombien-Ranisien-Jerzmanowicien…) se développent ; deux d'entre elles, l'Aurignacien (env. ) puis le Gravettien (env. ) se répandent largement en Europe. Ces expansions sont concomitantes à des mouvements de populations, retracés par la génétique, eux-mêmes corrélés aux fluctuations climatiques de l'époque. L'Aurignacien est caractérisé notamment par le développement du travail des matières osseuses (bois de rennes et os de mammouths), rare jusqu'alors, à des techniques de débitage de lamelles ainsi qu'à des objets de parure et au développement de l'art ; la grotte Chauvet, occupée à l'Aurignacien () et au Gravettien (), en est un exemple. Dès , à une période particulièrement froide, on trouve des traces de sédentarisation partielle dans l'est de l'Europe, sous la forme de campements bénéficiant d'infrastructures d'habitation (à la différence des abris de plein-air), autour desquels ont été retrouvés des sépultures et des statuettes d'argile. Mais Sapiens reste néanmoins, fondamentalement, un chasseur-cueilleur mobile nomadisant sur des distances de quelques centaines de kilomètres.

Entre se produit un intense refroidissement, le « maximum glaciaire » qui donne à l'Europe une configuration nettement différente de l'actuelle. Les études génétiques montrent que certains groupes, d'abord représentés en Europe du Nord-Ouest, sont repoussés dans le sud de l'Europe. C'est le moment où apparait l'industrie lithique Solutréennne, caractérisée par des pointes en pierre très fines et acérées, appelées « feuilles de laurier », servant sans doute de couteaux et armant l'extrémité des flèches et des sagaies. Émergent l'usage du propulseur et (probablement) de l'arc pour la chasse mais ces deux outils, cependant, ne se généraliseront qu'au Magdalénien qui lui succède. On a longtemps pensé que le Solutréen était aussi l'époque de l'invention de l'aiguille à chas jusqu'à la découverte, en 2016 en Sibérie, d'un tel artefact, daté de , attribué à l'Homme de Denisova. Le Magdalénien est caractérisé par un art pariétal particulièrement riche, comme en témoignent les grottes de Lascaux et d'Altamira et par le travail des matières osseuses.

La dernière période glaciaire s'achève de manière brutale. Un premier réchauffement rapide se produit vers , la température du Groenland augmente de plus de , c'est ce qu'on nomme le Bölling, qui libère des glaces une grande partie de l'Europe du Nord et de la Scandinavie, permettant leur peuplement depuis le sud. Avec ce retrait des glaces, de nouveaux apports de populations à partir du Proche-Orient font sentir leurs effets. Plus tard, vers , un retour à des conditions glaciaires se traduit par des températures extrêmement froides avant un réchauffement final, vers , qui marque la fin de la dernière glaciation et l'entrée dans l'Holocène avec l'instauration du climat actuel ; cela coïncide avec les débuts de l'extinction de la mégafaune européenne (mammouth laineux, rhinocéros laineux, cerf géant, ours des cavernes…), sans doute pour des raisons climatiques probablement aggravées par la prédation humaine. Le paysage et sa faune se recomposent, la forêt tempérée progresse en Europe à partir de , la chasse à l'arc se généralise et l'alimentation des hommes du Mésolithique devient extrêmement diversifiée (les escargots par exemple, sont consommés en très grande quantité dans certaines niches écologiques).

La néolithisation de l'Europe commence vers par diffusion de populations et de techniques apparues vers dans le croissant fertile, elle s'accompagne d'une forte croissance démographique. Elle est probablement autant due à un changement culturel qu'aux conditions climatiques. Les indicateurs de la néolithisation sont la domestication des plantes et des animaux (celle du chien étant cependant largement antérieure), la tendance à la sédentarisation (la sédentarisation précède cependant l'agriculture) avec le regroupement en villages et l'émergence de la poterie pour des contenants destinés au stockage de produits agricoles. Ce sont les débuts des sociétés agropastorales et la naissance du mégalithisme.

Cette néolithisation, venue du croissant fertile via l'Anatolie, emprunte deux chemins ; d'abord un courant méditerranéen par lequel se diffuse la culture de la céramique imprimée suivie de la culture de la céramique cardiale () ; ensuite un courant danubien, par lequel se diffuse la culture rubanée (vers ). Les études génétiques montrent que, outre une diffusion culturelle, l'Europe connait l'arrivée de populations d'agriculteurs, venues d'un foyer anatolien, qui ont suivi ces chemins danubien et méditerranéen. Les îles britanniques, en configuration insulaire depuis , connaissent ce processus plus tardivement, près d'un millénaire après celui de l'Europe continentale. La néolithisation est largement effective en Europe vers .

Des hypothèses linguistiques et archéologiques de la deuxième moitié du (l'hypothèse kourgane étant la plus largement reconnue) et des études génétiques du début du accréditent la thèse que des populations, ayant domestiqué le cheval et maîtrisant l'équitation ainsi que le transport en chariot, seraient venues de l'est de l'Europe, la steppe pontique, et se seraient répandues sur le continent à partir de , le dominant largement ; elles contribuent pour 75 % à l'ADN des peuples de la céramique cordée, héritière de la culture de la céramique rubanée, largement présente en Europe à ce moment. Leur foyer d'origine serait la culture Yamna, caractérisée par sa pratique de l'inhumation dans des tumuli nommés « kourganes ». Ils seraient aussi les locuteurs du proto-indo-européen, langue-mère de la quasi totalité des langues européennes. Georges Dumézil, au début du , postule que les sociétés d'origine indo-européenne partagent jusqu'à nos jour un mode de pensée, l'idéologie tripartite.

L'agriculture européenne commence sur les bords de la mer Égée, aux environs de Elle s'installe progressivement sur le continent, dans la zone danubienne et l'actuelle Hongrie (), sur les côtes méditerranéennes et le territoire de la France actuelle vers , en Germanie et sur le territoire des actuels Pays-Bas vers ; elle atteint les îles britanniques vers 

Vers apparaît le travail du cuivre, qui conjugué à celui de la pierre, caractérise le Chalcolithique ; des outils agricoles métallique plus efficaces apparaissent, l'araire se développe à la place de la houe. La civilisation minoenne, inspiratrice de la culture grecque, apparaît vers avec sa langue écrite en linéaire A, une des plus anciennes forme d'écriture en Europe avec les hiéroglyphes crétois. À son apogée, elle sera la première civilisation avancée de l'âge du bronze. L'âge du bronze date de la fin de la culture campaniforme, laquelle, entre et , couvre une notable partie de l'Europe de l'Ouest.

Le commencement des Germains se situe vers le en Suède méridionale, au Danemark et en Allemagne du Nord entre la Weser et l’Oder. Ils s'établissent dans la grande plaine européenne, du Rhin à la Vistule et de la Baltique au Danube, entre le et le début de l'ère chrétienne. Leur expansion vers le sud est arrêtée par l' () puis par les Romains ().

On distingue trois groupes linguistiques : le nordique, celui des Scandinaves ; l’Ostique ou Germains orientaux, celui des Goths, des Vandales, des Burgondes… ; enfin les Westiques (Germains occidentaux), en Allemagne, au Jutland et aux Pays-Bas.

Les Celtes s'installent entre l’âge du bronze moyen (env. ) et le début de l’âge du fer (env. ) dans une grande partie de l’Europe, du bassin des Carpates à l’est de la France. Leur origine est le centre de l'Europe, où étaient apparues les cultures caractérisées par leurs coutumes funéraires de l'enterrement sous tumulus ( - ) puis par la technique consistant à incinérer les cadavres et à conserver leurs cendres dans des urnes (civilisation des champs d'urnes, - ). 

Le noyau celte se situe à Hallstatt, en actuelle Autriche. Aux débuts de l'âge du fer, leur société, relativement égalitaire, se stratifie avec, à son sommet, des chefs militaires. Cela est probablement en lien avec la métallurgie du fer et, notamment, la conception d'armes, telles les épées droites caractéristiques, et la confection de pièces de harnachement plus efficaces qui donnent de l'importance aux cavaliers armés. Les Celtes excellent en effet dans le travail du fer, fabriquant, outre des armes, des outils tels que haches et ciseaux. Ils confectionnent aussi des poteries, ils inventent la tonnellerie, et ils exploitent le sel gemme, dont le commerce est source de richesse. On leur doit aussi, avec un apogée aux , les habitats structurés autour d'un oppidum, centre fortifié à vocation militaire, économique et cultuel. La période laténienne ou deuxième âge du fer, commençant au , est celle où les Celtes passent de la protohistoire à l'histoire, lorsqu'ils apparaissent dans les textes des auteurs grecs.

L'Europe antique est, pour notable partie, une Europe celtique, celle des peuples héritiers de la culture des tumulus, et en partie germanique, aux côtés de la Grèce antique et de sa brillante civilisation de l'Époque classique (), considérée comme le berceau culturel de la civilisation occidentale.

Pour ce qui concerne le terme et le concept, le mot « Europe » désigne d'abord, dans son acception géographique, la Grèce continentale. Le terme est mentionné pour la première fois vers , par Hésiode, dans sa "Théogonie". Anaximandre et Hécatée de Milet produisent, entre 600 et , des cartes représentant un territoire appelé Europe. Le mot prend aussi un sens politique lorsque les Grecs sont confrontés aux invasions venant d'Asie, principalement lors des guerres avec l'empire perse. Selon Jacqueline de Romilly, . L'Europe en tant qu'entité géographique se retrouve chez Ératosthène au , lequel présente une tripartition du monde connu par une carte où elle figure. Mais la distinction fondamentale durant l'antiquité est celle entre les Barbares, qui habitent ce qu'en latin on nomme "barbaricum" (« pays des Barbares »), et ceux qui appartiennent à l'aire culturelle grecque, puis gréco-romaine. Le royaume de Macédoine désigne l'Europe comme une entité politique : lorsque Philippe II part en Orient, en , il laisse en Macédoine un régent, Antipatros, qui porte le titre de « stratège d'Europe ».

Après l'époque hellénistique, l'Europe voit Rome commencer son expansion au et atteindre son apogée au . L'Europe est reconfigurée, son histoire devient celle de l'Empire romain pour la zone concernée.

La Grèce et le royaume de Macédoine sont supplantés au Les Celtes, qui se sont largement répandus en Europe, allant jusqu'à menacer Rome en , pris en tenaille par les attaques des tribus germaniques venues du nord, sont repoussés ou assimilés. À l'aube de l'ère chrétienne, les Romains, lorsque leur zone d'expansion dépasse la « ceinture celtique », se retrouvent entourés par les Germains qui deviennent . Les frontières orientales de l"'Imperium", limites avec les peuples germaniques, sont le Rhin et le Danube, tandis que sa frontière septentrionale est le mur d'Hadrien, qui le sépare des Pictes celtes.

À cette époque, entre et , Strabon rédige une géographie qui mentionne l'Europe et, déjà, des descriptions non seulement géographiques, mais aussi économiques et culturelles des territoires qu'il étudie. Au , Varron évoque une bipartition du monde au niveau du Bosphore, les parties situées au nord-ouest du détroit constituant l’Europe, celles situées au sud-est, l’Asie. Toujours au , Pline l'Ancien divise le monde en trois parties, l'Europe, l'Asie et l'Afrique.

Les Celtes présentent une certaine unité linguistique et culturelle, mais pas d'intégration politique ; ils bénéficient d'une organisation tribale, au plus en ligues de tribus, à l'instar des Germains. C'est donc l'Empire romain qui contribue à créer les prémices d'une unité européenne. Si la Grèce est le berceau culturel de l'Europe, Rome peut être considérée comme le berceau de sa civilisation
. L'influence romaine s'inscrit dans la culture, formant ce qu'on nomme la culture gréco-romaine via la langue latine, ainsi que dans les territoires et dans l'usage de l'espace via les voies romaines et l'urbanisation et esquisse même une Europe religieuse en diffusant le christianisme à partir du . Les sont ceux de la "Pax Romana", période de calme relatif, notamment politique, malgré des batailles toujours existantes sur les marches de l'Empire, notamment avec les peuples germaniques pour ce qui concerne la zone européenne. L'Empire est l'entité politique unificatrice définissant le mode de gestion politique ainsi que les limites (et les frontières, qui sont une forme particulière de limites), qui séparent le monde romain de celui des barbares. 

Le est une période de crise interne pour l'Empire romain qui subit aussi une pression croissante des peuples germaniques, invasions difficilement repoussées. L'empire intègre nombre de ces envahisseurs par des traités, faisant d'eux des fédérés qui fournissent des troupes à l'armée. Une Tétrarchie est mise en place en pour lutter contre les Barbares et, au , « Europe » désigne l’une des six provinces du diocèse de Thrace dont le territoire correspond à . 

Le christianisme, dont les adeptes sont par périodes persécutés, notamment au par Dioclétien, s'était répandu dans l'Empire, comme en témoigne l'épisode symbolique de la conversion de l'Empereur Constantin et l'édit de tolérance religieuse de Milan en En , il est déclaré religion officielle de l'Empire par Théodose et les autres cultes sont interdits. Même si, à ce moment, les chrétiens sont nettement minoritaires dans la population, cette christianisation officialisée aura une importance, donnant, au moment des royaumes barbares, une légitimité religieuse à un pouvoir royal qui en était, à l'origine, dépourvu.

En , l'Empire est définitivement scindé en deux, l'Empire romain d'Orient et l'Empire romain d'Occident. Le premier perdura : Inversement, dès le , l'Empire d'Occident se délite sous l'effet des attaques des peuples germains, appelées les invasions barbares : .

En Europe occidentale, la déposition du dernier empereur romain d'Occident en marque conventionnellement le passage de l'Antiquité au Moyen Âge.

 
En Europe occidentale la lente déliquescence de l’Empire romain d'Occident qui aboutit à la désunion et à l’émergence de nations parfois éphémères, au gré des invasions et conquêtes, ne fera jamais oublier l’héritage romain qui reste un modèle d’unité et de droit pour l’Europe, de l’Empire carolingien jusqu’à l’Empire napoléonien en passant par le Saint-Empire romain germanique. Les liens entre places commerciales européennes émergent.

Poursuivant la politique de conquête de ses prédécesseurs francs, Charlemagne étend son royaume. Sa politique d’expansion rejoint le désir de la papauté romaine d'asseoir la prépondérance de l’évêque de Rome par rapport aux patriarches orthodoxes et coptes. Le jour de Noël de l'an 800, Charlemagne est couronné "empereur des Romains" par le pape Léon III, à Rome, en la basilique Saint-Pierre. Cette union entre pouvoir temporel et religieux vise à réunir l’Europe en un empire chrétien d’Occident. De son vivant, Charlemagne se fait appeler "Pater Europae" (« père de l'Europe »), et parfois "Europa vel regnum Caroli" (l’Europe, ou le royaume de Charles).

L’Europe occidentale de Charlemagne est franco-germanique et chrétienne de rite latin, alors que l’Europe orientale sous l’influence de Constantinople est à dominante slave et de rite grec, mais les deux tendent à christianiser l’Europe du Nord, britannique, scandinave et russe. Alors qu’à Constantinople se concentrent les deux pouvoirs religieux et politique, en Occident le rôle de Rome y est essentiellement religieux, la capitale de Charlemagne se trouvant à Aix-la-Chapelle. Charlemagne tente une réunification avec l’Empire romain d'Orient vers l’an 800 mais il échoue, et son empire se désagrège rapidement après sa mort. En 962, Otton crée le Saint-Empire romain germanique, mais celui-ci ne peut s’étendre, contrecarré par la permanence de royaumes anciennement constitués, la France et l’Angleterre surtout, par ses luttes avec la papauté, puis par le développement de l’Empire ottoman lors de l’époque moderne.

L’Empire romain d'Orient (dit, depuis le , « byzantin ») est chrétien et de culture essentiellement grecque : il connaît d’importantes fluctuations de sa force et par conséquent de son territoire, qui s’étend à son apogée sur une grande partie du rivage méditerranéen, d’abord sous Justinien, puis sous les empereurs macédoniens, du au . Au cours des siècles, ses relations avec l’Occident se distendent puis se détériorent, alors que les musulmans montent en puissance à l’Est et s’emparent de la moitié de l’Anatolie au . Le schisme religieux de 1054 et l’agression militaire venue de l’Ouest en 1202 affaiblissent l’Empire d’Orient qui finit dépecé morceau par morceau par l’Empire ottoman avant de disparaître lors de la chute de Constantinople en 1453.

L'axe européen Bruges/Venise est déplacé à la fin du Moyen Âge. À l'époque où l'Empire d'Orient s'effondre, la Reconquista espagnole touche à sa fin. L'année 1492 est celle de l'Espagne, avec la reconquête du dernier royaume maure (Grenade) en péninsule ibérique et le premier voyage de Christophe Colomb, sous l'égide des "Rois catholiques" qui va ouvrir la voie à l'établissement des hégémonies européennes.

Le rêve d'un grand empire européen renaît au lors de l'affrontement entre François et Charles Quint, qui tous deux se disputent le trône du Saint-Empire. Grâce à l'appui des banquiers Fugger, Charles Quint l'emporte, se retrouvant à la tête d'un domaine très vaste, mais aussi très morcelé. Les diverses guerres menées contre la France ne donnent aucun résultat : durant deux siècles, le découpage de l'Europe va évoluer au gré des alliances matrimoniales et des guerres entre États. C'est face à la montée en puissance de l'Empire ottoman qu'une union des États chrétiens d'Europe apparaît : « Nous tenons de Gadès à l’Isler, une zone qui s’étend entre les deux mers et qui est la très courageuse et la très puissante Europe. Là, si nous nous unissions, nous ne serions pas seulement égaux à la Turquie, mais supérieurs à toute l’Asie » (Jean Louis Vivès).

Mais ce ciment du christianisme catholique, qui donnait un semblant d'union à cette Europe occidentale, éclate en morceaux avec la Réforme (ou plutôt les Réformes), dont l'impact politique est considérable, permettant néanmoins la formation des Provinces-Unies et de la Confédération suisse. Les guerres de religion, la guerre de Trente Ans, les guerres de Louis XIV rythment les et s. Les traités de Westphalie (1648) et celui du traité des Pyrénées en 1659, redessinent durablement la carte politique de l'Europe et l'équilibre des forces en présence.

L'Époque moderne est marquée par un renforcement des nationalismes en tous genres. C'est aussi l'époque où l'Europe s'étend très loin de ses frontières par la constitution des premiers empires coloniaux sur le continent américain, puis en Inde.

La Révolution française inaugure un bouleversement politique très important : les idées démocratiques apparaissent sur le devant de la scène et les campagnes de Napoléon puis le Congrès de Vienne vont remodeler profondément la carte de l'Europe et les mentalités. Honoré de Balzac a cette déclaration optimiste dans "Le Bal de Sceaux", (1830) : « Le seizième siècle n'a donné que la liberté religieuse à l'Europe, et le dix-neuvième lui donnera la liberté politique. »

À la fin d'un long processus, le voit se réaliser l'unité de l'Italie (de 1861 à 1870) et de l'Allemagne (en 1871), ainsi que la constitution de plusieurs nouveaux pays dans les Balkans, issus du démembrement de l'Empire ottoman, appelé alors « l'homme malade de l'Europe ».

C'est aussi l'apparition de nouveaux mouvements politiques prônant plus d'égalité (socialismes), voire le démantèlement du pouvoir des États (anarchismes). Ces idées se diffuseront par la suite, et avec plus ou moins de retard, largement hors des frontières de l'Europe.

La domination politique et économique de l'Europe sur le reste du monde s'est affirmée après qu'elle a bouleversé son économie lors des révolutions industrielles, développant sa productivité et amorçant une forte explosion démographique. Leur avance technologique, et notamment militaire, permit aux pays européens, en concurrence les uns contre les autres, d'étendre leur emprise sur les autres continents. Cette colonisation connut son apogée au début du (cet apogée s'achève en 1914), avant que les deux guerres mondiales ne bouleversent l'ordre établi.

La Première Guerre mondiale et ses conséquences favorisent l'émergence de plusieurs régimes totalitaires dont ceux, génocidaires, d'Adolf Hitler et de Joseph Staline. L'instabilité politique et économique débouche sur la Seconde Guerre mondiale et la domination nazie qui laissent l'Europe exsangue. Alors que la suprématie des pays européens occidentaux disparaît au profit de deux nouvelles superpuissances (les États-Unis et l'Union soviétique), des mouvements de libération se développent dans les colonies, aboutissant à l'indépendance de nombreux pays, notamment au cours du troisième quart du .

Parallèlement, alors que l'excédent démographique de l'Europe était tel qu'elle constituait un réservoir d'émigration massive tout au long du et au début du , les pays du continent furent confrontés à une stabilisation à partir de la Première Guerre mondiale, et parfois à une régression démographique ensuite (les guerres, génocides et famines y contribuant). Après la Seconde Guerre mondiale, l'Europe occidentale connaît un « baby boom » et un développement continu de l'économie, dont principalement l'industrie de production et de transformation, qui provoqua un appel de main d'œuvre transformant cette moitié de l'Europe en une terre d'immigration, notamment au cours des "Trente Glorieuses". Au même moment, la construction de l'Union européenne crée un marché commun entre États européens et une grande stabilité sur le continent.

L’Europe est au début du , quand on considère sa densité de population, le troisième foyer de peuplement derrière la Chine et l'Inde, avec des densités de populations parmi les plus élevées au monde dans certaines zones des Pays-Bas, de la Belgique, du Royaume-Uni, de l’Allemagne ou de l'Italie, d’autant que l’exode rural s’est renforcé ainsi que l’attractivité des littoraux avec des populations de plus en plus urbaines. En termes absolus, l'Europe et, "a fortiori", l'Union européenne, est cependant un « nain démographique ». Le continent (env. d'habitants ; UE env. d'habitants) se situe derrière l'Asie (env. d'habitants dont env. d'habitants pour la Chine et env. d'habitants pour l'Inde), l'Afrique (env. d'habitants) et l'Amérique (env. d'habitants) ; l'Eurasie, quant à elle, concentre environ d'habitants.

En 2005, le Conseil de l'Europe soulignait que depuis quelques décennies l’UE devait sa croissance démographique à l'immigration qui, dans les années 2000, est devenue le premier, puis le seul facteur d’augmentation de la population totale de l’UE. Ainsi deux millions de personnes sont venues s'installer en Europe en 2004 alors que l'accroissement naturel était négatif de . L'Allemagne est le pays le plus peuplé de l'UE. En 2007, de personnes, soit 16 % de la population de l'UE, résidaient dans des communes côtières.

Malgré les dizaines de millions de morts des deux guerres mondiales, l’Europe a connu une période d’explosion démographique aux , qui s’est accompagnée d’une forte pression sur l’environnement et les ressources non renouvelables (cf. empreinte écologique, empreinte énergétique, pression urbanistique, pollutions, etc.). Depuis quelques décennies, la population européenne tend à se stabiliser, à la suite d'une forte diminution de la natalité, qui reste toutefois encore largement compensée par la natalité de certains pays, par le recul de l’âge auquel les femmes font leurs premiers enfants, et surtout par une immigration régulière.

L'immigration est le premier moteur de la croissance dans une Union européenne à la population vieillissante. Le boom économique des années 1950-1960 avait poussé l'Europe à faire appel à une immigration massive, souvent issue de ses ex-colonies. Les Chinois, Indiens et Africains constituent l'un des principaux flux d'immigrants non originaires de l'UE. Après les Turcs, les Marocains forment le plus gros contingent.

Les études prospectives pour 2050 varient d’une population diminuant de 3 % (dans l'hypothèse d'un ISF remontant à 2,34), à -22 % voire -50 %. Les experts parlent alors de retournement démographique ou d'hiver démographique. Que la diminution soit due à la natalité est un phénomène inédit jusqu'à nos jours dans le monde. Ces chiffres ci-dessus doivent tous être utilisés avec prudence, la prospective démographique ayant toujours été prise en défaut et pouvant elle-même influer en retour sur les comportements individuels et collectifs et sur les politiques de soutien à la natalité ou à l’immigration. Pour d'autres, la population de l'Union européenne (UE) serait de de personnes en 2050 selon l'Organisation de coopération et de développement économiques (OCDE), et en 2060 selon Eurostat. La population de l'UE dépasserait ainsi celle des États-Unis ( de personnes en 2060 selon le Centre américain d'études sur l'immigration).

Toutefois, la situation démographique diffère pour chaque pays européen. Les pays de l'Europe de l'Est se sont inquiétés des évolutions démographiques dès les années 1960 et ont mis en place des politiques d'encouragement à la natalité. Cependant, les moyens utilisés, comme l'interdiction de l'avortement, n'auraient pu être acceptés au même moment en Europe de l'Ouest. Ces mesures n'ont d'ailleurs généralement pas produit d’effet satisfaisant ; et si la Pologne a maintenu sa population au cours de la période communiste, l'influence de l'Église catholique, qui imprègne la société polonaise, a sans doute été plus efficace que la politique nataliste.

Pour les pays d'Europe de l'Ouest, personne ne se risque, entre autres en Allemagne, à mettre sur la place publique l'évolution de la population sur la longue durée. Pour les responsables, tout passe par la politique d'immigration. Ils ne veulent pas toucher au tabou de la politique familiale en faveur de la fécondité, compte tenu du poids de la mauvaise conscience des années hitlériennes. La situation démographique empire en Europe pourtant : un rapport annuel sur la situation démographique des pays membres demandé autrefois par les autorités communautaires a été abandonné depuis 2000, désormais remplacé par un « Rapport social », où l'on communique à propos de chômage et de pauvreté sans jamais plus effleurer la dimension démographique. Autrement dit, l'UE s'interdit de voir la situation démographique de ses pays membres.

La croissance démographique s’est globalement poursuivie pour les membres de l'Union européenne, mais la population décroît dans certains pays, notamment en Europe de l'Est. Ce déclin démographique semble plus important et plus rapide dans les ex-pays de l’Est, dans quelques pays où la pauvreté et le renforcement des inégalités ont suivi l’effondrement du communisme, et aussi dans les régions touchées par la catastrophe nucléaire de Tchernobyl (la Biélorussie qui a reçu 70 % environ des retombées d'iode et de césium radioactifs et connaît depuis le plus fort taux d’avortement et le taux d’abandon d’enfants y est élevé).

Avec plus de d'habitants et sur une surface réduite pour une moyenne d'une langue pour d'habitants, l'Europe bénéficie d’une grande richesse ethnoculturelle et une pluralité de langues. Les cultures germaniques, slaves, latines et finno-ougrienne sont traduites par la diversité des langues parlées : et dialectes ont des racines indo-européennes ; latines et grecques au sud, germaniques au nord et au nord-ouest ; slaves à l'est et en Europe centrale, seul le groupe des langues finno-ougriennes (regroupant le finnois, estonien et le hongrois) et la langue basque ne font pas partie des langues indo-européennes.

Administrativement, l’allemand, l’anglais, le russe, le français, l'espagnol et l’italien dominent mais l’Europe est linguistiquement beaucoup plus riche puisque les 50 États européens (tous souverains, hormis Gibraltar) de la grande Europe géographique regroupent officielles, enrichies de secondaires non officielles. À tel point qu'Umberto Eco dit : « la langue de l'Europe, c'est la traduction ». Andreas Kaplan décrit l'Europe comme « offrant un maximum de diversité culturelle en un minimum de distance géographique ». Ces précédents chiffres peuvent paraître élevés, mais ils ne représentent que 3 % du total des langues vivantes encore parlées sur la planète.

En Europe de l’Ouest (France, Espagne, Royaume-Uni, Italie, etc.) les langues vernaculaires sont souvent régionales et minoritaires, parfois au bord de l’extinction, mais certaines (breton, alsacien, basque, corse, catalan, occitan, flamand, le dernier étant un dialecte du néerlandais), , et enseignées en France, plutôt à l’université, mais parfois dès l’enfance : école Diwan en Bretagne. En Espagne, c’est le cas du basque, du catalan et du galicien. Pour le Royaume-Uni, c’est le gallois, le gaélique écossais, le scots et l’irlandais. Le français est reconnu en Italie dans le Val d’Aoste, comme le sont le féroïen aux îles Féroé, ou le frison occidental aux Pays-Bas, etc.

Une langue unique n’est officiellement parlée : l’Islande (où l’on parle islandais), Malte (où la seule langue officielle est le maltais), le Liechtenstein (où l'on parle allemand), et la République de Saint-Marin (où l'on parle italien). L'État de la Cité du Vatican (plus petit État européen) est un cas à part : l’italien y est la langue véhiculaire, le latin (réputé langue morte) y est la langue juridique, le français y est la langue diplomatique (le Vatican se fait enregistrer comme État francophone auprès des organisations internationales), et l'allemand est la langue en usage dans l'armée (la Garde suisse). - Les autres États comptent tous plusieurs langues vernaculaires, tant dialectes que langues à part (plus ou moins reconnues et souvent non enseignées) et jusqu’à plus de l'Allemagne (), l'Azerbaïdjan (13), la Bulgarie (11), l'Espagne (14), la France (25), l'Italie (33), la Roumanie (14), le Royaume-Uni (12). La vaste Russie regroupe à elle seule à statut officiel sur son territoire.

Certaines langues régionales, sans statut officiel (quoique doublant parfois les noms de communes ou de rues) persistent et sont parfois protégées et enseignées, souvent avec le soutien de collectivités locales ou régionales (breton, corse, occitan en France, sarde en Italie, lapon en Scandinavie).

Les systèmes d'écriture en Europe reposent sur l'alphabet latin (sous diverses variantes), l'alphabet grec, l'alphabet cyrillique (sous diverses variantes).

Aux langues originaires des pays d’accueil s’ajoutent les langues maternelles des populations circulantes (Roms), migrantes ou réfugiées, et tout particulièrement l'arabe, le berbère, le turc, l'hindi, etc.

L'Europe a été confrontée au cours de son histoire aux besoins de langues véhiculaires. Ainsi la lingua franca, langue composite (mélange d'arabe, de français, portugais, espagnol, italien ou occitan, le tout variant dans le temps et l'espace), a été utilisée du Moyen Âge jusqu'au par les marins et dans les ports de la Méditerranée. De nombreux projets de langues construites sont apparus en Europe, avec notamment la création de l'espéranto en 1887, seule langue construite devenue langue vivante.

Sur une population totale d'environ d'habitants en 2010, l'Europe compte environ de catholiques (35 %), d'orthodoxes (27 %), de protestants (14 %) et de musulmans (6 %). Les personnes n'ayant pas de religion ou pratiquant une autre religion sont environ (18 %). Selon l'historien Geert Mak il existe au moins quatre communautés de culture et de traditions en Europe : la protestante du Nord, la catholique latine, la grecque orthodoxe et l'ottomane musulmane.

Le christianisme est la religion dominante en Europe et y est divisée en trois grandes confessions, (protestantisme, orthodoxie et catholicisme), réparties géographiquement de la façon suivante : 
Les catholiques sont majoritaires dans vingt-trois pays, les orthodoxes dans treize pays, les protestants dans neuf pays, les musulmans dans cinq pays (Albanie, Azerbaïdjan, Bosnie-Herzégovine, Kosovo et Turquie, les « sans religion » dans deux pays (République tchèque et Pays-Bas). À la fin du , la papauté a proclamé six saints patrons de l'Europe.

Il existe des minorités religieuses à l'intérieur de ces grands ensembles dont la plus importante est l'islam avec de musulmans soit près de 6 % de la population européenne totale :

Selon Emmanuel Todd, les systèmes familiaux en Europe sont d'une grande diversité.

Les pays qui ont tout ou partie de leur territoire sur le continent européen ou sont culturellement rattachés à l'Europe (selon les limites géographiques définies plus haut) sont au nombre de 51 :

Le nombre d'États souverains en Europe, qui s'élevait à plus de trois cents en 1789, était encore d'une soixantaine en 1815, au lendemain du congrès de Vienne. Après l'unification de l’Italie et de l’Allemagne, ce nombre était tombé à 19 en 1871 (20 avec la Turquie, qui contrôlait encore la majeure partie de la péninsule des Balkans). Il passa à 22 en 1878, lorsque le congrès de Berlin reconnut l'indépendance de la Roumanie, de la Serbie et du Monténégro. S'y ajoutèrent ensuite la Norvège (1905), la Bulgarie (1908) et l’Albanie (1912).

En 1914, l'Europe comptait donc généralement reconnus comme indépendants, non compris le Saint-Siège, l'ordre souverain de Malte et le territoire neutre de Moresnet : Albanie, Allemagne, Andorre, Autriche-Hongrie, Belgique, Bulgarie, Danemark, Espagne, France, Grèce, Italie, Liechtenstein, Luxembourg, Monaco, Monténégro, Norvège, Pays-Bas, Portugal, Roumanie, Royaume-Uni, Russie, Saint-Marin, Serbie, Suède, Suisse. La forme de gouvernement la plus répandue était la monarchie, puisque qu'on ne dénombrait alors que quatre républiques (la France, le Portugal, la Suisse et Saint-Marin) - huit en tenant compte des villes libres de Brême, Hambourg et Lübeck, inféodées à l'Empire allemand, et de la république monastique du mont Athos, placée sous le protectorat politique de la Grèce.
À la fin de l'année 1945, le nombre d'États était passé à 31 : Albanie, Allemagne, Andorre, Autriche, Belgique, Bulgarie, Danemark, Irlande, Espagne, Finlande, France, Grèce, Hongrie, Islande, Italie, Liechtenstein, Luxembourg, Monaco, Norvège, Pays-Bas, Pologne, Portugal, Roumanie, Royaume-Uni, Saint-Marin, Suède, Suisse, Tchécoslovaquie, URSS, Vatican, Yougoslavie. Plus de la moitié d'entre eux (19 sur 31) étaient encore des monarchies, y compris l'Albanie, la Bulgarie, l'Espagne, la Grèce, la Hongrie, l'Irlande, l'Italie et la Roumanie qui vivaient alors sous un régime transitoire. Depuis 1975, le nombre de monarchies s'est maintenu à douze, à savoir sept royaumes, un grand-duché, trois principautés et un État pontifical.

Parmi les nombreux États qui connurent une existence éphémère au lendemain des deux guerres mondiales, on peut citer les républiques autonomes de Rhénanie et du Palatinat (1923/1924), les villes libres de Dantzig (1920/1939), de Fiume (1920/1924), de Memel (1920/1923) et de Trieste (1947/1954), ainsi que le territoire de la Sarre, qui bénéficia d'un statut particulier de 1920 à 1935 et de 1947 à 1957.

Le nombre d'États européens parut se stabiliser à 34 avec l'accession à l'indépendance de Chypre (1960) et de Malte (1964). Il devait se maintenir à ce niveau jusqu'à la chute du mur de Berlin, en 1989. Après la réunification de l'Allemagne et l'éclatement des anciennes fédérations communistes (URSS, Yougoslavie, Tchécoslovaquie), puis avec la séparation de la Serbie et du Monténégro, le nombre d'États européens officiellement reconnus comme indépendants s'élevait à 45 en 2006 (50 avec l'Arménie, l'Azerbaïdjan, la Géorgie et tout ou partie de la Turquie et du Kazakhstan). Encore ce chiffre ne prend-il pas en compte les nombreux pays ou territoires dont le statut est contesté (Abkhazie, Chypre du Nord, Haut-Karabagh, Kosovo, Ossétie du Sud-Alanie, Saugeais, Principauté de Sealand, Seborga, Tchétchénie, Transnistrie)…

Par ailleurs, Israël fait partie de nombreuses associations européennes culturelles ou sportives (UEFA par exemple). L'Algérie, l'Égypte, Israël, le Liban, la Libye, le Maroc, la Syrie et la Tunisie font partie de l'Union européenne de radio-télévision. Le Maroc a participé au Concours Eurovision de la chanson en 1980 et Israël y participe depuis 1973. Ainsi, pour Pierre Beckouche, l’Europe est d’ores et déjà partie prenante d’un vaste ensemble macro-régional, appelé « Euroméditerranée », qui va de la Russie au Maroc en passant par le Moyen-Orient et qui est traversé de flux économiques, culturels et migratoires plus intenses qu'imaginé.

De nombreuses visions d'une Europe unie se sont affrontées au cours de l'histoire du continent, jusqu'à l'Union Européenne actuelle.

L'Europe n'a jamais connu d'unité politique parfaite. Certaines périodes d'une durée variable ont certes été marquées par la domination d'une vaste partie du continent par un pouvoir unique, qui s'est en général imposé par la force - ce fut ainsi le cas de l'Empire romain, de l'empire carolingien, de l'empire napoléonien et du Reich. Certaines familles royales ont également, par le biais de relations dynastiques, gouverné un grand nombre de pays européens, au premier rang desquelles la famille des Habsbourg. Mais on voit, tant hétéroclite est cette liste de candidats à l'hégémonie, que des projets d'unification européenne concurrents et divergents se sont affrontés sans qu'aucun ne parvienne vraiment à s'imposer.

L'Empire Romain est longtemps demeuré dans la mémoire des Européens comme symbole d'une unité perdue. Après sa chute en Occident en 476, Théodoric, Justinien, Charlemagne poursuivirent le rêve de la résurrection de l'Empire. Au Moyen Âge, la Papauté parvint enfin à s'imposer aux yeux d'une majorité de l'Europe comme l'héritière légitime de Rome, et à imposer au continent une forme d'unité, sous la forme de la Chrétienté médiévale : certes, les Papes ne possédaient qu'un pouvoir temporel limité sur les princes et les rois, mais jouissaient d'une autorité morale, religieuse et même juridique puissante. Surtout, la Chrétienté se conçoit elle-même comme une communauté, matérialisée positivement par l'union dans les croisades et négativement par la procédure de l’excommunication, avec des droits et des devoirs partagés (par exemple le respect des trêves et jours saints), et étendue au gros du continent (à l'exception des terres orthodoxes). 

La crise de la Chrétienté, l'affirmation des États proto-nationaux, l'affaiblissement de la Papauté, et surtout la Réforme qui brise l'unité de la Chrétienté font naitre la nécessité de repenser ce qui fait l'unité de l'Europe. C'est donc de la Renaissance que l'on peut dater la naissance de l'idée européenne moderne.

Au , déjà, des projets sont agités pour offrir paix et unité à l'Europe ravagée par des guerres intestines (Guerre de cent ans guerre hussite, guerres civiles de l'Espagne), dépeuplée par la grande peste, désunie spirituellement par le Grand schisme d'Occident et les hérésies (Wyclifisme, hussisme, pour ne nommer que les principales), menacée par l'expansion de l'empire du Grand Turc avec la prise de Constantinople. C'est le cas, par exemple, du projet d'union chrétienne de George de Podiebrad. 

Les Humanistes multiplieront les initiatives, aux , pour créer une Europe pacifiée et harmonieuse. Tandis que les évangélistes rêvent d'une Chrétienté rénovée, affranchie de la tutelle de Rome, des irénistes cherchent à réaliser la concorde entre les princes, sous l'égide d'une Raison médiatrice et partagée. Stefan Zweig loue en Érasme l'éblouissante incarnation de l'idéal européen des humanistes, lui qui institua un latin rénové comme langue de culture paneuropéenne, correspondant dans cette langue avec des intellectuels de tout le continent, et rêva d'une Europe réalisant par le pouvoir d'attraction de sa culture la concorde de l'humanité. Surtout, pour Zweig, Érasme fut celui qui prophétisa que l'union de l'Europe ne se ferait pas par la guerre, mais par des moyens pacifiques. On peut citer, parmi d'autres illustres précurseurs, Andrés Laguna de Segovia, qui en 1543 se lamentait sur la pauvre Europe déchirée et exsangue.

C'est peu de dire, toutefois, que le rêve humaniste ne devait pas immédiatement se réaliser. Certes, il exerça une influence certaine, même sur les élites politiques, devenant un idéal volontiers invoqué par les princes; ainsi lors de la signature en 1518 du traité de Londres, instaurant une « Paix Perpétuelle ». Mais la paix de 1518 fut rompue dès l'année suivante, et, dans le sillage de la Réforme, l'Europe s'enfonça dans la spirale sanglante des guerres de religion, en France et surtout en Allemagne, culminant dans le paroxysme de la Guerre de Trente Ans, qui embrasa le continent.

La Paix de Westphalie qui mit fin à cette guerre ne fonda pas une Union de l'Europe, mais au contraire officialisa une organisation de celle-ci fondée sur l'équilibre de puissances souveraines et régulièrement en guerre. Ce système qui régulait mais approfondissait la division européenne devait persister, perfectionné au par l'instauration de congrès réguliers , et renouvelé en 1815, jusqu'à la Première guerre mondiale.

Toutefois, cela n'empêcha pas que fleurissent, portés par des visionnaires, des projets d'union de l'Europe. Pour ne citer que des Français, Sully et Rousseau y ont révé; en 1712, l'abbé Castel de Saint-Pierre rend public son "Projet pour rendre la paix perpétuelle en Europe", et reçoit le soutien du philosophe Leibniz.

Le rêve européen reprend de la vigueur au , après la boucherie des guerres de la Révolution et de l'Empire. Dans une Europe dominée par la Sainte-Alliance, où triomphent tout ce que le Vieux Continent compte de réactionnaires, il est doté d'un nouveau contenu, social et humanitaire. Avant le Printemps des peuples en 1848, les républicains, démocrates et socialistes de toute l'Europe espèrent qu'une révolution ouvrirait la voie, conjointement, à une union pacifique du continent et à une réforme de ses sociétés dans un sens démocratique et égalitaire. Victor Hugo a rêvé qu'un jour existeraient les « États-Unis d'Europe »}, pendants des États-Unis d'Amérique, utopie humanitaire et prélude à l'unité de toute l'humanité. Son discours prononcé le , à l'occasion de l'ouverture du Congrès de la Paix à Paris, est resté célèbre. Il y évoque une Europe enfin pacifiée, unie sous un même gouvernement. La suite de l'Histoire prouva qu'il s'agissait d'une vision prophétique en avance sur son temps, avec la guerre de 1870 et les deux guerres mondiales.

Mais surtout, la « mystique européenne » fut vivement réactivée après la Première Guerre mondiale qui se chargea de démontrer, si besoin était, à un grand nombre d'intellectuels à quel point la guerre était absurde. Seule une Europe unie pouvait éviter le retour de l'horreur. Conscient du déclin de celle-ci face à l'Amérique (Albert Demangeon - 1920), ils cherchent la voie la plus sûre pour unifier le continent.

L'héritage culturel grec, le droit romain et l'unité chrétienne sont conçues par Paul Valéry comme les trois piliers de l'Europe, lors d'une conférence donnée à l'université de Zurich le . En 1923, le comte Richard Coudenhove-Kalergi publie "Paneuropa", ouvrage dans lequel il développe sa vision d'une Europe forte de d'individus, dont il exclut la Russie et la Grande-Bretagne, l'une considérée comme « asiatique » et l'autre plus préoccupée de toute manière par son Empire planétaire (vision partagée alors par les Britanniques eux-mêmes). C'est une vision qui s'appuie sur une analyse géopolitique d'un monde divisé en grands blocs antagonistes. Il rencontre un tel écho dans le monde intellectuel qu'il peut réunir à Vienne en 1926 un congrès avec plus de venus de différentes (l'un des premiers adhérents à son mouvement est le jeune maire de Cologne, Konrad Adenauer). Il trouve aussi le soutien de Louis Loucheur et Aristide Briand (qui sera d'ailleurs nommé président d'honneur du mouvement), mais dans l'ensemble les politiques ne le suivent pas et on le soupçonne parfois de travailler pour l'Allemagne. Quoi qu'il en soit le mouvement Pan-Europe est fondé et survivra jusqu'à nos jours (un membre de la famille des Habsbourg en est aujourd'hui le président). Le même Aristide Briand, alors président du Conseil, pourra s'appuyer sur ce mouvement pour appeler à la création d'une « sorte de lien fédéral » devant l'assemblée de la Société des Nations (SDN) en 1929.

Le , en accord avec les instances dirigeantes de la SDN, il remet aux autres gouvernements européens un mémorandum sur « l'organisation d'un régime d'Union fédérale européenne ». Il essuie un refus poli : c'est un échec.

La crise et la montée en puissance des totalitarismes étouffent progressivement tout espoir de construction européenne. L'Allemagne nazie conçoit l'Europe selon une vision pangermaniste, raciste et centrée autour d'une grande Allemagne. L'Europe n'est plus qu'un réservoir de matières premières et de main-d'œuvre, destinée à nourrir la machine de guerre nazie.

Mais la résistance pense aussi l'Europe, et tandis qu'elle mène le combat intérieur partout en Europe contre le fascisme et le nazisme, ses membres les plus éminents se réunissent afin de dessiner les contours d'une Europe post-Seconde Guerre mondiale.

Après la guerre Churchill appelle à son tour de ses vœux à l'unité européenne et crée un mouvement qui fusionne très peu de temps après avec celui de Richard Coudenhove-Kalergi. Devant ce qui est perçu comme le danger soviétique, les États-Unis lancent un vaste programme de reconstruction de l'Europe avec le plan Marshall. Celui-ci conditionne la formation d'une Europe financière appuyée sur des politiques monétaires concertées (création de l'OECE - Organisation Européenne de Coopération Économique). Il faut désormais attendre la déclaration Schuman du pour assister à la relance du vieux projet d'union européenne, cette fois lancée par étape, en commençant par l'un des secteurs économiques phares pour les Français comme pour les Allemands, l'industrie de la houille et de la sidérurgie. En plaçant ces productions sous la houlette d'une Haute Autorité, c'est le consentement prudent mais définitif d'un abandon de souveraineté qui transparaît. La CECA (Communauté Européenne du Charbon et de l'Acier) née le par la signature du Traité de Paris, elle réunit six États européens : le Luxembourg, la Belgique, les Pays-Bas, l'Italie, la RFA et la France. L'Europe est en marche.

→ Pays membres de la zone euro (19 membres) :
→ Pays non-membres de la zone euro (9 membres) :
→ 4 pays non-membres de l'Union européenne, mais ayant signé des accords spécifiques avec celle-ci au sein de l'Union douanière de l'Union européenne :





La Biélorussie et le Vatican sont les deux seuls États européens souverains et indépendants à n'être membres d'aucune organisation supranationale européenne. Cependant, le Vatican dispose d'un statut d'observateur au Comité des ministres du Conseil de l'Europe et fait également partie de la zone euro, tandis que la Biélorussie est candidate à l'adhésion au Conseil de l'Europe depuis 1993.


L'Europe, ou plus précisément l'Union européenne, est le plus important des pôles de la triade (États-Unis, Union européenne et Japon). Ces pôles centralisent 70 % de la richesse pour 14 % de la population. Si l’Europe est la région la plus riche et développée du monde, elle n'est pas un espace économiquement homogène : tous les pays européens ne sont pas des pays développés : l'Ukraine et la Moldavie font exception et sont classés comme pays à développement moyen avec un IDH inférieur à 0,8. L’Europe de l'Ouest et l'Europe du Nord très prospères contrastent avec certaines régions moins riches d'Europe centrale, d'Europe de l'Est (Moldavie, Ukraine, certaines régions de Roumanie, Russie) et d'Europe du Sud (Albanie, Serbie, Macédoine, certaines régions de Bulgarie, Italie du Sud, certaines régions d'Espagne, de Grèce et du Portugal).

La mégalopole européenne constitue le cœur économique de l'Europe. On peut ainsi distinguer principalement les pays de l'ancien bloc de l'Ouest, développés et avec une croissance faible et les pays de l'ancien bloc de l'est moins développés mais à plus forte croissance.

L’Union européenne, principal ensemble de la région, est en 2015 la deuxième puissance économique du monde. Tous ses pays membres commercent entre eux librement grâce au Marché commun, et dix-huit de ses pays ont accentué leur collaboration au sein de la zone euro. Des accords de libre-échange ont également été passés avec des pays partenaires, comme la Suisse.

L'Europe est un producteur important de céréales, de fruits et légumes, et de sucre, grâce aux cultures de betteraves, très développées en Ukraine et dans le nord de la France. Sur les six premières années de la décennie 2010, le continent a confirmé sa troisième place au palmarès des grands producteurs mondiaux de sucre, malgré un léger déclin, derrière les deux géants, le Brésil et l'Inde. Parmi les points forts de son agriculture, l'Europe était aussi troisième au palmarès des producteurs mondiaux de céréales au milieu des années 2010, dominé par les États-Unis.

Parler de "culture de l'Europe" est difficile, car de nombreuses cultures s'y sont succédé (et ont souvent assimilé des apports extra-européens) depuis plusieurs millénaires. Une définition de la culture de l'Europe doit nécessairement aussi tenir compte des limites géographiques du continent.

Le tourisme culturel tient une place singulière en Europe, elle est une des clés de l'avenir permettant d'assurer une puissante force d'attraction pour l'Europe. Elle touche essentiellement l'audience des musées, des monuments et des évènements culturels. Et donne lieu à des déplacements vacanciers. Par conséquent, elle est une mine de recette considérable pour les pays européens. L'activité touristique s'est notablement enrichie depuis une vingtaine d'années, et les modes de visite des touristes ont beaucoup évolué. Le tourisme étranger en France en est une vivante illustration.




Histoire :

Cartes : 


</doc>
<doc id="1012" url="https://fr.wikipedia.org/wiki?curid=1012" title="Euro">
Euro

L'euro (€) est la monnaie de l'union économique et monétaire, formée au sein de l'Union européenne ; elle est commune à dix-neuf États membres de l'Union européenne qui forment ainsi la zone euro. 

Quatre micro-États (Andorre, Monaco, Saint-Marin et le Vatican) sont également autorisés à utiliser l'euro, ainsi que deux pays européens non-membres, le Monténégro et le Kosovo qui l'utilisent "".

D'autres pays ont leurs monnaies nationales liées à l'euro (Bénin, Bosnie-Herzégovine, Burkina Faso, Cameroun, Cap-Vert, Comores, Congo (Brazzaville), Côte d'Ivoire, Gabon, Guinée équatoriale, Guinée-Bissau, Mali, Niger, Nouvelle-Calédonie, République centrafricaine, Sao Tomé-et-Principe, Sénégal, Tchad, Togo). 

En usage dès 1999 pour les transactions financières européennes, il est mis en circulation le sous sa forme fiduciaire. Il succède à l"'ECU", mise en service en 1979.

L'euro est la deuxième monnaie au monde pour le montant des transactions, derrière le dollar américain et devant le yuan chinois. Depuis , elle est la première monnaie au monde pour la quantité de billets en circulation.

Au janvier 2018, il y avait billets en circulation dans le monde, pour une valeur totale de , 
ainsi que pièces de monnaie 
pour une valeur totale de , 
l'ensemble représentant la somme de 

L'euro est géré par la Banque centrale européenne (BCE) qui siège à Francfort et par l'Eurosystème, composé des banques centrales des états de la zone euro. En tant que banque centrale indépendante, la BCE est l'unique instance ayant le pouvoir de fixer une politique monétaire pour l'ensemble de la zone euro. L'Eurosystème participe à l'impression, la frappe et la distribution des billets et pièces dans tous les États membres ; il veille également au bon fonctionnement des systèmes de paiements au sein de la zone euro.

Le traité de Maastricht, signé en 1992, oblige la plupart des États de l' à adopter l'euro dès qu'ils respectent certains critères monétaires et budgétaires, dits de convergence. Le Royaume-Uni et le Danemark ont cependant obtenu des options de retrait, tandis que la Suède (qui rejoint l' en 1995, soit après la signature du traité de Maastricht) refuse d'introduire l'euro, après un référendum négatif en 2003, et contourne au surplus l'obligation d'adopter l'euro en ne respectant pas un des critères de convergence. Néanmoins, tous les pays qui adhérent à l' depuis 1993 se sont engagés à adopter l'euro en temps voulu.

La gestion de l'euro dépend du contrôle de la Banque centrale européenne qui en mesure les flux, la masse monétaire, ainsi que les dettes des États membres.

Toutes les pièces en euro possèdent une face européenne commune (1, 2 et : "l'Europe dans le monde" ; 10, 20 et : "l'Europe comme une alliance d'États" ; 1 et : "l'Europe sans frontière") et une face spécifique au pays émetteur (y compris Monaco, Saint-Marin, le Vatican et Andorre, États en union monétaire avec leurs voisins immédiats qui sont autorisés à frapper leurs propres pièces).

Une nouvelle série de pièces est frappée depuis fin 2007 avec un décalage d'un an pour la monnaie italienne (qui frappe aussi les pièces du Vatican et de Saint-Marin). Se calquant sur la pratique décidée pour les billets de banque, elle représente désormais l'ensemble du continent européen (membre ou non de l'Union), afin d'éviter de devoir frapper de nouvelles séries à chaque élargissement. Les frontières n'y apparaissent donc plus.

Toutes les pièces sont utilisables dans tous les États membres, à l'exception des pièces de collection, qu'elles soient ou non en métal précieux, qui n'ont cours que dans le pays d'émission. Des problèmes de compatibilité sont cependant relevés sur certains automates (distributeurs automatiques, péages…).

Il existe également des pièces de collection, en métal précieux, qui n'ont cours légal que dans leur pays d'émission. Par exemple, gravée par Joaquin Jimenez (qui est également l'auteur de l’"Arbre Étoilé" des pièces de 1 et ), une pièce de en argent est frappée à deux millions d'exemplaires en 2008. Des pièces de 10 à , en argent et en or, sont mises en circulation de 2008 à 2010.

Les billets, quant à eux, ont une maquette commune à toute la zone euro. Les ponts, portes et fenêtres des billets symbolisent l'ouverture de l'Europe sur le reste du monde et les liens entre les peuples.

Le choix du graphisme des billets est de la compétence de la Banque centrale européenne alors que celui des pièces est de la compétence des États membres de l'Eurogroupe. La prochaine face commune est ainsi été décidée lors d'une réunion de l'Eurogroupe. Cette décision provoque une petite polémique de la part de quelques députés par l'absence, selon eux volontaire, de la Turquie sur le dessin retenu, au contraire de celui des billets.

Le , un nouveau billet de est mise en circulation ; il est le premier d'une nouvelle série de billets baptisée "Europe".

Le , c'est un nouveau billet de 10 euros qui fait son apparition. Puis, le , c'est le nouveau billet de 20 € qui est mis en circulation. Le billet de 50 € est mis en circulation le .

L'euro n'est pas la première monnaie à vocation européenne (et internationale). En effet, l'Union latine, née en 1865 à l'initiative de , marque une union monétaire, ou supranationale, signée et partagée par la France, la Belgique, la Suisse, l'Italie, la Grèce et, plus tard, l'Espagne et le Portugal, puis la Russie et certains pays d'Amérique latine. La Première Guerre mondiale (1914-1918) met fin à ce projet d'unification monétaire.

Le projet de créer une monnaie unique naît dans les années 1970 avec les turbulences du régime agrimonétaire, depuis la mise en œuvre de la Politique agricole commune, en 1962, et l'impossibilité de mettre en place un système de taux de change contrôlable.

La décision de créer l'euro est officialisée lors du traité de Maastricht. Lorsque les négociations sont engagées, les responsables savent qu'économiquement la constitution de la zone euro est un défi. En effet, les économistes savent, depuis les travaux de Robert Mundell (dans les années 1950) que, pour que des pays aient intérêt à avoir une même monnaie, ils doivent :

Pour Jean Pisani-Ferry, les responsables politiques des pays décident de passer outre, pour trois raisons :

Deux visions s'opposent :

L'euro est créé par les dispositions du traité de Maastricht, en 1992. Pour participer à la monnaie unique, les états membres sont censés répondre à des critères stricts tels qu'un déficit budgétaire de moins de 3 % de leur PIB, un endettement inférieur à 60 % du PIB (deux critères qui sont régulièrement bafoués après l'introduction de l'euro), une faible inflation et des taux d'intérêt proches de la moyenne de l'. Lors de la signature du traité de Maastricht, le Royaume-Uni et le Danemark obtiennent des options de retrait pour ne pas participer à l'union monétaire qui se traduirait par l'introduction de l'euro.

De nombreux économistes tels que Fred Arditti, Neil Dowling, Wim Duisenberg, Robert Mundell, Tommaso Padoa-Schioppa et Robert Tollison participent à la création de la monnaie unique.

L'appellation « euro » est officiellement adoptée à Madrid, le . L'espérantiste belge, Germain Pirlot, ancien professeur de français et d'histoire, est désigné pour dénommer la nouvelle monnaie ; il envoie une lettre au président de la Commission européenne, Jacques Santer, et suggère la dénomination « euro », le .

Les taux de conversion sont déterminés par le Conseil de l'Union européenne, sur la base d'une recommandation de la Commission européenne, établie sur les taux du marché au . Ils sont créés de sorte qu'une unité de compte européenne (ECU) serait égale à un euro. L'unité monétaire européenne était une unité de compte utilisée par l' et calculée sur la base des monnaies des États membres. Ce n'était pas une monnaie à part entière. Les taux n'ont pas pu être fixés plus tôt car la valeur d'un ECU dépendait des taux de change des monnaies ne participant pas à l'euro (comme la livre sterling), à la clôture, ce jour-là.

La procédure utilisée pour fixer le taux de change irrévocable entre la drachme grecque et l'euro est différente : alors que les taux de change pour les onze monnaies initiales sont déterminés quelques heures seulement avant que l'euro n'ait été introduit, le taux de conversion de la drachme grecque est fixé plusieurs mois à l'avance.

La monnaie est introduite sous forme immatérielle (chèques de voyage, transferts électroniques, services bancaires…), le , à minuit. Les monnaies nationales des pays participants (formant alors la toute nouvelle zone euro) cessent dès lors d'exister indépendamment. Les taux de change sont alors bloqués à taux fixes, les uns envers les autres. L'euro devient ainsi le successeur de l'unité de compte européenne (ECU). Les billets et pièces des anciennes monnaies continuent cependant à avoir cours légal jusqu'à ce que les billets et pièces en euro soient introduits, le .

La période de transition au cours de laquelle les anciens billets et les anciennes pièces sont échangés contre billets et pièces en euro durent environ deux mois, jusqu'au . La date officielle à laquelle les monnaies nationales cessent d'avoir cours légal varie d'un état membre à l'autre ; la période la plus courte est en Allemagne, où le Deutsche Mark cesse officiellement d'avoir cours légal le , bien que la période de transition y dure également deux mois. Même après que les monnaies nationales cessent d'avoir cours légal, elles continuent à être acceptées par les banques centrales nationales, pour des périodes plus ou moins longues, allant de plusieurs années à tout jamais (voir ici). Les premières pièces cessant d'avoir cours légal sont les pièces portugaises en escudo, qui cessent d'avoir cours légal le , bien que les billets restent échangeables jusqu'en 2022.

En 2002, l'euro est lauréat du Prix International Charlemagne.

L'adhésion à l'euro est obligatoire pour les nouveaux membres de l', mais chaque pays en fixe la date et doit respecter les conditions économiques nécessaires.

La zone euro s'étend progressivement :

Des trois membres de l' (à quinze) non participants, seuls le Royaume-Uni et le Danemark obtiennent une clause dite d’", leur permettant de rester en dehors de la monnaie commune, même s'ils venaient à remplir les conditions d'adhésion, clause confirmée par le traité de Rome de 2004. Cette clause ne leur interdit toutefois pas d'adhérer ultérieurement.

Aussi le Royaume-Uni, contrairement au Danemark, ne fait pas partie du mécanisme de change (), bien que remplissant les conditions du traité de Maastricht car il ne souhaite pas lier le taux de change de la livre sterling à l'euro ; depuis que la livre sterling a quitté le défunt SME (fondé sur l'ancienne unité de compte européenne, ou ECU), son cours par rapport à l'euro connaît des variations plus importantes que les autres monnaies des pays membres non adhérents, notamment durant les deux premières années de l'introduction de l'euro, qui s'est temporairement fortement déprécié par rapport au dollar américain, à la livre sterling et au franc suisse. Cette instabilité initiale est, semble-t-il, résolue et, depuis, la livre sterling suit de façon assez proche les évolutions du cours de l'euro (le franc suisse s'est aussi stabilisé par rapport à l'euro et il remplirait les conditions d'entrée dans le si la Suisse et le Liechtenstein décidaient de rejoindre l'Union européenne).

En revanche, la Suède s'est engagée à rejoindre à terme la monnaie commune et ce, dès qu'elle remplira les conditions du traité de Maastricht. Cependant, en raison d'une opinion publique qui reste favorable au maintien de la couronne suédoise, comme le montre le dernier référendum organisé sur ce sujet, le , la Suède ne remplit pas techniquement les conditions d'entrée dans le afin de ne pas être contrainte d'adopter automatiquement la monnaie commune.

Les états membres qui ont rejoint l' après la mise en place de l'euro sont tenus d'intégrer, à terme, la zone euro. Ceci suppose qu'ils intègrent d'abord le puis qu'ils remplissent les autres conditions d'adoption de l'euro. Ainsi, pour la Hongrie, la Pologne, la République tchèque, la Bulgarie et la Roumanie, tout nouvel élargissement n'est pas à prévoir , selon Valdis Dombrovskis, commissaire européen chargé de l'euro.

En , le ministre des Finances bulgare, Simeon Djankov, annonce que son pays renonce à abandonner sa monnaie nationale pour l'euro, du fait de l'incertitude entourant la pérennité de la monnaie unique. Notons cependant que la Bulgarie est juridiquement obligée d'adopter l'euro à terme, ayant ratifié son traité d'adhésion à l' sans bénéficier d’".

Au , 19 pays de l'Union européenne utilisent l'euro comme monnaie nationale.

À ceux-ci s'ajoutent quatre états hors ayant des accords officiels et utilisant donc l'euro de façon officielle, ainsi que deux autres états l'ayant adopté unilatéralement. Le cas des bases britanniques à Chypre est particulier : le traité d'indépendance de Chypre y prévoyait l'utilisation exclusive de la monnaie locale, ce qui a imposé un basculement vers l'euro ; cette particularité est prévue par le traité d'adhésion de Chypre et par le droit britannique.

L'euro est également " utilisé dans plusieurs pays hors d'Europe, comme au Zimbabwe où il circule aux côtés du dollar américain, du rand sud-africain, du pula botswannais et de la livre sterling.

En 2015, au sein de l'Union européenne et des pays candidats à l'entrée dans l'Union, se trouvent les monnaies suivantes qui ont toutes intégré le :

Toutefois, les phases préparatoires avant l'introduction de l'euro sont actuellement considérablement raccourcies, tous ces pays négociant déjà l'euro sur les marchés internationaux et disposant même de stocks de pièces et billets pour le marché des changes aux particuliers (notamment dans les zones touristiques). Dans certains de ces pays, de nombreux commerces acceptent les paiements en euro (parfois même aussi en pièces et billets), certains pratiquant même le double affichage sur un taux voisin du cours central défini dans le (qui autorise une variation de 15 % du cours, mais qui, en pratique, varie dans des marges très inférieures, le marché des changes étant déjà très stabilisé, ce qui permet même à certains pays de garantir unilatéralement leur taux de change par l'intervention de leur banque centrale), ou autorisant l'ouverture de comptes en euro pour les entreprises et les administrations, afin de limiter les frais relatifs aux opérations de change.

Dans les derniers jours précédant l'évaluation par la Commission européenne d'une devise après deux années de stabilité, il apparaît une instabilité temporaire du cours de cette devise liée à une anticipation du marché sur une prochaine convertibilité totale de cette devise, ce qui limite l'intérêt de conserver des fonds de garantie dans cette devise. Mais la BCE et les BCN veillent à limiter cette instabilité et assistent la BCN, candidate pour limiter cet impact temporaire, en achetant ou vendant massivement les surplus de change sur les marchés financiers. Une stabilisation forte en dernière minute est donc constatée autour du taux central défini dans le , sauf si les engagements financiers pris par la BCE sont trop importants et nécessitent un ajustement pour éviter d'imposer à la BCN candidate des dettes dès son entrée dans l'UME, qui ne lui permettraient plus de remplir les objectifs de stabilité de Maastricht. Aussi, le cours central, défini dans le , ne préfigure pas forcément le taux de conversion définitif qui sera appliqué (mais qui devrait rester tout de même dans la bande de fluctuation de 15 % autour du taux central).

Un certain nombre de devises, hors Union européenne, sont déjà liées à travers un taux de change, fixe ou variable, à l'euro :

Les monnaies suivantes des pays membres de l'Union européenne, ou candidats à l'adhésion, ne sont pas liées à l'euro.

Quatre micro-états enclavés dans l'Union européenne, sans en être membres, ont obtenu le droit d'utiliser l'euro : Andorre, Monaco, Saint-Marin et le Vatican ; ces états sont également autorisés à frapper un certain nombre de pièces de monnaie (officiellement depuis le pour Andorre, effectif à compter du ).

Deux autres états, ou entités européennes, non membres de l'Union, utilisent également l'euro " : le Monténégro et le Kosovo, sans dépendre de la BCE, sans pouvoir émettre de pièces ni de billets.

L'euro est également utilisé "" dans certains territoires d'outre-mer, non intégrés à l'Union, dont les habitants sont citoyens d'un pays de l'Union ; c'est le cas de Saint-Pierre-et-Miquelon et, dans une mesure plus infime, des TAAF.

Enfin, l'euro est accepté comme quasi seconde monnaie dans des régions d'états non-membres frontalières de la zone euro (Genève) ou pour des raisons touristiques (Polynésie).

La valeur de l'euro, exprimée dans les anciennes monnaies de ces pays, est la suivante :

Notes :
Notes :

L'ECU, qui était un panier, contenait des monnaies comme la livre sterling, qui n'ont pas été intégrées dans l'euro. Les deux devises européennes ne coïncident donc que brièvement, pendant les heures de fermeture des marchés entre la fin de 1998 et le début de 1999 et, si l'ECU existait encore, il aurait maintenant une valeur tout autre que celle de l'euro.

Afin de reconstituer la valeur qu'aurait eue l'euro par rapport au dollar américain avant sa cristallisation du , il convient d'utiliser les taux de change face au dollar d'une monnaie nationale, et de lui appliquer son taux de conversion en euro. Par exemple, sera divisé par la valeur du dollar en francs français. Le résultat du calcul figure sur le graphique ci-contre pour le franc français (en rouge) et le deutschemark (en bleu) pour toute la période qui va de l'introduction du régime des changes flottants par Richard Nixon à celle de l'euro.

Pendant les dix années précédant son introduction, l'euro aurait ainsi eu une valeur moyenne de l'ordre de , calculée avec le franc français, et , calculée avec le deutschemark.

Le marché des changes le plus actif de l'euro est bien évidemment celui comparé au dollar US ; la parité euro/dollar est l'instrument financier le plus traité dans le monde , c'est un indicateur phare, suivi quotidiennement par tous les milieux économiques et financiers.

À partir de l'introduction de l'euro, l'inflation perçue en France a été nettement plus élevée que l'inflation réelle.

Pourtant, l'inflation dans la zone euro n'a pas augmenté à partir de 2002, date d'introduction. Entre 2000 et 2006, elle a fluctué entre 1,9 et 2,3 %. En 2003, l'augmentation moyenne des prix à la consommation due au basculement à l'euro ne représente qu'environ 0,1 à 0,3 % du taux normal d'inflation de 2,3 % pour l'année en question. Ainsi, l'indice des prix au sein de la zone euro est resté inférieur à celui des pays européens qui n'ont pas introduit la monnaie commune.

La Banque de France a montré que l'introduction de l'euro s'est produite simultanément à une hausse importante des prix de l'immobilier et des produits pétroliers. Par ailleurs, certains secteurs (hôtellerie, tabac par exemple) ont connu de fortes hausses de prix depuis l'introduction de l'euro.

Le niveau faible de l'inflation globale s'explique également par les fortes baisses de prix observées depuis le début des années 2000 pour les biens d'équipement (ordinateurs, machines à laver, automobiles, téléphones mobiles), qui n'ont pas été ressenties par les consommateurs (phénomène de biais cognitif qui s'explique du fait qu'il ne s'agit pas de biens de première nécessité et dont la périodicité d'acquisition moyenne est supérieure à l'année). Pour cela, il est intéressant d'observer le taux d'inflation pour les ménages modestes, qui est calculé en tenant compte de leurs achats (la baguette de pain a un poids plus fort…).

Face à la polémique, le ministre des Finances français, Thierry Breton, propose, ultérieurement, un indice spécial lié au coût du panier d'achat au supermarché pour répondre aux critiques des associations de consommateurs. L'INSEE introduit un indicateur d'inflation personnalisé.

En mai 2017, l'Insee publie une étude dans laquelle elle confirme qu'il n'y a pas eu "d'inflation particulière" depuis l'introduction de l'euro.

Si la Banque centrale européenne a un objectif d'inflation de 2 %, il s'agit d'une moyenne : sur les 12 premières années, l'inflation est, en moyenne, de 1,5 % en Allemagne, de 1,8 % aux Pays-Bas mais de 3,3 % en Grèce, de 2,8 % en Espagne et de 2,5 % au Portugal. La perte de compétitivité qui a suivi le différentiel d'inflation est un des éléments clés d'explication de la crise de la zone euro. Ce problème est difficile à régler lorsque les pays ne peuvent pas dévaluer. En effet, seules deux solutions sont alors possibles : une dévaluation interne (baisse des salaires) dans les pays qui ont connu trop d'inflation ou une politique de relance dans les pays affectés par une inflation trop faible.

Depuis la crise de 2007, la zone euro est confrontée à un problème de déflation dans la plupart des pays qui la compose ce qui a amené la BCE à agir et à utiliser les outils de politique monétaire dont elle dispose pour tenter de faire remonter l'inflation. Certains sont habituels, comme la baisse du taux directeur (ce qui permet en théorie de relancer le crédit), d'autres exceptionnels comme l'achat d'actifs par le biais de l'assouplissement quantitatif (QE). Les résultats de cette politique sont, en 2017, insuffisants, et certains analystes plaident pour joindre à la politique anti-déflation de la BCE une politique de relance budgétaire de la part des États qui compose la zone euro, mais cette option suscite de vives controverses au sein de l'Eurogroupe, en particulier une opposition de l'Allemagne, qui plaide pour l'orthodoxie budgétaire partout dans la zone Euro.

La politique monétaire menée (au moins jusqu'au QE) conduit à un « euro fort », ou qualifiée par ses détracteurs d'« euro cher ». À terme, selon le centre de recherche économique CEE ", le maintien de l'orthodoxie financière, prôné par le gouvernement allemand et la BCE, et la politique de rigueur généralisée qui en découle, nécessiteront une révision du traité de Lisbonne, car ils pourraient avoir pour conséquence de réduire les prérogatives budgétaires et fiscales des états-membres, au-delà des dispositions du traité dans sa forme actuelle.

Le CEPII soulignait en 2012 que, par construction, l'euro empêche les taux de change de s'ajuster pour compenser les déséquilibres des balances commerciales des pays membres. En l'absence de ce canal, l'ajustement doit se faire par des taux d'inflation différenciés entre pays, ce qui suppose des dévaluations internes (baisse des salaires) pour les pays les moins compétitifs, ou par une montée en gamme des produits. Paul Krugman souligne à ce propos que l'Allemagne bénéficie d'un Euro légèrement sous-évalué par rapport au DM (si celui-ci était toujours en circulation), contrairement aux autres pays d'Europe, en particulier au sud, qui ont une monnaie sur-évaluée. Cette analyse a été confirmée par une étude du FMI datant de 2017 . Ce déséquilibre a sa part de responsabilité dans l'excédent commercial très élevé de l'Allemagne, qui est en partie responsable, selon nombre d’économistes, dont ceux du FMI, de l’anémie de la croissance européenne. De plus, l'épargne résultant de cet excédent s'investirait peu dans la zone euro et profiterait peu aux voisins de l'Allemagne.

Un sondage est effectué à la demande du ", durant l'été 2010. À la question , si une majorité de Néerlandais a répondu oui, 53 % des Allemands et des Espagnols ont répondu non, ainsi que 60 % des Français. Pourtant, à cette même date, un autre sondage indiquait que seuls 38 % des Français étaient en faveur d'un retour au Francs.

En 2015, dans un sondage "Eurobaromètre", 61 % des citoyens des pays du zone euro ont répondu, à la question , que l'euro était une bonne chose pour leur pays, alors que 30 % ont dit que c'était une mauvaise chose le niveau de soutien le plus élevé enregistré par Eurobaromètre depuis qu'il a commencé à demander cette question en 2002, et une importante augmentation depuis le plus bas niveau de soutien (moins de 50 %) enregistré par ce sondage, en 2007. Les pays les moins favorables à l'euro était l'Italie et la Chypre, les deux pays où moins de la majorité absolue ne se sont prononcés favorables à l'euro (en Italie, 49 % en faveur et 41 % contre ; en Chypre 50 % et 40 % respectivement) et la Lettonie (54 % en faveur, 29 % contre), pendant que les pays les plus favorables étaient le Luxembourg (79 % en faveur, 14 % contre), l'Irlande (75 % et 18 %), et l'Allemagne (70 % et 22 %).

En 2017, un sondage Ifop indique que 72 % des Français sont en faveur d'un maintien dans la zone euro. Seuls 28 % se sont déclarés en faveur d'une sortie dont une majorité est électrice du Front national.

Nombre d'économistes pointent le fait que les pays de la zone euro ne constituaient pas, en 2002, une zone monétaire optimale, et qu'un défaut de convergence des politiques économiques, et l'absence d'outils de gestion commune (trésor, budget fédéral) ne les rapprochent pas de cette configuration. Les économistes Milton Friedman et Martin Feldstein ont également exprimé leur doute à ce sujet. L'absence de trésor et de budget fédéral entraîne l'absence de transferts (en particulier fiscaux) entre pays en excédent et pays en déficit, ce qui pose un problème qui peut menacer à terme la viabilité de la zone Euro. Conscients du problème, un certain nombre de responsables européens, dont Emmanuel Macron, plaident pour une Europe budgétaire, tentant d'infléchir la position de l'Allemagne sur ce sujet.

L'euro est la deuxième monnaie de réserve dans le monde, loin derrière le dollar américain ; cependant, petit à petit, l'euro commence à augmenter comme monnaie de réserve dans le monde, passant de 17,9 %, en 1999, à 27,3 %, en 2009.

Ceci vient confirmer les propos d'Alan Greenspan, ancien président de la banque centrale des États-Unis, selon lesquels .

Une étude montre que l'introduction de l'euro a eu un effet positif sur le tourisme en Europe, avec une augmentation de 6,5 % du nombre de touristes au sein de la zone euro.

L'euro est actuellement présent dans les documents électroniques et les bases de données de nombreux pays, non seulement de l'Union économique et monétaire, mais aussi de nombreux pays du monde. Il faut signaler que cette devise, comme toutes les autres, ne fait pas encore partie d'une norme internationale de métadonnées (voir ""), en raison de la variabilité des monnaies et des prix soumis aux lois d'évolution des marchés ; cependant, la norme ISO est abondamment utilisée dans les bases de données et les échanges informatiques, et attribue le code EUR à l'euro, norme à caractère quasi obligatoire pour les transferts interbancaires de devises et la tenue des comptes à la place des symboles monétaires souvent ambigus (même si l'euro a un symbole bien défini, la présence de devises dérivées non régulées par l'ECB est source de nouvelles ambiguïtés).

Du fait d'alphabets différents les noms et divisions nationales de l'euro ne s'orthographient et ne se prononcent pas de la même façon dans tous les pays de la zone.

Du fait de la diversité des règles grammaticales au sein de la zone euro, le mot sur les pièces et billets est invariable et ne prend donc pas de . Toutefois, dans la langue française, alors qu'il ne se prononce pas, par us et usages à l'écrit, un « s » est généralement ajouté au pluriel. L'Académie française s'est prononcée en ce sens dans une note publiée au Journal officiel du .

En revanche, en France, le terme , prêtant à confusion, n'est généralement pas utilisé en français ; on parle de centime ou, dans une forme plus rare et déconseillée, d'eurocent (pour ne pas confondre avec les centimes de franc pendant la phase transitoire). Pour des raisons similaires, il est dit ' en espagnol, ' en italien, ' (pluriel ') en grec, alors que ce problème ne se pose pas en anglais, par exemple, langue dans laquelle il est adopté tel quel.

La liaison avec l'adjectif antéposé et le nom euro suit la règle habituelle du français, on prononce donc "un(n)euro", "dix(z)euros", "vingt(t)euros", "quatre-vingts(z)euros", "cent(t)euros".

En France, deux imprimeries fabriquent des billets de 5, 10 et : l'imprimerie de la Banque de France, à Chamalières (Puy-de-Dôme) et l'imprimerie de François-Charles Oberthur Fiduciaire, à Chantepie (Ille-et-Vilaine). Ces billets sont destinés à remplacer ceux qui sont trop usés, en France et dans toute l'Europe.

Les autres coupures sont fabriquées dans d'autres pays européens puis envoyées en France selon une sorte de contrat d'échange établi par la BCE. En revanche, les pièces françaises en euro sont toutes frappées à Pessac (Gironde), par la direction des Monnaies et médailles.







</doc>
<doc id="1013" url="https://fr.wikipedia.org/wiki?curid=1013" title="Empereur">
Empereur

Empereur est un titre monarchique, parfois héréditaire, porté par le souverain d'un Empire.

Du latin ' qui signifie « commander en maître, ordonner », du préfixe ' et du verbe "", préparer, apprêter. Il a donné le mot imperium, « commandement » d’où découle « "impérieux" ». 

« Empereur » est une déformation du titre d’"imperator" que portaient les généraux romains victorieux acclamés par leurs troupes. À l'origine de la République romaine, l’"" était celui qui commandait la mobilisation des citoyens. Par glissement de sens, il désignera les actes qui en découlent puis, vers la fin de la République, il désigna celui qui commandait l’armée. Pour Scipion l'Africain, c’était un titre que l’armée accordait au vainqueur avec l’ovation, dans le cadre du culte à Jupiter. Le titre d’"imperator" n'est pas une magistrature et n’a alors aucune valeur institutionnelle pour le Sénat romain.

Son sens actuel va apparaître avec Octavien lorsque ce dernier prit ' pour prénom, afin de conserver le souvenir perpétuel de ses victoires et de sa gloire. Après l’assassinat de Jules César, qui avait fait de lui son héritier, Octavien recevra l’"imperium" du Sénat le puis le , alors que César était élevé au rang des dieux, Octavien reçut le nom de '. 

C’est ce titre d’"Auguste" qui correspond à ce que l’on entend actuellement par "empereur", c’est-à-dire dirigeant de l’Empire. Plus largement, l’empereur à Rome est celui qui porte les titres suivant : ', ', "" et dans un premier temps "Princeps". L’équivalent en grec de ces termes, à savoir autocrate, sébastocrate et basileus a, par la suite, été utilisé dans l’Empire byzantin. Plus largement, la plupart des titres impériaux occidentaux renvoient aux termes latins, et tsar étant ainsi des déformations du titre de César.

Le féminin d'empereur est "impératrice" et l'adjectif correspondant est "impérial" ("impériale" au féminin).

La taille du territoire gouverné et la diversité religieuse et ethnique des peuples gouvernés peuvent être pris en considération. Ainsi, un roi peut porter deux titres telle la reine Victoria, reine du Royaume-Uni de Grande-Bretagne et d'Irlande (1837-1901) et Impératrice des Indes (1876-1901), sa fille la princesse Victoria qui fut également brièvement en 1888 à la fois reine de Prusse et Impératrice allemande, ou encore l'Empereur d'Autriche, également roi de Hongrie. Parfois, l'empereur est assimilé à une divinité, tel le au Japon.

En Europe, le titre impérial fut porté par les monarques qui se réclamaient de l'héritage impérial romano-byzantin. Ainsi, Charlemagne fut empereur d'Occident et Charles Quint le tout-puissant souverain du Saint-Empire romain germanique. En fait, jusqu'au milieu du , l'empereur affirmait sa prééminence théorique sur les rois (de France, d'Angleterre) dans toute l'étendue de la romanitas. Il en resta ensuite quelque chose, ainsi Philippe le Bel — et ses successeurs — s'affirmait ; en effet refusant la souveraineté de l'empereur, théoriquement situé au-dessus des rois, le roi de France prétendait avoir à l'intérieur de ses frontières les mêmes droits que l'empereur sur les autres rois, remettant ainsi en cause toute subordination à l'hégémonie impériale (en effet les rois de Bohême par exemple étaient bien plus influencés par l'empereur que le roi de France, qui prétendait traiter d'égal à égal avec ce dernier).

Les rois comme les empereurs sont des monarques. Il n'y a "a priori" pas de règle établie pour les distinguer. Tout juste notera-t-on que le rang d'empereur peut être supérieur à celui de roi, notamment s'il a autorité sur d'autres rois, alors que l'inverse semblerait étrange. Ainsi au sein de l'Empire allemand, entre 1870 et 1918 où l'empereur régnait sur des États organisés sous forme de royaumes tel le royaume de Bavière. De même en France sous l'Empire, Napoléon régnait au-dessus des rois qu'il avait placés dans les États satellites de l'empire (royaumes d'Italie, d'Espagne).

Actuellement, seul le Japon est sous le règne d'un tel souverain, l’Empereur du Japon.

Toutefois, plusieurs autres pays furent autrefois dirigés par des empereurs pour des périodes plus ou moins longues :


</doc>
<doc id="1015" url="https://fr.wikipedia.org/wiki?curid=1015" title="Empereur du Japon">
Empereur du Japon

L' est le chef de l'État japonais "de facto". Selon la Constitution promulguée en 1947 lors de l'occupation ayant suivi la Seconde Guerre mondiale, il a en réalité un rôle uniquement symbolique et détient sa fonction du peuple japonais.

L'empereur actuel, Akihito, est le dernier d’une succession que la tradition présente comme ininterrompue et que la légende fait commencer en 660 av. J.C. avec l’empereur Jinmu, qui descend de la déesse du Soleil Amaterasu, elle-même fille des dieux démiurges créateurs du monde terrestre (l'archipel japonais) : Izanagi et Izanami.

Pour les partisans du culte impérial, dont l'influence fut prédominante lors de l'expansion de l'ère Shōwa, l'empereur avait un statut divin, symbolisé par les insignes impériaux. Ce statut a fait l'objet d'une remise en question lors de l'occupation du pays par les forces américaines, ces dernières obligeant en conséquence Hirohito à renoncer officiellement, en janvier 1946, à sa nature de « divinité incarnée » ("akitsumikami") sans toutefois renoncer à son ascendance divine.

La liste officielle actuelle comprend 125 souverains (dont Akihito), parmi lesquels on trouve 8 impératrices (dont 2 régnèrent sous deux noms différents).

Le pouvoir impérial a souvent été usurpé de fait par des chefs de familles puissantes, dont les mieux connus sont les shoguns. Pour échapper aux pressions et conserver leur pouvoir, certains empereurs « retirés » affectèrent de laisser le trône à des membres de leur famille, tout en exerçant leur contrôle depuis les coulisses.

Après avoir résidé plusieurs siècles au Kyōto-gosho à Kyoto, les empereurs se sont installés au milieu du dans l’ancien château d'Edo (Tokyo), devenu Palais impérial : ou . L', située sur le domaine du Kōkyo, gère presque tout ce qui concerne l’empereur et sa famille : service du palais et menus, santé, sécurité, déplacements et emploi du temps officiel.

Jusqu’au milieu du , le titre du souverain japonais était ou . Les textes chinois le nommaient alors « Roi des Wa » ( : "waō", "wakokuō", "daiwaō").

Il a existé en japonais plusieurs appellations respectueuses pour l’empereur, employées à diverses époques et dans différentes circonstances (par les ministres, par l’empereur lui-même, lors des cérémonies religieuses etc.), mais beaucoup n’existaient que sous forme écrite et se lisaient toutes "Sumemima no mikoto" ou "Sumera mikoto", « sublime souverain qui règne au-dessus des nuages ». , littéralement « sublime porte », désignant à l'origine le palais impérial, fut adopté par métonymie pour désigner la fonction impériale (comme "l'Élysée" désigne la fonction présidentielle en France) aux époques Heian et Edo.

L’appellation la plus usitée de nos jours est "Tennō" (天皇), « empereur céleste ». Elle apparaît au Japon au sous le règne de l’empereur Tenji (r.661-672) ou Temmu (r.672-686). On pense généralement que les souverains japonais se sont inspirés de leur homologue chinois Tang Gaozong (628-683), qui s’était paré de cette appellation à l’origine réservée à des dieux du taoïsme, religion officielle de la famille impériale chinoise. Certains, cependant, pensent que le terme est d’origine japonaise et reflète l’origine divine des empereurs. Il est par la suite utilisé en alternance avec , titre habituel des empereurs de Chine depuis Qin Shihuangdi (prononcé "Shikōtei", en japonais, soit le Premier Empereur de Chine), préféré dans les documents diplomatiques. Désigné comme appellation officielle par la constitution Meiji, "Tennō" ne remplace entièrement "Kōtei" dans les documents officiels qu’à partir de 1936.

L'empereur régnant est généralement appelé ou . Les empereurs défunts sont nommés du nom de leur ère : pour Hirohito ; après son décès, Akihito sera connu comme l'« empereur Heisei » ou . Le terme , littéralement « empereur-roi », est utilisé pour les empereurs étrangers.

Il n'y a pas de documents précis antérieurs aux deux livres historiques, "Kojiki" et "Nihon Shoki", achevés en 712 et en 720 qui exposent la fondation mythologique du Japon et l'origine du premier empereur, Jinmu (660 ). Selon ces sources, "Jinmu" descend de la déesse japonaise "Amaterasu" (divinité du Soleil), les deux étant séparés par cinq générations. Les expressions « demi-dieu » et « dieu vivant » pour désigner l'empereur japonais y trouvent leur origine.

Depuis le premier empereur Jinmu (660 ) la même famille impériale règne sur le Japon (« lignée impériale » japonaise ou "kōtō"). La succession au trône impérial est régie par le principe agnatique : quel que soit l'empereur considéré, la règle la plus importante est que si l'on remonte sa généalogie par la ligne paternelle (son père, puis le père de son père, etc.) on arrive nécessairement au premier empereur Jinmu. Pour cette raison, tous les empereurs japonais sont en ligne « directe » avec l'empereur primordial selon un principe unique et inviolé, d'après la mythologie, depuis plus de 26 siècles. Il est impossible de prouver historiquement la véracité de ce récit sur l'ensemble de la période, l'époque pré-Jinmu et les premiers empereurs relevant du domaine de la légende, mais la force de ce principe n'en est pas diminuée, car celui-ci ne souffre d'aucune exception démontrée pour la période historique connue (plus de 15 siècles).

La « lignée impériale » ne se réduit donc pas à la série (liste) des empereurs ayant régné, faisant de la monarchie japonaise un objet tout à fait unique, non seulement dans le contexte culturel japonais, mais plus généralement en comparaison notamment avec les monarchies européennes, et ce pour au moins trois raisons principales. Primo, le lien mythologique avec la création du Japon, comme territoire (création par le père d'Amaterasu, le dieu Izanagi et sa compagne Izanami) puis comme empire (Jinmu, descendant d'Amaterasu). Secundo, l'unicité du principe primordial de succession à travers toutes les époques. Tertio, la remarquable longueur de la lignée (ininterrompue à ce jour). Ce système contribue en particulier à la stabilité de la société, aucune révolution changeant ou abolissant la monarchie n'ayant de fait éclaté au Japon. Par contraste, en Europe, le roi était traditionnellement celui qui dominait un pays, c'est-à-dire le vainqueur de la guerre ; la monarchie et la famille régnante ont subi des changements relativement fréquents. Comme la lignée impériale nipponne est unique, les empereurs n'ont pas de nom de famille.

Dans un monde en perpétuelle évolution, la « lignée impériale » préservée jusqu'à nos jours est donc d'une grande valeur symbolique pour le Japon. La volonté farouche de nombre de Japonais de préserver intacte cette valeur — c'est-à-dire au minimum ne pas changer la règle primordiale de succession — n'a à priori aucun lien avec une question de misogynie. Le plus grand obstacle est en effet la question de la succession de cette impératrice potentielle, car si le principe agnatique est ignoré ne serait-ce qu'une fois, la « lignée impériale » telle qu'elle a toujours été définie s'en trouverait interrompue à jamais. Bien qu'il y ait eu quelques impératrices, au sens de Tennō et non au sens d'épouse de Tennō, cela ne crée pas d'exceptions au principe premier de succession vu ci-dessus, car une exception aurait été créée si et seulement si les fils ou filles de ces impératrices né(e)s de pères extérieurs à la lignée impériale étaient devenus empereurs, ce qui ne fut jamais le cas.

Le rôle impérial suit donc deux axes majeurs : dimension ethnique (incluant l'aspect shintoïste) et dimension constitutionnelle. Tout d'abord, l'empereur symbolise tout autant la nation japonaise (le peuple et sa culture) en incarnant la « lignée impériale » qui fait le lien, grâce à sa constance, entre toutes les époques traversées par cette nation (cf. section détaillée). Il est lui-même le prêtre suprême du shintô (religion qui se fonde notamment sur la mythologie japonaise) et il personnifie ainsi des aspects divins (cf. section détaillée). Il symbolise d'autre part l'État du Japon, au sens de représentant de plus haut niveau, comme le Président de la république italienne ; il s'agit d'un rôle dont les modalités sont définies constitutionnellement (cf. section détaillée).

Certaines dates et des détails de l’histoire des empereurs font l’objet de controverses parmi les historiens japonais. Les 15 premiers souverains (dont une impératrice régente) sont considérés comme légendaires, et d’autres sont morts à un si jeune âge qu’ils peuvent difficilement avoir réellement gouverné. Néanmoins, les dates de règne de la liste complète restent la référence standard pour la détermination des ères de l'histoire japonaise (en japonais -yo).

L’empereur Ojin (r.270-310) serait le premier à avoir eu une existence réelle, mais la famille impériale actuelle remonterait à l’empereur Keitai (r.507-531), probablement fondateur d’une nouvelle dynastie plutôt qu’héritier de ses prédécesseurs. L’autorisation d’explorer partiellement les tumulus impériaux funéraires a été accordée en 2007 par l’Agence de la Famille impériale. Outre le respect dû aux ancêtres impériaux, raison avancée jusqu'ici pour refuser les recherches archéologiques, beaucoup soupçonnent la crainte de découvrir que certains occupants ne sont pas des empereurs, ou que des éléments pointent en direction d’une origine coréenne de la lignée, hypothèse d'ailleurs proposée depuis longtemps et envisagée ouvertement par l’empereur actuel lors d’une déclaration.
Les premiers empereurs historiques, souverains du Yamato, exerçaient leur pouvoir sur un domaine limité (nord de Kyūshū et sud-ouest de Honshū), qui s’est étendu progressivement vers le sud-ouest et le nord-est. Les territoires de Kyūshū et Honshū ne furent totalement dominés qu’au .

L’empereur du Japon subissait généralement la pression des familles alliées, dont les plus importantes furent Soga (530-645), Fujiwara (850-1070), Taira, Minamoto (1192-1331), Ashikaga (1336-1565) et Tokugawa (1603-1867). Certains souverains se retirèrent dans un monastère pour y échapper, continuant d’exercer une forte influence et maintenant leur successeur officiel dans leur dépendance. Ce stratagème n’évitait pas toujours les conflits, comme le montre la rébellion de Hōgen (1156). Néanmoins, la fonction impériale ne fut jamais officiellement usurpée ni remise en cause ; les shogun étaient ainsi officiellement investis par l’empereur. Il semble que cette fonction ait dès l'origine été surtout religieuse et symbolique, nonobstant l’existence d’empereurs forts. Dans leurs descriptions de l’empire du Soleil levant, Portugais et Espagnols comparaient les positions respectives de l’empereur et du shogun à celles du pape et de l’empereur du Saint-Empire.

Jusqu'à l'ère Meiji (1868), le bouddhisme était la foi des empereurs, malgré leurs liens avec le shintô. Du Moyen-Âge à l'époque moderne (1185-1868), on observe un syncrétisme shintô-bouddhisme ("shinbutsu shūgō"), les deux religions devenant indissociables l'une de l'autre. Au début de l'ère Meiji, le bouddhisme et le shintô sont séparés ("shinbutsu bunri"). L'autel bouddhiste de la famille impériale ("okurodo"), situé au sein du palais (alors à Kyoto), est notamment transféré au Sennyū-ji, le temple mortuaire de la maison impériale. Le gouvernement souhaite ainsi rétablir le pouvoir de l'empereur, et promouvoir l'établissement d'une nation axée autour du shintoïsme, le différenciant des autres religions.

Depuis l'époque Meiji, le premier rôle principal de l'empereur du Japon, c'est le prêtre suprême du shintoïsme. Il organise plus de vingt cérémonies religieuses par an dans les temples qui se trouvent au palais impérial. Ces sont "Kashikodokoro" (temple pour Amaterasu, déesse de la famille impériale), "Kōreiden" (temple des empereurs antécédents et des membres défunts de la famille impériale) et "Shinden" (temple pour tous les dieux du Japon). Depuis la fin de la Seconde guerre mondiale et la promulgation de la nouvelle Constitution du Japon, ces cérémonies ont cependant perdu leur caractère officiel, et sont considérées comme des actions privées de la famille impériale.

Les cérémonies annuelles célébrées par l'empereur du Japon sont :

Les cérémonies essentielles comme "Shihōhai" remontent au , époque de Heian. Or, l'empereur du Japon est très religieux et traditionnel comme tête du shintoïsme. Sa vie se base sur le Shintoïsme.

Avec la refonte de la constitution en 1868 sous l'ère Meiji, le shinto devint une religion d'État pour l'empire du Japon : le . L'empereur du Japon, descendant de la déesse Amaterasu et désormais chef de l'État et commandant suprême de la Marine et de l'Armée, fut l'objet d'un véritable culte. En 1889, fut établi un sanctuaire dédié à l’empereur Jimmu, le fondateur mythique de la dynastie. Ce sanctuaire porte le nom de .

La restauration de Meiji (1868) mit théoriquement fin au système féodal en plaçant la terre et la population directement sous juridiction impériale, tout en instaurant un régime représentatif. Néanmoins, le Conseil extra-gouvernemental des "genro" « pères du pays », composé de membres de factions ayant soutenu la Restauration, exerça une importante influence dès le règne de l'empereur Meiji, et la mauvaise santé de son successeur l’empereur Taishō permit aux chefs de l’Armée et de la marine impériale japonaise d'entreprendre une prise en main du pouvoir.

Le rôle de l'empereur devint toutefois prédominant sous l'ère Showa, et notamment lors de la constitution du quartier général impérial en 1937. Déjà chef de l'État et « commandant suprême de l'Armée et de la Marine » en vertu de la constitution, Hirohito devint le commandant d'une structure militaire indépendante du gouvernement et du conseil des ministres et composée essentiellement des représentants de l'Armée et de la Marine.

Le "Kokka shinto" prit une importance primordiale lors de l'expansionnisme du Japon durant l'ère Showa. En tant que Commandant officiel du quartier général impérial à compter de 1937, l'empereur Shōwa était considéré comme la pierre d'assise du , la « réunion des huit coins du monde sous un seul toit ». Il fut ainsi instrumentalisé pour justifier l'expansionnisme et la militarisation auprès de la population japonaise. La manifestation tangible qui faisait de l'empereur le représentant des dieux était les insignes impériaux.

Parmi les partisans les plus notables de cette doctrine, on compte le prince Kotohito Kan'in, chef d'état-major de l'armée impériale japonaise et le Premier ministre Kuniaki Koiso.

En 1945, le commandant suprême des forces alliées imposa une révision de la constitution, abolissant par le fait même les pouvoirs de l'empereur et le Kokka shinto.

Par la constitution de 1889, l'empereur avait déjà transféré une grande partie de ses anciens pouvoirs de monarque absolu aux représentants du peuple. Son rôle actuel est défini dans le chapitre I de la Constitution de 1946 : l'article premier le définit comme le symbole de l'État et de l’unité du peuple japonais ; l'article 3 dispose que pour toutes ses actions concernant les affaires d’État, l’autorisation du cabinet est nécessaire ; l’article 4 précise qu’il n’est pas compétent en matière de gouvernement ; l’article 6 lui donne le pouvoir d’accréditer le Premier ministre et le chef de la cour suprême (nommés respectivement par la diète et le cabinet) ; l’article 7 lui donne le pouvoir d’agir en chef de l'État avec l'approbation du cabinet. Contrairement à la plupart des monarchies constitutionnelles, l'empereur du Japon n'a donc aucun pouvoir réservé mais se retrouve dans une situation proche de celle du roi de Suède. Il remplit la plupart des rôles d'un chef d'État et est reconnu comme tel par les puissances étrangères (les accréditations diplomatiques lui sont présentées par les ambassadeurs étrangers par exemple). Il existe au Japon une controverse récurrente concernant la façon dont l'empereur doit être envisagé : chef de l'État, ou personne agissant comme chef de l'État.

Des tentatives des forces conservatrices dans les années cinquante pour amender la constitution afin de désigner clairement l'empereur comme chef de l'État furent rejetées. La restauration du statut de chef d'État de droit divin figure parmi les objectifs clairement affirmés par Nippon Kaigi, le principal lobby révisionniste japonais.

Les souverains précédant l’empereur Taishō Tennō (1912-1926) avaient plusieurs épouses et concubines d’origine noble, dont en principe une (ou plus rarement deux) impératrice en titre. Le choix de ces femmes ainsi que leur rang étaient déterminés selon leur famille de naissance. Il semble qu’à l’origine les impératrices provenaient du clan impérial lui-même. Par la suite, elles furent le plus souvent choisies dans le clan allié le plus puissant, qui fut tout d'abord les Soga aux . Le relais fut pris au début du (empereur Shomu) par les Fujiwara. L'habitude de choisir l'impératrice dans le clan impérial ou le principal clan allié faisait qu'une relation consanguine existait entre les conjoints impériaux, très rapprochée parfois, surtout dans les premiers siècles (demi-frère et sœur ou oncle et nièce). Le beau-père de l’empereur, qui était souvent son oncle maternel, exerçait un pouvoir important. Les Fujiwara, en particulier, s’attribuèrent de façon héréditaire les positions de régents (sessho et kampaku) et dominèrent la politique durant la période Heian (794-1185). Fujiwara no Michinaga (966-1027), pour assurer son pouvoir, fit créer une deuxième position d’impératrice : pour sa fille Shosi, égale à la position de détenue par Teishi, fille de son frère aîné Fujiwara no Michitaka. Même après l’ascension des shoguns Minamoto, Taira et Ashikaga, les cinq branches principales du clan Fujiwara (Ichijo, Kujo, Nijo, Konoe et Takatsukasa) continuèrent de fournir l'essentiel des impératrices. Ce fait fut entériné officiellement lors de la restauration de Meiji (1889) ; les filles des cinq grandes branches Fujiwara et du clan impérial furent désignées comme les seules aptes à accéder au statut d'impératrice. La dernière impératrice Fujiwara fut Teimei, épouse de Taisho. L’impératrice Kojun, femme de Hirohito, venait du clan impérial ; son fils Akihito fut le premier à épouser une femme qui ne venait pas de la noblesse (impératrice Michiko).

Les impératrices régnantes ou régentes étaient en général mises en place par la principale famille alliée pour protéger ses intérêts en l’absence d’un héritier mâle lié au clan, ou en cas de conflit insoluble entre deux prétendants. Durant leur règne, elles restèrent célibataires, à moins qu’elles ne soient arrivées ou revenues au pouvoir déjà veuves. La question du choix d’un empereur consort ne s’est donc jamais posée.

Selon l’historiographie traditionnelle, le titre d’empereur du Japon est toujours resté dans le même clan patrilinéaire (lignée Yamato) depuis les débuts légendaires de la dynastie au . Même si ce n’est pas la réalité, il est en tout cas vraisemblable que depuis le premier empereur historique (fin du ), les souverains successifs ont maintenu entre eux d’authentiques liens de consanguinité, d'autant plus que les épouses et concubines impériales étaient généralement issues d’un nombre limité de familles ; même Keitai (450-531), qui semble être venu d’un clan différent de celui de ses prédécesseurs, leur était apparenté par les femmes.

Le Trône du chrysanthème se transmettait selon le principe patrilinéaire, mais avec une certaine souplesse. Contrairement à la monarchie française, aucun ordre rigoureux de succession ne semble avoir été imposé, la transmission pouvant se faire de frère à frère aussi bien que de père à fils, avec dans ce dernier cas priorité aux fils de l’impératrice en titre, mais également possibilité d’adopter le fils d’un autre membre masculin de la famille. À l’époque de l’empereur Go-Saga (1220-1272), une alternance de la fonction impériale fut instaurée entre deux branches collatérales issues de deux princes impériaux. Le système finit mal, donnant lieu à l’apparition de deux empereurs rivaux, un du nord et un du sud. À partir du , la transmission du trône au fils aîné est devenu le mode le plus habituel, sans pour autant être une obligation officielle. Il était également possible à une princesse impériale de monter sur le trône, mais pas de le transmettre, c’est pourquoi les impératrices régnantes furent en général nommées en attente d’un candidat masculin valable et restèrent célibataires, à moins qu’elles ne soient déjà veuves. De nombreux empereurs abdiquèrent après quelque dix années de règne, soit pour diriger dans les coulisses, soit pour jouir d’une retraite confortable. La fonction impériale, à l’origine fortement religieuse, avait des aspects rituels très contraignants peu favorables à l’exercice effectif du pouvoir.

L’article 2 de la constitution de 1889 interdit formellement que le trône soit occupé par une femme. La loi sur la famille impériale précisa que les fils de l’empereur prenaient le pas sur ses frères et ses neveux dans la succession ; si l’empereur n’avait pas de fils, le trône passait à la branche collatérale la plus proche. L’empereur était autorisé à prendre une ou plusieurs concubines si nécessaire, ce que fera d’ailleurs l’empereur Meiji, l’impératrice étant stérile. En 1947, l’interdiction des femmes sur le trône fut maintenue et la taille de la famille impériale réduite aux descendants de l’empereur Taisho. Seuls les fils biologiques légitimes peuvent hériter du trône, excluant le recours aux adoptions et aux concubines.

À partir de la naissance en 2001 de la princesse Aiko, fille de l’actuel prince héritier Naruhito, un débat s’est élevé au Japon concernant la pertinence de soumettre à la Diète une proposition de révision des lois de succession visant à autoriser l’accession des femmes au Trône du chrysanthème. La réduction drastique du nombre de branches collatérales autorisées à hériter, associée à la stricte monogamie, peut en effet mener à une totale absence d’héritier mâle. Ainsi, le frère cadet de Naruhito, Akishino, avait à l’époque deux filles ; les trois autres héritiers possibles, frère ou cousins de Akihito, déjà quinqua ou sexagénaires, étaient aussi sans descendance mâle. En janvier 2005, le premier ministre Jun'ichirō Koizumi mit en place une commission composée de juges, d'universitaires et de cadres de l'administration pour étudier les changements possibles aux règles de succession et proposer des recommandations à cet égard. Le 25 octobre 2005, l’ouverture du trône aux femmes fut recommandée et en janvier 2006, Koizumi promit d’entreprendre un changement législatif, mais la venue au monde cette même année du prince Hisahito a suspendu ce projet, que le premier ministre Shinzō Abe a déclaré officiellement abandonné en janvier 2007.

Tous les empereurs et impératrices, depuis l'empereur Taishō en 1926, ont été inhumés dans un vaste complexe funéraire situé dans la ville d'Hachiōji à l'ouest de Tokyo, et généralement appelé depuis 1990. Le terme de renvoie à l'ancienne Province de Musashi, qui comprenait, entre autres, l'actuel territoire de la préfecture de Tokyo.

Cette nécropole était à l'origine, à la suite de sa création en 1927, et jusqu'en 1990, désignée sous le terme de , la ville d'Hachiōji faisant partie de la zone géographique appelée aire de Tama qui consiste en la partie occidentale de la préfecture de Tokyo, non comprise donc dans les 23 arrondissements tokyoïte. Il comprend :



</doc>
<doc id="1018" url="https://fr.wikipedia.org/wiki?curid=1018" title="Espagnol">
Espagnol

L’espagnol (en espagnol "español"), ou le castillan, est une langue romane parlée en Espagne et dans de nombreux pays d'Amérique et d'autres territoires dans le monde associés à un moment de leur histoire à l'Empire espagnol. 

La langue espagnole, issue du latin vulgaire parlé autour de la région cantabrique, au nord de la péninsule Ibérique, s'est diffusée en suivant l’extension du Royaume de Castille et fut menée en Afrique, aux Amériques et en Asie Pacifique avec l'expansion de l'empire espagnol entre le , circonstances historiques qui en font la langue romane la plus parlée dans le monde actuellement.

L'espagnol est maintenant la langue maternelle d'environ 468 millions de personnes et est utilisé par près de 559 millions de personnes, ce qui la hisse au troisième rang mondial pour le nombre de locuteurs, derrière le chinois mandarin et l'anglais, et au deuxième rang pour le nombre de locuteurs de naissance. L'espagnol est l'une des principales langues de communication internationale, avec l'anglais ou le français.

L'espagnol est de façon générale resté nettement archaïsant et demeure ainsi relativement proche du latin classique et de l'italien moderne, avec lequel il maintient un certain degré d'intercompréhension. Appartenant à la sous-branche ibéro-romane comme le portugais, l’espagnol permet également une certaine intercompréhension écrite, et dans une moindre mesure orale, avec celui-ci. L’espagnol est morphologiquement proche du français, du fait de leur origine latine commune, mais l'intercompréhension reste toutefois très limitée, bien que facilitée à l'écrit par le caractère archaïsant de l'orthographe française.

D'autre part, l'espagnol étant originaire de la région de Cantabrie, dans le nord de l'Espagne, il a reçu une forte influence du substrat formé par l'ancêtre du basque, en particulier au niveau morphologique, ainsi que, dans une moindre mesure, lexical.

L'espagnol partage avec les autres langues romanes la plupart des évolutions phonologiques et grammaticales caractéristiques du latin vulgaire, telles que l'abandon de la quantité vocalique, la perte des déclinaisons et la disparition des verbes déponents.

Les principales évolutions qui caractérisent l’espagnol sont :

En comparaison aux autres langues romanes, l'espagnol possède une typologie syntaxique particulièrement libre et avec des restrictions bien moindres concernant l'ordre des mots dans les phrases (typiquement : sujet-verbe-complément).

Un des traits syntaxiques caractéristiques de l'espagnol est l'ajout d'une préposition « a » devant les compléments d'objet renvoyant à une personne ou un être animé. Il peut être considéré comme une conséquence de la liberté syntaxique précédemment évoquée, le fait de pouvoir intervertir facilement les groupes syntaxiques dans une phrase entraînant possiblement une confusion entre sujet et objet, évitée grâce à l’emploi de la préposition. Ce trait concourt à une confusion que l'on rencontre dans l'usage des pronoms compléments directs et indirects (phénomènes qualifiés de Leísmo, laísmo et loísmo, le premier étant considéré comme correct dans certains cas d'un point de vue académique).

L'espagnol fait fréquemment usage d'un pronom complément indirect redondant en cas de présence du groupe nominal référent : "le digo a Carmen" : « je dis à Carmen » (littéralement : « je lui dis à Carmen »), et même "se lo digo a Carmen" (« je le lui dis à Carmen »).

Comme en latin et dans la plupart des autres langues romanes, et à la différence du français, l'usage des pronoms sujets est facultatif. Il n'est utilisé que pour lever une confusion dans certains cas de conjugaisons ou pour insister sur le sujet : "yo sabía la lección" (« je savais la leçon ») face à "ella sabía la lección" (« elle savait la leçon ») ou bien "trabajo muy bien" (« je travaille très bien ») opposé à "yo trabajo muy bien (tú no)" « moi, je travaille très bien (pas toi) ». 

De façon générale, le système de conjugaison de l'espagnol est resté morphologiquement très proche du latin.

Les quatre conjugaisons latines sont réduites à trois en espagnol. Les infinitifs latins en -ĀRE, -ĒRE et -ĪRE deviennent respectivement en espagnol "-ar", "-er" et "-ir" ; la troisième conjugaison latine, en -ĔRE, est redistribuée entre les deuxième et troisième conjugaisons de l'espagnol, "-er" et "-ir" (ex. : FACĔRE > "hacer", DICĔRE > "decir").

L'espagnol conserve avec une grande vitalité son passé simple, issu du parfait latin, qui tend à être remplacé par des formes analytiques dans d'autres langues romanes.

Comme dans d’autres langues romanes, on observe en espagnol une auxiliarisation du verbe "haber" (« avoir, posséder »). Celui-ci va permettre de construire les temps composés (suivis du participe-passé des verbes conjugués, qui reste toujours invariable en espagnol) mais aussi les nouveaux paradigmes du futur de l'indicatif (infinitif + "haber") pour remplacer le paradigme latin (CANTABO…) tombé en désuétude. Le conditionnel est construit par analogie, en utilisant l'auxiliaire simplifié à l'imparfait. "haber" a fini par perdre son sens original de « avoir, posséder » au profit de "tener", pour n’être plus qu’un verbe auxiliaire. Il conserve encore sa valeur sémantique d’origine dans certaines expressions lexicalisées, en particulier "haber de" + infinitif pour signifier une obligation et la forme "hay "< "ha allí" (« il y a ») ainsi que ses variantes dans les différents temps et modes ("había", "habrá", "hubo", etc. et même "ha habido" au passé-composé).

L'espagnol dispose actuellement de deux paradigmes de conjugaison pour le subjonctif imparfait, issus du plus-que-parfait latin, indicatif pour les formes en "-ra" (AMAVERAM>"amara"), et subjonctif pour les formes en "-se" (AMAVISSEM>"amase"). Bien que tous deux soient également admis sur un plan académique, le premier tend à se substituer au second, surtout dans le langage oral. De plus, il conserve encore dans certains usages, en particulier littéraires, sa valeur originelle d'indicatif plus-que-parfait, et est utilisé comme deuxième variante (libre) du conditionnel présent pour quelques verbes ("querer" > "quisiera "~ "querría" ; "deber" > "debiera "~ "debería" ; "haber" > "hubiera "~ "habría" ; "poder" > "pudiera "~ "podría"). De même, le subjonctif plus-que-parfait peut remplacer le conditionnel passé pour exprimer l'irréel du passé.

Les pronoms personnels compléments sont placés en enclise, c’est-à-dire collés immédiatement après le verbe, lorsque le verbe est à l’infinitif ("llamarse", « s’appeler » ; "dejarme", « me laisser », etc.), au gérondif ("mirándome", « en train de me regarder / me regardant ») ou l’impératif (comme en français : "mírame", « regarde-moi » ; et comme en français le pronom redevient proclitique si l’impératif est négatif : "no me mires", « ne me regarde pas »). Les pronoms sont susceptibles de se combiner, le pronom indirect se place alors en premier : "déjamelo", « laisse-le-moi ». Dans une combinaison, le pronom indirect de troisième personne devient "se" (habituellement pronom réfléchi) et non "le" : "díselo", « dis-le-lui ». Lorsque la forme verbale portant le pronom est associée à un semi-auxiliaire, on a la possibilité de rattacher le ou les pronoms à celui-ci en position proclitique : "está levantándose "~ "se está levantando" (« il est en train de se lever ») ; "¿Quieres callarte?"~"¿Te quieres callar?" (« Veux-tu te taire ? ») ; "suele decirme la verdad "~ "me suele decir la verdad" (« il me dit habituellement la vérité »). La construction enclitique est perçue comme légèrement plus soutenue. Les cas d’enclises de pronoms étaient beaucoup plus nombreux en ancien espagnol ("direvos", « je vous dirai », "os diré" en espagnol moderne, etc.) ; certains sont préservés dans des locutions figées. On trouve un phénomène analogue en portugais, en catalan, en occitan aranais, ainsi que, partiellement, en italien.

Jusqu'au , l'espagnol a maintenu un subjonctif futur en "-re" (à valeur fortement hypothétique), issu d'une fusion des paradigmes du subjonctif parfait et du futur antérieur (remplacé par la forme composée en utilisant "haber" au futur). Cette forme a pratiquement disparu de l’espagnol actuel et ne persiste que dans des expressions lexicalisées, des proverbes et certaines formules juridiques.

En raison de ses contacts prolongés avec d'autres langues, le lexique de l'Espagnol comporte bon nombre de mots issus d'emprunts, notamment aux langues paléo-hispaniques (ibère, hispano-celtique), au basque, à l'arabe et à différentes langues amérindiennes.

L'ibère se parlait tout au long de la côte orientale de la péninsule. Le fonds ibère consiste principalement en éléments géographiques et zoologiques, comprenant pour l'essentiel : "ardilla" « écureuil », "arroyo" « ruisseau », "balsa" « étang », "calabaza" « potiron » ("cf". catalan "carabassa"), "cama" « lit », "conejo" « lapin » (du latin "cuniculus"), "cuérrago" « lit de fleuve », "galápago" « tortue de mer » ("cf". cat. "calapèt" « crapaud »), "garma" « éboulis », "gazapo" « lapereau » ("cf". portugais "caçapo"), "gusano" ~ "gusarapo" « ver », "manteca" « saindoux », "maraña" « fourré », "marueco" ~ "morueco" « bélier » ("cf". cat. "marrà", "mardà"), "parra" « pied de vigne », "perro" « chien », autrefois « corniaud », "rebeco" « chamois, isard » (du latin "ibex", emprunté à l'ibère), "sima" « gouffre, abîme », "tamo" « menue paille ».

L'hispano-celtique regroupe plusieurs variétés, dont le gallaïque (au nord-ouest), le celtibère et le gaulois tardif (au nord-est). Le fonds celtique concerne notamment la botanique, la faune, le labourage et d'une moindre mesure l'artisanat. Au celtique remontent : "álamo" « peuplier blanc », "ambuesta" « poignée », "amelga" « champ défriché », "beleño" « jusquiame », "berro" « cresson » ("cf". français "berle"), "bezo" « babine », "bodollo" « faucille » ("cf". fr. "vouge"), "breca" « pandore (mollusque) » ("cf". poitevin "brèche" « vache bigarrée »), "brezo" « bruyère », "bruja" « sorcière », "cam(b)a" « chambige », "combleza" « maîtresse (d'un homme marié) », "corro" « cercle », "cresa" « asticot », "cueto" « butte, petite colline », "duerna" « pétrin », "galga" « galet », "gancho" « crochet », "garza" « héron », "greña" « enchevêtrement », "mocho" « bouc ou bélier châtré » ("cf". fr. "mouton"), "rodaballo" « turbot », "sábalo" « alose », "sel" « pâturage commun », "serna" « champ labouré », "taladro" « tarière », "terco" « têtu », "varga" « chaumière », "yezgo" « hièble ».

Le basque, adstrat du castillan, l'a aussi influencé, et ce dès sa naissance. Certains mots, comme (1) "izquierda" « le gauche », du basque "ezkerra" ("cf". cat. "esquerre", port. "esquerda"), (2) "madroño" « arbousier » ("cf". arag. "martuel", cat. "maduixa"), correspondant au basque "martotx" « ronce » et "martuts" « mûre », et (3) "zarza" « ronce » ("cf". port. "sarça"), qui provient du basque anc. "çarzi" (auj. "sasi"), ont eu du succès en évinçant le vieil espagnol "siniestro" « gauche » (aujourd'hui « sinistre »), "alborço" « fraisier » et "rubo" « ronce ». Certains d'entre eux ne semblent pas avoir connu de concurrent, comme "vega" « plaine fertile riveraine » (v.esp. "vayca", "vajka"), qui répond au basque "ibai" « fleuve », ou "sapo" « crapaud », du basque "zapo", face à son équivalent latin "escuerzo", et enfin d'autres, comme "muérdago" « gui » (du basque "mihura") et "cachorro" « chiot » (du "txakur", « chien») ont fait glisser de sens leurs anciens synonymes ("visco" « glu (à base de gui) », "cadillo" « caucalis »). D'autres encore sont de date récente, comme "zorra" « renard », emprunté au portugais et substantivisé à partir d'un "zorro" « oisif », lui-même tiré du basque "zuur" ~ "zur" ~ "zuhur" « prudent ». Ce mot est toujours concurrencé par le sobriquet "raposa", « la touffue », plus ancien : synonymie recherchée parce que le renard fait l'objet d'un tabou lexical.

Quelques vêtements sont passés du basque à l'espagnol, comme "chapela" (< "txapel", « béret basque »), face à "boina" (« béret »), "chamarra" (< "zamar" ou "txamar", "zamarra" ou "txamarra" avec l'article défini singulier, « veste »), des activités comme "pelotari" (« joueur de pelote basque »), "chistu" (< "txistu", « flûte basque »), "chalaparta" (< "txalaparta", instrument de percussion), "aquelarre" (« sabbat, de "akelarre", lui-même formé à partir de "aker" « bouc » + "larre" « pré », car ces rites, soi-disant présidés par Satan lui-même, sous la forme d'un bouc, avaient lieu dans des prés) et le nom de la langue basque, "euskera", "eusquera" ou "euskara" (< "euskara"), face à "vasco" ou le plutôt vieilli "vascuence". 

Plus récemment, des emprunts ayant rapport au contexte politique, comme "zulo" (« cache d'armes », du mot "zulo", « trou »), "kale borroka" (« guerrilla urbaine », de "kale", « rue » et "borroka" « combat »), "ikurriña" (d"'ikurrina", « drapeau basque »), "gudari" (de "gudari", « soldat », surtout pendant la Guerre civile espagnole) ou "abertzale" (« nationaliste basque ») sont devenus courants dans les médias espagnols. L'espagnol régional du Pays basque possède évidemment davantage d'emprunts, tels que "sirimiri" (« bruine, crachin», face à "llovizna"), "chirristra" (« toboggan», du basque "txirrista", face à "tobogán") ou bien "aita" (« père») et "ama" (« mère »), face à "papá" et "mamá" ; la gastronomie a également fourni des mots, tels que "marmitaco" ou "marmitako" (plat préparé par les pêcheurs avec du thon et des pommes de terre, du basque "marmitako"), "cocochas" (de "kokots", "kokotsa" avec l'article défini singulier, « barbillon, menton »), "chacolí" (du substantif "txakolin", sorte de vin blanc) ou "chistorra" (de "zistorra" et "txistorra", saucisson fin). 

À noter également le nom "órdago", de la phrase basque "Hor dago" ([il] est là), « renvi », à l'origine utilisée dans un jeu de cartes et qui veut dire aussi « épatant » dans l'expression "de órdago".

Environ mots espagnols dérivent du gotique, une langue germanique orientale qui fut parlée par les Wisigoths, un peuple qui domina une grande partie de la péninsule Ibérique du . Quelques mots d'origine francique ont également pénétré l'espagnol par le biais du français.


Héritage de l'époque musulmane, l'arabe apporta un grand nombre de mots à l’espagnol (plus de ).



Le caló (langue mixte issue de l'espagnol et du romaní, langue des Roms proche des langues indiennes, comme l'hindi, dont de nombreux mots sont similaires : "pani", « eau », etc.) a apporté un grand nombre de termes d’argot comme "gachó" « mec », "bato" « père », "biruji" « vent très froid », "camelar" « aimer », "chaval,a" « jeune », "currar" « bosser », "fetén" « excellent », "parné" « fric », "sobar" « pioncer », "pinrel" « panard », "pureta" « vieux, ancien », "chorar" « chaparder » ("cf". fr. "chourer"), "terne" « fort, robuste », "diñar" « donner », "mangue" « moi », "pañí" « eau », "chingar" « piquer, voler», "lacha" « honte », "pirarse" « s'en aller », "canguelo" « peur », "chachi" « super », "chanelar" « comprendre, piger », "chungo,a" « difficile », "jiñar" « caguer », "mangar" « piquer, voler», "clisos" « yeux », "jalar" « bouffer ».

Comme les autres langues romanes, l'espagnol a adopté l'alphabet latin et recourt à des diacritiques et des digrammes pour le compléter. Les accents écrits, utilisés en espagnol moderne pour marquer la voyelle tonique dans certains cas, ou pour distinguer certains homonymes, ont été utilisés de façon spontanée jusqu'à la standardisation de leur usage à la création de l'Académie royale espagnole au . De plus, le "u" porte un tréma dans de rares occasions, à savoir dans les suites "güe" et "güi" pour indiquer que le "u" se prononce (par exemple : "bilingüe", « bilingue »).

Le tilde est peut-être le plus célèbre des diacritiques espagnols ; il donne naissance à un caractère considéré comme une lettre à part entière, "ñ". Il s'agit à l'origine d'un digramme "NN", le second "N" ayant été abrégé par suspension au moyen d'un trait devenu ondulé, ~.

Ce sont les scribes espagnols qui ont inventé la cédille (, « petit "z" »), qui n'est cependant plus utilisée depuis le (le "ç", qui se notait [ts], est devenu un [θ] interdental noté "z" : "lança" est devenu , « lance », ou "c" devant "e" et "i" : , « aveugle »).

Les points d'exclamation et d'interrogation sont accompagnés par des signes du même type inversés, "¡" et "¿", placés au début de la proposition concernée (et non au début de la phrase) : (« Comment vas-tu ? »), (« Comme c'est étrange ! ») mais (« Si tu vas à Séville, tu m'achèteras un éventail ? »).

De façon générale, l'espagnol, à l'écrit, est une transcription proche de l'oral ; toutes les lettres doivent être prononcées, à de rares exceptions près (la plupart des "h" et le "u" des syllabes "gue", "gui", "que" et "qui"). L'apprentissage de la langue s'en trouve ainsi dans une certaine mesure facilité, autant pour les hispanophones de naissance comme pour ceux désireux d'apprendre la langue comme seconde langue. Les combinaisons de lettres "ph," "rh" et "th" ainsi que "ch" provenant du "kh" grec ne sont pas utilisées, et seules les consonnes "c", "r", "l" et "n" peuvent être doublées. Le "rr", comme "r" en début de mot, transcrit une consonne roulée alvéolaire voisée tandis que "ll" transcrit une consonne centrale liquide. La combinaison "qu" rend le son "k" devant "e" et "i" (comme habituellement en français).
Traditionnellement, "ch" et "ll" ont été considérées comme lettres à part entière et pour cette raison, dans le dictionnaire, elles se trouvaient classées en conséquence (par exemple : "camisa, claro, charla", ou "liar, luna, llama"). Les dictionnaires ont cependant, peu à peu, abandonné cette pratique et recourent à un classement alphabétique classique (comme en français). Cette situation a été régularisée par l'Académie royale espagnole dans une réforme orthographique publiée en 2010, qui stipule que "ch" et "ll" ne doivent plus être considérées comme des graphèmes indépendants mais seulement comme une combinaison de deux graphèmes.

Parmi les modalités les plus remarquables du castillan parlé en Espagne, on peut citer l'andalou (notamment caractérisé par la présence de seseo ou de ceceo selon les zones), le murcien, le castúo et le canarien.

On distingue cinq aires de variation topolectale de l’espagnol en Amérique :

Parmi les plus remarquables, on peut citer l'utilisation d'un autre système de pronoms personnels. Le pronom de la troisième personne du pluriel (qui sert en Espagne uniquement à s'adresser à un ensemble de personnes que l'on vouvoie) remplace en Amérique hispanique le . Ce pronom, ( personne du pluriel) équivaut en Espagne au « vous » français lorsque l'on s'adresse à un ensemble de personnes que l'on tutoie.

La prononciation ibérique de la consonne "c" (devant les lettres "e" et "i") ou "z" (devant "a", "o" et "u"), est une spirante interdentale (proche du "th" anglais dans le verbe "think", pas dans l'article "the"). En Amérique latine, le phonème se prononce presque toujours (phonétiquement proche du "s" français, le "s" péninsulaire étant plus palatal), un important trait commun avec le canarien et une grande partie de l'andalou. Cette prononciation, appelée en espagnol , est généralisée en Amérique hispanique.

Les traits communs avec le dialectes andalou et canarien s'expliquent par le fait que la colonisation de l'Amérique hispanique et tous les échanges commerciaux avec celle-ci ont pendant longtemps été centralisés à Séville (les îles Canaries servant alors d'intermédiaire avec la Péninsule Ibérique), ce qui permettait un meilleur contrôle des flux par la monarchie. Ceci explique que le dialecte andalou ait été dominant chez les migrants qui y passaient souvent de longs mois avant de pouvoir embarquer pour le Nouveau Monde.

Les nombreux esclaves africains déportés dans l'empire espagnol ont également influencé certaines différenciations des parlers d'Amérique et développé une forme d'espagnol particulière au contact des colons, tout en apportant leur accent africain.

De grandes disparités peuvent exister au niveau du lexique. Par exemple certains mots courant dans l'espagnol péninsulaire sont obscènes en Argentine, en Colombie ou au Mexique, tels que "coger" (« prendre » en espagnol européen, mais « avoir des relations sexuelles » dans beaucoup de pays d’Amérique du Sud).

La prononciation pour l'heure peut différer en Espagne et en Amérique. Pour ce qui est de la première moitié du cadran (12 à 6), l'usage est le même. Pour la deuxième moitié (6 à 12), il y a des variations. Par exemple, en Espagne, "Il est »" se dira tandis qu'en Amérique latine on préférera généralement , bien que le paradigme péninsulaire existe et soit quelquefois employé. "« Il est »" et "« Il est »" se disent de la même façon : .

Une conséquence du contact de la langue espagnole avec l'anglais est l'apparition d'un parler appelé « spanglish », qui est employé notamment par des locuteurs aux États-Unis. Cette variante de l'espagnol est étudiée dans certaines universités comme l'Université Amherst du Massachusetts.

En Europe, l'espagnol est langue officielle de l'Espagne (coofficielle suivant les régions autonomes), où elle est parlée par environ 46 millions de locuteurs. Dans le reste de l'Union européenne, on recense près de 29 millions d'hispanophones, en très grande majorité partiels. En Gibraltar il est parlé par 77% de la population (50% comme langue maternelle). En Andorre, il est la langue utilisé par presque 40 % de la population.

En Amérique, l'espagnol est la langue officielle de 19 des 35 pays du continent : Argentine, Bolivie, Chili, Colombie, Costa Rica, Cuba, République dominicaine, Équateur, Guatemala, Honduras, Mexique, Nicaragua, Panama, Paraguay, Pérou, Porto Rico, Salvador, Uruguay et Venezuela. Les populations hispanophones les plus nombreuses se trouvent au Mexique (120 millions), aux États-Unis (53 millions, ce qui représente une proportion d'environ 18 % de la population, avec une densité supérieure à 25 % dans les États frontaliers du Mexique), en Colombie (47 millions), en Argentine (42 millions), au Pérou (31 millions) et au Venezuela (30 millions). Il y a également un million d'hispanophones au Canada et un demi-million au Brésil (où l'apprentissage de la langue est obligatoire à l'école primaire depuis 2005). Il est aussi la langue maternelle de 40% de la population en Belize.

En Afrique, l'espagnol est la langue officielle de la Guinée équatoriale. Il est également parlé dans les régions nord du Maroc et au Sahara occidental, sans oublier les territoires espagnols de Ceuta, Melilla et les îles Canaries.

En Asie, plus de 3 millions de locuteurs existaient aux Philippines, mais aujourd'hui il y en a quelques milliers de moins que le demi-million de locuteurs hispanophones recensés en Australie.

En Oceanie, il est parlée dans le territoire chilien de l'Île de Pâques.

Enfin, une variété du castillan appelée selon les auteurs "ladino", "judesmo", "ispanyol" ou "judéo-espagnol" est parlée par la communauté juive séfarade originaire de la Péninsule ibérique en Israël, Turquie, ou encore Gibraltar.

Les prix Nobel de littérature en langue espagnole :

Le terme « espagnol » est recommandé par l'Académie royale espagnole (), et l'Association des académies de la langue espagnole en tant que dénomination internationale de la langue. Toutefois, cette appellation est peu employée voire rejetée dans des pays où l'espagnol est langue officielle, et où le terme de "castillan" est préféré :

Par ailleurs, la dénomination castillan peut désigner plus précisément :

Le substantif masculin « espagnol » (prononcé : ) est un probable emprunt à l'ancien languedocien ' ou ', issu, par l'intermédiaire du latin vulgaire "*hispaniolus", du latin classique "", de même sens.





</doc>
<doc id="1025" url="https://fr.wikipedia.org/wiki?curid=1025" title="EDF Gaz de France Distribution">
EDF Gaz de France Distribution

EDF Gaz de France Distribution était une ancienne direction commune aux deux entreprises Électricité de France et Gaz de France

Bien que n'ayant pas d'identité juridique propre, et n'ayant pas le statut de filiale, EDF Gaz de France Distribution était reconnue comme une direction mixte à EDF et Gaz de France. Son personnel et ses directions étaient « mixtes » et appartenaient aussi bien au groupe EDF qu'au groupe Gaz de France qui en partageaient la direction

Auparavant « Direction de la Distribution », puis Direction « EDF GDF Services » (DEGS), cette direction renommée en 2004 (« EDF Gaz de France Distribution ») est couramment abrégée en « EGD ».
Elle couvre tout le territoire français, y compris les départements d'Outre-Mer, mais hors territoires d'Outre-Mer et hors communes desservies par des régies locales. Sur ce territoire, elle a deux fonctions principales :

Elle emploie salariés. La production et le transport sont à la charge d'autres directions.

Dans le cadre de l'ouverture du marché de l'énergie, et pour permettre un accès au réseau de distribution non discriminatoire à tous les clients quel que soit le fournisseur choisi, EDF et Gaz de France ont mis en place à partir du 2004 deux gestionnaires de réseaux de distribution séparés, un pour l’électricité (EDF Réseau Distribution) et un pour le gaz (Gaz de France Réseau Distribution). EDF Gaz de France Distribution représentait le service commun de ces deux directions

Jusqu'en juillet 2007, EDF Gaz de France Distribution a assuré, en plus de ses missions de gestionnaire de réseau, le service clientèle (service client, facturation, gestion, vente...) des entreprises EDF et Gaz de France pour la clientèle des particuliers

EDF Gaz de France Distribution était composé de 102 Centres de distribution sur tout le territoire, regroupé en « Groupement de Centres » (Sud-Est, Ouest...etc.) qui assurent le pilotage et la stratégie d'EDF Gaz de France Distribution en région.

Chaque groupement de centre était composé de 4 lignes métier :


Il existait également une Fonction Support Logistique, assurant les fonctions transverses pour les 4 autres portefeuilles métier (RH, Immobilier...).

Depuis le , les activités d'EDF Gaz de France Distribution sont transférées dans deux entités : Électricité Réseau Distribution France (ERDF, devenu Enedis le , filiale du groupe Électricité de France) et Gaz Réseau Distribution France (GRDF, filiale du groupe Gaz de France, devenu GDF Suez en , puis Engie en ) qui forment un service commun de salariés, dédié à la gestion du réseau de distribution d'électricité et de gaz.


</doc>
<doc id="1026" url="https://fr.wikipedia.org/wiki?curid=1026" title="Electronic Entertainment Expo">
Electronic Entertainment Expo

L’, plus connu sous le nom de E3 ou E, est le salon international du jeu vidéo et des loisirs interactifs. Il se déroule aux États-Unis. À la suite d'une refonte du salon, après l'édition 2006, son nom avait été modifié pour devenir l'E3 Media and Business Summit lors des éditions 2007 et 2008.

Il est organisé chaque année par la Entertainment Software Association (ESA). Ce salon annuel, exclusivement réservé aux professionnels du secteur et aux journalistes, se tient au début du mois de juin au Los Angeles Convention Center à Los Angeles. Il s'étale sur trois jours ; des conférences pré-E3 se tiennent les jours précédant le salon, et mettant à l'honneur les constructeurs de machines (Nintendo, Sony et Microsoft) et les éditeurs (EA, Activision, Ubisoft, entre autres). Ces conférences sont généralement diffusées en direct en streaming sur internet ainsi que sur la chaîne de télévision américaine G4 TV, la chaîne française Nolife ou encore Game One qui diffusent et organisent des émissions en direct du salon.

En 1994, le jeu vidéo ne disposait pas de salon lui étant réservé et les professionnels du secteur se rencontraient dans des salons plus généralistes, comme le Consumer Electronics Show (CES), qui se déroulait deux fois par an. Il ouvre ses portes en 1995, débute avec visiteurs. Le syndicat des éditeurs américains, l'Interactive Digital Software Association (IDSA), crée alors son propre salon annuel, réservé aux professionnels ; la première édition de l’Electronic Entertainment Expo (E3) ouvre ses portes en 1995. Depuis 1998, un groupe, indépendant du salon, remet les Game Critics Awards qui récompensent les meilleurs jeux dans différentes catégories.

Les coûts d'expositions toujours croissants poussent les acteurs du secteur à se concentrer sur un nombre limité de salons annuels. Le salon de l'E3 a réussi à devenir incontournable et ce sont donc ses concurrents, le CES de Las Vegas, l'ECTS de Londres et le TGS de Tokyo, qui font les frais de ce recentrement. Malheureusement, l'E3 est victime de son succès et les gros éditeurs (Microsoft, Blizzard Entertainment et sa BlizzCon, etc.) préfèrent organiser leur propre salon de façon à réduire leurs coûts. Face à la possible disparition de cet énorme événement, Doug Lowenstein, président de la Entertainment Software Association, tient une conférence de presse. Il y explique que l'E3 se déroulera encore en 2007 et pour les années à venir, mais à Santa Monica. Mais l'E3 aura désormais les allures d'un salon de jeux vidéo conventionnel, il se déroulera dans plusieurs hôtels de la ville, assez proches les uns des autres, et les éditeurs auront chacun un lieu bien réservé ; les présentations de jeux ressembleront à des conférences de presse, le tout sera donc bien plus conventionnel qu'auparavant. Cependant, une salle commune à tous les éditeurs est prévue, mais l'ESA s'occupera de rendre chaque partie conventionnelle et pratique. À la suite de ces changements, le salon est renommé « E3 Media and Business Summit ». 

En 2007, le nombre de visiteurs décline à seulement . Pour l'édition suivante de 2008, en raison de plaintes et des difficultés à tenir le salon dans la petite ville de Santa Monica, l'E3 revient à Los Angeles. La nouvelle formule ne change pas de l'année précédente, l'évènement n'utilisera qu'un seul hall du Los Angeles Convention Center, contrairement aux éditions précédant l'E3 2007. La presse et les exposants ne sont toutefois pas satisfaits par cette formule, et certains menacent d'abandonner le salon. À la suite de ce second échec, l'ESA décide de revenir à la formule de 2006 pour l'E3 2009, en annonçant également que l'événement est avant tout destiné aux professionnels et à la presse. Le salon redevient ainsi l'Electronic Entertainment Expo, et les sessions des années 2009, 2010, 2011, 2012, 2013, 2014 et 2015 se déroulent au Los Angeles Convention Center.

Pour son édition 2015, la formule de l'E3 est toutefois légèrement modifiée en permettant à des non-professionnels d'entrer sur le salon. Cette ouverture au public reste partielle et ne se fait qu'à l'aide de l'une des quelques milliers d'invitations distribuées par certains éditeurs. Cependant, avec la démocratisation de l'info en direct et des conférences via Internet, les éditeurs sont de moins en moins intéressés par le salon, coûtant trop cher pour eux et étant moins efficace qu'un événement organisé seul.

En 2017, le salon s'ouvre pour la première fois au grand public, avec billets mis en vente le 13 février 2017.

Évolution du nombre de visiteurs à l'E3 de 2002 à 2017
Inclus les visiteurs grand public, hors journalistes et professionnels, non autorisés lors des éditions précédentes.


</doc>
<doc id="1027" url="https://fr.wikipedia.org/wiki?curid=1027" title="Europe du Nord">
Europe du Nord

Dans un sens restreint, l’Europe du Nord désigne généralement des pays nordiques de langues scandinaves (Norvège, Danemark, Suède et Islande) et de langues fenniques (Finlande et Estonie). Dans une vision plus large, l'Europe du Nord désigne toute la moitié nord de l'Europe.

D'un point de vue géographique, l'Europe du Nord peut se définir comme étant l'ensemble des régions s'ouvrant sur la Manche, la mer du Nord et la mer Baltique (de la même manière que l'Europe du Sud peut se définir par rapport à la Méditerranée).

La définition de l'Europe du Nord est variable selon les perceptions et, selon la vision la plus restrictive, englobe :

À la vision restreinte principalement sont souvent ajoutés :
et moins souvent :

De manière générale, toute classification reste subjective suivant les critères que l'on prend en compte (climatiques, linguistiques, historiques...).

Selon la définition de l'Organisation des Nations unies, l'Europe du Nord regroupe :


En 2008, l'Europe du Nord est la région d'Europe disposant du plus haut PIB par habitant et possède un taux de chômage très bas. Ce PIB est de 41 300,83 dollars américains par habitant.



</doc>
<doc id="1029" url="https://fr.wikipedia.org/wiki?curid=1029" title="Liste d'encyclopédies sur Internet">
Liste d'encyclopédies sur Internet

Une encyclopédie en ligne désigne une encyclopédie disponible sur Internet. Il en existe de nombreuses formes, certaines n'étant que le développement d'encyclopédies existantes, d'autres totalement inédites.



</doc>
<doc id="1030" url="https://fr.wikipedia.org/wiki?curid=1030" title="Eraserhead">
Eraserhead

Eraserhead est un film américain en noir et blanc écrit, réalisé et produit par David Lynch, sorti en 1977.

Ce premier long métrage du cinéaste découle d’un scénario de 22 pages qualifié par son auteur d’. Fidèle collaborateur de David Lynch à la suite du film, l’acteur Jack Nance ("Dune, Blue Velvet, Sailor et Lula, Mystères à Twin Peaks, Lost Highway") tient le rôle d’Henry, personnage principal.

Avec un budget estimé à dollars et principalement réalisée dans les locaux désaffectés de l’American Film Institute, la création du film demanda cinq années. Ce film est le récit d’un homme captif de son quotidien morne et sans issue qui cherche vainement à s'en échapper dans le rêve. L’être humain prisonnier de sa condition est un thème dans presque tous les films de Lynch : il ne peut s’évader que par la mort ("Twin Peaks, Mulholland Drive, Elephant Man"), une route ("Sailor et Lula"), ou la folie ("Lost Highway, Mulholland Drive").

Parce qu'il n'a pas été distribué à grande échelle au départ, la diffusion du film en salle fut assurée par les programmes nocturnes des cinémas : les dont le film fait partie deviendront de véritables films culte. "Eraserhead" est considéré comme un classique du cinéma fantastique, ce qui lui valut d’être retenu comme un film « culturel, historique ou esthétiquement important » par le National Film Registry en 2004. L'action prend place dans un centre industriel en décrépitude. La conception sonore du film (Alan Splet et David Lynch) est principalement constituée de bruits de machines.

"Eraserhead" débute par une séquence dans l'espace. La caméra s'approche d'une planète où se trouve une petite cabane dans laquelle un homme défiguré (Jack Fisk) tire des leviers. Un des leviers déclenche la chute d’une créature à l’apparence d’un ver dans une mare. Émergeant peu à peu de la mare, la lumière devient de plus en plus intense jusqu’à un fondu au blanc.

Henry Spencer (Jack Nance) est un imprimeur « en vacances » d’allure nerveuse. Le jeune homme arpente les terrains vagues près des usines jusqu’à son appartement. Là-bas, il apprend qu’il est invité chez les parents de sa copine, Mary X (Charlotte Stewart), qui ne l’avait pas contacté depuis longtemps. Henry croyait qu’elle avait mis fin à leur relation.

Le repas chez la belle-famille prend place dans une atmosphère encline au malaise. La mère de Mary X (Jeanne Bates) talonne véritablement Henry tandis que le père de Mary (Allen Joseph) est un personnage totalement déconnecté de la situation tendue qui règne autour de la table. Après avoir tenté de découper un poulet qui s'anime et se met à suinter un liquide noir, Henry apprend qu’il est le père d’un enfant prématuré de Mary. Il se voit donc dans l’obligation de se marier avec elle.

Mary et le bébé emménagent dans l’appartement d’une pièce d’Henry. On aperçoit alors l’enfant pour la première fois. Avec une apparence proche du fœtus d'agneau et hideusement déformé, il ne cesse de gémir. Ces couinements viendront à bout de Mary qui, incapable de dormir, quitte Henry et le laisse seul avec la petite créature.

Ce départ est suivi par une suite d’événements des plus étranges, incluant la rencontre avec la « dame dans le radiateur » (« Lady in the Radiator »), une femme blonde aux joues grotesquement hypertrophiées à la Betty Boop, qui chante et vit sur la petite scène d'un music-hall caché dans le radiateur de la chambre d’Henry. Il aura, par la suite, une relation sexuelle avec sa voisine, la « jolie fille de l’autre côté du couloir » (« Beautiful Girl Across the Hall »).

Le titre "Eraserhead" (« tête effaceuse » en français) prend toute sa signification durant le dernier quart d'heure du film. La tête d’Henry se détache alors de son corps et s’enfonce dans une flaque de sang, tombe du ciel pour atterrir dans une ruelle où elle s’ouvre. Un jeune garçon (Thomas Coulson) trouve la tête et l’emporte dans une fabrique de crayons où Paul (Darwin Joston), un réceptionniste, appelle son patron (Neil Moran) en appuyant avec insistance sur le bouton d'une sonnette. Le patron furieux entre dans la pièce mais change aussitôt d’humeur en apercevant ce que le jeune garçon leur apporte. On transporte la tête dans une autre pièce où un opérateur de machine à faire des crayons (Hal Landon Jr.) prend un échantillon du cerveau d’Henry et l’appose sur le bout d’un crayon. Il teste cette « gomme » qui s’avère efficace, et le jeune garçon est payé par le patron de l’usine. L'image d'Henry dans son lit laisse ensuite penser que toute cette séquence n'était qu'un rêve.

Un peu plus tard, Henry aperçoit par sa fenêtre deux hommes qui se battent dans la rue. Il tente d’aller voir la jolie fille de l’autre côté du couloir mais celle-ci est avec un autre homme. Le bébé est pris d'un rire sarcastique, Henry prend alors une paire de ciseaux et coupe les bandages dans lesquels l’enfant est enroulé. On s’aperçoit bien vite qu’ils donnent directement sur les organes vitaux de la créature. Pendant que celle-ci hurle de douleur, Henry plante les ciseaux dans ses poumons. Le système électrique de l’appartement disjoncte et les lampes se mettent à clignoter, puis s’éteignent. Une tête géante de l’enfant apparaît dans la chambre. Henry retourne sur la petite scène du music-hall où la dame du radiateur l'accueille tendrement dans ses bras. La scène est inondée de lumière et un bruit blanc fait un crescendo. Puis tout devient noir et silencieux pendant quelques secondes avant le générique de fin.





David Lynch trouva son inspiration pour "Eraserhead" dans les souvenirs de son passage à Philadelphie, où il étudia à l’Art Institute of Philadelphia et y vécut de 1965 à 1971. Il décrira son séjour dans la ville de l’amour fraternel comme « étrange, bizarre, à mi-chemin entre le rêve et la réalité ».

Après avoir reçu une bourse de l’American Film Institute en 1968, il réalise son troisième court-métrage, "The Grandmother", en 1970, où il fait la rencontre d’Alan Splet, un ingénieur du son avec qui il travaillera jusqu’à la fin des années 1980. C’est ce film qui lui permit d’accéder au "Center for Advanced Film Studies", à Los Angeles, où il déménage en 1971.

Nommé "Gardenback" au départ, durant 42 minutes et traitant de l’adultère, le projet de première année de Lynch au "Center for advanced film studies" sera remplacé par le scénario de 22 pages d’"Eraserhead". Au départ, le film sera financé par une bourse de accordée par l’AFI et tourné dans les sous-sols et les bâtiments désaffectés de l’institution.

En 1973, l’American Film Institute demande à voir le film et Lynch leur montre la scène du dîner chez les parents de Mary. L’institution retirera son financement la même année. Par la suite, le tournage fut intermittent et s’étala au total sur une période de plus de deux ans et demi, principalement causée par de faibles et ponctuels apports financiers de la part d’amis et de proches de l’équipe de tournage (notamment l’actrice Sissy Spacek, épouse de Jack Fisk). Les décors du film furent démantelés et reconstruits à plusieurs reprises.

En France, le film est sorti tout d'abord sous le titre de "Labyrinth Man", choisi par le premier distributeur en référence au "Elephant Man" du même David Lynch (1980).

La célèbre scène du découpage du poulet a été parodiée dans le film "Rex the Runt: Dreams".

Le film comporte environ trois cent répliques.

Le film a été rendu célèbre par une campagne de promotion orchestrée autour du fait qu’il était diffusé dans seulement quelques salles aux alentours de minuit. Les spectateurs se voyaient récompensés à la sortie de la séance d’un badge au message délibérément énigmatique, sur lequel on lisait : « "I saw it!" » (« Je l’ai vu ! »), sans plus de précisions.




</doc>
<doc id="1031" url="https://fr.wikipedia.org/wiki?curid=1031" title="Communautés autonomes d'Espagne">
Communautés autonomes d'Espagne

L'Espagne est divisée en 17 communautés autonomes, qui disposent toutes d'un régime plus ou moins large d'autonomie par rapport à l'État central.

Les bases de l'agencement général sont les lois cadres édictées par Madrid que les communautés autonomes, peuvent, si ce n'est se doivent, de compléter. Là aussi, la distinction entre État fédéral et État des autonomies apparaît : alors qu'un État fédéral répartit strictement l'ensemble des prérogatives entre les constituants, l'État des autonomies les partage (ex. L'Éducation est du ressort des États fédérés aux États-Unis alors qu'en Espagne, le programme est établi à 70 % par l'État central et à 30 % par les communautés). 

De par ce caractère, l'Espagne n'est donc pas une fédération, les communautés ne sont autonomes que dans l'aspect législatif et exécutif (Art. 148 premier alinéa). Il convient aussi de préciser que le chef des communautés autonomes est le roi d'Espagne qui nomme, sur proposition des parlements monocaméraux autonomes, le chef du gouvernement régional.

S'ajoute que l'article 147 précise que les communautés, une fois créées, entrent dans l'organisation juridique de l'État (dans le sens de l'état central), la dépendance judiciaire des communautés à l'organisation judiciaire de l'Espagne étant alors sacralisée.

Jusqu’en 2009 les communautés autonomes ne disposaient pas de ressources propres mais l’État central reversait le produit des impôts nationaux proportionnellement à la population de chaque communauté. Ainsi, même si la somme totale payée par les Catalans au titre de l’impôt est la deuxième plus forte d’Espagne et celle payée par les Andalous est l’avant-dernière, les deux communautés autonomes reçoivent environ autant de subvention de la part de l’État central (la population de la Catalogne et de l’Andalousie étant comparables en nombre).

Depuis la nouvelle loi de financement des administrations autonomes de 2009, les communautés autonomes disposent de plus d’autonomie fiscale. La loi prévoit deux types d’impôts à percevoir par les communautés autonomes du régime général du système fiscal espagnol :


La plus vaste des communautés autonomes est la Castille-et-León tandis que la moins étendue est la Communauté autonome des Îles Baléares. La plus peuplée est l'Andalousie alors que c'est La Rioja qui compte le moins d'habitants.

En ce qui concerne le PIB, en 2010, la Communauté de Madrid est première et La Rioja dernière.

Le Pays basque, également appelé Euskadi ou la Communauté autonome basque a l'indice d'innovation le plus haut des communautés autonomes avec 1,5 en 2007. De même les est de 1,90 en 2008 (Innobasque).

Enfin, pour ce qui est du revenu par tête (2008), le Pays basque domine un classement que ferme l'Estrémadure. La Communauté autonome basque a par tête en 2008, soit un montant supérieur de 33,8 % à la moyenne de l'État, qui est de , et aussi au-dessus de la moyenne des 27 pays de la UE, qui était de par personne. Madrid est la deuxième dans ce classement, avec par habitant, suivi de la Navarre, avec , tandis que l'Estrémadure est la dernière avec un PIB moyen par habitant et par année de .

Les communautés autonomes comprennent d'une à neuf provinces.
Dans le cas où une communauté se résume à une seule province, on parle de communauté autonome uni- ou monoprovinciale.

Les villes autonomes de Ceuta et Melilla ne sont pas à proprement parler des communautés autonomes, bien que leur fonctionnement soit assez similaire. Elles sont comptées comme des autonomies supplémentaires (parfois groupées en une seule ou non) quoique souvent oubliées.




</doc>
<doc id="1032" url="https://fr.wikipedia.org/wiki?curid=1032" title="Environnement">
Environnement

L'environnement est « l'ensemble des éléments (biotiques ou abiotiques) qui entourent un individu ou une espèce et dont certains contribuent directement à subvenir à ses besoins », ou encore comme « l'ensemble des conditions naturelles (physiques, chimiques, biologiques) et culturelles (sociologiques) susceptibles d’agir sur les organismes vivants et les activités humaines ».

La notion d'environnement naturel, souvent désignée par le seul mot « environnement », a beaucoup évolué au cours des derniers siècles et tout particulièrement des dernières décennies. L'environnement est compris comme l'ensemble des composants naturels de la planète Terre, comme l'air, l'eau, l'atmosphère, les roches, les végétaux, les animaux, et l'ensemble des phénomènes et interactions qui s'y déploient, c'est-à-dire tout ce qui entoure l'Homme et ses activités — bien que cette position centrale de l'Homme soit précisément un objet de controverse dans le champ de l'écologie.

Au , la protection de l'environnement est devenue un enjeu majeur, en même temps que s'imposait l'idée de sa dégradation à la fois globale et locale, à cause des activités humaines polluantes. La préservation de l'environnement est un des trois piliers du développement durable. C'est aussi le des huit objectifs du millénaire pour le développement, considéré par l'ONU comme ".

On trouve " en français dès 1265 dans le sens de « circuit, contour » puis à partir de 1487 dans le sens « action d'environner ». Le mot provient du verbe "environner", qui signifie "action d'entourer". Lui-même est un dénominatif de "environ", qui signifie "alentours".

Deux dictionnaires au attestent un emprunt à l'anglais "environment" mais pour traduire le mot "milieu". Bertrand Lévy précise que le mot, au sens d', apparait pour la première fois en 1964, il est dérivé de l’américain "environment". Avant, les géographes qui s'intéressaient au sujet et notamment Elisée Reclus utilisaient le terme "milieu".

Le mot "environnement" est polysémique, c'est-à-dire qu'il a plusieurs sens différents. Ayant le sens de base de "ce qui entoure", il peut prendre le sens de "cadre de vie", de "voisinage", d"'ambiance," ou encore de "contexte" (en linguistique). L'environnement au sens d"'environnement naturel qui entoure l'homme" est plus récent et s'est développé dans la seconde moitié du .

Le mot "environnement" est à différencier du mot "nature" qui désigne les éléments naturels, biotiques et abiotiques, considérés seuls, alors que la notion d'environnement s'intéresse à la nature au regard des activités humaines, et aux interactions entre l'homme et la nature. Il faut également le différencier de l'écologie, qui est la science ayant pour objet les relations des êtres vivants avec leur environnement, ainsi qu'avec les autres êtres vivants, c'est-à-dire, l'étude des écosystèmes. La notion d'environnement englobe aujourd'hui l'étude des milieux naturels, les impacts de l'homme sur l'environnement et les actions engagées pour les réduire.

L'environnement a acquis une valeur de bien commun, et a été compris comme étant aussi le support de vie nécessaire à toutes les autres espèces que l'Homme. En tant que patrimoine à raisonnablement exploiter pour pouvoir le léguer aux générations futures, il est le support de nombreux enjeux esthétiques, écologiques, économiques et socio-culturels, ainsi que spéculatifs (comme puits de carbone par exemple) et éthiques.

L'ONU rappelle dans son rapport GEO-4 que sa dégradation "" (…) et" ".

Ce même rapport rappelle que l'environnement fournit l'essentiel des ressources naturelles vitales de chacun (eau, air, sol, aliments, fibres, médicaments, etc.) et de l'Économie ; "".

L'histoire de l'environnement est une sous-division de l'histoire qui intéresse de plus en plus de chercheurs. Son but est d'étudier rétrospectivement l'état de l'environnement à différentes époques et ses interactions avec les activités humaines.

La prise de conscience de l'existence d'un environnement s'est développée par vague et de manière différente selon les époques, les régions et les cultures humaines. Certaines interprétations animistes ou religieuses, comme le bouddhisme, ont favorisé un certain respect de la vie, des ressources naturelles, et des paysages. Ce respect était motivé avant tout par des croyances religieuses, bien plus que par un réel désir de protection des milieux naturels. En effet, les concepts d'environnement économique, urbain ou civique tels que nous les définissons aujourd'hui ne semblent pas avoir été relevés par les ethnologues ni par les historiens.

Au , en Occident, le romantisme a mis en avant la beauté des paysages sauvages, parfois en les opposant aux paysages et à la misère des mondes ouvriers, et industriels. En vantant les beautés de la nature, les romantiques ont fait prendre conscience que ce bien était précieux et devait être préservé. C'est par cet intérêt porté au paysage que les sociétés humaines vont commencer à prendre en compte l'environnement. 

À partir de 1825 les peintres de l'École de Barbizon sortent de leurs ateliers, ils peignent directement la nature dans la forêt de Fontainebleau et souhaitent en préserver sa beauté. Contre les forestiers qui souhaitent planter des résineux au risque d'altérer le paysage, ils inventent l'écoterrorisme en s'opposant aux coupes et en arrachant les jeunes plants potentiellement disgracieux. En 1853, ils obtiennent que cette forêt soit classée sur plus d'un millier d’hectares pour un motif esthétique. En 1861, un décret impérial officialise ces "réserves artistiques". Ainsi la forêt de Fontainebleau devient le premier site naturel protégé au monde.Le géographe Élisée Reclus décrit avec émerveillement et poésie le milieu dans lequel vivent les hommes tout en constatant les effets du capitalisme sur l’agriculture et l’environnement. Précurseur de l'écologie, il sensibilise et incite ses lecteurs à endosser la responsabilité de la beauté de la nature, condition pour l’épanouissement de la nature et de l’humanité.

Les États-Unis créent le statut de parc national, avec le président Abraham Lincoln le et la Yosemite Valley devient le second site naturel protégé au monde. Le parc national de Yellowstone deviendra en 1872 le premier parc national. La France, en 1906, vote sa première loi sur la protection du paysage. À cette époque, c'est plutôt le paysage, et non l'écosystème qui guide les choix des élus pour les sites à protéger, comme le montre par exemple le classement des boucles de la Seine peints par les impressionnistes.En 1896, Arrhenius développe l'embryon de la première théorie environnementaliste, en étudiant l'effet de l'augmentation de la teneur en dioxyde de carbone (CO) dans l'atmosphère ; dans son article "De l'influence de l'acide carbonique dans l'air sur la température du sol", il cite la vapeur d'eau et le CO comme gaz à effet de serre, et emploie même le terme. Il propose certains calculs mettant en évidence l'élévation de la température en fonction de l'élévation de la concentration en CO ; il formule l'hypothèse du lien entre des variations de concentration au cours des âges géologiques, expliquant les variations de températures correspondantes.

Dès la fin du et pendant la majeure partie du , le développement mondial est très fort. La révolution industrielle et la forte croissance économique favorisent une industrie lourde et fortement consommatrice en ressources naturelles. Les nombreux conflits font prendre conscience de la rareté de certaines ressources, voire localement de leur épuisement.

Les premières catastrophes industrielles et écologiques visibles (marées noires, pollution de l'air et des cours d'eau) sensibilisent l'opinion publique et certains décideurs à la protection des écosystèmes.
La perception de l'environnement a également fortement progressé avec une meilleure diffusion des connaissances scientifiques et une meilleure compréhension des phénomènes naturels. La découverte et l'exploration de nouveaux milieux (Arctique, Antarctique, monde sous-marin) ont mis en évidence la fragilité de certains écosystèmes et la manière dont les activités humaines les affectent. Ils ont été respectivement et notamment vulgarisés par de nombreux auteurs, dont Paul-Émile Victor et le commandant Cousteau.

Dans le même temps, la connaissance rétrospective de l'histoire de la planète et des espèces progressait avec la paléoécologie, et la mise à jour de preuves scientifiques de catastrophes écologiques majeures qui ont fait disparaître successivement des espèces durant des millions d'années. Ces sciences du passé ont montré les liens forts qui lient la pérennité des espèces à leur environnement et au climat.

De nombreux outils scientifiques et techniques ont également contribué à une meilleure connaissance de l'environnement et donc à sa perception. Parmi les principaux, citons l'observation, puis l'analyse et la synthèse, photographie aérienne, puis satellitaire, et plus récemment, la modélisation prospective.

Vers la fin du , la prise de conscience de la nécessité de protéger l'environnement devient mondiale, avec la première conférence des Nations unies sur l'environnement à Stockholm en juin 1972. En juin 1992, lors du sommet de la Terre de Rio de Janeiro, l'environnement est défini comme un bien commun et un bien public. Depuis les années 1990, les mentalités évoluent très rapidement pour se rapprocher de la perception que nous avons aujourd'hui de l'environnement.

Cependant, la prise en compte de l'environnement dans les décisions et les pratiques environnementales diffère énormément d'un pays à l'autre. Dans les pays en voie de développement, où les préoccupations de la population sont très différentes de celles des pays développés, la protection de l'environnement occupe une place beaucoup plus marginale dans la société.

La Charte de l'environnement a été annoncée le 3 mai 2001 à Orléans par le président de la République française Jacques Chirac. Elle a été adossée à la Constitution française par la loi constitutionnelle du mars 2005. Par principe de précaution, elle stipule que : « Chacun a le droit de vivre dans un environnement équilibré et respectueux de la santé ». Avec la Charte de l’environnement, le droit à l’environnement devient une liberté fondamentale de valeur constitutionnelle. La Charte place en effet, désormais, les principes de sauvegarde de notre environnement au même niveau que les Droits de l’Homme et du Citoyen de 1789 et les droits économiques et sociaux du préambule de la constitution de 1946.

Depuis quasiment les débuts de l'art, l'environnement a été une source d'inspiration inépuisable pour l'homme. Les représentations d'animaux ou de paysages jalonnent l'histoire de l'art, et il n'est pas une époque qui fasse exception à la règle.

Les paysages occupent une part primordiale dans l'art en Extrême-Orient, notamment en Chine et au Japon, mais il faudra attendre la Renaissance en Europe pour voir les paysages prendre de l'importance dans la peinture. De nombreux peintres seront qualifiés de paysagistes, tant parmi les romantiques que parmi les impressionnistes.

Plus tard, les éléments environnementaux seront toujours très présents dans les nouvelles formes d'art, comme la photo, et plus tard, le cinéma. Plus récemment, des artistes ou des personnalités utilisent l'art pour sensibiliser la population à la défense de l'environnement : c'est le cas par exemple d'Al Gore, qui réalisa un film "An inconvenient truth", ou le photographe Yann-Arthus Bertrand.

La science a connu un développement considérable au cours du dernier siècle. Les connaissances scientifiques ont beaucoup progressé, en particulier dans le domaine de l'environnement. Certaines disciplines spécialement dédiées à l'environnement, qui n'existaient pas jusque-là sont même apparues récemment, comme l'écologie.
La mise au point de nouveaux moyens techniques, d'instruments de mesures et d'observation, a fait considérablement avancer la connaissance que nous avions de l'environnement, que ce soit au niveau du fonctionnement des êtres vivants et des interactions avec leur milieu, des écosystèmes. Les avancées de la physique et de la chimie nous ont permis de comprendre le fonctionnement des végétaux et plus globalement des corps vivants. L'avancée de la science a entraîné une plus grande mesurabilité des impacts humains sur l'environnement, d'où provient également une plus grande prise de conscience.

Les problématiques environnementales sont passées de problèmes locaux, comme la protection d'une espèce, à des problèmes mondiaux (trou dans la couche d'ozone, réchauffement de la planète, par exemple). La nécessité d'avoir des données mondiales est donc apparue, entraînant le besoin de mutualiser les données. Par nécessité, le monitorage (programme de surveillance) environnemental se développe aujourd'hui à échelle planétaire, aidée par les avancées techniques, politiques et idéologiques. L'Organisation des Nations unies offre un cadre international de travail : PNUE, ainsi que des conférences internationales, et des sommets mondiaux, comme celui de Rio, permettant ainsi à des chercheurs de divers horizons de rassembler leurs connaissances. Les problématiques environnementales étant récemment devenues mondiales, il est fondamental d'appréhender la recherche scientifique de manière globale, et non plus locale.

De nombreux pays ou groupes de pays ont également des communautés d'intervenants, d'indicateurs et de chercheurs spécialisés dans les thématiques environnementales, avec des programmes de mutualisation et d'échange des connaissances.

Des agences ou observatoires de l'environnement se sont constitués dans de nombreux pays. Ils relèvent, mesurent, et suivent des indicateurs environnementaux et produisent des statistiques, éventuellement agrégées au niveau local, régional, national, européen (ex : Eurobaromètre) et planétaire (sous l'égide de l'ONU et du Programme des Nations unies pour l'environnement (PNUE). Ce sont des outils d'aide à la décision.
L'idée d'une dégradation de l'environnement de la Terre dans laquelle vivent les humains, par l'effet de la pollution, est devenue largement majoritaire à la fin du : cet effet prend la forme d'une crise écologique globale. Plus qu'une idée, les faits démontrent que l'évolution de l'environnement est représentative d'une dégradation de l'habitat, imputable à l'activité humaine.

Pour mesurer cette dégradation, on peut se servir de plusieurs indicateurs :

Un rapport de l'OCDE a fait l'état des thématiques environnementales et leur a associé un « niveau d'inquiétude ». Cette étude montre que les impacts de l'homme sur l'environnement sont multiples et variés. Presque tous les éléments constituant l'environnement sont touchés par les activités humaines.

Ces impacts sur l'environnement sont liés à plusieurs facteurs, dont ceux évoqués le plus souvent sont la démographie et le développement économique. En effet, le lien entre la population et la pollution est évident : les impacts humains locaux sont proportionnels au nombre d'habitants d'une région, et il en est de même pour le nombre d'habitants sur la Terre. Mais la démographie n'est pas le seul facteur qui intervient dans cette équation. Le niveau de développement économique, les habitudes de vie, le climat et toute une multitude de facteurs, jouent un rôle très important dans les impacts sur l'environnement, ce qui amène de nombreux spécialistes à relativiser le rôle de la démographie et de la surpopulation dans les problèmes environnementaux.

Les problèmes liés aux sols sont souvent des problèmes d'ordre local. On parle de régression et dégradation des sols lorsqu'un sol perd en qualité ou que ses propriétés changent. Ils peuvent être divisés en deux catégories :

Selon le rapport de l'OCDE, trois points sont particulièrement préoccupants concernant l'eau. Il s'agit de la consommation d'eau et l'épuisement de la ressource, la pollution des eaux de surface et la pollution des eaux souterraines.

La gestion de l'eau en tant que ressource naturelle est une question préoccupante pour de nombreux états. Le rapport de l'OCDE qualifie ce problème comme "nécessitant une attention urgente". Toujours d'après ce rapport, un grand nombre d'humains vivent dans des zones soumises au stress hydrique. En 2030, en l'absence de mesures efficaces pour préserver les ressources en eau potable, il pourrait y avoir milliards de personnes concernées par le stress hydrique, dont 80 % de la population du BRIC (Brésil, Russie, Inde, Chine). Cette pénurie sera aggravée par l'augmentation de la population et donc des besoins en eau pour boire ou pour l'agriculture.

Le réchauffement de la planète aurait également des incidences fortes sur les ressources en eau. Des régions comme l'Asie centrale, l'Afrique sahélienne ou les grandes plaines des États-Unis pourraient connaître un assèchement dramatique pour les populations, leur approvisionnement en eau, et l'agriculture, comme le rappellent les études de l'UNFCCC.

Ce manque d'eau à l'échelle mondiale semble donc inéluctable, et s'annonce lourd de conséquences sur les activités humaines (agriculture, développement, énergie), et sur les relations diplomatiques internationales. En effet, les enjeux se multiplient autour de l'eau ; indispensable à la survie d'une population, elle l'est aussi pour l'agriculture, via l'irrigation, à la production d'énergie hydraulique. Les cours d'eau ne se limitant généralement pas à un seul État, ils sont devenus des enjeux géopolitiques stratégiques déterminants à la source de nombreux conflits. La plupart des états sont conscients de ces enjeux forts, comme en atteste la tenue régulière du forum alternatif mondial de l'eau.

La pénurie d'eau n'est pas la seule préoccupation à avoir vis-à-vis de la gestion des ressources en eau. L'évolution de leur qualité et de leur degré de pollution sont également inquiétants.

Parce que l'eau douce est une ressource précieuse, la pollution des nappes phréatiques, qui constituent une réserve importante d'eau douce relativement pure, et des lacs et des rivières, est sans doute la plus préoccupante. Ceux-ci étant également liés aux activités humaines, ils sont impactés, et leur état est globalement en cours de dégradation. Les pollutions des eaux douces se retrouvent dans les mers et les océans, de par le cycle de l'eau, et viennent ainsi aggraver la pollution marine.

La pollution des eaux peut être d'origine et de nature diverses et variées. Elle peut être :

La pollution atmosphérique, ou pollution de l'air, est une pollution d'origine diffuse qui peut avoir des effets locaux ou globaux. Le terme « pollution de l'air » signifie généralement "l'introduction directe ou indirecte dans l'air ambiant (à l'exception des espaces confinés) par l'homme de toute substance susceptible d'avoir des effets nocifs sur la santé humaine et/ou l'environnement dans son ensemble".

Comme pour l'eau, la pollution de l'air peut être de nature et d'origine diverses et variées. On distingue différents types de pollutions :

Les effets de cette pollution peuvent être régionaux ou mondiaux. Régionalement, on peut avoir :

À l'échelle de la planète, les effets de la pollution atmosphérique sont importants, et ont des impacts sur l'atmosphère et le climat de l'ensemble du globe. Les deux principaux effets de cette pollution sont :

Les activités humaines ont une incidence forte sur la biodiversité, c'est-à-dire sur l'avenir des espèces vivantes, animales et végétales. Le taux d'extinction actuel des espèces est de 100 à fois supérieur au taux moyen naturel constaté dans l'histoire de l'évolution de la planète. En 2007, l'UICN a évalué qu'une espèce d'oiseaux sur huit, un mammifère sur quatre, un amphibien sur trois et 70 % de toutes les plantes sont en péril. Cette extinction massive des temps modernes est souvent désignée par le nom d'extinction de l'Holocène.

L'origine de cette extinction massive d'espèces est principalement humaine, et notamment depuis les années 1500, où l'influence de l'homme a considérablement augmenté.

La surchasse et la surpêche sont à l'origine de la disparition ou facteurs de menaces sur plusieurs espèces, mais c'est surtout la destruction et la dégradation de l'habitat naturel qui a eu les plus importantes conséquences. L'anthropisation grandissante des milieux naturels, via la déforestation, l'imperméabilisation des sols, l'agriculture et l'élevage extensif, l'urbanisation des littoraux, l'introduction d'espèces invasives, mais aussi la pollution des eaux et des sols, ainsi que le changement climatique, sont autant de facteurs qui réduisent ou détruisent l'habitat de certaines espèces, causant parfois leur disparition.

La biodiversité fait l'objet d'études internationales dirigées par les Nations unies, "via" un groupe d'experts : l'IPBES. Elle est considérée comme un indicateur important, dont la dégradation serait significative pour la santé de la planète, mais aussi pour le bien-être humain. La préservation de la biodiversité est également une cible des objectifs du millénaire pour le développement.

Une ressource naturelle est un élément présent dans la nature, exploité ou non par les humains, et pouvant être renouvelable ou non renouvelable. Dans une approche quantitative, on parle de capital naturel.
La raréfaction des ressources naturelles est considérée comme inquiétante et représente une menace pour l'environnement et les activités humaines, qu'il s'agisse des ressources naturelles renouvelables, ou des ressources non renouvelables.

S'agissant des ressources renouvelables (poissons, forêts, etc.), leur surexploitation peut entraîner une baisse significative de la ressource disponible, diminuant ainsi sa capacité de renouvellement. Ce sont les problèmes de la surpêche et de la déforestation entre autres. Si rien n'est fait pour enrayer cette spirale, cela peut conduire à l'épuisement total de la ressource, comme cela s'est déjà produit localement sur l'île de Pâques, par exemple, où la déforestation a conduit à la disparition des arbres sur l'île et à l'extinction de plusieurs espèces.

Pour les ressources non renouvelables telles que les énergies fossiles et les minerais, l'impact de leur extraction sur l'environnement est relativement faible à court terme. C'est leur utilisation, qui produit souvent une pollution significative, et leur raréfaction qui sont une source d'inquiétude socio-économique. En effet, certaines de ces ressources sont une composante importante de l'activité humaine et économique. Leur extraction, continuellement en hausse, conduit à une baisse inquiétante des réserves, ce qui pose des problèmes pour les besoins des générations futures en matières premières.

L'apparition de certains types d'industrie et de nouvelles techniques au cours du a rendu possible des accidents ou des actions ayant des conséquences très importantes sur les hommes et sur de multiples domaines de l'environnement, tout en touchant des zones géographiques plus ou moins vastes. Certains de ces accidents, dont certaines grandes catastrophes industrielles ou certains accidents nucléaires, peuvent affecter des écosystèmes entiers et engendrer des séquelles graves sur l'environnement. On parle alors de catastrophe environnementale ou écologique. Le terme est parfois utilisé pour désigner, non pas un événement ponctuel, mais une action ayant des effets négatifs importants et constants sur l'environnement. Le thème a notamment été largement utilisé dans les médias pour parler de l'impact écologique du barrage des Trois-Gorges.

Les dégradations de l'environnement ont des effets importants, sur la santé humaine et la qualité de vie des populations, comme en attestent les études sur le sujet et les différents organismes chargés d'étudier la relation entre la santé et l'environnement. La qualité de l'environnement — notamment dans les régions fortement peuplées —, est devenue un véritable problème de santé publique.

Le lien entre santé et environnement a pris toute son importance depuis le sommet de la Terre de Rio en 1992 ; la protection de l'environnement est alors apparue comme une étape incontournable des politiques de santé publique mondiales. Ce lien est généralement désigné par le terme santé-environnement, et il est étudié par la médecine environnementale et le domaine des risques sanitaires.

Les domaines de l'environnement pour lesquels la pollution peut avoir les conséquences les plus néfastes sur les populations sont l'eau et l'air, ressources indispensables à la vie. La pollution des sols peut aussi générer, à plus long terme, des problématiques sanitaires.

L'eau et l'air peuvent être vecteurs de produits toxiques, CMR, non-biodégradables, allergisants ou eutrophisants mais aussi de virus, bactéries et autres agents pathogènes ayant des effets pathologiques directs, à court, moyen ou long terme, sur les organismes vivants.

Dans les dernières années, des moyens techniques ont été développés pour adapter les méthodes industrielles aux impacts de l'activité humaine sur l'environnement. Ces moyens peuvent être techniques, mais aussi législatifs et normatifs. Au niveau international, des accords comme le protocole de Kyoto imposent des quotas maximum d'émissions de gaz à effet de serre. D'autres accords règlent des points plus précis, comme la protection d'un lieu, d'une espèce menacée, ou l'interdiction d'une substance.

Dans les pays développés, les effluents, qu'ils soient liquides ou gazeux, sont majoritairement traités. Ces effluents peuvent être d'origine industrielle ou provenir des particuliers.

Dans la plupart des pays riches, les effluents sont traités lorsqu'ils sont polluants. Pour l'eau, les particuliers sont équipés de fosses septiques ou sont reliés à l'égout. Les rejets liquides passent alors par une station d'épuration avant d'être rejetés dans la nature. Pour les industries, la législation impose des normes qualitatives pour les rejets. Les industries possèdent leur propre station de traitement, ou sont elles aussi reliées à l'égout.

S'agissant de l'air, il existe là-aussi des normes imposant de traiter les rejets polluants. Ces normes sont cependant très dépendantes des techniques existantes, selon le principe de la meilleure technique disponible.

La situation est très différente dans les pays en voie de développement. La plupart des effluents ne sont pas du tout traités, par manque de moyens, ou par absence de législation contraignante. Les enjeux environnementaux sont véritablement importants ; des effluents non traités ont un impact fortement négatif, non seulement sur l'environnement, mais aussi sur la santé des habitants.

L'homme a un impact fort sur l'environnement via ses déchets. On estime que l'ensemble de l'humanité produit entre 3,4 et 4 milliards de tonnes de déchets par an, soit environ 600 kilos par an et par personne. Et ce chiffre est en constante augmentation

Comme pour les effluents, l'absence de gestion des déchets dans les pays pauvres ou sortant des circuits légaux dans le monde, entraînent des impacts négatifs sur l'environnement et la santé humaine. On estime qu'environ 75 % des déchets d’équipements électriques et électroniques (50 millions de tonnes par an) disparaissent des circuits officiels de retraitement, exportée en grande partie illégalement vers des décharges clandestines en Afrique (Ghana, Nigeria), en Asie (Chine, Inde, Pakistan, Bangladesh), ou encore en Amérique du Sud.
Pour éliminer les déchets, il faut tout d'abord les collecter. Ensuite, il existe différentes techniques pour les éliminer :

Pour minimiser l'impact environnemental des déchets, on parle souvent de la technique des trois R : 

La gestion des ressources naturelles est un enjeu environnemental de premier plan.

Dans le but de sauvegarder les ressources non renouvelables, et de préserver les ressources renouvelables, des techniques de gestion se sont mises en place.
Dans le cas du papier, certains labels certifient une gestion durable de la forêt, certifiant que l'exploitation respecte les rythmes de croissance des arbres et ne participe pas à la déforestation. Pour de nombreuses autres ressources, des labels existent, certifiant de techniques de gestion durables. Pour la pêche ou la chasse des quotas réglementaires imposent de respecter le rythme de renouvellement des espèces animales. Pour des espèces animales ou végétales menacées ou plus fragiles, il est possible de leur assurer une certaine protection grâce à des parcs naturels.

Dans ce domaine, les efforts restant à faire sont grands pour assurer une gestion durable de la majorité des ressources que nous utilisons. C'est pour cette raison que l'OCDE a en fait une de ses priorités.

Dans le but de préserver la biodiversité, de nombreux moyens ont été développés pour protéger les milieux naturels et les espèces qui y vivent.

Les réserves naturelles, qui existent dans de nombreux pays au monde, permettent de préserver des écosystèmes rares ou menacés en limitant l'urbanisation et les activités humaines dans les zones concernées. Pour les espèces menacées, l'UICN dresse et actualise une liste rouge répertoriant les espèces menacées d'extinction. Appuyées par des conventions internationales, comme la convention de Washington, des mesures sont prises pour leur préservation.

Plus récemment, la meilleure compréhension des espèces animales a permis la création des corridors biologiques, qui permettent de relier des milieux naturels entre eux, favorisant ainsi la migration et la dispersion des espèces.

La réduction des émissions de gaz à effet de serre est devenue un enjeu mondial majeur pour la lutte contre le réchauffement climatique.

La sobriété, le choix d'équipements moins gourmands en énergie sont là aussi les méthodes principalement employées. Le recours aux énergies renouvelables contribue, en réduisant les émissions de gaz à effet de serre, à combattre le réchauffement climatique, et représentent un avenir prometteur. Certains pays ont vu l'émergence et la progression de ces énergies ces dernières années, bien qu'elles restent encore marginales dans la plupart des pays.

Les énergies renouvelables englobent des techniques relativement récentes, comme l'énergie solaire thermique, l'énergie solaire photovoltaïque, mais aussi d'autres formes d'énergies qui sont utilisées depuis longtemps sous d'autres formes, comme la biomasse, l'énergie éolienne, la géothermie et l'énergie hydraulique.

En réponse à la croissance des impacts négatifs sur l'environnement, et en partie, par la place grandissante de l'intérêt pour l'environnement dans la société, les gouvernements ont élaboré ou mis en place des lois ou des normes techniques, dans le but de réduire les répercussions néfastes de l'activité humaine sur l'environnement.

Le modèle économique de société, de par la consommation d'énergie, de matières premières, et de par le progrès technique, est très étroitement lié avec les impacts sur l'environnement et sa protection. Pour beaucoup, adopter un modèle économique différent permettrait de réduire nos impacts : les deux modèles les plus couramment évoqués sont celui du développement durable et celui de la décroissance.

Le terme "développement durable" apparaît pour la première fois dans un rapport de l'UICN publié en 1980. La traduction du terme anglais "sustainable development" devrait être "développement soutenable", mais l'expression "développement durable" lui a été préférée. C'est le rapport Brundtland qui pose véritablement les bases du développement durable, et qui en donne la définition de référence : un développement qui répond aux besoins des générations du présent sans compromettre la capacité des générations futures à répondre aux leurs.

Comme le détaille le rapport Bruntland, cela implique un développement qui soit à la fois vivable (écologiquement supportable et socialement juste), viable (économiquement rentable et écologiquement supportable) et équitable (économiquement rentable et socialement juste), s'appuyant en cela sur ce qu'on appelle souvent les trois piliers du développement durable : l'économie, le social et l'environnement.

L'idée d'un développement soutenable signifie que l'on ne doit pas prendre à la Terre plus que ce qu'elle peut donner. Cela implique le recours aux énergies renouvelables, au recyclage pour les matières premières dont le stock n'est pas renouvelable (comme les métaux par exemple), mais aussi une bonne connaissance du rythme de renouvellement des espèces animales, des végétaux, de la qualité de l'air, de l'eau, et plus généralement, de toutes les ressources que nous utilisons ou sur lesquelles nous agissons. Le but de cette démarche est d'avoir une empreinte écologique suffisamment faible pour ne pas faire diminuer le capital naturel. Le développement durable a été décliné en programmes pour la préservation de l'environnement par la majorité des gouvernements et des instances internationales ; en effet, il existe aujourd'hui un consensus global autour de la nécessité de se préoccuper de la durabilité du développement.

Mais le développement durable est aussi l'objet de nombreuses critiques. Luc Ferry, par exemple, se demande « "qui voudrait plaider pour un « développement intenable » ! Évidemment personne ! […] L'expression chante plus qu'elle ne parle" ». Le développement durable peut également parfois être instrumentalisé, soit à des fins politiques pour légitimer des idées protectionnistes, par exemple, ou à des fins commerciales, comme argument de vente par des grandes sociétés. Enfin, le développement durable met la croissance économique au cœur de la stratégie de protection de l'environnement, accordant notamment une place importante à l'innovation et aux solutions techniques alors que certains de ses détracteurs estiment que c'est la croissance économique elle-même qui est à l'origine de la dégradation de l'environnement : c'est la théorie de la décroissance.

La décroissance est un modèle théorique qui prône la décroissance de l'économie dans le but de réduire les impacts humains sur l'environnement.

Ce courant de pensée a pris naissance avec les réflexions du club de Rome, qui publia un rapport en 1972, sous le nom de "The Limits to Growth", traduit en français par "Halte à la croissance ?" et aussi connu sous le nom de "Rapport Meadows". Ce rapport part du constat que la population humaine ne cesse de croître, ainsi que la consommation de biens matériels, de matières premières, d'énergie, et la pollution engendrée. Il préconise donc de se limiter à une croissance zéro, pour éviter d'épuiser les ressources naturelles.

Partant du même constat, les partisans de la décroissance, aussi appelés objecteurs de croissance, concentrent leurs critiques sur le choix du PIB comme indicateur de référence, jugeant ce dernier trop restrictif. En effet, cet indicateur ne prend pas en compte l'état de l'environnement et de ses ressources, pas plus que le bien-être humain. Pour eux, la meilleure solution serait d'entrer en décroissance économique de manière durable et d'abandonner ce qui n'est pas indispensable pour se contenter de satisfaire ses besoins naturels primaires sans entrer dans une société de consommation excessive.

Les partisans de la décroissance sont opposés au développement durable, qui accorde une place importante à la croissance et au développement technique.
Cette théorie est vivement critiquée, notamment sur le fait qu'elle ne prend pas en compte le fait que les progrès scientifiques et techniques pourraient permettre de moins polluer, remplacer les énergies fossiles par des énergies renouvelables, et qu'il est possible de maintenir une croissance économique sans augmenter les consommations d'énergie et de matières premières. Pour étayer cet argument, ils s'appuient par exemple sur l'évolution de l'intensité énergétique des grandes économies mondiales qui a significativement baissé depuis 20 ans.
Cette théorie a fait notamment l'objet des critiques de plusieurs « prix Nobel » d'économie, comme Amartya Sen ou Robert Solow, qui précisent que le progrès permettra de remplacer les matières premières manquantes, notamment par le biais du recyclage. Ils citent en exemple le rapport Meadows qui prédisait la fin du pétrole pour le début du . Enfin, un autre argument souvent repris est qu'un arrêt de la croissance économique serait préjudiciable aux pays les plus pauvres, dont la survie est très dépendante de la croissance, comme le prouve la crise économique de 2008-2009.

Historiquement, ce n'est véritablement qu'avec l'apparition des dans les pays développés que l'environnement a occupé une place dans le débat politique. C'est à la fin des années 1970 que les premiers ministères de l'environnement voient le jour, avec la création le 2 décembre 1970 de l'Environmental Protection Agency par le gouvernement Nixon aux États-Unis, suivi en janvier 1971 par la France et en mai de la même année par l'Australie. Petit à petit, l'ensemble des pays développés vont se doter d'un tel ministère, avec plus ou moins d'importance, et souvent à la suite d'une détérioration importante de l'environnement, comme en Allemagne à la suite de la catastrophe de Tchernobyl.

Depuis, la défense de l'environnement a pris une part croissante dans le débat politique, avec la création des partis verts. Les performances électorales de ces partis dans les pays développés se sont globalement améliorées des années 1980 à nos jours.

Aujourd'hui, certaines élections récentes montrent l'importance des questions environnementales dans les débats politiques. En France en 2007, le pacte écologique de Nicolas Hulot, demandant un engagement fort en matière d'environnement, a été ratifié par tous les candidats à l'élection présidentielle. À l'élection présidentielle américaine de 2008, les questions environnementales ont eu une place importante dans les débats, défendues ardemment par Barack Obama. Enfin, aux élections européennes de 2009, le très bon score du groupe des Verts dans les pays de l'Union européenne vient confirmer cette tendance : l'environnement est véritablement devenu un enjeu politique fort.

Illustrant la globalité du phénomène et sa place croissante dans le monde politique et géopolitique, les actions internationales en lien avec l'environnement se sont multipliées : sommets internationaux, accords et protocoles, journées mondiales, évolution des réglementations, etc.

La description de la politique environnementale des États-Unis fait l'objet d'un article spécifique.

Le manque de vision stratégique holistique bloque un certain nombre d'avancées pour l’environnement (ex Cf protocole de Kyoto et taxe carbone qui est l'exemple d'un échec majeur)

La première réunion internationale autour de l'environnement fut la Conférence internationale sur l’usage et la conservation de la biosphère, qui s'est réunie en 1968 à Paris. Elle permit aux différents acteurs présents d'entamer les discussions en vue du premier Sommet de la Terre, prévu à Stockholm en 1972.

Ces sommets de la Terre sont les principaux sommets internationaux consacrés à l'environnement, et se tiennent tous les 10 ans.
La conférence des Nations unies sur l'environnement de Stockholm en juin 1972, premier sommet international de grande ampleur consacrée à l'état de l'environnement, marque véritablement la prise de conscience d'un problème environnemental mondial, et de la nécessité d'une action concertée de préservation. Elle débouche sur une déclaration de principes et un plan d'action concrètes.

Le 3 mars 1973, la convention de Washington est adoptée par un grand nombre de pays. Elle a pour objectif de veiller à ce qu'aucun commerce ne mette en danger la pérennité d'une espèce animale dans son milieu naturel. Son combat le plus connu est peut-être celui contre le trafic d'ivoire, qui met en danger les éléphants d'Afrique. La même année est adoptée la convention MARPOL, qui réglemente les pratiques en vue de diminuer les pollutions marines.

Le sommet de la Terre de Nairobi, qui s'est tenu en 1982, a été un échec, du fait du faible intérêt de Ronald Reagan, alors président des États-Unis, du faible retentissement de ce sommet, et de l'absence de décisions importantes. Ce sommet n'est d'ailleurs pas considéré comme un sommet de la Terre.
En 1984, le Programme des Nations unies pour l'environnement (PNUE) organise la Conférence mondiale de l’industrie sur la gestion de l’environnement, à Versailles, puis l'année d'après la Conférence internationale sur l’évaluation du rôle du dioxyde de carbone et autres gaz à effets de serre à Villach, alors que les premières interrogations sur le réchauffement climatique commencent à surgir.

Le est signé le protocole de Montréal, qui vise à stopper les dégâts causés à la couche d'ozone, notamment en interdisant l'usage des chlorofluorocarbures et d'autres gaz nocifs pour la couche d'ozone. En 1989, la convention de Bâle réglemente le commerce des déchets, en interdisant notamment l'exportation de déchets des pays développés vers les pays en voie de développement pour échapper aux réglementations locales.

En juin 1992, lors du sommet de la Terre de Rio de Janeiro, l'environnement a été défini comme un « bien commun » ou un « bien public ». Les acteurs internationaux ont montré avoir pris conscience que la problématique environnementale ne pouvait pas être découplée des problèmes économiques, écologiques et sociaux, de sorte que l'environnement a été considéré comme un dénominateur des trois piliers du développement durable. Il a été intégré dans les objectifs des agendas 21 pour les collectivités territoriales.
Le est signé le protocole de Kyoto. Ce texte est d'une importance fondamentale puisque les pays l'ayant signé s'engagent à réduire leurs émissions en gaz à effet de serre, avec des objectifs chiffrés, et ce, pour essayer de limiter le réchauffement climatique. La mise en application du protocole et son suivi donneront lieu à une conférence internationale quasiment tous les ans. Ce protocole n'est entré en vigueur qu'en 2005, puisqu'il devait pour cela être ratifié par des pays dont les émissions en gaz à effet de serre représentent au moins 55 % des émissions mondiales.

En 2002, lors du Sommet de la Terre de Johannesburg, sous l'impulsion, entre autres, des grandes ONG environnementales, l'environnement et le développement durable ont touché le monde des entreprises. On a vu émerger le concept de responsabilité sociétale des entreprises, application des principes de développement durable aux entreprises, l'environnement étant un témoin de l'efficacité fonctionnelle des trois piliers (économique, écologique et le social) du développement durable.

Les préoccupations environnementales touchent également d'autres domaines, et apparaissent dans de nombreuses autres conférences ou sommets mondiaux (G8, G20, Conférences mondiales sur l'habitat, les villes, entre autres). Le conseil de sécurité des Nations unies s'est réuni en avril 2007 pour agir contre les changements climatiques et les dégradations de l'environnement, témoignant de l'importance de la question.

Le dernier sommet mondial important a été le sommet de Copenhague en décembre 2009, dont le bilan est mitigé qui a entamé la préparation de l'après-Kyoto, et essayé de lui donner un nouveau souffle en décidant d'engagements chiffrés en matière de réduction des émissions de gaz à effet de serre.

Les journées mondiales ou internationales sont souvent officialisées par l'Organisation des Nations unies. Un nombre croissant de journées internationales sont consacrées à des thèmes environnementaux, illustrant la place grandissante des thématiques environnementales dans la société. On y trouve, entre autres :


Le droit de l'environnement est une discipline relativement récente qui a pour objet l'étude ou l'élaboration de règles juridiques concernant l'utilisation, la protection, la gestion ou la restauration de l'environnement. C'est un droit technique et complexe, en pleine expansion, et dont les champs tendent à se densifier au fur et à mesure des avancées sociales, scientifiques et techniques. Il est dans un nombre croissant de pays matérialisé par un code de l'environnement, mais sans juridiction spécialisée à ce jour (il n'y a pas de juge de l'environnement, comme il peut y avoir un juge à l'enfance, une spécialité criminelle ou anti-terroriste). Dans certains pays il existe cependant des services de police, douane ou garde-côte ayant une spécialité environnementale.

Les textes de références sont généralement nationaux, sauf dans le cas de conventions, d'accords, et de systèmes de management internationaux, comme la norme de management environnemental ISO 14001. La plupart des pays cherchent désormais à harmoniser leurs textes réglementaires pour adopter une réponse plus adaptée aux problèmes mondiaux.

Sans que cela soit pour autant réglementé, de nombreuses ONG appellent à une éthique de l'environnement qui soit reconnue par la majorité. De même, certaines organisations demandent que soit développée la notion de crime environnemental, notion diversement définie à travers le monde.

Il existe de nombreuses associations et organisations non gouvernementales actives sur les questions d'environnement. Parmi les plus en vue au niveau international, on trouve :


En France, les associations peuvent être « agréées au titre de l'environnement » par le ministère de l'Écologie et du Développement durable. Ce sont des associations régies par la loi de 1901 qui contribuent à révéler des problèmes ou à trouver et tester des solutions dans les domaines de la protection de la nature et de l'environnement et de l'amélioration du cadre de vie (leur vigilance s'exerce sur l'ensemble du territoire). Il existe aussi des associations concernant l'éducation à l'environnement et au développement durable (EEDD) ou le lien santé-environnement, comme l'Association Santé Environnement France (ASEF).

L'économie de l'environnement est une sous-discipline de l'économie qui s'intéresse aux relations entre l'environnement et l'économie, c'est-à-dire aux coûts des atteintes à l'environnement, de la protection et de la connaissance de l'environnement, ainsi qu'à l'efficacité et à la conception d'instruments économiques pour changer les comportements à l'égard de l'environnement. Le problème qui se pose souvent est celui de la valeur marchande à attribuer à un bien environnemental, à une ressource ou à sa qualité. Par exemple, il est très difficile d'attribuer un montant à un air de bonne qualité ou de chiffrer les impacts d'une pollution sur l'eau. Les outils économiques permettant d'influencer les comportements sont nombreux, allant de la loi de l'offre et de la demande (qui rend moins accessible une ressource rare en augmentant son prix), les amendes, dont le calcul du montant peut s'avérer difficile, les licences, normes, permissions, etc.

Cela nécessite une prise en compte des problèmes relatifs aux externalités liées à une activité, qui induisent un coût environnemental non pris en compte par le responsable ; par exemple, un agriculteur ne va pas payer les coûts engendrés par une éventuelle pollution de l'eau par les pesticides, ou un transporteur ne va pas payer pour les gaz rejetés dans l'atmosphère. C'est la prise en compte de ces problèmes qui a fait naître le principe de pollueur-payeur, mais également les droits à polluer, dont l'exemple le plus connu est peut-être la bourse du carbone, prévue par le protocole de Kyoto

L'économie de l'environnement traite également des marchés associés au domaine de l'environnement, et dont la croissance est forte. Ces marchés répondent à des besoins de non-pollution, d'efficacité énergétique, de traitement de l'air, de l'eau, de propreté ou de dépollution. Cette croissance entraîne une hausse de la demande en personnel formé aux métiers de l'environnement.

Les métiers de l'environnement se sont fortement développés dans le contexte du développement durable, faisant de l'environnement un secteur économique en plein développement. Le Grenelle de l'Environnement en France, et les objectifs de croissance verte et de réduction des rejets de CO dans les pays industrialisés ont donné une nouvelle impulsion au développement des métiers de l'environnement. On peut les séparer en 5 grands domaines :
À cela il faut ajouter tous les métiers qui ne sont pas directement liés à l'environnement, mais qui comportent une fort dimension environnementale, comme les métiers de l'énergie, de la construction et de la thermique du bâtiment.

La forte croissance de ces métiers demande des formations adaptées, elles aussi en forte augmentation. Dans les pays développés, il est aujourd'hui possible de trouver de nombreuses formations spécialisées ou ayant un lien avec l'environnement.

La plupart des religions anciennes étaient respectueuses de l'environnement bien que la notion d'environnement à l'époque ne fût pas la même qu'aujourd'hui. Certaines religions animistes et celtiques faisaient des éléments de la nature, comme les sources, certains animaux ou plantes, des divinités. En effet, la non-compréhension de la nature lui conférait un aspect mystique qui aboutissait souvent à une divinisation de ses éléments.

Dans l'hindouisme, l'environnement a une grande importance. On traduit hindouisme par "sanatana dharma", qui, traduit approximativement, signifie l'« essence éternelle du cosmos » – la qualité qui lie tous les êtres humains, animaux et végétaux à l'univers alentour et éventuellement à Dieu, source de toute existence.

Le shintoïsme a également divinisé de nombreux éléments naturels, sous le nom de "kami".
Un kami peut être toute entité supérieure à l'homme par sa nature.

Le monde naturel joue un rôle important dans le judaïsme. Dans la loi juive (halakhah), on trouve des mises en garde pour la protection des arbres fruitiers, ou de tout ce qui relève du bien commun, y compris les éléments naturels constituant l'environnement. La gestion de la création a été confiée par Dieu à l'homme afin de lui assurer une base matérielle et un tremplin pour son développement spirituel. Le rapport du Judaïsme à la nature est donc marqué par le respect de ce qui appartient à Dieu (l'homme est gestionnaire, et non propriétaire) et le fait que tout élément sur terre a son rôle à jouer dans la création, pour le bien être de l'homme et l'harmonie de l'ensemble des créatures.

L'Église catholique alerta la communauté internationale dès les années 1970 sur un important manque d'éthique. Notamment le pape Paul VI, inquiet des nouvelles politiques agricoles, a pris position en 1970 lors du anniversaire de la FAO, puis a délivré un message fort en 1972 à l'ouverture de la Conférence des Nations unies sur l'environnement de Stockholm. Puis, en parallèle à l'œcuménisme prôné par Jean-Paul II, divers évènements chrétiens eurent lieu sur la question de l'environnement. De multiples initiatives œcuméniques ont abouti à proposer en 2007 de consacrer un temps pour la sauvegarde de la Création chaque année entre le septembre (journée de prière pour la sauvegarde de la Création chez les orthodoxes, adoptée ensuite par les catholiques) et le 4 octobre (fête de saint François d'Assise chez les catholiques). 

De même, la plupart des autorités religieuses islamiques se sont positionnées en faveur d'un plus grand respect de l'environnement.

En juin 2012, à l'approche de la Conférence des Nations unies sur le développement durable, Rio+20, le Saint-Siège rappelle .

En juin 2015, quelques mois avant la Conférence de Paris sur le climat (COP 21), le pape François publie l'encyclique "Laudato si'" (« sur la sauvegarde de la maison commune »). C'est la première encyclique d'un pape entièrement consacrée aux questions d'environnement, d'écologie intégrale, et de développement durable et intégral. Bien conscient des problèmes environnementaux de la planète, notamment de l'origine anthropique du réchauffement climatique, le pape souligne que ce sont les pauvres de la planète qui souffrent le plus de la dégradation de l'environnement, et il montre que la préservation de l'environnement ne peut pas être dissociée de la préoccupation d'aider les plus pauvres, ce qui constitue la dimension sociale de la doctrine de l'Église.

Sauf indication contraire, les sources présentées ici sont exclusivement en français ().





</doc>
<doc id="1034" url="https://fr.wikipedia.org/wiki?curid=1034" title="Enron">
Enron

Enron fut l'une des plus grandes entreprises américaines par sa capitalisation boursière. 
Outre ses activités initiales dans le gaz naturel, cette société texane avait monté un système de courtage par lequel elle achetait et revendait de l'électricité, notamment au réseau des distributeurs de courant de l'État de Californie. En décembre 2001, elle fit faillite en raison des pertes occasionnées par ses opérations spéculatives sur le marché de l'électricité, qui avaient été maquillées en bénéfices via des manipulations comptables. Cette faillite entraîna dans son sillage celle d'Arthur Andersen, qui auditait ses comptes.

En 1984, Kenneth Lay, 42 ans, prend la tête de la "Houston Natural Gas", un petit distributeur texan de gaz. Il est l'ancien sous-secrétaire à l'Énergie dans l'administration Reagan et est très lié à la famille Bush qui ont fait des affaires dans le pétrole, et à Dick Cheney, lui aussi patron dans le milieu pétrolier. 

En 1985, Enron nait de la fusion de "Houston Natural Gas" et de la "Internorth of Omaha". 

Son nom fut d'abord Enteron, composé de "En" pour Energy, de "on" de Houston et de "ter" pour la phonétique. Malheureusement ce mot veut dire intestin en anglais scientifique et les lettres "t" et "e" seront ôtées pour conserver: Enron. 
Quand l'entreprise démarra ses activités, elle était à la tête d'un réseau de gazoducs tout à fait respectable. Son "business model" restait traditionnel : production et transport de gaz, ainsi que la vente essentiellement sur les marchés de gros.

Au début des années 1990 et avec l'arrivée de Jeffrey Skilling, ancien consultant de McKinsey, Enron adopte un nouveau "model" avec la création de la "Gas Bank", une chambre de compensation pour le commerce du gaz. Enron est la contrepartie de toutes les transactions et, grâce à cette plate-forme, offre des produits financiers dérivés comme des swaps, options, ... à ses clients.

Par la suite, Enron entreprend une diversification en élargissant son marché à l'ensemble des matières premières et offre des dérivés sur un grand nombre de sous-jacents. Cette politique sera suivie en 2000 par le lancement du site EnronOnline, une plate-forme de trading où seront négociés jusqu'à .

Parmi les nouveaux produits lancés par Enron, on trouve :

Ce développement se fit sous la bienveillance du sénateur texan Phil Gramm, dont l'épouse était présidente de la "Commodity Futures Trading Commission" (CFTC, l'organe de contrôle des produits financiers dérivés, en particulier pour les matières premières). Dans le même temps, une vaste campagne de communication fut lancée auprès des consommateurs, notamment pour les persuader qu'une dérégulation du marché (cf. déréglementation) réduirait leur facture de 43 %.

En 1999, le magasine "Fortune" place Enron à la septième place des entreprises américaines et lui décerne cinq années consécutives le titre d'entreprise la plus innovante des États-Unis.

En interne, Enron créa plus de . 
Le but premier de ces sociétés était de permettre à des investisseurs de cofinancer des infrastructures longues à rentabiliser grâce à la titrisation. Ces sociétés permettaient aussi d'externaliser certains risques importants de la société mère pour éviter de la mettre en péril.

Enron utilisait largement ce type de sociétés non consolidées dans ces buts et par la suite pour sortir des actifs ou des passifs du bilan. Ces sociétés, dont les sièges sociaux étaient installés dans les îles Caïmans, les Bermudes ou les Bahamas, rendaient ainsi le bilan plus "présentable". Toutefois, de succinctes informations sur ces filiales étaient indiquées dans des notes en bas de page des documents d'information financière. 

L'entreprise poursuivait simultanément une politique de communication agressive. "«Je crois en Dieu et je crois dans le marché»", déclare le président Kenneth Lay le charismatique président de Enron. Il envoya aux salariés un courrier leur annonçant qu'il pensait que le cours de l'action gagnerait 800 % avant l'année 2010.

L'objectif est de permettre à Enron d'emprunter de l'argent sans que cela apparaisse dans ses comptes. L'opération implique trois acteurs : Enron, une filiale offshore d'Enron (comme Jedi, LJM ou Mahonia) et une banque (appelons-la banque A). Tous sont complices du montage. L'opération est ici largement simplifiée. 

D'abord la filiale vend pour un million de dollars de gaz à la banque A. La filiale, contrôlée par Enron, reçoit alors un million de dollars de la part de la banque A (un contrat de livraison de gaz est signé mais cette livraison n'a pas lieu ; seul son paiement est effectué). Enron vend ensuite pour un million de dollars de gaz à sa filiale. Enron reçoit donc un million de dollars de cette dernière. Enfin Enron achète à la banque A pour un million cinquante mille dollars de gaz, et paie en plusieurs fois. La banque A recevra, au terme du processus, un million cinquante mille dollars (les cinquante mille dollars sont, en réalité, des intérêts). 

Quel est le résultat ? L'opération équivaut pour Enron à contracter un prêt d'un million de dollars auprès de la banque A et le rembourser progressivement avec des intérêts. Mais cela apparaît dans les comptes comme une opération commerciale, et permet à Enron de se surendetter sans éveiller les soupçons.

D'autre part sur les résultats comptables, Jeff Skilling demanda comme condition à sa prise de la direction, de tenir une comptabilité sur la base des prix du marché et non pas sur des valeurs historiques, ce que le cabinet Arthur Andersen accepta. Il faut savoir qu'aucune pratique ne réglemente encore le nouveau "business model" d'Enron. Cette méthode de comptabilité à la valeur du marché qui est la règle dans le domaine de la finance est appliquée pour la première fois hors de ce milieu.

Cela permet d'inscrire en comptabilité non pas les bénéfices réels, mais les bénéfices à la valeur du cours du gaz au jour de la signature du contrat.

En 2000-2001, les actions Enron baissent fortement dans le sillage de l'explosion de la bulle Internet. Comme ces actions servent de garantie à de nombreux montages financiers réalisés entre Enron et les banques, celles-ci demandent le remboursement de ces emprunts camouflés qui, dès lors, réapparaissent dans le bilan d'Enron.

Le 20 août 2001, son PDG, Kenneth Lay, déclare à Business Week: « La société est probablement dans sa meilleure forme, la meilleure qu'elle ait jamais eue. » Il a pourtant vendu toutes ses actions Enron depuis six mois, empochant au passage une dizaine de millions de dollars de profit net.

Le 9 octobre 2001, Goldman Sachs qualifie Enron de "« best of the best »".

Le 29 octobre 2001, le PDG d'Enron joint le secrétaire au Commerce Donald Evans pour lui demander s'il peut influencer l'agence de cotation Moody's qui a dégradé la note de la dette à long terme de sa société. Evans estime qu'il ne peut intervenir.

Le , la SEC (le gendarme de la bourse américaine) ouvre une enquête. 

Le , la multinationale se déclare en faillite ; le cours de l'action chute à en quelques mois. En un an, sa valeur boursière a été divisée par 350. Environ sont immédiatement licenciés, tandis que des centaines de milliers de petits épargnants perdent l'essentiel de leur capital-retraite, car celui-ci était constitué principalement de parts dans l'entreprise (environ les deux tiers des actifs boursiers d'Enron étaient détenus par des fonds de pension ou des fonds de mutuelles) 

Des procédures pénales sont ouvertes contre les anciens dirigeants de l'entreprise : le trésorier, Ben Glisan fut condamné à cinq ans de prison. Le directeur financier, Andrew Fastow, à dix ans (son épouse, Lea, fut elle aussi condamnée pour avoir aidé à masquer les comptes). 

Le , Kenneth Lay, , est reconnu coupable de six chefs d'accusation, dont la fraude et le complot ; mais il décéda d'un infarctus le 6 juillet avant de commencer à purger sa peine. L'ancien numéro deux d'Enron, Jeffrey Skilling est également reconnu coupable de 19 des 28 accusations, dont fraude, complot, fausses déclarations et délit d'initié et condamné à vingt-quatre ans et quatre mois de prison le . La Cour suprême des États-Unis décide le 24 juin 2010 d'annuler la condamnation de Jeffrey Skilling, ancien PDG d'Enron, pour manquement à ses « obligations morales » lors de la faillite de la société en 2001. Le 21 juin 2013, sa peine initiale de 24 ans de détention est ramenée à 14 années.

Les anciens partenaires de l'entreprise sont également inquiétés par les poursuites judiciaires, notamment : le cabinet d'Arthur Andersen, qui est démantelé en 2002 suite à la faillite d'Enron, Citigroup, JP Morgan, Merrill Lynch, Deutsche Bank, la CIBC, et la banque Barclays.

David Birmingham, Giles Darby et Gary Mulgrew (Neil Coulbeck s'est suicidé), trois anciens banquiers britanniques de la NatWest accusés par la justice des États-Unis ont été extradés de leur pays le . 

Le logo d'Enron a fortement inspiré le logo de la société E Corp (Appelé Evil Corp par Elliot) que veut détruire FSociety dans la série Mr Robot





</doc>
<doc id="1035" url="https://fr.wikipedia.org/wiki?curid=1035" title="Eukaryota">
Eukaryota

Les eucaryotes (ou Eukaryota) sont un domaine regroupant tous les organismes, unicellulaires ou pluricellulaires, qui se caractérisent par la présence d'un noyau et généralement de mitochondries dans leurs cellules. Il s'oppose aux domaines des Eubacteria et des Archaea.

Selon la nomenclature de Whittaker et Margulis (1978), les Eucaryotes rassemblent quatre grands règnes du monde du vivant : les animaux, les champignons, les plantes et les protistes (ou bien les protozoaires), auxquels il convient d'ajouter les chromistes selon la nomenclature de Cavalier-Smith (1981), ce dernier proposant en 1998 d'intégrer ces règnes dans les deux empires des Prokaryota et Eukaryota. Cependant, la pertinence de ces règnes est remise en cause par les tenants du cladisme qui considèrent qu'il est préférable de ne pas reconnaître de groupes paraphylétiques (comme les protistes ou les protozoaires par exemple).

Le terme Eukaryota provient du grec "eu", « bien » et "karuon", « noyau ». Il signifie donc littéralement « ceux qui possèdent un véritable noyau ». Il s'oppose au concept de Prokaryota.

Les eucaryotes forment traditionnellement un empire du monde vivant, ou un domaine dans la classification proposée par Carl Woese. À cette occasion, ce dernier suggéra un changement de nom pour Eucarya, un terme aujourd'hui très peu employé, en dehors de quelques microbiologistes.

Le terme est aussi écrit sous la variante Eukarya, notamment par certains biologistes qui, à l'instar de Margulis et Chapman (2009), considèrent le taxon comme un super-règne.

Les cellules eucaryotes possèdent, par opposition aux procaryotes (Archées et Bactéries) :

Les plus anciens eucaryotes attestés seraient âgés de 1,6 Ga, certains acritarches dateraient approximativement de cette époque. Leur origine, toutefois, pourrait être encore plus ancienne. Grypania, vieille de , a été rapprochée des algues, et les formations de schistes noirs du Gabon, aussi anciens, suggèrent qu'une vie organisée faisant penser aux eucaryotes existait déjà. L'apparition des eucaryotes est encore plus ancienne. La présence de stérane, marqueur biochimique des eucaryotes dans des formations schisteuses australiennes suggèrent qu'à l'époque deux lignées s'étaient déjà différenciées il y a .

Les groupes modernes ont d'abord été retrouvés dans les archives fossiles il y a sous la forme d'une algue rouge. Mais là aussi, les origines sont plus anciennes puisqu'un fossile trouvé dans le bassin du Vindhya en Inde et datant de pourrait bien être une algue filamenteuse. D'autres cellules fossilisées datées de 1,6 milliard d'années et présentant des cellules compartimentées et des organelles ont été découvertes dans des roches sédimentaires en Inde centrale. Il semble y avoir deux types d'algues rouges nommées "Rafatazmia chitrakootensis" (filamenteuse et contenant de grands disques rhomboïdaux qui pourraient être des restes de chloroplastes) et "Ramathallus lobatus" (plus globulaire et charnue). Mieux dater l'apparition des premiers eucaryotes est important pour évaluer les vitesse et taux de mutations du génome dans le temps. Faute d'ADN, les chercheurs ne peuvent pas certifier qu'il s'agit d'algues rouges.

La mitochondrie serait le résultat de l'endosymbiose d'une alpha-protéobactérie (une ) par une cellule eucaryote primitive.

L'existence de gènes d'endosymbiotes (transférés au noyau de la cellule hôte et intégrés dans le génome de cette dernière) ou de leurs vestiges (demeurant dans le noyau alors que les organites eux-mêmes sont perdus ou dégénérés) révèle que les ancêtres d'Eucaryotes dépourvus de mitochondries ont contenu jadis de tels organites.

Les Eucaryotes comprennent deux clades : le taxon Unikonte (du grec ', « bâton, flagelle »), lequel représente les cellules eucaryotes possédant originellement un unique flagelle postérieur propulsif, est à l'origine des Opisthokontes (du grec ', « arrière ») regroupant les champignons et métazoaires ou animaux multicellulaires, et des Amoebozoaires ; le taxon Bikonte qui représente les cellules eucaryotes possédant primitivement deux flagelles antérieurs les tirant en avant et qui est à l'origine des plantes vertes.




</doc>
<doc id="1038" url="https://fr.wikipedia.org/wiki?curid=1038" title="Ensemble">
Ensemble

Un ensemble désigne "intuitivement" une collection d’objets (les éléments de l'ensemble), « une multitude qui peut être comprise comme un tout » (au sens d"'omnis"). 

Dans une approche axiomatique, la théorie des ensembles est une théorie de l'appartenance (un élément d'un ensemble est dit « appartenir » à cet ensemble). Le mot "ensemble" désigne alors un objet du domaine de cette théorie, dont les axiomes régissent les propriétés. La théorie des ensembles est utilisée pour fonder les mathématiques, et dans cette approche tout objet mathématique est "in fine" un ensemble.

Mais la notion d'ensemble est aussi une notion de base qui intervient dans à peu près tous les domaines des mathématiques.

La formulation en reviendrait au mathématicien Georg Cantor, qui énonçait : « Par "ensemble," nous entendons toute collection M d'objets "m" de notre intuition ou de notre pensée, définis et distincts, ces objets étant appelés les "éléments" de M ». Ceci était particulièrement novateur, s'agissant d'ensembles éventuellement infinis (ce sont ces derniers qui intéressaient Cantor).

Ce qui est en jeu au premier chef dans la notion d'ensemble, c'est la relation d’appartenance : un élément "appartient" à un ensemble. Ce sont les propriétés de cette relation que Zermelo, puis d'autres, ont axiomatisées en théorie des ensembles. Il est assez remarquable que l'on puisse s'en contenter pour une théorie qui peut potentiellement formaliser les mathématiques. Mais ce n'était pas l'intention de Cantor, et il n'avait pas non plus axiomatisé sa théorie. 

L'objet de cet article est de donner une approche intuitive de la notion d'ensemble, telle qu'elle est indiquée dans l'article théorie naïve des ensembles.

Un "ensemble" peut être vu comme une sorte de sac virtuel entourant ses éléments, ce que modélisent bien les "diagrammes de Venn". Souvent (ce n'est pas toujours possible), on essaye de le distinguer typographiquement de ses éléments, par exemple en utilisant une lettre latine majuscule, par exemple « "E" » ou « "A" », pour représenter l'ensemble, et des minuscules, telles que « "x" » ou « "n" », pour ses éléments.

Les "éléments" peuvent être de n’importe quelle nature : nombres, points géométriques, droites, fonctions, autres ensembles… On donne donc volontiers des exemples d'ensembles en dehors du monde mathématique. Par exemple : "lundi" est un élément de l’ensemble des "jours de la semaine" ; une "bibliothèque" est un ensemble de livres, etc.

Un même objet peut être élément de plusieurs ensembles : "4" est un élément de l'ensemble des nombres entiers, ainsi que de l’ensemble des "nombres pairs" (forcément entiers). Ces deux derniers ensembles sont "infinis", ils ont une infinité d’éléments.

L'appartenance d'un élément, noté par exemple "x", à un ensemble, noté par exemple "A", s’écrit : "x "∈ "A".

Cet énoncé peut se lire :

Le symbole « ∈ », dérive de la lettre grecque ε ("epsilon") introduite par Giuseppe Peano dès 1889. 
Pour Peano « "x" ε "A" » se lit « "x" est un "A" », par exemple « "x" ε N » se lit « "x" est un entier positif ou nul». Le ε renvoie à l'initiale du mot « est » (en latin, langue de l'article de Peano de 1889 !), en français, ou en italien (« è »). Bertrand Russell reprend les notations de Peano en 1903 dans les "Principles of Mathematics", ouvrage qui va participer à leur diffusion, et où est utilisée la forme arrondie vieillie du epsilon : « ϵ », en usage dans l'édition mathématique anglo-saxonne.

Comme souvent pour les relations, on barre ce symbole pour indiquer sa négation, la non-appartenance d’un objet à un ensemble :

En mathématiques – et pas seulement en mathématiques d'ailleurs –, on considère que deux objets sont égaux quand ils ont les mêmes propriétés, que l'on ne peut donc les distinguer l'un de l'autre – c'est la définition de l'égalité de Leibniz. Dire quand deux objets sont égaux, c'est-à-dire quand deux expressions désignent en fait le même objet, c'est donc donner une information sur ce que sont ces objets. En théorie des ensembles on décide qu'un ensemble est complètement caractérisé par ses éléments, son "extension", alors qu'il peut avoir plusieurs définitions. Par exemple, il n'y a pas lieu de distinguer l'ensemble des entiers différents d'eux-mêmes et l'ensemble des entiers supérieurs à tous les nombres premiers : ces deux ensembles sont tous les deux vides, donc égaux – ils ont bien les mêmes éléments –, même s'ils ont des définitions différentes, et sont vides pour des raisons très différentes. 

On dira donc que deux ensembles "A" et "B" sont égaux (on le notera comme d'habitude "A" = "B") quand ils ont exactement les mêmes éléments. Cette propriété est connue sous le nom d'extensionnalité :

où « ⇔ » désigne l'équivalence logique. Deux ensembles qui ont les mêmes éléments sont bien "identiques" : tout ce qui peut être dit de l'un peut être dit de l'autre. Si nous nous représentons les deux ensembles comme des sacs étiquetés chacun par leur nom, s’ils sont égaux, alors il s’agit en fait d’un seul et même sac avec "deux" étiquettes. Par contre, les propriétés d’un ensemble ne dépendent absolument pas de la nature ou de la forme du sac, seulement de son contenu.

Ainsi un ensemble est complètement déterminé par ses éléments. Quand un ensemble est fini, il est donc possible de le définir en donnant la liste de ses éléments, que l'on note traditionnellement entre accolades. Par exemple l'ensemble auxquels appartiennent les éléments 2, 3, et 5, et seulement ces éléments, est noté {2, 3, 5}. L'ensemble est défini en "extension". 

Mais on ne peut procéder ainsi en toute généralité, on ne pourrait définir ainsi un ensemble infini. Même si quelques artifices de notation qui ressemblent à la notation en extension sont possibles (voir ci-après), la façon la plus générale de définir un ensemble est de donner une propriété caractéristique des éléments de cet ensemble. Par exemple, on pourra définir l'ensemble des nombres premiers par une propriété caractéristique de ceux-ci : être différent de 1 et avoir pour seuls diviseurs 1 et lui-même. On parle de définition en "compréhension". L’ensemble {2, 3, 5} peut être défini en compréhension comme l’ensemble de tous les nombres premiers inférieurs à 6. La définition en extension des ensembles finis peut être vue comme un cas particulier simple de définition en compréhension : par exemple l'ensemble {2, 3, 5} est caractérisé par la propriété, pour un nombre entier, d'être égal à 2 ou à 3 ou à 5.

Quand on parle d'ensembles finis, c'est en un sens intuitif, sans avoir vraiment défini cette notion. Un ensemble est fini quand on peut compter ses éléments à l'aide d'entiers tous plus petits qu'un entier donné.

Les ensembles finis peuvent être "définis en extension", par la liste de leurs éléments, et décrits comme tels ; on place la liste des éléments d'un ensemble entre accolades, comme on l'a déjà vu pour l'ensemble {2, 3, 5}. Par exemple, l'ensemble des jours de la semaine peut être représenté par { lundi, mardi, mercredi, jeudi, vendredi, samedi, dimanche }.

La notation d'un ensemble en extension n'est pas unique : un même ensemble peut être noté en extension de façon différentes.
À cause de la propriété d'extensionnalité, il n'est pas question de distinguer des ensembles par le nombre de répétitions d'un même élément à ces ensembles : un élément appartient ou n'appartient pas à un ensemble, il ne peut appartenir à un ensemble une, deux, ou trois fois…

On pourrait imposer que la notation se fasse sans répétitions, ce serait assez malcommode dès qu'interviennent des variables : on ne pourrait noter un ensemble en extension sans devoir supposer que ses éléments sont distincts. 

Il peut arriver que l'on ait besoin d'ensemble « avec répétition », dans le cas fini, il s'agit plus justement, de suites finies à l'ordre des éléments près, on définit alors la notion de multiensemble fini (qui peut se définir à partir de la notion de suite finie).

Les ensembles réduits à un seul élément sont appelés singletons. Par exemple l'ensemble qui contient pour seul élément 0 est appelé « singleton 0 » et noté {0}.

Les ensembles qui ont exactement deux éléments sont appelées paires, la paire des éléments 1 et 2, notée {1,2}, ne doit pas être confondue avec le couple (1,2), qui a un ordre déterminé.

Quand on axiomatise la théorie des ensembles, les paires (et singletons) jouent un rôle particulier, voir l'article Axiome de la paire.

Par extensionnalité, il n'y a qu'un seul ensemble sans éléments, l'ensemble vide, que l'on note ∅ ou { }.

Un ensemble peut être "défini en compréhension", c’est-à-dire qu'on le définit par une propriété caractéristique parmi les éléments d'un ensemble donné. Ainsi l'ensemble des entiers naturels pairs est clairement défini en compréhension, par la propriété « être pair » parmi les entiers naturels. On peut utiliser la "notation d'un ensemble en compréhension", par exemple pour l'ensemble des entiers naturels pairs, on écrira (ℕ désignant l'ensemble des entiers naturels) :
On définira de la même façon (ℤ désignant l'ensemble des entiers relatifs) :

La formulation générale est : 

Il n'est pas pour autant possible de définir un ensemble par n'importe quelle propriété, et lever entièrement la restriction de la compréhension. Si c'était le cas on pourrait définir l'ensemble {x | x ∉ x}, ce qui conduit à une contradiction (c'est le paradoxe de Russell). La restriction de la compréhension à un ensemble connu protège contre ce genre de paradoxes, elle correspond directement au schéma d'axiomes de compréhension de la théorie de Zermelo. Cette restriction ne peut se lever que dans des cas particuliers précis, qui correspondent à d'autres axiomes de la théorie de Zermelo (axiome de la paire, axiome de la réunion, axiome de l'ensemble des parties).

On n'a pas dit ce que l'on entendait par « propriété » ou « condition ». Malgré la restriction précédente, on ne peut tout autoriser, sous peine d'autres paradoxes comme le paradoxe de Richard ou le paradoxe de Berry, qui fait intervenir, par exemple, « l'ensemble des entiers naturels définissables en moins de quinze mots français ». Il est nécessaire de préciser le langage dans lequel on peut définir ces conditions. En particulier ce langage doit être défini "a priori", et ne peut être étendu qu'à l'aide de définitions qui sont soit de simples abréviations, soit résultent de preuves d'existence et d'unicité.

Pour noter l'ensemble des carrés parfaits non nuls (voir exemple au paragraphe précédent) on peut utiliser la notation plus concise :
dont la forme générale est :
Elle représente l'ensemble des images d'un ensemble "E" par une application "f". L'ensemble obtenu s'appelle image directe de "E" par "f". 
Il s'agit d'une variante de la notation en compréhension ci-dessus. Elle se déduit de celle-ci, en utilisant la définition d'une fonction, si "F" est l'ensemble d'arrivée de la fonction "f" :
De cette notation dérivent d'autres notations faciles à comprendre

Ces notations ont leur avantage et leur inconvénient. D'un côté, elles facilitent une compréhension immédiate des ensembles considérés et rendent accessibles à l'intuition des objets plus compliqués. D'un autre côté, ces notations masquent un quantificateur existentiel indispensable dès lors que l'on veut utiliser cet ensemble.

Il existe d'autres notations commodes, en particulier pour les ensembles de nombres, et plus généralement pour les ensembles totalement ordonnés.

On peut utiliser des points de suspension, pour des notations inspirées de la notation en extension pour des ensembles de cardinalité infinie, ou finie mais non déterminée. Par exemple, l’ensemble des entiers naturels peut se noter par : ℕ = { 0, 1, 2, 3, … }. S'il est clair par ailleurs que "n" désigne un entier naturel, {1, 2, … , "n"}, voire {1, … , "n"} désigne en général l'ensemble des entiers supérieurs ou égaux à 1 et inférieurs ou égaux à "n". De même on peut écrire ℤ = { … , –3, –2, –1, 0, 1, 2, 3, … }.
Quand il y a un procédé itératif simple pour engendrer les éléments de l'ensemble, on peut se risquer à des notations comme {0, 2, 4, 6, … } pour l'ensemble des entiers naturels pairs
On peut bien sûr utiliser ces notations pour des ensembles ayant « beaucoup » d'éléments, {1, 2, … , 1000} plutôt que d'écrire les mille premiers nombres entiers non nuls, ou encore { 3, 5, … , 21 } plutôt que { 3, 5, 7, 9, 11, 13, 15, 17, 19, 21 }. 

Toutes ces notations ne sont pas systématiques, ni universelles, et pour les dernières au moins, pas très rigoureuses. On peut encore signaler, la notation, rigoureuse celle-ci, de certains sous-ensembles de la droite réelle, les intervalles. 

Par abus de notation, parfois on ne note pas la variable dans la définition en compréhension, mais seulement la propriété. Ainsi on note un ensemble en plaçant entre accolades la nature, ou une propriété caractéristique, des objets qui lui appartiennent. Par exemple la notation {chiens} désigne l’ensemble de tous les chiens ; pour prendre un exemple plus mathématique, on pourrait écrire parfois {pairs} pour l'ensemble des nombres pairs.



</doc>
<doc id="1040" url="https://fr.wikipedia.org/wiki?curid=1040" title="Ethernet">
Ethernet

Ethernet est un protocole de réseau local à commutation de paquets. C'est une norme internationale : "ISO/IEC 8802-3".

Depuis les années 1990, on utilise très fréquemment Ethernet sur paires torsadées pour la connexion des postes clients, et des versions sur fibre optique pour le cœur du réseau. Cette configuration a largement supplanté d'autres standards comme le , FDDI et ARCNET. Depuis quelques années, les variantes sans fil d'Ethernet (normes IEEE 802.11, dites « Wi-Fi ») ont connu un fort succès, aussi bien pour les installations personnelles que professionnelles.

Ethernet n'offre pas de garantie de bonne livraison des données, ce qui est laissé aux couches protocolaires supérieures.

Dans les premiers réseaux Ethernet, le câble coaxial diffusait les données à toutes les machines connectées, de la même façon que les ondes radiofréquences parviennent à tous les récepteurs. Le nom "Ethernet" dérive de cette analogie : avant le , on imaginait que les ondes se propageaient dans l’"éther", milieu hypothétique censé baigner l'Univers. Quant au suffixe , il s'agit de l'abréviation du mot (réseau) en anglais.

Ethernet a originellement été développé comme l'un des projets pionniers du Xerox PARC. Une histoire commune veut qu'il ait été inventé en 1973, date à laquelle Robert Metcalfe écrivit un mémo à ses patrons à propos du potentiel d'Ethernet. Metcalfe affirme qu'Ethernet a été inventé sur une période de plusieurs années. En 1975, Robert Metcalfe et David Boggs (l'assistant de Metcalfe) ont publié un document intitulé (Ethernet : commutation de paquets distribuée pour les réseaux informatiques locaux).

Metcalfe a quitté Xerox en 1979 pour promouvoir l'utilisation des ordinateurs personnels et des réseaux locaux, et a fondé l'entreprise 3Com. Il réussit à convaincre DEC, Intel et Xerox à travailler ensemble pour promouvoir Ethernet en tant que standard, au terme d'une période au cours de laquelle la réflexion des constructeurs s'oriente vers une informatique décentralisée.

La norme Ethernet I (10 Mb/s), ou « DIX » (DEC Intel Xerox) est publiée en 1980, suivie d'une révision Ethernet II en 1982. L'IEEE s'inspire du standard DIX et publie sa norme IEEE 802.3 en 1983.

Ethernet était à l'époque en compétition avec deux systèmes propriétaires, Token Ring (IBM, plus récent) et ARCnet (TRW-Matra, plus ancien) ; ces deux systèmes ont au fil du temps diminué en popularité puis disparu face à Ethernet, en raison de la baisse de coûts due à la production de masse, et aux modernisations ultérieures d'Ethernet. Ethernet avait par ailleurs moins de contraintes topologiques que le Token Ring (au CeBIT de 1995, on pouvait voir à titre expérimental un simili plafond blanc utilisé comme medium Ethernet, les signaux transitant par infrarouge). Pendant ce temps, 3Com est devenue une compagnie majeure du domaine des réseaux informatiques.

Ethernet est fondé sur le principe de membres (pairs) sur le réseau, envoyant des messages dans ce qui était essentiellement un système radio, captif à l'intérieur d'un fil ou d'un canal commun, parfois appelé "l'éther". Ainsi, Ethernet est conçu à l'origine pour une topologie physique et logique en bus (tous les signaux émis sont reçus par l'ensemble des machines connectées). Chaque pair est identifié par une clé globalement unique, appelée adresse MAC, pour s'assurer que tous les postes sur un réseau Ethernet aient des adresses distinctes sans configuration préalable.

Une technologie connue sous le nom de CSMA/CD (, ou écoute de porteuse avec accès multiples et détection de collision) régit la façon dont les postes accèdent au média. Au départ développée durant les années 1960 pour ALOHAnet à Hawaï en utilisant la radio, la technologie est relativement simple comparée à ou aux réseaux contrôlés par un maître. Lorsqu'un ordinateur veut envoyer de l'information, il obéit à l'algorithme suivant :

Une station qui détecte une collision émet sur le média un signal de collision appelé « jam signal » (une séquence de 4 à ).


En pratique, ceci fonctionne comme une discussion ordinaire, où les gens utilisent tous un médium commun (l'air) pour parler à quelqu'un d'autre. Avant de parler, chaque personne attend poliment que plus personne ne parle. Si deux personnes commencent à parler en même temps, les deux s'arrêtent et attendent un court temps aléatoire. Il y a de bonnes chances que les deux personnes attendent un délai différent, évitant donc une autre collision. Des temps d'attente en progression exponentielle sont utilisés lorsque plusieurs collisions surviennent à la suite.

Comme dans le cas d'un réseau non commuté, toutes les communications sont émises sur un médium partagé, toute information envoyée par un poste est reçue par tous les autres, même si cette information était destinée à une seule personne. Les ordinateurs connectés sur l'Ethernet doivent donc filtrer ce qui leur est destiné ou non. Ce type de communication « quelqu'un parle, tous les autres entendent » d'Ethernet était une de ses faiblesses, car, pendant que l'un des nœuds émet, toutes les machines du réseau reçoivent et doivent, de leur côté, observer le silence. Ce qui fait qu'une communication à fort débit entre seulement deux postes pouvait saturer tout un réseau local.

De même, comme les chances de collision sont proportionnelles au nombre de transmetteurs et aux données envoyées, le réseau devient extrêmement congestionné au-delà de 50 % de sa capacité (indépendamment du nombre de sources de trafic).

Suivant le débit utilisé, il faut tenir compte du domaine de collision régi par les lois de la physique et notamment la vitesse de propagation finie des signaux dans un câble de cuivre. Si l'on ne respecte pas des distances maximales entre machines, le protocole CSMA/CD devient inopérant et la détection des collisions ne fonctionne plus correctement.

Historiquement Ethernet utilisait des bus sur câbles coaxiaux, surtout de type 10BASE5 puis 10BASE2. Il fut ensuite adapté en 10BASE-T pour utiliser des topologies physiques en étoile sur câbles à paires torsadées, les pairs étant raccordés à des concentrateurs ("hubs"), ce qui ne change toutefois rien à la nature d'Ethernet: la topologie logique reste le bus, le médium reste partagé, tout le monde reçoit toutes les trames, il n'y a toujours qu'un seul segment, tout le monde voit les collisions.

Ethernet est un réseau de type diffusion (Broadcast), c'est-à-dire qu'il est possible d'envoyer (y compris dans ses évolutions ultérieures, sur demande) une trame donnée à toutes les stations raccordées au réseau Ethernet, qui constitue ainsi un domaine de diffusion (Broadcast domain).

Il est possible de raccorder deux segments Ethernet par le biais d'un pont (bridge) qui va répéter et retransmettre à l'identique (contrairement à un routeur) les trames d'un segment vers un autre segment. Les deux segments ainsi raccordés forment un seul domaine de diffusion, en revanche ils forment chacun leur propre domaine de collision (les collisions ne traversent pas le pont).

Pour résoudre les problèmes liés aux collisions, les commutateurs ("switches") ont été développés afin de maximiser la bande passante disponible, en reprenant les câbles à paires torsadées (et plus tard en y ajoutant la fibre optique). Un commutateur est une sorte de pont multiport, chaque lien point à point entre un hôte et le commutateur étant alors un segment avec son propre domaine de collision. Dans ce cas, les caractéristiques d'Ethernet changent nettement:

Historiquement Ethernet est un standard de fait décrit depuis 1980 par les spécifications Ethernet / DIX. Par ailleurs, l'IEEE a publié son propre standard IEEE 802.3 en 1983, s'inspirant de ce standard de fait. Il existe donc en fait un standard Ethernet II / DIX d'une part (de 1982), et une norme IEEE 802.3 d'autre part (de 1983). Les deux standards sont interopérables. Par la suite les mises à jour normatives ont été formalisées par l'IEEE, et 802.3 a du reste pris officiellement en compte les aspects de DIX en 1998 (révision 802.3-1998).

Bien qu'il implémente la couche physique (PHY) et la sous-couche (MAC) du modèle IEEE 802.3, le protocole Ethernet est classé dans les couches de liaison de données (niveau 2) et physique (niveau 1) du modèle OSI. En 802.3, la couche LLC (Logical Link Control) 802.2 fait la charnière entre les couches supérieures et la sous-couche MAC (Media Access Control) qui fait partie intégrante du processus 802.3 avec la couche physique ; les formats de trames que le standard définit sont normalisés et peuvent être encapsulés dans des protocoles autres que ses propres couches physiques MAC et PHY. Ces couches physiques font l'objet de normes séparées en fonction des débits, du support de transmission, de la longueur des liaisons et des conditions environnementales.

Ethernet a été standardisé sous le nom IEEE 802.3 :

Il y a quatre types de trames Ethernet (en dehors de l'Ethernet Experimental de 1975) : 

Ces différents types de trame ont des formats différents mais peuvent coexister et être distinguées entre elles sur un même médium physique par les membres du réseau.

La différence de base entre les trames Ethernet II et les autres trames est l'utilisation du champ de 16 bits (soit ) situé après les adresses MACs:
Par convention les valeurs de ce champ entre 0 et indiquent une taille de payload et donc permettent d'identifier une trame Ethernet 802.3 ; et les valeurs plus grandes indiquent un EtherType, et l'utilisation du format Ethernet II. Cette utilisation duale du même champ justifie son appellation courante de champ longueur/type.

L'IEEE 802.3 ayant initialement défini ce champ de après les adresses MAC comme la longueur du payload, il est fait appel à un nouveau champ pour préciser le payload transportés et les niveaux et types de service utilisés (Service Access Point). Les trames 802.3 doivent ainsi avoir un champ LLC de défini par la norme IEEE 802.2. Le LLC étant trop petit par rapport aux besoins potentiels, un champ supplémentaire SNAP de a été défini ultérieurement, utilisable en option. En examinant le champ LLC, il est possible de déterminer s'il est suivi par un champ SNAP ou non.

En outre, Novell a utilisé des trames 802.3 sans LLC (avant la normalisation IEEE 802.2) dans son système d'exploitation Netware pour y faire passer son protocole IPX. Netware ayant été très répandu (à une époque), ce non-standard en est devenu un de fait.

Note: Les valeurs de champ longueur/type entre et sont indéfinies et ne devraient jamais être employées.

Exemple de trame Ethernet II (

Le champ "Type de protocole" des trames Ethernet II peut prendre entres autres les valeurs suivantes :

"Remarques" :

La plupart des évolution ultérieures sont rendues possibles par la mise en œuvre de l'Ethernet commuté.

Une station et un commutateur qui se connectent ensemble peuvent utiliser l'auto-négociation, c'est à dire qu'ils négocient automatiquement sans configuration préalable nécessaire, les éléments de la communication Ethernet et notamment, la vitesse, le duplex, et l'utilisation ou pas de contrôle de flux.

En Ethernet commuté, toutes les stations du réseau peuvent communiquer en même temps (ou à des vitesses différentes, le média physique n'étant pas partagé), il est donc possible pour une station que son port soit saturé en réception par plusieurs communications entrantes. Le commutateur peut alors stocker temporairement et/ou détruire les trames qui ne peuvent être transmises, ou opter pour d'autres méthodes comme le "backpressure" ou les trames Pause.

Dans ce cas le commutateur génère un signal de collision factice vers la station émettrice (en fait il n'y a pas de collision puisqu'il s'agit d'Ethernet commuté, full-duplex, mais ce signal est toujours pris en compte), ce qui fait cesser temporairement son émission.

IEEE 802.3x définit un type de trame Pause qu'un équipement dont le lien sature en réception peut envoyer pour faire taire l'émetteur le temps que le lien ne soit plus saturé, fournissant ainsi un mécanisme normalisé de contrôle de flux.

Toutefois cette norme ne permet pas d'être spécifique en fonction du trafic (aucune prise en compte de types ou classes de trafic), tout le trafic de la station est stoppé. Par conséquent des trames Pause prenant en compte les classes de services sont normalisées par la norme IEEE 802.1Qbb (contrôle de flux Ethernet prenant en compte les priorités 802.1p).

La norme IEEE 802.1Q permet de faire circuler des réseaux virtuels au sein du réseau Ethernet physiques, en distinguant les trames de chaque VLAN (Virtual LAN) par un identifiant sur 12 bits de 1 à 4095. Il contient aussi une valeur de classe de service (802.1p) sur 3 bits.

La norme IEEE 802.1Q, en plus de définir des VLAN, inclut aussi une valeur de classe de service (802.1p) sur 3 bits qui permet de classifier et discriminer 8 classes de trafic (des Classes de Service – "Class of Service" ou CoS) pour traitement éventuel par un mécanisme de Qualité de Service / QoS (Quality of Service).

Il est désormais possible pour un commutateur de contrôler l'identité de la station et/ou de l'utilisateur avant de le laisser accéder au réseau (et le cas échéant de le placer dans un certain VLAN), grâce à la norme IEEE 802.1X.

Les normes IEEE 802.3af et IEEE 802.3at permettent à un commutateur d'alimenter électriquement un équipement raccordé en paire torsadée dans le cadre du concept de Power over Ethernet (PoE).

La section ci-dessous donne un bref résumé de tous les types de média d'Ethernet. En plus de tous ces standards officiels, plusieurs fabricants ont implémenté des types de média propriétaires pour différentes raisons—quelquefois pour supporter de plus longues distances sur de la fibre optique.




"(cf. cercle CREDO)"

Le standard Ethernet par seconde recouvre sept types de média différents pour les réseaux locaux, réseaux métropolitains et réseaux étendus. Il a été spécifié par le standard IEEE 802.3ae dont la première publication date de 2002, puis a été incorporé dans une révision de l'IEEE 802.3. La version Ethernet est 10 fois plus rapide que Gigabit Ethernet ; ceci est vrai jusqu'au niveau de la couche MAC seulement.


Ces deux familles de standards (40GBASE et 100GBASE) ont été initialement définies en 2010 sous la norme IEEE 802.3ba.

Ces deux famille de standards (200GBASE et 400GBASE) ont été définies en décembre 2017 sous la norme IEEE 802.3bs.

 Ethernet et ultérieurs supportent seulement le mode .

Sur les médias fibre, le mode LAN fonctionne à un débit ligne, au niveau de la fibre, de ce qui représente le débit MAC de pondéré par 66/64 rapport lié au codage de la couche PCS utilisant un code de ligne 64B66B. Le sur-débit de ce code est de 3 %, à comparer aux 25 % du code 8B10B du mode Gigabit Ethernet.

Il existe un mode WAN PHY permettant de transporter les trames Ethernet sur des liens SDH ou SONET encore en place dans beaucoup de réseaux. Le mode WAN PHY opère à un débit légèrement inférieur à 10Gbe, à savoir (ce qui correspond au débit STM64/OC192). Le conteneur virtuel 64c ou 192c véhicule des codes 64B66B.

Divers fabricants (Fiberxon, Sumitomo, Finisar, etc) proposent des modules optiques (ou cuivre, selon la technologie employée) appelés "transceivers" en anglais, permettant une interopérabilité. Ces modules permettent de convertir le signal optique (côté ligne) en un signal électrique différentiel (côté matériel) au débit de ; c'est donc l'équivalent de la couche PHY au niveau PMD du modèle OSI.

Il existe plusieurs normes pour ces "transceivers", par exemple (en 10 Gb/s) : XENPAK, XPAK, X2, XFP (normalisés selon le XFP MSA Group), SFP+ (normalisés selon le Small Form Factor Committee).

Ce signal de , trop rapide à l'époque de sa standardisation, ne pouvait pas être traité directement, il a donc fallu donc le paralléliser, en général sur . Des circuits dédiés spécialisés permettent cette conversion.
Le terme "serdes" vient de l'anglais pour "serialiser/deserialiser".

Le code en ligne utilisé 64B66B transforme le format XGMII ( de données plus de contrôle) en mots de . L'objectif est multiple :


Le code est composé de deux bits de synchronisation suivis de de donnée.

Les de données sont embrouillés par un embrouilleur auto synchronisé.

À ce niveau-là nous retrouvons un format équivalent MII, les couches suivantes : (MAC), (IP), (TCP/UDP) fonctionnant de façon similaire à gigabit Ethernet.


Ethernet dans le domaine des Télécommunications.



</doc>
<doc id="1041" url="https://fr.wikipedia.org/wiki?curid=1041" title="GNU Emacs">
GNU Emacs

GNU Emacs est l’une des deux versions les plus populaires de l’éditeur de texte Emacs (l’autre version est XEmacs). Ces deux versions sont majoritairement compatibles et ont de très nombreux points communs, décrits dans l’article Emacs.

Le manuel de GNU Emacs le décrit comme l’« incarnation GNU de l’éditeur plein écran avancé, auto-documenté, personnalisable et extensible qu’est Emacs ».

GNU Emacs a été développé par Richard Stallman depuis 1984 dans le cadre du projet GNU. Il s’appuie sur le langage Emacs Lisp.

GNU Emacs fait partie du projet GNU, et son développement reste actif encore de nos jours. Beaucoup de ses développeurs (mais pas tous) sont affiliés à la Free Software Foundation (FSF).

Stallman commence à travailler sur GNU Emacs en 1984, afin de produire un logiciel libre comme alternative à Emacs. Il fonctionne d’abord sur les systèmes Unix.

En 1991, à la suite de divergences d’opinion sur le développement, un fork du développement, dénommé "Epoch", donna naissance à Lucid Emacs, renommé plus tard XEmacs, offrant une version graphique et des fonctionnalités étendues pour l’époque. Cette version n’est pas approuvée par le projet GNU qui continue à développer GNU Emacs et pour lequel bon nombre des fonctionnalités mises en avant dans XEmacs existent désormais. Les deux versions se resynchronisent régulièrement.

Après un long temps de développement, la première version d’Emacs de la branche 22, la version 22.1, a été sortie le . Le développement de cette version a été assuré par Richard Stallman. Cette version apporte un nombre important de fonctionnalités ainsi qu’une refonte de l’interface graphique, reposant maintenant sur la bibliothèque GTK+.

Le , Richard Stallman annonce sa volonté d’arrêter d’être le mainteneur d’Emacs. Il laisse alors la place à Stefan Monnier et Chong Yidong.

Jusqu’en 1999, le développement de GNU Emacs était relativement hermétique, au point qu’il servait d’exemple pour le style « Cathédrale » dans l’ouvrage "La Cathédrale et le Bazar". Depuis lors, le projet a adopté une liste de diffusion publique sur le développement, et ouvert un accès anonyme à CVS.

Comme pour tous les projets GNU, il subsiste une règle particulière pour l’acceptation d’une partie de code significative : le détenteur des droits sur le code doit les céder à la FSF. Il existe toutefois une exception notable : le code de "MULE" (pour ""MULtilingual Extension"", « extension multilingue »), car il appartient au gouvernement japonais et la cession du copyright n’était pas possible.

Cette règle ne s’applique pas aux contributions mineures ou aux corrections de bugs. Il n’existe aucune définition rigoureuse de ce qu’est une "contribution mineure", mais il est habituel de considérer une contribution de moins de 15 lignes comme mineure.

Cette règle est prévue pour faciliter le respect du copyleft, afin que la FSF puisse défendre le logiciel devant un tribunal le cas échéant. Une telle obligation est connue pour avoir des effets négatifs sur les contributions. Certains affirment qu’elle affecte même les performances ; par exemple, l’incapacité de GNU Emacs à prendre en charge des fichiers volumineux de façon efficace serait à mettre sur le compte de cette obligation, qui découragerait les développeurs les plus sérieux. Mais d’après Richard Stallman, il est plus important que GNU Emacs soit libre que performant.

Le respect scrupuleux de cette règle permet d’asseoir la confiance juridique que l’on peut accorder à la licence libre de GNU Emacs (la GPL), et au logiciel libre lui-même, qui constitue le travail intellectuel de nombreux détenteurs de droits potentiels et contributeurs.

Historique des versions de GNU Emacs.

GNU Emacs est un logiciel libre distribué selon les termes de la licence publique générale GNU. 

Le code source ainsi que les fichiers binaires sont disponibles sur le serveur FTP du projet GNU ("cf. infra") et sur la plate-forme collaborative GNU Savannah. Les développeurs peuvent récupérer les sources en utilisant GNU Bazaar, le système décentralisé de contrôle des versions du projet GNU Emacs.

GNU Emacs est par ailleurs intégré dans tous les systèmes libres, notamment GNU/Linux, FreeBSD. Il fonctionne également sur de nombreux systèmes propriétaires dans le but de convaincre leurs utilisateurs de migrer vers des solutions libres.

Emacs s’est défini dès l’origine comme un éditeur extensible. Si les modules d’extension les plus populaires se retrouvent souvent intégrés, la plupart sont disponibles sur Internet. Tous les paquets tiers sont désormais accessibles depuis Emacs via un dépôt de code source exclusivement destiné aux extensions Emacs Lisp. Baptisé GNU ELPA (de l’anglais « "GNU Emacs Lisp Package Archive" »), le projet est en production depuis 2010. Son nom vient d’une plateforme similaire initiée par le dénommé Tom Tromey et du gestionnaire de paquets « "package.el" » d’un certain Phil Hagelberg.




</doc>
<doc id="1045" url="https://fr.wikipedia.org/wiki?curid=1045" title="Eicosane">
Eicosane

L'eicosane est un alcane linéaire de formule brute . Il possède 366319 isomères structuraux.


</doc>
<doc id="1047" url="https://fr.wikipedia.org/wiki?curid=1047" title="Estonie">
Estonie

L’Estonie, anciennement orthographié Esthonie, en forme longue la République d’Estonie, en estonien et , est un pays d’Europe du Nord et un État membre de l'Union européenne, situé sur la rive orientale de la mer Baltique et méridionale du golfe de Finlande. Le pays est bordé au nord par le golfe de Finlande, à l’ouest par la mer Baltique, au sud par la Lettonie et à l’est par la Russie. Ce pays est généralement regroupé avec la Lettonie et la Lituanie dans un ensemble géopolitique appelé pays baltes. Toutefois, depuis son retour à l’indépendance en 1991, l’Estonie cherche à se rapprocher des pays nordiques. L’Estonie fait partie de l’Union européenne depuis le et a intégré la zone euro le . Elle est membre de l’OTAN depuis le .

L’Estonie est un des trois pays baltes mais la langue utilisée se rattache à une branche complètement distincte de celles parlées en Lituanie et en Lettonie : l’estonien est une langue fennique, comme celles parlées en Finlande ou en Carélie (Russie), les langues fenniques font partie de la famille finno-ougrienne qui inclut les langues sames (Laponie linguistique).

Après avoir recouvré son indépendance en 1991, le pays, qui a opté pour un régime économique libéral, a connu une forte croissance économique grâce à une reconversion réussie de son industrie et la mise en place d’infrastructures modernes. L’Estonie est entrée violemment en récession lors de la crise économique de 2008-2010 avec notamment un taux de chômage supérieur à 14 %. Le gouvernement se fixe comme priorité de réduire l’inflation et de maîtriser les finances publiques. Le pays a, depuis 2009, le plus faible taux d’endettement public de l’Union européenne, remplit les critères de convergence et intègre la zone euro le .

L’Estonie est aussi membre de l’ONU, de l’OMC, du conseil de l'Europe, de l'OTAN, du Conseil des États de la mer Baltique ou encore de l’OCDE depuis 2010. Le pays est aussi observateur du Conseil nordique et de l’Organisation internationale de la francophonie depuis octobre 2010.

En octobre 2016, le nouveau musée national estonien a été inauguré à Tartu.

À l'issue de la dernière ère glaciaire, les premiers occupants à pénétrer sur le territoire estonien sont des populations nomades qui arrivent vers av. J.-C. Selon la théorie la plus répandue, le peuple finno-ougrien, dont descend la majorité des estoniens contemporains, arrive dans la région vers le millénaire en introduisant la céramique à peigne commune à plusieurs peuples rattachés à la même famille linguistique.

Au début du les rives sud de la mer Baltique constituent une des dernières contrées païennes d'Europe. Les croisades baltes (1200-1227), menées sur le territoire par un ordre de soldats templiers allemand, les chevaliers porte-glaive, réalisent la conquête du pays dont les habitants sont convertis à la foi chrétienne. Un État dominé conjointement par des princes-évêques et l'ordre des moines soldats, recouvrant à la fois le territoire de l'Estonie et de la Lettonie moderne, se met en place avec deux classes de population bien distinctes : d'une part une minorité d'origine allemande qui constitue l'élite politique, militaire, religieuse, intellectuelle et qui monopolise le commerce et la propriété foncière, d'autre part les paysans, finno-ougriens sur le territoire estonien, dont le statut va se dégrader au fil des siècles. Cette division perdure plus ou moins jusqu'en 1917. Entre 1418 et 1562, la région forme la Confédération livonienne. Au début du le pays, touché par la Réforme, opte pour le luthéranisme. Il est le théâtre de conflits qui l'opposent à des voisins de plus en plus puissants : la Russie, la Lituanie, la République des Deux Nations et la Suède. Finalement cette dernière annexe la région en 1595. Initialement, les souverains suédois ne remettent pas en cause la suprématie de la noblesse balte d'origine germanique descendante des chevaliers porte-glaives. Cette politique change avec la Grande Guerre du Nord. À compter de 1710 le territoire estonien devient pour deux siècles une région de l'Empire russe.

Au la noblesse foncière germanophone, à qui les dirigeants russes laissent une grande autonomie, maintient les paysans finno-ougriens dans le servage. Celui-ci n'est aboli qu'au début du siècle suivant en partie sous la pression du pouvoir russe, en partie grâce à quelques germanophones éclairés. Certains de ces derniers, qualifiés d'estophiles, s'intéressent à la langue, la culture et l'histoire des autochtones. Des intellectuels membres de la classe moyenne estonienne, qui commence à se former à cette époque, vont prendre le relais en faisant un travail de collecte de la mémoire populaire et en affinant la langue permettant l'apparition des premiers périodiques et ouvrages de fiction en estonien. À la fin du siècle la langue estonienne, dopée par une tentative de russification, commence à se substituer à l'allemand comme langue véhiculaire. En parallèle la proportion de paysans propriétaires s'accroît fortement. Au début du apparaissent les premiers partis politiques estoniens dont les revendications se cantonnent à une autonomie limitée et à l'égalité de statut avec les germanophones qui conservent une grande partie des pouvoirs.

Au cours de la guerre civile en Russie (1917-1922), la plupart des divisions militaires estoniennes (créées pendant la Première Guerre mondiale) combattirent contre l'Allemagne au côté des bolcheviks. Toutefois par la signature du traité de Brest-Litovsk, la Russie soviétique cède les États baltes à l'Empire allemand. Selon ce traité, l'Estonie, qui avait proclamé son indépendance le , aurait du être annexée par le Reich, mais la défaite allemande du lui permet de sauvegarder sa souveraineté, reconnue internationalement en 1919. Les terres agricoles encore détenues par la noblesse germanophone sont redistribuées aux paysans et un régime parlementaire s'installe. Celui-ci, menacé durant la Grande Dépression par la montée d'un mouvement populiste, se transforme en 1934 en régime semi-autoritaire.

En 1940, durant la Seconde Guerre mondiale, l'Estonie est d'abord envahie, comme le prévoyaient les clauses secrètes du Pacte germano-soviétique (en même temps que les deux autres pays baltes), par l'URSS qui y organise un "plébiscite" pour donner à l'annexion du pays l'apparence d'une légitimité. Toutefois, ni les États-Unis, ni le Parlement européen, ni la CEDH, ni le Conseil des droits de l'homme de l'ONU n'ont reconnu l'incorporation de l'Estonie parmi les 15 Républiques socialistes soviétiques ; de plus, la plupart des pays non-communistes membres de l'ONU ont continué à reconnaître "de jure" l'Estonie comme état souverain. Quelque furent déportés par les soviétiques et seule une minorité survécut au goulag ; ils furent remplacés après la guerre par des colons russes. Beaucoup d'Estoniens se réfugièrent dans la campagne où ils formèrent des « maquis ».

En 1941, l'Estonie est occupée par l'armée de la Wehrmacht, accueillie favorablement par une large majorité de la population en tant que libératrice après le régime de terreur du NKVD (il en sera de même sur tous les territoires soviétiques envahis durant les premières semaines de l'opération Barbarossa). Les élites germanophones quittent en masse le pays pour répondre à l'appel des autorités nazies. Lorsque l'Armée rouge revient en 1944, une partie de la population fuit par la mer par crainte de représailles et quitte définitivement le pays vers la Finlande ou la Suède. L'Estonie, malgré une longue résistance clandestine d'une partie de ses habitants, est transformée en une république socialiste intégrée dans l'URSS. La société estonienne et son économie sont profondément transformées par les Soviétiques. De nombreuses industries sont installées et l'agriculture est nationalisée. Une forte minorité russe s'installe pour diriger ces nouvelles activités. La dislocation de l'URSS en 1991 permet à l'Estonie de retrouver son indépendance à l'issue d'un processus pacifique. Le nouvel État se transforme rapidement grâce à une forte croissance de l'économie et la mise en place d'institutions politiques et économiques de type libéral. Soucieuse de conserver son indépendance face à un voisin russe qui souhaite retrouver son hégémonie, l'Estonie adhère à l'OTAN et intègre l'Union européenne en 2004.

L'Estonie est une démocratie parlementaire depuis le rétablissement de l'indépendance en 1991, néanmoins les lois actuellement en vigueur ne permettent aucune liberté d'expression, de rassemblement ou de droits politiques aux communistes.

Le mandat du Président de la République est de cinq ans. Il est élu au premier tour de scrutin par le "Riigikogu" (parlement) s'il obtient la majorité des deux tiers, et au second tour, si nécessaire, par un collège électoral composé des 101 députés du "Riigikogu" et d'un nombre d'élus locaux défini à chaque nouvelle élection. Son principal pouvoir est de nommer le Premier ministre qui doit obtenir la confiance du Riigikogu.

Élu pour la première fois en 2006, le président Toomas Hendrik Ilves a été réélu en août 2011. Jüri Ratas exerce la fonction de Premier ministre depuis le 23 novembre 2016.

Le Riigikogu est le nom estonien du parlement monocaméral de l'Estonie. Il comprend 101 députés, élus tous les quatre ans. L'Estonie étant une république parlementaire, le Riigikogu est le principal acteur du pouvoir estonien.

Riigi- vient de l'allemand "Reich" (État) et -kogu vient d'assemblée en estonien.

Les premières élections eurent lieu en 1920. Jusqu'en 1938, 5 autres élections se déroulèrent sur la base de trois constitutions différentes. Depuis 1922, les sessions du Riigikogu ont lieu dans le château de Toompea où une aile a été reconstruite pour devenir le bâtiment du parlement. En 1992, après 50 ans d'occupation soviétique, de nouvelles élections eurent lieu selon la nouvelle constitution adoptée durant l'été 1992.

Quatre partis politiques sont actuellement représentés au Riigikogu depuis les élections de 2011 et ont donc dépassé le seuil d'éligibilité de 5 %.

Le Riigikogu est entièrement équipé de matériel de vote informatique, les résultats sont transmis via internet et donc directement accessibles aux citoyens.

Son président actuel est Eiki Nestor du Parti social-démocrate (qui remplace Ene Ergma de l'Union de la patrie et Res Publica en mars 2014).


L'Estonie est un membre observateur au sein de l'Organisation internationale de la francophonie.

L'Estonie comprend 15 régions administratives, appelées "maakonnad" (au singulier "maakond") — le -maa signifie "pays" ou plus précisément "terre" :


D'une superficie () proche de celle des Pays-Bas (celle définie par le traité de paix de Tartu en 1920 était de ), l'Estonie est le plus septentrional des pays baltes, largement ouvert à l'ouest sur la mer Baltique, au nord sur le golfe de Finlande ( de côtes), bordé à l'est par la Russie (frontière de ) et au sud par la Lettonie (frontière de ). La côte estonienne est essentiellement rocheuse.

10 % du territoire est composé d'un archipel de plus de petites îles situées dans la Baltique dont les deux plus grandes sont Hiiumaa () et Saaremaa ().

La distance de Tallinn à Helsinki n'est que de alors qu'il faut pour aller à Rīga, pour rejoindre Saint-Pétersbourg et pour Stockholm.

L'Estonie est un pays de terres basses marécageuses. Des inondations ont régulièrement lieu au printemps. Le pays compte peu de cultures agricoles permanentes. 48 % du pays est constitué de bois et de forêts, la taïga, et 13 % de marais à tourbe. L'Estonie compte également plus de lacs. Le relief de l'Estonie est caractérisé par une altimétrie assez faible et un grand nombre de lacs et environ 150 rivières. Le point culminant est le Suur Munamagi, situé au sud-est du pays.

Le lac Peïpous est le quatrième plus grand lac d'Europe après les lacs Ladoga et Onega en Russie et le Vänern en Suède. Il ressemble à une véritable mer intérieure du point de vue de sa superficie et sert de frontière à l'est avec la Russie. Il est gelé en hiver pendant quatre mois et est navigable pendant les huit autres mois de l'année. À l'inverse, l'été avec les longues journées ensoleillées estoniennes, le lac est propice à la baignade et de nombreux Estoniens et Finlandais sont attirés par les plages de dunes sur son côté nord. Il présente même de nombreux campings gratuits, mode d'hébergement favori dans les pays nordiques. Le reste du lac est par contre davantage composé de marécages.

Grâce au courant Nord-Atlantique chaud, toute l'Europe du Nord (dont l'Estonie) jouit d'un climat considérablement plus doux que, par exemple, les mêmes latitudes en Amérique du Nord. La mer Baltique cause de grandes différences de climat entre les zones côtières et continentales.

Le climat est caractérisé par un hiver plutôt froid, un printemps doux et un peu pluvieux, un été relativement chaud et un long et doux automne (température moyenne en juillet + ; température moyenne en février ). Les premières neiges apparaissent vers novembre. La température peut descendre en dessous de l'hiver. Le mois le plus sec est le mois de mars avec en moyenne alors que la pluviométrie est la plus élevée au mois de juin avec une moyenne de .

Comme dans les autres pays nordiques, la latitude élevée de l'Estonie engendre une importante différence de lumière de jour entre l'hiver et l'été.

Les journées sont plus courtes au solstice d'hiver :
À l'inverse, les journées sont plus longues au solstice d'été ;

Le nombre annuel d'heures ensoleillées varie entre et , ce nombre étant plus élevé sur la côte et les îles et plus faible à l'intérieur du pays. Cela correspond à moins de la moitié de la quantité maximale de soleil possible.

Les Estoniens, comme les autres populations nordiques, sont très proches de la nature et soucieux de la préservation de l'environnement. L'Estonie pratique le libre droit d'accès à la nature comme la Finlande. Le camping sauvage est autorisé partout hors des villes et des endroits qui mentionnent une interdiction spécifique.

Une initiative de dépollution de grande ampleur a été mise en place en 2008 au niveau national par l'association "Teeme Ära", devenu par la suite Let's do it! World au niveau international. Les zones polluées par de nombreux déchets ainsi que les décharges sauvages ont été localisées par images satellite et par des citoyens qui renseignaient une base de données. Les coordonnées GPS de chaque endroit ont ensuite été communiquées aux participants qui pouvaient localiser les zones proches de chez eux et y intervenir pour s'occuper des déchets. Plusieurs dizaines de milliers d'Estoniens ont participé à ce projet. Cette expérience fut accompagnée d'une vaste campagne de sensibilisation. 80 % des déchets collectés par les bénévoles ont été recyclés.

En 2005, le PIB/habitant était de , le PIB en standard de pouvoir d'achat (SPA) par habitant de et le taux d'inflation de 4,1 % (2005). En juillet 2006, le taux de chômage était de 4,2 %.

L'Estonie se trouve dans une région d'Europe à fort potentiel économique, autour de la mer Baltique. Ces dernières années, elle a connu une croissance rapide (8,1 % en 2004, de 10,5 % en 2005 et de 11,4 % en 2006, selon Eurostat). Elle appartient, depuis 2001, au premier groupe des pays à fort niveau de développement humain ( sur 174).

L’une des plus libérales d'Europe du Nord, l’économie estonienne exporte machines-outils, équipements électriques et électroniques (comme les pièces de téléphonie mobile), logiciels et services liées aux NTIC, bois et produits textiles.

Microsoft Skype est une entreprise qui commercialise son logiciel propriétaire et le service lié de voix sur IP (VoIP) développé par les programmeurs Ahti Heinla, Priit Kasesalu et Jaan Tallinn pour les entrepreneurs Niklas Zennström et Janus Friis. Les trois Estoniens étaient déjà à l'origine du logiciel Kazaa.

L'Estonie est régulièrement citée comme modèle dans l'adoption des technologies de l'information et des télécommunications. Anneli Kavald, chargée de mission à l’Institut estonien en France, établit sur ce point une comparaison d'ordre culturel avec la France : « les Estoniens sont beaucoup plus réceptifs en matière de NTIC que les Français, qui, habitués au Minitel, ont parfois eu du mal à passer à autre chose. Et puis les Estoniens sont partis de zéro et cela leur a permis d’acquérir à une vitesse supérieure tout ce qu’il y avait à acquérir en matière de connaissances, même au niveau d’un simple utilisateur. Nous sommes très branchés mais sans forcément nous en rendre compte car, pour nous, il s’agit d’une norme. Nous nous plaignons parfois quand nous voyageons car, ailleurs, ces services ne sont pas obligatoirement disponibles. ».

L’économie, très dépendante sur le plan financier des banques suédoises, s’est révélée très fragile. La crise financière de 2008 a provoqué une débâcle dans ce petit pays baltique qui avait formé sa propre bulle immobilière : entre juin 2008 et juin 2009, le chômage a doublé, le PIB a reculé de 15 %, la production industrielle de 34 %. Le gouvernement tente de renverser la situation essentiellement par des coupes budgétaires afin de pouvoir remplir les conditions d'entrée dans la zone euro dès 2011. On attendait pour 2009 une contraction du PIB comprise entre -14 % et -15 % tandis que le pays connaîssait désormais la déflation qui a atteint - 0,1 % en 2009.

Le pays renoue avec la croissance à partir de 2010, et le gouvernement estime que l'Estonie retrouvera le niveau de PNB d'avant la crise économique à horizon 2015. Quant au taux de chômage, il s’élève en décembre 2011 à 11,7 % contre 15,2 % en janvier 2010, selon Eurostat.

À l'issue de la Première Guerre mondiale, plusieurs monnaies circulaient en Estonie, dont le mark allemand et le rouble soviétique. Elles furent remplacées en 1918 par le mark estonien, à parité avec le mark allemand. Après plusieurs dévaluations, celui-ci fut remplacé le par la couronne estonienne au taux de 1 couronne pour 100 marks. Cette première couronne estonienne fut à nouveau dévaluée en 1933 lors de la crise économique.

À la suite de l'invasion soviétique de 1940, la couronne estonienne se trouva remplacée par le rouble soviétique au taux de pour 0,8 couronne.

Après l'indépendance, une nouvelle couronne estonienne ("eesti kroon" ; abréviation internationale EEK), fut introduite en juin 1992 à parité fixe avec le mark allemand (1 DEM = 8 EEK). Cette nouvelle monnaie rejoint le mécanisme de taux de change européen II (MCE II) le , en vue d'une adoption de l'euro initialement prévue en janvier 2007 ( = , ± 15 %). Mais une inflation trop importante (environ 4 % sur 12 mois) retarde le passage à l'euro jusqu'à 2011.

Depuis le , la monnaie nationale est l'euro, avec une parité fixe de = 15,6466 EEK. Les pièces en euro de l'Estonie représentent toutes la carte du pays.

Afin de continuer à promouvoir la dématérialisation de toutes ses administrations commencée avec la promotion de sa dorsale internet, le X-Road, L'Estonie envisage d'émettre sa propre cryptomonnaie, le Estcoin

En 2010, la population de l'Estonie s'élève à habitants, contre en 2000. La démographie est marquée par une perte sensible de population depuis la fin des années 1990 (-4,9/1000 en 1998 ; -3,8/1000 en 1999), en raison du départ d'une partie de la population, comme dans les autres pays baltiques, mais surtout d'un indice de fécondité faible (1,37 enfant par femme en 2000 et 1,64 en 2010).

Les principales villes sont : Tallinn ( habitants), Tartu ( habitants), Narva ( habitants), Pärnu ( habitants).

Les Russes représentent la minorité la plus importante d'Estonie (environ personnes) et la plus ancienne. Elle est loin d'être homogène car elle est constituée de plusieurs sous-groupes dont l'arrivée sur le sol estonien est échelonnée dans le temps.

Tout au long de l'histoire de l'Estonie, des Russes se sont installés dans les villes pour occuper des métiers d'artisans et de commerçants : environ Russes étaient présents ou sont descendants de Russes présents avant l'invasion soviétique de 1939. 

Parmi ceux-ci, les vieux-croyants forment une communauté d'environ personnes, liée à l'Église vieille-orthodoxe pomore, pourchassée par le pouvoir tsariste et l'église officielle (Patriarcat de Moscou et de toute la Russie), et s'est installée au sur les rives du lac Peïpous. 

Le reste de la communauté s'est installé durant la période soviétique pour occuper les emplois générés par la construction en Estonie d'importants complexes industriels. Cette partie de la communauté russe liée à l'Église orthodoxe d'Estonie (Patriarcat de Moscou), qui représente environ 20 % de la population et qui est de plus fortement concentrée dans les villes industrielles du Nord-Est (Narva) et dans la capitale, est généralement mal intégrée. Peu parlent l'estonien et, vivant en communauté fermée, ils ont peu de contacts avec les Estoniens qui ne cherchent de toute façon pas à les fréquenter. Malgré tout, ont opté depuis l'indépendance pour la nationalité estonienne après avoir passé avec succès un examen linguistique et culturel tandis que , souvent âgés, choisissaient la nationalité russe. Environ russophones n'ont pas voulu trancher et sont depuis reconnus par l'État estonien comme étant des « non-citoyens ». Aujourd'hui encore, ils sont apatrides puisque leur pays d'origine, l'Union soviétique, a depuis disparu. Peu à peu, une partie de cette minorité russe quitte le pays.

D'autres minorités sont présentes : Biélorusses, Ukrainiens, Setus, juifs.

Certaines minorités ont disparu ou sont en voie de disparition : juifs, Allemands, ingriens, Suédois.
Les Luciuspeti
Une vieille tradition tant à nous faire penser que les luciuspeti sont apparus au cours de la renaissance estonienne, en 1436. ils étaient connus par l'intermédiaire de leur tradition, souvent moquée dans l'Europe entière, de se saluer en se léchant le bras. Ceci pour tester la capacité de l'interlocuteur à subir un bras mouillé, la pluie étant redondante.

Les Suédois d'Estonie (suédois : "Estlandssvenskar") sont les populations de langue suédoise qui se sont installées en Estonie, notamment pendant la période de colonisation du pays par la Suède (1561 - 1721). Comme du côté finlandais, ils résidaient principalement dans les îles du golfe de Riga (notamment Hiiumaa, Ruhnu, Naissaar et Vormsi) et sur les côtes ouest et nord du pays. La communauté suédoise, installée dans ces lieux depuis le et qui comptait alors membres a quitté le pays durant la Seconde Guerre mondiale. Il ne reste aujourd'hui que quelques centaines de Suédois d'Estonie.

La langue officielle de l'Estonie est l'estonien, qui est la langue maternelle de 69 % de la population du pays ainsi qu'une langue étrangère maîtrisée par 14 % de la population, ce qui fait que 82 % des habitants du pays savent parler estonien. Le russe est également très présent avec 30 % des habitants l'ayant comme langue maternelle et 42 % comme langue étrangère, 72 % des habitants comprennent ainsi le russe. La troisième langue la plus connue du pays est l'anglais, qui, bien qu'elle ne soit uniquement parlée comme langue maternelle par une infime minorité (0,07 %) est comprise par 38 % des habitants du pays.

De nos jours, les jeunes Estoniens privilégient l'anglais pour le plus grand nombre, comme langue étrangère.

L'allemand vient en seconde position et est très présent chez l'élite, et dans les domaines culturels et le tourisme.

Pour des raisons culturelles et historiques, les langues suédoise et finnoise sont aussi étudiées.

L'Estonie est un membre observateur de l’Organisation internationale de la francophonie, ce qui traduit la présence plus ou moins importante d'une certaine francophonie sur le territoire. Deux établissements participent à la promotion de la langue française : l’Institut français d’Estonie et l'école française de Tallinn.

De tout temps, l'Estonie s'est trouvée dans la sphère de culture européenne. Tallinn (Reval à l'époque) était, au Moyen Âge, la ville la plus orientale de la ligue hanséatique.

Forte des diverses cultures qui se sont côtoyées et succédé du fait des occupations successives, l'Estonie s'est forgé une culture particulière faite de tolérance et de respect envers l'étranger, quels que soient son pays ou sa culture. L'Estonie compte de nombreuses minorités : les Russes représentent 25,7 % de la population. Viennent ensuite les Ukrainiens : 2,1 % de la population ; 1,2 % de la population est biélorusse et 0,8 % finnoise... L'importance de la population russophone vient naturellement de l'occupation soviétique et de l'industrialisation forcenée dont l'Estonie avait fait l'objet à l'époque.

L'estonien n'est pas une langue indo-européenne mais finno-ougrienne de même que le finnois et le hongrois. L’estonien littéraire naît tardivement, entre les . Elle est surtout utilisée par des pasteurs allemands pour transmettre la littérature religieuse. Le plus ancien livre en estonien est le catéchisme de Wanradt et Köll, publié en 1535 à Wittenberg. On remarquera que c'est la Réforme qui est à l'origine de ce livre.

Le voit la naissance de la littérature nationale, et la langue écrite se répand par les almanachs et journaux, colportés jusqu’au fond des campagnes. La littérature est alors composée de récits imités d’œuvres allemandes. À partir de 1820, Kristjan Jaak Peterson est à l’origine de la poésie estonienne moderne. Dans les années 1850, à la suite des mouvements nationaux et romantiques, la littérature connaît un véritable essor, avec notamment la redécouverte du folklore national et la rédaction de l’épopée nationale, le "Kalevipoeg", composée par Friedrich Reinhold Kreutzwald, publiée entre 1857 et 1861 (voir , un conte typiquement estonien) dans les publications de la Société savante estonienne. L'édition populaire a été publiée en 1862 en Finlande. À cette période, entre 1860 et 1885, la nation estonienne prend conscience d’elle-même, et la littérature se développe rapidement. La poésie est un genre particulièrement vivace (et le reste aujourd’hui), symbolisée à cette époque par l’une des grandes poétesses de ce pays, Lydia Koidula. Comme dans le reste de l’Europe, la fin du voit le développement d’une littérature réaliste, en particulier avec Eduard Vilde.

Peu après, la littérature s’ouvre de plus en plus aux courants occidentaux, avec le groupe des « Jeunes Estoniens ».
C’est dans ce contexte qu’émerge l’une des figures estoniennes les plus connues à l’étranger, celle de la poétesse Marie Under. Les années vingt voient le retour du réalisme, avec Anton Hansen Tammsaare. La période de l’entre-deux-guerres, celle de l’indépendance, contraste fortement avec la suivante, celle de l’exil pour les uns, de la déportation en Sibérie pour les autres. La littérature estonienne en exil demeure très vivace, pour preuve les volumes en estonien qui sont parus entre 1945 et nos jours. En Estonie devenue soviétique, la littérature « bourgeoise » est brûlée, interdite, censurée, etc. Un certain renouveau se déclare après la mort de Staline, avec les débuts de grands auteurs comme Viivi Luik et Jaan Kaplinski, mais surtout le monument Jaan Kross qui est publié chez Robert Laffont. Il est l'auteur notamment du "Fou du Tzar" (1978), prix du meilleur livre étranger 1989. « Ses romans, aujourd'hui traduits en de nombreuses langues, font revivre pour la plupart des figures importantes de l'Histoire estonienne ou des Estoniens ayant atteint dans leur domaine une certaine notoriété internationale » comme le baron balte Timotheus von Bock du Fou du Tzar.

Une fois le retour de l’indépendance, l’Estonie libre retrouve une belle vitalité littéraire, marquée par l’émergence de nombreux jeunes auteurs, comme Tõnu Õnnepalu, en particulier grâce aux généreuses subventions de la Fondation pour la culture.

La musique est indissociable de la culture nationale, les Estoniens n'ont-ils pas été qualifiés de « Peuple chantant » ? Le premier festival pan-estonien de chant a eu lieu en 1869 à Tartu, où près de mille chanteurs et musiciens venus de tout le pays furent réunis. Aujourd'hui cette fête rassemble trente mille chanteurs et musiciens devant un public de personnes. Ces traditions ont inspiré en 1988 la « Révolution chantante », c'est en chantant que l'Estonie s'est libérée du joug soviétique. En 2001, l'Estonie a remporté le concours de l'Eurovision.

Il existe deux grands théâtres en Estonie : le théâtre Estonia à Tallinn fondé en 1865, le théâtre Vanemuine à Tartu fondé en 1883. Tous les registres y sont abordés.

Le cinéma estonien compte pour une très faible partie (2 %) du taux d'audience cinématographique du pays, mais est très productif surtout en ce qui concerne les films d'animation et documentaires. Un festival est proposé chaque été, consacré au film anthropologique à Pärnu et en hiver c'est à Tallinn que se déroule le « Festival de cinéma des nuits noires ».

Le cycliste Jaan Kirsipuu a été vainqueur de nombreuses étapes du Tour de France. À Sydney, la médaille d'or du décathlon a été remportée par Erki Nool, natif de Võru. À Pékin, c'est le discobole Gerd Kanter déjà champion du Monde à Osaka en 2007, qui décroche l'or olympique. Il succède à Erki Nool, sacré en 2000 à Sydney et à Jaak Uudmae — Estonien sautant pour l'URSS — vainqueur du triple saut en 1980 lors des Jeux de Moscou. Dans les sports d'hiver, les athlètes estoniens sont très productifs (une médaille d'or, une d'argent et une de bronze aux Jeux olympiques de Salt Lake City en 2002 (se plaçant devant la Suède et le Royaume-Uni) et trois médailles d'or aux Jeux olympiques de Turin en 2006. À noter que le champion d'échecs Paul Keres (1916-1975), au sommet de l'élite dans les années 1930-1960, était estonien et a concouru pour le pays, puis pour l'URSS. Il a même eu droit à son effigie sur le billet de banque de cinq couronnes. En rallye, Markko Märtin a remporté plusieurs épreuves au volant de la Ford Focus WRC puis de la Peugeot 307 en 2003, 2004 et 2005. Enfin, la joueuse de tennis Kaia Kanepi est devenue ces deux dernières années l'une des athlètes les plus populaires de son pays en intégrant le top 20 du classement WTA en 2010, après avoir notamment atteint les quarts de finale à Wimbledon et à l'US Open de tennis.

L'armée estonienne est de constitution récente. En 2011, 1,9 % du PNB est consacré à la défense, soit un budget de la défense de 280 millions d'euros. Ayant adopté une attitude prudente face à la Russie, l'Estonie compte sur l'OTAN pour protéger son espace aérien et sur l'Union européenne en cas de crise internationale. Elle participe à plusieurs missions à l'étranger sous le commandement des Nations unies ou de l'OTAN. Les forces estoniennes sont présentes en Afghanistan et un contingent est impliqué dans la guerre en Irak. Les forces estoniennes font partie de la KFOR au Kosovo et de la Force intérimaire des Nations unies au Liban renforcée. L'armée de l'air possédant plusieurs hélicoptères et avions légers de transport ainsi qu'une centaine de batteries anti-aériennes, son réseau radar est relié à celui de l'OTAN.

Les forces militaires de l'Estonie ont introduit une nouvelle formation basée sur la cyberguerre et la défense des infrastructures électroniques et infrastructures essentielles de la république d'Estonie. Actuellement, la principale organisation de cyber-défense estonienne est le CERT ("Computer Emergency Response Team of Estonia"), créée en 2006, comme organisation responsable de la gestion des incidents de sécurité dans des réseaux informatiques estoniens. Son but est de réduire le plus possible les dommages liés aux incidents de sécurité en répondant efficacement aux nouvelles menaces. L'Estonie a connu une série de cyber-attaques qui ont commencé le 27 avril 2007. Les dirigeants estoniens attribuent ces attaques aux autorités russes, lesquelles démentent. Le 25 juin 2007, le président estonien Toomas Hendrik Ilves a rencontré le président des États-Unis, George W. Bush. Parmi les sujets abordés, il y avait notamment les attaques sur l'infrastructure électronique estonienne. Ces attaques ont provoqué, dans un certain nombre d'organisations militaires mondiales, une reconsidération de l'importance de la sécurité de réseau dans la doctrine militaire moderne. Le 14 juin 2007, les ministres de la Défense de l'OTAN ont tenu une réunion à Bruxelles, publiant un communiqué sur une action immédiate commune. Cette action permit de mettre fin aux attaques à l'automne 2007. L'OTAN s'apprête à mettre en place en Estonie son futur centre cybernétique de défense, les Estoniens formeront ainsi les spécialistes du cyber-terrorisme, du cyber-espionnage et de la cyber-défense pour les forces de l'alliance atlantique.

Le , le président des États-Unis Barack Obama, alors en visite en Estonie où se déroule au même moment la guerre du Donbass en Ukraine, pays qui n'est pas membre de l'OTAN, et dont la Russie est accusée notamment par les membres de l'OTAN, d'envoyer des troupes de soldats russes sur sol ukrainien pour soutenir les pro-Russes, déclare que l'Estonie « ne sera jamais seule ».

L'Estonie a pour codes :




</doc>
<doc id="1048" url="https://fr.wikipedia.org/wiki?curid=1048" title="Estonien">
Estonien

L’estonien est une langue appartenant à la branche fennique de la famille des langues ouraliennes. Il est étroitement apparenté au finnois et plus lointainement au hongrois. Il est parlé par environ personnes, dont la très grande majorité () habite en Estonie.

Il peut y avoir inter-compréhension entre un locuteur du finnois et un locuteur estonien : les difficultés seraient de l'ordre de tournures grammaticales différentes, mais ce sont surtout les accents différents qui apporteraient des difficultés. Enfin, le finnois a intégré un certain nombre de mots suédois dans son vocabulaire. Pour les Estoniens, ce sont des mots de vocabulaire d'origine allemande, bas-allemande ou russe.

Le SIL International, organisme chargé d'attribuer les codes ISO 639-3, classe l'estonien comme une macro-langue (codice_1) et y inclut l'estonien standard (codice_2) et le võro (codice_3).

La base de données linguistiques Glottolog ne reconnait pas quant à elle le võro et inclut trois variétés dans l'estonien :

L'alphabet estonien comporte 27 lettres et est ordonné ainsi :

A a, B b, D d, E e, F f, G g, H h, I i, J j, K k, L l, M m, N n, O o, P p, R r, S s, Š š, Z z, Ž ž, T t, U u, V v, Õ õ, Ä ä, Ö ö, Ü ü

Les positions de "z" et "ž" dans cet alphabet sont à noter, ainsi que l'appartenance à cet alphabet des lettres diacritiquées en tant que lettres à part entière.

Les lettres "F", "Š", "Z" et "Ž" sont quatre lettres rencontrées uniquement dans certains mots d'emprunts. Il est également important de noter l'absence des lettres "C", "Q", "W", "X" et "Y", qui ne sont utilisées que dans des noms propres et ne le sont pas dans les mots de racine estonienne et sont exclues de l'alphabet officiel estonien.

Quelques-unes des lettres de l'alphabet ont des prononciations particulières ou différentes du français : "õ" note /ɤ/ (une voyelle postérieure non arrondie, semblable à un "o" français, mais vraiment postérieur, prononcé sans arrondissement des lèvres) ; "ä" produit le phonème /æ/ comme en finnois ("a" très ouvert comme dans "cat" en anglais) ; "ö", "ü" produisent respectivement les phonèmes /ø/ et /y/ comme en allemand. Quant aux lettres "š" et "ž", elles se prononcent respectivement /ʃ/ et /ʒ/ qui correspondent au "ch" et "j" du français.

Hormis ces quelques lettres, il faut prêter attention au fait que "E" se prononce [e] ("é"), que le "R" est roulé, que le "S" est toujours dur (comme dans le mot "jadis") et enfin, que le "U" a la même prononciation que "ou" en français. Il faut également veiller à aspirer le "H".

Enfin, en estonien, une voyelle double note une voyelle longue et une double consonne note une consonne géminée. Ceci a une valeur phonémique, qui peut différencier deux mots.

Sur le plan phonologique, l’estonien se caractérise notamment par l’existence de trois durées vocaliques et consonantiques : la plupart des phonèmes peuvent être brefs, longs ou surlongs. Cette présentation des faits a néanmoins été remise en question dans les années 1990. Plutôt que de décrire la durée des phonèmes, de nombreux linguistes préfèrent aujourd’hui décrire les trois « durées » syllabiques (voire des groupes de deux syllabes) et réduisent le système ternaire traditionnel à un emboîtement de deux oppositions binaires : les syllabes accentuées peuvent être brèves ou longues et les syllabes longues peuvent porter un « accent » fort ou faible, le terme d’« accent » désignant ici un ensemble de traits essentiellement prosodiques comprenant l’énergie articulatoire, la courbe intonative et la longueur relative de la syllabe accentuée et de la syllabe suivante.

L’accent tonique est sur la première syllabe des mots, sauf dans les mots d'emprunt relativement récents, où il s'est souvent maintenu à la place qu'il avait dans la langue d'origine.

L'orthographe ne distingue pas les phonèmes longs et surlongs ; les uns comme les autres sont notés par une lettre double, tandis que les phonèmes brefs sont notés par une lettre simple. La seule exception concerne les occlusives, pour lesquelles trois graphies différentes existent : les brèves sont notées "b", "d", "g", les longues "p", "t", "k", et les surlongues "pp", "tt", "kk".

Typologiquement, l’estonien représente une forme de transition entre langue agglutinante et langue flexionnelle. Il a subi au cours de son histoire une forte influence de l'allemand, dans son vocabulaire comme dans sa syntaxe. Il a par exemple développé un système de verbes à particules dont la forme et le fonctionnement rappellent les verbes à particules séparables de l'allemand.

L’estonien ne possède pas d’articles et ne connaît pas le genre grammatical. La déclinaison comprend 14 cas : nominatif, génitif, partitif, illatif, inessif, élatif, allatif, adessif, ablatif, translatif, terminatif, essif, abessif et comitatif.

L’une des particularités de ce système casuel est l'absence d'accusatif ; le complément d'objet peut être marqué, selon les contextes, par le nominatif, le génitif ou le partitif.

L’adjectif épithète s'accorde en cas et en nombre avec le substantif qu'il détermine, sauf au terminatif, à l'essif, à l'abessif et au comitatif où il n'y a pas d'accord en cas (l'adjectif est alors au génitif).

Le système verbal se caractérise par l'absence de temps dédié au futur (le « présent » est le temps du « non-passé ») et par l’existence de formes spéciales pour exprimer l'action accomplie par une personne indéterminée (l’équivalent du « on » français) ainsi que le discours rapporté (mode verbal spécifique appelé « mode oblique » ou « médiatif »). Il existe au moins deux infinitifs : le premier, terminé par le suffixe "-ma", est la forme qui figure dans les dictionnaires; il est utilisé par exemple après les verbes signifiant « devoir » ou « commencer à ». Le deuxième infinitif, terminé surtout en "-da" ou en "-ta" (mais aussi en "-la", "-na" ou "-ra"), s’utilise par exemple après les verbes signifiant « pouvoir », « vouloir », « aimer ». Certaines grammaires considèrent aussi comme un infinitif spécifique la forme en "-vat" (correspondant au médiatif présent) lorsqu'elle est employée après un verbe d'apparence (signifiant « sembler »).

La première transcription connue d’un mot estonien remonte peut-être au : dans sa "Cosmographie", Aethicus Ister mentionne une île du nom de Taraconta (Tharaconta). Certains auteurs pensent qu’il désignait peut-être par là l’Estonie ou sa plus grande île, Saaremaa. "Taraconta" peut en effet être interprété comme "Taara" + "kond". Taara était, selon certains, l’un des principaux dieux des anciens Estoniens ; le suffixe "-kond" désigne quant à lui une communauté de personnes, comme dans le mot "perekond" « famille », ou une entité territoriale, comme dans "maakond" « province ». "Taraconta" pourrait ainsi désigner les Estoniens comme les adorateurs de Taara.

À partir du , des sources écrites plus abondantes permettent d’avoir une idée plus précise de l’état de développement de la langue. C’est en effet à cette époque que les croisés allemands et scandinaves atteignent l’Estonie, qui était alors l’une des dernières terres païennes d’Europe. Les croisades contre les Estoniens ont été décrites au cours de la première moitié du dans la chronique latine "Heinrici Chronicon Livoniae" (chronique d'Henri le Letton), qui contient des mots et des fragments de phrase en estonien.

De nombreux noms propres et toponymes estoniens sont également attestés dès le . Un rôle d’impôt danois (""), établi entre 1219 et 1220, comprend environ 500 toponymes du nord de l’Estonie.

À la suite des croisades, une noblesse et une bourgeoisie allemandes s’établirent sur le territoire de l’ancienne Livonie, qui couvrait l’Estonie et la Lettonie actuelles. Bien que l’Estonie ait changé plusieurs fois de maître au cours de sept siècles d’occupation étrangère (Danemark, Pologne, Suède, Russie), l’estonien fut surtout influencé par le bas-allemand et le haut-allemand, ainsi que par le dialecte allemand de la Baltique qui se développa à partir d’eux. En particulier, le vocabulaire lié à la ville et la modernité s'inspire largement de l'allemand.

Le premier texte estonien conservé est celui du manuscrit de Kullamaa, qui date des années 1524-1528. Il s’agit d’une traduction des principales prières catholiques (« Notre Père », « Je vous salue Marie » et « Je crois en Dieu »). Lorsque la Réforme parvint en Estonie, la prédication en langue vernaculaire rendit nécessaire la traduction des textes religieux en estonien du nord et en estonien du sud.

Les premières grammaires et les premiers dictionnaires furent rédigés au . On dispose depuis cette époque d’un nombre important de textes conservés.

Au cours du Réveil national qui se produisit au milieu du , l’estonien, qui n’était auparavant que la langue des paysans, devint rapidement une langue de culture, notamment grâce à l’Université de Tartu, un des principaux foyers intellectuels. Il commença à être utilisé en littérature et dans les sciences. À la même époque furent publiées les premières études linguistiques en estonien. En 1884, Karl August Hermann fit paraître la première grammaire estonienne en estonien, qui contribua de façon importante à la standardisation de la langue.

Dans la deuxième moitié du , la population autochtone commença à se désigner sous le nom d’"eesti", probablement emprunté deux siècles plus tôt au suédois ou à l’allemand. Auparavant, la majorité des Estoniens se désignaient sous le nom de "maarahvas" « les gens du pays » et appelaient leur langue "maakeel" « la langue du pays ».

Durant les premières décennies du , les intellectuels estoniens se donnèrent pour mission de développer leur langue pour l’adapter à la culture européenne moderne. Un rôle important dans ce processus fut joué par le linguiste (et professeur de français) Johannes Aavik, qui s’efforça d’enrichir et d’embellir la langue littéraire. Il utilisa abondamment les ressources fournies par le finnois et les dialectes, mais créa également des mots et des morphèmes grammaticaux artificiels. Le français inspira nombre de ses propositions. Parallèlement à cette « rénovation linguistique » ("keeleuuendus") lancée par Aavik, un autre courant, dirigé par Johannes Voldemar Veski, se concentra sur l’élaboration des normes et le développement de la terminologie. Plusieurs milliers de termes, dans tous les domaines du savoir et de la vie, furent créés pendant cette période. Au cours du , un rôle essentiel dans la fixation de la langue standard fut joué par les dictionnaires normatifs. Le premier d’entre eux parut en 1918.

Pendant la période soviétique (1940-1991), la standardisation de la langue et le strict respect des normes devinrent une forme de résistance nationale. C’était une façon de s’opposer à l’idéologie soviétique, symbolisée par la langue russe. La langue était l’un des constituants fondamentaux de l’identité estonienne. Les autorités n’avaient d’ailleurs interdit ni l’étude scientifique de l’estonien ni son emploi dans aucun domaine de la vie publique (y compris l’éducation), ce qui permit aux Estoniens et à leur langue de résister à la russification et à la colonisation.
Dans les années 1990, les attitudes à l’égard de la norme linguistique se sont assouplies. Les sociolectes et autres variétés linguistiques non standard sont revenus à l’honneur.

Le 2004, l’estonien est devenu l’une des vingt langues officielles de l’Union européenne.






</doc>
<doc id="1050" url="https://fr.wikipedia.org/wiki?curid=1050" title="Essif">
Essif

En linguistique, l’essif est un cas grammatical exprimant un état ou une qualité. Il peut s'exprimer en français au moyen des locutions comme "en tant que", "en qualité de" ou "comme". Dans d’autres langues - en particulier le finnois et l’estonien -, il s’exprime au moyen d'un suffixe spécifique qui est ajouté au mot de base en finnois, et à la forme du génitif en estonien :

En finnois, l’essif peut aussi exprimer le temps: "maanantaina" → "au lundi", "kuudentena tammikuuta" → "au 6 janvier". Dans certaines expressions, il est utilisé avec son sens ancien de locatif : "Luen lehtiä kotona" → "Je lis les journaux à la maison" (i.e.: "dans le contexte de la maison") . Cette signification s’oppose à celle de l’inessif, qui veut dire "à l’intérieur de".

Le basque présente un tel cas grammatical, utilisant le suffixe -tzat directement accolé à la racine. Cette même combinaison peut également exprimer le cas translatif. Les grammaires basques cependant l'appellent et l'ont toujours appelé "prolatif", bien que ne correspondant pas à l'acception de ce terme pour les linguistes.


</doc>
<doc id="1051" url="https://fr.wikipedia.org/wiki?curid=1051" title="Espace vectoriel">
Espace vectoriel

En mathématiques, plus précisément en algèbre linéaire, un espace vectoriel est un ensemble muni d'une structure permettant d'effectuer des combinaisons linéaires.

Étant donné un corps K, un espace vectoriel "E" sur K est un groupe commutatif (dont la loi est notée +) muni d'une action « compatible » de K (au sens de la définition ci-dessous). Les éléments de "E" sont appelés vecteurs (ou points ), et les éléments de K des scalaires.

Pour une introduction au concept de vecteur, voir l'article introductif "Vecteur".

Soit K un corps commutatif, comme le corps commutatif ℚ des rationnels, celui, ℝ, des réels ou celui, ℂ, des complexes (on parlera dans ces cas d'espace vectoriel rationnel, réel ou complexe).

Un espace vectoriel sur K, ou K-espace vectoriel, est un ensemble "E", dont les éléments sont appelés vecteurs, muni de deux lois :
telles que les propriétés suivantes soient vérifiées.

De l'axiome 1, il découle que "E" est nécessairement non vide. En effet "E" contient au moins 0

Les axiomes 1 et 2 impliquent que 0 est « absorbant à droite » pour la loi • (i.e. le produit de 0 par un scalaire quelconque vaut 0) et que le produit d'un vecteur quelconque de "E" par le scalaire 0 (l'élément neutre additif du corps "K") vaut aussi 0. En effet λ•0= λ•(0 + 0) = λ•0 + λ•0 et en ajoutant l'opposé de λ•0 dans chaque membre on a 0 = λ•0. On procède de même pour montrer que 0. u = 0.

Enfin, –"v" (l'opposé de "v") est le produit de "v" par le scalaire –1, ce qui résulte de la propriété précédente et de l'axiome 2. On a donc pour tout vecteur "u" de "E" et tout scalaire λ :

Les vecteurs (éléments de "E") ont été ici écrits avec des lettres latines italiques, mais certains auteurs les notent par des lettres en gras, ou les surmontent d'une flèche.

Voici quelques exemples d'espaces vectoriels qui servent entre autres en analyse ou en géométrie :

La définition ci-dessus est celle des espaces vectoriels à gauche sur K. Les espaces vectoriels à droite sur K sont les espaces vectoriels à gauche sur le corps opposé à K. Si le corps K est commutatif, les notions d'espaces vectoriels à gauche et à droite coïncident, et l'on peut alors noter à gauche ou à droite (au choix) la multiplication par un scalaire.

Les notions de la théorie des espaces vectoriels qui ne sont valables, avec les définitions usuelles, que lorsque le corps est commutatif sont notamment celles liées à la multilinéarité (déterminant, trace, produits tensoriels, algèbre extérieure, algèbre sur un corps commutatif) ou aux fonctions polynomiales. Même si l'on ne se sert pas de ces notions, il faut faire attention à divers détails lorsque le corps de base n'est pas supposé commutatif. Par exemple, les homothéties n'existent (en tant qu'applications linéaires) que si le facteur scalaire est central dans le corps, et la multiplication scalaire doit être écrite du côté opposé des applications linéaires (donc avec le scalaire à droite si les applications linéaires sont notées à gauche de leurs arguments).

Les deux opérations sur un espace vectoriel permettent de définir les combinaisons linéaires, c'est-à-dire les sommes finies de vecteurs affectés de coefficients (scalaires). La combinaison linéaire d'une famille ("v") de vecteurs ayant pour coefficients (') est le vecteur ∑ "v". Lorsque l'ensemble d'indexation "I "est infini, il est nécessaire de supposer que la famille (') est à support fini, c'est-à-dire qu'il n'y a qu'un ensemble fini d'indices "i "pour lesquels "" est non nul.

Un sous-espace vectoriel de "E" est une partie non vide "F" de "E" stable par combinaisons linéaires. Muni des lois induites, "F" est alors un espace vectoriel. L' non vide (finie ou infinie) de sous-espaces vectoriels est un sous-espace vectoriel mais l', même finie, n'en est pas un en général.

Une famille ("v") de vecteurs de "E" est dite libre (sur K) ou encore les vecteurs de cette famille sont dits linéairement indépendants, si la seule combinaison linéaire des "v" égale au vecteur nul est celle dont tous les coefficients sont nuls. Dans le cas contraire, la famille est dite liée et les "v" sont dits linéairement dépendants.

Une famille constituée d'un seul vecteur est libre si et seulement si ce vecteur est non nul. Un couple de vecteurs est lié si et seulement si les deux vecteurs sont colinéaires. Si ("u", "v") est un couple de vecteurs linéairement indépendants, alors ("u", "v"), ("u + v", "v") et ("u", "u + v") sont eux aussi des couples de vecteurs non colinéaires, mais la famille ("u", "v", "u + v") est toujours liée.

Le sous-espace vectoriel engendré par une famille ("v") de vecteurs, noté (("v")), est le plus petit sous-espace (au sens de l'inclusion) contenant tous les vecteurs de cette famille. De manière équivalente, c'est l'ensemble des combinaisons linéaires des vecteurs "v". La famille engendre "E", ou encore est génératrice, si le sous-espace qu'elle engendre est "E "tout entier.

Une famille "B "de vecteurs de "E "est une base de "E" si elle est libre et génératrice ou, ce qui est équivalent, si tout vecteur de "E" s'exprime de manière unique comme combinaison linéaire des éléments de "B". L'existence d'une base pour tout K-espace vectoriel "E" se déduit du théorème de la base incomplète.

Étant donné un espace vectoriel "E" sur un corps K, toutes les bases de "E" ont le même cardinal, appelé dimension de "E".
Deux espaces vectoriels sur K sont isomorphes (c'est-à-dire reliés par un isomorphisme) si et seulement s'ils sont de même dimension.

Soient "E" et "F" deux espaces vectoriels sur un même corps K. Une application "f" de "E" vers "F" est dite linéaire si elle est additive et commute à la multiplication par les scalaires :

Autrement dit, "f" préserve les combinaisons linéaires.

Exemple : Soit "E"="F"=ℝ et "f" de "E" vers "F" définie par "f"(("x","y")) = ("x"-"y","x"+"y"). On vérifie sans peine que "f" est une application linéaire de E dans F, qu'on a illustrée ci-contre.

L'ensemble des applications linéaires de "E" dans "F" est souvent noté L("E", "F"). Si K est commutatif, L("E", "F") est un sous-espace vectoriel de l'espace des fonctions de "E" dans "F". Toute composée d'applications linéaires est linéaire. L'ensemble L("E", "E") des endomorphismes de "E" se note L("E"). Un isomorphisme d'espaces vectoriels est une application linéaire bijective. Un automorphisme est un endomorphisme bijectif. L'ensemble des automorphismes de "E" est le groupe linéaire GL("E").

Pour toute application linéaire "f" de "E" dans "F",

Le graphe de "f" est un sous-espace vectoriel de "E" × "F", dont l'intersection avec "E" × {0} est Ker("f") × {0}.

Une forme linéaire sur un K-espace vectoriel "E" est une application linéaire de "E" dans K. Si K est commutatif, les formes linéaires sur "E" forment un K-espace vectoriel appelé l'espace dual de "E" et noté "E"*. Les noyaux des formes linéaires non nulles sur "E "sont les hyperplans de "E".

La somme "F + G" de deux sous-espaces vectoriels "F" et "G", définie par

coïncide avec le sous-espace vectoriel engendré par "F"⋃"G". Cette construction se généralise à une famille quelconque (non vide) de sous-espaces vectoriels.

La formule de Grassmann relie les dimensions de "F "et "G "à celles de leur somme et de leur intersection :
formula_3

Les deux sous-espaces "F" et "G" de "E" sont dits « "en "somme directe » lorsque la décomposition de tout vecteur de leur somme "F + G "en une somme de deux vecteurs, l'un appartenant à "F" et l'autre à "G", est unique (il suffit pour cela que la décomposition de 0 soit unique, c'est-à-dire que "F"∩"G "= {0}). Cette définition se généralise à la somme d'une famille quelconque (non vide) ("F") de sous-espaces. Si cette somme est directe alors les "F" sont d'intersection nulle deux à deux mais la réciproque est fausse.

Une somme "F + G", lorsqu'elle est directe, est notée "F"⊕"G". Les sous-espaces "F" et "G" sont dits supplémentaires (l'un de l'autre) dans "E" s'ils sont en somme directe et si de plus, cette somme est égale à "E".
Le théorème de la base incomplète garantit que tout sous-espace vectoriel possède au moins un supplémentaire.

Soit une famille ("E") de K-espaces vectoriels. Le produit cartésien ∏ "E" hérite naturellement d'une structure de K-espace vectoriel, appelé espace vectoriel produit.

Les familles à support fini forment un sous-espace vectoriel de ∏ "E", appelé la somme directe des espaces "E" et noté ⊕ "E".

Lorsque tous les "E" sont égaux à K, ce produit et cette somme sont respectivement notés K (l'espace des fonctions de "I" dans K) et K (le sous-espace des fonctions à support fini, dont la dimension est égale au cardinal de "I"). Pour "I "= N, on construit ainsi l'espace K des suites dans K et le sous-espace K des suites à support fini.

Soit "F" un sous-espace vectoriel de "E".
L'espace quotient "E"/"F" (c'est-à-dire l'ensemble des classes d'équivalence de "E" pour la relation « "u "~ " v "si et seulement si "u – v "appartient à "F" », muni des opérations définies naturellement sur les classes) est un espace vectoriel tel que la projection "E "→ "E"/"F "(qui associe à "u "sa classe d'équivalence) soit linéaire de noyau "F".

Tous les sous-espaces supplémentaires de "F "dans "E "sont isomorphes à "E"/"F". Leur dimension commune, lorsqu'elle est finie, s'appelle la codimension de "F "dans "E".

Soit "E" un espace vectoriel engendré par un nombre fini "m" d'éléments.




La notion d'espace vectoriel naît conceptuellement de la géométrie affine avec l'introduction des coordonnées dans un repère du plan ou de l'espace usuel. Vers 1636, Descartes et Fermat donnèrent les bases de la géométrie analytique en associant la résolution d'une équation à deux inconnues à la détermination graphique d'une courbe du plan.

Afin de parvenir à une résolution géométrique sans utiliser la notion de coordonnées, Bolzano introduisit en 1804 des opérations sur les points, droites et plans, lesquelles sont les précurseurs des vecteurs. Ce travail trouve un écho dans la conception des coordonnées barycentriques par Möbius en 1827. L'étape fondatrice de la définition des vecteurs fut la définition par Bellavitis du bipoint, qui est un segment orienté (une extrémité est une origine et l'autre un but). La relation d'équipollence, qui rend équivalents deux bipoints lorsqu'ils déterminent un parallélogramme, achève ainsi de définir les vecteurs.

La notion de vecteur est reprise avec la présentation des nombres complexes par Argand et Hamilton, puis celle des quaternions par ce dernier, comme des éléments des espaces respectifs ℝ et ℝ. Le traitement par combinaison linéaire se retrouve dans les systèmes d'équations linéaires, définis par Laguerre dès 1867.

En 1857, Cayley introduisit la notation matricielle, qui permit d'harmoniser les notations et de simplifier l'écriture des applications linéaires entre espaces vectoriels. Il ébaucha également les opérations sur ces objets.

Vers la même époque, Grassmann reprit le calcul barycentrique initié par Möbius en envisageant des ensembles d'objets abstraits munis d'opérations. Son travail dépassait le cadre des espaces vectoriels car, en définissant aussi la multiplication, il aboutissait à la notion d'algèbre. On y retrouve néanmoins les concepts de dimension et d'indépendance linéaire, ainsi que le produit scalaire apparu en 1844. La primauté de ces découvertes est disputée à Cauchy avec la publication de "Sur les clefs algébrique" dans les "Comptes Rendus".

Peano, dont une contribution importante a été l'axiomatisation rigoureuse des concepts existants — notamment la construction des ensembles usuels — a été un des premiers à donner une définition contemporaine du concept d'espace vectoriel vers la fin du .

Un développement important de ce concept est dû à la construction des espaces de fonctions par Lebesgue, construction qui a été formalisée au cours du par Hilbert et Banach, lors de sa thèse de doctorat en 1920.

C'est à cette époque que l'interaction entre l'analyse fonctionnelle naissante et l'algèbre se fait sentir, notamment avec l'introduction de concepts clés tels que les espaces de fonctions "p"-intégrables ou encore les espaces de Hilbert. C'est à cette époque qu'apparaissent les premières études sur les espaces vectoriels de dimension infinie.

Sans disposer d'une définition des espaces vectoriels, une approche possible de la géométrie plane se fonde sur l'étude d'un plan affine de Desargues "P". Il comporte des points et des droites, avec une relation d'appartenance appelée incidence, dont les propriétés donnent un sens à l'alignement des points et au parallélisme des droites. On appelle homothétie-translation toute transformation de "P" préservant l'alignement et envoyant toute droite sur une droite parallèle. Hormis l'identité (considérée à la fois comme une homothétie et une translation), une telle transformation fixe au plus un point ; elle est appelée homothétie si elle fixe un point O, qui est alors son centre ; elle est appelée une translation sinon. L'ensemble des homothéties de centre fixé "O" forment un groupe commutatif pour la loi de composition, indépendant de O à isomorphisme près, noté K. Il est possible d'adjoindre un élément 0 pour former un corps K, dont la loi d'addition est encore définie à partir de "P". Tout scalaire non nul formula_4 correspond à une unique homothétie de centre O, et on dit que formula_4 est son rapport. L'ensemble des translations de "P" forme un K-espace vectoriel, ses lois étant les suivantes :
Le vecteur nul est l'identité. L'opposé d'un vecteur représenté par une translation "t" est le vecteur défini par "t".

Tout ceci se généralise aux espaces affines d'incidence (ou synthétiques) de dimensions (finies ou infinies) supérieures ou égales à 3 (ils sont alors de Desargues). Mais dans ce cas, si le nombre d'éléments des droite est égal à 2, la relation de parallélisme entre droites doit être incluse dans la définition des espaces affines. Donc, il y a intrinsèquement un espace vectoriel « sous-jacent » à tout tout plan affine de Desargues et à tout espace affine d'incidence.

Ces considérations permettent de faire le lien entre une approche moderne de la géométrie fondée sur l'algèbre linéaire, et une approche axiomatique.




</doc>
<doc id="1053" url="https://fr.wikipedia.org/wiki?curid=1053" title="Ergol">
Ergol

Un ergol, dans le domaine de l'astronautique, est une substance homogène employée seule ou en association avec d'autres substances et destinée à fournir de l'énergie. Les ergols sont les produits initiaux, séparés, utilisés dans un système propulsif à réaction. Ils sont constitués d'éléments oxydants (comburant) et réducteurs (carburant ou combustible).

Les termes correspondants en anglais sont "propellant" et "fuel".

Le terme d’ergols résiduels est employé pour désigner les ergols imbrûlés.

On assimile parfois ergols et propergols.

Les ergols sont classés selon :

Les propriétés recherchées des ergols sont :

Liste des principaux ergols liquides :


Liste des principaux ergols solides :




</doc>
<doc id="1054" url="https://fr.wikipedia.org/wiki?curid=1054" title="Émile Benveniste">
Émile Benveniste

Émile Benveniste (prononciation : /bɛ̃venist/) est un linguiste français né à Alep (Syrie) le , et mort à Versailles le . Né Ezra Benveniste, il a été naturalisé français en 1924. Il s'est illustré par ses travaux tant dans le domaine de la grammaire comparée des langues indo-européennes que dans celui de la linguistique générale.

Ses deux parents sont instituteurs de l'Alliance Israélite Universelle dans l'Empire ottoman, en Tunisie puis en Bulgarie. Titulaire d'une bourse de l'Alliance Israélite Universelle, il fait ses études à Paris au Petit séminaire israélite à partir de 1913. Après le baccalauréat, il abandonne les études religieuses. Il est licencié ès lettres en 1920, à 18 ans, et agrégé en 1922. Il fait son service militaire au Maroc pendant la guerre du Rif.

Élève d'Antoine Meillet à l'École pratique des hautes études, il enseigne lui-même dans cet établissement de 1927 à 1969, et au Collège de France, où il occupe la chaire de grammaire comparée de 1937 à 1969.

Fait prisonnier en 1940, il parvient à s'évader en novembre 1941 et se réfugie en Suisse, où il restera jusqu'en 1945, alors qu'il avait été exclu du Collège de France par le régime de Vichy.

Il exerce les fonctions de secrétaire adjoint de la Société de linguistique de Paris de 1945 à 1959, puis celle de secrétaire de 1959 à 1970. En 1960, il est élu membre de l'Académie des inscriptions et belles-lettres et, en 1965, membre de l'Accademia dei Lincei.

En 1961, il fonde, avec Claude Lévi-Strauss et Pierre Gourou, "L'Homme, revue française d'anthropologie". De 1964 à 1975, il dirige la "Revue des études arméniennes" (REA).

En décembre 1969, il est victime d'une attaque qui le laisse aphasique. Il meurt sept ans plus tard, en 1976, à l'âge de 74 ans.

Sa production scientifique s'est étalée sur une cinquantaine d'années, à partir de 1922. Les dix premières années sont principalement consacrées à sa discipline d'origine, l'iranien, avec quatre ouvrages et de très nombreux articles.

À partir de 1932, il se tourne véritablement vers la linguistique comparée des langues indo-européennes ; c'est dans cette période qu'il acquiert une dimension internationale, notamment avec la publication de sa thèse principale, "Les Origines de la formation des noms en indo-européen" (1935), où il propose une théorie de la racine indo-européenne qui a fortement marqué l'évolution ultérieure de la linguistique indo-européenne. Après la période difficile de la guerre, il fait paraître en 1948 "Noms d'agent et noms d'action en indo-européen", qui est, selon , « le plus beau livre de grammaire comparée qu'on ait écrit au vingtième siècle... le chef-d'œuvre, la cime du structuralisme classique européen ». Watkins cite comme « l'apport le plus durable de Benveniste à la grammaire comparée » l'idée résumée dans cette phrase (extraite de la conclusion de son article sur « Actif et moyen dans le verbe ») : « Il est dans la nature des faits linguistiques, puisqu'ils sont des signes, de se réaliser en oppositions et de ne signifier que par là. »

Dans la dernière période, l'intérêt pour la linguistique générale, aussi bien d'un point de vue formel que dans ses rapports avec l'organisation sociale, passe au premier plan mais toujours en lien direct avec la linguistique indo-européenne. Cet intérêt s'exprime pleinement dans ses "Problèmes de linguistique générale" (parus en 1966 et 1974), qui introduisent en France la linguistique de l'énonciation et dans sa dernière œuvre, le "Vocabulaire des institutions indo-européennes" (parue en 1969, quelques semaines avant que la maladie ne le frappe), fruit d'une démarche très novatrice par laquelle il cherche des significations sociales profondes, des « structures enfouies » sous les systèmes de distinctions sémantiques.





</doc>
<doc id="1055" url="https://fr.wikipedia.org/wiki?curid=1055" title="Espèce disparue">
Espèce disparue

En biologie et en écologie, une espèce disparue est une population réputée n’avoir plus aucun représentant vivant, ni dans la nature, ni en captivité. Avant l’apparition du clonage, on considérait que le moment de l’extinction correspondait à la mort du dernier individu de l’espèce.

Si les techniques de conservation de tissus ou de gamètes se perfectionnent, le clonage permettra peut-être de dupliquer le dernier individu connu d’une espèce végétale (ou quelques individus), mais non de retrouver la diversité génétique de l’espèce, et sans garantie que l’espèce puisse survivre dans la nature (par exemple si son pollinisateur spécialisé et/ou son habitat ont également disparu).

Depuis 1963, la liste rouge de l'UICN dresse la liste des espèces menacées ou disparues. En 1988, toutes les espèces connues d'oiseaux avaient été évaluées par l’UICN ainsi qu'en 1996, l’état de conservation de toutes les espèces de mammifères mondiaux.
Parmi les espèces décrites dans l'édition de 1996, 25 % des mammifères et 11 % des oiseaux étaient classées comme étant menacées. En 2006, l'UICN considère qu’une espèce de mammifère sur quatre, une espèce d’oiseaux sur huit, et un tiers des amphibiens sont menacés. Actuellement, une espèce animale ou végétale disparait toutes les dix-sept minutes.

Les paléontologues estiment qu’en temps normal, et à échelle géologique, la grande majorité des espèces « durent » de 1 à 10 millions d’années (5 millions en moyenne), avant soit de disparaître, soit de se modifier au point que l’on doive parler de nouvelles espèces. 

La Terre a connu cinq extinctions majeures induites par des catastrophes géoclimatiques, la dernière étant celle qui a connu la disparition des grands dinosaures (une seule famille de ce groupe a survécu : les oiseaux).

Un nombre croissant de scientifiques et d’ONG craignent que l’humanité soit en train de provoquer une sixième extinction de masse, avec un rythme d’extinction qui semble encore plus rapide que lors des grandes crises naturelles précédentes.

À titre d’exemple en « rythme normal », une espèce d’oiseau devrait disparaître par siècle, or c’est presque une espèce d’oiseau par an, cent fois plus, qui disparaît depuis le . Au début du , cinq espèces de plantes vasculaires disparaissent chaque jour (une tous les deux ans, rien que pour la Picardie dans le nord de la France (source : Conservatoire botanique de Bailleul), contre une tous les 25 ans dans le monde en temps normal. 

Plus de 260 vertébrés auraient récemment disparu (au ), alors que pour un nombre estimé à espèces de vertébrés, c’est une disparition par siècle qui devrait se produire. L’estimation des disparitions actuelles est probablement sous-estimée, en raison d’un grand nombre de petits invertébrés inconnus ou non suivis.

Pour les espèces récemment disparues (totalement, ou seulement dans leur milieu d’origine en cas de survie en captivité), et notamment dans les régions reculées ou peu prospectées par les biologistes, la notion de « disparu » est à considérer comme une probabilité élevée. Cela est rare, mais il arrive parfois que l’on retrouve un ou quelques individus d’une espèce que l’on croyait disparue : ainsi une tortue aquatique, "Rafetus swinhoei" pouvant atteindre de long pour , que l’on considérait comme éteinte dans la nature (seuls trois individus étaient connus en captivité) a récemment été observée à l’état sauvage sur les rives d’un lac du nord du Vietnam. Cette espèce reste bien sûr classée comme la plus menacée au monde parmi les tortues d’eau douce.

C’est une question scientifique à la fois éthique et pratique, qui ne fait pas l’objet de consensus, mais qui est étudiée. Sur le plan éthique, laisser faire des pratiques prédatrices et des gouvernances à courte vue, c'est nier non seulement le droit des autres espèces à exister, mais aussi le droit des générations humaines futures à bénéficier des mêmes ressources et « services-rendus par la biosphère » que les générations actuelles. Sur le plan pratique, certaines espèces "« récemment »" disparues (grands herbivores, grands carnivores) jouaient en effet des rôles fonctionnels et écopaysagers qui ne peuvent être remplacés par l’homme ou d’autres animaux plus petits.

Certaines, comme le "Sophora toromiro" de l’île de Pâques, y ont été réintroduits à partir de graines et plants conservés dans les carpothèques et jardins botaniques du monde. 

Des scientifiques étudient si d’autres espèces proches et adaptées aux mêmes milieux et climats pourraient les « "remplacer" ». Ils envisagent des expérimentations (en milieu confiné) par exemple d’introduction du lion ou de l’éléphant africain en Amérique du Nord pour respectivement « "remplacer" » le lion des cavernes et les espèces de mammouths qui n’ont pas survécu à l’occupation préhistorique.



</doc>
<doc id="1056" url="https://fr.wikipedia.org/wiki?curid=1056" title="Espèce extirpée">
Espèce extirpée

En biologie et écologie, une espèce extirpée est une espèce qui n'existe plus à un endroit ou dans un pays, mais que l'on peut retrouver dans d'autres régions du monde. On parle d'extinction locale.



</doc>
<doc id="1057" url="https://fr.wikipedia.org/wiki?curid=1057" title="Espèce en danger critique d'extinction">
Espèce en danger critique d'extinction

L'expression espèce en danger de disparition ou espèce en danger critique d'extinction désigne toute espèce en péril exposée à une disparition ou à une extinction imminente. C'est le dernier niveau de risque avant l'.

Plusieurs organismes se proposent d'évaluer le niveau de menace sur les différentes espèces (UICN, COSEPAC, etc.) et une espèce peut donc être considérée comme "en danger de disparition" par l'un mais pas par les autres. Il y aurait 878 espèces qui ont totalement disparu.

Quelques espèces classées comme « en danger critique d'extinction » par l'UICN :





</doc>
<doc id="1058" url="https://fr.wikipedia.org/wiki?curid=1058" title="Espèce menacée">
Espèce menacée

En biologie et écologie, l'expression « espèce menacée » s'applique à toute espèce risquant de disparaître à court ou moyen terme.

Selon le congrès mondial de l’UICN de septembre 2016, , ce qu'un article du 10 août, dans la revue "Nature" traduit sous le titre « Les ravages des fusils, des filets et des bulldozers » aussi qualifiés de grand tueurs parmi les facteurs de régression de 8 700 espèces animales et végétales évaluées et classées en 2016 comme menacées ou quasi menacées de disparition sur la liste rouge de l'UICN.

Une espèce est déclarée menacée si elle répond à des critères précis (disparition de l'habitat, déclin important de sa population, érosion génétique, chasse excessive ou surpêche ).

Ces critères, généralement établis ou validés par l'Union internationale pour la conservation de la nature (UICN), permettent d'affiner le risque d'extinction de l'espèce (actuel, à court et moyen terme) et de lui attribuer un statut de conservation et parfois de protection (espèce protégée).

Dans le cas des races locales domestiquées, il s'agit du patrimoine agricole et souvent d'espèces moins productive, mais rustiques et demandant moins de frais d'entretien. La préservation de certaines de ces espèces pourrait notamment faire partie des solutions d'adaptation au dérèglement climatique ou à la diffusion de certaines maladies (maladies animales ou zoonoses transmissibles à l'homme).

La liste rouge de l'UICN classe les espèces menacées en trois catégories, selon l'importance du risque de leur extinction : « vulnérable », « en danger » et « en danger critique d'extinction ». Une classification un peu similaire existe pour les races locales domestiquées d'intérêt agricole.

La convention de Washington (CITES) établit une liste des espèces protégées selon trois catégories organisées en annexes :

Espèces menacées par pays (exemples ; source : UICN, 2004) :

Elles sont classées par groupes taxonomiques :

Le Comité sur la situation des espèces en péril au Canada (COSEPAC) utilise le terme espèce en péril plutôt qu'espèce menacée, l'expression « espèce menacée » ne s'appliquant alors qu'à une partie des espèces pouvant disparaître.

Selon la classification COSEPAC des espèces, une "espèce menacée" est une espèce en péril susceptible de devenir une espèce en danger de disparition dans un avenir plus ou moins proche si les pressions s'exerçant sur elle (facteurs limitants), comme la disparition de l'habitat, ne sont pas supprimées. 
Ce terme désigne le statut donné à l'espèce quand le deuxième niveau de risque d'extinction est atteint.

Québec :

Avec la Loi sur les espèces menacées ou vulnérables, le gouvernement québécois s'est engagé à garantir la sauvegarde de l'ensemble de la diversité génétique du Québec.

En 2018, 78 espèces de la flore et 38 espèces de la faune sont légalement désignées menacées ou vulnérables au Québec.

Pour chaque espèce floristique légalement protégée au Québec, des plans de conservation seront élaborés afin d’identifier les actions à mettre en œuvre pour assurer leur survie à long terme. Les plans de conservation pour le Carex faux-lupulina et la Sagittaire à sépales dressés sous-espèce des estuaires sont maintenant disponibles.

L'UICN qu'en Europe, rien que pour les mammifères ; 

Les causes principales sont la perte, la dégradation et la fragmentation des habitats, les changements climatiques, la mortalité accidentelle (""), la pollution et l’homme (chasse, poison, pièges, introduction volontaire ou non d'espèces invasives et concurrentielles des espèces autochtones). L'UICN note que certains plans de conservation ont efficacement sauvé quelques espèces (mais ils ne concernent que 8 % des mammifères européens, et pas dans tous les pays). Une évaluation de l'état des populations de mammifères est en cours.

La commission a plusieurs fois alerté aussi sur l'importance de restaurer un réseau écologique européen, et de ne pas oublier les invertébrés et en particulier les invertébrés xylophages, souvent menacés par la manque de ressource en bois-morts ou sénescents dans les forêts européennes trop exploitées. De nombreuses espèces d'eau douce autrefois communes sont également en très forte régression ( reptiles ou amphibiens) ou menacées, dont l'anguille européenne (Anguilla anguilla). Plusieurs études observent également un déclin important chez les lichens et les champignons.

Il existe des listes rouges (nationales et régionales) d'espèces menacées.

La France se classe parmi les dix pays hébergeant le plus d'espèces menacées sur la planète. Elle jouit d'une position unique au monde en termes de richesses naturelles. Via ses départements (La Réunion, Guyane, Martinique, Guadeloupe, Mayotte) et ses territoires et collectivités d'Outre-Mer, elle est présente dans cinq des trente-quatre points chauds du globe, ces zones où la diversité biologique s'avère la plus grande mais la plus en danger et où les espèces endémiques sont très nombreuses.

La loi du 10 juillet 1976 protège déjà certaines espèces menacées en France. Elle en interdit la capture, la vente et l'achat et même la perturbation intentionnelle. En 2005, la loi concernait en métropole plus de animales sauvages (soit 52 % des vertébrés, 4 % des mollusques et 0,5 % des insectes, crustacés et échinodermes) et végétales (plus de 7 % des plantes, sans compter les mousses). Mais l'Outre-Mer concentre 80 % de la biodiversité or, la loi française ne s'exerce que sur ses départements, la Polynésie française, la Nouvelle-Calédonie et Wallis-et-Futuna possédant leur propre réglementation.

Le Costa Rica a mis en œuvre une politique de protection de la biodiversité : 25 % de son territoire est classé en parc national, réserve ou zone protégée.





</doc>
<doc id="1059" url="https://fr.wikipedia.org/wiki?curid=1059" title="Espèce vulnérable">
Espèce vulnérable

En biologie et en écologie, une espèce vulnérable (ou "préoccupante") est une espèce en péril car ses caractéristiques biologiques la rendent particulièrement sensible aux menaces liées aux activités humaines ou à certains phénomènes naturels.

Ce terme désigne le statut donné à l'espèce quand le premier niveau de risque d'extinction (suivant la classification COSEPAC) est atteint. Si la menace d'extinction de cette espèce augmente, elle est alors qualifiée d'espèce menacée.

L'Union internationale pour la conservation de la nature maintient une liste des espèces menacées, indiquant le degré de risque.



</doc>
<doc id="1060" url="https://fr.wikipedia.org/wiki?curid=1060" title="Domestication">
Domestication

La domestication est la modification du patrimoine génétique de populations d'êtres vivants par les humains en vue de répondre à leurs besoins. 

Ces modifications vont de l’isolement de populations (simple isolat de génotypes sauvages reproduits) au changement du génome et jusqu’à la création d’espèces nouvelles. On parle d'espèces domestiquées, de plantes ou d'animaux domestiqués.

La domestication est une activité humaine très ancienne, elle précède la sédentarisation et l’agriculture (domestication du chien ou du figuier par les chasseurs paléolithiques). Le terme « domestication » est utilisé par extension aux techniques et aux objets mis au service des besoins humains (domestication d'un fleuve, d'une énergie, etc.). Chez l'animal « domestiquer » s'utilise comme synonyme d'apprivoiser. 

La notion de besoin humain s'entend extensivement à toutes les activités humaines, utilitaires ou culturelles, et la domestication porte sur toutes les classes du vivant. Son étude relève de sciences multiples, sachant que la génétique permet depuis le de mieux connaitre ses étapes et ses processus.

On parle aussi de domestication pour des choses, telles la domestication de paysages, pour exprimer les modifications que les humains leur apportent pour qu'elles correspondent à leurs conceptions morales et philosophiques.

"Se reporter au tableau des dates et foyers par espèce, plus bas."

Les domestications s'étalent du néolithique à nos jours, à l'exception de celle du chien, qui a précédé de plusieurs millénaires l'élevage d'autres espèces et la sédentarisation. Notre époque, à partir du , est par contre riche en nouvelles espèces élevées, et on peut parler pour plusieurs d'entre elles de domestication.

Les dates et foyers des domestications anciennes ont été estimés par des méthodes essentiellement archéologiques ; il s'agit plus spécialement d'archéozoologie. Ces méthodes consistent à fouiller ou exploiter les résultats de fouilles de sites d'occupation humaine préhistorique. Les restes animaux sont datés selon les méthodes archéologiques : on détermine l'espèce à laquelle ils appartiennent, on estime également l'âge auquel ils sont morts, voire le type d'animaux (d'une forme éventuellement domestique) qu'ils représentent, et on s'appuie sur d'autres indices comme les traces observables d'abattage ou de découpe. L'enjeu est de déterminer si on est en présence d'animaux sauvages ou d'élevage, et plus globalement la nature de leurs relations avec les humains. Ainsi le squelette d'un chat retrouvé auprès d'un tombeau humain indique qu'il s'agissait probablement d'un animal de compagnie. Une certaine homogénéité d'âge des animaux dont on retrouve les restes indique qu'il s'agissait d'un élevage, où l'on abattait les animaux à l'âge optimal.

Les nouvelles techniques et en particulier l'étude de l'ADN mitochondrial permettent de réestimer les dates de domestication de même que l'arbre généalogique des espèces domestiques actuelles ; ces connaissances sont donc toujours en évolution. La lignée du chien en particulier se serait séparée de celle du loup il y a entre et ans. Il est possible que l'ancêtre du chien se soit à cette époque rapproché et associé aux groupes humains qu'il suivait, pour les restes qu'il pouvait obtenir, en ayant un rôle d'alerte voire d'auxiliaire de chasse. La date de domestication issue des sources archéologiques correspondrait alors à une relation devenue plus étroite et à un contrôle plus fort de l'homme.

Le processus de domestication et la diffusion des espèces et techniques d'élevage s'étalent sur des périodes longues et loin d'être parfaitement déterminées. On admet pour plusieurs espèces le principe de plusieurs foyers de domestication distincts. Cela n'exclut pas les croisements qui ont suivi et il semble vain de déterminer un ancêtre sauvage pour chaque race d'une espèce domestique.

Après celle du chien, le premier foyer de domestication fut le Moyen-Orient, en particulier sa partie qu'on appelle le Croissant fertile. On remarque ensuite l'Asie de l'Est, le bassin méditerranéen et l'Amérique du Sud. Certaines régions du monde n'ont connu aucune domestication d'espèces locales sinon de très récentes comme l'Australie ou l'Afrique australe.

Le nombre d'espèces domestiques disponibles s'est brusquement accru au de part et d'autre de l'Atlantique, avec ce qu'on nomme l'échange colombien. Le continent américain abritait alors cinq espèces animales domestiquées, dont seul le chien était connu dans l'Ancien Monde. Les chevaux et bœufs par exemple y sont alors apparus tandis qu'un grand nombre de plantes domestiques américaines, nouvelles en Europe, en Asie et en Afrique y ont été adoptées.

Plusieurs scénarios ont été proposés comme ayant mené à la domestication des espèces animales.
La tradition d'adoption de bébés animaux, voire leur allaitement au sein est souvent donnée pour origine de la domestication, étant donné que par le phénomène d'empreinte, il est facile d'obtenir de cette façon des animaux familiarisés par leur contact précoce avec les humains. Pourtant, le processus de domestication implique l'élevage de lignées d'animaux sur de nombreuses générations, ce qui n'est pas le cas si le recrutement se fait en permanence par prélèvement d'animaux sauvages. Par ailleurs cette pratique, toujours observée actuellement, est caractéristique des peuples de chasseurs-cueilleurs qui, précisément, n'ont pas d'animaux domestiques. Ces apprivoisements seraient donc intégrés à une culture basée sur la chasse et non l'élevage, et il y aurait une opposition entre sociétés « apprivoisatrices » et « domesticatrices ». Ce schéma ne paraît donc pas pouvoir être retenu directement comme moyen de domestication. Pourtant si le facteur culturel est sans doute essentiel pour expliquer la domestication voire la non-domestication d'une espèce, le système économique et culturel d'une société n'est pas figé. La plupart des sociétés fondées sur la chasse élève des chiens, pour lequel ce schéma a pu être un élément important de la domestication.

Il est possible que la domestication soit passée par une phase de mutualisme entre ces animaux et l'homme, c'est-à-dire un rapprochement et une aide dans l'intérêt mutuel. En effet, cette relation s'observe toujours chez le chien paria, et on présume qu'elle a été une étape de la domestication du porc.

Plusieurs espèces étaient les objets d'une tradition de chasse qui a évolué vers un contrôle des populations, et une gestion de population sauvage devenue raisonnée. Cette chasse a pu devenir sélective, visant par exemple les animaux les plus âgés et les mâles en surnombre, et conduire à un mode d'élevage extensif, puis intensifié. Tous ces stades sont actuellement pratiqués dans le cas du renne, dans des régions différentes. Ce processus semble avoir concerné plusieurs espèces, dont les chèvres et moutons, ainsi que les petits camélidés (lamas).

Enfin l'élevage a pu simplement commencer avec des animaux capturés puis élevés en stricte captivité. C'est le cas de la plupart des domestications récentes ou contemporaines. C'est dans cette situation que le contrôle et la sélection peuvent être les plus forts, permettant une transformation plus rapide des espèces élevées.

Le scénario de domestication d'une espèce peut avoir correspondu à l'un de ces schémas ou en avoir été une combinaison simultanée ou successive. Dans le cas du lapin, les étapes de la domestication à partir de la simple chasse ont été l'établissement de garennes fermées au Moyen Âge, qui constituaient des sortes de réserves de chasse. Dans certaines de ces garennes a été pratiquée une sélection, permise par la capture des animaux vivants grâce aux furets. Cette sélection a abouti à l'apparition des premières variétés de lapin au cours du , qui se distinguaient par leur coloris et leur taille. L'élevage a ensuite été intensifié et la forte sélection a abouti à une grande variabilité des races domestiques. Le comportement du lapin d'élevage a probablement évolué conjointement, du fait d'une sélection d'animaux moins farouches, celle-ci ayant pu être aussi bien intentionnelle qu'indirecte : les animaux plus difficiles à recapturer ne pouvant pas être donnés à de nouveaux éleveurs.

Les pratiques d'élevage et de sélection qu'on peut observer sur la période historique peuvent donner une idée de celles qui ont produit la domestication. Celles-là sont très variées, ainsi que les connaissances et représentations qu'ont les éleveurs de l'hérédité et de l'influence qu'ils peuvent avoir sur une population animale. Certains d'entre eux opèrent une sélection méthodique au sein d'un cheptel, d'autres ne conçoivent pas l'influence que peut avoir le choix des reproducteurs sur leurs produits, au sein d'une espèce ou variété. Ces éleveurs peuvent croire pourtant à l'intérêt d'acquérir une nouvelle lignée ou d'opérer des croisements avec des animaux de souches différentes de la leur et participer ainsi à leur diffusion.

La sélection exercée par les éleveurs est d'ailleurs loin d'avoir constamment la même direction, une pratique relevée pour plusieurs espèces et à différentes époques consistant par exemple à sacrifier les animaux ayant eu la croissance la plus forte ou la plus rapide afin de laisser les autres finir leur croissance. Cette pratique qui a vraisemblablement un effet de contre-sélection a d'ailleurs été dénoncée comme telle par des observateurs pour les moutons par exemple, ou en pisciculture d'étang où la pratique du « fond de pêche » consiste à repeupler un étang après sa pêche par vidange en y relâchant les poissons les plus petits. Ainsi, quoique la carpe ait eu une longue tradition d'élevage en France, les performances d'élevage de ce poisson étaient médiocres. Des lignées à croissance nettement plus rapide y ont été réintroduites à partir d'Europe centrale à la fin du , où un élevage sélectif était pratiqué. Parmi la diversité des pratiques, on relève aussi celle consistant à faire saillir une femelle par des congénères sauvages pour les qualités réelles ou supposées que cela procure aux produits de tels croisements (chien/loup ; porc/sanglier en Europe). Quoique ceci semble aller à l'encontre du processus de domestication, ces hybridations ont pu contribuer à conjuguer les caractères domestiques, en particulier comportementaux d'une espèce avec ceux d'une sous-espèce locale sauvage bien adaptée à son milieu. Cela a probablement été le cas des races de chiens nordiques.

La domestication d'une espèce est le fruit d'une histoire multiple qu'il est difficile de reconstituer. Ses facteurs importants sont les prédispositions de cette espèce, les pratiques des éleveurs ou proto-éleveurs sur de longues périodes qui opèrent une sélection consciente ou non et les échanges d'animaux qui permettent aux lignées les plus domestiquées de se diffuser.

Le processus de domestication commence lorsqu'un nombre restreint d'animaux est isolé de l'espèce sauvage. Cette population peut alors connaître un phénomène de microévolution, en s'adaptant aux conditions d'élevage et du fait de la sélection humaine. Cette évolution est marquée par l'apparition de traits domestiques, c'est-à-dire des nouveaux caractères interprétés comme des mutations génétiques conservées voire sélectionnées alors que les allèles qui les portent seraient restés rares ou auraient été éliminées par sélection naturelle à l'état sauvage. Ce sont des caractères morphologiques comme la taille plus grande ou plus petite que celle de l'espèce sauvage, des coloris nouveaux, le poil long, frisé ou encore la queue enroulée ; ce sont aussi des caractères physiologiques comme l'augmentation de la prolificité, et la précocité de la croissance. On note aussi la perte de caractères physiques comme les cornes pour une partie des races de mouton ou d'aptitudes comme une diminution de la mobilité ; de la vitesse de course ou de l'aptitude au vol, ainsi que la perte d'aptitudes comportementales. Ceci fonde une interprétation de la domestication comme altération du génotype, ce qui est indiscutable dans le cas de l'albinisme. De même et plus tôt, Buffon a décrit la domestication en termes de dégénérescence. La variabilité morphologique est importante chez certaines espèces et beaucoup moins chez d'autres comme le chameau de Bactriane. On interprète également les transformations de la domestication avec la notion de néoténie, selon laquelle des caractères morphologiques comme les oreilles pendantes ou comportementaux comme l'attachement, à l'origine propres aux stades juvéniles, se prolongent à l'état adulte.

Si les premières espèces domestiquées sont élevées depuis quelques millénaires, ce temps est-il pour autant indispensable à cette évolution ? Des expériences spécifiques ainsi que les domestications contemporaines montrent qu'avec une forte sélection, les transformations caractéristiques de la domestication peuvent apparaître relativement rapidement, dans l'intervalle d'une dizaine à quelques dizaines de générations.

Du point de vue "écologique", certaines espèces sont élevées à l'état domestiques dans un milieu identique ou proche de celui de leurs ancêtres sauvages comme le chameau ou le renne. À l'inverse, on remarque que le nombre relativement faible d'espèces domestiques est compensé par leur distribution souvent très large, dans des milieux et sous des climats variés et très différents de ceux d'où l'espèce est originaire. La poule, originaire de régions tropicales est élevée jusqu'au cercle polaire arctique, et le porc, originaire de régions tempérées, est élevé jusqu'en climat équatorial plutôt que d'autres espèces de suidés, originaires de ces climats mais non domestiquées. Le régime alimentaire des espèces domestiques peut varier très fortement du fait de l'accès aux ressources naturelles d'un nouveau milieu, et bien sûr avec l'alimentation artificielle parmi laquelle les céréales cultivées sont primordiales, y compris pour le chien. Il est difficile de déterminer à quel point ces changements de climat et de régime alimentaire se sont accompagnés d'une adaptation physiologique héréditaire vers une éventuelle tolérance des animaux domestiques à ces variations. Certains auteurs ont estimé dans le sens inverse que les espèces domestiques avaient été choisies parmi celles qui sont les moins spécialisées du point de vue alimentaire et écologique (espèces dites "Euryèces"). Les déplacements et introductions par l'être humain d'espèces domestiques dans des espaces où elles étaient absentes a eu des conséquences importantes sur les équilibres écologiques dès le néolithique, puisqu'ils pouvaient constituer des invasion biologique et entraîner la disparition d'espèces locales.

Au Paléolithique inférieur, il y a 2 millions d'années, des restes de loup gris l’ancêtre du chien, ont été retrouvés en association avec des restes d'hominidés. On peut donc en déduire que les loups se sont associés aux humains pour chasser des grandes proies. Cette association a fait évoluer le loup en chien et a conduit à la domestication actuelle du chien. Elle a eu lieu dans plusieurs endroits du globe.

Au paléolithique moyen, il y a ans, des cranes de loup associés aux restes humains ont été retrouvés dans la Grotte du Lazaret à Nice en France.

Au Paléolithique supérieur, il y a ans, des premières traces de chiens ont été découvertes sur des sites magdaléniens comme dans l'abri du Morin en Gironde.

Au Mésolithique, sur certains sites du Moyen-Orient il y a 8500 ans, des restes archéozoologiques témoignent de la domestication: les aurochs sont devenus des bœufs, les mouflons sont devenus des moutons, et les chèvres sauvages sont devenues des chèvres domestiques.

Au Néolithique, 8000 ans en arrière, on a trouvé une tombe qui renfermait les restes d'un homme et d'un chaton. On en a déduit que l'homme a domestiqué le chat pour chasser les souris qui profitaient des stocks de blé à cette époque.

La domestication est non seulement une modification des caractères physiques d'une espèce, mais aussi de son comportement. Cette évolution consiste en premier lieu en un caractère moins farouche, à une tolérance voire une familiarité plus facile à l'égard des humains et à l'atténuation des comportements potentiellement dangereux à leur égard. C'est aussi une adaptation aux conditions d'élevages, donc aux groupes importants et à la promiscuité, qui peuvent être mal tolérés par les congénères sauvages.

L'éthologue Konrad Lorenz a décrit notamment la domestication comme un appauvrissement des comportements sociaux spécialisés, au profit de l'hypertrophie des besoins de base comme la reproduction et l'alimentation. Le comportement social en général paraît en effet plus riche chez les animaux sauvages que chez leur congénères de races domestiquées.

Dans le cas du chien, l'évolution comportementale semble beaucoup plus radicale et ne peut en aucun cas être réduite à la perte du caractère farouche ou sauvage. La capacité des chiots à interpréter les signes de communication humains parait ainsi supérieure à celle des loups et des primates. L'attachement qu'un chien porte à son maître et la propension à lui obéir, bien que pouvant être l'objet d'une éducation ou dressage sont des caractères innés issus de la domestication.

L'éthologie est aussi évoquée concernant la domestication pour discuter des caractères comportementaux qui permettent ou ont permis à une espèce d'être domestiquée. Le principal d'entre eux serait le caractère social d'une espèce. Le fait qu'elle vive en groupe hiérarchisé (dans l'exemple du chien) aurait permis à l'éleveur d'exercer un contrôle sur ces animaux en prenant la position de l’élément dominant du groupe. La territorialité a pu être déterminante pour certaines espèces (dans l'exemple du chat) : le fait que certains individus d'espèces différentes se côtoient de manière répétées dans le temps a favorisé l'apprivoisement qui a pu déboucher sur la domestication. La communication interspécifique est une branche de l'éthologie qui en est à ses balbutiements. Le sujet est aussi vaste que le nombre d'espèces. Les cas de relation particulière interspécifique commencent à être documentés (lionne solitaire adoptant un bébé oryx, étalon solitaire cohabitant avec un chevreuil…), tendant à montrer que la domestication n'est peut-être qu'un cas particulièrement développé par la culture humaine de processus éthologiques exceptionnels existants.

Actuellement, les objectifs intentionnels de la domestication (dans le cas de nouvelles espèces) ou de l'amélioration des races domestiques concernent essentiellement la production (rarement le travail produit par les animaux). Ce sont l'adaptation aux conditions d'élevage, la prolificité, la vitesse de croissance, et souvent la qualité de la chair ou celle d'autres produits comme le lait ou la laine.

Les premiers registres découverts qui établissent des listes de lignées, montrant ainsi une formalisation de la sélection des animaux datent du en langue Hittite. La sélection moderne des espèces d'élevage fait appel à des outils notamment statistiques appliqués aux notions génétiques. Elle demande une évaluation aussi objective que possible des sujets et une organisation rigoureuse des programmes d'élevages, pour obtenir une amélioration des performances des lignées en fonction d'objectifs déterminés. Ces sélections sont souvent mises en œuvre par des organismes spécialisés.
La sélection sur des critères étroits de performance est critiquée pour les inconvénients qu'elle amène en termes de fragilité des sujets par exemple, et pour la menace qu'elle fait subir à la biodiversité des races domestiques, en leur substituant un nombre réduit de lignées. Elle tend en réponse à intégrer des critères plus larges de sélection, comme la facilité de mise-bas en plus de la performance laitière ou de croissance pour les bovins par exemple. Cette sélection peut tenter également de répondre à des besoins très précis, comme dans le cas du porc une réduction des éléments les plus polluants des déjections des animaux, qui posent problème en situation d'élevage intensif. D'autre-part, les variétés peu sélectionnées ou dites rustiques sont reconnues non seulement en tant que ressources génétiques potentielles, mais aussi pour leur adaptation à certains modes ou systèmes d'élevage de type extensif. Le CNRS estimait en 2005 que 50 % des races d'oiseaux domestiqués sont en voie de disparition. La sélection des animaux paraît donc liée à des objectifs et un type d'élevage précis. En outre, la prise en charge de la sélection par des organismes spécialisés peut réduire l'autonomie des producteurs et les rendre dépendant des orientations de ces organismes, notamment en types de productions.

Malgré ces limites, la sélection contemporaine montre une assez grande efficacité. Le « progrès génétique » obtenu peut être très sensible à l'échelle de quelques années, montrant que la transformation des espèces domestiques est loin d'être arrêtée. Les efforts portent également sur des nouvelles espèces d'élevage, en particulier parmi les poissons.

La domestication est aussi un phénomène culturel en ce qu'elle a impliqué lors des premiers élevages un bouleversement des rapports de l'homme avec la nature et avec les espèces concernées.
Les systèmes culturels humains et leur évolution semblent être en premier lieu le facteur qui a déterminé la domestication (ou la non domestication) des espèces.

La liste des espèces domestiques est modulable selon les critères adoptés. On limite en général celle des espèces domestiques les plus répandues et les plus anciennes à une trentaine. Cette liste est complétée par d'autres animaux dont l'élevage est ancien, par les nouvelles espèces domestiques puisque l'ancienneté de l'élevage de plusieurs espèces n'empêche pas que la domestication soit un phénomène contemporain, et par d'autres espèces en fonction de leur lien plus ou moins étroit avec l'homme.

Une partie des espèces dont il existe des variétés domestiquées ont vu leur forme sauvage disparaître à l'époque préhistorique comme pour le dromadaire ou tardivement pour l'auroch. Il existe pourtant des populations sauvages de ces deux espèces ainsi que du cheval par exemple, mais celles-là sont issues exclusivement de marronnage. Le lien de parenté entre une espèce domestique et l'espèce sauvage dont elle est issue est longtemps resté insoupçonné. Sa découverte, qui allait avec celle de la variabilité, au moins morphologique d'une espèce, a contribué à l'établissement des théories de l'évolution. Pour des espèces comme le cochon d'inde ou le mouton, l'espèce sauvage dont elles sont issues n'est toujours pas connue avec certitude, parmi plusieurs espèces proches.

Plusieurs animaux domestiques ont longtemps été considérés et classifiés comme des espèces distinctes de celles dont elles sont issues, lorsque celles-ci existent toujours à l'état sauvage. Actuellement et dans ce cas, la classification d'une variété domestiquée comme une sous-espèce de l'espèce dont elle est issue tend à s'imposer. Ainsi le nom scientifique du porc a été changé de "Sus domesticus" à "Sus scrofa domesticus", ce qui le désigne comme une sous-espèce du sanglier.

Dans cette liste, les cas du furet et du ver à soie ne font pas consensus : du point de vue légal pour le furet (classé dans certains pays dont la Suisse ou la Californie comme animal sauvage) et en tant qu'insecte qui ne serait pas concerné par la notion d'animal domestique pour le second. Ces deux espèces sont à d'autres points de vue parmi celles dont la domestication est la plus poussée. La carpe et le poisson rouge ne sont pas non plus toujours cités au sein d'une liste restrictive d'espèces domestiques.

Certaines espèces considérées comme distinctes et qui ont été domestiquées séparément sont néanmoins interfécondes. Elles partagent alors le genre. Ce sont par exemple le genre "Bos" qui réunit bœuf, zébu, yak, gayal et banteng, le genre "Camelus" : chameau de Bactriane et dromadaire, le genre "Lama" : lama et alpaga ou le genre "Anser" (les oies).

Certaines variétés domestiques peuvent alors être issues de l'hybridation de plusieurs espèces : le sanglier des Célèbes ("Sus celebensis") a été domestiqué séparément de l'espèce "Sus scrofa" et ne subsiste probablement à l'état domestique qu'au sein de variétés issues de l'hybridation de ces deux espèces.

Le cheval et l'âne (genre "Equus") donnent des hybrides stériles : mulet et bardot, ainsi que le canard de Barbarie et les races de canard domestique issues du canard colvert qui produisent le canard mulard.

On peut élargir la liste avec :
Les deux premières espèces, malgré l'ancienneté de leur élevage, ne sont en général pas détachées comme populations de celles de leurs congénères sauvages, et leur reproduction n'est pas entièrement contrôlée. Les suivants sont des animaux d'agrément et de volière, parfois opposés à ce titre aux animaux domestiques de rente. Le daim est dans ce cas, son élevage relevé en Égypte antique n'a probablement pas été continu jusqu’à nos jours.

Les critères qui font qu'une population est perçue ou non comme domestique ne correspondent pas toujours exactement à des faits biologiques ou techniques objectifs et la frontière entre animaux domestiques et sauvages est souvent floue.

Animaux de rente

Animaux de compagnie et d'ornement


Animaux d'étude

Les études et expérimentations ont utilisé fréquemment des animaux de différentes espèces domestiques. Certaines de ces espèces comme la souris et le rat semblent avoir été sélectionnées conjointement comme animaux de compagnie et de laboratoire. Une espèce au moins a été domestiquée à des fins uniquement scientifiques : la drosophile, dont la rapidité du cycle d'élevage, a fait un organisme modèle dans la recherche en génétique. Ces animaux augmentés par les biotechnologies dans les laboratoires sont appelés post-animaux.

Certaines espèces ont été élevées voire réellement domestiquées, mais ne le sont plus, ayant totalement disparu ou n'existant plus qu'à l'état sauvage. Ces cas sont cependant douteux : le degré de domestication des animaux peut être difficile à déterminer, ainsi Digard relève plusieurs espèces dont l'élevage paraît attesté en Égypte antique (des antilopinés des genres gazella, oryx, addax, ainsi que l'Ouette d'Égypte et la hyène tachetée), quoique leur cas pourrait être qualifié de détention d'espèces sauvages plutôt que de domestication.
D'après Buffon, la sarcelle était élevée pour sa viande par les Romains, tandis que le colvert n'a été domestiqué qu'au cours du Moyen Âge.
Pour deux autres cas, c'est l'identification de l'espèce qui n'est pas certaine : l'onagre, "Equus hemionus" aurait été domestiqué et utilisé notamment attelé dans la civilisation sumérienne (de 5000 à 2000 ans ). Néanmoins, sur les représentations qui paraissent l'attester il pourrait s'agir plutôt d’"Equus asinus" ; l'âne domestique originaire d'Afrique. En Europe la tourterelle des bois (streptopelia turtur) aurait été couramment élevée au Moyen Âge comme animal de compagnie. Dans ce cas également, il reste à confirmer qu'il s'agissait bien de cette espèce, qui n'existe de nos jours qu'à l'état sauvage, ou bien de la tourterelle domestique, qui n'est pas originaire d'Europe.

On relève l'utilisation d'éléphants de guerre dès la fin du en Perse sous le règne de Darius qui entreprit une expédition dans la vallée de l'Indus. Ils furent ensuite utilisés à la bataille de Gaugamèles (-331) puis par les troupes carthaginoises durant les Guerres puniques notamment celles d'Hannibal Barca au avant notre ère, leurs éléphants ayant traversé l'Espagne, les Pyrénées, le sud de la France et les Alpes. Pour ces derniers, il existe trois hypothèses d'identification : celle d'éléphants d'Asie, d'éléphants de forêt d'Afrique vivant dans les forêts d'Afrique du Nord, plus denses qu'actuellement selon Philippe Leveau et Jean-Pascal Jospin et enfin celle d'éléphants d'Afrique du Nord, espèce ou sous-espèce de Loxodonta, ayant supposément existé selon Gilbert Beaubatie bien qu'ils ne soient pas recensés par la taxinomie et qu'aucune étude paléontologique basée sur de potentiels ossements fossiles n'ait fait la preuve de leur existence. Par ailleurs l'éléphant était utilisé dès l'Antiquité lors d’exécutions.

Toutes les espèces élevées ou utilisées par l'être humain n'ont pas subi une évolution vers la domestication. Plusieurs d'entre elles font l'objet d'un élevage établi de rente pour la fourrure ou la peau comme le ragondin, le rat musqué, la martre, le crocodile, ou la chair comme la grenouille, l'écrevisse, l'escargot ou le cerf élaphe. Ces espèces sont rarement considérées comme domestiquées pour autant.

Pour une part d'entre elles, l'élevage durant plusieurs décennies a engendré des modifications qui peuvent être interprétées comme un début de domestication (voir par exemple les expériences de Dimitri Belyaev). C'est le cas des renards et des visons élevés pour leur fourrures, chez lesquels on a vu apparaître de nouveaux coloris au fil des décennies d'élevage. Cependant, ces espèces ont été très peu sélectionnées sur des critères d'apprivoisabilité et d'adaptation aux conditions d'élevage, ce qui pose des problèmes sérieux de stress et comportements pathologiques.

En aquaculture, les espèces de poissons peuvent être élevées sans domestication, soit du fait d'un mode d'élevage extensif laissant peu de prise au contrôle de la reproduction et à la sélection, soit par la limitation de l'élevage au grossissement après capture des juvéniles dans le milieu naturel, ce qui est le cas de l'anguille.

Plusieurs espèces de coquillages marins sont l'objet d'un élevage intensif (voir conchyliculture). C'est le cas en particulier de l'huître et de la moule. Il n'y a en général pas de contrôle de la reproduction mais captage du naissain sauvage, donc une perméabilité entre les populations sauvages et de production, ce qui se rapproche du cas des abeilles. La maîtrise de la reproduction et des premiers stades d'élevage, acquise ces dernières années pour l'huître par exemple, est cependant une forme de domestication de ces espèces.

On recense plusieurs espèces pour lesquelles il existe ou il a existé une tradition de dressage et d'utilisation, souvent pour la chasse, sans qu'un élevage durable et une sélection aient été pratiqués. La loutre et le grand cormoran ont été employés comme auxiliaires de pêche ; les faucons et de nombreuses espèces de rapaces sont dressées à la chasse, la fauconnerie étant une tradition toujours bien vivante. D'autres animaux comme le caracal au Moyen Âge, et le guépard, de jusqu’à nos jours sont employés pour la chasse. Le cas des macaques dressés à la cueillette de noix de coco en Thaïlande ne rend pas la liste exhaustive.

D'autres espèces sont élevées pour l'ornement, en particulier des oiseaux de cage et de volière, des reptiles et amphibiens de terrariophilie et des poissons d'aquariophilie, et ne sont pas les objets d'une sélection durable. Elles restent, biologiquement, légalement ou dans la perception qu'en ont leurs détenteurs, des espèces sauvages détenues ou élevées en captivité.

La domestication des plantes est probablement plus importante encore que celle des animaux pour l'espèce humaine. Les premières plantes ont été domestiquées autour de 9000 dans le Croissant fertile au Moyen-Orient. Il s'agissait d'annuelles à graines ou fruits comme le haricot, l'orge et bien sûr le blé. Le Moyen-Orient a particulièrement convenu à ces espèces ; le climat aux étés secs favorisant le développement des plantes à semer, et les divers étages d'altitude ont permis le développement d'une grande variété d'espèces. Avec la domestication s'est faite la transition d'une société de chasseur-cueilleurs à une société agricole et sédentaire. Ce changement aura mené par la suite, environ 4000 à 5000 ans plus tard, aux premières villes et à l'apparition de véritables civilisations.

La domestication autour de la même période a également débuté en Chine avec le riz, au Mexique avec le maïs, en Nouvelle-Guinée avec la canne à sucre et certains légumes-racine, mais aussi dans les Andes avec le piment ou en Équateur avec des légumes de la famille des courges, aubergines et concombres, ce qui remet en cause la théorie de la naissance de l'agriculture uniquement par des nécessités économiques et productives.

La domestication des plantes comme celle des animaux est un processus lent et progressif. Après les plantes annuelles, des pluriannuelles et des arbrisseaux et arbustes ont commencé à être domestiqués, parmi lesquels la vigne, le pommier et l'olivier. Quelques plantes n'ont été domestiquées que récemment comme le noyer du Queensland et le pacanier (noix de pécan). Dans différentes régions du monde, des espèces très variées ont été domestiquées. En Amérique du Nord, la courge, le maïs, et le haricot ont formé le cœur de l’alimentation des amérindiens alors que le riz et le soja étaient les cultures les plus importantes de l’Asie de l'Est.

On parle de centres d'origine et de centres de diversité (Nikolai Vavilov décrivait en 1926 dix centres de diversité pour l'ensemble des plantes domestiques, dans "Études sur l'origine des plantes cultivées").

Le critère initial de sélection de la domestication d’une céréale est de pouvoir être moissonnée sans que le grain ne se détache de l’épi, tout en conservant son pouvoir germinatif pour servir de semence. Cette difficulté a été résolue progressivement, permettant à la sélection de porter ensuite sur d'autres caractères comme l’adaptation de la plante à son environnement de culture ou sa productivité.

Au cours des millénaires, la sélection a rendu beaucoup d’espèces domestiquées très différentes des plantes d'origine. Les épis de maïs font maintenant plusieurs dizaines de fois la taille de ceux de leurs ancêtres sauvages. L'homme a aussi modifié directement les plantes par le greffage et maintenant le transgénisme.

Le nombre d’espèces végétales cultivées est beaucoup plus important que celui des espèces animales élevées, et il est plus difficile encore dans le règne végétal de dresser la liste des espèces domestiquées. On trouve ici un tableau des 30 espèces les plus cultivées dans le monde.

"Voir aussi le pour accéder à beaucoup d'autres articles concernant ces plantes."

Les raisons pour lesquelles on a domestiqué des espèces et pour lesquelles on les élève aujourd’hui sont très diverses. Il faut remarquer aussi qu’elles sont probablement distinctes : les interactions avec une espèce animale qui allaient amener à sa domestication n’avaient pas comme but immédiat ni comme projet d’en exploiter certains caractères qui le seront plus tard. L’exemple caractéristique en est la laine du mouton qui est un produit de la domestication, la toison de l’ancêtre du mouton n’ayant pas ces caractéristiques. L’exploitation de la laine s’est donc développée dans un second temps, le mouton ayant été probablement domestiqué pour sa viande.

Une vision opposée au propose une thèse qui considère que les animaux ont aussi un intérêt à la domestication selon le processus naturel de l'évolution, l'homme le soustrayant aux prédateurs, lui prodiguant des soins lorsqu'il est malade, favorisant sa reproduction.

Les animaux domestiques sont élevés pour les produits qu’ils donnent. Ce sont les produits alimentaires : viande, lait, œufs, ou non-alimentaires : laine, fourrure, cuir ainsi que d’autres produits accessoires comme les excréments pour la fertilisation voire comme combustible.
La production alimentaire est à notre époque la principale raison de l’élevage.

Leur fonction est souvent de fournir un travail ou service. C’est en particulier le transport avec les chevaux, ânes, bœufs, chameaux et même le chien. Les animaux ont longtemps été la principale énergie du travail agricole. L’utilisation de la force des animaux pour le transport et l’agriculture s’est développée jusqu’au début du avec le transport sur les canaux, tiré par des chevaux, et les progrès du matériel agricole avant la motorisation. "Voir 

La fonction d’auxiliaire de chasse a certainement été le premier métier du chien domestique. Celui-ci effectue des travaux très variés, de la garde, protection, la conduite de troupeau jusqu’aux fonctions modernes de chien d’aveugle. Certaines espèces fournissent un travail ou service particulier, de communication pour le pigeon voyageur ou un mode de chasse particulier pour le furet.
La détention et l’élevage d’animaux domestiques sans objectifs strictement utilitaires ne sont pas récents. Les animaux de compagnie sont particulièrement développés de nos jours, ceux d’ornement ont souvent une longue tradition, quoique de nouvelles espèces soient apparues à l’époque moderne, parmi les poissons notamment. Le combat d’animaux est une activité très ancienne et toujours vivace, qui engendre un élevage spécialisé. Les espèces les plus courantes sont les coqs, le poisson combattant, les chiens, les vaches et taureaux, et même un grillon ("Acheta domestica") en Chine.

Les animaux peuvent être les supports d’une activité sportive, ce qui est le cas des chevaux depuis l’Antiquité (souvent en association avec la chasse). On note encore d’autres destinations des animaux domestiques comme le spectacle.

L'agriculture et l'activité humaine liée aux espèces domestiques ont conduit à des modifications majeures de l'environnement, notamment par le déboisement, la dégradation des terres, et d'autres biais comme l'émission actuellement non négligeable de méthane, un gaz à effet de serre du fait de l'élevage abondant de ruminants.

L'agriculture et l'élevage ont permis l'accès à des ressources alimentaires beaucoup plus importantes pour un territoire donné, et par conséquent ont contribué au développement des populations humaines. L’archéologue et généticien Greger Larson explique que .

La domestication semble avoir induit chez l'espèce humaine elle-même des adaptations comme la faculté à digérer le lait plus élevée dans les populations d'Europe occidentale et d'Afrique par rapport aux populations asiatiques. La promiscuité avec des espèces animales a également favorisé l'apparition de zoonoses, maladies qui se transmettent de l'animal à l'homme, ainsi que des résistances à ces maladies. C'est également auprès des espèces sauvages que la concentration et les transports d'animaux peuvent devenir un facteur important de transmission voire d'évolution de maladies, alors que ces espèces en étaient à l'abri du fait de barrières naturelles à leur transmission.

La domestication en tant que relation, interaction ou contrôle humain sur une population animale existe sous différentes formes. Lorsqu’il ne s’agit plus de domestication à proprement parler, on peut employer le terme d’action domesticatoire. Si les modes d’élevage pour lesquels le contrôle humain est fort portent souvent sur des espèces anciennement domestiquées, les deux axes que sont le degré biologique de domestication et le mode d’élevage n’évoluent pas conjointement. Ils peuvent être croisés et faire apparaître autant de situations différentes : certains animaux sauvages peuvent être appropriés sur un territoire, faire l’objet d’un élevage, tandis qu’il existe des animaux domestiques sans propriétaire (pigeons des villes). D’autre part, du point de vue culturel, certains types d’interaction entre humains et animaux, quoique similaires, sont perçus de façon différente.

La forme la plus poussée de domestication correspond à l’élevage intensif, où l’éleveur fournit tout ce qui est nécessaire au développement des animaux, pour maximiser leur production ou permettre leur élevage sur des surfaces réduites. Elle correspond à un contrôle maximum sur les animaux. Si l’élevage intensif est "a priori" celui où l’éleveur a le contact le plus proche avec ses animaux, ce qui est le cas avec l’élevage laitier par exemple, l’intensification qui accompagne la modernisation tend au contraire à amoindrir l’interaction directe entre éleveur et animal. Ce type d’élevage concerne par ailleurs des espèces anciennement domestiquées comme d’autres qui ne le sont pas ou peu, particulièrement en aquaculture.
La pression domesticatoire peut être considérée comme moindre dans le cas d’élevage extensif, c’est-à-dire s’appuyant sur de plus grandes surfaces pour la même production, ce qui correspond en général à une plus grande autonomie des animaux.

Un élevage de type extensif n’exclut pourtant pas un contact très proche de l’éleveur avec les animaux, notamment dans les systèmes d’élevage traditionnels, non plus qu’une sélection réfléchie et stricte. Celle-ci est cependant souvent moins forte voire inexistante et ces systèmes valorisent en premier lieu l’adaptation des animaux à leur milieu d’élevage.

L’interaction des animaux de compagnie avec leurs maîtres est bien sûr particulièrement importante et ils peuvent être intégrés à une cellule familiale, ce qui est habituellement le cas du chien. Ils apportent souvent un soutien affectif, psychologique, voire physique en aidant à la mobilité personnelle et au transport.

Les activités pratiquées avec ces animaux relèvent souvent du sport ou des loisirs comme l’équitation ou la chasse. Ces activités exigent un apprentissage tant du côté humain qu’animal ainsi qu’un mode de communication particulière et pouvant être très élaboré.

L’absence de contraintes strictement utilitaires permet l’apparition de variétés et de types d’animaux très variés, chez les animaux d’ornement en particulier.

Le commensalisme est une forme d’interaction entre deux espèces. Plusieurs espèces animales sont commensales de l’homme en ce qu’elles vivent en fonction de son activité, quoique sans être directement contrôlées par lui. L'impact de ces espèces pour les activités humaines va de la nuisance au bénéfice mutuel, en passant par l'absence d'effet sensibles, ce qui correspond au "commensalisme" au sens strict. Ces relations peuvent être considérées comme des "cas limites" de la domestication.

Le qualificatif "domestique" du nom vernaculaire ou scientifique de plusieurs espèces correspond à cette acception, ce qui est le cas notamment de la mouche domestique ("Musca domestica"), de la souris domestique ("Mus musculus") sauvage (sa forme blanche est réellement domestiquée), ou du moineau domestique ("Passer domesticus"), dont l'homme ne contrôle pas les populations, mais qui se sont adaptés à son voisinage. On emploie la notion de synanthropie pour décrire l'adaptation qui accompagne cette relation à l'espèce humaine, lorsqu'elle a les caractères d'une véritable microévolution.

Le commensalisme concerne également des animaux plus gros, éliminant les déchets voire les charognes jusqu'en ville (vautour fauve, vautour noir en Afrique et en Amérique du Sud, chien paria en Orient) et de nouvelles espèces se sont adaptées aux villes comme la mouette rieuse ou le renard roux, notamment en Angleterre.

Le lien de certaines espèces avec l'être humain peut tendre vers le mutualisme lorsque celles-ci sont non seulement tolérées mais considérées comme utiles en tant que prédateurs d'insectes ou rongeurs nuisibles. Ce sont notamment la cigogne, ou l'hirondelle. Ceux-ci peuvent vivre en véritable association avec un système agricole dans lequel ils ont un rôle et une place, et bénéficier sinon de soin, au moins d'une protection de la part de l'homme. On relève des cas de véritables collaborations entre hommes et animaux "libres" comme celle des dauphins qui rabattent des bancs de poissons vers les filets de pêcheurs côtiers en Mauritanie par exemple, les hommes comme les dauphins ayant ainsi de meilleurs chances de capture. Les dingos australiens, quoique beaucoup plus indépendants des hommes que leur congénères domestiques, chassaient également en association avec l’homme.

Le caractère "obligatoire" d'une telle relation n'est pas toujours avéré, néanmoins l'extension de l'aire de répartition d'une espèce commensale de l'homme paraît le plus souvent conditionnée à cette relation et donc aux activités humaines. Ainsi la souris domestique de Saint-Kilda a disparu après l'évacuation des habitants de cet archipel.

Celui-ci représente un paradoxe dans la dualité sauvage/domestique. Au-delà de la détention et de l’élevage occasionnel d’animaux sauvages par des parcs zoologiques, des aquariums, des chercheurs ou des particuliers, qui peut concerner la plupart des espèces, il existe sous des formes et avec des objectifs variés. L’élevage d’animaux sauvages induit en fonction de son type et des espèces concernées des questions particulières, notamment juridiques au titre de la protection des espèces ou à propos de la propriété des animaux.

L’élevage conservatoire porte sur une espèce en général rare ou disparue à l’état sauvage, pour sa sauvegarde et éventuellement sa réintroduction. Dans ce cas, on redoute la domestication et on tente d’éviter que cet élevage modifie les caractères originels de l’espèce. La réussite de l’élevage en captivité lui-même et plus encore celle de la réintroduction des animaux dans leur milieu naturel, conditionnent l'atteinte des objectifs de l'élevage conservatoire mais, sous cette réserve, la préservation du patrimoine génétique d'une espèce sauvage est apparue tout à fait possible par un élevage même très artificialisé.

On élève des espèces de gibier en conditions artificielles pour produire des animaux sauvages destinés au repeuplement, des produits à chasser directement ou pour la production de viande. Les espèces sont typiquement : le faisan de Colchide ou le sanglier en Europe, et d’autres espèces suivant les régions du monde. Plusieurs espèces sauvages sans lien avec la chasse font également l’objet d’un élevage de production.
On appelle gestion de faune sauvage ou gestion cynégétique l’action coordonnée, de la part ou pour le compte de chasseurs, sur une partie des espèces sauvages d’un territoire. Elle comporte par exemple l’aménagement du territoire pour favoriser une espèce, le nourrissage occasionnel, l’apport de sel, la mise à disposition de cultures destinées au gibier, et surtout le choix réfléchi des prélèvements en nombre et en qualité (âge et sexe des animaux) ainsi que des introductions éventuelles (repeuplement). En tant que telle, on peut la qualifier « d’action domesticatoire », sans que cela présume nécessairement une évolution des espèces de gibier qui en sont l’objet en espèces domestiques.

Lorsque cette action est orientée vers la production, les anglo-saxons emploient le terme de "game ranching" qui peut être traduit comme élevage extensif, en milieu naturel, d’espèces sauvages ou de gibier. Cela consiste à gérer des populations, typiquement de grands herbivores comme des antilopes, dans leur milieu naturel et dans une optique de production, ou encore de chasse payante. Cette pratique est connue en Afrique australe, mais existe ou a existé sur les autres continents : en Amérique du Sud, la vigogne par exemple a fait et fait d'ailleurs encore l’objet de captures annuelles, où les animaux sont tondus et pour partie abattus. Cette pratique constitue de fait une action humaine de sélection, même si elle ne se fixe pas d'objectifs, sur les populations qui en sont l'objet. En Europe, le lièvre a fait l'objet d'un élevage de ce type.

De la même façon, une gestion de faune aquatique ou gestion halieutique est pratiquée pour le compte des pêcheurs dans les milieux aquatiques.

La gestion halieutique consiste à veiller à l’utilisation durable des ressources aquatiques ainsi qu’à la protection des processus écologiques et de la diversité biologique qui sont essentiels à leur maintien. Elle vise à faire en sorte que ces ressources aquatiques fournissent le maximum d’avantages durables et que la base de la ressource soit maintenue, en mer comme dans les eaux continentales.

Cette gestion de la faune aquatique peut conduire à une action domesticatoire plus ou moins poussée.

Ainsi en France, les espèces élevées en pisciculture d’étang sont peu transformées en dehors de la carpe, et le mode d’élevage correspond à un contrôle humain très faible. Les truites issues d’élevage relâchées en rivières, quoique biologiquement et techniquement plus domestiques, relèvent dans cette situation de la faune sauvage.

Une variante en est le "sea ranching" ou pacage marin qui consiste à ne contrôler qu’une partie du cycle d’élevage : en général la reproduction ou les premiers stades de développement, puis à relâcher les animaux pour grossissement en pleine mer en vue de leur recapture. Cette technique est appliquée au saumon, à la coquille Saint-Jacques.

L'expérience a également été menée avec les tortues de mer, espèces menacées et prisées pour leur chair ou leurs écailles, dont les premiers stades de développement sont sujets à une forte mortalité en milieu naturel. Les résultats sont mitigés, en raison de problèmes comportementaux observés chez certains sujets lorsqu'ils sont nés au sein d'un élevage puis relâchés au bout d'un certain âge, ou d'effondrements de la population sauvage lorsque la reproduction n'est pas réalisée au sein de l'élevage et que le ramassage continu des œufs dans la nature est trop important. Cet élevage controversé pourrait cependant endiguer partiellement le braconnage des tortues de mer, notamment celui de la tortue imbriquée.

L'exploitation d'une espèce à l'état sauvage, comme c'est le cas des cerfs, plutôt que son élevage plus étroitement contrôlé paraît relever de systèmes voire de choix qui comportent des dimensions techniques, biologiques, mais aussi historiques, sociales et culturelles.

On observe pour la plupart des espèces domestiques la possibilité de s’affranchir de la tutelle de l’homme, c’est-à-dire de reformer des populations vivant à l’état sauvage. Ce phénomène, appelé marronnage ou féralisation, survient notamment dans des milieux nouveaux pour l'espèce, notamment dans les îles, où celle-là peut se révéler invasive, et provoquer des dégâts écologiques comme la disparition d'espèces locales par prédation ou concurrence. Dans quelques cas, lorsqu'au contraire la forme sauvage de l'espèce est déjà présente, celle-ci peut subir une "« pollution génétique »" par croisement de ses représentants avec des animaux d'origine domestique.

Le marronnage est probablement un élément de l’histoire de la domestication de plusieurs espèces, celles-ci ayant pu être élevées, puis s’échapper dans un milieu où l’homme les aura introduites, avant d’être à nouveau domestiquées. Cela s’est vu dans la période historique pour les mustangs repris par les Indiens des Plaines.

Le marronnage semble montrer que la domestication d’une espèce n’est pas définitive ni irréversible. Cependant si ces animaux se montrent à nouveau tout à fait adaptés à la vie sauvage, ils gardent en général leurs caractères d’espèces ou de races domestiquées.

La théorie de l'autodomestication humaine avance que l'être humain s'est sélectionné génétiquement, la sélection naturelle laissant place à la sélection culturelle. 

Louis Bolk avait avancé la théorie de la néoténie ou théorie de la fœtalisation avançant que l'homme est un être juvénile. 

Les traits de néoténie (ou foetalisation) humaine s'expliqueraient ici par la domestication de l'homme par lui-même (ses parents, ses proches, la société).

Les expériences sur la domestication de Dmitri Beliaïev sur le renard argenté domestiqué montre que les animaux domestiqués (domestication par sélection génétique en évitant le contact humain) présentent, outre leur docilité, des traits de néoténie, une hausse de la sérotonine et une baisse de l'adrénaline, une période de reproduction plus longue.

Konrad Lorenz avait avancé l'idée de l'autodomestication humaine et postulé que la pression de sélection de l'homme par l'homme aurait conduit à une forme de dégénérescence de l'espèce humaine dont les plus touchées sont les races occidentales. Emil Kraepelin et Ernst Rüdin avançaient aussi cette idée d'autodomestication qui conduit à une dégénérescence de l'espèce ( voir Théorie de la dégénérescence).

Eugen Fischer, considérant que la blondeur et les yeux bleus sont des signes distinctifs de domestication, a proposé ces traits pour définir qui exterminer lors de la période nazi, alors même que les critères du aryen parfait étaient la blondeur et les yeux bleus.

Contrairement à l'eugénisme qui se projette dans l'avenir et a pour objectif d'améliorer le génome humain par diverses méthodes, la théorie de l'autodomestication avance que l'être humain est déjà le résultat d'une sélection génétique par lui-même sans en être conscient.

Une théorie avance que les bonobos pourraient aussi s'être autodomestiqués.

Miguel Ruiz aborde dans son livre "Les quatre accords toltèques", la domestication de l'homme par la transmission d'informations qui constituent le rêve et les règles du rêve. Notre nature personnelle ayant été perdue lors du processus de domestication. 

U. G. Krishnamurti aborde également la domestication de l'homme par la société via l'éducation, la culture et la religion. Cette domestication l'empêchant de se révéler et de s'éveiller dans ce qu'il appelle l'« état naturel ». Mais cette domestication est selon lui physique et pour s'en libérer le corps physique doit subir une mutation physique. Ce n'est pas une libération par l'esprit, mais une libération physique (chaque cellule stockant la connaissance).





</doc>
<doc id="1063" url="https://fr.wikipedia.org/wiki?curid=1063" title="Expression régulière">
Expression régulière

En informatique, une expression régulière ou expression normale ou expression rationnelle ou motif, est une chaîne de caractères, qui décrit, selon une syntaxe précise, un ensemble de chaînes de caractères possibles. Les expressions régulières sont également appelées "regex". Les expressions rationnelles sont issues des théories mathématiques des langages formels des années 1940. Leur capacité à décrire avec concision des "ensembles réguliers" explique qu’elles se retrouvent dans plusieurs domaines scientifiques dans les années d’après-guerre et justifie leur adoption en informatique. Les expressions régulières sont aujourd’hui utilisées pour programmer des logiciels avec des fonctionnalités de lecture, de contrôle, de modification, et d'analyse de textes ainsi que dans la manipulation des langues formelles que sont les langages informatiques.

Ces expressions "régulières" ont la qualité de pouvoir être décrites par des formules ou motifs, (en anglais patterns) bien plus simples que les autres moyens.

Dans les années 1940, Warren McCulloch et Walter Pitts ont décrit le système nerveux en modélisant les neurones par des automates simples. En 1956, le logicien Stephen Cole Kleene a ensuite décrit ces modèles en termes d’"ensembles réguliers" et d'automates. Il est considéré comme l'inventeur des expressions régulières. En 1959, Michael Rabin et Dana Scott proposent le premier traitement mathématique et rigoureux de ces concepts, ce qui leur vaudra le prix Turing en 1976.

Dans ce contexte, les expressions régulières correspondent aux grammaires de type 3 (voir Grammaire formelle) de la hiérarchie de Chomsky ; elles peuvent donc être utilisées pour décrire la morphologie d’une langue.

Ken Thompson a mis en œuvre la notation de Kleene dans l’éditeur qed, puis l’éditeur ed sous Unix, et finalement dans grep. Depuis lors, les expressions régulières ont été largement utilisées dans les utilitaires tels que lex ainsi que dans les langages de programmation nés sous Unix, tels que expr, awk, Perl, Tcl, Python, etc.

En sortant du cadre théorique, les expressions régulières ont acquis des fonctionnalités permettant de décrire des langages non rationnels. Un glissement sémantique s'est ainsi produit : la notion d'expression régulière n'a pas le même sens dans le contexte de l'informatique appliquée et dans la théorie des langages formels.

Initialement créées pour décrire des langages formels, les expressions régulières sont utilisées dans l’analyse et la manipulation des langages informatiques ; compilateurs et interprètes sont ainsi basés dessus.

Utilisée à la manière des outils de recherche de texte dans un document, une expression régulière décrit des chaînes de caractères ayant des propriétés communes, dans le but de les trouver dans un bloc de texte pour leur appliquer un traitement automatisé, comme un ajout, leur remplacement, leur modification ou leur suppression.

Beaucoup d'éditeurs de texte et la plupart des environnement de développement intégrés permettent de mettre en œuvre les expressions régulières. Un grand nombre d’utilitaires Unix savent les utiliser nativement. Les plus connus desquels étant GNU grep ou GNU sed qui, à la manière des éditeurs de texte, utilisent ces expressions pour parcourir de façon automatique un document à la recherche de morceaux de texte compatibles avec le motif de recherche, et éventuellement effectuer un ajout, une substitution ou une suppression.

Les interface en ligne de commande (ou shells) utilisent un système apparenté mais distinct et moins expressif appelé ou globbing.

Les expressions régulières sont fréquemment employées dans les activités d'administration système, de développement logiciel et de traitement automatique du langage naturel. Elles ont vu un nouveau champ d’application avec le développement d’Internet, et la diffusion de code malveillant ou de messages pourriels. Des filtres et des robots utilisant ces expressions sont utilisés pour détecter les éléments potentiellement nuisibles.

En théorie des langages formels, une expression régulière est une expression représentant un langage rationnel. Dans ce contexte, les expressions régulières ont un pouvoir expressif plus limité : cette notion a un sens plus large en informatique appliquée qu'en théorie des langages formels.

Une expression régulière est une suite de caractères typographiques (qu’on appelle plus simplement « motif » – « » en anglais) décrivant un ensemble de chaînes de caractères. Par exemple l’ensemble de mots « ex-équo, ex-equo, ex-aequo et ex-æquo » peut être condensé en un seul motif « ex-(a?e|æ|é)quo ». Les mécanismes de base pour former de telles expressions sont basés sur des caractères spéciaux de substitution, de groupement et de quantification.

Une barre verticale sépare le plus souvent deux expressions alternatives : « equo|aequo » désigne soit equo, soit aequo. Il est également possible d’utiliser des parenthèses pour définir le champ et la priorité de la détection, « (ae|e)quo » désignant le même ensemble que « aequo|equo » et de quantifier les groupements présents dans le motif en apposant des caractères de quantification à droite de ces groupements.

Les quantificateurs les plus répandus sont :

Les symboles dotés d'une sémantique particulière peuvent être appelés « opérateurs », « métacaractères » ou « caractères spéciaux ». Les caractères qui ne représentent qu'eux-mêmes sont dits « littéraux ».

Les expressions régulières peuvent être combinées, par exemple par concaténation, pour produire des expressions régulières plus complexes.

Lorsqu'une chaîne de caractère correspond à la description donnée par l'expression régulière, on dit qu'il y a « correspondance » entre la chaîne et le motif, ou que le motif « reconnaît » la chaîne. Cette correspondance peut concerner la totalité ou une partie de la chaîne de caractères. Par exemple, dans la phrase « Les deux équipes ont terminé ex-æquo et se sont saluées. », la sous-chaîne « ex-æquo » est reconnue par le motif « ex-(a?e|æ|é)quo ».

Par défaut, les expressions régulières sont sensibles à la casse. Lorsque c'est possible, elles tentent de reconnaître la plus grande sous-chaîne correspondant au motif : on dit qu'elles sont « gourmandes ». Par exemple, codice_7 reconnaît la totalité de la chaîne « Aaaaaaa » plutôt qu'une partie « Aaa » (gourmandise), mais elle ne reconnaît pas la chaîne « aaaA » (sensibilité à la casse).

Dans le domaine de l'informatique, un outil permettant de manipuler les expressions régulières est appelé un "moteur d'expressions régulières" ou "moteur d'expressions rationnelles". Il existe des standards permettant d'assurer une cohérence dans l'utilisation de ces outils.

Le standard POSIX propose trois jeux de normes :

Les expressions régulières de perl sont également un standard de fait, en raison de leur richesse expressive et de leur puissance. Tout en suivant leur propre évolution, elles sont par exemple à l'origine de la bibliothèque PCRE. ECMAScript propose également dans le document Standard ECMA-262 une norme employée par exemple par JavaScript.

Les notations ou leurs sémantiques peuvent varier légèrement d'un moteur d'expression régulière à l'autre. Ils peuvent ne respecter que partiellement ces normes, ou de manière incomplète, ou proposer leurs propres fonctionnalités, comme GNU ou le Framework .NET. Les spécificités de chacun sont abordées plus loin dans cet article.

Une classe de caractères désigne un ensemble de caractères. Elle peut être définie de différentes manières :

Des unions de classes de caractères peuvent être faites : codice_12 désigne l'ensemble constitué des caractères « 0 » à « 9 » et des lettres « a » et « b ». Certaines bibliothèques permettent également de faire des intersections de classes de caractères.

Entre les crochets, les métacaractères sont interprétés de manière littérale : codice_13 désigne l'ensemble constitué des caractères « . », « ? » et « * ».

Les classes de caractères les plus utilisées sont généralement fournies avec le moteur d'expression régulière. Un inventaire de ces classes est dressé dans la table ci-dessous.

La bibliothèque POSIX définit des classes au départ pour l'ASCII, puis, par extensions, pour d'autres formes de codages de caractères, en fonction des paramètres régionaux.

Dans Unicode et des langages comme le perl, des ensembles de caractères sont définis au travers de la notion de propriétés de caractères. Cela permet de désigner un ensemble de caractères en fonction de sa catégorie (exemples : lettre, ponctuation ouvrante, ponctuation fermante, séparateur, caractère de contrôle), en fonction du sens d'écriture (par exemple de gauche à droite ou de droite à gauche), en fonction de l'alphabet (exemples : latin, cyrillique, grec, Hiragana) ; en fonction de l'allocation des blocs, ou même selon les mêmes principes que les classes de caractères POSIX (à ce sujet, lire la section Expressions régulières et Unicode).

Par exemple, dans le standard POSIX, codice_14 fait correspondre un caractère parmi l’ensemble formé par les lettres capitales et les lettres minuscules « a » et « b ». Dans le standard ASCII, cette expression régulière s'écrirait codice_15.

La notion de classe d'équivalence ne doit pas être confondue avec la notion de classe de caractères.

Par exemple, dans la locale FR, la classe [=e=] regroupe l'ensemble des lettres {e, é, è, ë, ê}.

Ceci signifie que lorsqu'elles sont collationnées, les lettres {e, é, è, ë, ê} apparaissent dans le même jeu de caractères, après le "d", et avant le "f".

La plupart des standards et moteurs d'expressions régulières proposent des fonctions avancées. Notamment :


Les notations utilisées sont très variables. Ce chapitre regroupe d'une part les notations propres à différentes implémentations, et d'autre part, l'entreprise de normalisation.

Le standard [[POSIX]] a cherché à remédier à la prolifération des syntaxes et fonctionnalités, en offrant un standard d’expressions régulières configurables. On peut en obtenir un aperçu en lisant le manuel de codice_25 sous une grande partie des dialectes [[Unix]] dont [[GNU/Linux]]. Toutefois, même cette norme n’inclut pas toutes les fonctionnalités ajoutées aux expressions régulières de Perl.

Enfin, POSIX ajoute le support pour des plates-formes utilisant un jeu de caractère non basé sur l’ASCII, notamment [[EBCDIC]], et un support partiel des locales pour certains méta-caractères.

Les utilitaires du monde [[Unix]] tels que [[Stream Editor|sed]], [[GNU grep]], [[ed (logiciel)|ed]] ou [[vi]] utilisent par défaut la norme BRE (« "Basic Regular Expression" ») de POSIX. Dans celle-ci, les accolades, les parenthèses, le symbole « ? » et le symbole « + » ne sont pas des métacaractères : ils ne représentent qu'eux même. Pour prendre leur sémantique de métacaractères, ils ont besoin d'être [[Caractère d'échappement|échappés]] par le symbole « \ ».

Exemple : l'expression régulière codice_26 reconnaît « (abc)+ » mais pas « abcabd », pour laquelle codice_27 convient.

Les expressions régulières étendues POSIX (ERE pour « "Extented Regular Expression" ») sont souvent supportées dans les utilitaires des distributions Unix et GNU/Linux en incluant le [[drapeau (informatique)|drapeau]] -E dans la [[ligne de commande]] d’invocation de ces utilitaires. Contrairement aux expressions régulières basiques, elles reconnaissent les caractères vus précédemment comme des métacaractères. Ils doivent ainsi être échappés pour être interprétés littéralement.

La plupart des exemples donnés en présentation sont des expressions régulières étendues POSIX.

Comme les caractères codice_28, codice_29, codice_30, codice_31, codice_32, codice_3, codice_1, codice_5, codice_36, codice_37, codice_38 , codice_39 et codice_40 sont utilisés comme symboles spéciaux, ils doivent être référencés dans une [[caractère d'échappement|séquence d’échappement]] s’ils doivent désigner littéralement le caractère correspondant. Ceci se fait en les précédant avec une barre oblique inversée codice_40.

Des extensions semblables sont utilisées dans l’éditeur alternatif "[[emacs]]" qui utilise un jeu de commandes différent mais reprend les mêmes expressions régulières en apportant une notation étendue. Les expressions régulières étendues sont maintenant supportées aussi dans "[[vim]]", la version améliorée de "vi".

De plus, de nombreuses autres séquences d’échappement sont ajoutées pour désigner des classes de caractères prédéfinies. Elles sont spécifiques à chaque utilitaire ou parfois variables en fonction de la version ou la plate-forme (cependant elles sont stables depuis longtemps dans emacs qui a fait figure de précurseur de ces extensions, que d’autres auteurs ont partiellement implémentées de façon limitée ou différente).

[[python (langage)|Python]] utilise des expressions régulières basées sur les expressions régulières POSIX, avec quelques extensions ou différences.

Les éléments compatibles POSIX sont les suivants :

La séquence codice_56 désigne le caractère de retour arrière (codice_57 avec un codage compatible ASCII) lorsqu'elle est utilisée à l'intérieur d'une classe de caractère, et la limite d'un mot autrement.

Le [[Berkeley Software Distribution|système d'exploitation BSD]] utilise la bibliothèque "regex" écrite par . Compatible avec la norme POSIX 1003.2, cette bibliothèque est également utilisée par MySQL (avec les opérateurs REGEXP et NOT REGEXP) et PostgreSQL (avec l'opérateur « ~ » et ses variantes).

Le moteur d'expressions régulières du langage [[Tool Command Language|Tcl]] est issu de développements d'Henry Spencer postérieurs à ceux de la bibliothèque BSD. Les expressions régulières sont appelées Expressions régulières avancées (ou ARE, Advanced Regular Expressions) et sont légèrement différentes des expressions régulières étendues de POSIX. Les expressions régulières basiques et étendues sont également supportées.

[[Perl (langage)|Perl]] offre un ensemble d’extensions particulièrement riche. Ce [[langage de programmation]] connaît un succès très important dû à la présence d’opérateurs d’expressions régulières inclus dans le langage lui-même. Les extensions qu’il propose sont également disponibles pour d’autres programmes sous le nom de "lib [[PCRE]]" (, littéralement "bibliothèque d’expressions régulières compatible avec Perl"). Cette [[bibliothèque logicielle|bibliothèque]] a été écrite initialement pour le [[serveur de courrier électronique]] [[Exim]], mais est maintenant reprise par d’autres projets comme [[Langage de programmation Python|Python]], [[Apache (logiciel)|Apache]], [[Postfix]], [[KDE]], [[Analog]], [[PHP]] et [[Ferite]].

Les spécifications de [[Perl 6]] régularisent et étendent le mécanisme du système d’expressions régulières.
De plus il est mieux intégré au langage que dans Perl 5. Le contrôle du [[retour sur trace]] y est très fin. Le système de regex de Perl 6 est assez puissant pour écrire des [[analyseur syntaxique|analyseurs syntaxiques]] sans l’aide de modules externes d’analyse. Les expressions régulières y sont une forme de sous-routines et les grammaires une forme de [[classe (informatique)|classe]]. Le mécanisme est mis en œuvre en [[assembleur]] [[Parrot (machine virtuelle)|Parrot]] par le module [[PGE (Perl)|PGE]] dans la mise en œuvre Parrot de Perl 6 et en [[Haskell]] dans la mise en œuvre [[Pugs]]. Ces mises en œuvre sont une étape importante pour la réalisation d’un [[compilateur]] Perl 6 complet. Certaines des fonctionnalités des regexp de Perl 6, comme les captures nommées, sont intégrées depuis Perl 5.10.

PHP supporte deux formes de notations : la syntaxe [[POSIX]] (POSIX 1003.2) et celle, beaucoup plus riche et performante, de la bibliothèque [[PCRE]] (Perl Compatible Regular Expression).

D’autres utilitaires ajoutent souvent leurs propres conventions. 

Un des défauts reprochés à PHP est lié à son support limité des chaînes de caractères, alors même qu’il est principalement utilisé pour traiter du texte, puisque le texte ne peut y être représenté que dans un jeu de caractères codés sur , sans pouvoir préciser clairement quel codage est utilisé. En pratique, il faut donc adjoindre à PHP des bibliothèques de support pour le codage et le décodage des textes, ne serait-ce que pour les représenter en UTF-8. 

[[International Components for Unicode|ICU]] définit une bibliothèque portable pour le traitement de textes internationaux. Celle-ci est développée d’abord en [[C (langage)|langage C]] (version nommée ICU4C) ou pour la [[plate-forme Java]] (version nommée ICU4J). Des portages (ou adaptations) sont aussi disponibles dans de nombreux autres langages, en utilisant la bibliothèque développée pour le langage C (ou [[C++]]).

Les expressions régulières utilisables dans ICU reprennent les caractéristiques des expressions régulières de Perl, mais les complètent pour leur apporter le support intégral du jeu de caractères Unicode (voir la section suivante pour les questions relatives à la normalisation toujours en cours). Elles clarifient également leur signification en rendant les expressions régulières indépendantes du jeu de caractère codé utilisé dans les documents, puisque le jeu de caractères Unicode est utilisé comme codage pivot interne.

En effet, les expressions régulières de Perl (ou PCRE) ne sont pas portables pour traiter des documents utilisant des jeux de caractères codés différents, et ne supportent pas non plus correctement les jeux de caractères codés multi-octets (à longueur variable tels que [[ISO 2022]], [[Shift-JIS]], ou [[UTF-8]]), ou codés sur une ou plusieurs unités binaires de plus de (par exemple [[UTF-16]]) puisque le codage effectif de ces jeux sous forme de séquences d’octets dépend de la plate-forme utilisée pour le traitement (ordre de stockage des octets dans un mot de plus de ).

ICU résout cela en adoptant un traitement utilisant en interne un jeu unique défini sur et supportant la totalité du jeu de caractères universel (UCS), tel qu’il est défini dans la norme [[ISO 10646|ISO/IEC 10646]] et précisé sémantiquement dans le standard [[Unicode]] (qui ajoute à la norme le support de propriétés informatives ou normatives sur les caractères, et des recommandations pour le traitement automatique du texte, certaines de ces recommandations étant optionnelles ou informatives, d’autres étant devenues standards et intégrées au standard Unicode lui-même, d’autres enfin ayant acquis le statut de norme internationale à l’ISO ou de norme nationale dans certains pays).

ICU supporte les extensions suivantes, directement dans les expressions régulières, ou dans l’expression régulière d’une classe de caractères (entre codice_58) :

Les expressions régulières d’ICU sont actuellement parmi les plus puissantes et les plus expressives dans le traitement des documents multilingues. Elles sont largement à la base de la normalisation (toujours en cours) des expressions régulières Unicode (voir ci-dessous) et un sous-ensemble est supporté nativement dans la bibliothèque standard du langage [[Java (langage)|Java]] (qui utilise en interne un jeu de caractères portable à codage variable, basé sur [[UTF-16]] avec des extensions, et dont les unités de codage sont sur ).

ICU est une bibliothèque encore en évolution. En principe, elle devrait adopter toutes les extensions annoncées dans Perl (notamment les captures nommées), dans le but d’assurer l’interopérabilité avec Perl 5, Perl 6, et PCRE, et les autres langages de plus en plus nombreux qui utilisent cette syntaxe étendue, et les auteurs d’ICU et de Perl travaillent en concert pour définir une notation commune. Toutefois, ICU adopte en priorité les extensions adoptées dans les expressions régulières décrites dans le standard Unicode, puisque ICU sert de référence principale dans cette annexe standard d’Unicode.

Toutefois, il n’existe encore aucun standard ou norme technique pour traiter certains aspects importants des expressions régulières dans un contexte multilingue, notamment :

Pour préciser ces derniers aspects manquants, des métacaractères supplémentaires devraient pouvoir être utilisés pour contrôler ou filtrer les occurrences trouvées, ou bien un ordre normalisé imposé à la liste des occurrences retournées. Les auteurs d’applications doivent donc être vigilants sur ces points et s’assurer de lire toutes les occurrences trouvées et pas seulement la première, afin de pouvoir décider laquelle des occurrences est la mieux appropriée à une opération donnée.

Les expressions régulières ont originellement été utilisées avec les caractères [[ASCII]]. Beaucoup de moteurs d’expressions régulières peuvent maintenant gérer l’[[Unicode]]. Sur plusieurs points, le jeu de caractères codés utilisés ne fait aucune différence, mais certains problèmes surgissent dans l’extension des expressions régulières pour Unicode.

Une question est de savoir quel format de représentation interne d’Unicode est supporté. Tous les moteurs d’expressions régulières en ligne de commande attendent de l’[[UTF-8]], mais pour les bibliothèques, certaines attendent aussi de l’UTF-8, mais d’autres attendent un jeu codé sur UCS-2 uniquement (voire son extension [[UTF-16]] qui restreint aussi les séquences valides), ou sur UCS-4 uniquement (voire sa restriction normalisée [[UTF-32]]).

Une deuxième question est de savoir si l’intégralité de la plage des valeurs d’une version d’Unicode est supportée. Beaucoup de moteurs ne supportent que le [[Basic Multilingual Plane]], c’est-à-dire, les caractères encodables sur . Seuls quelques moteurs peuvent (dès 2006) gérer les plages de valeurs Unicode sur .

Une troisième question est de savoir comment les constructions ASCII sont étendues à l’Unicode.

Cependant, en pratique ce n’est souvent pas le cas :

Un autre domaine dans lequel des variations existent est l’interprétation des indicateurs d’insensibilité à la casse.

Une autre réponse à Unicode a été l’introduction des classes de caractères pour les blocs Unicode et les propriétés générales des caractères Unicode: 

Notes :

Il existe au moins trois familles d'[[algorithme]]s qui déterminent si une chaîne de caractères correspond à une expression régulière.

La plus ancienne approche, dite explicite, repose sur la traduction de l'expression régulière en un [[automate fini déterministe]] (AFD). La construction d'un tel automate pour une expression régulière de taille "m" a une [[Théorie de la complexité (informatique théorique)|complexité]] en taille et en mémoire en "[[Comparaison asymptotique|O]](2)" mais peut être exécutée sur une chaîne de taille "n" en un temps "O(n)".

Une approche alternative, dite implicite, consiste à simuler un [[automate fini non déterministe]] en construisant chaque AFD à la volée et en s'en débarrassant à l'étape suivante. Cette approche évite la complexité exponentielle de l'approche précédente, mais le temps d'exécution augmente en "O(mn)". Ces algorithmes sont rapides mais certaines fonctionnalités telles que la recapture de sous-chaînes et la quantification non gourmande sont difficiles à mettre en oeuvre.

La troisième approche consiste à confronter le motif à la chaîne de caractères par [[séparation et évaluation]] (« "backtracking" »). Sa complexité algorithmique est exponentielle dans le pire des cas, par exemple avec des motifs tels que codice_77, mais donne de bons résultats en pratique. Elle est plus flexible et autorise un plus grand pouvoir expressif, par exemple en simplifiant la recapture de sous-chaînes.

Certaines [[implémentation]]s tentent de combiner les qualités des différentes approches, en commençant la recherche avec un AFD, puis en utilisant le backtracking lorsque c'est nécessaire.



[[Catégorie:Langage formel]]
[[Catégorie:Automates finis et langages réguliers]]
[[Catégorie:Programmation informatique]]

</doc>
<doc id="1064" url="https://fr.wikipedia.org/wiki?curid=1064" title="Episyrphus">
Episyrphus

Episyrphus est un genre d'insectes diptères brachycères de la famille des syrphidés (ou syrphes), dont les larves ont pour proies principalement les pucerons colonisant la flore sauvage et aussi les arbres fruitiers, les cultures légumières, les grandes cultures...
Seule espèce appartenant au genre "Episyrphus" selon :

Espèces de ce genre selon :



</doc>
<doc id="1065" url="https://fr.wikipedia.org/wiki?curid=1065" title="Erigone (genre)">
Erigone (genre)

Erigone est un genre d'araignées aranéomorphes de la famille des Linyphiidae.

Les espèces de ce genre se rencontrent en Amérique, en Europe, en Asie, en Afrique et en Océanie.

Les mâles du genre "Erigone" sont remarquables par les épines qu'ils portent sur leur céphalothorax et les tibias de leurs pédipalpes. 

Ce sont des araignées prédatrices, elles ont pour proies de petits insectes comme les psylles et les diptères.
Selon :
Selon The World Spider Catalog (version 17.5, 2017) :




</doc>
<doc id="1066" url="https://fr.wikipedia.org/wiki?curid=1066" title="Enki Bilal">
Enki Bilal

Enes Bilal, dit Enki Bilal, né le à Belgrade, est un réalisateur, dessinateur et scénariste de bande dessinée français. Son œuvre se situe en partie dans la science-fiction et aborde les thèmes du temps ou de la mémoire. En 1987, il obtient le Grand prix du festival d'Angoulème.

Enki Bilal est né en République fédérative socialiste de Yougoslavie, d'un père bosniaque et d'une mère slovaque. Son patronyme, Bilal, est d'origine ottomane.

Son père était maître-tailleur et s'occupait personnellement de la garde-robe de Tito, qu'il avait connu dans la résistance et avec qui il avait sympathisé. Son enfance dans la Yougoslavie de Tito et son exil en France ont nourri l'univers envoûtant de ses albums.

En 1960, sa famille emménage à Paris, où son père était déjà parti s'installer. En 1967, les Bilal sont naturalisés.

Enki Bilal se lance d'abord dans la bande-dessinée. En 1971 il gagne un concours de bande dessinée, organisé par le journal "Pilote" et le Drugstore Number One, dans la catégorie aventures (Pilote page 53). En 1972 près un passage éclair aux Beaux-Arts, Enki Bilal publie sa première histoire, « Le Bol maudit », dans le journal "Pilote". En 1975 encontre avec le scénariste Pierre Christin et publie son premier album, "l'Appel des étoiles".

En 1980, première série personnelle, dans "Pilote", "La Foire aux immortels". La seconde partie, "La Femme piège", est éditée en album en 1986. Parallèlement, la collaboration entre Bilal et Christin se poursuit. Ils réalisent notamment, pour les éditions Dargaud et Autrement, plusieurs ouvrages d'illustrations et de photos détournées ("Los Angeles", "L'Étoile oubliée de Laurie Bloom" ; "Cœurs sanglants").

Bilal s'intéresse aussi au cinéma et à l'opéra. En 1982, il dessine sur verre une partie des décors de "La vie est un roman", film d'Alain Resnais. Deux ans plus tôt, il avait signé l'affiche d'un autre film de Resnais, "Mon oncle d'Amérique". En 1985 l fait des recherches graphiques pour "Le Nom de la rose", le film de Jean-Jacques Annaud d'après le roman d'Umberto Eco. En 1990 Bilal dessine les décors et costumes de "Roméo et Juliette" de Prokofiev, sur une chorégraphie de son ami Angelin Preljocaj. Il dessine les décors et costumes d"'O.P.A. Mia", un opéra de Denis Levaillant créé au Festival d'Avignon.

En 1984 Enki Bilal se fait journaliste à "Libération" le temps d'une interview de Gérard Manset, auteur-compositeur-interprète. Bilal avait déjà au début des années 1970 créé une illustration sur le thème de "La mort d'Orion" (album de Manset) et il illustrera la pochette d'un disque hommage en 1996.

Bilal participe aussi régulièrement à des expositions. En novembre 1991 c'est "Opéra bulle", deux mois d'exposition à la Grande Halle de La Villette, à Paris. En 1992, l'exposition "Transit" à la Grande Arche de la Défense, près de Paris. C'est aussi l'année de "Froid Équateur", troisième tome de la trilogie Nikopol dans lequel il invente le chessboxing. En 2013, il expose au Musée du Louvre une vingtaine de photographies de tableaux célèbres dans lesquelles il dessine des fantômes ("Les Fantômes du Louvre. Enki Bilal"). En 2013 également, il crée l'exposition "Mécanhumanimal, Enki Bilal au Musée des arts et métiers". Il y présente une rétrospective de son œuvre, ainsi qu'une sélection d'objets du Musée des arts et métiers qu'il a choisis dans les réserves, et rebaptisés en écho à son univers.

En janvier 1987, Bilal obtient le Grand Prix du Festival international de la bande dessinée d'Angoulême. En mai 2006, il crée l'illustration du timbre de France Europa sur le thème de l'intégration.

En 2011, il publie l'album "Julia et Roem" (Casterman) ainsi qu'un livre d'entretiens sur sa vie et son œuvre, "Ciels d'orage" (Flammarion).

Enki Bilal explore le temps à travers des mondes . Il évoque dans ses œuvres des thèmes marquant le futur comme la fin du communisme dans les années 1980, l'obscurantisme religieux dans les années 1990 ou le changement climatique au début des années 2010.

Il évoque souvent le thème de la mémoire, par exemple dans la série "Le Sommeil du monstre", où le héros utilise sa mémoire pour remonter dans le temps et se rappeler jusqu'aux premiers jours de son existence. Il se dit également sensible à la mémoire collective.











</doc>
<doc id="1070" url="https://fr.wikipedia.org/wiki?curid=1070" title="Ensemble musical">
Ensemble musical

Un ensemble musical est un groupe de musiciens habitués à pratiquer ensemble, en amateur ou professionnellement.

Par exemple, en musique classique, un quatuor à cordes constitue un « ensemble de solistes » — comprenant les premier et deuxième violons, l'alto et le violoncelle — tandis qu'une chorale à quatre voix constitue un « ensemble de pupitres » — comprenant le pupitre des sopranos, celui des altos, celui des ténors et celui des basses.

Que l'ensemble concerne des solistes ou des pupitres, le nom de la formation reflètera le nombre de parties sollicitées — de deux à dix, sachant qu'au-delà, il n'existe pas de terminologie usuelle.

Lorsqu'on a affaire à un ensemble de solistes, l'accompagnement éventuel — un ou plusieurs instruments, par exemple — n'est généralement pas pris en compte par la terminologie usuelle.

Par exemple, le « Trio des masques » du premier acte du "Don Giovanni" de Mozart constitue bien un trio — pour deux sopranos et un ténor — mais ces trois voix sont évidemment accompagnées par l'orchestre.






</doc>
<doc id="1072" url="https://fr.wikipedia.org/wiki?curid=1072" title="Espèce">
Espèce

Dans les sciences du vivant, l’espèce (du latin "", « type » ou « apparence ») est le taxon de base de la systématique. Il existe plus d'une vingtaine de définitions de l'espèce dans la littérature scientifique. La définition la plus communément admise est celle du concept biologique de l'espèce énoncé par Ernst Mayr en 1942 : une espèce est une population ou un ensemble de populations dont les individus peuvent effectivement ou potentiellement se reproduire entre eux et engendrer une descendance viable et féconde, dans des conditions naturelles. Ainsi, l'espèce est la plus grande unité de population au sein de laquelle le flux génétique est possible et les individus d'une même espèce sont donc génétiquement isolés d’autres ensembles équivalents du point de vue reproductif.

Pourtant le critère d’interfécondité ne peut pas toujours être vérifié : c'est le cas pour les fossiles, les organismes asexués ou pour des espèces rares ou difficiles à observer. D’autres définitions peuvent donc être utilisées :

L'espèce est un concept flou dont il existe une multitude de définitions dans la littérature scientifique. Dans son sens le plus simple, le concept de l'espèce permet de distinguer les différents types d'organismes vivants. Différentes définitions permettent d'identifier plus précisément les critères distinctifs de l'espèce. : c'est ici un concept humain décrivant à un moment donné une réalité concrète biologique en constante évolution. (l’« évolution » est la différence morphologique et génétique que l’on observe d’une génération à l’autre entre ascendants et descendants, qui ne sont jamais identiques sauf en cas de clonage, et ce sont aussi les changements dans l’effectif, l'aire de répartition et les comportements d’un groupe d'individus vivants). En outre, ce nom a pu changer en raison de nouvelles découvertes, descriptions ou analyses : ainsi, un même taxon peut avoir plusieurs dénominations successives et il arrive aussi que plusieurs espèces soient identifiées là où auparavant on n'en voyait qu'une, ou inversement, que l'on regroupe au sein d'une même espèces plusieurs nom (et types) différents (par exemple larves et adultes, ou bien mâles et femelles)

Avec le temps, les conditions et indications à réunir pour définir une espèce sont devenues plus nombreuses et strictes. Même si les citoyens et les pouvoirs publics n'en sont pas toujours conscients, la formation des spécialistes en classification (taxonomie) est essentielle pour la précision et la rigueur des travaux scientifiques concernant la biodiversité (mais aussi la minéralogie, la géologie et la paléontologie).

La définition la plus communément citée est celle du concept biologique de l'espèce énoncé par Ernst Mayr (1942) : « Les espèces sont des groupes de populations naturelles, effectivement ou potentiellement interfécondes, qui sont génétiquement isolées d’autres groupes similaires ». À cette définition, il a ensuite été rajouté que cette espèce doit pouvoir engendrer une progéniture viable et féconde. Ainsi, l'espèce est la plus grande unité de population au sein de laquelle le flux génétique est possible dans des conditions naturelles, les individus d'une même espèce étant génétiquement isolés d’autres ensembles équivalents du point de vue reproductif.

Le concept biologique de l'espèce s'appuie donc entièrement sur l'isolement reproductif (ou isolement génétique), c'est-à-dire l'ensemble des facteurs biologiques (barrières) qui empêchent les membres de deux espèces distinctes d'engendrer une progéniture viable et féconde. D'après Theodosius Dobzhansky, il est possible de distinguer les barrières intervenant avant l'accouplement ou la fécondation (barrières précopulatoires ou prézygotiques), et les barrières intervenant après (barrières postcopulatoires ou postzygotiques). Les barrières prézygotiques vont empêcher la copulation entre deux individus d'espèces différentes, ou la fécondation des ovules dans le cas où l'accouplement a bien lieu. Si la fécondation a lieu malgré tout, les barrières postzygotiques vont empêcher le zygote hybride de devenir un adulte viable et fécond. C'est cet isolement reproductif qui va empêcher le pool génétique de chaque espèce de s'échanger librement avec les autres et ainsi d'induire la conservation de caractères propres à chaque espèce.

Pour certaines espèces, l'isolement reproductif apparait de manière évidente (entre un animal et un végétal par exemple) mais dans le cas d'espèces étroitement apparentées, les barrières sont beaucoup moins claires. Il est donc important de préciser que la reproduction entre individus d'une même espèce doit être possible en conditions naturelles et que la progéniture doit être viable et féconde. Par exemple, le cheval et l'âne sont deux espèces interfécondes mais leurs hybrides (mulet, bardot) le sont rarement ; la progéniture n'est pas féconde, il s'agit bien de deux espèces différentes. De même, certaines espèces peuvent être croisées artificiellement mais ne se reproduisent pas ensemble dans le milieu naturel.

Néanmoins, le concept biologique de l'espèce possède certaines limites. L'isolement reproductif ne peut pas être déterminé dans le cas des fossiles et des organismes asexués (par exemple, les bactéries). De plus, il est difficile d'établir avec certitude la capacité d'un individu à s'accoupler avec d'autres types d'individus. Dans de nombreux groupes de végétaux (bouleau, chêne, saule…), il existe beaucoup d'espèces qui se croisent librement dans la nature sans que les taxonomistes les considèrent comme une seule et même espèce pour autant. De nombreuses autres définitions ont donc également cours pour passer outre les limites du concept biologique de l'espèce.

Le concept morphologique de l'espèce est le concept le plus généralement utilisé en pratique. Il consiste à identifier une espèce d'après ses caractéristiques structurales ou morphologiques distinctives. L'avantage de ce concept est qu'il est applicable aussi bien chez les organismes sexués qu'asexués et ne nécessite pas de connaître l'ampleur du flux génétique. Néanmoins, l'inconvénient majeur de ce concept réside dans la subjectivité de sa définition de l'espèce, qui peut aboutir à des désaccords quant aux critères retenus pour définir une espèce.

Une autre définition repose sur la notion de ressemblance (ou au contraire de degré de différence), concept encore très utilisé en paléontologie, où il n’y a pas d’autre option. Certains auteurs utilisent même ces deux principes pour définir les espèces.

L’étude de l’ADN permet de rechercher des ressemblances non visibles directement sur le plan physique (phénotype). Mais le critère quantitatif (nombre de gènes identiques) masque le critère qualitatif, par définition non mesurable. Ainsi, la classification des Orchidées de type Ophrys fait ressortir un grand nombre d’espèces, visiblement différentes (donc du point de vue phénotype) alors que leurs génotypes se sont révélés très proches. Le critère de ressemblance génétique est utilisé chez les bactéries (en plus des ressemblances phénotypiques). On sépare les espèces de manière que la variation génétique intraspécifique soit très inférieure à la variation interspécifique.

L’espèce biologique est aujourd’hui le plus souvent définie comme une communauté reproductive (interfécondité) de populations. Si cette définition se prête assez bien au règne animal, il est moins évident dans le règne végétal, où se produisent fréquemment des hybridations. On associe souvent le double critère de réunion par interfécondité et séparation par non-interfécondité, pour assurer la perpétuation de l’espèce.

Il existe aussi le concept d'espèce écologique, à relier à la notion de niche écologique. Une espèce est censée occuper une niche écologique propre. Cela revient à associer une espèce à des conditions de vie particulière. Cette définition proposée par Hutchinson et par Van Valen souffre des problèmes de recouvrement de niche (plusieurs espèces dont les niches sont très proches voire indiscernables).

Les espèces déterminantes sont des espèces retenues par certaines méthodes parce qu'elles sont remarquables pour la biodiversité ou menacées et jugées importantes dans l'écosystème (ou représentatives d'un habitat ou de l'état de l'écosystème) aux niveaux régional, national ou supranational pour élaborer certaines zonages (habitats déterminants, trame verte et bleue, ZNIEFF modernisées, Natura 2000)

Définir l'espèce de manière absolue semble très difficile, voire même impossible selon Darwin. Plusieurs historiens affirment d'ailleurs que si Darwin s’était arrêté au problème de la définition de l’espèce, il n’aurait jamais publié son livre majeur "De l'origine des espèces".

De manière simplificatrice, on peut ramener les diverses définitions qui ont été proposées sous trois rubriques différentes : concept typologique ou essentialiste de l'espèce (ressemblance morphologique par rapport à des individus de référence ou type) qui a prévalu pendant des siècles ; concept nominaliste (ressemblance phénoménologique des espèces qui n'ont pas d'existence) ; concept biologique ou populationnel (descendance d'ancêtres communs, liée au critère d'interfécondité) qui s’est imposé après l’avènement de la génétique mais suscite de nombreux problèmes au niveau de la classification scientifique des espèces. Ce qui a conduit des chercheurs à proposer d'abandonner la nomenclature linnéenne, de ne plus donner de noms aux différents rangs taxinomiques et d'éliminer, entre autres, le mot espèce du vocabulaire de la taxinomie. Ils veulent introduire à la place le concept de LITU (, ) qui représenterait le plus petit taxon que l’on puisse identifier.

Une question mérite d’être posée : la notion d’espèce constitue-t-elle une simple commodité de travail, ou possède-t-elle au contraire une réalité indépendante de notre système de classification ? Possède-t-elle une véritable signification dans l’absolu ? L’espèce est-elle une classe logique à laquelle des lois sont universellement applicables, ou a-t-elle la même réalité qu’un individu (par le lignage) ? Les réponses à ces considérations relèvent de l’épistémologie et de la sémantique opérationnelle autant que de la biologie.

Le problème se complique du fait que le critère d’interfécondité présente ou absente, n'est pas toujours applicable de façon tranchée : des populations A et A, A et A… A et A peuvent être interfécondes, alors que les populations A et A ne le sont pas. C'est le cas, par exemple, des populations de goélands réparties autour du globe (rapporté par Konrad Lorenz). On parle alors d’"espèce en anneau" ( variation clinale). La notion d’espèce se dissout alors dans une sorte de flou.

L’interfécondité ne permet donc pas de dire qu’il s’agit de mêmes espèces tandis que la non-interfécondité suffit à dire qu’il s’agit d’espèces différentes. Cette non-interfécondité doit être recherchée aussi et surtout dans les descendants : chevaux et ânes sont interféconds mais leurs hybrides (mulet, bardot) le sont rarement. Les deux populations forment donc des espèces différentes.

De même, certaines races de chiens (anciennement "Canis familiaris") s’hybrident sans problème — et ont une descendance féconde — avec des loups communs ("Canis lupus"), tandis que leur hybridation avec d’autres races de leur propre espèce "Canis familiaris" reste bien problématique - dans le cas par exemple d’une femelle Chihuahua et d’un mâle Saint-Bernard !

Cela s’explique par deux faits : le chien domestique est très polymorphe et c’est une sélection artificielle à partir de loups, ce dont il y a maintenant des preuves génétiques. On le nomme donc désormais "Canis lupus familiaris", c’est-à-dire comme sous-espèce du Loup, donc parfaitement interfécond avec lui… dans la limite de ce que permet physiquement l’utérus récepteur.

"Stricto sensu", le concept d'espèce suppose une hypothèse forte qui est la transitivité des interfécondations possibles ; en d'autres termes, on suppose que si X1 est interfécond avec X2, X2 avec X3, X1 sera interfécond avec Xn quelle que soit la longueur de la chaîne. Konrad Lorenz signale que cette supposition n'est pas toujours vraie, en particulier chez des oiseaux marins entre continents. Il faut d'ailleurs bien que ce genre de discontinuité existe pour qu'un phénomène de spéciation commence à apparaître lui aussi.

Les éleveurs en avaient vraisemblablement une notion non formalisée depuis l’origine même de l’élevage. Platon spéculera que puisque l’on voit des chevaux et des vaches, mais jamais d’hybride des deux, il doit exister quelque part une « forme idéale » qui contraint un animal à être l’un ou l’autre. Aristote préfèrera pour sa part éviter ces spéculations et se contenter de répertorier dans l’"Organon" ce qu’il observe. Albert le Grand s’y essaiera à son tour plus tard.

Concept empirique, la notion d’espèce a évolué avec le temps et son histoire a été marquée par la pensée de grands naturalistes comme Linné, Buffon, Lamarck et Darwin. Au , les espèces étaient considérées comme le résultat de la création divine et, à ce titre, étaient considérées comme des réalités objectives et immuables. Depuis l’avènement de la théorie de l’évolution, la notion d’espèce biologique a sensiblement évolué, mais aucun consensus n’a pu être obtenu sur sa définition.



La spéciation est le processus évolutif par lequel de nouvelles espèces apparaissent. La spéciation est à l'origine de la diversité biologique et constitue donc le point essentiel de la théorie de l'évolution. La spéciation peut suivre deux voies : l'anagénèse et la cladogénèse. L’anagénèse est une accumulation de changements graduels au cours du temps qui transforment une espèce ancestrale en une nouvelle espèce, cette voie modifie les caractéristiques d'une espèce mais ne permet pas d'augmenter le nombre d'espèces. La cladogénèse est la scission d'un patrimoine génétique en au moins deux patrimoines distincts, ce processus est à l'origine de la diversité biologique car il permet d'augmenter le nombre d'espèces.

En se basant sur les intervalles couverts par les espèces fossiles que l'on répertorie dans les sédiments bien datés, la durée de vie moyenne d'une espèce est de d'années environ. Certaines évoluent plus vite que, tels les mammifères et les oiseaux qui ont une durée de vie moyenne de l'ordre d'un million d'années, d'autres moins vite tels les bivalves qui atteignent environ d'années par espèce.

En classification classique ou phylogénétique, l’espèce est le taxon de base de la systématique, dont le rang se trouve juste en dessous du genre.

Dans la classification scientifique, une espèce vivante ou ayant vécu est désignée suivant les règles de la nomenclature binominale, établie par Carl von Linné au cours du . Suivant cette classification, le nom d'une espèce est constituée d'un binom latin (on dit habituellement binôme par erreur de traduction du terme anglais "binomen" et pas "binomial") qui combine le nom du genre avec une épithète spécifique. Autant que possible, le nom est suivi de la citation du nom de l'auteur, abrégé (en botanique) ou complet (en zoologie), qui a le premier décrit l'espèce sous ce nom. Le nom de l’espèce est l’ensemble du binom, et pas seulement l’épithète spécifique, suivi du nom d'auteur et de la date.

Par exemple, les êtres humains appartiennent au genre "Homo" et à l’espèce "Homo sapiens "Linné, 1758.

Les noms scientifiques sont « réputés » latins et s’écrivent en italique. Le genre prend une majuscule initiale tandis que l'épithète spécifique reste entièrement en minuscule.

Quand le genre est connu mais que l'espèce n'est pas déterminée, il est d’usage d’utiliser comme épithète provisoire l’abréviation du latin ' : « sp. », à la suite du nom du genre. Quand on veut désigner plusieurs espèces ou toutes les espèces d'un même genre, c'est l'abréviation « spp. » (pour ') qui est ajoutée. De même, « sous-espèce » est abrégé en « ssp. » (pour ') et « sspp. » au pluriel (pour '). Ces abréviations sont toujours écrites en caractères romains.

La nomenclature binominale, ainsi que d’autres aspects formels de la nomenclature biologique, constitue le « système linnéen ». Ce système de nomenclature permet de définir un nom unique pour chaque espèce, valable dans le monde entier, contrairement à la nomenclature vernaculaire.

Au sein d’une espèce donnée, une sous-espèce consiste en un groupe d’individus qui se trouvent isolés (pour des raisons géographiques, écologiques, anatomiques ou organoleptiques) et qui évoluent en dehors du courant génétique de la sous-espèce nominative, de référence.

Au bout d’un certain temps, ces groupes d’individus prennent des caractéristiques spécifiques qui les différencient l'une de l'autre. Ces caractères peuvent être nouveaux (apparition à la suite d'une mutation par exemple), mais dépendent de la fixation de caractéristiques variables chez l’espèce de base.

Ces deux bergeronnettes mâles ont été décrites comme deux sous-espèces différentes d’une même espèce, "Motacilla alba" :

Des sous-espèces différentes ont souvent la possibilité de se reproduire entre elles, car leurs différences ne sont pas (encore) suffisamment marquées pour constituer une barrière reproductive.

On peut s’interroger sur la validité de la définition d’une sous-espèce sachant que la définition du terme "espèce" reste fluctuante et controversée. Il en est ici de même et toutes les limites de la définition d’une espèce s’appliquent également pour celle d’une sous-espèce.

Carl von Linné recensait au siècle environ végétales et animales différentes dans la dixième édition (1758) du "". Depuis cette époque et jusqu'en 2014, près de d'espèces ont été décrites mais aujourd’hui, personne ne peut dire avec précision le nombre d’espèces existant sur la planète. Différentes estimations donnent un nombre total d'espèces variant entre à . Un consensus récent a proposé un nombre précis minimum de d’espèces (à l’exception des bactéries, trop difficiles à estimer).

Les eucaryotes sont les animaux, les champignons, les plantes, les protozoaires… Alors qu’on estime qu'entre d’espèces vivantes sur la planète Terre ont été découvertes (avec des extrapolations jusqu'à plus de d'espèces à découvrir), seulement d'espèces ont été décrites scientifiquement (témoin des difficultés liées à la notion d’espèce, ce nombre lui-même reste flou). Les espèces marines ne représentent que 13 % de l'ensemble des espèces décrites, soit environ , dont pour les seuls écosystèmes coralliens.

La grande majorité des espèces non décrites sont des insectes ( d'espèces suivant les estimations, qui vivraient principalement sur la canopée des forêts tropicales), des némathelminthes (ou vers ronds : ), et des eucaryotes unicellulaires : protozoaires ou protophytes, certains oomycètes, anciennement considérés comme des champignons, aujourd’hui classés dans les straménopiles ou les myxomycètes (moisissures visqueuses maintenant classées dans plusieurs groupes de protistes…).

Selon la liste rouge de l'UICN de 2006 et les données les plus récentes, les espèces vivantes "décrites" peuvent être réparties comme suit :


Environ espèces sont décrites chaque année, dont marines et près de de plantes à fleur ( répertoriées en 2015).

On estime qu’environ dix espèces disparaissent naturellement (c’est-à-dire hors de l’intervention de l’espèce humaine) chaque année, ou une sur par siècle. Mais il en est qui disparaissent aussi du fait de l’homme (voir dodo, diversité génétique…) : Edward Osborne Wilson en évalue le nombre à plusieurs milliers par an. D’après l’"Évaluation des écosystèmes pour le millénaire" de 2005, le taux de disparition des espèces depuis deux siècles est dix à cent fois supérieur au rythme naturel (hors grandes crises d'extinction), et sera encore multiplié par dix d'ici 2050, soit le rythme d'extinction naturel.

Dans les deux autres grands groupes du vivant (les archées et les bactéries), la notion d'espèce est sensiblement différente. Le nombre total est encore moins bien connu que chez les eucaryotes, avec des estimations qui varient entre et d'espèces… contre seulement de bactéries connues à l'heure actuelle.

Suivi ou précédé d'un adjectif, on écrit une espèce bovine, une espèce protégée Suivi d'un substantif, on écrit l'espèce Mulot sylvestre ou l'espèce "Apodemus sylvaticus".

« Une espèce de » est suivi d'un singulier ou d'un pluriel, selon que cette expression est prise dans le sens d'une approximation (sorte de) ou d'une population (groupe de). En français usuel, on écrit « Le bonobo est une espèce de singe » (une sorte de singe) mais un biologiste écrira de préférence « Le Bonobo est une espèce de primates » (un groupe de primates). En effet, en biologie, suivi d'un déterminant introduit par « de », on écrit une espèce (ou une sous-espèce) de mammifères, d'oiseaux, de reptiles ou bien des espèces d'insectes. 
Exemple : « "Solanum juzepczukii" est une espèce de plantes herbacées et tubéreuses de la famille Solanaceae » ou « la floraison de chaque espèce de plantes vivaces ».

On utilise les abréviations « sp. » au singulier et « spp. » au pluriel, qui correspondent au mot latin "". Cette abréviation s'emploie souvent après le nom d'un genre, pour indiquer « espèce non précisée », par exemple "Russula" sp. signifie « espèce du genre Russule ».



</doc>
<doc id="1073" url="https://fr.wikipedia.org/wiki?curid=1073" title="Placentalia">
Placentalia

Les placentaires (Placentalia) forment une division importante des mammifères et sont caractérisés par le fait qu'ils accouchent de juvéniles par contraste avec les marsupiaux qui accouchent de larves ou les monotrèmes qui pondent des œufs. Cela est rendu possible par la présence d'un placenta, plus développé et plus complexe que les marsupiaux, ce qui leur a donné leur nom.

Placentalia se définit comme un groupe-couronne ayant parfois le rang de cohorte ou d'infra-légion. La dénomination Eutheria est éventuellement utilisée en parlant des espèces actuelles, bien que ce clade soit en fait plus inclusif car comprenant non seulement les placentaires mais aussi tous les taxons fossiles qui en sont plus proches que des marsupiaux.

La caractéristique la plus populaire de ce groupe : le développement de la progéniture se passe en majeure partie au sein de l'utérus maternel grâce à un placenta permettant beaucoup plus d'échanges entre la mère et ladite progéniture. On distingue notamment un stade embryonnaire et un stade fœtal, au contraire des marsupiaux qui n'ont que le stade embryonnaire avant de naître à l'état larvaire. L'examen du pelvis d'Eomaia semble indiquer que les euthériens non-placentaires naissaient précocement, au stade larvaire comme des marsupiaux.

Parmi les autres caractéristiques qui singularisent les placentaires des non-placentaires au sein des euthériens (y compris fossiles) :


Les mammifères placentaires sont ancrés dans la branche des euthériens, parallèlement aux marsupiaux dans celle des métathériens.
La datation de l'ancêtre commun des placentaires est sujette à controverse. Les études de phylogénie moléculaire font remonter les grands groupes modernes de placentaires et leur dernier ancêtre commun au milieu du Crétacé (entre 90 et 105 millions d'années), mais cette datation est remise en cause, notamment par une étude combinant traits génétiques et morphologiques, et la date d'apparition des mammifères placentaires pourrait être de 65 millions d'années, soit entre et ans après l'extinction Crétacé-Tertiaire des dinosaures non-aviens suggérant ainsi le scénario d'une explosion radiative.
La plupart des études s'accordent à reconnaître quatre super-ordres au sein des mammifères placentaires : Xenarthra, Afrotheria, Euarchontoglires et Laurasiatheria, les deux derniers regroupés dans le clade Boreoeutheria. Les relations entre ces autres groupes font débat et leur phylogénie n'est toujours pas résolue :



</doc>
<doc id="1074" url="https://fr.wikipedia.org/wiki?curid=1074" title="Edgar Allan Poe">
Edgar Allan Poe

Edgar Allan Poe, né le à Boston et mort à Baltimore le , est un poète, romancier, nouvelliste, critique littéraire, dramaturge et éditeur américain, ainsi que l'une des principales figures du romantisme américain. Connu surtout pour ses contes il a donné à la nouvelle ses lettres de noblesse et est considéré comme l’inventeur du roman policier. Nombre de ses récits préfigurent les genres de la science-fiction et du fantastique. 

Né à Boston, Edgar Allan Poe perd ses parents, David Poe Jr. et Elizabeth Arnold, dans sa petite enfance ; il est recueilli par John et Frances Allan de Richmond, en Virginie, où il passe l’essentiel de ses jeunes années, si l’on excepte un séjour en Angleterre et en Écosse, dans une aisance relative. Après un bref passage à l’Université de Virginie et des tentatives de carrière militaire, Poe quitte les Allan. Sa carrière littéraire débute humblement par la publication anonyme, en 1827, de "Tamerlan et autres poèmes", un recueil de poèmes signés seulement « par un Bostonien ». Poe s’installe à Baltimore, où il vit auprès de sa famille paternelle et abandonne quelque peu la poésie pour la prose. En , il devient rédacteur-assistant au "Southern Literary Messenger" de Richmond, où il contribue à augmenter les abonnements et commence à développer son propre style de critique littéraire. La même année, à vingt-six ans, il épouse sa cousine germaine Virginia Clemm, alors âgée de .

Après l’échec de son roman "Les Aventures d'Arthur Gordon Pym", Poe réalise son premier recueil d’histoires, les "Contes du Grotesque et de l’Arabesque", en 1839. La même année, il devient rédacteur au "Burton's Gentleman's Magazine", puis au "Graham's Magazine" à Philadelphie. C'est à Philadelphie que nombre de ses œuvres parmi les plus connues ont été publiées. Dans cette ville, Poe a également projeté la création de son propre journal, "The Penn" (plus tard rebaptisé "The Stylus"), qui ne verra jamais le jour. En , il déménage à New York, où il travaille au "Broadway Journal", un magazine dont il devient finalement l’unique propriétaire.

En , Poe publie "Le Corbeau", qui connaît un succès immédiat. Mais, deux ans plus tard, son épouse Virginia meurt de tuberculose, le . Poe envisage de se remarier, mais aucun projet ne se réalisera. Le , Poe meurt à l’âge de à Baltimore. Les causes de sa mort n’ont pas pu être déterminées et ont été attribuées diversement à l’alcool, à une drogue, au choléra, à la rage, à une maladie du cœur, à une congestion cérébrale, etc.

L'influence de Poe a été et demeure importante, aux États-Unis comme dans l'ensemble du monde, non seulement sur la littérature, mais également sur d'autres domaines artistiques tels le cinéma et la musique, ou encore dans des domaines scientifiques. Bien qu'auteur américain, il a d’abord été reconnu et défendu par des auteurs français, Baudelaire et Mallarmé en tête. La critique contemporaine le situe parmi les plus remarquables écrivains de la littérature américaine du .

Il naît le dans une modeste pension de famille du 62, Carver Street, à Boston, dans le Massachusetts. Sa mère, Elizabeth Arnold (1787-1811) est la fille de deux acteurs londoniens, Henry (ou William Henry) Arnold et Elizabeth Smith. À la mort de son père, en 1796, elle suit sa mère en Amérique. Arrivée le 3 janvier à Boston à bord de l’"Oustram", elle monte sur les planches trois mois plus tard, âgée d'à peine neuf ans. Elle rejoint ensuite avec sa mère, qui meurt quelque temps après, une petite troupe de théâtre, les "Charleston Players"<ref name="eapoe.org/geninfo/poechron"> .</ref>.

Durant l'été 1802, à Alexandria, en Virginie, elle se marie avec le comédien Charles Hopkins, qui meurt trois ans plus tard, le . À 18 ans, déjà veuve, elle épouse un garçon tuberculeux et alcoolique de 21 ans, David Poe Jr., dont le père, le général David Poe Sr., un commerçant patriote de Baltimore originaire d'Irlande, s'était illustré durant la guerre d'indépendance. David Poe Jr. avait abandonné ses études de droit pour s'engager, en , dans les "Charleston Players". C'est là qu'il a rencontré Elizabeth Arnold Hopkins, qu'il épouse le . À l'époque, ils jouent au Federal Street Theater de Boston. Elizabeth est danseuse et chanteuse, mais David est alcoolique, tuberculeux et piètre acteur.

Edgar est le deuxième des trois enfants du couple. Son frère, William Henry Léonard, né le , mourra le , à l'âge de 24 ans, alcoolique et tuberculeux, tandis que sa sœur, Rosalie, née le , contractera à douze ans une maladie inconnue, peut-être une méningite, qui la laissera handicapée mentale et nécessitera une mise sous tutelle durant toute sa vie.

En , la famille quitte Boston pour le New York Park Theater. Le 18 octobre, David Poe, qui a sombré dans l'alcoolisme, joue son dernier rôle ; il fugue quelques mois plus tard, en . Il meurt sans doute peu après, en . La même année, Elizabeth donne naissance à une fille, Rosalie. Elle fait une tournée dans le Sud, accompagnée d'Edgar (William Henry a été confié à son grand-père paternel). Mais malade, elle ne joue que par intermittence.

Le , à Richmond (Virginie), malade, elle doit s'aliter. Le 25 novembre, un journal local lance un appel à la générosité des citoyens de Richmond, sous le titre « Au cœur humain » : « Mrs Poe, allongée sur son lit de douleur et entourée de ses enfants, demande votre aide et la demande peut-être pour la dernière fois ! ». Le , Elizabeth est emportée par le mal qui la ronge, peut-être la pneumonie, à l'âge de 24 ans, après avoir joué près de deux cents rôles, laissant ses enfants orphelins. Deux semaines après ses obsèques, le théâtre de Richmond brûle pendant une représentation, et la troupe, privée de théâtre, quitte la ville après avoir laissé Edgar et Rosalie à la charité de la bourgeoisie de la ville.

Tandis que William Henry demeure avec son grand-père David Poe et sa tante Maria Clemm, Edgar est recueilli par un couple de riches négociants de tabac et de denrées coloniales de Richmond, John et Frances Allan, et Rosalie (1810-1874) par les Mackenzie. Le , Edgar est baptisé par le révérend John Buchanan, vraisemblablement sous le nom d'« Edgar Allan Poe » et avec les Allan pour parrain et marraine.

Edgar passe son enfance à Richmond, chez ses parents adoptifs, qui l'élèvent avec tendresse. En 1814, à peine âgé de 5 ans, il commence ses études primaires sous la conduite de Clotilda ou Elizabeth Fisher. L'année suivante, il passe brièvement, à l'école de William Ewing. En 1815, en effet, John Allan (1780-1834), qui est d'origine écossaise, décide de partir au Royaume-Uni pour y étudier le marché et, si possible, ouvrir à Londres une succursale. La Bible occupe une grande place dans la vie d'Edgar, et ce malgré le rationaliste John Allan. Edgar, qui a six ans, quitte l'école de Richmond et embarque avec ses parents et la jeune sœur de Allan, Ann Moore Valentine (appelée Nancy) à Norfolk (Virginie) à bord du "Lothair".

Débarqués à Liverpool le 29 juillet, les Allan gagnent d'abord l'Écosse. Mais le marché écossais se révèle mauvais, et la famille s'installe bientôt à Londres. Edgar suit, de 1816 à 1818, des études primaires à l'école des demoiselles Dubourg (146 Sloan Street, Chelsea, Londres), où il est connu sous le nom de « "Master Allan" » et étudie notamment la géographie, l'orthographe et le catéchisme anglican, puis à la Manor House School de Londres, à Stoke Newington, dirigée par le révérend John Bransby (elle pourrait avoir servi de modèle au collège de "William Wilson"), sous le nom d'« Edgar Allan ». Il suit des études classiques et littéraires solides, apprenant le grec, le latin, le français et la danse. Il fait preuve d'un caractère irritable et parfois tyrannique envers ses camarades, mais obtient de brillants résultats scolaires, en latin et français notamment. L'école mettant également l'accent sur la condition physique des élèves, Edgar devient un athlète accompli. En août 1818, les Allan visitent l'île de Wight, probablement à l'occasion de vacances, et peut-être le site de Stonehenge. Mais la situation se dégrade. D'abord, sa mère adoptive, dont la santé a toujours été fragile, tombe sérieusement malade, ce qui a pour effet de la rendre nerveuse, irritable. Par ailleurs, en 1819, John Allan connaît de graves ennuis financiers : la bourse de tabac s'effondre, puis un employé l'escroque. Le jeune Edgar, qui est séparé de sa famille, fait une première fugue.

Le , la famille Allan est à Liverpool, où elle embarque sur le "Martha". Arrivée à New York le 22 juillet après 31 jours de trajet, elle prend le 28 un steamboat à destination de Norfolk et se réinstalle à Richmond, le 2 août. Edgar reprend le chemin de l'école, où il obtient, là aussi, d'excellents résultats, mais commence à manifester un certain penchant pour la solitude et la rêverie. En 1823, les affaires de John Allan sont moribondes et la vie à la maison des Allan s'en ressent. Edgar continue à rédiger des poèmes qu'il adresse aux élèves de l'école où se trouve sa sœur.

Les relations avec ses parents adoptifs sont ambivalentes. Il est encouragé par sa mère dans ses travaux d'écritures, mais les tours qu'il joue à certains habitants de Richmond causent le désespoir de son père. Ce dernier prend ombrage du caractère assez fier de l'adolescent, et s'éloigne progressivement de son épouse, toujours malade. Edgar, très attaché à Frances Allan (1784-1829), réprouve l'adultère de son père adoptif. John Allan voudrait voir Edgar devenir marchand, mais le jeune homme ne rêve que de poésie et envisage, à la rigueur, une carrière dans l'armée. Il trouve souvent refuge chez la mère d'un camarade, Jane Stith Stanard, qui est l'inspiratrice du poème "À Hélène" (1831). Son décès, en 1824, affectera grandement Edgar.

À la suite du décès de son oncle William Galt, en , John Allan hérite de plusieurs centaines de milliers de dollars. Cette somme lui permet de payer ses dettes et d'acheter un manoir en briques appelé « Moldavia » (pour dollars). Entre 1821 et 1825, Edgar fréquente les meilleures écoles privées de Richmond, où il reçoit l'éducation traditionnelle des gentlemen virginiens. Il est inscrit à l'English Classical School de John H. Clarke (1821-1822), qui lui fait lire Ovide, Virgile et César, puis Homère, Horace et le "De Officiis" de Cicéron, puis il fréquente le collège William Burke (1823-) et l'école du Ray Thomas et de son épouse.

À cette époque, il écrit ses premiers vers satiriques, tous perdus aujourd'hui, excepté "O Tempora! O Mores!" Par ailleurs, il est très influencé par l'œuvre et le personnage de Lord Byron. Bon élève, il se montre excellent nageur et passionné de saut en longueur. En juin ou , il nage six ou sept miles le long de la James River, tandis que son maître suit sur un bateau. Du 26 au , lors de son voyage aux États-Unis, le général La Fayette visite Richmond. Les volontaires juniors de la ville participent aux cérémonies organisées pour lui souhaiter la bienvenue ; Edgar est lieutenant des volontaires.

Le , il entre à la nouvelle université de Virginie, à Charlottesville, que vient de fonder Jefferson (elle a ouvert ses portes le ), où il suit avec brio des cours de langues ancienne et moderne. Mais M. Allan lui a donné juste assez d'argent pour s'inscrire. Excédé par les dettes de jeu et les frais courants d'Edgar, qui s'élèvent à dollars, alors qu'il vient de passer avec succès ses premiers examens, John Allan refuse de le réinscrire et le ramène à Richmond en pour l'employer dans sa maison de commerce. Par ailleurs, il ruine ses fiançailles avec Elmira Royster (1810-1888) ; le père de la jeune fille s'empresse de la marier à un riche négociant, Alexander Shelton.

Comme son beau-père refuse de le renvoyer à l'université, il quitte sa famille adoptive, probablement le , et s'embarque sous le nom d'Henri Le Rennet sur un bateau qui descend la James River jusqu'à Norfolk. Arrivé à Boston en avril, il espère survivre en publiant ses poèmes. Il y passe deux mois, comme acteur ou soldat, on l'ignore. Le 26 mai, sous le nom d'Edgar A. Perry (pseudonyme qu'il réutilisera pour signer certains contes), après s'être vieilli de quatre ans, il s'engage pour cinq ans comme artilleur de seconde classe dans l'armée fédérale. À la même époque, il fait paraître à ses frais, chez Calvin F.S. Thomas à Boston, une mince plaquette anonyme "Tamerlan et autres poèmes" sur laquelle est inscrit « A Bostonian » et dont 50 exemplaires à peine sont vendus. Il n'en existe aujourd'hui que 12 exemplaires.

En novembre, sa batterie est transférée à Fort Moultrie, sur l', face à Charleston (cette île servira de décor au très populaire "Scarabée d'or"). Malgré sa rapide promotion au grade d'artificier, puis de sergent-major (le 1829) et l'amitié de ses supérieurs, Edgar s'ennuie. John Allan lui refuse la lettre d'autorisation sans laquelle il ne peut démissionner. Le , la batterie d'artillerie où il sert est transférée au Fort Monroe en Virginie.

Le , Frances Keeling Allan meurt. Elle est inhumée le 2 mars au cimetière de Shockoe Hill. Prévenu tardivement, Edgar n'arrive que le soir du jour des funérailles de cette mère tant aimée. Durant ce séjour, Edgar se réconcilie provisoirement avec son père adoptif, qui accepte de l'aider à démissionner de l'armée et d'appuyer (sèchement) sa candidature à West Point, école des officiers de l'armée américaine. Le 4 avril, Edgar est libéré de l'armée.

Une nouvelle histoire de dettes entraîne une nouvelle brouille entre les deux hommes. Libéré de l'armée en , sans le sou, Edgar va attendre son admission à West Point à Baltimore. Il séjourne auprès de sa tante Maria Clemm (1790-1871), sœur cadette de son père, qui a perdu son mari en 1826 et vit dans un extrême dénuement, entourée de sa mère impotente, Elizabeth Cairnes Poe, d'un fils tuberculeux, Henry (1818-après 1836), et de deux filles, Elizabeth Rebecca (1815-1889) et Virginia (1822-1847), qui est éperdue d'admiration devant son cousin, ainsi que du frère d'Edgar, William Henry. Dans cette ville, il fait paraître un second recueil de poèmes, "Al Aaraaf, Tamerlan et poèmes mineurs" chez Hatch and Dunning en .

Muni de chaleureuses lettres de recommandation de ses anciens officiers et d'une froide supplique de John Allan, il se rend à pied à Washington, pour solliciter son admission dans la prestigieuse académie de John Eaton, Secrétaire à la guerre. Ses démarches n'ayant obtenu aucun succès, il retourne à Baltimore.

Edgar est admis à West Point en . Il y fait de brillantes études, meilleures dans les disciplines académiques que dans les exercices militaires. John Allan, cependant, se remarie avec Louisa Patterson, qui lui donnera trois fils. Excédé par l'avarice de John Allan, qui lui refuse à nouveau l'argent nécessaire à ses études, et réfractaire à la discipline, Edgar se fait volontairement renvoyer de West Point (en refusant de se rendre en classe ou à l'église) après jugement de la cour martiale, le . Le 6 mars, il quitte l'école avec des lettres de recommandation de ses supérieurs.

De retour à Baltimore, chez Maria Clemm, il recherche vainement un emploi. Ses articles et ses contes sont tous refusés. Enfin, il envoie cinq nouvelles au concours du "Philadelphia Saturday Courrier", qui promet au gagnant un prix de . Il n'obtient pas le prix, mais ses contes (notamment "Metzengerstein") sont publiés, sans son nom, en 1832 par le "Saturday Courrier" (qui les paie très mal).

Ainsi commence sa carrière de journaliste. Dans l'indigence, il pratique aussi le métier de pigiste nègre et continue son travail d'écrivain, consacrant ses loisirs et ses maigres revenus à l'éducation de sa petite cousine Virginia. En 1831, il fait paraître chez Elam Bliss à New York "Poèmes, seconde édition", dédié au « corps des cadets des États-Unis » et précédé du premier manifeste critique d'Edgar, la "Lettre à M…" (reprise par la suite sous le titre "Lettre à B…"), qui bénéficie d'un accueil peu favorable.

En 1833, le "New England" refuse de publier son premier recueil : "Contes du club de l'In-Folio". En revanche, en octobre, il gagne le du concours du "Baltimore Saturday Visiter" avec le "Manuscrit trouvé dans une bouteille", qui lui apporte une certaine notoriété et l'amitié de John P. Kennedy, membre du jury et célèbre romancier. Grâce à ses recommandations, il peut publier ses premiers comptes rendus de critique littéraire au "Southern Literary Messenger".

En , il est enfin engagé par Thomas W. White comme directeur de la section littéraire du journal. Toutefois, il n'est pas libre : il doit se conformer au programme de la revue, qui soutient la littérature sudiste, et satisfaire l'admiration infantile de T. W. White pour les discours des gentlemen virginiens. La griffe d'Edgar apparaît dans ses nombreux pamphlets contre les romanciers populaires (du Nord) de l'époque. Il s'attaque notamment au best-seller de Theodore Fay, "Norman Leslie", coqueluche de New York et des journaux nordistes tels le "Knickerbocker", le "Commercial Intelligencer" ou la "North American Review". Son talent de polémiste éclate, et il rénove l'esprit du "Southern". Ses opérations médiatiques, comme la série : « Autobiographies pastiches de lettres d'écrivains », font monter le nombre d'abonnés au journal.

Il épouse clandestinement Virginia le . Le , il l'épouse publiquement, et la jeune fille, qui n'a que 13 ans, le rejoint à Richmond avec sa mère.

Toutefois, il s'estime, à juste titre, mal payé et ne supporte plus les reproches (sur son supposé alcoolisme, notamment) dont l'accable, en public, T. W. White, pour empêcher son brillant rédacteur de prendre trop d'ascendant et garder le contrôle de son journal. Aussi décide-t-il de quitter le "Southern".

En , il s'installe à New York, où la "New York Review" lui a fait une proposition. Mais le journal a cessé de paraître quand il arrive. Mrs Clemm ouvre une pension à Manhattan, où Edgar s'installe avec Virginia. Il y achève "Les Aventures d'Arthur Gordon Pym" et y révise "Les Contes de l'In-Folio".

En 1838, il se fixe à Philadelphie pour reprendre ses activités régulières de journaliste appointé. Il tente d'y vivre de sa plume, mais ses quelques piges ne le sortent pas de la misère. La même année paraissent "Les Aventures d'Arthur Gordon Pym", qui n'ont aucun succès.

En , William Burton offre à Edgar la place de rédacteur en chef adjoint au "Burton's Gentleman's Magazine". Il y est encore moins libre qu'au "Southern", car il doit servir l'opportunisme de Burton, qui lui a recommandé de faire preuve d'indulgence dans ses comptes rendus critiques. Toutefois, il s'entend bien avec Burton, et leur collaboration permet au "Gent's Mag", qui publie "La Chute de la maison Usher", "Le Diable dans le beffroi" et "William Wilson", de devenir le mensuel le plus en vue de Philadelphie. En revanche, la publication en volume des "Contes du grotesque et de l'arabesque", en 1840, n'obtient qu'un succès d'estime. La même année, Edgar se livre à une critique de Longfellow, auquel il reproche le manque d'unité de ses textes, et inaugure une série de dénonciations de plagiats.

En , il entreprend la publication en livraisons successives d'un roman de l'Ouest, "Le Journal de Julius Rodman", médiocre fiction restée inachevée et pleine d'emprunts aux journaux de voyage contemporains. En juin, il quitte Burton pour fonder le "Pen Magazine", revue littéraire dont il serait le seul maître. Il fait circuler des tracts aux plus grandes célébrités littéraires américaines, mais le projet échoue lorsque le commanditaire, George Graham, se retire. En octobre, Graham, qui possède le "Saturday Evening Post" et le mensuel "Casket" achète pour dollars le "Burton's Gentleman's Magazine" (qui compte alors abonnés) et le rebaptise "Graham's Gentleman's Magazine". Dans le premier numéro paraît le conte "L'homme des foules".

En , Edgar est engagé comme rédacteur associé par son ami George Graham. Il touche un salaire annuel de . Pour la première fois, il jouit d'une réelle indépendance. La plupart de ses grands articles et l'essentiel de son œuvre critique ont paru dans les pages du "Graham's Magazine". C'est également la période la plus heureuse de sa vie. Il poursuit ses attaques contre les « cliques » et les « coteries » de New York et de Boston, qui dictent leur loi aux éditeurs et aux journalistes des grands centres urbains. Le tirage de la revue passe à , chiffre exceptionnel pour l'époque.

Un malheur vient cependant frapper sa famille. Un soir de , alors qu'elle chante pour des amis, Virginia est victime d'une hémorragie causée par la rupture d'un vaisseau de la gorge. Elle reste plusieurs mois entre la vie et la mort.

Peu après, le 6 mars, Edgar rencontre Charles Dickens, en tournée aux États-Unis, avec lequel il discute de l'instauration d'un copyright international. Dickens lui promet de lui trouver un éditeur en Angleterre. En mai, Edgar quitte le "Graham's Magazine", repris par le projet de fonder sa propre revue, baptisée cette fois "The Stylus".

En , il se porte candidat à un poste de l'administration qui lui laisserait le temps d'écrire, grâce aux contacts de son ami F. W. Thomas. Toutefois, malgré le soutien de Robert Tyler, le fils du président des États-Unis, il ne peut obtenir aucun poste. Pendant la campagne présidentielle de 1840, il avait rédigé plusieurs pamphlets politiques opportunistes contre le candidat démocrate Martin Van Buren ("Le Diable dans le beffroi") et son colistier Richard Mentor Johnson ("L'Homme qui était refait"), pour obtenir les bonnes grâces du parti whig. De retour à Philadelphie le 13 mars, il vit à nouveau de maigres piges.

En 1844, Edgar s'installe dans le nord de Manhattan, à la ferme Brennan, où il travaille avec acharnement à une "Histoire critique de la littérature américaine" qui ne verra jamais le jour. Par ailleurs, il écrit des "Marginalia", brèves notes journalistiques souvent tirées de ses articles antérieurs. Enfin, il accepte un emploi subalterne au "New York Mirror" de son ami Nathaniel Parker Willis et remet à plus tard son projet du "Stylus".

Le , il publie "Le Corbeau", qui a un succès extraordinaire. Paru dans l"'Evening Mirror", le poème est repris dans de nombreux journaux. Sa renommée grandit. Une sélection de ses contes paraît chez les prestigieux éditeurs Wiley et Putnam à New York, puis un recueil de poèmes, "Le Corbeau et autres poèmes" en .

Plusieurs de ses comptes rendus critiques sont publiés dans le "Broadway Journal" de Charles Frederick Briggs et John Brisco, hebdomadaire d'information artistique et culturelle. Le , il devient collaborateur permanent du journal et lance une campagne célèbre à New York sous le nom de « Guerre Longfellow » : Edgar et « Outis », un correspondant anonyme (Edgar lui-même selon certaines hypothèses), échangent de violentes diatribes, l'une ridiculisant Longfellow, l'autre accusant "Le Corbeau" de plagiat. En juillet, Edgar parvient à éliminer Briggs, l'un des deux actionnaires du journal. En octobre, Brisco cède ses parts à Edgar, qui concrétise alors son rêve, en devenant l'unique propriétaire de l'hebdomadaire. Toutefois, il s'aliène les journalistes et le public bostonien lors d'une conférence, volontairement obscure, sur son poème "Al Aaraaf". Le , Edgar dépose le bilan du "Broadway Journal" pour cause de dettes.

En mai, Virginia étant de plus en plus malade, la famille s'installe à Fordham, quartier du Bronx, dans la grande banlieue de New York. Il apprécie les jésuites de l'université de Fordham et flâne fréquemment dans son campus, conversant avec les étudiants et les professeurs. La tour du clocher de l'université de Fordham lui inspire le poème « Bells. » À cette époque, Edgar tombe gravement malade et, ne pouvant plus écrire, sombre dans la misère. Le foyer est soutenu par une amie, Marie Louis Shew, mais leur pauvreté est telle qu'un entrefilet dans le "New York Express" du 5 décembre appelle les amis du poète à l'aide.

Le , Virginia décède à Fordham, à l'âge de 24 ans. Edgar, gravement malade, est soigné par Mrs Shew et Maria Clemm. À cette époque, il est très occupé par son projet de poème en prose, "Eureka ou Essai sur l'univers matériel et spirituel". Il s'engage dans une quête frénétique d'amitiés féminines avec Mrs Lewis, dont il corrige les poèmes sentimentaux contre rétribution, avec Mrs Nancy Locke-Richmond (qui habite à Lowell, dans le Massachusetts), dont il s'éprend et qui sera l'Annie des derniers poèmes, enfin, avec Mrs Sarah Whitman (qui vit à Providence, dans le Rhode Island), poétesse spiritualiste à qui il adresse le second poème "À Hélène" et qu'il demande en mariage. En , dans des circonstances assez obscures, il absorbe une forte dose de laudanum qui manque de l'empoisonner. De plus, il s'est mis à boire, lors de la maladie de Virginia, entre 1842 et 1847, et il est victime de crises d'éthylisme. Il souffre même un moment d'une attaque de paralysie faciale.

Le 13 novembre, Mrs Whitman accepte de l'épouser s'il renonce à l'alcool. Le 23 décembre, à Providence, il donne devant deux mille personnes sa célèbre conférence sur "" (qui ne sera publiée qu'après sa mort). Deux jours plus tard, 25 décembre, doivent être célébrées les noces avec Mrs Whitman. Toutefois, le lendemain, celle-ci reçoit une lettre anonyme lui apprenant de prétendues « relations immorales » entre Edgar et une de ses amies. De plus, on lui apprend que son fiancé a passé la nuit à boire avec des jeunes gens dans une taverne de la ville. Aussitôt, elle décide de rompre avec lui.

De retour à Fordham, Edgar reprend son projet de revue littéraire avec E.H.N. Patterson. Après une visite à Mrs Richmond, il entreprend un voyage dans le Sud pour rassembler des fonds en faveur de sa revue. Parti de New York le , il séjourne tout l'été à Richmond, où il retrouve Elmira Royster Shelton, veuve depuis la mort de son mari en 1844, avec laquelle il songe à se marier, et redonne sa conférence sur "Le Principe poétique", qui rencontre un très grand succès. Il la refait également à Norfolk (Virginie).

Le 27 septembre, Edgar quitte Richmond en bateau pour Baltimore, où il débarque le lendemain. On perd alors sa trace pendant quatre jours.

Le , Joseph W. Walker envoie un message au James E. Snodgrass : « Cher Monsieur, — Il y a un monsieur, plutôt dans un mauvais état, au bureau de scrutins de Ryan, qui répond au nom d'Edgar A. Poe, et qui paraît dans une grande détresse et qui dit être connu de vous, et je vous assure qu'il a besoin de votre aide immédiate. Vôtre, en toute hâte, Jos. W. Walker. » L'endroit où Edgar réapparaît, plus connu sous le nom de « Gunner's Hall », était une taverne, qui (comme souvent à l'époque) servait de lieu de vote pendant les élections.

Le Snodgrass et Henry Herring, l'oncle d'Edgar, viennent chercher l'écrivain, qu'ils présument ivre. D'après les différents témoignages, au lieu de son costume de laine noir, il portait un manteau et un pantalon d'alpaga de coupe médiocre, vieillis et salis, et dont les coutures avaient lâché en plusieurs points, ainsi qu'une paire de chaussures usées aux talons et un vieux chapeau tout déchiré, presque en lambeaux, en feuilles de palmier. La chemise était toute chiffonnée et souillée, et il n'avait ni gilet ni faux-col.

Conduit au Washington College Hospital, il alterne entre des phases de conscience et d'inconscience. Aux questions qu'on lui pose, il répond par des phrases incohérentes. Son cousin, Neilson Poe, venu lui rendre visite, ne peut le voir. Edgar meurt, officiellement d'une « congestion cérébrale », le dimanche 7 octobre, à 3 h ou 5 h du matin. Il est inhumé dans le cimetière presbytérien de la ville, le Westminster Hall, maintenant intégré à l'école de droit de l'université du Maryland.

Plusieurs théories ont été émises pour expliquer la mort d'Edgar. On a prétendu, ainsi, qu'il serait mort des suites d'une trop grande consommation d'alcool. D'autres mettent en avant des ennuis de santé. En 1847, il avait été victime d'une longue maladie qui lui aurait causé une lésion au cerveau. De même, en 1848, le John W. Francis aurait diagnostiqué une maladie du cœur, diagnostic qu'Edgar Poe aurait d'ailleurs rejeté. Enfin, dans ses lettres à Maria Clemm, les 7 et 14 juillet, il indique qu'il est malade, parlant d'une amélioration de son état le 19. Parmi les maladies qui auraient pu causer sa mort, on a parlé de la tuberculose, de l'épilepsie, du diabète ou de la rage.

Autre hypothèse mise en avant : il aurait retrouvé des anciens de West Point, qui l'auraient invité à boire. Rentrant seul, dans un état d'ivresse, il aurait été volé et battu par des brutes et aurait erré dans les rues pendant la nuit avant de sombrer, inconscient.

Cependant, la théorie la plus largement admise est qu'il aurait été victime de la corruption et de la violence, qui sévissaient de manière notoire lors des élections. De fait, la ville était alors en pleine campagne électorale (pour la désignation du shérif, le ) et des agents des deux camps parcouraient les rues, d’un bureau de vote à l’autre, pour faire boire aux naïfs un cocktail d’alcool et de narcotiques afin de les traîner ainsi abasourdis au bureau de vote. Pour parfaire le stratagème, on changeait la tenue de la victime, qui pouvait être battue. Le faible cœur d'Edgar Poe n'aurait pas résisté à un tel traitement.

Poe est enterré lors d'une cérémonie réduite à sa plus simple expression et placé dans une tombe non marquée qui progressivement sera recouverte d'herbes.

En 1860, sa famille se mobilise pour offrir une pierre tombale de marbre blanc au poète négligé de Baltimore, portant l'épitaphe : « "Hic Tandem Felicis Conduntur Reliquae Edgar Allan Poe, Obiit Oct. VII 1849" » et sur l'autre face l'inscription : « "Jam parce sepulto" », mais la pierre est détruite accidentellement avant même sa mise en place.

Grâce à une souscription initiée en 1865 et relayée par les élèves de l'université du Maryland, Poe est réinhumé le sur un nouvel emplacement, et une véritable cérémonie est organisée sur sa nouvelle tombe le 17 novembre qui mentionne cette fois une date de naissance erronée (20 janvier au lieu du 19). Le nouveau monument n'a aucune épitaphe, même si plusieurs suggestions ont été faites, en particulier par Oliver Wendell Holmes. La pierre tombale mentionne seulement les noms et les dates de ses occupants. En 1885, les restes de Virginia Poe, enterrés en 1847 à New York, ont été apportés à Baltimore et inhumés avec ceux de Poe et de Maria Clemm, désormais réunis. Ce monument sera dégradé par le temps, remplacé par un monument en bronze, lui-même volé et remplacé.

Ce n'est finalement qu'en 1913 qu'une autre pierre commémorative est repositionnée, d'abord au mauvais endroit, puis finalement à l'emplacement originel de la tombe d'Edgar Poe, dans le cimetière presbytérien de Baltimore, avec l'épitaphe suivante, tirée du poème Le Corbeau : "« Quoth the Raven, "Nevermore." »" ("Le corbeau dit : « Jamais plus ! »").

Depuis 1949, les admirateurs de Poe se réunissent chaque année sur sa tombe, à l'anniversaire de sa naissance, le 19 janvier.

À l'occasion du bicentenaire de sa naissance, des funérailles solennelles, présidées par John Astin, ont été organisées par le Poe House and Museum de Baltimore le , son enterrement n'ayant pas été annoncé publiquement en 1849 et l'assistance autour de son cercueil s'étant alors résumée à dix personnes.

Chaque 19 janvier de 1949 à 2009, une mystérieuse personne a déposé sur sa tombe trois roses et une bouteille de cognac. L'identité du ("Poe Toaster") est toujours inconnue.

Doté d'une vaste intelligence, Edgar Allan Poe était un homme très courtois mais d'une férocité sans égale, qui le brouilla avec de nombreuses personnes. Ses amis étaient toujours frappés par sa tenue soignée à l'excès et la clarté de son élocution. De même, ses manuscrits se distinguent par la fermeté, la régularité et l'élégance de son écriture et ne comportent que peu de ratures. Très souvent, il écrivait sur des feuilles de bloc-notes qu'il collait les unes aux autres de manière à former des rouleaux très stricts. Une analyse graphologique de ces manuscrits a été réalisée, qui révélerait une intelligence , une indépendance extrême à l'égard des conventions, et qui contrôle, ou cherche toujours à contrôler, une extraordinaire sensibilité ; somme toute, un « cérébral ».

Dans son travail, il se méfiait du premier jet, du spontané. Pressé par le besoin d'argent, il livrait le plus souvent des contes non revus aux journaux ou revues auxquels ils étaient destinés. Toutefois, lors des republications, il apportait à ceux-ci d'importants changements, toujours dans le sens d'un meilleur resserrement du texte. Durant les derniers mois de son existence, il révisa de près ses fictions et ses écrits théoriques ou critiques en vue de la première grande édition de ses œuvres, qui parut à New York en 1850.

Très conscient de son intelligence, logicien, il aimait faire montre de ses capacités analytiques. Ainsi, lors de la publication en feuilleton de "Barnabé Rudge" (1841), roman de Dickens, il aurait deviné la fin de l'intrigue avant la parution des dernières livraisons. De même, "Le Mystère de Marie Roget" est inspiré d'un fait réel, l'assassinat de Mary Cecil Rogers à New York en 1841, dont le corps avait été retrouvé dans l'Hudson, près de la rive du New Jersey. Dans une lettre datée du , il explique que, dans son conte, en faisant faire à Dupin , il démontre et a , expliquant que la jeune femme n'a pas été assassinée, comme on le pensait, par une bande de voyous.

Sa supériorité dans l'art d'écrire fut aussi marquée par quelques canulars, où il appliqua sa théorie de l'effet. Le , il fit paraître dans un numéro spécial du "New York Sun" un conte, "Le Canard au ballon", présenté comme un fait réel. Par cette adroite mystification, il marquait son retour sur la scène littéraire new-yorkaise. Quant à "La Vérité sur le cas de M. Valdemar", conte paru en 1845, l'éditeur, qui le publia comme un pamphlet, et les journaux qui le reprirent dans les éditions anglaises le présentèrent comme un rapport scientifique (parce qu'ils avaient été dupés). Elizabeth Barrett Browning lui écrivit pour louer « la puissance de l'écrivain et cette faculté qu'il a de transformer d'improbables horreurs en choses qui paraissent si proches et si familières ».

Idéaliste, il était aussi très ambitieux, ce qu'il ne cachait pas. Il confia un jour à John Henry Ingram : .

Dès l'enfance, il lisait Byron, dont l'influence devait marquer ses premiers poèmes, Coleridge et la plupart des romantiques de son époque. Par la suite, il devait se démarquer de ces auteurs et se signala par des critiques assez féroces contre Coleridge. Il connaissait aussi parfaitement la littérature classique et goûtait particulièrement Pope. Il professa une grande admiration pour "Ondine", conte de Friedrich de La Motte-Fouqué, pour Shelley, pour le génie de Dickens (notamment pour "Le magasin d'antiquités"), pour Hawthorne. En revanche, il exprimait de sévères critiques à l'égard de Carlyle, d'Emerson (qu'il considérait comme la du premier), de Montaigne, dont l'emploi de la digression dans ses "Essais" était en contradiction avec ses idées sur la nécessaire unité d'un texte. De même, s'il pouvait dire de John Neal que « son art est grand, il est d'une nature élevée », il mettait en avant ses « échecs répétés […] dans le domaine de la "construction" de ses œuvres », due, selon lui, soit à une « déficience du sens de la totalité », soit à une « instabilité de tempérament ».

Malgré ses efforts, il ne vécut jamais dans une réelle aisance, mais connut souvent la misère, même s'il bénéficia de son vivant d'une réelle célébrité, surtout par ses activités de journaliste et son poème "Le Corbeau".

L'alcoolisme de Poe a été démesurément exagéré, pour suggérer que sa vie aurait été une longue suite de beuveries et le disqualifier en tant qu'auteur. D'abord, il est peu probable qu'il ait pu écrire ou concevoir ses poèmes ou ses contes sous l'influence de l'alcool, ne serait-ce qu'en raison de la longueur, de l'arrondi et de la construction soignée de ses phrases. Ensuite, son flirt avec l'alcool était intermittent ; s'il lui arrivait de boire plusieurs jours de suite, il pouvait ne pas toucher une goutte d'alcool pendant des mois ou des années.

Avant 1841, il n'existe aucun document témoignant de ses rapports à l'alcool. En , il écrivit au docteur J. Evans Snodgrass : . Il est possible qu'il ait découvert l'alcool à l'université en 1826, comme nombre d'autres jeunes gens, mais l'un de ses camarades a témoigné du fait qu'il était réputé, parmi les professeurs, pour sa sobriété, son calme et sa discipline. Par la suite, il est demeuré de longues années sans boire ; il obtint trois lettres de recommandation lors de son départ de l'armée en 1829. Sa consommation aurait repris à West Point, mais les témoignages à ce sujet sont douteux. Plus tard, l'un de ses amis a fait état d'une consommation modérée de liqueur, durant son séjour à Baltimore, en 1832.

C'est à Richmond, en 1835, qu'on trouve les premières traces avérées d'une consommation d'alcool excessive, mais occasionnelle. Dans sa lettre à Snodgrass, Poe explique : . Après plusieurs années de sobriété, à la suite de son départ dans le Nord, il semble qu'il se soit remis à boire, en diverses occasions, à l'époque de la maladie de son épouse, la succession des améliorations intermittentes et des rechutes l'ayant fait sombrer dans la dépression. Vers la fin , Poe rejoignit la division Shockoe Hill des "Sons of Temperance", à Richmond. Quant aux rumeurs d'alcoolisme, elles sont fondées sur le fait que, d'une part, il ne supportait pas l'alcool, et que, d'autre part, plusieurs personnes, soit qu'elles fussent fâchées avec lui (comme Thomas Dunn English), soit qu'elles pussent se compter comme ses ennemis, ont profité de ces quelques occurrences où il est apparu ivre pour généraliser et prétendre qu'il était alcoolique, cela afin de le blesser et de salir son honneur, puis sa mémoire. De même, si le vin est un thème fréquent, dans les contes de Poe, il apparaît toujours sur un mode satirique ; les personnages décrits comme des connaisseurs sont généralement ivres ou sots ; le plus noble des vins n'apparaît pas comme un moyen de rendre la vie plus agréable ou plus riche, mais comme un piège pour l'imprudent et le faible. Le vin servait à Poe de métaphore ; à travers lui, il se moquait des prétentions de l'Homme et dénonçait ses tares.

L'ambition d'Edgar Poe était de créer une véritable littérature nationale. En effet, à cette époque, l'influence européenne était prépondérante et la production du vieux continent affluait aux États-Unis dont la littérature ne brillait guère que par ses histoires d'horreur et ses romans sentimentaux. À ce titre, son œuvre de critique littéraire fut marquée par une véritable exigence de qualité, ainsi que la dénonciation des facilités et des plagiats. Longfellow fut la plus illustre de ses victimes ; il ne répondit jamais à ses accusations, encore que ses amis se fissent un plaisir, en réponse, de calomnier Edgar Poe dans les milieux littéraires new-yorkais.

Edgar Poe a laissé d'importants écrits théoriques, influencés par August Wilhelm Schlegel et Coleridge, qui permettent de donner sens à son œuvre. Ses réflexions littéraires renvoient à ses conceptions cosmogoniques. Dans , il explique que l'univers, à l'origine, était marqué par l'unicité. Il a éclaté par la suite en quelque chose que l'on pourrait rapprocher de la théorie du Big Bang, mais il aspire à retrouver son unité. C'est dans cet ouvrage, qui date de 1848, qu'est exposée la première solution plausible au paradoxe d'Olbers. De même, en littérature, l'unité doit l'emporter sur toute autre considération. D'où la théorie de l'effet unique qu'il développe dans "Philosophie de la composition" (traduit par Baudelaire sous le titre de "Genèse d'un poème"): le but de l'art est esthétique, c'est-à-dire l'effet qu'il crée chez le lecteur. Or, cet effet ne peut être maintenu que durant une brève période (le temps nécessaire à la lecture d'un poème lyrique, à l'exécution d'un drame, à l'observation d'un tableau, etc.). Pour lui, si l'épopée a quelque valeur, c'est qu'elle est composée d'une série de petits morceaux, chacun tourné vers un effet unique ou un sentiment, qui « élève l'âme ». Il associe l'aspect esthétique de l'art à l'idéalité pure, affirmant que l'humeur ou le sentiment créé par une œuvre d'art élève l'âme et constitue, de ce fait, une expérience spirituelle. Le poème, le conte, le roman ne doit tendre que vers sa réalisation, et toute digression doit être rejetée. De même, le roman à thèse, où l'intrigue est entrecoupée de dissertations sur tel ou tel sujet, est à proscrire. Adversaire du didactisme, Poe soutient, dans ses critiques littéraires, que l'instruction morale ou éthique appartient à un univers différent du monde de la poésie et de l'art, qui devrait seulement se concentrer sur la production d'une belle œuvre d'art.

L'univers, dit-il, est un poème de Dieu, c'est-à-dire qu'il est parfait. Mais l'Homme, aveugle aux œuvres de Dieu, ne voit pas cette perfection. C'est au poète, qui a l'intuition de cette perfection, grâce à son imagination créatrice, de la faire connaître à l'humanité. Mais certains poètes mégalomanes, guidés par ce que les Grecs anciens appelaient "hubris", au lieu d'admettre l'impossibilité de l'imitation parfaite de l'intrigue de Dieu par l'Homme, prétendent se livrer à une concurrence sacrilège. Marqués non par l'imagination créatrice, mais par la "fancy" , ils ne voient pas la perfection de la création divine ; leur esprit aveuglé interprète le monde en fonction de leur cœur, de leur propre tourment intérieur ; ils sont voués au néant par leur ambition prométhéenne. Dans la première catégorie, on peut citer le chevalier Auguste Dupin ("Double assassinat dans la Rue Morgue", "Le Mystère de Marie Roget" et "La Lettre volée"), William Legrand ("Le Scarabée d'or") ou le baron Ritzner von Jung ("Mystification"). De même, dans certains contes, l'illusion est révélée par un parent au narrateur fiévreux qui a fui une épidémie de choléra dans "Le Sphinx", par des lunettes qu'on offre au narrateur myope dans "Les Lunettes", par la révélation des causes psychosomatiques de la sorte de catalepsie dont souffre le narrateur dans "L'Enterrement prématuré". Dans la seconde catégorie, la figure la plus marquante est Roderick Usher, dont l'influence néfaste « contamine » le regard du narrateur et lui fait voir comme surnaturels des phénomènes qui ont, en fait, une explication rationnelle (Poe disséminant adroitement les indices de cette explication dans le texte).

Dans "La Lettre volée" (en anglais, "The Purloined Letter"), Edgar Poe imagine une intrigue où un certain « D. » (peut-être un frère du héros, le chevalier Auguste Dupin, comme semble l'indiquer la citation de la tragédie "Atrée et Thyeste" de Crébillon père : « Un destin si funeste, / S'il n'est digne d'Atrée, est digne de Thyeste. ») vole à une dame de qualité une lettre compromettante. Pour la cacher aux policiers, qui surveillent ses allers-retours et fouillent son hôtel pendant son absence, il la met bien en évidence dans un tableau accroché au mur. L'aveuglement des policiers, à l'esprit médiocre, renvoie à l'aveuglement des hommes, incapables de saisir la perfection de l'intrigue de Dieu. Quant à « D. », Poe le décrit comme dominé par la "fancy", au contraire du chevalier Dupin, qui finit par l'emporter grâce à son imagination créatrice.

La narration, chez Poe, est marquée par la polysémie, dont témoignent les nombreux jeux de mot, dans les textes tragiques comme dans les textes comiques. Le narrateur, qui se signale le plus souvent par des lectures néfastes (littérature fantastique à l'allemande, romans gothiques, ésotérisme, métaphysique), décrit une histoire déformée par sa "fancy", il ne maîtrise pas son écriture, dans laquelle plusieurs indices permettent d'appréhender la réalité sous-jacente.

Nombre d'histoires d'Edgar Poe, principalement celles qui devaient figurer dans les "Contes de l'In-Folio", qu'elles relèvent du tragique ou du comique, appartiennent au registre de la parodie. Son but est de démontrer l'inconsistance des fausses gloires de son temps, dont seuls quelques-uns ont échappé à l'oubli. Ainsi, "Metzengerstein" imite les horreurs inventées dans les romans gothiques, comme "Le Château d'Otrante" d'Horace Walpole ou "Les Élixirs du diable" d'Ernst Theodor Amadeus Hoffmann. L'histoire repose sur la croyance en la métempsycose, pour laquelle Edgar Poe a toujours manifesté un profond mépris et qui relevait pour lui de l'aliénation mentale. Dans "Le Duc de l'Omelette", il se moque des maniérismes et du style affecté de Nathaniel Parker Willis. Dans "Un événement à Jérusalem", qui reprend un roman de Horace Smith, "Zilhah, a Tale of the Holy City" (1829), il ridiculise l'orientalisme des romantiques. Quant à "Manuscrit trouvé dans une bouteille", il représente un pastiche des récits de voyage. De même, des contes comme "Bérénice" raillent les outrances auxquelles se livraient les revues de l'époque. "Le Roi Peste", de son côté, démonte les mécanismes du roman "Vivian Grey" (1826), récit plein de fantaisie débridée à travers lequel, non sans incongruité, Benjamin Disraeli entendait dénoncer l'ivrognerie. De même, dans "Comment écrire un article à la « Blackwood" » et "A Predicament", la satire dénonce l'absurdité des contes à sensation, qui faisaient la fortune du "Blackwood's Magazine", très célèbre revue d'Édimbourg. Quant à l'héroïne, Psyché Zenobia, c'est une femme de lettres américaine, un « bas-bleu », Margaret Fuller, dont les sympathies pour les transcendantalistes suffisaient à énerver Poe.

Plus largement, quand l'actualité ne venait pas lui fournir un sujet, il puisait assez souvent dans ses nombreuses lectures (que favorisait son travail de critique littéraire) pour concevoir et construire ses œuvres de fiction. Ainsi, "Hop Frog" est inspiré de l'accident advenu à Charles VI lors du bal des ardents, tel que l'a décrit Jean Froissart dans ses "Chroniques". De même, "William Wilson" est directement inspiré de la trame d'un poème dramatique que Byron aurait eu l'intention d'écrire, dont Washington Irving avait révélé le contenu dans "The Gift" en 1836. Nathaniel Hawthorne s'était lui-même servi de ce matériau pour rédiger "Howe's Masquerade". Il s'est également inspiré, pour sa nouvelle "La Barrique d'Amontillado", de "La Grande Bretèche" d'Honoré de Balzac.

Il pouvait aussi faire appel, comme tout écrivain, à son expérience personnelle. Ainsi, "Un matin sur le Wissahicon" relate au départ une promenade qu'il avait faite à Mom Rinker's Rock et la rencontre d'un daim apprivoisé, même s'il s'éloigne vite de la simple transcription de souvenirs pour se livrer à une contemplation émerveillée de la nature et à une réflexion sur l'altération des paysages créée par la présence humaine, et plus largement sur les rapports entre l'industrie humaine et la beauté (sa description perdant tout réalisme pour basculer dans l'onirisme et offrir un coup d'œil éphémère sur une vision céleste).

Edgar Poe est un auteur prolifique, qui laisse deux romans, de nombreux contes et poèmes, outre ses essais, ses critiques littéraires et son abondante correspondance. Une partie importante de ses contes et poèmes ont été traduits en français par Charles Baudelaire et Stéphane Mallarmé. D'une très grande qualité littéraire, ces traductions comportent cependant quelques erreurs et libertés par rapport à l'original, parfois graves pour la compréhension de la pensée de Poe. Si les poèmes ont pu faire l'objet de retraductions, le rôle joué par Baudelaire dans la célébrité de Poe en Europe empêche tout travail en ce sens, et seuls les textes qu'il a laissés de côté ont fait l'objet de traductions plus récentes. On trouve plusieurs contes et poèmes de Poe en accès libre sur le web.

Pendant longtemps, l'image d'Edgar Poe fut tronquée ; elle l'est encore dans une partie importante du public. Poe fut victime d'un pasteur baptiste bien-pensant, par ailleurs littérateur jaloux, Rufus Griswold (1815-1857) , qui s'acharna à détruire son image. Le , déjà, il écrivait dans le "New York Tribune" : « Edgar Poe est mort. Il est mort à Baltimore avant-hier. Ce faire-part étonnera beaucoup de personnes, mais peu en seront attristées. […] L'art littéraire a perdu une de ses plus brillantes et de ses plus bizarres célébrités. » Par la suite, chargé avec James Russell Lowell et Nathaniel Parker Willis d'assurer l'édition des "Œuvres posthumes " de Poe, il rédigea une notice biographique parue en tête du troisième tome, selon Claude Richard. Il prétendit ainsi qu'il était alcoolique, mélancolique, c'est-à-dire victime d'un déséquilibre mental, et que c'était un personnage sinistre qui avait des . Les légendes qu'il forgea eurent longtemps seules droit de cité, malgré les protestations des amis de Poe (Sarah Helen Whitman, John Neal, George Rex Graham, George W. Peck, Mrs Nichols ou Mrs Weiss). C'est grâce aux travaux de John Henry Ingram (1880), James A. Harrison (1902) et Arthur Hobson Quinn (1941) que la vérité sur le travail de l'écrivain fut rétablie, avec l'édition, en 1902, des œuvres complètes de Poe, dite "Virginia Édition", qui comporte dix-sept volumes.

En France même, où ses œuvres ont connu très tôt un large écho, grâce essentiellement aux efforts de Charles Baudelaire, nombre d'études témoignent d'une méconnaissance assez large du poète américain. Une part des légendes qui se colportent ont d'ailleurs été transmises par Baudelaire, lui-même, qui s'est reconnu dans cette image de l'écrivain hanté et misérable et l'a présenté avec trop d'insistance comme le parangon des poètes maudits et sulfureux. Même s'il dénonce largement les légendes colportées par Rufus Griswold (parmi lesquelles celle de l'alcoolisme de Poe), rappelant que, selon plusieurs témoins, il ne buvait généralement que fort peu, il décrit ce supposé alcoolisme comme « un moyen mnémonique, une méthode de travail ». De même, il lui attribue ses propres penchants pour la drogue.
Plus tard, en 1933, Marie Bonaparte se livra à une importante étude psychanalytique, qui est fréquemment citée parmi les grandes critiques de Poe et de son œuvre, et qui a eu une grande influence sur la réception de l'œuvre de Poe, ne serait-ce qu'en raison de son analyse des textes de Poe suivant le prisme de la psychanalyse freudienne. Cela dit, plusieurs critiques considèrent son ouvrage comme assez contestable dans sa manière de reproduire et d'amplifier certaines légendes véhiculées par Griswold. Par exemple, elle affirme qu'Edgar Poe aurait aperçu, dans sa petite enfance, ses parents faisant l'amour, déduisant de cet événement des complexes dont témoigneraient, selon elle, ses textes. Influencée par les légendes répétées à l'envi depuis Griswold, qui présentent Poe comme un être neurasthénique, alcoolique, drogué, marqué par la fatalité, elle fait partie des analystes qui considèrent que Poe a écrit une œuvre largement autobiographique, transcrivant sur le papier ses propres terreurs. Pour ce faire, si elle corrige certaines erreurs de la traduction de Baudelaire, elle se livre elle-même à certaines déformations, pour justifier son propos. Ainsi, la phrase : « Si dans maintes de mes productions, la terreur a été le thème, je soutiens que cette terreur n'est pas d'Allemagne, mais de l'âme . », tirée de la préface des "Contes du grotesque et de l'arabesque", devient, sous sa plume : « "Si dans maintes de mes productions, la terreur a été le thème, je soutiens que cette terreur n'est pas d'Allemagne, mais de" mon "âme" ». Pour ces critiques, cette lecture ignore pour une part le travail de l'écrivain et méconnaît la pensée de Poe, que l'auteur prétend qualifier de « nécrophile en partie refoulé en partie sublimé ». Ainsi, selon le psychanalyste Édouard Pichon, . Par ailleurs, et dans une perspective très différente de celle d'une Marie Bonaparte ou d'un René Laforgue, Jacques Lacan a également livré un commentaire psychanalytique de la nouvelle intitulée "La Lettre volée".

Depuis 1917, une statue d'Edgar Allan Poe réalisée par Moses Ezekiel est installée dans le campus de la faculté de droit de l'université de Baltimore, à l'initiative de l"'Edgar Allan Poe Memorial Association of Baltimore", fondée en par le "Women's Literary Club of Baltimore".

Une statue en bronze de l'auteur, œuvre de Charles Rudy, a été offerte à la ville de Richmond par le George Edward Barksdale. Installée avec un socle de granit rose sur le square près du Capitole de l'État de Virginie le , elle a été inaugurée le 7 octobre suivant.

Une plaque commémorative a été apposée le , pour le anniversaire de sa naissance, sur la façade d'un immeuble près de Carver Street (actuellement, Charles Street South), dans le quartier de Bay Village, à Boston, où il a vu le jour. Puis, le , lors du bicentenaire de sa naissance, le maire de Boston, Thomas Menino, a inauguré avec Paul Lewis, professeur à Boston College, le square Poe, situé dans le même quartier, à l'angle de Boylston Street et de Charles Street, en face du Boston Common.

L'université de Virginie, à Charlottesville, conserve la mémoire d'Edgar Allan Poe et de la chambre où il a vécu de à . On a donné son nom à l'allée (Poe Alley) qui borde le bâtiment.

La West 84th Street, à New York, a été baptisée « Edgar Allen Poe Street ». Elle est située dans l'Upper West Side, au nord-ouest de Manhattan, entre Riverside Park et Central Park, et coupée par Broadway. C'est là que se trouvait la ferme des Brennan, où les Poe ont vécu quelque temps entre 1844 et 1845. On trouve également une place à son nom dans le Bronx, à proximité du cottage où les Poe ont habité entre 1846 et 1849.

En 1927, une voie a été ouverte dans la zone de la butte Bergeyre, située dans le quartier du Combat, au sud-ouest du arrondissement de Paris, à proximité du parc des Buttes-Chaumont ; elle a été baptisée « rue Edgar-Poe » l'année suivante.

Plusieurs autres rues portent son nom dans le monde, notamment à Berkeley, Bologne, Carhaix-Plouguer, Fontaine-le-Comte, Hartsdale (État de New York), Le Havre, Laredo (Texas), Mérignac, Nîmes, Niort, Palerme, Palo Alto, Portland, Providence, Reggio d'Émilie, Richmond, São José dos Pinhais, San Diego, Staten Island, Tours, Woodmere (État de New York), Xàbia ; des avenues à Ames, Cleveland, Dayton, East Meadow, Lithopolis (Ohio), Mount Pleasant (Caroline du Sud), Newark, Northridge (Ohio), Somerset (New Jersey), Stafford (Virginie), Urbana, Vandalia (Ohio), Westfield, Worthington (Ohio) ; des places à Baldwin (État de New York), Fairfield, Piscataway, Shelton, South Plainfield (New Jersey), Westerville (Ohio) ; des cours à Annandale (Virginie), Kendall Park (New Jersey) et Morganville (New Jersey), à Norfolk, New Windsor (État de New York), North Wales (Pennsylvanie), Roxbury (New Jersey), Staten Island, Williamstown (New Jersey).

Plusieurs écoles ont adopté son nom, notamment les écoles élémentaires d’Arlington Heights (Illinois), de Suitland, dans le comté de Prince George (Maryland) (Maryland), ou de Girard Estate, au sud de Philadelphie, inscrite dans le NRHP depuis le , ainsi que l’école élémentaire et secondaire ("Junior High School") de San Antonio. À Paris, un lycée privé sous contrat, le « lycée Edgar-Poe », porte son nom depuis sa création en 1965 dans le arrondissement de Paris.

La plus ancienne des maisons existant encore où ait vécu Poe se trouve à Baltimore. Elle est conservée sous la forme d’un Musée Edgar Allan Poe. Poe est censé avoir vécu dans cette maison à 23 ans, quand il s’installa une première fois avec Maria Clemm et Virginia ainsi que sa grand-mère et, peut-être, son frère William Henry Leonard Poe. Elle est ouverte au public, de même que le siège de la Société Edgar Allan Poe.

Poe, son épouse Virginia et sa belle-mère Maria ont, par la suite, loué plusieurs maisons à Philadelphie, mais seule la dernière de ces maisons est encore debout. La maison de Spring Garden, où vécut l’auteur en 1843-1844, est aujourd’hui conservée par le Service des parcs nationaux en tant que Site historique national Edgar Allan Poe. Elle se situe entre la rue et la rue Spring Garden et est ouverte du mercredi au dimanche de 9 heures à 17 heures.

La dernière maison de Poe, un cottage dans le Bronx, à New York, est également conservée.

La plus ancienne maison de Richmond, baptisée « Virginia », où Poe n’a jamais vécu, est aujourd’hui le siège d’un Musée Edgar Allan Poe, centré sur les premières années de l’écrivain auprès de la famille Allan.

Au cinéma la première adaptation est le film français muet en 1928 "La Chute de la maison Usher" réalisé par Jean Epstein. Suit un court métrage muet d'horreur américain la même année : ' réalisé par James Sibley Watson and Melville Webber. Il faut attendre 1960 pour voir "La Chute de la maison Usher", film fantastique américain réalisé par Roger Corman. Dans les années 2000 plusieurs films ont été réalisés. ' film d'horreur anglais de Ken Russell interprété par lui-même et Mediæval Bæbes. L'année suivante : ' film hollandais en anglais avec Katherine Heigl et Jeremy London. En 2004 : "Usher" écrit et réalisé par Roger Leatherwood. Et en 2006, ' film policier réalisé par Hayley Cloake.

Trois opéras ont également été écrits : "La Chute de la maison Usher" opéra inachevé (il travailla à sa partition de 1908 à 1917, mais ne l'acheva jamais) en un acte et deux scènes que Claude Debussy composa sur son propre livret. Une première version de ' par Glass et une seconde ' un opéra rock du chanteur anglais Peter Hammill, fondateur du groupe Van der Graaf Generator, et réédité en 1999 dans sa version définitive.

La première adaptation eu lieu en 1914 : "La Conscience vengeresse" (' en anglais) film américain réalisé par D. W. Griffith. Puis "Le Cœur révélateur" (' en anglais) court-métrage américain réalisé par Jules Dassin, sorti en 1941. Un nouveau court métrage américain de moins de dix minutes portant le même titre ' sort en 1953. Un troisième film portant le même titre original sort en 1960, il s'agit d'un long métrage d'horreur de 78 minutes réalisé par Ernest Morris. En 2009 sort le long métrage anglo-américain ' réalisé par Michael Cuesta avec Josh Lucas, Lena Headey et Brian Cox. En 2012, Ryan Connolly sort un court-métrage d'horreur psychologique "Tell".

Le jeu vidéo ' dans ses énigmes fait référence à Poe et à '.

' a été adapté six fois au cinéma à commencer en 1915 par un film muet sur la biographie d'Edgar Allan Poe réalisé par Charles Brabin avec Charles Brabin dans le rôle d'Edgar Poe. Puis en 1935 sort le film d'horreur américain Le Corbeau (') de Lew Landers avec Boris Karloff et Béla Lugosi. En 1963 sort le film fantastique américain Le Corbeau de Roger Corman avec à nouveau Boris Karloff, Jack Nicholson et Vincent Price. La quatrième adaptation ' sort en 2006 dirigée par le réalisateur allemand Ulli Lommel. En 2011 le réalisateur britannique Richard Driscoll sort '. En 2012 la sixième adaptation se nomme "L'Ombre du mal" (ou "Le Corbeau" au Québec) (""), qui est un thriller américain réalisé par James McTeigue.




Voir la catégorie dédiée : 


Dès son vivant, Edgar Allan Poe a été traduit en de nombreuses langues, et par d'innombrables auteurs ou rédacteurs, célèbres ou inconnus du public, et avec des résultats littéraires comme commerciaux plus ou moins heureux. En langue française nous connaissons essentiellement les traductions faites par Charles Baudelaire, mais contrairement à l'idée répandue une recherche approfondie dans les archives historiques des journaux, gazettes et quotidiens de l'époque, et dans la presse nationale mais aussi régionale, montre que Baudelaire fut loin d'être le premier à tenter de faire connaître Edgar Poe au public français (avant lui il y eut Gustave Brunet dès 1844, Alphonse Borghers dès 1845, Emile Forgues en 1846, et Isabelle Meunier en 1847). Il existe notamment sur le site web officiel de l’"Edgar Allan Poe Society of Baltimore" une excellente étude très complète sur les nombreuses traductions et tentatives de traductions de l’œuvre d'Edgar Allan Poe de son vivant... et jusqu'au centenaire de sa mort en 1949 et jusque dans la presse régionale française. Une page d'une importance considérable pour les bibliographes tant elle donne de sources inattendues mais précises et vérifiables, objets potentiels de visites à des archives historiques de la presse ou à des bibliothèques de nos villes de province. Nous nous contenterons de citer ici les deux principaux traducteurs connus du public français : Baudelaire et Mallarmé, ainsi qu'un traducteur plus tardif mais important, Félix Rabbe, qui a publié en 1887 un livre de 355 pages contenant une traduction en français de plusieurs contes et poèmes parmi ceux restés jusque-là non traduits, un ouvrage réédité récemment en eBook gratuit.

Charles Baudelaire :
Stéphane Mallarmé :
Félix Rabbe :







</doc>
<doc id="1075" url="https://fr.wikipedia.org/wiki?curid=1075" title="Edgar Rice Burroughs">
Edgar Rice Burroughs

Edgar Rice Burroughs, né à Chicago le et mort à Los Angeles le , est un romancier américain, créateur de "Tarzan", l'homme-singe, l'un des personnages de fiction les plus connus au monde et de "John Carter", l'un des premiers héros de science fiction. Il est également l'auteur de plusieurs séries de science-fiction et de romans policiers.

Burroughs est le quatrième fils du Major George Tyler Burroughs (1833–1913), homme d'affaires et vétéran de la Guerre de Sécession, et de Mary Evaline (Zieger) Burroughs (1840–1920). Burroughs étudie dans plusieurs écoles, et durant l'épidémie de grippe de Chicago en 1891, il passe 6 mois dans le ranch de son frère sur la Raft River en Idaho.

Il suit ensuite les cours de la Phillips Academy à Andover, Massachusetts, puis il est diplômé de l'Académie militaire du Michigan en 1895. Il tente sans succès d'entrer à l'Académie militaire de West Point et s'enrôle comme simple soldat dans le 7e régiment de cavalerie (États-Unis) à Fort Grant, Arizona. Il est réformé en 1897 à cause d'un problème cardiaque.

Après une période d'errance et de petits boulots, Burroughs travaille dans l'entreprise familiale à partir de 1899. Il épouse son amour de jeunesse Emma Hulbert en janvier 1900. En 1904 il quitte son poste et cherche du travail en Idaho, mais il ne tarde pas à revenir à Chicago.

En 1911, après sept années de bas salaires, il commence à écrire une nouvelle, alors qu'il travaille comme marchand de taille-crayons. À cette époque Burroughs et Emma ont deux enfants, Joan (1908–1972), qui épousera plus tard James Pierce l'acteur incarnant Tarzan au cinéma, et Hulbert (1909–1991). Pendant cette période il bénéficie de grandes plages de temps libre, ce qui lui permet de lire nombre de Pulp (magazine). En 1929, il se rappellera cette période en ces termes :

Ciblant son travail pour ces magazines pulps, Burroughs publie sa première histoire, sous le pseudonyme de Normal Bean, intitulée "Les conquérants de Mars", en épisodes dans "Argosy" en 1912.

Rapidement il se met à écrire à plein temps, et avant même que "Les conquérants de Mars" aient été entièrement publiés, deux nouvelles histoires sont achevées dont "Tarzan seigneur de la jungle", qui est publié à partir d'octobre 1912 et deviendra son plus grand succès. En 1913, le couple a un troisième et dernier enfant John Coleman Burroughs (1913–1979).

Burroughs écrit également des récits de science-fiction et de fantasy, entraînant des aventuriers terriens vers diverses planètes, Barsoom (Mars), Amtor (Vénus), îles perdues, à l’intérieur de la Terre creuse dans le cas du Cycle de Pellucidar, ainsi que dans des westerns et des romans historiques.

Tarzan a un grand impact culturel à sa sortie. Burroughs est déterminé à capitaliser sur sa popularité de toutes les manières possibles, il prévoit d'exploiter Tarzan sous forme de comics (bande dessinée), de films et de produits dérivés. Des experts tentent de l'en dissuader, arguant que les différents médias entreraient en concurrence s'ils étaient diffusés en même temps. Mais Burroughs ne les écoute pas et prouve qu'il a raison : le public veut Tarzan sous toutes ses déclinaisons. Tarzan est toujours l'un des personnages de fiction les plus populaires.

En 1915 ou 1919, Burroughs achète un grand ranch au nord de Los Angeles, Californie, qu'il nomme « Tarzana ». Les habitants de la commune qui naît autour de ce ranch adoptent ce nom pour leur ville. Tarzana, Californie, est fondée en 1927 ou 1928.

De même, le lieu-dit de Tarzan, Texas, est reconnu officiellement en 1927 quand la poste en accepte le nom, à la suite de la popularité du premier film, "Tarzan chez les singes" (muet), avec Elmo Lincoln.

En 1923, Burroughs crée sa compagnie, , et commence à faire imprimer ses livres dans les années 1930.

En 1934, il divorce d'Emma et épouse l'ancienne actrice Florence Gilbert Dearholt (1904-1991) en 1935, ex-femme de son ami . Il adopte également les deux enfants des Dearholt. Le couple divorce en 1942.

Au moment de l'attaque de Pearl Harbor, Burroughs habite à Hawaï et, malgré la soixantaine avancée, il demande à devenir correspondant de guerre. Il devient ainsi l'un des correspondants de guerre américains les plus âgés. Une fois la guerre terminée, il déménage à Encino, Californie, où, après de nombreux problèmes de santé, il décède d'une crise cardiaque le , laissant presque 70 romans.

Le , sur Mars, est nommé en son honneur.

Les critiques les plus fréquentes à l'égard de l'œuvre de Burroughs font état du caractère très répétitif de ses histoires (un héros idéal part sauver son épouse/sa fiancée, elle-même perfection faite femme, enlevée par un « méchant » stéréotypé). Cependant, le succès de ses romans tient aussi à son incontestable imagination.

Que ce soit avec les aventures de Tarzan, celles de John Carter, les histoires de Vénus, ou celles d'une civilisation au cœur même de notre Terre (le "cycle de Pellucidar"), Edgar Rice Burroughs a inspiré de nombreux auteurs de science-fiction et de littérature fantastique du .





</doc>
<doc id="1076" url="https://fr.wikipedia.org/wiki?curid=1076" title="Effet de serre">
Effet de serre

L’effet de serre est un processus naturel résultant de l’influence de l'atmosphère sur les différents flux thermiques contribuant aux températures au sol d'une planète. La prise en compte de ce mécanisme est nécessaire pour expliquer les températures observées à la surface de la Terre et de Vénus. Dans le système solaire, l'essentiel de l'énergie thermique reçue par une planète provient du rayonnement solaire et, en l’absence d'atmosphère, une planète rayonne idéalement comme un corps noir, l'atmosphère d'une planète absorbe et réfléchit une partie de ces rayonnements modifiant ainsi l'équilibre thermique. Ainsi l'atmosphère isole la Terre du vide spatial comme une serre isole les plantes de l'air extérieur. 

L'usage de l'expression "effet de serre" s'est étendu dans le cadre de la vulgarisation du réchauffement climatique causé par les gaz à effet de serre qui bloquent et réfléchissent une partie du rayonnement thermique. Or le bilan thermique d'une serre s'explique essentiellement par une analyse de la convection et non du rayonnement : la chaleur s'accumule à l'intérieur de la serre car les parois bloquent les échanges convectifs entre l'intérieur et l'extérieur. Aussi, le terme scientifique, utilisé par la communauté des climatologues pour décrire l’influence des gaz à effet de serre, composants de l'atmosphère bloquant le rayonnement infrarouge, sur le bilan thermique de la Terre, est forçage radiatif. 

Les températures terrestres résultent d'interactions complexes entre les apports solaires perturbés par les cycles de l'orbite terrestre, de l'effet albédo de l'atmosphère, des courants de convection dans l'atmosphère et les océans, du cycle de l'eau et le forçage radiatif de l'atmosphère notamment. 

Dans les années 1780, Horace-Bénédict de Saussure mesure les effets thermiques du rayonnement solaire à l'aide de boîtes transparentes qu'il dispose dans la vallée et au sommet d'une montagne. 

En 1824, Joseph Fourier publie "Remarques générales sur les températures du globe terrestre et des espaces planétaires" dans lesquelles il affine l'analyse des expériences de Horace-Bénédict de Saussure en concluant « la température du sol est augmentée par l'interposition de l'atmosphère, parce que la chaleur solaire trouve moins d'obstacles pour pénétrer l'air, étant à l'état de lumière, qu'elle n'en trouve pour repasser dans l'air lorsqu'elle est convertie en chaleur obscure ».

En 1861, John Tyndall identifie les principaux responsables de ce mécanisme : la vapeur d'eau et le dioxyde de carbone. Il suggère alors qu'une modification de la composition de l'atmosphère peut avoir une influence sur l'évolution du climat. 

En 1896, Svante August Arrhenius propose la première estimation de l'impact du niveau de dioxyde de carbone sur les températures terrestres. Il estime qu'un doublement de la quantité de dioxyde de carbone devrait augmenter de la température moyenne. Il espère ainsi que l'exploitation du charbon permettra de surmonter la prochaine ère glaciaire due à l'orbite terrestre. Le géologue américain arrivera indépendamment aux mêmes conclusions.

En 1909, Robert Williams Wood montre que contrairement à une idée reçue le blocage du rayonnement infrarouge par le verre n'est pas le principal mécanisme qui explique le fonctionnement d'une serre. Par conséquent le terme scientifique, adopté par le GIEC, utilisé pour décrire l’influence des composants de l'atmosphère bloquant le rayonnement infrarouge sur le bilan thermique de la Terre est forçage radiatif et non effet de serre.

L'expression synthétique "effet de serre" provient de la vulgarisation au début des des résultats alarmants des recherches climatologiques. Alors que les climatologues analysent l'impact du dioxyde de carbone sur le climat sans parler d'effet de serre, les premières alertes pour infléchir les décisions politiques sont lancées au début des années 1980 en utilisant cette expression, reprise par la suite dans des rapports de plus en plus médiatisés, comme le rapport Brundtland (1987). En France, Jean-Marc Jancovici et Hervé Le Treut ont vulgarisé les risques liés à l'effet de serre depuis les années 1980.

Contrairement à une idée reçue, et comme le suggère ce nom, l'effet de serre, sous-entendu le mécanisme lié à l’absorption et à l'émission de radiations thermiques par le verre, n'est pas primordial dans le fonctionnement d'une serre. En 1909, Robert Williams Wood a réfuté par l'expérience cette explication. En remplaçant le verre qui recouvre une serre par du halite, un matériau totalement transparent aux infrarouges, Robert Wood mesure une augmentation similaire de température dans les deux cas. Aussi l'augmentation de température dans une serre ne s'explique pas par le fait que le verre réfléchit les infrarouges. L'expression « effet de serre » a néanmoins été conservée dans l'usage courant. Mais le terme scientifique, utilisé par la communauté scientifique pour décrire l’influence des composants de l'atmosphère bloquant le rayonnement infrarouge sur le bilan thermique de la Terre, est forçage radiatif.

Le fonctionnement d'une serre s'explique essentiellement par une analyse de la convection et non du rayonnement : la chaleur s'accumule à l'intérieur de la serre car les parois bloquent les échanges convectifs entre l'intérieur et l'extérieur.

Lorsque le rayonnement solaire atteint l'atmosphère terrestre, une partie (environ 30 %) est directement réfléchie, c'est-à-dire renvoyée vers l'espace, par l'air, les nuages blancs et la surface claire de la Terre () ; l'albédo est la mesure de cet effet de miroir. Les rayons incidents qui n'ont pas été réfléchis vers l'espace sont absorbés par l'atmosphère (20,7 %) et la surface terrestre (51 %).

Cette dernière partie du rayonnement absorbée par la surface du sol lui apporte de la chaleur qu'elle restitue à son tour, le jour comme la nuit, en direction de l'atmosphère. Le transfert de chaleur entre la Terre et l'atmosphère se fait, conformément au deuxième principe de la thermodynamique, du chaud (la terre) vers le froid (l'atmosphère) ; il se fait par convection (réchauffement et humidification de l'air au contact du sol puis ascension de cet air et libération de la chaleur latente de la vapeur d'eau lorsqu'elle se condense en nuages) et sous forme de rayonnements infrarouges lointains (dans la plage principalement, correspondant au « rayonnement du corps noir » pour la température du sol). L'effet de serre ne s'intéresse qu'à ces rayonnements, qui seront absorbés en partie par les gaz à effet de serre, ce qui contribue à réchauffer l'atmosphère.
Puis dans un troisième temps, cette chaleur contenue par l'atmosphère est réémise dans toutes les directions ; une partie s'échappe vers l'espace, mais une autre partie retourne vers la Terre et vient en déduction de l'apport de chaleur de la surface vers l'atmosphère, donc s'oppose au refroidissement de la surface. Il est à noter que l'excès de chaleur généré par les activités humaines, via l’effet de serre, est absorbé à 93 % par l'océan, qui atténue ainsi l'augmentation de la température dans l'atmosphère. L'océan global joue donc un rôle de thermostat planétaire et de contrôle des grands équilibres naturels planétaires.

Sans effet de serre (ce qui implique notamment : sans vapeur d'eau et sans nuages), et à albédo constant, la température moyenne sur Terre chuterait à . Mais à cette température la glace s'étendrait sur le globe, l'albédo terrestre augmenterait, et la température se stabiliserait vraisemblablement en dessous de (voir glaciation Varanger).

Les gaz à effet de serre sont des composants "gazeux" de l'atmosphère qui contribuent à l'effet de serre (sans perdre de vue que l'atmosphère contient d'autres composants non gazeux qui contribuent à l'effet de serre, comme les gouttes d'eau des nuages sur Terre). Ces gaz ont pour caractéristique commune d'absorber une partie des infrarouges émis par la surface de la Terre.

Les principaux gaz à effet de serre sont la vapeur d'eau, le dioxyde de carbone (), le méthane (), l'oxyde nitreux (ou protoxyde d'azote, de formule ) et l'ozone (). Les gaz à effet de serre industriels incluent les halocarbones lourds (fluorocarbones chlorés incluant les CFC, les molécules de HCFC-22 comme le fréon et le perfluorométhane) et l'hexafluorure de soufre ().

Contributions approximatives à l'effet de serre des principaux gaz, d'après le GIEC :

La plupart des gaz à effet de serre (GES) sont d'origine naturelle. Mais certains d'entre eux sont uniquement dus à l'activité humaine ou bien leur concentration dans l'atmosphère augmente en raison de cette activité. C'est le cas en particulier de l'ozone (), du dioxyde de carbone () et du méthane (). La preuve que l'augmentation du atmosphérique est d'origine humaine se fait par analyse isotopique. Par contre, ce dernier gaz rejeté dans l'atmosphère ne participe que pour 40 % à l'effet de serre additionnel provenant de l'activité humaine.

Répartition des gaz à effet de serre anthropiques (dus aux activités humaines) :
L'ozone est fourni en grande quantité par l'activité industrielle humaine, alors que les CFC encore largement utilisés détruisent, eux, l'ozone, ce qui fait que l'on peut constater un double phénomène : 

La combustion des carbones fossiles comme le charbon, le lignite, le pétrole ou le gaz naturel (méthane) rejette du en grande quantité dans l'atmosphère : la concentration atmosphérique de gaz carbonique a ainsi augmenté de , passant de la valeur pré-industrielle de aujourd'hui. Un des secteurs d'activités qui dégagent le plus de gaz à effet de serre est l'énergie : à ce sujet, voir l'article énergie et effet de serre. Ces combustibles augmentent, de plus, la concentration de gaz à effet de serre, car ils étaient enfouis dans le sol depuis des milliers d'années ce qui a rompu l'équilibre. Il s'agit d'un ajout additionnel de gaz carbonique dans l'atmosphère qui n'est pas non plus complètement compensé par une assez grande absorption : seule la moitié serait recyclée par la nature ; l'autre moitié resterait dans l'atmosphère et augmenterait l'effet de serre.

La seconde cause d'émission de gaz à effet de serre est la déforestation, qui est responsable à elle seule de 20 % des émissions mondiales. Les déboisements les plus importants concernent les trois grandes forêts tropicales que sont la forêt amazonienne, la forêt du bassin du Congo, et la forêt indonésienne. Il s'agit d'une des plus grandes causes, car tout le carbone absorbé par ces arbres est rediffusé dans l'air. S'il y avait replantation, cette quantité de dioxyde de carbone serait réabsorbée par un autre arbre, mais sans replantation, alors il n'y a qu'un ajout de la quantité de ce gaz dans l'air.

Les activités humaines dégagent donc une abondance de GES : les scientifiques du GIEC qui étudient le climat "estiment" que l'augmentation des teneurs en gaz d'origine anthropique est à l'origine d'un réchauffement climatique. 

En France, selon le groupe, les émissions de gaz à effet de serre proviennent des transports pour 26 %, suivis de l’industrie (22 %), de l’agriculture (19 %), des bâtiments et habitations (19 %), de la production et de la transformation de l’énergie (13 %), et du traitement des déchets (3 %). Depuis 1990, les émissions ont augmenté de plus de 20 % pour les transports et les bâtiments. En revanche, elles ont diminué de 22 % dans l’industrie, de 10 % dans le secteur agricole, de 9 % dans le secteur de l’énergie et de 8 % pour le traitement des déchets.

Dans le cadre de la réduction des émissions de gaz à effet de serre engendré par la circulation automobile, une étude réalisée pour le PREDIT a montré l'influence des politiques de stationnement sur les possibilités de limiter la génération de gaz à effet de serre. La démarche concerne les émissions liées à la construction de places de stationnement, à l'exploitation des parkings et surtout à la mobilité induite par l'offre de stationnement.

On craint au pire le déclenchement d'un effet « boule de neige » (rétroaction positive), où le réchauffement conduirait à un réchauffement encore accru, via la disparition des glaces (réduction de l'albédo) et surtout la libération de stocks naturels de GES actuellement fixés par le pergélisol, les hydrates de méthane marins, ou encore la biomasse. 

Si cela se produit et les réactions ne se terminent qu'après avoir produit une grande augmentation de la température, cela s'appelle un emballement de l'effet de serre (" en anglais).

Selon l'hypothèse du fusil à clathrates (" en anglais), un emballement de l'effet de serre "pourrait" être causé par la libération de méthane à partir des clathrates (hydrates de méthane qui tapissent le fond des océans) à la suite du réchauffement climatique. On suppose que l'extinction massive d'espèces lors du Permien-Trias a été causée par un tel emballement. Il est également estimé que de grandes quantités de méthane pourraient être libérées de la toundra sibérienne qui commence à dégeler, le méthane étant plus puissant comme gaz à effet de serre que le dioxyde de carbone.

Une telle hypothèse reste toutefois hautement improbable : des études récentes ont en effet prouvé que l'hydrate de méthane du fond des océans était stable, et que celui contenu dans le pergélisol n'avait que peu de chance de s'en échapper.

L'effet de serre n'est pas en soi nocif aux écosystèmes ; sans lui, la Terre ne serait qu'une boule de glace où la vie ne serait pas possible, car il n'y aurait pas d'eau liquide. Le danger pour les écosystèmes réside plutôt dans la variation trop rapide et trop importante des conditions climatiques pour que la plupart des espèces dites "évoluées" puissent s'adapter en cas de changements de température et de pluviométrie. Des écosystèmes marins et littoraux pourraient également être touchés par une hausse du niveau de la mer et des modifications des courants marins et des conditions physico-chimiques de l'eau de mer (acidité, taux de gaz dissous…). Les populations humaines seraient évidemment touchées par le réchauffement climatique. En effet, une hausse des températures aide à la prolifération des maladies infectieuses puisque celles-ci survivent mieux dans des milieux chauds et humides. 

Le GIEC envisage, selon les scénarios, des augmentations de à pour le siècle à venir en supposant que l'augmentation des rejets de continue au rythme des années (on n'a pas observé de ralentissement global des émissions, même depuis la signature du protocole de Kyoto par la plupart des pays). Un arrêt total et immédiat des rejets de carbone n'empêcherait cependant pas la température moyenne de la planète de continuer à augmenter pendant plusieurs dizaines à centaines d'années, car certains gaz à effet de serre ne disparaissent de l'atmosphère que très lentement.

Sur Vénus l'effet de serre a porté la température à plus de . Une étude affirme que cet effet ne serait pas dû au dioxyde de carbone qui constitue 96 % de l'atmosphère, mais à des constituants en très faibles quantités relatives tels que et . En effet, dans le domaine infrarouge correspondant au maximum d'émission thermique pour un corps à la température de la surface et de la basse atmosphère de Vénus, le présente des fenêtres de transmission très larges qui ne peuvent piéger efficacement le rayonnement infrarouge. En revanche, et absorbent les radiations dans ce domaine de longueurs d'onde, tout comme le font également les fines particules d'acide sulfurique qui constituent les nuages. Vénus, plus proche (72,3 %) du Soleil que la Terre, reçoit ainsi près du double (191 %) de l'énergie solaire reçue par celle-ci.

D'autres études contredisent cependant ce point et mettent en avant le rôle essentiel du dans l'effet de serre vénusien.

L'atmosphère de Mars contient une grande proportion de , néanmoins l'atmosphère de la planète est trop fine pour avoir un impact significatif sur la température (estimé à moins de 5,5°C). Le constituant environ 96 % de l'atmosphère Martienne, sa pression partielle est approximativement égale à la pression totale atmosphérique de , tandis que celle sur Terre est d'environ . Sur Terre, la fraction molaire en dans l'air est seulement de 0,04 %, soit par million (ppm).

L'effet de serre et le réchauffement climatique qu'il induit sont assez souvent confondus avec l'altération de la couche d'ozone. Il s'agit pourtant de deux phénomènes bien distincts, le premier concernant la rétention dans l'atmosphère des infrarouges (autrement dit de la chaleur) ; le second concernant l'augmentation de la transparence de l'atmosphère aux ultraviolets. Par ailleurs, si les principaux responsables de l'altération de la couche d'ozone, à savoir les CFC (chlorofluorocarbures, interdits dans les pays industriels dès 1989) sont aussi des gaz à effet de serre, l'inverse n'est pas vrai : les gaz à effet de serre tels que le dioxyde de carbone et le méthane n'ont aucun effet sur la couche d'ozone.








</doc>
<doc id="1079" url="https://fr.wikipedia.org/wiki?curid=1079" title="Escalade">
Escalade

L’escalade, ou grimpe, parfois appelée varappe (vieilli), est une pratique et un sport consistant à se déplacer le long d'une paroi pour atteindre le haut d'un relief ou d'une structure artificielle, par un cheminement appelé voie et avec ou sans aide de matériel. Le terrain de pratique va des blocs de faible hauteur aux parois de plusieurs centaines de mètres en passant par les murs d'escalade. Physiquement, l'escalade est un sport complet sollicitant aussi bien les mains et les bras que les jambes et le tronc ainsi que des aptitudes mentales importantes. Le pratiquant est couramment appelé « grimpeur ». 

Cette discipline se crée progressivement à la fin du dans les courses des premiers alpinistes vers les grands sommets, avant de se démocratiser au siècle suivant, devenant populaire dès la fin des années 1970. Les premières compétitions officielles sont organisées en 1988 par l'Union internationale des associations d'alpinisme (UIAA). Chaque année est organisée une Coupe du monde de difficulté, de bloc et de vitesse, et tous les deux ans des championnats du monde, l'ensemble étant supervisé par la Fédération internationale d'escalade ().

L'escalade comporte des risques variables selon les différentes spécialités qui ont, chacune, mis au point un équipement apportant de la sécurité, sauf l'escalade en solo intégral où le grimpeur évolue sans système d'assurage, comme cela a été montré par Patrick Edlinger dans les films de Jean-Paul Janssen "La Vie au bout des doigts" et "Opéra vertical" ou encore par les ascensions de bâtiments réalisées par Alain Robert.

À l'origine, l'escalade n'était pas pratiquée comme activité à part entière, mais consistait seulement en un moyen d'accéder à un endroit surélevé qui donnait un meilleur champ de vision ou une meilleure protection contre les dangers. Les hommes préhistoriques escaladaient notamment certaines parois rocheuses offrant des cavités en hauteur afin d'être protégés des animaux sauvages et autres prédateurs. Au fil des siècles, certains peuples se sont démarqués par leur aptitude à escalader des parois rocheuses, comme les Chinois dont il existe des aquarelles datant du qui représentent des hommes escaladant des rochers. Au , les Amérindiens Anasazis sont devenus réputés pour leurs qualités de grimpeurs qui leur permettaient d'installer leur village sur les hauteurs des falaises. Leurs aptitudes étaient tellement grandes que lorsque les Navajos sont arrivés dans la même région, ils pensaient que les Anasazis avaient des pouvoirs magiques qui leur permettaient de grimper ainsi. Le 28 juin 1492, Antoine de Ville réussit à atteindre le sommet du mont Aiguille dans le Vercors, réalisant ainsi la première ascension officiellement reconnue de l'histoire de l'alpinisme. Dès lors, l'escalade se retrouve intégrée à la pratique de l'alpinisme et permet aux alpinistes de réaliser l'ascension de sommets toujours plus haut qui restaient inaccessible par la marche.

À la fin du , l'alpinisme se développe et de nombreux clubs alpins se créent notamment en Allemagne, en France, en Italie, en Angleterre et aux États-Unis. Les alpinistes commencent à s'intéresser à la pratique seule de l'escalade en la séparant de l'ascension complète d'une montagne ; en 1886, Walter Parry Haskett Smith réalise l'ascension de Napes Needle, un piton rocheux de situé à flanc de montagne dans le Lake District en Angleterre. Cette ascension est reconnue comme étant le début de l'escalade comme activité à part entière, car, pour une fois, le but de l'ascension n'était pas d'atteindre un sommet mais juste de réussir à grimper ce morceau de roche. Avec l'augmentation de la difficulté des voies d'alpinisme, de nombreux alpinistes commencent à pratiquer l'escalade, notamment comme un moyen d'entraînement. Ils vont alors grimper sur les parois du Salève en Haute-Savoie, les blocs de Fontainebleau et les falaises de Lake District et de Dresde en Allemagne orientale lors de sorties organisées par les premiers clubs alpins fraîchement créés.

Les années suivantes, le niveau des grimpeurs progresse vite malgré le matériel encore très basique et les premières voies dans le de cotation sont rapidement ouvertes. En 1903, Siegfried Herford réalise l'ascension de ' (5) au Scafell en Angleterre et Olivier Perry-Smith celle de ' (4+/5) à Dresde en Allemagne. Ces deux voies atteignent alors la limite du système de cotation utilisé à l'époque et qui avait été créé par Hans Dülfer. Deux ans plus tard Perry-Smith ouvre un nouveau niveau de difficulté avec les réalisations de ' et de '. Ces voies seront classées par la suite dans le , lors de la mise en place du système de cotation proposé par Willo Welzenbach en 1925.

À cette époque, ce niveau est considéré comme la limite des possibilités humaines dans le domaine de l'escalade. Pendant des années l'escalade est pratiquée de manières très différentes selon les pays, les clubs alpins se réunissent alors à Chamonix en 1932 et fondent l'Union internationale des associations d'alpinisme (UIAA) afin de coordonner les actions des différents clubs et de gérer les problèmes inhérents au milieu de l'escalade. Durant la première moitié du , l'escalade progresse au rythme de l'évolution du matériel et des performances des grimpeurs, et des voies d'escalade de difficultés croissantes sont ouvertes au fil des années.

En 1945, avec la fin de la guerre, la Fédération française de la montagne (FFM) est créée à la demande du Haut commissariat aux sports afin de développer les sports de montagne comme l'alpinisme et l'escalade. Les années suivantes, l'escalade connaît un fort engouement, notamment aux États-Unis, et de nombreuses salles d'escalade sont ouvertes. De plus, l'apparition de nouveau matériel, comme les pitons à expansion, permet de pratiquer l'escalade dans des endroits inaccessibles jusqu'ici. La première voie américaine dans le est ouverte en 1957 par Royal Robbins, Mike Sherrick et Jerry Gallwas, en réussissant l'ascension de la face nord-ouest du Half Dome dans le Parc national de Yosemite. Cette réalisation est la première d'une longue série de réussites américaines au parc du Yosemite, mais aussi en Europe. En 1962, Gary Hemming, Royal Robbins et trois de leurs compatriotes ouvrent "La directe américaine" aux drus, puis en 1965, "La directissime" toujours aux Drus. Ils ouvrent aussi de nombreux itinéraires sur El Capitan comme ', (1961) ' (1964) ou encore "Mescalito" (1974), qui sont encore aujourd'hui des références de l'escalade artificielle. Parallèlement, l'escalade libre se développe peu à peu, en suivant le concept éthique consistant à ne pas endommager la voie avec trop de matériel et à réussir les ascensions sans aide.

Fort de leurs expériences sur les parois du Yosemite, les Américains font progresser l'escalade rapidement et de nouveaux degrés de cotation sont ouverts. En 1970, Ron Kauk réalise l'ascension d"'Astroman" (7a/5.11c), la première voie dans le , puis en 1972, John Bragg réussit le dévers de "Kansas City" le premier 7b et finalement en 1974, Steve Wunsch qui réussit "Supercrack", le premier 7c. Depuis la création de la FFM, la France est restée en retrait et n'a pas connu la même progression car l'escalade est restée peu médiatisée comparée à l'alpinisme. Mais, elle rattrape rapidement son retard avec notamment Jean-Claude Droyer, qui ouvre les premiers 6b en 1976 puis les premiers 6c et 7a en 1977, et surtout Patrick Berhault et Patrick Edlinger qui, dès la fin des années 1970, réalisent un grand nombre de premières au Verdon et à Buoux, ainsi que plusieurs ascensions en solo intégral.

Si pendant cette période, le développement de l'escalade a lieu essentiellement dans les pays occidentaux, le bloc de l'Est innove en organisant dès 1947 les premières compétitions d'escalade. À partir de cette date, l'URSS organise des compétitions qui sont la combinaison d'une épreuve de , semblable à la difficulté, et d'une épreuve de vitesse où les grimpeurs étaient assurés en moulinette par un câble d'acier. Ces compétitions sont principalement réservées aux athlètes russes jusque dans les années 1980.

En 1979, Toni Yaniro, un jeune grimpeur de 18 ans, ouvre le en réalisant "Grand Illusion" (8a/5.13b). Cependant, cette ascension est mal vue du milieu de la grimpe, à cause de la méthode employée par Yaniro : après chaque tentative, il laisse la corde mousquetonnée réalisant ensuite de nombreux essais en moulinette. Cette pratique de « travailler » une voie difficile avant son enchaînement en tête, habituelle de nos jours à haut-niveau, était rarement utilisée à cette époque ; l'éthique des grimpeurs privilégiait le style (ascension "à vue", engagement) plutôt que la difficulté. Trois ans plus tard, en 1982, le reportage de Jean-Paul Janssen, "La vie au bout des doigts", est diffusé dans l'émission sur Antenne 2 (devenue France 2). Le documentaire qui traite de la passion de Patrick Edlinger pour l'escalade et le solo intégral remporte un grand succès tant en France que dans le reste du monde, allant jusqu'à être nominé aux Oscars, et fait connaître ce sport au grand public. C'est à cette époque que l'escalade devient une discipline sportive à part entière et que sont organisées ses premières compétitions internationales.

Au milieu des années 1980, Andrea Mellano, un membre du groupe académique du Club alpin italien, et Emanuele Cassarà, un journaliste sportif italien préparent la première compétition moderne d'escalade et convainquent les meilleurs grimpeurs mondiaux d'y participer . Au même moment en France, le manifeste des 19 est signé par plusieurs grimpeurs de haut niveau afin de s'opposer à l'esprit de compétitivité dans ce sport. Malgré cela, la rencontre italienne, une épreuve de difficulté, a lieu le 7 juillet 1985 sur les falaises de Bardonecchia en Italie devant spectateurs ; les vainqueurs sont Catherine Destivelle chez les femmes et Stefan Glowacz chez les hommes. L'année suivante, le succès est encore plus grand et la finale, remportée par les Français Patrick Edlinger et Catherine Destivelle, est suivie par plusieurs télévisions européennes et plus de spectateurs. La même année, la France organise la première compétition en intérieur à Vaulx-en-Velin dans la banlieue lyonnaise. En 1988, l'UIAA reconnaît officiellement le circuit des "" puis, en 1989, la Coupe du monde d'escalade de difficulté et de vitesse.

Forte d'une reconnaissance mondiale, l'escalade se développe de plus en plus, appuyée par l'apparition des spits et plaquettes qui permettent d'augmenter la sécurité lors des ascensions laissant le grimpeur se concentrer davantage sur la technicité et la difficulté des voies. De plus, de nombreuses salles d'escalade sont construites dans les villes et des techniques d'entraînement scientifiques sont mises au point par Edlinger et Alain Ferrand. Cependant, le monde de l'escalade reste majoritairement dominé par les hommes, hormis quelques rares exceptions comme Catherine Destivelle qui réalise le premier 8a féminin en 1986.

Durant les années 1980, la cotation explose rapidement, notamment avec Wolfgang Güllich, un jeune grimpeur allemand. Ayant réussi en 1982 la première répétition de "Grand Illusion", la voie cotée 8a ouverte par Yaniro, Wolfgang pousse encore le niveau en 1984 et réalise la première ascension de ' à Altmühtal qui devient le premier 8b au monde. En 1985, il réussit le premier 8b+, ', puis en 1987 le premier 8c avec '. Mais c'est l'Anglais Ben Moon qui réalise la première voie cotée 8c+ en 1990 avec l'ascension de "Hubble" à Raven Tor au Royaume-Uni. Finalement en 1991, après un long entraînement spécifique, Wolfgang Güllich fait l'ascension dAction directe" et évalue sa cotation à 8c+/9a. Cependant de nombreux répétiteurs finiront par lui attribuer une cotation de 9a, en faisant ainsi la première voie dans le , qui est actuellement le plus haut degré de difficulté en escalade.

En haute montagne aussi, le niveau technique d'escalade rocheuse augmente rapidement au cours des années 1980. Sous l'impulsion notamment de Michel Piola, de nombreuses voies d'escalade sont ouvertes dans le massif du Mont-Blanc. La beauté et la difficulté de l'escalade sont alors préférés à l'atteinte des sommets.

Durant les années 1990, l'augmentation rapide de la cotation se calme, et le monde de l'escalade voit surtout de nombreux grimpeurs répéter les différentes voies ouvertes les années précédentes. La seule exception étant "Akira", une voie particulièrement difficile réalisée par Fred Rouhling en 1995 et qu'il évalue à 9b. Cependant cette ascension a toujours été remise en question par le milieu de l'escalade, principalement à cause du manque de preuve et cela même si personne n'a été en mesure de la répéter.

Parallèlement à cette augmentation du niveau global des grimpeurs et à l'ouverture de quantité de nouvelles voies d'escalade de toutes difficultés, une nouvelle discipline commence à se développer : le bloc. Proposant une escalade plus courte mais plus technique et difficile, le bloc permet de travailler certains enchaînements de mouvements sans la contrainte du matériel ni l'obligation d'escalader plusieurs mètres de parois avant d'arriver au passage difficile de la voie. Certains grimpeurs comme Fred Nicole y consacrent d'ailleurs une grande partie de leur temps, et le niveau ne tarde pas à augmenter avec le développement de la discipline. Les sites de Fontainebleau, Hueco Tanks ou encore Cresciano, deviennent rapidement les endroits incontournables de cette pratique et voient un grand nombre d'ouvertures de blocs cotés entre 7B et 8A. Mais c'est surtout vers le petit site d'escalade situé à Branson en Suisse que le monde se tourne. Une première fois en 1992, lorsque Fred Nicole réalise "La danse des Balrogs", le premier bloc coté 8B au monde, puis une seconde fois en 1996, où il réussit "Radja", le premier 8B+. La reconnaissance du bloc comme discipline d'escalade se traduit par son introduction en compétition, d'abord en 1998 comme test, puis de manière officielle l'année suivante.

Les années 1990 sont aussi marquées par l'arrivée de femmes dans le haut niveau de l'escalade. La Française Isabelle Patissier fait de nombreuses ascensions à haut niveau, notamment dans les gorges du Verdon et domine les compétitions avec l'Américaine Robyn Erbesfield. Mais c'est surtout Lynn Hill qui marquera l'escalade en 1993, en réussissant la première ascension en escalade libre de "" sur la paroi de El Capitan au Yosemite. Cette voie de répartie en 34 longueurs, n'avait alors jamais été réalisée dans ce type d'escalade, démontrant ainsi le potentiel féminin dans l'escalade. Cet exploit est suivi cinq ans plus tard par la première ascension féminine d'une voie cotée 8c, "Onky Tonky", réalisée par Josune Bereziartu.

En novembre 2000, la difficulté en bloc augmente une nouvelle fois avec l'ascension par Fred Nicole de "" à Cresciano en Suisse. Il évalue la cotation de ce bloc à 8C, ce qui déclenche rapidement une polémique, notamment sur le nombre de mouvements que requiert ce bloc.

En 2001, c'est un jeune grimpeur américain qui fait parler de lui en élevant à nouveau le niveau de difficulté. Alors âgé de seulement 20 ans, Chris Sharma réussi la première ascension de "Biographie", une voie cotée 9a+ qui avait été équipée en 1989 par Jean-Christophe Lafaille sur les falaises de Céüse en France. Les années suivantes seront notamment marquées par les nombreuses premières ascensions et répétitions à très haut niveau par une génération de jeunes grimpeurs ayant commencé l'escalade dès leur plus jeune âge. Certains se démarquant plus en bloc comme Paul Robinson ou Daniel Woods, d'autres en voies comme Chris Sharma et notamment Adam Ondra, qui est le plus jeune grimpeur au monde à avoir atteint le neuvième degré à l'âge de 13 ans. À partir de 2008, de nouveaux niveaux de cotations sont atteints, notamment avec Chris Sharma et Adam Ondra qui ouvrent plusieurs voies cotées 9b (', ') puis 9b+ (', ', Vasil Vasil).

Les années 2000 et 2010 sont aussi marquées par nombre de discussions et polémiques sur les cotations de voies et surtout de bloc au plus haut niveau. D'une part, parce que la cotation a augmenté très rapidement durant les vingt dernières années, et d'autre part parce que beaucoup de cotations sont revues à la baisse. Quelques grimpeurs comme Dave Graham, Nalle Hukkataival et Daniel Woods prennent même activement part aux discussions, tentant de redéfinir clairement les limites du très haut niveau.

Les femmes réalisent aussi des ascensions à très haut niveau, et après Josune Bereziartu, qui est longtemps restée la seule femme à réussir l'ascension d'une voie dans le neuvième degré, Sasha DiGiulian, Charlotte Durif et Muriel Sarkany atteignent aussi ce niveau en 2013. En 2016, elles sont une quinzaine à avoir atteint le . En bloc, c'est la jeune grimpeuse Ashima Shiraishi qui, à l'âge de 11 ans, fait surtout parler d'elle en réussissant à faire l'ascension de "", un bloc coté 8B/V13, difficulté alors atteinte par seulement quelques femmes ; en 2015, elle réalise une voie 9a+? et en 2016 la première à réaliser un bloc 8C/V15. En février 2017, Margo Hayes réalise "La Rambla" (9a+) et est désignée comme la première femme à atteindre ce niveau.

En 2007, la Fédération internationale d'escalade est fondée afin de développer les compétitions au niveau mondial. En 2011, à Arco, ont lieu les premiers championnats du monde de "paraclimbing" qui concernent les déficients visuels, les déficients neurologiques et les amputés. La première Coupe du monde de bloc handisport est organisée en 2014. Après plusieurs années de délibération, l'escalade est finalement intégrée aux Jeux olympiques de 2020 à Tokyo.

Il existe deux différents types d'escalade, classés selon les méthodes utilisées pour atteindre le sommet d'une voie :

De nombreux types de pratique de l'escalade sont distingués, selon la nature du terrain, la méthode d'ascension et le niveau d'équipement des sites d'escalade. L'équipement en place (les protections) dans les voies d'escalade est variable en fonction de la nature de celles-ci, du type de roche, de règles propres à un secteur géographique suivies par les grimpeurs locaux, ou de la personne ayant mis en place les équipements de la voie.

L'escalade sportive se pratique sur des voies entièrement équipées, où des points d'ancrage (spits ou broches scellées) ont été mis en place au préalable, compte tenu du cheminement envisagé de la voie, afin de permettre au grimpeur de se protéger en mousquetonnant sa corde. Apparue dans les années 1980, l'escalade sportive est un des types d'escalade les plus modernes, et c'est aussi la plus sécuritaire.

L'escalade sportive est notamment pratiquée lors des compétitions d'escalade de difficulté.

L'escalade dite « traditionnelle » (également dénommée « trad ») se pratique sur des voies peu ou pas équipées : elle associe l'escalade libre et l'usage exclusif de points d'assurage amovibles. Ces protections posées ne devant pas laisser de trace sur la paroi ni endommager le rocher (escalade propre), au contraire des trous forés pour insérer des pitons à expansion ou même des simples pitons. Le grimpeur pratiquant ce type d'escalade doit juger de la qualité de l'équipement qu'il rencontre et placer lui-même des protections supplémentaires : des coinceurs dans les fissures et les trous ; des sangles autour de becquets, lunules et arbres. La pose de protection n'est possible que si le rocher le permet, cette escalade se pratique donc typiquement sur des voies à fissures.

En France, l'escalade « traditionnelle » est globalement cantonnée aux voies en montagne ou aux sites classés par la Fédération française de la montagne et de l'escalade (FFME) comme terrains d'aventure. Dans d'autres pays, notamment la République tchèque, le Royaume-Uni et les États-Unis, cette pratique est majoritaire y compris sur des falaises de faible hauteur.

Le bloc se pratique sans baudrier ni corde sur des blocs ou murs rocheux de faible hauteur : il nécessite donc peu voire pas de matériel. Pour limiter les risques de blessure lors d'une chute au sol, un ou plusieurs (tapis de protection) sont posés au sol pour amortir les réceptions ; de plus, il est utile qu'un partenaire effectue une afin de guider et amortir la chute du grimpeur.

Pratiqué dès la fin du par les alpinistes qui y voyaient un simple support d'entraînement, le bloc est aujourd'hui une discipline sportive à part entière et l'objet de compétitions spécifiques. Au-delà de l'aspect ludique lié à des contraintes moins nombreuses, le bloc est aussi la recherche d'un absolu : le mouvement le plus esthétique permettant de résoudre un « problème » difficile. Certains passages de blocs peuvent en effet ne comporter que trois ou quatre mouvements, voire un seul, à l'exemple du jeté spectaculaire de "" (8A) dans le site de Fontainebleau.

L'escalade en solo ou se pratique de manière autonome, sans la présence d'un second grimpeur assurant le premier : le grimpeur évoluant seul peut donc soit s'assurer lui-même, soit progresser sans protection, on parle alors d'escalade en solo intégral.

L'escalade en solo avec auto-assurage se pratique de manière autonome, mais en utilisant des systèmes d'assurage. Ce type d'escalade peut être pratiqué dans le cadre de l'escalade libre ou de l'escalade artificielle. Elle fait appel à des techniques complexes d'assurage en tête ou bien sur corde tendue depuis le haut de la voie : leur mise en œuvre peut être facilitée par l'utilisation de matériels spécifiques, comme des dispositifs mécaniques de blocage ou anti-chute, des absorbeurs de chocs, des cordes statiques.

L'escalade en solo intégral se pratique seul et sans aucun système d'assurage. Certains grimpeurs sont particulièrement célèbres pour avoir réalisé de nombreuses ascensions en solo intégral. Parmi eux, Patrick Edlinger, qui fait de nombreuses ascensions dans les gorges du Verdon, devenu célèbre grâce aux films de Jean-Paul Janssen "La Vie au bout des doigts" et "Opéra vertical", mais aussi Alex Honnold qui a réussi plusieurs records en solo, comme l'enchaînement en 18 heures du ' en 2012, c'est-à-dire la trilogie de ', du "" et du mont Watkins dans le parc national de Yosemite.

Depuis le milieu des années 1990, le grimpeur français Alain Robert fait aussi régulièrement parler de lui dans les médias en faisant l'ascension de bâtiments publics de gratte-ciel comme la Burj Khalifa ou la Tour First. Ces réalisations sont faites la plupart du temps sans autorisation ce qui lui a valu nombre d'arrestations par les forces de l'ordre.

Le solo intégral est également pratiqué au-dessus de l'eau ; on parle alors de "psicobloc" ou de ' (« solo d'eau profonde » en anglais). Cette pratique permet de faire du solo intégral sans risquer de se tuer lors d'une chute, mais n'enlève pas complètement la possibilité de se blesser car l'impact sur l'eau peut être la source de contusion ou traumatisme. Apparu à la fin des années 1970, le psicobloc est particulièrement pratiqué sur les falaises de l'île de Majorque, dans les calanques de Marseille ou plus récemment en Thaïlande mais est resté peu connu du grand public.

Cette pratique a notamment été médiatisée par Edlinger dans "La Vie au bout des doigts" (1982), le court-métrage "Psicobloc" (2002), le premier topo consacré au psicobloc à Majorque (2006), Sharma réalisant l'arche "" (2007, 9b) ou l'organisation des compétitions "Psicobloc Masters" depuis 2013.

Le terrain essentiel de pratique de l'escalade est le rocher, mais il existe d'autres terrains de pratiques :

Les sites naturels d'escalade (SNE) comprennent l'ensemble des reliefs rocheux propices à la pratique de l'escalade. Les pratiquants distinguent ces sites selon les types géologiques de roche, le profil des parois, la longueur des voies et l'équipement permanent éventuellement en place. Les reliefs de haute montagne sont généralement considérés comme des lieux de pratique de l'alpinisme plus que de l'escalade, en raison de techniques et risques spécifiques (approche, conditions, neige, etc.)

Le type géologique se définit principalement par la nature des roches : le calcaire (site des gorges du Verdon, Calanques, Dolomites), le grès (Fontainebleau, Buoux), les poudingues et conglomérats (Mallos de Riglos, Canaille), le granite (massif du Mont-Blanc), roches volcaniques (Massif central, Allemagne, Islande), gneiss (Mercantour, Caroux), etc. La nature des roches, leurs déformations tectoniques (strates, cassures) et les effets de l'érosion (polissage, délitement, trous, taffoni) induisent d'importantes différences pour l'équipement et les mouvements d'escalade : type de prises, adhérence, facilité de protection, risques d'effritement ou chute de pierre, etc.

Les pratiquants distinguent aussi les sites selon le profil géométrique des parois qui induit des styles ou mouvements d'escalade spécifiques : dalle, paroi verticale, dévers. Ils distinguent aussi les faces (lisses) et les « faiblesses » d'une paroi : fissure, écaille, colonne, arête, dièdre, cheminée, etc. Les sites sont également distingués selon leur hauteur : le bloc (typiquement moins de cinq mètres), la couenne (typiquement moins de quarante mètres), le site de grandes voies (nécessitant plusieurs relais d'assurage), le "" (nécessitant plusieurs jours d'ascension).

Les sites sportifs sont des sites d'escalade où les points d'ancrage permettent d'assurer le grimpeur durant la totalité de son ascension. Les points sont généralement constitués de pitons à expansion ou de spits et le relais doit comporter au moins deux points d'ancrages reliés ou pouvant l'être. Selon les pays, la disposition et le matériel d'ancrage peuvent être soumis à des normes réglementaires.

L'équipement d'une falaise est généralement réalisé par des passionnés bénévoles. Après avoir repéré les secteurs d'intérêt, ils obtiennent l'autorisation d'usage auprès des propriétaires (parfois sous forme d'une convention signée avec une fédération sportive). Les équipeurs ou ouvreurs sécurisent le secteur en créant des sentiers d'approche, en purgeant la falaise des blocs et pierres instables, en taillant les arbres et la végétation trop envahissants et brossant éventuellement la roche. L'installation des points d'assurage peut être réalisée depuis le bas, en escaladant peu à peu la nouvelle voie (il s'agit alors d'une « ouverture »), ou plus communément depuis le haut en descendant en rappel. L'équipement d'une longueur de voie requiert typiquement entre une demi-journée et trois jours de travail. Les noms des voies d'un secteur et leur description (cotation, tracé) sont ensuite répertoriés dans des publications destinées aux grimpeurs : les topos.

À l'opposé des sites sportifs, le terrain d'aventure est un site où tout ou partie des équipements servant à l'assurage sont absents ou ne répondent pas aux normes. Le grimpeur doit alors poser lui-même ses protections afin d'assurer sa sécurité. C'est le genre de site où l'escalade traditionnelle peut être pratiquée.

L'escalade se pratique aussi bien en extérieur qu'en intérieur. Les structures artificielles d'escalade (SAE) permettent de pratiquer l'escalade sportive ou le bloc tout au long de l'année, en ville, quelles que soient les conditions météorologiques. Les SAE sont utilisées par certains grimpeurs pour l'entraînement hors-saison (en hiver) ou dans des créneaux horaires mieux adaptés aux obligations quotidiennes (en soirée, après les créneaux scolaires). Les SAE offrent aussi un lieu de pratique dans les régions peu fournies en falaises et blocs rocheux. Les SAE sont parfois considérées comme un lieu plus adapté ou sécurisant pour l'initiation à l'escalade. Les SAE sont également devenues le lieu de pratique préféré par de nombreux grimpeurs, qui se focalisent sur l'escalade en salle ou bien la compétition, et s'intéressent moins à l'escalade rocheuse. 

Les « pans » désignaient à l'origine de simples panneaux de bois de fabrication artisanale et de petite dimension, sur lesquels sont vissées des prises de main et de pied. Les premiers pans étaient bricolés à domicile par certains grimpeurs afin de s'entrainer régulièrement sur de courts passages, de faible hauteur (sans corde). Les pans se sont répandus dans les années 1990, au sein des clubs d'escalade et à l'intérieur des salles d'escalade à corde.

Un « mur d'escalade » est un mur ou une paroi artificielle sur laquelle de nombreuses prises synthétiques sont fixées afin de pouvoir escalader. La plupart du temps, les murs d'escalade sont fabriqués avec des panneaux plats recouverts d'un matériau antidérapant, mais dans certains cas ils présentent un certain relief afin de ressembler aux parois naturelles. 

Les termes "mur", "pan" et "SAE" sont imprécis, car il ne distingue pas les structures avec des points d'ancrage (escalade avec une corde) des structures sans points d'assurage (réception au sol ou sur tapis).

Une « salle d'escalade (à corde) » ou un « mur à corde » désigne souvent une large structure artificielle dédiées à l'escalade sportive, composée d'un ou plusieurs murs équipés de points d'ancrage, abrités typiquement à l'intérieur d'un bâtiment. De telles infrastructures peuvent être privées (entreprise) ou publiques (salle municipale), ouvertes au grand public (droit d'entrée payant) ou accessibles uniquement à certaines personnes (club sportif, infrastructure scolaire). Le système de cotation est en général le même que sur les sites naturels (SNE) et la possibilité de modifier facilement le type et la position des prises permet une grande variété dans la difficulté. Les ouvreurs (les personnes qui créent les voies) renseignent en général au pied des voies des fiches descriptives ou des tableaux récapitulatifs de leur niveau.

Les murs sont conçus généralement en intérieur pour la pratique de l'escalade en salle mais il existe des structures artificielles extérieures (en bois, plastique, béton, ciment, acier, etc.) construites dans cette optique ou détournées de leur usage premier au profit de l'escalade (château d'eau, viaduc, etc.).

L'engouement récent pour la pratique du bloc a conduit à la création de « salles de bloc » modernes, susceptibles de permettre des compétitions. Ces structures artificielles sont dédiées uniquement à l'escalade de bloc, telle que pratiquée dans les compétitions. Elles sont construites à l'intérieur d'un bâtiment, avec une importante surface à grimper, des profils très variés (gros dévers, toit, proue, bombés...). À l'identique de la pratique du bloc en extérieur, l'escalade y est pratiquée sans corde et à des hauteurs limitées. La chute des grimpeurs est amortie par d'épais tapis en mousse. Pour la FFME, ces structures de bloc se distinguent du simple « pan », par une hauteur suffisante et un vaste espace de pratique, de sécurité et de circulation. Dans de tels lieux, des ouvreurs créent régulièrement de nouveaux passages de bloc, en disposant des prises, en les identifiants par des repères (étiquettes, couleur des prises...) et en mentionnant la difficulté (code couleur).

L'escalade est un jeu de (dé)placements et d'équilibre. Le grimpeur doit apprendre à progresser et gérer son centre de gravité dans un univers vertical, et acquérir ainsi un vocabulaire gestuel. Les pieds servent à la progression et à l'équilibre par appui sur des prises, ou par traction (crochetage). La solution la plus facile afin de garder son équilibre est la règle des , qui est par ailleurs recommandée pour les débutants en escalade. Ce principe d'escalade consiste à garder en permanence au moins trois points d'appui lors de la progression, c'est-à-dire les deux pieds et une main ou les deux mains et un pied. Pour maintenir son centre de gravité de manière à faciliter la progression, le grimpeur doit se situer dans l'axe des appuis et proche du rocher. Sur la photo ci-contre, l'homme est en train de faire une partie de bloc, c'est-à-dire qu'il enchaîne une suite de mouvements parfois très compliqués, mais sur une courte distance.

Lorsque l'on débute en escalade, il est important d'apprendre à limiter l'effort fourni par les mains et les bras, les muscles des membres inférieurs étant nettement plus puissants et endurants que ceux des bras ; le rôle des pieds et des jambes est donc important afin de supporter une grande partie du poids du grimpeur. Pour progresser ou effectuer des rétablissements, le grimpeur doit parfois crocheter (se servir de) son talon pour s'équilibrer et réduire l'effort sur ses bras, ce qui lui permet de s'économiser et lui donne ainsi plus de chances de réussir sa voie ou son bloc. Les prises de mains peuvent cependant être utilisées dans de nombreuses directions et être tenues par seulement quelques doigts voire une seule phalange.

Certains mouvements spécifiques servent à la progression dans les cheminées, les toits, les fissures ou les dièdres. De plus, si la plupart des mouvements s'effectuent en statique, où au moins une prise est toujours maintenue durant la progression, les mouvements dynamiques, comme les jetés, ne sont pas exclus, obligeant le grimpeur à lâcher toutes les prises et points d'appuis afin de réussir son mouvement.

Il existe plusieurs techniques différentes pour la progression en escalade, en fonction du type d'ascension et des connaissances et capacités du grimpeur et de l'assureur. Elles font appel aux techniques d'assurage utilisées en terrain vertical.

Lors de l'escalade en tête, le premier grimpeur escalade la paroi sans que la corde soit installée en haut. Au fur et à mesure de sa progression, il relie la corde aux points d'assurage à portée de sa main, par exemple en « mousquetonnant » une dégaine à un piton à expansion puis la corde à cette dégaine. Le premier de cordée procède ainsi jusqu'à arriver au relais. S'il chute, il tombera d'une hauteur au moins égale à deux fois la distance du dernier point mousquetonné. La hauteur est même supérieure à deux fois cette distance du fait de l'élasticité de la corde et de la mobilité de l'assureur.

Arrivé au relais, soit la voie ne fait qu'une longueur (couenne) et il redescend immédiatement grâce à l'assureur (moulinette) ou de manière autonome (rappel), soit il fait monter le second grimpeur en l'assurant depuis le relais avec une technique adaptée. Le second reprend alors les dégaines lors de son ascension afin que le premier puisse les utiliser pour la longueur suivante.

Sur certains types de voies naturelles, l'usage d'une corde "à double" est recommandé pour des raisons de sécurité ou de confort. Par exemple, sur une voie en zigzag, la corde à double permet de réduire les frottements (tirage) ou les chocs aux points d'ancrage, en alternant les mousquetonnages.

L'escalade en second est pratiquée sur les voies de plusieurs longueurs. Dès que le grimpeur qui monte en tête atteint le relais, il s'y accroche de manière fixe (on dit qu'il se ). Il assure ensuite depuis le relais celui qui monte en second. Au fur et à mesure de sa progression, le second récupère les dégaines posées par le premier pour assurer sa progression.

Arrivé au relais, le second peut alors enchaîner sur la longueur suivante, qu'il grimpera alors en tête - on parle de . Il peut aussi rester au relais pour assurer son compagnon. Cette deuxième solution (dite de ), qui s'impose quand le second n'est pas assez expérimenté pour gérer une longueur en tête.
L'escalade « en flèche » est une variante où le premier de cordée grimpe en tête sur une corde à double (avec deux brins de cordes au lieu d'un) et est suivi par deux seconds. L'un des seconds assure alors le premier de cordée sur les deux brins de corde puis, une fois que le premier est arrivé au relais, les deux seconds grimpent simultanément assurés par le premier, chacun sur un seul brin de corde. Un système d'assurage spécifique (plaquette par exemple) est cependant nécessaire pour assurer les deux seconds simultanément. La flèche permet de réaliser l'ascension d'une voie de plusieurs longueurs à trois personnes au lieu de deux habituellement, et elle augmente la sécurité du grimpeur.

L'escalade en corde tendue est la progression simultanée des grimpeurs, sans assureur à l'arrêt. Lors de cette pratique, le grimpeur de tête commence l'ascension jusqu'à ce que la corde qui le relie au second se tende, à ce moment, le second commence à son tour à grimper. L'assurage s'effectue ainsi par le contrepoids d'un grimpeur par rapport à l'autre en cas de chute. Cette pratique nécessite une maîtrise particulière car elle présente des risques supplémentaires, mais elle permet d'avancer rapidement dans la voie car les relais ne sont pas obligatoires tant que le premier a le matériel nécessaire à la protection. Elle est fréquemment utilisée dans les longueurs faciles ou bien lors des records de vitesse sur des parois de plusieurs longueurs comme "The Nose" dans le parc national de Yosemite.

L'escalade dite se pratique avec la corde déjà passée dans le relais en haut de la voie. Le grimpeur est constamment assuré par le haut et n'a généralement pas à utiliser de dégaines lors de sa progression, tandis que l'assureur se trouve au pied de la voie. Cette technique peut être une façon de débuter l'escalade en limitant la crainte de la chute et les connaissances techniques de l'escalade en tête.

Ce type d'escalade est souvent pratiqué dans les afin de limiter le matériel nécessaire et de minimiser les risques.
En escalade sportive, la moulinette est fréquemment utilisée pour « travailler » une voie ou un passage à la limite de son niveau, mais la « réalisation » d'une voie se fait toujours en tête.

La difficulté d'une voie est représentée par un système de cotation, différent suivant les pays. En France, la cotation est signalée par un chiffre (3 - 9) avec des divisions en lettre de "a" à "c" ou avec un "+" ou un "-" si on utilise les anciennes notations. Par exemple, ... < 3a < 3b < 3c < 4a < ...< 9c. Certains topos et les montagnards utilisent des chiffres romains (IV, V+...). Un passage noté sous le 3 correspond à un sentier de randonnée où il peut falloir utiliser les mains. Parfois, on ajoute un + pour signifier que la voie est un peu plus difficile sans pour autant être du niveau supérieur (6b < 6b+ < 6c). On peut aussi donner deux cotations (5c/6a), par exemple si les prises sont difficiles à atteindre pour les petits.

Dans la pratique, les cotations démarrent généralement au 4 voire 3, le 1 correspondant historiquement à la station horizontale dans l'esprit de l'inventeur de cette échelle, Willo Welzenbach.

Il existe d’autres échelles de cotation, notamment aux États-Unis, en Angleterre et en Australie.
Le système de notation anglais propose deux cotations par voie, permettant de noter la difficulté et l'engagement, car la plupart des voies anglaises ne sont pas équipées, et sont parfois difficiles à protéger. La cotation en bloc diffère aussi de l'escalade en falaise.

Le matériel minimal pour pratiquer l'escalade est en général : les chaussons d'escalade afin d'assurer un bon contact entre les pieds du grimpeur et la paroi, et de la magnésie pour réduire l'humidité des mains.

Afin de protéger le grimpeur en cas de chute, du matériel supplémentaire peut être utilisé :

Pour le bloc, selon la hauteur du bloc, sa difficulté et la dangerosité de la réception en cas de chute, le matériel du grimpeur se complète d'un ou plusieurs . Il s'agit d'un matelas de réception qui permet l'amortissement d'une chute et la protection de la zone de réception qui est parfois rendue dangereuse par des cailloux, des racines ou des souches d'arbres. De plus, au moins une personne se charge de parer le grimpeur pour contrôler et amortir sa chute.

Dans ce cas, la hauteur atteinte par le grimpeur nécessite une protection supérieure à celle fournie par le crash pad. Du matériel supplémentaire va donc être utilisé afin d’éviter un retour au sol. Ce matériel se compose de la corde et des éléments de liaison qui vont permettre de l'utiliser.

La corde doit impérativement être dynamique, c'est-à-dire pourvue d'une certaine élasticité et d'une grande résistance aux frottements, à l'opposé des cordes statiques prévues pour une progression verticale sur celles-ci (comme en spéléologie). Elle sert à relier l'assureur au grimpeur afin de protéger ce dernier s'il chute.

Cette corde est attachée au grimpeur par un baudrier au moyen d'un nœud en huit ou d'un nœud de chaise, ceci pour permettre une fixation aisée mais solide et fiable, et aussi par confort lors des ascensions. Aux débuts de l'escalade, la corde était simplement attachée autour de la taille des grimpeurs, ce qui ne garantissait pas une totale sécurité en cas de chute et pouvait parfois être gênant pendant les ascensions.

L'autre extrémité de la corde est reliée à l'assureur au travers d'un dispositif d'assurage. Le défilement de la corde est alors contrôlé au fur et à mesure de la progression du grimpeur en , et l'assureur peut bloquer son défilement au cas où le grimpeur viendrait à chuter. Ce dispositif d'assurage est soit un frein (dans le cas d'un descendeur en huit ou d'un nœud de demi-cabestan), soit un dispositif auto-bloquant comme le grigri ou le cinch. Dans le cas d'une voie en plusieurs longueurs, l'assureur est obligatoirement attaché (ou ) à un relais (ou chaîne) qui est constitué d'au minimum deux points d'ancrage.

En escalade sportive, lors de son ascension, le grimpeur se contente de passer sa corde au travers de dégaines fixées sur les points d'ancrage de la paroi. Mais dans le cadre de l'escalade traditionnelle, c'est-à-dire pour les falaises peu ou pas équipées (souvent appelées ), du matériel supplémentaire est nécessaire pour la protection. Les dégaines sont alors fixées à des coinceurs ou des pitons parfois par le biais de sangles.

Dans tous les cas et pour des raisons de sécurité, ce matériel de base est souvent complété d'un casque afin de protéger le grimpeur comme l'assureur des chutes de pierres qui peuvent être plus ou moins fréquentes selon les sites d'escalade.

Pour l'escalade artificielle, l'équipement du grimpeur reprend celui utilisé pour l'escalade sportive. Il s'y ajoute tout équipement permettant une progression artificielle : coinceurs en nombre suffisant, étriers afin de se hisser sur l'ancrage pour en poser un nouveau, longes, des crochets permettant de s'arrimer temporairement, marteau pour poser des pitons, dégaines explosives pour soulager le poids sur les ancrages en cas de chutes, etc.

Dans ce cas, le matériel ne vise donc plus seulement à minimiser les conséquences d'une chute, mais il permet de créer des points d'appui supplémentaire utilisés par le grimpeur pour sa progression.

Les grimpeurs utilisent également systématiquement un casque car ils sont davantage exposés à se cogner la tête. Il est également possible de porter des genouillères car le grimpeur est souvent assis dans son baudrier avec les genoux contre le rocher pour poser ses ancrages.

L'escalade peut être pratiquée de manière très libre, cependant, comme tous les autres sports, elle est soumise à de nombreuses normes. Notamment sur tout ce qui concerne, la sécurité des grimpeurs. Puisque toute défaillance dans le matériel d'escalade peut avoir des conséquences sur la santé des pratiquants, les fabricants de ce matériel doivent respecter de normes strictes. Elles définissent les caractéristiques matérielles des équipements, en priorité les équipements de protection individuelle (EPI), leur contrôle qualité et l'information faite aux usagers sur ces équipements, ainsi que certaines obligations pour la pratique de l'escalade sur les structures artificielles.

En Europe, le Comité européen de normalisation (CEN) établit des directives, en concertation avec les acteurs concernés, que tout matériel vendu dans l'Union européenne doit respecter. Il doit aussi être conforme aux lois de l'Union européenne et porter le marquage CE (conformité européenne). En France, les normes sont harmonisées avec celles européennes par l'Association française de normalisation (AFNOR). De plus, ce matériel est soumis aux normalisations ISO tout au long de sa chaîne de fabrication afin d'assurer la qualité des composants.

La Commission européenne de normalisation établit les normes sur le plan européen, tandis que l'AFNOR traite des normes françaises. De plus, l'Union internationale des associations d'alpinisme (UIAA) définit également un label selon des normes souvent plus strictes que celles de la Commission européenne, et tous les fabricants adhérents à cette association mondiale doivent respecter un cahier des charges précis afin de bénéficier de ce label.

Les normes concernant les prises d'escalade ont été élaborées par la commission "S53V" et celles régissant l'utilisation des tapis de réception ont été établies par la commission "S530". Pour faire respecter ces normes, des organismes, habilités en France par le ministère de l'industrie, effectuent des contrôles réguliers. Toute irrégularité vis-à-vis de ces normes conduisant à un dommage corporel constitue une circonstance aggravante pour le fabricant.

Ces normes ou d'autres similaires sont respectées dans beaucoup d'autres pays en dehors de l'Europe.

La législation encadre également l'utilisation des EPI. Il existe trois catégories d'EPI pour protéger la personne : la première concerne les agressions superficielles, la seconde les agressions graves et la catégorie 3 protège contre les dangers mortels.

En escalade, les EPI de catégorie 1 sont par exemple les gants, les lunettes ou écrans de protection. Il est nécessaire qu'il comporte au moins la mention "CE". La seconde catégorie encadre notamment les casques et les crampons. Ils doivent comporter la mention "CE" et l'indication de l'année de fabrication : par exemple "CE12" pour un casque fabriqué en 2012. Enfin la catégorie 3 encadre par exemple les cordes, les baudriers, les mousquetons. Ceux-là doivent comporter la mention "CE", l'année de fabrication ainsi que le numéro du laboratoire agréé (par exemple "CE12987").

Les motivations qui amènent une personne à pratiquer l'escalade sont multiples et propres à chacun. Cependant il existe quelques domaines souvent cités afin d'expliquer les raisons de pratiquer l'escalade.

L'escalade en extérieur se pratique quasiment exclusivement en milieu naturel, ce qui donne l'occasion aux grimpeurs de visiter des sites à l'écart de la civilisation et de profiter du cadre. Nombre de sites d'escalade sont situés dans des parcs nationaux, comme les gorges du Verdon, le parc national de Yosemite ou encore Hueco Tanks, permettant ainsi aux grimpeurs de voir une faune et une flore spécifique, en plus de disposer de panoramas réputés. De plus, le fait de parcourir les parois lors d'ascensions permet de voir certains animaux ou paysages d'un point de vue unique, qui est impossible à avoir depuis les chemins habituels, comme les sentiers de randonnée.

Le défi physique que représente l'escalade est souvent source de motivations pour les grimpeurs. L'escalade demande de nombreuses capacités comme la force, la souplesse, l'endurance ou encore l'équilibre afin d'être pratiquée. De plus, elle demande d'avoir une musculature relativement complète car elle fait appel aux muscles des jambes et des bras mais aussi à ceux du dos, du torse et de la ceinture abdominale.

Une notion importante est la part psychologique que représente la pratique de l'escalade. Le grimpeur va devoir gérer son appréhension du vide ainsi que sa peur de la chute dans le cas d'une ascension en solo intégral, ceci afin d'être en mesure de réussir son ascension. La gestion de ce stress demande de la pratique et fait partie des problématiques qu'il est essentiel de savoir gérer et demande souvent au grimpeur d'apprendre à se dépasser mentalement. De plus, la réussite d'une ascension procure au grimpeur une satisfaction souvent motrice de motivation non seulement à cause de l'adrénaline, mais aussi car ce succès est lié à un sentiment de maîtrise de ses actions et de sa vie.

Les compétitions officielles d'escalade sont administrées à leur création par l'Union internationale des associations d'alpinisme (UIAA), puis dès 2007 par la Fédération internationale d'escalade (). Au niveau international, elles sont organisées sous deux formes, des championnats du monde qui ont lieu une fois tous les deux ans et une Coupe du monde qui se déroule en plusieurs étapes. L'escalade est aussi représentée aux Jeux mondiaux depuis son édition de 2005 à Duisbourg. De plus, des championnats continentaux se tiennent de manière bisannuelle, comme les Championnats d'Asie, les Championnats panaméricains et les Championnats d'Europe. Il existe aussi de nombreuses compétitions de niveau national administrées par les fédérations nationales de chaque pays, ainsi que des compétitions promotionnelles comme le Melloblocco, qui est organisée chaque année, depuis 2004, sur des blocs naturels dans la région de Val Masino en Italie ou encore le Petzl Roc Trip.

Les compétitions se tiennent le plus souvent en salle sur des murs d'escalade, mais aussi sur des murs extérieurs, permanents ou provisoires comme pour les étapes de la Coupe du monde qui se déroulent à Chamonix en France. Elles se déroulent généralement en trois tours : qualifications, demi-finale et finale, avec possibilité de super-finale en cas d'ex-æquo à la première place. Il existe trois disciplines principales : la difficulté, le bloc et la vitesse.

L'escalade est au programme des Jeux olympiques d'été de 2020 en tant que sport additionnel. La compétition devrait se présenter sous la forme d'une discipline olympique unique, car seulement deux médailles ont été attribuées par le CIO à l'escalade.

Durant les épreuves de difficulté, les concurrents grimpent les mêmes voies en tête, les uns après les autres. Ces voies doivent faire un minimum de de longueur pour de largeur et avoir une hauteur minimale de . Le vainqueur est celui qui atteint le plus haut point de la voie, en un seul essai. Une voie est réussie (comptée ) lorsque la dernière dégaine de la voie a été ; si elle n'est pas réussie la dernière prise tenue par le grimpeur est comptabilisée. Pour le classement, on tient compte également de la façon dont la dernière prise a été utilisée. Un grimpeur qui l'aura valorisée en initiant un mouvement vers la prise suivante sera classé devant celui qui l'aura simplement tenue. Depuis 2012, la durée d'ascension est prise en compte pour le classement en cas d'égalité. Cependant le temps limite pour la tentative de chaque grimpeur est de . Passé ce délai, le compétiteur est arrêté dans sa progression et la hauteur est mesurée à l'endroit de cet arrêt.

Durant le tour de qualification des compétitions de difficulté, les compétiteurs doivent grimper deux voies. Le classement est alors obtenu en effectuant la moyenne du classement obtenu sur chacune des deux voies. À l'issue des qualifications, sont retenus les 26 meilleurs pour la demi-finale. À l'issue de la demi-finale, il ne reste plus que les 8 mieux classés. En cas d'égalité sur un tour, les concurrents sont départagés d'après les résultats des tours précédents.

La plupart du temps, les compétiteurs doivent grimper la voie à vue. Cela signifie qu'ils ne sont pas autorisés à voir les autres grimpeurs sur la voie car autrement leurs concurrents pourraient voir les astuces ou les erreurs des grimpeurs les ayant précédés, ce qui leur donnerait un avantage important. Ils ne peuvent pas non plus recevoir de conseils d'autres grimpeurs, et n'ont qu'un temps limité pour observer et la voie à son pied. Sinon les grimpeurs grimpent la voie "flash", après avoir pu observer les techniques et enchaînements donnés par l'ouvreur de la voie, qui effectue une démonstration, puis par les autres grimpeurs.

Les épreuves officielles de bloc se déroulent sur un circuit à vue de cinq blocs pour les qualifications et de quatre blocs pour les demi-finales et les finales. Sur chaque bloc, les prises de départ à utiliser avec les mains et les pieds sont imposées, ainsi que la prise d'arrivée qui doit être tenue à deux mains. Une prise intermédiaire dite est également matérialisée.

Chaque compétiteur dispose d'un temps fixe, de cinq minutes durant les qualifications et les demi-finales, et de quatre minutes pour les finales, pour observer et tenter de réussir chacun des blocs, en réalisant plusieurs essais si nécessaire. Entre chaque bloc, il bénéficie d'une période de repos de même durée. Pour chaque tour, les compétiteurs sont classés selon : le nombre de blocs réussis, en ordre décroissant, puis la somme des nombres d'essais pour réussir les blocs, par ordre croissant, puis le nombre de prises bonus tenues, en ordre décroissant, et enfin la somme des nombres d'essais pour tenir les prises bonus, par ordre croissant.

La formule ' voit tous les compétiteurs d'une même catégorie disposer d'un temps commun, généralement deux à trois heures voire davantage, pour tenter de venir à bout du plus grand nombre de blocs possibles parmi les plusieurs dizaines qui leur sont proposés, dans l'ordre qu'ils choisissent. Le nombre d'essais n'est pas pris en compte. Chaque bloc réussi rapporte finalement divisés par le nombre de fois où il a été réussi (le grimpeur qui est seul à réussir un bloc reçoit points, si 5 grimpeurs en réussissent un autre, ils reçoivent chacun /5=200 points). Le vainqueur est celui qui aura obtenu le plus grand total de points. La formule ' est réservée au premier tour qualificatif des compétitions de bloc (parfois l'unique tour).

Les épreuves de vitesse se déroulent sur deux voies identiques durant lesquelles les concurrents doivent atteindre au plus vite le sommet. Le vainqueur est celui qui réalise le meilleur temps. Les grimpeurs qui tombent avant d'arriver au sommet de la voie sont disqualifiés. Lors des qualifications, chaque grimpeur effectue généralement deux essais. Le classement est effectué d'après le meilleur des deux temps ou d'après le total des deux temps réalisés.

Suivant le nombre de compétiteurs, les 4, 8 ou 16 mieux classés accèdent au tour final qui se déroule sous forme d'élimination directe. Le est opposé au dernier classé, le à l'avant-dernier, etc.

Le record du monde absolu est détenu depuis octobre 2012 par le Russe Evgeny Vaytsekhovsky qui a grimpé le mur officiel de en . Cela représente une progression de . Cette performance a été établie à l'occasion de la Coupe du monde d'escalade à Xining en Chine.

Des compétitions d'escalade handisport sont organisées. Les sportifs y concourent par catégories : celle des aveugles et mal-voyants, celle des amputés et handicapés physiques et celle des déficients neurologiques. Le premier championnat du monde d'escalade handisport a eu lieu en juillet 2011. Le premier championnat du monde de bloc handisport est organisé en 2014.

Durant son apprentissage, un grimpeur voit son niveau progresser au fur et à mesure de la pratique de l'escalade. Cependant, il peut présenter un souhait d'atteindre de meilleures performances soit dans le cadre de la compétition ou pour atteindre des objectifs personnels. Pour cela il peut mettre en place des techniques d'entraînement, par exemple en s'aidant de matériel spécifique.

L'entraînement s'organise selon différents plans : la technique, le physique et le plan mental et stratégique. Une progression dans ces différentes composantes permettra au grimpeur d'améliorer son niveau. Cela peut s'organiser en fonction du type de pratique : par exemple, sur le plan physique, les grimpeurs de bloc favorisent le développement de la puissance, les grimpeurs de voie cherchant en plus à améliorer leurs qualités de résistance et de récupération dans l'effort.

En premier lieu, le grimpeur entraîne — naturellement — sa technique de par la pratique de base de l'escalade. Il apprend alors à placer son corps de manière adéquate et doit aussi acquérir une maîtrise des placements de pieds afin d'économiser au maximum ses membres supérieurs. À partir d'un certain niveau, il doit aussi passer par un apprentissage et une mise en pratique des différents mouvements d'escalade afin de continuer à progresser. Ce but est souvent atteint en diversifiant les supports, les types de prises ou de rochers pour acquérir des techniques spécifiques supplémentaires. De plus, la pratique du bloc ou les entraînements dans des salles de bloc ou de pan permettent de travailler certains mouvements spécifiques.

Selon le type d'escalade pratiqué, il est nécessaire d'apprendre à utiliser le matériel de manière efficiente. Lors d'escalade en terrain d'aventure ou artificielle, la pose de points d'assurage est nécessaire, mais doit aussi être parfaitement maîtrisée, d'une part pour être certain du bon fonctionnement du matériel, mais aussi afin de passer le moins de temps à les mettre en place, car cela entame les réserves d'énergie du grimpeur et limite ses capacités lors de l'ascension.

En second lieu, le grimpeur cherche à améliorer son niveau sur le plan physique. Mais de par ses types d'efforts très différents, la pratique de l'escalade fait appel principalement à trois filières énergétiques : la , la résistance et la continuité. 
En améliorant sa force, le grimpeur sera capable d'être plus performant sur le plan musculaire, il pourra fournir une puissance musculaire plus importante pendant un temps réduit. S'il améliore sa résistance, il sera alors capable de fournir un effort d'intensité moyenne plus souvent. Pour finir, en entraînant sa continuité, il sera en mesure d'enchaîner les efforts après des courtes pauses ou repos.

Il est également nécessaire d'entraîner d'autres composantes physiques, notamment l'endurance, qui est la capacité à fournir un effort long, sans repos et s'inscrivant dans la durée, mais aussi la souplesse et l'élasticité de ses muscles (par étirements) ; cela a pour effet d'offrir plus de possibilités au corps, notamment pour atteindre des prises ou réaliser des mouvements qui n'étaient pas possibles auparavant. Le grimpeur peut également s'entraîner pour renforcer sa résistance articulaire. Lors de l'escalade, les articulations — et particulièrement les doigts — sont très sollicitées. C'est cela que le grimpeur entraînera particulièrement notamment pour éviter les blessures.

Enfin pour améliorer son niveau, le grimpeur pourra améliorer son mental et sa stratégie face à une voie. Avant d'entamer une escalade, le sportif a la possibilité de visualiser les prises présentes et les mouvements à effectuer pour atteindre le sommet. Cette préparation s'appelle la . Ainsi le grimpeur peut améliorer cette phase en favorisant une bonne mémorisation, une bonne concentration et une prise de décision juste.

Aussi, sur le plan mental la personne qui escalade peut parfaire sa confiance, notamment envers le matériel utilisé, envers son assureur qui le retient en cas de chute mais aussi en lui pour tenter des mouvements durs. Comme dans d'autres sports un esprit combatif est à privilégier pour atteindre ses objectifs.

Stratégiquement, le grimpeur s'entraînera aussi à mieux se gérer personnellement. Tout d'abord il peut apprendre à organiser sa grimpe pour éviter des creux de fatigue. Aussi, il fera attention dans ses cycles d'entraînement à bien s'échauffer pour éviter les blessures et aussi à reconnaître le moment opportun pour tenter une voie ou un mouvement difficile. Enfin en vue de maintenir un bon niveau de forme, le grimpeur apprendra à correctement s'hydrater et se nourrir mais aussi à éviter le surentraînement qui peut amener à se blesser, se fatiguer inutilement ou se démotiver.

Pour s'entraîner, les grimpeurs ont à disposition plusieurs moyens d'entraînement qui dépendent des objectifs fixés. De manière générale, un grimpeur peut pratiquer la course à pied, le footing ou la corde à sauter pour améliorer sa forme. Plus spécifiquement il peut pratiquer la musculation pour entraîner des muscles plus particuliers, par exemple avec une barre de traction ou des anneaux.

Des outils d'entraînement spécialisés pour l'escalade existent. Il s'agit par exemple du pan qui regroupe une quantité importante de prises afin d'offrir un grand échantillon de gestuelles possibles. Le pan Güllich ou la poutre permettent également un entraînement des mouvements spécifiques à l'escalade.

Enfin, plus généralement le grimpeur peut aussi se servir d'un chronomètre, d'un cardiofréquencemètre ou d'une caméra pour pouvoir mesurer ses performances et ensuite étudier comment les améliorer. Dans une situation de renforcement musculaire, l'entraînement peut consister à utiliser des poids, soit en musculation (haltère, tractions, etc.) soit en situation de grimpe.

L'escalade est considérée comme un sport extrême ; elle est notamment intégrée aux X Games, ce qui contribue à lui donner une image de sport à risques. Cependant, parmi les sports liés à la montagne, l'escalade est un des moins accidentogène.

Selon l'institut de veille sanitaire (France), sur la saison estivale 2000-2003, seulement 11 décès et 239 victimes liés à la pratique de l'escalade sont dénombrés, en comparaison de 203 et pour la randonnée pédestre, et 130 et pour l'alpinisme. Une étude annuelle menée par le Club alpin suisse depuis 1984 confirme la même tendance pour la Suisse, où l'escalade compte en moyenne 6 décès par année contre 44 pour la randonnée et 37 pour la haute montagne.

Relativement, ces chiffres sont bas. Ce constat est vrai également au rapport du nombre d'interventions. Selon le même rapport de l', sur .

Mais l'escalade, comme la plupart des sports, présente des risques. Ceux-ci sont principalement de deux natures, chute du grimpeur ou chute d'objets. Pour chacun, des existent afin de pallier ces dangers.

La chute du grimpeur, relativement fréquente en escalade, n'entraîne généralement pas de blessures car elle est amortie par la chaîne d'assurage : assureur, dispositif d'assurage, corde, points de progression et baudrier. Néanmoins, des défaillances dans cette chaîne peuvent causer une longue chute, une chute violente (chute de facteur 2), voire un retour au sol. Les défaillances les plus fréquentes sont une faute d'attention de l'assureur, un mauvais encordement, une mauvaise utilisation du dispositif d'assurage, voire une rupture de point de progression (surtout en escalade artificielle). De par les normes très strictes posées sur le matériel, les erreurs humaines dominent sur les défaillances du matériel.

Dans les sites naturels, des chutes d'objets peuvent se produire : rocher instable, bloc de glace (en cascade de glace), matériel perdu par les cordées situées au-dessus, ou même objets lancés par des individus inconscients situés en haut des voies. Afin de s'en protéger, le port du casque est vivement recommandé. De plus, le bon sens incitera les grimpeurs à crier ou s'ils doivent lancer une corde ou s'il leur arrive de faire glisser une pierre. Ce risque existe également mais dans une moindre mesure en intérieur. Cela peut arriver avec la chute de matériel lors de sa manipulation en haut de voie ou de prises d'escalade lors de leur installation sur le mur. Dans ce genre de cas, plutôt que de porter un casque, il est recommandé de ne pas passer dans la zone au sol où se situe le danger.

Bien que ce soit relativement rare, il arrive de se blesser lors de la pratique de l'escalade ou de l'entraînement. La gravité et la diversité des blessures peuvent varier grandement selon les cas, et on peut répertorier des blessures allant de la simple égratignure aux traumatismes lourds pouvant entraîner un handicap irréversible.

Les blessures sont issues de trois différentes causes : la chute du grimpeur, la chute d'un élément externe, la pratique sportive en elle-même.

Les blessures causées par la chute du grimpeur varient selon le type d'escalade pratiqué. Dans le cadre de l'escalade sportive, elles sont généralement mineures car le grimpeur peut disposer de nombreux points d'assurage qui l'empêcheront de faire une chute trop importante et donc de se blesser gravement. Les blessures sont alors dues au contact avec la paroi et vont des petites éraflures aux contusions. Le risque de toucher la paroi lors d'une chute varie grandement avec le type de voie pratiquée. Sur une voie en dévers, ce risque est réduit alors que sur une paroi en dalle, il est augmenté.

Lors de la pratique de l'escalade traditionnelle, les blessures peuvent rapidement être plus graves qu'en escalade sportive, car les points d'assurage sont, soit peu fiables, soit inexistants et le grimpeur doit placer lui-même ses propres protections. De ce fait les points sont susceptibles de ne pas supporter la violence d'une chute, ce qui augmente la hauteur potentielle de chute avant que le grimpeur ne soit retenu par la corde. Dès lors, le grimpeur risque de frapper violemment la paroi, ce qui peut conduire à des blessures graves. De plus, à cause de l'augmentation de la hauteur de chute, il arrive que le grimpeur ne soit pas retenu par la corde et qu'il finisse sa course en tombant au sol. Ce type d'accidents est souvent la cause de blessures graves comme des fractures des membres inférieurs, du bassin ou de la colonne vertébrale. Dans certains cas, il arrive que le grimpeur se retourne, se retrouvant ainsi dos à la paroi, et que sa tête ou son rachis heurte la roche. Ce type d'accident peut être rapidement très grave car le choc peut provoquer un traumatisme crânien.

Dans le cadre du bloc, les chutes peuvent avoir une conséquence supplémentaire car le grimpeur n'est assuré par aucune corde. C'est le qui amortira l'atterrissage et il n'est pas rare de voir des personnes se tordre la cheville lors d'une mauvaise réception. De plus, lors de l'escalade de blocs de grande hauteur, des lésions aux genoux, aux hanches et à la colonne vertébrale peuvent survenir. Une chute à côté du peut aussi être la cause de blessures car le grimpeur risque alors d'atterrir sur un rocher ou une racine d'arbre.

Lors de la pratique de l'escalade en solo intégral, le grimpeur n'a aucun système d'assurage, les blessures sont donc généralement fatales car la chute du grimpeur se termine immanquablement au sol. Les lésions varient principalement en fonction de la hauteur de chute et vont des fractures à la paralysie, si elles ne causent pas la mort du grimpeur.

La chute d'un élément externe, comme un morceau de roche, peut causer des blessures dont la gravité dépend de la taille de l'objet et de la hauteur de la chute. Cet accident demeure malgré tout assez rare. Les lésions vont de la simple égratignure jusqu'à, dans certains cas extrêmes, la mort du grimpeur ou de l'assureur. Il n'est pas rare de faire tomber de petits cailloux lors d'une ascension qui, s'ils ne sont pas une source de risques pour le grimpeur, peuvent l'être en revanche pour la personne qui s'occupe de l'assurage ou pour une autre cordée.

Le port d'un casque permet de limiter notablement les risques dans de tel cas ou, du moins, de limiter la gravité des blessures à la tête. Le décès de Jean Couzy dans le massif du Dévoluy en 1958, victime d'une chute de pierre, a contribué à la prise de conscience de l'importance du port du casque en escalade.

Les blessures causées par la pratique de l'escalade sont dues à des efforts trop importants sur une ou des régions du corps. Elles touchent principalement les articulations, les muscles et les tendons qui sont énormément sollicités lors de certains mouvements ou pour tenir des prises de petites tailles. Les doigts et les mains sont d'ailleurs particulièrement sujets aux lésions dues à un effort trop violent. Selon une étude en ligne, la main représente un tiers des lésions tandis que les membres inférieurs (genou, cheville et pied) en représentent un quart ; le reste est occupé par le rachis, l'épaule et le coude.

Une des plus courantes lésions de ce type est la rupture de poulie, qui arrive généralement lors d'une mise en charge violente sur une prise de petite taille ; elle consiste en une déchirure partielle ou complète d'une ou plusieurs poulies digitales, qui servent à maintenir les tendons fléchisseurs des doigts au contact du squelette. Cette lésion est assez spécifique à l'escalade

La tendinite est aussi une des affections régulièrement rencontrées en escalade à cause des efforts répétés sur les tendons. Elles apparaissent le plus souvent au niveau des doigts et du poignet, mais peuvent aussi survenir au coude ou à l'épaule.

Dans les traumatismes, les membres inférieurs sont les plus visés lors de la pratique du bloc tandis que ce sont les membres supérieurs et en particulier la main qui sont touchés lors de la pratique de la voie.

L'encadrement en escalade se fait généralement par des moniteurs d'escalade ou des guides de montagne, mais dans le cadre de cours d'éducation physique et sportive ou de camps sportifs, il peut aussi être fait par des professeurs d'éducation physique. Un encadrant qualifié dispense les connaissances permettant d'évoluer en sécurité dans la pratique de l'escalade.

En France, l'encadrement bénévole se distingue de celui rémunéré.

Ainsi dans le milieu associatif, les clubs sportifs liés au milieu de la montagne, affiliés au Club alpin français, à la Fédération française de la montagne et de l'escalade ou à la Fédération sportive et gymnique du travail, dispensent des formations d'initiateur fédéral escalade. Ces initiateurs escalade seront alors habilités à encadrer des groupes de grimpeurs même si ces derniers ne sont astreints à aucune obligation de formation. Ces formations sont néanmoins fortement conseillées.

Parmi ces formations, certaines sont tournées uniquement vers les SAE tandis que d'autres portent sur les SNE. Les premières sont plus rapides à passer tandis que les secondes sont plus polyvalentes. Le monitorat fédéral permet cela également mais permet aussi l'accès à la performance ainsi qu'un encadrement sur "grands espaces" (sites de plusieurs longueurs).

Les moniteurs d'escalade titulaires d'un Brevet d'État d'éducateur sportif option escalade sont formés pour encadrer et enseigner l'escalade contre rémunération dans toutes ses dimensions, à condition que l'altitude soit inférieure à . Ils sont aussi habilités à encadrer et enseigner le canyonisme. Les guides de haute montagne, formés en France par l'École nationale des sports de montagne, disposent aussi de ces prérogatives, sans limite d'altitude.

En 2011, les titulaires du Certificat de spécialisation en activité escalade rattaché au Brevet professionnel de la jeunesse, de l’éducation populaire et du sport "activité physique pour tous" peuvent encadrer la pratique de l'escalade contre rémunération en SAE et en SNE, sur voie d'une longueur (jusqu'au premier relais), d'une longueur maximum de et classée en secteur découverte. Le Diplôme d'État de la jeunesse, de l'éducation populaire et du sport faisant partie de la "nouvelle filière des diplômes" complète cette gamme de formations.

Les formations STAPS donnent également les prérogatives pour encadrer l'escalade contre rémunération. Les diplômes STAPS sont référés au répertoire national des certifications professionnelles. Il s'agit pour un titulaire d'une licence STAPS d'obtenir sa carte professionnelle auprès de la direction départementale de la jeunesse et des sports. Deux approches sont possibles :

En Suisse, l'Association suisse des guides de montagne délivre un titre de moniteurs d'escalade après une formation et des examens pratiques et théoriques. Le candidat doit notamment être capable de grimper une voie cotée 7b pour les hommes et 7a+ pour les femmes.

Comme d'autres sports de nature, l'escalade en extérieur crée des débats à propos de ses impacts négatifs sur l'environnement. 

L'escalade implique une présence humaine sur des falaises, un milieu naturel généralement inaccessible, et aux abords de celles-ci. Les grimpeurs peuvent ainsi avoir des impacts importants sur les espèces rupestres animales et végétales, parfois fragiles ou menacées. Ils sont source de dérangement (visuel, sonore) pour la faune rupestre : oiseaux en période de nidification, rapaces, lézards et serpents, chauve-souris, bouquetins, etc. En nettoyant les falaises (purge de rochers, débroussaillage), en aménageant les abords (accès) ou en piétinant la végétation, les grimpeurs peuvent détruire des espèces végétales fragiles et rares, ou favoriser l'érosion des sols.

À l'identique d'autres activités de nature, la surfréquentation de certains secteurs peut être une source de pollution ou de nuisances pour les propriétaires de terrains et les riverains : déchets abandonnés, bivouac et camping sauvage, stationnement de véhicules, bruit, perturbation du gibier ou du bétail.

Selon les pays, l'escalade peut être restreinte ou interdite dans certains secteurs, pour des motifs du préservation du biotope. La pratique de l'escalade est parfois l'objet de concertations, de conflits ou de conventions, entre les grimpeurs, les associations de défense de l'environnement, les parcs nationaux et les pouvoirs publics (mairie, élus), les propriétaires et usagers des terrains.

Pour le public non-initié, l'escalade est souvent associée à une aventure, une activité risquée, un sport extrême procurant des « sensations fortes » basées sur la peur du vide et d'une chute au sol. Cette image est souvent reprise par les médias, alors qu'elle ne correspond pas à la majorité des pratiques modernes toujours plus sécurisées, sur falaises équipées ou structures artificielles, et que la majorité des grimpeurs exprime au contraire un « refus du risque » et une idée d'aventure basée sur la performance sportive.

De par ses différents aspects, l'escalade véhicule d'autres images qui sont parfois utilisées dans le milieu de la publicité, autant télévisée que papier. L'image d'une cordée de deux grimpeurs soudés est intéressante pour renforcer l'esprit d'équipe dans une entreprise. Les aspects de dépassement de soi, de concentration et de détermination motivent aussi les agences de communication à utiliser l'escalade.

Duracell fait pratiquer ce sport à un de ses lapins mythiques pour vanter les mérites de longévité ou d'endurance de ses piles. La marque de barres de céréales Grany utilise l'aspect avec une publicité mettant en image Patrick Edlinger diffusée en 2004. Cette publicité en reprend une diffusée plus tôt en 1988 de la même marque mettant en avant le du sport pour vanter les mérites du produit. La technique puriste de Patrick Edlinger est aussi utilisée pour apporter une image des barres Grany.








</doc>
<doc id="1080" url="https://fr.wikipedia.org/wiki?curid=1080" title="Extrême droite">
Extrême droite

Le terme extrême droite est employé pour classer des mouvements, des organisations et des partis politiques historiquement disposés à l'extrême droite des hémicycles parlementaires. La question de sa délimitation suscite le débat, surtout celle de ses « invariants », mais plusieurs usages en sont distingués. 

Auparavant, étaient d'extrême droite les mouvements comme l'Action française, marqués par la défense de la tradition contre l'époque moderne (et emblématiquement, contre la franc-maçonnerie), l'anti-parlementarisme, l'autoritarisme, la doctrine basée sur la race, les idées réactionnaires. Ceux qui se réclament de ces idées aujourd'hui, comme nombre de catholiques traditionalistes, y sont toujours classés.

De plus, le terme est associé aux mouvements défaits par la Seconde Guerre mondiale, tels que le fascisme italien et le national-socialisme allemand, fondamentalement rejetés depuis lors, ce qui en explique l'usage souvent péjoratif. 

De fait, l'extrême droite se distingue de la droite par une contestation du capitalisme voire du libéralisme mais s'en rapproche par sa vision des institutions démocratiques. L'ordre spontané que les libéraux trouvent dans l'économie politique, est plutôt l'unité organique de la nation, de la race ou de la communauté de croyants. Par sa croyance en des lois intangibles (surnaturelles ou naturelles), l'extrême droite partage avec les conservateurs le refus de la révolution permanente (auquel répond la métapolitique) et avec Burke, la critique des droits de l'homme.

Par extension, certains qualifient d'extrême droite les mouvements qui s'opposent à un sens sinistriste de l'histoire, et partant, à leur vision de la gauche. Si le populisme, le nationalisme ou au moins le patriotisme ont été portés par de nombreuses gauches, certains internationalistes aujourd'hui n'hésitent pas à placer leurs partisans à l'extrême droite. De même chez les adversaires des positions royalistes, national-syndicalistes, ethno-différencialistes ou traditionalistes.

Enfin, il est à noter que la plupart des mouvements concernés rejettent le qualificatif d'extrême droite. Les catholiques traditionalistes et certains fascismes rejettent la dualité communisme-capitalisme, qu'ils estiment tous deux matérialistes et aliénants — d'où le concept de Troisième voie en France.

La science politique anglo-saxonne parle plus volontiers de "droite radicale" signifiant par là qu'elle se situe à la limite de la droite démocratique, libérale ou conservatrice.

Pour se distinguer de l'extrême droite, la droite traditionnelle entretient, majoritairement, un refus d'alliance, dont une illustration est le en Belgique. Dans certains pays, comme les Pays-Bas, l’Autriche ou Israël, des alliances sont nouées. Enfin, dans d'autres pays, la droite classique tente de préempter les thèmes porteurs de l'extrême droite.

Certaines personnalités ou structures politiques peuvent évoluer considérablement comme l'illustre, en Italie, la trajectoire du Mouvement social italien (néo-fasciste) devenu Alliance nationale (centre-droit) ou, en France, Alain Madelin, Gérard Longuet, Patrick Devedjian, Hervé Novelli.

Si les mouvements ou partis d'extrême droite sont divers, leurs socles idéologiques comportent des points communs : un patriotisme, un nationalisme et un traditionalisme encore plus poussés qu'à droite, un fort attachement à des valeurs nationales, identitaires, culturelles et/ou religieuses, et, parfois, un discours économique et social plus contestataire et se voulant plus proche des milieux populaires que celui de la droite traditionnelle.

La xénophobie fait aussi partie de ce socle commun. Des politologues affirment que « "deux sujets essentiels cristallisent le vote d'extrême-droite : la xénophobie et le discours sécuritaire" ». Cependant, selon la géopolitologue Béatrice Giblin-Delvallet, « "Les ressorts communs à la montée de l'extrême droite en Europe que sont l'immigration musulmane, la mondialisation (à laquelle la désindustrialisation et la montée du chômage sont associées) et l'Union européenne ne suffisent cependant pas à effacer les particularités des situations nationales de chaque État." ».

Le terme d'extrême droite englobe des mouvements parfois contradictoires :

En 2005, à partir d’une enquête menée dans cinq pays européens (Allemagne, Belgique, France, Italie, Pays-Bas) auprès de militant(e)s d’extrême droite, un chercheur de l'Université libre d'Amsterdam démontre que le trait commun qui structure leur identité politique est la stigmatisation dont ils font l’objet.

Ainsi, comme beaucoup de formations qualifiées d' récusent cette étiquette, une véritable catégorisation politique s'avère difficile. En France, Jean-Marie Le Pen déclare par exemple être . Les partis concernés préfèrent souvent d'autres appellations, comme , , , ou simplement .

Il sera donc question d’ à propos de la tendance activiste et protestataire issue directement des mouvements fascistes, nazis et racistes des années 1930 à 1960. Le terme de sera utilisé pour les partis constitués plus récemment autour de problématiques liées à la crise : chômage, immigration, identité nationale, etc. et qui mettent en œuvre des stratégies de prise de pouvoir électorale.

Les moyens mis en œuvre par les partis ou organisations d'extrême droite sont divers :

La base électorale de l'extrême droite reste avant tout les milieux populaires : petits commerçants, artisans, ouvriers, etc. Elle suit en effet une ligne « anti-élite » (voire parfois contre-révolutionnaire), se différenciant ainsi de la droite conservatrice et libérale.

En France, par exemple, et selon la géographe Catherine Bernié-Boissard (coauteur du livre "Vote FN, pourquoi ?"), .

Selon une étude de l'Église réformée de France, l'électorat de l'extrême droite est majoritairement masculin, peu diplômé et anti-politique.

À partir du cas de la France, Michel Winock dans son ouvrage "Nationalisme, antisémitisme et fascisme en France" (2004), donne les neuf caractéristiques suivantes aux mouvements d’extrême droite qui découlent du discours de la décadence, « vieille chanson que les Français entendent depuis la Révolution » :

Les interprétations qu’il en donne sont de quatre ordres :

Pour Jean-Yves Camus, dans un contexte de mondialisation et de montée des inégalités, l'extrême droite « s'impose plus que jamais comme principale force de contestation du consensus idéologique imposé par le modèle social ultralibéral ».

L'extrême droite est composée de divers courants (convergents ou antagonistes) parmi lesquels :

D'autres formations sont selon certains observateurs considérées comme d'extrême droite, mais cette classification peut être davantage controversée :

Selon les pays et les contextes, le terme « extrême droite » est, de fait, utilisé pour trois types de visions ou de mouvances :

La présence de ces mouvements par pays est aussi très variable. La situation des États-Unis, avec une extrême droite à l'influence marginale s'oppose ainsi à celles de plusieurs pays européens, (Autriche, Norvège, Danemark, Suède…) où leur présence parlementaire est de plus en plus marquée. De façon intermédiaire, des pays tels que le Japon disposent de mouvements d'extrême droite non parlementaires, mais ayant une forte capacité d'influence sur le principal parti au pouvoir. 

Dans certains pays, comme l’Autriche, Israël et plus récemment la Norvège, l’extrême droite participe parfois aux coalitions gouvernementales avec la droite et le centre-droit, et possède donc à l’occasion des ministres. Au Danemark, l'extrême droite a, au début des années 2000, apporté son soutien à un gouvernement, sans toutefois y participer.

Depuis 2000, une dizaine de pays européens ont connu des participations de l'extrême droite au gouvernement ou bien un soutien parlementaire à des gouvernements, pouvant être de centre-droit ou de centre-gauche : Parti de la liberté d'Autriche (FPÖ), Parti national slovaque (SNS), Patriotes unis de Bulgarie, Parti pour la liberté des Pays-Bas (PVV), Parti populaire danois, Parti du Progrès norvégien, les Vrais Finlandais, Union démocratique du centre en Suisse, la Ligue du Nord italienne et l'Alerte populaire orthodoxe (LAOS) en Grèce.

Si dans les années 1970, dirigeait une formation d'extrême droite en Suède (le Parti du Reich Nordique), c'est à partir du que les partis d'extrême droite et de droite populiste européens se féminisent, plusieurs de leurs figures de proue étant des femmes, comme Marine Le Pen (France), Siv Jensen (Norvège), Krisztina Morvai (Hongrie), Pia Kjaersgaard (Danemark), Anke Van dermeersch (Belgique), Alessandra Mussolini (Italie) ou encore Eleni Zaroulia (Grèce). Ces personnalités et leurs partis respectifs ne se situent pas tous sur la même ligne politique, certains étant issus d'une droite radicale populiste, d'autres étant clairement fascisants.

En France, l'extrême droite remonte à la deuxième moitié du et coïncide avec la fin de la monarchie. Parmi les idées caractéristiques des mouvements classés à l'extrême droite, ont figuré à titres divers notamment l'antiparlementarisme, l'islamophobie, l'antisémitisme ou le nationalisme ou bien encore l'homophobie et le sexisme chez certaines personnes. Les mouvements actuellement classés à l'extrême droite en Europe sont souvent accusés de racisme et de xénophobie en raison de leur hostilité générale à l'immigration et des positions ouvertement racistes revendiquées par certains d'entre eux.








</doc>
<doc id="1084" url="https://fr.wikipedia.org/wiki?curid=1084" title="Exemples d'équations différentielles">
Exemples d'équations différentielles

Cet article présente quelques exemples d'équations différentielles.

Les équations différentielles sont utilisées pour résoudre des problèmes en physique, en ingénierie et dans plusieurs autres sciences.

Les équations différentielles les plus simples sont les équations linéaires homogènes du premier ordre. Par exemple :

où formula_2 est une fonction connue admettant des primitives. Nous pouvons résoudre cette équation en la réorganisant:

où formula_4. En l'intégrant on obtient

où "A" est une constante arbitraire. (On peut vérifier que y est solution)

Prenons une masse reliée à un ressort. Il exerce sur celle-ci une force de rappel proportionnelle à l'extension ou la compression du ressort par rapport à sa longueur au repos. Nous négligeons les autres forces : gravité, frottement, etc. Nous pouvons alors décrire l'allongement du ressort à un temps formula_6 comme une fonction formula_7. Cette fonction vérifie alors l'équation différentielle suivante :

Dont les solutions sont :

Pour déterminer les constantes formula_10 et formula_11, nous utilisons les conditions initiales qui permettent de décrire l'état du système à un instant donné (correspondant en général à formula_12).

Par exemple si nous supposons qu'à l'instant formula_12 l'extension du ressort est d'une unité de longueur (formula_14), et la masse est immobile (formula_15). Nous pouvons en déduire

d'où l'on déduit formula_17.

et donc formula_19.

En conséquence formula_20 est solution de l'équation différentielle étudiée.

Plus souvent en physique pour les oscillations simples non amorties on utilise la solution de la forme:

Pour l'exemple cité on procède:

Ce qui donne formula_26
et par conséquent formula_27

D'où le résultat formula_28

La solution la plus générale en fonction de conditions initiales quelconques formula_29 et formula_30 est donnée par l'équation :

Le modèle précédent négligeait les forces de frottement. De ce fait l'oscillation libre pouvait durer indéfiniment, ce qui n'est jamais observé en réalité.

Les frottements sont en général une force proportionnelle à la vitesse (formula_32) et opposée au mouvement. En rajoutant ce terme notre équation différentielle devient :

Ceci est une équation différentielle linéaire à coefficients constants, homogène et du second ordre, que nous pouvons résoudre.
En cherchant une solution de la forme particulière formula_36, nous constatons que formula_37 doit vérifier l'équation caractéristique suivante :

Si formula_39 nous avons deux racines complexes formula_40, et la solution (avec les conditions initiales identiques au cas précédent) a la forme suivante :

Le système étudié (le pendule pesant dans le référentiel terrestre supposé galiléen) est le siège d'oscillations libres amorties.
Le centre d'inertie de la masse a une trajectoire que décrit la courbe suivante : 
"(ce sont les positions du centre d'inertie de la masse, en fonction du temps, avec formula_43 correspondant à une position d'équilibre)"

NB : la courbe présente une allure proche d'un régime critique : la position d'équilibre est à peine franchie, et on ne compte guère plus d'une pseudo-période d'oscillations.



</doc>
